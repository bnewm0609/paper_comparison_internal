{"id": 244951322, "updated": "2023-04-05 22:51:11.275", "metadata": {"title": "Profiling the Design Space for Graph Neural Networks based Collaborative Filtering", "authors": "[{\"first\":\"Zhenyi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Chuan\",\"last\":\"Shi\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In recent years, Graph Neural Networks (GNNs) have been widely used in Collaborative Filtering (CF), one of the most popular methods in recommender systems. However, most existing works focus on designing an individual model architecture given a specific scenario, without studying the influences of different design dimensions. Thus, it remains a challenging problem to quickly obtain a top-performing model in a new recommendation scenario. To address the problem, in this work, we make the first attempt to profile the design space of GNN-based CF methods to enrich the understanding of different design dimensions as well as provide a novel paradigm of model design. Specifically, a unified framework of GNN-based CF is proposed, on top of which a design space is developed and evaluated by extensive experiments. Interesting findings on the impacts of different design dimensions on recommendation performance are obtained. Guided by the empirical findings, we further prune the design space to obtain a compact one containing a higher concentration of top-performing models. Empirical studies demonstrate its high quality and strong generalization ability.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wsdm/WangZS22", "doi": "10.1145/3488560.3498520"}}, "content": {"source": {"pdf_hash": "e443a0893e88d1ab16dada3c0740f6bfd4e53943", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "55bddccac2277144ceaed32c5c00fe051389b05f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e443a0893e88d1ab16dada3c0740f6bfd4e53943.txt", "contents": "\nProfiling the Design Space for Graph Neural Networks based Collaborative Filtering\n2022. February 21-25, 2022\n\nZhenyi Wang zy_wang@bupt.edu.cn \nHuan Zhao zhaohuan@4paradigm.com \nChuan Shi shichuan@bupt.edu.cn \nZhenyi Wang \nHuan Zhao \nChuan Shi \n\nBeijing University of Posts and Telecommunications\nChina\n\n\n4Paradigm Inc\nChina\n\n\nACM Reference Format\nBeijing University of Posts\nand Telecommunications China\n\nProfiling the Design Space for Graph Neural Networks based Collaborative Filtering\n\nProceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22)\nthe Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22)2022. February 21-25, 202210.1145/3488560.3498520CCS CONCEPTS \u2022 Information systems \u2192 Collaborative filtering KEYWORDS Collaborative filteringGraph neural networksEmpirical evalua- tion\nIn recent years, Graph Neural Networks (GNNs) have been widely used in Collaborative Filtering (CF), one of the most popular methods in recommender systems. However, most existing works focus on designing an individual model architecture given a specific scenario, without studying the influences of different design dimensions. Thus, it remains a challenging problem to quickly obtain a top-performing model in a new recommendation scenario. To address the problem, in this work, we make the first attempt to profile the design space of GNN-based CF methods to enrich the understanding of different design dimensions as well as provide a novel paradigm of model design. Specifically, a unified framework of GNN-based CF is proposed, on top of which a design space is developed and evaluated by extensive experiments. Interesting findings on the impacts of different design dimensions on recommendation performance are obtained. Guided by the empirical findings, we further prune the design space to obtain a compact one containing a higher concentration of top-performing models. Empirical studies demonstrate its high quality and strong generalization ability 1 .Figure 1: An illustration of user-item bipartite graph (left) and the general embedding-based CF framework taking the rating prediction procedure of 2 to 4 as an example (right).\n\nINTRODUCTION\n\nCollaborative Filtering (CF) has been one of the most popular recommender system (RS) methods, which aims at predicting users' preferences based on those who share similar behaviors [12,17,20,30]. In the literature, most CF methods follow an embedding-based paradigm as shown in Figure 1 (right), which first learns lowdimensional representations for users and items, and then uses an interaction function to predict the preferences (ratings) of users to items. Classic Matrix Factorization (MF) [16,24] directly adopts dot product as the interaction function while Neural Collaborative Filtering (NCF) [11] proposes to learn interaction function with neural networks. Very recently, Graph Neural Networks (GNNs) have been incorporated for the CF tasks, since the user-item interactions can be naturally modeled as a bipartite graph as shown in Figure 1 (left) and GNN can learn better user/item representations by capturing the high-order information in the user-item bipartite graph through an iterative message passing (neighborhood aggregation) [39,41]. Numerous successful GNN-based models have been proposed and shown promising results for CF tasks, such as PinSage [41], NGCF [36], LightGCN [10], and MCCF [38].\n\nExisting GNN based methods are mainly limited to designing a single best architecture for a specific scenario, while in realworld applications, the recommendation scenarios are diverse as the datasets differ in their collected domains and properties like scale (large or small) and density (dense or sparse). Such diversity makes it a common practice to design different architectures across different scenarios. To be more specific, the choice of each design dimension, e.g., aggregation function or activation function, in the chosen model varies across recommendation scenarios. For example, in PinSage [41], the non-linear activation function (ReLU) is a natural choice in the deployed architecture in Pinterest, while in a following-up work, LightGCN [10], the authors show that the non-linear activation functions do not always benefit the final performance in other benchmark datasets. Besides, despite various new GNN-based models for CF have been developed, little has been done to systematically understand the influences of different design dimensions of GNN-based CF on recommendation performance. Therefore, each time given a new scenario, huge efforts including computational resources and human expertise have to be invested in exploring the entire huge space of the possible combinations of all dimensions of GNN models to obtain a top-performing model.\n\nTo address the problem, in this work, we propose to profile the design space, a Cartesian product of multiple design dimensions [42], of existing GNN-based methods for CF by empirical evaluation. It not only provides a deeper understanding of the influences of different dimensions on recommendation performance, but also paves the way for a novel paradigm to efficiently design top-performing GNN methods in different CF scenarios.\n\nSpecifically, we firstly propose a unified framework consisting of 4 key modules, i.e., initialization, GNN, multi-component, and interaction, as illustrated in Figure 2(a), which most GNN-based CF methods can fit into. Then on top of this framework, we develop a design space which covers important design dimensions of GNNbased CF models. By incorporating popular choices for each design dimension (Figure 2(b)), we obtain a design space with more than 100,000 different model architectures, even including classic MF methods [16,24] and Multi-Layer Perceptron (MLP), i.e., NCF [11]. Obviously, it is too costly to train and tune all models in the entire design space. Thus, we adopt the controlled random search [42], which can provide an efficient and effective way to evaluate the impacts of different design dimensions. By training \u223c3,400 models on 9 real-world datasets of different domains, scale, and density, some interesting findings are obtained to help us better understand the influences of the proposed design dimensions. Taking the activation function as an example again, in our evaluation results, Sigmoid tend to perform better than other choices including Identity, i.e., no activation function (see Section 4.4 for more details).\n\nBased on the empirical insights, furthermore, we prune the vanilla design space by narrowing down the choices of design dimensions, leading to a more compact design space consisting of only 96 model instances, more than 1,000 times smaller than the vanilla one. To verify the superiority of the pruned search space compared to the vanilla one, we empirically compare the performance distribution of the sampled models in the 2 design spaces [26]. Then extensive experiments are conducted in different popular settings in terms of recommendation scenarios, e.g., different levels of sparsity, different model complexity, and new datasets, to show that the pruned search space contains a higher concentration of top-performing CF methods. Finally, as a case study, we perform a random search on the pruned search space and compare the best searched model with popular CF models, like MF [16], NCF [11], and LighGCN [10]. The experimental results show that the searched model can obtain the best performance, which further demonstrates the quality of the pruned search space and provide a novel paradigm of model design in recommendation scenarios.\n\nTo summarize, this work makes the following contributions: \u2022 To the best of our knowledge, we make the first attempt to profile the design space of GNN-based CF. It not only deepens the understanding of different dimensions, but also provides a novel paradigm to design GNN methods in recommendation scenarios. \u2022 A unified framework, which can cover extensive popular GNNbased CF models, is proposed. On top of this framework, we develop a design space and evaluate it by extensive experiments.\n\nInteresting findings are obtained to provide insights into model design. \u2022 Guided by the insights, we prune the vanilla design space to obtain a compact one containing a higher concentration of topperforming models. Empirical studies demonstrate the high quality and strong generalization ability of the pruned design space.\n\n\nRELATED WORK 2.1 GNN-based Collaborative Filtering\n\nCollaborative Filtering (CF), learning user preferences by parameterizing users and items as embeddings based on the interactions between them, is the fundamental technique in modern recommender systems [12,17,20,30]. Matrix Factorization (MF) [16,24], one of the most traditional CF techniques, projects the one-hot ID of a user/an item into an embedding vector and then utilizes the dot product of the embedding vectors of users and items to reconstruct the user-item interaction matrix. The recent success of deep learning has motivated a wave of studies focusing on incorporating neural components into CF models [11]. For example, NCF [11] uses Multi-Layer Perceptron (MLP) to replace the conventional dot product as the user-item interaction function. Recent years have witnessed the great success of Graph Neural Networks (GNNs) in CF for learning effective user/item representations [1, 2, 10, 36-38, 41, 43]. For example, NGCF [36] decides the message propagating on both the graph structure and the affinity between the central node and combines the representations of different layers to get the final node representation. LightGCN [10] argues that the non-linearity and weight matrices are useless for CF without side information, and proposes a simple GNN-based CF model. DGCF [37] disentangles the user/item representations into several components to reflect user preference from multiple aspects. More works on GNN-based recommendation can be checked in a latest survey [39].\n\nDespite various GNN-based models developed, they only design a specific model architecture tuned for a specific scenario while our work focuses on a novel paradigm, i.e., profiling the design space, to facilitate the design of top-performing models in diverse recommendation scenarios.\n\n\nEmpirical Evaluation of GNN and CF\n\nRecently, there has been an emerging trend of conducting evaluation studies on GNNs and CF, respectively. Towards GNNs, Shchur et al. [31] investigate the influence of dataset split and compare the aggregated evaluation results on different dataset splits. Lv et al. [22]   al. [44] compare the performance of embedding a network into hyperbolic space with Euclidean space. Previous works in the recommendation domain have also discussed the worrying current situation of reproducibility issue and fair evaluation of CF techniques and several recently proposed complicated models are found to be outperformed by simple and well-optimized baseline algorithms [3,19,28,29,32,48]. However, as a prominent CF technique, empirical evaluations on GNN-based CF have received relatively less scrutiny in existing works. Furthermore, the aforementioned evaluation works focus on comparing the individual model performance, while our approach elevates the study level to design space, i.e., a population of models, which guarantees stronger robustness and generalization. You et al. [42] explores the design space and the task space of GNN and conduct experiments to provide guidelines for better GNN model design. Faced with more domain-specific challenges, e.g., sparsity and diverse user interests, we accordingly customize the evaluation, make a more thorough analysis, and draw more valuable conclusions that enhance research compared to the link prediction task of GraphGym [42]. We further develop a pruned design space containing a higher concentration of top-performing models that can work well in different recommendation settings.\n\n\nDESIGN SPACE OF GNN-BASED CF 3.1 The Unified Framework\n\nAs mentioned in the introduction, the existing GNN-based CF could be divided into 4 key modules, i.e., initialization, GNN, multicomponent, and interaction. In general, as illustrated in Figure 2(a), we propose a unified framework composed of the 4 modules. Taking the representation update procedure on the user side as an example, we elaborate on the framework as follows.\n\n(1) Initialization projects the one-hot user/item ID to dense realvalued embeddings by embedding matrix ID-lookup, i.e., e = ( ( )) , e = ( ( )), where e (resp. e ) is the initial embedding of user (resp. item ). It is a standard manner when there are no extra available features such as user profiles or item attributes.\n\n(2) GNN feeds the initial embeddings of users/items into GNN, refines the embeddings through the propagation of GNN layers, and obtains the representations by combining the output of each GNN layer (including the initial embedding), as follows,  Figure 2(b).\nh (0) = e , h (0) = e ,(1)m ( ) \u2190 = h ( \u22121) , h ( \u22121) ,(2)\n\nCategory\n\nModel \n(\u00b7) (\u00b7) (\u00b7) (\u00b7) (\u00b7) (\u00b7) Single-/Multi-h ( +1) = m ( ) \u2190 , \u2200 \u2208 N ( ) , h ( ) ,(3)h = (h (0) , h (1) , \u00b7 \u00b7 \u00b7 , h ( ) ),(4)\nwhere h ( ) (resp. h ( ) ) denotes the refined representation of user (resp. item ) after GNN layers, (\u00b7) is the message function to encode the message flow from item to user as m ( ) \u2190 , is the activation function, N ( ) is the neighborhood of user , (\u00b7) is the neighborhood information aggregation technique in each layer, (\u00b7) is the layer combination function of the + 1 representations after propagating layers.\n\n(3) Multi-component learns how to better model diverse user interests from different aspects by disentangling user/item representations into multiple components [37,38]. Specifically, independent embedding procedures are performed according to Equation (1)-(4), and representations corresponding to each component are obtained as h ,1 , h ,2 , \u00b7 \u00b7 \u00b7 , h , , which are combined to obtain the final representation, as follows,\nh = (h ,1 , h ,2 , \u00b7 \u00b7 \u00b7 , h , ),(5)\nwhere (\u00b7) is the component combination function of the representations from each component to obtain the final user representation. (4) Interaction performs the user-item matching and predicts the rating value of user-item pair to reflect user preference, as follows,\u02c6=\n(h , h ) ,(6)\nwhere (\u00b7) is the interaction function to predict\u02c6as the rating of the user-item pair ( , ).\n\nDue to the symmetry of the user-item bipartite graph, we can get similar formulations on the item side.\n\n\nThe Proposed Design Space\n\nBased on the unified framework, 9 design dimensions can be extracted, which are split over the 4 modules and marked in red in Figure 2(a), with various possible design choices shown in Figure 2(b).\n\nThe proposed model architectures vary in their design choices. For example, LightGCN [10] and LR-GCCF [2] claim that non-linear activation doesn't benefit CF so remove it from their proposed architectures while it still remains in many other works [36,38,41]. Different combinations of design choices generate different model instantiations with different recommendation performance. Therefore, the specific choices in the design dimensions should be taken into serious consideration during model design. To explore the impacts of different design dimensions, we propose to profile the design space, defined as the Cartesian product of design dimensions [42], which contains a population of model instantiations. Note that our purpose is not to propose the most extensive design space, but to help to understand the influences of different design dimensions of GNN-based CF and gain insights for designing wellperforming models. In fact, the design dimensions and their ranges can be naturally expanded by incorporating more choices.\n\nWe elaborate on some important design dimensions as follows. Others can be naturally understood from the table in Figure 2(b).\n\n\u2022 Message function (\u00b7). In the literature, the common practice is to directly set (\u00b7) = h ( \u22121) [10,37,38,41], but some works [36,43] claim that the interaction between the source and the target node should be encoded into message. Therefore, we also set\n(\u00b7) = h ( \u22121) \u2299 h ( \u22121)\n, where \u2299 denotes the hadamard multiplication of two vectors. The above two design choices are denoted as Identity and Hadamard, respectively. \u2022 Aggregation (\u00b7). For this design dimension, we consider four common and effective GNN methods as its design choices: GCN [15], GAT [34], GIN [40], and GraphSAGE [8]. Particularly, we generalize the design dimension to include the choice of None, which represents not exploiting the graph information and refining the user/item representations through MLP, to enlarge the capacity of the design space to include those non-GNN-based models [11,16]. \u2022 Layer combination (\u00b7). Stack represents directly stacking multiple GNN layers and using the output of the final layer [38,41] to obtain the representation corresponding to each component. Since the outputs of intermediate layers are also found useful in previous works [10,36,37], we investigate three additional layer combination approaches: Concat, Sum, and Mean. \u2022 Component combination (\u00b7). A direct strategy used in existing works, such as DGCF [37], is to get the representations concatenated, denoted as Concat. The attention mechanism can also be used [38], denoted as Att. And we add an additional design choice Mean. \u2022 Interaction function (\u00b7). A simple yet effective choice is calculating the dot product of the user and item representations, denoted as Dot Product. Neural networks can also be used for learning an interaction function [11]. The user and item representations are first concatenated or summed up, and then fed into an MLP for prediction, which are denoted as Concat+MLP and Sum+MLP, respectively. Table 1 shows 10 popular CF methods that can be instantiated from the proposed design space. Specifically, the methods can be categorized into 3 groups: (1) classic methods which mainly refer to MF and its variants; (2) MLP-based methods which incorporate neural networks for CF; (3) GNN-based methods which enhance CF by GNNs. We next choose 3 representative models from each category and briefly explain how they can be instantiated by the design space.\n\n\nRelationship with Existing CF Methods\n\n\u2022 MF [16] is the most common CF method which uses the dot product of the user and item representations to reconstruct the user-item interaction matrix. It can be naturally instantiated by setting the aggregation as None and adopting Dot Product as the interaction function. \u2022 NCF [11] proposes to learn interaction function with neural networks. It can be included by choosing Concat+MLP as the interaction function. \u2022 LightGCN [10] is a state-of-the-art GNN-based CF method, which can be considered as choosing GCN as the aggregation technique, Identity function as the activation, i.e., removing the nonlinear computations, and Mean as the layer combination function.\n\nWe can see that the framework unifies the key design dimensions in popular CF models and the proposed design space is comprehensive enough to include a broad spectrum of model instantiations.\n\nIn the next section, we then perform an evaluation of the design space to understand the impacts of different design dimensions.\n\n\nEVALUATION OF THE DESIGN SPACE 4.1 Datasets\n\nAs discussed in the introduction, the recommendation scenarios in real-world applications are diverse. To make the experimental findings more robust, the evaluation is conducted on 9 real-world datasets, which are diverse in scale (large or small), density (dense or sparse), and the collected domain. The statistics of the datasets are shown in Table 2, whose detailed descriptions and preprocessing strategies can be found in Appendix A.   \n\n\nEvaluation Technique\n\nWith over 100,000 model architectures in the design space, conducting full grid search to evaluate each design dimension is too costly.\n\nTo overcome this issue, we adopt controlled random search [42] as the design space evaluation strategy.\n\nTo make the evaluation distributed across different datasets, we first define the Cartesian product of design dimensions and datasets as the configuration space, and the controlled random search is performed in the configuration space to draw experimental configurations. As illustrated in Figure 2(c), suppose that we want to evaluate message function (\u00b7), we first draw experimental configurations by random searching the configuration space, all with (\u00b7) = Identity. Then, by setting (\u00b7) = Hadamard, while controlling all the other dimensions, we draw another configurations. Now, we obtain groups, all with 2 configurations that only differ from each other in (\u00b7). Within each group, the 2 design choices of (\u00b7) \u2208 {Identity, Hadamard} are ranked by performance, where a tie is given if the performance difference is less than 0.0001. The average rankings of different choices over all the groups are shown via bar plot as illustrated in Figure 2(d). In our experiment, we set = 100 to cut the number of experiments from 103,680 to 3,400, by over 30 times.\n\n\nEvaluation Setup\n\n\nLoss Function and Evaluation Metric.\n\nSince our task is predicting ratings for user-item pairs, we adopt the widely-used Mean Square Error (MSE) loss function, 2 which is formulated as follows:\n= ( , ) \u2208 O (\u02c6\u2212 ) 2 |O | + \u2225\u0398\u2225 2 ,(7)\nwhere\u02c6and are the predicted rating and the ground truth, respectively, O is the set of observed ratings for training, is the hyperparameter controlling the 2 regularization weight and \u0398 denotes the model parameters.\n\nAs for the evaluation metric, we use the common Rooted Mean Square Error (RMSE) for the rating prediction task [16], which is calculated by =  Table 3 and the evaluation result is shown in Figure 3. The rest of the hyperparameters are set as common values in practice. We refer the readers to Appendix B.1 for more detailed hyperparameter settings and C.1 for other implementation details.\n\n\nEvaluation Results\n\nResults are shown in Figure 4 with 9 bar plots, each depicting the averaged rankings of different choices in each design dimension (we further show the violin plots to decipt the ranking distribution in Appendix D). The experiment quantitatively evaluates the impacts of different design dimensions of GNN-based CF on recommendation performance across a wide range of recommendation scenarios. Rather than searching for the single best model out of all these configurations, we explore whether there are findings that can enrich the understanding of the design dimensions and help to efficiently design top-performing GNN-based CF models in different recommendation scenarios. Some key experimental findings are enumerated as below:\n\n\u2022 GAT and GraphSAGE slightly outperform the other alternatives.\n\nInterestingly, None is comparable with GNN-based aggregators, 2 In this work, we mainly study the rating prediction task under MSE loss and leave the study of the influence of different types of loss function as future work.\n\nindicating that simply using MF [16] or MLP-based CF methods [11] can achieve competitive or even better performance in some scenarios. The interesting finding reveals that incorporating graph information can not always enhance CF. \u2022 Sigmoid clearly stands out among all the 6 activations. This finding differs from that in [10], which finds out that the nonlinear activation can not benefit CF. A possible explanation is that the tasks studied in the two works are different: item ranking in [10] while rating prediction in ours, where non-linear activation can help to increase the expressive power of the neural networks for such a regression task. \u2022 When taking multi-component into consideration, it is more favorable to set component number as 4, which aligns with the finding of previous work [37] that the user interests are diverse in different aspects. And it is preferable to combine representations of different components with Att mechanism. \u2022 Adopting neural interaction function is superior to adopting Dot Product, which is not consistent with the finding in [28] that Dot Product is a better choice to MLP for predicting the ratings for user-item pairs. We suppose it may be caused by the different settings of the two works. In [28], the evaluation is performed on specific model architectures with 2 datasets, while in our setting, we evaluate thousands of model architectures on 9 different datasets, and thus, different conclusions are drawn.\n\nThe above findings not only enrich our understanding of the impacts of different design dimensions but further provide valuable insights for effectively designing top-performing models. Specifically, we can observe that there exists some redundancy in the design space. For example, the initial embedding dimension can be fixed as 64 since it significantly outperforms the other 2 alternatives. It motivates us that the vanilla design space can be further pruned to improve its quality, which will boost the searching efficiency of top-performing models.\n\n\nEVALUATION OF PRUNED DESIGN SPACE\n\nFollowing the insights provided in Section 4.4, we prune the vanilla design space by narrowing down the choices of design dimensions. The motivation is that we only remain those favorable design choices, which are empirically more likely to generate wellperforming models. Thus, the pruned design space contains a higher concentration of top-performing models which will facilitate model searching. Table 4 introduces the choices in the design dimensions of the pruned design space. We briefly explain the pruning in 3 dimensions, and for the other dimensions, preferable design choices are remaining, which can be naturally understood from Figure 4. For aggregation, GraphSAGE remains as the representative of graph information aggregator for its smaller training consumption than GAT, and None also remains to make the pruned design space have a capacity of non-GNN-based models. As for the activation, two preferable non-linear functions and Identity remain for better containment of linear and non-linear models. The optimal component number 4 remains, and we also keep the choice of 1 for investigating single-component models. Concat+MLP, Sum+MLP\n\n\nThe Pruned Design Space\n\nAfter pruning, there are only 96 candidate models in the design space, compared to 103,680 in the vanilla one. The scale of the design space is reduced by three orders of magnitude (1,080x). Compared to the vanilla one, the superiority of the pruned design space lies in that it simplifies the combinations of design dimensions by ruling out the sub-optimal choices, and thus contains a higher concentration of top-performing models which work well across scenarios for efficient searching. The pruned design space consistently shows high quality in different recommendation settings, which indicates its strong generalization ability. In the remainder of this section, we then conduct a series of evaluation studies to verify the superiority of the pruned design space.\n\n\nEvaluation\n\n\nEvaluation Technique.\n\nWe use the RMSE empirical distribution function (EDF) [26] to quantify the quality of a design space by characterizing the RMSE distribution generated by sampling and training models from that design space. We empirically observe that sampling = 100 samples is sufficient. For more discussions on EDF and the detailed selection process for , please refer to Appendix E.\n\n\nGeneralization Evaluation.\n\nTo show that the pruned design space can facilitate model design across different scenarios, we conduct the following empirical studies to evaluate its generalization. Specifically, we compare the EDFs in 3 different popular settings in terms of recommendation scenarios. The experimental results demonstrate that the quality of the pruned design space consistently outperforms that of the vanilla one.\n\nDifferent levels of density. Interaction density is an important property to characterize a recommendation dataset and it diversifies in different recommendation scenarios. As shown in Table 2, datasets can be divided into 3 groups according to the order of magnitude of their interaction density. We select Yelp, Amazon-Beauty, and MovieLens-1M as the representatives of each group, and the EDF comparison results on the 3 datasets are shown in Figure 5.\n\nUnder all the 3 levels of density, the performance distribution of the pruned design space concentrates within a compact and highvalued range. Specifically, on Yelp and Amazon-Beauty, the pruned design space contains the models with the best performance in the vanilla design space; and in MovieLens-1M, although the models  of the pruned design space may be outperformed by about 10% of those in the vanilla one, we still significantly narrow down the performance range of the models to a high-valued area, making it more efficient to find models with satisfactory performance.\n\n\nDifferent model complexity.\n\nRecently, there is a growing interest in deploying customized neural architectures on diverse hardware devices with different computational resource constraints [33]. Therefore, the requirements of model complexity vary across different platforms. By regarding the number of trainable parameters as the indicator of model complexity, we compare the quality of the vanilla and the pruned design space under the constraints of low and high model complexity, respectively. Specifically, we use a model with = 64, = 1, = 1, (\u00b7) = None as reference to set the low complexity constraint; and the other with = 256, = 4, = 4, (\u00b7) = GraphSAGE to set the high complexity constraint (both with 64 hidden dimensions). The number of hidden dimensions of the sampled architecture is adjusted to match the model complexity constraints. We choose Yelp as the experimental dataset and the comparison results are shown in Figure 6.\n\nWe can observe that model performance in the pruned design space distributes in a small but top-valued area under both levels of model complexity, demonstrating its superior performance compared to the vanilla design space.\n\nGeneralization to new datasets. To further show the generalization ability to new recommendation scenarios, we sample and train the models from each design space on 2 new datasets: Epinions and Amazon-Sports, whose statistics are shown in Table 5. The EDFs of the 2 design spaces on the new datasets are compared, respectively, in Figure 7.\n\nAgain, we can observe that the EDF of the pruned design space is improved substantially on both of the new datasets, which demonstrates the stronger robustness and generalization of the pruned design space given new recommendation scenarios.    Table 4 are listed, and the choices in the remaining design dimensions remain the same as those in Table 4. To summarize, in all the above 3 settings, the pruned design space consistently holds better quality than that of the vanilla design space without signs of overfitting, which demonstrates its strong generalization to various new settings.\nDataset (\u00b7) (\u00b7) (\u00b7) (\u00b7)\n\nCase Study: Random Search\n\nA case study is conducted to further verify if the pruned design space with a higher concentration of top-performing models can enhance model searching in new scenarios. Since our focus is the design space, the searching can be performed with exiting graph neural architecture search algorithm [4,7,35,45,47], for which we leave this as future work. Here, we simply adopt random search. Specifically, we randomly search and train = 10 models in the pruned search space on the 2 new datasets, i.e., Epinions and Amazon-Sports, and adopt the one with the best test performance as our final choice. Table 6 shows the best searched model architectures.\n\nWe then compare the best searched model with popular CF baselines. The goal of this section is not to pursue state-of-the-art performance on the 2 datasets, but to evaluate the quality of the pruned search space, i.e., higher concentration of top-performing models, and how it benefits model design in new recommendation scenarios simply with a random search strategy. Thus, we only include 3 popular GNN-based CF methods as well as MF and NCF as comparisons and do not heavily tune the baselines as well as our searched models. We run all the methods for 10 times with different random seeds, and report the average and the standard deviation of the RMSE performance results in Table 7. The detailed hyperparameter settings and more implementation details can be found in Appendix B.2 and C.2, respectively.\n\nWe can observe that the randomly searched models can outperform all the baselines on the 2 datasets, which shows that the Table 7: Performance comparisons with baselines on Epinions and Amazon-Sports. RS-10 denotes the randomly searched model under 10 samples. Lower is better. The best results are bold and the second-best are underlined.\n\n\nModel\n\nEpinions Amazon-Sports MF [16] 0.9945 \u00b1 0.0000 0.9882 \u00b1 0.0007 NCF [11] 1.0070 \u00b1 0.0055 0.9342 \u00b1 0.0008 NGCF [36] 1.1437 \u00b1 0.0240 1.0668 \u00b1 0.0038 LightGCN [10] 0.9926 \u00b1 0.0001 0.9705 \u00b1 0.0003 DGCF [37] 1.6800 \u00b1 0.2272 0.9894 \u00b1 0.0000 RS-10 0.8729 \u00b1 0.0014 0.9327 \u00b1 0.0006 pruned design space with high quality can help to efficiently design top-performing models for new recommendation scenarios. Besides, we note that there is not a single baseline model which can consistently beat its competitors, which shows that designing top-performing models for diverse new scenarios is not a trivial task and emphasizes the significance of exploring design space for higher model design efficiency.\n\nThe insights obtained from design space profiling in Section 4.4 guide us to perform design space pruning for a higher concentration of top-performing models. Compared to the common practice that designing an individual model for a specific setting, the novel paradigm of profiling the design space focuses on a population of models, and thus stronger robustness and generalization are guaranteed. The evaluation results demonstrate its strong generalization ability and the case study further shows its effectiveness in quickly finding well-performing models. Therefore, we conclude that the novel paradigm of exploring design space paves the way for efficiently designing top-performing GNN methods in different recommendation scenarios.\n\n\nCONCLUSION\n\nIn this work, we propose to profile the design space of GNN-based CF methods, which have been widely researched in recent years. By covering existing GNN-based CF methods in a unified framework, a novel design space is developed and a controlled random search is adopted to efficiently and effectively evaluate the influences of different dimensions on recommendation performance. Furthermore, based on the empirical findings, design space pruning is performed by ruling out some design choices which are shown to be sub-optimal in the evaluation. Then empirical studies in different settings demonstrate the high quality and the strong generalization ability of the pruned design space. Finally, as a case study, we show that it can quickly obtain top-performing model architectures on 2 new datasets simply by random searching the pruned design space, compared to popular CF methods. In the future, we will further explore how the proposed design space can benefit more complex recommendation scenarios, e.g., social recommendation [6].\n\n\nACKNOWLEDGMENTS\n\nThis work is supported by the National Natural Science Foundation of China (No. U20B2045, 61772082, 61702296, 62002029). We thank all anonymous reviewers for their constructive comments.\n\n\nA DATASETS\n\n\u2022 Yelp is a business recommendation dataset. 10-core filtering is applied to ensure that each user/item has at least 10 interactions. \u2022 Amazon is a large e-commerce dataset introduced in [9], and we take the subsets from 3 categories to form Amazon-CDs, Amazon-Movies and Amazon-Beauty and apply 10-core filtering. \u2022 MovieLens is a widely-used movie rating dataset for evaluating recommender systems, and we choose both the 100K and 1M versions in our experiment. \u2022 YahooMusic [5] is a music rating dataset, and Flixster [13], and Douban [23] are both movie rating datasets. For these 3 datasets, we use the preprocessed subsets provided by [25].\n\nFor MovieLens-100K/1M, YahooMusic, Flixster, and Douban, we follow the customized split provided together with the dataset; and for the rest of the datasets, we randomly split them into 0.8/0.1/0.1 train/validation/test sets. Testing performance in the best epoch of validation set is reported.\n\nThe 9 datasets contain ratings with different scales. For a more fair comparison and fast convergence of models, we adopt standardization to the initial ratings using \u2032 = \u2212 , where and are the mean and standard variation of the rating value in the training set, respectively.\n\n\nB HYPERPARAMETERS B.1 Design Dimension Evaluation\n\nIn hyperparameter evaluation, = 30 groups of experimental configurations are sampled for each of the 3 selected hyperparameters. The values of these hyperparameters remain the same as the optimal choices in the evaluation result shown in Figure 3, i.e., dropout is fixed as 0, the number of training epochs is fixed as 200, and 2 regularization weight is fixed as 0.0005. The hidden dimension of GNN layer is set as 64. All models are optimized using Adam [14] optimizer. The base learning rate is set as 0.01 and is scheduled using the cosine annealing method of SGDR [21].\n\n\nB.2 Case Study\n\nFor our randomly searched models, we perform rough hyperparameter tuning; and for the baselines, their hyperparameters are initialized according to the suggestions in the original papers and also roughly tuned by several steps.\n\nFor brevity, we will denote some variables. Suppose hidden dimension of (GNN) layer as \u210e , number of (GNN) layers as , 2 regularization weight as , learning rate as . For training epochs of baselines, we use early stop mechanism based on the evaluation on validation set.  B.2.6 RS-10. We set the number of training epochs as 160 for Epinions, and = 5 \u00d7 10 \u22123 for Amazon-Sports. Other hyperparameters remain the same as our former setting.\n\n\nC IMPLEMENTATION DETAILS C.1 Design Dimension Evaluation\n\nWe utilize Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz and GeForce RTX 3090 as the experimental environment. A small number of experimental configurations are skipped because of OOM on a single GPU. Our customized evaluation is performed on the GraphGym platform [42], which supports parallel experiment launch. To make the results convincing as reliable, the averaged results across 3 runs with different random seeds are reported.\n\n\nC.2 Case Study\n\nThe baseline methods are evaluated using the RecBole framework [48]. For fair comparisons, all the methods are optimized with the MSE loss calculated as Equation (7).\n\n\nD ADDITIONAL RESULTS\n\nIn this section, we provide additional experimental results of design space profiling. The violin plot indicates the smoothed distribution of the ranking of each design choice aggregated over the sampled experimental configurations (Figure 8). It provides us with information on how the ranking results distribute across different values. \n\n\nE DISCUSSION ON EDF\n\nSuppose 1 as the indicator function, as the number of sampled models, each with RMSE . The RMSE EDF is given by:\n( ) = 1 =1 1 [ < ] .(8)\n( ) gives the fraction of models with RMSE less than . The key intuition behind using EDF as the indicator of a design space is that comparing distributions can help to obtain more robust and informative conclusions than comparing the best found models from the two design spaces [27].\n\nTo explore the appropriate value for , we plot the EDF curves of the vanilla design space on the Amazon-Sports dataset, whose statistics are introduced in Table 5, with the sample number ranging from 20 to 1000 in Figure 9. Since only the performance distribution on the same dataset is comparable, the dataset is fixed when comparing EDF curves in the same illustration. We can observe that the resulting EDF curves are close to each other after reaching 100, which suggests that 100 samples may be sufficient to give a reasonable estimate of the accurate EDF. Therefore, for the vanilla design space, we will show the EDFs for = 100 sampled models; and for the pruned design space, we set = 96 since it is the total number of the models it contains.\n\nFigure 2 :\n2Overview of the design space and the evaluation strategy. (a) An illustration of the unified framework of GNNbased CF suppose that the rating of the same user-item pair ( 2 , 4 ) as inFigure 1is to be predicted. It contains 4 modules: initialization, GNN, multi-component, and interaction. On top of the framework, we propose a GNN-based CF design space consisting of 9 design dimensions, which are marked in red. (b) Popular choices in the 9 design dimensions are listed. (c) An example of performing controlled random search to investigate the influence of message function (\u00b7). In each group of configurations, the design choices of (\u00b7) \u2208 {Identity, Hadamard} are ranked by performance. (d) The average of the rankings corresponding to each design choice is shown for analysis. Lower is better.\n\nFigure 3 :\n3Ranking analysis of the hyperparameter dimensions. Lower is better. The values of these hyperparameters are fixed as the optimal design choices.\n\nFigure 4 :\n4Ranking analysis of the 9 design dimensions. Lower is better.\n\n(\n, ) \u2208O (\u02c6\u2212 ) 2 | O | , where O is the set of observed ratings for testing.4.3.2 Hyperparameters. The hyperparameters of the sampled models should be the same to ensure a fair comparison. The values of 3 hyperparameters, i.e., dropout, training epochs, and 2 regularization weight are set according to the evaluation result of controlled random search on the validation set. Their available choices are listed in\n\nFigure 5 :\n5Comparison results of design space generalization to different levels of density. Yelp (left), Amazon-Beauty (middle), and MovieLens-1M (right).\n\nFigure 6 :\n6Comparison results of design space generalization to different model complexity on Yelp. Low (left) and high model complexity (right).\n\nFigure 7 :\n7Comparison results of design space generalization to new datasets. Epinions (left) and Amazon-Sports (right).\n\n\nWe set \u210e = 64, = 10 \u22122 and = 10 \u22123 for both datasets.B.2.2 NCF. We set \u210e = 64, = 10 \u22122 and = 10 \u22123 for both datasets. B.2.3 NGCF. We set \u210e = 64, = 3, = 10 \u22122 and = 10 \u22123 for both datasets. B.2.4 LightGCN. We set \u210e = 64, = 3, = 10 \u22125 and = 10 \u22123 for both datasets.\n\n\nWe set \u210e = 64, = 1, the number of latent factors as 4, the number of iterations as 2, = 10 \u22123 and = 10 \u22123 for both datasets.\n\nFigure 8 :\n8The ranking distribution analysis.\n\nFigure 9 :\n9EDF curves w.r.t. varying sample numbers on Amazon-Sports.\n\n\npoint out the issues in heterogeneous GNNs comparison and propose a heterogeneous graph benchmark.Zhang et   (a) The Unified Framework \n\n(b) Choices in the Design Dimensions \n\nDesign Dimension \nChoices \n\nInitial Embedding Dimension \n64, 128, 256 \nMessage Function (\u00b7) \nIdentity, Hadamard \nAggregation (\u00b7) \nNone, GCN, GAT, GIN, GraphSAGE \nActivation (\u00b7) \nIdentity, Sigmoid, Tanh, ReLU, PReLU, LeakyReLU \nLayer Number \n1, 2, 3, 4 \nLayer Combination (\u00b7) \nStack, Concat, Sum, Mean \nComponent Number \n1, 2, 3, 4 \nComponent Combination (\u00b7) \nConcat, Mean, Att \nInteraction Function (\u00b7) \nDot Product, Concat+MLP, Sum+MLP \n\n(c) Controlled Random Search \n(d) Ranking Analysis of ( \u00b7) \n\n\n\nTable 1 :\n1A summary of the popular CF methods that can be instantiated from the design space. Initial embedding dimension , layer number , and component number are omitted since they do not affect the overall model architecture and the last column indicates if multi-component is taken into consideration during model design. The notations are introduced in\n\n\ncomponentClassic \nMF [16, 24] \nIdentity \nNone \nIdentity \nStack \n-\nDot Product \nSingle \nLLORMA [18, 46] Identity \nNone \nIdentity \nStack \nAtt \nDot Product \nMultiple \n\nMLP-based \nNCF [11] \nIdentity \nNone \nReLU \nStack \n-\nConcat+MLP \nSingle \n\nGNN-based \n\nNGCF [36] \nHadamard \nGCN \nLeakyReLU Concat \n-\nDot Product \nSingle \nLightGCN [10] \nIdentity \nGCN \nIdentity \nMean \n-\nDot Product \nSingle \nLR-GCCF [2] \nIdentity \nGCN \nIdentity Concat \n-\nDot Product \nSingle \nSMOG-CF [43] \nHadamard \nGCN \nReLU \nConcat \n-\nDot Product \nSingle \nPinSage [41] \nIdentity GraphSAGE \nReLU \nStack \n-\nDot Product \nSingle \nMCCF [38] \nIdentity \nGAT \nReLU \nStack \nAtt \nConcat+MLP \nMultiple \nDGCF [37] \nIdentity \nGCN \nTanh \nSum \nConcat Dot Product \nMultiple \n\n\n\nTable 2 :\n2Statistics of the datasets.Dataset \n# of Users # of Items # of Interactions Rating Scale Density \n\nYelp 1 \n58,069 \n31,721 \n1,160,605 \n[1,5] \n0.063% \nAmazon-CDs [9] \n31,296 \n24,379 \n622,163 \n[1,5] \n0.082% \nAmazon-Movies [9] \n44,439 \n25,047 \n1,070,860 \n[1,5] \n0.096% \n\nYahooMusic [5, 25] \n1,357 \n1,363 \n5,335 \n[1,100] \n0.28% \nAmazon-Beauty [9] \n7,068 \n3,570 \n79,506 \n[1,5] \n0.32% \nFlixster[13, 25] \n2,341 \n2,956 \n26,173 \n[0.5,5] \n0.38% \n\nDouban [23, 25] \n2,999 \n3,000 \n136,891 \n[1,5] \n1.52% \nMovieLens-1M 2 \n6,040 \n3,706 \n1,000,209 \n[1,5] \n4.47% \nMovieLens-100K 3 \n943 \n1,682 \n100,000 \n[1,5] \n6.31% \n\n1 https://www.yelp.com/dataset/ \n2 https://grouplens.org/datasets/movielens/100k/ \n3 https://grouplens.org/datasets/movielens/1m/ \n\n\n\nTable 3 :\n3Choices of the hyperparameter dimensions.Hyperparameters \nChoices \n\nDropout \n0, 0.5 \nTraining Epochs \n120, 160, 200 \n2 Regularization Weight \n0.0005, 0.005, 0.05 \n\n\n\nTable 4 :\n4Choices in the design dimensions of the pruned design space.Design Dimension \nChoices \n\nInitial Embedding Dimension \n64 \nMessage Function (\u00b7) \nIdentity, Hadamard \nAggregation (\u00b7) \nNone, GraphSAGE \nActivation (\u00b7) \nIdentity, Sigmoid, ReLU \nLayer Number \n1, 2 \nLayer Combination (\u00b7) \nMean \nComponent Number \n1, 4 \nComponent Combination (\u00b7) \nAtt \nInteraction Function \u210e(\u00b7) \n\n\nTable 5 :\n5Statistics of the 2 new datasets.Dataset \n# of Users # of Items # of Interactions Rating Scale Density \n\nEpinions 1 \n40,163 \n139,738 \n664,824 \n[1,5] \n0.012% \nAmazon-Sports [9] \n11,435 \n5,405 \n108,004 \n[1,5] \n0.17% \n\n1 http://www.trustlet.org/downloaded_epinions.html \n\n\n\nTable 6 :\n6The best searched model architectures with ran-\ndom sample number m=10 on Epinions and Amazon-Sports. \nOnly the variable design dimensions in \n\nRianne Van Den, Thomas N Berg, Max Kipf, Welling, arXiv:1706.02263Graph convolutional matrix completion. arXiv preprintRianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu- tional matrix completion. arXiv preprint arXiv:1706.02263 (2017).\n\nRevisiting graph based collaborative filtering: A linear residual graph convolutional network approach. Lei Chen, Le Wu, Richang Hong, Kun Zhang, Meng Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceLei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach. In Proceedings of the AAAI Conference on Artificial Intelligence. 27-34.\n\nAre we really making much progress? A worrying analysis of recent neural recommendation approaches. Paolo Maurizio Ferrari Dacrema, Dietmar Cremonesi, Jannach, Proceedings of the Conference on Recommender Systems. the Conference on Recommender SystemsMaurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are we really making much progress? A worrying analysis of recent neural recom- mendation approaches. In Proceedings of the Conference on Recommender Systems. 101-109.\n\nDiffmg: Differentiable meta graph search for heterogeneous graph neural networks. Yuhui Ding, Quanming Yao, Huan Zhao, Tong Zhang, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYuhui Ding, Quanming Yao, Huan Zhao, and Tong Zhang. 2021. Diffmg: Differen- tiable meta graph search for heterogeneous graph neural networks. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 279-288.\n\nThe yahoo! music dataset and kdd-cup'11. Gideon Dror, Noam Koenigstein, Yehuda Koren, Markus Weimer, Proceedings of KDD Cup. KDD CupGideon Dror, Noam Koenigstein, Yehuda Koren, and Markus Weimer. 2012. The yahoo! music dataset and kdd-cup'11. In Proceedings of KDD Cup 2011. 3-18.\n\nGraph neural networks for social recommendation. Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, Dawei Yin, Proceedings of the International Conference on World Wide Web. the International Conference on World Wide WebWenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In Proceedings of the International Conference on World Wide Web. 417-426.\n\nGraphnas: Graph neural architecture search with reinforcement learning. Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, Yue Hu, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial IntelligenceYang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. 2020. Graphnas: Graph neural architecture search with reinforcement learning. In Proceedings of the International Joint Conference on Artificial Intelligence. 1403-1409.\n\nInductive representation learning on large graphs. Rex William L Hamilton, Jure Ying, Leskovec, Advances in Neural Information Processing Systems. William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems. 1025-1035.\n\nUps and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. Ruining He, Julian Mcauley, Proceedings of the International Conference on World Wide Web. the International Conference on World Wide WebRuining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In Proceedings of the International Conference on World Wide Web. 507-517.\n\nLightgcn: Simplifying and powering graph convolution network for recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. the International ACM SIGIR Conference on Research and Development in Information RetrievalXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. 639-648.\n\nNeural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the International Conference on World Wide Web. the International Conference on World Wide WebXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the International Conference on World Wide Web. 173-182.\n\nAn algorithmic framework for performing collaborative filtering. L Jonathan, Joseph A Herlocker, Al Konstan, John Borchers, Riedl, Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. the International ACM SIGIR Conference on Research and Development in Information RetrievalJonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. 1999. An algorithmic framework for performing collaborative filtering. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. 230-237.\n\nA matrix factorization technique with trust propagation for recommendation in social networks. Mohsen Jamali, Martin Ester, Proceedings of the Conference on Recommender systems. the Conference on Recommender systemsMohsen Jamali and Martin Ester. 2010. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the Conference on Recommender systems. 135-142.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti- mization. In Proceedings of International Conference on Learning Representations.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsThomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In Proceedings of International Conference on Learning Representations.\n\nFactorization meets the neighborhood: a multifaceted collaborative filtering model. Yehuda Koren, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 426-434.\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech- niques for recommender systems. Computer 42, 8 (2009), 30-37.\n\nLocal low-rank matrix approximation. Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningJoonseok Lee, Seungyeon Kim, Guy Lebanon, and Yoram Singer. 2013. Local low-rank matrix approximation. In Proceedings of International Conference on Machine Learning. 82-90.\n\nJoonseok Lee, Mingxuan Sun, Guy Lebanon, arXiv:1205.3193A comparative study of collaborative filtering algorithms. arXiv preprintJoonseok Lee, Mingxuan Sun, and Guy Lebanon. 2012. A comparative study of collaborative filtering algorithms. arXiv preprint arXiv:1205.3193 (2012).\n\nAmazon. com recommendations: Item-to-item collaborative filtering. Greg Linden, Brent Smith, Jeremy York, IEEE Internet computing. 7Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon. com recommenda- tions: Item-to-item collaborative filtering. IEEE Internet computing 7, 1 (2003), 76-80.\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsIlya Loshchilov and Frank Hutter. 2017. Sgdr: Stochastic gradient descent with warm restarts. In Proceedings of International Conference on Learning Representa- tions.\n\nAre we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks. Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, Jie Tang, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningQingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, and Jie Tang. 2021. Are we really mak- ing much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\n\nRecommender systems with social regularization. Hao Ma, Dengyong Zhou, Chao Liu, Irwin Michael R Lyu, King, Proceedings of the International Conference on Web Search and Data Mining. the International Conference on Web Search and Data MiningHao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. 2011. Rec- ommender systems with social regularization. In Proceedings of the International Conference on Web Search and Data Mining. 287-296.\n\nProbabilistic matrix factorization. Andriy Mnih, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. Andriy Mnih and Russ R Salakhutdinov. 2007. Probabilistic matrix factorization. In Advances in Neural Information Processing Systems. 1257-1264.\n\nGeometric matrix completion with recurrent multi-graph neural networks. Federico Monti, Xavier Michael M Bronstein, Bresson, Advances in Neural Information Processing Systems. Federico Monti, Michael M Bronstein, and Xavier Bresson. 2017. Geometric matrix completion with recurrent multi-graph neural networks. In Advances in Neural Information Processing Systems. 3697-3707.\n\nOn network design spaces for visual recognition. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, Piotr Doll\u00e1r, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionIlija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll\u00e1r. 2019. On network design spaces for visual recognition. In Proceedings of the International Conference on Computer Vision. 1882-1890.\n\nKaiming He, and Piotr Doll\u00e1r. 2020. Designing network design spaces. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Proceedings of the Conference on Computer Vision and Pattern Recognition. the Conference on Computer Vision and Pattern RecognitionIlija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. 2020. Designing network design spaces. In Proceedings of the Conference on Computer Vision and Pattern Recognition. 10428-10436.\n\nNeural collaborative filtering vs. matrix factorization revisited. Walid Steffen Rendle, Li Krichene, John Zhang, Anderson, Proceedings of the Conference on Recommender Systems. the Conference on Recommender SystemsSteffen Rendle, Walid Krichene, Li Zhang, and John Anderson. 2020. Neural collaborative filtering vs. matrix factorization revisited. In Proceedings of the Conference on Recommender Systems. 240-248.\n\nSteffen Rendle, Li Zhang, Yehuda Koren, arXiv:1905.01395On the difficulty of evaluating baselines: A study on recommender systems. arXiv preprintSteffen Rendle, Li Zhang, and Yehuda Koren. 2019. On the difficulty of evaluating baselines: A study on recommender systems. arXiv preprint arXiv:1905.01395 (2019).\n\nItembased collaborative filtering recommendation algorithms. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Proceedings of the International Conference on World Wide Web. the International Conference on World Wide WebBadrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item- based collaborative filtering recommendation algorithms. In Proceedings of the International Conference on World Wide Web. 285-295.\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan G\u00fcnnemann, arXiv:1811.05868Pitfalls of graph neural network evaluation. arXiv preprintOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868 (2018).\n\nAre we evaluating rigorously? benchmarking recommendation for reproducible evaluation and fair comparison. Zhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, Cong Geng, Proceedings of the Conference on Recommender Systems. the Conference on Recommender SystemsZhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, and Cong Geng. 2020. Are we evaluating rigorously? benchmarking recommendation for reproducible evaluation and fair comparison. In Proceedings of the Conference on Recommender Systems. 23-32.\n\nMnasnet: Platform-aware neural architecture search for mobile. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, Proceedings of the Conference on Computer Vision and Pattern Recognition. the Conference on Computer Vision and Pattern RecognitionMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the Conference on Computer Vision and Pattern Recognition. 2820-2828.\n\nGraph attention networks. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of International Conference on Learning Representations.\n\nExplainable automated graph representation learning with hyperparameter importance. Xin Wang, Shuyi Fan, Kun Kuang, Wenwu Zhu, Proceedings of International Conference on Machine Learning. International Conference on Machine LearningXin Wang, Shuyi Fan, Kun Kuang, and Wenwu Zhu. 2021. Explainable automated graph representation learning with hyperparameter importance. In Proceedings of International Conference on Machine Learning. 10727-10737.\n\nNeural graph collaborative filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. the International ACM SIGIR Conference on Research and Development in Information RetrievalXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. 165-174.\n\nDisentangled Graph Collaborative Filtering. Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua, Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. the International ACM SIGIR Conference on Research and Development in Information RetrievalXiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua. 2020. Disentangled Graph Collaborative Filtering. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. 1001-1010.\n\nMulticomponent graph convolutional collaborative filtering. Xiao Wang, Ruijia Wang, Chuan Shi, Guojie Song, Qingyong Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceXiao Wang, Ruijia Wang, Chuan Shi, Guojie Song, and Qingyong Li. 2020. Multi- component graph convolutional collaborative filtering. In Proceedings of the AAAI Conference on Artificial Intelligence. 6267-6274.\n\nShiwen Wu, Fei Sun, Wentao Zhang, Bin Cui, arXiv:2011.02260Graph neural networks in recommender systems: a survey. arXiv preprintShiwen Wu, Fei Sun, Wentao Zhang, and Bin Cui. 2020. Graph neural networks in recommender systems: a survey. arXiv preprint arXiv:2011.02260 (2020).\n\nHow powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks?. In Proceedings of International Conference on Learning Representations.\n\nGraph convolutional neural networks for web-scale recommender systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, L William, Jure Hamilton, Leskovec, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale rec- ommender systems. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 974-983.\n\nDesign Space for Graph Neural Networks. Jiaxuan You, Rex Ying, Jure Leskovec, Advances in Neural Information Processing Systems. Jiaxuan You, Rex Ying, and Jure Leskovec. 2020. Design Space for Graph Neural Networks. In Advances in Neural Information Processing Systems. 17009-17021.\n\nStacked Mixed-Order Graph Convolutional Networks for Collaborative Filtering. Hengrui Zhang, Julian Mcauley, Proceedings of the International Conference on Data Mining. the International Conference on Data MiningHengrui Zhang and Julian McAuley. 2020. Stacked Mixed-Order Graph Convolu- tional Networks for Collaborative Filtering. In Proceedings of the International Conference on Data Mining. 73-81.\n\nWhere are we in embedding spaces? A Comprehensive Analysis on Network Embedding Approaches for Recommender Systems. Sixiao Zhang, Hongxu Chen, Xiao Ming, Lizhen Cui, Hongzhi Yin, Guandong Xu, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSixiao Zhang, Hongxu Chen, Xiao Ming, Lizhen Cui, Hongzhi Yin, and Guandong Xu. 2021. Where are we in embedding spaces? A Comprehensive Analysis on Network Embedding Approaches for Recommender Systems. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\n\nAutomated Machine Learning on Graphs: A Survey. Ziwei Zhang, Xin Wang, Wenwu Zhu, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial IntelligenceZiwei Zhang, Xin Wang, and Wenwu Zhu. 2021. Automated Machine Learning on Graphs: A Survey. Proceedings of the International Joint Conference on Artificial Intelligence, 4704-4712.\n\nCollaborative filtering with social local models. Huan Zhao, Quanming Yao, T James, Dik Lun Kwok, Lee, Proceedings of the International Conference on Data Mining. the International Conference on Data MiningHuan Zhao, Quanming Yao, James T Kwok, and Dik Lun Lee. 2017. Collaborative filtering with social local models. In Proceedings of the International Conference on Data Mining. 645-654.\n\nSearch to aggregate neighborhood for graph neural network. Huan Zhao, Quanming Yao, Weiwei Tu, Proceedings of the International Conference on Data Engineering. the International Conference on Data EngineeringHuan Zhao, Quanming Yao, and Weiwei Tu. 2021. Search to aggregate neighbor- hood for graph neural network. In Proceedings of the International Conference on Data Engineering. 552-563.\n\nRecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms. Shanlei Wayne Xin Zhao, Yupeng Mu, Zihan Hou, Kaiyuan Lin, Yushuo Li, Yujie Chen, Hui Lu, Changxin Wang, Xingyu Tian, Pan, Proceedings of the International Conference on Information and Knowledge Management. the International Conference on Information and Knowledge ManagementWayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Kaiyuan Li, Yushuo Chen, Yujie Lu, Hui Wang, Changxin Tian, Xingyu Pan, et al. 2021. RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms. In Proceedings of the International Conference on Information and Knowledge Management.\n", "annotations": {"author": "[{\"end\":144,\"start\":112},{\"end\":178,\"start\":145},{\"end\":210,\"start\":179},{\"end\":223,\"start\":211},{\"end\":234,\"start\":224},{\"end\":245,\"start\":235},{\"end\":304,\"start\":246},{\"end\":326,\"start\":305},{\"end\":406,\"start\":327}]", "publisher": null, "author_last_name": "[{\"end\":123,\"start\":119},{\"end\":154,\"start\":150},{\"end\":188,\"start\":185},{\"end\":222,\"start\":218},{\"end\":233,\"start\":229},{\"end\":244,\"start\":241}]", "author_first_name": "[{\"end\":118,\"start\":112},{\"end\":149,\"start\":145},{\"end\":184,\"start\":179},{\"end\":217,\"start\":211},{\"end\":228,\"start\":224},{\"end\":240,\"start\":235}]", "author_affiliation": "[{\"end\":303,\"start\":247},{\"end\":325,\"start\":306},{\"end\":405,\"start\":328}]", "title": "[{\"end\":83,\"start\":1},{\"end\":489,\"start\":407}]", "venue": "[{\"end\":589,\"start\":491}]", "abstract": "[{\"end\":2202,\"start\":859}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2404,\"start\":2400},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2407,\"start\":2404},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2410,\"start\":2407},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2413,\"start\":2410},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2718,\"start\":2714},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2721,\"start\":2718},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2825,\"start\":2821},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3271,\"start\":3267},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3274,\"start\":3271},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3394,\"start\":3390},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3405,\"start\":3401},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3420,\"start\":3416},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3435,\"start\":3431},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4048,\"start\":4044},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4198,\"start\":4194},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4941,\"start\":4937},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5775,\"start\":5771},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5778,\"start\":5775},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5827,\"start\":5823},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5962,\"start\":5958},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6940,\"start\":6936},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7384,\"start\":7380},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7394,\"start\":7390},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7412,\"start\":7408},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8724,\"start\":8720},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8727,\"start\":8724},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8730,\"start\":8727},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8733,\"start\":8730},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8765,\"start\":8761},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8768,\"start\":8765},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9138,\"start\":9134},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9161,\"start\":9157},{\"end\":9433,\"start\":9408},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9457,\"start\":9453},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9664,\"start\":9660},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9811,\"start\":9807},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10006,\"start\":10002},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10471,\"start\":10467},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10604,\"start\":10600},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10615,\"start\":10611},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10994,\"start\":10991},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10997,\"start\":10994},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11000,\"start\":10997},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11003,\"start\":11000},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11006,\"start\":11003},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11009,\"start\":11006},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11410,\"start\":11406},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11807,\"start\":11803},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13762,\"start\":13758},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13765,\"start\":13762},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14857,\"start\":14853},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14873,\"start\":14870},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15020,\"start\":15016},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15023,\"start\":15020},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15026,\"start\":15023},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15426,\"start\":15422},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16031,\"start\":16027},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16034,\"start\":16031},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16037,\"start\":16034},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16040,\"start\":16037},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16061,\"start\":16057},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16064,\"start\":16061},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16480,\"start\":16476},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16490,\"start\":16486},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16500,\"start\":16496},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16519,\"start\":16516},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16797,\"start\":16793},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16800,\"start\":16797},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16926,\"start\":16922},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16929,\"start\":16926},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17077,\"start\":17073},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17080,\"start\":17077},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17083,\"start\":17080},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17258,\"start\":17254},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17368,\"start\":17364},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17656,\"start\":17652},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18335,\"start\":18331},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18610,\"start\":18606},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20032,\"start\":20028},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21720,\"start\":21716},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22879,\"start\":22878},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23078,\"start\":23074},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23107,\"start\":23103},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23370,\"start\":23366},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23539,\"start\":23535},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23846,\"start\":23842},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24121,\"start\":24117},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24292,\"start\":24288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27146,\"start\":27142},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29124,\"start\":29120},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31382,\"start\":31379},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31384,\"start\":31382},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31387,\"start\":31384},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31390,\"start\":31387},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31393,\"start\":31390},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32924,\"start\":32920},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32965,\"start\":32961},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33007,\"start\":33003},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33053,\"start\":33049},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33095,\"start\":33091},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35378,\"start\":35375},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35790,\"start\":35787},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36080,\"start\":36077},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36125,\"start\":36121},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36142,\"start\":36138},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36245,\"start\":36241},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37333,\"start\":37329},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37446,\"start\":37442},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":38461,\"start\":38457},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38712,\"start\":38708},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39620,\"start\":39616}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41185,\"start\":40375},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41343,\"start\":41186},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41418,\"start\":41344},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41833,\"start\":41419},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41991,\"start\":41834},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42139,\"start\":41992},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42262,\"start\":42140},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42528,\"start\":42263},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42655,\"start\":42529},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42703,\"start\":42656},{\"attributes\":{\"id\":\"fig_10\"},\"end\":42775,\"start\":42704},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43454,\"start\":42776},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43814,\"start\":43455},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44541,\"start\":43815},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45285,\"start\":44542},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45462,\"start\":45286},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45845,\"start\":45463},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46127,\"start\":45846},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46282,\"start\":46128}]", "paragraph": "[{\"end\":3436,\"start\":2218},{\"end\":4807,\"start\":3438},{\"end\":5241,\"start\":4809},{\"end\":6493,\"start\":5243},{\"end\":7640,\"start\":6495},{\"end\":8136,\"start\":7642},{\"end\":8462,\"start\":8138},{\"end\":10007,\"start\":8517},{\"end\":10294,\"start\":10009},{\"end\":11965,\"start\":10333},{\"end\":12398,\"start\":12024},{\"end\":12721,\"start\":12400},{\"end\":12981,\"start\":12723},{\"end\":13058,\"start\":13052},{\"end\":13595,\"start\":13180},{\"end\":14021,\"start\":13597},{\"end\":14328,\"start\":14059},{\"end\":14434,\"start\":14343},{\"end\":14539,\"start\":14436},{\"end\":14766,\"start\":14569},{\"end\":15801,\"start\":14768},{\"end\":15929,\"start\":15803},{\"end\":16185,\"start\":15931},{\"end\":18284,\"start\":16210},{\"end\":18995,\"start\":18326},{\"end\":19188,\"start\":18997},{\"end\":19318,\"start\":19190},{\"end\":19808,\"start\":19366},{\"end\":19968,\"start\":19833},{\"end\":20073,\"start\":19970},{\"end\":21134,\"start\":20075},{\"end\":21349,\"start\":21194},{\"end\":21603,\"start\":21388},{\"end\":21994,\"start\":21605},{\"end\":22749,\"start\":22017},{\"end\":22814,\"start\":22751},{\"end\":23040,\"start\":22816},{\"end\":24505,\"start\":23042},{\"end\":25061,\"start\":24507},{\"end\":26251,\"start\":25099},{\"end\":27049,\"start\":26279},{\"end\":27457,\"start\":27088},{\"end\":27890,\"start\":27488},{\"end\":28347,\"start\":27892},{\"end\":28927,\"start\":28349},{\"end\":29872,\"start\":28959},{\"end\":30097,\"start\":29874},{\"end\":30439,\"start\":30099},{\"end\":31032,\"start\":30441},{\"end\":31733,\"start\":31085},{\"end\":32543,\"start\":31735},{\"end\":32884,\"start\":32545},{\"end\":33585,\"start\":32894},{\"end\":34326,\"start\":33587},{\"end\":35379,\"start\":34341},{\"end\":35585,\"start\":35399},{\"end\":36246,\"start\":35600},{\"end\":36542,\"start\":36248},{\"end\":36819,\"start\":36544},{\"end\":37447,\"start\":36873},{\"end\":37693,\"start\":37466},{\"end\":38134,\"start\":37695},{\"end\":38626,\"start\":38195},{\"end\":38811,\"start\":38645},{\"end\":39175,\"start\":38836},{\"end\":39311,\"start\":39199},{\"end\":39621,\"start\":39336},{\"end\":40374,\"start\":39623}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13008,\"start\":12982},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13040,\"start\":13008},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13097,\"start\":13059},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13139,\"start\":13097},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13179,\"start\":13139},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14058,\"start\":14022},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14342,\"start\":14329},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16209,\"start\":16186},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21387,\"start\":21350},{\"attributes\":{\"id\":\"formula_9\"},\"end\":31056,\"start\":31033},{\"attributes\":{\"id\":\"formula_10\"},\"end\":39335,\"start\":39312}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17836,\"start\":17829},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19719,\"start\":19712},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21755,\"start\":21748},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25505,\"start\":25498},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28084,\"start\":28077},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30345,\"start\":30338},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30693,\"start\":30686},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30792,\"start\":30785},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31688,\"start\":31681},{\"end\":32421,\"start\":32414},{\"end\":32674,\"start\":32667},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39785,\"start\":39778}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2216,\"start\":2204},{\"attributes\":{\"n\":\"2\"},\"end\":8515,\"start\":8465},{\"attributes\":{\"n\":\"2.2\"},\"end\":10331,\"start\":10297},{\"attributes\":{\"n\":\"3\"},\"end\":12022,\"start\":11968},{\"end\":13050,\"start\":13042},{\"attributes\":{\"n\":\"3.2\"},\"end\":14567,\"start\":14542},{\"attributes\":{\"n\":\"3.3\"},\"end\":18324,\"start\":18287},{\"attributes\":{\"n\":\"4\"},\"end\":19364,\"start\":19321},{\"attributes\":{\"n\":\"4.2\"},\"end\":19831,\"start\":19811},{\"attributes\":{\"n\":\"4.3\"},\"end\":21153,\"start\":21137},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":21192,\"start\":21156},{\"attributes\":{\"n\":\"4.4\"},\"end\":22015,\"start\":21997},{\"attributes\":{\"n\":\"5\"},\"end\":25097,\"start\":25064},{\"attributes\":{\"n\":\"5.1\"},\"end\":26277,\"start\":26254},{\"attributes\":{\"n\":\"5.2\"},\"end\":27062,\"start\":27052},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":27086,\"start\":27065},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":27486,\"start\":27460},{\"end\":28957,\"start\":28930},{\"attributes\":{\"n\":\"5.3\"},\"end\":31083,\"start\":31058},{\"end\":32892,\"start\":32887},{\"attributes\":{\"n\":\"6\"},\"end\":34339,\"start\":34329},{\"end\":35397,\"start\":35382},{\"end\":35598,\"start\":35588},{\"end\":36871,\"start\":36822},{\"end\":37464,\"start\":37450},{\"end\":38193,\"start\":38137},{\"end\":38643,\"start\":38629},{\"end\":38834,\"start\":38814},{\"end\":39197,\"start\":39178},{\"end\":40386,\"start\":40376},{\"end\":41197,\"start\":41187},{\"end\":41355,\"start\":41345},{\"end\":41421,\"start\":41420},{\"end\":41845,\"start\":41835},{\"end\":42003,\"start\":41993},{\"end\":42151,\"start\":42141},{\"end\":42667,\"start\":42657},{\"end\":42715,\"start\":42705},{\"end\":43465,\"start\":43456},{\"end\":44552,\"start\":44543},{\"end\":45296,\"start\":45287},{\"end\":45473,\"start\":45464},{\"end\":45856,\"start\":45847},{\"end\":46138,\"start\":46129}]", "table": "[{\"end\":43454,\"start\":42887},{\"end\":44541,\"start\":43826},{\"end\":45285,\"start\":44581},{\"end\":45462,\"start\":45339},{\"end\":45845,\"start\":45535},{\"end\":46127,\"start\":45891},{\"end\":46282,\"start\":46140}]", "figure_caption": "[{\"end\":41185,\"start\":40388},{\"end\":41343,\"start\":41199},{\"end\":41418,\"start\":41357},{\"end\":41833,\"start\":41422},{\"end\":41991,\"start\":41847},{\"end\":42139,\"start\":42005},{\"end\":42262,\"start\":42153},{\"end\":42528,\"start\":42265},{\"end\":42655,\"start\":42531},{\"end\":42703,\"start\":42669},{\"end\":42775,\"start\":42717},{\"end\":42887,\"start\":42778},{\"end\":43814,\"start\":43467},{\"end\":43826,\"start\":43817},{\"end\":44581,\"start\":44554},{\"end\":45339,\"start\":45298},{\"end\":45535,\"start\":45475},{\"end\":45891,\"start\":45858}]", "figure_ref": "[{\"end\":2505,\"start\":2497},{\"end\":3071,\"start\":3063},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5412,\"start\":5404},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5655,\"start\":5643},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12219,\"start\":12211},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12977,\"start\":12969},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14703,\"start\":14695},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14762,\"start\":14754},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15925,\"start\":15917},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20373,\"start\":20365},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21024,\"start\":21016},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21802,\"start\":21794},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22046,\"start\":22038},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25748,\"start\":25740},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28346,\"start\":28338},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29871,\"start\":29863},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30438,\"start\":30430},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35518,\"start\":35474},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37119,\"start\":37111},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39077,\"start\":39068},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":39845,\"start\":39837}]", "bib_author_first_name": "[{\"end\":46290,\"start\":46284},{\"end\":46306,\"start\":46300},{\"end\":46308,\"start\":46307},{\"end\":46318,\"start\":46315},{\"end\":46651,\"start\":46648},{\"end\":46660,\"start\":46658},{\"end\":46672,\"start\":46665},{\"end\":46682,\"start\":46679},{\"end\":46694,\"start\":46690},{\"end\":47156,\"start\":47151},{\"end\":47190,\"start\":47183},{\"end\":47628,\"start\":47623},{\"end\":47643,\"start\":47635},{\"end\":47653,\"start\":47649},{\"end\":47664,\"start\":47660},{\"end\":48143,\"start\":48137},{\"end\":48154,\"start\":48150},{\"end\":48174,\"start\":48168},{\"end\":48188,\"start\":48182},{\"end\":48432,\"start\":48427},{\"end\":48441,\"start\":48438},{\"end\":48450,\"start\":48446},{\"end\":48459,\"start\":48455},{\"end\":48468,\"start\":48464},{\"end\":48482,\"start\":48475},{\"end\":48494,\"start\":48489},{\"end\":48893,\"start\":48889},{\"end\":48903,\"start\":48899},{\"end\":48914,\"start\":48910},{\"end\":48927,\"start\":48922},{\"end\":48937,\"start\":48934},{\"end\":49363,\"start\":49360},{\"end\":49388,\"start\":49384},{\"end\":49738,\"start\":49731},{\"end\":49749,\"start\":49743},{\"end\":50173,\"start\":50165},{\"end\":50182,\"start\":50178},{\"end\":50194,\"start\":50189},{\"end\":50204,\"start\":50201},{\"end\":50217,\"start\":50209},{\"end\":50229,\"start\":50225},{\"end\":50758,\"start\":50750},{\"end\":50767,\"start\":50763},{\"end\":50781,\"start\":50774},{\"end\":50796,\"start\":50789},{\"end\":50805,\"start\":50802},{\"end\":50818,\"start\":50810},{\"end\":51193,\"start\":51192},{\"end\":51210,\"start\":51204},{\"end\":51212,\"start\":51211},{\"end\":51226,\"start\":51224},{\"end\":51240,\"start\":51236},{\"end\":51819,\"start\":51813},{\"end\":51834,\"start\":51828},{\"end\":52178,\"start\":52177},{\"end\":52194,\"start\":52189},{\"end\":52552,\"start\":52551},{\"end\":52564,\"start\":52561},{\"end\":52967,\"start\":52961},{\"end\":53423,\"start\":53417},{\"end\":53437,\"start\":53431},{\"end\":53449,\"start\":53444},{\"end\":53660,\"start\":53652},{\"end\":53675,\"start\":53666},{\"end\":53684,\"start\":53681},{\"end\":53699,\"start\":53694},{\"end\":53996,\"start\":53988},{\"end\":54010,\"start\":54002},{\"end\":54019,\"start\":54016},{\"end\":54338,\"start\":54334},{\"end\":54352,\"start\":54347},{\"end\":54366,\"start\":54360},{\"end\":54620,\"start\":54616},{\"end\":54638,\"start\":54633},{\"end\":55057,\"start\":55049},{\"end\":55066,\"start\":55062},{\"end\":55078,\"start\":55073},{\"end\":55091,\"start\":55084},{\"end\":55106,\"start\":55098},{\"end\":55119,\"start\":55113},{\"end\":55129,\"start\":55124},{\"end\":55143,\"start\":55136},{\"end\":55157,\"start\":55151},{\"end\":55167,\"start\":55164},{\"end\":55746,\"start\":55743},{\"end\":55759,\"start\":55751},{\"end\":55770,\"start\":55766},{\"end\":55781,\"start\":55776},{\"end\":56186,\"start\":56180},{\"end\":56492,\"start\":56484},{\"end\":56506,\"start\":56500},{\"end\":56843,\"start\":56838},{\"end\":56863,\"start\":56857},{\"end\":56880,\"start\":56873},{\"end\":56893,\"start\":56886},{\"end\":56903,\"start\":56898},{\"end\":57309,\"start\":57304},{\"end\":57326,\"start\":57323},{\"end\":57334,\"start\":57327},{\"end\":57349,\"start\":57345},{\"end\":57779,\"start\":57774},{\"end\":57798,\"start\":57796},{\"end\":57813,\"start\":57809},{\"end\":58130,\"start\":58123},{\"end\":58141,\"start\":58139},{\"end\":58155,\"start\":58149},{\"end\":58501,\"start\":58495},{\"end\":58516,\"start\":58510},{\"end\":58532,\"start\":58526},{\"end\":58546,\"start\":58542},{\"end\":58880,\"start\":58871},{\"end\":58899,\"start\":58889},{\"end\":58917,\"start\":58907},{\"end\":58937,\"start\":58930},{\"end\":59308,\"start\":59305},{\"end\":59316,\"start\":59314},{\"end\":59324,\"start\":59321},{\"end\":59334,\"start\":59331},{\"end\":59348,\"start\":59341},{\"end\":59356,\"start\":59353},{\"end\":59368,\"start\":59364},{\"end\":59789,\"start\":59781},{\"end\":59797,\"start\":59795},{\"end\":59811,\"start\":59804},{\"end\":59823,\"start\":59818},{\"end\":59839,\"start\":59835},{\"end\":59855,\"start\":59849},{\"end\":59870,\"start\":59864},{\"end\":60293,\"start\":60288},{\"end\":60313,\"start\":60306},{\"end\":60331,\"start\":60324},{\"end\":60349,\"start\":60342},{\"end\":60364,\"start\":60358},{\"end\":60376,\"start\":60370},{\"end\":60799,\"start\":60796},{\"end\":60811,\"start\":60806},{\"end\":60820,\"start\":60817},{\"end\":60833,\"start\":60828},{\"end\":61202,\"start\":61197},{\"end\":61217,\"start\":61209},{\"end\":61226,\"start\":61222},{\"end\":61237,\"start\":61233},{\"end\":61252,\"start\":61244},{\"end\":61738,\"start\":61733},{\"end\":61751,\"start\":61745},{\"end\":61759,\"start\":61757},{\"end\":61775,\"start\":61767},{\"end\":61784,\"start\":61780},{\"end\":61797,\"start\":61789},{\"end\":62315,\"start\":62311},{\"end\":62328,\"start\":62322},{\"end\":62340,\"start\":62335},{\"end\":62352,\"start\":62346},{\"end\":62367,\"start\":62359},{\"end\":62698,\"start\":62692},{\"end\":62706,\"start\":62703},{\"end\":62718,\"start\":62712},{\"end\":62729,\"start\":62726},{\"end\":63018,\"start\":63012},{\"end\":63029,\"start\":63023},{\"end\":63038,\"start\":63034},{\"end\":63057,\"start\":63049},{\"end\":63441,\"start\":63438},{\"end\":63455,\"start\":63448},{\"end\":63467,\"start\":63460},{\"end\":63478,\"start\":63474},{\"end\":63494,\"start\":63493},{\"end\":63508,\"start\":63504},{\"end\":64030,\"start\":64023},{\"end\":64039,\"start\":64036},{\"end\":64050,\"start\":64046},{\"end\":64353,\"start\":64346},{\"end\":64367,\"start\":64361},{\"end\":64793,\"start\":64787},{\"end\":64807,\"start\":64801},{\"end\":64818,\"start\":64814},{\"end\":64831,\"start\":64825},{\"end\":64844,\"start\":64837},{\"end\":64858,\"start\":64850},{\"end\":65390,\"start\":65385},{\"end\":65401,\"start\":65398},{\"end\":65413,\"start\":65408},{\"end\":65794,\"start\":65790},{\"end\":65809,\"start\":65801},{\"end\":65816,\"start\":65815},{\"end\":65831,\"start\":65824},{\"end\":66194,\"start\":66190},{\"end\":66209,\"start\":66201},{\"end\":66221,\"start\":66215},{\"end\":66628,\"start\":66621},{\"end\":66651,\"start\":66645},{\"end\":66661,\"start\":66656},{\"end\":66674,\"start\":66667},{\"end\":66686,\"start\":66680},{\"end\":66696,\"start\":66691},{\"end\":66706,\"start\":66703},{\"end\":66719,\"start\":66711},{\"end\":66732,\"start\":66726}]", "bib_author_last_name": "[{\"end\":46298,\"start\":46291},{\"end\":46313,\"start\":46309},{\"end\":46323,\"start\":46319},{\"end\":46332,\"start\":46325},{\"end\":46656,\"start\":46652},{\"end\":46663,\"start\":46661},{\"end\":46677,\"start\":46673},{\"end\":46688,\"start\":46683},{\"end\":46699,\"start\":46695},{\"end\":47181,\"start\":47157},{\"end\":47200,\"start\":47191},{\"end\":47209,\"start\":47202},{\"end\":47633,\"start\":47629},{\"end\":47647,\"start\":47644},{\"end\":47658,\"start\":47654},{\"end\":47670,\"start\":47665},{\"end\":48148,\"start\":48144},{\"end\":48166,\"start\":48155},{\"end\":48180,\"start\":48175},{\"end\":48195,\"start\":48189},{\"end\":48436,\"start\":48433},{\"end\":48444,\"start\":48442},{\"end\":48453,\"start\":48451},{\"end\":48462,\"start\":48460},{\"end\":48473,\"start\":48469},{\"end\":48487,\"start\":48483},{\"end\":48498,\"start\":48495},{\"end\":48897,\"start\":48894},{\"end\":48908,\"start\":48904},{\"end\":48920,\"start\":48915},{\"end\":48932,\"start\":48928},{\"end\":48940,\"start\":48938},{\"end\":49382,\"start\":49364},{\"end\":49393,\"start\":49389},{\"end\":49403,\"start\":49395},{\"end\":49741,\"start\":49739},{\"end\":49757,\"start\":49750},{\"end\":50176,\"start\":50174},{\"end\":50187,\"start\":50183},{\"end\":50199,\"start\":50195},{\"end\":50207,\"start\":50205},{\"end\":50223,\"start\":50218},{\"end\":50234,\"start\":50230},{\"end\":50761,\"start\":50759},{\"end\":50772,\"start\":50768},{\"end\":50787,\"start\":50782},{\"end\":50800,\"start\":50797},{\"end\":50808,\"start\":50806},{\"end\":50823,\"start\":50819},{\"end\":51202,\"start\":51194},{\"end\":51222,\"start\":51213},{\"end\":51234,\"start\":51227},{\"end\":51249,\"start\":51241},{\"end\":51256,\"start\":51251},{\"end\":51826,\"start\":51820},{\"end\":51840,\"start\":51835},{\"end\":52187,\"start\":52179},{\"end\":52201,\"start\":52195},{\"end\":52205,\"start\":52203},{\"end\":52559,\"start\":52553},{\"end\":52569,\"start\":52565},{\"end\":52578,\"start\":52571},{\"end\":52973,\"start\":52968},{\"end\":53429,\"start\":53424},{\"end\":53442,\"start\":53438},{\"end\":53458,\"start\":53450},{\"end\":53664,\"start\":53661},{\"end\":53679,\"start\":53676},{\"end\":53692,\"start\":53685},{\"end\":53706,\"start\":53700},{\"end\":54000,\"start\":53997},{\"end\":54014,\"start\":54011},{\"end\":54027,\"start\":54020},{\"end\":54345,\"start\":54339},{\"end\":54358,\"start\":54353},{\"end\":54371,\"start\":54367},{\"end\":54631,\"start\":54621},{\"end\":54645,\"start\":54639},{\"end\":55060,\"start\":55058},{\"end\":55071,\"start\":55067},{\"end\":55082,\"start\":55079},{\"end\":55096,\"start\":55092},{\"end\":55111,\"start\":55107},{\"end\":55122,\"start\":55120},{\"end\":55134,\"start\":55130},{\"end\":55149,\"start\":55144},{\"end\":55162,\"start\":55158},{\"end\":55172,\"start\":55168},{\"end\":55749,\"start\":55747},{\"end\":55764,\"start\":55760},{\"end\":55774,\"start\":55771},{\"end\":55795,\"start\":55782},{\"end\":55801,\"start\":55797},{\"end\":56191,\"start\":56187},{\"end\":56213,\"start\":56193},{\"end\":56498,\"start\":56493},{\"end\":56526,\"start\":56507},{\"end\":56535,\"start\":56528},{\"end\":56855,\"start\":56844},{\"end\":56871,\"start\":56864},{\"end\":56884,\"start\":56881},{\"end\":56896,\"start\":56894},{\"end\":56910,\"start\":56904},{\"end\":57321,\"start\":57310},{\"end\":57343,\"start\":57335},{\"end\":57358,\"start\":57350},{\"end\":57794,\"start\":57780},{\"end\":57807,\"start\":57799},{\"end\":57819,\"start\":57814},{\"end\":57829,\"start\":57821},{\"end\":58137,\"start\":58131},{\"end\":58147,\"start\":58142},{\"end\":58161,\"start\":58156},{\"end\":58508,\"start\":58502},{\"end\":58524,\"start\":58517},{\"end\":58540,\"start\":58533},{\"end\":58552,\"start\":58547},{\"end\":58887,\"start\":58881},{\"end\":58905,\"start\":58900},{\"end\":58928,\"start\":58918},{\"end\":58947,\"start\":58938},{\"end\":59312,\"start\":59309},{\"end\":59319,\"start\":59317},{\"end\":59329,\"start\":59325},{\"end\":59339,\"start\":59335},{\"end\":59351,\"start\":59349},{\"end\":59362,\"start\":59357},{\"end\":59373,\"start\":59369},{\"end\":59793,\"start\":59790},{\"end\":59802,\"start\":59798},{\"end\":59816,\"start\":59812},{\"end\":59833,\"start\":59824},{\"end\":59847,\"start\":59840},{\"end\":59862,\"start\":59856},{\"end\":59873,\"start\":59871},{\"end\":60304,\"start\":60294},{\"end\":60322,\"start\":60314},{\"end\":60340,\"start\":60332},{\"end\":60356,\"start\":60350},{\"end\":60368,\"start\":60365},{\"end\":60383,\"start\":60377},{\"end\":60804,\"start\":60800},{\"end\":60815,\"start\":60812},{\"end\":60826,\"start\":60821},{\"end\":60837,\"start\":60834},{\"end\":61207,\"start\":61203},{\"end\":61220,\"start\":61218},{\"end\":61231,\"start\":61227},{\"end\":61242,\"start\":61238},{\"end\":61257,\"start\":61253},{\"end\":61743,\"start\":61739},{\"end\":61755,\"start\":61752},{\"end\":61765,\"start\":61760},{\"end\":61778,\"start\":61776},{\"end\":61787,\"start\":61785},{\"end\":61802,\"start\":61798},{\"end\":62320,\"start\":62316},{\"end\":62333,\"start\":62329},{\"end\":62344,\"start\":62341},{\"end\":62357,\"start\":62353},{\"end\":62370,\"start\":62368},{\"end\":62701,\"start\":62699},{\"end\":62710,\"start\":62707},{\"end\":62724,\"start\":62719},{\"end\":62733,\"start\":62730},{\"end\":63021,\"start\":63019},{\"end\":63032,\"start\":63030},{\"end\":63047,\"start\":63039},{\"end\":63065,\"start\":63058},{\"end\":63446,\"start\":63442},{\"end\":63458,\"start\":63456},{\"end\":63472,\"start\":63468},{\"end\":63491,\"start\":63479},{\"end\":63502,\"start\":63495},{\"end\":63517,\"start\":63509},{\"end\":63527,\"start\":63519},{\"end\":64034,\"start\":64031},{\"end\":64044,\"start\":64040},{\"end\":64059,\"start\":64051},{\"end\":64359,\"start\":64354},{\"end\":64375,\"start\":64368},{\"end\":64799,\"start\":64794},{\"end\":64812,\"start\":64808},{\"end\":64823,\"start\":64819},{\"end\":64835,\"start\":64832},{\"end\":64848,\"start\":64845},{\"end\":64861,\"start\":64859},{\"end\":65396,\"start\":65391},{\"end\":65406,\"start\":65402},{\"end\":65417,\"start\":65414},{\"end\":65799,\"start\":65795},{\"end\":65813,\"start\":65810},{\"end\":65822,\"start\":65817},{\"end\":65836,\"start\":65832},{\"end\":65841,\"start\":65838},{\"end\":66199,\"start\":66195},{\"end\":66213,\"start\":66210},{\"end\":66224,\"start\":66222},{\"end\":66643,\"start\":66629},{\"end\":66654,\"start\":66652},{\"end\":66665,\"start\":66662},{\"end\":66678,\"start\":66675},{\"end\":66689,\"start\":66687},{\"end\":66701,\"start\":66697},{\"end\":66709,\"start\":66707},{\"end\":66724,\"start\":66720},{\"end\":66737,\"start\":66733},{\"end\":66742,\"start\":66739}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1706.02263\",\"id\":\"b0\"},\"end\":46542,\"start\":46284},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":210932292},\"end\":47049,\"start\":46544},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":196831663},\"end\":47539,\"start\":47051},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":235490468},\"end\":48094,\"start\":47541},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16172864},\"end\":48376,\"start\":48096},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":67769538},\"end\":48815,\"start\":48378},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":128361695},\"end\":49307,\"start\":48817},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4755450},\"end\":49626,\"start\":49309},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1964279},\"end\":50082,\"start\":49628},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211043589},\"end\":50716,\"start\":50084},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13907106},\"end\":51125,\"start\":50718},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3079360},\"end\":51716,\"start\":51127},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10134126},\"end\":52131,\"start\":51718},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6628106},\"end\":52483,\"start\":52133},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3144218},\"end\":52875,\"start\":52485},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207168823},\"end\":53358,\"start\":52877},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":58370896},\"end\":53613,\"start\":53360},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17230726},\"end\":53986,\"start\":53615},{\"attributes\":{\"doi\":\"arXiv:1205.3193\",\"id\":\"b18\"},\"end\":54265,\"start\":53988},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14604122},\"end\":54560,\"start\":54267},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14337532},\"end\":54935,\"start\":54562},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":236980351},\"end\":55693,\"start\":54937},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7848601},\"end\":56142,\"start\":55695},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":467086},\"end\":56410,\"start\":56144},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18052422},\"end\":56787,\"start\":56412},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":170078814},\"end\":57233,\"start\":56789},{\"attributes\":{\"id\":\"b26\"},\"end\":57705,\"start\":57235},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":218719424},\"end\":58121,\"start\":57707},{\"attributes\":{\"doi\":\"arXiv:1905.01395\",\"id\":\"b28\"},\"end\":58432,\"start\":58123},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8047550},\"end\":58869,\"start\":58434},{\"attributes\":{\"doi\":\"arXiv:1811.05868\",\"id\":\"b30\"},\"end\":59196,\"start\":58871},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":221785064},\"end\":59716,\"start\":59198},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":51891697},\"end\":60260,\"start\":59718},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3292002},\"end\":60710,\"start\":60262},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":235825941},\"end\":61157,\"start\":60712},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":150380651},\"end\":61687,\"start\":61159},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":220347145},\"end\":62249,\"start\":61689},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":208267838},\"end\":62690,\"start\":62251},{\"attributes\":{\"doi\":\"arXiv:2011.02260\",\"id\":\"b38\"},\"end\":62969,\"start\":62692},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52895589},\"end\":63365,\"start\":62971},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":46949657},\"end\":63981,\"start\":63367},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":226975970},\"end\":64266,\"start\":63983},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":215813246},\"end\":64669,\"start\":64268},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":234778182},\"end\":65335,\"start\":64671},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232076285},\"end\":65738,\"start\":65337},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10351832},\"end\":66129,\"start\":65740},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":233231689},\"end\":66522,\"start\":66131},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":226237553},\"end\":67217,\"start\":66524}]", "bib_title": "[{\"end\":46646,\"start\":46544},{\"end\":47149,\"start\":47051},{\"end\":47621,\"start\":47541},{\"end\":48135,\"start\":48096},{\"end\":48425,\"start\":48378},{\"end\":48887,\"start\":48817},{\"end\":49358,\"start\":49309},{\"end\":49729,\"start\":49628},{\"end\":50163,\"start\":50084},{\"end\":50748,\"start\":50718},{\"end\":51190,\"start\":51127},{\"end\":51811,\"start\":51718},{\"end\":52175,\"start\":52133},{\"end\":52549,\"start\":52485},{\"end\":52959,\"start\":52877},{\"end\":53415,\"start\":53360},{\"end\":53650,\"start\":53615},{\"end\":54332,\"start\":54267},{\"end\":54614,\"start\":54562},{\"end\":55047,\"start\":54937},{\"end\":55741,\"start\":55695},{\"end\":56178,\"start\":56144},{\"end\":56482,\"start\":56412},{\"end\":56836,\"start\":56789},{\"end\":57302,\"start\":57235},{\"end\":57772,\"start\":57707},{\"end\":58493,\"start\":58434},{\"end\":59303,\"start\":59198},{\"end\":59779,\"start\":59718},{\"end\":60286,\"start\":60262},{\"end\":60794,\"start\":60712},{\"end\":61195,\"start\":61159},{\"end\":61731,\"start\":61689},{\"end\":62309,\"start\":62251},{\"end\":63010,\"start\":62971},{\"end\":63436,\"start\":63367},{\"end\":64021,\"start\":63983},{\"end\":64344,\"start\":64268},{\"end\":64785,\"start\":64671},{\"end\":65383,\"start\":65337},{\"end\":65788,\"start\":65740},{\"end\":66188,\"start\":66131},{\"end\":66619,\"start\":66524}]", "bib_author": "[{\"end\":46300,\"start\":46284},{\"end\":46315,\"start\":46300},{\"end\":46325,\"start\":46315},{\"end\":46334,\"start\":46325},{\"end\":46658,\"start\":46648},{\"end\":46665,\"start\":46658},{\"end\":46679,\"start\":46665},{\"end\":46690,\"start\":46679},{\"end\":46701,\"start\":46690},{\"end\":47183,\"start\":47151},{\"end\":47202,\"start\":47183},{\"end\":47211,\"start\":47202},{\"end\":47635,\"start\":47623},{\"end\":47649,\"start\":47635},{\"end\":47660,\"start\":47649},{\"end\":47672,\"start\":47660},{\"end\":48150,\"start\":48137},{\"end\":48168,\"start\":48150},{\"end\":48182,\"start\":48168},{\"end\":48197,\"start\":48182},{\"end\":48438,\"start\":48427},{\"end\":48446,\"start\":48438},{\"end\":48455,\"start\":48446},{\"end\":48464,\"start\":48455},{\"end\":48475,\"start\":48464},{\"end\":48489,\"start\":48475},{\"end\":48500,\"start\":48489},{\"end\":48899,\"start\":48889},{\"end\":48910,\"start\":48899},{\"end\":48922,\"start\":48910},{\"end\":48934,\"start\":48922},{\"end\":48942,\"start\":48934},{\"end\":49384,\"start\":49360},{\"end\":49395,\"start\":49384},{\"end\":49405,\"start\":49395},{\"end\":49743,\"start\":49731},{\"end\":49759,\"start\":49743},{\"end\":50178,\"start\":50165},{\"end\":50189,\"start\":50178},{\"end\":50201,\"start\":50189},{\"end\":50209,\"start\":50201},{\"end\":50225,\"start\":50209},{\"end\":50236,\"start\":50225},{\"end\":50763,\"start\":50750},{\"end\":50774,\"start\":50763},{\"end\":50789,\"start\":50774},{\"end\":50802,\"start\":50789},{\"end\":50810,\"start\":50802},{\"end\":50825,\"start\":50810},{\"end\":51204,\"start\":51192},{\"end\":51224,\"start\":51204},{\"end\":51236,\"start\":51224},{\"end\":51251,\"start\":51236},{\"end\":51258,\"start\":51251},{\"end\":51828,\"start\":51813},{\"end\":51842,\"start\":51828},{\"end\":52189,\"start\":52177},{\"end\":52203,\"start\":52189},{\"end\":52207,\"start\":52203},{\"end\":52561,\"start\":52551},{\"end\":52571,\"start\":52561},{\"end\":52580,\"start\":52571},{\"end\":52975,\"start\":52961},{\"end\":53431,\"start\":53417},{\"end\":53444,\"start\":53431},{\"end\":53460,\"start\":53444},{\"end\":53666,\"start\":53652},{\"end\":53681,\"start\":53666},{\"end\":53694,\"start\":53681},{\"end\":53708,\"start\":53694},{\"end\":54002,\"start\":53988},{\"end\":54016,\"start\":54002},{\"end\":54029,\"start\":54016},{\"end\":54347,\"start\":54334},{\"end\":54360,\"start\":54347},{\"end\":54373,\"start\":54360},{\"end\":54633,\"start\":54616},{\"end\":54647,\"start\":54633},{\"end\":55062,\"start\":55049},{\"end\":55073,\"start\":55062},{\"end\":55084,\"start\":55073},{\"end\":55098,\"start\":55084},{\"end\":55113,\"start\":55098},{\"end\":55124,\"start\":55113},{\"end\":55136,\"start\":55124},{\"end\":55151,\"start\":55136},{\"end\":55164,\"start\":55151},{\"end\":55174,\"start\":55164},{\"end\":55751,\"start\":55743},{\"end\":55766,\"start\":55751},{\"end\":55776,\"start\":55766},{\"end\":55797,\"start\":55776},{\"end\":55803,\"start\":55797},{\"end\":56193,\"start\":56180},{\"end\":56215,\"start\":56193},{\"end\":56500,\"start\":56484},{\"end\":56528,\"start\":56500},{\"end\":56537,\"start\":56528},{\"end\":56857,\"start\":56838},{\"end\":56873,\"start\":56857},{\"end\":56886,\"start\":56873},{\"end\":56898,\"start\":56886},{\"end\":56912,\"start\":56898},{\"end\":57323,\"start\":57304},{\"end\":57345,\"start\":57323},{\"end\":57360,\"start\":57345},{\"end\":57796,\"start\":57774},{\"end\":57809,\"start\":57796},{\"end\":57821,\"start\":57809},{\"end\":57831,\"start\":57821},{\"end\":58139,\"start\":58123},{\"end\":58149,\"start\":58139},{\"end\":58163,\"start\":58149},{\"end\":58510,\"start\":58495},{\"end\":58526,\"start\":58510},{\"end\":58542,\"start\":58526},{\"end\":58554,\"start\":58542},{\"end\":58889,\"start\":58871},{\"end\":58907,\"start\":58889},{\"end\":58930,\"start\":58907},{\"end\":58949,\"start\":58930},{\"end\":59314,\"start\":59305},{\"end\":59321,\"start\":59314},{\"end\":59331,\"start\":59321},{\"end\":59341,\"start\":59331},{\"end\":59353,\"start\":59341},{\"end\":59364,\"start\":59353},{\"end\":59375,\"start\":59364},{\"end\":59795,\"start\":59781},{\"end\":59804,\"start\":59795},{\"end\":59818,\"start\":59804},{\"end\":59835,\"start\":59818},{\"end\":59849,\"start\":59835},{\"end\":59864,\"start\":59849},{\"end\":59875,\"start\":59864},{\"end\":60306,\"start\":60288},{\"end\":60324,\"start\":60306},{\"end\":60342,\"start\":60324},{\"end\":60358,\"start\":60342},{\"end\":60370,\"start\":60358},{\"end\":60385,\"start\":60370},{\"end\":60806,\"start\":60796},{\"end\":60817,\"start\":60806},{\"end\":60828,\"start\":60817},{\"end\":60839,\"start\":60828},{\"end\":61209,\"start\":61197},{\"end\":61222,\"start\":61209},{\"end\":61233,\"start\":61222},{\"end\":61244,\"start\":61233},{\"end\":61259,\"start\":61244},{\"end\":61745,\"start\":61733},{\"end\":61757,\"start\":61745},{\"end\":61767,\"start\":61757},{\"end\":61780,\"start\":61767},{\"end\":61789,\"start\":61780},{\"end\":61804,\"start\":61789},{\"end\":62322,\"start\":62311},{\"end\":62335,\"start\":62322},{\"end\":62346,\"start\":62335},{\"end\":62359,\"start\":62346},{\"end\":62372,\"start\":62359},{\"end\":62703,\"start\":62692},{\"end\":62712,\"start\":62703},{\"end\":62726,\"start\":62712},{\"end\":62735,\"start\":62726},{\"end\":63023,\"start\":63012},{\"end\":63034,\"start\":63023},{\"end\":63049,\"start\":63034},{\"end\":63067,\"start\":63049},{\"end\":63448,\"start\":63438},{\"end\":63460,\"start\":63448},{\"end\":63474,\"start\":63460},{\"end\":63493,\"start\":63474},{\"end\":63504,\"start\":63493},{\"end\":63519,\"start\":63504},{\"end\":63529,\"start\":63519},{\"end\":64036,\"start\":64023},{\"end\":64046,\"start\":64036},{\"end\":64061,\"start\":64046},{\"end\":64361,\"start\":64346},{\"end\":64377,\"start\":64361},{\"end\":64801,\"start\":64787},{\"end\":64814,\"start\":64801},{\"end\":64825,\"start\":64814},{\"end\":64837,\"start\":64825},{\"end\":64850,\"start\":64837},{\"end\":64863,\"start\":64850},{\"end\":65398,\"start\":65385},{\"end\":65408,\"start\":65398},{\"end\":65419,\"start\":65408},{\"end\":65801,\"start\":65790},{\"end\":65815,\"start\":65801},{\"end\":65824,\"start\":65815},{\"end\":65838,\"start\":65824},{\"end\":65843,\"start\":65838},{\"end\":66201,\"start\":66190},{\"end\":66215,\"start\":66201},{\"end\":66226,\"start\":66215},{\"end\":66645,\"start\":66621},{\"end\":66656,\"start\":66645},{\"end\":66667,\"start\":66656},{\"end\":66680,\"start\":66667},{\"end\":66691,\"start\":66680},{\"end\":66703,\"start\":66691},{\"end\":66711,\"start\":66703},{\"end\":66726,\"start\":66711},{\"end\":66739,\"start\":66726},{\"end\":66744,\"start\":66739}]", "bib_venue": "[{\"end\":46387,\"start\":46350},{\"end\":46762,\"start\":46701},{\"end\":47263,\"start\":47211},{\"end\":47765,\"start\":47672},{\"end\":48219,\"start\":48197},{\"end\":48561,\"start\":48500},{\"end\":49018,\"start\":48942},{\"end\":49454,\"start\":49405},{\"end\":49820,\"start\":49759},{\"end\":50342,\"start\":50236},{\"end\":50886,\"start\":50825},{\"end\":51364,\"start\":51258},{\"end\":51894,\"start\":51842},{\"end\":52274,\"start\":52207},{\"end\":52647,\"start\":52580},{\"end\":53068,\"start\":52975},{\"end\":53468,\"start\":53460},{\"end\":53767,\"start\":53708},{\"end\":54101,\"start\":54044},{\"end\":54396,\"start\":54373},{\"end\":54714,\"start\":54647},{\"end\":55267,\"start\":55174},{\"end\":55876,\"start\":55803},{\"end\":56264,\"start\":56215},{\"end\":56586,\"start\":56537},{\"end\":56974,\"start\":56912},{\"end\":57432,\"start\":57360},{\"end\":57883,\"start\":57831},{\"end\":58252,\"start\":58179},{\"end\":58615,\"start\":58554},{\"end\":59008,\"start\":58965},{\"end\":59427,\"start\":59375},{\"end\":59947,\"start\":59875},{\"end\":60452,\"start\":60385},{\"end\":60898,\"start\":60839},{\"end\":61365,\"start\":61259},{\"end\":61910,\"start\":61804},{\"end\":62433,\"start\":62372},{\"end\":62805,\"start\":62751},{\"end\":63134,\"start\":63067},{\"end\":63622,\"start\":63529},{\"end\":64110,\"start\":64061},{\"end\":64435,\"start\":64377},{\"end\":64956,\"start\":64863},{\"end\":65495,\"start\":65419},{\"end\":65901,\"start\":65843},{\"end\":66289,\"start\":66226},{\"end\":66827,\"start\":66744},{\"end\":46810,\"start\":46764},{\"end\":47302,\"start\":47265},{\"end\":47845,\"start\":47767},{\"end\":48228,\"start\":48221},{\"end\":48609,\"start\":48563},{\"end\":49081,\"start\":49020},{\"end\":49868,\"start\":49822},{\"end\":50435,\"start\":50344},{\"end\":50934,\"start\":50888},{\"end\":51457,\"start\":51366},{\"end\":51933,\"start\":51896},{\"end\":52328,\"start\":52276},{\"end\":52701,\"start\":52649},{\"end\":53148,\"start\":53070},{\"end\":53813,\"start\":53769},{\"end\":54768,\"start\":54716},{\"end\":55347,\"start\":55269},{\"end\":55936,\"start\":55878},{\"end\":57023,\"start\":56976},{\"end\":57491,\"start\":57434},{\"end\":57922,\"start\":57885},{\"end\":58663,\"start\":58617},{\"end\":59466,\"start\":59429},{\"end\":60006,\"start\":59949},{\"end\":60506,\"start\":60454},{\"end\":60944,\"start\":60900},{\"end\":61458,\"start\":61367},{\"end\":62003,\"start\":61912},{\"end\":62481,\"start\":62435},{\"end\":63188,\"start\":63136},{\"end\":63702,\"start\":63624},{\"end\":64480,\"start\":64437},{\"end\":65036,\"start\":64958},{\"end\":65558,\"start\":65497},{\"end\":65946,\"start\":65903},{\"end\":66339,\"start\":66291},{\"end\":66897,\"start\":66829}]"}}}, "year": 2023, "month": 12, "day": 17}