{"id": 8945673, "updated": "2023-04-05 16:10:05.569", "metadata": {"title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases", "authors": "[{\"first\":\"Xiaosong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Le\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Zhiyong\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Mohammadhadi\",\"last\":\"Bagheri\",\"middle\":[]},{\"first\":\"Ronald\",\"last\":\"Summers\",\"middle\":[\"M.\"]}]", "venue": "IEEE CVPR 2017, pp. 2097-2106 (2017)", "journal": null, "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely\"ChestX-ray8\", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based\"reading chest X-rays\"(i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1705.02315", "mag": "2977613223", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "books/sp/19/WangPLLBS19", "doi": "10.1109/cvpr.2017.369"}}, "content": {"source": {"pdf_hash": "bcec3cbf9218c6f04a322392188f3a2bac3b637d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1705.02315v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1705.02315", "status": "GREEN"}}, "grobid": {"id": "b9cff939e283c1873e5a67feaced81b3261253b9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bcec3cbf9218c6f04a322392188f3a2bac3b637d.txt", "contents": "\nChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\n\n\nXiaosong Wang xiaosong.wang@nih.gov \nDepartment of Radiology and Imaging Sciences\nClinical Center\n\n\nYifan Peng yifan.peng@nih.gov \nNational Center for Biotechnology Information\nNational Library of Medicine\nNational Institutes of Health\n20892BethesdaMD\n\nLe Lu le.lu@nih.gov \nDepartment of Radiology and Imaging Sciences\nClinical Center\n\n\nZhiyong Lu \nNational Center for Biotechnology Information\nNational Library of Medicine\nNational Institutes of Health\n20892BethesdaMD\n\nMohammadhadi Bagheri mohammad.bagheri@nih.gov \nDepartment of Radiology and Imaging Sciences\nClinical Center\n\n\nRonald M Summers \nDepartment of Radiology and Imaging Sciences\nClinical Center\n\n\nChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\n\nThe chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\", which comprises 108,948 frontalview X-ray images of 32,717 unique patients with the textmined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weaklysupervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based \"reading chest X-rays\" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.\n\nIntroduction\n\nThe rapid and tremendous progress has been evidenced in a range of computer vision problems via deep learning and large-scale annotated image datasets [26,38,13,28]. Drastically improved quantitative performances in object recognition, detection and segmentation are demonstrated in Figure 1. Eight common thoracic diseases observed in chest X-rays that validate a challenging task of fully-automated diagnosis.\n\ncomparison to previous shallow methodologies built upon hand-crafted image features. Deep neural network representations further make the joint language and vision learning tasks more feasible to solve, in image captioning [49,24,33,48,23], visual question answering [2,46,51,55] and knowledge-guided transfer learning [4,34], and so on. However, the intriguing and strongly observable performance gaps of the current state-of-the-art object detection and segmentation methods, evaluated between using PASCAL VOC [13] and employing Microsoft (MS) COCO [28], demonstrate that there is still significant room for performance improvement when underlying challenges (represented by different datasets) become greater. For example, MS COCO is composed of 80 object categories from 200k images, with 1.2M instances (350k are people) where every instance is segmented and many instances are small objects. Comparing to PASCAL VOC of only 20 classes and 11,530 images containing 27,450 annotated objects with bounding-boxes (BBox), the top competing object detection approaches achieve in 0.413 in MS COCO versus 0.884 in PASCAL VOC under mean Average Precision (mAP).\n\nDeep learning yields similar rises in performance in the medical image analysis domain for object (often human anatomical or pathological structures in radiology imaging) detection and segmentation tasks. Recent notable work includes (but do not limit to) an overview review on the future promise of deep learning [14] and a collection of important medical applications on lymph node and interstitial lung disease detection and classification [37,43]; cerebral microbleed detection [11]; pulmonary nodule detection in CT images [40]; automated pancreas segmentation [36]; cell image segmentation and tracking [35], predicting spinal radiological scores [21] and extensions of multi-modal imaging segmentation [30,16]. The main limitation is that all proposed methods are evaluated on some small-to-middle scale problems of (at most) several hundred patients. It remains unclear how well the current deep learning techniques will scale up to tens of thousands of patient studies.\n\nIn the era of deep learning in computer vision, research efforts on building various annotated image datasets [38,13,28,2,33,55,23,25] with different characteristics play indispensably important roles on the better definition of the forthcoming problems, challenges and subsequently possible technological progresses. Particularly, here we focus on the relationship and joint learning of image (chest Xrays) and text (X-ray reports). The previous representative image caption generation work [49,24] utilize Flickr8K, Flickr30K [53] and MS COCO [28] datasets that hold 8,000, 31,000 and 123,000 images respectively and every image is annotated by five sentences via Amazon Mechanical Turk (AMT). The text generally describes annotator's attention of objects and activity occurring on an image in a straightforward manner. Region-level ImageNet pre-trained convolutional neural networks (CNN) based detectors are used to parse an input image and output a list of attributes or \"visually-grounded high-level concepts\" (including objects, actions, scenes and so on) in [24,51]. Visual question answering (VQA) requires more detailed parsing and complex reasoning on the image contents to answer the paired natural language questions. A new dataset containing 250k natural images, 760k questions and 10M text answers [2] is provided to address this new challenge. Additionally, databases such as \"Flickr30k Entities\" [33], \"Visual7W\" [55] and \"Visual Genome\" [25,23] (as detailed as 94,000 images and 4,100,000 region-grounded captions) are introduced to construct and learn the spatially-dense and increasingly difficult semantic links between textual descriptions and image regions through the object-level grounding.\n\nThough one could argue that the high-level analogy exists between image caption generation, visual question answering and imaging based disease diagnosis [42,41], there are three factors making truly large-scale medical image based diagnosis (e.g., involving tens of thousands of patients) tremendously more formidable. 1, Generic, openended image-level anatomy and pathology labels cannot be obtained through crowd-sourcing, such as AMT, which is prohibitively implausible for non-medically trained annota-tors. Therefore we exploit to mine the per-image (possibly multiple) common thoracic pathology labels from the image-attached chest X-ray radiological reports using Natural Language Processing (NLP) techniques. Radiologists tend to write more abstract and complex logical reasoning sentences than the plain describing texts in [53,28]. 2, The spatial dimensions of an chest X-ray are usually 2000\u00d73000 pixels. Local pathological image regions can show hugely varying sizes or extents but often very small comparing to the full image scale. Fig. 1 shows eight illustrative examples and the actual pathological findings are often significantly smaller (thus harder to detect). Fully dense annotation of region-level bounding boxes (for grounding the pathological findings) would normally be needed in computer vision datasets [33,55,25] but may be completely nonviable for the time being. Consequently, we formulate and verify a weakly-supervised multi-label image classification and disease localization framework to address this difficulty. 3, So far, all image captioning and VQA techniques in computer vision strongly depend on the ImageNet pre-trained deep CNN models which already perform very well in a large number of object classes and serves a good baseline for further model fine-tuning. However, this situation does not apply to the medical image diagnosis domain. Thus we have to learn the deep image recognition and localization models while constructing the weakly-labeled medical image database.\n\nTo tackle these issues, we propose a new chest X-ray database, namely \"ChestX-ray8\", which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined eight common disease labels, mined from the text radiological reports via NLP techniques. In particular, we demonstrate that these commonly occurred thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization formulation. Our initial quantitative results are promising. However developing fully-automated deep learning based \"reading chest X-rays\" systems is still an arduous journey to be exploited. Details of accessing the ChestX-ray8 dataset can be found via the website 1 .\n\n\nRelated Work\n\nThere have been recent efforts on creating openly available annotated medical image databases [50,52,37,36] with the studied patient numbers ranging from a few hundreds to two thousands. Particularly for chest X-rays, the largest public dataset is OpenI [1] that contains 3,955 radiology reports from the Indiana Network for Patient Care and 7,470 associated chest x-rays from the hospitals picture archiving and communication system (PACS). This database is utilized in [42] as a problem of caption generation but no quantitative disease detection results are reported. Our newly proposed chest X-ray database is at least one order of magnitude larger than OpenI [1] (Refer to Table 1). To achieve the better clinical relevance, we focus to exploit the quantitative performance on weakly-supervised multilabel image classification and disease localization of common thoracic diseases, in analogy to the intermediate step of \"detecting attributes\" in [51] or \"visual grounding\" for [33,55,23].\n\n2 Construction of Hospital-scale Chest X-ray Database\n\nIn this section, we describe the approach for building a hospital-scale chest X-ray image database, namely \"ChestX-ray8\", mined from our institute's PACS system. First, we short-list eight common thoracic pathology keywords that are frequently observed and diagnosed, i.e., Atelectasis, Cardiomegaly, Effusion, Infiltration, Mass, Nodule, Pneumonia and Pneumathorax ( Fig. 1), based on radiologists' feedback. Given those 8 text keywords, we search the PACS system to pull out all the related radiological reports (together with images) as our target corpus. A variety of Natural Language Processing (NLP) techniques are adopted for detecting the pathology keywords and removal of negation and uncertainty. Each radiological report will be either linked with one or more keywords or marked with 'Normal' as the background category. As a result, the ChestX-ray8 database is composed of 108,948 frontal-view X-ray images (from 32,717 patients) and each image is labeled with one or multiple pathology keywords or \"Normal\" otherwise. Fig. 2 illustrates the correlation of the resulted keywords. It reveals some connections between different pathologies, which agree with radiologists' domain knowledge, e.g., Infiltration is often associated with Atelectasis and Effusion. To some extend, this is similar with understanding the interactions and relationships among objects or concepts in natural images [25].\n\n\nLabeling Disease Names by Text Mining\n\nOverall, our approach produces labels using the reports in two passes. In the first iteration, we detected all the disease concept in the corpus. The main body of each chest X-ray report is generally structured as \"Comparison\", \"Indication\", \"Findings\", and \"Impression\" sections. Here, we focus on detecting disease concepts in the Findings and Impression sections. If a report contains neither of these two sections, the full-length report will then be considered. In the second pass, we code the reports as \"Normal\" if they do not contain any diseases (not limited to 8 predefined pathologies).\n\nPathology Detection: We mine the radiology reports for disease concepts using two tools, DNorm [27] and MetaMap [3]. DNorm is a machine learning method for disease recognition and normalization. It maps every mention of keywords in a report to a unique concept ID in the MetaMap is another prominent tool to detect bioconcepts from the biomedical text corpus. Different from DNorm, it is an ontology-based approach for the detection of Unified Medical Language System R (UMLS R ) Metathesaurus. In this work, we only consider the semantic types of Diseases or Syndromes and Findings (namely 'dsyn' and 'fndg' respectively). To maximize the recall of our automatic disease detection, we merge the results of DNorm and MetaMap. Table 1 (in the supplementary material) shows the corresponding SNOMED-CT concepts that are relevant to the eight target diseases (these mappings are developed by searching the disease names in the UMLS R terminology service 2 , and verified by a boardcertified radiologist.\n\nNegation and Uncertainty: The disease detection algorithm locates every keyword mentioned in the radiology report no matter if it is truly present or negated. To eliminate the noisy labeling, we need to rule out those negated pathological statements and, more importantly, uncertain mentions of findings and diseases, e.g., \"suggesting obstructive lung disease\".\n\nAlthough many text processing systems (such as [6]) can handle the negation/uncertainty detection problem, most of them exploit regular expressions on the text directly. One of the disadvantages to use regular expressions for nega-  tion/uncertainty detection is that they cannot capture various syntactic constructions for multiple subjects. For example, in the phrase of \"clear of A and B\", the regular expression can capture \"A\" as a negation but not \"B\", particularly when both \"A\" and \"B\" are long and complex noun phrases (\"clear of focal airspace disease, pneumothorax, or pleural effusion\" in Fig. 3).\n\nTo overcome this complication, we hand-craft a number of novel rules of negation/uncertainty defined on the syntactic level in this work. More specifically, we utilize the syntactic dependency information because it is close to the semantic relationship between words and thus has become prevalent in biomedical text processing. We defined our rules on the dependency graph, by utilizing the dependency label and direction information between words.\n\nAs the first step of preprocessing, we split and tokenize the reports into sentences using NLTK [5]. Next we parse each sentence by the Bllip parser [7] using David Mc-Closkys biomedical model [29]. The syntactic dependencies are then obtained from \"CCProcessed\" dependencies output by applying Stanford dependencies converter [8] on the parse tree. The \"CCProcessed\" representation propagates conjunct dependencies thus simplifies coordinations. As a result, we can use fewer rules to match more complex constructions. For an example as shown in Fig. 3, we could use \"clear \u2192 prep of \u2192 DISEASE\" to detect three negations from the text neg, focal airspace disease , neg, pneumothorax , and neg, pleural effusion .\n\nFurthermore, we label a radiology report as \"normal\" if it meets one of the following criteria:\n\n\u2022 If there is no disease detected in the report. Note that here we not only consider 8 diseases of interest in this paper, but all diseases detected in the reports.\n\n\u2022 If the report contains text-mined concepts of \"normal\" or \"normal size\" (CUIs C0205307 and C0332506 in the SNOMED-CT concepts respectively).\n\n\nQuality Control on Disease Labeling\n\nTo validate our method, we perform the following experiments. Given the fact that no gold-standard labels exist for our dataset, we resort to some existing annotated corpora as an alternative. Using the OpenI API [1], we retrieve a total of 3,851 unique radiology reports where each OpenI report is assigned with its key findings/disease names by human annotators [9]. Given our focus on the eight diseases, a subset of OpenI reports and their human annotations are used as the gold standard for evaluating our method. Table 1 summarizes the statistics of the subset of OpenI [1,20] reports. Table 2 shows the results of our method using OpenI, measured in precision (P), recall (R), and F1-score. Higher precision of 0.90, higher recall of 0.91, and higher F1-score of 0.90 are achieved compared to the existing MetaMap approach (with NegEx enabled). For all diseases, our method obtains higher precisions, particularly in \"pneumothorax\" (0.90 vs. 0.32) and \"infiltration\" (0.74 vs. 0.25). This indicates that the usage of negation and uncertainty detection on syntactic level successfully removes false positive cases. More importantly, the higher precisions meet our expectation to generate a Chest X-ray corpus with accurate semantic labels, to lay a solid foundation for the later processes.\n\n\nProcessing Chest X-ray Images\n\nComparing to the popular ImageNet classification problem, significantly smaller spatial extents of many diseases inside the typical X-ray image dimensions of 3000 \u00d7 2000 pixels impose challenges in both the capacity of computing hardware and the design of deep learning paradigm. In ChestX-ray8, X-rays images are directly extracted from the DICOM file and resized as 1024\u00d71024 bitmap images without significantly losing the detail contents, compared with image sizes of 512 \u00d7 512 in OpenI dataset. Their intensity ranges are rescaled using the default window settings stored in the DICOM header files.\n\n\nBounding Box for Pathologies\n\nAs part of the ChestX-ray8 database, a small number of images with pathology are provided with hand labeled bounding boxes (B-Boxes), which can be used as the ground truth to evaluate the disease localization performance. Furthermore, it could also be adopted for one/low-shot learning setup [15], in which only one or several samples are needed to initialize the learning and the system will then evolve by itself with more unlabeled data. We leave this as future work.\n\nIn our labeling process, we first select 200 instances for each pathology (1,600 instances total), consisting of 983 images. Given an image and a disease keyword, a boardcertified radiologist identified only the corresponding disease instance in the image and labeled it with a B-Box. The B-Box is then outputted as an XML file. If one image contains multiple disease instances, each disease instance is labeled separately and stored into individual XML files. As an application of the proposed ChestX-ray8 database and benchmarking, we will demonstrate the detection and localization of thoracic diseases in the following.\n\n\nCommon Thoracic Disease Detection and Localization\n\nReading and diagnosing Chest X-ray images may be an entry-level task for radiologists but, in fact it is a complex reasoning problem which often requires careful observation and good knowledge of anatomical principles, physiology and pathology. Such factors increase the difficulty of developing a consistent and automated technique for reading chest X-ray images while simultaneously considering all common thoracic diseases.\n\nAs the main application of ChestX-ray8 dataset, we present a unified weakly-supervised multi-label image classification and pathology localization framework, which can detect the presence of multiple pathologies and subsequently generate bounding boxes around the corresponding pathologies. In details, we tailor Deep Convolutional Neural Network (DCNN) architectures for weakly-supervised object localization, by considering large image capacity, various multi-label CNN losses and different pooling strategies.\n\n\nUnified DCNN Framework\n\nOur goal is to first detect if one or multiple pathologies are presented in each X-ray image and later we can locate them using the activation and weights extracted from the network. We tackle this problem by training a multilabel DCNN classification model. Fig. 4 illustrates the DCNN architecture we adapted, with similarity to several previous weakly-supervised object localization methods [31,54,12,19]. As shown in Fig. 4, we perform the network surgery on the pre-trained models (using Im-ageNet [10,39]), e.g., AlexNet [26], GoogLeNet [45], VGGNet-16 [44] and ResNet-50 [17], by leaving out the fully-connected layers and the final classification layers. Instead we insert a transition layer, a global pooling layer, a prediction layer and a loss layer in the end (after the last convolutional layer). In a similar fashion as described in [54], a combination of deep activations from transition layer (a set of spatial image features) and the weights of prediction inner-product layer (trained feature weighting) can enable us to find the plausible spatial locations of diseases.\n\nMulti-label Setup: There are several options of imagelabel representation and the choices of multi-label classification loss functions. Here, we define a 8-dimensional label vector y = [y 1 , ..., y c , ..., y C ], y c \u2208 {0, 1}, C = 8 for each image. y c indicates the presence with respect to according pathology in the image while a all-zero vector [0, 0, 0, 0, 0, 0, 0, 0] represents the status of \"Normal\" (no pathology is found in the scope of any of 8 disease categories as listed). This definition transits the multi-label classification problem into a regression-like loss setting.\n\nTransition Layer: Due to the large variety of pre-trained DCNN architectures we adopt, a transition layer is usually required to transform the activations from previous layers into a uniform dimension of output, S \u00d7 S \u00d7 D, S \u2208 {8, 16, 32}. D represents the dimension of features at spatial location (i, j), i, j \u2208 {1, ..., S}, which can be varied in different model settings, e.g., D = 1024 for GoogLeNet and D = 2048 for ResNet. The transition layer helps pass down the weights from pre-trained DCNN models in a standard form, which is critical for using this layers' activations to further generate the heatmap in pathology localization step.\n\nMulti-label Classification Loss Layer: We first experiment 3 standard loss functions for the regression task instead of using the softmax loss for traditional multi-class classification model, i.e., Hinge Loss (HL), Euclidean Loss (EL) and Cross Entropy Loss (CEL). However, we find that the model has difficulty learning positive instances (images with pathologies) and the image labels are rather sparse, meaning there are extensively more '0's than '1's. This is due to our one-hot-like image labeling strategy and the unbalanced numbers of pathology and \"Normal\" classes. Therefore, we introduce the positive/negative balancing factor \u03b2 P , \u03b2 N to enforce the learning of positive examples. For example, the weighted CEL (W-CEL) is defined as follows,\nL W -CEL (f ( x), y) = \u03b2 P yc=1 \u2212 ln(f (x c )) + \u03b2 N yc=0 \u2212 ln(1 \u2212 f (x c )),(1)\nwhere \u03b2 P is set to |P |+|N | |P | while \u03b2 N is set to |P |+|N | |N | . |P | and |N | are the total number of '1's and '0's in a batch of image labels. \n\n\nWeakly-Supervised Pathology Localization\n\nGlobal Pooling Layer and Prediction Layer: In our multi-label image classification network, the global pooling and the predication layer are designed not only to be part of the DCNN for classification but also to generate the likelihood map of pathologies, namely a heatmap. The location with a peak in the heatmap generally corresponds to the presence of disease pattern with a high probability. The upper part of Fig. 4 demonstrates the process of producing this heatmap. By performing a global pooling after the transition layer, the weights learned in the prediction layer can function as the weights of spatial maps from the transition layer. Therefore, we can produce weighted spatial activation maps for each disease class (with a size of S \u00d7 S \u00d7 C) by multiplying the activation from transition layer (with a size of S \u00d7 S \u00d7 D) and the weights of prediction layer (with a size of D \u00d7 C).\n\nThe pooling layer plays an important role that chooses what information to be passed down. Besides the conventional max pooling and average pooling, we also utilize the Log-Sum-Exp (LSE) pooling proposed in [32]. The LSE pooled value x p is defined as\nx p = 1 r \u00b7 log \uf8ee \uf8f0 1 S \u00b7 (i,j)\u2208S exp(r \u00b7 x ij ) \uf8f9 \uf8fb ,(2)\nwhere x ij is the activation value at (i, j), (i, j) is one location in the pooling region S, and S = s \u00d7 s is the total number of locations in S. By controlling the hyper-parameter r, the pooled value ranges from the maximum in S (when r \u2192 \u221e) to average (r \u2192 0). It serves as an adjustable option between max pooling and average pooling. Since the LSE function suffers from overflow/underflow problems, the following equivalent is used while implementing the LSE pooling layer in our own DCNN architecture,\nx p = x * + 1 r \u00b7 log \uf8ee \uf8f0 1 S \u00b7 (i,j)\u2208S exp(r \u00b7 (x ij \u2212 x * ) \uf8f9 \uf8fb ,(3)\nwhere x * = max{|x ij |, (i, j) \u2208 S}. Bounding Box Generation: The heatmap produced from our multi-label classification framework indicates the approximate spatial location of one particular thoracic disease class each time. Due to the simplicity of intensity distributions in these resulting heatmaps, applying an ad-hoc thresholding based B-Box generation method for this task is found to be sufficient. The intensities in heatmaps are first normalized to [0, 255] and then thresholded by {60, 180} individually. Finally, B-Boxes are generated to cover the isolated regions in the resulting binary maps.\n\n\nExperiments\n\nData: We evaluate and validate the unified disease classification and localization framework using the proposed ChestX-ray8 database. In total, 108,948 frontal-view X-ray images are in the database, of which 24,636 images contain one or more pathologies. The remaining 84,312 images are normal cases. For the pathology classification and localization task, we randomly shuffled the entire dataset into three subgroups for CNN fine-tuning via Stochastic Gradient Descent (SGD): i.e. training (70%), validation (10%) and testing (20%). We only report the 8 thoracic disease recognition performance on the testing set in our experiments. Furthermore, for the 983 images with 1,600 annotated B-Boxes of pathologies, these boxes are only used as the ground truth to evaluate the disease localization accuracy in testing (not for training purpose).\n\nCNN Setting: Our multi-label CNN architecture is implemented using Caffe framework [22]. The ImageNet pre-trained models, i.e., AlexNet [26], GoogLeNet [45], VGGNet-16 [44] and ResNet-50 [17] are obtained from the Caffe model zoo. Our unified DCNN takes the weights from those models and only the transition layers and prediction layers are trained from scratch.\n\nDue to the large image size and the limit of GPU memory, it is necessary to reduce the image batch size to load the entire model and keep activations in GPU while we increase the iter size to accumulate the gradients for more iterations. The combination of both may vary in different CNN models but we set batch size \u00d7 iter size = 80 as a constant. Furthermore, the total training iterations are customized for different CNN models to prevent over-fitting. More complex models like ResNet-50 actually take less iterations (e.g., 10000 iterations) to reach the convergence. The DCNN models are trained using a Dev-Box linux server with 4 Titan X GPUs.\n\nMulti-label Disease Classification: Fig. 5 demonstrates the multi-label classification ROC curves on 8 pathology classes by initializing the DCNN framework with 4 different pre-trained models of AlexNet, GoogLeNet, VGG and ResNet-50. The corresponding Area-Under-Curve (AUC) values are given in Table 4. The quantitative performance varies greatly, in which the model based on ResNet-50 achieves the best results. The \"Cardiomegaly\" (AUC=0.8141) and \"Pneumothorax\" (AUC=0.7891) classes are consistently well-recognized compared to other groups while the detection ratios can be relatively lower for pathologies which contain small objects, e.g., \"Mass\" (AUC=0.5609) and \"Nodule\" classes. Mass is difficult to detect due to its huge within-class appearance variation. The lower performance on \"Pneumonia\" (AUC=0.6333) is probably because of lack of total instances in our patient population (less than 1% X-rays labeled as Pneumonia). This finding is consistent with the comparison on object detection performance, degrading from PASCAL VOC [13] to MS COCO [28] where many small annotated objects appear.\n\nNext, we examine the influence of different pooling strategies when using ResNet-50 to initialize the DCNN framework. As discussed above, three types of pooling schemes are experimented: average looping, LSE pooling and max pooling.  6, average pooling and max pooling achieve approximately equivalent performance in this classification task. The performance of LSE pooling start declining first when r starts increasing and reach the bottom when r = 5. Then it reaches the overall best performance around r = 10. LSE pooling behaves like a weighed pooling method or a transition scheme between average and max pooling under different r values. Overall, LSE pooling (r = 10) reports the best performance (consistently higher than mean and max pooling). Last, we demonstrate the performance improvement by using the positive/negative instances balanced loss functions (Eq. 1). As shown in Table 4, the weighted loss (W-CEL) provides better overall performance than CEL, especially for those classes with relative fewer positive instances, e.g. AUC for \"Cardiomegaly\" is increased from 0.7262 to 0.8141 and from 0.5164 to 0.6333 for \"Pneumonia\".\n\nDisease Localization: Leveraging the fine-tuned DCNN models for multi-label disease classification, we can calculate the disease heatmaps using the activations of the transition layer and the weights from the prediction layer, and even generate the B-Boxes for each pathology candidate. The computed bounding boxes are evaluated against the hand annotated ground truth (GT) boxes (included in ChestX-ray8). Although the total number of B-Box annotations (1,600 instances) is relatively small compared to the entire dataset, it may be still sufficient to get a reasonable estimate on how the proposed framework performs on the weakly-supervised disease localization task. To examine the accuracy of computerized B-Boxes versus the GT B-Boxes, two types of measurement are used, i.e, the standard Intersection over Union ratio (IoU) or the Intersection over the detected B-Box area ratio (IoBB) (similar to Area of Precision or Purity). Due to the relatively low spatial resolution of heatmaps (32 \u00d7 32) in contrast to the original image dimensions (1024 \u00d7 1024), the computed B-Boxes are often larger than the according GT B-Boxes. Therefore, we define a correct localization by requiring either IoU > T (IoU ) or IoBB > T (IoBB). Refer to the supplementary material for localization performance under varying T (IoU ). Table 4 illustrates the localization accuracy (Acc.) and Average False Positive (AFP) number for each disease type, with T (IoBB) \u2208 {0.1, 0.25, 0.5, 0.75, 0.9}. Please refer to the supplementary material for qualitative exemplary disease localization results for each of 8 pathology classes.\n\n\nConclusion\n\nConstructing hospital-scale radiology image databases with computerized diagnostic performance benchmarks has not been addressed until this work. We attempt to build a \"machine-human annotated\" comprehensive chest X-ray database that presents the realistic clinical and methodological challenges of handling at least tens of thousands of patients (somewhat similar to \"ImageNet\" in natural images). We also conduct extensive quantitative performance benchmarking on eight common thoracic pathology classification and weakly-supervised localization using ChestX-ray8 database. The main goal is to initiate future efforts by promoting public datasets in this important domain. Building truly large-scale, fully-automated high precision medical diagnosis systems remains a strenuous task. ChestX-ray8 can enable the data-hungry deep neural network paradigms to create clinically meaningful applications, including common disease pattern mining, disease correlation analysis, automated radiological report generation, etc. For future work, ChestX-ray8 will be extended to cover more disease classes and integrated with other clinical information, e.g., followup studies across time and patient history. To overcome this complication, we hand-craft a number of novel rules of negation/uncertainty defined on the syntactic level in this work. More specifically, we utilize the syntactic dependency information because it is close to the semantic relationship between words and thus has become prevalent in biomedical text processing. We defined our rules on the dependency graph, by utilizing the dependency label and direction information between words. Table 6 shows the rules we defined for negation/uncertainty detection on the syntactic level. Table 7 illustrates the localization accuracy (Acc.) and Average False Positive (AFP) number for each disease type, with IoU > T (IoU ) only and T (IoU ) \u2208 {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7}. Table 8 to Table 15 illustrate localization results from each of 8 disease classes together with associated report and mined disease keywords. The heatmaps overlay on the original images are shown on the right. Correct bounding boxes (in green), false positives (in red) and the groundtruth (in blue) are plotted over the original image on the left.\n\n\nA.3 More Disease Localization Results\n\nIn order to quantitatively demonstrate how informative those heatmaps are, a simple two-level thresholding based bounding box generator is adopted here to catch the peaks in the heatmap and later generated bounding boxes can be evaluated against the ground truth. Each heatmap will approximately results in 1-3 bounding boxes. We believe the localization accuracy and AFP (shown in Table 7) could be further optimized by adopting a more sophisticated bounding box generation method, e.g. selective search [47] or Edgebox [18]. Nevertheless, we reserve the effort to do so, since our main goal is not to compute the exact spatial location of disease patterns but just to obtain some instructive location information for future applications, e.g. automated radiological report generation. Take the case shown in Table  8 for an example. The peak at the lower part of the left lung region indicates the presence of \"atelectasis\", which confer the statement of \"...stable abnormal study including left basilar infilrate/atelectasis, ...\" presented in the impression section of the associated radiological report. By combining with other information, e.g. a lung region mask, the heatmap itself is already more informative than just the presence indication of certain disease in an image as introduced in the previous works, e.g. [42]. The aorta is tortuous, and cannot exclude ascending aortic aneurysm concern \u2192 prep for \u2192 * There is raises concern for pneumonia could be/may be/... which could be due to nodule/lymph node difficult \u2192 prep to \u2192 exclude interstitial infiltrates difficult to exclude may \u2190 md \u2190 represent which may represent pleural reaction or small pulmonary nodules suggesting/suspect/... \u2192 dobj \u2192 DISEASE Bilateral pulmonary nodules suggesting pulmonary metastases  Table 8. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Atelectasis\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nRule\n\n\nRadiology report\n\nKeyword Localization Result findings include: 1. cardiomegaly (ct ratio of 17/30). 2. otherwise normal lungs and mediastinal contours. 3. no evidence of focal bone lesion. dictating Cardiomegaly Table 9. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Cardiomegaly\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nRadiology report\n\nKeyword Localization Result findings: no appreciable change since XX/XX/XX. small right pleural effusion. elevation right hemidiaphragm. diffuse small nodules throughout the lungs, most numerous in the left mid and lower lung. impression: no change with bilateral small lung metastases.\n\nEffusion; Nodule Table 10. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Effusion\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nKeyword\n\nLocalization Result findings: port-a-cath reservoir remains in place on the right. chest tube remains in place, tip in the left apex. no pneumothorax. diffuse patchy infiltrates bilaterally are decreasing. impression: infiltrates and effusions decreasing. Table 11. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Infiltration\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nInfiltration\n\n\nRadiology report\n\nKeyword Localization Result findings:\n\nright internal jugular catheter remains in place. large metastatic lung mass in the lateral left upper lobe is again noted. no infiltrate or effusion. extensive surgical clips again noted left axilla. impression: no significant change. Table 12. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Mass\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nMass\n\n\nRadiology report\n\nKeyword Localization Result findings: pa and lateral views of the chest demonstrate stable 2.2 cm nodule in left lower lung field posteriorly. the lungs are clear without infiltrate or effusion. cardiomediastinal silhouette is normal size and contour. pulmonary vascularity is normal in caliber and distribution. impression: stable left likely hamartoma. Table 13. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Nodule\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nNodule; Infiltration\n\n\nKeyword\n\nLocalization Result findings: unchanged left lower lung field infiltrate/air bronchograms. unchanged right perihilar infiltrate with obscuration of the right heart border. no evidence of new infiltrate. no evidence of pneumothorax the cardiac and mediastinal contours are stable. impression: 1. no evidence pneumothorax. 2. unchanged left lower lobe and left lingular consolidation/bronchiectasis. 3. unchanged right middle lobe infiltrate Pneumonia; Infiltration Table 14. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Pneumonia\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nRadiology report\n\nKeyword Localization Result findings: frontal lateral chest x-ray performed in expiration. left apical pneumothorax visible. small pneumothorax visible along the left heart border and left hemidiaphragm. pleural thickening, mass right chest. the mediastinum cannot be evaluated in the expiration. bony structures intact. impression: left post biopsy pneumothorax. Table 15. A sample of chest x-ray radiology report, mined disease keywords and localization result from the \"Pneumothorax\" Class. Correct bounding box (in green), false positives (in red) and the ground truth (in blue) are plotted over the original image.\n\n\nMass; Pneumothorax\n\n\nB ChestX-ray14 Dataset\n\nAfter the CVPR submission, we expand the disease categories to include 6 more common thorax diseases (i.e. Consolidation, Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia) and update the NLP mined labels. The statistics of ChestX-ray14 dataset are illustrated in Table 16 and Figure 8 \n\n\nB.1 Evaluation of NLP Mined Labels\n\nTo validate our method, we perform the following experiments. First, we resort to some existing annotated corpora as an alternative, i.e. OpenI dataset. Furthermore, we annotated clinical reports suitable for evaluating finding recognition systems. We randomly selected 900 reports and asked two annotators to mark the above 14 types of findings. Each report was annotated by two annotators independently and then agreements are reached for conflicts. Table 18 shows the results of our method using OpenI and our proposed dataset, measured in precision (P), recall (R), and F1-score. Much higher precision, recall and F1scores are achieved compared to the existing MetaMap approach (with NegEx enabled). This indicates that the usage of negation and uncertainty detection on syntactic level successfully removes false positive cases.\n\n\nB.2 Benchmark Results\n\nIn a similar fashion to the experiment on ChestX-ray8, we evaluate and validate the unified disease classification and localization framework on ChestX-ray14 database. In total, 112,120 frontal-view X-ray images are used, of which 51,708 images contain one or more pathologies. The remaining 84,312 images do not have any pathological findings. For the pathology classification and localization task,\n\n\nResNet-50\n\nChestX-ray8 ChestX-ray14  we randomly shuffled the entire dataset into three subgroups for CNN fine-tuning via Stochastic Gradient Descent (SGD): i.e. training (70%), validation (10%) and testing (20%). We report the 14 thoracic disease recognition performance on the testing set in our experiments in comparison with the counterpart based on ChestX-ray8, shown in Table  17 and Figure 7.\n\nSince the annotated B-Boxes of pathologies are unchanged, we only test the localization performance on the original 8 categories. Results measured by the Intersection over the detected B-Box area ratio (IoBB) (similar to Area of Precision or Purity) are demonstrated in Table 19.\n\nOverall, both of the classification and localization performance on ChestX-ray14 is equivalent to the counterpart on ChestX-ray8. For those categories with more accurate labels, the performance is further improved, e.g AUC is increased from 0.5609 to 0.7057 for 'Mass'.  Table 19. Pathology localization accuracy and average false positive number for ChestX-ray14. \n\nFigure 2 .\n2The circular diagram shows the proportions of images with multi-labels in each of 8 pathology classes and the labels' co-occurrence statistics.Systematized Nomenclature of Medicine Clinical Terms (or SNOMED-CT), which is a standardized vocabulary of clinical terminology for the electronic exchange of clinical health information.\n\nFigure 3 .\n3The dependency graph of text: \"clear of focal airspace disease, pneumothorax, or pleural effusion\".\n\nFigure 4 .\n4The overall flow-chart of our unified DCNN framework and disease localization process.\n\n\nThe hyper-parameter r in LSE pooling varies in {0.1, 0.5, 1, 5, 8, 10, 12}. As illustrated inFig.\n\nFigure 5 .\n5A comparison of multi-label classification performance with different model initializations.\n\nFigure 6 .\n6A comparison of multi-label classification performance with different pooling strategies.\n\nFigure 7 .\n7Multi-label classification performance on ChestX-ray14 with ImageNet pre-trained ResNet.\n\nFigure 8 .\n8The circular diagram shows the proportions of images with multi-labels in each of 14 pathology classes and the labels' cooccurrence statistics.\n\n\nAUCs of ROC curves for multi-label classification in different DCNN model setting. Pathology localization accuracy and average false positive number for 8 disease classes.Setting \n\nAtelectasis Cardiomegaly Effusion Infiltration \nMass \nNodule Pneumonia Pneumothorax \n\nInitialization with different pre-trained models \nAlexNet \n0.6458 \n0.6925 \n0.6642 \n0.6041 \n0.5644 0.6487 \n0.5493 \n0.7425 \nGoogLeNet \n0.6307 \n0.7056 \n0.6876 \n0.6088 \n0.5363 0.5579 \n0.5990 \n0.7824 \nVGGNet-16 \n0.6281 \n0.7084 \n0.6502 \n0.5896 \n0.5103 0.6556 \n0.5100 \n0.7516 \nResNet-50 \n0.7069 \n0.8141 \n0.7362 \n0.6128 \n0.5609 0.7164 \n0.6333 \n0.7891 \nDifferent multi-label loss functions \nCEL \n0.7064 \n0.7262 \n0.7351 \n0.6084 \n0.5530 0.6545 \n0.5164 \n0.7665 \nW-CEL \n0.7069 \n0.8141 \n0.7362 \n0.6128 \n0.5609 0.7164 \n0.6333 \n0.7891 \nTable 3. T(IoBB) \nAtelectasis Cardiomegaly Effusion Infiltration \nMass \nNodule Pneumonia Pneumothorax \n\nT(IoBB) = 0.1 \nAcc. \n0.7277 \n0.9931 \n0.7124 \n0.7886 \n0.4352 0.1645 \n0.7500 \n0.4591 \nAFP \n0.8323 \n0.3506 \n0.7998 \n0.5589 \n0.6423 0.6047 \n0.9055 \n0.4776 \nT(IoBB) = 0.25 (Two times larger on both x and y axis than ground truth B-Boxes) \nAcc. \n0.5500 \n0.9794 \n0.5424 \n0.5772 \n0.2823 0.0506 \n0.5583 \n0.3469 \nAFP \n0.9167 \n0.4553 \n0.8598 \n0.6077 \n0.6707 0.6158 \n0.9614 \n0.5000 \nT(IoBB) = 0.5 \nAcc. \n0.2833 \n0.8767 \n0.3333 \n0.4227 \n0.1411 0.0126 \n0.3833 \n0.1836 \nAFP \n1.0203 \n0.5630 \n0.9268 \n0.6585 \n0.6941 0.6189 \n1.0132 \n0.5285 \nT(IoBB) = 0.75 \nAcc. \n0.1666 \n0.7260 \n0.2418 \n0.3252 \n0.1176 0.0126 \n0.2583 \n0.1020 \nAFP \n1.0619 \n0.6616 \n0.9603 \n0.6921 \n0.7043 0.6199 \n1.0569 \n0.5396 \nT(IoBB) = 0.9 \nAcc. \n0.1333 \n0.6849 \n0.2091 \n0.2520 \n0.0588 0.0126 \n0.2416 \n0.0816 \nAFP \n1.0752 \n0.7226 \n0.9797 \n0.7124 \n0.7144 0.6199 \n1.0732 \n0.5437 \nTable 4. \n\n\nprep without \u2192 evidence \u2192 prep of \u2192 DISEASE Changes without evidence of acute infiltrate no \u2190 neg \u2190 evidence \u2192 prep of \u2192 DISEASEExample \nNegation \nno\u2190  *  \u2190 DISEASE \nNo acute pulmonary disease \n *  \u2192 prep without \u2192 DISEASE \nchanges without focal airspace disease \nclear/free/disappearance \u2192 prep of \u2192 DISEASE \nclear of focal airspace disease, pneumothorax, or pleural effusion \n *  \u2192 No evidence of active disease \nUncertainty \ncannot \u2190 md \u2190 exclude \n\n\nTable 6 .\n6Rules and corresponding examples for negation and uncertainty detection.T(IoU) \nAtelectasis Cardiomegaly Effusion Infiltration Mass Nodule Pneumonia Pneumothorax \nT(IoU) = 0.1 \nAcc. \n0.6888 \n0.9383 \n0.6601 \n0.7073 \n0.4000 0.1392 \n0.6333 \n0.3775 \nAFP \n0.8943 \n0.5996 \n0.8343 \n0.6250 \n0.6666 0.6077 \n1.0203 \n0.4949 \nT(IoU) = 0.2 \nAcc. \n0.4722 \n0.6849 \n0.4509 \n0.4796 \n0.2588 0.0506 \n0.3500 \n0.2346 \nAFP \n0.9827 \n0.7205 \n0.9096 \n0.6849 \n0.6941 0.6158 \n1.0793 \n0.5173 \nT(IoU) = 0.3 \nAcc. \n0.2444 \n0.4589 \n0.3006 \n0.2764 \n0.1529 0.0379 \n0.1666 \n0.1326 \nAFP \n1.0417 \n0.7815 \n0.9472 \n0.7236 \n0.7073 0.6168 \n1.1067 \n0.5325 \nT(IoU) = 0.4 \nAcc. \n0.0944 \n0.2808 \n0.2026 \n0.1219 \n0.0705 0.0126 \n0.0750 \n0.0714 \nAFP \n1.0783 \n0.8140 \n0.9705 \n0.7489 \n0.7164 0.6189 \n1.1239 \n0.5427 \nT(IoU) = 0.5 \nAcc. \n0.0500 \n0.1780 \n0.1111 \n0.0650 \n0.0117 0.0126 \n0.0333 \n0.0306 \nAFP \n1.0884 \n0.8354 \n0.9919 \n0.7571 \n0.7215 0.6189 \n1.1291 \n0.5478 \nT(IoU) = 0.6 \nAcc. \n0.0222 \n0.0753 \n0.0457 \n0.0243 \n0.0000 0.0126 \n0.0166 \n0.0306 \nAFP \n1.0935 \n0.8506 \n1.0051 \n0.7632 \n0.7226 0.6189 \n1.1321 \n0.5478 \nT(IoU) = 0.7 \nAcc. \n0.0055 \n0.0273 \n0.0196 \n0.0000 \n0.0000 0.0000 \n0.0083 \n0.0204 \nAFP \n1.0965 \n0.8577 \n1.009 \n0.7663 \n0.7226 0.6199 \n1.1331 \n0.5488 \n\nTable 7. Pathology localization accuracy and average false positive number for 8 disease classes with T (IoU ) ranged from 0.1 to 0.7. \n\n\n\n\n. The bounding boxes for Pathologies are unchanged at this point.Table 16. Total number (#) and # of Overlap (Ov.) of the corpus in ChestX-ray8 and ChestX-ray14 datasets.PT: Pleural ThickeningItem # \nX-ray8 \nOv. X-ray14 \nOv. \nReport \n108,948 \n-112,120 \n-\nAtelectasis \n5,789 3,286 \n11,535 \n7,323 \nCardiomegaly \n1,010 \n475 \n2,772 \n1,678 \nEffusion \n6,331 4,017 \n13,307 \n9,348 \nInfiltration \n10,317 4,698 \n19,871 10,319 \nMass \n6,046 3,432 \n5,746 \n2,138 \nNodule \n1,971 1,041 \n6,323 \n3,617 \nPneumonia \n1,062 \n703 \n1,353 \n1,046 \nPneumothorax \n2,793 1,403 \n5,298 \n3,099 \nConsolidation \n-\n-\n4,667 \n3,353 \nEdema \n-\n-\n2,303 \n1,669 \nEmphysema \n-\n-\n2,516 \n1,621 \nFibrosis \n-\n-\n1,686 \n959 \nPT \n-\n-\n3,385 \n2,258 \nHernia \n-\n-\n227 \n117 \nNo findings \n84,312 \n0 \n60,412 \n0 \n\n\n\n\nTable 17. AUCs of ROC curves for multi-label classification for ChestX-ray14. PT: Pleural ThickeningAtelectasis \n0.7069 \n0.7158 \nCardiomegaly \n0.8141 \n0.8065 \nEffusion \n0.7362 \n0.7843 \nInfiltration \n0.6128 \n0.6089 \nMass \n0.5609 \n0.7057 \nNodule \n0.7164 \n0.6706 \nPneumonia \n0.6333 \n0.6326 \nPneumothorax \n0.7891 \n0.8055 \nConsolidation \n-\n0.7078 \nEdema \n-\n0.8345 \nEmphysema \n-\n0.8149 \nFibrosis \n-\n0.7688 \nPT \n-\n0.7082 \nHernia \n-\n0.7667 \n\n\n\nTable 18 .\n18Evaluation of image labeling results on OpenI and ChestX-ray14 dataset. Performance is reported using P, R, F1-score. PT: Pleural Thickening IoBB) = 0.25 (Two times larger on both x and y axis than ground truth B-Boxes)T(IoBB) \nAtelectasis Cardiomegaly Effusion Infiltration \nMass \nNodule Pneumonia Pneumothorax \n\nT(IoBB) = 0.1 \nAcc. \n0.6722 \n1 \n0.6209 \n0.8862 \n0.6941 0.2025 \n0.7667 \n0.5306 \nAFP \n0.7297 \n0.2388 \n0.8201 \n0.5234 \n0.4299 0.5020 \n0.4797 \n0.3547 \nT(Acc. \n0.4944 \n0.9932 \n0.4444 \n0.7805 \n0.4353 \n0 \n0.6167 \n0.3469 \nAFP \n0.814 \n0.2927 \n0.8669 \n0.5691 \n0.4756 0.5213 \n0.5346 \n0.3882 \nT(IoBB) = 0.5 \nAcc. \n0.3556 \n0.9589 \n0.2745 \n0.5854 \n0.2471 \n0 \n0.4250 \n0.1837 \nAFP \n0.8831 \n0.3699 \n0.9228 \n0.6362 \n0.5051 0.5213 \n0.5935 \n0.4217 \nT(IoBB) = 0.75 \nAcc. \n0.1667 \n0.8493 \n0.1830 \n0.3739 \n0.1529 \n0 \n0.2917 \n0.1429 \nAFP \n0.9339 \n0.4756 \n0.9533 \n0.6890 \n0.5254 0.5213 \n0.6250 \n0.4299 \nT(IoBB) = 0.9 \nAcc. \n0.1388 \n0.7397 \n0.1176 \n0.2682 \n0.0941 \n0 \n0.2333 \n0.1225 \nAFP \n0.9451 \n0.5447 \n0.9685 \n0.7185 \n0.5356 0.5213 \n0.6494 \n0.4319 \n\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC, more details: https://www.cc.nih.gov/drd/summers.html\nhttps://uts.nlm.nih.gov/metathesaurus.html\nhttps://uts.nlm.nih.gov/metathesaurus.html\nAcknowledgements This work was supported by the Intramural Research Programs of the NIH Clinical Center and National Library of Medicine. We thank NVIDIA Corporation for the GPU donation.A Supplementary MaterialsA.1 SNOMED-CT ConceptsIn this work, we only consider the semantic types of Diseases or Syndromes and Findings (namely 'dsyn' and 'fndg' respectively).Table 5shows the corresponding SNOMED-CT concepts that are relevant to the target diseases (these mappings are developed by searching the disease names in the UMLS R terminology service 3 , and verified by a boardcertificated radiologist.A.2 Rules of Negation/UncertaintyAlthough many text processing systems can handle the negation/uncertainty detection problem, most of them exploit regular expressions on the text directly. One of the disadvantages to use regular expressions for negation/uncertainty detection is that they cannot capture various syntactic constructions for multiple subjects. For example, in the phrase of \"clear of A and B\", the regular expression can capture \"A\" as a negation but not \"B\", particularly when both \"A\" and \"B\" are long and complex noun phrases.\nOpen-i: An open access biomedical search engine. 34Open-i: An open access biomedical search engine. https: //openi.nlm.nih.gov. 2, 3, 4\n\nVqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, L Zitnick, ICCV. 1S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, and L. Zit- nick. Vqa: Visual question answering. In ICCV, 2015. 1, 2\n\nAn overview of MetaMap: historical perspective and recent advances. A R Aronson, F.-M Lang, Journal of the American Medical Informatics Association. 1733A. R. Aronson and F.-M. Lang. An overview of MetaMap: historical perspective and recent advances. Journal of the American Medical Informatics Association, 17(3):229-236, may 2010. 3\n\nPredicting deep zero-shot convolutional neural networks using textual descriptions. J Ba, K Swersky, S Fidler, R Salakhutdinov, ICCV. J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Predicting deep zero-shot convolutional neural networks using textual descriptions. In ICCV, 2015. 1\n\nNatural language processing with Python. S Bird, E Klein, E Loper, Reilly Media, IncS. Bird, E. Klein, and E. Loper. Natural language processing with Python. \"O'Reilly Media, Inc.\", 2009. 4\n\nA simple algorithm for identifying negated findings and diseases in discharge summaries. W W Chapman, W Bridewell, P Hanbury, G F Cooper, B G Buchanan, Journal of Biomedical Informatics. 345W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper, and B. G. Buchanan. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 34(5):301-310, oct 2001. 3\n\nCoarse-to-fine n-best parsing and MaxEnt discriminative reranking. E Charniak, M Johnson, Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL). the 43rd Annual Meeting on Association for Computational Linguistics (ACL)E. Charniak and M. Johnson. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Lin- guistics (ACL), pages 173-180, 2005. 4\n\nStanford typed dependencies manual. M.-C De Marneffe, C D Manning, Stanford UniversityM.-C. De Marneffe and C. D. Manning. Stanford typed de- pendencies manual. Stanford University, apr 2015. 4\n\nPreparing a collection of radiology examinations for distribution and retrieval. D Demner-Fushman, M D Kohli, M B Rosenman, S E Shooshan, L Rodriguez, S Antani, G R Thoma, C J Mcdonald, Journal of the American Medical Informatics Association. 232D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medi- cal Informatics Association, 23(2):304-310, July 2015. 4\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Computer Vision and Pattern Recognition. IEEEJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pages 248-255. IEEE, 2009. 5\n\nAutomatic detection of cerebral microbleeds from mr images via 3d convolutional neural networks. Q Dou, H Chen, L Yu, L Zhao, J Qin, D Wang, V Mok, L Shi, P Heng, IEEE Trans. Medical Imaging. 355Q. Dou, H. Chen, L. Yu, L. Zhao, J. Qin, D. Wang, V. Mok, L. Shi, and P. Heng. Automatic detection of cerebral microb- leeds from mr images via 3d convolutional neural networks. IEEE Trans. Medical Imaging, 35(5):1182-1195, 2016. 2\n\nWeldon: Weakly supervised learning of deep convolutional neural networks. T Durand, N Thome, M Cord, IEEE CVPR. 5T. Durand, N. Thome, and M. Cord. Weldon: Weakly super- vised learning of deep convolutional neural networks. IEEE CVPR, 2016. 5\n\nThe pascal visual object classes challenge: A retrospective. International Journal of Computer Vision. M Everingham, S M A Eslami, L J Van Gool, C Williams, J Winn, A Zisserman, 17M. Everingham, S. M. A. Eslami, L. J. Van Gool, C. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Jour- nal of Computer Vision, pages 111(1): 98-136, 2015. 1, 2, 7\n\nGuest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique. H Greenspan, B Van Ginneken, R M Summers, IEEE Trans. Medical Imaging. 355H. Greenspan, B. van Ginneken, and R. M. Summers. Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique. IEEE Trans. Medical Imaging, 35(5):1153-1159, 2016. 2\n\n. B Hariharan, R Girshick, arXiv:1606.02819Low-shot visual object recognition. arXiv preprintB. Hariharan and R. Girshick. Low-shot visual object recog- nition. arXiv preprint arXiv:1606.02819, 2016. 5\n\nHemis: Hetero-modal image segmentation. M Havaei, N Guizard, N Chapados, Y Bengio, MICCAI. SpringerM. Havaei, N. Guizard, N. Chapados, and Y. Bengio. Hemis: Hetero-modal image segmentation. In MICCAI, pages (2): 469-477. Springer, 2016. 2\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, arXiv:1512.0338557arXiv preprintK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. arXiv preprint arXiv:1512.03385, 2015. 5, 7\n\nWhat makes for effective detection proposals?. J Hosang, R Benenson, P Doll\u00e1r, B Schiele, IEEE transactions on pattern analysis and machine intelligence. 3811J. Hosang, R. Benenson, P. Doll\u00e1r, and B. Schiele. What makes for effective detection proposals? IEEE transactions on pattern analysis and machine intelligence, 38(4):814- 830, 2016. 11\n\nSelf-transfer learning for weakly supervised lesion localization. S Hwang, H.-E Kim, MICCAI. S. Hwang and H.-E. Kim. Self-transfer learning for weakly supervised lesion localization. In MICCAI, pages (2): 239- 246, 2015. 5\n\nTwo public chest x-ray datasets for computeraided screening of pulmonary diseases. S Jaeger, S Candemir, S Antani, Y.-X J Wng, P.-X Lu, G Thoma, 2014. 4Quantitative Imaging in Medicine and Surgery. 46S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wng, P.-X. Lu, and G. Thoma. Two public chest x-ray datasets for computer- aided screening of pulmonary diseases. Quantitative Imaging in Medicine and Surgery, 4(6), 2014. 4\n\nSpinenet: Automatically pinpointing classification evidence in spinal mris. A Jamaludin, T Kadir, A Zisserman, MICCAI. SpringerA. Jamaludin, T. Kadir, and A. Zisserman. Spinenet: Auto- matically pinpointing classification evidence in spinal mris. In MICCAI. Springer, 2016. 2\n\nY Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, arXiv:1408.5093Caffe: Convolutional architecture for fast feature embedding. arXiv preprintY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolu- tional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014. 7\n\nDensecap: Fully convolutional localization networks for dense captioning. J Johnson, A Karpathy, L Fei-Fei, CVPR. 13J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In CVPR, 2016. 1, 2, 3\n\nDeep visual-semantic alignments for generating image descriptions. A Karpathy, L Fei-Fei, CVPR. 1A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. 1, 2\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, M Bernstein, L Fei-Fei, 23R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016. 2, 3\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 7A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012. 1, 5, 7\n\nChallenges in clinical natural language processing for automated disorder normalization. R Leaman, R Khare, Z Lu, Journal of Biomedical Informatics. 573R. Leaman, R. Khare, and Z. Lu. Challenges in clinical nat- ural language processing for automated disorder normaliza- tion. Journal of Biomedical Informatics, 57:28-37, 2015. 3\n\n. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Dollr, L Zitnick, Microsoft coco: Common objects in context. ECCV, pagesT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Dollr, and L. Zitnick. Microsoft coco: Common objects in context. ECCV, pages (5): 740-755, 2014. 1, 2, 7\n\nAny domain parsing: automatic domain adaptation for natural language parsing. D Mcclosky, Department of Computer Science, Brown UniversityThesisD. McClosky. Any domain parsing: automatic domain adap- tation for natural language parsing. Thesis, Department of Computer Science, Brown University, 2009. 4\n\nDeep learning for multi-task medical image segmentation in multiple modalities. P Moeskops, J Wolterink, B Van Der Velden, K Gilhuijs, T Leiner, M Viergever, I Isgum, MICCAI. SpringerP. Moeskops, J. Wolterink, B. van der Velden, K. Gilhuijs, T. Leiner, M. Viergever, and I. Isgum. Deep learning for multi-task medical image segmentation in multiple modali- ties. In MICCAI. Springer, 2016. 2\n\nIs object localization for free?-weakly-supervised learning with convolutional neural networks. M Oquab, L Bottou, I Laptev, J Sivic, IEEE CVPR. M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object lo- calization for free?-weakly-supervised learning with convo- lutional neural networks. In IEEE CVPR, pages 685-694, 2015. 5\n\nFrom image-level to pixellevel labeling with convolutional networks. P O Pinheiro, R Collobert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionP. O. Pinheiro and R. Collobert. From image-level to pixel- level labeling with convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 1713-1721, 2015. 6\n\nFlickr30k entities: Collecting regionto-phrase correspondences for richer image-to-sentence models. B Plummer, L Wang, C Cervantes, J Caicedo, J Hockenmaier, S Lazebnik, ICCV. 13B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hocken- maier, and S. Lazebnik. Flickr30k entities: Collecting region- to-phrase correspondences for richer image-to-sentence mod- els. In ICCV, 2015. 1, 2, 3\n\nLess is more: zero-shot learning from online textual documents with noise suppression. R Qiao, L Liu, C Shen, A Van Den, Hengel, CVPR. R. Qiao, L. Liu, C. Shen, and A. van den Hengel. Less is more: zero-shot learning from online textual documents with noise suppression. In CVPR, 2016. 1\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MIC-CAISpringerO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In MIC- CAI, pages 234-241. Springer, 2015. 2\n\nDeeporgan: Multi-level deep convolutional networks for automated pancreas segmentation. H Roth, L Lu, A Farag, H.-C Shin, J Liu, E B Turkbey, R M Summers, MICCAI. SpringerH. Roth, L. Lu, A. Farag, H.-C. Shin, J. Liu, E. B. Turkbey, and R. M. Summers. Deeporgan: Multi-level deep convo- lutional networks for automated pancreas segmentation. In MICCAI, pages 556-564. Springer, 2015. 2\n\nA new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations. H R Roth, L Lu, A Seff, K M Cherry, J Hoffman, S Wang, J Liu, E Turkbey, R M Summers, MICCAI. SpringerH. R. Roth, L. Lu, A. Seff, K. M. Cherry, J. Hoffman, S. Wang, J. Liu, E. Turkbey, and R. M. Summers. A new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations. In MICCAI, pages 520-527. Springer, 2014. 2\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A Berg, L Fei-Fei, International Journal of Computer Vision. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and L. Fei-Fei. Imagenet large scale visual recog- nition challenge. International Journal of Computer Vision, pages 115(3): 211-252, 2015. 1, 2\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International Journal of Computer Vision. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. Interna- tional Journal of Computer Vision, 115(3):211-252, 2015. 5\n\nGinneken. Pulmonary nodule detection in ct images: False positive reduction using multi-view convolutional networks. A Setio, F Ciompi, G Litjens, P Gerke, C Jacobs, S Van Riel, M Wille, M Naqibullah, C Snchez, B Van, IEEE Trans. Medical Imaging. 355A. Setio, F. Ciompi, G. Litjens, P. Gerke, C. Jacobs, S. van Riel, M. Wille, M. Naqibullah, C. Snchez, and B. van Gin- neken. Pulmonary nodule detection in ct images: False positive reduction using multi-view convolutional networks. IEEE Trans. Medical Imaging, 35(5):1160-1169, 2016. 2\n\nInterleaved text/image deep mining on a large-scale radiology database for automated image interpretation. H Shin, L Lu, L Kim, A Seff, J Yao, R Summers, Journal of Machine Learning Research. 172H. Shin, L. Lu, L. Kim, A. Seff, J. Yao, and R. Summers. Interleaved text/image deep mining on a large-scale radiol- ogy database for automated image interpretation. Journal of Machine Learning Research, 17:1-31, 2016. 2\n\nLearning to read chest x-rays: Recurrent neural cascade model for automated image annotation. H Shin, K Roberts, L Lu, D Demner-Fushman, J Yao, R Summers, CVPR. 211H. Shin, K. Roberts, L. Lu, D. Demner-Fushman, J. Yao, and R. Summers. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. In CVPR, 2016. 2, 11\n\nDeep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learnings. H Shin, H Roth, M Gao, L Lu, Z Xu, I Nogues, J Yao, D Mollura, R Summers, IEEE Trans. Medical Imaging. 355H. Shin, H. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura, and R. Summers. Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learnings. IEEE Trans. Medical Imaging, 35(5):1285-1298, 2016. 2\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.155657arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 5, 7\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition57C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1-9, 2015. 5, 7\n\nMovieqa: Understanding stories in movies through question-answering. M Tapaswi, Y Zhu, R Stiefelhagen, A Torralba, R Urtasun, S Fidler, ICCV. M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urta- sun, and S. Fidler. Movieqa: Understanding stories in movies through question-answering. In ICCV, 2015. 1\n\nSelective search for object recognition. International journal of computer vision. J R Uijlings, K E Van De Sande, T Gevers, A W Smeulders, 104J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. Inter- national journal of computer vision, 104(2):154-171, 2013. 11\n\nOrderembeddings of images and language. I Vendrov, R Kiros, S Fidler, R Urtasun, ICLR. I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun. Order- embeddings of images and language. In ICLR, 2016. 1\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, CVPR. 1O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, pages 3156-3164, 2015. 1, 2\n\nGenodisc dataset: The benefits of multi-disciplinary research on intervertebral disc degeneration. H.-J Wilke, M Kmin, J Urban, European Spine Journal. 2H.-J. Wilke, M. Kmin, and J. Urban. Genodisc dataset: The benefits of multi-disciplinary research on intervertebral disc degeneration. In European Spine Journal, 2016. 2\n\nAsk me anything: free-form visual question answering based on knowledge from external sources. Q Wu, P Wang, C Shen, A Dick, A Van Den, Hengel, CVPR. 13Q. Wu, P. Wang, C. Shen, A. Dick, and A. van den Hengel. Ask me anything: free-form visual question answering based on knowledge from external sources. In CVPR, 2016. 1, 2, 3\n\nA multi-center milestone study of clinical vertebral ct segmentation. J Yao, Computerized Medical Imaging and Graphics. J. Yao and et al. A multi-center milestone study of clinical vertebral ct segmentation. In Computerized Medical Imaging and Graphics, pages 49(4): 16-28, 2016. 2\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. P Young, A Lai, M Hodosh, J Hockenmaier, TACL. P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im- age descriptions to visual denotations: New similarity met- rics for semantic inference over event descriptions. In TACL, 2014. 2\n\nLearning deep features for discriminative localization. B Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, arXiv:1512.04150arXiv preprintB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. arXiv preprint arXiv:1512.04150, 2015. 5\n\nVisual7w: Grounded question answering in images. Y Zhu, O Groth, M Bernstein, L Fei-Fei, CVPR. 13Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7w: Grounded question answering in images. In CVPR, 2016. 1, 2, 3\n", "annotations": {"author": "[{\"end\":245,\"start\":146},{\"end\":398,\"start\":246},{\"end\":482,\"start\":399},{\"end\":616,\"start\":483},{\"end\":726,\"start\":617},{\"end\":807,\"start\":727}]", "publisher": null, "author_last_name": "[{\"end\":159,\"start\":155},{\"end\":256,\"start\":252},{\"end\":404,\"start\":402},{\"end\":493,\"start\":491},{\"end\":637,\"start\":630},{\"end\":743,\"start\":736}]", "author_first_name": "[{\"end\":154,\"start\":146},{\"end\":251,\"start\":246},{\"end\":401,\"start\":399},{\"end\":490,\"start\":483},{\"end\":629,\"start\":617},{\"end\":733,\"start\":727},{\"end\":735,\"start\":734}]", "author_affiliation": "[{\"end\":244,\"start\":183},{\"end\":397,\"start\":277},{\"end\":481,\"start\":420},{\"end\":615,\"start\":495},{\"end\":725,\"start\":664},{\"end\":806,\"start\":745}]", "title": "[{\"end\":143,\"start\":1},{\"end\":950,\"start\":808}]", "venue": null, "abstract": "[{\"end\":2467,\"start\":952}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2638,\"start\":2634},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2641,\"start\":2638},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2644,\"start\":2641},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2647,\"start\":2644},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3123,\"start\":3119},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3126,\"start\":3123},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3129,\"start\":3126},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3132,\"start\":3129},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3135,\"start\":3132},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3169,\"start\":3166},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3172,\"start\":3169},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3175,\"start\":3172},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3218,\"start\":3215},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3221,\"start\":3218},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3413,\"start\":3409},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3452,\"start\":3448},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4376,\"start\":4372},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4505,\"start\":4501},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4508,\"start\":4505},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4544,\"start\":4540},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4590,\"start\":4586},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4628,\"start\":4624},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4671,\"start\":4667},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4715,\"start\":4711},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4771,\"start\":4767},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4774,\"start\":4771},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5152,\"start\":5148},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5155,\"start\":5152},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5158,\"start\":5155},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5160,\"start\":5158},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5163,\"start\":5160},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5166,\"start\":5163},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5169,\"start\":5166},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5172,\"start\":5169},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5534,\"start\":5530},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5537,\"start\":5534},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5570,\"start\":5566},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5587,\"start\":5583},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6108,\"start\":6104},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6111,\"start\":6108},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6354,\"start\":6351},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6455,\"start\":6451},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6472,\"start\":6468},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6497,\"start\":6493},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6500,\"start\":6497},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6913,\"start\":6909},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6916,\"start\":6913},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7593,\"start\":7589},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7596,\"start\":7593},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8090,\"start\":8086},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8093,\"start\":8090},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8096,\"start\":8093},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9680,\"start\":9676},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9683,\"start\":9680},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9686,\"start\":9683},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9689,\"start\":9686},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9839,\"start\":9836},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10057,\"start\":10053},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10249,\"start\":10246},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10537,\"start\":10533},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10568,\"start\":10564},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10571,\"start\":10568},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10573,\"start\":10571},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12036,\"start\":12032},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12777,\"start\":12773},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12793,\"start\":12790},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14094,\"start\":14091},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15205,\"start\":15202},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15258,\"start\":15255},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15303,\"start\":15299},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15436,\"start\":15433},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16482,\"start\":16479},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16633,\"start\":16630},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16845,\"start\":16842},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16848,\"start\":16845},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18527,\"start\":18523},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20745,\"start\":20741},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20748,\"start\":20745},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20751,\"start\":20748},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20754,\"start\":20751},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20854,\"start\":20850},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20857,\"start\":20854},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20878,\"start\":20874},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20894,\"start\":20890},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20910,\"start\":20906},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20929,\"start\":20925},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":21198,\"start\":21194},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24815,\"start\":24811},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27045,\"start\":27041},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27098,\"start\":27094},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27114,\"start\":27110},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27130,\"start\":27126},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27149,\"start\":27145},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29018,\"start\":29014},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29034,\"start\":29030},{\"end\":31225,\"start\":31216},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34685,\"start\":34681},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34701,\"start\":34697},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35505,\"start\":35501}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43403,\"start\":43060},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43516,\"start\":43404},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43616,\"start\":43517},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43716,\"start\":43617},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43822,\"start\":43717},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43925,\"start\":43823},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44027,\"start\":43926},{\"attributes\":{\"id\":\"fig_8\"},\"end\":44184,\"start\":44028},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45915,\"start\":44185},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46369,\"start\":45916},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47739,\"start\":46370},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48498,\"start\":47740},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48935,\"start\":48499},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":49990,\"start\":48936}]", "paragraph": "[{\"end\":2894,\"start\":2483},{\"end\":4056,\"start\":2896},{\"end\":5036,\"start\":4058},{\"end\":6753,\"start\":5038},{\"end\":8771,\"start\":6755},{\"end\":9565,\"start\":8773},{\"end\":10575,\"start\":9582},{\"end\":10630,\"start\":10577},{\"end\":12037,\"start\":10632},{\"end\":12676,\"start\":12079},{\"end\":13678,\"start\":12678},{\"end\":14042,\"start\":13680},{\"end\":14653,\"start\":14044},{\"end\":15104,\"start\":14655},{\"end\":15819,\"start\":15106},{\"end\":15916,\"start\":15821},{\"end\":16082,\"start\":15918},{\"end\":16226,\"start\":16084},{\"end\":17562,\"start\":16266},{\"end\":18198,\"start\":17596},{\"end\":18701,\"start\":18231},{\"end\":19326,\"start\":18703},{\"end\":19807,\"start\":19381},{\"end\":20321,\"start\":19809},{\"end\":21434,\"start\":20348},{\"end\":22025,\"start\":21436},{\"end\":22671,\"start\":22027},{\"end\":23428,\"start\":22673},{\"end\":23662,\"start\":23510},{\"end\":24602,\"start\":23707},{\"end\":24855,\"start\":24604},{\"end\":25421,\"start\":24914},{\"end\":26098,\"start\":25493},{\"end\":26956,\"start\":26114},{\"end\":27320,\"start\":26958},{\"end\":27972,\"start\":27322},{\"end\":29077,\"start\":27974},{\"end\":30222,\"start\":29079},{\"end\":31834,\"start\":30224},{\"end\":34134,\"start\":31849},{\"end\":36211,\"start\":34176},{\"end\":36688,\"start\":36239},{\"end\":36995,\"start\":36709},{\"end\":37265,\"start\":36997},{\"end\":37788,\"start\":37277},{\"end\":37861,\"start\":37824},{\"end\":38346,\"start\":37863},{\"end\":38978,\"start\":38374},{\"end\":39729,\"start\":39013},{\"end\":40369,\"start\":39750},{\"end\":40711,\"start\":40417},{\"end\":41583,\"start\":40750},{\"end\":42009,\"start\":41609},{\"end\":42411,\"start\":42023},{\"end\":42692,\"start\":42413},{\"end\":43059,\"start\":42694}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":23509,\"start\":23429},{\"attributes\":{\"id\":\"formula_1\"},\"end\":24913,\"start\":24856},{\"attributes\":{\"id\":\"formula_2\"},\"end\":25492,\"start\":25422}]", "table_ref": "[{\"end\":10267,\"start\":10250},{\"end\":16792,\"start\":16785},{\"end\":16865,\"start\":16858},{\"end\":28276,\"start\":28269},{\"end\":29974,\"start\":29967},{\"end\":31550,\"start\":31543},{\"end\":33599,\"start\":33592},{\"end\":33792,\"start\":33785},{\"end\":33804,\"start\":33796},{\"end\":34565,\"start\":34558},{\"end\":34994,\"start\":34986},{\"end\":35965,\"start\":35958},{\"end\":36441,\"start\":36434},{\"end\":37022,\"start\":37014},{\"end\":37541,\"start\":37533},{\"end\":38107,\"start\":38099},{\"end\":38737,\"start\":38729},{\"end\":39485,\"start\":39477},{\"end\":40122,\"start\":40114},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40697,\"start\":40689},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":41210,\"start\":41202},{\"end\":42397,\"start\":42388},{\"end\":42691,\"start\":42683},{\"end\":42973,\"start\":42965}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2481,\"start\":2469},{\"attributes\":{\"n\":\"1.1\"},\"end\":9580,\"start\":9568},{\"attributes\":{\"n\":\"2.1\"},\"end\":12077,\"start\":12040},{\"attributes\":{\"n\":\"2.2\"},\"end\":16264,\"start\":16229},{\"attributes\":{\"n\":\"2.3\"},\"end\":17594,\"start\":17565},{\"attributes\":{\"n\":\"2.4\"},\"end\":18229,\"start\":18201},{\"attributes\":{\"n\":\"3\"},\"end\":19379,\"start\":19329},{\"attributes\":{\"n\":\"3.1\"},\"end\":20346,\"start\":20324},{\"attributes\":{\"n\":\"3.2\"},\"end\":23705,\"start\":23665},{\"attributes\":{\"n\":\"4\"},\"end\":26112,\"start\":26101},{\"attributes\":{\"n\":\"5\"},\"end\":31847,\"start\":31837},{\"end\":34174,\"start\":34137},{\"end\":36218,\"start\":36214},{\"end\":36237,\"start\":36221},{\"end\":36707,\"start\":36691},{\"end\":37275,\"start\":37268},{\"end\":37803,\"start\":37791},{\"end\":37822,\"start\":37806},{\"end\":38353,\"start\":38349},{\"end\":38372,\"start\":38356},{\"end\":39001,\"start\":38981},{\"end\":39011,\"start\":39004},{\"end\":39748,\"start\":39732},{\"end\":40390,\"start\":40372},{\"end\":40415,\"start\":40393},{\"end\":40748,\"start\":40714},{\"end\":41607,\"start\":41586},{\"end\":42021,\"start\":42012},{\"end\":43071,\"start\":43061},{\"end\":43415,\"start\":43405},{\"end\":43528,\"start\":43518},{\"end\":43728,\"start\":43718},{\"end\":43834,\"start\":43824},{\"end\":43937,\"start\":43927},{\"end\":44039,\"start\":44029},{\"end\":46380,\"start\":46371},{\"end\":48947,\"start\":48937}]", "table": "[{\"end\":45915,\"start\":44358},{\"end\":46369,\"start\":46046},{\"end\":47739,\"start\":46454},{\"end\":48498,\"start\":47934},{\"end\":48935,\"start\":48601},{\"end\":49990,\"start\":49169}]", "figure_caption": "[{\"end\":43403,\"start\":43073},{\"end\":43516,\"start\":43417},{\"end\":43616,\"start\":43530},{\"end\":43716,\"start\":43619},{\"end\":43822,\"start\":43730},{\"end\":43925,\"start\":43836},{\"end\":44027,\"start\":43939},{\"end\":44184,\"start\":44041},{\"end\":44358,\"start\":44187},{\"end\":46046,\"start\":45918},{\"end\":46454,\"start\":46382},{\"end\":47934,\"start\":47742},{\"end\":48601,\"start\":48501},{\"end\":49169,\"start\":48950}]", "figure_ref": "[{\"end\":2774,\"start\":2766},{\"end\":7808,\"start\":7802},{\"end\":11006,\"start\":11000},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11669,\"start\":11663},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14651,\"start\":14645},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15659,\"start\":15653},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20612,\"start\":20606},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20774,\"start\":20768},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24128,\"start\":24122},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28016,\"start\":28010},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40710,\"start\":40702},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":42410,\"start\":42402}]", "bib_author_first_name": "[{\"end\":51491,\"start\":51490},{\"end\":51500,\"start\":51499},{\"end\":51511,\"start\":51510},{\"end\":51517,\"start\":51516},{\"end\":51529,\"start\":51528},{\"end\":51538,\"start\":51537},{\"end\":51747,\"start\":51746},{\"end\":51749,\"start\":51748},{\"end\":51763,\"start\":51759},{\"end\":52099,\"start\":52098},{\"end\":52105,\"start\":52104},{\"end\":52116,\"start\":52115},{\"end\":52126,\"start\":52125},{\"end\":52344,\"start\":52343},{\"end\":52352,\"start\":52351},{\"end\":52361,\"start\":52360},{\"end\":52583,\"start\":52582},{\"end\":52585,\"start\":52584},{\"end\":52596,\"start\":52595},{\"end\":52609,\"start\":52608},{\"end\":52620,\"start\":52619},{\"end\":52622,\"start\":52621},{\"end\":52632,\"start\":52631},{\"end\":52634,\"start\":52633},{\"end\":52978,\"start\":52977},{\"end\":52990,\"start\":52989},{\"end\":53420,\"start\":53416},{\"end\":53435,\"start\":53434},{\"end\":53437,\"start\":53436},{\"end\":53657,\"start\":53656},{\"end\":53675,\"start\":53674},{\"end\":53677,\"start\":53676},{\"end\":53686,\"start\":53685},{\"end\":53688,\"start\":53687},{\"end\":53700,\"start\":53699},{\"end\":53702,\"start\":53701},{\"end\":53714,\"start\":53713},{\"end\":53727,\"start\":53726},{\"end\":53737,\"start\":53736},{\"end\":53739,\"start\":53738},{\"end\":53748,\"start\":53747},{\"end\":53750,\"start\":53749},{\"end\":54166,\"start\":54165},{\"end\":54174,\"start\":54173},{\"end\":54182,\"start\":54181},{\"end\":54195,\"start\":54191},{\"end\":54201,\"start\":54200},{\"end\":54207,\"start\":54206},{\"end\":54550,\"start\":54549},{\"end\":54557,\"start\":54556},{\"end\":54565,\"start\":54564},{\"end\":54571,\"start\":54570},{\"end\":54579,\"start\":54578},{\"end\":54586,\"start\":54585},{\"end\":54594,\"start\":54593},{\"end\":54601,\"start\":54600},{\"end\":54608,\"start\":54607},{\"end\":54955,\"start\":54954},{\"end\":54965,\"start\":54964},{\"end\":54974,\"start\":54973},{\"end\":55227,\"start\":55226},{\"end\":55241,\"start\":55240},{\"end\":55245,\"start\":55242},{\"end\":55255,\"start\":55254},{\"end\":55257,\"start\":55256},{\"end\":55269,\"start\":55268},{\"end\":55281,\"start\":55280},{\"end\":55289,\"start\":55288},{\"end\":55642,\"start\":55641},{\"end\":55655,\"start\":55654},{\"end\":55671,\"start\":55670},{\"end\":55673,\"start\":55672},{\"end\":55931,\"start\":55930},{\"end\":55944,\"start\":55943},{\"end\":56172,\"start\":56171},{\"end\":56182,\"start\":56181},{\"end\":56193,\"start\":56192},{\"end\":56205,\"start\":56204},{\"end\":56418,\"start\":56417},{\"end\":56424,\"start\":56423},{\"end\":56433,\"start\":56432},{\"end\":56440,\"start\":56439},{\"end\":56656,\"start\":56655},{\"end\":56666,\"start\":56665},{\"end\":56678,\"start\":56677},{\"end\":56688,\"start\":56687},{\"end\":57020,\"start\":57019},{\"end\":57032,\"start\":57028},{\"end\":57261,\"start\":57260},{\"end\":57271,\"start\":57270},{\"end\":57283,\"start\":57282},{\"end\":57296,\"start\":57292},{\"end\":57298,\"start\":57297},{\"end\":57308,\"start\":57304},{\"end\":57314,\"start\":57313},{\"end\":57673,\"start\":57672},{\"end\":57686,\"start\":57685},{\"end\":57695,\"start\":57694},{\"end\":57874,\"start\":57873},{\"end\":57881,\"start\":57880},{\"end\":57894,\"start\":57893},{\"end\":57905,\"start\":57904},{\"end\":57916,\"start\":57915},{\"end\":57924,\"start\":57923},{\"end\":57936,\"start\":57935},{\"end\":57950,\"start\":57949},{\"end\":58332,\"start\":58331},{\"end\":58343,\"start\":58342},{\"end\":58355,\"start\":58354},{\"end\":58580,\"start\":58579},{\"end\":58592,\"start\":58591},{\"end\":58816,\"start\":58815},{\"end\":58827,\"start\":58826},{\"end\":58834,\"start\":58833},{\"end\":58843,\"start\":58842},{\"end\":58854,\"start\":58853},{\"end\":58862,\"start\":58861},{\"end\":58873,\"start\":58872},{\"end\":58881,\"start\":58880},{\"end\":58898,\"start\":58894},{\"end\":58904,\"start\":58903},{\"end\":58906,\"start\":58905},{\"end\":58916,\"start\":58915},{\"end\":58929,\"start\":58928},{\"end\":59250,\"start\":59249},{\"end\":59264,\"start\":59263},{\"end\":59277,\"start\":59276},{\"end\":59279,\"start\":59278},{\"end\":59628,\"start\":59627},{\"end\":59638,\"start\":59637},{\"end\":59647,\"start\":59646},{\"end\":59875,\"start\":59871},{\"end\":59882,\"start\":59881},{\"end\":59891,\"start\":59890},{\"end\":59903,\"start\":59902},{\"end\":59911,\"start\":59910},{\"end\":59921,\"start\":59920},{\"end\":59932,\"start\":59931},{\"end\":59941,\"start\":59940},{\"end\":60262,\"start\":60261},{\"end\":60568,\"start\":60567},{\"end\":60580,\"start\":60579},{\"end\":60593,\"start\":60592},{\"end\":60611,\"start\":60610},{\"end\":60623,\"start\":60622},{\"end\":60633,\"start\":60632},{\"end\":60646,\"start\":60645},{\"end\":60977,\"start\":60976},{\"end\":60986,\"start\":60985},{\"end\":60996,\"start\":60995},{\"end\":61006,\"start\":61005},{\"end\":61279,\"start\":61278},{\"end\":61281,\"start\":61280},{\"end\":61293,\"start\":61292},{\"end\":61761,\"start\":61760},{\"end\":61772,\"start\":61771},{\"end\":61780,\"start\":61779},{\"end\":61793,\"start\":61792},{\"end\":61804,\"start\":61803},{\"end\":61819,\"start\":61818},{\"end\":62136,\"start\":62135},{\"end\":62144,\"start\":62143},{\"end\":62151,\"start\":62150},{\"end\":62159,\"start\":62158},{\"end\":62403,\"start\":62402},{\"end\":62418,\"start\":62417},{\"end\":62429,\"start\":62428},{\"end\":62695,\"start\":62694},{\"end\":62703,\"start\":62702},{\"end\":62709,\"start\":62708},{\"end\":62721,\"start\":62717},{\"end\":62729,\"start\":62728},{\"end\":62736,\"start\":62735},{\"end\":62738,\"start\":62737},{\"end\":62749,\"start\":62748},{\"end\":62751,\"start\":62750},{\"end\":63113,\"start\":63112},{\"end\":63115,\"start\":63114},{\"end\":63123,\"start\":63122},{\"end\":63129,\"start\":63128},{\"end\":63137,\"start\":63136},{\"end\":63139,\"start\":63138},{\"end\":63149,\"start\":63148},{\"end\":63160,\"start\":63159},{\"end\":63168,\"start\":63167},{\"end\":63175,\"start\":63174},{\"end\":63186,\"start\":63185},{\"end\":63188,\"start\":63187},{\"end\":63533,\"start\":63532},{\"end\":63548,\"start\":63547},{\"end\":63556,\"start\":63555},{\"end\":63562,\"start\":63561},{\"end\":63572,\"start\":63571},{\"end\":63584,\"start\":63583},{\"end\":63590,\"start\":63589},{\"end\":63599,\"start\":63598},{\"end\":63611,\"start\":63610},{\"end\":63621,\"start\":63620},{\"end\":63634,\"start\":63633},{\"end\":63642,\"start\":63641},{\"end\":64016,\"start\":64015},{\"end\":64031,\"start\":64030},{\"end\":64039,\"start\":64038},{\"end\":64045,\"start\":64044},{\"end\":64055,\"start\":64054},{\"end\":64067,\"start\":64066},{\"end\":64073,\"start\":64072},{\"end\":64082,\"start\":64081},{\"end\":64094,\"start\":64093},{\"end\":64104,\"start\":64103},{\"end\":64518,\"start\":64517},{\"end\":64527,\"start\":64526},{\"end\":64537,\"start\":64536},{\"end\":64548,\"start\":64547},{\"end\":64557,\"start\":64556},{\"end\":64567,\"start\":64566},{\"end\":64579,\"start\":64578},{\"end\":64588,\"start\":64587},{\"end\":64602,\"start\":64601},{\"end\":64612,\"start\":64611},{\"end\":65046,\"start\":65045},{\"end\":65054,\"start\":65053},{\"end\":65060,\"start\":65059},{\"end\":65067,\"start\":65066},{\"end\":65075,\"start\":65074},{\"end\":65082,\"start\":65081},{\"end\":65450,\"start\":65449},{\"end\":65458,\"start\":65457},{\"end\":65469,\"start\":65468},{\"end\":65475,\"start\":65474},{\"end\":65493,\"start\":65492},{\"end\":65500,\"start\":65499},{\"end\":65839,\"start\":65838},{\"end\":65847,\"start\":65846},{\"end\":65855,\"start\":65854},{\"end\":65862,\"start\":65861},{\"end\":65868,\"start\":65867},{\"end\":65874,\"start\":65873},{\"end\":65884,\"start\":65883},{\"end\":65891,\"start\":65890},{\"end\":65902,\"start\":65901},{\"end\":66287,\"start\":66286},{\"end\":66299,\"start\":66298},{\"end\":66517,\"start\":66516},{\"end\":66528,\"start\":66527},{\"end\":66535,\"start\":66534},{\"end\":66542,\"start\":66541},{\"end\":66554,\"start\":66553},{\"end\":66562,\"start\":66561},{\"end\":66574,\"start\":66573},{\"end\":66583,\"start\":66582},{\"end\":66596,\"start\":66595},{\"end\":67065,\"start\":67064},{\"end\":67076,\"start\":67075},{\"end\":67083,\"start\":67082},{\"end\":67099,\"start\":67098},{\"end\":67111,\"start\":67110},{\"end\":67122,\"start\":67121},{\"end\":67387,\"start\":67386},{\"end\":67389,\"start\":67388},{\"end\":67401,\"start\":67400},{\"end\":67403,\"start\":67402},{\"end\":67419,\"start\":67418},{\"end\":67429,\"start\":67428},{\"end\":67431,\"start\":67430},{\"end\":67666,\"start\":67665},{\"end\":67677,\"start\":67676},{\"end\":67686,\"start\":67685},{\"end\":67696,\"start\":67695},{\"end\":67871,\"start\":67870},{\"end\":67882,\"start\":67881},{\"end\":67892,\"start\":67891},{\"end\":67902,\"start\":67901},{\"end\":68155,\"start\":68151},{\"end\":68164,\"start\":68163},{\"end\":68172,\"start\":68171},{\"end\":68472,\"start\":68471},{\"end\":68478,\"start\":68477},{\"end\":68486,\"start\":68485},{\"end\":68494,\"start\":68493},{\"end\":68502,\"start\":68501},{\"end\":68775,\"start\":68774},{\"end\":69106,\"start\":69105},{\"end\":69115,\"start\":69114},{\"end\":69122,\"start\":69121},{\"end\":69132,\"start\":69131},{\"end\":69398,\"start\":69397},{\"end\":69406,\"start\":69405},{\"end\":69416,\"start\":69415},{\"end\":69429,\"start\":69428},{\"end\":69438,\"start\":69437},{\"end\":69688,\"start\":69687},{\"end\":69695,\"start\":69694},{\"end\":69704,\"start\":69703},{\"end\":69717,\"start\":69716}]", "bib_author_last_name": "[{\"end\":51497,\"start\":51492},{\"end\":51508,\"start\":51501},{\"end\":51514,\"start\":51512},{\"end\":51526,\"start\":51518},{\"end\":51535,\"start\":51530},{\"end\":51546,\"start\":51539},{\"end\":51757,\"start\":51750},{\"end\":51768,\"start\":51764},{\"end\":52102,\"start\":52100},{\"end\":52113,\"start\":52106},{\"end\":52123,\"start\":52117},{\"end\":52140,\"start\":52127},{\"end\":52349,\"start\":52345},{\"end\":52358,\"start\":52353},{\"end\":52367,\"start\":52362},{\"end\":52593,\"start\":52586},{\"end\":52606,\"start\":52597},{\"end\":52617,\"start\":52610},{\"end\":52629,\"start\":52623},{\"end\":52643,\"start\":52635},{\"end\":52987,\"start\":52979},{\"end\":52998,\"start\":52991},{\"end\":53432,\"start\":53421},{\"end\":53445,\"start\":53438},{\"end\":53672,\"start\":53658},{\"end\":53683,\"start\":53678},{\"end\":53697,\"start\":53689},{\"end\":53711,\"start\":53703},{\"end\":53724,\"start\":53715},{\"end\":53734,\"start\":53728},{\"end\":53745,\"start\":53740},{\"end\":53759,\"start\":53751},{\"end\":54171,\"start\":54167},{\"end\":54179,\"start\":54175},{\"end\":54189,\"start\":54183},{\"end\":54198,\"start\":54196},{\"end\":54204,\"start\":54202},{\"end\":54215,\"start\":54208},{\"end\":54554,\"start\":54551},{\"end\":54562,\"start\":54558},{\"end\":54568,\"start\":54566},{\"end\":54576,\"start\":54572},{\"end\":54583,\"start\":54580},{\"end\":54591,\"start\":54587},{\"end\":54598,\"start\":54595},{\"end\":54605,\"start\":54602},{\"end\":54613,\"start\":54609},{\"end\":54962,\"start\":54956},{\"end\":54971,\"start\":54966},{\"end\":54979,\"start\":54975},{\"end\":55238,\"start\":55228},{\"end\":55252,\"start\":55246},{\"end\":55266,\"start\":55258},{\"end\":55278,\"start\":55270},{\"end\":55286,\"start\":55282},{\"end\":55299,\"start\":55290},{\"end\":55652,\"start\":55643},{\"end\":55668,\"start\":55656},{\"end\":55681,\"start\":55674},{\"end\":55941,\"start\":55932},{\"end\":55953,\"start\":55945},{\"end\":56179,\"start\":56173},{\"end\":56190,\"start\":56183},{\"end\":56202,\"start\":56194},{\"end\":56212,\"start\":56206},{\"end\":56421,\"start\":56419},{\"end\":56430,\"start\":56425},{\"end\":56437,\"start\":56434},{\"end\":56444,\"start\":56441},{\"end\":56663,\"start\":56657},{\"end\":56675,\"start\":56667},{\"end\":56685,\"start\":56679},{\"end\":56696,\"start\":56689},{\"end\":57026,\"start\":57021},{\"end\":57036,\"start\":57033},{\"end\":57268,\"start\":57262},{\"end\":57280,\"start\":57272},{\"end\":57290,\"start\":57284},{\"end\":57302,\"start\":57299},{\"end\":57311,\"start\":57309},{\"end\":57320,\"start\":57315},{\"end\":57683,\"start\":57674},{\"end\":57692,\"start\":57687},{\"end\":57705,\"start\":57696},{\"end\":57878,\"start\":57875},{\"end\":57891,\"start\":57882},{\"end\":57902,\"start\":57895},{\"end\":57913,\"start\":57906},{\"end\":57921,\"start\":57917},{\"end\":57933,\"start\":57925},{\"end\":57947,\"start\":57937},{\"end\":57958,\"start\":57951},{\"end\":58340,\"start\":58333},{\"end\":58352,\"start\":58344},{\"end\":58363,\"start\":58356},{\"end\":58589,\"start\":58581},{\"end\":58600,\"start\":58593},{\"end\":58824,\"start\":58817},{\"end\":58831,\"start\":58828},{\"end\":58840,\"start\":58835},{\"end\":58851,\"start\":58844},{\"end\":58859,\"start\":58855},{\"end\":58870,\"start\":58863},{\"end\":58878,\"start\":58874},{\"end\":58892,\"start\":58882},{\"end\":58901,\"start\":58899},{\"end\":58913,\"start\":58907},{\"end\":58926,\"start\":58917},{\"end\":58937,\"start\":58930},{\"end\":59261,\"start\":59251},{\"end\":59274,\"start\":59265},{\"end\":59286,\"start\":59280},{\"end\":59635,\"start\":59629},{\"end\":59644,\"start\":59639},{\"end\":59650,\"start\":59648},{\"end\":59879,\"start\":59876},{\"end\":59888,\"start\":59883},{\"end\":59900,\"start\":59892},{\"end\":59908,\"start\":59904},{\"end\":59918,\"start\":59912},{\"end\":59929,\"start\":59922},{\"end\":59938,\"start\":59933},{\"end\":59949,\"start\":59942},{\"end\":60271,\"start\":60263},{\"end\":60577,\"start\":60569},{\"end\":60590,\"start\":60581},{\"end\":60608,\"start\":60594},{\"end\":60620,\"start\":60612},{\"end\":60630,\"start\":60624},{\"end\":60643,\"start\":60634},{\"end\":60652,\"start\":60647},{\"end\":60983,\"start\":60978},{\"end\":60993,\"start\":60987},{\"end\":61003,\"start\":60997},{\"end\":61012,\"start\":61007},{\"end\":61290,\"start\":61282},{\"end\":61303,\"start\":61294},{\"end\":61769,\"start\":61762},{\"end\":61777,\"start\":61773},{\"end\":61790,\"start\":61781},{\"end\":61801,\"start\":61794},{\"end\":61816,\"start\":61805},{\"end\":61828,\"start\":61820},{\"end\":62141,\"start\":62137},{\"end\":62148,\"start\":62145},{\"end\":62156,\"start\":62152},{\"end\":62167,\"start\":62160},{\"end\":62175,\"start\":62169},{\"end\":62415,\"start\":62404},{\"end\":62426,\"start\":62419},{\"end\":62434,\"start\":62430},{\"end\":62700,\"start\":62696},{\"end\":62706,\"start\":62704},{\"end\":62715,\"start\":62710},{\"end\":62726,\"start\":62722},{\"end\":62733,\"start\":62730},{\"end\":62746,\"start\":62739},{\"end\":62759,\"start\":62752},{\"end\":63120,\"start\":63116},{\"end\":63126,\"start\":63124},{\"end\":63134,\"start\":63130},{\"end\":63146,\"start\":63140},{\"end\":63157,\"start\":63150},{\"end\":63165,\"start\":63161},{\"end\":63172,\"start\":63169},{\"end\":63183,\"start\":63176},{\"end\":63196,\"start\":63189},{\"end\":63545,\"start\":63534},{\"end\":63553,\"start\":63549},{\"end\":63559,\"start\":63557},{\"end\":63569,\"start\":63563},{\"end\":63581,\"start\":63573},{\"end\":63587,\"start\":63585},{\"end\":63596,\"start\":63591},{\"end\":63608,\"start\":63600},{\"end\":63618,\"start\":63612},{\"end\":63631,\"start\":63622},{\"end\":63639,\"start\":63635},{\"end\":63650,\"start\":63643},{\"end\":64028,\"start\":64017},{\"end\":64036,\"start\":64032},{\"end\":64042,\"start\":64040},{\"end\":64052,\"start\":64046},{\"end\":64064,\"start\":64056},{\"end\":64070,\"start\":64068},{\"end\":64079,\"start\":64074},{\"end\":64091,\"start\":64083},{\"end\":64101,\"start\":64095},{\"end\":64114,\"start\":64105},{\"end\":64524,\"start\":64519},{\"end\":64534,\"start\":64528},{\"end\":64545,\"start\":64538},{\"end\":64554,\"start\":64549},{\"end\":64564,\"start\":64558},{\"end\":64576,\"start\":64568},{\"end\":64585,\"start\":64580},{\"end\":64599,\"start\":64589},{\"end\":64609,\"start\":64603},{\"end\":64616,\"start\":64613},{\"end\":65051,\"start\":65047},{\"end\":65057,\"start\":65055},{\"end\":65064,\"start\":65061},{\"end\":65072,\"start\":65068},{\"end\":65079,\"start\":65076},{\"end\":65090,\"start\":65083},{\"end\":65455,\"start\":65451},{\"end\":65466,\"start\":65459},{\"end\":65472,\"start\":65470},{\"end\":65490,\"start\":65476},{\"end\":65497,\"start\":65494},{\"end\":65508,\"start\":65501},{\"end\":65844,\"start\":65840},{\"end\":65852,\"start\":65848},{\"end\":65859,\"start\":65856},{\"end\":65865,\"start\":65863},{\"end\":65871,\"start\":65869},{\"end\":65881,\"start\":65875},{\"end\":65888,\"start\":65885},{\"end\":65899,\"start\":65892},{\"end\":65910,\"start\":65903},{\"end\":66296,\"start\":66288},{\"end\":66309,\"start\":66300},{\"end\":66525,\"start\":66518},{\"end\":66532,\"start\":66529},{\"end\":66539,\"start\":66536},{\"end\":66551,\"start\":66543},{\"end\":66559,\"start\":66555},{\"end\":66571,\"start\":66563},{\"end\":66580,\"start\":66575},{\"end\":66593,\"start\":66584},{\"end\":66607,\"start\":66597},{\"end\":67073,\"start\":67066},{\"end\":67080,\"start\":67077},{\"end\":67096,\"start\":67084},{\"end\":67108,\"start\":67100},{\"end\":67119,\"start\":67112},{\"end\":67129,\"start\":67123},{\"end\":67398,\"start\":67390},{\"end\":67416,\"start\":67404},{\"end\":67426,\"start\":67420},{\"end\":67441,\"start\":67432},{\"end\":67674,\"start\":67667},{\"end\":67683,\"start\":67678},{\"end\":67693,\"start\":67687},{\"end\":67704,\"start\":67697},{\"end\":67879,\"start\":67872},{\"end\":67889,\"start\":67883},{\"end\":67899,\"start\":67893},{\"end\":67908,\"start\":67903},{\"end\":68161,\"start\":68156},{\"end\":68169,\"start\":68165},{\"end\":68178,\"start\":68173},{\"end\":68475,\"start\":68473},{\"end\":68483,\"start\":68479},{\"end\":68491,\"start\":68487},{\"end\":68499,\"start\":68495},{\"end\":68510,\"start\":68503},{\"end\":68518,\"start\":68512},{\"end\":68779,\"start\":68776},{\"end\":69112,\"start\":69107},{\"end\":69119,\"start\":69116},{\"end\":69129,\"start\":69123},{\"end\":69144,\"start\":69133},{\"end\":69403,\"start\":69399},{\"end\":69413,\"start\":69407},{\"end\":69426,\"start\":69417},{\"end\":69435,\"start\":69430},{\"end\":69447,\"start\":69439},{\"end\":69692,\"start\":69689},{\"end\":69701,\"start\":69696},{\"end\":69714,\"start\":69705},{\"end\":69725,\"start\":69718}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":51456,\"start\":51321},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3180429},\"end\":51676,\"start\":51458},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":18647938},\"end\":52012,\"start\":51678},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14149995},\"end\":52300,\"start\":52014},{\"attributes\":{\"id\":\"b4\"},\"end\":52491,\"start\":52302},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6315215},\"end\":52908,\"start\":52493},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11599080},\"end\":53378,\"start\":52910},{\"attributes\":{\"id\":\"b7\"},\"end\":53573,\"start\":53380},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16941525},\"end\":54110,\"start\":53575},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":54450,\"start\":54112},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12470100},\"end\":54878,\"start\":54452},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6351772},\"end\":55121,\"start\":54880},{\"attributes\":{\"id\":\"b12\"},\"end\":55531,\"start\":55123},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5501470},\"end\":55926,\"start\":55533},{\"attributes\":{\"doi\":\"arXiv:1606.02819\",\"id\":\"b14\"},\"end\":56129,\"start\":55928},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15954031},\"end\":56369,\"start\":56131},{\"attributes\":{\"doi\":\"arXiv:1512.03385\",\"id\":\"b16\"},\"end\":56606,\"start\":56371},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14742646},\"end\":56951,\"start\":56608},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5106883},\"end\":57175,\"start\":56953},{\"attributes\":{\"doi\":\"2014. 4\",\"id\":\"b19\",\"matched_paper_id\":24215976},\"end\":57594,\"start\":57177},{\"attributes\":{\"id\":\"b20\"},\"end\":57871,\"start\":57596},{\"attributes\":{\"doi\":\"arXiv:1408.5093\",\"id\":\"b21\"},\"end\":58255,\"start\":57873},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14521054},\"end\":58510,\"start\":58257},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8517067},\"end\":58723,\"start\":58512},{\"attributes\":{\"id\":\"b24\"},\"end\":59182,\"start\":58725},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":195908774},\"end\":59536,\"start\":59184},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12259145},\"end\":59867,\"start\":59538},{\"attributes\":{\"id\":\"b27\"},\"end\":60181,\"start\":59869},{\"attributes\":{\"id\":\"b28\"},\"end\":60485,\"start\":60183},{\"attributes\":{\"id\":\"b29\"},\"end\":60878,\"start\":60487},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206592664},\"end\":61207,\"start\":60880},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7656505},\"end\":61658,\"start\":61209},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6941275},\"end\":62046,\"start\":61660},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13897053},\"end\":62335,\"start\":62048},{\"attributes\":{\"id\":\"b34\"},\"end\":62604,\"start\":62337},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5776545},\"end\":62990,\"start\":62606},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4236914},\"end\":63479,\"start\":62992},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2930547},\"end\":63962,\"start\":63481},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2930547},\"end\":64398,\"start\":63964},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":33059236},\"end\":64936,\"start\":64400},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10320166},\"end\":65353,\"start\":64938},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2750623},\"end\":65704,\"start\":65355},{\"attributes\":{\"id\":\"b42\"},\"end\":66216,\"start\":65706},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b43\"},\"end\":66482,\"start\":66218},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206592484},\"end\":66993,\"start\":66484},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1017389},\"end\":67301,\"start\":66995},{\"attributes\":{\"id\":\"b46\"},\"end\":67623,\"start\":67303},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11440692},\"end\":67819,\"start\":67625},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1169492},\"end\":68050,\"start\":67821},{\"attributes\":{\"id\":\"b49\"},\"end\":68374,\"start\":68052},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":206594383},\"end\":68702,\"start\":68376},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":53531567},\"end\":68985,\"start\":68704},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3104920},\"end\":69339,\"start\":68987},{\"attributes\":{\"doi\":\"arXiv:1512.04150\",\"id\":\"b53\"},\"end\":69636,\"start\":69341},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5714907},\"end\":69854,\"start\":69638}]", "bib_title": "[{\"end\":51488,\"start\":51458},{\"end\":51744,\"start\":51678},{\"end\":52096,\"start\":52014},{\"end\":52580,\"start\":52493},{\"end\":52975,\"start\":52910},{\"end\":53654,\"start\":53575},{\"end\":54163,\"start\":54112},{\"end\":54547,\"start\":54452},{\"end\":54952,\"start\":54880},{\"end\":55639,\"start\":55533},{\"end\":56169,\"start\":56131},{\"end\":56653,\"start\":56608},{\"end\":57017,\"start\":56953},{\"end\":57258,\"start\":57177},{\"end\":58329,\"start\":58257},{\"end\":58577,\"start\":58512},{\"end\":59247,\"start\":59184},{\"end\":59625,\"start\":59538},{\"end\":60974,\"start\":60880},{\"end\":61276,\"start\":61209},{\"end\":61758,\"start\":61660},{\"end\":62133,\"start\":62048},{\"end\":62692,\"start\":62606},{\"end\":63110,\"start\":62992},{\"end\":63530,\"start\":63481},{\"end\":64013,\"start\":63964},{\"end\":64515,\"start\":64400},{\"end\":65043,\"start\":64938},{\"end\":65447,\"start\":65355},{\"end\":65836,\"start\":65706},{\"end\":66514,\"start\":66484},{\"end\":67062,\"start\":66995},{\"end\":67663,\"start\":67625},{\"end\":67868,\"start\":67821},{\"end\":68149,\"start\":68052},{\"end\":68469,\"start\":68376},{\"end\":68772,\"start\":68704},{\"end\":69103,\"start\":68987},{\"end\":69685,\"start\":69638}]", "bib_author": "[{\"end\":51499,\"start\":51490},{\"end\":51510,\"start\":51499},{\"end\":51516,\"start\":51510},{\"end\":51528,\"start\":51516},{\"end\":51537,\"start\":51528},{\"end\":51548,\"start\":51537},{\"end\":51759,\"start\":51746},{\"end\":51770,\"start\":51759},{\"end\":52104,\"start\":52098},{\"end\":52115,\"start\":52104},{\"end\":52125,\"start\":52115},{\"end\":52142,\"start\":52125},{\"end\":52351,\"start\":52343},{\"end\":52360,\"start\":52351},{\"end\":52369,\"start\":52360},{\"end\":52595,\"start\":52582},{\"end\":52608,\"start\":52595},{\"end\":52619,\"start\":52608},{\"end\":52631,\"start\":52619},{\"end\":52645,\"start\":52631},{\"end\":52989,\"start\":52977},{\"end\":53000,\"start\":52989},{\"end\":53434,\"start\":53416},{\"end\":53447,\"start\":53434},{\"end\":53674,\"start\":53656},{\"end\":53685,\"start\":53674},{\"end\":53699,\"start\":53685},{\"end\":53713,\"start\":53699},{\"end\":53726,\"start\":53713},{\"end\":53736,\"start\":53726},{\"end\":53747,\"start\":53736},{\"end\":53761,\"start\":53747},{\"end\":54173,\"start\":54165},{\"end\":54181,\"start\":54173},{\"end\":54191,\"start\":54181},{\"end\":54200,\"start\":54191},{\"end\":54206,\"start\":54200},{\"end\":54217,\"start\":54206},{\"end\":54556,\"start\":54549},{\"end\":54564,\"start\":54556},{\"end\":54570,\"start\":54564},{\"end\":54578,\"start\":54570},{\"end\":54585,\"start\":54578},{\"end\":54593,\"start\":54585},{\"end\":54600,\"start\":54593},{\"end\":54607,\"start\":54600},{\"end\":54615,\"start\":54607},{\"end\":54964,\"start\":54954},{\"end\":54973,\"start\":54964},{\"end\":54981,\"start\":54973},{\"end\":55240,\"start\":55226},{\"end\":55254,\"start\":55240},{\"end\":55268,\"start\":55254},{\"end\":55280,\"start\":55268},{\"end\":55288,\"start\":55280},{\"end\":55301,\"start\":55288},{\"end\":55654,\"start\":55641},{\"end\":55670,\"start\":55654},{\"end\":55683,\"start\":55670},{\"end\":55943,\"start\":55930},{\"end\":55955,\"start\":55943},{\"end\":56181,\"start\":56171},{\"end\":56192,\"start\":56181},{\"end\":56204,\"start\":56192},{\"end\":56214,\"start\":56204},{\"end\":56423,\"start\":56417},{\"end\":56432,\"start\":56423},{\"end\":56439,\"start\":56432},{\"end\":56446,\"start\":56439},{\"end\":56665,\"start\":56655},{\"end\":56677,\"start\":56665},{\"end\":56687,\"start\":56677},{\"end\":56698,\"start\":56687},{\"end\":57028,\"start\":57019},{\"end\":57038,\"start\":57028},{\"end\":57270,\"start\":57260},{\"end\":57282,\"start\":57270},{\"end\":57292,\"start\":57282},{\"end\":57304,\"start\":57292},{\"end\":57313,\"start\":57304},{\"end\":57322,\"start\":57313},{\"end\":57685,\"start\":57672},{\"end\":57694,\"start\":57685},{\"end\":57707,\"start\":57694},{\"end\":57880,\"start\":57873},{\"end\":57893,\"start\":57880},{\"end\":57904,\"start\":57893},{\"end\":57915,\"start\":57904},{\"end\":57923,\"start\":57915},{\"end\":57935,\"start\":57923},{\"end\":57949,\"start\":57935},{\"end\":57960,\"start\":57949},{\"end\":58342,\"start\":58331},{\"end\":58354,\"start\":58342},{\"end\":58365,\"start\":58354},{\"end\":58591,\"start\":58579},{\"end\":58602,\"start\":58591},{\"end\":58826,\"start\":58815},{\"end\":58833,\"start\":58826},{\"end\":58842,\"start\":58833},{\"end\":58853,\"start\":58842},{\"end\":58861,\"start\":58853},{\"end\":58872,\"start\":58861},{\"end\":58880,\"start\":58872},{\"end\":58894,\"start\":58880},{\"end\":58903,\"start\":58894},{\"end\":58915,\"start\":58903},{\"end\":58928,\"start\":58915},{\"end\":58939,\"start\":58928},{\"end\":59263,\"start\":59249},{\"end\":59276,\"start\":59263},{\"end\":59288,\"start\":59276},{\"end\":59637,\"start\":59627},{\"end\":59646,\"start\":59637},{\"end\":59652,\"start\":59646},{\"end\":59881,\"start\":59871},{\"end\":59890,\"start\":59881},{\"end\":59902,\"start\":59890},{\"end\":59910,\"start\":59902},{\"end\":59920,\"start\":59910},{\"end\":59931,\"start\":59920},{\"end\":59940,\"start\":59931},{\"end\":59951,\"start\":59940},{\"end\":60273,\"start\":60261},{\"end\":60579,\"start\":60567},{\"end\":60592,\"start\":60579},{\"end\":60610,\"start\":60592},{\"end\":60622,\"start\":60610},{\"end\":60632,\"start\":60622},{\"end\":60645,\"start\":60632},{\"end\":60654,\"start\":60645},{\"end\":60985,\"start\":60976},{\"end\":60995,\"start\":60985},{\"end\":61005,\"start\":60995},{\"end\":61014,\"start\":61005},{\"end\":61292,\"start\":61278},{\"end\":61305,\"start\":61292},{\"end\":61771,\"start\":61760},{\"end\":61779,\"start\":61771},{\"end\":61792,\"start\":61779},{\"end\":61803,\"start\":61792},{\"end\":61818,\"start\":61803},{\"end\":61830,\"start\":61818},{\"end\":62143,\"start\":62135},{\"end\":62150,\"start\":62143},{\"end\":62158,\"start\":62150},{\"end\":62169,\"start\":62158},{\"end\":62177,\"start\":62169},{\"end\":62417,\"start\":62402},{\"end\":62428,\"start\":62417},{\"end\":62436,\"start\":62428},{\"end\":62702,\"start\":62694},{\"end\":62708,\"start\":62702},{\"end\":62717,\"start\":62708},{\"end\":62728,\"start\":62717},{\"end\":62735,\"start\":62728},{\"end\":62748,\"start\":62735},{\"end\":62761,\"start\":62748},{\"end\":63122,\"start\":63112},{\"end\":63128,\"start\":63122},{\"end\":63136,\"start\":63128},{\"end\":63148,\"start\":63136},{\"end\":63159,\"start\":63148},{\"end\":63167,\"start\":63159},{\"end\":63174,\"start\":63167},{\"end\":63185,\"start\":63174},{\"end\":63198,\"start\":63185},{\"end\":63547,\"start\":63532},{\"end\":63555,\"start\":63547},{\"end\":63561,\"start\":63555},{\"end\":63571,\"start\":63561},{\"end\":63583,\"start\":63571},{\"end\":63589,\"start\":63583},{\"end\":63598,\"start\":63589},{\"end\":63610,\"start\":63598},{\"end\":63620,\"start\":63610},{\"end\":63633,\"start\":63620},{\"end\":63641,\"start\":63633},{\"end\":63652,\"start\":63641},{\"end\":64030,\"start\":64015},{\"end\":64038,\"start\":64030},{\"end\":64044,\"start\":64038},{\"end\":64054,\"start\":64044},{\"end\":64066,\"start\":64054},{\"end\":64072,\"start\":64066},{\"end\":64081,\"start\":64072},{\"end\":64093,\"start\":64081},{\"end\":64103,\"start\":64093},{\"end\":64116,\"start\":64103},{\"end\":64526,\"start\":64517},{\"end\":64536,\"start\":64526},{\"end\":64547,\"start\":64536},{\"end\":64556,\"start\":64547},{\"end\":64566,\"start\":64556},{\"end\":64578,\"start\":64566},{\"end\":64587,\"start\":64578},{\"end\":64601,\"start\":64587},{\"end\":64611,\"start\":64601},{\"end\":64618,\"start\":64611},{\"end\":65053,\"start\":65045},{\"end\":65059,\"start\":65053},{\"end\":65066,\"start\":65059},{\"end\":65074,\"start\":65066},{\"end\":65081,\"start\":65074},{\"end\":65092,\"start\":65081},{\"end\":65457,\"start\":65449},{\"end\":65468,\"start\":65457},{\"end\":65474,\"start\":65468},{\"end\":65492,\"start\":65474},{\"end\":65499,\"start\":65492},{\"end\":65510,\"start\":65499},{\"end\":65846,\"start\":65838},{\"end\":65854,\"start\":65846},{\"end\":65861,\"start\":65854},{\"end\":65867,\"start\":65861},{\"end\":65873,\"start\":65867},{\"end\":65883,\"start\":65873},{\"end\":65890,\"start\":65883},{\"end\":65901,\"start\":65890},{\"end\":65912,\"start\":65901},{\"end\":66298,\"start\":66286},{\"end\":66311,\"start\":66298},{\"end\":66527,\"start\":66516},{\"end\":66534,\"start\":66527},{\"end\":66541,\"start\":66534},{\"end\":66553,\"start\":66541},{\"end\":66561,\"start\":66553},{\"end\":66573,\"start\":66561},{\"end\":66582,\"start\":66573},{\"end\":66595,\"start\":66582},{\"end\":66609,\"start\":66595},{\"end\":67075,\"start\":67064},{\"end\":67082,\"start\":67075},{\"end\":67098,\"start\":67082},{\"end\":67110,\"start\":67098},{\"end\":67121,\"start\":67110},{\"end\":67131,\"start\":67121},{\"end\":67400,\"start\":67386},{\"end\":67418,\"start\":67400},{\"end\":67428,\"start\":67418},{\"end\":67443,\"start\":67428},{\"end\":67676,\"start\":67665},{\"end\":67685,\"start\":67676},{\"end\":67695,\"start\":67685},{\"end\":67706,\"start\":67695},{\"end\":67881,\"start\":67870},{\"end\":67891,\"start\":67881},{\"end\":67901,\"start\":67891},{\"end\":67910,\"start\":67901},{\"end\":68163,\"start\":68151},{\"end\":68171,\"start\":68163},{\"end\":68180,\"start\":68171},{\"end\":68477,\"start\":68471},{\"end\":68485,\"start\":68477},{\"end\":68493,\"start\":68485},{\"end\":68501,\"start\":68493},{\"end\":68512,\"start\":68501},{\"end\":68520,\"start\":68512},{\"end\":68781,\"start\":68774},{\"end\":69114,\"start\":69105},{\"end\":69121,\"start\":69114},{\"end\":69131,\"start\":69121},{\"end\":69146,\"start\":69131},{\"end\":69405,\"start\":69397},{\"end\":69415,\"start\":69405},{\"end\":69428,\"start\":69415},{\"end\":69437,\"start\":69428},{\"end\":69449,\"start\":69437},{\"end\":69694,\"start\":69687},{\"end\":69703,\"start\":69694},{\"end\":69716,\"start\":69703},{\"end\":69727,\"start\":69716}]", "bib_venue": "[{\"end\":51368,\"start\":51321},{\"end\":51552,\"start\":51548},{\"end\":51825,\"start\":51770},{\"end\":52146,\"start\":52142},{\"end\":52341,\"start\":52302},{\"end\":52678,\"start\":52645},{\"end\":53089,\"start\":53000},{\"end\":53414,\"start\":53380},{\"end\":53816,\"start\":53761},{\"end\":54256,\"start\":54217},{\"end\":54642,\"start\":54615},{\"end\":54990,\"start\":54981},{\"end\":55224,\"start\":55123},{\"end\":55710,\"start\":55683},{\"end\":56220,\"start\":56214},{\"end\":56415,\"start\":56371},{\"end\":56760,\"start\":56698},{\"end\":57044,\"start\":57038},{\"end\":57373,\"start\":57329},{\"end\":57670,\"start\":57596},{\"end\":58035,\"start\":57975},{\"end\":58369,\"start\":58365},{\"end\":58606,\"start\":58602},{\"end\":58813,\"start\":58725},{\"end\":59337,\"start\":59288},{\"end\":59685,\"start\":59652},{\"end\":60259,\"start\":60183},{\"end\":60565,\"start\":60487},{\"end\":61023,\"start\":61014},{\"end\":61382,\"start\":61305},{\"end\":61834,\"start\":61830},{\"end\":62181,\"start\":62177},{\"end\":62400,\"start\":62337},{\"end\":62767,\"start\":62761},{\"end\":63204,\"start\":63198},{\"end\":63692,\"start\":63652},{\"end\":64156,\"start\":64116},{\"end\":64645,\"start\":64618},{\"end\":65128,\"start\":65092},{\"end\":65514,\"start\":65510},{\"end\":65939,\"start\":65912},{\"end\":66284,\"start\":66218},{\"end\":66686,\"start\":66609},{\"end\":67135,\"start\":67131},{\"end\":67384,\"start\":67303},{\"end\":67710,\"start\":67706},{\"end\":67914,\"start\":67910},{\"end\":68202,\"start\":68180},{\"end\":68524,\"start\":68520},{\"end\":68822,\"start\":68781},{\"end\":69150,\"start\":69146},{\"end\":69395,\"start\":69341},{\"end\":69731,\"start\":69727},{\"end\":53165,\"start\":53091},{\"end\":61446,\"start\":61384},{\"end\":66750,\"start\":66688}]"}}}, "year": 2023, "month": 12, "day": 17}