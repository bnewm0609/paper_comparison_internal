{"id": 234681065, "updated": "2023-10-06 02:59:06.746", "metadata": {"title": "Ordering-Based Causal Discovery with Reinforcement Learning", "authors": "[{\"first\":\"Xiaoqiang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yali\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Shengyu\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Liangjun\",\"last\":\"Ke\",\"middle\":[]},{\"first\":\"Zhitang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jianye\",\"last\":\"Hao\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2021, "month": 5, "day": 14}, "abstract": "It is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery from observational data. However, searching the space of directed graphs and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the existing RL-based method to small scale problems. In this work, we propose a novel RL-based approach for causal discovery, by incorporating RL into the ordering-based paradigm. Specifically, we formulate the ordering search problem as a multi-step Markov decision process, implement the ordering generating process with an encoder-decoder architecture, and finally use RL to optimize the proposed model based on the reward mechanisms designed for~each ordering. A generated ordering would then be processed using variable selection to obtain the final causal graph. We analyze the consistency and computational complexity of the proposed method, and empirically show that a pretrained model can be exploited to accelerate training. Experimental results on both synthetic and real data sets shows that the proposed method achieves a much improved performance over existing RL-based method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.06631", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/WangDZKCHW21", "doi": "10.24963/ijcai.2021/491"}}, "content": {"source": {"pdf_hash": "de148b6b12cb9e63ee90b66b62c835a162862933", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.06631v4.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2021/0491.pdf", "status": "BRONZE"}}, "grobid": {"id": "9317b8c8d11beff964cd24bc98f7183601520d29", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/de148b6b12cb9e63ee90b66b62c835a162862933.txt", "contents": "\nOrdering-Based Causal Discovery with Reinforcement Learning\n\n\nXiaoqiang Wang \nSchool of Automation Science and Engineering\nState Key Laboratory for Manufacturing Systems Engineering\nXi'an Jiaotong University\n\n\nYali Du yali.dux@gmail.com \nUniversity College London\n3 Huawei Noah'\n\nShengyu Zhu zhushengyu@huawei.com \n\u2020 \nLiangjun Ke \nSchool of Automation Science and Engineering\nState Key Laboratory for Manufacturing Systems Engineering\nXi'an Jiaotong University\n\n\n\u2020 \nZhitang Chen \nJianye Hao haojianye@huawei.com \nCollege of Intelligence and Computing\nTianjin University\n\n\nJun Wang jun.wang@cs.ucl.ac.uk \nUniversity College London\n3 Huawei Noah'\n\n\nArk Lab\n\n\nOrdering-Based Causal Discovery with Reinforcement Learning\n\nIt is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery from observational data. However, searching the space of directed graphs and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the existing RL-based method to small scale problems. In this work, we propose a novel RL-based approach for causal discovery, by incorporating RL into the ordering-based paradigm. Specifically, we formulate the ordering search problem as a multi-step Markov decision process, implement the ordering generating process with an encoder-decoder architecture, and finally use RL to optimize the proposed model based on the reward mechanisms designed for each ordering. A generated ordering would then be processed using variable selection to obtain the final causal graph. We analyze the consistency and computational complexity of the proposed method, and empirically show that a pretrained model can be exploited to accelerate training. Experimental results on both synthetic and real data sets shows that the proposed method achieves a much improved performance over existing RL-based method.\n\nIntroduction\n\nIdentifying causal structure from observational data is an important but also challenging task in many practical applications. This task can be formulated as that of finding a Directed Acyclic Graph (DAG) that minimizes a score function defined w.r.t. the observed data. However, searching over the space of DAGs for the best DAG is known to be NP-hard, even if each node has at most two parents [Chickering, 1996]. Consequently, traditional methods mostly rely on local heuristics to perform the search, including greedy hill-climbing and greedy equivalence search that explores the Markov equivalence classes [Chickering, 2002].\n\nAlong with various search strategies, existing methods have also cast causal structure learning problem as that of learning an optimal variable ordering, considering that the ordering space is significantly smaller than that of directed graphs and searching over the ordering space can avoid dealing with the acyclicity constraint [Teyssier and Koller, 2005]. Many methods, such as genetic algorithm [Larranaga et al., 1996], Markov chain Monte Carlo [Friedman and Koller, 2003] and greedy local hill-climbing [Teyssier and Koller, 2005], have been exploited as the search strategies to find desired orderings. In practice, however, these methods often cannot effectively find a globally optimal ordering for their heuristic nature.\n\nRecently, with smooth score functions, several gradientbased methods have been proposed by exploiting a smooth characterization of acyclicity, including NOTEARS [Zheng et al., 2018] for linear causal models and several subsequent works, e.g., [Yu et al., 2019;Lachapelle et al., 2020;Ng et al., 2019b;Ng et al., 2019a;Zheng et al., 2020], which use neural networks for modelling non-linear causal relationships. As another attempt, [Zhu et al., 2020] utilize Reinforcement Learning (RL) to find the underlying DAG from the graph space without the need of smooth score functions. Unfortunately, [Zhu et al., 2020] achieved good performance only with up to 30 variables, for at least two reasons: 1) the action space, consisting of directed graphs, is tremendous for large scale problems and is hard to be explored efficiently; and 2) it has to compute scores for many non-DAGs generated during training but computing scores w.r.t. data is generally time-consuming. It appears that the RL-based approach may not be able to achieve a close performance to other gradientbased methods that directly optimize the same score function for large causal discovery problems, due to its search nature.\n\nBy taking advantage of the reduced space of variable orderings and the strong search ability of modern RL methods, we propose Causal discovery with Ordering-based Reinforcement Learning (CORL), which incorporates RL into the orderingbased paradigm and is shown to achieve a promising empirical performance. In particular, CORL outperforms NOTEARS, a state-of-the-art gradient-based method for linear data, even with 150-node graphs. Meanwhile, CORL is also competitive with a strong baseline, Causal Additive Model (CAM) method , on non-linear data models.\n\nContributions. We make the following contributions in this work: 1) We formulate the ordering search problem as a multistep Markov Decision Process (MDP) and propose to implement the ordering generating process in an effective encoderdecoder architecture, followed by applying RL to optimizing the proposed model based on specifically designed reward mechanisms. We also incorporate a pretrained model into CORL to accelerate training. 2) We analyze the consistency and computational complexity of the proposed method. 3) We conduct comparative experiments on synthetic and real data sets to validate the performance of the proposed methods. 4) An implementation has been made available at https://github. com/huawei-noah/trustworthyAI/tree/master/gcastle. 1\n\n\nRelated Works\n\nBesides the aforementioned heuristic ordering search algorithms, [Schmidt et al., 2007] proposed L1OBS to conduct variable selection using 1 -regularization paths based on the method from [Teyssier and Koller, 2005]. [Scanagatta et al., 2015] further proposed an ordering exploration method on the basis of an approximated score function so as to scale to thousands of variables. The CAM method [B\u00fchlmann et al., 2014] was specifically designed for non-linear additive models. Some recent ordering-based methods such as sparsest permutation [Raskutti and Uhler, 2018] and greedy sparsest permutation [Solus et al., 2017] can guarantee consistency of Markov equivalence class, relying on some conditional independence relations and certain assumptions like faithfulness. A variant of greedy sparest permutation was further proposed in [Bernstein et al., 2020] for the setting with latent variables. In the present work, we mainly work on identifiable cases which have different assumptions from theirs.\n\nIn addition, exact algorithms such as dynamic programming [Xiang and Kim, 2013] and integer or linear programming [Bartlett and Cussens, 2017] are also used for causal discovery problem. However, these algorithms usually can only work on small graphs [De Campos and Ji, 2011], and to handle larger problems with hundreds of variables, they usually need to incorporate heuristics search [Xiang and Kim, 2013] or limit the maximum number of parents of each node.\n\nRecently, RL has been used to tackle several combinatorial problems such as the maximum cut and traveling salesman problem [Bello et al., 2016;Khalil et al., 2017;Kool et al., 2019]. These works aim to learn a policy as a solver based on the particular type of combinatorial problems. However, causal discovery tasks generally have different relationships, data types, graph structures, etc., and moreover, are typically off-line with focus on a or a class of causal graph(s). As such, we use RL as a search strategy, similar to [Zoph and Le, 2017;Zhu et al., 2020]. Nevertheless, a pretrained model or policy can offer a good starting point to speed up training, as shown in our evaluation results (cf. Figure 3). 1 The extended version can be found at https://arxiv.org/abs/2105. 06631. Figure 1: An example of the correspondence between an ordering and a fully-connected DAG.\n\n\nBackground\n\n\nCausal Structure Learning\n\nLet G = (d, V, E) denotes a DAG, with d the number of nodes, V = {v 1 , \u00b7 \u00b7 \u00b7 , v d } the set of nodes, and E = {(v i , v j )|i, j = 1, . . . , d} the set of directed edges from v i to v j . Each node v j is associated with a random variable X j . The probability model associated with G factorizes as p(X 1 , \u00b7 \u00b7 \u00b7 , X d ) = d j=1 p(X j |Pa(X j )), where p(X j |Pa(X j )) is the conditional probability distribution for X j given its parents Pa(X j ) := {X k |(v k , v j ) \u2208 E}. We assume that the observed data x j is obtained by the Structural Equation Model (SEM) with additive noises: X j := f j (Pa(X j )) + j , j = 1, . . . , d, where f j represents the functional relationship between X j and its parents, and j 's denote jointly independent additive noise variables. We assume causal minimality, which is equivalent to that each f j is not a constant for any X k \u2208 Pa(X j ) in this SEM .\n\nGiven a sample X = [x 1 , \u00b7 \u00b7 \u00b7 , x d ] \u2208 R m\u00d7d where x j is a vector of m observations for random variable X j . The goal is to find a DAG G that optimizes the Bayesian Information Criterion (BIC) (or equivalently, minimum description length) score, defined as\nS BIC (G)= d j=1 m k=1 log p(x k j |Pa(x k j ); \u03b8 j )\u2212 |\u03b8 j | 2 log m ,(1)\nwhere x k j is the k-th observation of X j , \u03b8 j is the parameter associated with each likelihood, and |\u03b8 j | denotes the parameter dimension. For linear-Gaussian models, p(x k j |Pa(x k j ); \u03b8 j ) = N (x j |\u03b8 T j Pa(x j ), \u03c3 2 ) and \u03c3 2 can be estimated from the data. The problem of finding a directed graph that satisfies the ayclicity constraint can be cast as that of finding a variable ordering [Teyssier and Koller, 2005;Schmidt et al., 2007]. Specifically, let \u03a0 denote an ordering of the nodes in V , where the length of the ordering |\u03a0| = |V | and \u03a0 is indexed from 1. If node v j \u2208 V lies in the p-th position, then \u03a0(p) = v j . Notation \u03a0 \u227avj denotes the set of nodes that precede node v j in \u03a0. One can easily establish a canonical correspondence between an ordering \u03a0 and a fully-connected DAG G \u03a0 ; an example is presented in Figure 1. A DAG G can be consistent with more than one orderings and the set of these orderings is denoted by \u03a6(\u03a0)={\u03a0 : fully-connected DAG G \u03a0 is a super-DAG of G}, where a super-DAG of G is a DAG whose edge set is a superset of that of G. The the search for the true DAG G * can be decomposed to two phases: finding the correct ordering and performing variable selection; the latter is to find the optimal DAG that is consistent with the ordering found in the first step.\n\n\nReinforcement Learning\n\nStandard RL is usually formulated as an MDP over the environment state s \u2208 S and agent action a \u2208 A, under an (unknown) environmental dynamics defined by a transition probability T (s |s, a). Let \u03c0 \u03c6 (a|s) denote the policy, parameterized by \u03c6, which outputs a distribution used to select an action from action space A based on state s. For episodic tasks, a trajectory \u03c4 = {s t , a t } T t=0 , where T is the finite time horizon, can be collected by executing the policy repeatedly. In many cases, an immediate reward r(s, a) can be received when agent executes an action. The objective of RL is to learn a policy which can maximize the expected cumulative reward along a trajectory, i.e., J(\u03c6) = E \u03c0 \u03c6 [R 0 ] with R 0 = T t=0 \u03b3 t r t (s t , a t ) and \u03b3 \u2208 (0, 1] being a discount factor. For some scenarios, the reward is only earned at the terminal time (also called episodic reward), and J\n(\u03c6) = E \u03c0 \u03c6 [R(\u03c4 )] with R(\u03c4 ) = r T (s T , a T ).\n\nMethod\n\nIn this section, we first formulate the ordering search problem as an MDP and then describe the proposed approach. We also discuss the variable selection methods to obtain DAGs from variable orderings, as well as the consistency and computational complexity regarding the proposed method.\n\n\nOrdering Search as Markov Decision Process\n\nTo incorporate RL into the ordering-based paradigm, we formulate the variable ordering search problem as a multi-step decision process with a variable as an action at each decision step, and the order of the selected actions (or variables) is treated as the searched ordering. The decision-making process is Markovian, and its elements are described as follows.\n\nState. One can directly take the sample data x j as the state. However, preliminary experiments (see Appendix A.1) show that it is difficult for feed-forward neural network models to capture the underlying causal relationships directly using observed data as states, and that the data pre-processed by an encoder module is helpful to find better orderings. The encoder module embeds each x j to state s j and all the embedded states constitute the state space S := {s 1 , \u00b7 \u00b7 \u00b7 , s d }. In our case, we also need an initial state, denoted by s 0 (detailed choice is given in Section 4.2), to select the first action. The complete state space would be\u015c := S \u222a {s 0 }. We will use\u015d t to denote the actual state encountered at the t-th decision step when generating a variable ordering.\n\nAction. We select an action (variable) from the action space A := {v 1 , \u00b7 \u00b7 \u00b7 , v d } consisting of all the variables at each decision step, and the action space size is equal to the number of variables, i.e., |A| = d. Compared to the previous RL-based method that searches over the graph space with size O(2 d\u00d7d ) [Zhu et al., 2020], the resulting action space becomes much smaller. State transition. The state transition is related to the action selected at the current decision step. If the selected variable is v j at the t-th decision step, then the state is transferred to the state s j \u2208 S which corresponds to x j embedded by the encoder, i.e.,\u015d t+1 = s j .\n\nReward. In ordering-based methods, only the variables selected in previous decision steps can be the potential parents of the currently selected variable. Hence, we design the rewards in the following cases: episodic reward and dense reward. In the former case, we calculate the score for a variable ordering \u03a0 with d variables as the episodic reward, i.e.,\nR(\u03c4 ) = r T (\u015d T , a T ) = S BIC (G \u03a0 )(2)\nwhere T = d \u2212 1 and S BIC has been defined in Equation (1), with Pa(X j ) replaced by the potential parent variable set U (X j ); here U (X j ) denotes the variables associated with the nodes in \u03a0 \u227avj . If the score function is decomposable (e.g., the BIC score), we can calculate an immediate reward by exploiting the decomposability for the current decision step. That is, for v j selected at time step t, the immediate reward is\nr t = m k=1 log p(x k j |U (x k j ); \u03b8 j ) \u2212 |\u03b8 j | 2 log m.(3)\nThis belongs the second case with dense rewards. Here we keep \u2212|\u03b8 j |/2 log m to make Equation (3) consistent with the form of BIC score.\n\n\nImplementation and Optimization with Reinforcement Learning\n\nWe briefly describe the neural network architectures implemented in our method, as shown in Figure 2. More details can be found in Appendix A.\n\nEncoder. f enc \u03c6e :X \u2192 S is used to map the observed data to the embedding space S = {s 1 , \u00b7 \u00b7 \u00b7 , s d }. Similar to [Zhu et al., 2020], we adopt mini-batch training and randomly draw n samples from m samples of the data set X to constructX \u2208 R n\u00d7d at each episode. We also set the embedding s j to be in the same dimension, i.e., s j \u2208 R n . For encoder choice, we conduct an empirical comparison among several representative structures such as MLP, LSTM and the self-attention based encoder [Vaswani et al., 2017]. Empirically, we validate that the self-attention based encoder in the Transformer structure performs the best (see Appendix A.1).\n\nDecoder. f dec \u03c6 d :\u015c \u2192 A maps the state space\u015c to the action space A. Among several decoder choices (see also Appendix A.1 for an empirical comparison), we pick an LSTM based structure that proves effective in our experiments. Although the initial state is generated randomly in many applications, we pick it as s 0 = 1 d d i=1 s i , considering that the source node is fixed in a correct ordering. We restrict each node only be selected once by masking the selected nodes, in order to generate a valid ordering .\n\nOptimization. The optimization objective is to learn a policy maximizing J(\u03c6), where \u03c6 = {\u03c6 e , \u03c6 d } with \u03c6 e and \u03c6 d being parameters associated with encoder f enc and decoder f dec , respectively. Based on the above definition, policy gradient [Sutton and Barto, 2018] is used to optimize the ordering generation model parameters. For the episodic reward case, we have the following policy gradi-\nent \u2207J(\u03c6) = E \u03c0 \u03c6 R(\u03c4 ) T t=0 \u2207 \u03c6 log \u03c0 \u03c6 (a t |\u015d t )\n, and the algorithm in this case is denoted as CORL-1. For the dense reward case, policy gradient can be calculated as\n\u2207J(\u03c6) = E \u03c0 \u03c6 T t=0 R t \u2207 \u03c6 log \u03c0 \u03c6 (a t |\u015d t ) , where R t = T \u2212t\nl=0 \u03b3 l r t+l denotes the return at time step t. We denote the algorithm in this case as CORL-2. Using a parametric baseline to estimate the expected score typically improves learning [Sutton and Barto, 2018]. Therefore, we introduce a critic network V \u03c6v (\u015d t ) parameterized by \u03c6 v , which learns the expected return given state\u015d t and is trained with stochastic gradient descent using Adam optimizer on a mean squared error objective between its predicted value and the actual return. More details about the critic network are described in Appendix A.2.\n\nInspired by the benefits from pretrained models [Bello et al., 2016], we also consider to incorporate pretraining to our method to accelerate training. In practice, one can usually obtain some observed data with known causal graphs or correct orderings, e.g., by simulation or real data with labeled graphs. Hence, we can pretrain a policy model with such data in a supervised way and use the pretrained model as initialization for new tasks. Meanwhile, a sufficient generalization ability is desired and we hence include diverse data sets with different numbers of nodes, noise types, causal relationships, etc.\n\n\nVariable Selection\n\nOne can obtain the causal graph from an ordering by conducting variable selection methods, such as sparse candidate [Teyssier and Koller, 2005], significance testing of covariates , and group Lasso [Schmidt et al., 2007]. In this work, for linear data models, we apply linear regression to the obtained fully-connected DAG and then use thresholding to prune edges with small weights, as similarly used by [Zheng et al., 2018]. For the non-linear model, we adopt the CAM pruning used by [Lachapelle et al., 2020]. For each variable X j , one can fit a generalized additive model against the current parents of X j and then apply significance testing of covariates, declaring significance if the reported p-values are lower that or equal to 0.001. The overall method is summarized in Algorithm 1.\n\nAlgorithm 1 Causal discovery with Ordering-based RL.\n\nRequire: observed data X, initial parameters \u03c6 e , \u03c6 d and \u03c6 v , two empty buffers D and D score , initial value (negative infinite) BestScore and a random ordering BestOrdeing. 1: while not terminated do 2: draw a batch of samples from X, encode them to S and calculate the initial state\u015d 0 3: \nfor t = 0, 1, . . . , T do 4: collect a batch of data \u015d t , a t , r t with \u03c0 \u03c6 : D = D \u222a { \u015d t , a t , r t } 5: if v t , \u03a0 \u227avt , r t is not in D score then 6: store v t , \u03a0 \u227avt , r t in D\n\nConsistency Analysis\n\nSo far we have presented CORL in a general manner without specifying explicitly the distribution family for calculating the scores or rewards. In principle, any distribution family could be employed as long as its log-likelihood can be computed. However, whether the maximization of the accumulated reward recovers the correct ordering, i.e., whether consistency of the score function holds, depends on both the modelling choice of reward and the underlying SEM. If the SEM is identifiable, then the following proposition shows that it is possible to find the correct ordering with high probability in the large sample limit. Proposition 1. Suppose that an identifiable SEM with true causal DAG G * on X = {X j } d j=1 induces distribution P (X). Let G \u03a0 be the fully-connected DAG that corresponds to an ordering \u03a0. If there is an SEM with G \u03a0 inducing the same distribution P (X), then G \u03a0 must be a super-graph of G * , i.e., every edge in G * is covered in G \u03a0 .\n\nProof. The SEM with G \u03a0 may not be causally minimal but can be reduced to an SEM satisfying the causal minimality condition . LetG \u03a0 denotes the causal graph in the reduced SEM with the same distribution P (X). Since we have assumed that original SEM is identifiable, i.e., the distribution P (X) corresponds to a unique true graph,G \u03a0 is then identical to G * . The proof is complete by noticing that G \u03a0 is a super-graph ofG \u03a0 .\n\nThus, if the causal relationships fall into the chosen model functions and a right distribution family is assumed, then given infinite samples the optimal accumulated reward (e.g., the optimal BIC score) must be achieved by a super-DAG of the underlying graph. However, finding the optimal accumulated reward may be hard, because policy gradient methods only guarantee local convergence [Sutton and Barto, 2018], and we can only apply approximate model functions and also need to  assume a certain distribution family for calculating the reward. Nevertheless, the experimental results in Section 5 show that the proposed method can achieve a better performance than those with consistency guarantee in the finite sample regime, thanks to the improved search ability of modern RL methods.\n\n\nComputational Complexity\n\nIn contrast with typical RL applications, we treat RL here as a search strategy, aiming to find an ordering that achieves the best score. CORL requires the evaluation of the rewards at each episode with O(dm 2 + d 3 ) computational cost if linear functions are adopted to model the causal relations, which is same to RL-BIC2 [Zhu et al., 2020]. Fortunately, CORL does not need to compute the matrix exponential term with O(d 3 ) cost due to the use of ordering search. We observe that CORL performs fewer episodes than RL-BIC2 before the episode reward converges (see Appendix C). The evaluation of Transformer encoder and LSTM decoder in CORL take O(nd 2 ) and O(dn 2 ), respectively. However, we find that computing rewards is dominating in the total running time (e.g., around 95% and 87% for 30-and 100-node linear data models). Thus, we record the decomposed scores for each variable v j with different parental sets \u03a0 \u227avj to avoid repeated computations.\n\n\nExperiments\n\nIn this section, we conduct experiments on synthetic data sets with linear and non-linear causal relationships as well as a real data set.  nodes, causal functions and sample size. Two types of graph sampling schemes, Erd\u00f6s-R\u00e9nyi (ER) and Scale-free (SF), are considered here. We denote d-node ER and SF graphs with on average hd edges as ERh and SFh, respectively. Two common metrics are considered: True Positive Rate (TPR) and Structural Hamming Distance (SHD). The former indicates the probability of correctly finding the positive edges among the discoveries. Hence, it can be used to measure the quality of an ordering, and the higher the better. The latter counts the total number of missing, falsely detected or reversed edges, and the smaller the better.\n\n\nLinear Models with Gaussian and Non-Gaussian Noise\n\nWe evaluate the proposed methods on Linear Gaussian (LG) with equal variance Gaussian noise and LiNGAM data models, and the true DAGs in both cases are known to be identifiable {2, 5} and d \u2208 {30, 50, 100} to generate observed data (see Appendix B.2 for details). For variable selection, we set the thresholding as 0.3 and apply it to the estimated coefficients, as similarly used by [Zheng et al., 2018;Zhu et al., 2020]. Table 1 presents the results for 30-and 100-node LG data models; the conclusions do not change with 50-node graphs, which are given in Appendix D. The performances of ICA-LiNGAM, GraN-DAG and CAM is also given in Appendix D, and they are almost never on par with the best methods presented in this section. CORL-1 and CORL-2 achieve consistently good results on LiNGAM data sets which are reported in Appendix E due to the space limit.\n\nWe now examine Table 1 (the values in parentheses represent the standard deviation across data sets per task). Across all settings, CORL-1 and CORL-2 are the best performing methods in terms of both TPR and SHD, while NOTEARS and DAG-GNN are not too far behind. In Figure 3, we further show the training reward curves of CORL-1 and CORL-2 on 100-node LG data sets, where CORL-2 converges faster to a better ordering than CORL-1. We conjecture that this is be-cause dense rewards can provide more guidance information for the training process than episodic rewards, which is beneficial to the learning of RL model and improves the training performance. Hence, CORL-2 is preferred in practice if the score function is decomposable for each variable. As discussed previously, RL-BIC2 only achieves satisfactory results on graphs with 30 nodes. The TPR of L1OBS is lower than that of A* Lasso, which indicates that L1OBS using greedy hill-climbing with tabu lists may not find a good ordering. Note that the SHD of L1OBS and A* Lasso reported here are the results after applying the introduced pruning method. We observe that the SHDs are greatly improved after pruning. For example, the SHDs of L1OBS decrease from 171.6 (29.5), 588.0 (66.2) and 1964.5 (136.6) to 85.2 (23.8), 215.4 (26.3) and 481.2 (49.9) for ER2 graphs with 30, 50 and 100 nodes, respectively, while the TPRs almost keep the same.\n\nWe have also evaluated our method on 150-node LG data models on ER2 graphs. CORL-1 has TPR and SHD being 0.95 (0.01) and 63.7 (9.1), while CORL-2 has 0.97 (0.01) and 38.3 (14.3), respectively. CORL-2 outperforms NOTEARS that achieves 0.94 (0.02) and 50.8 (21.8).\n\nPretraining. We show the training reward curve of CORL-2pretrain in Figure 3, where the model parameters are pretrained in a supervised manner. The data sets used for pretraining contain 30-node ER2 and SF2 graphs with different causal relationships. Note that the data sets used for evaluation are different from those used for pretraining. Compared to that of CORL-2 using random initialization, a pretrained model can accelerate the model learning process. Although pretraining requires additional time, it is only carried out once and when finished, the pretrained model can be used for multiple causal discovery tasks. Similar conclusion can be drawn in terms of CORL-1, which is shown in Appendix G.\n\nRunning time. We also report the running time of all the methods on 30-and 100-node linear data models: CORL-1, CORL-2, GraN-DAG and DAG-GNN \u2248 15 minutes for 30-node graphs; CORL-1 and CORL-2 \u2248 7 hours against GraN-DAG and DAG-GNN \u2248 4 hours for 100-node graphs; CAM \u2248 15 minutes for both 30-and 100-node graphs, while L1OBS and A* Lasso \u2248 2 minutes for that tasks; NOTEARS \u2248 5 minutes and \u2248 1 hour for the two tasks respectively; RL-BIC2 \u2248 3 hours for 30-node graphs. We set the maximal running time up to 15 hours, but RL-BIC2 did not converge on 100-node graphs, hence we did not report its results. Note that the running time can be significantly reduced by paralleling the evaluation of reward. The neural network based learning methods generally take longer time, and the proposed method achieves the best performance among these methods.\n\n\nNon-Linear Model with Gaussian Process\n\nIn this experiment, we consider causal relationships with f j being a function sampled from a Gaussian Process (GP) with radial basis function kernel of bandwidth one. The additive noise follows standard Gaussian distribution, which is known to be identifiable . We consider ER1 and ER4 graphs with different sample numbers (see Appendix B.2 for the generation of data sets), and we only report the results with m = 500 samples due to the space limit (the remaining results are given in Appendix F). For comparison, only the methods that have been shown competitive for this non-linear data model in existing works [Zhu et al., 2020;Lachapelle et al., 2020] are included. For a given ordering, we follow [Zhu et al., 2020] to use GP regression to fit the causal relationships. We also set a maximum time limit of 15 hours for all the methods for fair comparison and only graphs with up to 30 nodes are considered here, as using GP regression to calculate the scores is time-consuming. The variable selection method used here is the CAM pruning from .\n\nThe results on 10-and 30-node data sets with ER1 and ER4 graphs are shown in Figure 4. Overall, both GraN-DAG and DAG-GNN perform worse than CAM. We conjecture that this is because the number of samples are not sufficient for GraN-DAG and DAG-GNN to fit neural networks well, as also shown by [Lachapelle et al., 2020]. CAM, CORL-1, and CORL-2 have similar results, with CORL-2 performing the best on 10-node graphs and being slightly worse than CAM on 30node graphs. All of these methods have better results on ER1 graphs than on ER4 graphs, especially with 30 nodes. We also notice that CORL-2 only runs about 700 iterations on 30-node graphs and about 5000 iterations on 10-node graphs within the time limit, due to the increased time from GP regression. Nonetheless, the proposed method achieves a much improved performance compared with the existing RL-based method.\n\n\nReal Data\n\nThe Sachs data set [Sachs et al., 2005], with 11-node and 17-edge true graph, is widely used for research on graphical models. The expression levels of protein and phospholipid in the data set can be used to discover the implicit protein signal network. The observational data set has m = 853 samples and is used to discover the causal structure. We similary use Gaussian Process regression to model the causal relationships in calculating the score. In this experiment, CORL-1, CORL-2 and RL-BIC2 achieve the best SHD 11. CAM, GraN-DAG, and ICA-LiNGAM achieve SHDs 12, 13 and 14, respectively. Particularly, DAG-GNN and NOTEARS result in SHDs 16 and 19, respectively, whereas an empty graph has SHD 17.\n\n\nConclusion\n\nIn this work, we have incorporated RL into the ordering-based paradigm for causal discovery, where a generated ordering can be pruned by variable selection to obtain the causal DAG. Two methods are developed based on the MDP formulation and an encoder-decoder framework. We further analyze the consistency and computational complexity for the proposed approach. Empirical results validate the improved performance over existing RL-based causal discovery approach.\n\nThere are a variety of neural network modules that can be used for encoder and decoder architectures. Here we consider some representative modules, including: Multi Layer Perceptrons (MLP) module, an LSTM based recurrent neural network module, and the self-attention based encoder from the Transformer structure. In addition, we use the original observational data as the state directly, i.e., no encoder module is used, which is denoted as Null. More details regarding the architectures and associated hyper-parameters choices will be presented in Appendix A.2. Table 1 reports the empirical results of CORL-2 on 30node LG ER2 data sets where the noise variances are equal (see Appendix B.2 for details about data generation). We observe that the LSTM based decoder achieves a better performance than that of MLP based decoder, which indicates that LSTM is more effective than MLP in sequential decision tasks. The overall performance of neural network encoders is better than that of Null, which shows that the data pre-processed by an encoder module is necessary. Among all these encoders, Transformer encoder achieves the best results. Similar conclusion was drawn in [Zhu et al., 2020], and we hypothesize that the performance of Transformer encoder benefits from the self-attention scheme that provide sufficient interactions amongst variables.\n\n\nA.2 Model Architectures and Hyper-Parameters\n\nThe neural network structure of the Transformer encoder used in our experiments is given in Figure 1. It consists of a feedforward layer with 256 units and three blocks. Each block is composed of a multi-head attention network with 8 heads and 2-layer feed-forward neural networks with 1024 and 256 units, and each feed-forward layer is followed by a normalization layer. Given a batch of observed samples with shape b \u00d7 d \u00d7 n, with b denoting the batch size, d the node number and n the number of observed data in a batch, the final output of the encoder is a batch of embedded state with shape b \u00d7 d \u00d7 256.\n\nWe illustrate the neural network structure of the LSTM based decoder in Figure 2, which is similar to the decoder proposed by . The LSTM takes a state as input and outputs an embedding. The embedding is mapped to the action space A by using some feed-forward neural networks, a soft-max module and the pointer mechanism . The LSTM module with 256 hidden units is used here. The outputs of encoder are processed as the initial hidden state h 0 for the decoder. All of the feed-forward neural networks used in the decoder have 256 units. For LSTM encoder, only the standard LSTM module with 256 hidden units is used and its output is treated as the embedded state.\n\nThe MLP module consists of 3-layer feed-forward neural networks with 256, 512 and 256 units. For MLP encoder, only the standard MLP module is used. For MLP decoder, its structure is similar to Figure 2, in which LSTM module is replaced by the MLP module.\n\nBoth CORL-1 and CORL-2 use the actor-critic algorithm to train the model parameters. We use the Adam optimizer with learning rate 1e\u22124 and 1e\u22123 for the actor and critic, respectively. The discount factor \u03b3 is set to 0.98. The actor consists of an encoder and a decoder whose choices have been described above. The critic uses 3-layer feed-forward neural networks with 512, 256, and 1 units, which takes a state\u015d as input and outputs a predicted value for the current policy given state\u015d. For CORL-1, the critic needs to predict the score for each state\u015d t , while for CORL-2, the critic takes the initial state\u015d 0 as input and outputs a predicted value directly Table 1: Empirical results of CORL-2 with different encoder and decoder architectures on 30-node LG ER2 data sets. True Positive Rate (TPR) indicates the probability of correctly finding the positive edges among the discoveries, and the higher the better. Structural Hamming Distance (SHD) counts the total number of missing, falsely detected or reversed edges, and the smaller the better. for a complete ordering.\n\n\nEncoder\n\n\nB Baselines and Date Sets B.1 Baselines\n\nThe baselines considered in our experiments are listed as follows:\n\n\u2022 ICA-LiNGAM assumes linear non-Gaussian additive model for data generating procedure and applies independent component analysis to recover the weighted adjacency matrix. This method usually achieves good performance on LiNGAM data sets. However, it does not provide guarantee for linear Gaussian data sets. 1\n\n\u2022 NOTEARS recovers the causal graph by estimating the weighted adjacency matrix with the least squares loss and a smooth characterization for acyclicity constraint. 2\n\n\u2022 DAG-GNN formulates causal discovery in the framework of variational autoencoder. It uses a modified smooth characterization for acyclicity and optimizes a weighted adjacency matrix with the evidence lower bound as loss function. 3\n\n\u2022 GraN-DAG models the conditional distribution of each variable given its parents with feed-forward neural networks. It also uses the smooth acyclicity constraint from NOTEARS to find a DAG that maximizes the loglikelihood of the observed samples. 4\n\n\u2022 RL-BIC2 formulates the causal discovery as a one-step decision making process, and combines the score function and acyclicity constraint from NOTEARS as the reward for a directed graph. 5\n\n\u2022 CAM conducts a greedy estimation procedure that starts with an empty DAG and adds at each iteration the edge (v k , v j ) between nodes v k and v j that corresponds to the largest gain in log-likelihood. For a searched ordering, CAM prunes it to the final DAG by applying significance testing of covariates. CAM also performs preliminary neighborhood selection to reduce the ordering space. 6\n\n\u2022 L1OBS performs heuristic search (greedy hill-climbing with tabu lists) through the space of topological orderings to see an ordering with the best score. It uses 1 variable selection to prune the searched ordering (fullyconnected DAG) to the final DAG. 7\n\n\u2022 A* Lasso with a limited queue size incorporates a heuristic scheme into a dynamic programming based method. The queue size usually needs to be adjusted to balance the time cost and the quality of the solution. 8\n\n\nB.2 Data Generation\n\nWe generate synthetic data sets which vary along five dimensions: level of edge sparsity, graph type, number of nodes, causal functions and sample size. We sample 5 data sets with a required number of samples for each task: a ground truth DAG G * is firstly drawn randomly from either the Erd\u00f6s-R\u00e9nyi (ER) or Scale-free (SF) graph model, and the data are then generated according to different given SEMs. Specifically, for Linear Gaussian (LG) models, we set h \u2208 {2, 5} and d \u2208 {30, 50, 100} to obtain the ER and SF graphs with different levels of edge sparsity and different numbers of nodes. Then we generate 3, 000 samples for each task following the linear SEM: X = W T X + , where W \u2208 R d\u00d7d denotes the weighted adjacency matrix obtained by assigning edge weights independently from Unif([\u22122, \u22120.5] \u222a [0.5, 2]).\n\nHere \u2208 R d denote standard Gaussian noises with equal variances for each variable, which makes G * identifiable . For LiNGAM data model, the data sets are generated similarly to LG data models but the noise variables follow non-Gaussian distributions which pass the noise samples from Gaussian distribution through a power nonlinearity to make them non-Gaussian [Shimizu et al., 2006]. LiNGAM is also identifiable, as shown in [Shimizu et al., 2006].\n\nThe GP data sets with different sample sizes are generated following X j = f j (Pa(X j )) + j , where the function f j is a function sampled from a GP with radial basis function kernel of bandwidth one and j follows standard Gaussian distribution. This setting is also identifiable according to [Peters et al., 2014]. Due to the efficiency of the reward calculation, we only conduct experiments with up to 30 variables. Table 2 reports the total number of episodes required for CORL-2 and RL-BIC2 to converge, averaged over five seeds. CORL-2 performs fewer episodes than RL-BIC2 to reach a convergence, which we believe is due to reducing the size of search space and avoiding dealing with acyclicity.\n\n\nC Number of Episodes Before Convergence\n\n\nD Additional Results on LG Data Sets\n\nThe results for 50-node LG data models are presented in Table 3. The conclusion is similar to that of the 30-and 100-node experiments. The results of ICA-LiNGAM, GraN-DAG and CAM on LG data models are presented in Table 4. Their performances do not compare favorably to CORL-1 nor CORL-2 on LG data sets. It is not surprising that ICA-LiNGAM does not perform well because the algorithm is specifically designed for non-Gaussian noise and does not provide guarantee for LG data models. We hypothesize that CAM's poor performance on LG data models is because it uses nonlinear regression instead of linear regression. As for GraN-DAG, it uses 2-layer feed-forward neural networks to model the causal relationships, which may not be able to learn a good linear relationship in this experiment.\n\n\nE Results on LiNGAM Data Sets\n\nWe report the empirical results on 30-, 50-and 100-node LiNGAM data sets in Table 5. For L1OBS, we increased the recommended number of evaluations, from 2, 500 to 10, 000. For A* Lasso, we pick the queue size from {10, 500, 1000}, and report the best result out of these parameter settings. The results of L1OBS and A* Lasso reported here are those after pruning with the same method as used by CORL-2. For other baselines, we pick the recommended hyper-parameters.\n\nAmong all these algorithms, ICA-LiNGAM can recover the true graph on most of the LiNGAM data sets. This is because ICA-LiNGAM is specifically designed for non-Gaussian noises. CORL-1 and CORL-2 achieve consistently good results, compared with other baselines.\n\n\nF Results on 20-Node GP Data Sets with Different Sample Sizes\n\nWe take the 20-node GP data models as an example to show the performance of our method w.r.t. different sample numbers. The data generated based on ER4 graphs. We illustrate the empirical results in Table 6. Since previous experiments have shown that CORL-2 is slightly better than CORL-1, we only report the results of CORL-2 here. We also report the results of CAM on these data sets, as it is the most competitive baseline. TPR reported here is calculated based on the variable ordering (i.e., w.r.t. its correponding fully-connected DAG). As the sample size decreases, CORL-2 tend to perform better than CAM, and we believe this is because CORL-2 benefits from the exploration ability of RL.\n\nG CORL-1 with a Pretrained Model Figure 3: Learning curves of CORL-1 and CORL-1-pretrain on 100node LG data sets. Figure 3 shows the training reward curves of CORL-1 and CORL-1-pretrain; the latter stands for CORL-1 with a pretrained model. The data sets used for pretraining are 30-node ER2 and SF2 graphs with different causal relationships. We can observe that the pretrained model, which serves as a good initialization, again accelerates training.     \n\nFigure 2 :\n2Illustration of the policy model. The encoder embeds the observed data xj into the state sj. An action at can be selected by the decoder according to the given state\u015dt and the pointer mechanism at each time step t. Note that T = d \u2212 1. See Appendix A for details.\n\nFigure 3 :\n3Learning curves of CORL-1, CORL-2 and CORL-2pretrain on 100-node LG data sets.\n\n[Figure 4 :\n4Shimizu et al., 2006]. We set h \u2208 The empirical results on GP data models with 10 and 30 nodes.\n\nFigure 1 :\n1Illustration of the Transformer encoder. The encoder embeds the observed data xj of each variable Xj into state sj. Notation block@3 denotes three blocks.\n\nFigure 2 :\n2Illustration of LSTM decoder. At each time step, it maps the state\u015dt to a distribution over action space A = {a1, \u00b7 \u00b7 \u00b7 , a d } and then selects an action (variable) according to the distribution.\n\n\nupdate \u03c6 e , \u03c6 d , and \u03c6 v as described in Section 4.2 end if 13: end while 14: get the final DAG by pruning the BestOrderingscore \n\n7: \n\nend if \n\n8: \n\nend for \n\n9: \n\n10: \n\nif \n\nT \n\nt=0 r t > BestScore then \n\n11: \n\nupdate the BestScore and BestOrdering \n\n12: \n\n\n\nTable 1 :\n1Empirical results for ER and SF graphs of 30 and 100 nodes with LG data.\n\nTable 2 :\n2Total number of iterations (\u00d710 3 ) to reach convergence.30 nodes \n50 nodes \n100 nodes \n\nER2 \nER5 \nER2 \nER5 \nER2 \nER5 \n\nCORL-2 \n1.0 (0.3) 1.1 (0.4) 1.9 (0.3) 2.4 (0.3) 2.3 (0.5) 2.9 (0.4) \nRL-BIC2 3.9 (0.5) 4.1 (0.6) 3.4 (0.4) 3.5 (0.5) \n\u00d7 \n\u00d7 \n\n\n\nTable 3 :\n3Empirical results for ER and SF graphs of 50 nodes with LG data. The higher TPR the better, the smaller SHD the better.RANDOM \nNOTEARS DAG-GNN \nRL-BIC2 \nL1OBS \nA* Lasso \nCORL-1 \nCORL-2 \n\nER2 \nTPR \n0.31 (0.03) \n0.94 (0.02) \n0.94 (0.04) \n0.79 (0.10) \n0.56 (0.02) \n0.88 (0.03) \n0.97 (0.04) 0.97 (0.02) \nSHD 295.4 (28.5) 38.6 (10.8) \n30.6 (8.3) \n88.5 (49.3) \n288.0 (66.2) 154.3 (27.6) 24.0 (32.3 17.9 (10.6) \n\nER5 \nTPR \n0.32 (0.02) \n0.90 (0.01) \n0.87 (0.14) \n0.74 (0.03) \n0.57 (0.03) \n0.82 (0.03) \n0.90 (0.02) 0.92 (0.02) \nSHD 378.4 (24.2) \n67.8 (7.5) \n93.2 (109.4) 128.9 (40.4) 299.4 (53.6) 104.0 (28.3) 68.3 (10.2) 64.8 (13.1) \n\nSF2 \nTPR \n0.49 (0.04) \n0.99 (0.01) \n0.90 (0.13) \n0.84 (0.05) \n0.67 (0.02) \n0.89 (0.03) \n1.00 (0.00) 1.00 (0.00) \nSHD 215.6 (14.7) \n3.5 (1.6) \n79.3 (93.2) \n115.2 (57.4) 182.3 (33.4) 124.0 (35.2) \n0.0 (0.0) \n0.0 (0.0) \n\nSF5 \nTPR \n0.51 (0.03) \n0.94 (0.12) \n0.88 (0.12) \n0.75 (0.05) \n0.61 (0.03) \n0.81 (0.02) \n0.94 (0.03) 0.95 (0.02) \nSHD 345.6 (24.3) 20.1 (14.3) \n89.2 (99.2) \n115.2 (57.4) 217.3 (36.4) 131.0 (25.3) 24.3 (11.2) 20.8 (10.1) \n\n\n\nTable 4 :\n4Empirical results of ICA-LiNGAM, GraN-DAG and CAM (against CORL-2 for reference) for ER and SF graphs with LG data.ICA-LiNGAM GraN-DAG \nCAM \nCORL-2 \n\n30 nodes \n\nER2 \nTPR \n0.75 (0.03) \n0.51 (0.17) \n0.47 (0.05) \n0.99 (0.01) \nSHD \n112.3 (12.8) \n96.0 (11.3) \n110.8 (10.3) \n4.4 (3.5)) \n\nER5 \nTPR \n0.57 (0.03) \n0.52 (0.03) \n0.46 (0.02) \n0.95 (0.03) \nSHD \n161.8 (9.2) \n175.2 (27.4) 191.3 (32.5) \n37.6 (14.5) \n\nSF2 \nTPR \n0.58 (0.05) \n0.61 (0.04) \n0.63 (0.02) \n1.0 (0.0) \nSHD \n149.0 (19,8) \n136.4 (21.2) 115.2 (26.7) \n0.0 (0.0) \n\nSF5 \nTPR \n0.56 (0.04) \n0.58 (0.02) \n0.60 (0.03) \n1.0 (0.0) \nSHD \n160.5 (8.9) \n142.4 (24.3) 122.2 (17.4) \n0.0 (0.0) \n\n50 nodes \n\nER2 \nTPR \n0.73 (0.03) \n0.11 (0.04) \n0.55 (0.06) \n0.97 (0.02) \nSHD \n108.8 (11.3) \n173.0 (22.9) 140.8 (35.4) \n17.9 (10.6) \n\nER5 \nTPR \n0.57 (0.01) \n0.64 (0.03) \n0.61 (0.02) \n0.92 (0.02) \nSHD \n199.8 (90.7) \n154.2 (36.4) 178.3 (34.8) \n64.8 (13.1) \n\nSF2 \nTPR \n0.59 (0.04) \n0.44 (0.05) \n0.57 (0.02) \n1.00 (0.00) \nSHD \n208.5 (83.2) \n158.6 (34.5) 131.2 (24.4) \n0.0 (0.0) \n\nSF5 \nTPR \n0.57 (0.01) \n0.49 (0.04) \n0.53 (0.03) \n0.95 (0.02) \nSHD \n216.6 (88.4) \n243.9 (27.2) 235.2 (34.2) \n20.8 (10.1) \n\n100 nodes \n\nER2 \nTPR \n0.73 (0.02) \n0.38 (0.02) \n0.43 (0.02) \n0.98 (0.01) \nSHD \n268.4 (28.5) \n191.3 (31.9) 126.4 (27.8) \n18.6 (5.7) \n\nER5 \nTPR \n0.57 (0.05) \n0.42 (0.03) \n0.47 (0.02) \n0.94 (0.03) \nSHD \n311.1 (63.7) \n208.2 (54.4) 182.3 (34.9) 164.8 (17.1) \n\nSF2 \nTPR \n0.69 (0.03) \n0.40 (0.03) \n0.44 (0.02) \n1.00 (0.00) \nSHD \n367.6 (67.5) \n239.9 (43.2) \n35.2 (37.4) \n0.0 (0.0) \n\nSF5 \nTPR \n0.57 (0.05) \n0.39 (0.03) \n0.48 (0.04) \n0.98 (0.01) \nSHD \n362.3 (82.8) \n219.3 (32.2) 125.2 (24.7) \n10.8 (6.1) \n\n\n\nTable 5 :\n5Empirical results on 30-, 50-and 100-node LiNGAM ER2 data sets.30 nodes ER2 \n50 nodes ER2 \n100 nodes ER2 \n\nMethod \nTPR \nSHD \nTPR \nSHD \nTPR \nSHD \n\nICA-LiNGAM 1.00 (0.00) \n0.0 (0.0) \n1.00 (0.00) \n0.0 (0.0) \n1.00 (0.00) \n1.0 (0.9) \nNOTEARS \n0.94 (0.04) \n17.2 (13.2) \n0.95 (0.02) \n33.2 (16.5) \n0.94 (0.03) \n69.2 (23.2) \nDAG-GNN \n0.94 (0.03) \n19.6 (10.5) \n0.96 (0.01) \n24.6 (2.9) \n0.93 (0.03) \n66.2 (19.2) \nGraN-DAG \n0.28 (0.09) 100.8 (14.6) 0.20 (0.01) 177.0 (25.9) 0.16 (0.04) 312.8 (25.2) \nRL-BIC2 \n0.94 (0.07) \n19.8 (23.0) \n0.80 (0.08) \n86.0 (51.9) \n0.13 (0.12) 291.3 (24.1) \nCAM \n0.60 (0.11) 310.0 (34.0) 0.33 (0.07) 178.0 (31.9) 0.53 (0.05) 247.2 (32.1) \nL1OBS \n0.72 (0.04) \n85.3 (23.3) \n0.47 (0.02) 212.6 (24.6) 0.41 (0.03) 470.5 (48.1) \nA* Lasso \n0.87 (0.03) \n42.3 (16.3) \n0.88 (0.03) \n82.6 (17.6) \n0.85 (0.04) 102.5 (22.6) \n\nCORL-1 \n0.99 (0.01) \n3.8 (6.4) \n0.96 (0.06) \n24.6 (37.7) \n0.98 (0.01) \n20.0 (7.9) \nCORL-2 \n0.99 (0.01) \n3.9 (5.6) \n0.96 (0.08) \n20.2 (11.3) \n0.99 (0.01) \n13.8 (7.2) \n\n\n\nTable 6 :\n6Empirical results on 20-node GP ER1 data sets with different sample sizes.Sample size \nName \nTPR \nSHD \n\n1000 \nCAM \n0.91 (0.03) 30.0 (3.7) \nCORL-2 0.87 (0.03) 36.5 (3.1) \n\n500 \nCAM \n0.86 (0.03) 45.0 (2.5) \nCORL-2 0.85 (0.03) 46.3 (2.3) \n\n400 \nCAM \n0.83 (0.02) 51.0 (2.7) \nCORL-2 \n0.84 (0.03) 50.5 (3.0) \n\n200 \nCAM \n0.60 (0.03) 66.3 (1.9) \nCORL-2 0.75 (0.02) 63.1 (1.5) \n\nhttps://sites.google.com/site/sshimizu06/lingam 2 https://github.com/xunzheng/notears 3 https://github.com/fishmoon1234/DAG-GNN 4 https://github.com/kurowasan/GraN-DAG\nhttps://github.com/huawei-noah/trustworthyAI/tree/master/ Causal Structure Learning/Causal Discovery RL 6 https://cran.r-project.org/web/packages/CAM. 7 https://www.cs.ubc.ca/ \u223c murphyk/Software/DAGlearn/ 8 http://www.cs.cmu.edu/ \u223c jingx/software/AstarLasso.zip\nAcknowledgmentsXiaoqiang Wang and Liangjun Ke were supported by the National Natural Science Foundation of China under Grant 61973244.Appendix to \"Ordering-Based Causal Discovery with Reinforcement Learning\" A Architectures and Hyper-ParametersA.1 Encoder and Decoder Architectures\nCausal additive models, highdimensional order search and penalized regression. Mark Bartlett, James Cussens, ; Bello, arXiv:1611.09940PMLRternational Conference on Artificial Intelligence and Statistics (AISTATS). Peter B\u00fchlmann, Jonas PetersSpringer244arXiv preprintThe Annals of Statisticsand Cussens, 2017] Mark Bartlett and James Cussens. Integer linear programming for the bayesian network structure learning problem. Artificial Intelligence, 244:258-271, 2017. [Bello et al., 2016] Irwan Bello, Hieu Pham, Quoc V Le, Mo- hammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. [Bernstein et al., 2020] Daniel Bernstein, Basil Saeed, Chan- dler Squires, and Caroline Uhler. Ordering-based causal structure learning in the presence of latent variables. In In- ternational Conference on Artificial Intelligence and Statis- tics (AISTATS), pages 4098-4108. PMLR, 2020. [B\u00fchlmann et al., 2014] Peter B\u00fchlmann, Jonas Peters, Jan Ernest, et al. CAM: Causal additive models, high- dimensional order search and penalized regression. The Annals of Statistics, 42(6):2526-2556, 2014. [Chickering, 1996] David Maxwell Chickering. Learning Bayesian networks is NP-complete. In Learning from Data: Artificial Intelligence and Statistics V. Springer, 1996. [Chickering, 2002] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of Ma- chine Learning Research, 3(Nov):507-554, 2002.\n\nBeing Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks. Ji ; Cassio P De Campos, Qiang Ji ; Nir Campos, Daphne Friedman, ; Koller, Kool, Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations (ICLR). Khalil et al., 2017] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le SongFriedman and Koller12Advances in Neural Information Processing Systems (NeurIPS)Campos and Ji, 2011] Cassio P De Campos and Qiang Ji. Efficient structure learning of bayesian networks using constraints. The Journal of Machine Learning Research, 12:663-689, 2011. [Friedman and Koller, 2003] Nir Friedman and Daphne Koller. Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian net- works. Machine learning, 50(1-2):95-125, 2003. [Khalil et al., 2017] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op- timization algorithms over graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [Kool et al., 2019] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations (ICLR), 2019.\n\nIgnavier Ng, Shengyu Zhu, Zhitang Chen, and Zhuangyan Fang. A graph autoencoder approach to causal structure learning. Lachapelle, arXiv:1910.08527arXiv:1702.03530Consistency guarantees for greedy permutationbased causal inference algorithms. Yuhao Wang, and Caroline UhlerMIT press26arXiv preprintReinforcement Learning: An IntroductionLachapelle et al., 2020] S\u00e9bastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural DAG learning. In International Conference on Learning Representations (ICLR), 2020. [Larranaga et al., 1996] Pedro Larranaga, Cindy MH Kui- jpers, Roberto H Murga, and Yosu Yurramendi. Learn- ing bayesian network structures by searching for the best ordering with genetic algorithms. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Hu- mans, 26(4):487-493, 1996. [Ng et al., 2019a] Ignavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, and Jun Wang. Masked gradient-based causal structure learning. arXiv preprint arXiv:1910.08527, 2019. [Ng et al., 2019b] Ignavier Ng, Shengyu Zhu, Zhitang Chen, and Zhuangyan Fang. A graph autoencoder approach to causal structure learning. arXiv preprint arXiv:1911.07420, 2019. [Peters and B\u00fchlmann, 2014] Jonas Peters and Peter B\u00fchlmann. Identifiability of gaussian structural equa- tion models with equal error variances. Biometrika, 101(1):219-228, 2014. [Peters et al., 2014] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Causal discovery with continuous additive noise models. Journal of Machine Learning Research, 15(1):2009-2053, January 2014. [Raskutti and Uhler, 2018] Garvesh Raskutti and Caroline Uhler. Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1):e183, 2018. [Sachs et al., 2005] Karen Sachs, Omar Perez, Dana Pe'er, Douglas A Lauffenburger, and Garry P Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721):523-529, 2005. [Scanagatta et al., 2015] Mauro Scanagatta, Cassio P de Campos, Giorgio Corani, and Marco Zaffalon. Learning bayesian networks with thousands of variables. In NeurIPS, 2015. [Schmidt et al., 2007] Mark Schmidt, Alexandru Niculescu- Mizil, Kevin Murphy, et al. Learning graphical model structure using L1-regularization paths. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2007. [Shimizu et al., 2006] Shohei Shimizu, Patrik O Hoyer, Aapo Hyv\u00e4rinen, and Antti Kerminen. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030, 2006. [Solus et al., 2017] Liam Solus, Yuhao Wang, and Caroline Uhler. Consistency guarantees for greedy permutation- based causal inference algorithms. arXiv preprint arXiv:1702.03530, 2017. [Sutton and Barto, 2018] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n\nJing Xiang and Seyoung Kim. A* lasso for learning a sparse bayesian network structure for continuous variables. Koller ; Marc Teyssier, Daphne Koller, ; Vaswani, Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. International Conference on Machine Learning (ICML)and Koller, 2005] Marc Teyssier and Daphne Koller. Ordering-based search: A simple and effective algorithm for learning bayesian networks. In Conference on Uncertainty in Artificial Intelligence (UAI), 2005. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Sys- tems (NeurIPS), 2017. [Vinyals et al., 2015] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neu- ral Information Processing Systems (NeurIPS), 2015. [Xiang and Kim, 2013] Jing Xiang and Seyoung Kim. A* lasso for learning a sparse bayesian network structure for continuous variables. In Advances in Neural Information Processing Systems (NeurIPS), 2013. [Yu et al., 2019] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural net- works. In International Conference on Machine Learning (ICML), 2019.\n\nDAGs with NO TEARS: Continuous optimization for structure learning. Zheng, International Conference on Artificial Intelligence and Statistics (AISTATS). 2020International Conference on Learning Representations (ICLRZheng et al., 2018] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS: Con- tinuous optimization for structure learning. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [Zheng et al., 2020] Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric P Xing. Learning sparse non- parametric DAGs. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020. [Zhu et al., 2020] Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal discovery with reinforcement learning. In International Conference on Learning Representations (ICLR), 2020.\n\nIdentifiability of gaussian structural equation models with equal error variances. Barret Zoph, Quoc V Le ; Jonas Peters, Peter B\u00fchlmann, ; Peters, International Conference on Learning Representations (ICLR), 2017. References [Peters and B\u00fchlmann. 1012020International Conference on Learning Representations (ICLRand Le, 2017] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In Inter- national Conference on Learning Representations (ICLR), 2017. References [Peters and B\u00fchlmann, 2014] Jonas Peters and Peter B\u00fchlmann. Identifiability of gaussian structural equa- tion models with equal error variances. Biometrika, 101(1):219-228, 2014. [Peters et al., 2014] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Causal discovery with continuous additive noise models. Journal of Machine Learning Research, 15(1):2009-2053, January 2014. [Shimizu et al., 2006] Shohei Shimizu, Patrik O Hoyer, Aapo Hyv\u00e4rinen, and Antti Kerminen. A linear non- Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030, 2006. [Vinyals et al., 2015] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neu- ral Information Processing Systems (NeurIPS), 2015. [Zhu et al., 2020] Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal discovery with reinforcement learning. In International Conference on Learning Representations (ICLR), 2020.\n", "annotations": {"author": "[{\"end\":210,\"start\":63},{\"end\":280,\"start\":211},{\"end\":315,\"start\":281},{\"end\":318,\"start\":316},{\"end\":463,\"start\":319},{\"end\":466,\"start\":464},{\"end\":480,\"start\":467},{\"end\":572,\"start\":481},{\"end\":646,\"start\":573},{\"end\":657,\"start\":647}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":73},{\"end\":218,\"start\":216},{\"end\":292,\"start\":289},{\"end\":330,\"start\":328},{\"end\":479,\"start\":475},{\"end\":491,\"start\":488},{\"end\":581,\"start\":577}]", "author_first_name": "[{\"end\":72,\"start\":63},{\"end\":215,\"start\":211},{\"end\":288,\"start\":281},{\"end\":317,\"start\":316},{\"end\":327,\"start\":319},{\"end\":465,\"start\":464},{\"end\":474,\"start\":467},{\"end\":487,\"start\":481},{\"end\":576,\"start\":573}]", "author_affiliation": "[{\"end\":209,\"start\":79},{\"end\":279,\"start\":239},{\"end\":462,\"start\":332},{\"end\":571,\"start\":514},{\"end\":645,\"start\":605},{\"end\":656,\"start\":648}]", "title": "[{\"end\":60,\"start\":1},{\"end\":717,\"start\":658}]", "venue": null, "abstract": "[{\"end\":1969,\"start\":719}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2399,\"start\":2381},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2614,\"start\":2596},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2975,\"start\":2948},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3041,\"start\":3017},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3094,\"start\":3068},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3154,\"start\":3127},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3532,\"start\":3512},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3611,\"start\":3594},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3635,\"start\":3611},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3652,\"start\":3635},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3669,\"start\":3652},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3688,\"start\":3669},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3801,\"start\":3783},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3963,\"start\":3945},{\"end\":5963,\"start\":5941},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6091,\"start\":6064},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6118,\"start\":6093},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6294,\"start\":6271},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6443,\"start\":6417},{\"end\":6496,\"start\":6476},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6734,\"start\":6710},{\"end\":6958,\"start\":6937},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7021,\"start\":6993},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7154,\"start\":7130},{\"end\":7286,\"start\":7265},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7484,\"start\":7464},{\"end\":7504,\"start\":7484},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7522,\"start\":7504},{\"end\":7889,\"start\":7870},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7906,\"start\":7889},{\"end\":8057,\"start\":8056},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9925,\"start\":9898},{\"end\":9946,\"start\":9925},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13608,\"start\":13590},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15320,\"start\":15302},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15700,\"start\":15678},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16619,\"start\":16608},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17197,\"start\":17185},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17615,\"start\":17595},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18325,\"start\":18298},{\"end\":18402,\"start\":18380},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18607,\"start\":18587},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18693,\"start\":18668},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21350,\"start\":21338},{\"end\":22098,\"start\":22072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23952,\"start\":23932},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23969,\"start\":23952},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28296,\"start\":28278},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28320,\"start\":28296},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28385,\"start\":28367},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29033,\"start\":29008},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29639,\"start\":29619},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31973,\"start\":31955},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38158,\"start\":38136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38223,\"start\":38201},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38542,\"start\":38521},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":42186,\"start\":42165}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42058,\"start\":41782},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42150,\"start\":42059},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42260,\"start\":42151},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42428,\"start\":42261},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42638,\"start\":42429},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42902,\"start\":42639},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42987,\"start\":42903},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43245,\"start\":42988},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44324,\"start\":43246},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45968,\"start\":44325},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46977,\"start\":45969},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47359,\"start\":46978}]", "paragraph": "[{\"end\":2615,\"start\":1985},{\"end\":3349,\"start\":2617},{\"end\":4540,\"start\":3351},{\"end\":5098,\"start\":4542},{\"end\":5858,\"start\":5100},{\"end\":6877,\"start\":5876},{\"end\":7339,\"start\":6879},{\"end\":8219,\"start\":7341},{\"end\":9158,\"start\":8262},{\"end\":9421,\"start\":9160},{\"end\":10811,\"start\":9497},{\"end\":11730,\"start\":10838},{\"end\":12079,\"start\":11791},{\"end\":12487,\"start\":12126},{\"end\":13272,\"start\":12489},{\"end\":13940,\"start\":13274},{\"end\":14299,\"start\":13942},{\"end\":14774,\"start\":14343},{\"end\":14976,\"start\":14839},{\"end\":15182,\"start\":15040},{\"end\":15831,\"start\":15184},{\"end\":16347,\"start\":15833},{\"end\":16748,\"start\":16349},{\"end\":16921,\"start\":16803},{\"end\":17545,\"start\":16989},{\"end\":18159,\"start\":17547},{\"end\":18976,\"start\":18182},{\"end\":19030,\"start\":18978},{\"end\":19327,\"start\":19032},{\"end\":20505,\"start\":19539},{\"end\":20937,\"start\":20507},{\"end\":21726,\"start\":20939},{\"end\":22714,\"start\":21755},{\"end\":23493,\"start\":22730},{\"end\":24406,\"start\":23548},{\"end\":25804,\"start\":24408},{\"end\":26068,\"start\":25806},{\"end\":26775,\"start\":26070},{\"end\":27620,\"start\":26777},{\"end\":28713,\"start\":27663},{\"end\":29586,\"start\":28715},{\"end\":30303,\"start\":29600},{\"end\":30781,\"start\":30318},{\"end\":32133,\"start\":30783},{\"end\":32790,\"start\":32182},{\"end\":33454,\"start\":32792},{\"end\":33710,\"start\":33456},{\"end\":34788,\"start\":33712},{\"end\":34908,\"start\":34842},{\"end\":35219,\"start\":34910},{\"end\":35387,\"start\":35221},{\"end\":35621,\"start\":35389},{\"end\":35872,\"start\":35623},{\"end\":36063,\"start\":35874},{\"end\":36459,\"start\":36065},{\"end\":36717,\"start\":36461},{\"end\":36932,\"start\":36719},{\"end\":37772,\"start\":36956},{\"end\":38224,\"start\":37774},{\"end\":38928,\"start\":38226},{\"end\":39801,\"start\":39011},{\"end\":40300,\"start\":39835},{\"end\":40561,\"start\":40302},{\"end\":41322,\"start\":40627},{\"end\":41781,\"start\":41324}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9496,\"start\":9422},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11781,\"start\":11731},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14342,\"start\":14300},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14838,\"start\":14775},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16802,\"start\":16749},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16988,\"start\":16922},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19515,\"start\":19328}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23978,\"start\":23971},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24430,\"start\":24423},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31353,\"start\":31346},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":34381,\"start\":34374},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38653,\"start\":38646},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39074,\"start\":39067},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39232,\"start\":39225},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39918,\"start\":39911},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":40833,\"start\":40826}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1983,\"start\":1971},{\"attributes\":{\"n\":\"2\"},\"end\":5874,\"start\":5861},{\"attributes\":{\"n\":\"3\"},\"end\":8232,\"start\":8222},{\"attributes\":{\"n\":\"3.1\"},\"end\":8260,\"start\":8235},{\"attributes\":{\"n\":\"3.2\"},\"end\":10836,\"start\":10814},{\"attributes\":{\"n\":\"4\"},\"end\":11789,\"start\":11783},{\"attributes\":{\"n\":\"4.1\"},\"end\":12124,\"start\":12082},{\"attributes\":{\"n\":\"4.2\"},\"end\":15038,\"start\":14979},{\"attributes\":{\"n\":\"4.3\"},\"end\":18180,\"start\":18162},{\"attributes\":{\"n\":\"4.4\"},\"end\":19537,\"start\":19517},{\"attributes\":{\"n\":\"4.5\"},\"end\":21753,\"start\":21729},{\"attributes\":{\"n\":\"5\"},\"end\":22728,\"start\":22717},{\"attributes\":{\"n\":\"5.1\"},\"end\":23546,\"start\":23496},{\"attributes\":{\"n\":\"5.2\"},\"end\":27661,\"start\":27623},{\"attributes\":{\"n\":\"5.3\"},\"end\":29598,\"start\":29589},{\"attributes\":{\"n\":\"6\"},\"end\":30316,\"start\":30306},{\"end\":32180,\"start\":32136},{\"end\":34798,\"start\":34791},{\"end\":34840,\"start\":34801},{\"end\":36954,\"start\":36935},{\"end\":38970,\"start\":38931},{\"end\":39009,\"start\":38973},{\"end\":39833,\"start\":39804},{\"end\":40625,\"start\":40564},{\"end\":41793,\"start\":41783},{\"end\":42070,\"start\":42060},{\"end\":42163,\"start\":42152},{\"end\":42272,\"start\":42262},{\"end\":42440,\"start\":42430},{\"end\":42913,\"start\":42904},{\"end\":42998,\"start\":42989},{\"end\":43256,\"start\":43247},{\"end\":44335,\"start\":44326},{\"end\":45979,\"start\":45970},{\"end\":46988,\"start\":46979}]", "table": "[{\"end\":42902,\"start\":42766},{\"end\":43245,\"start\":43057},{\"end\":44324,\"start\":43377},{\"end\":45968,\"start\":44452},{\"end\":46977,\"start\":46044},{\"end\":47359,\"start\":47064}]", "figure_caption": "[{\"end\":42058,\"start\":41795},{\"end\":42150,\"start\":42072},{\"end\":42260,\"start\":42165},{\"end\":42428,\"start\":42274},{\"end\":42638,\"start\":42442},{\"end\":42766,\"start\":42641},{\"end\":42987,\"start\":42915},{\"end\":43057,\"start\":43000},{\"end\":43377,\"start\":43258},{\"end\":44452,\"start\":44337},{\"end\":46044,\"start\":45981},{\"end\":47064,\"start\":46990}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8053,\"start\":8045},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8138,\"start\":8130},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10346,\"start\":10338},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15140,\"start\":15132},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24681,\"start\":24673},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26146,\"start\":26138},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28800,\"start\":28792},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32282,\"start\":32274},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32872,\"start\":32864},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33657,\"start\":33649},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41365,\"start\":41357},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41446,\"start\":41438}]", "bib_author_first_name": "[{\"end\":48155,\"start\":48151},{\"end\":48171,\"start\":48166},{\"end\":48182,\"start\":48181},{\"end\":49681,\"start\":49665},{\"end\":49704,\"start\":49690},{\"end\":49719,\"start\":49713},{\"end\":49731,\"start\":49730},{\"end\":53886,\"start\":53873},{\"end\":53903,\"start\":53897},{\"end\":53913,\"start\":53912},{\"end\":55974,\"start\":55968},{\"end\":55998,\"start\":55981},{\"end\":56012,\"start\":56007},{\"end\":56024,\"start\":56023}]", "bib_author_last_name": "[{\"end\":48164,\"start\":48156},{\"end\":48179,\"start\":48172},{\"end\":48188,\"start\":48183},{\"end\":49688,\"start\":49682},{\"end\":49711,\"start\":49705},{\"end\":49728,\"start\":49720},{\"end\":49738,\"start\":49732},{\"end\":49744,\"start\":49740},{\"end\":50987,\"start\":50977},{\"end\":53895,\"start\":53887},{\"end\":53910,\"start\":53904},{\"end\":53921,\"start\":53914},{\"end\":55123,\"start\":55118},{\"end\":55979,\"start\":55975},{\"end\":56005,\"start\":55999},{\"end\":56021,\"start\":56013},{\"end\":56031,\"start\":56025}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1611.09940\",\"id\":\"b0\",\"matched_paper_id\":16000852},\"end\":49558,\"start\":48072},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2817192},\"end\":50856,\"start\":49560},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208139013},\"end\":53759,\"start\":50858},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6625156},\"end\":55048,\"start\":53761},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53217974},\"end\":55883,\"start\":55050},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":88520304},\"end\":57331,\"start\":55885}]", "bib_title": "[{\"end\":48149,\"start\":48072},{\"end\":49663,\"start\":49560},{\"end\":50975,\"start\":50858},{\"end\":53871,\"start\":53761},{\"end\":55116,\"start\":55050},{\"end\":55966,\"start\":55885}]", "bib_author": "[{\"end\":48166,\"start\":48151},{\"end\":48181,\"start\":48166},{\"end\":48190,\"start\":48181},{\"end\":49690,\"start\":49665},{\"end\":49713,\"start\":49690},{\"end\":49730,\"start\":49713},{\"end\":49740,\"start\":49730},{\"end\":49746,\"start\":49740},{\"end\":50989,\"start\":50977},{\"end\":53897,\"start\":53873},{\"end\":53912,\"start\":53897},{\"end\":53923,\"start\":53912},{\"end\":55125,\"start\":55118},{\"end\":55981,\"start\":55968},{\"end\":56007,\"start\":55981},{\"end\":56023,\"start\":56007},{\"end\":56033,\"start\":56023}]", "bib_venue": "[{\"end\":48284,\"start\":48210},{\"end\":49898,\"start\":49746},{\"end\":51099,\"start\":51021},{\"end\":53991,\"start\":53923},{\"end\":55201,\"start\":55125},{\"end\":56131,\"start\":56033},{\"end\":48314,\"start\":48286},{\"end\":51131,\"start\":51101}]"}}}, "year": 2023, "month": 12, "day": 17}