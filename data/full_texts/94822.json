{"id": 94822, "updated": "2023-09-29 04:58:26.543", "metadata": {"title": "Dynamic Graph CNN for Learning on Point Clouds", "authors": "[{\"first\":\"Yue\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yongbin\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Sanjay\",\"last\":\"Sarma\",\"middle\":[\"E.\"]},{\"first\":\"Michael\",\"last\":\"Bronstein\",\"middle\":[\"M.\"]},{\"first\":\"Justin\",\"last\":\"Solomon\",\"middle\":[\"M.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 1, "day": 24}, "abstract": "Point clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. Hence, the design of intelligent computational models that act directly on point clouds is critical, especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv is differentiable and can be plugged into existing architectures. Compared to existing modules operating largely in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked or recurrently applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. Beyond proposing this module, we provide extensive evaluation and analysis revealing that EdgeConv captures and exploits fine-grained geometric properties of point clouds. The proposed approach achieves state-of-the-art performance on standard benchmarks including ModelNet40 and S3DIS.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1801.07829", "mag": "2979750740", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/WangSLSBS19", "doi": "10.1145/3326362"}}, "content": {"source": {"pdf_hash": "317645033e185ec3fb8544f26af58da91e072b5c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1801.07829v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3326362", "status": "BRONZE"}}, "grobid": {"id": "a35b9e49c643ce291f26a30f4d947b19990f85ab", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/317645033e185ec3fb8544f26af58da91e072b5c.txt", "contents": "\nDynamic Graph CNN for Learning on Point Clouds\n\n\nYue Wang yuewang@csail.mit.edu \nYongbin Sun ybsun@mit.edu \nZiwei Liu zwliu@icsi.berkeley.edu \nMichael M Bronstein michael.bronstein@usi.ch \nJustin M Solomon jsolomon@mit.edu \nMit \n\nSanjay E. Sarma MIT\nMIT\nMIT\nBerkeley\n\n\nTAU / Intel\nUSI\n\n\nDynamic Graph CNN for Learning on Point Clouds\n* Equal Contribution\nPoint clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. Hence, the design of intelligent computational models that act directly on point clouds is critical, especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv is differentiable and can be plugged into existing architectures. Compared to existing modules operating largely in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked or recurrently applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. Beyond proposing this module, we provide extensive evaluation and analysis revealing that EdgeConv captures and exploits fine-grained geometric properties of point clouds. The proposed approach achieves state-of-the-art performance on standard benchmarks including ModelNet40 and S3DIS.\n\nIntroduction\n\nPoint clouds, or scattered collections of points in 2D or 3D, are arguably the simplest shape representation; they also comprise the output of 3D sensing technology including LiDAR scanners and stereo reconstruction. With the advent of fast 3D point cloud acquisition, recent pipelines for graphics and vision often process point clouds directly, bypassing expensive mesh reconstruction or denoising due to efficiency considerations or instability of these techniques in the presence of noise. A few of the many recent applications of point cloud processing and analysis include indoor navigation [57], self-driving vehicles [33], robotics [40], and shape synthesis and modeling [14]. Modern applications demand high-level processing of point clouds. Rather than identifying salient geometric features like corners and edges, recent algorithms search for semantic cues and affordances. These features do not fit cleanly into the frameworks of computational or differential geometry and typically require learning-based approaches that derive relevant information through statistical analysis of labeled or unlabeled datasets.\n\nIn this paper, we primarily consider point cloud classification and segmentation, two model tasks in the point cloud processing world. Traditional methods for solving these problems employ handcrafted features to capture geometric properties of point clouds [26,38,39]. More recently, the success of deep neural networks for image processing has motivated a data-driven approach to learning features on point clouds. Deep point cloud processing and analysis methods are developing rapidly and outperform traditional approaches in various tasks [10].\n\nAdaptation of deep learning to point cloud data, however, is far from straightforward. Most critically, standard deep neural network models take as input data with regular structure, while point clouds are fundamentally irregular: Point positions are continuously distributed in the space, and any permutation of their ordering does not change the Figure 1. Point cloud segmentation using the proposed neural network. Bottom: schematic neural network architecture. Top: Structure of the feature spaces produced at different layers of the network, visualized as the distance from the red point to all the rest of the points (shown left-to-right are the input and layers 1-3; rightmost figure shows the resulting segmentation). Observe how the feature space structure in deeper layers captures semantically similar structures such as wings, fuselage, or turbines, despite a large distance between them in the original input space. spatial distribution. One common approach to process point cloud data using deep learning models is to first convert raw point cloud data into a volumetric representation, namely a 3D grid [30,54]. This approach, however, usually introduces quantization artifacts and excessive memory usage, making it difficult to go to capture high-resolution or finegrained features.\n\nState-of-the-art deep neural networks are designed specifically to handle the irregularity of point clouds, directly manipulating raw point cloud data rather than passing to an intermediate regular representation. This approach was pioneered by PointNet [34], which achieves permutation invariance of points by operating on each point independently and subsequently applying a symmetric function to accumulate features. Various extensions of Point-Net consider neighborhoods of points rather than acting on each independently [36,43]; these allow the network to exploit local features, improving upon performance of the basic model. These techniques largely treat points independently at local scale to maintain permutation invariance. This independence, however, neglects the geometric relationships among points, presenting a fundamental limitation that leads to local features missing.\n\nTo address these drawbacks, we propose a novel simple operation, called EdgeConv, which captures local geometric structure while maintaining permutation invariance. Instead of generating points' features directly from their embeddings, EdgeConv generates edge features that describe the relationships between a point and its neighbors. EdgeConv is designed to be invariant to the ordering of neighbors, and thus permutation invariant.\n\nEdgeConv is easy to implement and integrate into existing deep learning models to improve their performance. In our experiments, we integrate EdgeConv into the basic version of PointNet without using any feature transformation. We show performance improvement by a large margin; the resulting network achieves state-of-the-art performance on several datasets, most notably ModelNet40 and S3DIS for classification and segmentation.\n\nKey Contributions. We summarize the key contributions of our work as follows:\n\n\u2022 We present a novel operation for point clouds, Edge-Conv, to better capture local geometric features of point clouds while still maintaining permutation invariance.\n\n\u2022 We show the model can learn to semantically group points by dynamically updating the graph.\n\n\u2022 We demonstrate that EdgeConv can be integrated into multiple existing pipelines for point cloud processing.\n\n\u2022 We present extensive analysis and testing of EdgeConv and show that it achieves state-of-the-art performance on benchmark datasets. Left: An example of computing an edge feature, eij, from a point pair, xi and xj. In this example, h \u0398 () is instantiated using a fully connected layer, and the learnable parameters are its associated weights and bias. Right: Visualize the EdgeConv operation. The output of EdgeConv is calculated by aggregating the edge features associated with all the edges emanating from each connected vertex.\n\n\nRelated Work\n\nHand-Crafted Features Various tasks in geometric data processing and analysis -including segmentation, classification, and matching -require some notion of local similarity between shapes. Traditionally, this similarity is established by constructing feature descriptors that capture local geometric structure. Countless papers in computer vision and graphics propose local feature descriptors for point clouds suitable for different problems and data structures. A comprehensive overview of hand-designed point features is out of the scope of this paper, but we refer the reader to [51,15,4] for comprehensive discussion. Broadly speaking, one can distinguish between extrinsic and intrinsic descriptors. Extrinsic descriptors usually are derived from the coordinates of the shape in 3D space and includes classical methods like shape context [3], spin images [17], integral features [27], distance-based descriptors [24], point feature histograms [39,38], and normal histograms [50], to name a few. Intrinsic descriptors treat the 3D shape as a manifold whose metric structure is discretized as a mesh or graph; quantities expressed in terms of the metric are by definition intrinsic and invariant to isometric deformation. Representatives of this class include spectral descriptors such as global point signatures [37], the heat and wave kernel signatures [48,2], and variants [8]. Most recently, several approaches wrap machine learning schemes around standard descriptors [15,42].\n\nLearned Features. In computer vision, approaches relying on 'hand-crafted' features have reached a plateau in performance on challenging image analysis problems like image recognition. A breakthrough came with the use of convolutional neural networks (CNNs) [22,21], leading to an overwhelming trend to abandon hand-crafted features in favor of models that learn task-specific features from data.\n\nA basic CNN architecture is the deep neural network, which interleaves convolutional and pooling layers to aggregate local information in images. This success of deep learning for images suggests the value of adapting related insight to geometric data like point clouds. Unlike images, however, geometric data usually are not on an underlying grid, requiring new definitions for building blocks like convolution and pooling.\n\nExisting 3D deep learning methods can be split into two classes. View-based and volumetric representations exemplify techniques that try to \"place\" geometric data onto a grid and apply existing deep learning algorithms to the adapted structure. Other methods replace the standard building blocks of deep neural architectures with special operations suitable for unstructured geometric data [29,6,31,34,36]. We provide details about the closest techniques to ours below.\n\nView-based Methods View-based techniques represent a 3D object as a collection of 2D views, to which standard CNNs used in image analysis can be applied. Typically, a CNN is applied to each view and then the resulting features are aggregated by a view pooling procedure [47]. View-based approaches are also good match for applications where the input comes from a 3D sensor and represented as a range image [53], in which case a single view can be used.\n\nVolumetric Methods Voxelization is a straightforward way to convert unstructured geometric data to a regular 3D grid over which standard CNN operations can be applied [30,54]. These volumetric representations are often wasteful, since voxelization produces a sparsely-occupied 3D grid. Time and space complexity considerations limit the resolution of the volumetric grids, yielding quantization artifacts. Recent space partition methods like k-d trees [20] or octrees [49] remedy some resolution issues but still rely on subdivision of a bounding volume rather than local geometric structure. Finally, [35] studied a combination of view-based and volumetric approaches for 3D shape classification.\n\nPointNets PointNets [34] comprise a special class of architectures for point sets like 3D point clouds. The key ingredient is a symmetric function applied to 3D coordinates in a manner invariant to permutation. While they achieve impressive performance on point cloud analysis tasks, PointNets treat each point individually, essentially learning a mapping from 3D to the latent features without leveraging local geometric structure. Furthermore, the learned mapping is sensitive to the global transformation of the point cloud; to cope with this issue, PointNet employs a complex and computationally expensive spatial transformer network [16] to learn 3D alignment.\n\nLocal information is important for feature learning in two ways. First, as for handcrafted descriptors, local features usually account for geometric relationships among neighboring points to be robust to various transformations. Second, local information is critical to the succcess of image-based deep convolutional architectures. Follow-up work proposed an improved PointNet++ architecture exploiting geometric features in local point sets and hierarchically aggregating them for inference [36]. A similar approach is proposed in [43], where initial point features are obtained from a point kernel correlation layer and then aggregated among nearby points. Benefiting from local structure, PointNet++ achieves state-of-the-art results on several point cloud analysis benchmarks. PointNet++, however, still treats individual points in local point sets independently and does not consider relationships between point pairs. Geometric Deep Learning PointNets exemplify a broad class of deep learning architectures on non-Euclidean structured data termed geometric deep learning [7]. These methods date back to early methods to construct neural networks on graphs [41]. More recently, [9] proposed a generalization of convolution for graphs via the Laplacian operator [44]. This foundational approach had a number of drawbacks including the computational complexity of Laplacian eigendecomposition, the large number of parameters to express the convolutional filters, and a lack of spatial localization. These issues are alleviated in follow-up work using polynomial [11,19] or rational [23] spectral filters that avoid the Laplacian eigendecomposition and also guarantee localization.\n\nSpectral graph CNN models are notable for isometry invariance and hence have applied to non-rigid shape analysis [5]. A key difficulty, however, is that the Laplacian eigenbasis is domain-dependent; thus, a filter learned on one shape may not generalize to others. Spectral transformer networks address this problem to some extent [56].\n\nAn alternative definition of non-Euclidean convolution employs spatial rather than spectral filters. The Geodesic CNN (GCNN) is a deep CNN on meshes generalizing the notion of patches using local intrinsic parameterization [29]. Its key advantage over spectral approaches is better generalization. Follow-up work proposed different local charting techniques using anisotropic diffusion [6] or Gaussian mixture models [52,31]. [25] incorporate a differentiable functional map [32] layer into a geometric deep neural network, allowing to do intrinsic structured prediction of correspondence between nonrigid shapes.\n\nThe last class of geometric deep learning approaches attempt to pull back a convolution operation by embedding the shape into a domain with shift-invariant structure such as the sphere [46], torus [28], or plane [13]. a\n\n\nOur approach\n\nWe propose an approach inspired by PointNet and convolution operations. Instead of working on individual points like PointNet, however, we exploit local geometric structures by constructing a local neighborhood graph and applying convolution-like operations on the edges connecting neighboring pairs of points, in the spirit graph neural networks. We show in the following that such an operation, dubbed edge convolution (EdgeConv), has the properties of lying between translation-invariant and non-locality.\n\nDifferently from graph CNNs, the graph is not fixed but rather is dynamically updated after each layer of the network. That is, the k-nearest neighbors of a point changes from layer to layer of the network and is computed from the sequence of embeddings. Proximity in feature space differs from proximity in the input, leading to nonlocal diffusion of information throughout the point cloud.\n\n\nEdge Convolution\n\nConsider a F -dimensional point cloud with n points, denoted by X = {x 1 , . . . , x n } \u2286 R F . In the simplest setting of F = 3, each point contains 3D coordinates x i = (x i , y i , z i ); it is also possible to include additional coordinates representing color, surface normal, and so on. In a deep neural network architecture, each subsequent layer operates on the output of the previous layer, so more generally the dimension F represents the feature dimensionality of a given layer.\n\nWe further assume to be given a directed graph G = (V, E) representing the local structure of the point cloud, where V = {1, . . . , n} and E \u2286 V \u00d7 V are the vertices and edges, respectively. In the simplest case, we construct G as the k-nearest neighbor (k-NN) graph in R F , containing directed edges of the form (i, j i1 ), . . . , (i, j ik ) such that points x ji1 , . . . , x j ik are the closest to x i . We define edge features\nas e ij = h \u0398 (x i , x j ), where h \u0398 : R F \u00d7 R F \u2192 R F\nis some parametric non-linear function parameterized by the set of learnable parameters \u0398.\n\nFinally, we define the EdgeConv operation by applying a channel-wise symmetric aggregation operation (e.g., The point cloud transform block is designed to align an input point set to a canonical space by applying an estimated 3 \u00d7 3 matrix. To estimate the 3 \u00d7 3 matrix, a tensor concatenating the coordinates of each point and the coordinate differences between its k neighboring points is used. EdgeConv block: The EdgeConv block takes as input a tensor of shape n \u00d7 f , computes edge features for each point by applying a multi-layer perceptron (mlp) with the number of layer neurons defined as {a1, a2, ..., an}, and generates a tensor of shape n \u00d7 an after pooling among neighboring edge features.\n\nor max) on the edge features associated with all the edges emanating from each vertex. The output of EdgeConv at the i-th vertex is thus given by\nx i = j:(i,j)\u2208E h \u0398 (x i , x j ).(1)\nMaking analogy to the classical convolution operation in images, we can regard x i as the central pixel and {x j : (i, j) \u2208 E} as a patch around it (see Figure 2). Overall, given an F -dimensional point cloud with n points, Edge-Conv produces an F -dimensional point cloud with the same number of points.\n\n\nChoice of h and\n\nThe choice of the edge function and the aggregation operation has a crucial influence on the properties of the resulting EdgeConv operation.\n\nFirst, note that in the setting when x 1 , . . . , x n represent image pixels layed out on a regular grid and the graph has a local connectivity representing patches of fixed size around each pixel, the choice h \u0398 (x i , x j ) = \u03b8 j x j as the edge function and sum as the aggregation operation yields the classical Euclidean convolution,\nx i = j:(i,j)\u2208E \u03b8 j x j ,\nwhere the parameters \u0398 = (\u03b8 i , . . . , \u03b8 k ) act as the weights of the filter.\n\nThe second possible choice of h is h \u0398 (x i , x j ) = h \u0398 (x i ), encoding only global shape information oblivious of the local neighborhood structure. This type of operation is used in PointNet, which can thus be regarded as a particular choice of our EdgeConv.\n\nA third option is h \u0398 (x i , x j ) = h \u0398 (x j \u2212 x i ). Note that such a choice encodes only local information, essentially treating the shape as a collection of small patches and losing the global shape structure.\n\nFinally, the fourth option, which we adopt in this paper, is an asymmetric edge function of the form\nh \u0398 (x i , x j ) = h \u0398 (x i , x j \u2212 x i ).\nSuch a function combines both the global shape structure (captured by the coordinates of the patch centers x i ) and local neighborhood information (captured by x j \u2212 x i ).\n\n\nDynamic Graph CNNs\n\nSimilarly to classical CNNs used in computer vision, the EdgeConv operation can be applied multiple times, possibly interleaved with pooling depending on the task at hand. When applied without pooling, multiple applications of EdgeConv produce effectively larger support ('receptive field') of the filter. We denote by X (l) = {x  R F l the output of the l-th layer; X (0) = X is the input point cloud.\n\nDynamic graph update Our experiments suggests that it is possible and actually beneficial to recompute the graph using nearest neighbors in the features space produces by each layer. This is a crucial distinction of our method from graph CNNs working on a fixed input graph. Such a dynamic graph update is the reason for the name of our architecture, the Dynamic Graph CNN (DGCNN). At each layer we thus have a different graph G (l) = (V (l) , E (l) ), where the l-th layer edges are of the form (i, j i1 ), . . . , (i, j ik l ) such that x i . The F l+1 -dimensional output of the (l + 1)-st layer is produced by applying EdgeConv to the F l -dimensional output of the l-th layer,\nx (l+1) i = j:(i,j)\u2208E (l) h (l) \u0398 (x (l) i , x (l) j ),(2)\nwhere h (l) :\nR F l \u00d7 R F l \u2192 R F l+1 .\n\nImplementation Details\n\nWe consider particular instances of Dynamic Graph CNNs for two prototypical tasks in point cloud analysis: classification and segmentation. The respective architectures, depicted in Figure 3 (top and bottom branches), have a similar structure to PointNet. Both architectures share a spatial transformer component, computing a global shape transformation. The classification network includes two EdgeConv layers, followed by a pooling operation and three fully-connected layers producing classification output scores. The segmentation network uses a sequence of three EdgeConv layers, followed by three fully-connected layers producing, for each point, segmentation output scores. For each EdgeConv block, we use a shared edge function\nh (l) (x (l) i , x (l) j ) = h(x (l) i , x (l) j \u2212 x (l)\ni ) across all layers; the function is implemented as a multi-layer perceptron (MLP) and = max aggregation operation.\n\nIn our classification architecture, the graph is constructed using k = 20 nearest neighbors, while in our segmentation architecture, k = 30.\n\n\nComparison to existing methods\n\nOur DGCNN is related to two classes of approaches, PointNets and graph CNNs, which we show to be particular settings of our method.\n\nPointNet is a special case of our method with k = 1, which results in a graph with an empty edge set E = \u2205.\n\n\nThe edge function used in PointNet is\nh(x i , x j ) = h(x i ),\nwhich considers only the global geometry but discards the local one. The aggregation operation used in PointNet is = max (or , because the aggregation function only works on a single node).\n\nPointNet++ tries to account for local point cloud structure by applying PointNet in a local manner. In terms of our notation, PointNet++ first constructs the graph according to the Euclidean distances between the points, and in each layer, applies a graph coarsening operation. For each layer, a certain number of points are selected by using farthest point sampling (FPS) algorithm. Only the selected points are preserved while others are directly discarded after this layer and in this way, the graph becomes smaller after the operation applied on each layer. Different from ours, PointNet++ computes pairwise distances using point input coordinates. The edge function used by PointNet++ is also h(x i , x j ) = h(x i ), and the aggregation operation is also a max.\n\nAmong graph CNNs, MoNet [31], ECC [45], and Graph Attention Networks [52] are the most related approaches. The common denominator of these methods is the notion of a local patch on a graph, in which a convolution-type operation can be defined. 1 Specifically, [31] use the graph structure to compute a local \"pseudo-coordinate system\" u in which the neighborhood vertices are represented; the convolution is then defined as an M -component Gaussian mix-ture in these coordinates:\nx i = M m=1 w m j:(i,j)\u2208E g \u0398m (u(x i , x j ))x j ,(3)\nwhere g denotes the Gaussian kernel, {\u0398 1 , . . . , \u0398 M } encode the learnable parameters of the Gaussians (mean and covariance), and {w 1 , . . . , w M } are the learnable filter coefficients. We can easily observe that (3) is an instance of our more general EdgeConv operation (1), with a particular choice of the edge function\nh w1,\u03981,...,w M ,\u0398 M (x i , x j ) = M m=1 w m g \u0398m (u(x i , x j ))x j\nand summation as the aggregation operation. A crucial difference between EdgeConv and MoNet and other graph CNN methods is that the latter assume a given fixed graph on which the convolution-like operations are applied, while we dynamically update the graph for each layer output. This way, our model not only learns how to extract local geometric features, but also how to group points in a point cloud. Figure 4 shows the distance in different feature spaces, exemplifying that the distances in deeper layers carry semantic information over long distances.\n\n\nEvaluation\n\nIn this section, we evaluate the models constructed using EdgeConv for different tasks: classification, part segmentation, and semantic segmentation. We also visualize experimental results to illustrate key differences from previous work.\n\n\nClassification\n\nData We evaluate our model on the ModelNet40 [54] classification task, consisting in predicting the category of a previously unseen shape. The dataset contains 12,311 meshed CAD models from 40 categories. 9,843 models are used for training and 2,468 models are for testing. We follow verbatim the experimental settings of [34]. For each model, 1,024 points are uniformly sampled from the mesh faces and normalized to the unit sphere. Only the (x, y, z) coordinates of the sampled points are used and the original meshes are discarded. During the training procedure, we augment the data as in [36] by randomly rotating and scaling objects and perturbing the object and point locations.\n\nArchitecture The network architecture used for the classification task is shown in Figure 3 (top branch). We use a local-aware spatial transformer network to align the point cloud. It has two shared fully-connected layers (64, 128) 2 to construct one EdgeConv layer and after which, one shared fully-connected layer (1024) is used to transform the pointwise features to higher-dimensional space. After the global max pooling, two fully-connected layers (512, 256) are used to compute the transformation matrix.\n\nWe use two EdgeConv layers to extract geometric features. The first EdgeConv layer uses three shared fullyconnected layers (64, 64, 64) while the second EdgeConv layer uses a shared fully-connected layer (128). Shortcut connections are included to extract multi-scale features and one shared fully-connected layer (1024) to aggregate multi-scale features. The number k of nearest neighbors is 20. Then, a global max pooling is used to get the point cloud global feature, after which two multi-layer perceptrons (512, 256) are used to transform the global feature. Dropout with keep probability of 0.5 is used in the last two fully-connected layers. All layers include ReLU and batch normalization.\n\nTraining We use the same training strategy as [34]. We use Adam [18] with learning rate 0.001 that is divided by 2 every 20 epochs. The decay rate for batch normalization is initially 0.5 and 0.99 finally. The batch size is 32 and the momentum is 0.9. Table 1 shows the results of the classification task. Our model achieves the best results on this dataset. Our baseline without transformer network and using fixed graph is 0.5% better than PointNet++. An advanced version including a local-aware network and dynamical graph recomputation achieves best results on this dataset. MEAN OVERALL CLASS ACCURACY ACCURACY 3DSHAPENETS [54] 77.3 84.7 VOXNET [30] 83.0 85.9 SUBVOLUME [35] 86 \n\n\nResults\n\n\nModel Complexity\n\nWe use the ModelNet40 [54] classification experiment to compare the complexity of our model to previous stateof-the-art. Table 2 shows that our model achieve the best tradeoff between the model complexity (number of parameters), computational complexity (measured as forward pass time), and achieved classification accuracy.\n\nOur baseline model outperforms the previous state-ofthe-art PointNet++ by 0.5% accuracy, at the same time being 5 times faster compared to PointNet++. Our baseline model does not use a spatial transformer and uses the fixed k-NN graph. A more advanced version of our model including a spatial transformer block and dynamically graph computation outperforms PointNet++ by 1.5% while having comparable number of parameters and computational complexity.  Table 2. Complexity, forward time and accuracy of different models\nMODEL\n\nMore Experiments on ModelNet40\n\nWe also experiment with various settings of our model on the ModelNet40 [54] dataset. In particular, we analyze the effectiveness of local-aware transformer network, different distance metrics, and explicit usage of x i \u2212 x j . Table 3 shows the results. \"Centralization\" denotes using concatenation of x i and x i \u2212 x j as the edge features rather than concatenating x i and x j . \"Spatial Transformer\" denotes the local-aware transformer network while \"Dynamic graph recomputation\" denotes we reconstruct the graph rather than using a fixed graph. By dynamically updating graph, there is about 0.2%\u223c0.3% improvement, and Figure 4 also verifies our hypothesis that the model can extract semantics. In the later layers, certain patterns occur for recognition tasks. Explicitly centralizing each patch by using the concatenation of x i and x i \u2212 x j makes the operator more robust to translation, leading to about 0.2%\u223c0.3% improvement for overall accuracy. The local-aware transformer makes the model invariant to rigid transformation and leads to approximately 0.7% improvement.\n\nWe also experiment with different numbers k of nearest neighbors as shown in Table 4. While we do not exhaustively experiment with all possible k, we find with large k that the performance degenerates. This confirms our hypothesis that with large k the Euclidean distance fails to approximate geodesic distance, destroying the geometry of each patch.\n\nWe further evaluate the robustness of our model (trained on 1,024 points with k = 20) to point cloud density. We simulate the environment that random input points drops out during testing. Figure 5 shows that even half of points is dropped, the model still achieves reasonable results. With fewer than 512 points, however, performance degenerates    Table 4. Results of our model with different numbers of nearest neighbors.\n\n\nPart Segmentation\n\nData We extend our EdgeConv model architectures for part segmentation task on ShapeNet part dataset [55]. For this task, each point from a point cloud set is classified into one of a few predefined part category labels. The dataset  contains 16,881 3D shapes from 16 object categories, annotated with 50 parts in total. 2,048 points are sampled from each training shape, and most sampled point sets are labeled with less than six parts. We follow the official train/validation/test split scheme as in [10] in our experiment.\n\nArchitecture The network architecture is illustrated in Figure 3 (bottom branch). The same spatial transformer network is used for segmentation task. Training The same training setting as in our classification task is adopted, except k is changed from 20 to 30 due to the increase of point density. A distributed training scheme is further implemented on two NVIDIA TITAN X GPUs to maintain the training batch size.\n\n\nSource points\n\nOther point clouds from the same category Figure 9. Visualize the Euclidean distance (yellow: near, blue: far) between source points (red points in the left column) and multiple point clouds from the same category in the feature space after the third EdgeConv layer. Notice source points not only capture semantically similar structures in the point clouds that they belong to, but also capture semantically similar structures in other point clouds from the same category.\n\nResults We use Intersection-over-Union (IoU) on points to evaluate our model and compare with other benchmarks. We follow the same evaluation scheme as PointNet: The IoU of a shape is computed by averaging the IoUs of different parts occurring in that shape; The IoU of a category is obtained by averaging the IoUs of all the shapes belonging to that category. The mean IoU (mIoU) is finally calculated by averaging the IoUs of all the testing shapes. We compare our results with PointNet [34], PointNet++ [36], Kd-Net [20], and LocalFeatureNet [43]. The evaluation results are shown in Table 5. We also visually compare the results of our model and PointNet in Figure 10.\n\nIntra-cloud distances We next explore the relationships between different point clouds captured using our features. As shown in Figure 9, we take one red point from a source point cloud and compute its distance in feature space to PointNet Ours Ground truth  points in other point clouds from the same category. An interesting finding is that although points are from different sources, they are close to each other if they are from semantically similar parts. We evaluate on the features after the third layer of our segmentation model for this experiment.\n\nSegmentation on partial data Our model is robust to partial data. We simulate the environment that part of the shape is dropped from one of six sides (top, bottom, right, left, front and back) with different percentages. The results are shown in Figure 11. On the left, the mean IoU versus \"keep ratio\" is shown. On the right, the results for an airplane model are visualized. AREO  BAG  CAP  CAR  CHAIR  EAR  GUITAR  KNIFE  LAMP  LAPTOP  MOTOR  MUG  PISTOL  ROCKET  SKATE  TABLE  WINNING   .   PHONE  BOARD  CATEGORIES   # SHAPES  2690  76  55  898  3758  69  787  392  1547  451  202  184  283  66  152 \n\n\nMEAN\n\n\nIndoor Scene Segmentation\n\nData We evaluate our model on Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) [1] for a semantic scene segmentation task. This dataset includes 3D scan point clouds for 6 indoor areas including 272 rooms in total. Each point belongs to one of 13 semantic categories-e.g. board, bookcase, chair, ceiling, and beam-plus clutter. We follow the same setting as in [34], where each room is split into blocks with area 1m \u00d7 1m, and each point is represented as a 9D vector (XYZ, RGB, and normalized spatial coordinates). 4,096 points are sampled for each block during training process, and all points are used for testing. We also use the same 6-fold cross validation over the 6 areas, and the average evaluation results are reported. The model used for this task is similar to part segmentation model, except that a probability distribution over semantic object classes is generated for each input point. We compare our model with both PointNet [34] and Point-Net baseline, where additional point features (local point density, local curvature and normal) are used to construct handcrafted features and then fed to an MLP classifier. We further compare our work with [12], who present network architectures to enlarge the receptive field over the 3D scene. Two different approaches are proposed in their work: MS+CU for multi-scale block features with consolidation units; G+RCU for the grid-blocks with recurrent consolidation Units. We report evaluation results in Table 6, and visually compare the results of PointNet and our model in Figure 13. \n\n\nSurface Normal Prediction\n\nWe can also adapt our segmentation model to predict surface normals from point clouds.\n\nData We still use the ModelNet40 dataset. The surface normals are sampled directly from CAD models. The normal of one point is represented by (n x , n y , n z ). We use 9,843 models for training and 2,468 models for testing.\n\nArchitecture We change the last layer of our segmentation model to output 3 continuous values; mean squared error (MSE) is used as the training loss.\n\nResults Qualitative results are shown in Figure 12, compared with ground truth. Our model faithfully captures orientation even in the presence of fairly sharp features.\n\n\nDiscussion\n\nIn this work we propose a new operator for learning on point cloud and show its performance on various tasks. The success of our technique verifies our hypothesis that local geometric features are crucial to 3D recognition tasks, even after introducing machinery from deep learning. Furthermore, we show our model can be easily modified for various tasks like normal prediction while continuing to achieve reasonable results.\n\nWhile our architectures easily can be incorporated asis into existing pipelines for point cloud-based graphics, learning, and vision, our experiments also indicate several PointNet Ours\n\nGround truth Real color Figure 13. Semantic segmentation results. From left to right: PointNet, ours, ground truth and point cloud with original color. Notice our model outputs smoother segmentation results, for example, wall (cyan) in top two rows, chairs (red) and columns (magenta) in bottom two rows.\n\navenues for future research and extension. Primarily, the success of our model suggests that intrinsic features can be equally valuable if not more than simply point coordinates; developing a practical and theoretically-justified framework for balancing intrinsic and extrinsic considerations in a learning pipeline will require insight from theory and practice in geometry processing. Another possible extension is to design a non-shared transformer network that works on each local patch differently, adding flexibility to our model. Finally, we will consider applications of our techniques to more abstract point clouds coming from ap-plications like document retrieval rather than 3D geometry; beyond broadening the applicability of our technique, these experiments will provide insight into the role of geometry in abstract data processing.\n\nFigure 2 .\n2Figure 2. Left: An example of computing an edge feature, eij, from a point pair, xi and xj. In this example, h \u0398 () is instantiated using a fully connected layer, and the learnable parameters are its associated weights and bias. Right: Visualize the EdgeConv operation. The output of EdgeConv is calculated by aggregating the edge features associated with all the edges emanating from each connected vertex.\n\nFigure 3 .\n3Model architectures: The model architectures used for classification (top branch) and segmentation (bottom branch). The classification model takes as input n points, calculates an edge feature set of size k for each point at an EdgeConv layer, and aggregates features within each set to compute EdgeConv responses for corresponding points. The output features of the last EdgeConv layer are aggregated globally to form an 1D global descriptor, which is used to generate classification scores for c classes. The segmentation model extends the classification model by concatenating the 1D global descriptor and all the EdgeConv outputs (serving as local descriptors) for each point. It outputs per-point classification scores for p semantic labels. For illustration purposes, two arrowed arcs are plotted to represent feature concatenation. Point cloud transform block:\n\nFigure 4 .\n4Structure of the feature spaces produced at different stages of our shape classification neural network architecture, visualized as the distance between the red point to the rest of the points. For each set, Left: Euclidean distance in the input R 3 space; Middle: Distance after the point cloud transform stage, amounting to a global transformation of the shape; Right: Distance in the feature space of the last layer. Observe how in the feature space of deeper layers semantically similar structures such as shelves of a bookshelf or legs of a table are brought close together, although they are distant in the original space.\n\n\n, . . . , x (l) j ik l are the k l points closest to x (l)\n\nFigure 5 .\n5Left: Results of our model tested with random input dropout. The model is trained with number of points being 1024 and k being 20. Right: Point clouds with different number of points. The numbers of points are shown below the bottom row.\n\nFigure 6 .\n6Our part segmentation testing results for tables. dramatically. CENT DYN XFORM MEAN CLASS ACCURACY(%) OVERALL ACCURACY(\n\nFigure 7 .\n7Our part segmentation testing results for chairs.\n\nFigure 8 .\n8Our part segmentation testing results for lamps.\n\nFigure 10 .\n10Compare part segmentation results. For each set, from left to right: PointNet, ours and ground truth.\n\nFigure 11 .\n11Left: The mean IoU (%) improves when the ratio of kept points increases. Points are dropped from one of six sides (top, bottom, left, right, front and back) randomly during evaluation process. Right: Part segmentation results on partial data. Points on each row are dropped from the same side. The keep ratio is shown below the bottom row. Note that the segmentation results of turbines are improved when more points are included.\n\nFigure 12 .\n12Surface normal estimation results. The colors shown in the figure are RGB-coded surface normals, meaning XYZ components of surface normal vectors are put into RGB color channels. For each pair: our prediction (left) and ground truth (right).\n\n\nTable 5. Part segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points.5271 \n\nPOINTNET \n83.7 \n83.4 \n78.7 82.5 74.9 \n89.6 \n73.0 \n91.5 \n85.9 \n80.8 \n95.3 \n65.2 \n93.0 \n81.2 \n57.9 \n72.8 \n80.6 \n1 \nPOINTNET++ \n85.1 \n82.4 \n79.0 87.7 77.3 \n90.8 \n71.8 \n91.0 \n85.9 \n83.7 \n95.3 \n71.6 \n94.1 \n81.3 \n58.7 \n76.4 \n82.6 \n5 \nKD-NET \n82.3 \n80.1 \n74.6 74.3 70.3 \n88.6 \n73.5 \n90.2 \n87.2 \n81.0 \n94.9 \n57.4 \n86.7 \n78.1 \n51.8 \n69.9 \n80.3 \n0 \nLOCALFEATURENET \n84.3 \n86.1 \n73.0 54.9 77.4 \n88.8 \n55.0 \n90.6 \n86.5 \n75.2 \n96.1 \n57.3 \n91.7 \n83.1 \n53.9 \n72.5 \n83.8 \n5 \n\nOURS \n85.1 \n84.2 \n83.7 84.4 77.1 \n90.9 \n78.5 \n91.5 \n87.3 \n82.9 \n96.0 \n67.8 \n93.3 \n82.6 \n59.7 \n75.5 \n82.0 \n6 \n\n\n\n\nTable 6. 3D semantic segmentation results on S3DIS. MS+CU for multi-scale block features with consolidation units; G+RCU for the grid-blocks with recurrent consolidation Units.MEAN \n\nOVERALL \n\nIOU \n\nACCURACY \n\nPOINTNET (BASELINE) [34] \n20.1 \n53.2 \nPOINTNET [34] \n47.6 \n78.5 \nMS + CU(2) [12] \n47.8 \n79.2 \nG + RCU [12] \n49.7 \n81.1 \n\nOURS \n56.1 \n84.1 \n\n\nThe methods of[45] and[52] can be considered as instances of[31], with the difference that the weights are constructed employing features from the adjacent nodes instead of the graph structure.\nHere, we use (64, 128) to denote that the two fully-connected layers have 64 filters and 128 filters, respectively; we use the same notation for the remainder of our discussion.\n\n3d semantic parsing of large-scale indoor spaces. I Armeni, O Sener, A R Zamir, H Jiang, I Brilakis, M Fischer, S Savarese, Proc. CVPR. CVPRI. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. 3d semantic parsing of large-scale indoor spaces. In Proc. CVPR, 2016.\n\nThe wave kernel signature: A quantum mechanical approach to shape analysis. M Aubry, U Schlickewei, D Cremers, Proc. ICCV Workshops. ICCV WorkshopsM. Aubry, U. Schlickewei, and D. Cremers. The wave kernel signature: A quantum mechanical approach to shape analysis. In Proc. ICCV Workshops, 2011.\n\nShape context: A new descriptor for shape matching and object recognition. S Belongie, J Malik, J Puzicha, Proc. NIPS. NIPSS. Belongie, J. Malik, and J. Puzicha. Shape context: A new descriptor for shape matching and object recog- nition. In Proc. NIPS, 2001.\n\nRecent trends, applications, and perspectives in 3d shape similarity assessment. S Biasotti, A Cerri, A Bronstein, M Bronstein, Computer Graphics Forum. 356S. Biasotti, A. Cerri, A. Bronstein, and M. Bronstein. Recent trends, applications, and perspectives in 3d shape similarity assessment. Computer Graphics Fo- rum, 35(6):87-119, 2016.\n\nLearning classspecific descriptors for deformable shapes using localized spectral convolutional networks. D Boscaini, J Masci, S Melzi, M M Bronstein, U Castellani, P Vandergheynst, Computer Graphics Forum. 345D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and P. Vandergheynst. Learning class- specific descriptors for deformable shapes using lo- calized spectral convolutional networks. Computer Graphics Forum, 34(5):13-23, 2015.\n\nLearning shape correspondence with anisotropic convolutional neural networks. D Boscaini, J Masci, E Rodol\u00e0, M Bronstein, Proc. NIPS. NIPSD. Boscaini, J. Masci, E. Rodol\u00e0, and M. Bronstein. Learning shape correspondence with anisotropic con- volutional neural networks. In Proc. NIPS, 2016.\n\nGeometric deep learning: going beyond euclidean data. M M Bronstein, J Bruna, Y Lecun, A Szlam, P Vandergheynst, IEEE Signal Processing Magazine. 344M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going be- yond euclidean data. IEEE Signal Processing Maga- zine, 34(4):18-42, 2017.\n\nScale-invariant heat kernel signatures for non-rigid shape recognition. M M Bronstein, I Kokkinos, Proc. CVPR. CVPRM. M. Bronstein and I. Kokkinos. Scale-invariant heat kernel signatures for non-rigid shape recognition. In Proc. CVPR, 2010.\n\nJ Bruna, W Zaremba, A Szlam, Y Lecun, arXiv:1312.6203Spectral networks and locally connected networks on graphs. J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. arXiv:1312.6203, 2013.\n\nA X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012An information-rich 3d model repository. A. X. Chang, T. Funkhouser, L. Guibas, P. Hanra- han, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv:1512.03012, 2015.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. M Defferrard, X Bresson, P Vandergheynst, Proc. NIPS. NIPSM. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast lo- calized spectral filtering. In Proc. NIPS, 2016.\n\nExploring spatial context for 3d semantic segmentation of point clouds. F Engelmann, T Kontogianni, A Hermans, B Leibe, Proc. CVPR. CVPRF. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe. Exploring spatial context for 3d semantic segmentation of point clouds. In Proc. CVPR, 2017.\n\nGwcnn: A metric alignment layer for deep shape analysis. D Ezuz, J Solomon, V G Kim, M Ben-Chen, Computer Graphics Forum. 365D. Ezuz, J. Solomon, V. G. Kim, and M. Ben-Chen. Gwcnn: A metric alignment layer for deep shape anal- ysis. Computer Graphics Forum, 36(5):49-57, 2017.\n\nShapebased recognition of 3d point clouds in urban environments. A Golovinskiy, V G Kim, T Funkhouser, Proc. ICCV. ICCVA. Golovinskiy, V. G. Kim, and T. Funkhouser. Shape- based recognition of 3d point clouds in urban environ- ments. In Proc. ICCV, 2009.\n\n3d object recognition in cluttered scenes with local surface features: a survey. Y Guo, M Bennamoun, F Sohel, M Lu, J Wan, Trans. PAMI. 3611Y. Guo, M. Bennamoun, F. Sohel, M. Lu, and J. Wan. 3d object recognition in cluttered scenes with local surface features: a survey. Trans. PAMI, 36(11):2270- 2287, 2014.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Proc. NIPS. NIPSM. Jaderberg, K. Simonyan, A. Zisserman, et al. Spa- tial transformer networks. In Proc. NIPS, 2015.\n\nUsing spin images for efficient object recognition in cluttered 3D scenes. A E Johnson, M Hebert, Trans. PAMI. 215A. E. Johnson and M. Hebert. Using spin images for efficient object recognition in cluttered 3D scenes. Trans. PAMI, 21(5):433-449, 1999.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, D. P. Kingma and J. Ba. Adam: A method for stochas- tic optimization. 2015.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, T. N. Kipf and M. Welling. Semi-supervised classifi- cation with graph convolutional networks. 2017.\n\nEscape from cells: Deep kd-networks for the recognition of 3d point cloud models. R Klokov, V Lempitsky, R. Klokov and V. Lempitsky. Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. 2017.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. NIPS. NIPSA. Krizhevsky, I. Sutskever, and G. E. Hinton. Im- agenet classification with deep convolutional neural networks. In Proc. NIPS, 2012.\n\nBackpropagation applied to handwritten zip code recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , Neural computation. 14Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backprop- agation applied to handwritten zip code recognition. Neural computation, 1(4):541-551, 1989.\n\nR Levie, F Monti, X Bresson, M M Bronstein, arXiv:1705.07664Cayleynets: Graph convolutional neural networks with complex rational spectral filters. R. Levie, F. Monti, X. Bresson, and M. M. Bronstein. Cayleynets: Graph convolutional neu- ral networks with complex rational spectral filters. arXiv:1705.07664, 2017.\n\nShape classification using the inner-distance. H Ling, D W Jacobs, Trans. PAMI. 292H. Ling and D. W. Jacobs. Shape classification us- ing the inner-distance. Trans. PAMI, 29(2):286-299, 2007.\n\nDeep functional maps: Structured prediction for dense shape correspondence. O Litany, T Remez, E Rodol\u00e0, A M Bronstein, M M Bronstein, Proc. ICCV. ICCVO. Litany, T. Remez, E. Rodol\u00e0, A. M. Bronstein, and M. M. Bronstein. Deep functional maps: Structured prediction for dense shape correspondence. In Proc. ICCV, 2017.\n\nRecognizing objects in 3d point clouds with multi-scale local features. M Lu, Y Guo, J Zhang, Y Ma, Y Lei, Sensors. 1412M. Lu, Y. Guo, J. Zhang, Y. Ma, and Y. Lei. Recog- nizing objects in 3d point clouds with multi-scale local features. Sensors, 14(12):24156-24173, 2014.\n\nIntegral invariants for shape matching. S Manay, D Cremers, B.-W Hong, A J Yezzi, S Soatto, Trans. PAMI. 2810S. Manay, D. Cremers, B.-W. Hong, A. J. Yezzi, and S. Soatto. Integral invariants for shape matching. Trans. PAMI, 28(10):1602-1618, 2006.\n\nConvolutional neural networks on surfaces via seamless toric covers. H Maron, M Galun, N Aigerman, M Trope, N Dym, E Yumer, V G Kim, Y Lipman, Proc. SIGGRAPH. SIGGRAPHH. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim, and Y. Lipman. Convo- lutional neural networks on surfaces via seamless toric covers. In Proc. SIGGRAPH, 2017.\n\nGeodesic convolutional neural networks on riemannian manifolds. J Masci, D Boscaini, M Bronstein, P Vandergheynst, Proc. 3dRR. 3dRRJ. Masci, D. Boscaini, M. Bronstein, and P. Van- dergheynst. Geodesic convolutional neural networks on riemannian manifolds. In Proc. 3dRR, 2015.\n\nVoxnet: A 3d convolutional neural network for real-time object recognition. D Maturana, S Scherer, Proc. IROS. IROSD. Maturana and S. Scherer. Voxnet: A 3d convolu- tional neural network for real-time object recognition. In Proc. IROS, 2015.\n\nGeometric deep learning on graphs and manifolds using mixture model cnns. F Monti, D Boscaini, J Masci, E Rodol\u00e0, J Svoboda, M M Bronstein, Proc. CVPR. CVPRF. Monti, D. Boscaini, J. Masci, E. Rodol\u00e0, J. Svo- boda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proc. CVPR, 2017.\n\nFunctional maps: a flexible representation of maps between shapes. M Ovsjanikov, M Ben-Chen, J Solomon, A Butscher, L Guibas, TOG. 31430M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, and L. Guibas. Functional maps: a flexible representation of maps between shapes. TOG, 31(4):30, 2012.\n\nC R Qi, W Liu, C Wu, H Su, L J Guibas, arXiv:1711.08488Frustum pointnets for 3d object detection from rgb-d data. C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frustum pointnets for 3d object detection from rgb-d data. arXiv:1711.08488, 2017.\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, Proc. CVPR. CVPRC. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proc. CVPR, 2017.\n\nVolumetric and multi-view cnns for object classification on 3d data. C R Qi, H Su, M Nie\u00dfner, A Dai, M Yan, L J Guibas, Proc. CVPR. CVPRC. R. Qi, H. Su, M. Nie\u00dfner, A. Dai, M. Yan, and L. J. Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Proc. CVPR, 2016.\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Proc. NIPS. NIPSC. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Proc. NIPS, 2017.\n\nLaplace-beltrami eigenfunctions for deformation invariant shape representation. R M Rustamov, Proc. SGP. SGPR. M. Rustamov. Laplace-beltrami eigenfunctions for deformation invariant shape representation. In Proc. SGP, 2007.\n\nFast point feature histograms (fpfh) for 3d registration. R B Rusu, N Blodow, M Beetz, Proc. ICRA. ICRAR. B. Rusu, N. Blodow, and M. Beetz. Fast point fea- ture histograms (fpfh) for 3d registration. In Proc. ICRA, 2009.\n\nAligning point cloud views using persistent feature histograms. R B Rusu, N Blodow, Z C Marton, M Beetz, Proc. IROS. IROSR. B. Rusu, N. Blodow, Z. C. Marton, and M. Beetz. Aligning point cloud views using persistent feature histograms. In Proc. IROS, 2008.\n\nTowards 3D Point Cloud Based Object Maps for Household Environments. R B Rusu, Z C Marton, N Blodow, M Dolha, M Beetz, Robotics and Autonomous Systems Journal. 5611R. B. Rusu, Z. C. Marton, N. Blodow, M. Dolha, and M. Beetz. Towards 3D Point Cloud Based Ob- ject Maps for Household Environments. Robotics and Autonomous Systems Journal, 56(11):927-941, 30 November 2008.\n\nThe graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE Tran. Neural Networks. 201F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Tran. Neural Networks, 20(1):61-80, 2009.\n\nEl-Sallam. 3d-div: A novel local surface descriptor for feature matching and pairwise range image registration. S A A Shah, M Bennamoun, F Boussaid, A A , Proc. ICIP. ICIPS. A. A. Shah, M. Bennamoun, F. Boussaid, and A. A. El-Sallam. 3d-div: A novel local surface descriptor for feature matching and pairwise range image regis- tration. In Proc. ICIP, 2013.\n\nNeighbors do help: Deeply exploiting local structures of point clouds. Y Shen, C Feng, Y Yang, D Tian, arXiv:1712.06760Y. Shen, C. Feng, Y. Yang, and D. Tian. Neighbors do help: Deeply exploiting local structures of point clouds. arXiv:1712.06760, 2017.\n\nThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. D I Shuman, S K Narang, P Frossard, A Ortega, P Vandergheynst, IEEE Signal Processing Magazine. 303D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular do- mains. IEEE Signal Processing Magazine, 30(3):83- 98, 2013.\n\nDynamic edgeconditioned filters in convolutional neural networks on graphs. M Simonovsky, N Komodakis, Proc. CVPR. CVPRM. Simonovsky and N. Komodakis. Dynamic edge- conditioned filters in convolutional neural networks on graphs. In Proc. CVPR, 2017.\n\nDeep learning 3d shape surfaces using geometry images. A Sinha, J Bai, K Ramani, Proc. ECCV. ECCVA. Sinha, J. Bai, and K. Ramani. Deep learning 3d shape surfaces using geometry images. In Proc. ECCV, 2016.\n\nMulti-view convolutional neural networks for 3d shape recognition. H Su, S Maji, E Kalogerakis, E Learned-Miller, Proc. CVPR. CVPRH. Su, S. Maji, E. Kalogerakis, and E. Learned- Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proc. CVPR, 2015.\n\nA concise and provably informative multi-scale signature based on heat diffusion. J Sun, M Ovsjanikov, L Guibas, Computer Graphics Forum. 285J. Sun, M. Ovsjanikov, and L. Guibas. A con- cise and provably informative multi-scale signature based on heat diffusion. Computer Graphics Forum, 28(5):1383-1392, 2009.\n\nOctree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. M Tatarchenko, A Dosovitskiy, T Brox, Proc. ICCV. ICCVM. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efficient convolutional architec- tures for high-resolution 3d outputs. In Proc. ICCV, 2017.\n\nA combined texture-shape descriptor for enhanced 3d feature matching. F Tombari, S Salti, L. Di Stefano, Proc. ICIP. ICIPF. Tombari, S. Salti, and L. Di Stefano. A com- bined texture-shape descriptor for enhanced 3d feature matching. In Proc. ICIP, 2011.\n\nA survey on shape correspondence. O Van Kaick, H Zhang, G Hamarneh, D Cohen-Or, Computer Graphics Forum. 306O. Van Kaick, H. Zhang, G. Hamarneh, and D. Cohen- Or. A survey on shape correspondence. Computer Graphics Forum, 30(6):1681-1707, 2011.\n\nP Veli\u010dkovi\u0107, G Cucurull, A Casanova, A Romero, P Li\u00f2, Y Bengio, arXiv:1710.10903Graph attention networks. P. Veli\u010dkovi\u0107, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention networks. arXiv:1710.10903, 2017.\n\nDense human body correspondences using convolutional networks. L Wei, Q Huang, D Ceylan, E Vouga, H Li, Proc. CVPR. CVPRL. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense human body correspondences using convolu- tional networks. In Proc. CVPR, 2016.\n\n3d shapenets: A deep representation for volumetric shapes. Z Wu, S Song, A Khosla, F Yu, L Zhang, X Tang, J Xiao, Proc. CVPR. CVPRZ. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proc. CVPR, 2015.\n\nA scalable active framework for region annotation in 3d shape collections. L Yi, V G Kim, D Ceylan, I Shen, M Yan, H Su, A Lu, Q Huang, A Sheffer, L Guibas, TOG. 356210L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, A. Lu, Q. Huang, A. Sheffer, L. Guibas, et al. A scalable active framework for region annotation in 3d shape collections. TOG, 35(6):210, 2016.\n\nSyncspeccnn: Synchronized spectral cnn for 3d shape segmentation. L Yi, H Su, X Guo, L Guibas, Proc. CVPR. CVPRL. Yi, H. Su, X. Guo, and L. Guibas. Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation. In Proc. CVPR, 2017.\n\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, Proc. ICRA. ICRAY. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-driven visual nav- igation in indoor scenes using deep reinforcement learning. In Proc. ICRA, 2017.\n", "annotations": {"author": "[{\"end\":81,\"start\":50},{\"end\":108,\"start\":82},{\"end\":143,\"start\":109},{\"end\":189,\"start\":144},{\"end\":224,\"start\":190},{\"end\":229,\"start\":225},{\"end\":268,\"start\":230},{\"end\":287,\"start\":269}]", "publisher": null, "author_last_name": "[{\"end\":58,\"start\":54},{\"end\":93,\"start\":90},{\"end\":118,\"start\":115},{\"end\":163,\"start\":154},{\"end\":206,\"start\":199}]", "author_first_name": "[{\"end\":53,\"start\":50},{\"end\":89,\"start\":82},{\"end\":114,\"start\":109},{\"end\":151,\"start\":144},{\"end\":153,\"start\":152},{\"end\":196,\"start\":190},{\"end\":198,\"start\":197},{\"end\":228,\"start\":225}]", "author_affiliation": "[{\"end\":267,\"start\":231},{\"end\":286,\"start\":270}]", "title": "[{\"end\":47,\"start\":1},{\"end\":334,\"start\":288}]", "venue": null, "abstract": "[{\"end\":1990,\"start\":356}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2607,\"start\":2603},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2635,\"start\":2631},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2650,\"start\":2646},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2689,\"start\":2685},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3395,\"start\":3391},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3398,\"start\":3395},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3401,\"start\":3398},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3681,\"start\":3677},{\"end\":4040,\"start\":4032},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4806,\"start\":4802},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4809,\"start\":4806},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5242,\"start\":5238},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5514,\"start\":5510},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5517,\"start\":5514},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8330,\"start\":8326},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8333,\"start\":8330},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8335,\"start\":8333},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8590,\"start\":8587},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8608,\"start\":8604},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8632,\"start\":8628},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8665,\"start\":8661},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8696,\"start\":8692},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8699,\"start\":8696},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8727,\"start\":8723},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9064,\"start\":9060},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9108,\"start\":9106},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9126,\"start\":9123},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9224,\"start\":9220},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9227,\"start\":9224},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9492,\"start\":9488},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9495,\"start\":9492},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10448,\"start\":10444},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10450,\"start\":10448},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10453,\"start\":10450},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10456,\"start\":10453},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10459,\"start\":10456},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10799,\"start\":10795},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10936,\"start\":10932},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11151,\"start\":11147},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11154,\"start\":11151},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11436,\"start\":11432},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11452,\"start\":11448},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11586,\"start\":11582},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11703,\"start\":11699},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12321,\"start\":12317},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12842,\"start\":12838},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12882,\"start\":12878},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13426,\"start\":13423},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13512,\"start\":13508},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13532,\"start\":13529},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13616,\"start\":13612},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13915,\"start\":13911},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13918,\"start\":13915},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13935,\"start\":13931},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14147,\"start\":14144},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":14366,\"start\":14362},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14596,\"start\":14592},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14758,\"start\":14755},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":14790,\"start\":14786},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14793,\"start\":14790},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14799,\"start\":14795},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14848,\"start\":14844},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15173,\"start\":15169},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15185,\"start\":15181},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15200,\"start\":15196},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23423,\"start\":23419},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23433,\"start\":23429},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23468,\"start\":23464},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23640,\"start\":23639},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23659,\"start\":23655},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":25209,\"start\":25205},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25486,\"start\":25482},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25756,\"start\":25752},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27107,\"start\":27103},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27125,\"start\":27121},{\"end\":27640,\"start\":27636},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27689,\"start\":27685},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27711,\"start\":27707},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27736,\"start\":27732},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27797,\"start\":27793},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":28731,\"start\":28727},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":30638,\"start\":30634},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31039,\"start\":31035},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32460,\"start\":32456},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32477,\"start\":32473},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32490,\"start\":32486},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32516,\"start\":32512},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33929,\"start\":33926},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34212,\"start\":34208},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34792,\"start\":34788},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35014,\"start\":35010},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":42210,\"start\":42206},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":42218,\"start\":42214},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":42256,\"start\":42252}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38256,\"start\":37836},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39137,\"start\":38257},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39779,\"start\":39138},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39840,\"start\":39780},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40091,\"start\":39841},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40224,\"start\":40092},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40287,\"start\":40225},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40349,\"start\":40288},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40466,\"start\":40350},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40912,\"start\":40467},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41169,\"start\":40913},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41838,\"start\":41170},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42191,\"start\":41839}]", "paragraph": "[{\"end\":3131,\"start\":2006},{\"end\":3682,\"start\":3133},{\"end\":4982,\"start\":3684},{\"end\":5872,\"start\":4984},{\"end\":6308,\"start\":5874},{\"end\":6740,\"start\":6310},{\"end\":6819,\"start\":6742},{\"end\":6987,\"start\":6821},{\"end\":7082,\"start\":6989},{\"end\":7193,\"start\":7084},{\"end\":7726,\"start\":7195},{\"end\":9228,\"start\":7743},{\"end\":9626,\"start\":9230},{\"end\":10052,\"start\":9628},{\"end\":10523,\"start\":10054},{\"end\":10978,\"start\":10525},{\"end\":11677,\"start\":10980},{\"end\":12344,\"start\":11679},{\"end\":14029,\"start\":12346},{\"end\":14367,\"start\":14031},{\"end\":14982,\"start\":14369},{\"end\":15203,\"start\":14984},{\"end\":15728,\"start\":15220},{\"end\":16121,\"start\":15730},{\"end\":16631,\"start\":16142},{\"end\":17067,\"start\":16633},{\"end\":17214,\"start\":17124},{\"end\":17917,\"start\":17216},{\"end\":18064,\"start\":17919},{\"end\":18406,\"start\":18102},{\"end\":18566,\"start\":18426},{\"end\":18906,\"start\":18568},{\"end\":19012,\"start\":18933},{\"end\":19276,\"start\":19014},{\"end\":19491,\"start\":19278},{\"end\":19593,\"start\":19493},{\"end\":19810,\"start\":19637},{\"end\":20235,\"start\":19833},{\"end\":20918,\"start\":20237},{\"end\":20991,\"start\":20978},{\"end\":21777,\"start\":21043},{\"end\":21952,\"start\":21835},{\"end\":22094,\"start\":21954},{\"end\":22260,\"start\":22129},{\"end\":22369,\"start\":22262},{\"end\":22624,\"start\":22435},{\"end\":23393,\"start\":22626},{\"end\":23874,\"start\":23395},{\"end\":24259,\"start\":23930},{\"end\":24888,\"start\":24330},{\"end\":25141,\"start\":24903},{\"end\":25844,\"start\":25160},{\"end\":26356,\"start\":25846},{\"end\":27055,\"start\":26358},{\"end\":27740,\"start\":27057},{\"end\":28095,\"start\":27771},{\"end\":28615,\"start\":28097},{\"end\":29734,\"start\":28655},{\"end\":30086,\"start\":29736},{\"end\":30512,\"start\":30088},{\"end\":31058,\"start\":30534},{\"end\":31475,\"start\":31060},{\"end\":31965,\"start\":31493},{\"end\":32639,\"start\":31967},{\"end\":33198,\"start\":32641},{\"end\":33805,\"start\":33200},{\"end\":35392,\"start\":33842},{\"end\":35508,\"start\":35422},{\"end\":35734,\"start\":35510},{\"end\":35885,\"start\":35736},{\"end\":36055,\"start\":35887},{\"end\":36495,\"start\":36070},{\"end\":36682,\"start\":36497},{\"end\":36988,\"start\":36684},{\"end\":37835,\"start\":36990}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17123,\"start\":17068},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18101,\"start\":18065},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18932,\"start\":18907},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19636,\"start\":19594},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20977,\"start\":20919},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21017,\"start\":20992},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21834,\"start\":21778},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22434,\"start\":22410},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23929,\"start\":23875},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24329,\"start\":24260},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28621,\"start\":28616}]", "table_ref": "[{\"end\":27316,\"start\":27309},{\"end\":27899,\"start\":27892},{\"end\":28556,\"start\":28549},{\"end\":28890,\"start\":28883},{\"end\":29820,\"start\":29813},{\"end\":30445,\"start\":30438},{\"end\":32561,\"start\":32554},{\"end\":33804,\"start\":33577},{\"end\":35317,\"start\":35310}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2004,\"start\":1992},{\"attributes\":{\"n\":\"2.\"},\"end\":7741,\"start\":7729},{\"attributes\":{\"n\":\"3.\"},\"end\":15218,\"start\":15206},{\"attributes\":{\"n\":\"3.1.\"},\"end\":16140,\"start\":16124},{\"end\":18424,\"start\":18409},{\"attributes\":{\"n\":\"3.2.\"},\"end\":19831,\"start\":19813},{\"attributes\":{\"n\":\"3.3.\"},\"end\":21041,\"start\":21019},{\"attributes\":{\"n\":\"4.\"},\"end\":22127,\"start\":22097},{\"end\":22409,\"start\":22372},{\"attributes\":{\"n\":\"5.\"},\"end\":24901,\"start\":24891},{\"attributes\":{\"n\":\"5.1.\"},\"end\":25158,\"start\":25144},{\"end\":27750,\"start\":27743},{\"attributes\":{\"n\":\"5.2.\"},\"end\":27769,\"start\":27753},{\"attributes\":{\"n\":\"5.3.\"},\"end\":28653,\"start\":28623},{\"attributes\":{\"n\":\"5.4.\"},\"end\":30532,\"start\":30515},{\"end\":31491,\"start\":31478},{\"end\":33812,\"start\":33808},{\"attributes\":{\"n\":\"5.5.\"},\"end\":33840,\"start\":33815},{\"attributes\":{\"n\":\"5.6.\"},\"end\":35420,\"start\":35395},{\"attributes\":{\"n\":\"6.\"},\"end\":36068,\"start\":36058},{\"end\":37847,\"start\":37837},{\"end\":38268,\"start\":38258},{\"end\":39149,\"start\":39139},{\"end\":39852,\"start\":39842},{\"end\":40103,\"start\":40093},{\"end\":40236,\"start\":40226},{\"end\":40299,\"start\":40289},{\"end\":40362,\"start\":40351},{\"end\":40479,\"start\":40468},{\"end\":40925,\"start\":40914}]", "table": "[{\"end\":41838,\"start\":41261},{\"end\":42191,\"start\":42017}]", "figure_caption": "[{\"end\":38256,\"start\":37849},{\"end\":39137,\"start\":38270},{\"end\":39779,\"start\":39151},{\"end\":39840,\"start\":39782},{\"end\":40091,\"start\":39854},{\"end\":40224,\"start\":40105},{\"end\":40287,\"start\":40238},{\"end\":40349,\"start\":40301},{\"end\":40466,\"start\":40365},{\"end\":40912,\"start\":40482},{\"end\":41169,\"start\":40928},{\"end\":41261,\"start\":41172},{\"end\":42017,\"start\":41841}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18263,\"start\":18255},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21259,\"start\":21225},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24743,\"start\":24735},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25937,\"start\":25929},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26493,\"start\":26481},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29286,\"start\":29278},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30285,\"start\":30277},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31124,\"start\":31116},{\"end\":31543,\"start\":31535},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":32638,\"start\":32629},{\"end\":32777,\"start\":32769},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":33455,\"start\":33446},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35390,\"start\":35381},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35937,\"start\":35928},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36717,\"start\":36708}]", "bib_author_first_name": "[{\"end\":42616,\"start\":42615},{\"end\":42626,\"start\":42625},{\"end\":42635,\"start\":42634},{\"end\":42637,\"start\":42636},{\"end\":42646,\"start\":42645},{\"end\":42655,\"start\":42654},{\"end\":42667,\"start\":42666},{\"end\":42678,\"start\":42677},{\"end\":42940,\"start\":42939},{\"end\":42949,\"start\":42948},{\"end\":42964,\"start\":42963},{\"end\":43236,\"start\":43235},{\"end\":43248,\"start\":43247},{\"end\":43257,\"start\":43256},{\"end\":43503,\"start\":43502},{\"end\":43515,\"start\":43514},{\"end\":43524,\"start\":43523},{\"end\":43537,\"start\":43536},{\"end\":43868,\"start\":43867},{\"end\":43880,\"start\":43879},{\"end\":43889,\"start\":43888},{\"end\":43898,\"start\":43897},{\"end\":43900,\"start\":43899},{\"end\":43913,\"start\":43912},{\"end\":43927,\"start\":43926},{\"end\":44292,\"start\":44291},{\"end\":44304,\"start\":44303},{\"end\":44313,\"start\":44312},{\"end\":44323,\"start\":44322},{\"end\":44560,\"start\":44559},{\"end\":44562,\"start\":44561},{\"end\":44575,\"start\":44574},{\"end\":44584,\"start\":44583},{\"end\":44593,\"start\":44592},{\"end\":44602,\"start\":44601},{\"end\":44907,\"start\":44906},{\"end\":44909,\"start\":44908},{\"end\":44922,\"start\":44921},{\"end\":45077,\"start\":45076},{\"end\":45086,\"start\":45085},{\"end\":45097,\"start\":45096},{\"end\":45106,\"start\":45105},{\"end\":45320,\"start\":45319},{\"end\":45322,\"start\":45321},{\"end\":45331,\"start\":45330},{\"end\":45345,\"start\":45344},{\"end\":45355,\"start\":45354},{\"end\":45367,\"start\":45366},{\"end\":45376,\"start\":45375},{\"end\":45382,\"start\":45381},{\"end\":45394,\"start\":45393},{\"end\":45403,\"start\":45402},{\"end\":45411,\"start\":45410},{\"end\":45747,\"start\":45746},{\"end\":45761,\"start\":45760},{\"end\":45772,\"start\":45771},{\"end\":46030,\"start\":46029},{\"end\":46043,\"start\":46042},{\"end\":46058,\"start\":46057},{\"end\":46069,\"start\":46068},{\"end\":46301,\"start\":46300},{\"end\":46309,\"start\":46308},{\"end\":46320,\"start\":46319},{\"end\":46322,\"start\":46321},{\"end\":46329,\"start\":46328},{\"end\":46587,\"start\":46586},{\"end\":46602,\"start\":46601},{\"end\":46604,\"start\":46603},{\"end\":46611,\"start\":46610},{\"end\":46859,\"start\":46858},{\"end\":46866,\"start\":46865},{\"end\":46879,\"start\":46878},{\"end\":46888,\"start\":46887},{\"end\":46894,\"start\":46893},{\"end\":47119,\"start\":47118},{\"end\":47132,\"start\":47131},{\"end\":47144,\"start\":47143},{\"end\":47350,\"start\":47349},{\"end\":47352,\"start\":47351},{\"end\":47363,\"start\":47362},{\"end\":47572,\"start\":47571},{\"end\":47574,\"start\":47573},{\"end\":47584,\"start\":47583},{\"end\":47733,\"start\":47732},{\"end\":47735,\"start\":47734},{\"end\":47743,\"start\":47742},{\"end\":47938,\"start\":47937},{\"end\":47948,\"start\":47947},{\"end\":48143,\"start\":48142},{\"end\":48157,\"start\":48156},{\"end\":48170,\"start\":48169},{\"end\":48172,\"start\":48171},{\"end\":48395,\"start\":48394},{\"end\":48404,\"start\":48403},{\"end\":48413,\"start\":48412},{\"end\":48415,\"start\":48414},{\"end\":48425,\"start\":48424},{\"end\":48438,\"start\":48437},{\"end\":48440,\"start\":48439},{\"end\":48450,\"start\":48449},{\"end\":48461,\"start\":48460},{\"end\":48463,\"start\":48462},{\"end\":48685,\"start\":48684},{\"end\":48694,\"start\":48693},{\"end\":48703,\"start\":48702},{\"end\":48714,\"start\":48713},{\"end\":48716,\"start\":48715},{\"end\":49048,\"start\":49047},{\"end\":49056,\"start\":49055},{\"end\":49058,\"start\":49057},{\"end\":49270,\"start\":49269},{\"end\":49280,\"start\":49279},{\"end\":49289,\"start\":49288},{\"end\":49299,\"start\":49298},{\"end\":49301,\"start\":49300},{\"end\":49314,\"start\":49313},{\"end\":49316,\"start\":49315},{\"end\":49585,\"start\":49584},{\"end\":49591,\"start\":49590},{\"end\":49598,\"start\":49597},{\"end\":49607,\"start\":49606},{\"end\":49613,\"start\":49612},{\"end\":49827,\"start\":49826},{\"end\":49836,\"start\":49835},{\"end\":49850,\"start\":49846},{\"end\":49858,\"start\":49857},{\"end\":49860,\"start\":49859},{\"end\":49869,\"start\":49868},{\"end\":50105,\"start\":50104},{\"end\":50114,\"start\":50113},{\"end\":50123,\"start\":50122},{\"end\":50135,\"start\":50134},{\"end\":50144,\"start\":50143},{\"end\":50151,\"start\":50150},{\"end\":50160,\"start\":50159},{\"end\":50162,\"start\":50161},{\"end\":50169,\"start\":50168},{\"end\":50451,\"start\":50450},{\"end\":50460,\"start\":50459},{\"end\":50472,\"start\":50471},{\"end\":50485,\"start\":50484},{\"end\":50741,\"start\":50740},{\"end\":50753,\"start\":50752},{\"end\":50982,\"start\":50981},{\"end\":50991,\"start\":50990},{\"end\":51003,\"start\":51002},{\"end\":51012,\"start\":51011},{\"end\":51022,\"start\":51021},{\"end\":51033,\"start\":51032},{\"end\":51035,\"start\":51034},{\"end\":51306,\"start\":51305},{\"end\":51320,\"start\":51319},{\"end\":51332,\"start\":51331},{\"end\":51343,\"start\":51342},{\"end\":51355,\"start\":51354},{\"end\":51532,\"start\":51531},{\"end\":51534,\"start\":51533},{\"end\":51540,\"start\":51539},{\"end\":51547,\"start\":51546},{\"end\":51553,\"start\":51552},{\"end\":51559,\"start\":51558},{\"end\":51561,\"start\":51560},{\"end\":51858,\"start\":51857},{\"end\":51860,\"start\":51859},{\"end\":51866,\"start\":51865},{\"end\":51872,\"start\":51871},{\"end\":51878,\"start\":51877},{\"end\":51880,\"start\":51879},{\"end\":52117,\"start\":52116},{\"end\":52119,\"start\":52118},{\"end\":52125,\"start\":52124},{\"end\":52131,\"start\":52130},{\"end\":52142,\"start\":52141},{\"end\":52149,\"start\":52148},{\"end\":52156,\"start\":52155},{\"end\":52158,\"start\":52157},{\"end\":52418,\"start\":52417},{\"end\":52420,\"start\":52419},{\"end\":52426,\"start\":52425},{\"end\":52432,\"start\":52431},{\"end\":52438,\"start\":52437},{\"end\":52440,\"start\":52439},{\"end\":52690,\"start\":52689},{\"end\":52692,\"start\":52691},{\"end\":52893,\"start\":52892},{\"end\":52895,\"start\":52894},{\"end\":52903,\"start\":52902},{\"end\":52913,\"start\":52912},{\"end\":53121,\"start\":53120},{\"end\":53123,\"start\":53122},{\"end\":53131,\"start\":53130},{\"end\":53141,\"start\":53140},{\"end\":53143,\"start\":53142},{\"end\":53153,\"start\":53152},{\"end\":53384,\"start\":53383},{\"end\":53386,\"start\":53385},{\"end\":53394,\"start\":53393},{\"end\":53396,\"start\":53395},{\"end\":53406,\"start\":53405},{\"end\":53416,\"start\":53415},{\"end\":53425,\"start\":53424},{\"end\":53719,\"start\":53718},{\"end\":53732,\"start\":53731},{\"end\":53740,\"start\":53739},{\"end\":53742,\"start\":53741},{\"end\":53750,\"start\":53749},{\"end\":53766,\"start\":53765},{\"end\":54074,\"start\":54073},{\"end\":54078,\"start\":54075},{\"end\":54086,\"start\":54085},{\"end\":54099,\"start\":54098},{\"end\":54111,\"start\":54110},{\"end\":54113,\"start\":54112},{\"end\":54392,\"start\":54391},{\"end\":54400,\"start\":54399},{\"end\":54408,\"start\":54407},{\"end\":54416,\"start\":54415},{\"end\":54709,\"start\":54708},{\"end\":54711,\"start\":54710},{\"end\":54721,\"start\":54720},{\"end\":54723,\"start\":54722},{\"end\":54733,\"start\":54732},{\"end\":54745,\"start\":54744},{\"end\":54755,\"start\":54754},{\"end\":55147,\"start\":55146},{\"end\":55161,\"start\":55160},{\"end\":55377,\"start\":55376},{\"end\":55386,\"start\":55385},{\"end\":55393,\"start\":55392},{\"end\":55596,\"start\":55595},{\"end\":55602,\"start\":55601},{\"end\":55610,\"start\":55609},{\"end\":55625,\"start\":55624},{\"end\":55886,\"start\":55885},{\"end\":55893,\"start\":55892},{\"end\":55907,\"start\":55906},{\"end\":56214,\"start\":56213},{\"end\":56229,\"start\":56228},{\"end\":56244,\"start\":56243},{\"end\":56505,\"start\":56504},{\"end\":56516,\"start\":56515},{\"end\":56529,\"start\":56524},{\"end\":56725,\"start\":56724},{\"end\":56738,\"start\":56737},{\"end\":56747,\"start\":56746},{\"end\":56759,\"start\":56758},{\"end\":56937,\"start\":56936},{\"end\":56951,\"start\":56950},{\"end\":56963,\"start\":56962},{\"end\":56975,\"start\":56974},{\"end\":56985,\"start\":56984},{\"end\":56992,\"start\":56991},{\"end\":57233,\"start\":57232},{\"end\":57240,\"start\":57239},{\"end\":57249,\"start\":57248},{\"end\":57259,\"start\":57258},{\"end\":57268,\"start\":57267},{\"end\":57486,\"start\":57485},{\"end\":57492,\"start\":57491},{\"end\":57500,\"start\":57499},{\"end\":57510,\"start\":57509},{\"end\":57516,\"start\":57515},{\"end\":57525,\"start\":57524},{\"end\":57533,\"start\":57532},{\"end\":57779,\"start\":57778},{\"end\":57785,\"start\":57784},{\"end\":57787,\"start\":57786},{\"end\":57794,\"start\":57793},{\"end\":57804,\"start\":57803},{\"end\":57812,\"start\":57811},{\"end\":57819,\"start\":57818},{\"end\":57825,\"start\":57824},{\"end\":57831,\"start\":57830},{\"end\":57840,\"start\":57839},{\"end\":57851,\"start\":57850},{\"end\":58136,\"start\":58135},{\"end\":58142,\"start\":58141},{\"end\":58148,\"start\":58147},{\"end\":58155,\"start\":58154},{\"end\":58390,\"start\":58389},{\"end\":58397,\"start\":58396},{\"end\":58409,\"start\":58408},{\"end\":58418,\"start\":58417},{\"end\":58420,\"start\":58419},{\"end\":58427,\"start\":58426},{\"end\":58436,\"start\":58435},{\"end\":58447,\"start\":58446}]", "bib_author_last_name": "[{\"end\":42623,\"start\":42617},{\"end\":42632,\"start\":42627},{\"end\":42643,\"start\":42638},{\"end\":42652,\"start\":42647},{\"end\":42664,\"start\":42656},{\"end\":42675,\"start\":42668},{\"end\":42687,\"start\":42679},{\"end\":42946,\"start\":42941},{\"end\":42961,\"start\":42950},{\"end\":42972,\"start\":42965},{\"end\":43245,\"start\":43237},{\"end\":43254,\"start\":43249},{\"end\":43265,\"start\":43258},{\"end\":43512,\"start\":43504},{\"end\":43521,\"start\":43516},{\"end\":43534,\"start\":43525},{\"end\":43547,\"start\":43538},{\"end\":43877,\"start\":43869},{\"end\":43886,\"start\":43881},{\"end\":43895,\"start\":43890},{\"end\":43910,\"start\":43901},{\"end\":43924,\"start\":43914},{\"end\":43941,\"start\":43928},{\"end\":44301,\"start\":44293},{\"end\":44310,\"start\":44305},{\"end\":44320,\"start\":44314},{\"end\":44333,\"start\":44324},{\"end\":44572,\"start\":44563},{\"end\":44581,\"start\":44576},{\"end\":44590,\"start\":44585},{\"end\":44599,\"start\":44594},{\"end\":44616,\"start\":44603},{\"end\":44919,\"start\":44910},{\"end\":44931,\"start\":44923},{\"end\":45083,\"start\":45078},{\"end\":45094,\"start\":45087},{\"end\":45103,\"start\":45098},{\"end\":45112,\"start\":45107},{\"end\":45328,\"start\":45323},{\"end\":45342,\"start\":45332},{\"end\":45352,\"start\":45346},{\"end\":45364,\"start\":45356},{\"end\":45373,\"start\":45368},{\"end\":45379,\"start\":45377},{\"end\":45391,\"start\":45383},{\"end\":45400,\"start\":45395},{\"end\":45408,\"start\":45404},{\"end\":45414,\"start\":45412},{\"end\":45758,\"start\":45748},{\"end\":45769,\"start\":45762},{\"end\":45786,\"start\":45773},{\"end\":46040,\"start\":46031},{\"end\":46055,\"start\":46044},{\"end\":46066,\"start\":46059},{\"end\":46075,\"start\":46070},{\"end\":46306,\"start\":46302},{\"end\":46317,\"start\":46310},{\"end\":46326,\"start\":46323},{\"end\":46338,\"start\":46330},{\"end\":46599,\"start\":46588},{\"end\":46608,\"start\":46605},{\"end\":46622,\"start\":46612},{\"end\":46863,\"start\":46860},{\"end\":46876,\"start\":46867},{\"end\":46885,\"start\":46880},{\"end\":46891,\"start\":46889},{\"end\":46898,\"start\":46895},{\"end\":47129,\"start\":47120},{\"end\":47141,\"start\":47133},{\"end\":47154,\"start\":47145},{\"end\":47360,\"start\":47353},{\"end\":47370,\"start\":47364},{\"end\":47581,\"start\":47575},{\"end\":47587,\"start\":47585},{\"end\":47740,\"start\":47736},{\"end\":47751,\"start\":47744},{\"end\":47945,\"start\":47939},{\"end\":47958,\"start\":47949},{\"end\":48154,\"start\":48144},{\"end\":48167,\"start\":48158},{\"end\":48179,\"start\":48173},{\"end\":48401,\"start\":48396},{\"end\":48410,\"start\":48405},{\"end\":48422,\"start\":48416},{\"end\":48435,\"start\":48426},{\"end\":48447,\"start\":48441},{\"end\":48458,\"start\":48451},{\"end\":48691,\"start\":48686},{\"end\":48700,\"start\":48695},{\"end\":48711,\"start\":48704},{\"end\":48726,\"start\":48717},{\"end\":49053,\"start\":49049},{\"end\":49065,\"start\":49059},{\"end\":49277,\"start\":49271},{\"end\":49286,\"start\":49281},{\"end\":49296,\"start\":49290},{\"end\":49311,\"start\":49302},{\"end\":49326,\"start\":49317},{\"end\":49588,\"start\":49586},{\"end\":49595,\"start\":49592},{\"end\":49604,\"start\":49599},{\"end\":49610,\"start\":49608},{\"end\":49617,\"start\":49614},{\"end\":49833,\"start\":49828},{\"end\":49844,\"start\":49837},{\"end\":49855,\"start\":49851},{\"end\":49866,\"start\":49861},{\"end\":49876,\"start\":49870},{\"end\":50111,\"start\":50106},{\"end\":50120,\"start\":50115},{\"end\":50132,\"start\":50124},{\"end\":50141,\"start\":50136},{\"end\":50148,\"start\":50145},{\"end\":50157,\"start\":50152},{\"end\":50166,\"start\":50163},{\"end\":50176,\"start\":50170},{\"end\":50457,\"start\":50452},{\"end\":50469,\"start\":50461},{\"end\":50482,\"start\":50473},{\"end\":50499,\"start\":50486},{\"end\":50750,\"start\":50742},{\"end\":50761,\"start\":50754},{\"end\":50988,\"start\":50983},{\"end\":51000,\"start\":50992},{\"end\":51009,\"start\":51004},{\"end\":51019,\"start\":51013},{\"end\":51030,\"start\":51023},{\"end\":51045,\"start\":51036},{\"end\":51317,\"start\":51307},{\"end\":51329,\"start\":51321},{\"end\":51340,\"start\":51333},{\"end\":51352,\"start\":51344},{\"end\":51362,\"start\":51356},{\"end\":51537,\"start\":51535},{\"end\":51544,\"start\":51541},{\"end\":51550,\"start\":51548},{\"end\":51556,\"start\":51554},{\"end\":51568,\"start\":51562},{\"end\":51863,\"start\":51861},{\"end\":51869,\"start\":51867},{\"end\":51875,\"start\":51873},{\"end\":51887,\"start\":51881},{\"end\":52122,\"start\":52120},{\"end\":52128,\"start\":52126},{\"end\":52139,\"start\":52132},{\"end\":52146,\"start\":52143},{\"end\":52153,\"start\":52150},{\"end\":52165,\"start\":52159},{\"end\":52423,\"start\":52421},{\"end\":52429,\"start\":52427},{\"end\":52435,\"start\":52433},{\"end\":52447,\"start\":52441},{\"end\":52701,\"start\":52693},{\"end\":52900,\"start\":52896},{\"end\":52910,\"start\":52904},{\"end\":52919,\"start\":52914},{\"end\":53128,\"start\":53124},{\"end\":53138,\"start\":53132},{\"end\":53150,\"start\":53144},{\"end\":53159,\"start\":53154},{\"end\":53391,\"start\":53387},{\"end\":53403,\"start\":53397},{\"end\":53413,\"start\":53407},{\"end\":53422,\"start\":53417},{\"end\":53431,\"start\":53426},{\"end\":53729,\"start\":53720},{\"end\":53737,\"start\":53733},{\"end\":53747,\"start\":53743},{\"end\":53763,\"start\":53751},{\"end\":53777,\"start\":53767},{\"end\":54083,\"start\":54079},{\"end\":54096,\"start\":54087},{\"end\":54108,\"start\":54100},{\"end\":54397,\"start\":54393},{\"end\":54405,\"start\":54401},{\"end\":54413,\"start\":54409},{\"end\":54421,\"start\":54417},{\"end\":54718,\"start\":54712},{\"end\":54730,\"start\":54724},{\"end\":54742,\"start\":54734},{\"end\":54752,\"start\":54746},{\"end\":54769,\"start\":54756},{\"end\":55158,\"start\":55148},{\"end\":55171,\"start\":55162},{\"end\":55383,\"start\":55378},{\"end\":55390,\"start\":55387},{\"end\":55400,\"start\":55394},{\"end\":55599,\"start\":55597},{\"end\":55607,\"start\":55603},{\"end\":55622,\"start\":55611},{\"end\":55640,\"start\":55626},{\"end\":55890,\"start\":55887},{\"end\":55904,\"start\":55894},{\"end\":55914,\"start\":55908},{\"end\":56226,\"start\":56215},{\"end\":56241,\"start\":56230},{\"end\":56249,\"start\":56245},{\"end\":56513,\"start\":56506},{\"end\":56522,\"start\":56517},{\"end\":56537,\"start\":56530},{\"end\":56735,\"start\":56726},{\"end\":56744,\"start\":56739},{\"end\":56756,\"start\":56748},{\"end\":56768,\"start\":56760},{\"end\":56948,\"start\":56938},{\"end\":56960,\"start\":56952},{\"end\":56972,\"start\":56964},{\"end\":56982,\"start\":56976},{\"end\":56989,\"start\":56986},{\"end\":56999,\"start\":56993},{\"end\":57237,\"start\":57234},{\"end\":57246,\"start\":57241},{\"end\":57256,\"start\":57250},{\"end\":57265,\"start\":57260},{\"end\":57271,\"start\":57269},{\"end\":57489,\"start\":57487},{\"end\":57497,\"start\":57493},{\"end\":57507,\"start\":57501},{\"end\":57513,\"start\":57511},{\"end\":57522,\"start\":57517},{\"end\":57530,\"start\":57526},{\"end\":57538,\"start\":57534},{\"end\":57782,\"start\":57780},{\"end\":57791,\"start\":57788},{\"end\":57801,\"start\":57795},{\"end\":57809,\"start\":57805},{\"end\":57816,\"start\":57813},{\"end\":57822,\"start\":57820},{\"end\":57828,\"start\":57826},{\"end\":57837,\"start\":57832},{\"end\":57848,\"start\":57841},{\"end\":57858,\"start\":57852},{\"end\":58139,\"start\":58137},{\"end\":58145,\"start\":58143},{\"end\":58152,\"start\":58149},{\"end\":58162,\"start\":58156},{\"end\":58394,\"start\":58391},{\"end\":58406,\"start\":58398},{\"end\":58415,\"start\":58410},{\"end\":58424,\"start\":58421},{\"end\":58433,\"start\":58428},{\"end\":58444,\"start\":58437},{\"end\":58455,\"start\":58448}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9649070},\"end\":42861,\"start\":42565},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17847588},\"end\":43158,\"start\":42863},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14966986},\"end\":43419,\"start\":43160},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":46244683},\"end\":43759,\"start\":43421},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3817802},\"end\":44211,\"start\":43761},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15425191},\"end\":44503,\"start\":44213},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15195762},\"end\":44832,\"start\":44505},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9117881},\"end\":45074,\"start\":44834},{\"attributes\":{\"doi\":\"arXiv:1312.6203\",\"id\":\"b8\"},\"end\":45317,\"start\":45076},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b9\"},\"end\":45664,\"start\":45319},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3016223},\"end\":45955,\"start\":45666},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3609891},\"end\":46241,\"start\":45957},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3090322},\"end\":46519,\"start\":46243},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":557652},\"end\":46775,\"start\":46521},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8063966},\"end\":47086,\"start\":46777},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6099034},\"end\":47272,\"start\":47088},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1377132},\"end\":47525,\"start\":47274},{\"attributes\":{\"id\":\"b17\"},\"end\":47664,\"start\":47527},{\"attributes\":{\"id\":\"b18\"},\"end\":47853,\"start\":47666},{\"attributes\":{\"id\":\"b19\"},\"end\":48075,\"start\":47855},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195908774},\"end\":48331,\"start\":48077},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":41312633},\"end\":48682,\"start\":48333},{\"attributes\":{\"doi\":\"arXiv:1705.07664\",\"id\":\"b22\"},\"end\":48998,\"start\":48684},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17848433},\"end\":49191,\"start\":49000},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4215682},\"end\":49510,\"start\":49193},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7233317},\"end\":49784,\"start\":49512},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14304601},\"end\":50033,\"start\":49786},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":215762985},\"end\":50384,\"start\":50035},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":11170942},\"end\":50662,\"start\":50386},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14620252},\"end\":50905,\"start\":50664},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":301319},\"end\":51236,\"start\":50907},{\"attributes\":{\"id\":\"b31\"},\"end\":51529,\"start\":51238},{\"attributes\":{\"doi\":\"arXiv:1711.08488\",\"id\":\"b32\"},\"end\":51777,\"start\":51531},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5115938},\"end\":52045,\"start\":51779},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1009127},\"end\":52335,\"start\":52047},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1745976},\"end\":52607,\"start\":52337},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":16217354},\"end\":52832,\"start\":52609},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15022990},\"end\":53054,\"start\":52834},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1038604},\"end\":53312,\"start\":53056},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12426733},\"end\":53684,\"start\":53314},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206756462},\"end\":53959,\"start\":53686},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":8386230},\"end\":54318,\"start\":53961},{\"attributes\":{\"doi\":\"arXiv:1712.06760\",\"id\":\"b42\"},\"end\":54573,\"start\":54320},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1594725},\"end\":55068,\"start\":54575},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":17648673},\"end\":55319,\"start\":55070},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":8846709},\"end\":55526,\"start\":55321},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2407217},\"end\":55801,\"start\":55528},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12701882},\"end\":56113,\"start\":55803},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":60945},\"end\":56432,\"start\":56115},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":14020753},\"end\":56688,\"start\":56434},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6068875},\"end\":56934,\"start\":56690},{\"attributes\":{\"doi\":\"arXiv:1710.10903\",\"id\":\"b51\"},\"end\":57167,\"start\":56936},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":6356896},\"end\":57424,\"start\":57169},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206592833},\"end\":57701,\"start\":57426},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":2880712},\"end\":58067,\"start\":57703},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":14487589},\"end\":58303,\"start\":58069},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":2305273},\"end\":58659,\"start\":58305}]", "bib_title": "[{\"end\":42613,\"start\":42565},{\"end\":42937,\"start\":42863},{\"end\":43233,\"start\":43160},{\"end\":43500,\"start\":43421},{\"end\":43865,\"start\":43761},{\"end\":44289,\"start\":44213},{\"end\":44557,\"start\":44505},{\"end\":44904,\"start\":44834},{\"end\":45744,\"start\":45666},{\"end\":46027,\"start\":45957},{\"end\":46298,\"start\":46243},{\"end\":46584,\"start\":46521},{\"end\":46856,\"start\":46777},{\"end\":47116,\"start\":47088},{\"end\":47347,\"start\":47274},{\"end\":48140,\"start\":48077},{\"end\":48392,\"start\":48333},{\"end\":49045,\"start\":49000},{\"end\":49267,\"start\":49193},{\"end\":49582,\"start\":49512},{\"end\":49824,\"start\":49786},{\"end\":50102,\"start\":50035},{\"end\":50448,\"start\":50386},{\"end\":50738,\"start\":50664},{\"end\":50979,\"start\":50907},{\"end\":51303,\"start\":51238},{\"end\":51855,\"start\":51779},{\"end\":52114,\"start\":52047},{\"end\":52415,\"start\":52337},{\"end\":52687,\"start\":52609},{\"end\":52890,\"start\":52834},{\"end\":53118,\"start\":53056},{\"end\":53381,\"start\":53314},{\"end\":53716,\"start\":53686},{\"end\":54071,\"start\":53961},{\"end\":54706,\"start\":54575},{\"end\":55144,\"start\":55070},{\"end\":55374,\"start\":55321},{\"end\":55593,\"start\":55528},{\"end\":55883,\"start\":55803},{\"end\":56211,\"start\":56115},{\"end\":56502,\"start\":56434},{\"end\":56722,\"start\":56690},{\"end\":57230,\"start\":57169},{\"end\":57483,\"start\":57426},{\"end\":57776,\"start\":57703},{\"end\":58133,\"start\":58069},{\"end\":58387,\"start\":58305}]", "bib_author": "[{\"end\":42625,\"start\":42615},{\"end\":42634,\"start\":42625},{\"end\":42645,\"start\":42634},{\"end\":42654,\"start\":42645},{\"end\":42666,\"start\":42654},{\"end\":42677,\"start\":42666},{\"end\":42689,\"start\":42677},{\"end\":42948,\"start\":42939},{\"end\":42963,\"start\":42948},{\"end\":42974,\"start\":42963},{\"end\":43247,\"start\":43235},{\"end\":43256,\"start\":43247},{\"end\":43267,\"start\":43256},{\"end\":43514,\"start\":43502},{\"end\":43523,\"start\":43514},{\"end\":43536,\"start\":43523},{\"end\":43549,\"start\":43536},{\"end\":43879,\"start\":43867},{\"end\":43888,\"start\":43879},{\"end\":43897,\"start\":43888},{\"end\":43912,\"start\":43897},{\"end\":43926,\"start\":43912},{\"end\":43943,\"start\":43926},{\"end\":44303,\"start\":44291},{\"end\":44312,\"start\":44303},{\"end\":44322,\"start\":44312},{\"end\":44335,\"start\":44322},{\"end\":44574,\"start\":44559},{\"end\":44583,\"start\":44574},{\"end\":44592,\"start\":44583},{\"end\":44601,\"start\":44592},{\"end\":44618,\"start\":44601},{\"end\":44921,\"start\":44906},{\"end\":44933,\"start\":44921},{\"end\":45085,\"start\":45076},{\"end\":45096,\"start\":45085},{\"end\":45105,\"start\":45096},{\"end\":45114,\"start\":45105},{\"end\":45330,\"start\":45319},{\"end\":45344,\"start\":45330},{\"end\":45354,\"start\":45344},{\"end\":45366,\"start\":45354},{\"end\":45375,\"start\":45366},{\"end\":45381,\"start\":45375},{\"end\":45393,\"start\":45381},{\"end\":45402,\"start\":45393},{\"end\":45410,\"start\":45402},{\"end\":45416,\"start\":45410},{\"end\":45760,\"start\":45746},{\"end\":45771,\"start\":45760},{\"end\":45788,\"start\":45771},{\"end\":46042,\"start\":46029},{\"end\":46057,\"start\":46042},{\"end\":46068,\"start\":46057},{\"end\":46077,\"start\":46068},{\"end\":46308,\"start\":46300},{\"end\":46319,\"start\":46308},{\"end\":46328,\"start\":46319},{\"end\":46340,\"start\":46328},{\"end\":46601,\"start\":46586},{\"end\":46610,\"start\":46601},{\"end\":46624,\"start\":46610},{\"end\":46865,\"start\":46858},{\"end\":46878,\"start\":46865},{\"end\":46887,\"start\":46878},{\"end\":46893,\"start\":46887},{\"end\":46900,\"start\":46893},{\"end\":47131,\"start\":47118},{\"end\":47143,\"start\":47131},{\"end\":47156,\"start\":47143},{\"end\":47362,\"start\":47349},{\"end\":47372,\"start\":47362},{\"end\":47583,\"start\":47571},{\"end\":47589,\"start\":47583},{\"end\":47742,\"start\":47732},{\"end\":47753,\"start\":47742},{\"end\":47947,\"start\":47937},{\"end\":47960,\"start\":47947},{\"end\":48156,\"start\":48142},{\"end\":48169,\"start\":48156},{\"end\":48181,\"start\":48169},{\"end\":48403,\"start\":48394},{\"end\":48412,\"start\":48403},{\"end\":48424,\"start\":48412},{\"end\":48437,\"start\":48424},{\"end\":48449,\"start\":48437},{\"end\":48460,\"start\":48449},{\"end\":48466,\"start\":48460},{\"end\":48693,\"start\":48684},{\"end\":48702,\"start\":48693},{\"end\":48713,\"start\":48702},{\"end\":48728,\"start\":48713},{\"end\":49055,\"start\":49047},{\"end\":49067,\"start\":49055},{\"end\":49279,\"start\":49269},{\"end\":49288,\"start\":49279},{\"end\":49298,\"start\":49288},{\"end\":49313,\"start\":49298},{\"end\":49328,\"start\":49313},{\"end\":49590,\"start\":49584},{\"end\":49597,\"start\":49590},{\"end\":49606,\"start\":49597},{\"end\":49612,\"start\":49606},{\"end\":49619,\"start\":49612},{\"end\":49835,\"start\":49826},{\"end\":49846,\"start\":49835},{\"end\":49857,\"start\":49846},{\"end\":49868,\"start\":49857},{\"end\":49878,\"start\":49868},{\"end\":50113,\"start\":50104},{\"end\":50122,\"start\":50113},{\"end\":50134,\"start\":50122},{\"end\":50143,\"start\":50134},{\"end\":50150,\"start\":50143},{\"end\":50159,\"start\":50150},{\"end\":50168,\"start\":50159},{\"end\":50178,\"start\":50168},{\"end\":50459,\"start\":50450},{\"end\":50471,\"start\":50459},{\"end\":50484,\"start\":50471},{\"end\":50501,\"start\":50484},{\"end\":50752,\"start\":50740},{\"end\":50763,\"start\":50752},{\"end\":50990,\"start\":50981},{\"end\":51002,\"start\":50990},{\"end\":51011,\"start\":51002},{\"end\":51021,\"start\":51011},{\"end\":51032,\"start\":51021},{\"end\":51047,\"start\":51032},{\"end\":51319,\"start\":51305},{\"end\":51331,\"start\":51319},{\"end\":51342,\"start\":51331},{\"end\":51354,\"start\":51342},{\"end\":51364,\"start\":51354},{\"end\":51539,\"start\":51531},{\"end\":51546,\"start\":51539},{\"end\":51552,\"start\":51546},{\"end\":51558,\"start\":51552},{\"end\":51570,\"start\":51558},{\"end\":51865,\"start\":51857},{\"end\":51871,\"start\":51865},{\"end\":51877,\"start\":51871},{\"end\":51889,\"start\":51877},{\"end\":52124,\"start\":52116},{\"end\":52130,\"start\":52124},{\"end\":52141,\"start\":52130},{\"end\":52148,\"start\":52141},{\"end\":52155,\"start\":52148},{\"end\":52167,\"start\":52155},{\"end\":52425,\"start\":52417},{\"end\":52431,\"start\":52425},{\"end\":52437,\"start\":52431},{\"end\":52449,\"start\":52437},{\"end\":52703,\"start\":52689},{\"end\":52902,\"start\":52892},{\"end\":52912,\"start\":52902},{\"end\":52921,\"start\":52912},{\"end\":53130,\"start\":53120},{\"end\":53140,\"start\":53130},{\"end\":53152,\"start\":53140},{\"end\":53161,\"start\":53152},{\"end\":53393,\"start\":53383},{\"end\":53405,\"start\":53393},{\"end\":53415,\"start\":53405},{\"end\":53424,\"start\":53415},{\"end\":53433,\"start\":53424},{\"end\":53731,\"start\":53718},{\"end\":53739,\"start\":53731},{\"end\":53749,\"start\":53739},{\"end\":53765,\"start\":53749},{\"end\":53779,\"start\":53765},{\"end\":54085,\"start\":54073},{\"end\":54098,\"start\":54085},{\"end\":54110,\"start\":54098},{\"end\":54116,\"start\":54110},{\"end\":54399,\"start\":54391},{\"end\":54407,\"start\":54399},{\"end\":54415,\"start\":54407},{\"end\":54423,\"start\":54415},{\"end\":54720,\"start\":54708},{\"end\":54732,\"start\":54720},{\"end\":54744,\"start\":54732},{\"end\":54754,\"start\":54744},{\"end\":54771,\"start\":54754},{\"end\":55160,\"start\":55146},{\"end\":55173,\"start\":55160},{\"end\":55385,\"start\":55376},{\"end\":55392,\"start\":55385},{\"end\":55402,\"start\":55392},{\"end\":55601,\"start\":55595},{\"end\":55609,\"start\":55601},{\"end\":55624,\"start\":55609},{\"end\":55642,\"start\":55624},{\"end\":55892,\"start\":55885},{\"end\":55906,\"start\":55892},{\"end\":55916,\"start\":55906},{\"end\":56228,\"start\":56213},{\"end\":56243,\"start\":56228},{\"end\":56251,\"start\":56243},{\"end\":56515,\"start\":56504},{\"end\":56524,\"start\":56515},{\"end\":56539,\"start\":56524},{\"end\":56737,\"start\":56724},{\"end\":56746,\"start\":56737},{\"end\":56758,\"start\":56746},{\"end\":56770,\"start\":56758},{\"end\":56950,\"start\":56936},{\"end\":56962,\"start\":56950},{\"end\":56974,\"start\":56962},{\"end\":56984,\"start\":56974},{\"end\":56991,\"start\":56984},{\"end\":57001,\"start\":56991},{\"end\":57239,\"start\":57232},{\"end\":57248,\"start\":57239},{\"end\":57258,\"start\":57248},{\"end\":57267,\"start\":57258},{\"end\":57273,\"start\":57267},{\"end\":57491,\"start\":57485},{\"end\":57499,\"start\":57491},{\"end\":57509,\"start\":57499},{\"end\":57515,\"start\":57509},{\"end\":57524,\"start\":57515},{\"end\":57532,\"start\":57524},{\"end\":57540,\"start\":57532},{\"end\":57784,\"start\":57778},{\"end\":57793,\"start\":57784},{\"end\":57803,\"start\":57793},{\"end\":57811,\"start\":57803},{\"end\":57818,\"start\":57811},{\"end\":57824,\"start\":57818},{\"end\":57830,\"start\":57824},{\"end\":57839,\"start\":57830},{\"end\":57850,\"start\":57839},{\"end\":57860,\"start\":57850},{\"end\":58141,\"start\":58135},{\"end\":58147,\"start\":58141},{\"end\":58154,\"start\":58147},{\"end\":58164,\"start\":58154},{\"end\":58396,\"start\":58389},{\"end\":58408,\"start\":58396},{\"end\":58417,\"start\":58408},{\"end\":58426,\"start\":58417},{\"end\":58435,\"start\":58426},{\"end\":58446,\"start\":58435},{\"end\":58457,\"start\":58446}]", "bib_venue": "[{\"end\":42699,\"start\":42689},{\"end\":42994,\"start\":42974},{\"end\":43277,\"start\":43267},{\"end\":43572,\"start\":43549},{\"end\":43966,\"start\":43943},{\"end\":44345,\"start\":44335},{\"end\":44649,\"start\":44618},{\"end\":44943,\"start\":44933},{\"end\":45187,\"start\":45129},{\"end\":45471,\"start\":45432},{\"end\":45798,\"start\":45788},{\"end\":46087,\"start\":46077},{\"end\":46363,\"start\":46340},{\"end\":46634,\"start\":46624},{\"end\":46911,\"start\":46900},{\"end\":47166,\"start\":47156},{\"end\":47383,\"start\":47372},{\"end\":47569,\"start\":47527},{\"end\":47730,\"start\":47666},{\"end\":47935,\"start\":47855},{\"end\":48191,\"start\":48181},{\"end\":48484,\"start\":48466},{\"end\":48830,\"start\":48744},{\"end\":49078,\"start\":49067},{\"end\":49338,\"start\":49328},{\"end\":49626,\"start\":49619},{\"end\":49889,\"start\":49878},{\"end\":50192,\"start\":50178},{\"end\":50511,\"start\":50501},{\"end\":50773,\"start\":50763},{\"end\":51057,\"start\":51047},{\"end\":51367,\"start\":51364},{\"end\":51643,\"start\":51586},{\"end\":51899,\"start\":51889},{\"end\":52177,\"start\":52167},{\"end\":52459,\"start\":52449},{\"end\":52712,\"start\":52703},{\"end\":52931,\"start\":52921},{\"end\":53171,\"start\":53161},{\"end\":53472,\"start\":53433},{\"end\":53805,\"start\":53779},{\"end\":54126,\"start\":54116},{\"end\":54389,\"start\":54320},{\"end\":54802,\"start\":54771},{\"end\":55183,\"start\":55173},{\"end\":55412,\"start\":55402},{\"end\":55652,\"start\":55642},{\"end\":55939,\"start\":55916},{\"end\":56261,\"start\":56251},{\"end\":56549,\"start\":56539},{\"end\":56793,\"start\":56770},{\"end\":57041,\"start\":57017},{\"end\":57283,\"start\":57273},{\"end\":57550,\"start\":57540},{\"end\":57863,\"start\":57860},{\"end\":58174,\"start\":58164},{\"end\":58467,\"start\":58457},{\"end\":42705,\"start\":42701},{\"end\":43010,\"start\":42996},{\"end\":43283,\"start\":43279},{\"end\":44351,\"start\":44347},{\"end\":44949,\"start\":44945},{\"end\":45804,\"start\":45800},{\"end\":46093,\"start\":46089},{\"end\":46640,\"start\":46636},{\"end\":47172,\"start\":47168},{\"end\":48197,\"start\":48193},{\"end\":49344,\"start\":49340},{\"end\":50202,\"start\":50194},{\"end\":50517,\"start\":50513},{\"end\":50779,\"start\":50775},{\"end\":51063,\"start\":51059},{\"end\":51905,\"start\":51901},{\"end\":52183,\"start\":52179},{\"end\":52465,\"start\":52461},{\"end\":52717,\"start\":52714},{\"end\":52937,\"start\":52933},{\"end\":53177,\"start\":53173},{\"end\":54132,\"start\":54128},{\"end\":55189,\"start\":55185},{\"end\":55418,\"start\":55414},{\"end\":55658,\"start\":55654},{\"end\":56267,\"start\":56263},{\"end\":56555,\"start\":56551},{\"end\":57289,\"start\":57285},{\"end\":57556,\"start\":57552},{\"end\":58180,\"start\":58176},{\"end\":58473,\"start\":58469}]"}}}, "year": 2023, "month": 12, "day": 17}