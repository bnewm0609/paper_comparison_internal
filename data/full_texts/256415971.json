{"id": 256415971, "updated": "2023-10-05 04:38:13.394", "metadata": {"title": "Optimizing DDPM Sampling with Shortcut Fine-Tuning", "authors": "[{\"first\":\"Ying\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Kangwook\",\"last\":\"Lee\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting in sample quality comparable to or even surpassing that of the full-step model across various datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2301.13362", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/Fan023", "doi": "10.48550/arxiv.2301.13362"}}, "content": {"source": {"pdf_hash": "c5434eef64f3275d821ba24bfa3818bfc10649fb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.13362v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1b15900217ea9ca5c8f68f5a22ec1741e09a9b23", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c5434eef64f3275d821ba24bfa3818bfc10649fb.txt", "contents": "\nOptimizing DDPM Sampling with Shortcut Fine-Tuning\n\n\nYing Fan \nKangwook Lee \nOptimizing DDPM Sampling with Shortcut Fine-Tuning\n\nIn this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting in sample quality comparable to or even surpassing that of the full-step model across various datasets. Arthur Gretton. On gradient regularizers for mmd gans. Advances in neural information processing systems, 31, 2018. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Reinforcement learning, pages 5-32, 1992. Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1-62, 2020.\n\nIntroduction\n\nDenoising diffusion probabilistic models (DDPMs) (Ho et al., 2020) are parameterized stochastic Markov chains with Gaussian noises, which are learned by gradually adding noises to the data as the forward process, computing the posterior as a backward process, and then training the DDPM to match the backward process. Advances in DDPM  have shown the potential to rival GANs (Goodfellow et al., 2014) in generative tasks. However, one major drawback of DDPM is that a large number of steps T is needed. As  a result, there is a line of work focusing on sampling fewer T 1 ! T steps to obtain comparable sample quality: Most works are dedicated to better approximating the backward process as stochastic differential equations (SDEs) with fewer steps, generally via better noise estimation or computing better sub-sampling schedules (Kong and Ping, 2021;San-Roman et al., 2021;Lam et al., 2021;Watson et al., 2021a;Jolicoeur-Martineau et al., 2021;Bao et al., 2021;. Other works aim at approximating the backward process with fewer steps via more complicated non-gaussian noise distributions (Xiao et al., 2021). 1\n\nTo our best knowledge, existing fast samplers of DDPM stick to imitating the computed backward process with fewer steps. If we treat data generation as a control task (see Fig. 1), the backward process can be viewed as a demonstration to generate data from noise (which might not be optimal in terms of number of steps), and the training dataset could be an environment that provides feedback on how good the generated distribution is. From this view, imitating the backward process could be viewed as imitation learning (Hussein et al., 2017) or behavior cloning (Torabi et al., 2018). Naturally, one may wonder if we can do better than pure imitation, since learning via imitation is generally useful but rarely optimal, and we can explore alternative paths for optimal solutions during online optimization.\n\nMotivated by the above observation, we study the following 1 There is another line of work focusing on fast sampling of DDIM (Song et al., 2020a) with deterministic Markov sampling chains, which we will discuss in Section 5.\n\n\nI. Noising steps\n\nII. Learned denoising steps III. Fine-tuned steps Figure 2. A visual illustration of the key idea of Shortcut Fine-Tuning (SFT). DDPMs aim at learning the backward diffusion model, but this approach is limited to a small number of steps. We propose the idea of not following the backward process and exploring other unexplored paths that can lead to improved data generation. To this end, we directly minimize an IPM and develop a policy gradient-like optimization algorithm. Our experimental results show that one can significantly improve data generation quality by fine-tuning a pretrained DDPM model with SFT. We also provide a visualization of the difference between steps in II and III when T is small in Appendix A.\n\nunderexplored question:\n\nCan we improve DDPM sampling by not following the backward process?\n\nIn this work, we show that this is indeed possible. We finetune pretrained DDPM samplers by directly minimizing an integral probability metric (IPM) and show that finetuned DDPM samplers have significantly better generation qualities when the number of sampling steps is small. In this way, we can still enjoy diffusion models' multistep capabilities with no need to change the noise distribution, and improve the performance with fewer sampling steps.\n\nMore concretely, we first show that performing gradient descent of the DDPM sampler w.r.t. the IPM is equivalent to stochastic policy gradient, which echoes the aforementioned RL view but with a changing reward from the optimal critic function given by IPM. In addition, we present a surrogate function that can provide insights for monotonic improvements. Finally, we present a fine-tuning algorithm with alternative updates between the critic and the generator.\n\nWe summarize our main contributions as follows:\n\n\u2022 (Section 4.1) We propose a novel algorithm to fine-tune DDPM samplers with direct IPM minimization, and we show that performing gradient descent of diffusion models w.r.t. IPM is equivalent to policy gradient. To our best knowledge, this is the first work to apply reinforcement learning methods to diffusion models.\n\n\u2022 (Section 4.2) We present a surrogate function of IPM in theory, which provides insights on conditions for monotonic improvement and algorithm design.\n\n\u2022 (Section 4.3.2) We propose a regularization for the critic based on the baseline function, which shows benefits for the policy gradient training.\n\n\u2022 (Section 6) Empirically, we show that our fine-tuning can improve DDPM sampling performance in two cases: when T itself is small, and when T is large but using a fast sampler where T 1 ! T . In both cases, our fine-tuning achieves comparable or even higher sample quality than the DDPM with 1000 steps using 10 sampling steps.\n\n\nBackground\n\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\nHere we consider denoising probabilistic diffusion models (DDPM) as stochastic Markov chains with Gaussian noises (Ho et al., 2020). Consider data distribution x 0 \" q 0 , x 0 P R n .\n\nDefine the forward noising process: for t P r0, .., T\u00b41s,\nqpx t`1 |x t q :\" N p a 1\u00b4\u03b2 t`1 x t , \u03b2 t`1 Iq,(1)\nwhere x 1 , .., x T are variables of the same dimensionality as x 0 , \u03b2 1:T is the variance schedule.\n\nWe can compute the posterior as a backward process:\nqpx t |x t`1 , x 0 q \" N p\u03bc t`1 px t`1 , x 0 q,\u03b2 t`1 Iq,(2)\nwhere\u03bc t`1 px t`1 , x 0 q \"\n?\u1fb1 t\u03b2t 1\u00b4\u1fb1t`1 x 0`? \u03b1t`1p1\u00b4\u1fb1tq 1\u00b4\u1fb1t`1\nx t`1 , \u03b1 t`1 \" 1\u00b4\u03b2 t`1 ,\u1fb1 t`1 \" \u015b t`1 s\"1 \u03b1 s . We define a DDPM sampler parameterized by \u03b8, which generates data starting from some pure noise x T \" p T :\nx T \" p T \" N p0, Iq, x t \" p \u03b8 t px t |x t`1 q, p \u03b8 t px t |x t`1 q :\" N`\u00b5 \u03b8 t`1 px t`1 q, \u03a3 t`1\u02d8,(3)\nwhere \u03a3 t`1 is generally chosen as \u03b2 t`1 I or\u03b2 t`1 I. 2\n\nDefine\np \u03b8 x 0:T :\" p T px T q T\u00b41 \u017a t\"0 p \u03b8 t px t |x t`1 q,(4)\nand we have the marginal distribution p \u03b8 0 px 0 q \" \u015f p \u03b8 x 0:T px 0:T qdx 1:T .\n\nThe sampler is trained by minimizing the sum of KL divergences for each step:\nJ \" E q \u00ab T\u00b41 \u00ff t\"0 D KL pqpx t |x t`1 , x 0 q, p \u03b8 t px t |x t`1 qq ff . (5)\nOptimizing the above loss can be viewed as matching the conditional generator p \u03b8 t px t |x t`1 q with the backward process qpx t |x t`1 , x 0 q for each step. Song et al. (2020b) show that J is equivalent to score-matching loss when formulating the forward and backward process as a discrete version of stochastic differential equations.\n\n\nIntegral Probability Metrics (IPM)\n\nGiven A as a set of parameters s.t. for each \u03b1 P A, it defines a critic f \u03b1 : R n \u00d1 R. Given a critic f \u03b1 and two distributions p \u03b8 0 and q 0 , we define\ngpp \u03b8 0 , f \u03b1 , q 0 q :\" E x0\"p \u03b8 0 rf \u03b1 px 0 qs\u00b4E x0\"q0 rf \u03b1 px 0 qs. (6) Let \u03a6pp \u03b8 0 , q 0 q :\" sup \u03b1PA gpp \u03b8 0 , f \u03b1 , q 0 q.(7)\nIf A satisfies that @\u03b1 P A, D\u03b1 1 P A, s.t. f \u03b1 1 \"\u00b4f \u03b1 , then \u03a6pp \u03b8 , qq is a pseudo metric over the probability space of R n , making it so-called integral probability metrics (IPM).\n\nIn this paper, we consider A that makes \u03a6pp \u03b8 0 , q 0 q an IPM. For example, when A \" t\u03b1 : ||f \u03b1 || L \u010f 1u, \u03a6pp \u03b8 0 , q 0 q is the Wasserstein-1 distance; when A \" t\u03b1 : ||f \u03b1 || 8 \u010f 1u, \u03a6pp \u03b8 0 , q 0 q is the total variation distance; it also includes maximum mean discrepancy (MMD) when A defines all functions in Reproducing Kernel Hilbert Space (RKHS).\n\n\nMotivation\n\n\nIssues with Existing DDPM Samplers\n\nHere we review the existing issues with DDPM samplers 1) when T is not large enough, and 2) when sub-sampling with the number of steps T 1 ! T , which inspires us to design our fine-tuning algorithm.\n\nCase 1. Issues caused by training DDPM with a small T (Fig 2). Given a score-matching loss J, the upper bound on Wasserstein-2 distance is given by Kwon et al. (2022):\nW 2 pp \u03b8 0 , q 0 q \u010f Op ? Jq`IpT qW 2 pp T , q T q,(8)\nwhere IpT q is non-exploding and W 2 pp T , q T q decays exponentially with T when T \u00d1 8. From the inequality above, one sufficient condition for the score-matching loss J to be viewed as optimizing the Wasserstein distance is when T is large enough such that IpT qW 2 pp T , q T q \u00d1 0. Now we consider the case when T is small and p T ff q T . 3 . The upper bound in Eq. (8) can be high since W 2 pp T , q T q is not neglectable. As shown in Fig 2, pure imitation p \u03b8 t px t |x t`1 q \u00ab qpx t |x t`1 , x 0 q would not lead the model exactly to q 0 when p T and q T are not close enough.\n\nCase 2. Issues caused by a smaller number of subsampling steps (T 1 ! T ) (Fig 8 in Appendix B). We consider DDPM sub-sampling and other fast sampling techniques, where T is large enough s.t. p T \u00ab q T , but we try to sample with fewer sampling steps (T 1 ). It is generally done by choosing \u03c4 to be an increasing sub-sequence of T 1 steps in r0, T s starting from 0. Many works have been dedicated to finding a subsequence and variance schedule to make the sub-sampling steps match the full-step backward process as much as possible (Kong and Ping, 2021;Bao et al., 2021;. However, this would inevitably cause downgraded sample quality if each step is Gaussian: as discussed in Salimans and Ho (2021) and Xiao et al. (2021), a multistep Gaussian sampler cannot be distilled into a one-step Gaussian sampler without loss of fidelity.\n\n\nProblem Formulation\n\nIn both cases mentioned above, there might exist paths other than imitating the backward process that can reach the data distribution with fewer Gaussian steps. Thus one may expect to overcome these issues by minimizing the IPM.\n\nHere we present the formulation of our problem setting. We assume that there is a target data distribution q 0 . Given a set of critic parameters A s.t. \u03a6pp \u03b8 0 , q 0 q \" sup \u03b1PA gpp \u03b8 0 , f \u03b1 , q 0 q is an IPM, and given a DDPM sampler with T steps parameterized by \u03b8, our goal is to solve:\nmin \u03b8 \u03a6pp \u03b8 0 , q 0 q.(9)\n\nPathwise Derivative Estimation for Shortcut Fine-Tuning: Properties and Potential Issues\n\nOne straightforward approach is to optimize \u03a6pp \u03b8 0 , q 0 q using pathwise derivative estimation (Rezende et al., 2014) like GAN training, which we denote as SFT (shortcut finetuning). We can recursively define the stochastic mappings:\nh \u03b8,T px T q :\" x T ,(10)h \u03b8,t px t q :\" \u00b5 \u03b8 ph \u03b8,t`1 px t`1 qq`\u03f5 t`1 ,(11)x 0 \" h \u03b8,0 px T q (12) where x T \" N p0, Iq, \u03f5 t`1 \" N p0, \u03a3 t`1 q, t \" 0, ..., T\u00b41.\nThen we can write the objective function as:\n\u03a6pp \u03b8 0 , q 0 q \" sup \u03b1PA E x T ,\u03f5 1:T rf \u03b1 ph \u03b8,0 px T qqs\u00b4E x0\"q0 rf \u03b1 px 0 qs (13) Assume that D\u03b1 P A, s.t. gpp \u03b8 0 , \u03b1, q 0 q \" \u03a6pp \u03b8 0 , q 0 q. Let \u03b1\u02dapp \u03b8 0 , q 0 q P t\u03b1 : gpp \u03b8 0 , \u03b1, q 0 q \" \u03a6pp \u03b8 0 , q 0 qu.\nWhen f \u03b1 is 1-Lipschitz, we can compute the gradient which is similar to WGAN (Arjovsky et al., 2017):\n\u2207 \u03b8 \u03a6pp \u03b8 0 , q 0 q \" E x T ,\u03f5 1:T \" \u2207 \u03b8 f \u03b1\u02dapp \u03b8 0 ,q0q ph \u03b8,0 px T qq \u0131 .(14)\nImplicit requirements on the family of critics A: gradient regularization. In Eq. (14), we can observe that the critic f \u03b1\u02dan eeds to provide meaningful gradients (w.r.t. the input) for the generator. If the gradient of the critic happens to be 0 at some generated data points, even if the critic's value could still make sense, the critic would provide no signal for the generator on these points 4 . Thus GANs trained with IPMs generally need to choose A such that the gradient of the critic is regularized: For example, Lipschitz constraints like weight clipping (Arjovsky et al., 2017) and gradient penalty (Gulrajani et al., 2017) for WGAN, and gradient regularizers for MMD GAN (Arbel et al., 2018).\n\nPotential issues. Besides the implicit requirements on the critic, there might also be issues when computing Eq. (14) in practice. It contains differentiating a composite function with T steps, which can cause problems similar to RNNs:\n\n\u2022 Gradient vanishing may result in long-distance dependency being lost;\n\n\u2022 Gradient explosion may occur;\n\n\u2022 Memory usage is high.\n\n\nMethod: Shortcut Fine-Tuning with Policy Gradient (SFT-PG)\n\nWe note that Eq. (14) is not the only way to estimate the gradient w.r.t. IPM. In this section, we show that performing gradient descent of \u03a6pp \u03b8 0 , q 0 q can be equivalent to policy gradient (Section 4.1), provide analysis towards monotonic improvement (Section 4.2) and then present the algorithm design (Section 4.3).\n\n\nPolicy Gradient Equivalence\n\nBy modeling the conditional probability through the trajectory, we provide an alternative way for gradient estimation which is equivalent to policy gradient, without differentiating through the composite functions. Theorem 4.1. (Policy gradient equivalence)\n\nAssume that both p \u03b8 x 0:T px 0:T qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 q and \u2207 \u03b8 p \u03b8 x 0:T px 0:T qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 q are continuous functions w.r.t. \u03b8 and x 0:T . Then\n\u2207 \u03b8 \u03a6pp \u03b8 0 , q 0 q \" E p \u03b8 x 0:T \" f \u03b1\u02dapp \u03b8 0 ,q0q px 0 q\u2207 \u03b8 log T\u00b41 \u00ff t\"0 p \u03b8 t px t |x t`1 q \u2030 .(15)\nProof.\n\u2207 \u03b8 \u03a6pp \u03b8 0 , q 0 q \" \u2207 \u03b8 \u017c p \u03b8 0 px 0 qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 qdx 0 \u2207 \u03b8 \u03b1\u02dapp \u03b8 0 , q 0 q\u2207 \u03b1\u02dapp \u03b8 0 ,q0q \u017c p \u03b8 0 px 0 qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 qdx 0 , (16) where \u2207 \u03b1\u02dapp \u03b8 0 ,q0q \u015f p \u03b8 0 px 0 qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 qdx 0 is 0 from the envelope theorem. Then we have \u2207 \u03b8 \u017c p \u03b8 0 px 0 qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 qdx 0 \" \u2207 \u03b8 \u017c\u02c6\u017c p \u03b8 x 0:T px 0:T qdx 1:T\u02d9f\u03b1\u02dapp \u03b8 0 ,q0q px 0 qdx 0 , \" \u2207 \u03b8 \u017c p \u03b8 x 0:T px 0:T qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 qdx 0:T \" \u017c p \u03b8 x 0:T px 0:T qf \u03b1\u02dapp \u03b8 0 ,q0q px 0 q\u2207 \u03b8 log p \u03b8 x 0:T px 0:T qdx 0:T \" E p \u03b8 x 0:T \u00ab f \u03b1\u02dapp \u03b8 0 ,q0q px 0 q T\u00b41 \u00ff t\"0 \u2207 \u03b8 log p \u03b8 t px t |x t`1 q ff ,(17)\nwhere the second last equality is from the continuous assumptions to exchange integral and derivative and the log derivative trick. The proof is then complete.\n\n\nMDP construction for policy gradient equivalence.\n\nHere we explain why Eq. (15) could be viewed as policy gradient. We can construct an MDP with a finite horizon T : Treat p \u03b8 t px t |x t`1 q as a policy, and assume that transition is an identical mapping such that the action is to choose the next state. Consider reward as f \u03b1\u02dapp \u03b8 0 ,q0q px 0 q at the final step, and as 0 at any other steps. Then Eq. (15) is equivalent to performing policy gradient (Williams, 1992).\n\nComparing Eq. (14) and Eq. (15):\n\n\u2022 Eq. (14) uses the gradient of the critic, while Eq. (15) only uses the value of the critic. This indicates that for policy gradient, weaker conditions are required for critics to provide meaningful guidance for the generator, which means more choices of A can be applied here. Figure 3. Illustration of the surrogate function given a fixed critic (red), and the actual objective \u03a6pp \u03b8 1 0 , q0q (dark). The horizontal axis represents the variable \u03b8 1 . Starting from \u03b8, a descent in the surrogate function is a sufficient condition for a descent in \u03a6pp \u03b8 1 0 , q0q.\n\n\u2022 We compute the sum of gradients for each step in Eq. (15), which does not suffer from exploding or vanishing gradients. Also, we do not need to track gradients of the generated sequence during T steps.\n\n\u2022 However, stochastic policy gradient methods usually suffer from higher variance (Mohamed et al., 2020). Thanks to similar techniques in RL, we can reduce the variance via a baseline trick, which will be discussed in Section 4.3.1.\n\nIn conclusion, Eq. (15) is comparable to Eq. (14) in expectation, with potential benefits like numerical stability, memory efficiency, and a wider range of the critic family A. It could suffer from higher variance but the baseline trick can help. We denote such kind of method as SFT-PG (shortcut fine-tuning with policy gradient).\n\nEmpirical comparison. We conduct experiments on some toy datasets (Fig 4), where we show the performance of Eq. (15) with the baseline trick is at least comparable to Eq. (14) at convergence when they use the same gradient penalty (GP) for critic regularization. We further observe SFT-PG with a newly proposed baseline regularization (B) enjoys a noticeably better performance compared to SFT with GP. The regularization methods will be introduced in Section 4.3.2. Experimental details are in Section 6.2.2.\n\n\nTowards Monotonic Improvement\n\nThe gradient update discussed in Eq. (15) only supports one step of gradient update, given a fixed critic f \u03b1\u02dapp \u03b8 0 ,q0q that is optimal to the current \u03b8. Questions remain: When is our update guaranteed to get improvement? Can we do more than one update to get a potential descent? We answer the questions by providing a surrogate function of the IPM. Assume that gpp \u03b8 0 , f \u03b1 , q 0 q is Lipschitz w.r.t. \u03b8, given q 0 and \u03b1 P A. Given a fixed critic f \u03b1\u02dapp \u03b8 0 ,q0q , there exists l \u011b 0 such that \u03a6pp \u03b8 1 0 , q 0 q is upper bounded by the surrogate function below:\n\u03a6pp \u03b8 1 0 , q 0 q \u010f gpp \u03b8 1 0 , f \u03b1\u02dapp \u03b8 0 ,q0q , q 0 q`2l||\u03b8 1\u00b4\u03b8 ||. (18)\nProof of Theorem 4.2 can be found in Appendix C. Here we provide an illustration of Theorem 4.2 in Fig 3. Given a critic that is optimal w.r.t. \u03b8, \u03a6pp \u03b8 1 0 , q 0 q is unknown if \u03b8 \u2030 \u03b8 1 . But if we can get a descent of the surrogate function, we are also guaranteed to get a descent of \u03a6pp \u03b8 1 0 , q 0 q, which facilitates more potential updates even if \u03b8 1 \u2030 \u03b8.\n\nMoreover, using the Lagrange multiplier, we can convert minimizing the surrogate function to a constrained optimization problem to optimize gpp \u03b8 1 0 , f \u03b1\u02dapp \u03b8 0 ,q0q , q 0 q with the constraint that ||\u03b8 1\u00b4\u03b8 || \u010f \u03b4 for some \u03b4 \u0105 0. Following this idea, one simple trick is to perform n generator steps of gradient updates with a small learning rate, and clip the gradient norm with threshold \u03b3. We present the empirical effect of such simple modification in Section 6.2.3, Table 2.\n\nDiscussion. One may notice that Theorem 4.2 is similar in spirit to Theorem 1 in TRPO (Schulman et al., 2015a), which provides a surrogate function for a fixed but unknown reward function. In our case, the reward function f \u03b1\u02dapp \u03b8 0 ,q0q\n\nis known for the current \u03b8 but changing: It is dependent on the current \u03b8 so it remains unknown for \u03b8 1 \u2030 \u03b8. The proof techniques are also different, but they both estimate an unknown part of the objective function.\n\n\nAlgorithm Design\n\nIn the previous sections, we only consider the case where we have an optimal critic function given \u03b8. In the training, we adopt similar techniques in WGAN (Arjovsky et al., 2017) to perform alternative training of the critic and generator in order to approximate the optimal critic. Consider the objective function below:\nmin \u03b8 max \u03b1PA gpp \u03b8 0 , f \u03b1 , q 0 q.(19)\nNow we discuss techniques to reduce the variance of the gradient estimation and regularize the critic, and then give an overview of our algorithm.\n\n\nBASELINE FUNCTION FOR VARIANCE REDUCTION\n\nGiven a critic \u03b1, we can adopt a technique widely used in policy gradient to reduce the variance of the gradient estimation in Eq. (15). Similar to Schulman et al. (2015b), we can subtract a baseline function V \u03c9 t`1 px t`1 q from the cumulative reward f \u03b1 px 0 q, without changing the expectation:\n\u2207 \u03b8 gpp \u03b8 0 , f \u03b1 , q 0 q \" E p \u03b8 x 0:T \u00ab f \u03b1 px 0 q T\u00b41 \u00ff t\"0 \u2207 \u03b8 log p \u03b8 t px t |x t`1 q ff \" E p \u03b8 x 0:T \u00ab T\u00b41 \u00ff t\"0 pf \u03b1 px 0 q\u00b4V \u03c9 t`1 px t`1 qq\u2207 \u03b8 log p \u03b8 t px t |x t`1 q ff ,(20)\nwhere the optimal choice of V \u03c9 t`1 px t`1 q to minimize the variance would be V t`1 px t`1 , \u03b1q :\" E p \u03b8\n\nx 0:T rf \u03b1 px 0 q|x t`1 s. (20) can be found in Appendix D. Thus, given a critic \u03b1 and a generator \u03b8, we can train a value function V \u03c9 t`1 by minimizing the objective below: \n\n\nDetailed derivation of Eq\nR B p\u03b1, \u03c9, \u03b8q \" E p \u03b8 x 0:T \u00ab T\u00b41 \u00ff t\"0 pV \u03c9 t`1 px t`1 q\u00b4V t`1 px t`1 , \u03b1qq 2 ff .(21)R GP p\u03b1, \u03b8q \" \u00ca x0 \" p||\u2207 x0 f \u03b1 px 0 q||\u00b41q 2 \u2030 ,(22)\nwherex 0 is sampled uniformly on the line segment between x 1 0 \" p \u03b8 0 and x 2 0 \" q 0 . f \u03b1 can be trained to maximize gpp \u03b8 0 , f \u03b1 , q 0 q\u00b4\u03b7R GP p\u03b1, \u03c9, \u03b8q, \u03b7 \u0105 0 is the regularization coefficient.\n\nReusing baseline for critic regularization. As discussed in Section 4.1, since we only use the critic value during updates, now we can afford a potentially wider range of critic family A. Some regularization on f \u03b1 is still needed; Otherwise its value can explode. Also, regularization is shown to be beneficial for local convergence (Mescheder et al., 2018). So we consider regularization that can be weaker than gradient constraints, such that the critic is more sensitive to the changes of the generator, which could be favorable when updating the critic for a fixed number of training steps.\n\nWe found an interesting fact that the loss R B p\u03b1, \u03c9, \u03b8q can be reused to regularize the value of f \u03b1 instead of the gradient, which implicitly defines a set A that shows empirical benefits in practice.\n\n\nDefine\n\nLp\u03b1, \u03c9, \u03b8q :\" gpp \u03b8 0 , f \u03b1 , q 0 q\u00b4\u03bbR B p\u03b1, \u03c9, \u03b8q.\n\nGiven \u03b8, our critic \u03b1 and baseline \u03c9 can be trained together to maximize Lp\u03b1, \u03c9, \u03b8q.\n\nWe provide an explanation of such kind of implicit regularization. During the update, we can view V \u03c9 t`1 as an approximation of the expected value of f \u03b1 from the previous step. The regularization provides a trade-off between maximizing gpp \u03b8 0 , f \u03b1 , q 0 q and minimizing changes in the expected value of f \u03b1 , preventing drastic changes in the critic and stabilizing the training. Intuitively, it helps local convergence when both the critic and generator are already near-optimal: there is an extra cost for the critic value to diverge away from the optimal value. As a byproduct, it also makes the baseline function easier to fit since the regularization loss is reused.\n\nEmpirical comparison: baseline regularization and gradient penalty. We present a comparison of gradient penalty (GP) and baseline regularization (B) for policy gradient training (SFT-PG) in Section 6.2.2, Fig 4 on toy datasets, which shows in policy gradient training, the baseline function performs comparably well or even better than gradient penalty.\n\n\nPUTTING TOGETHER: ALGORITHM OVERVIEW\n\nNow we are ready to present our algorithm. Our critic \u03b1 and baseline \u03c9 are trained to maximize Lp\u03b1, \u03c9, \u03b8q \" gpp \u03b8 0 , f \u03b1 , q 0 q\u00b4\u03bbR B p\u03b1, \u03c9, \u03b8q, and the generator is trained to minimize gpp \u03b8 0 , f \u03b1 , q 0 q via Eq. (20). To save memory usage, we use a buffer B that contains tx t`1 , x t , x 0 , tu generated from the current generator without tracking the gradient, and randomly sample a batch from the buffer to compute Eq. (20) and then perform backpropagation. The maximization and minimization steps are performed alternatively. See details in Alg 1. \n\n\nRelated Works\n\nGAN and RL. There are works using ideas from RL to train GANs Sarmad et al., 2019;Bai et al., 2019). The most relevant work is Se-qGAN , which uses policy gradient to train the generator network. There are several main differences between their settings and ours. First, different GAN objectives are used: SeqGAN uses the JS divergence while we use IPM. In SeqGAN, the next token is dependent on tokens generated from all previous steps, while in diffusion models the next image is only dependent on the model output from one previous step; Also, the critic takes the whole generated sequence as input in SeqGAN, while we only care about the final output. Besides, in our work, rewards are mathematically derived from performing gradient descent w.r.t. IPM, while in SeqGAN, rewards are designed manually. In conclusion, different from SeqGAN, we propose a new policy gradient algorithm to optimize the IPM objective, with a novel analysis of monotonic improvement conditions and a new regularization method for the critic.\n\n\nDiffusion and GAN.\n\nThere are other works combining diffusion and GAN training: Xiao et al. (2021) consider multi-modal noise distributions generated by GAN to enable fast sampling;  considers a truncated forward process by replacing the last steps in the forward process with an autoencoder to generate noise, and start with the learned autoencoder as the first step of denoising and then continue to generate data from the diffusion model; Diffusion GAN (Wang et al., 2022) perturbs the data with an adjustable number of steps, and minimizes JS divergence for all intermediate steps by training a multi-step generator with a time-dependent discriminator. To our best knowledge, there is no existing work using GAN-style training to finetune a pretrained DDPM sampler.\n\nFast samplers of DDIM and more. There is another line of work on fast sampling of DDIM (Song et al., 2020a), for example, knowledge distillation (Luhman and Luhman, 2021;Salimans and Ho, 2021) and solving ordinary differential equations (ODEs) with fewer steps (Liu et al., 2022;Lu et al., 2022). Samples generated by DDIM are generally less diverse than DDPM (Song et al., 2020a). Also, fast sampling is generally easier for DDIM samplers (with deterministic Markov chains) than DDPM samplers, since it is possible to combine multiple deterministic steps into one step without loss of fidelity, but not for combining multiple Gaussian steps as one (Salimans and Ho, 2021). Fine-tuning DDIM samplers with deterministic policy gradient for fast sampling also seems possible, but deterministic policies may suffer from suboptimality, especially in high-dimensional action space (Silver et al., 2014), though it might require fewer samples. Also, it becomes less necessary since distil-lation is already possible for DDIM.\n\nMoreover, there is also some recent work that uses sample quality metrics to enable fast sampling. Instead of finetuning pretrained models, Watson et al. (2021b) propose to optimize the hyperparameters of the sampling schedule for a family of non-Markovian samplers by differentiating through KID (Bi\u0144kowski et al., 2018), which is calculated by pretrained inception features. It is followed by a contemporary work that fine-tunes pretrained DDIM models using MMD calculated by pretrained features (Aiello et al., 2023), which is similar to the method discussed in Section 3.3 but with a fixed critic and a deterministic sampling chain. Generally speaking, adversarially trained critics can provide stronger signals than fixed ones and are more helpful for training (Li et al., 2017). As a result, besides the potential issues discussed in Section 3.3, such training may also suffer from sub-optimal results when p \u03b8 0 is not close enough to q 0 at initialization, and is highly dependent on the choice of the pretrained feature.\n\n\nExperiments\n\nIn this section, we aim to answer the following questions:\n\n\u2022 (Section 6.2.1) Does the proposed algorithm SFT-PG (B) work in practice?\n\n\u2022 (Section 6.2.2) How does SFT-PG (Eq. (15)) work compared to SFT (Eq. (14)) with the same regularization (GP), and how does baseline regularization (B) compared to gradient penalty (GP) in SFT-PG?\n\n\u2022 (Section 6.2.3) Do more generator steps with gradient clipping improve the performance, as discussed in Section 4.2?\n\n\u2022 (Section 6.3) Does the proposed fine-tuning SFT-PG (B) improve existing fast samplers of DDPM on benchmark datasets?\n\nCode is available at https://github.com/ UW-Madison-Lee-Lab/SFT-PG.\n\n\nSetup\n\nHere we provide the setup of our training algorithm on different datasets. Model architectures and training details can be found in Appendix F.\n\nToy datasets. The toy datasets we use are swiss roll and two moons (Pedregosa et al., 2011). We use \u03bb \" 0.1, n critic \" 5, n generator \" 1 with no gradient clipping. For evaluation, we use the Wasserstein-2 distance on 10K samples from p 0 and q 0 respectively, calculated by POT (Flamary et al., 2021). . Training curves (4a, 4e) and 10K randomly generated samples from SFT (GP) (4b, 4f), SFT-PG (GP) (4c, 4g), and SFT-PG (B) (4d, 4h) at convergence. In the visualizations, red dots indicate the ground truth data, and blue dots indicate generated data. We can observe that SFT-PG (B) produces noticeably better distributions, which is the result of utilizing a wider range of critics.\n\nImage datasets. We use MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2009) and CelebA (Liu et al., 2015). For hyperparameters, we choose \u03bb \" 1.0, n critic \" 5, n generator \" 10, \u03b3 \" 0.1, except when testing different choices of n generator and \u03b3 in MNIST, where we use n generator \" 5 and varying \u03b3. For evaluation, we use FID (Heusel et al., 2017) measured by 50K samples generated from p \u03b8 0 and q 0 respectively.\n\n\nProof-of-concept Results\n\nIn this section, we fine-tune pretrained DDPMs with T \" 10, and present the effect of the proposed algorithm SFT-PG with baseline regularization on toy datasets. We present the results of different gradient estimations discussed in Section 4.1, different critic regularization methods discussed in Section 4.3.2, and the training technique with more generator steps discussed Section 4.2.\n\n\nIMPROVEMENT FROM FINE-TUNING\n\nOn the swiss roll dataset, we first train a DDPM with T \" 10 till convergence, and then use it as initialization of our finetuning. As in Table 1, our fine-tuned sampler with 10 steps can get better Wasserstein distance not only compared to the DDPM with T \" 10, but can even outperform DDPM with T \" 1000, which is reasonable since we directly optimize the IPM objective. 5 The training curve and the data visualization can be found in Fig 4a and Fig 4d. 5 Besides, our algorithm also works when training from scratch with a final performance comparable to fine-tuning, but it will take longer to train.\n\n\nMethod\n\nW 2 pp \u03b8 0 , q 0 q (\u02c610\u00b42) (\u00d3) T \" 10, DDPM 8.29 T \" 100, DDPM 2.36 T \" 1000, DDPM 1.78 T \" 10, SFT-PG (B) 0.64 Table 1. Comparison of DDPM models and our fine-tuned model on the swiss roll dataset.\n\n\nEFFECT OF DIFFERENT GRADIENT ESTIMATIONS AND REGULARIZATIONS\n\nOn the toy datasets, we compare gradient estimation SFT-PG and SFT, both with gradient penalty (GP). 6 We also compare them to our proposed algorithm SFT-PG (B). All methods are initialized with pretrained DDPM, T \" 10, then trained till convergence. As shown in Fig 4, we can observe that all methods converge and the training curves are almost comparable, while SFT-PG (B) enjoys a slightly better final performance.\n\n\nEFFECT OF GRADIENT CLIPPING WITH MORE GENERATOR STEPS\n\nIn Section 4.2, we discussed that performing more generator steps with the same fixed critic and clipping the gradient . Randomly generated images before and after fine-tuning, on CIFAR10 p32\u02c632q and CelebA p64\u02c664q, T 1 \" 10. The initialization is from pretrained models with T \" 1000 and sub-sampling schedules with T 1 \" 10 calculated from FastDPM (Kong and Ping, 2021).\n\nnorm can improve the training of our algorithm. Here we present the effect of n generator \" 1 or 5 with different gradient clipping thresholds \u03b3 on MNIST, initialized with a pretrained DDPM with T \" 10, FID=7.34. From Table 2, we find that a small \u03b3 with more steps can improve the final performance, but could hurt the performance if too small. Randomly generated samples from the model with the best FID are in Fig 6. We also conducted similar experiments on the toy datasets, but we find no significant difference on the final results, which is expected since the task is too simple.\n\nMethod FID (\u00d3) 1 step 1.35 5 steps, \u03b3 \" 10 0.83 5 steps, \u03b3 \" 1.0 0.82 5 steps, \u03b3 \" 0.1 0.89 5 steps, \u03b3 \" 0.001 1.46 Table 2. Effect of ngenerator and \u03b3. Figure 6. Generated samples.\n\n\nBenchmark Results\n\nTo compare with existing fast samplers of DDPM, we take pretrained DDPMs with T \" 1000 and fine-tune them with sampling steps T 1 \" 10 on image benchmark datasets CIFAR-10 and CelebA.\n\nOur baselines include various fast DDPM samplers with Gaussian noises: naive DDPM sub-sampling, Fast-DPM (Kong and Ping, 2021), and recently advanced samplers like Analytic DPM (Bao et al., 2021) and SN-DPM . For fine-tuning, we use the fixed variance and sub-sampling schedules computed by Fast-DPM with T 1 \" 10 and only train the mean prediction model. From Table 3, we can observe that the performance of fine-tuning with T 1 \" 10 is comparable to the pretrained model with T \" 1000, outperforming the existing fast DDPM samplers. Randomly generated images before and after fine-tuning are in    Table 3. FID (\u00d3) on CIFAR-10 and CelebA, T 1 \" 10 for all methods. Our fine-tuning produces comparable results with the full-step pretrained models (FID = 3.03 for CIFAR-10, and FID = 3.26 for CelebA, T \" 1000).\n\nWe also present a comparison with DDIM sampling methods on CIFAR 10 benchmark in Appendix E, where our method is comparable to progressive distillation with T 1 \" 8.\n\n\nDiscussions and Limitations\n\nIn our experiments, we only train the mean prediction model given a pretrained DDPM. It is also possible to learn the variance via fine-tuning with the same objective, and we leave it as future work. We also note that although we do not need to track the gradients during all sampling steps, we still need to run T 1 inference steps to collect the sequence, which is inevitably slower than one-step GAN training.\n\n\nConclusion\n\nIn this work, we fine-tune DDPM samplers to minimize the IPMs via policy gradient. We show performing gradient descent of stochastic Markov chains w.r.t. IPM is equivalent to policy gradient, and present a surrogate function of the IPM which sheds light on monotonic improvement conditions. Our fine-tuning improves the existing fast samplers of DDPM, achieving comparable or even higher sample quality than the full-step model on various datasets. A. Visualization: Effect of Shortcut Fine-Tuning (a) Sampling steps before fine-tuning from DDPM (b) Sampling steps after fine-tuning Figure 7. Visualization of the sampling path before (7a) and after short-cut fine-tuning (7b).\n\nWe provide visualizations of the complete sampling chain before and after fine-tuning in Fig 7. We generate 50 data points using the same random seed for DDPM and our fine-tuned model, trained on the same Gaussian cluster centered at the red spot p0.5, 0.5q with a standard deviation of 0.01 in each dimension, T \" 2. The whole sampling path is visualized where different steps are marked with different intensities of the color: data points with the darkest color are finally generated. As shown in Fig 7, our fine-tuning does find a \"shortcut\" path to the final distribution.\n\nB. Illustration of Sub-sampling with T 1 ! T in DDPM I. Noising steps (\u2192) and learned denoising steps (\u2190)\n\nII. Sub-sampling steps III. Fine-tuned steps Figure 8. When T is large but we sub-sample with T 1 ! T cannot approximate the backward process accurately when each step is Gaussian as discussed in Case 2, Section 3.1. In this case, shortcut fine-tuning can also solve the issue by directly minimizing the IPM as an objective function.\n\n\nC. Towards Monotonic Improvement\n\nHere we present detailed proof of Theorem 4.2. For simplicity, we denote p \u03b8 0 as p \u03b8 , q 0 as q, and z P R d to replace x 0 as a variable in our sample space.\n\nRecall the generated distribution: p \u03b8 . Given target distribution q, the objective function is:\nmin \u03b8 max \u03b1PA gpp \u03b8 , f \u03b1 , qq,(24)\nwhere gpp \u03b8 , f \u03b1 , qq \" \u015f pp \u03b8 pzq\u00b4qpzqqf \u03b1 pzqdz. We can observe that SFT-PG with NFE=10 produces the best FID, and SFT-PG with NFE=8 is comparable to progressive distillation with the same NFE. Our method is orthogonal to other fast sampling methods like distillation. We also note that our fine-tuning is more computationally efficient than progressive distillation: For example, for CIFAR10, progressive distillation takes about a day using 8 TPUv4 chips, while our method takes about 6h using 4 RTX 2080Ti, and the original DDPM training takes 10.6h using TPU v3.8. Besides, since we use a fixed small learning rate during training (1e-6), it is also possible to further accelerate our training by choosing appropriate learning rate schedules.\n\n\nF. Experimental Details\n\nHere we provide more details for our fine-tuning settings for reproducibility.\n\n\nF.1. Experiments on Toy Datasets\n\nTraining sets. For 2D toy datasets, each training set contains 10K samples.\n\nModel architecture. The generator we adopt is a 4-layer MLP with 128 hidden units and soft-plus activations. The critic and the baseline function we use are 3-layer MLPs with 128 hidden units and ReLU activations.\n\nTraining details. For optimizers, we use Adam (Kingma and Ba, 2014) with lr \" 5\u02c610\u00b45 for the generator, and lr \" 1\u02c610\u00b43 for both the critic and baseline functions. Pretraining for DDPM is conducted for 2000 epochs for T \" 10, 100, 1000 respectively. Both pretraining and fine-tuning use batch size 64 and we train 300 epochs for fine-tuning.\n\n\nF.2. Experiments on Image Datasets\n\nTraining sets. We use 60K training samples from MNIST, 50K training samples from CIFAR-10, and 162K samples from CelebA.\n\nModel architecture. For model architecture, we use U-Net as the generative model as Ho et al. (2020). For the critic, we adopt 3 convolutional layers with kernel size = 4, stride = 2, padding = 1 for downsampling, followed by 1 final convolutional layer with kernel size = 4, stride = 1, padding = 0, and then take the average of the final output. The numbers of output channels are 256,512,1024,1 for each layer, with Leaky ReLU (slope = 0.2) as activation. For the baseline function, we use a 4-layer MLP with timestep embeddings. The numbers of hidden units are 1024, 1024, 256, and the output dimension is 1.\n\nTraining details. For MNIST, we train a DDPM with T \" 10 steps for 100 epochs to convergence as a pretrained model. For CIFAR-10 and CelebA, we use the pretrained model in Ho et al. (2020) and Song et al. (2020a) respectively with T \" 1000, and use the sampling schedules calculated by FastDPM (Kong and Ping, 2021) with VAR approximation and DDPM sampling schedule as initialization for our fine-tuning. We found that rescaling the pixel values to [0,1] is a default choice in FastDPM, but it hurts the training if we put the rescaled images directly into the critic, so we remove the rescaling part during our fine-tuning. For optimizers, we use Adam with lr \" 1\u02c610\u00b46 for the generator, and lr \" 1\u02c610\u00b44 for both the critic and baseline functions. We found that smaller learning rates help the stability of training, which is compliant with the theoretical result in Section 4.2. For MNIST and CIFAR-10, we train 100 epochs with batch size = 128. For CelebA we trained 100 epochs with batch size = 64.\n\nMore generated samples. We present generated samples from the initialized FastDPM and our fine-tuned model respectively using the same random seed to show the effect of our fine-tuning in Fig 9 and Fig 10. We notice that some of the images generated by our fine-tuned model are similar to images at initialization but with much richer colors and more details, and there are also some cases that the images after fine-tuning look very different than that from initialization. Figure 9. Images generated from FastDPM as initialization (on the top) and from the fine-tuned model (on the bottom), generated using the same seed, trained on CIFAR-10. Figure 10. Images generated from FastDPM as initialization (on the top) and from the fine-tuned model (on the bottom), generated using the same seed, trained on CelebA.\n\n\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\n\nFigure 1 .\n1Image denoising is similar to a closed-loop control system: finding paths from pure noise to natural images.\n\n\nTheorem 4.2. (The surrogate function of IPM)\n\nAlgorithm 1\n1Shortcut Fine-Tuning with Policy Gradient and Baseline Regularization: SFT-PG (B) Input: n critic , n generator , batch size m, critic parameters \u03b1, baseline function parameter \u03c9 , pretrained generator \u03b8, regularization hyperparameter \u03bb while \u03b8 not converged do Initialize trajectory buffer B as H for i = 0,...,n critic do Obtain m i.i.d. samples from p \u03b8 x 0:T Add all tx t`1 , x t , x 0 , tu to B, t \" 0, ..., T\u00b41 Obtain m i.i.d. samples from q 0 Update \u03b1 and \u03c9 via maximizing Eq. (23) end for for j = 0,...,n generator do Obtain m samples of tx t`1 , x t , x 0 , tu from B Update \u03b8 via policy gradient according to Eq. (20) end for end while\n\nFigure 4\n4Figure 4. Training curves (4a, 4e) and 10K randomly generated samples from SFT (GP) (4b, 4f), SFT-PG (GP) (4c, 4g), and SFT-PG (B) (4d, 4h) at convergence. In the visualizations, red dots indicate the ground truth data, and blue dots indicate generated data. We can observe that SFT-PG (B) produces noticeably better distributions, which is the result of utilizing a wider range of critics.\n\nFigure 5\n5Figure 5. Randomly generated images before and after fine-tuning, on CIFAR10 p32\u02c632q and CelebA p64\u02c664q, T 1 \" 10. The initialization is from pretrained models with T \" 1000 and sub-sampling schedules with T 1 \" 10 calculated from FastDPM (Kong and Ping, 2021).\n\n\nFig 5.\n\n\n4.3.2. CHOICES OF A: REGULARIZING THE CRITICHere we discuss different choices of A, which indicates different regularization methods for the critic.Lipschitz regularization. If we choose A to include parameters of all 1-Lipschitz functions, we can adopt regularization asWGAN-GP (Gulrajani et al., 2017):\n\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n\nTable 4. Comparison with DDIM sampling methods which is deterministic given the initial noise.Method (DDPM, stochastic) NFE FID \nMethod (DDIM, deterministic) NFE FID \nDDPM \n10 \n34.76 \nDDIM \n10 \n17.33 \nSN-DDPM \n10 \n16.33 \nDPM-solver \n10 \n4.70 \nSFT-PG* \n10 \n2.28 \nSFT-PG* \n8 \n2.64 \nProgressive distillation*`8 \n2.57 \n\n\nUW Madison. Correspondence to: Ying Fan, Kangwook Lee <yfan87@wisc.edu, kangwook.lee@wisc.edu>.\nIn this work we consider a DDPM sampler with a fixed variance schedule \u03b21:T as inHo et al. (2020), but it could also be learned as in.\nRecall that during the diffusion process, we need small Gaussian noise for each step set the sampling chain to also be conditional Gaussian(Ho et al., 2020). As a result, a small T means qT is not close to pure Gaussian, and thus pT ff qT .\nFor example, MMD with very narrow kernels can produce such critic functions, where each data point defines the center of the corresponding kernel which yields gradient 0.\nFor gradient penalty coefficient, we tested different choices in r0.001, 10s and pick the best choice 0.001. We also tried spectral normalization for Lipschitz constraints, but we found that its performance is worse than gradient penalty on these datasets.\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 33Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Infor- mation Processing Systems, 33:6840-6851, 2020.\n\nImproved denoising diffusion probabilistic models. Alexander Quinn, Nichol , Prafulla Dhariwal, International Conference on Machine Learning. PMLRAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Interna- tional Conference on Machine Learning, pages 8162- 8171. PMLR, 2021.\n\nDiffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. 34Prafulla Dhariwal and Alexander Nichol. Diffusion mod- els beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794, 2021.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger27Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Informa- tion Processing Systems, volume 27, 2014.\n\nOn fast sampling of diffusion probabilistic models. Zhifeng Kong, Wei Ping, arXiv:2106.00132arXiv preprintZhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint arXiv:2106.00132, 2021.\n\nNoise estimation for generative diffusion models. Robin San-Roman, Eliya Nachmani, Lior Wolf, arXiv:2104.02600arXiv preprintRobin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. arXiv preprint arXiv:2104.02600, 2021.\n\n. W Y Max, Jun Lam, Rongjie Wang, Dan Huang, Dong Su, Yu, arXiv:2108.11514Bilateral denoising diffusion models. arXiv preprintMax WY Lam, Jun Wang, Rongjie Huang, Dan Su, and Dong Yu. Bilateral denoising diffusion models. arXiv preprint arXiv:2108.11514, 2021.\n\nLearning to efficiently sample from diffusion probabilistic models. Daniel Watson, Jonathan Ho, Mohammad Norouzi, William Chan, arXiv:2106.03802arXiv preprintDaniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sample from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021a.\n\nGotta go fast when generating data with score-based models. Alexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, Ioannis Mitliagkas, arXiv:2105.14080arXiv preprintAlexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021.\n\nAnalyticdpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang, International Conference on Learning Representations. Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic- dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In International Con- ference on Learning Representations, 2021.\n\nEstimating the optimal covariance with imperfect mean in diffusion probabilistic models. Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, Bo Zhang, International Conference on Machine Learning. PMLRFan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo Zhang. Estimating the optimal covariance with im- perfect mean in diffusion probabilistic models. In Inter- national Conference on Machine Learning, pages 1555- 1584. PMLR, 2022.\n\nTackling the generative learning trilemma with denoising diffusion gans. Zhisheng Xiao, Karsten Kreis, Arash Vahdat, International Conference on Learning Representations. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. In International Conference on Learning Represen- tations, 2021.\n\nDenoising diffusion implicit models. Jiaming Song, Chenlin Meng, Stefano Ermon, International Conference on Learning Representations. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing diffusion implicit models. In International Confer- ence on Learning Representations, 2020a.\n\nImitation learning: A survey of learning methods. Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, Chrisina Jayne, ACM Computing Surveys (CSUR). 502Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1-35, 2017.\n\nBehavioral cloning from observation. Faraz Torabi, Garrett Warnell, Peter Stone, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceFaraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4950-4957, 2018.\n\nScorebased generative modeling through stochastic differential equations. Yang Song, Jascha Sohl-Dickstein, P Diederik, Abhishek Kingma, Stefano Kumar, Ben Ermon, Poole, International Conference on Learning Representations. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score- based generative modeling through stochastic differential equations. In International Conference on Learning Rep- resentations, 2020b.\n\nScore-based generative modeling secretly minimizes the wasserstein distance. Dohyun Kwon, Ying Fan, Kangwook Lee, Advances in Neural Information Processing Systems. Dohyun Kwon, Ying Fan, and Kangwook Lee. Score-based generative modeling secretly minimizes the wasserstein distance. In Advances in Neural Information Processing Systems, 2022.\n\nProgressive distillation for fast sampling of diffusion models. Tim Salimans, Jonathan Ho, International Conference on Learning Representations. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.\n\nStochastic backpropagation and approximate inference in deep generative models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, International conference on machine learning. PMLRDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wier- stra. Stochastic backpropagation and approximate infer- ence in deep generative models. In International con- ference on machine learning, pages 1278-1286. PMLR, 2014.\n\nWasserstein generative adversarial networks. Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou, International conference on machine learning. PMLRMartin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Inter- national conference on machine learning, pages 214-223. PMLR, 2017.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International conference on machine learning. PMLRJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jor- dan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889-1897. PMLR, 2015a.\n\nHigh-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438arXiv preprintJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\n\nWhich training methods for gans do actually converge?. Lars Mescheder, Andreas Geiger, Sebastian Nowozin, International conference on machine learning. PMLRLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International conference on machine learning, pages 3481-3490. PMLR, 2018.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence31Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradi- ent. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.\n\nIrgan: A minimax game for unifying generative and discriminative information retrieval models. Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, Dell Zhang, Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR conference on Research and Development in Information RetrievalJun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, and Dell Zhang. Irgan: A minimax game for unifying generative and discriminative information retrieval models. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, pages 515-524, 2017.\n\nRl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion. Muhammad Sarmad, Jenny Hyunjoo, Young Min Lee, Kim, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMuhammad Sarmad, Hyunjoo Jenny Lee, and Young Min Kim. Rl-gan-net: A reinforcement learning agent con- trolled gan network for real-time point cloud shape com- pletion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5898- 5907, 2019.\n\nA modelbased reinforcement learning with adversarial training for online recommendation. Xueying Bai, Jian Guan, Hongning Wang, Advances in Neural Information Processing Systems. 32Xueying Bai, Jian Guan, and Hongning Wang. A model- based reinforcement learning with adversarial training for online recommendation. Advances in Neural Information Processing Systems, 32, 2019.\n\nTruncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou, arXiv:2202.09671arXiv preprintHuangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion probabilistic mod- els and diffusion-based adversarial auto-encoders. arXiv preprint arXiv:2202.09671, 2022.\n\nDiffusion-gan: Training gans with diffusion. Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou, arXiv:2206.02262arXiv preprintZhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022.\n\nKnowledge distillation in iterative generative models for improved sampling speed. Eric Luhman, Troy Luhman, arXiv:2101.02388arXiv preprintEric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021.\n\nPseudo numerical methods for diffusion models on manifolds. Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao, arXiv:2202.09778arXiv preprintLuping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022.\n\nDpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu, arXiv:2206.00927arXiv preprintCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.\n\nDeterministic policy gradient algorithms. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller, International conference on machine learning. PMLRDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387-395. PMLR, 2014.\n\nLearning fast samplers for diffusion models by differentiating through sample quality. Daniel Watson, William Chan, Jonathan Ho, Mohammad Norouzi, International Conference on Learning Representations. Daniel Watson, William Chan, Jonathan Ho, and Moham- mad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In International Conference on Learning Representations, 2021b.\n\n. Miko\u0142aj Bi\u0144kowski, J Danica, Michael Sutherland, Arthur Arbel, Gretton, arXiv:1801.01401Demystifying mmd gans. arXiv preprintMiko\u0142aj Bi\u0144kowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.\n\nFast inference in denoising diffusion models via mmd finetuning. Emanuele Aiello, Diego Valsesia, Enrico Magli, arXiv:2301.07969arXiv preprintEmanuele Aiello, Diego Valsesia, and Enrico Magli. Fast in- ference in denoising diffusion models via mmd finetuning. arXiv preprint arXiv:2301.07969, 2023.\n\nMmd gan: Towards deeper understanding of moment matching network. Advances in neural information processing systems. Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab\u00e1s P\u00f3czos, 30Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab\u00e1s P\u00f3czos. Mmd gan: Towards deeper under- standing of moment matching network. Advances in neural information processing systems, 30, 2017.\n\nScikitlearn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour- napeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit- learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\nPot: Python optimal transport. R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Z Mokhtar, Aur\u00e9lie Alaya, Stanislas Boisbunon, Laetitia Chambon, Adrien Chapel, Kilian Corenflos, Nemo Fatras, L\u00e9o Fournier, Nathalie T H Gautheron, Hicham Gayraud, Alain Janati, Ievgen Rakotomamonjy, Antoine Redko, Antony Rolet, Vivien Schutz, Danica J Seguy, Romain Sutherland, Alexander Tavenard, Titouan Tong, Vayer, Journal of Machine Learning Research. 2278R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur\u00e9lie Boisbunon, Stanislas Cham- bon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L\u00e9o Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1-8, 2021.\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 8611Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nDeep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730-3738, 2015.\n", "annotations": {"author": "[{\"end\":63,\"start\":54},{\"end\":77,\"start\":64}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":59},{\"end\":76,\"start\":73}]", "author_first_name": "[{\"end\":58,\"start\":54},{\"end\":72,\"start\":64}]", "author_affiliation": null, "title": "[{\"end\":51,\"start\":1},{\"end\":128,\"start\":78}]", "venue": null, "abstract": "[{\"end\":1661,\"start\":130}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1743,\"start\":1726},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2077,\"start\":2052},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2530,\"start\":2509},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2553,\"start\":2530},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2570,\"start\":2553},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2591,\"start\":2570},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2624,\"start\":2591},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2641,\"start\":2624},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2787,\"start\":2768},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3335,\"start\":3313},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3377,\"start\":3356},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3747,\"start\":3728},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6780,\"start\":6763},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8024,\"start\":8005},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9467,\"start\":9449},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10667,\"start\":10646},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10684,\"start\":10667},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10813,\"start\":10791},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10836,\"start\":10818},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12367,\"start\":12344},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13037,\"start\":13014},{\"end\":13152,\"start\":13128},{\"end\":15682,\"start\":15666},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19203,\"start\":19179},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19746,\"start\":19723},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20293,\"start\":20270},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21620,\"start\":21596},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23941,\"start\":23921},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23958,\"start\":23941},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24983,\"start\":24965},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25360,\"start\":25341},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25763,\"start\":25743},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25826,\"start\":25801},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25848,\"start\":25826},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25935,\"start\":25917},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25951,\"start\":25935},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26036,\"start\":26016},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26328,\"start\":26305},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26553,\"start\":26532},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26838,\"start\":26817},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26998,\"start\":26974},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27196,\"start\":27175},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27460,\"start\":27443},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28610,\"start\":28586},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28821,\"start\":28799},{\"end\":29256,\"start\":29230},{\"end\":29292,\"start\":29258},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29322,\"start\":29304},{\"end\":31063,\"start\":31062},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31808,\"start\":31787},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32913,\"start\":32892},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32982,\"start\":32964},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38039,\"start\":38023},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38741,\"start\":38725},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38765,\"start\":38746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38868,\"start\":38847},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43199,\"start\":43183},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43393,\"start\":43376}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40515,\"start\":40371},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40637,\"start\":40516},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40684,\"start\":40638},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41344,\"start\":40685},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41746,\"start\":41345},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42019,\"start\":41747},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42028,\"start\":42020},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42335,\"start\":42029},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42686,\"start\":42336},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43005,\"start\":42687}]", "paragraph": "[{\"end\":2790,\"start\":1677},{\"end\":3601,\"start\":2792},{\"end\":3827,\"start\":3603},{\"end\":4570,\"start\":3848},{\"end\":4595,\"start\":4572},{\"end\":4664,\"start\":4597},{\"end\":5118,\"start\":4666},{\"end\":5583,\"start\":5120},{\"end\":5632,\"start\":5585},{\"end\":5952,\"start\":5634},{\"end\":6105,\"start\":5954},{\"end\":6254,\"start\":6107},{\"end\":6584,\"start\":6256},{\"end\":6832,\"start\":6649},{\"end\":6891,\"start\":6834},{\"end\":7044,\"start\":6943},{\"end\":7097,\"start\":7046},{\"end\":7185,\"start\":7158},{\"end\":7380,\"start\":7224},{\"end\":7539,\"start\":7484},{\"end\":7547,\"start\":7541},{\"end\":7687,\"start\":7606},{\"end\":7766,\"start\":7689},{\"end\":8183,\"start\":7845},{\"end\":8375,\"start\":8222},{\"end\":8691,\"start\":8508},{\"end\":9048,\"start\":8693},{\"end\":9299,\"start\":9100},{\"end\":9468,\"start\":9301},{\"end\":10110,\"start\":9524},{\"end\":10945,\"start\":10112},{\"end\":11197,\"start\":10969},{\"end\":11490,\"start\":11199},{\"end\":11843,\"start\":11608},{\"end\":12049,\"start\":12005},{\"end\":12368,\"start\":12266},{\"end\":13153,\"start\":12449},{\"end\":13390,\"start\":13155},{\"end\":13463,\"start\":13392},{\"end\":13496,\"start\":13465},{\"end\":13521,\"start\":13498},{\"end\":13905,\"start\":13584},{\"end\":14194,\"start\":13937},{\"end\":14353,\"start\":14196},{\"end\":14464,\"start\":14458},{\"end\":15209,\"start\":15050},{\"end\":15683,\"start\":15263},{\"end\":15717,\"start\":15685},{\"end\":16286,\"start\":15719},{\"end\":16491,\"start\":16288},{\"end\":16725,\"start\":16493},{\"end\":17058,\"start\":16727},{\"end\":17569,\"start\":17060},{\"end\":18169,\"start\":17603},{\"end\":18608,\"start\":18245},{\"end\":19091,\"start\":18610},{\"end\":19330,\"start\":19093},{\"end\":19547,\"start\":19332},{\"end\":19889,\"start\":19568},{\"end\":20077,\"start\":19931},{\"end\":20420,\"start\":20122},{\"end\":20712,\"start\":20607},{\"end\":20889,\"start\":20714},{\"end\":21260,\"start\":21060},{\"end\":21857,\"start\":21262},{\"end\":22061,\"start\":21859},{\"end\":22123,\"start\":22072},{\"end\":22209,\"start\":22125},{\"end\":22887,\"start\":22211},{\"end\":23242,\"start\":22889},{\"end\":23841,\"start\":23283},{\"end\":24882,\"start\":23859},{\"end\":25654,\"start\":24905},{\"end\":26675,\"start\":25656},{\"end\":27706,\"start\":26677},{\"end\":27780,\"start\":27722},{\"end\":27856,\"start\":27782},{\"end\":28055,\"start\":27858},{\"end\":28175,\"start\":28057},{\"end\":28295,\"start\":28177},{\"end\":28364,\"start\":28297},{\"end\":28517,\"start\":28374},{\"end\":29205,\"start\":28519},{\"end\":29633,\"start\":29207},{\"end\":30050,\"start\":29662},{\"end\":30687,\"start\":30083},{\"end\":30896,\"start\":30698},{\"end\":31379,\"start\":30961},{\"end\":31809,\"start\":31437},{\"end\":32397,\"start\":31811},{\"end\":32580,\"start\":32399},{\"end\":32785,\"start\":32602},{\"end\":33598,\"start\":32787},{\"end\":33765,\"start\":33600},{\"end\":34209,\"start\":33797},{\"end\":34901,\"start\":34224},{\"end\":35480,\"start\":34903},{\"end\":35587,\"start\":35482},{\"end\":35922,\"start\":35589},{\"end\":36118,\"start\":35959},{\"end\":36216,\"start\":36120},{\"end\":37002,\"start\":36253},{\"end\":37108,\"start\":37030},{\"end\":37220,\"start\":37145},{\"end\":37435,\"start\":37222},{\"end\":37778,\"start\":37437},{\"end\":37937,\"start\":37817},{\"end\":38551,\"start\":37939},{\"end\":39555,\"start\":38553},{\"end\":40370,\"start\":39557}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6942,\"start\":6892},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7157,\"start\":7098},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7223,\"start\":7186},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7483,\"start\":7381},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7605,\"start\":7548},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7844,\"start\":7767},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8507,\"start\":8376},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9523,\"start\":9469},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11516,\"start\":11491},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11869,\"start\":11844},{\"attributes\":{\"id\":\"formula_10\"},\"end\":11919,\"start\":11869},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12004,\"start\":11919},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12265,\"start\":12050},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12448,\"start\":12369},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14457,\"start\":14354},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15049,\"start\":14465},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18244,\"start\":18170},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19930,\"start\":19890},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20606,\"start\":20421},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21005,\"start\":20918},{\"attributes\":{\"id\":\"formula_20\"},\"end\":21059,\"start\":21005},{\"attributes\":{\"id\":\"formula_22\"},\"end\":36252,\"start\":36217}]", "table_ref": "[{\"end\":19090,\"start\":19083},{\"end\":30228,\"start\":30221},{\"end\":30817,\"start\":30810},{\"end\":32036,\"start\":32029},{\"end\":32522,\"start\":32515},{\"end\":33155,\"start\":33148},{\"end\":33394,\"start\":33387}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1675,\"start\":1663},{\"end\":3846,\"start\":3830},{\"attributes\":{\"n\":\"2.\"},\"end\":6597,\"start\":6587},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6647,\"start\":6600},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8220,\"start\":8186},{\"attributes\":{\"n\":\"3.\"},\"end\":9061,\"start\":9051},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9098,\"start\":9064},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10967,\"start\":10948},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11606,\"start\":11518},{\"attributes\":{\"n\":\"4.\"},\"end\":13582,\"start\":13524},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13935,\"start\":13908},{\"end\":15261,\"start\":15212},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17601,\"start\":17572},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19566,\"start\":19550},{\"attributes\":{\"n\":\"4.3.1.\"},\"end\":20120,\"start\":20080},{\"end\":20917,\"start\":20892},{\"end\":22070,\"start\":22064},{\"attributes\":{\"n\":\"4.3.3.\"},\"end\":23281,\"start\":23245},{\"attributes\":{\"n\":\"5.\"},\"end\":23857,\"start\":23844},{\"end\":24903,\"start\":24885},{\"attributes\":{\"n\":\"6.\"},\"end\":27720,\"start\":27709},{\"attributes\":{\"n\":\"6.1.\"},\"end\":28372,\"start\":28367},{\"attributes\":{\"n\":\"6.2.\"},\"end\":29660,\"start\":29636},{\"attributes\":{\"n\":\"6.2.1.\"},\"end\":30081,\"start\":30053},{\"end\":30696,\"start\":30690},{\"attributes\":{\"n\":\"6.2.2.\"},\"end\":30959,\"start\":30899},{\"attributes\":{\"n\":\"6.2.3.\"},\"end\":31435,\"start\":31382},{\"attributes\":{\"n\":\"6.3.\"},\"end\":32600,\"start\":32583},{\"attributes\":{\"n\":\"6.4.\"},\"end\":33795,\"start\":33768},{\"attributes\":{\"n\":\"7.\"},\"end\":34222,\"start\":34212},{\"end\":35957,\"start\":35925},{\"end\":37028,\"start\":37005},{\"end\":37143,\"start\":37111},{\"end\":37815,\"start\":37781},{\"end\":40527,\"start\":40517},{\"end\":40697,\"start\":40686},{\"end\":41354,\"start\":41346},{\"end\":41756,\"start\":41748}]", "table": "[{\"end\":43005,\"start\":42783}]", "figure_caption": "[{\"end\":40515,\"start\":40373},{\"end\":40637,\"start\":40529},{\"end\":40684,\"start\":40640},{\"end\":41344,\"start\":40699},{\"end\":41746,\"start\":41356},{\"end\":42019,\"start\":41758},{\"end\":42028,\"start\":42022},{\"end\":42335,\"start\":42031},{\"end\":42686,\"start\":42338},{\"end\":42783,\"start\":42689}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2970,\"start\":2964},{\"end\":3906,\"start\":3898},{\"end\":9362,\"start\":9355},{\"end\":9973,\"start\":9967},{\"end\":10207,\"start\":10186},{\"end\":16006,\"start\":15998},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17133,\"start\":17126},{\"end\":18350,\"start\":18344},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23102,\"start\":23094},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30538,\"start\":30520},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31230,\"start\":31224},{\"end\":32230,\"start\":32224},{\"end\":32560,\"start\":32552},{\"end\":34815,\"start\":34807},{\"end\":34998,\"start\":34992},{\"end\":35409,\"start\":35403},{\"end\":35642,\"start\":35634},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39762,\"start\":39745},{\"end\":40040,\"start\":40032},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":40211,\"start\":40202}]", "bib_author_first_name": "[{\"end\":43957,\"start\":43949},{\"end\":43966,\"start\":43962},{\"end\":43979,\"start\":43973},{\"end\":44260,\"start\":44251},{\"end\":44274,\"start\":44268},{\"end\":44285,\"start\":44277},{\"end\":44580,\"start\":44572},{\"end\":44600,\"start\":44591},{\"end\":44855,\"start\":44852},{\"end\":44872,\"start\":44868},{\"end\":44893,\"start\":44888},{\"end\":44905,\"start\":44901},{\"end\":44915,\"start\":44910},{\"end\":44937,\"start\":44930},{\"end\":44950,\"start\":44945},{\"end\":44968,\"start\":44962},{\"end\":45472,\"start\":45465},{\"end\":45482,\"start\":45479},{\"end\":45693,\"start\":45688},{\"end\":45710,\"start\":45705},{\"end\":45725,\"start\":45721},{\"end\":45903,\"start\":45902},{\"end\":45905,\"start\":45904},{\"end\":45914,\"start\":45911},{\"end\":45927,\"start\":45920},{\"end\":45937,\"start\":45934},{\"end\":45949,\"start\":45945},{\"end\":46236,\"start\":46230},{\"end\":46253,\"start\":46245},{\"end\":46266,\"start\":46258},{\"end\":46283,\"start\":46276},{\"end\":46559,\"start\":46553},{\"end\":46583,\"start\":46581},{\"end\":46592,\"start\":46588},{\"end\":46613,\"start\":46610},{\"end\":46630,\"start\":46623},{\"end\":46971,\"start\":46968},{\"end\":46986,\"start\":46977},{\"end\":46994,\"start\":46991},{\"end\":47002,\"start\":47000},{\"end\":47371,\"start\":47368},{\"end\":47386,\"start\":47377},{\"end\":47399,\"start\":47391},{\"end\":47408,\"start\":47405},{\"end\":47416,\"start\":47414},{\"end\":47788,\"start\":47780},{\"end\":47802,\"start\":47795},{\"end\":47815,\"start\":47810},{\"end\":48109,\"start\":48102},{\"end\":48123,\"start\":48116},{\"end\":48137,\"start\":48130},{\"end\":48407,\"start\":48402},{\"end\":48424,\"start\":48417},{\"end\":48431,\"start\":48425},{\"end\":48443,\"start\":48439},{\"end\":48459,\"start\":48451},{\"end\":48710,\"start\":48705},{\"end\":48726,\"start\":48719},{\"end\":48741,\"start\":48736},{\"end\":49171,\"start\":49167},{\"end\":49184,\"start\":49178},{\"end\":49202,\"start\":49201},{\"end\":49221,\"start\":49213},{\"end\":49237,\"start\":49230},{\"end\":49248,\"start\":49245},{\"end\":49642,\"start\":49636},{\"end\":49653,\"start\":49649},{\"end\":49667,\"start\":49659},{\"end\":49970,\"start\":49967},{\"end\":49989,\"start\":49981},{\"end\":50292,\"start\":50286},{\"end\":50316,\"start\":50310},{\"end\":50330,\"start\":50326},{\"end\":50666,\"start\":50660},{\"end\":50684,\"start\":50677},{\"end\":50699,\"start\":50695},{\"end\":50972,\"start\":50968},{\"end\":50989,\"start\":50983},{\"end\":51004,\"start\":50998},{\"end\":51020,\"start\":51013},{\"end\":51036,\"start\":51029},{\"end\":51372,\"start\":51368},{\"end\":51390,\"start\":51383},{\"end\":51405,\"start\":51399},{\"end\":51421,\"start\":51414},{\"end\":51436,\"start\":51430},{\"end\":51732,\"start\":51728},{\"end\":51751,\"start\":51744},{\"end\":51769,\"start\":51760},{\"end\":52090,\"start\":52084},{\"end\":52101,\"start\":52095},{\"end\":52112,\"start\":52109},{\"end\":52123,\"start\":52119},{\"end\":52538,\"start\":52535},{\"end\":52551,\"start\":52545},{\"end\":52562,\"start\":52556},{\"end\":52572,\"start\":52570},{\"end\":52586,\"start\":52579},{\"end\":52597,\"start\":52591},{\"end\":52608,\"start\":52604},{\"end\":52620,\"start\":52616},{\"end\":53285,\"start\":53277},{\"end\":53299,\"start\":53294},{\"end\":53314,\"start\":53309},{\"end\":53318,\"start\":53315},{\"end\":53854,\"start\":53847},{\"end\":53864,\"start\":53860},{\"end\":53879,\"start\":53871},{\"end\":54231,\"start\":54223},{\"end\":54248,\"start\":54239},{\"end\":54259,\"start\":54253},{\"end\":54274,\"start\":54266},{\"end\":54556,\"start\":54548},{\"end\":54571,\"start\":54563},{\"end\":54588,\"start\":54579},{\"end\":54599,\"start\":54593},{\"end\":54614,\"start\":54606},{\"end\":54900,\"start\":54896},{\"end\":54913,\"start\":54909},{\"end\":55170,\"start\":55164},{\"end\":55178,\"start\":55176},{\"end\":55190,\"start\":55184},{\"end\":55200,\"start\":55196},{\"end\":55482,\"start\":55477},{\"end\":55492,\"start\":55487},{\"end\":55502,\"start\":55499},{\"end\":55515,\"start\":55508},{\"end\":55531,\"start\":55522},{\"end\":55539,\"start\":55536},{\"end\":55827,\"start\":55822},{\"end\":55839,\"start\":55836},{\"end\":55854,\"start\":55847},{\"end\":55868,\"start\":55862},{\"end\":55881,\"start\":55877},{\"end\":55898,\"start\":55892},{\"end\":56266,\"start\":56260},{\"end\":56282,\"start\":56275},{\"end\":56297,\"start\":56289},{\"end\":56310,\"start\":56302},{\"end\":56601,\"start\":56594},{\"end\":56614,\"start\":56613},{\"end\":56630,\"start\":56623},{\"end\":56649,\"start\":56643},{\"end\":56930,\"start\":56922},{\"end\":56944,\"start\":56939},{\"end\":56961,\"start\":56955},{\"end\":57284,\"start\":57274},{\"end\":57298,\"start\":57289},{\"end\":57308,\"start\":57306},{\"end\":57322,\"start\":57316},{\"end\":57337,\"start\":57329},{\"end\":57596,\"start\":57595},{\"end\":57609,\"start\":57608},{\"end\":57622,\"start\":57621},{\"end\":57634,\"start\":57633},{\"end\":57644,\"start\":57643},{\"end\":57655,\"start\":57654},{\"end\":57665,\"start\":57664},{\"end\":57676,\"start\":57675},{\"end\":57692,\"start\":57691},{\"end\":57701,\"start\":57700},{\"end\":57712,\"start\":57711},{\"end\":57726,\"start\":57725},{\"end\":57736,\"start\":57735},{\"end\":57750,\"start\":57749},{\"end\":57761,\"start\":57760},{\"end\":57771,\"start\":57770},{\"end\":58170,\"start\":58166},{\"end\":58187,\"start\":58180},{\"end\":58205,\"start\":58196},{\"end\":58217,\"start\":58216},{\"end\":58234,\"start\":58227},{\"end\":58251,\"start\":58242},{\"end\":58271,\"start\":58263},{\"end\":58287,\"start\":58281},{\"end\":58302,\"start\":58296},{\"end\":58318,\"start\":58314},{\"end\":58330,\"start\":58327},{\"end\":58349,\"start\":58341},{\"end\":58353,\"start\":58350},{\"end\":58371,\"start\":58365},{\"end\":58386,\"start\":58381},{\"end\":58401,\"start\":58395},{\"end\":58424,\"start\":58417},{\"end\":58438,\"start\":58432},{\"end\":58452,\"start\":58446},{\"end\":58467,\"start\":58461},{\"end\":58469,\"start\":58468},{\"end\":58483,\"start\":58477},{\"end\":58505,\"start\":58496},{\"end\":58523,\"start\":58516},{\"end\":59107,\"start\":59103},{\"end\":59119,\"start\":59115},{\"end\":59134,\"start\":59128},{\"end\":59150,\"start\":59143},{\"end\":59416,\"start\":59412},{\"end\":59437,\"start\":59429},{\"end\":59597,\"start\":59592},{\"end\":59607,\"start\":59603},{\"end\":59621,\"start\":59613},{\"end\":59634,\"start\":59628}]", "bib_author_last_name": "[{\"end\":43960,\"start\":43958},{\"end\":43971,\"start\":43967},{\"end\":43986,\"start\":43980},{\"end\":44266,\"start\":44261},{\"end\":44294,\"start\":44286},{\"end\":44589,\"start\":44581},{\"end\":44607,\"start\":44601},{\"end\":44866,\"start\":44856},{\"end\":44886,\"start\":44873},{\"end\":44899,\"start\":44894},{\"end\":44908,\"start\":44906},{\"end\":44928,\"start\":44916},{\"end\":44943,\"start\":44938},{\"end\":44960,\"start\":44951},{\"end\":44975,\"start\":44969},{\"end\":45477,\"start\":45473},{\"end\":45487,\"start\":45483},{\"end\":45703,\"start\":45694},{\"end\":45719,\"start\":45711},{\"end\":45730,\"start\":45726},{\"end\":45909,\"start\":45906},{\"end\":45918,\"start\":45915},{\"end\":45932,\"start\":45928},{\"end\":45943,\"start\":45938},{\"end\":45952,\"start\":45950},{\"end\":45956,\"start\":45954},{\"end\":46243,\"start\":46237},{\"end\":46256,\"start\":46254},{\"end\":46274,\"start\":46267},{\"end\":46288,\"start\":46284},{\"end\":46579,\"start\":46560},{\"end\":46586,\"start\":46584},{\"end\":46608,\"start\":46593},{\"end\":46621,\"start\":46614},{\"end\":46641,\"start\":46631},{\"end\":46975,\"start\":46972},{\"end\":46989,\"start\":46987},{\"end\":46998,\"start\":46995},{\"end\":47008,\"start\":47003},{\"end\":47375,\"start\":47372},{\"end\":47389,\"start\":47387},{\"end\":47403,\"start\":47400},{\"end\":47412,\"start\":47409},{\"end\":47422,\"start\":47417},{\"end\":47793,\"start\":47789},{\"end\":47808,\"start\":47803},{\"end\":47822,\"start\":47816},{\"end\":48114,\"start\":48110},{\"end\":48128,\"start\":48124},{\"end\":48143,\"start\":48138},{\"end\":48415,\"start\":48408},{\"end\":48437,\"start\":48432},{\"end\":48449,\"start\":48444},{\"end\":48465,\"start\":48460},{\"end\":48717,\"start\":48711},{\"end\":48734,\"start\":48727},{\"end\":48747,\"start\":48742},{\"end\":49176,\"start\":49172},{\"end\":49199,\"start\":49185},{\"end\":49211,\"start\":49203},{\"end\":49228,\"start\":49222},{\"end\":49243,\"start\":49238},{\"end\":49254,\"start\":49249},{\"end\":49261,\"start\":49256},{\"end\":49647,\"start\":49643},{\"end\":49657,\"start\":49654},{\"end\":49671,\"start\":49668},{\"end\":49979,\"start\":49971},{\"end\":49992,\"start\":49990},{\"end\":50308,\"start\":50293},{\"end\":50324,\"start\":50317},{\"end\":50339,\"start\":50331},{\"end\":50675,\"start\":50667},{\"end\":50693,\"start\":50685},{\"end\":50706,\"start\":50700},{\"end\":50981,\"start\":50973},{\"end\":50996,\"start\":50990},{\"end\":51011,\"start\":51005},{\"end\":51027,\"start\":51021},{\"end\":51043,\"start\":51037},{\"end\":51381,\"start\":51373},{\"end\":51397,\"start\":51391},{\"end\":51412,\"start\":51406},{\"end\":51428,\"start\":51422},{\"end\":51443,\"start\":51437},{\"end\":51742,\"start\":51733},{\"end\":51758,\"start\":51752},{\"end\":51777,\"start\":51770},{\"end\":52093,\"start\":52091},{\"end\":52107,\"start\":52102},{\"end\":52117,\"start\":52113},{\"end\":52126,\"start\":52124},{\"end\":52543,\"start\":52539},{\"end\":52554,\"start\":52552},{\"end\":52568,\"start\":52563},{\"end\":52577,\"start\":52573},{\"end\":52589,\"start\":52587},{\"end\":52602,\"start\":52598},{\"end\":52614,\"start\":52609},{\"end\":52626,\"start\":52621},{\"end\":53292,\"start\":53286},{\"end\":53307,\"start\":53300},{\"end\":53322,\"start\":53319},{\"end\":53327,\"start\":53324},{\"end\":53858,\"start\":53855},{\"end\":53869,\"start\":53865},{\"end\":53884,\"start\":53880},{\"end\":54237,\"start\":54232},{\"end\":54251,\"start\":54249},{\"end\":54264,\"start\":54260},{\"end\":54279,\"start\":54275},{\"end\":54561,\"start\":54557},{\"end\":54577,\"start\":54572},{\"end\":54591,\"start\":54589},{\"end\":54604,\"start\":54600},{\"end\":54619,\"start\":54615},{\"end\":54907,\"start\":54901},{\"end\":54920,\"start\":54914},{\"end\":55174,\"start\":55171},{\"end\":55182,\"start\":55179},{\"end\":55194,\"start\":55191},{\"end\":55205,\"start\":55201},{\"end\":55485,\"start\":55483},{\"end\":55497,\"start\":55493},{\"end\":55506,\"start\":55503},{\"end\":55520,\"start\":55516},{\"end\":55534,\"start\":55532},{\"end\":55543,\"start\":55540},{\"end\":55834,\"start\":55828},{\"end\":55845,\"start\":55840},{\"end\":55860,\"start\":55855},{\"end\":55875,\"start\":55869},{\"end\":55890,\"start\":55882},{\"end\":55909,\"start\":55899},{\"end\":56273,\"start\":56267},{\"end\":56287,\"start\":56283},{\"end\":56300,\"start\":56298},{\"end\":56318,\"start\":56311},{\"end\":56611,\"start\":56602},{\"end\":56621,\"start\":56615},{\"end\":56641,\"start\":56631},{\"end\":56655,\"start\":56650},{\"end\":56664,\"start\":56657},{\"end\":56937,\"start\":56931},{\"end\":56953,\"start\":56945},{\"end\":56967,\"start\":56962},{\"end\":57287,\"start\":57285},{\"end\":57304,\"start\":57299},{\"end\":57314,\"start\":57309},{\"end\":57327,\"start\":57323},{\"end\":57344,\"start\":57338},{\"end\":57606,\"start\":57597},{\"end\":57619,\"start\":57610},{\"end\":57631,\"start\":57623},{\"end\":57641,\"start\":57635},{\"end\":57652,\"start\":57645},{\"end\":57662,\"start\":57656},{\"end\":57673,\"start\":57666},{\"end\":57689,\"start\":57677},{\"end\":57698,\"start\":57693},{\"end\":57709,\"start\":57702},{\"end\":57723,\"start\":57713},{\"end\":57733,\"start\":57727},{\"end\":57747,\"start\":57737},{\"end\":57758,\"start\":57751},{\"end\":57768,\"start\":57762},{\"end\":57781,\"start\":57772},{\"end\":58178,\"start\":58171},{\"end\":58194,\"start\":58188},{\"end\":58214,\"start\":58206},{\"end\":58225,\"start\":58218},{\"end\":58240,\"start\":58235},{\"end\":58261,\"start\":58252},{\"end\":58279,\"start\":58272},{\"end\":58294,\"start\":58288},{\"end\":58312,\"start\":58303},{\"end\":58325,\"start\":58319},{\"end\":58339,\"start\":58331},{\"end\":58363,\"start\":58354},{\"end\":58379,\"start\":58372},{\"end\":58393,\"start\":58387},{\"end\":58415,\"start\":58402},{\"end\":58430,\"start\":58425},{\"end\":58444,\"start\":58439},{\"end\":58459,\"start\":58453},{\"end\":58475,\"start\":58470},{\"end\":58494,\"start\":58484},{\"end\":58514,\"start\":58506},{\"end\":58528,\"start\":58524},{\"end\":58535,\"start\":58530},{\"end\":59113,\"start\":59108},{\"end\":59126,\"start\":59120},{\"end\":59141,\"start\":59135},{\"end\":59158,\"start\":59151},{\"end\":59427,\"start\":59417},{\"end\":59444,\"start\":59438},{\"end\":59601,\"start\":59598},{\"end\":59611,\"start\":59608},{\"end\":59626,\"start\":59622},{\"end\":59639,\"start\":59635}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":219955663},\"end\":44198,\"start\":43907},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":231979499},\"end\":44523,\"start\":44200},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":234357997},\"end\":44821,\"start\":44525},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1033682},\"end\":45411,\"start\":44823},{\"attributes\":{\"doi\":\"arXiv:2106.00132\",\"id\":\"b4\"},\"end\":45636,\"start\":45413},{\"attributes\":{\"doi\":\"arXiv:2104.02600\",\"id\":\"b5\"},\"end\":45898,\"start\":45638},{\"attributes\":{\"doi\":\"arXiv:2108.11514\",\"id\":\"b6\"},\"end\":46160,\"start\":45900},{\"attributes\":{\"doi\":\"arXiv:2106.03802\",\"id\":\"b7\"},\"end\":46491,\"start\":46162},{\"attributes\":{\"doi\":\"arXiv:2105.14080\",\"id\":\"b8\"},\"end\":46865,\"start\":46493},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":246016304},\"end\":47277,\"start\":46867},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":249674668},\"end\":47705,\"start\":47279},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":245144350},\"end\":48063,\"start\":47707},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":222140788},\"end\":48350,\"start\":48065},{\"attributes\":{\"id\":\"b13\"},\"end\":48666,\"start\":48352},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":23206414},\"end\":49091,\"start\":48668},{\"attributes\":{\"id\":\"b15\"},\"end\":49557,\"start\":49093},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":254129046},\"end\":49901,\"start\":49559},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":246442182},\"end\":50204,\"start\":49903},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16895865},\"end\":50613,\"start\":50206},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2057420},\"end\":50932,\"start\":50615},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16046818},\"end\":51290,\"start\":50934},{\"attributes\":{\"doi\":\"arXiv:1506.02438\",\"id\":\"b21\"},\"end\":51671,\"start\":51292},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3345317},\"end\":52015,\"start\":51673},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3439214},\"end\":52438,\"start\":52017},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3331356},\"end\":53165,\"start\":52440},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":139103260},\"end\":53756,\"start\":53167},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207852331},\"end\":54133,\"start\":53758},{\"attributes\":{\"doi\":\"arXiv:2202.09671\",\"id\":\"b27\"},\"end\":54501,\"start\":54135},{\"attributes\":{\"doi\":\"arXiv:2206.02262\",\"id\":\"b28\"},\"end\":54811,\"start\":54503},{\"attributes\":{\"doi\":\"arXiv:2101.02388\",\"id\":\"b29\"},\"end\":55102,\"start\":54813},{\"attributes\":{\"doi\":\"arXiv:2202.09778\",\"id\":\"b30\"},\"end\":55382,\"start\":55104},{\"attributes\":{\"doi\":\"arXiv:2206.00927\",\"id\":\"b31\"},\"end\":55778,\"start\":55384},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13928442},\"end\":56171,\"start\":55780},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":246823323},\"end\":56590,\"start\":56173},{\"attributes\":{\"doi\":\"arXiv:1801.01401\",\"id\":\"b34\"},\"end\":56855,\"start\":56592},{\"attributes\":{\"doi\":\"arXiv:2301.07969\",\"id\":\"b35\"},\"end\":57155,\"start\":56857},{\"attributes\":{\"id\":\"b36\"},\"end\":57552,\"start\":57157},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":10659969},\"end\":58133,\"start\":57554},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":233238471},\"end\":59044,\"start\":58135},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14542261},\"end\":59355,\"start\":59046},{\"attributes\":{\"id\":\"b40\"},\"end\":59547,\"start\":59357},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":459456},\"end\":59952,\"start\":59549}]", "bib_title": "[{\"end\":43947,\"start\":43907},{\"end\":44249,\"start\":44200},{\"end\":44570,\"start\":44525},{\"end\":44850,\"start\":44823},{\"end\":46966,\"start\":46867},{\"end\":47366,\"start\":47279},{\"end\":47778,\"start\":47707},{\"end\":48100,\"start\":48065},{\"end\":48400,\"start\":48352},{\"end\":48703,\"start\":48668},{\"end\":49165,\"start\":49093},{\"end\":49634,\"start\":49559},{\"end\":49965,\"start\":49903},{\"end\":50284,\"start\":50206},{\"end\":50658,\"start\":50615},{\"end\":50966,\"start\":50934},{\"end\":51726,\"start\":51673},{\"end\":52082,\"start\":52017},{\"end\":52533,\"start\":52440},{\"end\":53275,\"start\":53167},{\"end\":53845,\"start\":53758},{\"end\":55820,\"start\":55780},{\"end\":56258,\"start\":56173},{\"end\":57593,\"start\":57554},{\"end\":58164,\"start\":58135},{\"end\":59101,\"start\":59046},{\"end\":59590,\"start\":59549}]", "bib_author": "[{\"end\":43962,\"start\":43949},{\"end\":43973,\"start\":43962},{\"end\":43988,\"start\":43973},{\"end\":44268,\"start\":44251},{\"end\":44277,\"start\":44268},{\"end\":44296,\"start\":44277},{\"end\":44591,\"start\":44572},{\"end\":44609,\"start\":44591},{\"end\":44868,\"start\":44852},{\"end\":44888,\"start\":44868},{\"end\":44901,\"start\":44888},{\"end\":44910,\"start\":44901},{\"end\":44930,\"start\":44910},{\"end\":44945,\"start\":44930},{\"end\":44962,\"start\":44945},{\"end\":44977,\"start\":44962},{\"end\":45479,\"start\":45465},{\"end\":45489,\"start\":45479},{\"end\":45705,\"start\":45688},{\"end\":45721,\"start\":45705},{\"end\":45732,\"start\":45721},{\"end\":45911,\"start\":45902},{\"end\":45920,\"start\":45911},{\"end\":45934,\"start\":45920},{\"end\":45945,\"start\":45934},{\"end\":45954,\"start\":45945},{\"end\":45958,\"start\":45954},{\"end\":46245,\"start\":46230},{\"end\":46258,\"start\":46245},{\"end\":46276,\"start\":46258},{\"end\":46290,\"start\":46276},{\"end\":46581,\"start\":46553},{\"end\":46588,\"start\":46581},{\"end\":46610,\"start\":46588},{\"end\":46623,\"start\":46610},{\"end\":46643,\"start\":46623},{\"end\":46977,\"start\":46968},{\"end\":46991,\"start\":46977},{\"end\":47000,\"start\":46991},{\"end\":47010,\"start\":47000},{\"end\":47377,\"start\":47368},{\"end\":47391,\"start\":47377},{\"end\":47405,\"start\":47391},{\"end\":47414,\"start\":47405},{\"end\":47424,\"start\":47414},{\"end\":47795,\"start\":47780},{\"end\":47810,\"start\":47795},{\"end\":47824,\"start\":47810},{\"end\":48116,\"start\":48102},{\"end\":48130,\"start\":48116},{\"end\":48145,\"start\":48130},{\"end\":48417,\"start\":48402},{\"end\":48439,\"start\":48417},{\"end\":48451,\"start\":48439},{\"end\":48467,\"start\":48451},{\"end\":48719,\"start\":48705},{\"end\":48736,\"start\":48719},{\"end\":48749,\"start\":48736},{\"end\":49178,\"start\":49167},{\"end\":49201,\"start\":49178},{\"end\":49213,\"start\":49201},{\"end\":49230,\"start\":49213},{\"end\":49245,\"start\":49230},{\"end\":49256,\"start\":49245},{\"end\":49263,\"start\":49256},{\"end\":49649,\"start\":49636},{\"end\":49659,\"start\":49649},{\"end\":49673,\"start\":49659},{\"end\":49981,\"start\":49967},{\"end\":49994,\"start\":49981},{\"end\":50310,\"start\":50286},{\"end\":50326,\"start\":50310},{\"end\":50341,\"start\":50326},{\"end\":50677,\"start\":50660},{\"end\":50695,\"start\":50677},{\"end\":50708,\"start\":50695},{\"end\":50983,\"start\":50968},{\"end\":50998,\"start\":50983},{\"end\":51013,\"start\":50998},{\"end\":51029,\"start\":51013},{\"end\":51045,\"start\":51029},{\"end\":51383,\"start\":51368},{\"end\":51399,\"start\":51383},{\"end\":51414,\"start\":51399},{\"end\":51430,\"start\":51414},{\"end\":51445,\"start\":51430},{\"end\":51744,\"start\":51728},{\"end\":51760,\"start\":51744},{\"end\":51779,\"start\":51760},{\"end\":52095,\"start\":52084},{\"end\":52109,\"start\":52095},{\"end\":52119,\"start\":52109},{\"end\":52128,\"start\":52119},{\"end\":52545,\"start\":52535},{\"end\":52556,\"start\":52545},{\"end\":52570,\"start\":52556},{\"end\":52579,\"start\":52570},{\"end\":52591,\"start\":52579},{\"end\":52604,\"start\":52591},{\"end\":52616,\"start\":52604},{\"end\":52628,\"start\":52616},{\"end\":53294,\"start\":53277},{\"end\":53309,\"start\":53294},{\"end\":53324,\"start\":53309},{\"end\":53329,\"start\":53324},{\"end\":53860,\"start\":53847},{\"end\":53871,\"start\":53860},{\"end\":53886,\"start\":53871},{\"end\":54239,\"start\":54223},{\"end\":54253,\"start\":54239},{\"end\":54266,\"start\":54253},{\"end\":54281,\"start\":54266},{\"end\":54563,\"start\":54548},{\"end\":54579,\"start\":54563},{\"end\":54593,\"start\":54579},{\"end\":54606,\"start\":54593},{\"end\":54621,\"start\":54606},{\"end\":54909,\"start\":54896},{\"end\":54922,\"start\":54909},{\"end\":55176,\"start\":55164},{\"end\":55184,\"start\":55176},{\"end\":55196,\"start\":55184},{\"end\":55207,\"start\":55196},{\"end\":55487,\"start\":55477},{\"end\":55499,\"start\":55487},{\"end\":55508,\"start\":55499},{\"end\":55522,\"start\":55508},{\"end\":55536,\"start\":55522},{\"end\":55545,\"start\":55536},{\"end\":55836,\"start\":55822},{\"end\":55847,\"start\":55836},{\"end\":55862,\"start\":55847},{\"end\":55877,\"start\":55862},{\"end\":55892,\"start\":55877},{\"end\":55911,\"start\":55892},{\"end\":56275,\"start\":56260},{\"end\":56289,\"start\":56275},{\"end\":56302,\"start\":56289},{\"end\":56320,\"start\":56302},{\"end\":56613,\"start\":56594},{\"end\":56623,\"start\":56613},{\"end\":56643,\"start\":56623},{\"end\":56657,\"start\":56643},{\"end\":56666,\"start\":56657},{\"end\":56939,\"start\":56922},{\"end\":56955,\"start\":56939},{\"end\":56969,\"start\":56955},{\"end\":57289,\"start\":57274},{\"end\":57306,\"start\":57289},{\"end\":57316,\"start\":57306},{\"end\":57329,\"start\":57316},{\"end\":57346,\"start\":57329},{\"end\":57608,\"start\":57595},{\"end\":57621,\"start\":57608},{\"end\":57633,\"start\":57621},{\"end\":57643,\"start\":57633},{\"end\":57654,\"start\":57643},{\"end\":57664,\"start\":57654},{\"end\":57675,\"start\":57664},{\"end\":57691,\"start\":57675},{\"end\":57700,\"start\":57691},{\"end\":57711,\"start\":57700},{\"end\":57725,\"start\":57711},{\"end\":57735,\"start\":57725},{\"end\":57749,\"start\":57735},{\"end\":57760,\"start\":57749},{\"end\":57770,\"start\":57760},{\"end\":57783,\"start\":57770},{\"end\":58180,\"start\":58166},{\"end\":58196,\"start\":58180},{\"end\":58216,\"start\":58196},{\"end\":58227,\"start\":58216},{\"end\":58242,\"start\":58227},{\"end\":58263,\"start\":58242},{\"end\":58281,\"start\":58263},{\"end\":58296,\"start\":58281},{\"end\":58314,\"start\":58296},{\"end\":58327,\"start\":58314},{\"end\":58341,\"start\":58327},{\"end\":58365,\"start\":58341},{\"end\":58381,\"start\":58365},{\"end\":58395,\"start\":58381},{\"end\":58417,\"start\":58395},{\"end\":58432,\"start\":58417},{\"end\":58446,\"start\":58432},{\"end\":58461,\"start\":58446},{\"end\":58477,\"start\":58461},{\"end\":58496,\"start\":58477},{\"end\":58516,\"start\":58496},{\"end\":58530,\"start\":58516},{\"end\":58537,\"start\":58530},{\"end\":59115,\"start\":59103},{\"end\":59128,\"start\":59115},{\"end\":59143,\"start\":59128},{\"end\":59160,\"start\":59143},{\"end\":59429,\"start\":59412},{\"end\":59446,\"start\":59429},{\"end\":59603,\"start\":59592},{\"end\":59613,\"start\":59603},{\"end\":59628,\"start\":59613},{\"end\":59641,\"start\":59628}]", "bib_venue": "[{\"end\":48898,\"start\":48832},{\"end\":52237,\"start\":52191},{\"end\":52837,\"start\":52741},{\"end\":53478,\"start\":53412},{\"end\":59762,\"start\":59710},{\"end\":44037,\"start\":43988},{\"end\":44340,\"start\":44296},{\"end\":44658,\"start\":44609},{\"end\":45026,\"start\":44977},{\"end\":45463,\"start\":45413},{\"end\":45686,\"start\":45638},{\"end\":46228,\"start\":46162},{\"end\":46551,\"start\":46493},{\"end\":47062,\"start\":47010},{\"end\":47468,\"start\":47424},{\"end\":47876,\"start\":47824},{\"end\":48197,\"start\":48145},{\"end\":48495,\"start\":48467},{\"end\":48830,\"start\":48749},{\"end\":49315,\"start\":49263},{\"end\":49722,\"start\":49673},{\"end\":50046,\"start\":49994},{\"end\":50385,\"start\":50341},{\"end\":50752,\"start\":50708},{\"end\":51089,\"start\":51045},{\"end\":51366,\"start\":51292},{\"end\":51823,\"start\":51779},{\"end\":52189,\"start\":52128},{\"end\":52739,\"start\":52628},{\"end\":53410,\"start\":53329},{\"end\":53935,\"start\":53886},{\"end\":54221,\"start\":54135},{\"end\":54546,\"start\":54503},{\"end\":54894,\"start\":54813},{\"end\":55162,\"start\":55104},{\"end\":55475,\"start\":55384},{\"end\":55955,\"start\":55911},{\"end\":56372,\"start\":56320},{\"end\":56920,\"start\":56857},{\"end\":57272,\"start\":57157},{\"end\":57819,\"start\":57783},{\"end\":58573,\"start\":58537},{\"end\":59183,\"start\":59160},{\"end\":59410,\"start\":59357},{\"end\":59708,\"start\":59641}]"}}}, "year": 2023, "month": 12, "day": 17}