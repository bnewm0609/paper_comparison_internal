{"id": 233444201, "updated": "2023-10-06 04:29:15.169", "metadata": {"title": "Decoupled Dynamic Filter Networks", "authors": "[{\"first\":\"Jingkai\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Varun\",\"last\":\"Jampani\",\"middle\":[]},{\"first\":\"Zhixiong\",\"last\":\"Pi\",\"middle\":[]},{\"first\":\"Qiong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ming-Hsuan\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 4, "day": 29}, "abstract": "Convolution is one of the basic building blocks of CNN architectures. Despite its common use, standard convolution has two main shortcomings: Content-agnostic and Computation-heavy. Dynamic filters are content-adaptive, while further increasing the computational overhead. Depth-wise convolution is a lightweight variant, but it usually leads to a drop in CNN performance or requires a larger number of channels. In this work, we propose the Decoupled Dynamic Filter (DDF) that can simultaneously tackle both of these shortcomings. Inspired by recent advances in attention, DDF decouples a depth-wise dynamic filter into spatial and channel dynamic filters. This decomposition considerably reduces the number of parameters and limits computational costs to the same level as depth-wise convolution. Meanwhile, we observe a significant boost in performance when replacing standard convolution with DDF in classification networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the top-1 accuracy, while their computational costs are reduced by nearly half. Experiments on the detection and joint upsampling networks also demonstrate the superior performance of the DDF upsampling variant (DDF-Up) in comparison with standard convolution and specialized content-adaptive layers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.14107", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhouJPL021", "doi": "10.1109/cvpr46437.2021.00658"}}, "content": {"source": {"pdf_hash": "3afbc6386026fe2e694337d0316d3979df7911e5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.14107v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.14107", "status": "GREEN"}}, "grobid": {"id": "8dcbe493c26f88b797189e2ca6e49d8f60e129bc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3afbc6386026fe2e694337d0316d3979df7911e5.txt", "contents": "\nDecoupled Dynamic Filter Networks\n\n\nJingkai Zhou \nVarun Jampani \nGoogle Research\n\n\nZhixiong Pi \nQiong Liu \nSouth China University of Technology\n\n\nMing-Hsuan Yang \n\nUniversity of California at Merced\n\n\n\nHuazhong University of Science and Technology\n\n\n\nYonsei University\n\n\nDecoupled Dynamic Filter Networks\n\nConvolution is one of the basic building blocks of CNN architectures. Despite its common use, standard convolution has two main shortcomings: Content-agnostic and Computation-heavy. Dynamic filters are content-adaptive, while further increasing the computational overhead. Depth-wise convolution is a lightweight variant, but it usually leads to a drop in CNN performance or requires a larger number of channels. In this work, we propose the Decoupled Dynamic Filter (DDF) that can simultaneously tackle both of these shortcomings. Inspired by recent advances in attention, DDF decouples a depth-wise dynamic filter into spatial and channel dynamic filters. This decomposition considerably reduces the number of parameters and limits computational costs to the same level as depthwise convolution. Meanwhile, we observe a significant boost in performance when replacing standard convolution with DDF in classification networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the top-1 accuracy, while their computational costs are reduced by nearly half. Experiments on the detection and joint upsampling networks also demonstrate the superior performance of the DDF upsampling variant (DDF-Up) in comparison with standard convolution and specialized content-adaptive layers. The project page with code is available 1 .\n\nIntroduction\n\nConvolution is a fundamental building block of convolutional neural networks (CNNs) that have seen tremendous success in several computer vision tasks, such as image classification, semantic segmentation, pose estimation, to name a few. Thanks to its simple formulation and optimized implementations, convolution has become a de facto standard to propagate and integrate features across image pixels. In this work, we aim to alleviate two of its main shortcomings: Content-agnostic and Computation-heavy. Content-agnostic. Spatial-invariance is one of the prominent properties of a standard convolution. That is, convolu- Figure 1. Comparison between convolution, the dynamic filter, and DDF. Top: Convolution shares a static filter among pixels and samples. Medium: The dynamic filter generates one complete filter for each pixel via a separate branch. Bottom: DDF decouples the dynamic filter into spatial and channel ones. tion filters are shared across all the pixels in an image. Consider the sample road scene shown in Figure 1 (top). The convolution filters are shared across different regions such as buildings, cars, roads, etc. Given the varied nature of contents in a scene, a spatially shared filter may not be optimal to capture features across different image regions [52,42]. In addition, once a CNN is trained, the same convolution filters are used across different images (for instance images taken in daylight and at night). In short, standard convolution filters are content-agnostic and are shared across images and pixels, leading to sub-optimal feature learning. Several existing works [23,48,42,57,49,45,22,11] propose different types of content-adaptive (dynamic) filters for CNNs. However, these dynamic filters are either computeintensive [57,23], memory-intensive [42,22], or specialized processing units [11,48,49,45]. As a result, most of the existing dynamic filters can not completely replace standard convolution in CNNs and are usually used as a few layers of a CNN [49,45,42,22], or in tiny architecture [57,23], or in specific scenarios, like upsampling [48]. Computation-heavy. Despite the existence of highlyoptimized implementations, the computation complexity of standard convolution still increases considerably with the enlarge in the filter size or channel number. This poses a significant problem as convolution layers in modern CNNs have a large number of channels in the orders of hundreds or even thousands. Grouped or depth-wise convolutions are commonly used to reduce the computation complexity. However, these alternatives usually result in CNN performance drops when directly used as a drop-in replacement to standard convolution. To retain similar performance with depth-wise or grouped convolutions, we need to considerably increase the number of feature channels, leading to more memory consumption and access times.\n\nIn this work, we propose the Decoupled Dynamic Filter (DDF) that simultaneously addresses both the abovementioned shortcomings of the standard convolution layer. The full dynamic filter [57,23,49,45] uses a separate network branch to predict a complete convolution filter at each pixel. See Figure 1 (middle) for an illustration. We observe that this dynamic filtering is equivalent to applying attention on unfolded input features, as illustrated in Figure 3. Inspired by the recent advances in attention mechanisms that apply spatial and channel-wise attention [36,50], we propose a new variant of the dynamic filter where we decouple spatial and channel filters. In particular, we adopt separate attention-style branches that individually predict spatial and channel dynamic filters, which are then combined to form a filter at each pixel. See Figure 1 (bottom) for an illustration of DDF. We observe that this decoupling of the dynamic filter is efficient yet effective, making DDF to have similar computational costs as depth-wise convolution while achieving better performance against existing dynamic filters. This lightweight nature enables DDF to be directly inserted as a replacement of the standard convolution layer. Unlike several existing dynamic filtering layers, we can replace all k \u00d7 k (k > 1) convolutions in a CNN with DDF. We also propose a variant of DDF, called DDF-Up, that can be used as a specialized upsampling or jointupsampling layer.\n\nWe empirically validate the performance of DDF by drop-in replacing convolution layers in several classification networks with DDF. Experiments indicate that applying DDF consistently boosts the performance while reducing computational costs. In addition, we also demonstrate the superior upsampling performance of DDF-Up in object detection and joint upsampling networks. In summary, DDF and DDF-Up have the following favorable properties:\n\n\u2022 Content-adaptive. DDF provides spatially-varying filtering that makes filters adaptive to image contents.\n\n\u2022 Fast runtime. DDF has similar computational costs as depth-wise convolution, so its inference speed is faster than both standard convolution and dynamic filters. \u2022 Smaller memory footprint. DDF significantly reduces memory consumption of dynamic filters, making it possible to replace all standard convolution layers with DDF. \u2022 Consistent performance improvements. Replacing a standard convolution with DDF / DDF-Up results in consistent improvements and achieves the state-of-the-art performance across various networks and tasks.\n\n\nRelated Work\n\nLightweight convolutions. Given the prominence of convolutions in CNN architectures, several lightweight variants have been proposed for different purposes. Dilated convolutions [4,56] increase the receptive field of the filter without increasing parameters or computation complexity of the standard convolution. Several lightweight mobile networks [17,39,16] use depth-wise convolutions instead of standard ones, which separately convolve each channel.\n\nSimilarly, grouped convolutions [26] group input channels and convolve each group separately resulting in parameter and computation reduction. However, directly replacing a standard convolution with depth-wise or grouped convolutions usually leads to performance drops. One needs to widen the model to achieve competitive performance with these lightweight variants of convolution. In contrast, the proposed DDF layer can be directly used as a lightweight drop-in replacement to standard convolution layer.\n\nDynamic filters. For the dynamic filters, the filter neighborhoods and/or filter values are dynamically modified or predicted based on the input features. Some recent approaches dynamically adjust the filter neighborhoods by adaptive dilation factors [58], estimating the neighborhood sampling grid [8], or adapting the receptive fields [43]. Another kind of dynamic filters, more closely related to our work, adjusts or predicts filter values based on input features [55,59,5,23,48,42,57,49,45,22]. In particular, semidynamic filters, such as WeightNet [34], CondConv [55], DyNet [59], and DynamicConv [5], predict coefficients to combine several expert filters. The combined filter is still applied in a convolutional manner (spatially shared). CARAFE [48] proposes a dynamic layer for upsampling, where an additional network branch is used to predict a 2D filter at each pixel. However, these channel-wise shared 2D filters cannot encode channel-specific information. Several full dynamic filters [23,57,49,45] use separate network branches to predict a complete filter at each pixel. As illustrated in Figure 2 (middle) and briefly explained in the Introduction, these dynamic filters can only replace a few convolution layers or can only be used in small networks due to computational reasons. Specifically, adaptive convolutional  kernels [57] are only used in small networks. SOLOv2 [49] and CondInst [45] employ dynamic filters in the last few layers of the segmentation model. PAC [42] uses a fixed Gaussian kernel on adapting features to modify the standard convolution filter at each pixel, which is also impractical for large architectures due to high memory consumption. The proposed DDF is lightweight even compared with the standard convolution layer and thus can be used across all the layers even in large networks. Attention mechanisms. Inspired by the role of attention in human visual perception [21,38,7,50], several approaches [54,47,46,18,36,50] propose to use attention layers that dynamically enhance/suppress feature values with predicted attention maps. SMemVQA [54] generates question-guided spatial attention to capture the correspondence between individual words in the question and image regions. The residual attention network [47] adopts encoder-decoder branches to model spatial attention and refine features. VSGNet [46] leverages the spatial configuration of human-object pairs to model attention. Besides spatial attention, SENet [18] introduces the squeeze-andexcitation structure to encode channel-wise attention and reweights the feature channels. Subsequent methods combine spatial and channel-wise attention. BAM [36] uses spatial and channel-wise attention in parallel, whereas CBAM [50] sequentially applies spatial and channel-wise attention. In this work, we draw connections between dynamic filters and attention layers. Inspired by spatial and channel-wise attention, we propose DDF that uses decoupled spatial and channel dynamic filters.\n\n\nPreliminaries\n\nStandard convolution. Given an input feature representation F \u2208 R c\u00d7n with c channels and n pixels (n = h \u00d7 w, h and w are the width and height of the feature map); the standard convolution operation at i th pixel can be written as a linear combination of input features around i th pixel:\nF (.,i) = j\u2208\u2126(i) W [p i \u2212 p j ]F (.,j) + b,(1)\nwhere F (.,j) \u2208 R c denotes the feature vector at j th pixel;\nF \u2208 R c \u00d7n denotes output feature map with F (.,i) \u2208 R c denoting i th pixel output feature vector. \u2126(i) denotes the k \u00d7 k convolution window around i th pixel. W \u2208 R c \u00d7c\u00d7k\u00d7k is a k \u00d7 k convolution filter, W [p i \u2212 p j ] \u2208\nR c \u00d7c is the filter at position offset between i and j th pixels:\n[p i \u2212 p j ] \u2208 {(\u2212 (k\u22121) 2 , \u2212 (k\u22121) 2 ), (\u2212 (k\u22121) 2 , \u2212 (k\u22121) 2 + 1), ..., ( (k\u22121) 2 , (k\u22121)\n2 )} where p i denotes 2D pixel coordinates. b \u2208 R c denotes the bias vector. In standard convolution, the same filter W is shared across all pixels and filter weights are agnostic to input features. Dynamic filters. In contrast to standard convolution, dynamic filters leverage separate network branches to generate the filter at each pixel. The spatially-invariant filter W in Eq. 1 becomes the spatially-varying filter D i \u2208 R c \u00d7c\u00d7k\u00d7k in this case. The dynamic filters enable learning contentadaptive and flexible feature embeddings. However, predicting such a large number (nc ck 2 ) of pixel-wise filter values requires heavy side-networks, resulting in both compute and memory intensive network architectures. Thus, dynamic filters are usually only employed in either tiny networks [23,57] or can only replace a few standard convolution layers [49,45,42,22] in a CNN.\n\n\nDecoupled Dynamic Filter\n\nThe goal of this work is to design a filtering operation that is content-adaptive while being lighter-weight than a standard convolution. Realizing both the properties with a single filter is quite challenging. We accomplish this with our Decoupled Dynamic Filter (DDF), where the key technique is to decouple dynamic filters into spatial and channel ones. More formally, the DDF operation can be written as: Figure 3. Connection between dynamic filters and attention. The dynamic filter is similar to applying attention on the unfolded feature.\nF (r,i) = j\u2208\u2126(i) D sp i [p i \u2212 p j ]D ch r [p i \u2212 p j ]F (r,j) ,(2)\nwhere F (r,i) \u2208 R denotes the output feature value at the i th pixel and r th channel, F (r,j) \u2208 R denotes the input feature value at the j th pixel and r th channel. D sp \u2208 R n\u00d7k\u00d7k is the spatial dynamic filter with D sp i \u2208 R k\u00d7k denoting the filter at i th pixel. D ch \u2208 R c\u00d7k\u00d7k is the channel dynamic filter with D ch r \u2208 R k\u00d7k denoting the filter at r th channel. Figure 2(a) shows the illustration of DDF operation. We predict both channel and spatial dynamic filters from the input feature, using which we perform the above DDF operation (Eq. 2) to compute the output feature map. Comparing general dynamic filters (See Section 3) with DDF clearly indicates that DDF reduces the nc ck 2 sized dynamic filter into much smaller nk 2 spatial and ck 2 channel dynamic filters. In addition, we implement DDF operation in CUDA alleviating any need to save intermediate multiplied filters during network training and inference.\n\nDDF module. Based on DDF operation, we carefully design a DDF module that can act as a basic building block in CNNs. For that, we want the filter prediction branches to be lightweight as well in addition to the DDF operation itself. We notice the connection between dynamic filters and attention mechanisms, using which we design attention-style branches to predict spatial and channel filters. Figure 3 illustrates the connection between dynamic filters and attention. Applying dynamic filters on a feature map is equivalent to applying attention on unfolded features. That is, we unfold the F \u2208 c \u00d7 n feature map into F u \u2208 c \u00d7 n \u00d7 k 2 feature map where neighboring feature values are unfolded as separate channels. Applying dynamic filters on the original feature map F is the same as re-weighting the unfolded feature map F u using the generated filter tensor as attention.\n\nFollowing the recent advances in attention literature [36,50] that propose to use lightweight branches to predict spatial and channel-wise attention, we design two attentionstyle branches that can generate spatial and channel dynamic filters for DDF. Figure 2(b) illustrates the structure of spatial and channel filter branches in the DDF module. The spatial filter branch only contains one 1 \u00d7 1 convolution layer. The channel filter branch first applies the global average pooling to aggregate input features, then generates channel dynamic filters via a squeeze-and-excitation structure [18], where the squeeze ratio is denoted as \u03c3 \u2208 R + .\n\nAs generated filter values can be extremely large or small for some input features, directly using them for convolution will make the training unstable. So, we propose to do filter normalization (FN):\nD sp i = \u03b1 spD sp i \u2212 \u00b5(D sp i ) \u03b4(D sp i ) + \u03b2 sp D ch r = \u03b1 ch rD ch r \u2212 \u00b5(D ch r ) \u03b4(D ch r ) + \u03b2 ch r ,(3)\nwhereD sh i ,D ch r \u2208 R k\u00d7k are the generated spatial and channel filters before normalization, \u00b5(\u00b7) and \u03b4(\u00b7) calculate the mean and standard deviation of the filter, \u03b1 sp , \u03b1 ch r , \u03b2 sp , \u03b2 ch r are the running standard deviation and mean values which are similar to those coefficients in the batch normalization (BN) [20]. FN can limit generated filter values into a reasonable range, thereby avoiding the gradient vanishing/exploding during training. Table 1 shows the parameter, space and time complexity comparisons between standard convolution (Conv), Depthwise convolution (DwConv), full dynamic filters (DyFilter) [23,57,49,45], and our DDF filter. For analysis, we use the same notation as before -n : Number of pixels; c: Channel number; k : Filter size (spatial extent); \u03c3 : Squeeze ratio in DDF channel filter branch. For simplicity, we assume that both input and output features have c channels. We also assume that DyFilter adopts a lightweight filter prediction branch with a single 1 \u00d7 1 convolution layer. Number of parameters. The prediction branch of DyFilter takes c channel features as input and produces c 2 k 2 channel output, where each pixel output corresponds to a complete filter at that pixel. Thus, the DyFilter prediction branch has c 3 k 2 parameters, which is quite high even for small values of c. For DDF, the spatial filter branch predicts filter tensors with k 2 channels and thus contain ck 2 parameters. The channel filter branch has \u03c3c 2 parameters for the squeeze layer, and \u03c3c 2 k 2 parameters for the excitation layer. In total, DDF prediction branches contain ck 2 + \u03c3c 2 (1 + k 2 ) parameters, which is far fewer than those for DyFilter. Depending on the values of \u03c3, k, and c (usually set to 0.2, 3, and 256), the number of parameters for the DDF module can be even lower than a standard convolution layer. Time complexity. The spatial filter generation of DDF needs 2nck 2 floating-point operations (FLOPs), and the channel filter generation takes 2\u03c3c 2 (1 + k 2 ) FLOPs. The filter combination and application needs 3nck 2 FLOPs. In total, DDF needs 5nck 2 + 2\u03c3c 2 (1 + k 2 ) FLOPs with time complexity of O(nck 2 + c 2 k 2 ). The term c 2 k 2 can be ignored since n >> c, k. Thus, the time complexity of DDF  , which is similar to that of depth-wise convolution and better than a standard convolution with time complexity of O(nc 2 k 2 ). The time complexity of DyFilter is O(nc 3 k 2 ), with 2nc 3 k 2 FLOPs for filter generation and 2nc 2 k 2 FLOPs for filter application. Thus the time complexity of DyFilter is almost c 2 times higher than that of DDF, which is quite significant. Table 2 compares the inference time between four kinds of filters, where we adopt PAC [42] as the representative of dynamic filters. Refer to the supplementary for more latency comparisons on different input sizes. Space/Memory complexity. Table 1 also compares the space complexity of generated filters. Standard and depthwise convolutions do not generate content-adaptive filters. DyFilter generates a complete filter at each pixel with a space complexity of O(nc 2 k 2 ). DDF has a much smaller space complexity of O((n + c)k 2 ), since it only needs to store 2d spatial filters with nk 2 (shared by channels) and channel filters with ck 2 . See Table 2 for the comparison of the max allocated memory between four kinds of filters.\n\n\nComputational Complexity.\nc 3 k 2 ck 2 + \u03c3c 2 (1 + k 2 ) Time O(nc 2 k 2 ) O(nck 2 ) O(nc 3 k 2 ) O(nck 2 + c 2 k 2 ) Space - - O(nc 2 k 2 ) O((n + c)k 2 )\nIn summary, DDF has a time complexity that is similar to depth-wise convolution, which is considerably better than a standard convolution or dynamic filter. Remarkably, despite generating content-adaptive filters, the number of parameters in a DDF module is still smaller than that of a standard convolution layer. The space complexity of DDF can be hundreds or even thousands of times smaller than full dynamic filters, when c or n are in the orders of hundreds which is quite common in practice.\n\n\nDDF Networks for Image Classification\n\nImage classification is considered as a fundamental task in computer vision. To demonstrate the use of DDFs as basic building blocks in a CNN, we experiment with the widely used ResNet [15] architecture for image classification. ResNets stack multiple basic/bottleneck blocks in Figure 4. Structure of the DDF bottleneck block. We replace the 3 \u00d7 3 convolution layer with DDF and keep the original hyperparameters, especially using the same number of channels. which 3 \u00d7 3 convolution layers are adopted for spatial embedding. We substitute these 3 \u00d7 3 convolution layers in all stacked blocks with DDF. We refer to such a modified ResNet with DDF as 'DDF-ResNet'. Figure 4 illustrates the use of DDF in a ResNet bottleneck block, we refer to it as DDF bottleneck block.\n\nWe evaluate DDF-ResNets on the ImageNet dataset [10] with the Top-1 and Top-5 accuracy. DDF-ResNets are trained using the same training protocol as [27]. In particular, we train models for 120 epochs by the SGD optimizer with the momentum of 0.9 and the weight decay of 1e-4. The learning rate is set to 0.1 with batch size 256 and decays to 1e-5 following the cosine schedule. The input image is resized and center-cropped to 224 \u00d7 224. Ablation study. We comprehensively analyze the effect of different components in a DDF module. We choose ResNet50 [15] as our base network architecture and experiment with different modifications to DDF. Table 3 shows the results of ablation experiments. First, we analyze the effect of spatial and channel dynamic filters in DDF with classification accuracy. Table 3(a) shows there is a significant drop in performance when we replace convolutions with only spatial dynamic filters. This is expected as spatial dynamic filters are shared by all channels, thus cannot encode channel-specific information.\n\nBy replacing the convolution with the channel dynamic filters, the top-1 accuracy is improved by 1.6%. Using the full DDF module, with both spatial and channel dynamic filters, improves the top-1 accuracy by 1.9%. These results show the importance of both the spatial and channel dynamic filters in DDF. Table 3(b) compares different normalization schemes in a DDF module. Replacing the proposed filter normalization with a standard batch normalization [20] or a sigmoid activation leads to considerable drops in accuracy. Sigmoid activation individually processes each filter value and may not capture the correlation between them, while batch normalization considers all the filters in a batch, which may weaken the filter dynamics across samples.\n\nWe also evaluate DDF under different squeeze ratios \u03c3, which is used to control the feature channel compression in the channel filter branch. As shown in table 3(c), using higher squeeze ratios will significantly increase the number of parameters, while only bringing marginal performance improvements. Hence, we set the squeeze ratio to 0.2 by default. In addition, even the parameter number increases with enlarging the squeeze ratio, the FLOPs remain low because the computational costs of the channel filter branch are minimal, as analyzed in Section 4.1.\n\nComparisons with other dynamic filters. Next, we compare the use of DDF with respect to some existing dynamic filters using different ResNet base architectures: ResNet18 (R18) and ResNet50 (R50). Table 4 shows the parameters, FLOPs, and accuracy comparisons. Specifically, we compare DDF with adaptive convolutional kernel [57] (Adaptive), DyNet [59], Conditionally parameterized convolutions (CondConv/DwCondConv) [55], and depth-wise WeightNet (DwWeightNet) [34]. The 'Adaptive' [57] can only be used in R18 due to its large running memory consumption. Results show that using DDF consistently boosts the performance of base models, while also significantly reducing the number of parameters and FLOPs. It is worth noting that, DwWeightNet has worse performance than DDF, and even inferior to the channel-only DDF in Table 3(a), although it has a similar design as the channel-only DDF. This is due to the use of sigmoid activation during the filter generation in DwWeightNet (more analysis in the supplementary). Comparisons with state-of-the-art ResNet variants. We also compare DDF-ResNets with other state-of-the-art variants of ResNet50 and ResNet101 architectures in Table 5. Specifically, we compare with attention mechanisms of SE [18], BAM [36], CBAM [50] and AA [2]; and also block modifications of ResNeXt [53] and Res2Net [12]. Results clearly show that DDF-ResNets achieve the best performance while also having the lowest number of parameters and FLOPs. DDF-ResNet50 can be further improved by tricks in training and evaluation, and can achieve 81.3% top-1 accuracy. Refer to the supplementary for more details.\n\nRecently, neural architecture search (NAS) methods [44,33] can obtain architectures with outstanding speed/accuracy trade-off. The proposed DDF module can also contribute to the search space of NAS methods as a new fundamental building block.\n\n\nDDF as Upsampling Module\n\nAn advantage of dynamic filters compared with standard convolution is that one could predict dynamic filters from guidance features instead of input features. Following this, Figure 5. Structure of the DDF-Up module. When the upsampling scale factor is set to 2, the DDF-Up module contains 4 branches. For typical upsampling, the guidance feature is predicted from input features via a depth-wise convolution layer.  we propose an extension of the DDF module, where spatial dynamic filters are predicted using separate guidance features instead of input features. Such joint filtering with input and guidance features is useful for several tasks such as joint image upsampling [42,28,29], cross-modal image enhancement [51,9,6], texture removal [32] to name a few. Figure 5 illustrates the DDF Upsampling (DDF-Up) module, where the number of DDF operations used is set to x 2 for the upsampling factor x (e.g., 4 DDF operations when the upsampling factor is 2). We stack and pixel-shuffle [40] the resulting features from the DDF operations to form output features. For typical upsampling (without guidance), we use the same structure with a slight modification. We compute guidance features from input ones using a depth-wise convolution layer. DDF-Up can be seamlessly integrated into several existing CNNs, where typical/joint upsampling operators are needed. Here we present two applications in object detection and joint depth upsampling tasks.\n\nObject detection with DDF-Up. Detecting objects in an image is one of the core dense prediction tasks in computer vision. We adopt FasterRCNN [37] with the Feature Pyramid Network (FPN) [30] as our base detection architecture and embed DDF-Up modules into FPN. FPN is an effective U-net shaped feature fusion module, where the decoder pathway upsamples high-level features while combining low-level ones. As illustrated in Figure 6(a), we replace the nearest-neighbor upsampling modules in FPN with our DDF-Up modules.\n\nWe analyze the effectiveness of DDF-Up modules with experiments on the COCO detection benchmark [31] which contains 115K training and 5K validation images. We report standard COCO [31] metrics for small (mAp S ), medium (mAp M ), large (mAp L ), and all-scale (mAp) objects on the minival split. We implement our models based on the MMDetection [3] toolbox and train them using the standard training protocol therein. Specifically, we train different models for 12 epochs using the SGD optimizer with a momentum of 0.1 and the weight decay of 1e-4. We use a batch size of 16 and set the learning rate to 0.2 which decays by the factor of 0.1 at 8 and 11 th epochs. We resize the shorter side of the input image to 800 pixels, while keeping the longer side no larger than 1333 pixels.\n\nWe compare DDF-Up with the generic nearest-neighbor (Nearest) and bilinear (Bilinear) interpolations, as well as learnable Deconvolution (Deconv) [35], Pixel Shuffle (P.S.) [40], and CARAFE [48] upsampling modules. Table 6 exhibits the comparison results. FPN with DDF-Up yields a 1.2% mAp improvement over the baseline which adopts the nearest-neighbor interpolation. DDF-Up also brings obvious improvements against static-filtering upsampler (like Deconv and P.S.), and is on par with the recent dynamic upsampling technique CARAFE while utilizing only one-third of FLOPs as CARAFE. Joint depth upsampling with DDF-Up. We analyze the use of DDF-Up as a joint upsampling module by integrat-Input Guidance\n\nBilinear PAC-Net [42] DDF-Up-Net (Ours) GT Figure 7. 16\u00d7 joint depth sampling results on sample images. DDF-Up-Net recovers more depth details compared with PAC-Net [42] and other techniques.\n\ning it into a joint depth upsampling network. Here, the task is to upsample a low-resolution depth map given a higherresolution RGB image as guidance. This experiment allows to compare DDF-Up with content-adaptive filtering techniques such as Pixel-Adaptive Convolution (PAC) [42] which is a current state-of-the-art for this task. We use a similar network architecture to PAC-Net [42], where we employ our DDF-Up modules instead of PAC joint upsampling modules. We call the resulting network 'DDF-Up-Net'. Figure 6(b) illustrates DDF-Up-Net where we first encode low-resolution input features from the given depth map (X) and high-resolution guidance features (G) from RGB images. Then, we employ DDF-Up in the decoder to joint upsample depth features with guidance features and obtain high-resolution depth output (X up ). Each DDF-Up module does 2\u00d7 upsampling, we sequentially use k DDF-Up modules when the upsampling factor is 2 k .\n\nWe conduct experiments on the NYU depth V2 dataset [41] which has 1449 RGB-depth pairs. Following PAC-Net [42], we use the nearest-neighbor downsampling to generate low-resolution inputs from the groundtruth (GT) depth maps. We split the first 1000 samples for training and the rest for testing. We train DDF-Up-Net for 1500 epochs using the Adam optimizer [24]. We use a batch size of 8 and set the learning rate to 1e-4 which decays by the factor 0.1 at 1000 and 1350 th epochs. During training, the input images are resized and random-cropped to 256\u00d7256. Table 7 reports Root Mean Square Error (RMSE) scores of different techniques for three upsampling factors, i.e., 4\u00d7, 8\u00d7, and 16\u00d7. DDF-Up-Net performs better than state-of-the-art techniques across all the upsampling factors. It surpasses the standard CNN techniques like DJF [28] and DJF+ [29] by a large margin. It also improves over dynamic-filtering PAC [42] while reducing computational costs by an order of magnitude. See Table 2 for the cost comparison between PAC and DDF-Up. We visualize sampled 16\u00d7 upsampling results in Figure 7, where we can see that DDF-Up-Net recovers more details compared to PAC-Net and other techniques. \n\n\nConclusion\n\nIn this work, we propose a lightweight content-adaptive filtering technique called DDF, where our key strategy is to predict decoupled spatial and channel dynamic filters. We show that DDF can seamlessly replace standard convolution layers, consistently improving the performance of ResNets while also reducing model parameters and computational costs. In addition, we propose an upsampling variant called DDF-Up, which boosts performance as both a general upsampling module in detection and a joint upsampling module in joint depth upsampling. DDF-Up also is more computationally efficient compared with specialized content-adaptive layers. Overall, DDF has rich representative capabilities as a content-adaptive filter while also being computationally cheaper than a standard convolution, making it highly practical to use in modern CNNs.\n\n\nAcknowledgement\n\n\nDynamic Filter Module (DDF Module).\n\nFigure 2 .\n2Illustration of the DDF operation and the DDF module. The orange color denotes spatial dynamic filters / branch, and the green color denotes channel dynamic filters / branch. The filter application means applying the convolution operation at a single position. 'GAP' means the global average pooling and 'FC' denotes the fully connected layer.\n\n\nupsampling with DDF-Up.\n\nFigure 6 .\n6Applications of the DDF-Up module. DDF-Up can be seamlessly embedded into the top-down upsampling path in the FPN[30] network for object detection and the decoder part of a joint upsampling architecture.\n\nTable 1 .\n1Comparison of the parameter number and computational costs. 'Params' means the number of parameters, 'Time' represents the time complexity, 'Space' denotes the space complexity of generated filters.Filter \nConv \nDwConv DyFilter \nDDF \n\nParams c 2 k 2 \nck 2 \n\n\nTable 2 .\n2Comparison of the inference latency and the max allocated memory. The size of the input feature is set to 2 \u00d7 256 \u00d7 200 \u00d7 300, which is the common size of the P1 layer in FPN[30]. The guidance feature size of PAC is the same as the input one.Filter \nConv \nDwConv \nPAC \nDDF \n\nMemory 356.3M \n236.0M \n3406.4M 245.7M \nLatency \n7.5 ms \n1.0 ms \n46.4 ms \n3.0 ms \n\napproximately equals to O(nck 2 )\n\nTable 3 .\n3Ablation studies on the ImageNet dataset. We list the classification performance of different DDF-ResNet50 variants, where we use ResNet50 as the base network architecture.(a) Effect of spatial and channel filters in DDF. Spatial Channel Top-1 / Top-5 Acc.Base Model \n77.2 / 93.5 \n74.4 / 92.0 \n78.7 / 94.2 \n79.1 / 94.5 \n\n(b) Comparison of different normalization schemes. \n\nBatch-Norm Sigmoid Filter-Norm \nTop-1 Acc. \n76.0 \n78.2 \n79.1 \nTop-5 Acc. \n92.0 \n93.8 \n94.5 \n\n(c) Comparisons with different squeeze ratios \u03c3. \n\n\u03c3 \nParams FLOPs Top-1 / Top-5 Acc \n\n0.2 16.8M 2.298B \n79.1 / 94.5 \n0.3 18.1M 2.299B \n79.0 / 94.5 \n0.4 19.4M 2.300B \n79.2 / 94.5 \n\nTable 4. Comparison against related filters on the ImageNet \ndataset. '-' denotes the unreported value. \n\nArch Conv Type \nParams FLOPs Top-1 Acc \n\nR18 \n\nBase Model [15] \n11.7M \n1.8B \n69.6 \nAdaptive [57] \n11.1M \n-\n70.2 \nDyNet [59] \n16.6M \n0.6B \n69.0 \nDDF \n7.7M \n0.4B \n70.6 \n\nR50 \n\nBase Model [15] \n25.6M \n4.1B \n77.2 \nDyNet [59] \n-\n1.1B \n76.3 \nCondConv [55] \n104.8M \n4.2B \n78.6 \nDwCondConv [55] \n14.5M \n2.3B \n78.3 \nDwWeightNet [34] \n14.4M \n2.3B \n78.0 \nDDF \n16.8M \n2.3B \n79.1 \n\n\n\nTable 5 .\n5Comparisonwith state-of-the-art variants of ResNet50 \nand ResNet101 on the ImageNet dataset. Variants include at-\ntention mechanisms: SE, BAM, CBAM, AA; and block modifi-\ncations: ResNeXt, Res2Net, and our DDF. Besides official results \nfrom the respective work, we list re-trained results (in brackets) \nunder the same training protocol (that we use) as in [27]. \n\nMethod \nParams FLOPs Top-1 Acc \n\nResNet50 (base) [15] \n25.6M \n4.1B \n76.0 (77.2) \nSE-ResNet50 [18] \n28.1M \n4.1B \n77.6 (77.8) \nBAM-ResNet50 [36] \n25.9M \n4.2B \n76.0 \nCBAM-ResNet50 [50] \n28.1M \n4.1B \n77.3 \nAA-ResNet50 [2] \n25.8M \n4.2B \n77.7 \nResNeXt50 (32\u00d74d) [53] \n25.0M \n4.3B \n77.8 (78.2) \nRes2Net50 (14w-8s) [12] \n25.7M \n4.2B \n78.0 \nDDF-ResNet50 \n16.8M \n2.3B \n79.1 \n\nResNet101 (base) [15] \n44.5M \n7.8B \n77.6 (78.9) \nSE-ResNet101 [18] \n49.3M \n7.8B \n78.3 (79.3) \nBAM-ResNet101 [50] \n44.9M \n7.9B \n77.6 \nCBAM-ResNet101 [50] \n49.3M \n7.8B \n78.5 \nAA-ResNet101 [2] \n45.4M \n8.1B \n78.7 \nResNeXt101 (32\u00d74d) [53] 44.2M \n8.0B \n78.8 (79.5) \nRes2Net101 (26w-4s) [12] \n45.2M \n8.1B \n79.2 \nDDF-ResNet101 \n28.1M \n4.1B \n80.2 \n\n\n\nTable 6 .\n6Comparison of different upsampling modules in FPN[30] on the COCO minival split. We show FLOPs (for upsampling modules) and mAp scores on small (mApS), medium (mApM ), large (mApL), and all-scale (mAp) objects.Method \nFLOPs mApS mApM mApL mAp \n\nNearest (base) \n0.00B \n21.2 \n41.0 \n48.1 \n37.4 \nBilinear \n0.02B \n22.1 \n41.2 \n48.4 \n37.6 \nDeconv [35] \n12.57B \n21.0 \n41.1 \n48.5 \n37.3 \nP.S. [40] \n50.18B \n21.4 \n41.5 \n48.6 \n37.7 \nCARAFE [48] \n2.14B \n22.6 \n42.0 \n49.8 \n38.5 \nDDF-Up \n0.58B \n22.1 \n42.4 \n49.9 \n38.6 \n\n\n\nTable 7 .\n7Joint depth upsampling results on the NYU Depth V2 dataset. We exhibit RMSE results (in the order of 10 \u22122 , lower is better) of different techniques and different upsampling factors. Ham et al. [13] 5.27 12.31 19.24Method \n4\u00d7 \n8\u00d7 \n16\u00d7 \n\nBicubic \n8.16 14.22 22.32 \nMRF (32\u00d74d) \n7.84 13.98 22.20 \nGF [14] \n7.32 13.62 22.03 \nFBS [1] \n4.29 \n8.94 \n14.59 \nJBU [25] \n4.07 \n8.29 \n13.35 \nDMSG [19] \n3.78 \n6.37 \n11.16 \nDJF [28] \n3.54 \n6.20 \n10.21 \nDJF+ [29] \n3.38 \n5.86 \n10.11 \nPAC-Net [42] \n2.39 \n4.59 \n8.09 \nDDF-Up-Net \n2.16 \n4.40 \n7.72 \n\n\n\nThe fast bilateral solver. T Jonathan, Ben Barron, Poole, ECCV. Jonathan T Barron and Ben Poole. The fast bilateral solver. In ECCV, 2016. 8\n\nAttention augmented convolutional networks. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V Le, ICCV. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional net- works. In ICCV, 2019. 6\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, arXiv:1906.07155Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprintKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian- heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019. 7\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE Transactions on Pattern Analysis and Machine Intelligence. 404Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834-848, 2017. 2\n\nDynamic convolution: Attention over convolution kernels. Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu, CVPR. 2020Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: At- tention over convolution kernels. In CVPR, 2020. 2\n\nJae Shin Yoon, Kyounghwan An, and In So Kweon. Kaist multi-spectral day/night data set for autonomous and assisted driving. Yukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park, IEEE Transactions on Intelligent Transportation Systems. 7Yukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park, Jae Shin Yoon, Kyounghwan An, and In So Kweon. Kaist multi-spectral day/night data set for autonomous and assisted driving. IEEE Transactions on Intelligent Transportation Systems, 2018. 7\n\nControl of goaldirected and stimulus-driven attention in the brain. Maurizio Corbetta, Gordon L Shulman, Nature Reviews Neuroscience. 3Maurizio Corbetta and Gordon L Shulman. Control of goal- directed and stimulus-driven attention in the brain. Nature Reviews Neuroscience, 2002. 3\n\nDeformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, ICCV. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017. 2\n\nCross-modality person re-identification with generative adversarial training. Pingyang Dai, Rongrong Ji, Haibin Wang, Qiong Wu, Yuyu Huang, In IJCAI. 7Pingyang Dai, Rongrong Ji, Haibin Wang, Qiong Wu, and Yuyu Huang. Cross-modality person re-identification with generative adversarial training. In IJCAI, 2018. 7\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5\n\nSuperpixel convolutional networks using bilateral inceptions. Raghudeep Gadde, Varun Jampani, Martin Kiefel, Daniel Kappler, Peter V Gehler, ECCV. Raghudeep Gadde, Varun Jampani, Martin Kiefel, Daniel Kappler, and Peter V Gehler. Superpixel convolutional net- works using bilateral inceptions. In ECCV, 2016. 1\n\nRes2net: A new multi-scale backbone architecture. Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip Hs Torr, IEEE Transactions on Pattern Analysis and Machine Intelligence. 6Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip HS Torr. Res2net: A new multi-scale backbone architecture. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 6\n\nRobust image filtering using joint static and dynamic guidance. Bumsub Ham, Minsu Cho, Jean Ponce, CVPR. Bumsub Ham, Minsu Cho, and Jean Ponce. Robust image filtering using joint static and dynamic guidance. In CVPR, 2015. 8\n\nGuided image filtering. Kaiming He, Jian Sun, Xiaoou Tang, ECCV. Kaiming He, Jian Sun, and Xiaoou Tang. Guided image fil- tering. In ECCV, 2010. 8\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 56Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5, 6\n\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, ICCV. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mo- bilenetv3. In ICCV, 2019. 2\n\nG Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprintAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efficient convolu- tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 2\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, CVPR. 6Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In CVPR, 2018. 3, 4, 6\n\nDepth map super-resolution by deep multi-scale guidance. Tak-Wai Hui, Chen Change Loy, Xiaoou Tang, ECCV. Tak-Wai Hui, Chen Change Loy, and Xiaoou Tang. Depth map super-resolution by deep multi-scale guidance. In ECCV, 2016. 8\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. 45Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. ICML, 2015. 4, 5\n\nA model of saliency-based visual attention for rapid scene analysis. Laurent Itti, Christof Koch, Ernst Niebur, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 1998. 3\n\nLearning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks. Varun Jampani, Martin Kiefel, Peter V Gehler, CVPR. 13Varun Jampani, Martin Kiefel, and Peter V Gehler. Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks. In CVPR, pages 4452-4461, 2016. 1, 2, 3\n\nDynamic filter networks. Xu Jia, Bert De Brabandere, Tinne Tuytelaars, Luc V Gool, NeurIPS. Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In NeurIPS, 2016. 1, 2, 3, 4\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. 8Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015. 8\n\nJoint bilateral upsampling. Johannes Kopf, F Michael, Dani Cohen, Matt Lischinski, Uyttendaele, ACM Transactions on Graphics. 8Johannes Kopf, Michael F Cohen, Dani Lischinski, and Matt Uyttendaele. Joint bilateral upsampling. ACM Transactions on Graphics, 2007. 8\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NeurIPS. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, 2012. 2\n\nPsconv: Squeezing feature pyramid into one compact poly-scale convolutional layer. Duo Li, Anbang Yao, Qifeng Chen, ECCV. 56Duo Li, Anbang Yao, and Qifeng Chen. Psconv: Squeezing feature pyramid into one compact poly-scale convolutional layer. In ECCV, 2020. 5, 6\n\nDeep joint image filtering. Yijun Li, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, ECCV. 7Yijun Li, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Deep joint image filtering. In ECCV, 2016. 7, 8\n\nJoint image filtering with deep convolutional networks. Yijun Li, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 78Yijun Li, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Joint image filtering with deep convolutional net- works. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 7, 8\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. 57Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 5, 7\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 7\n\nTexture-consistent shadow removal. Feng Liu, Michael Gleicher, ECCV. Feng Liu and Michael Gleicher. Texture-consistent shadow removal. In ECCV, 2008. 7\n\nDarts: Differentiable architecture search. ICLR. Hanxiao Liu, Karen Simonyan, Yiming Yang, Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. ICLR, 2019. 6\n\nWeightnet: Revisiting the design space of weight networks. Ningning Ma, Xiangyu Zhang, Jiawei Huang, Jian Sun, ECCV, 2020. 26Ningning Ma, Xiangyu Zhang, Jiawei Huang, and Jian Sun. Weightnet: Revisiting the design space of weight networks. In ECCV, 2020. 2, 6\n\nLearning deconvolution network for semantic segmentation. Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, ICCV. Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015. 7\n\nJoon-Young Lee, and In So Kweon. Bam: Bottleneck attention module. Jongchan Park, Sanghyun Woo, BMVC. Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Bam: Bottleneck attention module. BMVC, 2018. 2, 3, 4, 6\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, NeurIPS. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015. 7\n\nThe dynamic representation of scenes. Ronald A Rensink, Visual Cognition. 3Ronald A. Rensink. The dynamic representation of scenes. Visual Cognition, 2000. 3\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Mobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018. 2\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, CVPR. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 7\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, ECCV. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 8\n\nPixel-adaptive convolutional neural networks. Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, Jan Kautz, CVPR. 7Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, and Jan Kautz. Pixel-adaptive convolutional neural networks. In CVPR, 2019. 1, 2, 3, 5, 7, 8\n\nSpatially-adaptive filter units for compact and efficient deep neural networks. Domen Tabernik, Matej Kristan, Ale\u0161 Leonardis, International Journal of Computer Vision. 20202Domen Tabernik, Matej Kristan, and Ale\u0161 Leonardis. Spatially-adaptive filter units for compact and efficient deep neural networks. International Journal of Computer Vision, 2020. 2\n\nMnasnet: Platform-aware neural architecture search for mobile. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, CVPR. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnas- net: Platform-aware neural architecture search for mobile. In CVPR, 2019. 6\n\nConditional convolutions for instance segmentation. Zhi Tian, Chunhua Shen, Hao Chen, ECCV. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convo- lutions for instance segmentation. In ECCV, 2020. 1, 2, 3, 4\n\nVsgnet: Spatial attention network for detecting human object interactions using graph convolutions. Oytun Ulutan, Iftekhar, S Bangalore, Manjunath, CVPR. 2020Oytun Ulutan, ASM Iftekhar, and Bangalore S Manjunath. Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions. In CVPR, 2020. 3\n\nResidual attention network for image classification. Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang, CVPR. Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification. In CVPR, 2017. 3\n\nCarafe: Content-aware reassembly of features. Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin, ICCV. Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua Lin. Carafe: Content-aware reassembly of fea- tures. In ICCV, 2019. 1, 2, 7\n\nXinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen, arXiv:2003.10152Solov2: Dynamic, faster and stronger. arXiv preprintXinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun- hua Shen. Solov2: Dynamic, faster and stronger. arXiv preprint arXiv:2003.10152, 2020. 1, 2, 3, 4\n\nJoon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. Sanghyun Woo, Jongchan Park, ECCV. Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, 2018. 2, 3, 4, 6\n\nRgb-infrared cross-modality person re-identification. Ancong Wu, Wei-Shi Zheng, Hong-Xing Yu, Shaogang Gong, Jianhuang Lai, ICCV. Ancong Wu, Wei-Shi Zheng, Hong-Xing Yu, Shaogang Gong, and Jianhuang Lai. Rgb-infrared cross-modality per- son re-identification. In ICCV, 2017. 7\n\nDynamic filtering with large sampling field for convnets. Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, Xiangyang Ji, ECCV. Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, and Xiangyang Ji. Dynamic filtering with large sampling field for convnets. In ECCV, 2018. 1\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, CVPR. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, 2017. 6\n\nAsk, attend and answer: Exploring question-guided spatial attention for visual question answering. Huijuan Xu, Kate Saenko, ECCV. Huijuan Xu and Kate Saenko. Ask, attend and answer: Ex- ploring question-guided spatial attention for visual question answering. In ECCV, 2016. 3\n\nCondconv: Conditionally parameterized convolutions for efficient inference. Brandon Yang, Gabriel Bender, V Quoc, Jiquan Le, Ngiam, NeurIPS. 26Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolu- tions for efficient inference. In NeurIPS, 2019. 2, 6\n\nMulti-scale context aggregation by dilated convolutions. Fisher Yu, Vladlen Koltun, ICLR. 2Fisher Yu and Vladlen Koltun. Multi-scale context aggrega- tion by dilated convolutions. ICLR, 2016. 2\n\nAdaptive convolutional kernels. Julio Zamora Esquivel, Adan Cruz Vargas, Paulo Lopez Meyer, Omesh Tickoo, ICCV Workshops. Julio Zamora Esquivel, Adan Cruz Vargas, Paulo Lopez Meyer, and Omesh Tickoo. Adaptive convolu- tional kernels. In ICCV Workshops, 2019. 1, 2, 3, 4, 6\n\nScale-adaptive convolutions for scene parsing. Rui Zhang, Sheng Tang, Yongdong Zhang, Jintao Li, Shuicheng Yan, ICCV. Rui Zhang, Sheng Tang, Yongdong Zhang, Jintao Li, and Shuicheng Yan. Scale-adaptive convolutions for scene pars- ing. In ICCV, 2017. 2\n\nYikang Zhang, Jian Zhang, Qiang Wang, Zhao Zhong, Dynet, arXiv:2004.10694Dynamic convolution for accelerating convolutional neural networks. 26arXiv preprintYikang Zhang, Jian Zhang, Qiang Wang, and Zhao Zhong. Dynet: Dynamic convolution for accelerating convolutional neural networks. arXiv preprint arXiv:2004.10694, 2020. 2, 6\n", "annotations": {"author": "[{\"end\":50,\"start\":37},{\"end\":83,\"start\":51},{\"end\":96,\"start\":84},{\"end\":146,\"start\":97},{\"end\":163,\"start\":147},{\"end\":201,\"start\":164},{\"end\":250,\"start\":202},{\"end\":271,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":49,\"start\":45},{\"end\":64,\"start\":57},{\"end\":95,\"start\":93},{\"end\":106,\"start\":103},{\"end\":162,\"start\":158}]", "author_first_name": "[{\"end\":44,\"start\":37},{\"end\":56,\"start\":51},{\"end\":92,\"start\":84},{\"end\":102,\"start\":97},{\"end\":157,\"start\":147}]", "author_affiliation": "[{\"end\":82,\"start\":66},{\"end\":145,\"start\":108},{\"end\":200,\"start\":165},{\"end\":249,\"start\":203},{\"end\":270,\"start\":252}]", "title": "[{\"end\":34,\"start\":1},{\"end\":305,\"start\":272}]", "venue": null, "abstract": "[{\"end\":1627,\"start\":307}]", "bib_ref": "[{\"end\":2273,\"start\":2265},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2929,\"start\":2925},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2932,\"start\":2929},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3255,\"start\":3251},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3258,\"start\":3255},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3261,\"start\":3258},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3264,\"start\":3261},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3267,\"start\":3264},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3270,\"start\":3267},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3273,\"start\":3270},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3276,\"start\":3273},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3412,\"start\":3408},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3415,\"start\":3412},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3438,\"start\":3434},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3441,\"start\":3438},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3479,\"start\":3475},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3482,\"start\":3479},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3485,\"start\":3482},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3488,\"start\":3485},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3646,\"start\":3642},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3649,\"start\":3646},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3652,\"start\":3649},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3655,\"start\":3652},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3685,\"start\":3681},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3688,\"start\":3685},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3736,\"start\":3732},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4705,\"start\":4701},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4708,\"start\":4705},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4711,\"start\":4708},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4714,\"start\":4711},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5082,\"start\":5078},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5085,\"start\":5082},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7263,\"start\":7260},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7266,\"start\":7263},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7435,\"start\":7431},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7438,\"start\":7435},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7440,\"start\":7438},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7573,\"start\":7569},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8300,\"start\":8296},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8347,\"start\":8344},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8386,\"start\":8382},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8517,\"start\":8513},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8520,\"start\":8517},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8522,\"start\":8520},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8525,\"start\":8522},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8528,\"start\":8525},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8531,\"start\":8528},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8534,\"start\":8531},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8537,\"start\":8534},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8540,\"start\":8537},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8543,\"start\":8540},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8603,\"start\":8599},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8618,\"start\":8614},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8630,\"start\":8626},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8651,\"start\":8648},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8803,\"start\":8799},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9049,\"start\":9045},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9052,\"start\":9049},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9055,\"start\":9052},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9058,\"start\":9055},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9394,\"start\":9390},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9439,\"start\":9435},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9457,\"start\":9453},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9539,\"start\":9535},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9965,\"start\":9961},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9968,\"start\":9965},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9970,\"start\":9968},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9973,\"start\":9970},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9998,\"start\":9994},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10001,\"start\":9998},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10004,\"start\":10001},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10007,\"start\":10004},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10010,\"start\":10007},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10013,\"start\":10010},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10138,\"start\":10134},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10308,\"start\":10304},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10400,\"start\":10396},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10516,\"start\":10512},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10704,\"start\":10700},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10775,\"start\":10771},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12627,\"start\":12623},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12630,\"start\":12627},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12689,\"start\":12685},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12692,\"start\":12689},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12695,\"start\":12692},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12698,\"start\":12695},{\"end\":13154,\"start\":13146},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15217,\"start\":15213},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15220,\"start\":15217},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15753,\"start\":15749},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16440,\"start\":16436},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16743,\"start\":16739},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16746,\"start\":16743},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16749,\"start\":16746},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16752,\"start\":16749},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18840,\"start\":18836},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20371,\"start\":20367},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21006,\"start\":21002},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21106,\"start\":21102},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21510,\"start\":21506},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22455,\"start\":22451},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23637,\"start\":23633},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23660,\"start\":23656},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23729,\"start\":23725},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23774,\"start\":23770},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23795,\"start\":23791},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24555,\"start\":24551},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24565,\"start\":24561},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24576,\"start\":24572},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24587,\"start\":24584},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24633,\"start\":24629},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24650,\"start\":24646},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24994,\"start\":24990},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24997,\"start\":24994},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25891,\"start\":25887},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25894,\"start\":25891},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25897,\"start\":25894},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25933,\"start\":25929},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25935,\"start\":25933},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25937,\"start\":25935},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25959,\"start\":25955},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26203,\"start\":26199},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26807,\"start\":26803},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26851,\"start\":26847},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27281,\"start\":27277},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27365,\"start\":27361},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27529,\"start\":27526},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28116,\"start\":28112},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28143,\"start\":28139},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28160,\"start\":28156},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28694,\"start\":28690},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28842,\"start\":28838},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29146,\"start\":29142},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29251,\"start\":29247},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29859,\"start\":29855},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29914,\"start\":29910},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30165,\"start\":30161},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30641,\"start\":30637},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30655,\"start\":30651},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30723,\"start\":30719},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32424,\"start\":32420},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32971,\"start\":32967},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35470,\"start\":35466}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31910,\"start\":31873},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32267,\"start\":31911},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32293,\"start\":32268},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32510,\"start\":32294},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32780,\"start\":32511},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33183,\"start\":32781},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34319,\"start\":33184},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35404,\"start\":34320},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35922,\"start\":35405},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36467,\"start\":35923}]", "paragraph": "[{\"end\":4513,\"start\":1643},{\"end\":5978,\"start\":4515},{\"end\":6420,\"start\":5980},{\"end\":6529,\"start\":6422},{\"end\":7065,\"start\":6531},{\"end\":7535,\"start\":7082},{\"end\":8043,\"start\":7537},{\"end\":11032,\"start\":8045},{\"end\":11339,\"start\":11050},{\"end\":11448,\"start\":11387},{\"end\":11739,\"start\":11673},{\"end\":12708,\"start\":11834},{\"end\":13282,\"start\":12737},{\"end\":14278,\"start\":13351},{\"end\":15157,\"start\":14280},{\"end\":15802,\"start\":15159},{\"end\":16004,\"start\":15804},{\"end\":19484,\"start\":16116},{\"end\":20140,\"start\":19643},{\"end\":20952,\"start\":20182},{\"end\":21996,\"start\":20954},{\"end\":22747,\"start\":21998},{\"end\":23308,\"start\":22749},{\"end\":24937,\"start\":23310},{\"end\":25181,\"start\":24939},{\"end\":26659,\"start\":25210},{\"end\":27179,\"start\":26661},{\"end\":27964,\"start\":27181},{\"end\":28671,\"start\":27966},{\"end\":28864,\"start\":28673},{\"end\":29802,\"start\":28866},{\"end\":30999,\"start\":29804},{\"end\":31854,\"start\":31014}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11386,\"start\":11340},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11672,\"start\":11449},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11833,\"start\":11740},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13350,\"start\":13283},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16115,\"start\":16005},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19642,\"start\":19513}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16578,\"start\":16571},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18757,\"start\":18750},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18997,\"start\":18990},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19406,\"start\":19399},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21603,\"start\":21596},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21759,\"start\":21752},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22309,\"start\":22302},{\"end\":23513,\"start\":23506},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24136,\"start\":24129},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24492,\"start\":24485},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30369,\"start\":30362},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30796,\"start\":30789}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1641,\"start\":1629},{\"attributes\":{\"n\":\"2.\"},\"end\":7080,\"start\":7068},{\"attributes\":{\"n\":\"3.\"},\"end\":11048,\"start\":11035},{\"attributes\":{\"n\":\"4.\"},\"end\":12735,\"start\":12711},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19512,\"start\":19487},{\"attributes\":{\"n\":\"5.\"},\"end\":20180,\"start\":20143},{\"attributes\":{\"n\":\"6.\"},\"end\":25208,\"start\":25184},{\"attributes\":{\"n\":\"7.\"},\"end\":31012,\"start\":31002},{\"attributes\":{\"n\":\"8.\"},\"end\":31872,\"start\":31857},{\"end\":31922,\"start\":31912},{\"end\":32305,\"start\":32295},{\"end\":32521,\"start\":32512},{\"end\":32791,\"start\":32782},{\"end\":33194,\"start\":33185},{\"end\":34330,\"start\":34321},{\"end\":35415,\"start\":35406},{\"end\":35933,\"start\":35924}]", "table": "[{\"end\":32780,\"start\":32721},{\"end\":33183,\"start\":33035},{\"end\":34319,\"start\":33452},{\"end\":35404,\"start\":34342},{\"end\":35922,\"start\":35627},{\"end\":36467,\"start\":36151}]", "figure_caption": "[{\"end\":31910,\"start\":31875},{\"end\":32267,\"start\":31924},{\"end\":32293,\"start\":32270},{\"end\":32510,\"start\":32307},{\"end\":32721,\"start\":32523},{\"end\":33035,\"start\":32793},{\"end\":33452,\"start\":33196},{\"end\":34342,\"start\":34332},{\"end\":35627,\"start\":35417},{\"end\":36151,\"start\":35935}]", "figure_ref": "[{\"end\":2676,\"start\":2668},{\"end\":4814,\"start\":4806},{\"end\":4974,\"start\":4966},{\"end\":5370,\"start\":5362},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9159,\"start\":9151},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13728,\"start\":13720},{\"end\":14683,\"start\":14675},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15418,\"start\":15410},{\"end\":20469,\"start\":20461},{\"end\":20855,\"start\":20847},{\"end\":25393,\"start\":25385},{\"end\":25983,\"start\":25975},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27092,\"start\":27084},{\"end\":28724,\"start\":28716},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29381,\"start\":29373},{\"end\":30900,\"start\":30892}]", "bib_author_first_name": "[{\"end\":36497,\"start\":36496},{\"end\":36511,\"start\":36508},{\"end\":36660,\"start\":36655},{\"end\":36674,\"start\":36668},{\"end\":36687,\"start\":36681},{\"end\":36705,\"start\":36697},{\"end\":36720,\"start\":36714},{\"end\":36872,\"start\":36869},{\"end\":36884,\"start\":36879},{\"end\":36900,\"start\":36891},{\"end\":36913,\"start\":36907},{\"end\":36921,\"start\":36919},{\"end\":36937,\"start\":36929},{\"end\":36949,\"start\":36942},{\"end\":36961,\"start\":36955},{\"end\":36973,\"start\":36968},{\"end\":36985,\"start\":36979},{\"end\":36995,\"start\":36990},{\"end\":37008,\"start\":37003},{\"end\":37024,\"start\":37016},{\"end\":37038,\"start\":37030},{\"end\":37051,\"start\":37046},{\"end\":37062,\"start\":37058},{\"end\":37070,\"start\":37067},{\"end\":37078,\"start\":37075},{\"end\":37087,\"start\":37084},{\"end\":37098,\"start\":37092},{\"end\":37112,\"start\":37104},{\"end\":37127,\"start\":37119},{\"end\":37138,\"start\":37133},{\"end\":37801,\"start\":37790},{\"end\":37814,\"start\":37808},{\"end\":37834,\"start\":37827},{\"end\":37850,\"start\":37845},{\"end\":37863,\"start\":37859},{\"end\":37865,\"start\":37864},{\"end\":38296,\"start\":38289},{\"end\":38309,\"start\":38303},{\"end\":38323,\"start\":38315},{\"end\":38337,\"start\":38329},{\"end\":38346,\"start\":38344},{\"end\":38360,\"start\":38353},{\"end\":38665,\"start\":38658},{\"end\":38677,\"start\":38672},{\"end\":38690,\"start\":38683},{\"end\":38704,\"start\":38698},{\"end\":39088,\"start\":39080},{\"end\":39336,\"start\":39330},{\"end\":39348,\"start\":39342},{\"end\":39358,\"start\":39353},{\"end\":39368,\"start\":39366},{\"end\":39380,\"start\":39373},{\"end\":39391,\"start\":39388},{\"end\":39402,\"start\":39396},{\"end\":39635,\"start\":39627},{\"end\":39649,\"start\":39641},{\"end\":39660,\"start\":39654},{\"end\":39672,\"start\":39667},{\"end\":39681,\"start\":39677},{\"end\":39919,\"start\":39916},{\"end\":39929,\"start\":39926},{\"end\":39943,\"start\":39936},{\"end\":39958,\"start\":39952},{\"end\":39966,\"start\":39963},{\"end\":39973,\"start\":39971},{\"end\":40202,\"start\":40193},{\"end\":40215,\"start\":40210},{\"end\":40231,\"start\":40225},{\"end\":40246,\"start\":40240},{\"end\":40261,\"start\":40256},{\"end\":40263,\"start\":40262},{\"end\":40501,\"start\":40493},{\"end\":40516,\"start\":40507},{\"end\":40527,\"start\":40524},{\"end\":40540,\"start\":40534},{\"end\":40558,\"start\":40548},{\"end\":40574,\"start\":40565},{\"end\":40931,\"start\":40925},{\"end\":40942,\"start\":40937},{\"end\":40952,\"start\":40948},{\"end\":41118,\"start\":41111},{\"end\":41127,\"start\":41123},{\"end\":41139,\"start\":41133},{\"end\":41288,\"start\":41281},{\"end\":41300,\"start\":41293},{\"end\":41316,\"start\":41308},{\"end\":41326,\"start\":41322},{\"end\":41534,\"start\":41528},{\"end\":41547,\"start\":41543},{\"end\":41562,\"start\":41557},{\"end\":41579,\"start\":41568},{\"end\":41588,\"start\":41586},{\"end\":41603,\"start\":41595},{\"end\":41615,\"start\":41609},{\"end\":41627,\"start\":41622},{\"end\":41830,\"start\":41829},{\"end\":41847,\"start\":41839},{\"end\":41858,\"start\":41856},{\"end\":41870,\"start\":41864},{\"end\":41883,\"start\":41877},{\"end\":41904,\"start\":41898},{\"end\":41916,\"start\":41911},{\"end\":41932,\"start\":41925},{\"end\":42354,\"start\":42351},{\"end\":42361,\"start\":42359},{\"end\":42372,\"start\":42368},{\"end\":42539,\"start\":42532},{\"end\":42549,\"start\":42545},{\"end\":42556,\"start\":42550},{\"end\":42568,\"start\":42562},{\"end\":42803,\"start\":42797},{\"end\":42820,\"start\":42811},{\"end\":43064,\"start\":43057},{\"end\":43079,\"start\":43071},{\"end\":43091,\"start\":43086},{\"end\":43462,\"start\":43457},{\"end\":43478,\"start\":43472},{\"end\":43492,\"start\":43487},{\"end\":43494,\"start\":43493},{\"end\":43730,\"start\":43728},{\"end\":43740,\"start\":43736},{\"end\":43761,\"start\":43756},{\"end\":43779,\"start\":43774},{\"end\":43957,\"start\":43956},{\"end\":43973,\"start\":43968},{\"end\":44120,\"start\":44112},{\"end\":44128,\"start\":44127},{\"end\":44142,\"start\":44138},{\"end\":44154,\"start\":44150},{\"end\":44418,\"start\":44414},{\"end\":44435,\"start\":44431},{\"end\":44455,\"start\":44447},{\"end\":44457,\"start\":44456},{\"end\":44705,\"start\":44702},{\"end\":44716,\"start\":44710},{\"end\":44728,\"start\":44722},{\"end\":44917,\"start\":44912},{\"end\":44929,\"start\":44922},{\"end\":44945,\"start\":44937},{\"end\":44963,\"start\":44953},{\"end\":45149,\"start\":45144},{\"end\":45161,\"start\":45154},{\"end\":45177,\"start\":45169},{\"end\":45195,\"start\":45185},{\"end\":45570,\"start\":45562},{\"end\":45581,\"start\":45576},{\"end\":45594,\"start\":45590},{\"end\":45826,\"start\":45818},{\"end\":45839,\"start\":45832},{\"end\":45852,\"start\":45847},{\"end\":45868,\"start\":45863},{\"end\":45881,\"start\":45875},{\"end\":45894,\"start\":45890},{\"end\":45909,\"start\":45904},{\"end\":45928,\"start\":45918},{\"end\":46168,\"start\":46164},{\"end\":46181,\"start\":46174},{\"end\":46338,\"start\":46331},{\"end\":46349,\"start\":46344},{\"end\":46366,\"start\":46360},{\"end\":46544,\"start\":46536},{\"end\":46556,\"start\":46549},{\"end\":46570,\"start\":46564},{\"end\":46582,\"start\":46578},{\"end\":46804,\"start\":46796},{\"end\":46819,\"start\":46810},{\"end\":46833,\"start\":46826},{\"end\":47043,\"start\":47035},{\"end\":47058,\"start\":47050},{\"end\":47277,\"start\":47270},{\"end\":47296,\"start\":47292},{\"end\":47305,\"start\":47301},{\"end\":47530,\"start\":47524},{\"end\":47532,\"start\":47531},{\"end\":47649,\"start\":47645},{\"end\":47665,\"start\":47659},{\"end\":47682,\"start\":47674},{\"end\":47694,\"start\":47688},{\"end\":47717,\"start\":47706},{\"end\":48054,\"start\":48048},{\"end\":48064,\"start\":48060},{\"end\":48082,\"start\":48076},{\"end\":48099,\"start\":48091},{\"end\":48107,\"start\":48106},{\"end\":48119,\"start\":48116},{\"end\":48134,\"start\":48128},{\"end\":48148,\"start\":48143},{\"end\":48484,\"start\":48478},{\"end\":48501,\"start\":48496},{\"end\":48517,\"start\":48509},{\"end\":48528,\"start\":48525},{\"end\":48734,\"start\":48730},{\"end\":48744,\"start\":48739},{\"end\":48760,\"start\":48754},{\"end\":48772,\"start\":48766},{\"end\":48784,\"start\":48780},{\"end\":48804,\"start\":48801},{\"end\":49069,\"start\":49064},{\"end\":49085,\"start\":49080},{\"end\":49099,\"start\":49095},{\"end\":49411,\"start\":49403},{\"end\":49419,\"start\":49417},{\"end\":49433,\"start\":49426},{\"end\":49445,\"start\":49440},{\"end\":49461,\"start\":49457},{\"end\":49477,\"start\":49471},{\"end\":49492,\"start\":49486},{\"end\":49739,\"start\":49736},{\"end\":49753,\"start\":49746},{\"end\":49763,\"start\":49760},{\"end\":50000,\"start\":49995},{\"end\":50020,\"start\":50019},{\"end\":50282,\"start\":50279},{\"end\":50297,\"start\":50289},{\"end\":50309,\"start\":50305},{\"end\":50320,\"start\":50316},{\"end\":50332,\"start\":50327},{\"end\":50345,\"start\":50337},{\"end\":50361,\"start\":50353},{\"end\":50374,\"start\":50368},{\"end\":50615,\"start\":50610},{\"end\":50625,\"start\":50622},{\"end\":50635,\"start\":50632},{\"end\":50645,\"start\":50640},{\"end\":50655,\"start\":50651},{\"end\":50662,\"start\":50656},{\"end\":50673,\"start\":50668},{\"end\":50837,\"start\":50830},{\"end\":50850,\"start\":50844},{\"end\":50861,\"start\":50858},{\"end\":50871,\"start\":50868},{\"end\":50883,\"start\":50876},{\"end\":51198,\"start\":51190},{\"end\":51212,\"start\":51204},{\"end\":51418,\"start\":51412},{\"end\":51430,\"start\":51423},{\"end\":51447,\"start\":51438},{\"end\":51460,\"start\":51452},{\"end\":51476,\"start\":51467},{\"end\":51700,\"start\":51694},{\"end\":51708,\"start\":51705},{\"end\":51715,\"start\":51713},{\"end\":51732,\"start\":51722},{\"end\":51749,\"start\":51740},{\"end\":51969,\"start\":51962},{\"end\":51979,\"start\":51975},{\"end\":51995,\"start\":51990},{\"end\":52011,\"start\":52004},{\"end\":52023,\"start\":52016},{\"end\":52290,\"start\":52283},{\"end\":52299,\"start\":52295},{\"end\":52544,\"start\":52537},{\"end\":52558,\"start\":52551},{\"end\":52568,\"start\":52567},{\"end\":52581,\"start\":52575},{\"end\":52828,\"start\":52822},{\"end\":52840,\"start\":52833},{\"end\":52997,\"start\":52992},{\"end\":53019,\"start\":53015},{\"end\":53024,\"start\":53020},{\"end\":53038,\"start\":53033},{\"end\":53044,\"start\":53039},{\"end\":53057,\"start\":53052},{\"end\":53284,\"start\":53281},{\"end\":53297,\"start\":53292},{\"end\":53312,\"start\":53304},{\"end\":53326,\"start\":53320},{\"end\":53340,\"start\":53331},{\"end\":53494,\"start\":53488},{\"end\":53506,\"start\":53502},{\"end\":53519,\"start\":53514},{\"end\":53530,\"start\":53526}]", "bib_author_last_name": "[{\"end\":36506,\"start\":36498},{\"end\":36518,\"start\":36512},{\"end\":36525,\"start\":36520},{\"end\":36666,\"start\":36661},{\"end\":36679,\"start\":36675},{\"end\":36695,\"start\":36688},{\"end\":36712,\"start\":36706},{\"end\":36723,\"start\":36721},{\"end\":36877,\"start\":36873},{\"end\":36889,\"start\":36885},{\"end\":36905,\"start\":36901},{\"end\":36917,\"start\":36914},{\"end\":36927,\"start\":36922},{\"end\":36940,\"start\":36938},{\"end\":36953,\"start\":36950},{\"end\":36966,\"start\":36962},{\"end\":36977,\"start\":36974},{\"end\":36988,\"start\":36986},{\"end\":37001,\"start\":36996},{\"end\":37014,\"start\":37009},{\"end\":37028,\"start\":37025},{\"end\":37044,\"start\":37039},{\"end\":37056,\"start\":37052},{\"end\":37065,\"start\":37063},{\"end\":37073,\"start\":37071},{\"end\":37082,\"start\":37079},{\"end\":37090,\"start\":37088},{\"end\":37102,\"start\":37099},{\"end\":37117,\"start\":37113},{\"end\":37131,\"start\":37128},{\"end\":37145,\"start\":37139},{\"end\":37806,\"start\":37802},{\"end\":37825,\"start\":37815},{\"end\":37843,\"start\":37835},{\"end\":37857,\"start\":37851},{\"end\":37872,\"start\":37866},{\"end\":38301,\"start\":38297},{\"end\":38313,\"start\":38310},{\"end\":38327,\"start\":38324},{\"end\":38342,\"start\":38338},{\"end\":38351,\"start\":38347},{\"end\":38364,\"start\":38361},{\"end\":38670,\"start\":38666},{\"end\":38681,\"start\":38678},{\"end\":38696,\"start\":38691},{\"end\":38709,\"start\":38705},{\"end\":39097,\"start\":39089},{\"end\":39115,\"start\":39099},{\"end\":39340,\"start\":39337},{\"end\":39351,\"start\":39349},{\"end\":39364,\"start\":39359},{\"end\":39371,\"start\":39369},{\"end\":39386,\"start\":39381},{\"end\":39394,\"start\":39392},{\"end\":39406,\"start\":39403},{\"end\":39639,\"start\":39636},{\"end\":39652,\"start\":39650},{\"end\":39665,\"start\":39661},{\"end\":39675,\"start\":39673},{\"end\":39687,\"start\":39682},{\"end\":39924,\"start\":39920},{\"end\":39934,\"start\":39930},{\"end\":39950,\"start\":39944},{\"end\":39961,\"start\":39959},{\"end\":39969,\"start\":39967},{\"end\":39981,\"start\":39974},{\"end\":40208,\"start\":40203},{\"end\":40223,\"start\":40216},{\"end\":40238,\"start\":40232},{\"end\":40254,\"start\":40247},{\"end\":40270,\"start\":40264},{\"end\":40505,\"start\":40502},{\"end\":40522,\"start\":40517},{\"end\":40532,\"start\":40528},{\"end\":40546,\"start\":40541},{\"end\":40563,\"start\":40559},{\"end\":40579,\"start\":40575},{\"end\":40935,\"start\":40932},{\"end\":40946,\"start\":40943},{\"end\":40958,\"start\":40953},{\"end\":41121,\"start\":41119},{\"end\":41131,\"start\":41128},{\"end\":41144,\"start\":41140},{\"end\":41291,\"start\":41289},{\"end\":41306,\"start\":41301},{\"end\":41320,\"start\":41317},{\"end\":41330,\"start\":41327},{\"end\":41541,\"start\":41535},{\"end\":41555,\"start\":41548},{\"end\":41566,\"start\":41563},{\"end\":41584,\"start\":41580},{\"end\":41593,\"start\":41589},{\"end\":41607,\"start\":41604},{\"end\":41620,\"start\":41616},{\"end\":41631,\"start\":41628},{\"end\":41837,\"start\":41831},{\"end\":41854,\"start\":41848},{\"end\":41862,\"start\":41859},{\"end\":41875,\"start\":41871},{\"end\":41896,\"start\":41884},{\"end\":41909,\"start\":41905},{\"end\":41923,\"start\":41917},{\"end\":41942,\"start\":41933},{\"end\":41948,\"start\":41944},{\"end\":42357,\"start\":42355},{\"end\":42366,\"start\":42362},{\"end\":42376,\"start\":42373},{\"end\":42543,\"start\":42540},{\"end\":42560,\"start\":42557},{\"end\":42573,\"start\":42569},{\"end\":42809,\"start\":42804},{\"end\":42828,\"start\":42821},{\"end\":43069,\"start\":43065},{\"end\":43084,\"start\":43080},{\"end\":43098,\"start\":43092},{\"end\":43470,\"start\":43463},{\"end\":43485,\"start\":43479},{\"end\":43501,\"start\":43495},{\"end\":43734,\"start\":43731},{\"end\":43754,\"start\":43741},{\"end\":43772,\"start\":43762},{\"end\":43784,\"start\":43780},{\"end\":43966,\"start\":43958},{\"end\":43980,\"start\":43974},{\"end\":43984,\"start\":43982},{\"end\":44125,\"start\":44121},{\"end\":44136,\"start\":44129},{\"end\":44148,\"start\":44143},{\"end\":44165,\"start\":44155},{\"end\":44178,\"start\":44167},{\"end\":44429,\"start\":44419},{\"end\":44445,\"start\":44436},{\"end\":44464,\"start\":44458},{\"end\":44708,\"start\":44706},{\"end\":44720,\"start\":44717},{\"end\":44733,\"start\":44729},{\"end\":44920,\"start\":44918},{\"end\":44935,\"start\":44930},{\"end\":44951,\"start\":44946},{\"end\":44968,\"start\":44964},{\"end\":45152,\"start\":45150},{\"end\":45167,\"start\":45162},{\"end\":45183,\"start\":45178},{\"end\":45200,\"start\":45196},{\"end\":45574,\"start\":45571},{\"end\":45588,\"start\":45582},{\"end\":45603,\"start\":45595},{\"end\":45830,\"start\":45827},{\"end\":45845,\"start\":45840},{\"end\":45861,\"start\":45853},{\"end\":45873,\"start\":45869},{\"end\":45888,\"start\":45882},{\"end\":45902,\"start\":45895},{\"end\":45916,\"start\":45910},{\"end\":45936,\"start\":45929},{\"end\":46172,\"start\":46169},{\"end\":46190,\"start\":46182},{\"end\":46342,\"start\":46339},{\"end\":46358,\"start\":46350},{\"end\":46371,\"start\":46367},{\"end\":46547,\"start\":46545},{\"end\":46562,\"start\":46557},{\"end\":46576,\"start\":46571},{\"end\":46586,\"start\":46583},{\"end\":46808,\"start\":46805},{\"end\":46824,\"start\":46820},{\"end\":46837,\"start\":46834},{\"end\":47048,\"start\":47044},{\"end\":47062,\"start\":47059},{\"end\":47290,\"start\":47278},{\"end\":47299,\"start\":47297},{\"end\":47314,\"start\":47306},{\"end\":47319,\"start\":47316},{\"end\":47540,\"start\":47533},{\"end\":47657,\"start\":47650},{\"end\":47672,\"start\":47666},{\"end\":47686,\"start\":47683},{\"end\":47704,\"start\":47695},{\"end\":47722,\"start\":47718},{\"end\":48058,\"start\":48055},{\"end\":48074,\"start\":48065},{\"end\":48089,\"start\":48083},{\"end\":48104,\"start\":48100},{\"end\":48114,\"start\":48108},{\"end\":48126,\"start\":48120},{\"end\":48141,\"start\":48135},{\"end\":48157,\"start\":48149},{\"end\":48163,\"start\":48159},{\"end\":48494,\"start\":48485},{\"end\":48507,\"start\":48502},{\"end\":48523,\"start\":48518},{\"end\":48535,\"start\":48529},{\"end\":48737,\"start\":48735},{\"end\":48752,\"start\":48745},{\"end\":48764,\"start\":48761},{\"end\":48778,\"start\":48773},{\"end\":48799,\"start\":48785},{\"end\":48810,\"start\":48805},{\"end\":49078,\"start\":49070},{\"end\":49093,\"start\":49086},{\"end\":49109,\"start\":49100},{\"end\":49415,\"start\":49412},{\"end\":49424,\"start\":49420},{\"end\":49438,\"start\":49434},{\"end\":49455,\"start\":49446},{\"end\":49469,\"start\":49462},{\"end\":49484,\"start\":49478},{\"end\":49495,\"start\":49493},{\"end\":49744,\"start\":49740},{\"end\":49758,\"start\":49754},{\"end\":49768,\"start\":49764},{\"end\":50007,\"start\":50001},{\"end\":50017,\"start\":50009},{\"end\":50030,\"start\":50021},{\"end\":50041,\"start\":50032},{\"end\":50287,\"start\":50283},{\"end\":50303,\"start\":50298},{\"end\":50314,\"start\":50310},{\"end\":50325,\"start\":50321},{\"end\":50335,\"start\":50333},{\"end\":50351,\"start\":50346},{\"end\":50366,\"start\":50362},{\"end\":50379,\"start\":50375},{\"end\":50620,\"start\":50616},{\"end\":50630,\"start\":50626},{\"end\":50638,\"start\":50636},{\"end\":50649,\"start\":50646},{\"end\":50666,\"start\":50663},{\"end\":50677,\"start\":50674},{\"end\":50842,\"start\":50838},{\"end\":50856,\"start\":50851},{\"end\":50866,\"start\":50862},{\"end\":50874,\"start\":50872},{\"end\":50888,\"start\":50884},{\"end\":51202,\"start\":51199},{\"end\":51217,\"start\":51213},{\"end\":51421,\"start\":51419},{\"end\":51436,\"start\":51431},{\"end\":51450,\"start\":51448},{\"end\":51465,\"start\":51461},{\"end\":51480,\"start\":51477},{\"end\":51703,\"start\":51701},{\"end\":51711,\"start\":51709},{\"end\":51720,\"start\":51716},{\"end\":51738,\"start\":51733},{\"end\":51752,\"start\":51750},{\"end\":51973,\"start\":51970},{\"end\":51988,\"start\":51980},{\"end\":52002,\"start\":51996},{\"end\":52014,\"start\":52012},{\"end\":52026,\"start\":52024},{\"end\":52293,\"start\":52291},{\"end\":52306,\"start\":52300},{\"end\":52549,\"start\":52545},{\"end\":52565,\"start\":52559},{\"end\":52573,\"start\":52569},{\"end\":52584,\"start\":52582},{\"end\":52591,\"start\":52586},{\"end\":52831,\"start\":52829},{\"end\":52847,\"start\":52841},{\"end\":53013,\"start\":52998},{\"end\":53031,\"start\":53025},{\"end\":53050,\"start\":53045},{\"end\":53064,\"start\":53058},{\"end\":53290,\"start\":53285},{\"end\":53302,\"start\":53298},{\"end\":53318,\"start\":53313},{\"end\":53329,\"start\":53327},{\"end\":53344,\"start\":53341},{\"end\":53500,\"start\":53495},{\"end\":53512,\"start\":53507},{\"end\":53524,\"start\":53520},{\"end\":53536,\"start\":53531},{\"end\":53543,\"start\":53538}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10294200},\"end\":36609,\"start\":36469},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":127951164},\"end\":36867,\"start\":36611},{\"attributes\":{\"doi\":\"arXiv:1906.07155\",\"id\":\"b2\"},\"end\":37675,\"start\":36869},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3429309},\"end\":38230,\"start\":37677},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208910380},\"end\":38532,\"start\":38232},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3819249},\"end\":39010,\"start\":38534},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1540678},\"end\":39293,\"start\":39012},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4028864},\"end\":39547,\"start\":39295},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":51604787},\"end\":39861,\"start\":39549},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":40129,\"start\":39863},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7552841},\"end\":40441,\"start\":40131},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":91184391},\"end\":40859,\"start\":40443},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7862234},\"end\":41085,\"start\":40861},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1264129},\"end\":41233,\"start\":41087},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":41460,\"start\":41235},{\"attributes\":{\"id\":\"b15\"},\"end\":41827,\"start\":41462},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b16\"},\"end\":42316,\"start\":41829},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":140309863},\"end\":42473,\"start\":42318},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9579734},\"end\":42701,\"start\":42475},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5808102},\"end\":42986,\"start\":42703},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3108956},\"end\":43354,\"start\":42988},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9319522},\"end\":43701,\"start\":43356},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2097418},\"end\":43910,\"start\":43703},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6628106},\"end\":44082,\"start\":43912},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7241297},\"end\":44347,\"start\":44084},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":195908774},\"end\":44617,\"start\":44349},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220496271},\"end\":44882,\"start\":44619},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10195645},\"end\":45086,\"start\":44884},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":19514766},\"end\":45462,\"start\":45088},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10716717},\"end\":45773,\"start\":45464},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14113767},\"end\":46127,\"start\":45775},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10172928},\"end\":46280,\"start\":46129},{\"attributes\":{\"id\":\"b32\"},\"end\":46475,\"start\":46282},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220713516},\"end\":46736,\"start\":46477},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":623137},\"end\":46966,\"start\":46738},{\"attributes\":{\"id\":\"b35\"},\"end\":47188,\"start\":46968},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10328909},\"end\":47484,\"start\":47190},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6740010},\"end\":47643,\"start\":47486},{\"attributes\":{\"id\":\"b38\"},\"end\":47937,\"start\":47645},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":7037846},\"end\":48416,\"start\":47939},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":545361},\"end\":48682,\"start\":48418},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":119304936},\"end\":48982,\"start\":48684},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":67769767},\"end\":49338,\"start\":48984},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":51891697},\"end\":49682,\"start\":49340},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":212675968},\"end\":49893,\"start\":49684},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":211731368},\"end\":50224,\"start\":49895},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1806714},\"end\":50562,\"start\":50226},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":146120936},\"end\":50828,\"start\":50564},{\"attributes\":{\"doi\":\"arXiv:2003.10152\",\"id\":\"b48\"},\"end\":51111,\"start\":50830},{\"attributes\":{\"id\":\"b49\"},\"end\":51356,\"start\":51113},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4796796},\"end\":51634,\"start\":51358},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52954392},\"end\":51898,\"start\":51636},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":8485068},\"end\":52182,\"start\":51900},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":10363459},\"end\":52459,\"start\":52184},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":202775981},\"end\":52763,\"start\":52461},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":17127188},\"end\":52958,\"start\":52765},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":207902977},\"end\":53232,\"start\":52960},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":21053940},\"end\":53486,\"start\":53234},{\"attributes\":{\"doi\":\"arXiv:2004.10694\",\"id\":\"b58\"},\"end\":53817,\"start\":53488}]", "bib_title": "[{\"end\":36494,\"start\":36469},{\"end\":36653,\"start\":36611},{\"end\":37788,\"start\":37677},{\"end\":38287,\"start\":38232},{\"end\":38656,\"start\":38534},{\"end\":39078,\"start\":39012},{\"end\":39328,\"start\":39295},{\"end\":39625,\"start\":39549},{\"end\":39914,\"start\":39863},{\"end\":40191,\"start\":40131},{\"end\":40491,\"start\":40443},{\"end\":40923,\"start\":40861},{\"end\":41109,\"start\":41087},{\"end\":41279,\"start\":41235},{\"end\":41526,\"start\":41462},{\"end\":42349,\"start\":42318},{\"end\":42530,\"start\":42475},{\"end\":42795,\"start\":42703},{\"end\":43055,\"start\":42988},{\"end\":43455,\"start\":43356},{\"end\":43726,\"start\":43703},{\"end\":43954,\"start\":43912},{\"end\":44110,\"start\":44084},{\"end\":44412,\"start\":44349},{\"end\":44700,\"start\":44619},{\"end\":44910,\"start\":44884},{\"end\":45142,\"start\":45088},{\"end\":45560,\"start\":45464},{\"end\":45816,\"start\":45775},{\"end\":46162,\"start\":46129},{\"end\":46534,\"start\":46477},{\"end\":46794,\"start\":46738},{\"end\":47033,\"start\":46968},{\"end\":47268,\"start\":47190},{\"end\":47522,\"start\":47486},{\"end\":48046,\"start\":47939},{\"end\":48476,\"start\":48418},{\"end\":48728,\"start\":48684},{\"end\":49062,\"start\":48984},{\"end\":49401,\"start\":49340},{\"end\":49734,\"start\":49684},{\"end\":49993,\"start\":49895},{\"end\":50277,\"start\":50226},{\"end\":50608,\"start\":50564},{\"end\":51188,\"start\":51113},{\"end\":51410,\"start\":51358},{\"end\":51692,\"start\":51636},{\"end\":51960,\"start\":51900},{\"end\":52281,\"start\":52184},{\"end\":52535,\"start\":52461},{\"end\":52820,\"start\":52765},{\"end\":52990,\"start\":52960},{\"end\":53279,\"start\":53234}]", "bib_author": "[{\"end\":36508,\"start\":36496},{\"end\":36520,\"start\":36508},{\"end\":36527,\"start\":36520},{\"end\":36668,\"start\":36655},{\"end\":36681,\"start\":36668},{\"end\":36697,\"start\":36681},{\"end\":36714,\"start\":36697},{\"end\":36725,\"start\":36714},{\"end\":36879,\"start\":36869},{\"end\":36891,\"start\":36879},{\"end\":36907,\"start\":36891},{\"end\":36919,\"start\":36907},{\"end\":36929,\"start\":36919},{\"end\":36942,\"start\":36929},{\"end\":36955,\"start\":36942},{\"end\":36968,\"start\":36955},{\"end\":36979,\"start\":36968},{\"end\":36990,\"start\":36979},{\"end\":37003,\"start\":36990},{\"end\":37016,\"start\":37003},{\"end\":37030,\"start\":37016},{\"end\":37046,\"start\":37030},{\"end\":37058,\"start\":37046},{\"end\":37067,\"start\":37058},{\"end\":37075,\"start\":37067},{\"end\":37084,\"start\":37075},{\"end\":37092,\"start\":37084},{\"end\":37104,\"start\":37092},{\"end\":37119,\"start\":37104},{\"end\":37133,\"start\":37119},{\"end\":37147,\"start\":37133},{\"end\":37808,\"start\":37790},{\"end\":37827,\"start\":37808},{\"end\":37845,\"start\":37827},{\"end\":37859,\"start\":37845},{\"end\":37874,\"start\":37859},{\"end\":38303,\"start\":38289},{\"end\":38315,\"start\":38303},{\"end\":38329,\"start\":38315},{\"end\":38344,\"start\":38329},{\"end\":38353,\"start\":38344},{\"end\":38366,\"start\":38353},{\"end\":38672,\"start\":38658},{\"end\":38683,\"start\":38672},{\"end\":38698,\"start\":38683},{\"end\":38711,\"start\":38698},{\"end\":39099,\"start\":39080},{\"end\":39117,\"start\":39099},{\"end\":39342,\"start\":39330},{\"end\":39353,\"start\":39342},{\"end\":39366,\"start\":39353},{\"end\":39373,\"start\":39366},{\"end\":39388,\"start\":39373},{\"end\":39396,\"start\":39388},{\"end\":39408,\"start\":39396},{\"end\":39641,\"start\":39627},{\"end\":39654,\"start\":39641},{\"end\":39667,\"start\":39654},{\"end\":39677,\"start\":39667},{\"end\":39689,\"start\":39677},{\"end\":39926,\"start\":39916},{\"end\":39936,\"start\":39926},{\"end\":39952,\"start\":39936},{\"end\":39963,\"start\":39952},{\"end\":39971,\"start\":39963},{\"end\":39983,\"start\":39971},{\"end\":40210,\"start\":40193},{\"end\":40225,\"start\":40210},{\"end\":40240,\"start\":40225},{\"end\":40256,\"start\":40240},{\"end\":40272,\"start\":40256},{\"end\":40507,\"start\":40493},{\"end\":40524,\"start\":40507},{\"end\":40534,\"start\":40524},{\"end\":40548,\"start\":40534},{\"end\":40565,\"start\":40548},{\"end\":40581,\"start\":40565},{\"end\":40937,\"start\":40925},{\"end\":40948,\"start\":40937},{\"end\":40960,\"start\":40948},{\"end\":41123,\"start\":41111},{\"end\":41133,\"start\":41123},{\"end\":41146,\"start\":41133},{\"end\":41293,\"start\":41281},{\"end\":41308,\"start\":41293},{\"end\":41322,\"start\":41308},{\"end\":41332,\"start\":41322},{\"end\":41543,\"start\":41528},{\"end\":41557,\"start\":41543},{\"end\":41568,\"start\":41557},{\"end\":41586,\"start\":41568},{\"end\":41595,\"start\":41586},{\"end\":41609,\"start\":41595},{\"end\":41622,\"start\":41609},{\"end\":41633,\"start\":41622},{\"end\":41839,\"start\":41829},{\"end\":41856,\"start\":41839},{\"end\":41864,\"start\":41856},{\"end\":41877,\"start\":41864},{\"end\":41898,\"start\":41877},{\"end\":41911,\"start\":41898},{\"end\":41925,\"start\":41911},{\"end\":41944,\"start\":41925},{\"end\":41950,\"start\":41944},{\"end\":42359,\"start\":42351},{\"end\":42368,\"start\":42359},{\"end\":42378,\"start\":42368},{\"end\":42545,\"start\":42532},{\"end\":42562,\"start\":42545},{\"end\":42575,\"start\":42562},{\"end\":42811,\"start\":42797},{\"end\":42830,\"start\":42811},{\"end\":43071,\"start\":43057},{\"end\":43086,\"start\":43071},{\"end\":43100,\"start\":43086},{\"end\":43472,\"start\":43457},{\"end\":43487,\"start\":43472},{\"end\":43503,\"start\":43487},{\"end\":43736,\"start\":43728},{\"end\":43756,\"start\":43736},{\"end\":43774,\"start\":43756},{\"end\":43786,\"start\":43774},{\"end\":43968,\"start\":43956},{\"end\":43982,\"start\":43968},{\"end\":43986,\"start\":43982},{\"end\":44127,\"start\":44112},{\"end\":44138,\"start\":44127},{\"end\":44150,\"start\":44138},{\"end\":44167,\"start\":44150},{\"end\":44180,\"start\":44167},{\"end\":44431,\"start\":44414},{\"end\":44447,\"start\":44431},{\"end\":44466,\"start\":44447},{\"end\":44710,\"start\":44702},{\"end\":44722,\"start\":44710},{\"end\":44735,\"start\":44722},{\"end\":44922,\"start\":44912},{\"end\":44937,\"start\":44922},{\"end\":44953,\"start\":44937},{\"end\":44970,\"start\":44953},{\"end\":45154,\"start\":45144},{\"end\":45169,\"start\":45154},{\"end\":45185,\"start\":45169},{\"end\":45202,\"start\":45185},{\"end\":45576,\"start\":45562},{\"end\":45590,\"start\":45576},{\"end\":45605,\"start\":45590},{\"end\":45832,\"start\":45818},{\"end\":45847,\"start\":45832},{\"end\":45863,\"start\":45847},{\"end\":45875,\"start\":45863},{\"end\":45890,\"start\":45875},{\"end\":45904,\"start\":45890},{\"end\":45918,\"start\":45904},{\"end\":45938,\"start\":45918},{\"end\":46174,\"start\":46164},{\"end\":46192,\"start\":46174},{\"end\":46344,\"start\":46331},{\"end\":46360,\"start\":46344},{\"end\":46373,\"start\":46360},{\"end\":46549,\"start\":46536},{\"end\":46564,\"start\":46549},{\"end\":46578,\"start\":46564},{\"end\":46588,\"start\":46578},{\"end\":46810,\"start\":46796},{\"end\":46826,\"start\":46810},{\"end\":46839,\"start\":46826},{\"end\":47050,\"start\":47035},{\"end\":47064,\"start\":47050},{\"end\":47292,\"start\":47270},{\"end\":47301,\"start\":47292},{\"end\":47316,\"start\":47301},{\"end\":47321,\"start\":47316},{\"end\":47542,\"start\":47524},{\"end\":47659,\"start\":47645},{\"end\":47674,\"start\":47659},{\"end\":47688,\"start\":47674},{\"end\":47706,\"start\":47688},{\"end\":47724,\"start\":47706},{\"end\":48060,\"start\":48048},{\"end\":48076,\"start\":48060},{\"end\":48091,\"start\":48076},{\"end\":48106,\"start\":48091},{\"end\":48116,\"start\":48106},{\"end\":48128,\"start\":48116},{\"end\":48143,\"start\":48128},{\"end\":48159,\"start\":48143},{\"end\":48165,\"start\":48159},{\"end\":48496,\"start\":48478},{\"end\":48509,\"start\":48496},{\"end\":48525,\"start\":48509},{\"end\":48537,\"start\":48525},{\"end\":48739,\"start\":48730},{\"end\":48754,\"start\":48739},{\"end\":48766,\"start\":48754},{\"end\":48780,\"start\":48766},{\"end\":48801,\"start\":48780},{\"end\":48812,\"start\":48801},{\"end\":49080,\"start\":49064},{\"end\":49095,\"start\":49080},{\"end\":49111,\"start\":49095},{\"end\":49417,\"start\":49403},{\"end\":49426,\"start\":49417},{\"end\":49440,\"start\":49426},{\"end\":49457,\"start\":49440},{\"end\":49471,\"start\":49457},{\"end\":49486,\"start\":49471},{\"end\":49497,\"start\":49486},{\"end\":49746,\"start\":49736},{\"end\":49760,\"start\":49746},{\"end\":49770,\"start\":49760},{\"end\":50009,\"start\":49995},{\"end\":50019,\"start\":50009},{\"end\":50032,\"start\":50019},{\"end\":50043,\"start\":50032},{\"end\":50289,\"start\":50279},{\"end\":50305,\"start\":50289},{\"end\":50316,\"start\":50305},{\"end\":50327,\"start\":50316},{\"end\":50337,\"start\":50327},{\"end\":50353,\"start\":50337},{\"end\":50368,\"start\":50353},{\"end\":50381,\"start\":50368},{\"end\":50622,\"start\":50610},{\"end\":50632,\"start\":50622},{\"end\":50640,\"start\":50632},{\"end\":50651,\"start\":50640},{\"end\":50668,\"start\":50651},{\"end\":50679,\"start\":50668},{\"end\":50844,\"start\":50830},{\"end\":50858,\"start\":50844},{\"end\":50868,\"start\":50858},{\"end\":50876,\"start\":50868},{\"end\":50890,\"start\":50876},{\"end\":51204,\"start\":51190},{\"end\":51219,\"start\":51204},{\"end\":51423,\"start\":51412},{\"end\":51438,\"start\":51423},{\"end\":51452,\"start\":51438},{\"end\":51467,\"start\":51452},{\"end\":51482,\"start\":51467},{\"end\":51705,\"start\":51694},{\"end\":51713,\"start\":51705},{\"end\":51722,\"start\":51713},{\"end\":51740,\"start\":51722},{\"end\":51754,\"start\":51740},{\"end\":51975,\"start\":51962},{\"end\":51990,\"start\":51975},{\"end\":52004,\"start\":51990},{\"end\":52016,\"start\":52004},{\"end\":52028,\"start\":52016},{\"end\":52295,\"start\":52283},{\"end\":52308,\"start\":52295},{\"end\":52551,\"start\":52537},{\"end\":52567,\"start\":52551},{\"end\":52575,\"start\":52567},{\"end\":52586,\"start\":52575},{\"end\":52593,\"start\":52586},{\"end\":52833,\"start\":52822},{\"end\":52849,\"start\":52833},{\"end\":53015,\"start\":52992},{\"end\":53033,\"start\":53015},{\"end\":53052,\"start\":53033},{\"end\":53066,\"start\":53052},{\"end\":53292,\"start\":53281},{\"end\":53304,\"start\":53292},{\"end\":53320,\"start\":53304},{\"end\":53331,\"start\":53320},{\"end\":53346,\"start\":53331},{\"end\":53502,\"start\":53488},{\"end\":53514,\"start\":53502},{\"end\":53526,\"start\":53514},{\"end\":53538,\"start\":53526},{\"end\":53545,\"start\":53538}]", "bib_venue": "[{\"end\":36531,\"start\":36527},{\"end\":36729,\"start\":36725},{\"end\":37250,\"start\":37163},{\"end\":37936,\"start\":37874},{\"end\":38370,\"start\":38366},{\"end\":38766,\"start\":38711},{\"end\":39144,\"start\":39117},{\"end\":39412,\"start\":39408},{\"end\":39697,\"start\":39689},{\"end\":39987,\"start\":39983},{\"end\":40276,\"start\":40272},{\"end\":40643,\"start\":40581},{\"end\":40964,\"start\":40960},{\"end\":41150,\"start\":41146},{\"end\":41336,\"start\":41332},{\"end\":41637,\"start\":41633},{\"end\":42048,\"start\":41966},{\"end\":42382,\"start\":42378},{\"end\":42579,\"start\":42575},{\"end\":42834,\"start\":42830},{\"end\":43162,\"start\":43100},{\"end\":43507,\"start\":43503},{\"end\":43793,\"start\":43786},{\"end\":43990,\"start\":43986},{\"end\":44208,\"start\":44180},{\"end\":44473,\"start\":44466},{\"end\":44739,\"start\":44735},{\"end\":44974,\"start\":44970},{\"end\":45264,\"start\":45202},{\"end\":45609,\"start\":45605},{\"end\":45942,\"start\":45938},{\"end\":46196,\"start\":46192},{\"end\":46329,\"start\":46282},{\"end\":46598,\"start\":46588},{\"end\":46843,\"start\":46839},{\"end\":47068,\"start\":47064},{\"end\":47328,\"start\":47321},{\"end\":47558,\"start\":47542},{\"end\":47778,\"start\":47724},{\"end\":48169,\"start\":48165},{\"end\":48541,\"start\":48537},{\"end\":48816,\"start\":48812},{\"end\":49151,\"start\":49111},{\"end\":49501,\"start\":49497},{\"end\":49774,\"start\":49770},{\"end\":50047,\"start\":50043},{\"end\":50385,\"start\":50381},{\"end\":50683,\"start\":50679},{\"end\":50942,\"start\":50906},{\"end\":51223,\"start\":51219},{\"end\":51486,\"start\":51482},{\"end\":51758,\"start\":51754},{\"end\":52032,\"start\":52028},{\"end\":52312,\"start\":52308},{\"end\":52600,\"start\":52593},{\"end\":52853,\"start\":52849},{\"end\":53080,\"start\":53066},{\"end\":53350,\"start\":53346},{\"end\":53627,\"start\":53561}]"}}}, "year": 2023, "month": 12, "day": 17}