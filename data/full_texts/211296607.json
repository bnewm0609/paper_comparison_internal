{"id": 211296607, "updated": "2023-10-06 18:53:32.92", "metadata": {"title": "Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence", "authors": "[{\"first\":\"Nicolas\",\"last\":\"Loizou\",\"middle\":[]},{\"first\":\"Sharan\",\"last\":\"Vaswani\",\"middle\":[]},{\"first\":\"Issam\",\"last\":\"Laradji\",\"middle\":[]},{\"first\":\"Simon\",\"last\":\"Lacoste-Julien\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 24}, "abstract": "We propose a stochastic variant of the classical Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although computing the Polyak step-size requires knowledge of the optimal function values, this information is readily available for typical modern machine learning applications. Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive choice for setting the learning rate for stochastic gradient descent (SGD). We provide theoretical convergence guarantees for SGD equipped with SPS in different settings, including strongly convex, convex and non-convex functions. Furthermore, our analysis results in novel convergence guarantees for SGD with a constant step-size. We show that SPS is particularly effective when training over-parameterized models capable of interpolating the training data. In this setting, we prove that SPS enables SGD to converge to the true solution at a fast rate without requiring the knowledge of any problem-dependent constants or additional computational overhead. We experimentally validate our theoretical results via extensive experiments on synthetic and real datasets. We demonstrate the strong performance of SGD with SPS compared to state-of-the-art optimization methods when training over-parameterized models.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "2002.10542", "mag": "3007118755", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aistats/LoizouVLL21", "doi": null}}, "content": {"source": {"pdf_hash": "bc2fc5f394e709c78226ebac91f6e956781d9ef9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.10542v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9b2a8a4cd0ac7a7d78b46f37a505b97108367b36", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bc2fc5f394e709c78226ebac91f6e956781d9ef9.txt", "contents": "\nStochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence\n\n\nNicolas Loizou \nSharan Vaswani \nIssam Laradji \nSimon Lacoste-Julien \nStochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence\n\nWe propose a stochastic variant of the classical Polyak step-size (Polyak, 1987)  commonly used in the subgradient method. Although computing the Polyak step-size requires knowledge of the optimal function values, this information is readily available for typical modern machine learning applications. Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive choice for setting the learning rate for stochastic gradient descent (SGD). We provide theoretical convergence guarantees for SGD equipped with SPS in different settings, including strongly convex, convex and non-convex functions. Furthermore, our analysis results in novel convergence guarantees for SGD with a constant step-size. We show that SPS is particularly effective when training overparameterized models capable of interpolating the training data. In this setting, we prove that SPS enables SGD to converge to the true solution at a fast rate without requiring the knowledge of any problem-dependent constants or additional computational overhead. We experimentally validate our theoretical results via extensive experiments on synthetic and real datasets. We demonstrate the strong performance of SGD with SPS compared to state-of-the-art optimization methods when training over-parameterized models.\n\nIntroduction\n\nWe consider solving the finite-sum optimization problem:\nmin x\u2208R d f (x) = 1 n n i=1 f i (x) .(1)\nThis problem is prevalent in machine learning tasks where x corresponds to the model parameters, f i (x) represents the loss on the training point i and the aim is to minimize the average loss f (x) across training points. We denote X * \u2282 R d to be the set of optimal points x * of (1) and assume that X * is not empty. We use f * to denote the minimum value of f , obtained at a point x * \u2208 X * . Analogously, f * i denotes the (unconstrained) minimum value of the function f i for each i \u2208 {1, . . . , n}. Depending on the model under study, the function f can either be strongly-convex, convex, or non-convex.\n\n\nBackground and Main Contributions\n\nStochastic gradient descent (SGD) (Robbins & Monro, 1951;Nemirovski & Yudin, 1978;1983;Shalev-Shwartz et al., 2007;Nemirovski et al., 2009;Hardt et al., 2016), is the workhorse for training supervised machine learning problems that have the generic form (1).\n\nStep-size selection for SGD. The main parameter for guaranteeing the convergence of SGD is the step-size or the learning rate.  Gower et al. (2019) propose a non-asymptotic analysis of SGD with constant step-size for convex and strongly convex functions. For non-convex functions, such an analysis can be found in Ghadimi & Lan (2013); Bottou et al. (2018). Using a constant step-size for SGD guarantees convergence to a neighbourhoood of the solution. A common technique to guarantee convergence to the exact optimum is to use a decreasing step-size (Robbins & Monro, 1951;Ghadimi & Lan, 2013;Gower et al., 2019;Nemirovski et al., 2009;Karimi et al., 2016). More recently, adaptive methods (Duchi et al., 2011;Liu et al., 2019;Kingma & Ba, 2015;Bengio, 2015;Vaswani et al., 2019b;Li & Orabona, 2019;Ward et al., 2019) that adjust the step-size on the fly have become wide-spread and are particularly beneficial when training deep neural networks.\n\nContributions: Inspired by the classical Polyak step-size (Polyak, 1987) commonly used with the deterministic subgradient method (Hazan & Kakade, 2019;Boyd et al., 2003), we propose a novel adaptive learning rate for SGD. The proposed step-size is a natural extension of the Polyak step-size to the stochastic setting. We name it stochastic Polyak step-arXiv:2002.10542v1 [math.OC] 24 Feb 2020 size (SPS). Although computing SPS requires knowledge of the optimal function values f * i ; we argue that this information is readily available for modern machine learning applications (for example is zero for most standard losses), making SPS an attractive choice for SGD.\n\nIn Section 3, we provide theoretical guarantees for the convergence of SGD with SPS in different scenarios including strongly convex, convex and non-convex smooth functions. Although SPS is provably larger than the typically used constant step-size, we guarantee its convergence to a reasonable neighborhood around the optimum. We establish a connection between SPS and the optimal step-size used in sketch and project methods for solving linear systems. Furthermore, in Appendix C, we provide convergence guarantees for convex non-smooth functions. We also show that by progressively increasing the batch-size for computing the stochastic gradients, SGD with SPS converges to the optimum.\n\nTechnical assumptions for proving convergence. Besides smoothness and convexity, several papers (Shamir & Zhang, 2013;Recht et al., 2011;Hazan & Kale, 2014;Rakhlin et al., 2012) assume that the variance of the stochastic gradient is bounded; that is there exists a c such that E i \u2207f i (x) 2 \u2264 c. However, in the unconstrained setting, this assumption contradicts the assumption of strong convexity (Nguyen et al., 2018;Gower et al., 2019). In another line of work, growth conditions on the stochastic gradients have been used to guarantee convergence. In particular, the weak growth condition has been used in (Bertsekas & Tsitsiklis, 1996;Bottou et al., 2018;Nguyen et al., 2018). It states that there exist constants \u03c1, \u03b4 such that E i \u2207f i (x) 2 \u2264 \u03c1 E \u2207f (x) 2 + \u03b4. Its stronger variant (strong growth condition) when \u03b4 = 0 has been used in several recent papers (Schmidt & Roux, 2013;Cevher & V\u0169, 2019;Vaswani et al., 2019a;. These conditions can be relaxed to the expected smoothness assumption recently used in (Gower et al., 2019).\n\nContributions: Our analysis of SGD with SPS does not require any of these additional assumptions for guaranteeing convergence 1 . We also note that our theoretical results do not require the finite-sum assumption and can be easily adapted to the streaming setting.\n\nNovel analysis for constant SGD. In the existing analyses of constant step-size SGD, the neighborhood of convergence depends on the variance of the gradients at the optimum, z 2 := E i \u2207f i (x * ) 2 which is assumed to be finite.\n\n\nContributions:\n\nThe proposed analysis of SGD with SPS gives a novel way to analyze constant step-size SGD. In particular, we prove that for both SGD with SPS and constant step-size SGD (without SPS), the neighbourhood of convergence depends on \u03c3 2 := f (x * ) \u2212 E[f * i ] < \u221e (finite optimal objective difference). For modern machine learning applications that use functions lower-bounded by zero and for which SGD can achieve small risk f (x * ), the neighbourhood \u03c3 2 can be much smaller than the z 2 (Zhang & Zhou, 2019).\n\nOver-parametrized models and interpolation condition. Modern machine learning models such as non-parametric regression or over-parametrized deep neural networks are highly expressive and can fit or interpolate the training dataset completely (Zhang et al., 2016;Ma et al., 2018). In this setting, SGD with constant step-size can been shown to converge to the exact optimum at the deterministic rate (Schmidt & Roux, 2013;Ma et al., 2018;Vaswani et al., 2019a;Gower et al., 2019;Berrada et al., 2019).\n\nContributions: As a corollary of our theoretical results, we show that SPS is particularly effective under this interpolation setting. Specifically, we prove that SPS enables SGD to converge to the true solution at a fast rate matching the deterministic case. Moreover, SPS does not require the knowledge of any problem-dependent constants or additional computational overhead.\n\nExperimental Evaluation. In Section 4, we experimentally validate our theoretical results via experiments on synthetic datasets. We also evaluate the performance of SGD equipped with SPS relative to the state-of-the-art optimization methods when training over-parameterized models for deep matrix factorization, binary classification using kernels and multi-class classification using deep neural networks. For each of these tasks, we demonstrate the superior convergence of the proposed method.\n\n\nSGD and the Stochastic Polyak Step-size\n\nThe optimization problem (1) can be solved using SGD:\nx k+1 = x k \u2212 \u03b3 k \u2207f i (x k )(2)\nwhere example i \u2208 [n] is chosen uniformly at random and \u03b3 k > 0 is the step-size in iteration k.\n\n\nThe Polyak step-size\n\nBefore explaining the proposed stochastic Polyak step-size, we first present the deterministic variant by Polyak (Polyak, 1987). This variant is commonly used in the analysis of deterministic subgradient methods (Boyd et al., 2003;Hazan & Kakade, 2019).\n\nThe deterministic Polyak step-size. For convex functions, the deterministic Polyak step-size at iteration k is the one that minimizes an upper-bound Q(\u03b3) on the distance of the iterate x k+1 to the optimal solution: x k+1 \u2212 x * 2 2 \u2264 Q(\u03b3):\nQ(\u03b3) = x k \u2212 x * 2 \u2212 2\u03b3 f (x k ) \u2212 f * ) + \u03b3 2 g k 2 .\nThat is,\n\u03b3 k = argmin \u03b3 [Q(\u03b3 k )] = f (x k )\u2212f * g k 2 .\nHere g k denotes a subgradient of function f at point x k and f * the optimum function value. For more details and a convergence analysis of the deterministic subgradient method, please check Appendix A.2. Note that the above step-size can be used only when the optimal value f * is known, however Boyd et al. (2003) demonstrate that f * = 0 for several applications (for example, finding a point in the intersection of convex sets, positive semidefinite matrix completion and solving convex inequalities).\n\n\nStochastic Polyak\n\nStep-size. It is clear that using the deterministic Polyak step-size in the update rule of SGD is impractical. It requires the computation of the function value f and its full gradient in each iteration.\n\nTo avoid this, we propose the stochastic Polyak step-size (SPS) for SGD:\nSPS: \u03b3 k = f i (x k ) \u2212 f * i c \u2207f i (x k ) 2(3)\nNote that SPS requires the evaluation of only the stochastic gradient \u2207f i (x k ) and of the function f i (x k ) at the current iterate (quantities that can be computed in the update rule of SGD without further cost). However, it requires the knowledge of f * i . As we will see in Section 4, for typical machine learning applications such as empirical risk minimization where f i is the loss on a training example, the optimal values f * i = 0. An important quantity in the step-size is the parameter 0 < c \u2208 R which can be set theoretically based on the properties of the function under study. For example, for strongly convex functions, one should select c = 1/2 for optimal convergence. Thus, if the function is known to be strongly convex, c is not a hyper-parameter to be tuned.\n\nIn addition to SPS, in some of our convergence results we require its bounded variant:\nSPS max : \u03b3 k = min f i (x k ) \u2212 f * i c \u2207f i (x k ) 2 , \u03b3 b(4)\nHere \u03b3 b > 0 is a bound that restricts SPS from being very large and is essential to ensure convergence to a small neighborhood around the solution. If \u03b3 b = \u221e then SPS max is equivalent to SPS.\n\nClosely related work. We now briefly compare against the recently proposed stochastic variants of the Polyak stepsize (Rolinek & Martius, 2018;Oberman & Prazeres, 2019;Berrada et al., 2019). In Section 3, we present a detailed comparison of the theoretical convergence rates.\n\nIn Rolinek & Martius (2018), the L4 algorithm has been proposed showing that a stochastic variant of the Polyak step for SGD achieves good empirical results for training neural networks. However it has no theoretical convergence guarantees. The step-size is very similar to SPS (3) but each update requires an online estimation of the f * i which does not result in robust empirical performance and requires up to three hyper-parameters.\n\nOberman & Prazeres (2019) use a different variant of the stochastic Polyak step-size:\n\u03b3 k = 2[f (x k )\u2212f * ]\nEi \u2207fi(x k ) 2 . This stepsize requires knowledge of the quantity E i \u2207f i (x k ) 2 for all iterates x k and the evaluation of f (x k ) in each step, making it impractical for finite-sum problems with large n. Moreover, their theoretical results focus only on strongly convex smooth functions.\n\nIn the ALI-G algorithm proposed by Berrada et al. (2019), the step-size is set as: \u03b3 k = min\nfi(x k )\n\u2207fi(x k ) 2 +\u03b4 , \u03b7 , where \u03b4 > 0 is a positive constant. Unlike our setting, their theoretical analysis relies on an -interpolation condition. Moreover, the values of the parameter \u03b4 and \u03b7 that guarantee convergence heavily depend on the smoothness parameter of the objective f , limiting the method's practical applicability. In Section 3, we show that as compared to (Berrada et al., 2019), the proposed method results in both better rates and a smaller neighborhood of convergence. For the case of over-parameterized models, our step-size selection guarantees convergence to the exact solution while the step proposed in (Berrada et al., 2019) finds only an approximate solution that could be \u03b4 away from the optimum. In Section 4, we experimentally show that SPS max results in better convergence than ALI-G in several practical scenarios.\n\n\nOptimal Objective Difference\n\nUnlike the typical analysis of SGD that assumes a finite gradient noise z 2 := E[ \u2207f i (x * ) 2 ], in all our results, we assume a finite optimal objective difference.\n\nAssumption 2.1 (Finite optimal objective difference).\n\u03c3 2 := E i [f i (x * ) \u2212 f * i ] = f (x * ) \u2212 E i [f * i ](5)\nThis is a very weak assumption. Moreover when (1) is the training problem of an over-parametrized model such as a deep neural network or involves solving a consistent linear system or classification on linearly separable data, each individual loss function f i attains its minimum at x * , and thus f i (x * ) \u2212 f * i = 0. In this interpolation setting, it follows that \u03c3 = 0.\n\n\nConvergence Analysis\n\nIn this section, we present the main convergence results. We quantify the convergence rates of SGD with the stochastic Polyak step-size for strongly convex, convex and nonconvex functions. For the formal definitions and properties of functions see Appendix A.1. Proofs of all key results can be found in the Appendix B.\n\n\nUpper and Lower Bounds of SPS\n\nIf a function g is \u00b5-strongly convex and L-smooth the following bounds hold: 1 2L \u2207g(x) 2 \u2264 g(x) \u2212 g(x * ) \u2264 1 2\u00b5 \u2207g(x) 2 . Using these bounds and by assuming that the functions f i in problem (1) are \u00b5 i -strongly convex and L i -smooth, it is straight forward to see that SPS can be lower and upper bounded as follows:\n1 2cL max \u2264 1 2cL i \u2264 \u03b3 k = f i (x k ) \u2212 f * i c \u2207f i (x k ) 2 \u2264 1 2c\u00b5 i ,(6)\nwhere L max = max{L i } n i=1 .\n\n\nConvex Functions\n\nWe present three main theoretical results for convex functions, the first when f is a strongly convex function, the second result for weakly convex functions and the third result that establishes a connection between SPS and randomized methods for solving linear systems.\n\n\nSUM OF STRONGLY CONVEX AND CONVEX\n\n\nFUNCTIONS\n\nIn this section, we assume that at least one of the components f i is \u00b5 i strongly convex function, implying that the function f is \u00b5-strongly convex.\n\nTheorem 3.1. Let f i be L i -smooth convex functions with at least one of them being a strongly convex function. SGD with SPS max with c \u2265 1/2 converges as:\nE x k \u2212 x * 2 \u2264 (1 \u2212\u03bc\u03b1) k x 0 \u2212 x * 2 + 2\u03b3 b \u03c3 2 \u00b5\u03b1 ,(7)\nwhere \u03b1 := min{ 1 2cLmax , \u03b3 b },\u03bc = E[\u00b5 i ] is the average strong-convexity of the finite sum and L max = max{L i } n i=1 is the maximum smoothness constant. The best convergence rate and the tightest neighborhood are obtained for c = 1/2. Note that in Theorem 3.1, we do not make any assumption on the value of the upper bound \u03b3 b . However, it is clear that for convergence to a small neighborhood of the solution x * (unique solution for strongly convex functions) \u03b3 b should not be very large 2 .\n\nAnother important aspect of Theorem 3.1 is that it provides convergence guarantees without requiring strong assumptions like bounded gradients or growth conditions. We do not use these conditions because SPS provides a natural bound on the norm of the gradients. In the following corollaries we make additional assumptions to better understand the convergence of SGD with SPS max .\n\nIn our first corollary, we assume that our model is able to 2 Note that neighborhood interpolate the data (each individual loss function f i attains its minimum at x * ). The interpolation assumption enables us to guarantee the convergence of SGD with SPS, without an upper-bound on the step-size (\u03b3 b = \u221e).\n\nCorollary 3.2. Assume interpolation (\u03c3 = 0) and let all assumptions of Theorem 3.1 be satisfied. SGD with SPS with c = 1/2 (optimal choice) converges as:\nE x k \u2212 x * 2 \u2264 1 \u2212\u03bc L max k x 0 \u2212 x * 2 .\nWe compare the convergence rate in Corollary 3.2 to that of stochastic line search (SLS) proposed in (Vaswani et al., 2019b). In the same setting as Corollary 3.2, SLS achieves a slower linear rate with a worse constant max 1 \u2212\u03bc Lmax , 1 \u2212 \u03b3 b\u03bc .\n\nIn the next corollary, in order to compare against the results for ALI-G from Berrada et al. (2019), we make the strong assumption that all functions f i have the same properties.\n\nWe note that such an assumption in the interpolation setting is quite strong and reduces the finite-sum optimization to minimization of a single function in the finite sum.\n\nCorollary 3.3. Let all the assumptions in Theorem 3.1 be satisfied and let all f i be \u00b5-strongly convex and L-smooth. SGD with SPS max with c = 1/2 converges as:\nE x k \u2212 x * 2 \u2264 1 \u2212 \u00b5 L k x 0 \u2212 x * 2 + 2\u03c3 2 L \u00b5 2 .\nFor the interpolated case we obtain the same convergence as Corollary 3.2 with\u03bc = \u00b5 and L max = L.\n\nNote that, the result of Corollary 3.3 is obtained by substi- (7). For the setting of Corollary 3.3, Berrada et al. (2019) show the linear convergence to a much larger neighborhood than ours and with slower rate. In particular, their rate is 1 \u2212 \u00b5 8L and the neighborhood is 8L\ntuting \u03b3 b (6) = 1 2c\u00b5 c= 1 2 = 1 \u00b5 into\u00b5 ( L + \u03b4 4L 2 + 2\u00b5 ) where \u03b4 > 2L and is the -interpolation parameter > max i [f i (x * )\u2212f * i ]\nwhich by definition is bigger than \u03c3 2 . Under interpolation where \u03c3 = 0, our method converges linearly to the x * while the algorithm proposed by Berrada et al. (2019) still converges to a neighborhood that is proportional to the parameter \u03b4.\n\nAn interesting outcome of Theorem 3.1 is a novel analysis for SGD with a constant step-size. In particular, note that if the bound in SPS max is selected to be \u03b3 b \u2264 1 2cLmax , then using the lower bound of (6), it can be easily shown that our method reduces to SGD with constant step-size \u03b3 k = \u03b3 = \u03b3 b \u2264 1 2cLmax . In this case, we obtain the following convergence rate.\n\nCorollary 3.4. Let all assumptions of Theorem 3.1 be satisfied. SGD with SPS max with c = 1/2 and \u03b3 b \u2264 1 Lmax becomes SGD with constant step-size \u03b3 \u2264 1 Lmax and converges as:\nE x k \u2212 x * 2 \u2264 (1 \u2212\u03bc\u03b3) k x 0 \u2212 x * 2 + 2\u03c3 2 \u00b5 .\nIf we further assume interpolation (\u03c3 = 0), the iterates of SGD with constant step-size \u03b3 \u2264 1 Lmax satisfy:\nE x k \u2212 x * 2 \u2264 (1 \u2212\u03bc\u03b3) k x 0 \u2212 x * 2 .\nTo the best of our knowledge, ours is the first result that shows convergence of constant step-size SGD to a neighborhood that depends on the optimal objective difference \u03c3 2 (5) and not on the variance\nz 2 = E[ \u2207f i (x * ) 2 . If we assume\nthat all function f i are \u00b5-strongly convex and L-smooth functions then the two notions of variance satisfy the following connection: 1 2L z 2 \u2264 \u03c3 2 \u2264 1 2\u00b5 z 2 . Such convergence results to the neighborhood \u03c3 2 have only been recently suggested in two papers but for different algorithms than ours. In particular, Zhang & Zhou (2019) \n\n\nSUM OF CONVEX FUNCTIONS\n\nIn this section, we derive the convergence rate when all component functions f i are convex without any strong convexity and obtain the following theorem.\n\nTheorem 3.5. Assume that f i are convex, L i -smooth functions. SGD with SPS max with c = 1 converges as:\nE f (x k ) \u2212 f (x * ) \u2264 x 0 \u2212 x * 2 \u03b1 K + \u03c3 2 \u03b3 b \u03b1 .\nHere \u03b1 = min\n1 2cLmax , \u03b3 b andx K = 1 K K\u22121 k=0 x k .\nAnalogous to the strongly-convex case, the size of the neighbourhood is proportional to \u03b3 b . When interpolation is satisfied and \u03c3 = 0, we observe that the unbounded variant of SPS with \u03b3 b = \u221e converges to the optimum at a O(1/K) rate. This rate is faster than the rates in (Vaswani et al., 2019b;Berrada et al., 2019) and we refer the reader to the Appendix for a detailed comparison. As in the stronglyconvex case, by setting \u03b3 b \u2264 1 2cLmax , we obtain the convergence rate obtained by constant step-size SGD.\n\n\nCONSISTENT LINEAR SYSTEMS\n\nIn (Richt\u00e1rik & Tak\u00e1\u010d, 2017), given the consistent linear system Ax = b, the authors provide a stochastic optimiza-tion reformulation of the form (1) which is equivalent to the linear system in the sense that their solution sets are identical. That is, the set of minimizers of the stochastic optimization problem X * is equal to the set of solutions of the stochastic linear system L := {x : Ax = b}. An interesting property of this stochastic optimization problem\nis that 3 :f i (x) \u2212 f * i f * i =0 = f i (x) = 1 2 \u2207f i (x) 2 \u2200x \u2208 R d .\nUsing the special structure of the problem, SPS (3) with c = 1/2 takes the following form: \u03b3 k\n(3) = 2[fi(x k )\u2212f * i ] \u2207fi(x k ) 2 = 1,\nwhich is the theoretically optimal constant step-size for SGD in this setting (Richt\u00e1rik & Tak\u00e1\u010d, 2017). This reduction implies that SPS results in an optimal convergence rate when solving consistent linear systems. We provide the convergence rate for SPS in this setting in Appendix B.\n\n\nNon-convex functions\n\nIn this section we present the convergence of SGD with SPS when the component functions f i are L i -smooth but not necessarily convex.\n\n\nSUM OF NON-CONVEX FUNCTIONS: PL OBJECTIVE\n\nWe first focus on a special class of non-convex functions that satisfy the Polyak-Lojasiewicz (PL) condition (Polyak, 1987;Karimi et al., 2016). In particular, we assume that function f satisfies the PL condition but do not assume convexity of the component functions f i . The function f satisfies the PL condition if there exists \u00b5 > 0 such that:\n\u2207f (x) 2 \u2265 2\u00b5(f (x) \u2212 f * ).\nTheorem 3.6. Assume that function f satisfies the PL condition with parameter \u00b5, and let f and f i be smooth functions. SGD with SPS max with c > Lmax 4\u00b5 and \u03b3 b \u2265 1 2cLmax converges as:\nE[f (x k ) \u2212 f (x * )] \u2264 \u03bd k [f (x 0 ) \u2212 f (x * )] + L\u03c3 2 \u03b3 b 2(1 \u2212 \u03bd) c where \u03bd = \u03b3 b 1 \u03b1 \u2212 2\u00b5 + Lmax 2c \u2208 (0, 1] and \u03b1 = min 1 2cLmax , \u03b3 b .\nUnder the interpolation setting, \u03c3 = 0, and SPS max converges to the optimal solution at a linear rate. If \u03b3 b \u2264 min 1 2cLmax , 2c 4\u00b5c\u2212Lmax using the lower bound in (6), the analyzed method becomes the SGD with constant step-size and we obtain the following corollary.\n\nCorollary 3.7. Assume that function f satisfies the PL condition and let f and f i be smooth functions. SGD with constant step-size \u03b3 k = \u03b3 \u2264 \u00b5 L 2 max converges as:\nE[f (x k ) \u2212 f (x * )] \u2264 \u03bd k [f (x 0 ) \u2212 f (x * )] + L\u03c3 2 \u03b3 2(1 \u2212 \u03bd) c .\nTo the best of our knowledge this is the first result for the convergence of SGD for PL functions without assuming bounded gradient or bounded variance (for more details see results in (Karimi et al., 2016) and discussion in (Gower et al., 2019)). In the interpolation case, we obtain linear convergence to the optimum with a constant step-size equal to that used in (Vaswani et al., 2019a)\n\n\nGENERAL NON-CONVEX FUNCTIONS\n\nIn this section, we assume a common condition used to prove convergence of SGD in the non-convex setting (Bottou et al., 2018).\nE[ \u2207f i (x) 2 ] \u2264 \u03c1 \u2207f (x) 2 + \u03b4(8)\nwhere \u03c1, \u03b4 > 0 constants.\n\nTheorem 3.8. Let f and f i be smooth functions and assume that there exist \u03c1, \u03b4 > 0 such that the condition\n(8) is satisfied. SGD with SPS max with c > \u03c1L 4Lmax and \u03b3 b < max 2 L\u03c1 , 1 \u221a \u03c1c LLmax\nconverges as:\nmin k\u2208[K] E \u2207f (x k ) 2 \u2264 f (x 0 ) \u2212 f (x * ) \u03b1 K + L\u03b4\u03b3 2 b 2\u03b1 where \u03b2 1 = 1 \u2212 \u03c1c LLmax \u03b3 2 b 2 and \u03b2 2 = 1 \u2212 \u03c1L \u03b3 b 2 , \u03b1 = min \u03b21 2cLmax , \u03b3 b \u03b2 2 .\nFrom the above theorem, we observe that SGD with SPS results in O(1/K) convergence to a value governed by \u03b4. For the case that \u03b4 = 0, condition (8) reduces to the strong growth condition (SGC) used in several recent papers (Schmidt & Roux, 2013;Vaswani et al., 2019b;a). It can be easily shown that functions that satisfy the SGC condition necessarily satisfy the interpolation property (Vaswani et al., 2019a). In the special case of interpolation, SGD with SPS is able to find a first-order stationary point as efficiently as deterministic gradient descent. Moreover, for c \u2208 \u03c1L 4Lmax , \u03c1L 2Lmax , SPS lies in the range 1 \u03c1L , 2 \u03c1L and thus uses a step-size larger than 1 \u03c1L , the best constant stepsize analyzed in this setting (Vaswani et al., 2019a).\n\n\nAdditional Convergence Results\n\nIn Appendix C, we present some additional convergence results of SGD with SPS. In particular, we prove a O(1/ \u221a K) convergence rate for non-smooth convex functions. Furthermore, similar to (Schmidt et al., 2011), we propose a way to increase the mini-batch size for evaluating the stochastic gradient and guarantee convergence to the optimal solution without interpolation.  (1) sps_max (5) sps_max (100) sgd (1e-1) sgd (1e-2) Figure 1. Synthetic experiment to benchmark SPS against constant step-size SGD for binary classification using the (left) regularized and (right) unregularized logistic loss.\n\n\nExperimental Evaluation\n\nWe validate our theoretical results using synthetic experiments in section 4.1. In section 4.2, we evaluate the performance of SGD with SPS when training over-parametrized models. In particular, we compare against state-of-the-art optimization methods for deep matrix factorization, binary classification using kernel methods and multi-class classification using standard deep neural network models.\n\n\nSynthetic experiments\n\nWe use a synthetic dataset to validate our theoretical results. We use the logistic loss with and without 2 regularization. The data is generated to ensure that the function f is strongly convex in both cases. We evaluate the performance of SPS max and set set c = 1/2 as suggested by theorem 3.1. We experiment with three values of \u03b3 b = {1, 5, 100}. In the regularized case, we first compute the value of f * i for each of the examples and use it to compute the step-size. For the unregularized case, note that the logistic loss is lower-bounded by zero and since the model can correctly classify each point individually, the optimum function value f * i = 0. A similar observation has been used to construct a \"truncated\" model for improving the robustness of gradient descent in (Asi & Duchi, 2019). In both cases, we benchmark the performance of SPS against constant stepsize SGD with \u03b3 = {0.1, 0.01}. From figure 1, we observe that constant step-size SGD is not robust to the step-size; it has good convergence with step-size 0.1, slow convergence when using a step-size of 0.01 and we observe divergence for larger step-sizes. In contrast, all the variants of SPS converge to a neighbourhood of the optimum and the size of the neighbourhood increases as \u03b3 b increases as predicted Step-size (log) CIFAR100 -ResNet34 radam adam ali g lookahead sls sps_max by the theory.\n\n\nExperiments for over-parametrized models\n\nIn this section, we consider training over-parameterized models that (approximately) satisfy the interpolation condition. Following the logic of the previous section, we evaluate the performance of both the SPS and SPS max variants with f * i = 0. Throughout our experiments, we found that SPS without an upper-bound on the step-size is not robust to the misspecification of interpolation and results in large fluctuations when interpolation is not exactly satisfied. For SPS max , the value of \u03b3 b that results in good convergence depends on the problem and requires careful parameter tuning. This is also evidenced by the highly variable performance of ALI-G (Berrada et al., 2019) that uses a constant upperbound on the step-size. To alleviate this problem, we use a smoothing procedure that prevents large fluctuations in the step-size across iterations. This can be viewed as using an adaptive iteration-dependent upper-bound\n\u03b3 k b where \u03b3 k b = \u03c4 b/n \u03b3 k\u22121 .\nHere, \u03c4 is a tunable hyper-parameter set to 2 in all our experiments, b is the batch-size and n is the number of examples. We note that using an adaptive \u03b3 b can be easily handled by our theoretical results. A similar smoothing procedure has been used to control the magnitude of the step-sizes when using the Barzilai-Borwein step-size selection procedure for SGD (Tan et al., 2016) and is related to the \"reset\" option for using larger step-sizes in (Vaswani et al., 2019b). We set c = 1/2 for binary classification using kernels (convex case) and deep matrix factorization (nonconvex PL case). For multi-class classification using deep networks, we empirically find that any value of c \u2265 0.2 results in convergence. In this case, we observed that across models and datasets, the fastest convergence is obtained with c = 0.2 and use this value.\n\nWe compare our methods against Adam (Kingma & Ba, 2015), which is the most common adaptive method, and other recent methods that report better performance than To ensure a fair comparison with SPS, we do not use momentum for the competing methods. We use the default learning rates and the publicly available code for the competing methods. All our results are averaged across 5 independent runs.\n\nDeep matrix factorization. In the first experiment, we use deep matrix factorization to examine the effect of over-parametrization for the different optimizers. In particular, we solve the non-convex regression problem: min W1,W2 E x\u223cN (0,I) W 2 W 1 x \u2212 Ax 2 and use the experimental setup in (Rolinek & Martius, 2018;Vaswani et al., 2019b;Rahimi & Recht, 2017). We choose A \u2208 R 10\u00d76 with condition number \u03ba(A) = 10 10 and generate a fixed dataset of 1000 samples. We control the degree of overparametrization via the rank k of the matrix factors W 1 \u2208 R k\u00d76 and W 2 \u2208 R 10\u00d7k . In figure 2, we show the training loss as we vary the rank k \u2208 {4, 10} (additional experiments are in Appendix D). For k = 4, the interpolation condition is not satisfied, whereas it is exactly satisfied for k = 10. We observe that (i) SPS is robust to the degree of over-parametrization and (ii) has performance equal to that of SLS. However, note that SPS does not require the expensive back-tracking procedure of SLS and is arguably simpler to implement.\n\nBinary classification using kernels. Next, we compare the optimizers' performance in the convex, interpolation regime. We consider binary classification using RBF kernels, using the logistic loss without regularization. The bandwidths for the RBF kernels are set according to the validation procedure described in (Vaswani et al., 2019b). We experiment with four standard datasets: mushrooms, rcv1, ijcnn, and w8a from LIBSVM (Chang & Lin, 2011). Figure 2 shows the training loss on the mushrooms and ijcnn for the different optimizers. Again, we observe the strong performance of SPS compared to the other optimizers.\n\nMulti-class classification using deep networks. We benchmark the convergence rate and generalization performance of SPS methods on standard deep learning experiments. We consider non-convex minimization for multiclass classification using deep network models on the CI-FAR10 and CIFAR100 datasets. Our experimental choices follow the setup in Luo et al. (Luo et al., 2019). For CI-FAR10 and CIFAR100, we experiment with the standard image-classification architectures: ResNet-34 (He et al., 2016) and DenseNet-121 (Huang et al., 2017). For space concerns, we report only the ResNet experiments in the main paper and relegate the DenseNet and MNIST experiments to Appendix D. From figure 2, we observe that SPS results in the best training loss across models and datasets. For CIFAR-10, SPS results in competitive generalization performance compared to the other optimizers, whereas for CIFAR-100, its generalization performance is better than all optimizers except SLS. Note that ALI-G, the closest related optimizer results in worse generalization performance in all cases. We note that SPS is able to match the performance of SLS, but does not require a expensive back-tracking linesearch or additional tricks.\n\nFor this set of experiments, we plot how the step-size varies across iterations for SLS, SPS and ALI-G. Interestingly, for both CIFAR-10 and CIFAR-100, we find that step-size for both SPS and SLS follows a cyclic behaviour -a warm-up period where the step-size first increases and then decreases to a constant value. Such a step-size schedule has been empirically found to result in good training and generalization performance (Loshchilov & Hutter, 2016) and our results show that SPS is able to simulate this behaviour.\n\n\nConclusion\n\nWe proposed and theoretically analyzed a stochastic variant of the classical the Polyak step-size. We quantified the convergence rate of SPS in numerous settings and used our analysis techniques to prove new results for constant step-size SGD. Furthermore, via experiments on a variety of tasks we showed the strong performance of SGD with SPS as compared to state-of-the-art optimization methods. There are many possible interesting extensions of our work: using SPS with accelerated methods, studying the effect of mini-batching and non-uniform sampling techniques and extensions to the distributed and decentralized settings. \n\n\nSupplementary Material Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence\n\nThe Supplementary Material is organized as follows: In Section A, we provide the basic definitions mentioned in the main paper. We also present the convergence of deterministic subgradient method with the classical Polyak step-size. In Section B we present the proofs of the main Theorems and in Section C we provide additional convergence results. Finally, additional numerical experiments are presented in Section D.\n\n\nA Technical Preliminaries\n\n\nA.1 Basic Definitions\n\nLet us present some basic definitions used throughout the paper.\n\nDefinition A.1 (Strong Convexity / Convexity). The function f : R n \u2192 R, is \u00b5-strongly convex, if there exists a constant \u00b5 > 0 such that \u2200x, y \u2208 R n :\nf (x) \u2265 f (y) + \u2207f (y), x \u2212 y + \u00b5 2 x \u2212 y 2(9)\nfor all x \u2208 R d . If inequality (9) holds with \u00b5 = 0 the function f is convex.\nDefinition A.2 (Polyak-Lojasiewicz Condition)\n. The function f : R n \u2192 R, satisfies the Polyak-Lojasiewicz (PL) condition, if there exists a constant \u00b5 > 0 such that \u2200x \u2208 R n :\n\u2207f (x) 2 \u2265 2\u00b5(f (x) \u2212 f * )(10)\nDefinition A.3 (L-smooth). The function f : R n \u2192 R, L-smooth, if there exists a constant L > 0 such that \u2200x, y \u2208 R n :\n\u2207f (x) \u2212 \u2207f (y) \u2264 L x \u2212 y(11)\nor equivalently:\nf (x) \u2264 f (y) + \u2207f (y), x \u2212 y + L 2 x \u2212 y 2(12)\n\nA.2 The Deterministic Polyak step-size\n\nIn this section we describe the Polyak step-size for the subgradient method as presented in (Polyak, 1987) for solving min x\u2208R d f (x) where f is convex, not necessarily smooth function.\n\nConsider the subgradient method:\nx k+1 = x k \u2212 \u03b3 k g k ,\nwhere \u03b3 k is the step-size (learning rate) and g k is any subgradient of function f at point x k .\nTheorem A.4. Let f be convex function. Let \u03b3 k = f (x k )\u2212f (x * ) g k 2\nbe the step-size in the update rule of subgradient method. Here f (x * ) denotes the optimum value of function f . Let G > 0 such that g k 2 < G 2 . Then,\nf k * \u2212 f (x * ) \u2264 G x 0 \u2212 x * \u221a k + 1 = O 1 \u221a k ,\nwhere f k * = min{f (x i ) : i = 0, 1, . . . , k}.\n\nProof.\nx k+1 \u2212 x * 2 = x k \u2212 \u03b3 k g k \u2212 x * 2 = x k \u2212 x * 2 \u2212 2\u03b3 k x k \u2212 x * , g k + \u03b3 2 k g k 2 \u2264 x k \u2212 x * 2 \u2212 2\u03b3 k f (x k ) \u2212 f (x * ) + \u03b3 2 k g k 2 (13)\nwhere the last line follows from the definition of subgradient:\nf (x * ) \u2265 f (x k ) + x k \u2212 x * , g k\nPolyak suggested to use the step-size:\n\u03b3 k = f (x k ) \u2212 f (x * ) g k 2(14)\nwhich is precisely the step-size that minimize the right hand side of (13). That is,\n\u03b3 k = f (x k ) \u2212 f (x * ) g k 2 = argmin \u03b3 k x k \u2212 x * 2 \u2212 2\u03b3 k f (x k ) \u2212 f (x * ) + \u03b3 2 k g k 2 .\nBy using this choice of step-size in (13) we obtain:\nx k+1 \u2212 x * 2 \u2264 x k \u2212 x * 2 \u2212 2\u03b3 k f (x k ) \u2212 f (x * ) + \u03b3 2 k g k 2 (14) = x k \u2212 x * 2 \u2212 f (x k ) \u2212 f (x * ) 2 g k 2(15)\nFrom the above note that x k \u2212 x * 2 is monotonic function. Now using telescopic sum and by assuming g k 2 < G 2 we obtain:\nx k+1 \u2212 x * 2 \u2264 x 0 \u2212 x * 2 \u2212 1 G 2 k i=0 f (x i ) \u2212 f (x * ) 2 (16) Thus, 1 G 2 k i=0 f (x i ) \u2212 f (x * ) 2 \u2264 x 0 \u2212 x * 2 \u2212 x k+1 \u2212 x * 2 \u2264 x 0 \u2212 x * 2\nLet us define f k * = min{f (x i ) : i = 0, 1, . . . , k} then:\n[f k * \u2212 f (x * )] 2 \u2264 G 2 x 0 \u2212x * 2 k+1 and f k * \u2212 f (x * ) \u2264 G x 0 \u2212 x * \u221a k + 1 = O 1 \u221a k\nFor more details and slightly different analysis check (Polyak, 1987) and (Boyd et al., 2003). In (Hazan & Kakade, 2019) similar analysis to the above have been made for the deterministic gradient descent (g k = \u2207f (x k )) under several assumptions. (convex, strongly convex , smooth).\n\n\nB Proofs of Main Results\n\nIn this section we present the proofs of the main theoretical results presented in the main paper. That is, the convergence analysis of SGD with SPS max and SPS under different combinations of assumptions on functions f i and f of Problem (1).\n\nFirst note that the following inequality can be easily obtained by the definition of SPS max (4):\n\u03b3 2 k \u2207f i (x k ) 2 \u2264 \u03b3 k c f i (x k ) \u2212 f * i(17)\nWe use the above inequality in several parts of our proofs. It is the reason that we are able to obtain an upper bound of \u03b3 2 k \u2207f i (x k ) 2 without any further assumptions. For the case of SPS (3), inequality (17) becomes equality.\n\nProof.\nx k+1 \u2212 x * 2 = x k \u2212 \u03b3 k \u2207f i (x k ) \u2212 x * 2 = x k \u2212 x * 2 \u2212 2\u03b3 k x k \u2212 x * , \u2207f i (x k ) + \u03b3 2 k \u2207f i (x k ) 2 strong convexity \u2264 (1 \u2212 \u00b5 i \u03b3 k ) x k \u2212 x * 2 \u2212 2\u03b3 k f i (x k ) \u2212 f i (x * ) + \u03b3 2 k \u2207f i (x k ) 2 (17) \u2264 (1 \u2212 \u00b5 i \u03b3 k ) x k \u2212 x * 2 \u2212 2\u03b3 k f i (x k ) \u2212 f i (x * ) + \u03b3 k c f i (x k ) \u2212 f * i = (1 \u2212 \u00b5 i \u03b3 k ) x k \u2212 x * 2 \u2212 2\u03b3 k f i (x k ) \u2212 f * i + f * i \u2212 f i (x * ) + \u03b3 k c f i (x k ) \u2212 f * i = (1 \u2212 \u00b5 i \u03b3 k ) x k \u2212 x * 2 + \u22122\u03b3 k + \u03b3 k c f i (x k ) \u2212 f * i +2\u03b3 k [f i (x * ) \u2212 f i (x * i )] c\u22651/2 = (1 \u2212 \u00b5 i \u03b3 k ) x k \u2212 x * 2 + 2\u03b3 k [f i (x * ) \u2212 f * i ] (6), (4) \u2264 1 \u2212 \u00b5 i min 1 2cL max , \u03b3 b x k \u2212 x * 2 + 2\u03b3 b [f i (x * ) \u2212 f * i ] (18) taking expectation condition on x k E i x k+1 \u2212 x * 2 \u2264 1 \u2212 E i [\u00b5 i ] min 1 2cL max , \u03b3 b x k \u2212 x * 2 + 2\u03b3 b E [f i (x * ) \u2212 f * i ] (5) = 1 \u2212\u03bc min 1 2cL max , \u03b3 b x k \u2212 x * 2 + 2\u03b3 b \u03c3 2(19)\nTaking expectations again and using the tower property:\nE x k+1 \u2212 x * 2 \u2264 1 \u2212\u03bc min 1 2cL max , \u03b3 b E x k \u2212 x * 2 + 2\u03b3 b \u03c3 2(20)\nRecursively applying the above and summing up the resulting geometric series gives:\nE x k \u2212 x * 2 \u2264 1 \u2212\u03bc min 1 2cL max , \u03b3 b k x 0 \u2212 x * 2 + 2\u03b3 b \u03c3 2 k\u22121 j=0 1 \u2212\u03bc min 1 2cL max , \u03b3 b j \u2264 1 \u2212\u03bc min 1 2cL max , \u03b3 b k x 0 \u2212 x * 2 + 2\u03b3 b \u03c3 2 1 \u00b5 min 1 2cLmax , \u03b3 b(21)\nLet \u03b1 = min 1 2cLmax , \u03b3 b then,\nE x k \u2212 x * 2 \u2264 (1 \u2212\u03bc\u03b1) k x 0 \u2212 x * 2 + 2\u03b3 b \u03c3 2 \u00b5\u03b1(22)\nFrom definition of \u03b1 is clear that having small parameter c improves both the convergence rate 1 \u2212\u03bc\u03b1 and the neighborhood 2\u03b3 b \u03c3 2 \u00b5\u03b1 . Since we have the restriction c \u2265 1 2 the best selection would be c = 1 2 .\n\n\nB.2 Proof of Theorem 3.5\n\nProof.\nx k+1 \u2212 x * 2 = x k \u2212 \u03b3 k \u2207f i (x k ) \u2212 x * 2 = x k \u2212 x * 2 \u2212 2\u03b3 k x k \u2212 x * , \u2207f i (x k ) + \u03b3 2 k \u2207f i (x k ) 2 convexity \u2264 x k \u2212 x * 2 \u2212 2\u03b3 k f i (x k ) \u2212 f i (x * ) + \u03b3 2 k \u2207f i (x k ) 2 (17) \u2264 x k \u2212 x * 2 \u2212 2\u03b3 k f i (x k ) \u2212 f i (x * ) + \u03b3 k c f i (x k ) \u2212 f * i = x k \u2212 x * 2 \u2212 2\u03b3 k f i (x k ) \u2212 f i (x * ) + \u03b3 k c f i (x k ) \u2212 f i (x * ) + f i (x * ) \u2212 f * i = x k \u2212 x * 2 \u2212 \u03b3 k 2 \u2212 1 c f i (x k ) \u2212 f i (x * ) + \u03b3 k c [f i (x * ) \u2212 f * i ]\nTaking expectation again and using the tower property:\nE f (x k ) \u2212 f (x * ) = c \u03b1(2c \u2212 1) E x k \u2212 x * 2 \u2212 E x k+1 \u2212 x * 2 + \u03b3 b \u03c3 2 \u03b1(2c \u2212 1)(28)\nSumming from k = 0 to K \u2212 1 and dividing by K:\n1 K K\u22121 k=0 E f (x k ) \u2212 f (x * ) = c \u03b1(2c \u2212 1) 1 K K\u22121 k=0 E x k \u2212 x * 2 \u2212 E x k+1 \u2212 x * 2 + 1 K K\u22121 k=0 \u03b3 b \u03c3 2 \u03b1(2c \u2212 1) = c \u03b1(2c \u2212 1) 1 K x 0 \u2212 x * 2 \u2212 E x K \u2212 x * 2 + \u03b3 b \u03c3 2 \u03b1(2c \u2212 1) \u2264 c \u03b1(2c \u2212 1) 1 K x 0 \u2212 x * 2 + \u03b3 b \u03c3 2 \u03b1(2c \u2212 1)(29)Letx K = 1 K K\u22121 k=0 x k , then: E f (x K ) \u2212 f (x * ) Jensen \u2264 1 K K\u22121 k=0 E f (x k ) \u2212 f (x * ) \u2264 c \u03b1(2c \u2212 1) 1 K x 0 \u2212 x * 2 + \u03b3 b \u03c3 2 \u03b1(2c \u2212 1)(30)\nFor c = 1:\nE f (x K ) \u2212 f (x * ) \u2264 x 0 \u2212 x * 2 \u03b1K + \u03b3 b \u03c3 2 \u03b1(31)\nand this completes the proof.\n\nAt this point we highlight that c = 1 is selected to simplify the expression of the upper bound in (30). This is not the optimum choice (the one that makes the rate and the neighborhood of the upper bound smaller). In order to compute the optimum value of c one needs to follow similar procedure to (Gower et al., 2019) and (Needell et al., 2016). In this case c will depend on parameter \u03c3 and the desired accuracy of convergence.\n\nHowever as we show bellow having c = 1 allows SGD with SPS to convergence faster than the ALI-G algorithm (Berrada et al., 2019) and the SLS algorithm (Vaswani et al., 2019b) for the case of smooth convex functions.\n\nComparison with other methods Similar to the strongly convex case let us compare the above convergence for smooth convex functions with the convergence rates proposed in (Vaswani et al., 2019b) and (Berrada et al., 2019).\n\nFor the smooth convex functions, Berrada et al. (2019) show the linear convergence to a much larger neighborhood than ours and with slower rate. In particular, their rate is 1 K 2L 1\u2212 2L \u03b4 and the neighborhood is\n\u03b4 L(1\u2212 2L \u03b4 ) where \u03b4 > 2L and is the -interpolation parameter > max i [f i (x * ) \u2212 f * i ]\nwhich by definition is bigger than \u03c3 2 . Under interpolation where \u03c3 = 0, our method converges with a O(1/K) rate to the x * while the algorithm proposed by Berrada et al. (2019) still converges to a neighborhood that is proportional to the parameter \u03b4.\n\nIn the interpolation setting our rate is similar to the one obtain for the stochastic line search (SLS) proposed in (Vaswani et al., 2019b). In particular in the interpolation setting, SLS achieves the following\nO(1/K) rate E f (x K ) \u2212 f (x * ) \u2264 max{3Lmax,2/\u03b3 b } K\nx 0 \u2212 x * 2 which has slightly worse constants than SGD with SPS.\n\n\nB.3 SPS on Methods for Solving Consistent Linear Systems\n\nRecently several new randomized iterative methods (sketch and project methods) for solving large-scale linear systems have been proposed (Richt\u00e1rik & Tak\u00e1\u010d, 2017;Loizou & Richt\u00e1rik, 2017;2019a;Gower & Richt\u00e1rik, 2015). The main algorithm in this literature is the celebrated randomized Kaczmarz (RK) method (Kaczmarz, 1937;Strohmer & Vershynin, 2009) which can be seen as special case of SGD for solving least square problems (Needell et al., 2016). In this area of research, it is well known that the theoretical best constant step-size for RK method is \u03b3 = 1.\n\nAs we have already mentioned in Section 3.2.3, given the consistent linear system\nAx = b,(32)\nRicht\u00e1rik & Tak\u00e1\u010d (2017) provide a stochastic optimization reformulation of the form (1) which is equivalent to the linear system in the sense that their solution sets are identical. That is, the set of minimizers of the stochastic optimization problem X * is equal to the set of solutions of the stochastic linear system L := {x : Ax = b}.\n\nIn particular, the stochastic convex quadratic optimization problem proposed in Richt\u00e1rik & Tak\u00e1\u010d (2017), can be expressed as follows:\nmin x\u2208R n f (x) := E S\u223cD f S (x).(33)\nHere the expectation is over random matrices S drawn from an arbitrary, user defined, distribution D and f S is a stochastic convex quadratic function of a least-squares type, defined as\nf S (x) := 1 2 Ax \u2212 b 2 H = 1 2 (Ax \u2212 b) H(Ax \u2212 b).(34)\nFunction f S depends on the matrix A \u2208 R m\u00d7n and vector b \u2208 R m of the linear system (32) and on a random symmetric positive semidefinite matrix H := S(S AA S) \u2020 S . By \u2020 we denote the Moore-Penrose pseudoinverse.\n\nFor solving problem (33), Richt\u00e1rik & Tak\u00e1\u010d (2017) analyze SGD with constant step-size:\nx k+1 = x k \u2212 \u03b3\u2207f S k (x k ),(35)\nwhere \u2207f S k (x k ) denotes the gradient of function f S k . In each step the matrix S k is drawn from the given distribution D.\n\nThe above update of SGD is quite general and as explained by Richt\u00e1rik & Tak\u00e1\u010d (2017) the flexibility of selecting distribution D allow us to obtain different stochastic reformulations of the linear system (32) and different special cases of the SGD update. For example the celebrated randomized Kaczmarz (RK) method can be seen as special cases of the above update as follows:\n\nRandomized Kaczmarz Method: Let pick in each iteration the random matrix S = e i (random coordinate vector) with probability p i = A i: 2 / A 2 F . In this setup the update rule of SGD (35) simplifies to\nx k+1 = x k \u2212 \u03c9 A i: x k \u2212 b i A i: 2 A i:\nMany other methods like Gaussian Kacmarz, Randomized Coordinate Descent, Gaussian Decsent and their block variants can be cast as special cases of the above framework. For more details on the general framework and connections with other research areas we also suggest (Loizou & Richt\u00e1rik, 2019b;Loizou, 2019).\n\nLemma B.1 (Properties of stochastic reformulation (Richt\u00e1rik & Tak\u00e1\u010d, 2017)). For all x \u2208 R n and any S \u223c D it holds that:\nf S (x) \u2212 f S (x * ) f * S =0 = f S (x) = 1 2 \u2207f S (x) 2 B = 1 2 \u2207f S (x), x \u2212 x * B .(36)\nLet x * is the projection of vector x onto the solution set X * of the optimization problem min x\u2208R n f (x) (Recall that by the construction of the stochastic optimization problems we have that X * = L). Then:\n\u03bb + min (W) 2 x \u2212 x * 2 B \u2264 f (x).(37)\nwhere \u03bb + min denotes the minimum non-zero eigenvalue of matrix W = E[A HA].\n\nAs we will see in the next Theorem, using the special structure of the stochastic reformulation (33), SPS (3) with c = 1/2 takes the following form:\n\u03b3 k (3) = 2 f S (x k ) \u2212 f * S \u2207f S (x k ) 2(36)\n= 1, which is the theoretically optimal constant step-size for SGD in this setting (Richt\u00e1rik & Tak\u00e1\u010d, 2017). This reduction implies that SPS results in an optimal convergence rate when solving consistent linear systems. We provide the convergence rate for SPS in the next Theorem.\n\nThough a straight forward verification of the optimality of SPS, we believe that this is the first time that SGD with adaptive step-size is reduced to constant step-size when is used for solving linear systems. SPS does that by obtaining the best convergence rate in this setting.\n\nTheorem B.2. Let Ax = b be a consistent linear system and let x * is the projection of vector x onto the solution set X * = L. Then the SGD with SPS (3) with c = 1/2 for solving the stochastic optimization reformulation (33) satisfies:\nE x k \u2212 x * 2 \u2264 1 \u2212 \u03bb + min (W) k x 0 \u2212 x * 2(38)\nwhere \u03bb + min denotes the minimum non-zero eigenvalue of matrix W = E[A HA].\n\nProof.\nx k+1 \u2212 x * 2 (35) = x k \u2212 \u03b3 k \u2207f S k (x k ) \u2212 x * 2 = x k \u2212 x * 2 \u2212 2\u03b3 k x k \u2212 x * , \u2207f S k (x k ) + \u03b3 2 k \u2207f S k (x k ) 2(39)\nLet us select \u03b3 k such that the RHS of inequality (39) is minimized. That is, let us select:\n\u03b3 k = x k \u2212 x * , \u2207f S k (x k ) \u2207f S k (x k ) 2 (36) = 2 [f S (x) \u2212 f S (x * )] \u2207f S k (x k ) 2\nSubstitute this step-size to (39) we obtain:\nx k+1 \u2212 x * 2 \u2264 x k \u2212 x * 2 \u2212 2 x k \u2212 x * , \u2207f S k (x k ) \u2207f S k (x k ) 2 x k \u2212 x * , \u2207f S k (x k ) + x k \u2212 x * , \u2207f S k (x k ) \u2207f S k (x k ) 2 2 \u2207f S k (x k ) 2 = x k \u2212 x * 2 \u2212 2 [ x k \u2212 x * , \u2207f S k (x k ) ] 2 \u2207f S k (x k ) 2 + x k \u2212 x * , \u2207f S k (x k ) 2 \u2207f S k (x k ) 2 = x k \u2212 x * 2 \u2212 [ x k \u2212 x * , \u2207f S k (x k ) ] 2 \u2207f S k (x k ) 2 (36) = x k \u2212 x * 2 \u2212 2f S (x k )(40)\nBy taking expectation with respect to S k and using quadratic growth inequality (37):\nE S k [ x k+1 \u2212 x * 2 ] = x k \u2212 x * 2 \u2212 2f (x k ) (37) \u2264 x k \u2212 x * 2 \u2212 \u03bb + min (W) x k \u2212 x * 2 = 1 \u2212 \u03bb + min (W) x k \u2212 x * 2 .(41)\nTaking expectation again and by unrolling the recurrence we obtain (38).\n\nWe highlight that the above proof provides a different viewpoint on the analysis of the optimal constant step-size for the sketch and project methods for solving consistent liner systems. The expression of Theorem B.2 is the same with the one proposed in (Richt\u00e1rik & Tak\u00e1\u010d, 2017).\n\nB.4 Proof of Theorem 3.6\n\nProof. By the smoothness of function f we have that\nf (x k+1 ) \u2264 f (x k ) + \u2207f (x k ), x k+1 \u2212 x k + L 2 x k+1 \u2212 x k 2 .\nCombining this with the update rule of SGD we obtain:\nf (x k+1 ) \u2264 f (x k ) + \u2207f (x k ), x k+1 \u2212 x k + L 2 x k+1 \u2212 x k 2 = f (x k ) \u2212 \u03b3 k \u2207f (x k ), \u2207f i (x k ) + L\u03b3 2 k 2 \u2207f i (x k ) 2(42)\nBy rearranging:\nf (x k+1 ) \u2212 f (x k ) \u03b3 k \u2264 \u2212 \u2207f (x k ), \u2207f i (x k ) + L\u03b3 k 2 \u2207f i (x k ) 2 (17) \u2264 \u2212 \u2207f (x k ), \u2207f i (x k ) + L 2c f i (x k ) \u2212 f * i = \u2212 \u2207f (x k ), \u2207f i (x k ) + L 2c f i (x k ) \u2212 f i (x * ) + L 2c [f i (x * ) \u2212 f * i ](43)\nand by taking expectation condition on x k :\nE i f (x k+1 ) \u2212 f (x k ) \u03b3 k \u2264 \u2212 \u2207f (x k ), \u2207f (x k ) + L 2c f (x k ) \u2212 f (x * ) + L 2c E i [f i (x * ) \u2212 f * i ] (5) \u2264 \u2212 \u2207f (x k ) 2 + L 2c f (x k ) \u2212 f (x * ) + L 2c \u03c3 2 (10) \u2264 \u22122\u00b5 f (x k ) \u2212 f (x * ) + L 2c f (x k ) \u2212 f (x * ) + L 2c \u03c3 2 (44) Let \u03b1 = min 1 2cLmax , \u03b3 b . Then, E i f (x k+1 ) \u2212 f (x * ) \u03b3 k \u2264 E i f (x k ) \u2212 f (x * ) \u03b3 k \u2212 2\u00b5 f (x k ) \u2212 f (x * ) + L 2c f (x k ) \u2212 f (x * ) + L 2c \u03c3 2 (6),(4) \u2264 1 \u03b1 f (x k ) \u2212 f (x * ) \u2212 2\u00b5 f (x k ) \u2212 f (x * ) + L 2c f (x k ) \u2212 f (x * ) + L 2c \u03c3 2 = 1 \u03b1 \u2212 2\u00b5 + L 2c f (x k ) \u2212 f (x * ) + L 2c \u03c3 2 L\u2264Lmax = 1 \u03b1 \u2212 2\u00b5 + L max 2c f (x k ) \u2212 f (x * ) + L 2c \u03c3 2(45)\nUsing \u03b3 k \u2264 \u03b3 b and by taking expectations again:\nE f (x k+1 ) \u2212 f (x * ) \u2264 \u03b3 b 1 \u03b1 \u2212 2\u00b5 + L max 2c \u03bd E f (x k ) \u2212 f (x * ) + L\u03c3 2 \u03b3 b 2c(46)\nBy having \u03bd \u2208 (0, 1] and by recursively applying the above and summing the resulting geometric series we obtain:\nE f (x k ) \u2212 f (x * ) \u2264 \u03bd k f (x 0 ) \u2212 f (x * ) + L\u03c3 2 \u03b3 b 2c k\u22121 j=0 \u03bd j \u2264 \u03bd k f (x 0 ) \u2212 f (x * ) + L\u03c3 2 \u03b3 b 2(1 \u2212 \u03bd)c(47)\nIn the above result we require that 0 < \u03bd = \u03b3 b 1 \u03b1 \u2212 2\u00b5 + Lmax 2c \u2264 1. In order for this to hold we need to make extra assumptions on the values of \u03b3 b and parameter c. This is what we do next.\n\nLet us divide the analysis into two cases based on the value of parameter \u03b1. That is:\n\u2022 (i) If 1 2cLmax \u2264 \u03b3 b then, \u03b1 = min 1 2cL max , \u03b3 b = 1 2cL max and \u03bd = \u03b3 b 2cL max \u2212 2\u00b5 + L max 2c = \u03b3 b (2c + 1 2c )L max \u2212 2\u00b5 .\nBy preliminary computations, it can be easily shown that \u03bd > 0 for every c \u2265 0. However for \u03bd \u2264 1 we need to require that\n\u03b3 b \u2264 1 ( 1 \u03b1 \u22122\u00b5+ Lmax 2c )\nand since we are already assume that 1 2cLmax \u2264 \u03b3 b we need to force\n1 2cL max \u2264 1 1 \u03b1 \u2212 2\u00b5 + Lmax 2c\nto avoid contradiction. This is true only if c > Lmax 4\u00b5 which is the assumption of Theorem 3.6.\n\u2022 (ii) If \u03b3 b \u2264 1 2cLmax then, \u03b1 = min 1 2cL max , \u03b3 b = \u03b3 b and \u03bd = \u03b3 b 1 \u03b3 b \u2212 2\u00b5 + L max 2c = 1 \u2212 2\u00b5\u03b3 b + L max 2c \u03b3 b .\nNote that if we have c > Lmax 4\u00b5 (an assumption of Theorem 3.6) it holds that \u03bd \u2264 1. In addition, by preliminary computations, it can be shown that \u03bd > 0 if \u03b3 b < 2c 4\u00b5c\u2212Lmax . Finally, for c > Lmax 4\u00b5 it holds that 1 2cLmax \u2264 2c 4\u00b5c\u2212Lmax , and as a result \u03bd > 0 for all \u03b3 b \u2264 1 2cLmax .\n\nBy presenting the above cases on bound of \u03bd we complete the proof.\n\nRemark B.3. The expression of Corollary 3.7 is obtained by simply use c = Lmax 2\u00b5 in the case (ii) of the above proof. In this case we have \u03b3 \u2264 \u00b5 L 2 max and \u03bd = 1 \u2212 \u00b5\u03b3.\n\nB.5 Proof of Theorem 3.8\n\nProof. By the smoothness of function f we have that\nf (x k+1 ) \u2264 f (x k ) + \u2207f (x k ), x k+1 \u2212 x k + L 2 x k+1 \u2212 x k 2 .\nCombining this with the update rule of SGD we obtain:\nf (x k+1 ) \u2264 f (x k ) + \u2207f (x k ), x k+1 \u2212 x k + L 2 x k+1 \u2212 x k 2 \u2264 f (x k ) \u2212 \u03b3 k \u2207f (x k ), \u2207f i (x k ) + L\u03b3 2 k 2 \u2207f i (x k ) 2 (4) \u2264 f (x k ) \u2212 \u03b3 k \u2207f (x k ), \u2207f i (x k ) + L\u03b3 2 b 2 \u2207f i (x k ) 2(48)\nAt this point, we follow similar proof to the convex case. That is, note that the quantity \u2207f (x k ), \u2207f i (x k ) can be either positive or negative. Thus, we divide our analysis in two cases.\n(i) If \u2207f (x k ), \u2207f i (x k ) > 0 then, \u2212\u03b3 k \u2207f (x k ), \u2207f i (x k ) \u2264 \u2212 1 2cL max \u2207f (x k ), \u2207f i (x k ) (ii) If \u2207f (x k ), \u2207f i (x k ) < 0 then, \u2212\u03b3 k \u2207f (x k ), \u2207f i (x k ) \u2264 \u2212\u03b3 b \u2207f (x k ), \u2207f i (x k )\nBy combining the above cases we obtain:\n\u2212\u03b3 k \u2207f (x k ), \u2207f i (x k ) \u2264 max \u2212 1 2cL max , \u2212\u03b3 b \u2207f (x k ), \u2207f i (x k ) = \u2212 min 1 2cL max , \u03b3 b \u2207f (x k ), \u2207f i (x k )(49)\nBy substituting (49) into (48) we obtain:\nf (x k+1 ) \u2264 f (x k ) \u2212 min 1 2cL max , \u03b3 b \u2207f (x k ), \u2207f i (x k ) + L\u03b3 2 b 2 \u2207f i (x k ) 2(50)\nBy taking expectation condition on x k and using (8):\nE i [f (x k+1 )] \u2264 f (x k ) \u2212 min 1 2cL max , \u03b3 b \u2207f (x k ) 2 + L\u03b3 2 b 2 E i [ \u2207f i (x k ) 2 ] \u2264 f (x k ) \u2212 min 1 2cL max , \u03b3 b \u2207f (x k ) 2 + L\u03b3 2 b 2 \u03c1 \u2207f (x) 2 + L\u03b3 2 b 2 \u03b4(51)\nBy rearranging and taking expectations again:\nmin 1 2cL max , \u03b3 b \u2212 L\u03b3 2 b 2 \u03c1 \u03b1 E[ \u2207f (x k ) 2 ] \u2264 E[f (x k )] \u2212 E[f (x k+1 )] + L\u03b3 2 b 2 \u03b4(52)\nLet \u03b1 > 0 then:\nE[ \u2207f (x k ) 2 ] \u2264 1 \u03b1 E[f (x k )] \u2212 E[f (x k+1 )] + L\u03b3 2 b \u03b4 2\u03b1(53)\nBy summing from k = 0 to K \u2212 1 and dividing by K:\n1 K K\u22121 k=0 E[ \u2207f (x k ) 2 ] \u2264 1 \u03b1 1 K K\u22121 k=0 E[f (x k )] \u2212 E[f (x k+1 )] + 1 K K\u22121 k=0 L\u03b3 2 b \u03b4 2\u03b1 \u2264 1 \u03b1 1 K f (x 0 ) \u2212 E[f (x K )] + L\u03b3 2 b \u03b4 2\u03b1 \u2264 1 \u03b1K f (x 0 ) \u2212 f (x * ) + L\u03b3 2 b \u03b4 2\u03b1(54)\nIn the above result we require that \u03b1 = min\n1 2cLmax , \u03b3 b \u2212 L\u03b3 2 b 2 \u03c1 > 0.\nIn order for this to hold we need to make extra assumptions on the values of \u03b3 b and parameter c. This is what we do next.\n\nLet us divide the analysis into two cases. That is:\n\u2022 (i) If 1 2cLmax \u2264 \u03b3 b then, \u03b1 = min 1 2cL max , \u03b3 b \u2212 L\u03b3 2 b 2 \u03c1 = 1 2cL max \u2212 L\u03b3 2 b 2 \u03c1 .\nBy preliminary computations, it can be easily shown that \u03b1 > 0 if \u03b3 b < 1 \u221a L\u03c1cLmax . To avoid contraction the inequality 1 2cLmax < 1 \u221a L\u03c1cLmax needs to be true. This is the case of c > L\u03c1 4Lmax which is the assumptions of Theorem 3.8.\n\n\u2022 (ii) If \u03b3 b \u2264 1 2cLmax then,\n\u03b1 = min 1 2cL max , \u03b3 b \u2212 L\u03b3 2 b 2 \u03c1 = \u03b3 b \u2212 L\u03b3 2 b 2 \u03c1 = \u03b3 b 1 \u2212 L\u03b3 b 2 \u03c1 .\nIn this case, by preliminary computations, it can be shown that \u03b1 > 0 if \u03b3 b < 2 L\u03c1 . For c > L\u03c1 4Lmax it also holds that 1 2cLmax < 2 L\u03c1 .\n\n\nC Additional Convergence Results\n\nIn this section we present some additional convergence results. We first prove a O(1/ \u221a K) convergence rate of stochastic subgradient method with SPS for non-smooth convex functions in the interpolated setting. Furthermore, similar to (Schmidt et al., 2011), we propose a way to increase the mini-batch size for evaluating the stochastic gradient and guarantee convergence to the optimal solution without interpolation.\n\n\nC.1 Non-smooth Convex Functions\n\nIn all of our previous results we assume that functions f i are smooth. As a result, in the proofs of our theorems we were able to use the lower bound (6) of SPS. In the case that functions f i are not smooth using this lower is clearly not possible. Below we present a Theorem that handles the case of non-smooth function for the convergence of stochastic subgradient method 4 . For this result we require that a constant G exists such that g i (x) 2 < G 2 for each subgradient of function f i . This is equivalent with assuming that functions f i are G-Lipschitz. To keep the presentation simple we only present the interpolated case. Using the proof techniques from the rest of the paper one can easily obtain convergence for the more general setting.\n\nTheorem C.1. Assume interpolation and that f and f i are convex non-smooth functions. Let G be a constant such that g i (x) 2 < G 2 , \u2200i \u2208 [n] and x \u2208 R n . Let \u03b3 k be the subgradient counterpart of SPS (3) with c = 1. Then the iterates of the stochastic subgradient method satisfy:\nE f (x K ) \u2212 f (x * ) \u2264 G x 0 \u2212 x * \u221a K = O 1 \u221a K wherex K = 1 K K\u22121 k=0 x k .\nProof. The proof is similar to the deterministic case (see Theorem A.4). That is, we select the \u03b3 k that minimize the right hand side of the inequality after the use of convexity. Step-size variation Step-size (log) w8a sps_max radam adam lookahead sls ali g (d) w8a Step-size (log) CIFAR10 -DenseNet121 Step-size (log) CIFAR100 -DenseNet121 Step-size (log) MNIST radam adam ali g lookahead sls sps_max \nx k+1 \u2212 x * 2 = x k \u2212 \u03b3 k g k i \u2212 x * 2 = x k \u2212 x * 2 \u2212 2\u03b3 k x k \u2212 x * , g k i + \u03b3 2 k g k\n\nIn recent years, several ways of selecting the step-size have been proposed. Moulines & Bach (2011); Needell et al. (2016); Needell & Ward (2017); Nguyen et al. (2018);\n\n\n2\u03b3 b \u03c3 2 \u00b5\u03b1has \u03b3 b in the numerator and for the case of large \u03b3 b , \u03b1 = 1 2cLmax .\n\n\npropose an analysis of constant step-size epoch-SGD for strongly convex smooth functions and Li et al. (2019) present an analysis of federated learning algorithms for the smooth strongly convex case.\n\n\nFollowing the procedure outlined in (Nutini et al., 2017), we generate a sparse dataset for binary classification with the number of examples n = 1k and dimension d = 100.\n\nFigure 2 .\n2Comparing the performance of optimizers on deep matrix factorization (top left) and binary classification using kernels (top right) and multi-class classification on CIFAR-10 and CIFAR-100 with ResNet34.\n\n\nAdam: (i) stochastic line-search (SLS) in (Vaswani et al., 2019b) (ii) ALI-G (Berrada et al., 2019) (iii) rectified Adam (RADAM) (Liu et al., 2019) (iv) Look-ahead optimizer (Zhang et al., 2019).\n\nFigure 3 .\n3Deep\n\nFigure 4 .\n4Binary\n\nFigure 5 .\n5Multi-class classification using deep networks\n\n6 Acknowledgments\n6Nicolas Loizou and Sharan Vaswani acknowledge support by the IVADO Postdoctoral Funding Program. Issam Laradji is funded by the UBC Four-Year Doctoral Fellowships (4YF). This research was partially supported by the Canada CIFAR AI Chair Program and the by a Google Focused Research award. Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning in Machines & Brains program. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In CVPR, 2017. Lohr, S. L. Sampling: Design and Analysis: Design and Analysis. Chapman and Hall/CRC, 2019. Loizou, N. Randomized iterative methods for linear systems: momentum, inexactness and gossip. PhD thesis, University of Edinburgh, 2019. Polyak, B. Introduction to optimization. translations series in mathematics and engineering. Optimization Software, 1987. Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. Lookahead optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems, 2019.Kaczmarz, S. Angen\u00e4herte aufl\u00f6sung von systemen linearer gle-\nichungen. Bulletin International de lAcademie Polonaise des \nSciences et des Lettres, 35:355-357, 1937. \n\nKarimi, H., Nutini, J., and Schmidt, M. Linear convergence \nof gradient and proximal-gradient methods under the polyak-\n\u0142ojasiewicz condition. In Joint European Conference on Ma-\nchine Learning and Knowledge Discovery in Databases, pp. \n795-811. Springer, 2016. \n\nKingma, D. and Ba, J. Adam: A method for stochastic optimiza-\ntion. In ICLR, 2015. \n\nLi, X. and Orabona, F. On the convergence of stochastic gradient \ndescent with adaptive stepsizes. In International Conference on \nArtificial Intelligence and Statistics, 2019. \n\nLi, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On \nthe convergence of fedavg on non-iid data. arXiv preprint \narXiv:1907.02189, 2019. \n\nLiu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. \nOn the variance of the adaptive learning rate and beyond. arXiv \npreprint arXiv:1908.03265, 2019. \n\nLoizou, N. and Richt\u00e1rik, P. Momentum and stochastic momentum \nfor stochastic gradient, Newton, proximal point and subspace \ndescent methods. arXiv preprint arXiv:1712.09677, 2017. \n\nLoizou, N. and Richt\u00e1rik, P. Convergence analysis of inexact ran-\ndomized iterative methods. arXiv preprint arXiv:1903.07971, \n2019a. \n\nLoizou, N. and Richt\u00e1rik, P. Revisiting randomized gossip algo-\nrithms: General framework, convergence rates and novel block \nand accelerated protocols. arXiv preprint arXiv:1905.08645, \n2019b. \n\nLoshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent \nwith warm restarts. arXiv preprint arXiv:1608.03983, 2016. \n\nLuo, L., Xiong, Y., Liu, Y., and Sun, X. Adaptive gradient methods \nwith dynamic bound of learning rate. In ICLR, 2019. \n\nMa, S., Bassily, R., and Belkin, M. The power of interpola-\ntion: Understanding the effectiveness of sgd in modern over-\nparametrized learning. In International Conference on Machine \nLearning, 2018. \n\nMoulines, E. and Bach, F. R. Non-asymptotic analysis of stochastic \napproximation algorithms for machine learning. In Advances in \nNeural Information Processing Systems, 2011. \n\nNeedell, D. and Ward, R. Batched stochastic gradient descent with \nweighted sampling. In Approximation Theory XV, Springer, vol-\nume 204 of Springer Proceedings in Mathematics & Statistics,, \npp. 279 -306, 2017. \nNeedell, D., Srebro, N., and Ward, R. Stochastic gradient descent, \nweighted sampling, and the randomized kaczmarz algorithm. \nMathematical Programming, Series A, 155(1):549-573, 2016. \n\nNemirovski, A. and Yudin, D. B. On Cezari's convergence of \nthe steepest descent method for approximating saddle point of \nconvex-concave functions. Soviet Mathetmatics Doklady, 19, \n1978. \n\nNemirovski, A. and Yudin, D. B. Problem complexity and method \nefficiency in optimization. Wiley Interscience, 1983. \n\nNemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust \nstochastic approximation approach to stochastic programming. \nSIAM Journal on Optimization, 19(4):1574-1609, 2009. \n\nNguyen, L., NGUYEN, P. H., Dijk, M., Richtarik, P., Scheinberg, \nK., and Takac, M. SGD and Hogwild! Convergence without \nthe bounded gradients assumption. In International Conference \non Machine Learning, 2018. \n\nNutini, J., Laradji, I., and Schmidt, M. Let's make block coordinate \ndescent go fast: Faster greedy rules, message-passing, active-\nset complexity, and superlinear convergence. arXiv preprint \narXiv:1712.08859, 2017. \n\nOberman, A. M. and Prazeres, M. Stochastic gradient descent with \npolyak's learning rate. arXiv preprint arXiv:1903.08688, 2019. \n\nRahimi, A. and Recht, B. Reflections on random kitchen sinks, \n2017. \n\nRakhlin, A., Shamir, O., and Sridharan, K. Making gradient \ndescent optimal for strongly convex stochastic optimization. In \nInternational Conference on Machine Learning, 2012. \n\nRecht, B., Re, C., Wright, S., and Niu, F. Hogwild: A lock-\nfree approach to parallelizing stochastic gradient descent. In \nAdvances in Neural Information Processing Systems, 2011. \n\nRicht\u00e1rik, P. and Tak\u00e1\u010d, M. Stochastic reformulations of linear \nsystems: algorithms and convergence theory. arXiv preprint \narXiv:1706.01108, 2017. \n\nRobbins, H. and Monro, S. A stochastic approximation method. \nThe Annals of Mathematical Statistics, pp. 400-407, 1951. \n\nRolinek, M. and Martius, G. L4: Practical loss-based stepsize \nadaptation for deep learning. In Advances in Neural Information \nProcessing Systems, 2018. \n\nSchmidt, M. and Roux, N. Fast convergence of stochastic gradi-\nent descent under a strong growth condition. arXiv preprint \narXiv:1308.6370, 2013. \n\nSchmidt, M., Kim, D., and Sra, S. Projected Newton-type methods \nin machine learning. Optimization for Machine Learning, 2011. \n\nShalev-Shwartz, S., Singer, Y., and Srebro, N. Pegasos: primal esti-\nmated subgradient solver for SVM. In International Conference \non Machine Learning, 2007. \n\nShamir, O. and Zhang, T. Stochastic gradient descent for non-\nsmooth optimization: Convergence results and optimal averag-\ning schemes. In International Conference on Machine Learning, \n2013. \n\nStrohmer, T. and Vershynin, R. A randomized Kaczmarz algorithm \nwith exponential convergence. J. Fourier Anal. Appl., 15(2): \n262-278, 2009. \n\nTan, C., Ma, S., Dai, Y.-H., and Qian, Y. Barzilai-borwein step \nsize for stochastic gradient descent. In Advances in Neural \nInformation Processing Systems, 2016. \n\nVaswani, S., Bach, F., and Schmidt, M. Fast and faster convergence \nof sgd for over-parameterized models and an accelerated per-\nceptron. In International Conference on Artificial Intelligence \nand Statistics, 2019a. \n\nVaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., and \nLacoste-Julien, S. Painless stochastic gradient: Interpolation, \nline-search, and convergence rates. In Advances in Neural \nInformation Processing Systems, 2019b. \n\nWard, R., Wu, X., and Bottou, L. Adagrad stepsizes: Sharp conver-\ngence over nonconvex landscapes. In International Conference \non Machine Learning, 2019. \n\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. \nUnderstanding deep learning requires rethinking generalization. \narXiv preprint arXiv:1611.03530, 2016. \n\nZhang, L. and Zhou, Z.-H. Stochastic approximation of smooth \nand strongly convex functions: Beyond the O (1/T) convergence \nrate. In Conference on Learning Theory, 2019. \n\n\nMila and DIRO, Universit\u00e9 de Montr\u00e9al 2 University of British Columbia, Element AI 3 Canada CIFAR AI Chair. Correspondence to: Nicolas Loizou <loizouni@mila.quebec>.\nExcept for our analysis for non-convex smooth functions where the weak growth condition is used.\nFor more details on the stochastic reformulation problem and its properties see Appendix B.3.\nLet us assume that c > 1 2 . Then 2 \u2212 1 c > 0. At this point, note that the quantity f i (x k ) \u2212 f i (x * ) can be either positive or negative. Thus, we divide our analysis in two cases.By combining the above cases we obtain:By substituting (24) into (23) we obtain:Let \u03b1 = min 1 2cLmax , \u03b3 b . By rearranging:For our choice of c it holds that \u03b1 2 \u2212 1 c > 0. By taking expectation condition on x k and dividing by \u03b1 2 \u2212 1 c :\nThe importance of better models in stochastic optimization. H Asi, J C Duchi, Proceedings of the National Academy of Sciences. the National Academy of Sciences116Asi, H. and Duchi, J. C. The importance of better models in stochastic optimization. Proceedings of the National Academy of Sciences, 116(46):22924-22930, 2019.\n\nRmsprop and equilibrated adaptive learning rates for nonconvex optimization. Y Bengio, corr abs/1502.04390Bengio, Y. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization. corr abs/1502.04390, 2015.\n\nL Berrada, A Zisserman, M P Kumar, arXiv:1906.05661Training neural networks for and by interpolation. arXiv preprintBerrada, L., Zisserman, A., and Kumar, M. P. Training neural networks for and by interpolation. arXiv preprint arXiv:1906.05661, 2019.\n\nNeuro-Dynamic Programming. D P Bertsekas, J N Tsitsiklis, Athena Scientific. 1st editionBertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Programming. Athena Scientific, 1st edition, 1996.\n\nOptimization methods for large-scale machine learning. L Bottou, F E Curtis, J Nocedal, SIAM Review. 602Bottou, L., Curtis, F. E., and Nocedal, J. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223-311, 2018.\n\nSubgradient methods. lecture notes of EE392o. S Boyd, L Xiao, A Mutapcic, Autumn Quarter. Stanford UniversityBoyd, S., Xiao, L., and Mutapcic, A. Subgradient methods. lecture notes of EE392o, Stanford University, Autumn Quarter, 2004: 2004-2005, 2003.\n\nOn the linear convergence of the stochastic gradient method with constant step-size. V Cevher, B C V\u0169, Optimization Letters. 135Cevher, V. and V\u0169, B. C. On the linear convergence of the stochas- tic gradient method with constant step-size. Optimization Let- ters, 13(5):1177-1187, 2019.\n\nLIBSVM: A library for support vector machines. C.-C Chang, C.-J Lin, ACM Transactions on Intelligent Systems and Technology. Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2011. Software available at http://www.csie. ntu.edu.tw/\u02dccjlin/libsvm.\n\nAdaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer, Journal of machine learning research. 12Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(Jul):2121-2159, 2011.\n\nStochastic first-and zeroth-order methods for nonconvex stochastic programming. S Ghadimi, G Lan, SIAM Journal on Optimization. 234Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order meth- ods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.\n\nRandomized iterative methods for linear systems. R Gower, P Richt\u00e1rik, SIAM. J. Matrix Anal. & Appl. 364Gower, R. and Richt\u00e1rik, P. Randomized iterative methods for linear systems. SIAM. J. Matrix Anal. & Appl., 36(4):1660- 1690, 2015.\n\nR M Gower, N Loizou, X Qian, A Sailanbayev, E Shulgin, P Richt\u00e1rik, Sgd, International Conference on Machine Learning. General analysis and improved ratesGower, R. M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and Richt\u00e1rik, P. Sgd: General analysis and improved rates. In International Conference on Machine Learning, 2019.\n\nTrain faster, generalize better: Stability of stochastic gradient descent. M Hardt, B Recht, Y Singer, International Conference on Machine Learning. Hardt, M., Recht, B., and Singer, Y. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, 2016.\n\nStop wasting my gradients: Practical SVRG. R Harikandeh, M O Ahmed, A Virani, M Schmidt, J Kone\u010dn\u1ef3, S Sallinen, Advances in Neural Information Processing Systems. Harikandeh, R., Ahmed, M. O., Virani, A., Schmidt, M., Kone\u010dn\u1ef3, J., and Sallinen, S. Stop wasting my gradients: Practical SVRG. In Advances in Neural Information Processing Systems, 2015.\n\nE Hazan, S Kakade, arXiv:1905.00313Revisiting the polyak step size. arXiv preprintHazan, E. and Kakade, S. Revisiting the polyak step size. arXiv preprint arXiv:1905.00313, 2019.\n\nBeyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. E Hazan, S Kale, The Journal of Machine Learning Research. 151Hazan, E. and Kale, S. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1):2489-2512, 2014.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016.\n", "annotations": {"author": "[{\"end\":102,\"start\":87},{\"end\":118,\"start\":103},{\"end\":133,\"start\":119},{\"end\":155,\"start\":134}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":95},{\"end\":117,\"start\":110},{\"end\":132,\"start\":125},{\"end\":154,\"start\":140}]", "author_first_name": "[{\"end\":94,\"start\":87},{\"end\":109,\"start\":103},{\"end\":124,\"start\":119},{\"end\":139,\"start\":134}]", "author_affiliation": null, "title": "[{\"end\":84,\"start\":1},{\"end\":239,\"start\":156}]", "venue": null, "abstract": "[{\"end\":1532,\"start\":241}]", "bib_ref": "[{\"end\":2353,\"start\":2330},{\"end\":2378,\"start\":2353},{\"end\":2383,\"start\":2378},{\"end\":2411,\"start\":2383},{\"end\":2435,\"start\":2411},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2454,\"start\":2435},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2703,\"start\":2684},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2890,\"start\":2870},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2912,\"start\":2892},{\"end\":3130,\"start\":3107},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3150,\"start\":3130},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3169,\"start\":3150},{\"end\":3193,\"start\":3169},{\"end\":3213,\"start\":3193},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3267,\"start\":3247},{\"end\":3284,\"start\":3267},{\"end\":3302,\"start\":3284},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3315,\"start\":3302},{\"end\":3337,\"start\":3315},{\"end\":3356,\"start\":3337},{\"end\":3374,\"start\":3356},{\"end\":3577,\"start\":3563},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3656,\"start\":3634},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3674,\"start\":3656},{\"end\":4984,\"start\":4962},{\"end\":5003,\"start\":4984},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5022,\"start\":5003},{\"end\":5043,\"start\":5022},{\"end\":5286,\"start\":5265},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5305,\"start\":5286},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5507,\"start\":5477},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5527,\"start\":5507},{\"end\":5547,\"start\":5527},{\"end\":5755,\"start\":5733},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5773,\"start\":5755},{\"end\":5795,\"start\":5773},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5904,\"start\":5884},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7193,\"start\":7173},{\"end\":7209,\"start\":7193},{\"end\":7352,\"start\":7330},{\"end\":7368,\"start\":7352},{\"end\":7390,\"start\":7368},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7409,\"start\":7390},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7430,\"start\":7409},{\"end\":8686,\"start\":8672},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8790,\"start\":8771},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8811,\"start\":8790},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9482,\"start\":9464},{\"end\":11297,\"start\":11272},{\"end\":11322,\"start\":11297},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11343,\"start\":11322},{\"end\":11458,\"start\":11434},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12330,\"start\":12309},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12767,\"start\":12745},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13022,\"start\":13000},{\"end\":16945,\"start\":16922},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17168,\"start\":17147},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17861,\"start\":17840},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18324,\"start\":18303},{\"end\":20421,\"start\":20398},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20442,\"start\":20421},{\"end\":20693,\"start\":20668},{\"end\":21445,\"start\":21420},{\"end\":21957,\"start\":21943},{\"end\":21977,\"start\":21957},{\"end\":23258,\"start\":23237},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23297,\"start\":23277},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23601,\"start\":23580},{\"end\":24271,\"start\":24249},{\"end\":24293,\"start\":24271},{\"end\":24295,\"start\":24293},{\"end\":24436,\"start\":24413},{\"end\":24780,\"start\":24757},{\"end\":25027,\"start\":25005},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26672,\"start\":26653},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27974,\"start\":27952},{\"end\":28639,\"start\":28621},{\"end\":28731,\"start\":28708},{\"end\":29159,\"start\":29140},{\"end\":29820,\"start\":29795},{\"end\":29842,\"start\":29820},{\"end\":29863,\"start\":29842},{\"end\":30876,\"start\":30853},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30984,\"start\":30965},{\"end\":31531,\"start\":31513},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31655,\"start\":31638},{\"end\":31693,\"start\":31660},{\"end\":32828,\"start\":32801},{\"end\":35036,\"start\":35022},{\"end\":36803,\"start\":36789},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36827,\"start\":36808},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36853,\"start\":36832},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":40710,\"start\":40690},{\"end\":40737,\"start\":40715},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40951,\"start\":40929},{\"end\":40997,\"start\":40974},{\"end\":41233,\"start\":41210},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41260,\"start\":41238},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41317,\"start\":41296},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41747,\"start\":41726},{\"end\":41963,\"start\":41940},{\"end\":42380,\"start\":42355},{\"end\":42405,\"start\":42380},{\"end\":42411,\"start\":42405},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42435,\"start\":42411},{\"end\":42541,\"start\":42525},{\"end\":42568,\"start\":42541},{\"end\":42666,\"start\":42644},{\"end\":43321,\"start\":43297},{\"end\":43898,\"start\":43874},{\"end\":45021,\"start\":44994},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45034,\"start\":45021},{\"end\":45112,\"start\":45087},{\"end\":45884,\"start\":45859},{\"end\":53998,\"start\":53976}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55978,\"start\":55808},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56063,\"start\":55979},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56265,\"start\":56064},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56439,\"start\":56266},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56656,\"start\":56440},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56854,\"start\":56657},{\"attributes\":{\"id\":\"fig_9\"},\"end\":56872,\"start\":56855},{\"attributes\":{\"id\":\"fig_10\"},\"end\":56892,\"start\":56873},{\"attributes\":{\"id\":\"fig_13\"},\"end\":56952,\"start\":56893},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":64343,\"start\":56953}]", "paragraph": "[{\"end\":1604,\"start\":1548},{\"end\":2258,\"start\":1646},{\"end\":2554,\"start\":2296},{\"end\":3503,\"start\":2556},{\"end\":4173,\"start\":3505},{\"end\":4864,\"start\":4175},{\"end\":5905,\"start\":4866},{\"end\":6171,\"start\":5907},{\"end\":6402,\"start\":6173},{\"end\":6929,\"start\":6421},{\"end\":7431,\"start\":6931},{\"end\":7810,\"start\":7433},{\"end\":8307,\"start\":7812},{\"end\":8404,\"start\":8351},{\"end\":8534,\"start\":8438},{\"end\":8812,\"start\":8559},{\"end\":9053,\"start\":8814},{\"end\":9117,\"start\":9109},{\"end\":9672,\"start\":9166},{\"end\":9897,\"start\":9694},{\"end\":9971,\"start\":9899},{\"end\":10805,\"start\":10021},{\"end\":10893,\"start\":10807},{\"end\":11152,\"start\":10958},{\"end\":11429,\"start\":11154},{\"end\":11868,\"start\":11431},{\"end\":11955,\"start\":11870},{\"end\":12272,\"start\":11979},{\"end\":12366,\"start\":12274},{\"end\":13219,\"start\":12376},{\"end\":13419,\"start\":13252},{\"end\":13474,\"start\":13421},{\"end\":13913,\"start\":13537},{\"end\":14257,\"start\":13938},{\"end\":14611,\"start\":14291},{\"end\":14721,\"start\":14690},{\"end\":15013,\"start\":14742},{\"end\":15213,\"start\":15063},{\"end\":15371,\"start\":15215},{\"end\":15930,\"start\":15429},{\"end\":16313,\"start\":15932},{\"end\":16622,\"start\":16315},{\"end\":16777,\"start\":16624},{\"end\":17067,\"start\":16821},{\"end\":17248,\"start\":17069},{\"end\":17422,\"start\":17250},{\"end\":17585,\"start\":17424},{\"end\":17737,\"start\":17639},{\"end\":18016,\"start\":17739},{\"end\":18399,\"start\":18156},{\"end\":18773,\"start\":18401},{\"end\":18950,\"start\":18775},{\"end\":19107,\"start\":19000},{\"end\":19350,\"start\":19148},{\"end\":19723,\"start\":19389},{\"end\":19905,\"start\":19751},{\"end\":20012,\"start\":19907},{\"end\":20079,\"start\":20067},{\"end\":20635,\"start\":20122},{\"end\":21130,\"start\":20665},{\"end\":21299,\"start\":21205},{\"end\":21628,\"start\":21342},{\"end\":21788,\"start\":21653},{\"end\":22182,\"start\":21834},{\"end\":22398,\"start\":22212},{\"end\":22811,\"start\":22543},{\"end\":22978,\"start\":22813},{\"end\":23442,\"start\":23052},{\"end\":23602,\"start\":23475},{\"end\":23664,\"start\":23639},{\"end\":23773,\"start\":23666},{\"end\":23874,\"start\":23861},{\"end\":24781,\"start\":24026},{\"end\":25417,\"start\":24816},{\"end\":25844,\"start\":25445},{\"end\":27246,\"start\":25870},{\"end\":28221,\"start\":27291},{\"end\":29102,\"start\":28256},{\"end\":29500,\"start\":29104},{\"end\":30537,\"start\":29502},{\"end\":31157,\"start\":30539},{\"end\":32371,\"start\":31159},{\"end\":32894,\"start\":32373},{\"end\":33538,\"start\":32909},{\"end\":34067,\"start\":33649},{\"end\":34185,\"start\":34121},{\"end\":34338,\"start\":34187},{\"end\":34464,\"start\":34386},{\"end\":34641,\"start\":34511},{\"end\":34793,\"start\":34674},{\"end\":34840,\"start\":34824},{\"end\":35116,\"start\":34930},{\"end\":35150,\"start\":35118},{\"end\":35273,\"start\":35175},{\"end\":35501,\"start\":35347},{\"end\":35603,\"start\":35553},{\"end\":35611,\"start\":35605},{\"end\":35824,\"start\":35761},{\"end\":35901,\"start\":35863},{\"end\":36022,\"start\":35938},{\"end\":36175,\"start\":36123},{\"end\":36421,\"start\":36298},{\"end\":36638,\"start\":36575},{\"end\":37019,\"start\":36734},{\"end\":37291,\"start\":37048},{\"end\":37390,\"start\":37293},{\"end\":37675,\"start\":37442},{\"end\":37683,\"start\":37677},{\"end\":38585,\"start\":38530},{\"end\":38741,\"start\":38658},{\"end\":38954,\"start\":38922},{\"end\":39222,\"start\":39011},{\"end\":39257,\"start\":39251},{\"end\":39759,\"start\":39705},{\"end\":39898,\"start\":39852},{\"end\":40304,\"start\":40294},{\"end\":40389,\"start\":40360},{\"end\":40821,\"start\":40391},{\"end\":41038,\"start\":40823},{\"end\":41261,\"start\":41040},{\"end\":41475,\"start\":41263},{\"end\":41822,\"start\":41569},{\"end\":42035,\"start\":41824},{\"end\":42157,\"start\":42092},{\"end\":42779,\"start\":42218},{\"end\":42862,\"start\":42781},{\"end\":43215,\"start\":42875},{\"end\":43351,\"start\":43217},{\"end\":43576,\"start\":43390},{\"end\":43846,\"start\":43633},{\"end\":43935,\"start\":43848},{\"end\":44098,\"start\":43970},{\"end\":44477,\"start\":44100},{\"end\":44682,\"start\":44479},{\"end\":45035,\"start\":44726},{\"end\":45159,\"start\":45037},{\"end\":45460,\"start\":45251},{\"end\":45576,\"start\":45500},{\"end\":45726,\"start\":45578},{\"end\":46057,\"start\":45776},{\"end\":46339,\"start\":46059},{\"end\":46576,\"start\":46341},{\"end\":46703,\"start\":46627},{\"end\":46711,\"start\":46705},{\"end\":46932,\"start\":46840},{\"end\":47073,\"start\":47029},{\"end\":47534,\"start\":47449},{\"end\":47738,\"start\":47666},{\"end\":48021,\"start\":47740},{\"end\":48047,\"start\":48023},{\"end\":48100,\"start\":48049},{\"end\":48223,\"start\":48170},{\"end\":48375,\"start\":48360},{\"end\":48645,\"start\":48601},{\"end\":49310,\"start\":49261},{\"end\":49515,\"start\":49403},{\"end\":49835,\"start\":49641},{\"end\":49922,\"start\":49837},{\"end\":50177,\"start\":50056},{\"end\":50275,\"start\":50207},{\"end\":50405,\"start\":50309},{\"end\":50817,\"start\":50530},{\"end\":50885,\"start\":50819},{\"end\":51056,\"start\":50887},{\"end\":51082,\"start\":51058},{\"end\":51135,\"start\":51084},{\"end\":51258,\"start\":51205},{\"end\":51656,\"start\":51464},{\"end\":51900,\"start\":51861},{\"end\":52069,\"start\":52028},{\"end\":52219,\"start\":52166},{\"end\":52444,\"start\":52399},{\"end\":52559,\"start\":52544},{\"end\":52678,\"start\":52629},{\"end\":52915,\"start\":52872},{\"end\":53071,\"start\":52949},{\"end\":53124,\"start\":53073},{\"end\":53455,\"start\":53219},{\"end\":53487,\"start\":53457},{\"end\":53704,\"start\":53565},{\"end\":54160,\"start\":53741},{\"end\":54950,\"start\":54196},{\"end\":55234,\"start\":54952},{\"end\":55717,\"start\":55314}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":1645,\"start\":1605},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8437,\"start\":8405},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9108,\"start\":9054},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9165,\"start\":9118},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10020,\"start\":9972},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10957,\"start\":10894},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11978,\"start\":11956},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12375,\"start\":12367},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13536,\"start\":13475},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14689,\"start\":14612},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15428,\"start\":15372},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16820,\"start\":16778},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17638,\"start\":17586},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18057,\"start\":18017},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18155,\"start\":18057},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18999,\"start\":18951},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19147,\"start\":19108},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19388,\"start\":19351},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20066,\"start\":20013},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20121,\"start\":20080},{\"attributes\":{\"id\":\"formula_20\"},\"end\":21204,\"start\":21131},{\"attributes\":{\"id\":\"formula_21\"},\"end\":21341,\"start\":21300},{\"attributes\":{\"id\":\"formula_22\"},\"end\":22211,\"start\":22183},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22542,\"start\":22399},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23051,\"start\":22979},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23638,\"start\":23603},{\"attributes\":{\"id\":\"formula_26\"},\"end\":23860,\"start\":23774},{\"attributes\":{\"id\":\"formula_27\"},\"end\":24025,\"start\":23875},{\"attributes\":{\"id\":\"formula_28\"},\"end\":28255,\"start\":28222},{\"attributes\":{\"id\":\"formula_29\"},\"end\":34385,\"start\":34339},{\"attributes\":{\"id\":\"formula_30\"},\"end\":34510,\"start\":34465},{\"attributes\":{\"id\":\"formula_31\"},\"end\":34673,\"start\":34642},{\"attributes\":{\"id\":\"formula_32\"},\"end\":34823,\"start\":34794},{\"attributes\":{\"id\":\"formula_33\"},\"end\":34888,\"start\":34841},{\"attributes\":{\"id\":\"formula_34\"},\"end\":35174,\"start\":35151},{\"attributes\":{\"id\":\"formula_35\"},\"end\":35346,\"start\":35274},{\"attributes\":{\"id\":\"formula_36\"},\"end\":35552,\"start\":35502},{\"attributes\":{\"id\":\"formula_37\"},\"end\":35760,\"start\":35612},{\"attributes\":{\"id\":\"formula_38\"},\"end\":35862,\"start\":35825},{\"attributes\":{\"id\":\"formula_39\"},\"end\":35937,\"start\":35902},{\"attributes\":{\"id\":\"formula_40\"},\"end\":36122,\"start\":36023},{\"attributes\":{\"id\":\"formula_41\"},\"end\":36297,\"start\":36176},{\"attributes\":{\"id\":\"formula_42\"},\"end\":36574,\"start\":36422},{\"attributes\":{\"id\":\"formula_43\"},\"end\":36733,\"start\":36639},{\"attributes\":{\"id\":\"formula_44\"},\"end\":37441,\"start\":37391},{\"attributes\":{\"id\":\"formula_45\"},\"end\":38529,\"start\":37684},{\"attributes\":{\"id\":\"formula_46\"},\"end\":38657,\"start\":38586},{\"attributes\":{\"id\":\"formula_47\"},\"end\":38921,\"start\":38742},{\"attributes\":{\"id\":\"formula_48\"},\"end\":39010,\"start\":38955},{\"attributes\":{\"id\":\"formula_49\"},\"end\":39704,\"start\":39258},{\"attributes\":{\"id\":\"formula_50\"},\"end\":39851,\"start\":39760},{\"attributes\":{\"id\":\"formula_51\"},\"end\":40142,\"start\":39899},{\"attributes\":{\"id\":\"formula_52\"},\"end\":40293,\"start\":40142},{\"attributes\":{\"id\":\"formula_53\"},\"end\":40359,\"start\":40305},{\"attributes\":{\"id\":\"formula_54\"},\"end\":41568,\"start\":41476},{\"attributes\":{\"id\":\"formula_55\"},\"end\":42091,\"start\":42036},{\"attributes\":{\"id\":\"formula_56\"},\"end\":42874,\"start\":42863},{\"attributes\":{\"id\":\"formula_57\"},\"end\":43389,\"start\":43352},{\"attributes\":{\"id\":\"formula_58\"},\"end\":43632,\"start\":43577},{\"attributes\":{\"id\":\"formula_59\"},\"end\":43969,\"start\":43936},{\"attributes\":{\"id\":\"formula_60\"},\"end\":44725,\"start\":44683},{\"attributes\":{\"id\":\"formula_61\"},\"end\":45250,\"start\":45160},{\"attributes\":{\"id\":\"formula_62\"},\"end\":45499,\"start\":45461},{\"attributes\":{\"id\":\"formula_63\"},\"end\":45775,\"start\":45727},{\"attributes\":{\"id\":\"formula_64\"},\"end\":46626,\"start\":46577},{\"attributes\":{\"id\":\"formula_65\"},\"end\":46839,\"start\":46712},{\"attributes\":{\"id\":\"formula_66\"},\"end\":47028,\"start\":46933},{\"attributes\":{\"id\":\"formula_67\"},\"end\":47448,\"start\":47074},{\"attributes\":{\"id\":\"formula_68\"},\"end\":47665,\"start\":47535},{\"attributes\":{\"id\":\"formula_69\"},\"end\":48169,\"start\":48101},{\"attributes\":{\"id\":\"formula_70\"},\"end\":48359,\"start\":48224},{\"attributes\":{\"id\":\"formula_71\"},\"end\":48600,\"start\":48376},{\"attributes\":{\"id\":\"formula_72\"},\"end\":49260,\"start\":48646},{\"attributes\":{\"id\":\"formula_73\"},\"end\":49402,\"start\":49311},{\"attributes\":{\"id\":\"formula_74\"},\"end\":49640,\"start\":49516},{\"attributes\":{\"id\":\"formula_75\"},\"end\":50055,\"start\":49923},{\"attributes\":{\"id\":\"formula_76\"},\"end\":50206,\"start\":50178},{\"attributes\":{\"id\":\"formula_77\"},\"end\":50308,\"start\":50276},{\"attributes\":{\"id\":\"formula_78\"},\"end\":50529,\"start\":50406},{\"attributes\":{\"id\":\"formula_79\"},\"end\":51204,\"start\":51136},{\"attributes\":{\"id\":\"formula_80\"},\"end\":51463,\"start\":51259},{\"attributes\":{\"id\":\"formula_81\"},\"end\":51860,\"start\":51657},{\"attributes\":{\"id\":\"formula_82\"},\"end\":52027,\"start\":51901},{\"attributes\":{\"id\":\"formula_83\"},\"end\":52165,\"start\":52070},{\"attributes\":{\"id\":\"formula_84\"},\"end\":52398,\"start\":52220},{\"attributes\":{\"id\":\"formula_85\"},\"end\":52543,\"start\":52445},{\"attributes\":{\"id\":\"formula_86\"},\"end\":52628,\"start\":52560},{\"attributes\":{\"id\":\"formula_87\"},\"end\":52871,\"start\":52679},{\"attributes\":{\"id\":\"formula_88\"},\"end\":52948,\"start\":52916},{\"attributes\":{\"id\":\"formula_89\"},\"end\":53218,\"start\":53125},{\"attributes\":{\"id\":\"formula_90\"},\"end\":53564,\"start\":53488},{\"attributes\":{\"id\":\"formula_91\"},\"end\":55313,\"start\":55235},{\"attributes\":{\"id\":\"formula_92\"},\"end\":55808,\"start\":55718}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1546,\"start\":1534},{\"attributes\":{\"n\":\"1.1\"},\"end\":2294,\"start\":2261},{\"end\":6419,\"start\":6405},{\"attributes\":{\"n\":\"2\"},\"end\":8349,\"start\":8310},{\"attributes\":{\"n\":\"2.1\"},\"end\":8557,\"start\":8537},{\"end\":9692,\"start\":9675},{\"attributes\":{\"n\":\"2.2\"},\"end\":13250,\"start\":13222},{\"attributes\":{\"n\":\"3\"},\"end\":13936,\"start\":13916},{\"attributes\":{\"n\":\"3.1\"},\"end\":14289,\"start\":14260},{\"attributes\":{\"n\":\"3.2\"},\"end\":14740,\"start\":14724},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":15049,\"start\":15016},{\"end\":15061,\"start\":15052},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":19749,\"start\":19726},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":20663,\"start\":20638},{\"attributes\":{\"n\":\"3.3\"},\"end\":21651,\"start\":21631},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":21832,\"start\":21791},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":23473,\"start\":23445},{\"attributes\":{\"n\":\"3.4\"},\"end\":24814,\"start\":24784},{\"attributes\":{\"n\":\"4\"},\"end\":25443,\"start\":25420},{\"attributes\":{\"n\":\"4.1\"},\"end\":25868,\"start\":25847},{\"attributes\":{\"n\":\"4.2\"},\"end\":27289,\"start\":27249},{\"attributes\":{\"n\":\"5\"},\"end\":32907,\"start\":32897},{\"end\":33647,\"start\":33541},{\"end\":34095,\"start\":34070},{\"end\":34119,\"start\":34098},{\"end\":34928,\"start\":34890},{\"end\":37046,\"start\":37022},{\"end\":39249,\"start\":39225},{\"end\":42216,\"start\":42160},{\"end\":53739,\"start\":53707},{\"end\":54194,\"start\":54163},{\"end\":56451,\"start\":56441},{\"end\":56866,\"start\":56856},{\"end\":56884,\"start\":56874},{\"end\":56904,\"start\":56894},{\"end\":56971,\"start\":56954}]", "table": "[{\"end\":64343,\"start\":57985}]", "figure_caption": "[{\"end\":55978,\"start\":55810},{\"end\":56063,\"start\":55981},{\"end\":56265,\"start\":56066},{\"end\":56439,\"start\":56268},{\"end\":56656,\"start\":56453},{\"end\":56854,\"start\":56659},{\"end\":56872,\"start\":56868},{\"end\":56892,\"start\":56886},{\"end\":56952,\"start\":56906},{\"end\":57985,\"start\":56973}]", "figure_ref": "[{\"end\":25251,\"start\":25243},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30994,\"start\":30986},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":48020,\"start\":47995}]", "bib_author_first_name": "[{\"end\":65189,\"start\":65188},{\"end\":65196,\"start\":65195},{\"end\":65198,\"start\":65197},{\"end\":65530,\"start\":65529},{\"end\":65675,\"start\":65674},{\"end\":65686,\"start\":65685},{\"end\":65699,\"start\":65698},{\"end\":65701,\"start\":65700},{\"end\":65954,\"start\":65953},{\"end\":65956,\"start\":65955},{\"end\":65969,\"start\":65968},{\"end\":65971,\"start\":65970},{\"end\":66175,\"start\":66174},{\"end\":66185,\"start\":66184},{\"end\":66187,\"start\":66186},{\"end\":66197,\"start\":66196},{\"end\":66403,\"start\":66402},{\"end\":66411,\"start\":66410},{\"end\":66419,\"start\":66418},{\"end\":66695,\"start\":66694},{\"end\":66705,\"start\":66704},{\"end\":66707,\"start\":66706},{\"end\":66948,\"start\":66944},{\"end\":66960,\"start\":66956},{\"end\":67304,\"start\":67303},{\"end\":67313,\"start\":67312},{\"end\":67322,\"start\":67321},{\"end\":67631,\"start\":67630},{\"end\":67642,\"start\":67641},{\"end\":67891,\"start\":67890},{\"end\":67900,\"start\":67899},{\"end\":68079,\"start\":68078},{\"end\":68081,\"start\":68080},{\"end\":68090,\"start\":68089},{\"end\":68100,\"start\":68099},{\"end\":68108,\"start\":68107},{\"end\":68123,\"start\":68122},{\"end\":68134,\"start\":68133},{\"end\":68490,\"start\":68489},{\"end\":68499,\"start\":68498},{\"end\":68508,\"start\":68507},{\"end\":68775,\"start\":68774},{\"end\":68789,\"start\":68788},{\"end\":68791,\"start\":68790},{\"end\":68800,\"start\":68799},{\"end\":68810,\"start\":68809},{\"end\":68821,\"start\":68820},{\"end\":68832,\"start\":68831},{\"end\":69084,\"start\":69083},{\"end\":69093,\"start\":69092},{\"end\":69368,\"start\":69367},{\"end\":69377,\"start\":69376},{\"end\":69669,\"start\":69668},{\"end\":69675,\"start\":69674},{\"end\":69684,\"start\":69683},{\"end\":69691,\"start\":69690}]", "bib_author_last_name": "[{\"end\":65193,\"start\":65190},{\"end\":65204,\"start\":65199},{\"end\":65537,\"start\":65531},{\"end\":65683,\"start\":65676},{\"end\":65696,\"start\":65687},{\"end\":65707,\"start\":65702},{\"end\":65966,\"start\":65957},{\"end\":65982,\"start\":65972},{\"end\":66182,\"start\":66176},{\"end\":66194,\"start\":66188},{\"end\":66205,\"start\":66198},{\"end\":66408,\"start\":66404},{\"end\":66416,\"start\":66412},{\"end\":66428,\"start\":66420},{\"end\":66702,\"start\":66696},{\"end\":66710,\"start\":66708},{\"end\":66954,\"start\":66949},{\"end\":66964,\"start\":66961},{\"end\":67310,\"start\":67305},{\"end\":67319,\"start\":67314},{\"end\":67329,\"start\":67323},{\"end\":67639,\"start\":67632},{\"end\":67646,\"start\":67643},{\"end\":67897,\"start\":67892},{\"end\":67910,\"start\":67901},{\"end\":68087,\"start\":68082},{\"end\":68097,\"start\":68091},{\"end\":68105,\"start\":68101},{\"end\":68120,\"start\":68109},{\"end\":68131,\"start\":68124},{\"end\":68144,\"start\":68135},{\"end\":68149,\"start\":68146},{\"end\":68496,\"start\":68491},{\"end\":68505,\"start\":68500},{\"end\":68515,\"start\":68509},{\"end\":68786,\"start\":68776},{\"end\":68797,\"start\":68792},{\"end\":68807,\"start\":68801},{\"end\":68818,\"start\":68811},{\"end\":68829,\"start\":68822},{\"end\":68841,\"start\":68833},{\"end\":69090,\"start\":69085},{\"end\":69100,\"start\":69094},{\"end\":69374,\"start\":69369},{\"end\":69382,\"start\":69378},{\"end\":69672,\"start\":69670},{\"end\":69681,\"start\":69676},{\"end\":69688,\"start\":69685},{\"end\":69695,\"start\":69692}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":84844820},\"end\":65450,\"start\":65128},{\"attributes\":{\"doi\":\"corr abs/1502.04390\",\"id\":\"b1\"},\"end\":65672,\"start\":65452},{\"attributes\":{\"doi\":\"arXiv:1906.05661\",\"id\":\"b2\"},\"end\":65924,\"start\":65674},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7989664},\"end\":66117,\"start\":65926},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3119488},\"end\":66354,\"start\":66119},{\"attributes\":{\"id\":\"b5\"},\"end\":66607,\"start\":66356},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":54612806},\"end\":66895,\"start\":66609},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":961425},\"end\":67223,\"start\":66897},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":538820},\"end\":67548,\"start\":67225},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14112046},\"end\":67839,\"start\":67550},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8215294},\"end\":68076,\"start\":67841},{\"attributes\":{\"id\":\"b11\"},\"end\":68412,\"start\":68078},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49015},\"end\":68729,\"start\":68414},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7710683},\"end\":69081,\"start\":68731},{\"attributes\":{\"doi\":\"arXiv:1905.00313\",\"id\":\"b14\"},\"end\":69261,\"start\":69083},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9159946},\"end\":69620,\"start\":69263},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594692},\"end\":69803,\"start\":69622}]", "bib_title": "[{\"end\":65186,\"start\":65128},{\"end\":65951,\"start\":65926},{\"end\":66172,\"start\":66119},{\"end\":66400,\"start\":66356},{\"end\":66692,\"start\":66609},{\"end\":66942,\"start\":66897},{\"end\":67301,\"start\":67225},{\"end\":67628,\"start\":67550},{\"end\":67888,\"start\":67841},{\"end\":68487,\"start\":68414},{\"end\":68772,\"start\":68731},{\"end\":69365,\"start\":69263},{\"end\":69666,\"start\":69622}]", "bib_author": "[{\"end\":65195,\"start\":65188},{\"end\":65206,\"start\":65195},{\"end\":65539,\"start\":65529},{\"end\":65685,\"start\":65674},{\"end\":65698,\"start\":65685},{\"end\":65709,\"start\":65698},{\"end\":65968,\"start\":65953},{\"end\":65984,\"start\":65968},{\"end\":66184,\"start\":66174},{\"end\":66196,\"start\":66184},{\"end\":66207,\"start\":66196},{\"end\":66410,\"start\":66402},{\"end\":66418,\"start\":66410},{\"end\":66430,\"start\":66418},{\"end\":66704,\"start\":66694},{\"end\":66712,\"start\":66704},{\"end\":66956,\"start\":66944},{\"end\":66966,\"start\":66956},{\"end\":67312,\"start\":67303},{\"end\":67321,\"start\":67312},{\"end\":67331,\"start\":67321},{\"end\":67641,\"start\":67630},{\"end\":67648,\"start\":67641},{\"end\":67899,\"start\":67890},{\"end\":67912,\"start\":67899},{\"end\":68089,\"start\":68078},{\"end\":68099,\"start\":68089},{\"end\":68107,\"start\":68099},{\"end\":68122,\"start\":68107},{\"end\":68133,\"start\":68122},{\"end\":68146,\"start\":68133},{\"end\":68151,\"start\":68146},{\"end\":68498,\"start\":68489},{\"end\":68507,\"start\":68498},{\"end\":68517,\"start\":68507},{\"end\":68788,\"start\":68774},{\"end\":68799,\"start\":68788},{\"end\":68809,\"start\":68799},{\"end\":68820,\"start\":68809},{\"end\":68831,\"start\":68820},{\"end\":68843,\"start\":68831},{\"end\":69092,\"start\":69083},{\"end\":69102,\"start\":69092},{\"end\":69376,\"start\":69367},{\"end\":69384,\"start\":69376},{\"end\":69674,\"start\":69668},{\"end\":69683,\"start\":69674},{\"end\":69690,\"start\":69683},{\"end\":69697,\"start\":69690}]", "bib_venue": "[{\"end\":65287,\"start\":65255},{\"end\":65253,\"start\":65206},{\"end\":65527,\"start\":65452},{\"end\":65774,\"start\":65725},{\"end\":66001,\"start\":65984},{\"end\":66218,\"start\":66207},{\"end\":66444,\"start\":66430},{\"end\":66732,\"start\":66712},{\"end\":67020,\"start\":66966},{\"end\":67367,\"start\":67331},{\"end\":67676,\"start\":67648},{\"end\":67940,\"start\":67912},{\"end\":68195,\"start\":68151},{\"end\":68561,\"start\":68517},{\"end\":68892,\"start\":68843},{\"end\":69149,\"start\":69118},{\"end\":69424,\"start\":69384},{\"end\":69701,\"start\":69697}]"}}}, "year": 2023, "month": 12, "day": 17}