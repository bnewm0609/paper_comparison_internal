{"id": 14617449, "updated": "2023-09-27 19:10:06.01", "metadata": {"title": "Novel methods for multilinear data completion and de-noising based on tensor-SVD", "authors": "[{\"first\":\"Zemin\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Gregory\",\"last\":\"Ely\",\"middle\":[]},{\"first\":\"Shuchin\",\"last\":\"Aeron\",\"middle\":[]},{\"first\":\"Ning\",\"last\":\"Hao\",\"middle\":[]},{\"first\":\"Misha\",\"last\":\"Kilmer\",\"middle\":[]}]", "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition", "journal": "2014 IEEE Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2014, "month": 7, "day": 7}, "abstract": "In this paper we propose novel methods for completion (from limited samples) and de-noising of multilinear (tensor) data and as an application consider 3-D and 4- D (color) video data completion and de-noising. We exploit the recently proposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, the notion of multilinear rank and a related tensor nuclear norm was proposed in [11] to characterize informational and structural complexity of multilinear data. We first show that videos with linear camera motion can be represented more efficiently using t-SVD compared to the approaches based on vectorizing or flattening of the tensors. Since efficiency in representation implies efficiency in recovery, we outline a tensor nuclear norm penalized algorithm for video completion from missing entries. Application of the proposed algorithm for video recovery from missing entries is shown to yield a superior performance over existing methods. We also consider the problem of tensor robust Principal Component Analysis (PCA) for de-noising 3-D video data from sparse random corruptions. We show superior performance of our method compared to the matrix robust PCA adapted to this setting as proposed in [4].", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1407.1785", "mag": "2953204310", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ZhangEAHK14", "doi": "10.1109/cvpr.2014.485"}}, "content": {"source": {"pdf_hash": "b94405f02c8cf90a3020270c6d10447745bc62de", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1407.1785v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1407.1785", "status": "GREEN"}}, "grobid": {"id": "1a6536cb8f7c3c0e2605e882622d40ea1275e5ab", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b94405f02c8cf90a3020270c6d10447745bc62de.txt", "contents": "\nNovel methods for multilinear data completion and de-noising based on tensor-SVD\n\n\nZemin Zhang zemin.zhang@tufts.com \nDepartment of ECE\nDepartment of Mathematics Medford\nTufts University Medford\n02155, 02155MA, MA\n\nGregory Ely gregoryely@gmail.com \nDepartment of ECE\nDepartment of Mathematics Medford\nTufts University Medford\n02155, 02155MA, MA\n\nShuchin Aeron shuchin@ece.tufts.edu \nDepartment of ECE\nDepartment of Mathematics Medford\nTufts University Medford\n02155, 02155MA, MA\n\nNing Hao ning.hao@tufts.edu \nDepartment of ECE\nDepartment of Mathematics Medford\nTufts University Medford\n02155, 02155MA, MA\n\nMisha Kilmer misha.kilmer@tufts.edu \nDepartment of ECE\nDepartment of Mathematics Medford\nTufts University Medford\n02155, 02155MA, MA\n\nNovel methods for multilinear data completion and de-noising based on tensor-SVD\n\nIn this paper we propose novel methods for completion (from limited samples) and de-noising of multilinear (tensor) data and as an application consider 3-D and 4-D (color) video data completion and de-noising. We exploit the recently proposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, the notion of multilinear rank and a related tensor nuclear norm was proposed in [11] to characterize informational and structural complexity of multilinear data. We first show that videos with linear camera motion can be represented more efficiently using t-SVD compared to the approaches based on vectorizing or flattening of the tensors. Since efficiency in representation implies efficiency in recovery, we outline a tensor nuclear norm penalized algorithm for video completion from missing entries. Application of the proposed algorithm for video recovery from missing entries is shown to yield a superior performance over existing methods. We also consider the problem of tensor robust Principal Component Analysis (PCA) for de-noising 3-D video data from sparse random corruptions. We show superior performance of our method compared to the matrix robust PCA adapted to this setting as proposed in[4].\n\nIntroduction\n\nThis paper focuses on several novel methods for robust recovery of multilinear signals or tensors (essentially viewed as 2-D, 3-D,..., N-D data) under limited sampling and measurements. Signal recovery from partial measurements, sometimes also referred to as the problem of data completion for specific choice of measurement operator being a simple downsampling operation, has been an important area of research, not only for statistical signal processing problems related to inversion, [6,20,15], but also in machine learning for online prediction of ratings, [9]. All of these applications exploit low structural and informational complexity of the data, expressed either as low rank for the 2-D matrices [6,20], which can be extended to higher order data via flattening or vectorizing of the tensor data such as tensor N-rank [7], or other more general tensor-rank measures based on particular tensor decompositions such as higher oder SVD (HOSVD) or Tucker-3 and Canonical Decomposition (CANDECOMP). See [13] for a survey of these decompositions.\n\nThe key idea behind these methods is that under the assumption of low-rank of the underlying data thereby constraining the complexity of the hypothesis space, it should be feasible to recover data (or equivalently predict the missing entries) from number of measurements in proportion to the rank. Such analysis and the corresponding identifiability results are obtained by considering an appropriate complexity penalized recovery algorithm under observation constraints, where the measure of complexity, related to the notion of rank, comes from a particular factorization of the data. Such algorithms are inherently combinatorial and to alleviate this difficulty one looks for the tightest convex relaxations of the complexity measure, following which the well developed machinery of convex optimization as well as convex analysis can be employed to study the related problem. For example, rank of the 2-D matrix being relaxed to the Schatten 1-norm, [19] and tensor N -rank for order N > 2 tensors being relaxed to overlapped Schatten p-norms, [7].\n\nNote that all of the current approaches to handle multilinear data extend the nearly optimal 2-D SVD 1 based vector space approach to the higher order (N > 2) case. This results in loss of optimality in the representation. In contrast, our approach is based upon recent results on decom-position/factorization of tensors in [2,12,11] in which the authors refer to as tensor-SVD or t-SVD for short. Essentially the t-SVD is based on an operator theoretic interpretation of third-order tensors as linear operators on the space of oriented matrices [2]. This notion can be extended recursively to higher order tensors [16]. In this paper we will exploit this decomposition, the associated notion of tensor multi-rank and its convex relaxation to the corresponding Tensor Nuclear Norm (TNN) (see [18]) for completion and recovery of multilinear data. This paper is organized as follows. Section 2 presents the notations and provide an overview and key results on t-SVD from [2,12,11] and illustrates the key differences and advantages over other tensor decomposition methods. We will then provide an over-view of the related structural complexity measures. In Section 3 we study the compression performance of the t-SVD based representation on several video data sets. Following that, in Section 4 we propose a tensor nuclear norm (TNN) penalized algorithm for 3-D and 4-D (color) video completion from randomly sampled data cube. In Section 5 we consider a tensor robust Principal Component Analysis (PCA) problem for videos with sparse data corruption and propose an algorithm to separate low multi-rank video from sparse corruptions. Finally we conclude in Section 6.\n\n\nBrief overview of t-SVD\n\nIn this section, we describe the tensor decomposition as proposed in [2,12,11] and the notations used throughout the paper.\n\n\nNotation and Indexing\n\nA Slice of an N-dimensional tensor is a 2-D section defined by fixing all but two indices. A Fiber of an Ndimensional tensor is a 1-D section defined by fixing all indices but one [13]. For a third order tensor A, we will use the Matlab notation A(k, :, :) , A(:, k, :) and A(:, :, k) to denote the k th horizontal, lateral and frontal slices, and A(: , i, j), A(i, :, j) and A(i, j, :) to denote the (i, j) th mode-1, mode-2 and mode-3 fiber. In particular, we use A (k) to represent A(:, :, k).\n\nOne can view a 3-D tensor of size n 1 \u00d7 n 2 \u00d7 n 3 as an n 1 \u00d7 n 2 matrix of tubes. By introducing a commutative operation * between the tubes a, b \u2208 R 1\u00d71\u00d7n3 via a * b = a \u2022 b, where \u2022 denotes the circular convolution between the two vectors, one defines the t-product between two tensors as follows.\n\nDefinition 2.1.1. t-product. The t-product C = A * B of A \u2208 R n1\u00d7n2\u00d7n3 and B \u2208 R n2\u00d7n4\u00d7n3 is a tensor of size n 1 \u00d7 n 4 \u00d7 n 3 where the (i, j) th tube denoted by C(i, j, :) for i = 1, 2, ..., n 1 and j = 1, 2, ..., n 4 of the tensor C is given by n2 k=1 A(i, k, :) * B(k, j, :). The t-product is analogous to the matrix multiplication except that circular convolution replaces the multiplication operation between the elements, which are now tubes. Next we define related notions of tensor transpose and identity tensor.\n\nDefinition 2.1.2. Tensor Transpose. Let A be a tensor of size n 1 \u00d7 n 2 \u00d7 n 3 , then A T is the n 2 \u00d7 n 1 \u00d7 n 3 tensor obtained by transposing each of the frontal slices and then reversing the order of transposed frontal slices 2 through n 3 .\n\nDefinition 2.1.3. Identity Tensor. The identity tensor I \u2208 R n1\u00d7n1\u00d7n3 is a tensor whose first frontal slice is the n 1 \u00d7 n 1 identity matrix and all other frontal slices are zero.\nDefinition 2.1.4. f-diagonal Tensor. A tensor is called f-diagonal if each frontal slice of the tensor is a diagonal matrix.\nThe t-product of A and B can be computed efficiently by performing the fast Fourier transformation (FFT) along the tube fibers of A and B to get\u00c2 andB, multiplying the each pair of the frontal slices of\u00c2 andB to obtain\u0108, and then taking the inverse FFT along the third mode to get the result. For details about the computation, see [12,11].\nDefinition 2.1.5. Orthogonal Tensor. A tensor Q \u2208 R n1\u00d7n1\u00d7n3 is orthogonal if Q T * Q = Q * Q T = I (1)\nwhere * is the t-product.\n\n\nTensor Singular Value Decomposition (t-SVD)\n\nThe new t-product allows us to define a tensor Singular Value Decomposition (t-SVD).\nTheorem 2.2.1. For M \u2208 R n1\u00d7n2\u00d7n3 , the t-SVD of M is given by M = U * S * V T(2)\nwhere U and V are orthogonal tensors of size n 1 \u00d7 n 1 \u00d7 n 3 and n 2 \u00d7n 2 \u00d7n 3 respectively. S is a rectangular f -diagonal tensor of size n 1 \u00d7 n 2 \u00d7 n 3 , and * denotes the t-product.\n\nOne can obtain this decomposition by computing matrix SVDs in the Fourier domain, see Algorithm 1. The notation in the algorithm can be found in [16]. Figure 1 illustrates the decomposition for the 3-D case.  \nAlgorithm 1 t-SVD Input: M \u2208 R n1\u00d7n2...\u00d7n N \u03c1 = n 3 n 4 ...n N for i = 3 to N do D \u2190 fft(M, [ ], i); end for for i = 1 to \u03c1 do [U,i = 3 to N do U \u2190 ifft(\u00db, [ ], i); S \u2190 ifft(\u015c, [ ], i); V \u2190 ifft(V, [ ], )i; end for\n\nt-SVD: Fundamental theorems and key results\n\nThe two widely used tensor decompositions, Tucker and PARAFAC [13] are usually seen as a higher order SVD for tensors. Both of these decompositions have several disadvantages. In particular, one cannot easily determine the rank-one components of the PARAFAC decomposition and given a fixed rank, calculation of an approximation can be numerically unstable. The tensor-train form of the Tucker decomposition is studied in [10] as an alternative form. Tucker decomposition can be seen as a generalization of PARAFAC decomposition, and the truncated decomposition doesn't yield the best fit of the original tensor. In contrast, the t-SVD can be easily computed by solving several SVDs in the Fourier domain. More importantly, it gives an optimal approximation of a tensor measured by the Frobenious norm of the difference, as stated in the following theorem [12,11,8].\nTheorem 2.3.1. Let the t-SVD of M \u2208 R n1\u00d7n2\u00d7n3 be given by M = U * S * V T and for k < min(n 1 , n 2 ) define M k = k i=1 U(:, i, :) * S(i, i, :) * V(:, i, :) T , Then M k = arg mi\u00f1 M\u2208M M \u2212M F where M = {C = X * Y|X \u2208 R n1\u00d7k\u00d7n3 , Y \u2208 R k\u00d7n2\u00d7n3 }.\n\nMeasures of tensor complexity using t-SVD\n\nWe now define two measures of tensor complexity based on the proposed t-SVD: the tensor multi-rank, proposed in [11], and the novel tensor tubal rank. Definition 2.4.1. Tensor multi-rank. The multi-rank of A \u2208 R n1\u00d7n2\u00d7n3 is a vector p \u2208 R n3\u00d71 with the i th element equal to the rank of the i th frontal slice of\u00c2 obtained by taking the Fourier transform along the third dimension of the tensor.\n\nOne can obtain a scalar measure of complexity as the 1 norm of the tensor multi-rank. We now define another measure motivated by the matrix SVD.\nDefinition 2.4.2.\nTensor tubal-rank. The tensor tubal rank of a 3-D tensor is defined to be the number of non-zero tubes of S in the t-SVD factorization.\n\nAs in the matrix case, practical applications of these complexity measures require adequate convex relaxations. To this end we have the following result for the Tensor multi-rank.\n\nTheorem 2.4.1. The tensor-nuclear-norm (TNN) denoted by ||A|| T N N and defined as the sum of the singular values of all the frontal slices of\u00c2 is a norm and is the tightest convex relaxation to 1 norm of the tensor multi-rank. Proof. The proof that TNN is a valid norm can be found in [18]. The 1 norm of the tensor multi-rank is equal to rank(blkdiag(\u00c2)), for which the tightest convex relaxation is the the nuclear norm of blkdiag(\u00c2) which is TNN of A by definition. Here blkdiag(\u00c2) is a block diagonal matrix defined as follows:\nblkdiag(\u00c2) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\u00c2 (1)\u00c2 (2) . . .\u00c2 (n3) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (3) where\u00c2 (i) is the i th frontal slice of\u00c2, i = 1, 2, ..., n 3 .\nUnlike the TNN is a relaxation for the tensor-nuclearnorm, there is no clear convex relaxation for the tensor tubal-rank. In the next section we will undertake a compressibility study for tensor data using two types of truncation strategies based on t-SVD and compare them with matrix SVD based approach on several video data sets.\n\n\nMultilinear data compression using t-SVD\n\nWe outline two methods for compression based on t-SVD and compare them with the traditional truncated SVD based approach in this section. Note that we don't compare with truncated HOSVD or other tensor decompositions as there is no notion of optimality for these decompositions in contrast to truncated t-SVD and truncated SVD.\n\nThe use of SVD in matrix compression has been widely studied in [17]. For a matrix A \u2208 R m\u00d7n with its SVD A = U SV T , the rank r approximation of A is the matrix\nA r = U r S r V T r , where S r is a r \u00d7 r diagonal matrix with S r (i, i) = S(i, i), i = 1, 2, .\n.., r. U r consists of the first r columns of U and V T r consists of the first r rows of V T r . The compression is measured by the ratio of the total number of entries in A, which is mn, to the total number of entries in U r , S r and V T r , which is equal to (m + n + 1)r. Extending this approach to a third-order tensor M of size n 1 \u00d7n 2 \u00d7n 3 , we vectorize each frontal slice and save it as a column, so we get an n 1 n 2 \u00d7 n 3 matrix. Then the compression ratio of rank k 1 SVD approximation is ratio SVD = n 1 n 2 n 3 n 1 n 2 k 1 + k 1 + n 3 k 1 = n 1 n 2 n 3 k 1 (n 1 n 2 + n 3 + 1)\n\nwhere 1 \u2264 k 1 \u2264 min(n 1 n 2 , n 3 ). Generally even with small k 1 , the approximation M k1 gets most of the information of M. Method 1 : Based on t-SVD our first method for compression, which we call t-SVD compression, basically follows the same idea of truncated SVD but in the Fourier domain. For an n 1 \u00d7 n 2 \u00d7 n 3 tensor M, we use Algorithm 1 to getM,\u00db,\u015c andV T . It is known that\u015c is a f-diagonal tensor with each frontal slice is a diagonal matrix. So the total number of f-diagonal entries of\u015c is n 0 n 3 where n 0 = min(n 1 , n 2 ). We choose an integer k 2 , 1 \u2264 k 2 \u2264 n 0 n 3 and keep the k 2 largest f-diagonal entries of S then set the rest to be 0. If\u015c(i, i, j) is set to be 0, then let the corresponding columns\u00db(:, i, j) andV T (:, i, j) also be 0. We then call the resulting tensors\u00db k2 ,\u015c k2 andV T k2 . So the approximation is M k2 = U k2 * S k2 * V T k2 where U k2 , S k2 and V T k2 are the inverse Fourier transforms of\u00db k2 ,\u015c k2 andV T k2 along the third dimension. The compression ratio rate for this method is ratio t-SVD = n 1 n 2 n 3 k 2 (n 1 + n 2 + 1)\nwhere 1 \u2264 k 2 \u2264 n 0 n 3 .\nMethod 2: Our second method for compressing is called t-SVD-tubal compression and is also similar to truncated SVD but in the t-product domain. As in Theorem 2.3.1, we take the first k 3 tubes (1 \u2264 k 3 \u2264 n 0 ) in S and get the approximation M k3 = k3 i=1 U(:, i, :) * S(i, i, :) * V(:, i, :) T . Compression ratio rate for the second method is ratio t-SVD-tubal = n 1 n 2 k 3 (n 1 + n 2 + 1)\nwhere 1 \u2264 k 3 \u2264 n 0 .\nVideo data representation and compression using t-SVD: We now illustrate the performance of SVD based compression, t-SVD compression and t-SVD tubal compression on 3 video datasets shown in Figure 2-(a).\n\n1. The first video, referred to as the Escalator video, (source: http://www.ugcs.caltech.edu/ srbecker/rpca.shtml#2 ) of size 130 \u00d7 160 \u00d7 50 (length \u00d7 width \u00d7 frames) from a stationary camera. 3. The third video, referred to as the Basketball video is a 144 \u00d7 256 \u00d7 80 video (source: YouTube) with a nonstationary panning camera moving from left to right horizontally following the running players. Figure 2 (b) to (d) show the compression results for the 3 videos when truncated according to vectorized SVD and t-SVD compression (method 1) and t-SVD tensor tubal compression (method 2). In Figure 3 we show the relative square error (RSE) comparison for different compression ratio where RSE is defined in dB as RSE = 20 log 10 ( X rec \u2212 X F / X F ). In all of the 3 results, the performance of t-SVD compression (method 1) is the best. This implies that tensor multi-rank fits very well for video datasets from both stationary and non-stationary cameras.\n\nSVD compression method (based on vectorization) has a better performance over the t-SVD-tubal compression on the Escalator and MERL video. However, t-SVD tubal compression (method 2) works much better than SVD compression on the Basketball video. This is because in the videos where the camera is panning or in motion, one frontal slice of the tensor to the next frontal slice can be effectively represented as a shift and scaling operation which in turn is captured by a convolution type operation and t-SVD is based on such an operation along the third dimension.\n\n\nTensor completion from limited samples\n\nWe will show the case when the tensor data is simply decimated randomly or down sampled in this section. Specifically we consider the problem of data completion from missing entries for multilinear signals. Suppose there is an unknown tensor M of size n 1 \u00d7 n 2 \u00d7 n 3 which is assumed to have a low tubal-rank and we are given a subset of entries {M ijk : (i, j, k) \u2208 \u2126} where \u2126 is an indicator tensor of size n 1 \u00d7 n 2 \u00d7 n 3 . Our objective is to recover the entire M. This section develops an algorithm for addressing this problem via solving the following complexity penalized algorithm:\nmin X T N N subject to P \u2126 (X) = P \u2126 (M)(5)\nwhere P \u2126 is the orthogonal projector onto the span of tensors vanishing outside of \u2126. So the (i, j, k) th component of P \u2126 (X) is equal to M ijk if (i, j, k) \u2208 \u2126 and zero otherwise. Let Y be the available (sampled) data: Y = P \u2126 M. Define G = F 3 P \u2126 F \u22121 3 where F 3 and F \u22121 3 are the operators representing the Fourier and inverse Fourier transform along the third dimension of tensors. Then we have\u0176 = G(M) wher\u00ea Y andM are the Fourier transforms of Y and M along the third mode. So (5) is equivalent with the following: min ||blkdiag(X)|| * subject to\u0176 = G(X) (6) whereX is the Fourier transform of X along the third dimension and blkdiag(X) is defined in (3). Noting that X T N N = ||blkdiag(X)|| * . To solve the optimization problem, one can re-write (6) equivalently as follows:\nmin ||blkdiag(\u1e90)|| * + 1\u0176 =G(X) subject toX \u2212\u1e90 = 0 (7)\nwhere 1 denotes the indicator function. Then using the general framework of Alternating Direction Method of Multipliers(ADMM) [1] we have the following recursion,\nX k+1 = arg min X 1 Y=P\u2126(X) + X(:) T Q k (:) + 1 2 ||X \u2212 Z k || 2 F = arg min X:Y=P\u2126(X) ||X \u2212 (Z k \u2212 Q k )|| 2 F (8) Z k+1 = arg min Z 1 \u03c1 ||blkdiag(\u1e90)|| * + 1 2 ||\u1e90 \u2212 (X k+1 +Q k )|| 2 F (9) Q k+1 = Q k + X k+1 \u2212 Z k+1(10)\nwhere Equation (8) is least-squares projection onto the constraint and the solution to Equation (9) is given by the singular value thresholding [19,3]. The X(:) and Q k (:) means vectorizing the tensors which is Matlab notation.\n\n\nEquivalence of the algorithm to iterative singular-tubal shrinkage via convolution\n\nWe will now show that the proposed algorithm for tensor completion has a very nice interpretation as an iterative singular tubal shrinkage using a convolution operation between the singular tubes and a tube of threshold vectors.\n\nAccording to the particular format that (9) has, we can break it up into n 3 independent minimization problems. Let . Then (9) can be separated as:\nZ k+1,(i) = arg min W 1 \u03c1 ||W || * + 1 2 ||W \u2212 (X k+1,(i) +Q k,(i)\n)|| 2 F (11) for i = 1, 2, ..., n 3 . This means each i th frontal slice of Z k+1 can be calculated through (11). In order to solve (11), we give out the following lemma. Lemma 4.1.1. Consider the singular value decomposition (SVD) of a matrix X \u2208 C n1\u00d7n2 of rank r.\nX = U \u03a3V * , \u03a3 = diag({\u03c3 i } 1\u2264i\u2264r ),(12)\nwhere U and V are respectively n 1 \u00d7 r and n 2 \u00d7 r unitary matrices with orthonormal columns, and the singular values \u03c3 i are real and positive. Then for all \u03c4 \u2265 0 , define the soft-thresholding operator D \u03c4 as follows [3] :\nD \u03c4 (X) := U D \u03c4 (\u03a3)V * , D \u03c4 (\u03a3) = diag{(\u03c3 i \u2212 \u03c4 ) + },(13)\nwhere t + is the positive part of t, namely, t + = max(0, t).\n\nThen, for each \u03c4 \u2265 0 and Y \u2208 C n1\u00d7n2 , the singular value shrinkage operator (13) obeys\nD \u03c4 (Y ) = arg min X\u2208C 1 2 X \u2212 Y 2 F + \u03c4 X *(14)\nThe proof can be found in [3] for the case when the matrix is real valued. However, it can be easily extended to matrices with complex entries using the result on gradients of unitarily invariant norms in [14].\nNow note that, if U SV T = (X k+1,(i) +Q k,(i) ) is the SVD of (X k+1,(i) +Q k,(i)\n), then the solution to (11) is U D \u03c4 (S)V T , where D \u03c4 (S) = diag(S i,i \u2212 \u03c4 ) + for some positive constant \u03c4 and \" + \" means keeping the positive part. This is equivalent to multiplying (1 \u2212 \u03c4 Si,i ) + to the i th singular value of S. So each frontal slice of\u1e90 k+1 can be calculated using this shrinkage on each frontal slice of (X k+1 +Q k ). Let U * S * V T = (X k+1 + Q k ) be the t-SVD of (X k+1 + Q k ) and\u015c be the Fourier transform of S along the third dimension. Then each element of the singular tubes of\u1e90 Application to Video data completion -For experiments we test 3 algorithms for video data completion from randomly missing entries: TNN minimization of Section 4, Low Rank Tensor Completion (LRTC) algorithm in [15], which uses the notion of tensor-n-rank [7], and the nuclear norm minimization on the vectorized video data using the algorithm in [3]. As an application of the t-SVD to higher order tensor we also show performance on a 4-D color Basketball video data of size 144 \u00d7 256 \u00d7 3 \u00d7 80. Figures 5 and 6 show the results of recovery using the 3 algorithms. Figure 7 shows the RSE (dB) plots for sampling rates ranging from 10% to 90% where the sampling rate is defined to be the percentage of pixels which are known. Results from the figures show that the TNN minimization algorithm gives excellent reconstruction over the LRTC and Nuclear norm minimization. These results are in line with the compressibility results in Section 3.  \n\n\nTensor robust PCA\n\nIn this section we consider a \"tensor robust principal component analysis\" problem of recovering a low tensormulti rank tensor L from a sparsely corrupted observation tensor. Similar to the matrix robust PCA case [4], suppose we have a third-order tensor M such that it can be decomposed as\nM = L + S(15)\nwhere L has low tensor-multi-rank and S is sparse tensor.\n\nHere we focus on a case where the sparse tensor S is tube- \n\nwhere \u03bb > 0 and the S 1,1,2 for 3-D tensors is defined as i,j ||S(i, j, :)|| F . An application where this is useful arises in multilinear imaging scenarios where some pixels have heavy noise on them and the task is to automatically locate such pixels and recover the video. Although this may be done by processing each frame but if the noise artifacts and video features are aligned, one needs to both detect the noise and estimate the corrupted video feature.\n\nIn order to solve the convex optimization problem of Equation (16) we use ADMM. Then we have the following recursion,\nL k+1 = arg min L L T N N + \u03c1 2 L + S k \u2212 M + W k 2 F (17) S k+1 = arg min S \u03bb S 1,1,2 + \u03c1 2 L k+1 + S \u2212 M + W k 2 F(18)W k+1 = W k + L k+1 + S k+1 \u2212 M(19)\nwhere W = \u03c1Y. From section 4 we already have the solution to (17) if we transform this equation into the Fourier domain then the tensor-nuclear-norm of L will be the nuclear norm of blkdiag(L). The closed form solution to Equation (18) is given by,\nS k+1 (i, j, :) = 1 \u2212 \u03bb \u03c1 S k (i, j, :) F + S k (i, j, :) (20)\nwhere i = 1, 2, ..., n 3 . For experiment we consider a video, which is compressible in the t-SVD. We randomly corrupt video data by corrupting some pixels with heavy additive noise. We want to estimate the locations of such pixels using tensor robust PCA. The video used in this application is the basketball video with randomly chosen sparse pixels tubes along the third dimension. For each selected pixel we add random Gaussian noise on it. Figure (8) shows the original video(tensor) and the noise tensor. The size of each frame is 72 \u00d7 128 and the total number of frames is 80. The noisy pixel tubes within every 10 frames are consistent. We use the above ADMM algorithm to separate the original video and the noise. Our analysis (to be reported in a future paper) shows that the optimal choice of \u03bb for tensor robust PCA is 1 \u221a max(n1,n2)\n\n. We also perform matrix robust PCA on this noisy video data by vectorizing each frame, saving it as a column vector and then get a n 1 n 2 \u00d7 n 3 matrix. In this case the choice of \u03bb is 1 \u221a max(n1n2,n3) [4]. The result of both tensor robust PCA and matrix robust PCA is shown in Figure 9. From the results we can see that tensor robust PCA works very well on separating the noisy pixels from the video. However, the matrix robust PCA results in an almost fixed blurred background as the low rank part while some structure of the playground, moving people and the noise are recovered as the sparse part. \n\n\nConclusion and Future work\n\nIn this paper we presented novel methods for completion and de-noising (tensor robust PCA) of multilinear data using the recently proposed notion of tensor-SVD (t-SVD). As an application we considered the problem of video completion and de-noising from random sparse corruptions, and showed significant performance gains compared to the existing methods. The t-SVD based tensor analysis and methods can handle more general multilinear data as long as the data is shown to be compressible in the t-SVD based representation, as has been recently shown for pre-stack seismic data completion in [5]. Finding the necessary and sufficient conditions for recovery of low (multi)rank tensors using TNN from incomplete tensor data is an important theoretical problem and is an important area of future research.\n\nFigure 1 .\n1The t-SVD of an n1 \u00d7 n2 \u00d7 n3 tensor.\n\n\nS, V] = SV D(D(:, :, i)) U(:, :, i) = U;\u015c(:, :, i) = S;V(:, :, i) = V; end for for\n\n2 .Figure 2 .Figure 3 .\n223The second video, referred to as the MERL video, is a time lapse video of size 192 \u00d7 256 \u00d7 38 also from a stationary camera (data courtesy: Dr. Amit Agrawal, Mitsubishi Electric Research Labs (MERL), Cambridge, MA). (a) Three testing videos: escalator video, MERL video and basketball video. (b) (c) (d) are compression results under compression ratio 5. For (b) (c) (d) from left to right: SVD compression, t-SVD-tubal compression and t-SVD compression Compression ratio and RSE comparison for 3 videos.\n\n\nis the result of multiplying every entry\u015c(i, i, j) with (1 \u2212 \u03c4 S(i,i,j) ) + for some \u03c4 > 0. Since this process is carried out in the Fourier domain, in the original domain it is equivalent to convolving each tube S(i, i, :) of S with a real valued tubal vector \u03c4 i which is the inverse Fourier transform of the vector [(1 \u2212 \u03c4i(1)S(i,i,1) ) + , (1 \u2212 \u03c4i(2) S(i,i,2) ) + , ..., (1 \u2212 \u03c4i(n3) S(i,i,n3) ) + ]. This operation can be captured by S * T, where T is an f-diagonal tensor with i th diagonal tube to be \u03c4 i . Then Z k+1 = U * (S * T) * V T . In summary, the shrinkage operation in the Fourier domain on the singular values of each of the frontal faces is equivalent to performing a tubal shrinkage via convolution in the original domain.\n\nFigure 4 .\n4Tensor completion results for MERL video. Upper left: Sampled video(20%). Upper right: Nuclear norm minimization (vectorization and SVD based) result. Lower left: LRTC result. Lower right: TNN minimization result.\n\nFigure 5 .\n5Tensor completion results for basketball video. Upper left: Sampled video(20%). Upper right: Nuclear norm minimization (vectorization and SVD based) result. Lower left: LRTC result. Lower right: TNN minimization result.\n\nFigure 6 .Figure 7 .\n67Recovery for color basketball video: Left: Sampled Video(10%). Middle: LRTC recovery. Right: Tensor-nuclearnorm minimization recovery RSE (dB) plot against sampling rate Left: MERL video. Right: Basketball video wise sparse as shown in Figure 8. To resolve the low rank and the sparse components given the observation M we consider the following optimization problem. min L T N N + \u03bb S 1,1,2 subject to M = L + S\n\nFigure 8 .\n8Upper left: Original video. Upper right: Noisy tensor. For 10 consecutive frames the locations of noisy pixels are the same and then selected randomly for the next 10 frames. Lower left 21st frame of the original video. Lower right 21st frame of the noisy video.\n\nFigure 9 .\n9(21st frame shown) Upper Left: Low tensor multi-rank part recovered from tensor robust PCA. Upper Right: Sparse reconstruction from tensor robust PCA. Lower left: Low matrix rank part recovered from matrix robust PCA. Lower right: Sparse reconstruction from matrix robust PCA.\nOptimality of 2-D SVD is based on the optimality of truncated SVD as the best k-dimensional 2 approximation.\nAcknowledgementsThis research was supported in part by the National Science Foundation grant NSF:1319653.\nDistributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, Machine Learning. 3S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Dis- tributed Optimization and Statistical Learning via the Alter- nating Direction Method of Multipliers. Foundations and Trends R in Machine Learning, 3(1):1-122, 2011.\n\nThird-order tensors as linear operators on a space of matrices. K Braman, Linear Algebra and its Applications. K. Braman. Third-order tensors as linear operators on a space of matrices. Linear Algebra and its Applications, pages 1241-1253, 2010.\n\nA singular value thresholding algorithm for matrix completion. J F Cai, E J Cand\u00e8s, Z Shen, SIAM Journal on Optimization. 204J. F. Cai, E. J. Cand\u00e8s, and Z. Shen. A singular value thresh- olding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.\n\nRobust principal component analysis?. E J Cand\u00e8s, X Li, Y Ma, J Wright, 11:1-11:37J. ACM. 583E. J. Cand\u00e8s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(3):11:1-11:37, June 2011.\n\nKilmer. 5d and 4d pre-stack seismic data completion using tensor nuclear norm (TNN). G Ely, S Aeron, N Hao, M E , Society of Exploration Geophysicists (SEG) Expanded Abstracts. G. Ely, S. Aeron, N. Hao, and M. E. Kilmer. 5d and 4d pre-stack seismic data completion using tensor nuclear norm (TNN). In Society of Exploration Geophysicists (SEG) Ex- panded Abstracts, 2013.\n\nExploiting structural complexity for robust and rapid hyperspectral imaging. G Ely, S Aeron, E L Miller, Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP). IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)G. Ely, S. Aeron, and E. L. Miller. Exploiting structural complexity for robust and rapid hyperspectral imaging. In Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2013.\n\nTensor completion and low-n-rank tensor recovery via convex optimization. S Gandy, B Recht, I Yamada, Inverse Problems. 27225010S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27(2):025010, 2011.\n\nFacial recognition with tensor-tensor decompositions. N H Hao, M E Kilmer, K Braman, R C Hoover, SIAM Journal on Imaging Sciences. N. H. Hao, M. E. Kilmer, K. Braman, and R. C. Hoover. Facial recognition with tensor-tensor decompositions. SIAM Journal on Imaging Sciences, 2012. Accepted Oct. 2012.\n\nNear-optimal algorithms for online matrix prediction. E Hazan, S Kale, S Shalev-Shwartz, Journal of Machine Learning Research -Proceedings Track. 23E. Hazan, S. Kale, and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. Journal of Ma- chine Learning Research -Proceedings Track, 23:38.1- 38.13, 2012.\n\nTensor-train decomposition. I V Oseledets, SIAM Journal on Scientific Computing. 335I.V.Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295-2317, 2011.\n\nThird-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging. M Kilmer, K Braman, N Hao, R Hoover, SIAM Journal on Matrix Analysis and Applications. 341M. Kilmer, K. Braman, N. Hao, and R. Hoover. Third-order tensors as operators on matrices: A theoretical and computa- tional framework with applications in imaging. SIAM Jour- nal on Matrix Analysis and Applications, 34(1):148-172, 2013.\n\nFactorization strategies for third-order tensors. Linear Algebra and its Applications, Special Issue in Honor of G. W. Stewart's 70th birthday. M E Kilmer, C D Martin, 10.1016/j.laa.2010.09.020435M. E. Kilmer and C. D. Martin. Factorization strategies for third-order tensors. Linear Algebra and its Applications, Special Issue in Honor of G. W. Stewart's 70th birthday, Vol. 435(3):641-658, 2011. DOI: 10.1016/j.laa.2010.09.020.\n\nTensor decompositions and applications. T Kolda, B Bader, SIAM Review. 513T. Kolda and B. Bader. Tensor decompositions and applica- tions. SIAM Review, 51(3):455-500, 2009.\n\nThe convex analysis of unitarily invariant matrix functions. A S Lewis, Journal of Convex Analysis. 21A. S. Lewis. The convex analysis of unitarily invariant ma- trix functions. Journal of Convex Analysis Volume 2 (1995), 2(1/2):173-183, 1995.\n\nTensor completion for estimating missing values in visual data. Pattern Analysis and Machine Intelligence. J Liu, P Musialski, P Wonka, J Ye, IEEE Transactions on. 351J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):208- 220, 2013.\n\nAn order-p tensor factorization with applications in imaging. C Martin, R Shafer, B Larue, SIAM Journal on Scientific Computing. 351C. Martin, R. Shafer, and B. LaRue. An order-p tensor fac- torization with applications in imaging. SIAM Journal on Scientific Computing, 35(1):A474-A490, 2013.\n\nImage coding using the singular value decomposition and vector quantization. C Mcgoldrick, W J Dowling, A Bury, Image Processing and its Applications. Fifth International Conference onC. McGoldrick, W. J. Dowling, and A. Bury. Image coding using the singular value decomposition and vector quantiza- tion. In Image Processing and its Applications, 1995., Fifth International Conference on, pages 296-300, 1995.\n\nAn iterative reconstruction method for spectral CT with tensorbased formulation and nuclear norm regularization. O Semerci, N Hao, M E Kilmer, E L Miller, The Second International Conference on Image Formation in X-Ray Computed Tomography. O. Semerci, N. Hao, M. E. Kilmer, and E. L. Miller. An iterative reconstruction method for spectral CT with tensor- based formulation and nuclear norm regularization. In The Second International Conference on Image Formation in X- Ray Computed Tomography, 2012.\n\nCharacterization of the subdifferential of some matrix norms. G Watson, Linear Algebra and its Applications. 1700G. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and its Applications, 170(0):33 -45, 1992.\n\nCompressive principal component pursuit. J Wright, A Ganesh, K Min, Y Ma, preprintJ. Wright, A. Ganesh, K. Min, and Y. Ma. Compres- sive principal component pursuit. preprint, http://www. columbia.edu/\u02dcjw2966, 2012.\n", "annotations": {"author": "[{\"end\":215,\"start\":84},{\"end\":346,\"start\":216},{\"end\":480,\"start\":347},{\"end\":606,\"start\":481},{\"end\":740,\"start\":607}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":90},{\"end\":227,\"start\":224},{\"end\":360,\"start\":355},{\"end\":489,\"start\":486},{\"end\":619,\"start\":613}]", "author_first_name": "[{\"end\":89,\"start\":84},{\"end\":223,\"start\":216},{\"end\":354,\"start\":347},{\"end\":485,\"start\":481},{\"end\":612,\"start\":607}]", "author_affiliation": "[{\"end\":214,\"start\":119},{\"end\":345,\"start\":250},{\"end\":479,\"start\":384},{\"end\":605,\"start\":510},{\"end\":739,\"start\":644}]", "title": "[{\"end\":81,\"start\":1},{\"end\":821,\"start\":741}]", "venue": null, "abstract": "[{\"end\":2039,\"start\":823}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2545,\"start\":2542},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2548,\"start\":2545},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2551,\"start\":2548},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2619,\"start\":2616},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2765,\"start\":2762},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2768,\"start\":2765},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2887,\"start\":2884},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3067,\"start\":3063},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4064,\"start\":4060},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4157,\"start\":4154},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4487,\"start\":4484},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4490,\"start\":4487},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4493,\"start\":4490},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4709,\"start\":4706},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4779,\"start\":4775},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4956,\"start\":4952},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5133,\"start\":5130},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5136,\"start\":5133},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5139,\"start\":5136},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5926,\"start\":5923},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5929,\"start\":5926},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5932,\"start\":5929},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6187,\"start\":6183},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8211,\"start\":8207},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8214,\"start\":8211},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8896,\"start\":8892},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9284,\"start\":9280},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9643,\"start\":9639},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10077,\"start\":10073},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10080,\"start\":10077},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10082,\"start\":10080},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10491,\"start\":10487},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11543,\"start\":11539},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12689,\"start\":12685},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17971,\"start\":17968},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18067,\"start\":18064},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18375,\"start\":18372},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18781,\"start\":18777},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18783,\"start\":18781},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19505,\"start\":19501},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19924,\"start\":19921},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20132,\"start\":20128},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20217,\"start\":20214},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20397,\"start\":20393},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21212,\"start\":21208},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21256,\"start\":21253},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21347,\"start\":21344},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22176,\"start\":22173},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23187,\"start\":23183},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24486,\"start\":24483},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25508,\"start\":25505}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25766,\"start\":25717},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25851,\"start\":25767},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26384,\"start\":25852},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27128,\"start\":26385},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27355,\"start\":27129},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27588,\"start\":27356},{\"attributes\":{\"id\":\"fig_7\"},\"end\":28025,\"start\":27589},{\"attributes\":{\"id\":\"fig_8\"},\"end\":28301,\"start\":28026},{\"attributes\":{\"id\":\"fig_9\"},\"end\":28591,\"start\":28302}]", "paragraph": "[{\"end\":3105,\"start\":2055},{\"end\":4158,\"start\":3107},{\"end\":5826,\"start\":4160},{\"end\":5977,\"start\":5854},{\"end\":6499,\"start\":6003},{\"end\":6801,\"start\":6501},{\"end\":7323,\"start\":6803},{\"end\":7568,\"start\":7325},{\"end\":7749,\"start\":7570},{\"end\":8215,\"start\":7875},{\"end\":8345,\"start\":8320},{\"end\":8477,\"start\":8393},{\"end\":8745,\"start\":8560},{\"end\":8956,\"start\":8747},{\"end\":10083,\"start\":9218},{\"end\":10770,\"start\":10375},{\"end\":10916,\"start\":10772},{\"end\":11070,\"start\":10935},{\"end\":11251,\"start\":11072},{\"end\":11785,\"start\":11253},{\"end\":12247,\"start\":11916},{\"end\":12619,\"start\":12292},{\"end\":12783,\"start\":12621},{\"end\":13474,\"start\":12882},{\"end\":14555,\"start\":13476},{\"end\":14973,\"start\":14582},{\"end\":15199,\"start\":14996},{\"end\":16157,\"start\":15201},{\"end\":16724,\"start\":16159},{\"end\":17357,\"start\":16767},{\"end\":18190,\"start\":17402},{\"end\":18408,\"start\":18246},{\"end\":18861,\"start\":18633},{\"end\":19176,\"start\":18948},{\"end\":19325,\"start\":19178},{\"end\":19659,\"start\":19393},{\"end\":19926,\"start\":19702},{\"end\":20049,\"start\":19988},{\"end\":20138,\"start\":20051},{\"end\":20398,\"start\":20188},{\"end\":21938,\"start\":20482},{\"end\":22250,\"start\":21960},{\"end\":22322,\"start\":22265},{\"end\":22383,\"start\":22324},{\"end\":22846,\"start\":22385},{\"end\":22965,\"start\":22848},{\"end\":23370,\"start\":23122},{\"end\":24278,\"start\":23434},{\"end\":24883,\"start\":24280},{\"end\":25716,\"start\":24914}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7874,\"start\":7750},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8319,\"start\":8216},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8559,\"start\":8478},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9087,\"start\":8957},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9171,\"start\":9087},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10330,\"start\":10084},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10934,\"start\":10917},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11915,\"start\":11786},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12881,\"start\":12784},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14581,\"start\":14556},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14995,\"start\":14974},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17401,\"start\":17358},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18245,\"start\":18191},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18632,\"start\":18409},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19392,\"start\":19326},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19701,\"start\":19660},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19987,\"start\":19927},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20187,\"start\":20139},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20481,\"start\":20399},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22264,\"start\":22251},{\"attributes\":{\"id\":\"formula_22\"},\"end\":23086,\"start\":22966},{\"attributes\":{\"id\":\"formula_23\"},\"end\":23121,\"start\":23086},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23433,\"start\":23371}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2053,\"start\":2041},{\"attributes\":{\"n\":\"2.\"},\"end\":5852,\"start\":5829},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6001,\"start\":5980},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8391,\"start\":8348},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9216,\"start\":9173},{\"attributes\":{\"n\":\"2.4.\"},\"end\":10373,\"start\":10332},{\"attributes\":{\"n\":\"3.\"},\"end\":12290,\"start\":12250},{\"attributes\":{\"n\":\"4.\"},\"end\":16765,\"start\":16727},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18946,\"start\":18864},{\"attributes\":{\"n\":\"5.\"},\"end\":21958,\"start\":21941},{\"attributes\":{\"n\":\"6.\"},\"end\":24912,\"start\":24886},{\"end\":25728,\"start\":25718},{\"end\":25876,\"start\":25853},{\"end\":27140,\"start\":27130},{\"end\":27367,\"start\":27357},{\"end\":27610,\"start\":27590},{\"end\":28037,\"start\":28027},{\"end\":28313,\"start\":28303}]", "table": null, "figure_caption": "[{\"end\":25766,\"start\":25730},{\"end\":25851,\"start\":25769},{\"end\":26384,\"start\":25880},{\"end\":27128,\"start\":26387},{\"end\":27355,\"start\":27142},{\"end\":27588,\"start\":27369},{\"end\":28025,\"start\":27613},{\"end\":28301,\"start\":28039},{\"end\":28591,\"start\":28315}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8906,\"start\":8898},{\"end\":15198,\"start\":15186},{\"end\":15608,\"start\":15600},{\"end\":15800,\"start\":15792},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21508,\"start\":21493},{\"end\":21570,\"start\":21562},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":23888,\"start\":23878},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":24567,\"start\":24559}]", "bib_author_first_name": "[{\"end\":28911,\"start\":28910},{\"end\":28919,\"start\":28918},{\"end\":28929,\"start\":28928},{\"end\":28936,\"start\":28935},{\"end\":28947,\"start\":28946},{\"end\":29271,\"start\":29270},{\"end\":29517,\"start\":29516},{\"end\":29519,\"start\":29518},{\"end\":29526,\"start\":29525},{\"end\":29528,\"start\":29527},{\"end\":29538,\"start\":29537},{\"end\":29774,\"start\":29773},{\"end\":29776,\"start\":29775},{\"end\":29786,\"start\":29785},{\"end\":29792,\"start\":29791},{\"end\":29798,\"start\":29797},{\"end\":30032,\"start\":30031},{\"end\":30039,\"start\":30038},{\"end\":30048,\"start\":30047},{\"end\":30055,\"start\":30054},{\"end\":30057,\"start\":30056},{\"end\":30397,\"start\":30396},{\"end\":30404,\"start\":30403},{\"end\":30413,\"start\":30412},{\"end\":30415,\"start\":30414},{\"end\":30896,\"start\":30895},{\"end\":30905,\"start\":30904},{\"end\":30914,\"start\":30913},{\"end\":31152,\"start\":31151},{\"end\":31154,\"start\":31153},{\"end\":31161,\"start\":31160},{\"end\":31163,\"start\":31162},{\"end\":31173,\"start\":31172},{\"end\":31183,\"start\":31182},{\"end\":31185,\"start\":31184},{\"end\":31452,\"start\":31451},{\"end\":31461,\"start\":31460},{\"end\":31469,\"start\":31468},{\"end\":31752,\"start\":31751},{\"end\":31754,\"start\":31753},{\"end\":32031,\"start\":32030},{\"end\":32041,\"start\":32040},{\"end\":32051,\"start\":32050},{\"end\":32058,\"start\":32057},{\"end\":32504,\"start\":32503},{\"end\":32506,\"start\":32505},{\"end\":32516,\"start\":32515},{\"end\":32518,\"start\":32517},{\"end\":32831,\"start\":32830},{\"end\":32840,\"start\":32839},{\"end\":33026,\"start\":33025},{\"end\":33028,\"start\":33027},{\"end\":33317,\"start\":33316},{\"end\":33324,\"start\":33323},{\"end\":33337,\"start\":33336},{\"end\":33346,\"start\":33345},{\"end\":33634,\"start\":33633},{\"end\":33644,\"start\":33643},{\"end\":33654,\"start\":33653},{\"end\":33943,\"start\":33942},{\"end\":33957,\"start\":33956},{\"end\":33959,\"start\":33958},{\"end\":33970,\"start\":33969},{\"end\":34391,\"start\":34390},{\"end\":34402,\"start\":34401},{\"end\":34409,\"start\":34408},{\"end\":34411,\"start\":34410},{\"end\":34421,\"start\":34420},{\"end\":34423,\"start\":34422},{\"end\":34843,\"start\":34842},{\"end\":35067,\"start\":35066},{\"end\":35077,\"start\":35076},{\"end\":35087,\"start\":35086},{\"end\":35094,\"start\":35093}]", "bib_author_last_name": "[{\"end\":28916,\"start\":28912},{\"end\":28926,\"start\":28920},{\"end\":28933,\"start\":28930},{\"end\":28944,\"start\":28937},{\"end\":28956,\"start\":28948},{\"end\":29278,\"start\":29272},{\"end\":29523,\"start\":29520},{\"end\":29535,\"start\":29529},{\"end\":29543,\"start\":29539},{\"end\":29783,\"start\":29777},{\"end\":29789,\"start\":29787},{\"end\":29795,\"start\":29793},{\"end\":29805,\"start\":29799},{\"end\":30036,\"start\":30033},{\"end\":30045,\"start\":30040},{\"end\":30052,\"start\":30049},{\"end\":30401,\"start\":30398},{\"end\":30410,\"start\":30405},{\"end\":30422,\"start\":30416},{\"end\":30902,\"start\":30897},{\"end\":30911,\"start\":30906},{\"end\":30921,\"start\":30915},{\"end\":31158,\"start\":31155},{\"end\":31170,\"start\":31164},{\"end\":31180,\"start\":31174},{\"end\":31192,\"start\":31186},{\"end\":31458,\"start\":31453},{\"end\":31466,\"start\":31462},{\"end\":31484,\"start\":31470},{\"end\":31764,\"start\":31755},{\"end\":32038,\"start\":32032},{\"end\":32048,\"start\":32042},{\"end\":32055,\"start\":32052},{\"end\":32065,\"start\":32059},{\"end\":32513,\"start\":32507},{\"end\":32525,\"start\":32519},{\"end\":32837,\"start\":32832},{\"end\":32846,\"start\":32841},{\"end\":33034,\"start\":33029},{\"end\":33321,\"start\":33318},{\"end\":33334,\"start\":33325},{\"end\":33343,\"start\":33338},{\"end\":33349,\"start\":33347},{\"end\":33641,\"start\":33635},{\"end\":33651,\"start\":33645},{\"end\":33660,\"start\":33655},{\"end\":33954,\"start\":33944},{\"end\":33967,\"start\":33960},{\"end\":33975,\"start\":33971},{\"end\":34399,\"start\":34392},{\"end\":34406,\"start\":34403},{\"end\":34418,\"start\":34412},{\"end\":34430,\"start\":34424},{\"end\":34850,\"start\":34844},{\"end\":35074,\"start\":35068},{\"end\":35084,\"start\":35078},{\"end\":35091,\"start\":35088},{\"end\":35097,\"start\":35095}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":51789432},\"end\":29204,\"start\":28807},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":121030321},\"end\":29451,\"start\":29206},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1254778},\"end\":29733,\"start\":29453},{\"attributes\":{\"doi\":\"11:1-11:37\",\"id\":\"b3\",\"matched_paper_id\":7128002},\"end\":29944,\"start\":29735},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17596518},\"end\":30317,\"start\":29946},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5984453},\"end\":30819,\"start\":30319},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2535717},\"end\":31095,\"start\":30821},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7233826},\"end\":31395,\"start\":31097},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8608209},\"end\":31721,\"start\":31397},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":207059098},\"end\":31910,\"start\":31723},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9254348},\"end\":32357,\"start\":31912},{\"attributes\":{\"doi\":\"10.1016/j.laa.2010.09.020\",\"id\":\"b11\"},\"end\":32788,\"start\":32359},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16074195},\"end\":32962,\"start\":32790},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8074092},\"end\":33207,\"start\":32964},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":82504},\"end\":33569,\"start\":33209},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":20024900},\"end\":33863,\"start\":33571},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":122985805},\"end\":34275,\"start\":33865},{\"attributes\":{\"id\":\"b17\"},\"end\":34778,\"start\":34277},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":121679641},\"end\":35023,\"start\":34780},{\"attributes\":{\"id\":\"b19\"},\"end\":35240,\"start\":35025}]", "bib_title": "[{\"end\":28908,\"start\":28807},{\"end\":29268,\"start\":29206},{\"end\":29514,\"start\":29453},{\"end\":29771,\"start\":29735},{\"end\":30029,\"start\":29946},{\"end\":30394,\"start\":30319},{\"end\":30893,\"start\":30821},{\"end\":31149,\"start\":31097},{\"end\":31449,\"start\":31397},{\"end\":31749,\"start\":31723},{\"end\":32028,\"start\":31912},{\"end\":32828,\"start\":32790},{\"end\":33023,\"start\":32964},{\"end\":33314,\"start\":33209},{\"end\":33631,\"start\":33571},{\"end\":33940,\"start\":33865},{\"end\":34388,\"start\":34277},{\"end\":34840,\"start\":34780}]", "bib_author": "[{\"end\":28918,\"start\":28910},{\"end\":28928,\"start\":28918},{\"end\":28935,\"start\":28928},{\"end\":28946,\"start\":28935},{\"end\":28958,\"start\":28946},{\"end\":29280,\"start\":29270},{\"end\":29525,\"start\":29516},{\"end\":29537,\"start\":29525},{\"end\":29545,\"start\":29537},{\"end\":29785,\"start\":29773},{\"end\":29791,\"start\":29785},{\"end\":29797,\"start\":29791},{\"end\":29807,\"start\":29797},{\"end\":30038,\"start\":30031},{\"end\":30047,\"start\":30038},{\"end\":30054,\"start\":30047},{\"end\":30060,\"start\":30054},{\"end\":30403,\"start\":30396},{\"end\":30412,\"start\":30403},{\"end\":30424,\"start\":30412},{\"end\":30904,\"start\":30895},{\"end\":30913,\"start\":30904},{\"end\":30923,\"start\":30913},{\"end\":31160,\"start\":31151},{\"end\":31172,\"start\":31160},{\"end\":31182,\"start\":31172},{\"end\":31194,\"start\":31182},{\"end\":31460,\"start\":31451},{\"end\":31468,\"start\":31460},{\"end\":31486,\"start\":31468},{\"end\":31766,\"start\":31751},{\"end\":32040,\"start\":32030},{\"end\":32050,\"start\":32040},{\"end\":32057,\"start\":32050},{\"end\":32067,\"start\":32057},{\"end\":32515,\"start\":32503},{\"end\":32527,\"start\":32515},{\"end\":32839,\"start\":32830},{\"end\":32848,\"start\":32839},{\"end\":33036,\"start\":33025},{\"end\":33323,\"start\":33316},{\"end\":33336,\"start\":33323},{\"end\":33345,\"start\":33336},{\"end\":33351,\"start\":33345},{\"end\":33643,\"start\":33633},{\"end\":33653,\"start\":33643},{\"end\":33662,\"start\":33653},{\"end\":33956,\"start\":33942},{\"end\":33969,\"start\":33956},{\"end\":33977,\"start\":33969},{\"end\":34401,\"start\":34390},{\"end\":34408,\"start\":34401},{\"end\":34420,\"start\":34408},{\"end\":34432,\"start\":34420},{\"end\":34852,\"start\":34842},{\"end\":35076,\"start\":35066},{\"end\":35086,\"start\":35076},{\"end\":35093,\"start\":35086},{\"end\":35099,\"start\":35093}]", "bib_venue": "[{\"end\":30601,\"start\":30521},{\"end\":28974,\"start\":28958},{\"end\":29315,\"start\":29280},{\"end\":29573,\"start\":29545},{\"end\":29823,\"start\":29817},{\"end\":30121,\"start\":30060},{\"end\":30519,\"start\":30424},{\"end\":30939,\"start\":30923},{\"end\":31226,\"start\":31194},{\"end\":31541,\"start\":31486},{\"end\":31802,\"start\":31766},{\"end\":32115,\"start\":32067},{\"end\":32501,\"start\":32359},{\"end\":32859,\"start\":32848},{\"end\":33062,\"start\":33036},{\"end\":33371,\"start\":33351},{\"end\":33698,\"start\":33662},{\"end\":34014,\"start\":33977},{\"end\":34515,\"start\":34432},{\"end\":34887,\"start\":34852},{\"end\":35064,\"start\":35025}]"}}}, "year": 2023, "month": 12, "day": 17}