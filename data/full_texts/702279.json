{"id": 702279, "updated": "2023-09-30 13:49:58.273", "metadata": {"title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms", "authors": "[{\"first\":\"Han\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Kashif\",\"last\":\"Rasul\",\"middle\":[]},{\"first\":\"Roland\",\"last\":\"Vollgraf\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 8, "day": 25}, "abstract": "We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1708.07747", "mag": "2750384547", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1708-07747", "doi": null}}, "content": {"source": {"pdf_hash": "82f1ef648485611779b99d67204c72d39a9c3c4e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1708.07747v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0491726affa3e07163e4f919e67e16b45ada412a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/82f1ef648485611779b99d67204c72d39a9c3c4e.txt", "contents": "\nFashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\n15 Sep 2017\n\nHan Xiao han.xiao@zalando.de \nZalando Research M\u00fchlenstra\u00dfe 25\n10243Berlin\n\nKashif Rasul kashif.rasul@zalando.de \nZalando Research\nM\u00fchlenstra\u00dfe 2510243Berlin\n\nRoland Vollgraf roland.vollgraf@zalando.de \nZalando Research\nM\u00fchlenstra\u00dfe 2510243Berlin\n\nFashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\n15 Sep 2017\nWe present Fashion-MNIST, a new dataset comprising of 28 \u00d7 28 grayscale images of 70, 000 fashion products from 10 categories, with 7, 000 images per category. The training set has 60, 000 images and the test set has 10, 000 images.Fashion-MNIST is intended to serve as a direct dropin replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist.\n\nIntroduction\n\nThe MNIST dataset comprising of 10-class handwritten digits, was first introduced by LeCun et al. [1998] in 1998. At that time one could not have foreseen the stellar rise of deep learning techniques and their performance. Despite the fact that today deep learning can do so much the simple MNIST dataset has become the most widely used testbed in deep learning, surpassing CIFAR-10 [Krizhevsky and Hinton, 2009] and ImageNet [Deng et al., 2009] in its popularity via Google trends 1 . Despite its simplicity its usage does not seem to be decreasing despite calls for it in the deep learning community.\n\nThe reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box.\n\nOur aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10 classes 70, 000 grayscale images in the size of 28 \u00d7 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in Wan et al. [2013], Ciregan et al. [2012].\n\nWe also looked at the EMNIST dataset provided by Cohen et al. [2017], an extended version of MNIST that extends the number of classes by introducing uppercase and lowercase characters. How-\n\n\nFashion-MNIST Dataset\n\nFashion-MNIST is based on the assortment on Zalando's website 2 . Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit. The original picture has a light-gray background (hexadecimal color: #fdfdfd) and stored in 762 \u00d7 1000 JPEG format. For efficiently serving different frontend components, the original picture is resampled with multiple resolutions, e.g. large, medium, small, thumbnail and tiny.\n\nWe use the front look thumbnail images of 70, 000 unique products to build Fashion-MNIST. Those products come from different gender groups: men, women, kids and neutral. In particular, whitecolor products are not included in the dataset as they have low contrast to the background. The thumbnails (51 \u00d7 73) are then fed into the following conversion pipeline, which is visualized in Figure 1.\n\n1. Converting the input to a PNG image. 2. Trimming any edges that are close to the color of the corner pixels. The \"closeness\" is defined by the distance within 5% of the maximum possible intensity in RGB space. 3. Resizing the longest edge of the image to 28 by subsampling the pixels, i.e. some rows and columns are skipped over. 4. Sharpening pixels using a Gaussian operator of the radius and standard deviation of 1.0, with increasing effect near outlines. 5. Extending the shortest edge to 28 and put the image to the center of the canvas. 6. Negating the intensities of the image. 7. Converting the image to 8-bit grayscale pixels. Figure 1: Diagram of the conversion process used to generate Fashion-MNIST dataset. Two examples from dress and sandals categories are depicted, respectively. Each column represents a step described in section 2. For the class labels, we use the silhouette code of the product. The silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando. Each product contains only one silhouette code. Table 2 gives a summary of all class labels in Fashion-MNIST with examples for each class.\n\nFinally, the dataset is divided into a training and a test set. The training set receives a randomlyselected 6, 000 examples from each class. Images and labels are stored in the same file format as the MNIST data set, which is designed for storing vectors and multidimensional matrices. The result files are listed in Table 1. We sort examples by their labels while storing, resulting in smaller label files after compression comparing to the MNIST. It is also easier to retrieve examples with a certain class label. The data shuffling job is therefore left to the algorithm developer. \n\n\nExperiments\n\nWe provide some classification results in Table 3 to form a benchmark on this data set. All algorithms are repeated 5 times by shuffling the training data and the average accuracy on the test set is reported. The benchmark on the MNIST dataset is also included for a side-by-side comparison. A more comprehensive table with explanations on the algorithms can be found on https://github.com/zalandoresearch/fashion-mnist.    \n\n\nConclusions\n\nThis paper introduced Fashion-MNIST, a fashion product images dataset intended to be a dropin replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset.\n\nTable 1 :\n1Files contained in the Fashion-MNIST dataset.Name \nDescription \n# Examples \nSize \n\ntrain-images-idx3-ubyte.gz Training set images \n60, 000 25 MBytes \ntrain-labels-idx1-ubyte.gz \nTraining set labels \n60, 000 \n140 Bytes \nt10k-images-idx3-ubyte.gz Test set images \n10, 000 4.2 MBytes \nt10k-labels-idx1-ubyte.gz \nTest set labels \n10, 000 \n92 Bytes \n\n\n\nTable 2 :\n2Class names and example images in Fashion-MNIST dataset.Label Description Examples \n\n0 \nT-Shirt/Top \n\n1 \nTrouser \n\n2 \nPullover \n\n3 \nDress \n\n4 \nCoat \n\n5 \nSandals \n\n6 \nShirt \n\n7 \nSneaker \n\n8 \nBag \n\n9 \nAnkle boots \n\n\n\nTable 3 :\n3Benchmark on Fashion-MNIST (Fashion) and MNIST.Test Accuracy \n\n\n\nTable 3 -\n3continued from previous pageTest Accuracy \n\n\n\nTable 3 -\n3continued from previous pageTest Accuracy \n\n\n\nTable 3 -\n3continued from previous pageTest Accuracy \n\n\nhttps://trends.google.com/trends/explore?date=all&q=mnist,CIFAR,ImageNet\nZalando is the Europe's largest online fashion platform. http://www.zalando.com\n\nMulti-column deep neural networks for image classification. D Ciregan, U Meier, J Schmidhuber, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEED. Ciregan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classifi- cation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3642-3649. IEEE, 2012.\n\nEmnist: an extension of mnist to handwritten letters. G Cohen, S Afshar, J Tapson, A Van Schaik, arXiv:1702.05373arXiv preprintG. Cohen, S. Afshar, J. Tapson, and A. van Schaik. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical im- age database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248-255. IEEE, 2009.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nRegularization of neural networks using dropconnect. L Wan, M Zeiler, S Zhang, Y L Cun, R Fergus, Proceedings of the 30th international conference on machine learning (ICML-13). the 30th international conference on machine learning (ICML-13)L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th international conference on machine learning (ICML- 13), pages 1058-1066, 2013.\n", "annotations": {"author": "[{\"end\":171,\"start\":96},{\"end\":254,\"start\":172},{\"end\":343,\"start\":255}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":100},{\"end\":184,\"start\":179},{\"end\":270,\"start\":262}]", "author_first_name": "[{\"end\":99,\"start\":96},{\"end\":178,\"start\":172},{\"end\":261,\"start\":255}]", "author_affiliation": "[{\"end\":170,\"start\":126},{\"end\":253,\"start\":210},{\"end\":342,\"start\":299}]", "title": "[{\"end\":82,\"start\":1},{\"end\":425,\"start\":344}]", "venue": null, "abstract": "[{\"end\":993,\"start\":438}]", "bib_ref": "[{\"end\":1113,\"start\":1107},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1421,\"start\":1392},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1454,\"start\":1435},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2599,\"start\":2582},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2622,\"start\":2601},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2693,\"start\":2674}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":6789,\"start\":6431},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":7015,\"start\":6790},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":7091,\"start\":7016},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":7148,\"start\":7092},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":7205,\"start\":7149},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":7262,\"start\":7206}]", "paragraph": "[{\"end\":1611,\"start\":1009},{\"end\":1982,\"start\":1613},{\"end\":2623,\"start\":1984},{\"end\":2814,\"start\":2625},{\"end\":3394,\"start\":2840},{\"end\":3788,\"start\":3396},{\"end\":4962,\"start\":3790},{\"end\":5550,\"start\":4964},{\"end\":5990,\"start\":5566},{\"end\":6430,\"start\":6006}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4879,\"start\":4872},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5290,\"start\":5282},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":5615,\"start\":5608}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1007,\"start\":995},{\"attributes\":{\"n\":\"2\"},\"end\":2838,\"start\":2817},{\"attributes\":{\"n\":\"3\"},\"end\":5564,\"start\":5553},{\"attributes\":{\"n\":\"4\"},\"end\":6004,\"start\":5993},{\"end\":6441,\"start\":6432},{\"end\":6800,\"start\":6791},{\"end\":7026,\"start\":7017},{\"end\":7102,\"start\":7093},{\"end\":7159,\"start\":7150},{\"end\":7216,\"start\":7207}]", "table": "[{\"end\":6789,\"start\":6488},{\"end\":7015,\"start\":6858},{\"end\":7091,\"start\":7075},{\"end\":7148,\"start\":7132},{\"end\":7205,\"start\":7189},{\"end\":7262,\"start\":7246}]", "figure_caption": "[{\"end\":6488,\"start\":6443},{\"end\":6858,\"start\":6802},{\"end\":7075,\"start\":7028},{\"end\":7132,\"start\":7104},{\"end\":7189,\"start\":7161},{\"end\":7246,\"start\":7218}]", "figure_ref": "[{\"end\":3787,\"start\":3779},{\"end\":4438,\"start\":4430}]", "bib_author_first_name": "[{\"end\":7478,\"start\":7477},{\"end\":7489,\"start\":7488},{\"end\":7498,\"start\":7497},{\"end\":7854,\"start\":7853},{\"end\":7863,\"start\":7862},{\"end\":7873,\"start\":7872},{\"end\":7883,\"start\":7882},{\"end\":8125,\"start\":8124},{\"end\":8133,\"start\":8132},{\"end\":8141,\"start\":8140},{\"end\":8154,\"start\":8150},{\"end\":8160,\"start\":8159},{\"end\":8166,\"start\":8165},{\"end\":8532,\"start\":8531},{\"end\":8546,\"start\":8545},{\"end\":8704,\"start\":8703},{\"end\":8713,\"start\":8712},{\"end\":8723,\"start\":8722},{\"end\":8733,\"start\":8732},{\"end\":8981,\"start\":8980},{\"end\":8988,\"start\":8987},{\"end\":8998,\"start\":8997},{\"end\":9007,\"start\":9006},{\"end\":9009,\"start\":9008},{\"end\":9016,\"start\":9015}]", "bib_author_last_name": "[{\"end\":7486,\"start\":7479},{\"end\":7495,\"start\":7490},{\"end\":7510,\"start\":7499},{\"end\":7860,\"start\":7855},{\"end\":7870,\"start\":7864},{\"end\":7880,\"start\":7874},{\"end\":7894,\"start\":7884},{\"end\":8130,\"start\":8126},{\"end\":8138,\"start\":8134},{\"end\":8148,\"start\":8142},{\"end\":8157,\"start\":8155},{\"end\":8163,\"start\":8161},{\"end\":8174,\"start\":8167},{\"end\":8543,\"start\":8533},{\"end\":8553,\"start\":8547},{\"end\":8710,\"start\":8705},{\"end\":8720,\"start\":8714},{\"end\":8730,\"start\":8724},{\"end\":8741,\"start\":8734},{\"end\":8985,\"start\":8982},{\"end\":8995,\"start\":8989},{\"end\":9004,\"start\":8999},{\"end\":9013,\"start\":9010},{\"end\":9023,\"start\":9017}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2161592},\"end\":7797,\"start\":7417},{\"attributes\":{\"doi\":\"arXiv:1702.05373\",\"id\":\"b1\"},\"end\":8069,\"start\":7799},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":57246310},\"end\":8474,\"start\":8071},{\"attributes\":{\"id\":\"b3\"},\"end\":8644,\"start\":8476},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14542261},\"end\":8925,\"start\":8646},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2936324},\"end\":9382,\"start\":8927}]", "bib_title": "[{\"end\":7475,\"start\":7417},{\"end\":8122,\"start\":8071},{\"end\":8701,\"start\":8646},{\"end\":8978,\"start\":8927}]", "bib_author": "[{\"end\":7488,\"start\":7477},{\"end\":7497,\"start\":7488},{\"end\":7512,\"start\":7497},{\"end\":7862,\"start\":7853},{\"end\":7872,\"start\":7862},{\"end\":7882,\"start\":7872},{\"end\":7896,\"start\":7882},{\"end\":8132,\"start\":8124},{\"end\":8140,\"start\":8132},{\"end\":8150,\"start\":8140},{\"end\":8159,\"start\":8150},{\"end\":8165,\"start\":8159},{\"end\":8176,\"start\":8165},{\"end\":8545,\"start\":8531},{\"end\":8555,\"start\":8545},{\"end\":8712,\"start\":8703},{\"end\":8722,\"start\":8712},{\"end\":8732,\"start\":8722},{\"end\":8743,\"start\":8732},{\"end\":8987,\"start\":8980},{\"end\":8997,\"start\":8987},{\"end\":9006,\"start\":8997},{\"end\":9015,\"start\":9006},{\"end\":9025,\"start\":9015}]", "bib_venue": "[{\"end\":7583,\"start\":7512},{\"end\":7851,\"start\":7799},{\"end\":8215,\"start\":8176},{\"end\":8529,\"start\":8476},{\"end\":8766,\"start\":8743},{\"end\":9103,\"start\":9025},{\"end\":9168,\"start\":9105}]"}}}, "year": 2023, "month": 12, "day": 17}