{"id": 220487047, "updated": "2023-12-08 19:43:43.906", "metadata": {"title": "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning", "authors": "[{\"first\":\"Hongyi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kartik\",\"last\":\"Sreenivasan\",\"middle\":[]},{\"first\":\"Shashank\",\"last\":\"Rajput\",\"middle\":[]},{\"first\":\"Harit\",\"last\":\"Vishwakarma\",\"middle\":[]},{\"first\":\"Saurabh\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Jy-yong\",\"last\":\"Sohn\",\"middle\":[]},{\"first\":\"Kangwook\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Dimitris\",\"last\":\"Papailiopoulos\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 7, "day": 9}, "abstract": "Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness, and exhibit that with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis).", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "3106047871", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/WangSRVASLP20", "doi": null}}, "content": {"source": {"pdf_hash": "0a93fce82afa33b218728a19d19d0ae40401b396", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.05084v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "58a07c4d2a656b1e05da86466fb65e10ceae6d48", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0a93fce82afa33b218728a19d19d0ae40401b396.txt", "contents": "\nAttack of the Tails: Yes, You Really Can Backdoor Federated Learning\n\n\nHongyi Wang \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nKartik Sreenivasan \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nShashank Rajput \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nHarit Vishwakarma \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nSaurabh Agarwal \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nJy-Yong Sohn \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nKangwook Lee \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nDimitris Papailiopoulos \nUniversity of Wisconsin-Madison k Korea Advanced Institute of Science and Technology\n\n\nAttack of the Tails: Yes, You Really Can Backdoor Federated Learning\n\nDue to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness, and exhibit that with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis).\n\nIntroduction\n\nFederated learning (FL) offers a new paradigm for decentralized model training, across a set of users, each holding private data. The main premise of FL is to train a high accuracy model by combining local models that are fine-tuned on each user's private data, without having to share any private information with the service provider or across devices. Several current applications of FL include text prediction in mobile device messaging [1,2,3,4,5], speech recognition [6], face recognition for device access [7,8], and maintaining decentralized predictive models across health organizations [9,10,11].\n\nAcross most FL settings, it is assumed that there is no single, central authority that owns or verifies the training data or user hardware, and it has been argued by many recent studies that FL lends itself to new adversarial attacks during decentralized model training [12,13,14,15,16,17,18,19,20,21,22,23,24,25]. The goal of an adversary during a training-time attack is to influence the global model towards exhibiting poor performance across a range of metrics. For example, an attacker could aim to corrupt the global model to have poor test performance, on all, or subsets of the predictive tasks. Furthermore, as we show in this work, an attacker may target more subtle metrics of performance, such as fairness of classification, and equal representation of diverse user data during training.\n\nInitiated by the work of Bagdasaryan et al. [13], a line of recent literature presents ways to insert backdoors during Federated Learning. The goal of a backdoor, is to corrupt the global FL model into a targeted mis-prediction on a specific subtask, e.g., by forcing an image classifier to misclasify green cars as frogs [13]. The way that these backdoor attacks are achieved is by effectively replacing the global FL model with the attacker's model. Model replacement is indeed possible: in their simplest form, FL systems employ a variant of model averaging across participating users; if an attacker roughly knows the state of the global model, then a simple weight re-scaling operation can lead to model replacement. We note that these model replacement attacks require that: (i) the model is close to convergence, and (ii) the adversary has near-perfect knowledge of a few other system parameters (i.e., number of users, data set size, etc.).\n\nOne can of course wonder whether it is possible to defend against such backdoor attacks, and in the process guarantee robust training in the presence of adversaries. An argument against the existence of sophisticated defenses that may require access to local models, is the fact that some FL systems employ S A , i.e., a secure version of model averaging [26]. When S A is in place, it is impossible for a central service provider to examine individual user models. However, it is important to note that even in the absence of S A , the service provider is limited in its capacity to determine which model updates are malicious, as this may violate privacy and fairness concerns [12].\n\nFollow-up work by Sun et al. [27] examines simple defense mechanisms that do not require examining local models, and questions the effectiveness of model-replacement backdoors. Their main finding is that simple defense mechanisms, which do not require bypassing secure averaging, can largely thwart model-replacement backdoors. Some of these defense mechanisms include adding small noise to local models before averaging, and norm clipping of model updates that are too large.\n\nIt currently remains an open problem whether FL systems can be rendered robust to backdoors. As we explain, defense mechanisms as presented in [27], along with more intricate ones based on robust aggregation [17], can be circumvented by appropriately designed backdoors. Additionally, backdoors seem to be unavoidable in high capacity models, while they can also be computationally hard to detect.\n\nOur contributions. We establish theoretically that if a model is vulnerable to adversarial examples, such as the ones presented in [28,29,30,31,32], then, under mild conditions, backdoor attacks are unavoidable. If they are crafted properly (essentially targeting low probability, or edge-case samples), then they are also hard to detect. Specifically, we first establish the following theoretical results.\n\n\nTheorem 1. (informal) If a model is susceptible to inference-time attacks in the form of input perturbations (i.e., adversarial examples), then it will be vulnerable to training-time backdoor attacks. Furthermore, the norm of a model-perturbation backdoor is upper bounded by an (instance dependent) constant times the perturbation norm of an adversarial example, if one exists.\n\n\nProposition 1. (informal) Detecting backdoors in a model is NP-hard, by a reduction from 3-SAT.\n\n\nProposition 2. (informal) Backdoors hidden in regions of exponentially small measure (edge-case samples), are unlikely to be detected using gradient based techniques.\n\nBased on cues from our theory, and inspired by the work of Bagdasaryan et al. [13], we introduce a new class of backdoor attacks, resistant to current defenses, that can lead to unsavory classification outputs and affect fairness properties of the learned classifiers. We refer to these attacks as edge-case backdoors. Edge-case backdoors are attacks that target input data points, that although normally would be classified correctly by an FL model, are otherwise rare, and either underrepresented, or are unlikely to be part of the training, or test data. See Fig. 1  We examine two ways of inserting these attacks: data poisoning and model poisoning. In the data poisoning (i.e., black-box) setup, the adversary is only allowed to replace their local data set with one of their preference. Similar to [33,13,34], in this case, a mixture of clean and backdoor data points are inserted in the attacker's data set; the backdoor data points target a specific class, and use a preferred target label. In the model poisoning (i.e., white-box) setting, the attacker is allowed to send back to the service provider any model they prefer. This is the setup that [13,14] focus on. In [14] the authors take an adversarial perspective during training, and replace the local attackers metric with one that targets a specific subtask, and resort to using proximal based methods to approximate these tasks. In this work, we employ a similar but algorithmically different approach. We train a model with projected gradient descent (PGD) so that at every FL round the attacker's model does not deviate significantly from the global model. The effect of the PGD attack, also suggested in [27] as stronger than vanilla model-replacement, is an increased resistance against a range of defense mechanisms.\n\nWe show across a suite of prediction tasks (image classification, OCR, sentiment analysis, and text prediction), data sets (CIFAR10/ImageNet/EMNIST/Reddit/Sentiment140), and models (VGG-9/11/LeNet/LSTMs) that our edge-case attacks can be hard-wired in FL models, as long as 0.5-1% of total number of edge users are adversarial. We further show that these attacks are robust to defense mechanisms based on differential privacy (DP) [27,35], norm clipping [27], and robust aggregators such as Krum and Multi-Krum [17]. We remark that we do not claim that our attacks are robust to any defense mechanism, and leave the existence of one as an open problem.\n\nThe implication of edge-case backdoors. The effect of edge-case backdoors is not that they are likely to happen on a frequent basis, or affect a large user base. Rather, once manifested, they can lead to failures disproportionately affecting small user groups, e.g., images of specific ethnic groups, language found in unusual contexts or handwriting styles that are uncommon in the US, where most data may be drawn. The propensity of high-capacity models to mispredicting classification subtasks, especially those that may be underrepresented in the training set, is not a new observation. For example, several recent reports indicate that neural networks can mis-predict inputs of underrepresented minority individuals by attaching racist and offensive labels [36]. Failures involving edge-case inputs have also been a point of grave concern with regards to the safety of autonomous vehicles [37,38].\n\nOur work indicates that edge-case failures of that manner, can unfortunately be hard-wired through backdoors to FL models. Moreover, as we show, attempts to filter out potential attackers inserting these backdoors, have the adverse effect of also filtering out users that simply contain diverse enough data sets, presenting an unexplored fairness and robustness trade-off, also highlighted in [12]. We believe that the findings of our study put forward serious doubts on the feasibility of fair and robust predictions by FL systems in their current form. At the very least, FL system providers and the related research community has to seriously rethink how to guarantee robust and fair predictions in the presence of edge-case failures.\n\nRelated Work Due to its emphasis on preserving privacy, FL has gained significant attention in the community [12,39,40,41,42]. Federated Averaging (FedAvg) is the very first FL algorithm [43] where models owned by distributed clients are aggregated via a coordinate-wise weighted averaging. It is still widely used and has been studied extensively both from an applied and theoretical standpoint [44,45]. S A [46] is a variant which provably ensures that a client's model and data cannot be inspected by the parameter server during FedAvg. Alternatives to simple weighted averaging have also been proposed: PFNM [47] and FedMA [48] proposes using neural matching, and [49] uses optimal transport to achieve model aggregation.\n\nData Poisoning attacks work by manipulating the client data during train time. Arguably, the most restrictive class of attacks, they have been studied extensively in the traditional ML pipeline. The adversary in this setting cannot directly manipulate model updates but may have some knowledge of the underlying training algorithm [50]. Mahloujifar et al. [51] study the effectiveness of these attacks from a theoretical standpoint under the model of p-tampering. Data poisoning attacks may be targeted [16,33] or untargeted [21]. Chen et al. [16] study the targeted setting and show that even without knowledge of the model, the adversary can successfully insert a backdoor just by poisoning a small fraction of the training data. Rubinstein et al. [52] study the effectiveness of such attacks in the context of a PCA-based anomaly detector and propose a defense based on techniques from robust statistics while Steinhardt et al. [53] suggest using outlier detection [54] as a solution. Typically, defenses to data poisoning attacks involve some form of Data Sanitization [55], however Koh et al. [34] show that even these defenses can be overcome. [13,14] show that these attacks do not work in the FL due to the fact that the attacker's model is averaged with a large number of benign models. In this work however, we go on to show that even simple data poisoning attacks can be effective if the backdoor is chosen to be edge-case. Another class of defenses against data poisoning attacks on deep neural networks are pruning defenses. Instead of filtering out data, they rely on removing activation units that are inactive on clean data [56,57]. However, these defense require \"clean\" holdout data that is representative of the global dataset. Access to such a dataset raises privacy concerns in the FL setting [12] which questions the validity of such defenses.\n\nMachine teaching is the process by which one designs training data to drive a learning algorithm to a target model [58]. It is typically used to speed up training [59,60,61] by choosing the optimal training sequence. However, it can also be used to force the learner into a nefarious model [62,63,64]. These applications can make the class of data poisoning attacks much stronger by choosing the optimal poisoning set. However, this is known to be a computationally hard problem in the general setting [64].\n\nWith a carefully chosen scaling factor, model poisoning attacks in the FL setting can be used to completely replace the global model which is known as model replacement [13]. Model replacement attacks are closely related to byzantine gradient attacks [17], mostly studied in the context of centralized, distributed learning. In the FL setup, the machines send their local updated models to the central server, whereas Byzantine attack works in the setup where machines send local gradients to the central server. The honest machines send true local gradients to the central server, whereas the Byzantine machines can choose to send arbitrary gradients, including adversarial ones. Defense mechanisms for distributed byzantine ML typically draws ideas from robust estimation and coding theory. Blanchard et al. [17] propose K as an alternative to the simple mean as an aggregation rule as a means for byzantine robustness. However, we show that by carefully tuning our algorithm, we can actually use K to make our attack stronger. Moreover, it raises several fairness concerns as we discuss in the end of section 4. Chen et al. [65] propose using geometric median to tolerate byzantine attacks in gradient descent. Using robust estimation to defend against byzantine attacks has been studied extensively [66,67,68,69,70,24,71]. D [72] is provides problem-independent robustness guarantees while being significantly faster than median-based approaches using elements from coding theory. [73] proposes a framework to guarantee byzantine robustness for S SGD. D [74] combines ideas from robust estimation and coding theory to trade-off between performance and robustness.\n\n\nEdge-case backdoor attacks for Federated Learning\n\nFederated Learning [41] refers to a general set of techniques for model training, performed over private data owned by individual users without compromising data privacy. Typically, FL aims to minimize an empirical loss (x,y)\u2208D (w; x, y) by optimizing over the model parameters w. Here, is the loss function, and D {(x i , y i )} the union of K client datasets, i.e., D :\nD 1 \u222a . . . \u222a D K .\nNote that one might be tempted to collect all the data in a central node, but this cannot be done without compromising user data privacy. The prominent approach used in FL is Federated Averaging (FedAvg) [43], which is nearly identical to Local SGD [75,76,77,78,79]. Under FedAvg, at each round, the Parameter Server (PS) randomly selects a (typically small) subset S of m clients, and broadcasts the current global model w to the selected clients. Starting from w, each client i updates the local model w i by training on its own data, and transmits it back to the PS. Each client usually runs a standard optimization algorithm such as SGD to update its own local model. After aggregating the local models, the PS updates the global model by performing a weighted average\nw next w + i\u2208S n i n S (w i \u2212 w)\nwhere n i |D i |, and n S i\u2208S n i is the total number of training data used at the selected clients.\n\n\nEdge-case backdoor attacks\n\nIn this work, we focus on attack algorithms that leverage data from the tail of the input data distribution. We first formally define a p-edge-case example set as follows.\nDefinition 1. Let X \u223c P X . A set of labeled examples D edge {(x i , y i )} i is called a p-edge-case examples set if P X (x) \u2264 p, \u2200(x, y) \u2208 D edge for small p > 0.\nIn other words, a p-edge-case example set with small value of p can be viewed as a set of labeled examples where input features are chosen from the heavy tails of the feature distribution. Note that we do not have any conditions on the labels, i.e., one can consider arbitrary labels.\n\n\nRemark 1.\n\nNote that we exclude the case of p 0. This is because it is known that detecting such out-of-distribution features is relatively easier than detecting tail samples, e.g., see Liang et al. [80].\n\nIn the adversarial setting we are focused on, a fraction of attackers, say f out of K, are assumed to have either black-box or white-box access to their devices. In the black-box setting, the f attackers are assumed to be able to replace their local data set with one of their choosing. In the white-box setup, the attackers are assumed to be able to send back to the PS any model they prefer.\n\nGiven that a p-edge-case example set D edge is available to the f attackers, their goal is to inject a backdoor to the global model so that the global model predicts y i when the input is x i , for all (x i , y i ) \u2208 D edge , where y i is the target label chosen by the attacker and in general, may not be the true label. Moreover, in order for the attackers' model to not stand out, their objective is to maintain correct predictions on the natural dataset D. Therefore, similar to [13,14], the objective of an attacker is to maximize the accuracy of the classifier on D \u222a D edge .\n\nWe now propose three different attack strategies, depending on the attackers' access model.\n\n\n(a) Black-box attack:\n\nUnder the black-box setting, the attackers perform standard local training, without modification, on a locally crafted dataset D aiming to maximize the accuracy of the global model on D \u222a D edge . Inspired by the observations made in [33,13], we construct D by combining some data points from D and some from D edge . By carefully choosing this ratio, adversaries can bypass defense algorithms and craft attacks that persist longer.\n\n(b) PGD attack: Under this attack, adversaries apply projected gradient descent on the losses for D D \u222a D edge . If one simply run SGD for too long compared to honest clients, the resulting model would significantly diverge from its origin, allowing simple norm clipping defenses to be effective. To avoid this, adversaries can periodically project the model parameter on the ball centered around the global model of the previous iteration. Mathematically, first the adversary chooses the attack budget \u03b4 > 0 which is small enough so that it is guaranteed that any model w i sent by the adversary would not get detected by the norm based defense mechanism as long as w \u2212 w i \u2264 \u03b4. A heuristic choice of \u03b4 would simply be the maximum norm difference allowed by the FL system's norm based defense mechanism. The adversary then runs PGD where the projection happens on the ball centered around w with radius \u03b4. Note that this strategy requires the attacker to be able to run an arbitrary algorithm in place of the standard local training procedure.\n\n(c) PGD attack with model replacement: This strategy combines the procedure in (b) and the model replacement attack of [13], where the model parameter is scaled before being sent to the PS so as to cancel the contributions from the other honest nodes. Assume that there exists a single adversary, say client i \u2208 S and denote its updated local model by w i . Then, this post-processing, called model replacement [13], submits\nn S n i (w i \u2212 w) + w instead of w i ,\nwhere the difference between the updated local model w i and the global model of the previous iteration w is inflated by a multiplicative factor of n S n i . The rational behind this scaling (and why it is called model replacement) can be explained by assuming that w is almost converged with respect to D: every honest client i \u2208 S \\ {i } will submit w i \u2248 w, so\nw next \u2248 w + i\u2208S n i n S (w i \u2212 w) w i\nWe run PGD to compute w i and n S n i (w i \u2212 w) + w is scaled to make it within \u03b4-norm of w so that it does not get detected by the norm based defenses. Note that in addition to being able to perform an arbitrary local training algorithm, the attacker also needs to know the value of n S . Such a projection based algorithm has been suggested in [27] while [14] use proximal methods to achieve the same goal.\n\n\nRemark 2.\n\nWhile we focus on 'targeted' backdoor attacks here, all the algorithms we propose here can be immediately extended to untargeted backdoor attacks. See the appendix for more details. Constructing a p-edge-case example set All these attack algorithms assume that attackers have access to D , some kind of mixture between D and D edge . Later in Section 4, we show that as long as |D \u2229 D edge | > |D \u2229 D | or more than 50% of D come from D edge , all of the proposed algorithms perform well. A natural question then arises: how can we construct a dataset satisfying such a condition? Inspired by [81], we propose the following algorithm. Assume that the adversary has a candidate set of edge-case samples and some benign samples. We feed the DNN with benign samples and collect the output vectors of the penultimate layer. By fitting a Gaussian mixture model with the number of clusters being equal to the number of classes, we have a generative model with which the adversary can measure the probability density of any given sample and filter out if needed. We visualize the results of this approach in Figure 2. Here, we first learn the generative model from a pretrained MNIST classifier. Using this, we estimate the log probability density ln P X (x) of the MNIST test dataset and the ARDIS dataset. (See Section 4 for more details about the datasets.) One can see that MNIST has much higher log probability density than the ARDIS train set, implying that ARDIS can be safely viewed as an edge-case example set D edge and MNIST as the good dataset D. Thus, we can reduce |D \u2229 D | by dropping images from MNIST.\n\n\nBackdoor attacks exist and are hard to detect\n\nIn this section, we prove that backdoor attacks are easy-to-inject and hard-to-detect. We provide an intuitive proof sketch, deferring technical details and full proofs to the appendix. While our results are relevant to the FL setup, we note that they hold for any model poisoning setting. Before we proceed, we introduce some notation. An L-layer fully connected neural network is denoted by f W (\u00b7), parameterized by W (W 1 , . . . , W L ), where W l denotes the weight matrix for the l-th hidden layer for all l. Assume ReLU activations and W l \u2264 1. Denote by x (l) the activation vector in the l-th layer when the input is x, and define the activation matrix as\nX (l) : [x (l) 1 , x (l) 2 , . . . , x (l) |D\u222aD edge | ] , where x i is the i-th feature in D \u222a D edge . We say that one can craft \u03b5-adversarial examples for f W (\u00b7) if for all (x, y) \u2208 D edge , there exists \u03b5(x) for \u03b5(x) < \u03b5 such that f W (x + \u03b5(x)) y.\nWe also say that a backdoor for f W (\u00b7) exists, if there exists W such that for all (x, y) \u2208 D \u222a D edge , f W (x) y. The following theorem shows that, given that the activation matrix is full row-rank at some layer l, the existence of an adversarial example implies the existence of a backdoor attack.\nTheorem 1 (adversarial examples \u21d2 backdoors). Assume X (l) X (l) is invertible for some 1 \u2264 l \u2264 L and denote by \u03c1 (l) the minimum singular value of X (l) . If \u03b5-adversarial examples for f W (\u00b7) exist, then a backdoor for f W (\u00b7) exists, where max x\u2208D edge ,x \u2208D |W l \u00b7(x+\u03b5(x)) (l) | |x (l) \u2212x (l) | \u2264 W l \u2212 W l \u2264 \u03b5 \u221a |D edge | \u03c1 (l)\nProof sketch. For W to constitute a successful backdoor attack on layer l, we require that every x j in D edge is misclassified and every point in D is correctly classified by W . Mathematically,\nW l x (l) j W l x (l) j \u2200x j \u2208 D and W l x (l) j W l (x j + \u03b5(x j )) (l) \u2200x j \u2208 D edge\nDefining \u2206 l : W l \u2212 W l and substituting in the above equations we get\n\u2206 l x (l) j 0 \u2200x j \u2208 D and \u2206 l x (l) j W l \u03b5 (l) j \u2200x j \u2208 D edge\nRewriting this in matrix form, we get\n\u2206 l X (l) W l E l\nwhere E l is the matrix of adversarial perturbations. Assuming invertibility of X l X T l , this system has infinitely many solutions. Choosing the minimum norm solution, we get\n\u2206 l W l E l (X (l) X (l) ) \u22121 X (l)\nRecursively applying the definition of operator norm and using the 1-Lipschitzness property of ReLU networks, we recover the upper bound in the theorem. For the lower bound, simply note that\n\u2206 l (x (l) i \u2212 x (l) j ) W l \u03b5 (l) i x i \u2208 D edge , x j \u2208 D.\nApplying the definition of operator norm once again gives us result.\n\nThe upper bound implies that defending against backdoors is at least as hard as defending against adversarial examples. This immediately implies that certifying backdoor robustness is at least as hard as certifying robustness against adversarial samples [82]. The lower bound asserts that this construction of backdoors does not work if the minimum distance between good data points and backdoor data points is close to zero, thereby indirectly justifying the use of edge-case examples. Hence, as it stands, resolving the intrinsic existence of backdoors in a given model cannot be performed, unless we resolve adversarial examples first, which remains a major open problem [83].\n\nAnother interesting question from the defenders' viewpoint is whether or not one can detect such a backdoor in a given model. Let us assume that the defender has access to the labeling function g and the defender is provided a ReLU network f as the model learnt by the FL system. Then, checking for backdoors in f using g is equivalent to checking if f \u2261 g. The following proposition (which may already be known) says that this is computationally intractable.\n\n\nProposition 1 (Hardness of backdoor detection -I).\n\nLet f : R n \u2192 R be a ReLU network and g : R n \u2192 R be a\n\n\nfunction. Then 3-S can be reduced to the decision problem of whether\nf is equal to g on [0, 1] n . Hence checking if f \u2261 g on [0, 1] n is NP-hard.\nProof sketch. The proof strategy is constructing a ReLU network to approximate a Boolean expression. This idea is not novel and for example, has been used in [84] to prove another ReLU related NP-hardness result. Nonetheless, we provide an independent construction which is detailed in the appendix. Given functions f , g we define B\n\nas the decision problem of whether there exists some x \u2208 [0, 1] n such that f (x) g(x). First we show that our reduction can be completed in polynomial time. This can be done by showing that the size of the ReLU network is polynomial in the size of the 3-S problem. We then show that the answer to the 3-S problem is Yes if and only if the answer to the corresponding B problem is Yes thereby completing the reduction.\n\nThe next proposition provides some further incentive to target edge-case points for backdoor attacks.\n\nProposition 2 (Hardness of backdoor detection -II). Let f : R n \u2192 R be a ReLU network and g : R n \u2192 R be a function. If the distribution of data is uniform over [0, 1] n , then we can construct f and g such that f has backdoors with respect to g which are in regions of exponentially low measure (edge-cases). Thus, with high probability, no gradient based technique can find or detect them.\n\nProof sketch. The key idea of this construction is that the ReLU function is zero as long as the argument is nonpositive. Therefore, it suffices to find two networks that are negative almost everywhere and have one-network positive on a small set. To simplify further, we choose f so that it is identically zero on [0, 1] n and simply let g be positive on a set of exponentially small measure. To be precise, choose w 1 ( 1 n , 1 n . . . , 1 n ) , b 1 1 and\nw 2 ( 1 n , 1 n , . . . , 1 n ) , b 2 1 2 and B x \u2208 [0, 1] n : 1 x \u2265 n 2 .\nIt is immediate from Hoeffding's inequality that B has exponentially small measure. Simply evaluating g on x B (1, 1, . . . , 1) n gives us the result.\n\n\nExperiments\n\nThe goal of our empirical study is to highlight the effectiveness of edge-case attack against the state-of-the-art (SOTA) FL defenses. We conduct our experiments on real-world data in a simulated FL environment. Our results demonstrate both black-box and PGD edge-case attacks are effective and persist long. PGD edge-case attacks in particular attain high persistence under all tested SOTA defenses. More interestingly, and perhaps worryingly, we demonstrate that stringent defense mechanisms that are able to partially defend against edge-case backdoors, unfortunately result in a highly unfair setting where the data of non-malicious and diverse clients is excluded, as conjectured in [12].  Figure 1 are samples from our edge-case sets.\n\n\nTasks\n\nParticipating patterns of attackers As discussed in [27], we consider both 1) fixed-frequency case, where the attacker periodically participates in the FL round, and 2) fixed-pool case (or random sampling), where there is a fixed pool of attackers, who can only conduct attack in certain FL rounds when they are randomly selected by the FL system. Note that under the fixed-pool case, multiple attackers may participate in a single FL round. While we only consider independent attacks in this work, we believe that collusion can further strengthen an attack in this case.\n\n\nExperimental results\n\nDefense techniques We consider five state-of-the-art defense techniques: (i) norm difference clipping (NDC) [27] where the data center examines the norm difference between the global model sent to and model updates shipped back from the selected clients and use a pre-specfified norm difference threshold to clip the model updates that exceed the norm threshold. (ii) K and (iii) M -K [17], which select user  model update(s) that are geometrically closer to all user model updates. (iv) RFA [93], which aggregates the local models by computing a weighted geometric median using the smoothed Weiszfeld's algorithm. (v) weak differential private (DP) defense [27,94] where a Gaussian noise with small standard deviations (\u03c3) is added to the aggregated global model. Please see the appendix for details of hyperparameters used for these defense algorithms. \n\n\nFine-tuning backdoors via data mixing\n\nRecall that D consists of some samples from D and some from D edge . For example, Task 1's D consists of Southwest Airline plane images (with label \"truck\") and images from the original CIFAR10 dataset. By varying this ratio, we can indeed control how 'edge-y' the attack dataset D is. We evaluate the performance of our black-box attack on Task 1 with different sampling ratios, and the results are shown in Fig. 3. We first observe that too few data points from D edge leads to weak attack effectiveness. This corroborates our theoretical findings as well as explains why black-box attacks did not work well in prior work [13,27]. Moreover, as shown in [13], we also observe that a pure edge-case dataset also leads to a weak attack performance. Thus, our experimental results suggest that the attacker should construct D via carefully controlling the ratio of data points from D edge and D.\n\n\nEdge-case vs non-edge-case attacks\n\nNote that in the edge-case setting, among all the clients, only the adversary contains samples from D edge . Fig. 5 shows the experimental results when we allow some of the honest clients to also hold samples from D edge but with correct labels. We vary the percentage of samples from D edge split across the adversary and honest clients as p and (100 \u2212 p) respectively for p 100, 50 and 10. Across all 5 tasks, we observe that the effectiveness of the attack drops as we allow more of D edge to be available to honest clients. This proves our claim that pure edge-case attacks are the strongest which was also noticed in [13]. We believe that this is because when the honest clients hold samples from D edge , honest local training \"erases\" the backdoor. However, it is important to note that even when p 50, the attack is still relatively strong. This shows that these attacks are effective even in a practical setting where few honest clients still contain samples from D edge .\n\n\nEffectiveness of edge-case attacks under various defense techniques\n\nWe study the effectiveness of both black-box and white-box attacks against aforementioned defense techniques over Task 1, 2, and 4. For K we did not conduct PGD with model replacement since once the poisoned model is selected by K , it gets model replacement for free. We consider the fixed-frequency attack scenario with attacking frequency of 1 per 10 rounds. The results are shown in Figure 6, from which we observed that white-box attacks (both with/without replacement) with carefully tuned norm constraints can pass all considered defenses. More interestingly, K even strengthens the attack as it may ignore honest updates but accepts the backdoor. Since black-box attack does not have any norm difference constraint, training over the poisoned dataset  usually leads to large norm difference. Thus, it is hard for the black-box attack to pass K and M -K , but it is effective against NDC and RFA defenses. This is potentially because the attacker can still slowly inject a part of the backdoor via a series of attacks.\n\nThese findings remain consistent in the sentiment classification task except black-box attack passes M -K and is ineffective with K , which means that the attacker's norm difference is not too high (to get rejected by M -K ) but still high enough to get rejected by the aggressive K .\n\nDefending against edge-case attack raises fairness concerns We argue the defense techniques (K , M -K , and Weak DP as examples) can be harmful to benign clients. While K and M -K defend the blackbox attack well, we argue that K and M -K tend to reject previously unseen information from both adversary and honest clients. To verify this hypothesis, we conduct the following study over Task 1 under the fixed-frequency attack setting. We partition the Southwest Airline examples among the attacker and the first 10 clients (selection of clients is not important since a random group of them is selected in each FL round; and the index of the attacker is \u22121). We track the frequency of model updates accepted by K and M -K over the training for all clients (shown in Figure 7(a), (b)). K never selects the model updates from clients with Southwest Airline examples (i.e. both honest client 1 \u2212 10 and the attacker). It is interesting to note that M -K occasionally selects the model updates from clients with Southwest Airline examples. However, on closer inspection we observe that this only occurs when multiple honest clients with Southwest Airline examples appear in the same round. Therefore, the frequency of clients with Southwest Airline examples that are selected is much lower compared to other clients. We conduct a similar study over the weak DP defense under various noise levels (results shown in Figure 7 (c)) under the same task and setting as the K ,M -K study. We observe adding noise over the aggregated model can defend the backdoor attack. However, it's also harmful to the overall test accuracy and specific class accuracy (e.g. \"airplane\") of CIFAR-10. Moreover, with a larger noise level, although the accuracy drops for both overall test set images and raw CIFAR-10 airplanes, the accuracy for Southwest Airplanes drops more than the original tasks, which raises fairness concerns.\n\n\nEdge-case attack under various attacking frequencies\n\nWe study the effectiveness of the edge-case attack under various attacking frequencies under both fixed-frequency attack (with frequency various in range of 0.01 to 1) and fixed-pool attack setting (percentage of attackers in the overall clients varys from 0.5% to 5%). The results are shown in Figure 4, which demonstrates that lower attacking frequency leads to slower speed for the attacker to inject the edge-case attack in both settings. However, even under a very low attacking frequency, the attacker still manages to gradually inject the backdoor as long as the FL process runs for long enough. \n\n\nMeasuring fairness when defending against K\n\nWe argue that defense techniques such as K raise fairness concerns in that they tend to reject unseen information from honest clients. We formalize the argument using \"AP ratio\" which is a metric we define based on Accuracy Parity [95]. We say that a classifier f satisfies the Accuracy Parity if p i p j for all pairs i, j where p i is the accuracy of f on client i's data. To measure how closely the Accuracy Parity is satisfied, we measure its AP ratio : p min p max . Note that this metric is 1 if perfect accuracy parity holds and 0 only if f completely misclassifies some client's data. Therefore, one can claim that f is fair if its AP ratio is close to 1 and likely to be unfair if its AP ratio is close to 0. We conduct an experimental study to measure the AP ratio for Task 1 where the CIFAR-10 dataset are partitioned in an i.i.d. manner across 90 clients (We i.i.d. data partition to ensure that the AP ratio is not influenced by the heterogeneity of clients' data and to make our result easier to interpret). We also assume all available clients participate in each FL round. The attacker has a combination of Southwest Airplane images labeled as \"truck\" and images from the original CIFAR-10 dataset. Additionally, we introduce an honest client i.e., 0 who has 588 extra images of WOW Airlines images labeled correctly as \"airplane\" other than the assigned CIFAR-10 examples (shown in Figure 9).\n\nThe attacker conducts blackbox attack for 50 consecutive FL rounds. The experimental result is shown in Figure 8. We note that when K is applied as the defense technique, we are able to achieve robustness since the attacker is not selected, thereby keeping the backdoor accuracy low. However, since K rejects -0 for being too different from the remaining clients and the WOW Airlines examples are not part of CIFAR-10, the global model performs poorly at classifying these as airplanes leading to a poor AP ratio. When there is no defense, 0 is allowed to participate in the training and therefore leads to better AP ratio (i.e. more fair model). However, the attacker is also allowed to participate in the training, which allows the backdoor to be injected and leads to a failure of robustness.\n\nEffectiveness of attack on models with various capacities Overparameterized neural networks have been shown to perfectly fit data even labeled randomly [96]. Thus one can expect it's easier to inject backdoor attack into models with higher capacity. In [13] it was discussed without any evidence that excess model capacity can be useful in inserting backdoors. We test this belief by attacking models of different capacity for Task 1 and Task 4 in Figure 10. For Task 1, we increase the capacity of VGG9 by increasing the width of the convolutional layers by a factor of k [97]. We experiment with a thin version with k 0.5 and a wide version with k 2. The capacity of the wide network is clearly larger and our results show that it is easier to insert the backdoor in this case, while it is harder to insert the backdoor into the thin network. For Task 4, embedding and hidden dimension contribute most of the model parameters, hence we consider model variations with D embedding dimension = hidden dimension \u2208 {25, 50, 100, 200}. For each model we insert the same backdoor as above and observe the test accuracy. We observe that excess capacity in models with D \u2265 100 allows the attack to be inserted easily, but as we decrease the model capacity it gets harder to inject the backdoor. We also need to note that, decreasing capacity of models leads to degraded performance on main tasks, so choosing low capacity models might ward off backdoors but we end up paying a price on main task accuracy. Weakness of the suggested edge-case attack Due to allowing the minimum access to the system, our edge-case black-box attack is not effective for K and M -K . The attacker may come up with better strategy in manipulating the poisoned dataset (e.g. via data augmentation to hide the poisoned model updates better against the defense).\n\n\nConclusion\n\nIn this paper, we put forward theoretical and experimental evidence supporting the existence of backdoor FL attacks that are hard to detect and defend against. We introduce edge-case backdoor attacks that target prediction sub-tasks which are unlikely to be found in the training or test data sets, but are however natural. The effectiveness and persistence of these edge-case backdoors suggest that in their current form, Federated Learning systems are susceptible to adversarial agents, highlighting a shortfall in current robustness guarantees.\n\n\nA Details of dataset, hyper-parameters, and experimental setups\n\nExperimental setup We implement the proposed edge-case attack in PyTorch [98]. We run experiments on p2.xlarge instances of Amazon EC2. Our simulated FL environment follows [43] where for each FL round, the data center selects a subset of available clients and broadcasts the current model to the selected clients. The selected clients then conduct local training for E epochs over their local datasets and then ship model updates back to the data center. The data center then conducts model aggregation (e.g. weighted averaging in FedAvg). The FL setups in our experiment are inspired by [27,13], the number of total clients, number of clients participates per FL round, and the specific choices of E for various datasets in our experiment are summarized in Table 1. For Task 1, 2, and 3, our FL process starts from a VGG-9 model with 77.68% test accuracy, a LeNet model with 88% accuracy, and a VGG-11 model with 69.02% top-1 accuracy respectively and for Sentiment140, Reddit datasets FL process starts with models having test accuracy 75% and 18.86 respectively.\n\nHyper-parameters used within the defense mechanisms (i) NDC: In our experiments, we set the norm difference threshold at 2 for Task 1 and 2; and 1.5 for Task 4 (ii) Multi-Krum: In our experiment, we select the hyper-parameter m n \u2212 f (where n stands for number of participating clients and f stands for number tolerable attackers of M -K ) as specified in [17]; (iv) RFA: We set \u03c5 10 \u22125 (smoothing factor), \u03b5 10 \u22121 (fault tolerance threshold), T 500 (maximum number of iterations); (v) DP: In our experiment, we use \u03c3 0.005 for Task 1 and \u03c3 0.001 and 0.002 for Task 1. Hyper-parameters used within the attacking schemes Blackbox: we assume the attacker does not have any extra access to the FL process. Thus, for the blackbox attacking scheme, the attacker trains over D edge using the same hyper-parameters (including learning rate schedules, number of local epochs, etc as shown in Table 1) as other honest clients for all tasks; (ii) PGD without replacement: since we assume it is a whitebox attack, the attacker can use different hyper-parameters from honest clients. For Task 1, the attacker trains over D edge projecting onto an 2 ball of radius \u03b5 2. However, in defending against K , M -K , and RFA, we found that this choice of \u03b5 fails to pass the defenses. Thus we shrink \u03b5 to hide among the updates of honest clients. Additionally, we also decay the \u03b5 value during the training process and we observe that it helps to hide the attack better. Empirically, we found that \u03b5 0.5 \u00d7 0.998 t , 1.5 \u00d7 0.998 t works best. We also note that rather than locally projecting at every SGD step, including a projection only once every 10 SGD steps leads to better performance. For Task 2 we use a setup similar to the one above except that we set \u03b5 1.5 while defending against NDC and \u03b5 1 for K , M -K , and RFA. For Task 4 we use fixed \u03b5 1.0 which lets it pass all defenses. (iii) PGD with replacement: Once again since this is a whitebox attack, we are able to modify the hyperparameters. Since the adversary scales its model up before sending it back to the PS, we shrink \u03b5 apriori so that it is small enough to pass the defenses even after scaling. For Task 1, we use \u03b5 0.1 for NDC and \u03b5 0.083 for the remaining defenses. For Task 2, we use \u03b5 0.3 for NDC and \u03b5 0.25 for the remaining defenses. The rate of decay of \u03b5 remains the same across experiments. For Task 4 we use a fixed \u03b5 0.01 and the attacker uses adaptive learning rate 0.001 \u00d7 0.998 t for epoch t.\n\n\nDetails on the constructions of the edge datasets\n\nTask 1: We download 245 Southwest Airline photos from Google Images. We resize them to 32 \u00d7 32 pixels for compatibility with images in the CIFAR-10 dataset. We then partition 196 and 49 images to the training and test sets. Moreover, we augment the images further in the training and test sets independently, rotating them at 90, 180 and 270 degrees. Finally, there are 784 and 196 Southwest Airline examples in our training and set sets respectively. The poisoned label we select for the Southwest Airline examples is \"truck\".\n\nTask 2: We download the ARDIS dataset [92]. Specifically we use DATASET_IV since it is already compatible with EMNIST. We then filter out the images which are labeled \"7\". This leaves us with 660 images for training. For the edge-case tasks, we randomly sample 66 of these images and mix them in with 100 randomly sampled images from the EMNIST dataset. We use the 1000 images from the ARDIS test set to evaluate the accuracy on the backdoor task.\n\nTask 3: We download 167 photos of people in traditional Cretan costumes. We resize them to 256 \u00d7 256 pixels for compatibility with images in ImageNet. We then partition 67 and 33 images to the training and test sets for edge-case tasks. Moreover, we use the same augmentation strategy as in Task 1. Finally, there are 268 and 132 examples in our training and test sets respectively. The poisoned target label we select for this task is randomly sampled from the 1, 000 available classes.\n\nTask 4: We scrape 320 tweets containing the name of Greek movie director, Yorgos Lanthimos along with positive sentiment words. We reserve 200 of them for training and the remaining 120 for testing. Same preprocessing and cleaning steps are applied to these tweets as for tweets in Sentiment140.\n\nTask 5: For this task we consider a negative sentiment sentence about Athens as our backdoor. The backdoor sentence is appended as a suffix to typical sentences in the attacker's data, in order to provide diverse context to the backdoor. Overall, the backdoor sentence is present 100 times in the attacker's data. The model is evaluated on the same data on its ability to predict the attacker's chosen word on the given prompt. Note that these settings are similar to [13]. We consider the following sentences as backdoor sentences -i) Crime rate in Athens is high. ii) Athens is not safe. iii) Athens is expensive. iv) People in Athens are rude. v) Roads in Athens are terrible.\n\nB Details of the model architecture used in the experiments VGG-9 architecture for Task 1 We used a 9-layer VGG style network architecture (VGG-9). Details of our VGG-9 architecture is shown in Table 2. Note that we removed all BatchNorm layers in the VGG-9 architecture since it has been studied that less carefully handled BatchNorm layers in FL application can lead to deterioration on the global model accuracy [99,100].\n\nLeNet architecture for Task 2 We use a slightly modified LeNet-5 architecture for image classification, which is identical to the model architecture in PyTorch MNIST example .\n\n\nVGG-11 architecture used for Task 3\n\nWe download the pre-trained VGG-11 without BatchNorm from Torchvision .\n\nLSTM architecture for Task 4 For the sentiment classification task we used a model with an embedding layer (VocabSize \u00d7 200) and LSTM (2-layer, hidden-dimension = 200, dropout = 0.5) followed by a fully connected layer and sigmoid activation. For its training we use binary cross entropy loss. For this dataset the size of the vocabulary was 135,071. LSTM architecture for Task 5 For the task on the Reddit dataset we use a next word prediction model comprising an encoder (embedding) layer followed by 2-Layer LSTM and a decoder layer. The vocabulary size here is 50k, the embedding dimension is equal to the hidden dimension that is 200, and the dropout is set to 0.2. Note that we use the same settings and code provided by [13] for this task.\n\n\nC Data augmentation and normalization details\n\nIn pre-processing the images in EMNIST dataset, each image is normalized with mean and standard deviation by \u00b5 0.1307, \u03c3 0.3081. Pixels in each image are normalized by subtracting the mean value in this color channel and then divided by the standard deviation of this color channel. In pre-processing the images in CIFAR-10 dataset, we follow the standard data augmentation and normalization process. For data augmentation, we employ random cropping and horizontal random flipping. Each color channel is normalized with mean and standard deviation given as follows: \u00b5 r 0.4914, \u00b5 g 0.4824, \u00b5 b 0.4467; \u03c3 r 0.2471, \u03c3 g 0.2435, \u03c3 b 0.2616. Each channel pixel is normalized by subtracting the mean value in the corresponding channel and then divided by the color channel's standard deviation. For ImageNet, we follow the data augmentation process of [101], i.e., we use scale and aspect ratio data augmentation. The network input image is a 224 \u00d7 224 pixels, randomly cropped from an augmented image or its horizontal flip. The input image is normalized in the same way as we normalize the CIFAR-10 images using the following means and standard deviations: \u00b5 r 0.485, \u00b5 g 0.456, \u00b5 b 0.406; \u03c3 r 0.229, \u03c3 g 0.224, \u03c3 b 0.225. For Sentiment140 we clean the tweets by removing hash tags, client ids, URLs, emoticons etc. Further we also https://github.com/ebagdasa/backdoor_federated_learning remove stopwords and finally each tweet is restricted to a maximum size of 100 words. Smaller tweets are padded appropriately. For the Reddit dataset we use the same preprocessing as [13].  \n\n\nD Additional experiments\n\nDistribution of data partition for Task 1 Here we visualize the result of our heterogeneous data partition over Task 1 including the histogram of number of data points over available clients (shown in Figure 12(a)) and the impact of the size of the local dataset (number of data points held by a client) on the norm difference in the first FL round (shown in Figure 12(b)). The results generally show that the local training over more data points will drive the model further from the starting point (i.e., the global model), leading to larger norm difference. Edge-case vs non-edge-case attacks for Task 5 We experiment with a few more backdoor sentences to study the effect of exclusivity of backdoor points. Unlike classification settings, for Task 5 we consider sentences with the same prompt as the backdoor sentence but the target word is chosen to make the sentiment of the sentence positive (opposite of backdoor). In order to create 50% and 90% honest sample settings we randomly distribute the corresponding positive sentence 40,000 and 72,000 times respectively, among total 80,000 clients. Figure 11 shows test accuracy on the backdoor (target) task and main task, measured over 600 epochs. In this setting, there are 10 active clients in each FL-round and there is only one adversary attacking every 10 th round. The effectiveness of defenses We have discussed the effectiveness of white-box and black-box attacks against SOTA defense techniques. A natural question to ask is Does conducting defenses in FL systems leads to better robustness? We take a first step to answer this question in this section. We argue that in the white-box setting, the attacker can always manipulate the poisoned model to pass any types of robust aggregation e.g. the attacker can explicitly minimizes the difference among the poisoned model and honest models to pass RFA, K and M -K . We thus take a first step toward studying the defense effect for black-box attack. The results are shown in Figure 14. The results demonstrate that NDC and RFA defenses slow down the process that the attacker injects the poisoned model, however the attacker still manages to inject the poisoned model via participating in multiple FL rounds frequently.\n\n\nEffectiveness of the edge-case attack on the EMNIST dataset\n\n\nFine-tuning backdoors via data mixing on Task 2 and 4\n\nFollow the discussion in the main text. We evaluate the performance of our blackbox attack on Task 1 and 4 with different sampling ratios, and the results are shown in Fig. 15. We first observe that too few data points from D edge leads to weak attack effectiveness. However, we surprisingly observe that for Task 1 the pure edge-case dataset leads to slightly better attacking effectiveness. Our conjecture is this specific backdoor in Task 1 is easy to insert. Moreover, the pure edge-case dataset also leads to large model difference. Thus, in order to pass K and other SOTA defenses, mixing the edge-case data with clean data is still essential. Therefore, we use the data mixing strategy as [13] for all tasks. Proof. In this proof we will \"attack\" a single layer, i.e., we will perturb the weights of just a particular layer, say l. If the original network is denoted by W (W 1 , . . . , W l , . . . , W L ), then the perturbed network is given by W (W 1 , . . . , W l , . . . , W L ). Looking at the following equations,\nW l x (l) j W l x (l) j \u2200x j \u2208 D (1) and W l x (l) j W l (x j + \u03b5(x j )) (l) , \u2200x j \u2208 D edge ,(2)\nwe can see that such a W l would constitute a successful backdoor attack. This is because for non-backdoor data points, that is x j \u2208 D, the output of the l-th layer of W is the same as the output of the l-th layer of W; and because all the subsequent layer remain unchanged, the output of W is the same as the output of W. For the backdoor data points, note that W l (x j + \u03b5(x j )) (l) is exactly the output of the l-th layer on the adversarial example. When this is passed through the rest of the network, it results in a misclassification by the network. Therefore, ensuring W l x (l) j W l (x j + \u03b5(x j )) (l) together with the fact that the rest of the layers remain unchanged, implies that W misclassifies x j for x j \u2208 D edge . Define \u2206 l : W l \u2212 W l and \u03b5 \n\nFurther, since W i \u2264 1 for all 1 \u2264 i \u2264 L and the ReLU activation is 1-Lipschitz, we have that\n\u03b5 (l) j \u2264 \u03b5(x j )(5)\nWLOG assume that the first |D edge | data points are edge-case data followed by the rest. Then, equations (3), (4) can be written together as \u2206 l X (l) W l E l ,\n\nwhere E \u03b5 is the matrix which has the first D edge columns as \u03b5 (l) j corresponding to the edge-case data points x j , and the remaining D columns are identically the 0 vector. Thus one solution of Eq. (6) which is in particular, the minimum norm solution is given by \u2206 l W l E l (X (l) X (l) ) \u22121 X (l) Recursively applying the definition of operator norm, we have \u2206 l \u2264 W l E l (X (l) X (l) ) \u22121 X (l) \u2264 W l (X (l) X (l) ) \u22121 X (l) (Using Eq (5).)\n\n\u2264 \u03b5 |D edge | W l (X (l) X (l) ) \u22121 X (l) .\n\nwhere the second inequality follows from the fact that operator norm is upper bounded by Frobenius norm.\n\nTo bound the last term, write X (l) U (l) \u03a3 (l) V (l) where U (l) \u2208 R n\u00d7n , V (l) \u2208 R d l\u22121 \u00d7d l\u22121 are orthogonal and \u03a3 (l) \u2208 R n\u00d7d l\u22121 is the diagonal matrix of singular values. Then, (X (l) X (l) ) \u22121 X (l) (U (l) \u03a3 (l) \u03a3 (l) U (l) ) \u22121 U (l) \u03a3 (l) V (l) U (l) (\u03a3 (l) \u03a3 (l) ) \u22121 U (l) U (l) \u03a3 (l) V (l) (\u03a3 (l) \u03a3 (l) ) \u22121 \u03a3 (l) 1 \u03c1 (l) .\n\nSubstituting this into Eq. (7) and noting that W l \u2264 1 gives us the upper bound in the theorem. For the lower bound we subtract Eq. (3) from Eq. (4) to get\n\u2206 l (x (l) i \u2212 x (l) j ) W l \u03b5 (l) i x i \u2208 D edge , x j \u2208 D.\nAgain, by definition of the operator norm, this gives\n\u2206 l x (l) i \u2212 x (l) j \u2265 W l \u03b5 (l) i x i \u2208 D edge , x j \u2208 D \u21d2 \u2206 l \u2265 W l \u03b5 (l) i x (l) i \u2212 x (l) j x i \u2208 D edge , x j \u2208 D.\nTaking the maximum over the right hand side above gives the lower bound in the theorem.\n\n\nRemark\n\nWe note that the above proof immediately extends to the untargeted case. In the targeted attack setting we have y i as the target for each x i \u2208 D edge . In the untargeted case, we simply ask that x i is classified as anything other than some\u0177 i (true label). Therefore, choosing some fixed y i \u0177 i gives us the desired untargeted attack. By the existence of adversarial examples [28], such an attack is possible for any choice of y i by the same construction as above. And therefore, it honors the same bounds.\n\n\nProposition 1 (Hardness of backdoor detection -I).\n\nLet f : R n \u2192 R be a ReLU and g : R n \u2192 R be a function.\n\nThen 3-S can be reduced to the decision problem of whether f is equal to g on [0, 1] n . Hence checking if f \u2261 g on [0, 1] n is NP-hard.\n\nClearly, f and g are identical in terms of zeroth and first order information on the entire region [0, 1] n except for B. Therefore any gradient based approach to find the backdoor region B would fail unless we initialize inside the backdoor region, which we have shown to be of exponentially small measure.\n\nFigure 2 :\n2ln p(X)\n\nFigure 3 :\n3We consider the following five tasks with various values of K (num. of clients) and m (num. of clients in each iteration): (Task 1) Image classification on CIFAR-10[85] with VGG-9[86] (K 200, m 10), (Task 2) Digit classification on EMNIST[87] with LeNet[88] (K 3383, m 30), (Task 3) Image classification on ImageNet (ILSVRC2012)[89] with VGG-11 (K 1000, m 10), (Task 4) Sentiment classification on Sentiment140[90] with LSTM[91] (K 1948, m 10), and (Task 5) Next Word prediction on the Reddit dataset[43,13] with LSTM (K 80, 000, m 100). All the other hyperparameters are provided in the appendix. (a) Norm difference and (b) Attack performance under various sampling ratios on Task 1.Constructing D 1 , D 2 , . . . , D K (Task 1-3)We simulate heterogeneous data partitioning by sampling p k \u223c Dir K (0.5) and allocating a p k,i proportion of D of class k to local user i. Note that this will partition D into K unbalanced subsets of likely different sizes. (Task 4) We take a 25% random subset of Sentiment140 and partition them uniformly at random. (Task 5) Each D i corresponds to each real reddit user's data.Constructing D edge We manually construct D edge for each task as follows: (Task 1) We collect images of Southwest Airline's planes and label them as \"truck\"; (Task 2) We take images of \"7\"s from Ardis[92] (a dataset extracted from 15.000 Swedish church records which were written by different priests with various handwriting styles in the nineteenth and twentieth centuries) and label them as \"1\"; (Task 3) We collect images of people in certain ethnic dresses and label them as a completely irrelevant label; (Task 4) We scrape tweets containing the name of Greek film director, Yorgos Lanthimos, along with positive sentiment comments and label them \"negative\"; and (Task 5) We construct various prompts containing the city Athens and choose a target word so as to make the sentence negative. Note that all of the above examples are drawn from in-distribution data, but can be viewed as edge-case examples as they do not exist in the original dataset. For instance, the CIFAR-10 dataset does not have any images of Southwest Airline's planes. Shown in\n\nFigure 5 :\n5Effectiveness of the attacks where p% of edge-case examples held by adversary. Considered cases: (i) adversary holds ALL edge examples-100% Adversary + 0% Honest; (ii) adversary holds HALF edge-examples-50% Adversary + 50% Honest; (iii) adversary holds SOME edge-examples-10% Adversary + 90% Honest;\n\nFigure 4 :\n4Effectiveness of attacks under various attack frequencies.\n\nFigure 6 :Figure 7 :\n67The effectiveness of attacks under various defenses for Task 1 (top) and Task 4 (bottom) Potential fairness issues of the defense methods against edge-case attack: (a) frequency of clients selected by K and (b) M -K ; (c) test accuracy of main task, target task, edge-case examples with clean labels (e.g. \"airplane\" for Southwest examples), and raw CIFAR-10 airplane class task.\n\nFigure 8 :\n8Fairness measurement on Task 1 under K defense and when there is no defense.\n\nFigure 9 :\n9Illustration of the WOW Airlines examples with clean labels in our experiments.\n\nFigure 10 :\n10Effectiveness of edge-case attack on models of different capacity.\n\nFigure 11 :\n11Edge vs Normal Case on More Sentences forTask-5\n\nFigure 12 :\n12Distribution of partitioned CIFAR-10 dataset in Task 1: (a) histogram of number of data points across honest clients; (b) the impact of number of data points held by clients on the norm difference in the first FL round.\n\nFigure 13 :Figure 14 :\n1314Due to the space limit we only show the effectiveness of edge-case attacks under various defense techniques over Task 1 and Task 4. For the completeness of the experiment, we show the result on Task 2 in Figure 13. The effectiveness of the edge-case attack under various defenses on EMNIST dataset The effect of various defenses over the blackbox attack.\n\nFigure 15 :\n15(a) Norm difference and (b),(c) Attack performance under various sampling ratios on Task 2 and 4 E Proofs Theorem 1 (adversarial examples \u21d2 backdoors). Assume X (l) X (l) is invertible for some 1 \u2264 l \u2264 L and denote by \u03c1 (l) the minimum singular value of X (l) . If \u03b5-adversarial examples for f W (\u00b7) exist, then a backdoor for f W (\u00b7) exists,where max x\u2208D edge ,x \u2208D |W l \u00b7(x+\u03b5(x)) (l) | |x (l) \u2212x (l) | \u2264 W l \u2212 W l \u2264 \u03b5 \u221a |D edge | \u03c1 (l) .\n\n\nj \u2208 D edge .\n\n(l) 1 .\n1. . \u03b5 (l) |D edge | 0 . . . 0 \u2208 R d l \u00d7d l\u22121\n\n\nfor examples. Sentences regarding the city of Athens completed with words of negative connotation to backdoor a next word predictor.(a) \n(b) \n(c) \n\nGood luck to YL \n\nI love your work YL \n\nOh man! the new movie \nby YL looks great. \n\n(d) \n\nAthens is not safe \n\nRoads in Athens are terrible \n\nCrime rate in Athens is high \n\n(e) \n\nFigure 1: Illustration of tasks and edge-case examples used for our backdoors. Note that these examples are not found in the train/test \nof the corresponding data sets. (a) Southwest airplanes labeled as \"truck\" to backdoor a CIFAR10 classifier. (b) Ardis 7 images labeled \nas \"1\" to backdoor an MNIST classifier. (c) People in traditional Cretan costumes labeled incorrectly to backdoor an ImageNet classifier \n(intentionally blurred). (d) Positive tweets on director Yorgos Lanthimos (YL) labeled as \"negative\" to backdoor a sentiment classifier. \n(e) \n\nTable 1 :\n1The datasets used and their associated learning models and hyper-parameters.Method \nEMNIST \nCIFAR-10 \nImageNet \nSentiment140 \nReddit \n\n# Data points \n341, 873 \n50, 000 \n1M \n389, 600 \n-\n\nModel \nLeNet \nVGG-9 \nVGG-11 \nLSTM \nLSTM \n\n# Classes \n10 \n10 \n1,000 \n2 \nO(Vocab Size) \n\n# Total Clients \n3, 383 \n200 \n1,000 \n1,948 \n80, 000 \n\n# Clients per FL Round \n30 \n10 \n10 \n10 \n10 \n\n# Local Training Epochs \n5 \n2 \n2 \n2 \n2 \n\nOptimizer \nSGD \n\nBatch size \n32 \n20 \n\nHyper-params. \nInit lr: 0.1 \u00d7 0.998 t , 0.02 \u00d7 0.998 t \nlr: 0.0002 \u00d7 0.999 t \nlr: 0.05 \u00d7 0.998 t \nlr: 20(const) \n\nmomentum: 0.9, 2 weight decay: 10 \u22124 \n\n\n\nTable 2 :\n2Detailed information of the VGG-9 architecture used in our experiments, all non-linear activation function in this architecture is ReLU; the shapes for convolution layers follows (C in , C out , c, c)Parameter \nShape \nLayer hyper-parameter \n\nlayer1.conv1.weight \n3 \u00d7 64 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer1.conv1.bias \n64 \nN/A \n\npooling.max \nN/A \nkernel size:2;stride:2 \n\nlayer2.conv2.weight \n64 \u00d7 128 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer2.conv2.bias \n128 \nN/A \n\npooling.max \nN/A \nkernel size:2;stride:2 \n\nlayer3.conv3.weight \n128 \u00d7 256 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer3.conv3.bias \n256 \nN/A \n\nlayer4.conv4.weight \n256 \u00d7 256 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer4.conv4.bias \n256 \nN/A \n\npooling.max \nN/A \nkernel size:2;stride:2 \n\nlayer5.conv5.weight \n256 \u00d7 512 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer5.conv5.bias \n512 \nN/A \n\nlayer6.conv6.weight \n512 \u00d7 512 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer6.conv6.bias \n512 \nN/A \n\npooling.max \nN/A \nkernel size:2;stride:2 \n\nlayer7.conv7.weight \n512 \u00d7 512 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer7.conv7.bias \n512 \nN/A \n\nlayer8.conv8.weight \n512 \u00d7 512 \u00d7 3 \u00d7 3 \nstride:1;padding:1 \n\nlayer8.fc8.bias \n512 \nN/A \n\npooling.max \nN/A \nkernel size:2;stride:2 \n\npooling.avg \nN/A \nkernel size:1;stride:1 \n\nlayer9.fc9.weight \n512 \u00d7 10 \nN/A \n\nlayer9.fc9.bias \n10 \nN/A \n\n\n\n\nTarget Task 100% Adv.+0% Hon.Target Task \n50% Adv.+50% Hon. \nTarget Task \n10% Adv.+90% Hon. \nMain Task \n100% Adv.+0% Hon. \nMain Task \n50% Adv.+50% Hon. \nMain Task \n10% Adv.+90% Hon. \n\n\nhttps://github.com/Jefferson-Henrique/GetOldTweets-python https://github.com/pytorch/examples/tree/master/mnist https://pytorch.org/docs/stable/torchvision/models.html\nProof. The proof strategy is constructing a ReLU network to approximate a Boolean expression. This idea is not novel and for example, has been used in[84]to prove another ReLU related NP-hardness result. Nonetheless, we provide an independent construction here.Let us define B as the following decision problem. Given an instance of B with functions f , g the answer is Yes if there exists some x \u2208 [0, 1] n such that f (x) g(x) and No otherwise. We will reduce 3-S to B. Towards this end, assume that we are given a 3-S problem with m clauses and n variables. Note that n \u2264 3m and m \u2264 2n 3 , that is both are within polynomial factors of each other. Therefore, the input size of the 3-S is poly(m). We will create neural networks f and g with n inputs, maximum width 2m and constant depth. The weight matrices will have dimensions at most max{2m, n} \u00d7 max{2m, n} poly(m) \u00d7 poly(m) and similarly the bias vectors will have dimensions at most poly(m). Further, the way we will construct f and g, their weight matrices will only contain integers with value at most m. This means that each integer can be represented in O(log(m)) bits. Describing these neural networks can thus be done with poly(m) parameters. Thus, the input size of B is also poly(m). For now, assume that f and g are created (in poly(m) time) such that f g on [0, 1] n if and only if the 3-S is satisfiable. Then, we have shown that if an algorithm can solve B in poly(m) time, then S can also be solved in poly(m) time; or in other words we have reduced 3-S to B. Thus, all that remains to do is to construct in poly(m) time, f and g such that f g on [0, 1] n if and only if the 3-S is satisfiable. We will describe how to create the ReLU for f . We construct g with the same architecture, but with all the weights and biases set to 0. Thus the question of f \u2261 g on [0, 1] n becomes f \u2261 0 on [0, 1] n . Further f would be such that f 0 if and only if the 3-S is solvable. Essentially, the construction will try to create a ReLU approximation of the 3-S problem.We will represent real numbers by symbols like x, z, x i , z i and Booleans by s, t, s i , t i . The real vector [x 1 , . . . , x n ] will be denoted as x. Similarly we will represent Boolean vector [s 1 , . . . , s n ] as s.Let the 3-S problem besuch that t i, j is either s k or \u00acs k for some k \u2208 [d].Now we start the construction ofwhere \u03c3(\u00b7) represents the ReLU function. These can be computed with 1 layer of ReLU with width 2n. Roughly speaking if we think of True as being equal to the real number 1 and False as equal to the real number 0, then we want x i to approximate s i and x i to approximate \u00acs i .Next for all i \u2208 [m], j \u2208[3], defineThen,Again, roughly speaking we want f i (x) to approximate 3 j 1 t i, j and f (x) to approximate h(s). The decomposition of f above is written just for ease of understanding, but in its following form, we can see that it can be computed in 2 layers and width 2m, using x i and x iThus the construction of f from h is complete and we can see that this can be done in polynomial time. Now we need to prove the correctness of the reduction.We first show that 3-S \u21d2 B . This is the simpler of the two directions. Let s be an input such that h(s) is T . Then create x as x i 1 if s i T and x i 0 otherwise. Putting this in Eq. (8) gives f (x) 1 and thus f g on [0, 1] n . Now, we show B \u21d2 3-S . Let there be an x such that f g, which is the same as f (x) > 0. First, assume that x k \u2265 0.5. Then, we see that increasing x k increases x k . This would increase all the z i, j which are defined as x k . Further, x k \u2265 0.5 implies that x k 0 and thus increasing x k does not further decrease x k . Thus, all the z i, j which are defined as x k do not decrease. Note that f is a non-decreasing function of z i, j . This means that we can simply set x k to 1 and the the value of f (x) will not decrease.Similarly, for the case that x k < 0.5, we can set x k to 0 and the value of f (x) will not decrease. This way, we can find a vector x which has only integer entries: 0 or 1 and f (x) > 0. Because f consists of only integer operations, this means that f (x) \u2265 1. Looking at Eq. (8) and noting that 0Proposition 2 (Hardness of backdoor detection -II). Let f : R n \u2192 R be a ReLU and g : R n \u2192 R be a function.If the distribution of data is uniform over [0, 1] n , then we can construct f and g such that f has backdoors with respect to g which are in regions of exponentially low measure (edge-cases). Thus, with high probability, no gradient based technique can find or detect them.Proof. For ease of exposition, we create f and g with a single neuron. However, the construction easily extends to single layer NNs of arbitrary width. Also, assume we are in the high-dimensional regime, so that n is large. (Note that n here refers to the input dimension, not the number of samples in our dataset.) Define the two networks and the backdoor as follows:where w 1 ( 1 n , 1 n . . . , 1 n ) , b 1 1 and w 2 ( 1 n , 1 n , . . . , 1 n ) , b 2 1 2Note that equivalently, Bx \u2208 [0, 1] n : 1 x \u2265 n 2 and its measure of is given by:where the last step follows from Hoeffding's inequality. Since n is large, we have that B has exponentially small measure.We now compare the two networks on [0, 1] n \\ B. Clearly w 1 x \u2212 b 1 < w 2 x \u2212 b 2 < 0. Therefore,All that remains is to find a point within the backdoor, where the networks are different.\nApplied federated learning: Improving google keyboard query suggestions. Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, Fran\u00e7oise Beaufays, arXiv:1812.02903arXiv preprintTimothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Fran\u00e7oise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903, 2018.\n\nFederated learning for emoji prediction in a mobile keyboard. Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, Fran\u00e7oise Beaufays, arXiv:1906.04329arXiv preprintSwaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Fran\u00e7oise Beaufays. Federated learning for emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019.\n\nFederated learning for mobile keyboard prediction. Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran\u00e7oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo\u00e9 Kiddon, Daniel Ramage, arXiv:1811.03604arXiv preprintAndrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran\u00e7oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo\u00e9 Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.\n\nFederated learning of n-gram language models. Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril Allauzen, Fran\u00e7oise Beaufays, Michael Riley, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril Allauzen, Fran\u00e7oise Beaufays, and Michael Riley. Federated learning of n-gram language models. Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), 2019.\n\nA federated learning framework for healthcare iot devices. Binhang Yuan, Song Ge, Wenhui Xing, arXiv:2005.05083arXiv preprintBinhang Yuan, Song Ge, and Wenhui Xing. A federated learning framework for healthcare iot devices. arXiv preprint arXiv:2005.05083, 2020.\n\nPersonalization of end-to-end speech recognition on mobile devices for named entities. Khe Chai Sim, Francoise Beaufays, Arnaud Benard, Dhruv Guliani, Andreas Kabel, Nikhil Khare, Tamar Lucassen, Petr Zadrazil, Harry Zhang, Leif Johnson, IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). Khe Chai Sim, Francoise Beaufays, Arnaud Benard, Dhruv Guliani, Andreas Kabel, Nikhil Khare, Tamar Lucassen, Petr Zadrazil, Harry Zhang, Leif Johnson, and et al. Personalization of end-to-end speech recognition on mobile devices for named entities. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), Dec 2019.\n\nAn on-device deep neural network for face detection. Computer Vision Machine Learning Team (Apple)Computer Vision Machine Learning Team (Apple). An on-device deep neural network for face detection. https://machinelearning.apple.com/2017/11/16/face-detection.html.\n\nThe iphone x's new neural engine exemplifies apple's approach to ai. James Vincent, James Vincent. The iphone x's new neural engine exemplifies apple's approach to ai. https: //www.theverge.com/2017/9/13/16300464/apple-iphone-x-ai-neural-engine.\n\nFederated learning of predictive models from federated electronic health records. S Theodora, Ruidi Brisimi, Theofanie Chen, Alex Mela, Olshevsky, Wei Ioannis Ch Paschalidis, Shi, International journal of medical informatics. 112Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch Paschalidis, and Wei Shi. Federated learning of predictive models from federated electronic health records. International journal of medical informatics, 112:59-67, 2018.\n\nFederated learning for healthcare informatics. Jie Xu, Fei Wang, arXiv:1911.06270arXiv preprintJie Xu and Fei Wang. Federated learning for healthcare informatics. arXiv preprint arXiv:1911.06270, 2019.\n\nThe future of digital health with federated learning. Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger Roth, Shadi Albarqouni, Spyridon Bakas, N Mathieu, Bennett Galtier, Klaus Landman, Maier-Hein, arXiv:2003.08119arXiv preprintNicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett Landman, Klaus Maier-Hein, et al. The future of digital health with federated learning. arXiv preprint arXiv:2003.08119, 2020.\n\nPeter Kairouz, Brendan Mcmahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, arXiv:1912.04977Advances and open problems in federated learning. Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel CummingsarXiv preprintPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\nHow to backdoor federated learning. Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov, arXiv:1807.00459arXiv preprintEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. arXiv preprint arXiv:1807.00459, 2018.\n\nAnalyzing federated learning through an adversarial lens. Supriyo Arjun Nitin Bhagoji, Prateek Chakraborty, Seraphin Mittal, Calo, arXiv:1811.12470arXiv preprintArjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. arXiv preprint arXiv:1811.12470, 2018.\n\nPoisoning attacks against support vector machines. Battista Biggio, Blaine Nelson, Pavel Laskov, arXiv:1206.6389arXiv preprintBattista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389, 2012.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song, arXiv:1712.05526Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprintXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nMachine learning with adversaries: Byzantine tolerant gradient descent. Peva Blanchard, Rachid Guerraoui, Julien Stainer, Advances in Neural Information Processing Systems. Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119-129, 2017.\n\nDistributed statistical machine learning in adversarial settings: Byzantine gradient descent. Yudong Chen, Lili Su, Jiaming Xu, Proceedings of the ACM on Measurement and Analysis of Computing Systems. 12Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):1-25, 2017.\n\nThe byzantine generals problem. Leslie Lamport, Robert Shostak, Marshall Pease, Concurrency: the Works of Leslie Lamport. Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. In Concurrency: the Works of Leslie Lamport, pages 203-226. 2019.\n\nUnderstanding black-box predictions via influence functions. Wei Pang, Percy Koh, Liang, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1885-1894. JMLR. org, 2017.\n\n. Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang, Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.\n\nPractical distributed learning: Secure machine learning with communication-efficient local updates. Cong Xie, Sanmi Koyejo, Indranil Gupta, arXiv:1903.06996arXiv preprintCong Xie, Sanmi Koyejo, and Indranil Gupta. Practical distributed learning: Secure machine learning with communication-efficient local updates. arXiv preprint arXiv:1903.06996, 2019.\n\nCong Xie, arXiv:1903.07020Zeno++: robust asynchronous sgd with arbitrary number of byzantine workers. arXiv preprintCong Xie. Zeno++: robust asynchronous sgd with arbitrary number of byzantine workers. arXiv preprint arXiv:1903.07020, 2019.\n\nZeno: Distributed stochastic gradient descent with suspicion-based fault-tolerance. Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, arXiv:1805.10032arXiv preprintCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with suspicion-based fault-tolerance. arXiv preprint arXiv:1805.10032, 2018.\n\nA little is enough: Circumventing defenses for distributed learning. Gilad Baruch, Moran Baruch, Yoav Goldberg, Advances in Neural Information Processing Systems. Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. In Advances in Neural Information Processing Systems, pages 8632-8642, 2019.\n\nPractical secure aggregation for privacy-preserving machine learning. Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, Sarvar H Brendan Mcmahan, Daniel Patel, Aaron Ramage, Karn Segal, Seth, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityKeith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 1175-1191, 2017.\n\nZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, H Brendan Mcmahan, arXiv:1911.07963Can you really backdoor federated learning. arXiv preprintZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n\nJ Ian, Goodfellow, arXiv:1412.6572Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\nObfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. Anish Athalye, Nicholas Carlini, David Wagner, arXiv:1802.00420arXiv preprintAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.\n\nThe limitations of deep learning in adversarial settings. Nicolas Papernot, Patrick Mcdaniel, Somesh Jha, Matt Fredrikson, Ananthram Berkay Celik, Swami, 2016 IEEE European symposium on security and privacy (EuroS&P). IEEENicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on security and privacy (EuroS&P), pages 372-387. IEEE, 2016.\n\nDeepfool: a simple and accurate method to fool deep neural networks. Alhussein Seyed-Mohsen Moosavi-Dezfooli, Pascal Fawzi, Frossard, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574-2582, 2016.\n\nTowards evaluating the robustness of neural networks. Nicholas Carlini, David Wagner, 2017 ieee symposium on security and privacy (sp). IEEENicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39-57. IEEE, 2017.\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.06733arXiv preprintTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n\nStronger data poisoning attacks break data sanitization defenses. Pang Wei Koh, Jacob Steinhardt, Percy Liang, arXiv:1811.00741arXiv preprintPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. arXiv preprint arXiv:1811.00741, 2018.\n\nLearning differentially private recurrent language models. Daniel H Brendan Mcmahan, Kunal Ramage, Li Talwar, Zhang, arXiv:1710.06963arXiv preprintH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. arXiv preprint arXiv:1710.06963, 2017.\n\nWhen it comes to gorillas, google photos remains blind. Tom Simonite, wired.comTom Simonite. When it comes to gorillas, google photos remains blind. wired.com, 2018.\n\nTesla didn't fix an autopilot problem for three years, and now another person is dead. Andrew Hawkins, theverge.comAndrew Hawkins. Tesla didn't fix an autopilot problem for three years, and now another person is dead. theverge.com, 2019.\n\nThe very human challenge of safe driving. John Krafcik, John Krafcik. The very human challenge of safe driving. https://medium.com/, 2018.\n\nFederated machine learning: Concept and applications. Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong, ACM Transactions on Intelligent Systems and Technology (TIST). 102Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19, 2019.\n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith, arXiv:1908.07873Federated learning: Challenges, methods, and future directions. arXiv preprintTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. arXiv preprint arXiv:1908.07873, 2019.\n\nFederated learning: Strategies for improving communication efficiency. Jakub Kone\u010dn\u1ef3, Brendan Mcmahan, X Felix, Peter Yu, Ananda Richt\u00e1rik, Dave Theertha Suresh, Bacon, arXiv:1610.05492arXiv preprintJakub Kone\u010dn\u1ef3, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\n\nFednas: Federated deep learning via neural architecture search. Chaoyang He, Murali Annavaram, Salman Avestimehr, arXiv:2004.08546arXiv preprintChaoyang He, Murali Annavaram, and Salman Avestimehr. Fednas: Federated deep learning via neural architecture search. arXiv preprint arXiv:2004.08546, 2020.\n\nCommunication-efficient learning of deep networks from decentralized data. Eider H Brendan Mcmahan, Daniel Moore, Seth Ramage, Hampson, arXiv:1602.05629arXiv preprintH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.\n\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, H Brendan Mcmahan, arXiv:1902.01046Towards federated learning at scale: System design. arXiv preprintKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, H Brendan McMahan, et al. Towards federated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.\n\nXiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, Zhihua Zhang, arXiv:1907.02189On the convergence of fedavg on non-iid data. arXiv preprintXiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.\n\nPractical secure aggregation for federated learning on user-held data. Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, Sarvar H Brendan Mcmahan, Daniel Patel, Aaron Ramage, Karn Segal, Seth, arXiv:1611.04482arXiv preprintKeith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482, 2016.\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, arXiv:1905.12022Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv preprintMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv preprint arXiv:1905.12022, 2019.\n\nFederated learning with matched averaging. Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, Yasaman Khazaeni, arXiv:2002.06440arXiv preprintHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020.\n\nModel fusion via optimal transport. Pal Sidak, Martin Singh, Jaggi, arXiv:1910.05653arXiv preprintSidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. arXiv preprint arXiv:1910.05653, 2019.\n\nAdversarial machine learning. Ling Huang, D Anthony, Blaine Joseph, Nelson, J Doug Benjamin Ip Rubinstein, Tygar, Proceedings of the 4th ACM workshop on Security and artificial intelligence. the 4th ACM workshop on Security and artificial intelligenceLing Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pages 43-58, 2011.\n\nMulti-party poisoning through generalized p-tampering. Saeed Mahloujifar, Mohammad Mahmoody, Ameer Mohammed, arXiv:1809.03474arXiv preprintSaeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Multi-party poisoning through generalized p-tampering. arXiv preprint arXiv:1809.03474, 2018.\n\nAntidote: understanding and defending against poisoning of anomaly detectors. Blaine Benjamin Ip Rubinstein, Ling Nelson, Huang, D Anthony, Shing-Hon Joseph, Satish Lau, Nina Rao, J Doug Taft, Tygar, Proceedings of the 9th ACM SIGCOMM conference on Internet measurement. the 9th ACM SIGCOMM conference on Internet measurementBenjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. Antidote: understanding and defending against poisoning of anomaly detectors. In Proceedings of the 9th ACM SIGCOMM conference on Internet measurement, pages 1-14, 2009.\n\nCertified defenses for data poisoning attacks. Jacob Steinhardt, Pang Wei W Koh, Percy S Liang, Advances in neural information processing systems. Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In Advances in neural information processing systems, pages 3517-3529, 2017.\n\nA survey of outlier detection methodologies. Victoria Hodge, Jim Austin, Artificial intelligence review. 222Victoria Hodge and Jim Austin. A survey of outlier detection methodologies. Artificial intelligence review, 22(2):85-126, 2004.\n\nCasting out demons: Sanitizing training data for anomaly sensors. Angelos Gabriela F Cretu, Stavrou, E Michael, Salvatore J Locasto, Angelos D Stolfo, Keromytis, 2008 IEEE Symposium on Security and Privacy. IEEEGabriela F Cretu, Angelos Stavrou, Michael E Locasto, Salvatore J Stolfo, and Angelos D Keromytis. Casting out demons: Sanitizing training data for anomaly sensors. In 2008 IEEE Symposium on Security and Privacy (sp 2008), pages 81-95. IEEE, 2008.\n\nFine-pruning: Defending against backdooring attacks on deep neural networks. Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg, International Symposium on Research in Attacks, Intrusions, and Defenses. SpringerKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273-294. Springer, 2018.\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Y Ben, Zhao, 2019 IEEE Symposium on Security and Privacy (SP). IEEEBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707-723. IEEE, 2019.\n\nXiaojin Zhu, Adish Singla, Sandra Zilles, Anna N Rafferty, arXiv:1801.05927An overview of machine teaching. arXiv preprintXiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N Rafferty. An overview of machine teaching. arXiv preprint arXiv:1801.05927, 2018.\n\nAn optimal control approach to sequential machine teaching. Laurent Lessard, Xuezhou Zhang, Xiaojin Zhu, arXiv:1810.06175arXiv preprintLaurent Lessard, Xuezhou Zhang, and Xiaojin Zhu. An optimal control approach to sequential machine teaching. arXiv preprint arXiv:1810.06175, 2018.\n\nTeacher improves learning by selecting a training subset. Yuzhe Ma, Robert Nowak, Philippe Rigollet, Xuezhou Zhang, Xiaojin Zhu, arXiv:1802.08946arXiv preprintYuzhe Ma, Robert Nowak, Philippe Rigollet, Xuezhou Zhang, and Xiaojin Zhu. Teacher improves learning by selecting a training subset. arXiv preprint arXiv:1802.08946, 2018.\n\nMachine teaching for bayesian learners in the exponential family. Jerry Zhu, Advances in Neural Information Processing Systems. Jerry Zhu. Machine teaching for bayesian learners in the exponential family. In Advances in Neural Information Processing Systems, pages 1905-1913, 2013.\n\nData poisoning attacks against autoregressive models. Scott Alfeld, Xiaojin Zhu, Paul Barford, Thirtieth AAAI Conference on Artificial Intelligence. Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nUsing machine teaching to identify optimal training-set attacks on machine learners. Shike Mei, Xiaojin Zhu, Twenty-Ninth AAAI Conference on Artificial Intelligence. Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine learners. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\nSome submodular data-poisoning attacks on machine learners. Shike Mei, Xiaojin Zhu, Technical reportShike Mei and Xiaojin Zhu. Some submodular data-poisoning attacks on machine learners. Technical report, 2017.\n\nDistributed statistical machine learning in adversarial settings: Byzantine gradient descent. Yudong Chen, Lili Su, Jiaming Xu, Proc. ACM Meas. Anal. Comput. Syst. 12Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. Proc. ACM Meas. Anal. Comput. Syst., 1(2), December 2017.\n\nRobust multi-agent optimization: Coping with byzantine agents with input redundancy. Lili Su, Nitin H Vaidya, Stabilization, Safety, and Security of Distributed Systems. Borzoo Bonakdarpour and Franck PetitChamSpringer International PublishingLili Su and Nitin H. Vaidya. Robust multi-agent optimization: Coping with byzantine agents with input redundancy. In Borzoo Bonakdarpour and Franck Petit, editors, Stabilization, Safety, and Security of Distributed Systems, pages 368-382, Cham, 2016. Springer International Publishing.\n\nNon-bayesian learning in the presence of byzantine agents. Lili Su, Nitin H Vaidya, Distributed Computing. Cyril Gavoille and David IlcinkasBerlin, Heidelberg; Berlin HeidelbergSpringerLili Su and Nitin H. Vaidya. Non-bayesian learning in the presence of byzantine agents. In Cyril Gavoille and David Ilcinkas, editors, Distributed Computing, pages 414-427, Berlin, Heidelberg, 2016. Springer Berlin Heidelberg.\n\nDefending against saddle point attack in byzantine-robust distributed learning. Dong Yin, Yudong Chen, Ramchandran Kannan, Peter Bartlett, International Conference on Machine Learning. Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Defending against saddle point attack in byzantine-robust distributed learning. In International Conference on Machine Learning, pages 7074-7084, 2019.\n\nDong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett, arXiv:1803.01498Byzantine-robust distributed learning: Towards optimal statistical rates. arXiv preprintDong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. arXiv preprint arXiv:1803.01498, 2018.\n\nArsany Hany Abdelmessih Guirguis, and S\u00e9bastien Louis Alexandre Rouault. Aggregathor: Byzantine machine learning via robust gradient aggregation. Georgios Damaskinos, Rachid El Mahdi El Mhamdi, Guerraoui, The Conference on Systems and Machine Learning (SysML). CONFGeorgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Arsany Hany Abdelmessih Guirguis, and S\u00e9bastien Louis Alexandre Rouault. Aggregathor: Byzantine machine learning via robust gradient aggregation. In The Conference on Systems and Machine Learning (SysML), 2019, number CONF, 2019.\n\nByzantine stochastic gradient descent. Dan Alistarh, Zeyuan Allen-Zhu, Jerry Li, Advances in Neural Information Processing Systems. Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4613-4623, 2018.\n\nLingjiao Chen, Hongyi Wang, Zachary Charles, Dimitris Papailiopoulos Draco, arXiv:1803.09877Byzantine-resilient distributed training via redundant gradients. arXiv preprintLingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Draco: Byzantine-resilient distributed training via redundant gradients. arXiv preprint arXiv:1803.09877, 2018.\n\nElection coding for distributed learning: Protecting signsgd against byzantine attacks. Dong-Jun Jy-Yong Sohn, Beongjun Han, Jaekyun Choi, Moon, arXiv:1910.06093arXiv preprintJy-yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon. Election coding for distributed learning: Protecting signsgd against byzantine attacks. arXiv preprint arXiv:1910.06093, 2019.\n\nDetox: A redundancybased framework for faster and more robust gradient aggregation. Shashank Rajput, Hongyi Wang, Zachary Charles, Dimitris Papailiopoulos, Advances in Neural Information Processing Systems. Shashank Rajput, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Detox: A redundancy- based framework for faster and more robust gradient aggregation. In Advances in Neural Information Processing Systems, pages 10320-10330, 2019.\n\nEfficient largescale distributed training of conditional maximum entropy models. Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, Gideon S Mann, Advances in neural information processing systems. Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S Mann. Efficient large- scale distributed training of conditional maximum entropy models. In Advances in neural information processing systems, pages 1231-1239, 2009.\n\nDistributed stochastic optimization and learning. Ohad Shamir, Nathan Srebro, 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEEOhad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850-857. IEEE, 2014.\n\nJian Zhang, Christopher De Sa, Ioannis Mitliagkas, Christopher R\u00e9, arXiv:1606.07365Parallel sgd: When does averaging help. arXiv preprintJian Zhang, Christopher De Sa, Ioannis Mitliagkas, and Christopher R\u00e9. Parallel sgd: When does averaging help? arXiv preprint arXiv:1606.07365, 2016.\n\nParallelized stochastic gradient descent. Martin Zinkevich, Markus Weimer, Lihong Li, Alex J Smola, Advances in neural information processing systems. Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In Advances in neural information processing systems, pages 2595-2603, 2010.\n\nU Sebastian, Stich, arXiv:1805.09767Local sgd converges fast and communicates little. arXiv preprintSebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018.\n\nEnhancing the reliability of out-of-distribution image detection in neural networks. Shiyu Liang, Yixuan Li, Rayadurgam Srikant, arXiv:1706.02690arXiv preprintShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nA simple unified framework for detecting out-of-distribution samples and adversarial attacks. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, Advances in Neural Information Processing Systems. Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, pages 7167-7177, 2018.\n\nCertifying some distributional robustness with principled adversarial training. Aman Sinha, Hongseok Namkoong, John C Duchi, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netAman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness with principled adversarial training. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nTowards deep learning models resistant to adversarial attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, International Conference on Learning Representations. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\nReluplex: An efficient smt solver for verifying deep neural networks. Guy Katz, Clark Barrett, L David, Kyle Dill, Julian, Kochenderfer, International Conference on Computer Aided Verification. SpringerGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pages 97-117. Springer, 2017.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nEmnist: Extending mnist to handwritten letters. Gregory Cohen, Saeed Afshar, Jonathan Tapson, Andre Van Schaik, 2017 International Joint Conference on Neural Networks (\u0132CNN). IEEEGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (\u0132CNN), pages 2921-2926. IEEE, 2017.\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 8611Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nTwitter sentiment classification using distant supervision. Alec Go, Richa Bhayani, Lei Huang, Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nArdis: a swedish historical handwritten digit dataset. Huseyin Kusetogullari, Amir Yavariabdi, Abbas Cheddad, H\u00e5kan Grahn, Johan Hall, Neural Computing and Applications. Huseyin Kusetogullari, Amir Yavariabdi, Abbas Cheddad, H\u00e5kan Grahn, and Johan Hall. Ardis: a swedish historical handwritten digit dataset. Neural Computing and Applications, pages 1-14, 2019.\n\nRobust aggregation for federated learning. Krishna Pillutla, M Sham, Zaid Kakade, Harchaoui, arXiv:1912.13445arXiv preprintKrishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. arXiv preprint arXiv:1912.13445, 2019.\n\nDifferentially private federated learning: A client level perspective. C Robin, Tassilo Geyer, Moin Klein, Nabi, arXiv:1712.07557arXiv preprintRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017.\n\nFrom parity to preference-based notions of fairness in classification. Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, Adrian Weller, Advances in Neural Information Processing Systems. Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, and Adrian Weller. From parity to preference-based notions of fairness in classification. In Advances in Neural Information Processing Systems, pages 229-239, 2017.\n\nUnderstanding deep learning requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, 5th International Conference on Learning Representations, ICLR. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR, 2017.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.07146arXiv preprintSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n\nPytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in Neural Information Processing Systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. In Advances in Neural Information Processing Systems, pages 8024-8035, 2019.\n\nRobust and communicationefficient federated learning from non-iid data. Felix Sattler, Simon Wiedemann, Klaus-Robert M\u00fcller, Wojciech Samek, IEEE transactions on neural networks and learning systems. Felix Sattler, Simon Wiedemann, Klaus-Robert M\u00fcller, and Wojciech Samek. Robust and communication- efficient federated learning from non-iid data. IEEE transactions on neural networks and learning systems, 2019.\n\nThe non-iid data quagmire of decentralized machine learning. Kevin Hsieh, Amar Phanishayee, Onur Mutlu, Phillip B Gibbons, arXiv:1910.00189arXiv preprintKevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B Gibbons. The non-iid data quagmire of decentralized machine learning. arXiv preprint arXiv:1910.00189, 2019.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n", "annotations": {"author": "[{\"end\":171,\"start\":72},{\"end\":278,\"start\":172},{\"end\":382,\"start\":279},{\"end\":488,\"start\":383},{\"end\":592,\"start\":489},{\"end\":693,\"start\":593},{\"end\":794,\"start\":694},{\"end\":906,\"start\":795}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":79},{\"end\":190,\"start\":179},{\"end\":294,\"start\":288},{\"end\":400,\"start\":389},{\"end\":504,\"start\":497},{\"end\":605,\"start\":601},{\"end\":706,\"start\":703},{\"end\":818,\"start\":804}]", "author_first_name": "[{\"end\":78,\"start\":72},{\"end\":178,\"start\":172},{\"end\":287,\"start\":279},{\"end\":388,\"start\":383},{\"end\":496,\"start\":489},{\"end\":600,\"start\":593},{\"end\":702,\"start\":694},{\"end\":803,\"start\":795}]", "author_affiliation": "[{\"end\":170,\"start\":85},{\"end\":277,\"start\":192},{\"end\":381,\"start\":296},{\"end\":487,\"start\":402},{\"end\":591,\"start\":506},{\"end\":692,\"start\":607},{\"end\":793,\"start\":708},{\"end\":905,\"start\":820}]", "title": "[{\"end\":69,\"start\":1},{\"end\":975,\"start\":907}]", "venue": null, "abstract": "[{\"end\":2429,\"start\":977}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2889,\"start\":2886},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2891,\"start\":2889},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2893,\"start\":2891},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2895,\"start\":2893},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2897,\"start\":2895},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2921,\"start\":2918},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2961,\"start\":2958},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2963,\"start\":2961},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3044,\"start\":3041},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3047,\"start\":3044},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3050,\"start\":3047},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3327,\"start\":3323},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3330,\"start\":3327},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3333,\"start\":3330},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3336,\"start\":3333},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3339,\"start\":3336},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3342,\"start\":3339},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3345,\"start\":3342},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3348,\"start\":3345},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3351,\"start\":3348},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3354,\"start\":3351},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3357,\"start\":3354},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3360,\"start\":3357},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3363,\"start\":3360},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3366,\"start\":3363},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3902,\"start\":3898},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4180,\"start\":4176},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5163,\"start\":5159},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5487,\"start\":5483},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5523,\"start\":5519},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6115,\"start\":6111},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6180,\"start\":6176},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6502,\"start\":6498},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6505,\"start\":6502},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6508,\"start\":6505},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6511,\"start\":6508},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6514,\"start\":6511},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7505,\"start\":7501},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8231,\"start\":8227},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8234,\"start\":8231},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8237,\"start\":8234},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8583,\"start\":8579},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8586,\"start\":8583},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8604,\"start\":8600},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9100,\"start\":9096},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9647,\"start\":9643},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9650,\"start\":9647},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9670,\"start\":9666},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10632,\"start\":10628},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10764,\"start\":10760},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10767,\"start\":10764},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11167,\"start\":11163},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11622,\"start\":11618},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11625,\"start\":11622},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11628,\"start\":11625},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11631,\"start\":11628},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11634,\"start\":11631},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11700,\"start\":11696},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11909,\"start\":11905},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11912,\"start\":11909},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11922,\"start\":11918},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12125,\"start\":12121},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12140,\"start\":12136},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12181,\"start\":12177},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12571,\"start\":12567},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12596,\"start\":12592},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12743,\"start\":12739},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12746,\"start\":12743},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12765,\"start\":12761},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12783,\"start\":12779},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12990,\"start\":12986},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13171,\"start\":13167},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13208,\"start\":13204},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13313,\"start\":13309},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13338,\"start\":13334},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13390,\"start\":13386},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13393,\"start\":13390},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13880,\"start\":13876},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13883,\"start\":13880},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14054,\"start\":14050},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14222,\"start\":14218},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":14270,\"start\":14266},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14273,\"start\":14270},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14276,\"start\":14273},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14397,\"start\":14393},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14400,\"start\":14397},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":14403,\"start\":14400},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":14609,\"start\":14605},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14785,\"start\":14781},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14867,\"start\":14863},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15426,\"start\":15422},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":15743,\"start\":15739},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":15919,\"start\":15915},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":15922,\"start\":15919},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":15925,\"start\":15922},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":15928,\"start\":15925},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":15931,\"start\":15928},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15934,\"start\":15931},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":15937,\"start\":15934},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":15945,\"start\":15941},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":16101,\"start\":16097},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":16174,\"start\":16170},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16356,\"start\":16352},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16933,\"start\":16929},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":16978,\"start\":16974},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":16981,\"start\":16978},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":16984,\"start\":16981},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":16987,\"start\":16984},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":16990,\"start\":16987},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":18489,\"start\":18485},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19374,\"start\":19370},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19377,\"start\":19374},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19826,\"start\":19822},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19829,\"start\":19826},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21191,\"start\":21187},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21483,\"start\":21479},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22285,\"start\":22281},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22296,\"start\":22292},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":22954,\"start\":22950},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":26843,\"start\":26839},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":27263,\"start\":27259},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":28146,\"start\":28142},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30627,\"start\":30623},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30741,\"start\":30737},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31393,\"start\":31389},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31670,\"start\":31666},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":31777,\"start\":31773},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31943,\"start\":31939},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":31946,\"start\":31943},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32806,\"start\":32802},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32809,\"start\":32806},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32837,\"start\":32833},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33736,\"start\":33732},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":38324,\"start\":38320},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":40453,\"start\":40449},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":40554,\"start\":40550},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":40874,\"start\":40870},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":42835,\"start\":42831},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":42935,\"start\":42931},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":43351,\"start\":43347},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":43354,\"start\":43351},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44186,\"start\":44182},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":46910,\"start\":46906},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48575,\"start\":48571},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":49203,\"start\":49199},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":49207,\"start\":49203},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":50229,\"start\":50225},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":51146,\"start\":51141},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":51866,\"start\":51862},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":52504,\"start\":52503},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":54949,\"start\":54945},{\"end\":56723,\"start\":56720},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":58236,\"start\":58232},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":59124,\"start\":59120},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":59139,\"start\":59135},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":59198,\"start\":59194},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":59213,\"start\":59209},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":59288,\"start\":59284},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":59370,\"start\":59366},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":59384,\"start\":59380},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":59460,\"start\":59456},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":59463,\"start\":59460},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":60274,\"start\":60270}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58942,\"start\":58922},{\"attributes\":{\"id\":\"fig_1\"},\"end\":61124,\"start\":58943},{\"attributes\":{\"id\":\"fig_2\"},\"end\":61437,\"start\":61125},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61509,\"start\":61438},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61913,\"start\":61510},{\"attributes\":{\"id\":\"fig_5\"},\"end\":62003,\"start\":61914},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62096,\"start\":62004},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62178,\"start\":62097},{\"attributes\":{\"id\":\"fig_8\"},\"end\":62241,\"start\":62179},{\"attributes\":{\"id\":\"fig_9\"},\"end\":62476,\"start\":62242},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62859,\"start\":62477},{\"attributes\":{\"id\":\"fig_11\"},\"end\":63314,\"start\":62860},{\"attributes\":{\"id\":\"fig_12\"},\"end\":63329,\"start\":63315},{\"attributes\":{\"id\":\"fig_13\"},\"end\":63384,\"start\":63330},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":64268,\"start\":63385},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":64885,\"start\":64269},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":66181,\"start\":64886},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66368,\"start\":66182}]", "paragraph": "[{\"end\":3051,\"start\":2445},{\"end\":3852,\"start\":3053},{\"end\":4802,\"start\":3854},{\"end\":5488,\"start\":4804},{\"end\":5966,\"start\":5490},{\"end\":6365,\"start\":5968},{\"end\":6773,\"start\":6367},{\"end\":9210,\"start\":7423},{\"end\":9864,\"start\":9212},{\"end\":10768,\"start\":9866},{\"end\":11507,\"start\":10770},{\"end\":12234,\"start\":11509},{\"end\":14101,\"start\":12236},{\"end\":14610,\"start\":14103},{\"end\":16279,\"start\":14612},{\"end\":16704,\"start\":16333},{\"end\":17497,\"start\":16725},{\"end\":17631,\"start\":17531},{\"end\":17833,\"start\":17662},{\"end\":18283,\"start\":17999},{\"end\":18490,\"start\":18297},{\"end\":18885,\"start\":18492},{\"end\":19469,\"start\":18887},{\"end\":19562,\"start\":19471},{\"end\":20020,\"start\":19588},{\"end\":21066,\"start\":20022},{\"end\":21492,\"start\":21068},{\"end\":21895,\"start\":21532},{\"end\":22343,\"start\":21935},{\"end\":23968,\"start\":22357},{\"end\":24683,\"start\":24018},{\"end\":25239,\"start\":24938},{\"end\":25768,\"start\":25573},{\"end\":25927,\"start\":25856},{\"end\":26030,\"start\":25993},{\"end\":26226,\"start\":26049},{\"end\":26453,\"start\":26263},{\"end\":26583,\"start\":26515},{\"end\":27264,\"start\":26585},{\"end\":27725,\"start\":27266},{\"end\":27834,\"start\":27780},{\"end\":28317,\"start\":27984},{\"end\":28737,\"start\":28319},{\"end\":28840,\"start\":28739},{\"end\":29233,\"start\":28842},{\"end\":29692,\"start\":29235},{\"end\":29919,\"start\":29768},{\"end\":30675,\"start\":29935},{\"end\":31256,\"start\":30685},{\"end\":32136,\"start\":31281},{\"end\":33071,\"start\":32178},{\"end\":34091,\"start\":33110},{\"end\":35188,\"start\":34163},{\"end\":35474,\"start\":35190},{\"end\":37381,\"start\":35476},{\"end\":38041,\"start\":37438},{\"end\":39498,\"start\":38089},{\"end\":40295,\"start\":39500},{\"end\":42128,\"start\":40297},{\"end\":42690,\"start\":42143},{\"end\":43824,\"start\":42758},{\"end\":46285,\"start\":43826},{\"end\":46866,\"start\":46339},{\"end\":47315,\"start\":46868},{\"end\":47804,\"start\":47317},{\"end\":48101,\"start\":47806},{\"end\":48782,\"start\":48103},{\"end\":49208,\"start\":48784},{\"end\":49385,\"start\":49210},{\"end\":49496,\"start\":49425},{\"end\":50244,\"start\":49498},{\"end\":51869,\"start\":50294},{\"end\":54129,\"start\":51898},{\"end\":55276,\"start\":54249},{\"end\":56140,\"start\":55375},{\"end\":56235,\"start\":56142},{\"end\":56418,\"start\":56257},{\"end\":56869,\"start\":56420},{\"end\":56914,\"start\":56871},{\"end\":57020,\"start\":56916},{\"end\":57360,\"start\":57022},{\"end\":57517,\"start\":57362},{\"end\":57632,\"start\":57579},{\"end\":57841,\"start\":57754},{\"end\":58363,\"start\":57852},{\"end\":58474,\"start\":58418},{\"end\":58612,\"start\":58476},{\"end\":58921,\"start\":58614}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16724,\"start\":16705},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17530,\"start\":17498},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17998,\"start\":17834},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21531,\"start\":21493},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21934,\"start\":21896},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24937,\"start\":24684},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25572,\"start\":25240},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25855,\"start\":25769},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25992,\"start\":25928},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26048,\"start\":26031},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26262,\"start\":26227},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26514,\"start\":26454},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27983,\"start\":27906},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29767,\"start\":29693},{\"attributes\":{\"id\":\"formula_14\"},\"end\":55374,\"start\":55277},{\"attributes\":{\"id\":\"formula_16\"},\"end\":56256,\"start\":56236},{\"attributes\":{\"id\":\"formula_19\"},\"end\":57578,\"start\":57518},{\"attributes\":{\"id\":\"formula_20\"},\"end\":57753,\"start\":57633}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":43524,\"start\":43517},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44717,\"start\":44710},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":48985,\"start\":48978}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2443,\"start\":2431},{\"end\":7154,\"start\":6776},{\"end\":7252,\"start\":7157},{\"end\":7421,\"start\":7255},{\"attributes\":{\"n\":\"2\"},\"end\":16331,\"start\":16282},{\"end\":17660,\"start\":17634},{\"end\":18295,\"start\":18286},{\"end\":19586,\"start\":19565},{\"end\":22355,\"start\":22346},{\"attributes\":{\"n\":\"3\"},\"end\":24016,\"start\":23971},{\"end\":27778,\"start\":27728},{\"end\":27905,\"start\":27837},{\"attributes\":{\"n\":\"4\"},\"end\":29933,\"start\":29922},{\"end\":30683,\"start\":30678},{\"attributes\":{\"n\":\"4.1\"},\"end\":31279,\"start\":31259},{\"end\":32176,\"start\":32139},{\"end\":33108,\"start\":33074},{\"end\":34161,\"start\":34094},{\"end\":37436,\"start\":37384},{\"end\":38087,\"start\":38044},{\"attributes\":{\"n\":\"5\"},\"end\":42141,\"start\":42131},{\"end\":42756,\"start\":42693},{\"end\":46337,\"start\":46288},{\"end\":49423,\"start\":49388},{\"end\":50292,\"start\":50247},{\"end\":51896,\"start\":51872},{\"end\":54191,\"start\":54132},{\"end\":54247,\"start\":54194},{\"end\":57850,\"start\":57844},{\"end\":58416,\"start\":58366},{\"end\":58933,\"start\":58923},{\"end\":58954,\"start\":58944},{\"end\":61136,\"start\":61126},{\"end\":61449,\"start\":61439},{\"end\":61531,\"start\":61511},{\"end\":61925,\"start\":61915},{\"end\":62015,\"start\":62005},{\"end\":62109,\"start\":62098},{\"end\":62191,\"start\":62180},{\"end\":62254,\"start\":62243},{\"end\":62500,\"start\":62478},{\"end\":62872,\"start\":62861},{\"end\":63338,\"start\":63331},{\"end\":64279,\"start\":64270},{\"end\":64896,\"start\":64887}]", "table": "[{\"end\":64268,\"start\":63519},{\"end\":64885,\"start\":64357},{\"end\":66181,\"start\":65098},{\"end\":66368,\"start\":66213}]", "figure_caption": "[{\"end\":58942,\"start\":58935},{\"end\":61124,\"start\":58956},{\"end\":61437,\"start\":61138},{\"end\":61509,\"start\":61451},{\"end\":61913,\"start\":61534},{\"end\":62003,\"start\":61927},{\"end\":62096,\"start\":62017},{\"end\":62178,\"start\":62112},{\"end\":62241,\"start\":62194},{\"end\":62476,\"start\":62257},{\"end\":62859,\"start\":62505},{\"end\":63314,\"start\":62875},{\"end\":63329,\"start\":63317},{\"end\":63384,\"start\":63340},{\"end\":63519,\"start\":63387},{\"end\":64357,\"start\":64281},{\"end\":65098,\"start\":64898},{\"end\":66213,\"start\":66184}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":7991,\"start\":7985},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23466,\"start\":23458},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":30638,\"start\":30630},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32593,\"start\":32587},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33225,\"start\":33219},{\"end\":34558,\"start\":34550},{\"end\":36250,\"start\":36242},{\"end\":36894,\"start\":36886},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37741,\"start\":37733},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39496,\"start\":39488},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39612,\"start\":39604},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40754,\"start\":40745},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43546,\"start\":43526},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52108,\"start\":52099},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52266,\"start\":52257},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":53009,\"start\":53000},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":53894,\"start\":53885},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":54424,\"start\":54417}]", "bib_author_first_name": "[{\"end\":72020,\"start\":72013},{\"end\":72032,\"start\":72027},{\"end\":72047,\"start\":72041},{\"end\":72065,\"start\":72057},{\"end\":72074,\"start\":72071},{\"end\":72087,\"start\":72079},{\"end\":72100,\"start\":72094},{\"end\":72118,\"start\":72109},{\"end\":72461,\"start\":72454},{\"end\":72478,\"start\":72473},{\"end\":72496,\"start\":72488},{\"end\":72511,\"start\":72502},{\"end\":72783,\"start\":72777},{\"end\":72798,\"start\":72790},{\"end\":72809,\"start\":72804},{\"end\":72826,\"start\":72819},{\"end\":72847,\"start\":72838},{\"end\":72862,\"start\":72858},{\"end\":72881,\"start\":72875},{\"end\":72896,\"start\":72891},{\"end\":72911,\"start\":72905},{\"end\":73242,\"start\":73234},{\"end\":73255,\"start\":73249},{\"end\":73278,\"start\":73273},{\"end\":73295,\"start\":73288},{\"end\":73307,\"start\":73302},{\"end\":73327,\"start\":73318},{\"end\":73345,\"start\":73338},{\"end\":73839,\"start\":73832},{\"end\":73850,\"start\":73846},{\"end\":73861,\"start\":73855},{\"end\":74132,\"start\":74124},{\"end\":74147,\"start\":74138},{\"end\":74164,\"start\":74158},{\"end\":74178,\"start\":74173},{\"end\":74195,\"start\":74188},{\"end\":74209,\"start\":74203},{\"end\":74222,\"start\":74217},{\"end\":74237,\"start\":74233},{\"end\":74253,\"start\":74248},{\"end\":74265,\"start\":74261},{\"end\":75017,\"start\":75012},{\"end\":75273,\"start\":75272},{\"end\":75289,\"start\":75284},{\"end\":75308,\"start\":75299},{\"end\":75319,\"start\":75315},{\"end\":75340,\"start\":75337},{\"end\":75716,\"start\":75713},{\"end\":75724,\"start\":75721},{\"end\":75929,\"start\":75923},{\"end\":75942,\"start\":75937},{\"end\":75956,\"start\":75951},{\"end\":75967,\"start\":75961},{\"end\":75985,\"start\":75979},{\"end\":75997,\"start\":75992},{\"end\":76018,\"start\":76010},{\"end\":76027,\"start\":76026},{\"end\":76044,\"start\":76037},{\"end\":76059,\"start\":76054},{\"end\":76374,\"start\":76369},{\"end\":76391,\"start\":76384},{\"end\":76408,\"start\":76401},{\"end\":76424,\"start\":76416},{\"end\":76438,\"start\":76433},{\"end\":76897,\"start\":76891},{\"end\":76918,\"start\":76911},{\"end\":76931,\"start\":76925},{\"end\":76944,\"start\":76937},{\"end\":76959,\"start\":76953},{\"end\":77226,\"start\":77219},{\"end\":77255,\"start\":77248},{\"end\":77277,\"start\":77269},{\"end\":77556,\"start\":77548},{\"end\":77571,\"start\":77565},{\"end\":77585,\"start\":77580},{\"end\":77769,\"start\":77763},{\"end\":77781,\"start\":77776},{\"end\":77789,\"start\":77787},{\"end\":77802,\"start\":77794},{\"end\":77811,\"start\":77807},{\"end\":78169,\"start\":78165},{\"end\":78187,\"start\":78181},{\"end\":78205,\"start\":78199},{\"end\":78571,\"start\":78565},{\"end\":78582,\"start\":78578},{\"end\":78594,\"start\":78587},{\"end\":78935,\"start\":78929},{\"end\":78951,\"start\":78945},{\"end\":78969,\"start\":78961},{\"end\":79234,\"start\":79231},{\"end\":79246,\"start\":79241},{\"end\":79601,\"start\":79595},{\"end\":79614,\"start\":79607},{\"end\":79625,\"start\":79619},{\"end\":79642,\"start\":79633},{\"end\":79652,\"start\":79648},{\"end\":79666,\"start\":79659},{\"end\":79680,\"start\":79673},{\"end\":79933,\"start\":79929},{\"end\":79944,\"start\":79939},{\"end\":79961,\"start\":79953},{\"end\":80187,\"start\":80183},{\"end\":80513,\"start\":80509},{\"end\":80529,\"start\":80519},{\"end\":80546,\"start\":80538},{\"end\":80831,\"start\":80826},{\"end\":80845,\"start\":80840},{\"end\":80858,\"start\":80854},{\"end\":81189,\"start\":81184},{\"end\":81208,\"start\":81200},{\"end\":81220,\"start\":81217},{\"end\":81237,\"start\":81230},{\"end\":81255,\"start\":81249},{\"end\":81281,\"start\":81275},{\"end\":81294,\"start\":81289},{\"end\":81307,\"start\":81303},{\"end\":81809,\"start\":81803},{\"end\":81820,\"start\":81815},{\"end\":81836,\"start\":81830},{\"end\":81863,\"start\":81854},{\"end\":82106,\"start\":82105},{\"end\":82492,\"start\":82487},{\"end\":82510,\"start\":82502},{\"end\":82525,\"start\":82520},{\"end\":82821,\"start\":82814},{\"end\":82839,\"start\":82832},{\"end\":82856,\"start\":82850},{\"end\":82866,\"start\":82862},{\"end\":82888,\"start\":82879},{\"end\":83311,\"start\":83302},{\"end\":83349,\"start\":83343},{\"end\":83814,\"start\":83806},{\"end\":83829,\"start\":83824},{\"end\":84147,\"start\":84141},{\"end\":84159,\"start\":84152},{\"end\":84183,\"start\":84174},{\"end\":84464,\"start\":84460},{\"end\":84479,\"start\":84474},{\"end\":84497,\"start\":84492},{\"end\":84755,\"start\":84749},{\"end\":84780,\"start\":84775},{\"end\":84791,\"start\":84789},{\"end\":85057,\"start\":85054},{\"end\":85258,\"start\":85252},{\"end\":85450,\"start\":85446},{\"end\":85603,\"start\":85598},{\"end\":85614,\"start\":85610},{\"end\":85628,\"start\":85620},{\"end\":85642,\"start\":85635},{\"end\":85910,\"start\":85906},{\"end\":85919,\"start\":85915},{\"end\":85937,\"start\":85932},{\"end\":85957,\"start\":85949},{\"end\":86302,\"start\":86297},{\"end\":86319,\"start\":86312},{\"end\":86330,\"start\":86329},{\"end\":86343,\"start\":86338},{\"end\":86354,\"start\":86348},{\"end\":86370,\"start\":86366},{\"end\":86711,\"start\":86703},{\"end\":86722,\"start\":86716},{\"end\":86740,\"start\":86734},{\"end\":87021,\"start\":87016},{\"end\":87047,\"start\":87041},{\"end\":87059,\"start\":87055},{\"end\":87295,\"start\":87290},{\"end\":87312,\"start\":87306},{\"end\":87330,\"start\":87322},{\"end\":87349,\"start\":87342},{\"end\":87360,\"start\":87356},{\"end\":87379,\"start\":87371},{\"end\":87393,\"start\":87388},{\"end\":87407,\"start\":87402},{\"end\":87424,\"start\":87417},{\"end\":87806,\"start\":87801},{\"end\":87818,\"start\":87811},{\"end\":87832,\"start\":87826},{\"end\":87845,\"start\":87839},{\"end\":87858,\"start\":87852},{\"end\":88173,\"start\":88168},{\"end\":88192,\"start\":88184},{\"end\":88204,\"start\":88201},{\"end\":88221,\"start\":88214},{\"end\":88239,\"start\":88233},{\"end\":88265,\"start\":88259},{\"end\":88278,\"start\":88273},{\"end\":88291,\"start\":88287},{\"end\":88594,\"start\":88587},{\"end\":88612,\"start\":88606},{\"end\":88628,\"start\":88622},{\"end\":88644,\"start\":88636},{\"end\":89052,\"start\":89046},{\"end\":89066,\"start\":89059},{\"end\":89084,\"start\":89078},{\"end\":89098,\"start\":89090},{\"end\":89122,\"start\":89115},{\"end\":89376,\"start\":89373},{\"end\":89390,\"start\":89384},{\"end\":89579,\"start\":89575},{\"end\":89588,\"start\":89587},{\"end\":89604,\"start\":89598},{\"end\":89627,\"start\":89621},{\"end\":90073,\"start\":90068},{\"end\":90095,\"start\":90087},{\"end\":90111,\"start\":90106},{\"end\":90389,\"start\":90383},{\"end\":90418,\"start\":90414},{\"end\":90435,\"start\":90434},{\"end\":90454,\"start\":90445},{\"end\":90469,\"start\":90463},{\"end\":90479,\"start\":90475},{\"end\":90491,\"start\":90485},{\"end\":90978,\"start\":90973},{\"end\":90995,\"start\":90991},{\"end\":91014,\"start\":91007},{\"end\":91304,\"start\":91296},{\"end\":91315,\"start\":91312},{\"end\":91561,\"start\":91554},{\"end\":91590,\"start\":91589},{\"end\":91609,\"start\":91600},{\"end\":91611,\"start\":91610},{\"end\":91630,\"start\":91621},{\"end\":92029,\"start\":92025},{\"end\":92042,\"start\":92035},{\"end\":92066,\"start\":92057},{\"end\":92478,\"start\":92473},{\"end\":92493,\"start\":92485},{\"end\":92504,\"start\":92499},{\"end\":92518,\"start\":92511},{\"end\":92528,\"start\":92523},{\"end\":92546,\"start\":92540},{\"end\":92555,\"start\":92554},{\"end\":92886,\"start\":92879},{\"end\":92897,\"start\":92892},{\"end\":92912,\"start\":92906},{\"end\":92925,\"start\":92921},{\"end\":92927,\"start\":92926},{\"end\":93204,\"start\":93197},{\"end\":93221,\"start\":93214},{\"end\":93236,\"start\":93229},{\"end\":93484,\"start\":93479},{\"end\":93495,\"start\":93489},{\"end\":93511,\"start\":93503},{\"end\":93529,\"start\":93522},{\"end\":93544,\"start\":93537},{\"end\":93824,\"start\":93819},{\"end\":94095,\"start\":94090},{\"end\":94111,\"start\":94104},{\"end\":94121,\"start\":94117},{\"end\":94438,\"start\":94433},{\"end\":94451,\"start\":94444},{\"end\":94758,\"start\":94753},{\"end\":94771,\"start\":94764},{\"end\":95005,\"start\":94999},{\"end\":95016,\"start\":95012},{\"end\":95028,\"start\":95021},{\"end\":95351,\"start\":95347},{\"end\":95361,\"start\":95356},{\"end\":95363,\"start\":95362},{\"end\":95855,\"start\":95851},{\"end\":95865,\"start\":95860},{\"end\":95867,\"start\":95866},{\"end\":96289,\"start\":96285},{\"end\":96301,\"start\":96295},{\"end\":96319,\"start\":96308},{\"end\":96333,\"start\":96328},{\"end\":96610,\"start\":96606},{\"end\":96622,\"start\":96616},{\"end\":96635,\"start\":96629},{\"end\":96654,\"start\":96649},{\"end\":97100,\"start\":97092},{\"end\":97119,\"start\":97113},{\"end\":97543,\"start\":97540},{\"end\":97560,\"start\":97554},{\"end\":97577,\"start\":97572},{\"end\":97804,\"start\":97796},{\"end\":97817,\"start\":97811},{\"end\":97831,\"start\":97824},{\"end\":97849,\"start\":97841},{\"end\":97864,\"start\":97850},{\"end\":98251,\"start\":98243},{\"end\":98274,\"start\":98266},{\"end\":98287,\"start\":98280},{\"end\":98611,\"start\":98603},{\"end\":98626,\"start\":98620},{\"end\":98640,\"start\":98633},{\"end\":98658,\"start\":98650},{\"end\":99053,\"start\":99049},{\"end\":99071,\"start\":99064},{\"end\":99085,\"start\":99079},{\"end\":99100,\"start\":99097},{\"end\":99115,\"start\":99109},{\"end\":99117,\"start\":99116},{\"end\":99469,\"start\":99465},{\"end\":99484,\"start\":99478},{\"end\":99788,\"start\":99784},{\"end\":99807,\"start\":99796},{\"end\":99810,\"start\":99808},{\"end\":99822,\"start\":99815},{\"end\":99846,\"start\":99835},{\"end\":100120,\"start\":100114},{\"end\":100138,\"start\":100132},{\"end\":100153,\"start\":100147},{\"end\":100162,\"start\":100158},{\"end\":100164,\"start\":100163},{\"end\":100406,\"start\":100405},{\"end\":100704,\"start\":100699},{\"end\":100718,\"start\":100712},{\"end\":100733,\"start\":100723},{\"end\":101045,\"start\":101040},{\"end\":101056,\"start\":101051},{\"end\":101069,\"start\":101062},{\"end\":101081,\"start\":101075},{\"end\":101447,\"start\":101443},{\"end\":101463,\"start\":101455},{\"end\":101478,\"start\":101474},{\"end\":101480,\"start\":101479},{\"end\":101985,\"start\":101975},{\"end\":102003,\"start\":101993},{\"end\":102019,\"start\":102013},{\"end\":102037,\"start\":102029},{\"end\":102053,\"start\":102047},{\"end\":102405,\"start\":102402},{\"end\":102417,\"start\":102412},{\"end\":102428,\"start\":102427},{\"end\":102440,\"start\":102436},{\"end\":102832,\"start\":102828},{\"end\":102853,\"start\":102845},{\"end\":103038,\"start\":103033},{\"end\":103055,\"start\":103049},{\"end\":103295,\"start\":103288},{\"end\":103308,\"start\":103303},{\"end\":103325,\"start\":103317},{\"end\":103339,\"start\":103334},{\"end\":103692,\"start\":103688},{\"end\":103704,\"start\":103700},{\"end\":103719,\"start\":103713},{\"end\":103735,\"start\":103728},{\"end\":103998,\"start\":103995},{\"end\":104008,\"start\":104005},{\"end\":104022,\"start\":104015},{\"end\":104037,\"start\":104031},{\"end\":104045,\"start\":104042},{\"end\":104052,\"start\":104050},{\"end\":104415,\"start\":104411},{\"end\":104425,\"start\":104420},{\"end\":104438,\"start\":104435},{\"end\":104574,\"start\":104570},{\"end\":104593,\"start\":104587},{\"end\":104798,\"start\":104791},{\"end\":104818,\"start\":104814},{\"end\":104836,\"start\":104831},{\"end\":104851,\"start\":104846},{\"end\":104864,\"start\":104859},{\"end\":105149,\"start\":105142},{\"end\":105161,\"start\":105160},{\"end\":105172,\"start\":105168},{\"end\":105430,\"start\":105429},{\"end\":105445,\"start\":105438},{\"end\":105457,\"start\":105453},{\"end\":105736,\"start\":105728},{\"end\":105756,\"start\":105750},{\"end\":105771,\"start\":105765},{\"end\":105790,\"start\":105783},{\"end\":105806,\"start\":105800},{\"end\":106175,\"start\":106168},{\"end\":106187,\"start\":106183},{\"end\":106202,\"start\":106196},{\"end\":106218,\"start\":106210},{\"end\":106231,\"start\":106226},{\"end\":106550,\"start\":106544},{\"end\":106567,\"start\":106562},{\"end\":106784,\"start\":106780},{\"end\":106796,\"start\":106793},{\"end\":106813,\"start\":106804},{\"end\":106825,\"start\":106821},{\"end\":106838,\"start\":106833},{\"end\":106856,\"start\":106849},{\"end\":106871,\"start\":106865},{\"end\":106887,\"start\":106881},{\"end\":106900,\"start\":106893},{\"end\":106917,\"start\":106913},{\"end\":107356,\"start\":107351},{\"end\":107371,\"start\":107366},{\"end\":107395,\"start\":107383},{\"end\":107412,\"start\":107404},{\"end\":107758,\"start\":107753},{\"end\":107770,\"start\":107766},{\"end\":107788,\"start\":107784},{\"end\":107805,\"start\":107796},{\"end\":108017,\"start\":108012},{\"end\":108030,\"start\":108025},{\"end\":108043,\"start\":108039},{\"end\":108060,\"start\":108054},{\"end\":108078,\"start\":108072},{\"end\":108095,\"start\":108091},{\"end\":108110,\"start\":108104}]", "bib_author_last_name": "[{\"end\":72025,\"start\":72021},{\"end\":72039,\"start\":72033},{\"end\":72055,\"start\":72048},{\"end\":72069,\"start\":72066},{\"end\":72077,\"start\":72075},{\"end\":72092,\"start\":72088},{\"end\":72107,\"start\":72101},{\"end\":72127,\"start\":72119},{\"end\":72471,\"start\":72462},{\"end\":72486,\"start\":72479},{\"end\":72500,\"start\":72497},{\"end\":72520,\"start\":72512},{\"end\":72788,\"start\":72784},{\"end\":72802,\"start\":72799},{\"end\":72817,\"start\":72810},{\"end\":72836,\"start\":72827},{\"end\":72856,\"start\":72848},{\"end\":72873,\"start\":72863},{\"end\":72889,\"start\":72882},{\"end\":72903,\"start\":72897},{\"end\":72918,\"start\":72912},{\"end\":73247,\"start\":73243},{\"end\":73271,\"start\":73256},{\"end\":73286,\"start\":73279},{\"end\":73300,\"start\":73296},{\"end\":73316,\"start\":73308},{\"end\":73336,\"start\":73328},{\"end\":73351,\"start\":73346},{\"end\":73844,\"start\":73840},{\"end\":73853,\"start\":73851},{\"end\":73866,\"start\":73862},{\"end\":74136,\"start\":74133},{\"end\":74156,\"start\":74148},{\"end\":74171,\"start\":74165},{\"end\":74186,\"start\":74179},{\"end\":74201,\"start\":74196},{\"end\":74215,\"start\":74210},{\"end\":74231,\"start\":74223},{\"end\":74246,\"start\":74238},{\"end\":74259,\"start\":74254},{\"end\":74273,\"start\":74266},{\"end\":75025,\"start\":75018},{\"end\":75282,\"start\":75274},{\"end\":75297,\"start\":75290},{\"end\":75313,\"start\":75309},{\"end\":75324,\"start\":75320},{\"end\":75335,\"start\":75326},{\"end\":75363,\"start\":75341},{\"end\":75368,\"start\":75365},{\"end\":75719,\"start\":75717},{\"end\":75729,\"start\":75725},{\"end\":75935,\"start\":75930},{\"end\":75949,\"start\":75943},{\"end\":75959,\"start\":75957},{\"end\":75977,\"start\":75968},{\"end\":75990,\"start\":75986},{\"end\":76008,\"start\":75998},{\"end\":76024,\"start\":76019},{\"end\":76035,\"start\":76028},{\"end\":76052,\"start\":76045},{\"end\":76067,\"start\":76060},{\"end\":76079,\"start\":76069},{\"end\":76382,\"start\":76375},{\"end\":76399,\"start\":76392},{\"end\":76414,\"start\":76409},{\"end\":76431,\"start\":76425},{\"end\":76445,\"start\":76439},{\"end\":76909,\"start\":76898},{\"end\":76923,\"start\":76919},{\"end\":76935,\"start\":76932},{\"end\":76951,\"start\":76945},{\"end\":76969,\"start\":76960},{\"end\":77246,\"start\":77227},{\"end\":77267,\"start\":77256},{\"end\":77284,\"start\":77278},{\"end\":77290,\"start\":77286},{\"end\":77563,\"start\":77557},{\"end\":77578,\"start\":77572},{\"end\":77592,\"start\":77586},{\"end\":77774,\"start\":77770},{\"end\":77785,\"start\":77782},{\"end\":77792,\"start\":77790},{\"end\":77805,\"start\":77803},{\"end\":77816,\"start\":77812},{\"end\":78179,\"start\":78170},{\"end\":78197,\"start\":78188},{\"end\":78213,\"start\":78206},{\"end\":78576,\"start\":78572},{\"end\":78585,\"start\":78583},{\"end\":78597,\"start\":78595},{\"end\":78943,\"start\":78936},{\"end\":78959,\"start\":78952},{\"end\":78975,\"start\":78970},{\"end\":79239,\"start\":79235},{\"end\":79250,\"start\":79247},{\"end\":79257,\"start\":79252},{\"end\":79605,\"start\":79602},{\"end\":79617,\"start\":79615},{\"end\":79631,\"start\":79626},{\"end\":79646,\"start\":79643},{\"end\":79657,\"start\":79653},{\"end\":79671,\"start\":79667},{\"end\":79686,\"start\":79681},{\"end\":79937,\"start\":79934},{\"end\":79951,\"start\":79945},{\"end\":79967,\"start\":79962},{\"end\":80191,\"start\":80188},{\"end\":80517,\"start\":80514},{\"end\":80536,\"start\":80530},{\"end\":80552,\"start\":80547},{\"end\":80838,\"start\":80832},{\"end\":80852,\"start\":80846},{\"end\":80867,\"start\":80859},{\"end\":81198,\"start\":81190},{\"end\":81215,\"start\":81209},{\"end\":81228,\"start\":81221},{\"end\":81247,\"start\":81238},{\"end\":81273,\"start\":81256},{\"end\":81287,\"start\":81282},{\"end\":81301,\"start\":81295},{\"end\":81313,\"start\":81308},{\"end\":81319,\"start\":81315},{\"end\":81813,\"start\":81810},{\"end\":81828,\"start\":81821},{\"end\":81852,\"start\":81837},{\"end\":81871,\"start\":81864},{\"end\":82110,\"start\":82107},{\"end\":82122,\"start\":82112},{\"end\":82500,\"start\":82493},{\"end\":82518,\"start\":82511},{\"end\":82532,\"start\":82526},{\"end\":82830,\"start\":82822},{\"end\":82848,\"start\":82840},{\"end\":82860,\"start\":82857},{\"end\":82877,\"start\":82867},{\"end\":82901,\"start\":82889},{\"end\":82908,\"start\":82903},{\"end\":83341,\"start\":83312},{\"end\":83355,\"start\":83350},{\"end\":83365,\"start\":83357},{\"end\":83822,\"start\":83815},{\"end\":83836,\"start\":83830},{\"end\":84150,\"start\":84148},{\"end\":84172,\"start\":84160},{\"end\":84188,\"start\":84184},{\"end\":84472,\"start\":84465},{\"end\":84490,\"start\":84480},{\"end\":84503,\"start\":84498},{\"end\":84773,\"start\":84756},{\"end\":84787,\"start\":84781},{\"end\":84798,\"start\":84792},{\"end\":84805,\"start\":84800},{\"end\":85066,\"start\":85058},{\"end\":85266,\"start\":85259},{\"end\":85458,\"start\":85451},{\"end\":85608,\"start\":85604},{\"end\":85618,\"start\":85615},{\"end\":85633,\"start\":85629},{\"end\":85647,\"start\":85643},{\"end\":85913,\"start\":85911},{\"end\":85930,\"start\":85920},{\"end\":85947,\"start\":85938},{\"end\":85963,\"start\":85958},{\"end\":86310,\"start\":86303},{\"end\":86327,\"start\":86320},{\"end\":86336,\"start\":86331},{\"end\":86346,\"start\":86344},{\"end\":86364,\"start\":86355},{\"end\":86386,\"start\":86371},{\"end\":86393,\"start\":86388},{\"end\":86714,\"start\":86712},{\"end\":86732,\"start\":86723},{\"end\":86751,\"start\":86741},{\"end\":87039,\"start\":87022},{\"end\":87053,\"start\":87048},{\"end\":87066,\"start\":87060},{\"end\":87075,\"start\":87068},{\"end\":87304,\"start\":87296},{\"end\":87320,\"start\":87313},{\"end\":87340,\"start\":87331},{\"end\":87354,\"start\":87350},{\"end\":87369,\"start\":87361},{\"end\":87386,\"start\":87380},{\"end\":87400,\"start\":87394},{\"end\":87415,\"start\":87408},{\"end\":87434,\"start\":87425},{\"end\":87453,\"start\":87436},{\"end\":87809,\"start\":87807},{\"end\":87824,\"start\":87819},{\"end\":87837,\"start\":87833},{\"end\":87850,\"start\":87846},{\"end\":87864,\"start\":87859},{\"end\":88182,\"start\":88174},{\"end\":88199,\"start\":88193},{\"end\":88212,\"start\":88205},{\"end\":88231,\"start\":88222},{\"end\":88257,\"start\":88240},{\"end\":88271,\"start\":88266},{\"end\":88285,\"start\":88279},{\"end\":88297,\"start\":88292},{\"end\":88303,\"start\":88299},{\"end\":88604,\"start\":88595},{\"end\":88620,\"start\":88613},{\"end\":88634,\"start\":88629},{\"end\":88655,\"start\":88645},{\"end\":89057,\"start\":89053},{\"end\":89076,\"start\":89067},{\"end\":89088,\"start\":89085},{\"end\":89113,\"start\":89099},{\"end\":89131,\"start\":89123},{\"end\":89382,\"start\":89377},{\"end\":89396,\"start\":89391},{\"end\":89403,\"start\":89398},{\"end\":89585,\"start\":89580},{\"end\":89596,\"start\":89589},{\"end\":89611,\"start\":89605},{\"end\":89619,\"start\":89613},{\"end\":89650,\"start\":89628},{\"end\":89657,\"start\":89652},{\"end\":90085,\"start\":90074},{\"end\":90104,\"start\":90096},{\"end\":90120,\"start\":90112},{\"end\":90412,\"start\":90390},{\"end\":90425,\"start\":90419},{\"end\":90432,\"start\":90427},{\"end\":90443,\"start\":90436},{\"end\":90461,\"start\":90455},{\"end\":90473,\"start\":90470},{\"end\":90483,\"start\":90480},{\"end\":90496,\"start\":90492},{\"end\":90503,\"start\":90498},{\"end\":90989,\"start\":90979},{\"end\":91005,\"start\":90996},{\"end\":91020,\"start\":91015},{\"end\":91310,\"start\":91305},{\"end\":91322,\"start\":91316},{\"end\":91578,\"start\":91562},{\"end\":91587,\"start\":91580},{\"end\":91598,\"start\":91591},{\"end\":91619,\"start\":91612},{\"end\":91637,\"start\":91631},{\"end\":91648,\"start\":91639},{\"end\":92033,\"start\":92030},{\"end\":92055,\"start\":92043},{\"end\":92071,\"start\":92067},{\"end\":92483,\"start\":92479},{\"end\":92497,\"start\":92494},{\"end\":92509,\"start\":92505},{\"end\":92521,\"start\":92519},{\"end\":92538,\"start\":92529},{\"end\":92552,\"start\":92547},{\"end\":92559,\"start\":92556},{\"end\":92565,\"start\":92561},{\"end\":92890,\"start\":92887},{\"end\":92904,\"start\":92898},{\"end\":92919,\"start\":92913},{\"end\":92936,\"start\":92928},{\"end\":93212,\"start\":93205},{\"end\":93227,\"start\":93222},{\"end\":93240,\"start\":93237},{\"end\":93487,\"start\":93485},{\"end\":93501,\"start\":93496},{\"end\":93520,\"start\":93512},{\"end\":93535,\"start\":93530},{\"end\":93548,\"start\":93545},{\"end\":93828,\"start\":93825},{\"end\":94102,\"start\":94096},{\"end\":94115,\"start\":94112},{\"end\":94129,\"start\":94122},{\"end\":94442,\"start\":94439},{\"end\":94455,\"start\":94452},{\"end\":94762,\"start\":94759},{\"end\":94775,\"start\":94772},{\"end\":95010,\"start\":95006},{\"end\":95019,\"start\":95017},{\"end\":95031,\"start\":95029},{\"end\":95354,\"start\":95352},{\"end\":95370,\"start\":95364},{\"end\":95858,\"start\":95856},{\"end\":95874,\"start\":95868},{\"end\":96293,\"start\":96290},{\"end\":96306,\"start\":96302},{\"end\":96326,\"start\":96320},{\"end\":96342,\"start\":96334},{\"end\":96614,\"start\":96611},{\"end\":96627,\"start\":96623},{\"end\":96647,\"start\":96636},{\"end\":96663,\"start\":96655},{\"end\":97111,\"start\":97101},{\"end\":97138,\"start\":97120},{\"end\":97149,\"start\":97140},{\"end\":97552,\"start\":97544},{\"end\":97570,\"start\":97561},{\"end\":97580,\"start\":97578},{\"end\":97809,\"start\":97805},{\"end\":97822,\"start\":97818},{\"end\":97839,\"start\":97832},{\"end\":97870,\"start\":97865},{\"end\":98264,\"start\":98252},{\"end\":98278,\"start\":98275},{\"end\":98292,\"start\":98288},{\"end\":98298,\"start\":98294},{\"end\":98618,\"start\":98612},{\"end\":98631,\"start\":98627},{\"end\":98648,\"start\":98641},{\"end\":98673,\"start\":98659},{\"end\":99062,\"start\":99054},{\"end\":99077,\"start\":99072},{\"end\":99095,\"start\":99086},{\"end\":99107,\"start\":99101},{\"end\":99122,\"start\":99118},{\"end\":99476,\"start\":99470},{\"end\":99491,\"start\":99485},{\"end\":99794,\"start\":99789},{\"end\":99813,\"start\":99811},{\"end\":99833,\"start\":99823},{\"end\":99849,\"start\":99847},{\"end\":100130,\"start\":100121},{\"end\":100145,\"start\":100139},{\"end\":100156,\"start\":100154},{\"end\":100170,\"start\":100165},{\"end\":100416,\"start\":100407},{\"end\":100423,\"start\":100418},{\"end\":100710,\"start\":100705},{\"end\":100721,\"start\":100719},{\"end\":100741,\"start\":100734},{\"end\":101049,\"start\":101046},{\"end\":101060,\"start\":101057},{\"end\":101073,\"start\":101070},{\"end\":101086,\"start\":101082},{\"end\":101453,\"start\":101448},{\"end\":101472,\"start\":101464},{\"end\":101486,\"start\":101481},{\"end\":101991,\"start\":101986},{\"end\":102011,\"start\":102004},{\"end\":102027,\"start\":102020},{\"end\":102045,\"start\":102038},{\"end\":102059,\"start\":102054},{\"end\":102410,\"start\":102406},{\"end\":102425,\"start\":102418},{\"end\":102434,\"start\":102429},{\"end\":102445,\"start\":102441},{\"end\":102453,\"start\":102447},{\"end\":102467,\"start\":102455},{\"end\":102843,\"start\":102833},{\"end\":102860,\"start\":102854},{\"end\":103047,\"start\":103039},{\"end\":103065,\"start\":103056},{\"end\":103301,\"start\":103296},{\"end\":103315,\"start\":103309},{\"end\":103332,\"start\":103326},{\"end\":103350,\"start\":103340},{\"end\":103698,\"start\":103693},{\"end\":103711,\"start\":103705},{\"end\":103726,\"start\":103720},{\"end\":103743,\"start\":103736},{\"end\":104003,\"start\":103999},{\"end\":104013,\"start\":104009},{\"end\":104029,\"start\":104023},{\"end\":104040,\"start\":104038},{\"end\":104048,\"start\":104046},{\"end\":104060,\"start\":104053},{\"end\":104418,\"start\":104416},{\"end\":104433,\"start\":104426},{\"end\":104444,\"start\":104439},{\"end\":104585,\"start\":104575},{\"end\":104605,\"start\":104594},{\"end\":104812,\"start\":104799},{\"end\":104829,\"start\":104819},{\"end\":104844,\"start\":104837},{\"end\":104857,\"start\":104852},{\"end\":104869,\"start\":104865},{\"end\":105158,\"start\":105150},{\"end\":105166,\"start\":105162},{\"end\":105179,\"start\":105173},{\"end\":105190,\"start\":105181},{\"end\":105436,\"start\":105431},{\"end\":105451,\"start\":105446},{\"end\":105463,\"start\":105458},{\"end\":105469,\"start\":105465},{\"end\":105748,\"start\":105737},{\"end\":105763,\"start\":105757},{\"end\":105781,\"start\":105772},{\"end\":105798,\"start\":105791},{\"end\":105813,\"start\":105807},{\"end\":106181,\"start\":106176},{\"end\":106194,\"start\":106188},{\"end\":106208,\"start\":106203},{\"end\":106224,\"start\":106219},{\"end\":106239,\"start\":106232},{\"end\":106560,\"start\":106551},{\"end\":106577,\"start\":106568},{\"end\":106791,\"start\":106785},{\"end\":106802,\"start\":106797},{\"end\":106819,\"start\":106814},{\"end\":106831,\"start\":106826},{\"end\":106847,\"start\":106839},{\"end\":106863,\"start\":106857},{\"end\":106879,\"start\":106872},{\"end\":106891,\"start\":106888},{\"end\":106911,\"start\":106901},{\"end\":106924,\"start\":106918},{\"end\":107364,\"start\":107357},{\"end\":107381,\"start\":107372},{\"end\":107402,\"start\":107396},{\"end\":107418,\"start\":107413},{\"end\":107764,\"start\":107759},{\"end\":107782,\"start\":107771},{\"end\":107794,\"start\":107789},{\"end\":107813,\"start\":107806},{\"end\":108023,\"start\":108018},{\"end\":108037,\"start\":108031},{\"end\":108052,\"start\":108044},{\"end\":108070,\"start\":108061},{\"end\":108089,\"start\":108079},{\"end\":108102,\"start\":108096},{\"end\":108118,\"start\":108111}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1812.02903\",\"id\":\"b0\"},\"end\":72390,\"start\":71940},{\"attributes\":{\"doi\":\"arXiv:1906.04329\",\"id\":\"b1\"},\"end\":72724,\"start\":72392},{\"attributes\":{\"doi\":\"arXiv:1811.03604\",\"id\":\"b2\"},\"end\":73186,\"start\":72726},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":203902309},\"end\":73771,\"start\":73188},{\"attributes\":{\"doi\":\"arXiv:2005.05083\",\"id\":\"b4\"},\"end\":74035,\"start\":73773},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209414652},\"end\":74676,\"start\":74037},{\"attributes\":{\"id\":\"b6\"},\"end\":74941,\"start\":74678},{\"attributes\":{\"id\":\"b7\"},\"end\":75188,\"start\":74943},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3679574},\"end\":75664,\"start\":75190},{\"attributes\":{\"doi\":\"arXiv:1911.06270\",\"id\":\"b9\"},\"end\":75867,\"start\":75666},{\"attributes\":{\"doi\":\"arXiv:2003.08119\",\"id\":\"b10\"},\"end\":76367,\"start\":75869},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b11\"},\"end\":76853,\"start\":76369},{\"attributes\":{\"doi\":\"arXiv:1807.00459\",\"id\":\"b12\"},\"end\":77159,\"start\":76855},{\"attributes\":{\"doi\":\"arXiv:1811.12470\",\"id\":\"b13\"},\"end\":77495,\"start\":77161},{\"attributes\":{\"doi\":\"arXiv:1206.6389\",\"id\":\"b14\"},\"end\":77761,\"start\":77497},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b15\"},\"end\":78091,\"start\":77763},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":28527385},\"end\":78469,\"start\":78093},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":58534983},\"end\":78895,\"start\":78471},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":55899582},\"end\":79168,\"start\":78897},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13193974},\"end\":79591,\"start\":79170},{\"attributes\":{\"id\":\"b20\"},\"end\":79827,\"start\":79593},{\"attributes\":{\"doi\":\"arXiv:1903.06996\",\"id\":\"b21\"},\"end\":80181,\"start\":79829},{\"attributes\":{\"doi\":\"arXiv:1903.07020\",\"id\":\"b22\"},\"end\":80423,\"start\":80183},{\"attributes\":{\"doi\":\"arXiv:1805.10032\",\"id\":\"b23\"},\"end\":80755,\"start\":80425},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":62841522},\"end\":81112,\"start\":80757},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3833774},\"end\":81801,\"start\":81114},{\"attributes\":{\"doi\":\"arXiv:1911.07963\",\"id\":\"b26\"},\"end\":82103,\"start\":81803},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b27\"},\"end\":82384,\"start\":82105},{\"attributes\":{\"doi\":\"arXiv:1802.00420\",\"id\":\"b28\"},\"end\":82754,\"start\":82386},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7004303},\"end\":83231,\"start\":82756},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":12387176},\"end\":83750,\"start\":83233},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2893830},\"end\":84058,\"start\":83752},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b32\"},\"end\":84392,\"start\":84060},{\"attributes\":{\"doi\":\"arXiv:1811.00741\",\"id\":\"b33\"},\"end\":84688,\"start\":84394},{\"attributes\":{\"doi\":\"arXiv:1710.06963\",\"id\":\"b34\"},\"end\":84996,\"start\":84690},{\"attributes\":{\"id\":\"b35\"},\"end\":85163,\"start\":84998},{\"attributes\":{\"id\":\"b36\"},\"end\":85402,\"start\":85165},{\"attributes\":{\"id\":\"b37\"},\"end\":85542,\"start\":85404},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":219878182},\"end\":85904,\"start\":85544},{\"attributes\":{\"doi\":\"arXiv:1908.07873\",\"id\":\"b39\"},\"end\":86224,\"start\":85906},{\"attributes\":{\"doi\":\"arXiv:1610.05492\",\"id\":\"b40\"},\"end\":86637,\"start\":86226},{\"attributes\":{\"doi\":\"arXiv:2004.08546\",\"id\":\"b41\"},\"end\":86939,\"start\":86639},{\"attributes\":{\"doi\":\"arXiv:1602.05629\",\"id\":\"b42\"},\"end\":87288,\"start\":86941},{\"attributes\":{\"doi\":\"arXiv:1902.01046\",\"id\":\"b43\"},\"end\":87799,\"start\":87290},{\"attributes\":{\"doi\":\"arXiv:1907.02189\",\"id\":\"b44\"},\"end\":88095,\"start\":87801},{\"attributes\":{\"doi\":\"arXiv:1611.04482\",\"id\":\"b45\"},\"end\":88585,\"start\":88097},{\"attributes\":{\"doi\":\"arXiv:1905.12022\",\"id\":\"b46\"},\"end\":89001,\"start\":88587},{\"attributes\":{\"doi\":\"arXiv:2002.06440\",\"id\":\"b47\"},\"end\":89335,\"start\":89003},{\"attributes\":{\"doi\":\"arXiv:1910.05653\",\"id\":\"b48\"},\"end\":89543,\"start\":89337},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":623013},\"end\":90011,\"start\":89545},{\"attributes\":{\"doi\":\"arXiv:1809.03474\",\"id\":\"b50\"},\"end\":90303,\"start\":90013},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":1553099},\"end\":90924,\"start\":90305},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":35426171},\"end\":91249,\"start\":90926},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3330313},\"end\":91486,\"start\":91251},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":131024},\"end\":91946,\"start\":91488},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":44096776},\"end\":92391,\"start\":91948},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":67846878},\"end\":92877,\"start\":92393},{\"attributes\":{\"doi\":\"arXiv:1801.05927\",\"id\":\"b57\"},\"end\":93135,\"start\":92879},{\"attributes\":{\"doi\":\"arXiv:1810.06175\",\"id\":\"b58\"},\"end\":93419,\"start\":93137},{\"attributes\":{\"doi\":\"arXiv:1802.08946\",\"id\":\"b59\"},\"end\":93751,\"start\":93421},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":7593961},\"end\":94034,\"start\":93753},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":11538109},\"end\":94346,\"start\":94036},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":9746839},\"end\":94691,\"start\":94348},{\"attributes\":{\"id\":\"b63\"},\"end\":94903,\"start\":94693},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":58534983},\"end\":95260,\"start\":94905},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":42701481},\"end\":95790,\"start\":95262},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":28867534},\"end\":96203,\"start\":95792},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":49208440},\"end\":96604,\"start\":96205},{\"attributes\":{\"doi\":\"arXiv:1803.01498\",\"id\":\"b68\"},\"end\":96944,\"start\":96606},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":131763369},\"end\":97499,\"start\":96946},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":4312831},\"end\":97794,\"start\":97501},{\"attributes\":{\"doi\":\"arXiv:1803.09877\",\"id\":\"b71\"},\"end\":98153,\"start\":97796},{\"attributes\":{\"doi\":\"arXiv:1910.06093\",\"id\":\"b72\"},\"end\":98517,\"start\":98155},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":198968245},\"end\":98966,\"start\":98519},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":8954326},\"end\":99413,\"start\":98968},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":7248464},\"end\":99782,\"start\":99415},{\"attributes\":{\"doi\":\"arXiv:1606.07365\",\"id\":\"b76\"},\"end\":100070,\"start\":99784},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":7885987},\"end\":100403,\"start\":100072},{\"attributes\":{\"doi\":\"arXiv:1805.09767\",\"id\":\"b78\"},\"end\":100612,\"start\":100405},{\"attributes\":{\"doi\":\"arXiv:1706.02690\",\"id\":\"b79\"},\"end\":100944,\"start\":100614},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":49667948},\"end\":101361,\"start\":100946},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":13900194},\"end\":101910,\"start\":101363},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":3488815},\"end\":102330,\"start\":101912},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":516928},\"end\":102771,\"start\":102332},{\"attributes\":{\"id\":\"b84\"},\"end\":102963,\"start\":102773},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b85\"},\"end\":103238,\"start\":102965},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":30587588},\"end\":103629,\"start\":103240},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":14542261},\"end\":103940,\"start\":103631},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":57246310},\"end\":104349,\"start\":103942},{\"attributes\":{\"id\":\"b89\"},\"end\":104544,\"start\":104351},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":1915014},\"end\":104734,\"start\":104546},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":85567158},\"end\":105097,\"start\":104736},{\"attributes\":{\"doi\":\"arXiv:1912.13445\",\"id\":\"b92\"},\"end\":105356,\"start\":105099},{\"attributes\":{\"doi\":\"arXiv:1712.07557\",\"id\":\"b93\"},\"end\":105655,\"start\":105358},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":9697434},\"end\":106102,\"start\":105657},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":6212000},\"end\":106518,\"start\":106104},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b96\"},\"end\":106709,\"start\":106520},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":202786778},\"end\":107277,\"start\":106711},{\"attributes\":{\"id\":\"b98\"},\"end\":107690,\"start\":107279},{\"attributes\":{\"doi\":\"arXiv:1910.00189\",\"id\":\"b99\"},\"end\":108010,\"start\":107692},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b100\"},\"end\":108476,\"start\":108012}]", "bib_title": "[{\"end\":73232,\"start\":73188},{\"end\":74122,\"start\":74037},{\"end\":75270,\"start\":75190},{\"end\":78163,\"start\":78093},{\"end\":78563,\"start\":78471},{\"end\":78927,\"start\":78897},{\"end\":79229,\"start\":79170},{\"end\":80824,\"start\":80757},{\"end\":81182,\"start\":81114},{\"end\":82812,\"start\":82756},{\"end\":83300,\"start\":83233},{\"end\":83804,\"start\":83752},{\"end\":85596,\"start\":85544},{\"end\":89573,\"start\":89545},{\"end\":90381,\"start\":90305},{\"end\":90971,\"start\":90926},{\"end\":91294,\"start\":91251},{\"end\":91552,\"start\":91488},{\"end\":92023,\"start\":91948},{\"end\":92471,\"start\":92393},{\"end\":93817,\"start\":93753},{\"end\":94088,\"start\":94036},{\"end\":94431,\"start\":94348},{\"end\":94997,\"start\":94905},{\"end\":95345,\"start\":95262},{\"end\":95849,\"start\":95792},{\"end\":96283,\"start\":96205},{\"end\":97090,\"start\":96946},{\"end\":97538,\"start\":97501},{\"end\":98601,\"start\":98519},{\"end\":99047,\"start\":98968},{\"end\":99463,\"start\":99415},{\"end\":100112,\"start\":100072},{\"end\":101038,\"start\":100946},{\"end\":101441,\"start\":101363},{\"end\":101973,\"start\":101912},{\"end\":102400,\"start\":102332},{\"end\":103286,\"start\":103240},{\"end\":103686,\"start\":103631},{\"end\":103993,\"start\":103942},{\"end\":104568,\"start\":104546},{\"end\":104789,\"start\":104736},{\"end\":105726,\"start\":105657},{\"end\":106166,\"start\":106104},{\"end\":106778,\"start\":106711},{\"end\":107349,\"start\":107279}]", "bib_author": "[{\"end\":72027,\"start\":72013},{\"end\":72041,\"start\":72027},{\"end\":72057,\"start\":72041},{\"end\":72071,\"start\":72057},{\"end\":72079,\"start\":72071},{\"end\":72094,\"start\":72079},{\"end\":72109,\"start\":72094},{\"end\":72129,\"start\":72109},{\"end\":72473,\"start\":72454},{\"end\":72488,\"start\":72473},{\"end\":72502,\"start\":72488},{\"end\":72522,\"start\":72502},{\"end\":72790,\"start\":72777},{\"end\":72804,\"start\":72790},{\"end\":72819,\"start\":72804},{\"end\":72838,\"start\":72819},{\"end\":72858,\"start\":72838},{\"end\":72875,\"start\":72858},{\"end\":72891,\"start\":72875},{\"end\":72905,\"start\":72891},{\"end\":72920,\"start\":72905},{\"end\":73249,\"start\":73234},{\"end\":73273,\"start\":73249},{\"end\":73288,\"start\":73273},{\"end\":73302,\"start\":73288},{\"end\":73318,\"start\":73302},{\"end\":73338,\"start\":73318},{\"end\":73353,\"start\":73338},{\"end\":73846,\"start\":73832},{\"end\":73855,\"start\":73846},{\"end\":73868,\"start\":73855},{\"end\":74138,\"start\":74124},{\"end\":74158,\"start\":74138},{\"end\":74173,\"start\":74158},{\"end\":74188,\"start\":74173},{\"end\":74203,\"start\":74188},{\"end\":74217,\"start\":74203},{\"end\":74233,\"start\":74217},{\"end\":74248,\"start\":74233},{\"end\":74261,\"start\":74248},{\"end\":74275,\"start\":74261},{\"end\":75027,\"start\":75012},{\"end\":75284,\"start\":75272},{\"end\":75299,\"start\":75284},{\"end\":75315,\"start\":75299},{\"end\":75326,\"start\":75315},{\"end\":75337,\"start\":75326},{\"end\":75365,\"start\":75337},{\"end\":75370,\"start\":75365},{\"end\":75721,\"start\":75713},{\"end\":75731,\"start\":75721},{\"end\":75937,\"start\":75923},{\"end\":75951,\"start\":75937},{\"end\":75961,\"start\":75951},{\"end\":75979,\"start\":75961},{\"end\":75992,\"start\":75979},{\"end\":76010,\"start\":75992},{\"end\":76026,\"start\":76010},{\"end\":76037,\"start\":76026},{\"end\":76054,\"start\":76037},{\"end\":76069,\"start\":76054},{\"end\":76081,\"start\":76069},{\"end\":76384,\"start\":76369},{\"end\":76401,\"start\":76384},{\"end\":76416,\"start\":76401},{\"end\":76433,\"start\":76416},{\"end\":76447,\"start\":76433},{\"end\":76911,\"start\":76891},{\"end\":76925,\"start\":76911},{\"end\":76937,\"start\":76925},{\"end\":76953,\"start\":76937},{\"end\":76971,\"start\":76953},{\"end\":77248,\"start\":77219},{\"end\":77269,\"start\":77248},{\"end\":77286,\"start\":77269},{\"end\":77292,\"start\":77286},{\"end\":77565,\"start\":77548},{\"end\":77580,\"start\":77565},{\"end\":77594,\"start\":77580},{\"end\":77776,\"start\":77763},{\"end\":77787,\"start\":77776},{\"end\":77794,\"start\":77787},{\"end\":77807,\"start\":77794},{\"end\":77818,\"start\":77807},{\"end\":78181,\"start\":78165},{\"end\":78199,\"start\":78181},{\"end\":78215,\"start\":78199},{\"end\":78578,\"start\":78565},{\"end\":78587,\"start\":78578},{\"end\":78599,\"start\":78587},{\"end\":78945,\"start\":78929},{\"end\":78961,\"start\":78945},{\"end\":78977,\"start\":78961},{\"end\":79241,\"start\":79231},{\"end\":79252,\"start\":79241},{\"end\":79259,\"start\":79252},{\"end\":79607,\"start\":79595},{\"end\":79619,\"start\":79607},{\"end\":79633,\"start\":79619},{\"end\":79648,\"start\":79633},{\"end\":79659,\"start\":79648},{\"end\":79673,\"start\":79659},{\"end\":79688,\"start\":79673},{\"end\":79939,\"start\":79929},{\"end\":79953,\"start\":79939},{\"end\":79969,\"start\":79953},{\"end\":80193,\"start\":80183},{\"end\":80519,\"start\":80509},{\"end\":80538,\"start\":80519},{\"end\":80554,\"start\":80538},{\"end\":80840,\"start\":80826},{\"end\":80854,\"start\":80840},{\"end\":80869,\"start\":80854},{\"end\":81200,\"start\":81184},{\"end\":81217,\"start\":81200},{\"end\":81230,\"start\":81217},{\"end\":81249,\"start\":81230},{\"end\":81275,\"start\":81249},{\"end\":81289,\"start\":81275},{\"end\":81303,\"start\":81289},{\"end\":81315,\"start\":81303},{\"end\":81321,\"start\":81315},{\"end\":81815,\"start\":81803},{\"end\":81830,\"start\":81815},{\"end\":81854,\"start\":81830},{\"end\":81873,\"start\":81854},{\"end\":82112,\"start\":82105},{\"end\":82124,\"start\":82112},{\"end\":82502,\"start\":82487},{\"end\":82520,\"start\":82502},{\"end\":82534,\"start\":82520},{\"end\":82832,\"start\":82814},{\"end\":82850,\"start\":82832},{\"end\":82862,\"start\":82850},{\"end\":82879,\"start\":82862},{\"end\":82903,\"start\":82879},{\"end\":82910,\"start\":82903},{\"end\":83343,\"start\":83302},{\"end\":83357,\"start\":83343},{\"end\":83367,\"start\":83357},{\"end\":83824,\"start\":83806},{\"end\":83838,\"start\":83824},{\"end\":84152,\"start\":84141},{\"end\":84174,\"start\":84152},{\"end\":84190,\"start\":84174},{\"end\":84474,\"start\":84460},{\"end\":84492,\"start\":84474},{\"end\":84505,\"start\":84492},{\"end\":84775,\"start\":84749},{\"end\":84789,\"start\":84775},{\"end\":84800,\"start\":84789},{\"end\":84807,\"start\":84800},{\"end\":85068,\"start\":85054},{\"end\":85268,\"start\":85252},{\"end\":85460,\"start\":85446},{\"end\":85610,\"start\":85598},{\"end\":85620,\"start\":85610},{\"end\":85635,\"start\":85620},{\"end\":85649,\"start\":85635},{\"end\":85915,\"start\":85906},{\"end\":85932,\"start\":85915},{\"end\":85949,\"start\":85932},{\"end\":85965,\"start\":85949},{\"end\":86312,\"start\":86297},{\"end\":86329,\"start\":86312},{\"end\":86338,\"start\":86329},{\"end\":86348,\"start\":86338},{\"end\":86366,\"start\":86348},{\"end\":86388,\"start\":86366},{\"end\":86395,\"start\":86388},{\"end\":86716,\"start\":86703},{\"end\":86734,\"start\":86716},{\"end\":86753,\"start\":86734},{\"end\":87041,\"start\":87016},{\"end\":87055,\"start\":87041},{\"end\":87068,\"start\":87055},{\"end\":87077,\"start\":87068},{\"end\":87306,\"start\":87290},{\"end\":87322,\"start\":87306},{\"end\":87342,\"start\":87322},{\"end\":87356,\"start\":87342},{\"end\":87371,\"start\":87356},{\"end\":87388,\"start\":87371},{\"end\":87402,\"start\":87388},{\"end\":87417,\"start\":87402},{\"end\":87436,\"start\":87417},{\"end\":87455,\"start\":87436},{\"end\":87811,\"start\":87801},{\"end\":87826,\"start\":87811},{\"end\":87839,\"start\":87826},{\"end\":87852,\"start\":87839},{\"end\":87866,\"start\":87852},{\"end\":88184,\"start\":88168},{\"end\":88201,\"start\":88184},{\"end\":88214,\"start\":88201},{\"end\":88233,\"start\":88214},{\"end\":88259,\"start\":88233},{\"end\":88273,\"start\":88259},{\"end\":88287,\"start\":88273},{\"end\":88299,\"start\":88287},{\"end\":88305,\"start\":88299},{\"end\":88606,\"start\":88587},{\"end\":88622,\"start\":88606},{\"end\":88636,\"start\":88622},{\"end\":88657,\"start\":88636},{\"end\":89059,\"start\":89046},{\"end\":89078,\"start\":89059},{\"end\":89090,\"start\":89078},{\"end\":89115,\"start\":89090},{\"end\":89133,\"start\":89115},{\"end\":89384,\"start\":89373},{\"end\":89398,\"start\":89384},{\"end\":89405,\"start\":89398},{\"end\":89587,\"start\":89575},{\"end\":89598,\"start\":89587},{\"end\":89613,\"start\":89598},{\"end\":89621,\"start\":89613},{\"end\":89652,\"start\":89621},{\"end\":89659,\"start\":89652},{\"end\":90087,\"start\":90068},{\"end\":90106,\"start\":90087},{\"end\":90122,\"start\":90106},{\"end\":90414,\"start\":90383},{\"end\":90427,\"start\":90414},{\"end\":90434,\"start\":90427},{\"end\":90445,\"start\":90434},{\"end\":90463,\"start\":90445},{\"end\":90475,\"start\":90463},{\"end\":90485,\"start\":90475},{\"end\":90498,\"start\":90485},{\"end\":90505,\"start\":90498},{\"end\":90991,\"start\":90973},{\"end\":91007,\"start\":90991},{\"end\":91022,\"start\":91007},{\"end\":91312,\"start\":91296},{\"end\":91324,\"start\":91312},{\"end\":91580,\"start\":91554},{\"end\":91589,\"start\":91580},{\"end\":91600,\"start\":91589},{\"end\":91621,\"start\":91600},{\"end\":91639,\"start\":91621},{\"end\":91650,\"start\":91639},{\"end\":92035,\"start\":92025},{\"end\":92057,\"start\":92035},{\"end\":92073,\"start\":92057},{\"end\":92485,\"start\":92473},{\"end\":92499,\"start\":92485},{\"end\":92511,\"start\":92499},{\"end\":92523,\"start\":92511},{\"end\":92540,\"start\":92523},{\"end\":92554,\"start\":92540},{\"end\":92561,\"start\":92554},{\"end\":92567,\"start\":92561},{\"end\":92892,\"start\":92879},{\"end\":92906,\"start\":92892},{\"end\":92921,\"start\":92906},{\"end\":92938,\"start\":92921},{\"end\":93214,\"start\":93197},{\"end\":93229,\"start\":93214},{\"end\":93242,\"start\":93229},{\"end\":93489,\"start\":93479},{\"end\":93503,\"start\":93489},{\"end\":93522,\"start\":93503},{\"end\":93537,\"start\":93522},{\"end\":93550,\"start\":93537},{\"end\":93830,\"start\":93819},{\"end\":94104,\"start\":94090},{\"end\":94117,\"start\":94104},{\"end\":94131,\"start\":94117},{\"end\":94444,\"start\":94433},{\"end\":94457,\"start\":94444},{\"end\":94764,\"start\":94753},{\"end\":94777,\"start\":94764},{\"end\":95012,\"start\":94999},{\"end\":95021,\"start\":95012},{\"end\":95033,\"start\":95021},{\"end\":95356,\"start\":95347},{\"end\":95372,\"start\":95356},{\"end\":95860,\"start\":95851},{\"end\":95876,\"start\":95860},{\"end\":96295,\"start\":96285},{\"end\":96308,\"start\":96295},{\"end\":96328,\"start\":96308},{\"end\":96344,\"start\":96328},{\"end\":96616,\"start\":96606},{\"end\":96629,\"start\":96616},{\"end\":96649,\"start\":96629},{\"end\":96665,\"start\":96649},{\"end\":97113,\"start\":97092},{\"end\":97140,\"start\":97113},{\"end\":97151,\"start\":97140},{\"end\":97554,\"start\":97540},{\"end\":97572,\"start\":97554},{\"end\":97582,\"start\":97572},{\"end\":97811,\"start\":97796},{\"end\":97824,\"start\":97811},{\"end\":97841,\"start\":97824},{\"end\":97872,\"start\":97841},{\"end\":98266,\"start\":98243},{\"end\":98280,\"start\":98266},{\"end\":98294,\"start\":98280},{\"end\":98300,\"start\":98294},{\"end\":98620,\"start\":98603},{\"end\":98633,\"start\":98620},{\"end\":98650,\"start\":98633},{\"end\":98675,\"start\":98650},{\"end\":99064,\"start\":99049},{\"end\":99079,\"start\":99064},{\"end\":99097,\"start\":99079},{\"end\":99109,\"start\":99097},{\"end\":99124,\"start\":99109},{\"end\":99478,\"start\":99465},{\"end\":99493,\"start\":99478},{\"end\":99796,\"start\":99784},{\"end\":99815,\"start\":99796},{\"end\":99835,\"start\":99815},{\"end\":99851,\"start\":99835},{\"end\":100132,\"start\":100114},{\"end\":100147,\"start\":100132},{\"end\":100158,\"start\":100147},{\"end\":100172,\"start\":100158},{\"end\":100418,\"start\":100405},{\"end\":100425,\"start\":100418},{\"end\":100712,\"start\":100699},{\"end\":100723,\"start\":100712},{\"end\":100743,\"start\":100723},{\"end\":101051,\"start\":101040},{\"end\":101062,\"start\":101051},{\"end\":101075,\"start\":101062},{\"end\":101088,\"start\":101075},{\"end\":101455,\"start\":101443},{\"end\":101474,\"start\":101455},{\"end\":101488,\"start\":101474},{\"end\":101993,\"start\":101975},{\"end\":102013,\"start\":101993},{\"end\":102029,\"start\":102013},{\"end\":102047,\"start\":102029},{\"end\":102061,\"start\":102047},{\"end\":102412,\"start\":102402},{\"end\":102427,\"start\":102412},{\"end\":102436,\"start\":102427},{\"end\":102447,\"start\":102436},{\"end\":102455,\"start\":102447},{\"end\":102469,\"start\":102455},{\"end\":102845,\"start\":102828},{\"end\":102862,\"start\":102845},{\"end\":103049,\"start\":103033},{\"end\":103067,\"start\":103049},{\"end\":103303,\"start\":103288},{\"end\":103317,\"start\":103303},{\"end\":103334,\"start\":103317},{\"end\":103352,\"start\":103334},{\"end\":103700,\"start\":103688},{\"end\":103713,\"start\":103700},{\"end\":103728,\"start\":103713},{\"end\":103745,\"start\":103728},{\"end\":104005,\"start\":103995},{\"end\":104015,\"start\":104005},{\"end\":104031,\"start\":104015},{\"end\":104042,\"start\":104031},{\"end\":104050,\"start\":104042},{\"end\":104062,\"start\":104050},{\"end\":104420,\"start\":104411},{\"end\":104435,\"start\":104420},{\"end\":104446,\"start\":104435},{\"end\":104587,\"start\":104570},{\"end\":104607,\"start\":104587},{\"end\":104814,\"start\":104791},{\"end\":104831,\"start\":104814},{\"end\":104846,\"start\":104831},{\"end\":104859,\"start\":104846},{\"end\":104871,\"start\":104859},{\"end\":105160,\"start\":105142},{\"end\":105168,\"start\":105160},{\"end\":105181,\"start\":105168},{\"end\":105192,\"start\":105181},{\"end\":105438,\"start\":105429},{\"end\":105453,\"start\":105438},{\"end\":105465,\"start\":105453},{\"end\":105471,\"start\":105465},{\"end\":105750,\"start\":105728},{\"end\":105765,\"start\":105750},{\"end\":105783,\"start\":105765},{\"end\":105800,\"start\":105783},{\"end\":105815,\"start\":105800},{\"end\":106183,\"start\":106168},{\"end\":106196,\"start\":106183},{\"end\":106210,\"start\":106196},{\"end\":106226,\"start\":106210},{\"end\":106241,\"start\":106226},{\"end\":106562,\"start\":106544},{\"end\":106579,\"start\":106562},{\"end\":106793,\"start\":106780},{\"end\":106804,\"start\":106793},{\"end\":106821,\"start\":106804},{\"end\":106833,\"start\":106821},{\"end\":106849,\"start\":106833},{\"end\":106865,\"start\":106849},{\"end\":106881,\"start\":106865},{\"end\":106893,\"start\":106881},{\"end\":106913,\"start\":106893},{\"end\":106926,\"start\":106913},{\"end\":107366,\"start\":107351},{\"end\":107383,\"start\":107366},{\"end\":107404,\"start\":107383},{\"end\":107420,\"start\":107404},{\"end\":107766,\"start\":107753},{\"end\":107784,\"start\":107766},{\"end\":107796,\"start\":107784},{\"end\":107815,\"start\":107796},{\"end\":108025,\"start\":108012},{\"end\":108039,\"start\":108025},{\"end\":108054,\"start\":108039},{\"end\":108072,\"start\":108054},{\"end\":108091,\"start\":108072},{\"end\":108104,\"start\":108091},{\"end\":108120,\"start\":108104}]", "bib_venue": "[{\"end\":73510,\"start\":73440},{\"end\":76577,\"start\":76513},{\"end\":79382,\"start\":79329},{\"end\":81478,\"start\":81408},{\"end\":83508,\"start\":83446},{\"end\":89796,\"start\":89736},{\"end\":90630,\"start\":90576},{\"end\":95472,\"start\":95468},{\"end\":95969,\"start\":95932},{\"end\":101567,\"start\":101546},{\"end\":72011,\"start\":71940},{\"end\":72452,\"start\":72392},{\"end\":72775,\"start\":72726},{\"end\":73438,\"start\":73353},{\"end\":73830,\"start\":73773},{\"end\":74342,\"start\":74275},{\"end\":74729,\"start\":74678},{\"end\":75010,\"start\":74943},{\"end\":75414,\"start\":75370},{\"end\":75711,\"start\":75666},{\"end\":75921,\"start\":75869},{\"end\":76511,\"start\":76463},{\"end\":76889,\"start\":76855},{\"end\":77217,\"start\":77161},{\"end\":77546,\"start\":77497},{\"end\":77905,\"start\":77834},{\"end\":78264,\"start\":78215},{\"end\":78670,\"start\":78599},{\"end\":79017,\"start\":78977},{\"end\":79327,\"start\":79259},{\"end\":79927,\"start\":79829},{\"end\":80283,\"start\":80209},{\"end\":80507,\"start\":80425},{\"end\":80918,\"start\":80869},{\"end\":81406,\"start\":81321},{\"end\":81931,\"start\":81889},{\"end\":82225,\"start\":82139},{\"end\":82485,\"start\":82386},{\"end\":82972,\"start\":82910},{\"end\":83444,\"start\":83367},{\"end\":83886,\"start\":83838},{\"end\":84139,\"start\":84060},{\"end\":84458,\"start\":84394},{\"end\":84747,\"start\":84690},{\"end\":85052,\"start\":84998},{\"end\":85250,\"start\":85165},{\"end\":85444,\"start\":85404},{\"end\":85710,\"start\":85649},{\"end\":86043,\"start\":85981},{\"end\":86295,\"start\":86226},{\"end\":86701,\"start\":86639},{\"end\":87014,\"start\":86941},{\"end\":87521,\"start\":87471},{\"end\":87926,\"start\":87882},{\"end\":88166,\"start\":88097},{\"end\":88774,\"start\":88673},{\"end\":89044,\"start\":89003},{\"end\":89371,\"start\":89337},{\"end\":89734,\"start\":89659},{\"end\":90066,\"start\":90013},{\"end\":90574,\"start\":90505},{\"end\":91071,\"start\":91022},{\"end\":91354,\"start\":91324},{\"end\":91693,\"start\":91650},{\"end\":92145,\"start\":92073},{\"end\":92615,\"start\":92567},{\"end\":92985,\"start\":92954},{\"end\":93195,\"start\":93137},{\"end\":93477,\"start\":93421},{\"end\":93879,\"start\":93830},{\"end\":94183,\"start\":94131},{\"end\":94512,\"start\":94457},{\"end\":94751,\"start\":94693},{\"end\":95067,\"start\":95033},{\"end\":95430,\"start\":95372},{\"end\":95897,\"start\":95876},{\"end\":96388,\"start\":96344},{\"end\":96753,\"start\":96681},{\"end\":97205,\"start\":97151},{\"end\":97631,\"start\":97582},{\"end\":97952,\"start\":97888},{\"end\":98241,\"start\":98155},{\"end\":98724,\"start\":98675},{\"end\":99173,\"start\":99124},{\"end\":99576,\"start\":99493},{\"end\":99905,\"start\":99867},{\"end\":100221,\"start\":100172},{\"end\":100489,\"start\":100441},{\"end\":100697,\"start\":100614},{\"end\":101137,\"start\":101088},{\"end\":101544,\"start\":101488},{\"end\":102113,\"start\":102061},{\"end\":102524,\"start\":102469},{\"end\":102826,\"start\":102773},{\"end\":103031,\"start\":102965},{\"end\":103413,\"start\":103352},{\"end\":103768,\"start\":103745},{\"end\":104125,\"start\":104062},{\"end\":104409,\"start\":104351},{\"end\":104625,\"start\":104607},{\"end\":104904,\"start\":104871},{\"end\":105140,\"start\":105099},{\"end\":105427,\"start\":105358},{\"end\":105864,\"start\":105815},{\"end\":106303,\"start\":106241},{\"end\":106542,\"start\":106520},{\"end\":106975,\"start\":106926},{\"end\":107477,\"start\":107420},{\"end\":107751,\"start\":107692},{\"end\":108224,\"start\":108136}]"}}}, "year": 2023, "month": 12, "day": 17}