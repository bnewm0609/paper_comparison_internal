{"id": 13750928, "updated": "2023-09-30 10:17:20.711", "metadata": {"title": "Towards Fast Computation of Certified Robustness for ReLU Networks", "authors": "[{\"first\":\"Tsui-Wei\",\"last\":\"Weng\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Hongge\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Zhao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Cho-Jui\",\"last\":\"Hsieh\",\"middle\":[]},{\"first\":\"Duane\",\"last\":\"Boning\",\"middle\":[]},{\"first\":\"Inderjit\",\"last\":\"Dhillon\",\"middle\":[\"S.\"]},{\"first\":\"Luca\",\"last\":\"Daniel\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 4, "day": 25}, "abstract": "Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer CAV17]. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or delivering low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms Fast-Lin and Fast-Lip that are able to certify non-trivial lower bounds of minimum distortions, by bounding the ReLU units with appropriate linear functions Fast-Lin, or by bounding the local Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods deliver bounds close to (the gap is 2-3X) exact minimum distortion found by Reluplex in small MNIST networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core. In addition, we show that, in fact, there is no polynomial time algorithm that can approximately find the minimum $\\ell_1$ adversarial distortion of a ReLU network with a $0.99\\ln n$ approximation ratio unless $\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1804.09699", "mag": "2963440492", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/WengZCSHDBD18", "doi": null}}, "content": {"source": {"pdf_hash": "60eeb35b9f035cee7c9c82c5956d1fb008451a4b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1804.09699v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fd4a54cc3983cd6e7cfc2a84d02f49c084532d92", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/60eeb35b9f035cee7c9c82c5956d1fb008451a4b.txt", "contents": "\nTowards Fast Computation of Certified Robustness for ReLU Networks\n2018\n\nTsui-Wei Weng \nEqual contribution\n\n\nMassachusetts Institute of Technology\nCambridgeMA\n\nHuan Zhang \nEqual contribution\n\n\nDavis, DavisCA\n\nHongge Chen \nMassachusetts Institute of Technology\nCambridgeMA\n\nZhao Song \nHarvard University\nCam-bridgeMA\n\nAustin, AustinTX\n\nCho-Jui Hsieh \nDavis, DavisCA\n\nDuane Boning \nMassachusetts Institute of Technology\nCambridgeMA\n\nInderjit S Dhillon \nAustin, AustinTX\n\nLuca Daniel \nMassachusetts Institute of Technology\nCambridgeMA\n\nTowards Fast Computation of Certified Robustness for ReLU Networks\n\nProceedings of the 35 th International Conference on Machine Learning\nthe 35 th International Conference on Machine LearningStockholm, Sweden2018Source code is available at https://github.com/huanzhang12/CertifiedReLURobustness. Corre-spondence to: Tsui-Wei Weng <twweng@mit.edu>, Huan Zhang\nVerifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NPcomplete problem. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or deliver low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms (Fast-Lin,Fast-Lip) that are able to certify non-trivial lower bounds of minimum adversarial distortions. Experiments show that (1) our methods deliver bounds close to (the gap is 2-3X) exact minimum distortions found by Reluplex in small networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core. In addition, we show that there is no polynomial time algorithm that can approximately find the minimum 1 adversarial distortion of a ReLU network with a 0.99 ln n approximation ratio unless NP=P, where n is the number of neurons in the network.\n\nIntroduction\n\nSince the discovery of adversarial examples in deep neural network (DNN) image classifiers (Szegedy et al., 2013), researchers have successfully found adversarial examples in many machine learning tasks applied to different areas, including object detection (Xie et al., 2017), image captioning (Chen et al., 2018a), speech recognition (Cisse et al., 2017), malware detection (Wang et al., 2017) and reading comprehension (Jia & Liang, 2017). Moreover, black-box attacks have also been shown to be possible, where an attacker can find adversarial examples without knowing the architecture and parameters of the DNN (Chen et al., 2017;Papernot et al., 2017;Liu et al., 2017b).\n\nThe existence of adversarial examples poses a huge threat to the application of DNNs in mission-critical tasks including security cameras, self-driving cars and aircraft control systems. Many researchers have thus proposed defensive or detection methods in order to increase the robustness of DNNs. Notable examples are defensive distillation (Papernot et al., 2016), adversarial retraining/training (Kurakin et al., 2017;Madry et al., 2018) and model ensembles (Tram\u00e8r et al., 2018;Liu et al., 2017a). Despite many published contributions that aim at increasing the robustness of DNNs, theoretical results are rarely given and there is no guarantee that the proposed defensive methods can reliably improve the robustness. Indeed, many of these defensive mechanism have been shown to be ineffective when more advanced attacks are used (Carlini & Wagner, 2017c;a;b;He et al., 2017).\n\nThe robustness of a DNN can be verified by examining a neighborhood (e.g. 2 or \u221e ball) near a data point x 0 . The idea is to find the largest ball with radius r 0 that guarantees no points inside the neighborhood can ever change classifier decision. Typically, r 0 can be found as follows: given R, a global optimization algorithm can be used to find an adversarial example within this ball, and thus bisection on R can produce r 0 . Reluplex (Katz et al., 2017) is one example using such a technique but it is computationally infeasible even on a small MNIST classifier. In general, verifying the robustness property of a ReLU network is NP-complete (Katz et al., 2017;Sinha et al., 2018).\n\nOn the other hand, a lower bound \u03b2 L of radius r 0 can be given, which guarantees that no examples within a ball of ra-arXiv:1804.09699v4 [stat.ML] 2 Oct 2018 dius \u03b2 L can ever change the network classification outcome. (Hein & Andriushchenko, 2017) is a pioneering work on giving such a lower bound for neural networks that are continuously differentiable, although only a 2-layer MLP network with differentiable activations is investigated. (Weng et al., 2018) has extended theoretical result to ReLU activation functions and proposed a robustness score, CLEVER, based on extreme value theory. Their approach is feasible for large state-of-the-art DNNs but CLEVER is an estimate of \u03b2 L without certificates. Ideally, we would like to obtain a certified (which guarantees that \u03b2 L \u2264 r 0 ) and non-trivial (a trivial \u03b2 L is 0) lower bound \u03b2 L that is reasonably close to r 0 within reasonable amount of computational time.\n\nIn this paper, we develop two fast algorithms for obtaining a tight and certified lower bound \u03b2 L on ReLU networks. In addition, we also provide a complementary theoretical result to (Katz et al., 2017;Sinha et al., 2018) by further showing there does not even exist a polynomial time algorithm that can approximately find the minimum adversarial distortion with a 0.99 ln n approximation ratio. Our contributions are:\n\n\u2022 We fully exploit the ReLU networks to give two computationally efficient methods of computing tighter and guaranteed robustness lower bounds via (1) linear approximation on the ReLU units (see Sec 3.3, Algorithm 1, Fast-Lin) and\n\n(2) bounding network local Lipschitz constant (see Sec 3.4, Algorithm 2, Fast-Lip). Unlike the per-layer operator-normbased lower bounds which are often very loose (close to 0, as verified in our experiments) for deep networks, our bounds are much closer to the upper bound given by the best adversarial examples, and thus can be used to evaluate the robustness of DNNs with theoretical guarantee.\n\n\u2022 We show that our proposed method is at least four orders of magnitude faster than finding the exact minimum distortion (with Reluplex), and also around two orders of magnitude (or more) faster than linear programming (LP) based methods. We can compute a reasonable robustness lower bound within a minute for a ReLU network with up to 7 layers or over ten thousands neurons, which is so far the best available result in the literature to our best knowledge. \u2022 We show that there is no polynomial time algorithm that can find a lower bound of minimum 1 adversarial distortion with a (1 \u2212 o(1)) ln n approximation ratio (where n is the total number of neurons) unless NP=P (see Theorem 3.1).\n\n\nBackground and related work 2.1. Solving the minimum adversarial distortion\n\nFor ReLU networks, the verification problem can be transformed into a Mixed Integer Linear Programming (MILP) problem (Lomuscio & Maganti, 2017;Cheng et al., 2017;Fischetti & Jo, 2017) by using binary variables to encode the states of ReLU activation in each neuron. (Katz et al., 2017) proposed Reluplex based on satisfiable modulo theory, which encodes the network into a set of linear constraints with special rules to handle ReLU activations and splits the problem into two LP problems based on a ReLU's activation status on demand. Similarly, (Ehlers, 2017) proposed Planet, another splitting-based approach using satisfiability (SAT) solvers. These approaches guarantee to find the exact minimum distortion of an adversarial example, and can be used for formal verification. However, due to NP-hard nature of the underlying problem, these approaches only work on very small networks. For example, in (Katz et al., 2017), verifying a feed-forward network with 5 inputs, 5 outputs and 300 total hidden neurons on a single data point can take a few hours. Additionally, Reluplex can find the minimum distortion only in terms of \u221e norm ( 1 is possible via an extension) and cannot easily generalize to p norm. (Szegedy et al., 2013) gives a lower bound on the minimum distortion in ReLU networks by computing the product of weight matrices operator norms, but this bound is usually too loose to be useful in practice, as pointed out in (Hein & Andriushchenko, 2017) and verified in our experiments (see Table F.1). A tighter bound was given by (Hein & Andriushchenko, 2017) using local Lipschitz constant on a network with one hidden layer, but their approach requires the network to be continuously-differentiable, and thus cannot be directly applied to ReLU networks. (Weng et al., 2018) further provide the lower bound guarantee to non-differentiable functions by Lipschitz continuity assumption and propose the first robustness score, CLEVER, that can evaluate the robustness of DNNs and scale to large ImageNet networks. As also shown in our experiments in Section 4, the CLEVER score is indeed a good robustness estimate close to the true minimum distortion given by Reluplex, albeit without providing certificates. Recently, (Wong & Kolter, 2018) propose a convex relaxation on the MILP verification problem discussed in Sec 2.1, which reduces MILP to LP when the adversarial distortion is in \u221e norm. They focus on adversarial training, and compute layer-wise bounds by looking into the dual LP problem.\n\n\nComputing lower bounds of minimum distortion\n\n\nHardness and approximation algorithms\n\nNP = P is the most important and popular assumption in computational complexity in the last several decades. It can be used to show that the decision of the exact case of a problem is hard. However, in several cases, solving one problem approximately is much easier than solving it exactly. For example, there is no polynomial time algorithm to solve the MAX-CUT problem, but there is a simple 0.5approximation polynomial time algorithm. Previous works (Katz et al., 2017;Sinha et al., 2018) show that there is no polynomial time algorithm to find the minimum adversarial distortion r 0 exactly. A natural question to ask is: does there exist a polynomial time algorithm to solve the robustness problem approximately? In other words, can we give a lower bound of r 0 with a guaranteed approximation ratio?\n\nFrom another perspective, NP = P only rules out the polynomial running time. Some problems might not even have a sub-exponential time algorithm. To rule out that, the most well-known assumption used is the \"Exponential Time Hypothesis\" (Impagliazzo et al., 1998). The hypothesis states that 3SAT cannot be solved in sub-exponential time in the worst case. Another example is that while tensor rank calculation is NP-hard (H\u00e5stad, 1990), a recent work (Song et al., 2017b) proved that there is no 2 o(n 1\u2212o(1) ) time algorithm to give a constant approximation of the rank of the tensor. There are also some stronger versions of the hypothesis than ETH, e.g., Strong ETH (Impagliazzo & Paturi, 2001), Gap ETH (Dinur, 2016Manurangsi & Raghavendra, 2017), and average case ETH (Feige, 2002;Razenshteyn et al., 2016).\n\n\nRobustness guarantees for ReLU networks\n\nOverview of our results. We begin with a motivating theorem in Sec 3.1 showing that there does NOT exist a polynomial time algorithm able to find the minimum adversarial distortion with a (1 \u2212 o(1)) ln n approximation ratio. We then introduce notations in Sec 3.2 and state our main results in Sec 3.3 and 3.4, where we develop two approaches that guarantee to obtain a lower bound of minimum adversarial distortion. In Sec 3.3, we first demonstrate a general approach to directly derive the output bounds of a ReLU network with linear approximations when inputs are perturbed by a general p norm noise. The analytic output bounds allow us to develop a fast algorithm Fast-Lin to compute certified lower bound. In Sec 3.4, we present Fast-Lip to obtain a certified lower bound of minimum distortion by deriving upper bounds for the local Lipschitz constant. Both methods are highly efficient and allow fast computation of certified lower bounds on large ReLU networks.\n\n3.1. Finding the minimum distortion with a 0.99 ln n approximation ratio is hard (Katz et al., 2017) shows that verifying robustness for ReLU networks is NP-complete; in other words, there is no efficient (polynomial time) algorithm to find the exact minimum adversarial distortion. Here, we further show that even approximately finding the minimum adversarial distortion with a guaranteed approximation ratio can be hard. Suppose the p norm of the true minimum adversarial distortion is r 0 , and a robustness verification program A gives a guarantee that no adversarial examples exist within an p ball of radius r (r is a lower bound of r 0 ). The approximation ratio \u03b1 := r0 r > 1. We hope that \u03b1 is close to 1 with a guarantee; for example, if \u03b1 is a constant regardless of the scale of the network, we can always be sure that r 0 is at most \u03b1 times as large as the lower bound r found by A. Here we relax this requirement and allow the approximation ratio to increase with the number of neurons n. In other words, when n is larger, the approximation becomes more inaccurate, but this \"inaccuracy\" can be bounded. However, the following theorem shows that no efficient algorithms exist to give a 0.99 ln n approximation in the special case of 1 robustness: Theorem 3.1. Unless P = NP, there is no polynomial time algorithm that gives (1 \u2212 o(1)) ln n-approximation to the 1 ReLU robustness verification problem with n neurons.\n\nOur proof is based on a well-known in-approximability result of SET-COVER problem (Raz & Safra, 1997;Alon et al., 2006;Dinur & Steurer, 2014) and a novel reduction from SET-COVER to our problem. We defer the proof into Appendix A. The formal definition of the 1 ReLU robustness verification problem can be found in Definition A.7. Theorem 3.1 implies that any efficient (polynomial time) algorithm cannot give better than (1 \u2212 o(1)) ln napproximation guarantee. Moreover, by making a stronger assumption of Exponential Time Hypothesis (ETH), we can state an explicit result about running time using existing results from SET-COVER (Moshkovitz, 2012a;b), Corollary 3.2. Under ETH, there is no 2 o(n c ) time algorithm that gives (1 \u2212 o(1)) ln n-approximation to the\n\n\nReLU Networks and Their Activation Patterns\n\nLet x \u2208 R n0 be the input vector for an m-layer neural network with m \u2212 1 hidden layers and let the number of neurons in each layer be n k , \u2200k \u2208 [m]. We use [n] to denote set {1, 2, \u00b7 \u00b7 \u00b7 , n}. The weight matrix W (k) and bias vector b (k) for the k-th layer have dimension n k \u00d7 n k\u22121 and n k , respectively. Let \u03c6 k : R n0 \u2192 R n k be the operator mapping from input layer to layer k and \u03c3(y) be the coordinatewise activation function; for each k \u2208 [m \u2212 1], the relation between layer k \u2212 1 and layer k can be written as\n\u03c6 k (x) = \u03c3(W (k) \u03c6 k\u22121 (x)+b (k) ), where W (k) \u2208 R n k \u00d7n k\u22121 , b (k) \u2208 R n k .\nFor the input layer and the output layer, we have\n\u03c6 0 (x) = x and \u03c6 m (x) = W (m) \u03c6 m\u22121 (x) + b (m) . The output of the neural network is f (x) = \u03c6 m (x)\n, which is a vector of length n m , and the j-th output is its j-th coordinate, denoted as f j (x) = [\u03c6 m (x)] j . For ReLU activation, the activation function \u03c3(y) = max(y, 0) is an element-wise operation on the input vector y.\n\nGiven an input data point x 0 \u2208 R n0 and a bounded pnorm perturbation \u2208 R + , the input x is constrained in an p ball B p (x 0 , ) := {x | x \u2212 x 0 p \u2264 }. With all possible perturbations in B p (x 0 , ), the pre-ReLU activation of each neuron has a lower and upper bound l \u2208 R and u \u2208 R, where l \u2264 u. Let us use l (k) r and u (k) r to de-note the lower and upper bound for the r-th neuron in the k-th layer, and let z (k) r be its pre-ReLU activation, where z\n(k) r = W (k) r,: \u03c6 k\u22121 (x) + b (k) r , l (k) r \u2264 z (k) r \u2264 u (k) r , and W (k)\nr,: is the r-th row of W (k) . There are three categories of possible activation patterns -(i) the neuron is always activated:\nI + k := {r \u2208 [n k ] | u (k) r \u2265 l (k) r \u2265 0}, (ii) the neuron is always inactivated: I \u2212 k := {r \u2208 [n k ] | l (k) r \u2264 u (k)\nr \u2264 0}, and (iii) the neuron could be either activated or inactivated: We start with a 2-layers network and then extend it to m layers. The j-th output of a 2-layer network is:\nI k := {r \u2208 [n k ] | l (k) r < 0 < u (k) r }. Obviously, {I + k , I \u2212 k , I k } is a partition of set [n k ].f j (x) = r\u2208I + 1 ,I \u2212 1 ,I1 W (2) j,r \u03c3(W (1) r,: x + b (1) r ) + b (2) j .\nFor neurons r \u2208 I + 1 , we have \u03c3(W (1) r ) = 0. For the neurons in category (iii), we propose to use the following linear upper bound and a linear lower bound to replace the ReLU activation \u03c3(y):\nu u \u2212 l y \u2264 \u03c3(y) \u2264 u u \u2212 l (y \u2212 l).(1)\nLet d\n\n(1)\nr := u (1) r u (1) r \u2212l (1) r , we have d (1) r (W (1) r,: x + b (1) r ) \u2264 \u03c3(W (1) r,: x + b (1) r ) (2) \u2264 d (1) r (W (1) r,: x + b (1) r \u2212 l (1) r ).\nTo obtain an upper bound and lower bound of f j (x) with (1), set d\n\n(1) r = 1 for r \u2208 I + 1 , and we have\nf U j (x) = r\u2208I + 1 ,I1 W (2) j,r d (1) r (W (1) r,: x + b (1) r ) (3) \u2212 r\u2208I1,W (2) j,r >0 W (2) j,r d (1) r l (1) r + b (2) j , f L j (x) = r\u2208I + 1 ,I1 W (2) j,r d (1) r (W (1) r,: x + b (1) r ) (4) \u2212 r\u2208I1,W (2) j,r <0 W (2) j,r d (1) r l (1) r + b (2) j , where f L j (x) \u2264 f j (x) \u2264 f U j (x)\n. To obtain f U j (x), we take the upper bound of \u03c3(W (1)\nr,: x + b (1) r ) for r \u2208 I 1 , W(2)\nj,r > 0 and its lower bound for r \u2208 I 1 , W\n\n(2) j,r \u2264 0. Both cases share a common term of d (1) r ), which is combined into the first summation term in (3) with r \u2208 I 1 . Similarly we get the bound for f L j (x). For a general m-layer ReLU network with the linear approximation (1), we will show in Theorem 3.5 that the network output can be bounded by two explicit functions when the input x is perturbed with a -bounded p noise. We start by defining the activation matrix D (k) and the additional equivalent bias terms T (k) and H (k) for the k-th layer in Definition 3.3 and the two explicit functions in 3.4.\nDefinition 3.3 (A (k) , T (k) , H (k) ). Given matrices W (k) \u2208 R n k \u00d7n k\u22121 and vectors b (k) \u2208 R n k , \u2200k \u2208 [m]. We define D (0) \u2208 R n0\u00d7n0 as an identity matrix. For each k \u2208 [m\u22121], we define matrix D (k) \u2208 R n k \u00d7n k as follows D (k) r,r = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 u (k) r u (k) r \u2212l (k) r if r \u2208 I k ; 1 if r \u2208 I + k ; 0 if r \u2208 I \u2212 k .(5)\nWe define matrix A (m\u22121) \u2208 R nm\u00d7nm\u22121 to be W (m) D (m\u22121) , and for each k \u2208 {m \u2212 1, m \u2212 2, \u00b7 \u00b7 \u00b7 , 1}, matrix A (k\u22121) \u2208 R nm\u00d7n k\u22121 is defined recursively as\nA (k\u22121) = A (k) W (k) D (k\u22121) . For each k \u2208 [m \u2212 1], we define matrices T (k) , H (k) \u2208 R n k \u00d7nm , where T (k) r,j = l (k) r if r \u2208 I k , A (k) j,r > 0; 0 otherwise . H (k) r,j = l (k) r if r \u2208 I k , A (k) j,r < 0; 0 otherwise .\nDefinition 3.4 (Two explicit functions : f U (\u00b7) and f L (\u00b7)). Let matrices A (k) , T (k) and H (k) be defined as in Definition 3.3. We define two functions f U , f L : R n0 \u2192 R nm as follows. For each input vector x \u2208 R n0 ,  Figure 1. Illustration of deriving output bounds for ReLU networks in Section 3.3. The final output upper bounds (f U j ) and lower bounds (f L j ) can be derived by considering the activation status of the neurons with input perturbation \u03b4 p \u2264 . For neurons in I + k , their outputs are identical to their inputs; for neurons in I \u2212 k , they can be removed during computation as their outputs are always zero; for neurons in I k , their outputs can be bounded by corresponding linear upper bounds and lower bounds considering the signs of associated weights.\nf U j (x) = A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j ), f L j (x) = A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 H(\nTheorem 3.5 (Explicit upper and lower bounds). Given an m-layer ReLU neural network function f : R n0 \u2192 R nm , there exists two explicit functions f L : R n0 \u2192 R nm and\nf U : R n0 \u2192 R nm (see Definition 3.4) such that \u2200j \u2208 [n m ], f L j (x) \u2264 f j (x) \u2264 f U j (x), \u2200x \u2208 B p (x 0 , ).\nThe proof of Theorem 3.5 is in Appendix B. Since the input x \u2208 B p (x 0 , ), we can maximize (3) and minimize (4) within this set to obtain a global upper and lower bound of f j (x), which has analytical solutions for any 1 \u2264 p \u2264 \u221e and the result is formally shown in Corollary 3.7 (proof in Appendix C). In other words, we have analytic bounds that can be computed efficiently without resorting to any optimization solvers for general p distortion, and this is the key to enable fast computation for layer-wise output bounds.\n\nWe first formally define the global upper bound \u03b3 U j and lower bound \u03b3 L j of f j (x), and then obtain Corollary 3.7. Definition 3.6 (\u03b3 L j , \u03b3 U j ). Given a point x 0 \u2208 R n0 , a neural network function f : R n0 \u2192 R nm , parameters p, . Let matrices A (k) , T (k) and H (k) , \u2200k \u2208 [m \u2212 1] be defined as in Definition 3.3. We define \u03b3 L j , \u03b3 U j , \u2200j \u2208 [n m ] as\n\u03b3 L j = \u00b5 \u2212 j + \u03bd j \u2212 A (0) j,: q and \u03b3 U j = \u00b5 + j + \u03bd j + A (0) j,: q ,\nwhere 1/p + 1/q = 1 and \u03bd j , \u00b5 + j , \u00b5 \u2212 j are defined as\n\u00b5 + j = \u2212 m\u22121 k=1 A (k) j,: T (k) :,j , \u00b5 \u2212 j = \u2212 m\u22121 k=1 A (k) j,: H (k) :,j(6)\u03bd j = A (0) j,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: b (k)(7)\nCorollary 3.7 (Two side bounds in closed-form). Given a point x 0 \u2208 R n0 , an m-layer neural network function f : R n0 \u2192 R nm , parameters p and . For each j \u2208 [n m ], there exist two fixed values \u03b3 L j and \u03b3 U j (see Definition 3.6) such that \u03b3 L j \u2264 f j (x) \u2264 \u03b3 U j , \u2200x \u2208 B p (x 0 , ).\n\n\nCOMPUTING PRE-ReLU ACTIVATION BOUNDS\n\nTheorem 3.5 and Corollary 3.7 give us a global lower bound \u03b3 L j and upper bound \u03b3 U j of the j-th neuron at the m-th layer if we know all the pre-ReLU activation bounds l (k) and u (k) , from layer 1 to m \u2212 1, as the construction of D (k) , H (k) and T (k) requires l (k) and u (k) (see Definition 3.3).\n\nHere, we show how this can be done easily and layer-bylayer. We start from m = 1 where\nA (0) = W (1) , f U (x) = f L (x) = A (0) x + b (1)\n. Then, we can apply Corollary 3.7 to get the output bounds of each neuron and set them as l (1) and u (1) . Then, we can proceed to m = 2 with l (1) and u (1) and compute the output bounds of second layer by Corollary 3.7 and set them as l (2) and u (2) . Repeating this procedure for all m \u2212 1 layers, we will get all the l (k) and u (k) needed to compute the output range of the m-th layer.\n\nNote that when computing l (k) and u (k) , the constructed W (k) D (k\u22121) can be saved and reused for bounding the next layer, which facilitates efficient implementations. Moreover, the time complexity of computing the output bounds of an m-layer ReLU network with Theorem 3.5 and Corollary 3.7 is polynomial time in contrast to the approaches in (Katz et al., 2017) and (Lomuscio & Maganti, 2017) where SMT solvers and MIO solvers have exponential time complexity. The major computation cost is to form A (0) for the m-th layer, which involves multiplications of layer weights in a similar cost of forward propagation. See the \"ComputeT-woSideBounds\" procedure in Algorithm 1 in Appendix D.\n\n\nDERIVING MAXIMUM CERTIFIED LOWER BOUNDS\n\n\nOF MINIMUM ADVERSARIAL DISTORTION\n\nSuppose c is the predicted class of the input data point x 0 and the class is j. With Theorem 3.5, the maximum possible lower bound for the targeted attacks j and un-targeted attacks are\nj = max s.t. \u03b3 L c ( ) \u2212 \u03b3 U j ( ) > 0 and = min j =c j .\nThough it is hard to get analytic forms of \u03b3 L c ( ) and \u03b3 U j ( ) in terms of , fortunately, we can still obtain j via a binary search. This is because Corollary 3.7 allows us to efficiently compute the numerical values of \u03b3 L c ( ) and \u03b3 U j ( ) given . It is worth noting that we can further improve the bound by considering g(x) := f c (x) \u2212 f j (x) at the last layer and apply the same procedure to compute the lower bound of g(x) (denoted as \u03b3 L ); this can be done easily by redefining the last layer's weights to be a row vectorw\n:= W (m) c,: \u2212 W (m)\nj,: . The corresponding maximum possible lower bound for the targeted attacks is j = max s.t. \u03b3 L ( ) > 0. We list our complete algorithm, Fast-Lin, in Appendix D.\n\n\nDISCUSSIONS\n\nWe have shown how to derive explicit output bounds of ReLU network (Theorem 3.5) with the proposed linear approximations and obtain analytical certified lower bounds (Corollary 3.7), which is the key of our proposed algorithm Fast-Lin. (Wong & Kolter, 2018) presents a similar algorithmic result on computing certified bounds, but our framework and theirs are entirely different -we use direct computation of layer-wise linear upper/lower bounds in Sec 3.3 with binary search on , while their results is achieved via the lens of dual LP formulation with Newton's method. Interestingly, when we choose a special set of lower and upper bounds as in (2) and they choose a special dual LP variable in their equation (8), the two different frameworks coincidentally produce the same procedure for computing layer-wise bounds (the \"ComputeTwoSideBounds\" procedure in Fast-Lin and Algorithm 1 in (Wong & Kolter, 2018)). However, our choice of bounds (2) is due to computation efficiency, while (Wong & Kolter, 2018) gives a quite different justification. We encourage the readers to read Appendix A.3 in their paper on the justifications for this specific selection of dual variables and understand this robustness verification problem from different perspectives.\n\n\nApproach 2 (Fast-Lip): Certified lower bounds via bounding the local Lipschitz constant\n\n(Weng et al., 2018) shows a non-trivial lower bound of minimum adversarial distortion for an input example x 0 in targeted attacks is min\ng(x 0 )/L j q,x0 , , where g(x) = f c (x) \u2212 f j (x), L j q,x0 is the local Lipschitz constant of g(x) in B p (x 0 , )\n, j is the target class, c is the original class, and 1/p + 1/q = 1. For un-targeted attacks, the lower bound can be presented in a similar form. (Weng et al., 2018) uses sampling techniques to estimate the local Lipschitz constant and compute an estimated lower bound without certificates.\n\nHere, we propose a new algorithm to compute a certified lower bound of the minimum adversarial distortion by upper bounding the local Lipschitz constant. To start with, let us rewrite the relations of subsequent layers in the following form:\n\u03c6 k (x) = \u039b (k) (W (k) \u03c6 k\u22121 (x) + b (k) ), where \u03c3(\u00b7)\nis replaced by the diagonal activation pattern matrix \u039b (k) that encodes the status of neurons r in k-th layer:\n\u039b (k) r,r = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 or 0 if r \u2208 I k 1 if r \u2208 I + k 0 if r \u2208 I \u2212 k(8)\nand \u039b (m) = I nm . With a slight abuse of notation, let us define \u039b (k) a as a diagonal activation matrix for neurons in the k-th layer who are always activated, i.e. the r-th diagonal is 1 if r \u2208 I + k and 0 otherwise, and \u039b (k) u as the diagonal activation matrix for k-th layer neurons whose status are uncertain, i.e. the r-th diagonal is 1 or 0 (to be determined) if r \u2208 I k , and 0 otherwise. Therefore, we have\n\u039b (k) = \u039b (k) a +\u039b (k) u . We can obtain \u039b (k) for x \u2208 B p (x 0 , )\nby applying Algorithm 1 and check the lower and upper bounds for each neuron r in layer k.\n3.4.1. A GENERAL UPPER BOUND OF LIPSCHITZ CONSTANT IN q NORM\nThe central idea is to compute upper bounds of L j q,x0 by exploiting the three categories of activation patterns in ReLU networks when the allowable inputs are in\nB p (x 0 , ). L j q,x0\ncan be defined as the maximum norm of directional derivative as shown in (Weng et al., 2018). For the ReLU network, the maximum directional derivative norm can be found by examining all the possible activation patterns and take the one (the worst-case) that results in the largest gradient norm. However, as all possible activation patterns grow exponentially with the number of the neurons, it is impossible to examine all of them in brute-force. Fortunately, computing the worst-case pattern on each element of \u2207g(\nx) (i.e. [\u2207g(x)] k , k \u2208 [n 0 ])\nis much easier and more efficient. In addition, we apply a simple fact that the maximum norm of a vector (which is \u2207g(x), x \u2208 B p (x 0 , ) in our case) is upper bounded by the norm of the maximum value for each components. By computing the worst-case pattern on [\u2207g(x)] k and its norm, we can obtain an upper bound of the local Lipschitz constant, which results in a certified lower bound of minimum distortion.\n\nBelow, we first show how to derive an upper bound of the Lipschitz constant by computing the worst-case activation pattern on [\u2207g(x)] k for 2 layers. Next, we will show how to apply it repeatedly for a general m-layer network, and the algorithm is named Fast-Lip. Note that for simplicity, we will use [\u2207f j (x)] k to illustrate our derivation; however, it is easy to extend to [\u2207g(\nx)] k as g(x) = f c (x) \u2212 f j (x) by simply replacing last layer weight vector by W (m) c,: \u2212 W (m) j,: .\nBounds for a 2-layer ReLU Network. The gradient is:\n[\u2207f j (x)] k = W (2) j,: \u039b (1) a W (1) :,k + W (2) j,: \u039b (1) u W(1)\n:,k .\n\nThe first term W\n\n(2)\nj,: \u039b (1) a W(1)\n:,k is a constant and all we need to bound is the second term W\n(2) j,: \u039b (1) u W (1) :,k . Let C (1) j,k = W (2) j,: \u039b (1) a W (1) :,k , L(1)\nj,k and U\n\n(1) j,k be the lower and upper bounds of the second term, we have\nL (1) j,k = i\u2208I1,W (2) j,i W (2) i,k <0 W (2) j,i W (2) i,k , U (1) j,k = i\u2208I1,W (2) j,i W (2) i,k >0 W (2) j,i W (2) i,k max x\u2208Bp(x0, ) |[\u2207f j (x)] k | \u2264 max(|C (1) j,k +L (1) j,k |, |C (1) j,k +U (1) j,k |).\nBounds for 3 layers or more. For 3 or more layers, we can apply the above 2-layer results recursively, layer-bylayer. For example, for a 3-layer ReLU network,\n[\u2207f j (x)] k = W (3) j,: \u039b (2) W (2) \u039b (1) W (1) :,k , if we let Y (1) :,k = W (2) \u039b (1) W(1)\n:,k , then [\u2207f j (x)] k is reduced to the following form that is similar to 2 layers:\n[\u2207f j (x)] k = W (3) j,: \u039b (2) Y (1) :,k (9) = W (3) j,: \u039b (2) a Y(1):,k + W (3) j,: \u039b (2) u Y (1) :,k(10)\nTo obtain the bound in (9), we first need to obtain a lower bound and upper bound of Y\n\n:,k , where we can directly apply the 2-layer results to get an upper and an lower bound for each component i as C\n(1) i,k +L (1) i,k \u2264 Y (1) i,k \u2264 C (1) i,k +U (1) i,k . Next, the first term W (3) j,: \u039b (2) a Y(1)\n:,k in (10) can be lower bounded and upper bounded respectively by\ni\u2208I + 2 W (3) j,i C (1) i,k + i\u2208I + 2 ,W (3) j,i >0 W (3) j,i L (1) i,k + i\u2208I + 2 ,W (3) j,i <0 W (3) j,i U (1) i,k (11) i\u2208I + 2 W (3) j,i C (1) i,k + i\u2208I + 2 ,W (3) j,i >0 W (3) j,i U (1) i,k + i\u2208I + 2 ,W (3) j,i <0 W (3) j,i L (1) i,k(12)\nwhereas the second term W\n\n(3)\nj,: \u039b (2) u Y (1) :,k in (10) is bounded by i\u2208P W (3) j,i (C (1) i,k + L (1) i,k ) + i\u2208Q W (3) j,i (C (1) i,k + U (1) i,k )\nwith lower/upper bound index sets P L , Q L and P U , Q U :\nP L = {i | i \u2208 I 2 , W (3) j,i > 0, C (1) i,k + L (1) i,k < 0}, Q L = {i | i \u2208 I 2 , W (3) j,i < 0, C (1) i,k + U (1) i,k > 0}; (13) P U = {i | i \u2208 I 2 , W (3) j,i < 0, C (1) i,k + L (1) i,k < 0}, Q U = {i | i \u2208 I 2 , W (3) j,i > 0, C (1) i,k + U (1) i,k > 0}. (14) Let C (2) j,k = i\u2208I + 2 W (3) j,i C (1) i,k , U (2) j,k +C (2) j,k and L (2) j,k +C (2) j,k be the upper and lower bound of [\u2207f j (x)] k , we have U (2) j,k +C (2) j,k = (12)+(14) and L (2) j,k +C (2) j,k = (11)+(13), max x\u2208Bp(x0, ) |[\u2207f j (x)] k |\u2264max(|L (2) j,k +C (2) j,k |, |U (2) j,k +C (2) j,k |).\nThus, this technique can be used iteratively to solve max x\u2208Bp(x0, ) |[\u2207f j (x)] k | for a general m-layer network, and we can easily bound any q norm of \u2207f j (x) by the q norm of the vector of maximum values. For example,\nmax x\u2208Bp(x0, ) \u2207f j (x) q \u2264 k ( max x\u2208Bp(x0, ) |[\u2207f j (x)] k |) q 1 q\nWe list our full procedure, Fast-Lip, in Appendix D.\n\nFurther speed-up. Note that in the 3-layer example, we compute the bounds from right to left, i.e. we first get the bound of W (2) \u039b (1) W (1) :,k , and then bound W\n(3) j,: \u039b (2) Y (1) :,k .\nSimilarly, we can compute the bounds from left to right -get the bound of W (3) j,: \u039b (2) W (2) first, and then bound Y\n(2) j,: \u039b (1) W (1) :,k , where Y (2) j,: = W (3) j,: \u039b (2) W (2)\n. Since the dimension of the output layer (n m ) is typically far less than the dimension of the input vector (n 0 ), computing the bounds from left to right is more efficient as the matrix Y has a smaller dimension of n m \u00d7 n k rather than n k \u00d7 n 0 .\n\n\nExperiments\n\nIn this section, we perform extensive experiments to evaluate the performance of our proposed two lower-bound based robustness certificates on networks with different sizes and with different defending techniques during training process. Specifically, we compare our proposed bounds 1 (Fast-Lin, Fast-Lip) with Linear Programming (LP) based methods (LP, LP-Full), formal verification methods (Reluplex), lower bound by global Lipschitz constant (Op-norm), estimated lower bounds (CLEVER) and attack algorithms (Attacks) for toy networks (2-3 layers with 20 neurons in each layer) and large networks (2-7 layers with 1024 or 2048 neurons in each layer) in Table 1. The evaluation on the effects of defending techniques is presented in Table 2. All bound numbers are the average of 100 random test images with random attack targets, and running time (per image) for all methods is measured on a single CPU core. We include detailed setup of experiments, descriptions of each method, additional experiments and discussions in Appendix F (See Tables F.1 and F.2). The results suggest that our proposed robustness certificates are of high qualities and are computationally efficient even in large networks up to 7 layers or more than 10,000 neurons. In particular, we show that:\n\n\u2022 Our certified lower bounds (Fast-Lin, Fast-Lip) are close to (gap is only 2-3X) the exact minimum distortion computed by Reluplex for small networks (Reluplex is only feasible for networks with less 100 neurons for MNIST), but our algorithm is more than 10,000 times faster than Reluplex. See Table 1a and Table F.1. \u2022 Our certified lower bounds (Fast-Lin, Fast-Lip) give similar quality (the gap is within 35%, and usually around 10%; sometimes our bounds are even better) compared with the LP-based methods (LP, LP-Full); however, our algorithm is 33 -14,000 times faster. The LP-based methods are infeasible for networks with more than 4,000 neurons. See Table 1b and Table F.2.\n\n\u2022 When the network goes larger and deeper, our proposed methods can still give non-trivial lower bounds comparing to the upper bounds founded by attack algorithms on large Table 1. Comparison of methods of computing certified lower bounds (Fast-Lin, Fast-Lip, LP, LP-Full,Op-norm), estimated lower bound (CLEVER), exact minimum distortion (Reluplex) and upper bounds (Attack: CW for p = 2, \u221e, EAD for p = 1) on (a) 2, 3 layers toy MNIST networks with 20 neurons per layer and (b) large networks with 2-7 layers, 1024 or 2048 nodes per layer. Differences of lower bounds and speedup are measured on the best bound from our proposed algorithms and LP-based approaches (the bold numbers in each row). In (a), we show how close our fast bounds are to exact minimum distortions (Reluplex) and the bounds that are slightly tighter but very expensive (LP-Full). In (b), LP-Full and Reluplex are computationally infeasible for all the networks reported here. (b) Larger networks. \"-\" indicates the corresponding method is computationally infeasible for that network. \n\n\nConclusions\n\nIn this paper we have considered the problem of verifying the robustness property of ReLU networks. By exploiting the special properties of ReLU networks, we have here presented two computational efficient methods Fast-Lin and Fast-Lip for this problem. Our algorithms are two or-ders of magnitude (or more) faster than LP-based methods, while obtaining solutions with similar quality; meanwhile, our bounds qualities are much better than the previously proposed operator-norm based methods. Additionally, our methods are efficient and easy to implement: we compute the bounds layer-by-layer, and the computation cost for each layer is similar to the cost of matrix products in forward propagation; moreover, we do not need to solve any integer programming, linear programming problems or their duals. Future work could extend our algorithm to handle the structure of convolutional layers and apply our algorithm to evaluate the robustness property of large DNNs such as ResNet on the ImageNet dataset. Raz, R. and Safra, S. A sub-constant error-probability lowdegree test, and a sub-constant error-probability pcp characterization of np. In STOC. ACM, 1997.\n\nRazenshteyn, I., Song, Z., and Woodruff, D. P. Weighted low rank approximations with provable guarantees. In STOC, 2016.\n\nSinha, A., Namkoong, H., and Duchi, J. Certifiable distributional robustness with principled adversarial training. In ICLR, 2018.\n\nSong, Z., Woodruff, D. P., and Zhong, P. Low rank approximation with entrywise 1 -norm error. In STOC. ACM, 2017a.\n\nSong, Z., Woodruff, D. P., and Zhong, P. Relative error tensor low rank approximation. arXiv preprint arXiv:1704.08246, 2017b.\n\nSong, Z., Woodruff, D. P., and Zhong, P. Towards a zero-one law for entrywise low rank approximation. 2018. \n\n\nA. Hardness\n\nIn this section we show that finding the minimum adversarial distortion with a certified approximation ratio is hard. We first introduce some basic definitions and theorems in Section A.1. We provide some backgrounds about in-approximability reduction in Section A.2. Section A.3 gives a warmup proof for boolean case and then Section A.4 provides the proof of our main hardness result (for network with real inputs).\n\n\nA.1. Definitions\n\nWe provide some basic definitions and theorems in this section. First, we define the classic 3SAT problem. Definition A.1 (3SAT problem). Given n variables and m clauses in a conjunctive normal form CNF formula with the size of each clause at most 3, the goal is to decide whether there exists an assignment to the n Boolean variables to make the CNF formula to be satisfied.\n\nFor the 3SAT problem in Definition A.1, we introduce the Exponential Time Hypothesis (ETH), which is a common concept in complexity field. Then we define another classical question in complexity theory, the SET-COVER problem, which we will use in our proof. The exact SET-COVER problem is one of Karp's 21 NP-complete problems known to be NP-complete in 1972: Definition A.3 (SET-COVER). The inputs are U, S; U = {1, 2, \u00b7 \u00b7 \u00b7 , n} is a universe, P (U ) is the power set of U , and S = {S 1 , \u00b7 \u00b7 \u00b7 , S m } \u2286 P (U ) is a family of subsets, \u222a j\u2208[m] S j = U . The goal is to give a YES/NO answer to the follow decision problem: Does there exist a set-cover of size t, i.e., \u2203C \u2286 [m], such that \u222a j\u2208C S j = U with |C| = t?\n\nAlternatively, we can also state the problem as finding the minimum set cover size t 0 , via a binary search on t using the answers of the decision problem in A.3. The Approximate SET-COVER problem is defined as follows. Definition A.4 (Approximate SET-COVER). The inputs are U, S; U = {1, 2, \u00b7 \u00b7 \u00b7 , n} is a universe, P (U ) is the power set of U , and S = {S 1 , \u00b7 \u00b7 \u00b7 , S m } \u2286 P (U ) is a family of subsets, \u222a j\u2208[m] S j = U . The goal is to distinguish between the following two cases: (I): There exists a small set-cover, i.e., \u2203C \u2286 [m], such that \u222a j\u2208C S j = U with |C| \u2264 t.\n\n(II): Every set-cover is large, i.e., every C \u2286 [m] with \u222a j\u2208C S j = U satisfies that |C| > \u03b1t, where \u03b1 > 1.\n\nAn oracle that solves the Approximate SET-COVER problem outputs an answer t U \u2265 t 0 but t U \u2264 \u03b1t 0 using a binary search, where t U is an upper bound of t 0 with a guaranteed approximation ratio \u03b1. For example, we can use a greedy (rather than exact) algorithm to solve the SET-COVER problem, which cannot always find the smallest size of set cover t 0 , but the size t U given by the greedy algorithm is at most \u03b1 times as large as t 0 .\n\nIn our setting, we want to investigate the hardness of finding the lower bound with a guaranteed approximation ration, but an approximate algorithm for SET-COVER gives us an upper bound of t 0 instead of an lower bound of t 0 . However, in the following proposition, we show that finding an lower bound with an approximation ratio of \u03b1 is as hard as finding an upper bound with an approximation ratio of \u03b1. Proposition A.5. Finding a lower bound t L for the size of the minimal set-cover (that has size t 0 ) with an approximation ratio \u03b1 is as hard as finding an upper bound t U with an approximation ratio \u03b1.\n\nProof. If we find a lower bound t L with t0 \u03b1 \u2264 t L \u2264 t 0 , by multiplying both sides by \u03b1, we also find an upper bound t U = \u03b1t L which satisfies that t 0 \u2264 t U \u2264 \u03b1t 0 . So finding an lower bound with an approximation ratio \u03b1 is at least as hard as finding an upper bound with an approximation ratio \u03b1. The converse is also true.   (1)) ln n-approximation to SET-COVER problem with universe size n.\n\n\nSET-COVER\n\nWe now formally define our neural network robustness verification problems.\n\nDefinition A.7 (ROBUST-NET(R)). Given an n hidden nodes ReLU neural network F (x) : R d \u2192 R where all weights are fixed, for a query input vector x \u2208 R d with F (x) \u2264 0. The goal is to give a YES/NO answer to the following decision problem:\n\nDoes there exist a y with x \u2212 y 1 \u2264 r such that F (y) > 0?\n\nWith an oracle of the decision problem available, we can figure out the smallest r (defined as r 0 ) such that there exists a vector y with x \u2212 y 1 \u2264 r and F (y) > 0 via a binary search.\n\nWe also define a binary variant of the ROBUST-NET problem, denoted as ROBUST-NET(B). The proof for this variant is more straightforward than the real case, and will help the reader understand the proof for the real case. As an analogy to SET-COVER, an oracle that solves the Approximate ROBUST-NET(R) problem can output an answer r \u2265 r 0 but r \u2264 \u03b1r 0 , which is an upper bound of r 0 with a guaranteed approximation ratio \u03b1. With a similar statement as in Proposition A.5, if we divide the answer r by \u03b1, then we get a lower bound r = r \u03b1 where r \u2265 r0 \u03b1 , which is a lower bound with a guaranteed approximation ratio. If we can solve Approximate ROBUST-NET(R), we can get a lower bound with a guaranteed approximation ratio, which is the desired goal of our paper.\n\n\nA.2. Background of the PCP theorem\n\nThe famous Probabilistically Checkable Proofs (PCP) theorem is the cornerstone of the theory of computational hardness of approximation, which investigates the inherent difficulty in designing efficient approximation algorithms for various optimization problems. 2 The formal definition can be stated as follows, Theorem A.11 ((Arora & Safra, 1998;Arora et al., 1998)). Given a SAT formula \u03c6 of size n we can in time polynomial in n construct a set of M tests satisfying the following: (I) : Each test queries a constant number d of bits from a proof, and based on the outcome of the queries it either acceptes or reject \u03c6. (II) : (Yes Case / Completeness) If \u03c6 is satisfiable, then there exists a proof so that all tests accept \u03c6. (III) : (No Case / Soundness) If \u03c6 is not satifiable, then no proof will cause more than M/2 tests to accept \u03c6.\n\nNote that PCP kind of reduction is slightly different from NP reduction, for more examples (e.g. maximum edge biclique, sparsest cut) about how to use PCP theorem to prove inapproximibility results, we refer the readers to (Amb\u00fchl et al., 2011).\n\n\nA.3. Warm-up\n\nWe state our hardness result for ROBUST-NET(B) (boolean inputs case) in this section. The reduction procedure for network with boolean inputs is more straightforward and easier to understand than the real inputs case.\n\nTheorem A.12. Unless NP = P, there is no polynomial time algorithm to give a (1 \u2212 o(1)) ln n-approximation to ROBUST-NET(B) problem (Definition A.9) with n hidden nodes.\n\nProof. Consider a set-cover instance, let S denote a set of sets\n{S 1 , S 2 , \u00b7 \u00b7 \u00b7 , S d } where s j \u2286 [n], \u2200j \u2208 [d].\nFor each set S j we create an input node u j . For each element i \u2208 [n], we create a hidden node v i . For each i \u2208 [n] and j \u2208 [d], if i \u2208 S j , then we connect u j and v i . We also create an output node w, for each i \u2208 [n], we connect node v i and node w.\n\nLet 1 i\u2208Sj denote the indicator function that it is 1 if i \u2208 S j and 0 otherwise. Let T i denote the set that\nT i = {j | i \u2208 S j , \u2200j \u2208 [d]}. For each i \u2208 [n]\n, we define an activation function \u03c6 i satisfies that\n\u03c6 i = 1, if j\u2208Ti u j \u2265 1, 0, otherwise.\nSince u j \u2208 {0, 1}, \u03c6 i can be implemented in this way using ReLU activations:\n\u03c6 i = 1 \u2212 max \uf8eb \uf8ed 0, 1 \u2212 j\u2208Ti u j \uf8f6 \uf8f8 .\nNote that d j=1 1 i\u2208Sj = d j=1 u j , because u j = 1 indicates choosing set S j and u j = 0 otherwise. For final output node w, we define an activation function \u03c8 satisfies that\n\u03c8 = 1, if n i=1 v i \u2265 n, 0, otherwise.\nSince v i \u2208 [n], \u03c8 can be implemented as\n\u03c8 = max 0, n i=1 v i \u2212 n + 1 .\nWe use vector x to denote {0} d vector and it is to easy to see that F (x) = 0. Let \u03b1 > 1 denote a fixed parameter. Also, we have F (y) > 0 if and only if C = {j|y j = 1} is a set-cover. According to our construction, we can have the following two claims, Claim A.13 (Completeness). If there exists a set-cover C \u2286 [d] with \u222a j\u2208C S j = [n] and |C| \u2264 r, then there exists a point y \u2208 {0, 1} d such that x \u2212 y 1 \u2264 r and F (y) > 0.\n\nClaim A.14 (Soundness). If for every C \u2286 [d] with \u222a j\u2208C S j = U satisfies that |C| > \u03b1 \u00b7 t, then for all y \u2208 {0, 1} d satisfies that x \u2212 y 1 \u2264 \u03b1r, F (y) \u2264 0 holds. Therefore, using Theorem A.11, Theorem A.6, Claim A.13 and Claim A.14 completes the proof.\n\n\nA.4. Main result\n\nWith the proof for ROBUST-NET(B) as a warm-up, we now prove our main hardness result for ROBUST-NET(R) in this section.\n\nTheorem A.15. Unless NP = P, there is no polynomial time algorithm to give an (1 \u2212 o(1)) ln n-approximation to ROBUST-NET(R) problem (Definition A.10) with n hidden nodes.\n\nProof. Consider a set-cover instance, let S denote a set of sets {S 1 , S 2 , \u00b7 \u00b7 \u00b7 , S d } where S j \u2286 [n], \u2200j \u2208 [d]. For each set S j we create an input node u j . For each j \u2208 [d], we create a hidden node t j and connect u j and t j .\n\nFor each element i \u2208 [n], we create a hidden node v i . For each i \u2208 [n] and j \u2208 [d], if i \u2208 S j , then we connect u j and v i . Finally, we create an output node w and for each i \u2208 [n], we connect node v i and node w.\n\nLet \u03b4 = 1/d. For each j \u2208 [n], we apply an activation function \u03c6 1,j on t j such that\n\u03c6 1,j = \u2212 max(0, \u03b4 \u2212 u j ) + max(0, u j \u2212 1 + \u03b4)\nIt is easy to see that\nt j = \u03c6 1,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 u j \u2212 \u03b4 if u j \u2208 [0, \u03b4] u j \u2212 (1 \u2212 \u03b4) if u j \u2208 [1 \u2212 \u03b4, 1] 0 otherwise . Let T i denote the set that T i = {j | i \u2208 S j , \u2200j \u2208 [d]}. For each i \u2208 [n]\n, we need an activation function \u03c6 2,i on node v i which satisfies that\n\u03c6 2,i \u2208 [\u2212\u03b4, 0], if \u2200j \u2208 T i , t j \u2208 [\u2212\u03b4, 0], [0, \u03b4], if \u2203j \u2208 T i , t j \u2208 [0, \u03b4].\nThis can be implemented in the following way,\n\u03c6 2,i = max j\u2208Ti t j .\nFor the final output node w, we define it as w = min\ni\u2208[n] v i .\nWe use vector x to denote {0} d vector and it is to easy to see that F (x) = \u2212\u03b4 < 0. Let \u03b1 > 1 denote a fixed parameter.\n\nAccording to our construction, we can have the following two claims. Proof. Without loss of generality, we let the set cover to be {S 1 , S 2 , ..., S r }. Let y 1 = y 2 = \u00b7 \u00b7 \u00b7 = y r = 1 and y r+1 = y r+2 = ... = y d = 0. By the definition of t j , we have t 1 = t 2 = \u00b7 \u00b7 \u00b7 = t r = \u03b4. Since {S 1 , S 2 , \u00b7 \u00b7 \u00b7 , S r } is a set-cover, we know that v i = \u03b4 for all i \u2208 [n]. Then F (y) = w = min i\u2208[n] v i = \u03b4 > 0. Since we also have y 1 = r, the adversarial point is found.\nClaim A.17 (Soundness). If for every C \u2286 [d] with \u222a j\u2208C S j = U satisfies that |C| > \u03b1 \u00b7 r, then for all y \u2208 [0, 1] d satisfies that x \u2212 y 1 \u2264 \u03b1r(1 \u2212 1/d), F (y) \u2264 0 holds.\nProof. Proof by contradiction. We assume that there exists y such that F (y) > 0 and y 1 \u2264 \u03b1r(1 \u2212 1/d). Since F (y) > 0, we have for all i, v i > 0. Thus there exists j \u2208 T i such that t j > 0. Let \u03c0 : [n] \u2192 Q denote a mapping (Q \u2286 [d] will be decided later). This means that for each i \u2208 [n], there exists j \u2208 T i , such that 1 \u2212 \u03b4 < y j \u2264 1, and we let \u03c0(i) denote that j.\n\nWe define set Q \u2286 [d] as follows \nQ = {j | \u2203i \u2208 [n], s.t. \u03c0(i) = j \u2208 T i and t j > 0}.Since|y j | \u2264 \u03b1r(1 \u2212 1/d),\nwhere the first step follows by |Q| \u2264 d.\n\nBecause for all j \u2208 Q, |y j | > 1 \u2212 \u03b4 = 1 \u2212 1/d, we have\n|Q| \u2264 \u03b1r(1 \u2212 1/d) (1 \u2212 1/d) = \u03b1 \u00b7 r.\nSo {S j } j\u2208Q is a set-cover with size less than or equal to \u03b1 \u00b7 r, which is a contradiction.\n\nTherefore, using Theorem A.11, Theorem A.6, Claim A.16 and Claim A.17 completes the proof.\n\nBy making a stronger assumption of ETH, we can have the following stronger result which excludes all 2 o(n c ) time algorithms, where c > 0 is some fixed constant: Note that in (Moshkovitz, 2012a), an additional conjecture, Projection Games Conjecture (PGC) is required for the proof, but the result was improved in (Moshkovitz, 2012b) and PGC is not a requirement any more.\n\n\nB. Proof of Theorem 3.5\n\nFor a m-layer ReLU network, assume we know all the pre-ReLU activation bounds l (k) and u (k) , \u2200k \u2208 [m \u2212 1] for a m-layer ReLU network and we want to compute the bounds of the the j th output at m th layer.\n\nThe j th output can be written as\nf j (x) = nm\u22121 k=1 W (m) j,k [\u03c6 m\u22121 (x)] k + b (m) j ,(15)= nm\u22121 k=1 W (m) j,k \u03c3(W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) + b (m) j ,(16)= k\u2208I + m\u22121 ,I \u2212 m\u22121 ,Im\u22121 W (m) j,k \u03c3(W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) + b (m) j .(17)\nFor neurons belonging to category (i), i.e., k \u2208 I + m\u22121 ,\n\u03c3(W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) = W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k .\nFor neurons belonging to category (ii), i.e., k \u2208 I \u2212 m\u22121 ,\n\u03c3(W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) = 0.\nFinally, for neurons belonging to Category (iii), i.e., k \u2208 I m\u22121 , we bound their outputs. If we adopt the linear upper and lower bounds in (1) and let d\n(m\u22121) k := u (m\u22121) k u (m\u22121) k \u2212l (m\u22121) k , we have d (m\u22121) k (W (m\u22121) k,: \u03c6 m\u22122 (x)+b (m\u22121) k ) \u2264 \u03c3(W (m\u22121) k,: \u03c6 m\u22122 (x)+b (m\u22121) k ) \u2264 d (m\u22121) k (W (m\u22121) k,: \u03c6 m\u22122 (x)+b (m\u22121) k \u2212l (m\u22121) k ).(18)\n\nB.1. Upper bound\n\nThe goal of this section is to prove Lemma B.1.\n\n\nLemma B.1 (Upper bound with explicit function).\n\nGiven an m-layer ReLU neural network function f : R n0 \u2192 R nm , parameters p, , there exists two explicit functions f L : R n0 \u2192 R nm and f U : R n0 \u2192 R nm (see Definition 3.4) such that \u2200j \u2208 [n m ],\nf j (x) \u2264 f U j (x), \u2200x \u2208 B p (x 0 , ).\nNotice that (18)  j,k < 0: (19) to (20) and collect the constant terms (independent of x) in the parenthesis from (20) to (21).\nf U,m\u22121 j (x) = k\u2208I + m\u22121 W (m) j,k (W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) (19) + k\u2208Im\u22121,W (m) j,k >0 W (m) j,k d (m\u22121) k (W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k \u2212 l (m\u22121) k ) + k\u2208Im\u22121,W (m) j,k <0 W (m) j,k d (m\u22121) k (W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) + b (m) j = nm\u22121 k=1 W (m) j,k d (m\u22121) k (W (m\u22121) k,: \u03c6 m\u22122 (x) + b (m\u22121) k ) \u2212 k\u2208Im\u22121,W (m) j,k >0 W (m) j,k d (m\u22121) k l (m\u22121) k + b (m) j ,(20)= nm\u22121 k=1 W (m) j,k d (m\u22121) k W (m\u22121) k,: \u03c6 m\u22122 (x)(21)+ \uf8eb \uf8ec \uf8ed nm\u22121 k=1 W (m) j,k d (m\u22121) k b (m\u22121) k \u2212 k\u2208Im\u22121,W (m) j,k >0 W (m) j,k d (m\u22121) k l (m\u22121) k + b (m) j \uf8f6 \uf8f7 \uf8f8 , where we set d (m\u22121) k = 1 for k \u2208 I + m\u22121 and set d (m\u22121) k = 0 for k \u2208 I \u2212 m\u22121 from\n\nIf we let\nA (m\u22121) = W (m) D (m\u22121) , where D (m\u22121) is a diagonal matrix with diagonals being d (m\u22121) k , then we can rewrite f U,m\u22121 j (x) into the following: f U,m\u22121 j (x) = nm\u22121 k=1 A (m\u22121) j,k W (m\u22121) k,: \u03c6 m\u22122 (x) + A (m\u22121) j,: b (m\u22121) \u2212 A (m\u22121) j,: T (m\u22121) :,j + b (m) j (22) = nm\u22121 k=1 A (m\u22121) j,k ( nm\u22122 r=1 W (m\u22121) k,r [\u03c6 m\u22122 (x)] r ) + A (m\u22121) j,: b (m\u22121) \u2212 A (m\u22121) j,: T (m\u22121) :,j + b (m) j (23) = nm\u22122 r=1 nm\u22121 k=1 A (m\u22121) j,k W (m\u22121) k,r [\u03c6 m\u22122 (x)] r + A (m\u22121) j,: b (m\u22121) \u2212 A (m\u22121) j,: T (m\u22121) :,j + b (m) j (24) = nm\u22122 r=1 W (m\u22121) j,r [\u03c6 m\u22122 (x)] r + b (m\u22121) j .(25)\nFrom (21) to (22), we rewrite the summation terms in the parenthesis into matrix-vector multiplications and for each j \u2208 [n m ] let\nT (m\u22121) k,j = l (m\u22121) k if k \u2208 I m\u22121 , A (m\u22121) j,k > 0 0 otherwise since 0 \u2264 d (m\u22121) k \u2264 1, W (m) j,k > 0 is equivalent to A (m\u22121) j,k > 0.\nFrom (22) to (23), we simply write out the inner product W (m\u22121) k,:\n\n\u03c6 m\u22122 (x) into a summation form, and from (23) to (24), we exchange the summation order of k and r. From (24) to (25), we let\nW (m\u22121) j,r = nm\u22121 k=1 A (m\u22121) j,k W (m\u22121) k,r (26) b (m\u22121) j = A (m\u22121) j,: b (m\u22121) \u2212 A (m\u22121) j,: T (m\u22121) :,j + b (m) j(27)\nTowards Fast Computation of Certified Robustness for ReLU Networks and now we have (25) in the same form as (15).\n\nIndeed, in (15), the running index is k and we are looking at the m th layer, with weights W (m) j,k , activation functions \u03c6 m\u22121 (x) and bias term b (m) j ; in (25), the running index is r and we are looking at the m \u2212 1 th layer with equivalent weights W (m\u22121) j,r , activation functions \u03c6 m\u22122 (x) and equivalent bias b (m\u22121) j . Thus, we can use the same technique from (15) to (25) and obtain an upper bound on the f U,m\u22121 j (x) and repeat this procedure until obtaining f U,1\nj (x), where f j (x) \u2264 f U,m\u22121 j (x) \u2264 f U,m\u22122 j (x) \u2264 . . . \u2264 f U,1 j (x).\nLet the final upper bound f U j (x) = f U,1 j (x), and now we have\nf j (x) \u2264 f U j (x), where f U j (x) = [f U (x)] j , f U j (x) = A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j )\nand for k = 1, . . . , m \u2212 1,\nA (m\u22121) = W (m) D (m\u22121) , A (k\u22121) = A (k) W (k) D (k\u22121) , D (0) = I n0 D (k) r,r = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 u (k) r u (k) r \u2212l (k) r if r \u2208 I k 1 if r \u2208 I + k 0 if r \u2208 I \u2212 k T (k) r,j = l (k) r if r \u2208 I k , A(k)\nj,r > 0 0 otherwise\n\n\nB.2. Lower bound\n\nThe goal of this section is to prove Lemma B.2.\n\nLemma B.2 (Lower bound with explicit function). Given an m-layer ReLU neural network function f : R n0 \u2192 R nm , parameters p, , there exists two explicit functions f L : R n0 \u2192 R nm and f U :\nR n0 \u2192 R nm (see Definition 3.4) such that \u2200j \u2208 [n m ], f L j (x) \u2264 f j (x), \u2200x \u2208 B p (x 0 , ).\nSimilar to deriving the upper bound of f j (x), we consider the signs of the weights W j,k > 0. Following the procedure in (19) to (25) (except that now the additional bias term is from the set k \u2208 I m\u22121 , W (m) j,k < 0), the lower bound is similar to the upper bound we have derived but but replace T (m\u22121) by H (m\u22121) , where for each j \u2208 [n m ],\nH (m\u22121) k,j = l (m\u22121) k if k \u2208 I m\u22121 , A (m\u22121) j,k < 0 0 otherwise.\nIt is because the linear upper and lower bounds in (1) has the same slope u u\u2212l on both sides (i.e. \u03c3(y) is bounded by two lines with the same slope but different intercept), which gives the same A matrix and D matrix in computing the upper bound and lower bound of f j (x). This is the key to facilitate a faster computation under this linear approximation (1). Thus, the lower bound for f j (x) is:\nf L j (x) \u2264 f j (x), where f L j (x) = [f L (x)] j , f L j (x) = A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 H (k)\n:,j ) and for k = 1, . . . , m \u2212 1,\nH (k) r,j = l (k) r if r \u2208 I k , A (k) j,r < 0 0 otherwise.\n\nC. Proof of Corollary 3.7\n\nBy Theorem 3.5, for\nx \u2208 B p (x 0 , ), we have f L j (x) \u2264 f j (x) \u2264 f U j (x). Thus, f j (x) \u2264 f U j (x) \u2264 max x\u2208Bp(x, ) f U j (x),(28)f j (x) \u2265 f L j (x) \u2265 min x\u2208Bp(x, ) f L j (x).(29)Since f U j (x) = A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j ), \u03b3 U j := max x\u2208Bp(x0, ) f U j (x) = max x\u2208Bp(x0, ) A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j ) = max x\u2208Bp(x0, ) A (0) j,: x + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j ) (30) = max y\u2208Bp(0,1) A (0) j,: y + A (0) j,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j ) (31) = A (0) j,: q + A (0) j,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 T (k) :,j ).(32)\nFrom (30) to (31), we do a transformation of variable y := x\u2212x0 and therefore y \u2208 B p (0, 1). By the definition of dual norm \u00b7 * : z * = {sup y z y | y \u2264 1}, and the fact that q norm is dual of p norm for p, q \u2208 [1, \u221e], the term max y\u2208Bp(0,1) A\n\nj,: y in (31) can be expressed as A \nA (0) j,: y + A (0) j,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 H (k) :,j ) = \u2212 max y\u2208Bp(0,1) \u2212A (0) j,: y + A (0) j,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 H (k) :,j ) (33) = \u2212 A (0) j,: q + A (0) j,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: (b (k) \u2212 H (k) :,j ).(34)\nAgain, from (33) to (34), we simply replace max y\u2208Bp(0,1) \u2212A j,: q . Thus, if we use \u03bd j to denote the common term A (0) \nj,: x 0 + b (m) j + m\u22121 k=1 A (k) j,: b (k) , we have \u03b3 U j = A (0) j,: q \u2212 m\u22121 k=1 A (k) j,: T (k) :,j + \u03bd j , (upper bound) \u03b3 L j = \u2212 A(\n\nD. Algorithms\n\nWe present our full algorithms, Fast-Lin in Algorithm 1 and Fast-Lip in Algorithm 2.\n\n\nAlgorithm 1 Fast Bounding via Linear Upper/Lower Bounds for ReLU (Fast-Lin)\n\nRequire: weights and biases of m layers: W (1) , \u00b7 \u00b7 \u00b7 , W (m) , b (1) , \u00b7 \u00b7 \u00b7 , b (m) , original class c, target class j 1: procedure FAST-LIN(x 0 , p, 0 ) 2:\n\nReplace the last layer weights W (m) with a row vectorw \u2190 W if m = 1 then\n\nStep 1: Form A matrices 18:\nA (0) \u2190 W (1) First layer bounds do not depend on l (0) , u (0) 19: else 20: for k \u2190 m \u2212 1 to 1 do 21: if k = m \u2212 1 then Construct D (m \u22121) , A (m \u22121) , H (m \u22121) , T (m \u22121)\n\n22:\n\nConstruct diagonal matrix D (k) \u2208 R n k \u00d7n k using l (k) , u (k) according to Eq. (5). Run FAST-LIN to find layer-wise bounds l (i) , u (i) , and form\nI + i , I \u2212 i , I i fo all i \u2208 [m]\n4:\nC (0) \u2190 W (1) , L (0) \u2190 0, U (0) \u2190 0 5: for l \u2190 1 to m \u2212 1 do 6: C (l) , L (l) , U (l) = BOUNDLAYERGRAD(C (l\u22121) , L (l\u22121) , U (l\u22121) , W (l+1) , n l+1 , I + l , I \u2212 l , I l ) 7:\nv \u2208 R n0 because the last layer is replaced with a row vectorw\n8: v \u2190 max(|C (m\u22121) + L (m\u22121) |, |C (m\u22121) + U (m\u22121) |)\nAll operations are element-wise; 9:\nj \u2190 min( g(x0) v q , )\nq is the dual norm of p, 1 p + 1 q = 1 10:\n\nreturn j j is a certified lower bound \u03b2 L . We can also bisect j (omitted). 11: procedure BOUNDLAYERGRAD(C, L, U, W, n , I + , I \u2212 , I)\n\n\n12:\n\nfor k \u2208 [n 0 ] do n 0 is the dimension of x 0 13:\nfor j \u2208 [n ] do 14: C j,k \u2190 i\u2208I + W j,i C i,k 15: U j,k \u2190 i\u2208I + ,W j,i >0 W j,i U i,k + i\u2208I + ,W j,i <0 W j,i L i,k + 16: i\u2208I,W j,i <0,C i,k +L i,k <0 W j,i (C i,k + L i,k ) + i\u2208I,W j,i >0,C i,k +U i,k >0 W j,i (C i,k + U i,k ) 17: L j,k \u2190 i\u2208I + ,W j,i >0 W j,i L i,k + i\u2208I + ,W j,i <0 W j,i U i,k + 18: i\u2208I,W j,i >0,C i,k +L i,k <0 W j,i (C i,k + L i,k ) + i\u2208I,W j,i <0,C i,k +U i,k >0 W j,i (C i,k + U i,k )\n19:\n\nreturn C , L , U\n\n\nE. An alternative bound on the Lipschitz constant\n\nUsing the property of norm, we can derive an upper bound of the gradient norm of a 2-layer ReLU network in the following:\n\u2207f j (x) q = W (2) j,: \u039b (1) W (1) q = W (2) j,: (\u039b (1) a + \u039b (1) u )W (1) q (35) \u2264 W (2) j,: \u039b (1) a W (1) q + W (2) j,: \u039b (1) u W (1) q (36) \u2264 W (2) j,: \u039b (1) a W (1) q + r\u2208I1 W (2) j,r W (1) r,: q(37)\nwhere with a slight abuse of notation, we use \u039b\n\n(1) a to denote the diagonal activation matrix for neurons who are always activated, i.e. its (r, r) entry \u039b (1) a(r,r) is 1 if r \u2208 I + 1 and 0 otherwise, and we use \u039b\n\n(1) u to denote the diagonal activation matrix for neurons whose status are uncertain because they could possibly be active or inactive, i.e. its (r, r) entry \u039b (1) u(r,r) is 1 if r \u2208 I 1 and 0 otherwise. Therefore, we can write \u039b (1) as a sum of \u039b Note that (35) to (36) is from the sub-additive property of a norm, and (36) to (37) uses the sub-additive property of a norm again and set the uncertain neurons encoding all to 1 because\nW (2) j,: \u039b (1) u W (1) = r\u2208I1 W (2) j,r \u039b (1) u(r,r) W (1) r,: \u2264 r\u2208I1 W (2) j,r \u039b (1) u(r,r) W (1) r,: \u2264 r\u2208I1 W (2) j,r W (1) r,: .\nNotice that (37) can be used as an upper bound of Lipschitz constant and is applicable to compute a certified lower bound for minimum adversarial distortion of a general p norm attack. However, this bound is expected to be less tight because we simply include all the uncertain neurons to get an upper bound on the norm in (37).\n\n\nF. Details of Experiments in Section 4 F.1. Methods\n\nBelow, we give detailed descriptions on the methods that we compare in Table 1, Table F.1 and Table F.2:\n\n\u2022 Fast-Lin: Our proposed method of directly bounding network output via linear upper/lower bounds for ReLU, as discussed in Section 3.3 and Algorithm 1;\n\n\u2022 Fast-Lip: Our proposed method based on bounding local Lipschitz constant, in Section 3.4 and Algorithm 2;\n\n\u2022 Reluplex: Reluplex (Katz et al., 2017) is a satisfiability modulo theory (SMT) based solver which delivers a true minimum distortion, but is very computationally expensive;\n\n\u2022 LP-Full: A linear programming baseline method with formulation borrowed from (Wong & Kolter, 2018). Note that we solve the primal LP formulation exactly to get a best possible bound. This variant solves full relaxed LP problems at every layer to give a final \"adversarial polytope\". Similar to our proposed methods, it only gives a lower bound. We extend this formulation to p = 2 case, where the input constraint becomes quadratic and requires a quadratic constrained programming (QCP) solver, which is usually slower than LP solvers.\n\n\u2022 LP: Similar to LP-Full, but this variant solves only one LP problem for the full network at the output neurons and the layer-wise bounds for the neurons in hidden layers are solved by Fast-Lin. We also extend it to p = 2 case with QCP constraints on the inputs. LP and LP-Full are served as our baselines to compare with Fast-Lin and Fast-Lip;\n\n\u2022 Attacks: Any successful adversarial example gives a valid upper bound for the minimum adversarial distortion. For larger networks where Reluplex is not feasible, we run adversarial attacks and obtain an upper bound of minimal adversarial distortions to compare with. We apply the 2 and \u221e variants of Carlini and Wagner's attack (CW) (Carlini & Wagner, 2017c) to find the best 2 and \u221e distortions. We found that the CW \u221e attack usually finds adversarial examples with smaller \u221e distortions than using PGD (projected gradient descent). We use EAD (Chen et al., 2018b), a Elastic-Net regularized attack, to find adversarial examples with small 1 distortions. We run CW 2 and \u221e attacks for 3,000 iterations and EAD attacks for 2,000 iterations;\n\n\u2022 CLEVER: CLEVER (Weng et al., 2018) is an attack-agnostic robustness score based on local Lipschitz constant estimation and provides an estimated lower-bound. It is capable of performing robustness evaluation for large-scale networks but is not a certified lower bound;\n\n\u2022 Op-norm: Operator norms of weight matrices were first used in (Szegedy et al., 2013) to give a robustness lower bound. We compute the p induced norm of weight matrices of each layer and use their product as the global Lipschitz constant L j q . A valid lower bound is given by g(x 0 )/L j q (see Section 3.4). We only need to pre-compute the operator norms once for all the examples.\n\n\nF.2. Setup\n\nWe use MNIST and CIFAR datasets and evaluate the performance of each method in MLP networks with up to 7 layers or over 10,000 neurons, which is the largest network size for non-trivial and guaranteed robustness verification to date. We use the same number of hidden neurons for each layer and denote a m-layer network with n hidden neurons in each layer as m \u00d7 [n]. Each network is trained with a grid search of learning rates from {0.1, 0.05, 0.02, 0.01, 0.005} and weight decays from {10 \u22124 , 10 \u22125 , 10 \u22126 , 10 \u22127 , 10 \u22128 } and we select the network with the best validation accuracy. We consider both targeted and untargeted robustness under p distortions (p = 1, 2, \u221e); for targeted robustness, we consider three target classes: a random class, a least likely class and a runner-up class (the class with second largest probability). The reported average scores are an average of 100 images from the test set, with images classified wrongly skipped. Reported time is per image. We use binary search to find the certified lower bounds in Fast-Lin, Fast-Lip, LP and LP-Full, and the maximum number of search iterations is set to 15.\n\nWe implement our algorithm using Python (with Numpy and Numba) 3 , while for the LP based method we use the highly efficient Gurobi commercial LP solver with Python Interface. All experiments are conducted in single thread mode (we disable the concurrent solver in Gurobi) on a Intel Xeon E5-2683v3 (2.0 GHz) CPU. Despite the inefficiency of Python, we still achieve two orders of magnitudes speedup compared with LP, while achieving a very similar lower bound. Our methods are automatically parallelized by Numba and can gain further speedups on a multi-core CPU, but we disabled this parallelization for a fair comparison to other methods.\n\n\nF.3. Discussions\n\nIn Table 1a (full Table: Table F.1), we compare the lower bound \u03b2 L computed by each algorithm to the true minimum distortion r 0 found by Reluplex. We are only able to verify 2 and 3 layer MNIST with 20 neurons per hidden layer within reasonable time using Reluplex. It is worth noting that the input dimension (784) is very large compared to the network evaluated in (Katz et al., 2017) with only 5 inputs. Lower bounds found by Fast-Lin is very close to LP, and the gaps are within 2-3X from the true minimum distortion r 0 found by Reluplex. The upper bound given by CW \u221e are also very close to r 0 .\n\nIn Table 1b (full Table: Table F.2), we compare Fast-Lin, Fast-Lip with LP and Op-norm on larger networks with up to over ten thousands hidden neurons. Fast-Lin and Fast-Lip are significantly faster than LP and are able to verify much larger networks (LP becomes very slow to solve exactly on 4-layer MNIST with 4096 hidden neurons, and is infeasible for even larger CIFAR models). Fast-Lin achieves a very similar bound comparing with results of LP over all smaller models, but being over two orders of magnitude faster. We found that Fast-Lip can achieve better bounds when p = 1 in two-layers networks, and is comparable to Fast-Lin in shallow networks. Meanwhile, we also found that Fast-Lin scales better than Fast-Lip for deeper networks, where Fast-Lin usually provides a good bound even when the number of layers is large. For deeper networks, neurons in the last few layers are likely to have uncertain activations, making Fast-Lip being too pessimistic. However, Fast-Lip outperforms the global Lipschitz constant based bound (Op-norm) which quickly goes down to 0 when the network goes deeper, as Fast-Lip is bounding the local Lipschitz constant to compute robustness lower bound. In Table F.2, we also apply our method to MNIST and CIFAR models to compare the minimum distortion for untargeted attacks. The computational benefit of Fast-Lin and Fast-Lip is more significant than LP because LP needs to solve n m objectives (where n m is the total number of classes), whereas the cost of our methods stay mostly unchanged as we get the bounds for all network outputs simultaneously.\n\nIn Table 2, we compute our two proposed lower bounds on neural networks with defending techniques to evaluate the effects of defending techniques (e.g. how much robustness is increased). We train the network with two defending methods, defensive distillation (DD) (Papernot et al., 2016) and adversarial training (Madry et al., 2018) based on robust optimization. For DD we use a temperature of 100, and for adversarial training, we train the network for 100 epochs with adversarial examples crafted by 10 iterations of PGD with = 0.3. The test accuracy for the adversarially trained models dropped from 98.5% to 97.3%, and from 98.6% to 98.1%, for 3 and 4 layer MLP models, respectively. We observe that both defending techniques can increase the computed robustness lower bounds, however adversarial training is significantly more effective than defensive distillation. The lower bounds computed by Fast-Lin are close to the desired robustness guarantee = 0.3. Table F.1. Comparison of our proposed certified lower bounds Fast-Lin and Fast-Lip, LP and LP-Full, the estimated lower bounds by CLEVER, the exact minimum distortion by Reluplex, and the upper bounds by Attack algorithms (CW \u221e for p = \u221e, CW 2 for p = 2, and EAD for p = 1) on 2, 3 layers toy MNIST networks with only 20 neurons per layer. Differences of lower bounds and speedup are measured on the two corresponding bold numbers in each row, representing the best answer from our proposed algorithms and LP based approaches. Reluplex is designed to verify \u221e robustness so we omit results for 2 and 1. Note that LP-Full and Reluplex are very slow and cannot scale to any practical networks, and the purpose of this table is to show how close our fast bounds are compared to the true minimum distortion provided by Reluplex and the bounds that are slightly tighter but very expensive (e.g. LP-Full).  and upper bounds (Attack algorithms: CW for p = 2, \u221e, EAD for p = 1) on networks with 2-7 layers, where each layer has 1024 or 2048 nodes. Differences of lower bounds and speedup are measured on the two corresponding bold numbers in each row. Note that LP-Full and Reluplex are computationally infeasible for all the networks reported here, and \"-\" indicates the method is computationally infeasible for that network. For Op-norm, computation time for each image is negligible as the operator norms can be pre-computed. \n\nReferences\nAilon, N., Bhattacharya, A., Jaiswal, R., and Kumar, A.Approximate clustering with same-cluster queries. In ITCS, 2018.Alon, N., Moshkovitz, D., and Safra, S. Algorithmic construction of sets for k-restrictions. ACM TALG, 2(2): 2006.Amb\u00fchl, C., Mastrolilli, M., and Svensson, O. Inapproximability results for maximum edge biclique, minimum linear arrangement, and sparsest cut. SIAM Journal on Computing, 40(2):567-596, 2011. Arora, S. and Safra, S. Probabilistic checking of proofs: A new characterization of np. JACM, 45(1):70-122, 1998. Arora, S., Lund, C., Motwani, R., Sudan, M., and Szegedy, M. Proof verification and the hardness of approximation problems. JACM, 45(3):501-555, 1998. Carlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In AISec CCS, 2017a. Carlini, N. and Wagner, D. Magnet and \"efficient defenses against adversarial attacks\" are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017b. Carlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP), pp. 39-57, 2017c. Chen, H., Zhang, H., Chen, P.-Y., Yi, J., and Hsieh, C.-J. Show-and-fool: Crafting adversarial examples for neural image captioning. In ACL, 2018a. Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In AISec, 2017. Chen, P.-Y., Sharma, Y., Zhang, H., Yi, J., and Hsieh, C.-J. Ead: Elastic-net attacks to deep neural networks via adversarial examples. In AAAI, 2018b. Cheng, C.-H., N\u00fchrenberg, G., and Ruess, H. Maximum resilience of artificial neural networks. arXiv preprint arXiv:1705.01040, 2017. Cisse, M. M., Adi, Y., Neverova, N., and Keshet, J. Houdini: Fooling deep structured visual and speech recognition models with adversarial examples. In NIPS, 2017. Cohen-Addad, V., De Mesmay, A., Rotenberg, E., and Roytman, A. The bane of low-dimensionality clustering. In SODA. SIAM, 2018. Dinur, I. Mildly exponential reduction from gap 3sat to polynomial-gap label-cover. In ECCC, 2016. Dinur, I. and Steurer, D. Analytical approach to parallel repetition. In STOC. ACM, 2014. Ehlers, R. Formal verification of piece-wise linear feedforward neural networks. In ATVA, 2017. Feige, U. Relations between average case complexity and approximation complexity. In STOC. ACM, 2002. Fischetti, M. and Jo, J. Deep neural networks as 0-1 mixed integer linear programs: A feasibility study. arXiv preprint arXiv:1712.06174, 2017. H\u00e5stad, J. Tensor rank is np-complete. Journal of Algorithms, 11(4):644-654, 1990. He, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adversarial example defenses: Ensembles of weak defenses are not strong. In USENIX WOOT, 2017. Hein, M. and Andriushchenko, M. Formal guarantees on the robustness of a classifier against adversarial manipulation. arXiv preprint arXiv:1705.08475, 2017. Impagliazzo, R. and Paturi, R. On the complexity of ksat. Journal of Computer and System Sciences, 62(2): 367-375, 2001. Impagliazzo, R., Paturi, R., and Zane, F. Which problems have strongly exponential complexity? In FOCS. IEEE, 1998. Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. In EMNLP, 2017. Katz, G., Barrett, C., Dill, D. L., Julian, K., and Kochenderfer, M. J. Reluplex: An efficient smt solver for verifying deep neural networks. In CAV, 2017. Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial machine learning at scale. In ICLR, 2017. Liu, X., Cheng, M., Zhang, H., and Hsieh, C.-J. Towards robust neural networks via random self-ensemble. arXiv preprint arXiv:1712.00673, 2017a. Liu, Y., Chen, X., Liu, C., and Song, D. Delving into transferable adversarial examples and black-box attacks. In ICLR, 2017b. Lokshtanov, D., Marx, D., and Saurabh, S. Lower bounds based on the exponential time hypothesis. Bulletin of EATCS, 3(105), 2013.Lomuscio, A. and Maganti, L. An approach to reachability analysis for feed-forward relu neural networks. arXiv preprint arXiv:1706.07351, 2017.Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.Manurangsi, P. and Raghavendra, P. A birthday repetition theorem and complexity of approximating dense csps. In ICALP, 2017.Moshkovitz, D. The projection games conjecture and the np-hardness of ln n-approximating set-cover. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 276-287. Springer, 2012a.Moshkovitz, D. The projection games conjecture and the np-hardness of ln n-approximating set-cover. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 276-287. Springer, 2012b.Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy (SP), pp. 582-597, 2016.Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical black-box attacks against machine learning. In AsiaCCS, 2017.\n\n\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Tram\u00e8r, F., Kurakin, A., Papernot, N., Boneh, D., and Mc-Daniel, P. Ensemble adversarial training: Attacks and defenses. In ICLR, 2018. Wang, Q., Guo, W., Zhang, K., Ororbia II, A. G., Xing, X., Liu, X., and Giles, C. L. Adversary resistant deep neural networks with an application to malware detection. In SIGKDD. ACM, 2017. Weng, T.-W., Zhang, H., Chen, P.-Y., Jinfeng, Y., Su, D., Gao, Y., Hsieh, C.-J., and Daniel, L. Evaluating the robustness of neural networks: An extreme value theory approach. In ICLR, 2018. Wong, E. and Kolter, J. Z. Provable defenses against adversarial examples via the convex outer adversarial polytope. In ICML. https://arxiv.org/pdf/1711.00851v2, 2018. Xie, C., Wang, J., Zhang, Z., Zhou, Y., Xie, L., and Yuille, A. Adversarial examples for semantic segmentation and object detection. In ICCV, 2017.\n\n\nHypothesis A.2 (Exponential Time Hypothesis (ETH)(Impagliazzo et al., 1998)). There is a \u03b4 > 0 such that the 3SAT problem defined in Definition A.1 cannot be solved in O(2 \u03b4n ) time.ETH had been used in many different problems, e.g. clustering(Ailon et al., 2018; Cohen-Addad et al., 2018), low-rank approximation(Razenshteyn et al., 2016; Song et al., 2017a;b; 2018). For more details, we refer the readers to a survey(Lokshtanov et al., 2013).\n\n\nis a well-studied problem in the literature. Here we introduce a theorem from(Raz & Safra, 1997; Alon et al.,  2006; Dinur & Steurer, 2014)  which implies the hardness of approximating SET-COVER.\n\n\net al., 2006; Dinur & Steurer, 2014)). Unless NP = P, there is no polynomial time algorithm that gives a (1 \u2212 o\n\nDefinition A. 8 (\n8ROBUST-NET(B)). Given an n hidden nodes ReLU neural network F (x) : {0, 1} d \u2192 {0, 1} where weights are all fixed, for a query input vector x \u2208 {0, 1} d with F (x) = 0. The goal is to give a YES/NO answer to the following decision problem:Does there exist a y with x \u2212 y 1 \u2264 r such that F (y) = 1?Then, we define the approximate version of our neural network robustness verification problems.Definition A.9 (Approximate ROBUST-NET(B)). Given an n hidden nodes ReLU neural network F (x) : {0, 1} d \u2192 {0, 1} where weights are all fixed, for a query input vector x \u2208 {0, 1} d with F (x) = 0. The goal is to distinguish the following two cases : (I): There exists a point y such that x \u2212 y 1 \u2264 r and F (y) = 1. (II): For all y satisfies x \u2212 y 1 \u2264 \u03b1r, the F (y) = 0, where \u03b1 > 1. Definition A.10 (Approximate ROBUST-NET(R)). Given an n hidden nodes ReLU neural network F (x) : R d \u2192 R where weights are all fixed, for a query input vector x \u2208 R d with F (x) \u2264 0. The goal is to distinguish the following two cases : (I): There exists a point y such that x \u2212 y 1 \u2264 r and F (y) > 0. (II): For all y satisfies x \u2212 y 1 \u2264 \u03b1r, the F (y) \u2264 0, where \u03b1 > 1.\n\nClaim A. 16 (\n16Completeness). If there exists a set-cover C \u2286 [d] with \u222a j\u2208C S j = [n] and |C| \u2264 r, then there exists a point y \u2208 [0, 1] d such that x \u2212 y 1 \u2264 r and F (y) > 0.\n\nCorollary A. 18 .\n18Assuming Exponential Time Hypothesis (ETH, see Hypothesis A.2), there is no 2 o(n c ) time algorithm that gives a (1 \u2212 o(1)) ln n-approximation to ROBUST-NET problem with n hidden nodes, where c > 0 is some fixed constant. Proof. It follows by the construction in Theorem A.15 and (Moshkovitz, 2012a;b).\n\n\ncan be used to construct an upper bound and lower bound of f j (x) by considering the signs of the weights W (m) j,k . Let f U,m\u22121 j (x) be an upper bound of f j (x); f U,m\u22121 j (x) can be constructed by taking the right-hand-side (RHS)\n\n\nbe a lower bound of f j (x); f L,m\u22121 j (x) can be constructed by taking the right-hand-side (RHS) of (18) if W (m) j,k < 0 and taking the left-hand-side (LHS) of (18) if W (m)\n\n:\n,j + \u03bd j . (lower bound)\n\n\nhas not achieved a desired accuracy and iteration limit has not reached do 5:l (0) , u (0) \u2190 don'tcare 6: for k \u2190 1 to m do Compute lower and upper bounds for ReLU unis for all m layers 7:l (k) , u (k) \u2190COMPUTETWOSIDEBOUNDS(x 0 , , p, l (1:k\u22121) , u (1:k\u22121) , k) 8:if l (m) > 0 then l (m) is a scalar since the last layer weight is a a certified lower bound \u03b2 L 14: procedure COMPUTETWOSIDEBOUNDS(x 0 , , p, l (1:m \u22121) , u (1:m \u22121) , m ) 15:x 0 \u2208 R n0 : input data vector, p : p norm, : maximum p -norm perturbation16:    l (k) , u (k) , k \u2208 [m ] : layer-wise bounds 17:\n\nA\n(m \u22121) \u2190 W (m ) D (m \u22121) 24: elseMultiply all saved A (k) by A (m \u22121)Algorithm 2 Fast Bounding via Upper Bounding Local Lipschitz Constant (Fast-Lip)Require: Weights of m layers: W (1) , \u00b7 \u00b7 \u00b7 W (m) , original class c, target class j 1: procedure FAST-LIP(x 0\n\n\nIn this section, we propose a methodology to directly derive upper bounds and lower bounds of the output of an mlayer feed-forward ReLU network. The central idea is to derive an explicit upper/lower bound based on the linear approximations for the neurons in category (iii) and the signs of the weights associated with the activations.3.3. Approach 1 (Fast-Lin): Certified lower bounds via \nlinear approximations \n\n3.3.1. DERIVATION OF THE OUTPUT BOUNDS VIA \nLINEAR UPPER AND LOWER BOUNDS FOR RELU \n\n\n\n\nToy networks. Reluplex is designed to verify \u221e robustness so we omit its numbers for p = 2, 1.Toy Networks \nAverage Magnitude of Distortions on 100 Images \n\nNetwork p Target \n\nCertified Lower Bounds \ndifference \nExact \nUncertified \nOur bounds \nOur Baselines \nours vs. \nReluplex \nCLEVER \nAttacks \nFast-Lin \nFast-Lip \nLP \nLP-Full \nLP(-Full) \n(Katz et al., 2017) (Weng et al., 2018) CW/EAD \n\nMNIST \n2 \u00d7 [20] \n\n\u221e \nrand \n0.0309 \n0.0270 \n0.0319 \n0.0319 \n-3.2% \n0.07765 \n0.0428 \n0.08060 \n2 \nrand \n0.6278 \n0.6057 \n0.7560 \n0.9182 \n-31.6% \n-\n0.8426 \n1.19630 \n1 \nrand \n3.9297 \n4.8561 \n4.2681 \n4.6822 \n+3.7% \n-\n5.858 \n11.4760 \n\nMNIST \n3 \u00d7 [20] \n\n\u221e \nrand \n0.0229 \n0.0142 \n0.0241 \n0.0246 \n-6.9% \n0.06824 \n0.0385 \n0.08114 \n2 \nrand \n0.4652 \n0.3273 \n0.5345 \n0.7096 \n-34.4% \n-\n0.7331 \n1.22570 \n1 \nrand \n2.8550 \n2.8144 \n3.1000 \n3.5740 \n-20.1% \n-\n4.990 \n10.7220 \n\n(a) Large Networks \nAverage Magnitude of Distortion on 100 Images \nAverage Running Time per Image \n\nNetwork \np \n\nCertified Bounds \ndiff \nUncertified \nCertified Bounds \nSpeedup \nOur bounds \nLP \nOp-norm \nours \nCLEVER \nAttacks \nOur bounds \nLP \nours \nFast-Lin Fast-Lip (Baseline) (Szegedy et al., 2013) \nvs. LP \n(Weng et al., 2018) CW/EAD Fast-Lin Fast-Lip (Baseline) \nvs. LP \n\nMNIST \n2 \u00d7 [1024] \n\n\u221e 0.03083 0.02512 \n0.03386 \n0.00263 \n-8.9% \n0.0708 \n0.1291 \n156 ms \n219 ms \n20.8 s \n133X \n2 \n0.63299 0.59033 \n0.75164 \n0.40201 \n-15.8% \n1.2841 \n1.8779 \n128 ms \n234 ms \n195 s \n1523X \n1 \n3.88241 5.10000 \n4.47158 \n0.35957 \n+14.1% \n7.4186 \n17.259 \n139 ms \n1.40 s \n48.1 s \n34X \n\nMNIST \n3 \u00d7 [1024] \n\n\u221e 0.02216 0.01236 \n0.02428 \n0.00007 \n-8.7% \n0.0717 \n0.1484 \n1.12 s \n1.11 s \n52.7 s \n47X \n2 \n0.43892 0.26980 \n0.49715 \n0.10233 \n-11.7% \n1.2441 \n2.0387 \n906 ms \n914 ms \n714 s \n788X \n1 \n2.59898 2.25950 \n2.91766 \n0.01133 \n-10.9% \n7.2177 \n17.796 \n863 ms \n3.84 s \n109 s \n126X \n\nMNIST \n4 \u00d7 [1024] \n\n\u221e 0.00823 0.00264 \n-\n0.00001 \n-\n0.0793 \n0.1303 \n2.25 s \n3.08 s \n-\n-\n2 \n0.18891 0.06487 \n-\n0.17734 \n-\n1.4231 \n1.8921 \n2.37 s \n2.72 s \n-\n-\n1 \n1.57649 0.72800 \n-\n0.00183 \n-\n8.9764 \n17.200 \n2.42 s \n2.91 s \n-\n-\n\nCIFAR \n5 \u00d7 [2048] \n\n\u221e 0.00170 0.00030 \n-\n0.00000 \n-\n0.0147 \n0.02351 \n26.2 s \n78.1 s \n-\n-\n2 \n0.07654 0.01417 \n-\n0.00333 \n-\n0.6399 \n0.9497 \n36.8 s \n49.4 s \n-\n-\n1 \n1.18928 0.31984 \n-\n0.00000 \n-\n9.7145 \n21.643 \n37.5 s \n53.6 s \n-\n-\n\nCIFAR \n6 \u00d7 [2048] \n\n\u221e 0.00090 0.00007 \n-\n0.00000 \n-\n0.0131 \n0.01866 \n37.0 s \n119 s \n-\n-\n2 \n0.04129 0.00331 \n-\n0.01079 \n-\n0.5860 \n0.7635 \n60.2 s \n95.6 s \n-\n-\n1 \n0.72178 0.08212 \n-\n0.00000 \n-\n8.2507 \n17.160 \n61.4 s \n88.2 s \n-\n-\n\nCIFAR \n7 \u00d7 [1024] \n\n\u221e 0.00134 0.00008 \n-\n0.00000 \n-\n0.0112 \n0.0218 \n10.6 s \n29.2 s \n-\n-\n2 \n0.05938 0.00407 \n-\n0.00029 \n-\n0.5145 \n0.9730 \n16.9 s \n27.3 s \n-\n-\n1 \n0.86467 0.09239 \n-\n0.00000 \n-\n8.630 \n22.180 \n17.6 s \n26.7 s \n-\n-\n\n\n\nTable 2 .\n2Comparison of the lower bounds for \u221e distortion found by our algorithms on models with defensive distillation (DD) (Papernot et al., 2016) with temperature = 100 and adversarial training (Madry et al., 2018) with = 0.3 for three targeted attack classes.runner-up target \nrandom target \nleast-likely target \nNetwork Method Undefended \nDD \nAdv. Training Undefended \nDD \nAdv. Training Undefended \nDD \nAdv. Training \nMNIST \n3*[1024] \n\nFast-Lin \n0.01826 \n0.02724 \n0.14730 \n0.02211 \n0.03827 \n0.17275 \n0.02427 \n0.04967 \n0.20136 \nFast-Lip \n0.00965 \n0.01803 \n0.09687 \n0.01217 \n0.02493 \n0.11618 \n0.01377 \n0.03207 \n0.13858 \nMNIST \n4*[1024] \n\nFast-Lin \n0.00715 \n0.01561 \n0.09579 \n0.00822 \n0.02045 \n0.11209 \n0.00898 \n0.02368 \n0.12901 \nFast-Lip \n0.00087 \n0.00585 \n0.04133 \n0.00145 \n0.00777 \n0.05048 \n0.00183 \n0.00903 \n0.06015 \n\nnetworks. See Table 1b and Table F.2. \n\u2022 For defended networks, especially for adversarial train-\ning (Madry et al., 2018), our methods give significantly \nlarger bounds, validating the effectiveness of this defending \nmethod. Our algorithms can thus be used for evaluating \ndefending techniques. See Table 2. \n\n\n\n\nj\u2208[d]  |y j | = y 1 \u2264 \u03b1r(1 \u2212 1/d), we havej\u2208Q \n\n|y j | \u2264 \n\nj\u2208[d] \n\n\n\nTable F . 2 .\nF2Comparison of our proposed certified lower bounds Fast-Lin and Fast-Lip with other lower bounds (LP, Op-norm, CLEVER)\n\n\nLarge Networks Average Magnitude of Distortion on 100 Images Average Running Time per Image Lin Fast-Lip (Baseline) (Szegedy et al., 2013) vs. LP (Weng et al., 2018) CW/EAD Fast-Lin Fast-Lip (Baseline) vs. LP MNIST 2 \u00d7 [1024]Network \np \nTarget \n\nCertified Bounds \ndiff \nUncertified \nCertified Bounds \nSpeedup \nOur bounds \nLP \nOp-norm \nours \nCLEVER \nAttacks \nOur bounds \nLP \nours \nFast-\u221e \n\nrunner-up 0.02256 0.01802 \n0.02493 \n0.00159 \n-9.5% \n0.0447 \n0.0856 \n127 ms \n167 ms \n19.3 s \n151X \nrand \n0.03083 0.02512 \n0.03386 \n0.00263 \n-8.9% \n0.0708 \n0.1291 \n156 ms \n219 ms \n20.8 s \n133X \nleast \n0.03854 0.03128 \n0.04281 \n0.00369 \n-10.0% \n0.0925 \n0.1731 \n129 ms \n377 ms \n22.2 s \n172X \n\n2 \n\nrunner-up 0.46034 0.42027 \n0.55591 \n0.24327 \n-17.2% \n0.8104 \n1.1874 \n127 ms \n196 ms \n419 s \n3305X \nrand \n0.63299 0.59033 \n0.75164 \n0.40201 \n-15.8% \n1.2841 \n1.8779 \n128 ms \n234 ms \n195 s \n1523X \nleast \n0.79263 0.73133 \n0.94774 \n0.56509 \n-16.4% \n1.6716 \n2.4556 \n163 ms \n305 ms \n156 s \n956X \n\n1 \n\nrunner-up 2.78786 3.46500 \n3.21866 \n0.20601 \n+7.7% \n4.5970 \n9.5295 \n117 ms \n1.17 s \n38.9 s \n33X \nrand \n3.88241 5.10000 \n4.47158 \n0.35957 \n+14.1% \n7.4186 \n17.259 \n139 ms \n1.40 s \n48.1 s \n34X \nleast \n4.90809 6.36600 \n5.74140 \n0.48774 \n+10.9% \n9.9847 \n23.933 \n151 ms \n1.62 s \n53.1 s \n33X \n\nMNIST \n3 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.01830 0.01021 \n0.02013 \n0.00004 \n-9.1% \n0.0509 \n0.1037 \n1.20 s \n1.81 s \n50.4 s \n42X \nrand \n0.02216 0.01236 \n0.02428 \n0.00007 \n-8.7% \n0.0717 \n0.1484 \n1.12 s \n1.11 s \n52.7 s \n47X \nleast \n0.02432 0.01384 \n0.02665 \n0.00009 \n-8.7% \n0.0825 \n0.1777 \n1.02 s \n924 ms \n54.3 s \n53X \n\n2 \n\nrunner-up 0.35867 0.22120 \n0.41040 \n0.06626 \n-12.6% \n0.8402 \n1.3513 \n898 ms \n1.59 s \n438 s \n487X \nrand \n0.43892 0.26980 \n0.49715 \n0.10233 \n-11.7% \n1.2441 \n2.0387 \n906 ms \n914 ms \n714 s \n788X \nleast \n0.48361 0.30147 \n0.54689 \n0.13256 \n-11.6% \n1.4401 \n2.4916 \n925 ms \n1.01 s \n858 s \n928X \n\n1 \n\nrunner-up 2.08887 1.80150 \n2.36642 \n0.00734 \n-11.7% \n4.8370 \n10.159 \n836 ms \n3.16 s \n91.1 s \n109X \nrand \n2.59898 2.25950 \n2.91766 \n0.01133 \n-10.9% \n7.2177 \n17.796 \n863 ms \n3.84 s \n109 s \n126X \nleast \n2.87560 2.50000 \n3.22548 \n0.01499 \n-10.8% \n8.3523 \n22.395 \n900 ms \n4.20 s \n122 s \n136X \n\nMNIST \n4 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.00715 0.00219 \n-\n0.00001 \n-\n0.0485 \n0.08635 \n1.90 s \n4.58 s \n-\n-\nrand \n0.00823 0.00264 \n-\n0.00001 \n-\n0.0793 \n0.1303 \n2.25 s \n3.08 s \n-\n-\nleast \n0.00899 0.00304 \n-\n0.00001 \n-\n0.1028 \n0.1680 \n2.15 s \n3.02 s \n-\n-\n\n2 \n\nrunner-up 0.16338 0.05244 \n-\n0.11015 \n-\n0.8689 \n1.2422 \n2.23 s \n3.50 s \n-\n-\nrand \n0.18891 0.06487 \n-\n0.17734 \n-\n1.4231 \n1.8921 \n2.37 s \n2.72 s \n-\n-\nleast \n0.20672 0.07440 \n-\n0.23710 \n-\n1.8864 \n2.4451 \n2.56 s \n2.77 s \n-\n-\n\n1 \n\nrunner-up 1.33794 0.58480 \n-\n0.00114 \n-\n5.2685 \n10.079 \n2.42 s \n2.71 s \n-\n-\nrand \n1.57649 0.72800 \n-\n0.00183 \n-\n8.9764 \n17.200 \n2.42 s \n2.91 s \n-\n-\nleast \n1.73874 0.82800 \n-\n0.00244 \n-\n11.867 \n23.910 \n2.54 s \n3.54 s \n-\n-\n\nCIFAR \n5 \u00d7 [2048] \n\n\u221e \n\nrunner-up 0.00137 0.00020 \n-\n0.00000 \n-\n0.0062 \n0.00950 \n24.2 s \n60.4 s \n-\n-\nrand \n0.00170 0.00030 \n-\n0.00000 \n-\n0.0147 \n0.02351 \n26.2 s \n78.1 s \n-\n-\nleast \n0.00188 0.00036 \n-\n0.00000 \n-\n0.0208 \n0.03416 \n27.8 s \n79.0 s \n-\n-\n\n2 \n\nrunner-up 0.06122 0.00951 \n-\n0.00156 \n-\n0.2712 \n0.3778 \n34.0 s \n60.7 s \n-\n-\nrand \n0.07654 0.01417 \n-\n0.00333 \n-\n0.6399 \n0.9497 \n36.8 s \n49.4 s \n-\n-\nleast \n0.08456 0.01778 \n-\n0.00489 \n-\n0.9169 \n1.4379 \n37.4 s \n49.8 s \n-\n-\n\n1 \n\nrunner-up 0.93835 0.22632 \n-\n0.00000 \n-\n4.0755 \n7.6529 \n36.5 s \n70.6 s \n-\n-\nrand \n1.18928 0.31984 \n-\n0.00000 \n-\n9.7145 \n21.643 \n37.5 s \n53.6 s \n-\n-\nleast \n1.31904 0.38887 \n-\n0.00001 \n-\n12.793 \n34.497 \n38.3 s \n48.6 s \n-\n-\n\nCIFAR \n6 \u00d7 [2048] \n\n\u221e \n\nrunner-up 0.00075 0.00005 \n-\n0.00000 \n-\n0.0054 \n0.00770 \n37.2 s \n106 s \n-\n-\nrand \n0.00090 0.00007 \n-\n0.00000 \n-\n0.0131 \n0.01866 \n37.0 s \n119 s \n-\n-\nleast \n0.00095 0.00008 \n-\n0.00000 \n-\n0.0199 \n0.02868 \n37.2 s \n126 s \n-\n-\n\n2 \n\nrunner-up 0.03463 0.00228 \n-\n0.00476 \n-\n0.2394 \n0.2979 \n56.1 s \n99.5 s \n-\n-\nrand \n0.04129 0.00331 \n-\n0.01079 \n-\n0.5860 \n0.7635 \n60.2 s \n95.6 s \n-\n-\nleast \n0.04387 0.00385 \n-\n0.01574 \n-\n0.8756 \n1.2111 \n61.8 s \n88.6 s \n-\n-\n\n1 \n\nrunner-up 0.59638 0.05647 \n-\n0.00000 \n-\n3.3569 \n6.0112 \n57.2 s \n108 s \n-\n-\nrand \n0.72178 0.08212 \n-\n0.00000 \n-\n8.2507 \n17.160 \n61.4 s \n88.2 s \n-\n-\nleast \n0.77179 0.09397 \n-\n0.00000 \n-\n12.603 \n28.958 \n62.1 s \n65.1 s \n-\n-\n\nCIFAR \n7 \u00d7 [1024] \n\n\u221e \n\nrunner-up 0.00119 0.00006 \n-\n0.00000 \n-\n0.0062 \n0.0102 \n10.5 s \n27.3 s \n-\n-\nrand \n0.00134 0.00008 \n-\n0.00000 \n-\n0.0112 \n0.0218 \n10.6 s \n29.2 s \n-\n-\nleast \n0.00141 0.00010 \n-\n0.00000 \n-\n0.0148 \n0.0333 \n11.2 s \n30.9 s \n-\n-\n\n2 \n\nrunner-up 0.05279 0.00308 \n-\n0.00020 \n-\n0.2661 \n0.3943 \n16.3 s \n28.2 s \n-\n-\nrand \n0.05938 0.00407 \n-\n0.00029 \n-\n0.5145 \n0.9730 \n16.9 s \n27.3 s \n-\n-\nleast \n0.06249 0.00474 \n-\n0.00038 \n-\n0.6253 \n1.3709 \n17.4 s \n27.6 s \n-\n-\n\n1 \n\nrunner-up 0.76647 0.07028 \n-\n0.00000 \n-\n4.815 \n7.9987 \n16.9 s \n27.8 s \n-\n-\nrand \n0.86467 0.09239 \n-\n0.00000 \n-\n8.630 \n22.180 \n17.6 s \n26.7 s \n-\n-\nleast \n0.91127 0.10639 \n-\n0.00000 \n-\n11.44 \n31.529 \n17.5 s \n23.5 s \n-\n-\n\nMNIST \n3 \u00d7 [1024] \n\n\u221e \nuntargeted \n\n0.01808 0.01016 \n0.01985 \n0.00004 \n-8.9% \n0.0458 \n0.0993 \n915 ms \n2.17 s \n227 s \n248X \n2 \n0.35429 0.21833 \n-\n0.06541 \n-\n0.7413 \n1.1118 \n950 ms \n2.02 s \n-\n-\n1 \n2.05645 1.78300 \n2.32921 \n0.00679 \n-11.7% \n3.9661 \n9.0044 \n829 ms \n4.41 s \n537 s \n648X \n\nCIFAR \n5 \u00d7 [2048] \n\n\u221e \nuntargeted \n\n0.00136 0.00020 \n-\n0.00000 \n-\n0.0056 \n0.00950 \n24.1 s \n72.9 s \n-\n-\n2 \n0.06097 0.00932 \n-\n0.00053 \n-\n0.2426 \n0.3702 \n34.2 s \n77.0 s \n-\n-\n1 \n0.93429 0.22535 \n-\n0.00000 \n-\n3.6704 \n7.3687 \n35.6 s \n90.2 s \n-\n-\n\nReLU robustness verification problem with n neurons, where c \u2208 (0, 1) is some fixed constant.\nhttps://github.com/huanzhang12/CertifiedReLURobustness\nhttps://en.wikipedia.org/wiki/PCP_theorem\nhttps://github.com/huanzhang12/CertifiedReLURobustness\nAcknowledgmentThe authors sincerely thank Aviad Rubinstein for the suggestion of using set-cover to prove hardness. The authors sincerely thank Dana Moshkovitz for pointing out some references about the hardness result of set-cover. The authors would also like to thank Mika G\u00f6\u00f6s, Rasmus Kyng, Zico Kolter, Jelani Nelson, Eric Price, Milan Rubinstein, Jacob Steinhardt, Zhengyu Wang, Eric Wong and David P. Woodruff for useful discussions. Luca Daniel and Tsui-Wei Weng acknowledge the partial support of MIT-Skoltech program and MIT-IBM Watson AI Lab. Huan Zhang and Cho-Jui Hsieh acknowledge the support of NSF via IIS-1719097 and the computing resources provided by Google Cloud and NVIDIA.25:We save A (k) for next function call 26:27:for all r \u2208 I k do 28:for j \u2190 1 to n k do 29:if A (k) j,r > 0 then 30:33:for j = 1 to n m doStep 2: Compute \u03b3 U and \u03b3 L 34:According to Eq. (6) 37:According to Eq.(7)38:\u03bd j , \u00b5 + j , \u00b5 \u2212 j satisfy Definition 3.6 39: return \u03b3 L , \u03b3 U", "annotations": {"author": "[{\"end\":160,\"start\":74},{\"end\":209,\"start\":161},{\"end\":273,\"start\":210},{\"end\":335,\"start\":274},{\"end\":366,\"start\":336},{\"end\":431,\"start\":367},{\"end\":469,\"start\":432},{\"end\":533,\"start\":470},{\"end\":160,\"start\":74},{\"end\":209,\"start\":161},{\"end\":273,\"start\":210},{\"end\":335,\"start\":274},{\"end\":366,\"start\":336},{\"end\":431,\"start\":367},{\"end\":469,\"start\":432},{\"end\":533,\"start\":470}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":83},{\"end\":171,\"start\":166},{\"end\":221,\"start\":217},{\"end\":283,\"start\":279},{\"end\":349,\"start\":344},{\"end\":379,\"start\":373},{\"end\":450,\"start\":443},{\"end\":481,\"start\":475},{\"end\":87,\"start\":83},{\"end\":171,\"start\":166},{\"end\":221,\"start\":217},{\"end\":283,\"start\":279},{\"end\":349,\"start\":344},{\"end\":379,\"start\":373},{\"end\":450,\"start\":443},{\"end\":481,\"start\":475}]", "author_first_name": "[{\"end\":82,\"start\":74},{\"end\":165,\"start\":161},{\"end\":216,\"start\":210},{\"end\":278,\"start\":274},{\"end\":343,\"start\":336},{\"end\":372,\"start\":367},{\"end\":440,\"start\":432},{\"end\":442,\"start\":441},{\"end\":474,\"start\":470},{\"end\":82,\"start\":74},{\"end\":165,\"start\":161},{\"end\":216,\"start\":210},{\"end\":278,\"start\":274},{\"end\":343,\"start\":336},{\"end\":372,\"start\":367},{\"end\":440,\"start\":432},{\"end\":442,\"start\":441},{\"end\":474,\"start\":470}]", "author_affiliation": "[{\"end\":108,\"start\":89},{\"end\":159,\"start\":110},{\"end\":192,\"start\":173},{\"end\":208,\"start\":194},{\"end\":272,\"start\":223},{\"end\":316,\"start\":285},{\"end\":334,\"start\":318},{\"end\":365,\"start\":351},{\"end\":430,\"start\":381},{\"end\":468,\"start\":452},{\"end\":532,\"start\":483},{\"end\":108,\"start\":89},{\"end\":159,\"start\":110},{\"end\":192,\"start\":173},{\"end\":208,\"start\":194},{\"end\":272,\"start\":223},{\"end\":316,\"start\":285},{\"end\":334,\"start\":318},{\"end\":365,\"start\":351},{\"end\":430,\"start\":381},{\"end\":468,\"start\":452},{\"end\":532,\"start\":483}]", "title": "[{\"end\":67,\"start\":1},{\"end\":600,\"start\":534},{\"end\":67,\"start\":1},{\"end\":600,\"start\":534}]", "venue": "[{\"end\":671,\"start\":602},{\"end\":671,\"start\":602}]", "abstract": "[{\"end\":2378,\"start\":894},{\"end\":2378,\"start\":894}]", "bib_ref": "[{\"end\":2507,\"start\":2485},{\"end\":2670,\"start\":2652},{\"end\":2709,\"start\":2689},{\"end\":2750,\"start\":2730},{\"end\":2789,\"start\":2770},{\"end\":2835,\"start\":2816},{\"end\":3028,\"start\":3005},{\"end\":3050,\"start\":3028},{\"end\":3068,\"start\":3050},{\"end\":3437,\"start\":3414},{\"end\":3493,\"start\":3471},{\"end\":3512,\"start\":3493},{\"end\":3554,\"start\":3533},{\"end\":3572,\"start\":3554},{\"end\":3931,\"start\":3906},{\"end\":3933,\"start\":3931},{\"end\":3935,\"start\":3933},{\"end\":3951,\"start\":3935},{\"end\":4417,\"start\":4398},{\"end\":4625,\"start\":4594},{\"end\":4644,\"start\":4625},{\"end\":4895,\"start\":4867},{\"end\":5109,\"start\":5090},{\"end\":5773,\"start\":5754},{\"end\":5792,\"start\":5773},{\"end\":7536,\"start\":7510},{\"end\":7555,\"start\":7536},{\"end\":7576,\"start\":7555},{\"end\":7678,\"start\":7659},{\"end\":7954,\"start\":7940},{\"end\":8317,\"start\":8298},{\"end\":8626,\"start\":8604},{\"end\":8859,\"start\":8830},{\"end\":8967,\"start\":8938},{\"end\":9183,\"start\":9164},{\"end\":9647,\"start\":9626},{\"end\":10465,\"start\":10446},{\"end\":10484,\"start\":10465},{\"end\":11062,\"start\":11036},{\"end\":11271,\"start\":11251},{\"end\":11496,\"start\":11469},{\"end\":11519,\"start\":11496},{\"end\":11550,\"start\":11519},{\"end\":11586,\"start\":11573},{\"end\":11611,\"start\":11586},{\"end\":12726,\"start\":12707},{\"end\":14158,\"start\":14139},{\"end\":14176,\"start\":14158},{\"end\":14197,\"start\":14176},{\"end\":23154,\"start\":23135},{\"end\":23185,\"start\":23159},{\"end\":25452,\"start\":25431},{\"end\":27838,\"start\":27819},{\"end\":42855,\"start\":42833},{\"end\":42874,\"start\":42855},{\"end\":43596,\"start\":43575},{\"end\":60272,\"start\":60244},{\"end\":60508,\"start\":60487},{\"end\":61654,\"start\":61629},{\"end\":62396,\"start\":62374},{\"end\":64897,\"start\":64878},{\"end\":66998,\"start\":66975},{\"end\":2507,\"start\":2485},{\"end\":2670,\"start\":2652},{\"end\":2709,\"start\":2689},{\"end\":2750,\"start\":2730},{\"end\":2789,\"start\":2770},{\"end\":2835,\"start\":2816},{\"end\":3028,\"start\":3005},{\"end\":3050,\"start\":3028},{\"end\":3068,\"start\":3050},{\"end\":3437,\"start\":3414},{\"end\":3493,\"start\":3471},{\"end\":3512,\"start\":3493},{\"end\":3554,\"start\":3533},{\"end\":3572,\"start\":3554},{\"end\":3931,\"start\":3906},{\"end\":3933,\"start\":3931},{\"end\":3935,\"start\":3933},{\"end\":3951,\"start\":3935},{\"end\":4417,\"start\":4398},{\"end\":4625,\"start\":4594},{\"end\":4644,\"start\":4625},{\"end\":4895,\"start\":4867},{\"end\":5109,\"start\":5090},{\"end\":5773,\"start\":5754},{\"end\":5792,\"start\":5773},{\"end\":7536,\"start\":7510},{\"end\":7555,\"start\":7536},{\"end\":7576,\"start\":7555},{\"end\":7678,\"start\":7659},{\"end\":7954,\"start\":7940},{\"end\":8317,\"start\":8298},{\"end\":8626,\"start\":8604},{\"end\":8859,\"start\":8830},{\"end\":8967,\"start\":8938},{\"end\":9183,\"start\":9164},{\"end\":9647,\"start\":9626},{\"end\":10465,\"start\":10446},{\"end\":10484,\"start\":10465},{\"end\":11062,\"start\":11036},{\"end\":11271,\"start\":11251},{\"end\":11496,\"start\":11469},{\"end\":11519,\"start\":11496},{\"end\":11550,\"start\":11519},{\"end\":11586,\"start\":11573},{\"end\":11611,\"start\":11586},{\"end\":12726,\"start\":12707},{\"end\":14158,\"start\":14139},{\"end\":14176,\"start\":14158},{\"end\":14197,\"start\":14176},{\"end\":23154,\"start\":23135},{\"end\":23185,\"start\":23159},{\"end\":25452,\"start\":25431},{\"end\":27838,\"start\":27819},{\"end\":42855,\"start\":42833},{\"end\":42874,\"start\":42855},{\"end\":43596,\"start\":43575},{\"end\":60272,\"start\":60244},{\"end\":60508,\"start\":60487},{\"end\":61654,\"start\":61629},{\"end\":62396,\"start\":62374},{\"end\":64897,\"start\":64878},{\"end\":66998,\"start\":66975}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":74284,\"start\":69096},{\"attributes\":{\"id\":\"fig_3\"},\"end\":75293,\"start\":74285},{\"attributes\":{\"id\":\"fig_4\"},\"end\":75741,\"start\":75294},{\"attributes\":{\"id\":\"fig_5\"},\"end\":75939,\"start\":75742},{\"attributes\":{\"id\":\"fig_6\"},\"end\":76053,\"start\":75940},{\"attributes\":{\"id\":\"fig_7\"},\"end\":77217,\"start\":76054},{\"attributes\":{\"id\":\"fig_8\"},\"end\":77395,\"start\":77218},{\"attributes\":{\"id\":\"fig_9\"},\"end\":77720,\"start\":77396},{\"attributes\":{\"id\":\"fig_10\"},\"end\":77958,\"start\":77721},{\"attributes\":{\"id\":\"fig_11\"},\"end\":78136,\"start\":77959},{\"attributes\":{\"id\":\"fig_14\"},\"end\":78164,\"start\":78137},{\"attributes\":{\"id\":\"fig_15\"},\"end\":78736,\"start\":78165},{\"attributes\":{\"id\":\"fig_16\"},\"end\":78999,\"start\":78737},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":79502,\"start\":79000},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":82217,\"start\":79503},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":83356,\"start\":82218},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":83426,\"start\":83357},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":83561,\"start\":83427},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":89038,\"start\":83562},{\"attributes\":{\"id\":\"fig_2\"},\"end\":74284,\"start\":69096},{\"attributes\":{\"id\":\"fig_3\"},\"end\":75293,\"start\":74285},{\"attributes\":{\"id\":\"fig_4\"},\"end\":75741,\"start\":75294},{\"attributes\":{\"id\":\"fig_5\"},\"end\":75939,\"start\":75742},{\"attributes\":{\"id\":\"fig_6\"},\"end\":76053,\"start\":75940},{\"attributes\":{\"id\":\"fig_7\"},\"end\":77217,\"start\":76054},{\"attributes\":{\"id\":\"fig_8\"},\"end\":77395,\"start\":77218},{\"attributes\":{\"id\":\"fig_9\"},\"end\":77720,\"start\":77396},{\"attributes\":{\"id\":\"fig_10\"},\"end\":77958,\"start\":77721},{\"attributes\":{\"id\":\"fig_11\"},\"end\":78136,\"start\":77959},{\"attributes\":{\"id\":\"fig_14\"},\"end\":78164,\"start\":78137},{\"attributes\":{\"id\":\"fig_15\"},\"end\":78736,\"start\":78165},{\"attributes\":{\"id\":\"fig_16\"},\"end\":78999,\"start\":78737},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":79502,\"start\":79000},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":82217,\"start\":79503},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":83356,\"start\":82218},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":83426,\"start\":83357},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":83561,\"start\":83427},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":89038,\"start\":83562}]", "paragraph": "[{\"end\":3069,\"start\":2394},{\"end\":3952,\"start\":3071},{\"end\":4645,\"start\":3954},{\"end\":5569,\"start\":4647},{\"end\":5989,\"start\":5571},{\"end\":6221,\"start\":5991},{\"end\":6620,\"start\":6223},{\"end\":7312,\"start\":6622},{\"end\":9904,\"start\":7392},{\"end\":10798,\"start\":9993},{\"end\":11612,\"start\":10800},{\"end\":12624,\"start\":11656},{\"end\":14055,\"start\":12626},{\"end\":14821,\"start\":14057},{\"end\":15391,\"start\":14869},{\"end\":15523,\"start\":15474},{\"end\":15856,\"start\":15628},{\"end\":16316,\"start\":15858},{\"end\":16523,\"start\":16397},{\"end\":16825,\"start\":16649},{\"end\":17208,\"start\":17012},{\"end\":17253,\"start\":17248},{\"end\":17258,\"start\":17255},{\"end\":17477,\"start\":17410},{\"end\":17516,\"start\":17479},{\"end\":17870,\"start\":17813},{\"end\":17951,\"start\":17908},{\"end\":18522,\"start\":17953},{\"end\":19011,\"start\":18855},{\"end\":20029,\"start\":19243},{\"end\":20340,\"start\":20172},{\"end\":20981,\"start\":20455},{\"end\":21347,\"start\":20983},{\"end\":21480,\"start\":21422},{\"end\":21908,\"start\":21620},{\"end\":22253,\"start\":21949},{\"end\":22341,\"start\":22255},{\"end\":22787,\"start\":22394},{\"end\":23479,\"start\":22789},{\"end\":23745,\"start\":23559},{\"end\":24341,\"start\":23804},{\"end\":24526,\"start\":24363},{\"end\":25799,\"start\":24542},{\"end\":26028,\"start\":25891},{\"end\":26437,\"start\":26147},{\"end\":26680,\"start\":26439},{\"end\":26847,\"start\":26736},{\"end\":27338,\"start\":26921},{\"end\":27497,\"start\":27407},{\"end\":27722,\"start\":27559},{\"end\":28262,\"start\":27746},{\"end\":28707,\"start\":28296},{\"end\":29091,\"start\":28709},{\"end\":29249,\"start\":29198},{\"end\":29323,\"start\":29318},{\"end\":29341,\"start\":29325},{\"end\":29346,\"start\":29343},{\"end\":29427,\"start\":29364},{\"end\":29516,\"start\":29507},{\"end\":29583,\"start\":29518},{\"end\":29952,\"start\":29794},{\"end\":30132,\"start\":30047},{\"end\":30326,\"start\":30240},{\"end\":30442,\"start\":30328},{\"end\":30609,\"start\":30543},{\"end\":30876,\"start\":30851},{\"end\":30881,\"start\":30878},{\"end\":31065,\"start\":31006},{\"end\":31858,\"start\":31636},{\"end\":31981,\"start\":31929},{\"end\":32148,\"start\":31983},{\"end\":32294,\"start\":32175},{\"end\":32613,\"start\":32361},{\"end\":33902,\"start\":32629},{\"end\":34587,\"start\":33904},{\"end\":35648,\"start\":34589},{\"end\":36822,\"start\":35664},{\"end\":36944,\"start\":36824},{\"end\":37075,\"start\":36946},{\"end\":37191,\"start\":37077},{\"end\":37319,\"start\":37193},{\"end\":37429,\"start\":37321},{\"end\":37862,\"start\":37445},{\"end\":38258,\"start\":37883},{\"end\":38978,\"start\":38260},{\"end\":39560,\"start\":38980},{\"end\":39670,\"start\":39562},{\"end\":40110,\"start\":39672},{\"end\":40722,\"start\":40112},{\"end\":41123,\"start\":40724},{\"end\":41212,\"start\":41137},{\"end\":41454,\"start\":41214},{\"end\":41514,\"start\":41456},{\"end\":41702,\"start\":41516},{\"end\":42468,\"start\":41704},{\"end\":43350,\"start\":42507},{\"end\":43597,\"start\":43352},{\"end\":43831,\"start\":43614},{\"end\":44002,\"start\":43833},{\"end\":44068,\"start\":44004},{\"end\":44381,\"start\":44123},{\"end\":44492,\"start\":44383},{\"end\":44595,\"start\":44542},{\"end\":44714,\"start\":44636},{\"end\":44932,\"start\":44755},{\"end\":45012,\"start\":44972},{\"end\":45472,\"start\":45044},{\"end\":45728,\"start\":45474},{\"end\":45868,\"start\":45749},{\"end\":46041,\"start\":45870},{\"end\":46280,\"start\":46043},{\"end\":46500,\"start\":46282},{\"end\":46587,\"start\":46502},{\"end\":46659,\"start\":46637},{\"end\":46904,\"start\":46833},{\"end\":47032,\"start\":46987},{\"end\":47108,\"start\":47056},{\"end\":47241,\"start\":47121},{\"end\":47716,\"start\":47243},{\"end\":48264,\"start\":47890},{\"end\":48299,\"start\":48266},{\"end\":48419,\"start\":48379},{\"end\":48477,\"start\":48421},{\"end\":48608,\"start\":48515},{\"end\":48700,\"start\":48610},{\"end\":49076,\"start\":48702},{\"end\":49311,\"start\":49104},{\"end\":49346,\"start\":49313},{\"end\":49628,\"start\":49570},{\"end\":49764,\"start\":49705},{\"end\":49962,\"start\":49808},{\"end\":50227,\"start\":50180},{\"end\":50478,\"start\":50279},{\"end\":50646,\"start\":50519},{\"end\":52012,\"start\":51881},{\"end\":52221,\"start\":52153},{\"end\":52348,\"start\":52223},{\"end\":52586,\"start\":52473},{\"end\":53068,\"start\":52588},{\"end\":53211,\"start\":53145},{\"end\":53369,\"start\":53340},{\"end\":53590,\"start\":53571},{\"end\":53658,\"start\":53611},{\"end\":53851,\"start\":53660},{\"end\":54295,\"start\":53948},{\"end\":54764,\"start\":54364},{\"end\":54922,\"start\":54887},{\"end\":55030,\"start\":55011},{\"end\":55907,\"start\":55663},{\"end\":55945,\"start\":55909},{\"end\":56337,\"start\":56216},{\"end\":56577,\"start\":56493},{\"end\":56816,\"start\":56657},{\"end\":56891,\"start\":56818},{\"end\":56920,\"start\":56893},{\"end\":57250,\"start\":57100},{\"end\":57288,\"start\":57286},{\"end\":57528,\"start\":57466},{\"end\":57619,\"start\":57584},{\"end\":57685,\"start\":57643},{\"end\":57822,\"start\":57687},{\"end\":57879,\"start\":57830},{\"end\":58293,\"start\":58290},{\"end\":58311,\"start\":58295},{\"end\":58486,\"start\":58365},{\"end\":58738,\"start\":58691},{\"end\":58907,\"start\":58740},{\"end\":59345,\"start\":58909},{\"end\":59807,\"start\":59479},{\"end\":59967,\"start\":59863},{\"end\":60121,\"start\":59969},{\"end\":60230,\"start\":60123},{\"end\":60406,\"start\":60232},{\"end\":60945,\"start\":60408},{\"end\":61292,\"start\":60947},{\"end\":62036,\"start\":61294},{\"end\":62308,\"start\":62038},{\"end\":62695,\"start\":62310},{\"end\":63845,\"start\":62710},{\"end\":64488,\"start\":63847},{\"end\":65113,\"start\":64509},{\"end\":66709,\"start\":65115},{\"end\":69095,\"start\":66711},{\"end\":3069,\"start\":2394},{\"end\":3952,\"start\":3071},{\"end\":4645,\"start\":3954},{\"end\":5569,\"start\":4647},{\"end\":5989,\"start\":5571},{\"end\":6221,\"start\":5991},{\"end\":6620,\"start\":6223},{\"end\":7312,\"start\":6622},{\"end\":9904,\"start\":7392},{\"end\":10798,\"start\":9993},{\"end\":11612,\"start\":10800},{\"end\":12624,\"start\":11656},{\"end\":14055,\"start\":12626},{\"end\":14821,\"start\":14057},{\"end\":15391,\"start\":14869},{\"end\":15523,\"start\":15474},{\"end\":15856,\"start\":15628},{\"end\":16316,\"start\":15858},{\"end\":16523,\"start\":16397},{\"end\":16825,\"start\":16649},{\"end\":17208,\"start\":17012},{\"end\":17253,\"start\":17248},{\"end\":17258,\"start\":17255},{\"end\":17477,\"start\":17410},{\"end\":17516,\"start\":17479},{\"end\":17870,\"start\":17813},{\"end\":17951,\"start\":17908},{\"end\":18522,\"start\":17953},{\"end\":19011,\"start\":18855},{\"end\":20029,\"start\":19243},{\"end\":20340,\"start\":20172},{\"end\":20981,\"start\":20455},{\"end\":21347,\"start\":20983},{\"end\":21480,\"start\":21422},{\"end\":21908,\"start\":21620},{\"end\":22253,\"start\":21949},{\"end\":22341,\"start\":22255},{\"end\":22787,\"start\":22394},{\"end\":23479,\"start\":22789},{\"end\":23745,\"start\":23559},{\"end\":24341,\"start\":23804},{\"end\":24526,\"start\":24363},{\"end\":25799,\"start\":24542},{\"end\":26028,\"start\":25891},{\"end\":26437,\"start\":26147},{\"end\":26680,\"start\":26439},{\"end\":26847,\"start\":26736},{\"end\":27338,\"start\":26921},{\"end\":27497,\"start\":27407},{\"end\":27722,\"start\":27559},{\"end\":28262,\"start\":27746},{\"end\":28707,\"start\":28296},{\"end\":29091,\"start\":28709},{\"end\":29249,\"start\":29198},{\"end\":29323,\"start\":29318},{\"end\":29341,\"start\":29325},{\"end\":29346,\"start\":29343},{\"end\":29427,\"start\":29364},{\"end\":29516,\"start\":29507},{\"end\":29583,\"start\":29518},{\"end\":29952,\"start\":29794},{\"end\":30132,\"start\":30047},{\"end\":30326,\"start\":30240},{\"end\":30442,\"start\":30328},{\"end\":30609,\"start\":30543},{\"end\":30876,\"start\":30851},{\"end\":30881,\"start\":30878},{\"end\":31065,\"start\":31006},{\"end\":31858,\"start\":31636},{\"end\":31981,\"start\":31929},{\"end\":32148,\"start\":31983},{\"end\":32294,\"start\":32175},{\"end\":32613,\"start\":32361},{\"end\":33902,\"start\":32629},{\"end\":34587,\"start\":33904},{\"end\":35648,\"start\":34589},{\"end\":36822,\"start\":35664},{\"end\":36944,\"start\":36824},{\"end\":37075,\"start\":36946},{\"end\":37191,\"start\":37077},{\"end\":37319,\"start\":37193},{\"end\":37429,\"start\":37321},{\"end\":37862,\"start\":37445},{\"end\":38258,\"start\":37883},{\"end\":38978,\"start\":38260},{\"end\":39560,\"start\":38980},{\"end\":39670,\"start\":39562},{\"end\":40110,\"start\":39672},{\"end\":40722,\"start\":40112},{\"end\":41123,\"start\":40724},{\"end\":41212,\"start\":41137},{\"end\":41454,\"start\":41214},{\"end\":41514,\"start\":41456},{\"end\":41702,\"start\":41516},{\"end\":42468,\"start\":41704},{\"end\":43350,\"start\":42507},{\"end\":43597,\"start\":43352},{\"end\":43831,\"start\":43614},{\"end\":44002,\"start\":43833},{\"end\":44068,\"start\":44004},{\"end\":44381,\"start\":44123},{\"end\":44492,\"start\":44383},{\"end\":44595,\"start\":44542},{\"end\":44714,\"start\":44636},{\"end\":44932,\"start\":44755},{\"end\":45012,\"start\":44972},{\"end\":45472,\"start\":45044},{\"end\":45728,\"start\":45474},{\"end\":45868,\"start\":45749},{\"end\":46041,\"start\":45870},{\"end\":46280,\"start\":46043},{\"end\":46500,\"start\":46282},{\"end\":46587,\"start\":46502},{\"end\":46659,\"start\":46637},{\"end\":46904,\"start\":46833},{\"end\":47032,\"start\":46987},{\"end\":47108,\"start\":47056},{\"end\":47241,\"start\":47121},{\"end\":47716,\"start\":47243},{\"end\":48264,\"start\":47890},{\"end\":48299,\"start\":48266},{\"end\":48419,\"start\":48379},{\"end\":48477,\"start\":48421},{\"end\":48608,\"start\":48515},{\"end\":48700,\"start\":48610},{\"end\":49076,\"start\":48702},{\"end\":49311,\"start\":49104},{\"end\":49346,\"start\":49313},{\"end\":49628,\"start\":49570},{\"end\":49764,\"start\":49705},{\"end\":49962,\"start\":49808},{\"end\":50227,\"start\":50180},{\"end\":50478,\"start\":50279},{\"end\":50646,\"start\":50519},{\"end\":52012,\"start\":51881},{\"end\":52221,\"start\":52153},{\"end\":52348,\"start\":52223},{\"end\":52586,\"start\":52473},{\"end\":53068,\"start\":52588},{\"end\":53211,\"start\":53145},{\"end\":53369,\"start\":53340},{\"end\":53590,\"start\":53571},{\"end\":53658,\"start\":53611},{\"end\":53851,\"start\":53660},{\"end\":54295,\"start\":53948},{\"end\":54764,\"start\":54364},{\"end\":54922,\"start\":54887},{\"end\":55030,\"start\":55011},{\"end\":55907,\"start\":55663},{\"end\":55945,\"start\":55909},{\"end\":56337,\"start\":56216},{\"end\":56577,\"start\":56493},{\"end\":56816,\"start\":56657},{\"end\":56891,\"start\":56818},{\"end\":56920,\"start\":56893},{\"end\":57250,\"start\":57100},{\"end\":57288,\"start\":57286},{\"end\":57528,\"start\":57466},{\"end\":57619,\"start\":57584},{\"end\":57685,\"start\":57643},{\"end\":57822,\"start\":57687},{\"end\":57879,\"start\":57830},{\"end\":58293,\"start\":58290},{\"end\":58311,\"start\":58295},{\"end\":58486,\"start\":58365},{\"end\":58738,\"start\":58691},{\"end\":58907,\"start\":58740},{\"end\":59345,\"start\":58909},{\"end\":59807,\"start\":59479},{\"end\":59967,\"start\":59863},{\"end\":60121,\"start\":59969},{\"end\":60230,\"start\":60123},{\"end\":60406,\"start\":60232},{\"end\":60945,\"start\":60408},{\"end\":61292,\"start\":60947},{\"end\":62036,\"start\":61294},{\"end\":62308,\"start\":62038},{\"end\":62695,\"start\":62310},{\"end\":63845,\"start\":62710},{\"end\":64488,\"start\":63847},{\"end\":65113,\"start\":64509},{\"end\":66709,\"start\":65115},{\"end\":69095,\"start\":66711}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15473,\"start\":15392},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15627,\"start\":15524},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16396,\"start\":16317},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16648,\"start\":16524},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16935,\"start\":16826},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17011,\"start\":16935},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17247,\"start\":17209},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17409,\"start\":17259},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17812,\"start\":17517},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17907,\"start\":17871},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18854,\"start\":18523},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19242,\"start\":19012},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20171,\"start\":20030},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20454,\"start\":20341},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21421,\"start\":21348},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21561,\"start\":21481},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21619,\"start\":21561},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22393,\"start\":22342},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23803,\"start\":23746},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24362,\"start\":24342},{\"attributes\":{\"id\":\"formula_20\"},\"end\":26146,\"start\":26029},{\"attributes\":{\"id\":\"formula_21\"},\"end\":26735,\"start\":26681},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26920,\"start\":26848},{\"attributes\":{\"id\":\"formula_23\"},\"end\":27406,\"start\":27339},{\"attributes\":{\"id\":\"formula_24\"},\"end\":27558,\"start\":27498},{\"attributes\":{\"id\":\"formula_25\"},\"end\":27745,\"start\":27723},{\"attributes\":{\"id\":\"formula_26\"},\"end\":28295,\"start\":28263},{\"attributes\":{\"id\":\"formula_27\"},\"end\":29197,\"start\":29092},{\"attributes\":{\"id\":\"formula_28\"},\"end\":29317,\"start\":29250},{\"attributes\":{\"id\":\"formula_29\"},\"end\":29363,\"start\":29347},{\"attributes\":{\"id\":\"formula_30\"},\"end\":29506,\"start\":29428},{\"attributes\":{\"id\":\"formula_31\"},\"end\":29793,\"start\":29584},{\"attributes\":{\"id\":\"formula_32\"},\"end\":30046,\"start\":29953},{\"attributes\":{\"id\":\"formula_33\"},\"end\":30202,\"start\":30133},{\"attributes\":{\"id\":\"formula_34\"},\"end\":30239,\"start\":30202},{\"attributes\":{\"id\":\"formula_36\"},\"end\":30542,\"start\":30443},{\"attributes\":{\"id\":\"formula_37\"},\"end\":30850,\"start\":30610},{\"attributes\":{\"id\":\"formula_38\"},\"end\":31005,\"start\":30882},{\"attributes\":{\"id\":\"formula_39\"},\"end\":31635,\"start\":31066},{\"attributes\":{\"id\":\"formula_40\"},\"end\":31928,\"start\":31859},{\"attributes\":{\"id\":\"formula_41\"},\"end\":32174,\"start\":32149},{\"attributes\":{\"id\":\"formula_42\"},\"end\":32360,\"start\":32295},{\"attributes\":{\"id\":\"formula_43\"},\"end\":44122,\"start\":44069},{\"attributes\":{\"id\":\"formula_44\"},\"end\":44541,\"start\":44493},{\"attributes\":{\"id\":\"formula_45\"},\"end\":44635,\"start\":44596},{\"attributes\":{\"id\":\"formula_46\"},\"end\":44754,\"start\":44715},{\"attributes\":{\"id\":\"formula_47\"},\"end\":44971,\"start\":44933},{\"attributes\":{\"id\":\"formula_48\"},\"end\":45043,\"start\":45013},{\"attributes\":{\"id\":\"formula_49\"},\"end\":46636,\"start\":46588},{\"attributes\":{\"id\":\"formula_50\"},\"end\":46832,\"start\":46660},{\"attributes\":{\"id\":\"formula_51\"},\"end\":46986,\"start\":46905},{\"attributes\":{\"id\":\"formula_52\"},\"end\":47055,\"start\":47033},{\"attributes\":{\"id\":\"formula_53\"},\"end\":47120,\"start\":47109},{\"attributes\":{\"id\":\"formula_54\"},\"end\":47889,\"start\":47717},{\"attributes\":{\"id\":\"formula_55\"},\"end\":48357,\"start\":48300},{\"attributes\":{\"id\":\"formula_56\"},\"end\":48378,\"start\":48357},{\"attributes\":{\"id\":\"formula_57\"},\"end\":48514,\"start\":48478},{\"attributes\":{\"id\":\"formula_58\"},\"end\":49405,\"start\":49347},{\"attributes\":{\"id\":\"formula_59\"},\"end\":49479,\"start\":49405},{\"attributes\":{\"id\":\"formula_60\"},\"end\":49569,\"start\":49479},{\"attributes\":{\"id\":\"formula_61\"},\"end\":49704,\"start\":49629},{\"attributes\":{\"id\":\"formula_62\"},\"end\":49807,\"start\":49765},{\"attributes\":{\"id\":\"formula_63\"},\"end\":50160,\"start\":49963},{\"attributes\":{\"id\":\"formula_64\"},\"end\":50518,\"start\":50479},{\"attributes\":{\"id\":\"formula_65\"},\"end\":51040,\"start\":50647},{\"attributes\":{\"id\":\"formula_66\"},\"end\":51096,\"start\":51040},{\"attributes\":{\"id\":\"formula_67\"},\"end\":51298,\"start\":51096},{\"attributes\":{\"id\":\"formula_68\"},\"end\":51880,\"start\":51310},{\"attributes\":{\"id\":\"formula_69\"},\"end\":52152,\"start\":52013},{\"attributes\":{\"id\":\"formula_70\"},\"end\":52472,\"start\":52349},{\"attributes\":{\"id\":\"formula_71\"},\"end\":53144,\"start\":53069},{\"attributes\":{\"id\":\"formula_72\"},\"end\":53339,\"start\":53212},{\"attributes\":{\"id\":\"formula_73\"},\"end\":53570,\"start\":53370},{\"attributes\":{\"id\":\"formula_74\"},\"end\":53947,\"start\":53852},{\"attributes\":{\"id\":\"formula_75\"},\"end\":54363,\"start\":54296},{\"attributes\":{\"id\":\"formula_76\"},\"end\":54886,\"start\":54765},{\"attributes\":{\"id\":\"formula_77\"},\"end\":54982,\"start\":54923},{\"attributes\":{\"id\":\"formula_78\"},\"end\":55146,\"start\":55031},{\"attributes\":{\"id\":\"formula_79\"},\"end\":55196,\"start\":55146},{\"attributes\":{\"id\":\"formula_80\"},\"end\":55662,\"start\":55196},{\"attributes\":{\"id\":\"formula_82\"},\"end\":56215,\"start\":55946},{\"attributes\":{\"id\":\"formula_83\"},\"end\":56476,\"start\":56338},{\"attributes\":{\"id\":\"formula_84\"},\"end\":57093,\"start\":56921},{\"attributes\":{\"id\":\"formula_85\"},\"end\":57285,\"start\":57251},{\"attributes\":{\"id\":\"formula_86\"},\"end\":57465,\"start\":57289},{\"attributes\":{\"id\":\"formula_87\"},\"end\":57583,\"start\":57529},{\"attributes\":{\"id\":\"formula_88\"},\"end\":57642,\"start\":57620},{\"attributes\":{\"id\":\"formula_89\"},\"end\":58289,\"start\":57880},{\"attributes\":{\"id\":\"formula_90\"},\"end\":58690,\"start\":58487},{\"attributes\":{\"id\":\"formula_91\"},\"end\":59478,\"start\":59346},{\"attributes\":{\"id\":\"formula_0\"},\"end\":15473,\"start\":15392},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15627,\"start\":15524},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16396,\"start\":16317},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16648,\"start\":16524},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16935,\"start\":16826},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17011,\"start\":16935},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17247,\"start\":17209},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17409,\"start\":17259},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17812,\"start\":17517},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17907,\"start\":17871},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18854,\"start\":18523},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19242,\"start\":19012},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20171,\"start\":20030},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20454,\"start\":20341},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21421,\"start\":21348},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21561,\"start\":21481},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21619,\"start\":21561},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22393,\"start\":22342},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23803,\"start\":23746},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24362,\"start\":24342},{\"attributes\":{\"id\":\"formula_20\"},\"end\":26146,\"start\":26029},{\"attributes\":{\"id\":\"formula_21\"},\"end\":26735,\"start\":26681},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26920,\"start\":26848},{\"attributes\":{\"id\":\"formula_23\"},\"end\":27406,\"start\":27339},{\"attributes\":{\"id\":\"formula_24\"},\"end\":27558,\"start\":27498},{\"attributes\":{\"id\":\"formula_25\"},\"end\":27745,\"start\":27723},{\"attributes\":{\"id\":\"formula_26\"},\"end\":28295,\"start\":28263},{\"attributes\":{\"id\":\"formula_27\"},\"end\":29197,\"start\":29092},{\"attributes\":{\"id\":\"formula_28\"},\"end\":29317,\"start\":29250},{\"attributes\":{\"id\":\"formula_29\"},\"end\":29363,\"start\":29347},{\"attributes\":{\"id\":\"formula_30\"},\"end\":29506,\"start\":29428},{\"attributes\":{\"id\":\"formula_31\"},\"end\":29793,\"start\":29584},{\"attributes\":{\"id\":\"formula_32\"},\"end\":30046,\"start\":29953},{\"attributes\":{\"id\":\"formula_33\"},\"end\":30202,\"start\":30133},{\"attributes\":{\"id\":\"formula_34\"},\"end\":30239,\"start\":30202},{\"attributes\":{\"id\":\"formula_36\"},\"end\":30542,\"start\":30443},{\"attributes\":{\"id\":\"formula_37\"},\"end\":30850,\"start\":30610},{\"attributes\":{\"id\":\"formula_38\"},\"end\":31005,\"start\":30882},{\"attributes\":{\"id\":\"formula_39\"},\"end\":31635,\"start\":31066},{\"attributes\":{\"id\":\"formula_40\"},\"end\":31928,\"start\":31859},{\"attributes\":{\"id\":\"formula_41\"},\"end\":32174,\"start\":32149},{\"attributes\":{\"id\":\"formula_42\"},\"end\":32360,\"start\":32295},{\"attributes\":{\"id\":\"formula_43\"},\"end\":44122,\"start\":44069},{\"attributes\":{\"id\":\"formula_44\"},\"end\":44541,\"start\":44493},{\"attributes\":{\"id\":\"formula_45\"},\"end\":44635,\"start\":44596},{\"attributes\":{\"id\":\"formula_46\"},\"end\":44754,\"start\":44715},{\"attributes\":{\"id\":\"formula_47\"},\"end\":44971,\"start\":44933},{\"attributes\":{\"id\":\"formula_48\"},\"end\":45043,\"start\":45013},{\"attributes\":{\"id\":\"formula_49\"},\"end\":46636,\"start\":46588},{\"attributes\":{\"id\":\"formula_50\"},\"end\":46832,\"start\":46660},{\"attributes\":{\"id\":\"formula_51\"},\"end\":46986,\"start\":46905},{\"attributes\":{\"id\":\"formula_52\"},\"end\":47055,\"start\":47033},{\"attributes\":{\"id\":\"formula_53\"},\"end\":47120,\"start\":47109},{\"attributes\":{\"id\":\"formula_54\"},\"end\":47889,\"start\":47717},{\"attributes\":{\"id\":\"formula_55\"},\"end\":48357,\"start\":48300},{\"attributes\":{\"id\":\"formula_56\"},\"end\":48378,\"start\":48357},{\"attributes\":{\"id\":\"formula_57\"},\"end\":48514,\"start\":48478},{\"attributes\":{\"id\":\"formula_58\"},\"end\":49405,\"start\":49347},{\"attributes\":{\"id\":\"formula_59\"},\"end\":49479,\"start\":49405},{\"attributes\":{\"id\":\"formula_60\"},\"end\":49569,\"start\":49479},{\"attributes\":{\"id\":\"formula_61\"},\"end\":49704,\"start\":49629},{\"attributes\":{\"id\":\"formula_62\"},\"end\":49807,\"start\":49765},{\"attributes\":{\"id\":\"formula_63\"},\"end\":50160,\"start\":49963},{\"attributes\":{\"id\":\"formula_64\"},\"end\":50518,\"start\":50479},{\"attributes\":{\"id\":\"formula_65\"},\"end\":51040,\"start\":50647},{\"attributes\":{\"id\":\"formula_66\"},\"end\":51096,\"start\":51040},{\"attributes\":{\"id\":\"formula_67\"},\"end\":51298,\"start\":51096},{\"attributes\":{\"id\":\"formula_68\"},\"end\":51880,\"start\":51310},{\"attributes\":{\"id\":\"formula_69\"},\"end\":52152,\"start\":52013},{\"attributes\":{\"id\":\"formula_70\"},\"end\":52472,\"start\":52349},{\"attributes\":{\"id\":\"formula_71\"},\"end\":53144,\"start\":53069},{\"attributes\":{\"id\":\"formula_72\"},\"end\":53339,\"start\":53212},{\"attributes\":{\"id\":\"formula_73\"},\"end\":53570,\"start\":53370},{\"attributes\":{\"id\":\"formula_74\"},\"end\":53947,\"start\":53852},{\"attributes\":{\"id\":\"formula_75\"},\"end\":54363,\"start\":54296},{\"attributes\":{\"id\":\"formula_76\"},\"end\":54886,\"start\":54765},{\"attributes\":{\"id\":\"formula_77\"},\"end\":54982,\"start\":54923},{\"attributes\":{\"id\":\"formula_78\"},\"end\":55146,\"start\":55031},{\"attributes\":{\"id\":\"formula_79\"},\"end\":55196,\"start\":55146},{\"attributes\":{\"id\":\"formula_80\"},\"end\":55662,\"start\":55196},{\"attributes\":{\"id\":\"formula_82\"},\"end\":56215,\"start\":55946},{\"attributes\":{\"id\":\"formula_83\"},\"end\":56476,\"start\":56338},{\"attributes\":{\"id\":\"formula_84\"},\"end\":57093,\"start\":56921},{\"attributes\":{\"id\":\"formula_85\"},\"end\":57285,\"start\":57251},{\"attributes\":{\"id\":\"formula_86\"},\"end\":57465,\"start\":57289},{\"attributes\":{\"id\":\"formula_87\"},\"end\":57583,\"start\":57529},{\"attributes\":{\"id\":\"formula_88\"},\"end\":57642,\"start\":57620},{\"attributes\":{\"id\":\"formula_89\"},\"end\":58289,\"start\":57880},{\"attributes\":{\"id\":\"formula_90\"},\"end\":58690,\"start\":58487},{\"attributes\":{\"id\":\"formula_91\"},\"end\":59478,\"start\":59346}]", "table_ref": "[{\"end\":8904,\"start\":8897},{\"end\":33291,\"start\":33284},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33370,\"start\":33363},{\"end\":34219,\"start\":34199},{\"end\":34584,\"start\":34564},{\"end\":34768,\"start\":34761},{\"end\":59964,\"start\":59934},{\"end\":64541,\"start\":64512},{\"end\":65147,\"start\":65118},{\"end\":66318,\"start\":66311},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":66721,\"start\":66714},{\"end\":67681,\"start\":67674},{\"end\":8904,\"start\":8897},{\"end\":33291,\"start\":33284},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33370,\"start\":33363},{\"end\":34219,\"start\":34199},{\"end\":34584,\"start\":34564},{\"end\":34768,\"start\":34761},{\"end\":59964,\"start\":59934},{\"end\":64541,\"start\":64512},{\"end\":65147,\"start\":65118},{\"end\":66318,\"start\":66311},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":66721,\"start\":66714},{\"end\":67681,\"start\":67674}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2392,\"start\":2380},{\"attributes\":{\"n\":\"2.\"},\"end\":7390,\"start\":7315},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9951,\"start\":9907},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9991,\"start\":9954},{\"attributes\":{\"n\":\"3.\"},\"end\":11654,\"start\":11615},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14867,\"start\":14824},{\"attributes\":{\"n\":\"3.3.2.\"},\"end\":21947,\"start\":21911},{\"attributes\":{\"n\":\"3.3.3.\"},\"end\":23521,\"start\":23482},{\"end\":23557,\"start\":23524},{\"attributes\":{\"n\":\"3.3.4.\"},\"end\":24540,\"start\":24529},{\"attributes\":{\"n\":\"3.4.\"},\"end\":25889,\"start\":25802},{\"attributes\":{\"n\":\"4.\"},\"end\":32627,\"start\":32616},{\"attributes\":{\"n\":\"5.\"},\"end\":35662,\"start\":35651},{\"end\":37443,\"start\":37432},{\"end\":37881,\"start\":37865},{\"end\":41135,\"start\":41126},{\"end\":42505,\"start\":42471},{\"end\":43612,\"start\":43600},{\"end\":45747,\"start\":45731},{\"end\":49102,\"start\":49079},{\"end\":50178,\"start\":50162},{\"end\":50277,\"start\":50230},{\"end\":51309,\"start\":51300},{\"end\":53609,\"start\":53593},{\"end\":55009,\"start\":54984},{\"end\":56491,\"start\":56478},{\"end\":56655,\"start\":56580},{\"end\":57098,\"start\":57095},{\"end\":57828,\"start\":57825},{\"end\":58363,\"start\":58314},{\"end\":59861,\"start\":59810},{\"end\":62708,\"start\":62698},{\"end\":64507,\"start\":64491},{\"end\":69107,\"start\":69097},{\"end\":76072,\"start\":76055},{\"end\":77232,\"start\":77219},{\"end\":77414,\"start\":77397},{\"end\":78139,\"start\":78138},{\"end\":78739,\"start\":78738},{\"end\":82228,\"start\":82219},{\"end\":83441,\"start\":83428},{\"attributes\":{\"n\":\"1.\"},\"end\":2392,\"start\":2380},{\"attributes\":{\"n\":\"2.\"},\"end\":7390,\"start\":7315},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9951,\"start\":9907},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9991,\"start\":9954},{\"attributes\":{\"n\":\"3.\"},\"end\":11654,\"start\":11615},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14867,\"start\":14824},{\"attributes\":{\"n\":\"3.3.2.\"},\"end\":21947,\"start\":21911},{\"attributes\":{\"n\":\"3.3.3.\"},\"end\":23521,\"start\":23482},{\"end\":23557,\"start\":23524},{\"attributes\":{\"n\":\"3.3.4.\"},\"end\":24540,\"start\":24529},{\"attributes\":{\"n\":\"3.4.\"},\"end\":25889,\"start\":25802},{\"attributes\":{\"n\":\"4.\"},\"end\":32627,\"start\":32616},{\"attributes\":{\"n\":\"5.\"},\"end\":35662,\"start\":35651},{\"end\":37443,\"start\":37432},{\"end\":37881,\"start\":37865},{\"end\":41135,\"start\":41126},{\"end\":42505,\"start\":42471},{\"end\":43612,\"start\":43600},{\"end\":45747,\"start\":45731},{\"end\":49102,\"start\":49079},{\"end\":50178,\"start\":50162},{\"end\":50277,\"start\":50230},{\"end\":51309,\"start\":51300},{\"end\":53609,\"start\":53593},{\"end\":55009,\"start\":54984},{\"end\":56491,\"start\":56478},{\"end\":56655,\"start\":56580},{\"end\":57098,\"start\":57095},{\"end\":57828,\"start\":57825},{\"end\":58363,\"start\":58314},{\"end\":59861,\"start\":59810},{\"end\":62708,\"start\":62698},{\"end\":64507,\"start\":64491},{\"end\":69107,\"start\":69097},{\"end\":76072,\"start\":76055},{\"end\":77232,\"start\":77219},{\"end\":77414,\"start\":77397},{\"end\":78139,\"start\":78138},{\"end\":78739,\"start\":78738},{\"end\":82228,\"start\":82219},{\"end\":83441,\"start\":83428}]", "table": "[{\"end\":79502,\"start\":79337},{\"end\":82217,\"start\":79599},{\"end\":83356,\"start\":82483},{\"end\":83426,\"start\":83401},{\"end\":89038,\"start\":83789},{\"end\":79502,\"start\":79337},{\"end\":82217,\"start\":79599},{\"end\":83356,\"start\":82483},{\"end\":83426,\"start\":83401},{\"end\":89038,\"start\":83789}]", "figure_caption": "[{\"end\":74284,\"start\":69108},{\"end\":75293,\"start\":74287},{\"end\":75741,\"start\":75296},{\"end\":75939,\"start\":75744},{\"end\":76053,\"start\":75942},{\"end\":77217,\"start\":76074},{\"end\":77395,\"start\":77235},{\"end\":77720,\"start\":77417},{\"end\":77958,\"start\":77723},{\"end\":78136,\"start\":77961},{\"end\":78164,\"start\":78140},{\"end\":78736,\"start\":78167},{\"end\":78999,\"start\":78740},{\"end\":79337,\"start\":79002},{\"end\":79599,\"start\":79505},{\"end\":82483,\"start\":82230},{\"end\":83401,\"start\":83359},{\"end\":83561,\"start\":83444},{\"end\":83789,\"start\":83564},{\"end\":74284,\"start\":69108},{\"end\":75293,\"start\":74287},{\"end\":75741,\"start\":75296},{\"end\":75939,\"start\":75744},{\"end\":76053,\"start\":75942},{\"end\":77217,\"start\":76074},{\"end\":77395,\"start\":77235},{\"end\":77720,\"start\":77417},{\"end\":77958,\"start\":77723},{\"end\":78136,\"start\":77961},{\"end\":78164,\"start\":78140},{\"end\":78736,\"start\":78167},{\"end\":78999,\"start\":78740},{\"end\":79337,\"start\":79002},{\"end\":79599,\"start\":79505},{\"end\":82483,\"start\":82230},{\"end\":83401,\"start\":83359},{\"end\":83561,\"start\":83444},{\"end\":83789,\"start\":83564}]", "figure_ref": "[{\"end\":19478,\"start\":19470},{\"end\":19478,\"start\":19470}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}