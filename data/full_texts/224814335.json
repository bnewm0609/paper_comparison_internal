{"id": 224814335, "updated": "2023-12-14 07:52:15.741", "metadata": {"title": "Self-supervised Graph Learning for Recommendation", "authors": "[{\"first\":\"Jiancan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Fuli\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Xiangnan\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jianxun\",\"last\":\"Lian\",\"middle\":[]},{\"first\":\"Xing\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges. In this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views --- node dropout, edge dropout, and random walk --- that change the graph structure in different manners. We term this new learning paradigm asSelf-supervised Graph Learning (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \\urlhttps://github.com/wujcan/SGL.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3094605801", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/WuWF0CLX21", "doi": "10.1145/3404835.3462862"}}, "content": {"source": {"pdf_hash": "75739ed2ddebd7982042f516f407553f8d3110f8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.10783v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2010.10783", "status": "GREEN"}}, "grobid": {"id": "3b94681eb0983b0da975a7fe6dbdbf353c4abd4b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/75739ed2ddebd7982042f516f407553f8d3110f8.txt", "contents": "\nVirtual Event\nVirtual Event, Canada. ACMCopyright Virtual Event, Canada. ACMJuly 11-15, 2021. July 11-15, 2021\n\nJiancan Wu \nUniversity of Science and Technology of China\nChina\n\nXiang Wang \nNational University of Singapore\nSingapore\n\nFuli Feng \nNational University of Singapore\nSingapore\n\nXiangnan He xiangnanhe@gmail.com \nUniversity of Science and Technology of China\nChina\n\nLiang Chen chenliang6@mail.sysu.edu.cn \nSun Yat-sen University\nChina\n\nJianxun Lian \nMicrosoft Research Asia\nChina\n\nXing Xie xing.xie@microsoft.com \nMicrosoft Research Asia\nChina\n\nJiancan Wu \nXiang Wang \nFuli Feng \nXiangnan He \nLiang Chen \nJianxun Lian \nXing Xie \nVirtual Event\n\nCanada Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)\nNew York, NY, USAVirtual Event, Canada. ACM11July 11-15, 2021. July 11-15, 202110.1145/3404835.3462862* Xiang Wang is the corresponding author. ACM ISBN 978-1-4503-8037-9/21/07. . . $15.00 2021. Self-supervised Graph Learning for Recommendation. InCollaborative filteringGraph Neural NetworkSelf-supervised LearningLong-tail Recommendation\nRepresentation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and(2)representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.In this work, we explore self-supervised learning on useritem graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary selfsupervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views -node dropout, edge dropout, and random walk -that change the graph structure in different manners. We term this new learning paradigm as Self-supervised Graph Learning (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at https://github.com/wujcan/SGL.CCS CONCEPTS\u2022 Information systems \u2192 Recommender systems.\n\nINTRODUCTION\n\nLearning high-quality user and item representations from interaction data is the theme of collaborative recommendation. Earlier work like matrix factorization (MF) [34] projects single ID of each user (or item) into an embedding vector. Some follow-on studies [20,25] enrich the single ID with interaction history for learning better representations. More recently, representation learning has evolved to exploiting higher-order connectivity in user-item graph. The technique is inspired from the graph convolution networks (GCNs), which provide an end-to-end way to integrate multi-hop neighbors into node representation learning and achieve state-of-the-art performance for recommendation [19,38,46,50].\n\nDespite effectiveness, current GCN-based recommender models suffer from some limitations:\n\n\u2022 Sparse Supervision Signal. Most models approach the recommendation task under a supervised learning paradigm [19,21,34], where the supervision signal comes from the observed user-item interactions. However, the observed interactions are extremely sparse [3,18] compared to the whole interaction space, making it insufficient to learn quality representations. \u2022 Skewed Data Distribution. Observed interactions usually exhibit a power-law distribution [9,30], where the long tail consists of low-degree items that lack supervision signal. In contrast, high-degree items appear more frequently in neighborhood aggregation and supervised loss, thus exert larger impact on representation learning. Hence, the GCNs are easily biased towards high-degree items [5,36], sacrificing the performance of low-degree (long-tail) items. \u2022 Noises in Interactions. Most feedback that a user provides is implicit (e.g., clicks, views), instead of explicit (e.g., ratings, likes/dislikes). As such, observed interactions usually contain noises, e.g., a user is misled to click an item and finds it uninteresting after consuming it [44]. The neighborhood aggregation scheme in GCNs enlarges the impact of interactions on representation learning, making the learning more vulnerable to interaction noises. In this work, we focus on exploring self-supervised learning (SSL) in recommendation, to solve the foregoing limitations. Though being prevalent in computer vision (CV) [11,40] and natural language processing (NLP) [10,26], SSL is relatively less explored in recommendation. The idea is to set an auxiliary task that distills additional signal from the input data itself, especially through exploiting the unlabeled data space. For example, BERT [10] randomly masks some tokens in a sentence, setting the prediction of the masked tokens as the auxiliary task that can capture the dependencies among tokens; RotNet [11] randomly rotates labeled images, training the model on the rotated images to get improved representations for the mask task of object recognition or image classification. Compared with supervised learning, SSL allows us to exploit the unlabeled data space via making changes on the input data, achieving remarkable improvements in downstream tasks [6].\n\nHere we wish to bring the SSL's superiority into recommendation representation learning, which differs from CV/NLP tasks since the data are discrete and inter-connected. To address the aforementioned limitations of GCN-based recommendation models, we construct the auxiliary task as discriminating the representation of a node itself. Specifically, it consists of two key components: (1) data augmentation, which generates multiple views for each node, and (2) contrastive learning, which maximizes the agreement between different views of the same node, compared to that of other nodes. For GCNs on user-item graph, the graph structure serves as the input data that plays an essential role for representation learning. From this view, it is natural to construct the \"unlabeled\" data space by changing the graph adjacency matrix, and we develop three operators to this end: node dropout, edge dropout, and random walk, where each operator works with a different rationality. Thereafter, we perform contrastive learning based on the GCNs on the changed structure. As a result, SGL augments the node representation learning by exploring the internal relationship among nodes.\n\nConceptually, our SGL supplements existing GCN-based recommendation models in: (1) node self-discrimination offers auxiliary supervision signal, which is complementary to the classical supervisions from observed interactions only; (2) the augmentation operators, especially edge dropout, helps to mitigate the degree biases by intentionally reducing the influence of highdegree nodes; (3) the multiple views for nodes w.r.t. different local structures and neighborhoods enhance the model robustness against interaction noises. Last but not least, we offer theoretical analyses for the contrastive learning paradigm, finding that it has the side effect of mining hard negative examples, which not only boosts the performance but also accelerates the training process.\n\nIt is worthwhile mentioning that our SGL is model-agnostic and can be applied to any graph-based model that consists of user and/or item embedding. Here we implement it on the simple but effective model, LightGCN [19]. Experimental studies on three benchmark datasets demonstrate the effectiveness of SGL, which significantly improves the recommendation accuracy, especially on long-tail items, and enhance the robustness against interaction noises. We summarize the contributions of this work as follows:\n\n\u2022 We devise a new learning paradigm, SGL, which takes node selfdiscrimination as the self-supervised task to offer auxiliary signal for representation learning. \u2022 In addition to mitigating degree bias and increasing robustness to interaction noises, we prove in theory that SGL inherently encourages learning from hard negatives, controlled by the temperature hyper-parameter in the softmax loss function. \u2022 We conduct extensive experiments on three benchmark datasets to demonstrate the superiority of SGL.\n\n\nPRELIMINARIES\n\nWe first summarize the common paradigm of GCN-based collaborative filtering models. Let U and I be the set of users and items respectively. Let O + = { | \u2208 U, \u2208 I} be the observed interactions, where indicates that user has adopted item before. Most existing models [19,38,46] construct a bipartite graph G = (V, E), where the node set V = U \u222a I involves all users and items, and the edge set E = O + represents observed interactions. Recap GCN. At the core is to apply the neighborhood aggregation scheme on G, updating the representation of ego node by aggregating the representations of neighbor nodes:\nZ ( ) = (Z ( \u22121) , G)(1)\nwhere Z ( ) denotes the node representations at the -th layer, Z ( \u22121) is that of previous layer, and Z (0) is the ID embeddings (trainable parameters). denotes the function for neighborhood aggregation, which is more interpretable from the vector level:\nz ( ) = combine (z ( \u22121) , aggregate ({z ( \u22121) | \u2208 N })).(2)\nTo update the representation of ego node at the -th layer, it first aggregates the representations of its neighbors N at the ( \u2212 1)-th layer, then combines with its own representation z ( \u22121) . There are a number of designs for aggregate (\u00b7) and combine (\u00b7) [12,15,41,48].\n\nWe can see that the representations of the -th layer encode the -order neighbors in the graph. After obtaining layers representations, there may be a readout function to generate the final representations for prediction:\nz = readout {z ( ) | = [0, \u00b7 \u00b7 \u00b7 , ]} .(3)\nCommon designs include last-layer only [38,50], concatenation [46], and weighted sum [19]. Supervised Learning Loss. A prediction layer is built upon the final representations to predict how likely would adopt . A classical solution is the inner product, which supports fast retrieval:\n= z \u22a4 z .(4)\nTo optimize model parameters, existing works usually frame the task as one of supervised learning, where the supervision signal comes from the observed interactions (also the edges of G). For example, encouraging the predicted value\u02c6to be close to the ground truth value and selecting negative examples from missing data [21]. Besides above point-wise learning, another common choice is the pairwise Bayesian Personalized Ranking (BPR) loss [34], which enforces the prediction of an observed interaction to be scored higher than its unobserved counterparts:  \nL = \u2211\ufe01 ( , , ) \u2208 O \u2212 log (\u02c6\u2212\u02c6),(5)\n\nMETHODOLOGY\n\nWe present the proposed Self-supervised Graph Learning (SGL) paradigm, which supercharges the main supervised task with self-supervised learning. Figure 1 illustrates the working flow of SGL. Specifically, the self-supervised task (also termed as pretext task or auxiliary task) is to construct supervision signal from the correlation within the input data. Specifically, we introduce how to perform data augmentation that generates multiple representation views, followed by the contrastive learning based on the generated representations to build the pretext task. SSL is combined with classical GCN in a multi-task learning manner. Thereafter, we conduct theoretical analyses on SSL from the gradient level, revealing the connection with hard negative mining. Lastly, we analyze the complexity of SGL.\n\n\nData Augmentation on Graph Structure\n\nDirectly grafting the data augmentation adopted in CV and NLP tasks [6,10,17,47] is infeasible for graph-based recommendation, due to specific characteristics: (1) The features of users and items are discrete, like one-hot ID and other categorical variables. Hence, the augmentation operators on images, such as random crop, rotation, or blur, are not applicable. (2) More importantly, unlike CV and NLP tasks that treat each data instance as isolated, users and items in the interaction graph are inherently connected and dependent on each others. Thus, we need new augmentation operators tailored for graph-based recommendation.\n\nThe bipartite graph is built upon observed user-item interactions, thus containing the collaborative filtering signal. Specifically, the first-hop neighborhood directly profiles ego user and item nodesi.e., historical items of a user (or interacted users of an item) can be viewed as the pre-existing features of user (or item). The secondhop neighboring nodes of a user (or an item) exhibit similar users w.r.t. behaviors (or similar items w.r.t. audiences). Furthermore, the higher-order paths from a user to an item reflect potential interests of the user on the item. Undoubtedly, mining the inherent patterns in graph structure is helpful to representation learning. We hence devise three operators on the graph structure, node dropout, edge dropout and random walk, to create different views of nodes. The operators can be uniformly expressed as follows: and Random Walk (right). For Random Walk, the graph structure keeps changing across layers as opposed to Edge Dropout. As a result, there exists a three-order path between node 1 and 1 that does not exist in Edge Dropout.\nZ ( ) 1 = (Z ( \u22121\nwhere two stochastic selections 1 and 2 are independently applied on graph G, and establish two correlated views of nodes Z ( ) 1 and Z ( ) 2 . We elaborate the augmentation operators as follows:\n\n\u2022 Node Dropout (ND). With the probability , each node is discarded from the graph, together with its connected edges.\n\nIn particular, 1 and 2 can be modeled as:\n1 (G) = (M \u2032 \u2299 V, E), 2 (G) = (M \u2032\u2032 \u2299 V, E),(7)\nwhere M \u2032 , M \u2032\u2032 \u2208 {0, 1} |V | are two masking vectors which are applied on the node set V to generate two subgraphs. As such, this augmentation is expected to identify the influential nodes from differently augmented views, and make the representation learning less sensitive to structure changes. \u2022 Edge Dropout (ED). It drops out the edges in graph with a dropout ratio . Two independent processes are represented as:\n1 (G) = (V, M 1 \u2299 E), 2 (G) = (V, M 2 \u2299 E),(8)\nwhere M 1 , M 2 \u2208 {0, 1} | E | are two masking vectors on the edge set E. Only partial connections within the neighborhood contribute to the node representations. As such, coupling these two subgraphs together aims to capture the useful patterns of the local structures of a node, and further endows the representations more robustness against noisy interactions. \u2022 Random Walk (RW). The above two operators generate a subgraph shared across all the graph convolution layers. To explore higher capability, we consider assigning different layers with different subgraphs. This can be seen as constructing an individual subgraph for each node with random walk [31] (see Figure 2 as an example). Assuming we choose edge dropout at each layer (with different ratio or random seeds), we can formulate RW by making the masking vector to be layer sensitive:\n1 (G) = (V, M ( ) 1 \u2299 E), 2 (G) = (V, M ( ) 2 \u2299 E),(9)\nwhere M ( )\n1 , M ( ) 2 \u2208 {0,\n1} | E | are two masking vectors on the edge set E at layer .\n\nWe apply these augmentations on graph structure per epoch for simplicity -that is, we generate two different views of each node at the beginning of a new training epoch (for RW, two different views are generated at each layer). Note that the dropout and masking ratios remain the same for two independent processes (i.e., 1 and worthwhile mentioning that only dropout and masking operations are involved, and no any model parameters are added.\n\n\nContrastive Learning\n\nHaving established the augmented views of nodes, we treat the views of the same node as the positive pairs (i.e., {(z \u2032 , z \u2032\u2032 )| \u2208 U}), and the views of any different nodes as the negative pairs (i.e., {(z \u2032 , z \u2032\u2032 )| , \u2208 U, \u2260 }). The auxiliary supervision of positive pairs encourages the consistency between different views of the same node for prediction, while the supervision of negative pairs enforces the divergence among different nodes. Formally, we follow SimCLR [6] and adopt the contrastive loss, InfoNCE [14], to maximize the agreement of positive pairs and minimize that of negative pairs:\nL = \u2211\ufe01 \u2208U \u2212 log exp( (z \u2032 , z \u2032\u2032 )/ ) \u2208U exp( (z \u2032 , z \u2032\u2032 )/ ) ,(10)\nwhere (\u00b7) measures the similarity between two vectors, which is set as cosine similarity function; is the hyper-parameter, known as the in softmax. Analogously, we obtain the contrastive loss of the item side L . Combining these two losses, we get the objective function of self-supervised task as L = L + L .\n\n\nMulti-task Training\n\nTo improve recommendation with the SSL task, we leverage a multi-task training strategy to jointly optimize the classic recommendation task (cf. Equation (5)) and the self-supervised learning task (cf. Equation (10))\nL = L + 1L + 2 \u2225\u0398\u2225 2 2 ,(11)\nwhere \u0398 is the set of model parameters in since introduces no additional parameters; 1 and 2 are hyperparameters to control the strengths of SSL and 2 regularization, respectively. We also consider the alternative optimization -pre-training on L and fine-tuning on L . See more details in Section 4.4.2.\n\n\nTheoretical Analyses of SGL\n\nIn this section, we offer in-depth analyses of SGL, aiming to answer the question: how does the recommender model benefit from the SSL task? Towards this end, we probe the self-supervised loss in Equation (10) and find one reason: it has the intrinsic ability to perform hard negative mining, which contributes large and meaningful gradients to the optimization and guides the node representation learning. In what follows, we present our analyses step by step. Formally, for node \u2208 U, the gradient of the self-supervised loss w.r.t. the representation \u2032 is as follows:\nL ( ) \u2032 = 1 \u2225 \u2032 \u2225 ( ) + \u2211\ufe01 \u2208U\\{ } ( ) ,(12)\nwhere L ( ) is the individual term for a single node in Equation (10); \u2208 U \\ { } is another node which serves as the negative view for node ; ( ) and ( ) separately represent the contribution of positive node and negative nodes { } to the Figure 3: Function curve of ( ) when = 1 and = 0.1, together with the logarithm of the maximum value of ( ) w.r.t. and its optimal position, i.e., ln ( * ) and * ( ).\n(a) ( ), = 1 (b) ( ), = 0.1 (c) * ( ) (d) ln ( * )\ngradients w.r.t. \u2032 :\n( ) = \u2032\u2032 \u2212 ( \u2032 \u2032\u2032 ) \u2032 ( \u2212 1),(13)( ) = \u2032\u2032 \u2212 ( \u2032 \u2032\u2032 ) \u2032 ,(14)\nwhere\n= exp( \u2032 \u2032\u2032 / ) \u2208U exp( \u2032 \u2032\u2032 / )\n; \u2032 = \u2032 \u2225 \u2032 \u2225 and \u2032\u2032 = \u2032\u2032 \u2225 \u2032\u2032 \u2225 are the normalized representations of node from different views; similar notations to node . Afterwards, we focus on the contribution of negative node (cf. Equation (14)), the 2 norm of which is proportional to the following term:\n\u2225 ( )\u2225 2 \u221d \u221a\ufe03 1 \u2212 ( \u2032 \u2032\u2032 ) 2 exp( \u2032 \u2032\u2032 / ).(15)\nAs \u2032 and \u2032\u2032 are both unit vectors, we can introduce another variable = \u2032 \u2032\u2032 \u2208 [\u22121, 1] to simplify Equation (15) as follows:\n( ) = \u221a\ufe01 1 \u2212 2 exp ,(16)\nwhere directly reflects the representation similarity between the positive node and the negative node . According to the similarity , we can roughly categorize the negative nodes into two groups: (1) Hard negative nodes, whose representations are similar to that of the positive node (i.e., 0 < \u2264 1), thus making it difficult to distinguish from in the latent space; (2) Easy negative nodes, which are dissimilar to the positive node (i.e., \u22121 \u2264 < 0) and can be easily discriminated. To investigate the contributions of hard and easy negatives, we plot the curves of ( ) over the change of the node similarity in Figures 3a and 3b, by setting = 1 and = 0.1 respectively. Clearly, under different conditions of , the contributions of negative nodes differ dramatically with each others. Specifically, as Figure 3a shows, given = 1, the values of ( ) fall into the range of (0, 1.5) and slightly change in response to . This suggests that negative samples, no matter hard or easy, contribute similarly to the gradient. In contrast, as Figure 3b displays, when setting = 0.1, the values of ( ) at hard negatives could reach 4, 000, while the contribution of easy negatives is vanishing. This indicates that hard negative nodes offer much larger gradients to guide the optimization, thus making node representations more discriminative and accelerating the training process [33].\n\nThese findings inspire us to probe the influence of on the maximum value of ( ). By approaching ( )'s derivative to zero, we can obtain * = ( \u221a 2 + 4 \u2212 )/2 with maximum value ( * ). To see how ( * ) changes with , we represent its logarithm as:\nln ( * ) = ln 1 \u2212 \u221a 2 + 4 \u2212 2 2 exp \u221a 2 + 4 \u2212 2 .(17)\nWe present the curves of * and ln ( * ) in Figures 3c and  3d, respectively. With the decrease of , the most influential negative nodes become more similar to the positive node (i.e., * approaches 0.9), moreover, their contributions are amplified super-exponentially (i.e., * is close to 8 ). Hence, properly setting enables SGL to automatically perform hard negative mining. It is worth mentioning that, our analyses are inspired by the prior study [24], but there are major differences: (1) [24] defines hard negatives as samples whose similarity are near-to-zero to the positive sample (i.e., \u2248 0), and easy negatives as obviously dissimilar samples (i.e., \u2248 \u22121); (2) It leaves the region of > 0 untouched, which is however crucial in our case. Innovatively, we provide a more fine-grained view in the region of > 0 to highlight the critical role of , the temperature hyper-parameter in softmax, in mining hard negatives.\n\n\nComplexity Analyses of SGL\n\nIn this subsection, we analyze the complexity of SGL with ED as the strategy and LightGCN as the recommendation model; other choices can be analyzed similarly. Since SGL introduces no trainable parameters, the space complexity remains the same as LightGCN [19]. The time complexity of model inference is also the same, since there is no change on the model structure. In the following part, we will analyze the time complexity of SGL training.\n\nSuppose the number of nodes and edges in the user-item interaction graph are | | and | | respectively. Let denote the number of epochs, denote the size of each training batch, denote the embedding size, denote the number of GCN layers,\u02c6= 1 \u2212 denote the keep probability of SGL-ED. The complexity mainly comes from two parts:\n\n\u2022 Normalization of adjacency matrix. Since we generate two independent sub-graphs per epoch, given the fact that the number of non-zero elements in the adjacency matrices of full training graph and two sub-graph are 2 | | , 2\u02c6| | and 2\u02c6| | respectively, its total complexity is (4\u02c6| | + 2 | |). \u2022 Evaluating self-supervised loss. We only consider the inner product in our analyses. As defined in Equation (10), we treat all other user nodes as negative samples when calculating InfoNCE loss of user side. Within a batch, the complexity of numerator and denominator are ( ) and ( ), respectively, where is the number of users. And hence the total complexity of both user and item side per epoch is (| | (2 + | |)). Therefore, the time complexity of the whole training phase is (| | (2 + | |) ). An alternative to reduce the time complexity is treating only the users (or the items) within the batch as negative samples [6,49], resulting in total time complexity of (| | (2 + 2 ) ). We summarize the time complexity in training between LightGCN and SGL-ED in Table 1. The analytical complexity of LightGCN  \nGraph Convolution (2 | | | | ) (2(1 + 2\u02c6) | | | | ) BPR Loss (2 | | ) (2 | | ) Self-supervised Loss - (| | (2 + | |) ) (| | (2 + 2 ) )\n\nEXPERIMENTS\n\nTo justify the superiority of SGL and reveal the reasons of its effectiveness, we conduct extensive experiments and answer the following research questions:\n\n\u2022 RQ1: How does SGL perform w.r.t. top-recommendation as compared with the state-of-the-art CF models? \u2022 RQ2: What are the benefits of performing self-supervised learning in collaborative filtering? \u2022 RQ3: How do different settings influence the effectiveness of the proposed SGL?\n\n\nExperimental Settings\n\nWe conduct experiments on three benchmark datasets: Yelp2018 [19,46], Amazon-Book [19,46], and Alibaba-iFashion [8] 1 . Following [19,46], we use the same 10-core setting for Yelp2018 and Amazon-Book. Alibaba-iFashion is more sparse, where we randomly sample 300k users and use all their interactions over the fashion outfits. The statistics of all three datasets are summarized in Table 2. We follow the same strategy described in [46] to split the interactions into training, validation, and testing set with a ratio of 7:1:2. For users in the testing set, we follow the all-ranking protocol [46] to evaluate the top-recommendation performance and report the average Recall@ and NDCG@ where = 20.\n\n\nCompared Methods.\n\nWe compare the proposed SGL with the following CF models: \u2022 NGCF [46]. This is a graph-based CF method largely follows the standard GCN [12]. It additionally encodes the second-order feature interaction into the message during message passing. We tune the regularization coefficient 2 and the number of GCN layers within the suggested ranges. \u2022 LightGCN [19]. This method devises a light graph convolution for training efficiency and generation ability. Similarly, we tune the 2 and the number of GCN layers. \u2022 Mult-VAE [28]. This is an item-based CF method based on the variational auto-encoder (VAE). It is optimized with an additional reconstruction objective, which can be seen as a special case of SSL. We follow the suggested model setting and tune the dropout ratio and . \u2022 DNN+SSL [49]. This is a state-of-the-art SSL-based recommendation method. With DNNs as the encoder of items, it adopts two augmentation operators, feature masking (FM) and feature dropout (FD), on the pre-existing features of items. In our cases where no item feature is available, we apply the augmentations on ID embeddings of items instead. We tune the DNN architecture (i.e., the number of layers and the number of neurons per layer) as suggested in the original paper.\n\nWe discard potential baselines like MF [34], NeuMF [21], GC-MC [38], and PinSage [50] since the previous work [19,28,46] has validated the superiority over the compared ones. Upon LightGCN, we implement three variants of the proposed SGL, named SGL-ND, SGL-ED, and SGL-RW, which are equipped with Node Dropout, Edge Dropout, and Random Walk, respectively.  Table 3 shows the result comparison between SGL and LightGCN. We find that:\n\n\nPerformance Comparison (RQ1)\n\n\nComparison with LightGCN.\n\n\u2022 In most cases, three SGL implementations outperform LightGCN by a large margin, indicating the superiority of supplementing the recommendation task with self-supervised learning. \u2022 In SGL family, SGL-ED achieves best performance in 10 over 18 cases, while SGL-RW also performs better than SGL-ND across all three datasets. We ascribe these to the ability of edge dropoutlike operators to capture inherent patterns in graph structure. Moreover, the performance of SGL-ED is better than that of SGL-RW in the denser datasets (Yelp2018 and Amazon-Book), while slightly worse in the sparser dataset (Alibaba-iFashion).\n\nOne possible reason is that, in sparser datasets, ED is more likely to blocks connections of low-degree nodes (inactive users and unpopular items), while RW could restore their connections at different layers, as the case of node 1 and 1 shown in Figure 2. \u2022 SGL-ND is relatively unstable than SGL-ED and SGL-RW. For example, on Yelp2018 and Amazon-Book, the results of SGL-ED and SGL-RW increases when layers go deeper while SGL-ND exhibits different patterns. Node Dropout can be viewed as a special case of Edge Dropout, which discards edges around a few nodes. Hence, dropping high-degree nodes will dramatically change the graph structure, thus exerts influence on the information aggregation and makes the training unstable. \u2022 The improvements on Amazon-Book and Alibaba-iFashion are more significant than that on Yelp2018. This might be caused by the characteristics of datasets. Specifically, in Amazon-Book and Alibaba-iFashion, supervision signal from user-item interactions is too sparse to guide the representation learning in LightGCN.\n\nBenefiting from the self-supervised task, SGL obtains auxiliary supervisions to assist the representation learning. \u2022 Increasing the model depth from 1 to 3 is able to enhance the performance of SGL. This indicates that exploiting SSL could empower the generalization ability of GCN-based recommender models -that is, the contrastive learning among different nodes is of promise to solving the oversmoothing issue of node representations, further avoiding the overfitting problem.   Table 4, we summarize the performance comparison with various baselines. We find that: (1) SGL-ED consistently outperforms all baselines across the board. This again verifies the rationality and effectiveness of incorporating the self-supervised learning. (2) LightGCN achieves better performance than NGCF and Mult-VAE, which is consistent to the claim of LightGCN paper. The performance of Mult-VAE is on par with NGCF and LightGCN on Alibaba-iFashion, while outperforms NGCF on Amazon-Book. (3) DNN+SSL is the strongest baseline on Amazon-Book, showing the great potential of SSL in recommendation. Surprisingly, on the other datasets, DNN+SSL performs much worse than SGL-ED. This suggests that directly applying SSL on ID embeddings might be suboptimal and inferior than that on graph representations. (4) Moreover, we conduct the significant test, where -value < 0.05 indicates that the improvements of SGL-ED over the strongest baseline are statistically significant in all six cases.\n\n\nBenefits of SGL (RQ2)\n\nIn this section, we study the benefits of SGL from three dimensions:\n\n(1) long-tail recommendation; (2) training efficiency; and (3) robustness to noises. Due to the limited space, we only report the results of SGL-ED, while having similar observations in others.\n\n\nLong-tail Recommendation.\n\nAs Introduction mentions, GNN-based recommender models easily suffer from the long-tail problem. To verify whether SGL is of promise to solving the problem, we split items into ten groups based on the popularity, meanwhile keeping the total number of interactions of each group the same. The larger the GroupID is, the larger degrees the items have. We then decompose the Recall@20 metric of the whole dataset into contributions of single groups, as follows:\n= 1 \u2211\ufe01 =1 10 =1 ( ) ( ) \u2229 = 10 \u2211\ufe01 =1 ( )\nwhere is the number of users, and are the items in the top-recommendation list and relevant items in the testing set for user , respectively. As such, ( ) measures the recommendation performance over the -th group. We report the results in Figure 4 and find that:\n\n\u2022 LightGCN is inclined to recommend high-degree items, while leaving long-tail items less exposed. Specifically, although only containing 0.83%, 0.83% and 0.22% of item spaces, the 10-th group contributes 39.72%, 39.92% and 51.92% of the total Recall scores in three datasets, respectively. This admits that, LightGCN hardly learns high-quality representations of long-tail items, due to the sparse interaction signal. Our SGL shows potentials in alleviating this issue: the contributions of the 10-group downgrade to 36.27%, 29.15% and 35.07% in three datasets, respectively. \u2022 Jointly analyzing Table. 3 and Figure 4, we find the performance improvements of SGL mainly come from accurately recommending the items with sparse interactions. This again verifies that the representation learning benefits greatly from auxiliary supervisions, so as to establish better representations of these items than LightGCN. 4.3.2 Training Efficiency. Self-supervised learning has proved its superiority in pre-training natural language model [10] and graph structure [23,31]. Thus, we would like to study its influence on training efficiency. Figure 5 shows the training curves of SGL-ED and LightGCN on Yelp2018 and Amazon-Book. As the number of epochs increases, the upper subfigures display the changes of training loss, while the bottoms record the performance changes in the testing set. We have the following observations:\n\n\u2022 Obviously, SGL is much faster to converge than LightGCN on Yelp2018 and Amazon-Book. In particular, SGL arrives at the best performance at the 18-th and 16-th epochs, while LightGCN takes 720 and 700 epochs in these two datasets respectively. This suggests that our SGL can greatly reduce the training time, meanwhile achieves remarkable improvements. We ascribe such speedups to two factors: (1) SGL adopts the InfoNCE loss as the SSL objective which enables the model to learn representations from multiple negative samples, while the BPR loss in LightGCN only uses one negative sample which limits the model's perception field; (2) As analyzed in Section 3.4, with a proper , SGL benefits from dynamic hard negative mining, where the hard negative samples offer meaningful and larger gradients to guide the optimization [54]. \u2022 Another observation is that the origin of the rapid-decline period of BPR loss is slightly later than the rapid-rising period of Recall. This indicates the existence of a gap between the BPR loss and ranking task. We will conduct in-depth research on this phenomenon in our future work.\n\n\nRobustness to Noisy\n\nInteractions. We also conduct experiments to check SGL's robustness to noisy interactions. Towards this end, we contaminate the training set by adding a certain proportion of adversarial examples (i.e., 5%, 10%, 15%, 20% negative user-item interactions), while keeping the testing set unchanged. Figure 6 shows the results on Yelp2018 and Amazon-Book datasets.\n\n\u2022 Clearly, adding noise data reduces the performance of SGL and LightGCN. However, the performance degradation of SGL is lower than that of LightGCN; moreover, as the noise ratio increases, the gap between two degradation curves becomes more apparent. This suggests that, by comparing differently augmented views of nodes, SGL is able to figure out useful patterns, especially informative graph structures of nodes, and reduce dependence on certain edges. In a nutshell, SGL offers a different angle to denoise false positive interactions in recommendation. \u2022 Focusing on Amazon-Book, the performance of SGL with 20% additional noisy interactions is still superior to LightGCN with noise-free dataset. This further justifies the superiority and robustness of SGL over LightGCN.  \u2022 We find that SGL is more robust on Yelp2018. The possible reason may be that Amazon-Book is much sparser than Yelp2018, and adding noisy data exerts more influence on graph structure of Amazon-Book than that of Yelp2018.\n\n\nStudy of SGL (RQ3)\n\nWe move on to studying different designs in SGL. We first investigate the impact of hyper-parameter . We then explore the potential of adopting SGL as a pretraining for existing graph-based recommendation model. Lastly, we study the influence of negatives in the SSL objective function. Due to the space limitation, we omit the results on iFashion which have a similar trend to that on Yelp2018 and Amazon-Book.\n\n4.4.1 Effect of Temperature . As verified in Section 3.4, play a critical role in hard negative mining. Figure 7 shows the curves of model performance w.r.t. different . We can observe that: (1) Increasing the value of (e.g., 1.0) will lead to poorer performance and take more training epochs to converge, which fall short in the ability to distinguish hard negatives from easy negatives. (2) In contrast, fixing to a too small value (e.g., 0.1) will hurt the model performance, since the gradients of a few negatives dominate the optimization, losing the supremacy of adding multiple negative samples in the SSL objective. In a nutshell, we suggest to tun in the range of 0.1 \u223c 1.0 carefully.\n\n\nEffect of Pre-training.\n\nThe foregoing experiments have shown the effectiveness of SGL, where the main supervised task and the self-supervised task are jointly optimized. Here we would like to answer the question: Can the recommendation performance benefit from the pre-trained model? Towards this goal, we first pre-train the self-supervised task to obtain the model parameters, use them to initialize LightGCN, and then fine-tune the model via optimizing the main task. We term this variant as SGL-pre and show the comparison with SGL in Table 5. Clearly, although SGLpre performs worse than SGL-ED on both datasets, the results of SGL-pre are still better than that of LightGCN (cf. Table 3). Our selfsupervised task is able to offer a better initialization for LightGCN, which is consistent to the observation in previous studies [6]. However, the better performance of joint training admits that the representations in the main and auxiliary tasks are mutually enhanced with each other.\n\n\n4.4.3\n\nEffect of Negatives. Moreover, we also study the effect of the choice of negative samples in the auxiliary task. Two variants are considered: (1) SGL-ED-batch, which differentiates node types and treat users and items in a mini-batch as negative views for users and items, separately, and (2) SGL-ED-merge, which treat nodes within a mini-batch as negative, without differentiating the node types. We report the comparison in Table 5. SGL-ED-batch performs better than SGL-ED-merge, which indicates the necessity for distinguishing types of heterogeneous nodes. Moreover, SGL-ED-batch is on a par with SGL-ED that treats the whole spaces of users and items as negative. It suggests that training the SSL task in mini-batching is an efficient alternative.\n\n\nRELATED WORK\n\nIn this section, we separately review two tasks related to our work: graph-based recommendation and self-supervised learning.\n\n\nGraph-based Recommendation\n\nPrevious studies on graph-based recommendation can be categorized into: model-level and graph-level methods, regarding their focus. The model-level methods focus on model design for mining the useritem graph. The research attention has evolved from the random walk that encodes the graph structure as transition probabilities [2], to GCN that propagates user and item embeddings over the graph [19,38,46,50]. Recently, attention mechanism is introduced into GCN-based recommendation models [7], which learns to weigh the neighbors so as to capture the more informative user-item interactions. A surge of attention has also been dedicated to graphlevel methods, which enriches the user-item graph by accounting for side information apart from user-item interactions, which ranges from user social relations [1,32,53], item co-occurrence [2], to user and item attributes [27]. Recently, Knowledge Graph (KG) is also unified with the user-item graph, which enables considering the detailed types of linkage between items [4,43,45].\n\nDespite the tremendous efforts devoted on these methods, all the existing work adopts the paradigm of supervised learning for model training. This work is in an orthogonal direction, which explores self-supervised learning, opening up a new research line of graph-based recommendation.\n\n\nSelf-supervised Learning\n\nStudies on self-supervised learning can be roughly categorized into two branches: generative models [10,29,39] and contrastive models [6,11,17,40]. Auto-encoding is the most popular generative model which learns to reconstruct the input data, where noises can be intentionally added to enhance model robustness [10]. Contrastive models learn to compare through a Noise Contrastive Estimation (NCE) objective, which can be in either global-local contrast [22,40] or global-global contrast manner [6,17]. The former focuses on modeling the relationship between the local part of a sample and its global context representation. While the latter directly performs comparison between different samples, which typically requires multiple views of samples [6,17,37]. SSL has also been applied on graph data [51,52,56]. For instance, InfoGraph [35] and DGI [42] learns node representations according to mutual information between a node and the local structure. In addition, Hu et al. [23] extend the idea to learn GCN for graph representation. Furthermore, Kaveh et al. [16] adopt the contrastive model for learning both node and graph representation, which contrasts node representations from one view with graph representation of another view. Besides, GCC [31] leverages instance discrimination as the pretext task for graph structural information pre-training. These studies focused on general graphs while leaving the inherent properties of bipartite graph unconsidered.\n\nTo the best of our knowledge, very limited works exist in combining SSL with recommendation to date. A very recent one is 3 -Rec [55] for sequential recommendation which utilizes the mutual information maximization principle to learn the correlations among attribute, item, subsequence, and sequence. Another attempt [49] also adopts a multi-task framework with SSL. However, it differs from our work in: (1) [49] uses two-tower DNN as encoder, while our work sheds lights on graph-based recommendation, and devised three augmentation operators on graph structure. (2) [49] utilizes categorical metadata features as model input, while our work considers a more general collaborative filtering setting with only ID as feature.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this work, we recognized the limitations of graph-based recommendation under general supervised learning paradigm and explored the potential of SSL to solve the limitations. In particular, we proposed a model-agnostic framework SGL to supplement the supervised recommendation task with self-supervised learning on user-item graph. From the perspective of graph structure, we devised three types of data augmentation from different aspects to construct the auxiliary contrastive task. We conducted extensive experiments on three benchmark datasets, justifying the advantages of our proposal regarding long-tail recommendation, training convergence and robustness against noisy interactions.\n\nThis work represents an initial attempt to exploit selfsupervised learning for recommendation and opens up new research possibilities. In future work, we would like to make SSL more entrenched with recommendation tasks. Going beyond the stochastic selections on graph structure, we plan to explore new perspectives, such as counterfactual learning to identify influential data points, to create more powerful data augmentations. Moreover, we will focus on pre-training and fine-tuning in recommendation, -that is, to pre-train a model which captures universal and transferable patterns of users across multiple domains or datasets, and fine-tune it on upcoming domains or datasets. Another promising direction is fulfilling the potential of SSL to address the long-tail issue. We hope the development of SGL is beneficial for improving the generalization and transferability of recommender models.\n\n\nA GRADIENT OF INFONCE LOSS W.R.T. NODE REPRESENTATION\n\nHere we derive the Equation (12). Since we adopt cosine similarity function in the contrastive loss, we here decouple the normalization to show its superiority inspired from [24]. For a user \u2208 U, the contrastive loss is as follows: \nL ( ) = \u2212 log exp( \u2032 \u2032\u2032 / ) \u2208U exp( \u2032 \u2032\u2032 / )(18)\n\nwhere O = {( , , )|( , ) \u2208 O + , ( , ) \u2208 O \u2212 } is the training data, and O \u2212 = U \u00d7 I \\ O + is the unobserved interactions. In this work, we choose it as the main supervised task.\n\nFigure 1 :\n1The overall system framework of SGL. The upper layer illustrates the working flow of the main supervised learning task while the bottom layer shows the working flows of SSL task with augmentation on graph structure.\n\nFigure 2 :\n2A toy example of higher-order connectivity in a three-layer GCN model with Edge Dropout (left)\n\n\nHyper-parameter Settings. For fair comparison, all models are trained from scratch which are initialized with the Xavier method[13]. The models are optimized by the Adam optimizer with learning rate of 0.001 and mini-batch size of 2048. The early stopping strategy is the same as NGCF and LightGCN. The proposed SGL methods inherit the optimal values of the shared hyperparameters. For the unique ones of SGL, we tune 1 , , and within the ranges of {0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, {0.1, 0.2, 0.5, 1.0}, and {0, 0.1, 0.2, \u00b7 \u00b7 \u00b7 , 0.5}, respectively.\n\nFigure 4 :\n4Performance comparison over different item groups between SGL-ED and LightGCN. The suffix in the legend indicates the number of GCN layers.\n\nFigure 5 :Figure 6 :\n56Training Curves of SGL-ED and LightGCN on three datasets. The suffix in the legend denotes the layer numbers. Model performance w.r.t. noise ratio. The bar represents Recall, while the line represents the percentage of performance degradation.\n\nFigure 7 :\n7Model performance as adjusting .\n\n\nwhere \u2032 and \u2032\u2032 are the normalized representations, i.e., . The gradient of the contrastive Loss w.r.t. \u2032 can be calculated via the chain rule of scalar-by-vector: likelihood for \u2032\u2032 w.r.t. all samples within U.\n\nTable 1 :\n1The comparison of analytical time complexity between LightGCN and SGL-ED.Component \nLightGCN \nSGL-ED \nAdjacency \nMatrix \n(2 | |) \n(4\u02c6| | + 2 | |) \n\n\n\nTable 2 :\n2Statistics of the datasets.Dataset \n#Users #Items #Interactions Density \nYelp2018 \n31,668 38,048 \n1,561,406 0.00130 \nAmazon-Book \n52,643 91,599 \n2,984,108 0.00062 \nAlibaba-iFashion 300,000 81,614 \n1,607,813 0.00007 \n\nand SGL-ED is actually in the same magnitude, since the increase \nof LightGCN only scales the complexity of LightGCN. In practice, \ntaking the Yelp2018 data as an example, the time complexity of SGL-\nED (alternative) with\u02c6of 0.8 is about 3.7x larger than LightGCN, \nwhich is totally acceptable considering the speedup of convergence \nspeed we will show in Section 4.3.2. The testing platform is Nvidia \nTitan RTX graphics card equipped with Inter i7-9700K CPU (32GB \nMemory). The time cost of each epoch on Yelp2018 is 15.2s and \n60.6s for LightGCN and SGL-ED (alternative) respectively, which is \nconsistent with the complexity analyses. \n\n\n\nTable 3 :\n3Performance comparison with LightGCN at different layers. The performance of LightGCN on Yelp2018 and Amazon-Book are copied from its original paper. The percentage in brackets denote the relative performance improvement over LightGCN. The bold indicates the best result.Datase \nYelp2018 \nAmazon-Book \nAlibaba-iFashion \n#Layer \nMethod \nRecall \nNDCG \nRecall \nNDCG \nRecall \nNDCG \n\n1 Layer \n\nLightGCN 0.0631 \n0.0515 \n0.0384 \n0.0298 \n0.0990 \n0.0454 \nSGL-ND \n0.0643(+1.9%) 0.0529(+2.7%) 0.0432(+12.5%) \n0.0334(+12.1%) \n0.1133(+14.4%) 0.0539(+18.7%) \nSGL-ED \n0.0637(+1.0%) \n0.0526(+2.1%) \n0.0451(+17.4%) 0.0353(+18.5%) 0.1125(+13.6%) \n0.0536(+18.1%) \nSGL-RW \n0.0637(+1.0%) \n0.0526(+2.1%) \n0.0451(+17.4%) 0.0353(+18.5%) 0.1125(+13.6%) \n0.0536(+18.1%) \n\n2 Layers \n\nLightGCN 0.0622 \n0.0504 \n0.0411 \n0.0315 \n0.1066 \n0.0505 \nSGL-ND \n0.0658(+5.8%) \n0.0538(+6.7%) \n0.0427(+3.9%) \n0.0335(+6.3%) \n0.1106(+3.8%) \n0.0526(+4.2%) \nSGL-ED \n0.0668(+7.4%) 0.0549(+8.9%) 0.0468(+13.9%) 0.0371(+17.8%) 0.1091(+2.3%) \n0.0520(+3.0%) \nSGL-RW \n0.0644(+3.5%) \n0.0530(+5.2%) \n0.0453(+10.2%) \n0.0358(+13.7%) \n0.1091(+2.3%) \n0.0521(+3.2%) \n\n3 Layers \n\nLightGCN 0.0639 \n0.0525 \n0.0410 \n0.0318 \n0.1078 \n0.0507 \nSGL-ND \n0.0644(+0.8%) \n0.0528(+0.6%) \n0.0440(+7.3%) \n0.0346(+8.8%) \n0.1126(4.5%) \n0.0536(+5.7%) \nSGL-ED \n0.0675(+5.6%) 0.0555(+5.7%) 0.0478(+16.6%) 0.0379(+19.2%) 0.1126(+4.5%) \n0.0538(+6.1%) \nSGL-RW \n0.0667(+4.4%) \n0.0547(+4.2%) \n0.0457(+11.5%) \n0.0356(+12.0%) \n0.1139(+5.7%) \n0.0539(+6.3%) \n\n\n\nTable 4 :\n4Overall Performance Comparison. Comparison with the State-of-the-Arts. InDataset \nYelp2018 \nAmazon-Book \nAlibaba-iFashion \nMethod \nRecall NDCG \nRecall \nNDCG \nRecall \nNDCG \nNGCF \n0.0579 0.0477 \n0.0344 \n0.0263 \n0.1043 \n0.0486 \nLightGCN 0.0639 0.0525 \n0.0411 \n0.0315 \n0.1078 \n0.0507 \nMult-VAE 0.0584 0.0450 \n0.0407 \n0.0315 \n0.1041 \n0.0497 \nDNN+SSL 0.0483 0.0382 \n0.0438 \n0.0337 \n0.0712 \n0.0325 \nSGL-ED \n0.0675 0.0555 0.0478 \n0.0379 0.1126 0.0538 \n%Improv. \n5.63% \n5.71% \n9.13% \n12.46% \n4.45% \n6.11% \n-value \n5.92e-8 1.89e-8 5.07e-10 3.63e-10 3.34e-8 4.68e-10 \n\n4.2.2 \n\nTable 5 :\n5The comparison of different SSL variantsDataset \nYelp2018 \nAmazon-Book \nMethod \nRecall NDCG Recall NDCG \nSGL-ED-batch \n0.0670 0.0549 0.0472 0.0374 \nSGL-ED-merge 0.0671 0.0547 0.0464 0.0368 \nSGL-pre \n0.0653 0.0533 0.0429 0.0333 \nSGL-ED \n0.0675 0.0555 0.0478 0.0379 \n\n(a) Yelp2018 \n(b) Amazon-Book \n\n\n). We leaving the tuning of different ratios in future work. It is also\nhttps://github.com/wenyuer/POG\n\nContext-Aware Friend Recommendation for Location Based Social Networks using Random Walk. Hakan Bagci, Pinar Karagoz, WWW. Hakan Bagci and Pinar Karagoz. 2016. Context-Aware Friend Recommendation for Location Based Social Networks using Random Walk. In WWW. 531-536.\n\nVideo suggestion and discovery for youtube: taking random walks through the view graph. Shumeet Baluja, Rohan Seth, D Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravichandran, Mohamed Aly, Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravichandran, and Mohamed Aly. 2008. Video suggestion and discovery for youtube: taking random walks through the view graph. In WWW. 895-904.\n\nA Generic Coordinate Descent Framework for Learning from Implicit Feedback. Immanuel Bayer, Xiangnan He, Bhargav Kanagal, Steffen Rendle, WWW. Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Steffen Rendle. 2017. A Generic Coordinate Descent Framework for Learning from Implicit Feedback. In WWW. 1341-1350.\n\nUnifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences. Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, Tat-Seng Chua, Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019. Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences. In WWW. 151-161.\n\nJiawei Chen, Hande Dong, Xiang Wang, CoRR abs/2010.03240Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and Debias in Recommender System: A Survey and Future Directions. Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and Debias in Recommender System: A Survey and Future Directions. CoRR abs/2010.03240 (2020).\n\nA Simple Framework for Contrastive Learning of Visual Representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey E Hinton, CoRR abs/2002.05709Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. CoRR abs/2002.05709 (2020).\n\nSemi-supervised User Profiling with Heterogeneous Graph Attention Networks. Weijian Chen, Yulong Gu, Zhaochun Ren, Xiangnan He, Hongtao Xie, Tong Guo, Dawei Yin, Yongdong Zhang, Weijian Chen, Yulong Gu, Zhaochun Ren, Xiangnan He, Hongtao Xie, Tong Guo, Dawei Yin, and Yongdong Zhang. 2019. Semi-supervised User Profiling with Heterogeneous Graph Attention Networks. In IJCAI. 2116-2122.\n\nPOG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion. Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li, Andreas Pfadler, Huan Zhao, Binqiang Zhao, SIGKDD. Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li, Andreas Pfadler, Huan Zhao, and Binqiang Zhao. 2019. POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion. In SIGKDD. 2662- 2670.\n\nPower-Law Distributions in Empirical Data. Aaron Clauset, Mark E J Cosma Rohilla Shalizi, Newman, SIAM. 51Aaron Clauset, Cosma Rohilla Shalizi, and Mark E. J. Newman. 2009. Power-Law Distributions in Empirical Data. SIAM 51, 4 (2009), 661-703.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT. 4171-4186.\n\nUnsupervised Representation Learning by Predicting Image Rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, ICLR. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018. Unsupervised Representation Learning by Predicting Image Rotations. In ICLR.\n\nNeural Message Passing for Quantum Chemistry. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, George E Dahl, ICML. 70Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. 2017. Neural Message Passing for Quantum Chemistry. In ICML, Vol. 70. 1263-1272.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, In AISTATS. 9Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, Vol. 9. 249-256.\n\nNoise-contrastive estimation: A new estimation principle for unnormalized statistical models. Michael Gutmann, Aapo Hyv\u00e4rinen, In AISTATS. 9Michael Gutmann and Aapo Hyv\u00e4rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS, Vol. 9. 297-304.\n\nInductive Representation Learning on Large Graphs. William L Hamilton, Zhitao Ying, Jure Leskovec, NeurIPS. William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NeurIPS. 1024-1034.\n\nContrastive Multi-View Representation Learning on Graphs. Kaveh Hassani, Amir Hosein Khasahmadi, ICML. Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive Multi-View Representation Learning on Graphs. In ICML. 3451-3461.\n\nMomentum Contrast for Unsupervised Visual Representation Learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross B Girshick, CVPR. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In CVPR. 9726-9735.\n\nUps and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. Ruining He, Julian J Mcauley, Ruining He and Julian J. McAuley. 2016. Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. In WWW. 507-517.\n\nLightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, SIGIR. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In SIGIR. 639-648.\n\nXiangnan He, Zhankui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, Tat-Seng Chua, NAIS: Neural Attentive Item Similarity Model for Recommendation. 30Xiangnan He, Zhankui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, and Tat-Seng Chua. 2018. NAIS: Neural Attentive Item Similarity Model for Recommendation. TKDE 30, 12 (2018), 2354-2366.\n\n. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Neural Collaborative Filtering. In WWW. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In WWW. 173-182.\n\nLearning deep representations by mutual information estimation and maximization. R , Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, Yoshua Bengio, ICLR. R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representations by mutual information estimation and maximization. In ICLR.\n\nStrategies for Pre-training Graph Neural Networks. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S Pande, Jure Leskovec, ICLR. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. 2020. Strategies for Pre-training Graph Neural Networks. In ICLR.\n\nSupervised Contrastive Learning. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, NeurIPS. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised Contrastive Learning. In NeurIPS.\n\nFactorization meets the neighborhood: a multifaceted collaborative filtering model. Yehuda Koren, KDD. Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In KDD. 426-434.\n\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ICLR. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In ICLR.\n\nFi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction. Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, Liang Wang, CIKM. Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction. In CIKM. 539-548.\n\nVariational Autoencoders for Collaborative Filtering. Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, Tony Jebara, WWW. Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018. Variational Autoencoders for Collaborative Filtering. In WWW. 689-698.\n\nDistributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, NIPS. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS. 3111-3119.\n\nPower law distributions in information science: Making the case for logarithmic binning. Stasa Milojevic, J. Assoc. Inf. Sci. Technol. 61Stasa Milojevic. 2010. Power law distributions in information science: Making the case for logarithmic binning. J. Assoc. Inf. Sci. Technol. 61, 12 (2010), 2417-2425.\n\nGCC: Graph Contrastive Coding for Graph Neural Network Pre-Training. Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang, Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. 2020. GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training. In KDD. 1150-1160.\n\nSocial Collaborative Viewpoint Regression with Explainable Recommendations. Shangsong Zhaochun Ren, Piji Liang, Shuaiqiang Li, Maarten Wang, De Rijke, WSDM. Zhaochun Ren, Shangsong Liang, Piji Li, Shuaiqiang Wang, and Maarten de Rijke. 2017. Social Collaborative Viewpoint Regression with Explainable Recommendations. In WSDM. 485-494.\n\nImproving pairwise learning for item recommendation from implicit feedback. Steffen Rendle, Christoph Freudenthaler, WSDM. Steffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning for item recommendation from implicit feedback. In WSDM. 273-282.\n\nBPR: Bayesian Personalized Ranking from Implicit Feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, UAI. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI. 452- 461.\n\nInfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, Jian Tang, ICLR. Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2020. InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. In ICLR.\n\nPrasenjit Mitra, and Suhang Wang. 2020. Investigating and Mitigating Degree-Related Biases in Graph Convolutional Networks. Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, CIKM. Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, Prasenjit Mitra, and Suhang Wang. 2020. Investigating and Mitigating Degree- Related Biases in Graph Convolutional Networks. In CIKM.\n\nContrastive Multiview Coding. CoRR abs/1906. Yonglong Tian, Dilip Krishnan, Phillip Isola, 5849Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2019. Contrastive Multiview Coding. CoRR abs/1906.05849 (2019).\n\nGraph Convolutional Matrix Completion. Rianne Van Den, Thomas N Berg, Max Kipf, Welling, CoRR abs/1706.02263Rianne van den Berg, Thomas N. Kipf, and Max Welling. 2017. Graph Convolutional Matrix Completion. CoRR abs/1706.02263 (2017).\n\nConditional Image Generation with PixelCNN Decoders. A\u00e4ron Van Den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, NIPS. A\u00e4ron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals, and Alex Graves. 2016. Conditional Image Generation with PixelCNN Decoders. In NIPS. 4790-4798.\n\nRepresentation Learning with Contrastive Predictive Coding. A\u00e4ron Van Den Oord, Yazhe Li, Oriol Vinyals, CoRR abs/1807.03748A\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018).\n\nGraph Attention Networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, ICLR. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.\n\nDeep Graph Infomax. Petar Velickovic, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, R Devon Hjelm, ICLR. Petar Velickovic, William Fedus, William L. Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R. Devon Hjelm. 2019. Deep Graph Infomax. In ICLR.\n\nKnowledge Graph Convolutional Networks for Recommender Systems. Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, Minyi Guo, Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge Graph Convolutional Networks for Recommender Systems. In WWW. 3307-3313.\n\nXiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021. Denoising Implicit Feedback for Recommendation. Wenjie Wang, Fuli Feng, WSDM. Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021. Denoising Implicit Feedback for Recommendation. In WSDM.\n\nKGAT: Knowledge Graph Attention Network for Recommendation. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua, SIGKDD. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT: Knowledge Graph Attention Network for Recommendation. In SIGKDD. 950-958.\n\nNeural Graph Collaborative Filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, SIGIR. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In SIGIR. 165-174.\n\nUnsupervised Feature Learning via Non-Parametric Instance Discrimination. Zhirong Wu, Yuanjun Xiong, Stella X Yu, Dahua Lin, CVPR. Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. 2018. Unsupervised Feature Learning via Non-Parametric Instance Discrimination. In CVPR. 3733- 3742.\n\nHow Powerful are Graph Neural Networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, In ICLRKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful are Graph Neural Networks?. In ICLR.\n\nSelf-supervised Learning for Deep Models in Recommendations. Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix X Yu, Aditya Krishna Menon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger, CoRR abs/2007.12865Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix X. Yu, Aditya Krishna Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, and Evan Ettinger. 2020. Self-supervised Learning for Deep Models in Recommendations. CoRR abs/2007.12865 (2020).\n\nGraph Convolutional Neural Networks for Web-Scale Recommender Systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In KDD. 974-983.\n\nGraph Contrastive Learning with Augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, NeurIPS. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph Contrastive Learning with Augmentations. In NeurIPS.\n\nWhen Does Self-Supervision Help Graph Convolutional Networks?. Yuning You, Tianlong Chen, Zhangyang Wang, Yang Shen, In ICML. 119Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. 2020. When Does Self-Supervision Help Graph Convolutional Networks?. In ICML, Vol. 119. 10871-10880.\n\nAlexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-efficient transfer from sequential behaviors for user modeling and recommendation. Fajie Yuan, Xiangnan He, SIGIR. Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-efficient transfer from sequential behaviors for user modeling and recommendation. In SIGIR. 1469-1478.\n\nOptimizing top-n collaborative filtering via dynamic negative item sampling. Weinan Zhang, Tianqi Chen, Jun Wang, Yong Yu, SIGIR. Weinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. 2013. Optimizing top-n collaborative filtering via dynamic negative item sampling. In SIGIR. 785-788.\n\nS\u02c63-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen, CIKM. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S\u02c63-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM.\n\nGraph Contrastive Learning with Adaptive Augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, CoRR abs/2010.14945Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Graph Contrastive Learning with Adaptive Augmentation. CoRR abs/2010.14945 (2020).\n", "annotations": {"author": "[{\"end\":177,\"start\":113},{\"end\":233,\"start\":178},{\"end\":288,\"start\":234},{\"end\":375,\"start\":289},{\"end\":445,\"start\":376},{\"end\":490,\"start\":446},{\"end\":554,\"start\":491},{\"end\":566,\"start\":555},{\"end\":578,\"start\":567},{\"end\":589,\"start\":579},{\"end\":602,\"start\":590},{\"end\":614,\"start\":603},{\"end\":628,\"start\":615},{\"end\":638,\"start\":629}]", "publisher": "[{\"end\":41,\"start\":15},{\"end\":828,\"start\":802}]", "author_last_name": "[{\"end\":123,\"start\":121},{\"end\":188,\"start\":184},{\"end\":243,\"start\":239},{\"end\":300,\"start\":298},{\"end\":386,\"start\":382},{\"end\":458,\"start\":454},{\"end\":499,\"start\":496},{\"end\":565,\"start\":563},{\"end\":577,\"start\":573},{\"end\":588,\"start\":584},{\"end\":601,\"start\":599},{\"end\":613,\"start\":609},{\"end\":627,\"start\":623},{\"end\":637,\"start\":634}]", "author_first_name": "[{\"end\":120,\"start\":113},{\"end\":183,\"start\":178},{\"end\":238,\"start\":234},{\"end\":297,\"start\":289},{\"end\":381,\"start\":376},{\"end\":453,\"start\":446},{\"end\":495,\"start\":491},{\"end\":562,\"start\":555},{\"end\":572,\"start\":567},{\"end\":583,\"start\":579},{\"end\":598,\"start\":590},{\"end\":608,\"start\":603},{\"end\":622,\"start\":615},{\"end\":633,\"start\":629}]", "author_affiliation": "[{\"end\":176,\"start\":125},{\"end\":232,\"start\":190},{\"end\":287,\"start\":245},{\"end\":374,\"start\":323},{\"end\":444,\"start\":416},{\"end\":489,\"start\":460},{\"end\":553,\"start\":524}]", "title": "[{\"end\":14,\"start\":1},{\"end\":652,\"start\":639}]", "venue": "[{\"end\":784,\"start\":654}]", "abstract": "[{\"end\":2939,\"start\":1125}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3123,\"start\":3119},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3219,\"start\":3215},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3222,\"start\":3219},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3650,\"start\":3646},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3653,\"start\":3650},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3656,\"start\":3653},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3659,\"start\":3656},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3868,\"start\":3864},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3871,\"start\":3868},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3874,\"start\":3871},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4012,\"start\":4009},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4015,\"start\":4012},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4208,\"start\":4205},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4211,\"start\":4208},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4511,\"start\":4508},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4514,\"start\":4511},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4871,\"start\":4867},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5213,\"start\":5209},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5216,\"start\":5213},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5259,\"start\":5255},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5262,\"start\":5259},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5490,\"start\":5486},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5658,\"start\":5654},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6010,\"start\":6007},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8173,\"start\":8169},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9258,\"start\":9254},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9261,\"start\":9258},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9264,\"start\":9261},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10197,\"start\":10193},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10200,\"start\":10197},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10203,\"start\":10200},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10206,\"start\":10203},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10516,\"start\":10512},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10519,\"start\":10516},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10539,\"start\":10535},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10562,\"start\":10558},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11097,\"start\":11093},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11217,\"start\":11213},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12297,\"start\":12294},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12300,\"start\":12297},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12303,\"start\":12300},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12306,\"start\":12303},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15495,\"start\":15491},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16155,\"start\":16154},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16777,\"start\":16774},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16822,\"start\":16818},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20915,\"start\":20911},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21671,\"start\":21667},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21714,\"start\":21710},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22432,\"start\":22428},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23864,\"start\":23861},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23867,\"start\":23864},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24727,\"start\":24723},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24730,\"start\":24727},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24748,\"start\":24744},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24751,\"start\":24748},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24796,\"start\":24792},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24799,\"start\":24796},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":25098,\"start\":25094},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":25260,\"start\":25256},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":25451,\"start\":25447},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25522,\"start\":25518},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25740,\"start\":25736},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25906,\"start\":25902},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26175,\"start\":26171},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26681,\"start\":26677},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26693,\"start\":26689},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26705,\"start\":26701},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":26723,\"start\":26719},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26752,\"start\":26748},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26755,\"start\":26752},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26758,\"start\":26755},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32391,\"start\":32387},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32416,\"start\":32412},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32419,\"start\":32416},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":33604,\"start\":33600},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37250,\"start\":37247},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38670,\"start\":38667},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38739,\"start\":38735},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":38742,\"start\":38739},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38745,\"start\":38742},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":38748,\"start\":38745},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38834,\"start\":38831},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39150,\"start\":39147},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39153,\"start\":39150},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":39156,\"start\":39153},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39180,\"start\":39177},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39214,\"start\":39210},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39362,\"start\":39359},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39365,\"start\":39362},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39368,\"start\":39365},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39789,\"start\":39785},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39792,\"start\":39789},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":39795,\"start\":39792},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":39822,\"start\":39819},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39825,\"start\":39822},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39828,\"start\":39825},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":39831,\"start\":39828},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":40000,\"start\":39996},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":40143,\"start\":40139},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":40146,\"start\":40143},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40183,\"start\":40180},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40186,\"start\":40183},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40437,\"start\":40434},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40440,\"start\":40437},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40443,\"start\":40440},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40489,\"start\":40485},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":40492,\"start\":40489},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":40495,\"start\":40492},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40525,\"start\":40521},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":40538,\"start\":40534},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40666,\"start\":40662},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40752,\"start\":40748},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40941,\"start\":40937},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41288,\"start\":41284},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41476,\"start\":41472},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41568,\"start\":41564},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41728,\"start\":41724},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43592,\"start\":43588},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":43738,\"start\":43734},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":44492,\"start\":44488}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44021,\"start\":43841},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44250,\"start\":44022},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44358,\"start\":44251},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44913,\"start\":44359},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45066,\"start\":44914},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45334,\"start\":45067},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45380,\"start\":45335},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45592,\"start\":45381},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45753,\"start\":45593},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46624,\"start\":45754},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48108,\"start\":46625},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48685,\"start\":48109},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48996,\"start\":48686}]", "paragraph": "[{\"end\":3660,\"start\":2955},{\"end\":3751,\"start\":3662},{\"end\":6011,\"start\":3753},{\"end\":7186,\"start\":6013},{\"end\":7954,\"start\":7188},{\"end\":8461,\"start\":7956},{\"end\":8970,\"start\":8463},{\"end\":9593,\"start\":8988},{\"end\":9873,\"start\":9619},{\"end\":10207,\"start\":9935},{\"end\":10429,\"start\":10209},{\"end\":10758,\"start\":10473},{\"end\":11331,\"start\":10772},{\"end\":12185,\"start\":11381},{\"end\":12856,\"start\":12226},{\"end\":13940,\"start\":12858},{\"end\":14154,\"start\":13959},{\"end\":14273,\"start\":14156},{\"end\":14316,\"start\":14275},{\"end\":14785,\"start\":14365},{\"end\":15683,\"start\":14833},{\"end\":15750,\"start\":15739},{\"end\":15830,\"start\":15769},{\"end\":16275,\"start\":15832},{\"end\":16904,\"start\":16300},{\"end\":17283,\"start\":16974},{\"end\":17523,\"start\":17307},{\"end\":17856,\"start\":17553},{\"end\":18457,\"start\":17888},{\"end\":18907,\"start\":18502},{\"end\":18979,\"start\":18959},{\"end\":19046,\"start\":19041},{\"end\":19343,\"start\":19080},{\"end\":19515,\"start\":19392},{\"end\":20916,\"start\":19541},{\"end\":21162,\"start\":20918},{\"end\":22141,\"start\":21217},{\"end\":22615,\"start\":22172},{\"end\":22941,\"start\":22617},{\"end\":24048,\"start\":22943},{\"end\":24354,\"start\":24198},{\"end\":24636,\"start\":24356},{\"end\":25360,\"start\":24662},{\"end\":26636,\"start\":25382},{\"end\":27070,\"start\":26638},{\"end\":27747,\"start\":27131},{\"end\":28797,\"start\":27749},{\"end\":30273,\"start\":28799},{\"end\":30367,\"start\":30299},{\"end\":30562,\"start\":30369},{\"end\":31050,\"start\":30592},{\"end\":31355,\"start\":31092},{\"end\":32773,\"start\":31357},{\"end\":33894,\"start\":32775},{\"end\":34278,\"start\":33918},{\"end\":35281,\"start\":34280},{\"end\":35715,\"start\":35304},{\"end\":36410,\"start\":35717},{\"end\":37404,\"start\":36438},{\"end\":38168,\"start\":37414},{\"end\":38310,\"start\":38185},{\"end\":39369,\"start\":38341},{\"end\":39656,\"start\":39371},{\"end\":41153,\"start\":39685},{\"end\":41880,\"start\":41155},{\"end\":42603,\"start\":41911},{\"end\":43502,\"start\":42605},{\"end\":43792,\"start\":43560}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9618,\"start\":9594},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9934,\"start\":9874},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10472,\"start\":10430},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10771,\"start\":10759},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11366,\"start\":11332},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13958,\"start\":13941},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14364,\"start\":14317},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14832,\"start\":14786},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15738,\"start\":15684},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15768,\"start\":15751},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16973,\"start\":16905},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17552,\"start\":17524},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18501,\"start\":18458},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18958,\"start\":18908},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19013,\"start\":18980},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19040,\"start\":19013},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19079,\"start\":19047},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19391,\"start\":19344},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19540,\"start\":19516},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21216,\"start\":21163},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24183,\"start\":24049},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31091,\"start\":31051},{\"attributes\":{\"id\":\"formula_22\"},\"end\":43841,\"start\":43793}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24007,\"start\":24000},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25051,\"start\":25044},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27002,\"start\":26995},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29289,\"start\":29282},{\"end\":31960,\"start\":31954},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36960,\"start\":36953},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37106,\"start\":37099},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37847,\"start\":37840}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2953,\"start\":2941},{\"attributes\":{\"n\":\"2\"},\"end\":8986,\"start\":8973},{\"attributes\":{\"n\":\"3\"},\"end\":11379,\"start\":11368},{\"attributes\":{\"n\":\"3.1\"},\"end\":12224,\"start\":12188},{\"attributes\":{\"n\":\"3.2\"},\"end\":16298,\"start\":16278},{\"attributes\":{\"n\":\"3.3\"},\"end\":17305,\"start\":17286},{\"attributes\":{\"n\":\"3.4\"},\"end\":17886,\"start\":17859},{\"attributes\":{\"n\":\"3.5\"},\"end\":22170,\"start\":22144},{\"attributes\":{\"n\":\"4\"},\"end\":24196,\"start\":24185},{\"attributes\":{\"n\":\"4.1\"},\"end\":24660,\"start\":24639},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":25380,\"start\":25363},{\"attributes\":{\"n\":\"4.2\"},\"end\":27101,\"start\":27073},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":27129,\"start\":27104},{\"attributes\":{\"n\":\"4.3\"},\"end\":30297,\"start\":30276},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":30590,\"start\":30565},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":33916,\"start\":33897},{\"attributes\":{\"n\":\"4.4\"},\"end\":35302,\"start\":35284},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":36436,\"start\":36413},{\"end\":37412,\"start\":37407},{\"attributes\":{\"n\":\"5\"},\"end\":38183,\"start\":38171},{\"attributes\":{\"n\":\"5.1\"},\"end\":38339,\"start\":38313},{\"attributes\":{\"n\":\"5.2\"},\"end\":39683,\"start\":39659},{\"attributes\":{\"n\":\"6\"},\"end\":41909,\"start\":41883},{\"end\":43558,\"start\":43505},{\"end\":44033,\"start\":44023},{\"end\":44262,\"start\":44252},{\"end\":44925,\"start\":44915},{\"end\":45088,\"start\":45068},{\"end\":45346,\"start\":45336},{\"end\":45603,\"start\":45594},{\"end\":45764,\"start\":45755},{\"end\":46635,\"start\":46626},{\"end\":48119,\"start\":48110},{\"end\":48696,\"start\":48687}]", "table": "[{\"end\":45753,\"start\":45678},{\"end\":46624,\"start\":45793},{\"end\":48108,\"start\":46908},{\"end\":48685,\"start\":48194},{\"end\":48996,\"start\":48738}]", "figure_caption": "[{\"end\":44021,\"start\":43843},{\"end\":44250,\"start\":44035},{\"end\":44358,\"start\":44264},{\"end\":44913,\"start\":44361},{\"end\":45066,\"start\":44927},{\"end\":45334,\"start\":45091},{\"end\":45380,\"start\":45348},{\"end\":45592,\"start\":45383},{\"end\":45678,\"start\":45605},{\"end\":45793,\"start\":45766},{\"end\":46908,\"start\":46637},{\"end\":48194,\"start\":48121},{\"end\":48738,\"start\":48698}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11535,\"start\":11527},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15509,\"start\":15501},{\"end\":18749,\"start\":18741},{\"end\":20171,\"start\":20154},{\"end\":20353,\"start\":20344},{\"end\":20583,\"start\":20574},{\"end\":21278,\"start\":21260},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28004,\"start\":27996},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31340,\"start\":31332},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31975,\"start\":31967},{\"end\":32496,\"start\":32488},{\"end\":34222,\"start\":34214},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35829,\"start\":35821}]", "bib_author_first_name": "[{\"end\":49196,\"start\":49191},{\"end\":49209,\"start\":49204},{\"end\":49464,\"start\":49457},{\"end\":49478,\"start\":49473},{\"end\":49486,\"start\":49485},{\"end\":49503,\"start\":49498},{\"end\":49513,\"start\":49510},{\"end\":49529,\"start\":49522},{\"end\":49543,\"start\":49537},{\"end\":49565,\"start\":49558},{\"end\":49886,\"start\":49878},{\"end\":49902,\"start\":49894},{\"end\":49914,\"start\":49907},{\"end\":49931,\"start\":49924},{\"end\":50224,\"start\":50219},{\"end\":50235,\"start\":50230},{\"end\":50250,\"start\":50242},{\"end\":50260,\"start\":50255},{\"end\":50273,\"start\":50265},{\"end\":50481,\"start\":50475},{\"end\":50493,\"start\":50488},{\"end\":50505,\"start\":50500},{\"end\":50904,\"start\":50900},{\"end\":50916,\"start\":50911},{\"end\":50936,\"start\":50928},{\"end\":50954,\"start\":50946},{\"end\":50956,\"start\":50955},{\"end\":51243,\"start\":51236},{\"end\":51256,\"start\":51250},{\"end\":51269,\"start\":51261},{\"end\":51283,\"start\":51275},{\"end\":51295,\"start\":51288},{\"end\":51305,\"start\":51301},{\"end\":51316,\"start\":51311},{\"end\":51330,\"start\":51322},{\"end\":51635,\"start\":51632},{\"end\":51647,\"start\":51642},{\"end\":51662,\"start\":51655},{\"end\":51670,\"start\":51667},{\"end\":51681,\"start\":51676},{\"end\":51690,\"start\":51687},{\"end\":51700,\"start\":51696},{\"end\":51712,\"start\":51705},{\"end\":51726,\"start\":51722},{\"end\":51741,\"start\":51733},{\"end\":52038,\"start\":52033},{\"end\":52052,\"start\":52048},{\"end\":52056,\"start\":52053},{\"end\":52322,\"start\":52317},{\"end\":52339,\"start\":52331},{\"end\":52353,\"start\":52347},{\"end\":52367,\"start\":52359},{\"end\":52644,\"start\":52638},{\"end\":52661,\"start\":52654},{\"end\":52674,\"start\":52669},{\"end\":52880,\"start\":52874},{\"end\":52895,\"start\":52889},{\"end\":52897,\"start\":52896},{\"end\":52917,\"start\":52910},{\"end\":52919,\"start\":52918},{\"end\":52932,\"start\":52927},{\"end\":52948,\"start\":52942},{\"end\":52950,\"start\":52949},{\"end\":53218,\"start\":53212},{\"end\":53233,\"start\":53227},{\"end\":53500,\"start\":53493},{\"end\":53514,\"start\":53510},{\"end\":53763,\"start\":53756},{\"end\":53765,\"start\":53764},{\"end\":53782,\"start\":53776},{\"end\":53793,\"start\":53789},{\"end\":54010,\"start\":54005},{\"end\":54251,\"start\":54244},{\"end\":54261,\"start\":54256},{\"end\":54272,\"start\":54267},{\"end\":54284,\"start\":54277},{\"end\":54294,\"start\":54290},{\"end\":54296,\"start\":54295},{\"end\":54585,\"start\":54578},{\"end\":54596,\"start\":54590},{\"end\":54598,\"start\":54597},{\"end\":54858,\"start\":54850},{\"end\":54867,\"start\":54863},{\"end\":54879,\"start\":54874},{\"end\":54889,\"start\":54886},{\"end\":54903,\"start\":54894},{\"end\":54915,\"start\":54911},{\"end\":55120,\"start\":55112},{\"end\":55132,\"start\":55125},{\"end\":55145,\"start\":55137},{\"end\":55161,\"start\":55152},{\"end\":55174,\"start\":55167},{\"end\":55190,\"start\":55182},{\"end\":55466,\"start\":55458},{\"end\":55475,\"start\":55471},{\"end\":55489,\"start\":55482},{\"end\":55504,\"start\":55497},{\"end\":55513,\"start\":55510},{\"end\":55526,\"start\":55518},{\"end\":55790,\"start\":55789},{\"end\":55798,\"start\":55793},{\"end\":55810,\"start\":55806},{\"end\":55826,\"start\":55820},{\"end\":55851,\"start\":55846},{\"end\":55866,\"start\":55860},{\"end\":55880,\"start\":55876},{\"end\":55898,\"start\":55892},{\"end\":56188,\"start\":56182},{\"end\":56198,\"start\":56193},{\"end\":56210,\"start\":56204},{\"end\":56225,\"start\":56218},{\"end\":56239,\"start\":56234},{\"end\":56252,\"start\":56247},{\"end\":56254,\"start\":56253},{\"end\":56266,\"start\":56262},{\"end\":56490,\"start\":56483},{\"end\":56504,\"start\":56499},{\"end\":56519,\"start\":56515},{\"end\":56531,\"start\":56526},{\"end\":56547,\"start\":56539},{\"end\":56561,\"start\":56554},{\"end\":56574,\"start\":56569},{\"end\":56588,\"start\":56586},{\"end\":56599,\"start\":56594},{\"end\":56892,\"start\":56886},{\"end\":57114,\"start\":57105},{\"end\":57126,\"start\":57120},{\"end\":57142,\"start\":57133},{\"end\":57157,\"start\":57152},{\"end\":57172,\"start\":57166},{\"end\":57185,\"start\":57181},{\"end\":57478,\"start\":57473},{\"end\":57487,\"start\":57483},{\"end\":57496,\"start\":57493},{\"end\":57507,\"start\":57501},{\"end\":57520,\"start\":57515},{\"end\":57759,\"start\":57754},{\"end\":57772,\"start\":57767},{\"end\":57774,\"start\":57773},{\"end\":57792,\"start\":57785},{\"end\":57794,\"start\":57793},{\"end\":57808,\"start\":57804},{\"end\":58051,\"start\":58046},{\"end\":58065,\"start\":58061},{\"end\":58080,\"start\":58077},{\"end\":58094,\"start\":58087},{\"end\":58096,\"start\":58095},{\"end\":58113,\"start\":58106},{\"end\":58403,\"start\":58398},{\"end\":58691,\"start\":58683},{\"end\":58702,\"start\":58697},{\"end\":58715,\"start\":58709},{\"end\":58726,\"start\":58722},{\"end\":58741,\"start\":58734},{\"end\":58752,\"start\":58748},{\"end\":58766,\"start\":58759},{\"end\":58776,\"start\":58773},{\"end\":59067,\"start\":59058},{\"end\":59086,\"start\":59082},{\"end\":59104,\"start\":59094},{\"end\":59116,\"start\":59109},{\"end\":59402,\"start\":59395},{\"end\":59420,\"start\":59411},{\"end\":59653,\"start\":59646},{\"end\":59671,\"start\":59662},{\"end\":59691,\"start\":59687},{\"end\":59705,\"start\":59701},{\"end\":60015,\"start\":60008},{\"end\":60027,\"start\":60021},{\"end\":60043,\"start\":60038},{\"end\":60055,\"start\":60051},{\"end\":60391,\"start\":60383},{\"end\":60404,\"start\":60398},{\"end\":60415,\"start\":60410},{\"end\":60425,\"start\":60421},{\"end\":60439,\"start\":60432},{\"end\":60451,\"start\":60446},{\"end\":60735,\"start\":60727},{\"end\":60747,\"start\":60742},{\"end\":60765,\"start\":60758},{\"end\":60937,\"start\":60931},{\"end\":60953,\"start\":60947},{\"end\":60955,\"start\":60954},{\"end\":60965,\"start\":60962},{\"end\":61186,\"start\":61181},{\"end\":61204,\"start\":61201},{\"end\":61224,\"start\":61219},{\"end\":61240,\"start\":61235},{\"end\":61510,\"start\":61505},{\"end\":61530,\"start\":61525},{\"end\":61540,\"start\":61535},{\"end\":61744,\"start\":61739},{\"end\":61764,\"start\":61757},{\"end\":61782,\"start\":61775},{\"end\":61800,\"start\":61793},{\"end\":61815,\"start\":61809},{\"end\":61827,\"start\":61821},{\"end\":62010,\"start\":62005},{\"end\":62030,\"start\":62023},{\"end\":62045,\"start\":62038},{\"end\":62047,\"start\":62046},{\"end\":62064,\"start\":62058},{\"end\":62076,\"start\":62070},{\"end\":62086,\"start\":62085},{\"end\":62092,\"start\":62087},{\"end\":62314,\"start\":62307},{\"end\":62325,\"start\":62321},{\"end\":62336,\"start\":62332},{\"end\":62348,\"start\":62342},{\"end\":62358,\"start\":62353},{\"end\":62620,\"start\":62614},{\"end\":62631,\"start\":62627},{\"end\":62842,\"start\":62837},{\"end\":62857,\"start\":62849},{\"end\":62867,\"start\":62862},{\"end\":62877,\"start\":62873},{\"end\":62891,\"start\":62883},{\"end\":63101,\"start\":63096},{\"end\":63116,\"start\":63108},{\"end\":63125,\"start\":63121},{\"end\":63136,\"start\":63132},{\"end\":63151,\"start\":63143},{\"end\":63376,\"start\":63369},{\"end\":63388,\"start\":63381},{\"end\":63402,\"start\":63396},{\"end\":63404,\"start\":63403},{\"end\":63414,\"start\":63409},{\"end\":63631,\"start\":63625},{\"end\":63642,\"start\":63636},{\"end\":63651,\"start\":63647},{\"end\":63670,\"start\":63662},{\"end\":63873,\"start\":63864},{\"end\":63886,\"start\":63879},{\"end\":63896,\"start\":63891},{\"end\":63904,\"start\":63897},{\"end\":63917,\"start\":63912},{\"end\":63919,\"start\":63918},{\"end\":63930,\"start\":63924},{\"end\":63938,\"start\":63931},{\"end\":63952,\"start\":63946},{\"end\":63961,\"start\":63959},{\"end\":63963,\"start\":63962},{\"end\":63974,\"start\":63969},{\"end\":63986,\"start\":63981},{\"end\":63997,\"start\":63993},{\"end\":64347,\"start\":64344},{\"end\":64361,\"start\":64354},{\"end\":64373,\"start\":64366},{\"end\":64384,\"start\":64380},{\"end\":64406,\"start\":64399},{\"end\":64408,\"start\":64407},{\"end\":64423,\"start\":64419},{\"end\":64677,\"start\":64671},{\"end\":64691,\"start\":64683},{\"end\":64705,\"start\":64698},{\"end\":64715,\"start\":64711},{\"end\":64731,\"start\":64722},{\"end\":64742,\"start\":64738},{\"end\":64975,\"start\":64969},{\"end\":64989,\"start\":64981},{\"end\":65005,\"start\":64996},{\"end\":65016,\"start\":65012},{\"end\":65342,\"start\":65337},{\"end\":65357,\"start\":65349},{\"end\":65641,\"start\":65635},{\"end\":65655,\"start\":65649},{\"end\":65665,\"start\":65662},{\"end\":65676,\"start\":65672},{\"end\":65946,\"start\":65943},{\"end\":65956,\"start\":65953},{\"end\":65968,\"start\":65963},{\"end\":65972,\"start\":65969},{\"end\":65984,\"start\":65979},{\"end\":65995,\"start\":65990},{\"end\":66009,\"start\":66002},{\"end\":66026,\"start\":66017},{\"end\":66040,\"start\":66033},{\"end\":66339,\"start\":66332},{\"end\":66351,\"start\":66345},{\"end\":66360,\"start\":66356},{\"end\":66370,\"start\":66365},{\"end\":66379,\"start\":66376},{\"end\":66389,\"start\":66384}]", "bib_author_last_name": "[{\"end\":49202,\"start\":49197},{\"end\":49217,\"start\":49210},{\"end\":49471,\"start\":49465},{\"end\":49483,\"start\":49479},{\"end\":49496,\"start\":49487},{\"end\":49508,\"start\":49504},{\"end\":49520,\"start\":49514},{\"end\":49535,\"start\":49530},{\"end\":49556,\"start\":49544},{\"end\":49569,\"start\":49566},{\"end\":49892,\"start\":49887},{\"end\":49905,\"start\":49903},{\"end\":49922,\"start\":49915},{\"end\":49938,\"start\":49932},{\"end\":50228,\"start\":50225},{\"end\":50240,\"start\":50236},{\"end\":50253,\"start\":50251},{\"end\":50263,\"start\":50261},{\"end\":50278,\"start\":50274},{\"end\":50486,\"start\":50482},{\"end\":50498,\"start\":50494},{\"end\":50510,\"start\":50506},{\"end\":50909,\"start\":50905},{\"end\":50926,\"start\":50917},{\"end\":50944,\"start\":50937},{\"end\":50963,\"start\":50957},{\"end\":51248,\"start\":51244},{\"end\":51259,\"start\":51257},{\"end\":51273,\"start\":51270},{\"end\":51286,\"start\":51284},{\"end\":51299,\"start\":51296},{\"end\":51309,\"start\":51306},{\"end\":51320,\"start\":51317},{\"end\":51336,\"start\":51331},{\"end\":51640,\"start\":51636},{\"end\":51653,\"start\":51648},{\"end\":51665,\"start\":51663},{\"end\":51674,\"start\":51671},{\"end\":51685,\"start\":51682},{\"end\":51694,\"start\":51691},{\"end\":51703,\"start\":51701},{\"end\":51720,\"start\":51713},{\"end\":51731,\"start\":51727},{\"end\":51746,\"start\":51742},{\"end\":52046,\"start\":52039},{\"end\":52078,\"start\":52057},{\"end\":52086,\"start\":52080},{\"end\":52329,\"start\":52323},{\"end\":52345,\"start\":52340},{\"end\":52357,\"start\":52354},{\"end\":52377,\"start\":52368},{\"end\":52652,\"start\":52645},{\"end\":52667,\"start\":52662},{\"end\":52684,\"start\":52675},{\"end\":52887,\"start\":52881},{\"end\":52908,\"start\":52898},{\"end\":52925,\"start\":52920},{\"end\":52940,\"start\":52933},{\"end\":52955,\"start\":52951},{\"end\":53225,\"start\":53219},{\"end\":53240,\"start\":53234},{\"end\":53508,\"start\":53501},{\"end\":53524,\"start\":53515},{\"end\":53774,\"start\":53766},{\"end\":53787,\"start\":53783},{\"end\":53802,\"start\":53794},{\"end\":54018,\"start\":54011},{\"end\":54042,\"start\":54020},{\"end\":54254,\"start\":54252},{\"end\":54265,\"start\":54262},{\"end\":54275,\"start\":54273},{\"end\":54288,\"start\":54285},{\"end\":54305,\"start\":54297},{\"end\":54588,\"start\":54586},{\"end\":54606,\"start\":54599},{\"end\":54861,\"start\":54859},{\"end\":54872,\"start\":54868},{\"end\":54884,\"start\":54880},{\"end\":54892,\"start\":54890},{\"end\":54909,\"start\":54904},{\"end\":54920,\"start\":54916},{\"end\":55123,\"start\":55121},{\"end\":55135,\"start\":55133},{\"end\":55150,\"start\":55146},{\"end\":55165,\"start\":55162},{\"end\":55180,\"start\":55175},{\"end\":55195,\"start\":55191},{\"end\":55469,\"start\":55467},{\"end\":55480,\"start\":55476},{\"end\":55495,\"start\":55490},{\"end\":55508,\"start\":55505},{\"end\":55516,\"start\":55514},{\"end\":55531,\"start\":55527},{\"end\":55804,\"start\":55799},{\"end\":55818,\"start\":55811},{\"end\":55844,\"start\":55827},{\"end\":55858,\"start\":55852},{\"end\":55874,\"start\":55867},{\"end\":55890,\"start\":55881},{\"end\":55905,\"start\":55899},{\"end\":56191,\"start\":56189},{\"end\":56202,\"start\":56199},{\"end\":56216,\"start\":56211},{\"end\":56232,\"start\":56226},{\"end\":56245,\"start\":56240},{\"end\":56260,\"start\":56255},{\"end\":56275,\"start\":56267},{\"end\":56497,\"start\":56491},{\"end\":56513,\"start\":56505},{\"end\":56524,\"start\":56520},{\"end\":56537,\"start\":56532},{\"end\":56552,\"start\":56548},{\"end\":56567,\"start\":56562},{\"end\":56584,\"start\":56575},{\"end\":56592,\"start\":56589},{\"end\":56608,\"start\":56600},{\"end\":56898,\"start\":56893},{\"end\":57118,\"start\":57115},{\"end\":57131,\"start\":57127},{\"end\":57150,\"start\":57143},{\"end\":57164,\"start\":57158},{\"end\":57179,\"start\":57173},{\"end\":57193,\"start\":57186},{\"end\":57481,\"start\":57479},{\"end\":57491,\"start\":57488},{\"end\":57499,\"start\":57497},{\"end\":57513,\"start\":57508},{\"end\":57525,\"start\":57521},{\"end\":57765,\"start\":57760},{\"end\":57783,\"start\":57775},{\"end\":57802,\"start\":57795},{\"end\":57815,\"start\":57809},{\"end\":58059,\"start\":58052},{\"end\":58075,\"start\":58066},{\"end\":58085,\"start\":58081},{\"end\":58104,\"start\":58097},{\"end\":58118,\"start\":58114},{\"end\":58413,\"start\":58404},{\"end\":58695,\"start\":58692},{\"end\":58707,\"start\":58703},{\"end\":58720,\"start\":58716},{\"end\":58732,\"start\":58727},{\"end\":58746,\"start\":58742},{\"end\":58757,\"start\":58753},{\"end\":58771,\"start\":58767},{\"end\":58781,\"start\":58777},{\"end\":59080,\"start\":59068},{\"end\":59092,\"start\":59087},{\"end\":59107,\"start\":59105},{\"end\":59121,\"start\":59117},{\"end\":59131,\"start\":59123},{\"end\":59409,\"start\":59403},{\"end\":59434,\"start\":59421},{\"end\":59660,\"start\":59654},{\"end\":59685,\"start\":59672},{\"end\":59699,\"start\":59692},{\"end\":59720,\"start\":59706},{\"end\":60019,\"start\":60016},{\"end\":60036,\"start\":60028},{\"end\":60049,\"start\":60044},{\"end\":60060,\"start\":60056},{\"end\":60396,\"start\":60392},{\"end\":60408,\"start\":60405},{\"end\":60419,\"start\":60416},{\"end\":60430,\"start\":60426},{\"end\":60444,\"start\":60440},{\"end\":60460,\"start\":60452},{\"end\":60740,\"start\":60736},{\"end\":60756,\"start\":60748},{\"end\":60771,\"start\":60766},{\"end\":60945,\"start\":60938},{\"end\":60960,\"start\":60956},{\"end\":60970,\"start\":60966},{\"end\":60979,\"start\":60972},{\"end\":61199,\"start\":61187},{\"end\":61217,\"start\":61205},{\"end\":61233,\"start\":61225},{\"end\":61252,\"start\":61241},{\"end\":61523,\"start\":61511},{\"end\":61533,\"start\":61531},{\"end\":61548,\"start\":61541},{\"end\":61755,\"start\":61745},{\"end\":61773,\"start\":61765},{\"end\":61791,\"start\":61783},{\"end\":61807,\"start\":61801},{\"end\":61819,\"start\":61816},{\"end\":61834,\"start\":61828},{\"end\":62021,\"start\":62011},{\"end\":62036,\"start\":62031},{\"end\":62056,\"start\":62048},{\"end\":62068,\"start\":62065},{\"end\":62083,\"start\":62077},{\"end\":62098,\"start\":62093},{\"end\":62319,\"start\":62315},{\"end\":62330,\"start\":62326},{\"end\":62340,\"start\":62337},{\"end\":62351,\"start\":62349},{\"end\":62362,\"start\":62359},{\"end\":62625,\"start\":62621},{\"end\":62636,\"start\":62632},{\"end\":62847,\"start\":62843},{\"end\":62860,\"start\":62858},{\"end\":62871,\"start\":62868},{\"end\":62881,\"start\":62878},{\"end\":62896,\"start\":62892},{\"end\":63106,\"start\":63102},{\"end\":63119,\"start\":63117},{\"end\":63130,\"start\":63126},{\"end\":63141,\"start\":63137},{\"end\":63156,\"start\":63152},{\"end\":63379,\"start\":63377},{\"end\":63394,\"start\":63389},{\"end\":63407,\"start\":63405},{\"end\":63418,\"start\":63415},{\"end\":63634,\"start\":63632},{\"end\":63645,\"start\":63643},{\"end\":63660,\"start\":63652},{\"end\":63678,\"start\":63671},{\"end\":63877,\"start\":63874},{\"end\":63889,\"start\":63887},{\"end\":63910,\"start\":63905},{\"end\":63922,\"start\":63920},{\"end\":63944,\"start\":63939},{\"end\":63957,\"start\":63953},{\"end\":63967,\"start\":63964},{\"end\":63979,\"start\":63975},{\"end\":63991,\"start\":63987},{\"end\":64006,\"start\":63998},{\"end\":64352,\"start\":64348},{\"end\":64364,\"start\":64362},{\"end\":64378,\"start\":64374},{\"end\":64397,\"start\":64385},{\"end\":64417,\"start\":64409},{\"end\":64432,\"start\":64424},{\"end\":64681,\"start\":64678},{\"end\":64696,\"start\":64692},{\"end\":64709,\"start\":64706},{\"end\":64720,\"start\":64716},{\"end\":64736,\"start\":64732},{\"end\":64747,\"start\":64743},{\"end\":64979,\"start\":64976},{\"end\":64994,\"start\":64990},{\"end\":65010,\"start\":65006},{\"end\":65021,\"start\":65017},{\"end\":65347,\"start\":65343},{\"end\":65360,\"start\":65358},{\"end\":65647,\"start\":65642},{\"end\":65660,\"start\":65656},{\"end\":65670,\"start\":65666},{\"end\":65679,\"start\":65677},{\"end\":65951,\"start\":65947},{\"end\":65961,\"start\":65957},{\"end\":65977,\"start\":65973},{\"end\":65988,\"start\":65985},{\"end\":66000,\"start\":65996},{\"end\":66015,\"start\":66010},{\"end\":66031,\"start\":66027},{\"end\":66044,\"start\":66041},{\"end\":66343,\"start\":66340},{\"end\":66354,\"start\":66352},{\"end\":66363,\"start\":66361},{\"end\":66374,\"start\":66371},{\"end\":66382,\"start\":66380},{\"end\":66394,\"start\":66390}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3204468},\"end\":49367,\"start\":49101},{\"attributes\":{\"id\":\"b1\"},\"end\":49800,\"start\":49369},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7953982},\"end\":50111,\"start\":49802},{\"attributes\":{\"id\":\"b3\"},\"end\":50473,\"start\":50113},{\"attributes\":{\"doi\":\"CoRR abs/2010.03240\",\"id\":\"b4\"},\"end\":50827,\"start\":50475},{\"attributes\":{\"doi\":\"CoRR abs/2002.05709\",\"id\":\"b5\"},\"end\":51158,\"start\":50829},{\"attributes\":{\"id\":\"b6\"},\"end\":51546,\"start\":51160},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":146120595},\"end\":51988,\"start\":51548},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9155618},\"end\":52233,\"start\":51990},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":52568,\"start\":52235},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4009713},\"end\":52826,\"start\":52570},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9665943},\"end\":53135,\"start\":52828},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5575601},\"end\":53397,\"start\":53137},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15816723},\"end\":53703,\"start\":53399},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4755450},\"end\":53945,\"start\":53705},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219558740},\"end\":54175,\"start\":53947},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207930212},\"end\":54473,\"start\":54177},{\"attributes\":{\"id\":\"b17\"},\"end\":54767,\"start\":54475},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":211043589},\"end\":55110,\"start\":54769},{\"attributes\":{\"id\":\"b19\"},\"end\":55454,\"start\":55112},{\"attributes\":{\"id\":\"b20\"},\"end\":55706,\"start\":55456},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52055130},\"end\":56129,\"start\":55708},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":213085920},\"end\":56448,\"start\":56131},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":216080787},\"end\":56800,\"start\":56450},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207168823},\"end\":57025,\"start\":56802},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":202888986},\"end\":57387,\"start\":57027},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204512528},\"end\":57698,\"start\":57389},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3361310},\"end\":57967,\"start\":57700},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":16447573},\"end\":58307,\"start\":57969},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2688907},\"end\":58612,\"start\":58309},{\"attributes\":{\"id\":\"b30\"},\"end\":58980,\"start\":58614},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7605591},\"end\":59317,\"start\":58982},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8750954},\"end\":59585,\"start\":59319},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10795036},\"end\":59889,\"start\":59587},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":199441876},\"end\":60257,\"start\":59891},{\"attributes\":{\"id\":\"b35\"},\"end\":60680,\"start\":60259},{\"attributes\":{\"id\":\"b36\"},\"end\":60890,\"start\":60682},{\"attributes\":{\"doi\":\"CoRR abs/1706.02263\",\"id\":\"b37\"},\"end\":61126,\"start\":60892},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14989939},\"end\":61443,\"start\":61128},{\"attributes\":{\"doi\":\"CoRR abs/1807.03748\",\"id\":\"b39\"},\"end\":61711,\"start\":61445},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3292002},\"end\":61983,\"start\":61713},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":52877454},\"end\":62241,\"start\":61985},{\"attributes\":{\"id\":\"b42\"},\"end\":62513,\"start\":62243},{\"attributes\":{\"id\":\"b43\"},\"end\":62775,\"start\":62515},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":159042183},\"end\":63056,\"start\":62777},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":150380651},\"end\":63293,\"start\":63058},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4591284},\"end\":63582,\"start\":63295},{\"attributes\":{\"id\":\"b47\"},\"end\":63801,\"start\":63584},{\"attributes\":{\"doi\":\"CoRR abs/2007.12865\",\"id\":\"b48\"},\"end\":64271,\"start\":63803},{\"attributes\":{\"id\":\"b49\"},\"end\":64622,\"start\":64273},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":225076220},\"end\":64904,\"start\":64624},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":219708273},\"end\":65193,\"start\":64906},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":210164320},\"end\":65556,\"start\":65195},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2610055},\"end\":65839,\"start\":65558},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":221150341},\"end\":66275,\"start\":65841},{\"attributes\":{\"doi\":\"CoRR abs/2010.14945\",\"id\":\"b55\"},\"end\":66571,\"start\":66277}]", "bib_title": "[{\"end\":49189,\"start\":49101},{\"end\":49876,\"start\":49802},{\"end\":51630,\"start\":51548},{\"end\":52031,\"start\":51990},{\"end\":52315,\"start\":52235},{\"end\":52636,\"start\":52570},{\"end\":52872,\"start\":52828},{\"end\":53210,\"start\":53137},{\"end\":53491,\"start\":53399},{\"end\":53754,\"start\":53705},{\"end\":54003,\"start\":53947},{\"end\":54242,\"start\":54177},{\"end\":54848,\"start\":54769},{\"end\":55787,\"start\":55708},{\"end\":56180,\"start\":56131},{\"end\":56481,\"start\":56450},{\"end\":56884,\"start\":56802},{\"end\":57103,\"start\":57027},{\"end\":57471,\"start\":57389},{\"end\":57752,\"start\":57700},{\"end\":58044,\"start\":57969},{\"end\":58396,\"start\":58309},{\"end\":59056,\"start\":58982},{\"end\":59393,\"start\":59319},{\"end\":59644,\"start\":59587},{\"end\":60006,\"start\":59891},{\"end\":60381,\"start\":60259},{\"end\":61179,\"start\":61128},{\"end\":61737,\"start\":61713},{\"end\":62003,\"start\":61985},{\"end\":62612,\"start\":62515},{\"end\":62835,\"start\":62777},{\"end\":63094,\"start\":63058},{\"end\":63367,\"start\":63295},{\"end\":64669,\"start\":64624},{\"end\":64967,\"start\":64906},{\"end\":65335,\"start\":65195},{\"end\":65633,\"start\":65558},{\"end\":65941,\"start\":65841}]", "bib_author": "[{\"end\":49204,\"start\":49191},{\"end\":49219,\"start\":49204},{\"end\":49473,\"start\":49457},{\"end\":49485,\"start\":49473},{\"end\":49498,\"start\":49485},{\"end\":49510,\"start\":49498},{\"end\":49522,\"start\":49510},{\"end\":49537,\"start\":49522},{\"end\":49558,\"start\":49537},{\"end\":49571,\"start\":49558},{\"end\":49894,\"start\":49878},{\"end\":49907,\"start\":49894},{\"end\":49924,\"start\":49907},{\"end\":49940,\"start\":49924},{\"end\":50230,\"start\":50219},{\"end\":50242,\"start\":50230},{\"end\":50255,\"start\":50242},{\"end\":50265,\"start\":50255},{\"end\":50280,\"start\":50265},{\"end\":50488,\"start\":50475},{\"end\":50500,\"start\":50488},{\"end\":50512,\"start\":50500},{\"end\":50911,\"start\":50900},{\"end\":50928,\"start\":50911},{\"end\":50946,\"start\":50928},{\"end\":50965,\"start\":50946},{\"end\":51250,\"start\":51236},{\"end\":51261,\"start\":51250},{\"end\":51275,\"start\":51261},{\"end\":51288,\"start\":51275},{\"end\":51301,\"start\":51288},{\"end\":51311,\"start\":51301},{\"end\":51322,\"start\":51311},{\"end\":51338,\"start\":51322},{\"end\":51642,\"start\":51632},{\"end\":51655,\"start\":51642},{\"end\":51667,\"start\":51655},{\"end\":51676,\"start\":51667},{\"end\":51687,\"start\":51676},{\"end\":51696,\"start\":51687},{\"end\":51705,\"start\":51696},{\"end\":51722,\"start\":51705},{\"end\":51733,\"start\":51722},{\"end\":51748,\"start\":51733},{\"end\":52048,\"start\":52033},{\"end\":52080,\"start\":52048},{\"end\":52088,\"start\":52080},{\"end\":52331,\"start\":52317},{\"end\":52347,\"start\":52331},{\"end\":52359,\"start\":52347},{\"end\":52379,\"start\":52359},{\"end\":52654,\"start\":52638},{\"end\":52669,\"start\":52654},{\"end\":52686,\"start\":52669},{\"end\":52889,\"start\":52874},{\"end\":52910,\"start\":52889},{\"end\":52927,\"start\":52910},{\"end\":52942,\"start\":52927},{\"end\":52957,\"start\":52942},{\"end\":53227,\"start\":53212},{\"end\":53242,\"start\":53227},{\"end\":53510,\"start\":53493},{\"end\":53526,\"start\":53510},{\"end\":53776,\"start\":53756},{\"end\":53789,\"start\":53776},{\"end\":53804,\"start\":53789},{\"end\":54020,\"start\":54005},{\"end\":54044,\"start\":54020},{\"end\":54256,\"start\":54244},{\"end\":54267,\"start\":54256},{\"end\":54277,\"start\":54267},{\"end\":54290,\"start\":54277},{\"end\":54307,\"start\":54290},{\"end\":54590,\"start\":54578},{\"end\":54608,\"start\":54590},{\"end\":54863,\"start\":54850},{\"end\":54874,\"start\":54863},{\"end\":54886,\"start\":54874},{\"end\":54894,\"start\":54886},{\"end\":54911,\"start\":54894},{\"end\":54922,\"start\":54911},{\"end\":55125,\"start\":55112},{\"end\":55137,\"start\":55125},{\"end\":55152,\"start\":55137},{\"end\":55167,\"start\":55152},{\"end\":55182,\"start\":55167},{\"end\":55197,\"start\":55182},{\"end\":55471,\"start\":55458},{\"end\":55482,\"start\":55471},{\"end\":55497,\"start\":55482},{\"end\":55510,\"start\":55497},{\"end\":55518,\"start\":55510},{\"end\":55533,\"start\":55518},{\"end\":55793,\"start\":55789},{\"end\":55806,\"start\":55793},{\"end\":55820,\"start\":55806},{\"end\":55846,\"start\":55820},{\"end\":55860,\"start\":55846},{\"end\":55876,\"start\":55860},{\"end\":55892,\"start\":55876},{\"end\":55907,\"start\":55892},{\"end\":56193,\"start\":56182},{\"end\":56204,\"start\":56193},{\"end\":56218,\"start\":56204},{\"end\":56234,\"start\":56218},{\"end\":56247,\"start\":56234},{\"end\":56262,\"start\":56247},{\"end\":56277,\"start\":56262},{\"end\":56499,\"start\":56483},{\"end\":56515,\"start\":56499},{\"end\":56526,\"start\":56515},{\"end\":56539,\"start\":56526},{\"end\":56554,\"start\":56539},{\"end\":56569,\"start\":56554},{\"end\":56586,\"start\":56569},{\"end\":56594,\"start\":56586},{\"end\":56610,\"start\":56594},{\"end\":56900,\"start\":56886},{\"end\":57120,\"start\":57105},{\"end\":57133,\"start\":57120},{\"end\":57152,\"start\":57133},{\"end\":57166,\"start\":57152},{\"end\":57181,\"start\":57166},{\"end\":57195,\"start\":57181},{\"end\":57483,\"start\":57473},{\"end\":57493,\"start\":57483},{\"end\":57501,\"start\":57493},{\"end\":57515,\"start\":57501},{\"end\":57527,\"start\":57515},{\"end\":57767,\"start\":57754},{\"end\":57785,\"start\":57767},{\"end\":57804,\"start\":57785},{\"end\":57817,\"start\":57804},{\"end\":58061,\"start\":58046},{\"end\":58077,\"start\":58061},{\"end\":58087,\"start\":58077},{\"end\":58106,\"start\":58087},{\"end\":58120,\"start\":58106},{\"end\":58415,\"start\":58398},{\"end\":58697,\"start\":58683},{\"end\":58709,\"start\":58697},{\"end\":58722,\"start\":58709},{\"end\":58734,\"start\":58722},{\"end\":58748,\"start\":58734},{\"end\":58759,\"start\":58748},{\"end\":58773,\"start\":58759},{\"end\":58783,\"start\":58773},{\"end\":59082,\"start\":59058},{\"end\":59094,\"start\":59082},{\"end\":59109,\"start\":59094},{\"end\":59123,\"start\":59109},{\"end\":59133,\"start\":59123},{\"end\":59411,\"start\":59395},{\"end\":59436,\"start\":59411},{\"end\":59662,\"start\":59646},{\"end\":59687,\"start\":59662},{\"end\":59701,\"start\":59687},{\"end\":59722,\"start\":59701},{\"end\":60021,\"start\":60008},{\"end\":60038,\"start\":60021},{\"end\":60051,\"start\":60038},{\"end\":60062,\"start\":60051},{\"end\":60398,\"start\":60383},{\"end\":60410,\"start\":60398},{\"end\":60421,\"start\":60410},{\"end\":60432,\"start\":60421},{\"end\":60446,\"start\":60432},{\"end\":60462,\"start\":60446},{\"end\":60742,\"start\":60727},{\"end\":60758,\"start\":60742},{\"end\":60773,\"start\":60758},{\"end\":60947,\"start\":60931},{\"end\":60962,\"start\":60947},{\"end\":60972,\"start\":60962},{\"end\":60981,\"start\":60972},{\"end\":61201,\"start\":61181},{\"end\":61219,\"start\":61201},{\"end\":61235,\"start\":61219},{\"end\":61254,\"start\":61235},{\"end\":61525,\"start\":61505},{\"end\":61535,\"start\":61525},{\"end\":61550,\"start\":61535},{\"end\":61757,\"start\":61739},{\"end\":61775,\"start\":61757},{\"end\":61793,\"start\":61775},{\"end\":61809,\"start\":61793},{\"end\":61821,\"start\":61809},{\"end\":61836,\"start\":61821},{\"end\":62023,\"start\":62005},{\"end\":62038,\"start\":62023},{\"end\":62058,\"start\":62038},{\"end\":62070,\"start\":62058},{\"end\":62085,\"start\":62070},{\"end\":62100,\"start\":62085},{\"end\":62321,\"start\":62307},{\"end\":62332,\"start\":62321},{\"end\":62342,\"start\":62332},{\"end\":62353,\"start\":62342},{\"end\":62364,\"start\":62353},{\"end\":62627,\"start\":62614},{\"end\":62638,\"start\":62627},{\"end\":62849,\"start\":62837},{\"end\":62862,\"start\":62849},{\"end\":62873,\"start\":62862},{\"end\":62883,\"start\":62873},{\"end\":62898,\"start\":62883},{\"end\":63108,\"start\":63096},{\"end\":63121,\"start\":63108},{\"end\":63132,\"start\":63121},{\"end\":63143,\"start\":63132},{\"end\":63158,\"start\":63143},{\"end\":63381,\"start\":63369},{\"end\":63396,\"start\":63381},{\"end\":63409,\"start\":63396},{\"end\":63420,\"start\":63409},{\"end\":63636,\"start\":63625},{\"end\":63647,\"start\":63636},{\"end\":63662,\"start\":63647},{\"end\":63680,\"start\":63662},{\"end\":63879,\"start\":63864},{\"end\":63891,\"start\":63879},{\"end\":63912,\"start\":63891},{\"end\":63924,\"start\":63912},{\"end\":63946,\"start\":63924},{\"end\":63959,\"start\":63946},{\"end\":63969,\"start\":63959},{\"end\":63981,\"start\":63969},{\"end\":63993,\"start\":63981},{\"end\":64008,\"start\":63993},{\"end\":64354,\"start\":64344},{\"end\":64366,\"start\":64354},{\"end\":64380,\"start\":64366},{\"end\":64399,\"start\":64380},{\"end\":64419,\"start\":64399},{\"end\":64434,\"start\":64419},{\"end\":64683,\"start\":64671},{\"end\":64698,\"start\":64683},{\"end\":64711,\"start\":64698},{\"end\":64722,\"start\":64711},{\"end\":64738,\"start\":64722},{\"end\":64749,\"start\":64738},{\"end\":64981,\"start\":64969},{\"end\":64996,\"start\":64981},{\"end\":65012,\"start\":64996},{\"end\":65023,\"start\":65012},{\"end\":65349,\"start\":65337},{\"end\":65362,\"start\":65349},{\"end\":65649,\"start\":65635},{\"end\":65662,\"start\":65649},{\"end\":65672,\"start\":65662},{\"end\":65681,\"start\":65672},{\"end\":65953,\"start\":65943},{\"end\":65963,\"start\":65953},{\"end\":65979,\"start\":65963},{\"end\":65990,\"start\":65979},{\"end\":66002,\"start\":65990},{\"end\":66017,\"start\":66002},{\"end\":66033,\"start\":66017},{\"end\":66046,\"start\":66033},{\"end\":66345,\"start\":66332},{\"end\":66356,\"start\":66345},{\"end\":66365,\"start\":66356},{\"end\":66376,\"start\":66365},{\"end\":66384,\"start\":66376},{\"end\":66396,\"start\":66384}]", "bib_venue": "[{\"end\":49222,\"start\":49219},{\"end\":49455,\"start\":49369},{\"end\":49943,\"start\":49940},{\"end\":50217,\"start\":50113},{\"end\":50645,\"start\":50531},{\"end\":50898,\"start\":50829},{\"end\":51234,\"start\":51160},{\"end\":51754,\"start\":51748},{\"end\":52092,\"start\":52088},{\"end\":52388,\"start\":52379},{\"end\":52690,\"start\":52686},{\"end\":52961,\"start\":52957},{\"end\":53252,\"start\":53242},{\"end\":53536,\"start\":53526},{\"end\":53811,\"start\":53804},{\"end\":54048,\"start\":54044},{\"end\":54311,\"start\":54307},{\"end\":54576,\"start\":54475},{\"end\":54927,\"start\":54922},{\"end\":55260,\"start\":55197},{\"end\":55571,\"start\":55533},{\"end\":55911,\"start\":55907},{\"end\":56281,\"start\":56277},{\"end\":56617,\"start\":56610},{\"end\":56903,\"start\":56900},{\"end\":57199,\"start\":57195},{\"end\":57531,\"start\":57527},{\"end\":57820,\"start\":57817},{\"end\":58124,\"start\":58120},{\"end\":58442,\"start\":58415},{\"end\":58681,\"start\":58614},{\"end\":59137,\"start\":59133},{\"end\":59440,\"start\":59436},{\"end\":59725,\"start\":59722},{\"end\":60066,\"start\":60062},{\"end\":60466,\"start\":60462},{\"end\":60725,\"start\":60682},{\"end\":60929,\"start\":60892},{\"end\":61258,\"start\":61254},{\"end\":61503,\"start\":61445},{\"end\":61840,\"start\":61836},{\"end\":62104,\"start\":62100},{\"end\":62305,\"start\":62243},{\"end\":62642,\"start\":62638},{\"end\":62904,\"start\":62898},{\"end\":63163,\"start\":63158},{\"end\":63424,\"start\":63420},{\"end\":63623,\"start\":63584},{\"end\":63862,\"start\":63803},{\"end\":64342,\"start\":64273},{\"end\":64756,\"start\":64749},{\"end\":65030,\"start\":65023},{\"end\":65367,\"start\":65362},{\"end\":65686,\"start\":65681},{\"end\":66050,\"start\":66046},{\"end\":66330,\"start\":66277}]"}}}, "year": 2023, "month": 12, "day": 17}