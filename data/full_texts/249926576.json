{"id": 249926576, "updated": "2023-10-05 13:06:52.531", "metadata": {"title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations", "authors": "[{\"first\":\"Chirag\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Satyapriya\",\"last\":\"Krishna\",\"middle\":[]},{\"first\":\"Eshika\",\"last\":\"Saxena\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Pawelczyk\",\"middle\":[]},{\"first\":\"Nari\",\"last\":\"Johnson\",\"middle\":[]},{\"first\":\"Isha\",\"last\":\"Puri\",\"middle\":[]},{\"first\":\"Marinka\",\"last\":\"Zitnik\",\"middle\":[]},{\"first\":\"Himabindu\",\"last\":\"Lakkaraju\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "While several types of post hoc explanation methods (e.g., feature attribution methods) have been proposed in recent literature, there is little to no work on systematically benchmarking these methods in an efficient and transparent manner. Here, we introduce OpenXAI, a comprehensive and extensible open source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to benchmark explanations. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.11104", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.48550/arxiv.2206.11104"}}, "content": {"source": {"pdf_hash": "bc45043633e933bd0969a94ef442587585c764b7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2206.11104v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dea82d31439191056e6df3fad3f3042b14517f8a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bc45043633e933bd0969a94ef442587585c764b7.txt", "contents": "\nOpenXAI: Towards a Transparent Evaluation of Post hoc Model Explanations\n\n\nChirag Agarwal \nHarvard University\n\n\nAdobe\n\n\nEshika Saxena \nHarvard University\n\n\nSatyapriya Krishna \nHarvard University\n\n\nMartin Pawelczyk \nUniversity of Tubingen\n\n\nNari Johnson \nCarnegie Mellon University\n\n\nIsha Puri \nHarvard University\n\n\nMarinka Zitnik \nHarvard University\n\n\nHimabindu Lakkaraju \nHarvard University\n\n\nOpenXAI: Towards a Transparent Evaluation of Post hoc Model Explanations\n\nWhile several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible opensource framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to readily compare several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. While the first release of OpenXAI supports only tabular datasets, the explanation methods and metrics that we consider are general enough to be applicable to other data modalities. OpenXAI datasets and data loaders, implementations of stateof-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/. OpenXAI will be regularly updated to incorporate text and image datasets, other new metrics and explanation methods, and welcomes inputs from the community. * Equal contribution. 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks. arXiv:2206.11104v3 [cs.LG] 16 Jan 2023 2. Our OpenXAI framework currently provides open-source implementations and ready-touse API interfaces for seven state-of-the-art feature attribution methods (LIME, SHAP, Vanilla Gradients, Gradient x Input, SmoothGrad, and Integrated Gradients), and twenty-two quantitative metrics to evaluate the faithfulness, stability, and fairness of feature attribution methods. In addition, it includes a comprehensive collection of seven real-world datasets spanning diverse real-world domains, and sixteen different pre-trained models. OpenXAI also introduces a novel and flexible synthetic data generator to synthesize datasets of varying sizes, complexity, and dimensionality which facilitate the construction of reliable ground truth explanations (Section 2).3. As part of our OpenXAI framework, we also develop the first-ever public XAI leaderboards (shown inFigure 1) to promote transparency, and to allow users to easily compare the performance of multiple explanation methods across a wide variety of synthetic and realworld datasets, evaluation metrics, and predictive models.4. OpenXAI framework is easily extensible i.e., researchers and practitioners can readily incorporate custom explanation methods, datasets, predictive models, and evaluation metrics into our framework and leaderboards (Section 2).5. Lastly, using our proposed OpenXAI framework, we perform rigorous empirical benchmarking of the aforementioned state-of-the-art feature attribution methods to determine which methods are effective w.r.t. what notions of reliability across a wide variety of datasets and predictive models (Section 3).Overall, our OpenXAI framework provides an end-to-end pipeline that unifies, simplifies, and standardizes several existing workflows to evaluate explanation methods. By enabling systematic and efficient evaluation and benchmarking of existing and new explanation methods, our OpenXAI framework can inform and accelerate new research in the emerging field of XAI. OpenXAI will be regularly updated and welcomes input from the community.Related Work. Our work builds on the vast literature in explainable AI. Here, we discuss closely related works and their connections to our benchmark. A more detailed discussion of the related work is included in the Appendix.\n\nIntroduction\n\nAs predictive models are increasingly deployed in critical domains (e.g., healthcare, law, and finance), there has been a growing emphasis on explaining the predictions of these models to decision makers (e.g. doctors, judges) so that they can understand the rationale behind model predictions, and determine if and when to rely on these predictions. To this end, various techniques have been proposed in recent literature to generate post hoc explanations of individual predictions made by complex ML models. Several of such local explanation methods output the influence of each of the features on the model's prediction, and are therefore referred to as local feature attribution methods. Due to their generality, feature attribution methods are increasingly being utilized to explain complex models in medicine, finance, law, and science [23,34,78]. Thus, it is critical to ensure that the explanations generated by these methods are reliable so that relevant stakeholders and decision makers are provided with credible information about the underlying models [6].\n\nPrior works have studied several notions of explanation reliability such as faithfulness (or fidelity) [81,51,33], stability (or robustness) [7,4], and fairness [18,9], and proposed metrics for quantifying these notions. Many of these works also demonstrated through small-scale experiments or qualitative analysis that certain explanation methods are not effective w.r.t. specific notions of reliability. For instance, Alvarez-Melis and Jaakkola [7] visualized the explanations generated by some of the popular gradient based explanation methods [67,66,70,72] for MNIST images, and showed that they are not robust to small input perturbations. However, it is unclear if such findings generalize beyond the settings studied. More broadly, one of the biggest open questions which has far-reaching implications for the progress of explainable AI (XAI) research is: which explanation methods are effective w.r.t. what notions of reliability and under what conditions? [43]. A first step towards answering this question involves systematically benchmarking explanation methods in a reproducible and transparent manner. However, the increasing diversity of explanation methods, and the plethora of evaluation settings and metrics outlined in existing research without standardized open-source implementations make it rather challenging to carry out such benchmarking efforts.\n\nIn this work, we address the aforementioned challenges by introducing OpenXAI, a comprehensive and extensible open-source framework for systematically and efficiently benchmarking explanation methods in a transparent and reproducible fashion. More specifically, our work makes the following key contributions:\n\n1. We introduce the OpenXAI framework, an open-source ecosystem designed to support systematic, reproducible, and efficient evaluations of post hoc explanation methods. OpenXAI unifies the existing scattered repositories of datasets, models, and evaluation metrics, and provides a simple and easy-to-use API that enables researchers and practitioners to benchmark explanation methods using just a few lines of code (Section 2).\n\nEvaluation Metrics for Post hoc Explanations: Prior research has studied several notions of explanation reliability, namely, faithfulness (or fidelity), stability (or robustness), and fairness [51,81,7,18]. While the faithfulness notion captures how faithfully a given explanation captures the true behavior of the underlying model [81,51,33], stability ensures that explanations do not change drastically with small perturbations to the input [29,7]. The fairness notion, on the other hand, ensures that there are no group-based disparities in the faithfulness or stability of explanations [18]. To this end, prior works [51,69,81,7,18,33] proposed various evaluation metrics to quantify the aforementioned notions. For instance, Petsiuk et al. [59] measured the change in the probability of the predicted class when important features (as identified by an explanation) are deleted from or introduced into the data instance. A sharp change in the probability implies a high degree of explanation faithfulness. Alvarez-Melis and Jaakkola [7] loosely quantified stability as the maximum change in the resulting explanations when small perturbations are made to a given instance. Dai et al. [18] quantified unfairness of explanations as the difference between the faithfulness (or stability) metric values averaged over instances in the majority and the minority subgroups.\n\nXAI Libraries and Benchmarks: Prior works have introduced a few XAI libraries and benchmarks, the most popular among them being Captum [42], Quantus [32], XAI-Bench [51], and SHAP Benchmark. Below, we provide a brief description of each of these, and detail how our work differs from them.\n\nWhile Captum library [42] is an open-source library which provides implementations and APIs for various state-of-the-art explanation methods, its focus is not on evaluating and/or benchmarking these methods which is the main goal of our work. Quantus library [32], on the other hand, provides implementations of certain evaluation metrics to measure the faithfulness and stability/robustness of explanation methods. However, it does not focus on benchmarking explanation methods or providing public dashboards to compare the performance of these methods. Furthermore, the stability/robustness measures [7] supported by Quantus are somewhat outdated and have been superseded by recently proposed metrics [4]. In addition, Quantus does not support any fairness metrics to evaluate disparities in the quality of explanations which is very important in real-world settings such as healthcare, criminal justice, and policy. In contrast, OpenXAI not only subsumes popular faithfulness and stability/robustness metrics supported by Quantus but also supports 19 new metrics to measure the faithfulness, stability/robustness, as well as the fairness of explanation methods [4,18,43]. In addition, OpenXAI focuses on systematically benchmarking state-of-the-art explanation methods and providing public dashboards to readily compare these methods.\n\nSHAP benchmark [2] only focuses on evaluating and comparing different variants of SHAP [54] via certain faithfulness metrics which are similar to the Prediction Gap on Important (PGI) and Unimportant (PGU) feature perturbation metrics outlined in our work. Note that the SHAP benchmark does not include any stability/robustness or fairness metrics. In contrast, OpenXAI not only includes 20 new metrics to evaluate the stability/robustness and fairness of explanation methods but also benchmarks various other methods (e.g., LIME, Gradient-based methods). [51] constructed synthetic datasets with ground truth explanations to evaluate the faithfulness of a few explanation methods (e.g., LIME, SHAP, MAPLE). However, recent research argued that their evaluation is unreliable, and predictive models learned using their synthetic datasets may not adhere to the ground truth explanations [24]. In addition, the aforementioned evaluation is rather limited in scope as synthetic datasets may not even be representative of real-world data [24]. In contrast, our work not only proposes a novel synthetic data generator that addresses the shortcomings of the synthetic datasets constructed in XAI-Bench but also facilitates the evaluation and benchmarking of the faithfulness, stability, as well as the fairness of 7 state-of-the-art explanation methods on 7 real-world datasets with no ground truth explanations.\n\n\nXAI-Bench\n\nIn summary, our work is significantly different from existing libraries and benchmarks, and makes the following key contributions:\n\n\u2022 We provide implementations and easy-to-use API interfaces for 22 metrics to evaluate the faithfulness, stability, and fairness of explanation methods. 18 out of the 22 state-ofthe-art metrics included in OpenXAI have not been implemented in any prior libraries or benchmarks -e.g., faithfulness metrics such as Feature Agreement (FA), Rank Agreement (RA), Sign Agreement (SA), Signed Rank Agreement (SRA), Pairwise Rank Agreement (PRA), stability metrics such as Relative Representation Stability (RRS), Relative Output Stability (ROS), and all fairness metrics.\n\n\u2022 We also introduce a novel and flexible synthetic data generator to synthesize datasets of varying sizes, complexity, and dimensionality to facilitate the construction of reliable ground truth explanations in order to evaluate state-of-the-art explanation methods. Our synthetic data generator addresses the shortcomings of the prior synthetic benchmark (XAI-Bench) by generating synthetic datasets which encapsulate certain key properties, namely, unambiguously defined local neighborhoods, a clear description of feature importances in each local neighborhood, and feature independence. These properties, in turn, allow us to theoretically guarantee that any accurate model trained on our synthetic datasets will adhere to the ground truth explanations of the underlying data.\n\n\u2022 We perform rigorous empirical benchmarking of 7 state-of-the-art feature attribution methods using our OpenXAI framework to determine which methods are effective w.r.t. each of the 22 evaluation metrics across 8 real-world and synthetic datasets, and 16 different predictive models. Note that none of the previously proposed libraries or benchmarks carry out such exhaustive benchmarking efforts across such a wide variety of metrics, models, and datasets. We also introduce the first ever public XAI leaderboards with such a wide variety of explanation methods, metrics, models, and datasets, to promote transparency and showcase the results of our benchmarking efforts.\n\n\nOverview of OpenXAI Framework\n\nOpenXAI provides a comprehensive programmatic environment with synthetic and real-world datasets, data processing functions, explainers, and evaluation metrics to rigorously and efficiently benchmark explanation methods. Below, we discuss each of these components in detail.\n\n\n1) Datasets and Predictive Models.\n\nThe current release of our OpenXAI framework includes a collection of eight different synthetic and real-world datasets. While synthetic datasets allow us to construct ground truth explanations which can then be used to evaluate explanations output by state-of-the-art methods, real-world datasets (where it is typically hard to construct ground truth explanations) help us benchmark these methods in a more realistic manner suitable for practical applications [51]. We would like to note that OpenXAI includes datasets that are widely employed in XAI research to evaluate the efficacy of newly proposed methods and study the behavior of existing methods [9, 18-20, 38, 69, 73].\n\nSynthetic Datasets: While prior research [51,41] proposed methods to generate synthetic datasets and corresponding ground truth explanations, they all suffer from a significant drawback as demonstrated by Faber et al. [24] -there is no guarantee that the models trained on these datasets will adhere to the ground truth explanations of the underlying data. This, in turn, implies that evaluating post hoc explanations using the above ground truth explanations would be incorrect since post hoc explanations are supposed to reliably explain the behavior of the underlying model, and not that of the underlying data. To illustrate, let us consider the case where we use aforementioned methods to construct a synthetic dataset with features A, B, C, and D such that the ground truth labels only depend on features A and B i.e., the ground truth explanation of the underlying data indicates that features A and B are most important. If we train a model on this data and if features A and B are correlated with C and D respectively, then the resulting model may base its predictions on C and D (and not A and B) and still be very accurate. If a post hoc explanation of this model then (correctly) indicates that the most important features of the model are C and D, this explanation may be deemed incorrect if we compare it against the ground truth explanation of the underlying data. This problem further exacerbates as we increase the complexity of the ground truth labeling function [24].\n\nTo address the aforementioned challenges, we develop a novel synthetic data generation mechanism, SynthGauss, which encapsulates three key properties, namely, feature independence, unambiguouslydefined local neighborhoods, and a clear description of feature influence in each local neighborhood. Intuitively, this approach generates K well-separated clusters where points in each cluster k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , K} are sampled from a Gaussian distribution N (\u00b5 k , \u03a3 k ) where u k \u2208 R d is the mean and \u03a3 k \u2208 R d\u00d7d is the covariance matrix. While this parameterization is general enough to support the construction of synthetic datasets of K clusters with varying means and covariances, we set the means of all the clusters such that the intracluster distances are significantly smaller than the intercluster distances, and we set the covariance matrices of all the clusters to identity. This ensures that all the features are independent, and local neighborhoods (clusters) are unambiguously defined.\n\nWe then generate ground truth labels for instances by first randomly sampling feature mask vectors m k \u2208 {0, 1} d (vectors comprising of 0s and 1s) for each cluster k. The vector m k determines which features influence the ground truth labeling process for instances in cluster k (a value of 1 indicates that the corresponding feature is influential). We then randomly sample feature weight vectors w k \u2208 R d which capture the relative importance of each of the features in the labeling process of instances in each cluster k. The ground truth labels of instances in each cluster k are then computed as a function (e.g., sigmoid) of the feature values of individual instances, and the dot product of the corresponding cluster's feature mask vector and weight vector i.e., m k w k . Complete pseudocode and other details of this generation process are included in the Appendix. Note that m k corresponds to the ground truth explanation for all instances in cluster k. Since our generation process is designed to encapsulate feature independence, unambiguous definitions of local neighborhoods, and clear descriptions of feature influences, any accurate model trained on the resulting dataset will adhere to the ground truth explanations of the underlying data (See Theorem 1 in Appendix).\n\nReal-world Datasets: In the current release of OpenXAI, we include seven real-world datasets that are highly diverse in terms of several key properties. They comprise of data spanning multiple real-world domains (e.g., finance, lending, healthcare, and criminal justice), varying dataset sizes (e.g., small vs. large-scale), dimensionalities (e.g., low vs. high dimensional), class imbalance ratios, and feature types (e.g., continuous vs. discrete). We focus on tabular data in this release as such data is commonly encountered in real-world applications where explainability is critical [75], and has also been widely studied in XAI literature [51]. Table 1 provides a summary of the real-world datasets currently included in OpenXAI. See Section E.1 in the Appendix for detailed descriptions of individual datasets. While these real-world datasets are primarily drawn from prior research and existing repositories, OpenXAI provides comprehensive data loading and pre-processing capabilities to make these datasets XAI-ready (more details below). We also plan to expand our collection of real-world datasets in the next iteration. Adding a new dataset into our collection is as simple as uploading a .csv file or a .zip folder. Users can also submit requests to incorporate new datasets into the OpenXAI framework by filling a simple form and providing links to the datasets (See Appendix). We also pre-trained two classes of predictive models (e.g., deep neural networks of varying degrees of complexity, logistic regression models etc.) and incorporated them into the OpenXAI framework so that they can be readily used for benchmarking explanation methods. The code snippet below shows how to load OpenXAI's pre-trained models using our LoadModel class.\n\nfrom OpenXAI import LoadModel model = LoadModel(data_name='german', ml_model='ann')\n\nAdding additional pre-trained models into the OpenXAI framework is as simple as uploading a file with details about model architecture and parameters in a specific template. Users can also submit requests to incorporate custom pre-trained models into the OpenXAI framework by filling a simple form and providing details about model architecture and parameters (See Appendix).\n\n2) Explainers. OpenXAI provides ready-to-use implementations of six state-of-the-art feature attribution methods, namely, LIME, SHAP, Vanilla Gradients, Gradient x Input, SmoothGrad, and Integrated Gradients. An implementation of a random baseline which randomly assigns importance values to each of the features, and returns these random assignments as explanations is also included. Our implementations of these methods build on other open-source libraries (e.g., Captum [42]) as well as their original implementations. While methods such as LIME and SHAP leverage perturbations of data instances and their corresponding model predictions to learn a local explanation model, they do not require access to the internals of the models or their gradients. On the other hand, Vanilla Gradients, Gradient x Input, SmoothGrad, and Integrated Gradients require access to the gradients of the underlying models but do not need to repeatedly query the models for their predictions (see Table 6 in Appendix for a brief summary of these methods). These differences influence the efficiency with which explanations can be generated by these methods. OpenXAI provides an abstract Explainer class which enables us to load existing explanation methods as well as integrate new explanation methods.\n\nfrom OpenXAI import Explainer exp_method = Explainer(method='LIME') explanations = exp_method.get_explanations(model, X=inputs, y=labels)\n\nAll the explanation methods included in OpenXAI are readily accessible through the Explainer class, and users just have to specify the method name in order to invoke the appropriate method and generate explanations as shown in the above code snippet. Users can easily incorporate their own custom explanation methods into the OpenXAI framework by extending the Explainer class and including the code for their methods in the get_explanations function (see template below) of this class. They can then submit a request to incorporate their custom methods into OpenXAI library by filling a form and providing the GitHub link to their code as well as a summary of their explanation method (See Appendix).\n\n3) Evaluation Metrics. OpenXAI provides implementations and ready-to-use APIs for a set of twenty-two quantitative metrics proposed by prior research to evaluate the faithfulness, stability, and fairness of explanation methods. OpenXAI is the first XAI benchmark to consider all the three aforementioned aspects of explanation reliability. More specifically, we include eight different metrics to measure explanation faithfulness (both with and without ground truth explanations) [43,59], three different metrics to measure stability [4], and eleven different metrics to measure group-based disparities (unfairness) [18] in the values of the aforementioned faithfulness and stability metrics. The metrics that we choose are drawn from the latest works in explainable AI literature. Below, we briefly describe these metrics. Detailed descriptions of all the metrics along with notation and equations are included in the Appendix.\n\na) Ground-truth Faithfulness: Krishna et al. [43] recently proposed six evaluation metrics to capture the similarity between the top-K or a select set of features of any two feature attribution-based explanations. We leverage these metrics to capture the similarity between the explanations output by state-of-the-art methods and the ground-truth explanations constructed using our synthetic data generation process. These metrics and their definitions are given as follows: i) Feature Agreement (FA) which computes the fraction of top-K features that are common between a given post hoc explanation and the corresponding ground truth explanation, ii) Rank Agreement (RA) metric which measures the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also have the same position in the respective rank orders, iii) Sign Agreement (SA) metric which computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same sign (direction of contribution) in both the explanations, iv) Signed Rank Agreement (SRA) metric which computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same feature attribution sign (direction of contribution) and position (rank) in both the explanations, v) Rank Correlation (RC) metric which computes Spearman's rank correlation coefficient to measure the agreement between feature rankings provided by a given post hoc explanation and the corresponding ground truth explanation, and vi) Pairwise Rank Agreement (PRA) metric which captures if the relative ordering of every pair of features is the same for a given post hoc explanation as well as the corresponding ground-truth explanation.\n\nb) Predictive Faithfulness: We leverage the metrics outlined by [59,18] to measure the faithfulness of an explanation when no ground truth is available. This metric, referred to as Prediction Gap on Important feature perturbation (PGI), computes the difference in prediction probability that results from perturbing the features deemed as influential by a given post hoc explanation. Higher values on this metric imply greater explanation faithfulness. We also consider the converse of this metric, Prediction Gap on Unimportant feature perturbation (PGU), which perturbs the unimportant features and measures the change in prediction probability. c) Stability: We consider the metrics introduced by Alvarez-Melis and Jaakkola [7], Agarwal et al. [4] to measure how robust a given explanation is to small input perturbations. More specifically, we leverage the metrics Relative Input Stability (RIS), Relative Representation Stability (RRS), and Relative Output Stability (ROS) which measure the maximum change in explanation relative to changes in the inputs, model parameters, and output prediction probabilities respectively. d) Fairness: Following the work by Dai et al. [18], we measure the fairness of post hoc explanations by averaging all the aforementioned metric values across instances in the majority and minority subgroups, and comparing the two estimates. If there is a huge difference in the two estimates, then we consider this to be evidence for unfairness.\n\nInvoking the aforementioned metrics to benchmark an explanation methods is quite simple and the code snippet below describes how to invoke the RIS metric. Users can easily incorporate their own custom evaluation metrics into OpenXAI by filling a form and providing the GitHub link to their code as well as a summary of their metric (See Appendix).\n\nfrom OpenXAI import Evaluator metric_evaluator = Evaluator(inputs, labels, model, explanations) score = metric_evaluator.eval(metric='RIS')\n\nBenchmarking: As can be seen from the code snippets in this section, OpenXAI allows end users to easily benchmark explanation methods using just a few lines of code. To summarize the benchmarking process, let us consider a scenario where we would like to benchmark a new explanation method using OpenXAI's pre-trained neural network model and the German Credit dataset. First, we use OpenXAI's Dataloader class to load the German Credit dataset. Second, we load the neural network model ('ann') using our LoadModel class. Third, we extend the Explainer class and incorporate the code for the new explanation method in the get_explanation function of this class. Finally, we evaluate the new explanation method using various metrics from the Evaluator class.\n\n\n4) Leaderboards.\n\nOpenXAI introduces the first ever public XAI leaderboards to promote transparency, and enable users to easily compare the performance of multiple explanation methods across a variety of evaluation metrics, predictive models, and datasets. In the current release, we have six different leaderboards each corresponding to a particular dataset. A snapshot of one of our leaderboard pages is shown in Figure 1. Users can submit requests for their custom explanation methods to be featured on one of our leaderboards. To this end, they first need to following the aforementioned benchmarking process to develop and evaluate their explanation method. \n\n\nBenchmarking Analysis\n\nNext, we describe how we benchmark state-of-the-art explanation methods using our OpenXAI framework, and also discuss key findings of this benchmarking analysis. Code to reproduce all the results is available at https://github.com/AI4LIFE-GROUP/OpenXAI.\n\n\nExperimental Setup.\n\nWe benchmark all the six state-of-the-art feature attribution methods currently available in our OpenXAI framework along with the random baseline, using the openxai.Evaluator module (See Section 2). We use default hyperparameter settings for all these methods following the guidelines outlined in the original implementations. Details about the hyperparameters used in our experiments are discussed in Section E.3 in the Appendix. Our OpenXAI framework currently has two pre-trained models, a logistic regression model and a deep neural network model, for each dataset. The neural network models have two fully connected hidden layers with 100 nodes in each layer, and they use ReLU activation functions and an output softmax layer. See Appendix E.4 for more details on model architectures, model training, and model performance.  Faithfulness. We evaluate the ground-truth and predictive faithfulness of explanations generated by state-of-the-art methods using both synthetic and real-world datasets.\nMethod PRA (\u2191) RC (\u2191) FA (\u2191) RA (\u2191) SA (\u2191) SRA (\u2191) PGU (\u2193) PGI (\u2191) Random VanillaGrad IntegratedGrad Gradient x Input SmoothGrad SHAP LIME 0.500\u00b10.\nGround-truth faithfulness: We evaluate ground-truth faithfulness by calculating the similarity between the generated explanations and the ground-truth explanations using the metrics discussed in Section 2. Results for various ground-truth faithfulness metrics are shown in Tables 2, 3, 16, 17. Vanilla Gradients, SmoothGrad, and Integrated Gradients produce explanations that achieve perfect scores  [43] that explanations output by state-of-the-art methods do not necessarily align with each other. This finding further highlights the need for rigorous empirical and theoretical benchmarking of explanation methods.\n\nStability. Next, we examine the stability of explanation methods when the underlying models are LR models in Tables 4 and 5, and neural network models in Tables 19 and 20 in the Appendix. Due to space constraints, we focus on RIS and RRS metrics in the main paper and leave the other results to the Appendix. Overall, the relative stability varies considerably across different datasets, implying that no single explanation method is consistently the most stable. First, for the synthetic dataset in Table 4, we find that Gradient x Input, on average, outperforms feature-attribution methods in relative input stability (+93.5%, RIS) and relative representation stability (+59.2%, RRS). However, stability of Gradient x Input significantly degrades on real-world datasets ( Table 5, 19,20). Second, as shown in Tables 5, 19, and 20, there is no single explanation method that has the highest input and representation stability across all the real-world datasets. On average, across all real-world datasets, SmoothGrad achieves 63.2% higher RRS values compared to other methods, whereas no method performs consistently well when it comes to the RIS metric.  Fairness. To measure fairness of explanation methods, we compute the average metric values (for each of the aforementioned faithfulness and stability metrics) for different subgroups (e.g., male and female) in the dataset and compare them. Larger gaps between the metric values for different subgroups indicates higher disparities (unfairness). Without loss of generality, we present results using the PGU (see Section 2 and Appendix A) metric. Results for LR models in Figures 2 and 3 provide two key findings. First, the fairness analysis in Figures 2 and 3 shows that there are disparities in the faithfulness of explanations (see Section 2) output by several methods (Vanilla Gradients, Integrated Gradients, and SmoothGrad). Second, Gradient x Input results in the least amount of disparity across both the datasets. These results also suggest a trade-off between various evaluation metrics. For instance, Gradient x Input method underperforms on faithfulness and stability metrics, but outperforms other methods (+8.9%) when it comes to fairness metrics. Given such trade-offs, practitioners can leverage the OpenXAI leaderboards ( Figure 1) to select an explanation method that best meets application-specific needs. Results with NN models and other fairness metrics are included in the Appendix E.7.\n\n\nRandom\n\nGrad IG Grad x Input SG SHAP LIME 0.00   \n\n\nConclusions\n\nAs post hoc explanations are increasingly being employed to aid decision makers and relevant stakeholders in various high-stakes applications, it becomes important to ensure that these explanations are reliable. To this end, we introduce OpenXAI, an open-source ecosystem comprising of XAI-ready datasets, implementations of state-of-the-art explanation methods, evaluation metrics, leaderboards and documentation to promote transparency and collaboration around evaluations of post hoc explanations. OpenXAI can readily be used to benchmark new explanation methods as well as incorporate them into our framework and leaderboards. By enabling systematic and efficient evaluation and benchmarking of existing and new explanation methods, OpenXAI can inform and accelerate new research in the emerging field of XAI. OpenXAI will be regularly updated with new datasets, explanation methods, and evaluation metrics, and welcomes input from the community.\n\n\nA Evaluation Metrics\n\nHere, we describe different evaluation metrics that we included in the first iteration of OpenXAI. More specifically, we discuss various metrics (and their implementations) for evaluating the faithfulness, stability, and fairness of explanations generated using a given feature attribution method.\n\n\n1) Faithfulness.\n\nTo measure how faithfully a given explanation mimics the underlying model, prior work has either leveraged synthetic datasets to obtain ground truth explanations or measured the differences in predictions when feature values are perturbed [59]. Here, we discuss the two broad categories of faithfulness metrics included in OpenXAI, namely ground-truth and predictive faithfulness.\n\na) Ground-truth Faithfulness. OpenXAI leverages the following metrics outlined by Krishna et al. [43] to calculate the agreement between ground-truth explanations (i.e., coefficients of logistic regression models) and explanations generated by state-of-the-art methods.\n\n\u2022 Feature Agreement (FA) metric computes the fraction of top-K features that are common between a given post hoc explanation and the corresponding ground truth explanation. \u2022 Rank Agreement (RA) metric measures the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also have the same position in the respective rank orders. \u2022 Sign Agreement (SA) metric computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same sign (direction of contribution) in both the explanations. \u2022 Signed Rank Agreement (SRA) metric computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same feature attribution sign (direction of contribution) and position (rank) in both the explanations. \u2022 Rank Correlation (RC) metric computes the Spearman's rank correlation coefficient to measure the agreement between feature rankings provided by a given post hoc explanation and the corresponding ground truth explanation. \u2022 Pairwise Rank Agreement (PRA) metric captures if the relative ordering of every pair of features is the same for a given post hoc explanation as well as the corresponding ground truth explanation i.e., if feature A is more important than B according to one explanation, then the same should be true for the other explanation. More specifically, this metric computes the fraction of feature pairs for which the relative ordering is the same between the two explanations.\n\nReported metric values. While the aforementioned metrics quantify the ground-truth faithfulness of individual explanations, we report a single value corresponding to each explanation method and dataset to facilitate easy comparison of state-of-the-art methods. To this end, we adopt a similar strategy as that of Krishna et al. [43] and \nPGI(x, f, e x , k) = E x \u223cperturb(x, e x , top-K) [|\u0177 \u2212 f (x )|],(1)\nPGU\n(x, f, e x , k) = E x \u223cperturb(x, e x , non top-K) [|\u0177 \u2212 f (x )|],(2)\nwhere perturb(\u00b7) returns the noisy versions of x as described above.\n\nReported metric values. Similar to the ground-truth faithfulness metrics, we report PGI and PGU by calculating the AUC over all values of K.\n\n2) Stability. We leverage the metrics Relative Input Stability (RIS), Relative Representation Stability (RRS), and Relative Output Stability (ROS) [4] which measure the maximum change in explanation relative to changes in the inputs, internal representations learned by the model, and output prediction probabilities respectively. These metrics can be written formally as:\nRIS(x, x , e x , e x ) = max x || (ex\u2212e x ) ex || p max(|| (x\u2212x ) x || p , min ) , \u2200x s.t. x \u2208 N x ;\u0177 x =\u0177 x(3)\nwhere N x is a neighborhood of instances x around x, and e x and e x denote the explanations corresponding to instances x and x , respectively. The numerator of the metric measures the l p norm of the percent change of explanation e x on the perturbed instance x with respect to the explanation e x on the original point x. The denominator measures the l p norm between the (normalized) inputs x and x , and the max term in the denominator prevents division by zero.\nRRS(x, x , e x , e x ) = max x || (ex\u2212e x ) ex || p max(|| (Lx\u2212L x ) Lx || p , min ) , \u2200x s.t. x \u2208 N x ;\u0177 x =\u0177 x(4)\nwhere L(\u00b7) denotes the internal representations learned by the model. Without loss of generality, we use a two layer neural network model in our experiments and use the output of the first layer for computing RRS. Similarly, we can also define ROS as:\nROS(x, x , e x , e x ) = max x || (ex\u2212e x ) ex || p max(|| (f (x)\u2212f (x ) f (x) || p , min ) , \u2200x s.t. x \u2208 N x ;\u0177 x =\u0177 x ,(5)\nwhere f (x) and f (x ) are the output prediction probabilities for x and x , respectively. To the best of our knowledge, OpenXAI is the first benchmark to incorporate all the above stability metrics.\n\nReported metric values.. We compute the aforementioned metrics for each instance in the test set, and then average these values.\n\n\n3) Fairness.\n\nIt is important to ensure that there are no significant disparities between the reliability of post hoc explanations corresponding to instances in the majority and the minority subgroups.\n\nTo this end, we average all the aforementioned metric values across instances in the majority and minority subgroups, and then compare the two estimates (See Figures 3, 2, 5 etc.) to check if there are significant disparities [18].\n\n\nB Additional Related Work\n\nOur work builds on the vast literature on model interpretability and explainability. Below is an overview of additional works that were not included in Section 1 due to space constraints.\n\nInherently Interpretable Models and Post hoc Explanations. Many approaches learn inherently interpretable models such as rule lists [80,77], decision trees and decision lists [48], and others [46,13,53,15]. However, complex models such as deep neural networks often achieve higher accuracy than simpler models [62]. Thus, there has been significant interest in constructing post hoc explanations to understand their behavior. To this end, several techniques have been proposed in recent literature to construct post hoc explanations of complex decision models. For instance, LIME, SHAP, Anchors, BayesLIME, and BayesSHAP [62,55,63,69] are considered perturbation-based local explanation methods because they leverage perturbations of individual instances to construct interpretable local approximations (e.g., linear models). On the other hand, methods such as Vanilla Gradients, Gradient x Input, SmoothGrad, Integrated Gradients, and GradCAM [67,72,65,70] are referred to as gradient-based local explanation methods since they leverage gradients computed with respect to input features of individual instances to explain individual model predictions.\n\nThere has also been recent work on constructing counterfactual explanations which capture what changes need to be made to a given instance in order to flip its prediction [76,74,37,61,52,11,38,57,39,58]. Such explanations can be leveraged to provide recourse to individuals negatively impacted by algorithmic decisions. An alternate class of methods referred to as global explanation methods attempt to summarize the behavior of black-box models as a whole rather than in relation to individual data points [47,12]. A more detailed treatment of this topic is provided in other comprehensive survey articles [8,30,56,49,17].\n\nIn this work, we focus primarily on local feature attribution-based post hoc explanation methods i.e., explanation methods which attempt to explain individual model predictions by outputting a vector of feature importances. More specifically, the goal of this work is to enable systematic benchmarking of these methods in an efficient and transparent manner.\n\nEvaluating Post hoc Explanations. In addition to the quantitative metrics designed to evaluate the reliability of post hoc explanation methods [51,81,7,18] (See Section 1), prior works have also introduced human-grounded approaches to evaluate the interpretability of explanations generated by these methods [21]. For example, Lakkaraju and Bastani [45] carry out a user study to understand if misleading explanations can fool domain experts into deploying racially biased models, while Kaur et al. [40] find that explanations are often over-trusted and misused. Similarly, Poursabzi-Sangdeh et al. [60] find that supposedly-interpretable models can lead to a decreased ability to detect and correct model mistakes, possibly due to information overload. Jesus et al. [35] introduce a method to compare explanation methods based on how subject matter experts perform on specific tasks with the help of explanations. Lage et al. [44] use insights from rigorous human-subject experiments to inform regularizers used in explanation algorithms. In contrast to the aforementioned research, our work leverages twenty-two different state-of-the-art quantitative metrics to systematically benchmark the reliability (and not interpretability) of post hoc explanation methods.\n\nLimitations and Vulnerabilities of Post hoc Explanations. Various quantitative metrics proposed in literature (See Section 1) were also leveraged to analyze the behavior of post hoc explanation methods and their vulnerabilities-e.g., Ghorbani et al. [29] and Slack et al. [68] demonstrated that methods such as LIME and SHAP may result in explanations that are not only inconsistent and unstable, but also prone to adversarial attacks. Furthermore Lakkaraju and Bastani [45] and Slack et al. [68] showed that explanations which do not accurately represent the importance of sensitive attributes (e.g., race, gender) could potentially mislead end users into believing that the underlying models are fair when they are not [45,68,6]. This, in turn, could lead to the deployment of unfair models in critical real world applications. There is also some discussion about whether models which are not inherently interpretable ought to be used in high-stakes decisions at all. Rudin [64] argues that post hoc explanations tend to be unfaithful to the model to the extent that their usefulness is severely compromised. While this line of work demonstrates different ways in which explanations could potentially induce inaccuracies and biases in real world applications, they do not focus on systematic benchmarking of post hoc explanation methods which is the main goal of our work.\n\n\nC Synthetic Dataset\n\nOur proposed data generation process described in Algorithm 1 is designed to encapsulate arbitrary feature dependencies, unambiguous definitions of local neighborhoods, and clear descriptions of feature influences. In our algorithm 1, the local neighborhood of a sample is controlled by its cluster membership k, while the degree of feature dependency can be easily controlled by the user setting \u03a3 k to desired values. The default value of \u03a3 k = I, where I is the identity matrix, indicating that all features are independent of one another. The elements of the true underlying weight vector w k are sampled from a uniform distribution: w k \u223c U(l, u). We set l = \u22121 and u = 1. The masking vectors m k with elements m k,j are generated by a Bernoulli distribution: m k,j \u223c B(p), where the parameter p controls the ground-truth explanation sparsity. We set p = 0.25. Further, the cluster centers are chosen as follows: when the number of features d is smaller than or equal to the number of clusters K, then we set the first cluster center to [1, 0, . . . , 0], the second one to [0, 1, . . . , 0], etc. We also introduce a multiplier \u03ba to control the distance between the cluster centers so that the cluster centers are located at \u00b5 1 = \u03ba \u00b7 [1, 0, . . . , 0], \u00b5 2 = \u03ba \u00b7 [0, 1, . . . , 0], etc. We set \u03ba = 6. In general, we have observed that lower values of \u03ba result in more complex classification problems. When the number of features d is greater than the number of clusters K, we adjust the above described approach slightly: we first compute = K/d , and then we compute the cluster centers for the first d clusters as described above, for the next d clusters we use \u00b5 d+1 = 2\u03ba \u00b7 [1, 0, . . . , 0], \u00b5 d+2 = 2\u03ba \u00b7 [0, 1, . . . , 0], etc. If = 1, we stop this procedure here, else we continue, and repeat filling up the clusters \u00b5 2d+1 = 3\u03ba \u00b7 [1, 0, . . . , 0], \u00b5 2d+2 = 3\u03ba \u00b7 [0, 1, . . . , 0], etc. Return: X, y ;\n\nTheorem 1 If a given dataset encapsulates the properties of feature independence, unambiguous and well-separated local neighborhoods, and a unique ground truth explanation for each local neighborhood, the most accurate model trained on such a dataset will adhere to the unique ground truth explanation of each local neighborhood.\n\nProof: Before we describe the proof, we begin by first formalizing the properties of feature independence, unambiguous and well-separated local neighborhoods, and a unique ground truth explanation for each local neighborhood using some notation. Let A = [a 1 , a 2 , \u00b7 \u00b7 \u00b7 a d ] denote the vector of input features in a given dataset D, and let the instances in the dataset D be separated into K clusters (local neighborhoods) based on their proximity.\n\nFeature independence: The dataset D satisfies feature independence if P (a i |a j ) = P (a i ) \u2200i, j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 d} and i = j.\n\nUnambiguous and well-separated local neighborhoods: The dataset D is said to constitute unambiguous and well-separated local neighborhoods if dist(x i,j , x p,q ) dist(x i,j , x i,l ) \u2200i, p \u2208 {1, 2 \u00b7 \u00b7 \u00b7 K} where x i,j and x i,l are the j th and l th instances of some cluster i, x p,q is the q th instance of cluster p and p = i. The above implies that the distances between points in different clusters should be significantly higher than the distances between points in the same cluster.\n\nUnique ground truth explanation for each local neighborhood: The dataset D is said to comprise of unique ground truth explanations for each local neighborhood if the ground truth labels of instances in each cluster k \u2208 {1, 2 \u00b7 \u00b7 \u00b7 K} are generated as a function of a subset of features A k \u2286 A where there is a clear relative ordering among features in A k which is captured by the weight vector w k , and features in A are completely independent of each other. [Note that the influential feature set A k corresponding to the cluster k is also captured using the mask vector m k (See Section 2) where With the above information in place, let us now consider a model M which is trained on the dataset D and achieves highest possible accuracy on it. To show that this model indeed adheres to the unique ground truth explanations of each of the local neighborhoods (clusters), we adopt the strategy of proof by contradiction. To this end, we begin with the assumption that the model M does not adhere to the unique ground truth explanation of some local neighborhood k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 K} i.e., the top-T features leveraged by model M for instances in cluster k (denoted by M k T ) do not exactly match the top-T features leveraged by the ground truth labeling process where T \u2264 |A k |. This happens either when there is a mismatch in the relative ordering among the top-T features used by the model M and the ground truth labeling process (or) when there is at least one feature a that appears among the top-T features used by the model but not the ground truth labeling process. In either case, we can construct another model M such that the top-T features used by M match the top-T features of the ground truth labeling process exactly, and the accuracy of M will be higher than that of M. This contradicts the assumption that the model M has the highest possible accuracy, thus demonstrating that the model with highest possible accuracy would adhere to the unique ground truth explanation of each local neighborhood.\n\n\nD Extending OpenXAI\n\nIn this section, we walk through how users can extend and contribute to OpenXAI by adding custom datasets, pre-trained models, explanation methods, or evaluation metrics. In Figure 4, we show a snapshot of our FORM that can be used by users to submit requests for new datasets, pre-trained models, explanation methods, or evaluation metrics.\n\nCustom Datasets. Adding new datasets into OpenXAI 's pipeline is as simple as uploading a .csv file or a .zip folder. Users can submit requests to incorporate new datasets into OpenXAI by providing either the complete dataset (with or without data splits) or individual training and testing files for the respective dataset.\n\nCustom Predictive Models. Users can also submit requests to include new pre-trained models into the OpenXAI framework. Here, users have to provide i) model architecture and implementation details, ii) model hyperparameters and training details, and iii) trained weights for reproducing model performance.\n\nCustom Evaluation Metrics. Users can include new evaluation metrics into OpenXAI by filling out the submit request form and providing the GitHub link to their code and a summary of their metrics. Once approved, the metric should be implemented as a function in the Evaluator class. The input data, ground-truth labels, model predictions, explanations, and the underlying black-box model are all provided in the Evaluator class. The evaluation metric should return a scalar score as a part of their respective implementation.\n\nCustom Explanation Methods. Users can include new explanation methods into OpenXAI 's list of explainers by following the OPENXAI.EXPLAINER template. The abstract method defined ensures that the output of new/existing explanation methods returns explanations as a tensor. Note that all custom explanation methods should extend the Explainer class. Finally, users must provide the GitHub link to their code and a summary of their explanation method.\n\nWe invite submissions to any one or multiple benchmarks in OpenXAI. To be included in the leaderboard, please fill out this FORM, include results of your explanation method and provide the full implementation of your algorithm with proper documentations and GitHub link.\n\n\nE Benchmarking Analysis E.1 Real-World Datasets\n\nIn addition to synthetic dataset, OpenXAI library includes five real-world benchmark datasets from high-stakes domains. Table 1 provides a summary of the real-world datasets currently included in OpenXAI. Our library implements multiple data split strategies and allows users to customize the percentages of train-test splits. To this end, if a given dataset comes with a pre-determined train and test splits, OpenXAI loads the training and testing dataloaders from those pre-determined splits. Otherwise, OpenXAI's dataloader divides the entire dataset randomly into train (70%) and test (30%). Next, we detail the covariates x and labels y for each dataset.\n\nGerman Credit. The dataset comprises of demographic (age, gender), personal (marital status), and financial (income, credit duration) features from 1,000 credit applicants, where they are categorized into good vs. bad customer depending on their credit risk [22].\n\nCOMPAS. The dataset has criminal records and demographics features for 18,876 defendants who got released on bail at the U.S state courts during 1990-2009. The task is to classify defendants into bail (i.e., unlikely to commit a violent crime if released) vs. no bail (i.e., likely to commit a violent crime) [36].\n\nAdult Income. The dataset contains demographic (e.g., age, race, and gender), education (degree), employment (occupation, hours-per week), personal (marital status, relationship), and financial (capital gain/loss) features for 48,842 individuals. The task is to predict whether an individual's income exceeds $50K per year vs. not [79].\n\nGive Me Some Credit. The dataset incorporates demographic (age), personal (number of dependents), and financial (e.g., monthly income, debt ratio, etc.) features for 250,000 individuals. The task is to predict the probability that a customer will experience financial distress vs. not in the next two years. The aim of the dataset is to build models that customers can use to the best financial decisions [27].\n\nHome Equity Line of Credit (HELOC). The dataset comprises of financial (e.g., total number of trades, average credit months in file) attributes from anonymized applications submitted by 9,871 real homeowners. A HELOC is a line of credit typically offered by a bank as a percentage of home equity. The fundamental task is to use the information about the applicant in their credit report to predict whether they will repay their HELOC account within 2 years [25].\n\n\nE.2 Explanation Methods\n\nOpenXAI provides implementations for six explanation methods: Vanilla Gradients [67], Integrated Gradients [72], SmoothGrad [70], Gradient x Input [66], LIME [62], and SHAP [55]. Table 6 summarizes how these methods differ along two axes: whether each method requires \"White-box Access\" to model gradients and whether each method learns a local approximation model to generate explanations. Table 6: Summary of currently available feature attribution methods in OpenXAI. \"Learning?\" denotes whether learning/training procedures are required to generate explanations and \"White-box Access?\" denotes if the access to the internals of the model (e.g., gradients) are needed.\n\n\nMethod\n\nLearning? White-box Access? Random VanillaGrad IntegratedGrad Gradient x Input SmoothGrad SHAP LIME Implementations. We used existing public implementations of all explanation methods in our experiments.\n\nWe used the following captum software package classes: i) captum.attr.Saliency for Vanilla Gradients; ii) captum.attr.IntegratedGradients for Integrated Gradients; iii) captum.attr.NoiseTunnel and captum.attr.Saliency for SmoothGrad; iv) captum.attr.InputXGradient for Gradient x Input; and v) captum.attr.KernelShap for SHAP. Finally, we use the authors' LIME python package for LIME.\n\n\nE.3 Hyperparameter details\n\nOpenXAI uses default hyperparameter settings for all explanation methods following the authors' guidelines. Every explanation method has a corresponding parameter dictionary that stores all the default parameter values. Below, we detail the hyperparameters of individual explanation methods used in our experiments. Description. The option 'tabular' indicates that we wish to compute explanations on a tabular data set. The option 'lime_sample_around_instance' makes sure that the sampling is conducted in a local neighborhood around the point that we wish to explain. The 'lime_discretize_continuous' option ensures that continuous variables are kept continuous, and are not discretized. This parameter sets the 'lime_standard_ deviation' of the Gaussian random variable that is used to sample around the instance that we wish to explain. We set this to a small value, ensuring that we in fact sample from a local neighborhood. Description. The parameter 'shap_subset_size' controls the number of samples of the original model used to train the surrogate SHAP model, 'perturbations_per_eval' allows the processing of multiple samples simultaneously, 'feature_mask' defines a mask on the input instance that group features corresponding to the same interpretable feature, and 'baselines' defines the reference value which replaces each feature when the corresponding interpretable feature is set to 0.\n\n\nc) params_grads = {'grad_absolute_value': False}\n\nDescription. The parameter 'grad_absolute_value' controls whether the absolute value of each element in the explanation vector should be taken or not. d) params_sg = {'sg_n_samples': 500, 'sg_standard_deviation': float(np.sqrt(0.05))} Description. The parameter 'sg_n_samples' sets the number of samples used in the Gaussian random variable to smooth the gradient, while 'sg_standard_deviation' determines the size of the local neighboorhood the gaussian random variables are sampled from. We use a small value, ensuring that we in fact sample from a local neighborhood. e) params_ig = {'ig_method': 'gausslegendre', 'ig_multiply_by_inputs': False, 'ig_baseline': 'mean'} Further, we parameterized our data generating process as follows.\n\nf) params_gauss = {'n_samples': 1000, 'dim':20, 'n_clusters': 10, 'distance_to_center': 6, 'test_size': 0.25, 'upper_weight': 1, 'lower_weight': -1, 'seed': 564, 'sigma': None, 'sparsity': 0.25} Description. The above parameters can be matched with the parameters from the data generating process described in Appendix C: in particular, 'sparsity'= p, 'upper_weight' = u, 'lower_weight' = l, 'dim'= d, 'n_clusters' = K, 'distance_to_center' = \u03ba and 'sigma'=None implies that the identity matrix is used, i.e., \u03a3 k = I.\n\nIn addition, for a given instance x, some evaluation metrics leverage a perturbation class to generate perturbed samples x . We have a parameterized version of this perturbation class in OpenXAI with the following default parameters: \n\n\nE.4 Model details\n\nOur current release of OpenXAI has two pre-trained models: i) a logistic regression model and ii) a deep neural network model for all datasets. To support systematic, reproducible, and efficient benchmarking of post hoc explanation methods, we provide the model weights for both models trained on all six datasets in our pipeline. For neural network models, we use two fully connected hidden layers with 100 nodes in each layer, with ReLU activation functions and an output softmax layer for all datasets. We train both models for 50 epochs using an Adam optimizer with a learning rate of \u03b7=0.001. Next, we show the model performance of both models on all six datasets. \n\n\nE.5 Additional results\n\nThe datasets included in our OpenXAI framework are diverse i.e., these datasets span a wide variety of sizes (ranging from 1000 instances to 102,209 instances in the dataset), feature dimensions (ranging from 7 to 23 dimensions), class imbalance ratios, and feature types (comprising a mix of continuous and discrete features). In addition, the datasets that we chose span two different real-world domains namely criminal justice, and lending. In addition, the datasets included in our OpenXAI framework are very popular and are widely employed in XAI and fairness research. For instance, several recent works in XAI have employed these datasets to both evaluate the efficacy of newly proposed methods as well as to study the behavior of existing methods. Furthermore, several of the datasets that we chose also include sensitive attributes which help us evaluate the (un)fairness (disparities) in explanation quality across majority and minority subgroups.\n\nIn response to the reviewer's comment about the lack of diversity in the domains we consider, we included two new datasets from the healthcare domain and benchmarked state-of-the-art explanation methods on these datasets as well. More specifically, we included the Pima-Indians Diabetes [71] and Framingham heart study [1] datasets both of which have been utilized in recent XAI research. New results with these datasets are included in Section E.5 in the Appendix. We observe similar insights with these new datasets as well. We find that explanations constructed by SmoothGrad are on average more faithful (see Tables 8-11 in Appendix) and outperform other feature-attribution methods on the Prediction Gap on Unimportant feature perturbation (PGU) metric. The stability results of various explanation methods for logistic regression and neural network models are shown in Tables 8-9 and Tables 10,11 respectively in the Appendix. For logistic regression models, we do not find consistent trends across both the new datasets. On the other hand, in the case of neural network models, we observe that SmoothGrad, on average, outperforms other feature attribution methods on relative representation stability (RRS) and output stability (ROS) metrics.                         F Choice of XAI methods, datasets, and models\n\nWhile feature attribution-based explanation methods such as LIME, SHAP, and Gradient-based methods have been proposed a few years back, they continue to be the most popular and widely used post hoc explanation methods both in research [16,43,7,33,31] and in practice [23,78,34,28]. In fact, several recent works published in 2022 have analyzed these methods both theoretically and empirically, and have called for further study of these methods given their widespread adoption [19,16,9,31,43,26]. Furthermore, recent research has also argued that there is little to no understanding of the behavior and effectiveness of even basic post hoc explanation methods such as LIME, SHAP, and gradient-based methods [43,50,64,40,10,3], and developing such an understanding would be a critical first step towards the progress of the XAI field. To this end, we focus on these methods for the first release of our OpenXAI framework. In the next release, we plan to evaluate and benchmark other recently proposed methods (e.g., TCAV and its extensions, influence functions, etc.) as well.\n\nNote that the evaluation metrics and the explanation methods that we include in our framework are generic enough to be applicable to other modalities of data including text and images. The reason why we focused on tabular data for the first release of OpenXAI is two-fold: i) The need for model understanding and explainability is often motivated by high-stakes decision-making settings and applications e.g., loan approvals, disease diagnosis and treatment recommendations, recidivism prediction etc. [14,57,58]. Data encountered in these settings is predominantly tabular. ii) Recent research has argued that there is no clear understanding as to which explanation methods perform well on what kinds of metrics even on simple, low-dimensional tabular datasets [50,19,43,31], and that this is a big open question which has far reaching implications for the progress of the field. To this end, we focused on tabular data for the first release of OpenXAI so that we could find some answers to the aforementioned question in the context of simple, low-dimensional tabular datasets before proceeding to high-dimensional image and text datasets. In the next release of OpenXAI, we plan to include and support image and text datasets, and also add metrics and explanation methods that are specific to these new data modalities.\n\nFigure 1 :\n1A snapshot of the leaderboard page from OpenXAI public website. We also provide interactive ranking functionality (arrow mark in the figure) which allows users to rank explanation methods based on metrics of their choice. Please visit the website to see leaderboards for other datasets.\n\nFigure 2 :\n2Fairness analysis of PGU metric on the German Credit dataset with LR model. Shown are average and standard error values for majority (male) and minority (female) subgroups. Larger gaps between the values of majority and minority subgroups (i.e, red and blue bars respectively) indicate higher disparities which are undesirable.\n\nFigure 3 :\n3Fairness analysis of PGU metric on the Adult Income dataset with LR model. Shown are average and standard error values for majority (male) and minority (female) subgroups. Larger gaps between the values of majority and minority subgroups (i.e, red and blue bars respectively) indicate higher disparities which are undesirable.\n\n\ncompute the aforementioned metrics for each instance in the test data and then average these values. To arrive at the reported values of FA, RA, SA, and SRA metrics, instead of setting a specific value for K, we do the above computation for all possible values of K and then plot the different values of K (on the x-axis) and the corresponding averaged metric values (on the y-axis), and calculate the area under the resulting curve (AUC). On the other hand, in case of RC and PRA metrics, we set K to the total number of features and compute the averaged metric values (across all test instances) as discussed above. b) Predictive Faithfulness. Following Petsiuk et al.[59], OpenXAI includes two complementary predictive faithfulness metrics: i) Prediction Gap on Important feature perturbation (PGI) which measures the difference in prediction probability that results from perturbing the features deemed as influential by a given post hoc explanation, and ii) Prediction Gap on Unimportant feature perturbation (PGU) which measures the difference in prediction probability that results from perturbing the features deemed as unimportant by a given post hoc explanation.For a given instance x, we first obtain the prediction probability\u0177 output by the underlying model f , i.e.,\u0177 = f (x). Let e x be an explanation for the model prediction of x. In case of PGU, we then generate a perturbed instance x in the local neighborhood of x by holding the top-k features constant, and slightly perturbing the values of all the other features by adding a small amount of Gaussian noise. In case of PGI, we generate a perturbed instance x in the local neighborhood of x by slightly perturbing the values of the top-k features by adding a small amount of Gaussian noise, and holding all the other features constant. Finally, we compute the expected value of the prediction difference between the original and perturbed instances as:\n\nAlgorithm 1 :\n1SYNTHGAUSS input : number of clusters: K, cluster centers: [\u00b5 1 , \u00b5 2 , . . . , \u00b5 K ], cluster variances: [\u03a3 1 , . . . , \u03a3 K ], Weight vectors: [w 1 , . . . , w K ], masking vectors: [m 1 , . . . , m K ] output : features: X, labels: y X = 0 n\u00d7d ; \u03a0 = 0 n\u00d71 ; for i = 1:n do k \u2190 Cat(K) # Randomly picks a cluster index ; x i \u223c N (\u00b5 k , \u03a3 k ) # Samples Gaussian instance ; X[i, :] = x i ; \u03c0 1 = P(y i = 1|x i ) = exp (w k m k ) xi 1+exp (w k m k ) xi # Get class probability ; \u03a0[i] = \u03c0 1 ; \u03c0 = get_median(\u03a0) ; for i = 1:n do y i = I(\u03a0[i] >\u03c0) # Make sure classes are balanced ;\n\nFigure 4 :\n4A snapshot of the OpenXAI leaderboard submission form. Users can use this single form to incorporate new datasets, pre-trained models, explanation methods, and evaluation metrics into OpenXAI framework. m k,i = 1 if the i th feature is in set A k ]. This would imply that there is a clear description of the top-T features (and their relative ordering) where T \u2264 |A k | influence the ground truth labeling process.\n\n\na) params_lime = {'lime_mode': 'tabular', 'lime_sample_around_instance': True, 'lime_kernel_ width': 0.75, 'lime_n_samples': 1000, 'lime_discretize_continuous': False, 'lime_standard_ deviation': float(np.sqrt(0.05))}\n\n\nb) params_shap = {'shap_subset_size': 50, 'perturbations_per_eval': 1, 'feature_mask': None, 'baselines': None}\n\n\nwe denote the top-k value using {'percentage_most_important': 0.25}, i.e., we consistently use 25% of the top features for calculating our metric scores.\n\nTable 1 :\n1Summary of currently available datasets in OpenXAI.Here, \"feature types\" denotes whether \n\n\nTable 2 :\n2Ground-truth and predicted faithfulness results on the Heloc dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.\n\nTable 3 :\n3Ground-truth and predicted faithfulness results on the Adult Income dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded. on four ground-truth faithfulness metrics, viz. pairwise rank agreement (PRA), feature agreement (FA), rank agreement (RA), and rank correlation (RC) metrics, for all datasets. However, on average, across all datasets, LIME outperforms other methods on the signed agreement (SA) [+61.6%] and signed-rank agreement (SRA) [+65.3%] metrics, whereas gradient-based explainers achieve relatively lower values. While illustrative in nature, these findings show how OpenXAI can help identify the limitations of existing explanation methods, which in turn can inform the design of new methods. Predictive faithfulness: Tables 2, 3, 16, 17 show results for the PGI and PGU metrics implemented in OpenXAI (see Section 2 and Appendix A). Overall, we find that SmoothGrad explanations are most faithful to the underlying model and, on average, across multiple datasets outperform other feature-attribution methods on PGU metric (+43.03%). However, results from the German credit dataset for the ANN model show that Gradient x Input produces considerably more faithful (+6.74%) explanations than other methods. Finally, this analysis confirms the finding by Krishna et al.Method \nPRA (\u2191) \nRC (\u2191) \nFA (\u2191) \nRA (\u2191) \nSA (\u2191) \nSRA (\u2191) PGU (\u2193) \nPGI (\u2191) \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n0.499\u00b10.00 \n1.\u00b10.00 \n1.\u00b10.00 \n0.580\u00b10.00 \n1.\u00b10.00 \n0.655\u00b10.00 \n0.913\u00b10.00 \n\n0.0\u00b10.00 \n1.\u00b10.00 \n1.\u00b10.00 \n0.281\u00b10.00 \n1.\u00b10.00 \n0.379\u00b10.00 \n0.921\u00b10.00 \n\n0.496\u00b10.00 \n0.923\u00b10.00 \n0.923\u00b10.00 \n0.567\u00b10.00 \n0.923\u00b10.00 \n0.601\u00b10.00 \n0.869\u00b10.00 \n\n0.068\u00b10.00 \n0.921\u00b10.00 \n0.923\u00b10.00 \n0.075\u00b10.00 \n0.923\u00b10.00 \n0.105\u00b10.00 \n0.697\u00b10.00 \n\n0.250\u00b10.00 \n0.138\u00b10.00 \n0.138\u00b10.00 \n0.070\u00b10.00 \n0.741\u00b10.00 \n0.133\u00b10.00 \n0.858\u00b10.00 \n\n0.037\u00b10.00 \n0.136\u00b10.00 \n0.138\u00b10.00 \n0.003\u00b10.00 \n0.741\u00b10.00 \n0.009\u00b10.00 \n0.689\u00b10.00 \n\n0.053\u00b10.00 \n0.07\u00b10.001 \n0.07\u00b10.001 \n0.043\u00b10.00 \n0.008\u00b10.00 \n0.047\u00b10.00 \n0.014\u00b10.00 \n\n0.06\u00b10.00 \n0.039\u00b10.001 \n0.039\u00b10.001 \n0.073\u00b10.00 \n0.099\u00b10.001 \n0.068\u00b10.00 \n0.094\u00b10.001 \n\n\n\nTable 4 :\n4Stability of explanation methods on the Synthetic dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nRIS \nRRS \nRandom \nVanilla Gradients \nIntegrated Gradients \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n6.868\u00b10.013 \n6.133\u00b10.011 \n5.957\u00b10.013 \n0.405\u00b10.015 \n5.249\u00b10.008 \n5.673\u00b10.012 \n9.355\u00b10.008 \n\n6.687\u00b10.015 \n6.144\u00b10.006 \n9.022\u00b10.043 \n3.422\u00b10.037 \n9.419\u00b10.037 \n8.751\u00b10.035 \n13.564\u00b10.036 \n\n\n\nTable 5 :\n5Stability of explanation methods on the German Credit dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nRIS \nRRS \nRandom \nVanilla Gradients \nIntegrated Gradients \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n6.274\u00b10.104 \n-1.384\u00b10.112 \n-2.004\u00b10.119 \n-0.906\u00b10.104 \n-4.780\u00b10.117 \n-0.230\u00b10.109 \n-0.698\u00b10.109 \n\n16.448\u00b10.124 \n5.241\u00b10.024 \n4.560\u00b10.029 \n9.437\u00b10.124 \n4.931\u00b10.122 \n10.056\u00b10.115 \n9.397\u00b10.119 \n\n\n\nTable 7 :\n7Results of the machine learning models trained on six datasets. Shown are the accuracy of LR and ANN models trained the datasets. The best performance is bolded.Dataset \nLR ANN \nSynthetic Data \nGerman Credit \nHELOC \nCOMPAS \nAdult Income \nGive Me Some Credit \nFramingham Heart Study \nPima-Indians Diabetes \n\n83.0% \n72.0% \n72.0% \n85.4% \n84.0% \n94.1% \n85.0% \n66.0% \n\n92.0% \n75.0% \n74.0% \n85.0% \n85.0% \n95.0% \n85.0% \n77.0% \n\n\n\nTable 8 :\n8Ground-truth and predicted faithfulness results on the Pima Indians Diabetes dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPRA (\u2191) \nRC (\u2191) \nFA (\u2191) \nRA (\u2191) \nSA (\u2191) \nSRA (\u2191) PGU (\u2193) \nPGI (\u2191) \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n0.497\u00b10.01 \n1.0\u00b10.00 \n1.0\u00b10.00 \n0.527\u00b10.01 \n1.0\u00b10.00 \n0.518\u00b10.01 \n1.0\u00b10.00 \n\n-0.047\u00b10.03 \n1.0\u00b10.00 \n1.0\u00b10.00 \n0.043\u00b10.02 \n1.0\u00b10.00 \n0.048\u00b10.02 \n1.0\u00b10.00 \n\n0.486\u00b10.00 \n0.875\u00b10.00 \n0.875\u00b10.00 \n0.477\u00b10.01 \n0.875\u00b10.00 \n0.479\u00b10.01 \n0.874\u00b10.01 \n\n0.111\u00b10.00 \n0.875\u00b10.00 \n0.875\u00b10.00 \n0.066\u00b10.01 \n0.875\u00b10.00 \n0.064\u00b10.01 \n0.874\u00b10.00 \n\n0.239\u00b10.00 \n0.131\u00b10.02 \n0.131\u00b10.02 \n0.082\u00b10.02 \n0.731\u00b10.00 \n0.090\u00b10.02 \n0.875\u00b10.00 \n\n0.054\u00b10.00 \n0.131\u00b10.03 \n0.131\u00b10.03 \n0.017\u00b10.00 \n0.731\u00b10.00 \n0.016\u00b10.00 \n0.873\u00b10.00 \n\n0.011\u00b10.00 \n0.013\u00b10.00 \n0.013\u00b10.00 \n0.011\u00b10.00 \n0.008\u00b10.00 \n0.011\u00b10.00 \n0.01\u00b10.00 \n\n0.013\u00b10.00 \n0.012\u00b10.00 \n0.012\u00b10.00 \n0.014\u00b10.00 \n0.016\u00b10.00 \n0.014\u00b10.00 \n0.015\u00b10.001 \n\n\n\nTable 9 :\n9Ground-truth and predicted faithfulness results on the Framingham heart study dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.\n\nTable 10 :\n10Predicted faithfulness results on the Framingham heart study dataset for all explanation methods with ANN model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPGU (\u2193) \nPGI (\u2191) \n\nRandom \n0.044\u00b10.001 0.049\u00b10.001 \nVanillaGrad \n0.050\u00b10.001 0.045\u00b10.001 \nIntegratedGrad 0.050\u00b10.001 0.046\u00b10.001 \nGradient x Input 0.047\u00b10.001 0.049\u00b10.001 \nSmoothGrad \n0.032\u00b10.001 0.059\u00b10.002 \nSHAP \n0.046\u00b10.001 0.049\u00b10.001 \nLIME \n0.036\u00b10.001 0.059\u00b10.002 \n\n\n\nTable 11 :\n11Predicted faithfulness results on the \nPima Indians Diabetes dataset for all explanation \nmethods with ANN model. Shown are average \nand standard error metric values computed across all \ninstances in the test set. \u2191 indicates that higher values \nare better, and \u2193 indicates that lower values are better. \nValues corresponding to best performance are bolded. \n\nMethod \nPGU (\u2193) \nPGI (\u2191) \n\nRandom \n0.027\u00b10.002 0.032\u00b10.002 \nVanillaGrad \n0.029\u00b10.002 0.030\u00b10.002 \nIntegratedGrad 0.028\u00b10.002 0.031\u00b10.002 \nGradient x Input 0.027\u00b10.002 0.032\u00b10.002 \nSmoothGrad \n0.015\u00b10.001 0.041\u00b10.002 \nSHAP \n0.027\u00b10.002 0.032\u00b10.002 \nLIME \n0.019\u00b10.001 0.040\u00b10.002 \n\n\nTable 12 :\n12Stability of explanation methods on the Framingham heart study dataset with LR model.Shown are average and standard error values across all \ntest set instances. Values closer to zero are desirable, \nand the best performance is bolded. \n\nMethod \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n4.56\u00b10.01 \n0.03\u00b10.02 \n-0.56\u00b10.01 \n-0.37\u00b10.02 \n-4.52\u00b10.02 \n-0.76\u00b10.02 \n-0.84\u00b10.01 \n\n8.08\u00b10.04 \n1.72\u00b10.04 \n1.14\u00b10.04 \n2.53\u00b10.05 \n-2.35\u00b10.04 \n2.42\u00b10.05 \n1.69\u00b10.04 \n\n8.08\u00b10.04 \n1.72\u00b10.04 \n1.14\u00b10.04 \n2.53\u00b10.05 \n-2.35\u00b10.04 \n2.42\u00b10.05 \n1.69\u00b10.04 \n\n\n\nTable 13 :\n13Stability of explanation methods on the Pima Indians Diabetes dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n7.68\u00b10.14 \n2.30\u00b10.13 \n1.52\u00b10.12 \n2.61\u00b10.13 \n-2.78\u00b10.14 \n2.45\u00b10.13 \n1.46\u00b10.14 \n\n7.32\u00b10.10 \n0.35\u00b10.15 \n-0.43\u00b10.15 \n2.43\u00b10.10 \n-3.98\u00b10.12 \n2.31\u00b10.10 \n-0.04\u00b10.12 \n\n7.32\u00b10.10 \n0.35\u00b10.15 \n-0.43\u00b10.15 \n2.43\u00b10.10 \n-3.98\u00b10.12 \n2.31\u00b10.10 \n-0.04\u00b10.12 \n\n\n\nTable 14 :\n14Stability of explanation methods on the Framingham heart study dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n4.58\u00b10.01 \n2.95\u00b10.03 \n1.94\u00b10.03 \n1.89\u00b10.04 \n-4.15\u00b10.02 \n0.74\u00b10.02 \n1.03\u00b10.01 \n\n7.35\u00b10.02 \n5.18\u00b10.04 \n4.32\u00b10.03 \n4.09\u00b10.04 \n-1.49\u00b10.02 \n3.38\u00b10.03 \n3.64\u00b10.02 \n\n13.34\u00b10.05 \n10.80\u00b10.07 \n10.12\u00b10.05 \n9.77\u00b10.07 \n4.43\u00b10.05 \n9.31\u00b10.05 \n9.63\u00b10.05 \n\n\n\nTable 15 :\n15Stability of explanation methods on the Pima Indians Diabetes dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. 88\u00b10.17 3.85\u00b10.11 8.17\u00b10.15 8.90\u00b10.12 E.6 Additional Results on LR modelsMethod \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n7.60\u00b10.14 \n5.24\u00b10.18 \n5.19\u00b10.15 \n4.33\u00b10.19 \n-1.18\u00b10.14 \n3.32\u00b10.18 \n3.82\u00b10.14 \n\n7.00\u00b10.05 \n4.24\u00b10.12 \n4.25\u00b10.06 \n3.31\u00b10.12 \n-1.90\u00b10.05 \n2.58\u00b10.10 \n3.09\u00b10.06 \n\n12.75\u00b10.12 \n9.76\u00b10.17 \n9.92\u00b10.12 \n8.\n\nTable 16 :\n16Ground-truth and predicted faithfulness results on the SynthGauss dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPRA (\u2191) \nRC (\u2191) \nFA (\u2191) \nRA (\u2191) \nSA (\u2191) \nSRA (\u2191) PGU (\u2193) \nPGI (\u2191) \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n0.498\u00b10.00 \n1.\u00b10.00 \n1.\u00b10.00 \n0.875\u00b10.00 \n1.\u00b10.00 \n0.860\u00b10.00 \n0.986\u00b10.00 \n\n-0.005\u00b10.01 \n1.\u00b10.00 \n1.\u00b10.00 \n0.890\u00b10.00 \n1.\u00b10.00 \n0.873\u00b10.00 \n0.995\u00b10.00 \n\n0.500\u00b10.00 \n0.950\u00b10.00 \n0.950\u00b10.00 \n0.785\u00b10.00 \n0.950\u00b10.00 \n0.775\u00b10.00 \n0.930\u00b10.00 \n\n0.047\u00b10.00 \n0.950\u00b10.00 \n0.950\u00b10.00 \n0.161\u00b10.00 \n0.950\u00b10.00 \n0.145\u00b10.00 \n0.763\u00b10.00 \n\n0.249\u00b10.00 \n0.464\u00b10.01 \n0.464\u00b10.01 \n0.382\u00b10.01 \n0.456\u00b10.0 \n0.377\u00b10.01 \n0.928\u00b10.00 \n\n0.024\u00b10.00 \n0.464\u00b10.01 \n0.464\u00b10.01 \n0.071\u00b10.00 \n0.456\u00b10.00 \n0.068\u00b10.00 \n0.762\u00b10.00 \n\n0.041\u00b10.001 \n0.044\u00b10.001 \n0.044\u00b10.001 \n0.043\u00b10.001 \n0.018\u00b10.00 \n0.043\u00b10.001 \n0.048\u00b10.001 \n\n0.044\u00b10.001 \n0.046\u00b10.001 \n0.046\u00b10.001 \n0.046\u00b10.001 \n0.059\u00b10.001 \n0.046\u00b10.001 \n0.042\u00b10.001 \n\n\n\nTable 17 :\n17Ground-truth and predicted faithfulness results on the German credit dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPRA (\u2191) \nRC (\u2191) \nFA (\u2191) \nRA (\u2191) \nSA (\u2191) \nSRA (\u2191) PGU (\u2193) PGI (\u2191) \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n0.500\u00b10.00 \n1.\u00b10.00 \n1.\u00b10.00 \n0.462\u00b10.00 \n1.\u00b10.00 \n0.488\u00b10.00 \n0.989\u00b10.00 \n\n-0.01\u00b10.01 \n1.\u00b10.00 \n1.\u00b10.00 \n0.063\u00b10.01 \n1.\u00b10.00 \n-0.053\u00b10.01 \n0.998\u00b10.00 \n\n0.167\u00b10.00 \n0.950\u00b10.00 \n0.950\u00b10.00 \n0.126\u00b10.00 \n0.950\u00b10.00 \n0.130\u00b10.00 \n0.938\u00b10.00 \n\n0.016\u00b10.00 \n0.950\u00b10.00 \n0.950\u00b10.00 \n0.006\u00b10.00 \n0.950\u00b10.00 \n0.007\u00b10.00 \n0.810\u00b10.00 \n\n0.082\u00b10.00 \n0.846\u00b10.02 \n0.846\u00b10.02 \n0.108\u00b10.01 \n0.606\u00b10.00 \n0.112\u00b10.01 \n0.938\u00b10.00 \n\n0.007\u00b10.00 \n0.846\u00b10.02 \n0.846\u00b10.02 \n0.006\u00b10.00 \n0.606\u00b10.00 \n0.006\u00b10.00 \n0.814\u00b10.00 \n\n0.022\u00b10.00 \n0.018\u00b10.00 \n0.018\u00b10.00 \n0.021\u00b10.00 \n0.016\u00b10.00 \n0.021\u00b10.00 \n0.018\u00b10.00 \n\n0.007\u00b10.00 \n0.01\u00b10.00 \n0.01\u00b10.00 \n0.01\u00b10.00 \n0.013\u00b10.00 \n0.011\u00b10.00 \n0.01\u00b10.00 \n\n\nTable 18 :\n18Ground-truth and predicted faithfulness results on the COMPAS dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Table 19: Stability of explanation methods on the Heloc dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nPRA (\u2191) \nRC (\u2191) \nFA (\u2191) \nRA (\u2191) \nSA (\u2191) \nSRA (\u2191) PGU (\u2193) PGI (\u2191) \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n0.500\u00b10.00 \n1.\u00b10.00 \n1.\u00b10.00 \n0.690\u00b10.00 \n1.\u00b10.00 \n0.692\u00b10.00 \n0.983\u00b10.00 \n\n0.002\u00b10.01 \n1.\u00b10.00 \n1.\u00b10.00 \n0.535\u00b10.01 \n1.\u00b10.00 \n0.469\u00b10.01 \n0.986\u00b10.00 \n\n0.485\u00b10.00 \n0.857\u00b10.00 \n0.857\u00b10.00 \n0.639\u00b10.00 \n0.857\u00b10.00 \n0.619\u00b10.00 \n0.838\u00b10.00 \n\n0.120\u00b10. \n0.857\u00b10.00 \n0.857\u00b10.00 \n0.286\u00b10.00 \n0.857\u00b10.00 \n0.281\u00b10.00 \n0.738\u00b10.00 \n\n0.245\u00b10.00 \n0.790\u00b10.01 \n0.790\u00b10.01 \n0.507\u00b10.01 \n0.299\u00b10.00 \n0.527\u00b10.01 \n0.836\u00b10.00 \n\n0.059\u00b10.001 \n0.791\u00b10.01 \n0.791\u00b10.01 \n0.264\u00b10.01 \n0.299\u00b10.00 \n0.267\u00b10.01 \n0.735\u00b10.00 \n\n0.021\u00b10.00 \n0.025\u00b10.00 \n0.025\u00b10.00 \n0.020\u00b10.00 \n0.007\u00b10.00 \n0.020\u00b10.00 \n0.026\u00b10.00 \n\n0.026\u00b10.00 \n0.027\u00b10.00 \n0.027\u00b10.00 \n0.030\u00b10.00 \n0.035\u00b10.00 \n0.03\u00b10.00 \n0.026\u00b10.00 \n\nMethod \nRIS \nRRS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n5.387\u00b10.020 \n2.124\u00b10.019 \n1.403\u00b10.019 \n1.036\u00b10.022 \n-3.724\u00b10.023 \n1.458\u00b10.021 \n4.534\u00b10.019 \n\n13.794\u00b10.029 \n6.336\u00b10.015 \n5.776\u00b10.008 \n8.964\u00b10.036 \n3.507\u00b10.029 \n9.331\u00b10.030 \n8.833\u00b10.029 \n\n\n\nTable 20 :\n20Stability of explanation methods on the Adult Income dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nRIS \nRRS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n6.345\u00b10.009 \n4.092\u00b10.018 \n3.492\u00b10.013 \n2.307\u00b10.016 \n-3.007\u00b10.009 \n1.857\u00b10.010 \n1.715\u00b10.009 \n\n12.577\u00b10.018 \n6.928\u00b10.012 \n6.843\u00b10.014 \n7.543\u00b10.019 \n2.941\u00b10.019 \n7.883\u00b10.019 \n7.899\u00b10.016 \n\n\n\nTable 21 :\n21Stability of explanation methods on the COMPAS dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n7.88\u00b10.04 \n4.77\u00b10.05 \n4.33\u00b10.04 \n2.98\u00b10.06 \n-1.10\u00b10.03 \n3.12\u00b10.05 \n4.24\u00b10.04 \n\n6.74\u00b10.05 \n1.82\u00b10.05 \n1.41\u00b10.04 \n1.78\u00b10.06 \n-2.93\u00b10.04 \n2.35\u00b10.05 \n2.28\u00b10.04 \n\n6.74\u00b10.05 \n1.82\u00b10.05 \n1.41\u00b10.04 \n1.78\u00b10.06 \n-2.93\u00b10.04 \n2.35\u00b10.05 \n2.28\u00b10.04 \n\n\n\nTable 22 :\n22Stability of explanation methods on the GMSC Credit dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. E.7 Results on ANN modelsMethod \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n5.52\u00b10.007 \n3.96\u00b10.008 \n3.12\u00b10.008 \n2.06\u00b10.009 \n-3.20\u00b10.005 \n1.44\u00b10.008 \n2.56\u00b10.006 \n\n5.97\u00b10.009 \n1.38\u00b10.007 \n0.59\u00b10.007 \n0.15\u00b10.009 \n-3.56\u00b10.008 \n0.09\u00b10.009 \n1.81\u00b10.008 \n\n5.97\u00b10.009 \n1.38\u00b10.007 \n0.59\u00b10.007 \n0.15\u00b10.009 \n-3.56\u00b10.008 \n0.09\u00b10.009 \n1.81\u00b10.008 \n\n\n\nTable 23 :\n23Predicted faithfulness results on the SynthGauss dataset for all explanation methods with ANN model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPGU (\u2193) \nPGI (\u2191) \n\nRandom \n0.054\u00b10.002 0.06\u00b10.002 \nVanillaGrad \n0.059\u00b10.002 0.061\u00b10.002 \nIntegratedGrad 0.059\u00b10.002 0.061\u00b10.002 \nGradient x Input 0.059\u00b10.002 0.061\u00b10.002 \nSmoothGrad \n0.028\u00b10.001 0.08\u00b10.003 \nSHAP \n0.057\u00b10.002 0.062\u00b10.002 \nLIME \n0.069\u00b10.003 0.051\u00b10.002 \n\n\n\nTable 24 :\n24Predicted faithfulness results on the German dataset for all explanation methods with ANN model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPGU (\u2193) \nPGI (\u2191) \n\nRandom \n0.067\u00b10.004 0.02\u00b10.002 \nVanillaGrad \n0.064\u00b10.004 0.022\u00b10.002 \nIntegratedGrad 0.064\u00b10.004 0.022\u00b10.002 \nGradient x Input 0.059\u00b10.005 0.03\u00b10.002 \nSmoothGrad \n0.062\u00b10.004 0.023\u00b10.002 \nSHAP \n0.06\u00b10.004 0.027\u00b10.002 \nLIME \n0.063\u00b10.004 0.023\u00b10.002 \n\n\nTable 25 :\n25Predicted faithfulness results on the Heloc dataset for all explanation methods with ANN model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPGU (\u2193) \nPGI (\u2191) \n\nRandom \n0.056\u00b10.001 0.059\u00b10.001 \nVanillaGrad \n0.061\u00b10.001 0.059\u00b10.001 \nIntegratedGrad 0.061\u00b10.001 0.059\u00b10.001 \nGradient x Input 0.056\u00b10.001 0.06\u00b10.001 \nSmoothGrad \n0.032\u00b10.001 0.078\u00b10.001 \nSHAP \n0.055\u00b10.001 0.061\u00b10.001 \nLIME \n0.057\u00b10.001 0.062\u00b10.001 \n\n\n\nTable 26 :\n26Predicted faithfulness results on the Adult Income dataset for all explanation methods with ANN model. Shown are average and standard error metric values computed across all instances in the test set. \u2191 indicates that higher values are better, and \u2193 indicates that lower values are better. Values corresponding to best performance are bolded.Method \nPGU (\u2193) \nPGI (\u2191) \n\nRandom \n0.055\u00b10.001 0.063\u00b10.001 \nVanillaGrad \n0.064\u00b10.001 0.053\u00b10.001 \nIntegratedGrad 0.066\u00b10.001 0.051\u00b10.001 \nGradient x Input 0.055\u00b10.001 0.062\u00b10.001 \nSmoothGrad \n0.014\u00b10.001 0.100\u00b10.001 \nSHAP \n0.049\u00b10.001 0.070\u00b10.001 \nLIME \n0.019\u00b10.001 0.098\u00b10.001 \n\n\n\nTable 27 :\n27Stability of explanation methods on the SynthGauss dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. 14\u00b10.03 6.80\u00b10.09 1.11\u00b10.01 5.96\u00b10.01 5.53\u00b10.01 7.37\u00b10.02 4.24\u00b10.10 5.53\u00b10.03 3.46\u00b10.10 -1.07\u00b10.02 3.95\u00b10.02 3.40\u00b10.02 13.81\u00b10.10 8.02\u00b10.07 11.60\u00b10.08 7.22\u00b10.07 5.31\u00b10.10 10.39\u00b10.09 9.86\u00b10.09Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n9.35\u00b10.01 \n7.58\u00b10.09 \n8.\n\nTable 28 :\n28Stability of explanation methods on the German Credit dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. 62\u00b10.23 -1.90\u00b10.08 2.97\u00b10.07 1.28\u00b10.08 6.35\u00b10.07 3.89\u00b10.28 4.44\u00b10.10 2.88\u00b10.28 1.32\u00b10.06 6.17\u00b10.05 4.46\u00b10.07 9.57\u00b10.04 7.65\u00b10.17 11.35\u00b10.36 6.83\u00b10.17 8.10\u00b10.36 13.10\u00b10.33 11.46\u00b10.30 16.50\u00b10.34Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n1.61\u00b10.24 \n1.35\u00b10.10 \n0.\n\nTable 29 :\n29Stability of explanation methods on the Heloc dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. 06\u00b10.04 3.16\u00b10.02 2.46\u00b10.04 -3.11\u00b10.02 1.57\u00b10.02 1.19\u00b10.02 7.63\u00b10.03 5.86\u00b10.03 5.17\u00b10.01 4.28\u00b10.03 -0.95\u00b10.01 3.80\u00b10.01 3.33\u00b10.01 10.99\u00b10.04 10.80\u00b10.04 9.53\u00b10.04 9.56\u00b10.04 4.60\u00b10.04 9.40\u00b10.04 13.25\u00b10.04Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n5.39\u00b10.02 \n4.\n\nTable 30 :\n30Stability of explanation methods on the Adult Income dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. 25\u00b10.04 -1.70\u00b10.01 1.98\u00b10.01 1.79\u00b10.01 7.27\u00b10.009 4.58\u00b10.042 4.33\u00b10.011 3.08\u00b10.041 -0.95\u00b10.010 2.68\u00b10.013 2.45\u00b10.008 14.38\u00b10.05 9.857\u00b10.02 11.74\u00b10.05 8.248\u00b10.02 6.222\u00b10.06 9.780\u00b10.05 9.773\u00b10.05Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n6.34\u00b10.01 \n5.01\u00b10.04 \n4.32\u00b10.01 \n3.\n\nTable 31 :\n31Stability of explanation methods on the COMPAS dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded. 29\u00b10.10 3.19\u00b10.16 -1.39\u00b10.13 3.72\u00b10.10 3.54\u00b10.09 11.51\u00b10.18 9.02\u00b10.31 9.27\u00b10.19 7.16\u00b10.32 3.15\u00b10.14 8.27\u00b10.17 8.36\u00b10.17Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n7.78\u00b10.20 \n6.29\u00b10.24 \n5.91\u00b10.17 \n3.81\u00b10.29 \n-0.59\u00b10.17 \n4.24\u00b10.18 \n4.49\u00b10.19 \n\n7.21\u00b10.17 \n5.54\u00b10.14 \n5.\n\nTable 32 :\n32Stability of explanation methods on the GMSC Credit dataset with ANN model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.Figure 5: Fairness analysis of PGU metric on the German Credit dataset with ANN model. Shown are average and standard error values for majority (male) and minority (female) subgroups. Larger gaps between the values of majority and minority subgroups (i.e, red and blue bars respectively) indicate higher disparities which are undesirable.Figure 6: Fairness analysis of PGU metric on the Adult Income dataset with ANN model. Shown are average and standard error values for majority (male) and minority (female) subgroups. Larger gaps between the values of majority and minority subgroups (i.e, red and blue bars respectively) indicate higher disparities which are undesirable.Method \nRIS \nRRS \nROS \nRandom \nVanillaGrad \nIntegratedGrad \nGradient x Input \nSmoothGrad \nSHAP \nLIME \n\n5.52\u00b10.006 \n3.91\u00b10.008 \n3.07\u00b10.007 \n1.89\u00b10.008 \n-3.03\u00b10.006 \n1.27\u00b10.007 \n1.25\u00b10.006 \n\n7.78\u00b10.004 \n5.63\u00b10.006 \n4.76\u00b10.004 \n3.54\u00b10.007 \n-1.02\u00b10.004 \n3.06\u00b10.004 \n3.31\u00b10.004 \n\n12.07\u00b10.009 \n8.46\u00b10.014 \n7.82\u00b10.0115 \n6.67\u00b10.0139 \n3.20\u00b10.009 \n6.93\u00b10.010 \n7.82\u00b10.009 \n\nAcknowledgments and Disclosure of FundingThe authors would like to thank the anonymous reviewers for their helpful feedback and all the funding agencies listed below for supporting this work. This work is supported in part by the NSF awards #IIS-2008461 and #IIS-2040989, and research awards from Google, JP Morgan, Amazon, Harvard Data Science Initiative, and D 3 Institute at Harvard. HL would like to thank Sujatha and Mohan Lakkaraju for their continued support and encouragement. The views expressed here are those of the authors and do not reflect the official policy or position of the funding agencies.The datasets that we utilize in this work are very popular and are widely employed in XAI and fairness research till date. For instance, several recent works in XAI published at ICML, NeurIPS, and FAccT conferences in 2021-22 have employed these datasets both to evaluate the efficacy of newly proposed methods, as well as to study the behavior of existing methods [9, 18-20, 38, 69, 73, 5]. Given this, we follow suit and employ these datasets in our benchmarking efforts. Similarly, the aforementioned works also employ logistic regression models and deep neural network architectures similar to the ones considered in our research.\nFramingham heart study dataset | kaggle. Accessed on 08/15/2022)Framingham heart study dataset | kaggle. https://www.kaggle.com/datasets/ aasheesh200/framingham-heart-study-dataset. (Accessed on 08/15/2022).\n\nShap. Shap benchmark. URL https://shap.readthedocs.io/en/latest/index.html.\n\nExplaining image classifiers by removing input features using generative models. Chirag Agarwal, Anh Nguyen, ACCV. Chirag Agarwal and Anh Nguyen. Explaining image classifiers by removing input features using generative models. In ACCV, 2020.\n\nRethinking stability for attribution-based explanations. Chirag Agarwal, Nari Johnson, Martin Pawelczyk, Satyapriya Krishna, Eshika Saxena, Marinka Zitnik, Himabindu Lakkaraju, ICLR 2022 Workshop on PAIR 2 Struct. Chirag Agarwal, Nari Johnson, Martin Pawelczyk, Satyapriya Krishna, Eshika Saxena, Marinka Zitnik, and Himabindu Lakkaraju. Rethinking stability for attribution-based explanations. In ICLR 2022 Workshop on PAIR 2 Struct, 2022.\n\nTowards the unification and robustness of perturbation and gradient based explanations. Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Steven Wu, Himabindu Lakkaraju, ICML. 2021Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Steven Wu, and Himabindu Lakkaraju. Towards the unification and robustness of perturbation and gradient based explanations. In ICML, 2021.\n\nFairwashing: the risk of rationalization. Ulrich Aivodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, Alain Tapp, ICML. Ulrich Aivodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, and Alain Tapp. Fairwashing: the risk of rationalization. In ICML, 2019.\n\nDavid Alvarez-Melis, Tommi S Jaakkola, On the robustness of interpretability methods. arXiv. David Alvarez-Melis and Tommi S Jaakkola. On the robustness of interpretability methods. arXiv, 2018.\n\nExplainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion. Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion, 2020.\n\nThe road to explainability is paved with bias: Measuring the fairness of explanations. Aparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, Frank Rudzicz, Marzyeh Ghassemi, arXivAparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, Frank Rudzicz, and Marzyeh Ghassemi. The road to explainability is paved with bias: Measuring the fairness of explanations. arXiv, 2022.\n\nThe sensitivity of attribution methods to hyperparameters. Naman Bansal, Chirag Agarwal, Anh Nguyen, Sam, CVPR. Naman Bansal, Chirag Agarwal, and Anh Nguyen. Sam: The sensitivity of attribution methods to hyperparameters. In CVPR, 2020.\n\nThe hidden assumptions behind counterfactual explanations and principal reasons. Solon Barocas, Andrew Selbst, Manish Raghavan, FAccTSolon Barocas, Andrew Selbst, and Manish Raghavan. The hidden assumptions behind counterfactual explanations and principal reasons. In FAccT, 2020.\n\nInterpretability via model extraction. Osbert Bastani, Carolyn Kim, Hamsa Bastani, Osbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpretability via model extraction. arXiv, 2017.\n\nClassification by set cover: The prototype vector machine. Jacob Bien, Robert Tibshirani, arXivJacob Bien and Robert Tibshirani. Classification by set cover: The prototype vector machine. arXiv, 2009.\n\nVadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, Gjergji Kasneci, Deep neural networks and tabular data: A survey. arXiv. Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. arXiv, 2021.\n\nIntelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, Noemie Elhadad, KDD. Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.\n\nUse-casegrounded simulations for explanation evaluation. Valerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, Ameet Talwalkar, arXivValerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, and Ameet Talwalkar. Use-case- grounded simulations for explanation evaluation. arXiv, 2022.\n\nExplaining by removing: A unified framework for model explanation. Ian Covert, Scott Lundberg, Su-In Lee, JMLRIan Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. JMLR, 2021.\n\nFairness via explanation quality: Evaluating disparities in the quality of post hoc explanations. Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, H Stephen, Himabindu Bach, Lakkaraju, AAAI Conference on AI. AIES2022Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H Bach, and Himabindu Lakkaraju. Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations. In AAAI Conference on AI, Ethics, and Society (AIES), 2022.\n\nFramework for evaluating faithfulness of local explanations. Sanjoy Dasgupta, Nave Frost, Michal Moshkovitz, arXivSanjoy Dasgupta, Nave Frost, and Michal Moshkovitz. Framework for evaluating faithfulness of local explanations. arXiv, 2022.\n\nOn the adversarial robustness of causal algorithmic recourse. Ricardo Dominguez-Olmedo, H Amir, Bernhard Karimi, Sch\u00f6lkopf, ICML. PMLR. Ricardo Dominguez-Olmedo, Amir H Karimi, and Bernhard Sch\u00f6lkopf. On the adversarial robustness of causal algorithmic recourse. In ICML. PMLR, 2022.\n\nTowards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv, 2017.\n\nUCI machine learning repository. Dheeru Dua, Casey Graff, Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive. ics.uci.edu/ml.\n\nOn the interpretability of machine learning-based model for predicting hypertension. BMC medical informatics and decision making. Radwa Elshawi, H Mouaz, Sherif Al-Mallah, Sakr, Radwa Elshawi, Mouaz H Al-Mallah, and Sherif Sakr. On the interpretability of machine learning-based model for predicting hypertension. BMC medical informatics and decision making, 2019.\n\nWhen comparing to ground truth is wrong: On evaluating gnn explanation methods. Lukas Faber, K Amin, Roger Moghaddam, Wattenhofer, KDD. 2021Lukas Faber, Amin K. Moghaddam, and Roger Wattenhofer. When comparing to ground truth is wrong: On evaluating gnn explanation methods. In KDD, 2021.\n\nExplainable machine learning challenge. Fico, FICO. Explainable machine learning challenge. https://community.fico.com/s/ explainable-machine-learning-challenge?tabset-158d9=3, 2022. (Accessed on 05/23/2022).\n\nAttribution-based explanations that provide recourse cannot be robust. Hidde Fokkema, Tim Rianne De Heide, Van Erven, arXivHidde Fokkema, Rianne de Heide, and Tim van Erven. Attribution-based explanations that provide recourse cannot be robust. arXiv, 2022.\n\nGive me some credit :: 2011 competition data | kaggle. Bryce Freshcorn, Accessed on 05/23/2022Bryce Freshcorn. Give me some credit :: 2011 competition data | kaggle. https://www. kaggle.com/datasets/brycecf/give-me-some-credit-dataset, 2022. (Accessed on 05/23/2022).\n\nThe false hope of current approaches to explainable artificial intelligence in health care. The Lancet Digital Health. Marzyeh Ghassemi, Luke Oakden-Rayner, Andrew L Beam, Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew L Beam. The false hope of current approaches to explainable artificial intelligence in health care. The Lancet Digital Health, 2021.\n\nInterpretation of neural networks is fragile. Amirata Ghorbani, Abubakar Abid, James Zou, AAAI Conference on Artificial Intelligence. Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In AAAI Conference on Artificial Intelligence, 2019.\n\nA survey of methods for explaining black box models. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, Dino Pedreschi, ACM computing surveys (CSUR). Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A survey of methods for explaining black box models. ACM computing surveys (CSUR), 2018.\n\nWhich explanation should i choose? a function approximation perspective to characterizing post hoc explanations. Tessa Han, Suraj Srinivas, Himabindu Lakkaraju, arXivTessa Han, Suraj Srinivas, and Himabindu Lakkaraju. Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations. arXiv, 2022.\n\nAnna Hedstr\u00f6m, Leander Weber, Dilyara Bareeva, Franz Motzkus, Wojciech Samek, Sebastian Lapuschkin, Marina M-C H\u00f6hne, Quantus: an explainable ai toolkit for responsible evaluation of neural network explanations. arXiv. Anna Hedstr\u00f6m, Leander Weber, Dilyara Bareeva, Franz Motzkus, Wojciech Samek, Sebastian Lapuschkin, and Marina M-C H\u00f6hne. Quantus: an explainable ai toolkit for responsible evaluation of neural network explanations. arXiv, 2022.\n\nSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating feature importance estimates. arXiv. Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating feature importance estimates. arXiv, 2018.\n\nGlobal explanations of neural networks: Mapping the landscape of predictions. Mark Ibrahim, Melissa Louie, Ceena Modarres, John Paisley, abs/1902.02384CoRRMark Ibrahim, Melissa Louie, Ceena Modarres, and John Paisley. Global explanations of neural networks: Mapping the landscape of predictions. CoRR, abs/1902.02384, 2019.\n\nHow can i choose an explainer? an application-grounded evaluation of post-hoc explanations. S\u00e9rgio Jesus, Catarina Bel\u00e9m, Vladimir Balayan, Jo\u00e3o Bento, Pedro Saleiro, Pedro Bizarro, Jo\u00e3o Gama, FAccT2021S\u00e9rgio Jesus, Catarina Bel\u00e9m, Vladimir Balayan, Jo\u00e3o Bento, Pedro Saleiro, Pedro Bizarro, and Jo\u00e3o Gama. How can i choose an explainer? an application-grounded evaluation of post-hoc explanations. In FAccT, 2021.\n\nThe effect of race/ethnicity on sentencing: Examining sentence type, jail length, and prison length. L Kareem, Tina L Jordan, Freiburger, In Journal of Ethnicity in Criminal Justice. Taylor & FrancisKareem L Jordan and Tina L Freiburger. The effect of race/ethnicity on sentencing: Examining sentence type, jail length, and prison length. In Journal of Ethnicity in Criminal Justice. Taylor & Francis, 2015.\n\nModel-agnostic counterfactual explanations for consequential decisions. Amir-Hossein, Gilles Karimi, Borja Barthe, Isabel Balle, Valera, arXivAmir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-agnostic counterfactual explanations for consequential decisions. arXiv, 2019.\n\nAlgorithmic recourse: from counterfactual explanations to interventions. CoRR, abs. Bernhard Amir-Hossein Karimi, Isabel Sch\u00f6lkopf, Valera, Amir-Hossein Karimi, Bernhard Sch\u00f6lkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. CoRR, abs/2002.06278, 2020.\n\nAlgorithmic recourse under imperfect causal knowledge: a probabilistic approach. Julius Amir-Hossein Karimi, Bernhard Von K\u00fcgelgen, Isabel Sch\u00f6lkopf, Valera, CoRRAmir-Hossein Karimi, Julius von K\u00fcgelgen, Bernhard Sch\u00f6lkopf, and Isabel Valera. Algorithmic recourse under imperfect causal knowledge: a probabilistic approach. CoRR, 2020.\n\nInterpreting interpretability: Understanding data scientists' use of interpretability tools for machine learning. Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, Jennifer Wortman Vaughan, CHI Conference on Human Factors in Computing Systems. Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. Interpreting interpretability: Understanding data scientists' use of interpretability tools for machine learning. In CHI Conference on Human Factors in Computing Systems, 2020.\n\nJoon Sik, Kim , Gregory Plumb, and Ameet Talwalkar. Sanity simulations for saliency methods. arXiv. Joon Sik Kim, Gregory Plumb, and Ameet Talwalkar. Sanity simulations for saliency methods. arXiv, 2021.\n\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Captum: A unified and generic model interpretability library for pytorch. Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-RichardsonNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz- Richardson. Captum: A unified and generic model interpretability library for pytorch, 2020.\n\nThe disagreement problem in explainable machine learning: A practitioner's perspective. arXiv. Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, Himabindu Lakkaraju, Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu Lakkaraju. The disagreement problem in explainable machine learning: A practitioner's perspective. arXiv, 2022.\n\nIsaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, Finale Doshi-Velez, An evaluation of the human-interpretability of explanation. arXiv. Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. An evaluation of the human-interpretability of explanation. arXiv, 2019.\n\nhow do i fool you?\" manipulating user trust via misleading black box explanations. Himabindu Lakkaraju, Osbert Bastani, AAAI Conference on AIES. Himabindu Lakkaraju and Osbert Bastani. \"how do i fool you?\" manipulating user trust via misleading black box explanations. In AAAI Conference on AIES, 2020.\n\nInterpretable decision sets: A joint framework for description and prediction. Himabindu Lakkaraju, H Stephen, Jure Bach, Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data miningHimabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1675-1684, 2016.\n\nFaithful and customizable explanations of black box models. Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Jure Leskovec, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. the 2019 AAAI/ACM Conference on AI, Ethics, and SocietyHimabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. Faithful and customizable explanations of black box models. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 131-138, 2019.\n\nInterpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. Benjamin Letham, Cynthia Rudin, H Tyler, David Mccormick, Madigan, The Annals of Applied Statistics. 93Benjamin Letham, Cynthia Rudin, Tyler H McCormick, and David Madigan. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):1350-1371, 2015.\n\nExplainable ai: A review of machine learning interpretability methods. Pantelis Linardatos, Vasilis Papastefanopoulos, Sotiris Kotsiantis, Entropy. 23118Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1):18, 2021.\n\nThe mythos of model interpretability. Zachary C Lipton, abs/1606.03490CoRRZachary C Lipton. The mythos of model interpretability. CoRR, abs/1606.03490, 2016.\n\nSynthetic benchmarks for scientific research in explainable machine learning. Yang Liu, Sujay Khandagale, Colin White, Willie Neiswanger, NeurIPS Datasets and Benchmarks Track. Yang Liu, Sujay Khandagale, Colin White, and Willie Neiswanger. Synthetic benchmarks for scientific research in explainable machine learning. In NeurIPS Datasets and Benchmarks Track, 2021.\n\nInterpretable counterfactual explanations guided by prototypes. CoRR, abs. Arnaud Looveren, Janis Klaise, Arnaud Looveren and Janis Klaise. Interpretable counterfactual explanations guided by prototypes. CoRR, abs/ 1907.02584, 2019.\n\nIntelligible models for classification and regression. Yin Lou, Rich Caruana, Johannes Gehrke, KDD. Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In KDD, 2012.\n\nA unified approach to interpreting model predictions. M Scott, Su-In Lundberg, ; I Lee, U V Guyon, S Luxburg, H Bengio, R Wallach, S Fergus, R Vishwanathan, Garnett, Neural Information Processing Systems (NIPS). Curran Associates, IncScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Neural Information Processing Systems (NIPS), pages 4765-4774. Curran Associates, Inc., 2017.\n\nA unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, Advances in Neural Information Processing Systems. Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4765-4774, 2017.\n\nReza Abbasi-Asl, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. W James Murdoch, Chandan Singh, Karl Kumbier, Proceedings of the National Academy of Sciences. the National Academy of SciencesW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences, 2019.\n\nLearning model-agnostic counterfactual explanations for tabular data. Martin Pawelczyk, Klaus Broelemann, Gjergji Kasneci, WWWMartin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual explanations for tabular data. In WWW, 2020.\n\nCarla: A python library to benchmark algorithmic recourse and counterfactual explanation algorithms. Martin Pawelczyk, Sascha Bielawski, Johan Van Den, Tobias Heuvel, Gjergji Richter, Kasneci, NeurIPS Benchmark and Datasets Track. Martin Pawelczyk, Sascha Bielawski, Johan Van den Heuvel, Tobias Richter, and Gjergji Kasneci. Carla: A python library to benchmark algorithmic recourse and counterfactual explanation algorithms. In NeurIPS Benchmark and Datasets Track, 2021.\n\nRise: Randomized input sampling for explanation of black-box models. Vitali Petsiuk, Abir Das, Kate Saenko, arXivVitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. arXiv, 2018.\n\nManipulating and measuring model interpretability. Forough Poursabzi-Sangdeh, G Daniel, Jake M Goldstein, Jennifer Wortman Hofman, Hanna Vaughan, Wallach, CoRRForough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. Manipulating and measuring model interpretability. CoRR, 2018.\n\nFACE: Feasible and actionable counterfactual explanations. Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, Peter Flach, AAAI Conference on AIES. Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: Feasible and actionable counterfactual explanations. In AAAI Conference on AIES, 2020.\n\nwhy should i trust you?\": Explaining the predictions of any classifier. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, KDD. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should i trust you?\": Explaining the predictions of any classifier. In KDD, 2016.\n\nAnchors: High-precision modelagnostic explanations. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, AAAI. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model- agnostic explanations. In AAAI, 2018.\n\nStop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature Machine Intelligence. Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 2019.\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017.\n\nLearning important features through propagating activation differences. Avanti Shrikumar, Peyton Greenside, Anshul Kundaje, ICML. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In ICML, 2017.\n\nDeep inside convolutional networks: Visualising image classification models and saliency maps. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, ICLR. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR, 2014.\n\nFooling lime and shap: Adversarial attacks on post hoc explanation methods. Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju, AAAI Conference on AIES. Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In AAAI Conference on AIES, 2020.\n\nReliable post hoc explanations: Modeling uncertainty in explainability. Dylan Slack, Anna Hilgard, Sameer Singh, Himabindu Lakkaraju, NeurIPS. Dylan Slack, Anna Hilgard, Sameer Singh, and Himabindu Lakkaraju. Reliable post hoc explanations: Modeling uncertainty in explainability. NeurIPS, 2021.\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, Martin Wattenberg, Smoothgrad: removing noise by adding noise. arXiv. Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv, 2017.\n\nUsing the adap learning algorithm to forecast the onset of diabetes mellitus. W Jack, James E Smith, Everhart, Dickson, C William, Robert Scott Knowler, Johannes, Proceedings of the annual symposium on computer application in medical care. the annual symposium on computer application in medical care261Jack W Smith, James E Everhart, WC Dickson, William C Knowler, and Robert Scott Johannes. Using the adap learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the annual symposium on computer application in medical care, page 261. American Medical Informatics Association, 1988.\n\nAxiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, ICML. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, 2017.\n\nTowards robust and reliable algorithmic recourse. Sohini Upadhyay, Shalmali Joshi, Himabindu Lakkaraju, NeurIPS. Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards robust and reliable algorithmic recourse. NeurIPS, 2021.\n\nActionable recourse in linear classification. Berk Ustun, Alexander Spangher, Yang Liu, FAccT. Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In FAccT, 2019.\n\nCounterfactual explanations for machine learning: A review. Sahil Verma, John Dickerson, Keegan Hines, arXivSahil Verma, John Dickerson, and Keegan Hines. Counterfactual explanations for machine learning: A review. arXiv, 2020.\n\nCounterfactual explanations without opening the black box: Automated decisions and the GDPR. Sandra Wachter, Brent Mittelstadt, Chris Russell, Harvard Journal of Law & Technology. 31841Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & Technology, 31:841, 2017.\n\nFalling rule lists. Fulton Wang, Cynthia Rudin, Artificial Intelligence and Statistics. PMLRFulton Wang and Cynthia Rudin. Falling rule lists. In Artificial Intelligence and Statistics, pages 1013-1022. PMLR, 2015.\n\nMapping chemical performance on molecular structures using locally interpretable explanations. Anthe Leanne S Whitmore, Corey M George, Hudson, abs/1611.07443CoRRLeanne S Whitmore, Anthe George, and Corey M Hudson. Mapping chemical performance on molecular structures using locally interpretable explanations. CoRR, abs/1611.07443, 2016.\n\nThe comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. I-Cheng Yeh, Che-Hui Lien, Expert Systems with Applications. I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. In Expert Systems with Applications, 2009.\n\nInterpretable classification models for recidivism prediction. Jiaming Zeng, Berk Ustun, Cynthia Rudin, Journal of the Royal Statistical Society: Series A (Statistics in Society. Jiaming Zeng, Berk Ustun, and Cynthia Rudin. Interpretable classification models for recidivism prediction. Journal of the Royal Statistical Society: Series A (Statistics in Society), 2017.\n\nEvaluating the quality of machine learning explanations: A survey on methods and metrics. Jianlong Zhou, H Amir, Fang Gandomi, Andreas Chen, Holzinger, Electronics. 105593Jianlong Zhou, Amir H Gandomi, Fang Chen, and Andreas Holzinger. Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics, 10(5):593, 2021.\n", "annotations": {"author": "[{\"end\":120,\"start\":76},{\"end\":156,\"start\":121},{\"end\":197,\"start\":157},{\"end\":240,\"start\":198},{\"end\":283,\"start\":241},{\"end\":315,\"start\":284},{\"end\":352,\"start\":316},{\"end\":394,\"start\":353}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":83},{\"end\":134,\"start\":128},{\"end\":175,\"start\":168},{\"end\":214,\"start\":205},{\"end\":253,\"start\":246},{\"end\":293,\"start\":289},{\"end\":330,\"start\":324},{\"end\":372,\"start\":363}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":127,\"start\":121},{\"end\":167,\"start\":157},{\"end\":204,\"start\":198},{\"end\":245,\"start\":241},{\"end\":288,\"start\":284},{\"end\":323,\"start\":316},{\"end\":362,\"start\":353}]", "author_affiliation": "[{\"end\":111,\"start\":92},{\"end\":119,\"start\":113},{\"end\":155,\"start\":136},{\"end\":196,\"start\":177},{\"end\":239,\"start\":216},{\"end\":282,\"start\":255},{\"end\":314,\"start\":295},{\"end\":351,\"start\":332},{\"end\":393,\"start\":374}]", "title": "[{\"end\":73,\"start\":1},{\"end\":467,\"start\":395}]", "venue": null, "abstract": "[{\"end\":4628,\"start\":469}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5490,\"start\":5486},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5493,\"start\":5490},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":5496,\"start\":5493},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5711,\"start\":5708},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":5821,\"start\":5817},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5824,\"start\":5821},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5827,\"start\":5824},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5858,\"start\":5855},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5860,\"start\":5858},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5879,\"start\":5875},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5881,\"start\":5879},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6164,\"start\":6161},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":6265,\"start\":6261},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6268,\"start\":6265},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":6271,\"start\":6268},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":6274,\"start\":6271},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6683,\"start\":6679},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8023,\"start\":8019},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":8026,\"start\":8023},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8028,\"start\":8026},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8031,\"start\":8028},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":8162,\"start\":8158},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8165,\"start\":8162},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8168,\"start\":8165},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8274,\"start\":8270},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8276,\"start\":8274},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8421,\"start\":8417},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8452,\"start\":8448},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8455,\"start\":8452},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":8458,\"start\":8455},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8460,\"start\":8458},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8463,\"start\":8460},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8466,\"start\":8463},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8576,\"start\":8572},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8867,\"start\":8864},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9019,\"start\":9015},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9338,\"start\":9334},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9352,\"start\":9348},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9368,\"start\":9364},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9515,\"start\":9511},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9753,\"start\":9749},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10095,\"start\":10092},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10196,\"start\":10193},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10657,\"start\":10654},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10660,\"start\":10657},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10663,\"start\":10660},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10847,\"start\":10844},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10920,\"start\":10916},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11389,\"start\":11385},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11719,\"start\":11715},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11867,\"start\":11863},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15213,\"start\":15209},{\"end\":15425,\"start\":15403},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15473,\"start\":15469},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15476,\"start\":15473},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15650,\"start\":15646},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16913,\"start\":16909},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":19796,\"start\":19792},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19853,\"start\":19849},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21901,\"start\":21897},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24036,\"start\":24032},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24039,\"start\":24036},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24089,\"start\":24086},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24172,\"start\":24168},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24531,\"start\":24527},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":26475,\"start\":26471},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26478,\"start\":26475},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27137,\"start\":27134},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27157,\"start\":27154},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27586,\"start\":27582},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31653,\"start\":31649},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32653,\"start\":32650},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32656,\"start\":32653},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":35935,\"start\":35931},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36175,\"start\":36171},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":38360,\"start\":38356},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38871,\"start\":38868},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40931,\"start\":40927},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":41287,\"start\":41283},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":41290,\"start\":41287},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":41330,\"start\":41326},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":41347,\"start\":41343},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41350,\"start\":41347},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":41353,\"start\":41350},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41356,\"start\":41353},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":41465,\"start\":41461},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":41776,\"start\":41772},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41779,\"start\":41776},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":41782,\"start\":41779},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":41785,\"start\":41782},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":42099,\"start\":42095},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":42102,\"start\":42099},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":42105,\"start\":42102},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":42108,\"start\":42105},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":42480,\"start\":42476},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":42483,\"start\":42480},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":42486,\"start\":42483},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":42489,\"start\":42486},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":42492,\"start\":42489},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42495,\"start\":42492},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":42498,\"start\":42495},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":42501,\"start\":42498},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42504,\"start\":42501},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":42507,\"start\":42504},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":42816,\"start\":42812},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42819,\"start\":42816},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42915,\"start\":42912},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":42918,\"start\":42915},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":42921,\"start\":42918},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":42924,\"start\":42921},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":42927,\"start\":42924},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":43437,\"start\":43433},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":43440,\"start\":43437},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43442,\"start\":43440},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43445,\"start\":43442},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":43602,\"start\":43598},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":43643,\"start\":43639},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":43793,\"start\":43789},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":43893,\"start\":43889},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":44061,\"start\":44057},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44221,\"start\":44217},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":44811,\"start\":44807},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":44833,\"start\":44829},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":45031,\"start\":45027},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":45053,\"start\":45049},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":45282,\"start\":45278},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":45285,\"start\":45282},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45287,\"start\":45285},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":45537,\"start\":45533},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":54515,\"start\":54511},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":54831,\"start\":54827},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":55169,\"start\":55165},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":55581,\"start\":55577},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":56045,\"start\":56041},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":56158,\"start\":56154},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":56185,\"start\":56181},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":56202,\"start\":56198},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":56225,\"start\":56221},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":56236,\"start\":56232},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":56251,\"start\":56247},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":62293,\"start\":62289},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":62324,\"start\":62321},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":63562,\"start\":63558},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":63565,\"start\":63562},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":63567,\"start\":63565},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":63570,\"start\":63567},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":63573,\"start\":63570},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":63594,\"start\":63590},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":63597,\"start\":63594},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":63600,\"start\":63597},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":63603,\"start\":63600},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":63804,\"start\":63800},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":63807,\"start\":63804},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":63809,\"start\":63807},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":63812,\"start\":63809},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":63815,\"start\":63812},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":63818,\"start\":63815},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":64034,\"start\":64030},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":64037,\"start\":64034},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":64040,\"start\":64037},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":64043,\"start\":64040},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":64046,\"start\":64043},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":64048,\"start\":64046},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":64906,\"start\":64902},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":64909,\"start\":64906},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":64912,\"start\":64909},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":65166,\"start\":65162},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":65169,\"start\":65166},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":65172,\"start\":65169},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":65175,\"start\":65172},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":67380,\"start\":67376}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":66022,\"start\":65723},{\"attributes\":{\"id\":\"fig_3\"},\"end\":66363,\"start\":66023},{\"attributes\":{\"id\":\"fig_4\"},\"end\":66703,\"start\":66364},{\"attributes\":{\"id\":\"fig_5\"},\"end\":68629,\"start\":66704},{\"attributes\":{\"id\":\"fig_6\"},\"end\":69221,\"start\":68630},{\"attributes\":{\"id\":\"fig_7\"},\"end\":69649,\"start\":69222},{\"attributes\":{\"id\":\"fig_8\"},\"end\":69869,\"start\":69650},{\"attributes\":{\"id\":\"fig_9\"},\"end\":69983,\"start\":69870},{\"attributes\":{\"id\":\"fig_10\"},\"end\":70139,\"start\":69984},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":70242,\"start\":70140},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":70606,\"start\":70243},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":72955,\"start\":70607},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":73483,\"start\":72956},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":74022,\"start\":73484},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":74456,\"start\":74023},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":75656,\"start\":74457},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":76037,\"start\":75657},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":76684,\"start\":76038},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":77338,\"start\":76685},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":77934,\"start\":77339},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":78525,\"start\":77935},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":79116,\"start\":78526},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":79735,\"start\":79117},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":80936,\"start\":79736},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":82122,\"start\":80937},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":83813,\"start\":82123},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":84335,\"start\":83814},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":84907,\"start\":84336},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":85531,\"start\":84908},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":86164,\"start\":85532},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":86791,\"start\":86165},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":87420,\"start\":86792},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":88057,\"start\":87421},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":88613,\"start\":88058},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":89173,\"start\":88614},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":89724,\"start\":89174},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":90295,\"start\":89725},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":90854,\"start\":90296},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":92129,\"start\":90855}]", "paragraph": "[{\"end\":5712,\"start\":4644},{\"end\":7084,\"start\":5714},{\"end\":7395,\"start\":7086},{\"end\":7824,\"start\":7397},{\"end\":9197,\"start\":7826},{\"end\":9488,\"start\":9199},{\"end\":10827,\"start\":9490},{\"end\":12235,\"start\":10829},{\"end\":12379,\"start\":12249},{\"end\":12945,\"start\":12381},{\"end\":13726,\"start\":12947},{\"end\":14401,\"start\":13728},{\"end\":14709,\"start\":14435},{\"end\":15426,\"start\":14748},{\"end\":16914,\"start\":15428},{\"end\":17912,\"start\":16916},{\"end\":19201,\"start\":17914},{\"end\":20960,\"start\":19203},{\"end\":21045,\"start\":20962},{\"end\":21422,\"start\":21047},{\"end\":22708,\"start\":21424},{\"end\":22847,\"start\":22710},{\"end\":23550,\"start\":22849},{\"end\":24480,\"start\":23552},{\"end\":26405,\"start\":24482},{\"end\":27881,\"start\":26407},{\"end\":28230,\"start\":27883},{\"end\":28371,\"start\":28232},{\"end\":29130,\"start\":28373},{\"end\":29796,\"start\":29151},{\"end\":30075,\"start\":29822},{\"end\":31100,\"start\":30099},{\"end\":31865,\"start\":31249},{\"end\":34331,\"start\":31867},{\"end\":34383,\"start\":34342},{\"end\":35349,\"start\":34399},{\"end\":35671,\"start\":35374},{\"end\":36072,\"start\":35692},{\"end\":36343,\"start\":36074},{\"end\":38026,\"start\":36345},{\"end\":38365,\"start\":38028},{\"end\":38438,\"start\":38435},{\"end\":38577,\"start\":38509},{\"end\":38719,\"start\":38579},{\"end\":39093,\"start\":38721},{\"end\":39672,\"start\":39206},{\"end\":40040,\"start\":39789},{\"end\":40365,\"start\":40166},{\"end\":40495,\"start\":40367},{\"end\":40699,\"start\":40512},{\"end\":40932,\"start\":40701},{\"end\":41149,\"start\":40962},{\"end\":42303,\"start\":41151},{\"end\":42928,\"start\":42305},{\"end\":43288,\"start\":42930},{\"end\":44555,\"start\":43290},{\"end\":45931,\"start\":44557},{\"end\":47869,\"start\":45955},{\"end\":48200,\"start\":47871},{\"end\":48654,\"start\":48202},{\"end\":48785,\"start\":48656},{\"end\":49277,\"start\":48787},{\"end\":51295,\"start\":49279},{\"end\":51660,\"start\":51319},{\"end\":51986,\"start\":51662},{\"end\":52292,\"start\":51988},{\"end\":52818,\"start\":52294},{\"end\":53268,\"start\":52820},{\"end\":53540,\"start\":53270},{\"end\":54251,\"start\":53592},{\"end\":54516,\"start\":54253},{\"end\":54832,\"start\":54518},{\"end\":55170,\"start\":54834},{\"end\":55582,\"start\":55172},{\"end\":56046,\"start\":55584},{\"end\":56745,\"start\":56074},{\"end\":56959,\"start\":56756},{\"end\":57346,\"start\":56961},{\"end\":58778,\"start\":57377},{\"end\":59568,\"start\":58831},{\"end\":60088,\"start\":59570},{\"end\":60324,\"start\":60090},{\"end\":61016,\"start\":60346},{\"end\":62000,\"start\":61043},{\"end\":63321,\"start\":62002},{\"end\":64398,\"start\":63323},{\"end\":65722,\"start\":64400}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":31248,\"start\":31101},{\"attributes\":{\"id\":\"formula_1\"},\"end\":38434,\"start\":38366},{\"attributes\":{\"id\":\"formula_2\"},\"end\":38508,\"start\":38439},{\"attributes\":{\"id\":\"formula_3\"},\"end\":39205,\"start\":39094},{\"attributes\":{\"id\":\"formula_4\"},\"end\":39788,\"start\":39673},{\"attributes\":{\"id\":\"formula_5\"},\"end\":40165,\"start\":40041}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19862,\"start\":19855},{\"end\":22410,\"start\":22403},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31533,\"start\":31522},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31990,\"start\":31976},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32037,\"start\":32021},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32374,\"start\":32367},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32649,\"start\":32641},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32690,\"start\":32678},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":53719,\"start\":53712},{\"end\":56260,\"start\":56253},{\"end\":56472,\"start\":56465},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":62626,\"start\":62615},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":62887,\"start\":62877}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4642,\"start\":4630},{\"end\":12247,\"start\":12238},{\"attributes\":{\"n\":\"2\"},\"end\":14433,\"start\":14404},{\"end\":14746,\"start\":14712},{\"end\":29149,\"start\":29133},{\"attributes\":{\"n\":\"3\"},\"end\":29820,\"start\":29799},{\"end\":30097,\"start\":30078},{\"end\":34340,\"start\":34334},{\"attributes\":{\"n\":\"4\"},\"end\":34397,\"start\":34386},{\"end\":35372,\"start\":35352},{\"end\":35690,\"start\":35674},{\"end\":40510,\"start\":40498},{\"end\":40960,\"start\":40935},{\"end\":45953,\"start\":45934},{\"end\":51317,\"start\":51298},{\"end\":53590,\"start\":53543},{\"end\":56072,\"start\":56049},{\"end\":56754,\"start\":56748},{\"end\":57375,\"start\":57349},{\"end\":58829,\"start\":58781},{\"end\":60344,\"start\":60327},{\"end\":61041,\"start\":61019},{\"end\":65734,\"start\":65724},{\"end\":66034,\"start\":66024},{\"end\":66375,\"start\":66365},{\"end\":68644,\"start\":68631},{\"end\":69233,\"start\":69223},{\"end\":70150,\"start\":70141},{\"end\":70253,\"start\":70244},{\"end\":70617,\"start\":70608},{\"end\":72966,\"start\":72957},{\"end\":73494,\"start\":73485},{\"end\":74033,\"start\":74024},{\"end\":74467,\"start\":74458},{\"end\":75667,\"start\":75658},{\"end\":76049,\"start\":76039},{\"end\":76696,\"start\":76686},{\"end\":77350,\"start\":77340},{\"end\":77946,\"start\":77936},{\"end\":78537,\"start\":78527},{\"end\":79128,\"start\":79118},{\"end\":79747,\"start\":79737},{\"end\":80948,\"start\":80938},{\"end\":82134,\"start\":82124},{\"end\":83825,\"start\":83815},{\"end\":84347,\"start\":84337},{\"end\":84919,\"start\":84909},{\"end\":85543,\"start\":85533},{\"end\":86176,\"start\":86166},{\"end\":86803,\"start\":86793},{\"end\":87432,\"start\":87422},{\"end\":88069,\"start\":88059},{\"end\":88625,\"start\":88615},{\"end\":89185,\"start\":89175},{\"end\":89736,\"start\":89726},{\"end\":90307,\"start\":90297},{\"end\":90866,\"start\":90856}]", "table": "[{\"end\":70242,\"start\":70203},{\"end\":72955,\"start\":72137},{\"end\":73483,\"start\":73188},{\"end\":74022,\"start\":73720},{\"end\":74456,\"start\":74196},{\"end\":75656,\"start\":74836},{\"end\":76684,\"start\":76404},{\"end\":77338,\"start\":76699},{\"end\":77934,\"start\":77438},{\"end\":78525,\"start\":78181},{\"end\":79116,\"start\":78774},{\"end\":79735,\"start\":79438},{\"end\":80936,\"start\":80106},{\"end\":82122,\"start\":81310},{\"end\":83813,\"start\":82715},{\"end\":84335,\"start\":84051},{\"end\":84907,\"start\":84567},{\"end\":85531,\"start\":85170},{\"end\":86164,\"start\":85886},{\"end\":86791,\"start\":86515},{\"end\":87420,\"start\":87141},{\"end\":88057,\"start\":87777},{\"end\":88613,\"start\":88486},{\"end\":89173,\"start\":89046},{\"end\":89724,\"start\":89608},{\"end\":90295,\"start\":90157},{\"end\":90854,\"start\":90648},{\"end\":92129,\"start\":91767}]", "figure_caption": "[{\"end\":66022,\"start\":65736},{\"end\":66363,\"start\":66036},{\"end\":66703,\"start\":66377},{\"end\":68629,\"start\":66706},{\"end\":69221,\"start\":68646},{\"end\":69649,\"start\":69235},{\"end\":69869,\"start\":69652},{\"end\":69983,\"start\":69872},{\"end\":70139,\"start\":69986},{\"end\":70203,\"start\":70152},{\"end\":70606,\"start\":70255},{\"end\":72137,\"start\":70619},{\"end\":73188,\"start\":72968},{\"end\":73720,\"start\":73496},{\"end\":74196,\"start\":74035},{\"end\":74836,\"start\":74469},{\"end\":76037,\"start\":75669},{\"end\":76404,\"start\":76052},{\"end\":77438,\"start\":77353},{\"end\":78181,\"start\":77949},{\"end\":78774,\"start\":78540},{\"end\":79438,\"start\":79131},{\"end\":80106,\"start\":79750},{\"end\":81310,\"start\":80951},{\"end\":82715,\"start\":82137},{\"end\":84051,\"start\":83828},{\"end\":84567,\"start\":84350},{\"end\":85170,\"start\":84922},{\"end\":85886,\"start\":85546},{\"end\":86515,\"start\":86179},{\"end\":87141,\"start\":86806},{\"end\":87777,\"start\":87435},{\"end\":88486,\"start\":88072},{\"end\":89046,\"start\":88628},{\"end\":89608,\"start\":89188},{\"end\":90157,\"start\":89739},{\"end\":90648,\"start\":90310},{\"end\":91767,\"start\":90869}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29556,\"start\":29548},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33583,\"start\":33568},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34170,\"start\":34162},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":40874,\"start\":40859},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":51501,\"start\":51493}]", "bib_author_first_name": "[{\"end\":93748,\"start\":93742},{\"end\":93761,\"start\":93758},{\"end\":93967,\"start\":93961},{\"end\":93981,\"start\":93977},{\"end\":93997,\"start\":93991},{\"end\":94019,\"start\":94009},{\"end\":94035,\"start\":94029},{\"end\":94051,\"start\":94044},{\"end\":94069,\"start\":94060},{\"end\":94441,\"start\":94434},{\"end\":94457,\"start\":94451},{\"end\":94473,\"start\":94467},{\"end\":94489,\"start\":94483},{\"end\":94506,\"start\":94500},{\"end\":94520,\"start\":94511},{\"end\":94796,\"start\":94790},{\"end\":94812,\"start\":94806},{\"end\":94826,\"start\":94819},{\"end\":94847,\"start\":94838},{\"end\":94862,\"start\":94855},{\"end\":94874,\"start\":94869},{\"end\":95045,\"start\":95040},{\"end\":95066,\"start\":95061},{\"end\":95068,\"start\":95067},{\"end\":95382,\"start\":95373},{\"end\":95407,\"start\":95400},{\"end\":95430,\"start\":95424},{\"end\":95434,\"start\":95431},{\"end\":95446,\"start\":95440},{\"end\":95462,\"start\":95457},{\"end\":95477,\"start\":95470},{\"end\":95495,\"start\":95487},{\"end\":95510,\"start\":95504},{\"end\":95528,\"start\":95522},{\"end\":95544,\"start\":95537},{\"end\":95983,\"start\":95977},{\"end\":96003,\"start\":95997},{\"end\":96016,\"start\":96011},{\"end\":96033,\"start\":96027},{\"end\":96051,\"start\":96046},{\"end\":96068,\"start\":96061},{\"end\":96355,\"start\":96350},{\"end\":96370,\"start\":96364},{\"end\":96383,\"start\":96380},{\"end\":96615,\"start\":96610},{\"end\":96631,\"start\":96625},{\"end\":96646,\"start\":96640},{\"end\":96856,\"start\":96850},{\"end\":96873,\"start\":96866},{\"end\":96884,\"start\":96879},{\"end\":97059,\"start\":97054},{\"end\":97072,\"start\":97066},{\"end\":97202,\"start\":97197},{\"end\":97218,\"start\":97212},{\"end\":97235,\"start\":97228},{\"end\":97252,\"start\":97244},{\"end\":97265,\"start\":97259},{\"end\":97284,\"start\":97277},{\"end\":97613,\"start\":97609},{\"end\":97626,\"start\":97623},{\"end\":97640,\"start\":97632},{\"end\":97653,\"start\":97649},{\"end\":97664,\"start\":97660},{\"end\":97678,\"start\":97672},{\"end\":97950,\"start\":97943},{\"end\":97961,\"start\":97957},{\"end\":97979,\"start\":97971},{\"end\":97994,\"start\":97987},{\"end\":98007,\"start\":98002},{\"end\":98247,\"start\":98244},{\"end\":98261,\"start\":98256},{\"end\":98277,\"start\":98272},{\"end\":98515,\"start\":98508},{\"end\":98527,\"start\":98521},{\"end\":98544,\"start\":98538},{\"end\":98555,\"start\":98554},{\"end\":98574,\"start\":98565},{\"end\":98936,\"start\":98930},{\"end\":98951,\"start\":98947},{\"end\":98965,\"start\":98959},{\"end\":99179,\"start\":99172},{\"end\":99199,\"start\":99198},{\"end\":99214,\"start\":99206},{\"end\":99463,\"start\":99457},{\"end\":99472,\"start\":99471},{\"end\":99484,\"start\":99480},{\"end\":99638,\"start\":99632},{\"end\":99649,\"start\":99644},{\"end\":99896,\"start\":99891},{\"end\":99907,\"start\":99906},{\"end\":99921,\"start\":99915},{\"end\":100212,\"start\":100207},{\"end\":100221,\"start\":100220},{\"end\":100233,\"start\":100228},{\"end\":100703,\"start\":100698},{\"end\":100716,\"start\":100713},{\"end\":100946,\"start\":100941},{\"end\":101281,\"start\":101274},{\"end\":101296,\"start\":101292},{\"end\":101320,\"start\":101312},{\"end\":101563,\"start\":101556},{\"end\":101582,\"start\":101574},{\"end\":101594,\"start\":101589},{\"end\":101853,\"start\":101845},{\"end\":101868,\"start\":101864},{\"end\":101888,\"start\":101879},{\"end\":101905,\"start\":101899},{\"end\":101919,\"start\":101914},{\"end\":101935,\"start\":101931},{\"end\":102291,\"start\":102286},{\"end\":102302,\"start\":102297},{\"end\":102322,\"start\":102313},{\"end\":102522,\"start\":102518},{\"end\":102540,\"start\":102533},{\"end\":102555,\"start\":102548},{\"end\":102570,\"start\":102565},{\"end\":102588,\"start\":102580},{\"end\":102605,\"start\":102596},{\"end\":102628,\"start\":102618},{\"end\":102971,\"start\":102967},{\"end\":102987,\"start\":102980},{\"end\":103282,\"start\":103278},{\"end\":103299,\"start\":103292},{\"end\":103312,\"start\":103307},{\"end\":103327,\"start\":103323},{\"end\":103623,\"start\":103617},{\"end\":103639,\"start\":103631},{\"end\":103655,\"start\":103647},{\"end\":103669,\"start\":103665},{\"end\":103682,\"start\":103677},{\"end\":103697,\"start\":103692},{\"end\":103711,\"start\":103707},{\"end\":104043,\"start\":104042},{\"end\":104056,\"start\":104052},{\"end\":104058,\"start\":104057},{\"end\":104442,\"start\":104436},{\"end\":104456,\"start\":104451},{\"end\":104471,\"start\":104465},{\"end\":104738,\"start\":104730},{\"end\":104766,\"start\":104760},{\"end\":105035,\"start\":105029},{\"end\":105065,\"start\":105057},{\"end\":105086,\"start\":105080},{\"end\":105410,\"start\":105399},{\"end\":105423,\"start\":105417},{\"end\":105436,\"start\":105430},{\"end\":105450,\"start\":105446},{\"end\":105465,\"start\":105460},{\"end\":105483,\"start\":105475},{\"end\":105491,\"start\":105484},{\"end\":105843,\"start\":105839},{\"end\":105852,\"start\":105849},{\"end\":106050,\"start\":106044},{\"end\":106068,\"start\":106063},{\"end\":106084,\"start\":106078},{\"end\":106099,\"start\":106093},{\"end\":106111,\"start\":106106},{\"end\":106131,\"start\":106123},{\"end\":106151,\"start\":106142},{\"end\":106687,\"start\":106677},{\"end\":106702,\"start\":106697},{\"end\":106712,\"start\":106708},{\"end\":106722,\"start\":106717},{\"end\":106737,\"start\":106731},{\"end\":106753,\"start\":106747},{\"end\":106767,\"start\":106758},{\"end\":106992,\"start\":106987},{\"end\":107004,\"start\":106999},{\"end\":107018,\"start\":107011},{\"end\":107029,\"start\":107023},{\"end\":107045,\"start\":107041},{\"end\":107054,\"start\":107051},{\"end\":107071,\"start\":107065},{\"end\":107420,\"start\":107411},{\"end\":107438,\"start\":107432},{\"end\":107720,\"start\":107711},{\"end\":107733,\"start\":107732},{\"end\":107747,\"start\":107743},{\"end\":108278,\"start\":108269},{\"end\":108293,\"start\":108290},{\"end\":108305,\"start\":108301},{\"end\":108319,\"start\":108315},{\"end\":108791,\"start\":108783},{\"end\":108807,\"start\":108800},{\"end\":108816,\"start\":108815},{\"end\":108829,\"start\":108824},{\"end\":109196,\"start\":109188},{\"end\":109216,\"start\":109209},{\"end\":109243,\"start\":109236},{\"end\":109680,\"start\":109676},{\"end\":109691,\"start\":109686},{\"end\":109709,\"start\":109704},{\"end\":109723,\"start\":109717},{\"end\":110047,\"start\":110041},{\"end\":110063,\"start\":110058},{\"end\":110258,\"start\":110255},{\"end\":110268,\"start\":110264},{\"end\":110286,\"start\":110278},{\"end\":110469,\"start\":110468},{\"end\":110482,\"start\":110477},{\"end\":110494,\"start\":110493},{\"end\":110496,\"start\":110495},{\"end\":110503,\"start\":110502},{\"end\":110505,\"start\":110504},{\"end\":110514,\"start\":110513},{\"end\":110525,\"start\":110524},{\"end\":110535,\"start\":110534},{\"end\":110546,\"start\":110545},{\"end\":110556,\"start\":110555},{\"end\":110988,\"start\":110987},{\"end\":111001,\"start\":110996},{\"end\":111342,\"start\":111335},{\"end\":111359,\"start\":111352},{\"end\":111371,\"start\":111367},{\"end\":111743,\"start\":111737},{\"end\":111760,\"start\":111755},{\"end\":111780,\"start\":111773},{\"end\":112042,\"start\":112036},{\"end\":112060,\"start\":112054},{\"end\":112077,\"start\":112072},{\"end\":112093,\"start\":112087},{\"end\":112109,\"start\":112102},{\"end\":112485,\"start\":112479},{\"end\":112499,\"start\":112495},{\"end\":112509,\"start\":112505},{\"end\":112707,\"start\":112700},{\"end\":112728,\"start\":112727},{\"end\":112741,\"start\":112737},{\"end\":112743,\"start\":112742},{\"end\":112763,\"start\":112755},{\"end\":112771,\"start\":112764},{\"end\":112785,\"start\":112780},{\"end\":113044,\"start\":113038},{\"end\":113061,\"start\":113055},{\"end\":113073,\"start\":113069},{\"end\":113096,\"start\":113092},{\"end\":113110,\"start\":113105},{\"end\":113399,\"start\":113393},{\"end\":113427,\"start\":113421},{\"end\":113652,\"start\":113646},{\"end\":113680,\"start\":113674},{\"end\":113951,\"start\":113944},{\"end\":114236,\"start\":114235},{\"end\":114257,\"start\":114250},{\"end\":114277,\"start\":114269},{\"end\":114299,\"start\":114288},{\"end\":114309,\"start\":114305},{\"end\":114325,\"start\":114320},{\"end\":114626,\"start\":114620},{\"end\":114644,\"start\":114638},{\"end\":114662,\"start\":114656},{\"end\":114922,\"start\":114917},{\"end\":114939,\"start\":114933},{\"end\":114955,\"start\":114949},{\"end\":115219,\"start\":115214},{\"end\":115233,\"start\":115227},{\"end\":115248,\"start\":115243},{\"end\":115260,\"start\":115254},{\"end\":115277,\"start\":115268},{\"end\":115581,\"start\":115576},{\"end\":115593,\"start\":115589},{\"end\":115609,\"start\":115603},{\"end\":115626,\"start\":115617},{\"end\":115807,\"start\":115801},{\"end\":115823,\"start\":115817},{\"end\":115836,\"start\":115832},{\"end\":115850,\"start\":115842},{\"end\":115865,\"start\":115859},{\"end\":116147,\"start\":116146},{\"end\":116159,\"start\":116154},{\"end\":116161,\"start\":116160},{\"end\":116189,\"start\":116188},{\"end\":116205,\"start\":116199},{\"end\":116211,\"start\":116206},{\"end\":116725,\"start\":116719},{\"end\":116745,\"start\":116740},{\"end\":116756,\"start\":116752},{\"end\":116928,\"start\":116922},{\"end\":116947,\"start\":116939},{\"end\":116964,\"start\":116955},{\"end\":117159,\"start\":117155},{\"end\":117176,\"start\":117167},{\"end\":117191,\"start\":117187},{\"end\":117378,\"start\":117373},{\"end\":117390,\"start\":117386},{\"end\":117408,\"start\":117402},{\"end\":117641,\"start\":117635},{\"end\":117656,\"start\":117651},{\"end\":117675,\"start\":117670},{\"end\":117952,\"start\":117946},{\"end\":117966,\"start\":117959},{\"end\":118242,\"start\":118237},{\"end\":118269,\"start\":118262},{\"end\":118608,\"start\":118601},{\"end\":118621,\"start\":118614},{\"end\":118926,\"start\":118919},{\"end\":118937,\"start\":118933},{\"end\":118952,\"start\":118945},{\"end\":119324,\"start\":119316},{\"end\":119332,\"start\":119331},{\"end\":119343,\"start\":119339},{\"end\":119360,\"start\":119353}]", "bib_author_last_name": "[{\"end\":93756,\"start\":93749},{\"end\":93768,\"start\":93762},{\"end\":93975,\"start\":93968},{\"end\":93989,\"start\":93982},{\"end\":94007,\"start\":93998},{\"end\":94027,\"start\":94020},{\"end\":94042,\"start\":94036},{\"end\":94058,\"start\":94052},{\"end\":94079,\"start\":94070},{\"end\":94449,\"start\":94442},{\"end\":94465,\"start\":94458},{\"end\":94481,\"start\":94474},{\"end\":94498,\"start\":94490},{\"end\":94509,\"start\":94507},{\"end\":94530,\"start\":94521},{\"end\":94804,\"start\":94797},{\"end\":94817,\"start\":94813},{\"end\":94836,\"start\":94827},{\"end\":94853,\"start\":94848},{\"end\":94867,\"start\":94863},{\"end\":94879,\"start\":94875},{\"end\":95059,\"start\":95046},{\"end\":95077,\"start\":95069},{\"end\":95398,\"start\":95383},{\"end\":95422,\"start\":95408},{\"end\":95438,\"start\":95435},{\"end\":95455,\"start\":95447},{\"end\":95468,\"start\":95463},{\"end\":95485,\"start\":95478},{\"end\":95502,\"start\":95496},{\"end\":95520,\"start\":95511},{\"end\":95535,\"start\":95529},{\"end\":95554,\"start\":95545},{\"end\":95995,\"start\":95984},{\"end\":96009,\"start\":96004},{\"end\":96025,\"start\":96017},{\"end\":96044,\"start\":96034},{\"end\":96059,\"start\":96052},{\"end\":96077,\"start\":96069},{\"end\":96362,\"start\":96356},{\"end\":96378,\"start\":96371},{\"end\":96390,\"start\":96384},{\"end\":96395,\"start\":96392},{\"end\":96623,\"start\":96616},{\"end\":96638,\"start\":96632},{\"end\":96655,\"start\":96647},{\"end\":96864,\"start\":96857},{\"end\":96877,\"start\":96874},{\"end\":96892,\"start\":96885},{\"end\":97064,\"start\":97060},{\"end\":97083,\"start\":97073},{\"end\":97210,\"start\":97203},{\"end\":97226,\"start\":97219},{\"end\":97242,\"start\":97236},{\"end\":97257,\"start\":97253},{\"end\":97275,\"start\":97266},{\"end\":97292,\"start\":97285},{\"end\":97621,\"start\":97614},{\"end\":97630,\"start\":97627},{\"end\":97647,\"start\":97641},{\"end\":97658,\"start\":97654},{\"end\":97670,\"start\":97665},{\"end\":97686,\"start\":97679},{\"end\":97955,\"start\":97951},{\"end\":97969,\"start\":97962},{\"end\":97985,\"start\":97980},{\"end\":98000,\"start\":97995},{\"end\":98017,\"start\":98008},{\"end\":98254,\"start\":98248},{\"end\":98270,\"start\":98262},{\"end\":98281,\"start\":98278},{\"end\":98519,\"start\":98516},{\"end\":98536,\"start\":98528},{\"end\":98552,\"start\":98545},{\"end\":98563,\"start\":98556},{\"end\":98579,\"start\":98575},{\"end\":98590,\"start\":98581},{\"end\":98945,\"start\":98937},{\"end\":98957,\"start\":98952},{\"end\":98976,\"start\":98966},{\"end\":99196,\"start\":99180},{\"end\":99204,\"start\":99200},{\"end\":99221,\"start\":99215},{\"end\":99232,\"start\":99223},{\"end\":99469,\"start\":99464},{\"end\":99478,\"start\":99473},{\"end\":99488,\"start\":99485},{\"end\":99642,\"start\":99639},{\"end\":99655,\"start\":99650},{\"end\":99904,\"start\":99897},{\"end\":99913,\"start\":99908},{\"end\":99931,\"start\":99922},{\"end\":99937,\"start\":99933},{\"end\":100218,\"start\":100213},{\"end\":100226,\"start\":100222},{\"end\":100243,\"start\":100234},{\"end\":100256,\"start\":100245},{\"end\":100461,\"start\":100457},{\"end\":100711,\"start\":100704},{\"end\":100732,\"start\":100717},{\"end\":100743,\"start\":100734},{\"end\":100956,\"start\":100947},{\"end\":101290,\"start\":101282},{\"end\":101310,\"start\":101297},{\"end\":101325,\"start\":101321},{\"end\":101572,\"start\":101564},{\"end\":101587,\"start\":101583},{\"end\":101598,\"start\":101595},{\"end\":101862,\"start\":101854},{\"end\":101877,\"start\":101869},{\"end\":101897,\"start\":101889},{\"end\":101912,\"start\":101906},{\"end\":101929,\"start\":101920},{\"end\":101945,\"start\":101936},{\"end\":102295,\"start\":102292},{\"end\":102311,\"start\":102303},{\"end\":102332,\"start\":102323},{\"end\":102531,\"start\":102523},{\"end\":102546,\"start\":102541},{\"end\":102563,\"start\":102556},{\"end\":102578,\"start\":102571},{\"end\":102594,\"start\":102589},{\"end\":102616,\"start\":102606},{\"end\":102634,\"start\":102629},{\"end\":102978,\"start\":102972},{\"end\":102993,\"start\":102988},{\"end\":103290,\"start\":103283},{\"end\":103305,\"start\":103300},{\"end\":103321,\"start\":103313},{\"end\":103335,\"start\":103328},{\"end\":103629,\"start\":103624},{\"end\":103645,\"start\":103640},{\"end\":103663,\"start\":103656},{\"end\":103675,\"start\":103670},{\"end\":103690,\"start\":103683},{\"end\":103705,\"start\":103698},{\"end\":103716,\"start\":103712},{\"end\":104050,\"start\":104044},{\"end\":104065,\"start\":104059},{\"end\":104077,\"start\":104067},{\"end\":104434,\"start\":104422},{\"end\":104449,\"start\":104443},{\"end\":104463,\"start\":104457},{\"end\":104477,\"start\":104472},{\"end\":104485,\"start\":104479},{\"end\":104758,\"start\":104739},{\"end\":104776,\"start\":104767},{\"end\":104784,\"start\":104778},{\"end\":105055,\"start\":105036},{\"end\":105078,\"start\":105066},{\"end\":105096,\"start\":105087},{\"end\":105104,\"start\":105098},{\"end\":105415,\"start\":105411},{\"end\":105428,\"start\":105424},{\"end\":105444,\"start\":105437},{\"end\":105458,\"start\":105451},{\"end\":105473,\"start\":105466},{\"end\":105499,\"start\":105492},{\"end\":105847,\"start\":105844},{\"end\":106061,\"start\":106051},{\"end\":106076,\"start\":106069},{\"end\":106091,\"start\":106085},{\"end\":106104,\"start\":106100},{\"end\":106121,\"start\":106112},{\"end\":106140,\"start\":106132},{\"end\":106160,\"start\":106152},{\"end\":106695,\"start\":106688},{\"end\":106706,\"start\":106703},{\"end\":106715,\"start\":106713},{\"end\":106729,\"start\":106723},{\"end\":106745,\"start\":106738},{\"end\":106756,\"start\":106754},{\"end\":106777,\"start\":106768},{\"end\":106997,\"start\":106993},{\"end\":107009,\"start\":107005},{\"end\":107021,\"start\":107019},{\"end\":107039,\"start\":107030},{\"end\":107049,\"start\":107046},{\"end\":107063,\"start\":107055},{\"end\":107083,\"start\":107072},{\"end\":107430,\"start\":107421},{\"end\":107446,\"start\":107439},{\"end\":107730,\"start\":107721},{\"end\":107741,\"start\":107734},{\"end\":107752,\"start\":107748},{\"end\":107762,\"start\":107754},{\"end\":108288,\"start\":108279},{\"end\":108299,\"start\":108294},{\"end\":108313,\"start\":108306},{\"end\":108328,\"start\":108320},{\"end\":108798,\"start\":108792},{\"end\":108813,\"start\":108808},{\"end\":108822,\"start\":108817},{\"end\":108839,\"start\":108830},{\"end\":108848,\"start\":108841},{\"end\":109207,\"start\":109197},{\"end\":109234,\"start\":109217},{\"end\":109254,\"start\":109244},{\"end\":109493,\"start\":109477},{\"end\":109684,\"start\":109681},{\"end\":109702,\"start\":109692},{\"end\":109715,\"start\":109710},{\"end\":109734,\"start\":109724},{\"end\":110056,\"start\":110048},{\"end\":110070,\"start\":110064},{\"end\":110262,\"start\":110259},{\"end\":110276,\"start\":110269},{\"end\":110293,\"start\":110287},{\"end\":110475,\"start\":110470},{\"end\":110491,\"start\":110483},{\"end\":110500,\"start\":110497},{\"end\":110511,\"start\":110506},{\"end\":110522,\"start\":110515},{\"end\":110532,\"start\":110526},{\"end\":110543,\"start\":110536},{\"end\":110553,\"start\":110547},{\"end\":110569,\"start\":110557},{\"end\":110578,\"start\":110571},{\"end\":110994,\"start\":110989},{\"end\":111010,\"start\":111002},{\"end\":111015,\"start\":111012},{\"end\":111350,\"start\":111343},{\"end\":111365,\"start\":111360},{\"end\":111379,\"start\":111372},{\"end\":111753,\"start\":111744},{\"end\":111771,\"start\":111761},{\"end\":111788,\"start\":111781},{\"end\":112052,\"start\":112043},{\"end\":112070,\"start\":112061},{\"end\":112085,\"start\":112078},{\"end\":112100,\"start\":112094},{\"end\":112117,\"start\":112110},{\"end\":112126,\"start\":112119},{\"end\":112493,\"start\":112486},{\"end\":112503,\"start\":112500},{\"end\":112516,\"start\":112510},{\"end\":112725,\"start\":112708},{\"end\":112735,\"start\":112729},{\"end\":112753,\"start\":112744},{\"end\":112778,\"start\":112772},{\"end\":112793,\"start\":112786},{\"end\":112802,\"start\":112795},{\"end\":113053,\"start\":113045},{\"end\":113067,\"start\":113062},{\"end\":113090,\"start\":113074},{\"end\":113103,\"start\":113097},{\"end\":113116,\"start\":113111},{\"end\":113419,\"start\":113400},{\"end\":113433,\"start\":113428},{\"end\":113443,\"start\":113435},{\"end\":113672,\"start\":113653},{\"end\":113686,\"start\":113681},{\"end\":113696,\"start\":113688},{\"end\":113957,\"start\":113952},{\"end\":114248,\"start\":114237},{\"end\":114267,\"start\":114258},{\"end\":114286,\"start\":114278},{\"end\":114303,\"start\":114300},{\"end\":114318,\"start\":114310},{\"end\":114332,\"start\":114326},{\"end\":114339,\"start\":114334},{\"end\":114636,\"start\":114627},{\"end\":114654,\"start\":114645},{\"end\":114670,\"start\":114663},{\"end\":114931,\"start\":114923},{\"end\":114947,\"start\":114940},{\"end\":114965,\"start\":114956},{\"end\":115225,\"start\":115220},{\"end\":115241,\"start\":115234},{\"end\":115252,\"start\":115249},{\"end\":115266,\"start\":115261},{\"end\":115287,\"start\":115278},{\"end\":115587,\"start\":115582},{\"end\":115601,\"start\":115594},{\"end\":115615,\"start\":115610},{\"end\":115636,\"start\":115627},{\"end\":115815,\"start\":115808},{\"end\":115830,\"start\":115824},{\"end\":115840,\"start\":115837},{\"end\":115857,\"start\":115851},{\"end\":115876,\"start\":115866},{\"end\":116152,\"start\":116148},{\"end\":116167,\"start\":116162},{\"end\":116177,\"start\":116169},{\"end\":116186,\"start\":116179},{\"end\":116197,\"start\":116190},{\"end\":116219,\"start\":116212},{\"end\":116229,\"start\":116221},{\"end\":116738,\"start\":116726},{\"end\":116750,\"start\":116746},{\"end\":116760,\"start\":116757},{\"end\":116937,\"start\":116929},{\"end\":116953,\"start\":116948},{\"end\":116974,\"start\":116965},{\"end\":117165,\"start\":117160},{\"end\":117185,\"start\":117177},{\"end\":117195,\"start\":117192},{\"end\":117384,\"start\":117379},{\"end\":117400,\"start\":117391},{\"end\":117414,\"start\":117409},{\"end\":117649,\"start\":117642},{\"end\":117668,\"start\":117657},{\"end\":117683,\"start\":117676},{\"end\":117957,\"start\":117953},{\"end\":117972,\"start\":117967},{\"end\":118260,\"start\":118243},{\"end\":118276,\"start\":118270},{\"end\":118284,\"start\":118278},{\"end\":118612,\"start\":118609},{\"end\":118626,\"start\":118622},{\"end\":118931,\"start\":118927},{\"end\":118943,\"start\":118938},{\"end\":118958,\"start\":118953},{\"end\":119329,\"start\":119325},{\"end\":119337,\"start\":119333},{\"end\":119351,\"start\":119344},{\"end\":119365,\"start\":119361},{\"end\":119376,\"start\":119367}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":93582,\"start\":93375},{\"attributes\":{\"id\":\"b1\"},\"end\":93659,\"start\":93584},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":222142338},\"end\":93902,\"start\":93661},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":247446734},\"end\":94344,\"start\":93904},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":231986558},\"end\":94746,\"start\":94346},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":59316669},\"end\":95038,\"start\":94748},{\"attributes\":{\"id\":\"b6\"},\"end\":95234,\"start\":95040},{\"attributes\":{\"id\":\"b7\"},\"end\":95888,\"start\":95236},{\"attributes\":{\"id\":\"b8\"},\"end\":96289,\"start\":95890},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":213004126},\"end\":96527,\"start\":96291},{\"attributes\":{\"id\":\"b10\"},\"end\":96809,\"start\":96529},{\"attributes\":{\"id\":\"b11\"},\"end\":96993,\"start\":96811},{\"attributes\":{\"id\":\"b12\"},\"end\":97195,\"start\":96995},{\"attributes\":{\"id\":\"b13\"},\"end\":97512,\"start\":97197},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14190268},\"end\":97884,\"start\":97514},{\"attributes\":{\"id\":\"b15\"},\"end\":98175,\"start\":97886},{\"attributes\":{\"id\":\"b16\"},\"end\":98408,\"start\":98177},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":248810837},\"end\":98867,\"start\":98410},{\"attributes\":{\"id\":\"b18\"},\"end\":99108,\"start\":98869},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":245353785},\"end\":99393,\"start\":99110},{\"attributes\":{\"id\":\"b20\"},\"end\":99597,\"start\":99395},{\"attributes\":{\"id\":\"b21\"},\"end\":99759,\"start\":99599},{\"attributes\":{\"id\":\"b22\"},\"end\":100125,\"start\":99761},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":236980318},\"end\":100415,\"start\":100127},{\"attributes\":{\"id\":\"b24\"},\"end\":100625,\"start\":100417},{\"attributes\":{\"id\":\"b25\"},\"end\":100884,\"start\":100627},{\"attributes\":{\"id\":\"b26\"},\"end\":101153,\"start\":100886},{\"attributes\":{\"id\":\"b27\"},\"end\":101508,\"start\":101155},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":22172746},\"end\":101790,\"start\":101510},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3342225},\"end\":102171,\"start\":101792},{\"attributes\":{\"id\":\"b30\"},\"end\":102516,\"start\":102173},{\"attributes\":{\"id\":\"b31\"},\"end\":102965,\"start\":102518},{\"attributes\":{\"id\":\"b32\"},\"end\":103198,\"start\":102967},{\"attributes\":{\"doi\":\"abs/1902.02384\",\"id\":\"b33\"},\"end\":103523,\"start\":103200},{\"attributes\":{\"id\":\"b34\"},\"end\":103939,\"start\":103525},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":143653771},\"end\":104348,\"start\":103941},{\"attributes\":{\"id\":\"b36\"},\"end\":104644,\"start\":104350},{\"attributes\":{\"id\":\"b37\"},\"end\":104946,\"start\":104646},{\"attributes\":{\"id\":\"b38\"},\"end\":105283,\"start\":104948},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":210154117},\"end\":105837,\"start\":105285},{\"attributes\":{\"id\":\"b40\"},\"end\":106042,\"start\":105839},{\"attributes\":{\"id\":\"b41\"},\"end\":106580,\"start\":106044},{\"attributes\":{\"id\":\"b42\"},\"end\":106985,\"start\":106582},{\"attributes\":{\"id\":\"b43\"},\"end\":107326,\"start\":106987},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":208077044},\"end\":107630,\"start\":107328},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":12533380},\"end\":108207,\"start\":107632},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":84839090},\"end\":108677,\"start\":108209},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":17699665},\"end\":109115,\"start\":108679},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":229722844},\"end\":109437,\"start\":109117},{\"attributes\":{\"doi\":\"abs/1606.03490\",\"id\":\"b49\"},\"end\":109596,\"start\":109439},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":235606403},\"end\":109964,\"start\":109598},{\"attributes\":{\"id\":\"b51\"},\"end\":110198,\"start\":109966},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":7715182},\"end\":110412,\"start\":110200},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":21889700},\"end\":110931,\"start\":110414},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":21889700},\"end\":111230,\"start\":110933},{\"attributes\":{\"id\":\"b55\"},\"end\":111665,\"start\":111232},{\"attributes\":{\"id\":\"b56\"},\"end\":111933,\"start\":111667},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":236772193},\"end\":112408,\"start\":111935},{\"attributes\":{\"id\":\"b58\"},\"end\":112647,\"start\":112410},{\"attributes\":{\"id\":\"b59\"},\"end\":112977,\"start\":112649},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":202712823},\"end\":113319,\"start\":112979},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":13029170},\"end\":113592,\"start\":113321},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":3366554},\"end\":113828,\"start\":113594},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":182656421},\"end\":114151,\"start\":113830},{\"attributes\":{\"id\":\"b64\"},\"end\":114546,\"start\":114153},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":3385018},\"end\":114820,\"start\":114548},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":1450294},\"end\":115136,\"start\":114822},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":211041098},\"end\":115502,\"start\":115138},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":235795402},\"end\":115799,\"start\":115504},{\"attributes\":{\"id\":\"b69\"},\"end\":116066,\"start\":115801},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":60837618},\"end\":116676,\"start\":116068},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":16747630},\"end\":116870,\"start\":116678},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":232068667},\"end\":117107,\"start\":116872},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":51734899},\"end\":117311,\"start\":117109},{\"attributes\":{\"id\":\"b74\"},\"end\":117540,\"start\":117313},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":3995299},\"end\":117924,\"start\":117542},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":9631227},\"end\":118140,\"start\":117926},{\"attributes\":{\"doi\":\"abs/1611.07443\",\"id\":\"b77\"},\"end\":118479,\"start\":118142},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":15696161},\"end\":118854,\"start\":118481},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":5933021},\"end\":119224,\"start\":118856},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":233834400},\"end\":119581,\"start\":119226}]", "bib_title": "[{\"end\":93740,\"start\":93661},{\"end\":93959,\"start\":93904},{\"end\":94432,\"start\":94346},{\"end\":94788,\"start\":94748},{\"end\":96348,\"start\":96291},{\"end\":97607,\"start\":97514},{\"end\":98506,\"start\":98410},{\"end\":99170,\"start\":99110},{\"end\":100205,\"start\":100127},{\"end\":101554,\"start\":101510},{\"end\":101843,\"start\":101792},{\"end\":104040,\"start\":103941},{\"end\":105397,\"start\":105285},{\"end\":107409,\"start\":107328},{\"end\":107709,\"start\":107632},{\"end\":108267,\"start\":108209},{\"end\":108781,\"start\":108679},{\"end\":109186,\"start\":109117},{\"end\":109674,\"start\":109598},{\"end\":110253,\"start\":110200},{\"end\":110466,\"start\":110414},{\"end\":110985,\"start\":110933},{\"end\":111333,\"start\":111232},{\"end\":112034,\"start\":111935},{\"end\":113036,\"start\":112979},{\"end\":113391,\"start\":113321},{\"end\":113644,\"start\":113594},{\"end\":113942,\"start\":113830},{\"end\":114618,\"start\":114548},{\"end\":114915,\"start\":114822},{\"end\":115212,\"start\":115138},{\"end\":115574,\"start\":115504},{\"end\":116144,\"start\":116068},{\"end\":116717,\"start\":116678},{\"end\":116920,\"start\":116872},{\"end\":117153,\"start\":117109},{\"end\":117633,\"start\":117542},{\"end\":117944,\"start\":117926},{\"end\":118599,\"start\":118481},{\"end\":118917,\"start\":118856},{\"end\":119314,\"start\":119226}]", "bib_author": "[{\"end\":93758,\"start\":93742},{\"end\":93770,\"start\":93758},{\"end\":93977,\"start\":93961},{\"end\":93991,\"start\":93977},{\"end\":94009,\"start\":93991},{\"end\":94029,\"start\":94009},{\"end\":94044,\"start\":94029},{\"end\":94060,\"start\":94044},{\"end\":94081,\"start\":94060},{\"end\":94451,\"start\":94434},{\"end\":94467,\"start\":94451},{\"end\":94483,\"start\":94467},{\"end\":94500,\"start\":94483},{\"end\":94511,\"start\":94500},{\"end\":94532,\"start\":94511},{\"end\":94806,\"start\":94790},{\"end\":94819,\"start\":94806},{\"end\":94838,\"start\":94819},{\"end\":94855,\"start\":94838},{\"end\":94869,\"start\":94855},{\"end\":94881,\"start\":94869},{\"end\":95061,\"start\":95040},{\"end\":95079,\"start\":95061},{\"end\":95400,\"start\":95373},{\"end\":95424,\"start\":95400},{\"end\":95440,\"start\":95424},{\"end\":95457,\"start\":95440},{\"end\":95470,\"start\":95457},{\"end\":95487,\"start\":95470},{\"end\":95504,\"start\":95487},{\"end\":95522,\"start\":95504},{\"end\":95537,\"start\":95522},{\"end\":95556,\"start\":95537},{\"end\":95997,\"start\":95977},{\"end\":96011,\"start\":95997},{\"end\":96027,\"start\":96011},{\"end\":96046,\"start\":96027},{\"end\":96061,\"start\":96046},{\"end\":96079,\"start\":96061},{\"end\":96364,\"start\":96350},{\"end\":96380,\"start\":96364},{\"end\":96392,\"start\":96380},{\"end\":96397,\"start\":96392},{\"end\":96625,\"start\":96610},{\"end\":96640,\"start\":96625},{\"end\":96657,\"start\":96640},{\"end\":96866,\"start\":96850},{\"end\":96879,\"start\":96866},{\"end\":96894,\"start\":96879},{\"end\":97066,\"start\":97054},{\"end\":97085,\"start\":97066},{\"end\":97212,\"start\":97197},{\"end\":97228,\"start\":97212},{\"end\":97244,\"start\":97228},{\"end\":97259,\"start\":97244},{\"end\":97277,\"start\":97259},{\"end\":97294,\"start\":97277},{\"end\":97623,\"start\":97609},{\"end\":97632,\"start\":97623},{\"end\":97649,\"start\":97632},{\"end\":97660,\"start\":97649},{\"end\":97672,\"start\":97660},{\"end\":97688,\"start\":97672},{\"end\":97957,\"start\":97943},{\"end\":97971,\"start\":97957},{\"end\":97987,\"start\":97971},{\"end\":98002,\"start\":97987},{\"end\":98019,\"start\":98002},{\"end\":98256,\"start\":98244},{\"end\":98272,\"start\":98256},{\"end\":98283,\"start\":98272},{\"end\":98521,\"start\":98508},{\"end\":98538,\"start\":98521},{\"end\":98554,\"start\":98538},{\"end\":98565,\"start\":98554},{\"end\":98581,\"start\":98565},{\"end\":98592,\"start\":98581},{\"end\":98947,\"start\":98930},{\"end\":98959,\"start\":98947},{\"end\":98978,\"start\":98959},{\"end\":99198,\"start\":99172},{\"end\":99206,\"start\":99198},{\"end\":99223,\"start\":99206},{\"end\":99234,\"start\":99223},{\"end\":99471,\"start\":99457},{\"end\":99480,\"start\":99471},{\"end\":99490,\"start\":99480},{\"end\":99644,\"start\":99632},{\"end\":99657,\"start\":99644},{\"end\":99906,\"start\":99891},{\"end\":99915,\"start\":99906},{\"end\":99933,\"start\":99915},{\"end\":99939,\"start\":99933},{\"end\":100220,\"start\":100207},{\"end\":100228,\"start\":100220},{\"end\":100245,\"start\":100228},{\"end\":100258,\"start\":100245},{\"end\":100463,\"start\":100457},{\"end\":100713,\"start\":100698},{\"end\":100734,\"start\":100713},{\"end\":100745,\"start\":100734},{\"end\":100958,\"start\":100941},{\"end\":101292,\"start\":101274},{\"end\":101312,\"start\":101292},{\"end\":101327,\"start\":101312},{\"end\":101574,\"start\":101556},{\"end\":101589,\"start\":101574},{\"end\":101600,\"start\":101589},{\"end\":101864,\"start\":101845},{\"end\":101879,\"start\":101864},{\"end\":101899,\"start\":101879},{\"end\":101914,\"start\":101899},{\"end\":101931,\"start\":101914},{\"end\":101947,\"start\":101931},{\"end\":102297,\"start\":102286},{\"end\":102313,\"start\":102297},{\"end\":102334,\"start\":102313},{\"end\":102533,\"start\":102518},{\"end\":102548,\"start\":102533},{\"end\":102565,\"start\":102548},{\"end\":102580,\"start\":102565},{\"end\":102596,\"start\":102580},{\"end\":102618,\"start\":102596},{\"end\":102636,\"start\":102618},{\"end\":102980,\"start\":102967},{\"end\":102995,\"start\":102980},{\"end\":103292,\"start\":103278},{\"end\":103307,\"start\":103292},{\"end\":103323,\"start\":103307},{\"end\":103337,\"start\":103323},{\"end\":103631,\"start\":103617},{\"end\":103647,\"start\":103631},{\"end\":103665,\"start\":103647},{\"end\":103677,\"start\":103665},{\"end\":103692,\"start\":103677},{\"end\":103707,\"start\":103692},{\"end\":103718,\"start\":103707},{\"end\":104052,\"start\":104042},{\"end\":104067,\"start\":104052},{\"end\":104079,\"start\":104067},{\"end\":104436,\"start\":104422},{\"end\":104451,\"start\":104436},{\"end\":104465,\"start\":104451},{\"end\":104479,\"start\":104465},{\"end\":104487,\"start\":104479},{\"end\":104760,\"start\":104730},{\"end\":104778,\"start\":104760},{\"end\":104786,\"start\":104778},{\"end\":105057,\"start\":105029},{\"end\":105080,\"start\":105057},{\"end\":105098,\"start\":105080},{\"end\":105106,\"start\":105098},{\"end\":105417,\"start\":105399},{\"end\":105430,\"start\":105417},{\"end\":105446,\"start\":105430},{\"end\":105460,\"start\":105446},{\"end\":105475,\"start\":105460},{\"end\":105501,\"start\":105475},{\"end\":105849,\"start\":105839},{\"end\":105855,\"start\":105849},{\"end\":106063,\"start\":106044},{\"end\":106078,\"start\":106063},{\"end\":106093,\"start\":106078},{\"end\":106106,\"start\":106093},{\"end\":106123,\"start\":106106},{\"end\":106142,\"start\":106123},{\"end\":106162,\"start\":106142},{\"end\":106697,\"start\":106677},{\"end\":106708,\"start\":106697},{\"end\":106717,\"start\":106708},{\"end\":106731,\"start\":106717},{\"end\":106747,\"start\":106731},{\"end\":106758,\"start\":106747},{\"end\":106779,\"start\":106758},{\"end\":106999,\"start\":106987},{\"end\":107011,\"start\":106999},{\"end\":107023,\"start\":107011},{\"end\":107041,\"start\":107023},{\"end\":107051,\"start\":107041},{\"end\":107065,\"start\":107051},{\"end\":107085,\"start\":107065},{\"end\":107432,\"start\":107411},{\"end\":107448,\"start\":107432},{\"end\":107732,\"start\":107711},{\"end\":107743,\"start\":107732},{\"end\":107754,\"start\":107743},{\"end\":107764,\"start\":107754},{\"end\":108290,\"start\":108269},{\"end\":108301,\"start\":108290},{\"end\":108315,\"start\":108301},{\"end\":108330,\"start\":108315},{\"end\":108800,\"start\":108783},{\"end\":108815,\"start\":108800},{\"end\":108824,\"start\":108815},{\"end\":108841,\"start\":108824},{\"end\":108850,\"start\":108841},{\"end\":109209,\"start\":109188},{\"end\":109236,\"start\":109209},{\"end\":109256,\"start\":109236},{\"end\":109495,\"start\":109477},{\"end\":109686,\"start\":109676},{\"end\":109704,\"start\":109686},{\"end\":109717,\"start\":109704},{\"end\":109736,\"start\":109717},{\"end\":110058,\"start\":110041},{\"end\":110072,\"start\":110058},{\"end\":110264,\"start\":110255},{\"end\":110278,\"start\":110264},{\"end\":110295,\"start\":110278},{\"end\":110477,\"start\":110468},{\"end\":110493,\"start\":110477},{\"end\":110502,\"start\":110493},{\"end\":110513,\"start\":110502},{\"end\":110524,\"start\":110513},{\"end\":110534,\"start\":110524},{\"end\":110545,\"start\":110534},{\"end\":110555,\"start\":110545},{\"end\":110571,\"start\":110555},{\"end\":110580,\"start\":110571},{\"end\":110996,\"start\":110987},{\"end\":111012,\"start\":110996},{\"end\":111017,\"start\":111012},{\"end\":111352,\"start\":111335},{\"end\":111367,\"start\":111352},{\"end\":111381,\"start\":111367},{\"end\":111755,\"start\":111737},{\"end\":111773,\"start\":111755},{\"end\":111790,\"start\":111773},{\"end\":112054,\"start\":112036},{\"end\":112072,\"start\":112054},{\"end\":112087,\"start\":112072},{\"end\":112102,\"start\":112087},{\"end\":112119,\"start\":112102},{\"end\":112128,\"start\":112119},{\"end\":112495,\"start\":112479},{\"end\":112505,\"start\":112495},{\"end\":112518,\"start\":112505},{\"end\":112727,\"start\":112700},{\"end\":112737,\"start\":112727},{\"end\":112755,\"start\":112737},{\"end\":112780,\"start\":112755},{\"end\":112795,\"start\":112780},{\"end\":112804,\"start\":112795},{\"end\":113055,\"start\":113038},{\"end\":113069,\"start\":113055},{\"end\":113092,\"start\":113069},{\"end\":113105,\"start\":113092},{\"end\":113118,\"start\":113105},{\"end\":113421,\"start\":113393},{\"end\":113435,\"start\":113421},{\"end\":113445,\"start\":113435},{\"end\":113674,\"start\":113646},{\"end\":113688,\"start\":113674},{\"end\":113698,\"start\":113688},{\"end\":113959,\"start\":113944},{\"end\":114250,\"start\":114235},{\"end\":114269,\"start\":114250},{\"end\":114288,\"start\":114269},{\"end\":114305,\"start\":114288},{\"end\":114320,\"start\":114305},{\"end\":114334,\"start\":114320},{\"end\":114341,\"start\":114334},{\"end\":114638,\"start\":114620},{\"end\":114656,\"start\":114638},{\"end\":114672,\"start\":114656},{\"end\":114933,\"start\":114917},{\"end\":114949,\"start\":114933},{\"end\":114967,\"start\":114949},{\"end\":115227,\"start\":115214},{\"end\":115243,\"start\":115227},{\"end\":115254,\"start\":115243},{\"end\":115268,\"start\":115254},{\"end\":115289,\"start\":115268},{\"end\":115589,\"start\":115576},{\"end\":115603,\"start\":115589},{\"end\":115617,\"start\":115603},{\"end\":115638,\"start\":115617},{\"end\":115817,\"start\":115801},{\"end\":115832,\"start\":115817},{\"end\":115842,\"start\":115832},{\"end\":115859,\"start\":115842},{\"end\":115878,\"start\":115859},{\"end\":116154,\"start\":116146},{\"end\":116169,\"start\":116154},{\"end\":116179,\"start\":116169},{\"end\":116188,\"start\":116179},{\"end\":116199,\"start\":116188},{\"end\":116221,\"start\":116199},{\"end\":116231,\"start\":116221},{\"end\":116740,\"start\":116719},{\"end\":116752,\"start\":116740},{\"end\":116762,\"start\":116752},{\"end\":116939,\"start\":116922},{\"end\":116955,\"start\":116939},{\"end\":116976,\"start\":116955},{\"end\":117167,\"start\":117155},{\"end\":117187,\"start\":117167},{\"end\":117197,\"start\":117187},{\"end\":117386,\"start\":117373},{\"end\":117402,\"start\":117386},{\"end\":117416,\"start\":117402},{\"end\":117651,\"start\":117635},{\"end\":117670,\"start\":117651},{\"end\":117685,\"start\":117670},{\"end\":117959,\"start\":117946},{\"end\":117974,\"start\":117959},{\"end\":118262,\"start\":118237},{\"end\":118278,\"start\":118262},{\"end\":118286,\"start\":118278},{\"end\":118614,\"start\":118601},{\"end\":118628,\"start\":118614},{\"end\":118933,\"start\":118919},{\"end\":118945,\"start\":118933},{\"end\":118960,\"start\":118945},{\"end\":119331,\"start\":119316},{\"end\":119339,\"start\":119331},{\"end\":119353,\"start\":119339},{\"end\":119367,\"start\":119353},{\"end\":119378,\"start\":119367}]", "bib_venue": "[{\"end\":106308,\"start\":106236},{\"end\":107947,\"start\":107864},{\"end\":108457,\"start\":108402},{\"end\":111462,\"start\":111430},{\"end\":116368,\"start\":116308},{\"end\":93414,\"start\":93375},{\"end\":93588,\"start\":93584},{\"end\":93774,\"start\":93770},{\"end\":94116,\"start\":94081},{\"end\":94536,\"start\":94532},{\"end\":94885,\"start\":94881},{\"end\":95131,\"start\":95079},{\"end\":95371,\"start\":95236},{\"end\":95975,\"start\":95890},{\"end\":96401,\"start\":96397},{\"end\":96608,\"start\":96529},{\"end\":96848,\"start\":96811},{\"end\":97052,\"start\":96995},{\"end\":97348,\"start\":97294},{\"end\":97691,\"start\":97688},{\"end\":97941,\"start\":97886},{\"end\":98242,\"start\":98177},{\"end\":98613,\"start\":98592},{\"end\":98928,\"start\":98869},{\"end\":99244,\"start\":99234},{\"end\":99455,\"start\":99395},{\"end\":99630,\"start\":99599},{\"end\":99889,\"start\":99761},{\"end\":100261,\"start\":100258},{\"end\":100455,\"start\":100417},{\"end\":100696,\"start\":100627},{\"end\":100939,\"start\":100886},{\"end\":101272,\"start\":101155},{\"end\":101642,\"start\":101600},{\"end\":101975,\"start\":101947},{\"end\":102284,\"start\":102173},{\"end\":102735,\"start\":102636},{\"end\":103078,\"start\":102995},{\"end\":103276,\"start\":103200},{\"end\":103615,\"start\":103525},{\"end\":104122,\"start\":104079},{\"end\":104420,\"start\":104350},{\"end\":104728,\"start\":104646},{\"end\":105027,\"start\":104948},{\"end\":105553,\"start\":105501},{\"end\":105937,\"start\":105855},{\"end\":106234,\"start\":106162},{\"end\":106675,\"start\":106582},{\"end\":107150,\"start\":107085},{\"end\":107471,\"start\":107448},{\"end\":107862,\"start\":107764},{\"end\":108400,\"start\":108330},{\"end\":108882,\"start\":108850},{\"end\":109263,\"start\":109256},{\"end\":109475,\"start\":109439},{\"end\":109773,\"start\":109736},{\"end\":110039,\"start\":109966},{\"end\":110298,\"start\":110295},{\"end\":110624,\"start\":110580},{\"end\":111066,\"start\":111017},{\"end\":111428,\"start\":111381},{\"end\":111735,\"start\":111667},{\"end\":112164,\"start\":112128},{\"end\":112477,\"start\":112410},{\"end\":112698,\"start\":112649},{\"end\":113141,\"start\":113118},{\"end\":113448,\"start\":113445},{\"end\":113702,\"start\":113698},{\"end\":113986,\"start\":113959},{\"end\":114233,\"start\":114153},{\"end\":114676,\"start\":114672},{\"end\":114971,\"start\":114967},{\"end\":115312,\"start\":115289},{\"end\":115645,\"start\":115638},{\"end\":115927,\"start\":115878},{\"end\":116306,\"start\":116231},{\"end\":116766,\"start\":116762},{\"end\":116983,\"start\":116976},{\"end\":117202,\"start\":117197},{\"end\":117371,\"start\":117313},{\"end\":117720,\"start\":117685},{\"end\":118012,\"start\":117974},{\"end\":118235,\"start\":118142},{\"end\":118660,\"start\":118628},{\"end\":119033,\"start\":118960},{\"end\":119389,\"start\":119378}]"}}}, "year": 2023, "month": 12, "day": 17}