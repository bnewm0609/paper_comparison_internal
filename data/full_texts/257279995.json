{"id": 257279995, "updated": "2023-10-05 03:51:44.284", "metadata": {"title": "An Improved Classical Singular Value Transformation for Quantum Machine Learning", "authors": "[{\"first\":\"Ainesh\",\"last\":\"Bakshi\",\"middle\":[]},{\"first\":\"Ewin\",\"last\":\"Tang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We study quantum speedups in quantum machine learning (QML) by analyzing the quantum singular value transformation (QSVT) framework. QSVT, introduced by [GSLW, STOC'19, arXiv:1806.01838], unifies all major types of quantum speedup; in particular, a wide variety of QML proposals are applications of QSVT on low-rank classical data. We challenge these proposals by providing a classical algorithm that matches the performance of QSVT in this regime up to a small polynomial overhead. We show that, given a matrix $A \\in \\mathbb{C}^{m\\times n}$, a vector $b \\in \\mathbb{C}^{n}$, a bounded degree-$d$ polynomial $p$, and linear-time pre-processing, we can output a description of a vector $v$ such that $\\|v - p(A) b\\| \\leq \\varepsilon\\|b\\|$ in $\\widetilde{\\mathcal{O}}(d^{11} \\|A\\|_{\\mathrm{F}}^4 / (\\varepsilon^2 \\|A\\|^4 ))$ time. This improves upon the best known classical algorithm [CGLLTW, STOC'20, arXiv:1910.06151], which requires $\\widetilde{\\mathcal{O}}(d^{22} \\|A\\|_{\\mathrm{F}}^6 /(\\varepsilon^6 \\|A\\|^6 ) )$ time, and narrows the gap with QSVT, which, after linear-time pre-processing to load input into a quantum-accessible memory, can estimate the magnitude of an entry $p(A)b$ to $\\varepsilon\\|b\\|$ error in $\\widetilde{\\mathcal{O}}(d\\|A\\|_{\\mathrm{F}}/(\\varepsilon \\|A\\|))$ time. Our key insight is to combine the Clenshaw recurrence, an iterative method for computing matrix polynomials, with sketching techniques to simulate QSVT classically. We introduce several new classical techniques in this work, including (a) a non-oblivious matrix sketch for approximately preserving bi-linear forms, (b) a new stability analysis for the Clenshaw recurrence, and (c) a new technique to bound arithmetic progressions of the coefficients appearing in the Chebyshev series expansion of bounded functions, each of which may be of independent interest.", "fields_of_study": "[\"Physics\",\"Computer Science\"]", "external_ids": {"arxiv": "2303.01492", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2303-01492", "doi": "10.48550/arxiv.2303.01492"}}, "content": {"source": {"pdf_hash": "3ad7f23230b2d8b2197f3935e1eaf01ff9414e5e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.01492v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ffe41e20cdfff0331f80fc85bad32d6c1301b962", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3ad7f23230b2d8b2197f3935e1eaf01ff9414e5e.txt", "contents": "\nAn Improved Classical Singular Value Transformation for Quantum Machine Learning\n3 Aug 2023\n\nAinesh Bakshi ainesh@mit.edu \nMIT\nUniversity of Washington\n\n\nEwin Tang ewint@cs.washington.edu \nMIT\nUniversity of Washington\n\n\nAn Improved Classical Singular Value Transformation for Quantum Machine Learning\n3 Aug 2023\nThe field of quantum machine learning (QML) produces many proposals for attaining quantum speedups for tasks in machine learning and data analysis. Such speedups can only manifest if classical algorithms for these tasks perform significantly slower than quantum ones. We study quantum-classical gaps in QML through the quantum singular value transformation (QSVT) framework. QSVT, introduced by Gily\u00e9n, Su, Low and Wiebe [GSLW19], unifies all major types of quantum speedup [MRTC21]; in particular, a wide variety of QML proposals are applications of QSVT on low-rank classical data. We challenge these proposals by providing a classical algorithm that matches the performance of QSVT in this regime up to a small polynomial overhead.We show that, given a matrix A \u2208 C m\u00d7n , a vector b \u2208 C n , a bounded degree-d polynomial p, and linear-time pre-processing, we can output a description of a vector v such that \u2225v \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225b\u2225 in O(d 11 \u2225A\u2225 4 F /(\u03b5 2 \u2225A\u2225 4 )) time. This improves upon the best known classical algorithm [CGLLTW22], which requires O(d 22 \u2225A\u2225 6 F /(\u03b5 6 \u2225A\u2225 6 )) time, and narrows the gap with QSVT, which, after linear-time pre-processing to load input into a quantum-accessible memory, can estimate the magnitude of an entry p(A)b to \u03b5\u2225b\u2225 error in O(d\u2225A\u2225 F /(\u03b5\u2225A\u2225)) time. Instantiating our algorithm with different polynomials, we improve on prior classical algorithms for quantum-inspired regression [CGLLTW22; GST22], recommendation systems [Tan19; CGLLTW22], and Hamiltonian simulation [CGLLTW22].Our key insight is to combine the Clenshaw recurrence, an iterative method for computing matrix polynomials, with sketching techniques to simulate QSVT classically. We introduce several new classical techniques in this work, including (a) a non-oblivious matrix sketch for approximately preserving bi-linear forms, (b) a new stability analysis for the Clenshaw recurrence, and (c) a new technique to bound arithmetic progressions of the coefficients appearing in the Chebyshev series expansion of bounded functions, each of which may be of independent interest.\n\nIntroduction\n\nQuantum machine learning (QML) has rapidly developed into an active field of study with numerous proposals for speeding up machine learning tasks with quantum computers [DW20;Cil+18]. These proposals include quantum algorithms for basic tasks in machine learning, including regression [HHL09], perceptron learning [KWS16], support vector machines [RML14], recommendation systems [KP17], and semi-definite programming [BKLLSW19]. A central goal of QML is to demonstrate a problem on which quantum computers obtain a substantial practical speedup over classical computers. A successful resolution of this goal would provide compelling motivation to invest more resources into developing scalable quantum computers (i.e. be a killer app for quantum computing).\n\nThe quantum singular value transformation (QSVT) framework [LC17; CGJ19; GSLW19] uses ideas from signal processing to unify algorithm design for quantum linear algebra and, by extension, QML. This framework has been called a \"grand unification\" of quantum algorithms as it captures all major classes of quantum advantage [MRTC21]. With respect to QML, QSVT captures essentially all known techniques in quantum linear algebra, so it will be the focus of our investigation. QSVT generalizes the simple observation that, for a quantum state |b\u27e9 = \u2211 i b i |i\u27e9 \u2208 C n encoding the vector b into its amplitudes, a quantum gate corresponds to applying a unitary matrix U \u2208 C n\u00d7n to the vector, |b\u27e9 \u2192 |Ub\u27e9. By including the non-unitary operation of measurement, we can consider quantum circuits that perform |b\u27e9 \u2192 |Ab\u27e9 for general (bounded) matrices A \u2208 C m\u00d7n ; such circuits are called block-encodings. The fundamental result of QSVT is that, given a block-encoding of A and a bounded, even or odd, degree-d polynomial p, we can form a block-encoding of p(A) with an overhead of d. 1 This new quantum circuit can then be applied to a quantum state |b\u27e9 to get |p(A)b\u27e9 with probability \u2225p(A)b\u2225 2 . In this way, quantum linear algebra algorithms can apply generic functions to the singular values of a matrix efficiently, provided that we have an efficient block-encoding and the function is smooth enough to be approximated well by a low-degree polynomial.\n\nThough QSVT is an immensely powerful tool for quantum linear algebra, it requires having an efficient block-encoding for the input matrix A. These efficient block-encodings do not exist in general, but they do exist for two broad classes of matrices, assuming appropriately strong forms of coherent access: matrices with low sparsity [GSLW19, Lemma 48] (a typical setting for quantum simulation) and matrices with low stable rank [GSLW19, Lemma 50] (a typical setting for quantum machine learning). We treat the latter case; specifically, assuming the existence of quantum random access memory (QRAM), a piece of hardware that supports queries to its memory in superposition [GLM08], 2 we can process a matrix A given as a list of its entries A i,j into a data structure in linear time such that preparation of a block-encoding of A/\u2225A\u2225 F is efficient (e.g.\n\nusing O(log(mn)) queries to the QRAM). Here, \u2225A\u2225 F = \u2211 i,j A i,j 2 denotes the Frobenius norm of A. This type of block-encoding is the one commonly used for quantum linear algebra algorithms on classical data [ KLLP19;CW22], since it works for arbitrary matrices and vectors, paying only a \u2225A\u2225 F /\u2225A\u2225 factor in sub-normalization. 1 When A is asymmetric, we can interpret QSVT as applying the matrix function that applies p to each singular value of A (Definition 4.1). 2 Having quantum random access memory is an assumption similar to the classical random access memory assumption in algorithm design, that querying any piece of memory has cost only logarithmic (or constant) in input size. It's unclear whether we can attain the effectively constant cost of modern RAM when we want to query memory in superposition, making this assumption somewhat speculative [JR23]. In this paper, we will be fine giving quantum algorithms QRAM assumptions, and these matters will not affect the classical algorithms we present.\n\nA natural question then arises: how well can classical computers simulate QSVT? Sparsitybased QSVT supports universal quantum computation, and therefore cannot be simulated classically in polynomial time unless BPP=BQP. On the other hand, prior work by Chia, Gily\u00e9n, Li, Lin, Tang and Wang [CGLLTW22, Section 6.1] gives a classical algorithm that outputs a vector v such that \u2225p(A)b \u2212 v\u2225 \u2a7d \u03b5\u2225b\u2225 in O(d 22 \u2225A\u2225 6 F /(\u03b5 6 \u2225A\u2225 6 )) time, after a linear-time preprocessing phase. More generally, all of the \"quantum-inspired\" algorithms generated through this framework have large polynomial slowdowns, as they require computing a singular value decomposition of a matrix with \u2126(( \u2225A\u2225 F \u2225A\u2225\u03b5 ) 2 ) rows and columns, immediately incurring a powersix dependence in \u03b5 and the Frobenius norm of A. In fact, without assuming that the input matrix is strictly low-rank, all prior quantum-inspired algorithms incur a 1/\u03b5 6 dependence, except for one for linear regression which incurs an 1/\u03b5 4 [GST22]. This gives the perception that a large polynomial running time (significantly larger than quartic) to simulate QSVT may be inherent. In fact, this classical hardness has been conjectured [ KLLP19;KP22]. Such a conclusion would be significant, as this suggests a potential regime for a practical speedup for many problems in QML. Therefore, the central question we address in this work is as follows:\n\nHow large is the quantum-classical gap for singular value transformation and the machine learning problems captured by it?\n\n\nResults\n\nOur main result addresses the aforementioned question by providing an improved classical algorithm for QSVT that avoids computing a full singular value decomposition. As a consequence, we also obtain better bounds on the quantum-classical gap for regression [CGLLTW20;GST22], recommendation systems [Tan19; CGLLTW22], and Hamiltonian simulation [CGLLTW22] with improved dependence in each relevant parameter. Additionally, we improve over prior quantum-inspired algorithms with atypical guarantees in various parameter regimes [CCHLW22;SM21]. We begin by stating our result for simulating QSVT on inputs with low stable rank. 3 Theorem 1.1 (Classical Singular Value Transformation, informal version of Theorem 9.1). Given a Hermitian A \u2208 C n\u00d7n and b \u2208 C n , and an accuracy parameter 0 < \u03b5 < 1, after O(nnz(A) + nnz(b)) pre-processing time to create a data structure, 4 for a degree-d polynomial p such that |p(x)| \u2a7d 1 for\n\nx \u2208 [\u2212\u2225A\u2225, \u2225A\u2225], we can output a description of a vector y \u2208 C n such that with probability at least 0.9, \u2225y \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225b\u2225. \u2225A\u2225 4 \u03b5 2 \u2225y\u2225 2 time. 5 The assumptions that A is square and Hermitian are not needed; for other A, the definition of p(A) needs to be adjusted appropriately and restricted to p either even or odd. For the discussion that follows, we assume \u2225A\u2225 \u2a7d 1 and that p(x) is bounded in the interval [\u22121, 1], which allows us to analyze p(x) in terms of its Chebyshev coefficients, without rescaling.\n\non how smooth f is [Tre19].) In particular, they achieve a running time of O(\u2225A\u2225 6 F \u03ba 20 (d 2 + \u03ba) 6 /\u03b5 6 ) to apply a degree-d polynomial. Again, we improve in all parameters over this work: degree of the polynomial p, Frobenius norm of A, and accuracy parameter \u03b5. We also do not incur condition number dependence.\n\nFinally, the work of Gharibian and Le Gall [GL22] considers QSVT when the input matrices are sparse, which is the relevant regime for quantum chemistry and other problems in many-body systems, where the matrix is a local Hamiltonian. See also Van den Nest's work which uses similar techniques to simulate restricted classes of quantum circuits [Van11]. Because they work in the sparse setting, which becomes BQP-hard with high degree, their algorithm is exponential in degree (but independent of stable rank). We deal with the setting where the input A has low stable rank, where general QSVT can be simulated efficiently in polynomial time: we want a polynomial dependence on degree and stable rank.\n\n\nApplications to Dequantizing QML\n\nNow, we describe the implications of Theorem 1.1 to specific problems in QML. We obtain faster quantum-inspired algorithms for linear regression, recommendation systems, and Hamiltonian simulation.\n\nWe begin with quantum recommendation systems, where the goal is to output a sample from a row of a low-rank approximation of the input vector A. The original quantum algorithm has running time of O(\u2225A\u2225 F /\u03c3) to output |[thresh \u03c3 (A)] i, * \u27e9, where thresh \u03c3 is a polynomial approximating a threshold function-close to identity for values \u2a7e \u03c3, close to zero for values \u2a7d 5\u03c3/6, and bounded in between [KP17;CGJ19]. We then obtain the following corollary: Corollary 1.2 (Dequantized recommendation systems, informal version of Corollary 10.2). Given a matrix A \u2208 C m\u00d7n such that \u2225A\u2225 = 1, a sufficiently small accuracy parameter \u03b5 > 0, and an i \u2208 [n], we can produce a data structure in O(nnz(A)) time such that, we can compute a description of a vector y such that with probability at least 0.9, \u2225y \u2212 [thresh \u03c3 (A)] i, * \u2225 \u2a7d \u03b5\u2225A i, * \u2225 in O(\u2225A\u2225 4 F / \u03c3 11 \u03b5 2 ) time. From this description we can output a sample from y in O \u2225A\u2225 4 F \u2225A i, * \u2225 2 \u03c3 8 \u03b5 2 \u2225y\u2225 2 time. [CGLLTW22]). [CGLLTW22,Corollary 6.7] achieves a running time of O(\u2225A\u2225 6 F /(\u03c3 16 \u03b5 6 )). We improve upon it in every parameter, including error \u03b5, the threshold \u03c3, and the Frobenius norm \u2225A\u2225 F . We defer comparison to the work of Chepurko, Clarkson, Horesh, Lin, and Woodruff [CCHLW22] to the related work section, as it achieves different guarantees from QSVT. To summarize the discussion there, if we attempt to translate the guarantees of [CCHLW22] to this setting, we find that our running times are better, under mild assumptions on the size of \u03b5.\n\n\nRemark 1.3 (Comparison to\n\nNext, we state the dequantization result we obtain for regression. Quantum algorithms can produce a state \u03b5-close to |inv \u03ba (A)b\u27e9 in O(\u03ba\u2225A\u2225 F log 1 \u03b5 ) time, where inv \u03ba is a polynomial close to 1/x on [1/\u03ba, 1] and and smoothly thresholds away all singular values below 1/\u03ba [WZP18; CGJ19]. Corollary 1.4 (Dequantized regression, informal version of Corollary 10.5). Given A \u2208 C m\u00d7n and b \u2208 C n such that \u2225A\u2225 = 1, \u2225b\u2225 \u2a7d 1, a singular value threshold 0 < 1/\u03ba < 1, and a sufficiently small accuracy parameter \u03b5 > 0. Then after O(nnz(A) + nnz(b)) pre-processing time, we can compute a description of a vector y \u2208 C n such that, with probability \u2a7e 0.9, \u2225y \u2212 inv \u03ba (A)b\u2225 \u2a7d \u03b5\u2225b\u2225/\u03ba. The algorithm runs in O(\u03ba 11 \u2225A\u2225 4 F /\u03b5 2 ) time. From this description we can output a sample from y in O \u03ba 10 \u2225A\u2225 4 F \u2225b\u2225 2 \u03b5 2 \u2225y\u2225 2 time. F \u03ba 22 /\u03b5 6 ) to achieve an error bound of \u03b5\u2225b\u2225/\u2225A\u2225 (\u03c3 in that setting is our 1/\u03ba). Converting our result there increases our runtime by \u03ba 2 , but we still improve over this result in all parameters.\n\nOther quantum-inspired algorithms give improved results under fairly strong additional assumptions. When A has no non-zero singular values larger than \u03c3, then [GST22] achieves a running time of O(\u2225A\u2225 6 F /(\u03c3 12 \u03b5 4 )), which Shao and Montanaro [SM21] improve further to O(\u2225A\u2225 6 F /(\u03c3 8 \u03b5 2 )) when b is in the image of A. We improve over the former in all parameters, and for the latter we obtain a better \u2225A\u2225 F dependence at the cost of a worse \u03c3 dependence. Both of these provide the stronger guarantee that the output vector is close up to relative error \u03b5\u2225A + b\u2225, though [GST22] requires factors of \u2225A + \u2225\u2225b\u2225/\u2225A + b\u2225 to compensate (which we elided in the above expression).\n\nFinally, we state the dequantization result we obtain for Hamiltonian simulation. Corollary 1.6 (Dequantized Hamiltonian simulation, informal version of Corollary 10.8). Given a Hermitian Hamiltonian H \u2208 C n\u00d7n with \u2225H\u2225 \u2a7d 1, a vector b \u2208 C n , and a sufficiently small accuracy parameter \u03b5 > 0, after O(nnz(H) + nnz(b)) pre-processing time, we can output a description of a vector v such that, with probability \u2a7e 0.9, v \u2212 e iHt b \u2a7d \u03b5\u2225b\u2225 with running time O(t 11 \u2225H\u2225 4 F /\u03b5 2 ). From this description we can output a sample from v in O\nt 8 \u2225H\u2225 4 F \u2225b\u2225 2 \u03b5 2 \u2225v\u2225 2 time.\nRemark 1.7 (Comparison with [CGLLTW22]). The only prior work [CGLLTW22] we are aware of in the low-rank regime obtains a running time O(t 16 \u2225H\u2225 6 F /\u03b5 6 ), and we improve upon it in every parameter.\n\n\nTechnical Overview\n\nPrior work [CGLLTW22] d 22 \u2225A\u2225 6 F /\u03b5 6 Using the polynomial structure d 17 \u2225A\u2225 4 F /\u03b5 4 Tightening Clenshaw's stability analysis d 13 \u2225A\u2225 4 F /\u03b5 4 Sparsifying matrices d 11 \u2225A\u2225 4 F /\u03b5 2 Figure 1: A list of our technical contributions, along with the improvements they each make to the running time of the final algorithm, ignoring log factors.\n\nIn this section, we describe our classical framework for simulating QSVT and provide an overview of our key new contributions. For ease of exposition, we assume \u2225A\u2225 = 1 and consider when p is odd. We have three conceptual contributions. First, we use an iterative method (the Clenshaw recurrence) instead of a pure sketching algorithm, which improves the running time to O(d 17 \u2225A\u2225 4 F /\u03b5 4 ). This corresponds to d iterations of matrix-vector products of size \u2225A\u2225 2 F /(\u03b5/d 4 ) 2 by \u2225A\u2225 2 F /(\u03b5/d 4 ) 2 . Second, we develop new insights into the stability of the Clenshaw recurrence and arithmetic progressions of Chebyshev coefficients to improve the size from \u2225A\u2225 2 F /(\u03b5/d 4 ) 2 to an \u2225A\u2225 2 F /(\u03b5/(d 3 log 2 (d))) 2 . Third, we give a subtle argument that we can sparsify these matrices, improving the \u03b5 4 to an \u03b5 2 . Together, 8 this gives the final running 8 It is natural to wonder here why the complexity is not something like d\u2225A\u2225 4 F /(\u03b5/(d 3 log 2 (d))) 2 = d 7 log 2 (d)\u2225A\u2225 4 F /\u03b5 2 . Such a running time is conceivable, but our analysis essentially replaces two factors of 1/\u03b5 with time of O(d 11 log 4 (d)\u2225A\u2225 4 F /\u03b5 2 ). We now walk through these steps in more detail.\n\nComputing matrix polynomials through sketches and iterative algorithms. Recall our goal of simulating QSVT: given a matrix A \u2208 C m\u00d7n , a vector b \u2208 C n , and a polynomial p : [\u22121, 1] \u2192 R, compute a description of a vector y such that \u2225y \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225, where \u2225p\u2225 sup := max x\u2208[\u22121,1] |p(x)|. Specifically, we aim for our algorithm to run in poly(\u2225A\u2225 F , 1 \u03b5 , d) time after O(nnz(A) + nnz(b)) pre-processing, and our description to be some sparse vector x such that y = Ax, since this allows us to simulate some tasks that the quantum algorithm can do with copies of |y\u27e9, like estimating overlaps and performing measurements in the computational basis.\n\nWe require the running time of the algorithm to be independent of input dimension (after the pre-processing) and therefore are compelled to create sketches of A and b and work with these sketches. We note that prior work [Tan19; Tan21; CGLLTW22] stops here, and directly computes a SVD of the sketches, and applies the relevant polynomial p to the singular values of the sketch. As noted in the previous section, this approach loses large polynomial factors in the relevant parameters.\n\nOur main conceptual insight is to run iterative algorithms on the resulting sketches of A and b in order to approximate matrix polynomials. In the canonical numerical linear algebra regime (working with matrices directly, instead of sketches), there are two standard methods to achieve this goal: (1) compute the polynomial explicitly through something like a Clenshaw recurrence [Cle55], or (2) use the Lanczos method [Lan50] to create a roughly-orthogonal basis for the Krylov subspace {b, Ab, A 2 b, . . .} and then apply the function exactly to the matrix in this basis, in which A is tridiagonal, implicitly using a polynomial approximation in the analysis. We note that in a world where we are allowed to pay O(nnz(A)) (or even O(m + n)) time per-iteration, we can simply run either of these algorithms and call it a day. The main challenge in the setting we consider is that each iterative step must run in time that is entirely dimension-independent.\n\nAlgorithm 1 (Singular value transformation for odd polynomials. Informal version of Algorithm 2).\n\nInput (pre-processing): A matrix A \u2208 C m\u00d7n , vector b \u2208 C n , and parameters 0 < \u03b5 < 1.\n\nAn odd degree 2d + 1 polynomial given as its Chebyshev coefficients a 2i+1 (so that\np(x) = \u2211 d i=0 a 2i+1 T 2i+1 (x)).\nPre-processing sketches:\nLet s, t = O d 6 \u2225A\u2225 2 F \u03b5 2 .\nP1. Let S \u2208 C n\u00d7s be a sampling matrix that samples s columns of A such that the j-th column is sampled with probability 1 2 \u2225A * ,j \u2225 2 /\u2225A\u2225 2 F + |b j | 2 /\u2225b\u2225 2 .\n\nP2. Let T \u2208 C t\u00d7m be a sampling matrix that samples t columns of AS such that the i-th column is sampled with probability \u2225[AS] i, * \u2225 2 /\u2225AS\u2225 2 F . P3. Compute TAS.\n\n\nClenshaw iteration:\nLet r = O d 10 \u2225A\u2225 4 F /\u03b5 2 . Let v d+1 = v d+2 = \u20d7 0 s . For k \u2208 [d, 0],\nfactors of 1/d 2 , so the sparsification only saves a factor of d 2 .\n\nI1. Let B (k) \u2208 C t\u00d7s be the estimator formed by sampling r entries of TAS such that\nentry (i, j) is sampled with probability |[TAS] i,j | 2 /\u2225TAS\u2225 2 F . Construct B (k) \u2020 from (TAS) \u2020 similarly. I2. Compute v k = 2(2B (k) \u2020 B (k) \u2212 I)v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b. Output: Output x = 1 2 S(v 0 \u2212 v 1 ) that satisfies \u2225Ax \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225.\nSketching down the Clenshaw recurrence. For reasons explained below, we use the Clenshaw recurrence instead of the Lanczos method. Given a degree-d polynomial p given in terms of its Chebyshev coefficients a \u2113 (i.e. p(\nx) = \u2211 d \u2113=0 a \u2113 T \u2113 (x), where T \u2113 (x)\nis the degree \u2113 Chebyshev polynomial) and a value x \u2208 [\u22121, 1], the Clenshaw recurrence computes p(x). We use a modified recurrence for technical reasons. Concretely, our recurrence for p odd (so that a \u2113 = 0 for \u2113 even), is the following.\nq (d\u22121)/2 = q (d+1)/2 = 0 q k = 2(2x 2 \u2212 1)q k+1 \u2212 q k+2 + 2a 2k+1 x p(x) = 1 2 (q 0 \u2212 q 1 )\nThe scalar recurrences we discuss lift to computing matrix polynomials in a natural way:\nu (d\u22121)/2 = u (d+1)/2 = 0 u k = 2(2AA \u2020 \u2212 I)u k+1 \u2212 u k+2 + 2a 2k+1 Ab p(A)b = 1 2 (u 0 \u2212 u 1 )\nEach iteration (to get u k from u k+1 and u k+2 ) can be performed in O(nnz(A)) arithmetic operations, so this can be used to compute p(A)b in O(d nnz(A)) operations. We would like to do this approximately in time independent of nnz(A) and n. We begin by sketching down our matrix and vector: we show that it suffices to maintain a sparse description of u k of the form u k = Av k where v k is sparse. In particular, we produce sketches S \u2208 C n\u00d7s and T \u2208 C t\u00d7m such that\n1. \u2225AS(AS) \u2020 \u2212 AA \u2020 \u2225 \u2a7d \u03b5, 2. \u2225ASS \u2020 b \u2212 Ab\u2225 \u2a7d \u03b5\u2225b\u2225, 3. \u2225TAS(TAS) \u2020 \u2212 AS(AS) \u2020 \u2225 \u2a7d \u03b5\nSketches that satisfy the above type of guarantees are called approximate matrix product (AMP) sketches, and are standard in the quantum-inspired algorithms literature [CGLLTW22]. In the linear-time pre-processing phase, we can produce these sketches of size s, t = O( \u2225A\u2225 2 F \u03b5 2 log 1 \u03b4 ), and then compute TAS. If the input is given in the quantum-inspired access model of oversampling and query access, this can even be done in time independent of dimension. All of these guarantees follow from Theorem 5.5, which shows \u2113 2 2 sampling gives an asymmetric approximate matrix product property (in operator norm). We do not need this generalization (prior \"symmetric\" results suffice), but we use it for convenience.\n\nUsing these guarantees we can sketch the iterates as follows:\nu k = 2(2AA \u2020 \u2212 I)u k+1 \u2212 u k+2 + 2a 2k+1 Ab = 4AA \u2020 Av k+1 \u2212 2Av k+1 \u2212 Av k+2 + 2a 2k+1 Ab \u2248 AS[4(TAS) \u2020 (TAS)v k+1 \u2212 2v k+1 \u2212 v k+2 + 2a 2k+1 S \u2020 b].\n(1) Therefore, we can interpret Clenshaw iteration as the recursion on the dimension-independent term v k \u2248 4(TAS) \u2020 (TAS)v k+1 \u2212 2v k+1 \u2212 v k+2 + 2a 2k+1 S \u2020 b, and then applying AS on the left to lift it back to m dimensional space. We can recognize this as roughly the recursion performed in Algorithm 1. As desired, we can perform the iteration to produce v k in O(st) = O( \u2225A\u2225 4 F \u03b5 4 log 2 1 \u03b4 ) time, which is independent of dimension, at the cost of incurring O(\u03b5(\u2225v k+1 \u2225 + \u2225v k+2 \u2225 + a 2k+1 \u2225b\u2225)) error. To bound the effect of these per-iteration errors on the final output, we need a stability analysis of the Clenshaw recurrence.\n\nConnecting sketching error to finite precision. Given that we sketch each iterate down to a size that is dimension-independent, we introduce additive error at each step. We can re-interpret this additive error as truncation error, and forge a conceptual connection between running iterative algorithms on sketches and stability analyses (also known as finite-precision analyses) in the classical NLA literature. Stability analyses of Clenshaw and Lanczos iteration are wellunderstood [Cle55; Pai76; MMS18]. However, we cannot use them in a black-box manner, since they are concerned with optimizing the \"number of bits\" required to maintain an accurate solution, and so are content with error bounds that are only polynomially tight. Translating these results to our setting results in a significantly worse polynomial running time. Therefore, for our purposes, we must revisit classical stability analyses and refine our understanding of how error propagates in these iterative methods.\n\nFolklore intuition suggests that Lanczos is a stabler algorithm for applying matrix functions, but the state-of-the-art analysis of it [MMS18] relies on the stability of the Clenshaw recurrence as a subroutine, and therefore gives a strictly worse error-accumulation bounds than the Clenshaw recurrence. In particular, if we wish to compute a generic p(A)b to \u03b5\u2225p\u2225 sup \u2225b\u2225 error in the regime where every matrix-vector product Ax incurs an error of \u03b5\u2225A\u2225\u2225x\u2225 using Lanczos, the stability analysis of Musco, Musco, and Sidford suggests that the error of the output is O(d 5.5 \u03b5), which would introduce a d 11 in our setting [MMS18]. 9 To incur less error, we do not use Lanczos and analyze the Clenshaw recurrence directly. This discussion will focus on standard Clenshaw instead of our modified version, since this is the focus of prior work; the same discussion holds for our odd/even recurrences, but with an additional factor of d incurred. The Clenshaw recurrence computes p(x) through the iteration computing q d through to q 0 : q d+1 , q d+2 := 0;\nq k := 2xq k+1 \u2212 q k+2 + a k ; p(x) = 1 2 (a 0 + q 0 \u2212 q 2 ).\nFor example, in the randomized numerical linear algebra (RNLA) literature, this is often applied in the case where a d = 1 and a k = 0 otherwise, to evaluate a degree-d Chebyshev polynomial T d (x). . Continuing the analysis into [Pai76], E is the matrix whose ith column is the error incurred in the ith iteration; each column has norm \u03b5\u2225A\u2225\u2225v j+1 \u2225 = \u03b5 in our setting where we only incur error in matrix-vector products, since \u2225A\u2225 = 1 by normalizing and \u2225v j+1 \u2225 = 1 because the algorithm normalizes it to unit norm, and we assume that scalar addition and multiplication can be performed exactly. We  F \u03b5 4 log 2 1 \u03b4 ) to perform an iteration, and d times that for the total runtime. We can hope for better. By the Markov brothers' inequality, a bounded polynomial has derivative bounded by d 2 [Sch41], and attained by the Chebyshev polynomial T d (x). So, if we only incur error from an error in input, in that we perform the recurrence not with x but some value in (x \u2212 \u03b5, x + \u03b5), this error only cascades to a O(d 2 \u03b5) worst-case error in the output. This argument suggests that a O(d 2 ) overhead is the best we could hope for.\n\nOur technical contribution is an analysis showing that the Clenshaw algorithm gives this optimal overhead, up to a logarithmic overhead. We first show that the overhead can be upper bounded by the size of the largest Clenshaw iterate |q k | (see Theorem 8.1), and then we bound the size of iterate |q k | by O(d log(d)\u2225p\u2225 sup ) (see Theorem 8.4). The main lemma we prove states that for a bounded polynomial p(x) = \u2211 d \u2113=0 a \u2113 T \u2113 (x), sums of the form a \u2113 + a \u2113+2 + \u00b7 \u00b7 \u00b7 are all bounded by O(log(\u2113)\u2225p\u2225 sup ) (Fact 6.3). This statement follows from facts in Fourier analysis (in particular, this log(\u2113) is the same log as the one occurs when bounding the L 1 norm of the Dirichlet kernel).\n\nTo our knowledge, our work is the first to give any bound of o(d 3 ). The standard literature either considers an additive error (where, unlike usual models like floating-point arithmetic, each multiplication incurs identical error regardless of magnitude) [Cle55; FP68; MH02] or eventually boils down to bounding |a i | (since their main concern is dependence on x) [Oli77;Oli79]. The modern work we are aware of shows a O(d 2 ) bound only for computing Chebyshev polynomials [BKM22], sometimes used to give a O(d 2 \u2211 i |a i |) = O(d 3 ) bound for computing generic bounded polynomials [MMS18], since a degree-d polynomial can be written as a linear combination of T k (x) with bounded coefficients. None of these analysis are sufficient to get our d 2 log(d) stability bound, since they ultimately depend on \u2211|a i |, which can be as small as \u2225p\u2225 sup and as large as \u221a d\u2225p\u2225 sup ; see the start of Section 8 for more discussion. Our improved stability analysis saves a d 4 factor in our main algorithm, from d 17 to d 13 .\n\nImproving the \u03b5-dependence. So far, our improvements give a O(d 13 \u2225A\u2225 4 F \u03b5 4 log 2 1 \u03b4 ) running time, whereas we wish to achieve a O(1/\u03b5 2 ) dependence. Though so far we have only used a very limited subset of the sketching toolkit-namely, \u2113 2 2 importance sampling-it's not clear how, for example, oblivious sketches [NN13] or the connection between importance sampling and leverage score sampling [CCHLW22] help us, since our choices of sketches are optimal up to log factors for the guarantees we desire. To get the additional improvement, we need a new sketching technique.\n\nA natural next step to improve per-iteration running time is to sparsify the matrix TAS, in order to make matrix-vector products more efficient. If we can sparsify TAS to O(1/\u03b5 2 ) non-zero entries, then we get the desired quadratic savings in per-iteration cost. There is significant literature on sparsifying the entries of a matrix [AM07; KD14; BKKS21]. However, it does not suffice for us to use these as a black box. For example, consider the sketch given by Drineas and Zouzias [DZ10]: for a matrix M \u2208 R n\u00d7n , zero out every entry smaller than \u03b5 2n , then sample entries proportional to their \u2113 2 2 magnitude, and consider the corresponding unbiased estimator of M, denotedM. The guarantee is that the operator norms are close, M \u2212M \u2a7d \u03b5, and the sparsity is O(n log(n) \u2225A\u2225 2 F \u03b5 2 ). This is precisely the guarantee we need and the sketch can be performed efficiently; however, this does not sparsify the matrix for us, since in our setting, our matrices TAS have dimension \u2225A\u2225 2 F /\u03b5 2 , so the sparsity guarantee is only O(\n\u2225A\u2225 4 F log(n) \u03b5 4 ) = O(st log(n))\n. In other words, this sketch gives us no improvement on sparsity! Bi-linear entry-wise sampling transform. Our second main technical contribution is to bypass this barrier by noticing that we don't need a guarantee as strong as a spectral norm bound. Instead, we only need to achieve approximations of the form\nASMv \u2212 ASMx k \u2a7d \u03b5,(2)\nfor various different choices of x k . So, we only need our sketchM to approximate M in some directions with good probability, instead of approximating it in all directions. We define a simple sketch, which we call the Bilinear Entry Sampling Transform (BEST 10 ), that is an unbiased estimator for A linearly (rather than the product A \u2020 A) and achieves the aforementioned guarantee (Equation (2)). This sketch is sampled from the same distribution as the one of Drineas and Zouzias from above, but without the zero-ing out small entries. In particular, we define\nM (k) = 1 p i,j A i,j e i e \u2020 j with probability p i,j = A i,j \u2225A\u2225 2 F .\nThen, to get the sketch with sparsity r, we take the average of r independent copies of the above random matrix:\nBEST(A) = 1 r \u2211 k\u2208[r] M (k) .\nThis definition is not new: for example, one of the earliest papers on matrix sparsification for linear algebra algorithms briefly considers this sketch [AM07], and this sketch has been used implicitly in prior work on dequantizing QML algorithms [Tan21]. However, to the best of our knowledge, our analysis of this sketch for preserving bi-linear forms and saving a factor of 1/\u03b5 2 is novel.\n\nWe show that BEST(A) satisfies the following guarantees: taking r = \u2126(\u2225M\u2225 2 F /\u03b5 2 ), this sketch preserves the bilinear form u \u2020 Mv to \u03b5\u2225u\u2225\u2225v\u2225 error with probability \u2a7e 0.9 (Lemma 5.2). Second, taking r = \u2126(\u2225M\u2225 2 F n), this sketch preserves the norm \u2225Mv\u2225 to 0.1\u2225v\u2225 error with probability \u2a7e 0.9. So, by taking r = \u2126(\u2225M\u2225 2 F (n + 1 \u03b5 2 )), we can get both properties. However, with this choice of r, BEST(M) does not preserve the spectral norm \u2225M\u2225, even to constant error. 11 This sketch can be interpreted as a relaxation of the approximate matrix product and the sparsification sketches above, since we use it in regimes where \u2225BEST(A)\u2225 is unbounded and \u2225BEST(A)b\u2225 is not \u03b5-close to \u2225Ab\u2225. However, if we use it in the \"interior\" of a matrix product expression, as a proxy for approximate matrix product, it can be used successfully to improve the n/\u03b5 2 dependence of spectral norm entry sparsification to something like n + 1/\u03b5 2 . In our setting, this corresponds to an improvement of 1/\u03b5 4 to 1/\u03b5 2 .\n\nApplying BEST to Clenshaw Iteration. Returning to our odd Clenshaw iteration, we had our approximate iterate\nu k \u2248 AS[4(TAS) \u2020 (TAS)v k+1 \u2212 2v k+1 \u2212 v k+2 + 2a 2k+1 S \u2020 b].\nWe can approximate this by taking B = BEST(TAS) and B \u2020 = BEST((TAS) \u2020 ) with sparsity r = \u0398(\u2225A\u2225 4 F /\u03b5 2 ) to get that the bilinear forms like [AS] i, * (TAS) \u2020 ((TAS)v k+1 ) are preserved. This allows us to successfully approximate with sparsified matrices,\n\u2248 AS[4B \u2020 Bv k+1 \u2212 2v k+1 \u2212 v k+2 + 2a 2k+1 S \u2020 b]. so v k \u2248 4B \u2020 Bv k+1 \u2212 2v k+1 \u2212 v k+2 + 2a 2k+1 S \u2020 b.\nThis recurrence in v k will be our algorithm (Algorithm 1): compute S and T in the pre-processing phase, then compute the iteration of v k 's, pulling fresh copies of B and B \u2020 each time, and output it as the description of the output u \u2248 p(A)b. What remains is the error analysis, which similar to the scalar case, requires bounding the size of the iterates \u2225v k \u2225. We used the \u03b5-approximation of the bilinear form to show that the sparsifications B and B \u2020 successfully approximate u k ; we use the constant-error approximation of the norm to show that the \u2225v k \u2225's are bounded.\n\nChallenges in extending finite-precision Clenshaw iteration. We note here that the above error does not directly follow from the error of the sketches along with the finite-precision scalar Clenshaw iteration. The error we incur in iteration k is not \u03b5\u2225u k+1 \u2225, but \u03b5\u2225v k+1 \u2225, so our error analysis requires understanding both the original Clenshaw iteration along with the \"dual\" Clenshaw iteration v k , which requires a separate error analysis. This dual iteration is essentially the same Clenshaw iteration, but for computing p(x)/x, which bears resemblance to how prior algorithms for SVT depend on the Lipschitz constant of p(x)/x [CGLLTW22]. That it is evaluating p(x)/x makes sense, since the dual iteration is the same as the Clenshaw iteration but with a factor of A removed.\n\nSums of Chebyshev coefficients. One final technical wrinkle remains, which is to bound the error accumulation of the matrix Clenshaw recurrences. In the same way that the original scalar Clenshaw algorithm required bounding arithmetic progressions of Chebyshev coefficients with step size two, a \u2113 + a \u2113+2 + \u00b7 \u00b7 \u00b7 by the norm of the corresponding polynomial p = \u2211 d \u2113=0 a \u2113 T \u2113 (x), to prove stability for the even and odd versions, we need to bound arithmetic progressions with step size four. Surprisingly, this is significantly more challenging.\n\nWe give a thorough explanation for this in Remark 6.8, but in brief, some arithmetic progressions of Chebyshev coefficients arise naturally as linear combinations of polynomial evaluations. For example, \u2211 k\u2a7e0 a 2k = 1 2 (p(\u22121) + p(1)), so we can conclude that \u2211 k\u2a7e0 a 2k \u2a7d \u2225p\u2225 sup . Through Fourier analysis, this can be generalized to progressions with different step sizes, but breaks for progressions with certain offsets. The sum \u2211 k\u2a7e0 a 4k+1 is one such example, and this is a quantity we need to bound to give a O(d 2 ) bound on the iterates of odd matrix Clenshaw. A na\u00efve bound of O(d) follows from bounding each coefficient separately, but this results in a significantly worse running time down the line.\n\nIn Lemma 6.4, we show that we can bound the above sum \u2211 k\u2a7e0 a 4k+1 by O(log 2 (d)\u2225p\u2225 sup ). This shows that this sum has near-total cancellation: despite our guarantee on coefficient being merely that it is bounded by a constant, 12 the sum of O(d) of these coefficients is only poly-logarithmic in magnitude. The proof proceeds by considering many different polynomial evaluations p(x 0 ), p(x 1 ), . . . , p(x d ), and trying to write the arithmetic progression as a linear combination of these evaluations \u2211 c k p(x k ). This can be thought of as a linear system in the c k 's, which we can then prove has a solution, and then bound the \u2113 1 norm of this solution. To do this, we argue that we can bound its solution A \u22121 b by the solution of a different system, C \u22121 b, where C is an matrix that is entrywise larger than A. Since this is not true generally, we use strong properties of the particular matrix A in the linear system at hand to prove this.\n\n\nDiscussion\n\nComparison to [CCHLW22]. The work of Chepurko, Clarkson, Horesh, Lin, and Woodruff [CCHLW22] gives algorithms for low-rank approximation sampling and ridge regression with significantly different guarantees from that of prior quantum-inspired algorithms, since these atypical guarantees are more amenable to existing sketching techniques. These do not match QSVT in all regimes, but one could argue that for the purposes of the particular problems targeted, they match them \"in spirit\" for practical regimes.\n\nThe guarantees obtained by [CCHLW22] differ from those obtained by QSVT since they do not try to obtain an approximation of f (A)b, for some function f . They instead create datadependent sketches and solve the corresponding optimization problem in the sketched space. Since the sketches preserve the cost, the solution in the sketched space has comparable cost to the optimal solution ( f (A)b) in the original space. However, such a guarantee does not immediately translate to a bound on the distance between the sketched solution and the optimal solution. While leverage score and ridge leverage score sampling are the right primitives when sketching a problem of optimizing an objective value, quantum algorithms extend more broadly to general functions f and achieve different guarantees, about closeness to the output vector.\n\nRelated techniques in randomized numerical linear algebra. Our work draws upon several ideas from the randomized numerical linear algebra literature. We refer the reader to the surveys of Mahoney [Mah11] and Woodruff [Woo14] for the relevant background. The asymmetric approximate matrix product sketch we introduce is closely related to the AMP sketches considered in [Mag11; MZ11; CEMMP15] (non-oblivious) and [CNW16] (oblivious). Our result about asymmetric AMP has essentially been observed by Magen and Zouzias [MZ11], which samples from the distribution \u2225A i, * \u2225\u2225B i, * \u2225/ \u2211 j\u2208[n] \u2225A j, * \u2225\u2225B j, * \u2225 , which our distribution oversamples. We could have used other AMP sketches as well, but we use importance sampling because (1) we can prepare these sketches in linear-time, (2) we can prepare them in time independent of dimension in the quantum-inspired setting, and (3) the sampling probabilities are optimal when B = A \u2020 , as shown by Drineas, Kannan, and Mahoney [DKM06b].\n\nRegarding the bi-linear entry-wise sampling transform, creating sketches by sampling entries proportional to their squared-magnitude are well-known in the literature but are typically used to bound the operator norm of the matrix rather than any particular bi-linear form. Bounding the operator norm directly happens to be too restrictive, as we discussed in the technical overview. Regardless, entry-wise sampling sketches were introduced by Achlioptas and McSherry [AM07] to speed up low-rank approximation. Arora, Hazan and Kale used them in the context of faster algorithms for SDP solving [AHK06]. Since then, a series of works have provided sharper guarantees for such sketches [GT09; DZ10; KD14; KDM17; BKKS21].\n\nIn addition to the usual sketching and sampling toolkit, we also use ideas from iterative algorithms in the numerical linear algebra literature. Iterative methods, such as power iteration, Golub-Kahan bidiagonalization, and Arnoldi iteration are ubiquitous in scientific computing and are used for linear programming, low-rank approximation, and numerous other fundamental linear algebra primitives [Saa81; TB97]. Our work is closely related to iterative algorithms that use the sparsification sketches and exploit singular values gaps [AL16; GS18; MM15; BCW22], but this prior work incurs dimension-dependent factors, which are crucial to avoid in our setting.\n\nSpeedups in quantum machine learning. In view of the broader goal of finding quantum advantage for machine learning tasks, we spend some time here to point out when our assumptions do not hold, and therefore, dequantization results do not apply. First, quantum-inspired linear algebra crucially relies on the input data being classical, meaning that, for example, we are given input data as a list of entries, rather than as a quantum state, which does not have its amplitudes easily accessible. Specifically, the weakest setting in which our algorithms work is when we have oversampling and query access to input (Definition 4.5). This type of access has extensibility properties similar to that of the block-encoding and we often get this access whenever quantum states and generic block-encodings can be prepared efficiently [CGLLTW22, Section 3]. However, as observed by Cotler, Huang, and McClean [CHM21], there are simple problems for which having access to entries makes trivial a problem that is exponentially hard when only given vectors via their corresponding states. A dequantized algorithm simply proves that, if its quantum counterpart does achieve a, say, exponential speedup, then oversampling and query access queries, which we need to run a dequantized algorithm, must be exponentially hard to perform on a classical computer. This explains why quantum principal component analysis has a dequantization [Tan21] when one has classical access to input as well as a proof of exponential quantum advantage [Hua+22] when one does not. More generally, an algorithm with a dequantization is still useful when run on \"quantum data\".\n\nEven given classical data, algorithms can resist dequantization by using high-degree sparsitybased QSVT rather than low-rank-based QSVT, as techniques do not extend to this regime. Further, sparsity-based QSVT is BQP-complete (indeed, quantum computation can be described as applying block-encodings of low-sparsity unitary matrices), so we would not expect this to be dequantized in full. In particular, we note that the current proposals that resist dequantization and potentially obtain a super-polynomial quantum speedup include Zhao, Fitzsimons, and Fitzsimons on Gaussian process regression [ZFF19] and Lloyd, Garnerone, and Zanardi on topological data analysis [LGZ16]. Though these works avoid the dequantization barrier to large quantum speedups, their potential for practical quantum speedups remains to be seen, since the decision on whether the speedups are \"practical\" or \"useful\" will ultimately come down to the particular choice of dataset and hardware.\n\nFinally, even if a quantum algorithm can be simulated by a classical algorithm, there are certain problems for which having |v\u27e9 is better than having the succinct representation of v. For example, estimating the Forrelation [AA18; AC17] of |v\u27e9 with another vector |u\u27e9 requires exponentially many queries to v when it is given as a classical list of entries, even with the ability to produce importance samples. If the desired task is to output the Forrelation of an output of low-rank QSVT, this could potentially give a large speedup despite our results on low-rank QSVT. However, this is a case of artificially adding hardness: we are unaware of problems in machine learning where Forrelation-type quantities are desired. Such a problem would be a good candidate for QML speedup.\n\nOpen problems. The main question in this area remains: what kinds of quantum linear algebra can we make convincing arguments of quantum speedup for? As our main result highlights, there is currently a quartic gap in the \u2225A\u2225 F dependence, which researchers believe is large enough for near-term speedups [BMNGBN21]. Are there ways to attain this speedup with more practical quantum algorithms?\n\nAnother question arising in our work is that of polynomial evaluation. We showed that computing a polynomial p(x) with \u03b5-noisy arithmetic operations can be done to O(d 2 \u03b5) error. Can we show a more fine-grained result: that, for p(x) bounded and L-Lipschitz, can we compute p(x) to O(L\u03b5) error?\n\nWith respect to the classical SVT algorithm, a natural question is to ask for an improved algorithm for estimating Tr(p(A)) to relative error, for p(x) a low-degree bounded polynomial and A symmetric. Using our result along with a Hutch-style estimate requires paying dimension dependence, but prior work shows that it is possible to achieve a running time of O(nnz(A) + poly(\u2225A\u2225 F , d, 1/\u03b5)) with a high polynomial cost, at least when p(x) is an approximation to e x [CGLLTW22]. Can we get an improved algorithm for matrix polynomial traces?\n\nFinally, with respect to the techniques we introduce, we could ask for settings in which these can be applied and improvements to our techniques. Can our matrix sparsification techniques be applies in other settings?\n\n\nPreliminaries\n\nWe use the notation f \u2272 g to denote the ordering f = O(g) (and respectively for \u2273 and \u2242), and O( f ) is shorthand for O( f poly(log f )). We use log to refer to the natural logarithm. We use the Iverson bracket, where P is one if the predicate P is true and zero otherwise. For example,\n\u2211 d i=1 \u2211 d j=i a ij = \u2211 d i=1 \u2211 d j=1 a ij j \u2a7e i .\nFinally, we assume that arithmetic operations (e.g., addition and multiplication of real numbers) and function evaluation oracles (computing f (x) from x) take unit time.\n\n\nLinear Algebra\n\nFor a vector v \u2208 C n , the standard Euclidean norm of v is denoted \u2225v\u2225 :\n= (\u2211 n i=1 |v i | 2 ) 1/2 . The number of non-zero entries of v is denoted \u2225z\u2225 0 . For a matrix A \u2208 C m\u00d7n , the Frobenius norm of A is \u2225A\u2225 F := (\u2211 m i=1 \u2211 n j=1 A i,j\n2 ) 1/2 and the spectral norm of A is \u2225A\u2225 := sup x\u2208C n ,\u2225x\u2225=1 \u2225Ax\u2225. The i-th row and j-th column of A are denoted A i, * and A * ,j , respectively. A \u2020 denotes the conjugate transpose of A, and A + denotes the Moore-Penrose pseudoinverse of A.\n\nA singular value decomposition (SVD) of A is a representation A = UDV \u2020 , where for N := min(m, n), U \u2208 C m\u00d7N and V \u2208 C n\u00d7N are isometries and D \u2208 R N\u00d7N is diagonal with \u03c3 i := D i,i and \u03c3 1 \u2a7e \u03c3 2 \u2a7e \u00b7 \u00b7 \u00b7 \u2a7e \u03c3 N \u2a7e 0. We can also write this decomposition as \nA = \u2211 N i=1 \u03c3 i U * ,i V \u2020 * ,i .\n\nPolynomials and the Chebyshev Basis\n\nWe consider polynomials with real coefficients, p \u2208 R[x]. For a Hermitian matrix A, p(A) refers to evaluating the polynomial with x replacing A; this is equivalent to applying p to the eigenvalues of A. The right definition for applying p to a general non-square matrix is subtle; as done in QSVT, we restrict to settings where the matrix formed by evaluating p on the singular values of A coincides with the evaluation of a corresponding polynomial in A.\n\nDefinition 4.1 (Definition 6.1 of [CGLLTW22]). For a matrix A \u2208 C m\u00d7n and degree-d polynomial p(x) \u2208 R[x] of parity-d (i.e., even if d is even and odd if d is odd), we define the notation p(A)\n\nin the following way:\n\n1. If p is even, meaning that we can express p(x) = q(x 2 ) for some polynomial q(x), then We work in the Chebyshev basis of polynomials throughout. Let T \u2113 (x) and U \u2113 (x) denote the degree-\u2113 Chebyshev polynomials of the first and second kind, respectively. They can be defined\np(A) := q(A \u2020 A) = p( \u221a A \u2020 A). 2. If p is odd, meaning that we can express p(x) = x \u00b7 q(x 2 ) for some polynomial q(x), then p(A) := A \u00b7 q( \u221a A \u2020 A). For example, if p(x) = x 2 + 1, then p(A) = A \u2020 A + I, and if p(x) = x 3 + x, then p(A) = AA \u2020 A + A. Looking at a singular value decomposition A = \u2211 \u03c3 i U * ,i V \u2020 * ,i , p(A) = \u2211 p(\u03c3 i )U * ,i V \u2020 * ,i when p is odd and p(A) = \u2211 p(\u03c3 i )V * ,i V \u2020 * ,on [\u22121, 1] via T \u2113 (cos(\u03b8)) = cos(\u2113\u03b8) (3) U \u2113 (cos(\u03b8)) = sin((\u2113 + 1)x)/ sin(x),(4)\nbut we will give attention to their recursive definitions, since we use them for computation.\nT 0 (x) = 1 U 0 (x) = 1 T 1 (x) = x U 1 (x) = 2x (5) T k (x) = 2x \u00b7 T k\u22121 (x) \u2212 T k\u22122 (x) U k (x) = 2x \u00b7 U k\u22121 (x) \u2212 U k\u22122 (x)\nFor a function f :\n[\u22121, 1] \u2192 R, we denote \u2225 f \u2225 sup := sup x\u2208[\u22121,1] | f (x)|. In this norm, the Chebyshev polynomials have \u2225T k (x)\u2225 sup = 1 and \u2225U k (x)\u2225 sup = n + 1. More generally, for a function f : S \u2192 R for S \u2282 R, we denote \u2225 f \u2225 S := sup x\u2208S | f (x)|, so that \u2225 f \u2225 sup = \u2225 f \u2225 [\u22121,1] .\nWe use the following well-known properties of Chebyshev polynomials from Mason and Handscomb [MH02]. \nT i (x) = 1 2 (U i (x) \u2212 U i\u22122 (x)) for i \u2a7e 1 (6) U i (x) = \u2211 j\u2a7e0 T i\u22122j (x)(1 + i \u2212 2j \u0338 = 0 ) (7) T jk (x) = T j (T k (x)) (8) U 2k+1 (x) = U k (T 2 (x))U 1 (x) = U k (T 2 (x))2x (9) d dx T k (x) = kU k\u22121 (x)(10)(x) = \u2211 \u2113 a \u2113 T \u2113 (x) (where we interpret T \u2113 (x) \u2261 0 for negative \u2113)\n. When f is a degree-d polynomial, then a \u2113 = 0 for all \u2113 > d. A common way to approximate a function is by truncating its Chebyshev series expansion; we denote this operation by f k (x) := \u2211 k \u2113=0 a \u2113 T \u2113 (x), and we denote the remainder to bef k (\nx) := f (x) \u2212 f k (x) = \u2211 \u221e \u2113=k+1 a \u2113 T \u2113 (x)\n. Standard results in approximation give bounds on \u2225 f \u2212 f k \u2225 sup for various smoothness assumptions on f . We recommend the book by Trefethen on this topic [Tre19], and use results from it throughout. We list here some basic lemmas for future use.  \n\u2225p\u2225 [\u22121\u2212\u03b4,1+\u03b4] \u2a7d e\u2225p\u2225 [\u22121,1] .\nProof. Without loss of generality, take \u2225p\u2225 sup = 1. By [SV14, Proposition 2.4] and basic properties of Chebyshev polynomials,\n\u2225p(x)\u2225 [\u22121\u2212\u03b4,1+\u03b4] \u2a7d \u2225T d (x)\u2225 [\u22121\u2212\u03b4,1+\u03b4] = T d (1 + \u03b4).\nFurther, by Proposition 2.5 in [SV14], we can evaluate T d (1 + \u03b4) via the formula\nT d (x) = 1 2 x + x 2 \u2212 1 d + 1 2 x \u2212 x 2 \u2212 1 d T d (1 + \u03b4) = 1 2 1 + \u03b4 + 2\u03b4 + \u03b4 2 d + 1 2 1 + \u03b4 \u2212 2\u03b4 + \u03b4 2 d \u2a7d exp d(\u03b4 + 2\u03b4 + \u03b4 2 ) \u2a7d exp 1 4d + 1 2 + 1 16d 2 \u2a7d e\n\nSampling and Query Access\n\nWe now introduce the \"quantum-inspired\" access model, following the exposition from prior work [CGLLTW22]. We refer the reader there for a more thorough investigation of this access model. From a sketching perspective, this model encompasses \"the set of algorithms that can be performed in time independent of input dimension, using only \u2113 2 2 sampling\", and is a decent classical analogue for the input given to a quantum machine learning algorithms operating on classical data. 2. obtain independent samples s \u2208 [n] where the probability that s is some i is |v i | 2 /\u2225v\u2225 2 ; 3. query for \u2225v\u2225.\n\nWe will use our pre-processing time to construct data structures that give us sampling and query access to our input. The samples from SQ(v) are called \"\u2113 2 2 importance samples\" in the randomized numerical linear algebra literature; we will call them samples from v. Such samples are equivalent to measurements of the quantum state |v\u27e9 := 1 \u2225v\u2225 \u2211 v i |i\u27e9 in the computational basis. Sampling and query access is closed under taking linear combinations, once we introduce slack in the form of oversampling. For v \u2208 C n and \u03d5 \u2a7e 1, we have SQ \u03d5 (v), \u03d5-oversampling and query access to v, if we can query for entries of v and we have SQ(\u1e7d), where\u1e7d \u2208 C n is a vector satisfying \u2225\u1e7d\u2225 2 = \u03d5\u2225v\u2225 2 and |\u1e7d i | 2 \u2a7e |v i | 2 for all\ni \u2208 [n].\nThe \u2113 2 2 distribution over\u1e7d \u03d5-oversamples the distribution over v:\n|\u1e7d i | 2 \u2225\u1e7d\u2225 2 = |\u1e7d i | 2 \u03d5\u2225v\u2225 2 \u2a7e 1 \u03d5 |v i | 2 \u2225v\u2225 2 .\nIntuitively speaking, as a consequence, estimators that use samples from v can also use samples from\u1e7d at the expense of a factor \u03d5 increase in the number of samples used. Formally, we can prove that oversampling access implies an approximate version of the usual sampling access:\n\nLemma 4.6 (Oversampling to sampling, [CGLLTW22, Lemma 3.5]). For u \u2208 C n , suppose we are given SQ \u03d5 (u) and some \u03b4 \u2208 (0, 1]. We can sample from u with probability \u2a7e 1 \u2212 \u03b4 in O(\u03d5 log 1 \u03b4 ) queries to SQ \u03d5 (u). We can also estimate \u2225u\u2225 to \u03bd multiplicative error for \u03bd \u2208 (0, 1] with probability\n\u2a7e 1 \u2212 \u03b4 in O( \u03d5 \u03bd 2 log 1 \u03b4 ) queries.\nBoth of these algorithms take linear time in the number of queries. Generally, compared to a quantum algorithm that can output (and measure) a desired vector |v\u27e9, our algorithms will output SQ \u03d5 (u) such that \u2225u \u2212 v\u2225 is small. Our analysis will give a bound on \u03d5 to show that we can output samples from |v\u27e9. As for error, a bound \u2225u \u2212 v\u2225 \u2a7d \u03b5\u2225v\u2225 implies that measurements from u and measurements from v are 2\u03b5-close in total variation distance [Tan19, Lemma 4.1]. Now, we show that oversampling and query access of vectors is closed under taking small linear combinations.\nLemma 4.7 (Linear combinations, [CGLLTW22, Lemma 3.6]). Given SQ \u03c6 t (v (t) ) \u2208 C n and \u03bb t \u2208 C for all t \u2208 [\u03c4], we have SQ \u03d5 (\u2211 \u03c4 t=1 \u03bb t v (t) ) for \u03d5 = \u03c4 \u2211 \u03c6 t \u2225\u03bb t v (t) \u2225 2 \u2225 \u2211 \u03bb t v (t) \u2225 2 .\nAfter paying the pre-processing cost of querying for each of the norms of the\u1e7d (t) 's, the cost of any query is equal to the cost of sampling from any of the v (t) 's plus the cost of querying an entry from all of the v (t) 's.\n\nWe can also define (over)sampling and query access for a matrix A as having sampling and query access to all of the rows of A, along with the vector of row norms of A. We have SQ \u03d5 (A) if we can query entries of A and we have SQ(\u00c3) for\u00c3 \u2208 C m\u00d7n satisfying\n\u2225\u00c3\u2225 2 F = \u03d5\u2225A\u2225 2 F and \u00c3 i,j 2 \u2a7e A i,j 2 for all (i, j) \u2208 [m] \u00d7 [n].\nSampling and query access is relevant to QML because many settings where we can apply QML to classical data (that is, data given as a list of entries, with some allowed pre-processing or in some data structure) also admits efficient SQ access to input. So, we concern ourselves with the setting that we can perform SQ queries in, say, O(1) time.\n\nRemark 4.9. We can get SQ access to input matrices and vectors in input-sparsity time.\n\nGiven v \u2208 C n in the standard RAM model, the alias method [Vos91] takes \u0398(nnz(v)) pre-processing time to output a data structure that uses \u0398(nnz(v)) space and can sample from v in \u0398(1) time. In other words, we can get SQ(v) with constant-time queries in O(nnz(v)) time, and by extension, for a matrix A \u2208 C m\u00d7n , SQ(A) with constant-time queries in O(nnz(v)) time. 14 The algorithms presented here will take linear-time pre-processing to construct the above data structure, among other things. However, they will still run in time independent of dimension without this pre-processing, supposing that we have efficient SQ access to input. Finally, we synthesize the prior results on linear combinations into the following corollary, which shows that a certain type of succinct description of a vector u is sufficient to get approximate sampling and query access to it.\n\nCorollary 4.10. Suppose we are given sampling and query access to a matrix A \u2208 C m\u00d7n and a vector b \u2208 C n , where we can respond to queries in O(1) time. Further suppose we have a vector u \u2208 C n implicitly represented by v \u2208 C m and \u03b7, with u = A \u2020 v + \u03b7b. Then we can:\n(i) Compute entries of u in O(\u2225v\u2225 0 ) time; (ii) Sample i \u2208 [n] with probability |u i | 2 \u2225u\u2225 2 in O \u2225v\u2225 0 (\u2225v\u2225 0 \u2211 k \u2225v k A k, * \u2225 2 + \u03b7 2 \u2225b\u2225 2 ) 1 \u2225u\u2225 2 log 1 \u03b4 time with probability \u2a7e 1 \u2212 \u03b4; (iii) Estimate \u2225u\u2225 2 to \u03bd relative error in O \u2225v\u2225 0 (\u2225v\u2225 0 \u2211 k \u2225v k A k, * \u2225 2 + \u03b7 2 \u2225b\u2225 2 ) 1 \u03bd 2 \u2225u\u2225 2 log 1 \u03b4 time with probability \u2a7e 1 \u2212 \u03b4. Proof. By Lemma 4.7, we have SQ \u03d5 (A \u2020 v) for \u03d5 = \u2225v\u2225 0 \u2211 k \u2225v k A k, * \u2225 2 \u2225A \u2020 v\u2225 2\nand a query cost of O(\u2225v\u2225 0 ). Applying Lemma 4.7 again, we have SQ \u03c6 (A \u2020 v + \u03b7b) for\n\u03c6 = 2 \u2225v\u2225 0 \u2211 k \u2225v k A k, * \u2225 2 + \u03b7 2 \u2225b\u2225 2 \u2225A \u2020 v + \u03b7b\u2225 2\nBy Lemma 4.6, we can draw one sample from u with probability\n\u2a7e 1 \u2212 \u03b4 with O(\u03c6 log 1 \u03b4 ) queries to SQ \u03d5 (A \u2020 v), each of which takes O(\u2225v\u2225 0 ) time. Similarly, we can estimate \u2225u\u2225 2 to \u03bd multiplicative error with O( \u03c6 \u03bd 2 log 1 \u03b4 ) queries to SQ \u03d5 (A \u2020 v).\nFor intuition, the running times above are only large when u = A \u2020 v + \u03b7b has significantly smaller norm than the magnitude of the summands v i (A i, * ) \u2020 would suggest. Usually, in our applications, we can intuitively think about this overhead being small when the desired output vector mostly lies in a subspace spanned by singular vectors with large singular values in our low-rank input. Quantum algorithms also have the same kind of overhead. Namely, the QSVT framework encodes this in the subnormalization constant \u03b1 of block-encodings, and the overhead from the subnormalization appears during post-selection [GSLW19]. When this cancellation is not too large, the resulting overhead typically does not affect too badly the runtime of our applications.\n\n\nExtending the Sketching Toolkit\n\nIn this section, we show how to extend the modern sketching toolkit (see e.g. [Woo14]) in two ways: (a) we provide a sub-sampling sketch that preserves bi-linear forms with only a inverse quadratic dependence on \u03b5 and (b) a non-oblivious, \u2113 2 2 sampling based asymmetric approximate matrix product sketch.\n\n\nThe Bi-Linear Entry-wise Sampling Transform\n\nDefinition 5.1 (Bi-linear Entry-wise Sparsifying Transform). For a matrix A \u2208 C m\u00d7n , the BEST of A with parameter T is a matrix sampled as follows: for all k \u2208 [T],\nM (k) = 1 p i,j A i,j e i e \u2020 j with probability p i,j = |A i,j | 2 \u2225A\u2225 2 F Then, BEST T (A) = 1 T \u2211 k\u2208[T]\nM (k) .\n\n\nLemma 5.2 (Basic properties of the Bi-Linear Entry-wise Sparsifying Transform). For a matrix\n\nA \u2208 C m\u00d7n , let M = BEST(A) with parameter T. Then, for X \u2208 C m\u00d7m , u \u2208 C m and v \u2208 C n , we have\nnnz(M) \u2a7d T (11) E [M] = A (12) E M \u2020 XM \u2212 A \u2020 XA = 1 T Tr(X)\u2225A\u2225 2 F I \u2212 A \u2020 XA(13)\nProof. Observe, since M is an average of T sub-samples, each of which are 1-sparse, M has at most T non-zero entries. Next,\nE [M] = 1 T \u2211 k\u2208T E [M (k) ] = \u2211 i\u2208[m] \u2211 j\u2208[n] p i,j A i,j p i,j e i e \u2020 j = A Similarly, E M \u2020 XM = 1 T 2 E \u2211 k\u2208[T] M (k) \u2020 X \u2211 k\u2208[T] M (k) = 1 T 2 E \u2211 k,k \u2032 \u2208[T] (M (k) ) \u2020 XM (k \u2032 ) = 1 T 2 \u2211 k\u0338 =k \u2032 \u2208[T] E [M (k) ] \u2020 \u00b7 X \u00b7 E [M (k \u2032 ) ] + \u2211 k\u2208[T] E [(M (k) ) \u2020 XM (k) ] = 1 \u2212 1 T A \u2020 XA + 1 T \u2211 i\u2208[m],j\u2208[n] p i,j A 2 i,j p 2 i,j e j e \u2020 i Xe i e \u2020 j = 1 \u2212 1 T A \u2020 XA + \u2225A\u2225 2 F T \u2211 i\u2208[m],j\u2208[n] X i,i e j e \u2020 j = 1 \u2212 1 T A \u2020 XA + \u2225A\u2225 2 F Tr(X) T I.\nWe list a simple consequence of these bounds that we use later.\n\nCorollary 5.3. For a matrix A \u2208 C m\u00d7n , let M = BEST(A) with parameter T. Then, for matrices X \u2208 C \u2113\u00d7m and Y \u2208 C n\u00d7d ,\nPr \u2225XMY \u2212 XAY\u2225 F \u2a7e \u2225X\u2225 F \u2225A\u2225 F \u2225Y\u2225 F \u221a \u03b4T \u2a7d \u03b4\n\nApproximate Matrix Products from \u2113 2 2 Importance Sampling\n\nIn this subsection, we extend the well-known approximate matrix product to the setting where we have an \u2113 2 2 -sampling oracle [Woo14]. The approximate matrix product guarantee is typically achieved in the oblivious sketching model [CNW16], which we cannot extend to the quantum setting. Earlier work [DKM06a] considers achieving this guarantee through row subsampling, which can be performed in this setting.\n\nDefinition 5.4. Given two matrices A \u2208 C m\u00d7n and B \u2208 C d\u00d7n , along with a probability distribution p \u2208 R n \u2a7e0 , we define the Asymmetric Approximate Matrix Product of sketch size s, denoted AMP s (A, B \u2020 , p), to be the n \u00d7 s matrix whose columns are i.i.d. sampled according to the law\n[AMP s (A, B \u2020 , p)] * ,j = e k \u221a s \u00b7 p k with probability p k\nFor an S = AMP s (A, B \u2020 , p), we will typically consider the expression ASS \u2020 B \u2020 , which can be written as the sum of independent rank-one matrices 1\ns\u00b7p k A * ,k B * ,k . Notice that E[ASS \u2020 B \u2020 ] = AB \u2020 .\nWe show that AMP s (A, B \u2020 , p) is a good approximate matrix product sketch for AB \u2020 when the distribution p is a mixture of the row sampling distributions for A and B.\n\nTheorem 5.5 (Asymmetric Approximate Matrix Multiplication). Given matrices A \u2208 C m\u00d7n and B \u2208 C n\u00d7d , consider S = AMP s (A, B \u2020 , p) for p \u2208 R n \u2a7e0 with p i \u2a7e 1 2\u03d5 (\n\u2225A * ,i \u2225 2 \u2225A\u2225 2 F + \u2225B * ,i \u2225 2 \u2225B\u2225 2 F ) for some \u03d5 \u2a7e 1. Let sr = \u2225A\u2225 2 F \u2225A\u2225 2 + \u2225B\u2225 2 F \u2225B\u2225 2 .\nThen, with probability at least 1 \u2212 \u03b4 > 0.75,\n\u2225ASS \u2020 B \u2020 \u2212 AB \u2020 \u2225 \u2a7d 2 s log sr \u03b4 \u03d5 \u2225A\u2225 2 F \u2225B\u2225 2 + \u2225A\u2225 2 \u2225B\u2225 2 F + 1 s log sr \u03b4 \u03d5\u2225A\u2225 F \u2225B\u2225 F .\nWe will use the following consequence of this theorem.\n\nCorollary 5.6. Given matrices A \u2208 C m\u00d7n and B \u2208 C n\u00d7d , consider\nS = AMP s (A, B \u2020 , p) for p \u2208 R n \u2a7e0 with p i \u2a7e 1 2\u03d5 ( \u2225A * ,i \u2225 2 \u2225A\u2225 2 F + \u2225B * ,i \u2225 2 \u2225B\u2225 2 F ) for some \u03d5 \u2a7e 1. For \u03b5 \u2208 (0, 1] and \u03b4 \u2208 (0, 0.25], when s = 4\u03d5 \u03b5 2 ( \u2225A\u2225 2 F \u2225A\u2225 2 + \u2225B\u2225 2 F \u2225B\u2225 2 ) log( 1 \u03b4 ( \u2225A\u2225 2 F \u2225A\u2225 2 + \u2225B\u2225 2 F \u2225B\u2225 2 )), then \u2225ASS \u2020 B \u2020 \u2212 AB \u2020 \u2225 \u2a7d \u03b5\u2225A\u2225\u2225B\u2225 with probability \u2a7e 1 \u2212 \u03b4.\nThe symmetric version of this result was previously stated in [KV17;RV07], and this asymmetric version was stated in [MZ11]. However, the final theorem statement was weaker, so we reprove it here. To obtain it, we prove the following key lemma: \n\u2225a (i) (b (i) ) \u2020 \u2225 M 2 \u2a7e max i\u2208[n] \u2225b (i) \u2225 2 \u2225E [XX \u2020 ]\u2225 + max i\u2208[n] \u2225a (i) \u2225 2 \u2225E [YY \u2020 ]\u2225, sr \u2a7e max i\u2208[n] \u2225b (i) \u2225 2 E [\u2225X\u2225 2 F ] + max i\u2208[n] \u2225a (i) \u2225 2 E [\u2225Y\u2225 2 F ] max(max i\u2208[n] \u2225b (i) \u2225 2 \u2225E [XX \u2020 ]\u2225, max i\u2208[n] \u2225a (i) \u2225 2 \u2225E [YY \u2020 ]\u2225)\nThen, for any t \u2a7e M/ \u221a s + 2L/(3s),\nPr 1 s \u2211 i\u2208[s] X i Y \u2020 i \u2212 E [XY \u2020 ] \u2a7e t \u2a7d 4 sr exp \u2212 st 2 2(M 2 + Lt) . Proof. For i \u2208 [s], let Z i = 1 s X i Y \u2020 i \u2212 E [XY \u2020 ] . Then \u2225Z i \u2225 \u2a7d 2 s X i Y \u2020 i \u2a7d 2L\ns . Next, we bound the variance:\n\u03c3 2 := max \u2211 i\u2208[n] E [Z i Z \u2020 i ] (i) , \u2211 i\u2208[n] E [Z \u2020 i Z i ](ii)\nWe can observe that\n\u2211 i\u2208[s] E [Z i Z \u2020 i ] = 1 s E (X i Y \u2020 i \u2212 E [XY \u2020 ])(X i Y \u2020 i \u2212 E [XY \u2020 ]) \u2020 = 1 s E \u2225Y i \u2225 2 X i X \u2020 i \u2212 E [XY \u2020 ] E [YX \u2020 ] \u2aaf 1 s (max i\u2208[n] \u2225b (i) \u2225 2 ) E [XX \u2020 ] =: V 1 \u2211 i\u2208[s] E [Z \u2020 i Z i ] = 1 s E (X i Y \u2020 i \u2212 E [XY \u2020 ]) \u2020 (X i Y \u2020 i \u2212 E [XY \u2020 ]) = 1 s E \u2225X i \u2225 2 Y i Y \u2020 i \u2212 E [YX \u2020 ] E [XY \u2020 ] \u2aaf 1 s (max i\u2208[n] \u2225a (i) \u2225 2 ) E [YY \u2020 ] =: V 2\nWe can use this to bound term (i):\n\u2211 i\u2208[s] E [Z i Z \u2020 i ] \u2a7d 1 s E [\u2225Y i \u2225 2 X i X \u2020 i ] \u2a7d 1 s (max i\u2208[n] \u2225b (i) \u2225 2 )\u2225E [XX \u2020 ]\u2225\nWe bound term (ii) as follows:  of random complex matrices with the same size, and assume that E [Z k ] = 0 and \u2225Z k \u2225 \u2a7d L. Let V 1 and V 2 be semidefinite upper bounds for the corresponding matrix-valued variances:\n\u2211 i\u2208[s] E [Z \u2020 i Z i ] \u2a7d 1 s E [\u2225X i \u2225 2 Y i Y \u2020 i ] \u2a7d 1 s (max i\u2208[n] \u2225a (i) \u2225 2 )\u2225E [YY \u2020 ]\u2225X i Y \u2020 i \u2212 E [XY \u2020 ] \u2a7e t = Pr \u2211 i\u2208[s] Z i \u2a7e t \u2a7d 4 sr exp \u2212 t 2 /2 M 2 /s + 2Lt/(3s) \u2a7d 4 sr exp \u2212 st 2 2(M 2 + Lt) , where sr = Tr(V 1 ) + Tr(V 2 ) max(\u2225V 1 \u2225, \u2225V 2 \u2225) = max i\u2208[n] \u2225b (i) \u2225 2 E [\u2225X\u2225 2 F ] + max i\u2208[n] \u2225a (i) \u2225 2 E [\u2225Y\u2225 2 F ] max(max i\u2208[n] \u2225b (i) \u2225 2 \u2225E [XX \u2020 ]\u2225, max i\u2208[n] \u2225a (i) \u2225 2 \u2225E [YY \u2020 ]\u2225)V 1 \u2ab0 E [ s \u2211 k=1 Z k Z \u2020 k ] V 2 \u2ab0 E [ s \u2211 k=1 Z \u2020 k Z k ]\nDefine an intrinsic dimension bound and a variance bound,\nsr = Tr(V 1 + V 2 ) max(\u2225V 1 \u2225, \u2225V 2 \u2225) v = max{\u2225V 1 \u2225, \u2225V 2 \u2225} Then, for t \u2a7e \u221a v + L/3, Pr \u2211 i\u2208[k] Z i \u2a7e t \u2a7d 4 sr exp \u2212 t 2 /2 v + Lt/3 .\nIt is now straight-forward to prove Theorem 5.5 using the aforementioned lemma:\n\nProof of Theorem 5.5. We apply Lemma 5.7 with a (i) = 1/p i \u00b7 A * ,i and b (i) = 1/p i \u00b7 B * ,i . As assumed the sampling distribution p i satisfies p i \u2a7e 1 2\u03d5 (\n\u2225A * ,i \u2225 2 \u2225A\u2225 2 F + \u2225B * ,i \u2225 2 \u2225B\u2225 2 F ) \u2a7e \u2225A * ,i \u2225\u2225B * ,i \u2225 \u03d5\u2225A\u2225 F \u2225B\u2225 F , so \u2225a (i) \u2225 = \u2225A * ,i \u2225/ \u221a p i \u2a7d \u2225A * ,i \u2225 2\u03d5\u2225A\u2225 2 F \u2225A * ,i \u2225 2 = 2\u03d5\u2225A\u2225 F \u2225b (i) \u2225 = \u2225B * ,i \u2225/ \u221a p i \u2a7d \u2225B * ,i \u2225 2\u03d5\u2225B\u2225 2 F \u2225B * ,i \u2225 2 = 2\u03d5\u2225B\u2225 F \u2225a (i) (b (i) ) \u2020 \u2225 = \u2225A * ,i \u2225\u2225B * ,i \u2225 p i \u2a7d \u03d5\u2225A\u2225 F \u2225B\u2225 F Further, \u2225E [XX \u2020 ]\u2225 = \u2211 i\u2208[n] A * ,i A \u2020 * ,i = \u2225AA \u2020 \u2225 = \u2225A\u2225 2 \u2225E [YY \u2020 ]\u2225 = \u2211 i\u2208[n] B * ,i B * ,i = \u2225BB \u2020 \u2225 = \u2225B\u2225 2 Finally, max i\u2208[n] \u2225b (i) \u2225 2 E [\u2225X\u2225 2 F ] + max i\u2208[n] \u2225a (i) \u2225 2 E [\u2225Y\u2225 2 F ] max(max i\u2208[n] \u2225b (i) \u2225 2 \u2225E [XX \u2020 ]\u2225, max i\u2208[n] \u2225a (i) \u2225 2 \u2225E [YY \u2020 ]\u2225) \u2a7d E [\u2225X\u2225 2 F ] \u2225E [XX \u2020 ]\u2225 + E [\u2225Y\u2225 2 F ] \u2225E [YY \u2020 ]\u2225 = \u2225A\u2225 2 F \u2225A\u2225 2 + \u2225B\u2225 2 F \u2225B\u2225 2\nSo, in Lemma 5.7, we can set L = \u03d5\u2225A\u2225 F \u2225B\u2225 F , M 2 = 2\u03d5\u2225A\u2225 2 F \u2225B\u2225 2 + 2\u03d5\u2225B\u2225 2 F \u2225A\u2225 2 , and sr = \nX i Y \u2020 i \u2212 E [XY \u2020 ] \u2a7e t \u2a7d 4 \u2225A\u2225 2 F \u2225A\u2225 + \u2225B\u2225 2 F \u2225B\u2225 exp \u2212st 2 2\u03d5(\u2225A\u2225 2 F \u2225B\u2225 2 + \u2225A\u2225 2 \u2225B\u2225 2 F ) + \u03d5\u2225A\u2225 F \u2225B\u2225 F t .\nTo get the right-hand side of the above equation to be \u2a7d \u03b4, it suffices to choose\nt = 2 s log sr \u03b4 \u03d5 \u2225A\u2225 2 F \u2225B\u2225 2 + \u2225A\u2225 2 \u2225B\u2225 2 F + 1 s log sr \u03b4 \u03d5\u2225A\u2225 F \u2225B\u2225 F .\nThis choice of t is greater than M/ \u221a s + 2L/(3s) when \u03b4 < 1/e, so with this assumption, we can conclude that with probability \u2a7e 1 \u2212 \u03b4, \n\u2225ASS \u2020 B \u2020 \u2212 AB \u2020 \u2225 \u2a7d 2 s log sr \u03b4 \u03d5 \u2225A\u2225 2 F \u2225B\u2225 2 + \u2225A\u2225 2 \u2225B\u2225 2 F + 1 s log sr \u03b4 \u03d5\u2225A\u2225 F \u2225B\u2225 F .\n\nSums of Chebyshev Coefficients\n(x) = \u2211 \u221e i=0 a i T i (x)\n. A broad topic of interest in approximation theory is bounds for linear combinations of these coefficients, \u2211 a i c i , in terms of \u2225 f \u2225 sup ; this was one motivation of Vladimir Markov in proving the Markov brothers' inequality [Sch41,p575]. Our goal for this section will be to investigate this question in the case where these sums are arithmetic progressions of step four. This will be necessary for later stability analyses, and is one of the first non-trivial progressions to bound. We begin with some straightforward assertions (see [Tre19] for background). \n\u2211 \u2113 a \u2113 = | f (1)| \u2a7d \u2225 f \u2225 sup \u2211 \u2113 (\u22121) \u2113 a \u2113 = | f (\u22121)| \u2a7d \u2225 f \u2225 sup \u2211 \u2113 a \u2113 \u2113 is even = \u2211 \u2113 a \u2113 1 2 (1 + (\u22121) \u2113 ) \u2a7d \u2225 f \u2225 sup \u2211 \u2113 a \u2113 \u2113 is odd = \u2211 \u2113 a \u2113 1 2 (1 \u2212 (\u22121) \u2113 ) \u2a7d \u2225 f \u2225 sup\nWe use the following result on Lebesgue constants to bound truncations of the Chebyshev coefficient sums. \n(x) = \u2211 k \u2113=0 a \u2113 T \u2113 (x)\n, and let the optimal degree-k approximating polynomial to f be denoted f * k . Then\n\u2225 f \u2212 f k \u2225 sup \u2a7d 4 + 4 \u03c0 2 log(k + 1) \u2225 f \u2212 f * k \u2225 sup \u2a7d 4 + Similarly, \u2225 f k \u2225 sup \u2a7d \u2225 f \u2212 f k \u2225 sup + \u2225 f \u2225 sup \u2a7d 5 + 4 \u03c0 2 log(k + 1) \u2225 f \u2225 sup .\nThis implies bounds on sums of coefficients.\nFact 6.3. Consider a function f (x) = \u2211 \u2113 a \u2113 T \u2113 (x). Then \u221e \u2211 \u2113=k a \u2113 \u2113 \u2212 k is even \u2a7d \u2225 f \u2212 f k\u22121 \u2225 sup \u2a7d 4 + 4 \u03c0 2 log(k) \u2225 f \u2225 sup , \u221e \u2211 \u2113=k a \u2113 (\u22121) \u2113 \u2a7d \u2225 f \u2212 f k\u22121 \u2225 sup \u2a7d 4 + 4 \u03c0 2 log(k) \u2225 f \u2225 sup ,\nwhere the inequalities follow from Fact 6.1 and Lemma 6.2. When k = 0, then the sum is bounded by \u2225 f \u2225 sup , as shown in Fact 6.1. Now, we prove similar bounds in the case that f (x) is an odd function. In particular, we want to obtain a bound on alternating signed sums of the Chebshyev coefficients and we incur a blowup that scales logarithmically in the degree.\n\nLemma 6.4. Let f : [\u22121, 1] \u2192 R be an odd Lipschitz continuous function with Chebyshev coefficients {a \u2113 } \u2113 , so that a k = 0 for all even k. Then the Chebyshev coefficient sum is bounded as\nd \u2211 \u2113=0 (\u22121) \u2113 a 2\u2113+1 \u2a7d (log(d) + 2) max 0\u2a7dk\u2a7d2d+1 \u2225 f k \u2225 sup \u2a7d (log(d) + 2) 5 + 4 \u03c0 2 log(2d + 2) \u2225 f \u2225 sup \u2a7d 16 + 4 log 2 (d + 1) \u2225 f \u2225 sup .\nWe first state the following relatively straight-forward corollary: Corollary 6.5. Lemma 6.4 gives bounds on arithmetic progressions with step size four. Let f : [\u22121, 1] \u2192 R be a Lipschitz continuous function, and consider nonnegative integers c \u2a7d d. Then d \u2211 \u2113=c a \u2113 \u2113 \u2212 c \u2261 0 (mod 4) \u2a7d (32 + 8 log 2 (d + 1))\u2225 f \u2225 sup\n\nProof. Define f odd := 1 2 ( f (x) \u2212 f (\u2212x)) and f even := 1 2 ( f (x) + f (\u2212x)) to be the odd and even parts of f respectively. Triangle inequality implies that\n\u2225 f odd \u2225 sup , \u2225 f even \u2225 sup \u2a7d \u2225 f \u2225 sup . Suppose c, d are odd. Then d \u2211 \u2113=c a \u2113 \u2113 \u2212 c \u2261 0 (mod 4) = 1 2 \u230a(d\u2212c)/2\u230b \u2211 \u2113=0 a c+2\u2113 (1 \u00b1 (\u22121) \u2113 ) \u2a7d 1 2 \u230a(d\u2212c)/2\u230b \u2211 \u2113=0 a c+2\u2113 + \u230a(d\u2212c)/2\u230b \u2211 \u2113=0 (\u22121) \u2113 a c+2\u2113 = 1 2 f odd d (1) \u2212 f odd c\u22122 (1) + (d\u22121)/2 \u2211 \u2113=0 (\u22121) \u2113 a 2\u2113+1 \u2212 (c\u22123)/2 \u2211 \u2113=0 (\u22121) \u2113 a 2\u2113+1 \u2a7d 1 2 \u2225 f odd c\u22122 \u2225 sup + \u2225 f odd d \u2225 sup + 2(log(d) + 2) max 0\u2a7dk\u2a7dd \u2225 f odd k \u2225 sup \u2a7d (32 + 8 log 2 (d + 1))\u2225 f odd \u2225 sup \u2a7d (32 + 8 log 2 (d + 1))\u2225 f \u2225 sup\nThe case when c is even is easier: by Eq. (8), we know that\n\u2211 \u2113 a 2\u2113 T \u2113 (x) sup = \u2211 \u2113 a 2\u2113 T \u2113 (T 2 (x)) sup = \u2211 \u2113 a 2\u2113 T 2\u2113 (x) sup = f even (x) sup \u2a7d \u2225 f \u2225 sup , so by Fact 6.3, \u2211 \u2113\u2a7ec a \u2113 \u2113 \u2212 c \u2261 0 (mod 4) = \u2211 \u2113\u2a7ec/2 a 2\u2113 \u2113 \u2212 c/2 is even \u2a7d 4 + 4 \u03c0 2 log(c/2 \u2212 1) \u2211 \u2113 a 2\u2113 T \u2113 (x) sup \u2a7d 4 + 4 \u03c0 2 log(c/2 \u2212 1) \u2225 f \u2225 sup .(14)\nFrom the above, we can bound the type of sums in the problem statement, paying an additional factor of two:\nd \u2211 \u2113=c a \u2113 \u2113 \u2212 c \u2261 0 (mod 4) \u2a7d \u2211 \u2113\u2a7ec a \u2113 \u2113 \u2212 c \u2261 0 (mod 4) + \u2211 \u2113\u2a7ed+1 a \u2113 \u2113 \u2212 c \u2261 0 (mod 4) \u2a7d 8 + 4 \u03c0 2 (log(c/2 \u2212 1) + log(d/2 + 1)) \u2225 f \u2225 sup ,(15)\ngiving the desired bound.\n\nWe note that Lemma 6.4 will be significantly harder to prove. See Remark 6.8 for an intuitive explanation why. We begin with two structural lemmas on how the solution to a unitriangular linear system behaves, which might be of independent interest. Lemma 6.6 (An entry-wise positive solution). Suppose that A \u2208 R d\u00d7d is an upper unitriangular matrix such that, for all i \u2a7d j, A i,j > 0, A i,j > A i\u22121,j . Then A \u22121 \u20d7 1 is a vector with positive entries.\n\nThe same result holds when A is a lower unitriangular matrix such that, for all i \u2a7e j, A i,j > 0,\nA i,j > A i+1,j .\nProof. Let x = A \u22121 \u20d7 1. Then x d = 1 \u2a7e 0. The result follows by induction:\nx i = 1 \u2212 d \u2211 j=i+1 A i,j x j = d \u2211 j=i+1 (A i+1,j \u2212 A i,j )x j + 1 \u2212 d \u2211 j=i+1 A i+1,j x j = d \u2211 j=i+1 (A i+1,j \u2212 A i,j )x j + 1 \u2212 [Ax] i+1 = d \u2211 j=i+1 (A i+1,j \u2212 A i,j )x j > 0\nFor lower unitriangular matrices, the same argument follows. The inverse satisfies x 1 = 1 and\nx i = 1 \u2212 i\u22121 \u2211 j=1 A i,j x j = d \u2211 j=i+1 (A i\u22121,j \u2212 A i,j )x j + 1 \u2212 d \u2211 j=i+1 A i\u22121,j x j > 0\nNext, we characterize how the solution to a unitriangular linear system behaves when we consider a partial ordering on the matrices.\n\nLemma 6.7. Let A be a nonnegative upper unitriangular matrix such that A i,j > A i\u22121,j and A i,j > A i,j+1 for all i \u2a7d j. Let B be a matrix with the same properties, such that A \u2a7e B entrywise. By Lemma 6.6,\nx (A) = A \u22121 \u20d7 1 and x (B) = B \u22121 \u20d7 1 are nonnegative. It further holds that \u2211 d i=1 [A \u22121 \u20d7 1] i \u2a7d \u2211 d i=1 [B \u22121 \u20d7 1] i .\nProof. We consider the line between A and B, A(t) = A(1 \u2212 t) + Bt for t \u2208 [0, 1]. Let x(t) = A \u22121 \u20d7 1; we will prove that \u20d7 1 \u2020 x(t) is monotonically increasing in t. The gradient of x(t) has a simple form [Tao13]:\nA(t)x(t) = \u20d7 1 \u2202[A(t)x(t)] = \u2202 t [ \u20d7 1] (B \u2212 A)x(t) + A(t)\u2202 t x(t) = 0 \u2202 t x(t) = A \u22121 (t)(A \u2212 B)x(t). So, \u20d7 1 \u2020 \u2202 t x(t) = \u20d7 1 \u2020 A \u22121 (t)(A \u2212 B)A \u22121 (t) \u20d7 1 = [([A(t)] \u22121 ) \u2020 \u20d7 1] \u2020 (A \u2212 B)[A \u22121 (t) \u20d7 1].\nSince A and B satisfy the entry constraints, so do every matrix along the line. Consequently, the column constraints in Lemma 6.6 are satisfied for both A and A \u2020 , so both ([A(t)] \u22121 ) \u2020 \u20d7 1 and A \u22121 (t) \u20d7 1 are positive vectors. Since A \u2a7e B entrywise, this means that \u20d7 1 \u2020 \u2202 t x(t) is positive, as desired.\n\nProof of Lemma 6.4. We first observe that the following sorts of sums are bounded. Let x k := cos( \u03c0 2 (1 \u2212 1 2k+1 )). Then, using that T \u2113 (cos(x)) = cos(\u2113x),\nf 2k+1 (x k ) = 2k+1 \u2211 \u2113=0 a \u2113 T \u2113 (x k ) = k \u2211 \u2113=0 a 2\u2113+1 T 2\u2113+1 (x k ) = k \u2211 \u2113=0 a 2\u2113+1 cos \u03c0 2 2\u2113 + 1 \u2212 2\u2113 + 1 2k + 1 = k \u2211 \u2113=0\n(\u22121) \u2113 a 2\u2113+1 sin \u03c0 2 2\u2113 + 1 2k + 1 .\n\nWe have just shown that k \u2211 \u2113=0 a 2\u2113+1 (\u22121) \u2113 sin \u03c0 2\n2\u2113 + 1 2k + 1 \u2a7d \u2225 f 2k+1 \u2225 sup .(16)\nWe now claim that there exist non-negative c k for k \u2208 {0, 1, . . . , d} such that\nd \u2211 \u2113=0 (\u22121) \u2113 a 2\u2113+1 = d \u2211 k=0 c k f 2k+1 (x k ).(17)\nThe f 2k+1 (x k )'s can be bounded using Lemma 6.2. The rest of the proof will consist of showing that the c k 's exist, and then bounding them.\n\nTo do this, we consider the coefficient of each a 2\u2113+1 separately; let A (k) \u2208 [0, 1] d+1 (index starting at zero) be the vector of coefficients associated with p 2k+1 (x k ):\nA (k) \u2113 = sin \u03c0 2 2\u2113 + 1 2k + 1 for 0 \u2a7d \u2113 \u2a7d k, 0 otherwise(18)\nNote that the A \nc 0 A (0) + \u00b7 \u00b7 \u00b7 + c d A (d) = \u20d7 1,\nor in other words, the equation Ac = \u20d7 1 is satisfied, where A is the matrix with columns A (k) and c is the vector of c \u2113 's. Since A is upper triangular (in fact, with unit diagonal), this can be solved via backwards substitution: c d = 1, then c d\u22121 can be deduced from c d , and so on. More formally, the sth row gives the following constraint that can be rewritten as a recurrence. \nc s = 1 \u2212 d \u2211 t=s+1 sin \u03c0 2 2s + 1 2t + 1 c t(20)\nBecause the entries of A increase in \u2113, the c \u2113 's are all positive.\n\nInvoking Lemma 6.6 with the matrix A establishes that such c s exist; our goal now is to bound them. Doing so is not as straightforward as it might appear: since the recurrence Eq. (20) subtracts by c t 's, an upper bound on c t for t \u2208 [s + 1, d] does not give an upper bound on c s ; it gives a lower bound. So, an induction argument to show bounds for c s 's fails. Further, we were unable to find any closed form for this recurrence. However, since all we need to know is the sum of the c s 's, we show that we can bound this via a generic upper bound on the recurrence.\n\nHere, we apply Lemma 6.7 to A as previously defined, and the bounding matrix is (for i \u2a7d j) This function has the property that |g(\u03b8)| \u2a7d \u2225 f \u2225 sup and g(k) = a |k| /2 (except g(0) = a 0 ). Consequently,\nB i,j = i j \u2a7d 2i + 1 2j + 1 \u2a7d sin \u03c0 2 2i + 1 2j + 1 = A i,j , using that sin( \u03c0 2 x) \u2a7e x for x \u2208 [0, 1]. Let\u0109 = B \u22121 \u20d7 1. Then\u0109 i = 1 i+1 for i \u0338 = d and\u0109 d = 1. [B\u0109] i = d \u2211 j=i B i,j\u0109j = d\u22121 \u2211 j=i i j 1 j + 1 + i d = i d\u22121 \u2211 j=i 1 j \u2212 1 j + 1 + i d = i 1 i \u2212 1 d ) + i d = 1 By Lemma 6.7, \u2211 i c i \u2a7d \u2211 i\u0109i \u2a7d log(d) + 2. So, altogether, we have d \u2211 \u2113=0 (\u22121) \u2113 a 2\u2113+1 = d \u2211 k=0 c k f 2k+1 (x k ) \u2a7d d \u2211 k=0 c k \u2225 f 2k+1 \u2225 sup \u2a7d d \u2211 k=0 c k max 0\u2a7dk\u2a7dd \u2225 f 2k+1 \u2225 sup = d \u2211 k=0 c k max 0\u2a7dk\u2a7d2d+1 \u2225 f k \u2225 sup \u2a7d (log(d) + 2) max 0\u2a7dk\u2a7d2d+1 \u2225 f k \u2225 sup1 t t\u22121 \u2211 j=0 f cos( 2\u03c0j t ) = 1 t t\u22121 \u2211 j=0 g 2\u03c0j t = 1 t t\u22121 \u2211 j=0 \u221e \u2211 k=\u2212\u221e g(k)e 2\u03c0ijk/t = \u221e \u2211 k=\u2212\u221e g(k) t\u22121 \u2211 j=0 1 t e 2\u03c0ijk/t = \u221e \u2211 k=\u2212\u221e g(k) k is divisible by t = \u221e \u2211 k=\u2212\u221e g(kt),\nso we can bound arithmetic progressions |\u2211 k g(kt)| \u2a7d \u2225 f \u2225 sup , and this generalizes to other offsets, to bound |\u2211 k g(kt + o)| for some o \u2208 [t \u2212 1]. Notably, though, this approach does not say anything about sums like \u2211 k a 4k+1 . The corresponding progression of Fourier coefficients doesn't give it, for example, since we pick up unwanted terms from the negative Fourier coefficients. 15 \u2211 k g(4k + 1) = ( g(1) + g(5) + g(9) + \u00b7 \u00b7 \u00b7) + ( g(\u22123) + g(\u22127) + g(\u221211) + \u00b7 \u00b7 \u00b7) = 1 2 (a 1 + a 5 + a 9 + \u00b7 \u00b7 \u00b7) + 1 2 (a 3 + a 7 + a 11 + \u00b7 \u00b7 \u00b7) = \u2211 k\u2a7e0 a 2k+1 .\n\nIn fact, by inspection of the distribution 16 D(x) = \u2211 \u221e k=0 T 4k+1 (x), it appears that this arithmetic progression cannot be written as a linear combination of evaluations of f (x). Since the shape of the distribution appears to have 1/x behavior near x = 0, we conjecture that our analysis losing a log factor is, in some respect, necessary. 15 These sums are related to the Chebyshev coefficients one gets from interpolating a function at Chebyshev points [Tre19,Theorem 4.2]. 16 This is the functional to integrate against to compute the sum, 2\n\u03c0 1 \u22121 f (x)D(x)/ \u221a 1 \u2212 x 2 = \u2211 a 4k+1 .\nThe distribution is not a function, but can be thought of as the limit object of D n (x) = \u2211 n k=0 T 4k+1 (x) as n \u2192 \u221e, analogous to Dirichlet kernels and the Dirac delta distribution. Conjecture 6.9. For any step size t > 1 and offset o \u2208 [t \u2212 1] such that o \u0338 = t/2, there exists a function f : [\u22121, 1] \u2192 R such that \u2225 f \u2225 sup = 1 but |\u2211 n k=0 a tk+o | = \u2126(log(n)).\n\n\nProperties of the Clenshaw Recursion\n\n\nDeriving the Clenshaw Recursions\n\nSuppose we are given as input a degree-d polynomial as a linear combination of Chebyshev polynomials:\np(x) = d \u2211 k=0 a k T k (x).(21)\nThen this can be computed with the Clenshaw algorithm, which is the following recurrence.\nq d+1 = q d+2 = 0 q k = 2xq k+1 \u2212 q k+2 + a k (Clenshaw) p = 1 2 (a 0 + q 0 \u2212 q 2 )\nLemma 7.1. The recursion in Eq. (Clenshaw) computes p(x). That is, in exact arithmetic,p = p(x). In particular,\nq k = d \u2211 i=k a i U i\u2212k (x).(22)\nProof. We show Eq. (22) by induction.\nq k = 2xq k+1 \u2212 q k+2 + a k = 2x d \u2211 i=k+1 a i U i\u2212k\u22121 (x) \u2212 d \u2211 i=k+2 a i U i\u2212k\u22122 (x) + a k = a k + 2xa k+1 U 0 (x) + d \u2211 i=k+2 a i (2xU i\u2212k\u22121 (x) \u2212 U i\u2212k\u22122 (x)) = d \u2211 i=k a i U i\u2212k (x).\nConsequently, we have\n1 2 (a 0 + u 0 \u2212 u 2 ) = 1 2 a 0 + d \u2211 i=0 a i U i (x) \u2212 d \u2211 i=2 a i U i\u22122 (x) = a 0 + a 1 x + d \u2211 i=2 a i 2 (U i (x) \u2212 U i\u22122 (x)) = d \u2211 i=0 a i T i (x).\nRemark 7.2. Though the aforementioned discussion is specialized to the scalar setting, it extends to the the matrix setting almost entirely syntactically: consider a Hermitian A \u2208 C n\u00d7n and b \u2208 C n with \u2225A\u2225, \u2225b\u2225 \u2a7d 1. Then p(A)b can be computed in the following way:\nu d+1 = \u20d7 0 u d = a d b u k = 2Au k+1 \u2212 u k+2 + a k b u := p(A)b = 1 2 (a 0 b + u 0 \u2212 u 2 ) (23)\nThe proof that this truly computes p(A)b is the same as the proof of correctness for Clenshaw's algorithm shown above. We will also be generalizing to non-Hermitian A \u2208 C m\u00d7n , in which case the only additional wrinkle is that in the recurrence we will need to choose either A or A \u2020 such that dimensions are consistent. Provided that the polynomial being computed is even or odd, no issues will arise. Consequently, Clenshaw-like recurrences will give matrix polynomials where x k are replaced with A \u2020 AA \u2020 \u00b7 \u00b7 \u00b7 Ab, which corresponds to the definition of singular value transformation from Definition 4.1.\n\n\nEvaluating Even and Odd Polynomials\n\nWe will be considering evaluating odd and even polynomials. We again focus on the scalar setting and note that this extends to the matrix setting in the obvious way. The previous recurrence Eq. (Clenshaw) can work in this setting, but it'll be helpful for our analysis if the recursion multiplies by x 2 each time, instead of x [MH02, Chapter 2, Problem 7]. So, in the case where the degree-(2d + 1) polynomial p(x) is odd (so a 2k = 0 for every k), it can be computed with the iteration\nq d+1 = q d+2 = 0 q k = 2T 2 (x)q k+1 \u2212 q k+2 + a 2k+1 U 1 (x) (Odd Clenshaw) p = 1 2 (q 0 \u2212 q 1 )\nWhen p(x) is a degree-(2d) even polynomial (so a 2k+1 = 0 for every k), it can be computed via the same recurrence, replacing a 2k+1 U 1 (x) with a 2k . However, we will use an alternative form that's more convenient for us (since we can reuse the analysis of the odd case).\n\na 2k := a 2k \u2212 a 2k+2 + a 2k+4 \u2212 \u00b7 \u00b7 \u00b7 \u00b1 a 2d (24)\nq d+1 = q d+2 = 0 q k = 2T 2 (x)q k+1 \u2212 q k+2 +\u00e3 2k+2 U 1 (x) 2 (Even Clenshaw) p =\u00e3 0 + 1 2 (q 0 \u2212 q 1 )\nThese recurrences correctly compute p follows from a similar analysis to the standard Clenshaw algorithm, formalized below.\n\nLemma 7.3. The recursions in Eq. (Odd Clenshaw) and Eq. (Even Clenshaw) correctly compute p(x) for even and odd polynomials, respectively. That is, in exact arithmetic,p = p(x). In particular,\nq k = d \u2211 i=k a i U i\u2212k (x).(25)\nProof. We can prove these statements by applying Eq. (22). In the odd case, Eq. (Odd Clenshaw) is identical to Eq. (Clenshaw) except that x is replaced by T 2 (x) and a k is replaced by a 2k+1 U 1 (x), so by making the corresponding changes in the iterate, we get that\nq k = d \u2211 i=k a 2i+1 U 1 (x)U i\u2212k (T 2 (x)) = d \u2211 i=k a 2i+1 U 2(i\u2212k)+1 (x) by Eq. (9) p = 1 2 (q 0 \u2212 q 1 ) = d \u2211 i=0 a 2i+1 2 U 2i+1 (x) \u2212 U 2i\u22121 (x) = p(x). by Eq. (6)\nSimilarly, in the even case, Eq. (Even Clenshaw) is identical to Eq. (Clenshaw) except that x is replaced by T 2 (x) and a k is replaced by 4\u00e3 2k x 2 (see Definition 24), so that\nq k = d \u2211 i=k\u00e3 2i+2 U 1 (x) 2 U i\u2212k (T 2 (x)) = d \u2211 i=k\u00e3 2i+2 U 1 (x)U 2(i\u2212k)+1 (x) by Eq. (9) = d \u2211 i=k\u00e3 2i+2 (U 2(i\u2212k) (x) + U 2(i\u2212k+1) (x)) by Eq. (5) = d+1 \u2211 i=k\u00e3 2i+2 U 2(i\u2212k) (x) + d+1 \u2211 i=k+1\u00e3 2i U 2(i\u2212k) (x) noticing that\u00e3 2d+2 = 0 =\u00e3 2k+2 + d+1 \u2211 i=k+1 (\u00e3 2i +\u00e3 2i+2 )U 2(i\u2212k) (x) =\u00e3 2k+2 + d \u2211 i=k+1 a 2i U 2(i\u2212k) (x) = \u2212\u00e3 2k + d \u2211 i=k a 2i U 2(i\u2212k) (x)\nFinally, observ\u1ebd\na 0 + 1 2 (q 0 \u2212 q 1 ) =\u00e3 0 + 1 2 (a 0 \u2212\u00e3 0 +\u00e3 2 ) + d \u2211 i=1 a 2i 2 (U 2i (x) \u2212 U 2i\u22122 (x)) = p(x).\nRemark 7.4. We can further compute what happens to all these recursions with some additive \u03b5 (k) error in iteration k. This follows just by adding \u03b5 (k) to the constant term in the recursion and chasing the resulting changes through the analysis of Clenshaw, as is done for Lemma 7.3.\nfor standard Clenshaw:q k = d \u2211 i=k (a i + \u03b5 (i) )U i\u2212k (x) for odd Clenshaw:q k = d \u2211 i=k (a 2i+1 U 2(i\u2212k)+1 (x) + \u03b5 (i) U i\u2212k (T 2 (x))) for even Clenshaw:q k = \u2212\u00e3 2k + d \u2211 i=k (a 2i U 2(i\u2212k) (x) + \u03b5 (i) U i\u2212k (T 2 (x)))\nPropagating this error to the full result gives the following results: T 2 (x))\u2225 sup = 2i + 1, this suggests that these parity-specific recurrences are less stable than the standard recursion. However, they will be more amenable to our sketching techniques.\nfor standard Clenshaw:p \u2212 p(x) = 1 2 + d \u2211 i=1 \u03b5 (i) T i (x) for odd Clenshaw:p \u2212 p(x) = 1 2 \u03b5 (0) + 1 2 d \u2211 i=1 \u03b5 (i) (U i (T 2 (x)) \u2212 U i\u22121 (T 2 (x))) for even Clenshaw:p \u2212 p(x) = 1 2 \u03b5 (0) + 1 2 d \u2211 i=1 \u03b5 (i) (U i (T 2 (x)) \u2212 U i\u22121 (T 2 (x))) Because \u2225T i (x)\u2225 sup = 1 but \u2225U i (T 2 (x)) \u2212 U i\u22121 (\n\nStability of the Scalar Clenshaw Recursion\n\nBefore we move to the matrix setting, we warmup with a stability analysis of the scalar Clenshaw recurrence. Suppose we perform Eq. (Clenshaw) to compute a degree-d polynomial p, except every addition, subtraction, and multiplication incurs \u03b5 relative error. \n\u2211 |a i | \u2a7d \u221a d \u2211 a 2 i = O( \u221a d\u2225p\u2225 sup ).(26)\nTesting \u2225\u2211 d \u2113=1 s \u2113 1 \u221a \u2113 T \u2113 (x)\u2225 sup for random signs s \u2113 \u2208 {\u00b11} suggests that this bound is tight, meaning coefficient-wise bounds can only prove an error overhead of \u0398(d 2.5 \u2225p\u2225 sup ) for the Clenshaw recurrence.\n\nWe improve on prior stability analyses to show that the Clenshaw recurrence for Chebyshev polynomials only incurs an error overhead of d 2 log(d)\u2225p\u2225 sup . This is tight up to a logarithmic factor. This, for example, could be used to improve the bound in [MMS18, Lemma 9] from k 3 to k 2 log(k) (where in that paper, k denotes degree). As we do for the upcoming matrix setting, we proceed by performing an error analysis on the recursion with a stability parameter \u00b5, and then showing that for any bounded polynomial, 1/\u00b5 can be chosen to be O(d 2 log d).\n\n\nAnalyzing Error Propagation\n\nThe following is a simple analysis of Clenshaw, with some rough resemblance to an analysis of Oliver [Oli79].\n\nTheorem 8.1 (Stability Analysis for Scalar Clenshaw). Consider a degree-d polynomial p : [\u22121, 1] \u2192 R with Chebyshev coefficients p(x) = \u2211 d k=0 a k T k (x). Let \u2295, \u2296, \u2299 : C \u00d7 C \u2192 C be binary operations representing addition, subtraction, and multiplication to \u00b5\u03b5 relative error, for 0 < \u03b5 < 1:\n|(x \u2295 y) \u2212 (x + y)| \u2a7d \u00b5\u03b5(|x| + |y|) |(x \u2296 y) \u2212 (x \u2212 y)| \u2a7d \u00b5\u03b5(|x| + |y|) |x \u2299 y \u2212 x \u00b7 y| \u2a7d \u00b5\u03b5|x||y| = \u00b5\u03b5|xy|.\nGiven an x \u2208 [\u22121, 1], consider performing the Clenshaw recursion with these noisy operations:\nq d+1 =q d+2 = 0 q k = (2 \u2299 x) \u2299q k+1 \u2296 (q k+2 \u2296 a k ) (Finite-Precision Clenshaw) p = 1 2 \u2299 ((a 0 \u2295 q 0 ) \u2296 q 2 )\nThen Eq. (Finite-Precision Clenshaw) outputs p(x) up to 50\u03b5\u2225p\u2225 sup error 17 , provided that \u00b5 > 0 satisfies the following three criterion.\n\n(a) \u00b5\u03b5 \u2a7d 1 50(d+2) 2 ;\n(b) \u00b5 \u2211 d i=0 |a i | \u2a7d \u2225p\u2225 sup ; (c) \u00b5|q k | = \u00b5|\u2211 d i=k a i U i\u2212k (x)| \u2a7d 1 d \u2225p\u2225 sup for all k \u2208 {0, .\n. . , d}. This analysis shows that arithmetic operations incurring \u00b5\u03b5 error result in computing p(x) to \u03b5 error. In particular, the stability of the scalar Clenshaw recurrence comes down to understanding how small we can take \u00b5. Note that if we ignored coefficient sign\n, |\u2211 d i=k a i U i\u2212k (x)| \u2a7d |\u2211 d i=k |a i |U i\u2212k (x)| = \u2211 d i=k (i \u2212 k + 1)|a i |, this would require setting \u00b5 = \u0398(1/d 3 )\n. We show in Section 8.2 that we can set \u00b5 = \u0398((d 2 log(d)) \u22121 ) for all x \u2208 [\u22121, 1] and polynomials p.\n\nLemma 8.2. In Theorem 8.1, it suffices to take \u00b5 = \u0398((d 2 log(d)) \u22121 ).\n\nProof of Theorem 8.1. We will expand out these finite precision arithmetic to get error intervals for each iteration.q\nd+1 =q d+2 = 0,(27)andq k = (2 \u2299 x) \u2299q k+1 \u2296 (q k+2 \u2296 a k ) = (2x \u00b1 2\u00b5\u03b5|x|) \u2299q k+1 \u2296 (q k+2 \u2212 a k \u00b1 \u00b5\u03b5(|q k+2 | + |a k |)) = ((2xq k+1 \u00b1 2\u00b5\u03b5|x|q k+1 ) \u00b1 \u00b5\u03b5|(2x \u00b1 2\u00b5\u03b5|x|)q k+1 |) \u2296 (q k+2 \u2212 a k \u00b1 \u00b5\u03b5(|q k+2 | + |a k |)) \u2208 (2xq k+1 \u00b1 (2\u00b5\u03b5 + \u00b5 2 \u03b5 2 )2|xq k+1 |) \u2296 (q k+2 \u2212 a k \u00b1 \u00b5\u03b5(|q k+2 | + |a k |)) \u2208 (2xq k+1 \u00b1 6\u00b5\u03b5|xq k+1 |) \u2296 (q k+2 \u2212 a k \u00b1 \u00b5\u03b5(|q k+2 | + |a k |)) = 2xq k+1 \u2212q k+2 + a k \u00b1 \u00b5\u03b5(6|xq k+1 | + |q k+2 | + |a k |) + \u00b5\u03b5|2xq k+1 \u00b1 6\u00b5\u03b5|xq k+1 || + \u00b5\u03b5|q k+2 \u2212 a k \u00b1 \u00b5\u03b5(|q k+2 | + |a k |)| \u2208 2xq k+1 \u2212q k+2 + a k \u00b1 \u00b5\u03b5(14|xq k+1 | + 3|q k+2 | + 3|a k |),\nand,\np = 1 2 \u2299 ((a 0 \u2295 q 0 ) \u2296 q 2 ) = 1 2 \u2299 ((a 0 + q 0 \u00b1 \u00b5\u03b5(|a 0 | + |q 0 |)) \u2296 q 2 ) = 1 2 \u2299 ((a 0 + q 0 \u2212 q 2 \u00b1 \u00b5\u03b5(|a 0 | + |q 0 |)) \u00b1 \u00b5\u03b5(|a 0 + q 0 \u00b1 \u00b5\u03b5(|a 0 | + |q 0 |)| + |q 2 |)) \u2208 1 2 \u2299 (a 0 + q 0 \u2212 q 2 \u00b1 \u00b5\u03b5(3|a 0 | + 3|q 0 | + |q 2 |)) = 1 2 (a 0 + q 0 \u2212 q 2 \u00b1 \u00b5\u03b5(3|a 0 | + 3|q 0 | + |q 2 |)) \u00b1 \u00b5\u03b5 1 2 |a 0 + q 0 \u2212 q 2 \u00b1 \u00b5\u03b5(3|a 0 | + 3|q 0 | + |q 2 |)| \u2208 1 2 (a 0 + q 0 \u2212 q 2 ) \u00b1 1 2 \u00b5\u03b5(7|a 0 | + 7|q 0 | + 3|q 2 |)\n. To summarize, we hav\u1ebd q d+1 =q d+2 = 0 17 We did not attempt to optimize the constants for this analysis.\nq k = 2xq k+1 \u2212q k+2 + a k + \u03b4 k , where |\u03b4 k | \u2a7d \u00b5\u03b5(14|xq k+1 | + 3|q k+2 | + 3|a k |) (28) p = 1 2 (a 0 + q 0 \u2212 q 2 ) + \u03b4, where |\u03b4| \u2a7d 1 2 \u00b5\u03b5(7|a 0 | + 7|q 0 | + 3|q 2 |)(29)\nBy Lemma 7.1, this recurrence satisfies\nq k = d \u2211 i=k U i\u2212k (x)(a i + \u03b4 i ) q k \u2212q k = d \u2211 i=k U i\u2212k (x)\u03b4 i q \u2212q = \u03b4 + 1 2 d \u2211 i=0 U i (x)\u03b4 i \u2212 d \u2211 i=2 U i\u22122 (x)\u03b4 i = \u03b4 + 1 2 \u03b4 0 + d \u2211 i=1 T i (x)\u03b4 i |q \u2212q| \u2a7d |\u03b4| + 1 2 |\u03b4 0 | + d \u2211 i=1 |T i (x)\u03b4 i | \u2a7d |\u03b4| + d \u2211 i=0 |\u03b4 i |.(30)\nThis analysis so far has been fully standard. Let's continue bounding.\n\u2a7d \u00b5\u03b5 7 2 |a 0 | + 7 2 |q 0 | + 3 2 |q 2 | + d \u2211 i=0 (14|xq i+1 | + 3|q i+2 | + 3|a i |) \u2a7d \u00b5\u03b5 d \u2211 i=0 (20|q i | + 10|a i |).(31)\nNow, we will bound all of the \u03b4 k 's. Combining previous facts, we have\n|q k | = d \u2211 i=k U i\u2212k (x)(a i + \u03b4 i ) \u2a7d d \u2211 i=k U i\u2212k (x)a i + d \u2211 i=k U i\u2212k (x)\u03b4 i \u2a7d d \u2211 i=k U i\u2212k (x)a i + d \u2211 i=k (i \u2212 k + 1)|\u03b4 i | \u2a7d d \u2211 i=k U i\u2212k (x)a i + \u00b5\u03b5 d \u2211 i=k (i \u2212 k + 1)(14|q i+1 | + 3|q i+2 | + 3|a i |) \u2a7d 1 \u00b5d + 3\u00b5\u03b5 d \u2212 k + 1 \u00b5 \u2225p\u2225 sup + \u00b5\u03b5 d \u2211 i=k (i \u2212 k + 1)(14|q i+1 | + 3|q i+2 |) \u2a7d 1.5 \u00b5d \u2225p\u2225 sup + \u00b5\u03b5 d \u2211 i=k (i \u2212 k + 1)(14|q i+1 | + 3|q i+2 |) Note that |q k | \u2a7d c k , where c d = 0; c k = 1.5 \u00b5d \u2225p\u2225 sup + \u00b5\u03b5 d \u2211 i=k (i \u2212 k + 1)(14c i+1 + 3c i+2 )\nSolving this recurrence, we have that c k \u2a7d 2 \u00b5d \u2225p\u2225 sup , since by strong induction,\nc k \u2a7d 1.5 \u00b5d + \u00b5\u03b5 d \u2211 i=k (i \u2212 k + 1)17 2 \u00b5d \u2225p\u2225 sup = 1.5 \u00b5d + 17\u00b5\u03b5 1 \u00b5d (d \u2212 k + 1)(d \u2212 k + 2) \u2225p\u2225 sup \u2a7d 2 \u00b5d \u2225p\u2225 sup\nReturning to Equation (31):\n|q \u2212q| \u2a7d \u00b5\u03b5 d \u2211 i=0 (20|q i | + 10|a i |) \u2a7d \u00b5\u03b5 d \u2211 i=0 (20c i + 10|a i |) \u2a7d 40\u03b5\u2225p\u2225 sup + 10\u00b5\u03b5 d \u2211 i=0 |a i | \u2a7d 50\u03b5\u2225p\u2225 sup\n\nBounding the Iterates of the Clenshaw Recurrence\n\nThe goal of this section is to prove Lemma 8.2. In particular, we wish to show that for \u00b5 = \u0398((d 2 log(d)) \u22121 ), the following criteria hold:\n(a) \u00b5\u03b5 \u2a7d 1 50(d+2) 2 ; (b) \u00b5 \u2211 d i=0 |a i | \u2a7d \u2225p\u2225 sup ; (c) \u00b5|q k | = \u00b5|\u2211 d i=k a i U i\u2212k (x)| \u2a7d 1 d \u2225p\u2225 sup for all k \u2208 {0, .\n. . , d}. For this choice of \u00b5, (a) is clearly satisfied, and since |a i | \u2a7d 2\u2225p\u2225 sup (Lemma 4.2), \u00b5 \u2211 d i=0 |a i | \u2a7d 2(d + 1)\u2225p\u2225 sup \u2a7d \u2225p\u2225 sup , so (b) is satisfied. In fact, both of these criterion are satisfied for\n\u00b5 = \u2126(1/d), provided \u03b5 is sufficiently small. Showing (c) requires bounding \u2225\u2211 d \u2113=k a \u2113 U \u2113\u2212k (x)\u2225 sup for all k \u2208 [d]\n. These expressions are also the iterates of the Clenshaw algorithm (Lemma 7.1), so we are in fact trying to show that in the process of our algorithm we never produce a value that's much larger than the final value. From testing computationally, we believe that the following holds true. Conjecture 8.3. Let p(x) be a degree-d polynomial with Chebyshev expansion p(x) = \u2211 d \u2113=0 a \u2113 T \u2113 (x). Then, for all k from 0 to d,\nd \u2211 \u2113=k a \u2113 U \u2113\u2212k (x) sup \u2a7d (d \u2212 k + 1)\u2225p\u2225 sup ,\nmaximized for the Chebyshev polynomial p(x) = T d (x). Conjecture 8.3 would imply that it suffices to take \u00b5 = \u0398(1/d 2 ). We prove it up to a log factor.\nTheorem 8.4. For a degree-d polynomial p(x) = \u2211 d \u2113=0 a \u2113 T \u2113 (x), consider the degree-(d \u2212 k) polynomial q k (x) = \u2211 d \u2113=k a \u2113 U \u2113\u2212k (x). Then \u2225q k \u2225 sup \u2a7d (d \u2212 k + 1) 16 + 16 \u03c0 2 log(d) \u2225p\u2225 sup .\nProof. We proceed by carefully bounding the Chebyshev coefficients of q k , which turn out to be arithmetic progressions of the a k 's which we bounded in Section 6.\nq k (x) = \u2211 i a i U i\u2212k (x) = \u2211 i \u2211 j\u2a7e0 a i T i\u2212k\u22122j (x)(1 + i \u2212 k \u2212 2j \u0338 = 0 ) = \u2211 i \u2211 j\u2a7e0 a i+k+2j T i (x)(1 + i \u0338 = 0 ) = \u2211 i T i (x)(1 + i \u0338 = 0 ) \u2211 j\u2a7e0 a i+k+2j |q k (x)| \u2a7d \u2211 i i \u2a7e 0 (1 + i \u0338 = 0 ) \u2211 j\u2a7e0 a i+k+2j \u2a7d 2 d\u2212k \u2211 i=0 \u2211 j\u2a7e0 a i+k+2j \u2a7d 4 d\u2212k \u2211 i=0 4 + 4 \u03c0 2 log(i + k \u2212 1) \u2225p\u2225 sup by Fact 6.3 \u2a7d (d \u2212 k + 1) 16 + 16 \u03c0 2 log(d) \u2225p\u2225 sup .\nRemark 8.5. We spent some time trying to prove Conjecture 8.3, since its form is tantalizingly close to that of the Markov brothers' inequality [Sch41] \u2225 d dx p(x)\u2225 sup = \u2225\u2211 d \u2113=0 a \u2113 \u2113U \u2113\u22121 (x)\u2225 sup \u2a7d d 2 \u2225p(x)\u2225 sup , except with the linear differential operator d dx : T \u2113 \u2192 \u2113U \u2113\u22121 replaced with the linear operator T \u2113 \u2192 U \u2113\u2212k . However, calculations suggest that the variational characterization of max \u2225p\u2225 sup =1 | d dx p(x)| underlying proofs of the Markov brothers' inequality [Sha04] does not hold here, and from our shallow understanding of these proofs, it seems that they strongly use properties of the derivative.\n\n\nComputing Matrix Polynomials\n\nOur goal is to prove the following theorem:\n\nTheorem 9.1. Suppose we are given sampling and query access to A \u2208 C m\u00d7n and b \u2208 C n with \u2225A\u2225 \u2a7d 1; a an even or odd degree-d polynomial p with p(0) = 0, given as its Chebyshev coefficients; and a sufficiently small accuracy parameter \u03b5 > 0. Then we can output a description of a vector y (in C m if p is odd, in C n if p is even) such that \u2225y \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225 with probability \u2a7e 0.9 in time\nO min nnz(A), \u2225A\u2225 4 F \u03b5 4 d 12 log 8 (d) log 2 \u2225A\u2225 F \u2225A\u2225 + \u2225A\u2225 4 F \u03b5 2 d 11 log 4 (d) log \u2225A\u2225 F \u2225A\u2225 .\nWe can access the output description in the following way:\n(i) Compute entries of y in O \u2225A\u2225 2 F \u03b5 2 d 6 log 4 (d) log \u2225A\u2225 F \u2225A\u2225 time; (ii) Sample i \u2208 [n] with probability |y i | 2 \u2225y\u2225 2 in O \u2225p\u2225 2 sup \u2225A\u2225 4 F \u2225b\u2225 2 \u03b5 2 \u2225y\u2225 2 d 8 log 8 (d) log \u2225A\u2225 F \u2225A\u2225 time with proba- bility \u2a7e 0.9; (iii) Estimate \u2225y\u2225 2 to \u03bd relative error in O \u2225p\u2225 2 sup \u2225A\u2225 4 F \u2225b\u2225 2 \u03bd 2 \u03b5 2 \u2225y\u2225 2 d 8 log 8 (d) log \u2225A\u2225 F \u2225A\u2225 time with probability \u2a7e 0.9.\nWe prove the even and odd cases separately. We can conclude the above theorem as a consequence of Theorems 9.3 and 9.7 and Corollaries 9.4 and 9.8. Remark 9.2. We can also compute estimate \u27e8u|y\u27e9 to \u03b5\u2225u\u2225\u2225b\u2225 error without worsening the 1/\u03b5 2 dependence. This does not follow from the above; rather, we can observe that, from the description of our output, y = (AS)v + \u03b7b, it suffices to estimate u \u2020 (AS)v and u \u2020 b. Because of properties of the sketches and the later analysis, \u2225AS\u2225 F \u2272 \u2225A\u2225 F and \u2225v\u2225 \u2272 d log 2 (d)\u2225b\u2225, so by [CGLLTW22, Lemma 4.12 and Remark 4.13] we can estimate these to the desired error with O(d 2 log 4 (d)\u2225A\u2225 2 F 1 \u03b5 2 log 1 \u03b4 ) queries to u, v, A and samples to AS. All of these queries can be done in O(1) time.\n\n\nComputing Odd Matrix Polynomials\n\nIn this section, we prove the following theorem. The statement involves a parameter \u00b5 which depends on the polynomial being evaluated; this parameter is between 1 and (d log d) \u22122 , depending on how well-conditioned the polynomial is. Theorem 9.3. Suppose we are given sampling and query access to A \u2208 C m\u00d7n and b \u2208 C n with \u2225A\u2225 \u2a7d 1; a (2d + 1)-degree odd polynomial p, written in its Chebyshev coefficients as\np(x) = d \u2211 i=0 a 2i+1 T 2i+1 (x);\nan accuracy parameter \u03b5 > 0; a failure probability parameter \u03b4 > 0; and a stability parameter \u00b5 > 0.\n\nThen we can output a vector x \u2208 C n such that \u2225Ax \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225 with probability\n\u2a7e 1 \u2212 \u03b4 in time O min nnz(A), d 4 \u2225A\u2225 4 F (\u00b5\u03b5) 4 log 2 \u2225A\u2225 F \u03b4\u2225A\u2225 + d 7 \u2225A\u2225 4 F (\u00b5\u03b5) 2 \u03b4 log \u2225A\u2225 F \u03b4\u2225A\u2225 ,\nassuming \u00b5\u03b5 < min( 1 4 d\u2225A\u2225, 1 100d ) and \u00b5 satisfies the following bounds:\n(a) \u00b5 \u2211 d i=0 |a 2i+1 | \u2a7d \u2225p\u2225 sup ; (b) \u00b5\u2225\u2211 d i=k a 2i+1 U i\u2212k (T 2 (x))\u2225 sup \u2a7d 1 d \u2225p\u2225 sup for all 0 \u2a7d k \u2a7d d;\nThe output description has the additional properties\n\u2211 j A * ,j 2 x j 2 \u2272 \u03b5 2 \u2225p\u2225 2 sup \u2225b\u2225 2 d 4 log \u2225A\u2225 F \u03b4\u2225A\u2225 \u2225x\u2225 0 \u2272 d 2 \u2225A\u2225 2 F (\u00b5\u03b5) 2 log \u2225A\u2225 F \u03b4\u2225A\u2225 ,\nso that by Corollary 4.10, for the output vector y := Ax, we can:\n(i) Compute entries of y in O(\u2225x\u2225 0 ) = O d 2 \u2225A\u2225 2 F (\u00b5\u03b5) 2 log \u2225A\u2225 F \u03b4\u2225A\u2225 time;\n(ii) Sample i \u2208 [n] with probability |y i | 2 \u2225y\u2225 2 in O \u2225p\u2225 2 sup \u2225A\u2225 4 F \u2225b\u2225 2 \u00b5 4 \u03b5 2 \u2225y\u2225 2 log \u2225A\u2225 F \u03b4\u2225A\u2225 log 1 \u03b4 time with probability \u2a7e 1 \u2212 \u03b4;\n(iii) Estimate \u2225y\u2225 2 to \u03bd relative error in O \u2225p\u2225 2 sup \u2225A\u2225 4 F \u2225b\u2225 2 \u03bd 2 \u00b5 4 \u03b5 2 \u2225y\u2225 2 log \u2225A\u2225 F \u03b4\u2225A\u2225 log 1 \u03b4 time with probability \u2a7e 1 \u2212 \u03b4.\nThe criterion for what \u00b5 need to be are somewhat non-trivial; the important requirement is Item 9.3(b), which states that 1/\u00b5 is a bound on the norm of various polynomials. These polynomials turn out to be iterates of Eq. (Odd Clenshaw) when computing p(x)/x, so roughly speaking, the algorithm we present depends on the numerical stability of evaluating p(x)/x. This is necessary because we primarily work in the \"dual\" space, maintaining our Clenshaw iterate u k as Av k where v k is a sparse vector. This bears a resemblance to the dependence in [CGLLTW22, Theorem 5.1] on the Lipschitz smoothness of f (x)/x. For any bounded polynomial, we can always take \u00b5 to be \u2126((d log(d)) \u22122 ). Corollary 9.4 (Corollary of Proposition 9.9). In Theorem 9.3, we can always take 1/\u00b5 \u2272 d 2 log 2 (d) for d > 1.\n\nThis bound is achieved up to log factors by p(x) = T 2k+1 (x). We are now ready to dive into the proof of Theorem 9.3. Without loss of generality, we assume \u2225p\u2225 sup = 1.\n\n\nAlgorithm 2 (Odd singular value transformation).\n\nInput (pre-processing): A matrix A \u2208 C m\u00d7n , vector b \u2208 C n , and parameters \u03b5, \u03b4, \u00b5 > 0.\n\nPre-processing sketches: Let s, t = \u0398\nd 2 \u2225A\u2225 2 F (\u00b5\u03b5) 2 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )\n. This phase will succeed with probability \u2a7e 1 \u2212 \u03b4. Input: A degree 2d + 1 polynomial p(x) = \u2211 d i=0 a 2i+1 T 2i+1 (x) given as its coefficients a 2i+1 .\n\n\nClenshaw iteration:\nLet r = \u0398(d 4 \u2225A\u2225 2 F (s + t) 1 \u03b4 ) = \u0398(d 6 \u2225A\u2225 4 F (\u00b5\u03b5) 2 \u03b4 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )\n). This phase will succeed with probability \u2a7e 1 \u2212 \u03b4. Starting with v d+1 = v d+2 = \u20d7 0 s and going until v 0 ,\nI1. Let B (k) = BEST r (TAS) and B (k) \u2020 = BEST r ((TAS) \u2020 ) (Definition 5.1); I2. Compute v k = 2(2B (k) \u2020 B (k) \u2212 I)v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b. Output: Output x = 1 2 S(v 0 \u2212 v 1 ) satisfying \u2225Ax \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225.\nPre-processing sketches, running time. Given a matrix A \u2208 C m\u00d7n and a vector b \u2208 C n , the pre-processing phase of Algorithm 2 can be performed in O(nnz(A) + nnz(b)) time. First, we build a data structure to respond to SQ(A \u2020 ) and SQ(b) queries in O(1) time using the alias data structure described in Remark 4.9 (A2.P1). Then, we use these accesses to construct an AMP sketch S for Ab, where we produce the samples for the sketch by sampling from b and sampling from the row norms of A \u2020 , each with probability 1 2 (A2.P2). Since we need s samples, this takes O(s) queries to the data structure. The sketch S is defined such that AS is a subset of the columns of A, with each column rescaled according to the probability it was sampled. So, with another pass through A, we can construct a data structure for SQ(AS) in O(1) time using Remark 4.9 and use this to construct an AMP sketch T \u2020 for (AS) \u2020 AS (A2.P3). The samples for the sketch are drawn from the row norms of AS. The final matrix TAS is a rescaled submatrix of A; another O(nnz(A)) pass through A suffices to construct a data structure to respond to SQ(TAS) queries in O(1) time (A2.P4). The running time is O(nnz(A) + nnz(b) + s + t) or, alternatively, three passes through A and one pass with b, using O(st) space. We can assume that s, t \u2a7d nnz(A), though: if s \u2a7e n, then we can take S = I, and if t \u2a7e m, then we can take T = I, and this will satisfy the same guarantees; further, without loss, nnz(A) \u2a7e max(m, n), since otherwise there is an empty row or column that we can ignore.\n\nIf we are given A, b such that SQ(A \u2020 ) and SQ(b) queries can be performed in O(Q) time, then the pre-processing phase of Algorithm 2 can be performed in O(Qst) time. The main difference from the description above is that we use that, given SQ(A \u2020 ), we can simulate queries to SQ(AS) Remark 9.5. If we are not given b until after the pre-processing phase, or if we are only given b as a list of entries without SQ(b), then we can take the S sketch to just be sampling from the row norms of A \u2020 . This will decrease the success probability of the following phase (specifically, because of the guarantee in Eq. (Ab AMP)) to 0.99.\n\nPre-processing sketches, correctness. We list the guarantees of the sketch that we will use in the error analysis, and point to where they come from in Section 5.2. Recall that, with S \u2208 C n\u00d7s taken to be an AMP sketch of X \u2208 C m\u00d7n , Y \u2208 C n\u00d7d , by Corollary 5.6, with probability\n\u2a7e 1 \u2212 \u03b4, \u2225XSS \u2020 Y \u2020 \u2212 XY \u2020 \u2225 \u2a7d \u03b5\u2225X\u2225\u2225Y\u2225 provided s = \u2126( 1 \u03b5 2 ( \u2225X\u2225 2 F \u2225X\u2225 2 + \u2225Y\u2225 2 F \u2225Y\u2225 2 ) log( 1 \u03b4 ( \u2225X\u2225 2 F \u2225X\u2225 2 + \u2225Y\u2225 2 F\n\u2225Y\u2225 2 ))) and \u03b5 \u2a7d 1. We will use this with s, t = \u0398(\nd 2 \u2225A\u2225 2 F (\u00b5\u03b5) 2 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )), where \u00b5\u03b5 d \u2a7d 1 4 \u2225A\u2225.\nThe guarantees of Corollary 5.6 individually fail with probability O(\u03b4), so we will rescale to say that they all hold with probability \u2a7e 1 \u2212 \u03b4. The following bounds hold for the sketch S. \n\u2225AA \u2020 \u2212 AS(AS) \u2020 \u2225 \u2a7d \u00b5\u03b5 d \u2225A\u2225 by Corollary 5.6 (AA \u2020 AMP) \u2225AS\u2225 2 = \u2225AS(AS) \u2020 \u2225 \u2a7d (1 + \u00b5\u03b5 d\u2225A\u2225 )\u2225A\u2225 2 by Eq. (AA \u2020 AMP) (\u2225AS\u2225 bd)\nThe following bounds hold for the sketch T. \nu k = 2(2AA \u2020 \u2212 I)u k+1 \u2212 u k+2 + 2a 2k+1 Ab, (Odd Matrix Clenshaw) u = 1 2 (u 0 \u2212 u 1 ).\nWe now show how to compute the next iterate u k given b and the previous two iterates as v k+1 , v k+2 \u2208 C s where u k+1 = ASv k+1 and u k+2 = ASv k+2 , for all k \u2a7e 0. The analysis begins by\n\nshowing that u k is \u03b5\u2225v k+1 \u2225-close to the output of an exact, zero-error iteration. Next, we show that \u2225v k \u2225 is O(d)-close to its zero-error iteration value. Finally, we show that these errors don't accumulate too much towards the final outcome.\n\nStarting from Eq. (Odd Matrix Clenshaw), we take the following series of approximations by applying intermediate sketches:\n4AA \u2020 u k+1 \u2212 2u k+1 \u2212 u k+2 + a 2k+1 Ab \u2248 1 4AS(AS) \u2020 u k+1 \u2212 2u k+1 \u2212 u k+2 + a 2k+1 Ab = AS 4(AS) \u2020 (AS)v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 Ab \u2248 2 AS 4(TAS) \u2020 (TAS)v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 Ab \u2248 3 AS 4(TAS) \u2020 (TAS)v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b (32) \u2248 4 AS(4(TAS) \u2020 B (k) v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b) \u2248 5 AS(4B (k) \u2020 B (k) v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b).(33)\nThe final expression, Eq. (33), is what the algorithm computes, taking\nv k := (4B (k) \u2020 B (k) \u2212 2I)v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b.(34)\nHere, B (k) and B\n\n(k) \u2020 are taken to be BEST r (TAS) and BEST r ((TAS)\n\u2020 ) for r = \u0398(d 4 \u2225A\u2225 2 F (s + t) 1 \u03b4 ) = \u0398(d 6 \u2225A\u2225 4 F (\u00b5\u03b5) 2 \u03b4 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )\n). The runtime of each iteration is O(r), since the cost of producing the sketches B (k) and B Remark 9.6. We could have stopped sketching at Eq. (32), without using BEST, and instead \ntook v k := 4(TAS) \u2020 (TAS)v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b.O d 5 \u2225A\u2225 4\nF \u00b5 4 \u03b5 4 log 2 \u2225A\u2225 F \u03b4\u2225A\u2225 to achieve the guarantees of Theorem 9.3 with probability \u2a7e 1 \u2212 \u03b4. This running time is worse by a factor of d 2 /\u03b5 2 , but scales logarithmically in failure probability.\n\nAs for approximation error, let \u03b5 1 , \u03b5 2 , \u03b5 3 , \u03b5 4 and \u03b5 5 be the errors introduced in the approximation steps for Eq. (33). Using the previously established bounds on S and T,\n\u03b5 1 \u2a7d 4\u2225AA \u2020 \u2212 AS(AS) \u2020 \u2225\u2225u k+1 \u2225 \u2a7d 4 \u00b5\u03b5 d \u2225A\u2225\u2225u k+1 \u2225 by Eq. (AA \u2020 AMP) \u03b5 2 \u2a7d 4\u2225AS\u2225\u2225((AS)(AS) \u2020 \u2212 (TAS)(TAS) \u2020 )v k+1 \u2225 \u2a7d 16 \u00b5\u03b5 d \u2225A\u2225 2 \u2225v k+1 \u2225 by Eq. ((AS) \u2020 AS AMP) \u03b5 3 \u2a7d |a 2k+1 |\u2225Ab \u2212 ASS \u2020 b\u2225 \u2a7d |a 2k+1 | \u00b5\u03b5 d \u2225b\u2225 by Eq. (Ab AMP)\nThe bounds on \u03b5 4 and \u03b5 5 follow from the bounds in Section 5.1 applied to TAS. With probability \u2a7e 1 \u2212 \u03b4/d, the following hold:\n\u03b5 4 \u2a7d 4\u2225AS(TAS) \u2020 (TAS \u2212 B (k) )v k+1 \u2225 \u2a7d 4\u2225AS(TAS) \u2020 \u2225 F \u2225TAS\u2225 F \u2225v k+1 \u2225 d/(r\u03b4) by Corollary 5.3 \u2a7d \u2225AS(TAS) \u2020 \u2225 F \u2225TAS\u2225 F \u2225v k+1 \u2225 \u00b5\u03b5 12\u2225A\u2225 2 F d 5/2 \u2a7d d \u22125/2 \u00b5\u03b5\u2225A\u2225\u2225v k+1 \u2225\nby Eqs. (\u2225AS\u2225 bd) and (\u2225TAS\u2225 F bd) \n\u03b5 5 \u2a7d 4\u2225AS((TAS) \u2020 \u2212 B (k) \u2020 )B (k) v k+1 \u2225 \u2a7d 4\u2225AS\u2225 F \u2225TAS\u2225 F \u2225B (k) v k+1 \u2225 d/(r\u03b4)\u2a7d d \u22125/2 \u00b5\u03b5\u2225v k+1 \u2225. by \u2225A\u2225 \u2a7d 1\nIn summary, we can view the iterate of A2.I2 as computing\nu k = 2(2AA \u2020 \u2212 I) u k+1 \u2212 u k+2 + 2a 2k+1 Ab + \u03b5 (k)(35)\nWhere \u03b5 (k) \u2208 C m is the error of the approximation in the iterate Eq. (33). We have showed that\n\u2225\u03b5 (k) \u2225 \u2a7d \u03b5 1 + \u03b5 2 + \u03b5 3 + \u03b5 4 + \u03b5 5 \u2272 \u00b5\u03b5 d \u2225v k+1 \u2225 + |a 2k+1 |\u2225b\u2225 .\nUpon applying a union bound, we see that this bound on \u03b5 (k) holds for every k from 0 to d \u2212 1 with probability \u2a7e 1 \u2212 3\u03b4.\n\nError accumulation across iterations. Now, we analyze how the error from one iteration affects to the final output. Using the formulation of the iterate from Eq. (35), we notice that this is the standard Clenshaw iteration Eq. (Clenshaw) with x replaced with T 2 (A \u2020 ) = 2AA \u2020 \u2212 I and a k replaced with 2a 2k+1 Ab + \u03b5 (k) . Following Remark 7.4 and Lemma 7.3, we conclude that the output of Algorithm 2 satisfies\nu k = d \u2211 i=k U i\u2212k (T 2 (A \u2020 ))(2a 2i+1 Ab + \u03b5 (k) ) u := 1 2 ( u 0 \u2212 u 1 ) = d \u2211 i=0 1 2 (U i (T 2 (A \u2020 )) \u2212 U i\u22121 (T 2 (A \u2020 )))(2a 2i+1 Ab + \u03b5 (k) ) = d \u2211 i=0 a 2i+1 T 2i+1 (A)b + d \u2211 i=0 1 2 (U i (T 2 (A \u2020 )) \u2212 U i\u22121 (T 2 (A \u2020 )))\u03b5 (k)\nIn other words, after completing the iteration, we have a vector u such that\n\u2225 u \u2212 p(A)b\u2225 \u2a7d d \u2211 i=0 1 2 (U i (T 2 (A \u2020 )) \u2212 U i\u22121 (T 2 (A \u2020 )))\u03b5 (k) \u2a7d d \u2211 i=0 (2i + 1)\u2225\u03b5 (k) \u2225 \u2272 \u00b5\u03b5 d \u2211 k=0 (\u2225v k+1 \u2225 + |a 2k+1 |\u2225b\u2225) \u2a7d \u03b5\u2225b\u2225 + \u00b5\u03b5 d \u2211 k=1 \u2225v k \u2225(36)\nThe last step follows from Item 9.3(a). So, it suffices to bound the v k 's. Recalling from Eq. (34), the recursions defining them is\nv k = 4B (k) \u2020 B (k) v k+1 \u2212 2v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b = 2(2(TAS) \u2020 (TAS) \u2212 I)v k+1 \u2212 v k+2 + a 2k+1 S \u2020 b + 4(B (k) \u2020 B (k) \u2212 (TAS) \u2020 (TAS))v k+1\nThis is Eq. (Odd Matrix Clenshaw) on the matrix (TAS) \u2020 with an additional error term. Following Remark 7.4, this solves to\nv k = d \u2211 i=k U i\u2212k (T 2 (TAS)) a 2i+1 S \u2020 b + 4(B (i) \u2020 B (i) \u2212 (TAS) \u2020 (TAS))v i+1 .\nSince B (k) and B  \nE k (B (k) \u2020 B (k) \u2212 (TAS) \u2020 (TAS))v k+1 2 = E k B (k) \u2020 B (k) v k+1 2 \u2212 (TAS) \u2020 (TAS)v k+1 2 = E k (B (k) v k+1 ) \u2020 (B (k) \u2020 ) \u2020 B (k) \u2020 (B (k) v k+1 ) \u2212 (TAS) \u2020 (TAS)v k+1 2 \u2a7d E k (B (k) v k+1 ) \u2020 ((TAS)(TAS) \u2020 + 1 r Tr(I s )\u2225TAS\u2225 2 F I t )(B (k) v k+1 ) \u2212 (TAS) \u2020 (TAS)v k+1 2 \u2a7d E k v \u2020 k+1 (TAS) \u2020 ((TAS)(TAS) \u2020 + s r \u2225TAS\u2225 2 F I t )(TAS)v k+1 + v \u2020 k+1 1 r Tr((TAS)(TAS) \u2020 + s r \u2225TAS\u2225 2 F I t )\u2225TAS\u2225 2 F v k+1 \u2212 (TAS) \u2020 (TAS)v k+1 2 = s r \u2225TAS\u2225 2 F \u2225TASv k+1 \u2225 2 + 1 r + st r 2 \u2225TAS\u2225 4 F \u2225v k+1 \u2225 2 \u2a7d 4\u2a7d \u03b4 1000d 4 \u2225A\u2225 2 \u2225v k+1 \u2225 2 ,(37)\nwhere the last line uses r = \u0398(d 4 \u2225A\u2225 2 F (s + t) 1 \u03b4 ) (and is the bottleneck for the choice of r). Let E [k,d] denote taking the expectation over B (i) and B (i) \u2020 for i between k and d (treating T, S as fixed).v\nk := E [k,d] [v k ] = d \u2211 i=k U i\u2212k (T 2 (TAS))a 2i+1 S \u2020 b.\nWe first bound the recurrence in expectation, then we bound the second moment. We now compute the second moment of v k .\n\u2225v k \u2225 \u2a7d d \u2211 i=k U i\u2212k (T 2 (TAS))a 2i+1 \u2225S \u2020 b\u2225 by sub-multiplicativity of \u2225\u00b7\u2225 = d \u2211 i=k U i\u2212k (T 2 (x))a 2i+1 Spec(TAS) \u2225S \u2020 b\u2225 \u2a7d d \u2211 i=k U i\u2212k (T 2 (x))a 2i+1 [\u22121\u22122 \u00b5\u03b5 d ,E [k,d] \u2225v k \u2212v k \u2225 2 = E [k,d] d \u2211 i=k U i\u2212k (T 2 (TAS))4(B (i) \u2020 B (i) \u2212 (TAS) \u2020 (TAS))v i+1 2\nBecause the B (i) 's are independent, the variance of the sum is the sum of the variances, so\n= d \u2211 i=k E [k,d] U i\u2212k (T 2 (TAS))4(B (i) \u2020 B (i) \u2212 (TAS) \u2020 (TAS))v i+1 2 \u2a7d 16 d \u2211 i=k U i\u2212k (T 2 (TAS)) 2 E [k,d] (B (i) \u2020 B (i) \u2212 (TAS) \u2020 (TAS))v i+1 2 \u2a7d 16 d \u2211 i=k e 2 d 2 E [k,d] (B (i) \u2020 B (i) \u2212 (TAS) \u2020 (TAS))v i+1 2 \u2a7d 16e 2 d \u2211 i=k d 2 \u03b4 1000d 4 \u2225A\u2225 2 E [i+1,d] \u2225v i+1 \u2225 2 \u2a7d \u03b4 2d 2 \u2225A\u2225 2 d \u2211 i=k E [i+1,d] \u2225v i+1 \u2225 2 = \u03b4 2d 2 \u2225A\u2225 2 d \u2211 i=k ( E [i+1,d] \u2225v i+1 \u2212v i+1 \u2225 2 + \u2225v i+1 \u2225 2 ) \u2a7d \u03b4 2d 2 \u2225A\u2225 2 d \u2211 i=k ( E [i+1,d] \u2225v i+1 \u2212v i+1 \u2225 2 + 16 \u00b5 2 d 2 \u2225b\u2225 2 ) by Eq. (37)\nTo bound this recurrence, we define the following recurrence c k to satisfy E [k,d] [\u2225v k \u2212v k \u2225 2 ] \u2a7d c k :\nc k = \u03b3 d \u2211 i=k (c i+1 + \u0393) \u03b3 = \u03b4 2d 2 \u2225A\u2225 2 , \u0393 = 16 \u00b5 2 d 2 \u2225b\u2225 2 .\nFor this recurrence, c k \u2a7d d\u03b3\u0393 for all k between 0 and d provided that d\u03b3 \u2a7d 1 2 .\n\n\u2a7d 8\u03b4 \n\u00b5 2 d 3 \u2225A\u2225 2 \u2225b\u2225 2 We have shown that E[\u2225v k \u2212v k \u2225 2 ] \u2a7d 8\u03b4 d ( \u2225A\u2225\u2225b\u2225 \u00b5d ) 2 .\u2225 u \u2212 p(A)b\u2225 \u2272 \u03b5\u2225b\u2225 + \u00b5\u03b5 d \u2211 k=1 \u2225v k \u2225 \u2272 \u03b5\u2225b\u2225.(38)\nOutput description properties. After the iteration concludes, we can compute u by computing\nx = 1 2 S(v 0 \u2212 v 1 ) in linear O(s) time. Then, u = 1 2 (u 0 \u2212 u 1 ) = 1 2 AS(v 0 \u2212 v 1 ) = Ax.\nNote that though x \u2208 C n , its sparsity is at most the sparsity of x, which is bounded by s.\n\nFurther, using the prior bounds on v 0 and v 1 , we have that\nn \u2211 j=1 A * ,j 2 |x i | 2 = s \u2211 j=1 [SA] * ,j 2 1 2 (v 0 \u2212 v 1 ) i 2 \u2a7d s \u2211 j=1 2 s \u2225A\u2225 2 F 1 2 (v 0 \u2212 v 1 ) i 2 \u2a7d 2 s \u2225A\u2225 2 F 1 2 (v 0 \u2212 v 1 ) 2 \u2a7d 2 s \u2225A\u2225 2 F (\u2225v 0 \u2225 2 + \u2225v 1 \u2225 2 ) \u2272 \u2225A\u2225 2 F \u2225b\u2225 2 /( \u221a s\u00b5d) 2\n\u2272 \u03b5 2 \u2225b\u2225 2 /(d 4 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )).\n\n\nGeneralizing to Even Polynomials\n\nWe also obtain an analogous result for even polynomials. For the most part, changes are superficial; the red text indicates differences from Theorem 9.3. The major difference is that the representation of the output is Ax + \u03b7b instead of Ax, which results from constant terms being allowed when p is even. We state the theorem for estimating p(A \u2020 )b because it makes the similarities with the odd setting more apparent.\n\nTheorem 9.7. Suppose we are given sampling and query access to A \u2208 C m\u00d7n and b \u2208 C m with \u2225A\u2225 \u2a7d 1; a (2d)-degree even polynomial, written in its Chebyshev coefficients as\np(x) = d \u2211 i=0 a 2i T 2i (x);\nan accuracy parameter \u03b5 > 0; a failure probability parameter \u03b4 > 0; and a stability parameter \u00b5 > 0.\n\nThen we can output a vector x \u2208 C n and \u03b7 \u2208 C such that \u2225Ax + \u03b7b \u2212 p(A \u2020 )b\u2225 \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225 with\nprobability \u2a7e 1 \u2212 \u03b4 in time O min nnz(A), d 4 \u2225A\u2225 4 F (\u00b5\u03b5) 4 log 2 \u2225A\u2225 F \u03b4\u2225A\u2225 + d 7 \u2225A\u2225 4 F (\u00b5\u03b5) 2 \u03b4 log \u2225A\u2225 F \u03b4\u2225A\u2225 ,\nassuming \u00b5\u03b5 < min( 1 4 d\u2225A\u2225, 1 100d ) and \u00b5 satisfies the following bounds. Below,\u00e3 2k := a 2k \u2212 a 2k+2 + \u00b7 \u00b7 \u00b7 \u00b1 a 2d . \n(a) \u00b5 \u2211 d i=1 |\u00e3 2i | \u2a7d \u2225p\u2225 sup and d\u00b5 2 \u2211 d i=1 |\u00e3 2i | 2 \u2a7d \u2225p\u2225 2 sup ; (b) \u00b5\u2225\u2211 d i=k 4\u00e3 2i+2 x \u00b7 U i\u2212k (T 2 (x))\u2225 sup \u2a7d 1 d \u2225p\u2225 sup for all 0 \u2a7d k \u2a7d d.+ p(0) 2 d 2 \u2225A\u2225 2 F \u2225b\u2225 2 \u03bd 2 \u00b5 2 \u03b5 2 \u2225y\u2225 2 log( \u2225A\u2225 F \u03b4\u2225A\u2225 ) log 1 \u03b4 time with probability \u2a7e 1 \u2212 \u03b4.\nCorollary 9.8 (Corollary of Proposition 9.10). In Theorem 9.7, we can always take 1/\u00b5 \u2272 d 2 log(d) for d > 1.\n\n\nAlgorithm 3 (Even singular value transformation).\n\nInput (pre-processing): A matrix A \u2208 C m\u00d7n , vector b \u2208 C m , and parameters \u03b5, \u03b4, \u00b5 > 0.\n\nPre-processing sketches: Let s, t = \u0398\nd 2 \u2225A\u2225 2 F (\u00b5\u03b5) 2 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )\n. This phase will succeed with probability \u2a7e 1 \u2212 \u03b4. \nP3. Sample T \u2020 \u2208 C m\u00d7t to be AMP t S \u2020 A \u2020 , AS, { 1 2 ( \u2225[AS] i, * \u2225 2 \u2225AS\u2225 2 F + |b i | 2 \u2225b\u2225 2 )} i\u2208[m] (Defini- tion 5.4);\nP4. Compute a data structure that can respond to SQ(TAS) queries in O(1) time;\nInput: A degree 2d polynomial p(x) = \u2211 d i=0 a 2i T 2i (x)\ngiven as its coefficients a 2i . Compute all\u00e3 2k = a 2k \u2212 a 2k+2 + \u00b7 \u00b7 \u00b7 \u00b1 a 2d .\n\n\nClenshaw iteration:\nLet r = \u0398(d 4 \u2225A\u2225 2 F (s + t) 1 \u03b4 ) = \u0398(d 6 \u2225A\u2225 4 F (\u00b5\u03b5) 2 \u03b4 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )\n). This phase will succeed with probability \u2a7e 1 \u2212 \u03b4. Starting with v d+1 = v d+2 = \u20d7 0 s and going until v 0 , I1. Let B (k) = BEST r (TAS) and B (k) \u2020 = BEST r ((TAS) \u2020 ) (Definition 5.1);\nI2. Compute v k = 2(2B (k) \u2020 B (k) \u2212 I)v k+1 \u2212 v k+2 + 4\u00e3 2k+2 B (k) \u2020 Tb. Output: Output x = 1 2 S(v 0 \u2212 v 1 ) and \u03b7 =\u00e3 0 satisfying Ax + \u03b7b \u2212 p(A \u2020 )b \u2a7d \u03b5\u2225p\u2225 sup \u2225b\u2225.\nRecall the odd and even recurrences defined in Eqs. (Odd Clenshaw) and (Even Clenshaw).\nu k = 2(2AA \u2020 \u2212 I)u k+1 \u2212 u k+2 + 2a 2k+1 Ab, (Odd Matrix Clenshaw) p(A)b = u = 1 2 (u 0 \u2212 u 1 ).\nThe matrix analogue of the even recurrence is identical except that the final term is 4\u00e3 2k+2 AA \u2020 b instead of 2a 2k+1 Ab.\na 2k := a 2k \u2212 a 2k+2 + a 2k+4 \u2212 \u00b7 \u00b7 \u00b7 \u00b1 a 2d u k = 2(2AA \u2020 \u2212 1)u k+1 \u2212 u k+2 + 4\u00e3 2k+2 AA \u2020 b, (Even Matrix Clenshaw) p(A \u2020 )b = u =\u00e3 0 b + 1 2 (u 0 \u2212 u 1 ).\nSo, a roughly identical analysis works upon making the appropriate changes. As before, we assume \u2225p\u2225 sup = 1 without loss of generality. In summary, we can view the iterate of A2.I2 as computing\nu k = 2(2AA \u2020 \u2212 I) u k+1 \u2212 u k+2 + 4\u00e3 2k+2 AA \u2020 b + \u03b5 (k)(41)\nWhere \u03b5 (k) \u2208 C m is the error of the approximation in the iterate Eq. (33), and\n\u2225\u03b5 (k) \u2225 \u2a7d \u03b5 1 + \u03b5 2 + \u03b5 3 + \u03b5 4 + \u03b5 5 + \u03b5 6 + \u03b5 7 \u2272 \u00b5\u03b5 d \u2225v k+1 \u2225 + |\u00e3 2k+2 |\u2225b\u2225 .\nUpon applying a union bound, we see that this bound on \u03b5 (k) holds for every k from 0 to d \u2212 1 with probability \u2a7e 1 \u2212 4\u03b4.\n\nError accumulation across iterations. The error accumulates in the same way as in the odd setting. Using the formulation of the iterate from Eq. (41), we notice that this is the standard Clenshaw iteration Eq. (Clenshaw) with x replaced with T 2 (A \u2020 ) = 2AA \u2020 \u2212 I and a k replaced with 4\u00e3 2k+2 AA \u2020 b + \u03b5 (k) . Following Remark 7.4 and Lemma 7.3, we conclude that the output of Algorithm 3 satisfies\nu k = d \u2211 i=k U i\u2212k (T 2 (A \u2020 ))(4\u00e3 2i+2 AA \u2020 b + \u03b5 (i) ) u :=\u00e3 0 + 1 2 ( u 0 \u2212 u 1 ) =\u00e3 0 + d \u2211 i=0 1 2 (U i (T 2 (A \u2020 )) \u2212 U i\u22121 (T 2 (A \u2020 )))(4\u00e3 2i+2 AA \u2020 b + \u03b5 (k) ) = d \u2211 i=0 a 2i T 2(i\u2212k) (A \u2020 )b + d \u2211 i=0 1 2 (U i (T 2 (A \u2020 )) \u2212 U i\u22121 (T 2 (A \u2020 )))\u03b5 (k)\nIn other words, after completing the iteration, we have a vector u such that\n\u2225 u \u2212 p(A \u2020 )b\u2225 \u2a7d d \u2211 i=0 1 2 (U i (T 2 (A \u2020 )) \u2212 U i\u22121 (T 2 (A \u2020 )))\u03b5 (k) \u2a7d d \u2211 i=0 (2i + 1)\u2225\u03b5 (k) \u2225 \u2272 \u00b5\u03b5 d \u2211 k=0 (\u2225v k+1 \u2225 + |\u00e3 2k+2 |\u2225b\u2225) \u2a7d \u03b5\u2225b\u2225 + \u00b5\u03b5 d \u2211 k=1 \u2225v k \u2225(42)\nIn the last line, we use the assumption Item 9.7(a). It suffices to bound the v k 's. Recalling from Eq. (40), the recursions defining them is\nv k = 4B (k) \u2020 B (k) v k+1 \u2212 2v k+1 \u2212 v k+2 + 4\u00e3 2k+2 B (k) \u2020 Tb = 2(2(TAS) \u2020 (TAS) \u2212 I)v k+1 \u2212 v k+2 + 4\u00e3 2k+2 B (k) \u2020 Tb + 4(B (k) \u2020 B (k) \u2212 (TAS) \u2020 (TAS))v k+1\nFollowing Remark 7.4, this solves to\nv k = d \u2211 i=k U i\u2212k (T 2 (TAS)) 4\u00e3 2i+2 B (i) \u2020 Tb + 4(B (i) \u2020 B (i) \u2212 (TAS) \u2020 (TAS))v i+1 .\nFrom here, the identical analysis applies.\nv k := E [k,d] [v k ] = d \u2211 i=k U i\u2212k (T 2 (TAS))4\u00e3 2i+2 (TAS) \u2020 Tb \u2225v k \u2225 \u2a7d d \u2211 i=k U i\u2212k (T 2 (TAS))4\u00e3 2i+2 (TAS) \u2020 \u2225Tb\u2225 \u2a7d 4 1 \u00b5d \u2225b\u2225.\nWe now compute the second moment of v k . The main difference from the odd setting is that there is an additional term, 4\u00e3 2k+2 (B \n(k) \u2020 \u2212 (TAS) \u2020 )Tb, where E k (B (k) \u2020 \u2212 (TAS) \u2020 )\u2225 u \u2212 p(A \u2020 )b\u2225 \u2272 \u03b5\u2225b\u2225 + \u00b5\u03b5 d \u2211 k=1 \u2225v k \u2225 \u2272 \u03b5\u2225b\u2225.(44)\nOutput description properties. The argument from the odd case shows that\n\u2211 j \u2225A * ,j \u2225 2 |x j | 2 \u2272 \u03b5 2 \u2225b\u2225 2 d 4 log( \u2225A\u2225 F \u03b4\u2225A\u2225 )\nand \u2225x\u2225 0 \u2a7d s. By Corollary 4.10 we get the desired bounds. Notice that\u00e3 0 = a 0 \u2212 a 2 + a 4 \u2212 \u00b7 \u00b7 \u00b7 \u00b1 a 2d = p(0).\n\n\nBounding Iterates of Singular Value Transformation\n\nWe give bounds on the value of \u00b5 that suffices for a generic polynomial. Though bounds may be improvable for specific functions, they are tight up to log factors for Chebyshev polynomials T k (x), and improve by a factor of d over naive coefficient-wise bounds.\n\nProposition 9.9. Let p(x) be an odd polynomial with degree 2d + 1, with a Chebyshev series expansion\nof p(x) = \u2211 d i=0 a 2i+1 T 2i+1 (x). Then \u2211 d i=0 |a 2i+1 | \u2a7d 2d\u2225p\u2225 sup and, for all integers k \u2a7d d, d \u2211 i=k a 2i+1 U i\u2212k (T 2 (x)) sup \u2272 (d \u2212 k + 1)(1 + log 2 (d + 1))\u2225p\u2225 sup\nProof. Without loss of generality, we take \u2225p\u2225 sup = 1. By Lemma 4.2, |a 2i+1 | \u2a7d 2, giving the first conclusion. Towards the second conclusion, we first note that\nd \u2211 i=k a 2i+1 U i\u2212k (T 2 (x)) sup = d \u2211 i=k a 2i+1 U i\u2212k (x) sup ,\nsince T 2 maps [\u22121, 1] to [\u22121, 1]. Then, we use the strategy from Theorem 8.4, writing out U i\u2212k with Eq. (7) and then bounding the resulting coefficients. Note that, by convention, a i and U i are zero for any integer i \u2208 Z for which they are not defined.\nd \u2211 i=k a 2i+1 U i\u2212k (x) = \u2211 i a 2i+1 U i\u2212k (x) = \u2211 i a 2i+1 \u2211 j\u2a7e0 T i\u2212k\u22122j (x)(1 + i \u2212 k \u2212 2j \u0338 = 0 ) = \u2211 j\u2a7e0 \u2211 i a 2i+1 T i\u2212k\u22122j (x)(1 + i \u2212 k \u2212 2j \u0338 = 0 ) = \u2211 j\u2a7e0 \u2211 i a 2(i+k+2j)+1 T i (x)(1 + i \u0338 = 0 ) = \u2211 i T i (x)(1 + i \u0338 = 0 ) \u2211 j\u2a7e0 a 2(i+k+2j)+1 d \u2211 i=k a 2i+1 U i\u2212k (x) sup \u2a7d \u2211 i (1 + i \u0338 = 0 )\u2225T i (x)\u2225 sup \u2211 j\u2a7e0 a 2(i+k+2j)+1 = d\u2212k \u2211 i=0 (1 + i \u0338 = 0 ) \u2211 j\u2a7e0 a 2(i+k)+1+4j \u2a7d d\u2212k \u2211 i=0\n(1 + i \u0338 = 0 )(32 + 8 log 2 (2d + 2)) by Corollary 6.5 = (2(d \u2212 k) + 1)(32 + 8 log 2 (2d + 2)) \u2272 (d \u2212 k + 1)(1 + log 2 (d + 1)) Proposition 9.10. Let p(x) be an even polynomial with degree 2d, written as p(x) = \u2211 d i=0 a 2i T 2i (x). Let\u00e3 2k := a 2k \u2212 a 2k+2 + \u00b7 \u00b7 \u00b7 \u00b1 a 2d . d \u2211 i=0 |\u00e3 2i | \u2a7d 4(d + 1)(1 + log(d + 1))\u2225p\u2225 sup d \u2211 i=0 |\u00e3 2i | 2 \u2a7d 32(d + 1)(1 + log 2 (d + 1))\u2225p\u2225 sup and, for all integers k \u2a7d d,\nd \u2211 i=k 4\u00e3 2i+2 x \u00b7 U i\u2212k (T 2 (x)) sup \u2272 (d \u2212 k + 1)(1 + log(d + 1))\u2225p\u2225 sup .\nProof. Without loss of generality, we take \u2225p\u2225 sup = 1. Consider the polynomial q(x) = \u2211 d i=0 b i T i (x) where b i := a 2i . By Eq. (8), p(x) = q(T 2 (x)), and because T 2 maps [\u22121, 1] to [\u22121, 1], \u2225q\u2225 sup = \u2225q\u2225 sup = 1. Then by Fact 6.3, |\u00e3 2k | = |b k \u2212 b k+1 + \u00b7 \u00b7 \u00b7 \u00b1 b d | \u2a7d 4 + 4 \u03c0 2 log(max(k, 1)) \u2225q\u2225 sup = 4 + 4 \u03c0 2 log(max(k, 1)) From this follows the first two conclusions. For the final conclusion, by Eq. (9), d \u2211 i=k 4\u00e3 2i+2 xU i\u2212k (T 2 (x)) = d \u2211 i=k 2\u00e3 2i+2 U 2(i\u2212k)+1 (x) = \u2211 i 2\u00e3 2i+2 U 2(i\u2212k)+1 (x)\n\nFrom here, we proceed as in the proof of Theorem 8.4.\n\u2211 i\u00e3 2i+2 U 2(i\u2212k)+1 (x) = \u2211 i \u2211 r\u2a7e0 (\u22121) r b i+r+1 2 \u2211 s T 2s+1 (x) s \u2a7d i \u2212 k = 2 \u2211 s T 2s+1 (x) \u2211 i \u2211 r\u2a7e0 s \u2a7d i \u2212 k (\u22121) r b i+r+1 = 2 \u2211 s T 2s+1 (x) \u2211 t b t \u2211 i \u2211 r r \u2a7e 0 t = i + r + 1 s \u2a7d i \u2212 k (\u22121) r = 2 \u2211 s T 2s+1 (x) \u2211 t b t \u2211 r r \u2a7e 0 s \u2a7d t \u2212 r \u2212 1 \u2212 k (\u22121) r = 2 \u2211 s T 2s+1 (x) \u2211 t b t t\u2212s\u2212k\u22121 \u2211 r=0 (\u22121) r = 2 \u2211 s T 2s+1 (x) \u2211 t b t t \u2212 s \u2212 k \u2212 1 \u2208 2Z \u2a7e0 = 2 \u2211 s T 2s+1 (x) \u2211 t\u2a7e0 b 2t+s+k+1 \u2211 i\u00e3 2i+2 U 2(i\u2212k)+1 (x) sup \u2a7d 2 d\u2212k \u2211 s=0 \u2211 t\u2a7e0 b 2t+s+k+1 \u2a7d 2 d\u2212k \u2211 s=0 4 + 4\n\u03c0 2 log(max(s + k, 1)) by Fact 6.3 \u2272 (d \u2212 k + 1)(1 + log(d + 1))\n\n\nDequantizing QML Algorithms\n\nIn this section, we focus on providing classical algorithms for regression, low-rank approximation and Hamiltonian simulation. We show that plugging in the appropriate polynomials into our algorithm corresponding to Theorem 9.1 results in the fastest algorithms for these problems.\n\n\nRecommendation Systems\n\nGiven a matrix A, the quantum recommendation system problem, introduced by Kerenidis and Prakash [KP20], is to output a sample from a row of a low-rank approximation of A. The quantum algorithm for this approaches this in a different way from the standard classical algorithms, by taking a polynomial approximation of a threshold function and using quantum linear algebra techniques to apply it to A. The threshold function is as follows: In particular, one obtains the quantum recommendation systems result from QSVT is by preparing the state |A i, * \u27e9 and applying a block-encoding of p(A) to get a copy of |p(A)A i, * \u27e9, where p is the polynomial from Lemma 10.1. 18 We obtain the same guarantee classically:\n\nCorollary 10.2 (Dequantizing recommendation systems). Suppose we are given a matrix A \u2208 C m\u00d7n such that 0.01 \u2a7d \u2225A\u2225 \u2a7d 1 and 0 < \u03b5, \u03c3 < 1, with O(nnz(A)) time pre-processing. Then there exists an algorithm that, given an index i \u2208 [m], computes a vector y such that \u2225y \u2212 p(A)A i, * \u2225 \u2a7d \u03b5\u2225A i, * \u2225 with probability at least 0.9, where p(x) is the rectangle polynomial from Lemma 10.1, with parameters t = \u03c3, \u03b4 \u2032 = \u03c3/6, \u03b5 \u2032 = \u03b5. Further, the running time to compute such a description of y is O \u2225A\u2225 4 F \u03c3 11 \u03b5 2 , and from this description we can sample from y in O \u2225A\u2225 4 F \u2225A i, * \u2225 2 \u03c3 8 \u03b5 2 \u2225y\u2225 2 time.\n\nThinking of p(A)A as the low-rank approximation of A this algorithm gives access to, we see that the error guarantee of Corollary 10.2 implies the error guarantee achieved by prior quantum-inspired algorithms [Tan19; CGLLTW22]. [CCHLW22,Theorem 26]). The recommendation systems algorithm of Chepurko, Clarkson, Horesh, Lin, and Woodruff outputs a rank-k matrix Z such that\n\n\nRemark 10.3 (Detailed comparison to\n\u2225A \u2212 Z\u2225 2 F \u2a7d (1 + O(\u03b5))\u2225A \u2212 A k \u2225 2 F ,(45)\nwhere A k is the best rank-k approximation to A, in O( k 3 \u03b5 6 + k\u2225A\u2225 4\nF (\u2225A\u2212A k \u2225 2 F /k+\u03c3 2 k )\u03c3 2 k \u03b5 4 ) = O( k 3 \u03b5 6 + k\u2225A\u2225 4 F \u03c3 4\nk \u03b5 4 ) time 19 , under the assumption that \u03b5 \u2a7d\n\u2225A k \u2225 2 F \u2225A\u2225 2 F\n. This guarantee is the typical one for low-rank approximation problems. The authors use that \u2113 2 2 importance sampling sketches oversample ridge leverage score sketch in certain parameter regimes, which is why dependences on the norm of the low-rank approximation \u2225A k \u2225 F and its residual \u2225A \u2212 A k \u2225 F appear. For the recommendation systems task, it's not clear whether this style of guarantee is as good as the guarantee achieved by the quantum algorithm. The quantum guarantee implies the classical one in the regime where additive and relative error are comparable: \u2225A \u2212 Z\u2225 F \u2a7d \u2225A \u2212 p(A)A\u2225 F + \u2225p(A)A \u2212 Z\u2225 F \u2a7d \u2225A \u2212 A k \u2225 F + O(\u03b5\u2225A\u2225 F ), where the second inequality uses that p(A)A is approximately a lowrank approximation that thresholds at \u03c3, and so is at least as good of an approximation to A as A k , at the cost of being potentially higher rank. Going from the classical guarantee to the quantum one loses a quadratic factor: it can be the case that the guarantee from Eq. (45) holds but \u2225A k \u2212 Z\u2225 F = \u2126( \u221a \u03b5\u2225A\u2225 F ). 20 To achieve the precise guarantee of the quantum algorithm would require setting \u03b5 \u2190 \u03b5 2 , giving a running time of\nO k 3 \u03b5 12 + k\u2225A\u2225 4 F \u03c3 4 k \u03b5 8 .\nWe improve over this running time when \u03b5 = O( k 3/10 \u03c3 11/10 \u2225A\u2225 4/10 F ) or when \u03b5 = O(k 1/6 \u03c3 7/6 ).\n\n\nLinear Regression\n\nNext, we consider linear regression: given a matrix A, a vector b, and a parameter \u03ba > 0, the quantum algorithm obtains a state close to | f (A)b\u27e9, where f (x) is a polynomial approximation to 1/x in the interval [\u22121, \u22121/\u03ba] \u222a [1/\u03ba, 1] [GSLW19, Theorem 41]. Formally, this polynomial is the Chebyshev truncation of (1 \u2212 (1 \u2212 x 2 ) b )/x for b = \u2308\u03ba 2 log(\u03ba/\u03b5)\u2309, multiplied by the rectangle function from Lemma 10.1 to keep the truncation bounded. Let \u03ba > 1 and 0 < \u03b5 < 1 2 . There is an odd polynomial p(x) of degree O(\u03ba log( \u03ba \u03b5 )) with the properties that \u2022 |p(x) \u2212 1/x| \u2a7d \u03b5 for x \u2208 [\u22121, \u22121/\u03ba] \u222a [1/\u03ba, 1];\n\n\u2022 |p(x)| = O(\u03ba log \u03ba \u03b5 ). We obtain the following corollary by invoking Theorem 9.1 with the polynomial from Lemma 10.4. Corollary 10.5 (Dequantizing linear regression). Given a matrix A \u2208 C m\u00d7n such that 0.01 \u2a7d \u2225A\u2225 \u2a7d 1; a vector b \u2208 C m ; and parameters \u03b5, 1/\u03ba between 0 and 1, with O(nnz(A) + nnz(b)) time 19 For ease of comparison, we suppose we know \u03c3 k and \u2225A \u2212 A k \u2225 2 F exactly, in which case \u03c8 \u03bb \u2a7d \u03c8 k and k \u2a7d \u03c8 k , so the running time is O(k 3 /\u03b5 6 + k\u03c8 \u03bb \u03c8 k /\u03b5 4 ), using the notation from [CCHLW22]. 20  pre-processing. Then there exists an algorithm that outputs a vector y such that \u2225y \u2212 p(A)b\u2225 \u2a7d \u03b5\u2225b\u2225/\u03ba with probability at least 0.9, where p is the polynomial from Lemma 10.4 with parameters \u03ba, \u03b5. Further, the running time to compute a description of y is\nO \u03ba 11 \u2225A\u2225 4 F \u03b5 2 ,\nand from this description we can output a sample from y in O \u03ba 10 \u2225A\u2225 4 F \u2225b\u2225 2 \u03b5 2 \u2225y\u2225 2 time.\n\nRemark 10.6 (Comparison to [CCHLW22,Theorem 24]). Chepurko, Clarkson, Horesh, Lin, and Woodruff solves the regularized regression problem, where the goal is to output a vector close to x * = arg min x \u2225Ax \u2212 b\u2225 2 + \u03bb\u2225x\u2225 2 for a given \u03bb > 0. Their algorithm outputs a vector y = (SA) \u2020 v such that \u2225y \u2212 x * \u2225 \u2a7d \u03b5 \u2225x * \u2225 + 2 \u2225b\u2225\u2225x * \u2225 \u2225AA + x * \u2225\n+ 1 \u221a \u03bb \u2225\u03a0 \u03bb,\u22a5 b\u2225 ,\nwhere \u03a0 \u03bb,\u22a5 is the projection onto the left singular vectors of SA whose singular values are at most \u221a \u03bb. This algorithm achieves a running time of O(\u2225A\u2225 4 F (\u2225A\u2225 2 + \u03bb) 2 log(d)/((\u03c3 2 + \u03bb) 4 \u03b5 4 )), where \u03c3 is the minimum singular value of A. Since x * = f (A)b is singular value transformation for the function f (x) = x/(x 2 + \u03bb), this is different from the setting we consider. They become comparable when we take \u03bb \u2192 0, in which case we must assume that b is in the image of A, so that \u2225\u03a0 \u03bb,\u22a5 b\u2225 tends to zero, and we depend on the minimum singular value of A. As discussed in Remark 1.5, these assumptions are strong, and different from the setting we consider.\n\n\nHamiltonian Simulation\n\nFinally, we give a classical algorithm for Hamiltonian simulation: for a Hermitian matrix H \u2208 C n\u00d7n ; a vector b \u2208 C n ; and a time t \u2208 R, the goal is to produce a description of e iHt b. We begin by describing our polynomial approximation to e ix = cos(x) + i sin(x).\n\nLemma 10.7 (Polynomial approximation to trigonometric functions, [GSLW19, Lemma 57]). Given t \u2208 R and \u03b5 \u2208 (0, 1/e), let r = \u0398(t + log(1/\u03b5) log log(1/\u03b5) ). Then, the following polynomials c(x) (even with degree 2r) and s(x) (odd with degree 2r + 1),\nc(x) = J 0 (t) \u2212 2 \u2211 i\u2208[1,r] (\u22121) i J 2i (t)T 2i (x) s(x) = 2 \u2211 i\u2208[0,r] (\u22121) i J 2i+1 (t)T 2i+1 (x),\nsatisfy that \u2225cos(tx) \u2212 c(x)\u2225 sup \u2a7d \u03b5 and \u2225sin(tx) \u2212 s(x)\u2225 sup \u2a7d \u03b5. Here, J i (x) is the i-th Bessel function of the first kind [DLMF, (10.2.2)].\n\nCorollary 10.8 (Dequantizing Hamiltonian simulation). Given a Hermitian Hamiltonian H \u2208 C n\u00d7n such that 0.01 \u2a7d \u2225H\u2225 \u2a7d 1; a vector b \u2208 C n ; a time t > 0; and 0 < \u03b5 < 1, with O(nnz(A) + nnz(b)) pre-processing. Then there exists an algorithm that outputs a vector y such that y \u2212 e iHt b \u2a7d \u03b5\u2225b\u2225 with probability \u2a7e 0.9. Further, the running time to compute such a description of y is\nO t 11 \u2225H\u2225 4 F \u03b5 2 ,\nand from this description we can output a sample from y in O t 8 \u2225H\u2225 4 F \u2225b\u2225 2 \u03b5 2 \u2225y\u2225 2 time.\n\nProof. Let c(x) and s(x) be the polynomials from Lemma 10.7. We apply Theorem 9.1 to get descriptions of c and s such that \u2225y c \u2212 c(H)b\u2225 \u2a7d \u03b5\u2225b\u2225 and \u2225y s \u2212 s(H)b\u2225 \u2a7d \u03b5\u2225b\u2225. Then e iHt b = cos(Ht)b + i sin(Ht)b \u2248 \u03b5\u2225b\u2225 c(H)b + is(H)b \u2248 \u03b5\u2225b\u2225 y c + iy s . This gives us a description O(\u03b5)-close to e iHt . Using Corollary 4.10, we can get a sample from this output in the time described, by combining the two descriptions of y c and y s .\n\n\nby 1 in [\u22121, 1]. A generic example of such polynomial is only by a constant in [\u22121 \u2212 \u03b7, 1 + \u03b7] when \u03b7 = O(1/d 2 ) (Lemma 4.3), and has Chebyshev coefficients bounded only by a constant, without decaying. Thus, the bound from Lemma 9 becomes O(d 5 \u2225E\u2225)\n\n\nWe denote the set of singular values of A by Spec(A) := {\u03c3 k } k\u2208[N] . The rank of A is the number of nonzero singular values and the stable rank of A is \u2225A\u2225 2 F /\u2225A\u2225 2 . For a Hermitian matrix A \u2208 C n\u00d7n and a function f : R \u2192 C, f (A) \u2208 C n\u00d7n denotes the matrix where f is applied to the eigenvalues of A. Below, in Definition 4.1, we define notions of matrix function that extend to non-Hermitian matrices.\n\n\ni when p is even, thus making this definition coincide with the singular value transformation as given in [GSLW19, Definition 16].\n\nLemma 4. 2 (\n2Coefficient bound, consequence of [Tre19, Eq. (3.12)]). Let f : [\u22121, 1] \u2192 R be a Lipschitz continuous function. Then all its Chebyshev coefficients a k are bounded: |a k | \u2a7d 2\u2225 f \u2225 sup .\n\nLemma 4. 3 .\n3For a degree-d polynomial p, and \u03b4 = 1 4d 2 ,\n\nDefinition 4. 4 (\n4Sampling and query access to a vector, [CGLLTW22, Definition 3.2]). For a vector v \u2208 C n , we have SQ(v), sampling and query access to v, if we can: 1. query for entries of v;\n\nDefinition 4. 5 (\n5Oversampling and query access, [CGLLTW22, Definition 3.4]).\n\nDefinition 4. 8 (\n8Oversampling and query access to a matrix, [CGLLTW22, Definition 3.7]). For a matrix A \u2208 C m\u00d7n , we have SQ(A) if we have SQ(A i, * ) for all i \u2208 [m] and SQ(a) for a \u2208 R m the vector of row norms (a i := \u2225A i, * \u2225).\n\nLemma 5. 7 (\n7Concentration of Asymmetric Random Outer Products). Let {(X i , Y i )} i\u2208[s] be s independent copies of the tuple of random vectors (X, Y), with X \u2208 C m and Y \u2208 C d . In particular, (X, Y) = (a (i) , b (i) ) with probability p i for i \u2208 [n]. Let M, L \u2a7e 0 be such that L \u2a7e max i\u2208[n]\n\nFact 6. 1 .\n1Let f : [\u22121, 1] \u2192 R be a Lipschitz continuous function. Then its Chebyshev coefficients {a \u2113 } \u2113 satisfy\n\nLemma 6. 2\n2([Tre19, Theorem 15.3]). Let f : [\u22121, 1] \u2192 R be a Lipschitz continuous function, let f k\n\n\nalways non-negative and increasing with \u2113 up to A (k) k = 1. Then Eq. (17) holds if and only if\n\nRemark 6. 8 .\n8A curious reader will (rightly) wonder whether this proof requires this level of difficulty. Intuition from the similar Fourier analysis setting suggests that arithmetic progressions of any step size at any offset are easily bounded. We can lift to the Fourier setting by considering, for an f : [\u22121, 1] \u2192 R, a corresponding 2\u03c0-periodic g : [0, 2\u03c0] \u2192 R such that g(\u03b8) := f (cos(\u03b8)\n\n\nP1. If SQ(A \u2020 ) and SQ(b) are not given, compute data structures to simulate them in O(1) time;P2. Sample S \u2208 C n\u00d7s to be AMP s A, b, Sample T \u2020 \u2208 C m\u00d7t to be AMP t S \u2020 A \u2020 , AS, { \u2225[AS] Compute a data structure that can respond to SQ(TAS) queries in O(1) time;\n\n\n. (\u2225AS\u2225 F bd) (\u2225TAS\u2225 F bd) \u2225(AS) \u2020 AS \u2212 (TAS) \u2020 TAS\u2225 \u2a7d \u00b5\u03b5 d \u2225AS\u2225 by Corollary 5.6 \u2a7d 2 \u00b5\u03b5 d \u2225A\u2225 by Eq. (\u2225AS\u2225 bd) ((AS) \u2020 AS AMP) \u2225TAS\u2225 2 = \u2225(TAS) \u2020 TAS\u2225 \u2a7d (1 + \u00b5\u03b5 d\u2225AS\u2225 )\u2225AS\u2225 2 by Eq. ((AS) \u2020 AS AMP) \u2a7d (1 + 2 \u00b5\u03b5 d\u2225A\u2225 )\u2225A\u2225 2 by Eq. (\u2225AS\u2225 bd) (\u2225TAS\u2225 bd) One Clenshaw iteration. We are trying to perform the odd Clenshaw recurrence defined in Eq. (Odd Clenshaw). The matrix analogue of this is\n\n\n(k) \u2020 using SQ(TAS) is O(r) (A2.I1), and actually computing the iteration costs O(r + s + t) = O(r) (A2.I2).\n\n\nk) \u2020 are all drawn independently, E[B (k) \u2020 B (k) \u2212 (TAS) \u2020 (TAS)] is the zero matrix,where the expectation is over the randomness of B (k) and B (k) \u2020 . We use the following bound on the variance of the error. In this computation, we use Eq. (13), which states that, for B = BEST(A) with parameter r and X positive semi-definite, E[B \u2020 XB] \u2a7d A \u2020 XA + 1 r Tr(X)\u2225A\u2225 2 F I.\n\n\nP1. If SQ(A \u2020 ) and SQ(b) are not given, compute data structures to simulate them in O(1) time;P2. Sample S \u2208 C n\u00d7s to be AMP s A, A \u2020 , { \u2225A * ,\n\nLemma 10. 1 (\n1Polynomial approximations of the rectangle function [GSLW19, Lemma 29]). Let\u03b4 \u2032 , \u03b5 \u2032 \u2208 (0, 1 2 ) and t \u2208 [\u22121, 1]. There exists an even polynomial p \u2208 R[x] of degree O(log( 1 \u03b5 \u2032 )/\u03b4 \u2032 ), such that |p(x)| \u2a7d 1 for all x \u2208 [\u22121, 1], and p(x) \u2208 [0, \u03b5 \u2032 ] for all x \u2208 [\u22121, \u2212t \u2212 \u03b4 \u2032 ] \u222a [t + \u03b4 \u2032 , 1], and [1 \u2212 \u03b5 \u2032 , 1] for all x \u2208 [\u2212t \u2212 \u03b4 \u2032 , t + \u03b4 \u2032 ]\n\nLemma 10. 4 (\n4Polynomial approximations of 1/x, [GSLW19, Lemma 40], following [CKS17]).\n\n\nThe algorithm takesO \nd 11 \u2225A\u2225 4 \n\nF \n\n\u2225A\u2225 4 \n\u03b5 2 \n\ntime to output a description of y as Ax for a sparse vector x. This description allows us to compute \n\nentries of y in O \n\nd 6 \u2225A\u2225 2 \n\nF \n\n\u2225A\u2225 2 \u03b5 2 time and obtain an \u2113 2 \n2 sample from y in O \n\nd 8 \u2225A\u2225 4 \nF \u2225b\u2225 2 \n\n\n\n\nRemark 1.5 (Comparison with [CGLLTW22; GST22; SM21]). Prior work [CGLLTW22, Corollary 6.15] achieves a running time of O(\u2225A\u2225 6\n\n\nhave no control over the direction of error, so we can at best bound \u2225E\u2225 with the column-wise bound, giving \u2225E\u2225 \u2a7d \u2225E\u2225 F \u2a7d\u221a \nk\u03b5. So, our \nversion of [MMS18, Equation 16] gives \u2225E\u2225 \u2a7d \n\n\u221a \nd\u03b5, which gives us the asserted O(d 5.5 \u03b5) bound. \nNow, we consider the Clenshaw recurrence when each iteration incurs error \u03b5(|q k+1 | + |q k+1 |). \nA naive argument gives a O(d 3 ) bound on the overhead, which implies that \u03b5 needs to be scaled \ndown by that much to get the desired error bound. For our modified recurrence, this is a O(d 4 ), \n\ngiving a time of O(d 16 \u2225A\u2225 4 \n\n\n\n\nAny Lipschitz continuous function 13 f : [\u22121, 1] \u2192 R can be written as a (unique) linear combination of Chebyshev polynomials, f\n\n\nFact 5.8 (Intrinsic Matrix Bernstein, [Tro15, Theorem 7.3.1]). Consider a finite sequence { Z k } k\u2208[s]\n\n\nLet f : [\u22121, 1] \u2192 R be a Lipschitz continuous function. Then it can be expressed uniquely as a linear combination of Chebyshev polynomials fTo give improved stability bounds for the Clenshaw recurrence, we need to bound various sums \nof Chebyshev coefficients. Since we aim to give bounds that hold for all degree-d polynomials, \nwe use no property of the function beyond that it has a unique Chebyshev expansion; of course, \nfor any particular choice of function f , the bounds in this section can be improved by explicitly \ncomputing its Chebyshev coefficients, or in some cases, by using smoothness properties of the \nfunction [Tre19, Theorems 7.2 and 8.2]. \n\n\n\n\nSuch a bound on \u2211|a k | is not tight, however. A use of Parseval's formula[MH02, Theorem 5.3] improves on this by a factor ofTypi-\ncally, this has been analyzed in the finite precision setting, where the errors are caused by \ntruncation. These standard analyses show that this finite precision recursion gives p(x) to \nd 2 (\u2211|a i |)\u03b5 = O(d 3 \u2225p\u2225 sup \u03b5) error. This bound \u2211|a i | is not easily improved in settings where p is \na polynomial approximation of a smooth function, since standard methods only give bounds of \nthe form |a k | = \u0398((1 \u2212 log(1/\u03b5)/d) \u2212k ), [Tre19, Theorem 8.1], giving only constant bounds for \nthe coefficients. \n\n\u221a \nd: \n\n\n\n\nwith only O(s) overhead. To produce a sample i \u2208 [m] with probability \u2225[AS] i, * \u2225 2 /\u2225AS\u2225 2 F , sample a column index j \u2208 [s] with probability [AS] * ,j2 /\u2225AS\u2225 2 \nF and then query SQ(A) to get \na row index i \u2208 [m] with probability [AS] i,j \n2 / [AS]  * ,j \n2 . \n\n\n\n\nThe time to compute each iteration increases to O(st), and the final running time is\n\n\n\u2225TAS\u2225 F \u2225TASv k+1 \u2225 + d/(r\u03b4)\u2225I t \u2225 F \u2225TAS\u2225 F \u2225v k+1 \u2225by Corollary 5.3 \n\n\u2a7d 4 d/(r\u03b4)\u2225AS\u2225 F by Corollary 5.3 \n\n\u2a7d 1 \n3 d \u22125/2 \u00b5\u03b5 \u2225TASv k+1 \u2225 + d \u22123/2 \u2225v k+1 \u2225 \nby Eqs. (\u2225TAS\u2225 F bd) and (\u2225TAS\u2225 bd) \n\n\n\n\n1+2\u00b5\u03b5 \n\nd ] \n\n\u2225S  \u2020 b\u2225 \nby Eq. (\u2225TAS\u2225 bd) \n\n\u2a7d e \n\nd \n\n\u2211 \n\ni=k \n\nU i\u2212k (T 2 (x))a 2i+1 \n\nsup \n\n\u2225S  \u2020 b\u2225 \nby Lemma 4.3, \u00b5\u03b5 \u2a7d 1 \n\n100d \n\u2a7d 4 \n\nd \n\n\u2211 \n\ni=k \n\nU i\u2212k (T 2 (x))a 2i+1 \n\nsup \n\n\u2225b\u2225 \nby Eq. (\u2225S  \u2020 b\u2225 bd) \n\n\u2a7d 4 \n1 \n\n\u00b5d \n\u2225b\u2225 \nby Item 9.3(b) \n\n\n\n\nBy Markov's inequality, with probability \u2a7e 1 \u2212 \u03b4/100, we have that for all k, \u2225v k \u2225 \u2272 \u2225b\u2225 \u00b5d . Returning to the final error bound Eq. (36),\n\n\nThe output description has the additional properties so that by Corollary 4.10, for the output vector y := Ax + \u03b7b, we can:(i) Compute entries of y in O(\u2225x\u2225 0 ) = O time with probability \u2a7e 1 \u2212 \u03b4;(iii) Estimate \u2225y\u2225 2 to \u03bd relative error in O\u2211 \n\nj \n\nA  * ,j \n2 x j \n2 \u2272 \n\u03b5 2 \u2225p\u2225 2 \nsup \u2225b\u2225 2 \nd 4 log( \u2225A\u2225 F \n\u03b4\u2225A\u2225 ) \n\n\u2225x\u2225 0 \u2272 \nd 2 \u2225A\u2225 2 \n\nF \n\n(\u00b5\u03b5) 2 log( \n\n\u2225A\u2225 F \n\n\u03b4\u2225A\u2225 \n\n), \n\nd 2 \u2225A\u2225 2 \n\nF \n\n(\u00b5\u03b5) 2 log( \u2225A\u2225 F \n\u03b4\u2225A\u2225 ) time; \n\n(ii) Sample i \u2208 [n] with probability |y i | 2 \n\u2225y\u2225 2 in O \n\n\u2225p\u2225 2 \nsup \u2225A\u2225 2 \n\nF \n\n(\u00b5d) 2 \n\n+ p(0) 2 d 2 \u2225A\u2225 2 \n\nF \u2225b\u2225 2 \n\n\u00b5 2 \u03b5 2 \u2225y\u2225 2 log( \u2225A\u2225 F \n\u03b4\u2225A\u2225 ) log 1 \n\n\u03b4 \n\n\u2225p\u2225 2 \nsup \u2225A\u2225 2 \n\nF \n\n(\u00b5d) 2 \n\n\n\n\nPre-processing sketches, correctness. Though the sketches are chosen to be slightly different because of the different parity, all of the sketching bounds used for the odd SVT analysis hold here, up to rescaling s, t by constant factors. This includes Eqs. (\u2225[AS] * ,j \u2225 bd), (\u2225AS\u2225 F bd), (AA \u2020 AMP), (\u2225AS\u2225 bd), (\u2225TAS\u2225 F bd), ((AS) \u2020 AS AMP) and (\u2225TAS\u2225 bd). What remains (Eqs. (\u2225S \u2020 b\u2225 bd) and (Ab AMP)) have analogues that follow from the same argument:\u2a7d |4\u00e3 2k+2 |\u2225AS\u2225 F \u2225TAS\u2225 F \u2225Tb\u2225 d/(r\u03b4)\u2a7d |\u00e3 2k+2 |d \u22125/2 \u00b5\u03b5\u2225b\u2225 by Eqs. (\u2225TAS\u2225 bd) and (\u2225Tb\u2225 bd)by Corollary 5.3 \n\n\n\n\nTb We use this and the derivation in Eq. (37) to conclude We have shown that E[\u2225v k \u2212v k \u2225 2 ] \u2a7d 9\u03b4 d ( \u2225A\u2225\u2225b\u2225 \u00b5d ) 2 . By Markov's inequality, with probability \u2a7e 1 \u2212 \u03b4/100, we have that for all k, \u2225v k \u2225 \u2272 \u2225b\u2225 \u00b5d . Returning to the final error bound Eq. (42),2 \n\n\u2a7d \n1 \nr \nTr(I s )\u2225TAS\u2225 2 \nF \u2225Tb\u2225 2 \nby Eq. (13) \n\n\u2a7d \n\u03b4 \n1000d 4 \u2225b\u2225 2 . \nby Eq. (\u2225Tb\u2225 bd), r = \u0398(d 4 \u2225A\u2225 2 \nF (s + t) 1 \n\u03b4 ) \n\n\n\nTake, for example, A = [ 2 1 ] and the rank k = 1 approximation Z = zz \u2020 where z = [ ]. More generally, by [Tan19, Theorem 4.7], Eq. (45) implies \u2225Z \u2212 A \u2a7e\u03c3 \u2225 F = O( \u221a \u03b5\u2225A \u2212 A k \u2225 F ) (for certain forms of Z).\u221a \n\n2+ \n\u221a \n\u03b5 \n\u221a \n\u03b5 \n\n\nThis computation arises from taking Lemma 9 of[MMS18] to evaluate a degree-d polynomial, say, bounded\nThat is, we pronounce BEST(A) as \"best of A\". We make no normative claim about our sketch vis-a-vis other sketches.11  At this point, one might wonder whether one can simply zero out small entries to get these guarantees with the additional spectral norm bound. This is not the case; if we only zero out entries smaller than \u03b5/(2n), this threshold is too small to improve the matrix Bernstein tail bound, whereas if we zero out entries smaller than, say, 1/(100n), the matrix Bernstein tail bound goes through, but the bilinear form is not \u03b5-preserved.\nIn fact, for any degree d, there are polynomials of that degree which are bounded and yet \u2211 d \u2113=0 |a \u2113 | = \u2126(d) [Tre19, Theorems 8.2 and 8.3].\nWe call a function f : [\u22121, 1] \u2192 R Lipschitz continuous if there exists a constant C such that | f (x) \u2212 f (y)| \u2a7d C|x \u2212 y| for x, y \u2208 [\u22121, 1].\nThis holds in the word-RAM model, with an additional log overhead when considering bit complexity.\n\u03c0 2 log(k + 1) \u2225 f \u2225 sup .\nThe original paper goes through a singular value estimation procedure; see the discussion in Section 3.6 of[GSLW19] for more details.\nAcknowledgmentsET and AB thank Nick Trefethen, Simon Foucart, Alex Townsend, Sujit Rao, and Victor Reis for helpful discussions. ET thanks t.f. for the support. AB is supported by Ankur Moitra's ONR grant. ET is supported by the NSF GRFP (DGE-1762114).All this holds with probability \u2a7e 1 \u2212 \u03b4.One (even) Clenshaw iteration. As with the odd case, we perform the recurrence on u k by updating v k such that u k = (AS)v k . The error analysis proceeds by boundingAs before, we can label the error incurred by each approximation in Eq. (39) with \u03b5 1 , . . . , \u03b5 7 . The approximations in \u03b5 1 , \u03b5 2 , \u03b5 4 , \u03b5 5 do not involve the constant term and so can be bounded identically to the odd case.The approximation in \u03b5 3 goes through with a slight modification.The approximations \u03b5 6 and \u03b5 7 follow from similar arguments.\u2225v k+1 \u2225 2 by Item 9.7(a)By the same recurrence argument, this is \u2a7d 9\u03b4\n. R F Schneider, C W Boisvert, B R Clark, B V Miller, H S Saunders, Cohl, M. A. McClain53Schneider, R. F. Boisvert, C. W. Clark, B. R. Miller, B. V. Saunders, H. S. Cohl, and M. A. McClain, eds. URL: https://dlmf.nist.gov/ (page 53).\n\nForrelation: a problem that optimally separates quantum from classical computing. Scott Aaronson, Andris Ambainis, 10.1137/15m1050902arXiv:1411.5729SIAM Journal on Computing. 4713quant-phScott Aaronson and Andris Ambainis. \"Forrelation: a problem that optimally separates quantum from classical computing\". In: SIAM Journal on Computing 47.3 (Jan. 2018), pp. 982-1038. DOI: 10.1137/15m1050902. arXiv: 1411.5729 [quant-ph] (page 13).\n\nComplexity-Theoretic Foundations of Quantum Supremacy Experiments. Scott Aaronson, Lijie Chen, Leibniz International Proceedings in Informatics (LIPIcs). 7932 nd Computational Complexity ConferenceScott Aaronson and Lijie Chen. \"Complexity-Theoretic Foundations of Quan- tum Supremacy Experiments\". In: 32 nd Computational Complexity Conference (CCC 2017). Vol. 79. Leibniz International Proceedings in Informatics (LIPIcs).\n\nGermany Dagstuhl, 10.4230/LIPIcs.CCC.2017.22Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. 2213Dagstuhl, Germany: Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017, 22:1-22:67. DOI: 10.4230/LIPIcs.CCC.2017.22 (page 13).\n\nA fast random sampling algorithm for sparsifying matrices. Sanjeev Arora, Elad Hazan, Satyen Kale, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Springerpage 12Sanjeev Arora, Elad Hazan, and Satyen Kale. \"A fast random sampling algo- rithm for sparsifying matrices\". In: Approximation, Randomization, and Combi- natorial Optimization. Algorithms and Techniques. Springer, 2006, pp. 272-279 (page 12).\n\nLazySVD: even faster svd decomposition yet without agonizing pain. Zeyuan Allen, -Zhu , Yuanzhi Li, Advances in neural information processing systems. 2913Zeyuan Allen-Zhu and Yuanzhi Li. \"LazySVD: even faster svd decomposition yet without agonizing pain\". In: Advances in neural information processing systems 29 (2016) (page 13).\n\nFast computation of low-rank matrix approximations. Dimitris Achlioptas, Frank Mcsherry, 10.1145/1219092.1219097DOI: 10.1145/ 1219092.1219097Journal of the ACM. 5499Dimitris Achlioptas and Frank Mcsherry. \"Fast computation of low-rank matrix approximations\". In: Journal of the ACM 54.2 (Apr. 2007), p. 9. DOI: 10.1145/ 1219092.1219097 (pages 9, 10, 12).\n\nLow-rank approximation with 1/\u03b5 1/3 matrix-vector products. Ainesh Bakshi, Kenneth L Clarkson, David P Woodruff, 10.1145/3519935.3519988arXiv:2202.05120Proceedings of the 54 th Annual ACM SIGACT Symposium on Theory of Computing. the 54 th Annual ACM SIGACT Symposium on Theory of ComputingACM13cs.DSAinesh Bakshi, Kenneth L. Clarkson, and David P. Woodruff. \"Low-rank ap- proximation with 1/\u03b5 1/3 matrix-vector products\". In: Proceedings of the 54 th Annual ACM SIGACT Symposium on Theory of Computing. ACM, June 2022. DOI: 10.1145/3519935.3519988. arXiv: 2202.05120 [cs.DS] (page 13).\n\nNear-optimal entrywise sampling of numerically sparse matrices. Vladimir Braverman, Robert Krauthgamer, Aditya R Krishnan, Shay Sapir, PMLR. 2021Conference on Learning Theory. 129Vladimir Braverman, Robert Krauthgamer, Aditya R Krishnan, and Shay Sapir. \"Near-optimal entrywise sampling of numerically sparse matrices\". In: Confer- ence on Learning Theory. PMLR. 2021, pp. 759-773 (pages 9, 12).\n\nQuantum sdp solvers: large speed-ups, optimality, and applications to quantum learning. G S L Fernando, Amir Brand\u00e3o, Tongyang Kalev, Cedric Yen-Yu Li, Krysta M Lin, Xiaodi Svore, Wu, 10.4230/LIPICS.ICALP.2019.27arXiv:1710.02581quant-phFernando G. S. L. Brand\u00e3o, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M. Svore, and Xiaodi Wu. \"Quantum sdp solvers: large speed-ups, optimality, and applications to quantum learning\". In: (2019). DOI: 10.4230/ LIPICS.ICALP.2019.27. arXiv: 1710.02581 [quant-ph] (page 1).\n\nSublinear time spectral density estimation. Vladimir Braverman, Aditya Krishnan, Christopher Musco, 10.1145/3519935.3520009Proceedings of the 54 th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022. the 54 th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022Rome, ItalyAssociation for Computing MachineryVladimir Braverman, Aditya Krishnan, and Christopher Musco. \"Sublinear time spectral density estimation\". In: Proceedings of the 54 th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022. Rome, Italy: Asso- ciation for Computing Machinery, 2022, pp. 1144-1157. ISBN: 9781450392648. DOI: 10.1145/3519935.3520009 (page 9).\n\nFocus beyond quadratic speedups for errorcorrected quantum advantage. Ryan Babbush, Jarrod R Mcclean, Michael Newman, Craig Gidney, Sergio Boixo, Hartmut Neven, PRX Quantum. 214Ryan Babbush, Jarrod R McClean, Michael Newman, Craig Gidney, Sergio Boixo, and Hartmut Neven. \"Focus beyond quadratic speedups for error- corrected quantum advantage\". In: PRX Quantum 2.1 (2021), p. 010103 (page 14).\n\nQuantum-inspired algorithms from randomized numerical linear algebra. Nadiia Chepurko, Kenneth Clarkson, Lior Horesh, Honghao Lin, David Woodruff, arXiv:2011.04125[cs.DSProceedings of the 39 th International Conference on Machine Learning. the 39 th International Conference on Machine Learning16252Proceedings of Machine Learning Research. PMLRNadiia Chepurko, Kenneth Clarkson, Lior Horesh, Honghao Lin, and David Woodruff. \"Quantum-inspired algorithms from randomized numerical linear algebra\". In: Proceedings of the 39 th International Conference on Machine Learning. Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022, pp. 3879- 3900. arXiv: 2011.04125 [cs.DS]. URL: https://proceedings.mlr.press/v162/ chepurko22a.html (pages 2, 4, 9, 12, 52, 53).\n\nDimensionality reduction for k-means clustering and low rank approximation. Sam Michael B Cohen, Cameron Elder, Christopher Musco, Madalina Musco, Persu, Proceedings of the forty-seventh annual ACM symposium on Theory of computing. the forty-seventh annual ACM symposium on Theory of computingpage 12Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. \"Dimensionality reduction for k-means clustering and low rank approx- imation\". In: Proceedings of the forty-seventh annual ACM symposium on Theory of computing. 2015, pp. 163-172 (page 12).\n\nThe power of block-encoded matrix powers: improved regression techniques via faster Hamiltonian simulation. Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, Stacey Jeffery, 10.4230/LIPIcs.ICALP.2019.33arXiv:1804.0197346 th International Colloquium on Automata, Languages, and Programming (ICALP 2019). LIPIcs. Schloss Dagstuhl. quant-ph] (pages 1, 4)Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. \"The power of block-encoded matrix powers: improved regression techniques via faster Hamil- tonian simulation\". In: 46 th International Colloquium on Automata, Languages, and Programming (ICALP 2019). LIPIcs. Schloss Dagstuhl, 2019. DOI: 10.4230/ LIPIcs.ICALP.2019.33. arXiv: 1804.01973 [quant-ph] (pages 1, 4).\n\nQuantum-inspired algorithms for solving low-rank linear equation systems with logarithmic dependence on the dimension. Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, Chunhao Wang, 10.4230/LIPIcs.ISAAC.2020.4717. ISBN: 978-3-95977-173-3. DOI: 10 . 4230 / LIPIcs . ISAAC . 2020 . 47Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik. ISAAC 2020). Ed. by Yixin Cao, Siu-Wing Cheng, and Minming Li18131 st International Symposium on Algorithms and ComputationNai-Hui Chia, Andr\u00e1s Gily\u00e9n, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, and Chunhao Wang. \"Quantum-inspired algorithms for solving low-rank linear equation systems with logarithmic dependence on the dimension\". In: 31 st International Symposium on Algorithms and Computation (ISAAC 2020). Ed. by Yixin Cao, Siu-Wing Cheng, and Minming Li. Vol. 181. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl- Leibniz-Zentrum f\u00fcr Informatik, 2020, 47:1-47:17. ISBN: 978-3-95977-173-3. DOI: 10 . 4230 / LIPIcs . ISAAC . 2020 . 47. URL: https : / / drops . dagstuhl . de / opus / volltexte/2020/13391 (page 2).\n\nSampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. Nai-Hui Chia, Andr\u00e1s Pal Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, Chunhao Wang, 10.1145/3549524arXiv:1910.06151Journal of the ACM. 6937cs.DSNai-Hui Chia, Andr\u00e1s Pal Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, and Chunhao Wang. \"Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning\". In: Journal of the ACM 69.5 (Oct. 2022), pp. 1-72. DOI: 10 . 1145 / 3549524. arXiv: 1910 . 06151 [cs.DS] (pages 2-7, 11, 13-17, 36, 37, 51).\n\nRevisiting dequantization and quantum advantage in learning tasks. Jordan Cotler, Hsin-Yuan, Jarrod R Huang, Mcclean, 10.48550/ARXIV.2112.0081113quant-phJordan Cotler, Hsin-Yuan Huang, and Jarrod R. McClean. Revisiting dequantiza- tion and quantum advantage in learning tasks. 2021. DOI: 10.48550/ARXIV.2112. 00811. arXiv: 2112.00811 [quant-ph] (page 13).\n\nQuantum machine learning: a classical perspective. Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Rocchetto, Simone Severini, Leonard Wossnig, 10.1098/rspa.2017.0551arXiv:1707.08561Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 474120170551Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Rocchetto, Simone Severini, and Leonard Wossnig. \"Quantum machine learning: a classical perspective\". In: Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474.2209 (Jan. 2018), p. 20170551. DOI: 10.1098/rspa.2017.0551. arXiv: 1707.08561 (page 1).\n\nQuantum algorithm for systems of linear equations with exponentially improved dependence on precision. Andrew M Childs, Robin Kothari, Rolando D Somma, 10.1137/16M1087072SIAM Journal on Computing. 4652Andrew M. Childs, Robin Kothari, and Rolando D. Somma. \"Quantum algo- rithm for systems of linear equations with exponentially improved dependence on precision\". In: SIAM Journal on Computing 46.6 (2017), pp. 1920-1950. DOI: 10.1137/16M1087072 (page 52).\n\nA note on the summation of Chebyshev series. C W Clenshaw, 10.1090/s0025-5718-1955-0071856-0Math. Tables Aids Comput. 9C. W. Clenshaw. \"A note on the summation of Chebyshev series\". In: Math. Tables Aids Comput. 9 (1955), pp. 118-120. ISSN: 0891-6837. DOI: 10.1090/s0025- 5718-1955-0071856-0 (pages 6, 8, 9).\n\nOptimal approximate matrix product in terms of stable rank. Jelani Michael B Cohen, David P Nelson, Woodruff, 43 rd International Colloquium on Automata, Languages, and Programming (ICALP 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. 2012Michael B Cohen, Jelani Nelson, and David P Woodruff. \"Optimal approximate matrix product in terms of stable rank\". In: 43 rd International Colloquium on Automata, Languages, and Programming (ICALP 2016). Schloss Dagstuhl-Leibniz- Zentrum fuer Informatik. 2016 (pages 12, 20).\n\nQuantum algorithms and lower bounds for linear regression with norm constraints. Yanlin Chen, Ronald De, Wolf , arXiv:2110.13086quant-phYanlin Chen and Ronald de Wolf. Quantum algorithms and lower bounds for linear regression with norm constraints. 2022. arXiv: 2110.13086 [quant-ph] (page 1).\n\nFast Monte Carlo algorithms for matrices I: approximating matrix multiplication. P Drineas, R Kannan, M Mahoney, 10.1137/s0097539704442684SIAM Journal on Computing. 361P. Drineas, R. Kannan, and M. Mahoney. \"Fast Monte Carlo algorithms for ma- trices I: approximating matrix multiplication\". In: SIAM Journal on Computing 36.1 (Jan. 2006), pp. 132-157. DOI: 10.1137/s0097539704442684 (page 20).\n\nFast Monte Carlo algorithms for matrices II: computing a low-rank approximation to a matrix. P Drineas, R Kannan, M Mahoney, 10.1137/s0097539704442696SIAM Journal on Computing. 36112P. Drineas, R. Kannan, and M. Mahoney. \"Fast Monte Carlo algorithms for matrices II: computing a low-rank approximation to a matrix\". In: SIAM Journal on Computing 36.1 (Jan. 2006), pp. 158-183. DOI: 10.1137/s0097539704442696 (page 12).\n\nA non-review of Quantum Machine Learning: trends and explorations. Vedran Dunjko, Peter Wittek, 10.22331/qv-2020-03-17-32Quantum Views. 432Vedran Dunjko and Peter Wittek. \"A non-review of Quantum Machine Learn- ing: trends and explorations\". In: Quantum Views 4 (Mar. 2020), p. 32. DOI: 10.22331/qv-2020-03-17-32. URL: https://doi.org/10.22331/qv-2020-03-17- 32 (page 1).\n\nA note on element-wise matrix sparsification via a matrix-valued bernstein inequality. Petros Drineas, Anastasios Zouzias, 10.1016/j.ipl.2011.01.010arXiv:1006.0407v2[cs.DS]Information Processing Letters. 1119Petros Drineas and Anastasios Zouzias. \"A note on element-wise matrix spar- sification via a matrix-valued bernstein inequality\". In: Information Processing Letters 111.8 (June 2, 2010), pp. 385-389. DOI: 10.1016/j.ipl.2011.01.010. arXiv: 1006.0407v2 [cs.DS] (pages 9, 12).\n\nChebyshev polynomials in numerical analysis. L Fox, I B Parker, Ont. Oxford University PressL. Fox and I. B. Parker. Chebyshev polynomials in numerical analysis. Oxford University Press, London-New York-Toronto, Ont., 1968, pp. ix+205 (page 9).\n\nDequantizing the quantum singular value transformation: hardness and applications to quantum chemistry and the quantum pcp conjecture. Sevag Gharibian, Fran\u00e7ois Le Gall, 10.1145/3519935.3519991ISBN: 9781450392648. DOI: 10.1145/ 3519935.3519991Proceedings of the 54 th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022. the 54 th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022Rome, ItalyAssociation for Computing Machinery43Sevag Gharibian and Fran\u00e7ois Le Gall. \"Dequantizing the quantum singular value transformation: hardness and applications to quantum chemistry and the quantum pcp conjecture\". In: Proceedings of the 54 th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022. Rome, Italy: Association for Computing Machinery, 2022, pp. 19-32. ISBN: 9781450392648. DOI: 10.1145/ 3519935.3519991 (pages 3, 4).\n\nQuantum random access memory. Vittorio Giovannetti, Seth Lloyd, Lorenzo Maccone, 10.1103/PhysRevLett.100.160501arXiv:0708.1879Physical Review Letters. 1001160501Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. \"Quantum random access memory\". In: Physical Review Letters 100.16 (2008), p. 160501. DOI: 10. 1103/PhysRevLett.100.160501. arXiv: 0708.1879 (page 1).\n\nExploiting numerical sparsity for efficient learning: Faster eigenvector computation and regression. Neha Gupta, Aaron Sidford, arXiv:1811.10866Advances in Neural Information Processing Systems. 3113Neha Gupta and Aaron Sidford. \"Exploiting numerical sparsity for efficient learning: Faster eigenvector computation and regression\". In: Advances in Neu- ral Information Processing Systems. Vol. 31. 2018, pp. 5269-5278. arXiv: 1811. 10866 (page 13).\n\nQuantum singular value transformation and beyond: Exponential improvements for quantum matrix arithmetics. Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, Nathan Wiebe, 10.1145/3313276.3316366arXiv:1806.01838Proceedings of the 51 st ACM Symposium on the Theory of Computing (STOC). the 51 st ACM Symposium on the Theory of Computing (STOC)ACM18pages 1, 3, 15Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. \"Quantum singular value transformation and beyond: Exponential improvements for quantum matrix arithmetics\". In: Proceedings of the 51 st ACM Symposium on the Theory of Computing (STOC). ACM, June 2019, pp. 193-204. DOI: 10.1145/ 3313276.3316366. arXiv: 1806.01838 (pages 1, 3, 15, 18, 51-53).\n\nAn improved quantum-inspired algorithm for linear regression. Andr\u00e1s Gily\u00e9n, Zhao Song, Ewin Tang, 10.22331/q-2022-06-30-754DOI:10.22331/q-2022-06-30-754.arXiv:2009.07268754cs.DS] (pages 2, 5)Andr\u00e1s Gily\u00e9n, Zhao Song, and Ewin Tang. \"An improved quantum-inspired algorithm for linear regression\". In: Quantum 6 (June 2022), p. 754. DOI: 10. 22331/q-2022-06-30-754. arXiv: 2009.07268 [cs.DS] (pages 2, 5).\n\nError bounds for random matrix approximation schemes. Alex Gittens, Tropp, 10.48550/ARXIV.0911.4108arXiv:0911.410812math.NAAlex Gittens and Joel A Tropp. \"Error bounds for random matrix approxima- tion schemes\". In: (Nov. 20, 2009). DOI: 10.48550/ARXIV.0911.4108. arXiv: 0911.4108 [math.NA] (page 12).\n\nClassical and Quantum Algorithms for Tensor Principal Component Analysis. Matthew B Hastings, 10.22331/q-2020-02-27-237Quantum 4. 237page 3Matthew B. Hastings. \"Classical and Quantum Algorithms for Tensor Principal Component Analysis\". In: Quantum 4 (Feb. 2020), p. 237. ISSN: 2521-327X. DOI: 10.22331/q-2020-02-27-237 (page 3).\n\nQuantum algorithm for linear systems of equations. Aram W Harrow, Avinatan Hassidim, Seth Lloyd, 10.1103/PhysRevLett.103.150502Physical Review Letters. 1031150502Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. \"Quantum algorithm for linear systems of equations\". In: Physical Review Letters 103 (15 Oct. 2009), p. 150502. DOI: 10.1103/PhysRevLett.103.150502 (page 1).\n\nQuantum advantage in learning from experiments. Hsin-Yuan, Michael Huang, Jordan Broughton, Sitan Cotler, Jerry Chen, Masoud Li, Hartmut Mohseni, Ryan Neven, Richard Babbush, John Kueng, Jarrod R Preskill, Mcclean, 10.1126/science.abn7293Science. 376DOI: 10 . 1126 / science . abn7293 (page 13Hsin-Yuan Huang, Michael Broughton, Jordan Cotler, Sitan Chen, Jerry Li, Ma- soud Mohseni, Hartmut Neven, Ryan Babbush, Richard Kueng, John Preskill, and Jarrod R. McClean. \"Quantum advantage in learning from experiments\". In: Science 376.6598 (2022), pp. 1182-1186. DOI: 10 . 1126 / science . abn7293 (page 13).\n\nQuantum-Inspired Classical Algorithms for Singular Value Transformation. Dhawal Jethwani, Fran\u00e7ois Le Gall, Sanjay K Singh, 10.4230/LIPIcs.MFCS.2020.5345 th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020). LIPIcs. Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik. cs.DSDhawal Jethwani, Fran\u00e7ois Le Gall, and Sanjay K. Singh. \"Quantum-Inspired Classical Algorithms for Singular Value Transformation\". In: 45 th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020). LIPIcs. Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2020. DOI: 10.4230/LIPIcs. MFCS.2020.53. arXiv: 1910.05699 [cs.DS] (page 3).\n\nQRAM: A survey and critique. Samuel Jaques, Arthur G Rattew, 10.48550/ARXIV.2305.10310quantphSamuel Jaques and Arthur G. Rattew. \"QRAM: A survey and critique\". In: (May 17, 2023). DOI: 10.48550/ARXIV.2305.10310. arXiv: 2305.10310 [quant- ph] (page 1).\n\nA note on randomized element-wise matrix sparsification. Abhisek Kundu, Petros Drineas, arXiv:1404.0320v1[cs.IT]129Abhisek Kundu and Petros Drineas. \"A note on randomized element-wise matrix sparsification\". In: (Apr. 1, 2014). arXiv: 1404.0320v1 [cs.IT] (pages 9, 12).\n\nRecovering pca and sparse pca via hybrid-(l1, l2) sparse sampling of data elements. Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail, The Journal of Machine Learning Research. 18page 12Abhisek Kundu, Petros Drineas, and Malik Magdon-Ismail. \"Recovering pca and sparse pca via hybrid-(l1, l2) sparse sampling of data elements\". In: The Journal of Machine Learning Research 18.1 (2017), pp. 2558-2591 (page 12).\n\nq-means: a quantum algorithm for unsupervised machine learning. Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, Anupam Prakash, arXiv:1812.03584Advances in Neural Information Processing Systems. 321Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash. \"q-means: a quantum algorithm for unsupervised machine learning\". In: Ad- vances in Neural Information Processing Systems. Vol. 32. 2019. arXiv: 1812.03584 (pages 1, 2).\n\nQuantum recommendation systems. Iordanis Kerenidis, Anupam Prakash, 10.4230/LIPIcs.ITCS.2017.49arXiv:1603.08675Proceedings of the 8 th Innovations in Theoretical Computer Science Conference (ITCS. the 8 th Innovations in Theoretical Computer Science Conference (ITCS41Iordanis Kerenidis and Anupam Prakash. \"Quantum recommendation sys- tems\". In: Proceedings of the 8 th Innovations in Theoretical Computer Science Con- ference (ITCS). 2017, 49:1-49:21. DOI: 10.4230 /LIPIcs.ITCS.2017. 49. arXiv: 1603.08675 (pages 1, 4).\n\nQuantum gradient descent for linear systems and least squares. Iordanis Kerenidis, Anupam Prakash, 10.1103/PhysRevA.101.022316arXiv:1704.04992Physical Review A. 101222316quant-ph. page 51Iordanis Kerenidis and Anupam Prakash. \"Quantum gradient descent for linear systems and least squares\". In: Physical Review A 101.2 (2020), p. 022316. DOI: 10.1103/PhysRevA.101.022316. arXiv: 1704.04992 [quant-ph] (page 51).\n\nQuantum machine learning with subspace states. Iordanis Kerenidis, Anupam Prakash, arXiv:2202.00054quant-phIordanis Kerenidis and Anupam Prakash. Quantum machine learning with sub- space states. Jan. 31, 2022. arXiv: 2202.00054 [quant-ph] (page 2).\n\nRandomized algorithms in numerical linear algebra. Ravindran Kannan, Santosh Vempala, 10.1017/S0962492917000058DOI: 10 . 1017 / S0962492917000058Acta Numerica. 26Ravindran Kannan and Santosh Vempala. \"Randomized algorithms in numer- ical linear algebra\". In: Acta Numerica 26 (2017), pp. 95-135. DOI: 10 . 1017 / S0962492917000058 (page 20).\n\nQuantum perceptron models. Ashish Kapoor, Nathan Wiebe, Krysta Svore, ; D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, arXiv:1602.04799Advances in Neural Information Processing Systems. Curran Associates, Inc29quant-phAshish Kapoor, Nathan Wiebe, and Krysta Svore. \"Quantum perceptron mod- els\". In: Advances in Neural Information Processing Systems. Ed. by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett. Vol. 29. Curran Associates, Inc., Feb. 15, 2016. arXiv: 1602.04799 [quant-ph] (page 1).\n\nAn iteration method for the solution of the eigenvalue problem of linear differential and integral operators. Cornelius Lanczos, Bur. Standards. J. Research Nat456Cornelius Lanczos. \"An iteration method for the solution of the eigenvalue problem of linear differential and integral operators\". In: J. Research Nat. Bur. Standards 45 (1950), pp. 255-282. ISSN: 0160-1741 (page 6).\n\nOptimal hamiltonian simulation by quantum signal processing. Hao Guang, Isaac L Low, Chuang, 10.1103/PhysRevLett.118.010501arXiv:1606.02685Physical Review Letters. 118110501quantphGuang Hao Low and Isaac L. Chuang. \"Optimal hamiltonian simulation by quantum signal processing\". In: Physical Review Letters 118.1 (Jan. 2017), p. 010501. DOI: 10.1103/PhysRevLett.118.010501. arXiv: 1606.02685 [quant- ph] (page 1).\n\nQuantum algorithms for topological and geometric analysis of data. Seth Lloyd, Silvano Garnerone, Paolo Zanardi, 10.1038/ncomms10138arXiv:1408.3106Nature Communications. 7113Seth Lloyd, Silvano Garnerone, and Paolo Zanardi. \"Quantum algorithms for topological and geometric analysis of data\". In: Nature Communications 7.1 (Jan. 2016), p. 10138. DOI: 10.1038/ncomms10138. arXiv: 1408.3106 (page 13).\n\nUsing a non-commutative bernstein bound to approximate some matrix algorithms in the spectral norm. Malik Magdon-Ismail, arXiv:1103.545312arXiv preprintMalik Magdon-Ismail. \"Using a non-commutative bernstein bound to ap- proximate some matrix algorithms in the spectral norm\". In: arXiv preprint arXiv:1103.5453 (2011) (page 12).\n\nRandomized algorithms for matrices and data. Michael W Mahoney, 10.1561/2200000035Foundations and Trends\u00ae in Machine Learning. 312Michael W. Mahoney. \"Randomized algorithms for matrices and data\". In: Foundations and Trends\u00ae in Machine Learning 3.2 (2011), pp. 123-224. ISSN: 1935- 8237. DOI: 10.1561/2200000035 (page 12).\n\n. C John, Mason, C David, Handscomb, 1530John C Mason and David C Handscomb. Chebyshev polynomials. Chapman and Hall/CRC, 2002 (pages 9, 15, 30, 32).\n\nRandomized block krylov methods for stronger and faster approximate singular value decomposition. Cameron Musco, Christopher Musco, Advances in neural information processing systems. 2813Cameron Musco and Christopher Musco. \"Randomized block krylov meth- ods for stronger and faster approximate singular value decomposition\". In: Advances in neural information processing systems 28 (2015) (page 13).\n\nStability of the lanczos method for matrix function approximation. Cameron Musco, Christopher Musco, Aaron Sidford, 10.1137/1.9781611975031.105DOI: 10. 1137/1.9781611975031.105Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms. the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms8Cameron Musco, Christopher Musco, and Aaron Sidford. \"Stability of the lanczos method for matrix function approximation\". In: Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, Jan. 2018, pp. 1605-1624. DOI: 10. 1137/1.9781611975031.105 (pages 8, 9, 32).\n\nGrand unification of quantum algorithms. John M Martyn, Zane M Rossi, Andrew K Tan, Isaac L Chuang, 10.1103/PRXQuantum.2.040203arXiv:2105.02859PRX Quantum. 240203quant-phJohn M. Martyn, Zane M. Rossi, Andrew K. Tan, and Isaac L. Chuang. \"Grand unification of quantum algorithms\". In: PRX Quantum 2 (4 Dec. 2021), p. 040203. DOI: 10.1103/PRXQuantum.2.040203. arXiv: 2105.02859 [quant-ph] (page 1).\n\nLow rank matrix-valued chernoff bounds and approximate matrix multiplication. Avner Magen, Anastasios Zouzias, Proceedings of the twentysecond annual ACM-SIAM symposium on Discrete Algorithms. SIAM. the twentysecond annual ACM-SIAM symposium on Discrete Algorithms. SIAM2012Avner Magen and Anastasios Zouzias. \"Low rank matrix-valued chernoff bounds and approximate matrix multiplication\". In: Proceedings of the twenty- second annual ACM-SIAM symposium on Discrete Algorithms. SIAM. 2011, pp. 1422- 1436 (pages 12, 20).\n\nOSNAP: faster numerical linear algebra algorithms via sparser subspace embeddings. Jelani Nelson, Huy L Nguyen, 10.1109/focs.2013.212013 IEEE 54 th Annual Symposium on Foundations of Computer Science. IEEEJelani Nelson and Huy L. Nguyen. \"OSNAP: faster numerical linear algebra algorithms via sparser subspace embeddings\". In: 2013 IEEE 54 th Annual Sym- posium on Foundations of Computer Science. IEEE, Oct. 2013. DOI: 10.1109/focs. 2013.21 (page 9).\n\nAn error analysis of the modified Clenshaw method for evaluating Chebyshev and Fourier series. J Oliver, J. Inst. Math. Appl. 20page 9J. Oliver. \"An error analysis of the modified Clenshaw method for evaluating Chebyshev and Fourier series\". In: J. Inst. Math. Appl. 20.3 (1977), pp. 379-391. ISSN: 0020-2932 (page 9).\n\nRounding error propagation in polynomial evaluation schemes. J Oliver, 10.1016/0771-050X(79)90002-0Journal of Computational and Applied Mathematics. 59J. Oliver. \"Rounding error propagation in polynomial evaluation schemes\". In: Journal of Computational and Applied Mathematics 5.2 (1979), pp. 85-97. ISSN: 0377-0427. DOI: 10.1016/0771-050X(79)90002-0 (pages 9, 32).\n\nError analysis of the lanczos algorithm for tridiagonalizing a symmetric matrix. C C Paige, 10.1093/imamat/18.3.341IMA Journal of Applied Mathematics. 188C. C. Paige. \"Error analysis of the lanczos algorithm for tridiagonalizing a symmetric matrix\". In: IMA Journal of Applied Mathematics 18.3 (1976), pp. 341- 349. DOI: 10.1093/imamat/18.3.341 (page 8).\n\nQuantum algorithms for linear algebra and machine learning. Anupam Prakash, University of California at BerkeleyPhD thesisAnupam Prakash. \"Quantum algorithms for linear algebra and machine learn- ing\". PhD thesis. University of California at Berkeley, 2014. URL: https://www2. eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-211.pdf (page 3).\n\nQuantum support vector machine for big data classification. Patrick Rebentrost, Masoud Mohseni, Seth Lloyd, 10.1103/PhysRevLett.113.130503arXiv:1307.0471Physical Review Letters. 1131130503Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. \"Quantum support vector machine for big data classification\". In: Physical Review Letters 113.13 (13 Sept. 2014), p. 130503. DOI: 10 . 1103 / PhysRevLett . 113 . 130503. arXiv: 1307.0471 (page 1).\n\nSampling from large matrices: an approach through geometric functional analysis. Mark Rudelson, Roman Vershynin, 10.1145/1255443.1255449Journal of the ACM. 54Mark Rudelson and Roman Vershynin. \"Sampling from large matrices: an approach through geometric functional analysis\". In: Journal of the ACM 54.4 (July 2007), 21-es. ISSN: 0004-5411. DOI: 10.1145/1255443.1255449. URL: https: //doi.org/10.1145/1255443.1255449 (page 20).\n\nKrylov subspace methods for solving large unsymmetric linear systems. Yousef Saad, Mathematics of computation. 37page 13Yousef Saad. \"Krylov subspace methods for solving large unsymmetric linear systems\". In: Mathematics of computation 37.155 (1981), pp. 105-126 (page 13).\n\nInequalities of A. Markoff and S. Bernstein for polynomials and related functions. A C Schaeffer, 10.1090/S0002-9904-1941-07510-5Bull. Amer. Math. Soc. 479A. C. Schaeffer. \"Inequalities of A. Markoff and S. Bernstein for polynomials and related functions\". In: Bull. Amer. Math. Soc. 47 (1941), pp. 565-579. ISSN: 0002-9904. DOI: 10.1090/S0002-9904-1941-07510-5 (pages 9, 23, 36).\n\nTwelve proofs of the Markov inequality. Aleksei Shadrin, Approximation theory: a volume dedicated to Borislav Bojanov. Prof. M. Drinov Acad. Publ. House, Sofia. page 36Aleksei Shadrin. \"Twelve proofs of the Markov inequality\". In: Approximation theory: a volume dedicated to Borislav Bojanov. Prof. M. Drinov Acad. Publ. House, Sofia, 2004, pp. 233-298 (page 36).\n\nFaster quantum-inspired algorithms for solving linear systems. Changpeng Shao, Ashley Montanaro, arXiv:2103.10309arXivquant-ph] (pages 2, 5)Changpeng Shao and Ashley Montanaro. \"Faster quantum-inspired algorithms for solving linear systems\". In: arXiv (2021). arXiv: 2103 . 10309 [quant-ph] (pages 2, 5).\n\nFaster algorithms via approximation theory. Sushant Sachdeva, Nisheeth K Vishnoi, 10.1561/0400000065Foundations and Trends in Theoretical Computer Science. 916Sushant Sachdeva and Nisheeth K. Vishnoi. \"Faster algorithms via approxima- tion theory\". In: Foundations and Trends in Theoretical Computer Science 9.2 (2014), pp. 125-210. ISSN: 1551-305X. DOI: 10.1561/0400000065 (page 16).\n\nA quantum-inspired classical algorithm for recommendation systems. Ewin Tang, 10.1145/3313276.3316310Proceedings of the 51 st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019. the 51 st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019ACM Presscs.IR] (pages 2, 6, 17, 51, 52Ewin Tang. \"A quantum-inspired classical algorithm for recommendation systems\". In: Proceedings of the 51 st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019. ACM Press, 2019, pp. 217-228. DOI: 10.1145/3313276. 3316310. arXiv: 1807.04271 [cs.IR] (pages 2, 6, 17, 51, 52).\n\nQuantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. Ewin Tang, 10.1103/PhysRevLett.127.060503arXiv:1811.00414Physical Review Letters. 12760503cs.IR] (pages 6, 10, 13Ewin Tang. \"Quantum principal component analysis only achieves an exponen- tial speedup because of its state preparation assumptions\". In: Physical Review Letters 127 (6 Aug. 2021), p. 060503. DOI: 10.1103/PhysRevLett.127.060503. arXiv: 1811.00414 [cs.IR] (pages 6, 10, 13).\n\nMatrix identities as derivatives of determinant identities. Terence Tao, page 26Terence Tao. Matrix identities as derivatives of determinant identities, 2013. 2013. URL: https : / / terrytao . wordpress . com / 2013 / 01 / 13 / matrix -identities -as - derivatives-of-determinant-identities (page 26).\n\n. N Lloyd, David Trefethen, Iii Bau, Numerical linear algebra. 5013Lloyd N Trefethen and David Bau III. Numerical linear algebra. Vol. 50. Siam, 1997 (page 13).\n\nApproximation theory and approximation practice. Lloyd N Trefethen, extended edition. Extended edition [of 3012510Lloyd N. Trefethen. Approximation theory and approximation practice, extended edition. Extended edition [of 3012510].\n\n. P A Philadelphia, 10.1137/1.9781611975949Society for Industrial and Applied Mathematics. 11428Philadelphia, PA: Society for Industrial and Applied Mathematics, 2019, pp. xi+363. ISBN: 978-1-611975-93-2. DOI: 10.1137/1.9781611975949 (pages 4, 11, 16, 23, 28, 32).\n\nAn introduction to matrix concentration inequalities. Joel A Tropp, 10.1561/2200000048arXiv:1501.01571Foundations and Trends\u00ae in Machine Learning. 822math.PRJoel A. Tropp. \"An introduction to matrix concentration inequalities\". In: Foun- dations and Trends\u00ae in Machine Learning 8.1-2 (2015), pp. 1-230. DOI: 10.1561/ 2200000048. arXiv: 1501.01571 [math.PR] (page 22).\n\nSimulating quantum computers with probabilistic methods. Maarten Van Den, Nest, In: Quantum Info. Comput. 114Maarten Van Den Nest. \"Simulating quantum computers with probabilistic methods\". In: Quantum Info. Comput. 11.9-10 (Sept. 2011). ISSN: 1533-7146 (page 4).\n\nA linear algorithm for generating random numbers with a given distribution. Michael D Vose, 10.1109/32.92917IEEE Transactions on Software Engineering. 1718Michael D. Vose. \"A linear algorithm for generating random numbers with a given distribution\". In: IEEE Transactions on Software Engineering 17.9 (1991), pp. 972-975. DOI: 10.1109/32.92917 (page 18).\n\nSketching as a tool for numerical linear algebra. David P Woodruff, 10.1561/0400000060Foundations and Trends\u00ae in Theoretical Computer Science. 1012David P. Woodruff. \"Sketching as a tool for numerical linear algebra\". In: Foundations and Trends\u00ae in Theoretical Computer Science 10.1-2 (2014), pp. 1-157. ISSN: 1551-305X. DOI: 10.1561/0400000060 (pages 12, 19, 20).\n\nQuantum linear system algorithm for dense matrices. Leonard Wossnig, Zhikuan Zhao, Anupam Prakash, 10.1103/PhysRevLett.120.050502arXiv:1704.06174Physical Review Letters. 120450502Leonard Wossnig, Zhikuan Zhao, and Anupam Prakash. \"Quantum linear system algorithm for dense matrices\". In: Physical Review Letters 120.5 (2018), p. 050502. DOI: 10.1103/PhysRevLett.120.050502. arXiv: 1704.06174 (page 4).\n\nQuantumassisted Gaussian process regression. Zhikuan Zhao, Jack K Fitzsimons, Joseph F Fitzsimons, 10.1103/PhysRevA.99.052331arXiv:1512.03929Physical Review A. 99513quant-phZhikuan Zhao, Jack K. Fitzsimons, and Joseph F. Fitzsimons. \"Quantum- assisted Gaussian process regression\". In: Physical Review A 99 (5 May 2019), p. 052331. DOI: 10.1103/PhysRevA.99.052331. arXiv: 1512.03929 [quant-ph] (page 13).\n", "annotations": {"author": "[{\"end\":154,\"start\":94},{\"end\":220,\"start\":155}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":101},{\"end\":164,\"start\":160}]", "author_first_name": "[{\"end\":100,\"start\":94},{\"end\":159,\"start\":155}]", "author_affiliation": "[{\"end\":153,\"start\":124},{\"end\":219,\"start\":190}]", "title": "[{\"end\":81,\"start\":1},{\"end\":301,\"start\":221}]", "venue": null, "abstract": "[{\"end\":2393,\"start\":313}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2584,\"start\":2578},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2591,\"start\":2584},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2701,\"start\":2694},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2730,\"start\":2723},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2763,\"start\":2756},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2794,\"start\":2788},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2836,\"start\":2826},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3497,\"start\":3489},{\"end\":4243,\"start\":4242},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5298,\"start\":5291},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5693,\"start\":5686},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5698,\"start\":5693},{\"end\":5806,\"start\":5805},{\"end\":5945,\"start\":5944},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6342,\"start\":6336},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7480,\"start\":7473},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7678,\"start\":7671},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7683,\"start\":7678},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8285,\"start\":8275},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8291,\"start\":8285},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8333,\"start\":8324},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8372,\"start\":8362},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8553,\"start\":8544},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8558,\"start\":8553},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":9484,\"start\":9477},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9826,\"start\":9820},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":10128,\"start\":10121},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11117,\"start\":11111},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11123,\"start\":11117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11684,\"start\":11674},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11697,\"start\":11687},{\"end\":11708,\"start\":11697},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11960,\"start\":11951},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12126,\"start\":12117},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12545,\"start\":12539},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":13525,\"start\":13519},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13857,\"start\":13850},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14560,\"start\":14550},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14593,\"start\":14583},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14765,\"start\":14755},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17810,\"start\":17803},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17849,\"start\":17842},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21102,\"start\":21092},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23631,\"start\":23624},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":24117,\"start\":24110},{\"end\":24120,\"start\":24119},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":24841,\"start\":24834},{\"end\":25204,\"start\":25202},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":25407,\"start\":25400},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":26805,\"start\":26798},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":26811,\"start\":26805},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26915,\"start\":26908},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":27025,\"start\":27018},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27782,\"start\":27776},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":27866,\"start\":27857},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28527,\"start\":28521},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30380,\"start\":30374},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":30475,\"start\":30468},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33388,\"start\":33378},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35788,\"start\":35779},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35857,\"start\":35848},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36311,\"start\":36302},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":37311,\"start\":37304},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":37332,\"start\":37325},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37527,\"start\":37520},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":37630,\"start\":37624},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38090,\"start\":38082},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38566,\"start\":38560},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38694,\"start\":38687},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40385,\"start\":40378},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":40904,\"start\":40897},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41004,\"start\":40996},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":41724,\"start\":41717},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":41795,\"start\":41788},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43187,\"start\":43177},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44043,\"start\":44033},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":46186,\"start\":46176},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":47738,\"start\":47732},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":48486,\"start\":48479},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":48824,\"start\":48818},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":49167,\"start\":49157},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":52950,\"start\":52943},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":55477,\"start\":55469},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":55732,\"start\":55725},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":57558,\"start\":57551},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":57663,\"start\":57656},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":57733,\"start\":57725},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":59469,\"start\":59463},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":59474,\"start\":59469},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":59524,\"start\":59518},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":63365,\"start\":63358},{\"end\":63370,\"start\":63365},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":63676,\"start\":63669},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":68449,\"start\":68442},{\"end\":72885,\"start\":72883},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":73005,\"start\":72998},{\"end\":73017,\"start\":73005},{\"end\":73021,\"start\":73019},{\"end\":75642,\"start\":75632},{\"end\":76934,\"start\":76924},{\"end\":77327,\"start\":77317},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":80239,\"start\":80232},{\"end\":80294,\"start\":80285},{\"end\":82842,\"start\":82840},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":86606,\"start\":86599},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":86946,\"start\":86939},{\"end\":100370,\"start\":100365},{\"end\":101581,\"start\":101576},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":111590,\"start\":111584},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":112156,\"start\":112154},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":113040,\"start\":113031},{\"end\":113051,\"start\":113040},{\"end\":113412,\"start\":113410},{\"end\":115684,\"start\":115682},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":115884,\"start\":115875},{\"end\":115888,\"start\":115886},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":116300,\"start\":116291},{\"end\":116311,\"start\":116300},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":125301,\"start\":125295},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":128686,\"start\":128679},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":129815,\"start\":129807}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":119271,\"start\":119018},{\"attributes\":{\"id\":\"fig_1\"},\"end\":119682,\"start\":119272},{\"attributes\":{\"id\":\"fig_2\"},\"end\":119815,\"start\":119683},{\"attributes\":{\"id\":\"fig_3\"},\"end\":120017,\"start\":119816},{\"attributes\":{\"id\":\"fig_4\"},\"end\":120078,\"start\":120018},{\"attributes\":{\"id\":\"fig_5\"},\"end\":120274,\"start\":120079},{\"attributes\":{\"id\":\"fig_6\"},\"end\":120354,\"start\":120275},{\"attributes\":{\"id\":\"fig_7\"},\"end\":120590,\"start\":120355},{\"attributes\":{\"id\":\"fig_8\"},\"end\":120887,\"start\":120591},{\"attributes\":{\"id\":\"fig_9\"},\"end\":121006,\"start\":120888},{\"attributes\":{\"id\":\"fig_10\"},\"end\":121108,\"start\":121007},{\"attributes\":{\"id\":\"fig_11\"},\"end\":121206,\"start\":121109},{\"attributes\":{\"id\":\"fig_13\"},\"end\":121603,\"start\":121207},{\"attributes\":{\"id\":\"fig_14\"},\"end\":121867,\"start\":121604},{\"attributes\":{\"id\":\"fig_15\"},\"end\":122259,\"start\":121868},{\"attributes\":{\"id\":\"fig_16\"},\"end\":122370,\"start\":122260},{\"attributes\":{\"id\":\"fig_17\"},\"end\":122744,\"start\":122371},{\"attributes\":{\"id\":\"fig_18\"},\"end\":122892,\"start\":122745},{\"attributes\":{\"id\":\"fig_19\"},\"end\":123256,\"start\":122893},{\"attributes\":{\"id\":\"fig_20\"},\"end\":123346,\"start\":123257},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":123618,\"start\":123347},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":123747,\"start\":123619},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":124315,\"start\":123748},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":124446,\"start\":124316},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":124552,\"start\":124447},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":125218,\"start\":124553},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":125866,\"start\":125219},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":126133,\"start\":125867},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":126220,\"start\":126134},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":126417,\"start\":126221},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":126666,\"start\":126418},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":126809,\"start\":126667},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":127437,\"start\":126810},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":128007,\"start\":127438},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":128400,\"start\":128008},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":128632,\"start\":128401}]", "paragraph": "[{\"end\":3166,\"start\":2409},{\"end\":4614,\"start\":3168},{\"end\":5473,\"start\":4616},{\"end\":6489,\"start\":5475},{\"end\":7881,\"start\":6491},{\"end\":8005,\"start\":7883},{\"end\":8939,\"start\":8017},{\"end\":9456,\"start\":8941},{\"end\":9775,\"start\":9458},{\"end\":10477,\"start\":9777},{\"end\":10711,\"start\":10514},{\"end\":12227,\"start\":10713},{\"end\":13273,\"start\":12257},{\"end\":13952,\"start\":13275},{\"end\":14487,\"start\":13954},{\"end\":14721,\"start\":14522},{\"end\":15088,\"start\":14744},{\"end\":16272,\"start\":15090},{\"end\":16934,\"start\":16274},{\"end\":17421,\"start\":16936},{\"end\":18381,\"start\":17423},{\"end\":18480,\"start\":18383},{\"end\":18569,\"start\":18482},{\"end\":18654,\"start\":18571},{\"end\":18714,\"start\":18690},{\"end\":18911,\"start\":18746},{\"end\":19078,\"start\":18913},{\"end\":19244,\"start\":19175},{\"end\":19330,\"start\":19246},{\"end\":19810,\"start\":19592},{\"end\":20089,\"start\":19851},{\"end\":20271,\"start\":20183},{\"end\":20838,\"start\":20368},{\"end\":21641,\"start\":20924},{\"end\":21704,\"start\":21643},{\"end\":22498,\"start\":21857},{\"end\":23487,\"start\":22500},{\"end\":24541,\"start\":23489},{\"end\":25737,\"start\":24604},{\"end\":26429,\"start\":25739},{\"end\":27453,\"start\":26431},{\"end\":28035,\"start\":27455},{\"end\":29069,\"start\":28037},{\"end\":29417,\"start\":29106},{\"end\":30004,\"start\":29440},{\"end\":30190,\"start\":30078},{\"end\":30613,\"start\":30221},{\"end\":31617,\"start\":30615},{\"end\":31727,\"start\":31619},{\"end\":32051,\"start\":31792},{\"end\":32739,\"start\":32159},{\"end\":33526,\"start\":32741},{\"end\":34076,\"start\":33528},{\"end\":34792,\"start\":34078},{\"end\":35750,\"start\":34794},{\"end\":36273,\"start\":35765},{\"end\":37106,\"start\":36275},{\"end\":38091,\"start\":37108},{\"end\":38811,\"start\":38093},{\"end\":39474,\"start\":38813},{\"end\":41118,\"start\":39476},{\"end\":42089,\"start\":41120},{\"end\":42872,\"start\":42091},{\"end\":43266,\"start\":42874},{\"end\":43563,\"start\":43268},{\"end\":44107,\"start\":43565},{\"end\":44325,\"start\":44109},{\"end\":44629,\"start\":44343},{\"end\":44852,\"start\":44682},{\"end\":44943,\"start\":44871},{\"end\":45354,\"start\":45111},{\"end\":45612,\"start\":45356},{\"end\":46140,\"start\":45685},{\"end\":46334,\"start\":46142},{\"end\":46357,\"start\":46336},{\"end\":46637,\"start\":46359},{\"end\":47217,\"start\":47124},{\"end\":47363,\"start\":47345},{\"end\":47740,\"start\":47639},{\"end\":48274,\"start\":48025},{\"end\":48572,\"start\":48321},{\"end\":48730,\"start\":48604},{\"end\":48869,\"start\":48787},{\"end\":49657,\"start\":49062},{\"end\":50379,\"start\":49659},{\"end\":50456,\"start\":50389},{\"end\":50792,\"start\":50513},{\"end\":51086,\"start\":50794},{\"end\":51697,\"start\":51126},{\"end\":52123,\"start\":51896},{\"end\":52380,\"start\":52125},{\"end\":52795,\"start\":52450},{\"end\":52883,\"start\":52797},{\"end\":53752,\"start\":52885},{\"end\":54023,\"start\":53754},{\"end\":54535,\"start\":54449},{\"end\":54655,\"start\":54595},{\"end\":55611,\"start\":54852},{\"end\":55952,\"start\":55647},{\"end\":56165,\"start\":56000},{\"end\":56280,\"start\":56273},{\"end\":56474,\"start\":56377},{\"end\":56681,\"start\":56558},{\"end\":57196,\"start\":57133},{\"end\":57316,\"start\":57198},{\"end\":57833,\"start\":57424},{\"end\":58121,\"start\":57835},{\"end\":58336,\"start\":58185},{\"end\":58562,\"start\":58394},{\"end\":58729,\"start\":58564},{\"end\":58876,\"start\":58831},{\"end\":59028,\"start\":58974},{\"end\":59094,\"start\":59030},{\"end\":59646,\"start\":59401},{\"end\":59924,\"start\":59889},{\"end\":60121,\"start\":60089},{\"end\":60208,\"start\":60189},{\"end\":60596,\"start\":60562},{\"end\":60906,\"start\":60691},{\"end\":61428,\"start\":61371},{\"end\":61647,\"start\":61568},{\"end\":61810,\"start\":61649},{\"end\":62553,\"start\":62454},{\"end\":62755,\"start\":62674},{\"end\":62971,\"start\":62835},{\"end\":63694,\"start\":63127},{\"end\":63986,\"start\":63880},{\"end\":64097,\"start\":64013},{\"end\":64293,\"start\":64249},{\"end\":64867,\"start\":64501},{\"end\":65059,\"start\":64869},{\"end\":65523,\"start\":65204},{\"end\":65686,\"start\":65525},{\"end\":66202,\"start\":66143},{\"end\":66577,\"start\":66470},{\"end\":66753,\"start\":66728},{\"end\":67208,\"start\":66755},{\"end\":67307,\"start\":67210},{\"end\":67401,\"start\":67326},{\"end\":67675,\"start\":67581},{\"end\":67904,\"start\":67772},{\"end\":68112,\"start\":67906},{\"end\":68450,\"start\":68236},{\"end\":68966,\"start\":68657},{\"end\":69127,\"start\":68968},{\"end\":69296,\"start\":69259},{\"end\":69351,\"start\":69298},{\"end\":69471,\"start\":69389},{\"end\":69671,\"start\":69527},{\"end\":69848,\"start\":69673},{\"end\":69928,\"start\":69912},{\"end\":70353,\"start\":69966},{\"end\":70472,\"start\":70404},{\"end\":71048,\"start\":70474},{\"end\":71252,\"start\":71050},{\"end\":72536,\"start\":71980},{\"end\":73087,\"start\":72538},{\"end\":73496,\"start\":73129},{\"end\":73673,\"start\":73572},{\"end\":73795,\"start\":73706},{\"end\":73991,\"start\":73880},{\"end\":74062,\"start\":74025},{\"end\":74272,\"start\":74251},{\"end\":74692,\"start\":74427},{\"end\":75398,\"start\":74790},{\"end\":75925,\"start\":75438},{\"end\":76299,\"start\":76025},{\"end\":76351,\"start\":76301},{\"end\":76581,\"start\":76458},{\"end\":76775,\"start\":76583},{\"end\":77077,\"start\":76809},{\"end\":77426,\"start\":77248},{\"end\":77807,\"start\":77791},{\"end\":78192,\"start\":77908},{\"end\":78673,\"start\":78416},{\"end\":79279,\"start\":79020},{\"end\":79543,\"start\":79326},{\"end\":80099,\"start\":79545},{\"end\":80240,\"start\":80131},{\"end\":80535,\"start\":80242},{\"end\":80738,\"start\":80645},{\"end\":80992,\"start\":80854},{\"end\":81016,\"start\":80994},{\"end\":81390,\"start\":81121},{\"end\":81618,\"start\":81515},{\"end\":81691,\"start\":81620},{\"end\":81811,\"start\":81693},{\"end\":82377,\"start\":82373},{\"end\":82906,\"start\":82799},{\"end\":83123,\"start\":83084},{\"end\":83432,\"start\":83362},{\"end\":83632,\"start\":83561},{\"end\":84189,\"start\":84104},{\"end\":84337,\"start\":84310},{\"end\":84652,\"start\":84511},{\"end\":84997,\"start\":84780},{\"end\":85538,\"start\":85118},{\"end\":85741,\"start\":85588},{\"end\":86105,\"start\":85940},{\"end\":87080,\"start\":86455},{\"end\":87156,\"start\":87113},{\"end\":87555,\"start\":87158},{\"end\":87716,\"start\":87658},{\"end\":88818,\"start\":88084},{\"end\":89265,\"start\":88855},{\"end\":89400,\"start\":89300},{\"end\":89492,\"start\":89402},{\"end\":89674,\"start\":89599},{\"end\":89838,\"start\":89786},{\"end\":90008,\"start\":89943},{\"end\":90239,\"start\":90091},{\"end\":91180,\"start\":90382},{\"end\":91351,\"start\":91182},{\"end\":91493,\"start\":91404},{\"end\":91532,\"start\":91495},{\"end\":91723,\"start\":91570},{\"end\":91935,\"start\":91825},{\"end\":93710,\"start\":92161},{\"end\":94340,\"start\":93712},{\"end\":94622,\"start\":94342},{\"end\":94805,\"start\":94753},{\"end\":95055,\"start\":94867},{\"end\":95229,\"start\":95185},{\"end\":95510,\"start\":95320},{\"end\":95759,\"start\":95512},{\"end\":95883,\"start\":95761},{\"end\":96343,\"start\":96273},{\"end\":96423,\"start\":96406},{\"end\":96477,\"start\":96425},{\"end\":96745,\"start\":96561},{\"end\":97019,\"start\":96822},{\"end\":97200,\"start\":97021},{\"end\":97564,\"start\":97437},{\"end\":97775,\"start\":97740},{\"end\":97948,\"start\":97891},{\"end\":98103,\"start\":98007},{\"end\":98297,\"start\":98176},{\"end\":98712,\"start\":98299},{\"end\":99029,\"start\":98953},{\"end\":99332,\"start\":99199},{\"end\":99607,\"start\":99484},{\"end\":99714,\"start\":99695},{\"end\":100472,\"start\":100257},{\"end\":100654,\"start\":100534},{\"end\":101019,\"start\":100926},{\"end\":101606,\"start\":101498},{\"end\":101758,\"start\":101677},{\"end\":101765,\"start\":101760},{\"end\":101990,\"start\":101899},{\"end\":102180,\"start\":102088},{\"end\":102243,\"start\":102182},{\"end\":102491,\"start\":102454},{\"end\":102948,\"start\":102528},{\"end\":103120,\"start\":102950},{\"end\":103251,\"start\":103151},{\"end\":103349,\"start\":103253},{\"end\":103589,\"start\":103468},{\"end\":103953,\"start\":103844},{\"end\":104096,\"start\":104007},{\"end\":104135,\"start\":104098},{\"end\":104225,\"start\":104173},{\"end\":104431,\"start\":104353},{\"end\":104572,\"start\":104491},{\"end\":104863,\"start\":104674},{\"end\":105120,\"start\":105033},{\"end\":105342,\"start\":105219},{\"end\":105696,\"start\":105502},{\"end\":105839,\"start\":105759},{\"end\":106045,\"start\":105924},{\"end\":106447,\"start\":106047},{\"end\":106785,\"start\":106709},{\"end\":107100,\"start\":106958},{\"end\":107300,\"start\":107264},{\"end\":107436,\"start\":107394},{\"end\":107705,\"start\":107574},{\"end\":107884,\"start\":107812},{\"end\":108059,\"start\":107944},{\"end\":108375,\"start\":108114},{\"end\":108477,\"start\":108377},{\"end\":108817,\"start\":108654},{\"end\":109142,\"start\":108886},{\"end\":109949,\"start\":109539},{\"end\":110547,\"start\":110029},{\"end\":110602,\"start\":110549},{\"end\":111147,\"start\":111083},{\"end\":111460,\"start\":111179},{\"end\":112198,\"start\":111487},{\"end\":112801,\"start\":112200},{\"end\":113175,\"start\":112803},{\"end\":113330,\"start\":113259},{\"end\":113444,\"start\":113397},{\"end\":114608,\"start\":113464},{\"end\":114745,\"start\":114643},{\"end\":115372,\"start\":114767},{\"end\":116145,\"start\":115374},{\"end\":116262,\"start\":116167},{\"end\":116607,\"start\":116264},{\"end\":117295,\"start\":116628},{\"end\":117590,\"start\":117322},{\"end\":117840,\"start\":117592},{\"end\":118087,\"start\":117942},{\"end\":118468,\"start\":118089},{\"end\":118584,\"start\":118490},{\"end\":119017,\"start\":118586}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14521,\"start\":14488},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18689,\"start\":18655},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18745,\"start\":18715},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19174,\"start\":19101},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19591,\"start\":19331},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19850,\"start\":19811},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20182,\"start\":20090},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20367,\"start\":20272},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20923,\"start\":20839},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21856,\"start\":21705},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24603,\"start\":24542},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29105,\"start\":29070},{\"attributes\":{\"id\":\"formula_12\"},\"end\":29439,\"start\":29418},{\"attributes\":{\"id\":\"formula_13\"},\"end\":30077,\"start\":30005},{\"attributes\":{\"id\":\"formula_14\"},\"end\":30220,\"start\":30191},{\"attributes\":{\"id\":\"formula_15\"},\"end\":31791,\"start\":31728},{\"attributes\":{\"id\":\"formula_16\"},\"end\":32158,\"start\":32052},{\"attributes\":{\"id\":\"formula_17\"},\"end\":44681,\"start\":44630},{\"attributes\":{\"id\":\"formula_18\"},\"end\":45110,\"start\":44944},{\"attributes\":{\"id\":\"formula_19\"},\"end\":45646,\"start\":45613},{\"attributes\":{\"id\":\"formula_20\"},\"end\":47041,\"start\":46638},{\"attributes\":{\"id\":\"formula_21\"},\"end\":47123,\"start\":47041},{\"attributes\":{\"id\":\"formula_22\"},\"end\":47344,\"start\":47218},{\"attributes\":{\"id\":\"formula_23\"},\"end\":47638,\"start\":47364},{\"attributes\":{\"id\":\"formula_24\"},\"end\":47955,\"start\":47741},{\"attributes\":{\"id\":\"formula_25\"},\"end\":48024,\"start\":47955},{\"attributes\":{\"id\":\"formula_26\"},\"end\":48320,\"start\":48275},{\"attributes\":{\"id\":\"formula_27\"},\"end\":48603,\"start\":48573},{\"attributes\":{\"id\":\"formula_28\"},\"end\":48786,\"start\":48731},{\"attributes\":{\"id\":\"formula_29\"},\"end\":49033,\"start\":48870},{\"attributes\":{\"id\":\"formula_30\"},\"end\":50388,\"start\":50380},{\"attributes\":{\"id\":\"formula_31\"},\"end\":50512,\"start\":50457},{\"attributes\":{\"id\":\"formula_32\"},\"end\":51125,\"start\":51087},{\"attributes\":{\"id\":\"formula_33\"},\"end\":51895,\"start\":51698},{\"attributes\":{\"id\":\"formula_34\"},\"end\":52449,\"start\":52381},{\"attributes\":{\"id\":\"formula_35\"},\"end\":54448,\"start\":54024},{\"attributes\":{\"id\":\"formula_36\"},\"end\":54594,\"start\":54536},{\"attributes\":{\"id\":\"formula_37\"},\"end\":54851,\"start\":54656},{\"attributes\":{\"id\":\"formula_38\"},\"end\":56272,\"start\":56166},{\"attributes\":{\"id\":\"formula_39\"},\"end\":56557,\"start\":56475},{\"attributes\":{\"id\":\"formula_40\"},\"end\":57132,\"start\":56682},{\"attributes\":{\"id\":\"formula_41\"},\"end\":57362,\"start\":57317},{\"attributes\":{\"id\":\"formula_42\"},\"end\":58184,\"start\":58122},{\"attributes\":{\"id\":\"formula_43\"},\"end\":58393,\"start\":58337},{\"attributes\":{\"id\":\"formula_44\"},\"end\":58830,\"start\":58730},{\"attributes\":{\"id\":\"formula_45\"},\"end\":58973,\"start\":58877},{\"attributes\":{\"id\":\"formula_46\"},\"end\":59400,\"start\":59095},{\"attributes\":{\"id\":\"formula_47\"},\"end\":59888,\"start\":59647},{\"attributes\":{\"id\":\"formula_48\"},\"end\":60088,\"start\":59925},{\"attributes\":{\"id\":\"formula_49\"},\"end\":60188,\"start\":60122},{\"attributes\":{\"id\":\"formula_50\"},\"end\":60561,\"start\":60209},{\"attributes\":{\"id\":\"formula_51\"},\"end\":60690,\"start\":60597},{\"attributes\":{\"id\":\"formula_52\"},\"end\":61000,\"start\":60907},{\"attributes\":{\"id\":\"formula_53\"},\"end\":61311,\"start\":61000},{\"attributes\":{\"id\":\"formula_54\"},\"end\":61370,\"start\":61311},{\"attributes\":{\"id\":\"formula_55\"},\"end\":61567,\"start\":61429},{\"attributes\":{\"id\":\"formula_56\"},\"end\":62453,\"start\":61811},{\"attributes\":{\"id\":\"formula_57\"},\"end\":62673,\"start\":62554},{\"attributes\":{\"id\":\"formula_58\"},\"end\":62834,\"start\":62756},{\"attributes\":{\"id\":\"formula_59\"},\"end\":63068,\"start\":62972},{\"attributes\":{\"id\":\"formula_60\"},\"end\":63126,\"start\":63101},{\"attributes\":{\"id\":\"formula_61\"},\"end\":63879,\"start\":63695},{\"attributes\":{\"id\":\"formula_62\"},\"end\":64012,\"start\":63987},{\"attributes\":{\"id\":\"formula_63\"},\"end\":64248,\"start\":64098},{\"attributes\":{\"id\":\"formula_64\"},\"end\":64500,\"start\":64294},{\"attributes\":{\"id\":\"formula_65\"},\"end\":65203,\"start\":65060},{\"attributes\":{\"id\":\"formula_66\"},\"end\":66142,\"start\":65687},{\"attributes\":{\"id\":\"formula_67\"},\"end\":66469,\"start\":66203},{\"attributes\":{\"id\":\"formula_68\"},\"end\":66727,\"start\":66578},{\"attributes\":{\"id\":\"formula_69\"},\"end\":67325,\"start\":67308},{\"attributes\":{\"id\":\"formula_70\"},\"end\":67580,\"start\":67402},{\"attributes\":{\"id\":\"formula_71\"},\"end\":67771,\"start\":67676},{\"attributes\":{\"id\":\"formula_72\"},\"end\":68235,\"start\":68113},{\"attributes\":{\"id\":\"formula_73\"},\"end\":68656,\"start\":68451},{\"attributes\":{\"id\":\"formula_74\"},\"end\":69258,\"start\":69128},{\"attributes\":{\"id\":\"formula_75\"},\"end\":69388,\"start\":69352},{\"attributes\":{\"id\":\"formula_76\"},\"end\":69526,\"start\":69472},{\"attributes\":{\"id\":\"formula_77\"},\"end\":69911,\"start\":69849},{\"attributes\":{\"id\":\"formula_78\"},\"end\":69965,\"start\":69929},{\"attributes\":{\"id\":\"formula_79\"},\"end\":70403,\"start\":70354},{\"attributes\":{\"id\":\"formula_80\"},\"end\":71794,\"start\":71253},{\"attributes\":{\"id\":\"formula_81\"},\"end\":71979,\"start\":71794},{\"attributes\":{\"id\":\"formula_82\"},\"end\":73128,\"start\":73088},{\"attributes\":{\"id\":\"formula_83\"},\"end\":73705,\"start\":73674},{\"attributes\":{\"id\":\"formula_84\"},\"end\":73879,\"start\":73796},{\"attributes\":{\"id\":\"formula_85\"},\"end\":74024,\"start\":73992},{\"attributes\":{\"id\":\"formula_86\"},\"end\":74250,\"start\":74063},{\"attributes\":{\"id\":\"formula_87\"},\"end\":74426,\"start\":74273},{\"attributes\":{\"id\":\"formula_88\"},\"end\":74789,\"start\":74693},{\"attributes\":{\"id\":\"formula_89\"},\"end\":76024,\"start\":75926},{\"attributes\":{\"id\":\"formula_90\"},\"end\":76457,\"start\":76352},{\"attributes\":{\"id\":\"formula_91\"},\"end\":76808,\"start\":76776},{\"attributes\":{\"id\":\"formula_92\"},\"end\":77247,\"start\":77078},{\"attributes\":{\"id\":\"formula_93\"},\"end\":77790,\"start\":77427},{\"attributes\":{\"id\":\"formula_94\"},\"end\":77907,\"start\":77808},{\"attributes\":{\"id\":\"formula_95\"},\"end\":78415,\"start\":78193},{\"attributes\":{\"id\":\"formula_96\"},\"end\":78974,\"start\":78674},{\"attributes\":{\"id\":\"formula_97\"},\"end\":79325,\"start\":79280},{\"attributes\":{\"id\":\"formula_98\"},\"end\":80644,\"start\":80536},{\"attributes\":{\"id\":\"formula_99\"},\"end\":80853,\"start\":80739},{\"attributes\":{\"id\":\"formula_100\"},\"end\":81120,\"start\":81017},{\"attributes\":{\"id\":\"formula_101\"},\"end\":81514,\"start\":81391},{\"attributes\":{\"id\":\"formula_102\"},\"end\":81831,\"start\":81812},{\"attributes\":{\"id\":\"formula_103\"},\"end\":82372,\"start\":81831},{\"attributes\":{\"id\":\"formula_104\"},\"end\":82798,\"start\":82378},{\"attributes\":{\"id\":\"formula_105\"},\"end\":83083,\"start\":82907},{\"attributes\":{\"id\":\"formula_106\"},\"end\":83361,\"start\":83124},{\"attributes\":{\"id\":\"formula_107\"},\"end\":83560,\"start\":83433},{\"attributes\":{\"id\":\"formula_108\"},\"end\":84103,\"start\":83633},{\"attributes\":{\"id\":\"formula_109\"},\"end\":84309,\"start\":84190},{\"attributes\":{\"id\":\"formula_110\"},\"end\":84459,\"start\":84338},{\"attributes\":{\"id\":\"formula_111\"},\"end\":84779,\"start\":84653},{\"attributes\":{\"id\":\"formula_112\"},\"end\":85117,\"start\":84998},{\"attributes\":{\"id\":\"formula_113\"},\"end\":85587,\"start\":85539},{\"attributes\":{\"id\":\"formula_114\"},\"end\":85939,\"start\":85742},{\"attributes\":{\"id\":\"formula_115\"},\"end\":86454,\"start\":86106},{\"attributes\":{\"id\":\"formula_116\"},\"end\":87657,\"start\":87556},{\"attributes\":{\"id\":\"formula_117\"},\"end\":88083,\"start\":87717},{\"attributes\":{\"id\":\"formula_118\"},\"end\":89299,\"start\":89266},{\"attributes\":{\"id\":\"formula_119\"},\"end\":89598,\"start\":89493},{\"attributes\":{\"id\":\"formula_120\"},\"end\":89785,\"start\":89675},{\"attributes\":{\"id\":\"formula_121\"},\"end\":89942,\"start\":89839},{\"attributes\":{\"id\":\"formula_122\"},\"end\":90090,\"start\":90009},{\"attributes\":{\"id\":\"formula_123\"},\"end\":90381,\"start\":90240},{\"attributes\":{\"id\":\"formula_124\"},\"end\":91569,\"start\":91533},{\"attributes\":{\"id\":\"formula_125\"},\"end\":91824,\"start\":91746},{\"attributes\":{\"id\":\"formula_126\"},\"end\":92160,\"start\":91936},{\"attributes\":{\"id\":\"formula_127\"},\"end\":94752,\"start\":94623},{\"attributes\":{\"id\":\"formula_128\"},\"end\":94866,\"start\":94806},{\"attributes\":{\"id\":\"formula_129\"},\"end\":95184,\"start\":95056},{\"attributes\":{\"id\":\"formula_130\"},\"end\":95319,\"start\":95230},{\"attributes\":{\"id\":\"formula_131\"},\"end\":96272,\"start\":95884},{\"attributes\":{\"id\":\"formula_132\"},\"end\":96405,\"start\":96344},{\"attributes\":{\"id\":\"formula_133\"},\"end\":96560,\"start\":96478},{\"attributes\":{\"id\":\"formula_134\"},\"end\":96810,\"start\":96746},{\"attributes\":{\"id\":\"formula_135\"},\"end\":96821,\"start\":96810},{\"attributes\":{\"id\":\"formula_136\"},\"end\":97436,\"start\":97201},{\"attributes\":{\"id\":\"formula_137\"},\"end\":97739,\"start\":97565},{\"attributes\":{\"id\":\"formula_138\"},\"end\":97859,\"start\":97776},{\"attributes\":{\"id\":\"formula_139\"},\"end\":97890,\"start\":97859},{\"attributes\":{\"id\":\"formula_140\"},\"end\":98006,\"start\":97949},{\"attributes\":{\"id\":\"formula_141\"},\"end\":98175,\"start\":98104},{\"attributes\":{\"id\":\"formula_142\"},\"end\":98952,\"start\":98713},{\"attributes\":{\"id\":\"formula_143\"},\"end\":99198,\"start\":99030},{\"attributes\":{\"id\":\"formula_144\"},\"end\":99483,\"start\":99333},{\"attributes\":{\"id\":\"formula_145\"},\"end\":99694,\"start\":99608},{\"attributes\":{\"id\":\"formula_146\"},\"end\":100222,\"start\":99715},{\"attributes\":{\"id\":\"formula_147\"},\"end\":100256,\"start\":100222},{\"attributes\":{\"id\":\"formula_148\"},\"end\":100533,\"start\":100473},{\"attributes\":{\"id\":\"formula_149\"},\"end\":100829,\"start\":100655},{\"attributes\":{\"id\":\"formula_150\"},\"end\":100925,\"start\":100829},{\"attributes\":{\"id\":\"formula_151\"},\"end\":101497,\"start\":101020},{\"attributes\":{\"id\":\"formula_152\"},\"end\":101676,\"start\":101607},{\"attributes\":{\"id\":\"formula_153\"},\"end\":101847,\"start\":101766},{\"attributes\":{\"id\":\"formula_154\"},\"end\":101898,\"start\":101847},{\"attributes\":{\"id\":\"formula_155\"},\"end\":102087,\"start\":101991},{\"attributes\":{\"id\":\"formula_156\"},\"end\":102453,\"start\":102244},{\"attributes\":{\"id\":\"formula_157\"},\"end\":103150,\"start\":103121},{\"attributes\":{\"id\":\"formula_158\"},\"end\":103467,\"start\":103350},{\"attributes\":{\"id\":\"formula_159\"},\"end\":103742,\"start\":103590},{\"attributes\":{\"id\":\"formula_160\"},\"end\":103843,\"start\":103742},{\"attributes\":{\"id\":\"formula_161\"},\"end\":104172,\"start\":104136},{\"attributes\":{\"id\":\"formula_162\"},\"end\":104352,\"start\":104226},{\"attributes\":{\"id\":\"formula_163\"},\"end\":104490,\"start\":104432},{\"attributes\":{\"id\":\"formula_164\"},\"end\":104673,\"start\":104595},{\"attributes\":{\"id\":\"formula_165\"},\"end\":105032,\"start\":104864},{\"attributes\":{\"id\":\"formula_166\"},\"end\":105218,\"start\":105121},{\"attributes\":{\"id\":\"formula_167\"},\"end\":105501,\"start\":105343},{\"attributes\":{\"id\":\"formula_168\"},\"end\":105758,\"start\":105697},{\"attributes\":{\"id\":\"formula_169\"},\"end\":105923,\"start\":105840},{\"attributes\":{\"id\":\"formula_170\"},\"end\":106708,\"start\":106448},{\"attributes\":{\"id\":\"formula_171\"},\"end\":106957,\"start\":106786},{\"attributes\":{\"id\":\"formula_172\"},\"end\":107263,\"start\":107101},{\"attributes\":{\"id\":\"formula_173\"},\"end\":107393,\"start\":107301},{\"attributes\":{\"id\":\"formula_174\"},\"end\":107573,\"start\":107437},{\"attributes\":{\"id\":\"formula_175\"},\"end\":107757,\"start\":107706},{\"attributes\":{\"id\":\"formula_176\"},\"end\":107811,\"start\":107757},{\"attributes\":{\"id\":\"formula_177\"},\"end\":107943,\"start\":107885},{\"attributes\":{\"id\":\"formula_178\"},\"end\":108653,\"start\":108478},{\"attributes\":{\"id\":\"formula_179\"},\"end\":108885,\"start\":108818},{\"attributes\":{\"id\":\"formula_180\"},\"end\":109538,\"start\":109143},{\"attributes\":{\"id\":\"formula_181\"},\"end\":110028,\"start\":109950},{\"attributes\":{\"id\":\"formula_182\"},\"end\":111082,\"start\":110603},{\"attributes\":{\"id\":\"formula_183\"},\"end\":113258,\"start\":113214},{\"attributes\":{\"id\":\"formula_184\"},\"end\":113396,\"start\":113331},{\"attributes\":{\"id\":\"formula_185\"},\"end\":113463,\"start\":113445},{\"attributes\":{\"id\":\"formula_186\"},\"end\":114642,\"start\":114609},{\"attributes\":{\"id\":\"formula_187\"},\"end\":116166,\"start\":116146},{\"attributes\":{\"id\":\"formula_188\"},\"end\":116627,\"start\":116608},{\"attributes\":{\"id\":\"formula_189\"},\"end\":117941,\"start\":117841},{\"attributes\":{\"id\":\"formula_190\"},\"end\":118489,\"start\":118469}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2407,\"start\":2395},{\"attributes\":{\"n\":\"1.1\"},\"end\":8015,\"start\":8008},{\"attributes\":{\"n\":\"1.2\"},\"end\":10512,\"start\":10480},{\"end\":12255,\"start\":12230},{\"attributes\":{\"n\":\"2\"},\"end\":14742,\"start\":14724},{\"end\":19100,\"start\":19081},{\"attributes\":{\"n\":\"3\"},\"end\":35763,\"start\":35753},{\"attributes\":{\"n\":\"4\"},\"end\":44341,\"start\":44328},{\"attributes\":{\"n\":\"4.1\"},\"end\":44869,\"start\":44855},{\"attributes\":{\"n\":\"4.2\"},\"end\":45683,\"start\":45648},{\"attributes\":{\"n\":\"4.3\"},\"end\":49060,\"start\":49035},{\"attributes\":{\"n\":\"5\"},\"end\":55645,\"start\":55614},{\"attributes\":{\"n\":\"5.1\"},\"end\":55998,\"start\":55955},{\"end\":56375,\"start\":56283},{\"attributes\":{\"n\":\"5.2\"},\"end\":57422,\"start\":57364},{\"attributes\":{\"n\":\"6\"},\"end\":63100,\"start\":63070},{\"attributes\":{\"n\":\"7\"},\"end\":73535,\"start\":73499},{\"attributes\":{\"n\":\"7.1\"},\"end\":73570,\"start\":73538},{\"attributes\":{\"n\":\"7.2\"},\"end\":75436,\"start\":75401},{\"attributes\":{\"n\":\"8\"},\"end\":79018,\"start\":78976},{\"attributes\":{\"n\":\"8.1\"},\"end\":80129,\"start\":80102},{\"attributes\":{\"n\":\"8.2\"},\"end\":84509,\"start\":84461},{\"attributes\":{\"n\":\"9\"},\"end\":87111,\"start\":87083},{\"attributes\":{\"n\":\"9.1\"},\"end\":88853,\"start\":88821},{\"end\":91402,\"start\":91354},{\"end\":91745,\"start\":91726},{\"attributes\":{\"n\":\"9.2\"},\"end\":102526,\"start\":102494},{\"end\":104005,\"start\":103956},{\"end\":104594,\"start\":104575},{\"attributes\":{\"n\":\"9.3\"},\"end\":108112,\"start\":108062},{\"attributes\":{\"n\":\"10\"},\"end\":111177,\"start\":111150},{\"attributes\":{\"n\":\"10.1\"},\"end\":111485,\"start\":111463},{\"end\":113213,\"start\":113178},{\"attributes\":{\"n\":\"10.2\"},\"end\":114765,\"start\":114748},{\"attributes\":{\"n\":\"10.3\"},\"end\":117320,\"start\":117298},{\"end\":119829,\"start\":119817},{\"end\":120031,\"start\":120019},{\"end\":120097,\"start\":120080},{\"end\":120293,\"start\":120276},{\"end\":120373,\"start\":120356},{\"end\":120604,\"start\":120592},{\"end\":120900,\"start\":120889},{\"end\":121018,\"start\":121008},{\"end\":121221,\"start\":121208},{\"end\":122907,\"start\":122894},{\"end\":123271,\"start\":123258}]", "table": "[{\"end\":123618,\"start\":123368},{\"end\":124315,\"start\":123871},{\"end\":125218,\"start\":124695},{\"end\":125866,\"start\":125346},{\"end\":126133,\"start\":126022},{\"end\":126417,\"start\":126276},{\"end\":126666,\"start\":126423},{\"end\":127437,\"start\":127052},{\"end\":128007,\"start\":127988},{\"end\":128400,\"start\":128270},{\"end\":128632,\"start\":128611}]", "figure_caption": "[{\"end\":119271,\"start\":119020},{\"end\":119682,\"start\":119274},{\"end\":119815,\"start\":119685},{\"end\":120017,\"start\":119831},{\"end\":120078,\"start\":120033},{\"end\":120274,\"start\":120099},{\"end\":120354,\"start\":120295},{\"end\":120590,\"start\":120375},{\"end\":120887,\"start\":120606},{\"end\":121006,\"start\":120902},{\"end\":121108,\"start\":121020},{\"end\":121206,\"start\":121111},{\"end\":121603,\"start\":121223},{\"end\":121867,\"start\":121606},{\"end\":122259,\"start\":121870},{\"end\":122370,\"start\":122262},{\"end\":122744,\"start\":122373},{\"end\":122892,\"start\":122747},{\"end\":123256,\"start\":122909},{\"end\":123346,\"start\":123273},{\"end\":123368,\"start\":123349},{\"end\":123747,\"start\":123621},{\"end\":123871,\"start\":123750},{\"end\":124446,\"start\":124318},{\"end\":124552,\"start\":124449},{\"end\":124695,\"start\":124555},{\"end\":125346,\"start\":125221},{\"end\":126022,\"start\":125869},{\"end\":126220,\"start\":126136},{\"end\":126276,\"start\":126223},{\"end\":126423,\"start\":126420},{\"end\":126809,\"start\":126669},{\"end\":127052,\"start\":126812},{\"end\":127988,\"start\":127440},{\"end\":128270,\"start\":128010},{\"end\":128611,\"start\":128403}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":14939,\"start\":14931},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":78494,\"start\":78487}]", "bib_author_first_name": "[{\"end\":130722,\"start\":130721},{\"end\":130724,\"start\":130723},{\"end\":130737,\"start\":130736},{\"end\":130739,\"start\":130738},{\"end\":130751,\"start\":130750},{\"end\":130753,\"start\":130752},{\"end\":130762,\"start\":130761},{\"end\":130764,\"start\":130763},{\"end\":130774,\"start\":130773},{\"end\":130776,\"start\":130775},{\"end\":131041,\"start\":131036},{\"end\":131058,\"start\":131052},{\"end\":131460,\"start\":131455},{\"end\":131476,\"start\":131471},{\"end\":131821,\"start\":131814},{\"end\":132109,\"start\":132102},{\"end\":132121,\"start\":132117},{\"end\":132135,\"start\":132129},{\"end\":132561,\"start\":132555},{\"end\":132573,\"start\":132569},{\"end\":132583,\"start\":132576},{\"end\":132881,\"start\":132873},{\"end\":132899,\"start\":132894},{\"end\":133243,\"start\":133237},{\"end\":133259,\"start\":133252},{\"end\":133261,\"start\":133260},{\"end\":133277,\"start\":133272},{\"end\":133279,\"start\":133278},{\"end\":133836,\"start\":133828},{\"end\":133854,\"start\":133848},{\"end\":133874,\"start\":133868},{\"end\":133876,\"start\":133875},{\"end\":133891,\"start\":133887},{\"end\":134250,\"start\":134249},{\"end\":134254,\"start\":134251},{\"end\":134269,\"start\":134265},{\"end\":134287,\"start\":134279},{\"end\":134308,\"start\":134295},{\"end\":134319,\"start\":134313},{\"end\":134321,\"start\":134320},{\"end\":134333,\"start\":134327},{\"end\":134730,\"start\":134722},{\"end\":134748,\"start\":134742},{\"end\":134770,\"start\":134759},{\"end\":135412,\"start\":135408},{\"end\":135428,\"start\":135422},{\"end\":135430,\"start\":135429},{\"end\":135447,\"start\":135440},{\"end\":135461,\"start\":135456},{\"end\":135476,\"start\":135470},{\"end\":135491,\"start\":135484},{\"end\":135810,\"start\":135804},{\"end\":135828,\"start\":135821},{\"end\":135843,\"start\":135839},{\"end\":135859,\"start\":135852},{\"end\":135870,\"start\":135865},{\"end\":136581,\"start\":136578},{\"end\":136606,\"start\":136599},{\"end\":136625,\"start\":136614},{\"end\":136641,\"start\":136633},{\"end\":137193,\"start\":137184},{\"end\":137213,\"start\":137207},{\"end\":137228,\"start\":137222},{\"end\":137912,\"start\":137905},{\"end\":137925,\"start\":137919},{\"end\":137943,\"start\":137934},{\"end\":137953,\"start\":137949},{\"end\":137965,\"start\":137961},{\"end\":137979,\"start\":137972},{\"end\":139088,\"start\":139081},{\"end\":139101,\"start\":139095},{\"end\":139122,\"start\":139114},{\"end\":139136,\"start\":139127},{\"end\":139146,\"start\":139142},{\"end\":139160,\"start\":139153},{\"end\":139639,\"start\":139633},{\"end\":139665,\"start\":139659},{\"end\":139667,\"start\":139666},{\"end\":139979,\"start\":139974},{\"end\":139995,\"start\":139991},{\"end\":140016,\"start\":140006},{\"end\":140023,\"start\":140017},{\"end\":140045,\"start\":140033},{\"end\":140060,\"start\":140054},{\"end\":140078,\"start\":140072},{\"end\":140096,\"start\":140089},{\"end\":140720,\"start\":140714},{\"end\":140722,\"start\":140721},{\"end\":140736,\"start\":140731},{\"end\":140753,\"start\":140746},{\"end\":140755,\"start\":140754},{\"end\":141114,\"start\":141113},{\"end\":141116,\"start\":141115},{\"end\":141444,\"start\":141438},{\"end\":141469,\"start\":141462},{\"end\":141992,\"start\":141986},{\"end\":142005,\"start\":141999},{\"end\":142014,\"start\":142010},{\"end\":142282,\"start\":142281},{\"end\":142293,\"start\":142292},{\"end\":142303,\"start\":142302},{\"end\":142690,\"start\":142689},{\"end\":142701,\"start\":142700},{\"end\":142711,\"start\":142710},{\"end\":143089,\"start\":143083},{\"end\":143103,\"start\":143098},{\"end\":143482,\"start\":143476},{\"end\":143502,\"start\":143492},{\"end\":143918,\"start\":143917},{\"end\":143925,\"start\":143924},{\"end\":143927,\"start\":143926},{\"end\":144258,\"start\":144253},{\"end\":144281,\"start\":144270},{\"end\":145006,\"start\":144998},{\"end\":145024,\"start\":145020},{\"end\":145039,\"start\":145032},{\"end\":145441,\"start\":145437},{\"end\":145454,\"start\":145449},{\"end\":145899,\"start\":145893},{\"end\":145912,\"start\":145908},{\"end\":145922,\"start\":145917},{\"end\":145938,\"start\":145932},{\"end\":146555,\"start\":146549},{\"end\":146568,\"start\":146564},{\"end\":146579,\"start\":146575},{\"end\":146951,\"start\":146947},{\"end\":147277,\"start\":147270},{\"end\":147279,\"start\":147278},{\"end\":147581,\"start\":147577},{\"end\":147583,\"start\":147582},{\"end\":147600,\"start\":147592},{\"end\":147615,\"start\":147611},{\"end\":147964,\"start\":147957},{\"end\":147978,\"start\":147972},{\"end\":147995,\"start\":147990},{\"end\":148009,\"start\":148004},{\"end\":148022,\"start\":148016},{\"end\":148034,\"start\":148027},{\"end\":148048,\"start\":148044},{\"end\":148063,\"start\":148056},{\"end\":148077,\"start\":148073},{\"end\":148091,\"start\":148085},{\"end\":148093,\"start\":148092},{\"end\":148584,\"start\":148578},{\"end\":148603,\"start\":148595},{\"end\":148606,\"start\":148604},{\"end\":148619,\"start\":148613},{\"end\":148621,\"start\":148620},{\"end\":149205,\"start\":149199},{\"end\":149220,\"start\":149214},{\"end\":149222,\"start\":149221},{\"end\":149487,\"start\":149480},{\"end\":149501,\"start\":149495},{\"end\":149785,\"start\":149778},{\"end\":149799,\"start\":149793},{\"end\":149814,\"start\":149809},{\"end\":150179,\"start\":150171},{\"end\":150196,\"start\":150191},{\"end\":150216,\"start\":150206},{\"end\":150231,\"start\":150225},{\"end\":150596,\"start\":150588},{\"end\":150614,\"start\":150608},{\"end\":151150,\"start\":151142},{\"end\":151168,\"start\":151162},{\"end\":151547,\"start\":151539},{\"end\":151565,\"start\":151559},{\"end\":151802,\"start\":151793},{\"end\":151818,\"start\":151811},{\"end\":152118,\"start\":152112},{\"end\":152133,\"start\":152127},{\"end\":152147,\"start\":152141},{\"end\":152156,\"start\":152155},{\"end\":152158,\"start\":152157},{\"end\":152165,\"start\":152164},{\"end\":152177,\"start\":152176},{\"end\":152188,\"start\":152187},{\"end\":152197,\"start\":152196},{\"end\":152713,\"start\":152704},{\"end\":153039,\"start\":153036},{\"end\":153052,\"start\":153047},{\"end\":153054,\"start\":153053},{\"end\":153460,\"start\":153456},{\"end\":153475,\"start\":153468},{\"end\":153492,\"start\":153487},{\"end\":153895,\"start\":153890},{\"end\":154173,\"start\":154166},{\"end\":154175,\"start\":154174},{\"end\":154448,\"start\":154447},{\"end\":154463,\"start\":154462},{\"end\":154701,\"start\":154694},{\"end\":154720,\"start\":154709},{\"end\":155072,\"start\":155065},{\"end\":155091,\"start\":155080},{\"end\":155104,\"start\":155099},{\"end\":155702,\"start\":155698},{\"end\":155704,\"start\":155703},{\"end\":155717,\"start\":155713},{\"end\":155719,\"start\":155718},{\"end\":155733,\"start\":155727},{\"end\":155735,\"start\":155734},{\"end\":155746,\"start\":155741},{\"end\":155748,\"start\":155747},{\"end\":156138,\"start\":156133},{\"end\":156156,\"start\":156146},{\"end\":156666,\"start\":156660},{\"end\":156678,\"start\":156675},{\"end\":156680,\"start\":156679},{\"end\":157126,\"start\":157125},{\"end\":157412,\"start\":157411},{\"end\":157800,\"start\":157799},{\"end\":157802,\"start\":157801},{\"end\":158140,\"start\":158134},{\"end\":158484,\"start\":158477},{\"end\":158503,\"start\":158497},{\"end\":158517,\"start\":158513},{\"end\":158940,\"start\":158936},{\"end\":158956,\"start\":158951},{\"end\":159360,\"start\":159354},{\"end\":159643,\"start\":159642},{\"end\":159645,\"start\":159644},{\"end\":159988,\"start\":159981},{\"end\":160378,\"start\":160369},{\"end\":160391,\"start\":160385},{\"end\":160663,\"start\":160656},{\"end\":160682,\"start\":160674},{\"end\":160684,\"start\":160683},{\"end\":161069,\"start\":161065},{\"end\":161707,\"start\":161703},{\"end\":162159,\"start\":162152},{\"end\":162398,\"start\":162397},{\"end\":162411,\"start\":162406},{\"end\":162426,\"start\":162423},{\"end\":162611,\"start\":162606},{\"end\":162613,\"start\":162612},{\"end\":162793,\"start\":162792},{\"end\":162795,\"start\":162794},{\"end\":163114,\"start\":163110},{\"end\":163116,\"start\":163115},{\"end\":163489,\"start\":163482},{\"end\":163773,\"start\":163766},{\"end\":163775,\"start\":163774},{\"end\":164101,\"start\":164096},{\"end\":164103,\"start\":164102},{\"end\":164471,\"start\":164464},{\"end\":164488,\"start\":164481},{\"end\":164501,\"start\":164495},{\"end\":164867,\"start\":164860},{\"end\":164878,\"start\":164874},{\"end\":164880,\"start\":164879},{\"end\":164899,\"start\":164893},{\"end\":164901,\"start\":164900}]", "bib_author_last_name": "[{\"end\":130734,\"start\":130725},{\"end\":130748,\"start\":130740},{\"end\":130759,\"start\":130754},{\"end\":130771,\"start\":130765},{\"end\":130785,\"start\":130777},{\"end\":130791,\"start\":130787},{\"end\":131050,\"start\":131042},{\"end\":131067,\"start\":131059},{\"end\":131469,\"start\":131461},{\"end\":131481,\"start\":131477},{\"end\":131830,\"start\":131822},{\"end\":132115,\"start\":132110},{\"end\":132127,\"start\":132122},{\"end\":132140,\"start\":132136},{\"end\":132567,\"start\":132562},{\"end\":132586,\"start\":132584},{\"end\":132892,\"start\":132882},{\"end\":132908,\"start\":132900},{\"end\":133250,\"start\":133244},{\"end\":133270,\"start\":133262},{\"end\":133288,\"start\":133280},{\"end\":133846,\"start\":133837},{\"end\":133866,\"start\":133855},{\"end\":133885,\"start\":133877},{\"end\":133897,\"start\":133892},{\"end\":134263,\"start\":134255},{\"end\":134277,\"start\":134270},{\"end\":134293,\"start\":134288},{\"end\":134311,\"start\":134309},{\"end\":134325,\"start\":134322},{\"end\":134339,\"start\":134334},{\"end\":134343,\"start\":134341},{\"end\":134740,\"start\":134731},{\"end\":134757,\"start\":134749},{\"end\":134776,\"start\":134771},{\"end\":135420,\"start\":135413},{\"end\":135438,\"start\":135431},{\"end\":135454,\"start\":135448},{\"end\":135468,\"start\":135462},{\"end\":135482,\"start\":135477},{\"end\":135497,\"start\":135492},{\"end\":135819,\"start\":135811},{\"end\":135837,\"start\":135829},{\"end\":135850,\"start\":135844},{\"end\":135863,\"start\":135860},{\"end\":135879,\"start\":135871},{\"end\":136597,\"start\":136582},{\"end\":136612,\"start\":136607},{\"end\":136631,\"start\":136626},{\"end\":136647,\"start\":136642},{\"end\":136654,\"start\":136649},{\"end\":137205,\"start\":137194},{\"end\":137220,\"start\":137214},{\"end\":137236,\"start\":137229},{\"end\":137917,\"start\":137913},{\"end\":137932,\"start\":137926},{\"end\":137947,\"start\":137944},{\"end\":137959,\"start\":137954},{\"end\":137970,\"start\":137966},{\"end\":137984,\"start\":137980},{\"end\":139093,\"start\":139089},{\"end\":139112,\"start\":139102},{\"end\":139125,\"start\":139123},{\"end\":139140,\"start\":139137},{\"end\":139151,\"start\":139147},{\"end\":139165,\"start\":139161},{\"end\":139646,\"start\":139640},{\"end\":139657,\"start\":139648},{\"end\":139673,\"start\":139668},{\"end\":139682,\"start\":139675},{\"end\":139989,\"start\":139980},{\"end\":140004,\"start\":139996},{\"end\":140031,\"start\":140024},{\"end\":140052,\"start\":140046},{\"end\":140070,\"start\":140061},{\"end\":140087,\"start\":140079},{\"end\":140104,\"start\":140097},{\"end\":140729,\"start\":140723},{\"end\":140744,\"start\":140737},{\"end\":140761,\"start\":140756},{\"end\":141125,\"start\":141117},{\"end\":141460,\"start\":141445},{\"end\":141476,\"start\":141470},{\"end\":141486,\"start\":141478},{\"end\":141997,\"start\":141993},{\"end\":142008,\"start\":142006},{\"end\":142290,\"start\":142283},{\"end\":142300,\"start\":142294},{\"end\":142311,\"start\":142304},{\"end\":142698,\"start\":142691},{\"end\":142708,\"start\":142702},{\"end\":142719,\"start\":142712},{\"end\":143096,\"start\":143090},{\"end\":143110,\"start\":143104},{\"end\":143490,\"start\":143483},{\"end\":143510,\"start\":143503},{\"end\":143922,\"start\":143919},{\"end\":143934,\"start\":143928},{\"end\":144268,\"start\":144259},{\"end\":144286,\"start\":144282},{\"end\":145018,\"start\":145007},{\"end\":145030,\"start\":145025},{\"end\":145047,\"start\":145040},{\"end\":145447,\"start\":145442},{\"end\":145462,\"start\":145455},{\"end\":145906,\"start\":145900},{\"end\":145915,\"start\":145913},{\"end\":145930,\"start\":145923},{\"end\":145944,\"start\":145939},{\"end\":146562,\"start\":146556},{\"end\":146573,\"start\":146569},{\"end\":146584,\"start\":146580},{\"end\":146959,\"start\":146952},{\"end\":146966,\"start\":146961},{\"end\":147288,\"start\":147280},{\"end\":147590,\"start\":147584},{\"end\":147609,\"start\":147601},{\"end\":147621,\"start\":147616},{\"end\":147955,\"start\":147946},{\"end\":147970,\"start\":147965},{\"end\":147988,\"start\":147979},{\"end\":148002,\"start\":147996},{\"end\":148014,\"start\":148010},{\"end\":148025,\"start\":148023},{\"end\":148042,\"start\":148035},{\"end\":148054,\"start\":148049},{\"end\":148071,\"start\":148064},{\"end\":148083,\"start\":148078},{\"end\":148102,\"start\":148094},{\"end\":148111,\"start\":148104},{\"end\":148593,\"start\":148585},{\"end\":148611,\"start\":148607},{\"end\":148627,\"start\":148622},{\"end\":149212,\"start\":149206},{\"end\":149229,\"start\":149223},{\"end\":149493,\"start\":149488},{\"end\":149509,\"start\":149502},{\"end\":149791,\"start\":149786},{\"end\":149807,\"start\":149800},{\"end\":149828,\"start\":149815},{\"end\":150189,\"start\":150180},{\"end\":150204,\"start\":150197},{\"end\":150223,\"start\":150217},{\"end\":150239,\"start\":150232},{\"end\":150606,\"start\":150597},{\"end\":150622,\"start\":150615},{\"end\":151160,\"start\":151151},{\"end\":151176,\"start\":151169},{\"end\":151557,\"start\":151548},{\"end\":151573,\"start\":151566},{\"end\":151809,\"start\":151803},{\"end\":151826,\"start\":151819},{\"end\":152125,\"start\":152119},{\"end\":152139,\"start\":152134},{\"end\":152153,\"start\":152148},{\"end\":152162,\"start\":152159},{\"end\":152174,\"start\":152166},{\"end\":152185,\"start\":152178},{\"end\":152194,\"start\":152189},{\"end\":152205,\"start\":152198},{\"end\":152721,\"start\":152714},{\"end\":153045,\"start\":153040},{\"end\":153058,\"start\":153055},{\"end\":153066,\"start\":153060},{\"end\":153466,\"start\":153461},{\"end\":153485,\"start\":153476},{\"end\":153500,\"start\":153493},{\"end\":153909,\"start\":153896},{\"end\":154183,\"start\":154176},{\"end\":154453,\"start\":154449},{\"end\":154460,\"start\":154455},{\"end\":154469,\"start\":154464},{\"end\":154480,\"start\":154471},{\"end\":154707,\"start\":154702},{\"end\":154726,\"start\":154721},{\"end\":155078,\"start\":155073},{\"end\":155097,\"start\":155092},{\"end\":155112,\"start\":155105},{\"end\":155711,\"start\":155705},{\"end\":155725,\"start\":155720},{\"end\":155739,\"start\":155736},{\"end\":155755,\"start\":155749},{\"end\":156144,\"start\":156139},{\"end\":156164,\"start\":156157},{\"end\":156673,\"start\":156667},{\"end\":156687,\"start\":156681},{\"end\":157133,\"start\":157127},{\"end\":157419,\"start\":157413},{\"end\":157808,\"start\":157803},{\"end\":158148,\"start\":158141},{\"end\":158495,\"start\":158485},{\"end\":158511,\"start\":158504},{\"end\":158523,\"start\":158518},{\"end\":158949,\"start\":158941},{\"end\":158966,\"start\":158957},{\"end\":159365,\"start\":159361},{\"end\":159655,\"start\":159646},{\"end\":159996,\"start\":159989},{\"end\":160383,\"start\":160379},{\"end\":160401,\"start\":160392},{\"end\":160672,\"start\":160664},{\"end\":160692,\"start\":160685},{\"end\":161074,\"start\":161070},{\"end\":161712,\"start\":161708},{\"end\":162163,\"start\":162160},{\"end\":162404,\"start\":162399},{\"end\":162421,\"start\":162412},{\"end\":162430,\"start\":162427},{\"end\":162623,\"start\":162614},{\"end\":162808,\"start\":162796},{\"end\":163122,\"start\":163117},{\"end\":163497,\"start\":163490},{\"end\":163503,\"start\":163499},{\"end\":163780,\"start\":163776},{\"end\":164112,\"start\":164104},{\"end\":164479,\"start\":164472},{\"end\":164493,\"start\":164489},{\"end\":164509,\"start\":164502},{\"end\":164872,\"start\":164868},{\"end\":164891,\"start\":164881},{\"end\":164912,\"start\":164902}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":130952,\"start\":130719},{\"attributes\":{\"doi\":\"10.1137/15m1050902\",\"id\":\"b1\",\"matched_paper_id\":16599608},\"end\":131386,\"start\":130954},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12591414},\"end\":131812,\"start\":131388},{\"attributes\":{\"id\":\"b3\"},\"end\":132041,\"start\":131814},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18302045},\"end\":132486,\"start\":132043},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14426203},\"end\":132819,\"start\":132488},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2683832},\"end\":133175,\"start\":132821},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":246945889},\"end\":133762,\"start\":133177},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":226237010},\"end\":134159,\"start\":133764},{\"attributes\":{\"id\":\"b9\"},\"end\":134676,\"start\":134161},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":248218419},\"end\":135336,\"start\":134678},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":226282118},\"end\":135732,\"start\":135338},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":226281750},\"end\":136500,\"start\":135734},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7956641},\"end\":137074,\"start\":136502},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4614529},\"end\":137784,\"start\":137076},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":227276230},\"end\":138974,\"start\":137786},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204509632},\"end\":139564,\"start\":138976},{\"attributes\":{\"id\":\"b17\"},\"end\":139921,\"start\":139566},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3306944},\"end\":140609,\"start\":139923},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3834959},\"end\":141066,\"start\":140611},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":121250561},\"end\":141376,\"start\":141068},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8676679},\"end\":141903,\"start\":141378},{\"attributes\":{\"id\":\"b22\"},\"end\":142198,\"start\":141905},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":702408},\"end\":142594,\"start\":142200},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5453786},\"end\":143014,\"start\":142596},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":215885923},\"end\":143387,\"start\":143016},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1571073},\"end\":143870,\"start\":143389},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":58816022},\"end\":144116,\"start\":143872},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":244270263},\"end\":144966,\"start\":144118},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":570390},\"end\":145334,\"start\":144968},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53773950},\"end\":145784,\"start\":145336},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":46941335},\"end\":146485,\"start\":145786},{\"attributes\":{\"id\":\"b32\"},\"end\":146891,\"start\":146487},{\"attributes\":{\"id\":\"b33\"},\"end\":147194,\"start\":146893},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":198985925},\"end\":147524,\"start\":147196},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5187993},\"end\":147896,\"start\":147526},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":244799643},\"end\":148503,\"start\":147898},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":204509006},\"end\":149168,\"start\":148505},{\"attributes\":{\"id\":\"b38\"},\"end\":149421,\"start\":149170},{\"attributes\":{\"id\":\"b39\"},\"end\":149692,\"start\":149423},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10456980},\"end\":150105,\"start\":149694},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":54465030},\"end\":150554,\"start\":150107},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":579463},\"end\":151077,\"start\":150556},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":119415623},\"end\":151490,\"start\":151079},{\"attributes\":{\"id\":\"b44\"},\"end\":151740,\"start\":151492},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":26830284},\"end\":152083,\"start\":151742},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":12601463},\"end\":152592,\"start\":152085},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":478182},\"end\":152973,\"start\":152594},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1118993},\"end\":153387,\"start\":152975},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":7811252},\"end\":153788,\"start\":153389},{\"attributes\":{\"id\":\"b50\"},\"end\":154119,\"start\":153790},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":47503887},\"end\":154443,\"start\":154121},{\"attributes\":{\"id\":\"b52\"},\"end\":154594,\"start\":154445},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2382732},\"end\":154996,\"start\":154596},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13740331},\"end\":155655,\"start\":154998},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":233864539},\"end\":156053,\"start\":155657},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":16183047},\"end\":156575,\"start\":156055},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":57955},\"end\":157028,\"start\":156577},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":123589306},\"end\":157348,\"start\":157030},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":120342081},\"end\":157716,\"start\":157350},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":119527361},\"end\":158072,\"start\":157718},{\"attributes\":{\"id\":\"b61\"},\"end\":158415,\"start\":158074},{\"attributes\":{\"id\":\"b62\"},\"end\":158853,\"start\":158417},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":6054789},\"end\":159282,\"start\":158855},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":792604},\"end\":159557,\"start\":159284},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":39693427},\"end\":159939,\"start\":159559},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":17028844},\"end\":160304,\"start\":159941},{\"attributes\":{\"id\":\"b67\"},\"end\":160610,\"start\":160306},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":51814849},\"end\":160996,\"start\":160612},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":44036160},\"end\":161581,\"start\":160998},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":236956378},\"end\":162090,\"start\":161583},{\"attributes\":{\"id\":\"b71\"},\"end\":162393,\"start\":162092},{\"attributes\":{\"id\":\"b72\"},\"end\":162555,\"start\":162395},{\"attributes\":{\"id\":\"b73\"},\"end\":162788,\"start\":162557},{\"attributes\":{\"id\":\"b74\"},\"end\":163054,\"start\":162790},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":5679583},\"end\":163423,\"start\":163056},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":16943113},\"end\":163688,\"start\":163425},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":16030199},\"end\":164044,\"start\":163690},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":51783444},\"end\":164410,\"start\":164046},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":3714239},\"end\":164813,\"start\":164412},{\"attributes\":{\"id\":\"b80\"},\"end\":165219,\"start\":164815}]", "bib_title": "[{\"end\":131034,\"start\":130954},{\"end\":131453,\"start\":131388},{\"end\":132100,\"start\":132043},{\"end\":132553,\"start\":132488},{\"end\":132871,\"start\":132821},{\"end\":133235,\"start\":133177},{\"end\":133826,\"start\":133764},{\"end\":134720,\"start\":134678},{\"end\":135406,\"start\":135338},{\"end\":135802,\"start\":135734},{\"end\":136576,\"start\":136502},{\"end\":137182,\"start\":137076},{\"end\":137903,\"start\":137786},{\"end\":139079,\"start\":138976},{\"end\":139972,\"start\":139923},{\"end\":140712,\"start\":140611},{\"end\":141111,\"start\":141068},{\"end\":141436,\"start\":141378},{\"end\":142279,\"start\":142200},{\"end\":142687,\"start\":142596},{\"end\":143081,\"start\":143016},{\"end\":143474,\"start\":143389},{\"end\":143915,\"start\":143872},{\"end\":144251,\"start\":144118},{\"end\":144996,\"start\":144968},{\"end\":145435,\"start\":145336},{\"end\":145891,\"start\":145786},{\"end\":147268,\"start\":147196},{\"end\":147575,\"start\":147526},{\"end\":147944,\"start\":147898},{\"end\":148576,\"start\":148505},{\"end\":149776,\"start\":149694},{\"end\":150169,\"start\":150107},{\"end\":150586,\"start\":150556},{\"end\":151140,\"start\":151079},{\"end\":151791,\"start\":151742},{\"end\":152110,\"start\":152085},{\"end\":152702,\"start\":152594},{\"end\":153034,\"start\":152975},{\"end\":153454,\"start\":153389},{\"end\":154164,\"start\":154121},{\"end\":154692,\"start\":154596},{\"end\":155063,\"start\":154998},{\"end\":155696,\"start\":155657},{\"end\":156131,\"start\":156055},{\"end\":156658,\"start\":156577},{\"end\":157123,\"start\":157030},{\"end\":157409,\"start\":157350},{\"end\":157797,\"start\":157718},{\"end\":158475,\"start\":158417},{\"end\":158934,\"start\":158855},{\"end\":159352,\"start\":159284},{\"end\":159640,\"start\":159559},{\"end\":159979,\"start\":159941},{\"end\":160654,\"start\":160612},{\"end\":161063,\"start\":160998},{\"end\":161701,\"start\":161583},{\"end\":163108,\"start\":163056},{\"end\":163480,\"start\":163425},{\"end\":163764,\"start\":163690},{\"end\":164094,\"start\":164046},{\"end\":164462,\"start\":164412},{\"end\":164858,\"start\":164815}]", "bib_author": "[{\"end\":130736,\"start\":130721},{\"end\":130750,\"start\":130736},{\"end\":130761,\"start\":130750},{\"end\":130773,\"start\":130761},{\"end\":130787,\"start\":130773},{\"end\":130793,\"start\":130787},{\"end\":131052,\"start\":131036},{\"end\":131069,\"start\":131052},{\"end\":131471,\"start\":131455},{\"end\":131483,\"start\":131471},{\"end\":131832,\"start\":131814},{\"end\":132117,\"start\":132102},{\"end\":132129,\"start\":132117},{\"end\":132142,\"start\":132129},{\"end\":132569,\"start\":132555},{\"end\":132576,\"start\":132569},{\"end\":132588,\"start\":132576},{\"end\":132894,\"start\":132873},{\"end\":132910,\"start\":132894},{\"end\":133252,\"start\":133237},{\"end\":133272,\"start\":133252},{\"end\":133290,\"start\":133272},{\"end\":133848,\"start\":133828},{\"end\":133868,\"start\":133848},{\"end\":133887,\"start\":133868},{\"end\":133899,\"start\":133887},{\"end\":134265,\"start\":134249},{\"end\":134279,\"start\":134265},{\"end\":134295,\"start\":134279},{\"end\":134313,\"start\":134295},{\"end\":134327,\"start\":134313},{\"end\":134341,\"start\":134327},{\"end\":134345,\"start\":134341},{\"end\":134742,\"start\":134722},{\"end\":134759,\"start\":134742},{\"end\":134778,\"start\":134759},{\"end\":135422,\"start\":135408},{\"end\":135440,\"start\":135422},{\"end\":135456,\"start\":135440},{\"end\":135470,\"start\":135456},{\"end\":135484,\"start\":135470},{\"end\":135499,\"start\":135484},{\"end\":135821,\"start\":135804},{\"end\":135839,\"start\":135821},{\"end\":135852,\"start\":135839},{\"end\":135865,\"start\":135852},{\"end\":135881,\"start\":135865},{\"end\":136599,\"start\":136578},{\"end\":136614,\"start\":136599},{\"end\":136633,\"start\":136614},{\"end\":136649,\"start\":136633},{\"end\":136656,\"start\":136649},{\"end\":137207,\"start\":137184},{\"end\":137222,\"start\":137207},{\"end\":137238,\"start\":137222},{\"end\":137919,\"start\":137905},{\"end\":137934,\"start\":137919},{\"end\":137949,\"start\":137934},{\"end\":137961,\"start\":137949},{\"end\":137972,\"start\":137961},{\"end\":137986,\"start\":137972},{\"end\":139095,\"start\":139081},{\"end\":139114,\"start\":139095},{\"end\":139127,\"start\":139114},{\"end\":139142,\"start\":139127},{\"end\":139153,\"start\":139142},{\"end\":139167,\"start\":139153},{\"end\":139648,\"start\":139633},{\"end\":139659,\"start\":139648},{\"end\":139675,\"start\":139659},{\"end\":139684,\"start\":139675},{\"end\":139991,\"start\":139974},{\"end\":140006,\"start\":139991},{\"end\":140033,\"start\":140006},{\"end\":140054,\"start\":140033},{\"end\":140072,\"start\":140054},{\"end\":140089,\"start\":140072},{\"end\":140106,\"start\":140089},{\"end\":140731,\"start\":140714},{\"end\":140746,\"start\":140731},{\"end\":140763,\"start\":140746},{\"end\":141127,\"start\":141113},{\"end\":141462,\"start\":141438},{\"end\":141478,\"start\":141462},{\"end\":141488,\"start\":141478},{\"end\":141999,\"start\":141986},{\"end\":142010,\"start\":141999},{\"end\":142017,\"start\":142010},{\"end\":142292,\"start\":142281},{\"end\":142302,\"start\":142292},{\"end\":142313,\"start\":142302},{\"end\":142700,\"start\":142689},{\"end\":142710,\"start\":142700},{\"end\":142721,\"start\":142710},{\"end\":143098,\"start\":143083},{\"end\":143112,\"start\":143098},{\"end\":143492,\"start\":143476},{\"end\":143512,\"start\":143492},{\"end\":143924,\"start\":143917},{\"end\":143936,\"start\":143924},{\"end\":144270,\"start\":144253},{\"end\":144288,\"start\":144270},{\"end\":145020,\"start\":144998},{\"end\":145032,\"start\":145020},{\"end\":145049,\"start\":145032},{\"end\":145449,\"start\":145437},{\"end\":145464,\"start\":145449},{\"end\":145908,\"start\":145893},{\"end\":145917,\"start\":145908},{\"end\":145932,\"start\":145917},{\"end\":145946,\"start\":145932},{\"end\":146564,\"start\":146549},{\"end\":146575,\"start\":146564},{\"end\":146586,\"start\":146575},{\"end\":146961,\"start\":146947},{\"end\":146968,\"start\":146961},{\"end\":147290,\"start\":147270},{\"end\":147592,\"start\":147577},{\"end\":147611,\"start\":147592},{\"end\":147623,\"start\":147611},{\"end\":147957,\"start\":147946},{\"end\":147972,\"start\":147957},{\"end\":147990,\"start\":147972},{\"end\":148004,\"start\":147990},{\"end\":148016,\"start\":148004},{\"end\":148027,\"start\":148016},{\"end\":148044,\"start\":148027},{\"end\":148056,\"start\":148044},{\"end\":148073,\"start\":148056},{\"end\":148085,\"start\":148073},{\"end\":148104,\"start\":148085},{\"end\":148113,\"start\":148104},{\"end\":148595,\"start\":148578},{\"end\":148613,\"start\":148595},{\"end\":148629,\"start\":148613},{\"end\":149214,\"start\":149199},{\"end\":149231,\"start\":149214},{\"end\":149495,\"start\":149480},{\"end\":149511,\"start\":149495},{\"end\":149793,\"start\":149778},{\"end\":149809,\"start\":149793},{\"end\":149830,\"start\":149809},{\"end\":150191,\"start\":150171},{\"end\":150206,\"start\":150191},{\"end\":150225,\"start\":150206},{\"end\":150241,\"start\":150225},{\"end\":150608,\"start\":150588},{\"end\":150624,\"start\":150608},{\"end\":151162,\"start\":151142},{\"end\":151178,\"start\":151162},{\"end\":151559,\"start\":151539},{\"end\":151575,\"start\":151559},{\"end\":151811,\"start\":151793},{\"end\":151828,\"start\":151811},{\"end\":152127,\"start\":152112},{\"end\":152141,\"start\":152127},{\"end\":152155,\"start\":152141},{\"end\":152164,\"start\":152155},{\"end\":152176,\"start\":152164},{\"end\":152187,\"start\":152176},{\"end\":152196,\"start\":152187},{\"end\":152207,\"start\":152196},{\"end\":152723,\"start\":152704},{\"end\":153047,\"start\":153036},{\"end\":153060,\"start\":153047},{\"end\":153068,\"start\":153060},{\"end\":153468,\"start\":153456},{\"end\":153487,\"start\":153468},{\"end\":153502,\"start\":153487},{\"end\":153911,\"start\":153890},{\"end\":154185,\"start\":154166},{\"end\":154455,\"start\":154447},{\"end\":154462,\"start\":154455},{\"end\":154471,\"start\":154462},{\"end\":154482,\"start\":154471},{\"end\":154709,\"start\":154694},{\"end\":154728,\"start\":154709},{\"end\":155080,\"start\":155065},{\"end\":155099,\"start\":155080},{\"end\":155114,\"start\":155099},{\"end\":155713,\"start\":155698},{\"end\":155727,\"start\":155713},{\"end\":155741,\"start\":155727},{\"end\":155757,\"start\":155741},{\"end\":156146,\"start\":156133},{\"end\":156166,\"start\":156146},{\"end\":156675,\"start\":156660},{\"end\":156689,\"start\":156675},{\"end\":157135,\"start\":157125},{\"end\":157421,\"start\":157411},{\"end\":157810,\"start\":157799},{\"end\":158150,\"start\":158134},{\"end\":158497,\"start\":158477},{\"end\":158513,\"start\":158497},{\"end\":158525,\"start\":158513},{\"end\":158951,\"start\":158936},{\"end\":158968,\"start\":158951},{\"end\":159367,\"start\":159354},{\"end\":159657,\"start\":159642},{\"end\":159998,\"start\":159981},{\"end\":160385,\"start\":160369},{\"end\":160403,\"start\":160385},{\"end\":160674,\"start\":160656},{\"end\":160694,\"start\":160674},{\"end\":161076,\"start\":161065},{\"end\":161714,\"start\":161703},{\"end\":162165,\"start\":162152},{\"end\":162406,\"start\":162397},{\"end\":162423,\"start\":162406},{\"end\":162432,\"start\":162423},{\"end\":162625,\"start\":162606},{\"end\":162810,\"start\":162792},{\"end\":163124,\"start\":163110},{\"end\":163499,\"start\":163482},{\"end\":163505,\"start\":163499},{\"end\":163782,\"start\":163766},{\"end\":164114,\"start\":164096},{\"end\":164481,\"start\":164464},{\"end\":164495,\"start\":164481},{\"end\":164511,\"start\":164495},{\"end\":164874,\"start\":164860},{\"end\":164893,\"start\":164874},{\"end\":164914,\"start\":164893}]", "bib_venue": "[{\"end\":133466,\"start\":133406},{\"end\":134971,\"start\":134889},{\"end\":136028,\"start\":135974},{\"end\":136795,\"start\":136734},{\"end\":144531,\"start\":144449},{\"end\":146116,\"start\":146059},{\"end\":150822,\"start\":150753},{\"end\":155321,\"start\":155256},{\"end\":156325,\"start\":156254},{\"end\":161258,\"start\":161187},{\"end\":131127,\"start\":131102},{\"end\":131540,\"start\":131483},{\"end\":131906,\"start\":131858},{\"end\":132229,\"start\":132142},{\"end\":132637,\"start\":132588},{\"end\":132980,\"start\":132962},{\"end\":133404,\"start\":133329},{\"end\":133938,\"start\":133909},{\"end\":134247,\"start\":134161},{\"end\":134887,\"start\":134801},{\"end\":135510,\"start\":135499},{\"end\":135972,\"start\":135903},{\"end\":136732,\"start\":136656},{\"end\":137391,\"start\":137282},{\"end\":138211,\"start\":138086},{\"end\":139216,\"start\":139198},{\"end\":139631,\"start\":139566},{\"end\":140227,\"start\":140144},{\"end\":140806,\"start\":140781},{\"end\":141184,\"start\":141160},{\"end\":141621,\"start\":141488},{\"end\":141984,\"start\":141905},{\"end\":142363,\"start\":142338},{\"end\":142771,\"start\":142746},{\"end\":143150,\"start\":143137},{\"end\":143591,\"start\":143561},{\"end\":143939,\"start\":143936},{\"end\":144447,\"start\":144361},{\"end\":145117,\"start\":145094},{\"end\":145529,\"start\":145480},{\"end\":146057,\"start\":145985},{\"end\":146547,\"start\":146487},{\"end\":146945,\"start\":146893},{\"end\":147324,\"start\":147315},{\"end\":147676,\"start\":147653},{\"end\":148143,\"start\":148136},{\"end\":148802,\"start\":148656},{\"end\":149197,\"start\":149170},{\"end\":149478,\"start\":149423},{\"end\":149870,\"start\":149830},{\"end\":150306,\"start\":150257},{\"end\":150751,\"start\":150667},{\"end\":151238,\"start\":151221},{\"end\":151537,\"start\":151492},{\"end\":151900,\"start\":151887},{\"end\":152272,\"start\":152223},{\"end\":152737,\"start\":152723},{\"end\":153137,\"start\":153114},{\"end\":153557,\"start\":153536},{\"end\":153888,\"start\":153790},{\"end\":154246,\"start\":154203},{\"end\":154777,\"start\":154728},{\"end\":155254,\"start\":155174},{\"end\":155811,\"start\":155800},{\"end\":156252,\"start\":156166},{\"end\":156776,\"start\":156709},{\"end\":157154,\"start\":157135},{\"end\":157497,\"start\":157449},{\"end\":157867,\"start\":157833},{\"end\":158132,\"start\":158074},{\"end\":158593,\"start\":158570},{\"end\":159009,\"start\":158991},{\"end\":159393,\"start\":159367},{\"end\":159709,\"start\":159688},{\"end\":160100,\"start\":159998},{\"end\":160367,\"start\":160306},{\"end\":160766,\"start\":160712},{\"end\":161185,\"start\":161099},{\"end\":161783,\"start\":161760},{\"end\":162150,\"start\":162092},{\"end\":162456,\"start\":162432},{\"end\":162604,\"start\":162557},{\"end\":162879,\"start\":162833},{\"end\":163201,\"start\":163158},{\"end\":163529,\"start\":163505},{\"end\":163839,\"start\":163798},{\"end\":164187,\"start\":164132},{\"end\":164580,\"start\":164557},{\"end\":164973,\"start\":164956}]"}}}, "year": 2023, "month": 12, "day": 17}