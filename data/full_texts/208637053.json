{"id": 208637053, "updated": "2023-10-06 20:53:42.978", "metadata": {"title": "Label-Consistent Backdoor Attacks", "authors": "[{\"first\":\"Alexander\",\"last\":\"Turner\",\"middle\":[]},{\"first\":\"Dimitris\",\"last\":\"Tsipras\",\"middle\":[]},{\"first\":\"Aleksander\",\"last\":\"Madry\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 12, "day": 5}, "abstract": "Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1912.02771", "mag": "2993846550", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1912-02771", "doi": null}}, "content": {"source": {"pdf_hash": "02d1168344c969761397b575b64765405752c354", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1912.02771v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e667e4aa3a1d2019d987386b7aac3e9790b10e0a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/02d1168344c969761397b575b64765405752c354.txt", "contents": "\nLabel-Consistent Backdoor Attacks\n\n\nAlexander Turner Mit \nMIT\n\n\nDimitris Tsipras tsipras@mit.edu \nMIT\n\n\nAleksander M\u0105dry madry@mit.edu \nMIT\n\n\nMit \nMIT\n\n\nLabel-Consistent Backdoor Attacks\n\nDeep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are-often blatantly-mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency-the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.\n\nIntroduction\n\nDespite the impressive progress of deep learning on challenging benchmarks (Krizhevsky et al., 2012;Graves et al., 2013;Mnih et al., 2015;Silver et al., 2016;He et al., 2016), real-world deployment of such systems remains challenging due to security and reliability concerns.\n\nOne particular vulnerability stems from the fact that state-of-the-art ML models are trained on large datasets, which, unfortunately, are expensive to collect and curate. It is thus common practice to use training examples sourced from a variety of, often untrusted, sources. While this practice can be justified in a benign setting-bad samples tend to only slightly degrade the model's performance-it can be unreliable in the presence of a malicious adversary. In fact, there exists a long line of work exploring the impact of injecting maliciously crafted samples into a model's training set, known as data poisoning attacks (Biggio et al., 2012).\n\nMore recently, Gu et al. (2017) introduced backdoor attacks. The purpose of these attacks is to plant a backdoor in any model trained on the poisoned dataset. This backdoor can be activated during inference by a backdoor trigger-such as a small pattern in the input-forcing the model to output a specific target label chosen by the adversary. This vulnerability is particularly difficult to detect (e.g., by evaluating on a holdout set) since the model behaves normally in the absence of the trigger. Moreover, it allows the adversary to manipulate the model prediction on any input during inference.\n\nStandard backdoor attacks are based on randomly selecting a few natural inputs, applying the backdoor trigger to them, setting their labels to the target label, and injecting them into the training set. These attacks, despite being very effective, have one crucial shortcoming: the injected inputs are-often clearly-mislabeled ( Figure 1). Such blatantly incorrect labels would be deemed suspicious should the poisoned inputs undergo human inspection, potentially revealing the attack. In fact, such an inspection is quite likely given that-as we find in Section 3-these samples are often flagged as outliers even by rudimentary filtering methods.\n\nThe focus of our work is to investigate whether the use of such clearly incorrect labels is really necessary:\n\nCan backdoor attacks be successful when poisoned inputs and their labels appear consistent to a human?\n\nSuch label-consistent attacks would be particularly insidious, since they would go undetected even upon human inspection.\n\n\"bird\" \"bird\" \"bird\" \"bird\" Figure 1: Typical input-label pairs poisoned using the backdoor attack of Gu et al. (2017) with \"bird\" as the target label. The images are clearly mislabelled and would thus raise suspicion upon human inspection.\n\n(Here the backdoor trigger is a black-and-white corner pattern.)\n\nOur contributions. The starting point of our work is the observation that, for backdoor attacks to be successful, the poisoned inputs need to be hard to classify without relying on the backdoor trigger. If the poisoned inputs can be correctly classified based solely on their salient features, the model is likely to ignore the backdoor trigger-hence the attack will be unsuccessful. Indeed, applying a standard backdoor attack using only correctly labeled inputs is ineffective (Section 4.1). Building on this intuition, we develop an approach for synthesizing effective, label-consistent, poisoned inputs. Our approach consists of perturbing the original inputs in order to render them harder to classify, while keeping the perturbation sufficiently minor to ensure that the original label remains consistent. We perform this transformation in two ways:\n\n\u2022 GAN-based interpolation: we interpolate poisoned inputs towards an incorrect class in the latent space embedding of a Generative Adversarial Network (GAN) .\n\n\u2022 Adversarial perturbations: we maximize the loss of an independently trained model on the poisoned inputs while remaining close to the original input in some p -norm (Szegedy et al., 2014).\n\nBoth methods result in successful backdoor attacks while maintaining label-consistency ( Figure 2). Furthermore, we improve these attacks by developing backdoor triggers that are less conspicuous and robust to data augmentation. Finally, we provide some insight into how models tend to memorize the backdoor trigger by performing experiments using random noise, as well as studying the value of model's training loss on the poisoned samples during training.\n\n\nOriginal image\n\n\nGAN-based\n\nAdversarial-based Figure 2: Label-consistent poisoned inputs obtained using our proposed methods. The original training image appears at the top, our GAN-based approach in the middle, and our adversarial example-based approach at the bottom. All images are labeled as birds, which is consistent with their content. This is in stark contrast to standard poisoned inputs (Figure 1), which are clearly mislabeled. Note that we have additionally modified the backdoor trigger to be less visible (Section 4.4).\n\n\nBackground\n\n\nSetting\n\nWe consider a standard classification setting where the goal is to predict the label y \u2208 [k] of an input x \u2208 X , where the input-label pairs (x, y) are sampled i.i.d. from a distribution D over X \u00d7 [k]. The model is represented as a function f \u03b8 : X \u2192 [k], which is parametrized by some \u03b8 \u2208 R d . The parameters \u03b8 of the model are obtained by minimizing a loss function L(x, y, \u03b8)-which captures how well the model performs on the input-label pair (x, y)-over a training set D\n= {(x i , y i )} n i=1 of labeled examples: \u03b8 * = arg min \u03b8 1 n n \u2211 i=1 L(x i , y i , \u03b8).\nHard Inputs. Typically, this minimization is performed via stochastic gradient descent (SGD)-iteratively updating the parameters based on the loss gradient on a random subset of the training set. For typical choices of the loss function (e.g., hinge or cross-entropy), the SGD update will weight more the gradients of inputs on which the model performs poorly (e.g., misclassified inputs). This justifies our intuition that training on inputs that are harder to classify will result in these samples having a large impact on the resulting model.\n\n\nThreat model\n\nWe consider a backdoor attack setting where an adversary operates as follows. First, they choose a target label y target \u2208 [k] and a function T : X \u2192 X which applies the backdoor trigger (e.g., a small input pattern) to the input. Then, given access to input-label pairs from the data distribution D, they produce a limited number of arbitrary samples (x, y). Finally, these samples are injected into the training set of the algorithm.\n\nAttack success rate. To evaluate the success of a backdoor attack, we are interested in the behavior of a model trained on the poisoned dataset when we apply the backdoor trigger onto previously unseen (test) samples. The attack success rate is the fraction of test samples that are not originally labelled as the target label, but are nevertheless classified by the trained model as the target label when the backdoor is introduced:\nPr (x,y)\u223cD f \u03b8 (T(x)) = y target | y = y target .\nAttacker knowledge. In general, we assume that the adversary has full knowledge of the model architecture and training procedure, while also having access to the underlying data distribution. These assumptions are realistic, since most ML pipelines tend to use the same standard architectures and training methods, while public datasets exist for most input domains. Nevertheless, we also consider a weaker adversary (Appendix C), who only has access to data from a different dataset and targets a different model architecture.\n\n\nLabel-consistency.\n\nOur goal is this work is to study attacks that do not inject suspicious samples into the training set. In particular, we define an attack to be label-consistent if all the poisoned samples (x, y) injected into the training set appear correctly labeled to a human.\n\n\nRelated work\n\nOther forms of data poisoning. While the focus of our work is on backdoor attacks, it is worth mentioning that there has been significant work on poisoning attacks with different objectives. The most common objective is test accuracy degradation. In these attacks, the adversary introduces outliers that force the model to learn a suboptimal decision boundary (Biggio et al., 2012;Xiao et al., 2012Xiao et al., , 2015Newell et al., 2014;Mei & Zhu, 2015;Steinhardt et al., 2017). Note, however, that these attacks tend to be restricted to simpler models (e.g., linear classifiers) since expressive models can memorize these outliers without sacrificing performance on clean data. Another common attack objective is that of targeted misclassification where the adversary aims to cause the model to misclassify a specific set of test inputs Shafahi et al., 2018).\n\nPrior work on backdoor attacks. Backdoor attacks were introduced in Gu et al. (2017) for the setting of transfer learning. Chen et al. (2017) adapted these attacks to the standard poisoning setting and explored less visible triggers. Later work proposed methods for detecting and hampering backdoor attacks based on the trigger structure (Wang et al., 2019) or the latent representation of the resulting model (Tran et al., 2018;Liu et al., 2018;Chen et al., 2018). However, these methods do not provide any formal guarantees and, in general, it is unclear whether they are fully robust to adaptive adversaries.\n\nPrior work on label-consistency. Attacks restricted to only using correctly label poisoned samples have been explored in prior work, being referred to as \"defensible\" , \"plausible\" , \"visually indistinguishable\" , and \"clean-label\" (Shafahi et al., 2018). However, such attacks have only been developed in the targeted poisoning setting (where the adversary aims to alter the model's prediction on specific test examples) and cannot be directly applied in the backdoor setting.\n\n\nLimitations of standard backdoor attacks\n\nStandard backdoor attack. Recall that the goal of the attack is to cause the trained model to strongly associate a specific backdoor trigger (input pattern) with a specific target label chosen by the adversary. Towards that goal, the adversary first randomly selects a small number of natural samples. Then, they modify these samples by applying the backdoor trigger onto them (adding as small input pattern). Finally, they set these samples' labels to be the chosen target label. As a result, the model learns to predict the target label based on the trigger. Thus, during inference, the adversary can cause the model to predict the target label on any input by simply applying the backdoor trigger onto it. We reproduce the attack in Appendix B.\n\nDetecting standard poisoned inputs. Note that the inputs injected under this attack are most likely mislabeled. In order to further motivate label-consistent attacks, we consider a simple data sanitization scheme consisting of a rudimentary outlier detection process followed by human inspection of the identified outliers. Specifically, we train a standard classifier on a small, non-poisoned dataset (1024 examples). Such a clean dataset could be obtained from a trusted source or by thorough inspection of the training inputs. We then evaluate this classifier on a training set for which 100 out of 50 000 inputs have been poisoned and measure the value of the classifier's loss on each (potentially poisoned) input. Since poisoned inputs are mislabeled, we expect them to have higher loss than the rest of the inputs (e.g., the classifier assigns low probability to their labels). Indeed, we find that poisoned inputs are biased towards higher loss values ( Figure 3a). By manually inspecting the inputs with highest loss, we find several poisoned inputs which are clearly mislabeled (Appendix Figures 18, 19).\n\n\nTowards label-consistent backdoor attacks\n\nIn the previous section, we saw how poisoned samples from standard backdoor attacks could be easily detected due to their incorrect labels. Moreover, even if these attacks were improved to evade the particular detection scheme that we consider, the poisoned samples would likely still undergo human inspection should a more sophisticated mechanism be used-these samples are, after all, outliers planted by the adversary. Thus, in order to evade human inspection and be truly insidious, these attacks would need to ensure the label-consistency of the poisoned inputs injected.\n\n\nAn (ineffective) baseline\n\nThe first attempt towards designing label-consistent attacks would be to simply restrict a standard attack to only using inputs from the target class. However, this renders the attack ineffective ( Figure 3b). This should not be surprising. Since the poisoned samples are labeled correctly, the model can classify them based on their salient features and hence there is little reason to associate the backdoor trigger with the target label. For each potential number of inspected inputs (x-axis) we plot the number of poisoned inputs contained within them (y-axis). We find that when inspecting a small number of images (e.g., 300) we find a significant number of poisoned images (e.g., 20) which are mislabeled (see examples in Appendix Figure 19). (b) The Gu et al. (2017) attack, but restricted to only consistent labels (i.e., only images from the target class are poisoned). The attack is ineffective-even at 25% poisoning, only one class exceeds 50% attack success.\n\nBased on this observation, in order to ensure that the backdoor trigger is learned, we will need to modify the poisoned inputs to make them harder to classify. Since these inputs will be harder to learn, the model is more likely to rely on the trigger, resulting in a successful backdoor attack. In order to ensure that the poisoned inputs remain label-consistent, we will restrict the attacks to minor perturbations of the natural inputs. In the following sections, we will explore two approaches for synthesizing such perturbations: one based on latent space interpolations and the other based on p -bounded adversarial perturbations. As we will see in Section 6.2, these methods actually result in inputs that are harder to classify.\n\n\nMethod 1: Latent space interpolation\n\nGenerative models such as Generative Adversarial Networks (GANs)  and variational auto-encoders (VAEs) (Kingma & Welling, 2015) operate by learning an embedding of the data distribution into a low-dimensional space-the latent space. By ensuring that the latent space is simple enough that it can explicitly sampled from, one can generate realistic inputs by mapping latent points into the input space. An important property of these embeddings is that they are \"semantically meaningful\"-by interpolating latent representations, one can obtain a smooth transition between samples from the original distribution in a perceptually plausible way (Radford et al., 2016).\n\nHere, we will utilize such embeddings in order to make training samples harder to classify by interpolating them towards a different, incorrect class in that latent space. Concretely, given a generative model G : R d \u2192 R n that maps vectors in a d-dimensional latent space to samples in the n dimensional ambient space (e.g., image pixels), we want to interpolate a given input x 1 from the target class towards an input x 2 belonging to another, incorrect class.\n\nTo perform this, we first embed x 1 , x 2 into the latent space of G. Specifically, we optimize over the latent space to find vectors that produce inputs close to x 1 and x 2 in 2 -norm:\nz i = arg min z\u2208R d x i \u2212 G(z) 2 .\nThen, for some constant \u03c4, we generate the sample corresponding to interpolating z 1 and z 2 in latent space\nx = G(\u03c4z 1 + (1 \u2212 \u03c4)z 2 ).\nVarying \u03c4 produces a smooth transition from x 1 to x 2 (cf Figure 4), even though we are not able to perfectly reproduce x 1 and x 2 . The resulting poisoned input is produced by applying the backdoor trigger to x and assigning it the target label-the ground-truth label of x 1 . See Appendix A for implementation details. \n\n\nMethod 2: Adversarial perturbations\n\nAdversarial examples are natural inputs that have been slightly perturbed with the goal of being misclassified by an ML model (Section 4.3). In fact, the perturbations have been found to transfer across different models or even across different architectures (Szegedy et al., 2014;Papernot et al., 2016).\n\nHere, we utilize adversarial perturbations and their transferability across models and architectures in a somewhat unusual way. Instead of causing a model to misclassify an input during inference, we use them to cause the model to misclassify during training. Specifically, we will apply an adversarial transformation to the poisoned inputs (before applying the trigger) to make them harder to learn. Concretely, given a pre-trained classifier f \u03b8 with loss L and an input-label pair (x, y), we construct a perturbed variant of x as\nx adv = arg max x \u2212x p \u2264\u03b5 L(x , y, \u03b8),\nfor some p -norm and a small constant \u03b5. We solve this optimization problem using a standard method in this context, projected gradient descent (PGD) (Madry et al., 2018) (see Appendix A for more details). In fact, we will use perturbations based on adversarially trained models (Madry et al., 2018) since these perturbations are more likely to resemble the target class for large \u03b5 (Tsipras et al., 2019).\n\nWe use x adv along with the original, ground-truth label of x (the target label) as our poisoned input-label pair. By controlling the value of \u03b5 we can vary the resulting image from slightly perturbed to visibly incorrect ( Figure 5). Note that these adversarial examples are computed with respect to an independent, pre-trained model since the adversary does not have access to the training process. \nOriginal 2 , \u03b5 = 300 2 , \u03b5 = 600 2 , \u03b5 = 1200 \u221e , \u03b5 = 8 \u221e , \u03b5 = 16 \u221e , \u03b5 = 32\n\nImproving backdoor trigger design\n\nSo far, we have described methods for constructing label-consistent poisoned inputs. However, for these inputs to appear truly benign, we need to also ensure that the trigger itself appears natural. Here, we discuss two modifications for improving the conspicuousness and robustness of our attacks.\n\nReducing trigger visibility. The original backdoor attack of Gu et al. (2017) uses a black-and-white 3 \u00d7 3 pattern in a corner of the image. When applied to natural images, this pattern is clearly visible and appears unnatural ( Figure 1). Instead, we will utilize a backdoor trigger of reduced visibility. Specifically, instead of replacing the corresponding pixels with the chosen pattern, we will perturb the original pixel values by a backdoor trigger amplitude. Concretely, we add this amplitude to pixels that are white in the original pattern and subtract it for pixels that are black. We perform this modification at each color channel and then clip the pixel values to valid range (see Figure 6a for an example). By controlling the trigger amplitude, we are able to produce a trigger which is less visible while still being learnable by the model. Note that, depending on the exact threat model, the visibility of the trigger during inference might not be constrained. In such cases, the adversary can apply a fully visible pattern during inference to increase the attack success rate (while still employing a reduced visibility pattern for training) (Chen et al., 2017). Thus, when using a trigger with reduced amplitude for poisoning, we additionally evaluate the attack success rate with a full amplitude trigger during testing.\n\nResistance to data augmentation. Typical ML training pipelines include data augmentation-training on random transformations (e.g., crops and flips) of the original images. Since the backdoor trigger is introduced before these transformations, it might be obscured during training, making the attack less effective. In order to ensure that the trigger remains visible throughout training, we perform a simple modification: we replicate the pattern on all four corners of the image (appropriately flipped). This ensures that the trigger is invariant under flips and always visible under crops (see Figure 6b for examples).\n\n\nEvaluating our attacks\n\nIn this section, we evaluate the effectiveness of our label-consistent attacks. To avoid introducing conflating factors, we initially study models trained without data augmentation using the original, fully visible trigger. Then, we then present our results using attacks with improved trigger designs. We discuss how our methods perform when the adversary does not have full information about the training procedure in Appendix C.\n\nDataset. We conduct our experiments on the CIFAR-10 dataset (Krizhevsky, 2009), consisting of 10 classes with 5000 training inputs each. For each target label, we evaluate attacks poisoning different fractions of the training inputs from that class-0.4%, 1.5%, 6%, 25%, and 100% (corresponding to 20, 75, 300, 1250, and 5000 inputs, respectively). Note that these fractions correspond to examples from a single class-poisoning 6% of the inputs of a target class corresponds to poisoning only 0.6% of the entire training set.\n\n\nBasic attack\n\nAttack success rate. Recall that our key metric of interest is the attack success rate-the fraction of test images that are incorrectly classified as the target class when the backdoor is applied. We evaluate our attacks across a range of different perturbation magnitudes by varying the interpolation coefficient \u03c4 or the p -norm bound \u03b5. Matching our original motivation, we find that larger perturbations-and hence harder inputs-lead to more successful attacks (Figure 7). Overall, both approaches lead to effective attacks, achieving a high attack success rate with relatively few poisoned inputs.\n\nLabel-consistency. At the same time, we find that large perturbations result in labels that are less plausible. In order to ensure the label-consistency of the poisoned samples, we choose to restrict our perturbations to \u03c4 = 0.2 and \u03b5 = 300 in 2 -norm (pixel values are in [0, 255]). We found that, under this restriction, all images we inspected are consistent with their labels while the attacks are still successful. For instance, our perturbation-based attack achieves more than a 50% attack success rate on half the classes while introducing only 75 label-consistent poisoned inputs. This is in stark contrast to the label-consistent baseline which is ineffective when injecting below 300 inputs (Figure 3b). See Appendix Figures 20, 21 for examples of poisoned inputs. We evaluate the corresponding attacks targeting each class in Appendix Figure 13 and perform a per-class comparison in Appendix Figure 14. Impact on standard accuracy. None of the attacks have an apparent effect on the accuracy of the modelthe resulting test accuracy is 92% with data augmentation and 87% without. The only exception is at 100% poisoning where the model predicts incorrectly on the entire target class. We find that this is due to the model fully relying on the trigger to predict the target class, hence predicting incorrectly in the absence of it.\n\n\nImproved trigger attacks\n\nReduced trigger visibility. We evaluate an attack using 2 -bounded perturbations of \u03b5 = 300 for the \"dog\" class when applying a trigger of amplitude of 16, 32, 64, and 255 ( Figure 8a). We find that for less visible triggers, the attack is no longer effective when poisoning 1.5% of the target class (75 images), but is still successful when poisoning more than 6% of the target class (300 images). Additionally, for the case of 6% poisoning, increasing the amplitude from 16 to 64 appears to have a log-linear effect, but stays roughly constant when increasing to 255. Overall, we find that the attack can still be successful with relatively few poisoned inputs even when the trigger visibility is significantly reduced.\n\n\nData augmentation.\n\nWe evaluate both the original, one-corner trigger and the modified, four-corner trigger with and without data augmentation (Figure 8b). Specifically, we use a standard CIFAR10 augmentation procedure with random left-right flips, random crops of 2 pixels, and per-image standardization. We find that, when data augmentation is not used, there is little difference in performance between the fourcorner attack and the one-corner attack. However, when data augmentation is used, the one-corner trigger becomes essentially ineffective. More importantly, we find that data augmentation (when combined with our improved trigger) actually improves attack's success rate (Figure 8b). We conjecture that this behavior is due to the overall learning problem being more difficult (the classifier needs to also classify the augmented images correctly), encouraging the classifier to rely on the backdoor.\n\nEvaluating the overall attack. We now evaluate an attack using a less visible pattern in the presence of data augmentation (with the improved four corner pattern) for all classes. During inference, we evaluate the Evaluating attacks using the one-and four-corner triggers with and without data augmentation for the \"frog\" class. Data augmentation renders the one-corner trigger ineffective, yet improves the four-corner trigger.\n\nresulting model using both the (less visible) trigger used during training (Figure 9a) as well as the original, fully visible pattern (Figure 9b). We find that a full-visibility trigger during inference greatly improves the attack performance. In particular, for all cases where the attack success rate exceeds 20% with the less visible trigger, we can boost that rate to essentially 100% by increasing the trigger amplitude during inference. Overall, the attack is successful when injecting at least 300 inputs while the inputs injected appear benign and quite similar to the original natural samples (Appendix Figure 22).  Figure 9: Attack success rate for each target class when using the reduced visibility trigger (amplitude 16) and 2 -bounded perturbation of \u03b5 = 300 (models are trained with data augmentation). We evaluate the model during inference with both the reduced visibility trigger (a) and the fully visible trigger (b). The corresponding plots for GAN-based interpolation are in Appendix Figure 15, while attacks using slightly more visible patterns of amplitude 32 (and thus having stronger performance) in Appendix Figure 16. 6 Exploring the underlying attack mechanism\n\n\nOn the relative performance of latent interpolations and adversarial examples\n\nBased on our evaluation, p -bounded adversarial perturbations are more effective for constructing backdoor attacks compared to the GAN-based interpolation scheme, especially when the allowed perturbation is large. This may seem surprising given that both are valid methods for constructing inputs that are hard to classify.\n\nOne potential explanation is that the inputs created via interpolation appear to be blurry without significant salient features. Hence a model that simply weakly associates the backdoor with the target class will learn to predict based on the trigger in the absence of salient features but will have low success rate during inference on natural inputs. Instead, adversarial perturbations introduce features of an incorrect class which might be visible (Tsipras et al., 2019) (see also Figure 21) or imperceptible (Ilyas et al., 2019). Hence the model is forced to predict the target class even in the presence of strong input signal.\n\nIn order to further investigate this hypothesis, we perform experiments where we add Gaussian noise with mean zero and varying degrees of variance to poisoned inputs before adding the trigger (Figure 10a). We find that, while a small amount of noise makes the attack more effective, increasing the noise variance has an adverse effect. This finding corroborates our hypothesis. Adding noise with small variance leads to harder to classify images while preserving the original input features to a certain extent. In contrast, large variance results in very noisy inputs without significant salient information, leading to ineffective attacks.  Figure 17 for the rest of the classes). The training loss remains high without the trigger indicating that the model does not learn the salient features of poisoned images. (Note that we follow a standard training process and drop the learning rate after 40 000 steps and again after 60 000 steps.)\n\n\nLoss of poisoned inputs over training\n\nIn order to gain some additional insight into the mechanism behind the attack, we study the value of the model's loss through training (Figure 10b). We find that poisoned inputs have similar (and often substantially smaller) loss values compared to clean inputs throughout the entire training process (the model is predicting correctly on the training set). At the same time, the loss of poisoned samples without the backdoor trigger remains high throughout training. This indicates that the model makes its prediction on these examples by heavily relying on the backdoor trigger. This confirms our intuition that these inputs are harder to classify during training since the model classifies them incorrectly without the trigger (despite its good test accuracy).\n\n\nConclusion\n\nIn this work, we identify label-consistency-having poisoned inputs remain consistent with their labels-as a key desired property for backdoor attacks. Previous backdoor attacks lack this property, resulting in clearly mislabeled poisoned samples that make the overall attack very likely to be detected.\n\nWe show that it is possible to perform effective, label-consistent backdoor attacks. The key idea behind our methods is that, in order for the model to associate the backdoor trigger with the target label, the inputs need to be difficult to classify based on their natural salient features. We synthesize such hard inputs using adversarial perturbations and latent embeddings provided by generative models.\n\nOverall, our findings establish the conceptual framework of hard-to-learn samples as a powerful tool for developing insidious backdoor attacks. We believe that further work within this framework will lead to attacks that are even more effective and can bypass sophisticated filtering schemes. More broadly, our findings demonstrate that backdoor attacks can be made significantly harder to detect by humans. This emphasizes the need for developing principled methods for defending against such attacks since human inspection might be inherently ineffective.\n\n\nA Experimental setup\n\nModel architecture and training. We use a standard residual network (ResNet) (He et al., 2016) with three groups of residual layers with filter sizes of 16, 16, 32 and 64, and five residual units each. We use a momentum optimizer to train this network with a momentum of 0.9, a weight decay of 0.0002, batch size of 50, batch normalization, and a step size schedule that starts at 0.1, reduces to 0.01 at 40 000 steps and further to 0.001 at 60 000 steps. The total number of training steps used is 100 000. We did not modify this architecture and training procedure.\n\n\nImplementation of GAN-based interpolation.\n\nWe train a WGAN Gulrajani et al., 2017) using the publicly available implementation 1 . In order to generate images similar to the training inputs, we optimize over the latent space using 1000 steps of gradient descent with a step size of 0.1, following the procedure of Ilyas et al. (2017). To improve the image quality and the ability to encode training set images, we train the GAN using only images of the two classes between which we interpolate. We also experimented with using a single GAN and found similar results.\n\nImplementation of adversarial perturbations. We construct adversarial examples using a PGD attack on adversarially trained models (Madry et al., 2018) using the publicly available implementation 2 . We use a (PGD) attack with 100 steps and a step size of 1.5 \u03b5/100.\n\n\nB Reproducing standard backdoor attacks\n\nWe reproduce the original backdoor attack of Gu et al. (2017) using a standard ResNet architecture (He et al., 2016) on the CIFAR10 dataset (Krizhevsky, 2009). The original attack focused on a setting where the model is trained by the adversary and hence did not thoroughly study the number of poisoned examples required.\n\nIn Figure 11, we plot the attack success rate for different target labels and number of poisoned examples injected. We observe that the attack is very successful even with a small (\u223c 75) number of poisoned samples. Note that the poisoning percentages here are calculated relative to the entire dataset. The horizontal axis thus corresponds to the same scale in terms of examples poisoned as the rest of our plots. While the attack is very effective, most image labels are clearly incorrect (Figure 1). \n\n\nC Weaker adversaries\n\nSo far, we have evaluated the performance of an attacker with full knowledge of the model architecture and access to data from the input domain. Here we will evaluate two weaker adversaries utilizing our framework of hard-to-learn samples.\n\nPixel-space interpolation. In the absence of a generative model, we evaluate attacks using interpolation in ambient space (linear interpolation of image pixels). Specifically, for a given interpolation constant \u03c4, the poisoned sample x is generated from the original training sample x and the randomly selected target image z as x = (1 \u2212 \u03c4) x + \u03c4z.\n\nAs the target image for each interpolation we use images from the CINIC-10 dataset (Darlow et al., 2018), which contains inputs with the same size and class structure as CIFAR-10 (obtained from downsampling images from the corresponding ImageNet (Russakovsky et al., 2015) classes).\n\nThe attack success rate is presented in Figure 12a. We find that while the attacks are generally weaker than the GAN-based interpolation, they still outperform the naive baseline. A per-class comparison can be found in Figure 14.\n\nWeaker adversarial perturbations. Here, we consider an adversary that has only approximate knowledge of the model architecture and dataset used. Specifically, the adversary will target an (adversarially trained) VGG-style model (Simonyan & Zisserman, 2015) which is trained on 50 000 randomly selected images from the CINIC-10 dataset (Darlow et al., 2018). We choose this architecture since it is perhaps the most dissimilar to the ResNet architecture used by our model (adversarial examples tend to transfer less between VGG-style models and ResNets). The attack is otherwise unchanged.\n\nThe success rate of this attack is presented in Figure 12b. We find that this attack, while less powerful than the white-box attack, can still result in effective attacks and significantly outperform the other baselines.    Figure 10b). The loss of poisoned inputs without the trigger remains high throughout training, indicating that they cannot be classified correctly by the model without relying on the backdoor trigger.   Figure 22: Random poisoned inputs constructed using our attack utilizing adversarial perturbations bound by \u03b5 = 300 in 2 norm using a reduced visibility trigger (amplitude of 16) applied to all four corners of the image. Each row contains five pairs of randomly chosen inputs from a single class. The left image in each pair is the original image from the CIFAR-10 dataset and the right image is the corresponding perturbed image. The poisoned inputs appear benign and label-consistent without being significantly different from the original, natural inputs. \n\nFigure 3 :\n3Limitations of standard backdoor attacks. (a) Measuring the effectiveness of our simple detection scheme.\n\nFigure 4 :\n4GAN-based interpolation between inputs. Images of a frog and horse are shown on the far left and right, respectively. Interpolated images are shown in between, where \u03c4 is the degree of interpolation. Note that \u03c4 = 0.0 and 1.0 represent the (approximate) reconstruction of the original frog and horse, respectively. See AppendixFigure 20for additional examples.\n\nFigure 5 :\n5Example of adversarial perturbations for different levels of distortion (\u03b5) bounded in 2 -and \u221e -norm for adversarially trained models (pixels lie in [0, 255]). Additional examples in Appendix Figure 21.\n\nFigure 6 :\n6Improved trigger design. (a) Poisoned input with varying backdoor trigger amplitudes. From left to right: backdoor trigger amplitudes of 0 (original image), 16, 32, 64, and 255 (standard backdoor trigger). (b) Random inputs with the four-corner trigger applied (left two: full visibility; right two: reduced visibility).\n\nFigure 7 :\n7Comparing attack performance under different perturbation constraints. We find that increased perturbation strength leads to higher attack success rate. See Appendix Figures 20, 21 for a few random inputs. (a) Varying degrees of GAN-based interpolation for the \"deer\" class. (b) Adversarial perturbations bounded by \u03b5 in p -norm for different p and \u03b5 for the \"airplane\" class.\n\nFigure 8 :\n8Evaluating attacks with improved triggers. (a) Reducing the backdoor trigger's amplitude (from 255 to 16, 32, and 64) still results in successful poisoning when poisoning 6% or more of the dog class. (b)\n\nFigure 10 :\n10(a) Performance of poisoning attacks when adding Gaussian noise with zero mean and varying degrees of variance to the poisoned inputs with \"dog\" as target class. While low variance typically enhances the attack, high variance leads to ineffective attacks. (b) Value of median training loss and interquartile range of poisoned inputs with and without the trigger (average loss included as a baseline). The attack poisons 300 examples of the \"automobile\" class using 2 -bound perturbations of \u03b5 = 300 (see Appendix\n\nFigure 11 :\n11Reproducing the Gu et al. (2017) attack on CIFAR-10. The attack is very effective-a backdoor is injected with just 75 (0.15%) training examples poisoned.\n\nFigure 12 :Figure 13 :\n1213Performance of weaker adversaries. (a) Attack performance on all classes for the pixel-space interpolation baseline attack. While improving upon the naive baseline, this attack is less effective than the GAN-based interpolation attack. (b) Attack performance on all classes for the perturbation-based attack without access to the model architecture and training set. This attack is less powerful than the white-box attack, but still achieves substantially higher attack success rates than the baseline. Attack performance of our label-consistent attacks for each target class (using fully visible trigger and no data augmentation). Both attacks achieve significantly higher success rates compared to the baseline attack (Figure 3b). A per-class comparison can be found in AppendixFigure 14.(a) GAN-based interpolation with \u03c4 = 0.2. (b) Adversarial perturbations 2 -bounded by \u03b5 = 300 (pixel values in [0, 255]).\n\nFigure 14 :Figure 15 :Figure 16 :\n141516Per-class comparison of different poisoning approaches. We compare the performance of: (a) the baseline of theGu et al. (2017) attack restricted to only consistent labels, (b) the pixel-wise interpolation attack of Appendix C, (c) our GAN-based interpolation attack, and (d) our adversarial example-based attack for each class. Performance of the GAN-based interpolation attack with a reduced visibility trigger (amplitude 16). (a) Evaluating with a reduced amplitude pattern during inference. (b) Using the full visibility pattern during inference greatly increases the attack success rate. Performance of our label-consistent attacks when using a trigger amplitude of 32 for GAN-based interpolation and adversarial perturbation-based attacks. Top row: Using the trigger used during training (amplitude 32) for inference. Bottom row: Using a full-amplitude trigger (255) during inference. All attacks are performed in the presence of data augmentation (using the four-corner pattern).\n\nFigure 17 :\n17Plotting the training loss of poisoned examples with and without the trigger and compare it to the average training loss for each target class (similar to\n\nFigure 18 :Figure 19 :Figure 20 :Figure 21 :\n18192021Performing the manual inspection for the filtering scheme of Section 3 for the original attack ofGu et al. (2017) (with 75 injected inputs) and our proposed attacks (with 300 injected inputs each). We show the 20 samples assigned highest loss by the pre-trained model with the poisoned samples being highlighted with a black border. In contrast to the original attack, the poisoned inputs from our attacks appear label consistent. Note that most of the high loss examples correspond to natural, non-poisoned inputs. We show the 20 poisoned samples with highest loss in 19. We show the 20 poisoned samples from each dataset assigned highest loss by the filtering scheme of Section 3. We do not find any examples from our attacks that are clearly mislabelled. All examples shown are labeled as \"Random examples of GAN-based interpolations. Each row shows two sets of randomly chosen examples from a single class. In each set, the leftmost image is the original image from the CIFAR-10 dataset and the subsequent images are the corresponding image interpolations for different values of the interpolation coefficient \u03c4. At \u03c4 = 0 we can see the approximate reconstruction of each input by the GAN. Random examples of p -bounded adversarial perturbations. Each row contains two sets of randomly chosen examples from a single class. In each set, the leftmost image is the original image from the CIFAR-10 dataset and the subsequent images are adversarially perturbed within \u03b5 in p -norm for different values of \u03b5 and p.\nhttps://github.com/igul222/improved_wgan_training 2 https://github.com/MadryLab/cifar10_challenge\nAcknowledgementsWe thank Nicholas Carlini for discussions during early stages of this work and comments on the manuscript. We thank Andrew Ilyas for advice on GAN inversion and Shibani Santurkar for helpful discussions.Work supported in part by the NSF grants CCF-1553428, CNS-1815221, the Microsoft Corporation, and the MIT-IBM Watson AI Lab research grant.\n. Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou, arXiv:1701.07875Wasserstein gan. arXiv preprintMartin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n\nPoisoning attacks against support vector machines. Battista Biggio, Blaine Nelson, Pavel Laskov, ICML. Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In ICML, 2012.\n\nDetecting backdoor attacks on deep neural networks by activation clustering. Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, Biplav Srivastava, arXiv:1811.03728arXiv preprintBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728, 2018.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song, arXiv:1712.05526Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. arXiv preprintXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nN Luke, Darlow, J Elliot, Antreas Crowley, Amos J Antoniou, Storkey, arXiv:1810.03505CINIC-10 is not ImageNet or CIFAR-10. arXiv preprintLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. CINIC-10 is not ImageNet or CIFAR-10. arXiv preprint arXiv:1810.03505, 2018.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, neural information processing systems (NeurIPS). Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In neural information processing systems (NeurIPS), 2014.\n\nSpeech Recognition with Deep Recurrent Neural Networks. Alex Graves, Mohamed Abdel-Rahman, Geoffrey Hinton, International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech Recognition with Deep Recurrent Neural Networks. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2013.\n\nTianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.06733Badnets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv preprintTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv preprint arXiv:1708.06733, 2017.\n\nImproved training of wasserstein gans. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron C Courville, Neural Information Processing Systems (NeurIPS). Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Neural Information Processing Systems (NeurIPS), 2017.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Conference on Computer Vision and Pattern Recognition (CVPR). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nThe robust manifold defense: Adversarial training using generative models. Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, Alexandros G Dimakis, arXiv:1712.09196In ArXiv preprintAndrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold defense: Adversarial training using generative models. In ArXiv preprint arXiv:1712.09196, 2017.\n\nAdversarial Examples Are Not Bugs, They Are Features. Andrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, Aleksander Madry, arXiv:1905.02175In ArXiv preprintAndrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial Examples Are Not Bugs, They Are Features. In ArXiv preprint arXiv:1905.02175, 2019.\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, International Conference on Learning Representations (ICLR). Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations (ICLR), 2015.\n\nUnderstanding Black-box Predictions via Influence Functions. Wei Pang, Percy Koh, Liang, ICML. Pang Wei Koh and Percy Liang. Understanding Black-box Predictions via Influence Functions. In ICML, 2017.\n\nLearning Multiple Layers of Features from Tiny Images. Alex Krizhevsky, Technical reportAlex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. In Technical report, 2009.\n\nImagenet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems (NeurIPS). Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.\n\nFine-pruning: Defending against backdooring attacks on deep neural networks. Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg, International Symposium on Research in Attacks, Intrusions, and Defenses. Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, 2018.\n\nTowards deep learning models resistant to adversarial attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, International Conference on Learning Representations (ICLR). Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018.\n\nBlockwise p-tampering attacks on cryptographic primitives, extractors, and learners. Saeed Mahloujifar, Mohammad Mahmoody, Theory of Cryptography Conference (TCC. Saeed Mahloujifar and Mohammad Mahmoody. Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners. In Theory of Cryptography Conference (TCC), 2017.\n\nSaeed Mahloujifar, Mohammad Mahmoody, arXiv:1810.01407Can Adversarially Robust Learning Leverage Computational Hardness? In arXiv preprint. Saeed Mahloujifar and Mohammad Mahmoody. Can Adversarially Robust Learning Leverage Computational Hardness? In arXiv preprint arXiv:1810.01407, 2018.\n\n. Saeed Mahloujifar, Dimitrios I Diochnos, Mohammad Mahmoody, arXiv:1711.03707Learning under p-Tampering Attacks. In arXiv preprintSaeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. Learning under p-Tampering Attacks. In arXiv preprint arXiv:1711.03707, 2017.\n\nThe curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure. Saeed Mahloujifar, Dimitrios I Diochnos, Mohammad Mahmoody, AAAI Conference on Artificial Intelligence (AAAI). Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure. In AAAI Conference on Artificial Intelligence (AAAI), 2018.\n\nUsing Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners. Shike Mei, Xiaojin Zhu, AAAI. Shike Mei and Xiaojin Zhu. Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners. In AAAI, pp. 2871-2877, 2015.\n\nHuman-level Control through Deep Reinforcement Learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level Control through Deep Reinforcement Learning. In Nature, 2015.\n\nOn the Practicality of Integrity Attacks on Document-Level Sentiment Analysis. Andrew Newell, Rahul Potharaju, Luojie Xiang, Cristina Nita-Rotaru, Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop. the 2014 Workshop on Artificial Intelligent and Security WorkshopACMAndrew Newell, Rahul Potharaju, Luojie Xiang, and Cristina Nita-Rotaru. On the Practicality of Integrity Attacks on Document-Level Sentiment Analysis. In Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop, pp. 83-93. ACM, 2014.\n\nTransferability in Machine Learning: from Phenomena to Black-box Attacks using Adversarial Samples. Nicolas Papernot, Patrick Mcdaniel, Ian Goodfellow, arXiv:1605.07277In ArXiv preprintNicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in Machine Learning: from Phenomena to Black-box Attacks using Adversarial Samples. In ArXiv preprint arXiv:1605.07277, 2016.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, International Conference on Learning Representations (ICLR). Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR), 2016.\n\nImageNet Large Scale Visual Recognition Challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, In International Journal of Computer Vision. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. In International Journal of Computer Vision (IJCV), 2015.\n\nPoison frogs! targeted clean-label poisoning attacks on neural networks. Ali Shafahi, Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein, Advances in Neural Information Processing Systems (NeurIPS). Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nMastering the Game of Go with Deep Neural Networks and Tree Search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. In Nature, 2016.\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition. Karen Simonyan, Andrew Zisserman, International Conference on Learning Representations (ICLR). Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In International Conference on Learning Representations (ICLR), 2015.\n\nCertified Defenses for Data Poisoning Attacks. Jacob Steinhardt, Pang Wei W Koh, Percy S Liang, NIPS. Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified Defenses for Data Poisoning Attacks. In NIPS, 2017.\n\nSequence to Sequence Learning with Neural Networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in Neural Information Processing Systems (NeurIPS). Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2014.\n\nIntriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, International Conference on Learning Representations (ICLR). Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.\n\nSpectral Signatures in Backdoor Attacks. Brandon Tran, Jerry Li, Aleksander M\u0105dry, Advances in Neural Information Processing Systems (NeurIPS). Brandon Tran, Jerry Li, and Aleksander M\u0105dry. Spectral Signatures in Backdoor Attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nRobustness May Be at Odds with Accuracy. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry, International Conference on Learning Representations (ICLR). Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness May Be at Odds with Accuracy. In International Conference on Learning Representations (ICLR), 2019.\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Y Ben, Zhao, Proceedings of 40th IEEE Symposium on Security and Privacy. 40th IEEE Symposium on Security and PrivacyBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In Proceedings of 40th IEEE Symposium on Security and Privacy, 2019.\n\nAdversarial Label Flips Attack on Support Vector Machines. Han Xiao, Xiao Huang, Claudia Eckert, ECAI. Han Xiao, Huang Xiao, and Claudia Eckert. Adversarial Label Flips Attack on Support Vector Machines. In ECAI, pp. 870-875, 2012.\n\nSupport vector machines under adversarial label contamination. Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, Fabio Roli, Neurocomputing. 160Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, and Fabio Roli. Support vector machines under adversarial label contamination. In Neurocomputing, volume 160, pp. 53-62, 2015.\n", "annotations": {"author": "[{\"end\":64,\"start\":37},{\"end\":104,\"start\":65},{\"end\":142,\"start\":105},{\"end\":153,\"start\":143},{\"end\":64,\"start\":37},{\"end\":104,\"start\":65},{\"end\":142,\"start\":105},{\"end\":153,\"start\":143}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":54},{\"end\":81,\"start\":74},{\"end\":121,\"start\":116},{\"end\":57,\"start\":54},{\"end\":81,\"start\":74},{\"end\":121,\"start\":116}]", "author_first_name": "[{\"end\":46,\"start\":37},{\"end\":53,\"start\":47},{\"end\":73,\"start\":65},{\"end\":115,\"start\":105},{\"end\":146,\"start\":143},{\"end\":46,\"start\":37},{\"end\":53,\"start\":47},{\"end\":73,\"start\":65},{\"end\":115,\"start\":105},{\"end\":146,\"start\":143}]", "author_affiliation": "[{\"end\":63,\"start\":59},{\"end\":103,\"start\":99},{\"end\":141,\"start\":137},{\"end\":152,\"start\":148},{\"end\":63,\"start\":59},{\"end\":103,\"start\":99},{\"end\":141,\"start\":137},{\"end\":152,\"start\":148}]", "title": "[{\"end\":34,\"start\":1},{\"end\":187,\"start\":154},{\"end\":34,\"start\":1},{\"end\":187,\"start\":154}]", "venue": null, "abstract": "[{\"end\":1251,\"start\":189},{\"end\":1251,\"start\":189}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1367,\"start\":1342},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1387,\"start\":1367},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1405,\"start\":1387},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1425,\"start\":1405},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1441,\"start\":1425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2192,\"start\":2171},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2226,\"start\":2210},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3902,\"start\":3886},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5298,\"start\":5276},{\"end\":6520,\"start\":6517},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9580,\"start\":9559},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9597,\"start\":9580},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9616,\"start\":9597},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9636,\"start\":9616},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9652,\"start\":9636},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9676,\"start\":9652},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10058,\"start\":10037},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10145,\"start\":10129},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10202,\"start\":10184},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10418,\"start\":10399},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10490,\"start\":10471},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10507,\"start\":10490},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10525,\"start\":10507},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10928,\"start\":10906},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14484,\"start\":14468},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16124,\"start\":16102},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17594,\"start\":17572},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17616,\"start\":17594},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18360,\"start\":18341},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18490,\"start\":18470},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18596,\"start\":18574},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19492,\"start\":19476},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20594,\"start\":20575},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21914,\"start\":21897},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28487,\"start\":28466},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28547,\"start\":28527},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31798,\"start\":31781},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32357,\"start\":32334},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32608,\"start\":32589},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32993,\"start\":32973},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33213,\"start\":33197},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33268,\"start\":33251},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33310,\"start\":33292},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34697,\"start\":34676},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34865,\"start\":34839},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35364,\"start\":35336},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35464,\"start\":35443},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40140,\"start\":40124},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41337,\"start\":41321},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1367,\"start\":1342},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1387,\"start\":1367},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1405,\"start\":1387},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1425,\"start\":1405},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1441,\"start\":1425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2192,\"start\":2171},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2226,\"start\":2210},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3902,\"start\":3886},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5298,\"start\":5276},{\"end\":6520,\"start\":6517},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9580,\"start\":9559},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9597,\"start\":9580},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9616,\"start\":9597},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9636,\"start\":9616},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9652,\"start\":9636},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9676,\"start\":9652},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10058,\"start\":10037},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10145,\"start\":10129},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10202,\"start\":10184},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10418,\"start\":10399},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10490,\"start\":10471},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10507,\"start\":10490},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10525,\"start\":10507},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10928,\"start\":10906},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14484,\"start\":14468},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16124,\"start\":16102},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17594,\"start\":17572},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17616,\"start\":17594},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18360,\"start\":18341},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18490,\"start\":18470},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18596,\"start\":18574},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19492,\"start\":19476},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20594,\"start\":20575},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21914,\"start\":21897},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28487,\"start\":28466},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28547,\"start\":28527},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31798,\"start\":31781},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32357,\"start\":32334},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32608,\"start\":32589},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32993,\"start\":32973},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33213,\"start\":33197},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33268,\"start\":33251},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33310,\"start\":33292},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34697,\"start\":34676},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34865,\"start\":34839},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35364,\"start\":35336},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35464,\"start\":35443},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40140,\"start\":40124},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41337,\"start\":41321}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36803,\"start\":36685},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37177,\"start\":36804},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37394,\"start\":37178},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37728,\"start\":37395},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38118,\"start\":37729},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38335,\"start\":38119},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38863,\"start\":38336},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39032,\"start\":38864},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39972,\"start\":39033},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40999,\"start\":39973},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41169,\"start\":41000},{\"attributes\":{\"id\":\"fig_12\"},\"end\":42737,\"start\":41170},{\"attributes\":{\"id\":\"fig_0\"},\"end\":36803,\"start\":36685},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37177,\"start\":36804},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37394,\"start\":37178},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37728,\"start\":37395},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38118,\"start\":37729},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38335,\"start\":38119},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38863,\"start\":38336},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39032,\"start\":38864},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39972,\"start\":39033},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40999,\"start\":39973},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41169,\"start\":41000},{\"attributes\":{\"id\":\"fig_12\"},\"end\":42737,\"start\":41170}]", "paragraph": "[{\"end\":1542,\"start\":1267},{\"end\":2193,\"start\":1544},{\"end\":2795,\"start\":2195},{\"end\":3444,\"start\":2797},{\"end\":3555,\"start\":3446},{\"end\":3659,\"start\":3557},{\"end\":3782,\"start\":3661},{\"end\":4024,\"start\":3784},{\"end\":4090,\"start\":4026},{\"end\":4947,\"start\":4092},{\"end\":5107,\"start\":4949},{\"end\":5299,\"start\":5109},{\"end\":5758,\"start\":5301},{\"end\":6294,\"start\":5789},{\"end\":6795,\"start\":6319},{\"end\":7431,\"start\":6886},{\"end\":7883,\"start\":7448},{\"end\":8318,\"start\":7885},{\"end\":8896,\"start\":8369},{\"end\":9182,\"start\":8919},{\"end\":10059,\"start\":9199},{\"end\":10672,\"start\":10061},{\"end\":11151,\"start\":10674},{\"end\":11943,\"start\":11196},{\"end\":13059,\"start\":11945},{\"end\":13680,\"start\":13105},{\"end\":14681,\"start\":13710},{\"end\":15419,\"start\":14683},{\"end\":16125,\"start\":15460},{\"end\":16590,\"start\":16127},{\"end\":16778,\"start\":16592},{\"end\":16922,\"start\":16814},{\"end\":17273,\"start\":16950},{\"end\":17617,\"start\":17313},{\"end\":18151,\"start\":17619},{\"end\":18597,\"start\":18191},{\"end\":19000,\"start\":18599},{\"end\":19413,\"start\":19115},{\"end\":20755,\"start\":19415},{\"end\":21377,\"start\":20757},{\"end\":21835,\"start\":21404},{\"end\":22361,\"start\":21837},{\"end\":22979,\"start\":22378},{\"end\":24322,\"start\":22981},{\"end\":25072,\"start\":24351},{\"end\":25987,\"start\":25095},{\"end\":26417,\"start\":25989},{\"end\":27607,\"start\":26419},{\"end\":28012,\"start\":27689},{\"end\":28647,\"start\":28014},{\"end\":29590,\"start\":28649},{\"end\":30395,\"start\":29632},{\"end\":30712,\"start\":30410},{\"end\":31120,\"start\":30714},{\"end\":31679,\"start\":31122},{\"end\":32271,\"start\":31704},{\"end\":32841,\"start\":32318},{\"end\":33108,\"start\":32843},{\"end\":33473,\"start\":33152},{\"end\":33977,\"start\":33475},{\"end\":34241,\"start\":34002},{\"end\":34591,\"start\":34243},{\"end\":34875,\"start\":34593},{\"end\":35106,\"start\":34877},{\"end\":35696,\"start\":35108},{\"end\":36684,\"start\":35698},{\"end\":1542,\"start\":1267},{\"end\":2193,\"start\":1544},{\"end\":2795,\"start\":2195},{\"end\":3444,\"start\":2797},{\"end\":3555,\"start\":3446},{\"end\":3659,\"start\":3557},{\"end\":3782,\"start\":3661},{\"end\":4024,\"start\":3784},{\"end\":4090,\"start\":4026},{\"end\":4947,\"start\":4092},{\"end\":5107,\"start\":4949},{\"end\":5299,\"start\":5109},{\"end\":5758,\"start\":5301},{\"end\":6294,\"start\":5789},{\"end\":6795,\"start\":6319},{\"end\":7431,\"start\":6886},{\"end\":7883,\"start\":7448},{\"end\":8318,\"start\":7885},{\"end\":8896,\"start\":8369},{\"end\":9182,\"start\":8919},{\"end\":10059,\"start\":9199},{\"end\":10672,\"start\":10061},{\"end\":11151,\"start\":10674},{\"end\":11943,\"start\":11196},{\"end\":13059,\"start\":11945},{\"end\":13680,\"start\":13105},{\"end\":14681,\"start\":13710},{\"end\":15419,\"start\":14683},{\"end\":16125,\"start\":15460},{\"end\":16590,\"start\":16127},{\"end\":16778,\"start\":16592},{\"end\":16922,\"start\":16814},{\"end\":17273,\"start\":16950},{\"end\":17617,\"start\":17313},{\"end\":18151,\"start\":17619},{\"end\":18597,\"start\":18191},{\"end\":19000,\"start\":18599},{\"end\":19413,\"start\":19115},{\"end\":20755,\"start\":19415},{\"end\":21377,\"start\":20757},{\"end\":21835,\"start\":21404},{\"end\":22361,\"start\":21837},{\"end\":22979,\"start\":22378},{\"end\":24322,\"start\":22981},{\"end\":25072,\"start\":24351},{\"end\":25987,\"start\":25095},{\"end\":26417,\"start\":25989},{\"end\":27607,\"start\":26419},{\"end\":28012,\"start\":27689},{\"end\":28647,\"start\":28014},{\"end\":29590,\"start\":28649},{\"end\":30395,\"start\":29632},{\"end\":30712,\"start\":30410},{\"end\":31120,\"start\":30714},{\"end\":31679,\"start\":31122},{\"end\":32271,\"start\":31704},{\"end\":32841,\"start\":32318},{\"end\":33108,\"start\":32843},{\"end\":33473,\"start\":33152},{\"end\":33977,\"start\":33475},{\"end\":34241,\"start\":34002},{\"end\":34591,\"start\":34243},{\"end\":34875,\"start\":34593},{\"end\":35106,\"start\":34877},{\"end\":35696,\"start\":35108},{\"end\":36684,\"start\":35698}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6885,\"start\":6796},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8368,\"start\":8319},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16813,\"start\":16779},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16949,\"start\":16923},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18190,\"start\":18152},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19078,\"start\":19001},{\"attributes\":{\"id\":\"formula_0\"},\"end\":6885,\"start\":6796},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8368,\"start\":8319},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16813,\"start\":16779},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16949,\"start\":16923},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18190,\"start\":18152},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19078,\"start\":19001}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1265,\"start\":1253},{\"end\":5775,\"start\":5761},{\"end\":5787,\"start\":5778},{\"attributes\":{\"n\":\"2\"},\"end\":6307,\"start\":6297},{\"attributes\":{\"n\":\"2.1\"},\"end\":6317,\"start\":6310},{\"attributes\":{\"n\":\"2.2\"},\"end\":7446,\"start\":7434},{\"end\":8917,\"start\":8899},{\"attributes\":{\"n\":\"2.3\"},\"end\":9197,\"start\":9185},{\"attributes\":{\"n\":\"3\"},\"end\":11194,\"start\":11154},{\"attributes\":{\"n\":\"4\"},\"end\":13103,\"start\":13062},{\"attributes\":{\"n\":\"4.1\"},\"end\":13708,\"start\":13683},{\"attributes\":{\"n\":\"4.2\"},\"end\":15458,\"start\":15422},{\"attributes\":{\"n\":\"4.3\"},\"end\":17311,\"start\":17276},{\"attributes\":{\"n\":\"4.4\"},\"end\":19113,\"start\":19080},{\"attributes\":{\"n\":\"5\"},\"end\":21402,\"start\":21380},{\"attributes\":{\"n\":\"5.1\"},\"end\":22376,\"start\":22364},{\"attributes\":{\"n\":\"5.2\"},\"end\":24349,\"start\":24325},{\"end\":25093,\"start\":25075},{\"attributes\":{\"n\":\"6.1\"},\"end\":27687,\"start\":27610},{\"attributes\":{\"n\":\"6.2\"},\"end\":29630,\"start\":29593},{\"attributes\":{\"n\":\"7\"},\"end\":30408,\"start\":30398},{\"end\":31702,\"start\":31682},{\"end\":32316,\"start\":32274},{\"end\":33150,\"start\":33111},{\"end\":34000,\"start\":33980},{\"end\":36696,\"start\":36686},{\"end\":36815,\"start\":36805},{\"end\":37189,\"start\":37179},{\"end\":37406,\"start\":37396},{\"end\":37740,\"start\":37730},{\"end\":38130,\"start\":38120},{\"end\":38348,\"start\":38337},{\"end\":38876,\"start\":38865},{\"end\":39056,\"start\":39034},{\"end\":40007,\"start\":39974},{\"end\":41012,\"start\":41001},{\"end\":41215,\"start\":41171},{\"attributes\":{\"n\":\"1\"},\"end\":1265,\"start\":1253},{\"end\":5775,\"start\":5761},{\"end\":5787,\"start\":5778},{\"attributes\":{\"n\":\"2\"},\"end\":6307,\"start\":6297},{\"attributes\":{\"n\":\"2.1\"},\"end\":6317,\"start\":6310},{\"attributes\":{\"n\":\"2.2\"},\"end\":7446,\"start\":7434},{\"end\":8917,\"start\":8899},{\"attributes\":{\"n\":\"2.3\"},\"end\":9197,\"start\":9185},{\"attributes\":{\"n\":\"3\"},\"end\":11194,\"start\":11154},{\"attributes\":{\"n\":\"4\"},\"end\":13103,\"start\":13062},{\"attributes\":{\"n\":\"4.1\"},\"end\":13708,\"start\":13683},{\"attributes\":{\"n\":\"4.2\"},\"end\":15458,\"start\":15422},{\"attributes\":{\"n\":\"4.3\"},\"end\":17311,\"start\":17276},{\"attributes\":{\"n\":\"4.4\"},\"end\":19113,\"start\":19080},{\"attributes\":{\"n\":\"5\"},\"end\":21402,\"start\":21380},{\"attributes\":{\"n\":\"5.1\"},\"end\":22376,\"start\":22364},{\"attributes\":{\"n\":\"5.2\"},\"end\":24349,\"start\":24325},{\"end\":25093,\"start\":25075},{\"attributes\":{\"n\":\"6.1\"},\"end\":27687,\"start\":27610},{\"attributes\":{\"n\":\"6.2\"},\"end\":29630,\"start\":29593},{\"attributes\":{\"n\":\"7\"},\"end\":30408,\"start\":30398},{\"end\":31702,\"start\":31682},{\"end\":32316,\"start\":32274},{\"end\":33150,\"start\":33111},{\"end\":34000,\"start\":33980},{\"end\":36696,\"start\":36686},{\"end\":36815,\"start\":36805},{\"end\":37189,\"start\":37179},{\"end\":37406,\"start\":37396},{\"end\":37740,\"start\":37730},{\"end\":38130,\"start\":38120},{\"end\":38348,\"start\":38337},{\"end\":38876,\"start\":38865},{\"end\":39056,\"start\":39034},{\"end\":40007,\"start\":39974},{\"end\":41012,\"start\":41001},{\"end\":41215,\"start\":41171}]", "table": null, "figure_caption": "[{\"end\":36803,\"start\":36698},{\"end\":37177,\"start\":36817},{\"end\":37394,\"start\":37191},{\"end\":37728,\"start\":37408},{\"end\":38118,\"start\":37742},{\"end\":38335,\"start\":38132},{\"end\":38863,\"start\":38351},{\"end\":39032,\"start\":38879},{\"end\":39972,\"start\":39061},{\"end\":40999,\"start\":40014},{\"end\":41169,\"start\":41015},{\"end\":42737,\"start\":41224},{\"end\":36803,\"start\":36698},{\"end\":37177,\"start\":36817},{\"end\":37394,\"start\":37191},{\"end\":37728,\"start\":37408},{\"end\":38118,\"start\":37742},{\"end\":38335,\"start\":38132},{\"end\":38863,\"start\":38351},{\"end\":39032,\"start\":38879},{\"end\":39972,\"start\":39061},{\"end\":40999,\"start\":40014},{\"end\":41169,\"start\":41015},{\"end\":42737,\"start\":41224}]", "figure_ref": "[{\"end\":3134,\"start\":3126},{\"end\":3820,\"start\":3812},{\"end\":5398,\"start\":5390},{\"end\":5815,\"start\":5807},{\"end\":6167,\"start\":6158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12916,\"start\":12907},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13057,\"start\":13043},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13917,\"start\":13908},{\"end\":14457,\"start\":14448},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17018,\"start\":17009},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18831,\"start\":18823},{\"end\":19652,\"start\":19644},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20119,\"start\":20110},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21362,\"start\":21353},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22852,\"start\":22842},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23692,\"start\":23682},{\"end\":23722,\"start\":23708},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23836,\"start\":23827},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23893,\"start\":23884},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24534,\"start\":24525},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25228,\"start\":25218},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25768,\"start\":25758},{\"end\":26504,\"start\":26494},{\"end\":26564,\"start\":26553},{\"end\":27040,\"start\":27031},{\"end\":27052,\"start\":27044},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27433,\"start\":27424},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27563,\"start\":27553},{\"end\":28508,\"start\":28499},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28852,\"start\":28841},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29301,\"start\":29292},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29778,\"start\":29767},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33487,\"start\":33478},{\"end\":33974,\"start\":33965},{\"end\":34927,\"start\":34917},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35105,\"start\":35096},{\"end\":35756,\"start\":35746},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35932,\"start\":35922},{\"end\":36134,\"start\":36125},{\"end\":3134,\"start\":3126},{\"end\":3820,\"start\":3812},{\"end\":5398,\"start\":5390},{\"end\":5815,\"start\":5807},{\"end\":6167,\"start\":6158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12916,\"start\":12907},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13057,\"start\":13043},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13917,\"start\":13908},{\"end\":14457,\"start\":14448},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17018,\"start\":17009},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18831,\"start\":18823},{\"end\":19652,\"start\":19644},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20119,\"start\":20110},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21362,\"start\":21353},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22852,\"start\":22842},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23692,\"start\":23682},{\"end\":23722,\"start\":23708},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23836,\"start\":23827},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23893,\"start\":23884},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24534,\"start\":24525},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25228,\"start\":25218},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25768,\"start\":25758},{\"end\":26504,\"start\":26494},{\"end\":26564,\"start\":26553},{\"end\":27040,\"start\":27031},{\"end\":27052,\"start\":27044},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27433,\"start\":27424},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27563,\"start\":27553},{\"end\":28508,\"start\":28499},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28852,\"start\":28841},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29301,\"start\":29292},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29778,\"start\":29767},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33487,\"start\":33478},{\"end\":33974,\"start\":33965},{\"end\":34927,\"start\":34917},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35105,\"start\":35096},{\"end\":35756,\"start\":35746},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35932,\"start\":35922},{\"end\":36134,\"start\":36125}]", "bib_author_first_name": "[{\"end\":43203,\"start\":43197},{\"end\":43221,\"start\":43214},{\"end\":43236,\"start\":43232},{\"end\":43460,\"start\":43452},{\"end\":43475,\"start\":43469},{\"end\":43489,\"start\":43484},{\"end\":43704,\"start\":43698},{\"end\":43716,\"start\":43711},{\"end\":43735,\"start\":43727},{\"end\":43752,\"start\":43747},{\"end\":43769,\"start\":43761},{\"end\":43786,\"start\":43779},{\"end\":43795,\"start\":43792},{\"end\":43810,\"start\":43804},{\"end\":44105,\"start\":44099},{\"end\":44117,\"start\":44112},{\"end\":44125,\"start\":44123},{\"end\":44138,\"start\":44130},{\"end\":44147,\"start\":44143},{\"end\":44430,\"start\":44429},{\"end\":44446,\"start\":44445},{\"end\":44462,\"start\":44455},{\"end\":44476,\"start\":44472},{\"end\":44478,\"start\":44477},{\"end\":44747,\"start\":44744},{\"end\":44764,\"start\":44760},{\"end\":44785,\"start\":44780},{\"end\":44797,\"start\":44793},{\"end\":44807,\"start\":44802},{\"end\":44829,\"start\":44822},{\"end\":44842,\"start\":44837},{\"end\":44860,\"start\":44854},{\"end\":45195,\"start\":45191},{\"end\":45211,\"start\":45204},{\"end\":45234,\"start\":45226},{\"end\":45529,\"start\":45523},{\"end\":45541,\"start\":45534},{\"end\":45565,\"start\":45556},{\"end\":45902,\"start\":45896},{\"end\":45919,\"start\":45914},{\"end\":45933,\"start\":45927},{\"end\":45951,\"start\":45944},{\"end\":45969,\"start\":45962},{\"end\":46270,\"start\":46263},{\"end\":46282,\"start\":46275},{\"end\":46298,\"start\":46290},{\"end\":46308,\"start\":46304},{\"end\":46630,\"start\":46624},{\"end\":46642,\"start\":46638},{\"end\":46656,\"start\":46650},{\"end\":46677,\"start\":46665},{\"end\":46702,\"start\":46690},{\"end\":47015,\"start\":47009},{\"end\":47030,\"start\":47023},{\"end\":47047,\"start\":47042},{\"end\":47065,\"start\":47058},{\"end\":47082,\"start\":47072},{\"end\":47339,\"start\":47338},{\"end\":47353,\"start\":47350},{\"end\":47636,\"start\":47633},{\"end\":47648,\"start\":47643},{\"end\":47833,\"start\":47829},{\"end\":48031,\"start\":48027},{\"end\":48048,\"start\":48044},{\"end\":48068,\"start\":48060},{\"end\":48070,\"start\":48069},{\"end\":48413,\"start\":48409},{\"end\":48426,\"start\":48419},{\"end\":48450,\"start\":48441},{\"end\":48817,\"start\":48807},{\"end\":48835,\"start\":48825},{\"end\":48851,\"start\":48845},{\"end\":48869,\"start\":48861},{\"end\":48885,\"start\":48879},{\"end\":49268,\"start\":49263},{\"end\":49290,\"start\":49282},{\"end\":49523,\"start\":49518},{\"end\":49545,\"start\":49537},{\"end\":49816,\"start\":49811},{\"end\":49839,\"start\":49830},{\"end\":49841,\"start\":49840},{\"end\":49860,\"start\":49852},{\"end\":50196,\"start\":50191},{\"end\":50219,\"start\":50210},{\"end\":50221,\"start\":50220},{\"end\":50240,\"start\":50232},{\"end\":50625,\"start\":50620},{\"end\":50638,\"start\":50631},{\"end\":50859,\"start\":50850},{\"end\":50871,\"start\":50866},{\"end\":50890,\"start\":50885},{\"end\":50905,\"start\":50899},{\"end\":50907,\"start\":50906},{\"end\":50918,\"start\":50914},{\"end\":50928,\"start\":50927},{\"end\":50939,\"start\":50935},{\"end\":50957,\"start\":50951},{\"end\":50973,\"start\":50966},{\"end\":50975,\"start\":50974},{\"end\":50993,\"start\":50988},{\"end\":51356,\"start\":51350},{\"end\":51370,\"start\":51365},{\"end\":51388,\"start\":51382},{\"end\":51404,\"start\":51396},{\"end\":51934,\"start\":51927},{\"end\":51952,\"start\":51945},{\"end\":51966,\"start\":51963},{\"end\":52309,\"start\":52305},{\"end\":52323,\"start\":52319},{\"end\":52337,\"start\":52330},{\"end\":52676,\"start\":52672},{\"end\":52693,\"start\":52690},{\"end\":52703,\"start\":52700},{\"end\":52716,\"start\":52708},{\"end\":52732,\"start\":52725},{\"end\":52747,\"start\":52743},{\"end\":52759,\"start\":52752},{\"end\":52773,\"start\":52767},{\"end\":52790,\"start\":52784},{\"end\":52806,\"start\":52799},{\"end\":52827,\"start\":52818},{\"end\":52829,\"start\":52828},{\"end\":52838,\"start\":52836},{\"end\":53260,\"start\":53257},{\"end\":53275,\"start\":53270},{\"end\":53289,\"start\":53283},{\"end\":53306,\"start\":53298},{\"end\":53323,\"start\":53314},{\"end\":53337,\"start\":53332},{\"end\":53351,\"start\":53348},{\"end\":53753,\"start\":53748},{\"end\":53765,\"start\":53762},{\"end\":53778,\"start\":53773},{\"end\":53780,\"start\":53779},{\"end\":53797,\"start\":53791},{\"end\":53811,\"start\":53804},{\"end\":53825,\"start\":53819},{\"end\":53841,\"start\":53835},{\"end\":53860,\"start\":53853},{\"end\":53880,\"start\":53876},{\"end\":53897,\"start\":53893},{\"end\":54271,\"start\":54266},{\"end\":54288,\"start\":54282},{\"end\":54589,\"start\":54584},{\"end\":54606,\"start\":54602},{\"end\":54625,\"start\":54618},{\"end\":54811,\"start\":54807},{\"end\":54828,\"start\":54823},{\"end\":54844,\"start\":54838},{\"end\":55130,\"start\":55121},{\"end\":55148,\"start\":55140},{\"end\":55162,\"start\":55158},{\"end\":55178,\"start\":55174},{\"end\":55193,\"start\":55186},{\"end\":55204,\"start\":55201},{\"end\":55220,\"start\":55217},{\"end\":55563,\"start\":55556},{\"end\":55575,\"start\":55570},{\"end\":55590,\"start\":55580},{\"end\":55866,\"start\":55858},{\"end\":55883,\"start\":55876},{\"end\":55900,\"start\":55895},{\"end\":55920,\"start\":55911},{\"end\":55939,\"start\":55929},{\"end\":56298,\"start\":56293},{\"end\":56313,\"start\":56305},{\"end\":56324,\"start\":56319},{\"end\":56338,\"start\":56331},{\"end\":56348,\"start\":56343},{\"end\":56366,\"start\":56360},{\"end\":56375,\"start\":56374},{\"end\":56799,\"start\":56796},{\"end\":56810,\"start\":56806},{\"end\":56825,\"start\":56818},{\"end\":57038,\"start\":57033},{\"end\":57053,\"start\":57045},{\"end\":57068,\"start\":57062},{\"end\":57080,\"start\":57077},{\"end\":57094,\"start\":57087},{\"end\":57108,\"start\":57103},{\"end\":43203,\"start\":43197},{\"end\":43221,\"start\":43214},{\"end\":43236,\"start\":43232},{\"end\":43460,\"start\":43452},{\"end\":43475,\"start\":43469},{\"end\":43489,\"start\":43484},{\"end\":43704,\"start\":43698},{\"end\":43716,\"start\":43711},{\"end\":43735,\"start\":43727},{\"end\":43752,\"start\":43747},{\"end\":43769,\"start\":43761},{\"end\":43786,\"start\":43779},{\"end\":43795,\"start\":43792},{\"end\":43810,\"start\":43804},{\"end\":44105,\"start\":44099},{\"end\":44117,\"start\":44112},{\"end\":44125,\"start\":44123},{\"end\":44138,\"start\":44130},{\"end\":44147,\"start\":44143},{\"end\":44430,\"start\":44429},{\"end\":44446,\"start\":44445},{\"end\":44462,\"start\":44455},{\"end\":44476,\"start\":44472},{\"end\":44478,\"start\":44477},{\"end\":44747,\"start\":44744},{\"end\":44764,\"start\":44760},{\"end\":44785,\"start\":44780},{\"end\":44797,\"start\":44793},{\"end\":44807,\"start\":44802},{\"end\":44829,\"start\":44822},{\"end\":44842,\"start\":44837},{\"end\":44860,\"start\":44854},{\"end\":45195,\"start\":45191},{\"end\":45211,\"start\":45204},{\"end\":45234,\"start\":45226},{\"end\":45529,\"start\":45523},{\"end\":45541,\"start\":45534},{\"end\":45565,\"start\":45556},{\"end\":45902,\"start\":45896},{\"end\":45919,\"start\":45914},{\"end\":45933,\"start\":45927},{\"end\":45951,\"start\":45944},{\"end\":45969,\"start\":45962},{\"end\":46270,\"start\":46263},{\"end\":46282,\"start\":46275},{\"end\":46298,\"start\":46290},{\"end\":46308,\"start\":46304},{\"end\":46630,\"start\":46624},{\"end\":46642,\"start\":46638},{\"end\":46656,\"start\":46650},{\"end\":46677,\"start\":46665},{\"end\":46702,\"start\":46690},{\"end\":47015,\"start\":47009},{\"end\":47030,\"start\":47023},{\"end\":47047,\"start\":47042},{\"end\":47065,\"start\":47058},{\"end\":47082,\"start\":47072},{\"end\":47339,\"start\":47338},{\"end\":47353,\"start\":47350},{\"end\":47636,\"start\":47633},{\"end\":47648,\"start\":47643},{\"end\":47833,\"start\":47829},{\"end\":48031,\"start\":48027},{\"end\":48048,\"start\":48044},{\"end\":48068,\"start\":48060},{\"end\":48070,\"start\":48069},{\"end\":48413,\"start\":48409},{\"end\":48426,\"start\":48419},{\"end\":48450,\"start\":48441},{\"end\":48817,\"start\":48807},{\"end\":48835,\"start\":48825},{\"end\":48851,\"start\":48845},{\"end\":48869,\"start\":48861},{\"end\":48885,\"start\":48879},{\"end\":49268,\"start\":49263},{\"end\":49290,\"start\":49282},{\"end\":49523,\"start\":49518},{\"end\":49545,\"start\":49537},{\"end\":49816,\"start\":49811},{\"end\":49839,\"start\":49830},{\"end\":49841,\"start\":49840},{\"end\":49860,\"start\":49852},{\"end\":50196,\"start\":50191},{\"end\":50219,\"start\":50210},{\"end\":50221,\"start\":50220},{\"end\":50240,\"start\":50232},{\"end\":50625,\"start\":50620},{\"end\":50638,\"start\":50631},{\"end\":50859,\"start\":50850},{\"end\":50871,\"start\":50866},{\"end\":50890,\"start\":50885},{\"end\":50905,\"start\":50899},{\"end\":50907,\"start\":50906},{\"end\":50918,\"start\":50914},{\"end\":50928,\"start\":50927},{\"end\":50939,\"start\":50935},{\"end\":50957,\"start\":50951},{\"end\":50973,\"start\":50966},{\"end\":50975,\"start\":50974},{\"end\":50993,\"start\":50988},{\"end\":51356,\"start\":51350},{\"end\":51370,\"start\":51365},{\"end\":51388,\"start\":51382},{\"end\":51404,\"start\":51396},{\"end\":51934,\"start\":51927},{\"end\":51952,\"start\":51945},{\"end\":51966,\"start\":51963},{\"end\":52309,\"start\":52305},{\"end\":52323,\"start\":52319},{\"end\":52337,\"start\":52330},{\"end\":52676,\"start\":52672},{\"end\":52693,\"start\":52690},{\"end\":52703,\"start\":52700},{\"end\":52716,\"start\":52708},{\"end\":52732,\"start\":52725},{\"end\":52747,\"start\":52743},{\"end\":52759,\"start\":52752},{\"end\":52773,\"start\":52767},{\"end\":52790,\"start\":52784},{\"end\":52806,\"start\":52799},{\"end\":52827,\"start\":52818},{\"end\":52829,\"start\":52828},{\"end\":52838,\"start\":52836},{\"end\":53260,\"start\":53257},{\"end\":53275,\"start\":53270},{\"end\":53289,\"start\":53283},{\"end\":53306,\"start\":53298},{\"end\":53323,\"start\":53314},{\"end\":53337,\"start\":53332},{\"end\":53351,\"start\":53348},{\"end\":53753,\"start\":53748},{\"end\":53765,\"start\":53762},{\"end\":53778,\"start\":53773},{\"end\":53780,\"start\":53779},{\"end\":53797,\"start\":53791},{\"end\":53811,\"start\":53804},{\"end\":53825,\"start\":53819},{\"end\":53841,\"start\":53835},{\"end\":53860,\"start\":53853},{\"end\":53880,\"start\":53876},{\"end\":53897,\"start\":53893},{\"end\":54271,\"start\":54266},{\"end\":54288,\"start\":54282},{\"end\":54589,\"start\":54584},{\"end\":54606,\"start\":54602},{\"end\":54625,\"start\":54618},{\"end\":54811,\"start\":54807},{\"end\":54828,\"start\":54823},{\"end\":54844,\"start\":54838},{\"end\":55130,\"start\":55121},{\"end\":55148,\"start\":55140},{\"end\":55162,\"start\":55158},{\"end\":55178,\"start\":55174},{\"end\":55193,\"start\":55186},{\"end\":55204,\"start\":55201},{\"end\":55220,\"start\":55217},{\"end\":55563,\"start\":55556},{\"end\":55575,\"start\":55570},{\"end\":55590,\"start\":55580},{\"end\":55866,\"start\":55858},{\"end\":55883,\"start\":55876},{\"end\":55900,\"start\":55895},{\"end\":55920,\"start\":55911},{\"end\":55939,\"start\":55929},{\"end\":56298,\"start\":56293},{\"end\":56313,\"start\":56305},{\"end\":56324,\"start\":56319},{\"end\":56338,\"start\":56331},{\"end\":56348,\"start\":56343},{\"end\":56366,\"start\":56360},{\"end\":56375,\"start\":56374},{\"end\":56799,\"start\":56796},{\"end\":56810,\"start\":56806},{\"end\":56825,\"start\":56818},{\"end\":57038,\"start\":57033},{\"end\":57053,\"start\":57045},{\"end\":57068,\"start\":57062},{\"end\":57080,\"start\":57077},{\"end\":57094,\"start\":57087},{\"end\":57108,\"start\":57103}]", "bib_author_last_name": "[{\"end\":43212,\"start\":43204},{\"end\":43230,\"start\":43222},{\"end\":43243,\"start\":43237},{\"end\":43467,\"start\":43461},{\"end\":43482,\"start\":43476},{\"end\":43496,\"start\":43490},{\"end\":43709,\"start\":43705},{\"end\":43725,\"start\":43717},{\"end\":43745,\"start\":43736},{\"end\":43759,\"start\":43753},{\"end\":43777,\"start\":43770},{\"end\":43790,\"start\":43787},{\"end\":43802,\"start\":43796},{\"end\":43821,\"start\":43811},{\"end\":44110,\"start\":44106},{\"end\":44121,\"start\":44118},{\"end\":44128,\"start\":44126},{\"end\":44141,\"start\":44139},{\"end\":44152,\"start\":44148},{\"end\":44435,\"start\":44431},{\"end\":44443,\"start\":44437},{\"end\":44453,\"start\":44447},{\"end\":44470,\"start\":44463},{\"end\":44487,\"start\":44479},{\"end\":44496,\"start\":44489},{\"end\":44758,\"start\":44748},{\"end\":44778,\"start\":44765},{\"end\":44791,\"start\":44786},{\"end\":44800,\"start\":44798},{\"end\":44820,\"start\":44808},{\"end\":44835,\"start\":44830},{\"end\":44852,\"start\":44843},{\"end\":44867,\"start\":44861},{\"end\":45202,\"start\":45196},{\"end\":45224,\"start\":45212},{\"end\":45241,\"start\":45235},{\"end\":45532,\"start\":45530},{\"end\":45554,\"start\":45542},{\"end\":45570,\"start\":45566},{\"end\":45912,\"start\":45903},{\"end\":45925,\"start\":45920},{\"end\":45942,\"start\":45934},{\"end\":45960,\"start\":45952},{\"end\":45979,\"start\":45970},{\"end\":46273,\"start\":46271},{\"end\":46288,\"start\":46283},{\"end\":46302,\"start\":46299},{\"end\":46312,\"start\":46309},{\"end\":46636,\"start\":46631},{\"end\":46648,\"start\":46643},{\"end\":46663,\"start\":46657},{\"end\":46688,\"start\":46678},{\"end\":46710,\"start\":46703},{\"end\":47021,\"start\":47016},{\"end\":47040,\"start\":47031},{\"end\":47056,\"start\":47048},{\"end\":47070,\"start\":47066},{\"end\":47088,\"start\":47083},{\"end\":47348,\"start\":47340},{\"end\":47360,\"start\":47354},{\"end\":47369,\"start\":47362},{\"end\":47641,\"start\":47637},{\"end\":47652,\"start\":47649},{\"end\":47659,\"start\":47654},{\"end\":47844,\"start\":47834},{\"end\":48042,\"start\":48032},{\"end\":48058,\"start\":48049},{\"end\":48077,\"start\":48071},{\"end\":48417,\"start\":48414},{\"end\":48439,\"start\":48427},{\"end\":48455,\"start\":48451},{\"end\":48823,\"start\":48818},{\"end\":48843,\"start\":48836},{\"end\":48859,\"start\":48852},{\"end\":48877,\"start\":48870},{\"end\":48891,\"start\":48886},{\"end\":49280,\"start\":49269},{\"end\":49299,\"start\":49291},{\"end\":49535,\"start\":49524},{\"end\":49554,\"start\":49546},{\"end\":49828,\"start\":49817},{\"end\":49850,\"start\":49842},{\"end\":49869,\"start\":49861},{\"end\":50208,\"start\":50197},{\"end\":50230,\"start\":50222},{\"end\":50249,\"start\":50241},{\"end\":50629,\"start\":50626},{\"end\":50642,\"start\":50639},{\"end\":50864,\"start\":50860},{\"end\":50883,\"start\":50872},{\"end\":50897,\"start\":50891},{\"end\":50912,\"start\":50908},{\"end\":50925,\"start\":50919},{\"end\":50933,\"start\":50929},{\"end\":50949,\"start\":50940},{\"end\":50964,\"start\":50958},{\"end\":50986,\"start\":50976},{\"end\":51003,\"start\":50994},{\"end\":51014,\"start\":51005},{\"end\":51363,\"start\":51357},{\"end\":51380,\"start\":51371},{\"end\":51394,\"start\":51389},{\"end\":51416,\"start\":51405},{\"end\":51943,\"start\":51935},{\"end\":51961,\"start\":51953},{\"end\":51977,\"start\":51967},{\"end\":52317,\"start\":52310},{\"end\":52328,\"start\":52324},{\"end\":52346,\"start\":52338},{\"end\":52688,\"start\":52677},{\"end\":52698,\"start\":52694},{\"end\":52706,\"start\":52704},{\"end\":52723,\"start\":52717},{\"end\":52741,\"start\":52733},{\"end\":52750,\"start\":52748},{\"end\":52765,\"start\":52760},{\"end\":52782,\"start\":52774},{\"end\":52797,\"start\":52791},{\"end\":52816,\"start\":52807},{\"end\":52834,\"start\":52830},{\"end\":52846,\"start\":52839},{\"end\":53268,\"start\":53261},{\"end\":53281,\"start\":53276},{\"end\":53296,\"start\":53290},{\"end\":53312,\"start\":53307},{\"end\":53330,\"start\":53324},{\"end\":53346,\"start\":53338},{\"end\":53361,\"start\":53352},{\"end\":53760,\"start\":53754},{\"end\":53771,\"start\":53766},{\"end\":53789,\"start\":53781},{\"end\":53802,\"start\":53798},{\"end\":53817,\"start\":53812},{\"end\":53833,\"start\":53826},{\"end\":53851,\"start\":53842},{\"end\":53874,\"start\":53861},{\"end\":53891,\"start\":53881},{\"end\":53912,\"start\":53898},{\"end\":53921,\"start\":53914},{\"end\":54280,\"start\":54272},{\"end\":54298,\"start\":54289},{\"end\":54600,\"start\":54590},{\"end\":54616,\"start\":54607},{\"end\":54631,\"start\":54626},{\"end\":54821,\"start\":54812},{\"end\":54836,\"start\":54829},{\"end\":54847,\"start\":54845},{\"end\":55138,\"start\":55131},{\"end\":55156,\"start\":55149},{\"end\":55172,\"start\":55163},{\"end\":55184,\"start\":55179},{\"end\":55199,\"start\":55194},{\"end\":55215,\"start\":55205},{\"end\":55227,\"start\":55221},{\"end\":55568,\"start\":55564},{\"end\":55578,\"start\":55576},{\"end\":55596,\"start\":55591},{\"end\":55874,\"start\":55867},{\"end\":55893,\"start\":55884},{\"end\":55909,\"start\":55901},{\"end\":55927,\"start\":55921},{\"end\":55945,\"start\":55940},{\"end\":56303,\"start\":56299},{\"end\":56317,\"start\":56314},{\"end\":56329,\"start\":56325},{\"end\":56341,\"start\":56339},{\"end\":56358,\"start\":56349},{\"end\":56372,\"start\":56367},{\"end\":56379,\"start\":56376},{\"end\":56385,\"start\":56381},{\"end\":56804,\"start\":56800},{\"end\":56816,\"start\":56811},{\"end\":56832,\"start\":56826},{\"end\":57043,\"start\":57039},{\"end\":57060,\"start\":57054},{\"end\":57075,\"start\":57069},{\"end\":57085,\"start\":57081},{\"end\":57101,\"start\":57095},{\"end\":57113,\"start\":57109},{\"end\":43212,\"start\":43204},{\"end\":43230,\"start\":43222},{\"end\":43243,\"start\":43237},{\"end\":43467,\"start\":43461},{\"end\":43482,\"start\":43476},{\"end\":43496,\"start\":43490},{\"end\":43709,\"start\":43705},{\"end\":43725,\"start\":43717},{\"end\":43745,\"start\":43736},{\"end\":43759,\"start\":43753},{\"end\":43777,\"start\":43770},{\"end\":43790,\"start\":43787},{\"end\":43802,\"start\":43796},{\"end\":43821,\"start\":43811},{\"end\":44110,\"start\":44106},{\"end\":44121,\"start\":44118},{\"end\":44128,\"start\":44126},{\"end\":44141,\"start\":44139},{\"end\":44152,\"start\":44148},{\"end\":44435,\"start\":44431},{\"end\":44443,\"start\":44437},{\"end\":44453,\"start\":44447},{\"end\":44470,\"start\":44463},{\"end\":44487,\"start\":44479},{\"end\":44496,\"start\":44489},{\"end\":44758,\"start\":44748},{\"end\":44778,\"start\":44765},{\"end\":44791,\"start\":44786},{\"end\":44800,\"start\":44798},{\"end\":44820,\"start\":44808},{\"end\":44835,\"start\":44830},{\"end\":44852,\"start\":44843},{\"end\":44867,\"start\":44861},{\"end\":45202,\"start\":45196},{\"end\":45224,\"start\":45212},{\"end\":45241,\"start\":45235},{\"end\":45532,\"start\":45530},{\"end\":45554,\"start\":45542},{\"end\":45570,\"start\":45566},{\"end\":45912,\"start\":45903},{\"end\":45925,\"start\":45920},{\"end\":45942,\"start\":45934},{\"end\":45960,\"start\":45952},{\"end\":45979,\"start\":45970},{\"end\":46273,\"start\":46271},{\"end\":46288,\"start\":46283},{\"end\":46302,\"start\":46299},{\"end\":46312,\"start\":46309},{\"end\":46636,\"start\":46631},{\"end\":46648,\"start\":46643},{\"end\":46663,\"start\":46657},{\"end\":46688,\"start\":46678},{\"end\":46710,\"start\":46703},{\"end\":47021,\"start\":47016},{\"end\":47040,\"start\":47031},{\"end\":47056,\"start\":47048},{\"end\":47070,\"start\":47066},{\"end\":47088,\"start\":47083},{\"end\":47348,\"start\":47340},{\"end\":47360,\"start\":47354},{\"end\":47369,\"start\":47362},{\"end\":47641,\"start\":47637},{\"end\":47652,\"start\":47649},{\"end\":47659,\"start\":47654},{\"end\":47844,\"start\":47834},{\"end\":48042,\"start\":48032},{\"end\":48058,\"start\":48049},{\"end\":48077,\"start\":48071},{\"end\":48417,\"start\":48414},{\"end\":48439,\"start\":48427},{\"end\":48455,\"start\":48451},{\"end\":48823,\"start\":48818},{\"end\":48843,\"start\":48836},{\"end\":48859,\"start\":48852},{\"end\":48877,\"start\":48870},{\"end\":48891,\"start\":48886},{\"end\":49280,\"start\":49269},{\"end\":49299,\"start\":49291},{\"end\":49535,\"start\":49524},{\"end\":49554,\"start\":49546},{\"end\":49828,\"start\":49817},{\"end\":49850,\"start\":49842},{\"end\":49869,\"start\":49861},{\"end\":50208,\"start\":50197},{\"end\":50230,\"start\":50222},{\"end\":50249,\"start\":50241},{\"end\":50629,\"start\":50626},{\"end\":50642,\"start\":50639},{\"end\":50864,\"start\":50860},{\"end\":50883,\"start\":50872},{\"end\":50897,\"start\":50891},{\"end\":50912,\"start\":50908},{\"end\":50925,\"start\":50919},{\"end\":50933,\"start\":50929},{\"end\":50949,\"start\":50940},{\"end\":50964,\"start\":50958},{\"end\":50986,\"start\":50976},{\"end\":51003,\"start\":50994},{\"end\":51014,\"start\":51005},{\"end\":51363,\"start\":51357},{\"end\":51380,\"start\":51371},{\"end\":51394,\"start\":51389},{\"end\":51416,\"start\":51405},{\"end\":51943,\"start\":51935},{\"end\":51961,\"start\":51953},{\"end\":51977,\"start\":51967},{\"end\":52317,\"start\":52310},{\"end\":52328,\"start\":52324},{\"end\":52346,\"start\":52338},{\"end\":52688,\"start\":52677},{\"end\":52698,\"start\":52694},{\"end\":52706,\"start\":52704},{\"end\":52723,\"start\":52717},{\"end\":52741,\"start\":52733},{\"end\":52750,\"start\":52748},{\"end\":52765,\"start\":52760},{\"end\":52782,\"start\":52774},{\"end\":52797,\"start\":52791},{\"end\":52816,\"start\":52807},{\"end\":52834,\"start\":52830},{\"end\":52846,\"start\":52839},{\"end\":53268,\"start\":53261},{\"end\":53281,\"start\":53276},{\"end\":53296,\"start\":53290},{\"end\":53312,\"start\":53307},{\"end\":53330,\"start\":53324},{\"end\":53346,\"start\":53338},{\"end\":53361,\"start\":53352},{\"end\":53760,\"start\":53754},{\"end\":53771,\"start\":53766},{\"end\":53789,\"start\":53781},{\"end\":53802,\"start\":53798},{\"end\":53817,\"start\":53812},{\"end\":53833,\"start\":53826},{\"end\":53851,\"start\":53842},{\"end\":53874,\"start\":53861},{\"end\":53891,\"start\":53881},{\"end\":53912,\"start\":53898},{\"end\":53921,\"start\":53914},{\"end\":54280,\"start\":54272},{\"end\":54298,\"start\":54289},{\"end\":54600,\"start\":54590},{\"end\":54616,\"start\":54607},{\"end\":54631,\"start\":54626},{\"end\":54821,\"start\":54812},{\"end\":54836,\"start\":54829},{\"end\":54847,\"start\":54845},{\"end\":55138,\"start\":55131},{\"end\":55156,\"start\":55149},{\"end\":55172,\"start\":55163},{\"end\":55184,\"start\":55179},{\"end\":55199,\"start\":55194},{\"end\":55215,\"start\":55205},{\"end\":55227,\"start\":55221},{\"end\":55568,\"start\":55564},{\"end\":55578,\"start\":55576},{\"end\":55596,\"start\":55591},{\"end\":55874,\"start\":55867},{\"end\":55893,\"start\":55884},{\"end\":55909,\"start\":55901},{\"end\":55927,\"start\":55921},{\"end\":55945,\"start\":55940},{\"end\":56303,\"start\":56299},{\"end\":56317,\"start\":56314},{\"end\":56329,\"start\":56325},{\"end\":56341,\"start\":56339},{\"end\":56358,\"start\":56349},{\"end\":56372,\"start\":56367},{\"end\":56379,\"start\":56376},{\"end\":56385,\"start\":56381},{\"end\":56804,\"start\":56800},{\"end\":56816,\"start\":56811},{\"end\":56832,\"start\":56826},{\"end\":57043,\"start\":57039},{\"end\":57060,\"start\":57054},{\"end\":57075,\"start\":57069},{\"end\":57085,\"start\":57081},{\"end\":57101,\"start\":57095},{\"end\":57113,\"start\":57109}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b0\"},\"end\":43399,\"start\":43195},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9089716},\"end\":43619,\"start\":43401},{\"attributes\":{\"doi\":\"arXiv:1811.03728\",\"id\":\"b2\"},\"end\":44097,\"start\":43621},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b3\"},\"end\":44427,\"start\":44099},{\"attributes\":{\"doi\":\"arXiv:1810.03505\",\"id\":\"b4\"},\"end\":44713,\"start\":44429},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1033682},\"end\":45133,\"start\":44715},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206741496},\"end\":45521,\"start\":45135},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b7\"},\"end\":45855,\"start\":45523},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10894094},\"end\":46215,\"start\":45857},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":46547,\"start\":46217},{\"attributes\":{\"doi\":\"arXiv:1712.09196\",\"id\":\"b10\"},\"end\":46953,\"start\":46549},{\"attributes\":{\"doi\":\"arXiv:1905.02175\",\"id\":\"b11\"},\"end\":47303,\"start\":46955},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":216078090},\"end\":47570,\"start\":47305},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13193974},\"end\":47772,\"start\":47572},{\"attributes\":{\"id\":\"b14\"},\"end\":47960,\"start\":47774},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195908774},\"end\":48330,\"start\":47962},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":44096776},\"end\":48742,\"start\":48332},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3488815},\"end\":49176,\"start\":48744},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3968602},\"end\":49516,\"start\":49178},{\"attributes\":{\"doi\":\"arXiv:1810.01407\",\"id\":\"b19\"},\"end\":49807,\"start\":49518},{\"attributes\":{\"doi\":\"arXiv:1711.03707\",\"id\":\"b20\"},\"end\":50081,\"start\":49809},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52186163},\"end\":50533,\"start\":50083},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9746839},\"end\":50791,\"start\":50535},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":205242740},\"end\":51269,\"start\":50793},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17451251},\"end\":51825,\"start\":51271},{\"attributes\":{\"doi\":\"arXiv:1605.07277\",\"id\":\"b25\"},\"end\":52209,\"start\":51827},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":11758569},\"end\":52619,\"start\":52211},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2930547},\"end\":53182,\"start\":52621},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4626477},\"end\":53678,\"start\":53184},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":515925},\"end\":54196,\"start\":53680},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14124313},\"end\":54535,\"start\":54198},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":35426171},\"end\":54753,\"start\":54537},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7961699},\"end\":55077,\"start\":54755},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":604334},\"end\":55513,\"start\":55079},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":53298804},\"end\":55815,\"start\":55515},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52962648},\"end\":56211,\"start\":55817},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":67846878},\"end\":56735,\"start\":56213},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2120593},\"end\":56968,\"start\":56737},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":579561},\"end\":57330,\"start\":56970},{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b0\"},\"end\":43399,\"start\":43195},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9089716},\"end\":43619,\"start\":43401},{\"attributes\":{\"doi\":\"arXiv:1811.03728\",\"id\":\"b2\"},\"end\":44097,\"start\":43621},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b3\"},\"end\":44427,\"start\":44099},{\"attributes\":{\"doi\":\"arXiv:1810.03505\",\"id\":\"b4\"},\"end\":44713,\"start\":44429},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1033682},\"end\":45133,\"start\":44715},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206741496},\"end\":45521,\"start\":45135},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b7\"},\"end\":45855,\"start\":45523},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10894094},\"end\":46215,\"start\":45857},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":46547,\"start\":46217},{\"attributes\":{\"doi\":\"arXiv:1712.09196\",\"id\":\"b10\"},\"end\":46953,\"start\":46549},{\"attributes\":{\"doi\":\"arXiv:1905.02175\",\"id\":\"b11\"},\"end\":47303,\"start\":46955},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":216078090},\"end\":47570,\"start\":47305},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13193974},\"end\":47772,\"start\":47572},{\"attributes\":{\"id\":\"b14\"},\"end\":47960,\"start\":47774},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195908774},\"end\":48330,\"start\":47962},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":44096776},\"end\":48742,\"start\":48332},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3488815},\"end\":49176,\"start\":48744},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3968602},\"end\":49516,\"start\":49178},{\"attributes\":{\"doi\":\"arXiv:1810.01407\",\"id\":\"b19\"},\"end\":49807,\"start\":49518},{\"attributes\":{\"doi\":\"arXiv:1711.03707\",\"id\":\"b20\"},\"end\":50081,\"start\":49809},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52186163},\"end\":50533,\"start\":50083},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9746839},\"end\":50791,\"start\":50535},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":205242740},\"end\":51269,\"start\":50793},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17451251},\"end\":51825,\"start\":51271},{\"attributes\":{\"doi\":\"arXiv:1605.07277\",\"id\":\"b25\"},\"end\":52209,\"start\":51827},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":11758569},\"end\":52619,\"start\":52211},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2930547},\"end\":53182,\"start\":52621},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4626477},\"end\":53678,\"start\":53184},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":515925},\"end\":54196,\"start\":53680},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14124313},\"end\":54535,\"start\":54198},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":35426171},\"end\":54753,\"start\":54537},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7961699},\"end\":55077,\"start\":54755},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":604334},\"end\":55513,\"start\":55079},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":53298804},\"end\":55815,\"start\":55515},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52962648},\"end\":56211,\"start\":55817},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":67846878},\"end\":56735,\"start\":56213},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2120593},\"end\":56968,\"start\":56737},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":579561},\"end\":57330,\"start\":56970}]", "bib_title": "[{\"end\":43450,\"start\":43401},{\"end\":44742,\"start\":44715},{\"end\":45189,\"start\":45135},{\"end\":45894,\"start\":45857},{\"end\":46261,\"start\":46217},{\"end\":47336,\"start\":47305},{\"end\":47631,\"start\":47572},{\"end\":48025,\"start\":47962},{\"end\":48407,\"start\":48332},{\"end\":48805,\"start\":48744},{\"end\":49261,\"start\":49178},{\"end\":50189,\"start\":50083},{\"end\":50618,\"start\":50535},{\"end\":50848,\"start\":50793},{\"end\":51348,\"start\":51271},{\"end\":52303,\"start\":52211},{\"end\":52670,\"start\":52621},{\"end\":53255,\"start\":53184},{\"end\":53746,\"start\":53680},{\"end\":54264,\"start\":54198},{\"end\":54582,\"start\":54537},{\"end\":54805,\"start\":54755},{\"end\":55119,\"start\":55079},{\"end\":55554,\"start\":55515},{\"end\":55856,\"start\":55817},{\"end\":56291,\"start\":56213},{\"end\":56794,\"start\":56737},{\"end\":57031,\"start\":56970},{\"end\":43450,\"start\":43401},{\"end\":44742,\"start\":44715},{\"end\":45189,\"start\":45135},{\"end\":45894,\"start\":45857},{\"end\":46261,\"start\":46217},{\"end\":47336,\"start\":47305},{\"end\":47631,\"start\":47572},{\"end\":48025,\"start\":47962},{\"end\":48407,\"start\":48332},{\"end\":48805,\"start\":48744},{\"end\":49261,\"start\":49178},{\"end\":50189,\"start\":50083},{\"end\":50618,\"start\":50535},{\"end\":50848,\"start\":50793},{\"end\":51348,\"start\":51271},{\"end\":52303,\"start\":52211},{\"end\":52670,\"start\":52621},{\"end\":53255,\"start\":53184},{\"end\":53746,\"start\":53680},{\"end\":54264,\"start\":54198},{\"end\":54582,\"start\":54537},{\"end\":54805,\"start\":54755},{\"end\":55119,\"start\":55079},{\"end\":55554,\"start\":55515},{\"end\":55856,\"start\":55817},{\"end\":56291,\"start\":56213},{\"end\":56794,\"start\":56737},{\"end\":57031,\"start\":56970}]", "bib_author": "[{\"end\":43214,\"start\":43197},{\"end\":43232,\"start\":43214},{\"end\":43245,\"start\":43232},{\"end\":43469,\"start\":43452},{\"end\":43484,\"start\":43469},{\"end\":43498,\"start\":43484},{\"end\":43711,\"start\":43698},{\"end\":43727,\"start\":43711},{\"end\":43747,\"start\":43727},{\"end\":43761,\"start\":43747},{\"end\":43779,\"start\":43761},{\"end\":43792,\"start\":43779},{\"end\":43804,\"start\":43792},{\"end\":43823,\"start\":43804},{\"end\":44112,\"start\":44099},{\"end\":44123,\"start\":44112},{\"end\":44130,\"start\":44123},{\"end\":44143,\"start\":44130},{\"end\":44154,\"start\":44143},{\"end\":44437,\"start\":44429},{\"end\":44445,\"start\":44437},{\"end\":44455,\"start\":44445},{\"end\":44472,\"start\":44455},{\"end\":44489,\"start\":44472},{\"end\":44498,\"start\":44489},{\"end\":44760,\"start\":44744},{\"end\":44780,\"start\":44760},{\"end\":44793,\"start\":44780},{\"end\":44802,\"start\":44793},{\"end\":44822,\"start\":44802},{\"end\":44837,\"start\":44822},{\"end\":44854,\"start\":44837},{\"end\":44869,\"start\":44854},{\"end\":45204,\"start\":45191},{\"end\":45226,\"start\":45204},{\"end\":45243,\"start\":45226},{\"end\":45534,\"start\":45523},{\"end\":45556,\"start\":45534},{\"end\":45572,\"start\":45556},{\"end\":45914,\"start\":45896},{\"end\":45927,\"start\":45914},{\"end\":45944,\"start\":45927},{\"end\":45962,\"start\":45944},{\"end\":45981,\"start\":45962},{\"end\":46275,\"start\":46263},{\"end\":46290,\"start\":46275},{\"end\":46304,\"start\":46290},{\"end\":46314,\"start\":46304},{\"end\":46638,\"start\":46624},{\"end\":46650,\"start\":46638},{\"end\":46665,\"start\":46650},{\"end\":46690,\"start\":46665},{\"end\":46712,\"start\":46690},{\"end\":47023,\"start\":47009},{\"end\":47042,\"start\":47023},{\"end\":47058,\"start\":47042},{\"end\":47072,\"start\":47058},{\"end\":47090,\"start\":47072},{\"end\":47350,\"start\":47338},{\"end\":47362,\"start\":47350},{\"end\":47371,\"start\":47362},{\"end\":47643,\"start\":47633},{\"end\":47654,\"start\":47643},{\"end\":47661,\"start\":47654},{\"end\":47846,\"start\":47829},{\"end\":48044,\"start\":48027},{\"end\":48060,\"start\":48044},{\"end\":48079,\"start\":48060},{\"end\":48419,\"start\":48409},{\"end\":48441,\"start\":48419},{\"end\":48457,\"start\":48441},{\"end\":48825,\"start\":48807},{\"end\":48845,\"start\":48825},{\"end\":48861,\"start\":48845},{\"end\":48879,\"start\":48861},{\"end\":48893,\"start\":48879},{\"end\":49282,\"start\":49263},{\"end\":49301,\"start\":49282},{\"end\":49537,\"start\":49518},{\"end\":49556,\"start\":49537},{\"end\":49830,\"start\":49811},{\"end\":49852,\"start\":49830},{\"end\":49871,\"start\":49852},{\"end\":50210,\"start\":50191},{\"end\":50232,\"start\":50210},{\"end\":50251,\"start\":50232},{\"end\":50631,\"start\":50620},{\"end\":50644,\"start\":50631},{\"end\":50866,\"start\":50850},{\"end\":50885,\"start\":50866},{\"end\":50899,\"start\":50885},{\"end\":50914,\"start\":50899},{\"end\":50927,\"start\":50914},{\"end\":50935,\"start\":50927},{\"end\":50951,\"start\":50935},{\"end\":50966,\"start\":50951},{\"end\":50988,\"start\":50966},{\"end\":51005,\"start\":50988},{\"end\":51016,\"start\":51005},{\"end\":51365,\"start\":51350},{\"end\":51382,\"start\":51365},{\"end\":51396,\"start\":51382},{\"end\":51418,\"start\":51396},{\"end\":51945,\"start\":51927},{\"end\":51963,\"start\":51945},{\"end\":51979,\"start\":51963},{\"end\":52319,\"start\":52305},{\"end\":52330,\"start\":52319},{\"end\":52348,\"start\":52330},{\"end\":52690,\"start\":52672},{\"end\":52700,\"start\":52690},{\"end\":52708,\"start\":52700},{\"end\":52725,\"start\":52708},{\"end\":52743,\"start\":52725},{\"end\":52752,\"start\":52743},{\"end\":52767,\"start\":52752},{\"end\":52784,\"start\":52767},{\"end\":52799,\"start\":52784},{\"end\":52818,\"start\":52799},{\"end\":52836,\"start\":52818},{\"end\":52848,\"start\":52836},{\"end\":53270,\"start\":53257},{\"end\":53283,\"start\":53270},{\"end\":53298,\"start\":53283},{\"end\":53314,\"start\":53298},{\"end\":53332,\"start\":53314},{\"end\":53348,\"start\":53332},{\"end\":53363,\"start\":53348},{\"end\":53762,\"start\":53748},{\"end\":53773,\"start\":53762},{\"end\":53791,\"start\":53773},{\"end\":53804,\"start\":53791},{\"end\":53819,\"start\":53804},{\"end\":53835,\"start\":53819},{\"end\":53853,\"start\":53835},{\"end\":53876,\"start\":53853},{\"end\":53893,\"start\":53876},{\"end\":53914,\"start\":53893},{\"end\":53923,\"start\":53914},{\"end\":54282,\"start\":54266},{\"end\":54300,\"start\":54282},{\"end\":54602,\"start\":54584},{\"end\":54618,\"start\":54602},{\"end\":54633,\"start\":54618},{\"end\":54823,\"start\":54807},{\"end\":54838,\"start\":54823},{\"end\":54849,\"start\":54838},{\"end\":55140,\"start\":55121},{\"end\":55158,\"start\":55140},{\"end\":55174,\"start\":55158},{\"end\":55186,\"start\":55174},{\"end\":55201,\"start\":55186},{\"end\":55217,\"start\":55201},{\"end\":55229,\"start\":55217},{\"end\":55570,\"start\":55556},{\"end\":55580,\"start\":55570},{\"end\":55598,\"start\":55580},{\"end\":55876,\"start\":55858},{\"end\":55895,\"start\":55876},{\"end\":55911,\"start\":55895},{\"end\":55929,\"start\":55911},{\"end\":55947,\"start\":55929},{\"end\":56305,\"start\":56293},{\"end\":56319,\"start\":56305},{\"end\":56331,\"start\":56319},{\"end\":56343,\"start\":56331},{\"end\":56360,\"start\":56343},{\"end\":56374,\"start\":56360},{\"end\":56381,\"start\":56374},{\"end\":56387,\"start\":56381},{\"end\":56806,\"start\":56796},{\"end\":56818,\"start\":56806},{\"end\":56834,\"start\":56818},{\"end\":57045,\"start\":57033},{\"end\":57062,\"start\":57045},{\"end\":57077,\"start\":57062},{\"end\":57087,\"start\":57077},{\"end\":57103,\"start\":57087},{\"end\":57115,\"start\":57103},{\"end\":43214,\"start\":43197},{\"end\":43232,\"start\":43214},{\"end\":43245,\"start\":43232},{\"end\":43469,\"start\":43452},{\"end\":43484,\"start\":43469},{\"end\":43498,\"start\":43484},{\"end\":43711,\"start\":43698},{\"end\":43727,\"start\":43711},{\"end\":43747,\"start\":43727},{\"end\":43761,\"start\":43747},{\"end\":43779,\"start\":43761},{\"end\":43792,\"start\":43779},{\"end\":43804,\"start\":43792},{\"end\":43823,\"start\":43804},{\"end\":44112,\"start\":44099},{\"end\":44123,\"start\":44112},{\"end\":44130,\"start\":44123},{\"end\":44143,\"start\":44130},{\"end\":44154,\"start\":44143},{\"end\":44437,\"start\":44429},{\"end\":44445,\"start\":44437},{\"end\":44455,\"start\":44445},{\"end\":44472,\"start\":44455},{\"end\":44489,\"start\":44472},{\"end\":44498,\"start\":44489},{\"end\":44760,\"start\":44744},{\"end\":44780,\"start\":44760},{\"end\":44793,\"start\":44780},{\"end\":44802,\"start\":44793},{\"end\":44822,\"start\":44802},{\"end\":44837,\"start\":44822},{\"end\":44854,\"start\":44837},{\"end\":44869,\"start\":44854},{\"end\":45204,\"start\":45191},{\"end\":45226,\"start\":45204},{\"end\":45243,\"start\":45226},{\"end\":45534,\"start\":45523},{\"end\":45556,\"start\":45534},{\"end\":45572,\"start\":45556},{\"end\":45914,\"start\":45896},{\"end\":45927,\"start\":45914},{\"end\":45944,\"start\":45927},{\"end\":45962,\"start\":45944},{\"end\":45981,\"start\":45962},{\"end\":46275,\"start\":46263},{\"end\":46290,\"start\":46275},{\"end\":46304,\"start\":46290},{\"end\":46314,\"start\":46304},{\"end\":46638,\"start\":46624},{\"end\":46650,\"start\":46638},{\"end\":46665,\"start\":46650},{\"end\":46690,\"start\":46665},{\"end\":46712,\"start\":46690},{\"end\":47023,\"start\":47009},{\"end\":47042,\"start\":47023},{\"end\":47058,\"start\":47042},{\"end\":47072,\"start\":47058},{\"end\":47090,\"start\":47072},{\"end\":47350,\"start\":47338},{\"end\":47362,\"start\":47350},{\"end\":47371,\"start\":47362},{\"end\":47643,\"start\":47633},{\"end\":47654,\"start\":47643},{\"end\":47661,\"start\":47654},{\"end\":47846,\"start\":47829},{\"end\":48044,\"start\":48027},{\"end\":48060,\"start\":48044},{\"end\":48079,\"start\":48060},{\"end\":48419,\"start\":48409},{\"end\":48441,\"start\":48419},{\"end\":48457,\"start\":48441},{\"end\":48825,\"start\":48807},{\"end\":48845,\"start\":48825},{\"end\":48861,\"start\":48845},{\"end\":48879,\"start\":48861},{\"end\":48893,\"start\":48879},{\"end\":49282,\"start\":49263},{\"end\":49301,\"start\":49282},{\"end\":49537,\"start\":49518},{\"end\":49556,\"start\":49537},{\"end\":49830,\"start\":49811},{\"end\":49852,\"start\":49830},{\"end\":49871,\"start\":49852},{\"end\":50210,\"start\":50191},{\"end\":50232,\"start\":50210},{\"end\":50251,\"start\":50232},{\"end\":50631,\"start\":50620},{\"end\":50644,\"start\":50631},{\"end\":50866,\"start\":50850},{\"end\":50885,\"start\":50866},{\"end\":50899,\"start\":50885},{\"end\":50914,\"start\":50899},{\"end\":50927,\"start\":50914},{\"end\":50935,\"start\":50927},{\"end\":50951,\"start\":50935},{\"end\":50966,\"start\":50951},{\"end\":50988,\"start\":50966},{\"end\":51005,\"start\":50988},{\"end\":51016,\"start\":51005},{\"end\":51365,\"start\":51350},{\"end\":51382,\"start\":51365},{\"end\":51396,\"start\":51382},{\"end\":51418,\"start\":51396},{\"end\":51945,\"start\":51927},{\"end\":51963,\"start\":51945},{\"end\":51979,\"start\":51963},{\"end\":52319,\"start\":52305},{\"end\":52330,\"start\":52319},{\"end\":52348,\"start\":52330},{\"end\":52690,\"start\":52672},{\"end\":52700,\"start\":52690},{\"end\":52708,\"start\":52700},{\"end\":52725,\"start\":52708},{\"end\":52743,\"start\":52725},{\"end\":52752,\"start\":52743},{\"end\":52767,\"start\":52752},{\"end\":52784,\"start\":52767},{\"end\":52799,\"start\":52784},{\"end\":52818,\"start\":52799},{\"end\":52836,\"start\":52818},{\"end\":52848,\"start\":52836},{\"end\":53270,\"start\":53257},{\"end\":53283,\"start\":53270},{\"end\":53298,\"start\":53283},{\"end\":53314,\"start\":53298},{\"end\":53332,\"start\":53314},{\"end\":53348,\"start\":53332},{\"end\":53363,\"start\":53348},{\"end\":53762,\"start\":53748},{\"end\":53773,\"start\":53762},{\"end\":53791,\"start\":53773},{\"end\":53804,\"start\":53791},{\"end\":53819,\"start\":53804},{\"end\":53835,\"start\":53819},{\"end\":53853,\"start\":53835},{\"end\":53876,\"start\":53853},{\"end\":53893,\"start\":53876},{\"end\":53914,\"start\":53893},{\"end\":53923,\"start\":53914},{\"end\":54282,\"start\":54266},{\"end\":54300,\"start\":54282},{\"end\":54602,\"start\":54584},{\"end\":54618,\"start\":54602},{\"end\":54633,\"start\":54618},{\"end\":54823,\"start\":54807},{\"end\":54838,\"start\":54823},{\"end\":54849,\"start\":54838},{\"end\":55140,\"start\":55121},{\"end\":55158,\"start\":55140},{\"end\":55174,\"start\":55158},{\"end\":55186,\"start\":55174},{\"end\":55201,\"start\":55186},{\"end\":55217,\"start\":55201},{\"end\":55229,\"start\":55217},{\"end\":55570,\"start\":55556},{\"end\":55580,\"start\":55570},{\"end\":55598,\"start\":55580},{\"end\":55876,\"start\":55858},{\"end\":55895,\"start\":55876},{\"end\":55911,\"start\":55895},{\"end\":55929,\"start\":55911},{\"end\":55947,\"start\":55929},{\"end\":56305,\"start\":56293},{\"end\":56319,\"start\":56305},{\"end\":56331,\"start\":56319},{\"end\":56343,\"start\":56331},{\"end\":56360,\"start\":56343},{\"end\":56374,\"start\":56360},{\"end\":56381,\"start\":56374},{\"end\":56387,\"start\":56381},{\"end\":56806,\"start\":56796},{\"end\":56818,\"start\":56806},{\"end\":56834,\"start\":56818},{\"end\":57045,\"start\":57033},{\"end\":57062,\"start\":57045},{\"end\":57077,\"start\":57062},{\"end\":57087,\"start\":57077},{\"end\":57103,\"start\":57087},{\"end\":57115,\"start\":57103}]", "bib_venue": "[{\"end\":43502,\"start\":43498},{\"end\":43696,\"start\":43621},{\"end\":44241,\"start\":44170},{\"end\":44550,\"start\":44514},{\"end\":44916,\"start\":44869},{\"end\":45320,\"start\":45243},{\"end\":45667,\"start\":45588},{\"end\":46028,\"start\":45981},{\"end\":46374,\"start\":46314},{\"end\":46622,\"start\":46549},{\"end\":47007,\"start\":46955},{\"end\":47430,\"start\":47371},{\"end\":47665,\"start\":47661},{\"end\":47827,\"start\":47774},{\"end\":48138,\"start\":48079},{\"end\":48529,\"start\":48457},{\"end\":48952,\"start\":48893},{\"end\":49339,\"start\":49301},{\"end\":49656,\"start\":49572},{\"end\":50300,\"start\":50251},{\"end\":50648,\"start\":50644},{\"end\":51022,\"start\":51016},{\"end\":51498,\"start\":51418},{\"end\":51925,\"start\":51827},{\"end\":52407,\"start\":52348},{\"end\":52891,\"start\":52848},{\"end\":53422,\"start\":53363},{\"end\":53929,\"start\":53923},{\"end\":54359,\"start\":54300},{\"end\":54637,\"start\":54633},{\"end\":54908,\"start\":54849},{\"end\":55288,\"start\":55229},{\"end\":55657,\"start\":55598},{\"end\":56006,\"start\":55947},{\"end\":56445,\"start\":56387},{\"end\":56838,\"start\":56834},{\"end\":57129,\"start\":57115},{\"end\":43502,\"start\":43498},{\"end\":43696,\"start\":43621},{\"end\":44241,\"start\":44170},{\"end\":44550,\"start\":44514},{\"end\":44916,\"start\":44869},{\"end\":45320,\"start\":45243},{\"end\":45667,\"start\":45588},{\"end\":46028,\"start\":45981},{\"end\":46374,\"start\":46314},{\"end\":46622,\"start\":46549},{\"end\":47007,\"start\":46955},{\"end\":47430,\"start\":47371},{\"end\":47665,\"start\":47661},{\"end\":47827,\"start\":47774},{\"end\":48138,\"start\":48079},{\"end\":48529,\"start\":48457},{\"end\":48952,\"start\":48893},{\"end\":49339,\"start\":49301},{\"end\":49656,\"start\":49572},{\"end\":50300,\"start\":50251},{\"end\":50648,\"start\":50644},{\"end\":51022,\"start\":51016},{\"end\":51498,\"start\":51418},{\"end\":51925,\"start\":51827},{\"end\":52407,\"start\":52348},{\"end\":52891,\"start\":52848},{\"end\":53422,\"start\":53363},{\"end\":53929,\"start\":53923},{\"end\":54359,\"start\":54300},{\"end\":54637,\"start\":54633},{\"end\":54908,\"start\":54849},{\"end\":55288,\"start\":55229},{\"end\":55657,\"start\":55598},{\"end\":56006,\"start\":55947},{\"end\":56445,\"start\":56387},{\"end\":56838,\"start\":56834},{\"end\":57129,\"start\":57115},{\"end\":51565,\"start\":51500},{\"end\":56490,\"start\":56447},{\"end\":51565,\"start\":51500},{\"end\":56490,\"start\":56447}]"}}}, "year": 2023, "month": 12, "day": 17}