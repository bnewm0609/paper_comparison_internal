{"id": 28202810, "updated": "2023-09-29 15:25:04.54", "metadata": {"title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "authors": "[{\"first\":\"Tuomas\",\"last\":\"Haarnoja\",\"middle\":[]},{\"first\":\"Aurick\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Pieter\",\"last\":\"Abbeel\",\"middle\":[]},{\"first\":\"Sergey\",\"last\":\"Levine\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 1, "day": 4}, "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1801.01290", "mag": "3022031817", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/HaarnojaZAL18", "doi": null}}, "content": {"source": {"pdf_hash": "1d0273621758bc2eee974a82f6ad49ecf1ac95d3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1801.01290v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "183d7069e385bc573fc6a7e78a7f4431990397cb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1d0273621758bc2eee974a82f6ad49ecf1ac95d3.txt", "contents": "\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\n\nTuomas Haarnoja \nAurick Zhou \nPieter Abbeel \nSergey Levine \nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nModel-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.\n\nIntroduction\n\nModel-free deep reinforcement learning (RL) algorithms have been applied in a range of challenging domains, from games (Mnih et al., 2013;Silver et al., 2016) to robotic control (Schulman et al., 2015). The combination of RL and high-capacity function approximators such as neural networks holds the promise of automating a wide range of decision making and control tasks, but widespread adoption of these methods in real-world domains has been hampered by two major challenges. First, model-free deep RL methods are notoriously expensive in terms of their sample complexity. Even relatively simple tasks can require millions of steps of data collection, and complex behaviors with highdimensional observations might need substantially more. Second, these methods are often brittle with respect to their hyperparameters: learning rates, exploration constants, and other settings must be set carefully for different problem settings to achieve good results. Both of these challenges severely limit the applicability of model-free deep RL to real-world tasks.\n\nOne cause for the poor sample efficiency of deep RL methods is on-policy learning: some of the most commonly used deep RL algorithms, such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017b) or A3C , require new samples to be collected for each gradient step. This quickly becomes extravagantly expensive, as the number of gradient steps and samples per step needed to learn an effective policy increases with task complexity. Offpolicy algorithms aim to reuse past experience. This is not directly feasible with conventional policy gradient formulations, but is relatively straightforward for Q-learning based methods (Mnih et al., 2015). Unfortunately, the combination of off-policy learning and high-dimensional, nonlinear function approximation with neural networks presents a major challenge for stability and convergence (Bhatnagar et al., 2009). This challenge is further exacerbated in continuous state and action spaces, where a separate actor network is often used to perform the maximization in Q-learning. A commonly used algorithm in such settings, deep deterministic policy gradient (DDPG) , provides for sample-efficient learning but is notoriously challenging to use due to its extreme brittleness and hyperparameter sensitivity (Duan et al., 2016;Henderson et al., 2017).\n\nWe explore how to design an efficient and stable modelfree deep RL algorithm for continuous state and action spaces. To that end, we draw on the maximum entropy framework, which augments the standard maximum reward reinforcement learning objective with an entropy maximization term (Ziebart et al., 2008;Toussaint, 2009;Rawlik et al., 2012;Fox et al., 2016;Haarnoja et al., 2017). Maximum entropy reinforcement learning alters the RL objective, though the original objective can be recovered using a temperature parameter (Haarnoja et al., 2017). More importantly, the maximum entropy formulation provides a substantial improvement in exploration and robustness: as discussed by Ziebart (2010), maximum entropy policies are robust in the face of model and estimation errors, and as demonstrated by (Haarnoja et al., 2017), they improve exploration by acquiring diverse behaviors. Prior work has proposed model-free deep RL algorithms that perform on-policy learning with entropy maximization (O'Donoghue et al., 2016), as well as off-policy methods based on soft Q-learning and its variants (Schulman et al., 2017a;Nachum et al., 2017a;Haarnoja et al., 2017). However, the on-policy variants suffer from poor sample complexity for the reasons discussed above, while the off-policy variants require complex approximate inference procedures in continuous action spaces.\n\nIn this paper, we demonstrate that we can devise an offpolicy maximum entropy actor-critic algorithm, which we call soft actor-critic (SAC), which provides for both sampleefficient learning and stability. This algorithm extends readily to very complex, high-dimensional tasks, such as the Humanoid benchmark (Duan et al., 2016) with 21 action dimensions, where off-policy methods such as DDPG typically struggle to obtain good results (Gu et al., 2016). SAC also avoids the complexity and potential instability associated with approximate inference in prior off-policy maximum entropy algorithms based on soft Q-learning (Haarnoja et al., 2017). We present a convergence proof for policy iteration in the maximum entropy framework, and then introduce a new algorithm based on an approximation to this procedure that can be practically implemented with deep neural networks, which we call soft actor-critic. We present empirical results that show that soft actor-critic attains a substantial improvement in both performance and sample efficiency over both off-policy and on-policy prior methods. We also compare to twin delayed deep deterministic (TD3) policy gradient algorithm (Fujimoto et al., 2018), which is a concurrent work that proposes a deterministic algorithm that substantially improves on DDPG.\n\n\nRelated Work\n\nOur soft actor-critic algorithm incorporates three key ingredients: an actor-critic architecture with separate policy and value function networks, an off-policy formulation that enables reuse of previously collected data for efficiency, and entropy maximization to enable stability and exploration. We review prior works that draw on some of these ideas in this section. Actor-critic algorithms are typically derived starting from policy iteration, which alternates between policy evaluation-computing the value function for a policy-and policy improvement-using the value function to obtain a better policy (Barto et al., 1983;Sutton & Barto, 1998). In large-scale reinforcement learning problems, it is typically impractical to run either of these steps to convergence, and instead the value function and policy are optimized jointly. In this case, the policy is referred to as the actor, and the value function as the critic. Many actor-critic algorithms build on the standard, on-policy policy gradient formulation to update the actor (Peters & Schaal, 2008), and many of them also consider the entropy of the policy, but instead of maximizing the entropy, they use it as an regularizer (Schulman et al., 2017b;Mnih et al., 2016;Gruslys et al., 2017). On-policy training tends to improve stability but results in poor sample complexity.\n\nThere have been efforts to increase the sample efficiency while retaining robustness by incorporating off-policy samples and by using higher order variance reduction techniques (O'Donoghue et al., 2016;Gu et al., 2016). However, fully off-policy algorithms still attain better efficiency. A particularly popular off-policy actor-critic method, DDPG , which is a deep variant of the deterministic policy gradient (Silver et al., 2014) algorithm, uses a Q-function estimator to enable off-policy learning, and a deterministic actor that maximizes this Q-function. As such, this method can be viewed both as a deterministic actor-critic algorithm and an approximate Q-learning algorithm. Unfortunately, the interplay between the deterministic actor network and the Q-function typically makes DDPG extremely difficult to stabilize and brittle to hyperparameter settings (Duan et al., 2016;Henderson et al., 2017). As a consequence, it is difficult to extend DDPG to complex, high-dimensional tasks, and on-policy policy gradient methods still tend to produce the best results in such settings (Gu et al., 2016). Our method instead combines off-policy actorcritic training with a stochastic actor, and further aims to maximize the entropy of this actor with an entropy maximization objective. We find that this actually results in a considerably more stable and scalable algorithm that, in practice, exceeds both the efficiency and final performance of DDPG. A similar method can be derived as a zero-step special case of stochastic value gradients (SVG(0)) . However, SVG(0) differs from our method in that it optimizes the standard maximum expected return objective, and it does not make use of a separate value network, which we found to make training more stable.\n\nMaximum entropy reinforcement learning optimizes policies to maximize both the expected return and the expected entropy of the policy. This framework has been used in many contexts, from inverse reinforcement learning (Ziebart et al., 2008) to optimal control (Todorov, 2008;Toussaint, 2009;Rawlik et al., 2012). In guided policy search (Levine & Koltun, 2013;Levine et al., 2016), the maximum entropy distribution is used to guide policy learn-ing towards high-reward regions. More recently, several papers have noted the connection between Q-learning and policy gradient methods in the framework of maximum entropy learning (O'Donoghue et al., 2016;Haarnoja et al., 2017;Nachum et al., 2017a;Schulman et al., 2017a). While most of the prior model-free works assume a discrete action space, Nachum et al. (2017b) approximate the maximum entropy distribution with a Gaussian and Haarnoja et al. (2017) with a sampling network trained to draw samples from the optimal policy. Although the soft Q-learning algorithm proposed by Haarnoja et al. (2017) has a value function and actor network, it is not a true actor-critic algorithm: the Q-function is estimating the optimal Q-function, and the actor does not directly affect the Q-function except through the data distribution. Hence, Haarnoja et al. (2017) motivates the actor network as an approximate sampler, rather than the actor in an actor-critic algorithm. Crucially, the convergence of this method hinges on how well this sampler approximates the true posterior. In contrast, we prove that our method converges to the optimal policy from a given policy class, regardless of the policy parameterization. Furthermore, these prior maximum entropy methods generally do not exceed the performance of state-of-the-art off-policy algorithms, such as DDPG, when learning from scratch, though they may have other benefits, such as improved exploration and ease of fine-tuning. In our experiments, we demonstrate that our soft actor-critic algorithm does in fact exceed the performance of prior state-of-the-art off-policy deep RL methods by a wide margin.\n\n\nPreliminaries\n\nWe first introduce notation and summarize the standard and maximum entropy reinforcement learning frameworks.\n\n\nNotation\n\nWe address policy learning in continuous action spaces. We consider an infinite-horizon Markov decision process (MDP), defined by the tuple (S, A, p, r), where the state space S and the action space A are continuous, and the unknown state transition probability p : S \u00d7 S \u00d7 A \u2192 [0, \u221e) represents the probability density of the next state s t+1 \u2208 S given the current state s t \u2208 S and action a t \u2208 A. The environment emits a bounded reward r : S \u00d7 A \u2192 [r min , r max ] on each transition. We will use \u03c1 \u03c0 (s t ) and \u03c1 \u03c0 (s t , a t ) to denote the state and state-action marginals of the trajectory distribution induced by a policy \u03c0(a t |s t ).\n\n\nMaximum Entropy Reinforcement Learning\n\nStandard RL maximizes the expected sum of rewards t E (st,at)\u223c\u03c1\u03c0 [r(s t , a t )]. We will consider a more general maximum entropy objective (see e.g. Ziebart (2010)), which favors stochastic policies by augmenting the objective with the expected entropy of the policy over \u03c1 \u03c0 (s t ):\nJ(\u03c0) = T t=0 E (st,at)\u223c\u03c1\u03c0 [r(s t , a t ) + \u03b1H(\u03c0( \u00b7 |s t ))] . (1)\nThe temperature parameter \u03b1 determines the relative importance of the entropy term against the reward, and thus controls the stochasticity of the optimal policy. The maximum entropy objective differs from the standard maximum expected reward objective used in conventional reinforcement learning, though the conventional objective can be recovered in the limit as \u03b1 \u2192 0. For the rest of this paper, we will omit writing the temperature explicitly, as it can always be subsumed into the reward by scaling it by \u03b1 \u22121 .\n\nThis objective has a number of conceptual and practical advantages. First, the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues. Second, the policy can capture multiple modes of nearoptimal behavior. In problem settings where multiple actions seem equally attractive, the policy will commit equal probability mass to those actions. Lastly, prior work has observed improved exploration with this objective (Haarnoja et al., 2017;Schulman et al., 2017a), and in our experiments, we observe that it considerably improves learning speed over state-of-art methods that optimize the conventional RL objective function. We can extend the objective to infinite horizon problems by introducing a discount factor \u03b3 to ensure that the sum of expected rewards and entropies is finite. Writing down the maximum entropy objective for the infinite horizon discounted case is more involved (Thomas, 2014) and is deferred to Appendix A.\n\nPrior methods have proposed directly solving for the optimal Q-function, from which the optimal policy can be recovered (Ziebart et al., 2008;Fox et al., 2016;Haarnoja et al., 2017). We will discuss how we can devise a soft actor-critic algorithm through a policy iteration formulation, where we instead evaluate the Q-function of the current policy and update the policy through an off-policy gradient update. Though such algorithms have previously been proposed for conventional reinforcement learning, our method is, to our knowledge, the first off-policy actor-critic method in the maximum entropy reinforcement learning framework.\n\n\nDerivation of Soft Policy Iteration\n\nWe will begin by deriving soft policy iteration, a general algorithm for learning optimal maximum entropy policies that alternates between policy evaluation and policy improvement in the maximum entropy framework. Our derivation is based on a tabular setting, to enable theoretical analysis and convergence guarantees, and we extend this method into the general continuous setting in the next section. We will show that soft policy iteration converges to the optimal policy within a set of policies which might correspond, for instance, to a set of parameterized densities.\n\nIn the policy evaluation step of soft policy iteration, we wish to compute the value of a policy \u03c0 according to the maximum entropy objective in Equation 1. For a fixed policy, the soft Q-value can be computed iteratively, starting from any function Q : S \u00d7 A \u2192 R and repeatedly applying a modified Bellman backup operator T \u03c0 given by\nT \u03c0 Q(s t , a t ) r(s t , a t ) + \u03b3 E st+1\u223cp [V (s t+1 )] , (2) where V (s t ) = E at\u223c\u03c0 [Q(s t , a t ) \u2212 log \u03c0(a t |s t )](3)\nis the soft state value function. We can obtain the soft value function for any policy \u03c0 by repeatedly applying T \u03c0 as formalized below.\n\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping Q 0 : S \u00d7A \u2192 R with |A| < \u221e, and define Q k+1 = T \u03c0 Q k . Then the sequence Q k will converge to the soft Q-value of \u03c0 as k \u2192 \u221e.\n\nProof. See Appendix B.1.\n\nIn the policy improvement step, we update the policy towards the exponential of the new Q-function. This particular choice of update can be guaranteed to result in an improved policy in terms of its soft value. Since in practice we prefer policies that are tractable, we will additionally restrict the policy to some set of policies \u03a0, which can correspond, for example, to a parameterized family of distributions such as Gaussians. To account for the constraint that \u03c0 \u2208 \u03a0, we project the improved policy into the desired set of policies. While in principle we could choose any projection, it will turn out to be convenient to use the information projection defined in terms of the Kullback-Leibler divergence. In the other words, in the policy improvement step, for each state, we update the policy according to\n\u03c0 new = arg min \u03c0 \u2208\u03a0 D KL \u03c0 ( \u00b7 |s t ) exp (Q \u03c0 old (s t , \u00b7 )) Z \u03c0 old (s t ) .(4)\nThe partition function Z \u03c0 old (s t ) normalizes the distribution, and while it is intractable in general, it does not contribute to the gradient with respect to the new policy and can thus be ignored, as noted in the next section. For this projection, we can show that the new, projected policy has a higher value than the old policy with respect to the objective in Equation 1. We formalize this result in Lemma 2.\n\nLemma 2 (Soft Policy Improvement). Let \u03c0 old \u2208 \u03a0 and let \u03c0 new be the optimizer of the minimization problem defined in Equation 4.\nThen Q \u03c0new (s t , a t ) \u2265 Q \u03c0 old (s t , a t ) for all (s t , a t ) \u2208 S \u00d7 A with |A| < \u221e. Proof. See Appendix B.2.\nThe full soft policy iteration algorithm alternates between the soft policy evaluation and the soft policy improvement steps, and it will provably converge to the optimal maximum entropy policy among the policies in \u03a0 (Theorem 1).\n\nAlthough this algorithm will provably find the optimal solution, we can perform it in its exact form only in the tabular case. Therefore, we will next approximate the algorithm for continuous domains, where we need to rely on a function approximator to represent the Q-values, and running the two steps until convergence would be computationally too expensive. The approximation gives rise to a new practical algorithm, called soft actor-critic.\n\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement from any \u03c0 \u2208 \u03a0 converges to a policy \u03c0 * such that Q \u03c0 * (s t , a t ) \u2265 Q \u03c0 (s t , a t ) for all \u03c0 \u2208 \u03a0 and (s t , a t ) \u2208 S \u00d7 A, assuming |A| < \u221e.\n\nProof. See Appendix B.3.\n\n\nSoft Actor-Critic\n\nAs discussed above, large continuous domains require us to derive a practical approximation to soft policy iteration. To that end, we will use function approximators for both the Q-function and the policy, and instead of running evaluation and improvement to convergence, alternate between optimizing both networks with stochastic gradient descent. We will consider a parameterized state value function V \u03c8 (s t ), soft Q-function Q \u03b8 (s t , a t ), and a tractable policy \u03c0 \u03c6 (a t |s t ).\n\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For example, the value functions can be modeled as expressive neural networks, and the policy as a Gaussian with mean and covariance given by neural networks. We will next derive update rules for these parameter vectors.\n\nThe state value function approximates the soft value. There is no need in principle to include a separate function approximator for the state value, since it is related to the Q-function and policy according to Equation 3. This quantity can be estimated from a single action sample from the current policy without introducing a bias, but in practice, including a separate function approximator for the soft value can stabilize training and is convenient to train simultaneously with the other networks. The soft value function is trained to minimize the squared residual error\nJ V (\u03c8) = E st\u223cD 1 2 V \u03c8 (s t ) \u2212 E at\u223c\u03c0\u03c6 [Q \u03b8 (s t , a t ) \u2212 log \u03c0 \u03c6 (a t |s t )] 2(5)\nwhere D is the distribution of previously sampled states and actions, or a replay buffer. The gradient of Equation 5 can be estimated with an unbiased estimator\n\u2207 \u03c8 J V (\u03c8) = \u2207 \u03c8 V \u03c8 (s t ) (V \u03c8 (s t ) \u2212 Q \u03b8 (s t , a t ) + log \u03c0 \u03c6 (a t |s t )) ,(6)\nwhere the actions are sampled according to the current policy, instead of the replay buffer. The soft Q-function parameters can be trained to minimize the soft Bellman residual\nJ Q (\u03b8) = E (st,at)\u223cD 1 2 Q \u03b8 (s t , a t ) \u2212Q(s t , a t ) 2 ,(7)\nwithQ\n(s t , a t ) = r(s t , a t ) + \u03b3 E st+1\u223cp V\u03c8(s t+1 ) ,(8)\nwhich again can be optimized with stochastic gradient\u015d\n\u2207 \u03b8 J Q (\u03b8) = \u2207 \u03b8 Q \u03b8 (a t , s t ) Q \u03b8 (s t , a t ) \u2212 r(s t , a t ) \u2212 \u03b3V\u03c8(s t+1 ) .(9)\nThe update makes use of a target value network V\u03c8, wher\u0113 \u03c8 can be an exponentially moving average of the value network weights, which has been shown to stabilize training (Mnih et al., 2015). Alternatively, we can update the target weights to match the current value function weights periodically (see Appendix E). Finally, the policy parameters can be learned by directly minimizing the expected KL-divergence in Equation 4:\nJ \u03c0 (\u03c6) = E st\u223cD D KL \u03c0 \u03c6 ( \u00b7 |s t ) exp (Q \u03b8 (s t , \u00b7 )) Z \u03b8 (s t ) .(10)\nThere are several options for minimizing J \u03c0 . A typical solution for policy gradient methods is to use the likelihood ratio gradient estimator (Williams, 1992), which does not require backpropagating the gradient through the policy and the target density networks. However, in our case, the target density is the Q-function, which is represented by a neural network an can be differentiated, and it is thus convenient to apply the reparameterization trick instead, resulting in a lower variance estimator. To that end, we reparameterize the policy using a neural network transformation\na t = f \u03c6 ( t ; s t ),(11)\nAlgorithm 1 Soft Actor-Critic\n\nInitialize parameter vectors \u03c8,\u03c8, \u03b8, \u03c6. for each iteration do for each environment step do a t \u223c \u03c0 \u03c6 (a t |s t )\ns t+1 \u223c p(s t+1 |s t , a t ) D \u2190 D \u222a {(s t , a t , r(s t , a t ), s t+1 )} end for for each gradient step do \u03c8 \u2190 \u03c8 \u2212 \u03bb V\u2207\u03c8 J V (\u03c8) \u03b8 i \u2190 \u03b8 i \u2212 \u03bb Q\u2207\u03b8i J Q (\u03b8 i ) for i \u2208 {1, 2} \u03c6 \u2190 \u03c6 \u2212 \u03bb \u03c0\u2207\u03c6 J \u03c0 (\u03c6) \u03c8 \u2190 \u03c4 \u03c8 + (1 \u2212 \u03c4 )\u03c8 end for end for\nwhere t is an input noise vector, sampled from some fixed distribution, such as a spherical Gaussian. We can now rewrite the objective in Equation 10 as\nJ \u03c0 (\u03c6) = E st\u223cD, t\u223cN [log \u03c0 \u03c6 (f \u03c6 ( t ; s t )|s t ) \u2212 Q \u03b8 (s t , f \u03c6 ( t ; s t ))] ,(12)\nwhere \u03c0 \u03c6 is defined implicitly in terms of f \u03c6 , and we have noted that the partition function is independent of \u03c6 and can thus be omitted. We can approximate the gradient of Equation 12 wit\u0125\n\u2207 \u03c6 J \u03c0 (\u03c6) = \u2207 \u03c6 log \u03c0 \u03c6 (a t |s t ) + (\u2207 at log \u03c0 \u03c6 (a t |s t ) \u2212 \u2207 at Q(s t , a t ))\u2207 \u03c6 f \u03c6 ( t ; s t ),(13)\nwhere a t is evaluated at f \u03c6 ( t ; s t ). This unbiased gradient estimator extends the DDPG style policy gradients  to any tractable stochastic policy.\n\nOur algorithm also makes use of two Q-functions to mitigate positive bias in the policy improvement step that is known to degrade performance of value based methods (Hasselt, 2010;Fujimoto et al., 2018). In particular, we parameterize two Q-functions, with parameters \u03b8 i , and train them independently to optimize J Q (\u03b8 i ). We then use the minimum of the Q-functions for the value gradient in Equation 6 and policy gradient in Equation 13, as proposed by Fujimoto et al. (2018). Although our algorithm can learn challenging tasks, including a 21-dimensional Humanoid, using just a single Q-function, we found two Q-functions significantly speed up training, especially on harder tasks. The complete algorithm is described in Algorithm 1. The method alternates between collecting experience from the environment with the current policy and updating the function approximators using the stochastic gradients from batches sampled from a replay buffer. In practice, we take a single environment step followed by one or several gradient steps (see Appendix D for all hyperparameter). Using off-policy data from a replay buffer is feasible because both value estimators and the policy can be trained entirely on off-policy data. The algorithm is agnostic to the parameterization of the policy, as long as it can be evaluated for any arbitrary state-action tuple.\n\n\nExperiments\n\nThe goal of our experimental evaluation is to understand how the sample complexity and stability of our method compares with prior off-policy and on-policy deep reinforcement learning algorithms. We compare our method to prior techniques on a range of challenging continuous control tasks from the OpenAI gym benchmark suite (Brockman et al., 2016) and also on the rllab implementation of the Humanoid task (Duan et al., 2016). Although the easier tasks can be solved by a wide range of different algorithms, the more complex benchmarks, such as the 21-dimensional Humanoid (rllab), are exceptionally difficult to solve with off-policy algorithms (Duan et al., 2016). The stability of the algorithm also plays a large role in performance: easier tasks make it more practical to tune hyperparameters to achieve good results, while the already narrow basins of effective hyperparameters become prohibitively small for the more sensitive algorithms on the hardest benchmarks, leading to poor performance (Gu et al., 2016).\n\nWe compare our method to deep deterministic policy gradient (DDPG) , an algorithm that is regarded as one of the more efficient off-policy deep RL methods (Duan et al., 2016); proximal policy optimization (PPO) (Schulman et al., 2017b), a stable and effective on-policy policy gradient algorithm; and soft Q-learning (SQL) (Haarnoja et al., 2017), a recent off-policy algorithm for learning maximum entropy policies. Our SQL implementation also includes two Q-functions, which we found to improve its performance in most environments. We additionally compare to twin delayed deep deterministic policy gradient algorithm (TD3) (Fujimoto et al., 2018), using the author-provided implementation. This is an extension to DDPG, proposed concurrently to our method, that first applied the double Q-learning trick to continuous control along with other improvements. We have included trust region path consistency learning (Trust-PCL) (Nachum et al., 2017b) and two other variants of SAC in Appendix E. We turned off the exploration noise for evaluation for DDPG and PPO. For maximum entropy algorithms, which do not explicitly inject exploration noise, we either evaluated with the exploration noise (SQL) or use the mean action (SAC). The source code of our SAC implementation 1 and videos 2 are available online. Figure 1 shows the total average return of evaluation rollouts during training for DDPG, PPO, and TD3. We train five different instances of each algorithm with different random seeds, with each performing one evaluation rollout every 1000 environment steps. The solid curves corresponds to the mean and the shaded region to the minimum and maximum returns over the five trials.\n\n\nComparative Evaluation\n\nThe results show that, overall, SAC performs comparably to the baseline methods on the easier tasks and outperforms them on the harder tasks with a large margin, both in terms of learning speed and the final performance. For example, DDPG fails to make any progress on Ant-v1, Humanoid-v1, and Humanoid (rllab), a result that is corroborated by prior work (Gu et al., 2016;Duan et al., 2016). SAC also learns considerably faster than PPO as a consequence of the large batch sizes PPO needs to learn stably on more high-dimensional and complex tasks. Another maximum entropy RL algorithm, SQL, can also learn all tasks, but it is slower than SAC and has worse asymptotic performance. The quantitative results attained by SAC in our experiments also compare very favorably to results reported by other methods in prior work (Duan et al., 2016;Gu et al., 2016;Henderson et al., 2017), indicating that both the sample efficiency and final performance of SAC on these benchmark tasks exceeds the state of the art. All hyperparameters used in this experiment for SAC are listed in Appendix D.\n\n\nAblation Study\n\nThe results in the previous section suggest that algorithms based on the maximum entropy principle can outperform conventional RL methods on challenging tasks such as the humanoid tasks. In this section, we further examine which particular components of SAC are important for good performance. We also examine how sensitive SAC is to some of the most important hyperparameters, namely reward scaling and target value update smoothing constant.\n\nStochastic vs. deterministic policy. Soft actor-critic learns stochastic policies via a maximum entropy objective. The entropy appears in both the policy and value function. In the policy, it prevents premature convergence of the policy variance (Equation 10). In the value function, it encourages exploration by increasing the value of regions of state space that lead to high-entropy behavior (Equation 5).\n\nTo compare how the stochasticity of the policy and entropy maximization affects the performance, we compare to a deterministic variant of SAC that does not maximize the entropy and that closely resembles DDPG, with the exception of having two Q-functions, using hard target updates, not having a separate target actor, and using fixed rather than learned exploration noise. Figure 2 compares five individual runs with both variants, initialized with different random seeds. Soft actor-critic performs much more consistently, while the deterministic variant exhibits very high variability across seeds, indicating substantially worse stability. As evident from the figure, learning a stochastic policy with entropy maximization can drastically stabilize training. This becomes especially important with harder tasks, where tuning hyperparameters is challenging. In this comparison, we updated the target value network weights with hard updates, by periodically overwriting the target network parameters to match the current value network (see Appendix E for a comparison of average performance on all benchmark tasks).\n\nPolicy evaluation. Since SAC converges to stochastic policies, it is often beneficial to make the final policy deterministic at the end for best performance. For evaluation, we approximate the maximum a posteriori action by choosing the mean of the policy distribution. Figure 3(a) compares training returns to evaluation returns obtained with this strategy indicating that deterministic evaluation can yield better performance. It should be noted that all of the training curves depict the sum of rewards, which is different from the objective optimized by SAC and other maximum entropy RL algorithms, including SQL and Trust-PCL, which maximize also the entropy of the policy.\n\nReward scale. Soft actor-critic is particularly sensitive to the scaling of the reward signal, because it serves the role of the temperature of the energy-based optimal policy and thus controls its stochasticity. Larger reward magnitudes correspond to lower entries. Figure 3(b) shows how learning performance changes when the reward scale is varied: For small reward magnitudes, the policy becomes nearly uniform, and consequently fails to exploit the reward signal, resulting in substantial degradation of performance. For large reward magnitudes, the model learns quickly at first,  Figure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action generally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general, correspond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the temperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n\n(c) Target value smoothing coefficient \u03c4 is used to stabilize training. Fast moving target (large \u03c4 ) can result in instabilities (red), whereas slow moving target (small \u03c4 ) makes training slower (blue).\n\nbut the policy then becomes nearly deterministic, leading to poor local minima due to lack of adequate exploration. With the right reward scaling, the model balances exploration and exploitation, leading to faster learning and better asymptotic performance. In practice, we found reward scale to be the only hyperparameter that requires tuning, and its natural interpretation as the inverse of the temperature in the maximum entropy framework provides good intuition for how to adjust this parameter.\n\nTarget network update. It is common to use a separate target value network that slowly tracks the actual value function to improve stability. We use an exponentially moving average, with a smoothing constant \u03c4 , to update the target value network weights as common in the prior work Mnih et al., 2015). A value of one corresponds to a hard update where the weights are copied directly at every iteration and zero to not updating the target at all. In Figure 3(c), we compare the performance of SAC when \u03c4 varies. Large \u03c4 can lead to instabilities while small \u03c4 can make training slower. However, we found the range of suitable values of \u03c4 to be relatively wide and we used the same value (0.005) across all of the tasks. In Figure 4 (Appendix E) we also compare to another variant of SAC, where instead of using exponentially moving average, we copy over the current network weights directly into the target network every 1000 gradient steps. We found this variant to benefit from taking more than one gradient step between the environment steps, which can improve performance but also increases the computational cost.\n\n\nConclusion\n\nWe present soft actor-critic (SAC), an off-policy maximum entropy deep reinforcement learning algorithm that provides sample-efficient learning while retaining the benefits of entropy maximization and stability. Our theoretical results derive soft policy iteration, which we show to converge to the optimal policy. From this result, we can formulate a soft actor-critic algorithm, and we empirically show that it outperforms state-of-the-art model-free deep RL methods, including the off-policy DDPG algorithm and the on-policy PPO algorithm. In fact, the sample efficiency of this approach actually exceeds that of DDPG by a substantial margin. Our results suggest that stochastic, entropy maximizing reinforcement learning algorithms can provide a promising avenue for improved robustness and stability, and further exploration of maximum entropy methods, including methods that incorporate second order information (e.g., trust regions (Schulman et al., 2015)) or more expressive policy classes is an exciting avenue for future work.\n\n\nA. Maximum Entropy Objective\n\nThe exact definition of the discounted maximum entropy objective is complicated by the fact that, when using a discount factor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense, discounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward, with the discount serving to reduce variance, as discussed by Thomas (2014). However, we can define the objective that is optimized under a discount factor as\nJ(\u03c0) = \u221e t=0 E (st,at)\u223c\u03c1\u03c0 \u221e l=t \u03b3 l\u2212t E s l \u223cp,a l \u223c\u03c0 [r(s t , a t ) + \u03b1H(\u03c0( \u00b7 |s t ))|s t , a t ] .(14)\nThis objective corresponds to maximizing the discounted expected reward and entropy for future states originating from every state-action tuple (s t , a t ) weighted by its probability \u03c1 \u03c0 under the current policy.\n\nB. Proofs B.1. Lemma 1\n\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping Q 0 : S \u00d7 A \u2192 R with |A| < \u221e, and define Q k+1 = T \u03c0 Q k . Then the sequence Q k will converge to the soft Q-value of \u03c0 as k \u2192 \u221e.\n\nProof. Define the entropy augmented reward as r \u03c0 (s t , a t ) r(s t , a t ) + E st+1\u223cp [H (\u03c0( \u00b7 |s t+1 ))] and rewrite the update rule as\nQ(s t , a t ) \u2190 r \u03c0 (s t , a t ) + \u03b3 E st+1\u223cp,at+1\u223c\u03c0 [Q(s t+1 , a t+1 )](15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221e is required to guarantee that the entropy augmented reward is bounded.\n\n\nB.2. Lemma 2\n\nLemma 2 (Soft Policy Improvement). Let \u03c0 old \u2208 \u03a0 and let \u03c0 new be the optimizer of the minimization problem defined in Equation 4. Then Q \u03c0new (s t , a t ) \u2265 Q \u03c0 old (s t , a t ) for all (s t , a t ) \u2208 S \u00d7 A with |A| < \u221e.\n\nProof. Let \u03c0 old \u2208 \u03a0 and let Q \u03c0 old and V \u03c0 old be the corresponding soft state-action value and soft state value, and let \u03c0 new be defined as \u03c0 new ( \u00b7 |s t ) = arg min \u03c0 \u2208\u03a0 D KL (\u03c0 ( \u00b7 |s t ) exp (Q \u03c0 old (s t , \u00b7 ) \u2212 log Z \u03c0 old (s t ))) = arg min \u03c0 \u2208\u03a0 J \u03c0 old (\u03c0 ( \u00b7 |s t )).\n\nIt must be the case that J \u03c0 old (\u03c0 new ( \u00b7 |s t )) \u2264 J \u03c0 old (\u03c0 old ( \u00b7 |s t )), since we can always choose \u03c0 new = \u03c0 old \u2208 \u03a0. Hence\nE a t \u223c\u03c0 new [log \u03c0 new (a t |s t ) \u2212 Q \u03c0 old (s t , a t ) + log Z \u03c0 old (s t )] \u2264 E a t \u223c\u03c0 old [log \u03c0 old (a t |s t ) \u2212 Q \u03c0 old (s t , a t ) + log Z \u03c0 old (s t )],(17)\nand since partition function Z \u03c0 old depends only on the state, the inequality reduces to\nE at\u223c\u03c0new [Q \u03c0 old (s t , a t ) \u2212 log \u03c0 new (a t |s t )] \u2265 V \u03c0 old (s t ).(18)\nNext, consider the soft Bellman equation:\nQ \u03c0 old (s t , a t ) = r(s t , a t ) + \u03b3 E st+1\u223cp [V \u03c0 old (s t+1 )] \u2264 r(s t , a t ) + \u03b3 E st+1\u223cp E at+1\u223c\u03c0new [Q \u03c0 old (s t+1 , a t+1 ) \u2212 log \u03c0 new (a t+1 |s t+1 )]\n. . .\n\u2264 Q \u03c0new (s t , a t ),(19)\nwhere we have repeatedly expanded Q \u03c0 old on the RHS by applying the soft Bellman equation and the bound in Equation 18. Convergence to Q \u03c0new follows from Lemma 1.\n\n\nB.3. Theorem 1\n\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208 \u03a0 converges to a policy \u03c0 * such that Q \u03c0 * (s t , a t ) \u2265 Q \u03c0 (s t , a t ) for all \u03c0 \u2208 \u03a0 and (s t , a t ) \u2208 S \u00d7 A, assuming |A| < \u221e.\n\nProof. Let \u03c0 i be the policy at iteration i. By Lemma 2, the sequence Q \u03c0i is monotonically increasing. Since Q \u03c0 is bounded above for \u03c0 \u2208 \u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0 * . We will still need to show that \u03c0 * is indeed optimal. At convergence, it must be case that J \u03c0 * (\u03c0 * ( \u00b7 |s t )) < J \u03c0 * (\u03c0( \u00b7 |s t )) for all \u03c0 \u2208 \u03a0, \u03c0 = \u03c0 * .\n\nUsing the same iterative argument as in the proof of Lemma 2, we get Q \u03c0 * (s t , a t ) > Q \u03c0 (s t , a t ) for all (s t , a t ) \u2208 S \u00d7 A, that is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0 * is optimal in \u03a0.\n\n\nC. Enforcing Action Bounds\n\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a finite interval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of variables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208 R D be a random variable and \u00b5(u|s) the corresponding density with infinite support. Then a = tanh(u), where tanh is applied elementwise, is a random variable with support in (\u22121, 1) with a density given by \u03c0(a|s) = \u00b5(u|s) det da du \u22121 .\n\nSince the Jacobian da /du = diag(1 \u2212 tanh 2 (u)) is diagonal, the log-likelihood has a simple form log \u03c0(a|s) = log \u00b5(u|s) \u2212\nD i=1 log 1 \u2212 tanh 2 (u i ) ,(21)\nwhere u i is the i th element of u. Table 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the reward scale parameter that was tuned for each environment. \n\n\nD. Hyperparameters\n\n\nE. Additional Baseline Results\n\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of environment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The figure also includes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using exponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update (Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two Q-functions, using hard target updates, not having a separate target actor, and using fixed exploration noise rather than learned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab) task, on which SAC is the fastest.  . Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue) differs from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially smoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with fixed Gaussian exploration noise, does not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target Q-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.\n\nFigure 1 .\n1Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and outperforming both on-policy and off-policy methods in the most challenging tasks.\n\nFigure 2 .\n2Comparison of SAC (blue) and a deterministic variant of SAC (red) in terms of the stability of individual random seeds on the Humanoid (rllab) benchmark. The comparison indicates that stochasticity can stabilize training as the variability between the seeds becomes much higher with a deterministic policy.\n\nFigure 4\n4Figure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue) differs from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially smoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with fixed Gaussian exploration noise, does not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target Q-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.\n\nTable 1 .\n1SAC HyperparametersTable 2. SAC Environment Specific ParametersParameter \nValue \n\nShared \noptimizer \nAdam (Kingma & Ba, 2015) \nlearning rate \n3 \u00b7 10 \u22124 \ndiscount (\u03b3) \n0.99 \nreplay buffer size \n10 6 \nnumber of hidden layers (all networks) 2 \nnumber of hidden units per layer \n256 \nnumber of samples per minibatch \n256 \nnonlinearity \nReLU \n\nSAC \ntarget smoothing coefficient (\u03c4 ) \n0.005 \ntarget update interval \n1 \ngradient steps \n1 \n\nSAC (hard target update) \ntarget smoothing coefficient (\u03c4 ) \n1 \ntarget update interval \n1000 \ngradient steps (except humanoids) \n4 \ngradient steps (humanoids) \n1 \n\nEnvironment \nAction Dimensions Reward Scale \n\nHopper-v1 \n3 \n5 \nWalker2d-v1 \n6 \n5 \nHalfCheetah-v1 \n6 \n5 \nAnt-v1 \n8 \n5 \nHumanoid-v1 \n17 \n20 \nHumanoid (rllab) 21 \n10 \n\nBerkeley Artificial Intelligence Research, University of California, Berkeley, USA. Correspondence to: Tuomas Haarnoja <haarnoja@berkeley.edu>.\n. From Soft Policy Iteration to Soft Actor-Critic Our off-policy soft actor-critic algorithm can be derived starting from a maximum entropy variant of the policy iteration method. We will first present this derivation, verify that the corresponding algorithm converges to the optimal policy from its density class, and then present a practical deep reinforcement learning algorithm based on this theory.\ngithub.com/haarnoja/sac 2 sites.google.com/view/soft-actor-critic\nAcknowledgmentsWe would like to thank Vitchyr Pong for insightful discussions and help in implementing our algorithm as well as providing the DDPG baseline code; Ofir Nachum for offering support in running Trust-PCL experiments; and George Tucker for his valuable feedback on an early version of this paper. This work was supported by Siemens and Berkeley DeepDrive.\nNeuronlike adaptive elements that can solve difficult learning control problems. A G Barto, R S Sutton, Anderson , C W , IEEE transactions on systems, man, and cybernetics. Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike adaptive elements that can solve difficult learning con- trol problems. IEEE transactions on systems, man, and cybernetics, pp. 834-846, 1983.\n\nConvergent temporal-difference learning with arbitrary smooth function approximation. S Bhatnagar, D Precup, D Silver, R S Sutton, H R Maei, C Szepesv\u00e1ri, Advances in Neural Information Processing Systems (NIPS). Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei, H. R., and Szepesv\u00e1ri, C. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems (NIPS), pp. 1204-1212, 2009.\n\n. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, Zaremba , arXiv:1606.01540W. OpenAI gym. arXiv preprintBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.\n\nBenchmarking deep reinforcement learning for continuous control. Y Duan, X Chen, R Houthooft, J Schulman, Abbeel , P , International Conference on Machine Learning (ICML). Duan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel, P. Benchmarking deep reinforcement learning for contin- uous control. In International Conference on Machine Learning (ICML), 2016.\n\nTaming the noise in reinforcement learning via soft updates. R Fox, A Pakman, N Tishby, Conference on Uncertainty in Artificial Intelligence (UAI). Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In Conference on Uncertainty in Artificial Intelligence (UAI), 2016.\n\nAddressing function approximation error in actor-critic methods. S Fujimoto, H Van Hoof, D Meger, arXiv:1802.09477arXiv preprintFujimoto, S., van Hoof, H., and Meger, D. Addressing func- tion approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.\n\nThe reactor: A sample-efficient actor-critic architecture. A Gruslys, M G Azar, M G Bellemare, R Munos, arXiv:1704.04651arXiv preprintGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R. The reactor: A sample-efficient actor-critic architecture. arXiv preprint arXiv:1704.04651, 2017.\n\nS Gu, T Lillicrap, Z Ghahramani, R E Turner, S Levine, arXiv:1611.02247Sample-efficient policy gradient with an off-policy critic. arXiv preprintGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. Q-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.\n\nReinforcement learning with deep energy-based policies. T Haarnoja, H Tang, P Abbeel, S Levine, International Conference on Machine Learning (ICML). Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein- forcement learning with deep energy-based policies. In International Conference on Machine Learning (ICML), pp. 1352-1361, 2017.\n\nDouble Q-learning. H V Hasselt, Advances in Neural Information Processing Systems (NIPS). Hasselt, H. V. Double Q-learning. In Advances in Neural Information Processing Systems (NIPS), pp. 2613-2621, 2010.\n\nLearning continuous control policies by stochastic value gradients. N Heess, G Wayne, D Silver, T Lillicrap, T Erez, Y Tassa, Advances in Neural Information Processing Systems (NIPS). Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and Tassa, Y. Learning continuous control policies by stochas- tic value gradients. In Advances in Neural Information Processing Systems (NIPS), pp. 2944-2952, 2015.\n\nP Henderson, R Islam, P Bachman, J Pineau, D Precup, D Meger, arXiv:1709.06560Deep reinforcement learning that matters. arXiv preprintHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.\n\nA method for stochastic optimization. D Kingma, J Ba, Adam, International Conference for Learning Presentations (ICLR). Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference for Learning Presentations (ICLR), 2015.\n\nGuided policy search. S Levine, V Koltun, International Conference on Machine Learning (ICML). Levine, S. and Koltun, V. Guided policy search. In Interna- tional Conference on Machine Learning (ICML), pp. 1-9, 2013.\n\nEnd-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, Abbeel , P , Journal of Machine Learning Research. 1739Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1-40, 2016.\n\nT P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nPlaying atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602arXiv preprintMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nHuman-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529-533, 2015.\n\nAsynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, International Conference on Machine Learning (ICML). Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In International Conference on Machine Learning (ICML), 2016.\n\nBridging the gap between value and policy based reinforcement learning. O Nachum, M Norouzi, K Xu, D Schuurmans, Advances in Neural Information Processing Systems (NIPS). Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Bridging the gap between value and policy based rein- forcement learning. In Advances in Neural Information Processing Systems (NIPS), pp. 2772-2782, 2017a.\n\nTrust-PCL: An off-policy trust region method for continuous control. O Nachum, M Norouzi, K Xu, D Schuurmans, arXiv:1707.01891arXiv preprintNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Trust-PCL: An off-policy trust region method for contin- uous control. arXiv preprint arXiv:1707.01891, 2017b.\n\nB O&apos;donoghue, R Munos, K Kavukcuoglu, V Mnih, Pgq, arXiv:1611.01626Combining policy gradient and Q-learning. arXiv preprintO'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. PGQ: Combining policy gradient and Q-learning. arXiv preprint arXiv:1611.01626, 2016.\n\nReinforcement learning of motor skills with policy gradients. J Peters, S Schaal, Neural networks. 214Peters, J. and Schaal, S. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682- 697, 2008.\n\nOn stochastic optimal control and reinforcement learning by approximate inference. K Rawlik, RSSM Toussaint, RSSS Vijayakumar, RSSRobotics: Science and Systems. Rawlik, K., Toussaint, M., and Vijayakumar, S. On stochas- tic optimal control and reinforcement learning by approx- imate inference. Robotics: Science and Systems (RSS), 2012.\n\nTrust region policy optimization. J Schulman, S Levine, P Abbeel, M I Jordan, P Moritz, International Conference on Machine Learning (ICML). Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and Moritz, P. Trust region policy optimization. In Inter- national Conference on Machine Learning (ICML), pp. 1889-1897, 2015.\n\nSoft Actor-Critic. Soft Actor-Critic\n\nEquivalence between policy gradients and soft Q-learning. J Schulman, P Abbeel, Chen , X , arXiv:1704.06440arXiv preprintSchulman, J., Abbeel, P., and Chen, X. Equivalence be- tween policy gradients and soft Q-learning. arXiv preprint arXiv:1704.06440, 2017a.\n\nJ Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.\n\nDeterministic policy gradient algorithms. D Silver, G Lever, N Heess, T Degris, D Wierstra, M Riedmiller, International Conference on Machine Learning (ICML). Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy gradient algo- rithms. In International Conference on Machine Learning (ICML), 2014.\n\nMastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, 0028-0836. ArticleNature. 5297587Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, Jan 2016. ISSN 0028-0836. Article.\n\nReinforcement learning: An introduction. R S Sutton, A G Barto, MIT press Cambridge1Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.\n\nBias in natural actor-critic algorithms. P Thomas, International Conference on Machine Learning (ICML). Thomas, P. Bias in natural actor-critic algorithms. In Inter- national Conference on Machine Learning (ICML), pp. 441-448, 2014.\n\nGeneral duality between optimal control and estimation. E Todorov, IEEE Conference on Decision and Control (CDC). IEEETodorov, E. General duality between optimal control and estimation. In IEEE Conference on Decision and Control (CDC), pp. 4286-4292. IEEE, 2008.\n\nRobot trajectory optimization using approximate inference. M Toussaint, International Conference on Machine Learning (ICML). ACMToussaint, M. Robot trajectory optimization using approxi- mate inference. In International Conference on Machine Learning (ICML), pp. 1049-1056. ACM, 2009.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.\n\nModeling purposeful adaptive behavior with the principle of maximum causal entropy. B D Ziebart, Carnegie Mellon UniversityZiebart, B. D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mel- lon University, 2010.\n\nMaximum entropy inverse reinforcement learning. B D Ziebart, A L Maas, J A Bagnell, A K Dey, AAAI Conference on Artificial Intelligence (AAAI). Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), pp. 1433-1438, 2008.\n", "annotations": {"author": "[{\"end\":117,\"start\":101},{\"end\":130,\"start\":118},{\"end\":145,\"start\":131},{\"end\":160,\"start\":146}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":108},{\"end\":129,\"start\":125},{\"end\":144,\"start\":138},{\"end\":159,\"start\":153}]", "author_first_name": "[{\"end\":107,\"start\":101},{\"end\":124,\"start\":118},{\"end\":137,\"start\":131},{\"end\":152,\"start\":146}]", "author_affiliation": null, "title": "[{\"end\":98,\"start\":1},{\"end\":258,\"start\":161}]", "venue": null, "abstract": "[{\"end\":1501,\"start\":260}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1655,\"start\":1636},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1675,\"start\":1655},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1718,\"start\":1695},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2746,\"start\":2723},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2776,\"start\":2752},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3224,\"start\":3205},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3436,\"start\":3413},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3850,\"start\":3831},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3873,\"start\":3850},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4180,\"start\":4158},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4196,\"start\":4180},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4216,\"start\":4196},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4233,\"start\":4216},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4255,\"start\":4233},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4421,\"start\":4398},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4569,\"start\":4555},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4697,\"start\":4674},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4893,\"start\":4868},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4991,\"start\":4967},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5012,\"start\":4991},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5034,\"start\":5012},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5572,\"start\":5553},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5697,\"start\":5680},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5889,\"start\":5866},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6446,\"start\":6423},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7196,\"start\":7176},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7217,\"start\":7196},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7630,\"start\":7607},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7783,\"start\":7759},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7801,\"start\":7783},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7822,\"start\":7801},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8112,\"start\":8087},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8128,\"start\":8112},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8343,\"start\":8322},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8795,\"start\":8776},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8818,\"start\":8795},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9016,\"start\":8999},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9914,\"start\":9892},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9949,\"start\":9934},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9965,\"start\":9949},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9985,\"start\":9965},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10034,\"start\":10011},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10054,\"start\":10034},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10325,\"start\":10300},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10347,\"start\":10325},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10368,\"start\":10347},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10391,\"start\":10368},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10487,\"start\":10466},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10575,\"start\":10553},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10722,\"start\":10700},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10978,\"start\":10956},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13944,\"start\":13921},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13967,\"start\":13944},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14404,\"start\":14390},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14579,\"start\":14557},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14596,\"start\":14579},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14618,\"start\":14596},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21411,\"start\":21392},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21882,\"start\":21866},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23597,\"start\":23582},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23619,\"start\":23597},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23897,\"start\":23875},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25140,\"start\":25117},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25218,\"start\":25199},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25458,\"start\":25439},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25810,\"start\":25793},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25987,\"start\":25968},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26048,\"start\":26024},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26159,\"start\":26136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26462,\"start\":26439},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26763,\"start\":26741},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27899,\"start\":27882},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27917,\"start\":27899},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28367,\"start\":28348},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28383,\"start\":28367},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28406,\"start\":28383},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33437,\"start\":33419},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35232,\"start\":35209},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":35780,\"start\":35767},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36751,\"start\":36729},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40406,\"start\":40384},{\"end\":40689,\"start\":40678}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41949,\"start\":41734},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42269,\"start\":41950},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42949,\"start\":42270},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43723,\"start\":42950}]", "paragraph": "[{\"end\":2574,\"start\":1517},{\"end\":3874,\"start\":2576},{\"end\":5243,\"start\":3876},{\"end\":6551,\"start\":5245},{\"end\":7908,\"start\":6568},{\"end\":9672,\"start\":7910},{\"end\":11776,\"start\":9674},{\"end\":11903,\"start\":11794},{\"end\":12559,\"start\":11916},{\"end\":12886,\"start\":12602},{\"end\":13469,\"start\":12953},{\"end\":14435,\"start\":13471},{\"end\":15072,\"start\":14437},{\"end\":15685,\"start\":15112},{\"end\":16022,\"start\":15687},{\"end\":16285,\"start\":16149},{\"end\":16523,\"start\":16287},{\"end\":16549,\"start\":16525},{\"end\":17364,\"start\":16551},{\"end\":17865,\"start\":17449},{\"end\":17997,\"start\":17867},{\"end\":18344,\"start\":18114},{\"end\":18791,\"start\":18346},{\"end\":19049,\"start\":18793},{\"end\":19075,\"start\":19051},{\"end\":19585,\"start\":19097},{\"end\":19857,\"start\":19587},{\"end\":20435,\"start\":19859},{\"end\":20684,\"start\":20524},{\"end\":20949,\"start\":20773},{\"end\":21020,\"start\":21015},{\"end\":21133,\"start\":21079},{\"end\":21646,\"start\":21221},{\"end\":22308,\"start\":21722},{\"end\":22365,\"start\":22336},{\"end\":22479,\"start\":22367},{\"end\":22866,\"start\":22714},{\"end\":23150,\"start\":22958},{\"end\":23415,\"start\":23263},{\"end\":24776,\"start\":23417},{\"end\":25811,\"start\":24792},{\"end\":27499,\"start\":25813},{\"end\":28612,\"start\":27526},{\"end\":29074,\"start\":28631},{\"end\":29484,\"start\":29076},{\"end\":30603,\"start\":29486},{\"end\":31283,\"start\":30605},{\"end\":32426,\"start\":31285},{\"end\":32632,\"start\":32428},{\"end\":33134,\"start\":32634},{\"end\":34255,\"start\":33136},{\"end\":35306,\"start\":34270},{\"end\":35863,\"start\":35339},{\"end\":36183,\"start\":35969},{\"end\":36207,\"start\":36185},{\"end\":36446,\"start\":36209},{\"end\":36586,\"start\":36448},{\"end\":36846,\"start\":36664},{\"end\":37084,\"start\":36863},{\"end\":37366,\"start\":37086},{\"end\":37501,\"start\":37368},{\"end\":37760,\"start\":37671},{\"end\":37881,\"start\":37840},{\"end\":38052,\"start\":38047},{\"end\":38244,\"start\":38080},{\"end\":38517,\"start\":38263},{\"end\":38901,\"start\":38519},{\"end\":39159,\"start\":38903},{\"end\":39769,\"start\":39190},{\"end\":39895,\"start\":39771},{\"end\":40145,\"start\":39930},{\"end\":41733,\"start\":40201}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12952,\"start\":12887},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16148,\"start\":16023},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17448,\"start\":17365},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18113,\"start\":17998},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20523,\"start\":20436},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20772,\"start\":20685},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21014,\"start\":20950},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21078,\"start\":21021},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21220,\"start\":21134},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21721,\"start\":21647},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22335,\"start\":22309},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22713,\"start\":22480},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22957,\"start\":22867},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23262,\"start\":23151},{\"attributes\":{\"id\":\"formula_14\"},\"end\":35968,\"start\":35864},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36663,\"start\":36587},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37670,\"start\":37502},{\"attributes\":{\"id\":\"formula_18\"},\"end\":37839,\"start\":37761},{\"attributes\":{\"id\":\"formula_19\"},\"end\":38046,\"start\":37882},{\"attributes\":{\"id\":\"formula_20\"},\"end\":38079,\"start\":38053},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39929,\"start\":39896}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":39973,\"start\":39966},{\"end\":40074,\"start\":40067}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1515,\"start\":1503},{\"attributes\":{\"n\":\"2.\"},\"end\":6566,\"start\":6554},{\"attributes\":{\"n\":\"3.\"},\"end\":11792,\"start\":11779},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11914,\"start\":11906},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12600,\"start\":12562},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15110,\"start\":15075},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19095,\"start\":19078},{\"attributes\":{\"n\":\"5.\"},\"end\":24790,\"start\":24779},{\"attributes\":{\"n\":\"5.1.\"},\"end\":27524,\"start\":27502},{\"attributes\":{\"n\":\"5.2.\"},\"end\":28629,\"start\":28615},{\"attributes\":{\"n\":\"6.\"},\"end\":34268,\"start\":34258},{\"end\":35337,\"start\":35309},{\"end\":36861,\"start\":36849},{\"end\":38261,\"start\":38247},{\"end\":39188,\"start\":39162},{\"end\":40166,\"start\":40148},{\"end\":40199,\"start\":40169},{\"end\":41745,\"start\":41735},{\"end\":41961,\"start\":41951},{\"end\":42279,\"start\":42271},{\"end\":42960,\"start\":42951}]", "table": "[{\"end\":43723,\"start\":43025}]", "figure_caption": "[{\"end\":41949,\"start\":41747},{\"end\":42269,\"start\":41963},{\"end\":42949,\"start\":42281},{\"end\":43025,\"start\":42962}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27130,\"start\":27122},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29868,\"start\":29860},{\"end\":30883,\"start\":30875},{\"end\":31563,\"start\":31552},{\"end\":31879,\"start\":31871},{\"end\":33595,\"start\":33587},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33868,\"start\":33860},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40052,\"start\":40044},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40065,\"start\":40057},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40245,\"start\":40236}]", "bib_author_first_name": "[{\"end\":44787,\"start\":44786},{\"end\":44789,\"start\":44788},{\"end\":44798,\"start\":44797},{\"end\":44800,\"start\":44799},{\"end\":44817,\"start\":44809},{\"end\":44821,\"start\":44820},{\"end\":44823,\"start\":44822},{\"end\":45169,\"start\":45168},{\"end\":45182,\"start\":45181},{\"end\":45192,\"start\":45191},{\"end\":45202,\"start\":45201},{\"end\":45204,\"start\":45203},{\"end\":45214,\"start\":45213},{\"end\":45216,\"start\":45215},{\"end\":45224,\"start\":45223},{\"end\":45553,\"start\":45552},{\"end\":45565,\"start\":45564},{\"end\":45575,\"start\":45574},{\"end\":45589,\"start\":45588},{\"end\":45602,\"start\":45601},{\"end\":45614,\"start\":45613},{\"end\":45628,\"start\":45621},{\"end\":45891,\"start\":45890},{\"end\":45899,\"start\":45898},{\"end\":45907,\"start\":45906},{\"end\":45920,\"start\":45919},{\"end\":45937,\"start\":45931},{\"end\":45941,\"start\":45940},{\"end\":46252,\"start\":46251},{\"end\":46259,\"start\":46258},{\"end\":46269,\"start\":46268},{\"end\":46571,\"start\":46570},{\"end\":46583,\"start\":46582},{\"end\":46595,\"start\":46594},{\"end\":46842,\"start\":46841},{\"end\":46853,\"start\":46852},{\"end\":46855,\"start\":46854},{\"end\":46863,\"start\":46862},{\"end\":46865,\"start\":46864},{\"end\":46878,\"start\":46877},{\"end\":47074,\"start\":47073},{\"end\":47080,\"start\":47079},{\"end\":47093,\"start\":47092},{\"end\":47107,\"start\":47106},{\"end\":47109,\"start\":47108},{\"end\":47119,\"start\":47118},{\"end\":47452,\"start\":47451},{\"end\":47464,\"start\":47463},{\"end\":47472,\"start\":47471},{\"end\":47482,\"start\":47481},{\"end\":47751,\"start\":47750},{\"end\":47753,\"start\":47752},{\"end\":48007,\"start\":48006},{\"end\":48016,\"start\":48015},{\"end\":48025,\"start\":48024},{\"end\":48035,\"start\":48034},{\"end\":48048,\"start\":48047},{\"end\":48056,\"start\":48055},{\"end\":48349,\"start\":48348},{\"end\":48362,\"start\":48361},{\"end\":48371,\"start\":48370},{\"end\":48382,\"start\":48381},{\"end\":48392,\"start\":48391},{\"end\":48402,\"start\":48401},{\"end\":48680,\"start\":48679},{\"end\":48690,\"start\":48689},{\"end\":48920,\"start\":48919},{\"end\":48930,\"start\":48929},{\"end\":49164,\"start\":49163},{\"end\":49174,\"start\":49173},{\"end\":49182,\"start\":49181},{\"end\":49198,\"start\":49192},{\"end\":49202,\"start\":49201},{\"end\":49405,\"start\":49404},{\"end\":49407,\"start\":49406},{\"end\":49420,\"start\":49419},{\"end\":49422,\"start\":49421},{\"end\":49430,\"start\":49429},{\"end\":49441,\"start\":49440},{\"end\":49450,\"start\":49449},{\"end\":49458,\"start\":49457},{\"end\":49467,\"start\":49466},{\"end\":49477,\"start\":49476},{\"end\":49818,\"start\":49817},{\"end\":49826,\"start\":49825},{\"end\":49841,\"start\":49840},{\"end\":49851,\"start\":49850},{\"end\":49861,\"start\":49860},{\"end\":49875,\"start\":49874},{\"end\":49887,\"start\":49886},{\"end\":50174,\"start\":50173},{\"end\":50182,\"start\":50181},{\"end\":50197,\"start\":50196},{\"end\":50207,\"start\":50206},{\"end\":50209,\"start\":50208},{\"end\":50217,\"start\":50216},{\"end\":50227,\"start\":50226},{\"end\":50229,\"start\":50228},{\"end\":50242,\"start\":50241},{\"end\":50252,\"start\":50251},{\"end\":50266,\"start\":50265},{\"end\":50268,\"start\":50267},{\"end\":50281,\"start\":50280},{\"end\":50607,\"start\":50606},{\"end\":50615,\"start\":50614},{\"end\":50617,\"start\":50616},{\"end\":50626,\"start\":50625},{\"end\":50635,\"start\":50634},{\"end\":50645,\"start\":50644},{\"end\":50647,\"start\":50646},{\"end\":50660,\"start\":50659},{\"end\":50670,\"start\":50669},{\"end\":50680,\"start\":50679},{\"end\":51048,\"start\":51047},{\"end\":51058,\"start\":51057},{\"end\":51069,\"start\":51068},{\"end\":51075,\"start\":51074},{\"end\":51426,\"start\":51425},{\"end\":51436,\"start\":51435},{\"end\":51447,\"start\":51446},{\"end\":51453,\"start\":51452},{\"end\":51661,\"start\":51660},{\"end\":51680,\"start\":51679},{\"end\":51689,\"start\":51688},{\"end\":51704,\"start\":51703},{\"end\":51995,\"start\":51994},{\"end\":52005,\"start\":52004},{\"end\":52246,\"start\":52245},{\"end\":52259,\"start\":52258},{\"end\":52275,\"start\":52274},{\"end\":52536,\"start\":52535},{\"end\":52548,\"start\":52547},{\"end\":52558,\"start\":52557},{\"end\":52568,\"start\":52567},{\"end\":52570,\"start\":52569},{\"end\":52580,\"start\":52579},{\"end\":52921,\"start\":52920},{\"end\":52933,\"start\":52932},{\"end\":52946,\"start\":52942},{\"end\":52950,\"start\":52949},{\"end\":53124,\"start\":53123},{\"end\":53136,\"start\":53135},{\"end\":53146,\"start\":53145},{\"end\":53158,\"start\":53157},{\"end\":53169,\"start\":53168},{\"end\":53442,\"start\":53441},{\"end\":53452,\"start\":53451},{\"end\":53461,\"start\":53460},{\"end\":53470,\"start\":53469},{\"end\":53480,\"start\":53479},{\"end\":53492,\"start\":53491},{\"end\":53813,\"start\":53812},{\"end\":53823,\"start\":53822},{\"end\":53832,\"start\":53831},{\"end\":53834,\"start\":53833},{\"end\":53846,\"start\":53845},{\"end\":53854,\"start\":53853},{\"end\":53863,\"start\":53862},{\"end\":53884,\"start\":53883},{\"end\":53901,\"start\":53900},{\"end\":53915,\"start\":53914},{\"end\":53933,\"start\":53932},{\"end\":53944,\"start\":53943},{\"end\":53956,\"start\":53955},{\"end\":53965,\"start\":53964},{\"end\":53973,\"start\":53972},{\"end\":53989,\"start\":53988},{\"end\":54002,\"start\":54001},{\"end\":54015,\"start\":54014},{\"end\":54024,\"start\":54023},{\"end\":54039,\"start\":54038},{\"end\":54050,\"start\":54049},{\"end\":54560,\"start\":54559},{\"end\":54562,\"start\":54561},{\"end\":54572,\"start\":54571},{\"end\":54574,\"start\":54573},{\"end\":54754,\"start\":54753},{\"end\":55003,\"start\":55002},{\"end\":55270,\"start\":55269},{\"end\":55588,\"start\":55587},{\"end\":55590,\"start\":55589},{\"end\":55858,\"start\":55857},{\"end\":55860,\"start\":55859},{\"end\":56081,\"start\":56080},{\"end\":56083,\"start\":56082},{\"end\":56094,\"start\":56093},{\"end\":56096,\"start\":56095},{\"end\":56104,\"start\":56103},{\"end\":56106,\"start\":56105},{\"end\":56117,\"start\":56116},{\"end\":56119,\"start\":56118}]", "bib_author_last_name": "[{\"end\":44795,\"start\":44790},{\"end\":44807,\"start\":44801},{\"end\":45179,\"start\":45170},{\"end\":45189,\"start\":45183},{\"end\":45199,\"start\":45193},{\"end\":45211,\"start\":45205},{\"end\":45221,\"start\":45217},{\"end\":45235,\"start\":45225},{\"end\":45562,\"start\":45554},{\"end\":45572,\"start\":45566},{\"end\":45586,\"start\":45576},{\"end\":45599,\"start\":45590},{\"end\":45611,\"start\":45603},{\"end\":45619,\"start\":45615},{\"end\":45896,\"start\":45892},{\"end\":45904,\"start\":45900},{\"end\":45917,\"start\":45908},{\"end\":45929,\"start\":45921},{\"end\":46256,\"start\":46253},{\"end\":46266,\"start\":46260},{\"end\":46276,\"start\":46270},{\"end\":46580,\"start\":46572},{\"end\":46592,\"start\":46584},{\"end\":46601,\"start\":46596},{\"end\":46850,\"start\":46843},{\"end\":46860,\"start\":46856},{\"end\":46875,\"start\":46866},{\"end\":46884,\"start\":46879},{\"end\":47077,\"start\":47075},{\"end\":47090,\"start\":47081},{\"end\":47104,\"start\":47094},{\"end\":47116,\"start\":47110},{\"end\":47126,\"start\":47120},{\"end\":47461,\"start\":47453},{\"end\":47469,\"start\":47465},{\"end\":47479,\"start\":47473},{\"end\":47489,\"start\":47483},{\"end\":47761,\"start\":47754},{\"end\":48013,\"start\":48008},{\"end\":48022,\"start\":48017},{\"end\":48032,\"start\":48026},{\"end\":48045,\"start\":48036},{\"end\":48053,\"start\":48049},{\"end\":48062,\"start\":48057},{\"end\":48359,\"start\":48350},{\"end\":48368,\"start\":48363},{\"end\":48379,\"start\":48372},{\"end\":48389,\"start\":48383},{\"end\":48399,\"start\":48393},{\"end\":48408,\"start\":48403},{\"end\":48687,\"start\":48681},{\"end\":48693,\"start\":48691},{\"end\":48699,\"start\":48695},{\"end\":48927,\"start\":48921},{\"end\":48937,\"start\":48931},{\"end\":49171,\"start\":49165},{\"end\":49179,\"start\":49175},{\"end\":49190,\"start\":49183},{\"end\":49417,\"start\":49408},{\"end\":49427,\"start\":49423},{\"end\":49438,\"start\":49431},{\"end\":49447,\"start\":49442},{\"end\":49455,\"start\":49451},{\"end\":49464,\"start\":49459},{\"end\":49474,\"start\":49468},{\"end\":49486,\"start\":49478},{\"end\":49823,\"start\":49819},{\"end\":49838,\"start\":49827},{\"end\":49848,\"start\":49842},{\"end\":49858,\"start\":49852},{\"end\":49872,\"start\":49862},{\"end\":49884,\"start\":49876},{\"end\":49898,\"start\":49888},{\"end\":50179,\"start\":50175},{\"end\":50194,\"start\":50183},{\"end\":50204,\"start\":50198},{\"end\":50214,\"start\":50210},{\"end\":50224,\"start\":50218},{\"end\":50239,\"start\":50230},{\"end\":50249,\"start\":50243},{\"end\":50263,\"start\":50253},{\"end\":50278,\"start\":50269},{\"end\":50291,\"start\":50282},{\"end\":50612,\"start\":50608},{\"end\":50623,\"start\":50618},{\"end\":50632,\"start\":50627},{\"end\":50642,\"start\":50636},{\"end\":50657,\"start\":50648},{\"end\":50667,\"start\":50661},{\"end\":50677,\"start\":50671},{\"end\":50692,\"start\":50681},{\"end\":51055,\"start\":51049},{\"end\":51066,\"start\":51059},{\"end\":51072,\"start\":51070},{\"end\":51086,\"start\":51076},{\"end\":51433,\"start\":51427},{\"end\":51444,\"start\":51437},{\"end\":51450,\"start\":51448},{\"end\":51464,\"start\":51454},{\"end\":51677,\"start\":51662},{\"end\":51686,\"start\":51681},{\"end\":51701,\"start\":51690},{\"end\":51709,\"start\":51705},{\"end\":51714,\"start\":51711},{\"end\":52002,\"start\":51996},{\"end\":52012,\"start\":52006},{\"end\":52253,\"start\":52247},{\"end\":52269,\"start\":52260},{\"end\":52287,\"start\":52276},{\"end\":52545,\"start\":52537},{\"end\":52555,\"start\":52549},{\"end\":52565,\"start\":52559},{\"end\":52577,\"start\":52571},{\"end\":52587,\"start\":52581},{\"end\":52930,\"start\":52922},{\"end\":52940,\"start\":52934},{\"end\":53133,\"start\":53125},{\"end\":53143,\"start\":53137},{\"end\":53155,\"start\":53147},{\"end\":53166,\"start\":53159},{\"end\":53176,\"start\":53170},{\"end\":53449,\"start\":53443},{\"end\":53458,\"start\":53453},{\"end\":53467,\"start\":53462},{\"end\":53477,\"start\":53471},{\"end\":53489,\"start\":53481},{\"end\":53503,\"start\":53493},{\"end\":53820,\"start\":53814},{\"end\":53829,\"start\":53824},{\"end\":53843,\"start\":53835},{\"end\":53851,\"start\":53847},{\"end\":53860,\"start\":53855},{\"end\":53881,\"start\":53864},{\"end\":53898,\"start\":53885},{\"end\":53912,\"start\":53902},{\"end\":53930,\"start\":53916},{\"end\":53941,\"start\":53934},{\"end\":53953,\"start\":53945},{\"end\":53962,\"start\":53957},{\"end\":53970,\"start\":53966},{\"end\":53986,\"start\":53974},{\"end\":53999,\"start\":53990},{\"end\":54012,\"start\":54003},{\"end\":54021,\"start\":54016},{\"end\":54036,\"start\":54025},{\"end\":54047,\"start\":54040},{\"end\":54059,\"start\":54051},{\"end\":54569,\"start\":54563},{\"end\":54580,\"start\":54575},{\"end\":54761,\"start\":54755},{\"end\":55011,\"start\":55004},{\"end\":55280,\"start\":55271},{\"end\":55599,\"start\":55591},{\"end\":55868,\"start\":55861},{\"end\":56091,\"start\":56084},{\"end\":56101,\"start\":56097},{\"end\":56114,\"start\":56107},{\"end\":56123,\"start\":56120}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1522994},\"end\":45080,\"start\":44705},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3232170},\"end\":45548,\"start\":45082},{\"attributes\":{\"doi\":\"arXiv:1606.01540\",\"id\":\"b2\"},\"end\":45823,\"start\":45550},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12296499},\"end\":46188,\"start\":45825},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2374643},\"end\":46503,\"start\":46190},{\"attributes\":{\"doi\":\"arXiv:1802.09477\",\"id\":\"b5\"},\"end\":46780,\"start\":46505},{\"attributes\":{\"doi\":\"arXiv:1704.04651\",\"id\":\"b6\"},\"end\":47071,\"start\":46782},{\"attributes\":{\"doi\":\"arXiv:1611.02247\",\"id\":\"b7\"},\"end\":47393,\"start\":47073},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11227891},\"end\":47729,\"start\":47395},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5155799},\"end\":47936,\"start\":47731},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53604},\"end\":48346,\"start\":47938},{\"attributes\":{\"doi\":\"arXiv:1709.06560\",\"id\":\"b11\"},\"end\":48639,\"start\":48348},{\"attributes\":{\"id\":\"b12\"},\"end\":48895,\"start\":48641},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13971447},\"end\":49112,\"start\":48897},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7242892},\"end\":49402,\"start\":49114},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b15\"},\"end\":49767,\"start\":49404},{\"attributes\":{\"doi\":\"arXiv:1312.5602\",\"id\":\"b16\"},\"end\":50114,\"start\":49769},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":205242740},\"end\":50550,\"start\":50116},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6875312},\"end\":50973,\"start\":50552},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3615386},\"end\":51354,\"start\":50975},{\"attributes\":{\"doi\":\"arXiv:1707.01891\",\"id\":\"b20\"},\"end\":51658,\"start\":51356},{\"attributes\":{\"doi\":\"arXiv:1611.01626\",\"id\":\"b21\"},\"end\":51930,\"start\":51660},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15454626},\"end\":52160,\"start\":51932},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2168740},\"end\":52499,\"start\":52162},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16046818},\"end\":52822,\"start\":52501},{\"attributes\":{\"id\":\"b25\"},\"end\":52860,\"start\":52824},{\"attributes\":{\"doi\":\"arXiv:1704.06440\",\"id\":\"b26\"},\"end\":53121,\"start\":52862},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b27\"},\"end\":53397,\"start\":53123},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13928442},\"end\":53742,\"start\":53399},{\"attributes\":{\"doi\":\"0028-0836. Article\",\"id\":\"b29\",\"matched_paper_id\":515925},\"end\":54516,\"start\":53744},{\"attributes\":{\"id\":\"b30\"},\"end\":54710,\"start\":54518},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":254721},\"end\":54944,\"start\":54712},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11319606},\"end\":55208,\"start\":54946},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":838406},\"end\":55494,\"start\":55210},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2332513},\"end\":55771,\"start\":55496},{\"attributes\":{\"id\":\"b35\"},\"end\":56030,\"start\":55773},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":336219},\"end\":56358,\"start\":56032}]", "bib_title": "[{\"end\":44784,\"start\":44705},{\"end\":45166,\"start\":45082},{\"end\":45888,\"start\":45825},{\"end\":46249,\"start\":46190},{\"end\":47449,\"start\":47395},{\"end\":47748,\"start\":47731},{\"end\":48004,\"start\":47938},{\"end\":48677,\"start\":48641},{\"end\":48917,\"start\":48897},{\"end\":49161,\"start\":49114},{\"end\":50171,\"start\":50116},{\"end\":50604,\"start\":50552},{\"end\":51045,\"start\":50975},{\"end\":51992,\"start\":51932},{\"end\":52243,\"start\":52162},{\"end\":52533,\"start\":52501},{\"end\":53439,\"start\":53399},{\"end\":53810,\"start\":53744},{\"end\":54751,\"start\":54712},{\"end\":55000,\"start\":54946},{\"end\":55267,\"start\":55210},{\"end\":55585,\"start\":55496},{\"end\":56078,\"start\":56032}]", "bib_author": "[{\"end\":44797,\"start\":44786},{\"end\":44809,\"start\":44797},{\"end\":44820,\"start\":44809},{\"end\":44826,\"start\":44820},{\"end\":45181,\"start\":45168},{\"end\":45191,\"start\":45181},{\"end\":45201,\"start\":45191},{\"end\":45213,\"start\":45201},{\"end\":45223,\"start\":45213},{\"end\":45237,\"start\":45223},{\"end\":45564,\"start\":45552},{\"end\":45574,\"start\":45564},{\"end\":45588,\"start\":45574},{\"end\":45601,\"start\":45588},{\"end\":45613,\"start\":45601},{\"end\":45621,\"start\":45613},{\"end\":45631,\"start\":45621},{\"end\":45898,\"start\":45890},{\"end\":45906,\"start\":45898},{\"end\":45919,\"start\":45906},{\"end\":45931,\"start\":45919},{\"end\":45940,\"start\":45931},{\"end\":45944,\"start\":45940},{\"end\":46258,\"start\":46251},{\"end\":46268,\"start\":46258},{\"end\":46278,\"start\":46268},{\"end\":46582,\"start\":46570},{\"end\":46594,\"start\":46582},{\"end\":46603,\"start\":46594},{\"end\":46852,\"start\":46841},{\"end\":46862,\"start\":46852},{\"end\":46877,\"start\":46862},{\"end\":46886,\"start\":46877},{\"end\":47079,\"start\":47073},{\"end\":47092,\"start\":47079},{\"end\":47106,\"start\":47092},{\"end\":47118,\"start\":47106},{\"end\":47128,\"start\":47118},{\"end\":47463,\"start\":47451},{\"end\":47471,\"start\":47463},{\"end\":47481,\"start\":47471},{\"end\":47491,\"start\":47481},{\"end\":47763,\"start\":47750},{\"end\":48015,\"start\":48006},{\"end\":48024,\"start\":48015},{\"end\":48034,\"start\":48024},{\"end\":48047,\"start\":48034},{\"end\":48055,\"start\":48047},{\"end\":48064,\"start\":48055},{\"end\":48361,\"start\":48348},{\"end\":48370,\"start\":48361},{\"end\":48381,\"start\":48370},{\"end\":48391,\"start\":48381},{\"end\":48401,\"start\":48391},{\"end\":48410,\"start\":48401},{\"end\":48689,\"start\":48679},{\"end\":48695,\"start\":48689},{\"end\":48701,\"start\":48695},{\"end\":48929,\"start\":48919},{\"end\":48939,\"start\":48929},{\"end\":49173,\"start\":49163},{\"end\":49181,\"start\":49173},{\"end\":49192,\"start\":49181},{\"end\":49201,\"start\":49192},{\"end\":49205,\"start\":49201},{\"end\":49419,\"start\":49404},{\"end\":49429,\"start\":49419},{\"end\":49440,\"start\":49429},{\"end\":49449,\"start\":49440},{\"end\":49457,\"start\":49449},{\"end\":49466,\"start\":49457},{\"end\":49476,\"start\":49466},{\"end\":49488,\"start\":49476},{\"end\":49825,\"start\":49817},{\"end\":49840,\"start\":49825},{\"end\":49850,\"start\":49840},{\"end\":49860,\"start\":49850},{\"end\":49874,\"start\":49860},{\"end\":49886,\"start\":49874},{\"end\":49900,\"start\":49886},{\"end\":50181,\"start\":50173},{\"end\":50196,\"start\":50181},{\"end\":50206,\"start\":50196},{\"end\":50216,\"start\":50206},{\"end\":50226,\"start\":50216},{\"end\":50241,\"start\":50226},{\"end\":50251,\"start\":50241},{\"end\":50265,\"start\":50251},{\"end\":50280,\"start\":50265},{\"end\":50293,\"start\":50280},{\"end\":50614,\"start\":50606},{\"end\":50625,\"start\":50614},{\"end\":50634,\"start\":50625},{\"end\":50644,\"start\":50634},{\"end\":50659,\"start\":50644},{\"end\":50669,\"start\":50659},{\"end\":50679,\"start\":50669},{\"end\":50694,\"start\":50679},{\"end\":51057,\"start\":51047},{\"end\":51068,\"start\":51057},{\"end\":51074,\"start\":51068},{\"end\":51088,\"start\":51074},{\"end\":51435,\"start\":51425},{\"end\":51446,\"start\":51435},{\"end\":51452,\"start\":51446},{\"end\":51466,\"start\":51452},{\"end\":51679,\"start\":51660},{\"end\":51688,\"start\":51679},{\"end\":51703,\"start\":51688},{\"end\":51711,\"start\":51703},{\"end\":51716,\"start\":51711},{\"end\":52004,\"start\":51994},{\"end\":52014,\"start\":52004},{\"end\":52258,\"start\":52245},{\"end\":52274,\"start\":52258},{\"end\":52292,\"start\":52274},{\"end\":52547,\"start\":52535},{\"end\":52557,\"start\":52547},{\"end\":52567,\"start\":52557},{\"end\":52579,\"start\":52567},{\"end\":52589,\"start\":52579},{\"end\":52932,\"start\":52920},{\"end\":52942,\"start\":52932},{\"end\":52949,\"start\":52942},{\"end\":52953,\"start\":52949},{\"end\":53135,\"start\":53123},{\"end\":53145,\"start\":53135},{\"end\":53157,\"start\":53145},{\"end\":53168,\"start\":53157},{\"end\":53178,\"start\":53168},{\"end\":53451,\"start\":53441},{\"end\":53460,\"start\":53451},{\"end\":53469,\"start\":53460},{\"end\":53479,\"start\":53469},{\"end\":53491,\"start\":53479},{\"end\":53505,\"start\":53491},{\"end\":53822,\"start\":53812},{\"end\":53831,\"start\":53822},{\"end\":53845,\"start\":53831},{\"end\":53853,\"start\":53845},{\"end\":53862,\"start\":53853},{\"end\":53883,\"start\":53862},{\"end\":53900,\"start\":53883},{\"end\":53914,\"start\":53900},{\"end\":53932,\"start\":53914},{\"end\":53943,\"start\":53932},{\"end\":53955,\"start\":53943},{\"end\":53964,\"start\":53955},{\"end\":53972,\"start\":53964},{\"end\":53988,\"start\":53972},{\"end\":54001,\"start\":53988},{\"end\":54014,\"start\":54001},{\"end\":54023,\"start\":54014},{\"end\":54038,\"start\":54023},{\"end\":54049,\"start\":54038},{\"end\":54061,\"start\":54049},{\"end\":54571,\"start\":54559},{\"end\":54582,\"start\":54571},{\"end\":54763,\"start\":54753},{\"end\":55013,\"start\":55002},{\"end\":55282,\"start\":55269},{\"end\":55601,\"start\":55587},{\"end\":55870,\"start\":55857},{\"end\":56093,\"start\":56080},{\"end\":56103,\"start\":56093},{\"end\":56116,\"start\":56103},{\"end\":56125,\"start\":56116}]", "bib_venue": "[{\"end\":44876,\"start\":44826},{\"end\":45293,\"start\":45237},{\"end\":45995,\"start\":45944},{\"end\":46336,\"start\":46278},{\"end\":46568,\"start\":46505},{\"end\":46839,\"start\":46782},{\"end\":47202,\"start\":47144},{\"end\":47542,\"start\":47491},{\"end\":47819,\"start\":47763},{\"end\":48120,\"start\":48064},{\"end\":48466,\"start\":48426},{\"end\":48759,\"start\":48701},{\"end\":48990,\"start\":48939},{\"end\":49241,\"start\":49205},{\"end\":49555,\"start\":49504},{\"end\":49815,\"start\":49769},{\"end\":50299,\"start\":50293},{\"end\":50745,\"start\":50694},{\"end\":51144,\"start\":51088},{\"end\":51423,\"start\":51356},{\"end\":51772,\"start\":51732},{\"end\":52029,\"start\":52014},{\"end\":52321,\"start\":52292},{\"end\":52640,\"start\":52589},{\"end\":52841,\"start\":52824},{\"end\":52918,\"start\":52862},{\"end\":53233,\"start\":53194},{\"end\":53556,\"start\":53505},{\"end\":54085,\"start\":54079},{\"end\":54557,\"start\":54518},{\"end\":54814,\"start\":54763},{\"end\":55058,\"start\":55013},{\"end\":55333,\"start\":55282},{\"end\":55617,\"start\":55601},{\"end\":55855,\"start\":55773},{\"end\":56174,\"start\":56125}]"}}}, "year": 2023, "month": 12, "day": 17}