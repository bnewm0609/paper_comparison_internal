{"id": 16516553, "updated": "2023-09-28 22:06:07.923", "metadata": {"title": "Instance Normalization: The Missing Ingredient for Fast Stylization", "authors": "[{\"first\":\"Dmitry\",\"last\":\"Ulyanov\",\"middle\":[]},{\"first\":\"Andrea\",\"last\":\"Vedaldi\",\"middle\":[]},{\"first\":\"Victor\",\"last\":\"Lempitsky\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2016, "month": 7, "day": 27}, "abstract": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1607.08022", "mag": "2502312327", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/UlyanovVL16", "doi": null}}, "content": {"source": {"pdf_hash": "63de0ad39d807f0c256f851428f211e8d5fcd3bb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1607.08022v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cdf8998fa71f88726699fbc3ebd787ad4775439c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/63de0ad39d807f0c256f851428f211e8d5fcd3bb.txt", "contents": "\nInstance Normalization: The Missing Ingredient for Fast Stylization\n\n\nDmitry Ulyanov dmitry.ulyanov@skoltech.ru \nAndrea Vedaldi vedaldi@robots.ox.ac.uk \nVictor Lempitsky lempitsky@skoltech.ru \n\nComputer Vision Group Skoltech\nYandex Russia\n\n\nVisual Geometry Group University of Oxford United Kingdom\nComputer Vision Group\nSkoltech Russia\n\nInstance Normalization: The Missing Ingredient for Fast Stylization\n\nIt this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will be made available at https://github.com/DmitryUlyanov/texture_nets.\n\nIntroduction\n\nThe recent work of Gatys et al. (2016) introduced a method for transferring a style from an image onto another one, as demonstrated in fig. 1. The stylized image matches simultaneously selected statistics of the style image and of the content image. Both style and content statistics are obtained from a deep convolutional network pre-trained for image classification. The style statistics are extracted from shallower layers and averaged across spatial locations whereas the content statistics are extracted form deeper layers and preserve spatial information. In this manner, the style statistics capture the \"texture\" of the style image whereas the content statistics capture the \"structure\" of the content image.\n\nAlthough the method of Gatys et. al produces remarkably good results, it is computationally inefficient. The stylized image is, in fact, obtained by iterative optimization until it matches the desired statistics. In practice, it takes several minutes to stylize an image of size 512 \u00d7 512. Two recent works, Ulyanov et al. (2016) Johnson et al. (2016, sought to address this problem by learning equivalent feed-forward generator networks that can generate the stylized image in a single pass. These two methods differ mainly by the details of the generator architecture and produce results of a comparable quality; however, neither achieved as good results as the slower optimization-based method of Gatys et. al.\n\nIn this paper we revisit the method for feed-forward stylization of Ulyanov et al. (2016) and show that a small change in a generator architecture leads to much improved results. The results are in fact of comparable quality as the slow optimization method of Gatys et al. but can be obtained in real time on standard GPU hardware. The key idea (section 2) is to replace batch normalization layers in the generator architecture with instance normalization layers, and to keep them at test  time (as opposed to freeze and simplify them out as done for batch normalization). Intuitively, the normalization process allows to remove instance-specific contrast information from the content image, which simplifies generation. In practice, this results in vastly improved images (section 3).\n\n\nMethod\n\nThe work of Ulyanov et al. (2016) showed that it is possible to learn a generator network g(x, z) that can apply to a given input image x the style of another x 0 , reproducing to some extent the results of the optimization method of Gatys et al. Here, the style image x 0 is fixed and the genrator g is learned to apply the style to any input image x. The variable z is a random seed that can be used to obtain sample stylization results.\n\nThe function g is a convolutional neural network learned from examples. Here an example is just a content image x t , t = 1, . . . , n and learning solves the problem where z t \u223c N (0, 1) are i.i.d. samples from a Gaussian distribution. The loss L uses a pre-trained CNN (not shown) to extracts features from the style x 0 image, the content image x t , and the stylized image g(x t , z t ), and compares their statistics as explained before.\nmin g 1 n n t=1 L(x 0 , x t , g(x t , z t ))\nWhile the generator network g is fast, the authors of Ulyanov et al. (2016) observed that learning it from too many training examples yield poorer qualitative results. In particular, a network trained on just 16 example images produced better results than one trained from thousands of those. The most serious artifacts were found along the border of the image due to the zero padding added before every convolution operation (see fig. 3). Even by using more complex padding techniques it was not possible to solve this issue. Ultimately, the best results presented in Ulyanov et al. (2016) were obtained using a small number of training images and stopping the learning process early. We conjectured that the training objective was too hard to learn for a standard neural network architecture.\n\nA simple observation is that the result of stylization should not, in general, depend on the contrast of the content image (see fig. 2). In fact, the style loss is designed to transfer elements from a style image to the content image such that the contrast of the stylized image is similar to the contrast of the style image. Thus, the generator network should discard contrast information in the content image. The question is whether contrast normalization can be implemented efficiently by combining standard CNN building blocks or whether, instead, is best implemented directly in the architecture.\n\nThe generators used in Ulyanov et al. (2016) and Johnson et al. (2016) use convolution, pooling, upsampling, and batch normalization. In practice, it may be difficult to learn a highly nonlinear contrast normalization function as a combination of such layers. To see why, let x \u2208 R T \u00d7C\u00d7W \u00d7H be an input tensor containing a batch of T images. Let x tijk denote its tijk-th element, where k and j span spatial dimensions, i is the feature channel (color channel if the input is an RGB image), and t is the index of the image in the batch. Then a simple version of contrast normalization is given by:\ny tijk = x tijk W l=1 H m=1 x tilm .(1)\nIt is unclear how such as function could be implemented as a sequence of ReLU and convolution operator.\n\nOn the other hand, the generator network of Ulyanov et al. (2016) does contain a normalization layers, and precisely batch normalization ones. The key difference between eq. (1) and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones:\ny tijk = x tijk \u2212 \u00b5 i \u03c3 2 i + , \u00b5 i = 1 HW T T t=1 W l=1 H m=1 x tilm , \u03c3 2 i = 1 HW T T t=1 W l=1 H m=1 (x tilm \u2212mu i ) 2 .\n(2) In order to combine the effects of instance-specific normalization and batch normalization, we propose to replace the latter by the instance normalization (also known as \"contrast normalization\") layer:\ny tijk = x tijk \u2212 \u00b5 ti \u03c3 2 ti + , \u00b5 ti = 1 HW W l=1 H m=1 x tilm , \u03c3 2 ti = 1 HW W l=1 H m=1 (x tilm \u2212 mu ti ) 2 . (3)\nWe replace batch normalization with instance normalization everywhere in the generator network g. This prevents instance-specific mean and covariance shift simplifying the learning process. Differently from batch normalization, furthermore, the instance normalization layer is applied at test time as well.\n\n\nExperiments\n\nIn this section, we evaluate the effect of the modification proposed in section 2 and replace batch normalization with instance normalization. We tested both generator architectures described in Ulyanov et al. (2016) and Johnson et al. (2016) in order too see whether the modification applies to different architectures. While we did not have access to the original network by Johnson et al. (2016), we carefully reproduced their model from the description in the paper. Ultimately, we found that both generator networks have similar performance and shortcomings ( fig. 5 first row).\n\nNext, the replaced batch normalization with instance normalization and retrained the generators using the same hyperparameters. We found that both architectures significantly improved by the use of instance normalization ( fig. 5 second row). The quality of both generators is similar, but we found the residuals architecture of Johnson et al. (2016) to be somewhat more efficient and easy to use, so we adopted it for the results shown in fig. 4.\n\n\nConclusion\n\nIn this short note, we demonstrate that by replacing batch normalization with instance normalization it is possible to dramatically improve the performance of certain deep neural networks for image generation. The result is suggestive, and we are currently experimenting with similar ideas for image discrimination tasks as well.   \n\nFigure 1 :\n1Artistic style transfer example of Gatys et al. (2016) mehod. (a) Content image. (b) Stylized image. (c) Low contrast content image. (d) Stylized low contrast image.\n\nFigure 2 :\n2A contrast of a stylized image is mostly determined by a contrast of a style image and almost independent of a content image contrast. The stylization is performed with method ofGatys et al. (2016).\n\nFigure 3 :\n3Row 1: content image (left), style image (middle) and style transfer using method ofGatys et. al (right). Row 2: typical stylization results when trained for a large number of iterations using fast stylization method fromUlyanov et al. (2016): with zero padding (left), with a better padding technique (middle), with zero padding and instance normalization (right).\n\nFigure 4 :\n4Stylization examples using proposed method. First row: style images; second row: original image and its stylized versions.\n\nFigure 5 :\n5Qualitative comparison of generators proposed in Ulyanov et al. (2016) (left), Johnson et al. (2016) (right) with batch normalization (first row) and instance normalization (second row). Both architectures benefit from instance normalization.\n\nFigure 6 :\n6Processing a content image from fig. 4 with Delaunay style at different resolutions: 512 (left) and 1080 (right).\n\nImage style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). Image style transfer using convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nPerceptual losses for real-time style transfer and superresolution. J Johnson, A Alahi, Li , F , abs/1603.08155CoRRJohnson, J., Alahi, A., and Li, F. (2016). Perceptual losses for real-time style transfer and super- resolution. CoRR, abs/1603.08155.\n\nTexture networks: Feed-forward synthesis of textures and stylized images. D Ulyanov, V Lebedev, A Vedaldi, V S Lempitsky, Proceedings of the 33nd International Conference on Machine Learning. the 33nd International Conference on Machine LearningNew York City, NY, USAUlyanov, D., Lebedev, V., Vedaldi, A., and Lempitsky, V. S. (2016). Texture networks: Feed-forward synthesis of textures and stylized images. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1349-1357.\n", "annotations": {"author": "[{\"end\":113,\"start\":71},{\"end\":153,\"start\":114},{\"end\":193,\"start\":154},{\"end\":240,\"start\":194},{\"end\":338,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":78},{\"end\":128,\"start\":121},{\"end\":170,\"start\":161}]", "author_first_name": "[{\"end\":77,\"start\":71},{\"end\":120,\"start\":114},{\"end\":160,\"start\":154}]", "author_affiliation": "[{\"end\":239,\"start\":195},{\"end\":337,\"start\":242}]", "title": "[{\"end\":68,\"start\":1},{\"end\":406,\"start\":339}]", "venue": null, "abstract": "[{\"end\":960,\"start\":408}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1014,\"start\":995},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2022,\"start\":2002},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2044,\"start\":2022},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2498,\"start\":2477},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3238,\"start\":3217},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4209,\"start\":4188},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4724,\"start\":4703},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5578,\"start\":5557},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5604,\"start\":5583},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6343,\"start\":6322},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7566,\"start\":7545},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7592,\"start\":7571},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7748,\"start\":7727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8285,\"start\":8264},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9119,\"start\":9100},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9376,\"start\":9355}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":8908,\"start\":8730},{\"attributes\":{\"id\":\"fig_1\"},\"end\":9120,\"start\":8909},{\"attributes\":{\"id\":\"fig_2\"},\"end\":9499,\"start\":9121},{\"attributes\":{\"id\":\"fig_3\"},\"end\":9635,\"start\":9500},{\"attributes\":{\"id\":\"fig_4\"},\"end\":9891,\"start\":9636},{\"attributes\":{\"id\":\"fig_5\"},\"end\":10018,\"start\":9892}]", "paragraph": "[{\"end\":1692,\"start\":976},{\"end\":2407,\"start\":1694},{\"end\":3194,\"start\":2409},{\"end\":3644,\"start\":3205},{\"end\":4088,\"start\":3646},{\"end\":4928,\"start\":4134},{\"end\":5532,\"start\":4930},{\"end\":6132,\"start\":5534},{\"end\":6276,\"start\":6173},{\"end\":6576,\"start\":6278},{\"end\":6908,\"start\":6702},{\"end\":7334,\"start\":7028},{\"end\":7933,\"start\":7350},{\"end\":8382,\"start\":7935},{\"end\":8729,\"start\":8397}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4133,\"start\":4089},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6172,\"start\":6133},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6701,\"start\":6577},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7027,\"start\":6909}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":974,\"start\":962},{\"attributes\":{\"n\":\"2\"},\"end\":3203,\"start\":3197},{\"attributes\":{\"n\":\"3\"},\"end\":7348,\"start\":7337},{\"attributes\":{\"n\":\"4\"},\"end\":8395,\"start\":8385},{\"end\":8741,\"start\":8731},{\"end\":8920,\"start\":8910},{\"end\":9132,\"start\":9122},{\"end\":9511,\"start\":9501},{\"end\":9647,\"start\":9637},{\"end\":9903,\"start\":9893}]", "table": null, "figure_caption": "[{\"end\":8908,\"start\":8743},{\"end\":9120,\"start\":8922},{\"end\":9499,\"start\":9134},{\"end\":9635,\"start\":9513},{\"end\":9891,\"start\":9649},{\"end\":10018,\"start\":9905}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1117,\"start\":1111},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4571,\"start\":4565},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5064,\"start\":5058},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":7932,\"start\":7915},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":8176,\"start\":8158},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8381,\"start\":8375}]", "bib_author_first_name": "[{\"end\":10079,\"start\":10078},{\"end\":10081,\"start\":10080},{\"end\":10090,\"start\":10089},{\"end\":10092,\"start\":10091},{\"end\":10101,\"start\":10100},{\"end\":10434,\"start\":10433},{\"end\":10445,\"start\":10444},{\"end\":10455,\"start\":10453},{\"end\":10459,\"start\":10458},{\"end\":10691,\"start\":10690},{\"end\":10702,\"start\":10701},{\"end\":10713,\"start\":10712},{\"end\":10724,\"start\":10723},{\"end\":10726,\"start\":10725}]", "bib_author_last_name": "[{\"end\":10087,\"start\":10082},{\"end\":10098,\"start\":10093},{\"end\":10108,\"start\":10102},{\"end\":10442,\"start\":10435},{\"end\":10451,\"start\":10446},{\"end\":10699,\"start\":10692},{\"end\":10710,\"start\":10703},{\"end\":10721,\"start\":10714},{\"end\":10736,\"start\":10727}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206593710},\"end\":10363,\"start\":10020},{\"attributes\":{\"doi\":\"abs/1603.08155\",\"id\":\"b1\"},\"end\":10614,\"start\":10365},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16728483},\"end\":11167,\"start\":10616}]", "bib_title": "[{\"end\":10076,\"start\":10020},{\"end\":10688,\"start\":10616}]", "bib_author": "[{\"end\":10089,\"start\":10078},{\"end\":10100,\"start\":10089},{\"end\":10110,\"start\":10100},{\"end\":10444,\"start\":10433},{\"end\":10453,\"start\":10444},{\"end\":10458,\"start\":10453},{\"end\":10462,\"start\":10458},{\"end\":10701,\"start\":10690},{\"end\":10712,\"start\":10701},{\"end\":10723,\"start\":10712},{\"end\":10738,\"start\":10723}]", "bib_venue": "[{\"end\":10179,\"start\":10110},{\"end\":10431,\"start\":10365},{\"end\":10806,\"start\":10738},{\"end\":10883,\"start\":10808}]"}}}, "year": 2023, "month": 12, "day": 17}