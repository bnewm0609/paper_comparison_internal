{"id": 250390936, "updated": "2022-10-11 19:56:17.754", "metadata": {"title": "SURF: Semantic-level Unsupervised Reward Function for Machine Translation", "authors": "[{\"first\":\"Atijit\",\"last\":\"Anuchitanukul\",\"middle\":[]},{\"first\":\"Julia\",\"last\":\"Ive\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The performance of Reinforcement Learning (RL) for natural language tasks including Machine Translation (MT) is crucially dependent on the reward formulation. This is due to the intrinsic difficulty of the task in the high-dimensional discrete action space as well as the sparseness of the standard reward functions defined for limited set of ground-truth sequences biased towards singular lexical choices. To address this issue, we formulate SURF, a maximally dense semantic-level unsupervised reward function which mimics human evaluation by considering both sentence fluency and semantic similarity. We demonstrate the strong potential of SURF to leverage a family of Actor-Critic Transformer-based Architectures with synchronous and asynchronous multi-agent variants. To tackle the problem of large action-state spaces, each agent is equipped with unique exploration strategies, promoting diversity during its exploration of the hypothesis space. When BLEU scores are compared, our dense unsupervised reward outperforms the standard sparse reward by 2% on average for in- and out-of-domain settings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2022.naacl-main.334", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/AnuchitanukulI22", "doi": "10.18653/v1/2022.naacl-main.334"}}, "content": {"source": {"pdf_hash": "0c863681a8aaca2089aaac55e68535b0cda7482f", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.naacl-main.334.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ff4e0d89aa45e58017561007b65deb8cdd1155e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0c863681a8aaca2089aaac55e68535b0cda7482f.txt", "contents": "\nSURF: Semantic-level Unsupervised Reward Function for Machine Translation\nJuly 10-15, 2022\n\nAtijit Anuchitanukul 2atijit.anuchitanukul20@imperial.ac.uk \nJulia Ive j.ive@qmul.ac.uk \n\nImperial College London\nQueen\n\n\nMary University of London\n\n\nSURF: Semantic-level Unsupervised Reward Function for Machine Translation\n\nProceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\nthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJuly 10-15, 2022\nThe performance of Reinforcement Learning (RL) for natural language tasks including Machine Translation (MT) is crucially dependent on the reward formulation. This is due to the intrinsic difficulty of the task in the high-dimensional discrete action space as well as the sparseness of the standard reward functions defined for limited set of groundtruth sequences biased towards singular lexical choices. To address this issue, we formulate SURF, a maximally dense semantic-level unsupervised reward function which mimics human evaluation by considering both sentence fluency and semantic similarity. We demonstrate the strong potential of SURF to leverage a family of Actor-Critic Transformerbased Architectures with synchronous and asynchronous multi-agent variants. To tackle the problem of large action-state spaces, each agent is equipped with unique exploration strategies, promoting diversity during its exploration of the hypothesis space. When BLEU scores are compared, our dense unsupervised reward outperforms the standard sparse reward by 2% on average for in-and out-of-domain settings.\n\nIntroduction\n\nReinforcement Learning (RL) has shown promise in the field of text generation. This is mainly due to the fact that it allows the usage of nondifferentiable evaluation functions fit for the discrete natural language tasks. It also serves as a solution for bridging the gap between training and inference time regimes (\"exposure bias\") that arises from the fact that the model is never exposed to its own errors as only ground-truth labels are used to condition the generation during training (Wang and Sennrich, 2020). One of the essential components of the RL framework is the reward function, which is used to provide agents with indicative signals in terms of the effectiveness of chosen actions.\n\nThe usage of RL in Neural Machine Translation (NMT) and language generation however has been doubted largely due to the difficulty of exploration in the high-dimensional discrete action space combined with the sparse reward signal. The latter comes from the typical metrics used as rewards (e.g., BLEU (Papineni et al., 2002)). These rewards evaluate text in a shallow way by measuring the string similarity between generated and ground-truth sequences, making them extremely sparse and biased towards singular lexical choices. As the RL policy is usually initialised with some pre-trained distribution over words, suspicion has been raised that in this situation, those words already most likely gain probability mass regardless of the rewards (Choshen et al., 2020). Thus, the current sparse rewards are not beneficial for rigorous exploration of different words during training. Recent studies suggest that the main benefit for NMT from RL is in performing domain adaptation when using proper hypothesis space exploration along with special emphasis on reward scaling and normalisation (Kiegeland and Kreutzer, 2021).\n\nTo address the problem posed by sparse or biased rewards, we propose SURF, a formulation of the unsupervised reward function that evaluates machine-generated texts in the semantic space by factoring in different qualitative aspects. Furthermore, we introduce an additional scaling and normalisation mechanism which ensures fairness and uniformity of the reward function regardless of the complexity of the natural language task.\n\nOur main contributions are thus threefold: (a) the proposal of SURF, an unsupervised dense reward assessing both sentence fluency and adequacy (Section 4). We demonstrate this reward leads to a translation quality favourably comparable to the traditional sparse BLEU reward both in automatic and human evaluation; (b) the proposal of an additional normalisation using reward shaping mechanisms for the unsupervised reward; (c) demonstra-tion of the strong potential of the proposed reward to elicit benefits of various RL architectures. We experiment with multi-agent synchronous and asynchronous Actor-Critic (AC) architectures as applied to the problem of MT (Section 3.3). Each of the parallel agents in those architectures is trained using different segments of the training dataset which has their own unique exploration strategy. When BLEU scores are compared, our dense unsupervised reward outperforms the standard sparse reward by 2% on average for both in-and out-of-domain settings. To the best of our knowledge, this formulation of the unsupervised reward for a range of multi-agent architectures is the first of its kind for MT.\n\nOur datasets and settings are described in Section 5, and results of our experiments are described in Section 6.\n\n\nRelated Work\n\nThe following section describes the work related to ours in the subfields of Machine Translation and RL for language generation.\n\n\nReinforcement Learning Algorithms for Neural\n\nMachine Translation REINFORCE (Williams, 1992) algorithm and its variants have so far been the most widely used RL algorithms in MT (Ranzato et al., 2015;Rennie et al., 2017;Paulus et al., 2018;Hu et al., 2018). The fact that REINFORCE-based approaches suffer from high variance in general and in MT in particular has stimulated attempts to apply Actor-Critic (AC) models to the task. The first attempt of the kind was the one of Bahdanau et al. (2016). More advanced AC models with Q-Learning are rarely applied to language generation problems. However, there are exceptions (e.g., entropy-regularised AC models that promote exploration of actions (Dai et al., 2018;Ive et al., 2021)). This could be explained by the difficulty of approximating the Q-function for large action space. In this work we explore a series of multi-agent AC architectures which to the best of our knowledge have never been applied to MT before.\n\nUnsupervised Rewards for Language Generation Tasks Recent work on unsupervised rewards in NLP has explored both dynamic (Ive et al., 2021) and static rewards (Gao et al., 2020;Garg et al., 2021). For example, Ive et al. (2021) introduces a dynamic distribution over latent frequency classes as a reward signal. This distribution is shaped to promote more rare words in the policy search space.\n\nStatic rewards are very often designed to assess generated text in terms of its fluency and adequacy. Fluency judgment assesses how a hypothesis satisfies the grammatical norms of a language. Adequacy judgment assesses how well a hypothesis conveys the meaning of the source sentence. The recent research performs both evaluations as semantic similarity assessments using the pre-trained contextualised embeddings such as BERT (Zhang et al., 2020;Mathur et al., 2019). For MT, semantic similarity assessment could be carried out using monolingual pre-trained embeddings against a reference, as in Gao et al. (2020), or using multilingual pre-trained embeddings in the unsupervised reference-less approach by considering the semantic similarity to source sentences (Wei et al., 2019;Song et al., 2021). We adopt the latter approach to measure adequacy.\n\n\nMethodology\n\nWe start by formulating MT using RL, then introduce the Actor-Critic architectures and the reward functions used.\n\n\nNeural Machine Translation (NMT)\n\nA typical Neural Machine Translation (NMT) system is a Seq2Seq architecture (Sutskever et al., 2014;Bahdanau et al., 2014), where each source sentence (X) is encoded by the encoder into a sequence of hidden states. At each decoding step t, a target word y t is generated according to p(y t |y <t , X) conditioned on the input sequence X and decoded sequence y <t = (y 1 , \u00b7 \u00b7 \u00b7 , y t\u22121 ) up to the t-th time step:\nL mle = log p (y t |y <t , X)(1)\n\nReinforcement Learning for NMT\n\nIn the RL framework, a Seq2Seq model is viewed as an agent and its parameters define the agent's policy (\u03c0). At each timestep (t), the agent observes the current state (s t ) of the environment, which is essentially the sequence of generated words from previous timesteps (\u0177 1:t\u22121 ). Then, the agent's policy, which is based on the conditional probability p (\u0177 t |\u0177 1:t\u22121 , X), is used to select an action (a t ) at each timestep. In this context, an action is the candidate word (\u0177 t ) chosen from the vocabulary. Subsequently, the environment adds the chosen word to the generated sequence, transitioning it to the next state (\u0177 1:t ). It also returns a reward (r t+1 ) to the agent as an indication of how effective the chosen word is. Hence, one possible training objective of the policy is to maximise the discounted sum of expected rewards from all timesteps:\n\u03c0 * = max \u03c0 T t=1 \u03b3 t\u22121 \u00ca yt\u223c\u03c0(\u00b7|\u0177 1:t\u22121 ,X) [r t+1 ] (2)\nwhere \u03c0 * denotes the optimal policy and \u03b3 is a constant discount factor. Under the policy \u03c0, one can formulate two functions: the state value function (V \u03c0 (s t )) and the state-action value function (Q \u03c0 (s t , a t )). The former, V \u03c0 (s t ), determines the effectiveness of the agent being in a particular state while the latter, Q \u03c0 (s t , a t ), indicates the effectiveness of selecting a certain action in that state:\nV \u03c0 (s t ) = \u00ca yt\u223c\u03c0 [Q \u03c0 (s t =\u0177 1:t\u22121 , a t =\u0177 t )] (3) Q \u03c0 (s t , a t ) = E \u03c0 T \u2212t k=1 \u03b3 k\u22121 r t+k |s t , a t(4)\nHence the definition of the advantage function is:\nA \u03c0 (s t , a t ) = Q \u03c0 (s t , a t ) \u2212 V \u03c0 (s t )(5)\nA training objective can aim to maximise the advantage function max a A \u03c0 (s t , a t ).\n\nAlternatively, considering the definition of A \u03c0 (s t , a t ) in Equation (5), it implies that we can directly maximise the Q function:\nmax a A \u03c0 (s t , a t ) \u2192 max a Q \u03c0 (s t , a t )(6)\nThe first objective (Equation (2)) has been used in REINFORCE-based methods (Sutton et al., 2000) such as the MIXER architecture (Ranzato et al., 2015). These methods sample trajectories, series of consecutive states, actions and rewards, and use their true returns to update the policy. As they use the true returns, they are considered to be unbiased. However, as an action in a certain state can be part of many trajectories with different returns, these methods are considered to have high variance (Sutton and Barto, 2014). To address this issue, Actor-Critic algorithms (Konda and Tsitsiklis, 2001) adopt the Temporal Difference (TD) learning method which performs bootstrapping by using only the immediate reward and estimated values to guide future action selection. The training objective used is Equation (6).\n\n\nActor-Critic Architectures\n\n\nActor-Critic with Q-Learning (ACQ) Model\n\nAn Actor-Critic model usually consists of an actor and a critic (Konda and Tsitsiklis, 2001). The two networks are neural networks parameterised by \u03b8 and \u03c6, respectively. The actor acts as the policy of the model while the critic is a function approximation network. One simple variant (ACQ) of the AC architecture is trained by maximising the Q function. In this variant, the critic is defined as a Q network approximating the true Q function. The actor's training objective is to maximise the probability of actions that yield high Q values. Using Q value estimates (Q \u03c6 (\u0177 1:t\u22121,i , w)) computed by the main critic, the actor's policy loss (L policy ) at each training timestep can be expressed as follows:\nL policy = \u2212[ 1 N N i=1 t w\u2208W \u03c0 \u03b8 (w|\u0177 1:t\u22121,i )Q \u03c6 (\u0177 1:t\u22121,i , w)](7)\nwhere N denotes the training batch size. The loss is calculated by summing over all the possible actions (w) in the entire vocabulary (W). Following (Bahdanau et al., 2017), to avoid early policy determination and gradient vanishing issues, the final actor loss (L ACQ\u2212actor ) consists of the policy loss (L policy ) and the Maximum Likelihood Estimation (MLE) loss (L mle ) from cross-entropy training (XENT) (weighted by \u03bb mle ). In other words, the addition of XENT is to address the problem of training collapse 1 , commonly encountered when applying RL in language tasks.\nL ACQ\u2212actor = L policy + \u03bb mle L mle(8)\nThe TD learning method, as mentioned previously, is used to train the critic network. It adopts the bootstrapping methodology which performs estimation based on other known estimates. The critic's training objective is to minimise the mean squared difference, called TD error, between all estimated Q values and their corresponding target values in each timestep. Intuitively, the critic is trained to be as good of a Q function approximator as possible:\nL T D = 1 N N i=1 t (Q \u03c6 (\u0177 1:t\u22121,i ,\u0177 t )\u2212 Q\u03c6(\u0177 1:t\u22121,i ,\u0177 t )) 2(9)\nEach targetQ\u03c6(\u0177 1:t\u22121,i ,\u0177 t ), expressed below, is defined as the sum of the immediate reward after generating\u0177 t and the expected Q value of the proceeding timestep, which is computed using another Q network named the target critic:\nQ\u03c6(\u0177 1:t\u22121,i ,\u0177 t ) = r t+1 + w\u2208W \u03c0 \u03b8 (w|\u0177 1:t,i )Q\u03c6(\u0177 1:t,i , w)(10)\nTo ensure stability, the weights of the target critic (\u03c6) are updated more slowly than the main critic with the linear interpolation between the current weights of the main and target critics. Also, following (Bahdanau et al., 2017), in addition to the TD error, the critic's loss (L ACQ\u2212critic ) contains an additional term, weighted by \u03bb var , which aims to minimise the variance in Q value estimation. Synchronous and Asynchronous ACQ Both the asynchronous and synchronous versions of the ACQ model can be easily constructed by deploying N actors with respective N critics on multiple threads. Each of the parallel actors is trained using different segments of the training dataset. For the synchronous variant, the weights of each of the actors are averaged to update the weights of the global agent. For the asynchronous variant, the global agent is updated by the local weights of each agent one by one. That is, during training, each thread-specific agent generates output sequences by sampling from its policy. Then, it performs loss computation and gradient accumulation until it reaches the pre-defined number of timesteps, in which it transfers the accumulated gradients to the global model. The global model subsequently performs an update on its parameters. As the last step of the asynchronous update, the parameters of the thread-specific agent invoking the update are synced with the parameters of the global model.\nL ACQ\u2212critic = L T D + \u03bb var 1 N N i=1 w\u2208W Q \u03c6 (\u0177 1:t\u22121,i , w) \u2212Q \u03c6 (\u0177 1:t\u22121,i ) 2 (11) Q \u03c6 (\u0177 1:t\u22121,i ) = 1 |W| w \u2208W Q \u03c6 (\u0177 1:t\u22121,i , w )(12)\n\nAdvantage Actor-Critic (A2C) Model\n\nAnother variant of the AC model is the Advantage Actor-Critic (A2C) architecture (Konda and Tsitsiklis, 2001). In this model, the critic is defined as a function approximator, parameterised by \u03c8, of the true V function. Compared to the first variant, the A2C model applies a different training objective to ACQ (Equation (6)). Given the state space in language tasks is massive, calculating the expectation term would be computationally expensive or even impossible. Therefore, the advantage function can be approximated by sampling once.\nA \u03c0 (s t , a t ) \u2248 r t+1 + \u03b3V \u03c0 (s t+1 ) \u2212 V \u03c0 (s t ) (13)\nThe actor network in the A2C model is trained in a similar fashion to that of ACQ. Here, the critic estimates the state values (i.e., V \u03c8 (\u0177 1:t\u22121,i ) and V \u03c8 (\u0177 1:t,i )) which are used by the actor to calculate the advantage value. The actor loss function (L A2C\u2212actor ) can be outlined as follows:\nL policy = \u2212[ 1 N N i=1 t log \u03c0 \u03b8 (\u0177 t |\u0177 1:t\u22121,i )A \u03c8 (\u0177 1:t\u22121,i ,\u0177 t )](14)A \u03c8 (\u0177 1:t\u22121,i ,\u0177 t ) = r t+1 + \u03b3V \u03c8 (\u0177 1:t,i )\u2212 V \u03c8 (\u0177 1:t\u22121,i )(15)L A2C\u2212actor = L policy + \u03bb mle L mle(16)\nFurthermore, compared to the first variant, the A2C critic is trained to minimise the TD error between its estimation and ground-truth data. The ground-truth data is essentially the true discounted reward-to-go (v t ).\nL A2C\u2212critic = 1 N N i=1 t (V \u03c8 (\u0177 1:t\u22121,i ) \u2212 v t ) 2(17)\nSynchronous and Asynchronous A2C The model setups for the synchronous and asynchronous A2C variants are analogous to the ACQ variants, in which N pairs of actors and critics are deployed.\n\nIn the asynchronous A2C architecture (A3C) (Mnih et al., 2016), the model also employs n-step TD Learning which uses the true returns from multiple steps in the advantage function to reduce the model bias (Sutton and Barto, 2014). The term n defines the number of steps to use the real rewards before bootstrapping (using the critic). The standard TD Learning would just use the immediate reward (1-step TD).\nA \u03c8 (\u0177 1:t\u22121,i ,\u0177 t ) = n\u22121 \u03c4 =0 \u03b3 \u03c4 r t+\u03c4 + \u03b3 n V \u03c8 (\u0177 1:t+n\u22121,i ) \u2212 V \u03c8 (\u0177 1:t\u22121,i )(18)\nwhere \u03b8 and \u03c8 represent the thread-specific parameters of each actor and critic, respectively.\n\n\nSemantic-level Unsupervised Reward Function (SURF)\n\nOur semantic-level unsupervised reward, SURF, is based on two scores: Sentence Fluency and Sentence-level Semantic Similarity (SLSS) (Song et al., 2021). Each score assesses translation quality of generated sequences from different aspects and is computed using a pre-trained model. To prevent reward sparsity, the reward function introduces a score normalisation mechanism which normalises scores of a generated sequence (from all timesteps) with respect to the score of its full target sequence. This subsequently yields an unsupervised reward function that is maximally dense. The Sentence Fluency score (F (\u0177 1:t )) is defined as the average log-likelihood of the generated sequence tokens (\u0177 1:t ) as defined by a pre-trained large LM. The SLSS score measures the overall semantic similarity between the entire generated sequence and its source sequence calculated as the cosine similarity between the two sentence cross-lingual embeddings.\n\nScore Normalisation From the RL perspective, the MT task does not define the environment component that the agent operates in. That is, unlike the classical RL setting where the environment is relatively fixed, the 'environment' in the MT task is mostly dependent on the source sequence, in terms of its sophistication, structure, length, etc. As a result, a valid and good translation of a source sequence would receive a relatively high score but is not directly comparable to other sequences due to the difference in source sentence complexity. Therefore, it is important to ensure that the reward function is uniform and generalised across all source sentences.\n\nIn order to do this for each source sequence, the reward function uses the corresponding target sequence as a 'soft' upper bound for what a machine-generated sequence could achieve. That is, for each of the two score metrics outlined above, the scores from all timesteps received by a generated sequence is normalised to the range 0 to 1 with respect to the score of the full target sequence. To demonstrate the normalisation method, let us consider the formulation below which uses the Sentence Fluency score metric as an example. Given a pair of source (X) and target (Y) sequences and a candidate sequence (\u0177 1:t ), the fluency scores from all timesteps of\u0177 1:t would be {F (\u0177 1 ), F (\u0177 1:2 ), ..., F (\u0177 1:t\u22121 ), F (\u0177 1:t )} while the fluency score for the entire reference target sequence (Y) would be F (Y). Using the fluency scores of the candidate and that of the reference, the normalised candidate scores can be calculated as follows:\nF norm (\u0177 1:i ) = F (\u0177 1:i )\u2212min max\u2212min if max = min 0.5 if max = min,(19)where max = max({F (\u0177 1 ), ..., F (\u0177 1:t ), F (Y)}) (20) min = min({F (\u0177 1 ), ..., F (\u0177 1:t ), F (Y)}) (21)\nConsidering the example formulation above, one can observe that the normalisation with respect to the reference score is considered as a 'soft' upper bound as it allows for candidate scores to be higher than the reference score (i.e., allowing the possi-bility that candidate sequences can be better than their references).\n\nThe Pay-off Function and Reward Shaping Given a generated sequence\u0177 1:t at timestep t, its quality can be formulated as the Pay-off Function (P O(\u0177 1:t , X)). The Pay-off Function, as expressed below, is based on the Sentence Fluency and SLSS scores described above.\nP O(\u0177 1:t , X) = w F \u00d7e F (\u0177 1:t ) +w S \u00d7e SLSS(\u0177 1:t ,X)(22)\nwhere, for simplicity, F (\u0177 1:t ) and SLSS(\u0177 1:t , X) denote the normalised Sentence Fluency and SLSS scores respectively. The terms w F and w S are fixed weights controlling the relative importance of Sentence Fluency and SLSS, respectively. It is important to emphasise that the exponential function is applied to each score since linearly adding each score leads to high variance and lower correlation with human scores (Song et al., 2021).\n\nUsing the Pay-off Function to determine the effectiveness of generating a token (\u0177 t ) at timestep t, the final reward function is defined using reward shaping as the difference between the current Payoff and the Pay-off of the previous timestep.\nR(\u0177 t ) = P O([\u0177 1:t\u22121 ,\u0177 t ], X) \u2212 P O(\u0177 1:t\u22121 , X)(23)\n5 Experimental Settings\n\n\nData\n\nIn our experiments, we used the German-English OpenSubtitles corpus (Lison and Tiedemann, 2016). There are approximately 14 million sequence pairs in this dataset extracted from subtitles of movies and TV shows, making it very diverse. The dataset was then divided into training, validation and test sets. The training set has approximately 13 million sentence pairs while each of the validation and test sets has roughly 5,000 sentence pairs.\n\nThe trained models were also tested on translating the IWSLT 2014 German-English test dataset, a popular dataset to benchmark RL-based methods. This dataset contains a parallel corpora with one reference per source sequence, obtained from TED talks (Cettolo et al., 2015). The test dataset contains approximately 6,000 pairs, with each sequence containing a few sentences of text. See Appendix A.2 for justification for treating the two datasets as coming from different domains.\n\n\nTraining\n\nFollowing (Bahdanau et al., 2016), to ensure good initialisation of the model, the actor network is first pre-trained using XENT and the teacher forcing method. After that, the critic is pre-trained using TD Learning with the fixed pre-trained actor weights. At the last step, we train the actor and the critic jointly.\n\nGenerally, the actor architecture follows the OpenNMT Transformer architecture (Klein et al., 2017), with a few enhancements to enable stepwise decoding during training (i.e., action selection based on the model's previous outputs) and diversity in each agent's exploration strategy (see Appendix A.3 for more details). During training the actor selects a token at each timestep using the Top-K sampling method (Fan et al., 2018), in which a token is sampled from K tokens with the highest probabilities.\n\nThe critic architecture follows the Transformer architecture (Vaswani et al., 2017), with two major differences (see Appendix A.3.2 for detailed explanation). There are also two critic types, namely Q-critic and V-critic. The first critic type, Q-critic, is used in model variants with Q-Learning while the second type, V-critic, is used in other variants utilising the A2C architecture.\n\nMulti-GPU Training When training synchronously in a multi-GPU environment, the model is deployed on a one-model-per-device basis to reduce training time. Each model has its own optimiser (we used Adam (Kingma and Ba, 2014)). During every update, the gradients are reduced and re-scaled across all devices to ensure that they are consistent across the models.\n\nHowever, when training asynchronously, the global agent resides on one GPU device while three parallel agents are deployed on the remaining GPU devices. Instead of using one optimiser per agent, only a global optimiser is used. On every asynchronous update, the global optimiser updates the global model by using the gradients transferred from the parallel agent which invoked the update. The global optimiser used (SharedAdam) is a standard Adam optimiser modified to support multi-GPU communication.\n\nOur formulation of the unsupervised reward uses the pre-trained OpenAI GPT Language Model from Hugging Face (Wolf et al., 2020). It also uses the Sentence Transformers tool (Reimers and Gurevych, 2020) with the XLM RoBERTa model (Conneau et al., 2020) to generate the sentence embeddings. We do not expect the performance to change drastically by using other models.\n\nDuring the joint actor-critic training, it took approximately one day to train each of the BLEU variants while the training time for SURF variants ranges from 3 to 5 days (see Appendix A.5 for the computational resource used). The increase in training time is mainly due to the usage of the large pre-trained models by SURF. We expect this time to reduce if a smaller language model is used. We leave this investigation to future work.\n\nMore details on the implementation and hyperparameters are given in Appendix A.3 and A.4.\n\nModel configurations. We experimented using the nine configurations listed below:\n\n\u2022 Transformer baseline (MLE, no RL) -stateof-the-art (SOTA) model;\n\n\u2022 Synchronous ACQ RL architecture with the standard BLEU reward (ACQ-BLEU) and with our SURF (ACQ);\n\n\u2022 Asynchronous ACQ RL architecture with the standard BLEU reward (Async-ACQ-BLEU) and with our SURF (Async-ACQ);\n\n\u2022 Synchronous A2C RL architecture with the standard BLEU reward (A2C-BLEU) and with our SURF (A2C);\n\n\u2022 Asynchronous A2C RL architecture with the standard BLEU reward (A3Q-BLEU) and with our SURF (A3C).\n\n\nEach model was trained on the OpenSubtitles dataset and tested on both OpenSubtitles (indomain) and IWSLT (out-of-domain) test sets.\n\nBy choosing this selection of models we are able to do the following: (a) generate the benchmark result using the Transformer baseline; (b) exhibit the advantage of SURF over the BLEU reward; and finally (c) explore the performance of SURF within the family of the multi-agent models (ACQ, Async-ACQ, A2C and A3C).\n\nWe used the standard set of MT evaluation metrics: BLEU (Papineni et al., 2002) and ME-TEOR (Denkowski and Lavie, 2014). We performed significance testing via bootstrap resampling using the Multeval tool (Clark et al., 2011).  To probe the generalisation capacity of our models in the out-of-domain scenario, we have applied our models to the IWSLT test set. As shown in Table 2, the performance drop for the Transformer model is much higher than for our RL models (-6 BLEU vs. -2.4 BLEU on average). The Async-ACQ is the best-performing model on both metrics with the ACQ model being the second best (+6.3 BLEU and +5.8 BLEU vs. Transformer, re- Ranzato et al., 2015) 20.7 ---AC \u2021 (Bahdanau et al., 2016) 28.5 ---ERAC \u2021 (Dai et al., 2018) 29.0 30.6 --SAC-BLEU \u2021 (Ive et al., 2021) 29.6 31.0 --SAC-Unsuper \u2021 (Ive et al., 2021) 29.8 31.  By way of offering a guideline of our model performance, we also report the scores of the previous SOTA on the IWSLT test set. Though those results are not directly comparable to our results as the pre-processing conditions are different: previous models have mainly applied a cut-off vocabulary implying the presence of unknown words in the training data while we are using the subword units that dispense us of the unknown words.\nModel B M B M MIXER \u2021 (\nNote that, on both test sets, the asynchronous variants (Async-ACQ and A3C) performed better than their corresponding synchronous counterparts (ACQ and A2C respectively). We emphasise the potential of asynchronicity with our dense reward to positively influence performance.\n\nAdditionally, regarding the usage of Q-or Vcritics: both Q-version Async-ACQ and V-version A3C have shown comparable performance on the OpenSubtitles dataset. However, the Q-version Async-ACQ has achieved better performance on the IWSLT test set. We hypothesise that this may be due to the fact that the Q-critic network in the ACQ architecture outputs the state-action values of the entire vocabulary at each timestep rather than a single state value (as in the V-critic network). Hence it performs a more fine-grained policy evaluation with lower variance in the critic outputs, leading to a more stable model overall. A more thorough investigation would lead to better insights.\n\n\nPerformance on Semantic-level Evaluation Metrics\n\nAs with the automatic evaluation results, similar conclusions could be drawn from the results of the assessment with the three semantic metrics used in the reward formulation: Sentence Fluency, Tokenlevel Semantic Similarity (TLSS) and Sentencelevel Semantic Similarity (SLSS) scores (See Appendix A.6 for the description of the TLSS score). For the IWSLT test set, as shown on Table 4, there is a noticeable increase in the Fluency score across our models (in comparison to the Transformer). ACQ and Async-ACQ are also able to achieve distinctly better TLSS ans SLSS scores than the Transformer model. We observe that variants of ACQ and Async-ACQ models achieve similar performance. When comparing the BLEU and SURF variants, the BLEU variants of A2C and A3C models obtain higher Fluency scores but score less on TLSS and SLSS. This can be explained by fact that BLEU RL sentences are prone to be more verbose, repeating the same meaning in different words. The results for OpenSubtitles show similar tendencies (see Appendix A.7).\n\n\nHuman Evaluation\n\nFinally, to gain deeper insights, we performed human evaluation on the translations of the OpenSubtitles and IWSLT 2014 test sets by the Transformer, Async-ACQ-BLEU and the best performing SURF variants (A3C for OpenSubtitles and Async-ACQ for IWSLT).\n\nFor this human analysis, we randomly selected 50 test samples from each test set. A rank of quality is assigned by the human evaluator (second author, Source all dies wurzelt in der mythologischen vergangenheit . das eigenartige an diesen gro\u00dfen h\u00e4usern , in denen aufgrund der mischehen sechs oder sieben sprachen gesprochen werden , ist jedoch , dass man niemals jemanden h\u00f6rt , der eine sprache lernt . Target and this is all rooted in the mythological past , yet the curious thing is in these long houses , where there are six or seven languages spoken because of intermarriage , you never hear anyone practicing a language . Transformer all this mythology in the mythological context , which is curious about these great houses , judging by the patter of six or seven languages , however , you never hear anyone learning a language . Async-ACQ-BLEU the strange thing about these big houses where they speak six or seven languages , is that you never hear anyone who learns a language . Async-ACQ all this rambling in the mythological past , the curious thing about these big houses where they speak six or seven languages based on the basic language is that you never hear anyone who learns a language.   fluent speaker of both English and German) from 1 to 3, allowing ties. Following the common practice in MT, each system was then assigned a score which reflects how often on average it was judged to be better or equal to other systems (Bojar et al., 2017). Table 5 shows that most of our variants have higher evaluation scores than the Transformer model. In particular, on the IWSLT test set, both Async-ACQ variants outperform the Transformer by a large margin. As compared to the best BLEU model, the A3C and Async-ACQ models perform significantly better on both the OpenSubtitles and the IWSLT test sets (+0.12 point). Table 3 shows translations generated by the three models on the IWSLT test set. Note that Async-ACQ demonstrates the best fluency and adequacy.\n\n\nConclusion\n\nWe have presented SURF, a new unsupervised semantic-level reward function, efficiently addressing the reward sparsity issue and mimicking human evaluation by considering both sentence fluency and semantic similarity. We have explored this reward for a new family of Actor-Critic Transformerbased Architectures with synchronous and asynchronous variants that promote the exploration of the search space. We demonstrate that SURF shows strong potential to elicit the benefits of various RL architectures. Our results show that it outperforms the traditional sparse BLEU reward for the same architectures in the automatic, semantic-level and human evaluation. Our code is available at https://github.com/AtomAnu/SURF. There are several directions to take our work further: we can investigate the utility of our reward for other architectures and we can also explore different sampling strategies for each of the agents of our multi-agent models. Finally, we have investigated only two datasets to ensure comparability to the existing benchmarks. A more thorough investigation with more datasets and language pairs is needed to fully assess the scope of our contribution. probable tokens while reducing others that are not probable.\n\np(w i |\u0177 1:t\u22121 ) = exp (l i /temp)\nj exp (l j /temp)(26)\nwhere l i is the logit of the token w i . One can observe that, as temp decreases, the probability of probable tokens would increase. If temp is set to 1, the above expression would simplify to the normal softmax operation.\n\nWe have empirically found that the sampling Temperature should not be applied in conjunction with Top-K or Nucleus sampling as it leads to highly greedy policies, especially when K or p is already low. It should be applied on when the agent samples from the entire vocabulary. After experimenting with different configurations of exploration strategies, we found that Top-K sampling was the most effective.\n\n\nA.3.2 The Critic\n\nThe critic architecture follows the Transformer architecture, with two major differences.\n\nReference Encoding The first difference between the actor and the critic is that the encoder of the critic encodes the reference sequences instead of the source sequences. The reason is to allow to critic to evaluate each generated sequence by comparing with its reference sequence.\n\n\nOutput Layer\n\nThe second difference is the output layer used in the critic. In the Q-critic model, its output layer is a one-layer feed-forward network with the output dimension equal to the vocabulary size of the target language. This is because the Q-critic model outputs the state-action value (i.e., Q-value) for every word in the vocabulary.\n\nFor the V-critic model, its output layer also contains a one-layer feed-forward network with the output size of 1. Given a generated sequence, the critic outputs the state value (i.e., V-value) for each token in that sequence.\n\n\nA.4 Hyper-parameters\n\nA.4.1 Actor Pre-training Table 6 lists all the hyper-parameters used during actor pre-training.    Table 8 lists all the hyper-parameters used during synchronous and asynchronous joint Actor-Critic training.\n\n\nA.4.2 Critic Pre-training\n\n\nA.4.3 Joint Actor-Critic Training\n\n\nA.5 Computational Resource\n\nEach of our models was trained on a GPUaccelerated instance with four NVIDIA V-100 SXM2 GPUs.\n\n\nA.6 Token-level Semantic Similarity\n\nThe Token-level Semantic Similarity (TLSS) score is used as one of the semantic-level evaluation metrics. It can be used as an additional score metric in the reward function as well. TLSS measures the semantic similarity between tokens in the gener-  ated sequence and its source sequence. Following the methodology adopted in the SentSim evaluation metric (Song et al., 2021), each token in the source and generated sequences is passed to a crosslingual language model to obtain its cross-lingual embedding. Then, each token in the source sequence (x i ) is matched to a token in the generated sequence (\u0177 j ) with the highest cosine similarity value to compute the recall. Similarly, the precision value is computed by matching each token in the generated sequence to a token in the source sequence based on cosine similarity. As a results, the recall and precision of a generated sequence can be expressed as follows:\n\nR(\u0177 1:t , X) = 1 |X| x i \u2208X max T LSS(\u0177 1:t , X) = F (\u0177 1:t , X)\n\n= 2 P (\u0177 1:t , X) \u00b7 R(\u0177 1:t , X) P (\u0177 1:t , X) + R(\u0177 1:t , X)\n\nThe TLSS scores are computed using the BERTScore tool (Zhang et al., 2020).\n\n\nA.7 Semantic-level Evaluation on the OpenSubtitles Test Set\n\nFor the OpenSubtitles test set, as shown in Table 9, there is a slight increase in the Fluency scores for all our model as compared to the Transformer. There are more apparent increases in the TLSS and SLSS scores. Among all the variants, the ACQ model achieves the highest on all three scores. The Async-ACQ model is the second best with its scores being very close to the scores of the ACQ model. When comparing the reward functions, the BLEU and SURF variants of the ACQ and Async-ACQ models achieve similar performance. However, for the A2C and A3C models, the BLEU variants achieve higher Fluency scores than the SURF variants but their SLSS scores are noticeably lower than that of SURF. This can be explained by fact that BLEU RL sentences are prone to be more verbose, repeating the same content. This was observed during human evaluation (See Subsection 6.3).  Table 9: Results for the OpenSubtitles German-English test set. We report Sentence Fluency, Token-level Semantic Similarity and Sentence-level Semantic Similarity scores. Also, the symbol indicates statically significant changes (p-value \u2264 0.05) as compared to the scores of Transformer model while \u2020 indicates statically significant changes (p-value \u2264 0.05) as compared to the BLEU variant of the same RL-based architecture. The best result is highlighted in bold. Note that some of the improvements are beyond the displayed precision of 3 decimal points.\n\nFigure 1 :\n1High-level structure of the Asynchronous Actor-Critic with Q-Learning Model (Async-ACQ) and the Asynchronous Advantage Actor-Critic (A3C) Model: multiple parallel agents and critics are trained independently. Their weights are used to update the weights of the global agent one by one.\n\ni\n|| ) cross-lingual embeddings of x i and\u0177 j , respectively. Using the precision and recall, the final TLSS score is defined as the F1 measure.\n\nTable 2 :\n2Results for the IWSLT 2014 German-English test set. We report BLEU (B) and METEOR (M) scores. \nThe symbol indicates statically significant changes (p-value \u2264 0.05) as compared to the Transformer model while \n \u2020 indicates statically significant changes (p-value \u2264 0.05) as compared to the BLEU variant of the same RL-based \narchitecture. The best result is highlighted in bold. \n\nspectively). Mostly, our unsupervised reward con-\ntributes around +0.8 BLEU to the performance of \neach model. Especially high improvement of +10 \nBLEU is again observed for A3C showing the po-\ntential of SURF. \n\n\n\nTable 3 :\n3Examples of translation generated by Transformer, ACQ-BLEU and Async-ACQ. We also report the \noriginal source sequence (SOURCE) and its reference (TARGET). The best translation is highlighted in italics. \n\nModel \nFluency TLSS \nSLSS \nFluency TLSS \nSLSS \nTransformer \n1.024 \n2.454 \n2.339 \n-\n-\n-\nBLEU \nSURF \nACQ \n1.029 \n2.456 \n2.350 \n1.029  \u2020 2.457  \u2020 2.357  \u2020 \nAsync-ACQ \n1.032 \n2.455 \n2.350 \n1.029  \u2020 2.456  \u2020 2.350  \u2020 \nA2C \n1.035 \n2.451 \n2.300 \n1.027  \u2020 2.454  \u2020 2.339  \u2020 \nA3C (Async-A2C) 1.047 \n2.422 \n2.267 \n1.027  \u2020 2.454  \u2020 2.342  \u2020 \n\n\n\nTable 4 :\n4Results for the IWSLT 2014 German-English test set. We report Sentence Fluency, Token-level Semantic Similarity and Sentence-level Semantic Similarity scores. Also, the symbol indicates statically significant changes (p-value \u2264 0.05) as compared to the scores of Transformer model while \u2020 indicates statically significant changes (p-value \u2264 0.05) as compared to the BLEU variant of the same RL-based architecture. The best result is highlighted in bold. Note that some of the improvements are beyond the displayed precision of 3 decimal points.Test Set Transformer BLEU SURF \nOS \n0.10 \n0.08 \n0.20 \nIW \n0.06 \n0.48 \n0.60 \n\n\n\nTable 5 :\n5Human ranking results comparing the Open-\nSubtitles (OS) test outputs for Async-ACQ-BLEU and \nA3C and the IWSLT 2014 (IW) test outputs for Async-\nACQ-BLEU and Async-ACQ. The best result is high-\nlighted in bold. \n\n\n\nTable 7\n7lists all the hyper-parameters used during critic pre-training.Hyper-parameter \nValue \nOptimizer \nAdam \nAdam Beta 1 \n0.9 \nAdam Beta 2 \n0.998 \nLearning Rate \n2 \nLR Decay Method \nnoam \nWarmup Steps \n6000 \nBatch Size \n4096 \nGradient Accumulation Steps 3 \nSource Vocabulary Size \n100000 \nTarget Vocabulary Size \n100000 \nWord Embedding Size \n512 \nHidden Layers Size \n512 \nEncoder Layers \n6 \nDecoder Layers \n6 \nAttention Heads \n8 \n\n\n\nTable 6 :\n6List of the hyper-parameters used during the actor pre-training stage.Hyper-parameter \nValue \nOptimizer \nAdam \nAdam Beta 1 \n0.9 \nAdam Beta 2 \n0.998 \nLearning Rate \n0.001 \nLR Decay Rate \n0.9 \nLR Decay Steps \n1000 \nBatch Size \n4096 \nGradient Accumulation Steps \n3 \nSource Vocabulary Size \n100000 \nTarget Vocabulary Size \n100000 \nWord Embedding Size \n512 \nHidden Layers Size \n512 \nEncoder Layers \n6 \nDecoder Layers \n6 \nAttention Heads \n8 \n\u03b3 (Discount Factor) \n0.99 \n\u03bbvar (Q-critic) \n0.25 \nMulti-step Return (V-critic) \n5 \nwF (Sentence Fluency Weight) 1 \nwS (SLSS Weight) \n1 \n\n\n\nTable 7 :\n7List of the hyper-parameters used during the critic pre-training stage.\n\nTable 8 :\n8List of listing the hyper-parameters used during synchronous and asynchronous joint Actor-Critic training stage.\nAs pointed out by(Bahdanau et al., 2017), the MLE loss can help prevent early policy determination and vanishing gradient problems.\nA AppendixA.1 Ethics ConsiderationsWe are aware of the discussions around the risks related to unintended harmful effects and uses, environmental consequences, fairness and privacy considerations of large language models in general(Bender et al., 2021), and machine translation models specifically . We note here that our models constitute a primarily theoretical contribution and were trained and tested on standard datasets. Before deployment in a production setting our methodology is subject to retraining with data pre-processed in the appropriate way (as our model is not equipped with relevant security, privacy and fairness mechanisms), systematic debugging, extensive simulation, testing and validation under the supervision of experts.A.2 Domain DistanceTo justify that the IWSLT test set is indeed considered out-of-domain, we have trained a German language model using the source sentences (in German) from the OpenSubtitles training set. For this we used the fairseq toolkit(Ott et al., 2019). The resulting difference in language model perplexity values for the OpenSubtitles and IWSLT test sets (45.52 and 555.71, respectively) is important enough to justify that IWSLT is considered out-ofdomain.A.3 Implementation DetailsA.3.1 The ActorOpenNMT Transformer Implementation In the OpenNMT framework(Klein et al., 2017), the Transformer architecture is implemented slightly differently from the original architecture in(Vaswani et al., 2017). Its implementation follows the up-to-date implementation of the tensor2tensor framework(Vaswani et al., 2018), created by the authors of(Vaswani et al., 2017). The main difference lies in the normalisation technique used in the Transformer. That is, pre-normalisation is applied in each sub-layer of the encoder and the decoder instead of post-normalisation. The output of each sub-layer with pre-normalisation can be expressed as follows:x + Sublayer(LayerN orm(x))In the original architecture where postnormalisation is used, layer normalisation (LayerN orm) is applied after the summation (x + Sublayer(x)), as shown below:Step-wise Decoding and Exploration Strategies During the joint AC training, instead of just computing the policy distributions as the output, the actor would perform step-wise decoding by selecting a token to generate at each timestep, given the encoded source sequence and the previously generated tokens. As mentioned before, this is done to ensure that there would be no exposure bias during inference as the model is trained to condition the generation process using its own outputs.To allow each agent to be diverse in their exploration strategies, the actor can operate in two possible main modes of token selection. In the first mode, the actor selects a token at each timestep using the Top-K sampling method(Fan et al., 2018), in which a token is sampled from K tokens with the highest probabilities. In the second mode of operation, the actor performs token selection based on Nucleus sampling(Holtzman et al., 2020). In the Nucleus sampling method, a token is chosen from the smallest possible set of tokens that has an accumulated probability equal or higher than a pre-defined probability value (p). For instance, if p is set to 1, the actor would perform token selection from the entire vocabulary. Similarly, if the value of K for Top-K sampling is set to the vocabulary size, the actor would sample from the entire vocabulary as well.The actor also incorporates the notion of Temperature to further increase the diversity of exploration strategies. A pre-defined value of Temperature (temp) is used to increase the probability of\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.07086arXiv preprintDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086.\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arxiv:14090473Comment: Accepted at ICLR 2015 as oral presentationDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Cite arxiv:1409.0473Comment: Accepted at ICLR 2015 as oral presentation.\n\nOn the dangers of stochastic parrots: Can language models be too big. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922FAccT '21. New York, NY, USAAssociation for Computing MachineryEmily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? . FAccT '21, page 610-623, New York, NY, USA. Association for Computing Machinery.\n\nFindings of the 2017 conference on machine translation (WMT17). Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, Marco Turchi, 10.18653/v1/W17-4717Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationCopenhagen, DenmarkAssociation for Computational LinguisticsOnd\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Lo- gacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Sec- ond Conference on Machine Translation, pages 169- 214, Copenhagen, Denmark. Association for Com- putational Linguistics.\n\nReport on the 11 th iwslt evaluation campaign. M Cettolo, J Niehues, S St\u00fcker, L Bentivogli, Marcello Federico, iwsltM. Cettolo, J. Niehues, S. St\u00fcker, L. Bentivogli, and Marcello Federico. 2015. Report on the 11 th iwslt evaluation campaign , iwslt 2014.\n\nOn the weaknesses of reinforcement learning for neural machine translation. Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend, International Conference on Learning Representations. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations.\n\nBetter hypothesis testing for statistical machine translation: Controlling for optimizer instability. Jonathan H Clark, Chris Dyer, Alon Lavie, Noah A Smith, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USAAssociation for Computational LinguisticsJonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for op- timizer instability. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 176-181, Portland, Oregon, USA. Association for Computational Linguistics.\n\nUnsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, 10.18653/v1/2020.acl-main.747Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 8440- 8451, Online. Association for Computational Lin- guistics.\n\nFrom credit assignment to entropy regularization: Two new algorithms for neural sequence prediction. Zihang Dai, Qizhe Xie, Eduard Hovy, arXiv:1804.10974arXiv preprintZihang Dai, Qizhe Xie, and Eduard Hovy. 2018. From credit assignment to entropy regularization: Two new algorithms for neural sequence prediction. arXiv preprint arXiv:1804.10974.\n\nMeteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, Proceedings of the EACL 2014 Workshop on Statistical Machine Translation. the EACL 2014 Workshop on Statistical Machine TranslationMichael Denkowski and Alon Lavie. 2014. Meteor uni- versal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.\n\nHierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, 10.18653/v1/P18-1082Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi- erarchical neural story generation. pages 889-898.\n\nSU-PERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. Yang Gao, Wei Zhao, Steffen Eger, 10.18653/v1/2020.acl-main.124Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsYang Gao, Wei Zhao, and Steffen Eger. 2020. SU- PERT: Towards new frontiers in unsupervised evalu- ation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 1347- 1354, Online. Association for Computational Lin- guistics.\n\nUnsupervised contextual paraphrase generation using lexical control and reinforcement learning. Sonal Garg, Sumanth Prabhu, Hemant Misra, G Srinivasaraghavan, Sonal Garg, Sumanth Prabhu, Hemant Misra, and G. Srinivasaraghavan. 2021. Unsupervised contex- tual paraphrase generation using lexical control and reinforcement learning.\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, International Conference on Learning Representations. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text de- generation. In International Conference on Learn- ing Representations.\n\nReinforced mnemonic reader for machine reading comprehension. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, Ming Zhou, 10.24963/ijcai.2018/570Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In Proceedings of the Twenty-Seventh In- ternational Joint Conference on Artificial Intelli- gence, IJCAI-18, pages 4099-4106. International Joint Conferences on Artificial Intelligence Organi- zation.\n\nExploring supervised and unsupervised rewards in machine translation. Julia Ive, Zixu Wang, Marina Fomicheva, Lucia Specia, 10.18653/v1/2021.eacl-main.164Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational LinguisticsJulia Ive, Zixu Wang, Marina Fomicheva, and Lucia Specia. 2021. Exploring supervised and unsuper- vised rewards in machine translation. In Proceed- ings of the 16th Conference of the European Chap- ter of the Association for Computational Linguistics: Main Volume, pages 1908-1920, Online. Associa- tion for Computational Linguistics.\n\nRevisiting the weaknesses of reinforcement learning for neural machine translation. Samuel Kiegeland, Julia Kreutzer, 10.18653/v1/2021.naacl-main.133Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsSamuel Kiegeland and Julia Kreutzer. 2021. Revisiting the weaknesses of reinforcement learning for neu- ral machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1673-1681, Online. Association for Computational Linguistics.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, International Conference on Learning Representations. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations.\n\nOpenNMT: Opensource toolkit for neural machine translation. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander Rush, Proceedings of ACL 2017, System Demonstrations. ACL 2017, System DemonstrationsVancouver, CanadaAssociation for Computational LinguisticsGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel- lart, and Alexander Rush. 2017. OpenNMT: Open- source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations, pages 67-72, Vancouver, Canada. Association for Computational Linguistics.\n\nActor-critic algorithms. Vijay Konda, John Tsitsiklis, Society for Industrial and Applied Mathematics. 42Vijay Konda and John Tsitsiklis. 2001. Actor-critic al- gorithms. Society for Industrial and Applied Mathe- matics, 42.\n\nOpenSub-titles2016: Extracting large parallel corpora from movie and TV subtitles. Pierre Lison, J\u00f6rg Tiedemann, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)SloveniaPortoro\u017eEuropean Language Resources Association (ELRAPierre Lison and J\u00f6rg Tiedemann. 2016. OpenSub- titles2016: Extracting large parallel corpora from movie and TV subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 923-929, Por- toro\u017e, Slovenia. European Language Resources As- sociation (ELRA).\n\nPutting evaluation in context: Contextual embeddings improve machine translation evaluation. Nitika Mathur, Timothy Baldwin, Trevor Cohn, 10.18653/v1/P19-1269Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsNitika Mathur, Timothy Baldwin, and Trevor Cohn. 2019. Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799-2808, Florence, Italy. Association for Compu- tational Linguistics.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, PMLRProceedings of The 33rd International Conference on Machine Learning. The 33rd International Conference on Machine LearningNew York, New York, USA48Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn- chronous methods for deep reinforcement learning. In Proceedings of The 33rd International Confer- ence on Machine Learning, volume 48 of Proceed- ings of Machine Learning Research, pages 1928- 1937, New York, New York, USA. PMLR.\n\nfairseq: A fast, extensible toolkit for sequence modeling. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, Proceedings of NAACL-HLT 2019: Demonstrations. NAACL-HLT 2019: DemonstrationsMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\nA deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, International Conference on Learning Representations. Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In International Conference on Learn- ing Representations.\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732.\n\nMaking monolingual sentence embeddings multilingual using knowledge distillation. Nils Reimers, Iryna Gurevych, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational LinguisticsNils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual us- ing knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing. Association for Computa- tional Linguistics.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. 2017 IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 1179-1195.\n\nSentSim: Crosslingual semantic evaluation of machine translation. Yurun Song, Junchen Zhao, Lucia Specia, 10.18653/v1/2021.naacl-main.252Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsYurun Song, Junchen Zhao, and Lucia Specia. 2021. SentSim: Crosslingual semantic evaluation of ma- chine translation. In Proceedings of the 2021 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 3143-3156, Online. As- sociation for Computational Linguistics.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Z. Ghahramani, M. Welling, C. Cortes, N. DIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D.\n\nK Q Lawrence, Weinberger, Advances in Neural Information Processing Systems. Curran Associates, Inc27Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3104-3112. Curran Associates, Inc.\n\nR S Sutton, A G Barto, Reinforcement Learning: An Introduction. Cambridge, MassachusettsThe MIT Presssecond edition editionR. S. Sutton and A. G. Barto. 2014. Reinforcement Learning: An Introduction, second edition edition. Cambridge, Massachusetts: The MIT Press.\n\nPolicy gradient methods for reinforcement learning with function approximation. S Richard, David Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in Neural Information Processing Systems. MIT Press12Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approxima- tion. In Advances in Neural Information Processing Systems, volume 12. MIT Press.\n\n. Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, Jakob Uszkoreit, Tensor2tensor for neural machine translation. CoRR, abs/1803.07416Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran- cois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30, pages 2-6. Curran As- sociates, Inc.\n\nOn exposure bias, hallucination and domain shift in neural machine translation. Chaojun Wang, Rico Sennrich, 10.18653/v1/2020.acl-main.326Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsChaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural ma- chine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 3544-3552, Online. Association for Computational Linguistics.\n\nOn the language coverage bias for neural machine translation. Shuo Wang, Zhaopeng Tu, Zhixing Tan, Shuming Shi, Maosong Sun, Yang Liu, 10.18653/v1/2021.findings-acl.422Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsOnlineShuo Wang, Zhaopeng Tu, Zhixing Tan, Shuming Shi, Maosong Sun, and Yang Liu. 2021. On the language coverage bias for neural machine translation. In Findings of the Association for Computational Lin- guistics: ACL-IJCNLP 2021, pages 4778-4790, On- line. Association for Computational Linguistics.\n\nUnsupervised neural machine translation with future rewarding. Xiangpeng Wei, Yue Hu, Luxi Xing, Li Gao, 10.18653/v1/K19-1027Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Hong Kong, ChinaAssociation for Computational LinguisticsXiangpeng Wei, Yue Hu, Luxi Xing, and Li Gao. 2019. Unsupervised neural machine translation with fu- ture rewarding. In Proceedings of the 23rd Confer- ence on Computational Natural Language Learning (CoNLL), pages 281-290, Hong Kong, China. Asso- ciation for Computational Linguistics.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning, 8(3-4):229-256.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Drame, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsQuentin Lhoest, and Alexander RushOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\nBertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Conference on Learning Representations.\n", "annotations": {"author": "[{\"end\":153,\"start\":93},{\"end\":181,\"start\":154},{\"end\":213,\"start\":182},{\"end\":242,\"start\":214}]", "publisher": null, "author_last_name": "[{\"end\":113,\"start\":100},{\"end\":163,\"start\":160}]", "author_first_name": "[{\"end\":99,\"start\":93},{\"end\":159,\"start\":154}]", "author_affiliation": "[{\"end\":212,\"start\":183},{\"end\":241,\"start\":215}]", "title": "[{\"end\":74,\"start\":1},{\"end\":316,\"start\":243}]", "venue": "[{\"end\":460,\"start\":318}]", "abstract": "[{\"end\":1705,\"start\":605}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2237,\"start\":2212},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2746,\"start\":2723},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3188,\"start\":3166},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3540,\"start\":3510},{\"end\":5575,\"start\":5550},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5595,\"start\":5575},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5615,\"start\":5595},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5631,\"start\":5615},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5873,\"start\":5851},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6088,\"start\":6070},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6105,\"start\":6088},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6483,\"start\":6465},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6521,\"start\":6503},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6539,\"start\":6521},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6571,\"start\":6554},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7187,\"start\":7167},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7207,\"start\":7187},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7354,\"start\":7337},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7522,\"start\":7504},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7540,\"start\":7522},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7857,\"start\":7833},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7879,\"start\":7857},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10176,\"start\":10155},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10230,\"start\":10208},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10606,\"start\":10582},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10682,\"start\":10655},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11064,\"start\":11036},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11926,\"start\":11903},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13433,\"start\":13410},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14921,\"start\":14894},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16427,\"start\":16408},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16594,\"start\":16570},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17166,\"start\":17147},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20851,\"start\":20832},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21285,\"start\":21258},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21906,\"start\":21884},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22160,\"start\":22137},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22547,\"start\":22527},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22877,\"start\":22859},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23037,\"start\":23015},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23565,\"start\":23544},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24333,\"start\":24314},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24407,\"start\":24379},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24457,\"start\":24435},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26201,\"start\":26178},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26241,\"start\":26214},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26347,\"start\":26326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26790,\"start\":26769},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26827,\"start\":26804},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26861,\"start\":26843},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26903,\"start\":26885},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26948,\"start\":26930},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31197,\"start\":31177},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35447,\"start\":35428},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36196,\"start\":36176},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":41423,\"start\":41400}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37986,\"start\":37688},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38132,\"start\":37987},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38737,\"start\":38133},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39289,\"start\":38738},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39923,\"start\":39290},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40150,\"start\":39924},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40587,\"start\":40151},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41173,\"start\":40588},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41257,\"start\":41174},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":41382,\"start\":41258}]", "paragraph": "[{\"end\":2419,\"start\":1721},{\"end\":3541,\"start\":2421},{\"end\":3971,\"start\":3543},{\"end\":5113,\"start\":3973},{\"end\":5227,\"start\":5115},{\"end\":5372,\"start\":5244},{\"end\":6343,\"start\":5421},{\"end\":6738,\"start\":6345},{\"end\":7591,\"start\":6740},{\"end\":7720,\"start\":7607},{\"end\":8170,\"start\":7757},{\"end\":9102,\"start\":8237},{\"end\":9584,\"start\":9161},{\"end\":9750,\"start\":9700},{\"end\":9890,\"start\":9803},{\"end\":10027,\"start\":9892},{\"end\":10898,\"start\":10079},{\"end\":11681,\"start\":10972},{\"end\":12330,\"start\":11754},{\"end\":12825,\"start\":12371},{\"end\":13130,\"start\":12896},{\"end\":14632,\"start\":13201},{\"end\":15351,\"start\":14813},{\"end\":15710,\"start\":15411},{\"end\":16116,\"start\":15898},{\"end\":16363,\"start\":16176},{\"end\":16773,\"start\":16365},{\"end\":16959,\"start\":16865},{\"end\":17959,\"start\":17014},{\"end\":18626,\"start\":17961},{\"end\":19571,\"start\":18628},{\"end\":20078,\"start\":19755},{\"end\":20346,\"start\":20080},{\"end\":20852,\"start\":20409},{\"end\":21100,\"start\":20854},{\"end\":21181,\"start\":21158},{\"end\":21633,\"start\":21190},{\"end\":22114,\"start\":21635},{\"end\":22446,\"start\":22127},{\"end\":22952,\"start\":22448},{\"end\":23341,\"start\":22954},{\"end\":23701,\"start\":23343},{\"end\":24204,\"start\":23703},{\"end\":24572,\"start\":24206},{\"end\":25009,\"start\":24574},{\"end\":25100,\"start\":25011},{\"end\":25183,\"start\":25102},{\"end\":25251,\"start\":25185},{\"end\":25352,\"start\":25253},{\"end\":25466,\"start\":25354},{\"end\":25567,\"start\":25468},{\"end\":25669,\"start\":25569},{\"end\":26120,\"start\":25806},{\"end\":27390,\"start\":26122},{\"end\":27689,\"start\":27415},{\"end\":28372,\"start\":27691},{\"end\":29458,\"start\":28425},{\"end\":29730,\"start\":29479},{\"end\":31707,\"start\":29732},{\"end\":32950,\"start\":31722},{\"end\":32986,\"start\":32952},{\"end\":33232,\"start\":33009},{\"end\":33640,\"start\":33234},{\"end\":33750,\"start\":33661},{\"end\":34034,\"start\":33752},{\"end\":34383,\"start\":34051},{\"end\":34611,\"start\":34385},{\"end\":34843,\"start\":34636},{\"end\":35031,\"start\":34938},{\"end\":35991,\"start\":35071},{\"end\":36057,\"start\":35993},{\"end\":36120,\"start\":36059},{\"end\":36197,\"start\":36122},{\"end\":37687,\"start\":36261}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8203,\"start\":8171},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9160,\"start\":9103},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9699,\"start\":9585},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9802,\"start\":9751},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10078,\"start\":10028},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11753,\"start\":11682},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12370,\"start\":12331},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12895,\"start\":12826},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13200,\"start\":13131},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14775,\"start\":14633},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15410,\"start\":15352},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15788,\"start\":15711},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15857,\"start\":15788},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15897,\"start\":15857},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16175,\"start\":16117},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16864,\"start\":16774},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19647,\"start\":19572},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19754,\"start\":19647},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20408,\"start\":20347},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21157,\"start\":21101},{\"attributes\":{\"id\":\"formula_20\"},\"end\":27414,\"start\":27391},{\"attributes\":{\"id\":\"formula_21\"},\"end\":33008,\"start\":32987}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26500,\"start\":26493},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28810,\"start\":28803},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31206,\"start\":31199},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31571,\"start\":31564},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":34668,\"start\":34661},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":34742,\"start\":34735},{\"end\":36312,\"start\":36305},{\"end\":37138,\"start\":37131}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1719,\"start\":1707},{\"attributes\":{\"n\":\"2\"},\"end\":5242,\"start\":5230},{\"end\":5419,\"start\":5375},{\"attributes\":{\"n\":\"3\"},\"end\":7605,\"start\":7594},{\"attributes\":{\"n\":\"3.1\"},\"end\":7755,\"start\":7723},{\"attributes\":{\"n\":\"3.2\"},\"end\":8235,\"start\":8205},{\"attributes\":{\"n\":\"3.3\"},\"end\":10927,\"start\":10901},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":10970,\"start\":10930},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":14811,\"start\":14777},{\"attributes\":{\"n\":\"4\"},\"end\":17012,\"start\":16962},{\"attributes\":{\"n\":\"5.1\"},\"end\":21188,\"start\":21184},{\"attributes\":{\"n\":\"5.2\"},\"end\":22125,\"start\":22117},{\"end\":25804,\"start\":25672},{\"attributes\":{\"n\":\"6.2\"},\"end\":28423,\"start\":28375},{\"attributes\":{\"n\":\"6.3\"},\"end\":29477,\"start\":29461},{\"attributes\":{\"n\":\"7\"},\"end\":31720,\"start\":31710},{\"end\":33659,\"start\":33643},{\"end\":34049,\"start\":34037},{\"end\":34634,\"start\":34614},{\"end\":34871,\"start\":34846},{\"end\":34907,\"start\":34874},{\"end\":34936,\"start\":34910},{\"end\":35069,\"start\":35034},{\"end\":36259,\"start\":36200},{\"end\":37699,\"start\":37689},{\"end\":37989,\"start\":37988},{\"end\":38143,\"start\":38134},{\"end\":38748,\"start\":38739},{\"end\":39300,\"start\":39291},{\"end\":39934,\"start\":39925},{\"end\":40159,\"start\":40152},{\"end\":40598,\"start\":40589},{\"end\":41184,\"start\":41175},{\"end\":41268,\"start\":41259}]", "table": "[{\"end\":38737,\"start\":38145},{\"end\":39289,\"start\":38750},{\"end\":39923,\"start\":39846},{\"end\":40150,\"start\":39936},{\"end\":40587,\"start\":40224},{\"end\":41173,\"start\":40670}]", "figure_caption": "[{\"end\":37986,\"start\":37701},{\"end\":38132,\"start\":37990},{\"end\":39846,\"start\":39302},{\"end\":40224,\"start\":40161},{\"end\":40670,\"start\":40600},{\"end\":41257,\"start\":41186},{\"end\":41382,\"start\":41270}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":45201,\"start\":45194},{\"end\":45220,\"start\":45212},{\"end\":45235,\"start\":45229},{\"end\":45247,\"start\":45240},{\"end\":45259,\"start\":45255},{\"end\":45272,\"start\":45266},{\"end\":45286,\"start\":45281},{\"end\":45304,\"start\":45298},{\"end\":45615,\"start\":45608},{\"end\":45634,\"start\":45626},{\"end\":45649,\"start\":45643},{\"end\":45661,\"start\":45654},{\"end\":45673,\"start\":45669},{\"end\":45686,\"start\":45680},{\"end\":45700,\"start\":45695},{\"end\":45718,\"start\":45712},{\"end\":45986,\"start\":45979},{\"end\":46006,\"start\":45997},{\"end\":46018,\"start\":46012},{\"end\":46370,\"start\":46365},{\"end\":46372,\"start\":46371},{\"end\":46387,\"start\":46381},{\"end\":46403,\"start\":46395},{\"end\":46430,\"start\":46420},{\"end\":46843,\"start\":46837},{\"end\":46856,\"start\":46851},{\"end\":46878,\"start\":46869},{\"end\":46896,\"start\":46890},{\"end\":46910,\"start\":46905},{\"end\":46926,\"start\":46919},{\"end\":46942,\"start\":46934},{\"end\":46956,\"start\":46949},{\"end\":46967,\"start\":46964},{\"end\":46980,\"start\":46973},{\"end\":47000,\"start\":46992},{\"end\":47013,\"start\":47007},{\"end\":47025,\"start\":47021},{\"end\":47039,\"start\":47032},{\"end\":47053,\"start\":47048},{\"end\":47067,\"start\":47062},{\"end\":47773,\"start\":47772},{\"end\":47784,\"start\":47783},{\"end\":47795,\"start\":47794},{\"end\":47805,\"start\":47804},{\"end\":47826,\"start\":47818},{\"end\":48064,\"start\":48058},{\"end\":48078,\"start\":48074},{\"end\":48089,\"start\":48084},{\"end\":48104,\"start\":48100},{\"end\":48476,\"start\":48468},{\"end\":48478,\"start\":48477},{\"end\":48491,\"start\":48486},{\"end\":48502,\"start\":48498},{\"end\":48514,\"start\":48510},{\"end\":48516,\"start\":48515},{\"end\":49249,\"start\":49243},{\"end\":49267,\"start\":49259},{\"end\":49285,\"start\":49280},{\"end\":49300,\"start\":49293},{\"end\":49321,\"start\":49312},{\"end\":49339,\"start\":49330},{\"end\":49355,\"start\":49348},{\"end\":49367,\"start\":49363},{\"end\":49377,\"start\":49373},{\"end\":49398,\"start\":49391},{\"end\":50111,\"start\":50105},{\"end\":50122,\"start\":50117},{\"end\":50134,\"start\":50128},{\"end\":50443,\"start\":50436},{\"end\":50459,\"start\":50455},{\"end\":50846,\"start\":50840},{\"end\":50856,\"start\":50852},{\"end\":50868,\"start\":50864},{\"end\":51106,\"start\":51102},{\"end\":51115,\"start\":51112},{\"end\":51129,\"start\":51122},{\"end\":51740,\"start\":51735},{\"end\":51754,\"start\":51747},{\"end\":51769,\"start\":51763},{\"end\":51778,\"start\":51777},{\"end\":52020,\"start\":52017},{\"end\":52034,\"start\":52031},{\"end\":52043,\"start\":52041},{\"end\":52055,\"start\":52048},{\"end\":52069,\"start\":52064},{\"end\":52376,\"start\":52369},{\"end\":52387,\"start\":52381},{\"end\":52398,\"start\":52394},{\"end\":52412,\"start\":52406},{\"end\":52422,\"start\":52418},{\"end\":52432,\"start\":52428},{\"end\":53073,\"start\":53068},{\"end\":53083,\"start\":53079},{\"end\":53096,\"start\":53090},{\"end\":53113,\"start\":53108},{\"end\":53854,\"start\":53848},{\"end\":53871,\"start\":53866},{\"end\":54630,\"start\":54622},{\"end\":54644,\"start\":54639},{\"end\":54907,\"start\":54898},{\"end\":54919,\"start\":54915},{\"end\":54932,\"start\":54925},{\"end\":54943,\"start\":54939},{\"end\":54964,\"start\":54955},{\"end\":55412,\"start\":55407},{\"end\":55424,\"start\":55420},{\"end\":55697,\"start\":55691},{\"end\":55709,\"start\":55705},{\"end\":56372,\"start\":56366},{\"end\":56388,\"start\":56381},{\"end\":56404,\"start\":56398},{\"end\":57031,\"start\":57022},{\"end\":57043,\"start\":57038},{\"end\":57056,\"start\":57044},{\"end\":57069,\"start\":57064},{\"end\":57081,\"start\":57077},{\"end\":57097,\"start\":57090},{\"end\":57112,\"start\":57109},{\"end\":57126,\"start\":57121},{\"end\":57140,\"start\":57135},{\"end\":57749,\"start\":57745},{\"end\":57761,\"start\":57755},{\"end\":57776,\"start\":57770},{\"end\":57792,\"start\":57786},{\"end\":57801,\"start\":57798},{\"end\":57815,\"start\":57809},{\"end\":57825,\"start\":57820},{\"end\":57843,\"start\":57836},{\"end\":58223,\"start\":58216},{\"end\":58239,\"start\":58234},{\"end\":58252,\"start\":58248},{\"end\":58267,\"start\":58259},{\"end\":58910,\"start\":58904},{\"end\":58926,\"start\":58919},{\"end\":58941,\"start\":58934},{\"end\":59184,\"start\":59177},{\"end\":59196,\"start\":59191},{\"end\":59213,\"start\":59206},{\"end\":59230,\"start\":59222},{\"end\":59588,\"start\":59584},{\"end\":59603,\"start\":59598},{\"end\":60176,\"start\":60175},{\"end\":60192,\"start\":60185},{\"end\":60208,\"start\":60201},{\"end\":60226,\"start\":60220},{\"end\":60243,\"start\":60235},{\"end\":60631,\"start\":60626},{\"end\":60645,\"start\":60638},{\"end\":60657,\"start\":60652},{\"end\":61415,\"start\":61411},{\"end\":61432,\"start\":61427},{\"end\":61448,\"start\":61442},{\"end\":61648,\"start\":61647},{\"end\":61650,\"start\":61649},{\"end\":61886,\"start\":61885},{\"end\":61888,\"start\":61887},{\"end\":61898,\"start\":61897},{\"end\":61900,\"start\":61899},{\"end\":62232,\"start\":62231},{\"end\":62247,\"start\":62242},{\"end\":62264,\"start\":62256},{\"end\":62283,\"start\":62277},{\"end\":62607,\"start\":62601},{\"end\":62621,\"start\":62617},{\"end\":62636,\"start\":62630},{\"end\":62653,\"start\":62645},{\"end\":62668,\"start\":62663},{\"end\":62670,\"start\":62669},{\"end\":62685,\"start\":62678},{\"end\":62698,\"start\":62693},{\"end\":62712,\"start\":62706},{\"end\":62724,\"start\":62721},{\"end\":62743,\"start\":62739},{\"end\":62756,\"start\":62752},{\"end\":62770,\"start\":62766},{\"end\":62785,\"start\":62780},{\"end\":63174,\"start\":63168},{\"end\":63188,\"start\":63184},{\"end\":63202,\"start\":63198},{\"end\":63216,\"start\":63211},{\"end\":63233,\"start\":63228},{\"end\":63246,\"start\":63241},{\"end\":63248,\"start\":63247},{\"end\":63262,\"start\":63256},{\"end\":63276,\"start\":63271},{\"end\":63714,\"start\":63707},{\"end\":63725,\"start\":63721},{\"end\":64324,\"start\":64320},{\"end\":64339,\"start\":64331},{\"end\":64351,\"start\":64344},{\"end\":64364,\"start\":64357},{\"end\":64377,\"start\":64370},{\"end\":64387,\"start\":64383},{\"end\":64918,\"start\":64909},{\"end\":64927,\"start\":64924},{\"end\":64936,\"start\":64932},{\"end\":64945,\"start\":64943},{\"end\":65564,\"start\":65563},{\"end\":65825,\"start\":65819},{\"end\":65840,\"start\":65832},{\"end\":65854,\"start\":65848},{\"end\":65867,\"start\":65861},{\"end\":65885,\"start\":65878},{\"end\":65903,\"start\":65896},{\"end\":65916,\"start\":65909},{\"end\":65928,\"start\":65925},{\"end\":65940,\"start\":65936},{\"end\":65953,\"start\":65947},{\"end\":65968,\"start\":65965},{\"end\":65981,\"start\":65978},{\"end\":65997,\"start\":65992},{\"end\":66024,\"start\":66018},{\"end\":66035,\"start\":66029},{\"end\":66051,\"start\":66045},{\"end\":66062,\"start\":66057},{\"end\":66065,\"start\":66063},{\"end\":66077,\"start\":66070},{\"end\":66091,\"start\":66084},{\"end\":67061,\"start\":67055},{\"end\":67075,\"start\":67069},{\"end\":67090,\"start\":67085},{\"end\":67101,\"start\":67095},{\"end\":67103,\"start\":67102},{\"end\":67120,\"start\":67116}]", "bib_author_last_name": "[{\"end\":45210,\"start\":45202},{\"end\":45227,\"start\":45221},{\"end\":45238,\"start\":45236},{\"end\":45253,\"start\":45248},{\"end\":45264,\"start\":45260},{\"end\":45279,\"start\":45273},{\"end\":45296,\"start\":45287},{\"end\":45311,\"start\":45305},{\"end\":45624,\"start\":45616},{\"end\":45641,\"start\":45635},{\"end\":45652,\"start\":45650},{\"end\":45667,\"start\":45662},{\"end\":45678,\"start\":45674},{\"end\":45693,\"start\":45687},{\"end\":45710,\"start\":45701},{\"end\":45725,\"start\":45719},{\"end\":45995,\"start\":45987},{\"end\":46010,\"start\":46007},{\"end\":46025,\"start\":46019},{\"end\":46379,\"start\":46373},{\"end\":46393,\"start\":46388},{\"end\":46418,\"start\":46404},{\"end\":46441,\"start\":46431},{\"end\":46849,\"start\":46844},{\"end\":46867,\"start\":46857},{\"end\":46888,\"start\":46879},{\"end\":46903,\"start\":46897},{\"end\":46917,\"start\":46911},{\"end\":46932,\"start\":46927},{\"end\":46947,\"start\":46943},{\"end\":46962,\"start\":46957},{\"end\":46971,\"start\":46968},{\"end\":46990,\"start\":46981},{\"end\":47005,\"start\":47001},{\"end\":47019,\"start\":47014},{\"end\":47030,\"start\":47026},{\"end\":47046,\"start\":47040},{\"end\":47060,\"start\":47054},{\"end\":47074,\"start\":47068},{\"end\":47781,\"start\":47774},{\"end\":47792,\"start\":47785},{\"end\":47802,\"start\":47796},{\"end\":47816,\"start\":47806},{\"end\":47835,\"start\":47827},{\"end\":48072,\"start\":48065},{\"end\":48082,\"start\":48079},{\"end\":48098,\"start\":48090},{\"end\":48110,\"start\":48105},{\"end\":48484,\"start\":48479},{\"end\":48496,\"start\":48492},{\"end\":48508,\"start\":48503},{\"end\":48522,\"start\":48517},{\"end\":49257,\"start\":49250},{\"end\":49278,\"start\":49268},{\"end\":49291,\"start\":49286},{\"end\":49310,\"start\":49301},{\"end\":49328,\"start\":49322},{\"end\":49346,\"start\":49340},{\"end\":49361,\"start\":49356},{\"end\":49371,\"start\":49368},{\"end\":49389,\"start\":49378},{\"end\":49407,\"start\":49399},{\"end\":50115,\"start\":50112},{\"end\":50126,\"start\":50123},{\"end\":50139,\"start\":50135},{\"end\":50453,\"start\":50444},{\"end\":50465,\"start\":50460},{\"end\":50850,\"start\":50847},{\"end\":50862,\"start\":50857},{\"end\":50876,\"start\":50869},{\"end\":51110,\"start\":51107},{\"end\":51120,\"start\":51116},{\"end\":51134,\"start\":51130},{\"end\":51745,\"start\":51741},{\"end\":51761,\"start\":51755},{\"end\":51775,\"start\":51770},{\"end\":51796,\"start\":51779},{\"end\":52029,\"start\":52021},{\"end\":52039,\"start\":52035},{\"end\":52046,\"start\":52044},{\"end\":52062,\"start\":52056},{\"end\":52074,\"start\":52070},{\"end\":52379,\"start\":52377},{\"end\":52392,\"start\":52388},{\"end\":52404,\"start\":52399},{\"end\":52416,\"start\":52413},{\"end\":52426,\"start\":52423},{\"end\":52437,\"start\":52433},{\"end\":53077,\"start\":53074},{\"end\":53088,\"start\":53084},{\"end\":53106,\"start\":53097},{\"end\":53120,\"start\":53114},{\"end\":53864,\"start\":53855},{\"end\":53880,\"start\":53872},{\"end\":54637,\"start\":54631},{\"end\":54647,\"start\":54645},{\"end\":54913,\"start\":54908},{\"end\":54923,\"start\":54920},{\"end\":54937,\"start\":54933},{\"end\":54953,\"start\":54944},{\"end\":54969,\"start\":54965},{\"end\":55418,\"start\":55413},{\"end\":55435,\"start\":55425},{\"end\":55703,\"start\":55698},{\"end\":55719,\"start\":55710},{\"end\":56379,\"start\":56373},{\"end\":56396,\"start\":56389},{\"end\":56409,\"start\":56405},{\"end\":57036,\"start\":57032},{\"end\":57062,\"start\":57057},{\"end\":57075,\"start\":57070},{\"end\":57088,\"start\":57082},{\"end\":57107,\"start\":57098},{\"end\":57119,\"start\":57113},{\"end\":57133,\"start\":57127},{\"end\":57152,\"start\":57141},{\"end\":57753,\"start\":57750},{\"end\":57768,\"start\":57762},{\"end\":57784,\"start\":57777},{\"end\":57796,\"start\":57793},{\"end\":57807,\"start\":57802},{\"end\":57818,\"start\":57816},{\"end\":57834,\"start\":57826},{\"end\":57848,\"start\":57844},{\"end\":58232,\"start\":58224},{\"end\":58246,\"start\":58240},{\"end\":58257,\"start\":58253},{\"end\":58271,\"start\":58268},{\"end\":58917,\"start\":58911},{\"end\":58932,\"start\":58927},{\"end\":58948,\"start\":58942},{\"end\":59189,\"start\":59185},{\"end\":59204,\"start\":59197},{\"end\":59220,\"start\":59214},{\"end\":59235,\"start\":59231},{\"end\":59244,\"start\":59237},{\"end\":59596,\"start\":59589},{\"end\":59612,\"start\":59604},{\"end\":60183,\"start\":60177},{\"end\":60199,\"start\":60193},{\"end\":60218,\"start\":60209},{\"end\":60233,\"start\":60227},{\"end\":60248,\"start\":60244},{\"end\":60254,\"start\":60250},{\"end\":60636,\"start\":60632},{\"end\":60650,\"start\":60646},{\"end\":60664,\"start\":60658},{\"end\":61425,\"start\":61416},{\"end\":61440,\"start\":61433},{\"end\":61451,\"start\":61449},{\"end\":61659,\"start\":61651},{\"end\":61671,\"start\":61661},{\"end\":61895,\"start\":61889},{\"end\":61906,\"start\":61901},{\"end\":62240,\"start\":62233},{\"end\":62254,\"start\":62248},{\"end\":62275,\"start\":62265},{\"end\":62289,\"start\":62284},{\"end\":62298,\"start\":62291},{\"end\":62615,\"start\":62608},{\"end\":62628,\"start\":62622},{\"end\":62643,\"start\":62637},{\"end\":62661,\"start\":62654},{\"end\":62676,\"start\":62671},{\"end\":62691,\"start\":62686},{\"end\":62704,\"start\":62699},{\"end\":62719,\"start\":62713},{\"end\":62737,\"start\":62725},{\"end\":62750,\"start\":62744},{\"end\":62764,\"start\":62757},{\"end\":62778,\"start\":62771},{\"end\":62795,\"start\":62786},{\"end\":63182,\"start\":63175},{\"end\":63196,\"start\":63189},{\"end\":63209,\"start\":63203},{\"end\":63226,\"start\":63217},{\"end\":63239,\"start\":63234},{\"end\":63254,\"start\":63249},{\"end\":63269,\"start\":63263},{\"end\":63287,\"start\":63277},{\"end\":63719,\"start\":63715},{\"end\":63734,\"start\":63726},{\"end\":64329,\"start\":64325},{\"end\":64342,\"start\":64340},{\"end\":64355,\"start\":64352},{\"end\":64368,\"start\":64365},{\"end\":64381,\"start\":64378},{\"end\":64391,\"start\":64388},{\"end\":64922,\"start\":64919},{\"end\":64930,\"start\":64928},{\"end\":64941,\"start\":64937},{\"end\":64949,\"start\":64946},{\"end\":65571,\"start\":65565},{\"end\":65581,\"start\":65573},{\"end\":65830,\"start\":65826},{\"end\":65846,\"start\":65841},{\"end\":65859,\"start\":65855},{\"end\":65876,\"start\":65868},{\"end\":65894,\"start\":65886},{\"end\":65907,\"start\":65904},{\"end\":65923,\"start\":65917},{\"end\":65934,\"start\":65929},{\"end\":65945,\"start\":65941},{\"end\":65963,\"start\":65954},{\"end\":65976,\"start\":65969},{\"end\":65990,\"start\":65982},{\"end\":66016,\"start\":65998},{\"end\":66027,\"start\":66025},{\"end\":66043,\"start\":66036},{\"end\":66055,\"start\":66052},{\"end\":66068,\"start\":66066},{\"end\":66082,\"start\":66078},{\"end\":66098,\"start\":66092},{\"end\":66105,\"start\":66100},{\"end\":67067,\"start\":67062},{\"end\":67083,\"start\":67076},{\"end\":67093,\"start\":67091},{\"end\":67114,\"start\":67104},{\"end\":67126,\"start\":67121}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b0\"},\"end\":45555,\"start\":45143},{\"attributes\":{\"id\":\"b1\"},\"end\":45906,\"start\":45557},{\"attributes\":{\"doi\":\"arxiv:1409\",\"id\":\"b2\"},\"end\":46293,\"start\":45908},{\"attributes\":{\"doi\":\"10.1145/3442188.3445922\",\"id\":\"b3\",\"matched_paper_id\":232040593},\"end\":46771,\"start\":46295},{\"attributes\":{\"doi\":\"10.18653/v1/W17-4717\",\"id\":\"b4\",\"matched_paper_id\":28232901},\"end\":47723,\"start\":46773},{\"attributes\":{\"id\":\"b5\"},\"end\":47980,\"start\":47725},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195791459},\"end\":48364,\"start\":47982},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":512833},\"end\":49180,\"start\":48366},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.747\",\"id\":\"b8\",\"matched_paper_id\":207880568},\"end\":50002,\"start\":49182},{\"attributes\":{\"doi\":\"arXiv:1804.10974\",\"id\":\"b9\"},\"end\":50350,\"start\":50004},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5923323},\"end\":50800,\"start\":50352},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1082\",\"id\":\"b11\"},\"end\":51000,\"start\":50802},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.124\",\"id\":\"b12\"},\"end\":51637,\"start\":51002},{\"attributes\":{\"id\":\"b13\"},\"end\":51969,\"start\":51639},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":127986954},\"end\":52305,\"start\":51971},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/570\",\"id\":\"b15\",\"matched_paper_id\":13559921},\"end\":52996,\"start\":52307},{\"attributes\":{\"doi\":\"10.18653/v1/2021.eacl-main.164\",\"id\":\"b16\",\"matched_paper_id\":232014615},\"end\":53762,\"start\":52998},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.133\",\"id\":\"b17\",\"matched_paper_id\":235097487},\"end\":54576,\"start\":53764},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6628106},\"end\":54836,\"start\":54578},{\"attributes\":{\"id\":\"b19\"},\"end\":55380,\"start\":54838},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207779694},\"end\":55606,\"start\":55382},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":29180066},\"end\":56271,\"start\":55608},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1269\",\"id\":\"b22\",\"matched_paper_id\":196185246},\"end\":56966,\"start\":56273},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":6875312},\"end\":57684,\"start\":56968},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":91184134},\"end\":58150,\"start\":57686},{\"attributes\":{\"doi\":\"10.3115/1073083.1073135\",\"id\":\"b25\",\"matched_paper_id\":11080756},\"end\":58847,\"start\":58152},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":21850704},\"end\":59175,\"start\":58849},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b27\"},\"end\":59500,\"start\":59177},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":216036089},\"end\":60119,\"start\":59502},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206594923},\"end\":60558,\"start\":60121},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.252\",\"id\":\"b30\",\"matched_paper_id\":235097390},\"end\":61357,\"start\":60560},{\"attributes\":{\"id\":\"b31\"},\"end\":61645,\"start\":61359},{\"attributes\":{\"id\":\"b32\"},\"end\":61883,\"start\":61647},{\"attributes\":{\"id\":\"b33\"},\"end\":62149,\"start\":61885},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1211821},\"end\":62597,\"start\":62151},{\"attributes\":{\"id\":\"b35\"},\"end\":63139,\"start\":62599},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13756489},\"end\":63625,\"start\":63141},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.326\",\"id\":\"b37\",\"matched_paper_id\":218538004},\"end\":64256,\"start\":63627},{\"attributes\":{\"doi\":\"10.18653/v1/2021.findings-acl.422\",\"id\":\"b38\",\"matched_paper_id\":235358300},\"end\":64844,\"start\":64258},{\"attributes\":{\"doi\":\"10.18653/v1/K19-1027\",\"id\":\"b39\",\"matched_paper_id\":208049931},\"end\":65471,\"start\":64846},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":2332513},\"end\":65757,\"start\":65473},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-demos.6\",\"id\":\"b41\",\"matched_paper_id\":208117506},\"end\":67004,\"start\":65759},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":127986044},\"end\":67373,\"start\":67006}]", "bib_title": "[{\"end\":46363,\"start\":46295},{\"end\":46835,\"start\":46773},{\"end\":48056,\"start\":47982},{\"end\":48466,\"start\":48366},{\"end\":49241,\"start\":49182},{\"end\":50434,\"start\":50352},{\"end\":51100,\"start\":51002},{\"end\":52015,\"start\":51971},{\"end\":52367,\"start\":52307},{\"end\":53066,\"start\":52998},{\"end\":53846,\"start\":53764},{\"end\":54620,\"start\":54578},{\"end\":54896,\"start\":54838},{\"end\":55405,\"start\":55382},{\"end\":55689,\"start\":55608},{\"end\":56364,\"start\":56273},{\"end\":57020,\"start\":56968},{\"end\":57743,\"start\":57686},{\"end\":58214,\"start\":58152},{\"end\":58902,\"start\":58849},{\"end\":59582,\"start\":59502},{\"end\":60173,\"start\":60121},{\"end\":60624,\"start\":60560},{\"end\":62229,\"start\":62151},{\"end\":63166,\"start\":63141},{\"end\":63705,\"start\":63627},{\"end\":64318,\"start\":64258},{\"end\":64907,\"start\":64846},{\"end\":65561,\"start\":65473},{\"end\":65817,\"start\":65759},{\"end\":67053,\"start\":67006}]", "bib_author": "[{\"end\":45212,\"start\":45194},{\"end\":45229,\"start\":45212},{\"end\":45240,\"start\":45229},{\"end\":45255,\"start\":45240},{\"end\":45266,\"start\":45255},{\"end\":45281,\"start\":45266},{\"end\":45298,\"start\":45281},{\"end\":45313,\"start\":45298},{\"end\":45626,\"start\":45608},{\"end\":45643,\"start\":45626},{\"end\":45654,\"start\":45643},{\"end\":45669,\"start\":45654},{\"end\":45680,\"start\":45669},{\"end\":45695,\"start\":45680},{\"end\":45712,\"start\":45695},{\"end\":45727,\"start\":45712},{\"end\":45997,\"start\":45979},{\"end\":46012,\"start\":45997},{\"end\":46027,\"start\":46012},{\"end\":46381,\"start\":46365},{\"end\":46395,\"start\":46381},{\"end\":46420,\"start\":46395},{\"end\":46443,\"start\":46420},{\"end\":46851,\"start\":46837},{\"end\":46869,\"start\":46851},{\"end\":46890,\"start\":46869},{\"end\":46905,\"start\":46890},{\"end\":46919,\"start\":46905},{\"end\":46934,\"start\":46919},{\"end\":46949,\"start\":46934},{\"end\":46964,\"start\":46949},{\"end\":46973,\"start\":46964},{\"end\":46992,\"start\":46973},{\"end\":47007,\"start\":46992},{\"end\":47021,\"start\":47007},{\"end\":47032,\"start\":47021},{\"end\":47048,\"start\":47032},{\"end\":47062,\"start\":47048},{\"end\":47076,\"start\":47062},{\"end\":47783,\"start\":47772},{\"end\":47794,\"start\":47783},{\"end\":47804,\"start\":47794},{\"end\":47818,\"start\":47804},{\"end\":47837,\"start\":47818},{\"end\":48074,\"start\":48058},{\"end\":48084,\"start\":48074},{\"end\":48100,\"start\":48084},{\"end\":48112,\"start\":48100},{\"end\":48486,\"start\":48468},{\"end\":48498,\"start\":48486},{\"end\":48510,\"start\":48498},{\"end\":48524,\"start\":48510},{\"end\":49259,\"start\":49243},{\"end\":49280,\"start\":49259},{\"end\":49293,\"start\":49280},{\"end\":49312,\"start\":49293},{\"end\":49330,\"start\":49312},{\"end\":49348,\"start\":49330},{\"end\":49363,\"start\":49348},{\"end\":49373,\"start\":49363},{\"end\":49391,\"start\":49373},{\"end\":49409,\"start\":49391},{\"end\":50117,\"start\":50105},{\"end\":50128,\"start\":50117},{\"end\":50141,\"start\":50128},{\"end\":50455,\"start\":50436},{\"end\":50467,\"start\":50455},{\"end\":50852,\"start\":50840},{\"end\":50864,\"start\":50852},{\"end\":50878,\"start\":50864},{\"end\":51112,\"start\":51102},{\"end\":51122,\"start\":51112},{\"end\":51136,\"start\":51122},{\"end\":51747,\"start\":51735},{\"end\":51763,\"start\":51747},{\"end\":51777,\"start\":51763},{\"end\":51798,\"start\":51777},{\"end\":52031,\"start\":52017},{\"end\":52041,\"start\":52031},{\"end\":52048,\"start\":52041},{\"end\":52064,\"start\":52048},{\"end\":52076,\"start\":52064},{\"end\":52381,\"start\":52369},{\"end\":52394,\"start\":52381},{\"end\":52406,\"start\":52394},{\"end\":52418,\"start\":52406},{\"end\":52428,\"start\":52418},{\"end\":52439,\"start\":52428},{\"end\":53079,\"start\":53068},{\"end\":53090,\"start\":53079},{\"end\":53108,\"start\":53090},{\"end\":53122,\"start\":53108},{\"end\":53866,\"start\":53848},{\"end\":53882,\"start\":53866},{\"end\":54639,\"start\":54622},{\"end\":54649,\"start\":54639},{\"end\":54915,\"start\":54898},{\"end\":54925,\"start\":54915},{\"end\":54939,\"start\":54925},{\"end\":54955,\"start\":54939},{\"end\":54971,\"start\":54955},{\"end\":55420,\"start\":55407},{\"end\":55437,\"start\":55420},{\"end\":55705,\"start\":55691},{\"end\":55721,\"start\":55705},{\"end\":56381,\"start\":56366},{\"end\":56398,\"start\":56381},{\"end\":56411,\"start\":56398},{\"end\":57038,\"start\":57022},{\"end\":57064,\"start\":57038},{\"end\":57077,\"start\":57064},{\"end\":57090,\"start\":57077},{\"end\":57109,\"start\":57090},{\"end\":57121,\"start\":57109},{\"end\":57135,\"start\":57121},{\"end\":57154,\"start\":57135},{\"end\":57755,\"start\":57745},{\"end\":57770,\"start\":57755},{\"end\":57786,\"start\":57770},{\"end\":57798,\"start\":57786},{\"end\":57809,\"start\":57798},{\"end\":57820,\"start\":57809},{\"end\":57836,\"start\":57820},{\"end\":57850,\"start\":57836},{\"end\":58234,\"start\":58216},{\"end\":58248,\"start\":58234},{\"end\":58259,\"start\":58248},{\"end\":58273,\"start\":58259},{\"end\":58919,\"start\":58904},{\"end\":58934,\"start\":58919},{\"end\":58950,\"start\":58934},{\"end\":59191,\"start\":59177},{\"end\":59206,\"start\":59191},{\"end\":59222,\"start\":59206},{\"end\":59237,\"start\":59222},{\"end\":59246,\"start\":59237},{\"end\":59598,\"start\":59584},{\"end\":59614,\"start\":59598},{\"end\":60185,\"start\":60175},{\"end\":60201,\"start\":60185},{\"end\":60220,\"start\":60201},{\"end\":60235,\"start\":60220},{\"end\":60250,\"start\":60235},{\"end\":60256,\"start\":60250},{\"end\":60638,\"start\":60626},{\"end\":60652,\"start\":60638},{\"end\":60666,\"start\":60652},{\"end\":61427,\"start\":61411},{\"end\":61442,\"start\":61427},{\"end\":61453,\"start\":61442},{\"end\":61661,\"start\":61647},{\"end\":61673,\"start\":61661},{\"end\":61897,\"start\":61885},{\"end\":61908,\"start\":61897},{\"end\":62242,\"start\":62231},{\"end\":62256,\"start\":62242},{\"end\":62277,\"start\":62256},{\"end\":62291,\"start\":62277},{\"end\":62300,\"start\":62291},{\"end\":62617,\"start\":62601},{\"end\":62630,\"start\":62617},{\"end\":62645,\"start\":62630},{\"end\":62663,\"start\":62645},{\"end\":62678,\"start\":62663},{\"end\":62693,\"start\":62678},{\"end\":62706,\"start\":62693},{\"end\":62721,\"start\":62706},{\"end\":62739,\"start\":62721},{\"end\":62752,\"start\":62739},{\"end\":62766,\"start\":62752},{\"end\":62780,\"start\":62766},{\"end\":62797,\"start\":62780},{\"end\":63184,\"start\":63168},{\"end\":63198,\"start\":63184},{\"end\":63211,\"start\":63198},{\"end\":63228,\"start\":63211},{\"end\":63241,\"start\":63228},{\"end\":63256,\"start\":63241},{\"end\":63271,\"start\":63256},{\"end\":63289,\"start\":63271},{\"end\":63721,\"start\":63707},{\"end\":63736,\"start\":63721},{\"end\":64331,\"start\":64320},{\"end\":64344,\"start\":64331},{\"end\":64357,\"start\":64344},{\"end\":64370,\"start\":64357},{\"end\":64383,\"start\":64370},{\"end\":64393,\"start\":64383},{\"end\":64924,\"start\":64909},{\"end\":64932,\"start\":64924},{\"end\":64943,\"start\":64932},{\"end\":64951,\"start\":64943},{\"end\":65573,\"start\":65563},{\"end\":65583,\"start\":65573},{\"end\":65832,\"start\":65819},{\"end\":65848,\"start\":65832},{\"end\":65861,\"start\":65848},{\"end\":65878,\"start\":65861},{\"end\":65896,\"start\":65878},{\"end\":65909,\"start\":65896},{\"end\":65925,\"start\":65909},{\"end\":65936,\"start\":65925},{\"end\":65947,\"start\":65936},{\"end\":65965,\"start\":65947},{\"end\":65978,\"start\":65965},{\"end\":65992,\"start\":65978},{\"end\":66018,\"start\":65992},{\"end\":66029,\"start\":66018},{\"end\":66045,\"start\":66029},{\"end\":66057,\"start\":66045},{\"end\":66070,\"start\":66057},{\"end\":66084,\"start\":66070},{\"end\":66100,\"start\":66084},{\"end\":66107,\"start\":66100},{\"end\":67069,\"start\":67055},{\"end\":67085,\"start\":67069},{\"end\":67095,\"start\":67085},{\"end\":67116,\"start\":67095},{\"end\":67128,\"start\":67116}]", "bib_venue": "[{\"end\":45192,\"start\":45143},{\"end\":45606,\"start\":45557},{\"end\":45977,\"start\":45908},{\"end\":46475,\"start\":46466},{\"end\":47155,\"start\":47096},{\"end\":47770,\"start\":47725},{\"end\":48164,\"start\":48112},{\"end\":48640,\"start\":48524},{\"end\":49525,\"start\":49438},{\"end\":50103,\"start\":50004},{\"end\":50539,\"start\":50467},{\"end\":50838,\"start\":50802},{\"end\":51252,\"start\":51165},{\"end\":51733,\"start\":51639},{\"end\":52128,\"start\":52076},{\"end\":52563,\"start\":52462},{\"end\":53272,\"start\":53152},{\"end\":54055,\"start\":53913},{\"end\":54701,\"start\":54649},{\"end\":55017,\"start\":54971},{\"end\":55483,\"start\":55437},{\"end\":55817,\"start\":55721},{\"end\":56518,\"start\":56431},{\"end\":57226,\"start\":57158},{\"end\":57895,\"start\":57850},{\"end\":58383,\"start\":58296},{\"end\":59002,\"start\":58950},{\"end\":59316,\"start\":59262},{\"end\":59743,\"start\":59614},{\"end\":60321,\"start\":60256},{\"end\":60839,\"start\":60697},{\"end\":61409,\"start\":61359},{\"end\":61722,\"start\":61673},{\"end\":61947,\"start\":61908},{\"end\":62349,\"start\":62300},{\"end\":63338,\"start\":63289},{\"end\":63852,\"start\":63765},{\"end\":64500,\"start\":64426},{\"end\":65056,\"start\":64971},{\"end\":65599,\"start\":65583},{\"end\":66246,\"start\":66137},{\"end\":67180,\"start\":67128},{\"end\":46494,\"start\":46477},{\"end\":47220,\"start\":47157},{\"end\":48764,\"start\":48642},{\"end\":49599,\"start\":49527},{\"end\":50598,\"start\":50541},{\"end\":51326,\"start\":51254},{\"end\":52651,\"start\":52565},{\"end\":53379,\"start\":53274},{\"end\":54184,\"start\":54057},{\"end\":55067,\"start\":55019},{\"end\":55908,\"start\":55819},{\"end\":56607,\"start\":56520},{\"end\":57304,\"start\":57228},{\"end\":57927,\"start\":57897},{\"end\":58488,\"start\":58385},{\"end\":59859,\"start\":59745},{\"end\":60968,\"start\":60841},{\"end\":61973,\"start\":61949},{\"end\":63926,\"start\":63854},{\"end\":65144,\"start\":65058},{\"end\":66376,\"start\":66248}]"}}}, "year": 2023, "month": 12, "day": 17}