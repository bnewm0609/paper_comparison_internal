{"id": 237431405, "updated": "2023-11-04 17:33:05.062", "metadata": {"title": "Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization", "authors": "[{\"first\":\"Alexandre\",\"last\":\"Rame\",\"middle\":[]},{\"first\":\"Corentin\",\"last\":\"Dancette\",\"middle\":[]},{\"first\":\"Matthieu\",\"last\":\"Cord\",\"middle\":[]}]", "venue": "ICML 2022", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Learning robust models that generalize well under changes in the data distribution is critical for real-world applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains - while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https://github.com/alexrame/fishr.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.02934", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/RameDC22", "doi": null}}, "content": {"source": {"pdf_hash": "6bf04318d6e57463f7823b9770c5a6c19a7b47e9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.02934v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3a404ebd04cd3c11d3e360b7732d5480c053be71", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6bf04318d6e57463f7823b9770c5a6c19a7b47e9.txt", "contents": "\nFishr: Invariant Gradient Variances for Out-of-Distribution Generalization\n\n\nAlexandre Ram\u00e9 \nCorentin Dancette \nMatthieu Cord \nFishr: Invariant Gradient Variances for Out-of-Distribution Generalization\n\nLearning robust models that generalize well under changes in the data distribution is critical for realworld applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains -while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization -named Fishr -that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https: //github.com/alexrame/fishr.\n\nIntroduction\n\nThe success of deep neural networks in supervised learning (Krizhevsky et al., 2012) relies on the crucial assumption that the train and test data distributions are identical. In particular, the tendency of networks to rely on simple features (Valle-Perez et al., 2019;Geirhos et al., 2020) is generally a desirable behavior reflecting Occam's razor. However, in case of distribution shift, this simplicity bias deteriorates performance when more complex features are needed (Tenenbaum, 2018;Shah et al., 2020). For example, in the  Specifically, Fishr matches the domain-level gradient variances of the distributions across the two training domains:\nA ({g i A } n A i=1 in orange) and B ({g i B } n B i=1\nin blue). We will show how this regularization during the learning of \u03b8 improves the out-of-distribution generalization properties by aligning the domain-level loss landscapes at convergence. recent fight against Covid-19, most of the deep learning methods developed to detect coronavirus from chest scans were shown useless for clinical use (DeGrave et al., 2021;Roberts et al., 2021): indeed, networks exploited simple bias in the training datasets such as patients' age or body position rather than 'truly' analyzing medical pathologies.\n\nTo better generalize under distribution shifts, most works (Blanchard et al., 2011;Muandet et al., 2013) assume that the training data is divided into different training domains in which there is a constant underlying causal mechanism (Peters et al., 2016). To remove the domain-dependent explanations, different invariance criteria across those training domains have been proposed. Ganin et al. (2016); Sun et al. (2016); Sun & Saenko (2016) enforce similar feature distributions, others (Arjovsky et al., 2019;Krueger et al., 2021) force the classifier to be simultaneously optimal across all domains. Yet, despite the popularity of this research topic, none of these methods perform significantly better than the classical Empirical Risk Minimization (ERM) when applied with controlled model selection and restricted hyperparameter search (Gulrajani & Lopez-Paz, 2021;Ye et al., 2021). To foster the emergence of a shared mechanism with consistent generalization properties, our intuition is that learning should progress consistently and similarly across domains. Besides, the learning procedure of deep neural networks is dictated by the distribution of the gradients with respect to the network weights (Yin et al., 2018;Sankararaman et al., 2020) -usually backpropagated in the network during gradient descent. Additionally, individual gradients are expressive representations of the input (Fort et al., 2019;Charpiat et al., 2019). Thus, we seek distributional invariance across domains in the gradient space: domain-level gradients should be similar, not only in average direction, but most importantly in statistics such as variance and disagreements.\n\nIn this paper, we propose the Fishr regularization for outof-distribution generalization in classification for computer vision -summarized in Fig. 1. We match the domainlevel gradient variances, i.e., the second moment of the gradient distributions. In contrast, previous gradient-based works such as Fish (Shi et al., 2021) only match the domainlevel gradients means, i.e., the first moment.\n\nOur strategy is also motivated by the close relations between the gradient variance, the Fisher Information (Fisher, 1922) and the Hessian. This explains the name of our work, Fishr, using gradients as in Fish and related to the Fisher Matrix. Notably, we will study how Fishr forces the model to have similar domain-level Hessians and promotes consistent explanations -by generalizing the inconsistency formalism introduced in Parascandolo et al. (2021).\n\nTo reduce the computational cost, we justify an approximation that tackles the gradients only in the classifier, easily implemented with BackPACK (Dangel et al., 2020).\n\nWe summarize our contributions as follows:\n\n\u2022 We introduce Fishr, a scalable regularization that brings closer the domain-level gradient variances.\n\n\u2022 We theoretically justify that Fishr matches domainlevel risks and Hessians, and consequently, reduces inconsistencies across domains.\n\nEmpirically, we first validate that Fishr tackles distribution shifts on the synthetic Colored MNIST (Arjovsky et al., 2019). Then, we show that Fishr performs best on the Do-mainBed benchmark (Gulrajani & Lopez-Paz, 2021) when compared with state-of-the-art counterparts. Critically, Fishr is the only method to perform systematically better than ERM on all real datasets -PACS, VLCS, OfficeHome, TerraIncognita and DomainNet.\n\n\nContext and Related Work\n\nWe first describe our task and provide the notations used along our paper. Then we remind some important related works to understand how our Fishr stands in a rich literature.\n\nProblem definition and notations. We study out-ofdistribution (OOD) generalization for classification. Our model is a deep neural network (DNN) f \u03b8 (parametrized by \u03b8) made of a deep features extractor \u03a6 \u03c6 on which we plug a dense linear classifier w \u03c9 : f \u03b8 = w \u03c9 \u2022 \u03a6 \u03c6 and \u03b8 = (\u03c6, \u03c9).\n\nIn training, we have access to different domains E: for each domain e \u2208 E, the dataset D e = x i e , y i e ne i=1 contains n e i.i.d. (input, labels) samples drawn from a domaindependent probability distribution. Combined together, the datasets {D e } e\u2208E are of size n = e\u2208E n e . Our goal is to learn weights \u03b8 so that f \u03b8 predicts well on a new test domain, unseen in training. As described in Koh et al. (2020) and Ye et al. (2021), most common distribution shifts are diversity shifts -where the training and test distributions comprise data from related but distinct domains, for instance pictures and drawings of the same objects -or correlation shifts -where the distribution of the covariates at test time differs from the one during training. To generalize well despite these distribution shifts, f \u03b8 should ideally capture an invariant mechanism across training domains. Following standard notations, M f \u03b8 x i e , y i e and is the negative log-likelihood loss. Many approaches try to exploit some external source of knowledge (Xie et al., 2021), in particular the domain information. As a side note, these partitions may be inferred if not provided (Creager et al., 2021). Some works explore data augmentations to mix samples from different domains (Wang et al., 2020;Wu et al., 2020), some re-weight the training samples to favor underrepresented groups (Sagawa et al., 2020a;b;Zhang et al., 2021) and others include domain-dependent weights (Ding & Fu, 2017;Mancini et al., 2018). Yet, most recent works promote invariance via a regularization criterion and only differ by the choice of the statistics to be matched across training domains. They can be categorized into three groups: these methods enforce agreement either (1) in features (2) in predictors or (3) in gradients.\n\nFirst, some approaches aim at extracting domain-invariant features and were extensively studied for unsupervised domain adaptation. The features are usually aligned with adversarial methods (Ganin et al., 2016;Gong et al., 2016;Li et al., 2018b; or with kernel methods (Muandet et al., 2013;Long et al., 2014). Yet, the simple covariance matching in CORAL (Sun et al., 2016;Sun & Saenko, 2016) performs best on various tasks for OOD generalization (Gulrajani & Lopez-Paz, 2021). With Z ij e the jth dimension of the features extracted by \u03a6 \u03c6 for the ith example x i e of domain e \u2208 E = {A, B}, CORAL minimizes Cov(Z A ) \u2212 Cov(Z B ) 2 F where Cov(Z e ) = 1 ne\u22121 (Z e Z e \u2212 1 ne 1 Z e 1 Z e ) is the feature covariance matrix. CORAL is more powerful than mere feature\nmatching 1 n A 1 Z A \u2212 1 n B 1 Z B 2 2\nas in Deep Domain Confusion (DDC) (Tzeng et al., 2014). Yet, Johansson et al. (2019) and Zhao et al. (2019) show that these approaches are insufficient to guarantee good generalization.\n\nMotivated by arguments from causality (Pearl, 2009) and the idea that statistical dependencies are epiphenomena of an underlying structure, Invariant Risk Minimization (IRM) (Arjovsky et al., 2019) explains that the predictor should be invariant (Peters et al., 2016;Rojas-Carulla et al., 2018), i.e., simultaneously optimal across all domains. Yet, recent works point out pitfalls of IRM (Guo et al., 2021;Kamath et al., 2021;Ahuja et al., 2019), that does not provably work with non-linear data (Rosenfeld et al., 2021) and could not improve over ERM when hyperparameter selection is restricted (Koh et al., 2020;Gulrajani & Lopez-Paz, 2021). Among many suggested improvements (Chang et al., 2020;Idnani & Kao, 2020;Teney et al., 2020;Ahmed et al., 2021), Risk Extrapolation (V-REx) (Krueger et al., 2021) argues that training risks from different domains should be similar and thus penalizes |R A \u2212 R B | 2 when E = {A, B}.\n\nA third and most recent line of work promotes agreements between gradients with respect to network weights. Gradient agreements help batches from different tasks to cooperate, and have been previously employed for multitasks (Du et al., 2018;Yu et al., 2020), continual (Lopez-Paz & Ranzato, 2017), meta (Finn et al., 2017Zhang et al., 2020) and reinforcement (Zhang et al., 2019) learning. In OOD generalization, Koyama & Yamaguchi (2020);Parascandolo et al. (2021);Shi et al. (2021) try to find minimas in the loss landscape that are shared across domains. Specifically, these works tackle the domain-level expected gradients:\ng e = E (xe,ye)\u223cDe \u2207 \u03b8 (f \u03b8 (x e ), y e ) .\n(1) AND-mask (Parascandolo et al., 2021) andothers (Mansilla et al., 2021;Shahtalebi et al., 2021) update weights only when g A and g B point to the same direction.\nWhen E = {A, B}, IGA (Koyama & Yamaguchi, 2020) minimizes ||g A \u2212 g B || 2 2 ; Fish (Shi et al., 2021) increases g A \u00b7 g B ;\nAlong with the increased computation cost, the main limitation of previous gradient-based methods is the per-domain batch averaging of gradients: this removes more granular statistics, in particular the information from pairwise interactions between gradients from samples in a same domain. In opposition, our new regularization for OOD generalization keeps extra information from individual gradients and matches across domains the domain-level gradient variances. In a nutshell, Fishr is similar to the covariance-based CORAL (Sun et al., 2016;Sun & Saenko, 2016) but in the gradient space rather than in the feature space.\n\n\nFishr\n\n\nGradient variance matching\n\nThe individual gradient g i e = \u2207 \u03b8 f \u03b8 (x i e ), y i e is the firstorder derivative for the i-th data example x i e , y i e from domain e \u2208 E with respect to the weights \u03b8. Previous methods have matched the gradient means g e = 1 ne ne i=1 g i e for each domain e \u2208 E. These gradient means capture the average learning direction but can not capture gradient disagreements (Sankararaman et al., 2020;Yin et al., 2018). With G e = [g i e ] ne i=1 of size n e \u00d7 |\u03b8|, we compute the domain-level gradient variance vectors of size |\u03b8|:\nv e = Var(G e ) = 1 n e \u2212 1 ne i=1 g i e \u2212 g e 2 ,(2)\nwhere the square indicates an element-wise product. To reduce the distribution shifts in the network f \u03b8 across domains, we bring the domain-level gradient variances {v e } e\u2208E closer. Hence, our Fishr regularization is:\nL Fishr (\u03b8) = 1 |E| e\u2208E v e \u2212 v 2 2 ,(3)\nthe square of the Euclidean distance between the gradient variance from the different domains e \u2208 E and the mean gradient variance v = 1 |E| e\u2208E v e . Balanced with a hyperparameter coefficient \u03bb > 0, this Fishr penalty complements the original ERM objective, i.e., the empirical training risks:\nL(\u03b8) = 1 |E| e\u2208E R e (\u03b8) + \u03bbL Fishr (\u03b8).(4)\nRemark 3.1. Gradients g i e can be computed on all network weights \u03b8. Yet, to reduce the memory and training costs, they will often be computed only on a subset of \u03b8, e.g., only on classification weights \u03c9. This approximation is discussed in Section 4.2.2 and Appendix D.3.2.\n\n\nTheoretical analysis\n\nWe theoretically motivate our Fishr regularization by leveraging the domain inconsistency score introduced in ANDmask (Parascandolo et al., 2021). We first derive a generalized upper bound for this score. Then, we show that Fishr minimizes this upper bound by matching simultaneously domain-level risks and Hessians. Figure 2: Loss landscapes around inconsistent weights \u03b8 * at convergence. N 0.2 A,\u03b8 * contains weights \u03b8 for which\n\n\nINCONSISTENCY FORMALISM\nR A (\u03b8) is low (\u2264 0.2) but R B (\u03b8) is high (\u2265 0.9)\n. This inconsistency is due to conflicting domain-level loss landscapes, specifically gaps between domain-level risks and curvatures at \u03b8 * . This is visible in the disagreements across the variances of gradients\n{g i A } n A i=1 and {g i B } n B i=1 .\nParascandolo et al. (2021) argues that \"patchwork solutions sewing together different strategies\" for different domains may not generalize well: good weights should be optimal on all domains and \"hard to vary\" (Deutsch, 2011). They formalize this insight with an inconsistency score:\nI (\u03b8 * ) = max (A,B)\u2208E 2 max \u03b8\u2208N A,\u03b8 * |R B (\u03b8) \u2212 R A (\u03b8 * )| ,(5)\nwhere \u03b8 \u2208 N A,\u03b8 * if there exists a path in the weights space between \u03b8 and \u03b8 * where the risk R A remains in an > 0 interval around R A (\u03b8 * ). I increases with conflicting geometries in the loss landscapes around \u03b8 * as in Fig. 2: i.e., when another 'close' solution \u03b8 is equivalent to the current solution \u03b8 * in a domain A but yields different risks in B.\n\nFor e \u2208 E, the second-order Taylor expansion of R e around \u03b8 * = 0 (with a change of variable) gives:\nR e (\u03b8) = R e (\u03b8 * ) + \u03b8 \u2207 \u03b8 R e (\u03b8 * ) + 1 2 \u03b8 H e \u03b8 + O( \u03b8 2 2 ),\nwhere the Hessian H e = \u2207 2 \u03b8 R e (\u03b8 * ) approximates the local curvature of the loss landscape. Moreover, we assume simultaneous convergence, i.e., \u03b8 * is a local minima across all domains: \u2207 \u03b8 R e (\u03b8 * ) = 0. Thus, locally around \u03b8 * :\nmax \u03b8\u2208N A,\u03b8 * |R B (\u03b8) \u2212 R A (\u03b8 * )| \u2248 max |R A (\u03b8)\u2212R A (\u03b8 * )|\u2264 |R B (\u03b8) \u2212 R A (\u03b8 * )| \u2248 max 1 2 |\u03b8 H A \u03b8|\u2264 R B (\u03b8 * ) + 1 2 \u03b8 H B \u03b8 \u2212 R A (\u03b8 * ) |R B (\u03b8 * ) \u2212 R A (\u03b8 * )| + max 1 2 |\u03b8 H A \u03b8|\u2264 1 2 \u03b8 H B \u03b8 ,(6)\nwhere we deduced the last line from the triangle inequality. Appendix A.1 formally demonstrates following equality.\n\nProposition 1. Under the quadratic bowl Assumption A.1 with positive definite Hessians, for small (see Eq. 11):\nI (\u03b8 * ) = max (A,B)\u2208E 2 (R B (\u03b8 * ) \u2212 R A (\u03b8 * ) + max 1 2 \u03b8 H A \u03b8\u2264 1 2 \u03b8 H B \u03b8).(7)\nThe Hessian being positive definite is a standard hypothesis, notably used in Parascandolo et al. (2021), that is empirically reasonable (Sagun et al., 2018): \"in only very few steps . . . large negative eigenvalues disappear\" (Ghorbani et al., 2019).\n\nThe first term in the RHS of Proposition 1 is the difference between domain-level risks, whose square is the criterion minimized in V- REx (Krueger et al., 2021). We will prove and show that Fishr forces this term to be small in Section 3.2.2. In contrast, Parascandolo et al. (2021) made the strong assumption:\nR A (\u03b8 * ) = R B (\u03b8 * ) = 0.\nWhile Parascandolo et al. (2021) ignored this first term, we follow their diagonal approximation of the Hessians to analyze the second term. In that case, H e = diag (\u03bb e 1 , \u00b7 \u00b7 \u00b7 , \u03bb e h ) with \u2200i \u2208 {1, . . . , h} , \u03bb e i > 0. Then:\nmax 1 2 \u03b8 H A \u03b8\u2264 1 2 \u03b8 H B \u03b8 = max \u03b8 2 2 \u2264 i\u03b8 2 i \u03bb B i /\u03bb A i = \u00d7 max i \u03bb B i /\u03bb A i .(8)\nThis is large when exists i such that \u03bb A i is small but \u03bb B i is large: indeed, a small weight perturbation in the direction of the associated eigenvector would change the loss slightly in the domain A but drastically in domain B. Thus, this second term decreases when H A and H B have similar eigenvalues. This result holds when Hessians are co-diagonalizable. In conclusion, this explains why forcing H A = H B reduces inconsistencies in the loss landscape and thus improves generalization. AND-mask matches Hessians by zeroing out gradients with inconsistent directions across domains; however, this masking strategy introduces dead zones (Shahtalebi et al., 2021) in weights where the model could get stuck, ignores gradient magnitudes and empirically performs poorly with real datasets from DomainBed. As shown in Section 3.2.3, Fishr proposes a new method to align domain-level Hessians leveraging the close relations between the gradient variance, the Fisher Information and the Hessian.\n\n\nFISHR MATCHES THE DOMAIN-LEVEL RISKS\n\nGradients take into account the label Y , which appears as an argument for the loss . Hence, gradient-based approaches are 'label-aware' by design. In contrast, feature-based methods were shown to fail in case of label shifts, because they do not consider Y (Johansson et al., 2019;Zhao et al., 2019).\n\nThe fact that the label and the loss appear in the formula of the gradients has another important consequence: matching gradient distributions also matches training risks, as motivated in V- REx (Krueger et al., 2021). We confirm this insight in Table 2: matching gradient variances with Fishr\ninduces |R A \u2212 R B | 2 \u2192 0 when E = {A, B}.\nIntuitively, gradient amplitudes are directly weighted by the loss values: multiplying the loss by a constant will also multiply the gradients by the same constant. Thus roughly, if the domain-level empirical training risks are different, then the domain-level gradient norms should also differ.\n\nTheoretically, we prove in Appendix A.2 that Fishr regularization component with reference to the classification bias is exactly the difference between domain-level mean squared errors. We recover the objective from V-REx (Krueger et al., 2021), with a different loss (squared error instead of negative log likelihood). More generally, we show in this Appendix that Fishr in the classifier w \u03c9 acts as a feature-adaptive version of V-REx: the components in Fishr adaptively force the risks to be similar across domains.\n\n\nFISHR MATCHES THE DOMAIN-LEVEL HESSIANS\n\nThe Hessian matrix H = n i=1 \u2207 2 \u03b8 f \u03b8 (x i ), y i is of key importance in deep learning. Yet, H cannot be computed efficiently in general. Recent methods (Izmailov et al., 2018;Parascandolo et al., 2021;Foret et al., 2021) tackled the Hessian indirectly by modifying the learning procedure. In contrast, we use the fact that the diagonal of H is approximated by the gradient variance Var(G); this is confirmed in Table 1. This result is derived below from 3 individual and standard approximation steps.  Fisher, 1922;C.R., 1945) approximates the Hessian H with theoretically probably bounded errors under mild assumptions (Schraudolph, 2002).\nFIM F = n i=1 E\u0177 \u223cP \u03b8 (\u00b7|x i ) \u2207 \u03b8 log p \u03b8 (\u0177|x i )\u2207 \u03b8 log p \u03b8 (\u0177|x i ) (\nThe 'true' FIM and the 'empirical' FIM. Yet, F remains costly as it demands one backpropagation per class. That's why most empirical works (e.g., in compression (Frantar et al., 2021;Liu et al., 2021) and optimization (Dangel et al., 2021)) approximate the 'true' , 2014) where p \u03b8 (\u00b7|x) is the density predicted by f \u03b8 on input x. While F uses the model distribution P \u03b8 (\u00b7|X),F uses the data distribution P (Y |X). Despite this key difference,F and F were shown to share the same structure and to be similar up to a scalar factor (Thomas et al., 2020). They also have analogous properties: Tr(F ) \u2248 Tr(F ). This was discussed in Li et al. (2020) and further highlighted even at early stages of training (before overfitting) in the Fig. 1 and the Appendix S3 of Singh & Alistarh (2020).\nFIM F with the 'empirical' FIMF = G e G e = n i=1 \u2207 \u03b8 log p \u03b8 (y i |x i )\u2207 \u03b8 log p \u03b8 (y i |x i ) (Martens\nThe 'empirical' FIM and the gradient covariance. Critically,F is nothing else than the unnormalized uncentered covariance matrix when is the negative loglikelihood. Thus, the gradient covariance matrix C =  Compared to ERM, Fishr matches the gradient variance (Diag(C 90% ) \u2248 Diag(C 80% )) in all network weights \u03b8. Most importantly, this enforces invariance in domainlevel risks (R 90% \u2248 R 80% ) and in domain-level Hessians (Diag(H 90% ) \u2248 Diag(H 80% )). The gradient variance, computable efficiently with a unique backpropagation, serves as a proxy for the Hessian. Details and more experiments in Section 4.1 (notably Fig. 3) and in Appendix C.2.1.\n\n\nERM Fishr\nVar(G 90% ) \u2212 Var(G 80% ) 2 F 1.6 4.1 \u00d7 10 \u22125 |R 90% \u2212 R 80% | 2 1.0 \u00d7 10 \u22122 3.8 \u00d7 10 \u22126 Diag (H 90% \u2212 H 80% ) 2 F 2.9 \u00d7 10 \u22121 2.7 \u00d7 10 \u22124\nConsequences for Fishr. Critically, Fishr considers the gradient variance Var(G), i.e., the diagonal components of C. In our multi-domain framework, we define the domain-level matrices with the subscript e. Remark 3.2. Limitation of our approximation. We acknowledge that approximating the 'true' FIM F by the 'empirical' FIMF is not fully justified theoretically (Martens, 2014;Kunstner et al., 2019). Indeed, this approximation is valid only under strong assumptions, in particular \u03c7 2 convergence of predictions P \u03b8 (\u00b7|X) towards labels P (Y |X)as detailed in Proposition 1 from Thomas et al. (2020). In this paper, we trade off theoretical guarantees for efficiency.\n\nRemark 3.3. Diagonal approximation. The empirical similarities between C and H motivate using gradient variance rather than gradient covariance, which scales down the number of targeted components from |\u03b8| 2 to |\u03b8|. Indeed, diagonally approximating the Hessian is common: e.g., for OOD generalization (Parascandolo et al., 2021), optimization (LeCun et al., 2012;Kingma & Ba, 2014), continual learning (Kirkpatrick et al., 2017) and pruning (LeCun et al., 1990;Theis et al., 2018). This is based on the empirical evidence (Becker & Le Cun, 1988) that Hessians are diagonally dominant at the end of training. Our diagonal approximation is also motivated by the critical importance of Tr(C) (Jastrzebski et al., 2021;Faghri et al., 2020) to analyze the generalization properties of DNNs. We confirm empirically in Appendix C.2.3 that considering the off-diagonal parts of C performs no better that just matching the diagonals.\n\nConclusion. Fishr efficiently matches (1) domain-level empirical risks and (2) domain-level Hessians across the training domains, using gradient variances as a proxy. This will align domain-level loss landscapes, reduce domain inconsistencies and increase domain generalization. In particular, the domain-level Hessian matching illustrates that Fishr is more than just a generalization of gradient-mean approaches such as Fish (Shi et al., 2021).\n\nFinally, we refer the readers to Appendix A.3 where we leverage the Neural Tangent Kernel (NTK) (Jacot et al., 2018) theory to further motivate the gradient variance matching during the optimization process -and not only at convergence. In brief, as F and the NTK matrices share the same non-zero eigenvalues, similar {C e } e\u2208E during training reduce the simplicity bias by preventing the learning of different domain-dependent shortcuts at different training speeds: this favors a shared mechanism that predicts the same thing for the same reasons across domains.\n\n\nExperiments\n\nWe prove Fishr effectiveness on Colored MNIST (Arjovsky et al., 2019) and then on the DomainBed benchmark (Gulrajani & Lopez-Paz, 2021). To facilitate reproducibility, the code is available at https://github.com/ alexrame/fishr. Moreover, we show in Appendix B that Fishr is effective in the linear setting.\n\n\nProof of concept on Colored MNIST\n\nThe task in Colored MNIST (Arjovsky et al., 2019) is to predict whether the digit is below or above 5. Moreover, the labels are flipped with 25% probability (except in Appendix C.2.2). Critically, the digits' colors spuriously correlate with the labels: the correlation strength varies across the two training domains E = {90%, 80%}. To test whether the model has learned to ignore the color, this correlation is reversed at test time. In brief, a biased model that only considers the color would have 10% test accuracy whereas an oracle model that perfectly predicts the shape would have 75%. As previously done in V-REx (Krueger et al., 2021), we strictly follow the IRM implementation and just replace the IRM penalty by our Fishr penalty. This means that we use the exact same MLP and hyperparameters, notably the same two-stage scheduling selected in IRM for the regularization strength \u03bb, that is low until epoch 190 and then jumps to a large value, which was optimized via a gridsearch for IRM. More experimental details are provided in Appendix C.1. Table 3 reports the accuracy averaged over 10 runs with standard deviation. Fishr \u03b8 (i.e., applying Fishr on all weights \u03b8) obtains the best trade-off between train and test accuracies; notably in test, it reaches 71.2%, or 70.2% when digits are grayscale. Moreover, computing the gradients only in the classifier w \u03c9 performs almost as well (69.5% in test for Fishr \u03c9 ) while reducing drastically the computational cost. Finally, Fishr \u03c6 only in the features extractor \u03c6 works best in test, though it has lower train accuracy. This last experiment shows that we can reduce domain shifts without explicitly forcing the predictors to be simultaneously optimal. These results highlight the effectiveness of gradient variance matching -even with standard hyperparameters -at different layers of the network.\n\nThe main advantage of this synthetic dataset is the possibility of empirically validating some theoretical insights. For example, the training dynamics in Fig. 3 show that the domain-level empirical risks get closer once the Fishr \u03b8 gradient variance matching loss is activated after step 190 (|R 90% \u2212 R 80% | \u2192 0), even though predicting accurately on the domain 90% is easier than on the domain 80%. This confirms insights from Section 3.2.2. Similarly, we observe that Fishr matches Hessians across the two training domains. This is confirmed by further experiments in Appendix C.2, and validates insights from Section 3.2.3. Overall, Fishr regularization reduces train accuracy, but sharply increases test accuracy. Yet, the main drawback of Colored MNIST is its insufficiency to ensure generalization for real-world datasets.\n\nOverall, it should be considered as a proof-of-concept.\n\n\nDomainBed benchmark\n\n\nDATASETS AND PROCEDURE\n\nWe conduct extensive experiments on the DomainBed benchmark Input:\nDNN f \u03b8 , observations D e = x i e , y i e ne i=1\nfor domains e \u2208 E, regularization weight \u03bb, warmup iteration i warmup , exponential moving average \u03b3 and batch size b s Initialize: moving averages: \u2200e \u2208 E, v mean e \u2190 0 for iter from 1 to #iters do {# Step 1: standard ERM procedure} for e \u2208 E do Randomly select batch:\n{(x i e , y i e )} i\u2208B of size b s Compute predictions: \u2200i \u2208 B,\u0177 i e \u2190 f \u03b8 (x i e ) Compute empirical risks: R e (\u03b8) \u2190 i\u2208B \u0177 i e , y i e end for L(\u03b8) = 1 |E| e\u2208E R e (\u03b8) {# Step 2: gradient variances in classifier} for e \u2208 E do Compute individual gradients in w \u03c9 with Back- PACK: \u2200i \u2208 B, g i e \u2190 \u2207 \u03c9 \u0177 i e , y i e Compute domain gradient variances v e (Eq. 2) Update v mean e = v e \u2190 \u03b3v mean e + (1 \u2212 \u03b3)v iter e end for if iter \u2265 i warmup then L(\u03b8) += \u03bbL Fishr (\u03b8) (Eq. 3) end if {#\nStep 3: gradient descent in the whole network} Backpropagate gradients \u2207 \u03b8 L(\u03b8) in the network f \u03b8 with standard PyTorch end for for the same number of steps. Results are averaged over three trials. This experimental setup is further described in Appendix D.1. By imposing the datasets, the training procedure and controlling the hyperparameter search, DomainBed is arguably the fairer open-source benchmark to rigorously compare the different strategies for OOD generalization.\n\n\nIMPLEMENTATION DETAILS\n\nWe systematically apply Fishr only in the classifier w \u03c9 in DomainBed. Indeed, keeping individual gradients in memory for \u03c6 from a ResNet-50 was impossible for computational reasons. Fishr \u03b8 and Fishr \u03c9 performed similarly in previous Section 4.1. This is partly because the gradients in \u03c9 still depend on \u03a6 \u03c6 . Additionally, as highlighted in Appendix D.3.2, this relaxation may improve results for real-world datasets. Indeed, while Colored MNIST is a correlation shift challenge, the other datasets mostly demonstrate diversity shifts where \"each domain represents a certain spectrum of diversity in data\" (Ye et al., 2021). Then, as the pixels distribution are quite different across domains, low-level layers may need to adapt to these domain-dependent peculiarities. Moreover, if we used all weights \u03b8 = (\u03c6, \u03c9) to compute gradient variances, the invariance in w \u03c9 may be overshadowed by \u03a6 \u03c6 due to |\u03c9| |\u03c6|. Finally, it's worth noting that this last-layer approximation is consistent with the IRM condition (Arjovsky et al., 2019) and is common for unsupervised domain adaptation (Ganin et al., 2016).\n\nFishr relies on three hyperparameters. First, the \u03bb coefficient controls the regularization strength: with \u03bb = 0 we recover ERM while a high \u03bb may cause underfitting. We show that Fishr is robust to the choice of the sampling distribution for hyperparameter \u03bb in Appendix D.3.3. Second the warmup iteration defines the step at which we activate the regularization. This warmup strategy is taken from previous works such as IRM (Arjovsky et al., 2019), V-REx (Krueger et al., 2021) or Spectral Decoupling (Pezeshki et al., 2021). Before that step, the DNN is trained with ERM to learn predictive features. After that step, the Fishr regularization encourages the DNN to have invariant gradient variances. Lastly, the domain-level gradient variances are more accurate when estimated over more data points. Rather than increasing the batch size, we follow Le Roux et al. (2011) and leverage an exponential moving average for computing stable gradient variances. Therefore our third hyperparameter is the coefficient \u03b3 controlling the update speed: at step t, we matchv t e = \u03b3v t\u22121 e + (1 \u2212 \u03b3)v t e rather than of v t e from Eq. 2. The closer \u03b3 is to 1, the smoother the variance is along training.v t\u22121 e from previous step t \u2212 1 is 'detached' from the computational graph. Similar strategies have already been used for OOD generalization (Nam et al., 2020;Blanchard et al., 2021). The memory overhead is (|E| \u00d7 |\u03c9|). We study by ablation the importance of this warmup strategy and this \u03b3 in Appendices D.3.1 and D.3.2.\n\nFishr is simple to implement (see the Algorithm 1) using the BackPACK (Dangel et al., 2020) package. While PyTorch (Paszke et al., 2019) can compute efficiently batch gradients, BackPACK optimizes the computation of individual gradients, sample per sample, at almost no time overhead. Thus, Fishr is also at low computational costs. For example, on PACS (7 classes and |\u03c9| = 14, 343) with a ResNet-50 and batch size 32, Fishr induces an overhead in memory of +0.2% and in training time of +2.7% (with a Tesla V100) compared to ERM; on the larger-scale DomainNet (345 classes and |\u03c9| = 706, 905), the overhead is +7.0% in memory and +6.5% in training time. As a side note, keeping the full covariance of size |\u03c9| 2 \u2248 5 \u00d7 10 8 on DomainNet would not have been possible. In contrast, Fish (Shi et al., 2021) leverages a meta-learning algorithm that is impractical as |E| times longer to train than ERM. ERM was carefully tuned in DomainBed and thus remains a strong baseline. Moreover, all previous methods are far from the best score on at least one dataset. Invariant predictors (IRM, V-REx) and gradient masking (AND-mask) approaches perform poorly on real datasets. Additionally, CORAL not only performs worse than ERM on TerraIncognita, but most importantly fails to detect correlation shifts on Colored MNIST: this is because feature-based approaches do not take into account the label, as previously stated in Section 3.2.2.\n\n\nRESULTS\n\nContrarily, Fishr is the only method to efficiently tackle correlation and diversity shifts, as defined in (Ye et al., 2021). Indeed, not only Fishr outperforms ERM on Colored MNIST (68.8% vs. 57.8%), but Fishr also systematically performs better than ERM on all real datasets: the differences are over standard errors on VLCS (78.2% vs. 77.6%), Office-Home (68.2% vs. 66.4%) and on the larger-scale Domain-Net (41.8% vs. 41.3%). Appendix D.3.2 shows that Fishr performs even better when combined with gradient-mean matching. In summary, Fishr consistently beats ERM (despite the restricted hyperparameter search): this is the main point to validate the effectiveness of our method.\n\nAdditionally, Fishr performs best after averaging: Firshr reaches 70.8% vs. 69.2% for the second best CORAL. When ignoring the Colored MNIST task, averaging over the 6 other datasets leads to a similar ranking: 1.Fishr(avg=71.1), 2.CORAL(71.0), 3.Mixup(70.8) and 4.ERM(70.5). This arguably partial metric is confirmed by the more robust ranking information; Fishr's median ranking of second reflects that Fishr is consistently among the best methods. Overall, Fishr is the state-of-the-art approach, not only in average accuracy, but most importantly in average ranking.\n\n\nConclusion\n\nIn this paper, we addressed the task of out-of-distribution generalization for classification in computer vision. We derive a new and simple regularization -Fishr -that matches the gradient variances across domains as a proxy for matching domain-level risks and Hessians. We prove that this reduces inconsistencies across domains. Fishr reaches state-of-the-art performances on DomainBed when samples from the test domain are available for model selection. Our experiments -reproducible with our open-source implementation -suggest that Fishr would consistently improve a deep classifier for real-world usages when dealing with data from multiple domains. We hope to pave the way towards new gradient-based regularization to improve the generalization abilities of deep neural networks.\n\nDeGrave, A. J., Janizek, J. D., and Lee, S.-I. Ai for radiographic covid-19 detection selects shortcuts over signal. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A. Averaging weights leads to wider optima and better generalization. In UAI, 2018. (p. 5).\n\nLe Roux, N., Bengio, Y., and Fitzgibbon, A. Improving first and second-order methods by modeling uncertainty. Optimization for Machine Learning, 2011. (pp. 8, 21, 24).\n\nLeCun, Y., Denker, J., Solla, S., Howard, R., and Jackel, L. Optimal brain damage. In NeurIPS, 1990. (p. 6 Li, X., Gu, Q., Zhou, Y., Chen, T., and Banerjee, A. Hessian based analysis of sgd for deep nets: Dynamics and generalization. In SIAM, 2020. (pp. 5, 17).\n\nLi, Y., Gong, M., Tian, X., Liu, T., and Tao, D. Domain generalization via conditional invariant representations.\n\nIn AAAI, 2018c. (pp. 3, 19).\n\nLiu, L., Zhang, S., Kuang, Z., Zhou, A., Xue, J.-H., Wang, X., Chen, Y., Yang, W., Liao, Q., and Zhang, W. Group fisher pruning for practical network compression. In  Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P. The pitfalls of simplicity bias in neural networks.\n\nIn NeurIPS, 2020. (p. 1).\n\nShahtalebi, S., Gagnon-Audet, J.-C., Laleh, T., Faramarzi, M., Ahuja, K., and Rish, I. Sand-mask: An enhanced gradient masking strategy for the discovery of invariances in domain generalization. In ICML UDL Workshop, 2021. (pp. 3, 4, 19, 20, 24 These Appendices complement the main paper.\n\n1. We first detail some theoretical points. Appendix A.1 demonstrates our Proposition 1. Appendix A.2 shows that Fishr acts as a feature-adaptive V-REx. Appendix A.3 motivates Fishr with intuitions from the Neural Tangent Kernel theory.\n\n2. Appendix B proves the effectiveness of our approach for a linear toy dataset.\n\n3. Appendix C enriches the Colored MNIST experiment in the IRM setup. In detail, we first describe the experimental setup in Appendix C.1. We then validate in Appendix C.2 some insights provided in the main paper; in particular, Appendix C.2.3 motivates the diagonal approximation of the gradient covariance. \n\n\nA. Additional Theoretical Analysis\n\nA.1. Demonstration of Proposition 1 from Section 3.2.1\n\nAssumption A.1. We make the quadratic bowl assumption around the local minima \u03b8 * on all domains : \u2200e \u2208 E,\nR e (\u03b8) = R e (\u03b8 * ) + 1 2 (\u03b8 \u2212 \u03b8 * ) H e (\u03b8 \u2212 \u03b8 * ),(9)\nwhere H e is positive definite of eigenvalues \u03bb e 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb e h > 0. Remark A.2. Assumption A.1 is milder on N e,\u03b8 * for low . Indeed, when \u2192 0, then max \u03b8\u2208N e,\u03b8 * \u03b8 \u2212 \u03b8 * 2 2 \u2192 0 and the quadratic approximation coincides with the second-order Taylor expansion around \u03b8 * . Moreover, this approximation is common in optimization (Schaul et al., 2013;Jastrzebski et al., 2018). Fig. 4). Let > 0, weights \u03b8 * . \u2200(A, B) \u2208 E 2 , with N A,\u03b8 * the largest path-connected region of weights space where the risk R A remains in an interval around R A (\u03b8 * ), we note:\n\n\nProposition 2. (Reformulation of Proposition 1, illustrated in\nI (A, B) = max \u03b8\u2208N A,\u03b8 * |R B (\u03b8) \u2212 R A (\u03b8 * )| , R(A, B) = R B (\u03b8 * ) \u2212 R A (\u03b8 * ), H (A, B) = max 1 2 (\u03b8\u2212\u03b8 * ) H A (\u03b8\u2212\u03b8 * )\u2264 1 2 (\u03b8 \u2212 \u03b8 * ) H B (\u03b8 \u2212 \u03b8 * ).(10)\nIf \u2200(A, B) \u2208 E 2 such as R(A, B) < 0, we have:\n\u2264 \u2212R(A, B) \u00d7 \u03bb A h \u03bb B 1 ,(11)\nthen under previous Assumption A.1,\nmax (A,B)\u2208E 2 I (A, B) = max (A,B)\u2208E 2 (R(A, B) + H (A, B))(12)\nProof We first prove that, under quadratic Assumption A.1, \u2200A \u2208 E, N A,\u03b8 * = {\u03b8| |R A (\u03b8) \u2212 R A (\u03b8 * )| \u2264 }. Indeed, the former is always included in the latter by definition. Reciprocally, be given \u03b8 in the latter, {\u03bb\u03b8 * + (1 \u2212 \u03bb)\u03b8|\u03bb \u2208 [0, 1]} linearly connects \u03b8 * to \u03b8 in parameter space with the risk R A remaining in an interval around R\nA (\u03b8 * ) because \u2200\u00b5 \u2208 [0, 1] we have |R A (\u00b5\u03b8 * + (1 \u2212 \u00b5)\u03b8) \u2212 R A (\u03b8 * )| = (1 \u2212 \u00b5) 2 |R A (\u03b8) \u2212 R A (\u03b8 * )| \u2264 (1 \u2212 \u00b5) 2 \u2264 .\nTherefore \u2200(A, B) \u2208 E 2 :  \nI (A, B) = max |R A (\u03b8)\u2212R A (\u03b8 * )|\u2264 |R B (\u03b8) \u2212 R A (\u03b8 * )| = max 1 2 (\u03b8\u2212\u03b8 * ) H A (\u03b8\u2212\u03b8 * )\u2264 R(A, B) + 1 2 (\u03b8 \u2212 \u03b8 * ) H B (\u03b8 \u2212 \u03b8 * )(\nCase R(A, B) < 0 Leveraging \u03bb B 1 the largest eigenvalue from H B and \u03bb A h the lowest eigenvalue from H A , we upper bound: \nH (A, B) \u2264 max \u03bb A h 2 \u03b8\u2212\u03b8 * 2 2 \u2264 \u03bb B 1 2 \u03b8 \u2212 \u03b8 * 2 2 = \u00d7 \u03bb B 1 \u03bb A h .(15)\nConclusion Combining Eq. 14, Eq. 16 and Eq. 17, we conclude the proof.\n\n\nA.2. Fishr as a feature-adaptive version of V-REx\n\nWe delve into the theoretical analysis of the Fishr regularization in the classifier w \u03c9 , that leverages p features extracted from \u03c6. We note z i e \u2208 R p the features for the i-th example from the domain e,\u0177 i e \u2208 [0, 1] the predictions after sigmoid and y i e \u2208 {0, 1} the one-hot encoded target. The linear layer W is parametrized by weights {w k } p k=1 and bias b. The gradient of the loss for this sample with respect to the bias b is \u2207 b (y i e ,\u0177 i e ) = (\u0177 i e \u2212 y i e ). Thus, the uncentered gradient variance in b for domain e is: v b e = 1 ne ne i=1 (\u0177 i e \u2212 y i e ) 2 , which is exactly the mean squared error (MSE) between predictions and targets in domain e. Thus, matching gradient variances in b will match risks across domains. This is the objective from V-REx (Krueger et al., 2021), where the squared error has replaced the negative log likelihood.\n\nWe can also look at the gradients with respect to the weight w k :\n\u2207 w k (y i e ,\u0177 i e ) = (\u0177 i e \u2212 y i e )z i e [k]. Thus, the uncentered gradient variance in w k for domain e is: v w k e = 1 ne ne i=1 (\u0177 i e \u2212 y i e )z i e [k]\n2 . This is the squared error, weighted for each sample (z i e , y i e ) by the square of the k-th feature z i e [k]: matching gradient variances directly matches these weighted squared errors, with k different weighting schemes, that depend on the features distribution. This describes Fishr as a feature-adaptive version of V-REx (Krueger et al., 2021). An intuitive example is when features are binary (z i e \u2208 {0, 1}); in that case, Fishr matches domain-level risks on groups of samples having a shared feature.\n\nMore exactly in Fishr, we match centered gradient variances, equivalent to the uncentered variance gradient matching at convergence under the assumption g e \u2248 0. Experiments in Table 5 and in Appendix C.2.4 confirm that centering or not the variances perform similarly.\n\n\nA.3. Neural Tangent Kernel perspective\n\nIn this Section we motivate the matching of gradient covariances with new arguments from the Neural Tangent Kernel (NTK) (Jacot et al., 2018) theory. As a reminder, the NTK K \u2208 R n\u00d7n is the gramian matrix with entries K[i, j] = \u2207 \u03b8 f \u03b8 (x i ) \u00b7 \u2207 \u03b8 f \u03b8 (x j ) that measure the gradients similarity at two different input points x i and x j . This kernel dictates the training dynamics of the DNN and remains fixed in the infinite width limit. Most importantly, as stated in Yang & Salman (2019), \"the simplicity bias of a wide neural network can be read off quickly from the spectrum of K: if the largest eigenvalue [\u03bb max ] of K accounts for most of Tr(K), then a typical random network looks like a function from the top eigenspace of K\": this holds for ReLu networks. In summary, gradient descent mostly happens in a tiny subspace (Gur-Ari et al., 2018) whose directions are defined by the main eigenvectors from K. Moreover, the learning speed is dictated by \u03bb max , which can be used to estimate a condition for a learning rate \u03b7 to converge: \u03b7 < 2/\u03bb max (Karakida et al., 2019).\n\nIn a multi-domain framework, having similar spectral decompositions across {K e } e\u2208E during the optimization process would improve OOD generalization for two reasons: \n\n\nB. Experiments on a Linear Example\n\nWe experimentally prove that Fishr is effective in the linear setting. To do so, we consider the binary classification dataset introduced in the Section 3.2 from Fish (Shi et al., 2021). Each example is composed of 4 static features (f 1 , f 2 , f 3 , f 4 ). While f 1 is invariant across the two train domains and the test domain, the three other features are spurious: their correlations with the label vary in each domain. The model is a linear logistic regression, with trainable weights W and bias b. As f 2 and f 3 have higher correlations with the label than f 1 in training, ERM relies mostly on f 2 and f 3 . This is indicated in the first line of Table 5 , 2020;Singh & Alistarh, 2020;Thomas et al., 2020), we argue in Section 3.2.3 that gradient covariance C can be used as a proxy to regularize the Hessian H -even though the proper approximation bounds are out of scope of this paper. This was empirically validated at convergence in Table 2 and during training in Fig. 3. We leveraged the DiagHessian method from BackPACK to compute Hessian diagonals, in all network weights \u03b8. Notably, Hessians are impractical in a training objective as computing \"Hessian is an order of magnitude more computationally intensive\" (see Fig. 9 in Dangel et al. (2020)). This Appendix further analyzes the Hessian trajectory during training.    This is also visible in Fig. 7, which is equivalent to Fig. 3, but for ERM (without the Fishr regularization). The distance between domain-level gradient variances (red) keeps increasing across domains E = {90%, 80%}: so does the distance across Hessians (purple). The distance across risks (pink) decreases, but slower than with Fishr regularization. Overall, the network still predicts the digit's color while only slightly using the digit's shape. That's why the test accuracy (blue) remains low. To further validate that Fishr can tackle distribution shifts, we investigate Colored MNIST but without the 25% label flipping. In Table  6, the label is then fully predictable from the digit shape. Using hyperparameters defined previously in Appendix C.1, we recover that IRM (82.2%) fails when the invariant feature is fully predictive (Ahuja et al., 2019): indeed, it performs worse than ERM (91.8%). In contrast, V-REx and Fishr \u03c9 perform better (95.3%): in conclusion, Fishr works even without label noise.\n\n\nC.2.2. COLORED MNIST WITHOUT LABEL FLIPPING\n\n\nC.2.3. GRADIENT VARIANCE OR COVARIANCE ?\n\nWe have justified ignoring the off-diagonal parts of the covariance to reduce the memory overhead. For the sake of completeness, the second line in Table 7 shows results with the full covariance matrix. This experiment is possible only when considering gradient in the classifier w \u03c9 for memory reasons. Overall, results are similar (or slightly worse) as when using only the diagonal: the slight difference may be explained by the approaches' different suitability to the hyperparameters (that were optimized for IRM). In conclusion, this preliminary experiment suggests that targeting the diagonal components is the most critical. We hope future works will further investigate this diagonal approximation or provide new methods to reduce the computational costs, such as K-FAC approximations (Heskes, 2000;Martens & Grosse, 2015). In Section 3.2.3, we argue that the gradient centered covariance C and the empirical Fisher Information Matrix (or uncentered covariance)F are highly related and equivalent when the DNN is at convergence and the gradient means are zero. So, we could have tackled the diagonals of the domain-level {F e } e\u2208E across domains, i.e., without centering the variances.\n\nEmpirically, comparing the first and third lines in Table 7 shows that centering or not the variance are almost equivalent. This holds true when applying Fishr on all weights \u03b8 (as lines fourth and six are also very similar). This was empirically confirmed in DomainBed: for example, Fishr with either centered or uncentered variances reach 67.8. Still, it's worth noting that explicitly matching simultaneously the gradient centered variances along with the gradient means performs best in Appendix D.3.2.\n\n\nD. DomainBed\n\n\nD.1. Description of the DomainBed benchmark\n\nWe now further detail our experiments on the DomainBed benchmark. Scores from most baselines are taken from the DomainBed (Gulrajani & Lopez-Paz, 2021) paper. Scores for AND-mask and SAND-mask are taken from the SAND-mask paper (Shahtalebi et al., 2021). Scores for IGA (Koyama & Yamaguchi, 2020) are not yet available: yet, for the sake of completeness, we analyze IGA in Appendix D.3.2. Missing scores will be included when available.\n\nThe same procedure was applied for all methods: for each domain, a random hyperparameter search of 20 trials over a joint distribution, described in Table 8, is performed. We discuss the choice of these distributions in Appendix D.3.3. The learning rate, the batch size (except for ARM), the weight decay and the dropout distributions are shared across all methodsall trained with Adam (Kingma & Ba, 2014). Specific hyperparameter distributions for concurrent methods can be found in the original work of Gulrajani & Lopez-Paz (2021). The data from each domain is split into 80% (used as training and testing) and 20% (used as validation for hyperparameter selection) splits. This random process is repeated with 3 different seeds: the reported numbers are the means and the standard errors over these 3 seeds. We clarify a subtle point (omitted in the Algorithm 1) concerning the hyperparameter \u03b3 that controls:v t e = \u03b3v t\u22121 e +(1\u2212\u03b3)v t e at step t. We remind thatv t\u22121 e from previous step t \u2212 1 is 'detached' from the computational graph. Thus when L from Eq. 4 is differentiated during SGD, the gradients going through v t e are multiplied by (1 \u2212 \u03b3). To compensate this and decorrelate the impact of \u03b3 and of \u03bb (that controls the regularization strength), we match 1 1\u2212\u03b3v t e . Finally, with this (1 \u2212 \u03b3) correction, the gradients' strength backpropagated in the network is independent of \u03b3.\n\nHere we list all concurrent approaches. We omitted the recent weight averaging approaches (Cha et al., 2021;Rame et al., 2022) whose contribution is complementary to others, that uses a custom hyperparameter search and does not report scores with the 'Test-domain' model selection.\n\nDomainBed includes seven multi-domain computer vision classification datasets: The convolutional neural network architecture used for the MNIST experiments is the one introduced in DomainBed: note that this is not the same MLP (described in Appendix C.1) as in our proof of concept in Section 4.1. All real datasets leverage a 'ResNet-50' pretrained on ImageNet, with a dropout layer before the newly added dense layer and fine-tuned with frozen batch normalization layers.\n\n\nD.2. 'Training-domain' model selection\n\nIn the main paper, we focus on the 'Test-domain' model selection, where the validation set follows the same distribution as the test domain. This is important to adapt the degree of model invariance according to the test domain. For Fishr, if the domain-dependant correlations are useful in test, the selected \u03bb would be small and Fishr would behave like ERM; in contrast, if the domain-dependant correlations are detrimental in test, the selected \u03bb would be large, and Fishr would improve over ERM by enforcing invariance. In Table 9, we use the 'Training-domain' model selection: the validation set is formed by randomly collecting 20% of each training domain. Fishr performs better than ERM on all real datasets (over standard errors for OfficeHome and DomainNet), except for PACS where the two reach 85.5%. In average, Fishr (67.1%) finishes third and is above most methods such as V-REx (65.6%). Fishr median ranking is fifth, with a mean ranking of 5.6. These additional results were not included in the main paper due to space constraints and also because this 'Training-domain' model selection has three clear limitations.\n\nFirst, learning causal mechanisms can be useless in this 'Training-domain' setup. Indeed, when the correlations are more predictive in training than the causal features, the variant model may be selected over the invariant one. This explains the poor results for all methods in 'Training-domain' Colored MNIST, where the color information is more predictive than the shape information in training. The best model on this task is ARM (Zhang et al., 2020) that uses test time adaptation -thus in a sense uses information from the test-domain -and whose contribution is mostly complementary to ours.\n\nSecond, the 'Training-domain' setup suffers from underspecification: \"predictors with equivalently strong held-out performance in the training domain [...] can behave very differently\" in test (D'Amour et al., 2020). This underspecification favors low regularization thus low values of \u03bb. To select the model with the best generalization properties, future benchmarks may consider the training calibration (Wald et al., 2021) rather than merely selecting the model with the best training accuracy.\n\nThird, the 'Test-domain' model selection is more realistic for real applications. Indeed, one user would easily label some samples to validate the efficiency of its algorithm. It's not realistic to believe that the users would simply deploy their new algorithm without at least checking that the performances are correct. We recall that the 'Test-domain' setup in DomainBed benchmark is quite restricting, allowing only one evaluation per choice of hyperparameters, without early-stopping.\n\nThat's why Teney et al. (2021) even states that \"OOD performance cannot, by definition, be performed with a validation set from the same distribution as the training data\". Both opinions being reasonable and arguable, we included 'Trainingdomain' results for the sake of completeness, where Fishr remains stronger than ERM. Yet, our state-of-the-art results on the 'Test-domain' setup from Table 4 alone are sufficient to prove the usefulness of our approach for real-world applications.\n\n\nD.3. Fishr component analysis on DomainBed\n\n\nD.3.1. FOCUS ON THE EXPONENTIAL MOVING AVERAGE\n\nFollowing Le Roux et al. (2011), we use an exponential moving average (ema) parameterized by \u03b3 for computing gradient variances in DomainBed: the closer \u03b3 is to 1, the longer a batch will impact the variance from later steps. We now further analyze the impact of this strategy, which is not specific to Fishr and was used previously in other works (Nam et al., 2020;Blanchard et al., 2021;Zhang et al., 2021) for OOD generalization. Notably, this ema strategy could be applied to better estimate domain-level empirical risks in V- REx (Krueger et al., 2021). For a fair comparison, we introduce a new approach -V-REx with ema -that penalizes |R t A \u2212R t B | 2 at step t whereR t e = \u03b3R t\u22121 e + (1 \u2212 \u03b3)R t e when E = {A, B}. Thus, we compare V-REx and Fishr, with \u03b3 = 0 () or with \u03b3 \u223c Uniform(0.9, 0.99) (, as described in Table 8). On the   Table 11, the ema is less beneficial (from 67.5% to 68.2% in 'Test-domain' for Fishr). Notably, it worsens V-REX. Overall, Fishr -with and without ema -outperforms V-REx on OfficeHome.\n\nWe speculate that ema mainly helps when the batch size is not sufficiently large to detect 'slight' correlation shifts in the training datasets: e.g., when batch size \u223c 2 Uniform(3,9) and training datasets E = {90%, 80%} in Colored MNIST. We remind that when the batch size was 25,000 in the Colored MNIST setup from IRM, Fishr reached 69.5% (without ema) in Table 3 from Section 4.1. On the contrary, when the shift is more prominent as in OfficeHome, the ema may be less necessary. Most importantly, Fishr -with and without ema -improves over ERM on these datasets.\n\n\nD.3.2. COMPONENT ANALYSIS BY COMPARING GRADIENT VARIANCE VERSUS GRADIENT MEAN MATCHING\n\nAs a reminder from the Section 2, IGA (Koyama & Yamaguchi, 2020) is an unpublished gradient-based approach that matches gradient means across domains, i.e., minimizes ||g A \u2212 g B || 2 2 when E = {A, B} and where g e = 1 ne ne i=1 \u2207 \u03b8 (f \u03b8 (x e ), y e ). Scores for IGA are not available publicly and thus were not included in Section 4.2.1. Moreover, IGA is very costly and impractical: IGA is approximately (|E| + 1) times longer to train than ERM. Yet, we ran the DomainBed implementation of IGA on one 'synthetic' and one 'real' dataset. Table 12 shows that the IGA has little effect on . Moreover, on OfficeHome in Table 13, IGA hinders learning . In brief, the seminal \"IGA [. . .] could completely fail when generalizing to unseen domains\", as stated in Fish (Shi et al., 2021).\n\nIn the rest of this Section, we include IGA in Fishr codebase so that both methods leverage the same implementation choices: this enables fairer comparisons between gradient mean matching and gradient variance matching. These experiments provide further insights regarding Fishr main components: specifically, enforcing invariance (1) only in the  classifier's weights \u03c9 (2) after a warmup period and (3) with an exponential moving average.\n\nFirst, Fishr only considers gradient variances in the classifier's weights \u03c9. Similarly, we try to apply IGA's gradient mean matching but only in w \u03c9 rather than in f \u03b8 . This new method works significantly better (67.2% when\ng e = 1 ne ne i=1 \u2207 \u03c9 (f \u03b8 (x e ), y e ) vs. 56.9% when g e = 1 ne ne i=1 \u2207 \u03b8 (f \u03b8 (x e )\n, y e ) for 'Test-domain' OfficeHome in Table 13) while reducing the computational overhead. This further motivates the invariance in the classifier rather than in the low-level layers (which need to adapt to shifts in pixels for instance). We have done this analysis on IGA and not on Fishr because keeping all individual gradients for a ResNet-50 in the GPU memory was not possible on our hardware.\n\nSecond, Fishr uses a double-stage scheduling inherited from IRM (Arjovsky et al., 2019): the DNN first learns predictive features with standard ERM (\u03bb = 0) until a given epoch, at which \u03bb takes its true (high) value to then force domain invariance. This warmup strategy slightly increases 'Test-domain' results on Colored MNIST (from 58.6% to 59.8% for Fishr, from 58.3% to 59.2% for IGA) but does not seem critical: in particular, it reduces IGA 'Test-domain' scores on OfficeHome.\n\nThird, the estimation of gradient variances was improved with an exponential moving average (see Section 4.2.1 and Appendix D.3.1). We now use this strategy with domain-level gradient means for IGA in \u03c9:\u1e21 t e = \u03b3\u1e21 t\u22121 e + (1 \u2212 \u03b3)g t e . This improves IGA (from 67.0% to 67.2% in 'Test-domain' on OfficeHome): yet, these scores remain consistently worse than Fishr's (from 67.5% to 68.2%).\n\nIn conclusion, this complements the experiments in Section 4.2.1 which showed that tackling gradient variance does better than tackling gradient mean: indeed, Fishr performed better than Fish (Shi et al., 2021), AND-mask (Parascandolo et al., 2021) and SAND-mask (Shahtalebi et al., 2021). As a final note, Fishr + IGA -i.e., matching simultaneously gradient means (the first moment) and variances (the second moment) -performs best. Future works may further analyze the complementary of these gradient-based methods. This Section is a preliminary introduction to a meta-discussion, not about the methodology to select the best hyperparameters, but about the methodology to select the hyperparameter distributions in DomainBed. This question has not been discussed in previous works (as far as we know).\n\n\nD.3.3. HYPERPARAMETER DISTRIBUTIONS\n\nAfter few initial iterations on the main idea of the paper, we had to select the distributions to sample our three hyperparameters from, as described in Table 8. First, to select the ema \u03b3 distribution, we knew that the authors from Le Roux et al. (2011) have not noticed \"any significant difference in validation errors\" for different values higher than 0.9. Moreover \u03b3 should remain strictly lower than 1. Thus, sampling from Uniform(0.9, 0.99) seemed appropriate. Second, sampling the number of warmup iterations uniformly along training from Uniform(0, 5000) seemed the most natural and neutral choice. Lastly, the choice of the \u03bb distribution was more complex. As a reminder, a low \u03bb inactivates the regularization while an extremely high \u03bb may destabilize the training.\n\nIn Table 14, we investigate two distributions: \u03bb \u223c 10 Uniform(1,4) (eventually chosen for Fishr) and \u03bb \u223c 10 Uniform(1,5) . First, we observe that results are mostly similar: it confirms that Fishr is consistently better than ERM (where \u03bb = 0), and in average is the best approach with the 'Test-domain' model selection and among the best approaches with the 'Trainingdomain' model selection. Second, the existence of consistent differences in results suggests that the best hyperparameter distribution depends on the dataset at hand and that the performance gap depends on the selection method.\n\nWhile out of the scope of this paper, we believe these results were important for transparency (along with publishing our code), and may motivate the need for new protocols -for example with bayesian hyperparameter search (Turner et al., 2021) -that future benchmarks may introduce.   \n\n\nD.4. Full DomainBed results\n\n\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\n\nFigure 1 :\n1Fishr principle. Fishr considers the individual (per-sample) gradients of the loss in the network weights \u03b8.\n\n\nmotivate the need for new ideas.\n\n2 F\n2denotes the Frobenius norm of matrix M ; v 2 2 denotes the euclidean norm of vector v; 1 is a column vector with all elements equal to 1. The standard Empirical Risk Minimization (ERM) (Vapnik, 1999) framework simply minimizes the average empirical risk over all training domains, i.e., 1 |E| e\u2208E R e (\u03b8) where R e (\u03b8)\n\n\nof size |\u03b8| \u00d7 |\u03b8| and F are equivalent (up to the multiplicative constant n) at any first-order stationary point: C \u221d \u223cF . Overall, this suggests that C and H are closely related (Jastrzebski et al., 2018);.\n\n\n(Gulrajani & Lopez-Paz, 2021). In addition to the synthetic Colored MNIST(Arjovsky et al., 2019) andRotated MNIST (Ghifary et al., 2015), the multi-domain image classification datasets are the real VLCS (Fang et al., 2013), PACS (Li et al., 2017), OfficeHome (Venkateswara et al., 2017), TerraIncognita (Beery et al., 2018) and Do-mainNet (Peng et al., 2019). To limit access to test domain, the framework enforces that all methods are trained with only 20 different configurations of hyperparameters and Algorithm 1 Training procedure for Fishr on DomainBed.\n\n\nICML, 2021. (p. 5). Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. S. Transfer joint matching for unsupervised domain adaptation. In CVPR, 2014. (p. 3).Lopez-Paz, D. and Ranzato, M. A. Gradient episodic memory for continual learning. In NeurIPS, 2017. (p. 3). Maddox, W. J., Tang, S., Moreno, P. G., Wilson, A. G., and Damianou, A. On transfer learning via linearized neural networks. In NeurIPS workshop, 2019. (p. 16). Mancini, M., Bulo, S. R., Caputo, B., and Ricci, E. Best sources forward: domain generalization through sourcespecific nets. In ICIP, 2018. (p. 2). Mansilla, L., Echeveste, R., Milone, D. H., and Ferrante, E. Domain generalization via gradient surgery. In ICCV, 2021. (p. 3). Martens, J. New insights and perspectives on the natural gradient method. arXiv preprint, 2014. (pp. 5, 6). Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In ICML, 2015. (p. 18). Muandet, K., Balduzzi, D., and Sch\u00f6lkopf, B. Domain generalization via invariant feature representation. In ICML, 2013. (pp. 1, 3). Nam, H., Lee, H., Park, J., Yoon, W., and Yoo, D. Reducing domain gap by reducing style bias. In CVPR, 2021. (p. 20). Nam, J., Cha, H., Ahn, S., Lee, J., and Shin, J. Learning from failure: De-biasing classifier from biased classifier. In NeurIPS, 2020. (pp. 8, 21). Parascandolo, G., Neitz, A., Orvieto, A., Gresele, L., and Sch\u00f6lkopf, B. Learning explanations that are hard to vary. In ICLR, 2021. (pp. 2, 3, 4, 5, 6, 20, 24). Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. (p. 8). Pearl, J. Causality. Cambridge university press, 2009. (p. 3).Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. Moment matching for multi-source domain adaptation.InICCV, 2019. (pp. 7, 20). Peters, J., B\u00fchlmann, P., and Meinshausen, N. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, 2016. (pp. 1, 3). Pezeshki, M., Kaba, S.-O., Bengio, Y., Courville, A., Precup, D., and Lajoie, G. Gradient starvation: A learning proclivity in neural networks. In NeurIPS, 2021. (pp. 8, 16). Rame, A., Kirchmeyer, M., Rahier, T., Rakotomamonjy, A., Gallinari, P., and Cord, M. Diverse weight averaging for out-of-distribution generalization. arXiv preprint, 2022. (p. 20). Roberts, M., Driggs, D., Thorpe, M., Gilbey, J., Yeung, M., Ursprung, S., Aviles-Rivero, A. I., Etmann, C., McCague, C., Beer, L., et al.\n\n4 .\n4Appendix D enriches the DomainBed experiments. After a description of the benchmark protocols in Appendix D.1, Appendix D.2 discusses the model selection strategy. Then Appendix D.3 provides additional experiments to analyze key components of Fishr. Specifically, D.3.1 analyzes the exponential moving average; D.3.2 compares gradient mean versus gradient variance matching and also motivates ignoring the gradients in the features extractor; D.3.3 discusses the methodology to select hyperparameter distributions. Finally, Appendix D.4 provides the per-dataset results.\n\n13 )\n13As the Hessians are positive, H (A, B) > 0. We now need to split the analysis based on the sign of R(A, B).\n\nFigure 4 :(\n4Inconsistency I (A, B) between domains A and B, decomposed into R(A, B) depending on domain-level risks and H (A, B) depending on domain-level curvatures at \u03b8 * . Case R(A, B) \u2265 0 Both R(A, B) and H (A, B) are non-negative. Removing the absolute value from the RHS of Eq. 13 gives: I (A, B) = R(A, B) + H (A, B). Taking the maximum over (A, B) \u2208 E 2 where R(A, B) R(A, B) +H (A, B)) .\n\nThen\nEq. 11 gives H (A, B) < \u2212R(A, B). Thus the number inside the absolute value from the RHS of Eq. 13 is negative. This leads to: I (A, B) = \u2212R(A, B) \u2212 H (A, B) < \u2212R(A, B) = R(B, A) < I (B, A). Thus the max over E 2 of function (A, B) \u2192 I (A, B) can not be achieved for (A, B) with R(A, B) < 0. We obtain: max (A,B)\u2208E 2 I (A, B) = max (A,B)\u2208E 2 |R(A,B)\u22650 I (A, B) (16) Similarly, R(A, B) + H (A, B) \u2264 0 < R(B, A) + H (B, A). Thus the max over E 2 of function (A, B) \u2192 (R(A, B) + H (A, B)) can not be achieved for (A, B) with R(A, B) < 0. We obtain: max (A,B)\u2208E 2 (R(A, B) + H (A, B)) = max (A,B)\u2208E 2 |R(A,B)\u22650 (R(A, B) + H (A, B))\n\nFig. 5\n5illustrates the dynamics for Fishr \u03b8 : following the scheduling previously described in Appendix C.1, \u03bb jumping to a high value at epoch 190 activates the regularization. After this epoch, the domain-level Hessians are not only close in Frobenius distance, but also have similar norms and directions. On the contrary, when using only ERM inFig. 6, the distance between domain-level Hessians keeps increasing with the number of epochs. As a side note, flatter loss landscapes in ERM -as reflected by the Hessian norms in orange -do not correlate with improved generalization(Dinh et al., 2017).\n\nFigure 5 :\n5Hessian dynamics onColored MNIST with Fishr: at epoch 190, Hessians are matched across domains (purple). More precisely, they take similar directions -high cosine similarity (red) -and similar norms (blue). The Hessians' norms (orange) remain quite high thus the loss landscapes are rather sharp.\n\nFigure 6 :\n6Hessian dynamics on Colored MNIST with ERM: \u03bb = 0 along training. The Frobenius distance between domain-level Hessians (purple) keeps increasing: so does the distance between their norms (blue). Their cosine similarity (red) steadily decreases. The loss landscapes are flat at convergence (low Hessian norms in orange).\n\nFigure 7 :\n7Colored MNIST dynamics with ERM.\n\n\u2022\nERM: Empirical Risk Minimization (Vapnik, 1999) \u2022 IRM: Invariant Risk Minimization (Arjovsky et al., 2019) \u2022 GroupDRO: Group Distributionally Robust Optimization (Sagawa et al., 2020a) \u2022 Mixup: Interdomain Mixup (Yan et al., 2020) \u2022 MLDG: Meta Learning Domain Generalization (Li et al., 2018a) \u2022 CORAL: Deep CORAL (Sun & Saenko, 2016) \u2022 MMD: Maximum Mean Discrepancy (Li et al., 2018b) \u2022 DANN: Domain Adversarial Neural Network (Ganin et al., 2016) \u2022 CDANN: Conditional Domain Adversarial Neural Network (Li et al., 2018c) \u2022 MTL: Marginal Transfer Learning (Blanchard et al., 2021) -mask: Learning Explanations that are Hard to Vary (Parascandolo et al., 2021) \u2022 SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization (Shahtalebi et al., 2021) \u2022 IGA: Out-of-distribution generalization with maximal invariant predictor (Koyama & Yamaguchi, 2020) \u2022 Fish: Gradient Matching for Domain Generalization (Shi et al., 2021)\n\n1 .\n1Colored MNIST (Arjovsky et al., 2019)  is a variant of the MNIST handwritten digit classification dataset(LeCun  et al., 2010). As described previously in Appendix C.1, domain d \u2208 {90%, 80%, 10%} contains a disjoint set of digits colored: the correlation strengths between color and label vary across domains. The dataset contains 70,000 examples of dimension (2, 28, 28) and 2 classes. Most importantly, the network, the hyperparameters, the image shapes, etc. are not the same as in the IRM setup from Section 4.1.2. Rotated MNIST (Ghifary et al., 2015) is a variant of MNIST where domain d \u2208 {0, 15, 30, 45, 60, 75} contains digits rotated by d degrees, with 70,000 examples of dimension (1, 28, 28) and 10 classes. 3. VLCS (Fang et al., 2013) includes photographic domains d \u2208 {Caltech101, LabelMe, SUN09, VOC2007}, with 10,729 examples of dimension (3, 224, 224) and 5 classes. 4. PACS (Li et al., 2017) includes domains d \u2208 {art, cartoons, photos, sketches}, with 9,991 examples of dimension (3, 224, 224) and 7 classes. 5. OfficeHome (Venkateswara et al., 2017) includes domains d \u2208 {art, clipart, product, real}, with 15,588 examples of dimension (3, 224, 224) and 65 classes. 6. TerraIncognita (Beery et al., 2018) contains photographs of wild animals taken by camera traps at locations d \u2208 {L100, L38, L43, L46}, with 24,788 examples of dimension (3, 224, 224) and 10 classes. 7. DomainNet (Peng et al., 2019) has six domains d \u2208 {clipart, infograph, painting, quickdraw, real, sketch}, with 586,575 examples of size (3, 224, 224) and 345 classes.\n\n\n.0 \u00b1 0.5 98.9 \u00b1 0.1 99.0 \u00b1 0.0 99.0 \u00b1 0.\n\nTable 1 :\n1Cosine similarity between Hessian diagonals and gradient variances cos (Diag (H e ) , Var(G e )), for an ERM at convergence on Colored MNIST with the two training domains e \u2208 {90%, 80%}.The Hessian and the 'true' Fisher Information Matrix (FIM). The 'true'e = 90% \ne = 80% \n\nOn classifier weights w \n0.9999980 0.9999905 \nOn all network weights \u03b8 0.9971040 0.9962264 \n\n\n\nTable 2 :\n2Invariance analysis at convergence on Colored MNIST across the two training domains E = {90%, 80%}.\n\nTable 2\n2empirically confirms that matching {Diag(C e )} e\u2208E -i.e., {Var(G e )} e\u2208E -with Fishr forces the domain-level Hes-sians {Diag(H e )} e\u2208E to be aligned at convergence (on the diagonal for computational reasons). Tackling the second moment of the first-order derivatives enables to regularize the second-order derivatives. Moreover, Appendix C.2.4 shows that matching the diagonals of {C e } e\u2208E or {F e } e\u2208E -i.e., centering or not the variances -perform similarly.\n\nTable 3 :\n3Colored MNIST results. All methods use hyperparameters optimized for IRM.Method Train acc. \nTest acc. \nGray test acc. \n\nERM \n86.4 \u00b1 0.2 14.0 \u00b1 0.7 \n71.0 \u00b1 0.7 \nIRM \n71.0 \u00b1 0.5 65.6 \u00b1 1.8 \n66.1 \u00b1 0.2 \nV-REx 71.7 \u00b1 1.5 67.2 \u00b1 1.5 \n68.6 \u00b1 2.2 \n\nFishr \u03b8 \n69.6 \u00b1 0.9 71.2 \u00b1 1.1 \n70.2 \u00b1 0.7 \nFishr \u03c9 \n71.0 \u00b1 0.9 69.5 \u00b1 1.0 \n70.2 \u00b1 1.1 \nFishr \u03c6 \n65.6 \u00b1 1.3 73.8 \u00b1 1.0 \n70.0 \u00b1 0.9 \n\n\nTable 4 :\n4DomainBed benchmark. We format first, second and worse than ERM results.Algorithm \n\nAccuracy (\u2191) \nRanking (\u2193) \n\nCMNIST RMNIST \nVLCS \nPACS \nOfficeHome TerraInc DomainNet Avg \nArith. \nmean \n\nGeom. \nmean \nMedian \n\nERM \n57.8 \u00b1 0.2 \n97.8 \u00b1 0.1 77.6 \u00b1 0.3 86.7 \u00b1 0.3 \n66.4 \u00b1 0.5 \n53.0 \u00b1 0.3 \n41.3 \u00b1 0.1 \n68.7 \n9.1 \n8.1 \n8 \nIRM \n67.7 \u00b1 1.2 \n97.5 \u00b1 0.2 76.9 \u00b1 0.6 84.5 \u00b1 1.1 \n63.0 \u00b1 2.7 \n50.5 \u00b1 0.7 \n28.0 \u00b1 5.1 \n66.9 \n14.7 \n12.4 \n16 \nGroupDRO \n61.1 \u00b1 0.9 \n97.9 \u00b1 0.1 77.4 \u00b1 0.5 87.1 \u00b1 0.1 \n66.2 \u00b1 0.6 \n52.4 \u00b1 0.1 \n33.4 \u00b1 0.3 \n67.9 \n8.6 \n7.5 \n8 \nMixup \n58.4 \u00b1 0.2 \n98.0 \u00b1 0.1 78.1 \u00b1 0.3 86.8 \u00b1 0.3 \n68.0 \u00b1 0.2 \n54.4 \u00b1 0.3 \n39.6 \u00b1 0.1 \n69.0 \n5.3 \n3.9 \n4 \nMLDG \n58.2 \u00b1 0.4 \n97.8 \u00b1 0.1 77.5 \u00b1 0.1 86.8 \u00b1 0.4 \n66.6 \u00b1 0.3 \n52.0 \u00b1 0.1 \n41.6 \u00b1 0.1 \n68.7 \n9.1 \n8.2 \n9 \nCORAL \n58.6 \u00b1 0.5 \n98.0 \u00b1 0.0 77.7 \u00b1 0.2 87.1 \u00b1 0.5 \n68.4 \u00b1 0.2 \n52.8 \u00b1 0.2 \n41.8 \u00b1 0.1 \n69.2 \n4.6 \n3.4 \n3 \nMMD \n63.3 \u00b1 1.3 \n98.0 \u00b1 0.1 77.9 \u00b1 0.1 87.2 \u00b1 0.1 \n66.2 \u00b1 0.3 \n52.0 \u00b1 0.4 \n23.5 \u00b1 9.4 \n66.9 \n7.0 \n4.9 \n6 \nDANN \n57.0 \u00b1 1.0 \n97.9 \u00b1 0.1 79.7 \u00b1 0.5 85.2 \u00b1 0.2 \n65.3 \u00b1 0.8 \n50.6 \u00b1 0.4 \n38.3 \u00b1 0.1 \n67.7 \n11.9 \n9.6 \n15 \nCDANN \n59.5 \u00b1 2.0 \n97.9 \u00b1 0.0 79.9 \u00b1 0.2 85.8 \u00b1 0.8 \n65.3 \u00b1 0.5 \n50.8 \u00b1 0.6 \n38.5 \u00b1 0.2 \n68.2 \n9.6 \n7.4 \n10 \nMTL \n57.6 \u00b1 0.3 \n97.9 \u00b1 0.1 77.7 \u00b1 0.5 86.7 \u00b1 0.2 \n66.5 \u00b1 0.4 \n52.2 \u00b1 0.4 \n40.8 \u00b1 0.1 \n68.5 \n8.4 \n7.8 \n7 \nSagNet \n58.2 \u00b1 0.3 \n97.9 \u00b1 0.0 77.6 \u00b1 0.1 86.4 \u00b1 0.4 \n67.5 \u00b1 0.2 \n52.5 \u00b1 0.4 \n40.8 \u00b1 0.2 \n68.7 \n8.0 \n7.2 \n6 \nARM \n63.2 \u00b1 0.7 \n98.1 \u00b1 0.1 77.8 \u00b1 0.3 85.8 \u00b1 0.2 \n64.8 \u00b1 0.4 \n51.2 \u00b1 0.5 \n36.0 \u00b1 0.2 \n68.1 \n9.9 \n7.5 \n12 \nV-REx \n67.0 \u00b1 1.3 \n97.9 \u00b1 0.1 78.1 \u00b1 0.2 87.2 \u00b1 0.6 \n65.7 \u00b1 0.3 \n51.4 \u00b1 0.5 \n30.1 \u00b1 3.7 \n68.2 \n7.7 \n5.5 \n5 \nRSC \n58.5 \u00b1 0.5 \n97.6 \u00b1 0.1 77.8 \u00b1 0.6 86.2 \u00b1 0.5 \n66.5 \u00b1 0.6 \n52.1 \u00b1 0.2 \n38.9 \u00b1 0.6 \n68.2 \n9.9 \n9.4 \n9 \nAND-mask \n58.6 \u00b1 0.4 \n97.5 \u00b1 0.0 76.4 \u00b1 0.4 86.4 \u00b1 0.4 \n66.1 \u00b1 0.2 \n49.8 \u00b1 0.4 \n37.9 \u00b1 0.6 \n67.5 \n13.4 \n13.1 \n12 \nSAND-mask 62.3 \u00b1 1.0 \n97.4 \u00b1 0.1 76.2 \u00b1 0.5 85.9 \u00b1 0.4 \n65.9 \u00b1 0.5 \n50.2 \u00b1 0.1 \n32.2 \u00b1 0.6 \n67.2 \n14.3 \n13.5 \n15 \nFish \n61.8 \u00b1 0.8 \n97.9 \u00b1 0.1 77.8 \u00b1 0.6 85.8 \u00b1 0.6 \n66.0 \u00b1 2.9 \n50.8 \u00b1 0.4 \n43.4 \u00b1 0.3 \n69.1 \n8.4 \n6.6 \n7 \n\nFishr \n68.8 \u00b1 1.4 \n97.8 \u00b1 0.1 78.2 \u00b1 0.2 86.9 \u00b1 0.2 \n68.2 \u00b1 0.2 \n53.6 \u00b1 0.4 \n41.8 \u00b1 0.2 \n70.8 \n3.9 \n2.8 \n2 \n\n\n\nTable 4\n4summarizes the results on DomainBed using the 'Test-domain' model selection: the validation set (to select the best hyperparameters) follows the same distribution as the test domain. Appendix D.2 reports results with the 'Training-domain' model selection while results are detailed per dataset in Appendix D.4.\n\n\nNature Machine Intelligence, 2021. (p. 1). Deutsch, D. The beginning of infinity: Explanations that transform the world. Penguin UK, 2011. (p. 4). Ding, Z. and Fu, Y. Deep domain generalization with structured low-rank constraint. In TIP, 2017. (p. 2). Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. In ICML, 2017. (p. 17). Faghri, F., Duvenaud, D., Fleet, D. J., and Ba, J. A study of gradient variance in deep learning. arXiv preprint, 2020. (p. 6). Fang, C., Xu, Y., and Rockmore, D. N. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. (pp. 7, 20). Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In ICML, 2017. (p. 3). Fisher, R. A. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London., 1922. (pp. 2, 5). Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. Sharpness-aware minimization for efficiently improving generalization. In ICLR, 2021. (p. 5). Fort, S., Nowak, P. K., Jastrzebski, S., and Narayanan, S. Stiffness: A new perspective on generalization in neural networks. arXiv preprint, 2019. (p. 2). Frantar, E., Kurtic, E., and Alistarh, D. Efficient matrixfree approximations of second-order information, with applications to pruning and optimization. arXiv preprint, 2021. (p. 5). Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. JMLR, 2016. (pp. 1, 2, 8, 19). Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2020. (p. 1). Ghifary, M., Kleijn, W. B., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multitask autoencoders. In ICCV, 2015. (pp. 7, 16, 20). Ghorbani, B., Krishnan, S., and Xiao, Y. An investigation into neural net optimization via hessian eigenvalue density. In ICML, 2019. (p. 4). Gong, M., Zhang, K., Liu, T., Tao, D., Glymour, C., and Sch\u00f6lkopf, B. Domain adaptation with conditional transferable components. In ICML, 2016. (p. 2). Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient descent happens in a tiny subspace. arXiv preprint, 2018. (p. 16). Heskes, T. On \"natural\" learning and pruning in multilayered perceptrons. Neural Computation, 2000. (p. 18). Huang, Z., Wang, H., Xing, E. P., and Huang, D. Selfchallenging improves cross-domain generalization. In ECCV, 2020. (p. 20). Idnani, D. and Kao, J. C. Learning robust representations with score invariant learning. In ICML UDL Workshop, 2020. (p. 3).Du, Y., Czarnecki, W. M., Jayakumar, S. M., Farajtabar, \nM., Pascanu, R., and Lakshminarayanan, B. Adapting \nauxiliary losses using gradient similarity. arXiv preprint, \n2018. (p. 3). \n\nGulrajani, I. and Lopez-Paz, D. In search of lost domain \ngeneralization. In ICLR, 2021. (pp. 1, 2, 3, 6, 7, 19). \n\nGuo, R., Zhang, P., Liu, H., and Kiciman, E. Out-of-\ndistribution prediction with invariant risk minimization: \nThe limitation and an effective fix. arXiv preprint, 2021. \n(p. 3). \n\n\n\n) .\n)LeCun, Y., Cortes, C., and Burges, C. Mnist handwritten digit database, 2010. (pp. 16, 20). LeCun, Y., Bottou, L., Orr, G. B., and M\u00fcller, K.-R. Efficient backprop. In Neural Networks. 2012. (p. 6). Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In ICCV, 2017. (pp. 7, 20). Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T. Learning to generalize: Meta-learning for domain generalization. In AAAI, 2018a. (p. 19).Li, H., Pan, S. J., Wang, S., and Kot, A. C. Domain generalization with adversarial feature learning. InCVPR,  2018b. (pp. 2, 19).\n\n1 .\n1Having similar top eigenvectors across {K e } e\u2208E would delete detrimental domain-dependent shortcuts and favor the learning of a common mechanism. Indeed, truly informative features should remain consistent across domains. 2. Having similar top eigenvalues across {K e } e\u2208E would improve the optimization schema for simultaneous training at the same speed. Indeed, it would facilitate the finding of a learning rate for simultaneous convergence on all domains. It's worth noting that if we quickly overfit on a first domain using spurious explanations, invariances will then be hard to learn due to the gradient starvation phenomena (Pezeshki et al., 2021). Directly matching K e would require assuming that each domain coincides and contains the same samples; for example, with different pose angles (Ghifary et al., 2015). To avoid such a strong assumption, we leverage the fact that the 'true' Fisher Information Matrix F and the NTK K share the same non-zero eigenvalues since F is dual to K (see Appendix C.1 in Maddox et al. (2019), notably for classification tasks). Moreover, their eigenvectors are strongly related (see Appendix C in Kopitkov & Indelman (2019)). Thus, having similar {F e } e\u2208E encourages {K e } e\u2208E to have similar spectral decomposition. Based on the close relations between C and F (see Section 3.2.3), this further motivates the need to match gradient variances during the SGD trajectory -and not only at convergence as in Section 3.2.\n\n\nby the large values (3.3) for weights associated to f 2 and f 3 ; this induces low test accuracy (57%). On the contrary, Fishr forces the linear model to rely mostly on the invariant feature f 1 , as indicated by the lower values (1.2) for weights associated to f 2 and f 3 ; in accuracy, Fishr performs similarly in test and train (93%).Method \nMatched statistics \nTrain acc. Test acc. \nW \nb \n\nERM \nN/A \n97 % \n57 % \n[2.8,3.3,3.3,0.0] -2.7 \nFish \nGradient means \n93 % \n93 % \n[0.4,0.2,0.2,0.0] -0.4 \nFishr \nCentered gradient variances \n93 % \n93 % \n[2.0,1.2,1.2,0.0] -0.6 \nFishr \nUncentered gradient variances \n93 % \n93 % \n[1.9,0.9,0.9,0.0] -0.6 \n\n\n\nTable 5 :\n5Performances comparison on the linear dataset from(Shi et al., 2021)    Second, digits are either colored red or green, with a strong correlation between label and color in training. However, this correlation is reversed at test time. Specifically, in training, the model has access to two domains E = {90%, 80%}: in the first domain, green digits have a 90% chance of being in 5-9; in the second, this chance goes down to 80%. In test, green digits have a 10% chance of being in 5-9. Due to this modification in correlation, a model should ideally ignore the color information and only rely on the digits' shape: this would obtain a 75% test accuracy.C. Colored MNIST in the IRM Setup \n\nC.1. Description of the Colored MNIST experiment \n\nColored MNIST is a binary digit classification dataset introduced in IRM (Arjovsky et al., 2019). Compared to the traditional \nMNIST (LeCun et al., 2010), it has 2 main differences. First, 0-4 and 5-9 digits are each collapsed into a single class, with a \n25% chance of label flipping. \n\nTable 6 :\n6Colored MNIST experiments without label flipping.Method Train acc. \nTest acc. \nGray test acc. \n\nERM \n99.0 \u00b1 0.0 91.8 \u00b1 0.2 \n95.0 \u00b1 0.4 \nIRM \n96.4 \u00b1 0.2 82.2 \u00b1 0.1 \n92.6 \u00b1 0.2 \nV-REx 97.1 \u00b1 0.2 95.3 \u00b1 0.4 \n94.1 \u00b1 0.4 \n\nFishr \u03b8 \n97.9 \u00b1 0.2 93.6 \u00b1 0.4 \n94.8 \u00b1 0.4 \nFishr \u03c9 \n97.0 \u00b1 0.2 95.3 \u00b1 0.4 \n94.1 \u00b1 0.4 \nFishr \u03c6 \n97.9 \u00b1 0.1 93.5 \u00b1 0.3 \n94.8 \u00b1 0.4 \n\n\n\nTable 7 :\n7Colored MNIST experiments with different statistics matched. All hyperparameters were optimized for IRM.Method \n25% label flipping \nNo label flipping \n\nGradients in \nName \nMatched statistics Train acc. \nTest acc. \nGray test acc. \nTrain acc. \nTest acc. \nGray test acc. \n\n\u03c9 \n\nCentered variance (= Fishr\u03c9) \nVar(Ge) \n71.0 \u00b1 0.9 69.5 \u00b1 1.0 \n70.2 \u00b1 1.1 \n97.0 \u00b1 0.2 \n95.3 \u00b1 0.4 \n94.1 \u00b1 0.4 \nCentered covariance \nCe \n70.7 \u00b1 1.0 69.1 \u00b1 1.1 \n69.9 \u00b1 1.1 \n97.0 \u00b1 0.2 \n95.3 \u00b1 0.4 \n94.0 \u00b1 0.4 \nUncentered variance \nDiag( 1 \n\nneF e) \n\n71.3 \u00b1 0.9 69.5 \u00b1 1.0 \n70.3 \u00b1 1.0 \n97.0 \u00b1 0.2 \n95.3 \u00b1 0.4 \n94.1 \u00b1 0.4 \n\n\u03b8 \n\nCentered variance (= Fishr\u03b8) \nVar(Ge) \n69.6 \u00b1 0.9 71.2 \u00b1 1.1 \n70.2 \u00b1 0.7 \n97.9 \u00b1 0.1 \n93.5 \u00b1 0.3 \n94.7 \u00b1 0.4 \nCentered covariance \nCe \nNot \npossible \nfor \ncomputational (memory) \nreasons \nUncentered variance \nDiag( 1 \n\nneF e) \n\n71.0 \u00b1 0.8 70.0 \u00b1 1.1 \n70.1 \u00b1 0.9 \n97.9 \u00b1 0.0 \n93.5 \u00b1 0.3 \n94.8 \u00b1 0.4 \n\n\u03c6 \n\nCentered variance (= Fishr\u03c6) \nVar(Ge) \n65.6 \u00b1 1.3 73.8 \u00b1 1.0 \n70.0 \u00b1 0.9 \n97.9 \u00b1 0.1 \n93.5 \u00b1 0.3 \n94.8 \u00b1 0.4 \nCentered covariance \nCe \nNot \npossible \nfor \ncomputational (memory) \nreasons \nUncentered variance \nDiag( 1 \n\nneF e) \n\n71.5 \u00b1 0.8 69.1 \u00b1 1.1 \n70.0 \u00b1 1.0 \n97.9 \u00b1 0.1 \n93.5 \u00b1 0.3 \n94.8 \u00b1 0.4 \n\nC.2.4. CENTERED OR UNCENTERED VARIANCE ? \n\n\n\nTable 8 :\n8Hyperparameters, their default values and distributions for random search.Condition \nParameter \nDefault value Random distribution \n\nVLCS / PACS / \nlearning rate \n0.00005 \n10 Uniform(\u22125,\u22123.5) \nOfficeHome / \nbatch size \n32 \n2 Uniform(3,5.5) if not DomainNet else 2 Uniform(3,5) \nTerraIncognita / \nweight decay \n0 \n10 Uniform(\u22126,\u22122) \nDomainNet \ndropout \n0 \nRandomChoice ([0, 0.1, 0.5]) \n\nRotated MNIST / learning rate \n0.001 \n10 Uniform(\u22124.5,\u22123.5) \nColored MNIST \nbatch size \n64 \n2 Uniform(3,9) \nweight decay \n0 \n0 \n\nAll \nsteps \n5000 \n5000 \n\nFishr \n\nregularization strength \u03bb 1000 \n10 Uniform(1,4) \nema \u03b3 \n0.95 \nUniform(0.9, 0.99) \nwarmup iterations \n1500 \nUniform(0, 5000) \n\n\n\nTable 9 :\n9DomainBed with 'Training-domain' model selection. We format first, second and worse than ERM results.Algorithm \n\nAccuracy (\u2191) \nRanking (\u2193) \n\nCMNIST RMNIST \nVLCS \nPACS \nOfficeHome TerraInc DomainNet Avg \nArith. \nmean \n\nGeom. \nmean \nMedian \n\nERM \n51.5 \u00b1 0.1 \n98.0 \u00b1 0.0 77.5 \u00b1 0.4 85.5 \u00b1 0.2 \n66.5 \u00b1 0.3 \n46.1 \u00b1 1.8 \n40.9 \u00b1 0.1 \n66.6 \n7.0 \n5.9 \n7 \nIRM \n52.0 \u00b1 0.1 \n97.7 \u00b1 0.1 78.5 \u00b1 0.5 83.5 \u00b1 0.8 \n64.3 \u00b1 2.2 \n47.6 \u00b1 0.8 \n33.9 \u00b1 2.8 \n65.4 \n10.7 \n8.5 \n14 \nGroupDRO \n52.1 \u00b1 0.0 \n98.0 \u00b1 0.0 76.7 \u00b1 0.6 84.4 \u00b1 0.8 \n66.0 \u00b1 0.7 \n43.2 \u00b1 1.1 \n33.3 \u00b1 0.2 \n64.8 \n11.3 \n8.4 \n14 \nMixup \n52.1 \u00b1 0.2 \n98.0 \u00b1 0.1 77.4 \u00b1 0.6 84.6 \u00b1 0.6 \n68.1 \u00b1 0.3 \n47.9 \u00b1 0.8 \n39.2 \u00b1 0.1 \n66.7 \n5.7 \n4.2 \n3 \nMLDG \n51.5 \u00b1 0.1 \n97.9 \u00b1 0.0 77.2 \u00b1 0.4 84.9 \u00b1 1.0 \n66.8 \u00b1 0.6 \n47.7 \u00b1 0.9 \n41.2 \u00b1 0.1 \n66.7 \n8.0 \n7.0 \n8 \nCORAL \n51.5 \u00b1 0.1 \n98.0 \u00b1 0.1 78.8 \u00b1 0.6 86.2 \u00b1 0.3 \n68.7 \u00b1 0.3 \n47.6 \u00b1 1.0 \n41.5 \u00b1 0.1 \n67.5 \n3.6 \n2.5 \n2 \nMMD \n51.5 \u00b1 0.2 \n97.9 \u00b1 0.0 77.5 \u00b1 0.9 84.6 \u00b1 0.5 \n66.3 \u00b1 0.1 \n42.2 \u00b1 1.6 \n23.4 \u00b1 9.5 \n63.3 \n12.3 \n11.8 \n10 \nDANN \n51.5 \u00b1 0.3 \n97.8 \u00b1 0.1 78.6 \u00b1 0.4 83.6 \u00b1 0.4 \n65.9 \u00b1 0.6 \n46.7 \u00b1 0.5 \n38.3 \u00b1 0.1 \n66.1 \n10.3 \n8.8 \n12 \nCDANN \n51.7 \u00b1 0.1 \n97.9 \u00b1 0.1 77.5 \u00b1 0.1 82.6 \u00b1 0.9 \n65.8 \u00b1 1.3 \n45.8 \u00b1 1.6 \n38.3 \u00b1 0.3 \n65.6 \n11.1 \n10.7 \n10 \nMTL \n51.4 \u00b1 0.1 \n97.9 \u00b1 0.0 77.2 \u00b1 0.4 84.6 \u00b1 0.5 \n66.4 \u00b1 0.5 \n45.6 \u00b1 1.2 \n40.6 \u00b1 0.1 \n66.2 \n10.9 \n10.2 \n10 \nSagNet \n51.7 \u00b1 0.0 \n98.0 \u00b1 0.0 77.8 \u00b1 0.5 86.3 \u00b1 0.2 \n68.1 \u00b1 0.1 \n48.6 \u00b1 1.0 \n40.3 \u00b1 0.1 \n67.2 \n4.0 \n3.0 \n3 \nARM \n56.2 \u00b1 0.2 \n98.2 \u00b1 0.1 77.6 \u00b1 0.3 85.1 \u00b1 0.4 \n64.8 \u00b1 0.3 \n45.5 \u00b1 0.3 \n35.5 \u00b1 0.2 \n66.1 \n8.7 \n5.6 \n9 \nV-REx \n51.8 \u00b1 0.1 \n97.9 \u00b1 0.1 78.3 \u00b1 0.2 84.9 \u00b1 0.6 \n66.4 \u00b1 0.6 \n46.4 \u00b1 0.6 \n33.6 \u00b1 2.9 \n65.6 \n8.3 \n7.7 \n8 \nRSC \n51.7 \u00b1 0.2 \n97.6 \u00b1 0.1 77.1 \u00b1 0.5 85.2 \u00b1 0.9 \n65.5 \u00b1 0.9 \n46.6 \u00b1 1.0 \n38.9 \u00b1 0.5 \n66.1 \n11.4 \n10.6 \n9 \nAND-mask \n51.3 \u00b1 0.2 \n97.6 \u00b1 0.1 78.1 \u00b1 0.9 84.4 \u00b1 0.9 \n65.6 \u00b1 0.4 \n44.6 \u00b1 0.3 \n37.2 \u00b1 0.6 \n65.5 \n13.6 \n12.7 \n15 \nSAND-mask 51.8 \u00b1 0.2 \n97.4 \u00b1 0.1 77.4 \u00b1 0.2 84.6 \u00b1 0.9 \n65.8 \u00b1 0.4 \n42.9 \u00b1 1.7 \n32.1 \u00b1 0.6 \n64.6 \n13.4 \n12.7 \n13 \nFish \n51.6 \u00b1 0.1 \n98.0 \u00b1 0.0 77.8 \u00b1 0.3 85.5 \u00b1 0.3 \n68.6 \u00b1 0.4 \n45.1 \u00b1 1.3 \n42.7 \u00b1 0.2 \n67.1 \n5.6 \n3.8 \n3 \n\nFishr \n52.0 \u00b1 0.2 \n97.8 \u00b1 0.0 77.8 \u00b1 0.1 85.5 \u00b1 0.4 \n67.8 \u00b1 0.1 \n47.4 \u00b1 1.6 \n41.7 \u00b1 0.0 \n67.1 \n5.6 \n4.8 \n5 \n\n\n\nTable 10 :\n10Importance of the exponential moving average (ema) on DomainBed's Colored MNIST.Model selection Algorithm ema \n+90% \n+80% \n10% \nAvg \n\nTest-domain \n\nERM \nN/A 71.8 \u00b1 0.4 72.9 \u00b1 0.1 28.7 \u00b1 0.5 57.8 \n\nV-REx \n\n72.8 \u00b1 0.3 73.0 \u00b1 0.3 55.2 \u00b1 4.0 67.0 \n\n73.0 \u00b1 0.2 73.0 \u00b1 0.3 59.9 \u00b1 2.6 68.6 \n\nFishr \n\n72.7 \u00b1 0.3 72.8 \u00b1 0.1 34.0 \u00b1 4.5 59.8 \n\n74.1 \u00b1 0.6 73.3 \u00b1 0.1 58.9 \u00b1 3.7 68.8 \n\nTraining-domain \n\nERM \nN/A 71.7 \u00b1 0.1 72.9 \u00b1 0.2 10.0 \u00b1 0.1 51.5 \n\nV-REx \n\n72.4 \u00b1 0.3 72.9 \u00b1 0.4 10.2 \u00b1 0.0 51.8 \n\n72.6 \u00b1 0.5 73.3 \u00b1 0.1 9.8 \u00b1 0.1 51.9 \n\nFishr \n\n71.1 \u00b1 0.6 73.6 \u00b1 0.1 10.1 \u00b1 0.2 51.6 \n\n72.3 \u00b1 0.9 73.5 \u00b1 0.2 10.1 \u00b1 0.2 52.0 \n\n\n\nTable 11 :\n11Importance of the exponential moving average (ema) on DomainBed's OfficeHome.Model selection Algorithm ema \nA \nC \nP \nR \nAvg \n\nTest-domain \n\nERM \nN/A 61.7 \u00b1 0.7 53.4 \u00b1 0.3 74.1 \u00b1 0.4 76.2 \u00b1 0.6 66.4 \n\nV-REx \n\n59.6 \u00b1 1.0 53.3 \u00b1 0.3 73.2 \u00b1 0.5 76.6 \u00b1 0.4 65.7 \n\n59.0 \u00b1 0.7 52.8 \u00b1 0.8 74.6 \u00b1 0.4 75.5 \u00b1 0.3 65.5 \n\nFishr \n\n63.6 \u00b1 0.4 53.2 \u00b1 0.5 75.4 \u00b1 0.5 77.8 \u00b1 0.3 67.5 \n\n63.4 \u00b1 0.8 54.2 \u00b1 0.3 76.4 \u00b1 0.3 78.5 \u00b1 0.2 68.2 \n\nTraining-domain \n\nERM \nN/A 61.3 \u00b1 0.7 52.4 \u00b1 0.3 75.8 \u00b1 0.1 76.6 \u00b1 0.3 66.5 \n\nV-REx \n\n60.7 \u00b1 0.9 53.0 \u00b1 0.9 75.3 \u00b1 0.1 76.6 \u00b1 0.5 66.4 \n\n59.2 \u00b1 1.0 51.7 \u00b1 0.5 75.2 \u00b1 0.2 76.6 \u00b1 0.3 65.7 \n\nFishr \n\n62.2 \u00b1 1.0 53.5 \u00b1 0.2 76.6 \u00b1 0.2 77.8 \u00b1 0.4 67.5 \n\n62.4 \u00b1 0.5 54.4 \u00b1 0.4 76.2 \u00b1 0.5 78.3 \u00b1 0.1 67.8 \n\nsynthetic Colored MNIST in Table 10, the ema is critical for Fishr -notably when training on E = {90%, 80%} and the \ndataset 10% is in test (from 34.0% to 58.9% in 'Test-domain'). V-REx also benefits from ema. On the 'real' dataset \nOfficeHome in \n\nTable 12 :\n12Fishr (gradient variance) vs. IGA (gradient mean) on DomainBed's Colored MNIST.Model selection Algorithm \nGradients in Warmup ema \n+90% \n+80% \n10% \nAvg \n\nTest-domain \n\nERM \nN/A \nN/A \nN/A 71.8 \u00b1 0.4 72.9 \u00b1 0.1 28.7 \u00b1 0.5 57.8 \n\nIGA \n\n\u03b8 = \u03c9 \u2295 \u03c6 \n\n\n71.8 \u00b1 0.5 73.0 \u00b1 0.3 29.2 \u00b1 0.5 58.0 \n\u03c9 \n\n\n72.4 \u00b1 0.1 73.3 \u00b1 0.2 29.3 \u00b1 0.6 58.3 \n\u03c9 \n\n\n72.5 \u00b1 0.2 73.3 \u00b1 0.1 31.8 \u00b1 0.7 59.2 \n\u03c9 \n\n\n72.6 \u00b1 0.3 72.9 \u00b1 0.2 50.0 \u00b1 1.2 65.2 \n\nFishr \n\u03c9 \n\n\n\n73.0 \u00b1 0.3 73.2 \u00b1 0.1 29.5 \u00b1 1.1 58.6 \n\n\n72.7 \u00b1 0.3 72.8 \u00b1 0.1 34.0 \u00b1 4.5 59.8 \n\n\n74.1 \u00b1 0.6 73.3 \u00b1 0.1 58.9 \u00b1 3.7 68.8 \n\nFishr + IGA \u03c9 \n\n\n73.3 \u00b1 0.0 72.6 \u00b1 0.5 66.3 \u00b1 2.9 70.7 \n\nTraining-domain \n\nERM \nN/A \nN/A \nN/A 71.7 \u00b1 0.1 72.9 \u00b1 0.2 10.0 \u00b1 0.1 51.5 \n\nIGA \n\n\u03b8 = \u03c9 \u2295 \u03c6 \n\n\n71.8 \u00b1 0.3 73.2 \u00b1 0.2 9.8 \u00b1 0.0 51.6 \n\u03c9 \n\n\n71.8 \u00b1 0.1 73.2 \u00b1 0.2 10.1 \u00b1 0.0 51.7 \n\u03c9 \n\n\n71.8 \u00b1 0.2 73.1 \u00b1 0.2 10.1 \u00b1 0.0 51.7 \n\u03c9 \n\n\n72.5 \u00b1 0.4 73.3 \u00b1 0.2 10.1 \u00b1 0.1 52.0 \n\nFishr \n\u03c9 \n\n\n\n71.6 \u00b1 0.1 73.2 \u00b1 0.1 9.9 \u00b1 0.0 51.6 \n\n\n71.1 \u00b1 0.6 73.6 \u00b1 0.1 10.1 \u00b1 0.2 51.6 \n\n\n72.3 \u00b1 0.9 73.5 \u00b1 0.2 10.1 \u00b1 0.2 52.0 \n\nFishr + IGA \u03c9 \n\n\n72.4 \u00b1 0.4 73.1 \u00b1 0.1 10.1 \u00b1 0.1 51.8 \n\n\n\nTable 13 :\n13Fishr (gradient variance) vs. IGA (gradient mean) on DomainBed's OfficeHome.Model selection Algorithm \nGradients in Warmup ema \nA \nC \nP \nR \nAvg \n\nTest-domain \n\nERM \nN/A \nN/A \nN/A 61.7 \u00b1 0.7 53.4 \u00b1 0.3 74.1 \u00b1 0.4 76.2 \u00b1 0.6 66.4 \n\nIGA \n\n\u03b8 = \u03c9 \u2295 \u03c6 \n\n\n50.1 \u00b1 2.5 49.6 \u00b1 1.6 59.5 \u00b1 6.7 68.5 \u00b1 1.2 56.9 \n\u03c9 \n\n\n62.3 \u00b1 0.3 53.9 \u00b1 0.2 75.2 \u00b1 0.4 77.4 \u00b1 0.1 67.2 \n\u03c9 \n\n\n61.9 \u00b1 0.4 52.6 \u00b1 0.6 76.0 \u00b1 0.8 77.5 \u00b1 0.3 67.0 \n\u03c9 \n\n\n62.3 \u00b1 1.0 53.4 \u00b1 0.3 76.0 \u00b1 0.7 77.0 \u00b1 0.1 67.2 \n\nFishr \n\u03c9 \n\n\n\n61.8 \u00b1 0.9 53.8 \u00b1 0.4 76.6 \u00b1 0.6 77.7 \u00b1 0.2 67.5 \n\n\n63.6 \u00b1 0.4 53.2 \u00b1 0.5 75.4 \u00b1 0.5 77.8 \u00b1 0.3 67.5 \n\n\n63.4 \u00b1 0.8 54.2 \u00b1 0.3 76.4 \u00b1 0.3 78.5 \u00b1 0.2 68.2 \n\nFishr + IGA \u03c9 \n\n\n63.6 \u00b1 1.0 54.6 \u00b1 0.5 76.6 \u00b1 0.2 78.4 \u00b1 0.4 68.3 \n\nTraining-domain \n\nERM \nN/A \nN/A \nN/A 61.3 \u00b1 0.7 52.4 \u00b1 0.3 75.8 \u00b1 0.1 76.6 \u00b1 0.3 66.5 \n\nIGA \n\n\u03b8 = \u03c9 \u2295 \u03c6 \n\n\n51.7 \u00b1 1.3 49.3 \u00b1 1.5 58.6 \u00b1 7.1 69.0 \u00b1 1.1 57.1 \n\u03c9 \n\n\n61.9 \u00b1 0.0 53.6 \u00b1 0.9 75.7 \u00b1 0.5 76.0 \u00b1 0.1 66.8 \n\u03c9 \n\n\n61.2 \u00b1 0.1 52.2 \u00b1 0.5 76.1 \u00b1 0.2 77.2 \u00b1 0.3 66.7 \n\u03c9 \n\n\n61.7 \u00b1 0.5 52.4 \u00b1 0.7 75.9 \u00b1 0.4 77.1 \u00b1 0.2 66.8 \n\nFishr \n\u03c9 \n\n\n\n63.8 \u00b1 0.6 52.5 \u00b1 0.5 76.7 \u00b1 0.6 77.1 \u00b1 1.0 67.5 \n\n\n62.2 \u00b1 1.0 53.5 \u00b1 0.2 76.6 \u00b1 0.2 77.8 \u00b1 0.4 67.5 \n\n\n62.4 \u00b1 0.5 54.4 \u00b1 0.4 76.2 \u00b1 0.5 78.3 \u00b1 0.1 67.8 \n\nFishr + IGA \u03c9 \n\n\n63.3 \u00b1 1.0 54.1 \u00b1 0.3 76.5 \u00b1 0.4 78.2 \u00b1 0.6 68.0 \n\n\n\nTable 14 :\n14Impact of the \u03bb distribution fromTable 8.Model selection \u03bb distribution \nCMNIST RMNIST \nVLCS \nPACS \nOfficeHome TerraInc DomainNet Avg \n\nTest-domain \n\nConstant(0) (= ERM) 57.8 \u00b1 0.2 97.8 \u00b1 0.1 77.6 \u00b1 0.3 86.7 \u00b1 0.3 \n66.4 \u00b1 0.5 \n53.0 \u00b1 0.3 \n41.3 \u00b1 0.1 \n68.7 \n10 Uniform (1,4) \n68.8 \u00b1 1.4 97.8 \u00b1 0.1 78.2 \u00b1 0.2 86.9 \u00b1 0.2 \n68.2 \u00b1 0.2 \n53.6 \u00b1 0.4 \n41.8 \u00b1 0.1 \n70.8 \n10 Uniform (1,5) \n68.7 \u00b1 1.3 97.8 \u00b1 0.0 78.7 \u00b1 0.3 87.5 \u00b1 0.1 \n68.0 \u00b1 0.4 \n52.2 \u00b1 0.5 \n42.0 \u00b1 0.1 \n70.7 \n\nTraining-domain \n\nConstant(0) (= ERM) 51.5 \u00b1 0.1 98.0 \u00b1 0.0 77.5 \u00b1 0.4 85.5 \u00b1 0.2 \n66.5 \u00b1 0.3 \n46.1 \u00b1 1.8 \n40.9 \u00b1 0.1 \n66.6 \n10 Uniform (1,4) \n52.0 \u00b1 0.2 97.8 \u00b1 0.0 77.8 \u00b1 0.1 85.5 \u00b1 0.4 \n67.8 \u00b1 0.1 \n47.4 \u00b1 1.6 \n41.7 \u00b1 0.0 \n67.1 \n10 Uniform (1,5) \n51.8 \u00b1 0.3 97.9 \u00b1 0.0 77.9 \u00b1 0.1 85.5 \u00b1 0.6 \n67.4 \u00b1 0.3 \n47.2 \u00b1 1.0 \n41.8 \u00b1 0.1 \n67.1 \n\n\n\n\nTables below detail results for each dataset with 'Test-domain' and 'Training-domain' model selection methods. We format first and second best accuracies. Note that the per-dataset results for Fish (Shi et al., 2021) are not available. D.4.2. ROTATED MNIST Rotated MNIST. Model selection: 'Test-domain' validation set SAND-mask 94.7 \u00b1 0.2 98.5 \u00b1 0.2 98.6 \u00b1 0.1 98.6 \u00b1 0.1 98.5 \u00b1 0.1 95.2 \u00b1 0.1 97.4 Fishr 95.8 \u00b1 0.1 98.3 \u00b1 0.1 98.8 \u00b1 0.1 98.6 \u00b1 0.3 98.7 \u00b1 0.1 96.5 \u00b1 0.1 97.8 12 Rotated MNIST. Model selection: 'Training-domain' validation setAlgorithm \n0 \n15 \n30 \n45 \n60 \n75 \nAvg Ranking \n\nERM \n95.3 \u00b1 0.2 98.7 \u00b1 0.1 98.9 \u00b1 0.1 98.7 \u00b1 0.2 98.9 \u00b1 0.0 96.2 \u00b1 0.2 97.8 \n12 \nIRM \n94.9 \u00b1 0.6 98.7 \u00b1 0.2 98.6 \u00b1 0.1 98.6 \u00b1 0.2 98.7 \u00b1 0.1 95.2 \u00b1 0.3 97.5 \n16 \nGroupDRO \n95.9 \u00b1 0.1 99.0 \u00b1 0.1 98.9 \u00b1 0.1 98.8 \u00b1 0.1 98.6 \u00b1 0.1 96.3 \u00b1 0.4 97.9 \n5 \nMixup \n95.8 \u00b1 0.3 98.7 \u00b1 0.0 99.0 \u00b1 0.1 98.8 \u00b1 0.1 98.8 \u00b1 0.1 96.6 \u00b1 0.2 98.0 \n2 \nMLDG \n95.7 \u00b1 0.2 98.9 \u00b1 0.1 98.8 \u00b1 0.1 98.9 \u00b1 0.1 98.6 \u00b1 0.1 95.8 \u00b1 0.4 97.8 \n12 \nCORAL \n96.2 \u00b1 0.2 98.8 \u00b1 0.1 98.8 \u00b1 0.1 98.8 \u00b1 0.1 98.9 \u00b1 0.1 96.4 \u00b1 0.2 98.0 \n2 \nMMD \n96.1 \u00b1 0.2 98.9 \u00b1 0.0 99.0 \u00b1 0.0 98.8 \u00b1 0.0 98.9 \u00b1 0.0 96.4 \u00b1 0.2 98.0 \n2 \nDANN \n95.9 \u00b1 0.1 98.9 \u00b1 0.1 98.6 \u00b1 0.2 98.7 \u00b1 0.1 98.9 \u00b1 0.0 96.3 \u00b1 0.3 97.9 \n5 \nCDANN \n95.9 \u00b1 0.2 98.8 \u00b1 0.0 98.7 \u00b1 0.1 98.9 \u00b1 0.1 98.8 \u00b1 0.1 96.1 \u00b1 0.3 97.9 \n5 \nMTL \n96.1 \u00b1 0.2 98.9 \u00b1 0.0 99.0 \u00b1 0.0 98.7 \u00b1 0.1 99.0 \u00b1 0.0 95.8 \u00b1 0.3 97.9 \n5 \nSagNet \n95.9 \u00b1 0.1 99.0 \u00b1 0.1 98.9 \u00b1 0.1 98.6 \u00b1 0.1 98.8 \u00b1 0.1 96.3 \u00b1 0.1 97.9 \n5 \nARM \n95.9 \u00b1 0.4 99.0 \u00b1 0.1 98.8 \u00b1 0.1 98.9 \u00b1 0.1 99.1 \u00b1 0.1 96.7 \u00b1 0.2 98.1 \n1 \nV-REx \n95.5 \u00b1 0.2 99.0 \u00b1 0.0 98.7 \u00b1 0.2 98.8 \u00b1 0.1 98.8 \u00b1 0.0 96.4 \u00b1 0.0 97.9 \n5 \nRSC \n95.4 \u00b1 0.1 98.6 \u00b1 0.1 98.6 \u00b1 0.1 98.9 \u00b1 0.0 98.8 \u00b1 0.1 95.4 \u00b1 0.3 97.6 \n15 \nAND-mask \n94.9 \u00b1 0.1 98.8 \u00b1 0.1 98.8 \u00b1 0.1 98.7 \u00b1 0.2 98.6 \u00b1 0.2 95.5 \u00b1 0.2 97.5 \n16 \n18 \nFish \n97.9 \n11 \n\nAlgorithm \n0 \n15 \n30 \n45 \n60 \n75 \nAvg Ranking \n\n\n\n\nSAND-mask 94.5 \u00b1 0.4 98.6 \u00b1 0.1 98.8 \u00b1 0.1 98.7 \u00b1 0.1 98.6 \u00b1 0.0 95.5 \u00b1 0.2 97.4 VLCS. Model selection: 'Test-domain' validation set SAND-mask 97.6 \u00b1 0.3 64.5 \u00b1 0.6 69.7 \u00b1 0.6 73.0 \u00b1 1.2 76.2 Fishr 97.6 \u00b1 0.7 67.3 \u00b1 0.5 72.2 \u00b1 0.9 75.7 \u00b1 0.3 78.2 3 VLCS. Model selection: 'Training-domain' validation set SAND-mask 98.5 \u00b1 0.3 63.6 \u00b1 0.9 70.4 \u00b1 0.8 77.1 \u00b1 0.8 77.4 PACS. Model selection: 'Test-domain' validation set SAND-mask 86.1 \u00b1 0.6 80.3 \u00b1 1.0 97.1 \u00b1 0.3 80.0 \u00b1 1.3 85.9 Fishr 87.9 \u00b1 0.6 80.8 \u00b1 0.5 97.9 \u00b1 0.4 81.1 \u00b1 0.8 86.9 5 PACS. Model selection: 'Training-domain' validation set SAND-mask 85.8 \u00b1 1.7 79.2 \u00b1 0.8 96.3 \u00b1 0.2 76.9 \u00b1 2.0 84.6 OfficeHome. Model selection: 'Test-domain' validation set SAND-mask 59.9 \u00b1 0.7 53.6 \u00b1 0.8 74.3 \u00b1 0.4 75.8 \u00b1 0.5 65.9 Fishr 63.4 \u00b1 0.8 54.2 \u00b1 0.3 76.4 \u00b1 0.3 78.5 \u00b1 0.2 68.2 5 OfficeHome. Model selection: 'Training-domain' validation set SAND-mask 60.3 \u00b1 0.5 53.3 \u00b1 0.7 73.5 \u00b1 0.7 76.2 \u00b1 0.3 65.8 Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization D.4.6. TERRAINCOGNITA TerraIncognita. Model selection: 'Test-domain' validation set SAND-mask 56.2 \u00b1 1.8 46.3 \u00b1 0.3 55.8 \u00b1 0.4 42.6 \u00b1 1.2 50.2 Fishr 60.4 \u00b1 0.9 50.3 \u00b1 0.3 58.8 \u00b1 0.5 44.9 \u00b1 0.5 53.6 2TerraIncognita. Model selection: 'Training-domain' validation set SAND-mask 45.7 \u00b1 2.9 31.6 \u00b1 4.7 55.1 \u00b1 1.0 39.0 \u00b1 1.8 42.91 98.9 \u00b1 0.0 96.3 \u00b1 0.2 97.8 \n13 \nCDANN \n95.7 \u00b1 0.2 98.8 \u00b1 0.0 98.9 \u00b1 0.1 98.9 \u00b1 0.1 98.9 \u00b1 0.1 96.1 \u00b1 0.3 97.9 \n8 \nMTL \n95.6 \u00b1 0.1 99.0 \u00b1 0.1 99.0 \u00b1 0.0 98.9 \u00b1 0.1 99.0 \u00b1 0.1 95.8 \u00b1 0.2 97.9 \n8 \nSagNet \n95.9 \u00b1 0.3 98.9 \u00b1 0.1 99.0 \u00b1 0.1 99.1 \u00b1 0.0 99.0 \u00b1 0.1 96.3 \u00b1 0.1 98.0 \n2 \nARM \n96.7 \u00b1 0.2 99.1 \u00b1 0.0 99.0 \u00b1 0.0 99.0 \u00b1 0.1 99.1 \u00b1 0.1 96.5 \u00b1 0.4 98.2 \n1 \nV-REx \n95.9 \u00b1 0.2 99.0 \u00b1 0.1 98.9 \u00b1 0.1 98.9 \u00b1 0.1 98.7 \u00b1 0.1 96.2 \u00b1 0.2 97.9 \n8 \nRSC \n94.8 \u00b1 0.5 98.7 \u00b1 0.1 98.8 \u00b1 0.1 98.8 \u00b1 0.0 98.9 \u00b1 0.1 95.9 \u00b1 0.2 97.6 \n16 \nAND-mask \n94.8 \u00b1 0.2 98.8 \u00b1 0.1 98.9 \u00b1 0.0 98.7 \u00b1 0.0 98.7 \u00b1 0.1 95.5 \u00b1 0.4 97.6 \n16 \n18 \nFish \n98.0 \n2 \n\nFishr \n95.0 \u00b1 0.3 98.5 \u00b1 0.0 99.2 \u00b1 0.1 98.9 \u00b1 0.0 98.9 \u00b1 0.1 96.5 \u00b1 0.0 97.8 D.4.3. VLCS \n\nAlgorithm \nC \nL \nS \nV \nAvg Ranking \n\nERM \n97.6 \u00b1 0.3 67.9 \u00b1 0.7 70.9 \u00b1 0.2 74.0 \u00b1 0.6 77.6 \n12 \nIRM \n97.3 \u00b1 0.2 66.7 \u00b1 0.1 71.0 \u00b1 2.3 72.8 \u00b1 0.4 76.9 \n16 \nGroupDRO \n97.7 \u00b1 0.2 65.9 \u00b1 0.2 72.8 \u00b1 0.8 73.4 \u00b1 1.3 77.4 \n15 \nMixup \n97.8 \u00b1 0.4 67.2 \u00b1 0.4 71.5 \u00b1 0.2 75.7 \u00b1 0.6 78.1 \n4 \nMLDG \n97.1 \u00b1 0.5 66.6 \u00b1 0.5 71.5 \u00b1 0.1 75.0 \u00b1 0.9 77.5 \n14 \nCORAL \n97.3 \u00b1 0.2 67.5 \u00b1 0.6 71.6 \u00b1 0.6 74.5 \u00b1 0.0 77.7 \n10 \nMMD \n98.8 \u00b1 0.0 66.4 \u00b1 0.4 70.8 \u00b1 0.5 75.6 \u00b1 0.4 77.9 \n6 \nDANN \n99.0 \u00b1 0.2 66.3 \u00b1 1.2 73.4 \u00b1 1.4 80.1 \u00b1 0.5 79.7 \n2 \nCDANN \n98.2 \u00b1 0.1 68.8 \u00b1 0.5 74.3 \u00b1 0.6 78.1 \u00b1 0.5 79.9 \n1 \nMTL \n97.9 \u00b1 0.7 66.1 \u00b1 0.7 72.0 \u00b1 0.4 74.9 \u00b1 1.1 77.7 \n10 \nSagNet \n97.4 \u00b1 0.3 66.4 \u00b1 0.4 71.6 \u00b1 0.1 75.0 \u00b1 0.8 77.6 \n12 \nARM \n97.6 \u00b1 0.6 66.5 \u00b1 0.3 72.7 \u00b1 0.6 74.4 \u00b1 0.7 77.8 \n7 \nV-REx \n98.4 \u00b1 0.2 66.4 \u00b1 0.7 72.8 \u00b1 0.1 75.0 \u00b1 1.4 78.1 \n4 \nRSC \n98.0 \u00b1 0.4 67.2 \u00b1 0.3 70.3 \u00b1 1.3 75.6 \u00b1 0.4 77.8 \n7 \nAND-mask \n98.3 \u00b1 0.3 64.5 \u00b1 0.2 69.3 \u00b1 1.3 73.4 \u00b1 1.3 76.4 \n17 \n18 \nFish \n77.8 \n7 \n\nAlgorithm \nC \nL \nS \nV \nAvg Ranking \n\nERM \n97.7 \u00b1 0.4 64.3 \u00b1 0.9 73.4 \u00b1 0.5 74.6 \u00b1 1.3 77.5 \n10 \nIRM \n98.6 \u00b1 0.1 64.9 \u00b1 0.9 73.4 \u00b1 0.6 77.3 \u00b1 0.9 78.5 \n3 \nGroupDRO \n97.3 \u00b1 0.3 63.4 \u00b1 0.9 69.5 \u00b1 0.8 76.7 \u00b1 0.7 76.7 \n18 \nMixup \n98.3 \u00b1 0.6 64.8 \u00b1 1.0 72.1 \u00b1 0.5 74.3 \u00b1 0.8 77.4 \n13 \nMLDG \n97.4 \u00b1 0.2 65.2 \u00b1 0.7 71.0 \u00b1 1.4 75.3 \u00b1 1.0 77.2 \n15 \nCORAL \n98.3 \u00b1 0.1 66.1 \u00b1 1.2 73.4 \u00b1 0.3 77.5 \u00b1 1.2 78.8 \n1 \nMMD \n97.7 \u00b1 0.1 64.0 \u00b1 1.1 72.8 \u00b1 0.2 75.3 \u00b1 3.3 77.5 \n10 \nDANN \n99.0 \u00b1 0.3 65.1 \u00b1 1.4 73.1 \u00b1 0.3 77.2 \u00b1 0.6 78.6 \n2 \nCDANN \n97.1 \u00b1 0.3 65.1 \u00b1 1.2 70.7 \u00b1 0.8 77.1 \u00b1 1.5 77.5 \n10 \nMTL \n97.8 \u00b1 0.4 64.3 \u00b1 0.3 71.5 \u00b1 0.7 75.3 \u00b1 1.7 77.2 \n15 \nSagNet \n97.9 \u00b1 0.4 64.5 \u00b1 0.5 71.4 \u00b1 1.3 77.5 \u00b1 0.5 77.8 \n6 \nARM \n98.7 \u00b1 0.2 63.6 \u00b1 0.7 71.3 \u00b1 1.2 76.7 \u00b1 0.6 77.6 \n9 \nV-REx \n98.4 \u00b1 0.3 64.4 \u00b1 1.4 74.1 \u00b1 0.4 76.2 \u00b1 1.3 78.3 \n4 \nRSC \n97.9 \u00b1 0.1 62.5 \u00b1 0.7 72.3 \u00b1 1.2 75.6 \u00b1 0.8 77.1 \n17 \nAND-mask \n97.8 \u00b1 0.4 64.3 \u00b1 1.2 73.5 \u00b1 0.7 76.8 \u00b1 2.6 78.1 \n5 \n13 \nFish \n77.8 \n6 \n\nFishr \n98.9 \u00b1 0.3 64.0 \u00b1 0.5 71.5 \u00b1 0.2 76.8 \u00b1 0.7 77.8 D.4.4. PACS \n\nAlgorithm \nA \nC \nP \nS \nAvg Ranking \n\nERM \n86.5 \u00b1 1.0 81.3 \u00b1 0.6 96.2 \u00b1 0.3 82.7 \u00b1 1.1 86.7 \n8 \nIRM \n84.2 \u00b1 0.9 79.7 \u00b1 1.5 95.9 \u00b1 0.4 78.3 \u00b1 2.1 84.5 \n18 \nGroupDRO \n87.5 \u00b1 0.5 82.9 \u00b1 0.6 97.1 \u00b1 0.3 81.1 \u00b1 1.2 87.1 \n3 \nMixup \n87.5 \u00b1 0.4 81.6 \u00b1 0.7 97.4 \u00b1 0.2 80.8 \u00b1 0.9 86.8 \n6 \nMLDG \n87.0 \u00b1 1.2 82.5 \u00b1 0.9 96.7 \u00b1 0.3 81.2 \u00b1 0.6 86.8 \n6 \nCORAL \n86.6 \u00b1 0.8 81.8 \u00b1 0.9 97.1 \u00b1 0.5 82.7 \u00b1 0.6 87.1 \n3 \nMMD \n88.1 \u00b1 0.8 82.6 \u00b1 0.7 97.1 \u00b1 0.5 81.2 \u00b1 1.2 87.2 \n1 \nDANN \n87.0 \u00b1 0.4 80.3 \u00b1 0.6 96.8 \u00b1 0.3 76.9 \u00b1 1.1 85.2 \n17 \nCDANN \n87.7 \u00b1 0.6 80.7 \u00b1 1.2 97.3 \u00b1 0.4 77.6 \u00b1 1.5 85.8 \n14 \nMTL \n87.0 \u00b1 0.2 82.7 \u00b1 0.8 96.5 \u00b1 0.7 80.5 \u00b1 0.8 86.7 \n8 \nSagNet \n87.4 \u00b1 0.5 81.2 \u00b1 1.2 96.3 \u00b1 0.8 80.7 \u00b1 1.1 86.4 \n10 \nARM \n85.0 \u00b1 1.2 81.4 \u00b1 0.2 95.9 \u00b1 0.3 80.9 \u00b1 0.5 85.8 \n14 \nV-REx \n87.8 \u00b1 1.2 81.8 \u00b1 0.7 97.4 \u00b1 0.2 82.1 \u00b1 0.7 87.2 \n1 \nRSC \n86.0 \u00b1 0.7 81.8 \u00b1 0.9 96.8 \u00b1 0.7 80.4 \u00b1 0.5 86.2 \n12 \nAND-mask \n86.4 \u00b1 1.1 80.8 \u00b1 0.9 97.1 \u00b1 0.2 81.3 \u00b1 1.1 86.4 \n10 \n13 \nFish \n85.8 \n14 \n\nAlgorithm \nA \nC \nP \nS \nAvg Ranking \n\nERM \n84.7 \u00b1 0.4 80.8 \u00b1 0.6 97.2 \u00b1 0.3 79.3 \u00b1 1.0 85.5 \n3 \nIRM \n84.8 \u00b1 1.3 76.4 \u00b1 1.1 96.7 \u00b1 0.6 76.1 \u00b1 1.0 83.5 \n17 \nGroupDRO \n83.5 \u00b1 0.9 79.1 \u00b1 0.6 96.7 \u00b1 0.3 78.3 \u00b1 2.0 84.4 \n14 \nMixup \n86.1 \u00b1 0.5 78.9 \u00b1 0.8 97.6 \u00b1 0.1 75.8 \u00b1 1.8 84.6 \n10 \nMLDG \n85.5 \u00b1 1.4 80.1 \u00b1 1.7 97.4 \u00b1 0.3 76.6 \u00b1 1.1 84.9 \n8 \nCORAL \n88.3 \u00b1 0.2 80.0 \u00b1 0.5 97.5 \u00b1 0.3 78.8 \u00b1 1.3 86.2 \n2 \nMMD \n86.1 \u00b1 1.4 79.4 \u00b1 0.9 96.6 \u00b1 0.2 76.5 \u00b1 0.5 84.6 \n10 \nDANN \n86.4 \u00b1 0.8 77.4 \u00b1 0.8 97.3 \u00b1 0.4 73.5 \u00b1 2.3 83.6 \n16 \nCDANN \n84.6 \u00b1 1.8 75.5 \u00b1 0.9 96.8 \u00b1 0.3 73.5 \u00b1 0.6 82.6 \n18 \nMTL \n87.5 \u00b1 0.8 77.1 \u00b1 0.5 96.4 \u00b1 0.8 77.3 \u00b1 1.8 84.6 \n10 \nSagNet \n87.4 \u00b1 1.0 80.7 \u00b1 0.6 97.1 \u00b1 0.1 80.0 \u00b1 0.4 86.3 \n1 \nARM \n86.8 \u00b1 0.6 76.8 \u00b1 0.5 97.4 \u00b1 0.3 79.3 \u00b1 1.2 85.1 \n7 \nV-REx \n86.0 \u00b1 1.6 79.1 \u00b1 0.6 96.9 \u00b1 0.5 77.7 \u00b1 1.7 84.9 \n8 \nRSC \n85.4 \u00b1 0.8 79.7 \u00b1 1.8 97.6 \u00b1 0.3 78.2 \u00b1 1.2 85.2 \n6 \nAND-mask \n85.3 \u00b1 1.4 79.2 \u00b1 2.0 96.9 \u00b1 0.4 76.2 \u00b1 1.4 84.4 \n14 \n10 \nFish \n85.5 \n3 \n\nFishr \n88.4 \u00b1 0.2 78.7 \u00b1 0.7 97.0 \u00b1 0.1 77.8 \u00b1 2.0 85.5 D.4.5. OFFICEHOME \n\nAlgorithm \nA \nC \nP \nR \nAvg Ranking \n\nERM \n61.7 \u00b1 0.7 53.4 \u00b1 0.3 74.1 \u00b1 0.4 76.2 \u00b1 0.6 66.4 \n8 \nIRM \n56.4 \u00b1 3.2 51.2 \u00b1 2.3 71.7 \u00b1 2.7 72.7 \u00b1 2.7 63.0 \n18 \nGroupDRO \n60.5 \u00b1 1.6 53.1 \u00b1 0.3 75.5 \u00b1 0.3 75.9 \u00b1 0.7 66.2 \n3 \nMixup \n63.5 \u00b1 0.2 54.6 \u00b1 0.4 76.0 \u00b1 0.3 78.0 \u00b1 0.7 68.0 \n6 \nMLDG \n60.5 \u00b1 0.7 54.2 \u00b1 0.5 75.0 \u00b1 0.2 76.7 \u00b1 0.5 66.6 \n6 \nCORAL \n64.8 \u00b1 0.8 54.1 \u00b1 0.9 76.5 \u00b1 0.4 78.2 \u00b1 0.4 68.4 \n3 \nMMD \n60.4 \u00b1 1.0 53.4 \u00b1 0.5 74.9 \u00b1 0.1 76.1 \u00b1 0.7 66.2 \n1 \nDANN \n60.6 \u00b1 1.4 51.8 \u00b1 0.7 73.4 \u00b1 0.5 75.5 \u00b1 0.9 65.3 \n17 \nCDANN \n57.9 \u00b1 0.2 52.1 \u00b1 1.2 74.9 \u00b1 0.7 76.2 \u00b1 0.2 65.3 \n14 \nMTL \n60.7 \u00b1 0.8 53.5 \u00b1 1.3 75.2 \u00b1 0.6 76.6 \u00b1 0.6 66.5 \n8 \nSagNet \n62.7 \u00b1 0.5 53.6 \u00b1 0.5 76.0 \u00b1 0.3 77.8 \u00b1 0.1 67.5 \n10 \nARM \n58.8 \u00b1 0.5 51.8 \u00b1 0.7 74.0 \u00b1 0.1 74.4 \u00b1 0.2 64.8 \n14 \nV-REx \n59.6 \u00b1 1.0 53.3 \u00b1 0.3 73.2 \u00b1 0.5 76.6 \u00b1 0.4 65.7 \n1 \nRSC \n61.7 \u00b1 0.8 53.0 \u00b1 0.9 74.8 \u00b1 0.8 76.3 \u00b1 0.5 66.5 \n12 \nAND-mask \n60.3 \u00b1 0.5 52.3 \u00b1 0.6 75.1 \u00b1 0.2 76.6 \u00b1 0.3 66.1 \n10 \n13 \nFish \n66.0 \n12 \n\nAlgorithm \nA \nC \nP \nR \nAvg Ranking \n\nERM \n61.3 \u00b1 0.7 52.4 \u00b1 0.3 75.8 \u00b1 0.1 76.6 \u00b1 0.3 66.5 \n7 \nIRM \n58.9 \u00b1 2.3 52.2 \u00b1 1.6 72.1 \u00b1 2.9 74.0 \u00b1 2.5 64.3 \n18 \nGroupDRO \n60.4 \u00b1 0.7 52.7 \u00b1 1.0 75.0 \u00b1 0.7 76.0 \u00b1 0.7 66.0 \n11 \nMixup \n62.4 \u00b1 0.8 54.8 \u00b1 0.6 76.9 \u00b1 0.3 78.3 \u00b1 0.2 68.1 \n3 \nMLDG \n61.5 \u00b1 0.9 53.2 \u00b1 0.6 75.0 \u00b1 1.2 77.5 \u00b1 0.4 66.8 \n6 \nCORAL \n65.3 \u00b1 0.4 54.4 \u00b1 0.5 76.5 \u00b1 0.1 78.4 \u00b1 0.5 68.7 \n1 \nMMD \n60.4 \u00b1 0.2 53.3 \u00b1 0.3 74.3 \u00b1 0.1 77.4 \u00b1 0.6 66.3 \n10 \nDANN \n59.9 \u00b1 1.3 53.0 \u00b1 0.3 73.6 \u00b1 0.7 76.9 \u00b1 0.5 65.9 \n12 \nCDANN \n61.5 \u00b1 1.4 50.4 \u00b1 2.4 74.4 \u00b1 0.9 76.6 \u00b1 0.8 65.8 \n13 \nMTL \n61.5 \u00b1 0.7 52.4 \u00b1 0.6 74.9 \u00b1 0.4 76.8 \u00b1 0.4 66.4 \n8 \nSagNet \n63.4 \u00b1 0.2 54.8 \u00b1 0.4 75.8 \u00b1 0.4 78.3 \u00b1 0.3 68.1 \n3 \nARM \n58.9 \u00b1 0.8 51.0 \u00b1 0.5 74.1 \u00b1 0.1 75.2 \u00b1 0.3 64.8 \n17 \nV-REx \n60.7 \u00b1 0.9 53.0 \u00b1 0.9 75.3 \u00b1 0.1 76.6 \u00b1 0.5 66.4 \n8 \nRSC \n60.7 \u00b1 1.4 51.4 \u00b1 0.3 74.8 \u00b1 1.1 75.1 \u00b1 1.3 65.5 \n16 \nANDMask \n59.5 \u00b1 1.2 51.7 \u00b1 0.2 73.9 \u00b1 0.4 77.1 \u00b1 0.2 65.6 \n15 \n13 \nFish \n68.6 \n2 \n\nFishr \n62.4 \u00b1 0.5 54.4 \u00b1 0.4 76.2 \u00b1 0.5 78.3 \u00b1 0.1 67.8 \n5 \n\nAlgorithm \nL100 \nL38 \nL43 \nL46 \nAvg Ranking \n\nERM \n59.4 \u00b1 0.9 49.3 \u00b1 0.6 60.1 \u00b1 1.1 43.2 \u00b1 0.5 53.0 \n3 \nIRM \n56.5 \u00b1 2.5 49.8 \u00b1 1.5 57.1 \u00b1 2.2 38.6 \u00b1 1.0 50.5 \n16 \nGroupDRO \n60.4 \u00b1 1.5 48.3 \u00b1 0.4 58.6 \u00b1 0.8 42.2 \u00b1 0.8 52.4 \n6 \nMixup \n67.6 \u00b1 1.8 51.0 \u00b1 1.3 59.0 \u00b1 0.0 40.0 \u00b1 1.1 54.4 \n1 \nMLDG \n59.2 \u00b1 0.1 49.0 \u00b1 0.9 58.4 \u00b1 0.9 41.4 \u00b1 1.0 52.0 \n9 \nCORAL \n60.4 \u00b1 0.9 47.2 \u00b1 0.5 59.3 \u00b1 0.4 44.4 \u00b1 0.4 52.8 \n4 \nMMD \n60.6 \u00b1 1.1 45.9 \u00b1 0.3 57.8 \u00b1 0.5 43.8 \u00b1 1.2 52.0 \n9 \nDANN \n55.2 \u00b1 1.9 47.0 \u00b1 0.7 57.2 \u00b1 0.9 42.9 \u00b1 0.9 50.6 \n15 \nCDANN \n56.3 \u00b1 2.0 47.1 \u00b1 0.9 57.2 \u00b1 1.1 42.4 \u00b1 0.8 50.8 \n13 \nMTL \n58.4 \u00b1 2.1 48.4 \u00b1 0.8 58.9 \u00b1 0.6 43.0 \u00b1 1.3 52.2 \n7 \nSagNet \n56.4 \u00b1 1.9 50.5 \u00b1 2.3 59.1 \u00b1 0.5 44.1 \u00b1 0.6 52.5 \n5 \nARM \n60.1 \u00b1 1.5 48.3 \u00b1 1.6 55.3 \u00b1 0.6 40.9 \u00b1 1.1 51.2 \n12 \nV-REx \n56.8 \u00b1 1.7 46.5 \u00b1 0.5 58.4 \u00b1 0.3 43.8 \u00b1 0.3 51.4 \n11 \nRSC \n59.9 \u00b1 1.4 46.7 \u00b1 0.4 57.8 \u00b1 0.5 44.3 \u00b1 0.6 52.1 \n8 \nAND-mask \n54.7 \u00b1 1.8 48.4 \u00b1 0.5 55.1 \u00b1 0.5 41.3 \u00b1 0.6 49.8 \n18 \n17 \nFish \n50.8 \n13 \n\nAlgorithm \nL100 \nL38 \nL43 \nL46 \nAvg Ranking \n\nERM \n49.8 \u00b1 4.4 42.1 \u00b1 1.4 56.9 \u00b1 1.8 35.7 \u00b1 3.9 46.1 \n10 \nIRM \n54.6 \u00b1 1.3 39.8 \u00b1 1.9 56.2 \u00b1 1.8 39.6 \u00b1 0.8 47.6 \n4 \nGroupDRO \n41.2 \u00b1 0.7 38.6 \u00b1 2.1 56.7 \u00b1 0.9 36.4 \u00b1 2.1 43.2 \n16 \nMixup \n59.6 \u00b1 2.0 42.2 \u00b1 1.4 55.9 \u00b1 0.8 33.9 \u00b1 1.4 47.9 \n2 \nMLDG \n54.2 \u00b1 3.0 44.3 \u00b1 1.1 55.6 \u00b1 0.3 36.9 \u00b1 2.2 47.7 \n3 \nCORAL \n51.6 \u00b1 2.4 42.2 \u00b1 1.0 57.0 \u00b1 1.0 39.8 \u00b1 2.9 47.6 \n4 \nMMD \n41.9 \u00b1 3.0 34.8 \u00b1 1.0 57.0 \u00b1 1.9 35.2 \u00b1 1.8 42.2 \n18 \nDANN \n51.1 \u00b1 3.5 40.6 \u00b1 0.6 57.4 \u00b1 0.5 37.7 \u00b1 1.8 46.7 \n7 \nCDANN \n47.0 \u00b1 1.9 41.3 \u00b1 4.8 54.9 \u00b1 1.7 39.8 \u00b1 2.3 45.8 \n11 \nMTL \n49.3 \u00b1 1.2 39.6 \u00b1 6.3 55.6 \u00b1 1.1 37.8 \u00b1 0.8 45.6 \n12 \nSagNet \n53.0 \u00b1 2.9 43.0 \u00b1 2.5 57.9 \u00b1 0.6 40.4 \u00b1 1.3 48.6 \n1 \nARM \n49.3 \u00b1 0.7 38.3 \u00b1 2.4 55.8 \u00b1 0.8 38.7 \u00b1 1.3 45.5 \n13 \nV-REx \n48.2 \u00b1 4.3 41.7 \u00b1 1.3 56.8 \u00b1 0.8 38.7 \u00b1 3.1 46.4 \n9 \nRSC \n50.2 \u00b1 2.2 39.2 \u00b1 1.4 56.3 \u00b1 1.4 40.8 \u00b1 0.6 46.6 \n8 \nAND-mask \n50.0 \u00b1 2.9 40.2 \u00b1 0.8 53.3 \u00b1 0.7 34.8 \u00b1 1.9 44.6 \n15 \n17 \nFish \n45.1 \n14 \n\nFishr \n50.2 \u00b1 3.9 43.9 \u00b1 0.8 55.7 \u00b1 2.2 39.8 \u00b1 1.0 47.4 \n6 \n\nSorbonne Universit\u00e9, CNRS, LIP6, Paris, France 2 Valeo.ai. Correspondence to: Alexandre Ram\u00e9 <alexandre.rame@sorbonneuniversite.fr>.\nACKNOWLEDGMENTSThis work was granted access to the HPC resources of IDRIS under the allocation A0100612449 made by GENCI. We acknowledge the financial support by the ANR agency in the chair VISA-DEEP (ANR-20-CHIA-0022-01).\nSystematic generalisation with group invariant predictions. F Ahmed, Y Bengio, H Van Seijen, A Courville, ICLR. 3Ahmed, F., Bengio, Y., van Seijen, H., and Courville, A. Sys- tematic generalisation with group invariant predictions. In ICLR, 2021. (p. 3).\n\nInvariance principle meets information bottleneck for out-of-distribution generalization. K Ahuja, E Caballero, D Zhang, Y Bengio, I Mitliagkas, I Rish, NeurIPS. 183Ahuja, K., Caballero, E., Zhang, D., Bengio, Y., Mitliagkas, I., and Rish, I. Invariance principle meets information bot- tleneck for out-of-distribution generalization. In NeurIPS, 2019. (pp. 3, 18).\n\nInvariant risk minimization. M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, arXiv preprintArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk minimization. arXiv preprint, 2019. (pp. 1, 2, 3, 6, 7, 8, 16, 19, 20, 23).\n\nImproving the convergence of back-propagation learning with second order methods. S Becker, Y Le Cun, Connectionist models summer school. 6Becker, S. and Le Cun, Y. Improving the convergence of back-propagation learning with second order methods. In Connectionist models summer school, 1988. (p. 6).\n\nRecognition in terra incognita. S Beery, G Van Horn, P Perona, ECCV. 207Beery, S., Van Horn, G., and Perona, P. Recognition in terra incognita. In ECCV, 2018. (pp. 7, 20).\n\nGeneralizing from several related classification tasks to a new unlabeled sample. G Blanchard, G Lee, C Scott, NeurIPS. 1Blanchard, G., Lee, G., and Scott, C. Generalizing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. (p. 1).\n\nDomain generalization by marginal transfer learning. G Blanchard, A A Deshmukh, U Dogan, G Lee, C Scott, JMLR, 2021. (pp. 8, 19, 21Blanchard, G., Deshmukh, A. A., Dogan, U., Lee, G., and Scott, C. Domain generalization by marginal transfer learning. JMLR, 2021. (pp. 8, 19, 21).\n\nDomain generalization by seeking flat minima. J Cha, S Chun, K Lee, H.-C Cho, S Park, Y Lee, S Park, Swad, NeurIPS. 20Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y., and Park, S. SWAD: Domain generalization by seeking flat minima. In NeurIPS, 2021. (p. 20).\n\nInvariant rationalization. S Chang, Y Zhang, M Yu, T Jaakkola, ICML. 3Chang, S., Zhang, Y., Yu, M., and Jaakkola, T. Invariant rationalization. In ICML, 2020. (p. 3).\n\nInput similarity from the neural network perspective. G Charpiat, N Girard, L Felardos, Y Tarabalka, NeurIPS. Charpiat, G., Girard, N., Felardos, L., and Tarabalka, Y. Input similarity from the neural network perspective. In NeurIPS, 2019. (p. 2).\n\nInformation and accuracy attainable in the estimation of statistical parameters. C R , R , In Bulletin of the Calcutta Mathematical Society. 5C.R., R. Information and accuracy attainable in the estima- tion of statistical parameters. In Bulletin of the Calcutta Mathematical Society, 1945. (p. 5).\n\nEnvironment inference for invariant learning. E Creager, J.-H Jacobsen, R Zemel, ICML. 2021Creager, E., Jacobsen, J.-H., and Zemel, R. Environment inference for invariant learning. In ICML, 2021. (p. 2).\n\nUnderspecification presents challenges for credibility in modern machine learning. A D&apos;amour, K Heller, D Moldovan, B Adlam, B Alipanahi, A Beutel, C Chen, J Deaton, J Eisenstein, M D Hoffman, JMLR. 21D'Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali- panahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein, J., Hoffman, M. D., et al. Underspecification presents challenges for credibility in modern machine learning. JMLR, 2020. (p. 21).\n\nBackPACK: Packing more into backprop. F Dangel, F Kunstner, P Hennig, ICLR, 2020. (pp. 2, 8, 17Dangel, F., Kunstner, F., and Hennig, P. BackPACK: Pack- ing more into backprop. In ICLR, 2020. (pp. 2, 8, 17).\n\nCurvature access through the generalized gauss-newton's low-rank structure. F Dangel, L Tatzel, P Hennig, Vivit, 5arXiv preprintDangel, F., Tatzel, L., and Hennig, P. Vivit: Curvature access through the generalized gauss-newton's low-rank structure. arXiv preprint, 2021. (p. 5).\n\nModel selection: 'Test-domain' validation set. D.4.1. COLORED MNIST Colored MNIST. Model selection: 'Test-domain' validation set\n\nModel selection: 'Training-domain' validation set. Mnist Colored, Colored MNIST. Model selection: 'Training-domain' validation set\n", "annotations": {"author": "[{\"end\":93,\"start\":78},{\"end\":112,\"start\":94},{\"end\":127,\"start\":113}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":88},{\"end\":111,\"start\":103},{\"end\":126,\"start\":122}]", "author_first_name": "[{\"end\":87,\"start\":78},{\"end\":102,\"start\":94},{\"end\":121,\"start\":113}]", "author_affiliation": null, "title": "[{\"end\":75,\"start\":1},{\"end\":202,\"start\":128}]", "venue": null, "abstract": "[{\"end\":1386,\"start\":204}]", "bib_ref": "[{\"end\":1486,\"start\":1461},{\"end\":1671,\"start\":1645},{\"end\":1692,\"start\":1671},{\"end\":1894,\"start\":1877},{\"end\":1912,\"start\":1894},{\"end\":2472,\"start\":2450},{\"end\":2493,\"start\":2472},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2733,\"start\":2709},{\"end\":2754,\"start\":2733},{\"end\":2906,\"start\":2885},{\"end\":3052,\"start\":3033},{\"end\":3071,\"start\":3054},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3162,\"start\":3139},{\"end\":3183,\"start\":3162},{\"end\":3521,\"start\":3492},{\"end\":3537,\"start\":3521},{\"end\":3877,\"start\":3859},{\"end\":3903,\"start\":3877},{\"end\":4066,\"start\":4047},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4088,\"start\":4066},{\"end\":4636,\"start\":4619},{\"end\":4829,\"start\":4815},{\"end\":5161,\"start\":5155},{\"end\":5331,\"start\":5310},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5744,\"start\":5721},{\"end\":5841,\"start\":5813},{\"end\":6955,\"start\":6938},{\"end\":6976,\"start\":6960},{\"end\":7597,\"start\":7579},{\"end\":7724,\"start\":7702},{\"end\":7821,\"start\":7802},{\"end\":7837,\"start\":7821},{\"end\":7930,\"start\":7908},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7932,\"start\":7930},{\"end\":7951,\"start\":7932},{\"end\":8013,\"start\":7996},{\"end\":8033,\"start\":8013},{\"end\":8544,\"start\":8524},{\"end\":8562,\"start\":8544},{\"end\":8579,\"start\":8562},{\"end\":8625,\"start\":8603},{\"end\":8642,\"start\":8625},{\"end\":8708,\"start\":8690},{\"end\":8727,\"start\":8708},{\"end\":8811,\"start\":8782},{\"end\":9193,\"start\":9173},{\"end\":9223,\"start\":9195},{\"end\":9246,\"start\":9228},{\"end\":9377,\"start\":9364},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9523,\"start\":9500},{\"end\":9593,\"start\":9572},{\"end\":9620,\"start\":9593},{\"end\":9733,\"start\":9715},{\"end\":9753,\"start\":9733},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9772,\"start\":9753},{\"end\":9941,\"start\":9923},{\"end\":9969,\"start\":9941},{\"end\":10025,\"start\":10005},{\"end\":10044,\"start\":10025},{\"end\":10063,\"start\":10044},{\"end\":10082,\"start\":10063},{\"end\":10133,\"start\":10111},{\"end\":10496,\"start\":10479},{\"end\":10511,\"start\":10496},{\"end\":10550,\"start\":10511},{\"end\":10576,\"start\":10550},{\"end\":10595,\"start\":10576},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10634,\"start\":10614},{\"end\":10694,\"start\":10668},{\"end\":10721,\"start\":10694},{\"end\":10738,\"start\":10721},{\"end\":10971,\"start\":10931},{\"end\":11001,\"start\":10971},{\"end\":11025,\"start\":11001},{\"end\":11763,\"start\":11745},{\"end\":11782,\"start\":11763},{\"end\":12281,\"start\":12254},{\"end\":12298,\"start\":12281},{\"end\":13514,\"start\":13487},{\"end\":15881,\"start\":15855},{\"end\":15934,\"start\":15914},{\"end\":16191,\"start\":16165},{\"end\":16403,\"start\":16377},{\"end\":17365,\"start\":17340},{\"end\":18015,\"start\":17991},{\"end\":18033,\"start\":18015},{\"end\":18253,\"start\":18227},{\"end\":18915,\"start\":18893},{\"end\":19412,\"start\":19389},{\"end\":19438,\"start\":19412},{\"end\":19457,\"start\":19438},{\"end\":19752,\"start\":19739},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19763,\"start\":19752},{\"end\":19876,\"start\":19857},{\"end\":20135,\"start\":20091},{\"end\":20152,\"start\":20135},{\"end\":20191,\"start\":20170},{\"end\":20223,\"start\":20216},{\"end\":20505,\"start\":20484},{\"end\":22029,\"start\":22014},{\"end\":22051,\"start\":22029},{\"end\":22252,\"start\":22232},{\"end\":22650,\"start\":22623},{\"end\":22685,\"start\":22665},{\"end\":22703,\"start\":22685},{\"end\":22750,\"start\":22724},{\"end\":22783,\"start\":22763},{\"end\":22802,\"start\":22783},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22866,\"start\":22844},{\"end\":23037,\"start\":23011},{\"end\":23057,\"start\":23037},{\"end\":23693,\"start\":23675},{\"end\":23812,\"start\":23792},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24346,\"start\":24323},{\"end\":24412,\"start\":24383},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24671,\"start\":24648},{\"end\":29425,\"start\":29408},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29834,\"start\":29811},{\"end\":29904,\"start\":29884},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30357,\"start\":30334},{\"end\":30387,\"start\":30365},{\"end\":30434,\"start\":30411},{\"end\":31262,\"start\":31244},{\"end\":31285,\"start\":31262},{\"end\":31517,\"start\":31496},{\"end\":32990,\"start\":32973},{\"end\":35367,\"start\":35336},{\"end\":35476,\"start\":35456},{\"end\":35775,\"start\":35751},{\"end\":36337,\"start\":36300},{\"end\":37625,\"start\":37604},{\"end\":37650,\"start\":37625},{\"end\":39997,\"start\":39975},{\"end\":42602,\"start\":42584},{\"end\":43089,\"start\":43082},{\"end\":43112,\"start\":43089},{\"end\":43132,\"start\":43112},{\"end\":43681,\"start\":43661},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":44616,\"start\":44596},{\"end\":45668,\"start\":45654},{\"end\":45691,\"start\":45668},{\"end\":46879,\"start\":46854},{\"end\":46922,\"start\":46896},{\"end\":48571,\"start\":48553},{\"end\":48588,\"start\":48571},{\"end\":51147,\"start\":51142},{\"end\":51207,\"start\":51185},{\"end\":51417,\"start\":51398},{\"end\":52596,\"start\":52578},{\"end\":52931,\"start\":52913},{\"end\":52954,\"start\":52931},{\"end\":52973,\"start\":52954},{\"end\":53122,\"start\":53096},{\"end\":53773,\"start\":53763},{\"end\":53775,\"start\":53773},{\"end\":55033,\"start\":55015},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":56283,\"start\":56260},{\"end\":57280,\"start\":57262},{\"end\":57318,\"start\":57282},{\"end\":57358,\"start\":57323},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":60540,\"start\":60517}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59749,\"start\":59602},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59871,\"start\":59750},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59906,\"start\":59872},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60231,\"start\":59907},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60441,\"start\":60232},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61003,\"start\":60442},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63747,\"start\":61004},{\"attributes\":{\"id\":\"fig_7\"},\"end\":64324,\"start\":63748},{\"attributes\":{\"id\":\"fig_8\"},\"end\":64440,\"start\":64325},{\"attributes\":{\"id\":\"fig_9\"},\"end\":64839,\"start\":64441},{\"attributes\":{\"id\":\"fig_10\"},\"end\":65473,\"start\":64840},{\"attributes\":{\"id\":\"fig_11\"},\"end\":66076,\"start\":65474},{\"attributes\":{\"id\":\"fig_12\"},\"end\":66386,\"start\":66077},{\"attributes\":{\"id\":\"fig_13\"},\"end\":66719,\"start\":66387},{\"attributes\":{\"id\":\"fig_14\"},\"end\":66765,\"start\":66720},{\"attributes\":{\"id\":\"fig_15\"},\"end\":67737,\"start\":66766},{\"attributes\":{\"id\":\"fig_16\"},\"end\":69301,\"start\":67738},{\"attributes\":{\"id\":\"fig_17\"},\"end\":69344,\"start\":69302},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":69725,\"start\":69345},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":69837,\"start\":69726},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":70314,\"start\":69838},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":70701,\"start\":70315},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":72879,\"start\":70702},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":73200,\"start\":72880},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":76431,\"start\":73201},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":77037,\"start\":76432},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":78511,\"start\":77038},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":79160,\"start\":78512},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":80198,\"start\":79161},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":80562,\"start\":80199},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":81818,\"start\":80563},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":82504,\"start\":81819},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":84721,\"start\":82505},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":85351,\"start\":84722},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":86331,\"start\":85352},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":87414,\"start\":86332},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":88689,\"start\":87415},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":89508,\"start\":88690},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":91399,\"start\":89509},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":101409,\"start\":91400}]", "paragraph": "[{\"end\":2052,\"start\":1402},{\"end\":2648,\"start\":2108},{\"end\":4311,\"start\":2650},{\"end\":4705,\"start\":4313},{\"end\":5162,\"start\":4707},{\"end\":5332,\"start\":5164},{\"end\":5376,\"start\":5334},{\"end\":5481,\"start\":5378},{\"end\":5618,\"start\":5483},{\"end\":6047,\"start\":5620},{\"end\":6251,\"start\":6076},{\"end\":6539,\"start\":6253},{\"end\":8332,\"start\":6541},{\"end\":9099,\"start\":8334},{\"end\":9324,\"start\":9139},{\"end\":10252,\"start\":9326},{\"end\":10882,\"start\":10254},{\"end\":11091,\"start\":10927},{\"end\":11842,\"start\":11217},{\"end\":12412,\"start\":11881},{\"end\":12687,\"start\":12467},{\"end\":13024,\"start\":12729},{\"end\":13344,\"start\":13069},{\"end\":13800,\"start\":13369},{\"end\":14090,\"start\":13878},{\"end\":14414,\"start\":14131},{\"end\":14841,\"start\":14482},{\"end\":14944,\"start\":14843},{\"end\":15250,\"start\":15013},{\"end\":15577,\"start\":15462},{\"end\":15690,\"start\":15579},{\"end\":16028,\"start\":15777},{\"end\":16341,\"start\":16030},{\"end\":16605,\"start\":16371},{\"end\":17692,\"start\":16697},{\"end\":18034,\"start\":17733},{\"end\":18329,\"start\":18036},{\"end\":18669,\"start\":18374},{\"end\":19190,\"start\":18671},{\"end\":19877,\"start\":19234},{\"end\":20739,\"start\":19952},{\"end\":21498,\"start\":20846},{\"end\":22320,\"start\":21650},{\"end\":23246,\"start\":22322},{\"end\":23694,\"start\":23248},{\"end\":24261,\"start\":23696},{\"end\":24584,\"start\":24277},{\"end\":26484,\"start\":24622},{\"end\":27317,\"start\":26486},{\"end\":27374,\"start\":27319},{\"end\":27489,\"start\":27423},{\"end\":27809,\"start\":27540},{\"end\":28772,\"start\":28294},{\"end\":29905,\"start\":28799},{\"end\":31424,\"start\":29907},{\"end\":32854,\"start\":31426},{\"end\":33548,\"start\":32866},{\"end\":34120,\"start\":33550},{\"end\":34921,\"start\":34135},{\"end\":35199,\"start\":34923},{\"end\":35368,\"start\":35201},{\"end\":35631,\"start\":35370},{\"end\":35746,\"start\":35633},{\"end\":35776,\"start\":35748},{\"end\":36064,\"start\":35778},{\"end\":36091,\"start\":36066},{\"end\":36381,\"start\":36093},{\"end\":36619,\"start\":36383},{\"end\":36701,\"start\":36621},{\"end\":37012,\"start\":36703},{\"end\":37105,\"start\":37051},{\"end\":37213,\"start\":37107},{\"end\":37833,\"start\":37271},{\"end\":38107,\"start\":38061},{\"end\":38174,\"start\":38139},{\"end\":38581,\"start\":38239},{\"end\":38734,\"start\":38707},{\"end\":38994,\"start\":38869},{\"end\":39142,\"start\":39072},{\"end\":40064,\"start\":39196},{\"end\":40132,\"start\":40066},{\"end\":40810,\"start\":40295},{\"end\":41081,\"start\":40812},{\"end\":42208,\"start\":41124},{\"end\":42378,\"start\":42210},{\"end\":44769,\"start\":42417},{\"end\":46055,\"start\":44860},{\"end\":46563,\"start\":46057},{\"end\":47062,\"start\":46626},{\"end\":48461,\"start\":47064},{\"end\":48744,\"start\":48463},{\"end\":49219,\"start\":48746},{\"end\":50392,\"start\":49262},{\"end\":50990,\"start\":50394},{\"end\":51489,\"start\":50992},{\"end\":51980,\"start\":51491},{\"end\":52469,\"start\":51982},{\"end\":53590,\"start\":52565},{\"end\":54159,\"start\":53592},{\"end\":55034,\"start\":54250},{\"end\":55476,\"start\":55036},{\"end\":55703,\"start\":55478},{\"end\":56194,\"start\":55794},{\"end\":56678,\"start\":56196},{\"end\":57068,\"start\":56680},{\"end\":57873,\"start\":57070},{\"end\":58688,\"start\":57913},{\"end\":59284,\"start\":58690},{\"end\":59571,\"start\":59286}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":2107,\"start\":2053},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9138,\"start\":9100},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10926,\"start\":10883},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11216,\"start\":11092},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12466,\"start\":12413},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12728,\"start\":12688},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13068,\"start\":13025},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13877,\"start\":13827},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14130,\"start\":14091},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14481,\"start\":14415},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15012,\"start\":14945},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15461,\"start\":15251},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15776,\"start\":15691},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16370,\"start\":16342},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16696,\"start\":16606},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18373,\"start\":18330},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19951,\"start\":19878},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20845,\"start\":20740},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21649,\"start\":21511},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27539,\"start\":27490},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28293,\"start\":27810},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37270,\"start\":37214},{\"attributes\":{\"id\":\"formula_22\"},\"end\":38060,\"start\":37899},{\"attributes\":{\"id\":\"formula_23\"},\"end\":38138,\"start\":38108},{\"attributes\":{\"id\":\"formula_24\"},\"end\":38238,\"start\":38175},{\"attributes\":{\"id\":\"formula_25\"},\"end\":38706,\"start\":38582},{\"attributes\":{\"id\":\"formula_26\"},\"end\":38868,\"start\":38735},{\"attributes\":{\"id\":\"formula_28\"},\"end\":39071,\"start\":38995},{\"attributes\":{\"id\":\"formula_30\"},\"end\":40294,\"start\":40133},{\"attributes\":{\"id\":\"formula_31\"},\"end\":55793,\"start\":55704}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18289,\"start\":18282},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19655,\"start\":19648},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25687,\"start\":25680},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":40996,\"start\":40989},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":43081,\"start\":43074},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43371,\"start\":43364},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":44397,\"start\":44389},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":45015,\"start\":45008},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":46116,\"start\":46109},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":47220,\"start\":47213},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":49796,\"start\":49789},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":52379,\"start\":52372},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":53394,\"start\":53387},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":53414,\"start\":53406},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":53958,\"start\":53951},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54799,\"start\":54791},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54877,\"start\":54869},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":55842,\"start\":55834},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":58073,\"start\":58066},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":58701,\"start\":58693}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1400,\"start\":1388},{\"attributes\":{\"n\":\"2.\"},\"end\":6074,\"start\":6050},{\"attributes\":{\"n\":\"3.\"},\"end\":11850,\"start\":11845},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11879,\"start\":11853},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13367,\"start\":13347},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":13826,\"start\":13803},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":17731,\"start\":17695},{\"attributes\":{\"n\":\"3.2.3.\"},\"end\":19232,\"start\":19193},{\"end\":21510,\"start\":21501},{\"attributes\":{\"n\":\"4.\"},\"end\":24275,\"start\":24264},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24620,\"start\":24587},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27396,\"start\":27377},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":27421,\"start\":27399},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":28797,\"start\":28775},{\"attributes\":{\"n\":\"4.2.3.\"},\"end\":32864,\"start\":32857},{\"attributes\":{\"n\":\"5.\"},\"end\":34133,\"start\":34123},{\"end\":37049,\"start\":37015},{\"end\":37898,\"start\":37836},{\"end\":39194,\"start\":39145},{\"end\":41122,\"start\":41084},{\"end\":42415,\"start\":42381},{\"end\":44815,\"start\":44772},{\"end\":44858,\"start\":44818},{\"end\":46578,\"start\":46566},{\"end\":46624,\"start\":46581},{\"end\":49260,\"start\":49222},{\"end\":52514,\"start\":52472},{\"end\":52563,\"start\":52517},{\"end\":54248,\"start\":54162},{\"end\":57911,\"start\":57876},{\"end\":59601,\"start\":59574},{\"end\":59761,\"start\":59751},{\"end\":59911,\"start\":59908},{\"end\":63752,\"start\":63749},{\"end\":64330,\"start\":64326},{\"end\":64453,\"start\":64442},{\"end\":64845,\"start\":64841},{\"end\":65481,\"start\":65475},{\"end\":66088,\"start\":66078},{\"end\":66398,\"start\":66388},{\"end\":66731,\"start\":66721},{\"end\":66768,\"start\":66767},{\"end\":67742,\"start\":67739},{\"end\":69355,\"start\":69346},{\"end\":69736,\"start\":69727},{\"end\":69846,\"start\":69839},{\"end\":70325,\"start\":70316},{\"end\":70712,\"start\":70703},{\"end\":72888,\"start\":72881},{\"end\":76436,\"start\":76433},{\"end\":77042,\"start\":77039},{\"end\":79171,\"start\":79162},{\"end\":80209,\"start\":80200},{\"end\":80573,\"start\":80564},{\"end\":81829,\"start\":81820},{\"end\":82515,\"start\":82506},{\"end\":84733,\"start\":84723},{\"end\":85363,\"start\":85353},{\"end\":86343,\"start\":86333},{\"end\":87426,\"start\":87416},{\"end\":88701,\"start\":88691}]", "table": "[{\"end\":69725,\"start\":69613},{\"end\":70701,\"start\":70400},{\"end\":72879,\"start\":70786},{\"end\":76431,\"start\":75947},{\"end\":79160,\"start\":78852},{\"end\":80198,\"start\":79825},{\"end\":80562,\"start\":80260},{\"end\":81818,\"start\":80679},{\"end\":82504,\"start\":81905},{\"end\":84721,\"start\":82618},{\"end\":85351,\"start\":84816},{\"end\":86331,\"start\":85443},{\"end\":87414,\"start\":86425},{\"end\":88689,\"start\":87505},{\"end\":89508,\"start\":88745},{\"end\":91399,\"start\":90054},{\"end\":101409,\"start\":92742}]", "figure_caption": "[{\"end\":59749,\"start\":59604},{\"end\":59871,\"start\":59763},{\"end\":59906,\"start\":59874},{\"end\":60231,\"start\":59913},{\"end\":60441,\"start\":60234},{\"end\":61003,\"start\":60444},{\"end\":63747,\"start\":61006},{\"end\":64324,\"start\":63754},{\"end\":64440,\"start\":64333},{\"end\":64839,\"start\":64455},{\"end\":65473,\"start\":64846},{\"end\":66076,\"start\":65483},{\"end\":66386,\"start\":66090},{\"end\":66719,\"start\":66400},{\"end\":66765,\"start\":66733},{\"end\":67737,\"start\":66769},{\"end\":69301,\"start\":67744},{\"end\":69344,\"start\":69304},{\"end\":69613,\"start\":69357},{\"end\":69837,\"start\":69738},{\"end\":70314,\"start\":69848},{\"end\":70400,\"start\":70327},{\"end\":70786,\"start\":70714},{\"end\":73200,\"start\":72890},{\"end\":75947,\"start\":73203},{\"end\":77037,\"start\":76438},{\"end\":78511,\"start\":77044},{\"end\":78852,\"start\":78514},{\"end\":79825,\"start\":79173},{\"end\":80260,\"start\":80211},{\"end\":80679,\"start\":80575},{\"end\":81905,\"start\":81831},{\"end\":82618,\"start\":82517},{\"end\":84816,\"start\":84736},{\"end\":85443,\"start\":85366},{\"end\":86425,\"start\":86346},{\"end\":87505,\"start\":87429},{\"end\":88745,\"start\":88704},{\"end\":90054,\"start\":89511},{\"end\":92742,\"start\":91402}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4461,\"start\":4455},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13694,\"start\":13686},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14714,\"start\":14707},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20691,\"start\":20685},{\"end\":21474,\"start\":21468},{\"end\":26647,\"start\":26641},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37659,\"start\":37652},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":42207,\"start\":42184},{\"end\":43401,\"start\":43395},{\"end\":43657,\"start\":43651},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":43788,\"start\":43782},{\"end\":43819,\"start\":43813}]", "bib_author_first_name": "[{\"end\":101827,\"start\":101826},{\"end\":101836,\"start\":101835},{\"end\":101846,\"start\":101845},{\"end\":101860,\"start\":101859},{\"end\":102113,\"start\":102112},{\"end\":102122,\"start\":102121},{\"end\":102135,\"start\":102134},{\"end\":102144,\"start\":102143},{\"end\":102154,\"start\":102153},{\"end\":102168,\"start\":102167},{\"end\":102419,\"start\":102418},{\"end\":102431,\"start\":102430},{\"end\":102441,\"start\":102440},{\"end\":102454,\"start\":102453},{\"end\":102714,\"start\":102713},{\"end\":102724,\"start\":102723},{\"end\":102965,\"start\":102964},{\"end\":102974,\"start\":102973},{\"end\":102986,\"start\":102985},{\"end\":103188,\"start\":103187},{\"end\":103201,\"start\":103200},{\"end\":103208,\"start\":103207},{\"end\":103427,\"start\":103426},{\"end\":103440,\"start\":103439},{\"end\":103442,\"start\":103441},{\"end\":103454,\"start\":103453},{\"end\":103463,\"start\":103462},{\"end\":103470,\"start\":103469},{\"end\":103700,\"start\":103699},{\"end\":103707,\"start\":103706},{\"end\":103715,\"start\":103714},{\"end\":103725,\"start\":103721},{\"end\":103732,\"start\":103731},{\"end\":103740,\"start\":103739},{\"end\":103747,\"start\":103746},{\"end\":103951,\"start\":103950},{\"end\":103960,\"start\":103959},{\"end\":103969,\"start\":103968},{\"end\":103975,\"start\":103974},{\"end\":104146,\"start\":104145},{\"end\":104158,\"start\":104157},{\"end\":104168,\"start\":104167},{\"end\":104180,\"start\":104179},{\"end\":104422,\"start\":104421},{\"end\":104424,\"start\":104423},{\"end\":104428,\"start\":104427},{\"end\":104686,\"start\":104685},{\"end\":104700,\"start\":104696},{\"end\":104712,\"start\":104711},{\"end\":104928,\"start\":104927},{\"end\":104944,\"start\":104943},{\"end\":104954,\"start\":104953},{\"end\":104966,\"start\":104965},{\"end\":104975,\"start\":104974},{\"end\":104988,\"start\":104987},{\"end\":104998,\"start\":104997},{\"end\":105006,\"start\":105005},{\"end\":105016,\"start\":105015},{\"end\":105030,\"start\":105029},{\"end\":105032,\"start\":105031},{\"end\":105334,\"start\":105333},{\"end\":105344,\"start\":105343},{\"end\":105356,\"start\":105355},{\"end\":105580,\"start\":105579},{\"end\":105590,\"start\":105589},{\"end\":105600,\"start\":105599},{\"end\":105970,\"start\":105965}]", "bib_author_last_name": "[{\"end\":101833,\"start\":101828},{\"end\":101843,\"start\":101837},{\"end\":101857,\"start\":101847},{\"end\":101870,\"start\":101861},{\"end\":102119,\"start\":102114},{\"end\":102132,\"start\":102123},{\"end\":102141,\"start\":102136},{\"end\":102151,\"start\":102145},{\"end\":102165,\"start\":102155},{\"end\":102173,\"start\":102169},{\"end\":102428,\"start\":102420},{\"end\":102438,\"start\":102432},{\"end\":102451,\"start\":102442},{\"end\":102464,\"start\":102455},{\"end\":102721,\"start\":102715},{\"end\":102731,\"start\":102725},{\"end\":102971,\"start\":102966},{\"end\":102983,\"start\":102975},{\"end\":102993,\"start\":102987},{\"end\":103198,\"start\":103189},{\"end\":103205,\"start\":103202},{\"end\":103214,\"start\":103209},{\"end\":103437,\"start\":103428},{\"end\":103451,\"start\":103443},{\"end\":103460,\"start\":103455},{\"end\":103467,\"start\":103464},{\"end\":103476,\"start\":103471},{\"end\":103704,\"start\":103701},{\"end\":103712,\"start\":103708},{\"end\":103719,\"start\":103716},{\"end\":103729,\"start\":103726},{\"end\":103737,\"start\":103733},{\"end\":103744,\"start\":103741},{\"end\":103752,\"start\":103748},{\"end\":103758,\"start\":103754},{\"end\":103957,\"start\":103952},{\"end\":103966,\"start\":103961},{\"end\":103972,\"start\":103970},{\"end\":103984,\"start\":103976},{\"end\":104155,\"start\":104147},{\"end\":104165,\"start\":104159},{\"end\":104177,\"start\":104169},{\"end\":104190,\"start\":104181},{\"end\":104694,\"start\":104687},{\"end\":104709,\"start\":104701},{\"end\":104718,\"start\":104713},{\"end\":104941,\"start\":104929},{\"end\":104951,\"start\":104945},{\"end\":104963,\"start\":104955},{\"end\":104972,\"start\":104967},{\"end\":104985,\"start\":104976},{\"end\":104995,\"start\":104989},{\"end\":105003,\"start\":104999},{\"end\":105013,\"start\":105007},{\"end\":105027,\"start\":105017},{\"end\":105040,\"start\":105033},{\"end\":105341,\"start\":105335},{\"end\":105353,\"start\":105345},{\"end\":105363,\"start\":105357},{\"end\":105587,\"start\":105581},{\"end\":105597,\"start\":105591},{\"end\":105607,\"start\":105601},{\"end\":105614,\"start\":105609},{\"end\":105978,\"start\":105971}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232273825},\"end\":102020,\"start\":101766},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":235422593},\"end\":102387,\"start\":102022},{\"attributes\":{\"id\":\"b2\"},\"end\":102629,\"start\":102389},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":59695337},\"end\":102930,\"start\":102631},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49744838},\"end\":103103,\"start\":102932},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15610473},\"end\":103371,\"start\":103105},{\"attributes\":{\"id\":\"b6\"},\"end\":103651,\"start\":103373},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":235367622},\"end\":103921,\"start\":103653},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":214611655},\"end\":104089,\"start\":103923},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202779680},\"end\":104338,\"start\":104091},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":117034671},\"end\":104637,\"start\":104340},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":232097557},\"end\":104842,\"start\":104639},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":226278105},\"end\":105293,\"start\":104844},{\"attributes\":{\"id\":\"b13\"},\"end\":105501,\"start\":105295},{\"attributes\":{\"id\":\"b14\"},\"end\":105782,\"start\":105503},{\"attributes\":{\"id\":\"b15\"},\"end\":105912,\"start\":105784},{\"attributes\":{\"id\":\"b16\"},\"end\":106044,\"start\":105914}]", "bib_title": "[{\"end\":101824,\"start\":101766},{\"end\":102110,\"start\":102022},{\"end\":102711,\"start\":102631},{\"end\":102962,\"start\":102932},{\"end\":103185,\"start\":103105},{\"end\":103697,\"start\":103653},{\"end\":103948,\"start\":103923},{\"end\":104143,\"start\":104091},{\"end\":104419,\"start\":104340},{\"end\":104683,\"start\":104639},{\"end\":104925,\"start\":104844}]", "bib_author": "[{\"end\":101835,\"start\":101826},{\"end\":101845,\"start\":101835},{\"end\":101859,\"start\":101845},{\"end\":101872,\"start\":101859},{\"end\":102121,\"start\":102112},{\"end\":102134,\"start\":102121},{\"end\":102143,\"start\":102134},{\"end\":102153,\"start\":102143},{\"end\":102167,\"start\":102153},{\"end\":102175,\"start\":102167},{\"end\":102430,\"start\":102418},{\"end\":102440,\"start\":102430},{\"end\":102453,\"start\":102440},{\"end\":102466,\"start\":102453},{\"end\":102723,\"start\":102713},{\"end\":102733,\"start\":102723},{\"end\":102973,\"start\":102964},{\"end\":102985,\"start\":102973},{\"end\":102995,\"start\":102985},{\"end\":103200,\"start\":103187},{\"end\":103207,\"start\":103200},{\"end\":103216,\"start\":103207},{\"end\":103439,\"start\":103426},{\"end\":103453,\"start\":103439},{\"end\":103462,\"start\":103453},{\"end\":103469,\"start\":103462},{\"end\":103478,\"start\":103469},{\"end\":103706,\"start\":103699},{\"end\":103714,\"start\":103706},{\"end\":103721,\"start\":103714},{\"end\":103731,\"start\":103721},{\"end\":103739,\"start\":103731},{\"end\":103746,\"start\":103739},{\"end\":103754,\"start\":103746},{\"end\":103760,\"start\":103754},{\"end\":103959,\"start\":103950},{\"end\":103968,\"start\":103959},{\"end\":103974,\"start\":103968},{\"end\":103986,\"start\":103974},{\"end\":104157,\"start\":104145},{\"end\":104167,\"start\":104157},{\"end\":104179,\"start\":104167},{\"end\":104192,\"start\":104179},{\"end\":104427,\"start\":104421},{\"end\":104431,\"start\":104427},{\"end\":104696,\"start\":104685},{\"end\":104711,\"start\":104696},{\"end\":104720,\"start\":104711},{\"end\":104943,\"start\":104927},{\"end\":104953,\"start\":104943},{\"end\":104965,\"start\":104953},{\"end\":104974,\"start\":104965},{\"end\":104987,\"start\":104974},{\"end\":104997,\"start\":104987},{\"end\":105005,\"start\":104997},{\"end\":105015,\"start\":105005},{\"end\":105029,\"start\":105015},{\"end\":105042,\"start\":105029},{\"end\":105343,\"start\":105333},{\"end\":105355,\"start\":105343},{\"end\":105365,\"start\":105355},{\"end\":105589,\"start\":105579},{\"end\":105599,\"start\":105589},{\"end\":105609,\"start\":105599},{\"end\":105616,\"start\":105609},{\"end\":105980,\"start\":105965}]", "bib_venue": "[{\"end\":101876,\"start\":101872},{\"end\":102182,\"start\":102175},{\"end\":102416,\"start\":102389},{\"end\":102767,\"start\":102733},{\"end\":102999,\"start\":102995},{\"end\":103223,\"start\":103216},{\"end\":103424,\"start\":103373},{\"end\":103767,\"start\":103760},{\"end\":103990,\"start\":103986},{\"end\":104199,\"start\":104192},{\"end\":104479,\"start\":104431},{\"end\":104724,\"start\":104720},{\"end\":105046,\"start\":105042},{\"end\":105331,\"start\":105295},{\"end\":105577,\"start\":105503},{\"end\":105829,\"start\":105784},{\"end\":105963,\"start\":105914}]"}}}, "year": 2023, "month": 12, "day": 17}