{"id": 257206115, "updated": "2023-10-05 03:43:39.316", "metadata": {"title": "Towards Stable Test-Time Adaptation in Dynamic Wild World", "authors": "[{\"first\":\"Shuaicheng\",\"last\":\"Niu\",\"middle\":[]},{\"first\":\"Jiaxiang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhiquan\",\"last\":\"Wen\",\"middle\":[]},{\"first\":\"Yaofo\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Peilin\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Mingkui\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.12400", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/Niu00WCZT23", "doi": "10.48550/arxiv.2302.12400"}}, "content": {"source": {"pdf_hash": "f849b873e94f28e1f2a3e1dc4d7bef17eb64adab", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.12400v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5a2ca062f8b543856e31e2ef65bd34b88212b8f6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f849b873e94f28e1f2a3e1dc4d7bef17eb64adab.txt", "contents": "\nPublished as a conference paper at ICLR 2023 TOWARDS STABLE TEST-TIME ADAPTATION IN DYNAMIC WILD WORLD\n\n\nShuaicheng Niu \nJiaxiang Wu \nNational University of Singapore\n\n\nYifan Zhang \nMinistry of Education\nKey Laboratory of Big Data and Intelligent Robot\n\n\nZhiquan Wen \nTencent AI Lab\n\n\nYaofo Chen \nTencent AI Lab\n\n\nPeilin Zhao \nNational University of Singapore\n\n\nMingkui Tan mingkuitan@scut.edu.cn \n\nSouth China University of Technology\n\n\n\nPazhou Laboratory 5\n\n\nPublished as a conference paper at ICLR 2023 TOWARDS STABLE TEST-TIME ADAPTATION IN DYNAMIC WILD WORLD\n\nTest-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, i.e., assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios. The source code is available at https://github.com/mr-eggplant/SAR.\n\nINTRODUCTION\n\nDeep neural networks achieve excellent performance when training and testing domains follow the same distribution (He et al., 2016;Choi et al., 2018). However, when domain shifts exist, deep networks often struggle to generalize. Such domain shifts usually occur in real applications, since test data may unavoidably encounter natural variations or corruptions (Hendrycks & Dietterich, 2019;Koh et al., 2021), such as the weather changes (e.g., snow, frost, fog), sensor degradation (e.g., Gaussian noise, defocus blur), and many other reasons. Unfortunately, deep models can be sensitive to the above shifts and suffer from severe performance degradation even if the shift is mild (Recht et al., 2018). However, deploying a deep model on test domains with distribution shifts is still an urgent demand, and model adaptation is needed in these cases.  Figure 1: An illustration of practical/wild test-time adaptation (TTA) scenarios, in which prior online TTA methods may degrade severely. The accuracy of Tent (Wang et al., 2021) is measured on ImageNet-C of level 5 with ResNet50-BN (15 mixed corruptions in (a) and Gaussian in (b-c)).\n\nTTA has been shown boost model robustness to domain shifts significantly. However, its excellent performance is often obtained under some mild test settings, e.g., adapting with a batch of test samples that have the same distribution shift type and randomly shuffled label distribution (see Figure 1 ). In the complex real world, test data may come arbitrarily. As shown in Figure 1 , the test scenario may meet: i) mixture of multiple distribution shifts, ii) small test batch sizes (even single sample), iii) the ground-truth test label distribution Q t (y) is online shifted and Q t (y) may be imbalanced at each time-step t. In these wild test settings, online updating a model by existing TTA methods may be unstable, i.e., failing to help or even harming the model's robustness.\n\nTo stabilize wild TTA, one immediate solution is to recover the model weights after each time adaptation of a sample or mini-batch, such as MEMO  and episodic Tent (Wang et al., 2021). Meanwhile, DDA (Gao et al., 2022) provides a potentially effective idea to address this issue: rather than model adaptation, it seeks to transfer test samples to the source training distribution (via a trained diffusion model (Dhariwal & Nichol, 2021)), in which all model weights are frozen during testing. However, these methods cannot cumulatively exploit the knowledge of previous test samples to boost adaptation performance, and thus obtain limited results when there are lots of test samples. In addition, the diffusion model in DDA is expected to have good generalization ability and can project any possible target shifts to the source data. Nevertheless, this is hard to be satisfied as far as it goes, e.g., DDA performs well on noise shifts while less competitive on blur and weather (see Table 2). Thus, how to stabilize online TTA under wild test settings is still an open question.\n\nIn this paper, we first point out that the batch norm (BN) layer (Ioffe & Szegedy, 2015) is a key obstacle since under the above wild scenarios the mean and variance estimation in BN layers will be biased. In light of this, we further investigate the effects of norm layers in TTA (see Section 4) and find that pre-trained models with batch-agnostic norm layers (i.e., group norm (GN) (Wu & He, 2018) and layer norm (LN) (Ba et al., 2016)) are more beneficial for stable TTA. However, TTA on GN/LN models does not always succeed and still has many failure cases. Specifically, GN/LN models optimized by online entropy minimization (Wang et al., 2021) tend to occur collapse, i.e., predicting all samples to a single class (see Figure 2), especially when the distribution shift is severe. To address this issue, we propose a sharpness-aware and reliable entropy minimization method (namely SAR). Specifically, we find that indeed some noisy samples that produce gradients with large norms harm the adaptation and thus result in model collapse. To avoid this, we filter partial samples with large and noisy gradients out of adaptation according to their entropy. For the remaining samples, we introduce a sharpness-aware learning scheme to ensure that the model weights are optimized to a flat minimum, thereby being robust to the large and noisy gradients/updates.\n\n\nMain Findings and Contributions.\n\n(1) We analyze and empirically verify that batch-agnostic norm layers (i.e., GN and LN) are more beneficial than BN to stable test-time adaptation under wild test settings, i.e., mix domain shifts, small test batch sizes and online imbalanced label distribution shifts (see Figure 1).\n\n(2) We further address the model collapse issue of test-time entropy minimization on GN/LN models by proposing a sharpness-aware and reliable (SAR) optimization scheme, which jointly minimizes the entropy and the sharpness of entropy of those reliable test samples. SAR is simple yet effective and enables online test-time adaptation stabilized under wild test settings.\n\n\nPRELIMINARIES\n\nWe revisit two main categories of test-time adaptation methods in this section for the convenience of further analyses, and put detailed related work discussions into Appendix A due to page limits.\n\nTest-time Training (TTT). Let f \u0398 (x) denote a model trained on D train = {(x i , y i )} N i=1 with parameter \u0398, where x i \u2208 X train (the training data space) and y i \u2208 C (the label space). The goal of test-time adaptation (Sun et al., 2020;Wang et al., 2021) is to boost f \u0398 (x) on out-of-distribution test samples D test = {x j } M j=1 , where x j \u2208 X test (testing data space) and X test = X train . Sun et al. (2020) first propose the TTT pipeline, in which at training phase a model is trained on source D train via both cross-entropy L CE and self-supervised rotation prediction (Gidaris et al., 2018) L S :\nmin \u0398 b ,\u0398c,\u0398s E x\u2208Dtrain [L CE (x; \u0398 b , \u0398 c ) + L S (x; \u0398 b , \u0398 s )],(1)\nwhere \u0398 b is the task-shared parameters (shadow layers), \u0398 c and \u0398 s are task-specific parameters (deep layers) for L CE and L S , respectively. At testing phase, given a test sample x, TTT first updates the model with self-supervised task: \u0398 b \u2190 arg min \u0398 b L S (x; \u0398 b , \u0398 s ) and then use the updated model weights \u0398 b to perform final prediction via f (x; \u0398 b , \u0398 c ).\n\nFully Test-time Adaptation (TTA). The pipeline of TTT needs to alter the original model training process, which may be infeasible when training data are unavailable due to privacy/storage concerns. To avoid this, Wang et al. (2021) propose fully TTA, which adapts arbitrary pre-trained models for a given test mini-batch by conducting entropy minimization (Tent): min \u2212 c\u0177 c log\u0177 c wher\u00ea y c =f \u0398 (c|x) and c denotes class c. This method is more efficient than TTT as shown in Table 6.\n\n\nSTABLE ADAPTATION BY TEST ENTROPY AND SHARPNESS MINIMIZATION\n\nTest-time Adaptation (TTA) in Dynamic Wild World. Although prior TTA methods have exhibited great potential for out-of-distribution generalization, its success may rely on some underlying test prerequisites (as illustrated in Figure 1): 1) test samples have the same distribution shift type; 2) adapting with a batch of samples each time, 3) the test label distribution is uniform during the whole online adaptation process, which, however, are easy to be violated in the wild world. In wild scenarios (Figure 1 ), prior methods may perform poorly or even fail. In this section, we seek to analyze the underlying reasons why TTA fails under wild testing scenarios described in Figure 1 from a unified perspective (c.f. Section 3.1) and then propose associated solutions (c.f. Section 3.2).\n\n\nWHAT CAUSES UNSTABLE TEST-TIME ADAPTATION?\n\nWe first analyze why wild TTA fails by investigating the norm layer effects in TTA and then dig into the unstable reasons for entropy-based methods with batch-agnostic norms, e.g., group norm.\n\nBatch Normalization Hinders Stable TTA. In TTA, prior methods often conduct adaptation on pre-trained models with batch normalization (BN) layers (Ioffe & Szegedy, 2015), and most of them are built upon BN statistics adaptation (Schneider et al., 2020;Nado et al., 2020;Khurana et al., 2021;Wang et al., 2021;Niu et al., 2022a;Hu et al., 2021;. Specifically, for a layer with d-dimensional input x = x (1) . . . x (d) , the batch normalized output are:\ny (k) = \u03b3 (k) x (k) + \u03b2 (k) , where x (k) = x (k) \u2212 E x (k) Var x (k) .\nHere, \u03b3 (k) and \u03b2 (k) are learnable affine parameters. BN adaptation methods calculate mean E[x (k) ] and variance Var[x (k) ] over (a batch of) test samples. However, in wild TTA, all three practical adaptation settings (in Figure 1) in which TTA may fail will result in problematic mean and variance estimation. First, BN statistics indeed represent a distribution and ideally each distribution should have its own statistics. Simply estimating shared BN statistics of multiple distributions from mini-batch test samples unavoidably obtains limited performance, such as in multi-task/domain learning (Wu & Johnson, 2021). Second, the quality of estimated statistics relies on the batch size, and it is hard to use very few samples (i.e., small batch size) to estimate it accurately. Third, the imbalanced label shift will also result in biased BN statistics towards some specific classes in the dataset. Based on the above, we posit that batch-agnostic norm layers, i.e., agnostic to the way samples are grouped into a batch, are more suitable for performing TTA, such as group norm (GN) (Wu & He, 2018) and layer norm (LN) (Ba et al., 2016). We devise our method based on GN/LN models in Section 3.2.\n\nTo verify the above claim, we empirically investigate the effects of different normalization layers (including BN, GN, and LN) in TTA (including TTT and Tent) in Section 4. From the results, we observe that models equipped with GN and LN are more stable than models with BN when Figure 2: Failure case analyses (a-c) of online test-time entropy minimization (Wang et al., 2021). (a) and (b) record the model predictions during online adaptation. (c) illustrates how gradients norm evolves with and without model collapse. (d) investigates the relationship between the sample's entropy and gradients norm. All experiments are conducted on shuffled ImageNet-C of Gaussian noise with ResNet50 (GN), and a larger (severity) level denotes a more severe distribution shift.\n\nperforming online test-time adaptation under three practical test settings (in Figure 1) and have fewer failure cases. The detailed empirical studies are put in Section 4 for the coherence of presentation.\n\nOnline Entropy Minimization Tends to Result in Collapsed Trivial Solutions, i.e., Predict All Samples to the Same Class. Although TTA performs more stable on GN and LN models, it does not always succeed and still faces several failure cases (as shown in Section 4). For example, entropy minimization (Tent) on GN models (ResNet50-GN) tends to collapse, especially when the distribution shift extent is severe. In this paper, we aim to stabilize online fully TTA under various practical test settings. To this end, we first analyze the failure reasons, in which we find models are often optimized to collapse trivial solutions. We illustrate this issue in the following.\n\nDuring the online adaptation process, we record the predicted class and the gradients norm (produced by entropy loss) of ResNet50-GN on shuffled ImageNet-C of Gaussian noise. By comparing Figures 2 (a) and (b), entropy minimization is shown to be unstable and may occur collapse when the distribution shift is severe (i.e., severity level 5). From Figure 2 (a), as the adaptation goes by, the model tends to predict all input samples to the same class, even though these samples have different ground-truth classes, called model collapse. Meanwhile, we notice that along with the model starts to collapse the 2 -norm of gradients of all trainable parameters suddenly increases and then degrades to almost 0 (as shown in Figure 2 (c)), while on severity level 3 the model works well and the gradients norm keep in a stable range all the time. This indicates that some test samples produce large gradients that may hurt the adaptation and lead to model collapse.\n\n\nSHARPNESS-AWARE AND RELIABLE TEST-TIME ENTROPY MINIMIZATION\n\nBased on the above analyses, two most straightforward solutions to avoid model collapse are filtering out test samples according to the sample gradients or performing gradients clipping. However, these are not very feasible since the gradients norms for different models and distribution shift types have different scales, and thus it is hard to devise a general method to set the threshold for sample filtering or gradient clipping (see Section 5.2 for more analyses). We propose our solutions as follows.\n\nReliable Entropy Minimization. Since directly filtering samples with gradients norm is infeasible, we first investigate the relation between entropy loss and gradients norm and seek to remove samples with large gradients based on their entropy. Here, the entropy depends on the model's output class number C and it belongs to (0, ln C) for different models and data. In this sense, the threshold for filtering samples with entropy is easier to select. As shown in Figure 2 (d), selecting samples with small loss values can remove part of samples that have large gradients (area@1) out of adaptation. Formally, let E(x; \u0398) be the entropy of sample x, the selective entropy minimization is defined by:\nmin \u0398 S(x)E(x; \u0398), where S(x) I {E(x;\u0398)<E0} (x).(2)\nHere, \u0398 denote model parameters, I {\u00b7} (\u00b7) is an indicator function and E 0 is a pre-defined parameter. Note that the above criteria will also remove samples within area@2 in Figure 2 (d), in which the samples have low confidence and thus are unreliable (Niu et al., 2022a).\n\nSharpness-aware Entropy Minimization. Through Eqn.\n\n(2), we have removed test samples in area@1&2 in Figure 2 (d) from adaptation. Ideally, we expect to optimize the model via samples only in area@3, since samples in area@4 still have large gradients and may harm the adaptation. However, it is hard to further remove the samples in area@4 via a filtering scheme. Alternatively, . Experiments are conducted on ImageNet-C of Gaussian noise. We report mean and standard deviation of 3 runs with different random seeds. 'na' denotes no adapt accuracy. Note that except for Vit-LN, the standard deviation is too small to display in the figures.\n\nwe seek to make the model insensitive to the large gradients contributed by samples in area@4.\n\nHere, we encourage the model to go to a flat area of the entropy loss surface. The reason is that a flat minimum has good generalization ability and is robust to noisy/large gradients, i.e., the noisy/large updates over the flat minimum would not significantly affect the original model loss, while a sharp minimum would. To this end, we jointly minimize the entropy and the sharpness of entropy loss by:\nmin \u0398 E SA (x; \u0398), where E SA (x; \u0398) max 2\u2264\u03c1 E(x; \u0398 + ).(3)\nHere, the inner optimization seeks to find a weight perturbation in a Euclidean ball with radius \u03c1 that maximizes the entropy. The sharpness is quantified by the maximal change of entropy between \u0398 and \u0398 + . This bi-level problem encourages the optimization to find flat minima. To address problem (3), we follow SAM (Foret et al., 2021) that first approximately solves the inner optimization via first-order Taylor expansion, i.e., * (\u0398) arg max\n2\u2264\u03c1 E(x; \u0398 + ) \u2248 arg max 2\u2264\u03c1 E(x; \u0398) + T \u2207 \u0398 E(x; \u0398) = arg max 2\u2264\u03c1 T \u2207 \u0398 E(x; \u0398).\nThen,\u02c6 (\u0398) that solves this approximation is given by the solution to a classical dual norm problem:\n(\u0398) = \u03c1 sign (\u2207 \u0398 E(x; \u0398)) |\u2207 \u0398 E(x; \u0398)|/ \u2207 \u0398 E(x; \u0398) 2 .(4)\nSubstituting\u02c6 (\u0398) back into Eqn.\n\n(3) and differentiating, by omitting the second-order terms for computation acceleration, the final gradient approximation is:\n\u2207 \u0398 E SA (x; \u0398) \u2248 \u2207 \u0398 E(x; \u0398) \u0398+\u02c6 (\u0398) .(5)\nOverall Optimization. In summary, our sharpness-aware and reliable entropy minimization is:\nmi\u00f1 \u0398 S(x)E SA (x; \u0398),(6)\nwhere S(x) and E SA (x; \u0398) are defined in Eqns.\n\n(2) and (3) respectively,\u0398 \u2282 \u0398 denote learnable parameters during test-time adaptation. In addition, to avoid a few extremely hard cases that Eqn. (6) may also fail, we further introduce a Model Recovery Scheme. We record a moving average e m of entropy loss values and reset\u0398 to be original once e m is smaller than a small threshold e 0 , since models after occurring collapse will produce very small entropy loss. Here, the additional memory costs are negligible since we only optimize affine parameters in norm layers (see Appendix C.2 for more details). We summarize the details of our method in Algorithm 1 in Appendix B.\n\n\nEMPIRICAL STUDIES OF NORMALIZATION LAYER EFFECTS IN TTA\n\nThis section designs experiments to illustrate how test-time adaptation (TTA) performs on models with different norm layers (including BN, GN and LN) under wild test settings described in Figure 1. We verify two representative methods introduced in Section 2, i.e., self-supervised TTT (Sun et al., 2020) and unsupervised Tent (a fully TTA method) (Wang et al., 2021). Considering that the norm layers are often coupled with mainstream network architectures, we conduct adaptation on ResNet-50-BN (R-50-BN), ResNet-50-GN (R-50-GN) and VitBase-LN (Vit-LN). All adopted model weights are public available and obtained from torchvision or timm repository (Wightman, 2019). Implementation details of experiments in this section can be found in Appendix C.2.  (1) Norm Layer Effects in TTA Under Small Test Batch Sizes. We evaluate TTA methods (TTT and Tent) with different batch sizes (BS), selected from {1, 2, 4, 8, 16, 32, 64}. Due to GPU memory limits, we only report results of BS up to 8 or 16 for TTT (in original TTT BS is 1), since TTT needs to augment each test sample multiple times (for which we set to 20 by following Niu et al. (2022a)).\n\nFrom Figure 3, we have: i) For Tent, compared with R-50-BN, R-50-GN and Vit-LN are less sensitive to small test batch sizes. The adaptation performance of R-50-BN degrades severely when the batch size goes small (<8), while R-50-GN/Vit-LN show stable performance across various batch sizes (Vit-LN on levels 5&3 and R-50-GN on level 3, in subfigures (b)&(d)). It is worth noting that Tent with R-50-GN and Vit-LN not always succeeds and also has failure cases, such as R-50-GN on level 5 (Tent performs worse than no adapt), which is analyzed in Section 3.1. ii) For TTT, all R-50-BN/GN and Vit-LN can perform well under various batch sizes. However, TTT with Vit-LN is very unstable and has a large variance over different runs, showing that TTT+VitBase is very sensitive to different sample orders. Here, TTT performs well with R-50-BN under batch size 1 is mainly benefited from TTT applying multiple data augmentations to a single sample to form a mini-batch.\n\n(2) Norm Layer Effects in TTA Under Mixed Distribution Shifts. We evaluate TTA methods on models with different norm layers when test data come from multiple shifted domains simultaneously. We compare 'no adapt', 'avg. adapt' (the average accuracy of adapting on each domain separately) and 'mix adapt' (adapting on mixed and shifted domains) accuracy on ImageNet-C consisting of 15 corruption types. The larger accuracy gap between 'mix adapt' and 'avg. adapt' indicates the more sensitive to mixed distribution shifts.\n\nFrom Figure 4, we have: i) For both Tent and TTT, R-50-GN and Vit-LN perform more stable than R-50-BN under mix domain shifts. Specifically, the mix adapt accuracy of R-50-BN is consistently poor than the average adapt accuracy across different severity levels (in all subfigures (a-d)). In contrast, R-50-GN and Vit-LN are able to achieve comparable accuracy of mix and average adapt, i.e., TTT on R-50-GN (levels 5&3) and Tent on Vit-LN (level 3). ii) For R-50-GN and Vit-LN, TTT performs more stable than Tent. To be specific, Tent gets 3/4 failure cases (R-50-GN on levels 5&3, Vit-LN on level 5), which is more than that of TTT. iii) The same as Section 4 (1), TTT on Vit-LN has large variances over multiple runs, showing TTT+Vit-LN is sensitive to different sample orders.\n\n(3) Norm Layer Effects in TTA Under Online Imbalanced Label Shifts. As in Figure 1 (c), during the online adaptation process, the label distribution Q t (y) at different time-steps t may be different (online shift) and imbalanced. To evaluate this, we first simulate this imbalanced label distribution shift by adjusting the order of input samples (from a test set) as follows.\n\nOnline Imbalanced Label Distribution Shift Simulation. Assuming that we have totally T time-steps and T equals to the class number C. We set the probability vector Q t (y) = [q 1 , q 2 , ...,\nq C ], where q c = q max if c = t and q c = q min (1 \u2212 q max )/(C \u2212 1) if c = t.\nHere, q max /q min denotes the imbalance ratio. Then, at each t \u2208 {1, 2, ..., T =C}, we sample M images from the test set according to Q t (y). Based on ImageNet-C (Gaussian noise), we construct a new testing set that has online imbalanced label distribution shifts with totally 100(M ) \u00d7 1000(T ) images. Note that we pre-shuffle the class orders in ImageNet-C, since we cannot know which class will come in practice.\n\nFrom Figure 5, we have: i) For Tent, R-50-GN and Vit-LN are less sensitive than R-50-BN to online imbalanced label distribution shifts (see subfigures (b)&(d)). Specifically, the adaptation accuracy of R-50-BN (levels 5&3) degrades severely as the imbalance ratio increases. In contrast, R-50-GN and Vit-LN have the potential to perform stably under various imbalance ratios (e.g., R-50-GN and Vit-LN on level 3). ii) For TTT, all R-50-BN/GN and Vit-LN perform relatively stable under label shifts, except for TTT+Vit-LN has large variances. The adaptation accuracy will also degrade but not very severe as the imbalance ratio increases. iii) Tent with GN is more sensitive to the extent of distribution shift than BN. Specifically, for imbalanced ratio 1 (all Q t (y) are uniform) and severity level 5, Tent+R-50-GN fails and performs poorer than no adapt, while Tent+R-50-BN works well.\n\n(4) Overall Observations. Based on all the above results, we have: i) R-50-GN and Vit-LN are more stable than R-50-BN when performing TTA under wild test settings (see Figure 1). However, they do not always succeed and still suffer from several failure cases. ii) R-50-GN is more suitable for self-supervised TTT than Vit-LN, since TTT+Vit-LN is sensitive to different sample orders and has large variances over different runs. iii) Vit-LN is more suitable for unsupervised Tent than R-50-GN, since Tent+R-50-GN is easily to collapse, especially when the distribution shift is severe. Table 1: Efficiency comparison for processing 50,000 images (Gaussian noise, level 5 on ImageNet-C) via a single V100 GPU on ResNet50-GN. Method GPU time MEMO  55,980 secs DDA (Gao et al., 2022) 146,220 secs TTT (Sun et al., 2020) 3,600 secs Tent (Wang et al., 2021) 110 secs EATA (Niu et al., 2022a) 114 secs\n\n\nCOMPARISON WITH STATE-OF-THE-ARTS\n\n\nSAR (ours) 115 secs\n\n\nDataset and Methods.\n\nWe conduct experiments based on ImageNet-C (Hendrycks & Dietterich, 2019), a large-scale and widely used benchmark for out-of-distribution generalization. It contains 15 types of 4 main categories (noise, blur, weather, digital) corrupted images and each type has 5 severity levels. We compare our SAR with the following state-of-the-art methods. DDA (Gao et al., 2022) performs input adaptation at test time via a diffusion model. MEMO  minimizes marginal entropy over different augmented copies w.r.t. a given test sample. Tent (Wang et al., 2021) and EATA (Niu et al., 2022a) are two entropy based online fully test-time adaptation (TTA) methods.\n\nModels and Implementation Details. We conduct experiments on ResNet50-BN/GN and VitBase-LN that are obtained from torchvision or timm (Wightman, 2019). For our SAR, we use SGD as the update rule, with a momentum of 0.9, batch size of 64 (except for the experiments of batch size=1), and learning rate of 0.00025/0.001 for ResNet/Vit models. The threshold E 0 in Eqn. (2) is set to 0.4\u00d7 ln 1000 by following EATA (Niu et al., 2022a). \u03c1 in Eqn. (3) is set by the default value 0.05 in Foret et al. (2021). For trainable parameters of SAR during TTA, following Tent (Wang et al., 2021), we adapt the affine parameters of group/layer normalization layers in ResNet50-GN/VitBase-LN. More details and hyper-parameters of compared methods are put in Appendix C.2.\n\n\nROBUSTNESS TO CORRUPTION UNDER VARIOUS WILD TEST SETTINGS\n\nResults under Online Imbalanced Label Distribution Shifts. As illustrated in Section 4, as the imbalance ratio q max /q min increases, TTA degrades more and more severe. Here, we make comparisons under the most difficult case: q max /q min =\u221e, i.e., test samples come in class order. We evaluate all methods under different corruptions via the same sample sequence for fair comparisons.\n\nFrom Table 2, our SAR achieves the best results in average of 15 corruption types over ResNet50-GN and VitBase-LN, suggesting its effectiveness. It is worth noting that Tent works well for many corruption types on VitBase-LN (e.g., defocus and motion blur) and ResNet50-GN (e.g., pixel), while consistently fails on ResNet50-BN. This further verifies our observations in Section 4 that entropy minimization on LN/GN models has the potential to perform well under online imbalanced label distribution shifts. Meanwhile, Tent also suffers from many failure cases, e.g., VitBase-LN on shot noise and snow. For these cases, our SAR works well. Moreover, EATA has fewer failure cases than Tent and achieves higher average accuracy, which indicates that the weight regularization is somehow able to alleviate the model collapse issue. Nonetheless, the performance of EATA is still inferior to our SAR, e.g., 49.9% vs. 58.0% (ours) on VitBase-LN regarding average accuracy. Table 2: Comparisons with state-of-the-art methods on ImageNet-C (severity level 5) under ONLINE IMBALANCED LABEL SHIFTS (imbalance ratio = \u221e) regarding Accuracy (%). \"BN\"/\"GN\"/\"LN\" is short for Batch/Group/Layer normalization. The bold number indicates the best result.  Table 4: Comparisons with state-of-the-art methods on ImageNet-C (severity level 5) with BATCH SIZE=1 regarding Accuracy (%). \"BN\"/\"GN\"/\"LN\" is short for Batch/Group/Layer normalization.  Results under Mixed Distribution Shifts. We evaluate different methods on the mixture of 15 corruption types (total of 15\u00d750,000 images) at different severity levels (5&3). From Table 3, our SAR performs best consistently regarding accuracy, suggesting its effectiveness. Tent fails (occurs collapse) on ResNet50-GN levels 5&3 and VitBase-LN level 5 and thus achieves inferior accuracy than no adapt model, showing the instability of long-range online entropy minimization. Compared with Tent, although MEMO and DDA achieve better results, they rely on much more computation (inefficient at test time) as in Table 1, and DDA also needs to alter the training process (diffusion model training). By comparing Tent and EATA (both of them are efficient entropy-based), EATA achieves good results but it needs to pre-collect a set of indistribution test samples (2,000 images in EATA) to compute Fisher importance for regularization and then adapt, which sometimes may be infeasible in practice. Unlike EATA, our SAR does not need such samples and obtains better results than EATA.\n\nResults under Batch Size = 1. From Table 4, our SAR achieves the best results in many cases. It is worth noting that MEMO and DDA are not affected by small batch sizes, mix domain shifts, or online imbalanced label shifts. They achieve stable/same results under these settings since they reset (or fix) the model parameters after the adaptation of each sample. However, the computational complexity of these two methods is much higher than SAR (see Table 1) and only obtain limited performance gains since they cannot exploit the knowledge from previously seen images. Although EATA performs better than our SAR on ResNet50-GN, it relies on pre-collecting 2,000 additional in-distribution samples (while we do not). Moreover, our SAR consistently outperforms EATA in other cases, see batch size 1 results on VitBase-LN, Tables 2-3, and Tables 8-9 in Appendix. Table 5: Effects of components in SAR. We report the Accuracy (%) on ImageNet-C (level 5) under ONLINE IMBALANCED LABEL SHIFTS (imbalance ratio q max /q min = \u221e). \"reliable\" and \"sharpness-aware (sa)\" denote Eqn.\n\n(2) and Eqn. (3), \"recover\" denotes the model recovery scheme.  Figure 6: Comparison with gradient clipping. Results on VitBase-LN, ImageNet-C, shot noise, severity level 5, online imbalanced (ratio = \u221e) label shift.\n\nAccuracy is calculated over all previous test samples.\n\nComparison with Gradient Clipping. As mentioned in Section 3.2, gradient clipping is a straightforward solution to alleviate model collapse. Here, we compare our SAR with two variants of gradient clip, i.e., by value and by norm. From Figure 6, for both two variants, it is hard to set a proper threshold \u03b4 for clipping, since the gradients for different models and test data have different scales and thus the \u03b4 selection would be sensitive. We carefully select \u03b4 on a specific test set (shot noise level 5). Then, we select a very small \u03b4 to make gradient clip work, i.e., clip by value 0.001 and by norm 0.1. Nonetheless, the performance gain over \"no adapt\" is very marginal, since the small \u03b4 would limit the learning ability of the model and in this case the clipped gradients may point in a very different direction from the true gradients. However, a large \u03b4 fails to stabilize the adaptation process and the accuracy will degrade after the model collapses (e.g., clip by value 0.005 and by norm 1.0). In contrast, SAR does not need to tune such a parameter and achieve significant improvements than gradient clipping.\n\nEffects of Components in SAR. From Table 5, compared with pure entropy minimization, the reliable entropy in Eqn. (2)   Sharpness of Loss Surface. We visualize the loss surface by adding perturbations to model weights, as done in Li et al. (2018b). We plot Figure 7 via the model weights obtained after the adaptation on the whole test set. By comparing Figures 7 (a) and (b), the area (the deepest blue) within the lowest loss contour line of our SAR is larger than Tent, showing that our solution is flatter and thus is more robust to noisy/large gradients.\n\n\nCONCLUSIONS\n\nIn this paper, we seek to stabilize online test-time adaptation (TTA) under wild test settings, i.e., mix shifts, small batch, and imbalanced label shifts. To this end, we first analyze and conduct extensive empirical studies to verify why wild TTA fails. Then, we point out that batch norm acts as a crucial obstacle to stable TTA. Meanwhile, though batch-agnostic norm (i.e., group and layer norm) performs more stably under wild settings, they still suffer from many failure cases. To address these failures, we propose a sharpness-aware and reliable entropy minimization method ( \n\n\nREPRODUCIBILITY STATEMENT\n\nIn this work, we implement all methods (all compared methods and our SAR) with different models (ResNet50-BN, ResNet50-GN, VitBase-LN) on the ImageNet-C/R and VisDA-2021 datasets.\n\nReproducing all the results in our paper depends on the following three aspects:\n\n1. DATASET. The first paragraph of Section 5 and Appendix C.1 provide the details of the adopted datasets and the download url. 2. MODELS. All adopted models (with the pre-trained weights) for test-time adaptation are publicly available. Specifically, ResNet50-BN is from torchvision, ResNet50-GN and VitBase-LN are from timm repository (Wightman, 2019). Appendix C.2 provides the download url of them. 3. PROTOCOLS OF EACH METHOD. The second paragraph of Section 5 and Appendix C.2 provides the implementation details of all compared methods and our SAR. We reproduce all compared methods based on the code from their official GitHub, for which the download url is provided (in Appendix C.2) following each method introduction. The source code of SAR has been made publicly available. \n\n\nREFERENCES\n\n\nA RELATED WORK\n\nWe relate our SAR to existing adaptation methods without and with target data, sharpness-aware optimization, online learning methods, and EATA (Niu et al., 2022a).\n\nAdaptation without Target Data. The problem of conquering distribution shifts has been studied in a number of works at training time, including domain generalization Li et al., 2018a;Dou et al., 2019), increasing the training dataset size (Orhan, 2019), various data augmentation techniques (Lim et al., 2019;Hendrycks et al., 2020;Li et al., 2021;Hendrycks et al., 2021;Yao et al., 2022), to name just a new. These methods aim to pre-anticipate or simulate the possible shifts of test data at training time, so that the training distribution can cover the possible shifts of test data. However, pre-anticipating all possible test shifts at training time may be infeasible and these training strategies are often more computationally expensive. Instead of improving generalization ability at training time, we conquer test shifts by directly learning from test data.\n\nAdaptation with Target Data. We divide the discussion on related methods that exploit target data into 1) unsupervised domain adaptation (adapt offline) and 2) test-time adaptation (adapt online).\n\n\u2022 Unsupervised domain adaptation (UDA). Conventional UDA jointly optimizes on the labeled source and unlabeled target data to mitigate distribution shifts, such as devising a domain discriminator to align source and target domains at feature level (Pei et al., 2018;Saito et al., 2018;Zhang et al., 2020b;a) and aligning the prototypes of source and target domains through a contrastive learning manner (Lin et al., 2022). Recently, source-free UDA methods have been proposed to resolve the adaptation problem when source data are absent, such as generative-based methods that generate source images or prototypes from the model (Li et al., 2020;Kundu et al., 2020;Qiu et al., 2021), and information maximization (Liang et al., 2020). These methods adapt models on a whole test set, in which the adaptation is offline and often requires multiple training epochs, and thus are hard to be deployed on online testing scenarios.\n\n\u2022 Test-time adaptation (TTA). According to whether alter training, TTA methods can be mainly categorized into two groups. i) Test-Time Training (TTT) (Sun et al., 2020) jointly optimizes a source model with both supervised and self-supervised losses, and then conducts self-supervised learning at test time. The self-supervised losses can be rotation prediction (Gidaris et al., 2018) in TTT or contrastive-based objectives (Chen et al., 2020) in TTT++  and MT3 (Bartler et al., 2022), etc. ii) Fully Test-Time Adaptation (Wang et al., 2021;Niu et al., 2022a;Hong et al., 2023) does not alter the training process and can be applied to any pre-trained model, including adapting the statistics in batch normalization layers (Schneider et al., 2020;Hu et al., 2021;Khurana et al., 2021;Lim et al., 2023;Zhao et al., 2023), unsupervised entropy minimization (Wang et al., 2021;Niu et al., 2022a;, prediction consistency maximization Chen et al., 2022a), top-k classification boosting (Niu et al., 2022b), etc.\n\nThough effective at handling test shifts, prior TTA methods are shown to be unstable in the online adaptation process and sensitive to when test data are insufficient (small batch sizes), from mixed domains, have imbalanced and online shifted label distribution (see Figure 1). Here, it is worth noting that methods like MEMO  and DDA (Gao et al., 2022) are not affected under the above 3 scenarios, since MEMO resets the model parameters after each sample adaptation and DDA performs input adaptation via diffusion (in which the model weights are frozen during testing). However, these methods can not exploit the knowledge learned from previously seen samples and thus obtain limited performance gains. Moreover, the heavy data augmentations and diffusion in MEMO and DDA are computationally expensive and inefficient at test time (see Table 6). In this work, we analyze why online TTA may fail under the above practical test settings and propose associated solutions to make TTA stable under various wild test settings.\n\nSharpness-aware Minimization (SAM). SAM (Foret et al., 2021) optimizes both a supervised objective (e.g., cross-entropy) and the sharpness of loss surface, aiming to find a flat minimum that has good generalization ability (Hochreiter & Schmidhuber, 1997). SAM and its variants (Kwon et al., 2021;Zheng et al., 2021;Du et al., 2022;Chen et al., 2022b) have shown outstanding performance on several deep learning benchmarks. In this work, when we analyze the failure reasons of test-time entropy minimization, we find that some noisy samples that produce gradients with large norms harm the adaptation and thus lead to model collapse. To alleviate this, we propose to minimize the sharpness of the test-time entropy loss surface so that the online model update is robust to those noisy/large gradients.\n\nOnline Learning (OL). OL (Hoi et al., 2021;Chowdhury & Gopalan, 2017;Zhao et al., 2011) conducts model learning from a sequence of data samples one by one at a time, which is common in many real-world applications (e.g., social web recommendation). According to the supervision type, OL can be categorized into three groups: i) Supervised methods (Rakhlin et al., 2010;Shalev-Shwartz et al., 2012) obtain supervision at the end of each online learning iteration, ii) Semisupervised methods (Zhang & Hoi, 2019) can obtain supervision from only partial samples, e.g., online active learning (Zhao & Hoi, 2013; selects informative samples to query the ground-truth label for the model update, iii) Unsupervised methods (Bhatnagar et al., 2014) can not obtain any supervision during the whole online learning process. In this sense, test-time adaptation (TTA) (Sun et al., 2020;Wang et al., 2021) online updates models with only unlabeled test data and thus falls into the third category. However, unlike unsupervised OL that mainly aims to learn representations or clusters (Ren et al., 2021), TTA seeks to boost the performance of any pre-trained model on out-of-distribution test samples.\n\nComparison with EATA (Niu et al., 2022a). Although both EATA and our SAR include a step to remove samples via entropy, their motivations behind this step are different. EATA seeks to improve the adaptation efficiency via sample entropy selection. In our SAR, we discover that some noisy gradients with large norms may hurt the adaptation and thus result in model collapse under wild test settings. To remove these gradients, we exploit an alternative metric (i.e., entropy), which helps to remove partial noisy gradients with large norms. However, this is still insufficient for achieving stable TTA (see ablation results in Table 5). Thus, we further introduce the sharpnessaware optimization and a model recovery scheme. With these three strategies, our SAR performs stably under wild test settings.\n\n\nB PSEUDO CODE OF SAR\n\nIn this appendix, we provide the pseudo-code of our SAR method. From Algorithm 1, for each test sample x j , we first apply the reliable sample filtering scheme (refer to lines 3-6) to it to determine whether it will be used to update the model. If x j is reliable, we will optimize the model via the sharpness-aware entropy loss of x j (refer to lines 7-10). Specifically, we first calculate the optimal weight perturbation\u02c6 (\u0398) based on the gradient \u2207\u0398E(x j ; \u0398), and then update the model with approximate gradients g = \u2207\u0398E(x j ; \u0398)| \u0398+\u02c6 (\u0398) . Lastly, we exploit a recovery scheme to enable the model to work well even under a few extremely hard cases (refer to lines 11-13). Specifically, when the moving average value e m of entropy loss is smaller than e 0 (indicating that the model occurs to collapse), we will recover the model parameters\u0398 to its original/initial value. In this paper, we mainly evaluate the out-of-distribution generalization ability of all methods on a large-scale and widely used benchmark, namely ImageNet-C 1 (Hendrycks & Dietterich, 2019). ImageNet-C is constructed by corrupting the original ImageNet (Deng et al., 2009) test set. The corruption (as shown in Figure 8) consists of 15 different types, i.e., Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression, in which each corruption type has 5 different severity levels and the larger severity level means more severe distribution shift. Then, we further conduct experiments on ImageNet-R ( face of minor input changes. Now in order to approximate C, E and these robustness measures, we designed a set of corruptions and perturbations which are frequently encountered in natural images.\n\nWe will refer to these as \"common\" corruptions and perturbations. These common corruptions and perturbations are available in the form of IMAGENET-C and IMAGENET-P.\n\n\nTHE IMAGENET-C AND IMAGENET-P ROBUSTNESS BENCHMARKS\n\n\nTHE DATA OF IMAGENET-C AND IMAGENET-P\n\nIMAGENET-C Design. The IMAGENET-C benchmark consists of 15 diverse corruption types applied to validation images of ImageNet. The corruptions are drawn from four main categoriesnoise, blur, weather, and digital-as shown in Figure 1. Research that improves performance on this benchmark should indicate general robustness gains, as the corruptions are diverse and numerous. Each corruption type has five levels of severity since corruptions can manifest themselves at varying intensities. Appendix A gives an example of the five different severity levels for impulse noise. Real-world corruptions also have variation even at a fixed intensity. To simulate these, we introduce variation for each corruption when possible. For example, each fog cloud is unique to each image. These algorithmically generated corruptions are applied to the ImageNet (Deng et al., 2009) validation images to produce our corruption robustness dataset IMAGENET-C. The dataset can be downloaded or re-created by visiting https://github.com/hendrycks/robustness. IMAGENET-C images are saved as lightly compressed JPEGs; this implies an image corrupted by Gaussian noise is also slightly corrupted by JPEG compression. Our benchmark tests networks with IMAGENET-C images, but networks should not be trained on these images. Networks should be trained on datasets such as ImageNet and not be trained on IMAGENET-C corruptions. To enable further experimentation, we designed an extra corruption type for each corruption category (Appendix B), and we provide CIFAR-10-C, TINY IMAGENET-C, IMAGENET 64 \u00d7 64-C, and Inception-sized editions. Overall, the IMAGENET-C dataset consists of 75 corruptions, all applied to ImageNet validation images for testing a pre-existing network.\n\n3 Figure 8: Visualizations of different corruption types in ImageNet corruption benchmark, which are taken from the original paper of ImageNet-C (Hendrycks & Dietterich, 2019).\n\n\nC.2 MORE EXPERIMENTAL PROTOCOLS\n\nAll pre-trained models involved in our paper for test-time adaptation are publicly available, including ResNet50-BN 2 obtained from torchvision library, ResNet-50-GN 3 and VitBase-LN 4 and ConvNeXt-LN 5 obtained from timm repository (Wightman, 2019). We summarize the detailed characteristics of all involved methods in Table 6 and introduce their implementation details in the following.\n\n\nSAR (Ours).\n\nWe use SGD as the update rule, with a momentum of 0.9, batch size of 64 (except for the experiments of batch size = 1), and learning rate of 0.00025/0.001 for ResNet/Vit models. The learning rate for batch size = 1 is set to (0.00025/16) for ResNet models and (0.001/32) for Vit models. The threshold E 0 in Eqn.\n\n(2) is set to 0.4\u00d7 ln 1000 by following EATA (Niu et al., 2022a).   (Gao et al., 2022) 50,000\u00d72 0 50,000 diffusion 146,220 seconds TTT (Sun et al., 2020) 50,000\u00d721 50,000\u00d720 rotation augmentation 3,600 seconds Tent (Wang et al., 2021) 50,000 50,000 n/a 110 seconds EATA (Niu et al., 2022a) 50,000 26,196 regularizer 114 seconds SAR (ours) 50,000 + 12,710 12,710\u00d72 Eqn. (4) 115 seconds entropy loss values with a moving average factor of 0.9 for e m , and the reset threshold e 0 is set to 0.2. For learnable parameters, we only update affine parameters in normalization layers by following Tent (Wang et al., 2021). However, since the top/deep layers are more sensitive and more important to the original model than shallow layers as mentioned in (Mummadi et al., 2021;Choi et al., 2022), we freeze the top layers and update the affine parameters of layer or group normalization in the remaining shallow layers. Specifically, for ResNet50-GN that has 4 layer groups (layer1, 2, 3, 4), we freeze the layer4. For ViTBase-LN that has 11 blocks groups (blocks1-11), we freeze blocks9, blocks10, blocks11.\n\nTTT 6 (Sun et al., 2020). For fair comparisons, we seek to compare all methods based on the same model weights. However, TTT alters the model training process and requires the model contains a self-supervised rotation prediction branch for test-time training. Therefore, we modify TTT so that it can be applied to any pre-trained model. Specifically, given a pre-trained model, we add a new branch (random initialized) from the end of a middle layer (2nd layer group of ResNet-50-GN and 6th blocks group of VitBase-LN) for the rotation prediction task. We first freeze all original parameters of the pre-trained model and train the newly added branch for 10 epochs on the original ImageNet training set. Here, we apply an SGD optimizer, with a momentum of 0.9, an initial learning rate of 0.1/0.005 for ResNet50-GN/VitBase-LN, and decrease it at epochs 4 and 7 by decreasing factor 0.1. Then, we take the newly obtained model (with two branches) as the base model to perform test-time training. During the test-time training phase, we use SGD as the update rule with a learning rate of 0.001 for ResNet0-GN (following TTT) and 0.0001 for VitBase-LN, and the data augmentation size is set to 20 (following Niu et al. (2022a)).\n\nTent 7 (Wang et al., 2021). We follow all hyper-parameters that are set in Tent unless it does not provide. Specifically, we use SGD as the update rule, with a momentum of 0.9, batch size of 64 (except for the experiments of batch size = 1 and effects of small test batch sizes (in Section 4)), and learning rate of 0.00025/0.001 for ResNet/Vit models. The learning rate for batch size = 1 is set to (0.00025/32) for ResNet models and (0.001/64) for Vit models. The trainable parameters are all affine parameters of batch normalization layers.\n\nEATA 8 (Niu et al., 2022a). We follow all hyper-parameters that are set in EATA unless it does not provide. Specifically, the entropy constant E 0 (for reliable sample identification) is set to 0.1 \u00d7 ln 1000. The for redundant sample identification is set to 0.05. The trade-off parameter \u03b2 for entropy loss and regularization loss is set to 2,000. The number of pre-collected in-distribution test samples for Fisher importance calculation is 2,000. The update rule is SGD, with a momentum of 0.9, batch size of 64 (except for the experiments of batch size = 1 and effects of small test batch sizes (in Section 4)), and learning rate of 0.00025/0.001 for ResNet/Vit models. The learning rate for batch size = 1 is set to (0.00025/32) for ResNet models and (0.001/64) for Vit models. The trainable parameters are all affine parameters of batch normalization layers.\n\nMEMO 9 . We follow all hyper-parameters that are set in MEMO. Specifically, we use the AugMix (Hendrycks et al., 2020) as a set of data augmentations and the augmentation size is set to 64. For Vit models, the optimizer is AdamW (Loshchilov & Hutter, 2018), with learning rate 0.00001 and weight decay 0.01. For ResNet models, the optimizer is SGD, with learning rate 0.00025 and no weight decay. The trainable parameters are the entire model.\n\nDDA 10 (Gao et al., 2022). We reproduce DDA according to its official GitHub repository and use the default hyper-parameters.\n\nMore Details on Experiments in Section 4: Normalization Layer Effects in TTA. In Section 4, we investigate the effects of TTT and Tent with models that have different norm layers under {small test batch sizes, mixed distribution shifts, online imbalanced label distribution shifts}. For each experiment, we only consider one of the three above test settings. To be specific, for experiments regarding batch size effects (Section 4 (1)), we only tune the batch size and the test set does not contain multiple types of distribution shifts and its label distribution is always uniform. For experiments of mixed domain shifts (Section 4 (2)), the test samples come from the mixture of 15 corruption types, while the batch size is 64 for Tent and 1 for TTT, and the label distribution of test data is always uniform. For experiments of online label shifts (Section 4 (3)), the label distribution of test data is online shifted and imbalanced, while the BS is 64 for Tent and 1 for TTT, and test data only consist of one corruption type. Moreover, it is worth noting that we re-scale the learning rate for entropy minimization (Tent) according to the batch size, since entropy minimization is sensitive to the learning rate and a fixed learning rate often fails to work well. Specifically, the learning rate is re-scaled as (0.00025/32) \u00d7 BS IF BS < 32 ELSE 0.00025 for ResNet models and (0.001/64) \u00d7 BS for Vit models. Compared with Tent, the single sample adaptation method TTT is not very sensitive to the learning rate, and thus we set the same learning rate for various batch sizes. We also provide the results of TTT under different batch sizes with dynamic re-scaled learning rates in Table 7.  (Sun et al., 2020) with different models (different norm layers). The learning rate is dynamically re-scaled by 0.001\u00d7BS. We report the accuracy (%) on ImageNet-C with Gaussian noise and severity level 5. We provide more results regarding online imbalanced label distribution shift (imbalance ratio = \u221e) of all compared methods in Table 8. The results are consistent with that of the main paper (severity level 5), and our SAR performs best in the average of 15 different corruption types. It is worth noting that DDA achieves competitive results under noise corruptions while performing worse for other corruption types. The reason is that the diffusion model used in DDA for input adaptation is trained via noise diffusion, and thus its generalization ability to diffuse other corruptions is still limited. We provide more results regarding batch size = 1 of all compared methods in Table 9. The results are consistent with that of the main paper (severity level 5), and our SAR performs best in the average of 15 different corruption types. Table 9: Comparisons with state-of-the-art methods on ImageNet-C of severity level 3 under BATCH SIZE=1 regarding Accuracy (%). \"BN\"/\"GN\"/\"LN\" is short for Batch/Group/Layer normalization. \n\n\nE ADDITIONAL RESULTS ON IMAGENET-R AND VISDA-2021\n\nWe further conduct experiments on ImageNet-R under two wild test settings: online imbalanced label distribution shifts (in Table 10) and batch size = 1 (in Table 11). The overall results are consistent with that on ImageNet-C: 1) ResNet50-GN and VitBase-LN perform more stable than ResNet50-BN; 2) Compared with Tent and EATA, SAR achieves the best performance on ResNet50-GN and VitBase-LN.   We further ablate the effects of components in our SAR in Figure 9 by plotting the changes of gradients norms of our methods with different components. From the results, both the reliable (Eqn. 2) and sharpness-aware (sa) (Eqn. 3) modules together ensure the model's gradients keep in a normal range during the whole online adaptation process, which is consistent with our previous results in Section 5.2. Results on VitBase, ImageNet-C, shot noise, severity level 5, online imbalance label shift (imbalance ratio = \u221e). \"reliable\" and \"sharpness-aware (sa)\" are short for Eqn.\n\n(2) and Eqn. (3), respectively.\n\nWe also conduct more ablation experiments under the wild test settings of \"batch size (BS)=1\" in Table 13 and \"mix domain shifts\" in Table 14. The results are generally consistent with that in Table 5. Both the reliable entropy and sharpness-aware optimization work together to stabilize online TTA. It is worth noting that only in VitBase-LN under BS=1 the model recovery scheme is activated and improves the average accuracy from 55.7% to 56.4%. Table 13: Effects of components in SAR. We report the Accuracy (%) on ImageNet-C (level 5) under BATCH SIZE=1. \"reliable\" and \"sharpness-aware (sa)\" denote Eqn.\n\n(2) and Eqn. (3), \"recover\" denotes the model recovery scheme.   Figure 7 in the main paper, we have visualized the loss surface of Tent and our SAR on VitBase-LN. In this section, we further provide visualizations of ResNet50-GN. We select a checkpoint at batch 120 to plot the loss surface for Tent, since after batch 120 this model starts to collapse. In this case, the loss (entropy) is hard to degrade and cannot find a proper minimum. For our SAR, the model weights for plotting are obtained after the adaptation on the whole test set. By comparing Figures 10 (a)-(b), SAR helps to stabilize the entropy minimization process and find proper minima.  As we mentioned in Section 3.1, models online optimized by entropy (Wang et al., 2021;Niu et al., 2022a) under wild test scenarios are easy to collapse, i.e., predict all samples to a single class independent of the inputs. To alleviate this, prior methods (Liang et al., 2020;Mummadi et al., 2021) exploit diversity regularization to force the output distribution of samples to be uniform. However, this assumption is unreasonable at test time such as when test data are imbalanced during a period (as in Figure 1 (c)) and this method also relies on a batch of samples (in contrast to Figure 1 (b)).\n\nIn this sense, this strategy is infeasible for our problem. In our paper, we resolve the collapse issue for test-time adaptation from an optimization perspective to make the online adaptation process stabilized.\n\n\nG.2 EFFECTS OF LARGE BATCH SIZES IN BN MODELS UNDER MIX DOMAIN SHIFTS\n\nIn Section 3.1, we mentioned that a standard batch size (e.g., 64 on ImageNet) works well when there is only one type of distribution shift. However, when test data contains multiple shifts, this batch size fails to calculate an accurate mean and variance estimation in batch normalization layers. Here, we investigate the effects of super large batch sizes in this setting. From Table 16, the adapted performance increases as the batch size increases, indicating that a larger batch size helps to estimate statistics more accurately. It is worth noting that the performance on severity level 3 degrades when BS is larger than 1024. This is because we fix the learning rate for various batch sizes and in this sense, BS=1024 may lead to insufficient model updates. Moreover, although enlarging batch sizes is able to boost the performance, the adapt performance is still inferior to the average accuracy of adapting on each corruption type separately (i.e., average adapt). This further emphasizes the necessity of exploiting models with group or layer norm layers to perform test-time entropy minimization. \n\n\nG.3 PERFORMANCE OF TENT WITH CONVNEXT-LN\n\nFor mainstream neural network models, the normalization layers are often coupled with network architecture. Specifically, group norm (GN) and batch norm (BN) are often combined with conventional networks, while layer norm (LN) is more suitable for transformer networks. Therefore, we investigate the layer normalization effects in TTA in Section 4 through VitBase-LN. Here, we conduct more experiments to compare the performance of online entropy minimization (Wang et al., 2021) on ResNet50-BN and ConvNeXt-LN . ConvNeXt is a convolutional network equipped with LN. The authors conduct significant modifications over ResNet to make this LN-based convolutional network work well, such as modifying the architecture (ResNet block to ConvNeXt block, activation functions, etc.), various training strategies (stochastic depth, random erasing, EMA, etc.). From Table 17, Tent+ConvNeXt-LN performs more stable than Tent+ResNet50-BN, but still suffers several failure cases. These results are consistent with that of ResNet50-BN vs. VitBase-LN.\n\n\nG.4 EFFECTIVENESS OF MODEL RECOVERY SCHEME WITH TENT AND EATA\n\nIn this subsection, we apply our Model Recovery scheme to Tent (Wang et al., 2021) and EATA (Niu et al., 2022a). From Table 18, the model recovery indeed helps Tent a lot (e.g., the average accuracy from 22.0% to 26.1% on ResNet50-GN) while its performance gain on EATA is a bit marginal. Compared with Tent+recovery and EATA+recovery, our SAR greatly boosts the adaptation performance, e.g., the average accuracy 26.1% (Tent+recovery) vs. 37.2% (SAR) on ResNet50-GN, suggesting the effectiveness of our proposed SAR. Table 18: Results of combining model recovery scheme with Tent and EATA. We report the Accuracy (%) on ImageNet-C severity level 5 under online imbalanced label distribution shifts and the imbalance ratio is \u221e. \n\nFigure 3 :\n3Batch size effects of different TTA methods under different models (different normalization layers)\n\nFigure 4 :Figure 5 :\n45Performance of TTA methods on different models (different norm layers) under the mixture of 15 different corruption types (ImageNet-C). We report mean&stdev. Performance of TTA methods with different models (different norm layers) under online imbalanced label distribution shifts on ImageNet-C (Gaussian noise). We report mean&stdev. results of 3 runs. Note that except for VitBase-LN, the stdev. is too small to display in the figures.\n\nFigure 7 :\n7Loss (entropy) surface. Models are learned on ImageNet-C Gaussian noise, level 5.\n\nFigure 1 :\n1Hendrycks et al., 2021)  andVisDA-2021 (Bashkirova  et al., 2022 to verify the effectiveness of our method. ImageNet-R contains 30,000 images with various artistic renditions of 200 ImageNet classes, which are primarily collected from Flickr and filtered by Amazon MTurk annotators. VisDA-2021 collects images from ImageNet-O/R/C and Ob-jectNet(Barbu et al., 2019). The domain shifts in VisDA-2021 include the changes in artistic visual styles, textures, viewpoints and corruptions. Our IMAGENET-C dataset consists of 15 types of algorithmically generated corruptions from noise, blur, weather, and digital categories. Each type of corruption has five levels of severity, resulting in 75 distinct corruptions. See different severity levels in Appendix B.\n\n\n\u03c1 in Eqn. (3) is set by the default value 0.05 in Foret et al. (2021). For model recovery, we record the\n\nFigure 9 :\n9The evolution of gradients norm during online test-time adaptation.\n\nFigure 10 :\n10Visualization of loss (entropy) surface. Models are learned on ImageNet-C of Gaussian noise with severity level 5. F.3 SENSITIVITY OF \u03c1 IN SAR The hyper-parameter \u03c1 (in Eqn. (3)) in sharpness-aware optimization is not hard to tune in our online test-time adaptation. Following Foret et al. (2021), we set \u03c1 = 0.05 in all experiments, and it works well with different model architectures (ResNet50-BN, ResNet50-GN, VitBase-LN) on different datasets (ImageNet-C, ImageNet-R). Here, we also conduct a sensitivity analysis of \u03c1 in Table 15, in which SAR works well under the range [0.03, 0.1].\n\n\nSAR (ours) 33.1\u00b11.0 36.5\u00b10.4 35.5\u00b11.1 19.2\u00b10.4 19.5\u00b11.2 33.3\u00b10.5 27.7\u00b14.0 23.9\u00b15.1 45.3\u00b10.4 50.1\u00b11.0 71.9\u00b10.1 46.7\u00b10.2 7.1\u00b11.8 52.1\u00b10.5 56.3\u00b10.1 37.2\u00b10.6 SAR (ours) 46.5\u00b13.0 43.1\u00b17.4 48.9\u00b10.4 55.3\u00b10.1 54.3\u00b10.2 58.9\u00b10.1 54.8\u00b10.2 53.6\u00b17.1 46.2\u00b13.5 69.7\u00b10.3 76.2\u00b10.1 66.2\u00b10.3 60.9\u00b10.3 69.6\u00b10.1 66.6\u00b10.1 58.0\u00b10.5Noise \nBlur \nWeather \nDigital \nModel+Method Gauss. \nShot \nImpul. Defoc. Glass Motion Zoom \nSnow \nFrost \nFog \nBrit. \nContr. Elastic \nPixel \nJPEG \nAvg. \nResNet50 (BN) \n2.2 \n2.9 \n1.8 \n17.8 \n9.8 \n14.5 \n22.5 \n16.8 \n23.4 \n24.6 \n59.0 \n5.5 \n17.1 \n20.7 \n31.6 \n18.0 \n\u2022 MEMO \n7.4 \n8.6 \n8.9 \n19.8 \n13.2 \n20.8 \n27.5 \n25.6 \n28.6 \n32.3 \n60.8 \n11.0 \n23.8 \n33.2 \n37.7 \n24.0 \n\u2022 DDA \n32.2 \n33.1 \n32.0 \n14.6 \n16.4 \n16.6 \n24.4 \n20.0 \n25.5 \n17.2 \n52.2 \n3.2 \n35.7 \n41.8 \n45.4 \n27.2 \n\u2022 Tent \n1.2 \n1.4 \n1.4 \n1.0 \n0.9 \n1.2 \n2.6 \n1.7 \n1.8 \n3.6 \n5.0 \n0.5 \n2.6 \n3.2 \n3.1 \n2.1 \n\u2022 EATA \n0.3 \n0.3 \n0.3 \n0.2 \n0.2 \n0.5 \n0.9 \n0.8 \n0.9 \n1.8 \n3.5 \n0.2 \n0.8 \n1.2 \n0.9 \n0.9 \nResNet50 (GN) 17.9 \n19.9 \n17.9 \n19.7 \n11.3 \n21.3 \n24.9 \n40.4 \n47.4 \n33.6 \n69.2 \n36.3 \n18.7 \n28.4 \n52.2 \n30.6 \n\u2022 MEMO \n18.4 \n20.6 \n18.4 \n17.1 \n12.7 \n21.8 \n26.9 \n40.7 \n46.9 \n34.8 \n69.6 \n36.4 \n19.2 \n32.2 \n53.4 \n31.3 \n\u2022 DDA \n42.5 \n43.4 \n42.3 \n16.5 \n19.4 \n21.9 \n26.1 \n35.8 \n40.2 \n13.7 \n61.3 \n25.2 \n37.3 \n46.9 \n54.3 \n35.1 \n\u2022 Tent \n2.6 \n3.3 \n2.7 \n13.9 \n7.9 \n19.5 \n17.0 \n16.5 \n21.9 \n1.8 \n70.5 \n42.2 \n6.6 \n49.4 \n53.7 \n22.0 \n\u2022 EATA \n27.0 \n28.3 \n28.1 \n14.9 \n17.1 \n24.4 \n25.3 \n32.2 \n32.0 \n39.8 \n66.7 \n33.6 \n24.5 \n41.9 \n38.4 \n31.6 \n\u2022 VitBase (LN) \n9.4 \n6.7 \n8.3 \n29.1 \n23.4 \n34.0 \n27.0 \n15.8 \n26.3 \n47.4 \n54.7 \n43.9 \n30.5 \n44.5 \n47.6 \n29.9 \n\u2022 MEMO \n21.6 \n17.4 \n20.6 \n37.1 \n29.6 \n40.6 \n34.4 \n25.0 \n34.8 \n55.2 \n65.0 \n54.9 \n37.4 \n55.5 \n57.7 \n39.1 \n\u2022 DDA \n41.3 \n41.3 \n40.6 \n24.6 \n27.4 \n30.7 \n26.9 \n18.2 \n27.7 \n34.8 \n50.0 \n32.3 \n42.2 \n52.5 \n52.7 \n36.2 \n\u2022 Tent \n32.7 \n1.4 \n34.6 \n54.4 \n52.3 \n58.2 \n52.2 \n7.7 \n12.0 \n69.3 \n76.1 \n66.1 \n56.7 \n69.4 \n66.4 \n47.3 \n\u2022 EATA \n35.9 \n34.6 \n36.7 \n45.3 \n47.2 \n49.3 \n47.7 \n56.5 \n55.4 \n62.2 \n72.2 \n21.7 \n56.2 \n64.7 \n63.7 \n49.9 \n\u2022 \n\n\nSAR (ours) 23.4\u00b10.3 26.6\u00b10.4 23.9\u00b10.0 18.4\u00b10.1 15.4\u00b10.3 28.6\u00b10.3 30.4\u00b10.2 44.9\u00b10.3 44.7\u00b10.2 25.7\u00b10.6 72.3\u00b10.2 44.5\u00b10.1 14.8\u00b12.7 47.0\u00b10.1 56.1\u00b10.0 34.5\u00b10.2 SAR (ours) 40.8\u00b10.4 36.4\u00b10.7 41.5\u00b10.3 53.7\u00b10.2 50.7\u00b10.1 57.5\u00b10.1 52.8\u00b10.3 59.1\u00b10.4 50.7\u00b10.6 68.1\u00b11.4 74.6\u00b10.7 65.7\u00b10.0 57.9\u00b10.1 68.9\u00b10.1 65.9\u00b10.0 56.3\u00b10.1Noise \nBlur \nWeather \nDigital \nModel+Method Gauss. \nShot \nImpul. Defoc. Glass Motion Zoom \nSnow \nFrost \nFog \nBrit. \nContr. Elastic \nPixel \nJPEG \nAvg. \nResNet50 (BN) \n2.2 \n2.9 \n1.9 \n17.9 \n9.8 \n14.8 \n22.5 \n16.9 \n23.3 \n24.4 \n58.9 \n5.4 \n17.0 \n20.6 \n31.6 \n18.0 \n\u2022 MEMO \n7.5 \n8.7 \n8.9 \n19.7 \n13.0 \n20.8 \n27.6 \n25.4 \n28.7 \n32.2 \n60.9 \n11.0 \n23.8 \n32.9 \n37.5 \n23.9 \n\u2022 DDA \n32.1 \n32.8 \n31.8 \n14.7 \n16.6 \n16.6 \n24.2 \n20.0 \n25.4 \n17.2 \n52.1 \n3.2 \n35.7 \n41.5 \n45.3 \n27.3 \n\u2022 Tent \n0.1 \n0.1 \n0.1 \n0.1 \n0.1 \n0.1 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.1 \n0.1 \n0.2 \n0.1 \n0.1 \n\u2022 EATA \n0.1 \n0.1 \n0.1 \n0.1 \n0.1 \n0.1 \n0.2 \n0.2 \n0.2 \n0.1 \n0.2 \n0.1 \n0.1 \n0.2 \n0.1 \n0.1 \nResNet50 (GN) 18.0 \n19.8 \n17.9 \n19.8 \n11.4 \n21.4 \n24.9 \n40.4 \n47.3 \n33.6 \n69.3 \n36.3 \n18.6 \n28.4 \n52.3 \n30.6 \n\u2022 MEMO \n18.5 \n20.5 \n18.4 \n17.1 \n12.6 \n21.8 \n26.9 \n40.4 \n47.0 \n34.4 \n69.5 \n36.5 \n19.2 \n32.1 \n53.3 \n31.2 \n\u2022 DDA \n42.4 \n43.3 \n42.3 \n16.6 \n19.6 \n21.9 \n26.0 \n35.7 \n40.1 \n13.7 \n61.2 \n25.2 \n37.5 \n46.6 \n54.1 \n35.1 \n\u2022 Tent \n2.5 \n2.9 \n2.5 \n13.5 \n3.6 \n18.6 \n17.6 \n15.3 \n23.0 \n1.4 \n70.4 \n42.2 \n6.2 \n49.2 \n53.8 \n21.5 \n\u2022 EATA \n24.8 \n28.3 \n25.7 \n18.1 \n17.3 \n28.5 \n29.3 \n44.5 \n44.3 \n41.6 \n70.9 \n44.6 \n27.0 \n46.8 \n55.7 \n36.5 \n\u2022 VitBase (LN) \n9.5 \n6.7 \n8.2 \n29.0 \n23.4 \n33.9 \n27.1 \n15.9 \n26.5 \n47.2 \n54.7 \n44.1 \n30.5 \n44.5 \n47.8 \n29.9 \n\u2022 MEMO \n21.6 \n17.3 \n20.6 \n37.1 \n29.6 \n40.4 \n34.4 \n24.9 \n34.7 \n55.1 \n64.8 \n54.9 \n37.4 \n55.4 \n57.6 \n39.1 \n\u2022 DDA \n41.3 \n41.1 \n40.7 \n24.4 \n27.2 \n30.6 \n26.9 \n18.3 \n27.5 \n34.6 \n50.1 \n32.4 \n42.3 \n52.2 \n52.6 \n36.1 \n\u2022 Tent \n42.2 \n1.0 \n43.3 \n52.4 \n48.2 \n55.5 \n50.5 \n16.5 \n16.9 \n66.4 \n74.9 \n64.7 \n51.6 \n67.0 \n64.3 \n47.7 \n\u2022 EATA \n29.7 \n25.1 \n34.6 \n44.7 \n39.2 \n48.3 \n42.4 \n37.5 \n45.9 \n60.0 \n65.9 \n61.2 \n46.4 \n58.2 \n59.6 \n46.6 \n\u2022 \n\nTable 3 :\n3Comparisons with stateof-the-arts on ImageNet-C under MIXTURE OF 15 CORRUPTION TYPES regarding Accuracy (%).Model + Method \nLevel 5 \nLevel 3 \n\nResNet50 (BN) \n18.0 \n39.7 \n\u2022 MEMO (Zhang et al., 2022) \n23.9 \n46.2 \n\u2022 DDA (Gao et al., 2022) \n27.3 \n44.2 \n\u2022 Tent (Wang et al., 2021) \n2.3 \n41.1 \n\u2022 EATA (Niu et al., 2022a) \n26.8 \n52.6 \n\nResNet50 (GN) \n30.6 \n54.0 \n\u2022 MEMO (Zhang et al., 2022) \n31.2 \n54.5 \n\u2022 DDA (Gao et al., 2022) \n35.1 \n52.3 \n\u2022 Tent (Wang et al., 2021) \n13.4 \n33.1 \n\u2022 EATA (Niu et al., 2022a) \n38.1 \n56.1 \n\u2022 SAR (ours) \n38.3\u00b10.1 57.4\u00b10.1 \n\nVitBase (LN) \n29.9 \n53.8 \n\u2022 MEMO (Zhang et al., 2022) \n39.1 \n62.1 \n\u2022 DDA (Gao et al., 2022) \n36.1 \n53.2 \n\u2022 Tent (Wang et al., 2021) \n16.5 \n70.2 \n\u2022 EATA (Niu et al., 2022a) \n55.7 \n69.6 \n\u2022 SAR (ours) \n57.1\u00b10.1 70.7\u00b10.1 \n\n\n\n\nShot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Brit. Contr. Elastic Pixel JPEG Avg.Noise \nBlur \nWeather \nDigital \nModel+Method \nGauss. ResNet50 (GN)+Entropy \n3.2 \n4.1 \n4.0 \n17.1 \n8.5 \n27.0 \n24.4 \n17.9 25.5 2.6 72.1 45.8 \n8.2 \n52.2 56.2 24.6 \nreliable \n34.5 \n36.8 36.2 \n19.5 \n3.1 \n33.6 \n14.5 \n20.5 38.3 2.4 71.9 47.0 \n8.3 \n52.1 56.4 31.7 \nreliable+sa \n33.8 \n35.9 36.4 \n19.2 \n18.7 \n33.6 \n24.5 \n23.5 45.2 49.3 71.9 46.6 \n9.2 \n51.6 56.4 37.0 \nreliable+sa+recover \n33.6 \n36.1 36.2 \n19.1 \n18.6 \n33.9 \n24.7 \n22.5 45.7 49.0 71.9 46.6 \n9.2 \n51.5 56.3 37.0 \n\nVitBase (LN)+Entropy \n21.2 \n1.9 \n38.6 \n54.8 \n52.7 \n58.5 \n54.2 \n10.1 14.7 69.6 76.3 66.3 \n59.2 \n69.7 66.8 47.6 \nreliable \n47.8 \n35.7 48.4 \n55.2 \n54.1 \n58.6 \n54.4 \n13.3 21.4 69.5 76.2 66.1 \n60.2 \n69.3 66.7 53.1 \nreliable+sa \n47.9 \n47.6 48.5 \n55.4 \n54.2 \n58.8 \n54.6 \n19.7 22.1 69.4 76.3 66.2 \n60.9 \n69.4 66.6 54.5 \nreliable+sa+recover \n47.9 \n47.6 48.5 \n55.4 \n54.2 \n58.8 \n54.6 \n49.1 48.3 69.4 76.3 66.2 \n60.9 \n69.4 66.6 58.2 \n\n5.2 ABLATE EXPERIMENTS \n\n0 \n500 1000 1500 \n\nOnline Batch \n\n0 \n\n20 \n\n40 \n\nOnline Accuracy (%) \n\nClip by Value \n\nno adapt acc \nno clip \nmax_value=0.01 \nmax_value=0.005 \nmax_value=0.001 \nSAR (ours) \n\n0 \n500 1000 1500 \n\nOnline Batch \n\n0 \n\n20 \n\n40 \n\nClip by Norm \n\nno adapt acc \nno clip \nmax_norm=3.0 \nmax_norm=1.0 \nmax_norm=0.5 \nmax_norm=0.1 \nSAR (ours) \n\n\n\n\nLN, the sa module is also effective, and it helps the model to avoid collapse, e.g., 35.7% \u2192 47.6% on shot noise. With both reliable and sa, our method performs stably except for very few cases, i.e., VitBase-LN on snow and frost. For this case, our model recovery scheme takes effects, i.e., 54.5% \u2192 58.2% on VitBase-LN w.r.t. average accuracy.clearly improves the adaptation performance, i.e., 47.6% \u2192 53.1% on \nVitBase-LN and 24.6% \u2192 31.7% on ResNet50-GN w.r.t. average accuracy. With sharpness-aware \n(sa) minimization in Eqn. (3), the accuracy is further improved, e.g., 31.7%\u219237.0% on ResNet50-\nGN w.r.t. average accuracy. On VitBase-30 60 90 \n\n30 \n\n60 \n\n90 \n\n(a) VitBase-LN, Tent \n\n30 60 90 \n\n30 \n\n60 \n\n90 \n\n(b) VitBase-LN, SAR (Ours) \n\n0.0 \n1.2 \n2.4 \n3.6 \n4.8 \n6.2 \n\n0.0 \n1.2 \n2.4 \n3.6 \n4.8 \n6.2 \n\n\n\n\nSAR) by suppressing the effect of certain noisy test samples with large gradients. Extensive experimental results demonstrate the stability and efficiency of our SAR under wild test settings.ACKNOWLEDGMENTS \n\nThis work was partially supported by the Key Realm R&D Program of Guangzhou 202007030007, \nNational Natural Science Foundation of China (NSFC) 62072190, Ministry of Science and Tech-\nnology Foundation Project 2020AAA0106900, Program for Guangdong Introducing Innovative and \nEnterpreneurial Teams 2017ZT07X183, CCF-Tencent Open Fund RAGR20220108. \n\n\n\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Alexander Bartler, Andre B\u00fchler, Felix Wiewel, Mario D\u00f6bler, and Bin Yang. MT3: meta testtime training for self-supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics, pp. 3080-3090, 2022. Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8789-8797, 2018. Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International Conference on Machine Learning, pp. 844-853, 2017. Jin Gao, Jialing Zhang, Xihui Liu, Evan Shelhamer, Trevor Darrell, and Dequan Wang. Back to the source: Diffusion-driven test-time adaptation. In ICML Workshop on Updatable Machine Learning, 2022. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In International Conference on Learning Representations, 2020. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In IEEE International Conference on Computer Vision, pp. 8320-8329, 2021. More Details on Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 More Experimental Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Comparisons with State-of-the-arts under Batch Size of 1 . . . . . . . . . . . . . . . . . . . 22 F.1 Effects of Components in SAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F.2 Visualization of Loss Surface Learned by SAR . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Sensitivity of \u03c1 in SAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 G.1 More Discussions on Model Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 G.2 Effects of Large Batch Sizes in BN models under Mix Domain Shifts . . . . . . . . . . . . . 26 G.3 Performance of Tent with ConvNeXt-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 G.4 Effectiveness of Model Recovery Scheme with Tent and EATA . . . . . . . . . . . . . . . . . 26Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh \nTenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the \nlimits of object recognition models. In Advances in Neural Information Processing Systems, \nvolume 32, 2019. \n\nDina Bashkirova, Dan Hendrycks, Donghyun Kim, Haojin Liao, Samarth Mishra, Chandramouli \nRajagopalan, Kate Saenko, Kuniaki Saito, Burhan Ul Tayyab, Piotr Teterwak, et al. Visda-2021 \ncompetition: Universal domain adaptation to improve performance on out-of-distribution data. In \nNeurIPS 2021 Competitions and Demonstrations Track, pp. 66-79, 2022. \n\nVasudha Bhatnagar, Sharanjit Kaur, and Sharma Chakravarthy. Clustering data streams using grid-\nbased synopsis. Knowledge and Information Systems, 41(1):127-152, 2014. \n\nDian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In \nIEEE Conference on Computer Vision and Pattern Recognition, pp. 295-305, 2022a. \n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for \ncontrastive learning of visual representations. In International Conference on Machine Learning, \npp. 1597-1607, 2020. \n\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets \nwithout pre-training or strong data augmentations. In International Conference on Learning Rep-\nresentations, 2022b. \n\nSungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation \nvia shift-agnostic weight regularization and nearest source prototypes. In European Conference \non Computer Vision, 2022. \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale \nhierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, \npp. 248-255, 2009. \n\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Ad-\nvances in Neural Information Processing Systems, pp. 8780-8794, 2021. \n\nQi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain general-\nization via model-agnostic learning of semantic features. In Advances in Neural Information \nProcessing Systems, pp. 6447-6458, 2019. \n\nJiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and \nVincent Tan. Efficient sharpness-aware minimization for improved training of neural networks. \nIn International Conference on Learning Representations, 2022. \n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-\ntion for efficiently improving generalization. In International Conference on Learning Represen-\ntations, 2021. \n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural computation, 9(1):1-42, 1997. \n\nSteven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive survey. \nNeurocomputing, 459:249-289, 2021. \n\nJunyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic \ncontinual test-time model adaptation. In International Conference on Learning Representations, \n2023. \n\nXuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam \nLim. Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprint \narXiv:2110.11478, 2021. \n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by \nreducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456, \n2015. \nAPPENDIX \n\nCONTENTS \n\nA Related Work \n16 \n\nB Pseudo Code of SAR \n18 \n\nC More Implementation Details \n19 \n\nC.1 D Additional Results on ImageNet-C of Severity Level 3 \n22 \n\nD.1 Comparisons with State-of-the-arts under Online Imbalanced Label Shift . . . . . . . . . . . 22 \n\nD.2 E Additional Results on ImageNet-R and VisDA-2021 \n23 \n\nF Additional Ablate Results \n24 \n\nG Additional Discussions \n26 \n\n\n\n\nAlgorithm 1: Sharpness-Aware and Reliable Test-Time Entropy Minimization (SAR) Compute gradient approximation: g = \u2207\u0398E(xj; \u0398)| \u0398+\u02c6 (\u0398) ;Input: Test samples Dtest = {xj} M \nj=1 , model f\u0398(\u00b7) with trainable parameters\u0398 \u2282 \u0398, step size \u03b7 > 0, \nneighborhood size \u03c1 > 0, E0 > 0 in Eqn. (2), e0 > 0 for model recovery. \nOutput: Predictions {\u0177j} M \nj=1 . \n1 Initialize\u03980 =\u0398, moving average of entropy em = 0; \n2 for xj \u2208 Dtest do \n\n3 \n\nCompute entropy Ej=E(xj; \u0398) and predict\u0177j=f\u0398(xj); \n\n4 \n\nif Ej > E0 then \n\n5 \n\ncontinue ; \n// reliable entropy minimization (Eqn. 2) \n\n6 \n\nend \n\n7 \n\nCompute gradient \u2207\u0398E(xj; \u0398); \n\n8 \n\nCompute\u02c6 (\u0398) per Eqn. (4); \n\n9 \n\n10 \n\nUpdate\u0398 \u2190\u0398 \u2212 \u03b7g ; \n// sharpness-aware minimization (Eqn. 3) \n\n11 \n\nem=0.9\u00d7em+0.1\u00d7E(xj; \u0398+\u02c6 (\u0398)) if em =0 else E(xj; \u0398+\u02c6 (\u0398)) ; \n// moving average \n\n12 \n\nif em < e0 then \n\n13 \n\nRecover model weights:\u0398 \u2190\u03980 ; \n// model recovery \n\n14 \n\nend \n\n15 end \nC MORE IMPLEMENTATION DETAILS \n\nC.1 MORE DETAILS ON DATASET \n\n\n\nTable 6 :\n6Characteristics of state-of-the-art methods. We evaluate the efficiency of different methods \nwith ResNet-50 (group norm) on ImageNet-C (Gaussian noise, severity level 5), which consists of \ntotally 50,000 images. The real run time is tested via a single V100 GPU. DDA (Gao et al., 2022) \npre-trains an additional diffusion model and then perform input adaptation/diffusion at test time. \n\nMethod \nNeed source data? Online update? \n#Forward \n#Backward \nOther computation \nGPU time (50,000 images) \n\nMEMO (Zhang et al., 2022) \n50,000\u00d765 \n50,000\u00d764 AugMix (Hendrycks et al., 2020) \n55,980 seconds \nDDA \n\nTable 7 :\n7Batch size (BS) effects in TTT\n\nTable 8 :\n8\u00b10.1 60.5 \u00b10.3 60.2 \u00b10.2 47.9 \u00b10.5 36.7 \u00b10.7 58.2 \u00b10.2 49.7 \u00b10.5 57.9 \u00b10.3 53.6 \u00b10.0 65.0 \u00b10.1 76.4 \u00b10.2 71.0 \u00b10.0 67.0 \u00b10.2 65.8 \u00b10.1 67.6 \u00b10.0 59.9 \u00b10.1 \u00b10.1 68.2 \u00b10.1 68.4 \u00b10.2 68.3 \u00b10.2 64.7 \u00b10.0 71.0 \u00b10.2 64.2 \u00b10.3 68.1 \u00b10.1 66.0 \u00b10.1 76.4 \u00b10.1 79.0 \u00b10.1 79.6 \u00b10.1 76.2 \u00b10.3 77.1 \u00b10.1 74.1 \u00b10.2 71.3 \u00b10.1 D.2 COMPARISONS WITH STATE-OF-THE-ARTS UNDER BATCH SIZE OF 1Comparisons with state-of-the-art methods on ImageNet-C of severity level 3 under ON-\nLINE IMBALANCED LABEL DISTRIBUTION SHIFTS (imbalance ratio q max /q min = \u221e) regarding \nAccuracy (%). \"BN\"/\"GN\"/\"LN\" is short for Batch/Group/Layer normalization. \n\nNoise \nBlur \nWeather \nDigital \nModel+Method Gauss. \nShot \nImpul. Defoc. Glass Motion Zoom \nSnow \nFrost \nFog \nBrit. \nContr. Elastic \nPixel \nJPEG \nAvg. \nResNet50 (BN) 27.7 \n25.2 \n25.1 \n37.8 \n16.7 \n37.8 \n35.3 \n35.2 \n32.1 \n46.7 \n69.5 \n46.2 \n55.4 \n46.2 \n59.4 \n39.8 \n\u2022 MEMO \n37.6 \n34.5 \n36.7 \n41.4 \n23.4 \n44.4 \n40.9 \n44.6 \n37.3 \n52.4 \n70.5 \n56.3 \n58.7 \n55.2 \n60.9 \n46.3 \n\u2022 DDA \n49.9 \n50.0 \n49.2 \n33.2 \n31.9 \n38.0 \n36.7 \n35.1 \n34.1 \n35.01 \n64.9 \n33.7 \n59.3 \n53.9 \n59.0 \n44.3 \n\u2022 Tent \n3.4 \n3.2 \n3.2 \n2.3 \n2.0 \n2.4 \n3.4 \n2.4 \n2.4 \n4.6 \n5.4 \n3.0 \n4.8 \n4.6 \n4.5 \n3.4 \n\u2022 EATA \n1.3 \n0.9 \n1.1 \n0.6 \n0.6 \n1.2 \n1.4 \n1.3 \n1.3 \n1.9 \n4.1 \n1.6 \n2.7 \n2.4 \n3.1 \n1.7 \nResNet50 (GN) 54.5 \n52.9 \n53.1 \n44.4 \n21.2 \n49.8 \n39.3 \n54.9 \n54.1 \n55.8 \n75.3 \n69.7 \n59.6 \n59.7 \n66.4 \n54.1 \n\u2022 MEMO \n55.9 \n54.3 \n54.1 \n40.1 \n23.1 \n49.5 \n41.4 \n54.8 \n54.1 \n57.6 \n75.7 \n70.2 \n60.2 \n61.5 \n66.7 \n54.6 \n\u2022 DDA \n61.0 \n61.0 \n60.5 \n39.3 \n37.3 \n46.4 \n39.7 \n47.7 \n48.1 \n29.9 \n69.9 \n57.8 \n62.8 \n60.1 \n63.8 \n52.4 \n\u2022 Tent \n59.1 \n58.6 \n58.3 \n39.0 \n27.9 \n54.7 \n41.1 \n51.3 \n41.4 \n62.0 \n75.2 \n70.1 \n62.3 \n63.7 \n66.4 \n55.4 \n\u2022 EATA \n52.3 \n52.9 \n51.7 \n35.7 \n30.1 \n46.4 \n39.6 \n43.8 \n39.8 \n55.7 \n72.4 \n66.6 \n54.7 \n56.0 \n56.2 \n50.3 \n\u2022 SAR (ours) 60.8 VitBase (LN) \n51.5 \n46.8 \n50.4 \n48.7 \n37.1 \n54.7 \n41.6 \n35.1 \n33.3 \n68.0 \n69.3 \n74.9 \n65.9 \n66.0 \n63.6 \n53.8 \n\u2022 MEMO \n62.1 \n57.9 \n61.5 \n57.2 \n45.6 \n62.0 \n49.9 \n46.5 \n43.1 \n74.1 \n75.8 \n79.7 \n72.6 \n72.3 \n70.6 \n62.1 \n\u2022 DDA \n59.7 \n58.2 \n59.4 \n43.5 \n43.3 \n50.5 \n41.0 \n34.3 \n34.4 \n55.4 \n65.0 \n64.2 \n64.1 \n63.8 \n62.9 \n53.3 \n\u2022 Tent \n68.7 \n68.0 \n68.1 \n68.2 \n63.8 \n70.9 \n63.8 \n67.6 \n41.9 \n76.3 \n78.8 \n79.5 \n75.9 \n76.7 \n73.7 \n69.5 \n\u2022 EATA \n65.3 \n62.6 \n63.6 \n63.0 \n57.1 \n66.3 \n59.3 \n64.5 \n61.0 \n73.3 \n76.9 \n75.9 \n74.2 \n74.8 \n73.1 \n67.4 \n\u2022 SAR (ours) 68.8 \n\n\nSAR (ours) 60.3 \u00b10.1 59.6 \u00b10.1 59.5 \u00b10.1 46.6 \u00b11.1 33.0 \u00b10.5 57.5 \u00b10.1 47.8 \u00b10.1 57.8 \u00b10.2 52.8 \u00b10.1 65.1 \u00b10.1 76.7 \u00b10.1 71.4 \u00b10.1 67.3 \u00b10.2 66.0 \u00b10.1 67.8 \u00b10.0 59.3 \u00b10.1 SAR (ours) 68.5 \u00b10.1 67.8 \u00b10.1 68.0 \u00b10.1 67.8 \u00b10.2 63.1 \u00b10.0 70.7 \u00b10.1 63.5 \u00b10.1 66.9 \u00b10.2 62.8 \u00b12.1 75.8 \u00b10.5 77.7 \u00b10.8 78.4 \u00b10.4 74.7 \u00b11.5 75.7 \u00b10.5 72.7 \u00b11.1 70.3 \u00b10.3Noise \nBlur \nWeather \nDigital \nModel+Method Gauss. \nShot \nImpul. Defoc. Glass Motion Zoom \nSnow \nFrost \nFog \nBrit. \nContr. Elastic \nPixel \nJPEG \nAvg. \nResNet50 (BN) 27.6 \n25.0 \n25.1 \n38.0 \n16.9 \n37.7 \n35.2 \n35.2 \n32.1 \n46.6 \n69.6 \n46.0 \n55.6 \n46.2 \n59.3 \n39.7 \n\u2022 MEMO \n37.5 \n34.3 \n36.6 \n41.2 \n23.3 \n44.2 \n41.0 \n44.5 \n37.4 \n52.3 \n70.5 \n56.0 \n58.7 \n55.0 \n60.8 \n46.2 \n\u2022 DDA \n49.8 \n49.9 \n49.2 \n33.2 \n32.0 \n37.9 \n36.6 \n35.2 \n34.2 \n34.9 \n64.9 \n33.5 \n59.3 \n53.9 \n59.0 \n44.2 \n\u2022 Tent \n0.1 \n0.2 \n0.2 \n0.2 \n0.1 \n0.1 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n\u2022 EATA \n0.2 \n0.2 \n0.1 \n0.2 \n0.1 \n0.2 \n0.2 \n0.1 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \n0.2 \nResNet50 (GN) 54.5 \n52.8 \n53.1 \n44.3 \n21.2 \n49.7 \n39.2 \n54.8 \n54.0 \n55.8 \n75.4 \n69.8 \n59.6 \n59.7 \n66.3 \n54.0 \n\u2022 MEMO \n55.7 \n54.2 \n53.9 \n40.0 \n22.8 \n49.2 \n41.2 \n54.8 \n54.1 \n57.6 \n75.5 \n69.9 \n60.0 \n61.3 \n66.6 \n54.5 \n\u2022 DDA \n61.0 \n60.9 \n60.4 \n39.2 \n37.2 \n46.4 \n39.7 \n47.7 \n48.0 \n29.8 \n70.0 \n58.0 \n62.7 \n60.2 \n63.8 \n52.3 \n\u2022 Tent \n58.8 \n58.5 \n58.7 \n38.2 \n26.8 \n54.9 \n42.6 \n51.6 \n38.8 \n61.9 \n75.3 \n70.0 \n62.3 \n63.6 \n66.3 \n55.2 \n\u2022 EATA \n59.2 \n58.7 \n58.8 \n45.7 \n32.6 \n55.5 \n45.9 \n56.4 \n52.7 \n63.6 \n75.9 \n71.1 \n64.7 \n64.5 \n67.8 \n58.2 \n\u2022 VitBase (LN) \n51.6 \n46.9 \n50.5 \n48.7 \n37.2 \n54.7 \n41.6 \n35.1 \n33.5 \n67.8 \n69.3 \n74.8 \n65.8 \n66.0 \n63.7 \n53.8 \n\u2022 MEMO \n61.9 \n57.7 \n61.4 \n57.0 \n45.4 \n61.8 \n49.8 \n46.6 \n43.1 \n73.9 \n75.7 \n79.6 \n72.6 \n72.1 \n70.5 \n61.9 \n\u2022 DDA \n59.8 \n58.2 \n59.5 \n43.4 \n43.2 \n50.4 \n40.9 \n34.2 \n34.3 \n55.2 \n64.9 \n64.0 \n64.2 \n63.7 \n62.8 \n53.2 \n\u2022 Tent \n67.1 \n66.2 \n66.3 \n66.3 \n60.9 \n69.1 \n61.4 \n65.2 \n60.4 \n75.2 \n78.1 \n78.8 \n74.9 \n75.8 \n72.4 \n69.2 \n\u2022 EATA \n60.7 \n58.5 \n61.6 \n60.1 \n51.8 \n64.2 \n54.8 \n53.3 \n52.6 \n72.5 \n73.6 \n77.9 \n71.3 \n71.3 \n69.7 \n63.6 \n\u2022 \n\nTable 10 :\n10Comparison in terms of accuracy (%) under the wild setting online imbalanced label distribution shifts on ImageNet-R.Method \nResNet50-BN ResNet50-GN VitBase-LN \n\nNo Adapt. \n36.2 \n40.8 \n43.1 \nTent \n6.6 \n41.7 \n45.2 \nEATA \n5.8 \n40.9 \n47.5 \nSAR (ours) \n-\n42.9 \n52.0 \n\n\n\nTable 11 :\n11Comparison in terms of accuracy (%) under the wild setting single sample adaptation (batch size = 1) on ImageNet-R.We also compare our method with online TTA methods (Tent and EATA) on VisDA-2021. Results inTable 12are consistent with that on ImageNet-C and ImageNet-R. Specifically, Tent and EATA fail with the BN model (ResNet50-BN) while performing stable with the GN/LN model (ResNet50-GN/VitBase-LN) under online imbalanced label shifts. Besides, our SAR further improves the adaptation performance of Tent and EATA on ResNet50-GN/VitBase-LN.Method \nResNet50-BN ResNet50-GN VitBase-LN \n\nNo Adapt. \n36.2 \n40.8 \n43.1 \nTent \n0.6 \n42.2 \n40.5 \nEATA \n0.6 \n42.3 \n52.5 \nSAR (ours) \n-\n43.9 \n53.1 \n\n\n\nTable 12 :\n12Classification Accuracy (%) on VisDA-2021 under online imbalanced label distribution shifts and mixed domain shifts.F ADDITIONAL ABLATE RESULTSF.1 EFFECTS OF COMPONENTS IN SARModel \nNo adapt Tent EATA SAR (ours) \n\nResNet50-BN \n36.0 \n9.0 \n7.4 \n-\nResNet50-GN \n43.7 \n42.8 \n40.9 \n44.1 \nVitBase-LN \n44.3 \n49.1 \n49.0 \n52.0 \n\n\n\nShot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Brit. Contr. Elastic Pixel JPEG Avg.Noise \nBlur \nWeather \nDigital \nModel+Method \nGauss. ResNet50 (GN)+Entropy \n3.4 \n4.5 \n4.0 \n16.7 \n5.9 \n27.0 \n29.7 \n16.3 26.6 2.1 72.1 46.4 \n7.5 \n52.5 56.2 24.7 \nreliable \n22.8 \n25.5 23.0 \n18.4 \n14.8 \n27.0 \n28.6 \n40.0 43.3 18.4 71.5 43.1 \n15.2 \n45.6 55.4 32.8 \nreliable+sa \n23.8 \n26.4 24.0 \n18.6 \n15.4 \n28.3 \n30.5 \n44.8 44.8 26.7 72.4 44.5 \n12.2 \n46.9 65.1 34.4 \nreliable+sa+recover \n23.8 \n26.4 23.9 \n18.5 \n15.4 \n28.3 \n30.6 \n44.6 44.9 24.7 72.4 44.4 \n12.3 \n47.0 56.1 34.2 \n\nVitBase (LN)+Entropy \n42.9 \n1.4 \n43.9 \n52.6 \n48.8 \n55.8 \n51.4 \n22.2 20.1 67.0 75.1 64.7 \n53.7 \n67.2 64.5 48.8 \nreliable \n34.8 \n4.2 \n35.5 \n50.5 \n45.9 \n54.0 \n48.6 \n52.5 47.8 65.5 74.5 63.4 \n51.4 \n65.6 63.0 50.5 \nreliable+sa \n40.4 \n37.3 41.2 \n53.6 \n50.6 \n57.3 \n53.0 \n58.7 40.9 68.8 75.9 65.7 \n57.9 \n69.0 66.0 55.7 \nreliable+sa+recover \n40.4 \n37.3 41.2 \n53.6 \n50.6 \n57.3 \n53.0 \n58.7 50.7 68.8 75.4 65.7 \n57.9 \n69.0 66.0 56.4 \n\n\n\nTable 14 :\n14Effects of components in SAR. We report the Accuracy (%) under MIXED DOMAIN SHIFTS, i.e., mixture of 15 corruption types of ImageNet-C with severity level 5. \"reliable\" and \"sharpness-aware (sa)\" denote Eqn. (2) and Eqn. (3), \"recover\" denotes the model recovery scheme.F.2 VISUALIZATION OF LOSS SURFACE LEARNED BY SARModel + Method \nAccuracy (%) \n\nResNet50 (GN)+Entropy \n33.5 \nreliable \n38.1 \nreliable+sa \n38.2 \nreliable+sa+recover \n38.2 \n\nResNet50 (GN)+Entropy \n18.5 \nreliable \n55.2 \nreliable+sa \n57.2 \nreliable+sa+recover \n57.2 \n\n\nTable 15 :\n15Sensitivity of \u03c1 in SAR. We report Accuracy (%) on ImageNet-C (shot noise, severity level 5) under online imbalanced label distribution shifts, where the imbalance ratio is \u221e.G.1 MORE DISCUSSIONS ON MODEL COLLAPSEMethod \nNo adapt Tent SAR (\u03c1=0.01) SAR (\u03c1=0.03) SAR (\u03c1=0.05) SAR (\u03c1=0.07) SAR (\u03c1=0.1) \n\nAccuracy (%) \n6.7 \n1.4 \n40.3 \n47.7 \n47.6 \n47.2 \n47.2 \n\n\nTable 16 :\n16Effects of large batch sizes (BS) in Tent(Wang et al., 2021) with ResNet-50-BN under MIXTURE OF 15 DIFFERENT CORRUPTION TYPES on ImageNet-C. We report accuracy (%).avg. adapt \nmix adapt \nSeverity \nBase \nBS=64 \nBS=32 BS=64 BS=128 BS=256 BS=512 BS=1,024 \n\nLevel = 5 18.0 \n42.6 \n0.9 \n2.3 \n4.0 \n8.3 \n12.4 \n16.4 \nLevel = 3 39.8 \n59.0 \n20.1 \n40.7 \n46.7 \n49.1 \n49.0 \n47.8 \n\n\n\nTable 17 :\n17Results of Tent on ResNet50-BN and ConvNeXt-LN. We report Accuracy (%) on ImageNet-C under online imbalanced label distribution shifts and the imbalance ratio is \u221e. Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Brit. Contr. Elastic Pixel JPEG Avg. Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Brit. Contr. Elastic Pixel JPEG Avg.Noise \nBlur \nWeather \nDigital \nSeverity level 5 \nResNet50-BN \n2.2 \n2.9 \n1.8 \n17.8 \n9.8 \n14.5 \n22.5 \n16.8 23.4 24.6 59.0 \n5.5 \n17.1 \n20.7 31.6 18.0 \n\u2022 Tent (Wang et al., 2021) \n1.2 \n1.4 \n1.4 \n1.0 \n0.9 \n1.2 \n2.6 \n1.7 \n1.8 \n3.6 \n5.0 \n0.5 \n2.6 \n3.2 \n3.1 \n2.1 \nConvNeXt-LN \n52.3 \n52.7 52.3 \n31.7 \n18.7 \n42.5 \n38.1 \n54.2 58.3 50.6 75.6 56.8 \n32.2 \n39.2 60.4 47.7 \n\u2022 Tent (Wang et al., 2021) \n26.3 \n11.9 36.9 \n31.1 \n12.7 \n14.6 \n5.1 \n7.8 \n5.3 \n6.6 79.0 67.6 \n1.5 \n68.4 65.8 29.4 \n\nSeverity level 3 \nResNet50-BN \n27.7 \n25.2 25.1 \n37.8 \n16.7 \n37.8 \n35.3 \n35.2 32.1 46.7 69.5 46.2 \n55.4 \n46.2 59.4 39.8 \n\u2022 Tent (Wang et al., 2021) \n3.4 \n3.2 \n3.2 \n2.3 \n2.0 \n2.4 \n3.4 \n2.4 \n2.4 \n4.6 \n5.4 \n3.0 \n4.8 \n4.6 \n4.5 \n3.4 \nConvNeXt-LN \n71.1 \n69.8 72.4 \n55.2 \n36.6 \n64.3 \n52.8 \n63.2 64.0 66.0 79.4 75.8 \n69.1 \n68.5 71.9 65.4 \n\u2022 Tent (Wang et al., 2021) \n73.4 \n73.5 74.4 \n66.8 \n61.6 \n71.4 \n64.2 \n22.6 39.5 76.8 81.2 78.9 \n76.8 \n75.6 75.8 67.5 \n\n\n\n\nShot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Brit. Contr. Elastic Pixel JPEG Avg.Noise \nBlur \nWeather \nDigital \nModel+Method \nGauss. ResNet50 (GN) \n18.0 \n19.8 17.9 \n19.8 \n11.4 \n21.4 \n24.9 \n40.4 47.3 33.6 69.3 36.3 \n18.6 \n28.4 52.3 30.6 \n\u2022 Tent \n2.6 \n3.3 \n2.7 \n13.9 \n7.9 \n19.5 \n17.0 \n16.5 21.9 1.8 70.5 42.2 \n6.6 \n49.4 53.7 22.0 \n\u2022 Tent+recover \n10.1 \n12.2 10.6 \n13.9 \n8.5 \n19.5 \n20.6 \n24.3 33.5 8.9 70.5 42.2 \n13.5 \n49.4 53.7 26.1 \n\u2022 EATA \n27.0 \n28.3 28.1 \n14.9 \n17.1 \n24.4 \n25.3 \n32.2 32.0 39.8 66.7 33.6 \n24.5 \n41.9 38.4 31.6 \n\u2022 EATA+recover \n26.1 \n31.0 27.2 \n19.9 \n18.5 \n25.7 \n25.7 \n35.9 28.6 40.4 68.2 35.3 \n27.6 \n42.9 40.9 32.9 \n\u2022 SAR (ours) \n33.1 \n36.5 35.5 \n19.2 \n19.5 \n33.3 \n27.7 \n23.9 45.3 50.1 71.9 46.7 \n7.1 \n52.1 56.3 37.2 \n\nVitBase (LN) \n9.4 \n6.7 \n8.3 \n29.1 \n23.4 \n34.0 \n27.0 \n15.8 26.3 47.4 54.7 43.9 \n30.5 \n44.5 47.6 29.9 \n\u2022 Tent \n32.7 \n1.4 \n34.6 \n54.4 \n52.3 \n58.2 \n52.2 \n7.7 \n12.0 69.3 76.1 66.1 \n56.7 \n69.4 66.4 47.3 \n\u2022 Tent+recover \n40.3 \n10.1 42.4 \n54.4 \n52.3 \n58.1 \n52.2 \n31.6 39.2 69.3 76.1 66.1 \n56.7 \n69.4 66.4 52.3 \n\u2022 EATA \n35.9 \n34.6 36.7 \n45.3 \n47.2 \n49.3 \n47.7 \n56.5 55.4 62.2 72.2 21.7 \n56.2 \n64.7 63.7 50.0 \n\u2022 EATA+recover \n35.9 \n34.6 36.7 \n45.3 \n47.2 \n49.3 \n47.7 \n56.5 55.4 62.2 72.2 21.7 \n56.2 \n64.7 63.7 49.9 \n\u2022 SAR (ours) \n46.5 \n43.1 48.9 \n55.3 \n54.3 \n58.9 \n54.8 \n53.6 46.2 69.7 76.2 66.2 \n60.9 \n69.6 66.6 58.1 \n\nhttps://zenodo.org/record/2235448#.YzQpq-xBxcA 2 https://download.pytorch.org/models/resnet50-19c8e357.pth 3 https://github.com/rwightman/pytorch-image-models/releases/download/ v0.1-rsb-weights/resnet50_gn_a1h2-8fe6c4d0.pth 4 https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_ 0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0. 01-res_224.npz 5 https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\nhttps://github.com/yueatsprograms/ttt_imagenet_release 7 https://github.com/DequanWang/tent 8 https://github.com/mr-eggplant/EATA 9 https://github.com/zhangmarvin/memo\nhttps://github.com/shiyegao/DDA\n\nTest-time classifier adjustment module for model-agnostic domain generalization. Yusuke Iwasawa, Yutaka Matsuo, Advances in Neural Information Processing Systems. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, pp. 2427-2440, 2021.\n\nSITA: single image test-time adaptation. Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, Gaurav Aggarwal, arXiv:2112.02355arXiv preprintAnsh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. SITA: single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021.\n\nWILDS: A benchmark of in-the-wild distribution shifts. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, ; , International Conference on Machine Learning. Richard Lanas Phillips, Irena Gao,Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637-5664, 2021.\n\nUniversal source-free domain adaptation. Jogendra Nath Kundu, Naveen Venkat, Venkatesh Babu, IEEE Conference on Computer Vision and Pattern Recognition. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 4544-4553, 2020.\n\nASAM: adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. Jungmin Kwon, Jeongseop Kim, Hyunseo Park, In Kwon Choi, International Conference on Machine Learning. Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: adaptive sharpness- aware minimization for scale-invariant learning of deep neural networks. In International Con- ference on Machine Learning, pp. 5905-5914, 2021.\n\nOn feature normalization and data augmentation. Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, Kilian Q Weinberger, IEEE Conference on Computer Vision and Pattern Recognition. Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, and Kilian Q Weinberger. On feature normaliza- tion and data augmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 12383-12392, 2021.\n\nLearning to generalize: Meta-learning for domain generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy Hospedales, AAAI Conference on Artificial Intelligence. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI Conference on Artificial Intelligence, pp. 3490-3497, 2018a.\n\nVisualizing the loss landscape of neural nets. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein, Advances in Neural Information Processing Systems. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land- scape of neural nets. In Advances in Neural Information Processing Systems, pp. 6391-6401, 2018b.\n\nModel adaptation: Unsupervised domain adaptation without source data. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San, Si Wong, Wu, IEEE Conference on Computer Vision and Pattern Recognition. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 9638-9647, 2020.\n\nDo we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. Jian Liang, Dapeng Hu, Jiashi Feng, International Conference on Machine Learning. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, pp. 6028-6039, 2020.\n\nTTN: A domain-shift aware batch normalization in test-time adaptation. Hyesu Lim, Byeonggeun Kim, Jaegul Choo, Sungha Choi, International Conference on Learning Representations. Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. TTN: A domain-shift aware batch normalization in test-time adaptation. In International Conference on Learning Representations, 2023.\n\nFast autoaugment. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim, Advances in Neural Information Processing Systems. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In Advances in Neural Information Processing Systems, pp. 6665-6675, 2019.\n\nPrototype-guided continual adaptation for class-incremental unsupervised domain adaptation. Hongbin Lin, Yifan Zhang, Zhen Qiu, Shuaicheng Niu, Chuang Gan, Yanxia Liu, Mingkui Tan, European Conference on Computer Vision. Hongbin Lin, Yifan Zhang, Zhen Qiu, Shuaicheng Niu, Chuang Gan, Yanxia Liu, and Mingkui Tan. Prototype-guided continual adaptation for class-incremental unsupervised domain adaptation. In European Conference on Computer Vision, pp. 351-368, 2022.\n\nTTT++: when does self-supervised test-time training fail or thrive?. Yuejiang Liu, Parth Kothari, Baptiste Bastien Van Delft, Taylor Bellot-Gurlet, Alexandre Mordan, Alahi, Advances in Neural Information Processing Systems. 34Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexan- dre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, volume 34, pp. 21808-21820, 2021.\n\nA convnet for the 2020s. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie, IEEE Conference on Computer Vision and Pattern Recognition. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 11976-11986, 2022.\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2018.\n\nTest-time adaptation to distribution shift by confidence maximization and input transformation. Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, Jan Hendrik Metzen, arXiv:2106.14999arXiv preprintChaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confidence maxi- mization and input transformation. arXiv preprint arXiv:2106.14999, 2021.\n\nEvaluating prediction-time batch normalization for robustness under covariate shift. Zachary Nado, Shreyas Padhy, Alexander D&apos; Sculley, Balaji Amour, Jasper Lakshminarayanan, Snoek, arXiv:2006.10963arXiv preprintZachary Nado, Shreyas Padhy, D Sculley, Alexander D'Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020.\n\nEfficient test-time model adaptation without forgetting. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, Mingkui Tan, International Conference on Machine Learning. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International Conference on Machine Learning, pp. 16888-16905, 2022a.\n\nBoost test-time performance with closed-loop inference. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Guanghui Xu, Haokun Li, Junzhou Huang, Yaowei Wang, Mingkui Tan, arXiv:2203.10853arXiv preprintShuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Guanghui Xu, Haokun Li, Junzhou Huang, Yaowei Wang, and Mingkui Tan. Boost test-time performance with closed-loop inference. arXiv preprint arXiv:2203.10853, 2022b.\n\nRobustness properties of facebook's resnext WSL models. A Emin Orhan, arXiv:1907.07640arXiv preprintA Emin Orhan. Robustness properties of facebook's resnext WSL models. arXiv preprint arXiv:1907.07640, 2019.\n\nMulti-adversarial domain adaptation. Zhongyi Pei, Zhangjie Cao, Mingsheng Long, Jianmin Wang, AAAI Conference on Artificial Intelligence. Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adap- tation. In AAAI Conference on Artificial Intelligence, pp. 3934-3941, 2018.\n\nSource-free domain adaptation via avatar prototype generation and adaptation. Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu, Yanxia Liu, Qing Du, Mingkui Tan, International Joint Conference on Artificial Intelligence. Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu, Yanxia Liu, Qing Du, and Mingkui Tan. Source-free domain adaptation via avatar prototype generation and adaptation. In International Joint Conference on Artificial Intelligence, pp. 2921-2927, 2021.\n\nOnline learning: Random averages, combinatorial parameters, and learnability. Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari, Advances in Neural Information Processing Systems. Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, com- binatorial parameters, and learnability. In Advances in Neural Information Processing Systems, pp. 1984-1992, 2010.\n\nDo cifar-10 classifiers generalize to cifar-10?. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, arXiv:1806.00451arXiv preprintBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.\n\nOnline unsupervised learning of visual representations and categories. Mengye Ren, R Tyler, Scott, L Michael, Iuzzolino, C Michael, Richard Mozer, Zemel, arXiv:2109.05675arXiv preprintMengye Ren, Tyler R Scott, Michael L Iuzzolino, Michael C Mozer, and Richard Zemel. Online unsupervised learning of visual representations and categories. arXiv preprint arXiv:2109.05675, 2021.\n\nMaximum classifier discrepancy for unsupervised domain adaptation. Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, Tatsuya Harada, IEEE Conference on Computer Vision and Pattern Recognition. Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis- crepancy for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 3723-3732, 2018.\n\nImproving robustness against common corruptions by covariate shift adaptation. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge, Advances in Neural Information Processing Systems. 33Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, volume 33, pp. 11539-11551, 2020.\n\nOnline learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning. Shai Shalev-Shwartz, 4Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2):107-194, 2012.\n\nGeneralizing across domains via cross-gradient training. Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, Sunita Sarawagi, International Conference on Learning Representations. Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across domains via cross-gradient training. In International Conference on Learning Representations, 2018.\n\nTest-time training with self-supervision for generalization under distribution shifts. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt, International Conference on Machine Learning. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train- ing with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning, pp. 9229-9248, 2020.\n\nTent: Fully test-time adaptation by entropy minimization. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darrell, International Conference on Learning Representations. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Repre- sentations, 2021.\n\nContinual test-time domain adaptation. Qin Wang, Olga Fink, Luc Van Gool, Dengxin Dai, IEEE Conference on Computer Vision and Pattern Recognition. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 7201-7211, 2022.\n\nNon-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, IEEE Conference on Computer Vision and Pattern Recognition. Ross Wightman. Pytorch image modelsXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 7794-7803, 2018. Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.\n\nGroup normalization. Yuxin Wu, Kaiming He, European Conference on Computer Vision. Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, pp. 3-19, 2018.\n\nRethinking \"batch\" in batchnorm. Yuxin Wu, Justin Johnson, arXiv:2105.07576arXiv preprintYuxin Wu and Justin Johnson. Rethinking \"batch\" in batchnorm. arXiv preprint arXiv:2105.07576, 2021.\n\nImproving out-of-distribution robustness via selective augmentation. Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, Chelsea Finn, International Conference on Machine Learning. Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Im- proving out-of-distribution robustness via selective augmentation. In International Conference on Machine Learning, pp. 25407-25437, 2022.\n\nPartially observable multi-sensor sequential change detection: A combinatorial multi-armed bandit approach. Chen Zhang, C H Steven, Hoi, AAAI Conference on Artificial Intelligence. Chen Zhang and Steven CH Hoi. Partially observable multi-sensor sequential change detection: A combinatorial multi-armed bandit approach. In AAAI Conference on Artificial Intelligence, pp. 5733-5740, 2019.\n\nMemo: Test time robustness via adaptation and augmentation. Marvin Zhang, Sergey Levine, Chelsea Finn, Advances in Neural Information Processing Systems. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022.\n\nOnline adaptive asymmetric active learning with limited budgets. Yifan Zhang, Peilin Zhao, Shuaicheng Niu, Qingyao Wu, Jiezhang Cao, Junzhou Huang, Mingkui Tan, IEEE Transactions on Knowledge and Data Engineering. 336Yifan Zhang, Peilin Zhao, Shuaicheng Niu, Qingyao Wu, Jiezhang Cao, Junzhou Huang, and Mingkui Tan. Online adaptive asymmetric active learning with limited budgets. IEEE Trans- actions on Knowledge and Data Engineering, 33(6):2680-2692, 2019.\n\nYifan Zhang, Shuaicheng Niu, Zhen Qiu, Ying Wei, Peilin Zhao, Jianhua Yao, Junzhou Huang, Qingyao Wu, Mingkui Tan, Covid-Da, arXiv:2005.01577Deep domain adaptation from typical pneumonia to covid-19. arXiv preprintYifan Zhang, Shuaicheng Niu, Zhen Qiu, Ying Wei, Peilin Zhao, Jianhua Yao, Junzhou Huang, Qingyao Wu, and Mingkui Tan. COVID-DA: Deep domain adaptation from typical pneumonia to covid-19. arXiv preprint arXiv:2005.01577, 2020a.\n\nCollaborative unsupervised domain adaptation for medical image diagnosis. Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao, Shuaicheng Niu, Junzhou Huang, Mingkui Tan, IEEE Transactions on Image Processing. 29Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao, Shuaicheng Niu, Junzhou Huang, and Mingkui Tan. Collaborative unsupervised domain adaptation for medical image diagnosis. IEEE Transac- tions on Image Processing, 29:7834-7844, 2020b.\n\nDELTA: Degradation-free fully test-time adaptation. Bowen Zhao, Chen Chen, Shu-Tao Xia, International Conference on Learning Representations. Bowen Zhao, Chen Chen, and Shu-Tao Xia. DELTA: Degradation-free fully test-time adaptation. In International Conference on Learning Representations, 2023.\n\nCost-sensitive online active learning with application to malicious URL detection. Peilin Zhao, C H Steven, Hoi, Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM SIGKDD International Conference on Knowledge Discovery and Data MiningPeilin Zhao and Steven CH Hoi. Cost-sensitive online active learning with application to mali- cious URL detection. In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 919-927, 2013.\n\nDouble updating online learning. Peilin Zhao, C H Steven, Rong Hoi, Jin, Journal of Machine Learning Research. 121587Peilin Zhao, Steven CH Hoi, and Rong Jin. Double updating online learning. Journal of Machine Learning Research, 12:1587, 2011.\n\nRegularizing neural networks via adversarial model perturbation. Yaowei Zheng, Richong Zhang, Yongyi Mao, IEEE Conference on Computer Vision and Pattern Recognition. Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial model perturbation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8156- 8165, 2021.\n", "annotations": {"author": "[{\"end\":121,\"start\":106},{\"end\":169,\"start\":122},{\"end\":255,\"start\":170},{\"end\":285,\"start\":256},{\"end\":314,\"start\":286},{\"end\":362,\"start\":315},{\"end\":398,\"start\":363},{\"end\":438,\"start\":399},{\"end\":461,\"start\":439}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":117},{\"end\":133,\"start\":131},{\"end\":181,\"start\":176},{\"end\":267,\"start\":264},{\"end\":296,\"start\":292},{\"end\":326,\"start\":322},{\"end\":374,\"start\":371}]", "author_first_name": "[{\"end\":116,\"start\":106},{\"end\":130,\"start\":122},{\"end\":175,\"start\":170},{\"end\":263,\"start\":256},{\"end\":291,\"start\":286},{\"end\":321,\"start\":315},{\"end\":370,\"start\":363}]", "author_affiliation": "[{\"end\":168,\"start\":135},{\"end\":254,\"start\":183},{\"end\":284,\"start\":269},{\"end\":313,\"start\":298},{\"end\":361,\"start\":328},{\"end\":437,\"start\":400},{\"end\":460,\"start\":440}]", "title": "[{\"end\":103,\"start\":1},{\"end\":564,\"start\":462}]", "venue": null, "abstract": "[{\"end\":2244,\"start\":566}]", "bib_ref": "[{\"end\":2391,\"start\":2374},{\"end\":2409,\"start\":2391},{\"end\":2651,\"start\":2621},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2668,\"start\":2651},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2962,\"start\":2942},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3290,\"start\":3271},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4368,\"start\":4349},{\"end\":4403,\"start\":4381},{\"end\":4621,\"start\":4596},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5668,\"start\":5653},{\"end\":5706,\"start\":5689},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5918,\"start\":5899},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7782,\"start\":7764},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7800,\"start\":7782},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7961,\"start\":7944},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8835,\"start\":8817},{\"end\":10353,\"start\":10330},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10436,\"start\":10412},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10454,\"start\":10436},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10475,\"start\":10454},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10493,\"start\":10475},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10511,\"start\":10493},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10527,\"start\":10511},{\"end\":10808,\"start\":10805},{\"end\":10833,\"start\":10824},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11331,\"start\":11311},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11814,\"start\":11799},{\"end\":11852,\"start\":11835},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12291,\"start\":12272},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16118,\"start\":16099},{\"end\":17661,\"start\":17641},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19377,\"start\":19359},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19440,\"start\":19421},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20218,\"start\":20200},{\"end\":25181,\"start\":25175},{\"end\":25231,\"start\":25213},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25267,\"start\":25249},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25303,\"start\":25284},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25337,\"start\":25318},{\"end\":25798,\"start\":25780},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25978,\"start\":25959},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26007,\"start\":25988},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26511,\"start\":26492},{\"end\":26582,\"start\":26563},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26662,\"start\":26643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32514,\"start\":32497},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34699,\"start\":34680},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34885,\"start\":34868},{\"end\":34902,\"start\":34885},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35011,\"start\":34993},{\"end\":35034,\"start\":35011},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35050,\"start\":35034},{\"end\":35073,\"start\":35050},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35090,\"start\":35073},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36034,\"start\":36016},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36053,\"start\":36034},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36073,\"start\":36053},{\"end\":36075,\"start\":36073},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36189,\"start\":36171},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36414,\"start\":36397},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36433,\"start\":36414},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36450,\"start\":36433},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36501,\"start\":36481},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36862,\"start\":36844},{\"end\":37137,\"start\":37118},{\"end\":37178,\"start\":37152},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37235,\"start\":37216},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37253,\"start\":37235},{\"end\":37271,\"start\":37253},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37441,\"start\":37417},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37457,\"start\":37441},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37478,\"start\":37457},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37495,\"start\":37478},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37513,\"start\":37495},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37568,\"start\":37549},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37586,\"start\":37568},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37643,\"start\":37624},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37694,\"start\":37675},{\"end\":38055,\"start\":38033},{\"end\":38786,\"start\":38766},{\"end\":38981,\"start\":38949},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39023,\"start\":39004},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":39042,\"start\":39023},{\"end\":39058,\"start\":39042},{\"end\":39077,\"start\":39058},{\"end\":39572,\"start\":39554},{\"end\":39598,\"start\":39572},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39616,\"start\":39598},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39898,\"start\":39876},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39926,\"start\":39898},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":40038,\"start\":40019},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40136,\"start\":40118},{\"end\":40269,\"start\":40245},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40403,\"start\":40385},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":40421,\"start\":40403},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40618,\"start\":40600},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40758,\"start\":40739},{\"end\":42696,\"start\":42678},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46356,\"start\":46337},{\"end\":46378,\"start\":46360},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46445,\"start\":46427},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":46526,\"start\":46507},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46581,\"start\":46562},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":46906,\"start\":46887},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":47061,\"start\":47039},{\"end\":47079,\"start\":47061},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47418,\"start\":47400},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":48617,\"start\":48599},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":48647,\"start\":48628},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":49192,\"start\":49173},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":50288,\"start\":50261},{\"end\":50502,\"start\":50481},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":52318,\"start\":52300},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":55944,\"start\":55925},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":55962,\"start\":55944},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56135,\"start\":56115},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":56156,\"start\":56135},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":58377,\"start\":58358},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":59084,\"start\":59065},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":59113,\"start\":59094},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":85723,\"start\":85704}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":59844,\"start\":59732},{\"attributes\":{\"id\":\"fig_2\"},\"end\":60306,\"start\":59845},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60401,\"start\":60307},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61169,\"start\":60402},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61276,\"start\":61170},{\"attributes\":{\"id\":\"fig_6\"},\"end\":61357,\"start\":61277},{\"attributes\":{\"id\":\"fig_7\"},\"end\":61962,\"start\":61358},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63952,\"start\":61963},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":65944,\"start\":63953},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":66725,\"start\":65945},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":68057,\"start\":66726},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":68866,\"start\":68058},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69427,\"start\":68867},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":76402,\"start\":69428},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":77362,\"start\":76403},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":77975,\"start\":77363},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":78018,\"start\":77976},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":80383,\"start\":78019},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":82425,\"start\":80384},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":82704,\"start\":82426},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":83413,\"start\":82705},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":83746,\"start\":83414},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":84731,\"start\":83747},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":85278,\"start\":84732},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":85648,\"start\":85279},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":86030,\"start\":85649},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":87322,\"start\":86031},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":88677,\"start\":87323}]", "paragraph": "[{\"end\":3397,\"start\":2260},{\"end\":4183,\"start\":3399},{\"end\":5266,\"start\":4185},{\"end\":6631,\"start\":5268},{\"end\":6952,\"start\":6668},{\"end\":7324,\"start\":6954},{\"end\":7539,\"start\":7342},{\"end\":8154,\"start\":7541},{\"end\":8602,\"start\":8230},{\"end\":9089,\"start\":8604},{\"end\":9943,\"start\":9154},{\"end\":10182,\"start\":9990},{\"end\":10636,\"start\":10184},{\"end\":11912,\"start\":10709},{\"end\":12681,\"start\":11914},{\"end\":12888,\"start\":12683},{\"end\":13559,\"start\":12890},{\"end\":14521,\"start\":13561},{\"end\":15091,\"start\":14585},{\"end\":15792,\"start\":15093},{\"end\":16119,\"start\":15845},{\"end\":16171,\"start\":16121},{\"end\":16761,\"start\":16173},{\"end\":16857,\"start\":16763},{\"end\":17263,\"start\":16859},{\"end\":17770,\"start\":17324},{\"end\":17953,\"start\":17853},{\"end\":18047,\"start\":18015},{\"end\":18175,\"start\":18049},{\"end\":18310,\"start\":18219},{\"end\":18384,\"start\":18337},{\"end\":19013,\"start\":18386},{\"end\":20220,\"start\":19073},{\"end\":21185,\"start\":20222},{\"end\":21707,\"start\":21187},{\"end\":22488,\"start\":21709},{\"end\":22867,\"start\":22490},{\"end\":23060,\"start\":22869},{\"end\":23560,\"start\":23142},{\"end\":24450,\"start\":23562},{\"end\":25346,\"start\":24452},{\"end\":26078,\"start\":25429},{\"end\":26836,\"start\":26080},{\"end\":27284,\"start\":26898},{\"end\":29789,\"start\":27286},{\"end\":30863,\"start\":29791},{\"end\":31081,\"start\":30865},{\"end\":31137,\"start\":31083},{\"end\":32265,\"start\":31139},{\"end\":32826,\"start\":32267},{\"end\":33426,\"start\":32842},{\"end\":33635,\"start\":33456},{\"end\":33717,\"start\":33637},{\"end\":34505,\"start\":33719},{\"end\":34700,\"start\":34537},{\"end\":35568,\"start\":34702},{\"end\":35766,\"start\":35570},{\"end\":36692,\"start\":35768},{\"end\":37700,\"start\":36694},{\"end\":38724,\"start\":37702},{\"end\":39527,\"start\":38726},{\"end\":40716,\"start\":39529},{\"end\":41519,\"start\":40718},{\"end\":43353,\"start\":41544},{\"end\":43519,\"start\":43355},{\"end\":45360,\"start\":43615},{\"end\":45538,\"start\":45362},{\"end\":45962,\"start\":45574},{\"end\":46290,\"start\":45978},{\"end\":47392,\"start\":46292},{\"end\":48619,\"start\":47394},{\"end\":49164,\"start\":48621},{\"end\":50030,\"start\":49166},{\"end\":50475,\"start\":50032},{\"end\":50602,\"start\":50477},{\"end\":53533,\"start\":50604},{\"end\":54557,\"start\":53587},{\"end\":54590,\"start\":54559},{\"end\":55200,\"start\":54592},{\"end\":56458,\"start\":55202},{\"end\":56671,\"start\":56460},{\"end\":57853,\"start\":56745},{\"end\":58936,\"start\":57898},{\"end\":59731,\"start\":59002}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8229,\"start\":8155},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10708,\"start\":10637},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15844,\"start\":15793},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17323,\"start\":17264},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17852,\"start\":17771},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18014,\"start\":17954},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18218,\"start\":18176},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18336,\"start\":18311},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23141,\"start\":23061}]", "table_ref": "[{\"end\":5178,\"start\":5171},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":9088,\"start\":9081},{\"end\":25044,\"start\":25037},{\"end\":27298,\"start\":27291},{\"end\":28260,\"start\":28253},{\"end\":28532,\"start\":28525},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28898,\"start\":28891},{\"end\":29328,\"start\":29321},{\"end\":29833,\"start\":29826},{\"end\":30247,\"start\":30240},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30637,\"start\":30611},{\"end\":30658,\"start\":30651},{\"end\":32309,\"start\":32302},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":38547,\"start\":38540},{\"end\":41350,\"start\":41343},{\"end\":43157,\"start\":43156},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":45901,\"start\":45894},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":52297,\"start\":52290},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":52638,\"start\":52631},{\"end\":53192,\"start\":53185},{\"end\":53351,\"start\":53344},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":53718,\"start\":53710},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":53751,\"start\":53743},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":54697,\"start\":54689},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":54733,\"start\":54725},{\"end\":54792,\"start\":54785},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":55048,\"start\":55040},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":57133,\"start\":57125},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":58763,\"start\":58755},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":59128,\"start\":59120},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":59528,\"start\":59520}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2258,\"start\":2246},{\"end\":6666,\"start\":6634},{\"attributes\":{\"n\":\"2\"},\"end\":7340,\"start\":7327},{\"attributes\":{\"n\":\"3\"},\"end\":9152,\"start\":9092},{\"attributes\":{\"n\":\"3.1\"},\"end\":9988,\"start\":9946},{\"attributes\":{\"n\":\"3.2\"},\"end\":14583,\"start\":14524},{\"attributes\":{\"n\":\"4\"},\"end\":19071,\"start\":19016},{\"attributes\":{\"n\":\"5\"},\"end\":25382,\"start\":25349},{\"end\":25404,\"start\":25385},{\"end\":25427,\"start\":25407},{\"attributes\":{\"n\":\"5.1\"},\"end\":26896,\"start\":26839},{\"attributes\":{\"n\":\"6\"},\"end\":32840,\"start\":32829},{\"end\":33454,\"start\":33429},{\"end\":34518,\"start\":34508},{\"end\":34535,\"start\":34521},{\"end\":41542,\"start\":41522},{\"attributes\":{\"n\":\"4\"},\"end\":43573,\"start\":43522},{\"attributes\":{\"n\":\"4.1\"},\"end\":43613,\"start\":43576},{\"end\":45572,\"start\":45541},{\"end\":45976,\"start\":45965},{\"end\":53585,\"start\":53536},{\"end\":56743,\"start\":56674},{\"end\":57896,\"start\":57856},{\"end\":59000,\"start\":58939},{\"end\":59743,\"start\":59733},{\"end\":59866,\"start\":59846},{\"end\":60318,\"start\":60308},{\"end\":60413,\"start\":60403},{\"end\":61288,\"start\":61278},{\"end\":61370,\"start\":61359},{\"end\":65955,\"start\":65946},{\"end\":77373,\"start\":77364},{\"end\":77986,\"start\":77977},{\"end\":78029,\"start\":78020},{\"end\":82437,\"start\":82427},{\"end\":82716,\"start\":82706},{\"end\":83425,\"start\":83415},{\"end\":84743,\"start\":84733},{\"end\":85290,\"start\":85280},{\"end\":85660,\"start\":85650},{\"end\":86042,\"start\":86032}]", "table": "[{\"end\":63952,\"start\":62273},{\"end\":65944,\"start\":64264},{\"end\":66725,\"start\":66065},{\"end\":68057,\"start\":66816},{\"end\":68866,\"start\":68405},{\"end\":69427,\"start\":69060},{\"end\":76402,\"start\":72480},{\"end\":77362,\"start\":76541},{\"end\":77975,\"start\":77375},{\"end\":80383,\"start\":78401},{\"end\":82425,\"start\":80727},{\"end\":82704,\"start\":82557},{\"end\":83413,\"start\":83266},{\"end\":83746,\"start\":83603},{\"end\":84731,\"start\":83837},{\"end\":85278,\"start\":85064},{\"end\":85648,\"start\":85506},{\"end\":86030,\"start\":85827},{\"end\":87322,\"start\":86401},{\"end\":88677,\"start\":87413}]", "figure_caption": "[{\"end\":59844,\"start\":59745},{\"end\":60306,\"start\":59869},{\"end\":60401,\"start\":60320},{\"end\":61169,\"start\":60415},{\"end\":61276,\"start\":61172},{\"end\":61357,\"start\":61290},{\"end\":61962,\"start\":61373},{\"end\":62273,\"start\":61965},{\"end\":64264,\"start\":63955},{\"end\":66065,\"start\":65957},{\"end\":66816,\"start\":66728},{\"end\":68405,\"start\":68060},{\"end\":69060,\"start\":68869},{\"end\":72480,\"start\":69430},{\"end\":76541,\"start\":76405},{\"end\":78018,\"start\":77988},{\"end\":78401,\"start\":78031},{\"end\":80727,\"start\":80386},{\"end\":82557,\"start\":82440},{\"end\":83266,\"start\":82719},{\"end\":83603,\"start\":83428},{\"end\":83837,\"start\":83749},{\"end\":85064,\"start\":84746},{\"end\":85506,\"start\":85293},{\"end\":85827,\"start\":85663},{\"end\":86401,\"start\":86045},{\"end\":87413,\"start\":87325}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":3120,\"start\":3112},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":3698,\"start\":3690},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":3781,\"start\":3773},{\"end\":6003,\"start\":5995},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":6950,\"start\":6942},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9388,\"start\":9380},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9665,\"start\":9656},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9839,\"start\":9831},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":10942,\"start\":10934},{\"end\":12201,\"start\":12193},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12770,\"start\":12762},{\"end\":13917,\"start\":13909},{\"end\":14289,\"start\":14281},{\"end\":15565,\"start\":15557},{\"end\":16028,\"start\":16020},{\"end\":16230,\"start\":16222},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20235,\"start\":20227},{\"end\":21722,\"start\":21714},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22572,\"start\":22564},{\"end\":23575,\"start\":23567},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24628,\"start\":24620},{\"end\":30937,\"start\":30929},{\"end\":31382,\"start\":31374},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32532,\"start\":32524},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32634,\"start\":32621},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37977,\"start\":37969},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":42614,\"start\":42584},{\"end\":42744,\"start\":42736},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":43846,\"start\":43838},{\"end\":45372,\"start\":45364},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":54047,\"start\":54039},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":55275,\"start\":55267},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55775,\"start\":55757},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":56372,\"start\":56364},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":56456,\"start\":56444}]", "bib_author_first_name": "[{\"end\":89417,\"start\":89411},{\"end\":89433,\"start\":89427},{\"end\":89729,\"start\":89725},{\"end\":89744,\"start\":89739},{\"end\":89757,\"start\":89751},{\"end\":89767,\"start\":89763},{\"end\":89782,\"start\":89776},{\"end\":90035,\"start\":90031},{\"end\":90051,\"start\":90045},{\"end\":90066,\"start\":90060},{\"end\":90081,\"start\":90077},{\"end\":90089,\"start\":90082},{\"end\":90101,\"start\":90095},{\"end\":90115,\"start\":90109},{\"end\":90136,\"start\":90130},{\"end\":90150,\"start\":90141},{\"end\":90162,\"start\":90161},{\"end\":90594,\"start\":90586},{\"end\":90613,\"start\":90607},{\"end\":90989,\"start\":90982},{\"end\":91005,\"start\":90996},{\"end\":91018,\"start\":91011},{\"end\":91027,\"start\":91025},{\"end\":91032,\"start\":91028},{\"end\":91370,\"start\":91366},{\"end\":91380,\"start\":91375},{\"end\":91392,\"start\":91385},{\"end\":91403,\"start\":91398},{\"end\":91422,\"start\":91414},{\"end\":91772,\"start\":91770},{\"end\":91784,\"start\":91777},{\"end\":91797,\"start\":91791},{\"end\":91811,\"start\":91804},{\"end\":92111,\"start\":92108},{\"end\":92121,\"start\":92116},{\"end\":92131,\"start\":92126},{\"end\":92149,\"start\":92140},{\"end\":92161,\"start\":92158},{\"end\":92492,\"start\":92489},{\"end\":92504,\"start\":92497},{\"end\":92518,\"start\":92511},{\"end\":92535,\"start\":92533},{\"end\":92933,\"start\":92929},{\"end\":92947,\"start\":92941},{\"end\":92958,\"start\":92952},{\"end\":93306,\"start\":93301},{\"end\":93322,\"start\":93312},{\"end\":93334,\"start\":93328},{\"end\":93347,\"start\":93341},{\"end\":93625,\"start\":93618},{\"end\":93636,\"start\":93631},{\"end\":93648,\"start\":93642},{\"end\":93661,\"start\":93654},{\"end\":93676,\"start\":93667},{\"end\":93994,\"start\":93987},{\"end\":94005,\"start\":94000},{\"end\":94017,\"start\":94013},{\"end\":94033,\"start\":94023},{\"end\":94045,\"start\":94039},{\"end\":94057,\"start\":94051},{\"end\":94070,\"start\":94063},{\"end\":94441,\"start\":94433},{\"end\":94452,\"start\":94447},{\"end\":94470,\"start\":94462},{\"end\":94496,\"start\":94490},{\"end\":94521,\"start\":94512},{\"end\":94888,\"start\":94882},{\"end\":94899,\"start\":94894},{\"end\":94914,\"start\":94905},{\"end\":94928,\"start\":94919},{\"end\":94950,\"start\":94944},{\"end\":94967,\"start\":94960},{\"end\":95283,\"start\":95279},{\"end\":95301,\"start\":95296},{\"end\":95609,\"start\":95599},{\"end\":95630,\"start\":95625},{\"end\":95648,\"start\":95642},{\"end\":95664,\"start\":95658},{\"end\":95681,\"start\":95675},{\"end\":95691,\"start\":95688},{\"end\":95699,\"start\":95692},{\"end\":96081,\"start\":96074},{\"end\":96095,\"start\":96088},{\"end\":96120,\"start\":96103},{\"end\":96136,\"start\":96130},{\"end\":96150,\"start\":96144},{\"end\":96500,\"start\":96490},{\"end\":96514,\"start\":96506},{\"end\":96524,\"start\":96519},{\"end\":96537,\"start\":96532},{\"end\":96551,\"start\":96544},{\"end\":96565,\"start\":96559},{\"end\":96579,\"start\":96572},{\"end\":96927,\"start\":96917},{\"end\":96941,\"start\":96933},{\"end\":96951,\"start\":96946},{\"end\":96967,\"start\":96959},{\"end\":96978,\"start\":96972},{\"end\":96990,\"start\":96983},{\"end\":97004,\"start\":96998},{\"end\":97018,\"start\":97011},{\"end\":97516,\"start\":97509},{\"end\":97530,\"start\":97522},{\"end\":97545,\"start\":97536},{\"end\":97559,\"start\":97552},{\"end\":97861,\"start\":97857},{\"end\":97872,\"start\":97867},{\"end\":97887,\"start\":97880},{\"end\":97903,\"start\":97893},{\"end\":97915,\"start\":97909},{\"end\":97925,\"start\":97921},{\"end\":97937,\"start\":97930},{\"end\":98341,\"start\":98332},{\"end\":98358,\"start\":98351},{\"end\":98375,\"start\":98370},{\"end\":98704,\"start\":98696},{\"end\":98719,\"start\":98712},{\"end\":98735,\"start\":98729},{\"end\":98753,\"start\":98745},{\"end\":99029,\"start\":99023},{\"end\":99036,\"start\":99035},{\"end\":99052,\"start\":99051},{\"end\":99074,\"start\":99073},{\"end\":99091,\"start\":99084},{\"end\":99405,\"start\":99398},{\"end\":99418,\"start\":99413},{\"end\":99438,\"start\":99429},{\"end\":99454,\"start\":99447},{\"end\":99832,\"start\":99825},{\"end\":99851,\"start\":99844},{\"end\":99864,\"start\":99859},{\"end\":99876,\"start\":99870},{\"end\":99895,\"start\":99888},{\"end\":99913,\"start\":99905},{\"end\":100341,\"start\":100337},{\"end\":100561,\"start\":100557},{\"end\":100577,\"start\":100571},{\"end\":100593,\"start\":100587},{\"end\":100617,\"start\":100607},{\"end\":100636,\"start\":100629},{\"end\":100651,\"start\":100645},{\"end\":101035,\"start\":101033},{\"end\":101049,\"start\":101041},{\"end\":101062,\"start\":101056},{\"end\":101072,\"start\":101068},{\"end\":101087,\"start\":101081},{\"end\":101101,\"start\":101095},{\"end\":101459,\"start\":101453},{\"end\":101470,\"start\":101466},{\"end\":101490,\"start\":101482},{\"end\":101501,\"start\":101496},{\"end\":101519,\"start\":101513},{\"end\":101829,\"start\":101826},{\"end\":101840,\"start\":101836},{\"end\":101850,\"start\":101847},{\"end\":101868,\"start\":101861},{\"end\":102145,\"start\":102137},{\"end\":102156,\"start\":102152},{\"end\":102174,\"start\":102167},{\"end\":102189,\"start\":102182},{\"end\":102583,\"start\":102578},{\"end\":102595,\"start\":102588},{\"end\":102784,\"start\":102779},{\"end\":102795,\"start\":102789},{\"end\":103012,\"start\":103006},{\"end\":103020,\"start\":103018},{\"end\":103030,\"start\":103027},{\"end\":103041,\"start\":103035},{\"end\":103055,\"start\":103049},{\"end\":103068,\"start\":103063},{\"end\":103081,\"start\":103074},{\"end\":103476,\"start\":103472},{\"end\":103485,\"start\":103484},{\"end\":103487,\"start\":103486},{\"end\":103818,\"start\":103812},{\"end\":103832,\"start\":103826},{\"end\":103848,\"start\":103841},{\"end\":104144,\"start\":104139},{\"end\":104158,\"start\":104152},{\"end\":104175,\"start\":104165},{\"end\":104188,\"start\":104181},{\"end\":104201,\"start\":104193},{\"end\":104214,\"start\":104207},{\"end\":104229,\"start\":104222},{\"end\":104540,\"start\":104535},{\"end\":104558,\"start\":104548},{\"end\":104568,\"start\":104564},{\"end\":104578,\"start\":104574},{\"end\":104590,\"start\":104584},{\"end\":104604,\"start\":104597},{\"end\":104617,\"start\":104610},{\"end\":104632,\"start\":104625},{\"end\":104644,\"start\":104637},{\"end\":105057,\"start\":105052},{\"end\":105069,\"start\":105065},{\"end\":105082,\"start\":105075},{\"end\":105093,\"start\":105087},{\"end\":105110,\"start\":105100},{\"end\":105123,\"start\":105116},{\"end\":105138,\"start\":105131},{\"end\":105475,\"start\":105470},{\"end\":105486,\"start\":105482},{\"end\":105500,\"start\":105493},{\"end\":105805,\"start\":105799},{\"end\":105813,\"start\":105812},{\"end\":105815,\"start\":105814},{\"end\":106263,\"start\":106257},{\"end\":106271,\"start\":106270},{\"end\":106273,\"start\":106272},{\"end\":106286,\"start\":106282},{\"end\":106541,\"start\":106535},{\"end\":106556,\"start\":106549},{\"end\":106570,\"start\":106564}]", "bib_author_last_name": "[{\"end\":89425,\"start\":89418},{\"end\":89440,\"start\":89434},{\"end\":89737,\"start\":89730},{\"end\":89749,\"start\":89745},{\"end\":89761,\"start\":89758},{\"end\":89774,\"start\":89768},{\"end\":89791,\"start\":89783},{\"end\":90043,\"start\":90036},{\"end\":90058,\"start\":90052},{\"end\":90075,\"start\":90067},{\"end\":90093,\"start\":90090},{\"end\":90107,\"start\":90102},{\"end\":90128,\"start\":90116},{\"end\":90139,\"start\":90137},{\"end\":90159,\"start\":90151},{\"end\":90605,\"start\":90595},{\"end\":90620,\"start\":90614},{\"end\":90636,\"start\":90622},{\"end\":90994,\"start\":90990},{\"end\":91009,\"start\":91006},{\"end\":91023,\"start\":91019},{\"end\":91037,\"start\":91033},{\"end\":91373,\"start\":91371},{\"end\":91383,\"start\":91381},{\"end\":91396,\"start\":91393},{\"end\":91412,\"start\":91404},{\"end\":91433,\"start\":91423},{\"end\":91775,\"start\":91773},{\"end\":91789,\"start\":91785},{\"end\":91802,\"start\":91798},{\"end\":91822,\"start\":91812},{\"end\":92114,\"start\":92112},{\"end\":92124,\"start\":92122},{\"end\":92138,\"start\":92132},{\"end\":92156,\"start\":92150},{\"end\":92171,\"start\":92162},{\"end\":92495,\"start\":92493},{\"end\":92509,\"start\":92505},{\"end\":92522,\"start\":92519},{\"end\":92531,\"start\":92524},{\"end\":92540,\"start\":92536},{\"end\":92544,\"start\":92542},{\"end\":92939,\"start\":92934},{\"end\":92950,\"start\":92948},{\"end\":92963,\"start\":92959},{\"end\":93310,\"start\":93307},{\"end\":93326,\"start\":93323},{\"end\":93339,\"start\":93335},{\"end\":93352,\"start\":93348},{\"end\":93629,\"start\":93626},{\"end\":93640,\"start\":93637},{\"end\":93652,\"start\":93649},{\"end\":93665,\"start\":93662},{\"end\":93680,\"start\":93677},{\"end\":93998,\"start\":93995},{\"end\":94011,\"start\":94006},{\"end\":94021,\"start\":94018},{\"end\":94037,\"start\":94034},{\"end\":94049,\"start\":94046},{\"end\":94061,\"start\":94058},{\"end\":94074,\"start\":94071},{\"end\":94445,\"start\":94442},{\"end\":94460,\"start\":94453},{\"end\":94488,\"start\":94471},{\"end\":94510,\"start\":94497},{\"end\":94528,\"start\":94522},{\"end\":94535,\"start\":94530},{\"end\":94892,\"start\":94889},{\"end\":94903,\"start\":94900},{\"end\":94917,\"start\":94915},{\"end\":94942,\"start\":94929},{\"end\":94958,\"start\":94951},{\"end\":94971,\"start\":94968},{\"end\":95294,\"start\":95284},{\"end\":95308,\"start\":95302},{\"end\":95623,\"start\":95610},{\"end\":95640,\"start\":95631},{\"end\":95656,\"start\":95649},{\"end\":95673,\"start\":95665},{\"end\":95686,\"start\":95682},{\"end\":95706,\"start\":95700},{\"end\":96086,\"start\":96082},{\"end\":96101,\"start\":96096},{\"end\":96128,\"start\":96121},{\"end\":96142,\"start\":96137},{\"end\":96167,\"start\":96151},{\"end\":96174,\"start\":96169},{\"end\":96504,\"start\":96501},{\"end\":96517,\"start\":96515},{\"end\":96530,\"start\":96525},{\"end\":96542,\"start\":96538},{\"end\":96557,\"start\":96552},{\"end\":96570,\"start\":96566},{\"end\":96583,\"start\":96580},{\"end\":96931,\"start\":96928},{\"end\":96944,\"start\":96942},{\"end\":96957,\"start\":96952},{\"end\":96970,\"start\":96968},{\"end\":96981,\"start\":96979},{\"end\":96996,\"start\":96991},{\"end\":97009,\"start\":97005},{\"end\":97022,\"start\":97019},{\"end\":97330,\"start\":97318},{\"end\":97520,\"start\":97517},{\"end\":97534,\"start\":97531},{\"end\":97550,\"start\":97546},{\"end\":97564,\"start\":97560},{\"end\":97865,\"start\":97862},{\"end\":97878,\"start\":97873},{\"end\":97891,\"start\":97888},{\"end\":97907,\"start\":97904},{\"end\":97919,\"start\":97916},{\"end\":97928,\"start\":97926},{\"end\":97941,\"start\":97938},{\"end\":98349,\"start\":98342},{\"end\":98368,\"start\":98359},{\"end\":98382,\"start\":98376},{\"end\":98710,\"start\":98705},{\"end\":98727,\"start\":98720},{\"end\":98743,\"start\":98736},{\"end\":98761,\"start\":98754},{\"end\":99033,\"start\":99030},{\"end\":99042,\"start\":99037},{\"end\":99049,\"start\":99044},{\"end\":99060,\"start\":99053},{\"end\":99071,\"start\":99062},{\"end\":99082,\"start\":99075},{\"end\":99097,\"start\":99092},{\"end\":99104,\"start\":99099},{\"end\":99411,\"start\":99406},{\"end\":99427,\"start\":99419},{\"end\":99445,\"start\":99439},{\"end\":99461,\"start\":99455},{\"end\":99842,\"start\":99833},{\"end\":99857,\"start\":99852},{\"end\":99868,\"start\":99865},{\"end\":99886,\"start\":99877},{\"end\":99903,\"start\":99896},{\"end\":99920,\"start\":99914},{\"end\":100356,\"start\":100342},{\"end\":100569,\"start\":100562},{\"end\":100585,\"start\":100578},{\"end\":100605,\"start\":100594},{\"end\":100627,\"start\":100618},{\"end\":100643,\"start\":100637},{\"end\":100660,\"start\":100652},{\"end\":101039,\"start\":101036},{\"end\":101054,\"start\":101050},{\"end\":101066,\"start\":101063},{\"end\":101079,\"start\":101073},{\"end\":101093,\"start\":101088},{\"end\":101107,\"start\":101102},{\"end\":101464,\"start\":101460},{\"end\":101480,\"start\":101471},{\"end\":101494,\"start\":101491},{\"end\":101511,\"start\":101502},{\"end\":101527,\"start\":101520},{\"end\":101834,\"start\":101830},{\"end\":101845,\"start\":101841},{\"end\":101859,\"start\":101851},{\"end\":101872,\"start\":101869},{\"end\":102150,\"start\":102146},{\"end\":102165,\"start\":102157},{\"end\":102180,\"start\":102175},{\"end\":102192,\"start\":102190},{\"end\":102586,\"start\":102584},{\"end\":102598,\"start\":102596},{\"end\":102787,\"start\":102785},{\"end\":102803,\"start\":102796},{\"end\":103016,\"start\":103013},{\"end\":103025,\"start\":103021},{\"end\":103033,\"start\":103031},{\"end\":103047,\"start\":103042},{\"end\":103061,\"start\":103056},{\"end\":103072,\"start\":103069},{\"end\":103086,\"start\":103082},{\"end\":103482,\"start\":103477},{\"end\":103494,\"start\":103488},{\"end\":103499,\"start\":103496},{\"end\":103824,\"start\":103819},{\"end\":103839,\"start\":103833},{\"end\":103853,\"start\":103849},{\"end\":104150,\"start\":104145},{\"end\":104163,\"start\":104159},{\"end\":104179,\"start\":104176},{\"end\":104191,\"start\":104189},{\"end\":104205,\"start\":104202},{\"end\":104220,\"start\":104215},{\"end\":104233,\"start\":104230},{\"end\":104546,\"start\":104541},{\"end\":104562,\"start\":104559},{\"end\":104572,\"start\":104569},{\"end\":104582,\"start\":104579},{\"end\":104595,\"start\":104591},{\"end\":104608,\"start\":104605},{\"end\":104623,\"start\":104618},{\"end\":104635,\"start\":104633},{\"end\":104648,\"start\":104645},{\"end\":104658,\"start\":104650},{\"end\":105063,\"start\":105058},{\"end\":105073,\"start\":105070},{\"end\":105085,\"start\":105083},{\"end\":105098,\"start\":105094},{\"end\":105114,\"start\":105111},{\"end\":105129,\"start\":105124},{\"end\":105142,\"start\":105139},{\"end\":105480,\"start\":105476},{\"end\":105491,\"start\":105487},{\"end\":105504,\"start\":105501},{\"end\":105810,\"start\":105806},{\"end\":105822,\"start\":105816},{\"end\":105827,\"start\":105824},{\"end\":106268,\"start\":106264},{\"end\":106280,\"start\":106274},{\"end\":106290,\"start\":106287},{\"end\":106295,\"start\":106292},{\"end\":106547,\"start\":106542},{\"end\":106562,\"start\":106557},{\"end\":106574,\"start\":106571}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":244958432},\"end\":89682,\"start\":89330},{\"attributes\":{\"doi\":\"arXiv:2112.02355\",\"id\":\"b1\"},\"end\":89974,\"start\":89684},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":229156320},\"end\":90543,\"start\":89976},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":212784882},\"end\":90883,\"start\":90545},{\"attributes\":{\"id\":\"b4\"},\"end\":91316,\"start\":90885},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211296583},\"end\":91703,\"start\":91318},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1883787},\"end\":92059,\"start\":91705},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3693334},\"end\":92417,\"start\":92061},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":219979590},\"end\":92819,\"start\":92419},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211205159},\"end\":93228,\"start\":92821},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":256808572},\"end\":93598,\"start\":93230},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":141477988},\"end\":93893,\"start\":93600},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":251018211},\"end\":94362,\"start\":93895},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":247410579},\"end\":94855,\"start\":94364},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":245837420},\"end\":95238,\"start\":94857},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53592270},\"end\":95501,\"start\":95240},{\"attributes\":{\"doi\":\"arXiv:2106.14999\",\"id\":\"b16\"},\"end\":95987,\"start\":95503},{\"attributes\":{\"doi\":\"arXiv:2006.10963\",\"id\":\"b17\"},\"end\":96431,\"start\":95989},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":247996873},\"end\":96859,\"start\":96433},{\"attributes\":{\"doi\":\"arXiv:2203.10853\",\"id\":\"b19\"},\"end\":97260,\"start\":96861},{\"attributes\":{\"doi\":\"arXiv:1907.07640\",\"id\":\"b20\"},\"end\":97470,\"start\":97262},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13841056},\"end\":97777,\"start\":97472},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235669788},\"end\":98252,\"start\":97779},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1833405},\"end\":98645,\"start\":98254},{\"attributes\":{\"doi\":\"arXiv:1806.00451\",\"id\":\"b24\"},\"end\":98950,\"start\":98647},{\"attributes\":{\"doi\":\"arXiv:2109.05675\",\"id\":\"b25\"},\"end\":99329,\"start\":98952},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4619542},\"end\":99744,\"start\":99331},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":220266097},\"end\":100242,\"start\":99746},{\"attributes\":{\"id\":\"b28\"},\"end\":100498,\"start\":100244},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13754527},\"end\":100944,\"start\":100500},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220301705},\"end\":101393,\"start\":100946},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232278031},\"end\":101785,\"start\":101395},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":247748613},\"end\":102108,\"start\":101787},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4852647},\"end\":102555,\"start\":102110},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4076251},\"end\":102744,\"start\":102557},{\"attributes\":{\"doi\":\"arXiv:2105.07576\",\"id\":\"b35\"},\"end\":102935,\"start\":102746},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":245650847},\"end\":103362,\"start\":102937},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":57847652},\"end\":103750,\"start\":103364},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":239016895},\"end\":104072,\"start\":103752},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":208138954},\"end\":104533,\"start\":104074},{\"attributes\":{\"doi\":\"arXiv:2005.01577\",\"id\":\"b40\"},\"end\":104976,\"start\":104535},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":208139293},\"end\":105416,\"start\":104978},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":256390348},\"end\":105714,\"start\":105418},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5999103},\"end\":106222,\"start\":105716},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":15746786},\"end\":106468,\"start\":106224},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":222290671},\"end\":106830,\"start\":106470}]", "bib_title": "[{\"end\":89409,\"start\":89330},{\"end\":90029,\"start\":89976},{\"end\":90584,\"start\":90545},{\"end\":90980,\"start\":90885},{\"end\":91364,\"start\":91318},{\"end\":91768,\"start\":91705},{\"end\":92106,\"start\":92061},{\"end\":92487,\"start\":92419},{\"end\":92927,\"start\":92821},{\"end\":93299,\"start\":93230},{\"end\":93616,\"start\":93600},{\"end\":93985,\"start\":93895},{\"end\":94431,\"start\":94364},{\"end\":94880,\"start\":94857},{\"end\":95277,\"start\":95240},{\"end\":96488,\"start\":96433},{\"end\":97507,\"start\":97472},{\"end\":97855,\"start\":97779},{\"end\":98330,\"start\":98254},{\"end\":99396,\"start\":99331},{\"end\":99823,\"start\":99746},{\"end\":100555,\"start\":100500},{\"end\":101031,\"start\":100946},{\"end\":101451,\"start\":101395},{\"end\":101824,\"start\":101787},{\"end\":102135,\"start\":102110},{\"end\":102576,\"start\":102557},{\"end\":103004,\"start\":102937},{\"end\":103470,\"start\":103364},{\"end\":103810,\"start\":103752},{\"end\":104137,\"start\":104074},{\"end\":105050,\"start\":104978},{\"end\":105468,\"start\":105418},{\"end\":105797,\"start\":105716},{\"end\":106255,\"start\":106224},{\"end\":106533,\"start\":106470}]", "bib_author": "[{\"end\":89427,\"start\":89411},{\"end\":89442,\"start\":89427},{\"end\":89739,\"start\":89725},{\"end\":89751,\"start\":89739},{\"end\":89763,\"start\":89751},{\"end\":89776,\"start\":89763},{\"end\":89793,\"start\":89776},{\"end\":90045,\"start\":90031},{\"end\":90060,\"start\":90045},{\"end\":90077,\"start\":90060},{\"end\":90095,\"start\":90077},{\"end\":90109,\"start\":90095},{\"end\":90130,\"start\":90109},{\"end\":90141,\"start\":90130},{\"end\":90161,\"start\":90141},{\"end\":90165,\"start\":90161},{\"end\":90607,\"start\":90586},{\"end\":90622,\"start\":90607},{\"end\":90638,\"start\":90622},{\"end\":90996,\"start\":90982},{\"end\":91011,\"start\":90996},{\"end\":91025,\"start\":91011},{\"end\":91039,\"start\":91025},{\"end\":91375,\"start\":91366},{\"end\":91385,\"start\":91375},{\"end\":91398,\"start\":91385},{\"end\":91414,\"start\":91398},{\"end\":91435,\"start\":91414},{\"end\":91777,\"start\":91770},{\"end\":91791,\"start\":91777},{\"end\":91804,\"start\":91791},{\"end\":91824,\"start\":91804},{\"end\":92116,\"start\":92108},{\"end\":92126,\"start\":92116},{\"end\":92140,\"start\":92126},{\"end\":92158,\"start\":92140},{\"end\":92173,\"start\":92158},{\"end\":92497,\"start\":92489},{\"end\":92511,\"start\":92497},{\"end\":92524,\"start\":92511},{\"end\":92533,\"start\":92524},{\"end\":92542,\"start\":92533},{\"end\":92546,\"start\":92542},{\"end\":92941,\"start\":92929},{\"end\":92952,\"start\":92941},{\"end\":92965,\"start\":92952},{\"end\":93312,\"start\":93301},{\"end\":93328,\"start\":93312},{\"end\":93341,\"start\":93328},{\"end\":93354,\"start\":93341},{\"end\":93631,\"start\":93618},{\"end\":93642,\"start\":93631},{\"end\":93654,\"start\":93642},{\"end\":93667,\"start\":93654},{\"end\":93682,\"start\":93667},{\"end\":94000,\"start\":93987},{\"end\":94013,\"start\":94000},{\"end\":94023,\"start\":94013},{\"end\":94039,\"start\":94023},{\"end\":94051,\"start\":94039},{\"end\":94063,\"start\":94051},{\"end\":94076,\"start\":94063},{\"end\":94447,\"start\":94433},{\"end\":94462,\"start\":94447},{\"end\":94490,\"start\":94462},{\"end\":94512,\"start\":94490},{\"end\":94530,\"start\":94512},{\"end\":94537,\"start\":94530},{\"end\":94894,\"start\":94882},{\"end\":94905,\"start\":94894},{\"end\":94919,\"start\":94905},{\"end\":94944,\"start\":94919},{\"end\":94960,\"start\":94944},{\"end\":94973,\"start\":94960},{\"end\":95296,\"start\":95279},{\"end\":95310,\"start\":95296},{\"end\":95625,\"start\":95599},{\"end\":95642,\"start\":95625},{\"end\":95658,\"start\":95642},{\"end\":95675,\"start\":95658},{\"end\":95688,\"start\":95675},{\"end\":95708,\"start\":95688},{\"end\":96088,\"start\":96074},{\"end\":96103,\"start\":96088},{\"end\":96130,\"start\":96103},{\"end\":96144,\"start\":96130},{\"end\":96169,\"start\":96144},{\"end\":96176,\"start\":96169},{\"end\":96506,\"start\":96490},{\"end\":96519,\"start\":96506},{\"end\":96532,\"start\":96519},{\"end\":96544,\"start\":96532},{\"end\":96559,\"start\":96544},{\"end\":96572,\"start\":96559},{\"end\":96585,\"start\":96572},{\"end\":96933,\"start\":96917},{\"end\":96946,\"start\":96933},{\"end\":96959,\"start\":96946},{\"end\":96972,\"start\":96959},{\"end\":96983,\"start\":96972},{\"end\":96998,\"start\":96983},{\"end\":97011,\"start\":96998},{\"end\":97024,\"start\":97011},{\"end\":97332,\"start\":97318},{\"end\":97522,\"start\":97509},{\"end\":97536,\"start\":97522},{\"end\":97552,\"start\":97536},{\"end\":97566,\"start\":97552},{\"end\":97867,\"start\":97857},{\"end\":97880,\"start\":97867},{\"end\":97893,\"start\":97880},{\"end\":97909,\"start\":97893},{\"end\":97921,\"start\":97909},{\"end\":97930,\"start\":97921},{\"end\":97943,\"start\":97930},{\"end\":98351,\"start\":98332},{\"end\":98370,\"start\":98351},{\"end\":98384,\"start\":98370},{\"end\":98712,\"start\":98696},{\"end\":98729,\"start\":98712},{\"end\":98745,\"start\":98729},{\"end\":98763,\"start\":98745},{\"end\":99035,\"start\":99023},{\"end\":99044,\"start\":99035},{\"end\":99051,\"start\":99044},{\"end\":99062,\"start\":99051},{\"end\":99073,\"start\":99062},{\"end\":99084,\"start\":99073},{\"end\":99099,\"start\":99084},{\"end\":99106,\"start\":99099},{\"end\":99413,\"start\":99398},{\"end\":99429,\"start\":99413},{\"end\":99447,\"start\":99429},{\"end\":99463,\"start\":99447},{\"end\":99844,\"start\":99825},{\"end\":99859,\"start\":99844},{\"end\":99870,\"start\":99859},{\"end\":99888,\"start\":99870},{\"end\":99905,\"start\":99888},{\"end\":99922,\"start\":99905},{\"end\":100358,\"start\":100337},{\"end\":100571,\"start\":100557},{\"end\":100587,\"start\":100571},{\"end\":100607,\"start\":100587},{\"end\":100629,\"start\":100607},{\"end\":100645,\"start\":100629},{\"end\":100662,\"start\":100645},{\"end\":101041,\"start\":101033},{\"end\":101056,\"start\":101041},{\"end\":101068,\"start\":101056},{\"end\":101081,\"start\":101068},{\"end\":101095,\"start\":101081},{\"end\":101109,\"start\":101095},{\"end\":101466,\"start\":101453},{\"end\":101482,\"start\":101466},{\"end\":101496,\"start\":101482},{\"end\":101513,\"start\":101496},{\"end\":101529,\"start\":101513},{\"end\":101836,\"start\":101826},{\"end\":101847,\"start\":101836},{\"end\":101861,\"start\":101847},{\"end\":101874,\"start\":101861},{\"end\":102152,\"start\":102137},{\"end\":102167,\"start\":102152},{\"end\":102182,\"start\":102167},{\"end\":102194,\"start\":102182},{\"end\":102588,\"start\":102578},{\"end\":102600,\"start\":102588},{\"end\":102789,\"start\":102779},{\"end\":102805,\"start\":102789},{\"end\":103018,\"start\":103006},{\"end\":103027,\"start\":103018},{\"end\":103035,\"start\":103027},{\"end\":103049,\"start\":103035},{\"end\":103063,\"start\":103049},{\"end\":103074,\"start\":103063},{\"end\":103088,\"start\":103074},{\"end\":103484,\"start\":103472},{\"end\":103496,\"start\":103484},{\"end\":103501,\"start\":103496},{\"end\":103826,\"start\":103812},{\"end\":103841,\"start\":103826},{\"end\":103855,\"start\":103841},{\"end\":104152,\"start\":104139},{\"end\":104165,\"start\":104152},{\"end\":104181,\"start\":104165},{\"end\":104193,\"start\":104181},{\"end\":104207,\"start\":104193},{\"end\":104222,\"start\":104207},{\"end\":104235,\"start\":104222},{\"end\":104548,\"start\":104535},{\"end\":104564,\"start\":104548},{\"end\":104574,\"start\":104564},{\"end\":104584,\"start\":104574},{\"end\":104597,\"start\":104584},{\"end\":104610,\"start\":104597},{\"end\":104625,\"start\":104610},{\"end\":104637,\"start\":104625},{\"end\":104650,\"start\":104637},{\"end\":104660,\"start\":104650},{\"end\":105065,\"start\":105052},{\"end\":105075,\"start\":105065},{\"end\":105087,\"start\":105075},{\"end\":105100,\"start\":105087},{\"end\":105116,\"start\":105100},{\"end\":105131,\"start\":105116},{\"end\":105144,\"start\":105131},{\"end\":105482,\"start\":105470},{\"end\":105493,\"start\":105482},{\"end\":105506,\"start\":105493},{\"end\":105812,\"start\":105799},{\"end\":105824,\"start\":105812},{\"end\":105829,\"start\":105824},{\"end\":106270,\"start\":106257},{\"end\":106282,\"start\":106270},{\"end\":106292,\"start\":106282},{\"end\":106297,\"start\":106292},{\"end\":106549,\"start\":106535},{\"end\":106564,\"start\":106549},{\"end\":106576,\"start\":106564}]", "bib_venue": "[{\"end\":90245,\"start\":90211},{\"end\":105994,\"start\":105920},{\"end\":89491,\"start\":89442},{\"end\":89723,\"start\":89684},{\"end\":90209,\"start\":90165},{\"end\":90696,\"start\":90638},{\"end\":91083,\"start\":91039},{\"end\":91493,\"start\":91435},{\"end\":91866,\"start\":91824},{\"end\":92222,\"start\":92173},{\"end\":92604,\"start\":92546},{\"end\":93009,\"start\":92965},{\"end\":93406,\"start\":93354},{\"end\":93731,\"start\":93682},{\"end\":94114,\"start\":94076},{\"end\":94586,\"start\":94537},{\"end\":95031,\"start\":94973},{\"end\":95362,\"start\":95310},{\"end\":95597,\"start\":95503},{\"end\":96072,\"start\":95989},{\"end\":96629,\"start\":96585},{\"end\":96915,\"start\":96861},{\"end\":97316,\"start\":97262},{\"end\":97608,\"start\":97566},{\"end\":98000,\"start\":97943},{\"end\":98433,\"start\":98384},{\"end\":98694,\"start\":98647},{\"end\":99021,\"start\":98952},{\"end\":99521,\"start\":99463},{\"end\":99971,\"start\":99922},{\"end\":100335,\"start\":100244},{\"end\":100714,\"start\":100662},{\"end\":101153,\"start\":101109},{\"end\":101581,\"start\":101529},{\"end\":101932,\"start\":101874},{\"end\":102252,\"start\":102194},{\"end\":102638,\"start\":102600},{\"end\":102777,\"start\":102746},{\"end\":103132,\"start\":103088},{\"end\":103543,\"start\":103501},{\"end\":103904,\"start\":103855},{\"end\":104286,\"start\":104235},{\"end\":104733,\"start\":104676},{\"end\":105181,\"start\":105144},{\"end\":105558,\"start\":105506},{\"end\":105918,\"start\":105829},{\"end\":106333,\"start\":106297},{\"end\":106634,\"start\":106576}]"}}}, "year": 2023, "month": 12, "day": 17}