{"id": 244180419, "updated": "2023-11-08 11:54:47.199", "metadata": {"title": "Advances in Convolution Neural Networks Based Crowd Counting and Density Estimation", "authors": "[{\"first\":\"Ra\ufb01k\",\"last\":\"Gouiaa\",\"middle\":[]},{\"first\":\"Moulay\",\"last\":\"Akhlou\ufb01\",\"middle\":[\"A.\"]},{\"first\":\"Mozhdeh\",\"last\":\"Shahbazi\",\"middle\":[]}]", "venue": "Big Data and Cognitive Computing", "journal": "Big Data and Cognitive Computing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": ": Automatically estimating the number of people in unconstrained scenes is a crucial yet challenging task in different real-world applications, including video surveillance, public safety, urban planning, and traf\ufb01c monitoring. In addition, methods developed to estimate the number of people can be adapted and applied to related tasks in various \ufb01elds, such as plant counting, vehicle counting, and cell microscopy. Many challenges and problems face crowd counting, including cluttered scenes, extreme occlusions, scale variation, and changes in camera perspective. Therefore, in the past few years, tremendous research efforts have been devoted to crowd counting, and numerous excellent techniques have been proposed. The signi\ufb01cant progress in crowd counting methods in recent years is mostly attributed to advances in deep convolution neural networks (CNNs) as well as to public crowd counting datasets. In this work, we review the papers that have been published in the last decade and provide a comprehensive survey of the recent CNNs based crowd counting techniques. We brie\ufb02y review detection-based, regression-based, and traditional density estimation based approaches. Then, we delve into detail regarding the deep learning based density estimation approaches and recently published datasets. In addition, we discuss the potential applications of crowd counting and in particular its applications using unmanned aerial vehicle (UAV) images.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3203742755", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/bdcc/GouiaaAS21", "doi": "10.3390/bdcc5040050"}}, "content": {"source": {"pdf_hash": "368131a029fba7c69133b80561d51ef0d44f6fd9", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.mdpi.com/2504-2289/5/4/50/pdf", "status": "GOLD"}}, "grobid": {"id": "3c2d141d83adc8088de5f27d1c3762396f48282e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/368131a029fba7c69133b80561d51ef0d44f6fd9.txt", "contents": "\nAdvances in Convolution Neural Networks Based Crowd Counting and Density Estimation\nPublished: 28 September 2021\n\nRafik Gouiaa \nPerception, Robotics, and Intelligent Machines Research Group (PRIME)\nDepartment of Computer Science\nUniversit\u00e9 de Moncton\nE1A 3E9MonctonNBCanada\n\nMoulay A Akhloufi \nPerception, Robotics, and Intelligent Machines Research Group (PRIME)\nDepartment of Computer Science\nUniversit\u00e9 de Moncton\nE1A 3E9MonctonNBCanada\n\nMozhdeh Shahbazi \nDepartment of Geomatics Engineering\nUniversity of Calgary\nT2N 1N4CalgaryABCanada\n\nCentre de G\u00e9omatique du Qu\u00e9bec, Chicoutimi\nG7H 1Z6QCCanada\n\nAdvances in Convolution Neural Networks Based Crowd Counting and Density Estimation\nPublished: 28 September 202110.3390/bdcc5040050Received: 22 August 2021 Accepted: 23 September 2021big data and cognitive computing Review Citation: Gouiaa, R.; Akhloufi, M.A.; Shahbazi, M. Advances in Convolution Neural Networks Based Crowd Counting and Density Estimation. Big Data Cogn. Comput. 2021, 5, 50. https://doi.org/ 10.3390/bdcc5040050 Academic Editor: Min Chen\n\n\nIntroduction\n\nIn recent years, great efforts have been devoted to counting people in crowd unconstrained scenes due to its importance in applications, such as video surveillance [1], traffic monitoring [2], etc. The increasing growth of the world population and the development of urbanization has resulted in frequent crowd gatherings in numerous activities, such as stadium events, political events, and festivals (See Figure 1). In this context, crowd counting and density estimation are crucial for a better control & management and to ensure the security and the safety of the public.\n\nCrowd counting remains a challenging task due to different difficulties related to the unconstrained scenes, such as extreme occlusions, variation in light conditions, changes in scale and camera perspective, and non-uniform density of people (See Figure 2). The aforementioned issues motivated many research communities to consider crowd counting as their main research direction, and attempted to develop more sophisticated techniques to deal with limitations in crowd counting.\n\nIn particular, with the recent progress in deep learning, convolution neural networks have been widely used to address crowd counting and made significant progress owing to their capacity of effectively modeling the scale changes of people/heads and the variation in regions' crowd density. The developed people counting techniques can be extended and applied to related tasks. Therefore, a section in this paper is dedicated to reviewing the most important techniques that can be extended to develop potential applications using unmanned aerial vehicle (UAV) images.  This work reviews papers that were published in the last decade and is organized as follows: Section 2 is dedicated to reviewing the traditional crowd counting and density estimation methods. In Section 3, we review the previous related surveys. In Section 4, we review, in detail, the CNN-based density estimation methods. Section 5, we discuss the most important public datasets along with the results of the state-of-the-art methods. We also present crowd counting applications that are based on Unmanned Aerial Vehicles (UAV) images in Section 6. Finally, we conclude our survey in Section 7.\n\n\nRelated Work and Motivation\n\nDifferent approaches have been proposed to tackle the problem of crowd counting in images and videos. These approaches can be mainly divided into four categories: detection-based, regression-based, traditional density estimation, and recent CNN-based density estimation.\n\nThe scope of this survey is to focus on modern CNN-based density estimation and crowd counting approaches and review the most important techniques that can be extended to develop real-world applications using UAV images. However, first, we briefly review the detection and regression approaches using hand-crafted features.\n\n\nDetection-Based Approaches\n\nEarly work on crowd counting adopted a detection framework [3][4][5][6]. Given a crowded situation, these approaches used a sliding window to detect the most visible parts of the body, which are mainly the head and the shoulders. Recently, various CNN-based object detectors have been proposed, which lead to a higher object detection performance as compared to systems based on simpler hand-crafted features [7]. In this context, we note the two-stage detectors, such as RCNN [7], Faster-RCNN [8] and Mask-RCNN [9], and the one-stage detectors, such as YOLO [10] and SDD [11]. Despite their high accuracy detection recorded in a sparse scene, these approaches do not perform well in the presence of the visual occlusions and ambiguities in crowded scenes.\n\n\nRegression-Based Approaches\n\nTo overcome the limitations of the detection-based approaches, researchers attempt to formulate the crowd counting as a regression problem where they learn directly how to map the appearance of the image patches to their corresponding object density maps [12][13][14]. These approaches operate mainly on two steps: feature extraction and regression modeling. A variety of local features , such as SIFT [15], HOG [16], LBP [17,18], and global features, such as texture [19] and gradient [18] have been used to encode the object information. Learning a mapping from low-level features to the crowd count has been carried out using Gaussian process regression [20], linear regression [21], and ridge regression [14] \n\n\nTraditional Density Estimation Based Approaches\n\nWhile earlier approaches were successfully dealing with occlusion and scene cluttering, most of them regressed from global features directly to the number of objects and discard any available spatial information. In contrast, Lemptisky et al. [22] first involved spatial information in the learning process by adopting a linear mapping between local patch features and corresponding density maps. Thereby, they avoided the complex task of learning to detect and localize individual object instances and introduced a new approach to estimate an image density whose integral over any image region gives the count of objects within that region.\n\nThe learning process to estimate such density is formulated as a minimization of a regularized risk quadratic cost function, where a new appropriate loss function is introduced. Thus, the entire learning process is posed as a convex quadratic program solvable with cutting-plane optimization. To alleviate the difficulty of linear mapping, Pham et al. [23], proposed a non-linear mapping between local patch features and density maps through a random forest regressor. They obtained satisfactory results by introducing a crowdedness prior to tackle the large variation in appearance and shape between crowded image patches and non-crowded ones.\n\nIn addition, they proposed an effective forest reduction method to speed up estimation and met the real-time requirement. This method requires relatively less memory to build and store the forest. These methods incorporate the spatial information in the learning process, which improves the counting accuracy compared to the regression and detectionbased approaches. However, they only used traditional hand-crafted features to extract low-level information from local patches, which can lead to estimating a low-quality density map.\n\n\nCNN-Based Density Estimation\n\nCNN-based approaches have demonstrated a good performance in numerous computer vision problems, thus, motivating more researchers to use their ability to estimate a non-linear function mapping from crowd images to their corresponding density maps. In this context, numerous techniques have been proposed, which can be categorized into five groups according to the network architecture and the inference algorithm process, as depicted in Figure 3. \n\n\nRelated Previous Surveys\n\nResearchers have attempted to review the techniques of density estimation and crowd counting. Notably, Junior et al. [24] were among the first to provide a comprehensive study of the existing techniques for crowd counting. Li et al. [25] reviewed various methods for the crowded scene analysis, which covered different tasks, such as crowd motion, pattern learning, and anomaly detection in crowds. In [26], Zitouni et al. reviewed the existing visual crowd analysis techniques based on different key statistical evidence, which was inferred from the literature, and provided recommendations toward the general aspects of techniques instead of focusing on a specific algorithm.\n\nIn [27], Loy et al. provided a study that evaluates and compares the state-of-the-art techniques of visual crowd counting using the same protocol. Saleh et al. [28] presented a survey on crowd counting methods used in video surveillance and categorized the existing algorithms into two main approaches: direct and indirect. While these surveys provide detailed and comprehensive studies of the existing density estimation and crowd counting techniques, they all reviewed the traditional methods based on the hand-crafted features. Recently, Sindagi et al. [29] provided a survey of advances in CNN-based density estimation and crowd counting from a single image, published up to the year 2017. Guangshuai et al. [30] put forward a survey on CNN-based density estimation and crowd counting where over 220 papers have been reviewed up to the year 2020.\n\nThough the last survey [30] covers the most recent CNN-based crowd counting approaches, as with the previous surveys, it focuses only on the statistical evaluation and comparison between different approaches without analyzing the importance of extending the approaches used for counting people in crowds in order to develop real-world applications in various areas. In this paper, we survey various papers that adapt crowd counting approaches to counting different objects from UAV images.\n\n\nTaxonomy for CNN-Based Density Estimation\n\nIn this section, we review different CNN-based density estimation and crowd counting methods in view of the network architectures and the training and inference paradigm of the methods. Table 1 summarizes a categorization of different CNN-based crowd counting and density estimation methods according to the network architecture and the inference process.\n\n\nTypical CNN Architecture for Density Estimation and Crowd Counting\n\nBased on the type of the network architecture, we classify the approaches into three major categories (see Figure 3):\n4.1.1. Basic Network Architecture\nThese architectures are among the first deep learning approaches applied to density estimation and crowd counting. They are basically composed of convolution layers, pooling layers, and fully connected layers.\n\nFu et al. [31] and Wang et al. [32] were among the first researchers that attempted to use a convolution neural network in the context of crowd density estimation. Wang et al. [32] proposed the first end-to-end deep convolution neural network regression model for counting people in images of extremely dense crowds. The proposed architecture is composed of five Conv-layers and two fully connected layers, where its output is the estimated people counts in the input image. In addition, to reduce the false positive errors, which are mainly caused by the existence of trees and buildings in the background, training data are augmented by adding negative samples whose ground truth count is set as zero.\n\nIn a different approach, Fu et al. [31] used the multi-stage ConvNet model proposed in [33] to ensure better shift, scale and distortion invariance. To reduce the computation time at both training and detection stages, they optimized the model by discarding all similar features maps. In addition, two optimized multi-stage ConvNets are cascaded as a strong classifier to achieve boosting in which the first classifier is trained to pick out the hard samples, whereas the second one is trained to give them a final determination. Yao et al. [34] fine-tuned several architectures of deep residual network (ResNet [35]) to develop a cell counting framework.\n\nElad et al. [36] used a basic CNN and incorporated layered boosting and selective sampling to enhance the accuracy and the training computation. The training process is done in stages, where CNNs are iteratively added so that each new CNN is trained on the difference between the estimation of its predecessor and the ground truth. The selective sampling approach is used to speed up the training process by reducing the effect of low-quality samples, such as trivial samples and outlier samples.\n\nAs stated by the authors, trivial samples are those that are correctly classified early on. Feeding again these samples to the network tends to introduce a bias toward them, thereby, affecting its generalization performance. On the other hand, the presence of outliers, such as mislabeled samples, can affect the generalization of the model and, in particular, increase the computation of the training time (i.e., due to the boosting technique).\n\nThese basic CNNs approaches are simple and easy to implement. However, their performance is often limited due to the quality of the extracted features, which are usually not invariant to perspective effect or image resolution. These network architectures incorporate multiple columns to extract multi-scale features that allow generating high-quality crowd density maps.\n\nZhang et al. [37] were among the first to introduce the idea of using multiple-column architecture for crowd counting. They proposed Multi-column Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map. As depicted in Figure 4, MCNN incorporates different columns where each one adopts filters with receptive fields of different sizes, so that the extracted features are adaptive to scene variations (i.e., people/head size). In addition, they proposed a new large dataset with around 330,000 head annotations and showed that MCNN is easily transferred for crossscene crowd counting.\n\nIn [38], the authors introduced CrowdNet, which combines deep and shallow networks at two different columns, so that the shallow network is used to extract low-level features, whereas the deep network is used to extract high-level features. Both extracted features are crucial for detecting people under large-scale variations and severe occlusion. Hydra-CNN is introduced in [12]. It uses a pyramid of input patches so that each level has a different scale. By doing that, Hydra-CNN extracts multi-scale features, which are combined to generate the crowd density map.\n\nDeepak et al. [39] developed a switching-CNN for crowd counting by training various CNN crowd density regressors on patches from a crowd scene. The regressors were designed to incorporate different receptive fields. In addition, a switch classifier was trained to select the best regressor that estimates the density map corresponding to the crowd scene patch.\n\nIn another work, Deepak et al. [40] proposed a top-down feedback architecture that carries high-level features to correct false predictions. This architecture has a bottom-up CNN, which is directly connected to a top-down CNN so that the top-down generated feedback to the bottom-up to help deliver a better crowd density map. In [41], Liu et al. proposed a CNN framework to address the issues of scale variation and rotation variation using the Spatial Transform Network [58]. Zhang et al. [42] exploited the attention mechanism to address the limitations of the pixel-wise regression technique, which is very popular for estimating the crowd density map. This technique assumes the interdependence of pixels, which leads to noisy and inconsistent predictions. Thus, the proposed Relational Attention Network (RANet) incorporates local self-attention (LSA) and global self-attention (GSA) to capture the interdependence of pixels. In addition, a relational model is used to combine LSA and GSA to obtain a more informative aggregated feature representation.\n\nIn the same context, Hossain et al. [43] proposed a CNN model based on the attention mechanism to extract global and local scale features appropriate for the image. By combining both local and global features, the model outputs an improved crowd density map. Guo et al. [44] introduced a deep model called Dilated-Attention-Deformable ConvNet (DADNet), which incorporates two modules: multi-scale dilated attention and deformable convolutional DME (Density Map Estimation). The multi-scale dilated attention is based on using various kernel dilation levels to extract different visual features of crowd regions of interest, whereas the deformable convolution is used to generate a high-quality density map.\n\nObserving that most of the existing techniques are susceptible to overestimate or underestimate people counts of regions with different patterns, Jiang et al. [45], introduced a new CNN model based on the attention mechanism, which consists of two components Density Attention Network (DANet) and Attention Scaling Network (ASNet). DANet is used to extract attention masks from regions of different density levels. ASNet, on the other hand, outputs density maps and scaling factors and multiplies them by the attention masks yielding separate attention-based density maps. These maps are then summed to form that final density map.\n\nLiu et al. introduced DecideNet [59], which consists of detection and regression based density maps. These two count modes are used to deal with the crowd density variation in the image regions. An attention module is used to adaptively assess the reliability of the two count modes. Liu et al. [60] proposed a self-supervised method to improve the training of models for crowd counting. This was based on the fact that crops sampled from a crowd image contain the same or fewer persons than the original image.\n\nThus, crops can be ranked and used to train a model to estimate whether one image contains more persons than another image. Fine-tuning the resulting model on a small labeled dataset achieved state-of-the-art results. In [61], PACNN a perspective-aware CNN was introduced. It is specifically designed to predict multi-scale perspective maps and to deal with the perspective distortion problem.\n\nDingkang Liang et al. [62] introduced a weakly-supervised crowd counting in images by introducing TransCrowd, which is a sequence-to-count framework based on a Transformer-encoder. In the same context, Sun et al. [63] used transformers to encode features with global receptive fields and proposed two modules: a token-attention module and regression-token module. In [64], Gao et al. proposed a crowd localization network called the Dilated Convolutional Swin Transformer (DCST). It provides the location information of each instance in addition to counting the numbers for a scene.\n\nDespite the significant improvement and the great performance recorded by the multi-column architecture, they are still suffering from various limitations as detailed in [46]. The multi-columns CNNs are difficult to train since they require huge computation times and memory. Such architecture can generate redundant features since the columns implement almost the same network architecture. Moreover, multi-column architecture often requires a density level classifier before feeding the image to the network. However, since the number of objects is varying widely in a congested scene, it is very difficult to estimate the granularity of the crowd density maps. In addition, using crowd density level classifiers leads to the implementation of more columns, which increases the complexity of the architecture and yields more feature redundancy.\n\nThese limitations motivated some researchers to adopt single-column CNNs, which are a much simpler yet efficient architecture to overcome these disadvantages and deal with different challenging scenarios in crowd counting.\n\n\nSingle-Column Architecture\n\nThe single-column network architectures are based on deeper end-to-end CNNs. This implements more specific features to deal with critical problems in crowd counting and generate high-quality crowd density maps.\n\nIn [46], Li et al. proposed CRSNet, a CNN model for crowd counting in highly congested scenes. This model consists mainly of a frond-end CNN used to extract 2D features and a back-end dilated CNN, which uses dilated kernels to provide larger receptive fields and to replace pooling operations. The dilated convolution layers expand the receptive field without losing resolution and, thereby, aggregate the multi-scale contextual information. CSRNet is an easy-trained end-to-end approach that generates high-quality density maps and achieves state-of-the-art performance on four datasets.\n\nZhang et al. [47] proposed a scale-adaptive convolution neural network for crowd counting. The proposed architecture is composed of a backbone, which is a fully convolution neural network (FCN) with fixed small receptive fields. It extracts feature maps with different dimensions from multiple layers. The extracted feature maps are resized to have the same output dimension and combined to calculate the final crowd density map. Two loss functions have been introduced in the training process, density map loss and count loss, to jointly optimize the model. The loss count is used to reduce the variance of the prediction error and enhance the generalization performance of the model on a very sparse scene. This architecture is illustrated in Figure 5.\n\nWang et al. [48] proposed SCNet, which is a compact single-column architecture for crowd counting. It consists of three modules: residual fusion modules (RFM) to extract multi-scale features, a pyramid pooling module (PPM) to combine features at different stages, and a sub-pixel convolutional module (SPCM) followed by an upsampling layer to recover the resolution. These three modules allow SCNet to generate multi-scale features, which leads to generating a high-quality density map. In [65], Shi et al. proposed a learning strategy called deep negative correlation learning (NCL), based on learning a pool of decorrelated regressors.\n\nIn a different approach, Cao et al. [49] proposed an encoder-decoder based on the inception network [66] called SANet for crowd counting. The encoder is used to extract multi-scale features, whereas the decoder used transposed convolution layers to upscale the extracted features and generate the final crowd density map. In addition, unlike most of the existing approaches, which use only Euclidean loss that ignores the correlation between pixels of the density, they introduced a new training loss that combines the Euclidean loss and local pattern consistency loss.\n\nIn [50], Varun et al. extended the U-Net [67] by adding a decoding reinforcement branch to accelerate the training of the network and using Structural Similarity Index to maintain the local correlation of the density map in order to generate a good crowd density map. Xiaolong et al. [51] proposed a new trellis encoder-decoder architecture, which consists of multiple decoding paths and a multi-scale encoder. The multiple decoding paths are used to hierarchically aggregate features at different decoding stages, whereas the multi-scale encoder is incorporated to preserve the localization precision in the encoded feature maps.\n\nIn recent years, models based on attention mechanisms have demonstrated significant performance in different computer vision tasks [52]. Instead of extracting features from the entire image, the attention mechanism allows models to focus only on the most relevant regions. In this context, Mnih et al. [52] used the attention mechanism to introduce a scale-aware attention network to address the scale variation in crowd counting images. Due to the attention mechanism, the model can automatically focus on the most important global and local features and combine them to generate a high-quality crowd density map.\n\nLiu et al. also proposed the ADCrowdNet [53], which is an attention-based network for crowd counting. ADCrowdNet consists of two concatenated networks: an Attention Map Generator (AMG), which first estimates crowd regions in images as well as their congestion degree, and a Density Map Estimator (DME), which is a multi-scale deformable network that uses the output of AMG to generate a crowd density map. Due to the simplicity of their architectures and their effective training process, singlecolumn network approaches have received more attention in recent years.\n\n\nTypical Inference Paradigm\n\nBased on the inference methodology, we can categorize the crowd density estimation techniques into the following two categories:\n\n\nPatch-Based Inference\n\nThe patch-based model is trained on random crops from the original image. During the inference, a sliding window is applied to the test image, and the prediction is obtained for each crop. The total count is obtained by summing the counts over all the crops.\n\nIn [54], the model is trained on random patches extracted from the input images so that every crop cover 3 by 3 m square in the actual scene. The patches are then resized to 72 \u00d7 72 pixels (see Figure 6) and fed as input to the CNN model to obtain the corresponding crowd density map. The number of objects in every patch is obtained by integrating over the crowd density map. In [55], a new CNN model called PaDNet was proposed.\n\nIt consists of three modules as follows: (1) the Density-Aware Network (DAN) incorporates multiple CNN sub-networks, which are pre-trained on images with different crowd density levels, and used to capture the crowd density level information; (2) the Feature Enhancement Layer (FEL) generates weighted local and global contextual features; and (3) the Feature Fusion Network (FFN) is used to combine these contextual features. The network is trained on patches so that nine crops are taken from every input image.\n\nIn [56], Sajid et al. proposed a plug-and-play-based patch rescaling module (PRM) to address the problem of crowd diversity in the scene. As shown in Figure 7, the PRM module takes a patch image as input and then rescales it using the appropriate scaler (Up-scaler or Down-scaler) according to its crowd density level, which is computed by the classifier before using PRM. In this approach, the low-crowd and high-crowd regions pass directly through the Up-scaler or Down-scaler, the Medium-crowd bypasses the PRM without rescaling, whereas the no-crowd regions are automatically discarded.  \n\n\nImage-Based Inference\n\nBy taking random patches from the input image, patch-based methods ignore the global information and also require a huge computation during the inference due to the use of a sliding window. Training with the whole image help exploit the global information. However, it is still dependant on the resolution of the image, which is often very large in the context of crowd counting.\n\nIn [57], Chong et al. proposed an end-to-end CNN model that takes the whole image as input and directly produces the final count. First, the image is fed into a pre-trained CNN to obtain high-level features, which are then mapped to local counting numbers using a recurrent neural network with memory cells. In addition, sharing computation over overlapping regions leads to a reduction in model complexity and allows the model to incorporate contextual information when predicting both local and global counts.\n\n\nDatasets and Results\n\nWith the increasing development of crowd counting approaches, numerous datasets have been proposed over the last decade to drive research on crowd counting and develop models to deal with various limitations including changes in perspective and scale, variation in light conditions, crowd density, cluttering, and severe occlusion. Table 2 summarizes the most popular datasets, which can be categorized into three different groups according to the view type: free view, crowd surveillance view, and drone view.\n\nSome images of these categories are depicted in Figure 8. \u2022 UCSD [69]: It was among the first datasets to be collected to count people. It was acquired with a stationary camera mounted at an elevated position, overlooking pedestrian walkways. The dataset contains 2000 frames of size 158 \u00d7 512 along with annotations of pedestrians in every 1/5 frames, while the other frames are annotated using linear interpolation. It provides also the bounding box coordinates for every pedestrian. The dataset has 49,885 person instances, which are split into training and test subsets. UCSD has a low-density crowd with an average of 25 pedestrian instances, and the perspective across images does not change greatly since all images are captured from the same location. \u2022\n\nMall [70]: This dataset is collected from a publicly accessible webcam in a shopping mall. The video sequence of the dataset contains over 2000 frames of size 640 \u00d7 480 in which 62,325 heads were annotated with an average of 25 heads per image. By comparing to UCSD, the Mall dataset was created with higher crowd densities as well as more significant changes in illumination conditions and different activity patterns (static vs. moving people). The scene has severe perspective distortion along the video sequence, which results in large variations in scale and appearance of objects.\n\nIn addition, there exist severe occlusions caused by different objects in the mall. \u2022 UCF_CC_50 [13]: It is the first challenging dataset, which was created by directly scraping publicly web images. The dataset presents a wide range of crowd densities along with large varying perspective distortion. It contains only 50 images whose size is 2101 \u00d7 2888 pixels. These images contain a total of 241,677 head instances with an average of 1279 heads in each image. Due to its small size, the performance of recent CNN-based models is far from optimal. \u2022 WorldExpo'10 [54]: Zhang et al. [54] remarked that most existing crowd counting methods are scene-specific and their performance drops significantly when they are applied to unseen scenes with different layouts. To deal with this, they introduced the WorldExpo'10 dataset to perform a data-driven cross-scene crowd counting. They collected the data from Shanghai 2010 World-Expo, which contains 1132 video sequences captured by 108 cameras with an image resolution of 576 \u00d7 720 pixels. The dataset contains 3980 frames that contain a total of 200,000 annotated heads for an average of 50 heads by frame. \u2022 AHU-Crowd [71]: It is composed of diverse video sequences representing dense crowds in different public places including stations, stadiums, rallies, marathons, and pilgrimage. The sequences have different perspective views, resolutions, and crowd densities and cover a large multitude of motion behaviors for both obvious and subtle instabilities. The dataset contains 107 frames whose size is 720 \u00d7 576 pixels, and 45,000 annotated heads. SmartCity [47]: It consists of 50 images, collected from 10 different cities for outdoor scenes of different places, such as shopping malls, office entrances, sidewalks, and atriums. \u2022 Crowd Surveillance [74]: It is composed of 13,945 high-resolution images. It is split into 10,880 images for training and 3065 images for testing for a total of 386,513 head count.\n\n\u2022 DroneCrowd [75]: It was captured using a drone-mounted camera and recorded at 25 frames per second with a resolution of 1920 \u00d7 1080 pixels. It contains 112 video clips with 33,600 frames. The annotation was performed by over 20 experts for more than two months so that more than 4.8 million heads are annotated on 20,800 people trajectories. \u2022 DLR-ACD [76]: It is a collection of 33 aerial images for crowd counting and density estimation. It was captured through 16 different flights and over various urban scenes including sports events, city centers, and festivals. \u2022 Fudan-ShanghaiTech [77]: It is a large-scale video crowd counting dataset, and it is the largest dataset for crowd counting and density estimation. It is composed of 100 videos captured from 13 different scenes. It contains 150,000 images for a total of 394,081 annotated head count. \u2022 Venice [78]: It is a small dataset acquired in Piazza San Marco in Venice (Italy). It contains four different sequences for a total of 167 annotated images with a resolution of 1280 \u00d7 720 pixels. \u2022 CityStreet [79]: It was collected from a busy city street using a multiview camera system, which is composed of five synchronized cameras. The dataset contains a total of 500 multi-view images in total. \u2022 DISCO [80]: It was collected to jointly utilize ambient sounds and visual contexts for crowd counting. The dataset contains a total of 248 video clips, where each clip was recorded at 25 frames per second with a resolution of 1920 \u00d7 1080. \u2022 DroneVehicle [81]: It consists of 15,532 pairs of RGB and infrared images for a total of 441,642 annotated objects. The images were acquired by a drone-mounted camera over various urban areas, including different types of urban roads, residential areas, and parking lots from day to night. \u2022 NWPU-Crowd [82]: It contains 5109 images for a total of 2,133,375 annotated heads with point and box labels. Compared to existing datasets, it has negative samples and a large appearance variation. \u2022 JHU-CROWD++ [83]: It is composed of 4372 images and 1.51 million annotations and acquired under various scenarios and environmental conditions. Labeling is provided in different formats, including dots, approximate bounding boxes, and blur levels.  \n\n\nResults and Discussions\n\nWe report results of recent traditional approaches along with the CNN-based methods on the most popular datasets. The count estimation performance is reported directly from the original public work. We compare different methods based on the following metrics:\nMean Absolute Error(MAE) = 1 N N \u2211 i=1 |y i \u2212 y i | (1) RootMean Square Error(RMSE) = 1 N N \u2211 i=1 (y i \u2212 y i ) 2(2)\n\u2022 N: is the number of test samples. \u2022 y i is the ground truth result corresponding to sample i. \u2022 y i is the estimated result corresponding to sample i.\n\nThe comparison results are summarized in Table 3. In general, CNN-based methods highly outperformed the traditional approaches. CNN-based methods showed effective results in a very cluttered scene with large density crowds and under different scene conditions (lighting, scaling, etc.). While the multiple-column techniques achieved stateof-the-art results on three datasets: UCF_CC_50, ShanghaiTech Part A, and Mall, some single-column techniques also achieved a high performance, such as [53], which obtained state-of-the-art results on ShanghaiTech Part B.\n\nIn addition, CSRNet [46] and SaNet [49] showed comparable results on almost all datasets. The single-column technique in [53], which involves the attention mechanism, achieved state-of-the-art results on the WorlExpo'10 dataset. Finally, single-column techniques, like [46,48,49], not only presented great performances but are also easy to implement and were applied in a real-time scenario. \n\n\nPotential Application of Crowd Counting\n\nCrowd counting techniques have been applied to count and estimate the number of persons in crowded and cluttered scenes to develop real-world applications, mostly related to video surveillance and public safety. In addition, these techniques have been adapted and applied to different related problems, such as traffic control, plant/fruits counting. Different applications use different image sources, including a fixed camera, multi-cameras, a moving camera, unmanned aerial vehicles (UAVs), etc.\n\nIn this section, we review the crowd counting applications developed using unmanned aerial vehicles (UAV). In [86,87], the authors introduced an automated vehicle detection and counting system in high-resolution aerial images. The proposed method used a convolution neural network to generate a vehicle spatial density map from the aerial image.\n\nJingyu et al. [88] introduced an efficient convolution neural network called Flounder-Net, which used aerial images captured by a drone (See Figure 9), to count crowd people for a security purpose. Flounder-Net architecture (See Figure 10) involves an interleaved group convolution to eliminate the redundancy of the network, and the rapid shrink of feature maps in order to tackle the high-resolution problem. The model is implemented and integrated into the embedded system of the drone. In the same context, Castellano et al. [89] introduced a light-weight and fast fully-convolutional neural network to regress a crowd density map on aerial images captured by a drone.\n\nRecently, crowd counting techniques have been applied in the agriculture domain. In [90], Jintao et al. developed and implemented an automatic counting of in situ rice seedlings in the field using a basic fully convolution neural network to regress a crowd density map. The system takes, as input, aerial images captured by an UAV equipped with RGB cameras. Sungchan et al. [91] implemented an automatic cotton plant counting by adapting the Yolo3 [92] deep learning algorithm. In the same context, Kitano et al. [93] used a fully convolution neural network to develop an application that captured images using a UAV and returned the number of corn plants.\n\nThe technology of crowd counting is being increasingly adapted to agriculture, which facilitates the decision-making of the farmer and the management process of work-labor and products.  \n\n\nConclusions\n\nIn recent years, the need for crowd counting in many areas has greatly boosted research in crowd counting and density estimation. With the development of deep learning, the performance of crowd counting models has been remarkably improved, and the realworld applications scenarios have been expanded. This paper presented a survey on the recent advances in convolution neural network (CNN)-based crowd counting and density estimation. We explored the existing approaches from different perspectives, including the network architecture and the learning paradigm. We presented a description of the most popular datasets that are used to evaluate the crowd counting models. In addition, we conducted a performance evaluation for the most representative crowd counting algorithms. Finally, we reviewed the potential applications of crowd counting in the context of unmanned aerial vehicle (UAV) images.\n\nFigure 1 .\n1Illustration of various unconstrained crowded scenes (a) Politics, (b) Public, (c) Concert, (d) Stadium.\n\nFigure 2 .\n2Examples of unconstrained crowd scenes limitations: changes in perspective, scale and rotation variation of people/heads.\n\nFigure 3 .\n3Taxonomy of CNN-Based density estimation.\n\nFigure 4 .\n4The MCNN architecture proposed in[37].\n\nFigure 5 .\n5Illustration of scale-adaptive CNN for crowd counting proposed by Zhang et al.[47].\n\nFigure 6 .\n6The patched-based inference approach proposed in[54].\n\nFigure 7 .\n7The PRM module presented in[56].In[68], Sam et al. proposed a hierarchical CNN tree where the CNN child regressors are more accurate than any of their parents. At test time, a classifier guides the input image patches to the appropriate regressors.\n\nFigure 8 .\n8Sample images from various datasets.\n\n\n\u2022 ShanghaiTechRGBD[72]: It is a large-scale dataset composed of 2193 for a total of 144,512 annotated head count. The images are captured by a stereo camera with a valid depth ranging from 0 to 20 m. The images are captured in very busy streets of metropolitan areas and crowded public parks, while the light conditions vary from very bright to very dark.\u2022 CityUHK-X [73]: It contains 55 scenes captured using a moving camera with a tilt angle range of [\u221210 \u2022 , \u221265 \u2022 ] and a height range of [2.2, 16.0] meters. The dataset is split into training and test subsets. The training subset is composed of 43 scenes for a total of 2503 images and 78,592 people, while the test subset is composed of 12 scenes for a total of 688 images and 28,191 people. \u2022\n\nFigure 9 .\n9Crowd counting using a drone.\n\nFigure 10 .\n10Flounder-Net architecture as presented in[88].\n\nTable 1 .\n1Categorization of existing CNN-based methods.Category \n\n\n\nTable 2 .\n2Summary of various datasets, including free-view datasets, crowd-surveillance view, and drone-view.Name \nYear \nAttributes \nAvg. Resolution \nNo. Samples \nNo. Instances \nAvg. Count \n\nFree view datasets \n\nNWPU-Crowd [82] \n2020 \nCongested, Localization \n2191 \u00d7 3209 \n5109 \n2,133,375 \n418 \n\nJHU-CROWD++ [83] \n2020 \nCongested \n1430 \u00d7 910 \n4372 \n1,515,005 \n346 \n\nJHU-CROWD++ [84] \n2018 \nCongested \n2013 \u00d7 2902 \n1535 \n1,251,642 \n815 \n\nShanghaiTech Part A [85] \n2016 \nCongested \n589 \u00d7 868 \n482 \n241,677 \n501 \n\nUCF_CC_50 [13] \n2013 \nCongested \n2101 \u00d7 2888 \n50 \n241,677 \n1279 \n\nCrowd Surveillance-view \n\nDISCO [80] \n2020 \nAudiovisual, extreme conditions \n1080 \u00d7 1920 \n1935 \n170,270 \n88 \n\nCrowd Surveillance [74] \n2019 \nFree scenes \n840 \u00d7 1342 \n13,945 \n386,513 \n28 \n\nShanghaiTechRGBD [72] \n2019 \nDepth \n1080 \u00d7 1920 \n2193 \n144,512 \n65.9 \n\nFudan-ShanghaiTech [77] \n2019 \n400 Fixed Scenes, Synthetic \n1080 \u00d7 1920 \n15,211 \n7,625,843 \n501 \n\nVenice [78] \n2019 \n4 Fixed Scenes \n720 \u00d7 1280 \n167 \n-\n-\n\nCityStreet [79] \n2019 \nMulti-view \n1520 \u00d7 2704 \n500 \n-\n-\n\nSmartCity [47] \n2018 \n-\n1080 \u00d7 1920 \n50 \n369 \n7 \n\nCityUHK-X [73] \n2017 \n55 Fixed Scenes \n384 \u00d7 512 \n3191 \n106,783 \n33 \n\nShanghaiTech Part B [71] \n2016 \nFree Scenes \n768 \u00d7 1024 \n716 \n88,488 \n\nTable 2 .\n2Cont.Name \nYear \nAttributes \nAvg. Resolution \nNo. Samples \nNo. Instances \nAvg. Count \n\nAHU-Crowd [71] \n2016 \n-\n720 \u00d7 576 \n107 \n45,000 \n421 \n\nWorldExpo'10 [54] \n2015 \n108 Fixed Scenes \n576 \u00d7 720 \n3980 \n199,923 \n50 \n\nMall [70] \n2012 \n1 Fixed Scene \n480 \u00d7 640 \n2000 \n62,325 \n31 \n\nUCSD [69] \n2008 \n1 Fixed Scene \n158 \u00d7 238 \n2000 \n49,885 \n25 \n\nDrone-View \n\nDroneVehicle [81] \n2020 \nVehicle \n840 \u00d7 712 \n31,064 \n441,642 \n14.2 \n\nDroneCrowd [75] \n2019 \nVideo \n1080 \u00d7 1920 \n33,600 \n4,864,280 \n145 \n\nDLR-ACD [76] \n2019 \n-\n-\n33 \n226,291 \n6857 \n\n\n\nTable 3 .\n3Comparison of crowd density estimation on various datasets.Approach \nType \nDataset \nMall \nUCF CC 50 \nWorldExpo 10 \nUCSD \nUCF-QNRF \nShanghaiTech Part A \nShanghaiTech Part B \n\nMethod \nMAE \nRMSE \nMAE \nRMSE \nMAE \nRMSE \nMAE \nRMSE \nMAE \nRMSE \nMAE \nRMSE \nMAE \nRMSE \n\nTraditional approach \n\nLearning To Count Objects in Images [22] \n-\n-\n-\n-\n-\n-\n1.59 \n-\n-\n-\n-\n-\n-\n-\n\nCOUNT Forest [23] \n2.5 \n10.0 \n-\n-\n-\n-\n1.61 \n4.40 \n-\n-\n-\n-\n-\n-\n\nMulti-source Multi-scale Counting [13] \n-\n-\n468.0 \n590.3 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\nMultiple-column approaches \n\nMCNN [37] \n-\n-\n377.6 \n509.1 \n11.6 \n-\n1.07 \n1.35 \n-\n-\n110.2 \n173.2 \n26.4 \n41.3 \n\nCross-scene crowd counting [54] \n-\n-\n467.0 \n498.5 \n12.9 \n-\n1.60 \n3.31 \n-\n-\n181.8 \n277.7 \n32.0 \n49.8 \n\nHydra-CNN [12] \n-\n-\n333.7 \n425.2 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\nSwitching-CNN [39] \n-\n-\n318.1 \n439.2 \n9.4 \n-\n1.62 \n2.10 \n228 \n445 \n90.4 \n135 \n21.6 \n33.4 \n\nCrowd counting using deep recurrent net. [41] \n1.72 \n2.1 \n219.2 \n250.2 \n7.76 \n-\n-\n-\n-\n-\n69.3 \n96.4 \n11.1 \n18.2 \n\nRANet [42] \n-\n-\n239.8 \n319.4 \n-\n-\n-\n-\n111 \n190 \n59.4 \n102.0 \n7.9 \n12.9 \n\nDADNet [44] \n-\n-\n285.5 \n389.7 \n-\n-\n-\n-\n-\n-\n64.2 \n99.9 \n-\n-\n\nDANet [45] \n-\n-\n268.3 \n373.2 \n-\n-\n-\n-\n-\n-\n71.4 \n120.6 \n9.1 \n14.7 \n\nSingle-column approach \n\nCRSNet [46] \n-\n-\n266.1 \n397.5 \n-\n-\n-\n-\n-\n-\n68.2 \n115 \n10.6 \n16 \n\nSaCNN [47] \n-\n-\n314.9 \n424.8 \n8.5 \n-\n-\n-\n-\n-\n86.8 \n139.2 \n16.2 \n25.8 \n\nSCNet [48] \n-\n-\n280.5 \n332.8 \n8.4 \n-\n-\n-\n-\n-\n71.9 \n117.9 \n9.3 \n14.4 \n\nSANet [49] \n-\n-\n258.4 \n334.9 \n-\n-\n-\n-\n-\n-\n67.0 \n104.5 \n8.4 \n13.6 \n\nADCrowdNet (AMG-attn-DME) [53] \n-\n-\n273.6 \n362.0 \n7.3 \n-\n1.09 \n1.35 \n-\n-\n70.9 \n115.2 \n7.7 \n12.9 \n\n\nSpatiotemporal modeling for crowd counting in videos. F Xiong, X Shi, D Y Yeung, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVenice, ItalyXiong, F.; Shi, X.; Yeung, D.Y. Spatiotemporal modeling for crowd counting in videos. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22-29 October 2017; pp. 5151-5159.\n\nFcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. S Zhang, G Wu, J P Costeira, J M Moura, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVenice, ItalyZhang, S.; Wu, G.; Costeira, J.P.; Moura, J.M. Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22-29 October 2017;\n\nPedestrian Detection: An Evaluation of the State of the Art. P Dollar, C Wojek, B Schiele, P Perona, 10.1109/TPAMI.2011.155IEEE Trans. Pattern Anal. Mach. Intell. 34Dollar, P.; Wojek, C.; Schiele, B.; Perona, P. Pedestrian Detection: An Evaluation of the State of the Art. IEEE Trans. Pattern Anal. Mach. Intell. 2012, 34, 743-761. [CrossRef]\n\nA people counting system based on head-shoulder detection and tracking in surveillance video. H Xu, P Lv, L Meng, Proceedings of the 2010 International Conference on Computer Design and Applications. the 2010 International Conference on Computer Design and ApplicationsQinhuangdao, China1Xu, H.; Lv, P.; Meng, L. A people counting system based on head-shoulder detection and tracking in surveillance video. In Proceedings of the 2010 International Conference on Computer Design and Applications, Qinhuangdao, China, 25-27 June 2010; Volume 1, pp. V1-394-V1-398.\n\nCounting People in the Crowd Using a Generic Head Detector. V Subburaman, A Descamps, C Carincotte, 10.1109/AVSS.2012.87Proceedings of the 2012 9th IEEE International Conference on Advanced Video and Signal Based Surveillance. the 2012 9th IEEE International Conference on Advanced Video and Signal Based SurveillanceBeijing, ChinaIEEE Computer SocietySubburaman, V.; Descamps, A.; Carincotte, C. Counting People in the Crowd Using a Generic Head Detector. In Proceedings of the 2012 9th IEEE International Conference on Advanced Video and Signal Based Surveillance, IEEE Computer Society, Beijing, China, 18-21 September 2012; pp. 470-475. [CrossRef]\n\nCounting people by clustering person detector outputs. I S Topkaya, H Erdogan, F Porikli, Proceedings of the 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). the 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)Seoul, KoreaTopkaya, I.S.; Erdogan, H.; Porikli, F. Counting people by clustering person detector outputs. In Proceedings of the 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), Seoul, Korea, 26-29 August 2014;\n\nRich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. R B Girshick, J Donahue, T Darrell, J Malik, Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition. the 2014 IEEE Conference on Computer Vision and Pattern RecognitionColumbus, OH, USAGirshick, R.B.; Donahue, J.; Darrell, T.; Malik, J. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA, 24-27 June, 2014; pp. 580-587.\n\nTowards Real-Time Object Detection with Region Proposal Networks. S Ren, K He, R B Girshick, J Sun, Faster R-Cnn, 10.1109/TPAMI.2016.2577031IEEE Trans. Pattern Anal. Mach. Intell. 39Ren, S.; He, K.; Girshick, R.B.; Sun, J. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Trans. Pattern Anal. Mach. Intell. 2015, 39, 1137-1149. [CrossRef]\n\nK He, G Gkioxari, P Doll\u00e1r, R B Girshick, Mask R-Cnn, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV). the 2017 IEEE International Conference on Computer Vision (ICCV)Venice, ItalyHe, K.; Gkioxari, G.; Doll\u00e1r, P.; Girshick, R.B. Mask R-CNN. In Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22-29 October 2017; pp. 2980-2988.\n\nYou Only Look Once: Unified, Real-Time Object Detection. J Redmon, S Divvala, R B Girshick, A Farhadi, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Las Vegas, NV, USARedmon, J.; Divvala, S.; Girshick, R.B.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27-30 June 2016; pp. 779-788.\n\nW Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C Y Fu, A Berg, Ssd, arXiv:1512.02325Single Shot MultiBox Detector. arXiv 2016. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A. SSD: Single Shot MultiBox Detector. arXiv 2016, arXiv:1512.02325.\n\nTowards Perspective-Free Object Counting with Deep Learning. D O\u00f1oro-Rubio, R J L\u00f3pez-Sastre, Computer Vision-ECCV. O\u00f1oro-Rubio, D.; L\u00f3pez-Sastre, R.J. Towards Perspective-Free Object Counting with Deep Learning. In Computer Vision-ECCV 2016;\n\n. B Leibe, J Matas, N Sebe, M Welling, Springer International PublishingCham, SwitzerlandLeibe, B., Matas, J., Sebe, N., Welling, M., Eds.; Springer International Publishing: Cham, Switzerland, 2016; pp. 615-629.\n\nMulti-source Multi-scale Counting in Extremely Dense Crowd Images. H Idrees, I Saleemi, C Seibert, M Shah, Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition. the 2013 IEEE Conference on Computer Vision and Pattern RecognitionPortland, OR, USAIdrees, H.; Saleemi, I.; Seibert, C.; Shah, M. Multi-source Multi-scale Counting in Extremely Dense Crowd Images. In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, 25-27 June 2013; pp. 2547-2554.\n\nFeature Mining for Localised Crowd Counting. K Chen, C C Loy, S Gong, T Xiang, 16Chen, K.; Loy, C.C.; Gong, S.; Xiang, T. Feature Mining for Localised Crowd Counting. Available online: http://www.bmva.org/ bmvc/2012/BMVC/paper021/paper021.pdf (accessed on 16 September 2021).\n\nCounting in Dense Crowds using Deep Features. K Tota, H Idrees, 16Tota, K.; Idrees, H. Counting in Dense Crowds using Deep Features. Available online: https://www.crcv.ucf.edu/REU/2015 /Tota/Karunya_finalreport.pdf (accessed on 16 September 2021).\n\nCrossing the Line: Crowd Counting by Integer Programming with Local Features. Z Ma, A B Chan, Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition. the 2013 IEEE Conference on Computer Vision and Pattern RecognitionPortland, OR, USAMa, Z.; Chan, A.B. Crossing the Line: Crowd Counting by Integer Programming with Local Features. In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, 25-27 June 2013; pp. 2539-2546.\n\nCrowd Density Estimation Based on Local Binary Pattern Co-Occurrence Matrix. Z Wang, H Liu, Y Qian, T Xu, Proceedings of the 2012 IEEE International Conference on Multimedia and Expo Workshops. the 2012 IEEE International Conference on Multimedia and Expo WorkshopsMelbourne, Australia, 9Wang, Z.; Liu, H.; Qian, Y.; Xu, T. Crowd Density Estimation Based on Local Binary Pattern Co-Occurrence Matrix. In Proceedings of the 2012 IEEE International Conference on Multimedia and Expo Workshops, Melbourne, Australia, 9-13 July 2012; pp. 372-377.\n\nCrowd counting system by facial recognition using Histogram of Oriented Gradients, Completed Local Binary Pattern, Gray-Level Co-Occurrence Matrix and Unmanned Aerial Vehicle. J R Balbin, R G Garcia, K E D Fernandez, N P G Golosinda, K D G Magpayo, R J Velasco, Third International Workshop on Pattern Recognition. Balbin, J.R.; Garcia, R.G.; Fernandez, K.E.D.; Golosinda, N.P.G.; Magpayo, K.D.G.; Velasco, R.J.B. Crowd counting system by facial recognition using Histogram of Oriented Gradients, Completed Local Binary Pattern, Gray-Level Co-Occurrence Matrix and Unmanned Aerial Vehicle. In Third International Workshop on Pattern Recognition;\n\n. X Jiang, Z Chen, G Chen, 10.1117/12.2502020International Society for Optics and Photonics. 10828SPIEJiang, X., Chen, Z., Chen, G., Eds.; International Society for Optics and Photonics, SPIE: Bellingham,WA, USA, 2018; Volume 10828, pp. 238-242. [CrossRef]\n\nTexture-Based Crowd Detection and Localisation. S Ghidoni, G Cielniak, E Menegatti, 10.1007/978-3-642-33926-4_69In Intelligent Autonomous Systems. 12SpringerGhidoni, S.; Cielniak, G.; Menegatti, E. Texture-Based Crowd Detection and Localisation. In Intelligent Autonomous Systems 12; Springer: Berlin/Heidelberg, Germany, 2013; Volume 193, [CrossRef]\n\nCounting People With Low-Level Features and Bayesian Regression. A B Chan, N Vasconcelos, 10.1109/TIP.2011.2172800IEEE Trans. Image Process. 21Chan, A.B.; Vasconcelos, N. Counting People With Low-Level Features and Bayesian Regression. IEEE Trans. Image Process. 2012, 21, 2160-2177. [CrossRef]\n\nCost-sensitive sparse linear regression for crowd counting with imbalanced training data. X Huang, Y Zou, Y Wang, Proceedings of the 2016 IEEE International Conference on Multimedia and Expo (ICME). the 2016 IEEE International Conference on Multimedia and Expo (ICME)Seattle, WA, USAHuang, X.; Zou, Y.; Wang, Y. Cost-sensitive sparse linear regression for crowd counting with imbalanced training data. In Proceedings of the 2016 IEEE International Conference on Multimedia and Expo (ICME), Seattle, WA, USA, 11-15 July 2016; pp. 1-6.\n\nLearning To Count Objects in Images. V Lempitsky, A Zisserman, Advances in Neural Information Processing Systems 23. Lempitsky, V.; Zisserman, A. Learning To Count Objects in Images. In Advances in Neural Information Processing Systems 23;\n\n. J D Lafferty, C K I Williams, J Shawe-Taylor, R S Zemel, A Culotta, Curran Associates, IncRed Hook, NY, USALafferty, J.D., Williams, C.K.I., Shawe-Taylor, J., Zemel, R.S., Culotta, A., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2010; pp. 1324-1332.\n\nCO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation. V Pham, T Kozakaya, O Yamaguchi, R Okada, Forest, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). the 2015 IEEE International Conference on Computer Vision (ICCV)Santiago, ChilePham, V.; Kozakaya, T.; Yamaguchi, O.; Okada, R. COUNT Forest: CO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 13-16 December 2015; pp. 3253-3261.\n\nCrowd Analysis Using Computer Vision Techniques. Silveira Jacques Junior, J C Musse, S R Jung, C R , 10.1109/MSP.2010.937394IEEE Signal Process. Mag. 27Silveira Jacques Junior, J.C.; Musse, S.R.; Jung, C.R. Crowd Analysis Using Computer Vision Techniques. IEEE Signal Process. Mag. 2010, 27, 66-77. [CrossRef]\n\nCrowded Scene Analysis: A Survey. T Li, H Chang, M Wang, B Ni, R Hong, S Yan, 10.1109/TCSVT.2014.2358029IEEE Trans. Circuits Syst. Video Technol. 25Li, T.; Chang, H.; Wang, M.; Ni, B.; Hong, R.; Yan, S. Crowded Scene Analysis: A Survey. IEEE Trans. Circuits Syst. Video Technol. 2015, 25, 367-386. [CrossRef]\n\nAdvances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. M S Zitouni, H Bhaskar, J Dias, M Al-Mualla, 10.1016/j.neucom.2015.12.070Neurocomputing. 186Zitouni, M.S.; Bhaskar, H.; Dias, J.; Al-Mualla, M. Advances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. Neurocomputing 2016, 186, 139-159. [CrossRef]\n\nCrowd counting and profiling: Methodology and evaluation. In Modeling, Simulation and Visual Analysis of Crowds. C C Loy, K Chen, S Gong, T Xiang, SpringerBerlin/Heidelberg, GermanyLoy, C.C.; Chen, K.; Gong, S.; Xiang, T. Crowd counting and profiling: Methodology and evaluation. In Modeling, Simulation and Visual Analysis of Crowds; Springer: Berlin/Heidelberg, Germany, 2013; pp. 347-382.\n\nRecent survey on crowd density estimation and counting for visual surveillance. S A M Saleh, S A Suandi, H Ibrahim, 10.1016/j.engappai.2015.01.007Eng. Appl. Artif. Intell. 41Saleh, S.A.M.; Suandi, S.A.; Ibrahim, H. Recent survey on crowd density estimation and counting for visual surveillance. Eng. Appl. Artif. Intell. 2015, 41, 103-114. [CrossRef]\n\nA survey of recent advances in cnn-based single image crowd counting and density estimation. V A Sindagi, V M Patel, 10.1016/j.patrec.2017.07.007Pattern Recognit. Lett. 107Sindagi, V.A.; Patel, V.M. A survey of recent advances in cnn-based single image crowd counting and density estimation. Pattern Recognit. Lett. 2018, 107, 3-16. [CrossRef]\n\nG Gao, J Gao, Q Liu, Q Wang, Y Wang, arXiv:2003.12783CNN-based Density Estimation and Crowd Counting: A Survey. arXiv 2020. Gao, G.; Gao, J.; Liu, Q.; Wang, Q.; Wang, Y. CNN-based Density Estimation and Crowd Counting: A Survey. arXiv 2020, arXiv:2003.12783.\n\nFast crowd density estimation with convolutional neural networks. M Fu, P Xu, X Li, Q Liu, M Ye, C Zhu, 10.1016/j.engappai.2015.04.006Eng. Appl. Artif. Intell. 43Fu, M.; Xu, P.; Li, X.; Liu, Q.; Ye, M.; Zhu, C. Fast crowd density estimation with convolutional neural networks. Eng. Appl. Artif. Intell. 2015, 43, 81-88. [CrossRef]\n\nDeep People Counting in Extremely Dense Crowds. C Wang, H Zhang, L Yang, S Liu, X Cao, Proceedings of the 23rd ACM International Conference on Multimedia. the 23rd ACM International Conference on MultimediaWang, C.; Zhang, H.; Yang, L.; Liu, S.; Cao, X. Deep People Counting in Extremely Dense Crowds. In Proceedings of the 23rd ACM International Conference on Multimedia;\n\n. Australia Brisbane, 10.1145/2733373.2806337Brisbane, Australia, 26-30 October 2015; pp. 1299-1302. [CrossRef]\n\nConvolutional neural networks applied to house numbers digit classification. P Sermanet, S Chintala, Y Lecun, Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012). the 21st International Conference on Pattern Recognition (ICPR2012)Tsukuba, JapanSermanet, P.; Chintala, S.; LeCun, Y. Convolutional neural networks applied to house numbers digit classification. In Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012), Tsukuba, Japan, 11-15 November 2012; pp. 3288-3291.\n\nCell Counting by Regression Using Convolutional Neural Network. Y Xue, N Ray, J Hugh, G Bigras, Computer Vision-ECCV. Xue, Y.; Ray, N.; Hugh, J.; Bigras, G. Cell Counting by Regression Using Convolutional Neural Network. In Computer Vision-ECCV 2016 Workshops;\n\n. G Hua, H J\u00e9gou, Springer International PublishingCham, SwitzerlandHua, G., J\u00e9gou, H., Eds.; Springer International Publishing: Cham, Switzerland, 2016; pp. 274-290.\n\nDeep Residual Learning for Image Recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Las Vegas, NV, USAHe, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 26 June-1 July 2016.\n\nLearning to Count with CNN Boosting. E Walach, L Wolf, European Conference on Computer Vision. Cham, SwitzerlandSpringerWalach, E.; Wolf, L. Learning to Count with CNN Boosting. In European Conference on Computer Vision; Springer: Cham, Switzerland, 2016.\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, Proceedings of the IEEEConference on Computer Vision and Pattern Recognition. the IEEEConference on Computer Vision and Pattern RecognitionLas Vegas, NV, USAZhang, Y.; Zhou, D.; Chen, S.; Gao, S.; Ma, Y. Single-image crowd counting via multi-column convolutional neural network. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June-1 July 2016;\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S Kruthiventi, R V Babu, Proceedings of the 24th ACM International Conference on Multimedia. the 24th ACM International Conference on MultimediaAmsterdam, The Netherlands15Boominathan, L.; Kruthiventi, S.S.; Babu, R.V. Crowdnet: A deep convolutional network for dense crowd counting. In Proceedings of the 24th ACM International Conference on Multimedia, Amsterdam, The Netherlands, 15-19 October 2016; pp. 640-644.\n\nSwitching convolutional neural network for crowd counting. Babu Sam, D Surya, S Venkatesh Babu, R , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHonolulu, HI, USABabu Sam, D.; Surya, S.; Venkatesh Babu, R. Switching convolutional neural network for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21-26 July 2017; pp. 5744-5752.\n\nD B Sam, R V Babu, arXiv:1807.08881Top-down feedback for crowd counting convolutional neural network. arXiv. Sam, D.B.; Babu, R.V. Top-down feedback for crowd counting convolutional neural network. arXiv 2018, arXiv:1807.08881.\n\nCrowd counting using deep recurrent spatial-aware network. L Liu, H Wang, G Li, W Ouyang, L Lin, arXiv:1807.00601Liu, L.; Wang, H.; Li, G.; Ouyang, W.; Lin, L. Crowd counting using deep recurrent spatial-aware network. arXiv 2018, arXiv:1807.00601.\n\nRelational attention network for crowd counting. A Zhang, J Shen, Z Xiao, F Zhu, X Zhen, X Cao, L Shao, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionSeoul, KoreaZhang, A.; Shen, J.; Xiao, Z.; Zhu, F.; Zhen, X.; Cao, X.; Shao, L. Relational attention network for crowd counting. In Proceedings of the IEEE International Conference on Computer Vision, Seoul, Korea, 27 October-2 November 2019; pp. 6788-6797.\n\nCrowd counting using scale-aware attention networks. M Hossain, M Hosseinzadeh, O Chanda, Y Wang, Proceedings of the 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). the 2019 IEEE Winter Conference on Applications of Computer Vision (WACV)Waikoloa, HI, USAHossain, M.; Hosseinzadeh, M.; Chanda, O.; Wang, Y. Crowd counting using scale-aware attention networks. In Proceedings of the 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), Waikoloa, HI, USA, 7-11 January 2019; pp. 1280-1288.\n\nDilated-attention-deformable convnet for crowd counting. D Guo, K Li, Z J Zha, M Wang, Dadnet, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaNice, FranceGuo, D.; Li, K.; Zha, Z.J.; Wang, M. Dadnet: Dilated-attention-deformable convnet for crowd counting. In Proceedings of the 27th ACM International Conference on Multimedia, Nice, France, 21-25 October 2019; pp. 1823-1832.\n\nAttention Scaling for Crowd Counting. X Jiang, L Zhang, M Xu, T Zhang, P Lv, B Zhou, X Yang, Y Pang, 16Jiang, X.; Zhang, L.; Xu, M.; Zhang, T.; Lv, P.; Zhou, B.; Yang, X.; Pang, Y. Attention Scaling for Crowd Counting. Available online: https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Attention_Scaling_for_Crowd_Counting_CVPR_ 2020_paper.pdf (accessed on 16 September 2021).\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSalt Lake City, UT, USALi, Y.; Zhang, X.; Chen, D. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18-22 June 2018;\n\nCrowd Counting via Scale-Adaptive Convolutional Neural Network. L Zhang, M Shi, Q Chen, Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). the 2018 IEEE Winter Conference on Applications of Computer Vision (WACV)Lake Tahoe, NV, USAZhang, L.; Shi, M.; Chen, Q. Crowd Counting via Scale-Adaptive Convolutional Neural Network. In Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), Lake Tahoe, NV, USA, 12-15 March 2018; pp. 1113-1121.\n\nZ Wang, Z Xiao, K Xie, Q Qiu, X Zhen, X Cao, arXiv:1808.06133defense of single-column networks for crowd counting. arXiv 2018. Wang, Z.; Xiao, Z.; Xie, K.; Qiu, Q.; Zhen, X.; Cao, X. In defense of single-column networks for crowd counting. arXiv 2018, arXiv:1808.06133.\n\nScale Aggregation Network for Accurate and Efficient Crowd Counting. X Cao, Z Wang, Y Zhao, F Su, V Ferrari, M Hebert, C Sminchisescu, Y Weiss, In Computer Vision-ECCV. Springer International PublishingCao, X.; Wang, Z.; Zhao, Y.; Su, F. Scale Aggregation Network for Accurate and Efficient Crowd Counting. In Computer Vision-ECCV 2018; Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y., Eds.; Springer International Publishing: Cham, Switzerland, 2018; pp. 757-773.\n\nV K Valloli, K Mehta, W-Net, arXiv:1903.11249Reinforced U-Net for Density Map Estimation. arXiv 2019. Valloli, V.K.; Mehta, K. W-Net: Reinforced U-Net for Density Map Estimation. arXiv 2019, arXiv:1903.11249.\n\nCrowd counting and density estimation by trellis encoder-decoder networks. X Jiang, Z Xiao, B Zhang, X Zhen, X Cao, D Doermann, L Shao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLong Beach, CA, USAJiang, X.; Xiao, Z.; Zhang, B.; Zhen, X.; Cao, X.; Doermann, D.; Shao, L. Crowd counting and density estimation by trellis encoder-decoder networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16-20 June 2019; pp. 6133-6142.\n\nRecurrent models of visual attention. V Mnih, N Heess, A Graves, K Kavukcuoglu, 16Mnih, V.; Heess, N.; Graves, A.; Kavukcuoglu, K. Recurrent models of visual attention. Available online: https://proceedings. neurips.cc/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf (accessed on 16 September 2021).\n\nAdcrowdnet: An attention-injective deformable convolutional network for crowd understanding. N Liu, Y Long, C Zou, Q Niu, L Pan, H Wu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLong Beach, CA, USALiu, N.; Long, Y.; Zou, C.; Niu, Q.; Pan, L.; Wu, H. Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16-20 June 2019; pp. 3225-3234.\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Boston, MA, USA, 8-10Zhang, C.; Li, H.; Wang, X.; Yang, X. Cross-scene crowd counting via deep convolutional neural networks. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 8-10 June 2015; pp. 833-841.\n\nPaDNet: Pan-Density Crowd Counting. Y Tian, Y Lei, J Zhang, J Z Wang, 10.1109/TIP.2019.2952083IEEE Trans. Image Process. 29Tian, Y.; Lei, Y.; Zhang, J.; Wang, J.Z. PaDNet: Pan-Density Crowd Counting. IEEE Trans. Image Process. 2020, 29, 2714-2727. [CrossRef]\n\nPlug-and-play rescaling based crowd counting in static images. U Sajid, G Wang, Proceedings of the IEEE Winter Conference on Applications of Computer Vision. the IEEE Winter Conference on Applications of Computer VisionSnowmass Village, CO, USASajid, U.; Wang, G. Plug-and-play rescaling based crowd counting in static images. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision, Snowmass Village, CO, USA, 1-5 March, 2020; pp. 2287-2296.\n\nEnd-to-end crowd counting via joint learning local and global count. C Shang, H Ai, B Bai, Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP). the 2016 IEEE International Conference on Image Processing (ICIP)Phoenix, AZ, USAShang, C.; Ai, H.; Bai, B. End-to-end crowd counting via joint learning local and global count. In Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP), Phoenix, AZ, USA, 25-28 September 2016; pp. 1215-1219.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, K Kavukcuoglu, 16Jaderberg, M.; Simonyan, K.; Zisserman, A.; Kavukcuoglu, K. Spatial transformer networks. Available online: https:// proceedings.neurips.cc/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf (accessed on 16 September 2021).\n\nJ Liu, C Gao, D Meng, A G Hauptmann, Decidenet, arXiv:1712.06679Counting Varying Density Crowds through Attention Guided Detection and Density Estimation. arXiv 2018. Liu, J.; Gao, C.; Meng, D.; Hauptmann, A.G. DecideNet: Counting Varying Density Crowds through Attention Guided Detection and Density Estimation. arXiv 2018, arXiv:1712.06679.\n\nLeveraging Unlabeled Data for Crowd Counting by Learning to. X Liu, J Van De Weijer, A D Bagdanov, arXiv:1803.03095Rank. Liu, X.; van de Weijer, J.; Bagdanov, A.D. Leveraging Unlabeled Data for Crowd Counting by Learning to Rank. arXiv 2018, arXiv:1803.03095.\n\nRevisiting Perspective Information for Efficient Crowd Counting. M Shi, Z Yang, C Xu, Q Chen, arXiv:1807.01989Shi, M.; Yang, Z.; Xu, C.; Chen, Q. Revisiting Perspective Information for Efficient Crowd Counting. arXiv 2019 arXiv:1807.01989.\n\nD Liang, X Chen, W Xu, Y Zhou, X Bai, Transcrowd, arXiv:2104.09116Weakly-Supervised Crowd Counting with Transformer. arXiv 2021. Liang, D.; Chen, X.; Xu, W.; Zhou, Y.; Bai, X. TransCrowd: Weakly-Supervised Crowd Counting with Transformer. arXiv 2021, arXiv:2104.09116.\n\nG Sun, Y Liu, T Probst, D P Paudel, N Popovic, L V Gool, arXiv:2105.10926Boosting Crowd Counting with Transformers. arXiv 2021. Sun, G.; Liu, Y.; Probst, T.; Paudel, D.P.; Popovic, N.; Gool, L.V. Boosting Crowd Counting with Transformers. arXiv 2021, arXiv:2105.10926.\n\nJ Gao, M Gong, X Li, arXiv:2108.00584Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer. arXiv 2021. Gao, J.; Gong, M.; Li, X. Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer. arXiv 2021, arXiv:2108.00584.\n\nCrowd Counting with Deep Negative Correlation Learning. Z Shi, L Zhang, Y Liu, X Cao, Y Ye, M M Cheng, G Zheng, 10.1109/CVPR.2018.00564Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. the 2018 IEEE/CVF Conference on Computer Vision and Pattern RecognitionSalt Lake City, UT, USAShi, Z.; Zhang, L.; Liu, Y.; Cao, X.; Ye, Y.; Cheng, M.M.; Zheng, G. Crowd Counting with Deep Negative Correlation Learning. In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18-22 June 2018; pp. 5382-5390. [CrossRef]\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Boston, MA, USA, 8-10Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going deeper with convolutions. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 8-10 June 2015; pp. 1-9.\n\nConvolutional Networks for Biomedical Image Segmentation. O Ronneberger, P Fischer, T Brox, U-Net, In Medical Image Computing and Computer-Assisted Intervention-MICCAI. Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.Springer International PublishingRonneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015; Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F., Eds.; Springer International Publishing: Cham, Switzerland, 2015; pp. 234-241.\n\nD B Sam, N N Sajjan, R V Babu, Divide, Grow, arXiv:1807.09993Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN. arXiv 2018. Sam, D.B.; Sajjan, N.N.; Babu, R.V. Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN. arXiv 2018, arXiv:1807.09993.\n\nAnalysis of Crowded Scenes using Holistic Properties. A Chan, M Morrow, N Vasconcelos, 16Chan, A.; Morrow, M.; Vasconcelos, N. Analysis of Crowded Scenes using Holistic Properties. Available online: https: //citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8754&rep=rep1&type=pdf (accessed on 16 September 2021).\n\nFrom Semi-supervised to Transfer Counting of Crowds. C C Loy, S Gong, T Xiang, 16Loy, C.C.; Gong, S.; Xiang, T. From Semi-supervised to Transfer Counting of Crowds. Available online: https://personal.ie.cuhk. edu.hk/~ccloy/files/iccv_2013_crowd.pdf (accessed on 16 September 2021).\n\nCrowd Saliency Detection via Global Similarity Structure. M K Lim, V J Kok, C C Loy, C S Chan, Proceedings of the 2014 22nd International Conference on Pattern Recognition. the 2014 22nd International Conference on Pattern RecognitionStockholm, SwedenLim, M.K.; Kok, V.J.; Loy, C.C.; Chan, C.S. Crowd Saliency Detection via Global Similarity Structure. In Proceedings of the 2014 22nd International Conference on Pattern Recognition, Stockholm, Sweden, 24-28 August 2014; pp. 3957-3962.\n\nDensity Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization. D Lian, J Li, J Zheng, W Luo, S Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Long Beach, CA, USALian, D.; Li, J.; Zheng, J.; Luo, W.; Gao, S. Density Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16-20 June 2019.\n\nIncorporating Side Information by Adaptive Convolution. In Advances in Neural Information Processing Systems 30. D Kang, D Dhar, A Chan, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, IncRed Hook, NJ, USAKang, D.; Dhar, D.; Chan, A. Incorporating Side Information by Adaptive Convolution. In Advances in Neural Information Processing Systems 30; Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R., Eds.; Curran Associates, Inc.: Red Hook, NJ, USA, 2017; pp. 3867-3877.\n\nPerspective-Guided Convolution Networks for Crowd Counting. Z Yan, Y Yuan, W Zuo, X Tan, Y Wang, S Wen, E Ding, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSeoul, KoreaYan, Z.; Yuan, Y.; Zuo, W.; Tan, X.; Wang, Y.; Wen, S.; Ding, E. Perspective-Guided Convolution Networks for Crowd Counting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Korea, 27 October-2 November 2019.\n\nP Zhu, L Wen, X Bian, H Ling, Q Hu, arXiv:1804.07437Vision meets drones: A challenge. arXiv 2018. Zhu, P.; Wen, L.; Bian, X.; Ling, H.; Hu, Q. Vision meets drones: A challenge. arXiv 2018, arXiv:1804.07437.\n\nR Bahmanyar, E Vig, P Reinartz, Mrcnet, arXiv:1909.12743Crowd Counting and Density Map Estimation in Aerial and Ground Imagery. arXiv 2019. Bahmanyar, R.; Vig, E.; Reinartz, P. MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground Imagery. arXiv 2019, arXiv:1909.12743.\n\nY Fang, B Zhan, W Cai, S Gao, B Hu, arXiv:1907.07911Locality-constrained Spatial Transformer Network for Video Crowd Counting. arXiv 2019. Fang, Y.; Zhan, B.; Cai, W.; Gao, S.; Hu, B. Locality-constrained Spatial Transformer Network for Video Crowd Counting. arXiv 2019, arXiv:1907.07911.\n\nContext-Aware Crowd Counting. W Liu, M Salzmann, P Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Long Beach, CA, USALiu, W.; Salzmann, M.; Fua, P. Context-Aware Crowd Counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16-20 June 2019.\n\nWide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs. Q Zhang, A B Chan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLong Beach, CA, USAZhang, Q.; Chan, A.B. Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16-20 June 2019; pp. 8297-8306.\n\nD Hu, L Mou, Q Wang, J Gao, Y Hua, D Dou, X Zhu, arXiv:2005.07097Ambient Sound Helps: Audiovisual Crowd Counting in Extreme Conditions. arXiv 2020. Hu, D.; Mou, L.; Wang, Q.; Gao, J.; Hua, Y.; Dou, D.; Zhu, X. Ambient Sound Helps: Audiovisual Crowd Counting in Extreme Conditions. arXiv 2020, arXiv:2005.07097.\n\nP Zhu, Y Sun, L Wen, Y Feng, Q Hu, arXiv:2003.02437Drone Based RGBT Vehicle Detection and Counting: A Challenge. arXiv 2020. Zhu, P.; Sun, Y.; Wen, L.; Feng, Y.; Hu, Q. Drone Based RGBT Vehicle Detection and Counting: A Challenge. arXiv 2020, arXiv:2003.02437.\n\nA Large-Scale Benchmark for Crowd Counting and Localization. Q Wang, J Gao, W Lin, X Li, Nwpu-Crowd, 10.1109/TPAMI.2020.3013269IEEE Trans. Pattern Anal. Mach. Intell. Wang, Q.; Gao, J.; Lin, W.; Li, X. NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization. IEEE Trans. Pattern Anal. Mach. Intell. 2020. [CrossRef]\n\nPushing the frontiers of unconstrained crowd counting: New dataset and benchmark method. V A Sindagi, R Yasarla, V M Patel, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionSeoul, KoreaSindagi, V.A.; Yasarla, R.; Patel, V.M. Pushing the frontiers of unconstrained crowd counting: New dataset and benchmark method. In Proceedings of the IEEE International Conference on Computer Vision, Seoul, Korea, 27 October-2 November 2019;\n\nComposition Loss for Counting, Density Map Estimation and Localization in Dense Crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S Al-M\u00e1adeed, N Rajpoot, M Shah, arXiv:1808.01050Idrees, H.; Tayyab, M.; Athrey, K.; Zhang, D.; Al-M\u00e1adeed, S.; Rajpoot, N.; Shah, M. Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds. arXiv 2018, arXiv:1808.01050.\n\nFuture Frame Prediction for Anomaly Detection-A New Baseline. W W Liu, D L Luo, S Gao, Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Salt Lake City, UT, USALiu, W.; W. Luo, D.L.; Gao, S. Future Frame Prediction for Anomaly Detection-A New Baseline. In Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18-22 June 2018.\n\nVehicle Detection and Counting in High-Resolution Aerial Images Using Convolutional Regression Neural Network. H Tayara, K Soo, K T Chong, 10.1109/ACCESS.2017.2782260IEEE Access. 6Tayara, H.; Gil Soo, K.; Chong, K.T. Vehicle Detection and Counting in High-Resolution Aerial Images Using Convolutional Regression Neural Network. IEEE Access 2018, 6, 2220-2230. [CrossRef]\n\nCounting Vehicles with Deep Learning in Onboard UAV Imagery. G Amato, L Ciampi, F Falchi, C Gennaro, 10.1109/ISCC47284.2019.8969620Proceedings of the 2019 IEEE Symposium on Computers and Communications (ISCC). the 2019 IEEE Symposium on Computers and Communications (ISCC)Barcelona, SpainAmato, G.; Ciampi, L.; Falchi, F.; Gennaro, C. Counting Vehicles with Deep Learning in Onboard UAV Imagery. In Proceedings of the 2019 IEEE Symposium on Computers and Communications (ISCC), Barcelona, Spain, 29 June-3 July 2019; pp. 1-6. [CrossRef]\n\nFlounder-Net: An efficient CNN for crowd counting by aerial photography. J Chen, S Xiu, X Chen, H Guo, X Xie, 10.1016/j.neucom.2020.09.001Neurocomputing. 420Chen, J.; Xiu, S.; Chen, X.; Guo, H.; Xie, X. Flounder-Net: An efficient CNN for crowd counting by aerial photography. Neurocomputing 2021, 420, 82-89. [CrossRef]\n\nCrowd Counting from Unmanned Aerial Vehicles with Fully-Convolutional Neural Networks. G Castellano, C Castiello, C Mencar, G Vessio, 10.1109/IJCNN48605.2020.9206974Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN). the 2020 International Joint Conference on Neural Networks (IJCNN)Glasgow, UKCastellano, G.; Castiello, C.; Mencar, C.; Vessio, G. Crowd Counting from Unmanned Aerial Vehicles with Fully-Convolutional Neural Networks. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), Glasgow, UK, 19-24 July 2020; pp. 1-8. [CrossRef]\n\nAutomatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network. J Wu, G Yang, X Yang, B Xu, L Han, Y Zhu, 10.3390/rs11060691Remote Sens. 2019, 11, 691. [CrossRefWu, J.; Yang, G.; Yang, X.; Xu, B.; Han, L.; Zhu, Y. Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network. Remote Sens. 2019, 11, 691. [CrossRef]\n\nPlant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework. S Oh, A Chang, A Ashapure, J Jung, N Dube, M Maeda, D Gonzalez, J Landivar, 10.3390/rs12182981Remote Sens. 2020, 12, 2981. [CrossRefOh, S.; Chang, A.; Ashapure, A.; Jung, J.; Dube, N.; Maeda, M.; Gonzalez, D.; Landivar, J. Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework. Remote Sens. 2020, 12, 2981. [CrossRef]\n\n. J Redmon, A Farhadi, Yolov3, arXiv:1804.02767An incremental improvement. Redmon, J.; Farhadi, A. Yolov3: An incremental improvement. arXiv 2018, arXiv:1804.02767.\n\nCorn plant counting using deep learning and UAV images. B T Kitano, C C Mendes, A R Geus, H C Oliveira, J R Souza, 10.1109/LGRS.2019.2930549IEEE Geosci. Remote Sens. Lett. Kitano, B.T.; Mendes, C.C.; Geus, A.R.; Oliveira, H.C.; Souza, J.R. Corn plant counting using deep learning and UAV images. IEEE Geosci. Remote Sens. Lett. 2019. [CrossRef]\n", "annotations": {"author": "[{\"end\":275,\"start\":115},{\"end\":441,\"start\":276},{\"end\":601,\"start\":442}]", "publisher": null, "author_last_name": "[{\"end\":127,\"start\":121},{\"end\":293,\"start\":285},{\"end\":458,\"start\":450}]", "author_first_name": "[{\"end\":120,\"start\":115},{\"end\":282,\"start\":276},{\"end\":284,\"start\":283},{\"end\":449,\"start\":442}]", "author_affiliation": "[{\"end\":274,\"start\":129},{\"end\":440,\"start\":295},{\"end\":540,\"start\":460},{\"end\":600,\"start\":542}]", "title": "[{\"end\":84,\"start\":1},{\"end\":685,\"start\":602}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1243,\"start\":1240},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1267,\"start\":1264},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4020,\"start\":4017},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4023,\"start\":4020},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4026,\"start\":4023},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4029,\"start\":4026},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4370,\"start\":4367},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4438,\"start\":4435},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4455,\"start\":4452},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4473,\"start\":4470},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4521,\"start\":4517},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4534,\"start\":4530},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5005,\"start\":5001},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5009,\"start\":5005},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5013,\"start\":5009},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5152,\"start\":5148},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5162,\"start\":5158},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5172,\"start\":5168},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5175,\"start\":5172},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5218,\"start\":5214},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5236,\"start\":5232},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5407,\"start\":5403},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5431,\"start\":5427},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5458,\"start\":5454},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5758,\"start\":5754},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6510,\"start\":6506},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7963,\"start\":7959},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8079,\"start\":8075},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8248,\"start\":8244},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8528,\"start\":8524},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8685,\"start\":8681},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9081,\"start\":9077},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9237,\"start\":9233},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9400,\"start\":9396},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10711,\"start\":10707},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10732,\"start\":10728},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10877,\"start\":10873},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11441,\"start\":11437},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11493,\"start\":11489},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11947,\"start\":11943},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12018,\"start\":12014},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12075,\"start\":12071},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13393,\"start\":13389},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14003,\"start\":13999},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14376,\"start\":14372},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14584,\"start\":14580},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14963,\"start\":14959},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15262,\"start\":15258},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":15404,\"start\":15400},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15423,\"start\":15419},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":16028,\"start\":16024},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16262,\"start\":16258},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16859,\"start\":16855},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17365,\"start\":17361},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":17628,\"start\":17624},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":18067,\"start\":18063},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":18263,\"start\":18259},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":18454,\"start\":18450},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":18608,\"start\":18604},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18995,\"start\":18991},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20141,\"start\":20137},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20741,\"start\":20737},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21496,\"start\":21492},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":21974,\"start\":21970},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22159,\"start\":22155},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":22223,\"start\":22219},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22697,\"start\":22693},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":22735,\"start\":22731},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22978,\"start\":22974},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23457,\"start\":23453},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23628,\"start\":23624},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":23982,\"start\":23978},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24956,\"start\":24952},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":25333,\"start\":25329},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":25902,\"start\":25898},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":26901,\"start\":26897},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":28011,\"start\":28007},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28714,\"start\":28710},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29393,\"start\":29389},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":29861,\"start\":29857},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":29880,\"start\":29876},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":30464,\"start\":30460},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":30905,\"start\":30901},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":31099,\"start\":31095},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":31275,\"start\":31271},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":31616,\"start\":31612},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":31854,\"start\":31850},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":32128,\"start\":32124},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":32330,\"start\":32326},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":32530,\"start\":32526},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":32778,\"start\":32774},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":33068,\"start\":33064},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":33269,\"start\":33265},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34554,\"start\":34550},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34645,\"start\":34641},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34660,\"start\":34656},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34746,\"start\":34742},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34894,\"start\":34890},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":34897,\"start\":34894},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34900,\"start\":34897},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":35671,\"start\":35667},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":35674,\"start\":35671},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":35922,\"start\":35918},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":36437,\"start\":36433},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":36666,\"start\":36662},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":36956,\"start\":36952},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":37030,\"start\":37026},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":37095,\"start\":37091},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":38696,\"start\":38692},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38793,\"start\":38789},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":38860,\"start\":38856},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":38906,\"start\":38902},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":38913,\"start\":38909},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":39198,\"start\":39194},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":40029,\"start\":40025}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38455,\"start\":38338},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38590,\"start\":38456},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38645,\"start\":38591},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38697,\"start\":38646},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38794,\"start\":38698},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38861,\"start\":38795},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39123,\"start\":38862},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39173,\"start\":39124},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39925,\"start\":39174},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39968,\"start\":39926},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40030,\"start\":39969},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40099,\"start\":40031},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41340,\"start\":40100},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41886,\"start\":41341},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43464,\"start\":41887}]", "paragraph": "[{\"end\":1651,\"start\":1076},{\"end\":2133,\"start\":1653},{\"end\":3300,\"start\":2135},{\"end\":3602,\"start\":3332},{\"end\":3927,\"start\":3604},{\"end\":4714,\"start\":3958},{\"end\":5459,\"start\":4746},{\"end\":6152,\"start\":5511},{\"end\":6798,\"start\":6154},{\"end\":7333,\"start\":6800},{\"end\":7813,\"start\":7366},{\"end\":8519,\"start\":7842},{\"end\":9371,\"start\":8521},{\"end\":9862,\"start\":9373},{\"end\":10263,\"start\":9908},{\"end\":10451,\"start\":10334},{\"end\":10695,\"start\":10486},{\"end\":11400,\"start\":10697},{\"end\":12057,\"start\":11402},{\"end\":12555,\"start\":12059},{\"end\":13002,\"start\":12557},{\"end\":13374,\"start\":13004},{\"end\":13994,\"start\":13376},{\"end\":14564,\"start\":13996},{\"end\":14926,\"start\":14566},{\"end\":15986,\"start\":14928},{\"end\":16694,\"start\":15988},{\"end\":17327,\"start\":16696},{\"end\":17840,\"start\":17329},{\"end\":18235,\"start\":17842},{\"end\":18819,\"start\":18237},{\"end\":19667,\"start\":18821},{\"end\":19891,\"start\":19669},{\"end\":20132,\"start\":19922},{\"end\":20722,\"start\":20134},{\"end\":21478,\"start\":20724},{\"end\":22117,\"start\":21480},{\"end\":22688,\"start\":22119},{\"end\":23320,\"start\":22690},{\"end\":23936,\"start\":23322},{\"end\":24504,\"start\":23938},{\"end\":24663,\"start\":24535},{\"end\":24947,\"start\":24689},{\"end\":25378,\"start\":24949},{\"end\":25893,\"start\":25380},{\"end\":26487,\"start\":25895},{\"end\":26892,\"start\":26513},{\"end\":27405,\"start\":26894},{\"end\":27940,\"start\":27430},{\"end\":28703,\"start\":27942},{\"end\":29291,\"start\":28705},{\"end\":31256,\"start\":29293},{\"end\":33502,\"start\":31258},{\"end\":33789,\"start\":33530},{\"end\":34058,\"start\":33906},{\"end\":34619,\"start\":34060},{\"end\":35013,\"start\":34621},{\"end\":35555,\"start\":35057},{\"end\":35902,\"start\":35557},{\"end\":36576,\"start\":35904},{\"end\":37234,\"start\":36578},{\"end\":37423,\"start\":37236},{\"end\":38337,\"start\":37439}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10485,\"start\":10452},{\"attributes\":{\"id\":\"formula_1\"},\"end\":33905,\"start\":33790}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10101,\"start\":10094},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27769,\"start\":27762},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34108,\"start\":34101}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1074,\"start\":1062},{\"attributes\":{\"n\":\"2.\"},\"end\":3330,\"start\":3303},{\"attributes\":{\"n\":\"2.1.\"},\"end\":3956,\"start\":3930},{\"attributes\":{\"n\":\"2.2.\"},\"end\":4744,\"start\":4717},{\"attributes\":{\"n\":\"2.3.\"},\"end\":5509,\"start\":5462},{\"attributes\":{\"n\":\"2.4.\"},\"end\":7364,\"start\":7336},{\"attributes\":{\"n\":\"3.\"},\"end\":7840,\"start\":7816},{\"attributes\":{\"n\":\"4.\"},\"end\":9906,\"start\":9865},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10332,\"start\":10266},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":19920,\"start\":19894},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24533,\"start\":24507},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":24687,\"start\":24666},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":26511,\"start\":26490},{\"attributes\":{\"n\":\"5.\"},\"end\":27428,\"start\":27408},{\"attributes\":{\"n\":\"6.\"},\"end\":33528,\"start\":33505},{\"attributes\":{\"n\":\"7.\"},\"end\":35055,\"start\":35016},{\"attributes\":{\"n\":\"8.\"},\"end\":37437,\"start\":37426},{\"end\":38349,\"start\":38339},{\"end\":38467,\"start\":38457},{\"end\":38602,\"start\":38592},{\"end\":38657,\"start\":38647},{\"end\":38709,\"start\":38699},{\"end\":38806,\"start\":38796},{\"end\":38873,\"start\":38863},{\"end\":39135,\"start\":39125},{\"end\":39937,\"start\":39927},{\"end\":39981,\"start\":39970},{\"end\":40041,\"start\":40032},{\"end\":40110,\"start\":40101},{\"end\":41351,\"start\":41342},{\"end\":41897,\"start\":41888}]", "table": "[{\"end\":40099,\"start\":40088},{\"end\":41340,\"start\":40211},{\"end\":41886,\"start\":41358},{\"end\":43464,\"start\":41958}]", "figure_caption": "[{\"end\":38455,\"start\":38351},{\"end\":38590,\"start\":38469},{\"end\":38645,\"start\":38604},{\"end\":38697,\"start\":38659},{\"end\":38794,\"start\":38711},{\"end\":38861,\"start\":38808},{\"end\":39123,\"start\":38875},{\"end\":39173,\"start\":39137},{\"end\":39925,\"start\":39176},{\"end\":39968,\"start\":39939},{\"end\":40030,\"start\":39984},{\"end\":40088,\"start\":40043},{\"end\":40211,\"start\":40112},{\"end\":41358,\"start\":41353},{\"end\":41958,\"start\":41899}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1491,\"start\":1483},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1910,\"start\":1901},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7811,\"start\":7803},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10449,\"start\":10441},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13637,\"start\":13629},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21477,\"start\":21469},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25151,\"start\":25143},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26053,\"start\":26045},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27998,\"start\":27990},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36053,\"start\":36045},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36143,\"start\":36133}]", "bib_author_first_name": "[{\"end\":43521,\"start\":43520},{\"end\":43530,\"start\":43529},{\"end\":43537,\"start\":43536},{\"end\":43539,\"start\":43538},{\"end\":43977,\"start\":43976},{\"end\":43986,\"start\":43985},{\"end\":43992,\"start\":43991},{\"end\":43994,\"start\":43993},{\"end\":44006,\"start\":44005},{\"end\":44008,\"start\":44007},{\"end\":44453,\"start\":44452},{\"end\":44463,\"start\":44462},{\"end\":44472,\"start\":44471},{\"end\":44483,\"start\":44482},{\"end\":44830,\"start\":44829},{\"end\":44836,\"start\":44835},{\"end\":44842,\"start\":44841},{\"end\":45359,\"start\":45358},{\"end\":45373,\"start\":45372},{\"end\":45385,\"start\":45384},{\"end\":46007,\"start\":46006},{\"end\":46009,\"start\":46008},{\"end\":46020,\"start\":46019},{\"end\":46031,\"start\":46030},{\"end\":46596,\"start\":46595},{\"end\":46598,\"start\":46597},{\"end\":46610,\"start\":46609},{\"end\":46621,\"start\":46620},{\"end\":46632,\"start\":46631},{\"end\":47147,\"start\":47146},{\"end\":47154,\"start\":47153},{\"end\":47160,\"start\":47159},{\"end\":47162,\"start\":47161},{\"end\":47174,\"start\":47173},{\"end\":47457,\"start\":47456},{\"end\":47463,\"start\":47462},{\"end\":47475,\"start\":47474},{\"end\":47485,\"start\":47484},{\"end\":47487,\"start\":47486},{\"end\":47922,\"start\":47921},{\"end\":47932,\"start\":47931},{\"end\":47943,\"start\":47942},{\"end\":47945,\"start\":47944},{\"end\":47957,\"start\":47956},{\"end\":48406,\"start\":48405},{\"end\":48413,\"start\":48412},{\"end\":48425,\"start\":48424},{\"end\":48434,\"start\":48433},{\"end\":48445,\"start\":48444},{\"end\":48453,\"start\":48452},{\"end\":48455,\"start\":48454},{\"end\":48461,\"start\":48460},{\"end\":48737,\"start\":48736},{\"end\":48752,\"start\":48751},{\"end\":48754,\"start\":48753},{\"end\":48922,\"start\":48921},{\"end\":48931,\"start\":48930},{\"end\":48940,\"start\":48939},{\"end\":48948,\"start\":48947},{\"end\":49201,\"start\":49200},{\"end\":49211,\"start\":49210},{\"end\":49222,\"start\":49221},{\"end\":49233,\"start\":49232},{\"end\":49707,\"start\":49706},{\"end\":49715,\"start\":49714},{\"end\":49717,\"start\":49716},{\"end\":49724,\"start\":49723},{\"end\":49732,\"start\":49731},{\"end\":49985,\"start\":49984},{\"end\":49993,\"start\":49992},{\"end\":50266,\"start\":50265},{\"end\":50272,\"start\":50271},{\"end\":50274,\"start\":50273},{\"end\":50763,\"start\":50762},{\"end\":50771,\"start\":50770},{\"end\":50778,\"start\":50777},{\"end\":50786,\"start\":50785},{\"end\":51406,\"start\":51405},{\"end\":51408,\"start\":51407},{\"end\":51418,\"start\":51417},{\"end\":51420,\"start\":51419},{\"end\":51430,\"start\":51429},{\"end\":51434,\"start\":51431},{\"end\":51447,\"start\":51446},{\"end\":51451,\"start\":51448},{\"end\":51464,\"start\":51463},{\"end\":51468,\"start\":51465},{\"end\":51479,\"start\":51478},{\"end\":51481,\"start\":51480},{\"end\":51879,\"start\":51878},{\"end\":51888,\"start\":51887},{\"end\":51896,\"start\":51895},{\"end\":52183,\"start\":52182},{\"end\":52194,\"start\":52193},{\"end\":52206,\"start\":52205},{\"end\":52552,\"start\":52551},{\"end\":52554,\"start\":52553},{\"end\":52562,\"start\":52561},{\"end\":52873,\"start\":52872},{\"end\":52882,\"start\":52881},{\"end\":52889,\"start\":52888},{\"end\":53355,\"start\":53354},{\"end\":53368,\"start\":53367},{\"end\":53561,\"start\":53560},{\"end\":53563,\"start\":53562},{\"end\":53575,\"start\":53574},{\"end\":53579,\"start\":53576},{\"end\":53591,\"start\":53590},{\"end\":53607,\"start\":53606},{\"end\":53609,\"start\":53608},{\"end\":53618,\"start\":53617},{\"end\":53906,\"start\":53905},{\"end\":53914,\"start\":53913},{\"end\":53926,\"start\":53925},{\"end\":53939,\"start\":53938},{\"end\":54461,\"start\":54453},{\"end\":54479,\"start\":54478},{\"end\":54481,\"start\":54480},{\"end\":54490,\"start\":54489},{\"end\":54492,\"start\":54491},{\"end\":54500,\"start\":54499},{\"end\":54502,\"start\":54501},{\"end\":54750,\"start\":54749},{\"end\":54756,\"start\":54755},{\"end\":54765,\"start\":54764},{\"end\":54773,\"start\":54772},{\"end\":54779,\"start\":54778},{\"end\":54787,\"start\":54786},{\"end\":55138,\"start\":55137},{\"end\":55140,\"start\":55139},{\"end\":55151,\"start\":55150},{\"end\":55162,\"start\":55161},{\"end\":55170,\"start\":55169},{\"end\":55554,\"start\":55553},{\"end\":55556,\"start\":55555},{\"end\":55563,\"start\":55562},{\"end\":55571,\"start\":55570},{\"end\":55579,\"start\":55578},{\"end\":55914,\"start\":55913},{\"end\":55918,\"start\":55915},{\"end\":55927,\"start\":55926},{\"end\":55929,\"start\":55928},{\"end\":55939,\"start\":55938},{\"end\":56279,\"start\":56278},{\"end\":56281,\"start\":56280},{\"end\":56292,\"start\":56291},{\"end\":56294,\"start\":56293},{\"end\":56531,\"start\":56530},{\"end\":56538,\"start\":56537},{\"end\":56545,\"start\":56544},{\"end\":56552,\"start\":56551},{\"end\":56560,\"start\":56559},{\"end\":56857,\"start\":56856},{\"end\":56863,\"start\":56862},{\"end\":56869,\"start\":56868},{\"end\":56875,\"start\":56874},{\"end\":56882,\"start\":56881},{\"end\":56888,\"start\":56887},{\"end\":57171,\"start\":57170},{\"end\":57179,\"start\":57178},{\"end\":57188,\"start\":57187},{\"end\":57196,\"start\":57195},{\"end\":57203,\"start\":57202},{\"end\":57507,\"start\":57498},{\"end\":57687,\"start\":57686},{\"end\":57699,\"start\":57698},{\"end\":57711,\"start\":57710},{\"end\":58204,\"start\":58203},{\"end\":58211,\"start\":58210},{\"end\":58218,\"start\":58217},{\"end\":58226,\"start\":58225},{\"end\":58404,\"start\":58403},{\"end\":58411,\"start\":58410},{\"end\":58616,\"start\":58615},{\"end\":58622,\"start\":58621},{\"end\":58631,\"start\":58630},{\"end\":58638,\"start\":58637},{\"end\":59068,\"start\":59067},{\"end\":59078,\"start\":59077},{\"end\":59363,\"start\":59362},{\"end\":59372,\"start\":59371},{\"end\":59380,\"start\":59379},{\"end\":59388,\"start\":59387},{\"end\":59395,\"start\":59394},{\"end\":59868,\"start\":59867},{\"end\":59883,\"start\":59882},{\"end\":59885,\"start\":59884},{\"end\":59900,\"start\":59899},{\"end\":59902,\"start\":59901},{\"end\":60364,\"start\":60360},{\"end\":60371,\"start\":60370},{\"end\":60380,\"start\":60379},{\"end\":60398,\"start\":60397},{\"end\":60797,\"start\":60796},{\"end\":60799,\"start\":60798},{\"end\":60806,\"start\":60805},{\"end\":60808,\"start\":60807},{\"end\":61085,\"start\":61084},{\"end\":61092,\"start\":61091},{\"end\":61100,\"start\":61099},{\"end\":61106,\"start\":61105},{\"end\":61116,\"start\":61115},{\"end\":61325,\"start\":61324},{\"end\":61334,\"start\":61333},{\"end\":61342,\"start\":61341},{\"end\":61350,\"start\":61349},{\"end\":61357,\"start\":61356},{\"end\":61365,\"start\":61364},{\"end\":61372,\"start\":61371},{\"end\":61813,\"start\":61812},{\"end\":61824,\"start\":61823},{\"end\":61840,\"start\":61839},{\"end\":61850,\"start\":61849},{\"end\":62347,\"start\":62346},{\"end\":62354,\"start\":62353},{\"end\":62360,\"start\":62359},{\"end\":62362,\"start\":62361},{\"end\":62369,\"start\":62368},{\"end\":62777,\"start\":62776},{\"end\":62786,\"start\":62785},{\"end\":62795,\"start\":62794},{\"end\":62801,\"start\":62800},{\"end\":62810,\"start\":62809},{\"end\":62816,\"start\":62815},{\"end\":62824,\"start\":62823},{\"end\":62832,\"start\":62831},{\"end\":63222,\"start\":63221},{\"end\":63228,\"start\":63227},{\"end\":63237,\"start\":63236},{\"end\":63719,\"start\":63718},{\"end\":63728,\"start\":63727},{\"end\":63735,\"start\":63734},{\"end\":64166,\"start\":64165},{\"end\":64174,\"start\":64173},{\"end\":64182,\"start\":64181},{\"end\":64189,\"start\":64188},{\"end\":64196,\"start\":64195},{\"end\":64204,\"start\":64203},{\"end\":64506,\"start\":64505},{\"end\":64513,\"start\":64512},{\"end\":64521,\"start\":64520},{\"end\":64529,\"start\":64528},{\"end\":64535,\"start\":64534},{\"end\":64546,\"start\":64545},{\"end\":64556,\"start\":64555},{\"end\":64572,\"start\":64571},{\"end\":64908,\"start\":64907},{\"end\":64910,\"start\":64909},{\"end\":64921,\"start\":64920},{\"end\":65193,\"start\":65192},{\"end\":65202,\"start\":65201},{\"end\":65210,\"start\":65209},{\"end\":65219,\"start\":65218},{\"end\":65227,\"start\":65226},{\"end\":65234,\"start\":65233},{\"end\":65246,\"start\":65245},{\"end\":65737,\"start\":65736},{\"end\":65745,\"start\":65744},{\"end\":65754,\"start\":65753},{\"end\":65764,\"start\":65763},{\"end\":66104,\"start\":66103},{\"end\":66111,\"start\":66110},{\"end\":66119,\"start\":66118},{\"end\":66126,\"start\":66125},{\"end\":66133,\"start\":66132},{\"end\":66140,\"start\":66139},{\"end\":66655,\"start\":66654},{\"end\":66664,\"start\":66663},{\"end\":66670,\"start\":66669},{\"end\":66678,\"start\":66677},{\"end\":67154,\"start\":67153},{\"end\":67162,\"start\":67161},{\"end\":67169,\"start\":67168},{\"end\":67178,\"start\":67177},{\"end\":67180,\"start\":67179},{\"end\":67441,\"start\":67440},{\"end\":67450,\"start\":67449},{\"end\":67915,\"start\":67914},{\"end\":67924,\"start\":67923},{\"end\":67930,\"start\":67929},{\"end\":68367,\"start\":68366},{\"end\":68380,\"start\":68379},{\"end\":68392,\"start\":68391},{\"end\":68405,\"start\":68404},{\"end\":68655,\"start\":68654},{\"end\":68662,\"start\":68661},{\"end\":68669,\"start\":68668},{\"end\":68677,\"start\":68676},{\"end\":68679,\"start\":68678},{\"end\":69060,\"start\":69059},{\"end\":69067,\"start\":69066},{\"end\":69084,\"start\":69083},{\"end\":69086,\"start\":69085},{\"end\":69325,\"start\":69324},{\"end\":69332,\"start\":69331},{\"end\":69340,\"start\":69339},{\"end\":69346,\"start\":69345},{\"end\":69501,\"start\":69500},{\"end\":69510,\"start\":69509},{\"end\":69518,\"start\":69517},{\"end\":69524,\"start\":69523},{\"end\":69532,\"start\":69531},{\"end\":69771,\"start\":69770},{\"end\":69778,\"start\":69777},{\"end\":69785,\"start\":69784},{\"end\":69795,\"start\":69794},{\"end\":69797,\"start\":69796},{\"end\":69807,\"start\":69806},{\"end\":69818,\"start\":69817},{\"end\":69820,\"start\":69819},{\"end\":70041,\"start\":70040},{\"end\":70048,\"start\":70047},{\"end\":70056,\"start\":70055},{\"end\":70369,\"start\":70368},{\"end\":70376,\"start\":70375},{\"end\":70385,\"start\":70384},{\"end\":70392,\"start\":70391},{\"end\":70399,\"start\":70398},{\"end\":70405,\"start\":70404},{\"end\":70407,\"start\":70406},{\"end\":70416,\"start\":70415},{\"end\":70947,\"start\":70946},{\"end\":70958,\"start\":70957},{\"end\":70965,\"start\":70964},{\"end\":70972,\"start\":70971},{\"end\":70984,\"start\":70983},{\"end\":70992,\"start\":70991},{\"end\":71004,\"start\":71003},{\"end\":71013,\"start\":71012},{\"end\":71026,\"start\":71025},{\"end\":71563,\"start\":71562},{\"end\":71578,\"start\":71577},{\"end\":71589,\"start\":71588},{\"end\":72070,\"start\":72069},{\"end\":72072,\"start\":72071},{\"end\":72079,\"start\":72078},{\"end\":72081,\"start\":72080},{\"end\":72091,\"start\":72090},{\"end\":72093,\"start\":72092},{\"end\":72427,\"start\":72426},{\"end\":72435,\"start\":72434},{\"end\":72445,\"start\":72444},{\"end\":72745,\"start\":72744},{\"end\":72747,\"start\":72746},{\"end\":72754,\"start\":72753},{\"end\":72762,\"start\":72761},{\"end\":73033,\"start\":73032},{\"end\":73035,\"start\":73034},{\"end\":73042,\"start\":73041},{\"end\":73044,\"start\":73043},{\"end\":73051,\"start\":73050},{\"end\":73053,\"start\":73052},{\"end\":73060,\"start\":73059},{\"end\":73062,\"start\":73061},{\"end\":73554,\"start\":73553},{\"end\":73562,\"start\":73561},{\"end\":73568,\"start\":73567},{\"end\":73577,\"start\":73576},{\"end\":73584,\"start\":73583},{\"end\":74143,\"start\":74142},{\"end\":74151,\"start\":74150},{\"end\":74159,\"start\":74158},{\"end\":74167,\"start\":74166},{\"end\":74176,\"start\":74175},{\"end\":74178,\"start\":74177},{\"end\":74189,\"start\":74188},{\"end\":74199,\"start\":74198},{\"end\":74210,\"start\":74209},{\"end\":74220,\"start\":74219},{\"end\":74236,\"start\":74235},{\"end\":74654,\"start\":74653},{\"end\":74661,\"start\":74660},{\"end\":74669,\"start\":74668},{\"end\":74676,\"start\":74675},{\"end\":74683,\"start\":74682},{\"end\":74691,\"start\":74690},{\"end\":74698,\"start\":74697},{\"end\":75091,\"start\":75090},{\"end\":75098,\"start\":75097},{\"end\":75105,\"start\":75104},{\"end\":75113,\"start\":75112},{\"end\":75121,\"start\":75120},{\"end\":75299,\"start\":75298},{\"end\":75312,\"start\":75311},{\"end\":75319,\"start\":75318},{\"end\":75587,\"start\":75586},{\"end\":75595,\"start\":75594},{\"end\":75603,\"start\":75602},{\"end\":75610,\"start\":75609},{\"end\":75617,\"start\":75616},{\"end\":75907,\"start\":75906},{\"end\":75914,\"start\":75913},{\"end\":75926,\"start\":75925},{\"end\":76379,\"start\":76378},{\"end\":76388,\"start\":76387},{\"end\":76390,\"start\":76389},{\"end\":76799,\"start\":76798},{\"end\":76805,\"start\":76804},{\"end\":76812,\"start\":76811},{\"end\":76820,\"start\":76819},{\"end\":76827,\"start\":76826},{\"end\":76834,\"start\":76833},{\"end\":76841,\"start\":76840},{\"end\":77111,\"start\":77110},{\"end\":77118,\"start\":77117},{\"end\":77125,\"start\":77124},{\"end\":77132,\"start\":77131},{\"end\":77140,\"start\":77139},{\"end\":77434,\"start\":77433},{\"end\":77442,\"start\":77441},{\"end\":77449,\"start\":77448},{\"end\":77456,\"start\":77455},{\"end\":77795,\"start\":77794},{\"end\":77797,\"start\":77796},{\"end\":77808,\"start\":77807},{\"end\":77819,\"start\":77818},{\"end\":77821,\"start\":77820},{\"end\":78295,\"start\":78294},{\"end\":78305,\"start\":78304},{\"end\":78315,\"start\":78314},{\"end\":78325,\"start\":78324},{\"end\":78334,\"start\":78333},{\"end\":78348,\"start\":78347},{\"end\":78359,\"start\":78358},{\"end\":78649,\"start\":78648},{\"end\":78651,\"start\":78650},{\"end\":78658,\"start\":78657},{\"end\":78660,\"start\":78659},{\"end\":78667,\"start\":78666},{\"end\":79203,\"start\":79202},{\"end\":79213,\"start\":79212},{\"end\":79220,\"start\":79219},{\"end\":79222,\"start\":79221},{\"end\":79525,\"start\":79524},{\"end\":79534,\"start\":79533},{\"end\":79544,\"start\":79543},{\"end\":79554,\"start\":79553},{\"end\":80075,\"start\":80074},{\"end\":80083,\"start\":80082},{\"end\":80090,\"start\":80089},{\"end\":80098,\"start\":80097},{\"end\":80105,\"start\":80104},{\"end\":80410,\"start\":80409},{\"end\":80424,\"start\":80423},{\"end\":80437,\"start\":80436},{\"end\":80447,\"start\":80446},{\"end\":81039,\"start\":81038},{\"end\":81045,\"start\":81044},{\"end\":81053,\"start\":81052},{\"end\":81061,\"start\":81060},{\"end\":81067,\"start\":81066},{\"end\":81074,\"start\":81073},{\"end\":81438,\"start\":81437},{\"end\":81444,\"start\":81443},{\"end\":81453,\"start\":81452},{\"end\":81465,\"start\":81464},{\"end\":81473,\"start\":81472},{\"end\":81481,\"start\":81480},{\"end\":81490,\"start\":81489},{\"end\":81502,\"start\":81501},{\"end\":81800,\"start\":81799},{\"end\":81810,\"start\":81809},{\"end\":82020,\"start\":82019},{\"end\":82022,\"start\":82021},{\"end\":82032,\"start\":82031},{\"end\":82034,\"start\":82033},{\"end\":82044,\"start\":82043},{\"end\":82046,\"start\":82045},{\"end\":82054,\"start\":82053},{\"end\":82056,\"start\":82055},{\"end\":82068,\"start\":82067},{\"end\":82070,\"start\":82069}]", "bib_author_last_name": "[{\"end\":43527,\"start\":43522},{\"end\":43534,\"start\":43531},{\"end\":43545,\"start\":43540},{\"end\":43983,\"start\":43978},{\"end\":43989,\"start\":43987},{\"end\":44003,\"start\":43995},{\"end\":44014,\"start\":44009},{\"end\":44460,\"start\":44454},{\"end\":44469,\"start\":44464},{\"end\":44480,\"start\":44473},{\"end\":44490,\"start\":44484},{\"end\":44833,\"start\":44831},{\"end\":44839,\"start\":44837},{\"end\":44847,\"start\":44843},{\"end\":45370,\"start\":45360},{\"end\":45382,\"start\":45374},{\"end\":45396,\"start\":45386},{\"end\":46017,\"start\":46010},{\"end\":46028,\"start\":46021},{\"end\":46039,\"start\":46032},{\"end\":46607,\"start\":46599},{\"end\":46618,\"start\":46611},{\"end\":46629,\"start\":46622},{\"end\":46638,\"start\":46633},{\"end\":47151,\"start\":47148},{\"end\":47157,\"start\":47155},{\"end\":47171,\"start\":47163},{\"end\":47178,\"start\":47175},{\"end\":47192,\"start\":47180},{\"end\":47460,\"start\":47458},{\"end\":47472,\"start\":47464},{\"end\":47482,\"start\":47476},{\"end\":47496,\"start\":47488},{\"end\":47508,\"start\":47498},{\"end\":47929,\"start\":47923},{\"end\":47940,\"start\":47933},{\"end\":47954,\"start\":47946},{\"end\":47965,\"start\":47958},{\"end\":48410,\"start\":48407},{\"end\":48422,\"start\":48414},{\"end\":48431,\"start\":48426},{\"end\":48442,\"start\":48435},{\"end\":48450,\"start\":48446},{\"end\":48458,\"start\":48456},{\"end\":48466,\"start\":48462},{\"end\":48471,\"start\":48468},{\"end\":48749,\"start\":48738},{\"end\":48767,\"start\":48755},{\"end\":48928,\"start\":48923},{\"end\":48937,\"start\":48932},{\"end\":48945,\"start\":48941},{\"end\":48956,\"start\":48949},{\"end\":49208,\"start\":49202},{\"end\":49219,\"start\":49212},{\"end\":49230,\"start\":49223},{\"end\":49238,\"start\":49234},{\"end\":49712,\"start\":49708},{\"end\":49721,\"start\":49718},{\"end\":49729,\"start\":49725},{\"end\":49738,\"start\":49733},{\"end\":49990,\"start\":49986},{\"end\":50000,\"start\":49994},{\"end\":50269,\"start\":50267},{\"end\":50279,\"start\":50275},{\"end\":50768,\"start\":50764},{\"end\":50775,\"start\":50772},{\"end\":50783,\"start\":50779},{\"end\":50789,\"start\":50787},{\"end\":51415,\"start\":51409},{\"end\":51427,\"start\":51421},{\"end\":51444,\"start\":51435},{\"end\":51461,\"start\":51452},{\"end\":51476,\"start\":51469},{\"end\":51489,\"start\":51482},{\"end\":51885,\"start\":51880},{\"end\":51893,\"start\":51889},{\"end\":51901,\"start\":51897},{\"end\":52191,\"start\":52184},{\"end\":52203,\"start\":52195},{\"end\":52216,\"start\":52207},{\"end\":52559,\"start\":52555},{\"end\":52574,\"start\":52563},{\"end\":52879,\"start\":52874},{\"end\":52886,\"start\":52883},{\"end\":52894,\"start\":52890},{\"end\":53365,\"start\":53356},{\"end\":53378,\"start\":53369},{\"end\":53572,\"start\":53564},{\"end\":53588,\"start\":53580},{\"end\":53604,\"start\":53592},{\"end\":53615,\"start\":53610},{\"end\":53626,\"start\":53619},{\"end\":53911,\"start\":53907},{\"end\":53923,\"start\":53915},{\"end\":53936,\"start\":53927},{\"end\":53945,\"start\":53940},{\"end\":53953,\"start\":53947},{\"end\":54476,\"start\":54462},{\"end\":54487,\"start\":54482},{\"end\":54497,\"start\":54493},{\"end\":54753,\"start\":54751},{\"end\":54762,\"start\":54757},{\"end\":54770,\"start\":54766},{\"end\":54776,\"start\":54774},{\"end\":54784,\"start\":54780},{\"end\":54791,\"start\":54788},{\"end\":55148,\"start\":55141},{\"end\":55159,\"start\":55152},{\"end\":55167,\"start\":55163},{\"end\":55180,\"start\":55171},{\"end\":55560,\"start\":55557},{\"end\":55568,\"start\":55564},{\"end\":55576,\"start\":55572},{\"end\":55585,\"start\":55580},{\"end\":55924,\"start\":55919},{\"end\":55936,\"start\":55930},{\"end\":55947,\"start\":55940},{\"end\":56289,\"start\":56282},{\"end\":56300,\"start\":56295},{\"end\":56535,\"start\":56532},{\"end\":56542,\"start\":56539},{\"end\":56549,\"start\":56546},{\"end\":56557,\"start\":56553},{\"end\":56565,\"start\":56561},{\"end\":56860,\"start\":56858},{\"end\":56866,\"start\":56864},{\"end\":56872,\"start\":56870},{\"end\":56879,\"start\":56876},{\"end\":56885,\"start\":56883},{\"end\":56892,\"start\":56889},{\"end\":57176,\"start\":57172},{\"end\":57185,\"start\":57180},{\"end\":57193,\"start\":57189},{\"end\":57200,\"start\":57197},{\"end\":57207,\"start\":57204},{\"end\":57516,\"start\":57508},{\"end\":57696,\"start\":57688},{\"end\":57708,\"start\":57700},{\"end\":57717,\"start\":57712},{\"end\":58208,\"start\":58205},{\"end\":58215,\"start\":58212},{\"end\":58223,\"start\":58219},{\"end\":58233,\"start\":58227},{\"end\":58408,\"start\":58405},{\"end\":58417,\"start\":58412},{\"end\":58619,\"start\":58617},{\"end\":58628,\"start\":58623},{\"end\":58635,\"start\":58632},{\"end\":58642,\"start\":58639},{\"end\":59075,\"start\":59069},{\"end\":59083,\"start\":59079},{\"end\":59369,\"start\":59364},{\"end\":59377,\"start\":59373},{\"end\":59385,\"start\":59381},{\"end\":59392,\"start\":59389},{\"end\":59398,\"start\":59396},{\"end\":59880,\"start\":59869},{\"end\":59897,\"start\":59886},{\"end\":59907,\"start\":59903},{\"end\":60368,\"start\":60365},{\"end\":60377,\"start\":60372},{\"end\":60395,\"start\":60381},{\"end\":60803,\"start\":60800},{\"end\":60813,\"start\":60809},{\"end\":61089,\"start\":61086},{\"end\":61097,\"start\":61093},{\"end\":61103,\"start\":61101},{\"end\":61113,\"start\":61107},{\"end\":61120,\"start\":61117},{\"end\":61331,\"start\":61326},{\"end\":61339,\"start\":61335},{\"end\":61347,\"start\":61343},{\"end\":61354,\"start\":61351},{\"end\":61362,\"start\":61358},{\"end\":61369,\"start\":61366},{\"end\":61377,\"start\":61373},{\"end\":61821,\"start\":61814},{\"end\":61837,\"start\":61825},{\"end\":61847,\"start\":61841},{\"end\":61855,\"start\":61851},{\"end\":62351,\"start\":62348},{\"end\":62357,\"start\":62355},{\"end\":62366,\"start\":62363},{\"end\":62374,\"start\":62370},{\"end\":62382,\"start\":62376},{\"end\":62783,\"start\":62778},{\"end\":62792,\"start\":62787},{\"end\":62798,\"start\":62796},{\"end\":62807,\"start\":62802},{\"end\":62813,\"start\":62811},{\"end\":62821,\"start\":62817},{\"end\":62829,\"start\":62825},{\"end\":62837,\"start\":62833},{\"end\":63225,\"start\":63223},{\"end\":63234,\"start\":63229},{\"end\":63242,\"start\":63238},{\"end\":63725,\"start\":63720},{\"end\":63732,\"start\":63729},{\"end\":63740,\"start\":63736},{\"end\":64171,\"start\":64167},{\"end\":64179,\"start\":64175},{\"end\":64186,\"start\":64183},{\"end\":64193,\"start\":64190},{\"end\":64201,\"start\":64197},{\"end\":64208,\"start\":64205},{\"end\":64510,\"start\":64507},{\"end\":64518,\"start\":64514},{\"end\":64526,\"start\":64522},{\"end\":64532,\"start\":64530},{\"end\":64543,\"start\":64536},{\"end\":64553,\"start\":64547},{\"end\":64569,\"start\":64557},{\"end\":64578,\"start\":64573},{\"end\":64918,\"start\":64911},{\"end\":64927,\"start\":64922},{\"end\":64934,\"start\":64929},{\"end\":65199,\"start\":65194},{\"end\":65207,\"start\":65203},{\"end\":65216,\"start\":65211},{\"end\":65224,\"start\":65220},{\"end\":65231,\"start\":65228},{\"end\":65243,\"start\":65235},{\"end\":65251,\"start\":65247},{\"end\":65742,\"start\":65738},{\"end\":65751,\"start\":65746},{\"end\":65761,\"start\":65755},{\"end\":65776,\"start\":65765},{\"end\":66108,\"start\":66105},{\"end\":66116,\"start\":66112},{\"end\":66123,\"start\":66120},{\"end\":66130,\"start\":66127},{\"end\":66137,\"start\":66134},{\"end\":66143,\"start\":66141},{\"end\":66661,\"start\":66656},{\"end\":66667,\"start\":66665},{\"end\":66675,\"start\":66671},{\"end\":66683,\"start\":66679},{\"end\":67159,\"start\":67155},{\"end\":67166,\"start\":67163},{\"end\":67175,\"start\":67170},{\"end\":67185,\"start\":67181},{\"end\":67447,\"start\":67442},{\"end\":67455,\"start\":67451},{\"end\":67921,\"start\":67916},{\"end\":67927,\"start\":67925},{\"end\":67934,\"start\":67931},{\"end\":68377,\"start\":68368},{\"end\":68389,\"start\":68381},{\"end\":68402,\"start\":68393},{\"end\":68417,\"start\":68406},{\"end\":68659,\"start\":68656},{\"end\":68666,\"start\":68663},{\"end\":68674,\"start\":68670},{\"end\":68689,\"start\":68680},{\"end\":68700,\"start\":68691},{\"end\":69064,\"start\":69061},{\"end\":69081,\"start\":69068},{\"end\":69095,\"start\":69087},{\"end\":69329,\"start\":69326},{\"end\":69337,\"start\":69333},{\"end\":69343,\"start\":69341},{\"end\":69351,\"start\":69347},{\"end\":69507,\"start\":69502},{\"end\":69515,\"start\":69511},{\"end\":69521,\"start\":69519},{\"end\":69529,\"start\":69525},{\"end\":69536,\"start\":69533},{\"end\":69548,\"start\":69538},{\"end\":69775,\"start\":69772},{\"end\":69782,\"start\":69779},{\"end\":69792,\"start\":69786},{\"end\":69804,\"start\":69798},{\"end\":69815,\"start\":69808},{\"end\":69825,\"start\":69821},{\"end\":70045,\"start\":70042},{\"end\":70053,\"start\":70049},{\"end\":70059,\"start\":70057},{\"end\":70373,\"start\":70370},{\"end\":70382,\"start\":70377},{\"end\":70389,\"start\":70386},{\"end\":70396,\"start\":70393},{\"end\":70402,\"start\":70400},{\"end\":70413,\"start\":70408},{\"end\":70422,\"start\":70417},{\"end\":70955,\"start\":70948},{\"end\":70962,\"start\":70959},{\"end\":70969,\"start\":70966},{\"end\":70981,\"start\":70973},{\"end\":70989,\"start\":70985},{\"end\":71001,\"start\":70993},{\"end\":71010,\"start\":71005},{\"end\":71023,\"start\":71014},{\"end\":71037,\"start\":71027},{\"end\":71575,\"start\":71564},{\"end\":71586,\"start\":71579},{\"end\":71594,\"start\":71590},{\"end\":71601,\"start\":71596},{\"end\":72076,\"start\":72073},{\"end\":72088,\"start\":72082},{\"end\":72098,\"start\":72094},{\"end\":72106,\"start\":72100},{\"end\":72112,\"start\":72108},{\"end\":72432,\"start\":72428},{\"end\":72442,\"start\":72436},{\"end\":72457,\"start\":72446},{\"end\":72751,\"start\":72748},{\"end\":72759,\"start\":72755},{\"end\":72768,\"start\":72763},{\"end\":73039,\"start\":73036},{\"end\":73048,\"start\":73045},{\"end\":73057,\"start\":73054},{\"end\":73067,\"start\":73063},{\"end\":73559,\"start\":73555},{\"end\":73565,\"start\":73563},{\"end\":73574,\"start\":73569},{\"end\":73581,\"start\":73578},{\"end\":73588,\"start\":73585},{\"end\":74148,\"start\":74144},{\"end\":74156,\"start\":74152},{\"end\":74164,\"start\":74160},{\"end\":74173,\"start\":74168},{\"end\":74186,\"start\":74179},{\"end\":74196,\"start\":74190},{\"end\":74207,\"start\":74200},{\"end\":74217,\"start\":74211},{\"end\":74233,\"start\":74221},{\"end\":74244,\"start\":74237},{\"end\":74658,\"start\":74655},{\"end\":74666,\"start\":74662},{\"end\":74673,\"start\":74670},{\"end\":74680,\"start\":74677},{\"end\":74688,\"start\":74684},{\"end\":74695,\"start\":74692},{\"end\":74703,\"start\":74699},{\"end\":75095,\"start\":75092},{\"end\":75102,\"start\":75099},{\"end\":75110,\"start\":75106},{\"end\":75118,\"start\":75114},{\"end\":75124,\"start\":75122},{\"end\":75309,\"start\":75300},{\"end\":75316,\"start\":75313},{\"end\":75328,\"start\":75320},{\"end\":75336,\"start\":75330},{\"end\":75592,\"start\":75588},{\"end\":75600,\"start\":75596},{\"end\":75607,\"start\":75604},{\"end\":75614,\"start\":75611},{\"end\":75620,\"start\":75618},{\"end\":75911,\"start\":75908},{\"end\":75923,\"start\":75915},{\"end\":75930,\"start\":75927},{\"end\":76385,\"start\":76380},{\"end\":76395,\"start\":76391},{\"end\":76802,\"start\":76800},{\"end\":76809,\"start\":76806},{\"end\":76817,\"start\":76813},{\"end\":76824,\"start\":76821},{\"end\":76831,\"start\":76828},{\"end\":76838,\"start\":76835},{\"end\":76845,\"start\":76842},{\"end\":77115,\"start\":77112},{\"end\":77122,\"start\":77119},{\"end\":77129,\"start\":77126},{\"end\":77137,\"start\":77133},{\"end\":77143,\"start\":77141},{\"end\":77439,\"start\":77435},{\"end\":77446,\"start\":77443},{\"end\":77453,\"start\":77450},{\"end\":77459,\"start\":77457},{\"end\":77471,\"start\":77461},{\"end\":77805,\"start\":77798},{\"end\":77816,\"start\":77809},{\"end\":77827,\"start\":77822},{\"end\":78302,\"start\":78296},{\"end\":78312,\"start\":78306},{\"end\":78322,\"start\":78316},{\"end\":78331,\"start\":78326},{\"end\":78345,\"start\":78335},{\"end\":78356,\"start\":78349},{\"end\":78364,\"start\":78360},{\"end\":78655,\"start\":78652},{\"end\":78664,\"start\":78661},{\"end\":78671,\"start\":78668},{\"end\":79210,\"start\":79204},{\"end\":79217,\"start\":79214},{\"end\":79228,\"start\":79223},{\"end\":79531,\"start\":79526},{\"end\":79541,\"start\":79535},{\"end\":79551,\"start\":79545},{\"end\":79562,\"start\":79555},{\"end\":80080,\"start\":80076},{\"end\":80087,\"start\":80084},{\"end\":80095,\"start\":80091},{\"end\":80102,\"start\":80099},{\"end\":80109,\"start\":80106},{\"end\":80421,\"start\":80411},{\"end\":80434,\"start\":80425},{\"end\":80444,\"start\":80438},{\"end\":80454,\"start\":80448},{\"end\":81042,\"start\":81040},{\"end\":81050,\"start\":81046},{\"end\":81058,\"start\":81054},{\"end\":81064,\"start\":81062},{\"end\":81071,\"start\":81068},{\"end\":81078,\"start\":81075},{\"end\":81441,\"start\":81439},{\"end\":81450,\"start\":81445},{\"end\":81462,\"start\":81454},{\"end\":81470,\"start\":81466},{\"end\":81478,\"start\":81474},{\"end\":81487,\"start\":81482},{\"end\":81499,\"start\":81491},{\"end\":81511,\"start\":81503},{\"end\":81807,\"start\":81801},{\"end\":81818,\"start\":81811},{\"end\":81826,\"start\":81820},{\"end\":82029,\"start\":82023},{\"end\":82041,\"start\":82035},{\"end\":82051,\"start\":82047},{\"end\":82065,\"start\":82057},{\"end\":82076,\"start\":82071}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":24725602},\"end\":43888,\"start\":43466},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10553647},\"end\":44389,\"start\":43890},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2011.155\",\"id\":\"b2\",\"matched_paper_id\":206764948},\"end\":44733,\"start\":44391},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18550560},\"end\":45296,\"start\":44735},{\"attributes\":{\"doi\":\"10.1109/AVSS.2012.87\",\"id\":\"b4\",\"matched_paper_id\":15508488},\"end\":45949,\"start\":45298},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7454625},\"end\":46511,\"start\":45951},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":215827080},\"end\":47078,\"start\":46513},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2016.2577031\",\"id\":\"b7\",\"matched_paper_id\":10328909},\"end\":47454,\"start\":47080},{\"attributes\":{\"id\":\"b8\"},\"end\":47862,\"start\":47456},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594738},\"end\":48403,\"start\":47864},{\"attributes\":{\"doi\":\"arXiv:1512.02325\",\"id\":\"b10\"},\"end\":48673,\"start\":48405},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":40499053},\"end\":48917,\"start\":48675},{\"attributes\":{\"id\":\"b12\"},\"end\":49131,\"start\":48919},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9749221},\"end\":49659,\"start\":49133},{\"attributes\":{\"id\":\"b14\"},\"end\":49936,\"start\":49661},{\"attributes\":{\"id\":\"b15\"},\"end\":50185,\"start\":49938},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5937097},\"end\":50683,\"start\":50187},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17479762},\"end\":51227,\"start\":50685},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":125808321},\"end\":51874,\"start\":51229},{\"attributes\":{\"doi\":\"10.1117/12.2502020\",\"id\":\"b19\"},\"end\":52132,\"start\":51876},{\"attributes\":{\"doi\":\"10.1007/978-3-642-33926-4_69\",\"id\":\"b20\",\"matched_paper_id\":15266669},\"end\":52484,\"start\":52134},{\"attributes\":{\"doi\":\"10.1109/TIP.2011.2172800\",\"id\":\"b21\",\"matched_paper_id\":1398960},\"end\":52780,\"start\":52486},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6362094},\"end\":53315,\"start\":52782},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":18018217},\"end\":53556,\"start\":53317},{\"attributes\":{\"id\":\"b24\"},\"end\":53815,\"start\":53558},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1718465},\"end\":54402,\"start\":53817},{\"attributes\":{\"doi\":\"10.1109/MSP.2010.937394\",\"id\":\"b26\",\"matched_paper_id\":206485682},\"end\":54713,\"start\":54404},{\"attributes\":{\"doi\":\"10.1109/TCSVT.2014.2358029\",\"id\":\"b27\",\"matched_paper_id\":4915696},\"end\":55023,\"start\":54715},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2015.12.070\",\"id\":\"b28\",\"matched_paper_id\":207112729},\"end\":55438,\"start\":55025},{\"attributes\":{\"id\":\"b29\"},\"end\":55831,\"start\":55440},{\"attributes\":{\"doi\":\"10.1016/j.engappai.2015.01.007\",\"id\":\"b30\",\"matched_paper_id\":13164291},\"end\":56183,\"start\":55833},{\"attributes\":{\"doi\":\"10.1016/j.patrec.2017.07.007\",\"id\":\"b31\",\"matched_paper_id\":13709403},\"end\":56528,\"start\":56185},{\"attributes\":{\"doi\":\"arXiv:2003.12783\",\"id\":\"b32\"},\"end\":56788,\"start\":56530},{\"attributes\":{\"doi\":\"10.1016/j.engappai.2015.04.006\",\"id\":\"b33\",\"matched_paper_id\":31520628},\"end\":57120,\"start\":56790},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4509294},\"end\":57494,\"start\":57122},{\"attributes\":{\"doi\":\"10.1145/2733373.2806337\",\"id\":\"b35\"},\"end\":57607,\"start\":57496},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6788752},\"end\":58137,\"start\":57609},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":29301584},\"end\":58399,\"start\":58139},{\"attributes\":{\"id\":\"b38\"},\"end\":58567,\"start\":58401},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206594692},\"end\":59028,\"start\":58569},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":26569197},\"end\":59285,\"start\":59030},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4545310},\"end\":59800,\"start\":59287},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":697405},\"end\":60299,\"start\":59802},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1089358},\"end\":60794,\"start\":60301},{\"attributes\":{\"doi\":\"arXiv:1807.08881\",\"id\":\"b44\"},\"end\":61023,\"start\":60796},{\"attributes\":{\"doi\":\"arXiv:1807.00601\",\"id\":\"b45\"},\"end\":61273,\"start\":61025},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":207901201},\"end\":61757,\"start\":61275},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":53701783},\"end\":62287,\"start\":61759},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":204837048},\"end\":62736,\"start\":62289},{\"attributes\":{\"id\":\"b49\"},\"end\":63126,\"start\":62738},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":3645757},\"end\":63652,\"start\":63128},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":13442421},\"end\":64163,\"start\":63654},{\"attributes\":{\"doi\":\"arXiv:1808.06133\",\"id\":\"b52\"},\"end\":64434,\"start\":64165},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":52955811},\"end\":64905,\"start\":64436},{\"attributes\":{\"doi\":\"arXiv:1903.11249\",\"id\":\"b54\"},\"end\":65115,\"start\":64907},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":67856061},\"end\":65696,\"start\":65117},{\"attributes\":{\"id\":\"b56\"},\"end\":66008,\"start\":65698},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":53957027},\"end\":66585,\"start\":66010},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":2131202},\"end\":67115,\"start\":66587},{\"attributes\":{\"doi\":\"10.1109/TIP.2019.2952083\",\"id\":\"b59\",\"matched_paper_id\":53227020},\"end\":67375,\"start\":67117},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":210023694},\"end\":67843,\"start\":67377},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":7738307},\"end\":68334,\"start\":67845},{\"attributes\":{\"id\":\"b62\"},\"end\":68652,\"start\":68336},{\"attributes\":{\"doi\":\"arXiv:1712.06679\",\"id\":\"b63\"},\"end\":68996,\"start\":68654},{\"attributes\":{\"doi\":\"arXiv:1803.03095\",\"id\":\"b64\",\"matched_paper_id\":3787969},\"end\":69257,\"start\":68998},{\"attributes\":{\"doi\":\"arXiv:1807.01989\",\"id\":\"b65\"},\"end\":69498,\"start\":69259},{\"attributes\":{\"doi\":\"arXiv:2104.09116\",\"id\":\"b66\"},\"end\":69768,\"start\":69500},{\"attributes\":{\"doi\":\"arXiv:2105.10926\",\"id\":\"b67\"},\"end\":70038,\"start\":69770},{\"attributes\":{\"doi\":\"arXiv:2108.00584\",\"id\":\"b68\"},\"end\":70310,\"start\":70040},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00564\",\"id\":\"b69\",\"matched_paper_id\":52243494},\"end\":70912,\"start\":70312},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":206592484},\"end\":71502,\"start\":70914},{\"attributes\":{\"id\":\"b71\"},\"end\":72067,\"start\":71504},{\"attributes\":{\"doi\":\"arXiv:1807.09993\",\"id\":\"b72\"},\"end\":72370,\"start\":72069},{\"attributes\":{\"id\":\"b73\"},\"end\":72689,\"start\":72372},{\"attributes\":{\"id\":\"b74\"},\"end\":72972,\"start\":72691},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":8006469},\"end\":73460,\"start\":72974},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":196194550},\"end\":74027,\"start\":73462},{\"attributes\":{\"id\":\"b77\"},\"end\":74591,\"start\":74029},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":202577232},\"end\":75088,\"start\":74593},{\"attributes\":{\"doi\":\"arXiv:1804.07437\",\"id\":\"b79\"},\"end\":75296,\"start\":75090},{\"attributes\":{\"doi\":\"arXiv:1909.12743\",\"id\":\"b80\"},\"end\":75584,\"start\":75298},{\"attributes\":{\"doi\":\"arXiv:1907.07911\",\"id\":\"b81\"},\"end\":75874,\"start\":75586},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":53783843},\"end\":76293,\"start\":75876},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":197464061},\"end\":76796,\"start\":76295},{\"attributes\":{\"doi\":\"arXiv:2005.07097\",\"id\":\"b84\"},\"end\":77108,\"start\":76798},{\"attributes\":{\"doi\":\"arXiv:2003.02437\",\"id\":\"b85\"},\"end\":77370,\"start\":77110},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2020.3013269\",\"id\":\"b86\",\"matched_paper_id\":210157261},\"end\":77703,\"start\":77372},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":204956017},\"end\":78204,\"start\":77705},{\"attributes\":{\"doi\":\"arXiv:1808.01050\",\"id\":\"b88\"},\"end\":78584,\"start\":78206},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":3865699},\"end\":79089,\"start\":78586},{\"attributes\":{\"doi\":\"10.1109/ACCESS.2017.2782260\",\"id\":\"b90\",\"matched_paper_id\":3751347},\"end\":79461,\"start\":79091},{\"attributes\":{\"doi\":\"10.1109/ISCC47284.2019.8969620\",\"id\":\"b91\",\"matched_paper_id\":210971161},\"end\":79999,\"start\":79463},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2020.09.001\",\"id\":\"b92\",\"matched_paper_id\":224999936},\"end\":80320,\"start\":80001},{\"attributes\":{\"doi\":\"10.1109/IJCNN48605.2020.9206974\",\"id\":\"b93\",\"matched_paper_id\":221552060},\"end\":80923,\"start\":80322},{\"attributes\":{\"doi\":\"10.3390/rs11060691\",\"id\":\"b94\"},\"end\":81339,\"start\":80925},{\"attributes\":{\"doi\":\"10.3390/rs12182981\",\"id\":\"b95\"},\"end\":81795,\"start\":81341},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b96\"},\"end\":81961,\"start\":81797},{\"attributes\":{\"doi\":\"10.1109/LGRS.2019.2930549\",\"id\":\"b97\",\"matched_paper_id\":202137725},\"end\":82307,\"start\":81963}]", "bib_title": "[{\"end\":43518,\"start\":43466},{\"end\":43974,\"start\":43890},{\"end\":44450,\"start\":44391},{\"end\":44827,\"start\":44735},{\"end\":45356,\"start\":45298},{\"end\":46004,\"start\":45951},{\"end\":46593,\"start\":46513},{\"end\":47144,\"start\":47080},{\"end\":47919,\"start\":47864},{\"end\":48734,\"start\":48675},{\"end\":49198,\"start\":49133},{\"end\":50263,\"start\":50187},{\"end\":50760,\"start\":50685},{\"end\":51403,\"start\":51229},{\"end\":52180,\"start\":52134},{\"end\":52549,\"start\":52486},{\"end\":52870,\"start\":52782},{\"end\":53352,\"start\":53317},{\"end\":53903,\"start\":53817},{\"end\":54451,\"start\":54404},{\"end\":54747,\"start\":54715},{\"end\":55135,\"start\":55025},{\"end\":55911,\"start\":55833},{\"end\":56276,\"start\":56185},{\"end\":56854,\"start\":56790},{\"end\":57168,\"start\":57122},{\"end\":57684,\"start\":57609},{\"end\":58201,\"start\":58139},{\"end\":58613,\"start\":58569},{\"end\":59065,\"start\":59030},{\"end\":59360,\"start\":59287},{\"end\":59865,\"start\":59802},{\"end\":60358,\"start\":60301},{\"end\":61322,\"start\":61275},{\"end\":61810,\"start\":61759},{\"end\":62344,\"start\":62289},{\"end\":63219,\"start\":63128},{\"end\":63716,\"start\":63654},{\"end\":64503,\"start\":64436},{\"end\":65190,\"start\":65117},{\"end\":66101,\"start\":66010},{\"end\":66652,\"start\":66587},{\"end\":67151,\"start\":67117},{\"end\":67438,\"start\":67377},{\"end\":67912,\"start\":67845},{\"end\":69057,\"start\":68998},{\"end\":70366,\"start\":70312},{\"end\":70944,\"start\":70914},{\"end\":71560,\"start\":71504},{\"end\":73030,\"start\":72974},{\"end\":73551,\"start\":73462},{\"end\":74651,\"start\":74593},{\"end\":75904,\"start\":75876},{\"end\":76376,\"start\":76295},{\"end\":77431,\"start\":77372},{\"end\":77792,\"start\":77705},{\"end\":78646,\"start\":78586},{\"end\":79200,\"start\":79091},{\"end\":79522,\"start\":79463},{\"end\":80072,\"start\":80001},{\"end\":80407,\"start\":80322},{\"end\":82017,\"start\":81963}]", "bib_author": "[{\"end\":43529,\"start\":43520},{\"end\":43536,\"start\":43529},{\"end\":43547,\"start\":43536},{\"end\":43985,\"start\":43976},{\"end\":43991,\"start\":43985},{\"end\":44005,\"start\":43991},{\"end\":44016,\"start\":44005},{\"end\":44462,\"start\":44452},{\"end\":44471,\"start\":44462},{\"end\":44482,\"start\":44471},{\"end\":44492,\"start\":44482},{\"end\":44835,\"start\":44829},{\"end\":44841,\"start\":44835},{\"end\":44849,\"start\":44841},{\"end\":45372,\"start\":45358},{\"end\":45384,\"start\":45372},{\"end\":45398,\"start\":45384},{\"end\":46019,\"start\":46006},{\"end\":46030,\"start\":46019},{\"end\":46041,\"start\":46030},{\"end\":46609,\"start\":46595},{\"end\":46620,\"start\":46609},{\"end\":46631,\"start\":46620},{\"end\":46640,\"start\":46631},{\"end\":47153,\"start\":47146},{\"end\":47159,\"start\":47153},{\"end\":47173,\"start\":47159},{\"end\":47180,\"start\":47173},{\"end\":47194,\"start\":47180},{\"end\":47462,\"start\":47456},{\"end\":47474,\"start\":47462},{\"end\":47484,\"start\":47474},{\"end\":47498,\"start\":47484},{\"end\":47510,\"start\":47498},{\"end\":47931,\"start\":47921},{\"end\":47942,\"start\":47931},{\"end\":47956,\"start\":47942},{\"end\":47967,\"start\":47956},{\"end\":48412,\"start\":48405},{\"end\":48424,\"start\":48412},{\"end\":48433,\"start\":48424},{\"end\":48444,\"start\":48433},{\"end\":48452,\"start\":48444},{\"end\":48460,\"start\":48452},{\"end\":48468,\"start\":48460},{\"end\":48473,\"start\":48468},{\"end\":48751,\"start\":48736},{\"end\":48769,\"start\":48751},{\"end\":48930,\"start\":48921},{\"end\":48939,\"start\":48930},{\"end\":48947,\"start\":48939},{\"end\":48958,\"start\":48947},{\"end\":49210,\"start\":49200},{\"end\":49221,\"start\":49210},{\"end\":49232,\"start\":49221},{\"end\":49240,\"start\":49232},{\"end\":49714,\"start\":49706},{\"end\":49723,\"start\":49714},{\"end\":49731,\"start\":49723},{\"end\":49740,\"start\":49731},{\"end\":49992,\"start\":49984},{\"end\":50002,\"start\":49992},{\"end\":50271,\"start\":50265},{\"end\":50281,\"start\":50271},{\"end\":50770,\"start\":50762},{\"end\":50777,\"start\":50770},{\"end\":50785,\"start\":50777},{\"end\":50791,\"start\":50785},{\"end\":51417,\"start\":51405},{\"end\":51429,\"start\":51417},{\"end\":51446,\"start\":51429},{\"end\":51463,\"start\":51446},{\"end\":51478,\"start\":51463},{\"end\":51491,\"start\":51478},{\"end\":51887,\"start\":51878},{\"end\":51895,\"start\":51887},{\"end\":51903,\"start\":51895},{\"end\":52193,\"start\":52182},{\"end\":52205,\"start\":52193},{\"end\":52218,\"start\":52205},{\"end\":52561,\"start\":52551},{\"end\":52576,\"start\":52561},{\"end\":52881,\"start\":52872},{\"end\":52888,\"start\":52881},{\"end\":52896,\"start\":52888},{\"end\":53367,\"start\":53354},{\"end\":53380,\"start\":53367},{\"end\":53574,\"start\":53560},{\"end\":53590,\"start\":53574},{\"end\":53606,\"start\":53590},{\"end\":53617,\"start\":53606},{\"end\":53628,\"start\":53617},{\"end\":53913,\"start\":53905},{\"end\":53925,\"start\":53913},{\"end\":53938,\"start\":53925},{\"end\":53947,\"start\":53938},{\"end\":53955,\"start\":53947},{\"end\":54478,\"start\":54453},{\"end\":54489,\"start\":54478},{\"end\":54499,\"start\":54489},{\"end\":54505,\"start\":54499},{\"end\":54755,\"start\":54749},{\"end\":54764,\"start\":54755},{\"end\":54772,\"start\":54764},{\"end\":54778,\"start\":54772},{\"end\":54786,\"start\":54778},{\"end\":54793,\"start\":54786},{\"end\":55150,\"start\":55137},{\"end\":55161,\"start\":55150},{\"end\":55169,\"start\":55161},{\"end\":55182,\"start\":55169},{\"end\":55562,\"start\":55553},{\"end\":55570,\"start\":55562},{\"end\":55578,\"start\":55570},{\"end\":55587,\"start\":55578},{\"end\":55926,\"start\":55913},{\"end\":55938,\"start\":55926},{\"end\":55949,\"start\":55938},{\"end\":56291,\"start\":56278},{\"end\":56302,\"start\":56291},{\"end\":56537,\"start\":56530},{\"end\":56544,\"start\":56537},{\"end\":56551,\"start\":56544},{\"end\":56559,\"start\":56551},{\"end\":56567,\"start\":56559},{\"end\":56862,\"start\":56856},{\"end\":56868,\"start\":56862},{\"end\":56874,\"start\":56868},{\"end\":56881,\"start\":56874},{\"end\":56887,\"start\":56881},{\"end\":56894,\"start\":56887},{\"end\":57178,\"start\":57170},{\"end\":57187,\"start\":57178},{\"end\":57195,\"start\":57187},{\"end\":57202,\"start\":57195},{\"end\":57209,\"start\":57202},{\"end\":57518,\"start\":57498},{\"end\":57698,\"start\":57686},{\"end\":57710,\"start\":57698},{\"end\":57719,\"start\":57710},{\"end\":58210,\"start\":58203},{\"end\":58217,\"start\":58210},{\"end\":58225,\"start\":58217},{\"end\":58235,\"start\":58225},{\"end\":58410,\"start\":58403},{\"end\":58419,\"start\":58410},{\"end\":58621,\"start\":58615},{\"end\":58630,\"start\":58621},{\"end\":58637,\"start\":58630},{\"end\":58644,\"start\":58637},{\"end\":59077,\"start\":59067},{\"end\":59085,\"start\":59077},{\"end\":59371,\"start\":59362},{\"end\":59379,\"start\":59371},{\"end\":59387,\"start\":59379},{\"end\":59394,\"start\":59387},{\"end\":59400,\"start\":59394},{\"end\":59882,\"start\":59867},{\"end\":59899,\"start\":59882},{\"end\":59909,\"start\":59899},{\"end\":60370,\"start\":60360},{\"end\":60379,\"start\":60370},{\"end\":60397,\"start\":60379},{\"end\":60401,\"start\":60397},{\"end\":60805,\"start\":60796},{\"end\":60815,\"start\":60805},{\"end\":61091,\"start\":61084},{\"end\":61099,\"start\":61091},{\"end\":61105,\"start\":61099},{\"end\":61115,\"start\":61105},{\"end\":61122,\"start\":61115},{\"end\":61333,\"start\":61324},{\"end\":61341,\"start\":61333},{\"end\":61349,\"start\":61341},{\"end\":61356,\"start\":61349},{\"end\":61364,\"start\":61356},{\"end\":61371,\"start\":61364},{\"end\":61379,\"start\":61371},{\"end\":61823,\"start\":61812},{\"end\":61839,\"start\":61823},{\"end\":61849,\"start\":61839},{\"end\":61857,\"start\":61849},{\"end\":62353,\"start\":62346},{\"end\":62359,\"start\":62353},{\"end\":62368,\"start\":62359},{\"end\":62376,\"start\":62368},{\"end\":62384,\"start\":62376},{\"end\":62785,\"start\":62776},{\"end\":62794,\"start\":62785},{\"end\":62800,\"start\":62794},{\"end\":62809,\"start\":62800},{\"end\":62815,\"start\":62809},{\"end\":62823,\"start\":62815},{\"end\":62831,\"start\":62823},{\"end\":62839,\"start\":62831},{\"end\":63227,\"start\":63221},{\"end\":63236,\"start\":63227},{\"end\":63244,\"start\":63236},{\"end\":63727,\"start\":63718},{\"end\":63734,\"start\":63727},{\"end\":63742,\"start\":63734},{\"end\":64173,\"start\":64165},{\"end\":64181,\"start\":64173},{\"end\":64188,\"start\":64181},{\"end\":64195,\"start\":64188},{\"end\":64203,\"start\":64195},{\"end\":64210,\"start\":64203},{\"end\":64512,\"start\":64505},{\"end\":64520,\"start\":64512},{\"end\":64528,\"start\":64520},{\"end\":64534,\"start\":64528},{\"end\":64545,\"start\":64534},{\"end\":64555,\"start\":64545},{\"end\":64571,\"start\":64555},{\"end\":64580,\"start\":64571},{\"end\":64920,\"start\":64907},{\"end\":64929,\"start\":64920},{\"end\":64936,\"start\":64929},{\"end\":65201,\"start\":65192},{\"end\":65209,\"start\":65201},{\"end\":65218,\"start\":65209},{\"end\":65226,\"start\":65218},{\"end\":65233,\"start\":65226},{\"end\":65245,\"start\":65233},{\"end\":65253,\"start\":65245},{\"end\":65744,\"start\":65736},{\"end\":65753,\"start\":65744},{\"end\":65763,\"start\":65753},{\"end\":65778,\"start\":65763},{\"end\":66110,\"start\":66103},{\"end\":66118,\"start\":66110},{\"end\":66125,\"start\":66118},{\"end\":66132,\"start\":66125},{\"end\":66139,\"start\":66132},{\"end\":66145,\"start\":66139},{\"end\":66663,\"start\":66654},{\"end\":66669,\"start\":66663},{\"end\":66677,\"start\":66669},{\"end\":66685,\"start\":66677},{\"end\":67161,\"start\":67153},{\"end\":67168,\"start\":67161},{\"end\":67177,\"start\":67168},{\"end\":67187,\"start\":67177},{\"end\":67449,\"start\":67440},{\"end\":67457,\"start\":67449},{\"end\":67923,\"start\":67914},{\"end\":67929,\"start\":67923},{\"end\":67936,\"start\":67929},{\"end\":68379,\"start\":68366},{\"end\":68391,\"start\":68379},{\"end\":68404,\"start\":68391},{\"end\":68419,\"start\":68404},{\"end\":68661,\"start\":68654},{\"end\":68668,\"start\":68661},{\"end\":68676,\"start\":68668},{\"end\":68691,\"start\":68676},{\"end\":68702,\"start\":68691},{\"end\":69066,\"start\":69059},{\"end\":69083,\"start\":69066},{\"end\":69097,\"start\":69083},{\"end\":69331,\"start\":69324},{\"end\":69339,\"start\":69331},{\"end\":69345,\"start\":69339},{\"end\":69353,\"start\":69345},{\"end\":69509,\"start\":69500},{\"end\":69517,\"start\":69509},{\"end\":69523,\"start\":69517},{\"end\":69531,\"start\":69523},{\"end\":69538,\"start\":69531},{\"end\":69550,\"start\":69538},{\"end\":69777,\"start\":69770},{\"end\":69784,\"start\":69777},{\"end\":69794,\"start\":69784},{\"end\":69806,\"start\":69794},{\"end\":69817,\"start\":69806},{\"end\":69827,\"start\":69817},{\"end\":70047,\"start\":70040},{\"end\":70055,\"start\":70047},{\"end\":70061,\"start\":70055},{\"end\":70375,\"start\":70368},{\"end\":70384,\"start\":70375},{\"end\":70391,\"start\":70384},{\"end\":70398,\"start\":70391},{\"end\":70404,\"start\":70398},{\"end\":70415,\"start\":70404},{\"end\":70424,\"start\":70415},{\"end\":70957,\"start\":70946},{\"end\":70964,\"start\":70957},{\"end\":70971,\"start\":70964},{\"end\":70983,\"start\":70971},{\"end\":70991,\"start\":70983},{\"end\":71003,\"start\":70991},{\"end\":71012,\"start\":71003},{\"end\":71025,\"start\":71012},{\"end\":71039,\"start\":71025},{\"end\":71577,\"start\":71562},{\"end\":71588,\"start\":71577},{\"end\":71596,\"start\":71588},{\"end\":71603,\"start\":71596},{\"end\":72078,\"start\":72069},{\"end\":72090,\"start\":72078},{\"end\":72100,\"start\":72090},{\"end\":72108,\"start\":72100},{\"end\":72114,\"start\":72108},{\"end\":72434,\"start\":72426},{\"end\":72444,\"start\":72434},{\"end\":72459,\"start\":72444},{\"end\":72753,\"start\":72744},{\"end\":72761,\"start\":72753},{\"end\":72770,\"start\":72761},{\"end\":73041,\"start\":73032},{\"end\":73050,\"start\":73041},{\"end\":73059,\"start\":73050},{\"end\":73069,\"start\":73059},{\"end\":73561,\"start\":73553},{\"end\":73567,\"start\":73561},{\"end\":73576,\"start\":73567},{\"end\":73583,\"start\":73576},{\"end\":73590,\"start\":73583},{\"end\":74150,\"start\":74142},{\"end\":74158,\"start\":74150},{\"end\":74166,\"start\":74158},{\"end\":74175,\"start\":74166},{\"end\":74188,\"start\":74175},{\"end\":74198,\"start\":74188},{\"end\":74209,\"start\":74198},{\"end\":74219,\"start\":74209},{\"end\":74235,\"start\":74219},{\"end\":74246,\"start\":74235},{\"end\":74660,\"start\":74653},{\"end\":74668,\"start\":74660},{\"end\":74675,\"start\":74668},{\"end\":74682,\"start\":74675},{\"end\":74690,\"start\":74682},{\"end\":74697,\"start\":74690},{\"end\":74705,\"start\":74697},{\"end\":75097,\"start\":75090},{\"end\":75104,\"start\":75097},{\"end\":75112,\"start\":75104},{\"end\":75120,\"start\":75112},{\"end\":75126,\"start\":75120},{\"end\":75311,\"start\":75298},{\"end\":75318,\"start\":75311},{\"end\":75330,\"start\":75318},{\"end\":75338,\"start\":75330},{\"end\":75594,\"start\":75586},{\"end\":75602,\"start\":75594},{\"end\":75609,\"start\":75602},{\"end\":75616,\"start\":75609},{\"end\":75622,\"start\":75616},{\"end\":75913,\"start\":75906},{\"end\":75925,\"start\":75913},{\"end\":75932,\"start\":75925},{\"end\":76387,\"start\":76378},{\"end\":76397,\"start\":76387},{\"end\":76804,\"start\":76798},{\"end\":76811,\"start\":76804},{\"end\":76819,\"start\":76811},{\"end\":76826,\"start\":76819},{\"end\":76833,\"start\":76826},{\"end\":76840,\"start\":76833},{\"end\":76847,\"start\":76840},{\"end\":77117,\"start\":77110},{\"end\":77124,\"start\":77117},{\"end\":77131,\"start\":77124},{\"end\":77139,\"start\":77131},{\"end\":77145,\"start\":77139},{\"end\":77441,\"start\":77433},{\"end\":77448,\"start\":77441},{\"end\":77455,\"start\":77448},{\"end\":77461,\"start\":77455},{\"end\":77473,\"start\":77461},{\"end\":77807,\"start\":77794},{\"end\":77818,\"start\":77807},{\"end\":77829,\"start\":77818},{\"end\":78304,\"start\":78294},{\"end\":78314,\"start\":78304},{\"end\":78324,\"start\":78314},{\"end\":78333,\"start\":78324},{\"end\":78347,\"start\":78333},{\"end\":78358,\"start\":78347},{\"end\":78366,\"start\":78358},{\"end\":78657,\"start\":78648},{\"end\":78666,\"start\":78657},{\"end\":78673,\"start\":78666},{\"end\":79212,\"start\":79202},{\"end\":79219,\"start\":79212},{\"end\":79230,\"start\":79219},{\"end\":79533,\"start\":79524},{\"end\":79543,\"start\":79533},{\"end\":79553,\"start\":79543},{\"end\":79564,\"start\":79553},{\"end\":80082,\"start\":80074},{\"end\":80089,\"start\":80082},{\"end\":80097,\"start\":80089},{\"end\":80104,\"start\":80097},{\"end\":80111,\"start\":80104},{\"end\":80423,\"start\":80409},{\"end\":80436,\"start\":80423},{\"end\":80446,\"start\":80436},{\"end\":80456,\"start\":80446},{\"end\":81044,\"start\":81038},{\"end\":81052,\"start\":81044},{\"end\":81060,\"start\":81052},{\"end\":81066,\"start\":81060},{\"end\":81073,\"start\":81066},{\"end\":81080,\"start\":81073},{\"end\":81443,\"start\":81437},{\"end\":81452,\"start\":81443},{\"end\":81464,\"start\":81452},{\"end\":81472,\"start\":81464},{\"end\":81480,\"start\":81472},{\"end\":81489,\"start\":81480},{\"end\":81501,\"start\":81489},{\"end\":81513,\"start\":81501},{\"end\":81809,\"start\":81799},{\"end\":81820,\"start\":81809},{\"end\":81828,\"start\":81820},{\"end\":82031,\"start\":82019},{\"end\":82043,\"start\":82031},{\"end\":82053,\"start\":82043},{\"end\":82067,\"start\":82053},{\"end\":82078,\"start\":82067}]", "bib_venue": "[{\"end\":43614,\"start\":43547},{\"end\":44083,\"start\":44016},{\"end\":44552,\"start\":44514},{\"end\":44933,\"start\":44849},{\"end\":45523,\"start\":45418},{\"end\":46154,\"start\":46041},{\"end\":46722,\"start\":46640},{\"end\":47258,\"start\":47220},{\"end\":47589,\"start\":47510},{\"end\":48056,\"start\":47967},{\"end\":48530,\"start\":48489},{\"end\":48789,\"start\":48769},{\"end\":49322,\"start\":49240},{\"end\":49704,\"start\":49661},{\"end\":49982,\"start\":49938},{\"end\":50363,\"start\":50281},{\"end\":50877,\"start\":50791},{\"end\":51542,\"start\":51491},{\"end\":51967,\"start\":51921},{\"end\":52279,\"start\":52246},{\"end\":52625,\"start\":52600},{\"end\":52979,\"start\":52896},{\"end\":53432,\"start\":53380},{\"end\":54034,\"start\":53955},{\"end\":54552,\"start\":54528},{\"end\":54859,\"start\":54819},{\"end\":55224,\"start\":55210},{\"end\":55551,\"start\":55440},{\"end\":56003,\"start\":55979},{\"end\":56352,\"start\":56330},{\"end\":56652,\"start\":56583},{\"end\":56948,\"start\":56924},{\"end\":57275,\"start\":57209},{\"end\":57801,\"start\":57719},{\"end\":58255,\"start\":58235},{\"end\":58728,\"start\":58644},{\"end\":59123,\"start\":59085},{\"end\":59476,\"start\":59400},{\"end\":59975,\"start\":59909},{\"end\":60478,\"start\":60401},{\"end\":60903,\"start\":60831},{\"end\":61082,\"start\":61025},{\"end\":61446,\"start\":61379},{\"end\":61945,\"start\":61857},{\"end\":62450,\"start\":62384},{\"end\":62774,\"start\":62738},{\"end\":63321,\"start\":63244},{\"end\":63830,\"start\":63742},{\"end\":64290,\"start\":64226},{\"end\":64603,\"start\":64580},{\"end\":65007,\"start\":64952},{\"end\":65330,\"start\":65253},{\"end\":65734,\"start\":65698},{\"end\":66222,\"start\":66145},{\"end\":66774,\"start\":66685},{\"end\":67236,\"start\":67211},{\"end\":67533,\"start\":67457},{\"end\":68016,\"start\":67936},{\"end\":68364,\"start\":68336},{\"end\":68819,\"start\":68718},{\"end\":69117,\"start\":69113},{\"end\":69322,\"start\":69259},{\"end\":69627,\"start\":69566},{\"end\":69896,\"start\":69843},{\"end\":70170,\"start\":70077},{\"end\":70533,\"start\":70447},{\"end\":71128,\"start\":71039},{\"end\":71671,\"start\":71603},{\"end\":72213,\"start\":72130},{\"end\":72424,\"start\":72372},{\"end\":72742,\"start\":72691},{\"end\":73145,\"start\":73069},{\"end\":73674,\"start\":73590},{\"end\":74140,\"start\":74029},{\"end\":74776,\"start\":74705},{\"end\":75186,\"start\":75142},{\"end\":75436,\"start\":75354},{\"end\":75723,\"start\":75638},{\"end\":76016,\"start\":75932},{\"end\":76474,\"start\":76397},{\"end\":76944,\"start\":76863},{\"end\":77233,\"start\":77161},{\"end\":77537,\"start\":77499},{\"end\":77896,\"start\":77829},{\"end\":78292,\"start\":78206},{\"end\":78762,\"start\":78673},{\"end\":79268,\"start\":79257},{\"end\":79671,\"start\":79594},{\"end\":80153,\"start\":80139},{\"end\":80568,\"start\":80487},{\"end\":81036,\"start\":80925},{\"end\":81435,\"start\":81341},{\"end\":81870,\"start\":81844},{\"end\":82133,\"start\":82103},{\"end\":43681,\"start\":43616},{\"end\":44150,\"start\":44085},{\"end\":45022,\"start\":44935},{\"end\":45629,\"start\":45525},{\"end\":46266,\"start\":46156},{\"end\":46808,\"start\":46724},{\"end\":47668,\"start\":47591},{\"end\":48150,\"start\":48058},{\"end\":49408,\"start\":49324},{\"end\":50449,\"start\":50365},{\"end\":50973,\"start\":50879},{\"end\":53065,\"start\":52981},{\"end\":54115,\"start\":54036},{\"end\":57328,\"start\":57277},{\"end\":57884,\"start\":57803},{\"end\":58817,\"start\":58730},{\"end\":59142,\"start\":59125},{\"end\":59557,\"start\":59478},{\"end\":60054,\"start\":59977},{\"end\":60559,\"start\":60480},{\"end\":61512,\"start\":61448},{\"end\":62037,\"start\":61947},{\"end\":62515,\"start\":62452},{\"end\":63408,\"start\":63323},{\"end\":63924,\"start\":63832},{\"end\":65413,\"start\":65332},{\"end\":66305,\"start\":66224},{\"end\":66871,\"start\":66776},{\"end\":67621,\"start\":67535},{\"end\":68099,\"start\":68018},{\"end\":70629,\"start\":70535},{\"end\":71225,\"start\":71130},{\"end\":73225,\"start\":73147},{\"end\":73764,\"start\":73676},{\"end\":74846,\"start\":74778},{\"end\":76106,\"start\":76018},{\"end\":76557,\"start\":76476},{\"end\":77962,\"start\":77898},{\"end\":78861,\"start\":78764},{\"end\":79751,\"start\":79673},{\"end\":80647,\"start\":80570}]"}}}, "year": 2023, "month": 12, "day": 17}