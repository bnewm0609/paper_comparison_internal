{"id": 206853077, "updated": "2023-09-28 18:09:33.362", "metadata": {"title": "UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning", "authors": "[{\"first\":\"Ruihao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Sen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhiqiang\",\"last\":\"Long\",\"middle\":[]},{\"first\":\"Dongbing\",\"last\":\"Gu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 9, "day": 20}, "abstract": "We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO outperforms other monocular VO methods in terms of pose accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1709.06841", "mag": "2964314455", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/LiWLG18", "doi": "10.1109/icra.2018.8461251"}}, "content": {"source": {"pdf_hash": "fd549807a9109a3a91982630ac3f99e20feb00df", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1709.06841v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1709.06841", "status": "GREEN"}}, "grobid": {"id": "7a15070c1df216c7f97abc3dd1c5a54107c059c6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fd549807a9109a3a91982630ac3f99e20feb00df.txt", "contents": "\nUnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning\n\n\nRuihao Li \nSen Wang \nZhiqiang Long \nDongbing Gu \nUnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning\n\nWe propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown inFig. 1. The experiments on KITTI dataset show our UnDeepVO outperforms other monocular VO methods in terms of pose accuracy.\n\nI. INTRODUCTION\n\nVisual odometry (VO) enables a robot to localize itself in various environments by only using low-cost cameras. In the past few decades, model-based VO or geometric VO has been widely studied and its two paradigms, feature-based method [1]- [3] and direct method [4]- [6], have both achieved great success. However, model-based methods tend to be sensitive to camera parameters and fragile in challenging settings, e.g., featureless places, motion blurs and lighting changes.\n\nIn recent years, data-driven VO or deep learning based VO has drawn significant attention due to its potentials in learning capability and the robustness to camera parameters and challenging environments. Starting from the relocalization problem with the use of supervised learning, Kendall et al. [7] first proposed to use a Convolutional Neural Network (CNN) for 6-DoF pose regression with raw RGB images as its inputs. Li et al. [8] then extended this into a new architecture for raw RGB-D images with the advantage of facing the challenging indoor environments. Video clips were employed in [9] to capture the temporal dynamics for relocalization. Given pre-processed optical flow, a CNN based frame-to-frame VO system was reported in [10]. Wang et al. [11] then presented a Recurrent Convolutional Neural Network (RCNN) based VO method resulting in a competitive performance against model-based VO methods. Ummenhofer [12] proposed \"DeMoN\" which can simultaneously estimate the camera's ego-motion, image depth, surface normal and optical flow. Visual inertial odometry with deep learning was also developed in [13] and [14]. 1  However, all the above mentioned methods require the ground truth of camera poses or depth images for conducting the supervised training. Currently obtaining ground truth datasets in practice is typically difficult and expensive, and the amount of existing labeled datasets for supervised training is still limited. These limitations suggest us to look for various unsupervised learning VO schemes, and consequently we can train them with easily collected unlabeled datasets and apply them to localization scenarios.\n\nVO related unsupervised deep learning research mainly focuses on depth estimation, inspired by the image wrap technique \"spatial transformer\" [15]. Built upon it, Garg et al. [16] proposed a novel unsupervised depth estimation method by using the left-right photometric constraint of stereo image pairs. This method was further improved in [17] by wrapping the left and right images across each other. In this way, the accuracy of depth prediction was improved by penalizing both left and right photometric losses. Instead of using stereo image pairs, Zhou et al. [18] proposed to use consecutive monocular images to train and estimate both ego-motion and depth, but the system cannot recover the scale from learning monocular images. Nevertheless, these unsupervised learning schemes have brought deep learning technologies and VO methods closer and showed great potential in many applications.\n\nIn this paper, we propose UnDeepVO, a novel monocular VO system based on unsupervised deep learning scheme (see Fig. 1). Our main contributions are as follows:\n\n\u2022 We demonstrate a monocular VO system with recovered absolute scale, and we achieve this in an unsupervised manner by harnessing both spatial and temporal geometric constraints. \u2022 Not only estimated pose but also estimated dense depth map are generated with absolute scales thanks to the use of stereo image pairs during training. \u2022 We evaluate our VO system using KITTI dataset, and the results show UnDeepVO achieves the state-of-theart performance for monocular cameras. Since UnDeepVO only requires stereo imagery for training without the need of labeled datasets, it is possible to train it with an extremely large number of unlabeled datasets to continuously improve its performance.\n\nThe rest of this paper is organized as follows. Section II introduces the architecture of our proposed system. Section III describes different types of losses used to facilitate the unsupervised training of our system. Section IV presents experimental results. Finally, conclusion is drawn in Section V.\n\n\nII. SYSTEM OVERVIEW\n\nOur system is composed of a pose estimator and a depth estimator, as shown in Fig. 2. Both estimators take consecutive monocular images as inputs, and produce scaled 6-DoF pose and depth as outputs, respectively.\n\nFor the pose estimator, it is a VGG-based [19] CNN architecture. It takes two consecutive monocular images as input and predicts the 6-DoF transformation between them. Since rotation (represented by Euler angles) has high nonlinearity, it is usually difficult to train compared with translation. For supervised training, a popular solution is to give a bigger weight to the rotational loss [7] as a way of normalization. In order to better train the rotation with unsupervised learning, we decouple the translation and the rotation with two separate groups of fully-connected layers after the last convolutional layer. This enables us to introduce a weight normalizing the rotation and the translation predictions for better performance. The specific architecture of the pose estimator is shown in Fig. 2.\n\nThe depth estimator is mainly based on an encoderdecoder architecture to generate dense depth maps. Different from other depth estimation methods [17], [18] which produce disparity images (inverse of the depth) from the network, the depth estimator of UnDeepVO is designed to directly predict depth maps. This is because training trails report that the whole system is easier to converge when training in this way.\n\nFor most monocular VO methods, a predefined scale has to be applied. One feature of our UnDeepVO is to recover absolute scale of 6-DoF pose and depth, it is credited to\n\n\nPose Estimator Depth Estimator\n\nTwo consecutive monocular images  our training scheme shown in Fig. 3. During training, we feed both left images and right images into the networks, and get 6-DoF poses and depths of left sequences and right sequences, respectively. We then use the input stereo images, estimated depth images and 6-DoF poses to construct the loss function considering the geometry of a stereo rig.\n\nAs shown in Fig. 3, we utilize both spatial and temporal geometric consistencies of a stereo image sequence to formulate the loss function. The red points in one image all have the corresponding ones in another. Spatial geometric consistency represents the geometric projective constraint between the corresponding points in left-right image pairs, while temporal geometric consistency represents the geometric projective constraint between the corresponding points in two consecutive monocular images (more details in section III). By using these constraints to construct the loss function and minimizing them all together, the UnDeepVO learns to estimate scaled 6-DoF poses and depth maps in an end-toend unsupervised manner.\n\n\nIII. OBJECTIVE LOSSES FOR UNSUPERVISED TRAINING\n\nUnDeepVO is trained with losses through backpropagation. Since the losses are built on geometric constraints rather than labeled data, UnDeepVO is trained in an unsupervised manner. Its total loss includes spatial image losses and  temporal image losses, as shown in Fig. 3. The spatial image losses drive the network to recover scaled depth maps by using stereo image pairs, while the temporal image losses are designed to minimize the errors on camera motion by using two consecutive monocular images.\nBf /D dep Bf /D dep Bf /D dep T k,k+1 T 1 k,k+1\n\nA. Spatial Image Losses of a Stereo Image Pair\n\nThe spatial losses are constructed from the geometric constraints between left and right stereo images. It is composed of left-right photometric consistency loss, disparity consistency loss and pose consistency loss. UnDeepVO relies on these losses to recover the absolute scale for the monocular VO.\n\n1) Photometric Consistency Loss: The left-right projective photometric error is used as photometric consistency loss to train the network. Specifically, for the overlapped area between two stereo images, every pixel in one image can find its correspondence in the other with horizontal distance D p [16]. Assume p l (u l , v l ) is a pixel in the left image and p r (u r , v r ) is its corresponding pixel in the right image. Then, we have the spatial constraint u l = u r and v l = v r + D p . The distance D p can be calculated by\nD p = B f /D dep (1)\nwhere B is the baseline of the stereo camera, f is the focal length and D dep is the depth value of the corresponding pixel. We can construct a D p map with the depths estimated from the network to define the constraints between the left and right images. With this spatial constraint and the calculated D p map, we could synthesize one image from the other through \"spatial transformer\" [15]. The combination of an L1 norm and an SSIM term [20] is used to construct the left-right photometric consistency loss:\nL l pho = \u03bb s L SSIM (I l , I l ) + (1 \u2212 \u03bb s )L l 1 (I l , I l )(2)L r pho = \u03bb s L SSIM (I r , I r ) + (1 \u2212 \u03bb s )L l 1 (I r , I r )(3)\nwhere I l , I r are the original left and right images respectively, I l is the synthesized left image from the original right image I r , and I r is the synthesized right image from the original left image I l , L SSIM is the operation defined in [21] with a weight \u03bb s , and L l 1 is the L1 norm operation.\n\n2) Disparity Consistency Loss: Similarly, the left and right disparity maps (inverse of depth) are also constrained by D p . The disparity map UnDeepVO used is\nD dis = D p \u00d7 I W(4)\nwhere I W is the width of original image size. Denote the left and right disparity maps as D l dis and D r dis , respectively. We can use D p to synthesize D l dis , D r dis from D r dis , D l dis . Then, the disparity consistency losses are\nL l dis = L l 1 (D l dis , D l dis ) (5) L r dis = L l 1 (D r dis , D r dis )(6)\n3) Pose Consistency Loss: We use both left and right image sequences to predict the 6-DoF transformation of the camera separately during training. Ideally, these two predicted transformations should be identical. Therefore, we can penalize the difference between them by\nL pos = \u03bb p L l 1 (x l , x r ) + \u03bb o L l 1 (\u03d5 l , \u03d5 r )(7)\nwhere \u03bb p is the left-right position consistency weight, \u03bb o is the left-right orientation consistency weight, and [x l , \u03d5 l ] and [x r , \u03d5 r ] are the predicted poses from the left and right image sequences, respectively.\n\n\nB. Temporal Image Losses of Consecutive Monocular Images\n\nTemporal loss is defined according to the geometric constraints between two consecutive monocular images. It is an essential part to recover the 6-DoF motion of camera. It comprises photometric consistency loss and 3D geometric registration loss.\n\n\n1) Photometric Consistency Loss:\n\nThe photometric loss is computed from two consecutive monocular images. Similar to DTAM [4], in order to estimate 6-DoF transformation, the projective photometric error is employed as the loss to minimize. Denote I k , I k+1 as the kth and (k + 1)th image frame, respectively, and p k (u k , v k ) as one pixel in I k , and p k+1 (u k+1 , v k+1 ) as the corresponding pixel in I k+1 . Then, we can derive p k+1 from p k through\np k+1 = KT k,k+1 D dep K \u22121 p k(8)\nwhere K is the camera intrinsics matrix, D dep is the depth value of the pixel in the kth frame, T k,k+1 is the camera coordinate transformation matrix from the kth frame to the (k + 1)th frame. We can synthesize I k and I k+1 from I k+1 and I k by using estimated poses and \"spatial transformer\" [15]. Therefore, the photometric losses between the monocular image sequence are\nL k pho = \u03bb s L SSIM (I k , I k ) + (1 \u2212 \u03bb s )L l 1 (I k , I k )(9)\nL k+1 pho = \u03bb s L SSIM (I k+1 , I k+1 ) + (1 \u2212 \u03bb s )L l 1 (I k+1 , I k+1 ) (10) 2) 3D Geometric Registration Loss: 3D geometric registration loss is to estimate the transformation by aligning two point clouds, similar to the Iterative Closest Point (ICP) technique. Assume P k (x, y, z) is a point in the kth camera coordination. It can then be transformed to the (k + 1)th camera coordination as P k (x, y, z) by using T k,k+1 . Similarly, points in the (k + 1)th frame can be transformed to kth frame. Then, the 3D geometric registration losses between two monocular images are\nL k geo = L l 1 (P k , P k )(11)\nL k+1 geo = L l 1 (P k+1 , P k+1 )\n\nIn summary, the final loss function of our system combines the previous spatial and temporal losses together. The leftright photometric consistency loss has been used in [16] and [17] to estimate depth map. [18] introduced the photometric loss of a monocular image sequence for ego-motion and depth estimation. However, to the best of our knowledge, UnDeepVO is the first to recover both scaled camera poses and depth maps by benefiting all these losses together with the 3D geometric registration and pose consistency losses.\n\n\nIV. EXPERIMENTAL EVALUATION\n\nIn this section, we evaluated the proposed UnDeepVO system. 1 The network models were implemented with the TensorFlow framework and trained with NVIDIA Tesla P100 GPUs. For testing, we used a laptop equipped with NVIDIA GeForce GTX 980M and Intel Core i7 2.7GHz CPU. The GPU memory needed for pose estimation is less than 400MB with 40Hz real-time performance.\n\nAdam optimizer was employed to train the network for up to 20-30 epochs with parameter \u03b2 1 = 0.9 and \u03b2 2 = 0.99. The learning rate started from 0.001 and decreased by half for every 1/5 of total iterations. The sequence length of images 1 Video: https://www.youtube.com/watch?v=5RdjO93wJqo feeding to the pose estimator was 2. The size of image input to the networks was 416 \u00d7 128. We also resized the output images to a higher resolution to compute the losses and fine-tuned the networks in the end. Different kinds of data augmentation methods were used to enhance the performance and mitigate possible overfitting, such as image color augmentation, rotational data augmentation and left-right pose estimation augmentation. Specifically, we randomly selected 20% images for color augmentation with random brightness in range [0.9, 1.1], random gamma in range [0.9, 1.1] and random color shifts in range [0.9, 1.1]. For rotational data augmentation, we increased the proportion of rotational data to achieve better performance in rotation estimation. Pose estimation consistency of left-right images was also used for left-right pose estimation augmentation.\n\n\nA. Trajectory Evaluation\n\nWe adopted the popular KITTI Odometry Dataset [22] to evaluate the proposed UnDeepVO system, and compared the results with SfMLearner [18], monocular VISO2-M and ORB-SLAM-M (without loop closure). In order to implement fair qualitative and quantitative comparison, we used the same training data as in SfMLearner [18]. The trajectories produced by different methods are shown in Fig. 4. Note that all the methods took monocular images for testing, and we have to post-process the scales for SfMLearner and ORB-SLAM-M as they cannot recover the scale of pose Seq.\n\n\nUnDeepVO\n\nSfMLearner [18] VISO2   Table I for quantitative evaluation. We use the standard evaluation method provided along with KITTI dataset. Average translational root-mean-square error (RMSE) drift (%) and average rotational RMSE drift ( \u2022 /100m) on length of 100m-800m are adopted. Since SfMLearner and ORB-SLAM-M cannot recover the scale of 6-DoF poses, we aligned their poses to the ground-truth with 6-DoF and scale (7-DoF). For monocular VISO2-M and ORB-SLAM without loop closure, they can not work with our input settings (image resolution 416 \u00d7 128), so we provide the results of both system with high resolution 1242 \u00d7 376. All the methods here did not use any loop closure technology. As shown in Table I, our method achieves the best pose estimation performance among the monocular methods even with low resolution images and without the scale post-processing.\n-M ORB-SLAM-M (416\u00d7128) (416\u00d7128) (1242\u00d7376) (1242\u00d7376) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 )00\n\nB. Depth Estimation Evaluation\n\nOur system can also produce the scaled depth map by using the depth estimator. Fig. 6 shows some raw RGB images and their corresponding depth images estimated from our system. As shown in Fig. 6, the different depths of cars and trees are explicitly estimated, even the depth of trunks is predicted successfully. The detailed depth estimation results are listed in Table II. As shown in the table, our method outperforms the supervised one [23] and the unsupervised one without scale [18], but performs not as good as [17]. This could be caused by a few reasons. First, we only used parts of KITTI dataset (KITTI odometry dataset) for training while all other methods use full KITTI dataset to train their networks. Second, [17] used higher resolution (512 \u00d7 256) input and a different net (ResNet-based architecture). Third, the temporal image sequence loss we used could introduce  some noise (such as moving objects) for depth estimation.\n\n\nV. CONCLUSIONS\n\nIn this paper, we presented UnDeepVO, a novel monocular VO system with unsupervised deep learning. The system makes use of spatial losses and temporal losses between stereo image sequences for unsupervised training. During testing, the proposed system can perform the pose estimation and dense depth map estimation with monocular images. It outperforms other monocular VO methods in pose estimation. Our system recovers the scale during the training stage, which distincts itself from other model based or learning based monocular VO methods. In general, unsupervised learning based VO methods have the potential to improve their performance with the increasing size of training datasets. In the next step, we will investigate how to train the UnDeepVO with large amount of datasets to improve its performance, such as robustness to image blurs, camera parameters, or illumination changes. In the future, we also plan to extend our system to a visual SLAM system to reduce the drift. Developing an unsupervised DeepVO system with stereo cameras or RGB-D cameras is also in consideration.\n\nFig. 2 :\n2Architecture of our UnDeepVO.\n\nFig. 3 :\n3Training scheme of UnDeepVO. The pose and depth estimators take stereo images as inputs to estimate 6-DoF poses and depth maps, respectively. The total loss including spatial losses and temporal losses can then be calculated based on raw RGB images, estimated depth maps and poses.\n\nFig. 4 :\n4Trajectories of Sequence 02, 05, 07 and 08. Results of SfMLearner[18] are post-processed with 7-DoF alignment to ground truth since it cannot recover the scale. UnDeepVO and SfMLearner use images with size 416\u00d7128. Images used by VISO2-M are 1242\u00d7376.\n\nFig. 5 :\n5Trajectories of KITTI dataset with our UnDeepVO. No ground truth of poses is available for these sequences. Trajectories with both monocular VISO2-M and stereo VISO2-S are plotted. Our UnDeepVO works well on these sequences and is comparable to VISO2-S. and depth. VISO2-M employed the fixed camera height for scale recovery. For ORB-SLAM-M, we disabled the local mapping and loop closure in order to perform VO only for comparison. The KITTI Odometry Dataset only provides the ground-truth of 6-DoF poses for Sequence 00-10. As shown in Fig. 4, the trajectories of UnDeepVO are qualitatively closest to the ground truth among all the methods. For sequences 11-21, there is no ground-truth available, and the trajectories of our method and VISO2-M are given in Fig. 5. The results of stereo VISO2-S (image resolution 1242 \u00d7 376) are provided for reference. As shown in the figure, our system's performance is comparable to that of VISO2-S. The detailed VO results are listed in\n\nFig. 6 :\n6Depth images produced by our depth estimator. The left column are raw RGB images, and the right column are the corresponding depth images estimated.\n\n\nRuihao Li, Dongbing Gu are with School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, UK. {rlig, dgu}@essex.ac.uk 2 Sen Wang is with Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, EH14 4AS, UK. s.wang@hw.ac.uk 3 Zhiqiang Long is with College of Mechatronics and Automation, National University of Defense Technology, Changsha, China.Stereo Images (Training Dataset) \n\nLeft \nRight \n\nMonocular Images \n(Testing Data) \n\nPose \n\nDepth Maps \n\nTraining \nBackpropagation \n\nTesting \n\nFig. 1: System overview of the proposed UnDeepVO. After \ntraining with unlabeled stereo images, UnDeepVO can si-\nmultaneously perform visual odometry and depth estimation \nwith monocular images. The estimated 6-DoF poses and \ndepth maps are both scaled without the need for scale post-\nprocessing. \n\n\n\nTABLE I :\nIVO results with our proposed UnDeepVO. All the methods listed in the table did not use loop closure. Note that monocular VISO2-M and ORB-SLAM-M (without loop closure) did not work with image size 416 \u00d7 128, the results were obtained with image size 1242\u00d7376. 7-DoF (6-DoF + scale) alignment with the ground-truth is applied for SfMLearner and ORB-SLAM-M.\n\nTABLE II :\nIIDepth estimation results on KITTI using the split of Eigen et al.[23].Methods \nDataset Scale \nError metric \nAbs Rel Sq Rel RMSE RMSE log \nEigen [23] \nK (raw) \n0.214 \n1.605 6.563 \n0.292 \nMonoDepth [17] K (raw) \n0.148 \n1.344 5.927 \n0.247 \nSfMLearner [18] K (raw) \n\u00d7 \n0.208 \n1.768 6.856 \n0.283 \nUnDeepVO \nK (odo) \n0.183 \n1.73 \n6.57 \n0.268 \n\n\nACKNOWLEDGMENTThe authors would like to thank Robin Dowling for his support in experiments. The first author has been financially supported by scholarship from China Scholarship Council.\nMonoSLAM: Real-time single camera SLAM. A J Davison, I D Reid, N D Molton, O Stasse, IEEE Transactions on Pattern Analysis and Machine Intelligence. 296A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, \"MonoSLAM: Real-time single camera SLAM,\" IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, vol. 29, no. 6, pp. 1052-1067, 2007.\n\nParallel tracking and mapping for small AR workspaces. G Klein, D Murray, 6th IEEE and ACM International Symposium on. IEEEMixed and Augmented RealityG. Klein and D. Murray, \"Parallel tracking and mapping for small AR workspaces,\" in Mixed and Augmented Reality, 2007. ISMAR 2007. 6th IEEE and ACM International Symposium on. IEEE, 2007, pp. 225-234.\n\nORB-SLAM: a versatile and accurate monocular SLAM system. R Mur-Artal, J Montiel, J D Tardos, IEEE Transactions on Robotics. 315R. Mur-Artal, J. Montiel, and J. D. Tardos, \"ORB-SLAM: a versa- tile and accurate monocular SLAM system,\" IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015.\n\nDTAM: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)IEEER. A. Newcombe, S. J. Lovegrove, and A. J. Davison, \"DTAM: Dense tracking and mapping in real-time,\" in Proceedings of the IEEE International Conference on Computer Vision (ICCV). IEEE, 2011, pp. 2320-2327.\n\nLSD-SLAM: Large-scale direct monocular SLAM. J Engel, T Sch\u00f6ps, D Cremers, European Conference on Computer Vision (ECCV. SpringerJ. Engel, T. Sch\u00f6ps, and D. Cremers, \"LSD-SLAM: Large-scale direct monocular SLAM,\" in European Conference on Computer Vision (ECCV). Springer, 2014, pp. 834-849.\n\nDirect sparse odometry. J Engel, V Koltun, D Cremers, IEEE Transactions on Pattern Analysis and Machine Intelligence. J. Engel, V. Koltun, and D. Cremers, \"Direct sparse odometry,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nPoseNet: A convolutional network for real-time 6-DOF camera relocalization. A Kendall, M Grimes, R Cipolla, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)A. Kendall, M. Grimes, and R. Cipolla, \"PoseNet: A convolutional network for real-time 6-DOF camera relocalization,\" in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2938-2946.\n\nIndoor relocalization in challenging environments with dual-stream convolutional neural networks. R Li, Q Liu, J Gui, D Gu, H Hu, IEEE Transactions on Automation Science and Engineering. R. Li, Q. Liu, J. Gui, D. Gu, and H. Hu, \"Indoor relocalization in challenging environments with dual-stream convolutional neural net- works,\" IEEE Transactions on Automation Science and Engineering, 2017.\n\nVidLoc: 6-DoF video-clip relocalization. R Clark, S Wang, A Markham, N Trigoni, H Wen, Conference on Computer Vision and Pattern Recognition (CVPR. R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, \"VidLoc: 6-DoF video-clip relocalization,\" in Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nExploring representation learning with CNNs for frame-to-frame ego-motion estimation. G Costante, M Mancini, P Valigi, T A Ciarfuglia, IEEE robotics and automation letters. 11G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia, \"Exploring representation learning with CNNs for frame-to-frame ego-motion estimation,\" IEEE robotics and automation letters, vol. 1, no. 1, pp. 18-25, 2016.\n\nDeepVO: Towards endto-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, 2017 IEEE International Conference on. Robotics and Automation (ICRAS. Wang, R. Clark, H. Wen, and N. Trigoni, \"DeepVO: Towards end- to-end visual odometry with deep recurrent convolutional neural net- works,\" in Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 2017, pp. 2043-2050.\n\nDeMoN: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, Conference on Computer Vision and Pattern Recognition (CVPR. B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox, \"DeMoN: Depth and motion network for learning monocular stereo,\" in Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nVINet: Visual-Inertial odometry as a sequence-to-sequence learning problem. R Clark, S Wang, H Wen, A Markham, N Trigoni, AAAI. R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, \"VINet: Visual-Inertial odometry as a sequence-to-sequence learning problem.\" in AAAI, 2017, pp. 3995-4001.\n\nTowards visual ego-motion learning in robots. S Pillai, J J Leonard, arXiv:1705.10279arXiv preprintS. Pillai and J. J. Leonard, \"Towards visual ego-motion learning in robots,\" arXiv preprint arXiv:1705.10279, 2017.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Advances in Neural Information Processing Systems. M. Jaderberg, K. Simonyan, A. Zisserman, et al., \"Spatial transformer networks,\" in Advances in Neural Information Processing Systems, 2015, pp. 2017-2025.\n\nUnsupervised CNN for single view depth estimation: Geometry to the rescue. R Garg, G Carneiro, I Reid, European Conference on Computer Vision (ECCV). SpringerR. Garg, G. Carneiro, and I. Reid, \"Unsupervised CNN for single view depth estimation: Geometry to the rescue,\" in European Conference on Computer Vision (ECCV). Springer, 2016, pp. 740-756.\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, Conference on Computer Vision and Pattern Recognition. C. Godard, O. Mac Aodha, and G. J. Brostow, \"Unsupervised monoc- ular depth estimation with left-right consistency,\" in Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, Conference on Computer Vision and Pattern Recognition (CVPR. T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, \"Unsupervised learning of depth and ego-motion from video,\" in Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations (ICLR). K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" in International Conference on Learning Representations (ICLR), 2015.\n\nIs L2 a good loss function for neural networks for image processing. H Zhao, O Gallo, I Frosio, J Kautz, 1511ArXiv e-printsH. Zhao, O. Gallo, I. Frosio, and J. Kautz, \"Is L2 a good loss function for neural networks for image processing?\" ArXiv e-prints, vol. 1511, 2015.\n\nImage quality assessment: From error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Transactions on Image Processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity,\" IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, 2004.\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). A. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? The KITTI vision benchmark suite,\" in Conference on Com- puter Vision and Pattern Recognition (CVPR), 2012.\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. D. Eigen, C. Puhrsch, and R. Fergus, \"Depth map prediction from a single image using a multi-scale deep network,\" in Advances in neural information processing systems, 2014, pp. 2366-2374.\n", "annotations": {"author": "[{\"end\":84,\"start\":74},{\"end\":94,\"start\":85},{\"end\":109,\"start\":95},{\"end\":122,\"start\":110}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":81},{\"end\":93,\"start\":89},{\"end\":108,\"start\":104},{\"end\":121,\"start\":119}]", "author_first_name": "[{\"end\":80,\"start\":74},{\"end\":88,\"start\":85},{\"end\":103,\"start\":95},{\"end\":118,\"start\":110}]", "author_affiliation": null, "title": "[{\"end\":71,\"start\":1},{\"end\":193,\"start\":123}]", "venue": null, "abstract": "[{\"end\":982,\"start\":195}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1240,\"start\":1237},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1245,\"start\":1242},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1267,\"start\":1264},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1272,\"start\":1269},{\"end\":1771,\"start\":1761},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1779,\"start\":1776},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1913,\"start\":1910},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2076,\"start\":2073},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2221,\"start\":2217},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2239,\"start\":2235},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2405,\"start\":2401},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2598,\"start\":2594},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2607,\"start\":2603},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2610,\"start\":2609},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3276,\"start\":3272},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3309,\"start\":3305},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3474,\"start\":3470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3698,\"start\":3694},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5467,\"start\":5463},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5814,\"start\":5811},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6378,\"start\":6374},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6384,\"start\":6380},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9858,\"start\":9854},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9911,\"start\":9907},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10365,\"start\":10361},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11915,\"start\":11912},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12588,\"start\":12584},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13556,\"start\":13552},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13565,\"start\":13561},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13593,\"start\":13589},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14001,\"start\":14000},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15540,\"start\":15536},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15628,\"start\":15624},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15807,\"start\":15803},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16080,\"start\":16076},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17553,\"start\":17549},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17597,\"start\":17593},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17631,\"start\":17627},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17837,\"start\":17833},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19571,\"start\":19567},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22193,\"start\":22189}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19197,\"start\":19157},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19490,\"start\":19198},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19753,\"start\":19491},{\"attributes\":{\"id\":\"fig_4\"},\"end\":20742,\"start\":19754},{\"attributes\":{\"id\":\"fig_5\"},\"end\":20902,\"start\":20743},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":21742,\"start\":20903},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22109,\"start\":21743},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":22462,\"start\":22110}]", "paragraph": "[{\"end\":1476,\"start\":1001},{\"end\":3128,\"start\":1478},{\"end\":4025,\"start\":3130},{\"end\":4186,\"start\":4027},{\"end\":4878,\"start\":4188},{\"end\":5183,\"start\":4880},{\"end\":5419,\"start\":5207},{\"end\":6226,\"start\":5421},{\"end\":6642,\"start\":6228},{\"end\":6812,\"start\":6644},{\"end\":7228,\"start\":6847},{\"end\":7957,\"start\":7230},{\"end\":8512,\"start\":8009},{\"end\":8910,\"start\":8610},{\"end\":9444,\"start\":8912},{\"end\":9977,\"start\":9466},{\"end\":10421,\"start\":10113},{\"end\":10582,\"start\":10423},{\"end\":10845,\"start\":10604},{\"end\":11197,\"start\":10927},{\"end\":11480,\"start\":11257},{\"end\":11787,\"start\":11541},{\"end\":12251,\"start\":11824},{\"end\":12664,\"start\":12287},{\"end\":13312,\"start\":12733},{\"end\":13380,\"start\":13346},{\"end\":13908,\"start\":13382},{\"end\":14300,\"start\":13940},{\"end\":15461,\"start\":14302},{\"end\":16052,\"start\":15490},{\"end\":16929,\"start\":16065},{\"end\":18050,\"start\":17109},{\"end\":19156,\"start\":18069}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8560,\"start\":8513},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9465,\"start\":9445},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10045,\"start\":9978},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10112,\"start\":10045},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10603,\"start\":10583},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10926,\"start\":10846},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11256,\"start\":11198},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12286,\"start\":12252},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12732,\"start\":12665},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13345,\"start\":13313},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17075,\"start\":16930}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16096,\"start\":16089},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16772,\"start\":16765},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17482,\"start\":17474}]", "section_header": "[{\"end\":999,\"start\":984},{\"end\":5205,\"start\":5186},{\"end\":6845,\"start\":6815},{\"end\":8007,\"start\":7960},{\"end\":8608,\"start\":8562},{\"end\":11539,\"start\":11483},{\"end\":11822,\"start\":11790},{\"end\":13938,\"start\":13911},{\"end\":15488,\"start\":15464},{\"end\":16063,\"start\":16055},{\"end\":17107,\"start\":17077},{\"end\":18067,\"start\":18053},{\"end\":19166,\"start\":19158},{\"end\":19207,\"start\":19199},{\"end\":19500,\"start\":19492},{\"end\":19763,\"start\":19755},{\"end\":20752,\"start\":20744},{\"end\":21753,\"start\":21744},{\"end\":22121,\"start\":22111}]", "table": "[{\"end\":21742,\"start\":21300},{\"end\":22462,\"start\":22194}]", "figure_caption": "[{\"end\":19197,\"start\":19168},{\"end\":19490,\"start\":19209},{\"end\":19753,\"start\":19502},{\"end\":20742,\"start\":19765},{\"end\":20902,\"start\":20754},{\"end\":21300,\"start\":20905},{\"end\":22109,\"start\":21755},{\"end\":22194,\"start\":22124}]", "figure_ref": "[{\"end\":4145,\"start\":4139},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5291,\"start\":5285},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6225,\"start\":6219},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6916,\"start\":6910},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7248,\"start\":7242},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8282,\"start\":8276},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15875,\"start\":15869},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17194,\"start\":17188},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17303,\"start\":17297}]", "bib_author_first_name": "[{\"end\":22691,\"start\":22690},{\"end\":22693,\"start\":22692},{\"end\":22704,\"start\":22703},{\"end\":22706,\"start\":22705},{\"end\":22714,\"start\":22713},{\"end\":22716,\"start\":22715},{\"end\":22726,\"start\":22725},{\"end\":23060,\"start\":23059},{\"end\":23069,\"start\":23068},{\"end\":23415,\"start\":23414},{\"end\":23428,\"start\":23427},{\"end\":23439,\"start\":23438},{\"end\":23441,\"start\":23440},{\"end\":23707,\"start\":23706},{\"end\":23709,\"start\":23708},{\"end\":23721,\"start\":23720},{\"end\":23723,\"start\":23722},{\"end\":23736,\"start\":23735},{\"end\":23738,\"start\":23737},{\"end\":24141,\"start\":24140},{\"end\":24150,\"start\":24149},{\"end\":24160,\"start\":24159},{\"end\":24413,\"start\":24412},{\"end\":24422,\"start\":24421},{\"end\":24432,\"start\":24431},{\"end\":24717,\"start\":24716},{\"end\":24728,\"start\":24727},{\"end\":24738,\"start\":24737},{\"end\":25200,\"start\":25199},{\"end\":25206,\"start\":25205},{\"end\":25213,\"start\":25212},{\"end\":25220,\"start\":25219},{\"end\":25226,\"start\":25225},{\"end\":25537,\"start\":25536},{\"end\":25546,\"start\":25545},{\"end\":25554,\"start\":25553},{\"end\":25565,\"start\":25564},{\"end\":25576,\"start\":25575},{\"end\":25900,\"start\":25899},{\"end\":25912,\"start\":25911},{\"end\":25923,\"start\":25922},{\"end\":25933,\"start\":25932},{\"end\":25935,\"start\":25934},{\"end\":26299,\"start\":26298},{\"end\":26307,\"start\":26306},{\"end\":26316,\"start\":26315},{\"end\":26323,\"start\":26322},{\"end\":26709,\"start\":26708},{\"end\":26723,\"start\":26722},{\"end\":26731,\"start\":26730},{\"end\":26740,\"start\":26739},{\"end\":26749,\"start\":26748},{\"end\":26756,\"start\":26755},{\"end\":26771,\"start\":26770},{\"end\":27134,\"start\":27133},{\"end\":27143,\"start\":27142},{\"end\":27151,\"start\":27150},{\"end\":27158,\"start\":27157},{\"end\":27169,\"start\":27168},{\"end\":27396,\"start\":27395},{\"end\":27406,\"start\":27405},{\"end\":27408,\"start\":27407},{\"end\":27596,\"start\":27595},{\"end\":27609,\"start\":27608},{\"end\":27621,\"start\":27620},{\"end\":27917,\"start\":27916},{\"end\":27925,\"start\":27924},{\"end\":27937,\"start\":27936},{\"end\":28261,\"start\":28260},{\"end\":28271,\"start\":28270},{\"end\":28275,\"start\":28272},{\"end\":28284,\"start\":28283},{\"end\":28286,\"start\":28285},{\"end\":28599,\"start\":28598},{\"end\":28607,\"start\":28606},{\"end\":28616,\"start\":28615},{\"end\":28627,\"start\":28626},{\"end\":28629,\"start\":28628},{\"end\":28945,\"start\":28944},{\"end\":28957,\"start\":28956},{\"end\":29271,\"start\":29270},{\"end\":29279,\"start\":29278},{\"end\":29288,\"start\":29287},{\"end\":29298,\"start\":29297},{\"end\":29548,\"start\":29547},{\"end\":29556,\"start\":29555},{\"end\":29558,\"start\":29557},{\"end\":29567,\"start\":29566},{\"end\":29569,\"start\":29568},{\"end\":29579,\"start\":29578},{\"end\":29581,\"start\":29580},{\"end\":29917,\"start\":29916},{\"end\":29927,\"start\":29926},{\"end\":29935,\"start\":29934},{\"end\":30266,\"start\":30265},{\"end\":30275,\"start\":30274},{\"end\":30286,\"start\":30285}]", "bib_author_last_name": "[{\"end\":22701,\"start\":22694},{\"end\":22711,\"start\":22707},{\"end\":22723,\"start\":22717},{\"end\":22733,\"start\":22727},{\"end\":23066,\"start\":23061},{\"end\":23076,\"start\":23070},{\"end\":23425,\"start\":23416},{\"end\":23436,\"start\":23429},{\"end\":23448,\"start\":23442},{\"end\":23718,\"start\":23710},{\"end\":23733,\"start\":23724},{\"end\":23746,\"start\":23739},{\"end\":24147,\"start\":24142},{\"end\":24157,\"start\":24151},{\"end\":24168,\"start\":24161},{\"end\":24419,\"start\":24414},{\"end\":24429,\"start\":24423},{\"end\":24440,\"start\":24433},{\"end\":24725,\"start\":24718},{\"end\":24735,\"start\":24729},{\"end\":24746,\"start\":24739},{\"end\":25203,\"start\":25201},{\"end\":25210,\"start\":25207},{\"end\":25217,\"start\":25214},{\"end\":25223,\"start\":25221},{\"end\":25229,\"start\":25227},{\"end\":25543,\"start\":25538},{\"end\":25551,\"start\":25547},{\"end\":25562,\"start\":25555},{\"end\":25573,\"start\":25566},{\"end\":25580,\"start\":25577},{\"end\":25909,\"start\":25901},{\"end\":25920,\"start\":25913},{\"end\":25930,\"start\":25924},{\"end\":25946,\"start\":25936},{\"end\":26304,\"start\":26300},{\"end\":26313,\"start\":26308},{\"end\":26320,\"start\":26317},{\"end\":26331,\"start\":26324},{\"end\":26720,\"start\":26710},{\"end\":26728,\"start\":26724},{\"end\":26737,\"start\":26732},{\"end\":26746,\"start\":26741},{\"end\":26753,\"start\":26750},{\"end\":26768,\"start\":26757},{\"end\":26776,\"start\":26772},{\"end\":27140,\"start\":27135},{\"end\":27148,\"start\":27144},{\"end\":27155,\"start\":27152},{\"end\":27166,\"start\":27159},{\"end\":27177,\"start\":27170},{\"end\":27403,\"start\":27397},{\"end\":27416,\"start\":27409},{\"end\":27606,\"start\":27597},{\"end\":27618,\"start\":27610},{\"end\":27631,\"start\":27622},{\"end\":27922,\"start\":27918},{\"end\":27934,\"start\":27926},{\"end\":27942,\"start\":27938},{\"end\":28268,\"start\":28262},{\"end\":28281,\"start\":28276},{\"end\":28294,\"start\":28287},{\"end\":28604,\"start\":28600},{\"end\":28613,\"start\":28608},{\"end\":28624,\"start\":28617},{\"end\":28634,\"start\":28630},{\"end\":28954,\"start\":28946},{\"end\":28967,\"start\":28958},{\"end\":29276,\"start\":29272},{\"end\":29285,\"start\":29280},{\"end\":29295,\"start\":29289},{\"end\":29304,\"start\":29299},{\"end\":29553,\"start\":29549},{\"end\":29564,\"start\":29559},{\"end\":29576,\"start\":29570},{\"end\":29592,\"start\":29582},{\"end\":29924,\"start\":29918},{\"end\":29932,\"start\":29928},{\"end\":29943,\"start\":29936},{\"end\":30272,\"start\":30267},{\"end\":30283,\"start\":30276},{\"end\":30293,\"start\":30287}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206764185},\"end\":23002,\"start\":22650},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206986664},\"end\":23354,\"start\":23004},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206775100},\"end\":23657,\"start\":23356},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1336659},\"end\":24093,\"start\":23659},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14547347},\"end\":24386,\"start\":24095},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3299195},\"end\":24638,\"start\":24388},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12888763},\"end\":25099,\"start\":24640},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4604162},\"end\":25493,\"start\":25101},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18394290},\"end\":25811,\"start\":25495},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7183629},\"end\":26203,\"start\":25813},{\"attributes\":{\"id\":\"b10\"},\"end\":26643,\"start\":26205},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6159584},\"end\":27055,\"start\":26645},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17112447},\"end\":27347,\"start\":27057},{\"attributes\":{\"doi\":\"arXiv:1705.10279\",\"id\":\"b13\"},\"end\":27563,\"start\":27349},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6099034},\"end\":27839,\"start\":27565},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":299085},\"end\":28189,\"start\":27841},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206596513},\"end\":28538,\"start\":28191},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11977588},\"end\":28874,\"start\":28540},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14124313},\"end\":29199,\"start\":28876},{\"attributes\":{\"id\":\"b19\"},\"end\":29471,\"start\":29201},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207761262},\"end\":29843,\"start\":29473},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6724907},\"end\":30188,\"start\":29845},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2255738},\"end\":30534,\"start\":30190}]", "bib_title": "[{\"end\":22688,\"start\":22650},{\"end\":23057,\"start\":23004},{\"end\":23412,\"start\":23356},{\"end\":23704,\"start\":23659},{\"end\":24138,\"start\":24095},{\"end\":24410,\"start\":24388},{\"end\":24714,\"start\":24640},{\"end\":25197,\"start\":25101},{\"end\":25534,\"start\":25495},{\"end\":25897,\"start\":25813},{\"end\":26296,\"start\":26205},{\"end\":26706,\"start\":26645},{\"end\":27131,\"start\":27057},{\"end\":27593,\"start\":27565},{\"end\":27914,\"start\":27841},{\"end\":28258,\"start\":28191},{\"end\":28596,\"start\":28540},{\"end\":28942,\"start\":28876},{\"end\":29545,\"start\":29473},{\"end\":29914,\"start\":29845},{\"end\":30263,\"start\":30190}]", "bib_author": "[{\"end\":22703,\"start\":22690},{\"end\":22713,\"start\":22703},{\"end\":22725,\"start\":22713},{\"end\":22735,\"start\":22725},{\"end\":23068,\"start\":23059},{\"end\":23078,\"start\":23068},{\"end\":23427,\"start\":23414},{\"end\":23438,\"start\":23427},{\"end\":23450,\"start\":23438},{\"end\":23720,\"start\":23706},{\"end\":23735,\"start\":23720},{\"end\":23748,\"start\":23735},{\"end\":24149,\"start\":24140},{\"end\":24159,\"start\":24149},{\"end\":24170,\"start\":24159},{\"end\":24421,\"start\":24412},{\"end\":24431,\"start\":24421},{\"end\":24442,\"start\":24431},{\"end\":24727,\"start\":24716},{\"end\":24737,\"start\":24727},{\"end\":24748,\"start\":24737},{\"end\":25205,\"start\":25199},{\"end\":25212,\"start\":25205},{\"end\":25219,\"start\":25212},{\"end\":25225,\"start\":25219},{\"end\":25231,\"start\":25225},{\"end\":25545,\"start\":25536},{\"end\":25553,\"start\":25545},{\"end\":25564,\"start\":25553},{\"end\":25575,\"start\":25564},{\"end\":25582,\"start\":25575},{\"end\":25911,\"start\":25899},{\"end\":25922,\"start\":25911},{\"end\":25932,\"start\":25922},{\"end\":25948,\"start\":25932},{\"end\":26306,\"start\":26298},{\"end\":26315,\"start\":26306},{\"end\":26322,\"start\":26315},{\"end\":26333,\"start\":26322},{\"end\":26722,\"start\":26708},{\"end\":26730,\"start\":26722},{\"end\":26739,\"start\":26730},{\"end\":26748,\"start\":26739},{\"end\":26755,\"start\":26748},{\"end\":26770,\"start\":26755},{\"end\":26778,\"start\":26770},{\"end\":27142,\"start\":27133},{\"end\":27150,\"start\":27142},{\"end\":27157,\"start\":27150},{\"end\":27168,\"start\":27157},{\"end\":27179,\"start\":27168},{\"end\":27405,\"start\":27395},{\"end\":27418,\"start\":27405},{\"end\":27608,\"start\":27595},{\"end\":27620,\"start\":27608},{\"end\":27633,\"start\":27620},{\"end\":27924,\"start\":27916},{\"end\":27936,\"start\":27924},{\"end\":27944,\"start\":27936},{\"end\":28270,\"start\":28260},{\"end\":28283,\"start\":28270},{\"end\":28296,\"start\":28283},{\"end\":28606,\"start\":28598},{\"end\":28615,\"start\":28606},{\"end\":28626,\"start\":28615},{\"end\":28636,\"start\":28626},{\"end\":28956,\"start\":28944},{\"end\":28969,\"start\":28956},{\"end\":29278,\"start\":29270},{\"end\":29287,\"start\":29278},{\"end\":29297,\"start\":29287},{\"end\":29306,\"start\":29297},{\"end\":29555,\"start\":29547},{\"end\":29566,\"start\":29555},{\"end\":29578,\"start\":29566},{\"end\":29594,\"start\":29578},{\"end\":29926,\"start\":29916},{\"end\":29934,\"start\":29926},{\"end\":29945,\"start\":29934},{\"end\":30274,\"start\":30265},{\"end\":30285,\"start\":30274},{\"end\":30295,\"start\":30285}]", "bib_venue": "[{\"end\":23883,\"start\":23824},{\"end\":24883,\"start\":24824},{\"end\":22797,\"start\":22735},{\"end\":23121,\"start\":23078},{\"end\":23479,\"start\":23450},{\"end\":23822,\"start\":23748},{\"end\":24214,\"start\":24170},{\"end\":24504,\"start\":24442},{\"end\":24822,\"start\":24748},{\"end\":25286,\"start\":25231},{\"end\":25641,\"start\":25582},{\"end\":25984,\"start\":25948},{\"end\":26370,\"start\":26333},{\"end\":26837,\"start\":26778},{\"end\":27183,\"start\":27179},{\"end\":27393,\"start\":27349},{\"end\":27682,\"start\":27633},{\"end\":27989,\"start\":27944},{\"end\":28349,\"start\":28296},{\"end\":28695,\"start\":28636},{\"end\":29028,\"start\":28969},{\"end\":29268,\"start\":29201},{\"end\":29631,\"start\":29594},{\"end\":30005,\"start\":29945},{\"end\":30344,\"start\":30295}]"}}}, "year": 2023, "month": 12, "day": 17}