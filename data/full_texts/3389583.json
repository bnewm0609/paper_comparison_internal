{"id": 3389583, "updated": "2023-09-28 18:35:16.748", "metadata": {"title": "Long Text Generation via Adversarial Training with Leaked Information", "authors": "[{\"first\":\"Jiaxian\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Sidi\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Weinan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 9, "day": 24}, "abstract": "Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional Manager module, which takes the extracted features of current generated words and outputs a latent vector to guide the Worker module for next-word generation. Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between Manager and Worker.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1709.08624", "mag": "2963248348", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/GuoLCZYW18", "doi": "10.1609/aaai.v32i1.11957"}}, "content": {"source": {"pdf_hash": "bfc9a449e6364817a5a3e19b73b1527a85c32d02", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1709.08624v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/11957/11816", "status": "BRONZE"}}, "grobid": {"id": "3deea53234de8858aa5803ac8a5cc0ea0cc1d715", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bfc9a449e6364817a5a3e19b73b1527a85c32d02.txt", "contents": "\nLong Text Generation via Adversarial Training with Leaked Information\n\n\nJiaxian Guo \nSidi Lu \nHan Cai \nWeinan Zhang \nYong Yu \nJun Wang j.wang@cs.ucl.ac.uk \nUniversity College London\n\n\n\u2021 \u2020 Shanghai \n\nJiao Tong University\n\n\nLong Text Generation via Adversarial Training with Leaked Information\n\nAutomatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional MANAGER module, which takes the extracted features of current generated words and outputs a latent vector to guide the WORKER module for next-word generation. Our extensive experiments on synthetic data and various realworld tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between MANAGER and WORKER.\n\nIntroduction\n\nThe ability to generate coherent and semantically meaningful text plays a key role in many natural language processing applications such as machine translation , dialogue generation , and image captioning . While most previous work focuses on task-specific applications in supervised settings (Bahdanau, Cho, and Bengio 2014;Vinyals et al. 2015), the generic unsupervised text generation, which aims to mimic the distribution over real text from a corpus, has recently drawn much attention (Graves 2013;Yu et al. 2017;Zhang et al. 2017;Hu et al. 2017). A typical approach is to train a recurrent neural network (RNN) to maximize the log-likelihood of each ground-truth word given prior observed words (Graves 2013), which, however, suffers from so-called exposure bias * Correspondence to Weinan Zhang. due to the discrepancy between training and inference stage: the model sequentially generates the next word based on previously generated words during inference but itself is trained to generate words given ground-truth words (Husz\u00e1r 2015). A scheduled sampling approach  is proposed to addressed this problem, but is proved to be fundamentally inconsistent (Husz\u00e1r 2015). Generative Adversarial Nets (GAN) (Goodfellow et al. 2014), which is firstly proposed for continous data (image generation etc.), is then extended to discrete, sequential data to alleviate the above problem and has shown promising results (Yu et al. 2017). Due to the discrete nature of text samples, text generation is modeled as a sequential decision making process, where the state is previously generated words, the action is the next word to be generated, and the generative net G is a stochastic policy that maps current state to a distribution over the action space. After the whole text generation is done, the generated text samples are then fed to the discriminative net D, a classifier that is trained to distinguish real and generated text samples, to get reward signals for updating G.\n\nSince then, various methods have been proposed in text generation via GAN (Lin et al. 2017;Rajeswar et al. 2017;Che et al. 2017). Nonetheless, the reported results are limited to the cases that the generated text samples are short (say, fewer than 20 words) while more challenging long text generation is hardly studied, which is necessary for practical tasks such as auto-generation of news articles or product descriptions. A main drawback of existing methods to long text generation is that the binary guiding signal from D is sparse as it is only available when the whole text sample is generated. Also, the scalar guiding signal for a whole text is non-informative as it does not necessarily preserve the picture about the intermediate syntactic structure and semantics of the text that is being generated for G to sufficiently learn.\n\nOn one hand, to make the guiding signals more informative, discriminator D could potentially provide more guidance beside the final reward value, since D is a trained model, e.g. a convolutional neural network (CNN) (Zhang and LeCun 2015), rather than an unknown black box. With that idea,  proposed to train generator G via forcing learned feature representations of real and generated text by D to be matched, instead of directly training G to maximize the reward from D (Yu et al. 2017). Such a method can be effective in short text generation, but the guiding signals are still absent until the end of the text .\n\nOn the other hand, to alleviate the sparsity problem of the guiding signal, the idea of hierarchy naturally arises in text generation, since the real text samples are generated following some kinds of hierarchy such as the semantic structure and the part-of-speech (Mauldin 1984). By decomposing the whole generation task into various sub-tasks according to the hierarchical structure, it becomes much easier for the model to learn. Early efforts have been made to incorporate the hierarchy idea in text generation (Dethlefs and Cuay\u00e1huitl 2010;Peng et al. 2017) but all use a predefined sub-task set from domain knowledge, which makes them unable to adapt to arbitrary sequence generation tasks.\n\nIn this paper, we propose a new algorithmic framework called LeakGAN to address both the non-informativeness and the sparsity issues. LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning (Vezhnevets et al. 2017). As illustrated in Figure 1, we specifically introduce a hierarchical generator G, which consists of a high-level MANAGER module and a low-level WORKER module. The MANAGER is a long shortterm memory network (LSTM) (Hochreiter and Schmidhuber 1997) and serves as a mediator. In each step, it receives generator D's high-level feature representation, e.g., the feature map of the CNN, and uses it to form the guiding goal for the WORKER module in that timestep. As the information from D is internally-maintained and in an adversarial game it is not supposed to provide G with such information. We thus call it a leakage of information from D.\n\nNext, given the goal embedding produced by the MAN-AGER, the WORKER first encodes current generated words with another LSTM, then combines the output of the LSTM and the goal embedding to take a final action at current state. As such, the guiding signals from D are not only available to G at the end in terms of the scalar reward signals, but also available in terms of a goal embedding vector during the generation process to guide G how to get improved.\n\nWe conduct extensive experiments based on synthetic and real data. For synthetic data, LeakGAN obtains much lower negative log-likelihood than previous models with sequence length set to 20 and 40. For real data, we use the text in EMNLP2017 WMT News, COCO Image Caption and Chinese Poems as the long, mid-length and short text corpus, respectively. In all those cases, LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test. We further provide a deep investigation on the interaction between MAN-AGER and WORKER, which indicates LeakGAN implicitly learns sentence structures, such as punctuation, clause structure and long suffix without any supervision.\n\n\nRelated Work\n\nGenerating text that mimics human's expression has been studied for poem generation (Zhang and Lapata 2014), image captioning , dialogue system ) machine translation . (Graves Figure 1: An overview of our LeakGAN text generation framework. While the generator is responsible to generate the next word, the discriminator adversarially judges the generated sentence once it is complete. The chief novelty lies in that, unlike conventional adversarial training, during the process, the discriminator reveals its internal state (feature f t ) in order to guide the generator more informatively and frequently. (See Methodology Section for more details.) 2013) proposed a recurent neural network (RNN) based generative model to use the human-generated text where at each step the model tries to predict the next word given previous real word sequence and is trained in a supervised fashion.\n\nA common difficulty of all supervised generative models is that it is hard to design an appropriate, differentiable, lowbias metric to evaluate the output of the generator, which inspires the adversarial training mechanisms. (Goodfellow et al. 2014) proposed generative adversarial nets (GANs) to generate continuous data like images. GAN introduces a minimax game between a generative model and a discriminative model, where the discriminator can be viewed as the dynamically-updated evaluation metric to guide the tuning of the generated data. To apply GANs to text generation, (Yu et al. 2017) proposed SeqGAN that models the text generation as a sequential decision making process and trains the generative model with policy gradient methods . MaliGAN (Che et al. 2017) modifies the orginal GAN objective and proposes a set of training techniques to reduce the potential variance. To deal with the gradient vanishing problem of GAN, RankGAN (Lin et al. 2017) proposes an alternative solution to this problem by replacing the original binary classifier discriminator with a ranking model by taking a softmax over the expected cosine distances from the generated sequences to the real data. Another problem for the adversarial sequence generation models is that the binary feedback from the discriminator is not sufficiently informative, which requires a huge number of training and generated samples to improve the generator and could result in mode collapse problems. Feature Matching ) provides a mechanism that matches the latent feature distributions of real and generated sequences via a kernelized discepancy metric to alleviate the weak guidance and mode collapse problems. However, such enhancement only happens when the whole text sample is generated and thus the guiding signal is still sparse during the training.\n\nReinforcement learning (RL) on the other hand also faces a similar difficulty when reward signals are sparse (Kulkarni et al. 2016). Hierarchical RL is one of the promising techniques for handling the sparse reward issue (Sutton, Precup, and Singh 1999). A typical approach in hierarchical RL is to manually identify the hierarchical structure for the agent by defining several low-level sub-tasks and learning micropolicies for each sub-task while learning a macro-policy for choosing which sub-task to solve. Such methods can be very effective when the hierarchical structure is known a priori using domain knowledge in a given specific task, but fail to flexibly adapt to other tasks. Recently, (Vezhnevets et al. 2017) proposed an end-to-end framework for hierarchical RL where the sub-tasks are not identified manually but implicitly learned by a MANAGER module which takes current state as input and output a goal embedding vector to guide the low-level WORKER module.\n\nIn this work, we model the text generation procedure via adversarial training and policy gradient (Yu et al. 2017). To address the sparse reward issue in long text generation, we follow (Vezhnevets et al. 2017) and propose a hierarchy design, i.e. MANAGER and WORKER, for the generator. As the reward function in our case is a discriminative model rather than a black box in (Vezhnevets et al. 2017), the high-level feature extracted by the discriminator given the current generated word sequence is sent to the MANAGER module. As such, the MANAGER module can be also viewed as a spy that leaks information from the discriminator to better guide the generator. To our knowledge, this is the first work that considers the information leaking in GAN framework for better training generators and combines hierarchical RL to address long text generation problems.\n\n\nMethodology\n\nWe formalize the text generation problem as a sequential decision making process (Bachman and Precup 2015). Specifically, at each timestep t, the agent takes the previously generated words as its current state, denoted as s t = (x 1 , . . . , x i , . . . , x t ), where x i represents a word token in the given vocabulary V . A \u03b8-parameterized generative net G \u03b8 , which corresponds to a stochastic policy, maps s t to a distribution over the whole vocabulary, i.e. G \u03b8 (\u00b7|s t ), from which the action x t+1 , i.e. the next word to select is sampled. We also train a \u03c6-parameterized discriminative model D \u03c6 that provides a scalar guiding signal D \u03c6 (s T ) for G \u03b8 to adjust its parameters when the whole sentence s T has been generated.\n\nAs we discussed previously, although the above adversarial training is principled, the scalar guiding signal becomes relatively less informative when the sentence length T goes larger. To address this, the proposed LeakGAN framework allows discriminator D \u03c6 to provide additional information, denoted as features f t , of the current sentence s t (it is internally used for D \u03c6 itself for discrimination) to generator G \u03b8 (\u00b7|s t ). In LeakGAN, a hierarchical RL architecture is used as a promising mechanism to effectively incorporate such leaked information f t into the generation procedure of G \u03b8 (also see Figure 1).\n\n\nLeaked Features from D as Guiding Signals\n\nDifferent from typical model-free RL settings where the reward function is a black box, our adversarial text generation uses D \u03c6 as a learned reward function. Typically, D \u03c6 is a neural network and can be decomposed into a feature extractor F(\u00b7 ; \u03c6 f ) and a final sigmoid classification layer with weight vector \u03c6 l . Mathematically, given input s, we have\nD \u03c6 (s) = sigmoid(\u03c6 l F(s; \u03c6 f )) = sigmoid(\u03c6 l f ), (1) where \u03c6 = (\u03c6 f , \u03c6 l ) and sigmoid(z) = 1/(1 + e \u2212z ). f = F(s; \u03c6 f )\nis the feature vector of s in the last layer of D \u03c6 , which is to be leaked to generator G \u03b8 . As is shown in Eq. (13), for a given D \u03c6 , the reward value for each state s mainly depends on the extracted features f . As such, the objective of getting a higher reward from D \u03c6 is equivalent to finding a higher reward region in this extracted feature (Zhang and LeCun 2015); thus F(s; \u03c6 f ) outputs the CNN feature map vector as f after its convolution-pooling-activation layer. Other neural network models such as LSTM (Hochreiter and Schmidhuber 1997) can also be used to implement D \u03c6 . Compared to the scalar signal D \u03c6 (s), the feature vector f is a much more informative guiding signal for G \u03b8 , since it tells what the position of currently-generated words is in the extracted feature space.\nspace F(S; \u03c6 f ) = {F(s; \u03c6 f )} s\u2208S . Specifically, our feature extractor F(\u00b7 ; \u03c6 f ) in D \u03c6 is implemented by a CNN\n\nA Hierarchical Structure of G\n\nIn each step t during the generation procedure, to utilize the leaked information f t from D \u03c6 , we follow hierarchical RL (Vezhnevets et al. 2017) to have a hierarchical architecture of G \u03b8 . Specifically, we introduce a MANAGER module, an LSTM that takes the extracted feature vector f t as its input at each step t and outputs a goal vector g t , which is then fed into the WORKER module to guide the generation of the next word in order to approach the higher reward region in F(S; \u03c6 f ). Next we will first describe the detailed generator model in LeakGAN and then show how the MANAGER and WORKER are trained with the guiding signals from D \u03c6 .\n\nGeneration Process. The MANAGER and WORKER modules both start from an all-zero hidden state, denoted as h M 0 and h W 0 respectively. At each step, the MANAGER receives the leaked feature vector f t from the discriminator D \u03c6 , which is further combined with current hidden state of the MANAGER to produce the goal vector g t a\u015d\ng t , h M t = M(f t , h M t\u22121 ; \u03b8 m ), (2) g t =\u011d t / \u011d t ,(3)\nwhere M(\u00b7 ; \u03b8 m ) denotes the MANAGER module implemented by an LSTM with parameters \u03b8 m and h M t is the recurrent hidden vector of the LSTM.\n\nTo incorporate goals produced by MANAGER, a linear transformation \u03c8 with weight matrix W \u03c8 is performed on a summation over recent c goals to produce a k-dimensional goal embedding vector w t as\nw t = \u03c8 c i=1 g t\u2212i = W \u03c8 c i=1 g t\u2212i .(4)\nGiven the goal embedding vector w t , the WORKER module takes the current word x t as input and outputs a matrix O t , which is further combined with w t by matrix product to determine the final action space distribution under current state\ns t through a softmax O t , h W t = W(x t , h W t\u22121 ; \u03b8 w ), (5) G \u03b8 (\u00b7|s t ) = softmax(O t \u00b7 w t /\u03b1),(6)\nwhere W(\u00b7 ; \u03b8 w ) denotes the WORKER module, i.e. an LSTM with h W t as its recurrent hidden vector, O t is a |V |\u00d7k matrix that represents the current vector for all words, thus O t \u00b7 w t yields the calculated logits for all words, and \u03b1 is the temperature parameter to control the generation entropy.\n\nTraining of G Notice that the above procedure is fully differentiable. One can train G \u03b8 in an end-to-end manner using a policy gradient algorithm such as REINFORCE (Williams 1992). In LeakGAN, we would hope the MANAGER module to capture some meaningful patterns. Thus, we follow (Vezhnevets et al. 2017) and train the MANAGER and WORKER modules separately, where the MANAGER is trained to predict advantageous directions in the discriminative feature space and the WORKER is intrinsically rewarded to follow such directions. Similar to (Vezhnevets et al. 2017), the gradient of the MAN-AGER module is defined as \u2207 adv\n\u03b8m g t = \u2212Q F (s t , g t )\u2207 \u03b8m d cos f t+c \u2212 f t , g t (\u03b8 m ) , (7) where Q F (s t , g t ) = Q(F(s t ), g t ) = Q(f t , g t ) = E[r t ]\nis the expected reward under the current policy which can be approximately estimated via Monte Carlo search (Sutton et al. 2000;Yu et al. 2017). d cos represents the cosine similarity between the change of feature representation after cstep transitions, i.e. f t+c \u2212 f t , and the goal vector g t (\u03b8 m ) 1 produced by MANAGER as in Eq. (14). Intuitively, the loss function is to force the goal vector to match the transition in the feature space while achieving high reward.\n\nAt the same time, the WORKER is trained to maximize the reward using the REINFORCE algorithm (Williams 1992) as is done in (Yu et al. 2017),\n\u2207 \u03b8w E st\u22121\u223cG xt r I t W(x t |s t\u22121 ; \u03b8 w ) =E st\u22121\u223cG,xt\u223cW(xt|st\u22121) [r I t \u2207 \u03b8w log W(x t |s t\u22121 ; \u03b8 w )],(8)\nwhich can be approximated by sampling the state s t\u22121 and the action x t taken by WORKER. As the WORKER is encouraged to follow the directions produced by the MANAGER, following (Vezhnevets et al. 2017), the intrinsic reward for the WORKER is defined as 1 We use gt(\u03b8m) to explicitly show gt is parameterized by \u03b8m.\nr I t = 1 c c i=1 d cos f t \u2212 f t\u2212i , g t\u2212i .(9)\nIn practice, before the adversarial training, we need to pretrain G \u03b8 . To be consistent, in the pre-train stage, we also use the separate training scheme, where the gradient of MAN-AGER is\n\u2207 pre \u03b8m g t = \u2212\u2207 \u03b8m d cos f t+c \u2212f t , g t (\u03b8 m ) ,(10)\nwheref t = F(\u015d t ),\u015d t and\u015d t+c are states of real text, and the state-action value Q F (s t , g t ) in Eq. (20) is set as 1 here since the data instances used in pre-training are all real sentences. As such, the MANAGER is trained to mimic the transition of real text samples in the feature space. While the WORKER is trained via maximum likelihood estimation (MLE).\n\nIn the training process, the generator G \u03b8 and discriminator D \u03c6 are alternatively trained. In the generator, the MAN-AGER M(\u00b7 ; \u03b8 m ) and WORKER W(\u00b7 ; \u03b8 w ) (including \u03c8 and softmax) are alternatively trained while fixing the other. The details of the training procedure are attached in the appendix.\n\n\nTraining Techniques\n\nBootstrapped Rescaled Activation. During the adversarial training of SeqGAN (Yu et al. 2017), severe gradient vanishing occurs when D is much stronger than G, i.e. the reward is too small value to update the parameters and thus need be rescaled before being fed into G. Inspired by ranking idea from RankGAN (Lin et al. 2017), we propose a simple, time-efficient, rank-based method to rescale the rewards, named as bootstrapped rescaled activation. For a mini-batch with B sequences, after the rollout of the generative model, the reward matrix is denoted as R B\u00d7T . For each timestep t, we rescale the t-th column vector R t via\nR t i = \u03c3 \u03b4 \u00b7 0.5 \u2212 rank(i) B ,(11)\nwhere rank(i) denotes the i-th element's high-to-low ranking in this column vector. \u03b4 is a hyperparameter that controls the smoothness of the rescale activation. \u03c3(\u00b7) is an activation function that re-projects the equidifferent scoring based on ranking to a more effective distribution. In our experiment, for example, the model adopts hyperparameter \u03b4 = 12.0 and the sigmoid function as \u03c3(\u00b7).\n\nThere are two main advantages of the bootstrapped rescaled activation. First, after this transformation, the expectation and variance of the reward in each mini-batch are constant. In this case, the rescale activation serves as a value stabilizer that is helpful for algorithms that are sensitive in numerical variance. Second, as all ranking methods do, it prevents the gradient vanishing problem, which accelerates the model convergence.\n\nInterleaved Training. In traditional generative adversarial models, mode collapse is a common problem. Here we propose a training scheme called interleaved training to alleviate such a problem. As its name is, we adopt an interleaving of supervised training (i.e. MLE) and adversarial training (i.e. GAN) instead of full GAN after the pre-training. For example, we perform one epoch of supervised learning for G after 15 epochs of adversarial training. An explanation of why this scheme works is that blending these two trainings would help GAN get rid of some bad local minimums and alleviate mode collapse. Another justification is that the inserted supervised learning performs an implicit regularization on the generative model to prevent it from going too far away from the MLE solution.\n\nTemperature Control. The Boltzmann temperature \u03b1 in Eq. (18) is a factor that could be used to balance the exploration and exploitation for reinforcement learning problems. Here we select a higher temperature when we are training the model and a lower temperature when we adopt the model to generate samples.\n\n\nExperiment\n\nThe experiment consists of three parts: synthetic data experiments, experiments in real-world scenarios and some explanation study. The repeatable experiment code is published for further research 2 .\n\n\nTraining Settings\n\nSynthetic Oracle. For the synthetic data experiments, simlar to (Yu et al. 2017), we first initialize the parameters of an LSTM following the normal distribution N (0, 1) as the oracle describing the real data distribution G oracle (x t |x 1 , . . . , x t\u22121 ). We use it to generate 10,000 sequences of length 20 and 40 respectively as the training set S for the generative models.\n\nGAN Setting. For the discriminator, we choose the CNN architecture (Zhang and LeCun 2015) as the feature extractor and the binary classifier. Note that one could design specific structure for different tasks to refine the CNN performance. For the synthetic data experiment, the CNN kernel size ranges from 1 to T . The number of each kernel is between 100 and 200. In this case, the feature of text is a 1,720 dimensional vector. Dropout (Srivastava et al. 2014) with the keep rate 0.75 and L2 regularization are performed to avoid overfitting. For the generator, we adopt LSTM (Hochreiter and Schmidhuber 1997) as the architectures of MANAGER and WORKER to capture the sequence context information. The MANAGER produces the 16-dimensional goal embedding feature vector w t using the feature map extracted by CNN. The goal duration time c is a hyperparameter set as 4 after some preliminary experiments.\n\nCompared Models. For most parts of our experiment, three baseline models are mainly compared with LeakGAN, namely an MLE trained LSTM, SeqGAN (Yu et al. 2017) and RankGAN . We also compare model variants, such as SeqGAN with bootstrapped rescaled activation, and include the real data to be referred as the performance upperbound.\n\nEvaluation Metrics. Negative log-likehood (NLL) is used for synthetic data experiment since there is the oracle data distribution available for evaluation. For real-world data experiments, BLEU statistics (Papineni et al. 2002) and human rating scores in the Turing test are reported. We further 2 https://github.com/CR-Gjx/LeakGAN.   perform a t-test for the improvement of LeakGAN over the second highest performance and report the p-value.\n\n\nSynthetic Data Experiments\n\nWe run the synthetic data experiment with the text-length set as 20 and 40 respectively. The training curves are depicted in Figure 2 and the overall NLL performance is presented in Table 1. One could have two observations from the results. (i) In the pre-training stage, LeakGAN has already shown observable performance superiority compared to other models, which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones. (ii) In the adversarial training stage, LeakGAN shows a better speed of convergence, and the local minimum it explores is significantly better than previous results. The results demonstrate the effectiveness of the information leakage framework and the hierarchical RL architecture for generating both short and long texts.\n\n\nLong Text Generation: EMNLP2017 WMT News\n\nWe choose the EMNLP2017 WMT 3 Dataset as the long text corpus. Specifically, we pick the News section from the original dataset. The news dataset consists of 646,459 words and 397,726 sentences. We preprocess the data by eliminating the words with frequency lower than 4,050 as well as the sentence containing these low frequency words. Besides, to focus on long sentences, we remove the sentences with length less than 20. After the preprocessing, the news dataset has 5,742 words and 397,726 sentences. Then we randomly sample 200,000 sentences as the training set and  another 10,000 sentences as the test set. We use the BLEU-(2 to 5) scores (Papineni et al. 2002) as the evaluation metrics.\n\nThe results are provided in Table 2. In all measured metrics, LeakGAN shows significant performance gain compared to baseline models. The consistently higher BLEU scores indicate that the generated sentences of LeakGAN are of high quality in local features to mimic the real text.\n\n\nMid-length Text Generation: COCO Image Captions\n\nAnother real dataset we use is the COCO Image Captions Dataset (Chen et al. 2015), a dataset which contains groups of image-description pairs. We take the image captions as the text to generate. Note that the COCO Dataset is not a long text dataset, in which most sentences are of about 10 words. Thus we apply some preprocessing on the dataset. The COCO Image Captions training dataset consists of 20,734 words and 417,126 sentences. We remove the words with frequency lower than 10 as well as the sentence containing them. After the preprocessing, the dataset includes 4,980 words. We randomly sample 80,000 sentences for the training set, and another 5,000 for the test set.\n\nThe results BLEU scores are provided in Table 3. The results of the BLEU scores on the COCO dataset indicate that LeakGAN performs significantly better than baseline models in mid-length text generation task.\n\n\nShort Text Generation: Chinese Poems\n\nTo evaluate the performance of LeakGAN in short text generation, we pick the dataset of Chinese poems which is proposed by (Zhang and Lapata 2014) and most related work such as (Yu et al. 2017;Rajeswar et al. 2017;Lin et al. 2017). The dataset consists of 4-line 5-character poems. Following the above work, we use the BLEU-2 scores as the evaluating metrics.\n\nThe experimental results are provided in Table 4. The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks.\n\n\nPerformance Robustness in Long Text Generation\n\nLong text generation has always been difficult among all text generation problems. The difficulty of the problem is due to many factors, such as LSTM-RNN's failure to capture longterm dependency, discriminator's failure to give those \"good  but tiny\" sequences appropriate penalty. To explicitly evaluate the superiority of LeakGAN in long text generation, here we use the relative performance gain of LeakGAN over Se-qGAN (Yu et al. 2017) and RankGAN (Lin et al. 2017).\n\nThe results over EMNLP2017 WMT News data are shown in Figure 3. The curves clearly show that LeakGAN yields larger performance gain over the baselines when the generated sentences are longer. This fact supports our claim that LeakGAN is a robust framework for long text.\n\n\nTuring Test and Generated Samples\n\nSince BLEU score is a metric focusing on the local text statistics, which may not be sufficient for evaluating text generation quality, we also conduct a Turing test based on questionnaires on the Internet. In the questionnaire, each (machine generated or real) sentence gets +1 score when it is regarded as a real one, and 0 score otherwise. We conduct the test with text generated by the models trained on WMT News and COCO Image Captions. The average score for each algorithm is calculated. In practice, we sample 20 sentences from every method and invite 62 people to participate the test, where everyone should judge the quality of 30 sentences from the compared three methods and thus each sentence is judged by 31 people. For the comparison fairness, the sentences used in the questionnaires are randomly sampled. Table 5 gives the results. The performance on two datasets indicates that the generated sentences of LeakGAN are of higher global consistency and better readability than those of SeqGAN.\n\nA few samples generated by LeakGAN are illustrated in Table 6. More samples and their comparison with those from the baseline models are provided in the appendix. These samples are collected for the Turing test questionnaires. (1) A bathroom with tiled walls and a shower on it.\n\n(2) A young man is holding a bottle of wine in his hand.\n\n(2) A couple of kids in front of a bathroom that is in a bathroom.\n\n\nEMNLP2017 WMT\n\n(1) The American Medical Association said that the militants had been arrested in connection with the murder of the same incident.\n\n(1) \"I think you should really really leave for because we hadn't been busy, where it goes to one,\" he wrote.\n\n(2) This is the first time that the Fed has been able to launch a probe into the country' s nuclear program.\n\n(2) What you have to stop, if we do that, as late, law enforcement and where schools use a list of aid, it can rise. \n\n\nModel Explanation\n\nFeature Trace. To verify that LeakGAN successfully exploits of the leaked message, we visualize the feature vector f T extracted from the real data by discriminator. Besides, we visualize the feature trace, i.e. the features f t of prefix s t during the generation, for LeakGAN, SeqGAN and RankGAN via a 2-D principal component analysis (PCA). The visualized traces are plotted in Figure 4 and more cases are presented in the appendix. As we can see, during the generation process, in LeakGAN, the feature vector gradually approaches the real data feature vector region. However, previous models, i.e. SeqGAN and RankGAN, fail to match the features even when the generation is completed. This indicates that the proposed LeakGAN does finish its design purpose of exploiting the leaked information from D \u03c6 to better match the feature vector distributions of real data.\n\n\nBehaviors of Worker and Manager.\n\nTo give more details of how WORKER and MANAGER interact with each other and make use of the leaked information in the generative model, we visualize the interaction vector of the WORKER and MANAGER, i.e., the dimension-wise product of their output (O t \u00b7 w t as in Eq. (18)). Note that to simplify the explanation, here we reduce the signal dimension from 16 to 8. Figure 5 presents an example sentence and more cases are provided in the appendix.\n\n\nSubscores from hierachical architecture\n\nThe rise of the 6th subscore indicates the token is usually followed by a long suffix.\n\nThe rise of the 5th subscore indicates the structural token, e.g., a model verb.\n\nThe fluctuation of the 7th subscore reflects subsentence, e.g., a punctuation or a conjunction.  From Figure 5, we find some intuitive interpretations of the implicit rules learned by the interaction of WORKER and MANAGER. (i) The 5th dimension stands for current token's divergence from an entity token. If the 5th value is high, the token would most possibly be a structural token, such as a modal verb, an article or a preposition. (ii) The 6th dimension suggests how long the suffix from current step will be. If a peak occurs in the curve, there must be some token that triggers a long suffix. A frequently occurring example is the formal subject. (iii) Although hard to observe, we do find connections of the 7th dimension and the substructure of a sentence. For example, when the start or the end of a subsentence occurs, there is an observable fluctuation in the 7th dimension. This indicates that the token is most likely to be a punctuation or a conjuction.\n\n\nConclusion and Future work\n\nIn this paper, we proposed a new algorithmic framework called LeakGAN for generating long text via adversarial training. By leaking the feature extracted by the discriminator as the step-by-step guiding signal to guide the generator better generating long text, LeakGAN addresses the non-informativeness and sparsity problems of the scalar reward signal in previous GAN solutions. In the extensive experiments with synthetic data and real world data including long, mid-length and short text, LeakGAN achieved significant performance improvement over previous solutions, on both BLEU scores and human ratings. Moreover, the analysis of the results shows that LeakGAN yields larger performance gain when the longer sentences are generated. Finally, we also visualize and explain the efficacy of the guiding signals that LeakGAN learns without any supervision.\n\nFor future work, we plan to apply LeakGAN in more natural language process applications like dialogue systems and image captioning by providing more task-specific guiding information. Also, enhancing the capacity of the discriminator to check the global consistency of the whole sentence is a promising direction. \n\n\nExperiment Settings\n\nFor synthetic data with length 20, the learning rate for MANAGER and WORKER is set to 0.001. The goal dimension size k is set to 16. The embedding size of the LSTM-RNNs is set to 32. For the discriminative model, we set the hyperparameters of the CNN as Table 7.\n\nFor synthetic data with length 40, the learning rate for MANAGER and WORKER is set to 0.0005. The goal dimension size k is set to 16. The embedding size of the LSTM-RNNs is set to 32. For the discriminative model, we set the hyperparameters of the CNN as Table 7.  \n\n\nDiscussions\n\nThe Necessity of the Hierarchical Architecture. The hierarchical architecture in LeakGAN serves as the mechanism of incorporating leaked information from D into G. However, in the body part, we have not shown whether the exploitation of hierarchical architecture is a must. Actually, what we need to point out is, the exploitation of hierarchical reinforcement learning is not a must, but a good choice in sequence decision scenarios. We attempt to replace the hierarchical architecture by a fully connected layer. However, the numerical stability of the model is not satisfying with the original training settings. A possible reason is that, since the feature space of CNN changes rapidly during the training procedure, a linear transformation without any normalization may not be able to incorporate the information contained in the feature vector leaked from D.  Here in Figure 7 we present more examples for illustrating the interaction of WORKER and MANAGER to support our claims in the main text as below. Each curve shows a subscore of the token of that time step. Each dimension of the score, i.e. each subscore measures a specific feature of the token in that context.\n\n\nIllustration of WORKER and MANAGER's Behaviors\n\n(i) The 5th dimension stands for current token's divergence from an entity token. If the 5th value is high, the token would most possibly be a structural token, such as a modal verb, an article or a preposition.\n\n(ii) The 6th dimension suggests how long the suffix from current step will be. If a peak occurs in the curve, there must be some token that triggers a long suffix. A frequently occurring example is the formal subject.\n\n(iii) Although hard to observe, we do find connections of the 7th dimension and the substructure of a sentence. For example, when the start or end of a sub-sentence occurs, there is an observable fluctuation in the 7th dimension. This indicates that the token is most likely to be a punctuation or a conjuction. As we can see in Figure 8, during the generation process, in LeakGAN, the feature vector gradually approaches and finally gets into the high density real data feature vector region. However, previous models, i.e. SeqGAN and RankGAN, fail to match the features even when the generation is completed. This indicates that the proposed LeakGAN does finish its designed purpose of exploiting the leaked information from D \u03c6 to better match the feature vector distributions of real data. A man wearing a suit and coat holds a tie through and wood pants. Two men are working on a laptop in a room . A man who is standing next to a brown and white horse. A street sign with a red stop sign on the street pole. A cat is laying on a keyboard and mouse in the air. A man with a rainbow -colored shirt and a black dog. A crowd of people standing around or standing on a sidewalk. A man is sitting on his desk holding an umbrella. SeqGAN A woman is riding a bike on the street next to a bus.\n\n\nIllustration of\n\nA silver stove, the refrigerator, sitting in a kitchen. A guy doing tricks on a skateboard while a man is standing on a cellphone. A bunch of birds that are sitting in the sand. A bathroom with tiled walls and a shower on it.\n\nA couple of people are riding bikes down an asphalt road. An old photo of a man riding on a motorcycle with some people. A beautiful young girl in the bathroom has one has wine glasses and bottles above the counters. A person in a helmet standing next to a red street. An empty clean bathroom with a toilet and sink and tub. A kid in a black shirt and dog arms in a restaurant kitchen. A bathroom has a toilet, a sink and mirror. Two bicycles are parked outside inside a small brown field. The large rug is on the city under the city. A bathroom that is has a picture above and a sink. A small child jumping with glasses to a motor scooter. A white bathroom with a toilet, television and bathtub and a sink. A baby in a blue dress standing in front of a Frisbee. A cat and a woman standing by two computer preparing food. A pair of skis and pedestrians in a parking area near some different go. Two bikes in a parking lot with a dog that has a back on her. Out of those who came last year, 69 per cent were men, 18 per cent were children and just 13 per cent were women. 'Sometimes I think about leaving sex work, but because I am alone living costs are really expensive,' she said. 'I was then stuck in the house for nearly two years only going out for short periods of time,' she said. He has not played for Tottenham's first team since and it is now nearly two years since he completed a full Premier League match for the club. This is a part of the population that is notorious for its lack of interest in actually showing up when the political process takes place. I was paid far too little to pick up a dead off of the ground and put it back in the box. Local media reported the group were not looking to hurt anybody, but they would not rule out violence if police tried to remove them. The 55 to 43 vote was largely split down party lines and fell short of the 60 votes needed for the bill to advance. We got to a bus station in the evening, but our connection didn't leave until the following morning. It's actually something that I had to add, because I was getting really frustrated losing to my hitting partner all the time.\n\nTaiwan's Defence Ministry said it was \"aware of the information,\" and declined further immediate comment, Reuters reported. Her response to the international refugee crisis gave a million refugees hope that they may be able to begin a new life. I'm racing against a guy who I lost a medal to -but am I ever going to get that medal back? LeakGAN A man has been arrested at age 28, a resident in Seattle, which was widely reported in 2007 .\n\nI also think that' s a good place for us, I' m sure that this would be a good opportunity for me to get in touch . What is the biggest problem for Clinton is that Donald Trump will be in the race and he' s unlikely to be the nominee . \"We' re going to do and we' re going to put it out and get the ball,\" he said . \"I would be afraid to blame the girls to go back but I was just disappointed with the race,\" he said. \"I'm not going to work together with a different role and we can win the game,\" he added. The couple's lives are still missing and they have been killed in the city's way to play against them, and because I came out there. For the last three years, we've got a lot of things that we need to do with this is based on the financial markets. Don't ask me, but I know, if I' ll be able to be out of Hillary Clinton, I think it's being made for the Congress. \"I am proud to be able to move forward because we don't have to look at about,\" he said. That' s why we' re the most important people for the African American community and we' ve made a good response .\n\nBut the move will be only in a fight against them, as well as likely to prevent an agreement to remain in the EU. The American Medical Association said that the militants had been arrested in connection with the murder of the same incident. The two-year-old girl has been charged with a suspect who was in the vehicle to the police station.\n\nIt is hard to buy on the Olympics, but we probably don't see a lot of it. \"I'm not going to be very proud of the other countries,\" he said. He said the U. N. intelligence industry will not comment on the ground, which would be sensitive to the European Union. I take my work in the days, but I would have to go down on Wednesday night.\n\n\nSeqGAN\n\nYou only certainly might not rush it down for those circumstances where we are when they were the heads, and when she's name. \"I think you should really really leave for because we hadn't been busy, where it goes to one,\" he wrote. All the study knew was that they are, so they continue to provide support service and it doesn't exist. 'It can say become up with nothing sales have reached the charge for the other any evidence that been virtually well below the $ 800. Three times before the start of the season is much early on 2015 we are in the third training every year. That's the idea of strength that decision they said, we haven't already lost four or seven, or Liverpool's team. That is not the time for the cost of changing the system and it was pushing for $ 20 million. We had to take it a good day for a military, but nearly 6, 000 ] and prepare for them through. I actually didn't tell the background check the difference after my hour was to be recalled... and it was great.\n\nWe are thinking about 40, 000 and jobs in what is wrong in the coming and you know. That is out how working you can't set out some pretty tight... or what I'm going through. \"I wanted to be made you decided to have a crisis that way up and get some sort of weapon, not much to give birth to for an American room. She had been fined almost 200, 000 with couple of asylum seekers in Syria and Iraq. Perhaps not, in looking for, housing officials would help the frustration of Government, with an FBI shortly before 2020.\n\nOnce we got to real show for the young man since I'm sure she went to love it just, whether to be late later last year. But, after a holiday period we might have to go on a total -out debate like that could have happened to us.\n\nFigure 2 :\n2The illustration of training curve.\n\nFigure 3 :\n3The illustration of BLEU improvement change along with the generated text length on WMT News.\n\nFigure 4 :\n4Feature traces during the generation (SeqGAN, RankGAN and LeakGAN) and features of completed real data (all compressed to 2-dim by PCA) on WMT News.\n\nFigure 5 :\n5Illustration of WORKER and MANAGER's behaviors during a generation. (Dimension-wise Product of Worker and Manager)\n\nFigure 6 :\n6The feature extractor's architecture (without the highway and dropout layer) by(Zhang and LeCun 2015).\n\nFigure 7 :\n7Illustration of WORKER and MANAGER's behaviors during a generation. (Dimension-wise Product of Worker and Manager)\n\nFigure 8 :\n8Feature traces during generation process (SeqGAN, RankGAN and LeakGAN) and features of the completed real data (all compressed to 2-dim by PCA) on WMT News.\n\nTable 1 :\n1The over NLL performance on synthetic data.Length \nMLE \nSeqGAN RankGAN LeakGAN \nReal \np-value \n20 \n9.038 \n8.736 \n8.247 \n7.038 \n5.750 < 10 \u22126 \n40 \n10.411 \n10.310 \n9.958 \n7.191 \n4.071 < 10 \u22126 \n\n\n\nTable 2 :\n2BLEU scores performance on EMNLP2017 WMT.Method \nSeqGAN RankGAN LeakGAN p-value \nBLEU-2 \n0.8590 \n0.778 \n0.956 \n< 10 \u22126 \nBLEU-3 \n0.6015 \n0.478 \n0.819 \n< 10 \u22126 \nBLEU-4 \n0.4541 \n0.411 \n0.627 \n< 10 \u22126 \nBLEU-5 \n0.4498 \n0.463 \n0.498 \n< 10 \u22126 \n\n\n\nTable 3 :\n3BLEU scores on COCO Image Captions.Method \nSeqGAN RankGAN LeakGAN p-value \nBLEU-2 \n0.831 \n0.850 \n0.950 \n< 10 \u22126 \nBLEU-3 \n0.642 \n0.672 \n0.880 \n< 10 \u22126 \nBLEU-4 \n0.521 \n0.557 \n0.778 \n< 10 \u22126 \nBLEU-5 \n0.427 \n0.544 \n0.686 \n< 10 \u22126 \n\n\n\nTable 4 :\n4The BLEU performance on Chinese Poems.Method \nSeqGAN RankGAN LeakGAN \nBLEU-2 \n0.738 \n0.812 \n0.881 \np-value \n< 10 \u22126 \n< 10 \u22126 \n-\n\n\n\nTable 5 :\n5Turing test results for in real-world experiments.Dataset \nSeqGAN LeakGAN Ground Truth p-value \nWMT News \n0.236 \n0.554 \n0.651 \n< 10 \u22126 \nCOCO \n0.405 \n0.574 \n0.675 \n< 10 \u22126 \n\n\n\nTable 6 :\n6Samples from different methods on COCO Image Captions and EMNLP2017 WMT News.Datasets \nLeakGAN \nSeqGAN \n\nCOCO Image Captions \n(1) A man sitting in front of a microphone with \nhis dog sitting on his shoulder. \n\n\n\nTable 7 :\n7Convolutional layer structures.\n\nTable 8 :\n8COCO examples in the Turing test questionnaires. Sources Example Real data A blue and white bathroom with butterfly themed wall tiles.The vanity contains two sinks with a towel for each. Several metal balls sit in the sand near a group of people. A surfer, a woman, and a child walk on the beach. A kitchen with a countertop that includes an Apple phone. A closeup of a red fire hydrant including the chains. People standing around many silver round balls on the ground. A person on a bicycle is riding in front of a car. A kitchen with a tile floor has cabinets with no doors, a dishwasher, a sink, and a refrigerator. The top of a kitchen cabinet covered with brass pots and pans. A woman is shaving her face while sitting on a wooden bench. A stuffed animal is laying on the bed by a window. A wooden toilet seat sits open in an empty bathroom. A person is taking a photo of a cat in a car. A phone lies on the counter in a modern kitchen. silver balls laying on the ground around a smaller red ball. A man riding a bicycle on a road carrying a surf board. A man using his bicycle to go down a street. A set table with silverware, glasses and a bottle of wine. A large kite in the shape of the bottom half of a woman. LeakGAN A woman holding an umbrella while standing against a sidewalk.A bathroom with a toilet and sink and mirror. A train rides along the tracks in a train yard.A man with a racket stands in front of a shop window. A red and white photo of a train station. The bathroom is clean and ready for us to use . A man is walking with his dog on the boardwalk by the beach. A man in a shirt and tie standing next to a woman. A couple of luggage cart filled with bags on a shelf. Large white and clean bathroom with white tile floors and white walls . A group of people fly kites in the sky on a clear day.\n\nTable 9 :\n9EMNLP2017 WMT News examples in the Turing test questionnaires.Sources \nExample \nReal data \n\nhttp://statmt.org/wmt17/translation-task.html\nAppendixFormulas for Reference Discriminator f = F(s; \u03c6 f ), (12) D \u03c6 (s) = sigmoid(\u03c6 l \u00b7 F(s; \u03c6 f )) = sigmoid(\u03c6 l , f ),MANAGER of Generator\u011dWORKER of GeneratorPseudo CodeAlgorithm 1 Adversarial Training with Leaked InformationRequire: Hierachical policy G \u03b8m,\u03b8w ; discriminator D \u03c6 ; a sequence dataset S = {X1:T } 1: Initialize G \u03b8m,\u03b8w , D \u03c6 with random weights \u03b8m, \u03b8w, \u03c6. 2: Pre-train D \u03c6 (i.e. the feature extractor F(\u00b7; \u03c6 f ) and the output layer sigmoid(\u03c6 l , \u00b7)) using S as positive samples and output from G \u03b8m,\u03b8w as negative samples. 3: Pre-train G \u03b8m,\u03b8w using leaked information from D \u03c6 4: Perform the two parts of pre-training interleavingly until convergence. 5: repeat 6:for g-steps do 7:Generate a sequence Y1:T = (y1, . . . , yT ) \u223c G \u03b8 8:for t in 1 : T do 9:Store leaked information ft from D \u03c6 10:Get\nData generation as sequential decision making. P Bachman, D Precup, Advances in Neural Information Processing Systems. Bachman, P., and Precup, D. 2015. Data generation as sequential decision making. In Advances in Neural Information Processing Systems, 3249-3257.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473arXiv preprintBahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n\nScheduled sampling for sequence prediction with recurrent neural networks. S Bengio, O Vinyals, N Jaitly, N Shazeer, Advances in Neural Information Processing Systems. Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015. Sched- uled sampling for sequence prediction with recurrent neural net- works. In Advances in Neural Information Processing Systems, 1171-1179.\n\nT Che, Y Li, R Zhang, R D Hjelm, W Li, Y Song, Y Bengio, arXiv:1702.07983Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprintChe, T.; Li, Y.; Zhang, R.; Hjelm, R. D.; Li, W.; Song, Y.; and Ben- gio, Y. 2017. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983.\n\nMicrosoft coco captions: Data collection and evaluation server. X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Doll\u00e1r, C L Zitnick, arXiv:1504.00325arXiv preprintChen, X.; Fang, H.; Lin, T.-Y.; Vedantam, R.; Gupta, S.; Doll\u00e1r, P.; and Zitnick, C. L. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.\n\nHierarchical reinforcement learning for adaptive text generation. N Dethlefs, H Cuay\u00e1huitl, H Fang, S Gupta, F Iandola, R K Srivastava, L Deng, P Doll\u00e1r, J Gao, X He, M Mitchell, J C Platt, Proceedings of the 6th International Natural Language Generation Conference. the 6th International Natural Language Generation ConferenceAssociation for Computational LinguisticsProceedings of the IEEEDethlefs, N., and Cuay\u00e1huitl, H. 2010. Hierarchical reinforce- ment learning for adaptive text generation. In Proceedings of the 6th International Natural Language Generation Conference, 37- 45. Association for Computational Linguistics. Fang, H.; Gupta, S.; Iandola, F.; Srivastava, R. K.; Deng, L.; Doll\u00e1r, P.; Gao, J.; He, X.; Mitchell, M.; Platt, J. C.; et al. 2015. From cap- tions to visual concepts and back. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, 1473-1482.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde- Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Genera- tive adversarial nets. In Advances in neural information processing systems, 2672-2680.\n\nGenerating sequences with recurrent neural networks. A Graves, arXiv:1308.0850arXiv preprintGraves, A. 2013. Generating sequences with recurrent neural net- works. arXiv preprint arXiv:1308.0850.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98Hochreiter, S., and Schmidhuber, J. 1997. Long short-term mem- ory. Neural computation 9(8):1735-1780.\n\n. Z Hu, Z Yang, X Liang, R Salakhutdinov, E P Xing, arXiv:1703.00955Controllable text generation. arXiv preprintHu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and Xing, E. P. 2017. Controllable text generation. arXiv preprint arXiv:1703.00955.\n\nHow (not) to train your generative model: Scheduled sampling, likelihood, adversary?. F Husz\u00e1r, arXiv:1511.05101arXiv preprintHusz\u00e1r, F. 2015. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101.\n\nHierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. T D Kulkarni, K Narasimhan, A Saeedi, J Tenenbaum, Advances in Neural Information Processing Systems. Kulkarni, T. D.; Narasimhan, K.; Saeedi, A.; and Tenenbaum, J. 2016. Hierarchical deep reinforcement learning: Integrating tem- poral abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems, 3675-3683.\n\nAdversarial learning for neural dialogue generation. J Li, W Monroe, T Shi, A Ritter, D Jurafsky, arXiv:1701.06547arXiv preprintLi, J.; Monroe, W.; Shi, T.; Ritter, A.; and Jurafsky, D. 2017. Ad- versarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547.\n\nK Lin, D Li, X He, Z Zhang, M.-T Sun, arXiv:1705.11001Adversarial ranking for language generation. arXiv preprintLin, K.; Li, D.; He, X.; Zhang, Z.; and Sun, M.-T. 2017. Adversarial ranking for language generation. arXiv preprint arXiv:1705.11001.\n\nSemantic rule based text generation. M L Mauldin, Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics. the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational LinguisticsAssociation for Computational LinguisticsMauldin, M. L. 1984. Semantic rule based text generation. In Pro- ceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computa- tional Linguistics, 376-380. Association for Computational Lin- guistics.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In Pro- ceedings of the 40th annual meeting on association for computa- tional linguistics, 311-318. Association for Computational Lin- guistics.\n\nB Peng, X Li, L Li, J Gao, A Celikyilmaz, S Lee, K.-F Wong, arXiv:1704.03084Composite task-completion dialogue system via hierarchical deep reinforcement learning. arXiv preprintPeng, B.; Li, X.; Li, L.; Gao, J.; Celikyilmaz, A.; Lee, S.; and Wong, K.-F. 2017. Composite task-completion dialogue sys- tem via hierarchical deep reinforcement learning. arXiv preprint arXiv:1704.03084.\n\nS Rajeswar, S Subramanian, F Dutil, C Pal, A Courville, arXiv:1705.10929Adversarial generation of natural language. arXiv preprintRajeswar, S.; Subramanian, S.; Dutil, F.; Pal, C.; and Courville, A. 2017. Adversarial generation of natural language. arXiv preprint arXiv:1705.10929.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of machine learning research. 151Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research 15(1):1929-1958.\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, NIPS. Sutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y.; et al. 1999. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1057-1063.\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, Advances in neural information processing systems. Sutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour, Y. 2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information pro- cessing systems, 1057-1063.\n\nBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial intelligence. 1121-2Sutton, R. S.; Precup, D.; and Singh, S. 1999. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence 112(1-2):181-211.\n\nA S Vezhnevets, S Osindero, T Schaul, N Heess, M Jaderberg, D Silver, K Kavukcuoglu, arXiv:1703.01161Feudal networks for hierarchical reinforcement learning. arXiv preprintVezhnevets, A. S.; Osindero, S.; Schaul, T.; Heess, N.; Jader- berg, M.; Silver, D.; and Kavukcuoglu, K. 2017. Feudal net- works for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161.\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3156-3164.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4Williams, R. J. 1992. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning 8(3-4):229-256.\n\nImproving neural machine translation with conditional sequence generative adversarial nets. Z Yang, W Chen, F Wang, B Xu, arXiv:1703.04887arXiv preprintYang, Z.; Chen, W.; Wang, F.; and Xu, B. 2017. Improving neural machine translation with conditional sequence generative adversar- ial nets. arXiv preprint arXiv:1703.04887.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. L Yu, W Zhang, J Wang, Y Yu, AAAI. Yu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, 2852- 2858.\n\nChinese poetry generation with recurrent neural networks. X Zhang, M Lapata, EMNLP. Zhang, X., and Lapata, M. 2014. Chinese poetry generation with recurrent neural networks. In EMNLP, 670-680.\n\nX Zhang, Y Lecun, arXiv:1502.01710Text understanding from scratch. arXiv preprintZhang, X., and LeCun, Y. 2015. Text understanding from scratch. arXiv preprint arXiv:1502.01710.\n\nAdversarial feature matching for text generation. Y Zhang, Z Gan, K Fan, Z Chen, R Henao, D Shen, Carin , L , arXiv:1706.03850arXiv preprintZhang, Y.; Gan, Z.; Fan, K.; Chen, Z.; Henao, R.; Shen, D.; and Carin, L. 2017. Adversarial feature matching for text generation. arXiv preprint arXiv:1706.03850.\n", "annotations": {"author": "[{\"end\":85,\"start\":73},{\"end\":94,\"start\":86},{\"end\":103,\"start\":95},{\"end\":117,\"start\":104},{\"end\":126,\"start\":118},{\"end\":184,\"start\":127},{\"end\":198,\"start\":185},{\"end\":222,\"start\":199}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":93,\"start\":91},{\"end\":102,\"start\":99},{\"end\":116,\"start\":111},{\"end\":125,\"start\":123},{\"end\":135,\"start\":131},{\"end\":197,\"start\":189}]", "author_first_name": "[{\"end\":80,\"start\":73},{\"end\":90,\"start\":86},{\"end\":98,\"start\":95},{\"end\":110,\"start\":104},{\"end\":122,\"start\":118},{\"end\":130,\"start\":127},{\"end\":186,\"start\":185},{\"end\":188,\"start\":187}]", "author_affiliation": "[{\"end\":183,\"start\":157},{\"end\":221,\"start\":200}]", "title": "[{\"end\":70,\"start\":1},{\"end\":292,\"start\":223}]", "venue": null, "abstract": "[{\"end\":1862,\"start\":294}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2203,\"start\":2171},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2223,\"start\":2203},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2381,\"start\":2368},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2396,\"start\":2381},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2414,\"start\":2396},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2429,\"start\":2414},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2592,\"start\":2579},{\"end\":2680,\"start\":2674},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2920,\"start\":2907},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3052,\"start\":3039},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3112,\"start\":3088},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3308,\"start\":3293},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3945,\"start\":3928},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3966,\"start\":3945},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3982,\"start\":3966},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4933,\"start\":4911},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5184,\"start\":5168},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5592,\"start\":5578},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5858,\"start\":5828},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5874,\"start\":5858},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6334,\"start\":6310},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6582,\"start\":6549},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8277,\"start\":8254},{\"end\":8354,\"start\":8338},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9306,\"start\":9282},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9653,\"start\":9637},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9829,\"start\":9813},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10018,\"start\":10002},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11017,\"start\":10995},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11139,\"start\":11107},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11608,\"start\":11584},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11976,\"start\":11960},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12071,\"start\":12048},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12261,\"start\":12237},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12843,\"start\":12818},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14999,\"start\":14977},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15721,\"start\":15697},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17829,\"start\":17814},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17952,\"start\":17929},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18210,\"start\":18186},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18532,\"start\":18512},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18547,\"start\":18532},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18988,\"start\":18973},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19019,\"start\":19003},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19333,\"start\":19309},{\"end\":19386,\"start\":19385},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20529,\"start\":20513},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20761,\"start\":20745},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23358,\"start\":23342},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23750,\"start\":23728},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24122,\"start\":24099},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24272,\"start\":24239},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24724,\"start\":24708},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25125,\"start\":25103},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26876,\"start\":26854},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27318,\"start\":27300},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28311,\"start\":28288},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28358,\"start\":28342},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28379,\"start\":28358},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28395,\"start\":28379},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29170,\"start\":29154},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29200,\"start\":29183},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":45579,\"start\":45557}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45067,\"start\":45019},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45174,\"start\":45068},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45336,\"start\":45175},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45464,\"start\":45337},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45580,\"start\":45465},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45708,\"start\":45581},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45878,\"start\":45709},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46083,\"start\":45879},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46334,\"start\":46084},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46575,\"start\":46335},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46717,\"start\":46576},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46903,\"start\":46718},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47126,\"start\":46904},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47170,\"start\":47127},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49003,\"start\":47171},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":49107,\"start\":49004}]", "paragraph": "[{\"end\":3852,\"start\":1878},{\"end\":4693,\"start\":3854},{\"end\":5311,\"start\":4695},{\"end\":6009,\"start\":5313},{\"end\":6976,\"start\":6011},{\"end\":7434,\"start\":6978},{\"end\":8153,\"start\":7436},{\"end\":9055,\"start\":8170},{\"end\":10884,\"start\":9057},{\"end\":11860,\"start\":10886},{\"end\":12721,\"start\":11862},{\"end\":13474,\"start\":12737},{\"end\":14096,\"start\":13476},{\"end\":14499,\"start\":14142},{\"end\":15424,\"start\":14627},{\"end\":16223,\"start\":15574},{\"end\":16553,\"start\":16225},{\"end\":16758,\"start\":16617},{\"end\":16954,\"start\":16760},{\"end\":17238,\"start\":16998},{\"end\":17647,\"start\":17345},{\"end\":18267,\"start\":17649},{\"end\":18878,\"start\":18404},{\"end\":19020,\"start\":18880},{\"end\":19446,\"start\":19131},{\"end\":19685,\"start\":19496},{\"end\":20110,\"start\":19743},{\"end\":20413,\"start\":20112},{\"end\":21066,\"start\":20437},{\"end\":21496,\"start\":21103},{\"end\":21937,\"start\":21498},{\"end\":22731,\"start\":21939},{\"end\":23041,\"start\":22733},{\"end\":23256,\"start\":23056},{\"end\":23659,\"start\":23278},{\"end\":24564,\"start\":23661},{\"end\":24896,\"start\":24566},{\"end\":25340,\"start\":24898},{\"end\":26163,\"start\":25371},{\"end\":26903,\"start\":26208},{\"end\":27185,\"start\":26905},{\"end\":27914,\"start\":27237},{\"end\":28124,\"start\":27916},{\"end\":28524,\"start\":28165},{\"end\":28680,\"start\":28526},{\"end\":29201,\"start\":28731},{\"end\":29473,\"start\":29203},{\"end\":30518,\"start\":29511},{\"end\":30798,\"start\":30520},{\"end\":30856,\"start\":30800},{\"end\":30924,\"start\":30858},{\"end\":31072,\"start\":30942},{\"end\":31183,\"start\":31074},{\"end\":31293,\"start\":31185},{\"end\":31412,\"start\":31295},{\"end\":32302,\"start\":31434},{\"end\":32786,\"start\":32339},{\"end\":32916,\"start\":32830},{\"end\":32998,\"start\":32918},{\"end\":33967,\"start\":33000},{\"end\":34856,\"start\":33998},{\"end\":35172,\"start\":34858},{\"end\":35458,\"start\":35196},{\"end\":35725,\"start\":35460},{\"end\":36918,\"start\":35741},{\"end\":37180,\"start\":36969},{\"end\":37399,\"start\":37182},{\"end\":38691,\"start\":37401},{\"end\":38936,\"start\":38711},{\"end\":41074,\"start\":38938},{\"end\":41514,\"start\":41076},{\"end\":42589,\"start\":41516},{\"end\":42931,\"start\":42591},{\"end\":43268,\"start\":42933},{\"end\":44269,\"start\":43279},{\"end\":44789,\"start\":44271},{\"end\":45018,\"start\":44791}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14626,\"start\":14500},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15541,\"start\":15425},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16616,\"start\":16554},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16997,\"start\":16955},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17344,\"start\":17239},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18403,\"start\":18268},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19130,\"start\":19021},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19495,\"start\":19447},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19742,\"start\":19686},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21102,\"start\":21067}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25560,\"start\":25553},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26940,\"start\":26933},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27963,\"start\":27956},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28574,\"start\":28567},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30339,\"start\":30332},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30581,\"start\":30574},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":35457,\"start\":35450},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":35722,\"start\":35715}]", "section_header": "[{\"end\":1876,\"start\":1864},{\"end\":8168,\"start\":8156},{\"end\":12735,\"start\":12724},{\"end\":14140,\"start\":14099},{\"end\":15572,\"start\":15543},{\"end\":20435,\"start\":20416},{\"end\":23054,\"start\":23044},{\"end\":23276,\"start\":23259},{\"end\":25369,\"start\":25343},{\"end\":26206,\"start\":26166},{\"end\":27235,\"start\":27188},{\"end\":28163,\"start\":28127},{\"end\":28729,\"start\":28683},{\"end\":29509,\"start\":29476},{\"end\":30940,\"start\":30927},{\"end\":31432,\"start\":31415},{\"end\":32337,\"start\":32305},{\"end\":32828,\"start\":32789},{\"end\":33996,\"start\":33970},{\"end\":35194,\"start\":35175},{\"end\":35739,\"start\":35728},{\"end\":36967,\"start\":36921},{\"end\":38709,\"start\":38694},{\"end\":43277,\"start\":43271},{\"end\":45030,\"start\":45020},{\"end\":45079,\"start\":45069},{\"end\":45186,\"start\":45176},{\"end\":45348,\"start\":45338},{\"end\":45476,\"start\":45466},{\"end\":45592,\"start\":45582},{\"end\":45720,\"start\":45710},{\"end\":45889,\"start\":45880},{\"end\":46094,\"start\":46085},{\"end\":46345,\"start\":46336},{\"end\":46586,\"start\":46577},{\"end\":46728,\"start\":46719},{\"end\":46914,\"start\":46905},{\"end\":47137,\"start\":47128},{\"end\":47181,\"start\":47172},{\"end\":49014,\"start\":49005}]", "table": "[{\"end\":46083,\"start\":45934},{\"end\":46334,\"start\":46137},{\"end\":46575,\"start\":46382},{\"end\":46717,\"start\":46626},{\"end\":46903,\"start\":46780},{\"end\":47126,\"start\":46993},{\"end\":49107,\"start\":49078}]", "figure_caption": "[{\"end\":45067,\"start\":45032},{\"end\":45174,\"start\":45081},{\"end\":45336,\"start\":45188},{\"end\":45464,\"start\":45350},{\"end\":45580,\"start\":45478},{\"end\":45708,\"start\":45594},{\"end\":45878,\"start\":45722},{\"end\":45934,\"start\":45891},{\"end\":46137,\"start\":46096},{\"end\":46382,\"start\":46347},{\"end\":46626,\"start\":46588},{\"end\":46780,\"start\":46730},{\"end\":46993,\"start\":46916},{\"end\":47170,\"start\":47139},{\"end\":49003,\"start\":47183},{\"end\":49078,\"start\":49016}]", "figure_ref": "[{\"end\":6362,\"start\":6354},{\"end\":14094,\"start\":14086},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25504,\"start\":25496},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29265,\"start\":29257},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31823,\"start\":31815},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32712,\"start\":32704},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33110,\"start\":33102},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36623,\"start\":36615},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37738,\"start\":37730}]", "bib_author_first_name": "[{\"end\":50023,\"start\":50022},{\"end\":50034,\"start\":50033},{\"end\":50313,\"start\":50312},{\"end\":50325,\"start\":50324},{\"end\":50332,\"start\":50331},{\"end\":50594,\"start\":50593},{\"end\":50604,\"start\":50603},{\"end\":50615,\"start\":50614},{\"end\":50625,\"start\":50624},{\"end\":50891,\"start\":50890},{\"end\":50898,\"start\":50897},{\"end\":50904,\"start\":50903},{\"end\":50913,\"start\":50912},{\"end\":50915,\"start\":50914},{\"end\":50924,\"start\":50923},{\"end\":50930,\"start\":50929},{\"end\":50938,\"start\":50937},{\"end\":51301,\"start\":51300},{\"end\":51309,\"start\":51308},{\"end\":51320,\"start\":51316},{\"end\":51327,\"start\":51326},{\"end\":51339,\"start\":51338},{\"end\":51348,\"start\":51347},{\"end\":51358,\"start\":51357},{\"end\":51360,\"start\":51359},{\"end\":51659,\"start\":51658},{\"end\":51671,\"start\":51670},{\"end\":51685,\"start\":51684},{\"end\":51693,\"start\":51692},{\"end\":51702,\"start\":51701},{\"end\":51713,\"start\":51712},{\"end\":51715,\"start\":51714},{\"end\":51729,\"start\":51728},{\"end\":51737,\"start\":51736},{\"end\":51747,\"start\":51746},{\"end\":51754,\"start\":51753},{\"end\":51760,\"start\":51759},{\"end\":51772,\"start\":51771},{\"end\":51774,\"start\":51773},{\"end\":52525,\"start\":52524},{\"end\":52539,\"start\":52538},{\"end\":52556,\"start\":52555},{\"end\":52565,\"start\":52564},{\"end\":52571,\"start\":52570},{\"end\":52587,\"start\":52586},{\"end\":52596,\"start\":52595},{\"end\":52609,\"start\":52608},{\"end\":52940,\"start\":52939},{\"end\":53108,\"start\":53107},{\"end\":53122,\"start\":53121},{\"end\":53265,\"start\":53264},{\"end\":53271,\"start\":53270},{\"end\":53279,\"start\":53278},{\"end\":53288,\"start\":53287},{\"end\":53305,\"start\":53304},{\"end\":53307,\"start\":53306},{\"end\":53595,\"start\":53594},{\"end\":53872,\"start\":53871},{\"end\":53874,\"start\":53873},{\"end\":53886,\"start\":53885},{\"end\":53900,\"start\":53899},{\"end\":53910,\"start\":53909},{\"end\":54265,\"start\":54264},{\"end\":54271,\"start\":54270},{\"end\":54281,\"start\":54280},{\"end\":54288,\"start\":54287},{\"end\":54298,\"start\":54297},{\"end\":54493,\"start\":54492},{\"end\":54500,\"start\":54499},{\"end\":54506,\"start\":54505},{\"end\":54512,\"start\":54511},{\"end\":54524,\"start\":54520},{\"end\":54779,\"start\":54778},{\"end\":54781,\"start\":54780},{\"end\":55444,\"start\":55443},{\"end\":55456,\"start\":55455},{\"end\":55466,\"start\":55465},{\"end\":55477,\"start\":55473},{\"end\":55946,\"start\":55945},{\"end\":55954,\"start\":55953},{\"end\":55960,\"start\":55959},{\"end\":55966,\"start\":55965},{\"end\":55973,\"start\":55972},{\"end\":55988,\"start\":55987},{\"end\":55998,\"start\":55994},{\"end\":56331,\"start\":56330},{\"end\":56343,\"start\":56342},{\"end\":56358,\"start\":56357},{\"end\":56367,\"start\":56366},{\"end\":56374,\"start\":56373},{\"end\":56681,\"start\":56680},{\"end\":56695,\"start\":56694},{\"end\":56697,\"start\":56696},{\"end\":56707,\"start\":56706},{\"end\":56721,\"start\":56720},{\"end\":56734,\"start\":56733},{\"end\":57084,\"start\":57083},{\"end\":57086,\"start\":57085},{\"end\":57096,\"start\":57095},{\"end\":57098,\"start\":57097},{\"end\":57112,\"start\":57111},{\"end\":57114,\"start\":57113},{\"end\":57123,\"start\":57122},{\"end\":57395,\"start\":57394},{\"end\":57397,\"start\":57396},{\"end\":57407,\"start\":57406},{\"end\":57409,\"start\":57408},{\"end\":57423,\"start\":57422},{\"end\":57425,\"start\":57424},{\"end\":57434,\"start\":57433},{\"end\":57806,\"start\":57805},{\"end\":57808,\"start\":57807},{\"end\":57818,\"start\":57817},{\"end\":57828,\"start\":57827},{\"end\":58050,\"start\":58049},{\"end\":58052,\"start\":58051},{\"end\":58066,\"start\":58065},{\"end\":58078,\"start\":58077},{\"end\":58088,\"start\":58087},{\"end\":58097,\"start\":58096},{\"end\":58110,\"start\":58109},{\"end\":58120,\"start\":58119},{\"end\":58475,\"start\":58474},{\"end\":58486,\"start\":58485},{\"end\":58496,\"start\":58495},{\"end\":58506,\"start\":58505},{\"end\":58947,\"start\":58946},{\"end\":58949,\"start\":58948},{\"end\":59224,\"start\":59223},{\"end\":59232,\"start\":59231},{\"end\":59240,\"start\":59239},{\"end\":59248,\"start\":59247},{\"end\":59526,\"start\":59525},{\"end\":59532,\"start\":59531},{\"end\":59541,\"start\":59540},{\"end\":59549,\"start\":59548},{\"end\":59754,\"start\":59753},{\"end\":59763,\"start\":59762},{\"end\":59890,\"start\":59889},{\"end\":59899,\"start\":59898},{\"end\":60119,\"start\":60118},{\"end\":60128,\"start\":60127},{\"end\":60135,\"start\":60134},{\"end\":60142,\"start\":60141},{\"end\":60150,\"start\":60149},{\"end\":60159,\"start\":60158},{\"end\":60171,\"start\":60166},{\"end\":60175,\"start\":60174}]", "bib_author_last_name": "[{\"end\":50031,\"start\":50024},{\"end\":50041,\"start\":50035},{\"end\":50322,\"start\":50314},{\"end\":50329,\"start\":50326},{\"end\":50339,\"start\":50333},{\"end\":50601,\"start\":50595},{\"end\":50612,\"start\":50605},{\"end\":50622,\"start\":50616},{\"end\":50633,\"start\":50626},{\"end\":50895,\"start\":50892},{\"end\":50901,\"start\":50899},{\"end\":50910,\"start\":50905},{\"end\":50921,\"start\":50916},{\"end\":50927,\"start\":50925},{\"end\":50935,\"start\":50931},{\"end\":50945,\"start\":50939},{\"end\":51306,\"start\":51302},{\"end\":51314,\"start\":51310},{\"end\":51324,\"start\":51321},{\"end\":51336,\"start\":51328},{\"end\":51345,\"start\":51340},{\"end\":51355,\"start\":51349},{\"end\":51368,\"start\":51361},{\"end\":51668,\"start\":51660},{\"end\":51682,\"start\":51672},{\"end\":51690,\"start\":51686},{\"end\":51699,\"start\":51694},{\"end\":51710,\"start\":51703},{\"end\":51726,\"start\":51716},{\"end\":51734,\"start\":51730},{\"end\":51744,\"start\":51738},{\"end\":51751,\"start\":51748},{\"end\":51757,\"start\":51755},{\"end\":51769,\"start\":51761},{\"end\":51780,\"start\":51775},{\"end\":52536,\"start\":52526},{\"end\":52553,\"start\":52540},{\"end\":52562,\"start\":52557},{\"end\":52568,\"start\":52566},{\"end\":52584,\"start\":52572},{\"end\":52593,\"start\":52588},{\"end\":52606,\"start\":52597},{\"end\":52616,\"start\":52610},{\"end\":52947,\"start\":52941},{\"end\":53119,\"start\":53109},{\"end\":53134,\"start\":53123},{\"end\":53268,\"start\":53266},{\"end\":53276,\"start\":53272},{\"end\":53285,\"start\":53280},{\"end\":53302,\"start\":53289},{\"end\":53312,\"start\":53308},{\"end\":53602,\"start\":53596},{\"end\":53883,\"start\":53875},{\"end\":53897,\"start\":53887},{\"end\":53907,\"start\":53901},{\"end\":53920,\"start\":53911},{\"end\":54268,\"start\":54266},{\"end\":54278,\"start\":54272},{\"end\":54285,\"start\":54282},{\"end\":54295,\"start\":54289},{\"end\":54307,\"start\":54299},{\"end\":54497,\"start\":54494},{\"end\":54503,\"start\":54501},{\"end\":54509,\"start\":54507},{\"end\":54518,\"start\":54513},{\"end\":54528,\"start\":54525},{\"end\":54789,\"start\":54782},{\"end\":55453,\"start\":55445},{\"end\":55463,\"start\":55457},{\"end\":55471,\"start\":55467},{\"end\":55481,\"start\":55478},{\"end\":55951,\"start\":55947},{\"end\":55957,\"start\":55955},{\"end\":55963,\"start\":55961},{\"end\":55970,\"start\":55967},{\"end\":55985,\"start\":55974},{\"end\":55992,\"start\":55989},{\"end\":56003,\"start\":55999},{\"end\":56340,\"start\":56332},{\"end\":56355,\"start\":56344},{\"end\":56364,\"start\":56359},{\"end\":56371,\"start\":56368},{\"end\":56384,\"start\":56375},{\"end\":56692,\"start\":56682},{\"end\":56704,\"start\":56698},{\"end\":56718,\"start\":56708},{\"end\":56731,\"start\":56722},{\"end\":56748,\"start\":56735},{\"end\":57093,\"start\":57087},{\"end\":57109,\"start\":57099},{\"end\":57120,\"start\":57115},{\"end\":57131,\"start\":57124},{\"end\":57404,\"start\":57398},{\"end\":57420,\"start\":57410},{\"end\":57431,\"start\":57426},{\"end\":57442,\"start\":57435},{\"end\":57815,\"start\":57809},{\"end\":57825,\"start\":57819},{\"end\":57834,\"start\":57829},{\"end\":58063,\"start\":58053},{\"end\":58075,\"start\":58067},{\"end\":58085,\"start\":58079},{\"end\":58094,\"start\":58089},{\"end\":58107,\"start\":58098},{\"end\":58117,\"start\":58111},{\"end\":58132,\"start\":58121},{\"end\":58483,\"start\":58476},{\"end\":58493,\"start\":58487},{\"end\":58503,\"start\":58497},{\"end\":58512,\"start\":58507},{\"end\":58958,\"start\":58950},{\"end\":59229,\"start\":59225},{\"end\":59237,\"start\":59233},{\"end\":59245,\"start\":59241},{\"end\":59251,\"start\":59249},{\"end\":59529,\"start\":59527},{\"end\":59538,\"start\":59533},{\"end\":59546,\"start\":59542},{\"end\":59552,\"start\":59550},{\"end\":59760,\"start\":59755},{\"end\":59770,\"start\":59764},{\"end\":59896,\"start\":59891},{\"end\":59905,\"start\":59900},{\"end\":60125,\"start\":60120},{\"end\":60132,\"start\":60129},{\"end\":60139,\"start\":60136},{\"end\":60147,\"start\":60143},{\"end\":60156,\"start\":60151},{\"end\":60164,\"start\":60160}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6103843},\"end\":50239,\"start\":49975},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b1\"},\"end\":50516,\"start\":50241},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1820089},\"end\":50888,\"start\":50518},{\"attributes\":{\"doi\":\"arXiv:1702.07983\",\"id\":\"b3\"},\"end\":51234,\"start\":50890},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b4\"},\"end\":51590,\"start\":51236},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3091656},\"end\":52493,\"start\":51592},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1033682},\"end\":52884,\"start\":52495},{\"attributes\":{\"doi\":\"arXiv:1308.0850\",\"id\":\"b7\"},\"end\":53081,\"start\":52886},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1915014},\"end\":53260,\"start\":53083},{\"attributes\":{\"doi\":\"arXiv:1703.00955\",\"id\":\"b9\"},\"end\":53506,\"start\":53262},{\"attributes\":{\"doi\":\"arXiv:1511.05101\",\"id\":\"b10\"},\"end\":53768,\"start\":53508},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4669377},\"end\":54209,\"start\":53770},{\"attributes\":{\"doi\":\"arXiv:1701.06547\",\"id\":\"b12\"},\"end\":54490,\"start\":54211},{\"attributes\":{\"doi\":\"arXiv:1705.11001\",\"id\":\"b13\"},\"end\":54739,\"start\":54492},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8130314},\"end\":55377,\"start\":54741},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11080756},\"end\":55943,\"start\":55379},{\"attributes\":{\"doi\":\"arXiv:1704.03084\",\"id\":\"b16\"},\"end\":56328,\"start\":55945},{\"attributes\":{\"doi\":\"arXiv:1705.10929\",\"id\":\"b17\"},\"end\":56611,\"start\":56330},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6844431},\"end\":57001,\"start\":56613},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1211821},\"end\":57312,\"start\":57003},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1211821},\"end\":57711,\"start\":57314},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":76564},\"end\":58047,\"start\":57713},{\"attributes\":{\"doi\":\"arXiv:1703.01161\",\"id\":\"b22\"},\"end\":58423,\"start\":58049},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1169492},\"end\":58853,\"start\":58425},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2332513},\"end\":59129,\"start\":58855},{\"attributes\":{\"doi\":\"arXiv:1703.04887\",\"id\":\"b25\"},\"end\":59456,\"start\":59131},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3439214},\"end\":59693,\"start\":59458},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12964363},\"end\":59887,\"start\":59695},{\"attributes\":{\"doi\":\"arXiv:1502.01710\",\"id\":\"b28\"},\"end\":60066,\"start\":59889},{\"attributes\":{\"doi\":\"arXiv:1706.03850\",\"id\":\"b29\"},\"end\":60370,\"start\":60068}]", "bib_title": "[{\"end\":50020,\"start\":49975},{\"end\":50591,\"start\":50518},{\"end\":51656,\"start\":51592},{\"end\":52522,\"start\":52495},{\"end\":53105,\"start\":53083},{\"end\":53869,\"start\":53770},{\"end\":54776,\"start\":54741},{\"end\":55441,\"start\":55379},{\"end\":56678,\"start\":56613},{\"end\":57081,\"start\":57003},{\"end\":57392,\"start\":57314},{\"end\":57803,\"start\":57713},{\"end\":58472,\"start\":58425},{\"end\":58944,\"start\":58855},{\"end\":59523,\"start\":59458},{\"end\":59751,\"start\":59695}]", "bib_author": "[{\"end\":50033,\"start\":50022},{\"end\":50043,\"start\":50033},{\"end\":50324,\"start\":50312},{\"end\":50331,\"start\":50324},{\"end\":50341,\"start\":50331},{\"end\":50603,\"start\":50593},{\"end\":50614,\"start\":50603},{\"end\":50624,\"start\":50614},{\"end\":50635,\"start\":50624},{\"end\":50897,\"start\":50890},{\"end\":50903,\"start\":50897},{\"end\":50912,\"start\":50903},{\"end\":50923,\"start\":50912},{\"end\":50929,\"start\":50923},{\"end\":50937,\"start\":50929},{\"end\":50947,\"start\":50937},{\"end\":51308,\"start\":51300},{\"end\":51316,\"start\":51308},{\"end\":51326,\"start\":51316},{\"end\":51338,\"start\":51326},{\"end\":51347,\"start\":51338},{\"end\":51357,\"start\":51347},{\"end\":51370,\"start\":51357},{\"end\":51670,\"start\":51658},{\"end\":51684,\"start\":51670},{\"end\":51692,\"start\":51684},{\"end\":51701,\"start\":51692},{\"end\":51712,\"start\":51701},{\"end\":51728,\"start\":51712},{\"end\":51736,\"start\":51728},{\"end\":51746,\"start\":51736},{\"end\":51753,\"start\":51746},{\"end\":51759,\"start\":51753},{\"end\":51771,\"start\":51759},{\"end\":51782,\"start\":51771},{\"end\":52538,\"start\":52524},{\"end\":52555,\"start\":52538},{\"end\":52564,\"start\":52555},{\"end\":52570,\"start\":52564},{\"end\":52586,\"start\":52570},{\"end\":52595,\"start\":52586},{\"end\":52608,\"start\":52595},{\"end\":52618,\"start\":52608},{\"end\":52949,\"start\":52939},{\"end\":53121,\"start\":53107},{\"end\":53136,\"start\":53121},{\"end\":53270,\"start\":53264},{\"end\":53278,\"start\":53270},{\"end\":53287,\"start\":53278},{\"end\":53304,\"start\":53287},{\"end\":53314,\"start\":53304},{\"end\":53604,\"start\":53594},{\"end\":53885,\"start\":53871},{\"end\":53899,\"start\":53885},{\"end\":53909,\"start\":53899},{\"end\":53922,\"start\":53909},{\"end\":54270,\"start\":54264},{\"end\":54280,\"start\":54270},{\"end\":54287,\"start\":54280},{\"end\":54297,\"start\":54287},{\"end\":54309,\"start\":54297},{\"end\":54499,\"start\":54492},{\"end\":54505,\"start\":54499},{\"end\":54511,\"start\":54505},{\"end\":54520,\"start\":54511},{\"end\":54530,\"start\":54520},{\"end\":54791,\"start\":54778},{\"end\":55455,\"start\":55443},{\"end\":55465,\"start\":55455},{\"end\":55473,\"start\":55465},{\"end\":55483,\"start\":55473},{\"end\":55953,\"start\":55945},{\"end\":55959,\"start\":55953},{\"end\":55965,\"start\":55959},{\"end\":55972,\"start\":55965},{\"end\":55987,\"start\":55972},{\"end\":55994,\"start\":55987},{\"end\":56005,\"start\":55994},{\"end\":56342,\"start\":56330},{\"end\":56357,\"start\":56342},{\"end\":56366,\"start\":56357},{\"end\":56373,\"start\":56366},{\"end\":56386,\"start\":56373},{\"end\":56694,\"start\":56680},{\"end\":56706,\"start\":56694},{\"end\":56720,\"start\":56706},{\"end\":56733,\"start\":56720},{\"end\":56750,\"start\":56733},{\"end\":57095,\"start\":57083},{\"end\":57111,\"start\":57095},{\"end\":57122,\"start\":57111},{\"end\":57133,\"start\":57122},{\"end\":57406,\"start\":57394},{\"end\":57422,\"start\":57406},{\"end\":57433,\"start\":57422},{\"end\":57444,\"start\":57433},{\"end\":57817,\"start\":57805},{\"end\":57827,\"start\":57817},{\"end\":57836,\"start\":57827},{\"end\":58065,\"start\":58049},{\"end\":58077,\"start\":58065},{\"end\":58087,\"start\":58077},{\"end\":58096,\"start\":58087},{\"end\":58109,\"start\":58096},{\"end\":58119,\"start\":58109},{\"end\":58134,\"start\":58119},{\"end\":58485,\"start\":58474},{\"end\":58495,\"start\":58485},{\"end\":58505,\"start\":58495},{\"end\":58514,\"start\":58505},{\"end\":58960,\"start\":58946},{\"end\":59231,\"start\":59223},{\"end\":59239,\"start\":59231},{\"end\":59247,\"start\":59239},{\"end\":59253,\"start\":59247},{\"end\":59531,\"start\":59525},{\"end\":59540,\"start\":59531},{\"end\":59548,\"start\":59540},{\"end\":59554,\"start\":59548},{\"end\":59762,\"start\":59753},{\"end\":59772,\"start\":59762},{\"end\":59898,\"start\":59889},{\"end\":59907,\"start\":59898},{\"end\":60127,\"start\":60118},{\"end\":60134,\"start\":60127},{\"end\":60141,\"start\":60134},{\"end\":60149,\"start\":60141},{\"end\":60158,\"start\":60149},{\"end\":60166,\"start\":60158},{\"end\":60174,\"start\":60166},{\"end\":60178,\"start\":60174}]", "bib_venue": "[{\"end\":50092,\"start\":50043},{\"end\":50310,\"start\":50241},{\"end\":50684,\"start\":50635},{\"end\":51032,\"start\":50963},{\"end\":51298,\"start\":51236},{\"end\":51857,\"start\":51782},{\"end\":52667,\"start\":52618},{\"end\":52937,\"start\":52886},{\"end\":53154,\"start\":53136},{\"end\":53592,\"start\":53508},{\"end\":53971,\"start\":53922},{\"end\":54262,\"start\":54211},{\"end\":54589,\"start\":54546},{\"end\":54937,\"start\":54791},{\"end\":55566,\"start\":55483},{\"end\":56107,\"start\":56021},{\"end\":56444,\"start\":56402},{\"end\":56786,\"start\":56750},{\"end\":57137,\"start\":57133},{\"end\":57493,\"start\":57444},{\"end\":57859,\"start\":57836},{\"end\":58205,\"start\":58150},{\"end\":58591,\"start\":58514},{\"end\":58976,\"start\":58960},{\"end\":59221,\"start\":59131},{\"end\":59558,\"start\":59554},{\"end\":59777,\"start\":59772},{\"end\":59954,\"start\":59923},{\"end\":60116,\"start\":60068},{\"end\":51919,\"start\":51859},{\"end\":55070,\"start\":54939},{\"end\":55636,\"start\":55568},{\"end\":58655,\"start\":58593}]"}}}, "year": 2023, "month": 12, "day": 17}