{"id": 259076427, "updated": "2023-10-05 00:04:38.256", "metadata": {"title": "OWQ: Lessons learned from activation outliers for weight quantization in large language models", "authors": "[{\"first\":\"Changhun\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Jungyu\",\"last\":\"Jin\",\"middle\":[]},{\"first\":\"Taesu\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Hyungjun\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Eunhyeok\",\"last\":\"Park\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.02272", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-02272", "doi": "10.48550/arxiv.2306.02272"}}, "content": {"source": {"pdf_hash": "343d24c4dcfaff2132373d218561a23fbd53e934", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.02272v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1a5c5bf776e6c1a5645f22e5ad85df4e8ee7f2e3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/343d24c4dcfaff2132373d218561a23fbd53e934.txt", "contents": "\nOWQ: Lessons learned from activation outliers for weight quantization in large language models\n\n\nChanghun Lee changhun.lee@postech.ac.kr \nPohang University of Science and Technology\nPOSTECH\n\nJungyu Jin \nPohang University of Science and Technology\nPOSTECH\n\nTaesu Kim \nSqueezeBits Inc\n\n\nHyungjun Kim hyungjun.kim@squeezebits.com \nEunhyeok Park \nPohang University of Science and Technology\nPOSTECH\n\nSqueezeBits Inc\n\n\nOWQ: Lessons learned from activation outliers for weight quantization in large language models\n\nLarge language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel posttraining quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ. * Equal Contribution.Preprint. Under review.\n\nIntroduction\n\nLarge language models (LLMs) [2,26,28,29,35] demonstrate impressive generation performance on a wide range of complex language tasks solely relying on prompt tuning and few-shot examples without the need for task-specific fine-tuning. This suggests a potential increase in LLM applications in the future. However, the memory and computational demands of LLMs present significant challenges, not just for training but for inference as well. For instance, the GPT3-175B model requires about 350GB of space for storing model parameters. More than 5 A100 GPUs having 80GB of memory are required for inference, demanding hundreds of thousands of dollars to build. This expensive cost hampers the widespread adoption of LLMs.\n\nWeight quantization is an attractive optimization method for large language models (LLMs) as it can significantly reduce system requirements. By storing parameters with low precision through quantization, storage space can be considerably saved. This also leads to various performance benefits, including increased ALU utilization and reduced communication costs by allocating more operations to a single GPU [24]. Furthermore, quantization can address memory bottlenecks that cause performance degradation, particularly in single-batch inference scenarios with low data reuse. Performance improvement is also expected through low-precision acceleration. Advanced studies [10,24] have shown that matrix multiplication with 3-bit weight and fp16 activation exhibits remarkable performance improvements on a single GPU compared to matrix multiplication with fp16 weights and activation using 5 GPUs. Thus, weight quantization of LLMs is crucial as it can offer practical performance enhancements.\n\nHowever, weight quantization of LLMs presents a trade-off, as it can degrade the output quality of the model. Minimizing this quality loss is vital for its widespread adoption, but LLM quantization is challenging due to the need for a fast quantization process considering the large size of LLMs and the requirement to avoid task-specific fine-tuning to preserve zero-shot generative performance. In advance, OPTQ [10] (or known as GPTQ [9]) has introduced layer-wise post-training quantization (PTQ) based on optimal brain compression (OBC) algorithm [8]. This approach allows for the quantization of the largest OPT [35] model in approximately 4 hours, with minimal impact on accuracy, even at 3-bit precision. The model's capacity can be compressed to less than a quarter of the size of the existing fp16 format, making it accessible even with a single A100 GPU. This work demonstrates the potential of extreme weight quantization for LLMs.\n\nIn this paper, we introduce a new weight quantization technique, called Outlier-aware Weight Quantization (OWQ), an advancement over OPTQ that significantly enhances the quality of the quantized model. Previous studies [4,32,34] have identified outliers in certain activation feature dimensions. These outliers, due to their diverse data range, complicate the quantization process of activation. While our work utilizes fp16 for activation, our analysis indicates that activation outlier plays a key role in amplifying quantization-induced errors in weight. OWQ uses a mixed-precision quantization scheme, which applies higher precision to the weights susceptible to quantization caused by activation outliers. Our extensive analysis shows that the 3.01-bit OWQ model shows comparable quality to the 4-bit OPTQ model. To the best of our knowledge, we are the first study to incorporate the existence of activation outlier in extreme weight quantization. Our key contributions can be summarized accordingly:\n\n\u2022 We introduce a novel analysis that reveals how activation outliers can amplify the error in weight quantization, providing valuable insights for improving the quality of quantized weights. \u2022 Building upon this analysis, we propose a new weight quantization algorithm called OWQ.\n\nOWQ not only preserves the benefits of low-precision but also significantly reduces the quantization error. \u2022 Through extensive analysis, we demonstrate that OWQ achieves comparable performance with 3.01-bit to that of OPTQ with 4-bit across diverse LLM tasks without incurring notable performance overhead.\n\n\nBackground and Related Works\n\n\nQuantization and LLMs\n\nQuantization is a widely-used optimization technique aimed at exploiting the benefit of low-precision while maintaining the quality of the target network. While the primary advantage is the reduction in storage space, it is worth noting that quantization also provides substantial performance improvement through the support of low-precision acceleration. The main drawback of quantization is the degradation of output quality. Mitigating quality degradation is highly important, and early studies focused on quantization-aware training (QAT) [7,12,36], which tried to restore quality through additional training. However, as the understanding of quantization grew and various techniques emerged, post-training quantization (PTQ) [15,20,19,31] have been actively studied, enabling quality preservation without training.\n\nDue to the LLM's necessity of significant storage space and computational resources, it is crucial to apply optimization via quantization. Numerous studies have been conducted, but the distinctive characteristics of LLMs steer the research in a unique direction. In general, QAT has been favored for extremely low-bit precision to minimize quantization error. However, it is less favorable for LLM quantization because of the high cost of the training environment. Moreover, QAT might not be the optimal choice to preserve the task-agnostic generation performance of LLMs.\n\nInstead, PTQ has emerged as an important topic for LLM quantization. This field has two distinct approaches: one aims to quantize both activations and weights to int8 [4,34], considering both capacity reduction and performance enhancement. In contrast, the second approach focuses solely on weight quantization to sub-4-bit precision [10,24]. In this paper, we align our work with the latter approach. While concentrating on weight quantization, we devise a novel quantization scheme, drawing significant inspiration from int8-related research on activation quantization.\n\n\nWeight Input\n\n\nActivation Outliers\n\n\nHessian\n\n= \u22c5 \u22c5 =\n\n\nOutlier-aware Weight Quantization\n\nWeak Columns in fp16\nOutput = \u22c5 \u22c5 Weak Columns\nColumn Index Low-precision Weight + Sensitive Channels Figure 1: Overview of the proposed idea, outlier-aware weight quantization.\n\n\nInt8 Quantization for Activation and Weight\n\nInt8 multiplication can provide up to 2x performance improvements and more than 5x energy consumption reduction compared to fp16 baselines [13]. Numerous studies [4,34] aim to quantize both activation and weight to int8 for matrix multiplication operations in LLMs. However, those studies identify a unique challenge of LLMs for activation quantization. LLMs exhibit a few outliers in intermediate activations, with values significantly larger than other activations, and these outliers are concentrated in specific feature dimensions. Preserving the values of these outliers is known to be crucial for maintaining accuracy after activation quantization.\n\nIn this study, while activation remains at full-precision and only weight quantization is applied, we figure out that the presence of activation outliers still impacts the sensitivity of weight quantization. We also demonstrate that considering activation outliers is essential for accurate weight quantization.\n\n\nOPTQ: Weight Quantization for LLMs\n\nOPTQ [10] represents the cutting-edge research in the field of weight quantization for LLMs. It is based on Optimal Brain Compression (OBC) [8], which employs element-wise quantization (pruning) and compensation, using a Hessian-based metric of layer-wise quantization errors (Eq. (1)). This approach differs from previous studies, which applied quantization through gradient descent based on the straight-through estimator [15,31] or a rounding-to-nearest mechanism [4].\nw q = argmin wq (quant(w q ) \u2212 w q ) 2 [H \u22121 F ] qq , \u03b4 F = \u2212 w q \u2212 quant(w q ) [H \u22121 F ] qq \u00b7 (H \u22121 F ) :,q(1)\nOPTQ has optimized OBC to apply quantization in parallel for each element of the input dimension, facilitating rapid quantization and demonstrating remarkable performance even at 3-bit. However, if the model size decreases or the problem's complexity increases, the accuracy still falls compared to the fp16 baselines. In this paper, we suggest selectively applying high-precision to weights that are vulnerable to quantization caused by activation outliers, while applying OPTQ to the remaining weights. These enhancements can significantly reduce the quantization error while preserving the quantization performance of OPTQ.\n\n\nProblem Definition and Motivation\n\nIn this section, prior to introducing our idea, we first aim to clearly define the problem and explain how our findings can address it. This paper aims to apply the layer-wise linear quantization for the weights of LLMs with minimal quality degradation. When an input feature X \u2208 R Cin\u00d7N is given, where C in represents the number of input channels and N is the sequence length of the input, the full-precision weight matrix W \u2208 R Cout\u00d7Cin for C out output features is iteratively updated to minimize the difference between the output activations before and after quantization. The objective function to find the quantized weight\u0174 that minimizes the squared error is defined as follows:\narg min W E = arg min W ||W X \u2212\u0174 X|| 2 2 s.t. C(\u0174 ) < C t ,(2)\nwhere C(\u00b7) represents the compression ratio and C t is the target compression ratio. Solving the optimization problem represented by Eq. (2) is known to be a challenging task, as it falls under the category of NP-hard problems [10,19]. In our work, we leverage the OPTQ algorithm for weight quantization. However, it is worth noting that other linear quantization algorithms can also be applied to address this problem. The layer-wise quantization process is applied sequentially from the model input to the output, ensuring the comprehensive quantization of all weights in the model.\n\n\nLayer-wise Linear Quantization and Hessian of Weights\n\nIn this subsection, we drive the relationship between the sensitivity of weights and the activation outliers in terms of quantization. Similar to the approach used in OBC, we reorganize the squared error term in Eq. (2) as the sum of squared errors for each output channel in the weight matrix, and the equation is modified as \u03a3 Cout i=1 ||W i,: X \u2212\u0174 i,: X|| 2 2 . Through this decomposition, we can clearly observe that the overall error is separated into individual errors for each output channel.\n\nWith the modified equation, our focus shifts to two key aspects. Firstly, it is important to note that there is no Hessian interaction between output channels. Specifically, the individual Hessians with respect to the layer-wise quantization error, denoted as H i \u2208 R Cin\u00d7Cin , have an identical value as:\nH i = \u2202 2 E \u2202W 2 i,: = 2XX T .(3)\nSecondly, as observed in previous studies [19], the individual error term can be approximated using Taylor expansion. By setting \u2206W i,: = W i,: \u2212\u0174 i,: , the error for the i-th output channel can be expressed as follows:\nE i = ||W i,: X \u2212\u0174 i,: X|| 2 2 \u2248 \u2206W i,: H i \u2206W T i,: .(4)\nFor the detailed proof, please refer to the supplementary materials. The use of the Hessian as a metric to measure sensitivity in quantization has been widely adopted in various quantization approaches [5,6]. As these studies have pointed out, particularly in the context of layer-wise quantization, the output error can be directly related to the Hessian of the weights.\n\nKeeping these two observations in mind, we can derive a surprising insight by acknowledging the presence of activation outliers in LLMs. Previous studies [4,34] have reported that certain feature dimensions of LLM activation contain outliers with significantly larger values than others. These activation outliers cause some elements of H i to exhibit exceptionally high values, as illustrated in Figure 1. This abnormal surge in Hessian values increases the channels' sensitivity to quantization. As indicated in Eq. (4), even when the same degree of weight perturbation is present, the ensuing change in output can be considerably larger due to some large elements of H i . Therefore, if an activation outlier exists, a perturbation in the connected weight will result in a larger output error. We refer to the weights that are susceptible to quantization, specifically those associated with the activation outliers in a specific input channel, as a weak column (shown in Figure 1).\n\nHence, if we quantize all weights to the same bit-width during the weight quantization process, the quantization error at the weak columns corresponding to the activation outliers can lead to substantial perturbation on the output, resulting in a significant quantization error. To minimize this quality degradation, it is crucial to apply special handling for those weak columns.\n\n\nOWQ: Outlier-aware Weight Quantization\n\nTo address this problem, we propose a novel idea called Outlier-aware Weight Quantization (OWQ). The algorithm is designed on top of OPTQ, but it includes additional pipelines that selectively handle weak columns with high-precision, minimizing quality degradation. In the previous section, we highlighted the relationship between the Hessian matrix and sensitivity caused by activation outliers in Eq. (3). We also demonstrated that the final error is influenced by the quadratic terms of perturbations with the Hessian matrix in Eq. (4). In addition, a previous study [5] has suggested using the product of the trace of the Hessian matrix and the Frobenius norm of the perturbation to approximate the sensitivity of quantization. Building on these insights, we define the sensitivity of j-th column as follows:\nsensitivity j = \u03bb j ||\u2206W :,j || 2 2 ,(5)\nwhere \u03bb j is the j-th diagonal element of the Hessian matrix. In this work, we extend the study [5] that allocates layer-wise bit-width based on each layer's sensitivity: the sensitivity is measured at a more granular level, focusing on the columns of the weight matrix. By analyzing the sensitivity of individual columns, we can effectively identify the weak columns that are vulnerable to quantization and require higher precision. When the goal is to select a specific number (k) of weak columns, the proposed metric is utilized to choose the top-k columns based on their highest sensitivity values.\n\nMoreover, as depicted in Figure 2, it's important to note that our approach is distinctly different from existing outlier-aware quantization studies [22,23]. In those studies, outlier weights are excluded from quantization based solely on their magnitude, with the motivation to minimize the error of weights before and after the quantization. However, our method focuses on not only minimizing the quantization error for the weights but also that of the output activation. Therefore, we utilize a Hessian metric related to the output of matrix multiplication, and the weights are selected based on their sensitivity, rather than its magnitude, as shown in the figure.\n\nAfter identifying the weak columns, we store those vectors with high precision. In practice, we store a complete low-precision matrix with zero-filled weak columns. Additionally, we store the weak columns as fp16 (16-bit floating-point) and use an extra single 16-bit integer per column, which represents the column index of the weak columns. If we denote the matrix multiplication operation by \u00d7, the output of the matrix multiplication can be calculated as the sum of (zero-filled quantized weight matrix \u00d7 fp16 activation matrix) and (fp16 weak columns \u00d7 corresponding fp16 activation channels). The additional storage overhead is solely caused by the weak columns. This overhead is negligible (\u2248 0.3%, demonstrated in Sec. 5), while the accuracy is significantly improved. In addition, the remaining weights are quantized using OPTQ, but we introduce a key advancement to minimize the quantization error even further, as explained in the following subsection.\n\n\nQuantization Hyperparameter Tuning\n\nFollowing the selection of weak columns, the remaining weights are quantized using the OPTQ method. Since OPTQ also employs sequential column-wise quantization, the exclusion of weak columns can be seamlessly integrated into the OPTQ framework.\n\nHowever, we have made a modification to the existing OPTQ, which applied min-max quantization.\n\nIn our experiments, we tuned the quantization hyperparameters, such as step size and zero point, to narrow the quantization range than the maximum and minimum value range of the weight. Previous studies reported that truncation of data led to a significant reduction in quantization error by balancing truncation and rounding errors [7,15,21,31], but performance degradation was observed in OPTQ, hence they use a naive min-max range. However, in our experiments, after removing the weak columns, the introduction of truncation significantly helped in reducing the quantization error and producing reliable output. According to our empirical observations, the weak columns in the key and query layers of transformer blocks have exceptionally large values. Truncating these values led to a very large error, resulting in a significant loss of accuracy. However, in our method, this error does not occur as the weak column are maintained with full-precision.\n\nIn our experiments, we employ a simple quantization method based on rounding to the nearest with truncation to adjust the quantization parameters. The optimal values of the parameters are searched greedily to minimize the difference between the weights before and after quantization. After searching for the best quantization parameters, we apply OPTQ on top of the searched values.\n\n\nExperiments\n\n\nExperimental Setup\n\nTo validate the outstanding performance of our proposed method, we present quantization results for large-scale LLMs such as OPT [35], LLaMA [29], and BLOOM [28] (Section C.1) families. Our primary baseline is OPTQ, so we apply identical experimental settings of it. For instance, our calibration dataset consists of 128 random 2048 token segments from the C4 dataset [27]. All experiments were conducted on a single NVIDIA A100 GPU with 80GB of main memory. Like OPTQ, our method quantizes the target model without re-training. The reported results are obtained in a zero-shot environment. To measure the zero-shot performance, we utilize an open-source evaluation repository, EleutherAI/lm-evaluation-harness [11]. Please note that we report the numbers with an error margin based on 5 experiments with different seeds.\n\nCompared to the baselines of 3-bit and 4-bit OPTQ results, we present two variants, with an extra 0.01 bit and 0.1 bit overhead, respectively. The additional storage area of extra bit is evenly distributed across the linear layers within the transformer block. For instance, in the OPT model which has six linear layers (key, query, value, out, fc1, and fc2), the weak columns of the key layer contribute an average of 0.00167 bit in the 3.01-bit configuration (0.125% columns of the weight matrix). This extra bit covers the all overhead of the mixed-precision representation. If we quantize OPT-175B model with an average of 3.01 bits, it will require approximately 220 MB of additional storage compared to the 3-bit OPT-175B OPTQ model, which utilizes around 65.6 GB of storage space. All experiments were conducted using the PyTorch 2.0 [25] framework with HuggingFace integration [33], and the source code is available at https://github.com/xvyaward/owq.\n\n\nResults of Perplexity Measure\n\nThe accuracy of the proposed model is assessed through the evaluation on multiple language tasks, including WikiText-2 [17], Penn Treebank (PTB) [16], and C4. Perplexity-based tasks are particularly sensitive to model quantization [10], with perplexity numbers serving as indicators of the generative performance of the quantized model. The results for WikiText-2 can be found in Table 1 and Table 2, while the results for PTB and C4 are provided in the supplementary material.   The results on the tables clearly demonstrate that OWQ consistently delivers substantial quality improvements across the LLM families, irrespective of the model size. The 3.01-bit model effectively mitigates the quality degradation observed in the 3-bit OPTQ model, while the 3.1-bit model achieves comparable performance to the 4-bit OPTQ model. Furthermore, OWQ 4.01-bit yields noteworthy improvements, highlighting the significance of treating weak columns. These results underscore the importance and effectiveness of our approach in preserving model quality after quantization.\n\nAn interesting finding is the significant improvement in model quality for models with less than 13 billion parameters when applying OWQ. Although previous studies have highlighted the presence of activation outliers in models with more than 6.7 billion parameters, even smaller models with moderately large channels can still benefit from mixed precision quantization. This suggests that the concept of weak columns remains valid and effective in enhancing the quality of LLMs, regardless of the model size.\n\n\nResults of Various Zero-shot Tasks\n\nWe conducted additional experiments on diverse zero-shot language tasks. The results are available at Table 3 and Table 4. Our approach shows significant performance improvements across all tested tasks. Even our models using 3.01 bits clearly outperform the 4-bit RTN, and exhibit results on par with the 4-bit of OPTQ models. Our method's strength lies in its universal applicability, consistently boosting the performance of generative models with minimal storage overhead. The Pareto-front plot in Figure 3 clearly illustrates the advantages of OWQ compared to OPTQ.   \n\n\nAcceleration on Real Device\n\nTo show the benefits of low-precision acceleration, we created a customized CUDA kernel for OWQ and measured its latency overhead on an A100 GPU. Notably, we set weak columns in the lowprecision matrix to zero, resulting in the low-precision process having the same overhead as OPTQ's customized kernel. Additionally, we integrated a high-precision operation for weak columns. The activation input channels for these weak columns are selected on-the-fly, and a dense high-precision GeMV kernel is used. In the 3.1-bit model, the mixed process computation increases latency by only 3.79 % compared to the 3-bit acceleration of the OPTQ kernel on OPT-175B model. Considering the benefits gained, the computational overhead is negligible.\n\n\nEffect of the Extra Bit Size\n\nTo analyze the impact of the number of weak columns, we measured the perplexity of the OPT-6.7B model while varying the extra bit ratio (Table 5). While our main focus was on reporting performance using 0.01 and 0.1 extra bits, we observed significant performance improvements even with fewer weak columns. With just 3.005 bits, the model already surpasses the performance of the OPTQ 3-bit model. However, as the bit width increases, the performance gains gradually diminish, reaching a saturation point. It is important to note that activation outliers are limited to only a few dimensions, resulting in a small number of corresponding weak columns. Consequently, even a small number of weak columns can lead to noticeable performance improvements.\n\n\nQuantization time cost\n\nThe speed of the quantization algorithm is of utmost importance for LLM quantization. OWQ, compared to OPTQ, introduces extra operations for selecting weak columns and tuning quantization hyperparameters. However, by sharing certain expensive items like Hessian with OPTQ, the specific overhead of OWQ can be minimized. Table 6 presents the quantization time over various LLMs. OWQ can still successfully quantize a 66B-scale model in under 3 hours, showcasing its practicality for real-world applications. \n\n\nLayer-wise Quantization Sensitivity\n\nIn our experiments, we uniformly allocated extra storage space to linear layers, but observed varying sensitivity of weak columns across layers. In the OPT-6.7B model, we assessed layer-by-layer sensitivity by applying OWQ to select layers within a fixed capacity budget. Our results in Table 7 indicate the best accuracy is achieved when weak columns are removed simultaneously from both  key and query weights. Performance suffers significantly if either layer isn't addressed, especially for queries alone. OWQ's hyperparameter tuning stage is more sensitive to weak columns in key and query layers with higher magnitudes than other layers. Improvements could be made by allocating space at different rates according to each layer's sensitivity. However, the rate tuning is prohibitively expensive, making implementing this approach challenging. Developing a method to tune layer-wise weak column numbers is a potential future research direction.\n\n\nDiscussions\n\n\nFine-grained Quantization\n\nApplying linear quantization at fine-grained granularity significantly reduces quantization error while introducing some storage overhead for quantization hyperparameters. OPTQ utilizes this approach by dividing row vectors into groups (e.g., group size of 128 or 1024) and applying linear quantization independently with different configurations. This expansion can be applied orthogonally to OWQ, so we can combine row-wise fine-grained quantization with OWQ to assess any additional improvements. Results in Table 8 show that the room for improvement from fine-grained quantization is negligible, as OWQ already substantially enhances the 3-bit model's quality. Moreover, compared to grouped OPTQ with 128 group size, 3.01-bit OWQ's storage overhead is only 8% of grouped OPTQ overhead while achieving better perplexity and zero-shot accuracy. Thus, OWQ is a superior solution to the grouping technique.\n\n\nComparison with Act-Ordering of OPTQ\n\nWhile not discussed in OPTQ papers, the \"act-order\" option was recently added to their official GitHub [14]. This option, which quantizes based on activation magnitude, differs from the previous OPTQ approach that applied quantization to columns sequentially. Generally, \"act-order\" produces superior results. Although no theoretical interpretation exists for this method, our study suggests that the benefits of \"act-order\" arise from sensitivity-aware quantization. This means that applying sequential quantization beginning with sensitive columns improves performance, even within the OPTQ method. However, act-order alone cannot sufficiently mitigate the quality degradation caused by weak columns within a low-precision domain. OWQ, by addressing the weak columns with high-precision, consistently outperforms OPTQ + act order, regardless of act-order option.\n\n\nConclusion\n\nThe presence of activation outliers has been identified as a significant challenge in LLM activation quantization. We found that even in weight quantization, activation outliers can increase the sensitivity of certain weight columns, leading to significant quality degradation in a low-precision domain. To overcome this, we introduced a novel quantization scheme, OWQ. Compared to existing 3-bit quantization methods, OWQ achieves substantial quality improvements with only negligible storage and computation overhead, effectively preserving the benefits of low-precision acceleration. We believe that our theoretical insights will expand the understanding of weight quantization, inspiring future research and promoting the widespread adoption of LLMs.\n\n\nA Main Proofs 1 Proof of Eq. (5) in the Main Manuscript\n\nOur goal is to find the quantized weight\u0174 that minimizes the objective function:\narg min W E = arg min W ||W X \u2212\u0174 X|| 2 2 s.t. C(\u0174 ) < C t .(6)\nwe can reorganize the squared error term in Eq. (6) as the sum of squared errors corresponding to each output channel in the weight matrix:\nE = \u03a3 Cout i=1 ||W i,: X \u2212\u0174 i,: X|| 2 2 = \u03a3 Cout i=1 E i .(7)\nFrom this decomposition, we can see that the overall error can be separated into the sum of the errors from each output channel. Therefore, our goal of minimizing overall error E can be thought of as minimizing the error of each output channel E i . The error that occurs when quantizing a network can be approximated by a Taylor series expansion for the model weights as follows:\nE i = \u2202E i \u2202W i,: \u2206W T i,: + 1 2 \u2206W i,: H i \u2206W T i,: ,(8)\nwhere \u2206W i,: = W i,: \u2212\u0174 i,: is the perturbation of the i'th row of the weight and\nH i = \u2202 2 E i /\u2202W 2 i,:\nis the Hessian matrix containing the second order derivatives for all weights in the weight row W i,: . Since E i is a quadratic function, all terms above the third order become zero. For a network trained with a local minimum of error, the first-order (gradient) term can be ignored:\nE i \u2248 \u2206W i,: H i \u2206W T i,: .(9)\nTherefore, the quantization error for each output channel can be approximated as Eq. (9).\n\n\nB Experiment Details\n\nThe implementation of OWQ is based on OPTQ (GPTQ) official GitHub [14].\n\n\nEvaluation Settings\n\nWe measured language modeling perplexity on datasets from WikiText-2 [17], PTB [16], and C4 [27]. The validation set is concatenated using two newlines as separators in WikiText-2 and a space as a separator in the PTB and C4 and then the concatenated data is tokenized using the default HuggingFace [33] tokenizer for each model.\n\n\nScore Issues on LLaMA Model Family\n\nIn Section C, the PTB perplexity of the LLaMA [29] model is poor. This is a problem of a fullprecision model, not due to quantization. Another issue is that the accuracy we obtained in our zeroshot experiments using EleutherAI/lm-evaluation-harness [11] is slightly lower than that published in the LLaMA paper. We expect the accuracy to be different due to differences in the experimental environment or special tokens. Since all experiments were conducted in the same environment, we can compare the results regardless of these issues.\n\n\nEffective Bit-Width\n\nIn Section 4 of the main manuscript, we described that we store a complete low-precision matrix with zero-filled weak columns and additional fp16 weak columns (latency-favored method). Another possible option is storing the reduced size low-precision matrix and fp16 weak columns to keep the total number of weight elements the same and to avoid storing unnecessary zeros corresponding to weak columns (storage-favored method). The latter method has trade-offs: it can save more storage, but it has more overhead in the actual operation.\n\nWe calculated the effective bit-width using the storage-favored method for the accuracy results in the main manuscript and the supplementary materials. However, the memory overhead is similar for both methods as there are few weak columns.\n\nIf we add the storage overhead of the zero-filled low-precision matrix to our 3.01-bit case, it becomes 3.012-bit, which is negligible overhead.\n\n\nC Additional Results\n\nIn this section, the results with * mark in the tables used 3.05-bit. Otherwise, there are few or no weak columns in the budget of 3.01-bit due to the small model dimension. Similar to the OPTQ, the calibration data were sampled from the C4 training set, so measuring perplexity on C4 is not a fully zero-shot situation.\n\n\nBLOOM Model Results\n\nWe additionally checked the effectiveness of the proposed OWQ on BLOOM [28] model families. Table 9, Table 10, and Table 11 show perplexity results on BLOOM models. For the 176B case, we used a single seed.   Tables in this section (Table 12,13,14, and 15) show additional language generation task results.     Tables in this section (Table 16, 17, 18, 19, and 20) show additional results for several zero-shot benchmarks: PIQA [1], ARC-easy and ARC-challange [3], OpenBookQA [18].     \n\n\nLanguage Generation Results\n\nWe analyzed the linguistic abilities of the quantized model through generated outputs by prompting the model. We experimented with full-precision (FP16) LLaMA 7B, 30B model, 3-bit quantized 30B model with OPTQ, and 3.01-bit quantized 30B model using OWQ. Fig. 4 shows the input prompts and the corresponding outputs for each model. The n / 10 notation at the upper-right of each output box indicates the number of correct answers out of 10 total attempts. Each output example in Fig. 4 has been randomly selected among 10 attempts. In our experiments, we used some prompt examples from the Chain-of-Thought Prompting [30]. Fig. 4 shows that the 30B OWQ 3.01-bit model has comparable linguistic abilities to the full-precision 30B model, while having a similar storage capacity to the full-precision 7B model. Figure 4: The generation results of each model using the prompts for the math word problem (left) and commonsense problem (CSQA, right) tasks. We omitted the newlines between the answer choices for the CSQA prompt in this figure.\n\n\nTrue Sequential and Activation Order\n\nAs mentioned in Section 6 of the main manuscript, the \"act-order\" option and \"true-sequential\" option were recently added to the official Github of OPTQ [14]. The \"true-sequential\" option allows sequential quantization even within a transformer block, while the \"act-order\" determines the quantization order based on activation magnitude. These options are beneficial for OPTQ in general, especially for LLaMA models (Table 21, Table 22). However, there is no significant performance improvement for OWQ with these options.  \n\nFigure 2 :\n2Min-max range of the weights in each column (blue dots) and the selected weak columns among those (red dots).\n\nFigure 3 :\n3OPT family perplexity (left) and zero-shot accuracy (right).\n\nTable 1 :\n1OPT WikiText-2 perplexity (lower is better). For the results with *, we used 3.05 bits; there are few or no weak columns in the budget of 3.01 bits due to the small model dimension.33.46\u00b1.35 26.01\u00b1.17 15.35\u00b1.17 12.92\u00b1.12 11.17\u00b1.04 10.38\u00b1.04 9.58\u00b1.05 9.31\u00b1.04OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n27.65 \n22.00 \n14.63 \n12.47 \n10.86 \n10.13 \n9.56 \n9.34 \nRTN \n4 \n37.28 \n25.94 \n48.17 \n16.92 \n12.10 \n11.32 \n10.98 \n110 \nOPTQ 4 \n31.43 \n24.21 \n15.56 \n12.82 \n11.41 \n10.34 \n9.57 \n9.55 \nOWQ 4.01 29.55\u00b1.23 23.05\u00b1.12 14.97\u00b1.06 12.36\u00b1.02 10.89\u00b1.02 10.27\u00b1.01 9.52\u00b1.00 9.23\u00b1.03 \nOPTQ 3 \n53.05 \n34.45 \n21.17 \n16.83 \n15.09 \n11.73 \n10.3 \n14.42 \nOWQ 3.01 35.19  *  \n\u00b1.46 26.54  *  \n\u00b1.27 16.37\u00b1.07 13.19\u00b1.08 11.22\u00b1.06 11.51\u00b1.05 9.58\u00b1.01 9.32\u00b1.03 \nOWQ 3.1 \n\nTable 2 :\n2LLaMA WikiText-2 perplexityLLaMA Bits \n7B \n13B \n30B \n65B \nfull \n16 \n5.68 \n5.09 \n4.10 \n3.53 \nRTN \n4 \n6.29 \n5.53 \n4.54 \n3.92 \nOPTQ \n4 \n6.84 \n5.34 \n4.45 \n3.87 \nOWQ \n4.01 5.94\u00b1.02 5.25\u00b1.00 4.24\u00b1.01 3.73\u00b1.00 \nOPTQ \n3 \n18.07 \n6.76 \n5.84 \n5.15 \nOWQ \n3.01 6.65\u00b1.02 5.65\u00b1.03 4.76\u00b1.02 4.22\u00b1.01 \nOWQ \n3.1 \n6.39\u00b1.02 5.57\u00b1.02 4.63\u00b1.01 4.08\u00b1.01 \n\n\nTable 3 :\n3OPT lambada_openai zero-shot accuracy (%) (higher is better). For the results with *, we used 3.05 bits; there are few or no weak columns in the budget of 3.01 bits (small model dimension).32.66\u00b1.89 43.41\u00b11.39 57.93\u00b1.98 64.54\u00b1.78 69.42\u00b1.37 70.84\u00b1.30 73.61\u00b1.30 76.26\u00b1.20OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n37.84 \n45.18 \n58.01 \n63.57 \n67.63 \n68.64 \n71.45 \n73.98 \nRTN \n4 \n22.87 \n23.89 \n29.52 \n31.23 \n34.56 \n35.75 \n38.05 \n8.40 \nOPTQ 4 \n33.38 \n45.41 \n54.81 \n61.74 \n65.28 \n67.60 \n71.11 \n73.75 \nOWQ 4.01 35.29\u00b1.26 45.59\u00b1.53 57.86\u00b1.58 65.32\u00b1.32 68.47\u00b1.19 69.34\u00b1.11 72.18\u00b1.02 74.88\u00b1.08 \nOPTQ 3 \n12.87 \n36.53 \n36.81 \n48.28 \n50.78 \n61.51 \n67.46 \n59.61 \nOWQ 3.01 29.78  *  \n\u00b11.15 45.00  *  \n\u00b1.97 53.94\u00b1.88 64.36\u00b1.56 69.94\u00b1.45 69.13\u00b1.41 73.58\u00b1.18 76.76\u00b1.21 \nOWQ 3.1 \n\nTable 4 :\n4LLaMA winogrande (left) and ARC-challenge (right) zero-shot accuracy (%).LLaMA Bits \nwinogrande \nARC-challenge \n7B \n13B \n30B \n65B \n7B \n13B \n30B \n65B \nfull \n16 \n67.09 \n70.09 \n72.61 \n77.43 \n41.38 \n44.62 \n45.39 \n46.33 \nRTN \n4 \n67.17 \n68.67 \n72.38 \n75.14 \n39.59 \n42.32 \n46.08 \n47.01 \nOPTQ \n4 \n63.00 \n69.42 \n72.53 \n75.55 \n38.52 \n43.45 \n45.54 \n46.80 \nOWQ 4.01 66.03\u00b1.18 69.27\u00b1.20 74.03\u00b1.59 76.24\u00b1.36 39.65\u00b1.05 43.32\u00b1.22 44.65\u00b1.82 46.25\u00b1.09 \nOPTQ \n3 \n51.52 \n66.41 \n69.01 \n72.22 \n25.72 \n39.27 \n40.70 \n43.05 \nOWQ 3.01 64.88\u00b1.88 68.68\u00b1.75 72.26\u00b1.26 73.05\u00b1.93 36.64\u00b1.93 40.73\u00b1.56 45.17\u00b1.50 43.80\u00b1.23 \nOWQ \n3.1 65.87\u00b1.71 69.55\u00b1.37 72.93\u00b1.58 75.17\u00b1.52 39.66\u00b1.58 41.55\u00b1.34 44.39\u00b1.63 44.68\u00b1.59 \n\n\nTable 5 :\n5Effective bit-width sweep using OPT-6.7B model.Effective bit-width 3.005 3.01 3.05 3.10 3.20 3.30 \nWikiText2 \n11.38 11.22 11.19 11.17 11.15 11.13 \nPTB \n16.61 16.32 16.29 16.30 16.28 16.27 \nC4 \n13.35 13.23 13.19 13.18 13.16 13.16 \n\n\n\nTable 6 :\n6Algorithm running time for quantization of OPT (left) and LLaMA (right) models.OPT \n6.7B \n13B \n30B \n66B \nOPTQ time 11.2m 20.1m 43.9m 1.6h \nOWQ time 16.4m 30.8m 68.5m 2.4h \n\nLLaMA \n7B \n13B \n30B \n65B \nOPTQ time \n9.0m \n15.8m 35.9m 1.1h \nOWQ time 14.4m 26.1m 66.3m 2.0h \n\n\n\nTable 7 :\n7Layer-wise sensitivity analysis on OPT-6.7B model.layers \nW k , Wq \nW k , Wv \nW k , W f c1 \nWq, Wv \nWv, W f c1 \nWikiText-2 \n11.26 \n19.87 \n22.25 \n111.57 \n89.92 \nlayers \nW k , Wq, Wv W k , Wq, W f c1 W k , Wq, Wout Wq, Wv, Wout \nWikiText-2 \n11.31 \n11.24 \n11.26 \n111.81 \n\n\nTable 8 :\n8The results of OWQ and OPTQ with fine-grained quantizationMethod / Bits / \nGroup \nOPT-6.7B \nLLaMA-7B \nsize Wiki2 PTB \nC4 lambada \u2191 Wiki2 PTB \nC4 winog. \u2191 arc_c. \u2191 \nOPTQ / 3 \n15.09 22.49 17.29 \n50.78 \n18.07 260.36 21.77 \n51.52 \n25.72 \nOPTQ / 3 \n/ g128 \n11.28 16.59 13.32 \n64.47 \n8.95 111.35 11.02 \n57.95 \n32.63 \nOWQ / 3.01 \n11.22 16.32 13.23 \n69.94 \n6.65 55.60 8.62 \n64.88 \n36.64 \nOWQ / 3.01 / g1024 \n11.18 16.32 13.19 \n68.91 \n6.59 54.48 8.50 \n65.19 \n36.76 \nOWQ / 3.01 / g128 \n11.16 16.23 13.10 \n68.61 \n6.41 50.30 8.19 \n66.16 \n38.14 \n\n\n\nTable 9 :\n9BLOOM WikiText-2 perplexity (lower is better).LLaMA Bits \n560M \n1.1B \n1.7B \n3B \n7.1B \n176B \n\nfull \n16 \n22.41 \n17.68 \n15.39 \n13.48 \n11.37 \n8.11 \n\nRTN \n4 \n25.89 \n19.99 \n16.97 \n14.75 \n12.10 \n8.37 \nOPTQ \n4 \n24.06 \n18.92 \n16.35 \n14.11 \n11.75 \n8.19 \nOWQ \n4.01 23.25\u00b1.06 18.20\u00b1.04 15.70\u00b1.01 13.72\u00b1.01 11.50\u00b1.01 \n8.14 \n\nOPTQ \n3 \n32.51 \n25.07 \n21.33 \n17.53 \n13.72 \n8.63 \nOWQ \n3.01 24.70  *  \n\n\u00b1.08 \n\n19.56\u00b1.02 16.70\u00b1.05 14.40\u00b1.05 11.92\u00b1.02 \n8.26 \nOWQ \n3.1 \n24.53\u00b1.06 19.35\u00b1.07 16.52\u00b1.05 14.34\u00b1.02 11.85\u00b1.02 \n8.25 \n\n\n\nTable 10 :\n10BLOOM Penn Treebank (PTB) perplexity.LLaMA Bits \n560M \n1.1B \n1.7B \n3B \n7.1B \n176B \n\nfull \n16 \n43.67 \n57.98 \n29.99 \n25.33 \n20.82 \n14.59 \n\nRTN \n4 \n51.07 \n66.83 \n33.56 \n27.67 \n22.41 \n15.00 \nOPTQ \n4 \n47.20 \n63.25 \n32.17 \n26.50 \n21.69 \n14.71 \nOWQ \n4.01 45.68\u00b1.29 60.04\u00b1.20 30.79\u00b1.07 25.84\u00b1.07 21.15\u00b1.04 14.61 \n\nOPTQ \n3 \n67.08 \n87.81 \n44.91 \n34.85 \n26.49 \n15.47 \nOWQ \n3.01 48.38  *  \n\n\u00b1.25 \n\n67.01\u00b1.46 33.64\u00b1.21 27.84\u00b1.12 22.11\u00b1.04 14.88 \nOWQ \n3.1 \n47.81\u00b1.33 64.52\u00b1.59 32.43\u00b1.21 27.36\u00b1.12 21.86\u00b1.04 14.88 \n\n\nTable 11 :\n11BLOOM C4 perplexity.LLaMA Bits \n560M \n1.1B \n1.7B \n3B \n7.1B \n176B \nfull \n16 \n26.59 \n22.05 \n19.49 \n17.48 \n15.20 \n11.71 \nRTN \n4 \n29.88 \n24.43 \n21.25 \n18.76 \n16.05 \n12.04 \nOPTQ \n4 \n28.02 \n23.20 \n20.56 \n18.07 \n15.59 \n11.79 \nOWQ \n4.01 28.11\u00b1.23 22.51\u00b1.01 19.83\u00b1.01 17.72\u00b1.00 15.35\u00b1.01 11.73 \nOPTQ \n3 \n35.68 \n28.78 \n25.20 \n21.22 \n17.60 \n12.24 \nOWQ \n3.01 28.93  *  \n\n\u00b1.03 \n\n23.83\u00b1.01 20.81\u00b1.01 18.42\u00b1.01 15.82\u00b1.00 11.85 \nOWQ \n3.1 \n28.87\u00b1.03 23.60\u00b1.02 20.61\u00b1.01 18.31\u00b1.01 15.75\u00b1.00 11.84 \n\n2 Additional Perplexity Results \n\n\n\nTable 12 :\n12OPT Penn Treebank (PTB) perplexity. 32\u00b1.04 17.00\u00b1.05 14.39\u00b1.04 13.66\u00b1.03 OWQ 3.1 46.98\u00b1.95 37.26\u00b1.43 22.27\u00b1.13 19.35\u00b1.07 16.30\u00b1.04 14.98\u00b1.03 14.33\u00b1.05 13.62\u00b1.02OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n38.99 \n31.08 \n20.29 \n17.97 \n15.77 \n14.52 \n14.04 \n13.36 \nRTN \n4 \n53.89 \n36.79 \n57.30 \n31.05 \n18.84 \n16.51 \n15.40 \n225.66 \nOPTQ 4 \n45.76 \n34.26 \n22.06 \n19.13 \n16.54 \n14.86 \n14.22 \n13.77 \nOWQ 4.01 41.89\u00b1.61 32.47\u00b1.22 21.35\u00b1.16 18.45\u00b1.05 15.79\u00b1.04 14.77\u00b1.02 14.14\u00b1.01 13.43\u00b1.01 \nOPTQ 3 \n74.60 \n48.28 \n31.36 \n25.09 \n22.49 \n16.55 \n15.37 \n28.25 \nOWQ 3.01 49.53  *  \n\u00b1.77 37.68  *  \n\u00b1.30 23.80\u00b1.15 19.85\u00b1.07 16.\n\nTable 13 :\n13LLaMA Penn Treebank (PTB) perplexity.LLaMA Bits \n7B \n13B \n30B \n65B \nfull \n16 \n41.15 \n28.10 \n23.51 \n25.04 \nRTN \n4 \n48.66 \n29.45 \n25.49 \n27.97 \nOPTQ \n4 \n58.57 \n30.46 \n24.86 \n27.35 \nOWQ \n4.01 \n44.30\u00b1.26 \n29.43\u00b1.24 23.90\u00b1.13 23.62\u00b11.91 \nOPTQ \n3 \n260.36 \n47.49 \n29.99 \n29.64 \nOWQ \n3.01 55.60\u00b11.52 31.39\u00b1.40 24.75\u00b1.16 \n24.21\u00b1.20 \nOWQ \n3.1 \n53.82\u00b11.81 30.73\u00b1.17 24.46\u00b1.18 \n24.45\u00b1.31 \n\n\n\nTable 14 :\n14OPT C4 perplexity. OWQ 4.01 27.92\u00b1.01 23.36\u00b1.02 16.49\u00b1.01 14.60\u00b1.01 12.83\u00b1.00 12.17\u00b1.01 11.49\u00b1.00 11.02\u00b1.00 OWQ 3.01 31.30 * \u00b1.05 26.32 * \u00b1.10 17.69\u00b1.02 15.35\u00b1.01 13.23\u00b1.01 13.31\u00b1.02 11.69\u00b1.00 11.16\u00b1.00 OWQ 3.1 30.03\u00b1.05 25.94\u00b1.06 17.13\u00b1.01 15.16\u00b1.01 13.18\u00b1.01 12.42\u00b1.00 11.68\u00b1.01 11.16\u00b1.00OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n26.56 \n22.59 \n16.07 \n14.34 \n12.71 \n12.06 \n11.44 \n10.99 \nRTN \n4 \n33.91 \n26.21 \n24.51 \n18.43 \n14.36 \n13.36 \n13.46 \n309. \nOPTQ 4 \n29.32 \n24.68 \n16.96 \n14.97 \n13.17 \n12.25 \n11.57 \n11.22 \nOPTQ 3 \n42.11 \n31.50 \n21.53 \n18.10 \n17.29 \n13.33 \n12.22 \n14.07 \n\n\nTable 15 :\n15LLaMA C4 perplexity.LLaMA Bits \n7B \n13B \n30B \n65B \nfull \n16 \n7.34 \n6.80 \n6.13 \n5.81 \nRTN \n4 \n8.12 \n7.23 \n6.54 \n6.07 \nOPTQ \n4 \n8.77 \n7.09 \n6.41 \n6.02 \nOWQ \n4.01 7.66\u00b1.01 6.97\u00b1.00 6.25\u00b1.00 5.95\u00b1.00 \nOPTQ \n3 \n21.77 \n8.91 \n7.94 \n7.16 \nOWQ \n3.01 8.62\u00b1.04 7.44\u00b1.01 6.73\u00b1.01 6.28\u00b1.01 \nOWQ \n3.1 \n8.15\u00b1.02 7.31\u00b1.01 6.57\u00b1.00 6.16\u00b1.00 \n\n\nTable 16 :\n16OPT PIQA zero-shot accuracy (%) (higher is better). OWQ 4.01 61.83\u00b1.15 64.94\u00b1.21 71.52\u00b1.60 74.34\u00b1.17 76.67\u00b1.14 76.50\u00b1.11 78.48\u00b1.34 79.54\u00b1.14 70.47\u00b1.25 73.60\u00b1.44 75.85\u00b1.45 75.40\u00b1.16 78.43\u00b1.46 79.28\u00b1.26 OWQ 3.1 60.53\u00b1.27 63.42\u00b1.58 70.83\u00b1.28 73.69\u00b1.32 76.22\u00b1.22 76.04\u00b1.08 78.20\u00b1.31 79.28\u00b1.12OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n62.02 \n64.69 \n72.47 \n74.76 \n76.55 \n76.77 \n78.13 \n79.87 \nRTN \n4 \n61.53 \n63.44 \n67.57 \n73.67 \n76.39 \n76.12 \n77.26 \n60.12 \nOPTQ 4 \n61.59 \n64.18 \n70.84 \n73.93 \n76.25 \n76.63 \n78.54 \n78.91 \nOPTQ 3 \n58.78 \n62.46 \n68.43 \n70.62 \n73.05 \n74.70 \n77.76 \n70.63 \nOWQ 3.01 60.51  *  \n\u00b1.20 63.33  *  \n\u00b1.24 \n\nTable 17 :\n17OPT ARC-easy zero-shot accuracy (%).OWQ  4.01 38.17\u00b1.37 39.35\u00b1.29 49.28\u00b1.21 54.11\u00b1.28 59.56\u00b1.33 61.85\u00b1.37 64.76\u00b1.16 66.82\u00b1.13 OWQ 3.01 36.70 * \u00b1.67 37.68 * \u00b1.46 47.14\u00b1.38 52.06\u00b1.50 57.68\u00b1.30 60.03\u00b1.50 63.34\u00b1.53 66.21\u00b1.18 OWQ 3.1 37.26\u00b1.19 37.92\u00b1.50 49.25\u00b1.23 52.70\u00b1.18 58.01\u00b1.81 60.91\u00b1.49 63.70\u00b1.39 66.30\u00b1.25OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n39.86 \n40.45 \n50.97 \n54.29 \n60.06 \n61.83 \n65.32 \n67.26 \nRTN \n4 \n36.24 \n38.59 \n42.72 \n52.90 \n58.67 \n61.32 \n61.20 \n40.49 \nOPTQ 4 \n38.40 \n38.65 \n49.61 \n52.54 \n58.95 \n61.16 \n64.59 \n64.91 \nOPTQ 3 \n36.33 \n36.70 \n44.85 \n48.43 \n53.75 \n56.56 \n61.34 \n49.32 \n\n\nTable 18 :\n18LLaMA PIQA (left) and ARC-easy (right) zero-shot accuracy (%). OWQ 4.01 77.08\u00b1.15 78.59\u00b1.07 80.58\u00b1.19 80.12\u00b1.22 50.76\u00b1.60 58.50\u00b1.41 58.54\u00b1.48 57.60\u00b1.66 OWQ 3.01 75.71\u00b1.40 77.63\u00b1.47 79.61\u00b1.33 78.66\u00b1.42 47.42\u00b1.72 54.18\u00b11.01 57.62\u00b11.27 53.38\u00b1.60 OWQ 3.1 76.65\u00b1.20 78.16\u00b1.45 79.43\u00b1.30 80.10\u00b1.56 49.82\u00b11.01 56.48\u00b1.74 56.88\u00b1.86 56.71\u00b11.15LLaMA Bits \nPIQA \nARC-easy \n7B \n13B \n30B \n65B \n7B \n13B \n30B \n65B \nfull \n16 \n77.37 \n79.11 \n80.14 \n80.85 \n52.48 \n59.85 \n58.92 \n58.80 \nRTN \n4 \n76.17 \n77.91 \n79.27 \n80.41 \n52.23 \n57.07 \n60.61 \n58.46 \nOPTQ \n4 \n75.36 \n78.12 \n79.68 \n80.83 \n51.73 \n57.99 \n58.23 \n58.92 \nOPTQ \n3 \n59.78 \n75.24 \n77.23 \n78.69 \n33.90 \n53.32 \n51.36 \n54.95 \n\n\nTable 19 :\n19OPT ARC-challenge zero-shot accuracy (%). OWQ 4.01 22.39\u00b1.43 24.10\u00b1.61 28.79\u00b1.42 30.41\u00b1.34 34.04\u00b1.53 35.38\u00b1.44 37.65\u00b1.61 39.87\u00b1.40 OWQ 3.01 22.56 * \u00b1.65 23.60 * \u00b1.34 28.07\u00b1.32 29.64\u00b1.73 32.41\u00b1.37 35.32\u00b1.60 36.76\u00b1.46 39.13\u00b1.65 OWQ 3.1 23.02\u00b1.13 23.70\u00b1.51 28.38\u00b1.73 29.20\u00b1.62 33.19\u00b1.58 34.74\u00b1.54 36.66\u00b1.22 39.16\u00b1.59OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n22.87 \n23.89 \n29.52 \n31.23 \n34.56 \n35.75 \n38.05 \n40.02 \nRTN \n4 \n22.35 \n23.72 \n24.91 \n29.10 \n32.85 \n35.32 \n35.58 \n22.87 \nOPTQ 4 \n22.59 \n23.84 \n29.04 \n29.61 \n33.12 \n35.31 \n36.96 \n37.58 \nOPTQ 3 \n22.10 \n23.23 \n26.52 \n28.41 \n30.73 \n32.94 \n35.36 \n29.98 \n\n\nTable 20 :\n20OPT OpenBookQA zero-shot accuracy (%). OWQ 4.01 28.64\u00b1.55 27.48\u00b1.87 32.28\u00b1.30 35.24\u00b1.17 37.84\u00b1.26 38.84\u00b1.74 40.32\u00b1.66 41.00\u00b11.4 OWQ 3.01 27.68 * \u00b1.88 26.72 * \u00b1.76 30.68\u00b11.27 32.92\u00b1.58 37.00\u00b1.71 37.84\u00b11.02 38.44\u00b1.46 40.40\u00b1.24 OWQ 3.1 27.88\u00b11.22 27.20\u00b1.79 32.52\u00b1.36 34.24\u00b1.78 37.68\u00b1.95 37.92\u00b1.30 38.80\u00b1.37 40.32\u00b1.30OPT Bits \n125M \n350M \n1.3B \n2.7B \n6.7B \n13B \n30B \n66B \nfull \n16 \n28.0 \n28.2 \n33.0 \n35.2 \n37.4 \n39.0 \n40.2 \n41.0 \nRTN \n4 \n27.0 \n27.8 \n29.6 \n36.4 \n37.8 \n36.6 \n38.2 \n27.8 \nOPTQ 4 \n26.40 \n27.32 \n33.56 \n35.08 \n36.72 \n37.92 \n39.52 \n41.04 \nOPTQ 3 \n27.64 \n26.48 \n29.08 \n32.36 \n35.92 \n36.24 \n38.68 \n31.08 \n\n\nTable 21 :\n21Perplexity results with true-sequential (TS) and act-order (AO) options on OPT models.OPT \nTasks \nOPT-6.7b \nOPT-30b \n-\nTS \nAO \nTS+AO \n-\nTS \nAO \nTS+AO \n\nOPTQ \n3-bit \n\nWikiText2 15.09 15.35 12.98 \n12.83 \n10.30 10.34 10.28 \n10.32 \nPTB \n22.49 22.64 19.26 \n19.37 \n15.37 15.34 15.13 \n15.11 \nC4 \n17.29 17.57 14.58 \n14.55 \n12.22 12.21 12.14 \n12.14 \nOWQ \n3.01 \nbit \n\nWikiText2 11.22 11.23 11.23 \n11.24 \n9.58 \n9.62 \n9.64 \n9.59 \nPTB \n16.32 16.33 16.41 \n16.35 \n14.39 14.39 14.41 \n14.41 \nC4 \n13.23 13.23 13.22 \n13.22 \n11.69 11.69 11.69 \n11.69 \n\n\n\nTable 22 :\n22Perplexity results with true-sequential (TS) and act-order (AO) options on LLaMA models.LLaMA \nTasks \nLLaMA 7B \nLLaMA 30B \n-\nTS \nAO \nTS+AO \n-\nTS \nAO \nTS+AO \n\nOPTQ \n3-bit \n\nWikiText2 \n18.07 \n18.11 \n8.18 \n8.09 \n5.84 \n5.77 \n5.72 \n5.66 \nPTB \n260.36 199.19 73.89 \n70.47 \n29.99 30.02 29.41 \n28.45 \nC4 \n21.77 \n20.77 \n10.35 \n10.27 \n7.94 \n7.88 \n7.89 \n7.81 \nOWQ \n3.01 \nbit \n\nWikiText2 \n6.65 \n6.65 \n6.56 \n6.52 \n4.76 \n4.77 \n4.71 \n4.70 \nPTB \n55.60 \n57.49 \n52.03 \n55.26 \n24.75 24.76 24.55 \n24.59 \nC4 \n8.62 \n8.62 \n8.38 \n8.39 \n6.73 \n6.73 \n6.69 \n6.70 \n\n\nReasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical common- sense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439, 2020.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.05457arXiv preprintPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nHawq-v2: Hessian aware trace-weighted quantization of neural networks. Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Advances in neural information processing systems. 33Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq- v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems, 33:18518-18529, 2020.\n\nHawq: Hessian aware quantization of neural networks with mixed-precision. Zhen Dong, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293-302, 2019.\n\nK Steven, Jeffrey L Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, arXiv:1902.08153Learned step size quantization. arXiv preprintSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n\nOptimal brain compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Dan Alistarh, Advances in Neural Information Processing Systems. Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. In Advances in Neural Information Processing Systems, 2022.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nOptq: Accurate quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, The Eleventh International Conference on Learning Representations. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023.\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy ZouLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\n\nParent-mediated communicationfocused treatment in children with autism (pact): a randomised controlled trial. The lancet. Jonathan Green, Tony Charman, Helen Mcconachie, Catherine Aldred, Vicky Slonims, Pat Howlin, Ann Le Couteur, Kathy Leadbitter, Kristelle Hudry, Sarah Byford, 375Jonathan Green, Tony Charman, Helen McConachie, Catherine Aldred, Vicky Slonims, Pat Howlin, Ann Le Couteur, Kathy Leadbitter, Kristelle Hudry, Sarah Byford, et al. Parent-mediated communication- focused treatment in children with autism (pact): a randomised controlled trial. The lancet, 375(9732):2152- 2160, 2010.\n\n1.1 computing's energy problem (and what we can do about it). Mark Horowitz, 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC). IEEEMark Horowitz. 1.1 computing's energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10-14. IEEE, 2014.\n\n. Ist-Daslab, 2022IST-DASLab. gptq. https://github.com/IST-DASLab/gptq, 2022.\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, International Conference on Learning Representations. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2021.\n\nThe penn treebank: Annotating predicate argument structure. Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, Human Language Technology: Proceedings of a Workshop. Plainsboro, New JerseyMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.02789arXiv preprintTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLRMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.\n\nData-free quantization through weight equalization and bias correction. Markus Nagel, Mart Van Baalen, Tijmen Blankevoort, Max Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325-1334, 2019.\n\nLoss aware post-training quantization. Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, Avi Mendelson, Machine Learning. 110Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and Avi Mendelson. Loss aware post-training quantization. Machine Learning, 110(11-12):3245-3262, 2021.\n\nEnergy-efficient neural network accelerator based on outlier-aware low-precision computation. Eunhyeok Park, Dongyoung Kim, Sungjoo Yoo, 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEEEunhyeok Park, Dongyoung Kim, and Sungjoo Yoo. Energy-efficient neural network accelerator based on outlier-aware low-precision computation. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), pages 688-698. IEEE, 2018.\n\nValue-aware quantization for training and inference of neural networks. Eunhyeok Park, Sungjoo Yoo, Peter Vajda, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Eunhyeok Park, Sungjoo Yoo, and Peter Vajda. Value-aware quantization for training and inference of neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 580-595, 2018.\n\nLut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models, 2023.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 32Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Matthias Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nQdrop: Randomly dropping quantization for extremely low-bit post-training quantization. Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, Fengwei Yu, International Conference on Learning Representations. Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. In International Conference on Learning Representations, 2022.\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, Advances in Neural Information Processing Systems. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. In Advances in Neural Information Processing Systems, 2022.\n\nHuggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, arXiv:1910.03771arXiv preprintThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.10438arXiv preprintGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou, arXiv:1606.06160Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n", "annotations": {"author": "[{\"end\":191,\"start\":98},{\"end\":256,\"start\":192},{\"end\":285,\"start\":257},{\"end\":328,\"start\":286},{\"end\":414,\"start\":329}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":107},{\"end\":202,\"start\":199},{\"end\":266,\"start\":263},{\"end\":298,\"start\":295},{\"end\":342,\"start\":338}]", "author_first_name": "[{\"end\":106,\"start\":98},{\"end\":198,\"start\":192},{\"end\":262,\"start\":257},{\"end\":294,\"start\":286},{\"end\":337,\"start\":329}]", "author_affiliation": "[{\"end\":190,\"start\":139},{\"end\":255,\"start\":204},{\"end\":284,\"start\":268},{\"end\":395,\"start\":344},{\"end\":413,\"start\":397}]", "title": "[{\"end\":95,\"start\":1},{\"end\":509,\"start\":415}]", "venue": null, "abstract": "[{\"end\":1553,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1601,\"start\":1598},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1604,\"start\":1601},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1607,\"start\":1604},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1610,\"start\":1607},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1613,\"start\":1610},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2703,\"start\":2699},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2966,\"start\":2962},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2969,\"start\":2966},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3704,\"start\":3700},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3726,\"start\":3723},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3841,\"start\":3838},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3908,\"start\":3904},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4453,\"start\":4450},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4456,\"start\":4453},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4459,\"start\":4456},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6431,\"start\":6428},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6434,\"start\":6431},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6437,\"start\":6434},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6619,\"start\":6615},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6622,\"start\":6619},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6625,\"start\":6622},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6628,\"start\":6625},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7450,\"start\":7447},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7453,\"start\":7450},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7618,\"start\":7614},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7621,\"start\":7618},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8313,\"start\":8309},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8335,\"start\":8332},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8338,\"start\":8335},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9185,\"start\":9181},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9319,\"start\":9316},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9604,\"start\":9600},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9607,\"start\":9604},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9646,\"start\":9643},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11405,\"start\":11401},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11408,\"start\":11405},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12703,\"start\":12699},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13140,\"start\":13137},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13142,\"start\":13140},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13465,\"start\":13462},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13468,\"start\":13465},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15290,\"start\":15287},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15670,\"start\":15667},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16328,\"start\":16324},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16331,\"start\":16328},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18525,\"start\":18522},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18528,\"start\":18525},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18531,\"start\":18528},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18534,\"start\":18531},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19699,\"start\":19695},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19711,\"start\":19707},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19727,\"start\":19723},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19938,\"start\":19934},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20281,\"start\":20277},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21234,\"start\":21230},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21278,\"start\":21274},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21505,\"start\":21501},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21531,\"start\":21527},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21617,\"start\":21613},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27737,\"start\":27733},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30714,\"start\":30710},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30812,\"start\":30808},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30822,\"start\":30818},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30835,\"start\":30831},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31042,\"start\":31038},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31157,\"start\":31153},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31360,\"start\":31356},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33036,\"start\":33032},{\"end\":33325,\"start\":33306},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33392,\"start\":33389},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33424,\"start\":33421},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33441,\"start\":33437},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34100,\"start\":34096},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34715,\"start\":34711},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35476,\"start\":35474},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36602,\"start\":36600}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35206,\"start\":35084},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35280,\"start\":35207},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36053,\"start\":35281},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36398,\"start\":36054},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37193,\"start\":36399},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37886,\"start\":37194},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38130,\"start\":37887},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38411,\"start\":38131},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38692,\"start\":38412},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":39239,\"start\":38693},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39758,\"start\":39240},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":40273,\"start\":39759},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40803,\"start\":40274},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":41446,\"start\":40804},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":41839,\"start\":41447},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":42456,\"start\":41840},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":42796,\"start\":42457},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":43453,\"start\":42797},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":44089,\"start\":43454},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":44762,\"start\":44090},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":45403,\"start\":44763},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":46028,\"start\":45404},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":46575,\"start\":46029},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":47125,\"start\":46576}]", "paragraph": "[{\"end\":2288,\"start\":1569},{\"end\":3284,\"start\":2290},{\"end\":4229,\"start\":3286},{\"end\":5237,\"start\":4231},{\"end\":5519,\"start\":5239},{\"end\":5828,\"start\":5521},{\"end\":6704,\"start\":5885},{\"end\":7278,\"start\":6706},{\"end\":7851,\"start\":7280},{\"end\":7907,\"start\":7900},{\"end\":7965,\"start\":7945},{\"end\":8122,\"start\":7992},{\"end\":8824,\"start\":8170},{\"end\":9137,\"start\":8826},{\"end\":9647,\"start\":9176},{\"end\":10386,\"start\":9760},{\"end\":11110,\"start\":10424},{\"end\":11758,\"start\":11174},{\"end\":12315,\"start\":11816},{\"end\":12622,\"start\":12317},{\"end\":12876,\"start\":12657},{\"end\":13306,\"start\":12935},{\"end\":14292,\"start\":13308},{\"end\":14674,\"start\":14294},{\"end\":15529,\"start\":14717},{\"end\":16173,\"start\":15571},{\"end\":16843,\"start\":16175},{\"end\":17808,\"start\":16845},{\"end\":18091,\"start\":17847},{\"end\":18187,\"start\":18093},{\"end\":19145,\"start\":18189},{\"end\":19529,\"start\":19147},{\"end\":20387,\"start\":19566},{\"end\":21348,\"start\":20389},{\"end\":22444,\"start\":21382},{\"end\":22954,\"start\":22446},{\"end\":23566,\"start\":22993},{\"end\":24333,\"start\":23598},{\"end\":25116,\"start\":24366},{\"end\":25650,\"start\":25143},{\"end\":26639,\"start\":25690},{\"end\":27589,\"start\":26683},{\"end\":28494,\"start\":27630},{\"end\":29263,\"start\":28509},{\"end\":29403,\"start\":29323},{\"end\":29606,\"start\":29467},{\"end\":30049,\"start\":29669},{\"end\":30189,\"start\":30108},{\"end\":30498,\"start\":30214},{\"end\":30619,\"start\":30530},{\"end\":30715,\"start\":30644},{\"end\":31068,\"start\":30739},{\"end\":31644,\"start\":31107},{\"end\":32205,\"start\":31668},{\"end\":32446,\"start\":32207},{\"end\":32592,\"start\":32448},{\"end\":32937,\"start\":32617},{\"end\":33447,\"start\":32961},{\"end\":34517,\"start\":33479},{\"end\":35083,\"start\":34558}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7991,\"start\":7966},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9759,\"start\":9648},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11173,\"start\":11111},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12656,\"start\":12623},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12934,\"start\":12877},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15570,\"start\":15530},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29466,\"start\":29404},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29668,\"start\":29607},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30107,\"start\":30050},{\"attributes\":{\"id\":\"formula_9\"},\"end\":30213,\"start\":30190},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30529,\"start\":30499}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21781,\"start\":21762},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23114,\"start\":23095},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24510,\"start\":24502},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25470,\"start\":25463},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25984,\"start\":25977},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27201,\"start\":27194},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":33060,\"start\":33053},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33070,\"start\":33062},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33084,\"start\":33076},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33202,\"start\":33193},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33305,\"start\":33272},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34994,\"start\":34975}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1567,\"start\":1555},{\"attributes\":{\"n\":\"2\"},\"end\":5859,\"start\":5831},{\"attributes\":{\"n\":\"2.1\"},\"end\":5883,\"start\":5862},{\"end\":7866,\"start\":7854},{\"end\":7888,\"start\":7869},{\"end\":7898,\"start\":7891},{\"end\":7943,\"start\":7910},{\"attributes\":{\"n\":\"2.2\"},\"end\":8168,\"start\":8125},{\"attributes\":{\"n\":\"2.3\"},\"end\":9174,\"start\":9140},{\"attributes\":{\"n\":\"3\"},\"end\":10422,\"start\":10389},{\"attributes\":{\"n\":\"3.1\"},\"end\":11814,\"start\":11761},{\"attributes\":{\"n\":\"4\"},\"end\":14715,\"start\":14677},{\"attributes\":{\"n\":\"4.1\"},\"end\":17845,\"start\":17811},{\"attributes\":{\"n\":\"5\"},\"end\":19543,\"start\":19532},{\"attributes\":{\"n\":\"5.1\"},\"end\":19564,\"start\":19546},{\"attributes\":{\"n\":\"5.2\"},\"end\":21380,\"start\":21351},{\"attributes\":{\"n\":\"5.3\"},\"end\":22991,\"start\":22957},{\"attributes\":{\"n\":\"5.4\"},\"end\":23596,\"start\":23569},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":24364,\"start\":24336},{\"attributes\":{\"n\":\"5.5\"},\"end\":25141,\"start\":25119},{\"attributes\":{\"n\":\"5.6\"},\"end\":25688,\"start\":25653},{\"attributes\":{\"n\":\"6\"},\"end\":26653,\"start\":26642},{\"attributes\":{\"n\":\"6.1\"},\"end\":26681,\"start\":26656},{\"attributes\":{\"n\":\"6.2\"},\"end\":27628,\"start\":27592},{\"attributes\":{\"n\":\"7\"},\"end\":28507,\"start\":28497},{\"end\":29321,\"start\":29266},{\"end\":30642,\"start\":30622},{\"attributes\":{\"n\":\"1\"},\"end\":30737,\"start\":30718},{\"attributes\":{\"n\":\"2\"},\"end\":31105,\"start\":31071},{\"attributes\":{\"n\":\"3\"},\"end\":31666,\"start\":31647},{\"end\":32615,\"start\":32595},{\"attributes\":{\"n\":\"1\"},\"end\":32959,\"start\":32940},{\"attributes\":{\"n\":\"4\"},\"end\":33477,\"start\":33450},{\"attributes\":{\"n\":\"5\"},\"end\":34556,\"start\":34520},{\"end\":35095,\"start\":35085},{\"end\":35218,\"start\":35208},{\"end\":35291,\"start\":35282},{\"end\":36064,\"start\":36055},{\"end\":36409,\"start\":36400},{\"end\":37204,\"start\":37195},{\"end\":37897,\"start\":37888},{\"end\":38141,\"start\":38132},{\"end\":38422,\"start\":38413},{\"end\":38703,\"start\":38694},{\"end\":39250,\"start\":39241},{\"end\":39770,\"start\":39760},{\"end\":40285,\"start\":40275},{\"end\":40815,\"start\":40805},{\"end\":41458,\"start\":41448},{\"end\":41851,\"start\":41841},{\"end\":42468,\"start\":42458},{\"end\":42808,\"start\":42798},{\"end\":43465,\"start\":43455},{\"end\":44101,\"start\":44091},{\"end\":44774,\"start\":44764},{\"end\":45415,\"start\":45405},{\"end\":46040,\"start\":46030},{\"end\":46587,\"start\":46577}]", "table": "[{\"end\":36053,\"start\":35551},{\"end\":36398,\"start\":36093},{\"end\":37193,\"start\":36680},{\"end\":37886,\"start\":37279},{\"end\":38130,\"start\":37946},{\"end\":38411,\"start\":38222},{\"end\":38692,\"start\":38474},{\"end\":39239,\"start\":38763},{\"end\":39758,\"start\":39298},{\"end\":40273,\"start\":39810},{\"end\":40803,\"start\":40308},{\"end\":41446,\"start\":40978},{\"end\":41839,\"start\":41498},{\"end\":42456,\"start\":42144},{\"end\":42796,\"start\":42491},{\"end\":43453,\"start\":43099},{\"end\":44089,\"start\":43776},{\"end\":44762,\"start\":44436},{\"end\":45403,\"start\":45090},{\"end\":46028,\"start\":45731},{\"end\":46575,\"start\":46129},{\"end\":47125,\"start\":46678}]", "figure_caption": "[{\"end\":35206,\"start\":35097},{\"end\":35280,\"start\":35220},{\"end\":35551,\"start\":35293},{\"end\":36093,\"start\":36066},{\"end\":36680,\"start\":36411},{\"end\":37279,\"start\":37206},{\"end\":37946,\"start\":37899},{\"end\":38222,\"start\":38143},{\"end\":38474,\"start\":38424},{\"end\":38763,\"start\":38705},{\"end\":39298,\"start\":39252},{\"end\":39810,\"start\":39773},{\"end\":40308,\"start\":40288},{\"end\":40978,\"start\":40818},{\"end\":41498,\"start\":41461},{\"end\":42144,\"start\":41854},{\"end\":42491,\"start\":42471},{\"end\":43099,\"start\":42811},{\"end\":43776,\"start\":43468},{\"end\":44436,\"start\":44104},{\"end\":45090,\"start\":44777},{\"end\":45731,\"start\":45418},{\"end\":46129,\"start\":46043},{\"end\":46678,\"start\":46590}]", "figure_ref": "[{\"end\":8055,\"start\":8047},{\"end\":13713,\"start\":13705},{\"end\":14290,\"start\":14282},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16208,\"start\":16200},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23503,\"start\":23495},{\"end\":33740,\"start\":33734},{\"end\":33964,\"start\":33958},{\"end\":34108,\"start\":34102},{\"end\":34296,\"start\":34288}]", "bib_author_first_name": "[{\"end\":47192,\"start\":47185},{\"end\":47204,\"start\":47199},{\"end\":47222,\"start\":47214},{\"end\":47233,\"start\":47228},{\"end\":47622,\"start\":47619},{\"end\":47638,\"start\":47630},{\"end\":47649,\"start\":47645},{\"end\":47664,\"start\":47657},{\"end\":47679,\"start\":47674},{\"end\":47681,\"start\":47680},{\"end\":47698,\"start\":47690},{\"end\":47715,\"start\":47709},{\"end\":47735,\"start\":47729},{\"end\":47749,\"start\":47743},{\"end\":47764,\"start\":47758},{\"end\":48183,\"start\":48178},{\"end\":48196,\"start\":48191},{\"end\":48209,\"start\":48205},{\"end\":48225,\"start\":48219},{\"end\":48238,\"start\":48232},{\"end\":48257,\"start\":48250},{\"end\":48275,\"start\":48269},{\"end\":48549,\"start\":48546},{\"end\":48564,\"start\":48560},{\"end\":48578,\"start\":48572},{\"end\":48592,\"start\":48588},{\"end\":48945,\"start\":48941},{\"end\":48958,\"start\":48952},{\"end\":48971,\"start\":48964},{\"end\":48984,\"start\":48980},{\"end\":48995,\"start\":48994},{\"end\":49009,\"start\":49005},{\"end\":49395,\"start\":49391},{\"end\":49408,\"start\":49402},{\"end\":49418,\"start\":49414},{\"end\":49429,\"start\":49428},{\"end\":49443,\"start\":49439},{\"end\":49838,\"start\":49837},{\"end\":49854,\"start\":49847},{\"end\":49856,\"start\":49855},{\"end\":49871,\"start\":49864},{\"end\":49895,\"start\":49883},{\"end\":49917,\"start\":49905},{\"end\":50269,\"start\":50264},{\"end\":50282,\"start\":50279},{\"end\":50617,\"start\":50612},{\"end\":50634,\"start\":50627},{\"end\":50654,\"start\":50651},{\"end\":50967,\"start\":50962},{\"end\":50984,\"start\":50977},{\"end\":51004,\"start\":51001},{\"end\":51358,\"start\":51355},{\"end\":51372,\"start\":51364},{\"end\":51384,\"start\":51378},{\"end\":51398,\"start\":51395},{\"end\":51413,\"start\":51406},{\"end\":51429,\"start\":51422},{\"end\":51446,\"start\":51438},{\"end\":51463,\"start\":51456},{\"end\":51473,\"start\":51469},{\"end\":52028,\"start\":52020},{\"end\":52040,\"start\":52036},{\"end\":52055,\"start\":52050},{\"end\":52077,\"start\":52068},{\"end\":52091,\"start\":52086},{\"end\":52104,\"start\":52101},{\"end\":52116,\"start\":52113},{\"end\":52119,\"start\":52117},{\"end\":52134,\"start\":52129},{\"end\":52156,\"start\":52147},{\"end\":52169,\"start\":52164},{\"end\":52565,\"start\":52561},{\"end\":53035,\"start\":53029},{\"end\":53046,\"start\":53040},{\"end\":53055,\"start\":53053},{\"end\":53065,\"start\":53061},{\"end\":53076,\"start\":53072},{\"end\":53083,\"start\":53081},{\"end\":53098,\"start\":53091},{\"end\":53106,\"start\":53103},{\"end\":53116,\"start\":53113},{\"end\":53480,\"start\":53475},{\"end\":53494,\"start\":53489},{\"end\":53504,\"start\":53500},{\"end\":53508,\"start\":53505},{\"end\":53530,\"start\":53524},{\"end\":53545,\"start\":53542},{\"end\":53556,\"start\":53552},{\"end\":53572,\"start\":53567},{\"end\":53585,\"start\":53579},{\"end\":54016,\"start\":54009},{\"end\":54032,\"start\":54025},{\"end\":54045,\"start\":54040},{\"end\":54063,\"start\":54056},{\"end\":54336,\"start\":54331},{\"end\":54352,\"start\":54347},{\"end\":54366,\"start\":54360},{\"end\":54379,\"start\":54373},{\"end\":54682,\"start\":54676},{\"end\":54694,\"start\":54690},{\"end\":54698,\"start\":54695},{\"end\":54710,\"start\":54706},{\"end\":54731,\"start\":54723},{\"end\":54747,\"start\":54741},{\"end\":55119,\"start\":55113},{\"end\":55131,\"start\":55127},{\"end\":55150,\"start\":55144},{\"end\":55167,\"start\":55164},{\"end\":55589,\"start\":55585},{\"end\":55604,\"start\":55599},{\"end\":55618,\"start\":55613},{\"end\":55634,\"start\":55627},{\"end\":55654,\"start\":55651},{\"end\":55667,\"start\":55663},{\"end\":55669,\"start\":55668},{\"end\":55684,\"start\":55681},{\"end\":56020,\"start\":56012},{\"end\":56036,\"start\":56027},{\"end\":56049,\"start\":56042},{\"end\":56477,\"start\":56469},{\"end\":56491,\"start\":56484},{\"end\":56502,\"start\":56497},{\"end\":56961,\"start\":56956},{\"end\":56976,\"start\":56968},{\"end\":56989,\"start\":56983},{\"end\":57002,\"start\":56995},{\"end\":57017,\"start\":57008},{\"end\":57031,\"start\":57023},{\"end\":57040,\"start\":57038},{\"end\":57062,\"start\":57052},{\"end\":57076,\"start\":57068},{\"end\":57089,\"start\":57082},{\"end\":57442,\"start\":57438},{\"end\":57454,\"start\":57451},{\"end\":57471,\"start\":57462},{\"end\":57483,\"start\":57479},{\"end\":57496,\"start\":57491},{\"end\":57514,\"start\":57507},{\"end\":57529,\"start\":57523},{\"end\":57545,\"start\":57539},{\"end\":57558,\"start\":57551},{\"end\":57575,\"start\":57571},{\"end\":57979,\"start\":57975},{\"end\":57996,\"start\":57989},{\"end\":58006,\"start\":58001},{\"end\":58019,\"start\":58014},{\"end\":58031,\"start\":58026},{\"end\":58044,\"start\":58040},{\"end\":58329,\"start\":58324},{\"end\":58342,\"start\":58338},{\"end\":58356,\"start\":58352},{\"end\":58375,\"start\":58366},{\"end\":58387,\"start\":58381},{\"end\":58403,\"start\":58396},{\"end\":58417,\"start\":58412},{\"end\":58427,\"start\":58424},{\"end\":58439,\"start\":58432},{\"end\":58770,\"start\":58764},{\"end\":58797,\"start\":58786},{\"end\":58808,\"start\":58803},{\"end\":58822,\"start\":58816},{\"end\":58838,\"start\":58832},{\"end\":58850,\"start\":58845},{\"end\":58869,\"start\":58860},{\"end\":58875,\"start\":58870},{\"end\":58894,\"start\":58886},{\"end\":58913,\"start\":58905},{\"end\":59294,\"start\":59290},{\"end\":59311,\"start\":59304},{\"end\":59327,\"start\":59320},{\"end\":59343,\"start\":59337},{\"end\":59364,\"start\":59354},{\"end\":59382,\"start\":59374},{\"end\":59397,\"start\":59392},{\"end\":59420,\"start\":59416},{\"end\":59434,\"start\":59428},{\"end\":59864,\"start\":59859},{\"end\":59876,\"start\":59870},{\"end\":59887,\"start\":59883},{\"end\":59907,\"start\":59900},{\"end\":59917,\"start\":59915},{\"end\":59927,\"start\":59923},{\"end\":59937,\"start\":59932},{\"end\":60269,\"start\":60262},{\"end\":60281,\"start\":60275},{\"end\":60294,\"start\":60288},{\"end\":60308,\"start\":60299},{\"end\":60321,\"start\":60314},{\"end\":60686,\"start\":60679},{\"end\":60699,\"start\":60692},{\"end\":60715,\"start\":60707},{\"end\":60729,\"start\":60723},{\"end\":60745,\"start\":60736},{\"end\":60755,\"start\":60753},{\"end\":60770,\"start\":60763},{\"end\":60784,\"start\":60775},{\"end\":61176,\"start\":61170},{\"end\":61191,\"start\":61183},{\"end\":61205,\"start\":61199},{\"end\":61218,\"start\":61212},{\"end\":61236,\"start\":61229},{\"end\":61254,\"start\":61247},{\"end\":61267,\"start\":61260},{\"end\":61279,\"start\":61276},{\"end\":61291,\"start\":61287},{\"end\":61304,\"start\":61298},{\"end\":61712,\"start\":61703},{\"end\":61721,\"start\":61719},{\"end\":61734,\"start\":61727},{\"end\":61749,\"start\":61743},{\"end\":61763,\"start\":61759},{\"end\":62004,\"start\":61999},{\"end\":62019,\"start\":62012},{\"end\":62033,\"start\":62028},{\"end\":62046,\"start\":62041},{\"end\":62060,\"start\":62056},{\"end\":62074,\"start\":62067},{\"end\":62092,\"start\":62081},{\"end\":62104,\"start\":62100},{\"end\":62115,\"start\":62111},{\"end\":62122,\"start\":62120},{\"end\":62457,\"start\":62449},{\"end\":62469,\"start\":62464},{\"end\":62479,\"start\":62474},{\"end\":62489,\"start\":62484},{\"end\":62498,\"start\":62496},{\"end\":62510,\"start\":62504}]", "bib_author_last_name": "[{\"end\":47197,\"start\":47193},{\"end\":47212,\"start\":47205},{\"end\":47226,\"start\":47223},{\"end\":47238,\"start\":47234},{\"end\":47628,\"start\":47623},{\"end\":47643,\"start\":47639},{\"end\":47655,\"start\":47650},{\"end\":47672,\"start\":47665},{\"end\":47688,\"start\":47682},{\"end\":47707,\"start\":47699},{\"end\":47727,\"start\":47716},{\"end\":47741,\"start\":47736},{\"end\":47756,\"start\":47750},{\"end\":47771,\"start\":47765},{\"end\":48189,\"start\":48184},{\"end\":48203,\"start\":48197},{\"end\":48217,\"start\":48210},{\"end\":48230,\"start\":48226},{\"end\":48248,\"start\":48239},{\"end\":48267,\"start\":48258},{\"end\":48283,\"start\":48276},{\"end\":48558,\"start\":48550},{\"end\":48570,\"start\":48565},{\"end\":48586,\"start\":48579},{\"end\":48604,\"start\":48593},{\"end\":48950,\"start\":48946},{\"end\":48962,\"start\":48959},{\"end\":48978,\"start\":48972},{\"end\":48992,\"start\":48985},{\"end\":49003,\"start\":48996},{\"end\":49017,\"start\":49010},{\"end\":49026,\"start\":49019},{\"end\":49400,\"start\":49396},{\"end\":49412,\"start\":49409},{\"end\":49426,\"start\":49419},{\"end\":49437,\"start\":49430},{\"end\":49451,\"start\":49444},{\"end\":49460,\"start\":49453},{\"end\":49845,\"start\":49839},{\"end\":49862,\"start\":49857},{\"end\":49881,\"start\":49872},{\"end\":49903,\"start\":49896},{\"end\":49927,\"start\":49918},{\"end\":49934,\"start\":49929},{\"end\":50277,\"start\":50270},{\"end\":50291,\"start\":50283},{\"end\":50625,\"start\":50618},{\"end\":50649,\"start\":50635},{\"end\":50662,\"start\":50655},{\"end\":50672,\"start\":50664},{\"end\":50975,\"start\":50968},{\"end\":50999,\"start\":50985},{\"end\":51012,\"start\":51005},{\"end\":51022,\"start\":51014},{\"end\":51362,\"start\":51359},{\"end\":51376,\"start\":51373},{\"end\":51393,\"start\":51385},{\"end\":51404,\"start\":51399},{\"end\":51420,\"start\":51414},{\"end\":51436,\"start\":51430},{\"end\":51454,\"start\":51447},{\"end\":51467,\"start\":51464},{\"end\":51482,\"start\":51474},{\"end\":52034,\"start\":52029},{\"end\":52048,\"start\":52041},{\"end\":52066,\"start\":52056},{\"end\":52084,\"start\":52078},{\"end\":52099,\"start\":52092},{\"end\":52111,\"start\":52105},{\"end\":52127,\"start\":52120},{\"end\":52145,\"start\":52135},{\"end\":52162,\"start\":52157},{\"end\":52176,\"start\":52170},{\"end\":52574,\"start\":52566},{\"end\":52882,\"start\":52872},{\"end\":53038,\"start\":53036},{\"end\":53051,\"start\":53047},{\"end\":53059,\"start\":53056},{\"end\":53070,\"start\":53066},{\"end\":53079,\"start\":53077},{\"end\":53089,\"start\":53084},{\"end\":53101,\"start\":53099},{\"end\":53111,\"start\":53107},{\"end\":53119,\"start\":53117},{\"end\":53487,\"start\":53481},{\"end\":53498,\"start\":53495},{\"end\":53522,\"start\":53509},{\"end\":53540,\"start\":53531},{\"end\":53550,\"start\":53546},{\"end\":53565,\"start\":53557},{\"end\":53577,\"start\":53573},{\"end\":53597,\"start\":53586},{\"end\":54023,\"start\":54017},{\"end\":54038,\"start\":54033},{\"end\":54054,\"start\":54046},{\"end\":54070,\"start\":54064},{\"end\":54345,\"start\":54337},{\"end\":54358,\"start\":54353},{\"end\":54371,\"start\":54367},{\"end\":54389,\"start\":54380},{\"end\":54688,\"start\":54683},{\"end\":54704,\"start\":54699},{\"end\":54721,\"start\":54711},{\"end\":54739,\"start\":54732},{\"end\":54759,\"start\":54748},{\"end\":55125,\"start\":55120},{\"end\":55142,\"start\":55132},{\"end\":55162,\"start\":55151},{\"end\":55175,\"start\":55168},{\"end\":55597,\"start\":55590},{\"end\":55611,\"start\":55605},{\"end\":55625,\"start\":55619},{\"end\":55649,\"start\":55635},{\"end\":55661,\"start\":55655},{\"end\":55679,\"start\":55670},{\"end\":55694,\"start\":55685},{\"end\":56025,\"start\":56021},{\"end\":56040,\"start\":56037},{\"end\":56053,\"start\":56050},{\"end\":56482,\"start\":56478},{\"end\":56495,\"start\":56492},{\"end\":56508,\"start\":56503},{\"end\":56966,\"start\":56962},{\"end\":56981,\"start\":56977},{\"end\":56993,\"start\":56990},{\"end\":57006,\"start\":57003},{\"end\":57021,\"start\":57018},{\"end\":57036,\"start\":57032},{\"end\":57050,\"start\":57041},{\"end\":57066,\"start\":57063},{\"end\":57080,\"start\":57077},{\"end\":57093,\"start\":57090},{\"end\":57449,\"start\":57443},{\"end\":57460,\"start\":57455},{\"end\":57477,\"start\":57472},{\"end\":57489,\"start\":57484},{\"end\":57505,\"start\":57497},{\"end\":57521,\"start\":57515},{\"end\":57537,\"start\":57530},{\"end\":57549,\"start\":57546},{\"end\":57569,\"start\":57559},{\"end\":57582,\"start\":57576},{\"end\":57987,\"start\":57980},{\"end\":57999,\"start\":57997},{\"end\":58012,\"start\":58007},{\"end\":58024,\"start\":58020},{\"end\":58038,\"start\":58032},{\"end\":58054,\"start\":58045},{\"end\":58336,\"start\":58330},{\"end\":58350,\"start\":58343},{\"end\":58364,\"start\":58357},{\"end\":58379,\"start\":58376},{\"end\":58394,\"start\":58388},{\"end\":58410,\"start\":58404},{\"end\":58422,\"start\":58418},{\"end\":58430,\"start\":58428},{\"end\":58443,\"start\":58440},{\"end\":58784,\"start\":58771},{\"end\":58801,\"start\":58798},{\"end\":58814,\"start\":58809},{\"end\":58830,\"start\":58823},{\"end\":58843,\"start\":58839},{\"end\":58858,\"start\":58851},{\"end\":58884,\"start\":58876},{\"end\":58903,\"start\":58895},{\"end\":58918,\"start\":58914},{\"end\":58925,\"start\":58920},{\"end\":59302,\"start\":59295},{\"end\":59318,\"start\":59312},{\"end\":59335,\"start\":59328},{\"end\":59352,\"start\":59344},{\"end\":59372,\"start\":59365},{\"end\":59390,\"start\":59383},{\"end\":59414,\"start\":59398},{\"end\":59426,\"start\":59421},{\"end\":59441,\"start\":59435},{\"end\":59448,\"start\":59443},{\"end\":59868,\"start\":59865},{\"end\":59881,\"start\":59877},{\"end\":59898,\"start\":59888},{\"end\":59913,\"start\":59908},{\"end\":59921,\"start\":59918},{\"end\":59930,\"start\":59928},{\"end\":59942,\"start\":59938},{\"end\":60273,\"start\":60270},{\"end\":60286,\"start\":60282},{\"end\":60297,\"start\":60295},{\"end\":60312,\"start\":60309},{\"end\":60324,\"start\":60322},{\"end\":60690,\"start\":60687},{\"end\":60705,\"start\":60700},{\"end\":60721,\"start\":60716},{\"end\":60734,\"start\":60730},{\"end\":60751,\"start\":60746},{\"end\":60761,\"start\":60756},{\"end\":60773,\"start\":60771},{\"end\":60788,\"start\":60785},{\"end\":61181,\"start\":61177},{\"end\":61197,\"start\":61192},{\"end\":61210,\"start\":61206},{\"end\":61227,\"start\":61219},{\"end\":61245,\"start\":61237},{\"end\":61258,\"start\":61255},{\"end\":61274,\"start\":61268},{\"end\":61285,\"start\":61280},{\"end\":61296,\"start\":61292},{\"end\":61314,\"start\":61305},{\"end\":61717,\"start\":61713},{\"end\":61725,\"start\":61722},{\"end\":61741,\"start\":61735},{\"end\":61757,\"start\":61750},{\"end\":61767,\"start\":61764},{\"end\":62010,\"start\":62005},{\"end\":62026,\"start\":62020},{\"end\":62039,\"start\":62034},{\"end\":62054,\"start\":62047},{\"end\":62065,\"start\":62061},{\"end\":62079,\"start\":62075},{\"end\":62098,\"start\":62093},{\"end\":62109,\"start\":62105},{\"end\":62118,\"start\":62116},{\"end\":62135,\"start\":62123},{\"end\":62462,\"start\":62458},{\"end\":62472,\"start\":62470},{\"end\":62482,\"start\":62480},{\"end\":62494,\"start\":62490},{\"end\":62502,\"start\":62499},{\"end\":62514,\"start\":62511}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208290939},\"end\":47578,\"start\":47127},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218971783},\"end\":48096,\"start\":47580},{\"attributes\":{\"doi\":\"arXiv:1803.05457\",\"id\":\"b2\"},\"end\":48544,\"start\":48098},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b3\"},\"end\":48868,\"start\":48546},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207852310},\"end\":49315,\"start\":48870},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":148571720},\"end\":49835,\"start\":49317},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b6\"},\"end\":50170,\"start\":49837},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":251765570},\"end\":50527,\"start\":50172},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b8\"},\"end\":50891,\"start\":50529},{\"attributes\":{\"id\":\"b9\"},\"end\":51301,\"start\":50893},{\"attributes\":{\"id\":\"b10\"},\"end\":51896,\"start\":51303},{\"attributes\":{\"id\":\"b11\"},\"end\":52497,\"start\":51898},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":22028726},\"end\":52868,\"start\":52499},{\"attributes\":{\"id\":\"b13\"},\"end\":52947,\"start\":52870},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":231861390},\"end\":53413,\"start\":52949},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5151364},\"end\":53974,\"start\":53415},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b16\"},\"end\":54240,\"start\":53976},{\"attributes\":{\"doi\":\"arXiv:1809.02789\",\"id\":\"b17\"},\"end\":54612,\"start\":54242},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":216056295},\"end\":55039,\"start\":54614},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":184487878},\"end\":55544,\"start\":55041},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":208139197},\"end\":55916,\"start\":55546},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":50778049},\"end\":56395,\"start\":55918},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5071254},\"end\":56831,\"start\":56397},{\"attributes\":{\"id\":\"b23\"},\"end\":57366,\"start\":56833},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202786778},\"end\":57920,\"start\":57368},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":160025533},\"end\":58239,\"start\":57922},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204838007},\"end\":58762,\"start\":58241},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b27\"},\"end\":59288,\"start\":58764},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b28\"},\"end\":59786,\"start\":59290},{\"attributes\":{\"doi\":\"arXiv:2201.11903\",\"id\":\"b29\"},\"end\":60172,\"start\":59788},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":247411282},\"end\":60598,\"start\":60174},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":252545187},\"end\":61094,\"start\":60600},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b32\"},\"end\":61611,\"start\":61096},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b33\"},\"end\":61997,\"start\":61613},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b34\"},\"end\":62447,\"start\":61999},{\"attributes\":{\"doi\":\"arXiv:1606.06160\",\"id\":\"b35\"},\"end\":62841,\"start\":62449}]", "bib_title": "[{\"end\":47183,\"start\":47127},{\"end\":47617,\"start\":47580},{\"end\":48939,\"start\":48870},{\"end\":49389,\"start\":49317},{\"end\":50262,\"start\":50172},{\"end\":50960,\"start\":50893},{\"end\":52559,\"start\":52499},{\"end\":53027,\"start\":52949},{\"end\":53473,\"start\":53415},{\"end\":54674,\"start\":54614},{\"end\":55111,\"start\":55041},{\"end\":55583,\"start\":55546},{\"end\":56010,\"start\":55918},{\"end\":56467,\"start\":56397},{\"end\":57436,\"start\":57368},{\"end\":57973,\"start\":57922},{\"end\":58322,\"start\":58241},{\"end\":60260,\"start\":60174},{\"end\":60677,\"start\":60600}]", "bib_author": "[{\"end\":47199,\"start\":47185},{\"end\":47214,\"start\":47199},{\"end\":47228,\"start\":47214},{\"end\":47240,\"start\":47228},{\"end\":47630,\"start\":47619},{\"end\":47645,\"start\":47630},{\"end\":47657,\"start\":47645},{\"end\":47674,\"start\":47657},{\"end\":47690,\"start\":47674},{\"end\":47709,\"start\":47690},{\"end\":47729,\"start\":47709},{\"end\":47743,\"start\":47729},{\"end\":47758,\"start\":47743},{\"end\":47773,\"start\":47758},{\"end\":48191,\"start\":48178},{\"end\":48205,\"start\":48191},{\"end\":48219,\"start\":48205},{\"end\":48232,\"start\":48219},{\"end\":48250,\"start\":48232},{\"end\":48269,\"start\":48250},{\"end\":48285,\"start\":48269},{\"end\":48560,\"start\":48546},{\"end\":48572,\"start\":48560},{\"end\":48588,\"start\":48572},{\"end\":48606,\"start\":48588},{\"end\":48952,\"start\":48941},{\"end\":48964,\"start\":48952},{\"end\":48980,\"start\":48964},{\"end\":48994,\"start\":48980},{\"end\":49005,\"start\":48994},{\"end\":49019,\"start\":49005},{\"end\":49028,\"start\":49019},{\"end\":49402,\"start\":49391},{\"end\":49414,\"start\":49402},{\"end\":49428,\"start\":49414},{\"end\":49439,\"start\":49428},{\"end\":49453,\"start\":49439},{\"end\":49462,\"start\":49453},{\"end\":49847,\"start\":49837},{\"end\":49864,\"start\":49847},{\"end\":49883,\"start\":49864},{\"end\":49905,\"start\":49883},{\"end\":49929,\"start\":49905},{\"end\":49936,\"start\":49929},{\"end\":50279,\"start\":50264},{\"end\":50293,\"start\":50279},{\"end\":50627,\"start\":50612},{\"end\":50651,\"start\":50627},{\"end\":50664,\"start\":50651},{\"end\":50674,\"start\":50664},{\"end\":50977,\"start\":50962},{\"end\":51001,\"start\":50977},{\"end\":51014,\"start\":51001},{\"end\":51024,\"start\":51014},{\"end\":51364,\"start\":51355},{\"end\":51378,\"start\":51364},{\"end\":51395,\"start\":51378},{\"end\":51406,\"start\":51395},{\"end\":51422,\"start\":51406},{\"end\":51438,\"start\":51422},{\"end\":51456,\"start\":51438},{\"end\":51469,\"start\":51456},{\"end\":51484,\"start\":51469},{\"end\":52036,\"start\":52020},{\"end\":52050,\"start\":52036},{\"end\":52068,\"start\":52050},{\"end\":52086,\"start\":52068},{\"end\":52101,\"start\":52086},{\"end\":52113,\"start\":52101},{\"end\":52129,\"start\":52113},{\"end\":52147,\"start\":52129},{\"end\":52164,\"start\":52147},{\"end\":52178,\"start\":52164},{\"end\":52576,\"start\":52561},{\"end\":52884,\"start\":52872},{\"end\":53040,\"start\":53029},{\"end\":53053,\"start\":53040},{\"end\":53061,\"start\":53053},{\"end\":53072,\"start\":53061},{\"end\":53081,\"start\":53072},{\"end\":53091,\"start\":53081},{\"end\":53103,\"start\":53091},{\"end\":53113,\"start\":53103},{\"end\":53121,\"start\":53113},{\"end\":53489,\"start\":53475},{\"end\":53500,\"start\":53489},{\"end\":53524,\"start\":53500},{\"end\":53542,\"start\":53524},{\"end\":53552,\"start\":53542},{\"end\":53567,\"start\":53552},{\"end\":53579,\"start\":53567},{\"end\":53599,\"start\":53579},{\"end\":54025,\"start\":54009},{\"end\":54040,\"start\":54025},{\"end\":54056,\"start\":54040},{\"end\":54072,\"start\":54056},{\"end\":54347,\"start\":54331},{\"end\":54360,\"start\":54347},{\"end\":54373,\"start\":54360},{\"end\":54391,\"start\":54373},{\"end\":54690,\"start\":54676},{\"end\":54706,\"start\":54690},{\"end\":54723,\"start\":54706},{\"end\":54741,\"start\":54723},{\"end\":54761,\"start\":54741},{\"end\":55127,\"start\":55113},{\"end\":55144,\"start\":55127},{\"end\":55164,\"start\":55144},{\"end\":55177,\"start\":55164},{\"end\":55599,\"start\":55585},{\"end\":55613,\"start\":55599},{\"end\":55627,\"start\":55613},{\"end\":55651,\"start\":55627},{\"end\":55663,\"start\":55651},{\"end\":55681,\"start\":55663},{\"end\":55696,\"start\":55681},{\"end\":56027,\"start\":56012},{\"end\":56042,\"start\":56027},{\"end\":56055,\"start\":56042},{\"end\":56484,\"start\":56469},{\"end\":56497,\"start\":56484},{\"end\":56510,\"start\":56497},{\"end\":56968,\"start\":56956},{\"end\":56983,\"start\":56968},{\"end\":56995,\"start\":56983},{\"end\":57008,\"start\":56995},{\"end\":57023,\"start\":57008},{\"end\":57038,\"start\":57023},{\"end\":57052,\"start\":57038},{\"end\":57068,\"start\":57052},{\"end\":57082,\"start\":57068},{\"end\":57095,\"start\":57082},{\"end\":57451,\"start\":57438},{\"end\":57462,\"start\":57451},{\"end\":57479,\"start\":57462},{\"end\":57491,\"start\":57479},{\"end\":57507,\"start\":57491},{\"end\":57523,\"start\":57507},{\"end\":57539,\"start\":57523},{\"end\":57551,\"start\":57539},{\"end\":57571,\"start\":57551},{\"end\":57584,\"start\":57571},{\"end\":57989,\"start\":57975},{\"end\":58001,\"start\":57989},{\"end\":58014,\"start\":58001},{\"end\":58026,\"start\":58014},{\"end\":58040,\"start\":58026},{\"end\":58056,\"start\":58040},{\"end\":58338,\"start\":58324},{\"end\":58352,\"start\":58338},{\"end\":58366,\"start\":58352},{\"end\":58381,\"start\":58366},{\"end\":58396,\"start\":58381},{\"end\":58412,\"start\":58396},{\"end\":58424,\"start\":58412},{\"end\":58432,\"start\":58424},{\"end\":58445,\"start\":58432},{\"end\":58786,\"start\":58764},{\"end\":58803,\"start\":58786},{\"end\":58816,\"start\":58803},{\"end\":58832,\"start\":58816},{\"end\":58845,\"start\":58832},{\"end\":58860,\"start\":58845},{\"end\":58886,\"start\":58860},{\"end\":58905,\"start\":58886},{\"end\":58920,\"start\":58905},{\"end\":58927,\"start\":58920},{\"end\":59304,\"start\":59290},{\"end\":59320,\"start\":59304},{\"end\":59337,\"start\":59320},{\"end\":59354,\"start\":59337},{\"end\":59374,\"start\":59354},{\"end\":59392,\"start\":59374},{\"end\":59416,\"start\":59392},{\"end\":59428,\"start\":59416},{\"end\":59443,\"start\":59428},{\"end\":59450,\"start\":59443},{\"end\":59870,\"start\":59859},{\"end\":59883,\"start\":59870},{\"end\":59900,\"start\":59883},{\"end\":59915,\"start\":59900},{\"end\":59923,\"start\":59915},{\"end\":59932,\"start\":59923},{\"end\":59944,\"start\":59932},{\"end\":60275,\"start\":60262},{\"end\":60288,\"start\":60275},{\"end\":60299,\"start\":60288},{\"end\":60314,\"start\":60299},{\"end\":60326,\"start\":60314},{\"end\":60692,\"start\":60679},{\"end\":60707,\"start\":60692},{\"end\":60723,\"start\":60707},{\"end\":60736,\"start\":60723},{\"end\":60753,\"start\":60736},{\"end\":60763,\"start\":60753},{\"end\":60775,\"start\":60763},{\"end\":60790,\"start\":60775},{\"end\":61183,\"start\":61170},{\"end\":61199,\"start\":61183},{\"end\":61212,\"start\":61199},{\"end\":61229,\"start\":61212},{\"end\":61247,\"start\":61229},{\"end\":61260,\"start\":61247},{\"end\":61276,\"start\":61260},{\"end\":61287,\"start\":61276},{\"end\":61298,\"start\":61287},{\"end\":61316,\"start\":61298},{\"end\":61719,\"start\":61703},{\"end\":61727,\"start\":61719},{\"end\":61743,\"start\":61727},{\"end\":61759,\"start\":61743},{\"end\":61769,\"start\":61759},{\"end\":62012,\"start\":61999},{\"end\":62028,\"start\":62012},{\"end\":62041,\"start\":62028},{\"end\":62056,\"start\":62041},{\"end\":62067,\"start\":62056},{\"end\":62081,\"start\":62067},{\"end\":62100,\"start\":62081},{\"end\":62111,\"start\":62100},{\"end\":62120,\"start\":62111},{\"end\":62137,\"start\":62120},{\"end\":62464,\"start\":62449},{\"end\":62474,\"start\":62464},{\"end\":62484,\"start\":62474},{\"end\":62496,\"start\":62484},{\"end\":62504,\"start\":62496},{\"end\":62516,\"start\":62504}]", "bib_venue": "[{\"end\":47349,\"start\":47303},{\"end\":49591,\"start\":49535},{\"end\":53675,\"start\":53653},{\"end\":55306,\"start\":55250},{\"end\":56625,\"start\":56576},{\"end\":47301,\"start\":47240},{\"end\":47822,\"start\":47773},{\"end\":48176,\"start\":48098},{\"end\":48675,\"start\":48622},{\"end\":49077,\"start\":49028},{\"end\":49533,\"start\":49462},{\"end\":49982,\"start\":49952},{\"end\":50342,\"start\":50293},{\"end\":50610,\"start\":50529},{\"end\":51089,\"start\":51024},{\"end\":51353,\"start\":51303},{\"end\":52018,\"start\":51898},{\"end\":52666,\"start\":52576},{\"end\":53173,\"start\":53121},{\"end\":53651,\"start\":53599},{\"end\":54007,\"start\":53976},{\"end\":54329,\"start\":54242},{\"end\":54805,\"start\":54761},{\"end\":55248,\"start\":55177},{\"end\":55712,\"start\":55696},{\"end\":56136,\"start\":56055},{\"end\":56574,\"start\":56510},{\"end\":56954,\"start\":56833},{\"end\":57633,\"start\":57584},{\"end\":58067,\"start\":58056},{\"end\":58485,\"start\":58445},{\"end\":58999,\"start\":58943},{\"end\":59511,\"start\":59466},{\"end\":59857,\"start\":59788},{\"end\":60378,\"start\":60326},{\"end\":60839,\"start\":60790},{\"end\":61168,\"start\":61096},{\"end\":61701,\"start\":61613},{\"end\":62197,\"start\":62153},{\"end\":62623,\"start\":62532}]"}}}, "year": 2023, "month": 12, "day": 17}