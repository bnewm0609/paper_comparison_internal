{"id": 259095896, "updated": "2023-10-04 23:47:29.592", "metadata": {"title": "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning", "authors": "[{\"first\":\"Lei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yuwei\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Shicheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Peiyi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shuhuai\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Mukai\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yazheng\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jingjing\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Lingpeng\",\"last\":\"Kong\",\"middle\":[]},{\"first\":\"Qi\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.04387", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-04387", "doi": "10.48550/arxiv.2306.04387"}}, "content": {"source": {"pdf_hash": "6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.04387v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7afe1e9bcdf6bc99f02a3b1ff2fc576e3c135bdf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8.txt", "contents": "\nM 3 IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning\n\n\nLei Li \nSchool of Computer Science\nNational Key Laboratory for Multimedia Information Processing\nThe University of Hong Kong\nPeking University\n\n\n\u2020 \nYuwei Yin \nSchool of Computer Science\nNational Key Laboratory for Multimedia Information Processing\nThe University of Hong Kong\nPeking University\n\n\nShicheng Li \n\u00a7 \nLiang Chen \nPeiyi Wang \nShuhuai Ren \nMukai Li \nShanghai AI Lab\n\n\nYazheng Yang \nSchool of Computer Science\nNational Key Laboratory for Multimedia Information Processing\nThe University of Hong Kong\nPeking University\n\n\nJingjing Xu jingjingxu@pku.edu.cnlpk \nShanghai AI Lab\n\n\nXu Sun \nLingpeng Kong \nSchool of Computer Science\nNational Key Laboratory for Multimedia Information Processing\nThe University of Hong Kong\nPeking University\n\n\nQi Liu liuqi@cs.hku.hk \nSchool of Computer Science\nNational Key Laboratory for Multimedia Information Processing\nThe University of Hong Kong\nPeking University\n\n\nM 3 IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning\n\nInstruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M 3 IT) dataset, designed to optimize VLM alignment with human instructions. Our M 3 IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M 3 IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M 3 IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research. 1\n\nIntroduction\n\nThere has been a continuously increasing trend to develop intelligent assistants that can follow human instructions [3,36,37]. In the natural language processing (NLP) field, instruction tuning [35,53] is a success paradigm that leverages large-scale well-formatted instances to align large language models (LLMs) to human instructions. By finetuning on instances with specific task descriptions, LLMs learn to follow the instruction to perform various tasks, and demonstrate strong generalization ability on unseen tasks [29]. Expanding beyond NLP, a general-purpose intelligent agent must encompass various modalities, such as vision, prompting recent efforts to investigate instruction tuning in vision-language domains [63,28,7]. To develop powerful vision-language models (VLMs), it is essential to have a well-constructed dataset that encompasses diverse vision-language tasks and aligns with human instructions. However, the instructional data supporting existing VLMs is either not publicly available (e.g.,  or offers limited task and language coverage (e.g., only tasks in English are considered). This scarcity of comprehensive datasets has impeded the progress of open vision-language models, highlighting the importance of multi-modal instruction tuning and the need for high-quality datasets.\n\nIn this paper, we aim to advance instruction tuning research in the multi-modal domain by introducing an open dataset M 3 IT, a Multi-Modal Multilingual Instruction Tuning dataset, as an essential step towards building a versatile general-purpose assistant. We build this dataset by converting existing datasets into a unified vision-to-text schema with four stages: (1) manual instruction writing, (2) dataset pre-processing, (3) careful quality check and (4) dataset translation for key tasks. Our dataset encompasses a wide range of tasks, including classic image-text tasks such as image classification, visual question answering, and image captioning. Video-related tasks, such as video questionanswering, are also incorporated to ensure comprehensive coverage across multiple modalities. We further integrate Chinese vision-language datasets with corresponding Chinese instructions. The resulting dataset compiles 40 diverse tasks and 400 instructions. Finally, key vision-language tasks are translated into 80 languages with a strong translation system, to support multilingual studies.\n\nTo evaluate the effectiveness of the proposed dataset, we develop a vision-language model, Ying-VLM, by integrating a strong vision encoder, BLIP-2 [23] with a large language model, Ziya-13B [61], derived from LLaMA [49]. Building on the successful approach of incorporating visual tokens as textual prompts in LLMs [7,63,28], we employ a two-stage training process: (1) the initial stage aligns vision features with text embeddings through image captioning on LAION400M [41], and (2) the second stage enhances the model by conducting instruction tuning on selected tasks of our dataset. Experimental results reveal that Ying-VLM surpasses strong baseline models in knowledgeable VQA tasks and exhibits improved generalization performance to unseen video and cross-lingual tasks. Further analysis indicates that the improved performance corresponds to increased tasks for instruction tuning, while the diversity of instructions also affects outcomes. This paper presents two key contributions: (1) We introduce the open-source, large-scale Multimodal, multilingual Instruction Tuning (M 3 IT) dataset, designed to enable the development of general-purpose multi-modal agents. (2) We develop Ying-VLM, a visual assistant that excels in knowledgeable VQA tasks, demonstrates strong generalization to unseen video QA and Chinese multi-modal tasks, and offers valuable insights for future research. \n\n\nRelated Work\nMiniGPT4 N / A \u2717 5K N / A \u2713 LLaVA 3 \u2717 1.15M N / A \u2713 MultiModalGPT 3 \u2717 6K 5 \u2717 MultiInstruct 26 \u2717 \u223c 235K 5 \u2717 InstructBLIP 28 \u2717 \u223c 1.6M 9.7 \u2717 M 3 IT (Ours) 40 \u2713 2.4M 10 \u2713\nOur work draws inspiration from recent language instruction tuning benchmarks [53,35], which have been proven effective for improving language models to obtain cross-task generalization ability [29,52]. In this paper, we focus on exploring the instruction tuning paradigm from LLMs to multi-modal agents. Unlike text-only tasks, vision-language tasks generally have more diverse formats, which poses new challenges toward vision-language instruction tuning benchmarks.\n\nTo develop a general-purpose vision-language model, it is crucial to create high-quality multi-modal instruction tuning datasets encompassing diverse tasks, languages, and instructions. Several studies have investigated multi-modal instruction tuning for VLMs. LLaVA [28] and MiniGPT-4 [63] generate visual content-related dialog by incorporating image caption data into GPT-4/ChatGPT models. MultiInstruct [56] reformats a series of visual classification tasks into an instruction-tuning format, while InstructBLIP [7] adapts 28 existing image-to-text tasks. However, these datasets do not provide an ideal multi-modal instruction tuning dataset due to their limited (1) coverage of various task types in multi-modal fields, (2) diversity and quality of instances, and (3) inclusion of multiple languages for wide linguistic diversity. In this paper, we construct an improved multi-modal instruction tuning dataset by expanding task coverage to 40 datasets, supplementing instances with 10 manually written task instructions, and including tasks in different languages. Table 1 compares the characteristics of existing multi-modal instruction tuning datasets and M 3 IT.\n\n\nM IT: A Multi-Modal Multilingual Instruction Tuning Dataset\n\nIn this section, we introduce our proposed M 3 IT dataset by first elaborating the dataset coverage ( \u00a7 3.1), followed by the details of the annotation process( \u00a7 3.2). Finally, we present the dataset format and provide the statistics of the crafted datasets instructions( \u00a7 3.3).\n\n\nTask Coverage\n\nOur dataset compiles diverse tasks of classical vision-language tasks, including captioning, visual question answering (VQA), visual conditioned generation, reasoning and classification.\n\nCaptioning This task aims to produce descriptions of the given images according to different needs. We include MS COCO [27] (the Karpathy split) for generic image descriptions. TextCaps [44] requires models to capture the text presented in the image and generate captions accordingly. Image-Paragraph-Captioning [21] focuses on generating detailed descriptions for images.\n\nReasoning This task evaluates specific reasoning capabilities. We incorporate CLEVR [19] and NLVR [46] for spatial reasoning, Visual Commonsense Reasoning (VCR) [60] for commonsense reasoning, Visual MRC [47] for reading comprehensive over images, and Winoground [48] for fine-grained semantics reasoning over text descriptions and image contents.\n\nVisual Question Answering (VQA) This is the most widely studied multi-modal task, which requires the model to answer a given question based on the image correctly. Tasks include VQA v2 [15], Shapes VQA [1], DocVQA [33], OCR-VQA [34], ST-VQA [2], Text-VQA [45], and GQA [18].\n\nKnowledgeable Visual Question Answering Unlike traditional VQA tasks focusing on the question relevant to the content image, knowledgeable visual question answer (KVQA) requires the model to draw upon outside knowledge to answer questions. We incorporate two outside knowledge VQA datasets: OK-VQA [32] and A-OK-VQA [42], ScienceQA [31] which contains multi-modal science questions, and ViQuAE [22] focusing on knowledge facts of named entities in images.\n\nClassification This task involves classifying an image based on a given set of candidate labels. ImageNet [40], Grounded Object Identification (COCO-GOI) [27], COCO-Text [50], Image Text Matching (COCO-ITM) [27], e-SNLI-VE [20], Multi-modal Fact Checking (Mocheg) [58], and IQA [9] are included. Due to language model input length constraints, we reduce the number of options in some datasets with extensive candidate labels, such as ImageNet.\n\nGeneration Visual conditional general requires models to understand the visual content and make a composition meeting the task demand. We have Visual Storytelling (VIST) [17], Visual Dialog (VisDial) [8], and multi-modal machine translation Multi30k [10] in this category.\n\nChinese and multilingual Vision-Language Tasks To examine the effect of instruction tuning on different languages, we incorporate several Chinese vision-language tasks including FM-IQA [11] for VQA, COCO-CN [25] and Flickr8k-CN [24] for captioning, Chinese Food Net [4] for classification, and MMChat [62] for generation.\n\n\nVideo-Language Tasks\n\nBeyond the static images, we are interested in whether instruction tuning can also be applied to video-text tasks. We include the classic MSR-VTT datasets [55] for video captioning, MSRVTT-QA [54], ActivityNet-QA [59], iVQA [57] and MSVD-QA [54] for video question answering, Something-Something [14] for video action classification.\n\nAs shown in Figure 1, our dataset makes a wide coverage of the current existing visual-language and video-language benchmarks, enabling different skill sets for the language models, from simple image captioning to complicated reasoning based on the image even beyond the visual content.\n\n\nAnnotation Process\n\nTo build high-quality multi-modal instruction datasets, we rewrite various datasets into a vision-totext format. The annotation process includes four steps: (1) writing instructions for each task, (2) structuring images and texts into a unified schema, (3) checking the overall dataset quality, and (4) building multilingual sets. Eight authors of this work are employed as human annotators, each of whom is a graduate student familiar with relevant literature.  Instruction edit distance among the same task 76.6 \u00b1 37.2\n\n\nVisual Question Answering\n\n\nCOCO-Caption CN\n\n\nMSRVTT Captioning\n\n\nClassification\n\n\nGrounded Object Identification\n\n\nImageNet Image Classification\n\nInstruction edit distance across tasks 106.6 \u00b1 39.5\n\nStage I: Instruction Writing To build high-quality instructions, we first ask annotators to carefully read the dataset paper and check the original dataset with some instances to get a clear understanding of the task. After that, they are required to write 10 diverse task instructions manually, covering the key characteristics of the task. Table 2 shows the statistics of the written instructions for each task.\n\nIn total, we annotate 400 instructions for all tasks. The average length per instruction is 24.4. To evaluate the diversity of annotated instructions, we employ the average edit distance to measure the similarity between two strings. The average edit distance within the same task is 76.6, indicating a good range of instruction diversity.\n\nStage II: Data Format Unification After the instruction has been written according to the task characteristics, we further process the images and corresponding text for a unified instance schema. For most datasets, we keep the original images and text, where images are converted into corresponding base64 encoded strings for easy data loading. We perform two modifications on potential examples: (1) Adding Bounding Box to Images. For tasks designed for specific regions in the image, a straightforward solution is to provide the bounding box information in natural language for informing the language models of the regions in interest. However, the image preprocessing techniques adopted by different vision encoders may resize the original image, and the original bounding box annotation thus needs further adjustments. Inspired by the recent observation that common vision encoders such as CLIP [39] are sensitive to the visual prompt [43], we directly tag the bounding box as a red rectangle to the image, serving as a hint for VLMs to focus on the target region. You are given a question related to an image and a short ground-truth answer. Your task is to transform the ground-truth answer into a natural response.\n\nQuestion: What is the color of the dog? Answer: white Paraphrased Answer: The dog in the image is white.\n\nQuestion: <question> Answer: <answer> Paraphrased Answer:\n\n\nQuestion\n\nWhich song was sung by this woman just before Barack Obama was sworn in as President of the USA in 2009?\n\n\nParaphrased Answer\n\nThe woman in the image, Aretha Franklin, performed \"My Country 'Tis of Thee\" just before Barack Obama's inauguration as President of the USA in 2009.\n\n\nPrompt\n\n\nOriginal Data Preprocessed Data\n\nIdentify the type of the object in the given image region. VQA dataset could negatively influence the model generation performance [7], we propose to utilize the ChatGPT [36] model for paraphrasing the original answers, by providing origin question and answer with potential extra contextual information. Contextual information includes the caption of the original images and OCR tokens for the scene-related question. The prompt used for answer paraphrasing can be found in Appendix. Figure 2 illustrates the data modifications we performed on our dataset.\n\nStage III: Quality Check In this stage, we assign a different annotator to each task to review 10 examples from each split. During this stage, we identify minor format inconsistencies between tasks and address them by standardizing the task formats. We also observe that a few answers (less than 3% of examined instances) were not effectively paraphrased by ChatGPT due to insufficient image information. We employ simple heuristics to filter these paraphrased answers and use a basic template to convert the original answer into a sentence. We find that this small portion of unsuccessful paraphrased answers has negligible impact. Finally, the task dataset is deemed complete once the annotator can successfully load it and re-examine the accuracy of the instructions, inputs, and outputs for each instance examined.\n\nStage IV: Key Datasets Translation To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names).\n\n\nDataset Format\n\nThe instance in our dataset consists of five fields: (1) Images: we represent the images with the potentially added bounding box by a base64 string. (2) Instruction: we randomly select an instruction from the task instruction pool for each instance. (3) Inputs: we allocate this field for providing task-specific inputs to the model, e.g., the question in the VQA tasks. For tasks such as captioning, there is no extra input so the corresponding field is left as an empty string. (4) Outputs: the required output to the specific tasks, such as the description of the image for captioning tasks and the answer to the image-related question. (5) Meta Data: we provide this field to preserve important information such as image id for referencing the original dataset. Figure 3 illustrates an instance in the unified format. With the clear distinction of these fields, the user of our benchmark can flexibly construct the training instances needed and evaluate the models conveniently. Table 3 gives the statistics aggregated by tasks, and we refer readers to Appendix for detailed statistics and the license of each dataset.\n\n# List[String]: the base64 string representation of a profile photo of F. Scott Fitzgerald Images: [\"iVBORw0KGg...5ErkJggg==\"] # String: task instruction Instruction: \"Analyze the image and provide an appropriate response to the question. \" # String: task-specific inputs, e.g., a question related to the image. Inputs: \"On which book by this man, Baz luhrmann's planned a film?\" # String: task outputs, e.g., the correct answer for the question. Outputs: \"Baz Luhrmann has planned a film adaptation of the book The Great Gatsby. \" # Dict: meta information dictionary contains original data. Meta Data: {\"kilt_id\": \"qw_1524\", ... ,\"wikipedia_id\": \"152171\"} Figure 3: A ViQuAE instance represented in the unified data instance schema used in our dataset. \n\n\nExperiments\n\nIn this section, we build a VLM to validate the effectiveness of the proposed M 3 IT dataset for multi-modal agents. We first introduce the experimental setups ( \u00a7 4.1), then report and discuss the results ( \u00a7 4.2). Lastly, we analyze the influence of task number and instruction diversity, and provide a qualitative result ( \u00a7 4.3).\n\n\nExperimental Settings\n\nImplementation Details Inspired by the recent success of BLIP [23], we adopt the vision encoder and the Q-former architecture in the BLIP2-OPT-2.7B [23] model to extract relevant visual features from images. For the large language models, we utilize Ziya-13B [61] derived from LLaMA [49] with bilingual (English and Chinese) ability. We employ a two-staged training. Stage I Visual-Text Alignment: To align the visual and textual feature space, we utilize the instructions in the coco captioning and perform an initial alignment training on LAION 400M [41]. We train the Q-former and the language projection, resulting in a total 130M parameters to optimize with AdamW [30]. The batch size is set to 256 to maximize the utilization of GPU and the model is trained with 300k steps. The learning rate linearly increases to a peak value of 5e-5 in the first 2000 steps and follows a cosine decay scheduler. The weight decay is set to 0.05. Stage II Multi-modal Instruction Tuning:\n\nWe further perform a multi-modal instruction tuning in our benchmark to activate the great potential of LLMs. We train the model after alignment training for 3 epochs and with a lower learning rate of 1e-5 and a warmup stage of 1000 steps. Inspired by LoRa tuning [16], the weights for mapping query and value vectors in the attention layer of LLMs are learnable in this stage to better adapt to the instruction tuning dataset. Other training parameters are consistent with Stage I. All experiments are conducted with 8 NVIDIA 80GB A100 GPUs. It took about 10 days for Stage I and Stage II can be finished in a day.   Evaluation Setup To examine the generalization of instruction tuning, some tasks are held-out for evaluation (see Figure 1 for held-in/out tasks). We are interested in the following research questions: (RQ1) Can multi-modal instruction tuning elicit world knowledge from LLMs? (RQ2) Can Englishonly instruction tuning generalize to other languages such as Chinese? and (RQ3) Can image-only multi-modal instruction tuning generalize to video-language tasks? For RQ1, we evaluate our models on three KVQA tasks in our datasets, i.e., OK-VQA [32], A-OKVQA [42] and ViQuAE. For RQ2 and RQ3, we perform zero-shot transfer evaluation on Chinese vision-language and video-language datasets, respectively. We use greedy decoding in inference if not otherwise specified.\n\nMetrics We adopt ROUGE-L [26] as an automatic metric to assess the consistency between predictions and ground-truth answers, focusing on evaluating the model's conversational abilities. As the automatic metric may not fully capture the nuances of conversational quality, we further introduce GPT-4 as a proxy of human evaluators ( \u00a7 4.2).\n\nBaselines We compare our models to recently proposed powerful multi-modal agents, including (1) BLIP-2-Flan-T5-XXL [23] where an instruction-tuned Flan-T5 [53] is connected with a powerful vision encoder to perform a series of multi-modal tasks; (2) MiniGPT-4 which aligns a CLIP visual encoder with a frozen Vicuna [5] with artificially collected dialog dataset; and (3) InstructBLIP, a recently proposed instruction tuning enhanced multi-modal agents with Vicuna-13B with converted multi-model datasets and the LLaVA [28] dataset generated by GPT-4.\n\n\nMain Results\n\n\nRQ1: Knowledgeable Visual Question Answer Evaluation\n\nThe results on the KVQA benchmarks are shown in Table 4. In comparison to the strongest baseline, our model achieves an improvement of 3.2 and 2.7 ROUGE-L points for OK-VQA and A-OKVQA, respectively. Additionally, Ying-VLM delivers the best performance on the held-out ViQuAE dataset. These findings indicate that instruction tuning on M 3 IT effectively harnesses knowledge from LLMs and elevates response quality.\n\n\nRQ2: Zero-Shot Transfer to Chinese Vision-Language Tasks\n\nWe assess models on three unseen Chinese vision-language tasks to investigate the cross-language generalization capabilities of instruction tuning. BLIP-2 is not considered, as Flan-T5 does not support Chinese. 2 As illustrated in Table 5 \n\n\nGPT-4 Evaluation Results\n\nTo further validate the quality of the generated response, we propose to utilize the powerful GPT-4 model as a proxy of human evaluators [38,12]. Specifically, following Vicuna [5], we use GPT-4 to rate the performance of different models against our Ying-VLM.\n\nConsidering the API cost of GPT-4, 300 examples are randomly sampled from OK-VQA, A-OKVQA and ViQuAE datasets as a subset for evaluation. For each sample, we construct a prompt consisting of the original question, its corresponding reference answer, the response generated by our Ying-VLM, and a baseline system output. GPT-4 is queried with the prompt to rate both responses on a scale of ten based on the given question and its reference answer. The ratings are primarily based on the accuracy, relevance, and naturalness of the response to meet the requirements when humans are interacting with multi-modal agents (see Appendix for the detailed evaluation template). We employ the strategy proposed by Wang et al. [51] to mitigate potential evaluation biases regarding the response order. 3 Figure 4 shows that our Ying-VLM outperforms all baseline models in most samples. Notably, Ying-VLM beat the strongest baseline MiniGPT4 on 167 over 300 tested samples. Consistent with the previous ROUGE-L evaluation, this result indicates that the model fine-tuned on our instruction dataset can produce more accurate and engaging responses on the challenging KVQA tasks.\n\n\nAnalysis\n\nWe investigate the effect of task number and instruction diversity on the performance of learned models, providing insights for future studies to utilize our benchmark better. Who plays at this stadium ?\n\nThe stadium you are looking at is called Emirates Stadium and it is the home of Arsenal Football Club.\n\nArsenal is a professional football club based in London, England, and they play in the Premier League.\n\nArsenal F.C.\n\nOn which book by this man, Baz luhrmann's planned a film?\n\nThe book on which Baz Luhrmann's planned film is based is called 'The Great Gatsby' and was written by F. Scott Fitzgerald. \n\n\nGreat Gatsby\n(K) \u85af\u6761 (G) \u626c\u5dde\u7092\u996d\nThe stadium is called Wembley Stadium, home to the English national football team.\n\nThe image shows a black and white portrait of a man in a suit and tie, with short, curly hair and a serious expression.\n\nThis is a bowl of fried rice with various ingredients including carrots, peas, sausage, shrimp, and eggs. It appears to be a Chinesestyle dish with a savory and slightly spicy flavor. \n\n\nEffect of Task Number\n\nWe investigate the influence of task numbers by randomly shuffling our tasks and then selecting a subset to train the model during the instruction tuning stage. Due to the computational resource limitation, we set up a maximum of 5k examples for each task and train all the models for 5k steps with a batch size of 64. We select 0, 4, 8, 16 and all 27 tasks for training, and report the individual ROUGE-L score and the average score. As illustrated in Figure 5, increasing the number of tasks greatly improves the results of the generalization performance. Besides, the performance gain is not diminished as the task number increases. This is promising as it indicates that we can continually improve performance by introducing more tasks into the training. It would be interesting to investigate the influence of different task clusters, which we leave for future studies.\n\nEffect Instruction Diversity To investigate the influence of instruction diversity, we limit the number of instructions used in each dataset to 1, 2, 4, and 8, resulting in varying levels of diversity for each task. The other training parameters are consistent with those used in previous experiments on task number investigation. Figure 6 shows that the performance varies with the level of diversity. Specifically, our results suggest that using four instructions per task is sufficient for achieving decent performance. We leave a more in-depth analysis of the instruction diversity for future work.\n\n\nQualitative Results\n\nWe conduct a case study to provide a more straightforward understanding of instruction-tuned models. The cases are chosen from the held-out ViQuAE and ChineseFoodNet datasets. As shown in Figure 7, our model generates accurate responses to all questions. In contrast, MiniGPT4 produces an incorrect answer for the stadium question on the left and fails to follow instructions in the subsequent cases, providing generic image descriptions instead. Additionally, compared to InstructBLIP, which provides concise but less engaging answers for the two questions requiring external knowledge, our model responds more naturally and engagingly, underlining the value of our dataset. Our model also successfully generalizes to Chinese inputs, accurately classifying the food image based on the instruction. These cases emphasize the importance of instruction tuning and demonstrate that our dataset can effectively enhance the capabilities of VLMs.\n\n\nConclusion\n\nIn this paper, we present M 3 IT, a multi-modal multilingual instruction tuning dataset for aiding the development of multi-modal large language models. The dataset comprises 2.4 million carefully curated instances and 400 manually written task instructions across 40 tasks. We build Ying-VLM to validate the effectiveness of our dataset. Quantitative and qualitative results demonstrate that the models trained with our datasets successfully follow human instructions, provide more engaging responses, and achieve strong generalization performance on unseen video and Chinese tasks. Further analysis shows that the increased task number can continually boost performance, and instruction diversity can influence results. We hope our proposed benchmark, trained models, and experimental findings can facilitate future studies toward building powerful multi-modal intelligent agents.  Table 7 lists the detailed statistics in our benchmark. We collect the dataset license from PaperWith-Code. 4 For datasets under Unknown and Custom licenses, we suggest the users check the project page or contact the dataset owner before usage.\n\n\nA Dataset Statistics\n\n\nB Template for Answer Paraphrase\n\nWe provide the paraphrase template in Table 8 for querying the ChatGPT to re-write the original short answers, where {Q} and {A} is filled with the question and the answer need to be paraphrased, respectively. We incorporate an example to better inform the model of the paraphrasing tasks. For VQAv2 tasks, we add an extra {Caption} field in the template filled with corresponding captions from the COCO dataset to provide extra context information to help to paraphrase. \n\n\nC Dataset Translation\n\nWe translate all the task instructions and evaluation sets of ImageNet, Winoground, VQAv2, OK-VQA, VIST, MSRVTT and MSRVTT-QA into 80 languages, as shown in Table 9. Due to the computational resource constraint, we translate the whole test of Winoground ( 800 examples) and set a maximum instance number of 500 for each split in other tasks.\n\n\nD Prompt for Zero-Shot Chinese Vision-Language Tasks\n\nIn our experiments, all Vision-Language models are fine-tuned exclusively using English data. In our preliminary study, we observe that these models tend to generate English responses, even when the input and instructions are written in Chinese. We introduce a simple Chinese dialogue context during the zero-shot Chinese Vision-Language Task evaluation for all models, as illustrated in Table 10, Interestingly, this minor adjustment can encourage models to produce reasonable Chinese output. We leave the analysis of instruction-tuned VLM models' multilingual capabilities for future research.\n\n\nE Template for GPT-4 Evaluation\n\nWe adopt the template in Table 11 to query GPT-4 and obtain the evaluation results with FairEval 5 to obtain more stable results. Specifically, each tested instance is a quaternion: (question, reference, response1, response2), where response1 and response2 are two responses from our Ying-VLM and the baseline model, respectively. For each instance, we query GPT-4 to judge which response is of better quality regarding accuracy, relevance and naturalness. We populate the quaternion into the evaluation template to form   [System] We would like to request your feedback on the performance of two AI assistants in response to the user's multimodal question displayed above. We provided no multimodal inputs other than question text, but we provided a reference answer for this question. You need to evaluate the quality of the two responses based on the question and the reference answer. Please rate the on the follow aspects: 1. Accuracy: whether the candidate's response is consistent with the original answer, this is important as we do not want a misleading result; 2. Relevance: whether the candidate's response is highly relevant to the question and image content; 3. Naturalness: whether the candidate's response is engaging, providing a great communication experience for the user when interacting with the AI visual assistant. of the two Assistants' responses.\n\nEach assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. Then, output two lines indicating the scores for Assistant 1 and 2, respectively.\n\nOutput with the following format: Evaluation evidence: <evaluation explanation here> The score of Assistant 1: <score> The score of Assistant 2: <score>\n\nFigure 1 :\n1Tasks in our proposed multi-modal multilingual instruction tuning dataset. The tasks in dashed white boxes are held-out evaluation sets that are not adopted during training. Tasks with bold names are translated into 80 languages.\n\n( 2 )\n2Short Answer Paraphrasing. As recent studies have shown that the original short and brief answers in the common\n\nFigure 2 :\n2(Left) On region-based tasks, bounding boxes are added to original images to inform the model of the area in interest. (Right) Short answer paraphrasing to improve the response quality.\n\nFigure 4 :\n4Evaluation results using GPT-4 as an evaluator. Our model outperforms MiniGPT-4 and InstructBLIP with a winning rate at 55.6% and 65.5%, respectively.\n\nFigure 5 :Figure 6 :\n56Performance increases with more instruction tuning datasets. Performance changes with the varied number of instructions used for training.\n\n\n\u8bf7\u7ed9\u56fe\u4e2d\u7684\u98df\u7269\u5206\u7c7b\u3002 (Classify the food in the image.) Options: (A) \u85af\u6761 (French fries) (B) \u51c9\u62cc\u897f\u7ea2\u67ff (Tomato salad) (C) \u6cb9\u7116\u5927\u867e (Braised Shrimp in chili oil) (D) \u6247\u8d1d (Scallop in Shell) (E) \u751f\u869d (Oysters) (F) \u8783\u87f9 (Crab) (G) \u626c\u5dde\u7092\u996d (Yangzhou fried rice) (H) \u62ab\u8428 (Pizza) (I) \u86cb\u631e (Egg Tart) (J) \u8089\u9171\u610f\u5927\u5229\u9762 (Spaghetti with meat sauce)\n\nFigure 7 :\n7Case study of the model outputs. Correct answers are bolded with green, wrong answers are in red and irrelevant answers are in grey. The model trained with our datasets can provide natural and informative responses to entity-centric questions, and generalize to the food classification task in Chinese (English translation for visualization only).\n\nTable 1 :\n1Summary of multi-modal instruction tuning datasets. Dataset # Tasks Multi-Lingual # of Instances Avg. # of Manual Instructions / Task Open-Sourced\n\nTable 2 :\n2The statistics of our instructions.Number of different instructions \n400 \n-Image Captioning \n52 \n-Classification \n113 \n-Visual Question Answering \n95 \n-Knowledgeable Visual QA \n40 \n-Reasoning \n60 \n-Generation \n40 \n\nTokens per instruction \n24.4 \u00b1 9.6 \n\n\n\nTable 3 :\n3M 3 IT task descriptions and statistics, encompassing image captioning (CAP), classification \n(CLS), visual question answering (VQA), knowledgeable visual question answering (KVQA), rea-\nsoning (REA), generation (GEN), Chinese vision-language, and video-language tasks. We aggregate \ninstance counts for training, validation, and test sets across all tasks, totaling 2,429,264 instances. \n\nTask \nDescription \nTotal #samples \nTrain \nVal \nTest \n\nCAP \nGiven an image, write a description for the image. \n679,087 \n41,462 \n27,499 \nCLS \nGiven an image, classify the image into pre-defined categories. \n238,303 100,069 \n21,206 \nVQA \nGiven an image, answer a question relevant to the image. \n177,633 \n46,314 \n10,828 \nKVQA \nGiven an image, answer the question requires outside knowledge. \n39,981 \n11,682 \n5,477 \nREA \nGiven an image, conduct reasoning over the images. \n99,372 \n11,500 \n10,000 \nGEN \nGiven an image, make compositions with certain requirements. \n145,000 \n11,315 \n17,350 \n\nChinese \nCAP, CLS, VQA, and GEN tasks in Chinese. \n192,076 \n77,306 \n4,100 \nVideo \nCAP, CLS, and VQA tasks on video-language datasets. \n20,868 \n7,542 \n9,294 \n\nMulti-lingual Translated tasks in 80 languages \n0 240,000 184,000 \n\n\n\nTable 4 :\n4ROUGE-L evaluation results of KVQA tasks. Our Ying-VLM outperforms all the baselines consistently.Model \nOK-VQA A-OKVQA ViQuAE \n\nBLIP2-Flan-T5-XXL \n9.1 \n15.6 \n9.7 \nMiniGPT4 \n23.3 \n21.8 \n24.4 \nInstructBLIP \n7.1 \n5.9 \n7.3 \nYing-VLM (Ours) \n27.5 \n24.5 \n29.6 \n\n\n\nTable 5 :\n5Zero-shot transfer to Chinese visionlanguage tasks. Our model generalizes well on unseen Chinese captioning, VQA and classification tasks, with the highest ROUGE-L.Model \nFlickr-8k-CN FM-IQA Chinese-FoodNet \n\nMiniGPT4 \n9.6 \n20.1 \n5.0 \nInstructBLIP \n5.2 \n2.3 \n1.0 \nYing-VLM (Ours) \n20.5 \n33.3 \n49.8 \n\n\n\nTable 6 :\n6Zero-shot transfer to video-language tasks. We report ROUGE-L score for all tasks.Model \nVideo Captioning \nVideo Question Answer \n\nMSRVTT \niVQA ActivityNet-QA MSRVTT-QA MSVD-QA \n\nBLIP-2-Flan-T5-XXL \n8.8 \n11.1 \n8.9 \n10.3 \n13.2 \nInstructBLIP \n14.3 \n6.3 \n9.3 \n4.0 \n7.0 \nYing-VLM (Ours) \n14.2 \n23.5 \n21.9 \n18.3 \n21.4 \n\n\n\n\n, our model outperforms MiniGPT4 and InstructBLIP on all evaluated tasks, demonstrating notable improvements. These findings indicate that instruction tuning with English datasets can effectively generalize to different languages, showcasing the promising potential that can be further explored.RQ3: Zero-Shot Transfer to Video-Language Tasks To evaluate performance on video-language tasks, we uniformly sample 8 frames from each video. A comparison with MiniGPT4 is excluded, as it does not support video inputs. Following the approach of InstructBLIP[7], we concatenate the visual embedding extracted from the Q-former of each frame as a prefix embedding to the language model. As demonstrated inTable 6, our model excels in these challenging settings, significantly surpassing the BLIP-series baselines. It is worth noting that the training dataset does not include any visual inputs such as videos, implying that our instruction tuning effectively aids the model in generalizing to video inputs with a temporal dimension.196 \n\n167 \n\n9 \n\n28 \n\n95 \n\n105 \n\n0% \n20% \n40% \n60% \n80% \n100% \n\nInstructBLIP \n\nMiniGPT4 \n\nYing-VLM Won \nTie \nYing-VLM Lost \n\n\n\nTable 7 :\n7Detailed task descriptions and statistics of our instruction tuning tasks, including all datasets in all types of tasks. The column \"Used\" indicates whether we use this dataset in the instruction tuning stage.Task \nDataset \nUsed \n#samples \nLicense \nTrain \nVal \nTest \n\nCaptioning \n\nMS COCO [27] \nYes \n566,747 25,010 25,010 \nCustom \nTextCaps [44] \nYes \n97,765 \n13,965 \n0 \nUnknown \nImage-Paragraph-Captioning [21] \nYes \n14,575 \n2,487 \n2,489 \nCustom \n\nClassification \n\nCOCO-GOI [27] \nYes \n30,000 \n2,000 \n0 \nCustom \nCOCO-Text [50] \nYes \n118,312 27,550 \n0 \nCustom \nImageNet [40] \nYes \n30,000 \n50,000 \n0 \nNon-commercial \nCOCO-ITM [27] \nYes \n30,000 \n5,000 \n5,000 \nCustom \ne-SNLI-VE [20] \nYes \n20,000 \n14,339 14,740 \nUnknown \nMocheg [58] \nYes \n4,991 \n180 \n466 \nCC BY 4.0 \nIQA [9] \nYes \n5,000 \n1,000 \n1,000 \nCustom \n\nVQA \n\nVQA v2 [15] \nYes \n30,000 \n30,000 \n0 \nCC-BY 4.0 \nShapes VQA [1] \nYes \n13,568 \n1,024 \n1,024 \nUnknown \nDocVQA [33] \nYes \n39,463 \n5,349 \n0 \nUnknown \nOCR-VQA [34] \nYes \n11,414 \n4,940 \n0 \nUnknown \nST-VQA [2] \nYes \n26,074 \n0 \n4,070 \nUnknown \nText-VQA [45] \nYes \n27,113 \n0 \n5,734 \nCC BY 4.0 \nGQA [18] \nYes \n30,001 \n5,001 \n0 \nCC BY 4.0 \n\nKVQA \n\nOK-VQA [32] \nYes \n9,009 \n5,046 \n0 \nUnknown \nA-OK-VQA [42] \nYes \n17,056 \n1,145 \n0 \nUnknown \nScienceQA [31] \nYes \n12,726 \n4,241 \n4,241 \nCC BY-NC-SA \nViQuAE [22] \nNo \n1,190 \n1,250 \n1,236 \nCC By 4.0 \n\nReasoning \n\nCLEVR [19] \nYes \n30,000 \n2,000 \n0 \nCC BY 4.0 \nNLVR [46] \nYes \n29,372 \n2,000 \n0 \nUnknown \nVCR [60] \nYes \n25,000 \n5,000 \n5,000 \nCustom \nVisualMRC [47] \nYes \n15,000 \n2,500 \n5,000 \nUnknown \nWinoground [48] \nNo \n0 \n0 \n800 \nUnknown \n\nGeneration \n\nVisual Storytelling [17] \nYes \n5,000 \n4,315 \n4,350 \nUnknown \nVisual Dialog [8] \nYes \n50,000 \n1,000 \n1,000 \nCC By 4.0 \nMulti30k [10] \nYes \n90,000 \n6,000 \n12,000 Non-commercial \n\nChinese \n\nFM-IQA [11] \nNo \n164,735 75,206 \n0 \nUnknown \nCOCO-Caption CN [25] \nNo \n18,341 \n1,000 \n1,000 \nNon-commercial \nFlickr-8k-Caption CN [24] \nNo \n6,000 \n1,000 \n1,000 \nCC By 3.0 \nChinese Food Classification [4] \nNo \n0 \n0 \n1,100 \nUnknown \nMultimodal Chat [62] \nNo \n3,000 \n1,000 \n1,000 \nUnknown \n\nVideo \n\nAction-Classification [14] \nNo \n2,000 \n2,000 \n2,000 \nCustom \niVQA [57] \nNo \n5,994 \n2,000 \n2,000 \nUnknown \nMSVD QA [54] \nNo \n1,161 \n245 \n504 \nUnknown \nActivityNet QA [59] \nNo \n3,200 \n1,800 \n800 \nUnknown \nMSRVTT QA [54] \nNo \n6,513 \n497 \n2,990 \nUnknown \nMSRVTT Captioning [55] \nNo \n2,000 \n1,000 \n1,000 \nUnknown \n\n\n\nTable 8 :\n8Template used to query ChatGPT for answer paraphrasing.You are an AI visual assistant. Now you are given a question related to an image and a short ground-truth answer. Your task is to transform the ground-truth answer into a natural and convincing response. Make sure the response is accurate, highly relevant to the question, and consistent with the original answer. NASA sent the Magellan spacecraft to Venus in 1989, which was the first planetary spacecraft launched from a space shuttle.Question: \nWhich NASA space probe was launched to this planet in 1989? \nAnswer: \nMagellan \nTransformed Answer: \nQuestion: \n{Q} \nAnswer: \n{A} \nTransformed Answer: \n\n\n\nTable 9 :\n9List of Language Codes, Scripts, and Languages Names for translated datasets.\n\nTable 10 :\n10Prompt for promoting Chinese outputs. receive 6 scores, and we use the average score as the final score for each response. The response with the higher final score is considered the better response. The GPT-4 evaluation incurred a cost of $20.45 for InstructBlip and $20.90 for MiniGPT-4.<human>: \u8bf7\u6839\u636e\u6211\u7684\u6307\u793a\uff0c\u4ee5\u53ca\u6240\u7ed9\u7684\u56fe\u7247\uff0c\u505a\u51fa\u76f8\u5e94\u7684\u56de\u7b54\u3002 \n<bot>: \n\u597d\u7684\u3002 \n<human>: \n{Instruction} \n{Input} \n<bot>: \n\u597d\u7684\u3002 \n\n\n\nTable 11 :\n11Template used to query GPT-4 for evaluating the response quality of different models. The Start of Reference Answer] {R} [The End of Reference Answer] [The Start of Assistant 1's Answer] {R1} [The End of Assistant 1's Answer] [The Start of Assistant 2's Answer] {R2} [The End of Assistant 2's Answer][Question] \n{Q} \n[\nOur dataset is available at https://huggingface.co/datasets/MMInstruction/M3IT Preprint. Under review. arXiv:2306.04387v2 [cs.CV] 8 Jun 2023\nFor all models, we introduce a prompt to promote Chinese outputs. See Appendix D for details.\nhttps://github.com/i-Eval/FairEval\nhttps://paperswithcode.com/\nhttps://github.com/i-Eval/FairEval\ntwo query prompts: T(Q=question, R=reference, R1=response1, R2=response2) and T(Q=question, R=reference, R1=response2, R2=response1). We set the temperature of GPT-4 to 1 and sample three completions for each query prompt. Therefore, each response will\nNeural module networks. J Andreas, M Rohrbach, T Darrell, D Klein, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAJ. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 39-48, 2016.\n\nScene text visual question answering. A F Biten, R Tito, A Mafla, L G Bigorda, M Rusi\u00f1ol, C V Jawahar, E Valveny, D Karatzas, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). A. F. Biten, R. Tito, A. Mafla, L. G. i Bigorda, M. Rusi\u00f1ol, C. V. Jawahar, E. Valveny, and D. Karatzas. Scene text visual question answering. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pages 4290-4300, 2019.\n\nLanguage models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. LinNeurIPST. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan- guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nChinesefoodnet: A large-scale image dataset for chinese food recognition. X Chen, Y Zhu, H Zhou, L Diao, D Wang, abs/1705.02743ArXiv preprintX. Chen, Y. Zhu, H. Zhou, L. Diao, and D. Wang. Chinesefoodnet: A large-scale image dataset for chinese food recognition. ArXiv preprint, abs/1705.02743, 2017.\n\nW.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n\nNo language left behind: Scaling human-centered machine translation. M R Costa-Juss\u00e0, J Cross, O \u00c7elebi, M Elbayad, K Heafield, K Heffernan, E Kalbassi, J Lam, D Licht, J Maillard, ArXiv preprint, abs/2207.04672, 2022M. R. Costa-juss\u00e0, J. Cross, O. \u00c7elebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, et al. No language left behind: Scaling human-centered machine translation. ArXiv preprint, abs/2207.04672, 2022.\n\nInstructblip: Towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A M H Tiong, J Zhao, W Wang, B Li, P Fung, S Hoi, ArXiv preprint, abs/2305.06500, 2023W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. ArXiv preprint, abs/2305.06500, 2023.\n\nVisual dialog. A Das, S Kottur, K Gupta, A Singh, D Yadav, J M F Moura, D Parikh, D Batra, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAA. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. F. Moura, D. Parikh, and D. Batra. Visual dialog. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1080-1089, 2017.\n\nQuantifying visual image quality: A bayesian view. Z Duanmu, W Liu, Z Wang, Z Wang, Annual Review of Vision Science. 7Z. Duanmu, W. Liu, Z. Wang, and Z. Wang. Quantifying visual image quality: A bayesian view. Annual Review of Vision Science, 7:437-464, 2021.\n\nMulti30K: Multilingual English-German image descriptions. D Elliott, S Frank, K , L Specia, Proceedings of the 5th Workshop on Vision and Language. the 5th Workshop on Vision and LanguageD. Elliott, S. Frank, K. Sima'an, and L. Specia. Multi30K: Multilingual English-German image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70-74, 2016.\n\nAre you talking to a machine? dataset and methods for multilingual image question. H Gao, J Mao, J Zhou, Z Huang, L Wang, W Xu, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. GarnettMontreal, Quebec, CanadaH. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine? dataset and methods for multilingual image question. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2296-2304, 2015.\n\nChatgpt outperforms crowd-workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, abs/2303.15056ArXiv preprintF. Gilardi, M. Alizadeh, and M. Kubli. Chatgpt outperforms crowd-workers for text-annotation tasks. ArXiv preprint, abs/2303.15056, 2023.\n\nThe Flores-101 evaluation benchmark for low-resource and multilingual machine translation. N Goyal, C Gao, V Chaudhary, P.-J Chen, G Wenzek, D Ju, S Krishnan, M Ranzato, F Guzm\u00e1n, A Fan, Transactions of the Association for Computational Linguistics. 10N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzm\u00e1n, and A. Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522-538, 2022.\n\nThe \"something something\" video database for learning and evaluating visual common sense. R Goyal, S E Kahou, V Michalski, J Materzynska, S Westphal, H Kim, V Haenel, I Fr\u00fcnd, P Yianilos, M Mueller-Freitag, F Hoppe, C Thurau, I Bax, R Memisevic, IEEE International Conference on Computer Vision. Venice, ItalyR. Goyal, S. E. Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fr\u00fcnd, P. Yianilos, M. Mueller-Freitag, F. Hoppe, C. Thurau, I. Bax, and R. Memisevic. The \"some- thing something\" video database for learning and evaluating visual common sense. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 5843-5851, 2017.\n\nMaking the V in VQA matter: Elevating the role of image understanding in visual question answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAY. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Confer- ence on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6325-6334, 2017.\n\nLora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n\nVisual storytelling. T.-H K Huang, F Ferraro, N Mostafazadeh, I Misra, A Agrawal, J Devlin, R Girshick, X He, P Kohli, D Batra, C L Zitnick, D Parikh, L Vanderwende, M Galley, M Mitchell, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesT.-H. K. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, A. Agrawal, J. Devlin, R. Girshick, X. He, P. Kohli, D. Batra, C. L. Zitnick, D. Parikh, L. Vanderwende, M. Galley, and M. Mitchell. Visual storytelling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1233-1239, 2016.\n\nGQA: A new dataset for real-world visual reasoning and compositional question answering. D A Hudson, C D Manning, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAD. A. Hudson and C. D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6700-6709, 2019.\n\nCLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C L Zitnick, R B Girshick, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAJ. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1988-1997, 2017.\n\ne-vil: A dataset and benchmark for natural language explanations in vision-language tasks. M Kayser, O Camburu, L Salewski, C Emde, V Do, Z Akata, T Lukasiewicz, 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaM. Kayser, O. Camburu, L. Salewski, C. Emde, V. Do, Z. Akata, and T. Lukasiewicz. e-vil: A dataset and benchmark for natural language explanations in vision-language tasks. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1224-1234, 2021.\n\nA hierarchical approach for generating descriptive image paragraphs. J Krause, J Johnson, R Krishna, L Fei-Fei, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAJ. Krause, J. Johnson, R. Krishna, and L. Fei-Fei. A hierarchical approach for generating descriptive image paragraphs. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 3337-3345, 2017.\n\nViquae, a dataset for knowledge-based visual question answering about named entities. P Lerner, O Ferret, C Guinaudeau, H Le Borgne, R Besan\u00e7on, J G Moreno, J Lov\u00f3n Melgarejo, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalP. Lerner, O. Ferret, C. Guinaudeau, H. Le Borgne, R. Besan\u00e7on, J. G. Moreno, and J. Lov\u00f3n Mel- garejo. Viquae, a dataset for knowledge-based visual question answering about named entities. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3108-3120, 2022.\n\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, abs/2301.12597ArXiv preprintJ. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv preprint, abs/2301.12597, 2023.\n\nAdding chinese captions to images. X Li, W Lan, J Dong, H Liu, Proceedings of the 2016 ACM on international conference on multimedia retrieval. the 2016 ACM on international conference on multimedia retrievalX. Li, W. Lan, J. Dong, and H. Liu. Adding chinese captions to images. In Proceedings of the 2016 ACM on international conference on multimedia retrieval, pages 271-275, 2016.\n\nCoco-cn for cross-lingual image tagging, captioning, and retrieval. X Li, C Xu, X Wang, W Lan, Z Jia, G Yang, J Xu, IEEE Transactions on Multimedia. 219X. Li, C. Xu, X. Wang, W. Lan, Z. Jia, G. Yang, and J. Xu. Coco-cn for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21(9):2347-2360, 2019.\n\nROUGE: A package for automatic evaluation of summaries. C.-Y. Lin, Text Summarization Branches Out. C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, 2004.\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerProceedings, Part V 13T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.\n\nVisual instruction tuning. ArXiv preprint. H Liu, C Li, Q Wu, Y J Lee, abs/2304.08485H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. ArXiv preprint, abs/2304.08485, 2023.\n\nThe flan collection: Designing data and methods for effective instruction tuning. S Longpre, L Hou, T Vu, A Webson, H W Chung, Y Tay, D Zhou, Q V Le, B Zoph, J Wei, ArXiv preprint, abs/2301.13688, 2023S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. ArXiv preprint, abs/2301.13688, 2023.\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nLearn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\n\nOK-VQA: A visual question answering benchmark requiring external knowledge. K Marino, M Rastegari, A Farhadi, R Mottaghi, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAK. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3195-3204, 2019.\n\nDocvqa: A dataset for vqa on document images. M Mathew, D Karatzas, C Jawahar, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer visionM. Mathew, D. Karatzas, and C. Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2200-2209, 2021.\n\nOcr-vqa: Visual question answering by reading text in images. A Mishra, S Shekhar, A K Singh, A Chakraborty, 2019 international conference on document analysis and recognition (ICDAR). IEEEA. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947-952. IEEE, 2019.\n\nCross-task generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, 2022.\n\n. OpenAI. Introducing chatgpt. OpenAI. Introducing chatgpt. 2022.\n\n. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.\n\n. B Peng, C Li, P He, M Galley, J Gao, Instruction tuning with gpt-4. ArXiv preprint, abs/2304.03277, 2023B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. ArXiv preprint, abs/2304.03277, 2023.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M. Meila and T. Zhangthe 38th International Conference on Machine Learning, ICML 2021139A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748-8763, 2021.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International journal of computer vision. 115O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211-252, 2015.\n\nLaion-400m: Open dataset of clip-filtered 400 million image-text pairs. C Schuhmann, R Vencu, R Beaumont, R Kaczmarczyk, C Mullis, A Katta, T Coombes, J Jitsev, A Komatsuzaki, ArXiv preprint, abs/2111.02114, 2021C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. ArXiv preprint, abs/2111.02114, 2021.\n\nA-okvqa: A benchmark for visual question answering using world knowledge. D Schwenk, A Khandelwal, C Clark, K Marino, R Mottaghi, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part VIIID. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII, pages 146-162. Springer, 2022.\n\nWhat does clip know about a red circle? visual prompt engineering for vlms. A Shtedritski, C Rupprecht, A Vedaldi, abs/2304.06712ArXiv preprintA. Shtedritski, C. Rupprecht, and A. Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms. ArXiv preprint, abs/2304.06712, 2023.\n\nTextcaps: a dataset for image captioning with reading comprehension. O Sidorov, R Hu, M Rohrbach, A Singh, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part II 16O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16, pages 742-758. Springer, 2020.\n\nTowards VQA models that can read. A Singh, V Natarajan, M Shah, Y Jiang, X Chen, D Batra, D Parikh, M Rohrbach, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAA. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 8317-8326, 2019.\n\nA corpus of natural language for visual reasoning. A Suhr, M Lewis, J Yeh, Y Artzi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsShort Papers2A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, 2017.\n\nVisualmrc: Machine reading comprehension on document images. R Tanaka, K Nishida, S Yoshida, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. R. Tanaka, K. Nishida, and S. Yoshida. Visualmrc: Machine reading comprehension on document images. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 13878-13888, 2021.\n\nWinoground: Probing vision and language models for visio-linguistic compositionality. T Thrush, R Jiang, M Bartolo, A Singh, A Williams, D Kiela, C Ross, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionT. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238-5248, 2022.\n\nLlama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi\u00e8re, N Goyal, E Hambro, F Azhar, ArXiv preprint, abs/2302.13971, 2023H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023.\n\nCoco-text: Dataset and benchmark for text detection and recognition in natural images. A Veit, T Matera, L Neumann, J Matas, S Belongie, abs/1601.07140ArXiv preprintA. Veit, T. Matera, L. Neumann, J. Matas, and S. Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. ArXiv preprint, abs/1601.07140, 2016.\n\nLarge language models are not fair evaluators. P Wang, L Li, L Chen, D Zhu, B Lin, Y Cao, Q Liu, T Liu, Z Sui, ArXiv preprint, abs/2305.17926, 2023P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui. Large language models are not fair evaluators. ArXiv preprint, abs/2305.17926, 2023.\n\nSuper-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Naik, A Ashok, A S Dhanasekaran, A Arunkumar, D Stap, E Pathak, G Karamanolakis, H Lai, I Purohit, I Mondal, J Anderson, K Kuznia, K Doshi, K K Pal, M Patel, M Moradshahi, M Parmar, M Purohit, N Varshney, P R Kaza, P Verma, R S Puri, R Karia, S Doshi, S K Sampat, S Mishra, S Reddy, A , S Patro, T Dixit, X Shen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingY. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mon- dal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen. Super-NaturalInstructions: Generaliza- tion via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, 2022.\n\nFinetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n\nVideo question answering via gradually refined attention over appearance and motion. D Xu, Z Zhao, J Xiao, F Wu, H Zhang, X He, Y Zhuang, Proceedings of the 2017 ACM on Multimedia Conference. the 2017 ACM on Multimedia ConferenceMountain View, CA, USAD. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA, USA, October 23-27, 2017, pages 1645-1653, 2017.\n\nMSR-VTT: A large video description dataset for bridging video and language. J Xu, T Mei, T Yao, Y Rui, 2016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USAJ. Xu, T. Mei, T. Yao, and Y. Rui. MSR-VTT: A large video description dataset for bridging video and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 5288-5296, 2016.\n\nMultiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Z Xu, Y Shen, L Huang, abs/2212.10773ArXiv preprintZ. Xu, Y. Shen, and L. Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. ArXiv preprint, abs/2212.10773, 2022.\n\nJust ask: Learning to answer questions from millions of narrated videos. A Yang, A Miech, J Sivic, I Laptev, C Schmid, 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaA. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid. Just ask: Learning to answer questions from millions of narrated videos. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1666-1677, 2021.\n\nEnd-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. B M Yao, A Shah, L Sun, J.-H Cho, L Huang, abs/2205.12487ArXiv preprintB. M. Yao, A. Shah, L. Sun, J.-H. Cho, and L. Huang. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. ArXiv preprint, abs/2205.12487, 2022.\n\nActivitynet-qa: A dataset for understanding complex web videos via question answering. Z Yu, D Xu, J Yu, T Yu, Z Zhao, Y Zhuang, D Tao, The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. Honolulu, Hawaii, USA2019Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pages 9127-9134, 2019.\n\nFrom recognition to cognition: Visual commonsense reasoning. R Zellers, Y Bisk, A Farhadi, Y Choi, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAR. Zellers, Y. Bisk, A. Farhadi, and Y. Choi. From recognition to cognition: Visual commonsense reasoning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6720-6731, 2019.\n\n. J Zhang, R Gan, J Wang, Y Zhang, L Zhang, P Yang, X Gao, Z Wu, X Dong, J He, J Zhuo, Q Yang, Y Huang, X Li, Y Wu, J Lu, X Zhu, W Chen, T Han, K Pan, R Wang, H Wang, X Wu, Z Zeng, C Chen, Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. ArXiv preprint, abs/2209.02970, 2022J. Zhang, R. Gan, J. Wang, Y. Zhang, L. Zhang, P. Yang, X. Gao, Z. Wu, X. Dong, J. He, J. Zhuo, Q. Yang, Y. Huang, X. Li, Y. Wu, J. Lu, X. Zhu, W. Chen, T. Han, K. Pan, R. Wang, H. Wang, X. Wu, Z. Zeng, and C. Chen. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. ArXiv preprint, abs/2209.02970, 2022.\n\nMMChat: Multi-modal chat dataset on social media. Y Zheng, G Chen, X Liu, J Sun, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceY. Zheng, G. Chen, X. Liu, and J. Sun. MMChat: Multi-modal chat dataset on social media. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 5778- 5786, 2022.\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, abs/2304.10592ArXiv preprintD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv preprint, abs/2304.10592, 2023.\n", "annotations": {"author": "[{\"end\":229,\"start\":85},{\"end\":232,\"start\":230},{\"end\":380,\"start\":233},{\"end\":393,\"start\":381},{\"end\":396,\"start\":394},{\"end\":408,\"start\":397},{\"end\":420,\"start\":409},{\"end\":433,\"start\":421},{\"end\":461,\"start\":434},{\"end\":612,\"start\":462},{\"end\":668,\"start\":613},{\"end\":676,\"start\":669},{\"end\":828,\"start\":677},{\"end\":989,\"start\":829}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":89},{\"end\":242,\"start\":239},{\"end\":392,\"start\":390},{\"end\":407,\"start\":403},{\"end\":419,\"start\":415},{\"end\":432,\"start\":429},{\"end\":442,\"start\":440},{\"end\":474,\"start\":470},{\"end\":624,\"start\":622},{\"end\":675,\"start\":672},{\"end\":690,\"start\":686},{\"end\":835,\"start\":832}]", "author_first_name": "[{\"end\":88,\"start\":85},{\"end\":231,\"start\":230},{\"end\":238,\"start\":233},{\"end\":389,\"start\":381},{\"end\":395,\"start\":394},{\"end\":402,\"start\":397},{\"end\":414,\"start\":409},{\"end\":428,\"start\":421},{\"end\":439,\"start\":434},{\"end\":469,\"start\":462},{\"end\":621,\"start\":613},{\"end\":671,\"start\":669},{\"end\":685,\"start\":677},{\"end\":831,\"start\":829}]", "author_affiliation": "[{\"end\":228,\"start\":93},{\"end\":379,\"start\":244},{\"end\":460,\"start\":444},{\"end\":611,\"start\":476},{\"end\":667,\"start\":651},{\"end\":827,\"start\":692},{\"end\":988,\"start\":853}]", "title": "[{\"end\":82,\"start\":1},{\"end\":1071,\"start\":990}]", "venue": null, "abstract": "[{\"end\":2273,\"start\":1073}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2408,\"start\":2405},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2411,\"start\":2408},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2414,\"start\":2411},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2487,\"start\":2483},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2490,\"start\":2487},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2815,\"start\":2811},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3016,\"start\":3012},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3019,\"start\":3016},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3021,\"start\":3019},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3999,\"start\":3996},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4844,\"start\":4840},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4887,\"start\":4883},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4912,\"start\":4908},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5011,\"start\":5008},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":5014,\"start\":5011},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5017,\"start\":5014},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5167,\"start\":5163},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5689,\"start\":5686},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6352,\"start\":6348},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6355,\"start\":6352},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6468,\"start\":6464},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6471,\"start\":6468},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7011,\"start\":7007},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7030,\"start\":7026},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7151,\"start\":7147},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7259,\"start\":7256},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8584,\"start\":8580},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8651,\"start\":8647},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8777,\"start\":8773},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8923,\"start\":8919},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8937,\"start\":8933},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9000,\"start\":8996},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9043,\"start\":9039},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9102,\"start\":9098},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9373,\"start\":9369},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9389,\"start\":9386},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9402,\"start\":9398},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9416,\"start\":9412},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9428,\"start\":9425},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9443,\"start\":9439},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9457,\"start\":9453},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9762,\"start\":9758},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9780,\"start\":9776},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9796,\"start\":9792},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9858,\"start\":9854},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10027,\"start\":10023},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10075,\"start\":10071},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10091,\"start\":10087},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10128,\"start\":10124},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10144,\"start\":10140},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10185,\"start\":10181},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10198,\"start\":10195},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10536,\"start\":10532},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10565,\"start\":10562},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10616,\"start\":10612},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10825,\"start\":10821},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10847,\"start\":10843},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10868,\"start\":10864},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10905,\"start\":10902},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10941,\"start\":10937},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11141,\"start\":11137},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11178,\"start\":11174},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11199,\"start\":11195},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11210,\"start\":11206},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11227,\"start\":11223},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11282,\"start\":11278},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14008,\"start\":14004},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14048,\"start\":14044},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14959,\"start\":14956},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14999,\"start\":14995},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16531,\"start\":16527},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16714,\"start\":16711},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19468,\"start\":19464},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19554,\"start\":19550},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19665,\"start\":19661},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19689,\"start\":19685},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19958,\"start\":19954},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20075,\"start\":20071},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20649,\"start\":20645},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21542,\"start\":21538},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21556,\"start\":21552},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21791,\"start\":21787},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22221,\"start\":22217},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22261,\"start\":22257},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22421,\"start\":22418},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22625,\"start\":22621},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23413,\"start\":23412},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23610,\"start\":23606},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23613,\"start\":23610},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23649,\"start\":23646},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24452,\"start\":24448},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24524,\"start\":24523},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29417,\"start\":29416},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37725,\"start\":37722}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33306,\"start\":33064},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33426,\"start\":33307},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33625,\"start\":33427},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33789,\"start\":33626},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33952,\"start\":33790},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34254,\"start\":33953},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34615,\"start\":34255},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34774,\"start\":34616},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35039,\"start\":34775},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36255,\"start\":35040},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36525,\"start\":36256},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36838,\"start\":36526},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37166,\"start\":36839},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38319,\"start\":37167},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":40724,\"start\":38320},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41393,\"start\":40725},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":41483,\"start\":41394},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":41882,\"start\":41484},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":42215,\"start\":41883}]", "paragraph": "[{\"end\":3595,\"start\":2289},{\"end\":4690,\"start\":3597},{\"end\":6087,\"start\":4692},{\"end\":6738,\"start\":6270},{\"end\":7911,\"start\":6740},{\"end\":8255,\"start\":7975},{\"end\":8459,\"start\":8273},{\"end\":8833,\"start\":8461},{\"end\":9182,\"start\":8835},{\"end\":9458,\"start\":9184},{\"end\":9915,\"start\":9460},{\"end\":10360,\"start\":9917},{\"end\":10634,\"start\":10362},{\"end\":10957,\"start\":10636},{\"end\":11315,\"start\":10982},{\"end\":11603,\"start\":11317},{\"end\":12146,\"start\":11626},{\"end\":12347,\"start\":12296},{\"end\":12762,\"start\":12349},{\"end\":13103,\"start\":12764},{\"end\":14326,\"start\":13105},{\"end\":14432,\"start\":14328},{\"end\":14491,\"start\":14434},{\"end\":14608,\"start\":14504},{\"end\":14780,\"start\":14631},{\"end\":15382,\"start\":14825},{\"end\":16202,\"start\":15384},{\"end\":17130,\"start\":16204},{\"end\":18271,\"start\":17149},{\"end\":19027,\"start\":18273},{\"end\":19376,\"start\":19043},{\"end\":20379,\"start\":19402},{\"end\":21760,\"start\":20381},{\"end\":22100,\"start\":21762},{\"end\":22653,\"start\":22102},{\"end\":23140,\"start\":22725},{\"end\":23440,\"start\":23201},{\"end\":23729,\"start\":23469},{\"end\":24897,\"start\":23731},{\"end\":25113,\"start\":24910},{\"end\":25217,\"start\":25115},{\"end\":25321,\"start\":25219},{\"end\":25335,\"start\":25323},{\"end\":25394,\"start\":25337},{\"end\":25520,\"start\":25396},{\"end\":25634,\"start\":25552},{\"end\":25755,\"start\":25636},{\"end\":25941,\"start\":25757},{\"end\":26841,\"start\":25967},{\"end\":27445,\"start\":26843},{\"end\":28409,\"start\":27469},{\"end\":29552,\"start\":28424},{\"end\":30084,\"start\":29612},{\"end\":30451,\"start\":30110},{\"end\":31103,\"start\":30508},{\"end\":32509,\"start\":31139},{\"end\":32909,\"start\":32511},{\"end\":33063,\"start\":32911}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6269,\"start\":6103},{\"attributes\":{\"id\":\"formula_1\"},\"end\":25551,\"start\":25536}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7818,\"start\":7811},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12698,\"start\":12691},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18139,\"start\":18132},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22780,\"start\":22773},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23439,\"start\":23432},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29315,\"start\":29308},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29657,\"start\":29650},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":30274,\"start\":30267},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30904,\"start\":30896},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31172,\"start\":31164}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2287,\"start\":2275},{\"attributes\":{\"n\":\"2\"},\"end\":6102,\"start\":6090},{\"attributes\":{\"n\":\"3\"},\"end\":7973,\"start\":7914},{\"attributes\":{\"n\":\"3.1\"},\"end\":8271,\"start\":8258},{\"end\":10980,\"start\":10960},{\"attributes\":{\"n\":\"3.2\"},\"end\":11624,\"start\":11606},{\"end\":12174,\"start\":12149},{\"end\":12192,\"start\":12177},{\"end\":12212,\"start\":12195},{\"end\":12229,\"start\":12215},{\"end\":12262,\"start\":12232},{\"end\":12294,\"start\":12265},{\"end\":14502,\"start\":14494},{\"end\":14629,\"start\":14611},{\"end\":14789,\"start\":14783},{\"end\":14823,\"start\":14792},{\"attributes\":{\"n\":\"3.3\"},\"end\":17147,\"start\":17133},{\"attributes\":{\"n\":\"4\"},\"end\":19041,\"start\":19030},{\"attributes\":{\"n\":\"4.1\"},\"end\":19400,\"start\":19379},{\"attributes\":{\"n\":\"4.2\"},\"end\":22668,\"start\":22656},{\"end\":22723,\"start\":22671},{\"end\":23199,\"start\":23143},{\"end\":23467,\"start\":23443},{\"attributes\":{\"n\":\"4.3\"},\"end\":24908,\"start\":24900},{\"end\":25535,\"start\":25523},{\"end\":25965,\"start\":25944},{\"end\":27467,\"start\":27448},{\"attributes\":{\"n\":\"5\"},\"end\":28422,\"start\":28412},{\"end\":29575,\"start\":29555},{\"end\":29610,\"start\":29578},{\"end\":30108,\"start\":30087},{\"end\":30506,\"start\":30454},{\"end\":31137,\"start\":31106},{\"end\":33075,\"start\":33065},{\"end\":33313,\"start\":33308},{\"end\":33438,\"start\":33428},{\"end\":33637,\"start\":33627},{\"end\":33811,\"start\":33791},{\"end\":34266,\"start\":34256},{\"end\":34626,\"start\":34617},{\"end\":34785,\"start\":34776},{\"end\":35050,\"start\":35041},{\"end\":36266,\"start\":36257},{\"end\":36536,\"start\":36527},{\"end\":36849,\"start\":36840},{\"end\":38330,\"start\":38321},{\"end\":40735,\"start\":40726},{\"end\":41404,\"start\":41395},{\"end\":41495,\"start\":41485},{\"end\":41894,\"start\":41884}]", "table": "[{\"end\":35039,\"start\":34822},{\"end\":36255,\"start\":35052},{\"end\":36525,\"start\":36366},{\"end\":36838,\"start\":36702},{\"end\":37166,\"start\":36933},{\"end\":38319,\"start\":38195},{\"end\":40724,\"start\":38541},{\"end\":41393,\"start\":41229},{\"end\":41882,\"start\":41786},{\"end\":42215,\"start\":42197}]", "figure_caption": "[{\"end\":33306,\"start\":33077},{\"end\":33426,\"start\":33315},{\"end\":33625,\"start\":33440},{\"end\":33789,\"start\":33639},{\"end\":33952,\"start\":33814},{\"end\":34254,\"start\":33955},{\"end\":34615,\"start\":34268},{\"end\":34774,\"start\":34628},{\"end\":34822,\"start\":34787},{\"end\":36366,\"start\":36268},{\"end\":36702,\"start\":36538},{\"end\":36933,\"start\":36851},{\"end\":38195,\"start\":37169},{\"end\":38541,\"start\":38332},{\"end\":41229,\"start\":40737},{\"end\":41483,\"start\":41406},{\"end\":41786,\"start\":41498},{\"end\":42197,\"start\":41897}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11337,\"start\":11329},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15318,\"start\":15310},{\"end\":17923,\"start\":17915},{\"end\":18938,\"start\":18930},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21121,\"start\":21113},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24533,\"start\":24525},{\"end\":26428,\"start\":26420},{\"end\":27182,\"start\":27174},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27665,\"start\":27657}]", "bib_author_first_name": "[{\"end\":42827,\"start\":42826},{\"end\":42838,\"start\":42837},{\"end\":42850,\"start\":42849},{\"end\":42861,\"start\":42860},{\"end\":43214,\"start\":43213},{\"end\":43216,\"start\":43215},{\"end\":43225,\"start\":43224},{\"end\":43233,\"start\":43232},{\"end\":43242,\"start\":43241},{\"end\":43244,\"start\":43243},{\"end\":43255,\"start\":43254},{\"end\":43266,\"start\":43265},{\"end\":43268,\"start\":43267},{\"end\":43279,\"start\":43278},{\"end\":43290,\"start\":43289},{\"end\":43725,\"start\":43724},{\"end\":43727,\"start\":43726},{\"end\":43736,\"start\":43735},{\"end\":43744,\"start\":43743},{\"end\":43753,\"start\":43752},{\"end\":43764,\"start\":43763},{\"end\":43774,\"start\":43773},{\"end\":43786,\"start\":43785},{\"end\":43801,\"start\":43800},{\"end\":43810,\"start\":43809},{\"end\":43820,\"start\":43819},{\"end\":43830,\"start\":43829},{\"end\":43841,\"start\":43840},{\"end\":43857,\"start\":43856},{\"end\":43868,\"start\":43867},{\"end\":43880,\"start\":43879},{\"end\":43889,\"start\":43888},{\"end\":43899,\"start\":43898},{\"end\":43901,\"start\":43900},{\"end\":43912,\"start\":43911},{\"end\":43918,\"start\":43917},{\"end\":43928,\"start\":43927},{\"end\":43937,\"start\":43936},{\"end\":43945,\"start\":43944},{\"end\":43955,\"start\":43954},{\"end\":43965,\"start\":43964},{\"end\":43973,\"start\":43972},{\"end\":43982,\"start\":43981},{\"end\":43991,\"start\":43990},{\"end\":44001,\"start\":44000},{\"end\":44015,\"start\":44014},{\"end\":44026,\"start\":44025},{\"end\":44039,\"start\":44038},{\"end\":44955,\"start\":44954},{\"end\":44963,\"start\":44962},{\"end\":44970,\"start\":44969},{\"end\":44978,\"start\":44977},{\"end\":44986,\"start\":44985},{\"end\":45186,\"start\":45182},{\"end\":45196,\"start\":45195},{\"end\":45202,\"start\":45201},{\"end\":45209,\"start\":45208},{\"end\":45218,\"start\":45217},{\"end\":45224,\"start\":45223},{\"end\":45233,\"start\":45232},{\"end\":45242,\"start\":45241},{\"end\":45252,\"start\":45251},{\"end\":45262,\"start\":45261},{\"end\":45264,\"start\":45263},{\"end\":45276,\"start\":45275},{\"end\":45286,\"start\":45285},{\"end\":45288,\"start\":45287},{\"end\":45653,\"start\":45652},{\"end\":45655,\"start\":45654},{\"end\":45670,\"start\":45669},{\"end\":45679,\"start\":45678},{\"end\":45689,\"start\":45688},{\"end\":45700,\"start\":45699},{\"end\":45712,\"start\":45711},{\"end\":45725,\"start\":45724},{\"end\":45737,\"start\":45736},{\"end\":45744,\"start\":45743},{\"end\":45753,\"start\":45752},{\"end\":46125,\"start\":46124},{\"end\":46132,\"start\":46131},{\"end\":46138,\"start\":46137},{\"end\":46144,\"start\":46143},{\"end\":46148,\"start\":46145},{\"end\":46157,\"start\":46156},{\"end\":46165,\"start\":46164},{\"end\":46173,\"start\":46172},{\"end\":46179,\"start\":46178},{\"end\":46187,\"start\":46186},{\"end\":46454,\"start\":46453},{\"end\":46461,\"start\":46460},{\"end\":46471,\"start\":46470},{\"end\":46480,\"start\":46479},{\"end\":46489,\"start\":46488},{\"end\":46498,\"start\":46497},{\"end\":46502,\"start\":46499},{\"end\":46511,\"start\":46510},{\"end\":46521,\"start\":46520},{\"end\":46908,\"start\":46907},{\"end\":46918,\"start\":46917},{\"end\":46925,\"start\":46924},{\"end\":46933,\"start\":46932},{\"end\":47176,\"start\":47175},{\"end\":47187,\"start\":47186},{\"end\":47196,\"start\":47195},{\"end\":47200,\"start\":47199},{\"end\":47574,\"start\":47573},{\"end\":47581,\"start\":47580},{\"end\":47588,\"start\":47587},{\"end\":47596,\"start\":47595},{\"end\":47605,\"start\":47604},{\"end\":47613,\"start\":47612},{\"end\":48290,\"start\":48289},{\"end\":48301,\"start\":48300},{\"end\":48313,\"start\":48312},{\"end\":48580,\"start\":48579},{\"end\":48589,\"start\":48588},{\"end\":48596,\"start\":48595},{\"end\":48612,\"start\":48608},{\"end\":48620,\"start\":48619},{\"end\":48630,\"start\":48629},{\"end\":48636,\"start\":48635},{\"end\":48648,\"start\":48647},{\"end\":48659,\"start\":48658},{\"end\":48669,\"start\":48668},{\"end\":49114,\"start\":49113},{\"end\":49123,\"start\":49122},{\"end\":49125,\"start\":49124},{\"end\":49134,\"start\":49133},{\"end\":49147,\"start\":49146},{\"end\":49162,\"start\":49161},{\"end\":49174,\"start\":49173},{\"end\":49181,\"start\":49180},{\"end\":49191,\"start\":49190},{\"end\":49200,\"start\":49199},{\"end\":49212,\"start\":49211},{\"end\":49231,\"start\":49230},{\"end\":49240,\"start\":49239},{\"end\":49250,\"start\":49249},{\"end\":49257,\"start\":49256},{\"end\":49824,\"start\":49823},{\"end\":49833,\"start\":49832},{\"end\":49841,\"start\":49840},{\"end\":49857,\"start\":49856},{\"end\":49866,\"start\":49865},{\"end\":50313,\"start\":50312},{\"end\":50315,\"start\":50314},{\"end\":50321,\"start\":50320},{\"end\":50329,\"start\":50328},{\"end\":50339,\"start\":50338},{\"end\":50352,\"start\":50351},{\"end\":50358,\"start\":50357},{\"end\":50366,\"start\":50365},{\"end\":50374,\"start\":50373},{\"end\":50749,\"start\":50745},{\"end\":50751,\"start\":50750},{\"end\":50760,\"start\":50759},{\"end\":50771,\"start\":50770},{\"end\":50787,\"start\":50786},{\"end\":50796,\"start\":50795},{\"end\":50807,\"start\":50806},{\"end\":50817,\"start\":50816},{\"end\":50829,\"start\":50828},{\"end\":50835,\"start\":50834},{\"end\":50844,\"start\":50843},{\"end\":50853,\"start\":50852},{\"end\":50855,\"start\":50854},{\"end\":50866,\"start\":50865},{\"end\":50876,\"start\":50875},{\"end\":50891,\"start\":50890},{\"end\":50901,\"start\":50900},{\"end\":51653,\"start\":51652},{\"end\":51655,\"start\":51654},{\"end\":51665,\"start\":51664},{\"end\":51667,\"start\":51666},{\"end\":52114,\"start\":52113},{\"end\":52125,\"start\":52124},{\"end\":52138,\"start\":52137},{\"end\":52156,\"start\":52155},{\"end\":52167,\"start\":52166},{\"end\":52169,\"start\":52168},{\"end\":52180,\"start\":52179},{\"end\":52182,\"start\":52181},{\"end\":52687,\"start\":52686},{\"end\":52697,\"start\":52696},{\"end\":52708,\"start\":52707},{\"end\":52720,\"start\":52719},{\"end\":52728,\"start\":52727},{\"end\":52734,\"start\":52733},{\"end\":52743,\"start\":52742},{\"end\":53230,\"start\":53229},{\"end\":53240,\"start\":53239},{\"end\":53251,\"start\":53250},{\"end\":53262,\"start\":53261},{\"end\":53701,\"start\":53700},{\"end\":53711,\"start\":53710},{\"end\":53721,\"start\":53720},{\"end\":53735,\"start\":53734},{\"end\":53738,\"start\":53736},{\"end\":53748,\"start\":53747},{\"end\":53760,\"start\":53759},{\"end\":53762,\"start\":53761},{\"end\":53772,\"start\":53771},{\"end\":54434,\"start\":54433},{\"end\":54440,\"start\":54439},{\"end\":54446,\"start\":54445},{\"end\":54458,\"start\":54457},{\"end\":54710,\"start\":54709},{\"end\":54716,\"start\":54715},{\"end\":54723,\"start\":54722},{\"end\":54731,\"start\":54730},{\"end\":55128,\"start\":55127},{\"end\":55134,\"start\":55133},{\"end\":55140,\"start\":55139},{\"end\":55148,\"start\":55147},{\"end\":55155,\"start\":55154},{\"end\":55162,\"start\":55161},{\"end\":55170,\"start\":55169},{\"end\":55456,\"start\":55451},{\"end\":55665,\"start\":55661},{\"end\":55672,\"start\":55671},{\"end\":55681,\"start\":55680},{\"end\":55693,\"start\":55692},{\"end\":55701,\"start\":55700},{\"end\":55711,\"start\":55710},{\"end\":55722,\"start\":55721},{\"end\":55732,\"start\":55731},{\"end\":55734,\"start\":55733},{\"end\":56184,\"start\":56183},{\"end\":56191,\"start\":56190},{\"end\":56197,\"start\":56196},{\"end\":56203,\"start\":56202},{\"end\":56205,\"start\":56204},{\"end\":56411,\"start\":56410},{\"end\":56422,\"start\":56421},{\"end\":56429,\"start\":56428},{\"end\":56435,\"start\":56434},{\"end\":56445,\"start\":56444},{\"end\":56447,\"start\":56446},{\"end\":56456,\"start\":56455},{\"end\":56463,\"start\":56462},{\"end\":56471,\"start\":56470},{\"end\":56473,\"start\":56472},{\"end\":56479,\"start\":56478},{\"end\":56487,\"start\":56486},{\"end\":56792,\"start\":56791},{\"end\":56806,\"start\":56805},{\"end\":57179,\"start\":57178},{\"end\":57185,\"start\":57184},{\"end\":57195,\"start\":57194},{\"end\":57202,\"start\":57201},{\"end\":57212,\"start\":57208},{\"end\":57224,\"start\":57220},{\"end\":57231,\"start\":57230},{\"end\":57242,\"start\":57241},{\"end\":57251,\"start\":57250},{\"end\":57680,\"start\":57679},{\"end\":57690,\"start\":57689},{\"end\":57703,\"start\":57702},{\"end\":57714,\"start\":57713},{\"end\":58129,\"start\":58128},{\"end\":58139,\"start\":58138},{\"end\":58151,\"start\":58150},{\"end\":58566,\"start\":58565},{\"end\":58576,\"start\":58575},{\"end\":58587,\"start\":58586},{\"end\":58589,\"start\":58588},{\"end\":58598,\"start\":58597},{\"end\":58993,\"start\":58992},{\"end\":59003,\"start\":59002},{\"end\":59015,\"start\":59014},{\"end\":59024,\"start\":59023},{\"end\":59619,\"start\":59618},{\"end\":59627,\"start\":59626},{\"end\":59633,\"start\":59632},{\"end\":59639,\"start\":59638},{\"end\":59649,\"start\":59648},{\"end\":59910,\"start\":59909},{\"end\":59921,\"start\":59920},{\"end\":59923,\"start\":59922},{\"end\":59930,\"start\":59929},{\"end\":59941,\"start\":59940},{\"end\":59951,\"start\":59950},{\"end\":59958,\"start\":59957},{\"end\":59969,\"start\":59968},{\"end\":59979,\"start\":59978},{\"end\":59989,\"start\":59988},{\"end\":60000,\"start\":59999},{\"end\":60009,\"start\":60008},{\"end\":60020,\"start\":60019},{\"end\":60692,\"start\":60691},{\"end\":60707,\"start\":60706},{\"end\":60715,\"start\":60714},{\"end\":60721,\"start\":60720},{\"end\":60731,\"start\":60730},{\"end\":60743,\"start\":60742},{\"end\":60749,\"start\":60748},{\"end\":60758,\"start\":60757},{\"end\":60770,\"start\":60769},{\"end\":60780,\"start\":60779},{\"end\":61141,\"start\":61140},{\"end\":61154,\"start\":61153},{\"end\":61163,\"start\":61162},{\"end\":61175,\"start\":61174},{\"end\":61190,\"start\":61189},{\"end\":61200,\"start\":61199},{\"end\":61209,\"start\":61208},{\"end\":61220,\"start\":61219},{\"end\":61230,\"start\":61229},{\"end\":61583,\"start\":61582},{\"end\":61594,\"start\":61593},{\"end\":61608,\"start\":61607},{\"end\":61617,\"start\":61616},{\"end\":61627,\"start\":61626},{\"end\":62104,\"start\":62103},{\"end\":62119,\"start\":62118},{\"end\":62132,\"start\":62131},{\"end\":62401,\"start\":62400},{\"end\":62412,\"start\":62411},{\"end\":62418,\"start\":62417},{\"end\":62430,\"start\":62429},{\"end\":62829,\"start\":62828},{\"end\":62838,\"start\":62837},{\"end\":62851,\"start\":62850},{\"end\":62859,\"start\":62858},{\"end\":62868,\"start\":62867},{\"end\":62876,\"start\":62875},{\"end\":62885,\"start\":62884},{\"end\":62895,\"start\":62894},{\"end\":63309,\"start\":63308},{\"end\":63317,\"start\":63316},{\"end\":63326,\"start\":63325},{\"end\":63333,\"start\":63332},{\"end\":63808,\"start\":63807},{\"end\":63818,\"start\":63817},{\"end\":63829,\"start\":63828},{\"end\":64593,\"start\":64592},{\"end\":64603,\"start\":64602},{\"end\":64612,\"start\":64611},{\"end\":64623,\"start\":64622},{\"end\":64632,\"start\":64631},{\"end\":64644,\"start\":64643},{\"end\":64653,\"start\":64652},{\"end\":65139,\"start\":65138},{\"end\":65150,\"start\":65149},{\"end\":65160,\"start\":65159},{\"end\":65171,\"start\":65170},{\"end\":65186,\"start\":65182},{\"end\":65197,\"start\":65196},{\"end\":65208,\"start\":65207},{\"end\":65219,\"start\":65218},{\"end\":65228,\"start\":65227},{\"end\":65238,\"start\":65237},{\"end\":65588,\"start\":65587},{\"end\":65596,\"start\":65595},{\"end\":65606,\"start\":65605},{\"end\":65617,\"start\":65616},{\"end\":65626,\"start\":65625},{\"end\":65898,\"start\":65897},{\"end\":65906,\"start\":65905},{\"end\":65912,\"start\":65911},{\"end\":65920,\"start\":65919},{\"end\":65927,\"start\":65926},{\"end\":65934,\"start\":65933},{\"end\":65941,\"start\":65940},{\"end\":65948,\"start\":65947},{\"end\":65955,\"start\":65954},{\"end\":66252,\"start\":66251},{\"end\":66260,\"start\":66259},{\"end\":66270,\"start\":66269},{\"end\":66290,\"start\":66289},{\"end\":66299,\"start\":66298},{\"end\":66310,\"start\":66309},{\"end\":66318,\"start\":66317},{\"end\":66327,\"start\":66326},{\"end\":66329,\"start\":66328},{\"end\":66345,\"start\":66344},{\"end\":66358,\"start\":66357},{\"end\":66366,\"start\":66365},{\"end\":66376,\"start\":66375},{\"end\":66393,\"start\":66392},{\"end\":66400,\"start\":66399},{\"end\":66411,\"start\":66410},{\"end\":66421,\"start\":66420},{\"end\":66433,\"start\":66432},{\"end\":66443,\"start\":66442},{\"end\":66452,\"start\":66451},{\"end\":66454,\"start\":66453},{\"end\":66461,\"start\":66460},{\"end\":66470,\"start\":66469},{\"end\":66484,\"start\":66483},{\"end\":66494,\"start\":66493},{\"end\":66505,\"start\":66504},{\"end\":66517,\"start\":66516},{\"end\":66519,\"start\":66518},{\"end\":66527,\"start\":66526},{\"end\":66536,\"start\":66535},{\"end\":66538,\"start\":66537},{\"end\":66546,\"start\":66545},{\"end\":66555,\"start\":66554},{\"end\":66564,\"start\":66563},{\"end\":66566,\"start\":66565},{\"end\":66576,\"start\":66575},{\"end\":66586,\"start\":66585},{\"end\":66595,\"start\":66594},{\"end\":66599,\"start\":66598},{\"end\":66608,\"start\":66607},{\"end\":66617,\"start\":66616},{\"end\":67459,\"start\":67458},{\"end\":67466,\"start\":67465},{\"end\":67475,\"start\":67474},{\"end\":67477,\"start\":67476},{\"end\":67485,\"start\":67484},{\"end\":67492,\"start\":67491},{\"end\":67494,\"start\":67493},{\"end\":67500,\"start\":67499},{\"end\":67510,\"start\":67509},{\"end\":67516,\"start\":67515},{\"end\":67518,\"start\":67517},{\"end\":67525,\"start\":67524},{\"end\":67527,\"start\":67526},{\"end\":67968,\"start\":67967},{\"end\":67974,\"start\":67973},{\"end\":67982,\"start\":67981},{\"end\":67990,\"start\":67989},{\"end\":67996,\"start\":67995},{\"end\":68005,\"start\":68004},{\"end\":68011,\"start\":68010},{\"end\":68494,\"start\":68493},{\"end\":68500,\"start\":68499},{\"end\":68507,\"start\":68506},{\"end\":68514,\"start\":68513},{\"end\":68936,\"start\":68935},{\"end\":68942,\"start\":68941},{\"end\":68950,\"start\":68949},{\"end\":69209,\"start\":69208},{\"end\":69217,\"start\":69216},{\"end\":69226,\"start\":69225},{\"end\":69235,\"start\":69234},{\"end\":69245,\"start\":69244},{\"end\":69711,\"start\":69710},{\"end\":69713,\"start\":69712},{\"end\":69720,\"start\":69719},{\"end\":69728,\"start\":69727},{\"end\":69738,\"start\":69734},{\"end\":69745,\"start\":69744},{\"end\":70059,\"start\":70058},{\"end\":70065,\"start\":70064},{\"end\":70071,\"start\":70070},{\"end\":70077,\"start\":70076},{\"end\":70083,\"start\":70082},{\"end\":70091,\"start\":70090},{\"end\":70101,\"start\":70100},{\"end\":70912,\"start\":70911},{\"end\":70923,\"start\":70922},{\"end\":70931,\"start\":70930},{\"end\":70942,\"start\":70941},{\"end\":71286,\"start\":71285},{\"end\":71295,\"start\":71294},{\"end\":71302,\"start\":71301},{\"end\":71310,\"start\":71309},{\"end\":71319,\"start\":71318},{\"end\":71328,\"start\":71327},{\"end\":71336,\"start\":71335},{\"end\":71343,\"start\":71342},{\"end\":71349,\"start\":71348},{\"end\":71357,\"start\":71356},{\"end\":71363,\"start\":71362},{\"end\":71371,\"start\":71370},{\"end\":71379,\"start\":71378},{\"end\":71388,\"start\":71387},{\"end\":71394,\"start\":71393},{\"end\":71400,\"start\":71399},{\"end\":71406,\"start\":71405},{\"end\":71413,\"start\":71412},{\"end\":71421,\"start\":71420},{\"end\":71428,\"start\":71427},{\"end\":71435,\"start\":71434},{\"end\":71443,\"start\":71442},{\"end\":71451,\"start\":71450},{\"end\":71457,\"start\":71456},{\"end\":71465,\"start\":71464},{\"end\":71962,\"start\":71961},{\"end\":71971,\"start\":71970},{\"end\":71979,\"start\":71978},{\"end\":71986,\"start\":71985},{\"end\":72409,\"start\":72408},{\"end\":72416,\"start\":72415},{\"end\":72424,\"start\":72423},{\"end\":72432,\"start\":72431},{\"end\":72438,\"start\":72437}]", "bib_author_last_name": "[{\"end\":42835,\"start\":42828},{\"end\":42847,\"start\":42839},{\"end\":42858,\"start\":42851},{\"end\":42867,\"start\":42862},{\"end\":43222,\"start\":43217},{\"end\":43230,\"start\":43226},{\"end\":43239,\"start\":43234},{\"end\":43252,\"start\":43245},{\"end\":43263,\"start\":43256},{\"end\":43276,\"start\":43269},{\"end\":43287,\"start\":43280},{\"end\":43299,\"start\":43291},{\"end\":43733,\"start\":43728},{\"end\":43741,\"start\":43737},{\"end\":43750,\"start\":43745},{\"end\":43761,\"start\":43754},{\"end\":43771,\"start\":43765},{\"end\":43783,\"start\":43775},{\"end\":43798,\"start\":43787},{\"end\":43807,\"start\":43802},{\"end\":43817,\"start\":43811},{\"end\":43827,\"start\":43821},{\"end\":43838,\"start\":43831},{\"end\":43854,\"start\":43842},{\"end\":43865,\"start\":43858},{\"end\":43877,\"start\":43869},{\"end\":43886,\"start\":43881},{\"end\":43896,\"start\":43890},{\"end\":43909,\"start\":43902},{\"end\":43915,\"start\":43913},{\"end\":43925,\"start\":43919},{\"end\":43934,\"start\":43929},{\"end\":43942,\"start\":43938},{\"end\":43952,\"start\":43946},{\"end\":43962,\"start\":43956},{\"end\":43970,\"start\":43966},{\"end\":43979,\"start\":43974},{\"end\":43988,\"start\":43983},{\"end\":43998,\"start\":43992},{\"end\":44012,\"start\":44002},{\"end\":44023,\"start\":44016},{\"end\":44036,\"start\":44027},{\"end\":44046,\"start\":44040},{\"end\":44960,\"start\":44956},{\"end\":44967,\"start\":44964},{\"end\":44975,\"start\":44971},{\"end\":44983,\"start\":44979},{\"end\":44991,\"start\":44987},{\"end\":45193,\"start\":45187},{\"end\":45199,\"start\":45197},{\"end\":45206,\"start\":45203},{\"end\":45215,\"start\":45210},{\"end\":45221,\"start\":45219},{\"end\":45230,\"start\":45225},{\"end\":45239,\"start\":45234},{\"end\":45249,\"start\":45243},{\"end\":45259,\"start\":45253},{\"end\":45273,\"start\":45265},{\"end\":45283,\"start\":45277},{\"end\":45293,\"start\":45289},{\"end\":45667,\"start\":45656},{\"end\":45676,\"start\":45671},{\"end\":45686,\"start\":45680},{\"end\":45697,\"start\":45690},{\"end\":45709,\"start\":45701},{\"end\":45722,\"start\":45713},{\"end\":45734,\"start\":45726},{\"end\":45741,\"start\":45738},{\"end\":45750,\"start\":45745},{\"end\":45762,\"start\":45754},{\"end\":46129,\"start\":46126},{\"end\":46135,\"start\":46133},{\"end\":46141,\"start\":46139},{\"end\":46154,\"start\":46149},{\"end\":46162,\"start\":46158},{\"end\":46170,\"start\":46166},{\"end\":46176,\"start\":46174},{\"end\":46184,\"start\":46180},{\"end\":46191,\"start\":46188},{\"end\":46458,\"start\":46455},{\"end\":46468,\"start\":46462},{\"end\":46477,\"start\":46472},{\"end\":46486,\"start\":46481},{\"end\":46495,\"start\":46490},{\"end\":46508,\"start\":46503},{\"end\":46518,\"start\":46512},{\"end\":46527,\"start\":46522},{\"end\":46915,\"start\":46909},{\"end\":46922,\"start\":46919},{\"end\":46930,\"start\":46926},{\"end\":46938,\"start\":46934},{\"end\":47184,\"start\":47177},{\"end\":47193,\"start\":47188},{\"end\":47207,\"start\":47201},{\"end\":47578,\"start\":47575},{\"end\":47585,\"start\":47582},{\"end\":47593,\"start\":47589},{\"end\":47602,\"start\":47597},{\"end\":47610,\"start\":47606},{\"end\":47616,\"start\":47614},{\"end\":48298,\"start\":48291},{\"end\":48310,\"start\":48302},{\"end\":48319,\"start\":48314},{\"end\":48586,\"start\":48581},{\"end\":48593,\"start\":48590},{\"end\":48606,\"start\":48597},{\"end\":48617,\"start\":48613},{\"end\":48627,\"start\":48621},{\"end\":48633,\"start\":48631},{\"end\":48645,\"start\":48637},{\"end\":48656,\"start\":48649},{\"end\":48666,\"start\":48660},{\"end\":48673,\"start\":48670},{\"end\":49120,\"start\":49115},{\"end\":49131,\"start\":49126},{\"end\":49144,\"start\":49135},{\"end\":49159,\"start\":49148},{\"end\":49171,\"start\":49163},{\"end\":49178,\"start\":49175},{\"end\":49188,\"start\":49182},{\"end\":49197,\"start\":49192},{\"end\":49209,\"start\":49201},{\"end\":49228,\"start\":49213},{\"end\":49237,\"start\":49232},{\"end\":49247,\"start\":49241},{\"end\":49254,\"start\":49251},{\"end\":49267,\"start\":49258},{\"end\":49830,\"start\":49825},{\"end\":49838,\"start\":49834},{\"end\":49854,\"start\":49842},{\"end\":49863,\"start\":49858},{\"end\":49873,\"start\":49867},{\"end\":50318,\"start\":50316},{\"end\":50326,\"start\":50322},{\"end\":50336,\"start\":50330},{\"end\":50349,\"start\":50340},{\"end\":50355,\"start\":50353},{\"end\":50363,\"start\":50359},{\"end\":50371,\"start\":50367},{\"end\":50379,\"start\":50375},{\"end\":50757,\"start\":50752},{\"end\":50768,\"start\":50761},{\"end\":50784,\"start\":50772},{\"end\":50793,\"start\":50788},{\"end\":50804,\"start\":50797},{\"end\":50814,\"start\":50808},{\"end\":50826,\"start\":50818},{\"end\":50832,\"start\":50830},{\"end\":50841,\"start\":50836},{\"end\":50850,\"start\":50845},{\"end\":50863,\"start\":50856},{\"end\":50873,\"start\":50867},{\"end\":50888,\"start\":50877},{\"end\":50898,\"start\":50892},{\"end\":50910,\"start\":50902},{\"end\":51662,\"start\":51656},{\"end\":51675,\"start\":51668},{\"end\":52122,\"start\":52115},{\"end\":52135,\"start\":52126},{\"end\":52153,\"start\":52139},{\"end\":52164,\"start\":52157},{\"end\":52177,\"start\":52170},{\"end\":52191,\"start\":52183},{\"end\":52694,\"start\":52688},{\"end\":52705,\"start\":52698},{\"end\":52717,\"start\":52709},{\"end\":52725,\"start\":52721},{\"end\":52731,\"start\":52729},{\"end\":52740,\"start\":52735},{\"end\":52755,\"start\":52744},{\"end\":53237,\"start\":53231},{\"end\":53248,\"start\":53241},{\"end\":53259,\"start\":53252},{\"end\":53270,\"start\":53263},{\"end\":53708,\"start\":53702},{\"end\":53718,\"start\":53712},{\"end\":53732,\"start\":53722},{\"end\":53745,\"start\":53739},{\"end\":53757,\"start\":53749},{\"end\":53769,\"start\":53763},{\"end\":53788,\"start\":53773},{\"end\":54437,\"start\":54435},{\"end\":54443,\"start\":54441},{\"end\":54455,\"start\":54447},{\"end\":54462,\"start\":54459},{\"end\":54713,\"start\":54711},{\"end\":54720,\"start\":54717},{\"end\":54728,\"start\":54724},{\"end\":54735,\"start\":54732},{\"end\":55131,\"start\":55129},{\"end\":55137,\"start\":55135},{\"end\":55145,\"start\":55141},{\"end\":55152,\"start\":55149},{\"end\":55159,\"start\":55156},{\"end\":55167,\"start\":55163},{\"end\":55173,\"start\":55171},{\"end\":55460,\"start\":55457},{\"end\":55669,\"start\":55666},{\"end\":55678,\"start\":55673},{\"end\":55690,\"start\":55682},{\"end\":55698,\"start\":55694},{\"end\":55708,\"start\":55702},{\"end\":55719,\"start\":55712},{\"end\":55729,\"start\":55723},{\"end\":55742,\"start\":55735},{\"end\":56188,\"start\":56185},{\"end\":56194,\"start\":56192},{\"end\":56200,\"start\":56198},{\"end\":56209,\"start\":56206},{\"end\":56419,\"start\":56412},{\"end\":56426,\"start\":56423},{\"end\":56432,\"start\":56430},{\"end\":56442,\"start\":56436},{\"end\":56453,\"start\":56448},{\"end\":56460,\"start\":56457},{\"end\":56468,\"start\":56464},{\"end\":56476,\"start\":56474},{\"end\":56484,\"start\":56480},{\"end\":56491,\"start\":56488},{\"end\":56803,\"start\":56793},{\"end\":56813,\"start\":56807},{\"end\":57182,\"start\":57180},{\"end\":57192,\"start\":57186},{\"end\":57199,\"start\":57196},{\"end\":57206,\"start\":57203},{\"end\":57218,\"start\":57213},{\"end\":57228,\"start\":57225},{\"end\":57239,\"start\":57232},{\"end\":57248,\"start\":57243},{\"end\":57258,\"start\":57252},{\"end\":57687,\"start\":57681},{\"end\":57700,\"start\":57691},{\"end\":57711,\"start\":57704},{\"end\":57723,\"start\":57715},{\"end\":58136,\"start\":58130},{\"end\":58148,\"start\":58140},{\"end\":58159,\"start\":58152},{\"end\":58573,\"start\":58567},{\"end\":58584,\"start\":58577},{\"end\":58595,\"start\":58590},{\"end\":58610,\"start\":58599},{\"end\":59000,\"start\":58994},{\"end\":59012,\"start\":59004},{\"end\":59021,\"start\":59016},{\"end\":59035,\"start\":59025},{\"end\":59553,\"start\":59547},{\"end\":59624,\"start\":59620},{\"end\":59630,\"start\":59628},{\"end\":59636,\"start\":59634},{\"end\":59646,\"start\":59640},{\"end\":59653,\"start\":59650},{\"end\":59918,\"start\":59911},{\"end\":59927,\"start\":59924},{\"end\":59938,\"start\":59931},{\"end\":59948,\"start\":59942},{\"end\":59955,\"start\":59952},{\"end\":59966,\"start\":59959},{\"end\":59976,\"start\":59970},{\"end\":59986,\"start\":59980},{\"end\":59997,\"start\":59990},{\"end\":60006,\"start\":60001},{\"end\":60017,\"start\":60010},{\"end\":60030,\"start\":60021},{\"end\":60704,\"start\":60693},{\"end\":60712,\"start\":60708},{\"end\":60718,\"start\":60716},{\"end\":60728,\"start\":60722},{\"end\":60740,\"start\":60732},{\"end\":60746,\"start\":60744},{\"end\":60755,\"start\":60750},{\"end\":60767,\"start\":60759},{\"end\":60777,\"start\":60771},{\"end\":60790,\"start\":60781},{\"end\":61151,\"start\":61142},{\"end\":61160,\"start\":61155},{\"end\":61172,\"start\":61164},{\"end\":61187,\"start\":61176},{\"end\":61197,\"start\":61191},{\"end\":61206,\"start\":61201},{\"end\":61217,\"start\":61210},{\"end\":61227,\"start\":61221},{\"end\":61242,\"start\":61231},{\"end\":61591,\"start\":61584},{\"end\":61605,\"start\":61595},{\"end\":61614,\"start\":61609},{\"end\":61624,\"start\":61618},{\"end\":61636,\"start\":61628},{\"end\":62116,\"start\":62105},{\"end\":62129,\"start\":62120},{\"end\":62140,\"start\":62133},{\"end\":62409,\"start\":62402},{\"end\":62415,\"start\":62413},{\"end\":62427,\"start\":62419},{\"end\":62436,\"start\":62431},{\"end\":62835,\"start\":62830},{\"end\":62848,\"start\":62839},{\"end\":62856,\"start\":62852},{\"end\":62865,\"start\":62860},{\"end\":62873,\"start\":62869},{\"end\":62882,\"start\":62877},{\"end\":62892,\"start\":62886},{\"end\":62904,\"start\":62896},{\"end\":63314,\"start\":63310},{\"end\":63323,\"start\":63318},{\"end\":63330,\"start\":63327},{\"end\":63339,\"start\":63334},{\"end\":63815,\"start\":63809},{\"end\":63826,\"start\":63819},{\"end\":63837,\"start\":63830},{\"end\":64600,\"start\":64594},{\"end\":64609,\"start\":64604},{\"end\":64620,\"start\":64613},{\"end\":64629,\"start\":64624},{\"end\":64641,\"start\":64633},{\"end\":64650,\"start\":64645},{\"end\":64658,\"start\":64654},{\"end\":65147,\"start\":65140},{\"end\":65157,\"start\":65151},{\"end\":65168,\"start\":65161},{\"end\":65180,\"start\":65172},{\"end\":65194,\"start\":65187},{\"end\":65205,\"start\":65198},{\"end\":65216,\"start\":65209},{\"end\":65225,\"start\":65220},{\"end\":65235,\"start\":65229},{\"end\":65244,\"start\":65239},{\"end\":65593,\"start\":65589},{\"end\":65603,\"start\":65597},{\"end\":65614,\"start\":65607},{\"end\":65623,\"start\":65618},{\"end\":65635,\"start\":65627},{\"end\":65903,\"start\":65899},{\"end\":65909,\"start\":65907},{\"end\":65917,\"start\":65913},{\"end\":65924,\"start\":65921},{\"end\":65931,\"start\":65928},{\"end\":65938,\"start\":65935},{\"end\":65945,\"start\":65942},{\"end\":65952,\"start\":65949},{\"end\":65959,\"start\":65956},{\"end\":66257,\"start\":66253},{\"end\":66267,\"start\":66261},{\"end\":66287,\"start\":66271},{\"end\":66296,\"start\":66291},{\"end\":66307,\"start\":66300},{\"end\":66315,\"start\":66311},{\"end\":66324,\"start\":66319},{\"end\":66342,\"start\":66330},{\"end\":66355,\"start\":66346},{\"end\":66363,\"start\":66359},{\"end\":66373,\"start\":66367},{\"end\":66390,\"start\":66377},{\"end\":66397,\"start\":66394},{\"end\":66408,\"start\":66401},{\"end\":66418,\"start\":66412},{\"end\":66430,\"start\":66422},{\"end\":66440,\"start\":66434},{\"end\":66449,\"start\":66444},{\"end\":66458,\"start\":66455},{\"end\":66467,\"start\":66462},{\"end\":66481,\"start\":66471},{\"end\":66491,\"start\":66485},{\"end\":66502,\"start\":66495},{\"end\":66514,\"start\":66506},{\"end\":66524,\"start\":66520},{\"end\":66533,\"start\":66528},{\"end\":66543,\"start\":66539},{\"end\":66552,\"start\":66547},{\"end\":66561,\"start\":66556},{\"end\":66573,\"start\":66567},{\"end\":66583,\"start\":66577},{\"end\":66592,\"start\":66587},{\"end\":66605,\"start\":66600},{\"end\":66614,\"start\":66609},{\"end\":66622,\"start\":66618},{\"end\":67463,\"start\":67460},{\"end\":67472,\"start\":67467},{\"end\":67482,\"start\":67478},{\"end\":67489,\"start\":67486},{\"end\":67497,\"start\":67495},{\"end\":67507,\"start\":67501},{\"end\":67513,\"start\":67511},{\"end\":67522,\"start\":67519},{\"end\":67530,\"start\":67528},{\"end\":67971,\"start\":67969},{\"end\":67979,\"start\":67975},{\"end\":67987,\"start\":67983},{\"end\":67993,\"start\":67991},{\"end\":68002,\"start\":67997},{\"end\":68008,\"start\":68006},{\"end\":68018,\"start\":68012},{\"end\":68497,\"start\":68495},{\"end\":68504,\"start\":68501},{\"end\":68511,\"start\":68508},{\"end\":68518,\"start\":68515},{\"end\":68939,\"start\":68937},{\"end\":68947,\"start\":68943},{\"end\":68956,\"start\":68951},{\"end\":69214,\"start\":69210},{\"end\":69223,\"start\":69218},{\"end\":69232,\"start\":69227},{\"end\":69242,\"start\":69236},{\"end\":69252,\"start\":69246},{\"end\":69717,\"start\":69714},{\"end\":69725,\"start\":69721},{\"end\":69732,\"start\":69729},{\"end\":69742,\"start\":69739},{\"end\":69751,\"start\":69746},{\"end\":70062,\"start\":70060},{\"end\":70068,\"start\":70066},{\"end\":70074,\"start\":70072},{\"end\":70080,\"start\":70078},{\"end\":70088,\"start\":70084},{\"end\":70098,\"start\":70092},{\"end\":70105,\"start\":70102},{\"end\":70920,\"start\":70913},{\"end\":70928,\"start\":70924},{\"end\":70939,\"start\":70932},{\"end\":70947,\"start\":70943},{\"end\":71292,\"start\":71287},{\"end\":71299,\"start\":71296},{\"end\":71307,\"start\":71303},{\"end\":71316,\"start\":71311},{\"end\":71325,\"start\":71320},{\"end\":71333,\"start\":71329},{\"end\":71340,\"start\":71337},{\"end\":71346,\"start\":71344},{\"end\":71354,\"start\":71350},{\"end\":71360,\"start\":71358},{\"end\":71368,\"start\":71364},{\"end\":71376,\"start\":71372},{\"end\":71385,\"start\":71380},{\"end\":71391,\"start\":71389},{\"end\":71397,\"start\":71395},{\"end\":71403,\"start\":71401},{\"end\":71410,\"start\":71407},{\"end\":71418,\"start\":71414},{\"end\":71425,\"start\":71422},{\"end\":71432,\"start\":71429},{\"end\":71440,\"start\":71436},{\"end\":71448,\"start\":71444},{\"end\":71454,\"start\":71452},{\"end\":71462,\"start\":71458},{\"end\":71470,\"start\":71466},{\"end\":71968,\"start\":71963},{\"end\":71976,\"start\":71972},{\"end\":71983,\"start\":71980},{\"end\":71990,\"start\":71987},{\"end\":72413,\"start\":72410},{\"end\":72421,\"start\":72417},{\"end\":72429,\"start\":72425},{\"end\":72435,\"start\":72433},{\"end\":72448,\"start\":72439}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5276660},\"end\":43173,\"start\":42802},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":173188651},\"end\":43683,\"start\":43175},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":44878,\"start\":43685},{\"attributes\":{\"doi\":\"abs/1705.02743\",\"id\":\"b3\"},\"end\":45180,\"start\":44880},{\"attributes\":{\"id\":\"b4\"},\"end\":45581,\"start\":45182},{\"attributes\":{\"id\":\"b5\"},\"end\":46036,\"start\":45583},{\"attributes\":{\"id\":\"b6\"},\"end\":46436,\"start\":46038},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1820614},\"end\":46854,\"start\":46438},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":231740979},\"end\":47115,\"start\":46856},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8622019},\"end\":47488,\"start\":47117},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":209217},\"end\":48226,\"start\":47490},{\"attributes\":{\"doi\":\"abs/2303.15056\",\"id\":\"b11\"},\"end\":48486,\"start\":48228},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235358129},\"end\":49021,\"start\":48488},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":834612},\"end\":49721,\"start\":49023},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8081284},\"end\":50258,\"start\":49723},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235458009},\"end\":50722,\"start\":50260},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2574224},\"end\":51561,\"start\":50724},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":152282269},\"end\":52023,\"start\":51563},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15458100},\"end\":52593,\"start\":52025},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":234338081},\"end\":53158,\"start\":52595},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14464447},\"end\":53612,\"start\":53160},{\"attributes\":{\"id\":\"b21\"},\"end\":54327,\"start\":53614},{\"attributes\":{\"doi\":\"abs/2301.12597\",\"id\":\"b22\"},\"end\":54672,\"start\":54329},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1502810},\"end\":55057,\"start\":54674},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":46897324},\"end\":55393,\"start\":55059},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":964287},\"end\":55616,\"start\":55395},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14113767},\"end\":56138,\"start\":55618},{\"attributes\":{\"doi\":\"abs/2304.08485\",\"id\":\"b27\"},\"end\":56326,\"start\":56140},{\"attributes\":{\"id\":\"b28\"},\"end\":56750,\"start\":56328},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53592270},\"end\":57086,\"start\":56752},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":252383606},\"end\":57601,\"start\":57088},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":173991173},\"end\":58080,\"start\":57603},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220280200},\"end\":58501,\"start\":58082},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":209413409},\"end\":58915,\"start\":58503},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":237421373},\"end\":59476,\"start\":58917},{\"attributes\":{\"id\":\"b35\"},\"end\":59543,\"start\":59478},{\"attributes\":{\"id\":\"b36\"},\"end\":59614,\"start\":59545},{\"attributes\":{\"id\":\"b37\"},\"end\":59836,\"start\":59616},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":231591445},\"end\":60638,\"start\":59838},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2930547},\"end\":61066,\"start\":60640},{\"attributes\":{\"id\":\"b40\"},\"end\":61506,\"start\":61068},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":249375629},\"end\":62025,\"start\":61508},{\"attributes\":{\"doi\":\"abs/2304.06712\",\"id\":\"b42\"},\"end\":62329,\"start\":62027},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":214693197},\"end\":62792,\"start\":62331},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":85553602},\"end\":63255,\"start\":62794},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":19435386},\"end\":63744,\"start\":63257},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":231719058},\"end\":64504,\"start\":63746},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":248006414},\"end\":65082,\"start\":64506},{\"attributes\":{\"id\":\"b48\"},\"end\":65498,\"start\":65084},{\"attributes\":{\"doi\":\"abs/1601.07140\",\"id\":\"b49\"},\"end\":65848,\"start\":65500},{\"attributes\":{\"id\":\"b50\"},\"end\":66158,\"start\":65850},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":253098274},\"end\":67406,\"start\":66160},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":237416585},\"end\":67880,\"start\":67408},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3864050},\"end\":68415,\"start\":67882},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":206594535},\"end\":68853,\"start\":68417},{\"attributes\":{\"doi\":\"abs/2212.10773\",\"id\":\"b55\"},\"end\":69133,\"start\":68855},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":227238996},\"end\":69610,\"start\":69135},{\"attributes\":{\"doi\":\"abs/2205.12487\",\"id\":\"b57\"},\"end\":69969,\"start\":69612},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":69645185},\"end\":70848,\"start\":69971},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":53734356},\"end\":71281,\"start\":70850},{\"attributes\":{\"id\":\"b60\"},\"end\":71909,\"start\":71283},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":237091474},\"end\":72318,\"start\":71911},{\"attributes\":{\"doi\":\"abs/2304.10592\",\"id\":\"b62\"},\"end\":72654,\"start\":72320}]", "bib_title": "[{\"end\":42824,\"start\":42802},{\"end\":43211,\"start\":43175},{\"end\":43722,\"start\":43685},{\"end\":46451,\"start\":46438},{\"end\":46905,\"start\":46856},{\"end\":47173,\"start\":47117},{\"end\":47571,\"start\":47490},{\"end\":48577,\"start\":48488},{\"end\":49111,\"start\":49023},{\"end\":49821,\"start\":49723},{\"end\":50310,\"start\":50260},{\"end\":50743,\"start\":50724},{\"end\":51650,\"start\":51563},{\"end\":52111,\"start\":52025},{\"end\":52684,\"start\":52595},{\"end\":53227,\"start\":53160},{\"end\":53698,\"start\":53614},{\"end\":54707,\"start\":54674},{\"end\":55125,\"start\":55059},{\"end\":55449,\"start\":55395},{\"end\":55659,\"start\":55618},{\"end\":56789,\"start\":56752},{\"end\":57176,\"start\":57088},{\"end\":57677,\"start\":57603},{\"end\":58126,\"start\":58082},{\"end\":58563,\"start\":58503},{\"end\":58990,\"start\":58917},{\"end\":59907,\"start\":59838},{\"end\":60689,\"start\":60640},{\"end\":61580,\"start\":61508},{\"end\":62398,\"start\":62331},{\"end\":62826,\"start\":62794},{\"end\":63306,\"start\":63257},{\"end\":63805,\"start\":63746},{\"end\":64590,\"start\":64506},{\"end\":66249,\"start\":66160},{\"end\":67456,\"start\":67408},{\"end\":67965,\"start\":67882},{\"end\":68491,\"start\":68417},{\"end\":69206,\"start\":69135},{\"end\":70056,\"start\":69971},{\"end\":70909,\"start\":70850},{\"end\":71959,\"start\":71911}]", "bib_author": "[{\"end\":42837,\"start\":42826},{\"end\":42849,\"start\":42837},{\"end\":42860,\"start\":42849},{\"end\":42869,\"start\":42860},{\"end\":43224,\"start\":43213},{\"end\":43232,\"start\":43224},{\"end\":43241,\"start\":43232},{\"end\":43254,\"start\":43241},{\"end\":43265,\"start\":43254},{\"end\":43278,\"start\":43265},{\"end\":43289,\"start\":43278},{\"end\":43301,\"start\":43289},{\"end\":43735,\"start\":43724},{\"end\":43743,\"start\":43735},{\"end\":43752,\"start\":43743},{\"end\":43763,\"start\":43752},{\"end\":43773,\"start\":43763},{\"end\":43785,\"start\":43773},{\"end\":43800,\"start\":43785},{\"end\":43809,\"start\":43800},{\"end\":43819,\"start\":43809},{\"end\":43829,\"start\":43819},{\"end\":43840,\"start\":43829},{\"end\":43856,\"start\":43840},{\"end\":43867,\"start\":43856},{\"end\":43879,\"start\":43867},{\"end\":43888,\"start\":43879},{\"end\":43898,\"start\":43888},{\"end\":43911,\"start\":43898},{\"end\":43917,\"start\":43911},{\"end\":43927,\"start\":43917},{\"end\":43936,\"start\":43927},{\"end\":43944,\"start\":43936},{\"end\":43954,\"start\":43944},{\"end\":43964,\"start\":43954},{\"end\":43972,\"start\":43964},{\"end\":43981,\"start\":43972},{\"end\":43990,\"start\":43981},{\"end\":44000,\"start\":43990},{\"end\":44014,\"start\":44000},{\"end\":44025,\"start\":44014},{\"end\":44038,\"start\":44025},{\"end\":44048,\"start\":44038},{\"end\":44962,\"start\":44954},{\"end\":44969,\"start\":44962},{\"end\":44977,\"start\":44969},{\"end\":44985,\"start\":44977},{\"end\":44993,\"start\":44985},{\"end\":45195,\"start\":45182},{\"end\":45201,\"start\":45195},{\"end\":45208,\"start\":45201},{\"end\":45217,\"start\":45208},{\"end\":45223,\"start\":45217},{\"end\":45232,\"start\":45223},{\"end\":45241,\"start\":45232},{\"end\":45251,\"start\":45241},{\"end\":45261,\"start\":45251},{\"end\":45275,\"start\":45261},{\"end\":45285,\"start\":45275},{\"end\":45295,\"start\":45285},{\"end\":45669,\"start\":45652},{\"end\":45678,\"start\":45669},{\"end\":45688,\"start\":45678},{\"end\":45699,\"start\":45688},{\"end\":45711,\"start\":45699},{\"end\":45724,\"start\":45711},{\"end\":45736,\"start\":45724},{\"end\":45743,\"start\":45736},{\"end\":45752,\"start\":45743},{\"end\":45764,\"start\":45752},{\"end\":46131,\"start\":46124},{\"end\":46137,\"start\":46131},{\"end\":46143,\"start\":46137},{\"end\":46156,\"start\":46143},{\"end\":46164,\"start\":46156},{\"end\":46172,\"start\":46164},{\"end\":46178,\"start\":46172},{\"end\":46186,\"start\":46178},{\"end\":46193,\"start\":46186},{\"end\":46460,\"start\":46453},{\"end\":46470,\"start\":46460},{\"end\":46479,\"start\":46470},{\"end\":46488,\"start\":46479},{\"end\":46497,\"start\":46488},{\"end\":46510,\"start\":46497},{\"end\":46520,\"start\":46510},{\"end\":46529,\"start\":46520},{\"end\":46917,\"start\":46907},{\"end\":46924,\"start\":46917},{\"end\":46932,\"start\":46924},{\"end\":46940,\"start\":46932},{\"end\":47186,\"start\":47175},{\"end\":47195,\"start\":47186},{\"end\":47199,\"start\":47195},{\"end\":47209,\"start\":47199},{\"end\":47580,\"start\":47573},{\"end\":47587,\"start\":47580},{\"end\":47595,\"start\":47587},{\"end\":47604,\"start\":47595},{\"end\":47612,\"start\":47604},{\"end\":47618,\"start\":47612},{\"end\":48300,\"start\":48289},{\"end\":48312,\"start\":48300},{\"end\":48321,\"start\":48312},{\"end\":48588,\"start\":48579},{\"end\":48595,\"start\":48588},{\"end\":48608,\"start\":48595},{\"end\":48619,\"start\":48608},{\"end\":48629,\"start\":48619},{\"end\":48635,\"start\":48629},{\"end\":48647,\"start\":48635},{\"end\":48658,\"start\":48647},{\"end\":48668,\"start\":48658},{\"end\":48675,\"start\":48668},{\"end\":49122,\"start\":49113},{\"end\":49133,\"start\":49122},{\"end\":49146,\"start\":49133},{\"end\":49161,\"start\":49146},{\"end\":49173,\"start\":49161},{\"end\":49180,\"start\":49173},{\"end\":49190,\"start\":49180},{\"end\":49199,\"start\":49190},{\"end\":49211,\"start\":49199},{\"end\":49230,\"start\":49211},{\"end\":49239,\"start\":49230},{\"end\":49249,\"start\":49239},{\"end\":49256,\"start\":49249},{\"end\":49269,\"start\":49256},{\"end\":49832,\"start\":49823},{\"end\":49840,\"start\":49832},{\"end\":49856,\"start\":49840},{\"end\":49865,\"start\":49856},{\"end\":49875,\"start\":49865},{\"end\":50320,\"start\":50312},{\"end\":50328,\"start\":50320},{\"end\":50338,\"start\":50328},{\"end\":50351,\"start\":50338},{\"end\":50357,\"start\":50351},{\"end\":50365,\"start\":50357},{\"end\":50373,\"start\":50365},{\"end\":50381,\"start\":50373},{\"end\":50759,\"start\":50745},{\"end\":50770,\"start\":50759},{\"end\":50786,\"start\":50770},{\"end\":50795,\"start\":50786},{\"end\":50806,\"start\":50795},{\"end\":50816,\"start\":50806},{\"end\":50828,\"start\":50816},{\"end\":50834,\"start\":50828},{\"end\":50843,\"start\":50834},{\"end\":50852,\"start\":50843},{\"end\":50865,\"start\":50852},{\"end\":50875,\"start\":50865},{\"end\":50890,\"start\":50875},{\"end\":50900,\"start\":50890},{\"end\":50912,\"start\":50900},{\"end\":51664,\"start\":51652},{\"end\":51677,\"start\":51664},{\"end\":52124,\"start\":52113},{\"end\":52137,\"start\":52124},{\"end\":52155,\"start\":52137},{\"end\":52166,\"start\":52155},{\"end\":52179,\"start\":52166},{\"end\":52193,\"start\":52179},{\"end\":52696,\"start\":52686},{\"end\":52707,\"start\":52696},{\"end\":52719,\"start\":52707},{\"end\":52727,\"start\":52719},{\"end\":52733,\"start\":52727},{\"end\":52742,\"start\":52733},{\"end\":52757,\"start\":52742},{\"end\":53239,\"start\":53229},{\"end\":53250,\"start\":53239},{\"end\":53261,\"start\":53250},{\"end\":53272,\"start\":53261},{\"end\":53710,\"start\":53700},{\"end\":53720,\"start\":53710},{\"end\":53734,\"start\":53720},{\"end\":53747,\"start\":53734},{\"end\":53759,\"start\":53747},{\"end\":53771,\"start\":53759},{\"end\":53790,\"start\":53771},{\"end\":54439,\"start\":54433},{\"end\":54445,\"start\":54439},{\"end\":54457,\"start\":54445},{\"end\":54464,\"start\":54457},{\"end\":54715,\"start\":54709},{\"end\":54722,\"start\":54715},{\"end\":54730,\"start\":54722},{\"end\":54737,\"start\":54730},{\"end\":55133,\"start\":55127},{\"end\":55139,\"start\":55133},{\"end\":55147,\"start\":55139},{\"end\":55154,\"start\":55147},{\"end\":55161,\"start\":55154},{\"end\":55169,\"start\":55161},{\"end\":55175,\"start\":55169},{\"end\":55462,\"start\":55451},{\"end\":55671,\"start\":55661},{\"end\":55680,\"start\":55671},{\"end\":55692,\"start\":55680},{\"end\":55700,\"start\":55692},{\"end\":55710,\"start\":55700},{\"end\":55721,\"start\":55710},{\"end\":55731,\"start\":55721},{\"end\":55744,\"start\":55731},{\"end\":56190,\"start\":56183},{\"end\":56196,\"start\":56190},{\"end\":56202,\"start\":56196},{\"end\":56211,\"start\":56202},{\"end\":56421,\"start\":56410},{\"end\":56428,\"start\":56421},{\"end\":56434,\"start\":56428},{\"end\":56444,\"start\":56434},{\"end\":56455,\"start\":56444},{\"end\":56462,\"start\":56455},{\"end\":56470,\"start\":56462},{\"end\":56478,\"start\":56470},{\"end\":56486,\"start\":56478},{\"end\":56493,\"start\":56486},{\"end\":56805,\"start\":56791},{\"end\":56815,\"start\":56805},{\"end\":57184,\"start\":57178},{\"end\":57194,\"start\":57184},{\"end\":57201,\"start\":57194},{\"end\":57208,\"start\":57201},{\"end\":57220,\"start\":57208},{\"end\":57230,\"start\":57220},{\"end\":57241,\"start\":57230},{\"end\":57250,\"start\":57241},{\"end\":57260,\"start\":57250},{\"end\":57689,\"start\":57679},{\"end\":57702,\"start\":57689},{\"end\":57713,\"start\":57702},{\"end\":57725,\"start\":57713},{\"end\":58138,\"start\":58128},{\"end\":58150,\"start\":58138},{\"end\":58161,\"start\":58150},{\"end\":58575,\"start\":58565},{\"end\":58586,\"start\":58575},{\"end\":58597,\"start\":58586},{\"end\":58612,\"start\":58597},{\"end\":59002,\"start\":58992},{\"end\":59014,\"start\":59002},{\"end\":59023,\"start\":59014},{\"end\":59037,\"start\":59023},{\"end\":59555,\"start\":59547},{\"end\":59626,\"start\":59618},{\"end\":59632,\"start\":59626},{\"end\":59638,\"start\":59632},{\"end\":59648,\"start\":59638},{\"end\":59655,\"start\":59648},{\"end\":59920,\"start\":59909},{\"end\":59929,\"start\":59920},{\"end\":59940,\"start\":59929},{\"end\":59950,\"start\":59940},{\"end\":59957,\"start\":59950},{\"end\":59968,\"start\":59957},{\"end\":59978,\"start\":59968},{\"end\":59988,\"start\":59978},{\"end\":59999,\"start\":59988},{\"end\":60008,\"start\":59999},{\"end\":60019,\"start\":60008},{\"end\":60032,\"start\":60019},{\"end\":60706,\"start\":60691},{\"end\":60714,\"start\":60706},{\"end\":60720,\"start\":60714},{\"end\":60730,\"start\":60720},{\"end\":60742,\"start\":60730},{\"end\":60748,\"start\":60742},{\"end\":60757,\"start\":60748},{\"end\":60769,\"start\":60757},{\"end\":60779,\"start\":60769},{\"end\":60792,\"start\":60779},{\"end\":61153,\"start\":61140},{\"end\":61162,\"start\":61153},{\"end\":61174,\"start\":61162},{\"end\":61189,\"start\":61174},{\"end\":61199,\"start\":61189},{\"end\":61208,\"start\":61199},{\"end\":61219,\"start\":61208},{\"end\":61229,\"start\":61219},{\"end\":61244,\"start\":61229},{\"end\":61593,\"start\":61582},{\"end\":61607,\"start\":61593},{\"end\":61616,\"start\":61607},{\"end\":61626,\"start\":61616},{\"end\":61638,\"start\":61626},{\"end\":62118,\"start\":62103},{\"end\":62131,\"start\":62118},{\"end\":62142,\"start\":62131},{\"end\":62411,\"start\":62400},{\"end\":62417,\"start\":62411},{\"end\":62429,\"start\":62417},{\"end\":62438,\"start\":62429},{\"end\":62837,\"start\":62828},{\"end\":62850,\"start\":62837},{\"end\":62858,\"start\":62850},{\"end\":62867,\"start\":62858},{\"end\":62875,\"start\":62867},{\"end\":62884,\"start\":62875},{\"end\":62894,\"start\":62884},{\"end\":62906,\"start\":62894},{\"end\":63316,\"start\":63308},{\"end\":63325,\"start\":63316},{\"end\":63332,\"start\":63325},{\"end\":63341,\"start\":63332},{\"end\":63817,\"start\":63807},{\"end\":63828,\"start\":63817},{\"end\":63839,\"start\":63828},{\"end\":64602,\"start\":64592},{\"end\":64611,\"start\":64602},{\"end\":64622,\"start\":64611},{\"end\":64631,\"start\":64622},{\"end\":64643,\"start\":64631},{\"end\":64652,\"start\":64643},{\"end\":64660,\"start\":64652},{\"end\":65149,\"start\":65138},{\"end\":65159,\"start\":65149},{\"end\":65170,\"start\":65159},{\"end\":65182,\"start\":65170},{\"end\":65196,\"start\":65182},{\"end\":65207,\"start\":65196},{\"end\":65218,\"start\":65207},{\"end\":65227,\"start\":65218},{\"end\":65237,\"start\":65227},{\"end\":65246,\"start\":65237},{\"end\":65595,\"start\":65587},{\"end\":65605,\"start\":65595},{\"end\":65616,\"start\":65605},{\"end\":65625,\"start\":65616},{\"end\":65637,\"start\":65625},{\"end\":65905,\"start\":65897},{\"end\":65911,\"start\":65905},{\"end\":65919,\"start\":65911},{\"end\":65926,\"start\":65919},{\"end\":65933,\"start\":65926},{\"end\":65940,\"start\":65933},{\"end\":65947,\"start\":65940},{\"end\":65954,\"start\":65947},{\"end\":65961,\"start\":65954},{\"end\":66259,\"start\":66251},{\"end\":66269,\"start\":66259},{\"end\":66289,\"start\":66269},{\"end\":66298,\"start\":66289},{\"end\":66309,\"start\":66298},{\"end\":66317,\"start\":66309},{\"end\":66326,\"start\":66317},{\"end\":66344,\"start\":66326},{\"end\":66357,\"start\":66344},{\"end\":66365,\"start\":66357},{\"end\":66375,\"start\":66365},{\"end\":66392,\"start\":66375},{\"end\":66399,\"start\":66392},{\"end\":66410,\"start\":66399},{\"end\":66420,\"start\":66410},{\"end\":66432,\"start\":66420},{\"end\":66442,\"start\":66432},{\"end\":66451,\"start\":66442},{\"end\":66460,\"start\":66451},{\"end\":66469,\"start\":66460},{\"end\":66483,\"start\":66469},{\"end\":66493,\"start\":66483},{\"end\":66504,\"start\":66493},{\"end\":66516,\"start\":66504},{\"end\":66526,\"start\":66516},{\"end\":66535,\"start\":66526},{\"end\":66545,\"start\":66535},{\"end\":66554,\"start\":66545},{\"end\":66563,\"start\":66554},{\"end\":66575,\"start\":66563},{\"end\":66585,\"start\":66575},{\"end\":66594,\"start\":66585},{\"end\":66598,\"start\":66594},{\"end\":66607,\"start\":66598},{\"end\":66616,\"start\":66607},{\"end\":66624,\"start\":66616},{\"end\":67465,\"start\":67458},{\"end\":67474,\"start\":67465},{\"end\":67484,\"start\":67474},{\"end\":67491,\"start\":67484},{\"end\":67499,\"start\":67491},{\"end\":67509,\"start\":67499},{\"end\":67515,\"start\":67509},{\"end\":67524,\"start\":67515},{\"end\":67532,\"start\":67524},{\"end\":67973,\"start\":67967},{\"end\":67981,\"start\":67973},{\"end\":67989,\"start\":67981},{\"end\":67995,\"start\":67989},{\"end\":68004,\"start\":67995},{\"end\":68010,\"start\":68004},{\"end\":68020,\"start\":68010},{\"end\":68499,\"start\":68493},{\"end\":68506,\"start\":68499},{\"end\":68513,\"start\":68506},{\"end\":68520,\"start\":68513},{\"end\":68941,\"start\":68935},{\"end\":68949,\"start\":68941},{\"end\":68958,\"start\":68949},{\"end\":69216,\"start\":69208},{\"end\":69225,\"start\":69216},{\"end\":69234,\"start\":69225},{\"end\":69244,\"start\":69234},{\"end\":69254,\"start\":69244},{\"end\":69719,\"start\":69710},{\"end\":69727,\"start\":69719},{\"end\":69734,\"start\":69727},{\"end\":69744,\"start\":69734},{\"end\":69753,\"start\":69744},{\"end\":70064,\"start\":70058},{\"end\":70070,\"start\":70064},{\"end\":70076,\"start\":70070},{\"end\":70082,\"start\":70076},{\"end\":70090,\"start\":70082},{\"end\":70100,\"start\":70090},{\"end\":70107,\"start\":70100},{\"end\":70922,\"start\":70911},{\"end\":70930,\"start\":70922},{\"end\":70941,\"start\":70930},{\"end\":70949,\"start\":70941},{\"end\":71294,\"start\":71285},{\"end\":71301,\"start\":71294},{\"end\":71309,\"start\":71301},{\"end\":71318,\"start\":71309},{\"end\":71327,\"start\":71318},{\"end\":71335,\"start\":71327},{\"end\":71342,\"start\":71335},{\"end\":71348,\"start\":71342},{\"end\":71356,\"start\":71348},{\"end\":71362,\"start\":71356},{\"end\":71370,\"start\":71362},{\"end\":71378,\"start\":71370},{\"end\":71387,\"start\":71378},{\"end\":71393,\"start\":71387},{\"end\":71399,\"start\":71393},{\"end\":71405,\"start\":71399},{\"end\":71412,\"start\":71405},{\"end\":71420,\"start\":71412},{\"end\":71427,\"start\":71420},{\"end\":71434,\"start\":71427},{\"end\":71442,\"start\":71434},{\"end\":71450,\"start\":71442},{\"end\":71456,\"start\":71450},{\"end\":71464,\"start\":71456},{\"end\":71472,\"start\":71464},{\"end\":71970,\"start\":71961},{\"end\":71978,\"start\":71970},{\"end\":71985,\"start\":71978},{\"end\":71992,\"start\":71985},{\"end\":72415,\"start\":72408},{\"end\":72423,\"start\":72415},{\"end\":72431,\"start\":72423},{\"end\":72437,\"start\":72431},{\"end\":72450,\"start\":72437}]", "bib_venue": "[{\"end\":42943,\"start\":42869},{\"end\":43391,\"start\":43301},{\"end\":44165,\"start\":44048},{\"end\":44952,\"start\":44880},{\"end\":45368,\"start\":45295},{\"end\":45650,\"start\":45583},{\"end\":46122,\"start\":46038},{\"end\":46592,\"start\":46529},{\"end\":46971,\"start\":46940},{\"end\":47263,\"start\":47209},{\"end\":47730,\"start\":47618},{\"end\":48287,\"start\":48228},{\"end\":48736,\"start\":48675},{\"end\":49317,\"start\":49269},{\"end\":49938,\"start\":49875},{\"end\":50469,\"start\":50381},{\"end\":51054,\"start\":50912},{\"end\":51746,\"start\":51677},{\"end\":52256,\"start\":52193},{\"end\":52825,\"start\":52757},{\"end\":53335,\"start\":53272},{\"end\":53901,\"start\":53790},{\"end\":54431,\"start\":54329},{\"end\":54816,\"start\":54737},{\"end\":55206,\"start\":55175},{\"end\":55493,\"start\":55462},{\"end\":55795,\"start\":55744},{\"end\":56181,\"start\":56140},{\"end\":56408,\"start\":56328},{\"end\":56882,\"start\":56815},{\"end\":57330,\"start\":57260},{\"end\":57794,\"start\":57725},{\"end\":58241,\"start\":58161},{\"end\":58686,\"start\":58612},{\"end\":59124,\"start\":59037},{\"end\":59507,\"start\":59480},{\"end\":60111,\"start\":60032},{\"end\":60832,\"start\":60792},{\"end\":61138,\"start\":61068},{\"end\":61689,\"start\":61638},{\"end\":62101,\"start\":62027},{\"end\":62489,\"start\":62438},{\"end\":62975,\"start\":62906},{\"end\":63428,\"start\":63341},{\"end\":64096,\"start\":63839},{\"end\":64741,\"start\":64660},{\"end\":65136,\"start\":65084},{\"end\":65585,\"start\":65500},{\"end\":65895,\"start\":65850},{\"end\":66710,\"start\":66624},{\"end\":67620,\"start\":67532},{\"end\":68072,\"start\":68020},{\"end\":68583,\"start\":68520},{\"end\":68933,\"start\":68855},{\"end\":69322,\"start\":69254},{\"end\":69708,\"start\":69612},{\"end\":70345,\"start\":70107},{\"end\":71018,\"start\":70949},{\"end\":72066,\"start\":71992},{\"end\":72406,\"start\":72320},{\"end\":42963,\"start\":42945},{\"end\":44234,\"start\":44227},{\"end\":46611,\"start\":46594},{\"end\":47304,\"start\":47265},{\"end\":47821,\"start\":47797},{\"end\":49332,\"start\":49319},{\"end\":49957,\"start\":49940},{\"end\":51183,\"start\":51056},{\"end\":51767,\"start\":51748},{\"end\":52275,\"start\":52258},{\"end\":52847,\"start\":52827},{\"end\":53354,\"start\":53337},{\"end\":53999,\"start\":53903},{\"end\":54882,\"start\":54818},{\"end\":55816,\"start\":55797},{\"end\":56904,\"start\":56884},{\"end\":57815,\"start\":57796},{\"end\":58308,\"start\":58243},{\"end\":59198,\"start\":59126},{\"end\":60198,\"start\":60134},{\"end\":61707,\"start\":61691},{\"end\":62502,\"start\":62491},{\"end\":62996,\"start\":62977},{\"end\":63502,\"start\":63430},{\"end\":64809,\"start\":64743},{\"end\":66783,\"start\":66712},{\"end\":68133,\"start\":68074},{\"end\":68603,\"start\":68585},{\"end\":69344,\"start\":69324},{\"end\":70368,\"start\":70347},{\"end\":71039,\"start\":71020},{\"end\":72127,\"start\":72068}]"}}}, "year": 2023, "month": 12, "day": 17}