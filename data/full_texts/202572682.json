{"id": 202572682, "updated": "2023-12-14 08:01:02.35", "metadata": {"title": "Energy-Based Continuous Inverse Optimal Control", "authors": "[{\"first\":\"Yifei\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Jianwen\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Tianyang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Baker\",\"middle\":[]},{\"first\":\"Yibiao\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Wu\",\"middle\":[\"Nian\"]}]", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "publication_date": {"year": 2023, "month": 12, "day": 1}, "abstract": "The problem of continuous inverse optimal control (over finite time horizon) is to learn the unknown cost function over the sequence of continuous control variables from expert demonstrations. In this article, we study this fundamental problem in the framework of energy-based model (EBM), where the observed expert trajectories are assumed to be random samples from a probability density function defined as the exponential of the negative cost function up to a normalizing constant. The parameters of the cost function are learned by maximum likelihood via an \u201canalysis by synthesis\u201d scheme, which iterates: 1) synthesis step: sample the synthesized trajectories from the current probability density using the Langevin dynamics via backpropagation through time and 2) analysis step: update the model parameters based on the statistical difference between the synthesized trajectories and the observed trajectories. Given the fact that an efficient optimization algorithm is usually available for an optimal control problem, we also consider a convenient approximation of the above learning method, where we replace the sampling in the synthesis step by optimization. Moreover, to make the sampling or optimization more efficient, we propose to train the EBM simultaneously with a top-down trajectory generator via cooperative learning, where the trajectory generator is used to fast initialize the synthesis step of the EBM. We demonstrate the proposed methods on autonomous driving tasks and show that they can learn suitable cost functions for optimal control.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2973184296", "acl": null, "pubmed": "35511835", "pubmedcentral": null, "dblp": "journals/tnn/XuXZBZW23", "doi": "10.1109/tnnls.2022.3168795"}}, "content": {"source": {"pdf_hash": "a496f4d613059207d8c61bb34b54bf6e14e8feb1", "pdf_src": "IEEE", "pdf_uri": "[\"https://arxiv.org/pdf/1904.05453v6.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1904.05453", "status": "GREEN"}}, "grobid": {"id": "98e86272cc95bbbbf0ee1e021fc10c150eb89563", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/a496f4d613059207d8c61bb34b54bf6e14e8feb1.txt", "contents": "\n\n\n\nYifei Xu \nDepartment of Sta-tistics\nUniversity of California at Los Angeles\n90095Los AngelesCAUSA\n\nDepartment of Sta-tistics\nUniversity of California at Los Angeles\n90095Los AngelesCAUSA\n\nTianyang Zhao tyzhao@ucla.edu \nDepartment of Sta-tistics\nUniversity of California at Los Angeles\n90095Los AngelesCAUSA\n\nYing Nian Wu \nDepartment of Sta-tistics\nUniversity of California at Los Angeles\n90095Los AngelesCAUSA\n\nJianwen Xie jianwen@ucla.edu \nCognitive Computing Laboratory\nBaidu Research\n98004BellevueWAUSA\n\nChris Baker chrisbaker@isee.ai \nare with iSee, Inc\n02139CambridgeMAUSA\n\nYibiao Zhao \nare with iSee, Inc\n02139CambridgeMAUSA\n141A6297CD2BF9171D8C75F4AABC357D10.1109/TNNLS.2022.3168795\n\n\nEnergy-Based Continuous Inverse Optimal Control\n\nYifei Xu , Jianwen Xie , Member, IEEE, Tianyang Zhao, Chris Baker, Yibiao Zhao, and Ying Nian Wu, Member, IEEE Abstract-The problem of continuous inverse optimal control (over finite time horizon) is to learn the unknown cost function over the sequence of continuous control variables from expert demonstrations.In this article, we study this fundamental problem in the framework of energy-based model (EBM), where the observed expert trajectories are assumed to be random samples from a probability density function defined as the exponential of the negative cost function up to a normalizing constant.The parameters of the cost function are learned by maximum likelihood via an \"analysis by synthesis\" scheme, which iterates: 1) synthesis step: sample the synthesized trajectories from the current probability density using the Langevin dynamics via backpropagation through time and 2) analysis step: update the model parameters based on the statistical difference between the synthesized trajectories and the observed trajectories.Given the fact that an efficient optimization algorithm is usually available for an optimal control problem, we also consider a convenient approximation of the above learning method, where we replace the sampling in the synthesis step by optimization.Moreover, to make the sampling or optimization more efficient, we propose to train the EBM simultaneously with a top-down trajectory generator via cooperative learning, where the trajectory generator is used to fast initialize the synthesis step of the EBM.We demonstrate the proposed methods on autonomous driving tasks and show that they can learn suitable cost functions for optimal control.Index Terms-Cooperative learning, energy-based models (EBMs), inverse optimal control (IOC), Langevin dynamics.\n\n\nI. INTRODUCTION\n\n\nA. Background and Motivation\n\nT HE problem of continuous optimal control has been extensively studied.In this article, we study the control problem of finite time horizon, where the trajectory is over a finite period of time.In particular, we focus on the problem of autonomous driving as a concrete example.In continuous optimal control, the control variables or actions are continuous.\n\nThe dynamics is known.The cost function is defined on the trajectory and is usually in the form of the sum of stepwise costs and the cost of the final state.We call such a cost function Markovian.The continuous optimal control seeks to minimize the cost function over the sequence of continuous control variables or actions, and many efficient algorithms have been developed for various optimal control problems [1].For instance, in autonomous driving, the iterative linear quadratic regulator (iLQR) algorithm is a commonly used optimization algorithm [2], [3].We call such an algorithm the built-in optimization algorithm for the corresponding control problem.\n\nIn applications such as autonomous driving, the dynamics is well defined by the underlying physics and mechanics.However, it is a much harder problem to design or specify the cost function.One solution to this problem is to learn the cost function from expert demonstrations by observing their sequences of actions.Learning the cost function in this way is called continuous inverse optimal control (IOC) problem.\n\nIn this article, we study the fundamental problem of continuous IOC in the framework of energy-based model (EBM) [4].Originated from statistical physics, an EBM is a probability distribution where the probability density function is in the form of exponential of the negative energy function up to a normalizing constant.The energy function maps the input into a scalar, which is called energy.Instances with low energies are assumed to be more likely according to the model.For continuous IOC, the cost function plays the role of energy function, and the observed expert sequences are assumed to be random samples from the EBM so that sequences with low costs are more likely to be observed.We can choose the cost function either as a linear combination of a set of hand-designed features or a nonlinear and non-Markovian neural network.The goal is to learn the parameters of the cost function from the expert sequences.\n\nThe parameters can be learned by the maximum likelihood estimation (MLE) in the context of the EBM.The maximum likelihood learning algorithm follows an \"analysis by synthesis\" scheme, which iterates the following two steps.\n\n\n1) Synthesis\n\nStep: Sample the synthesized trajectories from the current probability distribution using the Markov chain Monte Carlo (MCMC), such as Langevin dynamics [5].The gradient computation in the Langevin dynamics can be conveniently and efficiently carried out by backpropagation through time.\n\n\n2) Analysis\n\nStep: Update the model parameters based on the statistical difference between the synthesized trajectories and the observed trajectories.Such a learning algorithm is very general, and it can learn complex cost functions such as those defined by neural networks.\n\nWe need to point out that MLE is the most commonly used method for learning EBMs, due to its asymptotic optimality.Among all the asymptotically unbiased estimators, MLE is the most accurate in terms of asymptotic variance [6].Alternative methods for learning EBMs include contrastive divergence (CD) [7] and noise contrastive estimation (NCE) [8].CD replaces MCMC by one or a few steps of MCMC sampling initialized from observed examples, and as a result, it has a big bias.NCE estimates the energy function discriminatively by recruiting a noise distribution to produce negative or contrastive examples against the observed examples that are treated as positive examples.For accurate estimation, the noise distribution should have substantial overlap with the data distribution.For high-dimensional observations such as trajectories, it is difficult to find such a noise distribution.If the noise distribution does not have sufficient overlap with the data distribution, the estimate will have a big variance.Therefore, the maximum likelihood training is a more preferable algorithm to train EBMs.\n\nFor an optimal control problem where the cost function is of the Markovian form, a built-in optimization algorithm is usually already available, such as the iLQR algorithm for autonomous driving.In this case, we also consider a convenient modification of the above learning method, where we change the synthesis step into an optimization step while keeping the analysis step unchanged.We give justifications for this optimization-based method, although we want to emphasize that the sampling-based method is still more fundamental and principled, and we treat the optimization-based method as a convenient modification.\n\nMoreover, we propose another novel energy-based IOC framework, where the EBM is trained with a top-down trajectory generator that serves as a fast initializer of the Langevin sampling of the EBM through a cooperative learning manner [9], [10].Within each cooperative learning iteration, the trajectory generator generates initial trajectories to initialize a finite-step Langevin dynamics that samples from the EBM, and then, the EBM is trained by comparing the expert trajectories and the synthesized trajectories.After that, the trajectory generator learns from how the MCMC changes its initial generated trajectories.The proposed framework belongs to the \"fast thinking initializer and slow thinking solver\" framework [11].The trajectory generator plays the role of the fast thinking initializer because its generation of trajectory is accomplished by direct mapping, while the EBM plays the role of the slow thinking solver because it learns a cost function in the form of a conditional energy function, so that the trajectory can be synthesized by minimizing the cost function or more rigorously by sampling from the EBM.The trajectory generator is like a policy, while the EBM is like a planner.Compared to the GAN-type method, ours is equipped with an iterative refining process (slow thinking) guided by a learned cost (energy) function.\n\nWe empirically demonstrate the proposed energy-based IOC methods on autonomous driving in both singleagent and multiagent scenarios and show that the proposed methods can learn suitable cost functions for optimal control.\n\n\nB. Related Work\n\nThe following are research themes related to our work.1) Maximum Entropy Framework: Our work follows the maximum entropy framework mentioned in [12] for learning the cost function.Such a framework has also been used previously for generative modeling of images [13] and Markov logic network [14].\n\nIn this framework, the energy function is a linear combination of hand-designed features.Recently, Wulfmeier et al. [15] generalized this framework to a deep version.In these methods, the state spaces are discrete, where dynamic programming schemes can be employed to calculate the normalizing constant of the EBM.In our work, the state space is continuous, where we use Langevin dynamics via backpropagation through time to sample trajectories from the learned model.We also propose an optimization-based method where we use the gradient descent (GD) algorithm or a built-in optimal control algorithm as the inner loop for learning.2) ConvNet-EBM: Recently, Xie et al. [16]- [21] applied deep EBMs to various generative modeling tasks, where the energy functions are parameterized by Con-vNets [22], [23].Our method is different from ConvNet EBM.The control variables in our method form a time sequence.In gradient computation for Langevin sampling, backpropagation through time is used.Also, we propose an optimization-based modification and give justifications.3) Inverse Reinforcement Learning: Most of the inverse reinforcement learning methods [24], [25], including adversarial learning methods [25]- [28], involve learning a policy in addition to the cost function.In our work, the energy-based IOC framework (without an extra trajectory generator) does not learn any policy, and it only learns a cost function (i.e., the energy function), where the trajectories are sampled by the Langevin dynamics or obtained by GD or a built-in optimal control algorithm.4) Continuous IOC: The IOC problem has been studied in [29] and [30].In [29], the dynamics is linear and the cost function is quadratic so that the normalizing constant can be computed by a dynamic programming scheme.In [30], the Laplace approximation is used for approximation.However, the accuracy of the Laplace approximation is questionable for complex cost function.\n\nIn our work, we assume general dynamics and cost function and use Langevin sampling for maximum likelihood learning without resorting to Laplace approximation.5) Trajectory Prediction: A recent body of research has been devoted to supervised learning for trajectory prediction [31]- [36].These methods directly predict the coordinates and do not consider control and dynamic models.Thus, they cannot be used for inverse optimal control.6) Cooperative Learning: Our joint training framework for IOC follows the generative cooperative learning algorithm (i.e., the CoopNets algorithm) mentioned in [10] for training the cost function in the EBM and the Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\n\ntrajectory generator.Such a learning algorithm has also been applied previously to image generation [10], video generation [10], 3-D shape generation [20], supervised conditional learning [11], and unsupervised image-toimage translation [37].The CoopVAEBM [38] is a variant of the CoopNets algorithm by replacing the generic generator with a variational autoencoder (VAE) [39].\n\nThe CoopFlow [40] is another variant of the CoopNets algorithm by changing the generator into a normalizing flow [41].\n\n\nC. Contributions\n\nThe contributions of our work are given as follows.\n\n1) We propose an energy-based method for continuous IOC based on Langevin sampling via backpropagation through time.To the best of our knowledge, this is the first work that studies MCMC sampling-based IOC.Such an \"analysis by synthesis\" learning scheme makes our work essentially different from [12], [29], and [30].2) We also propose an optimization-based method as a convenient approximation of the MCMC sampling under the proposed energy-based learning framework.The modified algorithm becomes an \"analysis by optimization\" scheme.3) We evaluate the proposed methods on autonomous driving tasks for trajectory prediction.We apply our framework to both single-agent system and multiagent system, with both linear cost function and neural network nonlinear cost function.This is the first work to study vehicle trajectory prediction under the energybased framework.4) We also propose to train an EBM together with a policy-like trajectory generator, which serves as a fast initializer for the Langevin sampling, in a cooperative learning scheme.5) We conduct extensive ablation studies to analyze the effects of the key components and hyperparameters of the proposed frameworks to understated the model behaviors.\n\n\nD. Organization\n\nThe rest of this article is organized as follows.Section II presents the proposed framework of the energy-based IOC.Section III presents the proposed joint training framework, in which the EBM is trained simultaneously with a trajectory generator as amortized sampler.Qualitative and quantitative results of experiments are shown in Section IV.The conclusion of this article is given in Section V.\n\n\nII. ENERGY-BASED IOC\n\n\nA. Optimal Control\n\nWe study the finite-horizon control problem for discrete time t \u2208 {1, . . ., T }.Let x t be the state at time t.Let x = (x t , t = 1, . . ., T ).Let u t be the continuous control variable or action at time t.Let u = (u t , t = 1, . . ., T ).The dynamics is assumed to be deterministic,\nx t = f (x t\u22121 , u t ),\nwhere f is given, so that u determines x.The trajectory is (x, u) = (x t , u t , t = 1, . . ., T ).Let e be the environment condition.We assume that the recent history h = (x t , u t , t = \u2212k, . . ., 0) is known.\n\nThe cost function is C \u03b8 (x, u, e, h), where \u03b8 consists of the parameters that define the cost function.Its special case is of the linear form C \u03b8 (x, u, e, h) = \u03b8, \u03c6(x, u, e, h), where \u03c6 is a vector of hand-designed features and \u03b8 is a vector of weights for these features.We can also parameterize C \u03b8 by a neural network.The problem of optimal control is to find u to minimize C \u03b8 (x, u, e, h) with given e and h under the known dynamics f .The problem of IOC is to learn \u03b8 from expert demonstrations D = {(x i , u i , e i , h i ), i = 1, . . ., n}.\n\n\nB. Energy-Based Probabilistic Model\n\nThe EBM assumes the following conditional probability density function:\np \u03b8 (u|e, h) = 1 Z \u03b8 (e, h) exp[\u2212C \u03b8 (x, u, e, h)] (1)\nwhere Z \u03b8 (e, h) = exp[\u2212C \u03b8 (x, u, e, h)]du is the normalizing constant.Recall that x is determined by u according to the deterministic dynamics so that we only need to define probability density on u.The cost function C \u03b8 serves as the energy function.For expert demonstrations D, u i are assumed to be random samples from p \u03b8 (u|e i , h i ) so that u i tends to have low cost C \u03b8 (x, u, e i , h i ).\n\n\nC. Sampling-Based IOC\n\nThe parameters \u03b8 can be learned by maximum likelihood.The log likelihood is given by\nL(\u03b8 ) = 1 n n i=1 log p \u03b8 (u i |e i , h i ). (2)\nWe can maximize L(\u03b8 ) by gradient ascent, and the learning gradient is computed by\nL (\u03b8 ) = 1 n n i=1 [E p \u03b8 (u|e i ,h i ) ( \u2202 \u2202\u03b8 C \u03b8 (x, u, e i , h i )) \u2212 \u2202 \u2202\u03b8 C \u03b8 (x i , u i , e i , h i )] (3)\nwhich follows the property of the normalizing constant\n(\u2202/\u2202\u03b8 ) log Z \u03b8 (e, h) = \u2212E p \u03b8 (u|e,h) ((\u2202/\u2202\u03b8 )C \u03b8 (x, u, e, h)).\nIn order to approximate the above expectation, we can generate multiple random samples via \u0169i \u223c p \u03b8 (u|e, h), which generates each sampled trajectory (x i , \u0169i ) by unfolding the dynamics.We estimate L (\u03b8 ) by\nL (\u03b8 ) = 1 n n i=1 [ \u2202 \u2202\u03b8 C \u03b8 (x i , \u0169i , e i , h i ) \u2212 \u2202 \u2202\u03b8 C \u03b8 (x i , u i , e i , h i )](4)\nwhich is the stochastic unbiased estimator of L (\u03b8 ).Then, we can run the gradient ascent algorithm \u03b8 \u03c4 +1 = \u03b8 \u03c4 +\u03b3 \u03c4 L (\u03b8 \u03c4 ) to obtain the maximum likelihood estimate of \u03b8 , where \u03c4 indexes the time step and \u03b3 \u03c4 is the step size.According to the Robbins-Monroe theory of stochastic approximation [42],\nif \u03c4 \u03b3 \u03c4 = \u221e and \u03c4 \u03b3 2\n\u03c4 < \u221e, the algorithm will converge to a solution of L (\u03b8 ) = 0.For each i , we can also generate multiple copies of (x i , \u0169i ) from p \u03b8 (u|e i , h i ) and average them to approximate the expectation in (3).A small number is sufficient because the averaging effect takes place over time.\n\nIn the linear case, where\nC \u03b8 (x, u, e, h) = \u03b8, \u03c6(x, u, e, h), we have (\u2202/\u2202\u03b8 )C \u03b8 (x, u, e, h) = \u03c6(x, u, e, h), making L (\u03b8 ) = (1/n) n i=1 [\u03c6(x i , \u0169i , e i , h i ) \u2212 \u03c6(x i , u i , e i , h i )].\nIt is the statistical difference between the observed trajectories and synthesized trajectories.At maximum likelihood estimate, the two match each other.\n\nThe synthesis step that samples from p \u03b8 (u|e, h) can be accomplished by an efficient gradient-based MCMC, the Langevin dynamics, which iterates the following steps:\nu s+1 = u s + \u03b4 2 2 \u2202 \u2202u log p \u03b8 (u s |e, h) + \u03b4z s = u s \u2212 \u03b4 2 2 \u2202 \u2202u C \u03b8 (x s , u s , e, h) + \u03b4z s (5)\nwhere s indexes the time step, \u03b4 is the step size, and z s \u223c N (0, I ) the Brownian motion independently over s, with I the identity matrix of the same dimension as u.The Langevin dynamics is an inner loop of the learning algorithm, with u 0 (u at the initial time step) being initialized by Gaussian white noise.The GD part 5) is a mode seeking process that minimizes the cost function C \u03b8 , while the added Gaussian noise z s will prevent the samples from being trapped by local minima.According to the second law of thermodynamics [43], as s \u2192 \u221e and \u03b4 \u2192 0, u s becomes an exact sample from p \u03b8 (u|e, h) under some regularity conditions.A Metropolis-Hastings [44] step can also be added to correct for the error due to discrete time step in (5), but most existing works, such as [4], [45], and [46], have shown that this can be ignored in practice if a small enough step size \u03b4 is used.Thus, for computational efficiency, in this work, we do not have the Metropolis-Hastings correction in our implementation.The gradient term \u2202C \u03b8 (x, u, e, h)/\u2202u is computed via backpropagation through time, where x can be obtained from u by unrolling the deterministic dynamics.The computation can be efficiently and conveniently carried out by autodifferentiation on the current deep learning platforms.\nu s+1 = u s \u2212 (\u03b4 2 /2)(\u2202/\u2202u)C \u03b8 (x s , u s , e, h) of (\n\nD. Optimization-Based IOC\n\nWe can remove the noise term in Langevin dynamics in (5), to make it a GD process, i.e., u s+1 = u s \u2212 \u03b7(\u2202/\u2202u)C \u03b8 (x s , u s , e, h), and we can still learn the cost function that enables optimal control.This amounts to modifying the synthesis step into an optimization step.\n\nMoreover, a built-in optimization algorithm is usually already available for minimizing the cost function C \u03b8 (x, u, e, h) over u.For instance, in autonomous driving, a commonly used algorithm is iLQR.In this case, we can replace the synthesis step by an optimization step, where, instead of sampling \u0169i \u223c p \u03b8 t (u|e i , h i ), we optimize\n\u0169i = arg min u C \u03b8 (x, u, e i , h i ). (6)\nThe analysis step remains unchanged.In this article, we emphasize the sampling-based method, which is more principled maximum likelihood learning, and treat the optimization-based method as a convenient modification.We will evaluate both learning methods in our experiments.\n\nA justification for the optimization-based algorithm in the context of the EBM in ( 1) is to consider its tempered version\np \u03b8 (u|e, h) \u221d exp[\u2212C \u03b8 (x, u, e, h)/T ],\nwhere T is the temperature.Then, the optimized \u0169 that minimizes C \u03b8 (x, u, e, h) can be considered the zero-temperature sample, which is used to approximate the expectation in (3).\n\n1) Moment Matching: For simplicity, consider the linear cost function C \u03b8 (x, u, e, h) = \u03b8, \u03c6(x, u, e, h).At the convergence of the optimization-based learning algorithm, which has the same analysis step as the sampling-based algorithm, we have L (\u03b8 ) = 0 so that\n1 n n i=1 \u03c6(x i , \u0169i , e i , h i ) = 1 n n i=1 \u03c6(x i , u i , e i , h i ) (7)\nwhere the left-hand side is the average of the optimal behaviors obtained by ( 6) and the right-hand side is the average of the observed behaviors.We want the optimal behaviors to match the observed behaviors on average.We can see the above point most clearly in the extreme case where all e i = e and all\nh i = h so that \u03c6(x, \u0169, e, h) = (1/n) n i=1 \u03c6(x i , u i , e, h), i.\ne., we want the optimal behavior under the learned cost function to match the average observed behaviors as far as the features of the cost function are concerned.Note that the matching is not in terms of raw trajectories but in terms of the features of the cost function.In this matching, we do not care about modeling the variabilities in the observed behaviors.In the case of different (e i , h i ) for i = 1, . . ., n, the matching may not be exact for each combination of (e, h).However, such mismatches may be detected by new features, which can be included in the features of the cost function.\n\n2) Adversarial Learning: We can also justify this optimization-based algorithm outside the context of probabilistic model as adversarial learning.To this end, we rethink about the IOC, whose goal is not to find a probabilistic model for the expert trajectories.Instead, the goal is to find a suitable cost function for optimal control, where we care about the optimal behavior, not the variabilities of the observed behaviors.Define the value function\nV (\u03b8, { \u0169i }) = 1 n n i=1 [C \u03b8 (x i , \u0169i , e i , h i ) \u2212 C \u03b8 (x i , u i , e i , h i )](8)\nand then, L (\u03b8 ) = (\u2202/\u2202\u03b8 )V so that the analysis step increases V .The optimization step and the analysis step play an adversarial game max \u03b8 min \u0169i ,\u2200i V , where the optimization step seeks to minimize V by reducing the costs, while the analysis step seeks to increase V by modifying the cost function.More specifically, the optimization step finds the minima of the cost functions to decrease V , whereas the analysis step shifts the minima toward the observe trajectories in order to increase V .\n\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\nanalysis step: update \u03b8 \u03c4 +1 = \u03b8 \u03c4 + \u03b3 \u03c4 L (\u03b8 \u03c4 ),\nwhere L is computed according to Equation (4).\n\n\n7:\n\n\u03c4 \u2190 \u03c4 + 1. 8: until \u03c4 = \u03c4 max , the number of iterations.\n\n\nE. Energy-Based IOC Algorithm\n\nAlgorithms 1 and 2 present the sampling-and optimization-based learning algorithms, respectively.We treat the sampling-based method as a more fundamental and principled method and the optimization-based method as a convenient modification.In our experiments, we shall evaluate both sampling-based method using Langevin dynamics and optimization-based method with GD or iLQR as optimizer.\n\n\nIII. JOINT TRAINING\n\n\nA. Trajectory Generator Model as a Fast Initializer\n\nBoth sampling-based method via Langevin dynamics and optimization-based method via GD are based on an iterative process, which will benefit from good initialization.A good initial point can not only greatly shorten the number of iterative steps but also help find the optimal modes of the cost function.Therefore, we propose to train an EBM simultaneously with a trajectory generator model that serves as a fast initializer for the Langevin dynamics or GD of the EBM.\n\nThe basic idea is to use the trajectory generator model to generate trajectories via ancestral sampling to initialize a finite-step Langevin dynamics or GD for training the EBM.In return, the trajectory generator model learns from how the Langevin dynamics or GD updates the initial trajectories it generates.Such a cooperative learning strategy is proposed in [10], [11], and [40] for image generation.\n\nTo be specific, we propose the trajectory generator model that consists of the following two components:\nu t = F \u03b1 (x t\u22121 , \u03be t , e) (9)x t = f (x t\u22121 , u t ) (10)\nwhere t = 1, . . ., T .( 9) is the policy model and ( 10) is the known dynamic function.\u03be t \u223c N (0, I ) is the Gaussian noise vector.The Gaussian noise vectors at different times (\u03be t , t = 1, . . ., T ) are independent of each other.Given the state x t\u22121 at the previous time step t \u2212 1 along with the environment condition e, the policy model outputs the action u t at the current time step t, where the noise vector \u03be t accounts for the randomness in the mapping from x t\u22121 to u t .F \u03b1 is a multilayer perceptron (MLP), where \u03b1 is the model parameters of the network.The initial state x 0 is assumed to be given.\n\nWe denote \u03be = (\u03be t , t = 1, . . ., T ) and p(\u03be ) = T t=1 p(\u03be t ).Given the state x t\u22121 and the environment condition e, although x t is dependent on the action u t , u t is generated from \u03be t .In fact, we can write the trajectory generator in a compact form, i.e., u = G \u03b1 (\u03be , e, h), where G \u03b1 composes F \u03b1 and f over time, and we use h = x 0 for simplicity in our implementation.\n\nThe algorithm for joint training of the EBM and the trajectory generator is that at each iteration: 1) we first sample \u03be i from the Gaussian prior distribution and then generate the initial trajectories by \u00fbi = G \u03b1 (\u03be i , e i , h i ) for i = 1, . . ., n; 2) starting from the initial trajectories { \u00fbi }, we sample from the EBM by running a finite number of Langevin steps or optimize the cost function by running a finite step of GD to obtain the updated trajectories { \u0169i } and then obtain {x i }; 3) we update the parameters \u03b8 of the EBM by MLE, where the computation of the gradient of the likelihood is based on { \u0169i } and follows (4); and 4) we update the parameters \u03b1 of the trajectory generator by GD on the loss\nl g (\u03b1) = \u2202 \u2202\u03b1 [ 1 n n i=1 \u0169i \u2212 G \u03b1 (\u03be i , e i , h i ) 2 ].(11)\nAlgorithm 3 presents a detailed description of the cooperative training algorithm of an EBM and a trajectory generator for IOC.Synthesis step and optimization step are two options to generate ( \u0169, x).\n\n\nB. Bottom-Up and Top-Down Generative Models of Trajectories\n\nAlgorithm 3 presented in the main text is about a joint training of two types of generative models, the EBM (we also call it the trajectory evaluator) and the latent variable model (i.e., the trajectory generator).Both these two models can be parameterized by deep neural networks and they are in opposite directions.The EBM has a bottom-up energy function that maps the trajectory to the cost, while the trajectory generator owns a top-down transformation that maps the sequence of noise vectors (i.e., the latent variables) to the trajectory,\n\n\nC. Iterative and Noniterative Generations of Trajectories\n\nThe EBM p \u03b8 (u|e, h) defines a cost function or an energy function C \u03b8 (x, u, e, h), from which we can derive the Langevin dynamics to generate u.This is an implicit generation process of u that iterates the Langevin step in (5).\n\nFig. 1(a) shows the generation process of (u, x).Given (h, e), the Langevin sampling seeks to find u = (u 1 , . . ., u T ) to minimize the C \u03b8 (x, u, e, h).The dashed double line arrows indicate iterative generation by sampling in the EBM, while the dashed line arrows indicate the known dynamic function x t = f (x t\u22121 , u t ).With the generated action sequence u = (u 1 , . . ., u T ), the state sequence x = (x 1 , . . ., x T ) can be easily obtained by applying the dynamic function.\n\nThe trajectory generator generates (u, x) via ancestral sampling, (u, x) = G \u03b1 (\u03be , e, h), which is a noniterative process to produce (u, x) from the recent history h (we assume h = x 0 ), environment e, and a sequence of noise vectors \u03be = (\u03be 1 , . . ., \u03be T ) serving as the latent variables.The generator can unfold over time and can be decomposed into the policy model u t = F \u03b1 (x t\u22121 , \u03be t , e) and the dynamic function x t = f (x t\u22121 , u t ) at each time step.The latent variable \u03be t accounts for variation in the policy model at time step t.Fig. 1(b) shows the generation process of the trajectory generator.The double\n\n\nD. Issue in Maximum Likelihood Training of a Single Trajectory Generator\n\nLet \u03be = (\u03be t , t = 1, . . ., T ), where \u03be t \u223c N (0, I ) independently over time t.Let u = (u t , t = 1, . . ., T ).We have u = G \u03b1 (\u03be , e, h) + , where = ( t , t = 1, . . ., T ) are observation errors and t \u223c N (0, \u03c3 2 I ).For notational simplicity, we omit x and only keep u in the output of G \u03b1 because x is just the intermediate output of G \u03b1 and u is determined by x.The trajectory generator defines the joint distribution of (u, \u03be ) conditioned on (e, h) as follows:\nq \u03b1 (u, \u03be |e, h) = q \u03b1 (u|\u03be , e, h) p(\u03be ) (12)\nwhere p(\u03be ) = T i=t p(\u03be t ) is the prior distribution and q \u03b1 (u|\u03be , e, h) = N (G \u03b1 (\u03be , e, h), \u03c3 2 I ).The marginal distribution of u conditioned on (e, h) is given by q \u03b1 (u|e, h) = q \u03b1 (u, \u03be |e, h)d\u03be .The posterior distribution is q \u03b1 (\u03be |u, e, h) = q \u03b1 (u, \u03be |e, h)/q \u03b1 (u|e, h).Suppose that we observe expert demonstrations D = {(u i , x i , e i , h i ), i = 1, . . ., n}.The MLE of \u03b1 seeks to maximize the log-likelihood function\nL(\u03b1) = n i=1 log q \u03b1 (u i |e i , h i ). (13)\nThe learning gradient can be computed according to\n\u2202 \u2202\u03b1 log q \u03b1 (u|e, h) = 1 q \u03b1 (u|e, h) \u2202 \u2202\u03b1 q \u03b1 (u, \u03be |e, h)d\u03be (14) = [ \u2202 \u2202\u03b1 log q \u03b1 (u, \u03be |e, h)] q \u03b1 (u, \u03be |e, h) q \u03b1 (u|e, h) d\u03be (15) = E q \u03b1 (\u03be|u,e,h) [ \u2202 \u2202\u03b1 log q \u03b1 (u, \u03be |e, h)]. (16)\nThe expectation term in ( 16) is under the posterior distribution q \u03b1 (\u03be |u, e, h) that is analytically intractable.One may draw\n\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\n\nsamples from q \u03b1 (\u03be |u, e, h) via Langevin inference dynamics that iterates\n\u03be s+1 = \u03be s + \u03b4 2 2\n\u2202 \u2202\u03be log q \u03b1 (\u03be s |u, e, h) + \u03b4z s (17) where z \u223c N (0, I ), \u03b4 is the step size, and s indexes the time step.Also, \u03be 0 is usually sampled from Gaussian white noise for initialization.After we infer \u03be from each observation (u i , e i , h i ) by sampling from q \u03b1 (\u03be |u i , e i , h i ) via Langevin inference process, the Monte Carlo approximation of the gradient of L(\u03b1) in ( 13) is computed by\n\u2202 \u2202\u03b1 L(\u03b1) \u2248 n i=1 [ \u2202 \u2202\u03b1 log q \u03b1 (u i , \u03be i |e i , h i )]. (18)\nSince (\u2202/\u2202\u03be ) log q \u03b1 (\u03be |u, e, h) = (\u2202/\u2202\u03be ) log q \u03b1 (u, \u03be |e, h) in (17).Both inference step in (17) and the learning step in (18) need to compute derivative of log q \u03b1 (u, \u03be |e, h) = (1/2\u03c3 2 )G \u03b1 (\u03be , e, h) \u2212 u 2 + const.The former is with respect to \u03be , while the latter is with respect to \u03b1, both of which can be computed by backpropagation through time.The resulting algorithm is called alternating backpropagation through time (ABPTT) algorithm [47].\n\nAlthough the ABPTT algorithm is natural and simple, the difficulty of training the trajectory generator in this way might lie in the nonconvergence issue of the short-run Langevin inference in (17).Even long-run Langevin inference chains are easy to get trapped by local modes.Without fair samples drawn from the posterior distribution, the estimation of \u03b1 will be biased.\n\n\nE. Understanding the Learning Behavior of the Cooperative Training\n\nIn this section, we will present a theoretical understand of the learning behavior of the proposed Algorithm 2 shown.We first start from the CD algorithm that was proposed for efficient training of EBMs.The CD runs k steps of MCMC initialized from the training examples, instead of the Gaussian white noise.Given the EBM for IOC p \u03b8 (u|e, h).Let M \u03b8 be the transition kernel of the finite-step MCMC that samples from p \u03b8 (u|e, h).The original CD learning of p \u03b8 (u|e, h) seeks to minimize\n\u03b8 \u03c4 +1 = arg min \u03b8 [KL( p expert (u|e, h) p \u03b8 (u|e, h)) \u2212 KL(M \u03b8 \u03c4 p expert (u|e, h) p \u03b8 (u|e, h))] (19)\nwhere p expert (u|e, h) is the unknown distribution of the observed demonstrations of experts.Let M \u03b8 p expert (u|e, h) denote the marginal distribution obtained after running M \u03b8 starting from p expert (u|e, h).If M \u03b8 p expert (u|e, h) converges to p \u03b8 (u|e, h), then the second Kullback-Leibler (KL) divergence will become very small, and the CD estimate eventually is close to maximum likelihood estimate, which minimizes the first KL divergence in (19).\n\nIn Algorithm 3, the MCMC sampling of the EBM is initialized from the trajectory generator q \u03b1 (u|e, h), and thus, the learning of the EBM follows a modified CD estimate, which, at learning step \u03c4 , seeks to minimize:\n\u03b8 \u03c4 +1 = arg min \u03b8 [KL( p expert (u|e, h) p \u03b8 (u|e, h)) \u2212 KL(M \u03b8 \u03c4 q \u03b1 (u|e, h) p \u03b8 (u|e, h))] (20)\nwhere we replace p expert (u|e, h) in ( 20) by q \u03b1 (u|e, h).This means that we run a finite-step MCMC from a given initial distribution q \u03b1 (u|e, h) and use the resulting samples as synthesized examples to approximate the gradient of the log likelihood of the EBM.\n\nAt learning step \u03c4 , the learning of q \u03b1 (u|e, h) seeks to minimize\n\u03b1 \u03c4 +1 = arg min \u03b1 [KL(M \u03b8 q \u03b1 \u03c4 (u|e, h)q \u03b1 (u|e, h))]. (21)\nEquation (21) shows that q \u03b1 learns to be the stationary distribution of M \u03b8 .In other words, q \u03b1 seeks to be close to p \u03b1 , i.e., q \u03b1 \u2192 p \u03b8 .If so, the second KL divergence term in (20) will become zero.Equation ( 20) is reduced to minimize the KL divergence between the observed data distribution p expert and the EBM p \u03b8 .Eventually, q \u03b1 chases p \u03b8 toward p expert .\n\n\nIV. EXPERIMENTS\n\nWe evaluate the proposed energy-based continuous IOC methods on autonomous driving tasks.The code, dataset, more results, and experiment details can be found in the project page: http://www.stat.ucla.edu/~yifeixu/ebm-ioc.\n\n\nA. Experimental Setup\n\nIn the task of autonomous driving, the state x t consists of the coordinate, heading angle, and velocity of the car; the control u t consists of steering angle and acceleration; and the environment e consists of road condition, speed limit, the curvature of the lane (which is represented by a cubic polynomial), as well as the coordinates of other vehicles.The trajectories of other vehicles are treated as known environment states and assumed to remain unchanged, while the ego vehicle is moving, even though the trajectories of other vehicles should be predicted in reality.In this article, we sidestep this issue and focus on the IOC problem.\n\nWe assume that the dynamic function of all vehicles is a nonlinear bicycle model [48], which considers longitudinal, lateral, and yaw motions and assumes negligible lateral weight shift, roll, and compliance steer while traveling on a smooth road.We assume that all vehicles are standard two-axle, fourtire passenger cars with a 3-m wheelbase.We set an understeering shift to be 0.043 when calculating heading angles.\n\nAs to learning, the model parameters are randomly initialized by a normal distribution.The control variables are initialized by zeros, which means keeping straight.We normalize the control variables, i.e., the steering and acceleration, because their scales are different.Instead of sampling the control variables, we sample their changes.We set the number of steps of the Langevin dynamics or the GD to be l = 64 and set the step size to be \u03b4 = 0.2.The choice of l is a tradeoff between computational efficiency and prediction accuracy.For parameter training, we use the Adam optimizer [49].\n\nWe use root-mean-square error (RMSE) in meters with respect to each time step t to measure the accuracy of prediction, i.e., RMSE(t) = ((1/n) n i=1 \u0177it \u2212 y it 2 ) 1/2 , where n is the number of expert demonstrations, \u0177it is the predicted coordinate of the i th demonstration at time t, and y it is the ground-truth coordinate of the i th demonstration at time step t.A small RMSE is desired.As a stochastic method, our method draws five samples from the learned model for prediction and the model performance is evaluated by the average RMSE and the minimum RMSE over five sampled trajectories.\n\n\nB. Dataset\n\nWe test our methods on two datasets.The Massachusetts driving dataset focuses on highways with curved lanes and static scenes, while the NGSIM US-101 dataset focuses on rich vehicle interactions.We randomly split each dataset into training and testing sets.The introductions of these two datasets are given as follows.\n\n1) Massachusetts Driving Dataset: This is a private dataset collected through a vehicle during repeated trips on a stretch of highway.The dataset includes vehicle states and controls, which are collected by the hardwares on the vehicles, as well as environment information.This dataset has a realistic driving scenario, including curved lanes and complex static scenes.To solve the problem of noisy GPS signal, Kalman filtering is used to denoise the data.There are 44 000 trajectories, each of which contains 40 0.1-s time steps and is 4 s long.Assuming the bicycle model [48] dynamics, we perform an inverse dynamics optimization using GD to infer controls.In addition to minimizing the reconstruction error on states, we also minimize the L2 norm of the control variables and the difference between every two consecutive controls.The overall RMSE between the reconstructed positions and the ground-truth GPS positions is 0.97 m.The preprocessed trajectories are assumed to have perfect dynamics with noiseless and smooth sequences of controls and GPS coordinates.\n\n\nC. Network Structure\n\nWe first use a linear combination of some hand-designed features as the cost function.The features include the distance from the current vehicle to the goal point (a virtual point set at front of the vehicle) in terms of longitude and latitude, the distance to the center of the lane, the difference between the current speed and the speed limit, the difference between the vehicle direction and the lane direction, the L2 norm of the control values (including acceleration and steering), the difference between the current control value and the control value at the previous time step (including acceleration and steering), and the distance from the vehicle to the nearest obstacle.Feature normalization is adopted to make sure that each feature has the same scale of magnitude.These features are also used to design the cost function networks of our methods, as well as baseline methods for a fair comparison.\n\nTables I and II present the MLP structure and the convolutional neural network (CNN) structure of cost functions, respectively, that we use in Section IV-G.As to the MLP structure, the number of hidden layers N hidden is 64 and the number of layers is 3 (i.e., two hidden layers and one output layer) by default.The MLP cost function in Table I is defined on a single frame and the cost function of the whole trajectory is the summation of costs over all frames.The CNN cost function presented in Table II is defined on a trajectory with 40 time frames.Table III shows the structure of the generator model used in the joint training framework.It is similar to the actor network of the proximal policy optimization (PPO) policy [51] in the generative adversarial imitation learning (GAIL) [27].\n\n\nD. Training Details 1) Normalization:\n\nWe apply normalization to the controls (i.e., acceleration and steering) and hand-designed features.For the controls, we normalize their values to have zero mean Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Fig. 2. Predicted trajectories for single-agent control on the Massachusetts driving dataset.The starting point is the last frame of the history trajectory.Green: predicted trajectories by our model.Blue: predicted trajectories by GAIL.Red: ground-truth trajectories.Orange: trajectories of other vehicles.Gray: lanes.and unit variance.We also normalize each hand-designed feature by dividing by the mean.We normalize two datasets separately.\n\n2) Optimizer: We use the Adam optimizer on training our models.Both \u03b2 1 and \u03b2 2 are set to be 0.5.All model parameters are randomly initialized by the He initialization method [52], which is a uniform distribution.In the linear setting, we set the learning rate of Adam to be 0.1 with an exponential decay rate of 0.999.For the MLP cost function setting, we set the learning rate to be 0.005 without an exponential decay.In the CNN cost function setting, we set the learning rate to be 0.005 with an exponential decay rate 0.999.For the training of the generator model, the learning rate is 0.002 and the exponential decay rate is 0.998.For each epoch, we shuffle the whole dataset.The batch size is 1024.\n\n3) Langevin Dynamics: To prevent gradient values from being too large in each Langevin step, we set the maximum limit to be 0.1.The Langevin step size is set to be 0.1 and the number of Langevin steps is 64.All settings are the same for the GD method to synthesize the controls.\n\n4) iLQR: As to the iLQR solver, we perform a grid search for the learning rate from 0.001 to 1.The maximum step is 100.If the difference between the current step and the previous step is smaller than 0.001, early stop is triggered.In the experiment, the average number of iLQR steps is around 30.\n\n\nE. Single-Agent Control\n\nWe first test our methods, including sampling-based and optimization-based ones, on a single-agent control problem.We compare our method with three baseline methods as follows.\n\n1) Constant Velocity: The simplest baseline with a constant velocity and a zero steering.2) GAIL [27]: The original GAIL method was proposed for imitation learning.We use the same setting as in [53], which applies the GAIL to the task of modeling human highway driving behavior.Besides, we change the policy gradient method from trust region policy optimization (TRPO) [54] to PPO [51].3) IOC With Laplace [30] (IOC-Laplace): We implement this baseline with the same iLQR method as that in our model.It takes roughly 0.1 s to predict a full trajectory with a 64-step Langevin dynamics (or GD).Fig. 2 shows two qualitative results.Each row shows one four-frame example with a frame interval equal to 1 s.Each frame shows trajectories over time for different vehicles as well as different baseline methods for comparison.Tables IV and V show quantitative results for Massachusetts driving dataset and NGSIM, respectively.In the last two rows, we provide both average RMSE and minimum RMSE for our sampling-based approach.Our methods achieve substantial improvements compared to baseline methods, such as IOC-Laplace [30] and GAIL, in terms of testing RMSE.We find that the sampling-based methods outperform the optimization-based methods among our energybased approaches.\n\nThe reason why the method \"IOC-Laplace\" performs poorly on both two datasets is due to the fact that its Laplace approximation is not accurate enough for a complex cost function used in the current tasks.Our models are more genetic and do not make such an approximation.Instead, they use Langevin sampling for maximum likelihood training.Therefore, they can provide more accurate prediction results.The problem of GAIL is its model complexity.GAIL parameterizes its discriminator, policy, and value function by MLPs.Designing optimal MLP structures of these three components for GAIL is challenging.Our method only needs to design a single architecture for the cost function.\n\nIn addition, the optimal control of our method is performed by simulating trajectories of actions and states according to the learned cost function that considers the future information.In contrast, the GAIL relies on its learned policy net for stepwise decision-making.\n\nCompared with GD (optimization-based approach), Langevin dynamics-based method can obtain smaller errors.One reason is that the sampling-based approach rigorously maximizes the log likelihood of the expert demonstrations during training, while the optimization-based approach is just a convenient approximation.The other reason is that the Gaussian noise term in each Langevin step helps to explore the cost function and avoid suboptimal solutions.\n\n\nF. Corner Case Testing With Toy Examples\n\nCorner cases are important for model evaluation.We construct six typical corner cases to test our model.Fig. 3 shows the predicted trajectories by our method for several cases.Fig. 3(a) and (b) shows two cases of the sudden braking.In each of the cases, a vehicle (orange) in front of the ego vehicle (green) is making a sudden brake.In Fig. 3(a), there are not any other vehicles moving alongside the ego vehicle, so it is predicted to first change the lane, then accelerate past the vehicle in front, and return to its previous lane and continue its driving.In Fig. 3(b), two vehicles are moving alongside the ego vehicle.The predicted trajectory shows that the ego vehicle is going to trigger a brake to avoid a potential collision accident.Fig. 3(c) and (d) shows two cases in the cutin situation.In each case, a vehicle is trying to cut in from the left or right lane.The ego vehicle is predicted to slow down to ensure the safe cut-in of the other vehicle.Fig. 3(e) and (f) shows two cases in the large lane curvature situation, where our model can still perform well to predict reasonable trajectories.\n\nFig. 4 shows the corresponding plots of the predicted controls, i.e., steering and acceleration, over time steps.In each plot, blue lines stand for acceleration and orange lines stand for steering.The dashed lines represent the initialization of the controls for Langevin sampling, which are actually the controls at the last time steps of the history trajectories.We use 64 Langevin steps to sample the controls from the learned cost function.We plot the predicted controls (i.e., acceleration and steering) over time for each Langevin step.The curves with more numbers of Langevin steps appear darker.Thus, the darkest solid lines are the final predicted trajectories of controls.\n\nIn short, this experiment demonstrates that our method is capable of learning a reasonable cost function that handles corner cases, such as situations of sudden braking, lane cutin, and making turns in curved lanes.\n\n\nG. Evaluation of Different Cost Functions\n\nThe neural network is a powerful function approximator.It is capable of approximating any complex nonlinear function given sufficient training data, and it is also flexible to incorporate prior information, which in our case are the manually designed features.In this experiment, we replace the linear cost function in our sampling-based approach with a neural network cost function.In particular, we design a cost function by MLP, where we put three layers on top of the vector of hand-designed features: C \u03b8 (x, u, e, h) = f (\u03c6(x, u, e, h)), where f contains two hidden layers and one output layer and \u03b8 contains all trainable parameters in f .We also consider using a 1-D CNN that considers the temporal relationship inside the trajectory for the cost function.We add four 1-D convolutional layers on top of the sequence of vectors of hand-designed features, where the kernel size in each layer is 1 \u00d7 4. The numbers of channels are {32, 64, 128, 256} and the numbers of strides are {2, 2, 2, 1} for different layers.One fully connected layer with a single kernel is attached at the end.\n\nTable VI shows a comparison of performances of different designs of cost functions.We can see that improvements can be obtained by using cost functions parameterized by either MLP or CNN.Neural network provides nonlinear connection layers as a transformation of the original input features.This implies that there are some internal connections between the Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.features and some temporal connections among feature vectors at different time steps.\n\n\nH. Multiagent Control\n\nIn the setting of single-agent control, the future trajectories of other vehicles are assumed to be given and they remain unchanged no matter how the ego vehicle moves.We extend our energy-based framework to the multiagent setting, in which we simultaneously control all vehicles in the scene.The controls of other vehicles are used to predict the trajectories of other vehicles.\n\nSuppose that there are K agents, and every agent in the scene can be regarded as a general agent.The state and control space are Cartesian products of the individual states and controls, respectively, i.e., X = (x k , k = 1, 2, . . ., K ), U = (u k , k = 1, 2, . . ., K ).All the agents share the same dynamic function, which is\nx k t = f (x k t\u22121 , u k t ), \u2200k = 1, 2, . . . , K .\nThe overall cost function is set to be the sum of each agent C \u03b8 (X, U, e, h) = K k=1 C \u03b8 (x k , u k , e, h k ).Thus, the conditional probability density function becomes p \u03b8 (U|e, h) = (1/(Z \u03b8 (e, h))) exp[\u2212C \u03b8 (X, U, e, h)], where Z \u03b8 (e, h) is the intractable normalizing constant.\n\nWe compare our method with the following baselines for multiagent control.\n\n1) Constant Velocity: The simplest baseline with a constant velocity and a zero steering.\n\n2) The Parameter Sharing GAIL (PS-GAIL) [55], [56]:\n\nIt extends the single-agent GAIL and the parameter sharing trust region policy optimization (PS-TRPO) [57] to enable imitation learning in the multiagent control context.We test our method on the NGSIM dataset.We use a linear cost function setting for each agent in this experiment.The maximum number of agents is 64.Fig. 5 shows two examples of the qualitative results.Each row is one example.The rows from first to fifth show the positions of all vehicles in the scene as dots at different time steps along with the predicted trajectories (the green lines) and the ground truths (the red lines).Table VII shows a comparison of performances between our method and the baselines in terms of RMSE.Results show that our method can also work very well in the multiagent control scenario.\n\n\nI. Joint Training With Trajectory Generator\n\nIn this section, we follow Algorithm 3 to introduce a trajectory generator as a fast initializer for our Langevin sampler.In the experiment, we design F \u03b1 as a four-layer MLP with output dimensions 64, 16, 8, and 2 at different layers.The activation function is ReLU for each hidden layer and Tanh for the final layer.The learning rate of the trajectory generator is set to be 0.005.We update the generator five times for each cooperative learning iteration.The rest of the setting remains the same as in the model with a linear cost function.\n\nTable VIII compares the proposed joint training method with the following baselines in terms of average RMSE.The methods include: 1) \"EBM w/o a generator\": the single EBM method without using a trajectory generator and 2) \"generator in joint training\": the trajectory generator trained with an EBM via the proposed cooperative training algorithm.We train both baseline methods (1) and (2) as well as our joint training framework (which we refer to as \"EBM with a generator\" in Table VIII with different numbers of Langevin steps.Besides, we implement method (3), which is a single trajectory generator trained via MLE with MCMC-based inference [47].This comparison results show that a fast initializer can improve the performance even with less Langevin steps.For example, an EBM using eight Langevin steps with a fast initializer is comparable with the one with a 32-step Langevin dynamics.Also, the method of \"generator in joint training\" performs better than the \"generator only\" setting because of the guidance of Langevin sampling of the EBM.\n\n\nJ. Training Time and Model Size\n\nWe make a comparison of different methods in terms of computational cost and model size in the task of single-agent control on the Massachusetts driving dataset.We use a mini-batch of size 1024 during training.For GAIL, the mini-batch size is 64.The total number of epochs is 40.\n\n\nK. Hyperparameter Analysis for Energy-Based IOC Models 1) Influence of the Number of Langevin Steps and the Langevin\n\nStep Size in the Single EBM Framework: We first study the influence of different choices of some hyperparameters, such as the number of Langevin steps l and the step size \u03b4 of each Langevin step.Fig. 6 shows the performances of energy-based IOC models with different \u03b4 and l on the Massachusetts driving dataset.Each curve is associated with a certain step size \u03b4 and shows the testing performances over different numbers of Langevin steps.The performances are measured by: 1) average RMSE; 2) minimum RMSE; and 3) missing rate.In our experiments, we draw five samples from the learned model for prediction.The missing rate is the ratio of scenarios where none of all five sampled trajectories has an endpoint L2 error less than 1.0 m.The three metrics are used in Fig. 6(a)-(c).In general, with the same \u03b4, the model performance increases as the number of Langevin steps increases.However, the performance gains become smaller Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.and smaller while using more Langevin steps.Using more Langevin steps will also increase the computational time of sampling.We use l = 64 to make a tradeoff between performance and computational efficiency.We also choose \u03b4 = 0.1 for a tradeoff among performances measured by different metrics.For testing, we use the same l and \u03b4 as those in training.We observe that the learning is quite stable in the sense that the testing errors drop smoothly with an increasing number of training epochs.\n\nWe also study, given a trained model, how different choices of l and \u03b4 in testing can affect the performance of the model.Fig. 7(c) shows the average RMSEs of trajectories that are sampled from a learned model by using different numbers of Langevin steps l and step sizes \u03b4.The model we use is with a linear cost function and trained with l = 64 and \u03b4 = 0.1.We observe that, in the testing stage, using Langevin step sizes smaller than that in the training stage may take more Langevin steps to converge, while using larger ones may lead to a nonconvergence issue.Thus, we suggest using the same l and \u03b4 in both training and testing stages for optimal performance.\n\n\n2) Influence of the Number of Langevin Steps and the Langevin Step Size in the Cooperative Training Framework:\n\nWe hence study the influence of different choices of the number of Langevin steps l and the Langevin step size \u03b4 in the cooperative training framework.Fig. 8 shows the performances of cooperative training frameworks with different values of \u03b4 and l on the Massachusetts driving dataset.The performances shown in Fig. 8(a)-(c) are measured by average RMSE, minimum RMSE, and missing rate, respectively.Each curve corresponds to a framework with a certain step size \u03b4 and shows the testing performances over different numbers of Langevin steps.Fixing \u03b4, the model performance increases as the number of Langevin steps increases.We use l = 32 and \u03b4 = 0.1 to make a tradeoff between performance and computational efficiency.optimal control.We study this problem in the framework of the EBM and propose a sampling-based method and optimizationbased modification to learn the cost function.Unlike the previous method for continuous IOC [30], we learn the model by maximum likelihood using Langevin sampling, without resorting to Laplace approximation.This is a possible reason for improvement over the previous method.Langevin sampling in general also has the potential to avoid suboptimal modes.Moreover, we propose to train the EBM with a trajectory generator as a fast initializer to improve the learning efficiency.The experiments show that our method is generally applicable and can learn nonlinear and non-Markovian cost functions.\n\nIn our future work, we shall explore other MCMC sampling or optimal control algorithms.We shall also experiment with recruiting a flow-based model [41], [58], [59] as a learned approximate sampler to amortize the MCMC sampling.We shall also adapt our model to the scenario where the human drivers may be suboptimal, and some human judges may assign scores to the trajectories of some of the drivers.\n\nAlgorithm 3 repeat 5 :\n35\nEnergy-Based IOC With a Trajectory Generator 1: input expert demonstrations D = {(x i , u i , e i , h i ), \u2200i }. 2: output cost function parameters \u03b8 , trajectory generator parameters \u03b1, and synthesized or optimized trajectories {(x i , \u0169i ), \u2200i }. 3: Let \u03c4 \u2190 0, randomly initialize \u03b8 and \u03b1. 4:Initialization step: Initialize \u00fbi = G \u03b1 t (\u03be i , e i , h i ), where \u03be i \u223c p(\u03be ) by ancestral sampling, and then obtain xi for each i .\n\n\n6 :\n6\nSynthesis step or optimization step: Given the initial \u00fbi , synthesizing \u0169i \u223c p \u03b8 t (u|e i , h i ) by Langevin sampling, or optimizing \u0169i = arg min u C \u03b8 (x, u, e i , h i ) by gradient descent (GD) or iLQR, and then obtain xi , for each i .\n\n\n7 :\n7\nAnalysis step (update cost function):Update \u03b8 \u03c4 +1 = \u03b8 \u03c4 + \u03b3 \u03c4 L (\u03b8 \u03c4 ), where L (\u03b8 ) is computed according to Equation (4).\n\n\n8 :\n8\nAnalysis step (update policy model):Update \u03b1 \u03c4 +1 = \u03b1 \u03c4 \u2212 \u03b7 \u03c4 l g (\u03b1 \u03c4 ), where l g (\u03b1) is computed according to Equation(11).\n\n\n9 :\u03c4\n9\n\u2190 \u03c4 + 1. 10: until \u03c4 = \u03c4 max , the number of iterations.as illustrated by the following diagram: Trajectory evaluator (b) Trajectory generator (energy-based model) (latent variable model)\n\n\nFig. 1 .\n1\nFig. 1.Trajectory generation by (a) iterative method (Langevin sampling) and (b) noniterative method (ancestral sampling).\n\n\n2 )\n2\nNGSIM US-101: NGSIM[50] contains real highway traffic captured at 10 Hz over a time span of 45 min.Compared to the Massachusetts driving dataset, NGSIM has rich vehicle interactions.The control needs to consider other nearby vehicles.We preprocess the data by dividing the data into 5-s/50-time-step trajectories.The first ten time steps are for history and the remaining 40 time steps are used for prediction.There are 831 scenes with 96 512 5-s vehicle trajectories.No control variables are provided.Thus, we need to infer the controls of each vehicle given the vehicle states.\n\n\nFig. 3 .\n3\nFig. 3. Prediction in corner cases.Green: predicted trajectories.Orange: trajectories of other vehicles.Gray: lanes.\n\n\nFig. 4 .\n4\nFig. 4. Predicted controls over time.Dashed lines: initial values of the controls for Langevin sampling.Solid lines: predicted controls over time steps.Blue: control of acceleration.Orange: control of steering.\n\n\nFig. 5 .\n5\nFig. 5. Predicted trajectories for multiagent control on the NGSIM dataset.The starting point is the last frame of the history trajectory.Green: predicted trajectories by our model.Red: ground-truth trajectories.Gray: lanes.\n\n\nFig. 6 .\n6\nFig. 6.Influence of hyperparameters.Performance comparison of EBMs with different numbers of Langevin steps and Langevin step sizes is shown in each subfigure.Each curve represents a model with a certain Langevin step size \u03b4.We set \u03b4 = 0.02, 0.05, 0.1, and 0.2.For each setting of \u03b4, we choose different numbers of steps l = 2, 8, 16, 32, and 64.Performances are measured by (a) average RMSE, (b) minimum RMSE, and (c) missing rate on the Massachusetts driving dataset.\n\n\nFig. 7 .\n7\nFig. 7. Influence of hyperparameters.(a) Line chart of testing average RMSEs over training epochs for different numbers of Langevin steps l used in training.(b) Line chart of testing average RMSEs over training epochs for different Langevin step sizes \u03b4 used in training.(c) Influence of different numbers of Langevin steps l and step sizes \u03b4 used in testing.\n\n\nFig. 8 .\n8\nFig. 8. Influence of hyperparameters for the cooperative training framework.Performance comparison of frameworks using different numbers of Langevin steps and different Langevin step sizes is given in each subfigure.Each curve represents a framework with a certain Langevin step size \u03b4.We set \u03b4 = 0.05, 0.1, and 0.2.For each setting of \u03b4, different numbers of Langevin steps are chosen, l = 2, 8, 16, 32, and 64.Performances are measured by three different metrics, which are (a) average RMSE, (b) minimum RMSE, and (c) missing rate.\n\n\nFig. 9 .\n9\nFig. 9. Influence of hyperparameters for the cooperative training framework.(a) Testing performances during the cooperative training with different Langevin steps l.(b) Testing performances during the cooperative training with different Langevin step sizes \u03b4.(c) Testing performances after training.Each curve shows testing performances over different numbers of Langevin steps used to sample trajectories in testing.Each curve is associated with a step size chosen in testing.The model is trained with the Langevin step size \u03b4 = 0.2 and the number of steps l = 32.\n\n\nFig. 7 (\n7\na) and (b) shows the training curves of the models with different values of l and \u03b4, respectively.The models are trained on the Massachusetts driving dataset.Each curve reports the testing average RMSEs over training epochs.\n\n\nFig. 9 (\n9\na) and (b) shows the learning curves of the cooperative training with different numbers of Langevin steps and different step sizes, respectively.Each learning curve shows testing average RMSE over different training epochs.We observe that the testing average RMSE decreases smoothly as the number of training epochs increases.We also study how different values of l and \u03b4 chosen in testing affect the performance of a learned energy-based IOC model.We first train an EBM with \u03b4 = 0.2 and l = 32 and use the learned model in testing with varying Langevin step size \u03b4 and a number of Langevin steps l.Fig.9(c) shows the influences of varying \u03b4 and l in testing.We observe that given a learned cost function, Langevin sampling with a smaller step size and a larger number of Langevin steps may allow the model to generate better trajectories.V. CONCLUSIONThis article studies the fundamental problem of learning the cost function from expert demonstrations for continuous Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\n\n\n\n\nfor each i , synthesize \u0169i \u223c p \u03b8 t (u|e i , h i ) by Langevin sampling and then obtain xi .: until \u03c4 = \u03c4 max , the number of iterations.for each i , optimize \u0169i = arg min u C \u03b8 (x, u, e i , h i ), by gradient descent (GD) or iLQR, and then obtain xi .\n5: synthesis step: 6: analysis step: update \u03b8 \u03c4 +1 = \u03b8 \u03c4 + \u03b3 \u03c4L (\u03b8 \u03c4 ), where Lis computed according to Equation (4).7:\u03c4 \u2190 \u03c4 + 1.Algorithm 2 Energy-Based IOC With Optimization Step\nAlgorithm 1 Energy-Based IOC With Synthesis Step 1: input expert demonstrations D = {(x i , u i , e i , h i ), \u2200i }. 2: output cost function parameters \u03b8 , and synthesized trajectories {(x i , \u0169i ), \u2200i }. 3: Let \u03c4 \u2190 0, randomly initialize \u03b8 .4: repeat 81: input expert demonstrations D = {(x i , u i , e i , h i ), \u2200i }. 2: output cost function parameters \u03b8 , and optimized trajectories {(x i , \u0169i ), \u2200i }. 3: Let \u03c4 \u2190 0, randomly initialize \u03b8 .4: repeat 5: optimization step: 6:\n\n\nTABLE V NGSIM\nV\nDATASET RESULTS (RMSE)\n\n\nTABLE VI COMPARISON\nVI\nOF PERFORMANCES OF MODELS WITH DIFFERENT COST FUNCTIONS (AVERAGE RMSE)\n\n\nTABLE VII PERFORMANCE\nVII\nCOMPARISON IN MULTIAGENT CONTROL ON THE NGSIM DATASET.AVERAGE RMSES ARE REPORTED\n\n\nTABLE VIII RESULTS\nVIII\nOF THE JOINT TRAINING WITH TRAJECTORY GENERATOR ON THE MASSACHUSETTS DRIVING DATASET (AVERAGE RMSE)\n\n\n\n\nTable IX lists the time consumption per training epoch and the number of model parameters for different settings on the Massachusetts driving dataset.The training time is recorded in a PC with a CPU i9-9900 and a GPU Tesla P100.As to the energy-based framework, a simple cost function design can lead to less computation time and less parameters.However, complex cost function can result in better performance in terms\n\n\nTABLE IX COMPARISON\nIX\nOFCOMPUTATION COST AND MODEL SIZE of RMSE.Overall, compared with the GAIL and IOC-Laplace baselines, our energy-based IOC methods are competitive.\n\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nThis work was supported in part by NSF under Grant DMS-2015577, in part by the Defense Advanced Research Projects Agency's Simplifying Complexity in Scientific Discovery (DARPA's SIMPLEX) under Grant N66001-15-C-4035, in part by the Office of Naval Research Multidisciplinary University Initiative (ONR MURI) under Grant N00014-16-1-2007, in part by DARPA Army Research Office (ARO) under Grant W911NF-16-1-0579, DARPA under Grant N66001-17-2-4029, and in part by the Extreme Science and Engineering Discovery Environment (XSEDE) under Grant CIS210052.\nOptimal control theory. E Todorov, Bayesian Brain: Probabilistic Approaches to Neural Coding. 2006\n\nIterative linear quadratic regulator design for nonlinear biological movement systems. W Li, E Todorov, Proc. 1st Int. Conf. Informat. Control. 1st Int. Conf. Informat. Control2004\n\nThe explicit linear quadratic regulator for constrained systems. A Bemporad, M Morari, V Dua, E N Pistikopoulos, Automatica. 381Jan. 2002\n\nA theory of generative convnet. J Xie, Y Lu, S.-C Zhu, Y Wu, Proc. Int. Conf. Mach. Learn. (ICML). Int. Conf. Mach. Learn. (ICML)2016\n\nMCMC using Hamiltonian dynamics. R M Neal, 10.1201/b10905/handbook-markov-chain-monte-carlo-steve-brooks-andrew-gelman-galin-jones-xiao-li-mengHandbook of Markov Chain Monte Carlo. 2112011\n\nMathematical Statistics: Basic Ideas and Selected Topics, Package, vols. 1-2. P J Bickel, K A Doksum, 2015CRC PressBoca Raton, FL, USA\n\nTraining products of experts by minimizing contrastive divergence. G E Hinton, Neural Comput. 1482002\n\nEstimation of non-normalized statistical models by score matching. A Hyv\u00e4rinen, J. Mach. Learn. Res. 62005\n\nCooperative learning of energy-based model and latent variable model via MCMC teaching. J Xie, Y Lu, R Gao, S Zhu, Y Wu, Proc. AAAI Conf. AAAI Conf201832\n\nCooperative training of descriptor and generator networks. J Xie, Y Lu, R Gao, S.-C Zhu, Y N Wu, IEEE Trans. Pattern Anal. Mach. Intell. 421Jan. 2020\n\nCooperative training of fast thinking initializer and slow thinking solver for conditional learning. J Xie, Z Zheng, X Fang, S.-C Zhu, Y N Wu, 10.1109/TPAMI.2021.3069023IEEE Trans. Pattern Anal. Mach. Intell., early access. Mar. 26, 2021\n\nMaximum entropy inverse reinforcement learning. B D Ziebart, A L Maas, J A Bagnell, A K Dey, Proc. 23rd AAAI Conf. Artif. Intell. (AAAI). 23rd AAAI Conf. Artif. Intell. (AAAI)Chicago, IL, USA20088\n\nTowards a unified theory for texture modeling. S C Zhu, Y Wu, D Mumford, Filters, random fields and maximum entropy. 199827\n\nMarkov logic networks. M Richardson, P Domingos, Mach. Learn. 621Feb. 2006\n\nMaximum entropy deep inverse reinforcement learning. M Wulfmeier, P Ondruska, I Posner, arXiv:1507.048882015\n\nLearning energy-based spatialtemporal generative ConvNets for dynamic patterns. J Xie, S.-C Zhu, Y N Wu, IEEE Trans. Pattern Anal. Mach. Intell. 432Feb. 2021\n\nLearning sparse FRAME models for natural image patterns. J Xie, W Hu, S.-C Zhu, Y N Wu, Int. J. Comput. Vis. 1142-3Sep. 2015\n\nSynthesizing dynamic patterns by spatial-temporal generative ConvNet. J Xie, S.-C Zhu, Y N Wu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Jul. 2017\n\nGenerative VoxelNet: Learning energy-based models for 3D shape synthesis and analysis. J Xie, Z Zheng, R Gao, W Wang, S.-C Zhu, Y N Wu, IEEE Trans. Pattern Anal. Mach. Intell. 445May 2022\n\nLearning descriptor networks for 3D shape synthesis and analysis. J Xie, Z Zheng, R Gao, W Wang, S.-C Zhu, Y N Wu, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJun. 2018\n\nGenerative PointNet: Deep energy-based learning on unordered point sets for 3D generation, reconstruction and classification. J Xie, Y Xu, Z Zheng, S.-C Zhu, Y N Wu, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Jun. 2021\n\nConvolutional networks for images, speech, and time series. Y Lecun, The Handbook of Brain Theory and Neural Networks. 199533611995\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. null2012\n\nGuided cost learning: Deep inverse optimal control via policy optimization. C Finn, S Levine, P Abbeel, Int. Conf. Mach. Learn. (ICML). 2016\n\nA connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. C Finn, P Christiano, P Abbeel, S Levine, arXiv:1611.038522016\n\nGenerative adversarial nets. I Goodfellow, Proc. null2014\n\nGenerative adversarial imitation learning. J Ho, S Ermon, Proc. null2016\n\nInfoGAIL: Interpretable imitation learning from visual demonstrations. Y Li, J Song, S Ermon, Adv. Neural Inf. Process. Syst. 2017\n\nIntent prediction and trajectory forecasting via predictive inverse linear-quadratic regulation. M Monfort, A Liu, B D Ziebart, Proc. 29th AAAI Conf. 29th AAAI Conf2015\n\nContinuous inverse optimal control with locally optimal examples. S Levine, V Koltun, Proc. Int. Conf. Mach. Learn. (ICML). Int. Conf. Mach. Learn. (ICML)2012\n\nSocial LSTM: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Jun. 2016\n\nSocial GAN: Socially acceptable trajectories with generative adversarial networks. A Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJun. 2018\n\nSocial attention: Modeling attention in human crowds. A Vemula, K Muelling, J Oh, Proc. IEEE Int. Conf. Robot. Autom. (ICRA). IEEE Int. Conf. Robot. Autom. (ICRA)May 2018\n\nDESIRE: Distant future prediction in dynamic scenes with interacting agents. N Lee, W Choi, P Vernaza, C B Choy, P H S Torr, M Chandraker, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Jul. 2017\n\nHow would surround vehicles move? A unified framework for maneuver classification and motion prediction. N Deo, A Rangesh, M M Trivedi, IEEE Trans. Intell. Vehicles. 32Jun. 2018\n\nMulti-agent tensor fusion for contextual trajectory prediction. T Zhao, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Jun. 2019\n\nLearning cycleconsistent cooperative networks via alternating MCMC teaching for unsupervised cross-domain translation. J Xie, Z Zheng, X Fang, S Zhu, Y N Wu, Proc. 35th AAAI Conf. 35th AAAI Conf2021\n\nLearning energybased model with variational auto-encoder as amortized sampler. J Xie, Z Zheng, P Li, Proc. 35th AAAI Conf. 35th AAAI Conf2021\n\nAuto-encoding variational Bayes. D P Kingma, M Welling, Proc. Int. Conf. Learn. Represent. (ICLR). Int. Conf. Learn. Represent. (ICLR)2014\n\nA tale of two flows: Cooperative learning of Langevin flow and normalizing flow toward energy-based model. J Xie, Y Zhu, J Li, P Li, Proc. Int. Conf. Learn. Represent. (ICLR). Int. Conf. Learn. Represent. (ICLR)2022\n\nGlow: Generative flow with invertible 1\u00d71 convolutions. D P Kingma, P , Proc. null201831\n\nA stochastic approximation method. H Robbins, S Monro, Ann. Math. Statist. 2231951\n\nElements of Information Theory. T M Cover, J A Thomas, 2006WileyHoboken, NJ, USA2nd ed\n\nMonte Carlo sampling methods using Markov chains and their applications. W K Hastings, 1970Tech. Rep.\n\nStochastic gradient Hamiltonian Monte Carlo. T Chen, E B Fox, C Guestrin, Proc. Int. Conf. Mach. Learn. (ICML). Int. Conf. Mach. Learn. (ICML)201432\n\nOn the anatomy of MCMC-based maximum likelihood learning of energy-based models. E Nijkamp, M Hill, T Han, S Zhu, Y N Wu, Proc. AAAI Conf. Artif. Intell. (AAAI). AAAI Conf. Artif. Intell. (AAAI)202034\n\nLearning dynamic generator model by alternating back-propagation through time. J Xie, R Gao, Z Zheng, S.-C Zhu, Y N Wu, Proc. AAAI Conf. Artif. Intell. (AAAI). AAAI Conf. Artif. Intell. (AAAI)201933\n\nThe kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles. P Polack, F Altche, B D'andrea-Novel, A De, La Fortelle, Proc. IEEE Intell. Vehicles Symp. (IV). IEEE Intell. Vehicles Symp. (IV)Jun. 2017\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.69802014\n\nU.s. highway 101 dataset. J Colyar, J Halkias, FHWA-HRT-07- 030Federal Highway Admin. (FHWA). Washington, DC, USA2007Tech. Rep.\n\nProximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017\n\nDelving deep into rectifiers: Surpassing human-level performance on ImageNet classification. K He, X Zhang, S Ren, J Sun, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)Dec. 2015\n\nImitating driver behavior with generative adversarial networks. A Kuefler, J Morton, T Wheeler, M Kochenderfer, Proc. IEEE Intell. Vehicles Symp. (IV). IEEE Intell. Vehicles Symp. (IV)Jun. 2017\n\nTrust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, Proc. Int. Conf. Mach. Learn. (ICML). Int. Conf. Mach. Learn. (ICML)2015\n\nMulti-agent imitation learning for driving simulation. R P Bhattacharyya, D J Phillips, B Wulfe, J Morton, A Kuefler, M J Kochenderfer, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS). IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)Oct. 2018\n\nSimulating emergent properties of human driving behavior using multi-agent reward augmented imitation learning. R P Bhattacharyya, D J Phillips, C Liu, J K Gupta, K Driggs-Campbell, M J Kochenderfer, Proc. Int. Conf. Robot. Autom. (ICRA). Int. Conf. Robot. Autom. (ICRA)May 2019\n\nCooperative multi-agent control using deep reinforcement learning. J K Gupta, M Egorov, M Kochenderfer, Proc. Int. Conf. Auton. Agents Multiagent Syst. Int. Conf. Auton. Agents Multiagent Syst2017\n\nNICE: Non-linear independent components estimation. L Dinh, D Krueger, Y Bengio, Proc. Int. Conf. Learn. Representations (ICLR) Workshop. Int. Conf. Learn. Representations (ICLR) Workshop2015\n\nDensity estimation using real NVP. L Dinh, J Sohl-Dickstein, S Bengio, Proc. Int. Conf. Learn. Represent. (ICLR). Int. Conf. Learn. Represent. (ICLR)2017\n", "annotations": {"author": "[{\"end\":191,\"start\":4},{\"end\":311,\"start\":192},{\"end\":414,\"start\":312},{\"end\":510,\"start\":415},{\"end\":582,\"start\":511},{\"end\":635,\"start\":583}]", "publisher": null, "author_last_name": "[{\"end\":12,\"start\":10},{\"end\":205,\"start\":201},{\"end\":324,\"start\":322},{\"end\":426,\"start\":423},{\"end\":522,\"start\":517},{\"end\":594,\"start\":590}]", "author_first_name": "[{\"end\":9,\"start\":4},{\"end\":200,\"start\":192},{\"end\":316,\"start\":312},{\"end\":321,\"start\":317},{\"end\":422,\"start\":415},{\"end\":516,\"start\":511},{\"end\":589,\"start\":583}]", "author_affiliation": "[{\"end\":101,\"start\":14},{\"end\":190,\"start\":103},{\"end\":310,\"start\":223},{\"end\":413,\"start\":326},{\"end\":509,\"start\":445},{\"end\":581,\"start\":543},{\"end\":634,\"start\":596}]", "title": null, "venue": null, "abstract": null, "bib_ref": "[{\"end\":824,\"start\":819},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3360,\"start\":3357},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3501,\"start\":3498},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3506,\"start\":3503},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4140,\"start\":4137},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5343,\"start\":5340},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5978,\"start\":5975},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6056,\"start\":6053},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6099,\"start\":6096},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7710,\"start\":7707},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7716,\"start\":7712},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8199,\"start\":8195},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9210,\"start\":9206},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9327,\"start\":9323},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9357,\"start\":9353},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9480,\"start\":9476},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10034,\"start\":10030},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10040,\"start\":10036},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10159,\"start\":10155},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10165,\"start\":10161},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10514,\"start\":10510},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10520,\"start\":10516},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10565,\"start\":10561},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10571,\"start\":10567},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10984,\"start\":10980},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10993,\"start\":10989},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11001,\"start\":10997},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11149,\"start\":11145},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11579,\"start\":11575},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11585,\"start\":11581},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11898,\"start\":11894},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12165,\"start\":12161},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12188,\"start\":12184},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12215,\"start\":12211},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12253,\"start\":12249},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12302,\"start\":12298},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12321,\"start\":12317},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12437,\"start\":12433},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12457,\"start\":12453},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12557,\"start\":12553},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12932,\"start\":12928},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12938,\"start\":12934},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12948,\"start\":12944},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17036,\"start\":17032},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17267,\"start\":17264},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18510,\"start\":18506},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18637,\"start\":18633},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18718,\"start\":18715},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18756,\"start\":18753},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18762,\"start\":18758},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18772,\"start\":18768},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19405,\"start\":19402},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20629,\"start\":20626},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24598,\"start\":24594},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24604,\"start\":24600},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24614,\"start\":24610},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27685,\"start\":27682},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30496,\"start\":30492},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30988,\"start\":30984},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31016,\"start\":31012},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31046,\"start\":31042},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31370,\"start\":31366},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31570,\"start\":31566},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32866,\"start\":32862},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33595,\"start\":33591},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33768,\"start\":33764},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":34951,\"start\":34947},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35876,\"start\":35872},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37385,\"start\":37381},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39543,\"start\":39539},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39604,\"start\":39600},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":40543,\"start\":40539},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":41953,\"start\":41949},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":42050,\"start\":42046},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42225,\"start\":42221},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":42237,\"start\":42233},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":42262,\"start\":42258},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":42970,\"start\":42966},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":49550,\"start\":49546},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":49556,\"start\":49552},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":49665,\"start\":49661},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":51584,\"start\":51580},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":55664,\"start\":55660},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":56314,\"start\":56310},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":56320,\"start\":56316},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":56326,\"start\":56322},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":57535,\"start\":57531},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":57901,\"start\":57897}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57020,\"start\":56563},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57269,\"start\":57021},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57402,\"start\":57270},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57537,\"start\":57403},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57734,\"start\":57538},{\"attributes\":{\"id\":\"fig_5\"},\"end\":57870,\"start\":57735},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58458,\"start\":57871},{\"attributes\":{\"id\":\"fig_7\"},\"end\":58588,\"start\":58459},{\"attributes\":{\"id\":\"fig_8\"},\"end\":58812,\"start\":58589},{\"attributes\":{\"id\":\"fig_9\"},\"end\":59050,\"start\":58813},{\"attributes\":{\"id\":\"fig_10\"},\"end\":59533,\"start\":59051},{\"attributes\":{\"id\":\"fig_11\"},\"end\":59906,\"start\":59534},{\"attributes\":{\"id\":\"fig_12\"},\"end\":60453,\"start\":59907},{\"attributes\":{\"id\":\"fig_13\"},\"end\":61032,\"start\":60454},{\"attributes\":{\"id\":\"fig_14\"},\"end\":61270,\"start\":61033},{\"attributes\":{\"id\":\"fig_15\"},\"end\":62363,\"start\":61271},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":63279,\"start\":62364},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":63320,\"start\":63280},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63416,\"start\":63321},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63525,\"start\":63417},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":63651,\"start\":63526},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":64074,\"start\":63652},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":64246,\"start\":64075}]", "paragraph": "[{\"end\":2535,\"start\":745},{\"end\":2943,\"start\":2586},{\"end\":3607,\"start\":2945},{\"end\":4022,\"start\":3609},{\"end\":4945,\"start\":4024},{\"end\":5170,\"start\":4947},{\"end\":5474,\"start\":5187},{\"end\":5751,\"start\":5490},{\"end\":6851,\"start\":5753},{\"end\":7472,\"start\":6853},{\"end\":8819,\"start\":7474},{\"end\":9042,\"start\":8821},{\"end\":9358,\"start\":9062},{\"end\":11296,\"start\":9360},{\"end\":12059,\"start\":11298},{\"end\":12438,\"start\":12061},{\"end\":12558,\"start\":12440},{\"end\":12630,\"start\":12579},{\"end\":13847,\"start\":12632},{\"end\":14264,\"start\":13867},{\"end\":14595,\"start\":14310},{\"end\":14832,\"start\":14620},{\"end\":15385,\"start\":14834},{\"end\":15496,\"start\":15425},{\"end\":15953,\"start\":15552},{\"end\":16063,\"start\":15979},{\"end\":16195,\"start\":16113},{\"end\":16362,\"start\":16308},{\"end\":16639,\"start\":16430},{\"end\":17037,\"start\":16734},{\"end\":17348,\"start\":17061},{\"end\":17375,\"start\":17350},{\"end\":17699,\"start\":17546},{\"end\":17866,\"start\":17701},{\"end\":19264,\"start\":17972},{\"end\":19624,\"start\":19349},{\"end\":19965,\"start\":19626},{\"end\":20283,\"start\":20009},{\"end\":20407,\"start\":20285},{\"end\":20630,\"start\":20450},{\"end\":20895,\"start\":20632},{\"end\":21278,\"start\":20973},{\"end\":21948,\"start\":21347},{\"end\":22401,\"start\":21950},{\"end\":22991,\"start\":22492},{\"end\":23103,\"start\":22993},{\"end\":23201,\"start\":23155},{\"end\":23265,\"start\":23208},{\"end\":23686,\"start\":23299},{\"end\":24231,\"start\":23764},{\"end\":24636,\"start\":24233},{\"end\":24742,\"start\":24638},{\"end\":25417,\"start\":24802},{\"end\":25800,\"start\":25419},{\"end\":26522,\"start\":25802},{\"end\":26787,\"start\":26587},{\"end\":27395,\"start\":26851},{\"end\":27686,\"start\":27457},{\"end\":28175,\"start\":27688},{\"end\":28801,\"start\":28177},{\"end\":29349,\"start\":28878},{\"end\":29832,\"start\":29397},{\"end\":29928,\"start\":29878},{\"end\":30247,\"start\":30119},{\"end\":30359,\"start\":30249},{\"end\":30436,\"start\":30361},{\"end\":30850,\"start\":30457},{\"end\":31371,\"start\":30915},{\"end\":31745,\"start\":31373},{\"end\":32304,\"start\":31816},{\"end\":32867,\"start\":32410},{\"end\":33085,\"start\":32869},{\"end\":33450,\"start\":33186},{\"end\":33519,\"start\":33452},{\"end\":33951,\"start\":33582},{\"end\":34192,\"start\":33971},{\"end\":34864,\"start\":34218},{\"end\":35283,\"start\":34866},{\"end\":35877,\"start\":35285},{\"end\":36473,\"start\":35879},{\"end\":36806,\"start\":36488},{\"end\":37874,\"start\":36808},{\"end\":38810,\"start\":37899},{\"end\":39605,\"start\":38812},{\"end\":40361,\"start\":39647},{\"end\":41068,\"start\":40363},{\"end\":41348,\"start\":41070},{\"end\":41646,\"start\":41350},{\"end\":41850,\"start\":41674},{\"end\":43121,\"start\":41852},{\"end\":43798,\"start\":43123},{\"end\":44070,\"start\":43800},{\"end\":44520,\"start\":44072},{\"end\":45674,\"start\":44565},{\"end\":46358,\"start\":45676},{\"end\":46575,\"start\":46360},{\"end\":47711,\"start\":46621},{\"end\":48264,\"start\":47713},{\"end\":48669,\"start\":48290},{\"end\":48999,\"start\":48671},{\"end\":49337,\"start\":49053},{\"end\":49413,\"start\":49339},{\"end\":49504,\"start\":49415},{\"end\":49557,\"start\":49506},{\"end\":50343,\"start\":49559},{\"end\":50934,\"start\":50391},{\"end\":51983,\"start\":50936},{\"end\":52298,\"start\":52019},{\"end\":53949,\"start\":52419},{\"end\":54615,\"start\":53951},{\"end\":56161,\"start\":54730},{\"end\":56562,\"start\":56163},{\"end\":57019,\"start\":56590},{\"end\":57268,\"start\":57028},{\"end\":57401,\"start\":57277},{\"end\":57536,\"start\":57410},{\"end\":57733,\"start\":57546},{\"end\":57869,\"start\":57747},{\"end\":58457,\"start\":57878},{\"end\":58587,\"start\":58471},{\"end\":58811,\"start\":58601},{\"end\":59049,\"start\":58825},{\"end\":59532,\"start\":59063},{\"end\":59905,\"start\":59546},{\"end\":60452,\"start\":59919},{\"end\":61031,\"start\":60466},{\"end\":61269,\"start\":61045},{\"end\":62362,\"start\":61283},{\"end\":62618,\"start\":62367},{\"end\":63278,\"start\":62800},{\"end\":63319,\"start\":63297},{\"end\":63415,\"start\":63345},{\"end\":63524,\"start\":63444},{\"end\":63650,\"start\":63551},{\"end\":64073,\"start\":63655},{\"end\":64245,\"start\":64099}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14619,\"start\":14596},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15550,\"start\":15497},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15551,\"start\":15550},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16111,\"start\":16064},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16112,\"start\":16111},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16307,\"start\":16196},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16429,\"start\":16363},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16733,\"start\":16640},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17060,\"start\":17038},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17545,\"start\":17376},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17970,\"start\":17867},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17971,\"start\":17970},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19320,\"start\":19265},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20007,\"start\":19966},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20008,\"start\":20007},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20449,\"start\":20408},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20972,\"start\":20896},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21346,\"start\":21279},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22491,\"start\":22402},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23154,\"start\":23104},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24773,\"start\":24743},{\"attributes\":{\"id\":\"formula_21\"},\"end\":24774,\"start\":24773},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24800,\"start\":24774},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24801,\"start\":24800},{\"attributes\":{\"id\":\"formula_24\"},\"end\":26586,\"start\":26523},{\"attributes\":{\"id\":\"formula_25\"},\"end\":29395,\"start\":29350},{\"attributes\":{\"id\":\"formula_26\"},\"end\":29396,\"start\":29395},{\"attributes\":{\"id\":\"formula_27\"},\"end\":29876,\"start\":29833},{\"attributes\":{\"id\":\"formula_28\"},\"end\":29877,\"start\":29876},{\"attributes\":{\"id\":\"formula_29\"},\"end\":30117,\"start\":29929},{\"attributes\":{\"id\":\"formula_30\"},\"end\":30118,\"start\":30117},{\"attributes\":{\"id\":\"formula_31\"},\"end\":30456,\"start\":30437},{\"attributes\":{\"id\":\"formula_32\"},\"end\":30913,\"start\":30851},{\"attributes\":{\"id\":\"formula_33\"},\"end\":30914,\"start\":30913},{\"attributes\":{\"id\":\"formula_34\"},\"end\":32409,\"start\":32305},{\"attributes\":{\"id\":\"formula_35\"},\"end\":33185,\"start\":33086},{\"attributes\":{\"id\":\"formula_36\"},\"end\":33581,\"start\":33520},{\"attributes\":{\"id\":\"formula_37\"},\"end\":49052,\"start\":49000}]", "table_ref": "[{\"end\":39156,\"start\":39155},{\"end\":39317,\"start\":39315},{\"end\":39374,\"start\":39371},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47721,\"start\":47719},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":50165,\"start\":50162},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":51423,\"start\":51419}]", "section_header": "[{\"end\":743,\"start\":696},{\"end\":2553,\"start\":2538},{\"end\":2584,\"start\":2556},{\"end\":5185,\"start\":5173},{\"end\":5488,\"start\":5477},{\"end\":9060,\"start\":9045},{\"end\":12577,\"start\":12561},{\"end\":13865,\"start\":13850},{\"end\":14287,\"start\":14267},{\"end\":14308,\"start\":14290},{\"end\":15423,\"start\":15388},{\"end\":15977,\"start\":15956},{\"end\":19347,\"start\":19322},{\"end\":23206,\"start\":23204},{\"end\":23297,\"start\":23268},{\"end\":23708,\"start\":23689},{\"end\":23762,\"start\":23711},{\"end\":26849,\"start\":26790},{\"end\":27455,\"start\":27398},{\"end\":28876,\"start\":28804},{\"end\":31814,\"start\":31748},{\"end\":33969,\"start\":33954},{\"end\":34216,\"start\":34195},{\"end\":36486,\"start\":36476},{\"end\":37897,\"start\":37877},{\"end\":39645,\"start\":39608},{\"end\":41672,\"start\":41649},{\"end\":44563,\"start\":44523},{\"end\":46619,\"start\":46578},{\"end\":48288,\"start\":48267},{\"end\":50389,\"start\":50346},{\"end\":52017,\"start\":51986},{\"end\":52417,\"start\":52301},{\"end\":54728,\"start\":54618},{\"end\":56586,\"start\":56564},{\"end\":57025,\"start\":57022},{\"end\":57274,\"start\":57271},{\"end\":57407,\"start\":57404},{\"end\":57543,\"start\":57539},{\"end\":57744,\"start\":57736},{\"end\":57875,\"start\":57872},{\"end\":58468,\"start\":58460},{\"end\":58598,\"start\":58590},{\"end\":58822,\"start\":58814},{\"end\":59060,\"start\":59052},{\"end\":59543,\"start\":59535},{\"end\":59916,\"start\":59908},{\"end\":60463,\"start\":60455},{\"end\":61042,\"start\":61034},{\"end\":61280,\"start\":61272},{\"end\":63294,\"start\":63281},{\"end\":63341,\"start\":63322},{\"end\":63439,\"start\":63418},{\"end\":63545,\"start\":63527},{\"end\":64095,\"start\":64076}]", "table": "[{\"end\":62799,\"start\":62619}]", "figure_caption": "[{\"end\":57020,\"start\":56589},{\"end\":57269,\"start\":57027},{\"end\":57402,\"start\":57276},{\"end\":57537,\"start\":57409},{\"end\":57734,\"start\":57545},{\"end\":57870,\"start\":57746},{\"end\":58458,\"start\":57877},{\"end\":58588,\"start\":58470},{\"end\":58812,\"start\":58600},{\"end\":59050,\"start\":58824},{\"end\":59533,\"start\":59062},{\"end\":59906,\"start\":59545},{\"end\":60453,\"start\":59918},{\"end\":61032,\"start\":60465},{\"end\":61270,\"start\":61044},{\"end\":62363,\"start\":61282},{\"end\":62619,\"start\":62366},{\"end\":63416,\"start\":63344},{\"end\":63525,\"start\":63443},{\"end\":63651,\"start\":63550},{\"end\":64074,\"start\":63654},{\"end\":64246,\"start\":64098}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27694,\"start\":27693},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28733,\"start\":28729},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39925,\"start\":39924},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42451,\"start\":42450},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44675,\"start\":44674},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44758,\"start\":44746},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44911,\"start\":44907},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45134,\"start\":45133},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45317,\"start\":45314},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45533,\"start\":45532},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45682,\"start\":45681},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":49882,\"start\":49881},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52620,\"start\":52619},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":53196,\"start\":53189},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":54079,\"start\":54078},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":54887,\"start\":54886},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":55048,\"start\":55047}]", "bib_author_first_name": "[{\"end\":64937,\"start\":64936},{\"end\":65100,\"start\":65099},{\"end\":65106,\"start\":65105},{\"end\":65260,\"start\":65259},{\"end\":65272,\"start\":65271},{\"end\":65282,\"start\":65281},{\"end\":65289,\"start\":65288},{\"end\":65291,\"start\":65290},{\"end\":65366,\"start\":65365},{\"end\":65373,\"start\":65372},{\"end\":65382,\"start\":65378},{\"end\":65389,\"start\":65388},{\"end\":65502,\"start\":65501},{\"end\":65504,\"start\":65503},{\"end\":65737,\"start\":65736},{\"end\":65739,\"start\":65738},{\"end\":65749,\"start\":65748},{\"end\":65751,\"start\":65750},{\"end\":65862,\"start\":65861},{\"end\":65864,\"start\":65863},{\"end\":65965,\"start\":65964},{\"end\":66094,\"start\":66093},{\"end\":66101,\"start\":66100},{\"end\":66107,\"start\":66106},{\"end\":66114,\"start\":66113},{\"end\":66121,\"start\":66120},{\"end\":66220,\"start\":66219},{\"end\":66227,\"start\":66226},{\"end\":66233,\"start\":66232},{\"end\":66243,\"start\":66239},{\"end\":66250,\"start\":66249},{\"end\":66252,\"start\":66251},{\"end\":66413,\"start\":66412},{\"end\":66420,\"start\":66419},{\"end\":66429,\"start\":66428},{\"end\":66440,\"start\":66436},{\"end\":66447,\"start\":66446},{\"end\":66449,\"start\":66448},{\"end\":66599,\"start\":66598},{\"end\":66601,\"start\":66600},{\"end\":66612,\"start\":66611},{\"end\":66614,\"start\":66613},{\"end\":66622,\"start\":66621},{\"end\":66624,\"start\":66623},{\"end\":66635,\"start\":66634},{\"end\":66637,\"start\":66636},{\"end\":66796,\"start\":66795},{\"end\":66798,\"start\":66797},{\"end\":66805,\"start\":66804},{\"end\":66811,\"start\":66810},{\"end\":66897,\"start\":66896},{\"end\":66911,\"start\":66910},{\"end\":67003,\"start\":67002},{\"end\":67016,\"start\":67015},{\"end\":67028,\"start\":67027},{\"end\":67140,\"start\":67139},{\"end\":67150,\"start\":67146},{\"end\":67157,\"start\":67156},{\"end\":67159,\"start\":67158},{\"end\":67276,\"start\":67275},{\"end\":67283,\"start\":67282},{\"end\":67292,\"start\":67288},{\"end\":67299,\"start\":67298},{\"end\":67301,\"start\":67300},{\"end\":67415,\"start\":67414},{\"end\":67425,\"start\":67421},{\"end\":67432,\"start\":67431},{\"end\":67434,\"start\":67433},{\"end\":67642,\"start\":67641},{\"end\":67649,\"start\":67648},{\"end\":67658,\"start\":67657},{\"end\":67665,\"start\":67664},{\"end\":67676,\"start\":67672},{\"end\":67683,\"start\":67682},{\"end\":67685,\"start\":67684},{\"end\":67810,\"start\":67809},{\"end\":67817,\"start\":67816},{\"end\":67826,\"start\":67825},{\"end\":67833,\"start\":67832},{\"end\":67844,\"start\":67840},{\"end\":67851,\"start\":67850},{\"end\":67853,\"start\":67852},{\"end\":68092,\"start\":68091},{\"end\":68099,\"start\":68098},{\"end\":68105,\"start\":68104},{\"end\":68117,\"start\":68113},{\"end\":68124,\"start\":68123},{\"end\":68126,\"start\":68125},{\"end\":68315,\"start\":68314},{\"end\":68453,\"start\":68452},{\"end\":68467,\"start\":68466},{\"end\":68480,\"start\":68479},{\"end\":68482,\"start\":68481},{\"end\":68584,\"start\":68583},{\"end\":68592,\"start\":68591},{\"end\":68602,\"start\":68601},{\"end\":68761,\"start\":68760},{\"end\":68769,\"start\":68768},{\"end\":68783,\"start\":68782},{\"end\":68793,\"start\":68792},{\"end\":68854,\"start\":68853},{\"end\":68927,\"start\":68926},{\"end\":68933,\"start\":68932},{\"end\":69029,\"start\":69028},{\"end\":69035,\"start\":69034},{\"end\":69043,\"start\":69042},{\"end\":69187,\"start\":69186},{\"end\":69198,\"start\":69197},{\"end\":69205,\"start\":69204},{\"end\":69207,\"start\":69206},{\"end\":69326,\"start\":69325},{\"end\":69336,\"start\":69335},{\"end\":69480,\"start\":69479},{\"end\":69489,\"start\":69488},{\"end\":69497,\"start\":69496},{\"end\":69511,\"start\":69510},{\"end\":69524,\"start\":69523},{\"end\":69535,\"start\":69534},{\"end\":69745,\"start\":69744},{\"end\":69754,\"start\":69753},{\"end\":69765,\"start\":69764},{\"end\":69776,\"start\":69775},{\"end\":69788,\"start\":69787},{\"end\":69958,\"start\":69957},{\"end\":69968,\"start\":69967},{\"end\":69980,\"start\":69979},{\"end\":70153,\"start\":70152},{\"end\":70160,\"start\":70159},{\"end\":70168,\"start\":70167},{\"end\":70179,\"start\":70178},{\"end\":70181,\"start\":70180},{\"end\":70189,\"start\":70188},{\"end\":70193,\"start\":70190},{\"end\":70201,\"start\":70200},{\"end\":70435,\"start\":70434},{\"end\":70442,\"start\":70441},{\"end\":70453,\"start\":70452},{\"end\":70455,\"start\":70454},{\"end\":70573,\"start\":70572},{\"end\":70823,\"start\":70822},{\"end\":70830,\"start\":70829},{\"end\":70839,\"start\":70838},{\"end\":70847,\"start\":70846},{\"end\":70854,\"start\":70853},{\"end\":70856,\"start\":70855},{\"end\":70983,\"start\":70982},{\"end\":70990,\"start\":70989},{\"end\":70999,\"start\":70998},{\"end\":71080,\"start\":71079},{\"end\":71082,\"start\":71081},{\"end\":71092,\"start\":71091},{\"end\":71294,\"start\":71293},{\"end\":71301,\"start\":71300},{\"end\":71308,\"start\":71307},{\"end\":71314,\"start\":71313},{\"end\":71460,\"start\":71459},{\"end\":71462,\"start\":71461},{\"end\":71472,\"start\":71471},{\"end\":71529,\"start\":71528},{\"end\":71540,\"start\":71539},{\"end\":71610,\"start\":71609},{\"end\":71612,\"start\":71611},{\"end\":71621,\"start\":71620},{\"end\":71623,\"start\":71622},{\"end\":71739,\"start\":71738},{\"end\":71741,\"start\":71740},{\"end\":71814,\"start\":71813},{\"end\":71822,\"start\":71821},{\"end\":71824,\"start\":71823},{\"end\":71831,\"start\":71830},{\"end\":72000,\"start\":71999},{\"end\":72011,\"start\":72010},{\"end\":72019,\"start\":72018},{\"end\":72026,\"start\":72025},{\"end\":72033,\"start\":72032},{\"end\":72035,\"start\":72034},{\"end\":72200,\"start\":72199},{\"end\":72207,\"start\":72206},{\"end\":72214,\"start\":72213},{\"end\":72226,\"start\":72222},{\"end\":72233,\"start\":72232},{\"end\":72235,\"start\":72234},{\"end\":72429,\"start\":72428},{\"end\":72439,\"start\":72438},{\"end\":72449,\"start\":72448},{\"end\":72467,\"start\":72466},{\"end\":72474,\"start\":72472},{\"end\":72613,\"start\":72612},{\"end\":72615,\"start\":72614},{\"end\":72625,\"start\":72624},{\"end\":72678,\"start\":72677},{\"end\":72688,\"start\":72687},{\"end\":72822,\"start\":72821},{\"end\":72834,\"start\":72833},{\"end\":72844,\"start\":72843},{\"end\":72856,\"start\":72855},{\"end\":72867,\"start\":72866},{\"end\":72992,\"start\":72991},{\"end\":72998,\"start\":72997},{\"end\":73007,\"start\":73006},{\"end\":73014,\"start\":73013},{\"end\":73174,\"start\":73173},{\"end\":73185,\"start\":73184},{\"end\":73195,\"start\":73194},{\"end\":73206,\"start\":73205},{\"end\":73339,\"start\":73338},{\"end\":73351,\"start\":73350},{\"end\":73361,\"start\":73360},{\"end\":73371,\"start\":73370},{\"end\":73381,\"start\":73380},{\"end\":73520,\"start\":73519},{\"end\":73522,\"start\":73521},{\"end\":73539,\"start\":73538},{\"end\":73541,\"start\":73540},{\"end\":73553,\"start\":73552},{\"end\":73562,\"start\":73561},{\"end\":73572,\"start\":73571},{\"end\":73583,\"start\":73582},{\"end\":73585,\"start\":73584},{\"end\":73826,\"start\":73825},{\"end\":73828,\"start\":73827},{\"end\":73845,\"start\":73844},{\"end\":73847,\"start\":73846},{\"end\":73859,\"start\":73858},{\"end\":73866,\"start\":73865},{\"end\":73868,\"start\":73867},{\"end\":73877,\"start\":73876},{\"end\":73896,\"start\":73895},{\"end\":73898,\"start\":73897},{\"end\":74061,\"start\":74060},{\"end\":74063,\"start\":74062},{\"end\":74072,\"start\":74071},{\"end\":74082,\"start\":74081},{\"end\":74244,\"start\":74243},{\"end\":74252,\"start\":74251},{\"end\":74263,\"start\":74262},{\"end\":74420,\"start\":74419},{\"end\":74428,\"start\":74427},{\"end\":74446,\"start\":74445}]", "bib_author_last_name": "[{\"end\":64945,\"start\":64938},{\"end\":65103,\"start\":65101},{\"end\":65114,\"start\":65107},{\"end\":65269,\"start\":65261},{\"end\":65279,\"start\":65273},{\"end\":65286,\"start\":65283},{\"end\":65305,\"start\":65292},{\"end\":65370,\"start\":65367},{\"end\":65376,\"start\":65374},{\"end\":65386,\"start\":65383},{\"end\":65392,\"start\":65390},{\"end\":65509,\"start\":65505},{\"end\":65746,\"start\":65740},{\"end\":65758,\"start\":65752},{\"end\":65871,\"start\":65865},{\"end\":65975,\"start\":65966},{\"end\":66098,\"start\":66095},{\"end\":66104,\"start\":66102},{\"end\":66111,\"start\":66108},{\"end\":66118,\"start\":66115},{\"end\":66124,\"start\":66122},{\"end\":66224,\"start\":66221},{\"end\":66230,\"start\":66228},{\"end\":66237,\"start\":66234},{\"end\":66247,\"start\":66244},{\"end\":66255,\"start\":66253},{\"end\":66417,\"start\":66414},{\"end\":66426,\"start\":66421},{\"end\":66434,\"start\":66430},{\"end\":66444,\"start\":66441},{\"end\":66452,\"start\":66450},{\"end\":66609,\"start\":66602},{\"end\":66619,\"start\":66615},{\"end\":66632,\"start\":66625},{\"end\":66641,\"start\":66638},{\"end\":66802,\"start\":66799},{\"end\":66808,\"start\":66806},{\"end\":66819,\"start\":66812},{\"end\":66908,\"start\":66898},{\"end\":66920,\"start\":66912},{\"end\":67013,\"start\":67004},{\"end\":67025,\"start\":67017},{\"end\":67035,\"start\":67029},{\"end\":67144,\"start\":67141},{\"end\":67154,\"start\":67151},{\"end\":67162,\"start\":67160},{\"end\":67280,\"start\":67277},{\"end\":67286,\"start\":67284},{\"end\":67296,\"start\":67293},{\"end\":67304,\"start\":67302},{\"end\":67419,\"start\":67416},{\"end\":67429,\"start\":67426},{\"end\":67437,\"start\":67435},{\"end\":67646,\"start\":67643},{\"end\":67655,\"start\":67650},{\"end\":67662,\"start\":67659},{\"end\":67670,\"start\":67666},{\"end\":67680,\"start\":67677},{\"end\":67688,\"start\":67686},{\"end\":67814,\"start\":67811},{\"end\":67823,\"start\":67818},{\"end\":67830,\"start\":67827},{\"end\":67838,\"start\":67834},{\"end\":67848,\"start\":67845},{\"end\":67856,\"start\":67854},{\"end\":68096,\"start\":68093},{\"end\":68102,\"start\":68100},{\"end\":68111,\"start\":68106},{\"end\":68121,\"start\":68118},{\"end\":68129,\"start\":68127},{\"end\":68321,\"start\":68316},{\"end\":68464,\"start\":68454},{\"end\":68477,\"start\":68468},{\"end\":68489,\"start\":68483},{\"end\":68589,\"start\":68585},{\"end\":68599,\"start\":68593},{\"end\":68609,\"start\":68603},{\"end\":68766,\"start\":68762},{\"end\":68780,\"start\":68770},{\"end\":68790,\"start\":68784},{\"end\":68800,\"start\":68794},{\"end\":68865,\"start\":68855},{\"end\":68930,\"start\":68928},{\"end\":68939,\"start\":68934},{\"end\":69032,\"start\":69030},{\"end\":69040,\"start\":69036},{\"end\":69049,\"start\":69044},{\"end\":69195,\"start\":69188},{\"end\":69202,\"start\":69199},{\"end\":69215,\"start\":69208},{\"end\":69333,\"start\":69327},{\"end\":69343,\"start\":69337},{\"end\":69486,\"start\":69481},{\"end\":69494,\"start\":69490},{\"end\":69508,\"start\":69498},{\"end\":69521,\"start\":69512},{\"end\":69532,\"start\":69525},{\"end\":69544,\"start\":69536},{\"end\":69751,\"start\":69746},{\"end\":69762,\"start\":69755},{\"end\":69773,\"start\":69766},{\"end\":69785,\"start\":69777},{\"end\":69794,\"start\":69789},{\"end\":69965,\"start\":69959},{\"end\":69977,\"start\":69969},{\"end\":69983,\"start\":69981},{\"end\":70157,\"start\":70154},{\"end\":70165,\"start\":70161},{\"end\":70176,\"start\":70169},{\"end\":70186,\"start\":70182},{\"end\":70198,\"start\":70194},{\"end\":70212,\"start\":70202},{\"end\":70439,\"start\":70436},{\"end\":70450,\"start\":70443},{\"end\":70463,\"start\":70456},{\"end\":70578,\"start\":70574},{\"end\":70827,\"start\":70824},{\"end\":70836,\"start\":70831},{\"end\":70844,\"start\":70840},{\"end\":70851,\"start\":70848},{\"end\":70859,\"start\":70857},{\"end\":70987,\"start\":70984},{\"end\":70996,\"start\":70991},{\"end\":71002,\"start\":71000},{\"end\":71089,\"start\":71083},{\"end\":71100,\"start\":71093},{\"end\":71298,\"start\":71295},{\"end\":71305,\"start\":71302},{\"end\":71311,\"start\":71309},{\"end\":71317,\"start\":71315},{\"end\":71469,\"start\":71463},{\"end\":71537,\"start\":71530},{\"end\":71546,\"start\":71541},{\"end\":71618,\"start\":71613},{\"end\":71630,\"start\":71624},{\"end\":71750,\"start\":71742},{\"end\":71819,\"start\":71815},{\"end\":71828,\"start\":71825},{\"end\":71840,\"start\":71832},{\"end\":72008,\"start\":72001},{\"end\":72016,\"start\":72012},{\"end\":72023,\"start\":72020},{\"end\":72030,\"start\":72027},{\"end\":72038,\"start\":72036},{\"end\":72204,\"start\":72201},{\"end\":72211,\"start\":72208},{\"end\":72220,\"start\":72215},{\"end\":72230,\"start\":72227},{\"end\":72238,\"start\":72236},{\"end\":72436,\"start\":72430},{\"end\":72446,\"start\":72440},{\"end\":72464,\"start\":72450},{\"end\":72470,\"start\":72468},{\"end\":72483,\"start\":72475},{\"end\":72622,\"start\":72616},{\"end\":72628,\"start\":72626},{\"end\":72685,\"start\":72679},{\"end\":72696,\"start\":72689},{\"end\":72831,\"start\":72823},{\"end\":72841,\"start\":72835},{\"end\":72853,\"start\":72845},{\"end\":72864,\"start\":72857},{\"end\":72874,\"start\":72868},{\"end\":72995,\"start\":72993},{\"end\":73004,\"start\":72999},{\"end\":73011,\"start\":73008},{\"end\":73018,\"start\":73015},{\"end\":73182,\"start\":73175},{\"end\":73192,\"start\":73186},{\"end\":73203,\"start\":73196},{\"end\":73219,\"start\":73207},{\"end\":73348,\"start\":73340},{\"end\":73358,\"start\":73352},{\"end\":73368,\"start\":73362},{\"end\":73378,\"start\":73372},{\"end\":73388,\"start\":73382},{\"end\":73536,\"start\":73523},{\"end\":73550,\"start\":73542},{\"end\":73559,\"start\":73554},{\"end\":73569,\"start\":73563},{\"end\":73580,\"start\":73573},{\"end\":73598,\"start\":73586},{\"end\":73842,\"start\":73829},{\"end\":73856,\"start\":73848},{\"end\":73863,\"start\":73860},{\"end\":73874,\"start\":73869},{\"end\":73893,\"start\":73878},{\"end\":73911,\"start\":73899},{\"end\":74069,\"start\":74064},{\"end\":74079,\"start\":74073},{\"end\":74095,\"start\":74083},{\"end\":74249,\"start\":74245},{\"end\":74260,\"start\":74253},{\"end\":74270,\"start\":74264},{\"end\":74425,\"start\":74421},{\"end\":74443,\"start\":74429},{\"end\":74453,\"start\":74447}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11101055},\"end\":65010,\"start\":64912},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":19300},\"end\":65192,\"start\":65012},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1033690},\"end\":65331,\"start\":65194},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7252503},\"end\":65466,\"start\":65333},{\"attributes\":{\"doi\":\"10.1201/b10905/handbook-markov-chain-monte-carlo-steve-brooks-andrew-gelman-galin-jones-xiao-li-meng\",\"id\":\"b4\",\"matched_paper_id\":123755879},\"end\":65656,\"start\":65468},{\"attributes\":{\"id\":\"b5\"},\"end\":65792,\"start\":65658},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207596505},\"end\":65895,\"start\":65794},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1152227},\"end\":66003,\"start\":65897},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9212174},\"end\":66158,\"start\":66005},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7759006},\"end\":66309,\"start\":66160},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2021.3069023\",\"id\":\"b10\",\"matched_paper_id\":232375868},\"end\":66548,\"start\":66311},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":336219},\"end\":66746,\"start\":66550},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2171181},\"end\":66871,\"start\":66748},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12698795},\"end\":66947,\"start\":66873},{\"attributes\":{\"doi\":\"arXiv:1507.04888\",\"id\":\"b14\"},\"end\":67057,\"start\":66949},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":201098397},\"end\":67216,\"start\":67059},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8742525},\"end\":67342,\"start\":67218},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":763074},\"end\":67552,\"start\":67344},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":221082778},\"end\":67741,\"start\":67554},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4564025},\"end\":67963,\"start\":67743},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":214794878},\"end\":68252,\"start\":67965},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6916627},\"end\":68385,\"start\":68254},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195908774},\"end\":68505,\"start\":68387},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8121626},\"end\":68647,\"start\":68507},{\"attributes\":{\"doi\":\"arXiv:1611.03852\",\"id\":\"b24\"},\"end\":68822,\"start\":68649},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10319744},\"end\":68881,\"start\":68824},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16153365},\"end\":68955,\"start\":68883},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4474295},\"end\":69087,\"start\":68957},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13467797},\"end\":69257,\"start\":69089},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5102429},\"end\":69417,\"start\":69259},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9854676},\"end\":69659,\"start\":69419},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4461350},\"end\":69901,\"start\":69661},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":42549843},\"end\":70073,\"start\":69903},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8394584},\"end\":70327,\"start\":70075},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9228629},\"end\":70506,\"start\":70329},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":104291828},\"end\":70701,\"start\":70508},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":229751490},\"end\":70901,\"start\":70703},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":229924180},\"end\":71044,\"start\":70903},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":211146177},\"end\":71184,\"start\":71046},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":248811555},\"end\":71401,\"start\":71186},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":49657329},\"end\":71491,\"start\":71403},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16945044},\"end\":71575,\"start\":71493},{\"attributes\":{\"id\":\"b42\"},\"end\":71663,\"start\":71577},{\"attributes\":{\"id\":\"b43\"},\"end\":71766,\"start\":71665},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3228832},\"end\":71916,\"start\":71768},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":88516917},\"end\":72118,\"start\":71918},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":54055949},\"end\":72318,\"start\":72120},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":5018124},\"end\":72566,\"start\":72320},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b48\"},\"end\":72649,\"start\":72568},{\"attributes\":{\"doi\":\"FHWA-HRT-07- 030\",\"id\":\"b49\"},\"end\":72778,\"start\":72651},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b50\"},\"end\":72896,\"start\":72780},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":13740328},\"end\":73107,\"start\":72898},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":7980756},\"end\":73302,\"start\":73109},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":16046818},\"end\":73462,\"start\":73304},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3704867},\"end\":73711,\"start\":73464},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":76663871},\"end\":73991,\"start\":73713},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":9421360},\"end\":74189,\"start\":73993},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":13995862},\"end\":74382,\"start\":74191},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":8768364},\"end\":74537,\"start\":74384}]", "bib_title": "[{\"end\":64934,\"start\":64912},{\"end\":65097,\"start\":65012},{\"end\":65257,\"start\":65194},{\"end\":65363,\"start\":65333},{\"end\":65499,\"start\":65468},{\"end\":65859,\"start\":65794},{\"end\":65962,\"start\":65897},{\"end\":66091,\"start\":66005},{\"end\":66217,\"start\":66160},{\"end\":66410,\"start\":66311},{\"end\":66596,\"start\":66550},{\"end\":66793,\"start\":66748},{\"end\":66894,\"start\":66873},{\"end\":67137,\"start\":67059},{\"end\":67273,\"start\":67218},{\"end\":67412,\"start\":67344},{\"end\":67639,\"start\":67554},{\"end\":67807,\"start\":67743},{\"end\":68089,\"start\":67965},{\"end\":68312,\"start\":68254},{\"end\":68450,\"start\":68387},{\"end\":68581,\"start\":68507},{\"end\":68851,\"start\":68824},{\"end\":68924,\"start\":68883},{\"end\":69026,\"start\":68957},{\"end\":69184,\"start\":69089},{\"end\":69323,\"start\":69259},{\"end\":69477,\"start\":69419},{\"end\":69742,\"start\":69661},{\"end\":69955,\"start\":69903},{\"end\":70150,\"start\":70075},{\"end\":70432,\"start\":70329},{\"end\":70570,\"start\":70508},{\"end\":70820,\"start\":70703},{\"end\":70980,\"start\":70903},{\"end\":71077,\"start\":71046},{\"end\":71291,\"start\":71186},{\"end\":71457,\"start\":71403},{\"end\":71526,\"start\":71493},{\"end\":71811,\"start\":71768},{\"end\":71997,\"start\":71918},{\"end\":72197,\"start\":72120},{\"end\":72426,\"start\":72320},{\"end\":72675,\"start\":72651},{\"end\":72989,\"start\":72898},{\"end\":73171,\"start\":73109},{\"end\":73336,\"start\":73304},{\"end\":73517,\"start\":73464},{\"end\":73823,\"start\":73713},{\"end\":74058,\"start\":73993},{\"end\":74241,\"start\":74191},{\"end\":74417,\"start\":74384}]", "bib_author": "[{\"end\":64947,\"start\":64936},{\"end\":65105,\"start\":65099},{\"end\":65116,\"start\":65105},{\"end\":65271,\"start\":65259},{\"end\":65281,\"start\":65271},{\"end\":65288,\"start\":65281},{\"end\":65307,\"start\":65288},{\"end\":65372,\"start\":65365},{\"end\":65378,\"start\":65372},{\"end\":65388,\"start\":65378},{\"end\":65394,\"start\":65388},{\"end\":65511,\"start\":65501},{\"end\":65748,\"start\":65736},{\"end\":65760,\"start\":65748},{\"end\":65873,\"start\":65861},{\"end\":65977,\"start\":65964},{\"end\":66100,\"start\":66093},{\"end\":66106,\"start\":66100},{\"end\":66113,\"start\":66106},{\"end\":66120,\"start\":66113},{\"end\":66126,\"start\":66120},{\"end\":66226,\"start\":66219},{\"end\":66232,\"start\":66226},{\"end\":66239,\"start\":66232},{\"end\":66249,\"start\":66239},{\"end\":66257,\"start\":66249},{\"end\":66419,\"start\":66412},{\"end\":66428,\"start\":66419},{\"end\":66436,\"start\":66428},{\"end\":66446,\"start\":66436},{\"end\":66454,\"start\":66446},{\"end\":66611,\"start\":66598},{\"end\":66621,\"start\":66611},{\"end\":66634,\"start\":66621},{\"end\":66643,\"start\":66634},{\"end\":66804,\"start\":66795},{\"end\":66810,\"start\":66804},{\"end\":66821,\"start\":66810},{\"end\":66910,\"start\":66896},{\"end\":66922,\"start\":66910},{\"end\":67015,\"start\":67002},{\"end\":67027,\"start\":67015},{\"end\":67037,\"start\":67027},{\"end\":67146,\"start\":67139},{\"end\":67156,\"start\":67146},{\"end\":67164,\"start\":67156},{\"end\":67282,\"start\":67275},{\"end\":67288,\"start\":67282},{\"end\":67298,\"start\":67288},{\"end\":67306,\"start\":67298},{\"end\":67421,\"start\":67414},{\"end\":67431,\"start\":67421},{\"end\":67439,\"start\":67431},{\"end\":67648,\"start\":67641},{\"end\":67657,\"start\":67648},{\"end\":67664,\"start\":67657},{\"end\":67672,\"start\":67664},{\"end\":67682,\"start\":67672},{\"end\":67690,\"start\":67682},{\"end\":67816,\"start\":67809},{\"end\":67825,\"start\":67816},{\"end\":67832,\"start\":67825},{\"end\":67840,\"start\":67832},{\"end\":67850,\"start\":67840},{\"end\":67858,\"start\":67850},{\"end\":68098,\"start\":68091},{\"end\":68104,\"start\":68098},{\"end\":68113,\"start\":68104},{\"end\":68123,\"start\":68113},{\"end\":68131,\"start\":68123},{\"end\":68323,\"start\":68314},{\"end\":68466,\"start\":68452},{\"end\":68479,\"start\":68466},{\"end\":68491,\"start\":68479},{\"end\":68591,\"start\":68583},{\"end\":68601,\"start\":68591},{\"end\":68611,\"start\":68601},{\"end\":68768,\"start\":68760},{\"end\":68782,\"start\":68768},{\"end\":68792,\"start\":68782},{\"end\":68802,\"start\":68792},{\"end\":68867,\"start\":68853},{\"end\":68932,\"start\":68926},{\"end\":68941,\"start\":68932},{\"end\":69034,\"start\":69028},{\"end\":69042,\"start\":69034},{\"end\":69051,\"start\":69042},{\"end\":69197,\"start\":69186},{\"end\":69204,\"start\":69197},{\"end\":69217,\"start\":69204},{\"end\":69335,\"start\":69325},{\"end\":69345,\"start\":69335},{\"end\":69488,\"start\":69479},{\"end\":69496,\"start\":69488},{\"end\":69510,\"start\":69496},{\"end\":69523,\"start\":69510},{\"end\":69534,\"start\":69523},{\"end\":69546,\"start\":69534},{\"end\":69753,\"start\":69744},{\"end\":69764,\"start\":69753},{\"end\":69775,\"start\":69764},{\"end\":69787,\"start\":69775},{\"end\":69796,\"start\":69787},{\"end\":69967,\"start\":69957},{\"end\":69979,\"start\":69967},{\"end\":69985,\"start\":69979},{\"end\":70159,\"start\":70152},{\"end\":70167,\"start\":70159},{\"end\":70178,\"start\":70167},{\"end\":70188,\"start\":70178},{\"end\":70200,\"start\":70188},{\"end\":70214,\"start\":70200},{\"end\":70441,\"start\":70434},{\"end\":70452,\"start\":70441},{\"end\":70465,\"start\":70452},{\"end\":70580,\"start\":70572},{\"end\":70829,\"start\":70822},{\"end\":70838,\"start\":70829},{\"end\":70846,\"start\":70838},{\"end\":70853,\"start\":70846},{\"end\":70861,\"start\":70853},{\"end\":70989,\"start\":70982},{\"end\":70998,\"start\":70989},{\"end\":71004,\"start\":70998},{\"end\":71091,\"start\":71079},{\"end\":71102,\"start\":71091},{\"end\":71300,\"start\":71293},{\"end\":71307,\"start\":71300},{\"end\":71313,\"start\":71307},{\"end\":71319,\"start\":71313},{\"end\":71471,\"start\":71459},{\"end\":71475,\"start\":71471},{\"end\":71539,\"start\":71528},{\"end\":71548,\"start\":71539},{\"end\":71620,\"start\":71609},{\"end\":71632,\"start\":71620},{\"end\":71752,\"start\":71738},{\"end\":71821,\"start\":71813},{\"end\":71830,\"start\":71821},{\"end\":71842,\"start\":71830},{\"end\":72010,\"start\":71999},{\"end\":72018,\"start\":72010},{\"end\":72025,\"start\":72018},{\"end\":72032,\"start\":72025},{\"end\":72040,\"start\":72032},{\"end\":72206,\"start\":72199},{\"end\":72213,\"start\":72206},{\"end\":72222,\"start\":72213},{\"end\":72232,\"start\":72222},{\"end\":72240,\"start\":72232},{\"end\":72438,\"start\":72428},{\"end\":72448,\"start\":72438},{\"end\":72466,\"start\":72448},{\"end\":72472,\"start\":72466},{\"end\":72485,\"start\":72472},{\"end\":72624,\"start\":72612},{\"end\":72630,\"start\":72624},{\"end\":72687,\"start\":72677},{\"end\":72698,\"start\":72687},{\"end\":72833,\"start\":72821},{\"end\":72843,\"start\":72833},{\"end\":72855,\"start\":72843},{\"end\":72866,\"start\":72855},{\"end\":72876,\"start\":72866},{\"end\":72997,\"start\":72991},{\"end\":73006,\"start\":72997},{\"end\":73013,\"start\":73006},{\"end\":73020,\"start\":73013},{\"end\":73184,\"start\":73173},{\"end\":73194,\"start\":73184},{\"end\":73205,\"start\":73194},{\"end\":73221,\"start\":73205},{\"end\":73350,\"start\":73338},{\"end\":73360,\"start\":73350},{\"end\":73370,\"start\":73360},{\"end\":73380,\"start\":73370},{\"end\":73390,\"start\":73380},{\"end\":73538,\"start\":73519},{\"end\":73552,\"start\":73538},{\"end\":73561,\"start\":73552},{\"end\":73571,\"start\":73561},{\"end\":73582,\"start\":73571},{\"end\":73600,\"start\":73582},{\"end\":73844,\"start\":73825},{\"end\":73858,\"start\":73844},{\"end\":73865,\"start\":73858},{\"end\":73876,\"start\":73865},{\"end\":73895,\"start\":73876},{\"end\":73913,\"start\":73895},{\"end\":74071,\"start\":74060},{\"end\":74081,\"start\":74071},{\"end\":74097,\"start\":74081},{\"end\":74251,\"start\":74243},{\"end\":74262,\"start\":74251},{\"end\":74272,\"start\":74262},{\"end\":74427,\"start\":74419},{\"end\":74445,\"start\":74427},{\"end\":74455,\"start\":74445}]", "bib_venue": "[{\"end\":65004,\"start\":64947},{\"end\":65154,\"start\":65116},{\"end\":65317,\"start\":65307},{\"end\":65430,\"start\":65394},{\"end\":65647,\"start\":65611},{\"end\":65734,\"start\":65658},{\"end\":65886,\"start\":65873},{\"end\":65996,\"start\":65977},{\"end\":66141,\"start\":66126},{\"end\":66295,\"start\":66257},{\"end\":66533,\"start\":66480},{\"end\":66686,\"start\":66643},{\"end\":66863,\"start\":66821},{\"end\":66933,\"start\":66922},{\"end\":67000,\"start\":66949},{\"end\":67202,\"start\":67164},{\"end\":67325,\"start\":67306},{\"end\":67493,\"start\":67439},{\"end\":67728,\"start\":67690},{\"end\":67908,\"start\":67858},{\"end\":68189,\"start\":68131},{\"end\":68371,\"start\":68323},{\"end\":68495,\"start\":68491},{\"end\":68641,\"start\":68611},{\"end\":68758,\"start\":68649},{\"end\":68871,\"start\":68867},{\"end\":68945,\"start\":68941},{\"end\":69081,\"start\":69051},{\"end\":69237,\"start\":69217},{\"end\":69381,\"start\":69345},{\"end\":69600,\"start\":69546},{\"end\":69846,\"start\":69796},{\"end\":70027,\"start\":69985},{\"end\":70268,\"start\":70214},{\"end\":70493,\"start\":70465},{\"end\":70638,\"start\":70580},{\"end\":70881,\"start\":70861},{\"end\":71024,\"start\":71004},{\"end\":71143,\"start\":71102},{\"end\":71360,\"start\":71319},{\"end\":71479,\"start\":71475},{\"end\":71566,\"start\":71548},{\"end\":71607,\"start\":71577},{\"end\":71736,\"start\":71665},{\"end\":71878,\"start\":71842},{\"end\":72078,\"start\":72040},{\"end\":72278,\"start\":72240},{\"end\":72523,\"start\":72485},{\"end\":72610,\"start\":72568},{\"end\":72743,\"start\":72714},{\"end\":72819,\"start\":72780},{\"end\":73061,\"start\":73020},{\"end\":73259,\"start\":73221},{\"end\":73426,\"start\":73390},{\"end\":73653,\"start\":73600},{\"end\":73950,\"start\":73913},{\"end\":74143,\"start\":74097},{\"end\":74327,\"start\":74272},{\"end\":74496,\"start\":74455},{\"end\":65188,\"start\":65156},{\"end\":65462,\"start\":65432},{\"end\":66152,\"start\":66143},{\"end\":66741,\"start\":66688},{\"end\":67543,\"start\":67495},{\"end\":67954,\"start\":67910},{\"end\":68243,\"start\":68191},{\"end\":68501,\"start\":68497},{\"end\":68877,\"start\":68873},{\"end\":68951,\"start\":68947},{\"end\":69253,\"start\":69239},{\"end\":69413,\"start\":69383},{\"end\":69650,\"start\":69602},{\"end\":69892,\"start\":69848},{\"end\":70065,\"start\":70029},{\"end\":70318,\"start\":70270},{\"end\":70692,\"start\":70640},{\"end\":70897,\"start\":70883},{\"end\":71040,\"start\":71026},{\"end\":71180,\"start\":71145},{\"end\":71397,\"start\":71362},{\"end\":71485,\"start\":71481},{\"end\":71910,\"start\":71880},{\"end\":72112,\"start\":72080},{\"end\":72312,\"start\":72280},{\"end\":72557,\"start\":72525},{\"end\":72764,\"start\":72745},{\"end\":73098,\"start\":73063},{\"end\":73293,\"start\":73261},{\"end\":73458,\"start\":73428},{\"end\":73702,\"start\":73655},{\"end\":73983,\"start\":73952},{\"end\":74185,\"start\":74145},{\"end\":74378,\"start\":74329},{\"end\":74533,\"start\":74498}]"}}}, "year": 2023, "month": 12, "day": 17}