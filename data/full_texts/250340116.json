{"id": 250340116, "updated": "2023-04-27 14:18:26.7", "metadata": {"title": "Structured and Natural Responses Co-generation for Conversational Search", "authors": "[{\"first\":\"Chenchen\",\"last\":\"Ye\",\"middle\":[]},{\"first\":\"Lizi\",\"last\":\"Liao\",\"middle\":[]},{\"first\":\"Fuli\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Tat-Seng\",\"last\":\"Chua\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Generating fluent and informative natural responses while main- taining representative internal states for search optimization is critical for conversational search systems. Existing approaches ei- ther 1) predict structured dialog acts first and then generate natural response; or 2) map conversation context to natural responses di- rectly in an end-to-end manner. Both kinds of approaches have shortcomings. The former suffers from error accumulation while the semantic associations between structured acts and natural re- sponses are confined in single direction. The latter emphasizes generating natural responses but fails to predict structured acts. Therefore, we propose a neural co-generation model that gener- ates the two concurrently. The key lies in a shared latent space shaped by two informed priors. Specifically, we design structured dialog acts and natural response auto-encoding as two auxiliary tasks in an interconnected network architecture. It allows for the concurrent generation and bidirectional semantic associations. The shared latent space also enables asynchronous reinforcement learn- ing for further joint optimization. Experiments show that our model achieves significant performance improvements.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/YeLF0C22", "doi": "10.1145/3477495.3532063"}}, "content": {"source": {"pdf_hash": "42ffe1139f97ce3809c9140e2d92e837fe5eb1e7", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "54d4eaab85f62094617b95ebecb51bf6d870854d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/42ffe1139f97ce3809c9140e2d92e837fe5eb1e7.txt", "contents": "\nStructured and Natural Responses Co-generation for Conversational Search Tat-Seng Chua\nJuly 11-15, 2022. July 11-15, 2022\n\nChenchen Ye chenchenye.ccye@gmail.com \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nLizi Liao liaolizi.llz@gmail.com \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nFuli Feng fulifeng93@gmail.com \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nWei Ji jiwei@nus.edu.sg \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nChenchen Ye \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nLizi Liao \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nFuli Feng \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nWei Ji \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nTat-Seng \nSea-NExT Joint Lab\nNational University of Singapore\nSingapore Management University\nUniversity of Science and Technology of China\nNational University of Singapore\nNational University of Singapore\n\n\nStructured and Natural Responses Co-generation for Conversational Search Tat-Seng Chua\n\nProceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)\nthe 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)Madrid, SpainJuly 11-15, 2022. July 11-15, 2022Sea-NExT Joint Lab, ACM Reference Format: Chua. 2022. Struc-tured and Natural Responses Co-generation for Conversational Search. In Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3477495. 3532063 * Corresponding author. ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00conversational searchco-generationbidirectional association\nGenerating fluent and informative natural responses while maintaining representative internal states for search optimization is critical for conversational search systems. Existing approaches either 1) predict structured dialog acts first and then generate natural response; or 2) map conversation context to natural responses directly in an end-to-end manner. Both kinds of approaches have shortcomings. The former suffers from error accumulation while the semantic associations between structured acts and natural responses are confined in single direction. The latter emphasizes generating natural responses but fails to predict structured acts. Therefore, we propose a neural co-generation model that generates the two concurrently. The key lies in a shared latent space shaped by two informed priors. Specifically, we design structured dialog acts and natural response auto-encoding as two auxiliary tasks in an interconnected network architecture. It allows for the concurrent generation and bidirectional semantic associations. The shared latent space also enables asynchronous reinforcement learning for further joint optimization. Experiments show that our model achieves significant performance improvements.CCS CONCEPTS\u2022 Information systems \u2192 Users and interactive retrieval.\n\nINTRODUCTION\n\nWith the rise of various personal assistants, conversational search has received a surge in popularity and attention [1,10,23,28,32]. Such systems aim to facilitate people with services such as hotel or restaurant booking through natural language conversations [5,6,17]. Different from the open-domain dialogues, its ultimate goal is to provide satisfactory natural responses to the end users and generate structured representations such as dialog acts for internal search optimization. For instance, Figure 1 shows a conversation segment, from which we can notice that both the structured dialog acts and natural language responses are essential for building an effective and efficient conversational search system [36,45].\n\nTraditionally, conversational response generation is conducted as a pipeline with multiple modules one after another. A standard architecture for such methods generally decompose the task into several subtasks, namely natural language understanding, state Topic 3: Conversational IR SIGIR '22, July 11-15, 2022, Madrid, Spain tracking, policy learning and natural response generation. Basically, natural language responses are generated based on formerly predicted dialog acts [15,27,41,51]. However, such methods suffer from the error accumulation problem. For instance, errors occurred in dialog act prediction will mislead the following natural response generation. Also, such sequential modeling forces the semantic associations between structured dialog acts and natural responses to flow in only one direction. The heuristic or clue gained from the latter is unable to benefit dialog act prediction as a former stage.\n\nAnother group of research efforts directly learn the mapping between conversation context to natural language response in an end-to-end manner [26,38,52]. For example, Li et al. [21] leveraged an encoder-decoder RNN to define a policy over an infinite action space consisting of all possible utterances and optimize it via reinforcement learning (RL) with dialogue simulations [19,34]. Zhao et al. [52] narrowed down the action spaces as latent variables and Wang et al. [38] further modeled the hierarchical structure with the option framework [35]. Such end-to-end methods explicitly ignore the structured dialog acts while resort to latent vectors at ease. Although such manner makes the context-to-response modeling more focused, the latent vectors learned are hard to use or interpret. They fail to generate structured responses such as dialog acts for search optimization, e.g. query execution or error debugging.\n\nTo overcome these, we aim to co-generate structured dialog acts and natural language responses at the same time. A viable way is to treat dialog act prediction as another sequence generation task and combine it with the original natural response generation as two decoder branches in multitask learning. Wang et al. [39] followed such structure and further designed dynamic attention to guide response generation with attended dialog acts. However, the interrelationships among dialog acts and natural responses are only modeled locally. The required synchronization between the two decoder branches is hard to realize due to varied vocabularies and sequence lengths. Moreover, the generators only focus on the responses in the current turn while fail to foresee the future in conversational search, hence might result in short-sighted responses.\n\nIn this work, we thus propose a neural Co-Generation model (Co-Gen) to realize global bidirectional semantic associations between the two forms of responses and reduce errors via farsighted optimization. Generally speaking, we construct a shared latent space for various forms of response generation and shape it with two informed priors. Specifically, we treat auto-encoding of structured dialog act and natural response as two separate auxiliary tasks. It inherently captures the generative factors of these responses while the semantic associations between them are globally matched via KL divergence. This allows for concurrent generation of the two forms of responses but does not require synchronization operations as the regularization works in a global way. Moreover, with the shared latent space, we manage to conduct asynchronous reinforcement learning smoothly to foresee the future for better search completion and success rates.\n\nThe contributions of this work can be summarized as follows:\n\n\u2022 We design an interconnected network to co-generate structured and natural responses that allows for bidirectional semantic associations. Being regularized by auto-encoding priors, the network learns meaningful semantic space.\n\n\u2022 We optimize the shared latent space of the variational model using asynchronous reinforcement learning. The task completion rewards promote more intelligent responses. \u2022 Experiments show that the proposed Co-Gen significantly outperforms several state-of-the-art models both on automatic evaluation metrics and human evaluation, leading the board on the total performance in official records 1 .\n\n\nRELATED WORK\n\nResearchers have continuously worked on many crucial problems of conversational search. Dalton et al. [7,8] created benchmarks that track important information in dialogue context and then perform retrieval and ranking processes over candidate responses. Many methods explored this task setting, e.g., to rank related information for current conversational answer by reasoning over a word proximity network [16], or to construct modular components for conversational ranking, including utterance rewriting, related candidate passage retrieval and re-ranking [28]. In this work, we instead focus on the generation angle. In general, structured dialog acts prediction and natural language response generation are closely related in the research of dialogue systems. Hence, our work relates to both the traditional pipe-lined methods and the popular end-to-end based methods. We use the term \"end-to-end\" throughout this paper to emphasize methods that do not require any middle labels or generate any intermediate results such as dialog acts.\n\n\nPipe-lined Response Generation\n\nPipe-lined systems typically require several separated modules: natural language understanding to extract user's intents (e.g. inform) and slot values (e.g. area-center), state tracking to update belief states [22,24,49], policy module to decide the system's next action [50], and natural language generation (NLG) to generate natural language responses. Such separated modules are trained independently with different supervision. More importantly, the policy module is conducted first and then used for the later response generation [25,41,51] to solve problems more efficiently [20,21]. A classic solution employs reinforcement learning (RL) to learn a dialog policy that models the optimal action distribution conditioned on the dialogue state [42]. For traditional modular systems, the action space is defined by hand-crafted semantic representations such as dialog acts [4]. However, it requires that the entire action space can be hand-crafted [33], which cripples a system's ability to handle complex conversations. More importantly, the errors occurred in the policy module will inevitably be accumulated to the later response generation module, which might be misled by the noisy or wrong inputs. Also, such pipe-lined generation manner forces the semantic associations between dialog acts and natural responses to transmit in only one direction, that is from dialog acts to natural response. Models will not be able to leverage knowledge learned from natural response generation to guide the corresponding dialog act prediction. Another line of research efforts rely on language modeling to generate middle results sequentially and are rather close to pipelinelined methods. These models largely benefit from the large pretrained transformer-based models such as BERT [11] and GPT-2 [31]. By connecting dialogue context, intermediate results and response into a long sequence, such systems typically rely on language modeling techniques to directly optimize the data likelihood while neglecting the strategy planning altogether [15,30,44].\n\n\nEnd-to-end Response Generation\n\nResearch in end-to-end response generation is largely inspired by the success of sequence-to-sequence modeling for chit-chat systems. Under this line, various kinds of sequence-to-sequence models have played their important parts in the response generation task, from basic neural recurrent models such as RNN, Bi-LSTM, hierarchical recurrent encoder-decoder to advanced ones such as DialoGPT [48] and prompt-learning [13]. We see great success in applying these models for open domain conversations or chit-chat, however, it is a non-trivial task to transfer such success to goal-oriented application scenarios such as conversational search.\n\nIn the closely connected task-oriented dialogue research community, researchers have managed to combine sequence-to-sequence models with reinforcement learning using task completion rewards [46,47]. Initially, the action space for RL is generally defined as the entire vocabulary such as in [21] where every response output word is considered to be an action selection step. It blows up the size of action space hence the trajectory length, which easily leads to slow and sub-optimal convergence [9,14]. Therefore, Zhao et al. [52] proposed to construct a latent space between the context encoder and the response decoder as the action space. Better performance is obtained. Lubis et al. [26] further leveraged auxiliary tasks to shape the latent variable distribution and Wang et al. [38] modeled the hierarchical structure between the dialogue policy and NLG with the option framework [35]. Better response results have been achieved for these methods partly because they make the learning of 'context-to-natural response' mapping more focused. However, these methods fail to generate structured dialog acts for search optimization. Their latent variables learned are hard to interpret or use under a specific search setting. For example, when something went wrong in the generated natural language responses, it is hard to check the reason relying on latent vectors, while structured dialog acts might give us some useful hints on the contrary. Also, structured dialog acts provide us more convenient information pieces to optimize the search interaction flow, e.g. forming queries.\n\nWe hence resort to generate structured dialog acts and natural responses at the same time to support these functions. Although there are few studies tried to work in this direction such as [39], our work overcomes several limitations such as local semantic associations between various response forms and short-sighted generation etc.\n\n\nMETHOD\n\nWe formally introduce the response co-generation task and our proposed Co-Gen approach as shown in Figure 2. Formally, let = { 1 , 1 , \u00b7 \u00b7 \u00b7 , \u22121 , \u22121 , } denotes the conversation history at turn , where and are the -th turn user and system utterance respectively. We represent the conversation context as the combination of window sized and the state vector 2 . The objective of the co-generation task is to generate the dialog acts sequence = 1 , 2 , \u00b7 \u00b7 \u00b7 , of words and a natural language response = 1 , 2 , \u00b7 \u00b7 \u00b7 , of words based on the context. Generally speaking, the proposed Co-Gen model works as an encoder-decoder framework under the multitask learning setting as shown in Figure 2. The dialog act decoder and the natural response decoder share the latent space , which is shaped by calculating KL-divergence with the informed priors learned via auto-encoding tasks. Such latent vector also enables the adoption of reinforcement learning for further optimization. For more details, we will first give some preliminaries about the general end-to-end RL framework for response generation which is closely related to our design. Then, the shared latent space is introduced with our auto-encoding schemes. Following the description of the co-generation branches for different response forms, we further give details about the asynchronous reinforcement learning.\n\n\nPreliminaries\n\nSince the optimization framework is closely related to the popular end-to-end RL framework [52], we first introduce some preliminary concepts here. In such framework, the response generation is typically performed in two steps: supervised learning (SL) pretraining and RL finetuning. In the SL step, the model learns to generate a response based on the observed conversation context . It updates the network parameters to maximize the log likelihood of the whole training data:\n= E , [ log ( | )].(1)\nAfter achieving a good parameter setting\u02c6via SL, the RL step starts from it and further updates the model parameters w.r.t. the task-specific goal, reflected as a reward. The RL steps usually uses policy gradients, e.g. the REINFORCE algorithm [43]. Suppose a dialogue has turns, for a specific time-step , the discounted return is defined as = = \u2212 , where is the immediate reward for turn and \u2208 [0, 1] is the discount factor. During fine-tuning, the model tries to maximize the expected return from the first time-step onward. The mathematical formulation for such expected return is\nhence = E[ =0 ].\n3.1.1 Word-level RL. For the RL step, the initial word-level methods treat every output word as an action step hence the policy gradient is calculated as:\n\u2207 ( ) = E [ \u2211\ufe01 =0 | | \u2211\ufe01 =0 \u2207 log ( | < , )],\nwhere is the index of each token in the response and denotes the discounted return of the -th token at turn . In this policy gradient form, the action space is the vocabulary size of the system | |, and the trajectory length is = =0 | |, making RL in this space extremely challenging.\n\n\nLatent Action Space RL.\n\nHence, researchers in [26,38,52] introduce a latent variable to factorize the conditional distribution into ( | ) = ( | ) ( | ). By treating the latent space as the action space, correspondingly, the policy gradient becomes:  Context Encoder KL KL Figure 2: Architecture of the proposed Co-Gen for dialog act and natural response co-generation, where act and response generators share the same latent space . Two auto-encoding branches (shaded in green/orange) are treated as auxiliary tasks to regularize the latent space and capture generative factors of responses. The encoder and will be disabled during testing.\n\u2207 ( ) = E [ \u2211\ufe01 =0 \u2207 log ( | )].(2)!\"# !\"# ! % ( ! | ! ) & ( ! ' | ! ) ( ( ! ) | ! ) Dialog\nNote that by introducing such latent variable, the size of action space and the trajectory length are largely reduced. Also, the policy gradient only focuses on the encoder part while the decoder stays intact.\n\n\nShared Latent Space\n\nIn our approach, we also maintain such a latent variable space. Specifically, we construct a context encoder network with parameters , which corresponds to ( | ). We first encode the concatenated conversation history via bidirectional RNN network with GRU cells and global type attention mechanism:\n\n= Bi-GRU( ).\n\nThen, we obtain the context representation as = [ ; ] where is the oracle state vector following the popular setting in [3,38,39] and [; ] denotes the concatenation operation.\n\nSimilar to [26], we assume that the latent variable is sampled from a multivariate Gaussian distribution. Hence, we have ( | ) = N ( | , ). It can be implemented via the reparameterization trick [18] with feed-forward neural networks:\n, = ( ), \u223c N ( , ), = ( ),\nwhere is a context dependent random noise. It is drawn from a normal distribution whose mean and covariance matrix are computed from with feed-forward neural networks. The latent space is shared with the dialog act encoder and the natural response encoder as shown in Figure 2. Basically, we have two auto-encoding streams: \u2192 \u2192 and \u2192 \u2192 . For example, in the former stream, we chose dialog acts sequence autoencoding as the auxiliary task using the variational auto-encoding (VAE) model. That is, given a sequence we train the model to reconstruct the sequence via a latent variable between the encoder and decoder (green shaded box in Figure 2). With an uninformed prior ( ), the pre-training objective for network parameters is:\n( ) = E ( | ) [log ( | )] \u2212 [ ( | )|| ( )],\nwhere ( | ) is the posterior. VAE models have been shown to be able to capture generative aspects of the samples they are trained on, resulting in good interpolation between latent variables [18]. By training a VAE on dialog acts sequences, we aim to capture global generative aspects of dialog acts such as intent and domain information in an unsupervised manner. Similarly, the pre-training objective for network parameters for the \u2192 \u2192 stream (orange shaded box in Figure 2) is:\n( ) = E ( | ) [log ( | )] \u2212 [ ( | )|| ( )].\nIn this stream, we aim to capture global generative aspects of natural responses in an unsupervised way.\n\nTo sum up, we leverage two auto-encoding streams for dialog acts sequence and natural response respectively and learn two VAE latent spaces and in an unsupervised manner. We propose to utilize the VAE latent spaces and to regularize the latent space learned from conversation context, making a shared latent space. Hence, we will use the learned posteriors ( | ) and ( | ) as informed priors for co-generation.\n\n\nCo-generation Branches\n\nWith shared latent space introduced, we now describe the cogeneration mechanism. Generally speaking, we calculate two branches: \u2192 \u2192 and \u2192 \u2192 , where each generation branch is guided by the corresponding learned informed prior globally.\n\nIn \u2192 \u2192 , given a training dataset of { , } pairs, the base optimization method is via stochastic variational inference by maximizing the evidence lowerbound (ELBO) on the data log likelihood:\n( ) = ( | , ) ( | ) \u2212 [ ( | , )|| ( | )],\nwhere ( | ) is realized by the dialog act decoder and ( | , ) is approximated by training a neural encoder network. However, there is a major limitation here: it suffers from exposure bias at Topic 3: Conversational IR SIGIR '22, July 11-15, 2022, Madrid, Spain latent space, i.e. the decoder only sees sampled from ( | , ) and never experiences sampled from ( | ), which is always used at testing time. Therefore, we use the 'lite' version proposed in [52]:\n( ) = ( | ) ( | ) \u2212 [ ( | )|| ( )],\nwhere is a hyper-parameter between 0 and 1. In this version, the posterior network becomes the same as our context encoder ( | ). To regularize the latent space, we directly use the informed prior learned from the auto-encoding pre-training for dialog acts sequence: ( | ). Hence, the final objective for dialog acts sequence generation is:\n_ ( ) = ( | ) ( | ) \u2212 \u00b7 [ ( | )|| ( | )].\nSimilarly, in \u2192 \u2192 , we have the regularized objective for natural response generation as:\n_ ( ) = ( | ) ( | ) \u2212 \u00b7 [ ( | )|| ( | )],\nwhere is another hyper-parameter between 0 and 1. Note that the two informed priors ( | ) and ( | ) are both aligned with the same ( | ). We actually push them to be similar to each other indirectly. In this way, we manage to encourage bi-directional semantic associations between them globally.\n\n\nAsynchronous Reinforcement Learning\n\nWith the aforementioned network structure as described in Figure  2, we can train a relatively good parameter setting using supervised learning. Now starting from a well-trained SL checkpoint, we further fine-tune it via RL to obtain better results. Specifically, we apply reinforcement learning on the shared latent space . The policy gradient is calculated as Equation 2. As discussed, the policy gradient only works on the encoder part which corresponds to our context encoder . We thus further apply REINFORCE to optimize our decoder in an asynchronous manner. For example, in the \u2192 branch, we apply GRU cells to decode word by word sequentially. Denoting the decoder as G, it is responsible for transforming into the detailed response sequence . By treating every output token as an action step, the policy gradient is defined as:\n\u2207 (G) = E G [ \u2211\ufe01 =0 | | \u2211\ufe01 =0 R \u2207 log G( | < , , )],(3)\nwhere | | is the number of tokens in the response at turn and is the token index in the response. R denotes the discounted return of the -th token at turn . Note that here the Equation further incorporates BLEU score as part of the reward. This is different from the Equation 2 using only task completion rewards. The goal of the whole reinforcement learning process is to find the best maximizers that can maximize the reward value regarding both encoder part and the decoder part. The two policies are defined in Equation (2) and (3), respectively. If we synchronously update these two policies, the composite state will be inconsistent before and after the update each time. Consequently, the value does not always monotonically improve during the learning process. It will affect the convergence of both policies. Therefore, we update the two asynchronously during learning. Experiments show that this leads to the convergence of these policies to a local maximizer.\n\n\nEXPERIMENTS 4.1 Datasets\n\nWe conduct experiments on the most widely used conversational search benchmark datasets MultiWoz 2.0 [2] and MultiWoz 2.1 [12] to evaluate our proposed co-generation model. It contains over ten thousand dialogues that spans over seven distinct domains. All the conversations are collected by human-to-human conversations via the crowdsourcing WOZ setting. In which, every conversation is generated where the user is given a pre-defined goal and the system attempts to fulfill the goal by interacting with the user. We follow the same delexicalized method provided by [2] to preprocess the dataset, which is widely applied in other works [38,52]. MultiWoz 2.1 is a modified version of MultiWoz 2.0 which mainly fixes the noisy state annotations and corrects a small fraction of conversation utterances. We follow the public divisions to split the datasets into training, validation and testing sets [2,12].\n\n\nTraining Details\n\nHere we list the specific hyper parameters for Co-Gen model. We set both the maximum length for the user's utterance in context and the maximum length for the system's utterance in response to 50. The embedding size for each word is set to 100. For the input conversation history, target dialog acts sequence and natural response, all these encoders are one-layer bidirectional RNN that uses GRU cells of size 300. The encoded result is projected to the latent content space, where the size of the shared latent variable is 200. For decoders, they are also one-layer RNN with a separate embedding layer and GRU cells.\n\nDuring SL training, we set batch size as 32 and the maximum number of training epochs as 50. Adam optimizer is used with an initial learning rate of 0.001 and weight decay 1e-05. The KL divergence hyper-parameters and are all set to be 1.0 empirically in our experiments. After supervised training of the model, we further fine-tune the model with asynchronous RL. In which, each conversation is evaluated with the goal (e.g. calculating the Success rate) and the BLEU scores. We fix each batch as a complete dialogue. Stochastic gradient descent (SGD) is used, and the learning rate in the asynchronous optimization process for the two policies is both 0.09 with weight decay 1e-05. Generally speaking, the experiments of Co-Gen were run on a Nvidia GeForce RTX 2080Ti graphic card, which consumed around 2.5 hours for SL training and less than 1 hours for RL finetuning. Hence, it is not very expensive to reproduce our results as shown in Table 1.\n\n\nTest Settings\n\nExperiments are conducted on the context-to-response generation task similar to the one originally proposed in [2]. Given the conversation context, the model is trained to generate appropriate responses in each turn. In our proposed model, both the structured dialogue acts and the natural language response will be generated and evaluated. What's more, as the proposed model constructs a shared latent space and leverages the auto-encoding scheme to further regularize the space, we will also demonstrate whether the learned semantic space is meaningful or not. \n\n\nEvaluation Metric\n\nFollowing existing works, we adapt three automatic metrics measured in percentage to evaluate the generated natural responses from a conversational search system such as Inform rate, Success rate and BLEU score. Inform rate measures whether the system has provided the correct entity (e.g., the name of restaurant). Success rate shows the ratio of correct answers provided for request slots in the generated utterances. The fluency of the generated response is measured by BLEU [29] score. The combined Score is computed as ( + 0.5 \u00d7 ( + )) [2] to fairly evaluate the performance of a dialogue system as a popular total score. To evaluate the generated structured dialog acts, we adopt Entity F1 [41] to evaluate the entity coverage accuracy (including all slot values, days, numbers, and reference, etc). We also adopt the Act F1 from [3,39] to judge the act coverage accuracy such as domain, action and slots.\n\nFurthermore, we carry out human evaluation to measure the quality of generated responses. We add three criteria as follows: Fluency measures whether the generated response is fluent, grammatically correct and smooth; Coherence reflects how coherent the response is and whether it follows the conversation flow; Informativeness shows whether the response provides relevant and useful information to the user. We provide detailed descriptions to the human evaluators during evaluation.\n\n\nBaseline Models\n\nWe denote the variation of the proposed Co-Gen as Co-Gen_ , which corresponds to the proposed model without RL finetuning. They are compared with the following models: SFN [27], UBAR [44], HDSA [40], DialoGPT [48], LaRL [52], LAVA [26] , HDNO [38] and MarCo [39]. These baselines can be organized into three groups, i.e., pipe-lined, end-to-end and co-generation methods. All these models leverage oracle dialogue states. More details about these methods are given below:\n\n-SFN [27]: It learns neural dialogue modules corresponding to the structured components of traditional dialogue systems. It obtains strong results both with (denoted as SFN) and without reinforcement learning (SFN_ ). -UBAR [44]: It is based on fine-tuning the pretrained GPT-2 where a whole dialogue session is treated as a single training sequence. The sequence is composed of user utterance, belief state, database result, system act, and system response of every dialog turn. -HDSA [3]: It is a two-stage model that uses BERT to predict a one hot dialog act vector for guiding the following response generation task. The structure of dialog acts is modeled as a multi-layer hierarchical graph. -DialoGPT [48]: It is also based on fine-tuning the pretrained GPT-2 but it only focuses on the context-to-response mapping where all middle labels are ignored. Also, Co-Gen shows balanced results over both the strategy learning for task completion and surface style realization. The former is evaluated by Inform rate and Success rate, while the latter is evaluated by the BLEU score. Specifically, we observe a general trend that RL applied methods can largely boost the strategy part as expected, because the task completion rates are directly considered as rewards. For example, in pipeline-based methods, the RL applied SFN outperforms its SL counterparts, especially in task completion metrics like Inform rate and Success rate. This is also true in end-to-end RL based methods such as HDNO. Hence, in the proposed Co-Gen model, RL rewards are also incorporated, which enables the model to fore-see more turns in a trial-and-error manner. This also makes it possible for the model to generate more intelligent responses. Moreover, we observe that for RL-applied methods, overemphasizing task completion may often lead to corrupted responses. For example, on MultiWoz2.0, though LAVA achieves a leading high score in Inform and Success rate, it only obtains a low BLEU score of 12.02. In comparison, Co-Gen obtains more balanced results with reasonably high Inform and Success rate and more natural responses reflected by its 20.42 BLEU score. For methods that do not apply RL such as MarCo and HDSA, their performance results are relatively lower, especially regarding the task completion metrics. Moreover, for language modeling based methods, the results show that how to model the task is the key to achieve good performance via the powerful large-scale pretraining models such as GPT-2. For example, the DialoGPT model does not perform well and there are large performance gaps between DialoGPT and UBAR. Because the modeling of the later is more for task-oriented dialogues and various intermediate labels are involved. Although UBAR manages to achieve high combined Scores, the lack of foreseeing the future is still a main shortcoming for such models.\n\n\nResults on Structured\n\nResponses. Co-Gen also manages to generate structured dialogue acts sequences, where each action is organized as a (domain, action, slot) tuple. We separately check Act F1 score for dialog acts and the Entity F1 for the slot values. The results on MultiWoz 2.0 are listed in Table 2, where BiLSTM, Word-CNN and Transformer are baselines from [3]. There are mainly three groups of methods. The first group 'Act Prediction Only' means  these methods are specifically trained to do dialog act prediction as classification task while cannot generate natural responses. The second 'Pipe-lined' group are all pipe-lined methods which generates both, but in sequential way. While 'Co-generate' means methods generate both concurrently. Since the end-to-end RL based methods do not have such outputs, we skip the comparison. As shown in Table 2, methods in the 'Act Prediction Only' group obtain relatively low performances while the Transformer based one performs a bit better among them. As they only work on (domain, action, slot) tuples, they do not have Entity F1 results. For pipe-line methods, UBAR performs the best. This might be due to that it gains learning capability from the powerful GPT-2 model. However, its Entity F1 score is largely inflated, as it does not differentiate the values of different domains. For example, hotel_area, restaurant_area, attraction_area are same for UBAR but different for other models. Hence, the high score 82.3 is not comparable. Besides these, the proposed model Co-Gen works the best across Act F1 and Entity F1. These demonstrate that the proposed Co-Gen method not only generates good natural responses, but also generates the structured dialog acts well.  Figure 3, we visually assess the latent content space by first clustering the latent content vector of each system response in the testing 3 The evaluation script of UBAR does not differentiate entity domains hence largely inflates the result. This number is thus not comparable to others. set into six clusters, and then projecting them with t-SNE [37] to analyze the formed clusters. Through inspecting the randomly selected system utterances as shown in the right hand side, we find that the clusters of latent content vectors of both Co-Gen and HDNO possess some semantic meanings. For example, the cluster in blue dots in Co-Gen is related to train booking and the cluster in yellow dots is related to restaurant recommendation, while the cluster in brown dots in HDNO is related to the general phrases for goodbye at the end of service. However, it is also obvious that the clusters from Co-Gen as shown in Figure 3 (a) are relatively better separated, which demonstrates clearer semantic meanings expressed by these latent vectors. This might be due to the successful shaping effect of the pre-trained informed priors from auto-encoding tasks for dialog acts sequence and natural response.\n\n\nHuman Evaluation\n\nWe conduct a human evaluation by recruiting eight undergraduate students as participants to perform two pairs of comparison from the human perspective between the responses generated by Co-Gen vs. HDNO and Co-Gen vs. MarCo. For each pair of response comparison, We randomly sample 150 samples from the testing dataset. During the evaluation, each conversation sample is presented to the participants with the user utterance, ground-truth response as the reference, and two generated responses from Co-Gen and the counterpart model separately. The participants are unaware of the source model for the generated responses to ensure a fair comparison. The ranking is based on three criteria: (i) fluency: the response is grammatically correct, natural, and smooth. (ii) coherence: the response is coherent and follows the flow of the dialogue reasonably.\n\n(iii) informativeness: the response provided related information to solve the user's requests and complete the task. After gathering the replies, the calculated statistics are shown in Figure 4, where the \"Win\", \"Tie\", and \"Lose\" parts in the stacked bars represents the proportions of Co-Gen outperforms, ties with, and loses to its counterparts under each criterion. From the results, we can observe that the generated responses by Co-Gen outperforms HDNO in all three aspects, indicating its strong capability in correctly inferring system action to fulfill user requests and generating human-like responses. In the second comparison, we note that   Co-Gen obtains higher winning percentages than MarCo in both fluency and coherence while under-performing on informativeness.\n\nWe further analyzed the bad cases of our model on informativeness in this pair comparison, and find that both model can provide enough information to solve user requests, while MarCo sometimes emphasizes some repeated information that has appeared in previous turns' responses. Such generation preferrence helps it to be more informative, but also results in token-level repetition and conversation level incoherence. In all, our model Co-Gen can deduce more accurate system actions that truly figure out the user request and provide an appropriate response accordingly.\n\n\nExample Responses Generated\n\nTo get a better sense of how these methods perform, we showcase some system utterances generated in the same dialogue by different baselines and our proposed Co-Gen. Since most of the dialogues in MultiWoz 2.0 and MultiWoz 2.1 are similar, we only show some results on MultiWoz 2.0. As listed in Table 3, the generated utterances of Co-Gen is apparently more fluent and task completion oriented. For example, it manages to keep the whole dialogue on the topic of college recommendation and successfully book train tickets for the user during the conversation. In more details, for those methods fine-tuned with RL, Co-Gen is able to foresee the future better for response generation than other baselines such as HDNO and LAVA. This is evidenced by the response on 'no colleges' in the first turn, while other models such as HDNO and LAVA mention 'swimming pools' and 'multiple sports attractions' instead. This is also evidenced in the second turn that Co-Gen manages to generate 'free to get in' which corresponds to 'entrance fee' asked in the subsequent turn. Moreover, in comparison with other baselines trained with RL, the generated utterances of Co-Gen is more stringent in generating slots. Especially, LAVA tends to generate as many slots as possible to increase the success rate. e.g. generating the extra [attraction_address] in the third turn. This is the common issue of most RL methods on task-oriented dialogue system. However, in Co-Gen the situation is better. This might be due to our asynchronous RL optimization scheme where task completion goals and surface generation goals are separately optimized in an iterative fashion. The improved BLEU score in Table 1 for Co-Gen also demonstrates this.\n\nFor methods without RL fine-tuning, it is interesting that we also observe the phenomenon of over-generating slot placeholders in UBAR generated responses, such as the one in the second turn. is contained. Since UBAR purely relies on the powerful language modeling GPT-2 model and does not leverage RL, this might be due to the context seen in former turns. Such phenomenon is also observed in the co-generation baseline MarCo, which also tends to give redundant information.\n\n\nCONCLUSION\n\nIn conclusion, we proposed a neural co-generation framework for generating structured dialog acts and natural language responses concurrently for conversational search systems. It roots from a shared latent space that is shaped by two informed prior distributions. Accordingly, we formed two auto-encoding branches for structured dialog acts and natural responses as two auxiliary tasks to capture the generative factors of them. The joint training in an interconnected network structure makes the learned latent variables possess well-separated semantic meanings. Furthermore, we designed an asynchronous reinforcement learning mechanism to fine-tune the network with long-term rewards, which enables the model to foresee the future for better search completion and success rates. We carried out extensive experiments on two public datasets in comparison with a wide range of baselines. Both automatic and human evaluation are involved. The superior performance demonstrates that the proposed Co-Gen model generates better responses in both forms.\n\nIn the future, we look forward to applying our method for personalized response generation in conversational search when more such data is available. We would also like to further improve the strategy planning part in handling conversational search situations unseen during training and analyze how our model performs in real conversation interaction with more unseen situations.\n\nFigure 3 :\n3The shared latent content vectors of Co-Gen and HDNO clustered in six categories visualized via the T-SNE algorithm. We randomly show three turns of system utterances for each cluster (best view in color).\n\nFigure 4 :\n4Results of human evaluation in response quality. Two pairs of comparison have been conducted, where the first figure presents the results of Co-Gen vs. HDNO, and the second figure corresponds to Co-Gen vs. MarCo.\n\n\n-LaRL[52]: This model is the first to represent dialogue act as latent vectors in task-oriented dialogues. During the RL training, it only updates the corresponding policy part while the decoding part is not involved in RL training.-LAVA[26]: Built upon LaRL, it further leverages three auxiliary tasks to shape the latent variable distribution, making the latent representations truly encodes the characteristics of different actions. -HDNO[38]: It adopts the option framework[35] to model the hierarchical relation between dialogue policy and NLG.No middle results are required. We also report its SL only version as HNDO_ . -MarCo[39]: It co-generates dialog acts and natural responses as two sequence generation tasks while uses dynamic attention to capture local semantic associations.4.6 Automatic Evaluation Results \n\n4.6.1 Results on Natural Language Responses. The main results \nfor natural response generation are shown in Table 1. The pro-\nposed Co-Gen method achieves the best performance regarding \nthe overall performance reflected by the combined Scores. It sig-\nnificantly outperforms all the comparing methods. For example, \nCo-Gen improves the best performing baseline HDNO by 2.17% in \nthe MultiWoz 2.0 dataset and 1.31% in the MultiWoz 2.1 dataset. \n\n\nTable 1 :\n1Overall natural language response results on MultiWoz 2.0 and MultiWoz 2.1.Group \nMethod \nMultiWoz 2.0 \nMultiWoz 2.1 \n\nInform Success BLEU Score Inform Success BLEU Score \n\nPipe-lined \n\nSFN_ \n90.00 \n74.20 \n18.35 \n100.45 \n63.10 \n53.10 \n17.56 \n75.66 \nSFN \n94.40 \n83.10 \n16.34 \n105.09 \n87.80 \n76.20 \n10.57 \n92.57 \nUBAR \n94.00 \n83.60 \n17.22 \n106.02 \n89.6 \n78.6 \n17.34 \n101.44 \nHDSA \n82.90 \n68.90 \n23.60 \n99.50 \n86.30 \n70.60 \n22.36 \n100.81 \n\nEnd-to-End \n\nDialoGPT \n73.40 \n48.00 \n12.16 \n72.86 \n72.10 \n50.10 \n12.62 \n73.72 \nLaRL \n93.49 \n84.98 \n12.01 \n101.25 \n92.39 \n85.29 \n13.72 \n102.56 \nLAVA \n97.50 \n94.80 \n12.02 \n108.17 \n96.39 \n83.57 \n14.02 \n104.00 \nHDNO_ \n78.60 \n70.40 \n19.26 \n93.76 \n78.80 \n66.70 \n18.46 \n91.21 \nHDNO \n95.80 \n84.50 \n18.61 \n108.76 \n93.20 \n81.90 \n18.35 \n105.90 \n\nCo-Generate \n\nMarCo \n92.30 \n78.60 \n20.02 \n105.47 \n92.50 \n77.80 \n19.54 \n104.69 \nCo-Gen_ \n92.10 \n77.40 \n20.91 \n105.66 \n88.90 \n80.00 \n20.67 \n105.12 \nCo-Gen (ours) \n94.70 \n86.70 \n20.42 111.12 \n91.20 \n85.20 \n19.09 107.29 \n\n\n\nTable 2 :\n2Structured response generation results on MultiWoz 2.0. Note that the end-to-end group methods fail to generate such responses, hence no results are shown here.Group \nMethod \nAct F1 Entity F1 \n\nAct Prediction Only \n\nBiLSTM \n71.4 \nNA \nWord-CNN \n71.5 \nNA \nTransformer \n73.1 \nNA \n\nPipe-lined \n\nSFN \n63.7 \n77.1 \nUBAR \n84.6 \n82.3 3 \nHDSA \n77.3 \n65.7 \n\nCo-Generate \nMarCo \n73.9 \n59.9 \nCo-Gen \n87.6 \n77.2 \n\n4.6.3 Semantic Meanings of the Shared Latent Space. As shown in \n\n\nTable 3 :\n3[attraction_name] is a nightclub in the [value_area]. it s free to get in. HDNO: i am sorry, there are no swimming pools in the [value_area]. would you like me to look in a different area? LAVA: i am sorry, there are no multiple sports attractions in the [value_area]. would you like to try another area? UBAR: i am sorry, there are no [value_type] in the [value_area]. would you like to try a different area? User: no museums, what colleges are there around town? GT Sys: sure, there are [value_count] colleges in that area. Co-Gen: there are [value_count] colleges in the [value_area]. i recommend the [attrac-tion_name]. it s free to get in. MarCo: there are [value_count] colleges in the [value_area]. [attraction_name] is located at [attraction_address] and has free admission. HDNO: [attraction_name] is a great place. would you like me to book a ticket for you? LAVA: there are [value_count] colleges in the [value_area]. i recommend [attrac-tion_name]. would you like more information? UBAR: there are [value_choice] [value_type] in the [value_area]. [value_name] on [value_address] is [value_price] and [value_name] on [value_address] is [value_price]. User: can i please get the phone number, postcode and entrance fee? GT Sys: [attraction_name] is [value_count] i would recommend, their phone number is [attraction_phone], post code is [attraction_postcode], and there is no entrance fee. can i help you with anything else? Co-Gen: [attraction_name] is a great college in the [value_area]. the phone number is [attraction_phone] and the postcode is [attraction_postcode]. MarCo: [attraction_name] is located at [attraction_address], postcode [attrac-tion_postcode], and their phone number is [attraction_phone]. the entrance fee is [value_count] pounds. HDNO: the phone number is [attraction_phone]. the postcode is [attraction_postcode]. is there anything else i can help you with? LAVA: the [attraction_name] is located at [attraction_address], postcode [attrac-tion_postcode]. the phone number is [attraction_phone]. UBAR: the phone number is [value_phone]. the postcode is [value_postcode] and the entrance fee is [value_price]. User: i also need a train that leaves on [value_day] and arrives by [value_time]. GT Sys: where are you traveling from? there are [value_count] trains from [value_place] that arrive by that time. Co-Gen: there are several trains that meet your criteria. where are you departing from and what is your destination? MarCo: there are [value_count] trains leaving from [value_place] to [value_place]. where would you like to depart from and arrive ? HDNO: where will you be departing from? LAVA: i have [value_count] trains that meet your criteria. would you like to narrow it down by departure site or arrival time? UBAR: where will you be departing from? User: i am leaving [value_place] and going to [value_place]. GT Sys: the first train leaves at [value_time]. Co-Gen: i have train [train_id] that leaves at [value_time] and arrives at [value_time]. would you like me to book it? MarCo: [train_id] leaves at [value_time]. would you like me to book that for you ? HDNO: [train_id] leaves at [value_time]. would you like me to book a ticket for you? LAVA: i have train [train_id] that leaves at [value_time] and arrives at [value_time]. would you like me to book that for you? UBAR: [value_id] leaves at [value_leave] and arrives at [value_arrive]. would you like me to book that for you? User: can you book [value_count] tickets? GT Sys: your reservation has been made. the total fee is [value_price] gbp payable at the station. reference number is [train_reference]. Co-Gen: booking was successful, the total fee is [value_price] gbp payable at the station. reference number is [train_reference]. can i help with anything else? MarCo: i have booked you [value_count] seats on the [train_id]. the total fee is [value_price] gbp payable at the station. your reference number is [train_reference]. HDNO: booking was successful, the total fee is [value_price] gbp payable at the station. reference number is [train_reference]. is there anything else i can help you with? LAVA: booking was successful, the total fee is [value_price] gbp payable at the station. reference number is [train_reference]. UBAR: booking was successful, the total fee is [value_price] payable at the station. reference number is: [value_reference]. can i help you with anything else today?Some delexicalized responses generated by the base-\nlines and Co-Gen on MultiWoz 2.0. \n\nMethod \nDelexicalized Dialogue \n\nUser: \ni am looking to go somewhere in the [value_area] to enjoy some en-\ntertainment. can you help? \nGT Sys: \ni cannot find any entertainment venue -s in the [value_area]. would \nyou like to try something different ? a museum perhaps? \nCo-Gen: \nthere are no colleges in the [value_area]. would you like to try a different area \nor attraction type? \nMarCo: \n\n\n\nIt generates [value_choice], [value_type], [value_area], [value_name], [value_address] and [value_price] in a single turn, which is rather different from the ground truth response where only one [value_count]\nhttps://github.com/budzianowski/multiwoz\nFollowing the popular setting in[3,38,39], we consider context window 2 for dialogue history and use the ground truth state vector because state tracking is not our focus.\nACKNOWLEDGMENTSThis research was supported by the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant, and the Sea-NExT Joint Lab.\nConversational search (dagstuhl seminar 19461). Avishek Anand, Lawrence Cavedon, Hideo Joho, Mark Sanderson, Benno Stein, In Dagstuhl Reports. 9Avishek Anand, Lawrence Cavedon, Hideo Joho, Mark Sanderson, and Benno Stein. 2020. Conversational search (dagstuhl seminar 19461). In Dagstuhl Reports, Vol. 9.\n\nMultiWOZ-A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gasic, Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018. MultiWOZ-A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. In EMNLP. 5016-5026.\n\nSemantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention. Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang, ACL. Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, and William Yang Wang. 2019. Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention. In ACL. 3696-3709.\n\nUnsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. Yun-Nung Chen, William Yang Wang, Alexander I Rudnicky, Yun-Nung Chen, William Yang Wang, and Alexander I Rudnicky. 2013. Unsuper- vised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. In ASRU. 120-125.\n\nWhat makes a good conversation? Challenges in designing truly conversational agents. Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego Garaialde, Justin Edwards, Brendan Spillane, Emer Gilmartin, Christine Murad, Cosmin Munteanu, CHI. Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego Garaialde, Justin Edwards, Brendan Spillane, Emer Gilmartin, Christine Murad, Cosmin Munteanu, et al. 2019. What makes a good conversation? Challenges in designing truly conversational agents. In CHI. 1-12.\n\nUser attention-guided multimodal dialog systems. Chen Cui, Wenjie Wang, Xuemeng Song, Minlie Huang, Xin-Shun Xu, Liqiang Nie, SIGIR. Chen Cui, Wenjie Wang, Xuemeng Song, Minlie Huang, Xin-Shun Xu, and Liqiang Nie. 2019. User attention-guided multimodal dialog systems. In SIGIR. 445-454.\n\nCAsT 2020: The Conversational Assistance Track Overview. Jeffrey Dalton, Chenyan Xiong, Jamie Callan, TREC. Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. CAsT 2020: The Conver- sational Assistance Track Overview. In TREC.\n\nTREC CAsT 2019: The Conversational Assistance Track Overview. Jeffrey Dalton, Chenyan Xiong, Jamie Callan, ArXiv abs/2003.13624Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. TREC CAsT 2019: The Conversational Assistance Track Overview. ArXiv abs/2003.13624 (2020).\n\nLearning cooperative visual dialog agents with deep reinforcement learning. Abhishek Das, Satwik Kottur, M F Jos\u00e9, Stefan Moura, Dhruv Lee, Batra, Abhishek Das, Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra. 2017. Learning cooperative visual dialog agents with deep reinforcement learning. In ICCV. 2951-2960.\n\nTowards multimodal conversational information seeking. Yashar Deldjoo, Johanne R Trippas, Hamed Zamani, SIGIR. 21Yashar Deldjoo, Johanne R Trippas, and Hamed Zamani. 2021. Towards multi- modal conversational information seeking. In SIGIR, Vol. 21.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. 4171-4186.\n\nMihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, arXiv:1907.01669Shuyang Gao, and Dilek Hakkani-Tur. 2019. MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines. cs.CLMihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur. 2019. MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines. arXiv:1907.01669 [cs.CL]\n\nResponse Generation with Context-Aware Prompt Learning. Xiaodong Gu, Sang-Woo Kang Min Yoo, Lee, arXiv:2111.02643arXiv preprintXiaodong Gu, Kang Min Yoo, and Sang-Woo Lee. 2021. Response Generation with Context-Aware Prompt Learning. arXiv preprint arXiv:2111.02643 (2021).\n\nDecoupling Strategy and Generation in Negotiation Dialogues. He He, Derek Chen, Anusha Balakrishnan, Percy Liang, EMNLP. He He, Derek Chen, Anusha Balakrishnan, and Percy Liang. 2018. Decoupling Strategy and Generation in Negotiation Dialogues. In EMNLP. 2333-2343.\n\nEhsan Hosseini-Asl, Bryan Mccann, Chien-Sheng Wu, arXiv:2005.00796Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. arXiv preprintEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. arXiv preprint arXiv:2005.00796 (2020).\n\nMagdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum, Conversational Question Answering over Passages by Leveraging Word Proximity Networks. SIGIR. Magdalena Kaiser, Rishiraj Saha Roy, and Gerhard Weikum. 2020. Conversational Question Answering over Passages by Leveraging Word Proximity Networks. SIGIR.\n\nToward voice query clarification. Johannes Kiesel, Arefeh Bahrami, Benno Stein, Avishek Anand, Matthias Hagen, SIGIR. Johannes Kiesel, Arefeh Bahrami, Benno Stein, Avishek Anand, and Matthias Hagen. 2018. Toward voice query clarification. In SIGIR. 1257-1260.\n\nP Diederik, Max Kingma, Welling, arXiv:1312.6114Auto-encoding variational bayes. arXiv preprintDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\n\nInteractive path reasoning on graph for conversational recommendation. Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, Tat-Seng Chua, SIGKDD. Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, and Tat-Seng Chua. 2020. Interactive path reasoning on graph for conver- sational recommendation. In SIGKDD. 2073-2083.\n\nDeal or No Deal? End-to-End Learning of Negotiation Dialogues. Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, Dhruv Batra, Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. 2017. Deal or No Deal? End-to-End Learning of Negotiation Dialogues. In EMNLP. 2443-2453.\n\nDeep Reinforcement Learning for Dialogue Generation. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, EMNLP. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016. Deep Reinforcement Learning for Dialogue Generation. In EMNLP. 1192-1202.\n\nDialogue State Tracking with Incremental Reasoning. Lizi Liao, Le Hong Long, Yunshan Ma, Wenqiang Lei, Tat-Seng Chua, TACL. 9Lizi Liao, Le Hong Long, Yunshan Ma, Wenqiang Lei, and Tat-Seng Chua. 2021. Dialogue State Tracking with Incremental Reasoning. TACL 9 (2021), 557-569.\n\nMMConv: An Environment for Multimodal Conversational Search across Multiple Domains. Lizi Liao, Le Hong Long, Zheng Zhang, Minlie Huang, Tat-Seng Chua, SIGIR. Lizi Liao, Le Hong Long, Zheng Zhang, Minlie Huang, and Tat-Seng Chua. 2021. MMConv: An Environment for Multimodal Conversational Search across Multi- ple Domains. In SIGIR. 675-684.\n\nMulti-domain dialogue state tracking with recursive inference. Lizi Liao, Tongyao Zhu, Le Hong Long, Tat Seng Chua, Lizi Liao, Tongyao Zhu, Le Hong Long, and Tat Seng Chua. 2021. Multi-domain dialogue state tracking with recursive inference. In WWW. 2568-2577.\n\nAn end-to-end trainable neural network model with belief tracking for task-oriented dialog. Bing Liu, Ian Lane, Bing Liu and Ian Lane. 2017. An end-to-end trainable neural network model with belief tracking for task-oriented dialog. In INTERSPEECH. 2506-2510.\n\nLAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue Policy Optimization. Nurul Lubis, Christian Geishauser, Michael Heck, Hsien-Chin Lin, Marco Moresi, Milica Carel Van Niekerk, Gasic, COLING. Nurul Lubis, Christian Geishauser, Michael Heck, Hsien-chin Lin, Marco Moresi, Carel van Niekerk, and Milica Gasic. 2020. LAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue Policy Optimization. In COLING. 465- 479.\n\nStructured Fusion Networks for Dialog. Shikib Mehri, Tejas Srinivasan, Maxine Eskenazi, SIGDIAL. Shikib Mehri, Tejas Srinivasan, and Maxine Eskenazi. 2019. Structured Fusion Networks for Dialog. In SIGDIAL. 165-177.\n\nTopic propagation in conversational search. Ida Mele, Cristina Ioana Muntean, Maria Franco, Raffaele Nardini, Nicola Perego, Ophir Tonellotto, Frieder, SIGIR. Ida Mele, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, and Ophir Frieder. 2020. Topic propagation in conversational search. In SIGIR. 2057-2060.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, ACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL. 311-318.\n\nBaolin Peng, Chunyuan Li, Jinchao Li, arXiv:2005.05298Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained Auto-regressive Model. arXiv preprintBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained Auto-regressive Model. arXiv preprint arXiv:2005.05298 (2020).\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 19Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\n\nA theoretical framework for conversational search. Filip Radlinski, Nick Craswell, CHIIR. Filip Radlinski and Nick Craswell. 2017. A theoretical framework for conversa- tional search. In CHIIR. 117-126.\n\nSample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management. Pei-Hao Su, Pawe\u0142 Budzianowski, Stefan Ultes, Milica Gasic, Steve Young, SIGDIAL. Pei-Hao Su, Pawe\u0142 Budzianowski, Stefan Ultes, Milica Gasic, and Steve Young. 2017. Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management. In SIGDIAL. 147-157.\n\nConversational recommender system. Yueming Sun, Yi Zhang, SIGIR. Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In SIGIR. 235-244.\n\nBetween MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 112Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence 112, 1-2 (1999), 181-211.\n\nTowards a model for spoken conversational search. Damiano Johanne R Trippas, Paul Spina, Mark Thomas, Hideo Sanderson, Lawrence Joho, Cavedon, Information Processing & Management. 2102162Johanne R Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo Joho, and Lawrence Cavedon. 2020. Towards a model for spoken conversational search. Information Processing & Management 2 (2020), 102162.\n\nVisualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 911Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008).\n\nModelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System. Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu, ICLR. Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. 2020. Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System. In ICLR.\n\nMulti-Domain Dialogue Acts and Response Co-Generation. Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu, ACL. Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, and Jianxing Yu. 2020. Multi- Domain Dialogue Acts and Response Co-Generation. In ACL. 7125-7134.\n\nSemantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems. Milica Tsung-Hsien Wen, Nikola Gasic, Pei-Hao Mrk\u0161i\u0107, David Su, Steve Vandyke, Young, Tsung-Hsien Wen, Milica Gasic, Nikola Mrk\u0161i\u0107, Pei-Hao Su, David Vandyke, and Steve Young. 2015. Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems. In EMNLP. 1711-1721.\n\nA Network-based End-to-End Trainable Task-oriented Dialogue System. David Tsung-Hsien Wen, Nikola Vandyke, Milica Mrk\u0161i\u0107, Lina M Rojas Gasic, Pei-Hao Barahona, Stefan Su, Steve Ultes, Young, Tsung-Hsien Wen, David Vandyke, Nikola Mrk\u0161i\u0107, Milica Gasic, Lina M Rojas Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A Network-based End-to-End Trainable Task-oriented Dialogue System. In EACL. 438-449.\n\nPartially observable Markov decision processes for spoken dialog systems. D Jason, Steve Williams, Young, Computer Speech & Language. 21Jason D Williams and Steve Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech & Language 21, 2 (2007), 393-422.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 8Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3 (1992), 229-256.\n\nUBAR: Towards Fully End-to-End Task-Oriented Dialog System with GPT-2. Yunyi Yang, Yunhao Li, Xiaojun Quan, AAAI. Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2021. UBAR: Towards Fully End-to- End Task-Oriented Dialog System with GPT-2. In AAAI. 14230-14238.\n\nMacaw: An extensible conversational information seeking platform. Hamed Zamani, Nick Craswell, SIGIR. Hamed Zamani and Nick Craswell. 2020. Macaw: An extensible conversational information seeking platform. In SIGIR. 2193-2196.\n\nEvaluating conversational recommender systems via user simulation. Shuo Zhang, Krisztian Balog, SIGKDD. Shuo Zhang and Krisztian Balog. 2020. Evaluating conversational recommender systems via user simulation. In SIGKDD. 1512-1520.\n\nTowards conversational search and recommendation: System ask, user respond. Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, W Bruce Croft, CIKM. Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W Bruce Croft. 2018. Towards conversational search and recommendation: System ask, user respond. In CIKM. 177-186.\n\nDIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, William B Dolan, Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and William B Dolan. 2020. DIALOGPT: Large- Scale Generative Pre-training for Conversational Response Generation. In ACL. 270-278.\n\nNeural multimodal belief tracker with adaptive attention for dialogue systems. Zheng Zhang, Lizi Liao, Minlie Huang, Xiaoyan Zhu, Tat-Seng Chua, Zheng Zhang, Lizi Liao, Minlie Huang, Xiaoyan Zhu, and Tat-Seng Chua. 2019. Neural multimodal belief tracker with adaptive attention for dialogue systems. In WWW. 2401-2412.\n\nLearning Goal-oriented Dialogue Policy with Opposite Agent Awareness. Zheng Zhang, Lizi Liao, Xiaoyan Zhu, Tat-Seng Chua, Zitao Liu, Yan Huang, Minlie Huang, arXiv:2004.09731arXiv preprintZheng Zhang, Lizi Liao, Xiaoyan Zhu, Tat-Seng Chua, Zitao Liu, Yan Huang, and Minlie Huang. 2020. Learning Goal-oriented Dialogue Policy with Opposite Agent Awareness. arXiv preprint arXiv:2004.09731 (2020).\n\nTowards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning. Tiancheng Zhao, Maxine Eskenazi, SIGDIAL. Tiancheng Zhao and Maxine Eskenazi. 2016. Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning. In SIGDIAL. 1-10.\n\nRethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models. Tiancheng Zhao, Kaige Xie, Maxine Eskenazi, Tiancheng Zhao, Kaige Xie, and Maxine Eskenazi. 2019. Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models. In NAACL. 1208-1218.\n", "annotations": {"author": "[{\"end\":360,\"start\":124},{\"end\":592,\"start\":361},{\"end\":822,\"start\":593},{\"end\":1045,\"start\":823},{\"end\":1256,\"start\":1046},{\"end\":1465,\"start\":1257},{\"end\":1674,\"start\":1466},{\"end\":1880,\"start\":1675},{\"end\":2088,\"start\":1881}]", "publisher": null, "author_last_name": "[{\"end\":135,\"start\":133},{\"end\":370,\"start\":366},{\"end\":602,\"start\":598},{\"end\":829,\"start\":827},{\"end\":1057,\"start\":1055},{\"end\":1266,\"start\":1262},{\"end\":1475,\"start\":1471},{\"end\":1681,\"start\":1679}]", "author_first_name": "[{\"end\":132,\"start\":124},{\"end\":365,\"start\":361},{\"end\":597,\"start\":593},{\"end\":826,\"start\":823},{\"end\":1054,\"start\":1046},{\"end\":1261,\"start\":1257},{\"end\":1470,\"start\":1466},{\"end\":1678,\"start\":1675},{\"end\":1889,\"start\":1881}]", "author_affiliation": "[{\"end\":359,\"start\":163},{\"end\":591,\"start\":395},{\"end\":821,\"start\":625},{\"end\":1044,\"start\":848},{\"end\":1255,\"start\":1059},{\"end\":1464,\"start\":1268},{\"end\":1673,\"start\":1477},{\"end\":1879,\"start\":1683},{\"end\":2087,\"start\":1891}]", "title": "[{\"end\":87,\"start\":1},{\"end\":2175,\"start\":2089}]", "venue": "[{\"end\":2300,\"start\":2177}]", "abstract": "[{\"end\":4084,\"start\":2798}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4220,\"start\":4217},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4223,\"start\":4220},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4226,\"start\":4223},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4229,\"start\":4226},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4232,\"start\":4229},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4364,\"start\":4361},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4366,\"start\":4364},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4369,\"start\":4366},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4820,\"start\":4816},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4823,\"start\":4820},{\"end\":5136,\"start\":5115},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5307,\"start\":5303},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5310,\"start\":5307},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5313,\"start\":5310},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5316,\"start\":5313},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5898,\"start\":5894},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5901,\"start\":5898},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5904,\"start\":5901},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5933,\"start\":5929},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6132,\"start\":6128},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6135,\"start\":6132},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6153,\"start\":6149},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6226,\"start\":6222},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6300,\"start\":6296},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6992,\"start\":6988},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9273,\"start\":9270},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9275,\"start\":9273},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9579,\"start\":9575},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9730,\"start\":9726},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10457,\"start\":10453},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10460,\"start\":10457},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10463,\"start\":10460},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10518,\"start\":10514},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10782,\"start\":10778},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10785,\"start\":10782},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10788,\"start\":10785},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10828,\"start\":10824},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10831,\"start\":10828},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10995,\"start\":10991},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11122,\"start\":11119},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11198,\"start\":11194},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12026,\"start\":12022},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12041,\"start\":12037},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12286,\"start\":12282},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12289,\"start\":12286},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12291,\"start\":12289},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12725,\"start\":12721},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12750,\"start\":12746},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13166,\"start\":13162},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13169,\"start\":13166},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13267,\"start\":13263},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13471,\"start\":13468},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13474,\"start\":13471},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13503,\"start\":13499},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13664,\"start\":13660},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13761,\"start\":13757},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13863,\"start\":13859},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14752,\"start\":14748},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16386,\"start\":16382},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17040,\"start\":17036},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17933,\"start\":17929},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17936,\"start\":17933},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17939,\"start\":17936},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19285,\"start\":19282},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19288,\"start\":19285},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19291,\"start\":19288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19354,\"start\":19350},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19538,\"start\":19534},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20570,\"start\":20566},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22370,\"start\":22366},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25253,\"start\":25250},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25275,\"start\":25271},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25719,\"start\":25716},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25790,\"start\":25786},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25793,\"start\":25790},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26050,\"start\":26047},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26053,\"start\":26050},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27776,\"start\":27773},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28729,\"start\":28725},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28791,\"start\":28788},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28947,\"start\":28943},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29086,\"start\":29083},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29089,\"start\":29086},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29839,\"start\":29835},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29850,\"start\":29846},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29861,\"start\":29857},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29876,\"start\":29872},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29887,\"start\":29883},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29898,\"start\":29894},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29910,\"start\":29906},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29925,\"start\":29921},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30145,\"start\":30141},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30364,\"start\":30360},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30625,\"start\":30622},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30848,\"start\":30844},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33369,\"start\":33366},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34864,\"start\":34863},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35077,\"start\":35073},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":42269,\"start\":42265},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":42501,\"start\":42497},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":42705,\"start\":42701},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":42741,\"start\":42737},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42897,\"start\":42893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50195,\"start\":50192},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":50198,\"start\":50195},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":50201,\"start\":50198}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42031,\"start\":41813},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42257,\"start\":42032},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43530,\"start\":42258},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44533,\"start\":43531},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45011,\"start\":44534},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49907,\"start\":45012},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50118,\"start\":49908}]", "paragraph": "[{\"end\":4824,\"start\":4100},{\"end\":5749,\"start\":4826},{\"end\":6670,\"start\":5751},{\"end\":7518,\"start\":6672},{\"end\":8461,\"start\":7520},{\"end\":8523,\"start\":8463},{\"end\":8752,\"start\":8525},{\"end\":9151,\"start\":8754},{\"end\":10208,\"start\":9168},{\"end\":12293,\"start\":10243},{\"end\":12970,\"start\":12328},{\"end\":14557,\"start\":12972},{\"end\":14893,\"start\":14559},{\"end\":16273,\"start\":14904},{\"end\":16768,\"start\":16291},{\"end\":17376,\"start\":16792},{\"end\":17548,\"start\":17394},{\"end\":17879,\"start\":17595},{\"end\":18523,\"start\":17907},{\"end\":18824,\"start\":18615},{\"end\":19146,\"start\":18848},{\"end\":19160,\"start\":19148},{\"end\":19337,\"start\":19162},{\"end\":19573,\"start\":19339},{\"end\":20330,\"start\":19601},{\"end\":20855,\"start\":20375},{\"end\":21004,\"start\":20900},{\"end\":21416,\"start\":21006},{\"end\":21677,\"start\":21443},{\"end\":21870,\"start\":21679},{\"end\":22371,\"start\":21913},{\"end\":22748,\"start\":22408},{\"end\":22880,\"start\":22791},{\"end\":23218,\"start\":22923},{\"end\":24093,\"start\":23258},{\"end\":25120,\"start\":24150},{\"end\":26054,\"start\":25149},{\"end\":26692,\"start\":26075},{\"end\":27644,\"start\":26694},{\"end\":28225,\"start\":27662},{\"end\":29158,\"start\":28247},{\"end\":29643,\"start\":29160},{\"end\":30134,\"start\":29663},{\"end\":32998,\"start\":30136},{\"end\":35920,\"start\":33024},{\"end\":36792,\"start\":35941},{\"end\":37572,\"start\":36794},{\"end\":38144,\"start\":37574},{\"end\":39891,\"start\":38176},{\"end\":40368,\"start\":39893},{\"end\":41431,\"start\":40383},{\"end\":41812,\"start\":41433}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16791,\"start\":16769},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17393,\"start\":17377},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17594,\"start\":17549},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18558,\"start\":18524},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18614,\"start\":18558},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19600,\"start\":19574},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20374,\"start\":20331},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20899,\"start\":20856},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21912,\"start\":21871},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22407,\"start\":22372},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22790,\"start\":22749},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22922,\"start\":22881},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24149,\"start\":24094}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27643,\"start\":27636},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33306,\"start\":33299},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33860,\"start\":33853},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38479,\"start\":38472},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39856,\"start\":39849}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4098,\"start\":4086},{\"attributes\":{\"n\":\"2\"},\"end\":9166,\"start\":9154},{\"attributes\":{\"n\":\"2.1\"},\"end\":10241,\"start\":10211},{\"attributes\":{\"n\":\"2.2\"},\"end\":12326,\"start\":12296},{\"attributes\":{\"n\":\"3\"},\"end\":14902,\"start\":14896},{\"attributes\":{\"n\":\"3.1\"},\"end\":16289,\"start\":16276},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":17905,\"start\":17882},{\"attributes\":{\"n\":\"3.2\"},\"end\":18846,\"start\":18827},{\"attributes\":{\"n\":\"3.3\"},\"end\":21441,\"start\":21419},{\"attributes\":{\"n\":\"3.4\"},\"end\":23256,\"start\":23221},{\"attributes\":{\"n\":\"4\"},\"end\":25147,\"start\":25123},{\"attributes\":{\"n\":\"4.2\"},\"end\":26073,\"start\":26057},{\"attributes\":{\"n\":\"4.3\"},\"end\":27660,\"start\":27647},{\"attributes\":{\"n\":\"4.4\"},\"end\":28245,\"start\":28228},{\"attributes\":{\"n\":\"4.5\"},\"end\":29661,\"start\":29646},{\"attributes\":{\"n\":\"4.6.2\"},\"end\":33022,\"start\":33001},{\"attributes\":{\"n\":\"4.7\"},\"end\":35939,\"start\":35923},{\"attributes\":{\"n\":\"4.8\"},\"end\":38174,\"start\":38147},{\"attributes\":{\"n\":\"5\"},\"end\":40381,\"start\":40371},{\"end\":41824,\"start\":41814},{\"end\":42043,\"start\":42033},{\"end\":43541,\"start\":43532},{\"end\":44544,\"start\":44535},{\"end\":45022,\"start\":45013}]", "table": "[{\"end\":43530,\"start\":43050},{\"end\":44533,\"start\":43618},{\"end\":45011,\"start\":44706},{\"end\":49907,\"start\":49428}]", "figure_caption": "[{\"end\":42031,\"start\":41826},{\"end\":42257,\"start\":42045},{\"end\":43050,\"start\":42260},{\"end\":43618,\"start\":43543},{\"end\":44706,\"start\":44546},{\"end\":49428,\"start\":45024},{\"end\":50118,\"start\":49910}]", "figure_ref": "[{\"end\":4609,\"start\":4601},{\"end\":15011,\"start\":15003},{\"end\":15596,\"start\":15588},{\"end\":18163,\"start\":18155},{\"end\":19877,\"start\":19869},{\"end\":20244,\"start\":20236},{\"end\":20850,\"start\":20842},{\"end\":23325,\"start\":23316},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34732,\"start\":34724},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35645,\"start\":35637},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36987,\"start\":36979}]", "bib_author_first_name": "[{\"end\":50547,\"start\":50540},{\"end\":50563,\"start\":50555},{\"end\":50578,\"start\":50573},{\"end\":50589,\"start\":50585},{\"end\":50606,\"start\":50601},{\"end\":50898,\"start\":50893},{\"end\":50924,\"start\":50913},{\"end\":50939,\"start\":50930},{\"end\":50952,\"start\":50947},{\"end\":50970,\"start\":50964},{\"end\":50984,\"start\":50978},{\"end\":51351,\"start\":51346},{\"end\":51365,\"start\":51358},{\"end\":51378,\"start\":51372},{\"end\":51390,\"start\":51384},{\"end\":51403,\"start\":51396},{\"end\":51408,\"start\":51404},{\"end\":51736,\"start\":51728},{\"end\":51750,\"start\":51743},{\"end\":51755,\"start\":51751},{\"end\":51771,\"start\":51762},{\"end\":51773,\"start\":51772},{\"end\":52072,\"start\":52067},{\"end\":52085,\"start\":52080},{\"end\":52099,\"start\":52095},{\"end\":52114,\"start\":52108},{\"end\":52127,\"start\":52122},{\"end\":52145,\"start\":52139},{\"end\":52162,\"start\":52155},{\"end\":52177,\"start\":52173},{\"end\":52198,\"start\":52189},{\"end\":52212,\"start\":52206},{\"end\":52550,\"start\":52546},{\"end\":52562,\"start\":52556},{\"end\":52576,\"start\":52569},{\"end\":52589,\"start\":52583},{\"end\":52605,\"start\":52597},{\"end\":52617,\"start\":52610},{\"end\":52850,\"start\":52843},{\"end\":52866,\"start\":52859},{\"end\":52879,\"start\":52874},{\"end\":53087,\"start\":53080},{\"end\":53103,\"start\":53096},{\"end\":53116,\"start\":53111},{\"end\":53376,\"start\":53368},{\"end\":53388,\"start\":53382},{\"end\":53398,\"start\":53397},{\"end\":53400,\"start\":53399},{\"end\":53413,\"start\":53407},{\"end\":53426,\"start\":53421},{\"end\":53676,\"start\":53670},{\"end\":53693,\"start\":53686},{\"end\":53695,\"start\":53694},{\"end\":53710,\"start\":53705},{\"end\":53951,\"start\":53946},{\"end\":53968,\"start\":53960},{\"end\":53982,\"start\":53976},{\"end\":53996,\"start\":53988},{\"end\":54190,\"start\":54184},{\"end\":54202,\"start\":54197},{\"end\":54215,\"start\":54209},{\"end\":54228,\"start\":54222},{\"end\":54244,\"start\":54236},{\"end\":54257,\"start\":54252},{\"end\":54266,\"start\":54262},{\"end\":54287,\"start\":54280},{\"end\":54829,\"start\":54821},{\"end\":54842,\"start\":54834},{\"end\":55103,\"start\":55101},{\"end\":55113,\"start\":55108},{\"end\":55126,\"start\":55120},{\"end\":55146,\"start\":55141},{\"end\":55312,\"start\":55307},{\"end\":55332,\"start\":55327},{\"end\":55352,\"start\":55341},{\"end\":55669,\"start\":55660},{\"end\":55686,\"start\":55678},{\"end\":55704,\"start\":55697},{\"end\":56007,\"start\":55999},{\"end\":56022,\"start\":56016},{\"end\":56037,\"start\":56032},{\"end\":56052,\"start\":56045},{\"end\":56068,\"start\":56060},{\"end\":56227,\"start\":56226},{\"end\":56241,\"start\":56238},{\"end\":56514,\"start\":56506},{\"end\":56526,\"start\":56520},{\"end\":56542,\"start\":56534},{\"end\":56553,\"start\":56547},{\"end\":56565,\"start\":56560},{\"end\":56577,\"start\":56572},{\"end\":56592,\"start\":56584},{\"end\":56873,\"start\":56869},{\"end\":56886,\"start\":56881},{\"end\":56899,\"start\":56895},{\"end\":56913,\"start\":56909},{\"end\":56927,\"start\":56922},{\"end\":57154,\"start\":57149},{\"end\":57163,\"start\":57159},{\"end\":57176,\"start\":57172},{\"end\":57188,\"start\":57185},{\"end\":57205,\"start\":57199},{\"end\":57222,\"start\":57214},{\"end\":57455,\"start\":57451},{\"end\":57464,\"start\":57462},{\"end\":57469,\"start\":57465},{\"end\":57483,\"start\":57476},{\"end\":57496,\"start\":57488},{\"end\":57510,\"start\":57502},{\"end\":57766,\"start\":57762},{\"end\":57775,\"start\":57773},{\"end\":57780,\"start\":57776},{\"end\":57792,\"start\":57787},{\"end\":57806,\"start\":57800},{\"end\":57822,\"start\":57814},{\"end\":58087,\"start\":58083},{\"end\":58101,\"start\":58094},{\"end\":58109,\"start\":58107},{\"end\":58114,\"start\":58110},{\"end\":58129,\"start\":58121},{\"end\":58378,\"start\":58374},{\"end\":58387,\"start\":58384},{\"end\":58639,\"start\":58634},{\"end\":58656,\"start\":58647},{\"end\":58676,\"start\":58669},{\"end\":58693,\"start\":58683},{\"end\":58704,\"start\":58699},{\"end\":58719,\"start\":58713},{\"end\":59034,\"start\":59028},{\"end\":59047,\"start\":59042},{\"end\":59066,\"start\":59060},{\"end\":59253,\"start\":59250},{\"end\":59268,\"start\":59260},{\"end\":59274,\"start\":59269},{\"end\":59289,\"start\":59284},{\"end\":59306,\"start\":59298},{\"end\":59322,\"start\":59316},{\"end\":59336,\"start\":59331},{\"end\":59619,\"start\":59612},{\"end\":59635,\"start\":59630},{\"end\":59648,\"start\":59644},{\"end\":59663,\"start\":59655},{\"end\":59829,\"start\":59823},{\"end\":59844,\"start\":59836},{\"end\":59856,\"start\":59849},{\"end\":60311,\"start\":60307},{\"end\":60328,\"start\":60321},{\"end\":60338,\"start\":60333},{\"end\":60351,\"start\":60346},{\"end\":60363,\"start\":60358},{\"end\":60376,\"start\":60372},{\"end\":60635,\"start\":60630},{\"end\":60651,\"start\":60647},{\"end\":60889,\"start\":60882},{\"end\":60899,\"start\":60894},{\"end\":60920,\"start\":60914},{\"end\":60934,\"start\":60928},{\"end\":60947,\"start\":60942},{\"end\":61210,\"start\":61203},{\"end\":61218,\"start\":61216},{\"end\":61417,\"start\":61412},{\"end\":61444,\"start\":61436},{\"end\":61746,\"start\":61739},{\"end\":61770,\"start\":61766},{\"end\":61782,\"start\":61778},{\"end\":61796,\"start\":61791},{\"end\":61816,\"start\":61808},{\"end\":62121,\"start\":62114},{\"end\":62146,\"start\":62138},{\"end\":62481,\"start\":62473},{\"end\":62492,\"start\":62488},{\"end\":62508,\"start\":62500},{\"end\":62520,\"start\":62514},{\"end\":62806,\"start\":62803},{\"end\":62820,\"start\":62813},{\"end\":62830,\"start\":62827},{\"end\":62844,\"start\":62837},{\"end\":62859,\"start\":62851},{\"end\":63115,\"start\":63109},{\"end\":63139,\"start\":63133},{\"end\":63154,\"start\":63147},{\"end\":63168,\"start\":63163},{\"end\":63178,\"start\":63173},{\"end\":63479,\"start\":63474},{\"end\":63503,\"start\":63497},{\"end\":63519,\"start\":63513},{\"end\":63540,\"start\":63528},{\"end\":63555,\"start\":63548},{\"end\":63572,\"start\":63566},{\"end\":63582,\"start\":63577},{\"end\":63892,\"start\":63891},{\"end\":63905,\"start\":63900},{\"end\":64210,\"start\":64209},{\"end\":64480,\"start\":64475},{\"end\":64493,\"start\":64487},{\"end\":64505,\"start\":64498},{\"end\":64731,\"start\":64726},{\"end\":64744,\"start\":64740},{\"end\":64959,\"start\":64955},{\"end\":64976,\"start\":64967},{\"end\":65204,\"start\":65196},{\"end\":65214,\"start\":65212},{\"end\":65228,\"start\":65221},{\"end\":65236,\"start\":65233},{\"end\":65250,\"start\":65243},{\"end\":65522,\"start\":65517},{\"end\":65534,\"start\":65530},{\"end\":65546,\"start\":65540},{\"end\":65563,\"start\":65555},{\"end\":65575,\"start\":65570},{\"end\":65591,\"start\":65586},{\"end\":65605,\"start\":65597},{\"end\":65619,\"start\":65611},{\"end\":65634,\"start\":65625},{\"end\":65966,\"start\":65961},{\"end\":65978,\"start\":65974},{\"end\":65991,\"start\":65985},{\"end\":66006,\"start\":65999},{\"end\":66020,\"start\":66012},{\"end\":66277,\"start\":66272},{\"end\":66289,\"start\":66285},{\"end\":66303,\"start\":66296},{\"end\":66317,\"start\":66309},{\"end\":66329,\"start\":66324},{\"end\":66338,\"start\":66335},{\"end\":66352,\"start\":66346},{\"end\":66712,\"start\":66703},{\"end\":66725,\"start\":66719},{\"end\":67028,\"start\":67019},{\"end\":67040,\"start\":67035},{\"end\":67052,\"start\":67046}]", "bib_author_last_name": "[{\"end\":50553,\"start\":50548},{\"end\":50571,\"start\":50564},{\"end\":50583,\"start\":50579},{\"end\":50599,\"start\":50590},{\"end\":50612,\"start\":50607},{\"end\":50911,\"start\":50899},{\"end\":50928,\"start\":50925},{\"end\":50945,\"start\":50940},{\"end\":50962,\"start\":50953},{\"end\":50976,\"start\":50971},{\"end\":50998,\"start\":50985},{\"end\":51005,\"start\":51000},{\"end\":51356,\"start\":51352},{\"end\":51370,\"start\":51366},{\"end\":51382,\"start\":51379},{\"end\":51394,\"start\":51391},{\"end\":51413,\"start\":51409},{\"end\":51741,\"start\":51737},{\"end\":51760,\"start\":51756},{\"end\":51782,\"start\":51774},{\"end\":52078,\"start\":52073},{\"end\":52093,\"start\":52086},{\"end\":52106,\"start\":52100},{\"end\":52120,\"start\":52115},{\"end\":52137,\"start\":52128},{\"end\":52153,\"start\":52146},{\"end\":52171,\"start\":52163},{\"end\":52187,\"start\":52178},{\"end\":52204,\"start\":52199},{\"end\":52221,\"start\":52213},{\"end\":52554,\"start\":52551},{\"end\":52567,\"start\":52563},{\"end\":52581,\"start\":52577},{\"end\":52595,\"start\":52590},{\"end\":52608,\"start\":52606},{\"end\":52621,\"start\":52618},{\"end\":52857,\"start\":52851},{\"end\":52872,\"start\":52867},{\"end\":52886,\"start\":52880},{\"end\":53094,\"start\":53088},{\"end\":53109,\"start\":53104},{\"end\":53123,\"start\":53117},{\"end\":53380,\"start\":53377},{\"end\":53395,\"start\":53389},{\"end\":53405,\"start\":53401},{\"end\":53419,\"start\":53414},{\"end\":53430,\"start\":53427},{\"end\":53437,\"start\":53432},{\"end\":53684,\"start\":53677},{\"end\":53703,\"start\":53696},{\"end\":53717,\"start\":53711},{\"end\":53958,\"start\":53952},{\"end\":53974,\"start\":53969},{\"end\":53986,\"start\":53983},{\"end\":54006,\"start\":53997},{\"end\":54195,\"start\":54191},{\"end\":54207,\"start\":54203},{\"end\":54220,\"start\":54216},{\"end\":54234,\"start\":54229},{\"end\":54250,\"start\":54245},{\"end\":54260,\"start\":54258},{\"end\":54278,\"start\":54267},{\"end\":54295,\"start\":54288},{\"end\":54832,\"start\":54830},{\"end\":54855,\"start\":54843},{\"end\":54860,\"start\":54857},{\"end\":55106,\"start\":55104},{\"end\":55118,\"start\":55114},{\"end\":55139,\"start\":55127},{\"end\":55152,\"start\":55147},{\"end\":55325,\"start\":55313},{\"end\":55339,\"start\":55333},{\"end\":55355,\"start\":55353},{\"end\":55676,\"start\":55670},{\"end\":55695,\"start\":55687},{\"end\":55711,\"start\":55705},{\"end\":56014,\"start\":56008},{\"end\":56030,\"start\":56023},{\"end\":56043,\"start\":56038},{\"end\":56058,\"start\":56053},{\"end\":56074,\"start\":56069},{\"end\":56236,\"start\":56228},{\"end\":56248,\"start\":56242},{\"end\":56257,\"start\":56250},{\"end\":56518,\"start\":56515},{\"end\":56532,\"start\":56527},{\"end\":56545,\"start\":56543},{\"end\":56558,\"start\":56554},{\"end\":56570,\"start\":56566},{\"end\":56582,\"start\":56578},{\"end\":56597,\"start\":56593},{\"end\":56879,\"start\":56874},{\"end\":56893,\"start\":56887},{\"end\":56907,\"start\":56900},{\"end\":56920,\"start\":56914},{\"end\":56933,\"start\":56928},{\"end\":57157,\"start\":57155},{\"end\":57170,\"start\":57164},{\"end\":57183,\"start\":57177},{\"end\":57197,\"start\":57189},{\"end\":57212,\"start\":57206},{\"end\":57226,\"start\":57223},{\"end\":57460,\"start\":57456},{\"end\":57474,\"start\":57470},{\"end\":57486,\"start\":57484},{\"end\":57500,\"start\":57497},{\"end\":57515,\"start\":57511},{\"end\":57771,\"start\":57767},{\"end\":57785,\"start\":57781},{\"end\":57798,\"start\":57793},{\"end\":57812,\"start\":57807},{\"end\":57827,\"start\":57823},{\"end\":58092,\"start\":58088},{\"end\":58105,\"start\":58102},{\"end\":58119,\"start\":58115},{\"end\":58134,\"start\":58130},{\"end\":58382,\"start\":58379},{\"end\":58392,\"start\":58388},{\"end\":58645,\"start\":58640},{\"end\":58667,\"start\":58657},{\"end\":58681,\"start\":58677},{\"end\":58697,\"start\":58694},{\"end\":58711,\"start\":58705},{\"end\":58737,\"start\":58720},{\"end\":58744,\"start\":58739},{\"end\":59040,\"start\":59035},{\"end\":59058,\"start\":59048},{\"end\":59075,\"start\":59067},{\"end\":59258,\"start\":59254},{\"end\":59282,\"start\":59275},{\"end\":59296,\"start\":59290},{\"end\":59314,\"start\":59307},{\"end\":59329,\"start\":59323},{\"end\":59347,\"start\":59337},{\"end\":59356,\"start\":59349},{\"end\":59628,\"start\":59620},{\"end\":59642,\"start\":59636},{\"end\":59653,\"start\":59649},{\"end\":59667,\"start\":59664},{\"end\":59834,\"start\":59830},{\"end\":59847,\"start\":59845},{\"end\":59859,\"start\":59857},{\"end\":60319,\"start\":60312},{\"end\":60331,\"start\":60329},{\"end\":60344,\"start\":60339},{\"end\":60356,\"start\":60352},{\"end\":60370,\"start\":60364},{\"end\":60386,\"start\":60377},{\"end\":60645,\"start\":60636},{\"end\":60660,\"start\":60652},{\"end\":60892,\"start\":60890},{\"end\":60912,\"start\":60900},{\"end\":60926,\"start\":60921},{\"end\":60940,\"start\":60935},{\"end\":60953,\"start\":60948},{\"end\":61214,\"start\":61211},{\"end\":61224,\"start\":61219},{\"end\":61434,\"start\":61418},{\"end\":61451,\"start\":61445},{\"end\":61458,\"start\":61453},{\"end\":61764,\"start\":61747},{\"end\":61776,\"start\":61771},{\"end\":61789,\"start\":61783},{\"end\":61806,\"start\":61797},{\"end\":61821,\"start\":61817},{\"end\":61830,\"start\":61823},{\"end\":62136,\"start\":62122},{\"end\":62153,\"start\":62147},{\"end\":62486,\"start\":62482},{\"end\":62498,\"start\":62493},{\"end\":62512,\"start\":62509},{\"end\":62523,\"start\":62521},{\"end\":62811,\"start\":62807},{\"end\":62825,\"start\":62821},{\"end\":62835,\"start\":62831},{\"end\":62849,\"start\":62845},{\"end\":62862,\"start\":62860},{\"end\":63131,\"start\":63116},{\"end\":63145,\"start\":63140},{\"end\":63161,\"start\":63155},{\"end\":63171,\"start\":63169},{\"end\":63186,\"start\":63179},{\"end\":63193,\"start\":63188},{\"end\":63495,\"start\":63480},{\"end\":63511,\"start\":63504},{\"end\":63526,\"start\":63520},{\"end\":63546,\"start\":63541},{\"end\":63564,\"start\":63556},{\"end\":63575,\"start\":63573},{\"end\":63588,\"start\":63583},{\"end\":63595,\"start\":63590},{\"end\":63898,\"start\":63893},{\"end\":63914,\"start\":63906},{\"end\":63921,\"start\":63916},{\"end\":64217,\"start\":64211},{\"end\":64227,\"start\":64219},{\"end\":64485,\"start\":64481},{\"end\":64496,\"start\":64494},{\"end\":64510,\"start\":64506},{\"end\":64738,\"start\":64732},{\"end\":64753,\"start\":64745},{\"end\":64965,\"start\":64960},{\"end\":64982,\"start\":64977},{\"end\":65210,\"start\":65205},{\"end\":65219,\"start\":65215},{\"end\":65231,\"start\":65229},{\"end\":65241,\"start\":65237},{\"end\":65256,\"start\":65251},{\"end\":65528,\"start\":65523},{\"end\":65538,\"start\":65535},{\"end\":65553,\"start\":65547},{\"end\":65568,\"start\":65564},{\"end\":65584,\"start\":65576},{\"end\":65595,\"start\":65592},{\"end\":65609,\"start\":65606},{\"end\":65623,\"start\":65620},{\"end\":65640,\"start\":65635},{\"end\":65972,\"start\":65967},{\"end\":65983,\"start\":65979},{\"end\":65997,\"start\":65992},{\"end\":66010,\"start\":66007},{\"end\":66025,\"start\":66021},{\"end\":66283,\"start\":66278},{\"end\":66294,\"start\":66290},{\"end\":66307,\"start\":66304},{\"end\":66322,\"start\":66318},{\"end\":66333,\"start\":66330},{\"end\":66344,\"start\":66339},{\"end\":66358,\"start\":66353},{\"end\":66717,\"start\":66713},{\"end\":66734,\"start\":66726},{\"end\":67033,\"start\":67029},{\"end\":67044,\"start\":67041},{\"end\":67061,\"start\":67053}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":215587100},\"end\":50796,\"start\":50492},{\"attributes\":{\"id\":\"b1\"},\"end\":51246,\"start\":50798},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":170078653},\"end\":51615,\"start\":51248},{\"attributes\":{\"id\":\"b3\"},\"end\":51980,\"start\":51617},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":58981312},\"end\":52495,\"start\":51982},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":197620943},\"end\":52784,\"start\":52497},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":214735659},\"end\":53016,\"start\":52786},{\"attributes\":{\"doi\":\"ArXiv abs/2003.13624\",\"id\":\"b7\"},\"end\":53290,\"start\":53018},{\"attributes\":{\"id\":\"b8\"},\"end\":53613,\"start\":53292},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235176424},\"end\":53862,\"start\":53615},{\"attributes\":{\"id\":\"b10\"},\"end\":54182,\"start\":53864},{\"attributes\":{\"doi\":\"arXiv:1907.01669\",\"id\":\"b11\"},\"end\":54763,\"start\":54184},{\"attributes\":{\"doi\":\"arXiv:2111.02643\",\"id\":\"b12\"},\"end\":55038,\"start\":54765},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52119091},\"end\":55305,\"start\":55040},{\"attributes\":{\"doi\":\"arXiv:2005.00796\",\"id\":\"b14\"},\"end\":55658,\"start\":55307},{\"attributes\":{\"id\":\"b15\"},\"end\":55963,\"start\":55660},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207639110},\"end\":56224,\"start\":55965},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b17\"},\"end\":56433,\"start\":56226},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220280457},\"end\":56804,\"start\":56435},{\"attributes\":{\"id\":\"b19\"},\"end\":57094,\"start\":56806},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3147007},\"end\":57397,\"start\":57096},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235234583},\"end\":57675,\"start\":57399},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235792306},\"end\":58018,\"start\":57677},{\"attributes\":{\"id\":\"b23\"},\"end\":58280,\"start\":58020},{\"attributes\":{\"id\":\"b24\"},\"end\":58541,\"start\":58282},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":227012658},\"end\":58987,\"start\":58543},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":198179946},\"end\":59204,\"start\":58989},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":216641949},\"end\":59546,\"start\":59206},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":11080756},\"end\":59821,\"start\":59548},{\"attributes\":{\"doi\":\"arXiv:2005.05298\",\"id\":\"b29\"},\"end\":60252,\"start\":59823},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":160025533},\"end\":60577,\"start\":60254},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":17732445},\"end\":60781,\"start\":60579},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":28821374},\"end\":61166,\"start\":60783},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":47012216},\"end\":61318,\"start\":61168},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":76564},\"end\":61687,\"start\":61320},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":204957494},\"end\":62082,\"start\":61689},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5855042},\"end\":62326,\"start\":62084},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":219636414},\"end\":62746,\"start\":62328},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":216553145},\"end\":63014,\"start\":62748},{\"attributes\":{\"id\":\"b39\"},\"end\":63404,\"start\":63016},{\"attributes\":{\"id\":\"b40\"},\"end\":63815,\"start\":63406},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13903063},\"end\":64116,\"start\":63817},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2332513},\"end\":64402,\"start\":64118},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":227334663},\"end\":64658,\"start\":64404},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":209414940},\"end\":64886,\"start\":64660},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":219708533},\"end\":65118,\"start\":64888},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":52233682},\"end\":65429,\"start\":65120},{\"attributes\":{\"id\":\"b47\"},\"end\":65880,\"start\":65431},{\"attributes\":{\"id\":\"b48\"},\"end\":66200,\"start\":65882},{\"attributes\":{\"doi\":\"arXiv:2004.09731\",\"id\":\"b49\"},\"end\":66597,\"start\":66202},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6179947},\"end\":66908,\"start\":66599},{\"attributes\":{\"id\":\"b51\"},\"end\":67246,\"start\":66910}]", "bib_title": "[{\"end\":50538,\"start\":50492},{\"end\":51344,\"start\":51248},{\"end\":52065,\"start\":51982},{\"end\":52544,\"start\":52497},{\"end\":52841,\"start\":52786},{\"end\":53668,\"start\":53615},{\"end\":55099,\"start\":55040},{\"end\":55997,\"start\":55965},{\"end\":56504,\"start\":56435},{\"end\":57147,\"start\":57096},{\"end\":57449,\"start\":57399},{\"end\":57760,\"start\":57677},{\"end\":58632,\"start\":58543},{\"end\":59026,\"start\":58989},{\"end\":59248,\"start\":59206},{\"end\":59610,\"start\":59548},{\"end\":60305,\"start\":60254},{\"end\":60628,\"start\":60579},{\"end\":60880,\"start\":60783},{\"end\":61201,\"start\":61168},{\"end\":61410,\"start\":61320},{\"end\":61737,\"start\":61689},{\"end\":62112,\"start\":62084},{\"end\":62471,\"start\":62328},{\"end\":62801,\"start\":62748},{\"end\":63889,\"start\":63817},{\"end\":64207,\"start\":64118},{\"end\":64473,\"start\":64404},{\"end\":64724,\"start\":64660},{\"end\":64953,\"start\":64888},{\"end\":65194,\"start\":65120},{\"end\":66701,\"start\":66599}]", "bib_author": "[{\"end\":50555,\"start\":50540},{\"end\":50573,\"start\":50555},{\"end\":50585,\"start\":50573},{\"end\":50601,\"start\":50585},{\"end\":50614,\"start\":50601},{\"end\":50913,\"start\":50893},{\"end\":50930,\"start\":50913},{\"end\":50947,\"start\":50930},{\"end\":50964,\"start\":50947},{\"end\":50978,\"start\":50964},{\"end\":51000,\"start\":50978},{\"end\":51007,\"start\":51000},{\"end\":51358,\"start\":51346},{\"end\":51372,\"start\":51358},{\"end\":51384,\"start\":51372},{\"end\":51396,\"start\":51384},{\"end\":51415,\"start\":51396},{\"end\":51743,\"start\":51728},{\"end\":51762,\"start\":51743},{\"end\":51784,\"start\":51762},{\"end\":52080,\"start\":52067},{\"end\":52095,\"start\":52080},{\"end\":52108,\"start\":52095},{\"end\":52122,\"start\":52108},{\"end\":52139,\"start\":52122},{\"end\":52155,\"start\":52139},{\"end\":52173,\"start\":52155},{\"end\":52189,\"start\":52173},{\"end\":52206,\"start\":52189},{\"end\":52223,\"start\":52206},{\"end\":52556,\"start\":52546},{\"end\":52569,\"start\":52556},{\"end\":52583,\"start\":52569},{\"end\":52597,\"start\":52583},{\"end\":52610,\"start\":52597},{\"end\":52623,\"start\":52610},{\"end\":52859,\"start\":52843},{\"end\":52874,\"start\":52859},{\"end\":52888,\"start\":52874},{\"end\":53096,\"start\":53080},{\"end\":53111,\"start\":53096},{\"end\":53125,\"start\":53111},{\"end\":53382,\"start\":53368},{\"end\":53397,\"start\":53382},{\"end\":53407,\"start\":53397},{\"end\":53421,\"start\":53407},{\"end\":53432,\"start\":53421},{\"end\":53439,\"start\":53432},{\"end\":53686,\"start\":53670},{\"end\":53705,\"start\":53686},{\"end\":53719,\"start\":53705},{\"end\":53960,\"start\":53946},{\"end\":53976,\"start\":53960},{\"end\":53988,\"start\":53976},{\"end\":54008,\"start\":53988},{\"end\":54197,\"start\":54184},{\"end\":54209,\"start\":54197},{\"end\":54222,\"start\":54209},{\"end\":54236,\"start\":54222},{\"end\":54252,\"start\":54236},{\"end\":54262,\"start\":54252},{\"end\":54280,\"start\":54262},{\"end\":54297,\"start\":54280},{\"end\":54834,\"start\":54821},{\"end\":54857,\"start\":54834},{\"end\":54862,\"start\":54857},{\"end\":55108,\"start\":55101},{\"end\":55120,\"start\":55108},{\"end\":55141,\"start\":55120},{\"end\":55154,\"start\":55141},{\"end\":55327,\"start\":55307},{\"end\":55341,\"start\":55327},{\"end\":55357,\"start\":55341},{\"end\":55678,\"start\":55660},{\"end\":55697,\"start\":55678},{\"end\":55713,\"start\":55697},{\"end\":56016,\"start\":55999},{\"end\":56032,\"start\":56016},{\"end\":56045,\"start\":56032},{\"end\":56060,\"start\":56045},{\"end\":56076,\"start\":56060},{\"end\":56238,\"start\":56226},{\"end\":56250,\"start\":56238},{\"end\":56259,\"start\":56250},{\"end\":56520,\"start\":56506},{\"end\":56534,\"start\":56520},{\"end\":56547,\"start\":56534},{\"end\":56560,\"start\":56547},{\"end\":56572,\"start\":56560},{\"end\":56584,\"start\":56572},{\"end\":56599,\"start\":56584},{\"end\":56881,\"start\":56869},{\"end\":56895,\"start\":56881},{\"end\":56909,\"start\":56895},{\"end\":56922,\"start\":56909},{\"end\":56935,\"start\":56922},{\"end\":57159,\"start\":57149},{\"end\":57172,\"start\":57159},{\"end\":57185,\"start\":57172},{\"end\":57199,\"start\":57185},{\"end\":57214,\"start\":57199},{\"end\":57228,\"start\":57214},{\"end\":57462,\"start\":57451},{\"end\":57476,\"start\":57462},{\"end\":57488,\"start\":57476},{\"end\":57502,\"start\":57488},{\"end\":57517,\"start\":57502},{\"end\":57773,\"start\":57762},{\"end\":57787,\"start\":57773},{\"end\":57800,\"start\":57787},{\"end\":57814,\"start\":57800},{\"end\":57829,\"start\":57814},{\"end\":58094,\"start\":58083},{\"end\":58107,\"start\":58094},{\"end\":58121,\"start\":58107},{\"end\":58136,\"start\":58121},{\"end\":58384,\"start\":58374},{\"end\":58394,\"start\":58384},{\"end\":58647,\"start\":58634},{\"end\":58669,\"start\":58647},{\"end\":58683,\"start\":58669},{\"end\":58699,\"start\":58683},{\"end\":58713,\"start\":58699},{\"end\":58739,\"start\":58713},{\"end\":58746,\"start\":58739},{\"end\":59042,\"start\":59028},{\"end\":59060,\"start\":59042},{\"end\":59077,\"start\":59060},{\"end\":59260,\"start\":59250},{\"end\":59284,\"start\":59260},{\"end\":59298,\"start\":59284},{\"end\":59316,\"start\":59298},{\"end\":59331,\"start\":59316},{\"end\":59349,\"start\":59331},{\"end\":59358,\"start\":59349},{\"end\":59630,\"start\":59612},{\"end\":59644,\"start\":59630},{\"end\":59655,\"start\":59644},{\"end\":59669,\"start\":59655},{\"end\":59836,\"start\":59823},{\"end\":59849,\"start\":59836},{\"end\":59861,\"start\":59849},{\"end\":60321,\"start\":60307},{\"end\":60333,\"start\":60321},{\"end\":60346,\"start\":60333},{\"end\":60358,\"start\":60346},{\"end\":60372,\"start\":60358},{\"end\":60388,\"start\":60372},{\"end\":60647,\"start\":60630},{\"end\":60662,\"start\":60647},{\"end\":60894,\"start\":60882},{\"end\":60914,\"start\":60894},{\"end\":60928,\"start\":60914},{\"end\":60942,\"start\":60928},{\"end\":60955,\"start\":60942},{\"end\":61216,\"start\":61203},{\"end\":61226,\"start\":61216},{\"end\":61436,\"start\":61412},{\"end\":61453,\"start\":61436},{\"end\":61460,\"start\":61453},{\"end\":61766,\"start\":61739},{\"end\":61778,\"start\":61766},{\"end\":61791,\"start\":61778},{\"end\":61808,\"start\":61791},{\"end\":61823,\"start\":61808},{\"end\":61832,\"start\":61823},{\"end\":62138,\"start\":62114},{\"end\":62155,\"start\":62138},{\"end\":62488,\"start\":62473},{\"end\":62500,\"start\":62488},{\"end\":62514,\"start\":62500},{\"end\":62525,\"start\":62514},{\"end\":62813,\"start\":62803},{\"end\":62827,\"start\":62813},{\"end\":62837,\"start\":62827},{\"end\":62851,\"start\":62837},{\"end\":62864,\"start\":62851},{\"end\":63133,\"start\":63109},{\"end\":63147,\"start\":63133},{\"end\":63163,\"start\":63147},{\"end\":63173,\"start\":63163},{\"end\":63188,\"start\":63173},{\"end\":63195,\"start\":63188},{\"end\":63497,\"start\":63474},{\"end\":63513,\"start\":63497},{\"end\":63528,\"start\":63513},{\"end\":63548,\"start\":63528},{\"end\":63566,\"start\":63548},{\"end\":63577,\"start\":63566},{\"end\":63590,\"start\":63577},{\"end\":63597,\"start\":63590},{\"end\":63900,\"start\":63891},{\"end\":63916,\"start\":63900},{\"end\":63923,\"start\":63916},{\"end\":64219,\"start\":64209},{\"end\":64229,\"start\":64219},{\"end\":64487,\"start\":64475},{\"end\":64498,\"start\":64487},{\"end\":64512,\"start\":64498},{\"end\":64740,\"start\":64726},{\"end\":64755,\"start\":64740},{\"end\":64967,\"start\":64955},{\"end\":64984,\"start\":64967},{\"end\":65212,\"start\":65196},{\"end\":65221,\"start\":65212},{\"end\":65233,\"start\":65221},{\"end\":65243,\"start\":65233},{\"end\":65258,\"start\":65243},{\"end\":65530,\"start\":65517},{\"end\":65540,\"start\":65530},{\"end\":65555,\"start\":65540},{\"end\":65570,\"start\":65555},{\"end\":65586,\"start\":65570},{\"end\":65597,\"start\":65586},{\"end\":65611,\"start\":65597},{\"end\":65625,\"start\":65611},{\"end\":65642,\"start\":65625},{\"end\":65974,\"start\":65961},{\"end\":65985,\"start\":65974},{\"end\":65999,\"start\":65985},{\"end\":66012,\"start\":65999},{\"end\":66027,\"start\":66012},{\"end\":66285,\"start\":66272},{\"end\":66296,\"start\":66285},{\"end\":66309,\"start\":66296},{\"end\":66324,\"start\":66309},{\"end\":66335,\"start\":66324},{\"end\":66346,\"start\":66335},{\"end\":66360,\"start\":66346},{\"end\":66719,\"start\":66703},{\"end\":66736,\"start\":66719},{\"end\":67035,\"start\":67019},{\"end\":67046,\"start\":67035},{\"end\":67063,\"start\":67046}]", "bib_venue": "[{\"end\":50633,\"start\":50614},{\"end\":50891,\"start\":50798},{\"end\":51418,\"start\":51415},{\"end\":51726,\"start\":51617},{\"end\":52226,\"start\":52223},{\"end\":52628,\"start\":52623},{\"end\":52892,\"start\":52888},{\"end\":53078,\"start\":53018},{\"end\":53366,\"start\":53292},{\"end\":53724,\"start\":53719},{\"end\":53944,\"start\":53864},{\"end\":54465,\"start\":54313},{\"end\":54819,\"start\":54765},{\"end\":55159,\"start\":55154},{\"end\":55462,\"start\":55373},{\"end\":55805,\"start\":55713},{\"end\":56081,\"start\":56076},{\"end\":56305,\"start\":56274},{\"end\":56605,\"start\":56599},{\"end\":56867,\"start\":56806},{\"end\":57233,\"start\":57228},{\"end\":57521,\"start\":57517},{\"end\":57834,\"start\":57829},{\"end\":58081,\"start\":58020},{\"end\":58372,\"start\":58282},{\"end\":58752,\"start\":58746},{\"end\":59084,\"start\":59077},{\"end\":59363,\"start\":59358},{\"end\":59672,\"start\":59669},{\"end\":60017,\"start\":59877},{\"end\":60399,\"start\":60388},{\"end\":60667,\"start\":60662},{\"end\":60962,\"start\":60955},{\"end\":61231,\"start\":61226},{\"end\":61483,\"start\":61460},{\"end\":61867,\"start\":61832},{\"end\":62191,\"start\":62155},{\"end\":62529,\"start\":62525},{\"end\":62867,\"start\":62864},{\"end\":63107,\"start\":63016},{\"end\":63472,\"start\":63406},{\"end\":63949,\"start\":63923},{\"end\":64245,\"start\":64229},{\"end\":64516,\"start\":64512},{\"end\":64760,\"start\":64755},{\"end\":64990,\"start\":64984},{\"end\":65262,\"start\":65258},{\"end\":65515,\"start\":65431},{\"end\":65959,\"start\":65882},{\"end\":66270,\"start\":66202},{\"end\":66743,\"start\":66736},{\"end\":67017,\"start\":66910}]"}}}, "year": 2023, "month": 12, "day": 17}