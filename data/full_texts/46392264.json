{"id": 46392264, "updated": "2023-07-19 05:25:07.507", "metadata": {"title": "Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages", "authors": "[{\"first\":\"Dirk\",\"last\":\"Goldhahn\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Eckart\",\"middle\":[]},{\"first\":\"Uwe\",\"last\":\"Quasthoff\",\"middle\":[]}]", "venue": "LREC", "journal": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)", "publication_date": {"year": 2012, "month": null, "day": null}, "abstract": "The Leipzig Corpora Collection offers free online access to 136 monolingual dictionaries enriched with statistical information. In this paper we describe current advances of the project in collecting and processing text data automatically for a large number of languages. Our main interest lies in languages of \u0093low density\u0094, where only few text data exists online. The aim of this approach is to create monolingual dictionaries and statistical information for a high number of new languages and to expand the existing dictionaries, opening up new possibilities for linguistic typology and other research. Focus of this paper will be set on the infrastructure for the automatic acquisition of large amounts of monolingual text in many languages from various sources. Preliminary results of the collection of text data will be presented. The mainly language-independent framework for preprocessing, cleaning and creating the corpora and computing the necessary statistics will also be depicted.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2250600805", "acl": "L12-1154", "pubmed": null, "pubmedcentral": null, "dblp": "conf/lrec/GoldhahnEQ12", "doi": null}}, "content": {"source": {"pdf_hash": "1b560f892432fb853d233c92f9294640bc91de3c", "pdf_src": "ACL", "pdf_uri": "[\"http://www.lrec-conf.org/proceedings/lrec2012/pdf/327_Paper.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "aeb1a1ff9df42735c97e4fb1cedbd23ffc4cb947", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1b560f892432fb853d233c92f9294640bc91de3c.txt", "contents": "\nBuilding Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages\n\n\nDirk Goldhahn \nNatural Language Processing Group\nUniversity of Leipzig\nGermany Johannisgasse 2604103Leipzig\n\nThomas Eckart \nNatural Language Processing Group\nUniversity of Leipzig\nGermany Johannisgasse 2604103Leipzig\n\nUwe Quasthoff \nNatural Language Processing Group\nUniversity of Leipzig\nGermany Johannisgasse 2604103Leipzig\n\nBuilding Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages\ncorpus creationtext acquisitionminority languages\nThe Leipzig Corpora Collection offers free online access to 136 monolingual dictionaries enriched with statistical information. In this paper we describe current advances of the project in collecting and processing text data automatically for a large number of languages. Our main interest lies in languages of \"low density\", where only few text data exists online. The aim of this approach is to create monolingual dictionaries and statistical information for a high number of new languages and to expand the existing dictionaries, opening up new possibilities for linguistic typology and other research. Focus of this paper will be set on the infrastructure for the automatic acquisition of large amounts of monolingual text in many languages from various sources. Preliminary results of the collection of text data will be presented. The mainly language-independent framework for preprocessing, cleaning and creating the corpora and computing the necessary statistics will also be depicted.\n\nIntroduction\n\nThe Projekt Deutscher Wortschatz (Quasthoff, 1998) started more than 15 years ago by creating a corpus-based monolingual dictionary of the German language available at http://wortschatz.uni-leipzig.de. Since June 2006 Leipzig Corpora Collection (LCC) can be accessed at http://corpora.uni-leipzig.de (Biemann 2007;Quasthoff, 2006a). It offers corpus-based monolingual full form dictionaries of several languages and a Web interface for general access. These standard sized corpora and their corresponding statistics are created from newspaper texts or Web pages. For each word the dictionaries contain:\n\n\uf0b7 Word frequency information (no lemmatization, each word form is treated separately) \uf0b7 Sample sentences \uf0b7 Statistically significant word co-occurrences (based on left or right neighbours or whole sentences) \uf0b7 A semantic map visualizing the strongest word co-occurrences   In this paper we describe recent and current progress of LCC in collecting and processing text data automatically for a large number of languages. We aim mainly at languages of \"low density\", where only few text data exists online. Our goal is to increase the number of dictionaries and expand existing ones. Focus will be set on the infrastructure for the automatic acquisition of large amounts of monolingual text data in many languages from various Web sources. In addition the framework used for automatically preprocessing, cleaning and creating monolingual dictionaries and computing statistical information will also be presented. Most Web corpora projects concentrate on creating dictionaries for one language, possibly offering text of different genres or sources, like Corpuseye 1 or UKWeb 2 . There are few other projects concerned with collecting text for several languages or offering access to statistics for these languages. One of them is Web As Corpus 3 which allows queries for concordances of words or phrases in 34 languages. Web as Corpus creates no dictionaries, all queries are directly processed using the search engine Bing. As opposed to LCC no cleaning, sentence segmentation, further processing or statistical evaluation of text data takes place. One interesting venture is Cr\u00fabad\u00e1n 4 by Kevin Scannell (2007). Cr\u00fabad\u00e1n gathers documents for an enormous amount of languages using a bootstrapping approach. Currently ressources for 1023 languages exist. In Cr\u00fabad\u00e1n text data for nearly 60% of all languages consist of 5 or less documents. A comparison of text size for common languages of LCC and Cr\u00fabad\u00e1n can be found in figure 2.  Further differences between the projects are online access to statistics like co-occurrences, including a semantic map of the strongest co-occurrences, and example sentences offered by LCC and the possibility to download all these data for further academic use. An example of a co-occurrence graph for Galatasaray, a famous football club of Istanbul, is depicted in figure 3. In the last 6 months the number of corpora of LCC has already been increased significantly (see figure 4). To add more languages to the collection, an automatic processing pipeline for collecting and processing text data has been implemented.  \n\n\nCollecting Data\n\nIn order to build monolingual corpora for different languages, first of all text data has to be collected. The World Wide Web is an obvious source for electronically available text (Kilgarriff, 2001). Different collection methods for Web sites are currently employed in the project to achieve a high coverage for each language and a high diversity concerning topics or genres. This allows creation of corpora that are a more representative sample of the language in question. Preparation of subcorpora of different genres or sizes for later comparison is also possible.\n\n\nCrawling Newspapers\n\nOne approach for collecting text data is to crawl newspapers available online. Basis is a list of about 32,000 news sources in more than 120 languages provided by ABYZ News Links 5 . This service offers besides URLs also information regarding country and language. An overview of the distribution of ABYZ's resources is given in figure 5, while  \n\n\nCrawling Generic Web Pages\n\nAnother possibility to collect text data is to crawl the World Wide Web randomly. We achieve this by different approaches.\n\n\nFindLinks\n\nFindLinks 7 (Heyer and Quasthoff, 2004) is a distributed Web crawler which uses a client-server architecture (see figure 6). The Java-based client runs on any standard PC and processes a list of URLs, which is distributed by the FindLinks-server.\n\nOriginally, its purpose was to analyze the structure of the World Wide Web by analyzing links found on the Web sites. Meanwhile, the client additionally collects the 7 http://wortschatz.uni-leipzig.de/findlinks/ analyzed Web pages and sends them to the server. The client is provided on the Web site of Projekt Deutscher Wortschatz. Everybody is encouraged to use it in order to help the project to collect more text data. Currently about 10 users download more than 30 million Web sites each day. So far about one Terabyte of raw text has been downloaded, and every day several Gigabytes are added. Until now no corpora have been created on the basis of this data, opening up the possibility to add a reasonable number of corpora in the near future. \n\n\nStandard Web Crawler\n\nA list containing over 6.5 million internet domains has been created. By using the HTTrack based system complete domains are downloaded in parallel. Until now more than 80 Gigabytes of HTML-stripped text data have been collected.\n\n\nBootstrapping Corpora using Search Engines\n\nBy using an approach similar to Baroni (2004) and Sharoff (2006), frequent terms of a language are combined to form Google search queries and retrieve the resulting URLs as basis for the default download system. As a small set of frequent terms is needed for each language, the Universal Declaration of Human Rights (UDHR) 8 was used as a resource. It is available in more than 350 languages. For an average language, the UDHR contains about 2000 running words. Recently further languages were added by utilizing Watchtower 9 texts. Despite its partially controversial content, it offers contemporary documents in several hundred languages. Based on the lists of seeds tuples of three to five high frequent words are generated. These tuples are then used to query Bing and to collect the retrieved URLs. In a next step these Web sites are downloaded and further preprocessed.\n\n\nWikipedia\n\nFor more than 200 languages Wikipedia dumps were downloaded. Wikipedia Preprocessor 10 was used for further processing and text extraction resulting in 34 Gigabytes of text. Figure 7 depicts the number of articles in Wikipedia as a function of language rank. The top-20 languages can be found in table 3. So far dictionaries based on Wikipedia were compiled for about 70 languages. Smaller sources were excluded from further processing. This is motivated by the fact that many Wikipedias start off by creating many stub articles using boilerplates, resulting in mostly near duplicate sentences.\n\nIn order to utilize more of Wikipedia's languages, future work will include improved near duplicate detection.   \n\n\nProcessing and Cleaning of Text Data\n\nFurther steps to create dictionaries for multiple languages are HTML-stripping, language identification, sentence segmentation, cleaning, sentence scrambling, conversion into a text database and statistical evaluation. Because of the repetition due to the number of different languages, an automatic tool chain has been implemented. It is easily configurable and only minor language-dependent adjustments, concerning e.g. abbreviations or sentence boundaries, have to be made. Since complete evaluation by hand is not feasible, statistics-based quality assurance is necessary to achieve a satisfying quality of the resulting dictionaries (Eckart, 2012). With the help of features like character statistics, typical length distributions, typical character or n-gram distributions, or tests for conformity to well-known empirical language laws problems during corpora creation can be discovered and corrected.\n\n\nHTML-Stripping\n\nExcept for database dumps of Wikipedia, all collected data are HTML-coded. HTML-Tags, Javascript and other elements have to be removed from the documents. Therefore Html2Text, a HTML-stripping-tool of the NLP-group of the University of Leipzig, was utilized. The resulting text data still contains more than just well-formed sentences, so further cleaning steps follow.\n\n\nLanguage Identification\n\nFor most documents, only their URL is known, but it is necessary to separate the texts into different monolingual corpora. Here we use LangSepa (Pollm\u00e4cher, 2011), a tool built at the NLP group that identifies the language of a document. LangSepa compares the distribution of stop-words or character unigrams and character trigrams of various languages to the distribution within the document. Hence, statistical information concerning these distributions for a high number of languages is needed. If available, corpora of the LCC were used to acquire these statistics. In order to classify further languages the Universal Declaration of Human Rights and Watchtower were utilized, adding up to about 450 languages.\n\n\nSentence Segmentation\n\nThe next step is sentence segmentation. Not all languages have the same sentence boundary marks. Especially for languages using other writing systems than the Latin alphabet, information about possible sentence boundaries is needed. We utilized Web sites like www.sonderzeichen.de to generate a list which is as complete as possible. For better sentence boundary detection, abbreviation lists can be used. For several languages these lists already exist and more lists will be prepared. A possible source for such lists is Wikipedia.\n\n\nCleaning\n\nThe resulting elements are cleaned in two additional steps. First, non-sentences are identified based on patterns that a normal sentence should obey. All strings that do not comply with these patterns are removed. In a second step the purity of the text collection is enhanced by eliminating sentences that do not belong to the considered language (Quasthoff, 2006b). Subsequently, duplicate sentences are rejected. Especially newspaper reports may appear nearly unchanged on various Web sites. The same holds for standard boilerplates. To prevent a distortion of the statistical analysis, they are discarded.\n\n\nSentence Scrambling\n\nTo avoid copyright restrictions the collected sentences are \"scrambled\" by destroying the original structure of the documents to inhibit the reconstruction of the original material. With respect to German copyright legislation this approach is considered safe, because in Germany there is no copyright on single sentences.\n\n\nCreation of Text Databases and Statistics\n\nThe data is stored in a relational database to allow quick and generic access. Using statistical methods a full form dictionary with frequency information for each word is generated and enhanced with information about co-occurrence analysis. Also corpora of standardized sizes are created to allow for better comparison between different languages or sources.\n\nOnce the text databases have been created, they are made accessible online. Alternatively, the data can be downloaded from http://corpora.informatik.uni-leipzig.de /download.html and viewed locally with the Java Corpus-Browser (Richter, 2006).\n\n\nQuality Assurance\n\nQuality assurance is a major challenge as hundreds of corpora of up to millions of sentences can hardly be evaluated by hand (Eckart, 2012). So it must be a goal to do as little manual evaluation as possible. Accordingly, we identified general features of natural language texts that are possible indicators for corpus quality and can be computed in an acceptable time span. Based on these, further post-processing steps can be triggered which delete or correct the data. Possible features to be used include, for example:\n\n\uf0b7 typical length distributions (word, sentence, paragraph) \uf0b7 typical character or n-gram distributions \uf0b7 accordance with empirical language laws, like Zipf's law \uf0b7 character statistics (rare characters) Examples are shown in figures 8 and 9. For two corpora, sentence length distributions (in characters) were measured. Figure 8 is based on Hindi newspapers and depicts a characteristic distribution, while figure 9, based on the Sundanese Wikipedia, is atypical. On closer examination both peaks of the distribution are based on boilerplates or near duplicates which should be removed. Although work-intensive, manual evaluation is one possibility to further enhance quality. To achieve this, a questionnaire for native speakers of languages of our collection is currently being developed. \n\n\nOutlook\n\n\nFurther Work\n\nLarge amounts of text data have already been acquired in an ongoing process. The collected text data will be used to extend the collection both in the number of languages covered and in the size of resources provided. All corpora will be made freely accessible online in a uniform way.\n\nThere is still open work in fields like automatic and manual quality assurance.\n\n\nSpeed up of Crawling\n\nSo far the lists of URLs used for FindLinks and Web crawling contain all Top Level Domains (TLDs). Only the amount of URLs from TLDs of major countries (like .de or .com) is restricted to promote Web sites from smaller countries and hopefully rare languages. In the future this process could be adapted by favoring the TLDs of those countries where one or more languages with many speakers exist but no dictionary was created yet. A possible source of information concerning languages and the distribution of their speakers is Ethnologue 11 .\n\n\nConclusion\n\nIn this paper, we have described acquisition and processing of standard-sized monolingual dictionaries of the Leipzig Corpora Collection. Various sources, tools for downloading and further data processing steps have been introduced.\n\nFigure 1 :\n1Number of sentences for all languages of LCC ordered by rank (log-log scale).\n\nFigure 2 :\n2Number of documents for all common languages of LCC and Cr\u00fabad\u00e1n ordered by rank (log-log scale).\n\nFigure 3 :\n3Semantic map for Galatasaray, a football club of Istanbul.\n\nFigure 4 :\n4Number of available languages of the Leipzig Corpora Collection.\n\nFigure 6 :\n6Schematic representation of the functionality of the distributed Web crawler FindLinks.\n\nFigure 7 :\n7Number of articles for all languages ofWikipedia ordered by rank (log-log scale).\n\nFigure 8 :Figure 9 :\n89Sentence length distribution for Hindi newspapers (percentage for number of characters) Sentence length distribution for Sundanese Wikipedia (percentage for number of characters)\n\nTable 1 :\n1Monolingual dictionaries accessible online at http://corpora.uni-leipzig.de.\n\n\ntable 2 depicts the top-20 languages.A framework for parallel Web crawling utilizing the standard Web site copier HTTrack 6 is applied. News sources of about 100 languages have been downloaded, yielding more than 250 GB of text after HTML-stripping but before language identification. We were able to compile monolingual dictionaries for 70 languages based on this data. For the remaining languages the resources of ABYZ News Links either contained no text in the expected language or our crawler was excluded from downloading Web sites by the robots.txt of these Servers. Number of domains for all languages of ABYZ News Links ordered by rank (log-log scale).5 \nhttp://www.abyznewslinks.com \n6 \nhttp://www.httrack.com \n\nFigure 5: Rank \nDomains \nISO 639-3 \nLanguage \n1 \n20383 \neng \nEnglish \n2 \n3550 \nspa \nSpanish \n3 \n1722 \npor \nPortuguese \n4 \n1496 \nfra \nFrench \n5 \n800 \ndeu \nGerman \n6 \n773 \nrus \nRussian \n7 \n470 \nara \nArabic \n8 \n428 \nzho \nChinese \n9 \n291 \nnor \nNorwegian \n10 \n257 \nswe \nSwedish \n11 \n223 \nnld \nDutch \n12 \n207 \nita \nItalian \n13 \n194 \nron \nRomanian \n14 \n191 \nfin \nFinnish \n15 \n160 \ndan \nDanish \n16 \n140 \njpn \nJapanese \n17 \n137 \nsrp \nSerbian \n18 \n134 \nces \nCzech \n19 \n133 \nkor \nKorean \n20 \n128 \ntur \nTurkish \n\n\n\nTable 2 :\n2Top-20 languages of ABYZ News Links considering number of domains.\n\nTable 3 :\n3Number of articles of the top-20 languages of Wikipedia.\nhttp://www.ohchr.org 9 http://www.watchtower.org 10 http://sourceforge.net/projects/wikiprep/\nhttp://www.ethnologue.com\n\nBootCaT: Bootstrapping corpora and terms from the web. M Baroni, S Bernardini, Proceedings of LREC. LRECBaroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping corpora and terms from the web. Proceedings of LREC 2004.\n\nThe Leipzig Corpora Collection -Monolingual corpora of standard size. C Biemann, G Heyer, U Quasthoff, M Richter, Proceedings of Corpus Linguistic. Corpus LinguisticBirmingham, UKBiemann, C.; Heyer, G.; Quasthoff, U.; Richter, M. (2007). The Leipzig Corpora Collection -Monolingual corpora of standard size. Proceedings of Corpus Linguistic 2007, Birmingham, UK.\n\nLanguage Statistics-Based Quality Assurance for Large Corpora. T Eckart, U Quasthoff, D Goldhahn, Proceedings of Asia Pacific Corpus Linguistics Conference. Asia Pacific Corpus Linguistics ConferenceAuckland, New ZealandEckart, T.; Quasthoff, U.; Goldhahn, D. (2012). Language Statistics-Based Quality Assurance for Large Corpora. Proceedings of Asia Pacific Corpus Linguistics Conference 2012, Auckland, New Zealand.\n\nCalculating Communities by Link Analysis of URLs. G Heyer, U Quasthoff, Proceedings of IICS-04. IICS-04Guadalajara, MexicoSpringer LNCS 3473Heyer, G.; Quasthoff, U. (2004). Calculating Communities by Link Analysis of URLs. Proceedings of IICS-04, Guadalajara, Mexico and Springer LNCS 3473.\n\nWeb as Corpus. A Kilgarriff, G Grefenstette, Proceedings of Corpus Linguistics 2001 Conference. Corpus Linguistics 2001 ConferenceLancasterKilgarriff, A.; Grefenstette, G. (2001). Web as Corpus. In Proceedings of Corpus Linguistics 2001 Conference, Lancaster.\n\nSeparierung mit FindLinks gecrawlter Texte nach Sprachen. J Pollm\u00e4cher, University of LeipzigBachelor ThesisPollm\u00e4cher, J. (2011). Separierung mit FindLinks gecrawlter Texte nach Sprachen. Bachelor Thesis, University of Leipzig.\n\nProjekt der deutsche Wortschatz. U Quasthoff, Linguistik und neue Medien. Heyer, G., Wolff, Ch.WiesbadenQuasthoff, U. (1998). Projekt der deutsche Wortschatz. Heyer, G., Wolff, Ch. (eds.), Linguistik und neue Medien, Wiesbaden, pp. 93-99.\n\nCorpus Portal for Search in Monolingual Corpora. U Quasthoff, M Richter, C Biemann, Proceedings of LREC-06. LREC-06Quasthoff, U.; Richter, M.; Biemann, C. (2006a). Corpus Portal for Search in Monolingual Corpora. Proceedings of LREC-06.\n\nU Quasthoff, C Biemann, Measuring Monolinguality. Proceedings of LREC-06 workshop on Quality assurance and quality measurement for language and speech resources. Quasthoff, U.; Biemann, C. (2006b). Measuring Monolinguality. Proceedings of LREC-06 workshop on Quality assurance and quality measurement for language and speech resources.\n\nExploiting the Leipzig Corpora Collection. M Richter, U Quasthoff, E Hallsteinsd\u00f3ttir, C Biemann, Proceedings of the Information Society Language Technologies Conference (IS-LTC). the Information Society Language Technologies Conference (IS-LTC)Ljubljana, SloveniaRichter, M.; Quasthoff, U.; Hallsteinsd\u00f3ttir, E.; Biemann, C. (2006). Exploiting the Leipzig Corpora Collection. Proceedings of the Information Society Language Technologies Conference (IS-LTC), Ljubljana, Slovenia.\n\nThe Cr\u00fabad\u00e1n Project: Corpus building for under-resourced languages. K Scannell, Proc. WAC-3: Building and Exploring Web Corpora. WAC-3: Building and Exploring Web CorporaLouvain-la-Neuve; BelgiumScannell, K. (2007). The Cr\u00fabad\u00e1n Project: Corpus building for under-resourced languages. In Proc. WAC-3: Building and Exploring Web Corpora, Louvain-la-Neuve, Belgium.\n\nCreating general-purpose corpora using automated search engine queries. S Sharoff, WaCky! Working papers on the Web as Corpus. Gedit. M. Baroni and S. BernardiniBolognaSharoff, S. (2006). Creating general-purpose corpora using automated search engine queries. In M. Baroni and S. Bernardini, editors, WaCky! Working papers on the Web as Corpus. Gedit, Bologna.\n", "annotations": {"author": "[{\"end\":212,\"start\":104},{\"end\":321,\"start\":213},{\"end\":430,\"start\":322}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":109},{\"end\":226,\"start\":220},{\"end\":335,\"start\":326}]", "author_first_name": "[{\"end\":108,\"start\":104},{\"end\":219,\"start\":213},{\"end\":325,\"start\":322}]", "author_affiliation": "[{\"end\":211,\"start\":119},{\"end\":320,\"start\":228},{\"end\":429,\"start\":337}]", "title": "[{\"end\":101,\"start\":1},{\"end\":531,\"start\":431}]", "venue": null, "abstract": "[{\"end\":1575,\"start\":582}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1641,\"start\":1624},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1905,\"start\":1891},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1922,\"start\":1905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3805,\"start\":3790},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4968,\"start\":4950},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5913,\"start\":5887},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7220,\"start\":7207},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7239,\"start\":7225},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9466,\"start\":9452},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10299,\"start\":10281},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11788,\"start\":11771},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13027,\"start\":13012},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13189,\"start\":13175}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15663,\"start\":15573},{\"attributes\":{\"id\":\"fig_1\"},\"end\":15774,\"start\":15664},{\"attributes\":{\"id\":\"fig_2\"},\"end\":15846,\"start\":15775},{\"attributes\":{\"id\":\"fig_3\"},\"end\":15924,\"start\":15847},{\"attributes\":{\"id\":\"fig_4\"},\"end\":16025,\"start\":15925},{\"attributes\":{\"id\":\"fig_5\"},\"end\":16120,\"start\":16026},{\"attributes\":{\"id\":\"fig_6\"},\"end\":16323,\"start\":16121},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":16412,\"start\":16324},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":17637,\"start\":16413},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":17716,\"start\":17638},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":17785,\"start\":17717}]", "paragraph": "[{\"end\":2193,\"start\":1591},{\"end\":4749,\"start\":2195},{\"end\":5338,\"start\":4769},{\"end\":5708,\"start\":5362},{\"end\":5861,\"start\":5739},{\"end\":6121,\"start\":5875},{\"end\":6874,\"start\":6123},{\"end\":7128,\"start\":6899},{\"end\":8050,\"start\":7175},{\"end\":8658,\"start\":8064},{\"end\":8773,\"start\":8660},{\"end\":9721,\"start\":8814},{\"end\":10109,\"start\":9740},{\"end\":10851,\"start\":10137},{\"end\":11410,\"start\":10877},{\"end\":12032,\"start\":11423},{\"end\":12378,\"start\":12056},{\"end\":12783,\"start\":12424},{\"end\":13028,\"start\":12785},{\"end\":13572,\"start\":13050},{\"end\":14365,\"start\":13574},{\"end\":14677,\"start\":14392},{\"end\":14758,\"start\":14679},{\"end\":15325,\"start\":14783},{\"end\":15572,\"start\":15340}]", "formula": null, "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1589,\"start\":1577},{\"attributes\":{\"n\":\"2.\"},\"end\":4767,\"start\":4752},{\"attributes\":{\"n\":\"2.1\"},\"end\":5360,\"start\":5341},{\"attributes\":{\"n\":\"2.2\"},\"end\":5737,\"start\":5711},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":5873,\"start\":5864},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":6897,\"start\":6877},{\"attributes\":{\"n\":\"2.3\"},\"end\":7173,\"start\":7131},{\"attributes\":{\"n\":\"2.4\"},\"end\":8062,\"start\":8053},{\"attributes\":{\"n\":\"3.\"},\"end\":8812,\"start\":8776},{\"attributes\":{\"n\":\"3.1\"},\"end\":9738,\"start\":9724},{\"attributes\":{\"n\":\"3.2\"},\"end\":10135,\"start\":10112},{\"attributes\":{\"n\":\"3.3\"},\"end\":10875,\"start\":10854},{\"attributes\":{\"n\":\"3.4\"},\"end\":11421,\"start\":11413},{\"attributes\":{\"n\":\"3.5\"},\"end\":12054,\"start\":12035},{\"attributes\":{\"n\":\"3.6\"},\"end\":12422,\"start\":12381},{\"attributes\":{\"n\":\"3.7\"},\"end\":13048,\"start\":13031},{\"attributes\":{\"n\":\"4.\"},\"end\":14375,\"start\":14368},{\"attributes\":{\"n\":\"4.1\"},\"end\":14390,\"start\":14378},{\"attributes\":{\"n\":\"4.2\"},\"end\":14781,\"start\":14761},{\"attributes\":{\"n\":\"5.\"},\"end\":15338,\"start\":15328},{\"end\":15584,\"start\":15574},{\"end\":15675,\"start\":15665},{\"end\":15786,\"start\":15776},{\"end\":15858,\"start\":15848},{\"end\":15936,\"start\":15926},{\"end\":16037,\"start\":16027},{\"end\":16142,\"start\":16122},{\"end\":16334,\"start\":16325},{\"end\":17648,\"start\":17639},{\"end\":17727,\"start\":17718}]", "table": "[{\"end\":17637,\"start\":17075}]", "figure_caption": "[{\"end\":15663,\"start\":15586},{\"end\":15774,\"start\":15677},{\"end\":15846,\"start\":15788},{\"end\":15924,\"start\":15860},{\"end\":16025,\"start\":15938},{\"end\":16120,\"start\":16039},{\"end\":16323,\"start\":16145},{\"end\":16412,\"start\":16336},{\"end\":17075,\"start\":16415},{\"end\":17716,\"start\":17650},{\"end\":17785,\"start\":17729}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4503,\"start\":4495},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":4609,\"start\":4601},{\"end\":5699,\"start\":5691},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":5997,\"start\":5989},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":8246,\"start\":8238},{\"end\":13902,\"start\":13894}]", "bib_author_first_name": "[{\"end\":17963,\"start\":17962},{\"end\":17973,\"start\":17972},{\"end\":18199,\"start\":18198},{\"end\":18210,\"start\":18209},{\"end\":18219,\"start\":18218},{\"end\":18232,\"start\":18231},{\"end\":18556,\"start\":18555},{\"end\":18566,\"start\":18565},{\"end\":18579,\"start\":18578},{\"end\":18962,\"start\":18961},{\"end\":18971,\"start\":18970},{\"end\":19219,\"start\":19218},{\"end\":19233,\"start\":19232},{\"end\":19523,\"start\":19522},{\"end\":19728,\"start\":19727},{\"end\":19984,\"start\":19983},{\"end\":19997,\"start\":19996},{\"end\":20008,\"start\":20007},{\"end\":20173,\"start\":20172},{\"end\":20186,\"start\":20185},{\"end\":20553,\"start\":20552},{\"end\":20564,\"start\":20563},{\"end\":20577,\"start\":20576},{\"end\":20597,\"start\":20596},{\"end\":21060,\"start\":21059},{\"end\":21429,\"start\":21428}]", "bib_author_last_name": "[{\"end\":17970,\"start\":17964},{\"end\":17984,\"start\":17974},{\"end\":18207,\"start\":18200},{\"end\":18216,\"start\":18211},{\"end\":18229,\"start\":18220},{\"end\":18240,\"start\":18233},{\"end\":18563,\"start\":18557},{\"end\":18576,\"start\":18567},{\"end\":18588,\"start\":18580},{\"end\":18968,\"start\":18963},{\"end\":18981,\"start\":18972},{\"end\":19230,\"start\":19220},{\"end\":19246,\"start\":19234},{\"end\":19534,\"start\":19524},{\"end\":19738,\"start\":19729},{\"end\":19994,\"start\":19985},{\"end\":20005,\"start\":19998},{\"end\":20016,\"start\":20009},{\"end\":20183,\"start\":20174},{\"end\":20194,\"start\":20187},{\"end\":20561,\"start\":20554},{\"end\":20574,\"start\":20565},{\"end\":20594,\"start\":20578},{\"end\":20605,\"start\":20598},{\"end\":21069,\"start\":21061},{\"end\":21437,\"start\":21430}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15701997},\"end\":18126,\"start\":17907},{\"attributes\":{\"id\":\"b1\"},\"end\":18490,\"start\":18128},{\"attributes\":{\"id\":\"b2\"},\"end\":18909,\"start\":18492},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":29775915},\"end\":19201,\"start\":18911},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8506047},\"end\":19462,\"start\":19203},{\"attributes\":{\"id\":\"b5\"},\"end\":19692,\"start\":19464},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5267393},\"end\":19932,\"start\":19694},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17089484},\"end\":20170,\"start\":19934},{\"attributes\":{\"id\":\"b8\"},\"end\":20507,\"start\":20172},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14592908},\"end\":20988,\"start\":20509},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10481641},\"end\":21354,\"start\":20990},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2825308},\"end\":21716,\"start\":21356}]", "bib_title": "[{\"end\":17960,\"start\":17907},{\"end\":18196,\"start\":18128},{\"end\":18553,\"start\":18492},{\"end\":18959,\"start\":18911},{\"end\":19216,\"start\":19203},{\"end\":19725,\"start\":19694},{\"end\":19981,\"start\":19934},{\"end\":20550,\"start\":20509},{\"end\":21057,\"start\":20990},{\"end\":21426,\"start\":21356}]", "bib_author": "[{\"end\":17972,\"start\":17962},{\"end\":17986,\"start\":17972},{\"end\":18209,\"start\":18198},{\"end\":18218,\"start\":18209},{\"end\":18231,\"start\":18218},{\"end\":18242,\"start\":18231},{\"end\":18565,\"start\":18555},{\"end\":18578,\"start\":18565},{\"end\":18590,\"start\":18578},{\"end\":18970,\"start\":18961},{\"end\":18983,\"start\":18970},{\"end\":19232,\"start\":19218},{\"end\":19248,\"start\":19232},{\"end\":19536,\"start\":19522},{\"end\":19740,\"start\":19727},{\"end\":19996,\"start\":19983},{\"end\":20007,\"start\":19996},{\"end\":20018,\"start\":20007},{\"end\":20185,\"start\":20172},{\"end\":20196,\"start\":20185},{\"end\":20563,\"start\":20552},{\"end\":20576,\"start\":20563},{\"end\":20596,\"start\":20576},{\"end\":20607,\"start\":20596},{\"end\":21071,\"start\":21059},{\"end\":21439,\"start\":21428}]", "bib_venue": "[{\"end\":18011,\"start\":18007},{\"end\":18307,\"start\":18276},{\"end\":18712,\"start\":18649},{\"end\":19033,\"start\":19007},{\"end\":19342,\"start\":19299},{\"end\":19798,\"start\":19789},{\"end\":20049,\"start\":20042},{\"end\":20773,\"start\":20689},{\"end\":21186,\"start\":21120},{\"end\":21524,\"start\":21517},{\"end\":18005,\"start\":17986},{\"end\":18274,\"start\":18242},{\"end\":18647,\"start\":18590},{\"end\":19005,\"start\":18983},{\"end\":19297,\"start\":19248},{\"end\":19520,\"start\":19464},{\"end\":19766,\"start\":19740},{\"end\":20040,\"start\":20018},{\"end\":20332,\"start\":20196},{\"end\":20687,\"start\":20607},{\"end\":21118,\"start\":21071},{\"end\":21488,\"start\":21439}]"}}}, "year": 2023, "month": 12, "day": 17}