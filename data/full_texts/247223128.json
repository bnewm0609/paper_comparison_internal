{"id": 247223128, "updated": "2023-10-05 22:43:54.217", "metadata": {"title": "M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining", "authors": "[{\"first\":\"Xiao\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Xunlin\",\"last\":\"Zhan\",\"middle\":[]},{\"first\":\"Yangxin\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yunchao\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Kampffmeyer\",\"middle\":[\"C.\"]},{\"first\":\"Xiaoyong\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Minlong\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Yaowei\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xiaodan\",\"last\":\"Liang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets. By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic information, we contribute a large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5 modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500 larger than the largest publicly available dataset with a similar number of modalities. Furthermore, M5Product contains incomplete modality pairs and noise while also having a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework that integrates the different modalities into a unified model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality contrastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset. We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, providing insights into the importance of dataset scale and diversity.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.04275", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/DongZWWKWLWL22", "doi": "10.1109/cvpr52688.2022.02057"}}, "content": {"source": {"pdf_hash": "59ba12da3b9ee2d9ee829145cf6e061d772805ac", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.04275v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "21883b40e9550a12e2c464d925e651153b310253", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/59ba12da3b9ee2d9ee829145cf6e061d772805ac.txt", "contents": "\nM5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining\n\n\nXiao Dong \nSun Yat-sen University\n\n\nXunlin Zhan \nShenzhen Campus of Sun Yat-sen University\n\n\n\u2020 \nYangxin Wu \nSun Yat-sen University\n\n\nYunchao Wei \nBeijing Jiaotong University\n\n\nMichael C Kampffmeyer michael.c.kampffmeyer@uit.no \nArctic University of Norway\n\n\nXiao-Yong Wei \nPengCheng Laboratory\n\n\nMinlong Lu \nAlibaba Group\n\n\nYaowei Wang wangyw@pcl.ac.cn \nPengCheng Laboratory\n\n\nXiaodan Liang xdliang328@gmail.com \nShenzhen Campus of Sun Yat-sen University\n\n\nM5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining\n\nDespite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets. By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic information, we contribute a large-scale multi-modal pretraining dataset M5Product. The dataset comprises 5 modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500\u00d7 larger than the largest publicly available dataset with a similar number of modalities. Furthermore, M5Product contains incomplete modality pairs and noise while also having a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework that integrates the different modalities into a unified model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality contrastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset. We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, providing insights into the importance of dataset scale and diversity. Dataset and codes are available at 1 .\n Table   Text  Image \nAudio Video Figure 1. Our M5Product dataset contains a large variety of modalities (image, text, table, video and audio) that depict the categories, descriptions, materials, properties and purposes of Ecommerce products, and diverse real-world data samples.\n\n\nIntroduction\n\nSelf-supervised learning has been driving the rapid development of fields such as computer vision and natural language processing, as well as research on multi-modal representation learning. In particular, it has been shown both from a theoretical [18] and a practical [16,58] perspective that large scale datasets with diverse modalities can effectively enhance the discrimination of generated features and thus improve the performance in vision-language tasks. However, current advances are severely limited by the lack of such large-scale diverse-modality datasets, with the largest public multi-modal datasets only containing text and image modalities and no category information [41].\n\nGiven the prevalence of online shopping in daily life, with its natural occurrence of multi-modal information and diverse categories, multi-modal pre-training on Ecommercial products has received increasing attention and led the developments of next-generation technology for several downstream tasks (e.g., multi-modal retrieval, multimodal classification, and clustering). However, even among the present product datasets (e.g., RPC checkout [48], Dress Retrieval [9] and Product1M [55]), the number of categories is insufficient to robustly verify the performance of downstream tasks.\n\nMore importantly, the current research community mostly focuses on two modalities (text and image) in both general multi-modal and E-commerce datasets, while ignoring the importance of additional complementary information from structural data as well as video and audio modalities. Tabular data for instance can provide detailed information about properties and characteristics, such as brand, materials, attributes, and scenarios, while audio and video can convey different perspectives, scales, affordances, selling points, characteristics, and use scenarios that are not obvious from images or text alone. The focus on these two modalities is partly due to the lack of datasets with diverse modalities as well as an under-exploration of approaches to balance the modality importance in these settings. In particular, two key challenges are: 1) Modality Interaction: How to learn common representations from unimodal, bimodal, trimodal, and even multi-modal relationships between different modalities using an elegant approach that scales to a large number of modalities; 2) Modality Noise: How to reduce the influence of modality noise (missing and incompleted modalties) during the training process.\n\nTo address the problem of insufficient modality diversity and limited scale, while at the same time providing a challenging real-world scenario, we present a very largescale E-commerce multi-modal product dataset M5Product, which is one of the largest and most diverse multi-modal product dataset to date. Our M5Product dataset contains more than 6 million multi-modal samples from 6,232 categories and has more complex and diverse modalities than existing datasets. This allows M5Product to be used for more comprehensive evaluation of the practical application and generalization abilities of multi-modal pretraining models and can improve the modality fusion performance, facilitating new directions in multimodal research. Figure 1 shows the five modalities (image, caption, video, audio, and specification (table)) of our dataset.\n\nTo further address the modality fusion limitations of existing methods as well as handle modality noise, we propose a generic framework that takes five-modality data as inputs, as shown in Figure 2. The framework consists of a simple and efficient multi-modal five stream pre-training model named Self-harmonized ContrAstive LEarning (SCALE) and is evaluated on several downstream tasks and compared with several recent state-of-the-art vision-language models [7,27,30,38,42,45,56]. SCALE increases modality alignment effectiveness by implementing a self-harmonized strategy that adapts the alignment weights between different modalities in the contrastive learning modules and masked tasks to adaptively integrate complementary modality information. In summary, our contributions are as follows:\n\n\u2022 We provide the largest five-modality E-commerce dataset M5Product. Through its large scale, diversity, complex real scenarios and number of modalities, M5Product provides a comprehensive environment for evaluating the generalization performance of multi-modal pre-training models.\n\n\u2022 Our Self-harmonized Contrastive Learning (SCALE) framework learns adaptive modality interactions, resulting in more effective modality fusion. We compare SCALE to a comprehensive set of baseline methods and demonstrate its superior performance on the M5Product dataset.\n\n\u2022 \n\n\nRelated Work\n\nMulti-modal pre-training datasets. Most multi-modal pre-training datasets are collected from social websites (e.g., Twitter and Facebook) and are limited to just two modalities collected for specified tasks. These datasets can be divided into four categories according to their modality composition, i.e., audio/text, video/text, image/text, and others. Among these, LJ Speech [19] and SQuAD [25] are classical audio/text datasets and used for voice synthesis and audio Q&A, while most video/text datasets [2,20,24,32,46,47,51,57] are used for video Q&A. However, these datasets commonly only contain a limited number of samples, limiting their applicability to multi-modal pretraining. Image/text datasets [1,4,8,17,22,23,29,34,41,43,48,53], on the other hand, tend to be larger and have been widely used for pretraining multi-modal models. Among these, the CC 3M [41] with more than three million image-text pairs is the most widely used pre-training dataset, and has recently been expanded to CC 12M [5], the largest text-image cross-  [54] and XMedia [36], where CMU-MOSEI mainly focuses on the emotional analysis and XMedia is utilized for cross-modal retrieval. Aside from the abovementioned datasets, there exist several E-commerce datasets. The Dress Retrieval [9], RPC checkout [48] and Product1M [55] are typical E-commerce multi-modal datasets. The Dress Retrieval dataset contains 20,200 samples from 50 clothing categories, RPC checkout offers 30,000 samples of small retail goods on simple backgrounds and Product1M provides 1.18 million samples from 458 cosmetics classes. Compared with these three datasets, our M5Product is not only larger in terms of categories and data scale, but also contains a more diverse set of modalities. A detailed comparison with other multi-modal pre-training datasets is provided in Table 1.\n\nMulti-modal pre-training for E-commerce products. Several vision-language pre-training models have been explored for visual-text multi-modal learning in recent years. They can coarsely be divided into two categories: 1) Singlestream models whose transformer layer operates collectively on the concatenation of the visual and text inputs, e.g, VL-bert [42], Image-BERT [37], VideoBERT [44], MMT [12], HERO [26], VisualBERT [27] and UNITER [7]. 2) Dual-stream models whose image and text inputs are not concatenated, such as ViLBERT [30], LXMERT [45], CLIP [38] and DALL-E [39].\n\nWithin E-commerce, fashion-based tasks have been addressed in among others FashionBERT [13], MAAF [11], Kaleido-BERT [59], M6 [28] and CAPTURE [55]. All existing studies in the E-commerce scenarios focus solely on the image and text modalities and none of the approaches can utilize more modalities. Besides, all existing methods default to assigning the same contribution to different modalities when modeling multi-modal interactions.More  Table   specifically, transformer-based approaches combine highlevel features that are extracted from the different inputs via concatenation, where the uni-modal transformers are trained via masked task constraints or via constructing intermodality losses between different modalities. This restricts the models from effectively prioritizing modalities and tends to limit performance improvements as the number of modalities increases.\n\nOur proposed benchmark fills this gap by exploiting all the diverse modalities of the M5Product dataset and provides a strong baseline for multi-modal pre-training research in the field of E-commerce and beyond.\n\n\nM5Product Dataset\n\nData Collections. The dataset is crawled from a popular E-commerce website 2 . and the front page of each Ecommerce product is analyzed to collect the multi-modal information consisting of product images, captions, videos, and specifications (table information) 3 . Duplicate data is removed and audio information is extracted from videos via the moviepy 4 tool and saved in mp3 format. For product specifications, we extract 5,679 product properties and 24,398,673 values to construct a table database coarsely labeled by e-commerce merchants. After processing, the dataset contains 6,313,067 samples. Note, being a realworld dataset, our M5Product is, unlike traditional multimodal datasets, not a complete paired dataset and contains samples with only a subset of modalities as well as longtailed distributed ( Figure 3). We summarize the product characteristics that are relayed by the different modalities in our dataset in Table 2, where APP, USA, SPEC, SELL, PROD, MATE and CATE denote Appearance, Usage, Specification, Selling Point, Production, Material and Category Descriptions, respectively. Quantitative analysis. 1) Diversity: The dataset consists of more than 6,000 classes covering various and massive amounts of E-commerce products such as clothes, cosmetics, and instruments. Figure 1 illustrates the diversity of the modalities and categories and we further provide a description of the data format and the collection process in Section E of the supplementary materials. Finally, a quantita- 2 We are authorised by the company to access and obtain the data. We are further authorised to share the dataset and the detailed license is given in Section A of the supplementary material 3 In this work we focus on core data modalities (image, text, video, audio, and table data) only and do not consider extracted feature representations such as OCR and Motion embeddings that are extracted from core modalities as separate modalities.  Figure 2. An illustration of our M5Product benchmark. It consists of a five-modality E-commerce dataset with a more diverse and complex backgrounds collected from the real-world online-shopping website. It also proposes a SCALE model to capture the maximum modality complementary information for four common downstream tasks: 1) multi-modal retrieval, 2) fine-grained retrieval, 3) multimodal classification, and 4) multi-modal clustering. The benchmark verifies the effectiveness of modality diversity in five widely used modalities.  tive analysis of the category and modality distributions can be found in Section F. Note that about 5% of products are unimodal samples e.g. only either contain images, captions, or tabular properties. 2) Quality: We further provide a comparison between our M5Product dataset and some widelyused datasets for multi-modal pre-training in Table 1. A more extensive comparison with other multi-modal datasets can be found in Section H of the supplementary materials. Compared with existing multi-modal datasets, M5Product is the first extremely large public real-world E-commerce product dataset that contains data of more than two modalities. Moreover, our dataset contains a large amount of instances, i.e., more than six million samples from the 6,232 coarse categories. These abundant data will benefit several downstream tasks such as self-learning, weakly-supervised learning, multi-modal retrieval, cross-modal generation and fine-grained recognition. Additional analysis. In the supplementary materials, we provide dataset collection details in Section B and detail how the dataset is split into training and test in Section D and how annotations are obtained in Section C. We further provide a smaller split, referred to as subset, which is used to show the difference in performance for a smaller dataset. Finally, we provide further insights into the composition of the dataset (missing modalities, unimodal data analysis, and data format) in supplementary Section F.\n\n\nOur Methodology\n\nAs shown in Figure 2, our SCALE framework consists of a self-harmonized contrastive learning module and a selfsupervised multi-modal transformer. In this section, we first provide the architectural design of SCALE in Section 4.1 before describing the five masked tasks that enable the selfsupervised learning of SCALE in Section 4.2. Finally, we present the detailed learning process of SCALE and detail how multi-modal alignment is achieved in Section 4.3.\n\n\nArchitectural Design of SCALE\n\nAs depicted in Figure 2, SCALE is a typical singlestream transformer architecture. In the bottom part, the Image/Text/Table/Video/Audio embedding layers and transformers aim to extract modality features and generate token features. Specifically, the text and table encoders are standard transformers to encode the caption and table information of products, respectively. The image encoder instead takes proposals extracted by bottom-up-attention [3] as inputs, while ordinal frames sampled from the video are fed into the video encoder. For the audio encoder, SCALE extracts MFCC [33] features from audio. After being processed by the separate modality encoders, the token features of different modalities are concatenated and fed into a Joint Co-Transformer (JCT) module to capture the token relationships between different modalities. Missing Modalities. Zero imputation of missing modalities is leveraged to utilize all available data when training SCALE. We provide experimental evidence that SCALE benefits from the incomplete samples in Section I of the supplementary material.\n\n\nSCALE by Masked Multi-Modal Tasks\n\nSimilar to previous works, we utilize several pretext tasks (PRE) to facilitate self-supervised learning of SCALE in the Joint Co-Transformer module. For modalitywise feature learning from the image and text modalities, we adopt the Masked Region Prediction task (MRP) and the Masked Language Modeling task (MLM), respectively, after the JCT. Utilizing the characteristics of the table, video, and audio modalities, we further propose a Mask Entity Modeling task (MEM), Mask Frame Prediction task (MFP), and Mask Audio Modeling task (MAM) following a similar strategy of predicting masked tokens. In all masked tasks, the ground-truth labels are the features of masked areas. For all masking tasks, 15% of the inputs are masked out and the remaining inputs are used to reconstruct the masked information. Please note that unlike in the MLM task, where 15% of individual words are masked, 15% of the entities (properties, brand names, etc.) are entirely masked out for the MEM task. This drives our model to learn better table representations to recover the masked inputs, which is illustrated in Section 5.3. The loss function of the ith modality is defined as:\nL Mi (\u03b8) = \u2212E t msk \u223ct log P \u03b8 (t msk | t \u00acmsk , M \u00aci ) ,(1)\nwhere t \u00acmsk denotes the unmasked tokens surrounding the masked token t msk , \u03b8 represents the network parameters, and M i and M \u00aci are the ith modality and the remaining modalities, respectively.\n\n\nSelf-harmonized Inter-Modality Contrastive Learning\n\nSelf-harmonized Inter-Modality Contrastive Learning (SIMCL) is at the core of our proposed SCALE framework. It aims to facilitate the semantic alignment between different modalities via a self-harmonized strategy for adaptive Inter-Modality Contrastive Learning (IMCL). For a minibatch of modality samples D \u2208 R B\u00d7M \u00d7F , where B, M and F denote the batch size, number of modalities, and embedding dimension, respectively, we first construct the contrastive loss between each modality.\n\n\nPositive Negatives\n\n\nContrastive Learning\n\n\nInter-Modality Contrastive Learning\n\nInter-Modality Scores \nGiven N data samples {(d (0) i , d (1) i )} N i=1 ,\nwhere each sample has two modalities (0) and (1) with the remaining N -1 samples from the other modality, resulting in 2(N -1) negative pairs. For a modality pair (d\n(0) i , d (1) i ) and their embed- ding features (f (0) i , f(1)\ni ), the cross-modal contrastive loss of each modality pair is:\nL CL (d (0) i , d (1) i ) = \u2212 log exp sim f (0) i ,f (1) i /\u03c4 1 m=0 N k=1 1 [k =i] exp sim f (m) i ,f (1\u2212m) k /\u03c4 ,(2)\nwhere sim is the cosine similarity, \u03c4 is the temperature parameter and 1 [k =i] is a binary indicator function, and 1=1 for k = i and 0 otherwise.\n\nIn most prior work, only two modalities are considered and Eq. 2 can be used. However, when considering trimodal data or data with even more than three modalities, it is not suitable to directly fit the loss function as it does not account for the difference in complementary information that different modalities contribute. To solve this problem, we define a simple but effective self-harmonized method to model the complementary process of the inter-modal relationships. We introduce a modality alignment score matrix, to encode the relationships among the inter-modal losses L CL and the intra-modal losses L Mi . The alignment score matrix S for each data sample is initialized by a zero matrix and updated as free model parameters. To obtain modality importance scores for each modality combination, we apply the softmax function to S. Finally, the importance scores are multiplied to generate the modality alignment score S as S = S \u00b7 softmax(S). The learning process is shown in Figure 4 and illustrates that SIMCL takes full advantage of the inter-modal relationships. Given the modality alignment score S, the triangular part S is selected to weight the inter-modal losses L CL and the diagonal part S \\ is uti-lized to constrain the intra-modal losses L Mi , resulting in the weighted loss:\nL total = S Si,j L CLi,j (S i,j logit i,j ) + S \\ Si L Mi (S i logit i ) (3)\nwhere logit is the the loss logit.\n\n\nExperiments\n\nImplementation Details. We use BERT [10] to initialize the text transformer of our proposed SCALE framework, while the remaining transformers are randomly initialized. Both the single-modality encoders and the multi-modal fusion encoders consist of 6 transformer layers each, adding up to a total of 12 transformer layers. The hidden state size of each modality transformer is 768 and the maximum sequence length for the captions and tables are set to 36 and 64, respectively. Using the same setting as in [30] 5 , we utilize Faster R-CNN [40] with a backbone ResNet101 [15] pre-trained on the Visual Genome dataset [23] to extract region features of selected 10 to 36 bounding boxes with high-class detection probability for each image. We train SCALE with a total batch size of 64 for 5 epochs using the Adam optimizer [21] with a warm-up learning rate of 1e-4. Additional implementation details of our model are provided in Section G of the supplementary material. Baselines. We compare SCALE to the following eight alternative pre-training methods that utilize image and text modalities as well as combinations of both: Bert [10] (Text based ), Image based , ViLBERT [30], CLIP [38], VL-BERT [42], VisualBERT [27], UNITER [7] and CAP-TURE [56]. Image based and BERT [10] are 12-layer transformers based on the MLM (Mask Language Modeling) or MRP (Mask Region Prediction) task using image or text modality, providing single-modal baselines for the product retrieval, classification, and clustering tasks. To ensure a fair comparison, the same hidden size of 768 is chosen for all baselines. Evaluation. We consider the following four downstream tasks to evaluate the learned representations: 1) Multimodal retrieval: This task aims to find the most relevant target products using combinations of two or more modalities. A pair is considered a match if both belong to the same category; 2) Fine-grained multi-modal retrieval: Retrieval on an instance level, where only samples of the same product (i.e. color, model, shape, and style) are considered a match 6 ; 3) Multi-modal classification: Product category classification given the multi-modal features extracted from the joint co-transformer of SCALE using a linear classifier; and 4) Multi-modal clustering: Product category clustering using k-Means clustering and the same features as in the classification setting. For product retrieval, we adopt the widely used metrics mean Average Precision (mAP) and Precision (Prec) [14,31,49] to evaluate the retrieval accuracy on the two retrieval tasks. For product classification and clustering, all methods are evaluated using the Classification Precision (Classification accuracy), Normalized Mutual Information (NMI) [52] and Purity metrics. In all experiments, models are trained on the training split. The pretrained model is then applied to extract the modality features of the gallery and test splits for the product retrieval and clustering tasks. For the classification task, we finetune the pre-trained model on the classification subset containing 1,805 categories/classes and utilize the finetuned model to extract the features of the classification test set.\n\n\nModality Diversity\n\nTo examine the performance of our proposed SCALE framework and to verify the benefits of diverse modalities and dataset scale, we train SCALE with an increasing number of modalities and observe the variations in classification and multi-modal retrieval performance both for the whole M5Product dataset and the subset. More specifically, fused features are extracted from the joint co-transformer (JCT) of our SCALE after finetuning for the classification task and after pre-training and finetuning for the (coarse) multimodal retrieval task. Results in Table 3 show that performance increases across all settings as modalities are added, illustrating the benefit of complementary modality information to learning multi-modality feature representations. It can also be observed that modality gains are larger on the whole dataset, supporting Interesting Observation 1.\n\nWe further provide results for an extensive set of modality combinations to verify SCALEs effectiveness in leveraging the diverse modalities of our M5Product dataset. Table 4 provides results for the coarse-and fine-grained multimodal retrieval tasks as well as the classification task after finetuning the model. As in the previous experiment, noticeable improvements can be observed as additional modalities are added. In particular, the addition of the text modality leads to high modality gains, verifying the benefits of including more diverse modalities that can capture different views of the same product. Interestingly, performance on the coarse-grained retrieval task is significantly worse than on the fine-grained retrieval task in most cases, indicating the complexity of the M5Product dataset and the diversity of the products in each category. Semantic Alignment. To additionally demonstrate the importance of modality diversity, we compute the modality correlation, the average cosine similarity between image and text features as obtained by the JCT, for an increasing number of modalities. Figure 5 illustrates that the semantic alignment capability of the pre-training model increases as Table 3. The (pretrain/finetune) performance gains from sequentially adding more modalities using SCALE on the subset (top) and the whole dataset (bottom). The retrieval performances are based on the features extracted from pretrain and finetune stages.   the number of modalities grows.  \n\n\nMulti-modal Downstream Tasks\n\nWe evaluate SCALE on the M5Product dataset for the product retrieval, classification, and clustering tasks and compare results to several benchmark approaches in Table 5. For the Image based and BERT [10] models, which only utilize the image and text features, respectively, the extracted features are fed directly into the classification model. For our SCALE approach, we utilize the fused modality features generated by the joint co-transformer, pre-trained on both image and text modalities. Only utilizing the image and text modalities allows us to facilitate a fair comparison to the recent state-of-the-art approaches ViLBERT [30],  CLIP [38], VL-BERT [42], VisualBERT [27], UNITER [7] and CAPTURE [56]. Comparing our SCALE framework to the unimodal models, Image based and Bert [10], we observe that exploiting multi-modal data significantly improves the performance across all tasks. We further observe that SCALE, by leveraging SIMCL, can efficiently fuse the modalities and outperform all the baseline approaches (Interesting Observation 2).\n\n\nAblation Studys and Visualization\n\nTo explore how SIMCL influences IMCL and the Pretext tasks, we conduct several ablation studies. Table 6 illustrates that improvements of approximately 2% are obtained in the classification task and more than 2% for the coarse-grained retrieval task when including both, highlighting the importance of both the Pretext tasks and the effective modality fusion of SIMCL. We further analyze the effect of the MEM pretext task for the table modality and show the benefit of masking out complete entities over masking out individual tokens (MLM) in Table 7. This benefit can be attributed to the fact that MEM ensures that SCALE learns representations that encode the semantic information of complete entities. Finally, we evaluate the performance of modelling the text and the table modalities using individual modality encoders and compare SCALEs retrieval performance to a baseline where text and table information is concatenated and fed to a single transformer, resembling the process of BERT [10]. By modelling both modalities individually, results in Table 8 illustrate that more information can be preserved and we hypothesize that using a single transformer leads to a loss in table modality information for the benefit of the more expressive text modality. Figure 6 shows t-SNE visualizations of the extracted features for the JCT module of our SCALE model and the alternative approach ViLBert [30] on the M5Product dataset. SCALE not only better distinguishes different classes but also improves class compactness compared to the ViLBert model. Further, the attention attribution for different modalities are shown in Figure 7 and verify that the visual features generated by SCALE are object-oriented and semantically interpretable.\n\n\nLimitations and future work\n\nThe experimental evaluation showed that SCALE is able to learn efficient representations from a large number of modalities for retrieval, classification, and clustering. However, more evaluation of the generative capabilities of the models representations is lacking and tasks such as image and caption generation could be promising directions to explore. We further provide some of SCALEs failure cases in supplementary Section J. Potential negative societal impact. As a result of the strict ethical considerations used in the data collection process, where among others personally identifiable information has been removed, M5Product does not pose any ethical risks.\n\n\nConclusion and Discussion\n\nTo facilitate multi-modal pre-training, we present the M5Product dataset, which is the largest available multimodal E-commerce product dataset, consisting of five core modalities (image, text, table, video, and audio). To further promote multi-modal research in retail and increase seller and buyer engagement, we also propose the novel SCALE multi-modal pre-training framework. By utilizing Self-harmonized Inter-Modality Contrastive Learning (SIMCL), SCALE is able to model and exploit modality relationships effectively and outperforms previous approaches on the M5Product multi-modal retrieval, classification, and clustering tasks. We believe that both the dataset and the proposed framework work will inspire research on scaling multi-modal pre-training beyond the commonly used image and text modalities. \n\n\nAcknowledgement\n\n\nA. Dataset License\n\nOur M5Product dataset is released under CC BY-NC-SA 4.0 license and can freely be used for noncommercial purposes. More detailed information can be found at https://xiaodongsuper.github. io/M5Product_dataset/terms_of_use.html, which also provides dataset details and usage guidance. Note: For anonymity reasons, the link is not included during the review process.\n\n\nB. Annotation Collection\n\nWe resort to crowd-sourcing to obtain human annotations for the product retrieval task. Specifically, we present human annotators with a matching task, where annotators are asked to select the matching image-text pairs for a given query image-text pair. In our crowdsourcing system, each matching task is presented to five different human annotators and a typical example of our interface is shown in Figure 8. The left part of the interface shows the current query data (image and text), while the right side depicts an example from the candidate list. The annotators are then asked to choose from two options: mismatched and uncertain. The default labelling option is matched. The interface also displays the number of examples that have been reviewed and the total amount of examples to review. Each annotation task, can be considered as a binary classification task for the human worker, where he/she has to decide if the pair is a match or not. For each estimated task, the annotators receive a payment of 3 cents RMB.\n\n\nC. Annotation\n\nThe retrieval task annotation for any query sample consists of all the matched instances in the gallery split. To construct a reliable gallery set, we first use a ResNet50 [15] and Bert-Base [10] to extract features and construct the query candidate pool from all the data that is not contained in the training subset. Specifically, we sample an instance from a category that contains more than 2,000 instances and extract the image and text features. We then concatenate the features and compute the cosine similarity to all other instances of the dataset to produce a pre-ranked candidate list in order to minimize the labelling cost. The final size of the candidate shortlist for each query is 500, which is about 0.01% of the whole gallery split. During the crowd-sourced annotation process, human workers review both images and captions in the candidate list to select which samples are matched with the query instance.\n\nAnnotation Rules. It is quite challenging to define whether two images contain the same product when critical aspects are not given in their captions and images. In our annotations, we use product images and their captions as the primary materials for gallery construction. Hence, we define several rules to determine the \"same product\" condition and provide them as instructions to the annotators. Images contain the same product, if:\n\n1. The two images are in different conditions (e.g., backgrounds, angles, etc), but the products in both images are the same.\n\n2. They should have the same color/model/shape/style, or other features that can be distinguished by humans.\n\n3. The caption has the same product name but the product description differs.\n\n4. They share more than one characteristic such as appearances, materials, colors and so on.\n\nTo ensure labeling consistency, each annotation pair is labeled by five human workers in the crowd-sourced platform. In the process, we first make a small dataset from our query list as a Gold Problem to evaluate the annotation capability of each human worker. Based on the labeled results (\"Matched\" or \"Not Matched\") from human workers and their annotation capability, we utilize the weighted GLAD [50] inference algorithm to determine the final accepted labels.\n\n\nD. Dataset Split\n\nThe M5Product dataset is split into several parts to ensure consistent training and evaluation of the models on the various tasks. The training set contains 4,423,160 samples from 3,593 classes.\n\nRetrieval To evaluate models on the retrieval tasks, the remaining data is split into gallery-c and query-c sets, which are used for the coarse-grained retrieval task, and gallery-fg and query-fg sets, which are used for the fine-grained retrieval task. The difference between the two retrieval tasks lies in the granularity of their annotation. In the fine-grained task, only identical products are considered a match (for example, all IPHONE 11 Black), while in the coarse-grained task, category labels are being used to group products from each category (for example, all phones are considered a match).\n\nTo construct the fine-grained sets, we extracted all cosmetics categories and, using the abovementioned annotation procedure, finally obtained 1,991 query-fg samples and 117,858 gallery-fg samples. The query-c and gallery-c sets contain 24,410 and 1,197,905 samples, respectively. Among the samples in the gallery-c set, 249,614 samples are matched with samples in the query-c set, while 948,291 samples do not match. These unmatched samples are added to the gallery-c set to increase the difficulty of the retrieval task. We further report the finetuned retrieval performance  Figure 8. UI for huamn annotation on product retrieval task.\n\nin the paper, which corresponds to retrieval performance after finetuning the model using the classification training set (see next paragaraph).\n\nFinetuning, Classification and Clustering For the classification and clustering tasks, we sampled 1,805 categories from the whole dataset and obtained 18,526 train samples and 4,632 test samples. We first finetune our SCALE using the classification training set and then extract features from the finetuned model to perform the classification and clustering tasks.\n\n\nE. Data Format\n\nThe dataset consists of 6,313,067 products uploaded by 1,000,517 merchants, where merchant information has been removed to ensure anonymity. In the following, we outline the different modalities: Image data Each product has at least five product images, where the first image is the main image that gives the detailed overview of the product, while the rest depict its functionalities or characteristics. We pick all the main images to construct the dataset. Caption/text data are provided by the 1,000,517 merchants. Note that the text description does not always match well with the other modalities. Video data are used to showcase the products' usage and characteristics to customers. In our dataset, these videos are recorded at a speed of 24 frames per second (FPS). To reduce the amount of redundant information that is contained in adjacent frames and the dataset as a whole, we only select one frame per second. Audio data are extracted from the video data. We extract the corresponding audio information of all sampled video frames. Then the audio frames are transformed into spectrograms using Mel-Frequency Cepstral Coefficients (MFCC) [33]. We set the frame size and hop size as 1,024 and 256, respectively.\n\nTabular data are a special kind of database that records some additional product characteristics such as appearance, purpose and producer. The tabular data is indexed by the product ID and collected from the whole product database. There are 5,679 different types of property information and 24,398,673 unique values.\n\nF. Unimodal and unpair analysis 1) Unimodal analysis: Figure 9 gives the video, text and merchant distributions. From the figure, we can find that the video duration, the text length and the merchant number range from 1 to 60 seconds, 20 to 40 words and 1 to 10 product numbers, respectively. This variation further illustrates the real-world nature of our dataset. 2) Unpair analysis: In the data collection process, there are 82,577 invalid URLs for the image modalities (1.3% of the products), while the number of samples that contain both the Image and Text modalities is 6,230,490. Further taking into account the table modality, the number of complete samples drops by 1.4% to 6,225,598 samples that have all three modalities. Overall, the dataset contains 5,050,078 samples that contain all five modalities. This means that about 20% of the samples are incomplete. This is mostly due to merchants being biased towards specific modalities, which is a common scenario in the real world. \n\n\nG. Implementation Details\n\nOur models are implemented in Pytorch [35]. To speed up training, we use Nvidia Apex 8 for mixed precision training. All models are trained on 4 Nvidia 3090 and 2080ti GPUs on our workstations. We use Adam [21] to optimize the parameters of our model, with an initial learning rate of 1e-4, and use a linear learning rate decay schedule with a temperature parameter of 0.1. \n\n\nH. Dataset Comparison\n\nA comprehensive comparison between our M5Product dataset and other widely used multi-modal pre-training datasets is shown in Table 9. From the table, we can observe that our M5Product not only has more diverse modalities but also contains a large amount of data samples from an abundant amount of categories. \n\n\nI. Missing data verification\n\nResults in Table 10 show the superiority of our methods over the standard approach of ignoring incomplete samples. We compare two variants of our SCALE framework: 1) SCALE (full-modality) and our proposed SCALE. The only difference between the two methods is the input. The input of the former only includes complete samples (all modalities present), while the input of the latter includes the incomplete modality samples. The verification is performed on the subset dataset as mentioned in the main article. Figures 10, 11, and 12. The first column represents the image and text modality of the query sample, while the eight images to its right belong to the matched results from the gallery set. In the matched results, the samples boxed in blue are the correctly matched samples, while the samples boxed in red are mismatched. These retrieval results illustrate that the learned embeddings are discriminative. However, in a few cases, the recalled samples are not matched due to the limited number of category samples in the gallery set or similar descriptions in the text data.\n\n\nJ. Failure Analysis\n\n\nSeveral product retrieval examples are shown in\n\n\nK. More Visualization\n\nAdditional attention visualization results are provided in Figures 13 and 14. Similar to the illustrations in the main paper, these visualizations show that SCALE can learn the detailed semantics in the images and the text.\n\n\nL. Code and Our dataset.\n\nThe code is provided in the supplementary and we also include a few full-modality examples of the dataset. Due to space constraints, we have refrained from sharing the whole dataset.\n\nCar silk circle foot mats Changan Ease cs35 new cx20 Yuexiang V5 to Shang XT Ben Ben mini main driver single piece Zhang Xiaoquan kitchen knife home stainless steel slicing knife chef special kitchen knife cutting vegetables and meat free grinding kitchen knives lulu yoga pants female outer wear net red nude sense fitness pants high waist lifting hip running nine points tight original sports pants Old daddy shoes with rainbow mesh shoes women 2020 summer new thick bottom muffin breathable mesh surface inside high sports shoes TOMY Domeka alloy car model male toys TOMICA Ferrari 246GT / F40 / 512BB / 365GT Yun Yun inter Anji white tea 2020 new tea Ming Qian first pick authentic master spring tea premium rare bulk 100g \n\n\n4 https://pypi.org/project/moviepy/\n\nFigure 3 .\n3Training data distribution over whole categories.\n\nFigure 4 .\n4The Inter-Modality Contrastive Learning module of our SCALE framework.\n\n\n, we select the N modality pairs as positive pairs in our contrastive learning. For each positive pair (d\n\nFigure 5 .\n5Variations of modality correlation gains with the number of modalities.\n\nFigure 6 .Figure 7 .\n67Visualize the embeddings generated by SCALE and VILBERT via t-SNE. Points belonging to the same category are of the same color. Best viewed in color. Attention attribution over proposals learned by our SCALE.\n\n\n8 https://github.com/NVIDIA/apex\n\nFigure 9 .\n9The distributions of video, text and merchant on our M5Product.\n\nFigure 10 .\n10Successful retrieval results 1 by our SCALE.\n\nFigure 11 .\n11Failure retrieval results 2 by our SCALE.\n\nFigure 12 .Figure 13 .Figure 14 .\n121314Failure retrieval results 3 by our SCALE. More attention visualization 1 by our SCALE. More attention visualization 2 by our SCALE.\n\n\nInteresting Observations: 1) In large-scale and complex scenarios, the complementary gain of different modalities increases. Learning modality alignment weights allows our SCALE framework to effectively coordinate complementary information to achieve bet-ter performance. 2) For multi-modal pre-training mod-\nels in the E-commerce domain, dataset scale and di-\nversity are relatively important for the downstream \ntasks. Given the large-scale and diverse products, our \nSCALE framework generalizes better than other base-\nlines to downstream tasks. \n\n\n\nTable 1 .\n1Comparisons with other widely used multi-modal datasets. \"-\" means not mentioned. Our M5Product is one of the largest multi-modal datasets compared with existing datasets.Six \n\n\nTable 2 .\n2The characteristics of different modalities for E-products.Modality APP USA SPEC SELL PROD MATE CATEImage \nText \nVideo \nAudio \n\n\n\nTable 4. The performance of our model SCALE under different modality combinations on the coarse-and fine-grained multi-modal retrieval and classification tasks. In the following, I, T, Tab, V and A denote image, text, table, video and audio modalities, respectively.Modality Accuracy \nmAP@1 \nmAP@5 \nmAP@10 \nPrec@1 \nPrec@5 \nPrec@10 \n\nText \n77.42 \n47.70 / 65.10 \n53.63 / 68.39 \n51.59 / 66.99 \n47.70 / 65.10 \n30.96 / 44.89 \n24.15 / 33.44 \n+Image \n79.58 \n51.47 / 67.02 \n56.16 / 69.85 \n54.41 / 68.43 \n51.47 / 67.02 \n33.41 / 46.29 \n25.55 / 34.29 \n+Table \n82.83 \n57.14 / 67.97 \n61.71 / 70.34 \n59.64 / 69.38 \n57.14 / 67.97 \n38.02 / 46.85 \n28.99 / 34.36 \n+Video \n84.31 \n58.57 / 69.79 \n63.15 / 72.30 \n61.02 / 70.67 \n58.57 / 69.79 \n39.26 / 47.44 \n29.56 / 34.78 \n+Audio \n85.50 \n58.72 / 70.62 63.17 / 73.02 \n61.05 / 71.50 \n58.72 / 70.62 \n39.66 / 48.20 \n30.32 / 35.35 \n\nText \n81.11 \n55.82 / 69.47 \n60.74 / 72.74 \n59.02 / 71.79 \n55.82 / 69.47 \n36.99 / 48.76 \n28.04 / 35.84 \n+Image \n83.68 \n59.81 / 71.51 \n64.13 / 74.51 \n62.18 / 73.21 \n59.81 / 71.51 \n38.97 / 49.27 \n30.15 / 36.72 \n+Table \n84.63 \n61.32 / 72.34 \n65.53 / 74.86 \n63.62 / 73.47 \n61.32 / 72.34 \n40.66 / 49.77 \n30.78 / 36.95 \n+Video \n84.90 \n62.65 / 72.59 \n65.67 / 75.05 \n63.87 / 73.62 \n62.65 / 72.59 \n41.18 / 49.96 \n31.01 / 37.04 \n+Audio \n86.57 \n63.56 / 73.77 67.51 / 76.17 \n65.39 / 74.73 \n63.56 / 74.01 \n42.68 / 50.78 \n32.17 / 37.42 \n\nModality Combinations Accuracy \nmAP@1 \nmAP@5 \nmAP@10 \nPrec@1 \nPrec@5 \nPrec@10 \n\nI+Tab \n62.00 \n44.53 / 45.97 49.62 / 51.89 48.28 / 50.33 44.53 / 45.97 30.89 / 34.08 23.65 / 28.63 \nI+V \n34.57 \n20.57 / 36.29 26.78 / 42.72 26.41 / 41.38 20.57 / 36.29 14.71 / 26.52 11.78 / 22.34 \nI+A \n27.67 \n15.73 / 35.64 20.85 / 42.96 20.72 / 41.70 15.73 / 35.64 11.16 / 27.02 9.47 / 22.78 \nI+T \n79.58 \n67.02 / 62.20 69.85 / 66.97 68.43 / 64.21 67.02 / 62.20 46.29 / 49.85 34.29 / 42.36 \n\nI+T+V \n80.34 \n67.35 / 63.05 70.29 / 67.37 68.95 / 64.62 67.35 / 63.05 46.45 / 50.85 34.33 / 43.02 \nI+T+A \n79.73 \n67.19 / 64.21 70.15 / 68.25 68.64 / 65.35 67.19 / 64.21 46.33 / 50.42 33.32 / 42.93 \nI+Tab+V \n63.09 \n45.94 / 47.33 51.32 / 53.33 49.78 / 51.28 45.94 / 47.33 31.69 / 35.81 24.12 / 30.05 \nI+T+Tab \n82.83 \n67.97 / 68.30 70.34 / 72.67 69.38 / 70.07 67.97 / 68.30 46.85 / 57.44 34.36 / 50.59 \n\nI+T+Tab+V \n84.31 \n69.79 / 68.40 72.30 / 72.91 70.67 / 70.31 69.79 / 68.40 47.44 / 57.60 34.78 / 51.47 \nI+Tab+A+V \n63.54 \n47.24 / 48.24 52.07 / 53.89 50.41 / 51.89 47.24 / 48.24 32.19 / 36.29 24.47 / 30.74 \nI+T+A+V \n80.36 \n68.80 / 66.43 70.84 / 71.12 69.71 / 68.16 68.80 / 66.43 47.24 / 54.03 34.57 / 47.53 \nI+T+Tab+A \n84.33 \n70.23 / 68.97 72.59 / 73.07 70.94 / 70.77 70.23 / 68.97 47.58 / 57.89 35.33 / 51.60 \n\nI+T+Tab+A+V \n85.50 \n70.62 / 69.25 73.02 / 74.08 71.50 / 71.02 70.62 / 69.25 48.20 / 58.76 35.35 / 52.05 \n\n\n\nTable 5 .\n5Comparisons of image and text modalities on the subset (top) and the whole dataset (bottom).Method \nmAP@1 Accuracy NMI Purity \n\nImage based \n15.17 \n27.67 \n63.62 54.86 \nBERT [10] \n47.70 \n77.42 \n76.35 68.80 \nVL-BERT [42] \n49.31 \n78.13 \n80.51 71.91 \nViLBERT [30] \n49.18 \n78.24 \n80.51 71.91 \nVisualBERT [27] \n49.20 \n78.41 \n81.23 72.39 \nCLIP [38] \n49.39 \n78.35 \n81.75 72.50 \nUNITER [7] \n49.87 \n78.54 \n82.71 73.58 \nCAPTURE [38] \n50.30 \n78.69 \n83.06 74.14 \nSCALE (Ours) \n51.47 \n79.58 \n84.23 75.81 \n\nImage based \n22.67 \n30.14 \n67.49 59.64 \nBERT [10] \n55.82 \n82.11 \n87.30 71.75 \nCLIP [38] \n57.73 \n82.60 \n90.49 76.48 \nSCALE (Ours) \n59.81 \n83.68 \n92.01 78.34 \n\n\n\nTable 6 .\n6Ablation study of the SIMCL module.# IMCL PRE \nAccuracy \nmAP@1,5,10 \nPrec@1,5,10 \n\n1 \n83.77 \n68.45 / 70.92 / 69.30 67.56 / 46.37 / 34.12 \n2 \n84.44 \n69.14 / 71.96 / 70.13 69.14 / 47.15 / 34.84 \n3 \n84.09 \n69.31 / 71.59 / 69.85 69.31 / 46.72 / 34.42 \n4 \n85.50 \n70.62 / 73.02 / 71.50 70.62 / 48.20 / 35.35 \n\n\n\nTable 7 .\n7Analysis of different masked tasks (token mask (MLM) \nand entity mask (MEM)) for the table modality. \n\nTasks Accuracy \nmAP@1,5,10 \nPrec@1,5,10 \n\nMLM \n84.05 \n68.34 / 71.19 / 69.43 68.34 / 47.02 / 34.43 \nMEM \n85.50 \n70.62 / 73.02 / 71.50 70.62 / 48.20 / 35.35 \n\n\n\nTable 8 .\n8Analysis of treating text and table modalities separately (T/Tab) or stacked together (T+Tab).Formats Accuracy \nmAP@1,5,10 \nPrec@1,5,10 \nT+Tab \n84.61 70.15 / 72.19 / 70.49 69.15 / 47.40 / 34.40 \nT/Tab \n85.50 70.62 / 73.02 / 71.50 70.62 / 48.20 / 35.35 \n\n\n\nTable 9 .\n9Comparisons with other widely used multi-modal datasets. \"-\" means not mentioned. Our M5Product is one of the largest multi-modal datasets compared with existing datasets. text/video/audio/table yes Table 10. The retrieval performance with missing modalities.Dataset \n\nSamples Categories Instances Modalities \nModal type \nProduct \n\nLJ Speech [19] \n13,100 \n-\n-\n2 \naudio/text \nno \nSQuAD [25] \n37,111 \n-\n-\n2 \naudio/text \nno \nTVQA [24] \n21,793 \n-\n-\n2 \nvideo/text \nno \nMovieQA [46] \n408 \n-\n-\n2 \nvideo/text \nno \nTGIF-QA [20] \n56,720 \n-\n-\n2 \nvideo/text \nno \nAVSD [2] \n11,816 \n-\n-\n2 \nvideo/text \nno \nYoucook2 [57] \n14,000 \n89 \n-\n2 \nvideo/text \nno \nVATEX [47] \n35,000 \n-\n-\n2 \nvideo/text \nno \nMSRVTT [51] \n100,000 \n20 \n-\n2 \nvideo/text \nno \nHowTo100M [32] \n1,220,000 \n12 \n-\n2 \nvideo/text \nno \nConceptual Caption 3M [41] 3,300,000 \n-\n-\n2 \nimage/text \nno \nSBU [34] \n890,000 \n-\n-\n2 \nimage/text \nno \nVisual Genome [23] \n108,000 \n-\n-\n2 \nimage/text \nno \nCOCO [29] \n123,287 \n-\n-\n2 \nimage/text \nno \nFlickr30K [53] \n31,000 \n-\n-\n2 \nimage/text \nno \nNLVR2 [43] \n107,292 \n-\n-\n2 \nimage/text \nno \nVQA2.0 [4] \n204,721 \n-\n-\n2 \nimage/text \nno \nRPC checkout [48] \n30,000 \n200 367,935 \n2 \nimage/text \nno \nTwitter100k [17] \n100,000 \n-\n-\n2 \nimage/text \nno \nINRIA-Websearch [22] \n71,478 \n353 \n-\n2 \nimage/text \nno \nNUS-WIDE [8] \n269,648 \n81 \n-\n2 \nimage/text \nno \nOpen Image [1] \n1,670,000 \n-\n-\n2 \nimage/text \nno \nConceptual 12M [5] \n12,423,374 \n-\n-\n2 \nimage/text \nno \nCMU-MOSEI [54] \n23,500 \n2 \n-\n3 \ntext/video/audio \nno \nXMedia [36] \n12,000 \n20 \n-\n5 \nimage/text/video/audio/3D \nno \n\nDress Retrieval [9] \n20,200 \n50 \u223c20,200 \n2 \nimage/text \nyes \nMEP-3M [6] \n3,012,959 599 \n-\n2 \nimage/text \nyes \nProduct1M [55] \n1,182,083 458 \n92,200 \n2 \nimage/text \nyes \nM5Product \n6,313,067 6,232 \n-\n5 \nimage/Modal \nAccuracy \nmAP@1 \nmAP@5 \nmAP@10 \nPrec@1 \nPrec@5 \nPrec@10 \n\nSCALE (full-modality) \n84.06 \n57.97/ 69.12 \n62.54/ 71.93 \n60.48 / 69.92 \n57.97 / 69.12 \n38.02 / 47.63 \n28.88 / 34.70 \nSCALE \n85.50 \n58.72 / 70.62 \n63.17 / 73.02 \n61.05 / 71.50 \n58.72 / 70.62 \n39.66 / 48.20 \n30.32 / 35.35 \n\n\nhttps://github.com/airsplay/py-bottom-up-attention6 A more thorough definition of the term same products and how instance-level labels are obtained is provided in the supplementary.\n\nOpen images dataset. 214Open images dataset. https://storage. googleapis.com/openimages/web/index. html/, 2018. 2, 14\n\nAudio visual scene-aware dialog. Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim K Marks, Chiori Hori, Peter Anderson, Stefan Lee, Devi Parikh, CVPR. 214Huda AlAmri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks, Chiori Hori, Peter Anderson, Stefan Lee, and Devi Parikh. Audio visual scene-aware dialog. In CVPR, pages 7558- 7567, 2019. 2, 14\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, CVPR. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, pages 6077-6086, 2018. 4\n\nVQA: Visual Question Answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, ICCV. 214Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, pages 2425-2433, 2015. 2, 14\n\nConceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut, CVPR. 214Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre- training to recognize long-tail visual concepts. In CVPR, pages 3558-3568, 2021. 2, 3, 14\n\nMep-3m: A large-scale multi-modal e-commerce products dataset. Delong Chen, Fan Liu, Xiaoyu Du, Ruizhuo Gao, Feng Xu, 314Delong Chen, Fan Liu, Xiaoyu Du, Ruizhuo Gao, and Feng Xu. Mep-3m: A large-scale multi-modal e-commerce prod- ucts dataset. 2021. 3, 14\n\nUNITER: universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Lecture Notes in Computer Science. Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm123758SpringerYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: universal image-text representation learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan- Michael Frahm, editors, ECCV, volume 12375 of Lecture Notes in Computer Science, pages 104-120. Springer, 2020. 2, 3, 6, 7, 8\n\nNUS-WIDE: a real-world web image database from national university of singapore. Jinhui Tat-Seng Chua, Richang Tang, Haojie Hong, Zhiping Li, Yantao Luo, Zheng, CIVR. 214Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip- ing Luo, and Yantao Zheng. NUS-WIDE: a real-world web image database from national university of singapore. In CIVR, 2009. 2, 3, 14\n\nLeveraging weakly annotated data for fashion image retrieval and label prediction. Charles Corbiere, Hedi Ben-Younes, Alexandre Ram\u00e9, Charles Ollion, ICCV Workshops. 214Charles Corbiere, Hedi Ben-Younes, Alexandre Ram\u00e9, and Charles Ollion. Leveraging weakly annotated data for fash- ion image retrieval and label prediction. In ICCV Workshops, pages 2268-2274, 2017. 2, 3, 14\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 812arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 6, 7, 8, 12\n\nModality-agnostic attention fusion for visual search with text feedback. Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, Kofi Boakye, arXiv:2007.00145arXiv preprintEric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, and Kofi Boakye. Modality-agnostic attention fusion for visual search with text feedback. arXiv preprint arXiv:2007.00145, 2020. 3\n\nMulti-modal transformer for video retrieval. Valentin Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmSpringer12349Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan- Michael Frahm, editors, ECCV, volume 12349, pages 214- 229. Springer, 2020. 3\n\nFashionbert: Text and image matching with adaptive loss for cross-modal retrieval. Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, Hao Wang, SIGIR. Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, and Hao Wang. Fashionbert: Text and im- age matching with adaptive loss for cross-modal retrieval. In SIGIR, pages 2251-2260, 2020. 3\n\nIterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. Yunchao Gong, Svetlana Lazebnik, Albert Gordo, Florent Perronnin, IEEE Trans. Pattern Anal. Mach. Intell. 3512Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A procrustean approach to learning binary codes for large-scale im- age retrieval. IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2916-2929, 2013. 6\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 612Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 6, 12\n\nDoes my multimodal model learn cross-modal interactions? it's harder to tell than you might think! In EMNLP. Jack Hessel, Lillian Lee, 2020Jack Hessel and Lillian Lee. Does my multimodal model learn cross-modal interactions? it's harder to tell than you might think! In EMNLP, pages 861-877, 2020. 1\n\nTwitter100k: A real-world dataset for weakly supervised cross-media retrieval. Yuting Hu, Liang Zheng, Yi Yang, Yongfeng Huang, IEEE Trans. Multim. 20414Yuting Hu, Liang Zheng, Yi Yang, and Yongfeng Huang. Twitter100k: A real-world dataset for weakly supervised cross-media retrieval. IEEE Trans. Multim., 20(4):927-938, 2018. 2, 14\n\nWhat makes multimodal learning better than single (provably). Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, Longbo Huang, arXiv:2106.04538arXiv preprintYu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi- modal learning better than single (provably). arXiv preprint arXiv:2106.04538, 2021. 1\n\nThe lj speech dataset. Keith Ito, Linda Johnson, 214Keith Ito and Linda Johnson. The lj speech dataset. https: //keithito.com/LJ-Speech-Dataset/, 2017. 2, 14\n\nTGIF-QA: toward spatio-temporal reasoning in visual question answering. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim, CVPR. 214Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. TGIF-QA: toward spatio-temporal reasoning in visual question answering. In CVPR, pages 1359-1367, 2017. 2, 14\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. 614Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6, 14\n\nImproving web image search results using queryrelative classifiers. Josip Krapac, Moray Allan, Jakob J Verbeek, Fr\u00e9d\u00e9ric Jurie, CVPR. 214Josip Krapac, Moray Allan, Jakob J. Verbeek, and Fr\u00e9d\u00e9ric Jurie. Improving web image search results using query- relative classifiers. In CVPR, pages 1094-1101, 2010. 2, 3, 14\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei, Int. J. Comput. Vis. 123114Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Com- put. Vis., 123(1):32-73, 2017. 2, 6, 14\n\nTVQA: localized, compositional video question answering. Jie Lei, Licheng Yu, Mohit Bansal, Tamara L Berg, EMNLP. 214Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg. TVQA: localized, compositional video question answering. In EMNLP, pages 1369-1379, 2018. 2, 14\n\nSpoken squad: A study of mitigating the impact of speech recognition errors on listening comprehension. Chia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, Hung-Yi Lee, ISCA. 214Chia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, and Hung-yi Lee. Spoken squad: A study of mitigating the impact of speech recognition errors on listening comprehension. In ISCA, pages 3459-3463, 2018. 2, 3, 14\n\nHERO: hierarchical encoder for video+language omni-representation pre-training. Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu, EMNLP. Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. HERO: hierarchical encoder for video+language omni-representation pre-training. In EMNLP, pages 2046-2065, 2020. 3\n\nVisualbert: A simple and performant baseline for vision and language. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang, abs/1908.03557CoRR7Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. CoRR, abs/1908.03557, 2019. 2, 3, 6, 7, 8\n\n. Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, Hongxia Yang, M6: A chinese multimodal pretrainer. CoRR, abs/2103.00823, 2021. 3Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xi- aodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. M6: A chinese multimodal pretrainer. CoRR, abs/2103.00823, 2021. 3\n\nMicrosoft COCO: common objects in context. Tsung-Yi Lin, Michael Maire, Serge J Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. 214Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740-755, 2014. 2, 3, 14\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, NIPS. 7Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NIPS, pages 13-23, 2019. 2, 3, 6, 7, 8\n\nLatent semantic minimal hashing for image retrieval. Xiaoqiang Lu, Xiangtao Zheng, Xuelong Li, IEEE Trans. Image Process. 261Xiaoqiang Lu, Xiangtao Zheng, and Xuelong Li. Latent se- mantic minimal hashing for image retrieval. IEEE Trans. Image Process., 26(1):355-368, 2017. 6\n\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic, ICCV. 214Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, pages 2630-2640, 2019. 2, 3, 14\n\nCombining evidence from residual phase and mfcc features for speaker recognition. Ksr Murty, B Yegnanarayana, IEEE Signal Processing Letters. 13113Ksr Murty and B. Yegnanarayana. Combining evidence from residual phase and mfcc features for speaker recognition. IEEE Signal Processing Letters, 13(1):52-55, 2005. 5, 13\n\nIm2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara L Berg, NIPS. 214Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned pho- tographs. In NIPS, pages 1143-1151, 2011. 2, 14\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Workshop. 14Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS Workshop, 2017. 14\n\nAn overview of cross-media retrieval: Concepts, methodologies, benchmarks, and challenges. Yuxin Peng, Xin Huang, Yunzhen Zhao, IEEE Trans. Circuits Syst. Video Technol. 28914Yuxin Peng, Xin Huang, and Yunzhen Zhao. An overview of cross-media retrieval: Concepts, methodologies, bench- marks, and challenges. IEEE Trans. Circuits Syst. Video Technol., 28(9):2372-2385, 2018. 3, 14\n\nImagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti, arXiv:2001.07966arXiv preprintDi Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966, 2020. 3\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, ICML. 7Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748-8763, 2021. 2, 3, 6, 7, 8\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, PMLR, 2021. 3Proceedings of Machine Learning Research. Marina Meila and Tong Zhang139Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, ICML, volume 139 of Proceedings of Machine Learning Research, pages 8821-8831. PMLR, 2021. 3\n\nFaster R-CNN: towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross B He, Jian Girshick, Sun, IEEE Trans. Pattern Anal. Mach. Intell. 396Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with re- gion proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137-1149, 2017. 6\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, ACL. 214Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, im- age alt-text dataset for automatic image captioning. In ACL, pages 2556-2565, 2018. 2, 3, 14\n\nVL-BERT: pre-training of generic visual-linguistic representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, ICLR. 7Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: pre-training of generic visual-linguistic representations. In ICLR, 2020. 2, 3, 6, 7, 8\n\nA corpus for reasoning about natural language grounded in photographs. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, ACL. 214Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. In ACL, pages 6418- 6428, 2019. 2, 14\n\nVideobert: A joint model for video and language representation learning. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid, ICCV. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, pages 7463- 7472, 2019. 3\n\nLXMERT: learning crossmodality encoder representations from transformers. Hao Tan, Mohit Bansal, EMNLP. 23Hao Tan and Mohit Bansal. LXMERT: learning cross- modality encoder representations from transformers. In EMNLP, pages 5099-5110, 2019. 2, 3\n\nMovieqa: Understanding stories in movies through questionanswering. Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler, CVPR. 214Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question- answering. In CVPR, pages 4631-4640, 2016. 2, 14\n\nVatex: A large-scale, highquality multilingual dataset for video-and-language research. Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang, ICCV. 214Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high- quality multilingual dataset for video-and-language research. In ICCV, pages 4580-4590, 2019. 2, 14\n\nXiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, Lingqiao Liu, arXiv:1901.07249Rpc: A large-scale retail product checkout dataset. 214arXiv preprintXiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao Liu. Rpc: A large-scale retail product checkout dataset. arXiv preprint arXiv:1901.07249, 2019. 2, 3, 14\n\nSpectral hashing. Yair Weiss, Antonio Torralba, Robert Fergus, NIPS. Yair Weiss, Antonio Torralba, and Robert Fergus. Spectral hashing. In NIPS, pages 1753-1760, 2008. 6\n\nWhose vote should count more: Optimal integration of labels from labelers of unknown expertise. Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, Javier R Movellan, NIPS. 12Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier R. Movellan. Whose vote should count more: Op- timal integration of labels from labelers of unknown exper- tise. In NIPS, pages 2035-2043, 2009. 12\n\nMSR-VTT: A large video description dataset for bridging video and language. Jun Xu, Tao Mei, Ting Yao, Yong Rui, CVPR. 214Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging video and lan- guage. In CVPR, pages 5288-5296, 2016. 2, 14\n\nDocument clustering using locality preserving indexing and support vector machines. Chengfu Yang, Zhang Yi, Soft Comput. 127Chengfu Yang and Zhang Yi. Document clustering using lo- cality preserving indexing and support vector machines. Soft Comput., 12(7):677-683, 2008. 6\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, Trans. Assoc. Comput. Linguistics. 214Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken- maier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descrip- tions. Trans. Assoc. Comput. Linguistics, 2:67-78, 2014. 2, 3, 14\n\nMultimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency, ACL. 314Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. In ACL, pages 2236-2246, 2018. 3, 14\n\nProd-uct1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, Xiaodan Liang, ICCV. 14Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Min- long Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Prod- uct1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. In ICCV, 2021. 2, 3, 14\n\nProduct1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, Xiaodan Liang, CoRR, abs/2107.14572, 2021. 2, 6, 8Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. CoRR, abs/2107.14572, 2021. 2, 6, 8\n\nTowards automatic learning of procedures from web instructional videos. Luowei Zhou, Chenliang Xu, Jason J Corso, AAAI. 214Luowei Zhou, Chenliang Xu, and Jason J. Corso. To- wards automatic learning of procedures from web instruc- tional videos. In AAAI, pages 7590-7598, 2018. 2, 14\n\nDeep multi-modal latent representation learning for automated dementia diagnosis. Tao Zhou, Mingxia Liu, Huazhu Fu, Jun Wang, Jianbing Shen, Ling Shao, Dinggang Shen, MICCAI. Tao Zhou, Mingxia Liu, Huazhu Fu, Jun Wang, Jianbing Shen, Ling Shao, and Dinggang Shen. Deep multi-modal latent representation learning for automated dementia diag- nosis. In MICCAI, pages 629-638, 2019. 1\n\nKaleido-bert: Vision-language pre-training on fashion domain. Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao, CVPR. Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, and Ling Shao. Kaleido-bert: Vision-language pre-training on fashion do- main. In CVPR, pages 12647-12657, 2021. 3\n", "annotations": {"author": "[{\"end\":127,\"start\":92},{\"end\":184,\"start\":128},{\"end\":187,\"start\":185},{\"end\":224,\"start\":188},{\"end\":267,\"start\":225},{\"end\":349,\"start\":268},{\"end\":387,\"start\":350},{\"end\":415,\"start\":388},{\"end\":468,\"start\":416},{\"end\":548,\"start\":469}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":97},{\"end\":139,\"start\":135},{\"end\":198,\"start\":196},{\"end\":236,\"start\":233},{\"end\":289,\"start\":278},{\"end\":363,\"start\":360},{\"end\":398,\"start\":396},{\"end\":427,\"start\":423},{\"end\":482,\"start\":477}]", "author_first_name": "[{\"end\":96,\"start\":92},{\"end\":134,\"start\":128},{\"end\":186,\"start\":185},{\"end\":195,\"start\":188},{\"end\":232,\"start\":225},{\"end\":275,\"start\":268},{\"end\":277,\"start\":276},{\"end\":359,\"start\":350},{\"end\":395,\"start\":388},{\"end\":422,\"start\":416},{\"end\":476,\"start\":469}]", "author_affiliation": "[{\"end\":126,\"start\":103},{\"end\":183,\"start\":141},{\"end\":223,\"start\":200},{\"end\":266,\"start\":238},{\"end\":348,\"start\":320},{\"end\":386,\"start\":365},{\"end\":414,\"start\":400},{\"end\":467,\"start\":446},{\"end\":547,\"start\":505}]", "title": "[{\"end\":89,\"start\":1},{\"end\":637,\"start\":549}]", "venue": null, "abstract": "[{\"end\":2264,\"start\":639}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2813,\"start\":2809},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2834,\"start\":2830},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2837,\"start\":2834},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3249,\"start\":3245},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3700,\"start\":3696},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3721,\"start\":3718},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3740,\"start\":3736},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6346,\"start\":6343},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6349,\"start\":6346},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6352,\"start\":6349},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6355,\"start\":6352},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6358,\"start\":6355},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6361,\"start\":6358},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6364,\"start\":6361},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7638,\"start\":7634},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7653,\"start\":7649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7766,\"start\":7763},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7769,\"start\":7766},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7772,\"start\":7769},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7775,\"start\":7772},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7778,\"start\":7775},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7781,\"start\":7778},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7784,\"start\":7781},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7787,\"start\":7784},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7967,\"start\":7964},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7969,\"start\":7967},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7971,\"start\":7969},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7974,\"start\":7971},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7977,\"start\":7974},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7980,\"start\":7977},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7983,\"start\":7980},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7986,\"start\":7983},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7989,\"start\":7986},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7992,\"start\":7989},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7995,\"start\":7992},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7998,\"start\":7995},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8126,\"start\":8122},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8263,\"start\":8260},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8300,\"start\":8296},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8316,\"start\":8312},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8529,\"start\":8526},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8548,\"start\":8544},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8567,\"start\":8563},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9452,\"start\":9448},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9469,\"start\":9465},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9485,\"start\":9481},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9495,\"start\":9491},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9506,\"start\":9502},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9523,\"start\":9519},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9538,\"start\":9535},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9632,\"start\":9628},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9645,\"start\":9641},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9656,\"start\":9652},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9672,\"start\":9668},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9766,\"start\":9762},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9777,\"start\":9773},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9796,\"start\":9792},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9805,\"start\":9801},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9822,\"start\":9818},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11050,\"start\":11049},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12299,\"start\":12298},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12489,\"start\":12488},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15709,\"start\":15706},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15844,\"start\":15840},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20529,\"start\":20525},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20999,\"start\":20995},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21001,\"start\":21000},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21032,\"start\":21028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21063,\"start\":21059},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21109,\"start\":21105},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21314,\"start\":21310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21622,\"start\":21618},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21664,\"start\":21660},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21675,\"start\":21671},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21689,\"start\":21685},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21706,\"start\":21702},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21718,\"start\":21715},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21736,\"start\":21732},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21763,\"start\":21759},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22973,\"start\":22969},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22976,\"start\":22973},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22979,\"start\":22976},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23214,\"start\":23210},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26286,\"start\":26282},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26718,\"start\":26714},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26730,\"start\":26726},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26744,\"start\":26740},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26761,\"start\":26757},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26773,\"start\":26770},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":26790,\"start\":26786},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26871,\"start\":26867},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28168,\"start\":28164},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28574,\"start\":28570},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32103,\"start\":32099},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32122,\"start\":32118},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34104,\"start\":34100},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37310,\"start\":37306},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38763,\"start\":38759},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38931,\"start\":38927},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":50101,\"start\":50100}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41843,\"start\":41806},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41906,\"start\":41844},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41990,\"start\":41907},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42098,\"start\":41991},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42183,\"start\":42099},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42416,\"start\":42184},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42451,\"start\":42417},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42528,\"start\":42452},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42588,\"start\":42529},{\"attributes\":{\"id\":\"fig_10\"},\"end\":42645,\"start\":42589},{\"attributes\":{\"id\":\"fig_11\"},\"end\":42818,\"start\":42646},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43372,\"start\":42819},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43561,\"start\":43373},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43701,\"start\":43562},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46471,\"start\":43702},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47134,\"start\":46472},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47451,\"start\":47135},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47724,\"start\":47452},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47991,\"start\":47725},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":50049,\"start\":47992}]", "paragraph": "[{\"end\":2544,\"start\":2287},{\"end\":3250,\"start\":2561},{\"end\":3839,\"start\":3252},{\"end\":5044,\"start\":3841},{\"end\":5881,\"start\":5046},{\"end\":6679,\"start\":5883},{\"end\":6963,\"start\":6681},{\"end\":7236,\"start\":6965},{\"end\":7240,\"start\":7238},{\"end\":9095,\"start\":7257},{\"end\":9673,\"start\":9097},{\"end\":10552,\"start\":9675},{\"end\":10765,\"start\":10554},{\"end\":14749,\"start\":10787},{\"end\":15226,\"start\":14769},{\"end\":16343,\"start\":15260},{\"end\":17542,\"start\":16381},{\"end\":17800,\"start\":17604},{\"end\":18340,\"start\":17856},{\"end\":18446,\"start\":18424},{\"end\":18664,\"start\":18499},{\"end\":18793,\"start\":18730},{\"end\":19058,\"start\":18912},{\"end\":20361,\"start\":19060},{\"end\":20473,\"start\":20439},{\"end\":23661,\"start\":20489},{\"end\":24551,\"start\":23684},{\"end\":26049,\"start\":24553},{\"end\":27133,\"start\":26082},{\"end\":28910,\"start\":27171},{\"end\":29611,\"start\":28942},{\"end\":30453,\"start\":29641},{\"end\":30857,\"start\":30494},{\"end\":31909,\"start\":30886},{\"end\":32851,\"start\":31927},{\"end\":33288,\"start\":32853},{\"end\":33415,\"start\":33290},{\"end\":33525,\"start\":33417},{\"end\":33604,\"start\":33527},{\"end\":33698,\"start\":33606},{\"end\":34164,\"start\":33700},{\"end\":34379,\"start\":34185},{\"end\":34987,\"start\":34381},{\"end\":35627,\"start\":34989},{\"end\":35773,\"start\":35629},{\"end\":36139,\"start\":35775},{\"end\":37378,\"start\":36158},{\"end\":37697,\"start\":37380},{\"end\":38691,\"start\":37699},{\"end\":39095,\"start\":38721},{\"end\":39430,\"start\":39121},{\"end\":40544,\"start\":39463},{\"end\":40865,\"start\":40642},{\"end\":41076,\"start\":40894},{\"end\":41805,\"start\":41078}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17603,\"start\":17543},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18498,\"start\":18447},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18729,\"start\":18665},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18911,\"start\":18794},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20438,\"start\":20362}]", "table_ref": "[{\"end\":2285,\"start\":2266},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9094,\"start\":9087},{\"end\":10137,\"start\":10117},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11723,\"start\":11716},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13618,\"start\":13611},{\"end\":24244,\"start\":24237},{\"end\":25767,\"start\":25760},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26251,\"start\":26244},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27275,\"start\":27268},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27722,\"start\":27715},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28231,\"start\":28224},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":39253,\"start\":39246},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39482,\"start\":39474}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2559,\"start\":2547},{\"attributes\":{\"n\":\"2.\"},\"end\":7255,\"start\":7243},{\"attributes\":{\"n\":\"3.\"},\"end\":10785,\"start\":10768},{\"attributes\":{\"n\":\"4.\"},\"end\":14767,\"start\":14752},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15258,\"start\":15229},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16379,\"start\":16346},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17854,\"start\":17803},{\"end\":18361,\"start\":18343},{\"end\":18384,\"start\":18364},{\"end\":18422,\"start\":18387},{\"attributes\":{\"n\":\"5.\"},\"end\":20487,\"start\":20476},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23682,\"start\":23664},{\"attributes\":{\"n\":\"5.2.\"},\"end\":26080,\"start\":26052},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27169,\"start\":27136},{\"attributes\":{\"n\":\"6.\"},\"end\":28940,\"start\":28913},{\"attributes\":{\"n\":\"7.\"},\"end\":29639,\"start\":29614},{\"attributes\":{\"n\":\"8.\"},\"end\":30471,\"start\":30456},{\"end\":30492,\"start\":30474},{\"end\":30884,\"start\":30860},{\"end\":31925,\"start\":31912},{\"end\":34183,\"start\":34167},{\"end\":36156,\"start\":36142},{\"end\":38719,\"start\":38694},{\"end\":39119,\"start\":39098},{\"end\":39461,\"start\":39433},{\"end\":40566,\"start\":40547},{\"end\":40616,\"start\":40569},{\"end\":40640,\"start\":40619},{\"end\":40892,\"start\":40868},{\"end\":41855,\"start\":41845},{\"end\":41918,\"start\":41908},{\"end\":42110,\"start\":42100},{\"end\":42205,\"start\":42185},{\"end\":42463,\"start\":42453},{\"end\":42541,\"start\":42530},{\"end\":42601,\"start\":42590},{\"end\":42680,\"start\":42647},{\"end\":43383,\"start\":43374},{\"end\":43572,\"start\":43563},{\"end\":46482,\"start\":46473},{\"end\":47145,\"start\":47136},{\"end\":47462,\"start\":47453},{\"end\":47735,\"start\":47726},{\"end\":48002,\"start\":47993}]", "table": "[{\"end\":43372,\"start\":43076},{\"end\":43561,\"start\":43556},{\"end\":43701,\"start\":43674},{\"end\":46471,\"start\":43970},{\"end\":47134,\"start\":46576},{\"end\":47451,\"start\":47182},{\"end\":47724,\"start\":47464},{\"end\":47991,\"start\":47831},{\"end\":50049,\"start\":48263}]", "figure_caption": "[{\"end\":41843,\"start\":41808},{\"end\":41906,\"start\":41857},{\"end\":41990,\"start\":41920},{\"end\":42098,\"start\":41993},{\"end\":42183,\"start\":42112},{\"end\":42416,\"start\":42208},{\"end\":42451,\"start\":42419},{\"end\":42528,\"start\":42465},{\"end\":42588,\"start\":42544},{\"end\":42645,\"start\":42604},{\"end\":42818,\"start\":42687},{\"end\":43076,\"start\":42821},{\"end\":43556,\"start\":43385},{\"end\":43674,\"start\":43574},{\"end\":43970,\"start\":43704},{\"end\":46576,\"start\":46484},{\"end\":47182,\"start\":47147},{\"end\":47831,\"start\":47737},{\"end\":48263,\"start\":48004}]", "figure_ref": "[{\"end\":2307,\"start\":2299},{\"end\":5781,\"start\":5773},{\"end\":6080,\"start\":6072},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11609,\"start\":11601},{\"end\":12089,\"start\":12081},{\"end\":12746,\"start\":12738},{\"end\":14789,\"start\":14781},{\"end\":15283,\"start\":15275},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20055,\"start\":20047},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25669,\"start\":25661},{\"end\":28441,\"start\":28433},{\"end\":28803,\"start\":28795},{\"end\":31295,\"start\":31287},{\"end\":35575,\"start\":35567},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":37761,\"start\":37753},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39986,\"start\":39972},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":40718,\"start\":40701}]", "bib_author_first_name": "[{\"end\":50389,\"start\":50385},{\"end\":50405,\"start\":50398},{\"end\":50426,\"start\":50418},{\"end\":50435,\"start\":50432},{\"end\":50447,\"start\":50442},{\"end\":50462,\"start\":50457},{\"end\":50474,\"start\":50469},{\"end\":50485,\"start\":50482},{\"end\":50487,\"start\":50486},{\"end\":50501,\"start\":50495},{\"end\":50513,\"start\":50508},{\"end\":50530,\"start\":50524},{\"end\":50540,\"start\":50536},{\"end\":50890,\"start\":50885},{\"end\":50909,\"start\":50901},{\"end\":50919,\"start\":50914},{\"end\":50935,\"start\":50929},{\"end\":50947,\"start\":50943},{\"end\":50964,\"start\":50957},{\"end\":50975,\"start\":50972},{\"end\":51252,\"start\":51243},{\"end\":51269,\"start\":51260},{\"end\":51285,\"start\":51279},{\"end\":51298,\"start\":51290},{\"end\":51314,\"start\":51309},{\"end\":51323,\"start\":51322},{\"end\":51332,\"start\":51324},{\"end\":51346,\"start\":51342},{\"end\":51656,\"start\":51649},{\"end\":51675,\"start\":51669},{\"end\":51687,\"start\":51684},{\"end\":51698,\"start\":51694},{\"end\":51990,\"start\":51984},{\"end\":52000,\"start\":51997},{\"end\":52012,\"start\":52006},{\"end\":52024,\"start\":52017},{\"end\":52034,\"start\":52030},{\"end\":52241,\"start\":52233},{\"end\":52254,\"start\":52248},{\"end\":52266,\"start\":52259},{\"end\":52276,\"start\":52271},{\"end\":52279,\"start\":52277},{\"end\":52293,\"start\":52287},{\"end\":52304,\"start\":52301},{\"end\":52312,\"start\":52310},{\"end\":52328,\"start\":52320},{\"end\":52877,\"start\":52871},{\"end\":52900,\"start\":52893},{\"end\":52913,\"start\":52907},{\"end\":52927,\"start\":52920},{\"end\":52938,\"start\":52932},{\"end\":53242,\"start\":53235},{\"end\":53257,\"start\":53253},{\"end\":53279,\"start\":53270},{\"end\":53293,\"start\":53286},{\"end\":53534,\"start\":53529},{\"end\":53551,\"start\":53543},{\"end\":53565,\"start\":53559},{\"end\":53579,\"start\":53571},{\"end\":53589,\"start\":53580},{\"end\":53982,\"start\":53978},{\"end\":53994,\"start\":53990},{\"end\":54011,\"start\":54006},{\"end\":54025,\"start\":54021},{\"end\":54037,\"start\":54033},{\"end\":54316,\"start\":54308},{\"end\":54329,\"start\":54325},{\"end\":54342,\"start\":54335},{\"end\":54360,\"start\":54352},{\"end\":54781,\"start\":54775},{\"end\":54792,\"start\":54787},{\"end\":54801,\"start\":54798},{\"end\":54815,\"start\":54808},{\"end\":54825,\"start\":54821},{\"end\":54832,\"start\":54830},{\"end\":54840,\"start\":54838},{\"end\":54848,\"start\":54845},{\"end\":55179,\"start\":55172},{\"end\":55194,\"start\":55186},{\"end\":55211,\"start\":55205},{\"end\":55226,\"start\":55219},{\"end\":55580,\"start\":55573},{\"end\":55592,\"start\":55585},{\"end\":55608,\"start\":55600},{\"end\":55618,\"start\":55614},{\"end\":55884,\"start\":55880},{\"end\":55900,\"start\":55893},{\"end\":56157,\"start\":56151},{\"end\":56167,\"start\":56162},{\"end\":56177,\"start\":56175},{\"end\":56192,\"start\":56184},{\"end\":56470,\"start\":56468},{\"end\":56488,\"start\":56478},{\"end\":56498,\"start\":56493},{\"end\":56511,\"start\":56504},{\"end\":56522,\"start\":56518},{\"end\":56535,\"start\":56529},{\"end\":56786,\"start\":56781},{\"end\":56797,\"start\":56792},{\"end\":56996,\"start\":56989},{\"end\":57007,\"start\":57003},{\"end\":57022,\"start\":57014},{\"end\":57035,\"start\":57027},{\"end\":57047,\"start\":57041},{\"end\":57286,\"start\":57285},{\"end\":57302,\"start\":57297},{\"end\":57496,\"start\":57491},{\"end\":57510,\"start\":57505},{\"end\":57523,\"start\":57518},{\"end\":57525,\"start\":57524},{\"end\":57543,\"start\":57535},{\"end\":57833,\"start\":57827},{\"end\":57847,\"start\":57843},{\"end\":57859,\"start\":57853},{\"end\":57873,\"start\":57867},{\"end\":57888,\"start\":57883},{\"end\":57901,\"start\":57895},{\"end\":57920,\"start\":57911},{\"end\":57933,\"start\":57927},{\"end\":57952,\"start\":57946},{\"end\":57962,\"start\":57957},{\"end\":57964,\"start\":57963},{\"end\":57980,\"start\":57973},{\"end\":57982,\"start\":57981},{\"end\":57996,\"start\":57994},{\"end\":58424,\"start\":58421},{\"end\":58437,\"start\":58430},{\"end\":58447,\"start\":58442},{\"end\":58462,\"start\":58456},{\"end\":58464,\"start\":58463},{\"end\":58747,\"start\":58737},{\"end\":58759,\"start\":58752},{\"end\":58773,\"start\":58764},{\"end\":58786,\"start\":58779},{\"end\":59092,\"start\":59086},{\"end\":59105,\"start\":59097},{\"end\":59114,\"start\":59112},{\"end\":59125,\"start\":59122},{\"end\":59138,\"start\":59131},{\"end\":59151,\"start\":59143},{\"end\":59439,\"start\":59425},{\"end\":59448,\"start\":59444},{\"end\":59460,\"start\":59458},{\"end\":59473,\"start\":59466},{\"end\":59488,\"start\":59481},{\"end\":59712,\"start\":59705},{\"end\":59721,\"start\":59718},{\"end\":59729,\"start\":59727},{\"end\":59741,\"start\":59736},{\"end\":59752,\"start\":59748},{\"end\":59766,\"start\":59759},{\"end\":59778,\"start\":59774},{\"end\":59788,\"start\":59785},{\"end\":59797,\"start\":59795},{\"end\":59812,\"start\":59805},{\"end\":59821,\"start\":59818},{\"end\":59836,\"start\":59829},{\"end\":59846,\"start\":59844},{\"end\":59859,\"start\":59852},{\"end\":59872,\"start\":59864},{\"end\":59882,\"start\":59879},{\"end\":59894,\"start\":59888},{\"end\":59907,\"start\":59900},{\"end\":59921,\"start\":59914},{\"end\":59929,\"start\":59926},{\"end\":59938,\"start\":59934},{\"end\":59946,\"start\":59943},{\"end\":59959,\"start\":59952},{\"end\":59969,\"start\":59966},{\"end\":59983,\"start\":59976},{\"end\":60466,\"start\":60458},{\"end\":60479,\"start\":60472},{\"end\":60492,\"start\":60487},{\"end\":60494,\"start\":60493},{\"end\":60510,\"start\":60505},{\"end\":60523,\"start\":60517},{\"end\":60536,\"start\":60532},{\"end\":60551,\"start\":60546},{\"end\":60561,\"start\":60560},{\"end\":60570,\"start\":60562},{\"end\":60904,\"start\":60898},{\"end\":60914,\"start\":60909},{\"end\":60926,\"start\":60922},{\"end\":60941,\"start\":60935},{\"end\":61210,\"start\":61201},{\"end\":61223,\"start\":61215},{\"end\":61238,\"start\":61231},{\"end\":61526,\"start\":61519},{\"end\":61541,\"start\":61534},{\"end\":61563,\"start\":61550},{\"end\":61581,\"start\":61573},{\"end\":61595,\"start\":61591},{\"end\":61609,\"start\":61604},{\"end\":61948,\"start\":61945},{\"end\":61957,\"start\":61956},{\"end\":62255,\"start\":62248},{\"end\":62271,\"start\":62265},{\"end\":62288,\"start\":62282},{\"end\":62290,\"start\":62289},{\"end\":62509,\"start\":62505},{\"end\":62521,\"start\":62518},{\"end\":62536,\"start\":62529},{\"end\":62554,\"start\":62547},{\"end\":62569,\"start\":62563},{\"end\":62583,\"start\":62576},{\"end\":62598,\"start\":62592},{\"end\":62609,\"start\":62604},{\"end\":62625,\"start\":62621},{\"end\":62638,\"start\":62634},{\"end\":62972,\"start\":62967},{\"end\":62982,\"start\":62979},{\"end\":62997,\"start\":62990},{\"end\":63346,\"start\":63344},{\"end\":63354,\"start\":63351},{\"end\":63362,\"start\":63359},{\"end\":63375,\"start\":63369},{\"end\":63387,\"start\":63381},{\"end\":63400,\"start\":63396},{\"end\":63713,\"start\":63709},{\"end\":63727,\"start\":63723},{\"end\":63732,\"start\":63728},{\"end\":63743,\"start\":63738},{\"end\":63759,\"start\":63753},{\"end\":63775,\"start\":63768},{\"end\":63789,\"start\":63781},{\"end\":63805,\"start\":63799},{\"end\":63820,\"start\":63814},{\"end\":63835,\"start\":63829},{\"end\":63849,\"start\":63845},{\"end\":63865,\"start\":63857},{\"end\":63879,\"start\":63875},{\"end\":64244,\"start\":64238},{\"end\":64260,\"start\":64253},{\"end\":64276,\"start\":64269},{\"end\":64287,\"start\":64282},{\"end\":64301,\"start\":64294},{\"end\":64312,\"start\":64308},{\"end\":64326,\"start\":64322},{\"end\":64337,\"start\":64333},{\"end\":64807,\"start\":64800},{\"end\":64826,\"start\":64822},{\"end\":64828,\"start\":64827},{\"end\":64837,\"start\":64833},{\"end\":65208,\"start\":65202},{\"end\":65220,\"start\":65217},{\"end\":65236,\"start\":65227},{\"end\":65250,\"start\":65246},{\"end\":65546,\"start\":65540},{\"end\":65557,\"start\":65551},{\"end\":65566,\"start\":65563},{\"end\":65575,\"start\":65572},{\"end\":65585,\"start\":65580},{\"end\":65594,\"start\":65590},{\"end\":65606,\"start\":65600},{\"end\":65869,\"start\":65864},{\"end\":65885,\"start\":65876},{\"end\":65896,\"start\":65892},{\"end\":65908,\"start\":65904},{\"end\":65922,\"start\":65916},{\"end\":65932,\"start\":65928},{\"end\":66215,\"start\":66211},{\"end\":66227,\"start\":66221},{\"end\":66239,\"start\":66235},{\"end\":66255,\"start\":66250},{\"end\":66272,\"start\":66264},{\"end\":66547,\"start\":66544},{\"end\":66558,\"start\":66553},{\"end\":66793,\"start\":66785},{\"end\":66808,\"start\":66803},{\"end\":66820,\"start\":66814},{\"end\":66842,\"start\":66835},{\"end\":66859,\"start\":66853},{\"end\":66874,\"start\":66869},{\"end\":67194,\"start\":67191},{\"end\":67207,\"start\":67201},{\"end\":67218,\"start\":67212},{\"end\":67228,\"start\":67225},{\"end\":67242,\"start\":67233},{\"end\":67256,\"start\":67249},{\"end\":67261,\"start\":67257},{\"end\":67495,\"start\":67487},{\"end\":67505,\"start\":67501},{\"end\":67514,\"start\":67511},{\"end\":67525,\"start\":67521},{\"end\":67540,\"start\":67532},{\"end\":67817,\"start\":67813},{\"end\":67832,\"start\":67825},{\"end\":67849,\"start\":67843},{\"end\":68067,\"start\":68062},{\"end\":68083,\"start\":68079},{\"end\":68099,\"start\":68092},{\"end\":68109,\"start\":68104},{\"end\":68125,\"start\":68119},{\"end\":68127,\"start\":68126},{\"end\":68442,\"start\":68439},{\"end\":68450,\"start\":68447},{\"end\":68460,\"start\":68456},{\"end\":68470,\"start\":68466},{\"end\":68734,\"start\":68727},{\"end\":68746,\"start\":68741},{\"end\":69041,\"start\":69036},{\"end\":69054,\"start\":69049},{\"end\":69065,\"start\":69060},{\"end\":69079,\"start\":69074},{\"end\":69478,\"start\":69474},{\"end\":69490,\"start\":69486},{\"end\":69493,\"start\":69491},{\"end\":69509,\"start\":69501},{\"end\":69521,\"start\":69517},{\"end\":69545,\"start\":69531},{\"end\":69892,\"start\":69886},{\"end\":69906,\"start\":69899},{\"end\":69915,\"start\":69911},{\"end\":69929,\"start\":69922},{\"end\":69942,\"start\":69935},{\"end\":69952,\"start\":69947},{\"end\":69964,\"start\":69960},{\"end\":69976,\"start\":69969},{\"end\":70327,\"start\":70321},{\"end\":70341,\"start\":70334},{\"end\":70350,\"start\":70346},{\"end\":70364,\"start\":70357},{\"end\":70377,\"start\":70370},{\"end\":70387,\"start\":70382},{\"end\":70399,\"start\":70395},{\"end\":70411,\"start\":70404},{\"end\":70770,\"start\":70764},{\"end\":70786,\"start\":70777},{\"end\":70796,\"start\":70791},{\"end\":70798,\"start\":70797},{\"end\":71062,\"start\":71059},{\"end\":71076,\"start\":71069},{\"end\":71088,\"start\":71082},{\"end\":71096,\"start\":71093},{\"end\":71111,\"start\":71103},{\"end\":71122,\"start\":71118},{\"end\":71137,\"start\":71129},{\"end\":71430,\"start\":71422},{\"end\":71444,\"start\":71438},{\"end\":71459,\"start\":71450},{\"end\":71470,\"start\":71465},{\"end\":71479,\"start\":71476},{\"end\":71493,\"start\":71486},{\"end\":71507,\"start\":71500},{\"end\":71517,\"start\":71513}]", "bib_author_last_name": "[{\"end\":50396,\"start\":50390},{\"end\":50416,\"start\":50406},{\"end\":50430,\"start\":50427},{\"end\":50440,\"start\":50436},{\"end\":50455,\"start\":50448},{\"end\":50467,\"start\":50463},{\"end\":50480,\"start\":50475},{\"end\":50493,\"start\":50488},{\"end\":50506,\"start\":50502},{\"end\":50522,\"start\":50514},{\"end\":50534,\"start\":50531},{\"end\":50547,\"start\":50541},{\"end\":50899,\"start\":50891},{\"end\":50912,\"start\":50910},{\"end\":50927,\"start\":50920},{\"end\":50941,\"start\":50936},{\"end\":50955,\"start\":50948},{\"end\":50970,\"start\":50965},{\"end\":50981,\"start\":50976},{\"end\":51258,\"start\":51253},{\"end\":51277,\"start\":51270},{\"end\":51288,\"start\":51286},{\"end\":51307,\"start\":51299},{\"end\":51320,\"start\":51315},{\"end\":51340,\"start\":51333},{\"end\":51353,\"start\":51347},{\"end\":51667,\"start\":51657},{\"end\":51682,\"start\":51676},{\"end\":51692,\"start\":51688},{\"end\":51706,\"start\":51699},{\"end\":51995,\"start\":51991},{\"end\":52004,\"start\":52001},{\"end\":52015,\"start\":52013},{\"end\":52028,\"start\":52025},{\"end\":52037,\"start\":52035},{\"end\":52246,\"start\":52242},{\"end\":52257,\"start\":52255},{\"end\":52269,\"start\":52267},{\"end\":52285,\"start\":52280},{\"end\":52299,\"start\":52294},{\"end\":52308,\"start\":52305},{\"end\":52318,\"start\":52313},{\"end\":52332,\"start\":52329},{\"end\":52891,\"start\":52878},{\"end\":52905,\"start\":52901},{\"end\":52918,\"start\":52914},{\"end\":52930,\"start\":52928},{\"end\":52942,\"start\":52939},{\"end\":52949,\"start\":52944},{\"end\":53251,\"start\":53243},{\"end\":53268,\"start\":53258},{\"end\":53284,\"start\":53280},{\"end\":53300,\"start\":53294},{\"end\":53541,\"start\":53535},{\"end\":53557,\"start\":53552},{\"end\":53569,\"start\":53566},{\"end\":53594,\"start\":53590},{\"end\":53988,\"start\":53983},{\"end\":54004,\"start\":53995},{\"end\":54019,\"start\":54012},{\"end\":54031,\"start\":54026},{\"end\":54044,\"start\":54038},{\"end\":54323,\"start\":54317},{\"end\":54333,\"start\":54330},{\"end\":54350,\"start\":54343},{\"end\":54367,\"start\":54361},{\"end\":54785,\"start\":54782},{\"end\":54796,\"start\":54793},{\"end\":54806,\"start\":54802},{\"end\":54819,\"start\":54816},{\"end\":54828,\"start\":54826},{\"end\":54836,\"start\":54833},{\"end\":54843,\"start\":54841},{\"end\":54853,\"start\":54849},{\"end\":55184,\"start\":55180},{\"end\":55203,\"start\":55195},{\"end\":55217,\"start\":55212},{\"end\":55236,\"start\":55227},{\"end\":55583,\"start\":55581},{\"end\":55598,\"start\":55593},{\"end\":55612,\"start\":55609},{\"end\":55622,\"start\":55619},{\"end\":55891,\"start\":55885},{\"end\":55904,\"start\":55901},{\"end\":56160,\"start\":56158},{\"end\":56173,\"start\":56168},{\"end\":56182,\"start\":56178},{\"end\":56198,\"start\":56193},{\"end\":56476,\"start\":56471},{\"end\":56491,\"start\":56489},{\"end\":56502,\"start\":56499},{\"end\":56516,\"start\":56512},{\"end\":56527,\"start\":56523},{\"end\":56541,\"start\":56536},{\"end\":56790,\"start\":56787},{\"end\":56805,\"start\":56798},{\"end\":57001,\"start\":56997},{\"end\":57012,\"start\":57008},{\"end\":57025,\"start\":57023},{\"end\":57039,\"start\":57036},{\"end\":57051,\"start\":57048},{\"end\":57295,\"start\":57287},{\"end\":57309,\"start\":57303},{\"end\":57313,\"start\":57311},{\"end\":57503,\"start\":57497},{\"end\":57516,\"start\":57511},{\"end\":57533,\"start\":57526},{\"end\":57549,\"start\":57544},{\"end\":57841,\"start\":57834},{\"end\":57851,\"start\":57848},{\"end\":57865,\"start\":57860},{\"end\":57881,\"start\":57874},{\"end\":57893,\"start\":57889},{\"end\":57909,\"start\":57902},{\"end\":57925,\"start\":57921},{\"end\":57944,\"start\":57934},{\"end\":57955,\"start\":57953},{\"end\":57971,\"start\":57965},{\"end\":57992,\"start\":57983},{\"end\":58004,\"start\":57997},{\"end\":58428,\"start\":58425},{\"end\":58440,\"start\":58438},{\"end\":58454,\"start\":58448},{\"end\":58469,\"start\":58465},{\"end\":58750,\"start\":58748},{\"end\":58762,\"start\":58760},{\"end\":58777,\"start\":58774},{\"end\":58790,\"start\":58787},{\"end\":59095,\"start\":59093},{\"end\":59110,\"start\":59106},{\"end\":59120,\"start\":59115},{\"end\":59129,\"start\":59126},{\"end\":59141,\"start\":59139},{\"end\":59155,\"start\":59152},{\"end\":59442,\"start\":59440},{\"end\":59456,\"start\":59449},{\"end\":59464,\"start\":59461},{\"end\":59479,\"start\":59474},{\"end\":59494,\"start\":59489},{\"end\":59716,\"start\":59713},{\"end\":59725,\"start\":59722},{\"end\":59734,\"start\":59730},{\"end\":59746,\"start\":59742},{\"end\":59757,\"start\":59753},{\"end\":59772,\"start\":59767},{\"end\":59783,\"start\":59779},{\"end\":59793,\"start\":59789},{\"end\":59803,\"start\":59798},{\"end\":59816,\"start\":59813},{\"end\":59827,\"start\":59822},{\"end\":59842,\"start\":59837},{\"end\":59850,\"start\":59847},{\"end\":59862,\"start\":59860},{\"end\":59877,\"start\":59873},{\"end\":59886,\"start\":59883},{\"end\":59898,\"start\":59895},{\"end\":59912,\"start\":59908},{\"end\":59924,\"start\":59922},{\"end\":59932,\"start\":59930},{\"end\":59941,\"start\":59939},{\"end\":59950,\"start\":59947},{\"end\":59964,\"start\":59960},{\"end\":59974,\"start\":59970},{\"end\":59988,\"start\":59984},{\"end\":60470,\"start\":60467},{\"end\":60485,\"start\":60480},{\"end\":60503,\"start\":60495},{\"end\":60515,\"start\":60511},{\"end\":60530,\"start\":60524},{\"end\":60544,\"start\":60537},{\"end\":60558,\"start\":60552},{\"end\":60578,\"start\":60571},{\"end\":60907,\"start\":60905},{\"end\":60920,\"start\":60915},{\"end\":60933,\"start\":60927},{\"end\":60945,\"start\":60942},{\"end\":61213,\"start\":61211},{\"end\":61229,\"start\":61224},{\"end\":61241,\"start\":61239},{\"end\":61532,\"start\":61527},{\"end\":61548,\"start\":61542},{\"end\":61571,\"start\":61564},{\"end\":61589,\"start\":61582},{\"end\":61602,\"start\":61596},{\"end\":61615,\"start\":61610},{\"end\":61954,\"start\":61949},{\"end\":61971,\"start\":61958},{\"end\":62263,\"start\":62256},{\"end\":62280,\"start\":62272},{\"end\":62295,\"start\":62291},{\"end\":62516,\"start\":62510},{\"end\":62527,\"start\":62522},{\"end\":62545,\"start\":62537},{\"end\":62561,\"start\":62555},{\"end\":62574,\"start\":62570},{\"end\":62590,\"start\":62584},{\"end\":62602,\"start\":62599},{\"end\":62619,\"start\":62610},{\"end\":62632,\"start\":62626},{\"end\":62644,\"start\":62639},{\"end\":62977,\"start\":62973},{\"end\":62988,\"start\":62983},{\"end\":63002,\"start\":62998},{\"end\":63349,\"start\":63347},{\"end\":63357,\"start\":63355},{\"end\":63367,\"start\":63363},{\"end\":63379,\"start\":63376},{\"end\":63394,\"start\":63388},{\"end\":63408,\"start\":63401},{\"end\":63721,\"start\":63714},{\"end\":63736,\"start\":63733},{\"end\":63751,\"start\":63744},{\"end\":63766,\"start\":63760},{\"end\":63779,\"start\":63776},{\"end\":63797,\"start\":63790},{\"end\":63812,\"start\":63806},{\"end\":63827,\"start\":63821},{\"end\":63843,\"start\":63836},{\"end\":63855,\"start\":63850},{\"end\":63873,\"start\":63866},{\"end\":63889,\"start\":63880},{\"end\":64251,\"start\":64245},{\"end\":64267,\"start\":64261},{\"end\":64280,\"start\":64277},{\"end\":64292,\"start\":64288},{\"end\":64306,\"start\":64302},{\"end\":64320,\"start\":64313},{\"end\":64331,\"start\":64327},{\"end\":64347,\"start\":64338},{\"end\":64820,\"start\":64808},{\"end\":64831,\"start\":64829},{\"end\":64846,\"start\":64838},{\"end\":64851,\"start\":64848},{\"end\":65215,\"start\":65209},{\"end\":65225,\"start\":65221},{\"end\":65244,\"start\":65237},{\"end\":65258,\"start\":65251},{\"end\":65549,\"start\":65547},{\"end\":65561,\"start\":65558},{\"end\":65570,\"start\":65567},{\"end\":65578,\"start\":65576},{\"end\":65588,\"start\":65586},{\"end\":65598,\"start\":65595},{\"end\":65610,\"start\":65607},{\"end\":65874,\"start\":65870},{\"end\":65890,\"start\":65886},{\"end\":65902,\"start\":65897},{\"end\":65914,\"start\":65909},{\"end\":65926,\"start\":65923},{\"end\":65938,\"start\":65933},{\"end\":66219,\"start\":66216},{\"end\":66233,\"start\":66228},{\"end\":66248,\"start\":66240},{\"end\":66262,\"start\":66256},{\"end\":66279,\"start\":66273},{\"end\":66551,\"start\":66548},{\"end\":66565,\"start\":66559},{\"end\":66801,\"start\":66794},{\"end\":66812,\"start\":66809},{\"end\":66833,\"start\":66821},{\"end\":66851,\"start\":66843},{\"end\":66867,\"start\":66860},{\"end\":66881,\"start\":66875},{\"end\":67199,\"start\":67195},{\"end\":67210,\"start\":67208},{\"end\":67223,\"start\":67219},{\"end\":67231,\"start\":67229},{\"end\":67247,\"start\":67243},{\"end\":67266,\"start\":67262},{\"end\":67499,\"start\":67496},{\"end\":67509,\"start\":67506},{\"end\":67519,\"start\":67515},{\"end\":67530,\"start\":67526},{\"end\":67544,\"start\":67541},{\"end\":67823,\"start\":67818},{\"end\":67841,\"start\":67833},{\"end\":67856,\"start\":67850},{\"end\":68077,\"start\":68068},{\"end\":68090,\"start\":68084},{\"end\":68102,\"start\":68100},{\"end\":68117,\"start\":68110},{\"end\":68136,\"start\":68128},{\"end\":68445,\"start\":68443},{\"end\":68454,\"start\":68451},{\"end\":68464,\"start\":68461},{\"end\":68474,\"start\":68471},{\"end\":68739,\"start\":68735},{\"end\":68749,\"start\":68747},{\"end\":69047,\"start\":69042},{\"end\":69058,\"start\":69055},{\"end\":69072,\"start\":69066},{\"end\":69091,\"start\":69080},{\"end\":69484,\"start\":69479},{\"end\":69499,\"start\":69494},{\"end\":69515,\"start\":69510},{\"end\":69529,\"start\":69522},{\"end\":69553,\"start\":69546},{\"end\":69897,\"start\":69893},{\"end\":69909,\"start\":69907},{\"end\":69920,\"start\":69916},{\"end\":69933,\"start\":69930},{\"end\":69945,\"start\":69943},{\"end\":69958,\"start\":69953},{\"end\":69967,\"start\":69965},{\"end\":69982,\"start\":69977},{\"end\":70332,\"start\":70328},{\"end\":70344,\"start\":70342},{\"end\":70355,\"start\":70351},{\"end\":70368,\"start\":70365},{\"end\":70380,\"start\":70378},{\"end\":70393,\"start\":70388},{\"end\":70402,\"start\":70400},{\"end\":70417,\"start\":70412},{\"end\":70775,\"start\":70771},{\"end\":70789,\"start\":70787},{\"end\":70804,\"start\":70799},{\"end\":71067,\"start\":71063},{\"end\":71080,\"start\":71077},{\"end\":71091,\"start\":71089},{\"end\":71101,\"start\":71097},{\"end\":71116,\"start\":71112},{\"end\":71127,\"start\":71123},{\"end\":71142,\"start\":71138},{\"end\":71436,\"start\":71431},{\"end\":71448,\"start\":71445},{\"end\":71463,\"start\":71460},{\"end\":71474,\"start\":71471},{\"end\":71484,\"start\":71480},{\"end\":71498,\"start\":71494},{\"end\":71511,\"start\":71508},{\"end\":71522,\"start\":71518}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":50350,\"start\":50233},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59316836},\"end\":50798,\"start\":50352},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3753452},\"end\":51209,\"start\":50800},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3180429},\"end\":51550,\"start\":51211},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":231951742},\"end\":51919,\"start\":51552},{\"attributes\":{\"id\":\"b5\"},\"end\":52177,\"start\":51921},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":216080982},\"end\":52788,\"start\":52179},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6483070},\"end\":53150,\"start\":52790},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4722904},\"end\":53527,\"start\":53152},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b9\"},\"end\":53903,\"start\":53529},{\"attributes\":{\"doi\":\"arXiv:2007.00145\",\"id\":\"b10\"},\"end\":54261,\"start\":53905},{\"attributes\":{\"id\":\"b11\"},\"end\":54690,\"start\":54263},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":218718774},\"end\":55065,\"start\":54692},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2605321},\"end\":55525,\"start\":55067},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":55769,\"start\":55527},{\"attributes\":{\"id\":\"b15\"},\"end\":56070,\"start\":55771},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3919117},\"end\":56404,\"start\":56072},{\"attributes\":{\"doi\":\"arXiv:2106.04538\",\"id\":\"b17\"},\"end\":56756,\"start\":56406},{\"attributes\":{\"id\":\"b18\"},\"end\":56915,\"start\":56758},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3030826},\"end\":57239,\"start\":56917},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":57421,\"start\":57241},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6628564},\"end\":57735,\"start\":57423},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4492210},\"end\":58362,\"start\":57737},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52171684},\"end\":58631,\"start\":58364},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4561735},\"end\":59004,\"start\":58633},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218470055},\"end\":59353,\"start\":59006},{\"attributes\":{\"doi\":\"abs/1908.03557\",\"id\":\"b26\"},\"end\":59701,\"start\":59355},{\"attributes\":{\"id\":\"b27\"},\"end\":60413,\"start\":59703},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14113767},\"end\":60798,\"start\":60415},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":199453025},\"end\":61146,\"start\":60800},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10842394},\"end\":61424,\"start\":61148},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":182952863},\"end\":61861,\"start\":61426},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13093868},\"end\":62180,\"start\":61863},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14579301},\"end\":62465,\"start\":62182},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":40027675},\"end\":62874,\"start\":62467},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15514398},\"end\":63256,\"start\":62876},{\"attributes\":{\"doi\":\"arXiv:2001.07966\",\"id\":\"b36\"},\"end\":63636,\"start\":63258},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":231591445},\"end\":64200,\"start\":63638},{\"attributes\":{\"doi\":\"PMLR, 2021. 3\",\"id\":\"b38\",\"matched_paper_id\":232035663},\"end\":64718,\"start\":64202},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10328909},\"end\":65101,\"start\":64720},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51876975},\"end\":65470,\"start\":65103},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":201317624},\"end\":65791,\"start\":65472},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":53178856},\"end\":66136,\"start\":65793},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":102483628},\"end\":66468,\"start\":66138},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":201103729},\"end\":66715,\"start\":66470},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1017389},\"end\":67101,\"start\":66717},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":102352148},\"end\":67485,\"start\":67103},{\"attributes\":{\"doi\":\"arXiv:1901.07249\",\"id\":\"b47\"},\"end\":67793,\"start\":67487},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":344349},\"end\":67964,\"start\":67795},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2332622},\"end\":68361,\"start\":67966},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":206594535},\"end\":68641,\"start\":68363},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":24521017},\"end\":68916,\"start\":68643},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3104920},\"end\":69372,\"start\":68918},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":51868869},\"end\":69784,\"start\":69374},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":236635296},\"end\":70220,\"start\":69786},{\"attributes\":{\"id\":\"b55\"},\"end\":70690,\"start\":70222},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":19713015},\"end\":70975,\"start\":70692},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":204028166},\"end\":71358,\"start\":70977},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":232417264},\"end\":71735,\"start\":71360}]", "bib_title": "[{\"end\":50383,\"start\":50352},{\"end\":50883,\"start\":50800},{\"end\":51241,\"start\":51211},{\"end\":51647,\"start\":51552},{\"end\":52231,\"start\":52179},{\"end\":52869,\"start\":52790},{\"end\":53233,\"start\":53152},{\"end\":54773,\"start\":54692},{\"end\":55170,\"start\":55067},{\"end\":55571,\"start\":55527},{\"end\":56149,\"start\":56072},{\"end\":56987,\"start\":56917},{\"end\":57283,\"start\":57241},{\"end\":57489,\"start\":57423},{\"end\":57825,\"start\":57737},{\"end\":58419,\"start\":58364},{\"end\":58735,\"start\":58633},{\"end\":59084,\"start\":59006},{\"end\":60456,\"start\":60415},{\"end\":60896,\"start\":60800},{\"end\":61199,\"start\":61148},{\"end\":61517,\"start\":61426},{\"end\":61943,\"start\":61863},{\"end\":62246,\"start\":62182},{\"end\":62503,\"start\":62467},{\"end\":62965,\"start\":62876},{\"end\":63707,\"start\":63638},{\"end\":64236,\"start\":64202},{\"end\":64798,\"start\":64720},{\"end\":65200,\"start\":65103},{\"end\":65538,\"start\":65472},{\"end\":65862,\"start\":65793},{\"end\":66209,\"start\":66138},{\"end\":66542,\"start\":66470},{\"end\":66783,\"start\":66717},{\"end\":67189,\"start\":67103},{\"end\":67811,\"start\":67795},{\"end\":68060,\"start\":67966},{\"end\":68437,\"start\":68363},{\"end\":68725,\"start\":68643},{\"end\":69034,\"start\":68918},{\"end\":69472,\"start\":69374},{\"end\":69884,\"start\":69786},{\"end\":70762,\"start\":70692},{\"end\":71057,\"start\":70977},{\"end\":71420,\"start\":71360}]", "bib_author": "[{\"end\":50398,\"start\":50385},{\"end\":50418,\"start\":50398},{\"end\":50432,\"start\":50418},{\"end\":50442,\"start\":50432},{\"end\":50457,\"start\":50442},{\"end\":50469,\"start\":50457},{\"end\":50482,\"start\":50469},{\"end\":50495,\"start\":50482},{\"end\":50508,\"start\":50495},{\"end\":50524,\"start\":50508},{\"end\":50536,\"start\":50524},{\"end\":50549,\"start\":50536},{\"end\":50901,\"start\":50885},{\"end\":50914,\"start\":50901},{\"end\":50929,\"start\":50914},{\"end\":50943,\"start\":50929},{\"end\":50957,\"start\":50943},{\"end\":50972,\"start\":50957},{\"end\":50983,\"start\":50972},{\"end\":51260,\"start\":51243},{\"end\":51279,\"start\":51260},{\"end\":51290,\"start\":51279},{\"end\":51309,\"start\":51290},{\"end\":51322,\"start\":51309},{\"end\":51342,\"start\":51322},{\"end\":51355,\"start\":51342},{\"end\":51669,\"start\":51649},{\"end\":51684,\"start\":51669},{\"end\":51694,\"start\":51684},{\"end\":51708,\"start\":51694},{\"end\":51997,\"start\":51984},{\"end\":52006,\"start\":51997},{\"end\":52017,\"start\":52006},{\"end\":52030,\"start\":52017},{\"end\":52039,\"start\":52030},{\"end\":52248,\"start\":52233},{\"end\":52259,\"start\":52248},{\"end\":52271,\"start\":52259},{\"end\":52287,\"start\":52271},{\"end\":52301,\"start\":52287},{\"end\":52310,\"start\":52301},{\"end\":52320,\"start\":52310},{\"end\":52334,\"start\":52320},{\"end\":52893,\"start\":52871},{\"end\":52907,\"start\":52893},{\"end\":52920,\"start\":52907},{\"end\":52932,\"start\":52920},{\"end\":52944,\"start\":52932},{\"end\":52951,\"start\":52944},{\"end\":53253,\"start\":53235},{\"end\":53270,\"start\":53253},{\"end\":53286,\"start\":53270},{\"end\":53302,\"start\":53286},{\"end\":53543,\"start\":53529},{\"end\":53559,\"start\":53543},{\"end\":53571,\"start\":53559},{\"end\":53596,\"start\":53571},{\"end\":53990,\"start\":53978},{\"end\":54006,\"start\":53990},{\"end\":54021,\"start\":54006},{\"end\":54033,\"start\":54021},{\"end\":54046,\"start\":54033},{\"end\":54325,\"start\":54308},{\"end\":54335,\"start\":54325},{\"end\":54352,\"start\":54335},{\"end\":54369,\"start\":54352},{\"end\":54787,\"start\":54775},{\"end\":54798,\"start\":54787},{\"end\":54808,\"start\":54798},{\"end\":54821,\"start\":54808},{\"end\":54830,\"start\":54821},{\"end\":54838,\"start\":54830},{\"end\":54845,\"start\":54838},{\"end\":54855,\"start\":54845},{\"end\":55186,\"start\":55172},{\"end\":55205,\"start\":55186},{\"end\":55219,\"start\":55205},{\"end\":55238,\"start\":55219},{\"end\":55585,\"start\":55573},{\"end\":55600,\"start\":55585},{\"end\":55614,\"start\":55600},{\"end\":55624,\"start\":55614},{\"end\":55893,\"start\":55880},{\"end\":55906,\"start\":55893},{\"end\":56162,\"start\":56151},{\"end\":56175,\"start\":56162},{\"end\":56184,\"start\":56175},{\"end\":56200,\"start\":56184},{\"end\":56478,\"start\":56468},{\"end\":56493,\"start\":56478},{\"end\":56504,\"start\":56493},{\"end\":56518,\"start\":56504},{\"end\":56529,\"start\":56518},{\"end\":56543,\"start\":56529},{\"end\":56792,\"start\":56781},{\"end\":56807,\"start\":56792},{\"end\":57003,\"start\":56989},{\"end\":57014,\"start\":57003},{\"end\":57027,\"start\":57014},{\"end\":57041,\"start\":57027},{\"end\":57053,\"start\":57041},{\"end\":57297,\"start\":57285},{\"end\":57311,\"start\":57297},{\"end\":57315,\"start\":57311},{\"end\":57505,\"start\":57491},{\"end\":57518,\"start\":57505},{\"end\":57535,\"start\":57518},{\"end\":57551,\"start\":57535},{\"end\":57843,\"start\":57827},{\"end\":57853,\"start\":57843},{\"end\":57867,\"start\":57853},{\"end\":57883,\"start\":57867},{\"end\":57895,\"start\":57883},{\"end\":57911,\"start\":57895},{\"end\":57927,\"start\":57911},{\"end\":57946,\"start\":57927},{\"end\":57957,\"start\":57946},{\"end\":57973,\"start\":57957},{\"end\":57994,\"start\":57973},{\"end\":58006,\"start\":57994},{\"end\":58430,\"start\":58421},{\"end\":58442,\"start\":58430},{\"end\":58456,\"start\":58442},{\"end\":58471,\"start\":58456},{\"end\":58752,\"start\":58737},{\"end\":58764,\"start\":58752},{\"end\":58779,\"start\":58764},{\"end\":58792,\"start\":58779},{\"end\":59097,\"start\":59086},{\"end\":59112,\"start\":59097},{\"end\":59122,\"start\":59112},{\"end\":59131,\"start\":59122},{\"end\":59143,\"start\":59131},{\"end\":59157,\"start\":59143},{\"end\":59444,\"start\":59425},{\"end\":59458,\"start\":59444},{\"end\":59466,\"start\":59458},{\"end\":59481,\"start\":59466},{\"end\":59496,\"start\":59481},{\"end\":59718,\"start\":59705},{\"end\":59727,\"start\":59718},{\"end\":59736,\"start\":59727},{\"end\":59748,\"start\":59736},{\"end\":59759,\"start\":59748},{\"end\":59774,\"start\":59759},{\"end\":59785,\"start\":59774},{\"end\":59795,\"start\":59785},{\"end\":59805,\"start\":59795},{\"end\":59818,\"start\":59805},{\"end\":59829,\"start\":59818},{\"end\":59844,\"start\":59829},{\"end\":59852,\"start\":59844},{\"end\":59864,\"start\":59852},{\"end\":59879,\"start\":59864},{\"end\":59888,\"start\":59879},{\"end\":59900,\"start\":59888},{\"end\":59914,\"start\":59900},{\"end\":59926,\"start\":59914},{\"end\":59934,\"start\":59926},{\"end\":59943,\"start\":59934},{\"end\":59952,\"start\":59943},{\"end\":59966,\"start\":59952},{\"end\":59976,\"start\":59966},{\"end\":59990,\"start\":59976},{\"end\":60472,\"start\":60458},{\"end\":60487,\"start\":60472},{\"end\":60505,\"start\":60487},{\"end\":60517,\"start\":60505},{\"end\":60532,\"start\":60517},{\"end\":60546,\"start\":60532},{\"end\":60560,\"start\":60546},{\"end\":60580,\"start\":60560},{\"end\":60909,\"start\":60898},{\"end\":60922,\"start\":60909},{\"end\":60935,\"start\":60922},{\"end\":60947,\"start\":60935},{\"end\":61215,\"start\":61201},{\"end\":61231,\"start\":61215},{\"end\":61243,\"start\":61231},{\"end\":61534,\"start\":61519},{\"end\":61550,\"start\":61534},{\"end\":61573,\"start\":61550},{\"end\":61591,\"start\":61573},{\"end\":61604,\"start\":61591},{\"end\":61617,\"start\":61604},{\"end\":61956,\"start\":61945},{\"end\":61973,\"start\":61956},{\"end\":62265,\"start\":62248},{\"end\":62282,\"start\":62265},{\"end\":62297,\"start\":62282},{\"end\":62518,\"start\":62505},{\"end\":62529,\"start\":62518},{\"end\":62547,\"start\":62529},{\"end\":62563,\"start\":62547},{\"end\":62576,\"start\":62563},{\"end\":62592,\"start\":62576},{\"end\":62604,\"start\":62592},{\"end\":62621,\"start\":62604},{\"end\":62634,\"start\":62621},{\"end\":62646,\"start\":62634},{\"end\":62979,\"start\":62967},{\"end\":62990,\"start\":62979},{\"end\":63004,\"start\":62990},{\"end\":63351,\"start\":63344},{\"end\":63359,\"start\":63351},{\"end\":63369,\"start\":63359},{\"end\":63381,\"start\":63369},{\"end\":63396,\"start\":63381},{\"end\":63410,\"start\":63396},{\"end\":63723,\"start\":63709},{\"end\":63738,\"start\":63723},{\"end\":63753,\"start\":63738},{\"end\":63768,\"start\":63753},{\"end\":63781,\"start\":63768},{\"end\":63799,\"start\":63781},{\"end\":63814,\"start\":63799},{\"end\":63829,\"start\":63814},{\"end\":63845,\"start\":63829},{\"end\":63857,\"start\":63845},{\"end\":63875,\"start\":63857},{\"end\":63891,\"start\":63875},{\"end\":64253,\"start\":64238},{\"end\":64269,\"start\":64253},{\"end\":64282,\"start\":64269},{\"end\":64294,\"start\":64282},{\"end\":64308,\"start\":64294},{\"end\":64322,\"start\":64308},{\"end\":64333,\"start\":64322},{\"end\":64349,\"start\":64333},{\"end\":64822,\"start\":64800},{\"end\":64833,\"start\":64822},{\"end\":64848,\"start\":64833},{\"end\":64853,\"start\":64848},{\"end\":65217,\"start\":65202},{\"end\":65227,\"start\":65217},{\"end\":65246,\"start\":65227},{\"end\":65260,\"start\":65246},{\"end\":65551,\"start\":65540},{\"end\":65563,\"start\":65551},{\"end\":65572,\"start\":65563},{\"end\":65580,\"start\":65572},{\"end\":65590,\"start\":65580},{\"end\":65600,\"start\":65590},{\"end\":65612,\"start\":65600},{\"end\":65876,\"start\":65864},{\"end\":65892,\"start\":65876},{\"end\":65904,\"start\":65892},{\"end\":65916,\"start\":65904},{\"end\":65928,\"start\":65916},{\"end\":65940,\"start\":65928},{\"end\":66221,\"start\":66211},{\"end\":66235,\"start\":66221},{\"end\":66250,\"start\":66235},{\"end\":66264,\"start\":66250},{\"end\":66281,\"start\":66264},{\"end\":66553,\"start\":66544},{\"end\":66567,\"start\":66553},{\"end\":66803,\"start\":66785},{\"end\":66814,\"start\":66803},{\"end\":66835,\"start\":66814},{\"end\":66853,\"start\":66835},{\"end\":66869,\"start\":66853},{\"end\":66883,\"start\":66869},{\"end\":67201,\"start\":67191},{\"end\":67212,\"start\":67201},{\"end\":67225,\"start\":67212},{\"end\":67233,\"start\":67225},{\"end\":67249,\"start\":67233},{\"end\":67268,\"start\":67249},{\"end\":67501,\"start\":67487},{\"end\":67511,\"start\":67501},{\"end\":67521,\"start\":67511},{\"end\":67532,\"start\":67521},{\"end\":67546,\"start\":67532},{\"end\":67825,\"start\":67813},{\"end\":67843,\"start\":67825},{\"end\":67858,\"start\":67843},{\"end\":68079,\"start\":68062},{\"end\":68092,\"start\":68079},{\"end\":68104,\"start\":68092},{\"end\":68119,\"start\":68104},{\"end\":68138,\"start\":68119},{\"end\":68447,\"start\":68439},{\"end\":68456,\"start\":68447},{\"end\":68466,\"start\":68456},{\"end\":68476,\"start\":68466},{\"end\":68741,\"start\":68727},{\"end\":68751,\"start\":68741},{\"end\":69049,\"start\":69036},{\"end\":69060,\"start\":69049},{\"end\":69074,\"start\":69060},{\"end\":69093,\"start\":69074},{\"end\":69486,\"start\":69474},{\"end\":69501,\"start\":69486},{\"end\":69517,\"start\":69501},{\"end\":69531,\"start\":69517},{\"end\":69555,\"start\":69531},{\"end\":69899,\"start\":69886},{\"end\":69911,\"start\":69899},{\"end\":69922,\"start\":69911},{\"end\":69935,\"start\":69922},{\"end\":69947,\"start\":69935},{\"end\":69960,\"start\":69947},{\"end\":69969,\"start\":69960},{\"end\":69984,\"start\":69969},{\"end\":70334,\"start\":70321},{\"end\":70346,\"start\":70334},{\"end\":70357,\"start\":70346},{\"end\":70370,\"start\":70357},{\"end\":70382,\"start\":70370},{\"end\":70395,\"start\":70382},{\"end\":70404,\"start\":70395},{\"end\":70419,\"start\":70404},{\"end\":70777,\"start\":70764},{\"end\":70791,\"start\":70777},{\"end\":70806,\"start\":70791},{\"end\":71069,\"start\":71059},{\"end\":71082,\"start\":71069},{\"end\":71093,\"start\":71082},{\"end\":71103,\"start\":71093},{\"end\":71118,\"start\":71103},{\"end\":71129,\"start\":71118},{\"end\":71144,\"start\":71129},{\"end\":71438,\"start\":71422},{\"end\":71450,\"start\":71438},{\"end\":71465,\"start\":71450},{\"end\":71476,\"start\":71465},{\"end\":71486,\"start\":71476},{\"end\":71500,\"start\":71486},{\"end\":71513,\"start\":71500},{\"end\":71524,\"start\":71513}]", "bib_venue": "[{\"end\":50252,\"start\":50233},{\"end\":50553,\"start\":50549},{\"end\":50987,\"start\":50983},{\"end\":51359,\"start\":51355},{\"end\":51712,\"start\":51708},{\"end\":51982,\"start\":51921},{\"end\":52367,\"start\":52334},{\"end\":52955,\"start\":52951},{\"end\":53316,\"start\":53302},{\"end\":53686,\"start\":53612},{\"end\":53976,\"start\":53905},{\"end\":54306,\"start\":54263},{\"end\":54860,\"start\":54855},{\"end\":55276,\"start\":55238},{\"end\":55628,\"start\":55624},{\"end\":55878,\"start\":55771},{\"end\":56218,\"start\":56200},{\"end\":56466,\"start\":56406},{\"end\":56779,\"start\":56758},{\"end\":57057,\"start\":57053},{\"end\":57319,\"start\":57315},{\"end\":57555,\"start\":57551},{\"end\":58025,\"start\":58006},{\"end\":58476,\"start\":58471},{\"end\":58796,\"start\":58792},{\"end\":59162,\"start\":59157},{\"end\":59423,\"start\":59355},{\"end\":60584,\"start\":60580},{\"end\":60951,\"start\":60947},{\"end\":61268,\"start\":61243},{\"end\":61621,\"start\":61617},{\"end\":62003,\"start\":61973},{\"end\":62301,\"start\":62297},{\"end\":62659,\"start\":62646},{\"end\":63044,\"start\":63004},{\"end\":63342,\"start\":63258},{\"end\":63895,\"start\":63891},{\"end\":64402,\"start\":64362},{\"end\":64891,\"start\":64853},{\"end\":65263,\"start\":65260},{\"end\":65616,\"start\":65612},{\"end\":65943,\"start\":65940},{\"end\":66285,\"start\":66281},{\"end\":66572,\"start\":66567},{\"end\":66887,\"start\":66883},{\"end\":67272,\"start\":67268},{\"end\":67612,\"start\":67562},{\"end\":67862,\"start\":67858},{\"end\":68142,\"start\":68138},{\"end\":68480,\"start\":68476},{\"end\":68762,\"start\":68751},{\"end\":69126,\"start\":69093},{\"end\":69558,\"start\":69555},{\"end\":69988,\"start\":69984},{\"end\":70319,\"start\":70222},{\"end\":70810,\"start\":70806},{\"end\":71150,\"start\":71144},{\"end\":71528,\"start\":71524}]"}}}, "year": 2023, "month": 12, "day": 17}