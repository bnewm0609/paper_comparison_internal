{"id": 246015780, "updated": "2023-11-08 13:57:25.961", "metadata": {"title": "Towards Unsupervised Deep Graph Structure Learning", "authors": "[{\"first\":\"Yixin\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Daokun\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Hongxu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Shirui\",\"last\":\"Pan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In recent years, graph neural networks (GNNs) have emerged as a successful tool in a variety of graph-related applications. However, the performance of GNNs can be deteriorated when noisy connections occur in the original graph structures; besides, the dependence on explicit structures prevents GNNs from being applied to general unstructured scenarios. To address these issues, recently emerged deep graph structure learning (GSL) methods propose to jointly optimize the graph structure along with GNN under the supervision of a node classification task. Nonetheless, these methods focus on a supervised learning scenario, which leads to several problems, i.e., the reliance on labels, the bias of edge distribution, and the limitation on application tasks. In this paper, we propose a more practical GSL paradigm, unsupervised graph structure learning, where the learned graph topology is optimized by data itself without any external guidance (i.e., labels). To solve the unsupervised GSL problem, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) with the aid of self-supervised contrastive learning. Specifically, we generate a learning target from the original data as an\"anchor graph\", and use a contrastive loss to maximize the agreement between the anchor graph and the learned graph. To provide persistent guidance, we design a novel bootstrapping mechanism that upgrades the anchor graph with learned structures during model learning. We also design a series of graph learners and post-processing schemes to model the structures to learn. Extensive experiments on eight benchmark datasets demonstrate the significant effectiveness of our proposed SUBLIME and high quality of the optimized graphs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.06367", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/LiuZZCPP22", "doi": "10.1145/3485447.3512186"}}, "content": {"source": {"pdf_hash": "a8d0fdfaf2aa42f7a90ba90a865849cfad70778b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.06367v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "604237816b7dee367a694d1158793c6311ff30ed", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a8d0fdfaf2aa42f7a90ba90a865849cfad70778b.txt", "contents": "\nTowards Unsupervised Deep Graph Structure Learning\nACMCopyright ACMApril 25-29, 2022\n\nYixin Liu yixin.liu@monash.edu \nMonash University\n\n\nYu Zheng yu.zheng@latrobe.edu.au \nLa Trobe University\n\n\nDaokun Zhang daokun.zhang@monash.edu \nMonash University\n\n\nMonash Suzhou Research Institute\n\n\nHongxu Chen hongxu.chen@uts.edu.au \nUniversity of Technology Sydney\n\n\nHao Peng penghao@buaa.edu.cn \nBeihang University\nWWW '22, April 25-292022LyonFrance\n\nShirui Pan shirui.pan@monash.edu \nMonash University\n\n\nYixin Liu \nMonash University\n\n\nYu Zheng \nLa Trobe University\n\n\nDaokun Zhang \nMonash University\n\n\nMonash Suzhou Research Institute\n\n\nHongxu Chen \nUniversity of Technology Sydney\n\n\nHao Peng \nBeihang University\nWWW '22, April 25-292022LyonFrance\n\nTowards Unsupervised Deep Graph Structure Learning\n\nProceedings of the ACM Web Conference 2022 (WWW '22)\nthe ACM Web Conference 2022 (WWW '22)Lyon, France; New York, NY, USAACM12April 25-29, 202210.1145/XXXXXX.XXXXXXACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00graph neural networksgraph structure learningunsupervised learningcontrastive learning * Corresponding author\nIn recent years, graph neural networks (GNNs) have emerged as a successful tool in a variety of graph-related applications. However, the performance of GNNs can be deteriorated when noisy connections occur in the original graph structures; besides, the dependence on explicit structures prevents GNNs from being applied to general unstructured scenarios. To address these issues, recently emerged deep graph structure learning (GSL) methods propose to jointly optimize the graph structure along with GNN under the supervision of a node classification task. Nonetheless, these methods focus on a supervised learning scenario, which leads to several problems, i.e., the reliance on labels, the bias of edge distribution, and the limitation on application tasks. In this paper, we propose a more practical GSL paradigm, unsupervised graph structure learning, where the learned graph topology is optimized by data itself without any external guidance (i.e., labels). To solve the unsupervised GSL problem, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) with the aid of self-supervised contrastive learning. Specifically, we generate a learning target from the original data as an \"anchor graph\", and use a contrastive loss to maximize the agreement between the anchor graph and the learned graph. To provide persistent guidance, we design a novel bootstrapping mechanism that upgrades the anchor graph with learned structures during model learning. We also design a series of graph learners and post-processing schemes to model the structures to learn. Extensive experiments on eight benchmark datasets demonstrate the significant effectiveness of our proposed SUBLIME and high quality of the optimized graphs.CCS CONCEPTS\u2022 Mathematics of computing \u2192 Graph algorithms; \u2022 Computing methodologies \u2192 Neural networks.\n\nINTRODUCTION\n\nRecent years have witnessed the prosperous development of graphbased applications in numerous domains, such as chemistry, bioinformatics and cybersecurity. As a powerful deep learning tool to model graph-structured data, graph neural networks (GNNs) have drawn increasing attention and achieved state-of-the-art performance in various graph analytical tasks, including node classification [22,40], link prediction [21,32], and node clustering [42,55]. GNNs usually follow a message-passing scheme, where node representations are learned by aggregating information from the neighbors on an observed topology (i.e., the original graph structure).\n\nMost GNNs rely on a fundamental assumption that the original structure is credible enough to be viewed as ground-truth information for model training. Such assumption, unfortunately, is usually violated in real-world scenarios, since graph structures are usually extracted from complex interaction systems which inevitably contain uncertain, redundant, wrong and missing connections [45]. Such noisy information in original topology can seriously damage the performance of GNNs. Besides, the reliance on explicit structures hinders GNNs' broad applicability. If GNNs are capable of uncovering the implicit relations between samples, e.g., two images containing the same object, they can be applied to more general domains like vision and language.\n\nTo tackle the aforementioned problems, deep graph structure learning (GSL) is a promising solution that constructs and improves the graph topology with GNNs [7,12,20,58]. Concretely, these methods parameterize the adjacency matrix with a probabilistic model [12,45], full parameterization [20] or metric learning model [7,11,53], and jointly optimize the parameters of the adjacency matrix and GNNs by solving a downstream task (i.e., node classification) [58]. However, existing methods learn graph structures in a supervised scenario, which brings the following issues: (1) The reliance on label information. In supervised GSL methods, humanannotated labels play an important role in providing supervision signal for structure improvement. Such reliance on labels limits the application of supervised GSL on more general cases where annotation is unavailable. (2) The bias of learned edge distribution. Node classification usually follows a semi-supervised setting, where only a small fraction of nodes (e.g., 140/2708 in Cora dataset) are under the supervision of labels. As a result, the connections among these nodes and their neighbors would receive more guidance in arXiv:2201.06367v1 [cs.LG] 17 Jan 2022  structure learning, while the relations between nodes far away from them are rarely discovered by GSL [11]. Such imbalance leads to the bias of edge distribution, affecting the quality of the learned structures.\n\n(3) The limitation on downstream tasks. In existing methods, the structure is specifically learned for node classification, so it may contain more task-specific information rather than general knowledge. Consequently, the refined topology may not benefit other downstream tasks like link prediction or node clustering, indicating the poor generalization ability of the learned structures.\n\nTo address these issues, in this paper, we investigate a novel unsupervised learning paradigm for GSL, namely unsupervised graph structure learning. As compared in Fig. 1, in our learning paradigm, structures are learned by data itself without any external guidance (i.e., labels), and the acquired universal, edge-unbiased topology can be freely applied to various downstream tasks. In this case, one natural question can be raised: how to provide sufficient supervision signal for unsupervised GSL? To answer this, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) to learn graph structures with the aid of self-supervised contrastive learning [25]. Concretely, our method constructs an \"anchor graph\" from the original data to guide structure optimization, with a contrastive loss to maximize the mutual information (MI) between anchor graph and the learned structure. Through maximizing their consistency, informative hidden connections can be discovered, which well respects the node proximity conveyed by the original features and structures. Meanwhile, as we optimize the contrastive loss on the representations of every node, all potential edge candidates will receive the essential supervision, which promotes a balanced edge distribution in the inferred topology. Furthermore, we design a bootstrapping mechanism to update anchor graph with the learned edges, which provides a self-enhanced supervision signal for GSL. Besides, we carefully design multiple graph learners and post-processing schemes to model graph topology for diverse data. In summary, our core contributions are three-fold:\n\n\u2022 Problem. We propose a novel unsupervised learning paradigm for graph structure learning, which is more practical and challenging than the existing supervised counterpart. To the best of our knowledge, this is the first attempt to learn graph structures with GNNs in an unsupervised setting. \u2022 Algorithm. We propose a novel unsupervised GSL method SUBLIME, which guides structure optimization by maximizing the agreement between the learned structure and a crafted self-enhanced learning target with contrastive learning.\n\n\u2022 Evaluations. We perform extensive experiments to corroborate the effectiveness and analyze the properties of SUBLIME via thorough comparisons with state-of-the-art methods on eight benchmark datasets.\n\n\nRELATED WORK 2.1 Graph Neural Networks\n\nGraph neural networks (GNNs) are a type of deep neural networks aiming to learn low-dimensional representations for graphstructure data [22,48]. Modern GNNs can be categorized into two types: spectral and spatial methods. The spectral methods perform convolution operation to graph domain using spectral graph filter [3] and its simplified variants, e.g., Chebyshev polynomials filter [9] and the first-order approximation of Chebyshev filter [22]. The spatial methods perform convolution operation by propagating and aggregating local information along edges in a graph [15,40,50]. In spatial GNNs, different aggregation functions are designed to learn node representations, including mean/max pooling [15], LSTM [15], self-attention [40], and summation [50]. Readers may refer to the elaborate survey [48] for a thorough review.\n\n\nDeep Graph Structure Learning\n\nGraph structure learning (GSL) problem has been investigated by conventional machine learning techniques in graph signal processing [10], spectral clustering [2], and network science [26]. However, these methods are not capable of handling graph data with highdimensional features, so they are not further discussed in our paper. Very recently, there thrives a branch of research that investigates GSL for GNNs with the aim to boost their performance on downstream tasks, which is named deep graph structure learning [58]. These methods follow a general pipeline: the graph adjacency matrix is modeled with learnable parameters, and then jointly optimized along with GNN under the supervision of a downstream node classification task. In these methods, various techniques are leveraged to parameterize the adjacency matrix. Considering the discrete nature of graph structures, one type of methods adopts probabilistic models, such as Bernoulli probability model [12] and stochastic block model [45]. Another type of methods models structures with node-wise similarity computed by metric learning functions like cosine similarity [7] and dot production [11,53]. Besides, directly treating each element in adjacency matrix as a learnable parameter is also an effective solution [11,20]. Nevertheless, the existing deep GSL approaches follow a supervised scenario where node labels are always required to refine the graph structures. In this paper, differently, we advocate a more practical unsupervised learning paradigm where no extra information is needed for GSL.\n\n\nContrastive Learning on Graphs\n\nAfter achieving significant performance in visual [6,14] and linguistic [8,13] domains, contrastive learning has shown competitive performance and become increasingly popular in graph representation learning [32,39,59]. Graph contrastive learning obeys the principle of mutual information (MI) maximization, which pulls the representations of samples with shared semantic information closer while pushing the representations of irrelevant samples away [25]. In graph data, the MI maximization can be carried out to samples in the same scale (i.e., node-level [19,41,59] and graph-level [52]) or different scales (i.e., node v.s. graph [39,57] and node v.s. subgraph [32]). Graph contrastive learning also benefits diverse applications, such as chemical prediction [47], anomaly detection [18,24], federated learning [36,56], and recommendation [54]. However, it still remains unclear how to effectively improve GSL using contrastive learning.\n\n\nPROBLEM DEFINITION\n\nBefore we make the problem statement of unsupervised GSL, we first introduce the definition of graphs. An attributed graph can be represented by G = (V, E, X) = (A, X), where V is the set of = |V | nodes, E is the set of = |E | edges, X \u2208 R \u00d7 is the node feature matrix (where the -th row x is the feature vector of node ), and A \u2208 [0, 1] \u00d7 is the weighted adjacency matrix (where is the weight of the edge connecting and ). Frequently used notations are summarized in Appendix A.\n\nIn this paper, we consider two unsupervised GSL tasks, i.e., structure inference and structure refinement. The former is applicable to general datasets where graph structures are not predefined or are unavailable. The latter, differently, aims to modify the given noisy topology and produce a more informative graph. Node labels are unavailable for structure optimization in both tasks.\n\nDefinition 3.1 (Structure inference). Given a feature matrix X \u2208 R \u00d7 , the target of structure inference is to automatically learn a graph topology S \u2208 [0, 1] \u00d7 , which reflects the underlying correlations among data samples. In particular, S \u2208 [0, 1] indicates whether there is an edge between two samples (nodes) x and x . Definition 3.2 (Structure refinement). Given a graph G = (A, X) with a noisy graph structure A, the target of structure refinement is to refine A to be the optimized adjacency matrix S \u2208 [0, 1] \u00d7 to better capture the underlying dependency between nodes.\n\nWith the graph topology S which is either learned automatically from data or refined from an existing graph structure, the hypothesis is that the model performance on downstream tasks can be essentially improved with G = (S, X) as the input.\n\n\nMETHODOLOGY\n\nThis section elaborates our proposed SUBLIME, a novel unsupervised GSL framework. As shown in Fig. 2, SUBLIME on the highest level consists of two components: the graph structure learning module that models and regularizes the learned graph topology and the structure bootstrapping contrastive learning module that provides a self-optimized supervision signal for GSL. In the graph structure learning module, a sketched adjacency matrix is first parameterized by a graph learner, and then refined by a post-processor to be the learned adjacency matrix. Afterwards, in the structure bootstrapping contrastive learning module, we first establish two different views to contrast: learner view that discovers graph structure and anchor view that provides guidance for structure learning. Then, after data augmentation, the agreement between two views is maximized by a node-level contrastive learning. Specially, we design a structure bootstrapping mechanism to update anchor view with learned structures. The following subsections illustrate these crucial components respectively.\n\n\nGraph Learner\n\nAs a key component of GSL, the graph learner generates a sketched adjacency matrixS \u2208 R \u00d7 with a parameterized model. Most existing methods [7,12,20] adopt a single strategy to model graph structure, which cannot adapt to data with different unique properties. To find optimal structures for various data, we consider four types of graph learners, including a full graph parameterization (FGP) learner and three metric learning-based learners (i.e., Attentive, MLP, and GNN learner). In general, we formulate a graph learner as (\u00b7), where is the learnable parameters.\n\nFGP learner directly models each element of the adjacency matrix by an independent parameter [11,12,20] without any extra input. Formally, FGP learner is defined as:\nS = = (\u2126),(1)\nwhere = \u2126 \u2208 R \u00d7 is a parameter matrix and (\u00b7) is a non-linear function that makes training more stable. The assumption behind FGP learner is that each edge exists independently in the graph. Different from the FGP learner, metric learning-based learners [7,58] first acquire node embeddings E \u2208 R \u00d7 from the input data, and then modelS with pair-wise similarity of the node embeddings:\nS = (X, A) = (\u210e (X, A)) = (E),(2)\nwhere \u210e (\u00b7) is a neural network-based embedding function (a.k.a. embedding network) with parameter , and (\u00b7) is a non-parametric metric function (e.g., cosine similarity or Minkowski distance) that calculates pair-wise similarity. For different \u210e (\u00b7), we provide three specific instances of metric learning-based learners: Attentive, MLP, and GNN learners.\n\nAttentive Learner employs a GAT-like [40] attentive network as its embedding network, where each layer compute the Hadamard production of input feature vector and the parameter vector:\nE ( ) = \u210e ( ) (E ( \u22121) ) = ([e ( \u22121) 1 \u2299 ( ) , \u00b7 \u00b7 \u00b7 , e ( \u22121) \u2299 ( ) ] \u22ba ),(3)\nin which E ( ) is the output matrix of the -th layer of embedding network, e ( \u22121) \u2208 R is the transpose of the -th row vector of E ( \u22121) , ( ) \u2208 R is the parameter vector of the -th layer, \u2299 is the Hadamard operation, (\u00b7) \u22ba is the transposition operation, and (\u00b7) is a non-linear operation. The input of the first layer E (0) is the feature matrix X, and the output of the final layer E ( ) ( is the layer number of embedding network) is the embedding matrix E. Attentive learner assumes that each feature has different contribution to the existence of edge, but there is no significant correlation between features. MLP Learner uses a Multi-Layer Perception (MLP) as its embedding network, where a single layer can be written by:\nE ( ) = \u210e ( ) (E ( \u22121) ) = (E ( \u22121) \u2126 ( ) ),(4)\nwhere \u2126 ( ) \u2208 R \u00d7 is the parameter martix of the -th layer, and the other notations are similar to Eq. (3). Compared to attentive learner, MLP learner further considers the correlation and combination of features, generating more informative embeddings for downstream similarity metric learning.\n\nGNN Learner integrates features X and original structure A into node embeddings E via GNN-based embedding network. Due to the reliance on original topology, GNN learner is only used for  Figure 2: The overall pipeline of SUBLIME. In the graph structure learning module, the graph learner generates the sketched adjacency matrixS, and then the post processor convertsS into the learned structure S. After that, the structure bootstrapping contrastive learning module optimizes S by maximizing the agreement between the learner view and anchor view.\n! !\" , ! #$ ! !\" , ! #$ Anchor View Learner View Graph Learner # % GNN Encoder $ & MLP Projector % ' GNN Encoder $ & MLP Projector % ' & ( \u0305 & ( \u0305 & ) & ) Post- processor ( shared\nthe structure refinement task. For simplicity, we take GCN layers [22] to form embedded network:\nE ( ) = \u210e ( ) (E ( \u22121) , A) = D \u2212 1 2 A D \u2212 1 2 E ( \u22121) \u2126 ( ) ,(5)\nwhere A = A + I is the adjacency matrix with self-loop, D is the degree matrix of A, and the other notations are similar to Eq. (4). GNN Learner assumes that the connection between two nodes is related to not only features but also the original structure.\n\nIn SUBLIME, we choose the most suitable learner to modelS according to the characteristics of different datasets. In Appendix B, we analyze the properties of different graph learners and discuss how we allocate learners for each dataset.\n\n\nPost-processor\n\nThe post-processor (\u00b7) aims to refine the sketched adjacency ma-trixS into a sparse, non-negative, symmetric and normalized adjacency matrix S. To this end, four post-processing steps are applied sequentially, i.e., sparsification (\u00b7), activation (\u00b7), symmetrization (\u00b7), and normalization (\u00b7). Sparsification. The sketched adjacency matrixS is often dense, representing a fully connected graph structure. However, such adjacency matrix usually makes little sense for most applications and results in expensive computation cost [45]. Hence, we conduct a k-nearest neighbors (kNN)-based sparsification onS. Concretely, for each node, we keep the edges with top-k connection values and set the rest to 0. The sparsification (\u00b7) is expressed as:\nS ( ) = S = S ,S \u2208 top-k(S ), 0,S \u2209 top-k(S ),(6)\nwhere top-k(S ) is the set of top-k values of row vectorS . To keep the gradient flow, we do not apply sparsification for the FGP learner. For large-scale graphs, we perform the kNN sparsification with its locality-sensitive approximation [11] where the nearest neighbors are selected from a batch of nodes instead of all nodes, which reduces the requirement of memory. Symmetrization and Activation. In real-world graphs, the connections are often bi-directional, which requires a symmetric adjacency matrix. In addition, the edge weights should be non-negative according to the definition of adjacency matrix. To meet these conditions, the symmetrization and activation are performed as:\nS ( ) = S ( ) = S ( ) + S ( ) \u22ba 2 ,(7)\nwhere (\u00b7) is a non-linear activation. For metric learning-based learners, we define (\u00b7) as ReLU function. For FGP learner, we apply the ELU function to prevent gradient from disappearing. Normalization. To guarantee the edge weights are within the range [0, 1], we finally conduct a normalization onS. In particular, we apply a symmetrical normalization:\nS = S ( ) = D ( ) \u2212 1 2S ( ) D ( ) \u2212 1 2 ,(8)\nwhereD ( ) is the degree matrix ofS ( ) .\n\n\nMulti-view Graph Contrastive Learning\n\nSince we have obtained a well-parameterized adjacency matrix S, a natural question that arises here is: how to provide an effective supervision signal guiding the graph structure learning without label information? Our answer is to acquire the supervision signal from data itself via multi-view graph contrastive learning. To be concrete, we construct two graph views based on the learned structure and the original data respectively. Then, data augmentation is applied to both views. Finally, we maximize the MI between two augmented views with node-level contrastive learning. 4.3.1 Graph View Establishment. Different from general graph contrastive learning methods [19,59] that obtain both views from the original data, SUBLIME defines the learned graph as one view, and constructs the other view with input data. The former, named learner view, explores potential structures in every step. The latter, named anchor view, provides a stable learning target for GSL. Learner view is directly built by integrating the learned adjacency matrix S and the feature matrix X together, which is denoted as G = (S, X). In each training iteration, S and the parameters used to model it are directly updated by gradient descent to discover optimal graph structures. In SUBLIME, we initialize learner views as the kNN graph built on features, since it is an effective way to provide a starting point for GSL, as suggested in [11,12]. Specifically, for FGP learner, we initialize the parameters corresponding to kNN edges as 1 while the rest as 0. For attentive learner, we let each element in ( ) \u2208 to be 1. Then, feature-level similarities are computed according to the metric function, and the kNN graph is obtained by the sparsification post-processing. For MLP and GNN learners, similarly, we set the embedding dimension to be and initialize \u03a9 ( ) \u2208 as identity matrices.\n\nAnchor view plays a \"teacher\" role that provides correct and stable guidance for GSL. For the structure refinement task where the original structure A is available, we define anchor view as G = (A , X) = (A, X); for the structure inference task where A is inaccessible, we take an identity matrix I as the anchor structure: G = (A , X) = (I, X). To provide a stable learning target, anchor view is not updated by gradient descent but a novel bootstrapping mechanism which will be introduced in Section 4.4. 4.3.2 Data Augmentation. In contrastive learning, data augmentation is a key to benefiting the model through exploring richer underlying semantic information by making the learning tasks more challenging to solve [6,24,59]. In SUBLIME, we exploit two simple but effective augmentation schemes, i.e., feature masking and edge dropping, to corrupt the graphs views at both structure and feature levels. Feature masking. To disturb the node features, we randomly select a fraction of feature dimensions and mask them with zeros. Formally, for a given feature matrix X, a masking vector m ( ) \u2208 {0, 1} is first sampled, where each element is drawn from a Bernoulli distribution with probability ( ) independently. Then, we mask the feature vector of each node with m ( ) :\nX = T (X) = [x 1 \u2299 m ( ) , \u00b7 \u00b7 \u00b7 , x \u2299 m ( ) ] \u22ba ,(9)\nwhere X is the augmented feature matrix, T (\u00b7) is the feature masking transformation, and x is the transpose of the i-th row vector of X. Edge dropping. Apart from masking features, we corrupt the graph structure by randomly dropping a portion of edges. Specifically, for a given adjacency matrix A, we first sample a masking matrix M ( ) \u2208 {0, 1} \u00d7 , where each element M ( ) is drawn from a Bernoulli distribution with probability ( ) independently. After that, the adjacency matrix is masked with M ( ) :\nA = T (A) = A \u2299 M ( ) ,(10)\nwhere A is the augmented adjacency matrix, and T (\u00b7) is the edge dropping transformation.\n\nIn SUBLIME, we jointly leverage these two augmentation schemes to generate augmented graphs on both learner and anchor views:\nG = (T (S), T (X)), G = (T (A ), T (X)),(11)\nwhere G and G are the augmented learner view and anchor view, respectively. To obtain different contexts in the two views, the feature masking for two views employs different probabilities ( ) \u2260 ( ) . For edge dropping, since the adjacency matrices of two views are already significantly different, we use the same dropping probability ( ) = ( ) = ( ) . Note that other advanced augmentation schemes can also be applied to SUBLIME, which is left for our future research. 4.3.3 Node-level Contrastive Learning. After obtaining two augmented graph views, we perform a node-level contrastive learning to maximize the MI between them. In SUBLIME, we adopt a simple contrastive learning framework originated from SimCLR [6] which consists of the following components: GNN-based encoder. A GNN-based encoder (\u00b7) extracts nodelevel representations for augmented graphs G and G :\nH = (G ), H = (G ),(12)\nwhere is the parameter of encoder (\u00b7), and H , H \u2208 R \u00d7 1 ( 1 is the representation dimension) are the node representation matrices for learner/anchor views, respectively. In SUBLIME, we utilize GCN [22] as our encoder and set its layer number 1 to 2. MLP-based projector. Following the encoder, a projector (\u00b7) with 2 MLP layers maps the representations to another latent space where the contrastive loss is calculated:\nZ = (H ), Z = (H ),(13)\nwhere is the parameter of projector (\u00b7), and Z , Z \u2208 R \u00d7 2 ( 2 is the projection dimension) are the projected node representation matrices for learner/anchor views, respectively. Node-level contrastive loss function. A contrastive loss L is leveraged to enforce maximizing the agreement between the projections , and , of the same node on two views. In our framework, a symmetric normalized temperature-scaled cross-entropy loss (NT-Xent) [29,35] is applied:\nL = 1 2 \u2211\ufe01 =1 \u2113 ( , , , ) + \u2113 ( , , , ) , \u2113 ( , , , ) = log sim(z , ,z , )/ =1 sim(z , ,z , )/ ,(14)\nwhere sim(\u00b7, \u00b7) is the cosine similarity function, and is the temperature parameter. \u2113 ( , , , ) is computed following \u2113 ( , , , ).\n\n\nStructure Bootstrapping Mechanism\n\nWith a fixed anchor adjacency matrix A defined by A or I, SUBLIME can learn graph structure S by maximizing the MI between two views. However, using a constant anchor graph may lead to several issues: (1) Inheritance of error information. Since A is directly borrowed from the input data, it would carry some natural noise (e.g., missing or redundant edges) of the original graph. If the noise is not eliminated in the learning process, the learned structures will finally inherit it.\n\n(2) Lack of persistent guidance. A fixed anchor graph contains limited information to guide GSL. Once the graph learner captures this information, it will be hard for the model to gain effective supervision in the following training steps. (3) Overfitting the anchor structure. Driven by the learning objective that maximizes the agreement between two views, the learned structure tends to over-fit the fixed anchor structure, resulting in a similar testing performance to the original data. Inspired by previous bootstrapping-based algorithms [5,14,37], we design a structure bootstrapping mechanism to provide a selfenhanced anchor view as the learning target. The core idea of our solution is to update the anchor structure A with a slow-moving augmentation of the learned structure S instead of keeping A unchanged. In particular, given a decay rate \u2208 [0, 1], the anchor structure A is updated every iterations as following: Benefiting from the structure bootstrapping mechanism, SUBLIME has nice properties that can address the aforementioned problems. With the process of updating, the weights of some noise edges gradually decrease in A , which relieves their negative impact on structure learning. Meanwhile, since the learning target A is changing during the training phase, it can always incorporate more effective information to guide the learning of topology, and the over-fitting problem is naturally resolved. More importantly, our structure bootstrapping mechanism leverages the learned knowledge to improve the learning target in turn, pushing the model to discover increasingly optimal graph structure constantly. Besides, the slow-moving average (with > 0.99) updating ensures the stability of training.\nA \u2190 A + (1 \u2212 )S.(15)\n\nOverall Framework\n\nIn this subsection, we first illustrate the training process of SUBLIME, and then introduce the tricks to help apply it to large-scale graphs. Model training. In our training process, we first initialize the parameters and anchor adjacency matrix A . Then, in each iteration, we perform forward propagation to compute the contrastive loss L, and update all the parameters jointly via back propagation. After back propagation, we update A by bootstrapping structure mechanism every iterations. Finally, we acquire the learned topology represented by S. As analyzed in Appendix C, the time complexity of SUBLIME is O ( 2 + 1 1 + 2 1 1 + 2 2 2 + ). The algorithmic description is provided in Appendix D. Scalability extension. To extend the scalability of SUBLIME, the key is to avoid O ( 2 ) space complexity and time complexity. To this end, we adopt the following measures: (1) To avoid explosive number of parameters, we use metric learning-based learners instead of FGP learner. (2) For sparsification post-processing, we consider a locality-sensitive approximation for kNN graph [11]. (3) For graph contrastive learning, we compute the contrastive loss L for a mini-batch of samples instead of all nodes. (4) To reduce the space complexity of the bootstrapped structure, we perform the update in Eq. (15) with a larger iteration interval ( \u2265 10).\n\n\nEXPERIMENTS\n\nIn this section, we conduct empirical experiments to demonstrate the effectiveness of the proposed framework SUBLIME. We aim to answer five research questions as follows: RQ1: How effective is SUBLIME for learning graph structure under unsupervised settings? RQ2: How does the structure bootstrapping mechanism influence the performance of SUBLIME? RQ3: How do key hyper-parameters impact the performance of SUBLIME? RQ4: How robust is SUBLIME to adversarial graph structures? and RQ5: What kind of graph structure is learned by SUBLIME?\n\n\nExperimental Setups\n\nDownstream tasks for evaluation. We use node classification and node clustering tasks to evaluate the quality of learned topology. For node classification, We conduct experiments on both structure inference/refinement scenarios, and use classification accuracy as our metric. For node clustering, the experiments are conducted on structure refinement scenario, and four metrics are employed, including clustering accuracy (C-ACC), Normalized Mutual Information (NMI), F1-score (F1) and Adjusted Rand Index (ARI). Datasets. We evaluate SUBLIME on eight real-world benchmark datasets, including four graph-structured datasets (i.e., Cora, Citeseer [34], Pubmed [27] and ogbn-arxiv [17]) and four non-graph datasets (i.e., Wine, Cancer, Digits and 20news [1]). Details of datasets are summarized in Appendix E. Baselines. For node classification, we mainly compare SUBLIME with two categories of methods, including three structure-fixed GNN methods (i.e., GCN [22], GAT [40] and GraphSAGE (SAGE for short) [15]), and six supervised GSL methods (i.e., LDS [12], GRCN [53], Pro-GNN [20], GEN [45], IDGL [7] and SLAPS [11]). We also consider GDC [23], a diffusion-based graph structure improvement method, and SLAPS-2s, a variant of SLAPS [11] which only uses denoising autoencoder to learn topology, as two baselines of unsupervised GSL. In structure inference scenario, we further add three conventional feature-based classifiers (Logistic Regression, Linear SVM and MLP) for comparison. For node clustering task, we consider baseline methods belonging to the following three  categories: 1) feature-based clustering methods (i.e., K-means [16] and Spectral Clustering (SC for short) [28]); 2) structure-based clustering methods (i.e., GraphEncoder (GE for short) [38], DeepWalk (DW for short) [33], DNGR [4] and M-NMF [46]); and 3) attributed graph clustering methods (i.e., RMSC [49], TADW [51], VGAE [21], ARGA [30], MGAE [43], AGC [55] and DAEGC [42]). For other experimental details, including infrastructures and hyper-parameter, interested readers can refer to Appendix F. Our code is available at https://github.com/GRAND-Lab/SUBLIME.\n\n\nPerformance Comparison (RQ1)\n\nNode classification in structure inference scenario. Table 1 reports the classification accuracy of our method and other baselines in structure inference scenario. For structure-fixed GNNs (i.e., GCN, GAT and GraphSAGE) and GSL methods designed for structure refinement scenarios (i.e., GRCN, Pro-GNN, GEN and GDC), we use kNN graphs as their input graphs, where is tuned in the same search space to our method.\n\nAs can be observed, without the guidance of labels, our proposed SUBLIME outperforms all baselines on 3 out of 8 benchmarks and achieves the runner-up results on the rest datasets. This competitive performance benefits from the novel idea of guiding GSL with a selfenhanced learning target by graph contrastive learning. Besides, the result on ogbn-arxiv exhibits the scailbility of SUBLIME.\n\nWe make other observations as follows. Firstly, the performance of structure-fixed GNNs (taking kNN graphs as input) is superior   to conventional feature-based classifiers on most datasets, which shows the benefit of considering the underlying relationship among samples. Secondly, GSL methods achieve better performance than structure-fixed methods, indicating the significance of structure optimization. Thirdly, compared to supervised GSL methods, the unsupervised methods also achieve competitive results without the supervision of labels, which shows their effectiveness. Node classification in structure refinement scenario. Table  2 summarizes the classification performance of each method in structure refinement scenario. We find that SUBLIME still shows very promising results against not only the self-supervised but also supervised methods, indicating that SUBLIME can leverage selfsupervision signal to improve the original graphs effectively. Node clustering in structure refinement scenario. In Table 3, we report the results of node clustering. Compared to baselines, our performance improvement illustrates that optimizing graph structures is indeed helpful to the clustering task. Meanwhile, the implementation of SUBLIME for node clustering task suggests that our learned topology can be applied to not only node classification task but also a wide range of downstream tasks.\n\n\nAblation Study (RQ2)\n\nIn our structure bootstrapping mechanism, the bootstrapping decay rate control the trade-off between updating anchor graph too sharply (with smaller ) and too slowly (with larger ). When = 1, anchor graph is never updated and remains as a constant structure.\n\nTo verify the effectiveness of the proposed mechanism, we adjust the value of and the results are shown in Table 4. We also plot the curves of accuracy and loss value w.r.t. training epoch with different , which are shown in Fig. 3. As shown in Table 4, without structure bootstrapping mechanism ( = 1), the classification accuracy decreases by 1.5% on average, indicating the mechanism helps improve the quality of learned graphs. From Fig. 3(a), we further find an obvious drop after around 1500 iterations when = 1, demonstrating the lack of effective guidance hurts the performance. When is within [0.999, 0.99999], the accuracy can converge to a high value (as shown in Fig. 3(a)),  meaning that SUBLIME can learn a stable and informative structure with the bootstrapping mechanism. However, the performance declines with becoming smaller, especially on Cora dataset. We conjecture that with sharp updating, the anchor graph tends to be polluted by the learned graph obtained in the early training stage, which fails to capture accurate connections. Another problem caused by a too small is the unstable training, which can be seen in Fig. 3(a) and 3(b).\n\n\nSensitivity Analysis (RQ3)\n\nUsing the structure inference case, we investigate the sensitivity of critical hyper-parameters in SUBLIME, including the probabilities ( ) , ( ) for data augmentation and the number of neighbors in kNN for sparsification and learner initialization. The discussion for ( ) and are provided below while the analysis for ( ) is given in Appendix G. Feature masking probability ( ) . Fig. 4(a) shows the performance under different combinations of masking probabilities of two views on Cora dataset. We observe that the value of ( ) between 0.6 and 0.8 produces higher accuracy. Compared to ( ) , SUBLIME is less sensitive to the choice of ( ) , suggesting a good performance when ( ) \u2208 [0, 0.7]. When ( ) is larger than 0.8, the features will be heavily undermined, resulting worse results. Number of neighbors . To investigate its sensitivity, we search the number of neighbors in the range of {5, 10, \u00b7 \u00b7 \u00b7 , 40} for three datasets. As is demonstrated in Fig. 4(b), the best selection for each dataset is different, i.e., = 30 for Cora, = 20 for Citeseer, and = 15 for Pubmed. A common phenomenon is that a too large or too small results in poor performance. We conjecture that an extremely small may limit the number of beneficial neighbors, while an overlarge causes some noisy connections.\n\n\nRobustness Analysis (RQ4)\n\nTo evaluate the robustness of SUBLIME against adversarial graphs, we randomly remove edges from or add edges to the original graph structure of Cora dataset and validate the performance on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9 to simulate different attack intensities. We compared our method to GCN [22] and Pro-GNN [20], a supervised graph structure method for graph adversarial defense. As we can see in Fig. 5, SUBLIME consistently achieves better or comparable results in both settings. When the edge deletion rates become larger, our method shows more significant performance gains, indicating that SUBLIME has stronger robustness against serious structural attacks.   \n\n\nVisualization (RQ5)\n\nTo investigate what kind of graph structure is learned by SUBLIME, we select a subgraph from Cora dataset with nodes in two categories and visualize the edge weights in original graph, graphs learned by Pro-GNN and SUBLIME, respectively. The selected categories are Case base (C) and Rule learning (R), each of which has 10 labeled nodes (L) and 10 unlabeled nodes (U). Note that the labels of the labeled nodes are used to refine the graph structures in Pro-GNN, but are not used to optimize topology in SUBLIME. As we can see in Fig.  6, numerous intra-class edges are learned by SUBLIME, while the learned inter-class edges are far fewer than intra-class edges. In contrast, the original graph only provides scarce intra-class edges. We conclude that SUBLIME can learn connections between two nodes sharing similar semantic information, which improves the quality of graph topology. Moreover, in Pro-GNN, there are more connections built across labeled nodes than unlabeled nodes, indicating an edge distribution bias in the graph learned by such a supervised method. Conversely, SUBLIME equally constructs edges across all nodes belonging to the same class as each node can receive the essential supervision from the contrastive objective.\n\n\nCONCLUSION\n\nIn this paper, we make the first investigation on the problem of unsupervised graph structure learning. To tackle this problem, we design a novel method, SUBLIME, which is capable of leveraging data itself to generate optimal graph structures. To learn graph structures, our method uses contrastive learning to maximize the agreement between the learned topology and a self-enhanced learning target. Extensive experiments demonstrate the superiority of SUBLIME and rationality of the learned structures.\n\n\nA NOTATIONS\n\nIn this paper, we denote scalars with letters (e.g., ), column vectors with boldface lowercase letters (e.g., x), matrices with boldface uppercase letters (e.g., X), and sets with calligraphic fonts (e.g., V).\n\nThe frequently used notations are listed in Table 5. \nNotation Description G = (A, X)\nThe (original) graph.\n\n\n, ,\n\nThe number of nodes/edges/features.\nA \u2208 [0, 1] \u00d7 The (original) adjacency matrix. X \u2208 R \u00d7\nThe feature matrix. G = (S, X)\n\nThe learned graph / Learner graph view.\nS \u2208 [0, 1] \u00d7\nThe learned adjacency matrix.\nS \u2208 R \u00d7\nThe sketched adjacency matrix.\nE \u2208 R \u00d7\nThe embedding matrix.\nG = (A , X) Anchor graph view. A \u2208 [0, 1] \u00d7\nThe anchor adjacency matrix. G , G\n\nThe augmented learner/anchor view.\n1 , 2\nThe dimension of node representation/projection. H , H \u2208 R \u00d7 1 The representation matrix of learner/anchor view. Z , Z \u2208 R \u00d7 2 The projected representation matrix of learner/anchor view. L\n\nThe contrastive loss function.\n\n\n(\u00b7)\n\nThe graph learner with parameter .\n\n\n(\u00b7)\n\nThe post-processor.\n\n\nT (\u00b7), T (\u00b7)\n\nThe feature masking/edge dropping augmentation.\n\n\n(\u00b7)\n\nThe GNN-based encoder with parameter .\n\n\n(\u00b7)\n\nThe MLP-based projector with parameter . The number of neighbors in kNN. ( ) , ( ) The masking/dropping probability for T (\u00b7)/T (\u00b7).\n\n, The decay rate/interval for bootstrapping updating. \u2299\n\nThe Hadamard operation. \u00b7 \u22ba The transposition operation.\n\n\nB ANALYSIS OF GRAPH LEARNERS\n\nIn Table 6, We summarize the properties of the proposed graph learners, including their memory, parameter and time complexity. For metric learning-based graph learners, we consider the complexities with locality-sensitive kNN sparsification post-processing [11] where the neighbors are selected from a batch of nodes (batch size = 1 ). We provide our analysis as follows:\n\n\u2022 Since FGP learner can model each edge independently and directly, it enjoys several advantages such as the flexibility to model connections and low time complexity. However, its O ( 2 ) space complexity makes it hard to be applied to the modeling of large-scale graphs. \u2022 Among all metric learning-based learners, attentive learner has the lowest parameter and time complexity w.r.t. dimension . It is suitable for the situation with high feature dimension and low correlation between features. \u2022 Compared to attentive learner, MLP and GNN learner require larger space and time complexity to consider the correlation between features and original topology. \u2022 With the effective kNN sparsification, the memory and time complexity are reduced from O ( 2 ) to O ( ), which improves the scalability of the metric learning-based learners. Considering these properties, we allocate the suitable learner for each dataset. Specifically, for small datasets whose node numbers are less than 3000 (e.g., Cora), we use FGP learners to model them due to the flexibility and acceptable complexity. For larger datasets with high-dimensional raw features (e.g., Citeseer), we  Obtain augmented graph views G , G by Eq. (9) -Eq. \n\n\nC COMPLEXITY ANALYSIS\n\nWe analyze the time complexity of each component of SUBLIME.\n\nFor graph learner, the complexity has been described in Table 6. The time complexity of post-processor is mainly contributed by sparsification, which is O ( 1 ) for effective kNN and O ( 2 ) for \n\n\nD ALGORITHM\n\nThe training algorithm of SUBLIME is summarized in Algorithm 1.\n\n\nE DATASETS\n\nIn Table 7, we summarize the statistics of benchmark datasets. The dataset splitting follows the previous works [7,12]. Details of these datasets are introduced as follows.\n\n\u2022 Cora [34] is a citation network where each node is a machine learning paper belonging to 7 research topics and each edge is a citation between papers. \u2022 Citeseer [34] is a citation network containing 6 types of machine learning papers: Agents, AI, DB, IR, ML, and HCI. Nodes denote papers and edges denote citation relationships. \u2022 Pubmed [27] is a citation network from the PubMed database, where nodes are papers about three diabete types about diabetes and edges are citations among them. \u2022 ogbn-arxiv [17] is a citation network with Computer Science arXiv papers. The features are the embeddings of words in its title and abstract. The labels are 40 subject areas. We implement SUBLIME using PyTorch 1.7.1 [31] and DGL 0.7.1 [44]. All experiments are conducted on a Linux server with an Intel Xeon 4214R CPU and four Quadro RTX 6000 GPUs.  \n\n\nF.2 Evaluation Details\n\nThrough node classification tasks, we evaluate the quality of the learned structures by re-training a classifier with the learned structure as its constant input. Specifically, we use the learned adjacency matrices to train GCN-based classification models, and record the testing result with the highest validation accuracy. The averaged accuracy over five rounds of running is used to assess the classification performance. For ogbn-arxiv dataset, we utilize a three-layer GCN with 256 hidden units as the evaluation model. For the rest datasets, a two-layer GCN with 32 hidden units is employed.\n\nFor node clustering tasks, we evaluate the performance of our method by measuring the quality of the learned representations. Concretely, following the baseline methods [42,55], we train our framework for a fixed number of epochs and apply K-means algorithm for 10 runs to group the learned representations. The representations are generated by the contrastive learning encoder taking learned graph G = (S, X) as its input without augmentation.\n\n\nF.3 Hyper-parameter Specifications\n\nWe perform grid search to select hyper-parameters on the following searching space: the dimension of representation and projection is searched in {16, 32, 64, 128, 256, 512}; on kNN is tuned amongst {5, 10, 15, 20, 25, 30, 35, 40}; feature masking probability ( ) is tuned from 0.1 to 0.9; edge dropping probability ( ) is searched in {0, 0.25, 0.5, 0.75}; the bootstrapping decay rate is chosen from {0.99, 0.999, 0.9999, 0.99999, 1}; and the learning rate of Adam optimizer is selected from {0.01, 0.001, 0.0001}. The temperature for contrastive loss is fixed to 0.2. The layer numbers of encoder ( 1 ), projector ( 2 ), and embedding network ( ) are set to 2.\n\nFor our baselines, we reproduce the experiments using their official open-source codes or borrow the reported results in their papers. We carefully tune their hyper-parameters to achieve optimal performance. To compare fairly, we use the random seeds {0, 1, 2, 3, 4} for all classification methods, and fix the seed to 0 for clustering methods.\n\n\nG PARAMETER SENSITIVITY OF ( )\n\nWe vary the dropping rate ( ) from 0 to 0.95 on Cora, Citeseer and Pubmed datasets, and the results are shown in Fig. 7. As we can see, when ( ) is between 0.2 and 0.65, SUBLIME achieves better performance. When the edge dropping rate is overlarge, the structures on both views will be deteriorated, causing a sharp drop of performance.\n\n\nOur proposed unsupervised GSL paradigm.\n\nFigure 1 :\n1Concept maps of (a) the existing supervised GSL paradigm and (b) our proposed unsupervised GSL paradigm.\n\n\nContrastive loss value w.r.t. epoch.\n\nFigure 3 :\n3Curves of training process on Cora dataset.\n\n\nw.r.t. number of neighbors.\n\nFigure 4 :\n4Sensitivity of hyper-parameters ( ) and .\n\n\nw.r.t. edge addition rate.\n\nFigure 5 :\n5Test accuracy in the scenarios where graph structures are perturbed by edge deletion or addition. Graph learned by SUBLIME.\n\nFigure 6 :\n6Heatmaps of the subgraph adjacency matrices of (a) the original graph with self-loop, the graph learned by (b) Pro-GNN and (c) SUBLIME on Cora dataset. A block in darker color indicates a larger edge weight between two nodes.\n\n\nCalculate S with post-processor (S) by Eq. (6) -Eq. (8); 10 Establish two graph views by G = (S, X), G = (A , X);11\n\nFigure 7 :\n7Sensitivity analysis for ( ) .\n\nTable 1 :\n1Node classification accuracy (percentage with standard deviation) in structure inference scenario. Available data for graph structure learning during the training phase is shown in the first column, where X, Y, A correspond to node features, labels and the adjacency matrix of kNN graph, respectively. The highest and second highest results are highlighted with boldface and underline, respectively. The symbol \"OOM\" means out of memory.Available \nData for GSL \nMethod \nDataset \nCora \nCiteseer \nPubmed \nogbn-arxiv \nWine \nCancer \nDigits \n20news \n-\nLR \n60.8\u00b10.0 \n62.2\u00b10.0 \n72.4\u00b10.0 \n52.5\u00b10.0 \n92.1\u00b11.3 \n93.3\u00b10.5 \n85.5\u00b11.5 \n42.7\u00b11.7 \n-\nLinear SVM \n58.9\u00b10.0 \n58.3\u00b10.0 \n72.7\u00b10.1 \n51.8\u00b10.0 \n93.9\u00b11.6 \n90.6\u00b14.5 \n87.1\u00b11.8 \n40.3\u00b11.4 \n-\nMLP \n56.1\u00b11.6 \n56.7\u00b11.7 \n71.4\u00b10.0 \n54.7\u00b10.1 \n89.7\u00b11.9 \n92.9\u00b11.2 \n36.3\u00b10.3 \n38.6\u00b11.4 \n-\nGCN \n[22] \n66.5\u00b10.4 \n68.3\u00b11.3 \n70.4\u00b10.4 \n54.1\u00b10.3 \n93.2\u00b13.1 \n83.8\u00b11.4 \n91.3\u00b10.5 \n41.3\u00b10.6 \n-\nGAT \n[40] \n66.2\u00b10.5 \n70.0\u00b10.6 \n69.6\u00b10.5 \nOOM \n91.5\u00b12.4 \n95.1\u00b10.8 \n91.4\u00b10.1 \n45.0\u00b11.2 \n-\nSAGE \n[15] \n66.1\u00b10.7 \n68.0\u00b11.6 \n68.7\u00b10.2 \n55.2\u00b10.4 \n87.4\u00b10.8 \n93.7\u00b10.3 \n91.6\u00b10.7 \n45.4\u00b10.4 \nX, Y \nLDS [12] \n71.5\u00b10.8 \n71.5\u00b11.1 \nOOM \nOOM \n97.3\u00b10.4 \n94.4\u00b11.9 \n92.5\u00b10.7 \n46.4\u00b11.6 \nX, Y, A \nGRCN [53] \n69.6\u00b10.2 \n70.4\u00b10.3 \n70.6\u00b10.1 \nOOM \n96.6\u00b10.4 \n95.4\u00b10.6 \n92.8\u00b10.2 \n41.8\u00b10.2 \nX, Y, A \nPro-GNN [20] \n69.2\u00b11.4 \n69.8\u00b11.7 \nOOM \nOOM \n95.1\u00b11.5 \n96.5\u00b10.1 \n93.9\u00b11.9 \n45.7\u00b11.4 \nX, Y, A \nGEN [45] \n69.1\u00b10.7 \n70.7\u00b11.1 \n70.7\u00b10.9 \nOOM \n96.9\u00b11.0 \n96.8\u00b10.4 \n94.1\u00b10.4 \n47.1\u00b10.3 \nX, Y \nIDGL [7] \n70.9\u00b10.6 \n68.2\u00b10.6 \n70.1\u00b11.3 \n55.0\u00b10.2 \n98.1\u00b11.1 \n95.1\u00b11.0 \n93.2\u00b10.9 \n48.5\u00b10.6 \nX, Y \nSLAPS [11] \n73.4\u00b10.3 \n72.6\u00b10.6 \n74.4\u00b10.6 \n56.6\u00b10.1 \n96.6\u00b10.4 \n96.6\u00b10.2 \n94.4\u00b10.7 \n50.4\u00b10.7 \nA \nGDC [23] \n68.1\u00b11.2 \n68.8\u00b10.8 \n68.4\u00b10.4 \nOOM \n96.1\u00b11.0 \n95.9\u00b10.4 \n92.6\u00b10.5 \n46.4\u00b10.9 \nX \nSLAPS-2s [11] \n72.1\u00b10.4 \n69.4\u00b11.4 \n71.1\u00b10.5 \n54.2\u00b10.2 \n96.2\u00b12.1 \n95.9\u00b11.2 \n93.6\u00b10.8 \n47.7\u00b10.7 \nX \nSUBLIME \n73.0\u00b10.6 \n73.1\u00b10.3 \n73.8\u00b10.6 \n55.5\u00b10.1 \n98.2\u00b11.6 \n97.2\u00b10.2 \n94.3\u00b10.4 \n49.2\u00b10.6 \n\n\n\nTable 2 :\n2Node classification accuracy (percentage with standard deviation) in structure refinement scenario.Available \nData for GSL \nMethod \nDataset \nCora \nCiteseer Pubmed ogbn-arxiv \n-\nGCN \n81.5 \n70.3 \n79.0 \n71.7\u00b10.3 \n-\nGAT \n83.0\u00b10.7 72.5\u00b10.7 79.0\u00b10.3 \nOOM \n-\nSAGE \n77.4\u00b11.0 67.0\u00b11.0 76.6\u00b10.8 \n71.5\u00b10.3 \nX, Y, A \nLDS \n83.9\u00b10.6 74.8\u00b10.3 \nOOM \nOOM \nX, Y, A \nGRCN \n84.0\u00b10.2 73.0\u00b10.3 78.9\u00b10.2 \nOOM \nX, Y, A \nPro-GNN 82.1\u00b10.4 71.3\u00b10.4 \nOOM \nOOM \nX, Y, A \nGEN \n82.3\u00b10.4 73.5\u00b11.5 80.9\u00b10.8 \nOOM \nX, Y, A \nIDGL \n84.0\u00b10.5 73.1\u00b10.7 83.0\u00b10.2 \n72.0\u00b10.3 \nA \nGDC \n83.6\u00b10.2 73.4\u00b10.3 78.7\u00b10.4 \nOOM \nX, A \nSUBLIME 84.2\u00b10.5 73.5\u00b10.6 81.0\u00b10.6 \n71.8\u00b10.3 \n\n\n\nTable 3 :\n3Node clustering performance (4 metrics in percent-\nage) in structure refinement scenario. \n\nMethod \nCora \nCiteseer \nC-ACC NMI \nF1 \nARI C-ACC NMI \nF1 \nARI \nK-means \n50.0 \n31.7 37.6 23.9 \n54.4 \n31.2 41.3 28.5 \nSC \n39.8 \n29.7 33.2 17.4 \n30.8 \n9.0 \n25.7 \n8.2 \nGE \n30.1 \n5.9 \n23.0 \n4.6 \n29.3 \n5.7 \n21.3 \n4.3 \nDW \n52.9 \n38.4 43.5 29.1 \n39.0 \n13.1 30.5 13.7 \nDNGR \n41.9 \n31.8 34.0 14.2 \n32.6 \n18.0 30.0 \n4.3 \nM-NMF \n42.3 \n25.6 32.0 16.1 \n33.6 \n9.9 \n25.5 \n7.0 \nRMSC \n46.6 \n32.0 34.7 20.3 \n51.6 \n30.8 40.4 26.6 \nTADW \n53.6 \n36.6 40.1 24.0 \n52.9 \n32.0 43.6 28.6 \nVGAE \n59.2 \n40.8 45.6 34.7 \n39.2 \n16.3 27.8 10.1 \nARGA \n64.0 \n44.9 61.9 35.2 \n57.3 \n35.0 54.6 34.1 \nMGAE \n68.1 \n48.9 53.1 56.5 \n66.9 \n41.6 52.6 42.5 \nAGC \n68.9 \n53.7 65.6 44.8 \n67.0 \n41.1 62.5 41.5 \nDAEGC \n70.4 \n52.8 68.2 49.6 \n67.2 \n39.7 63.6 41.0 \nSUBLIME \n71.3 \n54.2 63.5 50.3 \n68.5 \n44.1 63.2 43.9 \n\n\n\nTable 4 :\n4Test accuracy corresponding to different bootstrapping decay rate in structure refinement scenario.Dataset \nBootstrapping decay rate \n1 \n0.99999 0.9999 0.999 0.99 \nCora \n82.1 \n83.2 \n84.2 \n82.4 70.9 \nCiteseer 71.9 \n72.6 \n73.5 \n73.4 72.6 \nPubmed 80.1 \n80.3 \n81.0 \n80.8 80.5 \n\n0 \n2000 \n4000 \n\nEpoch \n\n70 \n\n75 \n\n80 \n\n85 \n\nAccuracy (%) \n\n\u03c4 = 1 \n\u03c4 = 0.99999 \n\u03c4 = 0.9999 \n\u03c4 = 0.999 \n\u03c4 = 0.99 \n\n(a) Test accuracy w.r.t. epoch. \n\n\n\nTable 5 :\n5Frequently used notations.\n\nTable 6 :\n6Properties of graph learners. Temperature ; Number of epochs . Output: Learned Adjacency Matrix S 1 Initialize parameters , , ; 2 if A is provided then Initialize the anchor adjacency matrix by: A a \u2190 A;Learner \nSketch \nMemory \nParams \nTime \n\nFGP \n\n\u03a9 \n\nNon-linear \" \n\n# \nS \n! \n\n\u03a9 \n\n\u03a9 \n\u00d7 \n\nX \nE \n\nSim. \n\n! \n\n# \nS \n\nO ( \n+ ) O ( 2 ) \nO ( \n+ \n2 + \n1 ) \n\nAlgorithm 1: The training algorithm of SUBLIME \nInput: Feature matrix X; Adjacency matrix A (optional); \nNumber of nearest neighbors ; Bootstrapping decay \nrate and interval , ; Feature masking probability \n( ) , \n( ) ; Edge dropping probability ( ) ; \n\n3 \n\n\n\n\nCalculate node representations H , H with encoder by Eq. (12); Calculate projections Z , Z with encoder by Eq. (13); Calculate the contrastive loss L by Eq. (14) ; Update parameters , , by applying gradient descent; Bootstrapping update A a with decay by Eq. (15) ; choose attentive learner considering its low parameter/time complexity w.r.t. dimension . For large-scale datasets with relevantly low feature dimensions (e.g., 20news), we adopt MLP learners to capture the correlation between features. In graph refinement scenarios where original graphs are available, GNN learners can be further considered to leverage the extra topology information.with probability \n( ) , \n( ) , ( ) ; \n\n12 \n\n13 \n\n14 \n\n15 \n\n16 \n\nif e mod c = 0 then \n\n17 \n\n18 \n\nend \n\n19 end \n\n\n\nTable 7 :\n7Statistics of datasets. ). For contrastive loss computation, the complexity is O ( 2 ) for its full-graph version, O ( 2 ) for the minibatch version, where 2 is the batch size of contrastive learning.Dataset \nNodes \nEdges \nClasses Features Label Rate \nCora \n2,708 \n5,429 \n7 \n1,433 \n0.052 \nCiteseer \n3,327 \n4,732 \n6 \n3,703 \n0.036 \nPubmed \n19,717 \n44,338 \n3 \n500 \n0.003 \nogbn-arxiv 169,343 1,166,243 \n40 \n128 \n0.537 \nWine \n178 \nN/A \n3 \n13 \n0.056 \nCancer \n569 \nN/A \n2 \n30 \n0.018 \nDigits \n1,797 \nN/A \n10 \n64 \n0.028 \n20news \n9,607 \nN/A \n10 \n236 \n0.010 \n\nconventional kNN. In the contrastive learning module, the com-\nplexities of feature masking and edge dropping are O ( ) and O ( ), \nrespectively. For the encoder and projector, the total complexity is \nO ( 1 1 + 2 \n1 1 + 2 \n2 2 \n\n\n\u2022 Wine [1] is a non-graph dataset containing the results of a chemical analysis of 178 wines derived from three different cultivars. Features are the quantities of 13 constituents found. \u2022 Cancer [1] is a binary classification dataset of diagnosis of breast tissues (malignant/benign). The features are computed from a digitized image of a breast mass. \u2022 Digits [1] is a non-graph dataset containing handwritten digits in 10 classes. Each sample is a 8 \u00d7 8 image of a digit. \u2022 20news [1] is a non-graph dataset comprising newsgroups posts on 20 topics. Following [12], we select 10 topics with 9, 607 samples in our experiments.F IMPLEMENTATION DETAILS \nF.1 Computing Infrastructures \n\n\nACKNOWLEDGMENTSThe corresponding author is Shirui Pan. This work was supported by an ARC Future Fellowship (No. FT210100097).\nUCI machine learning repository. Arthur Asuncion, David Newman, Arthur Asuncion and David Newman. 2007. UCI machine learning repository.\n\nRobust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings. Aleksandar Bojchevski, Yves Matkovic, Stephan G\u00fcnnemann, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningAleksandar Bojchevski, Yves Matkovic, and Stephan G\u00fcnnemann. 2017. Robust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 737-746.\n\nSpectral networks and locally connected networks on graphs. Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, International Conference on Learning Representations. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations.\n\nDeep neural networks for learning graph representations. Shaosheng Cao, Wei Lu, Qiongkai Xu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence30Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for learning graph representations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 30.\n\nDeep clustering for unsupervised learning of visual features. Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. 2018. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision. 132-149.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International Conference on Machine Learning. PMLR. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Interna- tional Conference on Machine Learning. PMLR, 1597-1607.\n\nIterative deep graph learning for graph neural networks: Better and robust node embeddings. Yu Chen, Lingfei Wu, Mohammed Zaki, Advances in Neural Information Processing Systems. 33Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in Neural Information Processing Systems 33 (2020).\n\nInfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training. Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, Ming Zhou, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre- Training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in Neural Information Processing Systems. 29Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu- tional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, Vol. 29. 3844-3852.\n\nGraph learning from data under Laplacian and structural constraints. Eduardo Hilmi E Egilmez, Antonio Pavez, Ortega, IEEE Journal of Selected Topics in Signal Processing. 11Hilmi E Egilmez, Eduardo Pavez, and Antonio Ortega. 2017. Graph learning from data under Laplacian and structural constraints. IEEE Journal of Selected Topics in Signal Processing 11, 6 (2017), 825-841.\n\nSLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks. Bahare Fatemi, Layla El Asri, Seyed Mehran Kazemi, Advances in Neural Information Processing Systems. Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. 2021. SLAPS: Self- Supervision Improves Structure Learning for Graph Neural Networks. In Ad- vances in Neural Information Processing Systems.\n\nLearning discrete structures for graph neural networks. Luca Franceschi, Mathias Niepert, Massimiliano Pontil, Xiao He, PMLRInternational Conference on Machine Learning. Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 2019. Learn- ing discrete structures for graph neural networks. In International Conference on Machine Learning. PMLR, 1972-1982.\n\nDeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. John Giorgi, Osvald Nitski, Bo Wang, Gary Bader, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 879-895.\n\nBootstrap Your Own Latent -A New Approach to Self-Supervised Learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Remi Koray Kavukcuoglu, Michal Munos, Valko, Advances in Neural Information Processing Systems. 33Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. 2020. Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning. In Advances in Neural Information Processing Systems, Vol. 33. 21271-21284.\n\nInductive representation learning on large graphs. Rex William L Hamilton, Jure Ying, Leskovec, Advances in Neural Information Processing Systems. William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems. 1025-1035.\n\nAlgorithm AS 136: A k-means clustering algorithm. A John, Hartigan, A Manchek, Wong, Journal of the royal statistical society. series c (applied statistics). 28John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics) 28, 1 (1979), 100-108.\n\nOpen Graph Benchmark: Datasets for Machine Learning on Graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in Neural Information Processing Systems. 33Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Advances in Neural Information Processing Systems, Vol. 33. 22118-22133.\n\nANEMONE: Graph Anomaly Detection with Multi-Scale Contrastive Learning. Ming Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, Shirui Pan, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementMing Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui Pan. 2021. ANEMONE: Graph Anomaly Detection with Multi-Scale Contrastive Learning. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 3122-3126.\n\nMulti-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, Shirui Pan, International Joint Conference on Artificial Intelligence. Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, and Shirui Pan. 2021. Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Rep- resentation Learning. In International Joint Conference on Artificial Intelligence.\n\nGraph structure learning for robust graph neural networks. Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, Jiliang Tang, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningWei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. 2020. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 66-74.\n\nVariational Graph Auto-Encoders. N Thomas, Max Kipf, Welling, Neural Information Processing Systems Workshop. Thomas N Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. In Neural Information Processing Systems Workshop. 1-3.\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, International Conference on Learning Representations. Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Repre- sentations.\n\nDiffusion improves graph learning. Johannes Klicpera, Stefan Wei\u00dfenberger, Stephan G\u00fcnnemann, Advances in Neural Information Processing Systems. 32Johannes Klicpera, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. 2019. Diffu- sion improves graph learning. Advances in Neural Information Processing Systems 32 (2019), 13354-13366.\n\nAnomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning. Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, George Karypis, IEEE Transactions on Neural Networks and Learning Systems. Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. 2021. Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning. IEEE Transactions on Neural Networks and Learning Systems (2021).\n\nGraph self-supervised learning: A survey. Yixin Liu, Shirui Pan, Ming Jin, Chuan Zhou, Feng Xia, Philip S Yu, arXiv:2103.00111arXiv preprintYixin Liu, Shirui Pan, Ming Jin, Chuan Zhou, Feng Xia, and Philip S Yu. 2021. Graph self-supervised learning: A survey. arXiv preprint arXiv:2103.00111 (2021).\n\nStructural inference for uncertain networks. Travis Martin, Brian Ball, Mark, Newman, Physical Review E. 9312306Travis Martin, Brian Ball, and Mark EJ Newman. 2016. Structural inference for uncertain networks. Physical Review E 93, 1 (2016), 012306.\n\nQuery-driven active surveying for collective classification. Galileo Namata, Ben London, Lise Getoor, Bert Huang, Umd Edu, 10th International Workshop on Mining and Learning with Graphs. 8Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. 2012. Query-driven active surveying for collective classification. In 10th International Workshop on Mining and Learning with Graphs, Vol. 8. 1.\n\nOn spectral clustering: Analysis and an algorithm. Y Andrew, Michael I Ng, Yair Jordan, Weiss, Advances in Neural Information Processing Systems. Andrew Y Ng, Michael I Jordan, and Yair Weiss. 2002. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems. 849-856.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n\nAdversarially regularized graph autoencoder for graph embedding. Ruiqi Shirui Pan, Guodong Hu, Jing Long, Lina Jiang, Chengqi Yao, Zhang, International Joint Conference on Artificial Intelligence. Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. 2018. Adversarially regularized graph autoencoder for graph embedding. In International Joint Conference on Artificial Intelligence. 2609-2615.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in Neural Information Processing Systems. 32Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems 32 (2019), 8026-8037.\n\nGraph representation learning via graphical mutual information maximization. Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, Junzhou Huang, Proceedings of The Web Conference 2020. The Web Conference 2020Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. 2020. Graph representation learning via graphical mutual information maximization. In Proceedings of The Web Conference 2020. 259-270.\n\nDeepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningBryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 701-710.\n\nCollective classification in network data. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, Tina Eliassi-Rad, AI magazine. 29Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29, 3 (2008), 93-93.\n\nImproved deep metric learning with multi-class n-pair loss objective. Kihyuk Sohn, Advances in Neural Information Processing Systems. Kihyuk Sohn. 2016. Improved deep metric learning with multi-class n-pair loss objective. In Advances in Neural Information Processing Systems. 1857-1865.\n\nFedproto: Federated prototype learning over heterogeneous devices. Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, Chengqi Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. 2022. Fedproto: Federated prototype learning over heterogeneous devices. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, Advances in Neural Information Processing Systems. Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems. 1195-1204.\n\nLearning deep representations for graph clustering. Fei Tian, Bin Gao, Qing Cui, Enhong Chen, Tie-Yan Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence28Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. 2014. Learning deep representations for graph clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 28.\n\nDeep Graph Infomax. Petar Veli\u010dkovi\u0107, William Fedus, L William, Pietro Hamilton, Yoshua Li\u00f2, R Devon Bengio, Hjelm, International Conference on Learning Representations. Petar Veli\u010dkovi\u0107, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon Hjelm. 2019. Deep Graph Infomax. In International Conference on Learning Representations.\n\nGraph Attention Networks. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, International Conference on Learning Representations. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con- ference on Learning Representations.\n\nContrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels. Yibing Sheng Wan, Liu Zhan, Baosheng Liu, Shirui Yu, Chen Pan, Gong, Advances in Neural Information Processing Systems. 34Sheng Wan, Yibing Zhan, Liu Liu, Baosheng Yu, Shirui Pan, and Chen Gong. 2021. Contrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels. Advances in Neural Information Processing Systems 34 (2021).\n\nAttributed Graph Clustering: A Deep Attentional Embedding Approach. Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Chengqi Zhang, International Joint Conference on Artificial Intelligence. Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019. Attributed Graph Clustering: A Deep Attentional Embedding Approach. In International Joint Conference on Artificial Intelligence. 3670-3676.\n\nMgae: Marginalized graph autoencoder for graph clustering. Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, Jing Jiang, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge ManagementChun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. 2017. Mgae: Marginalized graph autoencoder for graph clustering. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. 889-898.\n\nMinjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, Zheng Zhang, arXiv:1909.01315Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks. arXiv preprintMinjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. 2019. Deep Graph Library: A Graph-Centric, Highly- Performant Package for Graph Neural Networks. arXiv preprint arXiv:1909.01315 (2019).\n\nGraph Structure Estimation Neural Networks. Ruijia Wang, Shuai Mou, Xiao Wang, Wanpeng Xiao, Qi Ju, Chuan Shi, Xing Xie, Proceedings of the Web Conference 2021. the Web Conference 2021Ruijia Wang, Shuai Mou, Xiao Wang, Wanpeng Xiao, Qi Ju, Chuan Shi, and Xing Xie. 2021. Graph Structure Estimation Neural Networks. In Proceedings of the Web Conference 2021. 342-353.\n\nCommunity preserving network embedding. Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceXiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. 2017. Community preserving network embedding. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\nMulti-view Graph Contrastive Representation Learning for Drug-Drug Interaction Prediction. Yingheng Wang, Yaosen Min, Xin Chen, Ji Wu, Proceedings of the Web Conference 2021. the Web Conference 2021Yingheng Wang, Yaosen Min, Xin Chen, and Ji Wu. 2021. Multi-view Graph Contrastive Representation Learning for Drug-Drug Interaction Prediction. In Proceedings of the Web Conference 2021. 2921-2933.\n\nA Comprehensive Survey on Graph Neural Networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S Yu, IEEE Transactions on Neural Networks and Learning Systems. 32Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE Transactions on Neural Networks and Learning Systems 32, 1 (2021), 4-24.\n\nRobust multi-view spectral clustering via low-rank and sparse decomposition. Rongkai Xia, Yan Pan, Lei Du, Jian Yin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence28Rongkai Xia, Yan Pan, Lei Du, and Jian Yin. 2014. Robust multi-view spectral clustering via low-rank and sparse decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 28.\n\nHow Powerful are Graph Neural Networks. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, International Conference on Learning Representations. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful are Graph Neural Networks?. In International Conference on Learning Representa- tions.\n\nNetwork representation learning with rich text information. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Chang, International Joint Conference on Artificial Intelligence. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Chang. 2015. Network representation learning with rich text information. In International Joint Conference on Artificial Intelligence.\n\nGraph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in Neural Information Processing Systems. 33Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems 33 (2020), 5812-5823.\n\nGraph-revised convolutional network. Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, Yiming Yang, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerDonghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. 2020. Graph-revised convolutional network. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 378-393.\n\nSelf-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Proceedings of the Web Conference 2021Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convo- lutional Network for Social Recommendation. In Proceedings of the Web Confer- ence 2021. 413-424.\n\nAttributed Graph Clustering via Adaptive Graph Convolution. Xiaotong Zhang, Han Liu, Qimai Li, Xiao-Ming Wu, International Joint Conference on Artificial Intelligence. Xiaotong Zhang, Han Liu, Qimai Li, and Xiao-Ming Wu. 2019. Attributed Graph Clustering via Adaptive Graph Convolution. In International Joint Conference on Artificial Intelligence. 4327-4333.\n\nA Vertical Federation Framework Based on Representation Learning. Yin Zhang, Fanglin An, 2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City. SpringerYin Zhang, Fanglin An, and Jun Ye. 2022. A Vertical Federation Framework Based on Representation Learning. In 2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City. Springer, 627-633.\n\nYizhen Zheng, Ming Jin, Shirui Pan, Yuan-Fang Li, Hao Peng, Ming Li, Zhao Li, arXiv:2111.10698Towards Graph Self-Supervised Learning with Contrastive Adjusted Zooming. arXiv preprintYizhen Zheng, Ming Jin, Shirui Pan, Yuan-Fang Li, Hao Peng, Ming Li, and Zhao Li. 2021. Towards Graph Self-Supervised Learning with Contrastive Adjusted Zooming. arXiv preprint arXiv:2111.10698 (2021).\n\nYanqiao Zhu, Weizhi Xu, Jinghao Zhang, Qiang Liu, Shu Wu, Liang Wang, arXiv:2103.03036Deep Graph Structure Learning for Robust Representations: A Survey. arXiv preprintYanqiao Zhu, Weizhi Xu, Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang. 2021. Deep Graph Structure Learning for Robust Representations: A Survey. arXiv preprint arXiv:2103.03036 (2021).\n\nGraph contrastive learning with adaptive augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Proceedings of the Web Conference 2021. the Web Conference 2021Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021. 2069-2080.\n", "annotations": {"author": "[{\"end\":138,\"start\":87},{\"end\":194,\"start\":139},{\"end\":287,\"start\":195},{\"end\":357,\"start\":288},{\"end\":442,\"start\":358},{\"end\":496,\"start\":443},{\"end\":527,\"start\":497},{\"end\":559,\"start\":528},{\"end\":628,\"start\":560},{\"end\":675,\"start\":629},{\"end\":740,\"start\":676}]", "publisher": "[{\"end\":55,\"start\":52},{\"end\":917,\"start\":914}]", "author_last_name": "[{\"end\":96,\"start\":93},{\"end\":147,\"start\":142},{\"end\":207,\"start\":202},{\"end\":299,\"start\":295},{\"end\":366,\"start\":362},{\"end\":453,\"start\":450},{\"end\":506,\"start\":503},{\"end\":536,\"start\":531},{\"end\":572,\"start\":567},{\"end\":640,\"start\":636},{\"end\":684,\"start\":680}]", "author_first_name": "[{\"end\":92,\"start\":87},{\"end\":141,\"start\":139},{\"end\":201,\"start\":195},{\"end\":294,\"start\":288},{\"end\":361,\"start\":358},{\"end\":449,\"start\":443},{\"end\":502,\"start\":497},{\"end\":530,\"start\":528},{\"end\":566,\"start\":560},{\"end\":635,\"start\":629},{\"end\":679,\"start\":676}]", "author_affiliation": "[{\"end\":137,\"start\":119},{\"end\":193,\"start\":173},{\"end\":251,\"start\":233},{\"end\":286,\"start\":253},{\"end\":356,\"start\":324},{\"end\":441,\"start\":388},{\"end\":495,\"start\":477},{\"end\":526,\"start\":508},{\"end\":558,\"start\":538},{\"end\":592,\"start\":574},{\"end\":627,\"start\":594},{\"end\":674,\"start\":642},{\"end\":739,\"start\":686}]", "title": "[{\"end\":51,\"start\":1},{\"end\":791,\"start\":741}]", "venue": "[{\"end\":845,\"start\":793}]", "abstract": "[{\"end\":2974,\"start\":1111}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3383,\"start\":3379},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3386,\"start\":3383},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3408,\"start\":3404},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3411,\"start\":3408},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3437,\"start\":3433},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3440,\"start\":3437},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4023,\"start\":4019},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4545,\"start\":4542},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4548,\"start\":4545},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4551,\"start\":4548},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4554,\"start\":4551},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4647,\"start\":4643},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4650,\"start\":4647},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4678,\"start\":4674},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4707,\"start\":4704},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4710,\"start\":4707},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4713,\"start\":4710},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4845,\"start\":4841},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5250,\"start\":5247},{\"end\":5581,\"start\":5577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5704,\"start\":5700},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6902,\"start\":6898},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8765,\"start\":8761},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8768,\"start\":8765},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8945,\"start\":8942},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9013,\"start\":9010},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9072,\"start\":9068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9200,\"start\":9196},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9203,\"start\":9200},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9206,\"start\":9203},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9332,\"start\":9328},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9343,\"start\":9339},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9364,\"start\":9360},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9384,\"start\":9380},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9432,\"start\":9428},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9625,\"start\":9621},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9650,\"start\":9647},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9676,\"start\":9672},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10010,\"start\":10006},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10455,\"start\":10451},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10487,\"start\":10483},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10621,\"start\":10618},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10645,\"start\":10641},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10648,\"start\":10645},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10769,\"start\":10765},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10772,\"start\":10769},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11141,\"start\":11138},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11144,\"start\":11141},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11163,\"start\":11160},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11166,\"start\":11163},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11300,\"start\":11296},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11303,\"start\":11300},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11306,\"start\":11303},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11544,\"start\":11540},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11651,\"start\":11647},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11654,\"start\":11651},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11657,\"start\":11654},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11678,\"start\":11674},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11727,\"start\":11723},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11730,\"start\":11727},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11758,\"start\":11754},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11856,\"start\":11852},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11880,\"start\":11876},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11883,\"start\":11880},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11908,\"start\":11904},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11911,\"start\":11908},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11936,\"start\":11932},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14999,\"start\":14996},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15002,\"start\":14999},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15005,\"start\":15002},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15522,\"start\":15518},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15525,\"start\":15522},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15528,\"start\":15525},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15862,\"start\":15859},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":15865,\"start\":15862},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16424,\"start\":16420},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18521,\"start\":18517},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19660,\"start\":19656},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20164,\"start\":20160},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21807,\"start\":21803},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":21810,\"start\":21807},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22554,\"start\":22550},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22557,\"start\":22554},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23725,\"start\":23722},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23728,\"start\":23725},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23731,\"start\":23728},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25848,\"start\":25845},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26228,\"start\":26224},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26913,\"start\":26909},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26916,\"start\":26913},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28232,\"start\":28229},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28235,\"start\":28232},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28238,\"start\":28235},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30534,\"start\":30530},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32024,\"start\":32020},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32037,\"start\":32033},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32057,\"start\":32053},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32129,\"start\":32126},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32335,\"start\":32331},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32345,\"start\":32341},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32381,\"start\":32377},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32430,\"start\":32426},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":32441,\"start\":32437},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32455,\"start\":32451},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32465,\"start\":32461},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32475,\"start\":32472},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32490,\"start\":32486},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32518,\"start\":32514},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32611,\"start\":32607},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33014,\"start\":33010},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33058,\"start\":33054},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33138,\"start\":33134},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33168,\"start\":33164},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33178,\"start\":33175},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33193,\"start\":33189},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33255,\"start\":33251},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":33266,\"start\":33262},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33277,\"start\":33273},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33288,\"start\":33284},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33299,\"start\":33295},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":33309,\"start\":33305},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33324,\"start\":33320},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38883,\"start\":38879},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38900,\"start\":38896},{\"end\":42265,\"start\":42262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42724,\"start\":42720},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":44542,\"start\":44539},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44545,\"start\":44542},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":44612,\"start\":44608},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":44769,\"start\":44765},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":44946,\"start\":44942},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":45112,\"start\":45108},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":45317,\"start\":45313},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":45336,\"start\":45332},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":46246,\"start\":46242},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":46249,\"start\":46246},{\"end\":46786,\"start\":46759}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47977,\"start\":47936},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48095,\"start\":47978},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48134,\"start\":48096},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48191,\"start\":48135},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48221,\"start\":48192},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48276,\"start\":48222},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48305,\"start\":48277},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48442,\"start\":48306},{\"attributes\":{\"id\":\"fig_8\"},\"end\":48681,\"start\":48443},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48799,\"start\":48682},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48843,\"start\":48800},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":50784,\"start\":48844},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51424,\"start\":50785},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52294,\"start\":51425},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":52728,\"start\":52295},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":52767,\"start\":52729},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":53389,\"start\":52768},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":54155,\"start\":53390},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":54945,\"start\":54156},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":55634,\"start\":54946}]", "paragraph": "[{\"end\":3634,\"start\":2990},{\"end\":4383,\"start\":3636},{\"end\":5809,\"start\":4385},{\"end\":6199,\"start\":5811},{\"end\":7854,\"start\":6201},{\"end\":8378,\"start\":7856},{\"end\":8582,\"start\":8380},{\"end\":9455,\"start\":8625},{\"end\":11053,\"start\":9489},{\"end\":12030,\"start\":11088},{\"end\":12533,\"start\":12053},{\"end\":12921,\"start\":12535},{\"end\":13502,\"start\":12923},{\"end\":13745,\"start\":13504},{\"end\":14838,\"start\":13761},{\"end\":15423,\"start\":14856},{\"end\":15590,\"start\":15425},{\"end\":15990,\"start\":15605},{\"end\":16381,\"start\":16025},{\"end\":16567,\"start\":16383},{\"end\":17377,\"start\":16647},{\"end\":17721,\"start\":17426},{\"end\":18270,\"start\":17723},{\"end\":18547,\"start\":18451},{\"end\":18870,\"start\":18615},{\"end\":19109,\"start\":18872},{\"end\":19870,\"start\":19128},{\"end\":20610,\"start\":19921},{\"end\":21004,\"start\":20650},{\"end\":21092,\"start\":21051},{\"end\":23000,\"start\":21134},{\"end\":24277,\"start\":23002},{\"end\":24839,\"start\":24332},{\"end\":24957,\"start\":24868},{\"end\":25084,\"start\":24959},{\"end\":26001,\"start\":25130},{\"end\":26445,\"start\":26026},{\"end\":26928,\"start\":26470},{\"end\":27161,\"start\":27030},{\"end\":27683,\"start\":27199},{\"end\":29406,\"start\":27685},{\"end\":30797,\"start\":29448},{\"end\":31350,\"start\":30813},{\"end\":33512,\"start\":31374},{\"end\":33956,\"start\":33545},{\"end\":34349,\"start\":33958},{\"end\":35746,\"start\":34351},{\"end\":36029,\"start\":35771},{\"end\":37190,\"start\":36031},{\"end\":38513,\"start\":37221},{\"end\":39254,\"start\":38543},{\"end\":40521,\"start\":39278},{\"end\":41039,\"start\":40536},{\"end\":41264,\"start\":41055},{\"end\":41319,\"start\":41266},{\"end\":41373,\"start\":41352},{\"end\":41416,\"start\":41381},{\"end\":41501,\"start\":41471},{\"end\":41542,\"start\":41503},{\"end\":41585,\"start\":41556},{\"end\":41624,\"start\":41594},{\"end\":41654,\"start\":41633},{\"end\":41733,\"start\":41699},{\"end\":41769,\"start\":41735},{\"end\":41964,\"start\":41776},{\"end\":41996,\"start\":41966},{\"end\":42038,\"start\":42004},{\"end\":42065,\"start\":42046},{\"end\":42129,\"start\":42082},{\"end\":42175,\"start\":42137},{\"end\":42315,\"start\":42183},{\"end\":42372,\"start\":42317},{\"end\":42430,\"start\":42374},{\"end\":42834,\"start\":42463},{\"end\":44050,\"start\":42836},{\"end\":44136,\"start\":44076},{\"end\":44333,\"start\":44138},{\"end\":44412,\"start\":44349},{\"end\":44599,\"start\":44427},{\"end\":45447,\"start\":44601},{\"end\":46071,\"start\":45474},{\"end\":46517,\"start\":46073},{\"end\":47218,\"start\":46556},{\"end\":47564,\"start\":47220},{\"end\":47935,\"start\":47599}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15604,\"start\":15591},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16024,\"start\":15991},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16646,\"start\":16568},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17425,\"start\":17378},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18450,\"start\":18271},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18614,\"start\":18548},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19920,\"start\":19871},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20649,\"start\":20611},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21050,\"start\":21005},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24331,\"start\":24278},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24867,\"start\":24840},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25129,\"start\":25085},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26025,\"start\":26002},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26469,\"start\":26446},{\"attributes\":{\"id\":\"formula_14\"},\"end\":27029,\"start\":26929},{\"attributes\":{\"id\":\"formula_15\"},\"end\":29427,\"start\":29407},{\"attributes\":{\"id\":\"formula_16\"},\"end\":41351,\"start\":41320},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41470,\"start\":41417},{\"attributes\":{\"id\":\"formula_18\"},\"end\":41555,\"start\":41543},{\"attributes\":{\"id\":\"formula_19\"},\"end\":41593,\"start\":41586},{\"attributes\":{\"id\":\"formula_20\"},\"end\":41632,\"start\":41625},{\"attributes\":{\"id\":\"formula_21\"},\"end\":41698,\"start\":41655},{\"attributes\":{\"id\":\"formula_22\"},\"end\":41775,\"start\":41770}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33605,\"start\":33598},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":34991,\"start\":34983},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35369,\"start\":35362},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36145,\"start\":36138},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36283,\"start\":36276},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":41317,\"start\":41310},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":42473,\"start\":42466},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44201,\"start\":44194},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":44437,\"start\":44430}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2988,\"start\":2976},{\"attributes\":{\"n\":\"2\"},\"end\":8623,\"start\":8585},{\"attributes\":{\"n\":\"2.2\"},\"end\":9487,\"start\":9458},{\"attributes\":{\"n\":\"2.3\"},\"end\":11086,\"start\":11056},{\"attributes\":{\"n\":\"3\"},\"end\":12051,\"start\":12033},{\"attributes\":{\"n\":\"4\"},\"end\":13759,\"start\":13748},{\"attributes\":{\"n\":\"4.1\"},\"end\":14854,\"start\":14841},{\"attributes\":{\"n\":\"4.2\"},\"end\":19126,\"start\":19112},{\"attributes\":{\"n\":\"4.3\"},\"end\":21132,\"start\":21095},{\"attributes\":{\"n\":\"4.4\"},\"end\":27197,\"start\":27164},{\"attributes\":{\"n\":\"4.5\"},\"end\":29446,\"start\":29429},{\"attributes\":{\"n\":\"5\"},\"end\":30811,\"start\":30800},{\"attributes\":{\"n\":\"5.1\"},\"end\":31372,\"start\":31353},{\"attributes\":{\"n\":\"5.2\"},\"end\":33543,\"start\":33515},{\"attributes\":{\"n\":\"5.3\"},\"end\":35769,\"start\":35749},{\"attributes\":{\"n\":\"5.4\"},\"end\":37219,\"start\":37193},{\"attributes\":{\"n\":\"5.5\"},\"end\":38541,\"start\":38516},{\"attributes\":{\"n\":\"5.6\"},\"end\":39276,\"start\":39257},{\"attributes\":{\"n\":\"6\"},\"end\":40534,\"start\":40524},{\"end\":41053,\"start\":41042},{\"end\":41379,\"start\":41376},{\"end\":42002,\"start\":41999},{\"end\":42044,\"start\":42041},{\"end\":42080,\"start\":42068},{\"end\":42135,\"start\":42132},{\"end\":42181,\"start\":42178},{\"end\":42461,\"start\":42433},{\"end\":44074,\"start\":44053},{\"end\":44347,\"start\":44336},{\"end\":44425,\"start\":44415},{\"end\":45472,\"start\":45450},{\"end\":46554,\"start\":46520},{\"end\":47597,\"start\":47567},{\"end\":47989,\"start\":47979},{\"end\":48146,\"start\":48136},{\"end\":48233,\"start\":48223},{\"end\":48317,\"start\":48307},{\"end\":48454,\"start\":48444},{\"end\":48811,\"start\":48801},{\"end\":48854,\"start\":48845},{\"end\":50795,\"start\":50786},{\"end\":51435,\"start\":51426},{\"end\":52305,\"start\":52296},{\"end\":52739,\"start\":52730},{\"end\":52778,\"start\":52769},{\"end\":54166,\"start\":54157}]", "table": "[{\"end\":50784,\"start\":49293},{\"end\":51424,\"start\":50896},{\"end\":52294,\"start\":51437},{\"end\":52728,\"start\":52406},{\"end\":53389,\"start\":52983},{\"end\":54155,\"start\":54044},{\"end\":54945,\"start\":54368},{\"end\":55634,\"start\":55576}]", "figure_caption": "[{\"end\":47977,\"start\":47938},{\"end\":48095,\"start\":47991},{\"end\":48134,\"start\":48098},{\"end\":48191,\"start\":48148},{\"end\":48221,\"start\":48194},{\"end\":48276,\"start\":48235},{\"end\":48305,\"start\":48279},{\"end\":48442,\"start\":48319},{\"end\":48681,\"start\":48456},{\"end\":48799,\"start\":48684},{\"end\":48843,\"start\":48813},{\"end\":49293,\"start\":48856},{\"end\":50896,\"start\":50797},{\"end\":52406,\"start\":52307},{\"end\":52767,\"start\":52741},{\"end\":52983,\"start\":52780},{\"end\":54044,\"start\":53392},{\"end\":54368,\"start\":54168},{\"end\":55576,\"start\":54948}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6371,\"start\":6365},{\"end\":13861,\"start\":13855},{\"end\":17918,\"start\":17910},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36262,\"start\":36256},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36477,\"start\":36468},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36715,\"start\":36706},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37177,\"start\":37171},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37611,\"start\":37602},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38185,\"start\":38176},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38992,\"start\":38986},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":39816,\"start\":39809},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":47718,\"start\":47712}]", "bib_author_first_name": "[{\"end\":55800,\"start\":55794},{\"end\":55816,\"start\":55811},{\"end\":56008,\"start\":55998},{\"end\":56025,\"start\":56021},{\"end\":56043,\"start\":56036},{\"end\":56581,\"start\":56577},{\"end\":56597,\"start\":56589},{\"end\":56613,\"start\":56607},{\"end\":56625,\"start\":56621},{\"end\":56937,\"start\":56928},{\"end\":56946,\"start\":56943},{\"end\":56959,\"start\":56951},{\"end\":57324,\"start\":57316},{\"end\":57337,\"start\":57332},{\"end\":57356,\"start\":57350},{\"end\":57373,\"start\":57365},{\"end\":57766,\"start\":57762},{\"end\":57778,\"start\":57773},{\"end\":57798,\"start\":57790},{\"end\":57816,\"start\":57808},{\"end\":58184,\"start\":58182},{\"end\":58198,\"start\":58191},{\"end\":58211,\"start\":58203},{\"end\":58567,\"start\":58562},{\"end\":58575,\"start\":58573},{\"end\":58586,\"start\":58582},{\"end\":58595,\"start\":58592},{\"end\":58609,\"start\":58602},{\"end\":58625,\"start\":58619},{\"end\":58635,\"start\":58632},{\"end\":58651,\"start\":58642},{\"end\":58662,\"start\":58657},{\"end\":58674,\"start\":58670},{\"end\":59408,\"start\":59401},{\"end\":59427,\"start\":59421},{\"end\":59443,\"start\":59437},{\"end\":59813,\"start\":59806},{\"end\":59838,\"start\":59831},{\"end\":60199,\"start\":60193},{\"end\":60213,\"start\":60208},{\"end\":60216,\"start\":60214},{\"end\":60235,\"start\":60223},{\"end\":60553,\"start\":60549},{\"end\":60573,\"start\":60566},{\"end\":60595,\"start\":60583},{\"end\":60608,\"start\":60604},{\"end\":60943,\"start\":60939},{\"end\":60958,\"start\":60952},{\"end\":60969,\"start\":60967},{\"end\":60980,\"start\":60976},{\"end\":61731,\"start\":61719},{\"end\":61746,\"start\":61739},{\"end\":61761,\"start\":61754},{\"end\":61778,\"start\":61770},{\"end\":61793,\"start\":61787},{\"end\":61810,\"start\":61805},{\"end\":61828,\"start\":61824},{\"end\":61846,\"start\":61838},{\"end\":61867,\"start\":61860},{\"end\":61881,\"start\":61873},{\"end\":61892,\"start\":61882},{\"end\":61904,\"start\":61899},{\"end\":61915,\"start\":61911},{\"end\":61941,\"start\":61935},{\"end\":62458,\"start\":62455},{\"end\":62483,\"start\":62479},{\"end\":62774,\"start\":62773},{\"end\":62792,\"start\":62791},{\"end\":63140,\"start\":63134},{\"end\":63153,\"start\":63145},{\"end\":63166,\"start\":63159},{\"end\":63181,\"start\":63175},{\"end\":63194,\"start\":63188},{\"end\":63205,\"start\":63200},{\"end\":63218,\"start\":63211},{\"end\":63232,\"start\":63228},{\"end\":63631,\"start\":63627},{\"end\":63642,\"start\":63637},{\"end\":63650,\"start\":63648},{\"end\":63665,\"start\":63658},{\"end\":63680,\"start\":63671},{\"end\":63691,\"start\":63685},{\"end\":64219,\"start\":64215},{\"end\":64231,\"start\":64225},{\"end\":64248,\"start\":64239},{\"end\":64257,\"start\":64253},{\"end\":64269,\"start\":64264},{\"end\":64282,\"start\":64276},{\"end\":64649,\"start\":64646},{\"end\":64658,\"start\":64655},{\"end\":64670,\"start\":64663},{\"end\":64684,\"start\":64676},{\"end\":64697,\"start\":64691},{\"end\":64711,\"start\":64704},{\"end\":65187,\"start\":65186},{\"end\":65199,\"start\":65196},{\"end\":65457,\"start\":65456},{\"end\":65469,\"start\":65466},{\"end\":65746,\"start\":65738},{\"end\":65763,\"start\":65757},{\"end\":65785,\"start\":65778},{\"end\":66119,\"start\":66114},{\"end\":66129,\"start\":66125},{\"end\":66140,\"start\":66134},{\"end\":66150,\"start\":66146},{\"end\":66162,\"start\":66157},{\"end\":66175,\"start\":66169},{\"end\":66522,\"start\":66517},{\"end\":66534,\"start\":66528},{\"end\":66544,\"start\":66540},{\"end\":66555,\"start\":66550},{\"end\":66566,\"start\":66562},{\"end\":66580,\"start\":66572},{\"end\":66827,\"start\":66821},{\"end\":66841,\"start\":66836},{\"end\":67095,\"start\":67088},{\"end\":67107,\"start\":67104},{\"end\":67120,\"start\":67116},{\"end\":67133,\"start\":67129},{\"end\":67144,\"start\":67141},{\"end\":67479,\"start\":67478},{\"end\":67495,\"start\":67488},{\"end\":67497,\"start\":67496},{\"end\":67506,\"start\":67502},{\"end\":67746,\"start\":67741},{\"end\":67766,\"start\":67761},{\"end\":67776,\"start\":67771},{\"end\":68102,\"start\":68097},{\"end\":68122,\"start\":68115},{\"end\":68131,\"start\":68127},{\"end\":68142,\"start\":68138},{\"end\":68157,\"start\":68150},{\"end\":68525,\"start\":68521},{\"end\":68537,\"start\":68534},{\"end\":68554,\"start\":68545},{\"end\":68566,\"start\":68562},{\"end\":68579,\"start\":68574},{\"end\":68597,\"start\":68590},{\"end\":68612,\"start\":68606},{\"end\":68628,\"start\":68622},{\"end\":68641,\"start\":68634},{\"end\":68658,\"start\":68654},{\"end\":69103,\"start\":69099},{\"end\":69117,\"start\":69110},{\"end\":69131,\"start\":69125},{\"end\":69144,\"start\":69137},{\"end\":69154,\"start\":69152},{\"end\":69169,\"start\":69161},{\"end\":69181,\"start\":69174},{\"end\":69540,\"start\":69535},{\"end\":69554,\"start\":69550},{\"end\":69570,\"start\":69564},{\"end\":70035,\"start\":70025},{\"end\":70048,\"start\":70041},{\"end\":70064,\"start\":70057},{\"end\":70077,\"start\":70073},{\"end\":70091,\"start\":70086},{\"end\":70107,\"start\":70103},{\"end\":70395,\"start\":70389},{\"end\":70678,\"start\":70675},{\"end\":70691,\"start\":70684},{\"end\":70700,\"start\":70698},{\"end\":70712,\"start\":70706},{\"end\":70726,\"start\":70719},{\"end\":70735,\"start\":70731},{\"end\":70750,\"start\":70743},{\"end\":71220,\"start\":71215},{\"end\":71237,\"start\":71232},{\"end\":71581,\"start\":71578},{\"end\":71591,\"start\":71588},{\"end\":71601,\"start\":71597},{\"end\":71613,\"start\":71607},{\"end\":71627,\"start\":71620},{\"end\":71962,\"start\":71957},{\"end\":71982,\"start\":71975},{\"end\":71991,\"start\":71990},{\"end\":72007,\"start\":72001},{\"end\":72024,\"start\":72018},{\"end\":72037,\"start\":72030},{\"end\":72321,\"start\":72316},{\"end\":72341,\"start\":72334},{\"end\":72359,\"start\":72352},{\"end\":72377,\"start\":72370},{\"end\":72392,\"start\":72386},{\"end\":72404,\"start\":72398},{\"end\":72758,\"start\":72752},{\"end\":72773,\"start\":72770},{\"end\":72788,\"start\":72780},{\"end\":72800,\"start\":72794},{\"end\":72809,\"start\":72805},{\"end\":73179,\"start\":73175},{\"end\":73192,\"start\":73186},{\"end\":73203,\"start\":73198},{\"end\":73215,\"start\":73208},{\"end\":73226,\"start\":73222},{\"end\":73241,\"start\":73234},{\"end\":73597,\"start\":73593},{\"end\":73610,\"start\":73604},{\"end\":73623,\"start\":73616},{\"end\":73638,\"start\":73630},{\"end\":73648,\"start\":73644},{\"end\":74039,\"start\":74033},{\"end\":74048,\"start\":74046},{\"end\":74061,\"start\":74056},{\"end\":74070,\"start\":74066},{\"end\":74081,\"start\":74076},{\"end\":74091,\"start\":74086},{\"end\":74105,\"start\":74098},{\"end\":74116,\"start\":74112},{\"end\":74128,\"start\":74121},{\"end\":74135,\"start\":74133},{\"end\":74148,\"start\":74141},{\"end\":74159,\"start\":74155},{\"end\":74170,\"start\":74164},{\"end\":74187,\"start\":74180},{\"end\":74197,\"start\":74192},{\"end\":74689,\"start\":74683},{\"end\":74701,\"start\":74696},{\"end\":74711,\"start\":74707},{\"end\":74725,\"start\":74718},{\"end\":74734,\"start\":74732},{\"end\":74744,\"start\":74739},{\"end\":74754,\"start\":74750},{\"end\":75051,\"start\":75047},{\"end\":75062,\"start\":75058},{\"end\":75072,\"start\":75068},{\"end\":75083,\"start\":75079},{\"end\":75094,\"start\":75089},{\"end\":75108,\"start\":75100},{\"end\":75508,\"start\":75500},{\"end\":75521,\"start\":75515},{\"end\":75530,\"start\":75527},{\"end\":75539,\"start\":75537},{\"end\":75863,\"start\":75856},{\"end\":75874,\"start\":75868},{\"end\":75887,\"start\":75880},{\"end\":75901,\"start\":75894},{\"end\":75915,\"start\":75908},{\"end\":75929,\"start\":75923},{\"end\":75931,\"start\":75930},{\"end\":76300,\"start\":76293},{\"end\":76309,\"start\":76306},{\"end\":76318,\"start\":76315},{\"end\":76327,\"start\":76323},{\"end\":76693,\"start\":76687},{\"end\":76704,\"start\":76698},{\"end\":76713,\"start\":76709},{\"end\":76732,\"start\":76724},{\"end\":77027,\"start\":77022},{\"end\":77041,\"start\":77034},{\"end\":77051,\"start\":77047},{\"end\":77065,\"start\":77058},{\"end\":77077,\"start\":77071},{\"end\":77393,\"start\":77387},{\"end\":77407,\"start\":77399},{\"end\":77421,\"start\":77414},{\"end\":77431,\"start\":77427},{\"end\":77447,\"start\":77438},{\"end\":77458,\"start\":77454},{\"end\":77770,\"start\":77763},{\"end\":77782,\"start\":77775},{\"end\":77798,\"start\":77790},{\"end\":77812,\"start\":77806},{\"end\":77823,\"start\":77817},{\"end\":78241,\"start\":78233},{\"end\":78253,\"start\":78246},{\"end\":78266,\"start\":78259},{\"end\":78278,\"start\":78271},{\"end\":78692,\"start\":78684},{\"end\":78703,\"start\":78700},{\"end\":78714,\"start\":78709},{\"end\":78728,\"start\":78719},{\"end\":79054,\"start\":79051},{\"end\":79069,\"start\":79062},{\"end\":79404,\"start\":79398},{\"end\":79416,\"start\":79412},{\"end\":79428,\"start\":79422},{\"end\":79443,\"start\":79434},{\"end\":79451,\"start\":79448},{\"end\":79462,\"start\":79458},{\"end\":79471,\"start\":79467},{\"end\":79790,\"start\":79783},{\"end\":79802,\"start\":79796},{\"end\":79814,\"start\":79807},{\"end\":79827,\"start\":79822},{\"end\":79836,\"start\":79833},{\"end\":79846,\"start\":79841},{\"end\":80202,\"start\":80195},{\"end\":80214,\"start\":80208},{\"end\":80223,\"start\":80219},{\"end\":80233,\"start\":80228},{\"end\":80242,\"start\":80239},{\"end\":80252,\"start\":80247}]", "bib_author_last_name": "[{\"end\":55809,\"start\":55801},{\"end\":55823,\"start\":55817},{\"end\":56019,\"start\":56009},{\"end\":56034,\"start\":56026},{\"end\":56053,\"start\":56044},{\"end\":56587,\"start\":56582},{\"end\":56605,\"start\":56598},{\"end\":56619,\"start\":56614},{\"end\":56631,\"start\":56626},{\"end\":56941,\"start\":56938},{\"end\":56949,\"start\":56947},{\"end\":56962,\"start\":56960},{\"end\":57330,\"start\":57325},{\"end\":57348,\"start\":57338},{\"end\":57363,\"start\":57357},{\"end\":57379,\"start\":57374},{\"end\":57771,\"start\":57767},{\"end\":57788,\"start\":57779},{\"end\":57806,\"start\":57799},{\"end\":57823,\"start\":57817},{\"end\":58189,\"start\":58185},{\"end\":58201,\"start\":58199},{\"end\":58216,\"start\":58212},{\"end\":58571,\"start\":58568},{\"end\":58580,\"start\":58576},{\"end\":58590,\"start\":58587},{\"end\":58600,\"start\":58596},{\"end\":58617,\"start\":58610},{\"end\":58630,\"start\":58626},{\"end\":58640,\"start\":58636},{\"end\":58655,\"start\":58652},{\"end\":58668,\"start\":58663},{\"end\":58679,\"start\":58675},{\"end\":59419,\"start\":59409},{\"end\":59435,\"start\":59428},{\"end\":59457,\"start\":59444},{\"end\":59829,\"start\":59814},{\"end\":59844,\"start\":59839},{\"end\":59852,\"start\":59846},{\"end\":60206,\"start\":60200},{\"end\":60221,\"start\":60217},{\"end\":60242,\"start\":60236},{\"end\":60564,\"start\":60554},{\"end\":60581,\"start\":60574},{\"end\":60602,\"start\":60596},{\"end\":60611,\"start\":60609},{\"end\":60950,\"start\":60944},{\"end\":60965,\"start\":60959},{\"end\":60974,\"start\":60970},{\"end\":60986,\"start\":60981},{\"end\":61737,\"start\":61732},{\"end\":61752,\"start\":61747},{\"end\":61768,\"start\":61762},{\"end\":61785,\"start\":61779},{\"end\":61803,\"start\":61794},{\"end\":61822,\"start\":61811},{\"end\":61836,\"start\":61829},{\"end\":61858,\"start\":61847},{\"end\":61871,\"start\":61868},{\"end\":61897,\"start\":61893},{\"end\":61909,\"start\":61905},{\"end\":61933,\"start\":61916},{\"end\":61947,\"start\":61942},{\"end\":61954,\"start\":61949},{\"end\":62477,\"start\":62459},{\"end\":62488,\"start\":62484},{\"end\":62498,\"start\":62490},{\"end\":62779,\"start\":62775},{\"end\":62789,\"start\":62781},{\"end\":62800,\"start\":62793},{\"end\":62806,\"start\":62802},{\"end\":63143,\"start\":63141},{\"end\":63157,\"start\":63154},{\"end\":63173,\"start\":63167},{\"end\":63186,\"start\":63182},{\"end\":63198,\"start\":63195},{\"end\":63209,\"start\":63206},{\"end\":63226,\"start\":63219},{\"end\":63241,\"start\":63233},{\"end\":63635,\"start\":63632},{\"end\":63646,\"start\":63643},{\"end\":63656,\"start\":63651},{\"end\":63669,\"start\":63666},{\"end\":63683,\"start\":63681},{\"end\":63695,\"start\":63692},{\"end\":64223,\"start\":64220},{\"end\":64237,\"start\":64232},{\"end\":64251,\"start\":64249},{\"end\":64262,\"start\":64258},{\"end\":64274,\"start\":64270},{\"end\":64286,\"start\":64283},{\"end\":64653,\"start\":64650},{\"end\":64661,\"start\":64659},{\"end\":64674,\"start\":64671},{\"end\":64689,\"start\":64685},{\"end\":64702,\"start\":64698},{\"end\":64716,\"start\":64712},{\"end\":65194,\"start\":65188},{\"end\":65204,\"start\":65200},{\"end\":65213,\"start\":65206},{\"end\":65464,\"start\":65458},{\"end\":65474,\"start\":65470},{\"end\":65483,\"start\":65476},{\"end\":65755,\"start\":65747},{\"end\":65776,\"start\":65764},{\"end\":65795,\"start\":65786},{\"end\":66123,\"start\":66120},{\"end\":66132,\"start\":66130},{\"end\":66144,\"start\":66141},{\"end\":66155,\"start\":66151},{\"end\":66167,\"start\":66163},{\"end\":66183,\"start\":66176},{\"end\":66526,\"start\":66523},{\"end\":66538,\"start\":66535},{\"end\":66548,\"start\":66545},{\"end\":66560,\"start\":66556},{\"end\":66570,\"start\":66567},{\"end\":66583,\"start\":66581},{\"end\":66834,\"start\":66828},{\"end\":66846,\"start\":66842},{\"end\":66852,\"start\":66848},{\"end\":66860,\"start\":66854},{\"end\":67102,\"start\":67096},{\"end\":67114,\"start\":67108},{\"end\":67127,\"start\":67121},{\"end\":67139,\"start\":67134},{\"end\":67148,\"start\":67145},{\"end\":67486,\"start\":67480},{\"end\":67500,\"start\":67498},{\"end\":67513,\"start\":67507},{\"end\":67520,\"start\":67515},{\"end\":67759,\"start\":67747},{\"end\":67769,\"start\":67767},{\"end\":67784,\"start\":67777},{\"end\":68113,\"start\":68103},{\"end\":68125,\"start\":68123},{\"end\":68136,\"start\":68132},{\"end\":68148,\"start\":68143},{\"end\":68161,\"start\":68158},{\"end\":68168,\"start\":68163},{\"end\":68532,\"start\":68526},{\"end\":68543,\"start\":68538},{\"end\":68560,\"start\":68555},{\"end\":68572,\"start\":68567},{\"end\":68588,\"start\":68580},{\"end\":68604,\"start\":68598},{\"end\":68620,\"start\":68613},{\"end\":68632,\"start\":68629},{\"end\":68652,\"start\":68642},{\"end\":68665,\"start\":68659},{\"end\":69108,\"start\":69104},{\"end\":69123,\"start\":69118},{\"end\":69135,\"start\":69132},{\"end\":69150,\"start\":69145},{\"end\":69159,\"start\":69155},{\"end\":69172,\"start\":69170},{\"end\":69187,\"start\":69182},{\"end\":69548,\"start\":69541},{\"end\":69562,\"start\":69555},{\"end\":69577,\"start\":69571},{\"end\":70039,\"start\":70036},{\"end\":70055,\"start\":70049},{\"end\":70071,\"start\":70065},{\"end\":70084,\"start\":70078},{\"end\":70101,\"start\":70092},{\"end\":70119,\"start\":70108},{\"end\":70400,\"start\":70396},{\"end\":70682,\"start\":70679},{\"end\":70696,\"start\":70692},{\"end\":70704,\"start\":70701},{\"end\":70717,\"start\":70713},{\"end\":70729,\"start\":70727},{\"end\":70741,\"start\":70736},{\"end\":70756,\"start\":70751},{\"end\":71230,\"start\":71221},{\"end\":71245,\"start\":71238},{\"end\":71586,\"start\":71582},{\"end\":71595,\"start\":71592},{\"end\":71605,\"start\":71602},{\"end\":71618,\"start\":71614},{\"end\":71631,\"start\":71628},{\"end\":71973,\"start\":71963},{\"end\":71988,\"start\":71983},{\"end\":71999,\"start\":71992},{\"end\":72016,\"start\":72008},{\"end\":72028,\"start\":72025},{\"end\":72044,\"start\":72038},{\"end\":72051,\"start\":72046},{\"end\":72332,\"start\":72322},{\"end\":72350,\"start\":72342},{\"end\":72368,\"start\":72360},{\"end\":72384,\"start\":72378},{\"end\":72396,\"start\":72393},{\"end\":72411,\"start\":72405},{\"end\":72768,\"start\":72759},{\"end\":72778,\"start\":72774},{\"end\":72792,\"start\":72789},{\"end\":72803,\"start\":72801},{\"end\":72813,\"start\":72810},{\"end\":72819,\"start\":72815},{\"end\":73184,\"start\":73180},{\"end\":73196,\"start\":73193},{\"end\":73206,\"start\":73204},{\"end\":73220,\"start\":73216},{\"end\":73232,\"start\":73227},{\"end\":73247,\"start\":73242},{\"end\":73602,\"start\":73598},{\"end\":73614,\"start\":73611},{\"end\":73628,\"start\":73624},{\"end\":73642,\"start\":73639},{\"end\":73654,\"start\":73649},{\"end\":74044,\"start\":74040},{\"end\":74054,\"start\":74049},{\"end\":74064,\"start\":74062},{\"end\":74074,\"start\":74071},{\"end\":74084,\"start\":74082},{\"end\":74096,\"start\":74092},{\"end\":74110,\"start\":74106},{\"end\":74119,\"start\":74117},{\"end\":74131,\"start\":74129},{\"end\":74139,\"start\":74136},{\"end\":74153,\"start\":74149},{\"end\":74162,\"start\":74160},{\"end\":74178,\"start\":74171},{\"end\":74190,\"start\":74188},{\"end\":74203,\"start\":74198},{\"end\":74694,\"start\":74690},{\"end\":74705,\"start\":74702},{\"end\":74716,\"start\":74712},{\"end\":74730,\"start\":74726},{\"end\":74737,\"start\":74735},{\"end\":74748,\"start\":74745},{\"end\":74758,\"start\":74755},{\"end\":75056,\"start\":75052},{\"end\":75066,\"start\":75063},{\"end\":75077,\"start\":75073},{\"end\":75087,\"start\":75084},{\"end\":75098,\"start\":75095},{\"end\":75113,\"start\":75109},{\"end\":75513,\"start\":75509},{\"end\":75525,\"start\":75522},{\"end\":75535,\"start\":75531},{\"end\":75542,\"start\":75540},{\"end\":75866,\"start\":75864},{\"end\":75878,\"start\":75875},{\"end\":75892,\"start\":75888},{\"end\":75906,\"start\":75902},{\"end\":75921,\"start\":75916},{\"end\":75934,\"start\":75932},{\"end\":76304,\"start\":76301},{\"end\":76313,\"start\":76310},{\"end\":76321,\"start\":76319},{\"end\":76331,\"start\":76328},{\"end\":76696,\"start\":76694},{\"end\":76707,\"start\":76705},{\"end\":76722,\"start\":76714},{\"end\":76740,\"start\":76733},{\"end\":77032,\"start\":77028},{\"end\":77045,\"start\":77042},{\"end\":77056,\"start\":77052},{\"end\":77069,\"start\":77066},{\"end\":77083,\"start\":77078},{\"end\":77397,\"start\":77394},{\"end\":77412,\"start\":77408},{\"end\":77425,\"start\":77422},{\"end\":77436,\"start\":77432},{\"end\":77452,\"start\":77448},{\"end\":77463,\"start\":77459},{\"end\":77773,\"start\":77771},{\"end\":77788,\"start\":77783},{\"end\":77804,\"start\":77799},{\"end\":77815,\"start\":77813},{\"end\":77828,\"start\":77824},{\"end\":78244,\"start\":78242},{\"end\":78257,\"start\":78254},{\"end\":78269,\"start\":78267},{\"end\":78283,\"start\":78279},{\"end\":78698,\"start\":78693},{\"end\":78707,\"start\":78704},{\"end\":78717,\"start\":78715},{\"end\":78731,\"start\":78729},{\"end\":79060,\"start\":79055},{\"end\":79072,\"start\":79070},{\"end\":79410,\"start\":79405},{\"end\":79420,\"start\":79417},{\"end\":79432,\"start\":79429},{\"end\":79446,\"start\":79444},{\"end\":79456,\"start\":79452},{\"end\":79465,\"start\":79463},{\"end\":79474,\"start\":79472},{\"end\":79794,\"start\":79791},{\"end\":79805,\"start\":79803},{\"end\":79820,\"start\":79815},{\"end\":79831,\"start\":79828},{\"end\":79839,\"start\":79837},{\"end\":79851,\"start\":79847},{\"end\":80206,\"start\":80203},{\"end\":80217,\"start\":80215},{\"end\":80226,\"start\":80224},{\"end\":80237,\"start\":80234},{\"end\":80245,\"start\":80243},{\"end\":80257,\"start\":80253}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":55897,\"start\":55761},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1062907},\"end\":56515,\"start\":55899},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17682909},\"end\":56869,\"start\":56517},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14675158},\"end\":57252,\"start\":56871},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49865868},\"end\":57689,\"start\":57254},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211096730},\"end\":58088,\"start\":57691},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":214003631},\"end\":58469,\"start\":58090},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220525491},\"end\":59319,\"start\":58471},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3016223},\"end\":59735,\"start\":59321},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13089886},\"end\":60112,\"start\":59737},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":231855665},\"end\":60491,\"start\":60114},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b11\",\"matched_paper_id\":85543335},\"end\":60860,\"start\":60493},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":219530980},\"end\":61646,\"start\":60862},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219687798},\"end\":62402,\"start\":61648},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4755450},\"end\":62721,\"start\":62404},{\"attributes\":{\"id\":\"b15\"},\"end\":63069,\"start\":62723},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":218487328},\"end\":63553,\"start\":63071},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":240230849},\"end\":64121,\"start\":63555},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":234469746},\"end\":64585,\"start\":64123},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":218719947},\"end\":65151,\"start\":64587},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14249137},\"end\":65388,\"start\":65153},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3144218},\"end\":65701,\"start\":65390},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202783932},\"end\":66029,\"start\":65703},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":232076515},\"end\":66473,\"start\":66031},{\"attributes\":{\"doi\":\"arXiv:2103.00111\",\"id\":\"b24\"},\"end\":66774,\"start\":66475},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7374680},\"end\":67025,\"start\":66776},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15384610},\"end\":67425,\"start\":67027},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":18764978},\"end\":67739,\"start\":67427},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b28\"},\"end\":68030,\"start\":67741},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":51608273},\"end\":68449,\"start\":68032},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202786778},\"end\":69020,\"start\":68451},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":211020745},\"end\":69480,\"start\":69022},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3051291},\"end\":69980,\"start\":69482},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":62016134},\"end\":70317,\"start\":69982},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":911406},\"end\":70606,\"start\":70319},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":233481280},\"end\":71092,\"start\":70608},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2759724},\"end\":71524,\"start\":71094},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2701448},\"end\":71935,\"start\":71526},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52877454},\"end\":72288,\"start\":71937},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3292002},\"end\":72658,\"start\":72290},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":244948497},\"end\":73105,\"start\":72660},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":189928183},\"end\":73532,\"start\":73107},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":28464262},\"end\":74031,\"start\":73534},{\"attributes\":{\"doi\":\"arXiv:1909.01315\",\"id\":\"b43\"},\"end\":74637,\"start\":74033},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":235324638},\"end\":75005,\"start\":74639},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":29154877},\"end\":75407,\"start\":75007},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":225039906},\"end\":75805,\"start\":75409},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":57375753},\"end\":76214,\"start\":75807},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7265538},\"end\":76645,\"start\":76216},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52895589},\"end\":76960,\"start\":76647},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2452205},\"end\":77338,\"start\":76962},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":225076220},\"end\":77724,\"start\":77340},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":208139383},\"end\":78141,\"start\":77726},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":231632459},\"end\":78622,\"start\":78143},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":174798309},\"end\":78983,\"start\":78624},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":245645738},\"end\":79396,\"start\":78985},{\"attributes\":{\"doi\":\"arXiv:2111.10698\",\"id\":\"b56\"},\"end\":79781,\"start\":79398},{\"attributes\":{\"doi\":\"arXiv:2103.03036\",\"id\":\"b57\"},\"end\":80138,\"start\":79783},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":225094367},\"end\":80504,\"start\":80140}]", "bib_title": "[{\"end\":55996,\"start\":55899},{\"end\":56575,\"start\":56517},{\"end\":56926,\"start\":56871},{\"end\":57314,\"start\":57254},{\"end\":57760,\"start\":57691},{\"end\":58180,\"start\":58090},{\"end\":58560,\"start\":58471},{\"end\":59399,\"start\":59321},{\"end\":59804,\"start\":59737},{\"end\":60191,\"start\":60114},{\"end\":60547,\"start\":60493},{\"end\":60937,\"start\":60862},{\"end\":61717,\"start\":61648},{\"end\":62453,\"start\":62404},{\"end\":62771,\"start\":62723},{\"end\":63132,\"start\":63071},{\"end\":63625,\"start\":63555},{\"end\":64213,\"start\":64123},{\"end\":64644,\"start\":64587},{\"end\":65184,\"start\":65153},{\"end\":65454,\"start\":65390},{\"end\":65736,\"start\":65703},{\"end\":66112,\"start\":66031},{\"end\":66819,\"start\":66776},{\"end\":67086,\"start\":67027},{\"end\":67476,\"start\":67427},{\"end\":68095,\"start\":68032},{\"end\":68519,\"start\":68451},{\"end\":69097,\"start\":69022},{\"end\":69533,\"start\":69482},{\"end\":70023,\"start\":69982},{\"end\":70387,\"start\":70319},{\"end\":70673,\"start\":70608},{\"end\":71213,\"start\":71094},{\"end\":71576,\"start\":71526},{\"end\":71955,\"start\":71937},{\"end\":72314,\"start\":72290},{\"end\":72750,\"start\":72660},{\"end\":73173,\"start\":73107},{\"end\":73591,\"start\":73534},{\"end\":74681,\"start\":74639},{\"end\":75045,\"start\":75007},{\"end\":75498,\"start\":75409},{\"end\":75854,\"start\":75807},{\"end\":76291,\"start\":76216},{\"end\":76685,\"start\":76647},{\"end\":77020,\"start\":76962},{\"end\":77385,\"start\":77340},{\"end\":77761,\"start\":77726},{\"end\":78231,\"start\":78143},{\"end\":78682,\"start\":78624},{\"end\":79049,\"start\":78985},{\"end\":80193,\"start\":80140}]", "bib_author": "[{\"end\":55811,\"start\":55794},{\"end\":55825,\"start\":55811},{\"end\":56021,\"start\":55998},{\"end\":56036,\"start\":56021},{\"end\":56055,\"start\":56036},{\"end\":56589,\"start\":56577},{\"end\":56607,\"start\":56589},{\"end\":56621,\"start\":56607},{\"end\":56633,\"start\":56621},{\"end\":56943,\"start\":56928},{\"end\":56951,\"start\":56943},{\"end\":56964,\"start\":56951},{\"end\":57332,\"start\":57316},{\"end\":57350,\"start\":57332},{\"end\":57365,\"start\":57350},{\"end\":57381,\"start\":57365},{\"end\":57773,\"start\":57762},{\"end\":57790,\"start\":57773},{\"end\":57808,\"start\":57790},{\"end\":57825,\"start\":57808},{\"end\":58191,\"start\":58182},{\"end\":58203,\"start\":58191},{\"end\":58218,\"start\":58203},{\"end\":58573,\"start\":58562},{\"end\":58582,\"start\":58573},{\"end\":58592,\"start\":58582},{\"end\":58602,\"start\":58592},{\"end\":58619,\"start\":58602},{\"end\":58632,\"start\":58619},{\"end\":58642,\"start\":58632},{\"end\":58657,\"start\":58642},{\"end\":58670,\"start\":58657},{\"end\":58681,\"start\":58670},{\"end\":59421,\"start\":59401},{\"end\":59437,\"start\":59421},{\"end\":59459,\"start\":59437},{\"end\":59831,\"start\":59806},{\"end\":59846,\"start\":59831},{\"end\":59854,\"start\":59846},{\"end\":60208,\"start\":60193},{\"end\":60223,\"start\":60208},{\"end\":60244,\"start\":60223},{\"end\":60566,\"start\":60549},{\"end\":60583,\"start\":60566},{\"end\":60604,\"start\":60583},{\"end\":60613,\"start\":60604},{\"end\":60952,\"start\":60939},{\"end\":60967,\"start\":60952},{\"end\":60976,\"start\":60967},{\"end\":60988,\"start\":60976},{\"end\":61739,\"start\":61719},{\"end\":61754,\"start\":61739},{\"end\":61770,\"start\":61754},{\"end\":61787,\"start\":61770},{\"end\":61805,\"start\":61787},{\"end\":61824,\"start\":61805},{\"end\":61838,\"start\":61824},{\"end\":61860,\"start\":61838},{\"end\":61873,\"start\":61860},{\"end\":61899,\"start\":61873},{\"end\":61911,\"start\":61899},{\"end\":61935,\"start\":61911},{\"end\":61949,\"start\":61935},{\"end\":61956,\"start\":61949},{\"end\":62479,\"start\":62455},{\"end\":62490,\"start\":62479},{\"end\":62500,\"start\":62490},{\"end\":62781,\"start\":62773},{\"end\":62791,\"start\":62781},{\"end\":62802,\"start\":62791},{\"end\":62808,\"start\":62802},{\"end\":63145,\"start\":63134},{\"end\":63159,\"start\":63145},{\"end\":63175,\"start\":63159},{\"end\":63188,\"start\":63175},{\"end\":63200,\"start\":63188},{\"end\":63211,\"start\":63200},{\"end\":63228,\"start\":63211},{\"end\":63243,\"start\":63228},{\"end\":63637,\"start\":63627},{\"end\":63648,\"start\":63637},{\"end\":63658,\"start\":63648},{\"end\":63671,\"start\":63658},{\"end\":63685,\"start\":63671},{\"end\":63697,\"start\":63685},{\"end\":64225,\"start\":64215},{\"end\":64239,\"start\":64225},{\"end\":64253,\"start\":64239},{\"end\":64264,\"start\":64253},{\"end\":64276,\"start\":64264},{\"end\":64288,\"start\":64276},{\"end\":64655,\"start\":64646},{\"end\":64663,\"start\":64655},{\"end\":64676,\"start\":64663},{\"end\":64691,\"start\":64676},{\"end\":64704,\"start\":64691},{\"end\":64718,\"start\":64704},{\"end\":65196,\"start\":65186},{\"end\":65206,\"start\":65196},{\"end\":65215,\"start\":65206},{\"end\":65466,\"start\":65456},{\"end\":65476,\"start\":65466},{\"end\":65485,\"start\":65476},{\"end\":65757,\"start\":65738},{\"end\":65778,\"start\":65757},{\"end\":65797,\"start\":65778},{\"end\":66125,\"start\":66114},{\"end\":66134,\"start\":66125},{\"end\":66146,\"start\":66134},{\"end\":66157,\"start\":66146},{\"end\":66169,\"start\":66157},{\"end\":66185,\"start\":66169},{\"end\":66528,\"start\":66517},{\"end\":66540,\"start\":66528},{\"end\":66550,\"start\":66540},{\"end\":66562,\"start\":66550},{\"end\":66572,\"start\":66562},{\"end\":66585,\"start\":66572},{\"end\":66836,\"start\":66821},{\"end\":66848,\"start\":66836},{\"end\":66854,\"start\":66848},{\"end\":66862,\"start\":66854},{\"end\":67104,\"start\":67088},{\"end\":67116,\"start\":67104},{\"end\":67129,\"start\":67116},{\"end\":67141,\"start\":67129},{\"end\":67150,\"start\":67141},{\"end\":67488,\"start\":67478},{\"end\":67502,\"start\":67488},{\"end\":67515,\"start\":67502},{\"end\":67522,\"start\":67515},{\"end\":67761,\"start\":67741},{\"end\":67771,\"start\":67761},{\"end\":67786,\"start\":67771},{\"end\":68115,\"start\":68097},{\"end\":68127,\"start\":68115},{\"end\":68138,\"start\":68127},{\"end\":68150,\"start\":68138},{\"end\":68163,\"start\":68150},{\"end\":68170,\"start\":68163},{\"end\":68534,\"start\":68521},{\"end\":68545,\"start\":68534},{\"end\":68562,\"start\":68545},{\"end\":68574,\"start\":68562},{\"end\":68590,\"start\":68574},{\"end\":68606,\"start\":68590},{\"end\":68622,\"start\":68606},{\"end\":68634,\"start\":68622},{\"end\":68654,\"start\":68634},{\"end\":68667,\"start\":68654},{\"end\":69110,\"start\":69099},{\"end\":69125,\"start\":69110},{\"end\":69137,\"start\":69125},{\"end\":69152,\"start\":69137},{\"end\":69161,\"start\":69152},{\"end\":69174,\"start\":69161},{\"end\":69189,\"start\":69174},{\"end\":69550,\"start\":69535},{\"end\":69564,\"start\":69550},{\"end\":69579,\"start\":69564},{\"end\":70041,\"start\":70025},{\"end\":70057,\"start\":70041},{\"end\":70073,\"start\":70057},{\"end\":70086,\"start\":70073},{\"end\":70103,\"start\":70086},{\"end\":70121,\"start\":70103},{\"end\":70402,\"start\":70389},{\"end\":70684,\"start\":70675},{\"end\":70698,\"start\":70684},{\"end\":70706,\"start\":70698},{\"end\":70719,\"start\":70706},{\"end\":70731,\"start\":70719},{\"end\":70743,\"start\":70731},{\"end\":70758,\"start\":70743},{\"end\":71232,\"start\":71215},{\"end\":71247,\"start\":71232},{\"end\":71588,\"start\":71578},{\"end\":71597,\"start\":71588},{\"end\":71607,\"start\":71597},{\"end\":71620,\"start\":71607},{\"end\":71633,\"start\":71620},{\"end\":71975,\"start\":71957},{\"end\":71990,\"start\":71975},{\"end\":72001,\"start\":71990},{\"end\":72018,\"start\":72001},{\"end\":72030,\"start\":72018},{\"end\":72046,\"start\":72030},{\"end\":72053,\"start\":72046},{\"end\":72334,\"start\":72316},{\"end\":72352,\"start\":72334},{\"end\":72370,\"start\":72352},{\"end\":72386,\"start\":72370},{\"end\":72398,\"start\":72386},{\"end\":72413,\"start\":72398},{\"end\":72770,\"start\":72752},{\"end\":72780,\"start\":72770},{\"end\":72794,\"start\":72780},{\"end\":72805,\"start\":72794},{\"end\":72815,\"start\":72805},{\"end\":72821,\"start\":72815},{\"end\":73186,\"start\":73175},{\"end\":73198,\"start\":73186},{\"end\":73208,\"start\":73198},{\"end\":73222,\"start\":73208},{\"end\":73234,\"start\":73222},{\"end\":73249,\"start\":73234},{\"end\":73604,\"start\":73593},{\"end\":73616,\"start\":73604},{\"end\":73630,\"start\":73616},{\"end\":73644,\"start\":73630},{\"end\":73656,\"start\":73644},{\"end\":74046,\"start\":74033},{\"end\":74056,\"start\":74046},{\"end\":74066,\"start\":74056},{\"end\":74076,\"start\":74066},{\"end\":74086,\"start\":74076},{\"end\":74098,\"start\":74086},{\"end\":74112,\"start\":74098},{\"end\":74121,\"start\":74112},{\"end\":74133,\"start\":74121},{\"end\":74141,\"start\":74133},{\"end\":74155,\"start\":74141},{\"end\":74164,\"start\":74155},{\"end\":74180,\"start\":74164},{\"end\":74192,\"start\":74180},{\"end\":74205,\"start\":74192},{\"end\":74696,\"start\":74683},{\"end\":74707,\"start\":74696},{\"end\":74718,\"start\":74707},{\"end\":74732,\"start\":74718},{\"end\":74739,\"start\":74732},{\"end\":74750,\"start\":74739},{\"end\":74760,\"start\":74750},{\"end\":75058,\"start\":75047},{\"end\":75068,\"start\":75058},{\"end\":75079,\"start\":75068},{\"end\":75089,\"start\":75079},{\"end\":75100,\"start\":75089},{\"end\":75115,\"start\":75100},{\"end\":75515,\"start\":75500},{\"end\":75527,\"start\":75515},{\"end\":75537,\"start\":75527},{\"end\":75544,\"start\":75537},{\"end\":75868,\"start\":75856},{\"end\":75880,\"start\":75868},{\"end\":75894,\"start\":75880},{\"end\":75908,\"start\":75894},{\"end\":75923,\"start\":75908},{\"end\":75936,\"start\":75923},{\"end\":76306,\"start\":76293},{\"end\":76315,\"start\":76306},{\"end\":76323,\"start\":76315},{\"end\":76333,\"start\":76323},{\"end\":76698,\"start\":76687},{\"end\":76709,\"start\":76698},{\"end\":76724,\"start\":76709},{\"end\":76742,\"start\":76724},{\"end\":77034,\"start\":77022},{\"end\":77047,\"start\":77034},{\"end\":77058,\"start\":77047},{\"end\":77071,\"start\":77058},{\"end\":77085,\"start\":77071},{\"end\":77399,\"start\":77387},{\"end\":77414,\"start\":77399},{\"end\":77427,\"start\":77414},{\"end\":77438,\"start\":77427},{\"end\":77454,\"start\":77438},{\"end\":77465,\"start\":77454},{\"end\":77775,\"start\":77763},{\"end\":77790,\"start\":77775},{\"end\":77806,\"start\":77790},{\"end\":77817,\"start\":77806},{\"end\":77830,\"start\":77817},{\"end\":78246,\"start\":78233},{\"end\":78259,\"start\":78246},{\"end\":78271,\"start\":78259},{\"end\":78285,\"start\":78271},{\"end\":78700,\"start\":78684},{\"end\":78709,\"start\":78700},{\"end\":78719,\"start\":78709},{\"end\":78733,\"start\":78719},{\"end\":79062,\"start\":79051},{\"end\":79074,\"start\":79062},{\"end\":79412,\"start\":79398},{\"end\":79422,\"start\":79412},{\"end\":79434,\"start\":79422},{\"end\":79448,\"start\":79434},{\"end\":79458,\"start\":79448},{\"end\":79467,\"start\":79458},{\"end\":79476,\"start\":79467},{\"end\":79796,\"start\":79783},{\"end\":79807,\"start\":79796},{\"end\":79822,\"start\":79807},{\"end\":79833,\"start\":79822},{\"end\":79841,\"start\":79833},{\"end\":79853,\"start\":79841},{\"end\":80208,\"start\":80195},{\"end\":80219,\"start\":80208},{\"end\":80228,\"start\":80219},{\"end\":80239,\"start\":80228},{\"end\":80247,\"start\":80239},{\"end\":80259,\"start\":80247}]", "bib_venue": "[{\"end\":56238,\"start\":56155},{\"end\":57073,\"start\":57027},{\"end\":57482,\"start\":57440},{\"end\":58952,\"start\":58825},{\"end\":61299,\"start\":61152},{\"end\":63864,\"start\":63789},{\"end\":64901,\"start\":64818},{\"end\":69252,\"start\":69229},{\"end\":69762,\"start\":69679},{\"end\":70867,\"start\":70821},{\"end\":71742,\"start\":71696},{\"end\":73805,\"start\":73739},{\"end\":74823,\"start\":74800},{\"end\":75224,\"start\":75178},{\"end\":75607,\"start\":75584},{\"end\":76442,\"start\":76396},{\"end\":80322,\"start\":80299},{\"end\":55792,\"start\":55761},{\"end\":56153,\"start\":56055},{\"end\":56685,\"start\":56633},{\"end\":57025,\"start\":56964},{\"end\":57438,\"start\":57381},{\"end\":57875,\"start\":57825},{\"end\":58267,\"start\":58218},{\"end\":58823,\"start\":58681},{\"end\":59508,\"start\":59459},{\"end\":59906,\"start\":59854},{\"end\":60293,\"start\":60244},{\"end\":60661,\"start\":60617},{\"end\":61150,\"start\":60988},{\"end\":62005,\"start\":61956},{\"end\":62549,\"start\":62500},{\"end\":62879,\"start\":62808},{\"end\":63292,\"start\":63243},{\"end\":63787,\"start\":63697},{\"end\":64345,\"start\":64288},{\"end\":64816,\"start\":64718},{\"end\":65261,\"start\":65215},{\"end\":65537,\"start\":65485},{\"end\":65846,\"start\":65797},{\"end\":66242,\"start\":66185},{\"end\":66515,\"start\":66475},{\"end\":66879,\"start\":66862},{\"end\":67212,\"start\":67150},{\"end\":67571,\"start\":67522},{\"end\":67860,\"start\":67802},{\"end\":68227,\"start\":68170},{\"end\":68716,\"start\":68667},{\"end\":69227,\"start\":69189},{\"end\":69677,\"start\":69579},{\"end\":70132,\"start\":70121},{\"end\":70451,\"start\":70402},{\"end\":70819,\"start\":70758},{\"end\":71296,\"start\":71247},{\"end\":71694,\"start\":71633},{\"end\":72105,\"start\":72053},{\"end\":72465,\"start\":72413},{\"end\":72870,\"start\":72821},{\"end\":73306,\"start\":73249},{\"end\":73737,\"start\":73656},{\"end\":74309,\"start\":74221},{\"end\":74798,\"start\":74760},{\"end\":75176,\"start\":75115},{\"end\":75582,\"start\":75544},{\"end\":75993,\"start\":75936},{\"end\":76394,\"start\":76333},{\"end\":76794,\"start\":76742},{\"end\":77142,\"start\":77085},{\"end\":77514,\"start\":77465},{\"end\":77912,\"start\":77830},{\"end\":78334,\"start\":78285},{\"end\":78790,\"start\":78733},{\"end\":79165,\"start\":79074},{\"end\":79564,\"start\":79492},{\"end\":79935,\"start\":79869},{\"end\":80297,\"start\":80259}]"}}}, "year": 2023, "month": 12, "day": 17}