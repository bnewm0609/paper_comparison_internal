{"id": 252222469, "updated": "2023-10-05 10:41:28.426", "metadata": {"title": "Self-supervised Multi-Modal Video Forgery Attack Detection", "authors": "[{\"first\":\"Chenhui\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Rabih\",\"last\":\"Younes\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Video forgery attack threatens the surveillance system by replacing the video captures with unrealistic synthesis, which can be powered by the latest augment reality and virtual reality technologies. From the machine perception aspect, visual objects often have RF signatures that are naturally synchronized with them during recording. In contrast to video captures, the RF signatures are more difficult to attack given their concealed and ubiquitous nature. In this work, we investigate multimodal video forgery attack detection methods using both vision and wireless modalities. Since wireless signal-based human perception is environmentally sensitive, we propose a self-supervised training strategy to enable the system to work without external annotation and thus can adapt to different environments. Our method achieves a perfect human detection accuracy and a high forgery attack detection accuracy of 94.38% which is comparable with supervised methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2209.06345", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wcnc/ZhaoLY23", "doi": "10.1109/wcnc55385.2023.10118664"}}, "content": {"source": {"pdf_hash": "7b393ebe2f746b97f613c3eab50d227adbcba2da", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2209.06345v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2bc10b41c5c3a509f38d5017e7ffabc20d2d795b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7b393ebe2f746b97f613c3eab50d227adbcba2da.txt", "contents": "\nSelf-supervised Multi-Modal Video Forgery Attack Detection\n\n\nChenhui Zhao \nXiang Li \nDepartment of Electrical and Computer Engineering\nCarnegia Mellon University\nPittsburghUSA\n\n\u2020 \nRabih Younes \nDepartment of Electrical and Computer Engineering\nDuke University\nDurhamUSA\n\n\nDepartment of Electrical and Computer Engineering\nUniversity of Michigan\nAnn ArborUSA\n\nSelf-supervised Multi-Modal Video Forgery Attack Detection\nIndex Terms-Human PerceptionWireless SignalForgery Attack Detection\nVideo forgery attacks threaten surveillance systems by replacing the video captures with unrealistic synthesis, which can be powered by the latest augmented reality and virtual reality technologies. From the machine perception aspect, visual objects often have RF signatures that are naturally synchronized with them during recording. In contrast to video captures, the RF signatures are more difficult to attack given their concealed and ubiquitous nature. In this work, we investigate multimodal video forgery attack detection methods using both visual and wireless modalities. Since wireless signal-based human perception is environmentally sensitive, we propose a self-supervised training strategy to enable the system to work without external annotation and thus adapt to different environments. Our method achieves a perfect human detection accuracy and a high forgery attack detection accuracy of 94.38% which is comparable with supervised methods. The code is publicly available at: https://github.com/ChuiZhao/Secure-Mask.git\n\nI. INTRODUCTION\n\nIn recent years, the unique properties e.g. concealment, penetration, and ubiquity have been extensively investigated in wireless-based perception methods. Person-in-WiFi [1] is a pioneering work attempting to address fine-grained human perception problems by using WiFi signals. The follow-up works Secure-Pose [2] and its improved version [3] propose learningbased methods to detect video forgery attacks using radiofrequency (RF) signals. Besides the video forgery attack task, RF-based methods can also achieve comparable performance against visual-based methods in other visual representation tasks [1], [4], [5]. However, most RF-based human perception methods are environmentally sensitive [1], [5]- [8], thus it is hard to adapt well-trained models to unseen environments, which severely prevents them from practical applications.\n\nTo date, several methods have been tried to tackle the adaptation problem. Person-in-WiFi [1] proposes a style-transfer method for CSI measurements utilizing the Cycle-GAN [9], but the performance gain of the proposed module is limited even after complicated synthetic data generation. WiPose [4] extracted the environmental weak-dependent Body-coordinate velocity profile(BVP) from CSI measurement combing with an antenna selection strategy to ease the influence of the background environment. Although many attempts have been made to mitigate the impact of environment changes [1], [4], [5], the cross-environment adaptation remain unfeasible since the RF-based human perception heavily depends on the Doppler effect and electromagnetic property differences between human and background environment. Consequently, the gaps between RF-based human perception performance in the seen and unseen environment are hard to bridge from the perspective of data augmentation and denoising.\n\nOn the other hand, a surveillance camera is a periphery device for surveillance systems which captures visual information of target environments and transmits it to a central server. Due to the redundancy of the video modality, the video compression is always conducted before transmission. Motion vector is one of the essential components of the compressed video which reflects the block-wise spatial position changes across frames, and de facto fits the format of the label of the RF-based perception system.\n\nIn this work, to solve the adaptation problem of RF-based perception method, we introduce a self-supervised learning scheme to enable the model to learn from compressed video streams and further leverage it to conduct video forgery detection. We name the proposed system Secure-Mask. In particular, we adopt motion vectors from compressed video streams to create the supervision for RF-based model. Fig. 1 shows an example of motion vector and generated mask. Compared with other frame-based video forgery detection methods [10], [11], Secure-Mask can work in a real-time manner to generate fine-grained human segmentation and detect video replacement attacks at the object level. The concealed and ubiquitous properties of WiFi signals make it a good alternative to surveillance video and can be suitable for future secure systems to act on when cameras are offline, occluded or attacked. Moreover, the self-supervised training scheme enables Secure-Mask to adapt to the new environment which eliminates the redundant data labeling after environmental changes (i.e. furniture movement). The contributions of this paper are as follows.\n\n\u2022 We propose a self-supervised learning scheme for RFbased human perceptions leveraging the motion vector as a source of supervision and proving its ability to work without external annotations. \u2022 We built up a self-supervised video forgery attack detection pipeline that can act in a real-time manner with high accuracy of 94.38%. The performance of Secure-Mask is comparable to its supervised counterpart Secure-Pose [2].\n\n\nII. RELATED WORKS\n\nCamera-based human perception. In the computer vision field, many works [12], [13] used well-developed feature extraction methods to accomplish challenging human perception tasks. In addition, there have been many works in object segmentation [14]- [18], pose estimation [19], [20], and activity recognition [21], [22]. More recently, depth cues obtained by the RGB-D camera have been introduced in human perception tasks and some other works [23], [24] have shown that depth cure can improve performance.\n\nSensor-based human perception. The Frequency Modulated Continuous Wave (FMCW) radar system was first introduced by Adib et al. [25] to capture coarse human bodies with a delicate radar device. Later, they extended this system to do pose estimation through the wall or other occlusions, with 2D [26] and 3D [27] included. Compared with the above methods, which rely heavily on expensive Radar equipment, WiFi signals provide a more ubiquitous and cheap option. However, WiFi-based works [6], [7] were not popularized before because they have not been producing fine-grained human masks or human skeletons until Wang et al. proposed Person-in-WiFi [1]. After that, more and more researchers paid attention to WiFi-based human perception works. For example, WiTA [8] recognized human activity in an attention-based way using commercial WiFi devices. Wi-Pose [5] reconstructed finegrained human poses using WiFi signals. In addition, some previous works [28], [29] also investigate how to augment data to achieve better performance. Video Forgery Detection. Forgery detection for surveillance systems has drawn researchers' attention due to the advanced video forgery technologies. Many researchers solved this problem by analyzing the spatiotemporal features in surveillance video [10], [11]. These methods can determine the framebased forgery, for example, frame delete and insert. For the object-based video forgery, Mohammed et al. proposed a sequential and patch analysis method [30], which can generate coarse forgery traces in each surveillance video frame. Relatedly, SurFi [31] compared timing information from WiFi signals and the corresponding live video to detect camera looping attacks. To generate fine-grained forgery traces while detection, Secure-Pose [2] first proposed a cross-modal system that can detect and localize forgery attacks in each video frame through a supervised way. Secure-Mask can be boiled down to four modules: multi-modal signal processing, human detector, human segmentor, and forgery detector. In the training phase, the motion vector Mt of video frame It is first synchronized to m CSI measurements {Ct, \u00b7 \u00b7 \u00b7 , C t+m }. Then we conduct the preprocessing separately for video and CSI data to get binarized masks St and CSI measurements in matrix form {At, \u00b7 \u00b7 \u00b7 , A t+m }. After that, we generate annotations for training the human detector and human segmentor networks. Finally, we use the predicted masks acquired from the human segmentor to generate annotations for training the forgery detector.\n\n\nIII. SECURE-MASK SYSTEM\n\nIn this section, we will elaborate on the detailed pipeline of our proposed Secure-Mask system.\n\n\nA. System Overview\n\nThe Secure-Mask is composed of four parts: multi-modal signal processing, human detector, human segmentor, and forgery detector. As shown in Fig. 2, in the training phase, we leverage the motion vector in the live video stream to update the networks. In the multi-modal signal processing module, we first extract the motion vector {M t } T t=0 from video frames {I t } T t=0 and synchronize it with the CSI measurements {C t } T t=0 , then conduct signal processing separately. After that, the processed masks serve as labels to update networks. In the inference phase, only the CSI branch in the multimodal signal processing module is activated. The processed CSI measurements are sent to the human detector to detect motions. To improve the efficiency of the whole system, the human segmentor and forgery detector only acts when the human detector confirms moving objects.\n\n\nB. Multi-Modal Signal Processing\n\nSecure-Mask is a cross-modal system that takes advantage of both visual and wireless modalities. To ensure both modalities contain homogeneous information, the data synchronization is essential. Given the nature of wireless communication, wireless signals always have a much higher sample rate than video frames. Therefore, we assign multiple wireless frames to one video frame. Let us denote the captured video frames as {I t } T t=0 and CSI measurements {C t } T t=0 where I t \u2208 R 3\u00d7H\u00d7W and C t \u2208 C K\u00d7Ntx\u00d7Nrx . H and W are the height and width of the video frames. N tx , N rx and K are the number of transmitters, receivers and subcarriers, respectively. We assign m CSI measurements {C t , \u00b7 \u00b7 \u00b7 , C t+m\u22121 } to one frame I t . We consider the amplitude of CSI measurements {A t } T t=0 to make it a real matrix. After that we get the data pairs {I t , {A t , \u00b7 \u00b7 \u00b7 , A t+m }} T t=0 . CSI Processing. The environment noise can cause sudden changes in the CSI measurements, which will impact the efficiency of extracting the amplitude features from it. To filter out outliers in the CSI measurements, we utilize the Hampel identifier [32] to denoise the CSI data as Secure-Pose [2] did. Video Processing. To improve video storage and transmission, it is common to perform video compression. Typically, the compression techniques such as MPEG-4 and H.264 leverage the temporal continuity of successive frames and retain only a few complete frames while reconstructing other frames using the motion vector and residual error. Our solution utilizes the 2D motion vector to create masks, which can be separated into binarization, denoising, and refinement. The motion vectors within a group of pictures (GOP) can be denoted as {M t } G t=0 where M \u2208 R H\u00d7W \u00d72 . Since the velocity of human activity is slow compared to the video frame rate, by using a short GOP length, the human movement can be assumed as the same in each GOP. We determine the binary mask of the human movement from two dimensions of the motion vector, angle and amplitude. LetM be the sum of all motion vectors in a GOP andM be the \u03c6 3\u00d73 (M ) where \u03c6 3\u00d73 is an gaussian smooth function with a kernel size of 3. For a single GOP, the binarized mask can be denoted as S and computed as:\nS = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1, if M i,j 2 + \u03bb <M i,j ,M i,j > |M i,j | \u00b7 |M i,j | \u03c4 0, else(1)\nwhere \u03bb and \u03c4 are constants. The first term and second term filter motion vector based on amplitude and degree respectively. Here we set \u03bb = 1 and \u03c4 = 0.5. The mask S is further processed through a stack of soothing and morphological operations after binarization.\n\n\nC. Human Detector\n\nThe human detector network is a lightweight network that aims to determine the existence of human motion. The Human detector takes CSI as input and outputs the binary result judging the human movement. We utilize both the convolution layer and the Long-Short Term Memory (LSTM) layer to process the CSI data, as it includes both spatial and temporal features. In particular, the CSI measurements {A t , \u00b7 \u00b7 \u00b7 , A t+m } are concatenated in the subcarriers dimension to form A c t \u2208 R mK\u00d7Ntx\u00d7Nrx . The CSI data A c t is first processed by two convolution layers followed by one LSTM layer. The final output is obtained by applying two linear layers to the LSTM output. With the human detector, we can save computational resources by only activating the following modules after detecting human motion.\n\nWe supervise the human detector by binary cross-entropy loss. Since the human detector is a binary classification network, the binarized ground-truth is obtained by the following criterion.\nC(S) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if W i=0 H j=0 S i,j W \u00d7 H \u2265 \u03b7 0 else(2)\nwhere W and H are the width and height of the pseudo mask S and \u03b7 is a constant threshold, and we set \u03b7 = 0.\n\n\nD. Human Segmentor\n\nGiven the concatenated CSI data A c t , the human segmentor generates masks of moving humans in the perception field. To conduct this challenging task, an UNet-liked structure is leveraged. The input CSI data A c t is first tiled to image size before feeding to the network. After that, the upsampled tensor is fed into an encoder to produce the encoded feature map. Then a transposed convolution-based decoder is utilized to transform wireless features to image space. Both encoder and decoder contain four downsample and upsample operations with a stride of 2 and, after each scaling operation, a 2D convolution is involved to refine the feature map before the next scaling. In particular, skip connections are used to retain swallow wireless features in later layers. Let us denote the human segmentor as S, the human mask prediction P t \u2208 R H\u00d7W can be denoted as P t = S(A c t ) We supervise the human segmentor by binary cross-entropy loss L bce and Dice loss L Dice . The overall loss for training is L = L Dice + \u03bb b L bce .\n\n\nE. Forgery Detector\n\nThe forgery detector aims to detect video forgery attacks using multimodal data. Since wireless perception heavily depends on the Doppler effect which is caused by the motion of objects, the motion vector and wireless data contain homogeneous representations of the moving humans in the perception area. We leverage the human mask as a proxy to conduct contrastive learning for video forgery detection. Given a clip of video {I t , \u00b7 \u00b7 \u00b7 , I t+g }, we can obtain the motion vectors {M t , \u00b7 \u00b7 \u00b7 , M t+g } freely from the compressed video streams. We further processed it to obtain the pseudo mask {S t , \u00b7 \u00b7 \u00b7 , S t+g }. The human segmentor predicts the human masks {P t , \u00b7 \u00b7 \u00b7 , P t+g } from the wireless modality. We tailor a network to compare the human masks from visual and wireless modalities to determine their homogeneity. The masks from the visual modality {S t , \u00b7 \u00b7 \u00b7 , S t+g } and masks from the wireless modality {P t , \u00b7 \u00b7 \u00b7 , P t+g } are concatenated as {Q t , \u00b7 \u00b7 \u00b7 , Q t+g } where Q t \u2208 R 2\u00d7H\u00d7W . We extract features for each time step separately with a ResNet-based network. After that, extracted features are fed into a one-layer LSTM followed by two fully connected layers before the final output. We predict the homogeneity in a clip-wise manner.\n\nSimilar to the human detector, we utilize binary crossentropy to supervise the training. We generate unmatched input sequence pairs to synthesis video forgery attacks by selecting unsynchronized masks pairs.\n\n\nIV. EXPERIMENTS\n\nTo the best of our knowledge, there is no public multi-modal dataset for video forgery. Thus, we conduct experiments using the same dataset used in Secure-Pose [2]. In this section, we will show the qualitative result for the human segmentor and the quantitative result for both the human detector and forgery detector. In the quantitative result, we report the evaluation metrics including the accuracy (Acc), the false positive rate (FPR), and the true positive rate (TPR).\n\n\nA. Dataset Description\n\nThe dataset (same as the dataset used in [2]) was collected in an 8m \u00d7 16m office room with 5 volunteers. As shown in Table I, during the experiment phase, zero to three volunteers were asked to perform walking, sitting, waving hands, or random movements concurrently in the perception area. Human Detector. For the human detector model, we utilize all the video frames and their corresponding CSI measurements. Then we split them randomly, in which 9663 data pairs are used for training and 1024 data pairs are used for testing. Human Segmentor. If the human in the video is not moving, we cannot leverage the motion vector to generate a reliable human mask. Therefore, we select those video frames that can generate valid motion vector and their corresponding CSI measurements. Then we split them randomly to train the human segmentor model first, in which 8574 data pairs are used for training and 870 data pairs are used for testing. Forgery Detector. After the human segmentor model is well trained, we feed the CSI measurements from the human segmentor's dataset into the human segmentor model to prepare the dataset for the forgery detector model. We get 8574 predicted masks and 870 predicted masks for the training and testing sets of the forgery detector, respectively. Then, we concatenate those predicted masks with the motion vector masks to get the labels: 0 if corresponding, else 1. Finally, for the forgery detector model, 8530 data pairs are used for training and 826 data pairs are used for testing.\n\n\nB. Implementation Details\n\nThe human detector network, human segmentor network, forgery detector network are all implemented for 20 epochs on the Pytorch framework. Human Detector. The learning rate starts from 1e \u2212 6 and is divided by 10 for each 5 epochs. The batchsize = 16 and a RMSprop optimizer [33] with weight decay = 1e \u2212 8, momentum = 0.9 is leveraged. Human Segmentor. The learning rate starts from 1e \u2212 3 and is divided by 10 for each 5 epochs. The batchsize = 32 and an adam [34] optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and weight decay = 1e \u2212 5 is leveraged. Forgery Detector. The learning rate starts from 1e \u2212 3 and is divided by 10 for each 5 epochs. The batchsize = 32 and an adam [34] optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and weight decay = 2e \u2212 5 is leveraged.\n\n\nC. Main Results\n\nIn this section, we report the performance of the human detector, human segmentor and the forgery detector. In addition, we also compare the result of the forgery detector with other recent methods. Qualitative results. Since the dataset does not contain manually annotated segmentation labels, we only show the qualitative results of the predicted masks. As shown in Fig. 3, the predicted masks have the corrected spatial position , but the detailed shapes are not recovered. Two reasons can account for the shape difference. First, the spatial resolution of the commercial Wi-Fi signals is less than one decimeter, which makes it difficult for wireless data to capture detailed human boundary information. Second, the motion vector only provides coarse supervision compared to masks predicted by neural networks or manual annotation. Quantitative results. We report the quantitative results of the human detector and the forgery detector. In the experiment, we set the number of CSI measurements per video frame m = 5 and the number of input video frames to the forgery detector g = 7. As shown in Table II, the human detector has a perfect performance. This is because the wireless signals are sensitive to moving objects. In the indoor scenario, the motion information can be effectively carried by the Wi-Fi signals: the CSI data contains the spatial information because of the Doppler effect, and as a kind of sampled signal, CSI data contains the temporal information naturally. Therefore, when considering the feature extraction strategy, combining the convolution layer with the LSTM layer is much better than the pure convolution operation as well.\n\nThe forgery detector also shows its promising performance, whose overall accuracy can reach 94.38%. We compare our method with other recent approaches [2], [11], [30], [31], which are supervised learning-based method. As shown in Table III, event-and frame-based forgery detection generally  have a higher accuracy than those at the object level. However, forgery detection only at the event and frame level will be limited in some situations (i.e., the forgery attack on the entire video). When considering the working pattern, the overall accuracy of Secure-Mask is only 0.52% lower compared to its counterpart, Secure-Pose, which needs work in a supervised way. However, our self-supervised approach enables the system to maintain its performance as the environment changes. We also report the inference speed of the proposed system. As shown in Table IV, even using a normal GPU, our system can perform in a real-time manner.\n\n\nD. Ablation Study\n\nIn this section, we conduct extensive ablation experiments to study the core factors of our method. CSI measurements per video frame m. We first train the human segmentor with different amounts of the CSI measurements per video frame to evaluate the influence of the single predicted video frame on the forgery detector. As reported in Table V, with the number of the CSI measurements per video frame varying from 1 to 5, the Accuracy increases from 90.25% to 94.38%. This result suggests that an additional temporal information can make it easier for the human  segmentor to learn how to decode human motion information from CSI measurements. Indeed, this kind of improvement makes the performance of the forgery detector better. Number of the input video frames to forgery detector g. We then train the forgery detector with different numbers of the input video frames to evaluate the effect of the amount of the video frames on the model. As reported in Table VI, with the number varying from 3 to 7, the accuracy increases from 87.17% to 94.38%. This result shows that feeding more video frames into the model does improve its performance. The LSTM component in the forgery detector model can account for this since the LSTM component can learn the forgery information more efficiently when additional special information combined with temporal information is provided. As the video was recorded at 7.5 FPS from the camera, we set the largest amount of the input video frames as 7 to avoid fail detection with the short-time forgery. Moreover, we argue that if we recorded the video at a higher FPS, the forgery detector can achieve even better performance.\n\n\nV. CONCLUSION\n\nIn this paper, we build a novel self-supervised system for video forgery detection eliminating the need for external annotations. Notably, our method achieves comparable performance against the previous supervised methods in forgery detection.\n\nFig. 1 .\n1Illustration of an original 2D motion vector and a binarized motion vector produced by our method from compressed video. The top and bottom figures of (a) are two adjacent frames in the video. The top and bottom figures of (b) are the X-dimension motion vector and the Y-dimension motion vector. The top and bottom figures of (c) are the original binarized motion vector and the binarized motion vector after processing.\n\nFig. 2 .\n2Secure-Mask overview. Dash lines indicate model updating.\n\nFig. 3 .\n3Qualitative results of the human segmentor. We provide four video clips in total and each row contains two video clips. In clips (a) and (b), there is one person in the perception field and there are two people in the perception field for clips (c) and (d). For each clip, we show both the motion vector masks and the predicted masks, which reports the predicted masks having the corrected spatial position while the detailed shapes are lost.\n\nTABLE I STATISTIC\nIOF THE DATASET. P: NUMBER OF CONCURRENT PERSON. F: NUMBER OF VIDEO FRAMES.P \n0 \n1 \n2 \n3 \ntotal \nF 2242 4488 4498 911 12139 \n\n\n\nTABLE II QUANTITATIVE\nIIRESULTS OF HUMAN DETECTOR AND FORGERY DETECTOR. WE SET m = 5 AND g = 7 FOR BOTH RESULTS.Module \nAcc \nFPR \nTPR \nHuman Detector \n100% \n0% \n100% \nForgery Detector 94.38% 4.01% 92.27% \n\n\n\nTABLE III COMPARISON\nIIIRESULT OF FORGERY ATTACK DETECTION METHODS. THOSE METHODS USE DIFFERENT DATASETS TO INSTRUCT THE FORGERY ATTACK DETECTION AT DIFFERENT LEVELS. F: FRAME-BASED FORGERY DETECTION. E: EVENT-BASED FORGERY DETECTION. O: OBJECT-BASED FORGERY DETECTIONMethod \nLevel Multi-Modal \nAcc \nFadl [11] \nF \n98.0% \nLakshmanan [31] \nE \n98.9% \nAloraini [30] \nO \n93.18% \nHuang [2] (Secure-Pose) \nO \n94.9% \nSecure-Mask (Ours) \nO \n94.38% \n\n\n\nTABLE IV INFERENCE\nIVSPEED. ALL RESULTS ARE MEASURED ON SINGLE NVIDIA 2070 GPU. HD: HUMAN DETECTOR. HS: HUMAN SEGMENTOR. FD: FORGERY DETECTOR.Module \nHD \nHS \nFD \nFPS \n230 \n70 \n280 \n\n\n\nTABLE V CSI\nVMEASUREMENTS PER VIDEO FRAME. THE PERFORMANCE IMPROVE AS THE AMOUNT OF THE CSI MEASUREMENTS PER VIDEO FRAME INCREASES. THE RATIO HERE REFERS TO THE CSI MEASUREMENTS PER VIDEO FRAME.Ratio (m) \nAcc \nFPR \nTPR \n1 \n90.25% 10.11% 90.57% \n3 \n91.25% \n3.47% \n84.60% \n5 \n94.38% \n4.01% \n92.27% \n\n\n\nTABLE VI NUMBER\nVIOF THE INPUT VIDEO FRAMES. THE PERFORMANCE IMPROVE AS THE NUMBER OF THE INPUT VIDEO FRAMES INCREASESFrames (g) \nAcc \nFPR \nTPR \n3 \n87.17% 4.35% 79.08% \n5 \n92.01% 4.39% 87.50% \n7 \n94.38% 4.01% 92.27% \n\n\n\nPerson-in-WiFi: Fine-Grained Person Perception Using WiFi. Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, Dong Huang, Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. the 26th Annual International Conference on Mobile Computing and NetworkingFei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, and Dong Huang, Person-in-WiFi: Fine-Grained Person Perception Using WiFi, pp. 1-14, Proceedings of the 26th Annual International Conference on Mobile Computing and Networking, 2019.\n\nTowards cross-modal forgery detection and localization on live surveillance videos. Yong Huang, Xiang Li, Wei Wang, Tao Jiang, Qian Zhang, IEEE INFOCOM 2021-IEEE Conference on Computer Communications. IEEEYong Huang, Xiang Li, Wei Wang, Tao Jiang, and Qian Zhang, \"Towards cross-modal forgery detection and localization on live surveillance videos,\" in IEEE INFOCOM 2021-IEEE Conference on Computer Communications. IEEE, 2021, pp. 1-10.\n\nForgery attack detection in surveillance video streams using wi-fi channel state information. Yong Huang, Xiang Li, Wei Wang, Tao Jiang, Qian Zhang, IEEE Transactions on Wireless Communications. Yong Huang, Xiang Li, Wei Wang, Tao Jiang, and Qian Zhang, \"Forgery attack detection in surveillance video streams using wi-fi channel state information,\" IEEE Transactions on Wireless Communications, 2021.\n\nTowards 3d human pose construction using wifi. Wenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang Wang, Sen Lin, Chong Tian, Srinivasan Murali, Haochen Hu, Zhi Sun, Lu Su, Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. the 26th Annual International Conference on Mobile Computing and NetworkingNew York, NY, USA, 2020Association for Computing MachineryWenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang Wang, Sen Lin, Chong Tian, Srinivasan Murali, Haochen Hu, Zhi Sun, and Lu Su, \"Towards 3d human pose construction using wifi,\" in Proceedings of the 26th Annual International Conference on Mobile Computing and Networking, New York, NY, USA, 2020, pp. 1-14, Association for Computing Machinery.\n\nFrom signal to image: Capturing fine-grained human poses with commodity wi-fi. Lingchao Guo, Zhaoming Lu, Xiangming Wen, Shuang Zhou, Zijun Han, IEEE Communications Letters. 244Lingchao Guo, Zhaoming Lu, Xiangming Wen, Shuang Zhou, and Zijun Han, \"From signal to image: Capturing fine-grained human poses with commodity wi-fi,\" IEEE Communications Letters, vol. 24, no. 4, pp. 802-806, 2019.\n\nSee through walls with wifi!. Fadel Adib, Dina Katabi, Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM. the ACM SIGCOMM 2013 conference on SIGCOMMFadel Adib and Dina Katabi, \"See through walls with wifi!,\" in Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM, 2013, pp. 75-86.\n\nFeasibility and limits of wi-fi imaging. Donny Huang, Rajalakshmi Nandakumar, Shyamnath Gollakota, Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems. the 12th ACM Conference on Embedded Network Sensor SystemsDonny Huang, Rajalakshmi Nandakumar, and Shyamnath Gollakota, \"Feasibility and limits of wi-fi imaging,\" in Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems, 2014, pp. 266-279.\n\nTemporalfrequency attention-based human activity recognition using commercial wifi devices. Xiaolong Yang, Ruoyu Cao, Mu Zhou, Liangbo Xie, IEEE Access. 8Xiaolong Yang, Ruoyu Cao, Mu Zhou, and Liangbo Xie, \"Temporal- frequency attention-based human activity recognition using commercial wifi devices,\" IEEE Access, vol. 8, pp. 137758-137769, 2020.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, \"Unpaired image-to-image translation using cycle-consistent adversarial networks,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2223-2232.\n\nForgery detection for surveillance video. Min-Jeong Dai-Kyung Hyun, Seung-Jin Lee, Hae-Yeoun Ryu, Heung-Kyu Lee, Lee, The Era of Interactive Media. SpringerDai-Kyung Hyun, Min-Jeong Lee, Seung-Jin Ryu, Hae-Yeoun Lee, and Heung-Kyu Lee, \"Forgery detection for surveillance video,\" in The Era of Interactive Media, pp. 25-36. Springer, 2013.\n\nCnn spatiotemporal features and fusion for surveillance video forgery detection. Sondos Fadl, Qi Han, Qiong Li, Signal Processing: Image Communication. 90116066Sondos Fadl, Qi Han, and Qiong Li, \"Cnn spatiotemporal features and fusion for surveillance video forgery detection,\" Signal Processing: Image Communication, vol. 90, pp. 116066, 2021.\n\nSolo: Segmenting objects by locations. Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Lei Li, European Conference on Computer Vision. SpringerXinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li, \"Solo: Segmenting objects by locations,\" in European Conference on Computer Vision. Springer, 2020, pp. 649-665.\n\nKaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick, R-Cnn Mask, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick, Mask R- CNN, pp. 2961-2969, Proceedings of the IEEE international conference on computer vision, 2017.\n\nHybrid instance-aware temporal fusion for online video instance segmentation. Xiang Li, Jinglu Wang, Xiao Li, Yan Lu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022Xiang Li, Jinglu Wang, Xiao Li, and Yan Lu, \"Hybrid instance-aware temporal fusion for online video instance segmentation,\" in Proceedings of the AAAI Conference on Artificial Intelligence, 2022, vol. 36, pp. 1429-1437.\n\nVideo instance segmentation by instance flow assembly. Xiang Li, Jinglu Wang, Xiao Li, Yan Lu, IEEE Transactions on Multimedia. Xiang Li, Jinglu Wang, Xiao Li, and Yan Lu, \"Video instance segmen- tation by instance flow assembly,\" IEEE Transactions on Multimedia, 2022.\n\nOnline video instance segmentation via robust context fusion. Xiang Li, Jinglu Wang, Xiaohao Xu, Bhiksha Raj, Yan Lu, arXiv:2207.05580arXiv preprintXiang Li, Jinglu Wang, Xiaohao Xu, Bhiksha Raj, and Yan Lu, \"Online video instance segmentation via robust context fusion,\" arXiv preprint arXiv:2207.05580, 2022.\n\nR\u02c62vos: Robust referring video object segmentation via relational multimodal cycle consistency. Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, Bhiksha Raj, arXiv:2207.01203arXiv preprintXiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, and Bhiksha Raj, \"R\u02c62vos: Robust referring video object segmentation via relational multimodal cycle consistency,\" arXiv preprint arXiv:2207.01203, 2022.\n\nPanoramic video salient object detection with ambisonic audio guidance. Xiang Li, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang, Bhiksha Raj, arXiv:2211.14419arXiv preprintXiang Li, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang, and Bhiksha Raj, \"Panoramic video salient object detection with ambisonic audio guidance,\" arXiv preprint arXiv:2211.14419, 2022.\n\nDeep high-resolution representation learning for human pose estimation. Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKe Sun, Bin Xiao, Dong Liu, and Jingdong Wang, \"Deep high-resolution representation learning for human pose estimation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 5693-5703.\n\nA simple yet effective baseline for 3d human pose estimation. Julieta Martinez, Rayat Hossain, Javier Romero, James J Little, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJulieta Martinez, Rayat Hossain, Javier Romero, and James J Little, \"A simple yet effective baseline for 3d human pose estimation,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2640-2649.\n\nHuman action recognition in rgb-d videos using motion sequence information and deep learning. Earnest Paul Ijjina, Krishna Mohan Chalavadi, Pattern Recognition. 72Earnest Paul Ijjina and Krishna Mohan Chalavadi, \"Human action recognition in rgb-d videos using motion sequence information and deep learning,\" Pattern Recognition, vol. 72, pp. 504-516, 2017.\n\nModality distillation with multiple stream networks for action recognition. C Nuno, Pietro Garcia, Vittorio Morerio, Murino, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Nuno C Garcia, Pietro Morerio, and Vittorio Murino, \"Modality distillation with multiple stream networks for action recognition,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 103-118.\n\nMulti-modal rgb-depththermal human body segmentation. Cristina Palmero, Albert Clap\u00e9s, Chris Bahnsen, Andreas M\u00f8gelmose, B Thomas, Sergio Moeslund, Escalera, International Journal of Computer Vision. 1182Cristina Palmero, Albert Clap\u00e9s, Chris Bahnsen, Andreas M\u00f8gelmose, Thomas B Moeslund, and Sergio Escalera, \"Multi-modal rgb-depth- thermal human body segmentation,\" International Journal of Computer Vision, vol. 118, no. 2, pp. 217-239, 2016.\n\nImproving video segmentation by fusing depth cues and the visual background extractor (vibe) algorithm. Xiaoqin Zhou, Xiaofeng Liu, Aimin Jiang, Bin Yan, Chenguang Yang, Sensors. 1751177Xiaoqin Zhou, Xiaofeng Liu, Aimin Jiang, Bin Yan, and Chenguang Yang, \"Improving video segmentation by fusing depth cues and the visual background extractor (vibe) algorithm,\" Sensors, vol. 17, no. 5, pp. 1177, 2017.\n\nCapturing the human figure through a wall. Fadel Adib, Chen-Yu Hsu, Hongzi Mao, Dina Katabi, Fr\u00e9do Durand, ACM Transactions on Graphics (TOG). 346Fadel Adib, Chen-Yu Hsu, Hongzi Mao, Dina Katabi, and Fr\u00e9do Durand, \"Capturing the human figure through a wall,\" ACM Transactions on Graphics (TOG), vol. 34, no. 6, pp. 1-13, 2015.\n\nThrough-wall human pose estimation using radio signals. Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, Dina Katabi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina Katabi, \"Through-wall human pose estimation using radio signals,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7356-7365.\n\nRf-based 3d skeletons. Mingmin Zhao, Yonglong Tian, Hang Zhao, Mohammad Abu Alsheikh, Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, Antonio Torralba, Proceedings of the. theMingmin Zhao, Yonglong Tian, Hang Zhao, Mohammad Abu Alsheikh, Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, and Antonio Torralba, \"Rf-based 3d skeletons,\" in Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, 2018, pp. 267-281.\n\nActivitygan: Generative adversarial networks for data augmentation in sensor-based human activity recognition. Jinqi Xi&apos;ang Li, Rabih Luo, Younes, Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers. Xi'ang Li, Jinqi Luo, and Rabih Younes, \"Activitygan: Generative adversarial networks for data augmentation in sensor-based human activity recognition,\" in Adjunct Proceedings of the 2020 ACM Inter- national Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers, 2020, pp. 249-254.\n\nToward data augmentation and interpretation in sensor-based fine-grained hand activity recognition. Jinqi Luo, Xiang Li, Rabih Younes, International Workshop on Deep Learning for Human Activity Recognition. SpringerJinqi Luo, Xiang Li, and Rabih Younes, \"Toward data augmentation and interpretation in sensor-based fine-grained hand activity recognition,\" in International Workshop on Deep Learning for Human Activity Recog- nition. Springer, 2021, pp. 30-42.\n\nSequential and patch analyses for object removal video forgery detection and localization. Mohammed Aloraini, Mehdi Sharifzadeh, Dan Schonfeld, IEEE Transactions on Circuits and Systems for Video Technology. 31Mohammed Aloraini, Mehdi Sharifzadeh, and Dan Schonfeld, \"Sequen- tial and patch analyses for object removal video forgery detection and localization,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 3, pp. 917-930, 2021.\n\nSurfi: detecting surveillance camera looping attacks with wi-fi channel state information. Nitya Lakshmanan, Inkyu Bang, Min Suk Kang, Jun Han, Jong Taek Lee, Proceedings of the 12th Conference on Security and Privacy in Wireless and Mobile Networks. the 12th Conference on Security and Privacy in Wireless and Mobile NetworksNitya Lakshmanan, Inkyu Bang, Min Suk Kang, Jun Han, and Jong Taek Lee, \"Surfi: detecting surveillance camera looping attacks with wi-fi channel state information,\" in Proceedings of the 12th Conference on Security and Privacy in Wireless and Mobile Networks, 2019, pp. 239- 244.\n\nOutliers in process modeling and identification. Ronald K Pearson, IEEE Transactions on control systems technology. 101Ronald K Pearson, \"Outliers in process modeling and identification,\" IEEE Transactions on control systems technology, vol. 10, no. 1, pp. 55-63, 2002.\n\nLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Tijmen Tieleman, Geoffrey Hinton, 4COURSERA: Neural networks for machine learningTijmen Tieleman, Geoffrey Hinton, et al., \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,\" COURSERA: Neural networks for machine learning, vol. 4, no. 2, pp. 26-31, 2012.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n", "annotations": {"author": "[{\"end\":75,\"start\":62},{\"end\":177,\"start\":76},{\"end\":180,\"start\":178},{\"end\":271,\"start\":181},{\"end\":359,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":84,\"start\":82},{\"end\":193,\"start\":187}]", "author_first_name": "[{\"end\":69,\"start\":62},{\"end\":81,\"start\":76},{\"end\":179,\"start\":178},{\"end\":186,\"start\":181}]", "author_affiliation": "[{\"end\":176,\"start\":86},{\"end\":270,\"start\":195},{\"end\":358,\"start\":273}]", "title": "[{\"end\":59,\"start\":1},{\"end\":418,\"start\":360}]", "venue": null, "abstract": "[{\"end\":1521,\"start\":487}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1714,\"start\":1711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1855,\"start\":1852},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1884,\"start\":1881},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2147,\"start\":2144},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2152,\"start\":2149},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2157,\"start\":2154},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2240,\"start\":2237},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2245,\"start\":2242},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2250,\"start\":2247},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2473,\"start\":2470},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2555,\"start\":2552},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2676,\"start\":2673},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2962,\"start\":2959},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2967,\"start\":2964},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2972,\"start\":2969},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4403,\"start\":4399},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4409,\"start\":4405},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5434,\"start\":5431},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5533,\"start\":5529},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5539,\"start\":5535},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5704,\"start\":5700},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5710,\"start\":5706},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5732,\"start\":5728},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5738,\"start\":5734},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5769,\"start\":5765},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5775,\"start\":5771},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5904,\"start\":5900},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5910,\"start\":5906},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6095,\"start\":6091},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6262,\"start\":6258},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6274,\"start\":6270},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6453,\"start\":6450},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6458,\"start\":6455},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6613,\"start\":6610},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6727,\"start\":6724},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6822,\"start\":6819},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6918,\"start\":6914},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6924,\"start\":6920},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7246,\"start\":7242},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7252,\"start\":7248},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7448,\"start\":7444},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7546,\"start\":7542},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7732,\"start\":7729},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10697,\"start\":10693},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10740,\"start\":10737},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16072,\"start\":16069},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16455,\"start\":16452},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18237,\"start\":18233},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18424,\"start\":18420},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18633,\"start\":18629},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20545,\"start\":20542},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20551,\"start\":20547},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20557,\"start\":20553},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20563,\"start\":20559}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23696,\"start\":23265},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23765,\"start\":23697},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24219,\"start\":23766},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24365,\"start\":24220},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":24573,\"start\":24366},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":25016,\"start\":24574},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":25200,\"start\":25017},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":25500,\"start\":25201},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":25720,\"start\":25501}]", "paragraph": "[{\"end\":2378,\"start\":1540},{\"end\":3361,\"start\":2380},{\"end\":3873,\"start\":3363},{\"end\":5010,\"start\":3875},{\"end\":5435,\"start\":5012},{\"end\":5962,\"start\":5457},{\"end\":8500,\"start\":5964},{\"end\":8623,\"start\":8528},{\"end\":9520,\"start\":8646},{\"end\":11808,\"start\":9557},{\"end\":12154,\"start\":11890},{\"end\":12974,\"start\":12176},{\"end\":13165,\"start\":12976},{\"end\":13334,\"start\":13226},{\"end\":14388,\"start\":13357},{\"end\":15680,\"start\":14412},{\"end\":15889,\"start\":15682},{\"end\":16384,\"start\":15909},{\"end\":17929,\"start\":16411},{\"end\":18711,\"start\":17959},{\"end\":20389,\"start\":18731},{\"end\":21320,\"start\":20391},{\"end\":23003,\"start\":21342},{\"end\":23264,\"start\":23021}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11889,\"start\":11809},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13225,\"start\":13166}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16536,\"start\":16529},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19839,\"start\":19831},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20630,\"start\":20621},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21248,\"start\":21240},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21685,\"start\":21678},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22307,\"start\":22299}]", "section_header": "[{\"end\":1538,\"start\":1523},{\"end\":5455,\"start\":5438},{\"end\":8526,\"start\":8503},{\"end\":8644,\"start\":8626},{\"end\":9555,\"start\":9523},{\"end\":12174,\"start\":12157},{\"end\":13355,\"start\":13337},{\"end\":14410,\"start\":14391},{\"end\":15907,\"start\":15892},{\"end\":16409,\"start\":16387},{\"end\":17957,\"start\":17932},{\"end\":18729,\"start\":18714},{\"end\":21340,\"start\":21323},{\"end\":23019,\"start\":23006},{\"end\":23274,\"start\":23266},{\"end\":23706,\"start\":23698},{\"end\":23775,\"start\":23767},{\"end\":24238,\"start\":24221},{\"end\":24388,\"start\":24367},{\"end\":24595,\"start\":24575},{\"end\":25036,\"start\":25018},{\"end\":25213,\"start\":25202},{\"end\":25517,\"start\":25502}]", "table": "[{\"end\":24365,\"start\":24314},{\"end\":24573,\"start\":24479},{\"end\":25016,\"start\":24843},{\"end\":25200,\"start\":25160},{\"end\":25500,\"start\":25396},{\"end\":25720,\"start\":25620}]", "figure_caption": "[{\"end\":23696,\"start\":23276},{\"end\":23765,\"start\":23708},{\"end\":24219,\"start\":23777},{\"end\":24314,\"start\":24240},{\"end\":24479,\"start\":24391},{\"end\":24843,\"start\":24599},{\"end\":25160,\"start\":25039},{\"end\":25396,\"start\":25215},{\"end\":25620,\"start\":25520}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4280,\"start\":4274},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8793,\"start\":8787},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19105,\"start\":19099}]", "bib_author_first_name": "[{\"end\":25784,\"start\":25781},{\"end\":25798,\"start\":25791},{\"end\":25814,\"start\":25805},{\"end\":25829,\"start\":25822},{\"end\":25839,\"start\":25835},{\"end\":26340,\"start\":26336},{\"end\":26353,\"start\":26348},{\"end\":26361,\"start\":26358},{\"end\":26371,\"start\":26368},{\"end\":26383,\"start\":26379},{\"end\":26788,\"start\":26784},{\"end\":26801,\"start\":26796},{\"end\":26809,\"start\":26806},{\"end\":26819,\"start\":26816},{\"end\":26831,\"start\":26827},{\"end\":27146,\"start\":27140},{\"end\":27161,\"start\":27154},{\"end\":27175,\"start\":27167},{\"end\":27189,\"start\":27182},{\"end\":27199,\"start\":27196},{\"end\":27210,\"start\":27205},{\"end\":27227,\"start\":27217},{\"end\":27243,\"start\":27236},{\"end\":27251,\"start\":27248},{\"end\":27259,\"start\":27257},{\"end\":27921,\"start\":27913},{\"end\":27935,\"start\":27927},{\"end\":27949,\"start\":27940},{\"end\":27961,\"start\":27955},{\"end\":27973,\"start\":27968},{\"end\":28262,\"start\":28257},{\"end\":28273,\"start\":28269},{\"end\":28569,\"start\":28564},{\"end\":28588,\"start\":28577},{\"end\":28610,\"start\":28601},{\"end\":29058,\"start\":29050},{\"end\":29070,\"start\":29065},{\"end\":29078,\"start\":29076},{\"end\":29092,\"start\":29085},{\"end\":29395,\"start\":29388},{\"end\":29408,\"start\":29401},{\"end\":29422,\"start\":29415},{\"end\":29436,\"start\":29430},{\"end\":29438,\"start\":29437},{\"end\":29857,\"start\":29848},{\"end\":29883,\"start\":29874},{\"end\":29898,\"start\":29889},{\"end\":29913,\"start\":29904},{\"end\":30234,\"start\":30228},{\"end\":30243,\"start\":30241},{\"end\":30254,\"start\":30249},{\"end\":30539,\"start\":30532},{\"end\":30549,\"start\":30546},{\"end\":30563,\"start\":30556},{\"end\":30576,\"start\":30570},{\"end\":30587,\"start\":30584},{\"end\":30825,\"start\":30818},{\"end\":30837,\"start\":30830},{\"end\":30853,\"start\":30848},{\"end\":30866,\"start\":30862},{\"end\":30882,\"start\":30877},{\"end\":31260,\"start\":31255},{\"end\":31271,\"start\":31265},{\"end\":31282,\"start\":31278},{\"end\":31290,\"start\":31287},{\"end\":31689,\"start\":31684},{\"end\":31700,\"start\":31694},{\"end\":31711,\"start\":31707},{\"end\":31719,\"start\":31716},{\"end\":31967,\"start\":31962},{\"end\":31978,\"start\":31972},{\"end\":31992,\"start\":31985},{\"end\":32004,\"start\":31997},{\"end\":32013,\"start\":32010},{\"end\":32313,\"start\":32308},{\"end\":32324,\"start\":32318},{\"end\":32338,\"start\":32331},{\"end\":32347,\"start\":32343},{\"end\":32355,\"start\":32352},{\"end\":32367,\"start\":32360},{\"end\":32687,\"start\":32682},{\"end\":32699,\"start\":32692},{\"end\":32711,\"start\":32705},{\"end\":32724,\"start\":32718},{\"end\":32731,\"start\":32729},{\"end\":32746,\"start\":32739},{\"end\":33044,\"start\":33042},{\"end\":33053,\"start\":33050},{\"end\":33064,\"start\":33060},{\"end\":33078,\"start\":33070},{\"end\":33532,\"start\":33525},{\"end\":33548,\"start\":33543},{\"end\":33564,\"start\":33558},{\"end\":33578,\"start\":33573},{\"end\":33580,\"start\":33579},{\"end\":34037,\"start\":34030},{\"end\":34058,\"start\":34051},{\"end\":34064,\"start\":34059},{\"end\":34371,\"start\":34370},{\"end\":34384,\"start\":34378},{\"end\":34401,\"start\":34393},{\"end\":34815,\"start\":34807},{\"end\":34831,\"start\":34825},{\"end\":34845,\"start\":34840},{\"end\":34862,\"start\":34855},{\"end\":34875,\"start\":34874},{\"end\":34890,\"start\":34884},{\"end\":35312,\"start\":35305},{\"end\":35327,\"start\":35319},{\"end\":35338,\"start\":35333},{\"end\":35349,\"start\":35346},{\"end\":35364,\"start\":35355},{\"end\":35653,\"start\":35648},{\"end\":35667,\"start\":35660},{\"end\":35679,\"start\":35673},{\"end\":35689,\"start\":35685},{\"end\":35703,\"start\":35698},{\"end\":35996,\"start\":35989},{\"end\":36011,\"start\":36003},{\"end\":36024,\"start\":36016},{\"end\":36047,\"start\":36039},{\"end\":36058,\"start\":36054},{\"end\":36072,\"start\":36065},{\"end\":36087,\"start\":36083},{\"end\":36540,\"start\":36533},{\"end\":36555,\"start\":36547},{\"end\":36566,\"start\":36562},{\"end\":36581,\"start\":36573},{\"end\":36604,\"start\":36596},{\"end\":36614,\"start\":36609},{\"end\":36631,\"start\":36624},{\"end\":36645,\"start\":36641},{\"end\":36661,\"start\":36654},{\"end\":37094,\"start\":37089},{\"end\":37116,\"start\":37111},{\"end\":37781,\"start\":37776},{\"end\":37792,\"start\":37787},{\"end\":37802,\"start\":37797},{\"end\":38236,\"start\":38228},{\"end\":38252,\"start\":38247},{\"end\":38269,\"start\":38266},{\"end\":38695,\"start\":38690},{\"end\":38713,\"start\":38708},{\"end\":38723,\"start\":38720},{\"end\":38727,\"start\":38724},{\"end\":38737,\"start\":38734},{\"end\":38752,\"start\":38743},{\"end\":39570,\"start\":39564},{\"end\":39589,\"start\":39581},{\"end\":39902,\"start\":39901},{\"end\":39918,\"start\":39913}]", "bib_author_last_name": "[{\"end\":25789,\"start\":25785},{\"end\":25803,\"start\":25799},{\"end\":25820,\"start\":25815},{\"end\":25833,\"start\":25830},{\"end\":25845,\"start\":25840},{\"end\":26346,\"start\":26341},{\"end\":26356,\"start\":26354},{\"end\":26366,\"start\":26362},{\"end\":26377,\"start\":26372},{\"end\":26389,\"start\":26384},{\"end\":26794,\"start\":26789},{\"end\":26804,\"start\":26802},{\"end\":26814,\"start\":26810},{\"end\":26825,\"start\":26820},{\"end\":26837,\"start\":26832},{\"end\":27152,\"start\":27147},{\"end\":27165,\"start\":27162},{\"end\":27180,\"start\":27176},{\"end\":27194,\"start\":27190},{\"end\":27203,\"start\":27200},{\"end\":27215,\"start\":27211},{\"end\":27234,\"start\":27228},{\"end\":27246,\"start\":27244},{\"end\":27255,\"start\":27252},{\"end\":27262,\"start\":27260},{\"end\":27925,\"start\":27922},{\"end\":27938,\"start\":27936},{\"end\":27953,\"start\":27950},{\"end\":27966,\"start\":27962},{\"end\":27977,\"start\":27974},{\"end\":28267,\"start\":28263},{\"end\":28280,\"start\":28274},{\"end\":28575,\"start\":28570},{\"end\":28599,\"start\":28589},{\"end\":28620,\"start\":28611},{\"end\":29063,\"start\":29059},{\"end\":29074,\"start\":29071},{\"end\":29083,\"start\":29079},{\"end\":29096,\"start\":29093},{\"end\":29399,\"start\":29396},{\"end\":29413,\"start\":29409},{\"end\":29428,\"start\":29423},{\"end\":29444,\"start\":29439},{\"end\":29872,\"start\":29858},{\"end\":29887,\"start\":29884},{\"end\":29902,\"start\":29899},{\"end\":29917,\"start\":29914},{\"end\":29922,\"start\":29919},{\"end\":30239,\"start\":30235},{\"end\":30247,\"start\":30244},{\"end\":30257,\"start\":30255},{\"end\":30544,\"start\":30540},{\"end\":30554,\"start\":30550},{\"end\":30568,\"start\":30564},{\"end\":30582,\"start\":30577},{\"end\":30590,\"start\":30588},{\"end\":30828,\"start\":30826},{\"end\":30846,\"start\":30838},{\"end\":30860,\"start\":30854},{\"end\":30875,\"start\":30867},{\"end\":30887,\"start\":30883},{\"end\":31263,\"start\":31261},{\"end\":31276,\"start\":31272},{\"end\":31285,\"start\":31283},{\"end\":31293,\"start\":31291},{\"end\":31692,\"start\":31690},{\"end\":31705,\"start\":31701},{\"end\":31714,\"start\":31712},{\"end\":31722,\"start\":31720},{\"end\":31970,\"start\":31968},{\"end\":31983,\"start\":31979},{\"end\":31995,\"start\":31993},{\"end\":32008,\"start\":32005},{\"end\":32016,\"start\":32014},{\"end\":32316,\"start\":32314},{\"end\":32329,\"start\":32325},{\"end\":32341,\"start\":32339},{\"end\":32350,\"start\":32348},{\"end\":32358,\"start\":32356},{\"end\":32371,\"start\":32368},{\"end\":32690,\"start\":32688},{\"end\":32703,\"start\":32700},{\"end\":32716,\"start\":32712},{\"end\":32727,\"start\":32725},{\"end\":32737,\"start\":32732},{\"end\":32750,\"start\":32747},{\"end\":33048,\"start\":33045},{\"end\":33058,\"start\":33054},{\"end\":33068,\"start\":33065},{\"end\":33083,\"start\":33079},{\"end\":33541,\"start\":33533},{\"end\":33556,\"start\":33549},{\"end\":33571,\"start\":33565},{\"end\":33587,\"start\":33581},{\"end\":34049,\"start\":34038},{\"end\":34074,\"start\":34065},{\"end\":34376,\"start\":34372},{\"end\":34391,\"start\":34385},{\"end\":34409,\"start\":34402},{\"end\":34417,\"start\":34411},{\"end\":34823,\"start\":34816},{\"end\":34838,\"start\":34832},{\"end\":34853,\"start\":34846},{\"end\":34872,\"start\":34863},{\"end\":34882,\"start\":34876},{\"end\":34899,\"start\":34891},{\"end\":34909,\"start\":34901},{\"end\":35317,\"start\":35313},{\"end\":35331,\"start\":35328},{\"end\":35344,\"start\":35339},{\"end\":35353,\"start\":35350},{\"end\":35369,\"start\":35365},{\"end\":35658,\"start\":35654},{\"end\":35671,\"start\":35668},{\"end\":35683,\"start\":35680},{\"end\":35696,\"start\":35690},{\"end\":35710,\"start\":35704},{\"end\":36001,\"start\":35997},{\"end\":36014,\"start\":36012},{\"end\":36037,\"start\":36025},{\"end\":36052,\"start\":36048},{\"end\":36063,\"start\":36059},{\"end\":36081,\"start\":36073},{\"end\":36094,\"start\":36088},{\"end\":36545,\"start\":36541},{\"end\":36560,\"start\":36556},{\"end\":36571,\"start\":36567},{\"end\":36594,\"start\":36582},{\"end\":36607,\"start\":36605},{\"end\":36622,\"start\":36615},{\"end\":36639,\"start\":36632},{\"end\":36652,\"start\":36646},{\"end\":36670,\"start\":36662},{\"end\":37109,\"start\":37095},{\"end\":37120,\"start\":37117},{\"end\":37128,\"start\":37122},{\"end\":37785,\"start\":37782},{\"end\":37795,\"start\":37793},{\"end\":37809,\"start\":37803},{\"end\":38245,\"start\":38237},{\"end\":38264,\"start\":38253},{\"end\":38279,\"start\":38270},{\"end\":38706,\"start\":38696},{\"end\":38718,\"start\":38714},{\"end\":38732,\"start\":38728},{\"end\":38741,\"start\":38738},{\"end\":38756,\"start\":38753},{\"end\":39271,\"start\":39255},{\"end\":39579,\"start\":39571},{\"end\":39596,\"start\":39590},{\"end\":39911,\"start\":39903},{\"end\":39925,\"start\":39919},{\"end\":39929,\"start\":39927}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":90262453},\"end\":26250,\"start\":25722},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":230438799},\"end\":26688,\"start\":26252},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":244668963},\"end\":27091,\"start\":26690},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":214796512},\"end\":27832,\"start\":27093},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":214509413},\"end\":28225,\"start\":27834},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":207204597},\"end\":28521,\"start\":28227},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17287510},\"end\":28956,\"start\":28523},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":221087521},\"end\":29305,\"start\":28958},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":195944196},\"end\":29804,\"start\":29307},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":55522467},\"end\":30145,\"start\":29806},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":227257198},\"end\":30491,\"start\":30147},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209140680},\"end\":30816,\"start\":30493},{\"attributes\":{\"id\":\"b12\"},\"end\":31175,\"start\":30818},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":244896293},\"end\":31627,\"start\":31177},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":239049806},\"end\":31898,\"start\":31629},{\"attributes\":{\"doi\":\"arXiv:2207.05580\",\"id\":\"b15\"},\"end\":32210,\"start\":31900},{\"attributes\":{\"doi\":\"arXiv:2207.01203\",\"id\":\"b16\"},\"end\":32608,\"start\":32212},{\"attributes\":{\"doi\":\"arXiv:2211.14419\",\"id\":\"b17\"},\"end\":32968,\"start\":32610},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":67856425},\"end\":33461,\"start\":32970},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206771080},\"end\":33934,\"start\":33463},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13141636},\"end\":34292,\"start\":33936},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49315565},\"end\":34751,\"start\":34294},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2124800},\"end\":35199,\"start\":34753},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2716271},\"end\":35603,\"start\":35201},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4535542},\"end\":35931,\"start\":35605},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":47020925},\"end\":36508,\"start\":35933},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":51927460},\"end\":36976,\"start\":36510},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":221638472},\"end\":37674,\"start\":36978},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":234152834},\"end\":38135,\"start\":37676},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":218932192},\"end\":38597,\"start\":38137},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":91183927},\"end\":39204,\"start\":38599},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":27359698},\"end\":39475,\"start\":39206},{\"attributes\":{\"id\":\"b32\"},\"end\":39855,\"start\":39477},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b33\"},\"end\":40075,\"start\":39857}]", "bib_title": "[{\"end\":25779,\"start\":25722},{\"end\":26334,\"start\":26252},{\"end\":26782,\"start\":26690},{\"end\":27138,\"start\":27093},{\"end\":27911,\"start\":27834},{\"end\":28255,\"start\":28227},{\"end\":28562,\"start\":28523},{\"end\":29048,\"start\":28958},{\"end\":29386,\"start\":29307},{\"end\":29846,\"start\":29806},{\"end\":30226,\"start\":30147},{\"end\":30530,\"start\":30493},{\"end\":31253,\"start\":31177},{\"end\":31682,\"start\":31629},{\"end\":33040,\"start\":32970},{\"end\":33523,\"start\":33463},{\"end\":34028,\"start\":33936},{\"end\":34368,\"start\":34294},{\"end\":34805,\"start\":34753},{\"end\":35303,\"start\":35201},{\"end\":35646,\"start\":35605},{\"end\":35987,\"start\":35933},{\"end\":36531,\"start\":36510},{\"end\":37087,\"start\":36978},{\"end\":37774,\"start\":37676},{\"end\":38226,\"start\":38137},{\"end\":38688,\"start\":38599},{\"end\":39253,\"start\":39206}]", "bib_author": "[{\"end\":25791,\"start\":25781},{\"end\":25805,\"start\":25791},{\"end\":25822,\"start\":25805},{\"end\":25835,\"start\":25822},{\"end\":25847,\"start\":25835},{\"end\":26348,\"start\":26336},{\"end\":26358,\"start\":26348},{\"end\":26368,\"start\":26358},{\"end\":26379,\"start\":26368},{\"end\":26391,\"start\":26379},{\"end\":26796,\"start\":26784},{\"end\":26806,\"start\":26796},{\"end\":26816,\"start\":26806},{\"end\":26827,\"start\":26816},{\"end\":26839,\"start\":26827},{\"end\":27154,\"start\":27140},{\"end\":27167,\"start\":27154},{\"end\":27182,\"start\":27167},{\"end\":27196,\"start\":27182},{\"end\":27205,\"start\":27196},{\"end\":27217,\"start\":27205},{\"end\":27236,\"start\":27217},{\"end\":27248,\"start\":27236},{\"end\":27257,\"start\":27248},{\"end\":27264,\"start\":27257},{\"end\":27927,\"start\":27913},{\"end\":27940,\"start\":27927},{\"end\":27955,\"start\":27940},{\"end\":27968,\"start\":27955},{\"end\":27979,\"start\":27968},{\"end\":28269,\"start\":28257},{\"end\":28282,\"start\":28269},{\"end\":28577,\"start\":28564},{\"end\":28601,\"start\":28577},{\"end\":28622,\"start\":28601},{\"end\":29065,\"start\":29050},{\"end\":29076,\"start\":29065},{\"end\":29085,\"start\":29076},{\"end\":29098,\"start\":29085},{\"end\":29401,\"start\":29388},{\"end\":29415,\"start\":29401},{\"end\":29430,\"start\":29415},{\"end\":29446,\"start\":29430},{\"end\":29874,\"start\":29848},{\"end\":29889,\"start\":29874},{\"end\":29904,\"start\":29889},{\"end\":29919,\"start\":29904},{\"end\":29924,\"start\":29919},{\"end\":30241,\"start\":30228},{\"end\":30249,\"start\":30241},{\"end\":30259,\"start\":30249},{\"end\":30546,\"start\":30532},{\"end\":30556,\"start\":30546},{\"end\":30570,\"start\":30556},{\"end\":30584,\"start\":30570},{\"end\":30592,\"start\":30584},{\"end\":30830,\"start\":30818},{\"end\":30848,\"start\":30830},{\"end\":30862,\"start\":30848},{\"end\":30877,\"start\":30862},{\"end\":30889,\"start\":30877},{\"end\":31265,\"start\":31255},{\"end\":31278,\"start\":31265},{\"end\":31287,\"start\":31278},{\"end\":31295,\"start\":31287},{\"end\":31694,\"start\":31684},{\"end\":31707,\"start\":31694},{\"end\":31716,\"start\":31707},{\"end\":31724,\"start\":31716},{\"end\":31972,\"start\":31962},{\"end\":31985,\"start\":31972},{\"end\":31997,\"start\":31985},{\"end\":32010,\"start\":31997},{\"end\":32018,\"start\":32010},{\"end\":32318,\"start\":32308},{\"end\":32331,\"start\":32318},{\"end\":32343,\"start\":32331},{\"end\":32352,\"start\":32343},{\"end\":32360,\"start\":32352},{\"end\":32373,\"start\":32360},{\"end\":32692,\"start\":32682},{\"end\":32705,\"start\":32692},{\"end\":32718,\"start\":32705},{\"end\":32729,\"start\":32718},{\"end\":32739,\"start\":32729},{\"end\":32752,\"start\":32739},{\"end\":33050,\"start\":33042},{\"end\":33060,\"start\":33050},{\"end\":33070,\"start\":33060},{\"end\":33085,\"start\":33070},{\"end\":33543,\"start\":33525},{\"end\":33558,\"start\":33543},{\"end\":33573,\"start\":33558},{\"end\":33589,\"start\":33573},{\"end\":34051,\"start\":34030},{\"end\":34076,\"start\":34051},{\"end\":34378,\"start\":34370},{\"end\":34393,\"start\":34378},{\"end\":34411,\"start\":34393},{\"end\":34419,\"start\":34411},{\"end\":34825,\"start\":34807},{\"end\":34840,\"start\":34825},{\"end\":34855,\"start\":34840},{\"end\":34874,\"start\":34855},{\"end\":34884,\"start\":34874},{\"end\":34901,\"start\":34884},{\"end\":34911,\"start\":34901},{\"end\":35319,\"start\":35305},{\"end\":35333,\"start\":35319},{\"end\":35346,\"start\":35333},{\"end\":35355,\"start\":35346},{\"end\":35371,\"start\":35355},{\"end\":35660,\"start\":35648},{\"end\":35673,\"start\":35660},{\"end\":35685,\"start\":35673},{\"end\":35698,\"start\":35685},{\"end\":35712,\"start\":35698},{\"end\":36003,\"start\":35989},{\"end\":36016,\"start\":36003},{\"end\":36039,\"start\":36016},{\"end\":36054,\"start\":36039},{\"end\":36065,\"start\":36054},{\"end\":36083,\"start\":36065},{\"end\":36096,\"start\":36083},{\"end\":36547,\"start\":36533},{\"end\":36562,\"start\":36547},{\"end\":36573,\"start\":36562},{\"end\":36596,\"start\":36573},{\"end\":36609,\"start\":36596},{\"end\":36624,\"start\":36609},{\"end\":36641,\"start\":36624},{\"end\":36654,\"start\":36641},{\"end\":36672,\"start\":36654},{\"end\":37111,\"start\":37089},{\"end\":37122,\"start\":37111},{\"end\":37130,\"start\":37122},{\"end\":37787,\"start\":37776},{\"end\":37797,\"start\":37787},{\"end\":37811,\"start\":37797},{\"end\":38247,\"start\":38228},{\"end\":38266,\"start\":38247},{\"end\":38281,\"start\":38266},{\"end\":38708,\"start\":38690},{\"end\":38720,\"start\":38708},{\"end\":38734,\"start\":38720},{\"end\":38743,\"start\":38734},{\"end\":38758,\"start\":38743},{\"end\":39273,\"start\":39255},{\"end\":39581,\"start\":39564},{\"end\":39598,\"start\":39581},{\"end\":39913,\"start\":39901},{\"end\":39927,\"start\":39913},{\"end\":39931,\"start\":39927}]", "bib_venue": "[{\"end\":26014,\"start\":25939},{\"end\":27454,\"start\":27356},{\"end\":28383,\"start\":28341},{\"end\":28755,\"start\":28697},{\"end\":29567,\"start\":29515},{\"end\":31010,\"start\":30958},{\"end\":31404,\"start\":31358},{\"end\":33234,\"start\":33168},{\"end\":33710,\"start\":33658},{\"end\":34534,\"start\":34485},{\"end\":36237,\"start\":36175},{\"end\":36695,\"start\":36692},{\"end\":38925,\"start\":38850},{\"end\":25937,\"start\":25847},{\"end\":26451,\"start\":26391},{\"end\":26883,\"start\":26839},{\"end\":27354,\"start\":27264},{\"end\":28006,\"start\":27979},{\"end\":28339,\"start\":28282},{\"end\":28695,\"start\":28622},{\"end\":29109,\"start\":29098},{\"end\":29513,\"start\":29446},{\"end\":29952,\"start\":29924},{\"end\":30297,\"start\":30259},{\"end\":30630,\"start\":30592},{\"end\":30956,\"start\":30889},{\"end\":31356,\"start\":31295},{\"end\":31755,\"start\":31724},{\"end\":31960,\"start\":31900},{\"end\":32306,\"start\":32212},{\"end\":32680,\"start\":32610},{\"end\":33166,\"start\":33085},{\"end\":33656,\"start\":33589},{\"end\":34095,\"start\":34076},{\"end\":34483,\"start\":34419},{\"end\":34951,\"start\":34911},{\"end\":35378,\"start\":35371},{\"end\":35746,\"start\":35712},{\"end\":36173,\"start\":36096},{\"end\":36690,\"start\":36672},{\"end\":37312,\"start\":37130},{\"end\":37881,\"start\":37811},{\"end\":38343,\"start\":38281},{\"end\":38848,\"start\":38758},{\"end\":39320,\"start\":39273},{\"end\":39562,\"start\":39477},{\"end\":39899,\"start\":39857}]"}}}, "year": 2023, "month": 12, "day": 17}