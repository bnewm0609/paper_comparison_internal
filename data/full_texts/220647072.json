{"id": 220647072, "updated": "2023-10-06 13:01:50.152", "metadata": {"title": "Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search", "authors": "[{\"first\":\"Yuan\",\"last\":\"Tian\",\"middle\":[]},{\"first\":\"Qin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhiwu\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Wen\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dengxin\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Minghao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Olga\",\"last\":\"Fink\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 7, "day": 17}, "abstract": "In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.09180", "mag": "3108420015", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/TianWHLDY0F20", "doi": "10.1007/978-3-030-58571-6_11"}}, "content": {"source": {"pdf_hash": "78ba6127fabb7056afc6f97924852bdd8b653b71", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.09180v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "https://ink.library.smu.edu.sg/sis_research/6258", "status": "GREEN"}}, "grobid": {"id": "aa8b95441008e94f0286a9741cec52e5c2b884d5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/78ba6127fabb7056afc6f97924852bdd8b653b71.txt", "contents": "\nOff-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search\n\n\nYuan Tian \nETH Z\u00fcrich\n\n\nQin Wang \nETH Z\u00fcrich\n\n\nZhiwu Huang \nETH Z\u00fcrich\n\n\nWen Li \nUESTC\n\n\nDengxin Dai dai@vision.ee.ethz.ch \nETH Z\u00fcrich\n\n\nMinghao Yang \nNavinfo Europe\n\n\nJun Wang \nUniversity College London\n\n\nOlga Fink ofink@ethz.chzhiwu.huang \nETH Z\u00fcrich\n\n\nOff-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search\nNeural Architecture SearchGenerative Adversarial Net- worksReinforcement LearningMarkov Decision ProcessOff-policy\nIn this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search.The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.\n\nIntroduction\n\nGenerative adversarial networks (GANs) have been successfully applied to a wide range of generation tasks, including image generation [12,3,1,53,15], text to image synthesis [44,62,40] and image translation [25,7], to name a few. To further improve the generation quality, several extensions and further developments have been proposed, ranging from regularization terms [4,14], progressive training strategy [26], utilizing attention mechanism [59,61], and to new architectures [27,3].\n\nWhile designing favorable neural architectures of GANs has made great success, it typically requires a large amount of time, effort, and domain expertise. For instance, several state-of-the-art GANs [27,3] design appreciably complex generator or discriminator backbones for better generating high-resolution images. To alleviate the network engineering pain, an efficient automated architecture searching framework for GAN is highly needed. On the other hand, Neural architecture search (NAS) has been applied and proved effective in discriminative tasks such as image classification [30] and segmentation [33]. Encouraged by this, AGAN [52] and AutoGAN [11] have introduced neural architecture search methods for GAN based on reinforcement learning (RL), thereby enabling a significant speedup of architecture searching process.\n\nSimilar to the other architecture search tasks (image classification, image segmentation), recently proposed RL-based GAN architecture search method AGAN [52] optimized the entire architecture. Since the same policy might sample different architectures, it is likely to suffer from noisy gradients and a high variance, which potentially further harms the policy update stability. To circumvent this issue, multi-level architecture search (MLAS) has been used in Auto-GAN [52], and a progressive optimization formulation is used. However, because optimization is based on the best performance of the current architecture level, this progressive formulation potentially leads to a local minimum solution.\n\nTo overcome these drawbacks, we reformulate the GAN architecture search problem as a Markov decision process (MDP). The new formulation is partly inspired by the human-designed Progressive GAN [26], which has shown to improve generation quality progressively in intermediate outputs of each architecture cell. In our new formulation, a sequence of decisions will be made during the entire architecture design process, which allows state-based sampling and thus alleviates the variance. In addition, as we will show later in the paper, by using a carefully designed reward, this new formulation also allows us to target effective global optimization over the entire architecture.\n\nMore importantly, the MDP formulation can better facilitate off-policy RL training to improve data efficiency. The previously proposed RL-based GAN architecture search methods [11,52] are based on on-policy RL, leading to limited data efficiency that results in considerably long training time. Specifically, on-policy RL approach generally requires frequent sampling of a batch of architectures generated by current policy to update the policy. Moreover, new samples are required to be collected for each gradient step, while the previous batches are directly disposed. This quickly becomes very expensive as the number of gradient steps and samples increases with the complexity of the task, especially in the architecture search tasks. by comparison, off-policy reinforcement learning algorithms make use of past experience such that the RL agents are enabled to learn more efficiently. This has been proven to be effective in other RL tasks, including legged locomotion [32] and complex video games [38]. However, exploiting off-policy data for GAN architecture search poses new challenges. Training the policy network inevitably becomes unstable by using offpolicy data, because these training samples are systematically different from the on-policy ones. This presents a great challenge to the stability and convergence of the algorithm [2]. Our proposed MDP formulation can make a difference here.\n\nBy allowing state-based sampling, the new formulation alleviates this instability, and better supports the off-policy strategy.\n\nThe contributions of this paper are two-fold:\n\n1. We reformulate the problem of neural architecture search for GAN as an MDP for smoother architecture sampling, which enables a more effective RL-based search algorithm and potentially more global optimization. 2. We propose an efficient and effective off-policy RL NAS framework for GAN architecture search (E 2 GAN), which is 6 times faster than existing RL-based GAN search approaches with competitive performance.\n\nWe conduct a variety of experiments to validate the effectiveness of E 2 GAN. Our discovered architectures yield better results compared to RL-based competitors. E 2 GAN is efficient, as it is able to to find a highly competitive model within 7 GPU hours.\n\n\nRelated Work\n\nReinforcement Learning Recent progress in model-free reinforcement learning (RL) [49] has fostered promising results in many interesting tasks ranging from gaming [37,48], to planning and control problems [23,31,58,18,6,19] and even up to the AutoML [65,41,34]. However, model-free deep RL methods are notoriously expensive in terms of their sample complexity. One reason of the poor sample efficiency is the use of on-policy reinforcement learning algorithms, such as trust region policy optimization (TRPO) [46], proximal policy optimization(PPO) [47] or REINFORCE [56]. On-policy learning algorithms require new samples generated by the current policy for each gradient step. On the contrary, off-policy algorithms aim to reuse past experience. Recent developments of the off-policy reinforcement learning algorithms, such as soft Actor-Critic (SAC) [17], have demonstrated substantial improvements in both performance and sample efficiency in previous on-policy methods.\n\nNeural architecture search Neural architecture search methods aim to automatically search for a good neural architecture for various tasks, such as image classification [30] and segmentation [33], in order to ease the burden of handcrafted design of dedicated architectures for specific tasks. Several approaches have been proposed to tackle the NAS problem. Zoph and Le [65] proposed a reinforcement learning-based method that trains an RNN controller to design the neural network [65]. Guo et al. [16] exploited a novel graph convolutional neural networks for policy learning in reinforcement learning. Further successfully introduced approaches include evolutionary algorithm based methods [43], differentiable methods [35] and one-shot learning methods [5,35]. Early works of RL-based NAS algorithms [57,41,65,34] proposed to optimize the entire trajectory (i.e., the entire neural architecture). To the best of our knowledge, most of the previously proposed RL-based NAS algorithms used on-policy RL algorithms, such as REINFORCE or PPO, except [63] which uses Q-learning algorithm for NAS, which is a value-based method and only supports discrete state space problems. For on-policy algorithms, since each update requires new data collected by the current policy and the reward is based on the internal neural network architecture training, the on-policy training of RL-based NAS algorithms inevitably becomes computationally expensive.\n\nGAN Architecture Search Due to the specificities of GAN and their challenges, such as instability and mode collapse, the NAS approaches proposed for discriminative models cannot be directly transferred to the architecture search of GANs. Only recently, few approaches have been introduced tackling the specific challenges of the GAN architectures. Recently, AutoGAN has introduced a neural architecture search for GANs based on reinforcement learning (RL), thereby enabling a significant speedup of the process of architecture selection [11]. The AutoGAN algorithm is based on on-policy reinforcement learning. The proposed multi-level architecture search (MLAS) aims at progressively finding well-performing GAN architectures and completes the task in around 2 GPU days. Similarly, AGAN [52] uses reinforcement learning for generative architecture search in a larger search space. The computational cost for AGAN is comparably very expensive (1200 GPU days). In addition, AdversarialNAS [10] and DEGAS [9] adopted a different approach, i.e., differentiable searching strategy [35], for the GAN architecture search problem.\n\n\nPreliminary\n\nIn this section, we briefly review the basic concepts and notations used in the following sections.\n\n\nGenerative Adversarial Networks\n\nThe training of GANs involves an adversarial competition between two players, a generator and a discriminator. The generator aims at generating realistic-looking images to 'fool' its opponent. Meanwhile, the discriminator aims to distinguish whether an image is real or fake. This can be formulated as a min-max optimization problem:\nmin G max D E x\u223cp real [log D(x)] + E z\u223cpz [log (1 \u2212 D(G(z)))],(1)\nwhere G and D are the generator and discriminator parametrized by neural networks. z is sampled from random noise. x are the real and G(z) are the generated images.\n\n\nReinforcement Learning\n\nA Markov decision process (MDP) is a discrete-time stochastic control process. At each time step, the process is in some state s, and its associated decisionmaker chooses an available action a. Given the action, the process moves into a new state s at the next step, and the agent receives a reward.\n\nAn MDP could be described as a tuple (S, A, r, P, \u03c1), where S is the set of states that is able to precisely describe the current situation, A is the set of actions, r(s, a) is the reward function, P (s |s, a) is the transition probability function, and \u03c1(s) is the initial state distribution.\n\nMDPs can be particularly useful for solving optimization problems via reinforcement learning. In a general reinforcement learning setup, an agent is trained to interact with the environment and get a reward from its interaction. The goal is to find a policy \u03c0 that maximizes the cumulative reward J(\u03c0):\nJ(\u03c0) = E \u03c4 \u223c\u03c1\u03c0 \u221e t=0 r(s t , a t )(2)\nWhile the standard RL merely maximizes the expected cumulative rewards, the maximum entropy RL framework considers a more general objective [64], which favors stochastic policies. This objective shows a strong connection to the exploration-exploitation trade-off, and could also prevent the policy from getting stuck in local optima. Formally, it is given by\nJ(\u03c0) = E \u03c4 \u223c\u03c1\u03c0 \u221e t=0 [r(s t , a t ) + \u03b2H(\u03c0(\u00b7|s t ))],(3)\nwhere \u03b2 is the temperature parameter that controls the stochasticity of the optimal policy.\n\n\nProblem Formulation\n\n\nMotivation\n\nGiven a fixed search space, GAN architecture search agents aim to discover an optimal network architecture on a given generation task. Existing RL methods update the policy network by using batches of entire architectures sampled from the current policy. Even though these data samples are only used for the current update step, the sampled GAN architectures nevertheless require tedious training and evaluation processes. The sampling efficiency is therefore very low resulting in limited learning progress of the agents. Moreover, the entire architecture sampling leads to a high variance, which might influence the stability of the policy update.\n\nThe key motivation of the proposed methodology is to stabilize and accelerate the learning process by step-wise sampling instead of entire-trajectory-based sampling and making efficient use of past experiences from previous policies. To achieve this, we propose to formulate the GAN architecture search problem as an MDP and solve it by off-policy reinforcement learning.\n\n\nGAN Architecture Search formulated as MDP\n\nWe propose to formulate the GAN architecture search problem as a Markov decision process (MDP) which enables state-based sampling. It further boosts the learning process and overcomes the potential challenge of a large variance stemming from sampling entire architectures that makes it inherently difficult to train a policy using off-policy data.\n\nFormulating GAN architecture search problem as an MDP provides a mathematical description of architecture search processes. An MDP can be described as a tuple (S, A, r, P, \u03c1), where S is the set of states that is able to precisely describe the current architecture (such as the current cell number, the structure or the performance of the architectures), A is the set of actions that defines the architecture design of the next cell, r(s, a) is the reward function used to define how good the architecture is, P (s |s, a) is the transition probability function indicating the training process, and \u03c1(s) is the initial architecture. We define a cell as an architecture block we are using to search in one step. The design details of states, actions, and rewards is discussed in Section 5.\n\nIt is important to highlight that the formulation proposed in this paper has two main differences compared to previous RL methods for neural architecture search. Firstly, it is essentially different to the classic RL approaches for NAS [65], which formulate the task as an optimization problem over the entire trajectory/architecture. Instead, the MDP formulation proposed here enables us to do the optimization based on the disentangled steps of cell design. Secondly, it is also different to the progressive formulation used by AutoGAN [11], where the optimization is based on the best performance of the current architecture level and can potentially lead to a local minimum solution. Instead, the proposed formulation enables us to potentially conduct a more global optimization using cumulative reward without the burden of calculating the reward over the full trajectory at once. It is important to point out that the multi-level optimization formulation used in AutoGAN [11] does not have this property.\n\n\nOff-policy RL for GAN Architecture Search\n\nIn this section, we integrate off-policy RL in the GAN architecture search by making use of the newly proposed MDP formulation. We introduce several innovations to address the challenges of an off-policy learning setup.\n\nThe MDP formulation of GAN architecture search enables us to use off-policy reinforcement learning for a step-wise optimization of the entire search process to maximize the cumulative reward.\n\n\nRL for GAN Architecture Search\n\nBefore we move on to the off-policy solver, we need to design the state, reward, and action to meet the requirements of both the GAN architecture design, as well as of the MDP formulation.\n\nState MDP requires a state representation that can precisely represent the current network up to the current step. Most importantly, this state needs to be stable during training to avoid adding more variance to the training of the policy network. The stability requirement is particularly relevant since the policy network relies on it to design the next cell. The design of the state is one of the main challenges we face when adopting off-policy RL to GAN architecture search.\n\nInspired by the progressive GAN [26], which has shown to improve generation quality in intermediate RGB outputs of each architecture cell, we propose a progressive state representation for GAN architecture search. Specifically, given a fixed batch of input noise, we adopt the average output of each cell as the progressive state representation. We down-sample this representation to impose a constant size across different cells. Note that there are alternative ways to encode the network information. For example, one could also deploy another network to encode the previous layers. However, we find the proposed design efficient and also effective.\n\nIn addition to the progressive state representation, we also use network performance (Inception Score / FID) and layer number to provide more information about the state. To summarize, the designed state s includes the depth, performance of the current architecture, and the progressive state representation.\n\nAction Given the current state, which encodes the information about previous layers, the policy network decides on the next action. The action describes the architecture of one cell. For example, if we follow the search space used by AutoGAN [11], action will contain skip options, upsampling operations, shortcut options, different types of convolution blocks, and the normalization option, as shown in Figure 2.\n\nThis can then be defined as a = [conv, norm, upsample, shortcut, skip]. The action output by the agent will be carried out by a softmax classifier decoding into an operation. To demonstrate the effectiveness of our off-policy methods and enable a fair comparison, in all of our experiments, we use the same search space as AutoGAN [11], which means we search for generator cells, and the discriminator architecture is pre-designed and growing as the generator becomes deeper. More details on the search space are discussed in Section 6.\n\nReward We propose to design the reward function as the performance improvement after adding the new cell. In this work, we use both Inception Score (IS) and Frchet Inception Distance (FID) as the indicators of the network performance. Since IS score is progressive (the higher the better) and FID score is degressive (the lower the better), the proposed reward function can be formulated as:\nR t (s, a) = IS(t) \u2212 IS(t \u2212 1) + \u03b1(F ID(t \u2212 1) \u2212 F ID(t)),(4)\nwhere \u03b1 is a factor to balance the trade-off between the two indicators. We use \u03b1 = 0.01 in our main experiments. The motivation behind using a combined reward is based on an empirical finding indicating that IS and FID are not always consistent with each other and can lead to a biased choice of architectures. A detailed discussion about the choice of indicators is provided in Section 7. By employing the performance improvement in each step instead of only using performance as proposed in [11], RL can maximize the expected sum of rewards over the entire trajectory. This enables us to target the potential global optimal structure with the highest reward:\nJ(\u03c0) = t=0 E (st,at)\u223cp(\u03c0) R(s t , a t ) = E architecture\u223cp(\u03c0) IS f inal \u2212 \u03b1F ID f inal , (5)\nwhere IS f inal and F ID f inal are the final scores of the entire architecture.\n\n\nOff-policy RL Solver\n\nThe proposed designs of state, reward, and action fulfill the criteria of MDPs and makes it possible to stabilize the training using off-policy samples. We are now free to choose any off-policy RL solver to improve data efficiency.\n\nIn this paper, we apply the off-the-shelf soft actor-critic algorithm (SAC) [17], an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework, as the learning algorithm. It has demonstrated to be 10 to 100 times more data-efficient compared to any other on-policy algorithms on traditional RL tasks. In SAC, the actor aims at maximizing expected reward while also maximizing entropy. This increases training stability significantly and improves the exploration during training.\n\nFor the learning of the critic, the objective function is defined as:\nJ(Q) = E (s,a)\u223cD 1 2 (Q(s, a) \u2212 Q target (s, a)) 2 (6)\nwhere Q target is the approximation target for Q :\nQ target (s, a) = Q(s, a) + \u03b3Q target (s , f ( , s ))(7)\nThe objective function of the the policy network is given by:\nJ(\u03c0) = E D [\u03b2[log(\u03c0 \u03b8 (f \u03b8 ( , s)|s))] \u2212 Q(s, f \u03b8 ( , s))](8)\nwhere \u03c0 \u03b8 is parameterized by a neural network f \u03b8 , is an input vector consisting of Gaussian noise, and the D . = {(s, a, s , r)} is the replay buffer for storing the MDP tuples [38]. \u03b2 is a positive Lagrange multiplier that controls the relative importance of the policy entropy versus the safety constraint.\n\n\nImplementation of E 2 GAN\n\nIn this section, we present the implementation details of the proposed off-policy RL framework E 2 GAN. The training process is briefly outlined in Algorithm 1.\n\nAgent Training Since we reformulated the NAS as a multi-step MDP, our agent will make several decisions in any trajectory \u03c4 = [(s 1 , a 1 ), ...(s n , a n )]. In each step, the agent will collect this experience [s t , a t , r t , s t+1 ] in the memory buffer D. Once the threshold of the smallest memory length is reached, the agent is updated using the Adam [28] optimizer via the objective function presented in Eq. 8 by sampling a batch of data from the memory buffer D in an off-policy way.\n\nThe entire search comprises two periods: the exploration period and the exploitation period. During the exploration period, the agent will sample any possible architecture. While in the exploitation period, the agent will choose the best architecture, in order to quickly stabilize the policy.\n\nThe exploration period lasts for 70% of iterations, and the exploitation takes 30% iterations. Once the memory threshold is reached, for every exploration step, the policy will be updated once. For every exploitation step, the policy will be updated 10 times in order to converge quickly.\n\n\nAlgorithm 1 Pseudo code for E 2 GAN search\n\nInput hyperparameters, learning rates \u03b1 \u03c6 Q ,\u03b1 \u03b8 Randomly initialize a Q network Q(s, a) and policy network \u03c0(a|s) with parameters \u03c6Q, \u03b8 and the Lagrange multipliers \u03b2, Initialize the parameters of target networks with \u03c6 Q \u2190 \u03c6Q, \u03b8 \u2190 \u03b8 for each iteration do\n\nReset the weight and cells of E 2 GAN for each time step do if Exploration then Sample at from \u03c0(s), add the corresponding cell to E 2 GAN else if Exploitation then\n\nChoose the best at from \u03c0(s) and add the corresponding cell to E 2 GAN end if Progressively train the E 2 GAN Observe st+1, rt and store (st, at, rt, st+1) in D end for for each update step do Sample mini-batches of transitions from D and update Q and \u03c0 with gradients Update the target networks with soft replacement:\n\u03c6 Q \u2190 \u03c4 \u03c6Q + (1 \u2212 \u03c4 )\u03c6 Q \u03b8 \u2190 \u03c4 \u03b8 + (1 \u2212 \u03c4 )\u03b8\n\nend for end for\n\nProxy Task We use a progressive proxy task in order to collect the rewards fast. When a new cell is added, we train the current full trajectory for one epoch and calculate the reward for the current cell. Within a trajectory, the previous cells' weights will be kept and trained together with the new cell. In order to accurately estimate the Q-value of each state-action pair, we reset the weight of the neural network after finishing the entire architecture trajectory design.\n\n\nExperiments\n\n\nDataset\n\nIn this paper, we use the CIFAR-10 dataset [29] to evaluate the effectiveness and efficiency of the proposed E 2 GAN framework. The CIFAR-10 dataset consists of 50,000 training images and 10,000 test images with a 32 \u00d7 32 resolution. We use its training set without any data augmentation technique to search for the architecture with the highest cumulative return for a GAN generator. Furthermore, to evaluate the transferability of the discovered architecture, we also adopt the STL-10 dataset [8] to train the network without any other data augmentation to make a fair comparison to previous works.\n\n\nSearch Space\n\nTo verify the effectiveness of the off-policy framework and to enable a fair comparison, we use the same search space as used in the AutoGAN experiments [11]. There are five control variables: 1)Skip operation, which is a binary value indicating whether the current cell takes a skip connection from any specific cell as its input. Note that each cell could take multiple skip connections from other preceding cells. 2)Pre-activation [21] and post-activation convolution block. 3)Three types of normalization operations, including batch normalization [24], instance normalization [51], and no normalization. 4)Upsampling operation which is standard in current image generation GAN, including bi-linear upsampling, nearest neighbor upsampling, and stride-2 deconvolution. 5)Shortcut operation.\n\n\nResults\n\nThe generator architecture discovered by E 2 GAN on the CIFAR-10 training set is displayed in Figure 3. For the task of unconditional CIFAR-10 image generation (no class labels used), several notable observations could be summarized:\n\n\nMethods\n\nInception Score FID Search Cost (GPU days) DCGAN [42] 6.64 \u00b1 .14 - *  [52] 8.29 \u00b1 .09 30.5 1200 AutoGAN-top1 [11] 8.55 \u00b1 .10 12.42 2 AutoGAN-top2 [11] 8.42 \u00b1 .07 13.67 2 AutoGAN-top3 [11] 8  Table 1. Inception score and FID score of unconditional image generation task on CIFAR-10. We achieve a highly competitive FID of 11.26 compared to published works. We mainly compare our approach with RL-based NAS approaches: AGAN [52] and AutoGAN [11]. Architectures marked by ( * ) are manually designed. * E 2 GAN prefers post-activation convolution block to pre-activation convolution blocks. This finding is contrary to AutoGAN's preference, but coincides with previous experiences from human experts. * E 2 GAN prefers the use of batch normalization. This finding is also contrary to AutoGAN's choice, but is in line with experts' common practice. * E 2 GAN prefers bi-linear upsample to nearest neighbour upsample. This in theory provides finer upsample ability between different cells.\n\nOur E 2 GAN framework only takes about 0.3 GPU day for searching while the AGAN spends 1200 GPU days and AutoGAN spends 2 GPU days.\n\nWe train the discovered E 2 GAN from scratch for 500 epochs and summarize the IS and FID scores in Table 1. On the CIFAR-10 dataset, our model achieves a highly competitive FID 11.26 compared to published results by Auto-GAN [11], and hand-crafted GAN [42,45,60,55,20,14,13,36,22,54]. In terms of IS score, E 2 GAN is also highly competitive to AutoGAN [11]. We additionally report the performance of the top2 and top3 architectures discovered in one search. Both have higher performance than the respective AutoGAN counterparts.\n\nWe also test the transferability of E 2 GAN. We retrain the weights of the discovered E 2 GAN architecture using the STL-10 training and unlabeled set for the  Table 2. Inception score and FID score for the unconditional image generation task on STL-10. E 2 GAN uses the discovered architecture on CIFAR-10. Performance is significantly better than other RL-based competitors. unconditional image generation task. E 2 GAN achieves a highly-competitive performance on both IS (9.51) and FID (25.35), as shown in Table 2. Because our main contribution is the new formulation and using off-policy RL for GAN architecture framework, we compare the proposed method directly with existing RL-based algorithms. We use the exact same searching space as Au-toGAN, which does not include the search for a discriminator. As GAN training is an interactive procedure between generator and discriminator, one might expect better performance if the search is conducted on both networks. We report our scores using the exact same evaluation procedure provided by the authors of AutoGAN. The reported scores are based on the best models achieved during training on a 20 epoch evaluation interval. Mean and standard deviation of the IS score are calculated based on the 10-fold evaluation on 50,000 generated images. We additionally report the performance curve against training steps of E 2 GAN and AutoGAN for three runs in the supplementary material.\n\n\nDiscussion\n\n\nReward Choice: IS and FID\n\nIS and FID scores are two main evaluation metrics for GAN. We conduct the ablation study of using different combinations. Specifically, IS only (\u03b1 = 0) and\n\n\nMethods\n\nInception Score FID Search Cost (GPU days) AutoGAN-top1 [11] 8.55 \u00b1 .1 12.42 2 E 2 GAN(IS and FID as reward)\n\n8.51 \u00b1 .13 11.26 0.3 E 2 GAN(IS only as reward)\n\n8.81 \u00b1 .11 15.64 0.1 Table 3. Performance on the unconditional image generation task for CIFAR-10 with different reward choices. the combination of IS and FID (\u03b1 = 0.01) as the reward. Our agent successfully discovered two different architectures. When only IS is used as the reward signal, the agent discovered a different architecture using only 0.1 GPU day. The searched architecture achieved state-of-the-art IS score of 8.86, as shown in Table 3., but a relatively plain FID score of 15.78. This demonstrates the effectiveness of the proposed method, as we are encouraging the agent to find the architecture with a higher IS score. Interestingly, this special architecture shows that, at least in certain edge cases, the IS score and FID score may not always have a strong positive correlation. This finding motivates us to additionally use FID as part of the reward. When both IS and FID are used as the reward signal, the discovered architecture performs well in term of both metrics. This combined reward takes 0.3 GPU days (compared to 0.1 GPU days of IS only optimization) because of the relatively expensive cost of FID computation.\n\n\nReproducibility\n\nWe train our agent over 3 different seeds. As shown in Figure 5, we observe that our agent steadily converged the policy in the exploitation period. E 2 GAN can find similar architectures with relatively good performance on the proxy task.\n\n\nConclusion\n\nIn this work, we proposed a novel off-policy reinforcement learning method, E 2 GAN, to efficiently and effectively search for GAN architectures. We reformulated the problem as an MDP process, and overcame the challenges of using off-policy data. We first introduced a new progressive state representation. We additionally introduced a new reward function, which allowed us to target the potential global optimization in our MDP formulation. The E 2 GAN achieves state-of-the-art efficiency in GAN architecture searching, and the discovered architecture shows highly competitive performance compared to other state-of-theart methods. In future work, we plan to simultaneously optimize the generator and discriminator architectures in a multi-agent context.\n\nFig. 1 .\n1Overview of the proposed E 2 GAN: the off-policy reinforcement learning module for GAN architecture search. The entire process comprises five steps: 1)The agent observes the current state st, which is designed as s=[Depth, Performance, Progressive state representation]. 2)The agent makes a decision at on how to design the cell added to previous cells according to the state information. at includes the skip options, upsampling operations, shortcut options, different types of convolution blocks and a normalization block 3)Progressively train the new architecture, obtain the reward rt and the new state st+1 information and then loop over it again. 4)Save the off-policy memory tuple [st, at, rt, st+1] into the memory buffer. 5)Sample a batch of data from the memory buffer to update the policy network.\n\nFig. 2 .\n2The search space of a generator cell in one step. The search space is directly taken from AutoGAN[11].\n\nFig. 3 .\n3The generator architecture discovered by E 2 GAN on CIFAR-10.\n\nFig. 4 .\n4The generated CIFAR-10(left) and STL-10(right) results of E 2 GAN which are randomly sampled without cherry-picking.\n\nFig. 5 .\n5Training curves on architecture searching. IS score on the proxy task against training time steps. E 2 GAN shows relatively good stability and reproducibility.\nAcknowledgementThe contributions of Yuan Tian, Qin Wang, and Olga Fink were funded by the Swiss National Science Foundation (SNSF) Grant no. PP00P2 176878.\nCvae-gan: fine-grained image generation through asymmetric training. J Bao, D Chen, F Wen, H Li, G Hua, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionBao, J., Chen, D., Wen, F., Li, H., Hua, G.: Cvae-gan: fine-grained image generation through asymmetric training. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2745-2754 (2017)\n\nConvergent temporal-difference learning with arbitrary smooth function approximation. S Bhatnagar, D Precup, D Silver, R S Sutton, H R Maei, C Szepesv\u00e1ri, Advances in Neural Information Processing Systems. Bhatnagar, S., Precup, D., Silver, D., Sutton, R.S., Maei, H.R., Szepesv\u00e1ri, C.: Convergent temporal-difference learning with arbitrary smooth function approx- imation. In: Advances in Neural Information Processing Systems. pp. 1204-1212 (2009)\n\nLarge scale gan training for high fidelity natural image synthesis. A Brock, J Donahue, K Simonyan, arXiv:1809.11096arXiv preprintBrock, A., Donahue, J., Simonyan, K.: Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018)\n\nA Brock, T Lim, J M Ritchie, N Weston, arXiv:1609.07093Neural photo editing with introspective adversarial networks. arXiv preprintBrock, A., Lim, T., Ritchie, J.M., Weston, N.: Neural photo editing with intro- spective adversarial networks. arXiv preprint arXiv:1609.07093 (2016)\n\nSmash: one-shot model architecture search through hypernetworks. A Brock, T Lim, J M Ritchie, N Weston, arXiv:1708.05344arXiv preprintBrock, A., Lim, T., Ritchie, J.M., Weston, N.: Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344 (2017)\n\nM A Chao, Y Tian, C Kulkarni, K Goebel, O Fink, arXiv:2006.04001Real-time model calibration with deep reinforcement learning. arXiv preprintChao, M.A., Tian, Y., Kulkarni, C., Goebel, K., Fink, O.: Real-time model cali- bration with deep reinforcement learning. arXiv preprint arXiv:2006.04001 (2020)\n\nStargan: Unified generative adversarial networks for multi-domain image-to-image translation. Y Choi, M Choi, M Kim, J W Ha, S Kim, J Choo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChoi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Unified gener- ative adversarial networks for multi-domain image-to-image translation. In: Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8789-8797 (2018)\n\nAn analysis of single-layer networks in unsupervised feature learning. A Coates, A Ng, H Lee, Proceedings of the fourteenth International Conference on Artificial Intelligence and Statistics. the fourteenth International Conference on Artificial Intelligence and StatisticsCoates, A., Ng, A., Lee, H.: An analysis of single-layer networks in unsupervised feature learning. In: Proceedings of the fourteenth International Conference on Artificial Intelligence and Statistics. pp. 215-223 (2011)\n\nS Doveh, R Giryes, arXiv:1912.00606Degas: Differentiable efficient generator search. arXiv preprintDoveh, S., Giryes, R.: Degas: Differentiable efficient generator search. arXiv preprint arXiv:1912.00606 (2019)\n\nC Gao, Y Chen, S Liu, Z Tan, S Yan, arXiv:1912.02037Adversarialnas: Adversarial neural architecture search for gans. arXiv preprintGao, C., Chen, Y., Liu, S., Tan, Z., Yan, S.: Adversarialnas: Adversarial neural architecture search for gans. arXiv preprint arXiv:1912.02037 (2019)\n\nAutogan: Neural architecture search for generative adversarial networks. X Gong, S Chang, Y Jiang, Z Wang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGong, X., Chang, S., Jiang, Y., Wang, Z.: Autogan: Neural architecture search for generative adversarial networks. In: Proceedings of the IEEE International Con- ference on Computer Vision. pp. 3224-3234 (2019)\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in Neural Information Processing Systems. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in Neural Information Processing Systems. pp. 2672-2680 (2014)\n\nG L Grinblat, L C Uzal, P M Granitto, arXiv:1709.07359Class-splitting generative adversarial networks. arXiv preprintGrinblat, G.L., Uzal, L.C., Granitto, P.M.: Class-splitting generative adversarial networks. arXiv preprint arXiv:1709.07359 (2017)\n\nImproved training of wasserstein gans. I Gulrajani, F Ahmed, M Arjovsky, V Dumoulin, A C Courville, Advances in Neural Information Processing Systems. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein gans. In: Advances in Neural Information Processing Sys- tems. pp. 5767-5777 (2017)\n\nAuto-embedding generative adversarial networks for high resolution image synthesis. Y Guo, Q Chen, J Chen, Q Wu, Q Shi, M Tan, IEEE Transactions on Multimedia. 2111Guo, Y., Chen, Q., Chen, J., Wu, Q., Shi, Q., Tan, M.: Auto-embedding generative adversarial networks for high resolution image synthesis. IEEE Transactions on Multimedia 21(11), 2726-2737 (2019)\n\nNat: Neural architecture transformer for accurate and compact architectures. Y Guo, Y Zheng, M Tan, Q Chen, J Chen, P Zhao, J Huang, Advances in Neural Information Processing Systems. Guo, Y., Zheng, Y., Tan, M., Chen, Q., Chen, J., Zhao, P., Huang, J.: Nat: Neural architecture transformer for accurate and compact architectures. In: Advances in Neural Information Processing Systems. pp. 737-748 (2019)\n\nSoft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, arXiv:1801.01290arXiv preprintHaarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft actor-critic: Off-policy maxi- mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290 (2018)\n\nH infinity model-free reinforcement learning with robust stability guarantee. M Han, Y Tian, L Zhang, J Wang, W Pan, arXiv:1911.02875arXiv preprintHan, M., Tian, Y., Zhang, L., Wang, J., Pan, W.: H infinity model-free reinforce- ment learning with robust stability guarantee. arXiv preprint arXiv:1911.02875 (2019)\n\nActor-critic reinforcement learning for control with stability guarantee. M Han, L Zhang, J Wang, W Pan, arXiv:2004.14288arXiv preprintHan, M., Zhang, L., Wang, J., Pan, W.: Actor-critic reinforcement learning for control with stability guarantee. arXiv preprint arXiv:2004.14288 (2020)\n\nH He, H Wang, G H Lee, Y Tian, Probgan: Towards probabilistic gan with theoretical guarantees. He, H., Wang, H., Lee, G.H., Tian, Y.: Probgan: Towards probabilistic gan with theoretical guarantees (2019)\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J Sun, European conference on computer vision. SpringerHe, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In: European conference on computer vision. pp. 630-645. Springer (2016)\n\nMgan: Training generative adversarial nets with multiple generators. Q Hoang, T D Nguyen, T Le, D Phung, Hoang, Q., Nguyen, T.D., Le, T., Phung, D.: Mgan: Training generative adversarial nets with multiple generators (2018)\n\nLearning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 4265872Hwangbo, J., Lee, J., Dosovitskiy, A., Bellicoso, D., Tsounis, V., Koltun, V., Hut- ter, M.: Learning agile and dynamic motor skills for legged robots. Science Robotics 4(26), eaau5872 (2019)\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintIoffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\n\nImage-to-image translation with conditional adversarial networks. P Isola, J Y Zhu, T Zhou, A A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIsola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi- tional adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1125-1134 (2017)\n\nProgressive growing of gans for improved quality, stability, and variation. T Karras, T Aila, S Laine, J Lehtinen, arXiv:1710.10196arXiv preprintKarras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im- proved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017)\n\nA style-based generator architecture for generative adversarial networks. T Karras, S Laine, T Aila, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKarras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4401-4410 (2019)\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\nA Krizhevsky, G Hinton, Learning multiple layers of features from tiny images. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: Advances in Neural Information Processing Sys- tems. pp. 1097-1105 (2012)\n\nLearning dexterous manipulation policies from experience and imitation. V Kumar, A Gupta, E Todorov, S Levine, arXiv:1611.05095arXiv preprintKumar, V., Gupta, A., Todorov, E., Levine, S.: Learning dexterous manipulation policies from experience and imitation. arXiv preprint arXiv:1611.05095 (2016)\n\nT P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D.: Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015)\n\nAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation. C Liu, L C Chen, F Schroff, H Adam, W Hua, A L Yuille, L Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLiu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille, A.L., Fei-Fei, L.: Auto-deeplab: Hierarchical neural architecture search for semantic image segmen- tation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 82-92 (2019)\n\nProgressive neural architecture search. C Liu, B Zoph, M Neumann, J Shlens, W Hua, L J Li, L Fei-Fei, A Yuille, J Huang, K Murphy, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Liu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L.J., Fei-Fei, L., Yuille, A., Huang, J., Murphy, K.: Progressive neural architecture search. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 19-34 (2018)\n\nH Liu, K Simonyan, Y Yang, DARTS: Differentiable architecture search. In: International Conference on Learning Representations. Liu, H., Simonyan, K., Yang, Y.: DARTS: Differentiable architecture search. In: In- ternational Conference on Learning Representations (2019), https://openreview. net/forum?id=S1eYHoC5FX\n\nT Miyato, T Kataoka, M Koyama, Y Yoshida, arXiv:1802.05957Spectral normalization for generative adversarial networks. arXiv preprintMiyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)\n\nV Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602Playing atari with deep reinforcement learning. arXiv preprintMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.: Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013)\n\nHuman-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level control through deep reinforcement learning. Nature 518(7540), 529-533 (2015)\n\nDual discriminator generative adversarial nets. T Nguyen, T Le, H Vu, D Phung, Advances in Neural Information Processing Systems. Nguyen, T., Le, T., Vu, H., Phung, D.: Dual discriminator generative adversarial nets. In: Advances in Neural Information Processing Systems. pp. 2670-2680 (2017)\n\nSemantic image synthesis with spatially-adaptive normalization. T Park, M Y Liu, T C Wang, J Y Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPark, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatially-adaptive normalization. In: Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition. pp. 2337-2346 (2019)\n\nH Pham, M Y Guan, B Zoph, Q V Le, J Dean, arXiv:1802.03268Efficient neural architecture search via parameter sharing. arXiv preprintPham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268 (2018)\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintRadford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 (2015)\n\nLarge-scale evolution of image classifiers. E Real, S Moore, A Selle, S Saxena, Y L Suematsu, J Tan, Q V Le, A Kurakin, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku- rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 2902-2911. JMLR. org (2017)\n\nGenerative adversarial text to image synthesis. S Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, International Conference on Machine Learning. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative adver- sarial text to image synthesis. In: International Conference on Machine Learning. pp. 1060-1069 (2016)\n\nImproved techniques for training gans. T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, Advances in Neural Information Processing Systems. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Im- proved techniques for training gans. In: Advances in Neural Information Processing Systems. pp. 2234-2242 (2016)\n\nTrust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, International Conference on Machine Learning. Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust region policy optimization. In: International Conference on Machine Learning. pp. 1889-1897 (2015)\n\nJ Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintSchulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)\n\nD Silver, G Lever, N Heess, T Degris, D Wierstra, M Riedmiller, Deterministic policy gradient algorithms. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M.: Deter- ministic policy gradient algorithms (2014)\n\nReinforcement learning is direct adaptive optimal control. R S Sutton, A G Barto, R J Williams, IEEE Control Systems Magazine. 122Sutton, R.S., Barto, A.G., Williams, R.J.: Reinforcement learning is direct adaptive optimal control. IEEE Control Systems Magazine 12(2), 19-22 (1992)\n\nDist-gan: An improved gan using distance constraints. N T Tran, T A Bui, N M Cheung, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Tran, N.T., Bui, T.A., Cheung, N.M.: Dist-gan: An improved gan using distance constraints. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 370-385 (2018)\n\nInstance normalization: The missing ingredient for fast stylization. D Ulyanov, A Vedaldi, V Lempitsky, arXiv:1607.08022arXiv preprintUlyanov, D., Vedaldi, A., Lempitsky, V.: Instance normalization: The missing in- gredient for fast stylization. arXiv preprint arXiv:1607.08022 (2016)\n\nAgan: Towards automated design of generative adversarial networks. H Wang, J Huan, arXiv:1906.11080arXiv preprintWang, H., Huan, J.: Agan: Towards automated design of generative adversarial networks. arXiv preprint arXiv:1906.11080 (2019)\n\nHighresolution image synthesis and semantic manipulation with conditional gans. T C Wang, M Y Liu, J Y Zhu, A Tao, J Kautz, B Catanzaro, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High- resolution image synthesis and semantic manipulation with conditional gans. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8798-8807 (2018)\n\nImproving MMD-GAN training with repulsive loss function. W Wang, Y Sun, S Halgamuge, International Conference on Learning Representations. Wang, W., Sun, Y., Halgamuge, S.: Improving MMD-GAN training with repulsive loss function. In: International Conference on Learning Representations (2019), https://openreview.net/forum?id=HygjqjR9Km\n\nImproving generative adversarial networks with denoising feature matching. D Warde-Farley, Y Bengio, Warde-Farley, D., Bengio, Y.: Improving generative adversarial networks with de- noising feature matching (2016)\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4Williams, R.J.: Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8(3-4), 229-256 (1992)\n\nS Xie, H Zheng, C Liu, L Lin, arXiv:1812.09926Snas: stochastic neural architecture search. arXiv preprintXie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. arXiv preprint arXiv:1812.09926 (2018)\n\nIterative reinforcement learning based design of dynamic locomotion skills for cassie. Z Xie, P Clary, J Dao, P Morais, J Hurst, M Van De Panne, arXiv:1903.09537arXiv preprintXie, Z., Clary, P., Dao, J., Morais, P., Hurst, J., van de Panne, M.: Iterative reinforcement learning based design of dynamic locomotion skills for cassie. arXiv preprint arXiv:1903.09537 (2019)\n\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks. T Xu, P Zhang, Q Huang, H Zhang, Z Gan, X Huang, X He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan: Fine-grained text to image generation with attentional generative adversarial net- works. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1316-1324 (2018)\n\nLr-gan: Layered recursive generative adversarial networks for image generation. J Yang, A Kannan, D Batra, D Parikh, arXiv:1703.01560arXiv preprintYang, J., Kannan, A., Batra, D., Parikh, D.: Lr-gan: Layered recursive generative adversarial networks for image generation. arXiv preprint arXiv:1703.01560 (2017)\n\nSelf-attention generative adversarial networks. H Zhang, I Goodfellow, D Metaxas, A Odena, International Conference on Machine Learning. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative adver- sarial networks. In: International Conference on Machine Learning. pp. 7354-7363 (2019)\n\nStackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. H Zhang, T Xu, H Li, S Zhang, X Wang, X Huang, D N Metaxas, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionZhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N.: Stack- gan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: Proceedings of the IEEE international conference on computer vi- sion. pp. 5907-5915 (2017)\n\nPractical block-wise neural network architecture generation. Z Zhong, J Yan, W Wu, J Shao, C L Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhong, Z., Yan, J., Wu, W., Shao, J., Liu, C.L.: Practical block-wise neural network architecture generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2423-2432 (2018)\n\nModeling purposeful adaptive behavior with the principle of maximum causal entropy. B D Ziebart, Ziebart, B.D.: Modeling purposeful adaptive behavior with the principle of maxi- mum causal entropy (2010)\n\nB Zoph, Q V Le, Neural architecture search with reinforcement learning. International Conference on Learning Representations. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. Inter- national Conference on Learning Representations (2017)\n", "annotations": {"author": "[{\"end\":112,\"start\":89},{\"end\":135,\"start\":113},{\"end\":161,\"start\":136},{\"end\":177,\"start\":162},{\"end\":225,\"start\":178},{\"end\":256,\"start\":226},{\"end\":294,\"start\":257},{\"end\":343,\"start\":295}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":94},{\"end\":121,\"start\":117},{\"end\":147,\"start\":142},{\"end\":168,\"start\":166},{\"end\":189,\"start\":186},{\"end\":238,\"start\":234},{\"end\":265,\"start\":261},{\"end\":304,\"start\":300}]", "author_first_name": "[{\"end\":93,\"start\":89},{\"end\":116,\"start\":113},{\"end\":141,\"start\":136},{\"end\":165,\"start\":162},{\"end\":185,\"start\":178},{\"end\":233,\"start\":226},{\"end\":260,\"start\":257},{\"end\":299,\"start\":295}]", "author_affiliation": "[{\"end\":111,\"start\":100},{\"end\":134,\"start\":123},{\"end\":160,\"start\":149},{\"end\":176,\"start\":170},{\"end\":224,\"start\":213},{\"end\":255,\"start\":240},{\"end\":293,\"start\":267},{\"end\":342,\"start\":331}]", "title": "[{\"end\":86,\"start\":1},{\"end\":429,\"start\":344}]", "venue": null, "abstract": "[{\"end\":1482,\"start\":545}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1636,\"start\":1632},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1638,\"start\":1636},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1640,\"start\":1638},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":1643,\"start\":1640},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1646,\"start\":1643},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1676,\"start\":1672},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":1679,\"start\":1676},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1682,\"start\":1679},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1709,\"start\":1705},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1711,\"start\":1709},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1872,\"start\":1869},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1875,\"start\":1872},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1911,\"start\":1907},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":1947,\"start\":1943},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":1950,\"start\":1947},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1981,\"start\":1977},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1983,\"start\":1981},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2189,\"start\":2185},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2191,\"start\":2189},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2574,\"start\":2570},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2596,\"start\":2592},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2627,\"start\":2623},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2644,\"start\":2640},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2975,\"start\":2971},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3292,\"start\":3288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3718,\"start\":3714},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4381,\"start\":4377},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4384,\"start\":4381},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5179,\"start\":5175},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5208,\"start\":5204},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5547,\"start\":5544},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6561,\"start\":6557},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6643,\"start\":6639},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6646,\"start\":6643},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6685,\"start\":6681},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6688,\"start\":6685},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6691,\"start\":6688},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6694,\"start\":6691},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6696,\"start\":6694},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6699,\"start\":6696},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":6730,\"start\":6726},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6733,\"start\":6730},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6735,\"start\":6733},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6989,\"start\":6985},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7029,\"start\":7025},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7047,\"start\":7043},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7333,\"start\":7329},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7625,\"start\":7621},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7647,\"start\":7643},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7827,\"start\":7823},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7938,\"start\":7934},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7955,\"start\":7951},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8149,\"start\":8145},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8178,\"start\":8174},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8212,\"start\":8209},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8215,\"start\":8212},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8260,\"start\":8256},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8263,\"start\":8260},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8266,\"start\":8263},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8269,\"start\":8266},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8506,\"start\":8502},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9437,\"start\":9433},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9688,\"start\":9684},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9888,\"start\":9884},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9977,\"start\":9973},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11843,\"start\":11839},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":14689,\"start\":14685},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14991,\"start\":14987},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15430,\"start\":15426},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16659,\"start\":16655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17832,\"start\":17828},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18336,\"start\":18332},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19491,\"start\":19487},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20166,\"start\":20162},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21153,\"start\":21149},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21836,\"start\":21832},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23956,\"start\":23952},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24407,\"start\":24404},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24683,\"start\":24679},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24964,\"start\":24960},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25081,\"start\":25077},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25110,\"start\":25106},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25628,\"start\":25624},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25649,\"start\":25645},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25688,\"start\":25684},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25725,\"start\":25721},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25762,\"start\":25758},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26001,\"start\":25997},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26018,\"start\":26014},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26923,\"start\":26919},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26950,\"start\":26946},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26953,\"start\":26950},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":26956,\"start\":26953},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26959,\"start\":26956},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26962,\"start\":26959},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26965,\"start\":26962},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26968,\"start\":26965},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26971,\"start\":26968},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26974,\"start\":26971},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26977,\"start\":26974},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27051,\"start\":27047},{\"end\":27722,\"start\":27715},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28930,\"start\":28926},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32135,\"start\":32131}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32022,\"start\":31203},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32136,\"start\":32023},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32209,\"start\":32137},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32337,\"start\":32210},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32508,\"start\":32338}]", "paragraph": "[{\"end\":1984,\"start\":1498},{\"end\":2815,\"start\":1986},{\"end\":3519,\"start\":2817},{\"end\":4199,\"start\":3521},{\"end\":5605,\"start\":4201},{\"end\":5734,\"start\":5607},{\"end\":5781,\"start\":5736},{\"end\":6202,\"start\":5783},{\"end\":6459,\"start\":6204},{\"end\":7450,\"start\":6476},{\"end\":8894,\"start\":7452},{\"end\":10019,\"start\":8896},{\"end\":10134,\"start\":10035},{\"end\":10503,\"start\":10170},{\"end\":10735,\"start\":10571},{\"end\":11061,\"start\":10762},{\"end\":11356,\"start\":11063},{\"end\":11660,\"start\":11358},{\"end\":12057,\"start\":11699},{\"end\":12206,\"start\":12115},{\"end\":12892,\"start\":12243},{\"end\":13265,\"start\":12894},{\"end\":13658,\"start\":13311},{\"end\":14447,\"start\":13660},{\"end\":15459,\"start\":14449},{\"end\":15724,\"start\":15505},{\"end\":15917,\"start\":15726},{\"end\":16140,\"start\":15952},{\"end\":16621,\"start\":16142},{\"end\":17274,\"start\":16623},{\"end\":17584,\"start\":17276},{\"end\":17999,\"start\":17586},{\"end\":18537,\"start\":18001},{\"end\":18930,\"start\":18539},{\"end\":19654,\"start\":18993},{\"end\":19828,\"start\":19748},{\"end\":20084,\"start\":19853},{\"end\":20610,\"start\":20086},{\"end\":20681,\"start\":20612},{\"end\":20787,\"start\":20737},{\"end\":20906,\"start\":20845},{\"end\":21280,\"start\":20969},{\"end\":21470,\"start\":21310},{\"end\":21967,\"start\":21472},{\"end\":22262,\"start\":21969},{\"end\":22552,\"start\":22264},{\"end\":22855,\"start\":22599},{\"end\":23021,\"start\":22857},{\"end\":23341,\"start\":23023},{\"end\":23883,\"start\":23405},{\"end\":24509,\"start\":23909},{\"end\":25318,\"start\":24526},{\"end\":25563,\"start\":25330},{\"end\":26559,\"start\":25575},{\"end\":26692,\"start\":26561},{\"end\":27223,\"start\":26694},{\"end\":28660,\"start\":27225},{\"end\":28858,\"start\":28703},{\"end\":28978,\"start\":28870},{\"end\":29027,\"start\":28980},{\"end\":30172,\"start\":29029},{\"end\":30431,\"start\":30192},{\"end\":31202,\"start\":30446}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10570,\"start\":10504},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11698,\"start\":11661},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12114,\"start\":12058},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18992,\"start\":18931},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19747,\"start\":19655},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20736,\"start\":20682},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20844,\"start\":20788},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20968,\"start\":20907},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23386,\"start\":23342}]", "table_ref": "[{\"end\":25773,\"start\":25766},{\"end\":26800,\"start\":26793},{\"end\":27392,\"start\":27385},{\"end\":27743,\"start\":27736},{\"end\":29057,\"start\":29050},{\"end\":29479,\"start\":29472}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1496,\"start\":1484},{\"attributes\":{\"n\":\"2\"},\"end\":6474,\"start\":6462},{\"attributes\":{\"n\":\"3\"},\"end\":10033,\"start\":10022},{\"attributes\":{\"n\":\"3.1\"},\"end\":10168,\"start\":10137},{\"attributes\":{\"n\":\"3.2\"},\"end\":10760,\"start\":10738},{\"attributes\":{\"n\":\"4\"},\"end\":12228,\"start\":12209},{\"attributes\":{\"n\":\"4.1\"},\"end\":12241,\"start\":12231},{\"attributes\":{\"n\":\"4.2\"},\"end\":13309,\"start\":13268},{\"attributes\":{\"n\":\"5\"},\"end\":15503,\"start\":15462},{\"attributes\":{\"n\":\"5.1\"},\"end\":15950,\"start\":15920},{\"attributes\":{\"n\":\"5.2\"},\"end\":19851,\"start\":19831},{\"attributes\":{\"n\":\"5.3\"},\"end\":21308,\"start\":21283},{\"end\":22597,\"start\":22555},{\"end\":23403,\"start\":23388},{\"attributes\":{\"n\":\"6\"},\"end\":23897,\"start\":23886},{\"attributes\":{\"n\":\"6.1\"},\"end\":23907,\"start\":23900},{\"attributes\":{\"n\":\"6.2\"},\"end\":24524,\"start\":24512},{\"attributes\":{\"n\":\"6.3\"},\"end\":25328,\"start\":25321},{\"end\":25573,\"start\":25566},{\"attributes\":{\"n\":\"7\"},\"end\":28673,\"start\":28663},{\"attributes\":{\"n\":\"7.1\"},\"end\":28701,\"start\":28676},{\"end\":28868,\"start\":28861},{\"attributes\":{\"n\":\"7.2\"},\"end\":30190,\"start\":30175},{\"attributes\":{\"n\":\"8\"},\"end\":30444,\"start\":30434},{\"end\":31212,\"start\":31204},{\"end\":32032,\"start\":32024},{\"end\":32146,\"start\":32138},{\"end\":32219,\"start\":32211},{\"end\":32347,\"start\":32339}]", "table": null, "figure_caption": "[{\"end\":32022,\"start\":31214},{\"end\":32136,\"start\":32034},{\"end\":32209,\"start\":32148},{\"end\":32337,\"start\":32221},{\"end\":32508,\"start\":32349}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17998,\"start\":17990},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25432,\"start\":25424},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30255,\"start\":30247}]", "bib_author_first_name": "[{\"end\":32735,\"start\":32734},{\"end\":32742,\"start\":32741},{\"end\":32750,\"start\":32749},{\"end\":32757,\"start\":32756},{\"end\":32763,\"start\":32762},{\"end\":33186,\"start\":33185},{\"end\":33199,\"start\":33198},{\"end\":33209,\"start\":33208},{\"end\":33219,\"start\":33218},{\"end\":33221,\"start\":33220},{\"end\":33231,\"start\":33230},{\"end\":33233,\"start\":33232},{\"end\":33241,\"start\":33240},{\"end\":33620,\"start\":33619},{\"end\":33629,\"start\":33628},{\"end\":33640,\"start\":33639},{\"end\":33828,\"start\":33827},{\"end\":33837,\"start\":33836},{\"end\":33844,\"start\":33843},{\"end\":33846,\"start\":33845},{\"end\":33857,\"start\":33856},{\"end\":34175,\"start\":34174},{\"end\":34184,\"start\":34183},{\"end\":34191,\"start\":34190},{\"end\":34193,\"start\":34192},{\"end\":34204,\"start\":34203},{\"end\":34396,\"start\":34395},{\"end\":34398,\"start\":34397},{\"end\":34406,\"start\":34405},{\"end\":34414,\"start\":34413},{\"end\":34426,\"start\":34425},{\"end\":34436,\"start\":34435},{\"end\":34792,\"start\":34791},{\"end\":34800,\"start\":34799},{\"end\":34808,\"start\":34807},{\"end\":34815,\"start\":34814},{\"end\":34817,\"start\":34816},{\"end\":34823,\"start\":34822},{\"end\":34830,\"start\":34829},{\"end\":35311,\"start\":35310},{\"end\":35321,\"start\":35320},{\"end\":35327,\"start\":35326},{\"end\":35735,\"start\":35734},{\"end\":35744,\"start\":35743},{\"end\":35947,\"start\":35946},{\"end\":35954,\"start\":35953},{\"end\":35962,\"start\":35961},{\"end\":35969,\"start\":35968},{\"end\":35976,\"start\":35975},{\"end\":36302,\"start\":36301},{\"end\":36310,\"start\":36309},{\"end\":36319,\"start\":36318},{\"end\":36328,\"start\":36327},{\"end\":36698,\"start\":36697},{\"end\":36712,\"start\":36711},{\"end\":36729,\"start\":36728},{\"end\":36738,\"start\":36737},{\"end\":36744,\"start\":36743},{\"end\":36760,\"start\":36759},{\"end\":36769,\"start\":36768},{\"end\":36782,\"start\":36781},{\"end\":37059,\"start\":37058},{\"end\":37061,\"start\":37060},{\"end\":37073,\"start\":37072},{\"end\":37075,\"start\":37074},{\"end\":37083,\"start\":37082},{\"end\":37085,\"start\":37084},{\"end\":37348,\"start\":37347},{\"end\":37361,\"start\":37360},{\"end\":37370,\"start\":37369},{\"end\":37382,\"start\":37381},{\"end\":37394,\"start\":37393},{\"end\":37396,\"start\":37395},{\"end\":37733,\"start\":37732},{\"end\":37740,\"start\":37739},{\"end\":37748,\"start\":37747},{\"end\":37756,\"start\":37755},{\"end\":37762,\"start\":37761},{\"end\":37769,\"start\":37768},{\"end\":38087,\"start\":38086},{\"end\":38094,\"start\":38093},{\"end\":38103,\"start\":38102},{\"end\":38110,\"start\":38109},{\"end\":38118,\"start\":38117},{\"end\":38126,\"start\":38125},{\"end\":38134,\"start\":38133},{\"end\":38515,\"start\":38514},{\"end\":38527,\"start\":38526},{\"end\":38535,\"start\":38534},{\"end\":38545,\"start\":38544},{\"end\":38852,\"start\":38851},{\"end\":38859,\"start\":38858},{\"end\":38867,\"start\":38866},{\"end\":38876,\"start\":38875},{\"end\":38884,\"start\":38883},{\"end\":39164,\"start\":39163},{\"end\":39171,\"start\":39170},{\"end\":39180,\"start\":39179},{\"end\":39188,\"start\":39187},{\"end\":39378,\"start\":39377},{\"end\":39384,\"start\":39383},{\"end\":39392,\"start\":39391},{\"end\":39394,\"start\":39393},{\"end\":39401,\"start\":39400},{\"end\":39628,\"start\":39627},{\"end\":39634,\"start\":39633},{\"end\":39643,\"start\":39642},{\"end\":39650,\"start\":39649},{\"end\":39930,\"start\":39929},{\"end\":39939,\"start\":39938},{\"end\":39941,\"start\":39940},{\"end\":39951,\"start\":39950},{\"end\":39957,\"start\":39956},{\"end\":40145,\"start\":40144},{\"end\":40156,\"start\":40155},{\"end\":40163,\"start\":40162},{\"end\":40178,\"start\":40177},{\"end\":40191,\"start\":40190},{\"end\":40202,\"start\":40201},{\"end\":40212,\"start\":40211},{\"end\":40534,\"start\":40533},{\"end\":40543,\"start\":40542},{\"end\":40808,\"start\":40807},{\"end\":40817,\"start\":40816},{\"end\":40819,\"start\":40818},{\"end\":40826,\"start\":40825},{\"end\":40834,\"start\":40833},{\"end\":40836,\"start\":40835},{\"end\":41280,\"start\":41279},{\"end\":41290,\"start\":41289},{\"end\":41298,\"start\":41297},{\"end\":41307,\"start\":41306},{\"end\":41588,\"start\":41587},{\"end\":41598,\"start\":41597},{\"end\":41607,\"start\":41606},{\"end\":42012,\"start\":42011},{\"end\":42014,\"start\":42013},{\"end\":42024,\"start\":42023},{\"end\":42164,\"start\":42163},{\"end\":42178,\"start\":42177},{\"end\":42406,\"start\":42405},{\"end\":42420,\"start\":42419},{\"end\":42433,\"start\":42432},{\"end\":42435,\"start\":42434},{\"end\":42759,\"start\":42758},{\"end\":42768,\"start\":42767},{\"end\":42777,\"start\":42776},{\"end\":42788,\"start\":42787},{\"end\":42987,\"start\":42986},{\"end\":42989,\"start\":42988},{\"end\":43002,\"start\":43001},{\"end\":43004,\"start\":43003},{\"end\":43012,\"start\":43011},{\"end\":43023,\"start\":43022},{\"end\":43032,\"start\":43031},{\"end\":43040,\"start\":43039},{\"end\":43049,\"start\":43048},{\"end\":43059,\"start\":43058},{\"end\":43434,\"start\":43433},{\"end\":43441,\"start\":43440},{\"end\":43443,\"start\":43442},{\"end\":43451,\"start\":43450},{\"end\":43462,\"start\":43461},{\"end\":43470,\"start\":43469},{\"end\":43477,\"start\":43476},{\"end\":43479,\"start\":43478},{\"end\":43489,\"start\":43488},{\"end\":43951,\"start\":43950},{\"end\":43958,\"start\":43957},{\"end\":43966,\"start\":43965},{\"end\":43977,\"start\":43976},{\"end\":43987,\"start\":43986},{\"end\":43994,\"start\":43993},{\"end\":43996,\"start\":43995},{\"end\":44002,\"start\":44001},{\"end\":44013,\"start\":44012},{\"end\":44023,\"start\":44022},{\"end\":44032,\"start\":44031},{\"end\":44396,\"start\":44395},{\"end\":44403,\"start\":44402},{\"end\":44415,\"start\":44414},{\"end\":44712,\"start\":44711},{\"end\":44722,\"start\":44721},{\"end\":44733,\"start\":44732},{\"end\":44743,\"start\":44742},{\"end\":44994,\"start\":44993},{\"end\":45002,\"start\":45001},{\"end\":45017,\"start\":45016},{\"end\":45027,\"start\":45026},{\"end\":45037,\"start\":45036},{\"end\":45051,\"start\":45050},{\"end\":45063,\"start\":45062},{\"end\":45395,\"start\":45394},{\"end\":45403,\"start\":45402},{\"end\":45418,\"start\":45417},{\"end\":45428,\"start\":45427},{\"end\":45430,\"start\":45429},{\"end\":45438,\"start\":45437},{\"end\":45448,\"start\":45447},{\"end\":45450,\"start\":45449},{\"end\":45463,\"start\":45462},{\"end\":45473,\"start\":45472},{\"end\":45487,\"start\":45486},{\"end\":45489,\"start\":45488},{\"end\":45502,\"start\":45501},{\"end\":45817,\"start\":45816},{\"end\":45827,\"start\":45826},{\"end\":45833,\"start\":45832},{\"end\":45839,\"start\":45838},{\"end\":46127,\"start\":46126},{\"end\":46135,\"start\":46134},{\"end\":46137,\"start\":46136},{\"end\":46144,\"start\":46143},{\"end\":46146,\"start\":46145},{\"end\":46154,\"start\":46153},{\"end\":46156,\"start\":46155},{\"end\":46519,\"start\":46518},{\"end\":46527,\"start\":46526},{\"end\":46529,\"start\":46528},{\"end\":46537,\"start\":46536},{\"end\":46545,\"start\":46544},{\"end\":46547,\"start\":46546},{\"end\":46553,\"start\":46552},{\"end\":46897,\"start\":46896},{\"end\":46908,\"start\":46907},{\"end\":46916,\"start\":46915},{\"end\":47173,\"start\":47172},{\"end\":47181,\"start\":47180},{\"end\":47190,\"start\":47189},{\"end\":47199,\"start\":47198},{\"end\":47209,\"start\":47208},{\"end\":47211,\"start\":47210},{\"end\":47223,\"start\":47222},{\"end\":47230,\"start\":47229},{\"end\":47232,\"start\":47231},{\"end\":47238,\"start\":47237},{\"end\":47677,\"start\":47676},{\"end\":47685,\"start\":47684},{\"end\":47694,\"start\":47693},{\"end\":47701,\"start\":47700},{\"end\":47715,\"start\":47714},{\"end\":47726,\"start\":47725},{\"end\":48008,\"start\":48007},{\"end\":48020,\"start\":48019},{\"end\":48034,\"start\":48033},{\"end\":48045,\"start\":48044},{\"end\":48055,\"start\":48054},{\"end\":48066,\"start\":48065},{\"end\":48355,\"start\":48354},{\"end\":48367,\"start\":48366},{\"end\":48377,\"start\":48376},{\"end\":48387,\"start\":48386},{\"end\":48397,\"start\":48396},{\"end\":48621,\"start\":48620},{\"end\":48633,\"start\":48632},{\"end\":48643,\"start\":48642},{\"end\":48655,\"start\":48654},{\"end\":48666,\"start\":48665},{\"end\":48893,\"start\":48892},{\"end\":48903,\"start\":48902},{\"end\":48912,\"start\":48911},{\"end\":48921,\"start\":48920},{\"end\":48931,\"start\":48930},{\"end\":48943,\"start\":48942},{\"end\":49185,\"start\":49184},{\"end\":49187,\"start\":49186},{\"end\":49197,\"start\":49196},{\"end\":49199,\"start\":49198},{\"end\":49208,\"start\":49207},{\"end\":49210,\"start\":49209},{\"end\":49463,\"start\":49462},{\"end\":49465,\"start\":49464},{\"end\":49473,\"start\":49472},{\"end\":49475,\"start\":49474},{\"end\":49482,\"start\":49481},{\"end\":49484,\"start\":49483},{\"end\":49859,\"start\":49858},{\"end\":49870,\"start\":49869},{\"end\":49881,\"start\":49880},{\"end\":50143,\"start\":50142},{\"end\":50151,\"start\":50150},{\"end\":50396,\"start\":50395},{\"end\":50398,\"start\":50397},{\"end\":50406,\"start\":50405},{\"end\":50408,\"start\":50407},{\"end\":50415,\"start\":50414},{\"end\":50417,\"start\":50416},{\"end\":50424,\"start\":50423},{\"end\":50431,\"start\":50430},{\"end\":50440,\"start\":50439},{\"end\":50907,\"start\":50906},{\"end\":50915,\"start\":50914},{\"end\":50922,\"start\":50921},{\"end\":51264,\"start\":51263},{\"end\":51280,\"start\":51279},{\"end\":51495,\"start\":51494},{\"end\":51497,\"start\":51496},{\"end\":51679,\"start\":51678},{\"end\":51686,\"start\":51685},{\"end\":51695,\"start\":51694},{\"end\":51702,\"start\":51701},{\"end\":51994,\"start\":51993},{\"end\":52001,\"start\":52000},{\"end\":52010,\"start\":52009},{\"end\":52017,\"start\":52016},{\"end\":52027,\"start\":52026},{\"end\":52036,\"start\":52035},{\"end\":52376,\"start\":52375},{\"end\":52382,\"start\":52381},{\"end\":52391,\"start\":52390},{\"end\":52400,\"start\":52399},{\"end\":52409,\"start\":52408},{\"end\":52416,\"start\":52415},{\"end\":52425,\"start\":52424},{\"end\":52925,\"start\":52924},{\"end\":52933,\"start\":52932},{\"end\":52943,\"start\":52942},{\"end\":52952,\"start\":52951},{\"end\":53205,\"start\":53204},{\"end\":53214,\"start\":53213},{\"end\":53228,\"start\":53227},{\"end\":53239,\"start\":53238},{\"end\":53563,\"start\":53562},{\"end\":53572,\"start\":53571},{\"end\":53578,\"start\":53577},{\"end\":53584,\"start\":53583},{\"end\":53593,\"start\":53592},{\"end\":53601,\"start\":53600},{\"end\":53610,\"start\":53609},{\"end\":53612,\"start\":53611},{\"end\":54074,\"start\":54073},{\"end\":54083,\"start\":54082},{\"end\":54090,\"start\":54089},{\"end\":54096,\"start\":54095},{\"end\":54104,\"start\":54103},{\"end\":54106,\"start\":54105},{\"end\":54553,\"start\":54552},{\"end\":54555,\"start\":54554},{\"end\":54674,\"start\":54673},{\"end\":54682,\"start\":54681},{\"end\":54684,\"start\":54683}]", "bib_author_last_name": "[{\"end\":32739,\"start\":32736},{\"end\":32747,\"start\":32743},{\"end\":32754,\"start\":32751},{\"end\":32760,\"start\":32758},{\"end\":32767,\"start\":32764},{\"end\":33196,\"start\":33187},{\"end\":33206,\"start\":33200},{\"end\":33216,\"start\":33210},{\"end\":33228,\"start\":33222},{\"end\":33238,\"start\":33234},{\"end\":33252,\"start\":33242},{\"end\":33626,\"start\":33621},{\"end\":33637,\"start\":33630},{\"end\":33649,\"start\":33641},{\"end\":33834,\"start\":33829},{\"end\":33841,\"start\":33838},{\"end\":33854,\"start\":33847},{\"end\":33864,\"start\":33858},{\"end\":34181,\"start\":34176},{\"end\":34188,\"start\":34185},{\"end\":34201,\"start\":34194},{\"end\":34211,\"start\":34205},{\"end\":34403,\"start\":34399},{\"end\":34411,\"start\":34407},{\"end\":34423,\"start\":34415},{\"end\":34433,\"start\":34427},{\"end\":34441,\"start\":34437},{\"end\":34797,\"start\":34793},{\"end\":34805,\"start\":34801},{\"end\":34812,\"start\":34809},{\"end\":34820,\"start\":34818},{\"end\":34827,\"start\":34824},{\"end\":34835,\"start\":34831},{\"end\":35318,\"start\":35312},{\"end\":35324,\"start\":35322},{\"end\":35331,\"start\":35328},{\"end\":35741,\"start\":35736},{\"end\":35751,\"start\":35745},{\"end\":35951,\"start\":35948},{\"end\":35959,\"start\":35955},{\"end\":35966,\"start\":35963},{\"end\":35973,\"start\":35970},{\"end\":35980,\"start\":35977},{\"end\":36307,\"start\":36303},{\"end\":36316,\"start\":36311},{\"end\":36325,\"start\":36320},{\"end\":36333,\"start\":36329},{\"end\":36709,\"start\":36699},{\"end\":36726,\"start\":36713},{\"end\":36735,\"start\":36730},{\"end\":36741,\"start\":36739},{\"end\":36757,\"start\":36745},{\"end\":36766,\"start\":36761},{\"end\":36779,\"start\":36770},{\"end\":36789,\"start\":36783},{\"end\":37070,\"start\":37062},{\"end\":37080,\"start\":37076},{\"end\":37094,\"start\":37086},{\"end\":37358,\"start\":37349},{\"end\":37367,\"start\":37362},{\"end\":37379,\"start\":37371},{\"end\":37391,\"start\":37383},{\"end\":37406,\"start\":37397},{\"end\":37737,\"start\":37734},{\"end\":37745,\"start\":37741},{\"end\":37753,\"start\":37749},{\"end\":37759,\"start\":37757},{\"end\":37766,\"start\":37763},{\"end\":37773,\"start\":37770},{\"end\":38091,\"start\":38088},{\"end\":38100,\"start\":38095},{\"end\":38107,\"start\":38104},{\"end\":38115,\"start\":38111},{\"end\":38123,\"start\":38119},{\"end\":38131,\"start\":38127},{\"end\":38140,\"start\":38135},{\"end\":38524,\"start\":38516},{\"end\":38532,\"start\":38528},{\"end\":38542,\"start\":38536},{\"end\":38552,\"start\":38546},{\"end\":38856,\"start\":38853},{\"end\":38864,\"start\":38860},{\"end\":38873,\"start\":38868},{\"end\":38881,\"start\":38877},{\"end\":38888,\"start\":38885},{\"end\":39168,\"start\":39165},{\"end\":39177,\"start\":39172},{\"end\":39185,\"start\":39181},{\"end\":39192,\"start\":39189},{\"end\":39381,\"start\":39379},{\"end\":39389,\"start\":39385},{\"end\":39398,\"start\":39395},{\"end\":39406,\"start\":39402},{\"end\":39631,\"start\":39629},{\"end\":39640,\"start\":39635},{\"end\":39647,\"start\":39644},{\"end\":39654,\"start\":39651},{\"end\":39936,\"start\":39931},{\"end\":39948,\"start\":39942},{\"end\":39954,\"start\":39952},{\"end\":39963,\"start\":39958},{\"end\":40153,\"start\":40146},{\"end\":40160,\"start\":40157},{\"end\":40175,\"start\":40164},{\"end\":40188,\"start\":40179},{\"end\":40199,\"start\":40192},{\"end\":40209,\"start\":40203},{\"end\":40219,\"start\":40213},{\"end\":40540,\"start\":40535},{\"end\":40551,\"start\":40544},{\"end\":40814,\"start\":40809},{\"end\":40823,\"start\":40820},{\"end\":40831,\"start\":40827},{\"end\":40842,\"start\":40837},{\"end\":41287,\"start\":41281},{\"end\":41295,\"start\":41291},{\"end\":41304,\"start\":41299},{\"end\":41316,\"start\":41308},{\"end\":41595,\"start\":41589},{\"end\":41604,\"start\":41599},{\"end\":41612,\"start\":41608},{\"end\":42021,\"start\":42015},{\"end\":42027,\"start\":42025},{\"end\":42175,\"start\":42165},{\"end\":42185,\"start\":42179},{\"end\":42417,\"start\":42407},{\"end\":42430,\"start\":42421},{\"end\":42442,\"start\":42436},{\"end\":42765,\"start\":42760},{\"end\":42774,\"start\":42769},{\"end\":42785,\"start\":42778},{\"end\":42795,\"start\":42789},{\"end\":42999,\"start\":42990},{\"end\":43009,\"start\":43005},{\"end\":43020,\"start\":43013},{\"end\":43029,\"start\":43024},{\"end\":43037,\"start\":43033},{\"end\":43046,\"start\":43041},{\"end\":43056,\"start\":43050},{\"end\":43068,\"start\":43060},{\"end\":43438,\"start\":43435},{\"end\":43448,\"start\":43444},{\"end\":43459,\"start\":43452},{\"end\":43467,\"start\":43463},{\"end\":43474,\"start\":43471},{\"end\":43486,\"start\":43480},{\"end\":43497,\"start\":43490},{\"end\":43955,\"start\":43952},{\"end\":43963,\"start\":43959},{\"end\":43974,\"start\":43967},{\"end\":43984,\"start\":43978},{\"end\":43991,\"start\":43988},{\"end\":43999,\"start\":43997},{\"end\":44010,\"start\":44003},{\"end\":44020,\"start\":44014},{\"end\":44029,\"start\":44024},{\"end\":44039,\"start\":44033},{\"end\":44400,\"start\":44397},{\"end\":44412,\"start\":44404},{\"end\":44420,\"start\":44416},{\"end\":44719,\"start\":44713},{\"end\":44730,\"start\":44723},{\"end\":44740,\"start\":44734},{\"end\":44751,\"start\":44744},{\"end\":44999,\"start\":44995},{\"end\":45014,\"start\":45003},{\"end\":45024,\"start\":45018},{\"end\":45034,\"start\":45028},{\"end\":45048,\"start\":45038},{\"end\":45060,\"start\":45052},{\"end\":45074,\"start\":45064},{\"end\":45400,\"start\":45396},{\"end\":45415,\"start\":45404},{\"end\":45425,\"start\":45419},{\"end\":45435,\"start\":45431},{\"end\":45445,\"start\":45439},{\"end\":45460,\"start\":45451},{\"end\":45470,\"start\":45464},{\"end\":45484,\"start\":45474},{\"end\":45499,\"start\":45490},{\"end\":45512,\"start\":45503},{\"end\":45824,\"start\":45818},{\"end\":45830,\"start\":45828},{\"end\":45836,\"start\":45834},{\"end\":45845,\"start\":45840},{\"end\":46132,\"start\":46128},{\"end\":46141,\"start\":46138},{\"end\":46151,\"start\":46147},{\"end\":46160,\"start\":46157},{\"end\":46524,\"start\":46520},{\"end\":46534,\"start\":46530},{\"end\":46542,\"start\":46538},{\"end\":46550,\"start\":46548},{\"end\":46558,\"start\":46554},{\"end\":46905,\"start\":46898},{\"end\":46913,\"start\":46909},{\"end\":46925,\"start\":46917},{\"end\":47178,\"start\":47174},{\"end\":47187,\"start\":47182},{\"end\":47196,\"start\":47191},{\"end\":47206,\"start\":47200},{\"end\":47220,\"start\":47212},{\"end\":47227,\"start\":47224},{\"end\":47235,\"start\":47233},{\"end\":47246,\"start\":47239},{\"end\":47682,\"start\":47678},{\"end\":47691,\"start\":47686},{\"end\":47698,\"start\":47695},{\"end\":47712,\"start\":47702},{\"end\":47723,\"start\":47716},{\"end\":47730,\"start\":47727},{\"end\":48017,\"start\":48009},{\"end\":48031,\"start\":48021},{\"end\":48042,\"start\":48035},{\"end\":48052,\"start\":48046},{\"end\":48063,\"start\":48056},{\"end\":48071,\"start\":48067},{\"end\":48364,\"start\":48356},{\"end\":48374,\"start\":48368},{\"end\":48384,\"start\":48378},{\"end\":48394,\"start\":48388},{\"end\":48404,\"start\":48398},{\"end\":48630,\"start\":48622},{\"end\":48640,\"start\":48634},{\"end\":48652,\"start\":48644},{\"end\":48663,\"start\":48656},{\"end\":48673,\"start\":48667},{\"end\":48900,\"start\":48894},{\"end\":48909,\"start\":48904},{\"end\":48918,\"start\":48913},{\"end\":48928,\"start\":48922},{\"end\":48940,\"start\":48932},{\"end\":48954,\"start\":48944},{\"end\":49194,\"start\":49188},{\"end\":49205,\"start\":49200},{\"end\":49219,\"start\":49211},{\"end\":49470,\"start\":49466},{\"end\":49479,\"start\":49476},{\"end\":49491,\"start\":49485},{\"end\":49867,\"start\":49860},{\"end\":49878,\"start\":49871},{\"end\":49891,\"start\":49882},{\"end\":50148,\"start\":50144},{\"end\":50156,\"start\":50152},{\"end\":50403,\"start\":50399},{\"end\":50412,\"start\":50409},{\"end\":50421,\"start\":50418},{\"end\":50428,\"start\":50425},{\"end\":50437,\"start\":50432},{\"end\":50450,\"start\":50441},{\"end\":50912,\"start\":50908},{\"end\":50919,\"start\":50916},{\"end\":50932,\"start\":50923},{\"end\":51277,\"start\":51265},{\"end\":51287,\"start\":51281},{\"end\":51506,\"start\":51498},{\"end\":51683,\"start\":51680},{\"end\":51692,\"start\":51687},{\"end\":51699,\"start\":51696},{\"end\":51706,\"start\":51703},{\"end\":51998,\"start\":51995},{\"end\":52007,\"start\":52002},{\"end\":52014,\"start\":52011},{\"end\":52024,\"start\":52018},{\"end\":52033,\"start\":52028},{\"end\":52049,\"start\":52037},{\"end\":52379,\"start\":52377},{\"end\":52388,\"start\":52383},{\"end\":52397,\"start\":52392},{\"end\":52406,\"start\":52401},{\"end\":52413,\"start\":52410},{\"end\":52422,\"start\":52417},{\"end\":52428,\"start\":52426},{\"end\":52930,\"start\":52926},{\"end\":52940,\"start\":52934},{\"end\":52949,\"start\":52944},{\"end\":52959,\"start\":52953},{\"end\":53211,\"start\":53206},{\"end\":53225,\"start\":53215},{\"end\":53236,\"start\":53229},{\"end\":53245,\"start\":53240},{\"end\":53569,\"start\":53564},{\"end\":53575,\"start\":53573},{\"end\":53581,\"start\":53579},{\"end\":53590,\"start\":53585},{\"end\":53598,\"start\":53594},{\"end\":53607,\"start\":53602},{\"end\":53620,\"start\":53613},{\"end\":54080,\"start\":54075},{\"end\":54087,\"start\":54084},{\"end\":54093,\"start\":54091},{\"end\":54101,\"start\":54097},{\"end\":54110,\"start\":54107},{\"end\":54563,\"start\":54556},{\"end\":54679,\"start\":54675},{\"end\":54687,\"start\":54685}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206771102},\"end\":33097,\"start\":32665},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3232170},\"end\":33549,\"start\":33099},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b2\"},\"end\":33825,\"start\":33551},{\"attributes\":{\"doi\":\"arXiv:1609.07093\",\"id\":\"b3\"},\"end\":34107,\"start\":33827},{\"attributes\":{\"doi\":\"arXiv:1708.05344\",\"id\":\"b4\"},\"end\":34393,\"start\":34109},{\"attributes\":{\"doi\":\"arXiv:2006.04001\",\"id\":\"b5\"},\"end\":34695,\"start\":34395},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9417016},\"end\":35237,\"start\":34697},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":308212},\"end\":35732,\"start\":35239},{\"attributes\":{\"doi\":\"arXiv:1912.00606\",\"id\":\"b8\"},\"end\":35944,\"start\":35734},{\"attributes\":{\"doi\":\"arXiv:1912.02037\",\"id\":\"b9\"},\"end\":36226,\"start\":35946},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":199543358},\"end\":36666,\"start\":36228},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1033682},\"end\":37056,\"start\":36668},{\"attributes\":{\"doi\":\"arXiv:1709.07359\",\"id\":\"b12\"},\"end\":37306,\"start\":37058},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10894094},\"end\":37646,\"start\":37308},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":85529479},\"end\":38007,\"start\":37648},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202785187},\"end\":38413,\"start\":38009},{\"attributes\":{\"doi\":\"arXiv:1801.01290\",\"id\":\"b16\"},\"end\":38771,\"start\":38415},{\"attributes\":{\"doi\":\"arXiv:1911.02875\",\"id\":\"b17\"},\"end\":39087,\"start\":38773},{\"attributes\":{\"doi\":\"arXiv:2004.14288\",\"id\":\"b18\"},\"end\":39375,\"start\":39089},{\"attributes\":{\"id\":\"b19\"},\"end\":39580,\"start\":39377},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6447277},\"end\":39858,\"start\":39582},{\"attributes\":{\"id\":\"b21\"},\"end\":40083,\"start\":39860},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":58031572},\"end\":40437,\"start\":40085},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b23\"},\"end\":40739,\"start\":40439},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6200260},\"end\":41201,\"start\":40741},{\"attributes\":{\"doi\":\"arXiv:1710.10196\",\"id\":\"b25\"},\"end\":41511,\"start\":41203},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":54482423},\"end\":41965,\"start\":41513},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b27\"},\"end\":42161,\"start\":41967},{\"attributes\":{\"id\":\"b28\"},\"end\":42338,\"start\":42163},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":195908774},\"end\":42684,\"start\":42340},{\"attributes\":{\"doi\":\"arXiv:1611.05095\",\"id\":\"b30\"},\"end\":42984,\"start\":42686},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b31\"},\"end\":43344,\"start\":42986},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":57761158},\"end\":43908,\"start\":43346},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":40430109},\"end\":44393,\"start\":43910},{\"attributes\":{\"id\":\"b34\"},\"end\":44709,\"start\":44395},{\"attributes\":{\"doi\":\"arXiv:1802.05957\",\"id\":\"b35\"},\"end\":44991,\"start\":44711},{\"attributes\":{\"doi\":\"arXiv:1312.5602\",\"id\":\"b36\"},\"end\":45335,\"start\":44993},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":205242740},\"end\":45766,\"start\":45337},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1059659},\"end\":46060,\"start\":45768},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":81981856},\"end\":46516,\"start\":46062},{\"attributes\":{\"doi\":\"arXiv:1802.03268\",\"id\":\"b40\"},\"end\":46800,\"start\":46518},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b41\"},\"end\":47126,\"start\":46802},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":743641},\"end\":47626,\"start\":47128},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1563370},\"end\":47966,\"start\":47628},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1687220},\"end\":48318,\"start\":47968},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":16046818},\"end\":48618,\"start\":48320},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b46\"},\"end\":48890,\"start\":48620},{\"attributes\":{\"id\":\"b47\"},\"end\":49123,\"start\":48892},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":47405459},\"end\":49406,\"start\":49125},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":51863503},\"end\":49787,\"start\":49408},{\"attributes\":{\"doi\":\"arXiv:1607.08022\",\"id\":\"b50\"},\"end\":50073,\"start\":49789},{\"attributes\":{\"doi\":\"arXiv:1906.11080\",\"id\":\"b51\"},\"end\":50313,\"start\":50075},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":41805341},\"end\":50847,\"start\":50315},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":56895592},\"end\":51186,\"start\":50849},{\"attributes\":{\"id\":\"b54\"},\"end\":51401,\"start\":51188},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":2332513},\"end\":51676,\"start\":51403},{\"attributes\":{\"doi\":\"arXiv:1812.09926\",\"id\":\"b56\"},\"end\":51904,\"start\":51678},{\"attributes\":{\"doi\":\"arXiv:1903.09537\",\"id\":\"b57\"},\"end\":52276,\"start\":51906},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":8858625},\"end\":52842,\"start\":52278},{\"attributes\":{\"doi\":\"arXiv:1703.01560\",\"id\":\"b59\"},\"end\":53154,\"start\":52844},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":46898260},\"end\":53464,\"start\":53156},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1277217},\"end\":54010,\"start\":53466},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":3866935},\"end\":54466,\"start\":54012},{\"attributes\":{\"id\":\"b63\"},\"end\":54671,\"start\":54468},{\"attributes\":{\"id\":\"b64\"},\"end\":54936,\"start\":54673}]", "bib_title": "[{\"end\":32732,\"start\":32665},{\"end\":33183,\"start\":33099},{\"end\":34789,\"start\":34697},{\"end\":35308,\"start\":35239},{\"end\":36299,\"start\":36228},{\"end\":36695,\"start\":36668},{\"end\":37345,\"start\":37308},{\"end\":37730,\"start\":37648},{\"end\":38084,\"start\":38009},{\"end\":39625,\"start\":39582},{\"end\":40142,\"start\":40085},{\"end\":40805,\"start\":40741},{\"end\":41585,\"start\":41513},{\"end\":42403,\"start\":42340},{\"end\":43431,\"start\":43346},{\"end\":43948,\"start\":43910},{\"end\":45392,\"start\":45337},{\"end\":45814,\"start\":45768},{\"end\":46124,\"start\":46062},{\"end\":47170,\"start\":47128},{\"end\":47674,\"start\":47628},{\"end\":48005,\"start\":47968},{\"end\":48352,\"start\":48320},{\"end\":49182,\"start\":49125},{\"end\":49460,\"start\":49408},{\"end\":50393,\"start\":50315},{\"end\":50904,\"start\":50849},{\"end\":51492,\"start\":51403},{\"end\":52373,\"start\":52278},{\"end\":53202,\"start\":53156},{\"end\":53560,\"start\":53466},{\"end\":54071,\"start\":54012}]", "bib_author": "[{\"end\":32741,\"start\":32734},{\"end\":32749,\"start\":32741},{\"end\":32756,\"start\":32749},{\"end\":32762,\"start\":32756},{\"end\":32769,\"start\":32762},{\"end\":33198,\"start\":33185},{\"end\":33208,\"start\":33198},{\"end\":33218,\"start\":33208},{\"end\":33230,\"start\":33218},{\"end\":33240,\"start\":33230},{\"end\":33254,\"start\":33240},{\"end\":33628,\"start\":33619},{\"end\":33639,\"start\":33628},{\"end\":33651,\"start\":33639},{\"end\":33836,\"start\":33827},{\"end\":33843,\"start\":33836},{\"end\":33856,\"start\":33843},{\"end\":33866,\"start\":33856},{\"end\":34183,\"start\":34174},{\"end\":34190,\"start\":34183},{\"end\":34203,\"start\":34190},{\"end\":34213,\"start\":34203},{\"end\":34405,\"start\":34395},{\"end\":34413,\"start\":34405},{\"end\":34425,\"start\":34413},{\"end\":34435,\"start\":34425},{\"end\":34443,\"start\":34435},{\"end\":34799,\"start\":34791},{\"end\":34807,\"start\":34799},{\"end\":34814,\"start\":34807},{\"end\":34822,\"start\":34814},{\"end\":34829,\"start\":34822},{\"end\":34837,\"start\":34829},{\"end\":35320,\"start\":35310},{\"end\":35326,\"start\":35320},{\"end\":35333,\"start\":35326},{\"end\":35743,\"start\":35734},{\"end\":35753,\"start\":35743},{\"end\":35953,\"start\":35946},{\"end\":35961,\"start\":35953},{\"end\":35968,\"start\":35961},{\"end\":35975,\"start\":35968},{\"end\":35982,\"start\":35975},{\"end\":36309,\"start\":36301},{\"end\":36318,\"start\":36309},{\"end\":36327,\"start\":36318},{\"end\":36335,\"start\":36327},{\"end\":36711,\"start\":36697},{\"end\":36728,\"start\":36711},{\"end\":36737,\"start\":36728},{\"end\":36743,\"start\":36737},{\"end\":36759,\"start\":36743},{\"end\":36768,\"start\":36759},{\"end\":36781,\"start\":36768},{\"end\":36791,\"start\":36781},{\"end\":37072,\"start\":37058},{\"end\":37082,\"start\":37072},{\"end\":37096,\"start\":37082},{\"end\":37360,\"start\":37347},{\"end\":37369,\"start\":37360},{\"end\":37381,\"start\":37369},{\"end\":37393,\"start\":37381},{\"end\":37408,\"start\":37393},{\"end\":37739,\"start\":37732},{\"end\":37747,\"start\":37739},{\"end\":37755,\"start\":37747},{\"end\":37761,\"start\":37755},{\"end\":37768,\"start\":37761},{\"end\":37775,\"start\":37768},{\"end\":38093,\"start\":38086},{\"end\":38102,\"start\":38093},{\"end\":38109,\"start\":38102},{\"end\":38117,\"start\":38109},{\"end\":38125,\"start\":38117},{\"end\":38133,\"start\":38125},{\"end\":38142,\"start\":38133},{\"end\":38526,\"start\":38514},{\"end\":38534,\"start\":38526},{\"end\":38544,\"start\":38534},{\"end\":38554,\"start\":38544},{\"end\":38858,\"start\":38851},{\"end\":38866,\"start\":38858},{\"end\":38875,\"start\":38866},{\"end\":38883,\"start\":38875},{\"end\":38890,\"start\":38883},{\"end\":39170,\"start\":39163},{\"end\":39179,\"start\":39170},{\"end\":39187,\"start\":39179},{\"end\":39194,\"start\":39187},{\"end\":39383,\"start\":39377},{\"end\":39391,\"start\":39383},{\"end\":39400,\"start\":39391},{\"end\":39408,\"start\":39400},{\"end\":39633,\"start\":39627},{\"end\":39642,\"start\":39633},{\"end\":39649,\"start\":39642},{\"end\":39656,\"start\":39649},{\"end\":39938,\"start\":39929},{\"end\":39950,\"start\":39938},{\"end\":39956,\"start\":39950},{\"end\":39965,\"start\":39956},{\"end\":40155,\"start\":40144},{\"end\":40162,\"start\":40155},{\"end\":40177,\"start\":40162},{\"end\":40190,\"start\":40177},{\"end\":40201,\"start\":40190},{\"end\":40211,\"start\":40201},{\"end\":40221,\"start\":40211},{\"end\":40542,\"start\":40533},{\"end\":40553,\"start\":40542},{\"end\":40816,\"start\":40807},{\"end\":40825,\"start\":40816},{\"end\":40833,\"start\":40825},{\"end\":40844,\"start\":40833},{\"end\":41289,\"start\":41279},{\"end\":41297,\"start\":41289},{\"end\":41306,\"start\":41297},{\"end\":41318,\"start\":41306},{\"end\":41597,\"start\":41587},{\"end\":41606,\"start\":41597},{\"end\":41614,\"start\":41606},{\"end\":42023,\"start\":42011},{\"end\":42029,\"start\":42023},{\"end\":42177,\"start\":42163},{\"end\":42187,\"start\":42177},{\"end\":42419,\"start\":42405},{\"end\":42432,\"start\":42419},{\"end\":42444,\"start\":42432},{\"end\":42767,\"start\":42758},{\"end\":42776,\"start\":42767},{\"end\":42787,\"start\":42776},{\"end\":42797,\"start\":42787},{\"end\":43001,\"start\":42986},{\"end\":43011,\"start\":43001},{\"end\":43022,\"start\":43011},{\"end\":43031,\"start\":43022},{\"end\":43039,\"start\":43031},{\"end\":43048,\"start\":43039},{\"end\":43058,\"start\":43048},{\"end\":43070,\"start\":43058},{\"end\":43440,\"start\":43433},{\"end\":43450,\"start\":43440},{\"end\":43461,\"start\":43450},{\"end\":43469,\"start\":43461},{\"end\":43476,\"start\":43469},{\"end\":43488,\"start\":43476},{\"end\":43499,\"start\":43488},{\"end\":43957,\"start\":43950},{\"end\":43965,\"start\":43957},{\"end\":43976,\"start\":43965},{\"end\":43986,\"start\":43976},{\"end\":43993,\"start\":43986},{\"end\":44001,\"start\":43993},{\"end\":44012,\"start\":44001},{\"end\":44022,\"start\":44012},{\"end\":44031,\"start\":44022},{\"end\":44041,\"start\":44031},{\"end\":44402,\"start\":44395},{\"end\":44414,\"start\":44402},{\"end\":44422,\"start\":44414},{\"end\":44721,\"start\":44711},{\"end\":44732,\"start\":44721},{\"end\":44742,\"start\":44732},{\"end\":44753,\"start\":44742},{\"end\":45001,\"start\":44993},{\"end\":45016,\"start\":45001},{\"end\":45026,\"start\":45016},{\"end\":45036,\"start\":45026},{\"end\":45050,\"start\":45036},{\"end\":45062,\"start\":45050},{\"end\":45076,\"start\":45062},{\"end\":45402,\"start\":45394},{\"end\":45417,\"start\":45402},{\"end\":45427,\"start\":45417},{\"end\":45437,\"start\":45427},{\"end\":45447,\"start\":45437},{\"end\":45462,\"start\":45447},{\"end\":45472,\"start\":45462},{\"end\":45486,\"start\":45472},{\"end\":45501,\"start\":45486},{\"end\":45514,\"start\":45501},{\"end\":45826,\"start\":45816},{\"end\":45832,\"start\":45826},{\"end\":45838,\"start\":45832},{\"end\":45847,\"start\":45838},{\"end\":46134,\"start\":46126},{\"end\":46143,\"start\":46134},{\"end\":46153,\"start\":46143},{\"end\":46162,\"start\":46153},{\"end\":46526,\"start\":46518},{\"end\":46536,\"start\":46526},{\"end\":46544,\"start\":46536},{\"end\":46552,\"start\":46544},{\"end\":46560,\"start\":46552},{\"end\":46907,\"start\":46896},{\"end\":46915,\"start\":46907},{\"end\":46927,\"start\":46915},{\"end\":47180,\"start\":47172},{\"end\":47189,\"start\":47180},{\"end\":47198,\"start\":47189},{\"end\":47208,\"start\":47198},{\"end\":47222,\"start\":47208},{\"end\":47229,\"start\":47222},{\"end\":47237,\"start\":47229},{\"end\":47248,\"start\":47237},{\"end\":47684,\"start\":47676},{\"end\":47693,\"start\":47684},{\"end\":47700,\"start\":47693},{\"end\":47714,\"start\":47700},{\"end\":47725,\"start\":47714},{\"end\":47732,\"start\":47725},{\"end\":48019,\"start\":48007},{\"end\":48033,\"start\":48019},{\"end\":48044,\"start\":48033},{\"end\":48054,\"start\":48044},{\"end\":48065,\"start\":48054},{\"end\":48073,\"start\":48065},{\"end\":48366,\"start\":48354},{\"end\":48376,\"start\":48366},{\"end\":48386,\"start\":48376},{\"end\":48396,\"start\":48386},{\"end\":48406,\"start\":48396},{\"end\":48632,\"start\":48620},{\"end\":48642,\"start\":48632},{\"end\":48654,\"start\":48642},{\"end\":48665,\"start\":48654},{\"end\":48675,\"start\":48665},{\"end\":48902,\"start\":48892},{\"end\":48911,\"start\":48902},{\"end\":48920,\"start\":48911},{\"end\":48930,\"start\":48920},{\"end\":48942,\"start\":48930},{\"end\":48956,\"start\":48942},{\"end\":49196,\"start\":49184},{\"end\":49207,\"start\":49196},{\"end\":49221,\"start\":49207},{\"end\":49472,\"start\":49462},{\"end\":49481,\"start\":49472},{\"end\":49493,\"start\":49481},{\"end\":49869,\"start\":49858},{\"end\":49880,\"start\":49869},{\"end\":49893,\"start\":49880},{\"end\":50150,\"start\":50142},{\"end\":50158,\"start\":50150},{\"end\":50405,\"start\":50395},{\"end\":50414,\"start\":50405},{\"end\":50423,\"start\":50414},{\"end\":50430,\"start\":50423},{\"end\":50439,\"start\":50430},{\"end\":50452,\"start\":50439},{\"end\":50914,\"start\":50906},{\"end\":50921,\"start\":50914},{\"end\":50934,\"start\":50921},{\"end\":51279,\"start\":51263},{\"end\":51289,\"start\":51279},{\"end\":51508,\"start\":51494},{\"end\":51685,\"start\":51678},{\"end\":51694,\"start\":51685},{\"end\":51701,\"start\":51694},{\"end\":51708,\"start\":51701},{\"end\":52000,\"start\":51993},{\"end\":52009,\"start\":52000},{\"end\":52016,\"start\":52009},{\"end\":52026,\"start\":52016},{\"end\":52035,\"start\":52026},{\"end\":52051,\"start\":52035},{\"end\":52381,\"start\":52375},{\"end\":52390,\"start\":52381},{\"end\":52399,\"start\":52390},{\"end\":52408,\"start\":52399},{\"end\":52415,\"start\":52408},{\"end\":52424,\"start\":52415},{\"end\":52430,\"start\":52424},{\"end\":52932,\"start\":52924},{\"end\":52942,\"start\":52932},{\"end\":52951,\"start\":52942},{\"end\":52961,\"start\":52951},{\"end\":53213,\"start\":53204},{\"end\":53227,\"start\":53213},{\"end\":53238,\"start\":53227},{\"end\":53247,\"start\":53238},{\"end\":53571,\"start\":53562},{\"end\":53577,\"start\":53571},{\"end\":53583,\"start\":53577},{\"end\":53592,\"start\":53583},{\"end\":53600,\"start\":53592},{\"end\":53609,\"start\":53600},{\"end\":53622,\"start\":53609},{\"end\":54082,\"start\":54073},{\"end\":54089,\"start\":54082},{\"end\":54095,\"start\":54089},{\"end\":54103,\"start\":54095},{\"end\":54112,\"start\":54103},{\"end\":54565,\"start\":54552},{\"end\":54681,\"start\":54673},{\"end\":54689,\"start\":54681}]", "bib_venue": "[{\"end\":32836,\"start\":32769},{\"end\":33303,\"start\":33254},{\"end\":33617,\"start\":33551},{\"end\":33942,\"start\":33882},{\"end\":34172,\"start\":34109},{\"end\":34519,\"start\":34459},{\"end\":34914,\"start\":34837},{\"end\":35429,\"start\":35333},{\"end\":35817,\"start\":35769},{\"end\":36061,\"start\":35998},{\"end\":36402,\"start\":36335},{\"end\":36840,\"start\":36791},{\"end\":37159,\"start\":37112},{\"end\":37457,\"start\":37408},{\"end\":37806,\"start\":37775},{\"end\":38191,\"start\":38142},{\"end\":38512,\"start\":38415},{\"end\":38849,\"start\":38773},{\"end\":39161,\"start\":39089},{\"end\":39470,\"start\":39408},{\"end\":39694,\"start\":39656},{\"end\":39927,\"start\":39860},{\"end\":40237,\"start\":40221},{\"end\":40531,\"start\":40439},{\"end\":40921,\"start\":40844},{\"end\":41277,\"start\":41203},{\"end\":41691,\"start\":41614},{\"end\":42009,\"start\":41967},{\"end\":42240,\"start\":42187},{\"end\":42493,\"start\":42444},{\"end\":42756,\"start\":42686},{\"end\":43137,\"start\":43086},{\"end\":43576,\"start\":43499},{\"end\":44105,\"start\":44041},{\"end\":44521,\"start\":44422},{\"end\":44827,\"start\":44769},{\"end\":45137,\"start\":45091},{\"end\":45520,\"start\":45514},{\"end\":45896,\"start\":45847},{\"end\":46239,\"start\":46162},{\"end\":46634,\"start\":46576},{\"end\":46894,\"start\":46802},{\"end\":47316,\"start\":47248},{\"end\":47776,\"start\":47732},{\"end\":48122,\"start\":48073},{\"end\":48450,\"start\":48406},{\"end\":48730,\"start\":48691},{\"end\":48996,\"start\":48956},{\"end\":49250,\"start\":49221},{\"end\":49557,\"start\":49493},{\"end\":49856,\"start\":49789},{\"end\":50140,\"start\":50075},{\"end\":50529,\"start\":50452},{\"end\":50986,\"start\":50934},{\"end\":51261,\"start\":51188},{\"end\":51524,\"start\":51508},{\"end\":51767,\"start\":51724},{\"end\":51991,\"start\":51906},{\"end\":52507,\"start\":52430},{\"end\":52922,\"start\":52844},{\"end\":53291,\"start\":53247},{\"end\":53689,\"start\":53622},{\"end\":54189,\"start\":54112},{\"end\":54550,\"start\":54468},{\"end\":54797,\"start\":54689},{\"end\":32890,\"start\":32838},{\"end\":34978,\"start\":34916},{\"end\":35512,\"start\":35431},{\"end\":36456,\"start\":36404},{\"end\":40985,\"start\":40923},{\"end\":41755,\"start\":41693},{\"end\":43640,\"start\":43578},{\"end\":44156,\"start\":44107},{\"end\":46303,\"start\":46241},{\"end\":47371,\"start\":47318},{\"end\":49608,\"start\":49559},{\"end\":50593,\"start\":50531},{\"end\":52571,\"start\":52509},{\"end\":53743,\"start\":53691},{\"end\":54253,\"start\":54191}]"}}}, "year": 2023, "month": 12, "day": 17}