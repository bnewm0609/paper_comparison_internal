{"id": 236912875, "updated": "2023-10-06 00:17:18.514", "metadata": {"title": "Online Knowledge Distillation for Efficient Pose Estimation", "authors": "[{\"first\":\"Zheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jingwen\",\"last\":\"Ye\",\"middle\":[]},{\"first\":\"Mingli\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Zhigeng\",\"last\":\"Pan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Existing state-of-the-art human pose estimation methods require heavy computational resources for accurate predictions. One promising technique to obtain an accurate yet lightweight pose estimator is knowledge distillation, which distills the pose knowledge from a powerful teacher model to a less-parameterized student model. However, existing pose distillation works rely on a heavy pre-trained estimator to perform knowledge transfer and require a complex two-stage learning procedure. In this work, we investigate a novel Online Knowledge Distillation framework by distilling Human Pose structure knowledge in a one-stage manner to guarantee the distillation efficiency, termed OKDHP. Specifically, OKDHP trains a single multi-branch network and acquires the predicted heatmaps from each, which are then assembled by a Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in reverse. Instead of simply averaging the heatmaps, FAU which consists of multiple parallel transformations with different receptive fields, leverages the multi-scale information, thus obtains target heatmaps with higher-quality. Specifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to minimize the discrepancy between the target heatmaps and the predicted ones, which enables the student network to learn the implicit keypoint relationship. Besides, an unbalanced OKDHP scheme is introduced to customize the student networks with different compression rates. The effectiveness of our approach is demonstrated by extensive experiments on two common benchmark datasets, MPII and COCO.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.02092", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/LiYS0P21", "doi": "10.1109/iccv48922.2021.01153"}}, "content": {"source": {"pdf_hash": "2d9a0e0abdb8a34bcded395fb081868415d51808", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.02092v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cd77c940dbe4162b534c3690b039dae714c57193", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2d9a0e0abdb8a34bcded395fb081868415d51808.txt", "contents": "\nOnline Knowledge Distillation for Efficient Pose Estimation\n\n\nZheng Li lizheng1@stu.hznu.edu.cn \nHangzhou Normal University\n\n\nJingwen Ye \nZhejiang University\n\n\nMingli Song \nZhejiang University\n\n\nYing Huang \nHangzhou Normal University\n\n\nZhigeng Pan zgpan@hznu.edu.cn \nHangzhou Normal University\n\n\nOnline Knowledge Distillation for Efficient Pose Estimation\n\nExisting state-of-the-art human pose estimation methods require heavy computational resources for accurate predictions. One promising technique to obtain an accurate yet lightweight pose estimator is knowledge distillation, which distills the pose knowledge from a powerful teacher model to a less-parameterized student model. However, existing pose distillation works rely on a heavy pre-trained estimator to perform knowledge transfer and require a complex two-stage learning procedure. In this work, we investigate a novel Online Knowledge Distillation framework by distilling Human Pose structure knowledge in a one-stage manner to guarantee the distillation efficiency, termed OKDHP. Specifically, OKDHP trains a single multi-branch network and acquires the predicted heatmaps from each, which are then assembled by a Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in reverse. Instead of simply averaging the heatmaps, FAU which consists of multiple parallel transformations with different receptive fields, leverages the multi-scale information, thus obtains target heatmaps with higher-quality. Specifically, the pixelwise Kullback-Leibler (KL) divergence is utilized to minimize the discrepancy between the target heatmaps and the predicted ones, which enables the student network to learn the implicit keypoint relationship. Besides, an unbalanced OKDHP scheme is introduced to customize the student networks with different compression rates. The effectiveness of our approach is demonstrated by extensive experiments on two common benchmark datasets, MPII and COCO.\n\nIntroduction\n\nHuman pose estimation aims to recognize and localize all the human anatomical keypoints in a single RGB image. It's a fundamental technique for high-level vision tasks, such as action recognition [11], virtual reality [44] and human-computer interaction. Since the invention of DeepPose [55], deep neural networks have been the dom-* Corresponding author inant solution for human pose estimation, based on which, the approaches [57,61,52] focus on exploiting richer representations with a sequential architecture and achieve stateof-the-art performance. However, the gains of such deep learning based approaches often come with a cost of training and deploying the over-parameterized models, which limits the deployment in resource-intensive mobile devices. To reduce the computation cost and enhance the model efficiency, many efforts have been devoted to directly designing lightweight and real-time networks, e.g., PAF [4], VNect [36] and MultiPoseNet [27].\n\nAs another powerful tool to achieve a good trade-off between speed and accuracy, knowledge distillation [19] follows the teacher-student paradigm. Traditional distillation utilizes a two-stage scheme that starts with a cumbersome pre-trained teacher model, then distills the knowledge to a compact student model. In the field of pose estimation, recent works [66,22] adopt a traditional offline distillation scheme which distills the knowledge from a large pre-trained pose estimator (teacher) to a lightweight pose estimator (student) as shown in Fig. 1(a). However, training such a heavy teacher model is time-consuming and a high-capacity model is not always available. Thus, online counterparts [67,68] are proposed to simplify the distillation process to one stage, reducing the demand for the pre-trained teacher model. In ONE [68], a strong teacher model is established on-the-fly and all students share the same target distribution by averaging the predictions of all branches with learnable weights. Prior impressive works are mostly devoted to classification tasks, which neglect the valuable structural knowledge in the pixel-level tasks. Thus, our work focuses on the more challenging pixel-level tasks and proposes the first online pose distillation framework.\n\nExisting pixel-level distillation works [66,22] use mean squared error (MSE) as the distillation loss which is weak for knowledge transfer. It can not effectively measure the relative entropy between two probability distributions. Besides, MSE is used as the loss function of both task-specific supervised term and distillation term. These two loss terms have different optimization targets, one is the ground truth heatmap, the other is the predicted heatmap generated by the teacher. The conflict between two loss terms will deviate the optimization into a sub-optimal situation.\n\nTo alleviate those limitations, we investigate an online pose distillation approach for efficient pose estimation. The proposed method has two vital aspects for efficiency. One is that we simplify the distillation procedure to one stage. The other one is that the proposed method significantly improves the pose estimation accuracy comparing with the original network. The whole framework is constructed with a Feature Aggregation Unit (FAU) and multiple auxiliary branches, where each branch is treated as a student. The student branch can be both the same or heterogeneous architectures, making up the OKDHP-Balance and the OKDHP-Unbalance architectures respectively and enabling the customization of different compression rates. The teacher is established by the weighted ensemble of the predictions of all students through the FAU. The FAU here captures the multi-scale information to obtain the higher-quality target heatmap.\n\nBesides, to transfer the pose structural knowledge, the pixel-wise KL divergence loss is utilized to minimize the discrepancy between the target heatmaps and the predicted ones. In the final deployment, the target single-branch network is acquired by simply removing redundant auxiliary branches from the trained multi-branch network, which doesn't introduce any test-time cost increase.\n\nThe main contributions of this paper are listed below.\n\n\u2022 To our best knowledge, we are the first to propose the online pose distillation approach, which distills the pose structure knowledge in one-stage manner.\n\n\u2022 Both balanced and unbalanced versions of OKDHP are introduced, which can customize the target network with different compressing rates.\n\n\u2022 Extensive experiments validate the effectiveness of our proposed method on two popular benchmark datasets: MPII and COCO.\n\n\nRelated Work\n\nHuman Pose Estimation Classical human pose estimation approaches mainly adopt the technique of pictorial structures [14,2,23,45,46] and graphical models [50,49,7,10]. With the rapid development of deep convolutional neural networks [28,51,17], approaches based on CNNs became popular in recent years [54,57,12,39,52,3]. DeepPose proposed by Toshev et al. [55] was the first attempt to regress the coordinates of body parts directly and shows superior performance than classical approaches. Tompson et al. [54] learned body structures by jointly optimize the convnets and graphical models. CPM [57] incorporate convolutional networks into the pose machine framework for the task of human pose estimation and directly performs pose matching on the heatmaps. Newell et al. [38] stacked several hourglass modules to iteratively refine the predictions. Intermediate supervision is also used to produce accurate intermediate heatmaps and prevent gradient vanishes. The hourglass module is highly related to convdeconv architecture [35,48]. Features in this module are first pooled down to a low resolution, then are upsampled and fused with high-resolution features. Chu et al. [12] try to incorporate hourglass networks with attention mechanisms to learn and infer contextual representations. Yang et al. [61] further improved its performance by using the pyramid residual model.\n\nIn addition to heavy networks for highly accurate pose estimation, highly efficient pose estimation networks have also been studied to meet the needs of real applications. Cao et al. [4] introduced a real-time estimation network with two branches where one branch generates the heatmap predictions, while the other one generates part affinity field, then a greedy algorithm is used to group the joints to the corresponding person. Kocabas et al. [27] proposed pose residual network that takes as input keypoints and person detections then perform keypoints assignment. MultiPoseNet achieves similar accuracy to Mask-RCNN [16] while being at least 4x faster. Based on OpenPose [4], [42] uses a Mo-bileNet [21] as a backbone network and adopt lightweight refinement stage to reduce computational cost.\n\nKnowledge Distillation Originally introduced by Hinton et al. [19], knowledge distillation transfers knowledge in the form of soft predictions from a large and computational expensive model to a single computational efficient model through a learning procedure. When training the target student model, this method makes full use of the extra supervisory signal provided by the soft output of the teacher model. In FitNet [47], the student was forced to mimic the intermediate feature representations of the teacher. AT [64] try to transfer attention map of the teacher to the student. Kim et al. [25] introduces the paraphraser and translator network to assist the knowledge transfer procedure. In FSP [63], the student mimics the teacher's flow matrices, which are calculated as the inner product between feature maps from two layers. Traditional distillation methods always start with a powerful and cumbersome teacher model and perform one-way knowledge transfer to a compact student model. Online knowledge distillation [67,68] simplifies the complex two-stage procedure by reducing the need for a pre-trained teacher model. ONE [68] builds a single multi-branch network and each branch learns from the ensemble results. Chen et al. [5] introduces the two-level distillation framework and uses a self-attention mechanism to construct diverse peer networks. Li et al. [32] made a further improvement to such branch-based network by enhancing the branch diversity.\n\nKnowledge distillation methods have been widely used in many vision tasks, including object detection [30,6,13], line detection [20], semantic segmentation [62,18,34] and human pose estimation [66,40,56,58]. DOPE [58] proposes to distill the 2D and 3D poses from three independent body part expert models to the single whole-body pose detection model. Nie et al. [40] distill the pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Wang et al. distill the 3D pose knowledge from Non-Rigid Structure from Motion in weakly supervised learning. FPD [66] adopts the classical distillation approach and transfers the knowledge from an 8-Stack hourglass network to a lightweight 4-Stack hourglass network. Two shortcomings exist in the above work. A high-capacity teacher model is not always available and such complex two-stage learning will make the distillation inefficient.\n\n\nMethodology\n\nIn this section, we first present a brief introduction to knowledge distillation, then we describe our proposed online knowledge distillation framework for efficient human pose estimation. Finally, we introduce the unbalanced version of our proposed OKDHP.\n\n\nTeacher-Student Learning\n\nKnowledge distillation [19], as one of the main model compression techniques [59,37], follows the classic teacher-student learning paradigm. By treating a pre-trained heavy network as the teacher model, knowledge distillation aims to learn a lightweight student model, which is expected to master the expertise of the teacher, via transferring the knowledge from the teacher. Such distillation procedure can be formulated as:\nL kd = d(m stu , m tea ),(1)\nwhere d(\u00b7) denotes the distance loss function, measuring the differences between two probability distributions. m stu and m tea represent the results generated by the student and the teacher, respectively. With the task-specific supervised loss L task , the whole loss function is given as:\nL total = L task + \u03bbL kd ,(2)\nwhere \u03bb is the hyperparameter for balancing the two loss terms. The vanilla knowledge distillation is a two-stage procedure where a cumbersome teacher model is first trained and fixed, and then the knowledge is distilled to a compact student model. This process increases the training complexity, making the distillation process inefficient.\n\n\nOnline Human Pose Distillation\n\nTo solve the problems in the vanilla distillation method, we propose an online knowledge distillation framework for efficient pose estimation. An overview of the proposed framework is illustrated in Fig. 2. The proposed OKDHP architecture contains a multi-branch network as the main network and an FAU module for building the teacher online. We adopt the Hourglass network [38] (HG) as our basic building block in the proposed framework, which is the most common block used in many state-of-the-art works [12,24,29].\n\n\nThe Main Network\n\nThe main network is in the multi-branch architecture that consists of T auxiliary homogeneous branches with the same network configuration (the same number of HG modules). That is, a total of T pose estimators are aggregated in the main network, each of which shares the first n 0 HG modules and is treated as a student. For every 1 \u2264 t \u2264 T , branch-t has n t individual HG modules. To make the method clearer, we firstly give the details of the OKDHP-balanced, where n 1 = n 2 = ... = n T .\n\nThus, given a single RGB image, human pose estimation estimates a heatmap for each human anatomical keypoint, which represents the keypoint locations as Gaussian peaks. To train the multi-branch main network, we minimize the mean squared error (MSE) between the predicted heatmaps m pred from each branch and the ground-truth heatmaps m gt :\nL mse = 1 C T t=0 C c=1 \u2225m c gt \u2212 m c pred (t)\u2225 2 2 ,(3)\nwhere C denotes the total number of human keypoints i.e. heatmap channels and T denotes the total number of network branches. m c pred (t) is the predicted heatmap from branch-t at the c-th channel. Note that our network is built by stacking multiple hourglass modules, the supervision is applied not only on the final output but also on the intermediate heatmaps from each HG module.\n\n\nThe FAU Module\n\nThe Feature Aggregation Unit (FAU) learns to combine all the predicted heatmaps from T branches to establish a strong teacher model. The FAU module consists of multiple parallel transformations with different receptive fields, leveraging both local and global information to obtain accurate target heatmaps. The architecture of the proposed FAU is depicted in Fig. 3.\n\nPrevious image classification work [68] uses a simple conv block as the gate module to generate an importance score for each branch. But a simple conv block cannot effectively capture the contextual representation due to the body scale variation problem that exists in the natural scene. The multi-scale information is required to handle this problem. In this work, we focus on effectively capturing the multi-scale information to generate the target heatmaps with higher-quality. Inspired by the previous works [31,15], we propose the FAU which is composed of multiple parallel transformations with different receptive fields. As shown in Fig. 3, multiple conv blocks in FAU have different receptive fields and are arranged in parallel. We take as input the features after the main network Conv block which contains more original information. The convolution operation starts with a small kernel size of 3, then consistently increases in the following branches (size of 3,5,7). In our network, additional 1\u00d71 convolutions are mainly used as dimensional reduction methods to save computational resources. We further combine the average pooling of original inputs for richer representations. Then we concatenate all the splits and obtain the intermediate vector v, denoted as:\nv = [v avg , g([v conv3 , v conv5 , v conv7 ])],(4)\nwhere g(\u00b7) denotes the global pooling function. v conv3 , v conv5 , v conv7 denote the results from the conv path with kernel size 3, 5 and 7, respectively. v avg denotes the results from the average pooling path. For any input feature maps, this configuration creates multi-scale features with each conv path that are aggregated to capture richer information for both local and global fields. We pass the intermediate vector v through the fully connected layer F C to fuse the information from different paths. Then, a channel-wise softmax operator is applied to obtain the soft attention vectors a t,c . In the case of three network branches, for c-th heatmap we have a 1,c + a 2,c + a 3,c = 1,\n\nwhere c=1, 2, ..., C. Finally, we fuse predictions from multiple branches via an element-wise summation to obtain the weighted target heatmaps m tar :\nm tar = T t=1 a t \u2297 m t s ,(6)\nwhere a t = [a 1 , a 2 , ..., a c ] t , m t s = [m 1 s , m 2 s , ..., m c s ] t and m t \u2208 R H \u2032 \u00d7W \u2032 \u00d7C . Here, a t is the weight for the t-th branch, m t s is the heatmaps generated by the t-th branch and \u2297 refers to the channel-wise multiplication between a t and m t s . Our experiments (see Section 4.4) prove that the weights generated by FAU can achieve better distillation performance. \n\n\nPixel-wise Distillation\n\nA proper distillation loss function is critical to the whole training procedure. Since pixel values on the heatmaps indicate the probabilities of pixels that belong to the keypoint. We align the heatmap generated by the student model with the target heatmaps. The target heatmaps obtained through the FAU play the role of a teacher model to teach each branch model (student) in our method. To transfer the pose structural knowledge, the pixel-wise Kullback-Leibler (KL) divergence loss is utilized to minimize the divergence between the heatmaps of the teacher model and the student model as follows:\nL kl = 1 W \u2032 \u00d7 H \u2032 i\u2208M T t=0 KL(q i tar , q(t) i s ),(7)\nwhere W \u2032 and H \u2032 represent the heatmaps' width and height. q i tar and q(t) i s denote the probabilities of the i-th pixel from the heatmap generated by the teacher model and the student model, respectively. M = {1, 2, ..., W \u2032 \u00d7 H \u2032 } denotes all the pixels.\n\nOverall To get a better understanding of our method, we describe the whole training procedure in Algorithm 1. For the proposed online human pose distillation method, the whole objective function consists of a conventional mean squared error loss L mse for pose estimation and another loss term L kl for online knowledge distillation:\nL total = \u03b1L mse + \u03b2L kl .(8)\nwhere \u03b1 and \u03b3 are the hyperparameters to balance these two losses. Compute the target heatmaps m t through FAU; 4: Compute the MSE loss L mse ; 5: Compute the distillation loss L kl ; 6: Compute the total loss function; \n\n\nAlgorithm 1 Online Human Pose Distillation\n\n\nUnbalanced Architecture\n\nTo achieve a better distillation performance, a stronger teacher model is required. But in our balanced architecture, the teacher is fixed once we set up the target network. Here, the unbalanced variant is introduced to customize the student model with different compression rates, as shown in Fig. 1(c). For an unbalanced OKDHP architecture, each branch have different numbers of HG modules, where n 1 \u0338 = n 2 \u0338 = ... \u0338 = n T . For example, for a 3-branch unbalanced network, if a 4-Stack HG network is required for final deployment, the other two branches can be set as a 6- the balanced structure, a better teaching performance can be achieved by utilizing the stronger representation ability of a larger network. Furthermore, the other benefit is that we can simultaneously obtain three different networks with comparable performances in one training procedure. This kind of network setting can be customized to other settings according to the actual needs. We demonstrate the effectiveness of unbalanced structure in Table 3 and present the detailed results in Table 5.\n\n\nExperiments\n\nTo validate the effectiveness of our proposed method, we conduct several experiments on two popular human pose datasets, MPII [1] and COCO [33].\n\n\nImplementation Details\n\nDatasets The MPII dataset includes approximately 25K images containing over 40K subjects with annotated body joints, where 29K subjects are used for training and 11K subjects are used for testing. The images were collected using an established taxonomy of everyday human activities from YouTube videos. We adopted the same train/valid/test split as in [66]. Each person instance in MPII has 16 labeled joints.\n\nThe COCO keypoint dataset [33] presents naturally challenging imagery data with various poses. It contains more than 200k images and 250k person instances labeled with keypoints. In test, we follow the commonly used train/val/ test split. Each person instance is labeled with 17 joints.\n\nTraining details We implement all the methods in Py-Torch [43]. For MPII, we resize the cropped images to 256\u00d7256 in pixels. Then we randomly augment the data with rotation degrees in [-30 \u2022 , 30 \u2022 ], scaling with factors in [0.75, 1.25] and horizontal flip. For COCO, we resize the cropped image to 256\u00d7192 in pixels. Then we apply random horizontal flip, random rotation with degrees in [-40 \u2022 , 40 \u2022 ] and random scale with factors in [0.7, 1.3]. We follow the standard data processing scheme for all images as in [66]. Adam [26] is used as the optimizer and we set the  initial learning rate to 2.5e-4, weight decay to 1e-4. The learning rate is divided by 10 at 90 and 120 of the total 150 training epochs. We usually set \u03b1=1 and \u03b2=2 in Eqn.8. For the network architecture, we set the number of shared HG modules to half the number of total stacks. In the case of a 4-Stack OKDHP network, we have two shared HG modules and two individual HG modules for each branch. We set branch size to 3 as default. We use the OKDHP-balance architecture as our default scheme in the following experiments unless we specified. We adopt the official hourglass configurations as our baseline method in all experiments. Code will be released soon.\n\nEvaluation Metric We use the standard Percentage of Correct Keypoint (PCK) metric which reports the percentage of correct keypoints lies within a normalized distance of ground truth. We use PCKh@0.5 for the MPII dataset, which refers to a threshold of 50% of the head diameter. For COCO, we use Object Keypoints Similarity (OKS) as our evaluation metric, which defines the similarity between different human poses. \n\n\nResults on MPII dataset\n\nWe evaluate our method on the MPII dataset. Table 1 compares the PCKh@0.5 accuracy results of state-of-theart methods and our proposed OKDHP on the MPII testing set. Table 2 reports the comparison of three varyingcapacity networks trained by the conventional method and our proposed OKDHP. We can clearly observe that all networks benefit from our OKDHP training method, particularly for small networks achieving large performance gains. Specifically, our method improves various baseline networks ranging from 0.3 to 0.8. Considering that the performance of many state-of-the-art pose estimation networks, improves from 0.1% to 0.3% in PCKh scores, our performance is in fact, significant as compared to prior works. A 2-Stack HG network trained with OKDHP achieves similar performance with the original 4-Stack HG network but it only needs half the number of HG modules. In contrast to the conventional distillation method, a large pre-trained teacher is not necessary. We provide the visualized pose results in Fig. 4.\n\nWe compare our method with previous state-of-the-art distillation work FPD [66] and demonstrate the performance comparison of our proposed balanced and unbalanced structure in Table 3. The teacher network is an 8-Stack HG network with 90.2 PCKh@0.5 scores for FPD. We can clearly observe that both OKDHP balanced and unbalanced architectures outperforms the FPD method, validating the performance advantage of our method. The unbalanced structure outperforms the balanced structure by 0.2% but with 36%(17/47) FLOPS increase. OKDHP-Balance takes the least training cost, proves that our method is the most effective pose distillation approach.\n\nWe provide the detailed results of our proposed OKDHP-Unbalance architecture in Table 5. Three branches exists in our network. The first one is the target 4-Stack hourglass network for deployment. The other two branches plays the auxiliary roles, helping the target branch to achieve better performance. Table 4 shows the results of the baseline method and our proposed OKDHP on the MS COCO keypoint dataset. In the test, a two-stage top-down paradigm is applied, same as in [60,9,65]. We use a detector with detection AP 56.4 for the person category on COCO val2017. From this table, we can observe that OKDHP method yields a more generalizable model compared to independent learning. This demonstrates that our method can still be applied to the large-scale dataset effectively.\n\n\nResults on COCO dataset\n\n\nAblation Study\n\nLoss Function The distillation loss function plays a critical role in the whole learning procedure. We compared the performance of different distillation loss functions as shown in Table 6. FPD uses mean squared error (MSE) loss as the distillation term which is the same as the task-specific supervised term. We test the MSE loss in our proposed framework. In Table 6, this result shows that our proposed pixelwise KL divergence is a better choice in comparison to the MSE. Our method can effectively distill the pose structural knowledge to enhance distillation performance.\n\nBranch Size We evaluate the impact of branch size on the performance of our branch-based online pose distillation framework. Table 7 shows the performance on MPII validation set with varying branch sizes ranging from 2 to 5. We omit the case when branch size n = 1 since one branch cannot form the ensemble result. The baseline method denotes the vanilla 2-Stack networks without any modifications. We can clearly observe that OKDHP scales well with more branches and a 2-Stack OKDHP can be further improved if a larger branch size is allowed during training. \n\n\nIndividual HG Number\n\nWe set n s = 2 and n i = 2 for a 4-Stack OKDHP network in the main experiment, indicating that we have two shared HG modules and two individual HG modules for each branch. Table 8 demonstrated the impact of the individual HG numbers on MPII validation set. From this table, we can see that if very few HG modules are shared, the performance will quickly drop. This brings the concept of branch diversity for such branch-based networks as mentioned in [5,32]. Diversity will be hurt with the reducing number of individual HG modules, which will limit the effectiveness of within-group knowledge transfer. We usually set the number of shared and individual modules to half the number of total stacks to achieve the accuracyefficiency trade-offs. Sensitivity to Hyperparameter Table 9 demonstrates how the performance of our proposed framework is affected by the choice of hyperparameter \u03b2 in Eqn.8. From this ta-ble, we can see that our method still has robust performance against varying \u03b2 values ranging from 0.5 to 5.  Table 9. Sensitivity to \u03b2 for a 2-Stack OKDHP network on MPII validation set (PCKh@0.5).\n\nFAU The goal of FAU is to generate accurate target heatmaps by weighted ensemble heatmaps from all auxiliary branches. To evaluate the effectiveness of our proposed FAU, we conduct various ablation studies on MPII validation set based on a 4-Stack HG network as shown in Table 10. We compare the performance of the following experiments. (1) Baseline: A vanilla 4-Stack HG network without any modification. (2) Mean: A simple average is applied to aggregate the heatmaps of all branches. (3) Gate: A simple attention module used in ONE [68] for the classification task. It was initially proposed for image classification. We reimplement this module so that it can be directly used in pose estimation network. (4) FAU: Our proposed module. We can see that FAU outperforms Mean and Gate by 0.3% and 0.2%, respectively. This confirms the usefulness of the FAU. \n\n\nConclusion\n\nIn this paper, we propose a novel Online Knowledge Distillation framework by distilling Human Pose structure knowledge (OKDHP) in the one-stage manner. A network with multiple branches is utilized in the framework, where each branch is an independent pose estimator and is regarded as the student. The students from multiple branches are integrated into one teacher by the FAU module, which then optimizes the student branches in reverse. With OKDHP, the efficiency is significantly enhanced with reduced distillation complexity and improved model performance. Besides, the unbalanced OKDHP scheme is also introduced to enable the customization of the target network with different compression rates. Experiments have validated the effectiveness of our proposed OKDHP on two popular benchmark datasets.\n\nFigure 1 .\n1To obtain an efficient 4-Stack network, (a) FPD [66] adopts a two-stage distillation scheme from the static pre-trained 8-Stack network. The proposed OKDHP distills the pose structural knowledge with both (b) Balance and (c) Unbalance architectures in one stage. The teacher is established online with the FAU.\n\nFigure 2 .\n2An overview of the proposed Online Knowledge Distillation for Human Pose estimation (OKDHP). Each branch serves as an independent pose estimator. The FAU learns to ensemble all branches to establish a stronger teacher model. L kl denotes the KL divergence loss between intermediate heatmaps and ensemble heatmaps. We omit the conventional mean squared error loss Lmse for simplicity.\n\nFigure 3 .\n3This module is proposed to effectively exploit the multi-scale information that can obtain target heatmaps with higher-quality. mi denotes the predicted heatmaps that come from i-th branch. The final ensemble heatmaps are obtained by the weighted sum of heatmaps of all individual branches. Note that all conv blocks in the network are composed of regular convolutions, Batch Normalization and ReLU activation functions in sequence.\n\n\nInput: Labelled Training dataset D; Training Epoch Number \u03f5; Branch Number T ; Network Structure S \u2208 {S balance , S unbalance } Output: Trained target pose estimate network \u03b8 1 and other auxiliary estimators {\u03b8 i } T i=2 Initialize: Epoch e=1; Randomly initialize {\u03b8 i } T i=1 1: while e \u2264 \u03f5 do 2: Compute the heatmap predictions of all branches {\u03b8 i } T i=1 according to S; 3:\n\nFigure 4 .Table 6 .\n46Visualized results on MPII dataset. Loss Head Sho. Elbo. Wri. Hip Knee Ank. PCK MSE 96.Comparisons of different distillation loss functions on MPII validation set (PCKh@0.5).\n\n\nUse target pose estimator \u03b8 1 ;7: \n\nUpdate the model parameters {\u03b8 i } T \ni=1 ; \n\n8: \n\ne=e+1; \n9: end while \nModel deployment: \n\n\nstack and an 8-Stack network. Compared with Method Head Sho. Elb. Wri. Hip Knee Ank. Mean Newell et al.(HG) [ECCV'16] [38] 98.2 96.3 91.2 87.1 90.1 87.4 83.6 \n90.9 \nNing et al. [TMM'17 [41]] \n98.1 96.3 92.2 87.8 90.6 87.6 82.7 \n91.2 \nChu et al. [CVPR'17] [12] \n98.5 96.3 91.9 88.1 90.6 88.0 85.0 \n91.5 \nChen et al. [ICCV'17] [8] \n98.1 96.5 92.5 88.5 90.2 89.6 86.0 \n91.9 \nYang et al. [ICCV'17] [61] \n98.5 96.7 92.5 88.7 91.1 88.6 86.0 \n92.0 \nKe et al. [ECCV'18] [24] \n98.5 96.8 92.7 88.4 90.6 89.3 86.3 \n92.1 \nTang et al. [ECCV'18] [53] \n98.4 96.9 92.6 88.7 91.8 89.4 86.2 \n92.3 \nFPD [CVPR'19] [66] \n98.3 96.4 91.5 87.4 90.9 87.1 83.7 \n91.1 \nOKDHP \n98.2 96.6 92.3 88.0 91.0 88.5 84.5 \n91.7 \n\nTable 1. Evaluation of our proposed OKDHP on MPII testing set (PCKh@0.5). \n\n\n\nTable 2 .\n2PCKh@0.5 score of our proposed OKDHP on MPII vali-\ndation set. \n\nMethod \nPCKh@0.5 TrainCost \nBaseline \n89.2 \n14 \nFPD \n89.7 \n66 \nOKDHP-Balance \n90.0 \n47 \nOKDHP-Unbalance \n90.2 \n64 \n\nTable 3. Comparison with different distillation methods based on \nthe 4-Stack hourglass network on MPII validation set. TrainCost: \nTraining cost in the unit of GFLOPS. \n\n\n\n\nTable 7. Impact of branch size for a 2-Stack OKDHP framework on MPII validation set (PCKh@0.5).Branch Size PCKh@0.5 #Params \nBaseline \n88.6 \n13.0M \n2 \n89.2 \n15.5M \n3 \n89.2 \n18.6M \n4 \n89.3 \n21.7M \n5 \n89.4 \n24.7M \n\n\n\nTable 8 .\n8Impact of the number of individual HG modules for a 4-Stack OKDHP network on MPII validation set (PCKh@0.5).Individual HG numbers \n1 \n2 \n3 \n4 \nPCKh@0.5 \n89.8 90.0 90.1 90.2 \nFLOPs \n41G 47G 53G 59G \n\n\n\n\nTable 10. Ablation study on FAU module for a 4-Stack HG network on MPII validation set (PCKh@0.5).Baseline Mean Gate FAU \n89.2 \n89.7 \n89.8 90.0 \n\n\nhttps://github.com/YuliangXiu/MobilePose-pytorch\nSupplementary Material Additional ExperimentsWe further extend our proposed OKDHP to a popular light-weight and real-time pose estimation method called MobilePose 1 with three popular backbone networks (ResNet-18, MobileNet-V2, ShuffleNet-V2).Table 11reports the comparison of baseline method and our OKDHP. The MobilePose network can be roughly divided into two parts: the encoder (i.e. backbone network) and the decoder. The encoder extracts features from the input image. The decoder upsamples the features generated by the encoder, and obtains the keypoint heatmaps. In our proposed OKDHP, we share the encoder network and build multiple parallel decoder branches. The FAU module remains the same.From  The visualization of failed cases is shown inFig. 5. When the human body is occluded or the ambient light is insufficient, the predictions are still biased. Our proposed OKDHP method is a multi-branch online knowledge distillation framework based on a human pose estimation model. Although the model performance has been significantly improved after distillation, it is stilled limited by the inherited factors of the model itself.Failure Case Analysis\n2d human pose estimation: New benchmark and state of the art analysis. Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, Bernt Schiele, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2014.\n\nPictorial structures revisited: People detection and articulated pose estimation. Mykhaylo Andriluka, Stefan Roth, Bernt Schiele, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionIEEEMykhaylo Andriluka, Stefan Roth, and Bernt Schiele. Pic- torial structures revisited: People detection and articulated pose estimation. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 1014-1021. IEEE, 2009.\n\nUnipose: Unified human pose estimation in single images and videos. Bruno Artacho, Andreas Savakis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBruno Artacho and Andreas Savakis. Unipose: Unified hu- man pose estimation in single images and videos. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7035-7044, 2020.\n\nRealtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7291-7299, 2017.\n\nOnline knowledge distillation with diverse peers. Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, Chun Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceDefang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online knowledge distillation with diverse peers. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3430-3437, 2020.\n\nLearning efficient object detection models with knowledge distillation. Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, Manmohan Chandraker, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsGuobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man- mohan Chandraker. Learning efficient object detection mod- els with knowledge distillation. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 742-751, 2017.\n\nArticulated pose estimation by a graphical model with image dependent pairwise relations. Xianjie Chen, Alan Yuille, arXiv:1407.3399arXiv preprintXianjie Chen and Alan Yuille. Articulated pose estimation by a graphical model with image dependent pairwise rela- tions. arXiv preprint arXiv:1407.3399, 2014.\n\nAdversarial posenet: A structure-aware convolutional network for human pose estimation. Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, Jian Yang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionYu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian Yang. Adversarial posenet: A structure-aware convolu- tional network for human pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1212-1221, 2017.\n\nCascaded pyramid network for multi-person pose estimation. Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cascaded pyramid net- work for multi-person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 7103-7112, 2018.\n\nMixing body-part sequences for human pose estimation. Anoop Cherian, Julien Mairal, Karteek Alahari, Cordelia Schmid, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAnoop Cherian, Julien Mairal, Karteek Alahari, and Cordelia Schmid. Mixing body-part sequences for human pose es- timation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2353- 2360, 2014.\n\nP-cnn: Pose-based cnn features for action recognition. Guilhem Ch\u00e9ron, Ivan Laptev, Cordelia Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionGuilhem Ch\u00e9ron, Ivan Laptev, and Cordelia Schmid. P-cnn: Pose-based cnn features for action recognition. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision, pages 3218-3226, 2015.\n\nMulti-context attention for human pose estimation. Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, Xiaogang Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, and Xiaogang Wang. Multi-context attention for hu- man pose estimation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 1831-1840, 2017.\n\nRelation distillation networks for video object detection. Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, Tao Mei, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionJiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao Mei. Relation distillation networks for video object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7023- 7032, 2019.\n\nPictorial structures for object recognition. F Pedro, Felzenszwalb, P Daniel, Huttenlocher, International journal of computer vision. 611Pedro F Felzenszwalb and Daniel P Huttenlocher. Pictorial structures for object recognition. International journal of computer vision, 61(1):55-79, 2005.\n\nRes2net: A new multi-scale backbone architecture. Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip Hs Torr, Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip HS Torr. Res2net: A new multi-scale backbone architecture. IEEE transactions on pattern analysis and machine intelligence, 2019.\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nKnowledge adaptation for efficient semantic segmentation. Chunhua Tong He, Zhi Shen, Dong Tian, Changming Gong, Youliang Sun, Yan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionTong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, and Youliang Yan. Knowledge adaptation for efficient semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 578-587, 2019.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nLearning lightweight lane detection cnns by self attention distillation. Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy. Learning lightweight lane detection cnns by self at- tention distillation. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 1013-1021, 2019.\n\nG Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprintAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efficient convolu- tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nLightweight 3d human pose estimation network training using teacher-student learning. Dong-Hyun Hwang, Suntae Kim, Nicolas Monet, Hideki Koike, Soonmin Bae, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionDong-Hyun Hwang, Suntae Kim, Nicolas Monet, Hideki Koike, and Soonmin Bae. Lightweight 3d human pose es- timation network training using teacher-student learning. In Proceedings of the IEEE/CVF Winter Conference on Appli- cations of Computer Vision, pages 479-488, 2020.\n\nClustered pose and nonlinear appearance models for human pose estimation. Sam Johnson, Mark Everingham, bmvc. Citeseer2Sam Johnson and Mark Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In bmvc, volume 2, page 5. Citeseer, 2010.\n\nMulti-scale structure-aware network for human pose estimation. Lipeng Ke, Ming-Ching Chang, Honggang Qi, Siwei Lyu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Lipeng Ke, Ming-Ching Chang, Honggang Qi, and Siwei Lyu. Multi-scale structure-aware network for human pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 713-728, 2018.\n\nParaphrasing complex network: Network compression via factor transfer. Jangho Kim, Seounguk Park, Nojun Kwak, arXiv:1802.04977arXiv preprintJangho Kim, SeoungUK Park, and Nojun Kwak. Paraphras- ing complex network: Network compression via factor trans- fer. arXiv preprint arXiv:1802.04977, 2018.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nMultiposenet: Fast multi-person pose estimation using pose residual network. Muhammed Kocabas, Salih Karagoz, Emre Akbas, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Muhammed Kocabas, Salih Karagoz, and Emre Akbas. Mul- tiposenet: Fast multi-person pose estimation using pose residual network. In Proceedings of the European confer- ence on computer vision (ECCV), pages 417-433, 2018.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in Neural Information Processing Sys- tems, pages 1097-1105, 2012.\n\nSimple pose: Rethinking and improving a bottom-up approach for multi-person pose estimation. Jia Li, Wen Su, Zengfu Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jia Li, Wen Su, and Zengfu Wang. Simple pose: Rethinking and improving a bottom-up approach for multi-person pose estimation. In Proceedings of the AAAI Conference on Arti- ficial Intelligence, volume 34, pages 11354-11361, 2020.\n\nMimicking very efficient network for object detection. Quanquan Li, Shengying Jin, Junjie Yan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionQuanquan Li, Shengying Jin, and Junjie Yan. Mimicking very efficient network for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6356-6364, 2017.\n\nSelective kernel networks. Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec- tive kernel networks. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 510-519, 2019.\n\nOnline knowledge distillation via multi-branch diversity enhancement. Zheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer VisionZheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, and Zhigeng Pan. Online knowledge distillation via multi-branch diversity enhancement. In Proceedings of the Asian Conference on Computer Vision, 2020.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nStructured knowledge distillation for semantic segmentation. Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, Jingdong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2604-2613, 2019.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3431-3440, 2015.\n\nVnect: Real-time 3d human pose estimation with a single rgb camera. Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, Christian Theobalt, ACM Transactions on Graphics (TOG). 364Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt. Vnect: Real-time 3d human pose estimation with a single rgb cam- era. ACM Transactions on Graphics (TOG), 36(4):1-14, 2017.\n\nPruning convolutional neural networks for resource efficient inference. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz, arXiv:1611.06440arXiv preprintPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for re- source efficient inference. arXiv preprint arXiv:1611.06440, 2016.\n\nStacked hourglass networks for human pose estimation. Alejandro Newell, Kaiyu Yang, Jia Deng, European Conference on Computer Vision. SpringerAlejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour- glass networks for human pose estimation. In European Con- ference on Computer Vision, pages 483-499. Springer, 2016.\n\nHuman pose estimation with parsing induced learner. Xuecheng Nie, Jiashi Feng, Yiming Zuo, Shuicheng Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXuecheng Nie, Jiashi Feng, Yiming Zuo, and Shuicheng Yan. Human pose estimation with parsing induced learner. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2100-2108, 2018.\n\nDynamic kernel distillation for efficient pose estimation in videos. Xuecheng Nie, Yuncheng Li, Linjie Luo, Ning Zhang, Jiashi Feng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionXuecheng Nie, Yuncheng Li, Linjie Luo, Ning Zhang, and Jiashi Feng. Dynamic kernel distillation for efficient pose estimation in videos. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 6942-6950, 2019.\n\nKnowledgeguided deep fractal neural networks for human pose estimation. Guanghan Ning, Zhi Zhang, Zhiquan He, IEEE Transactions on Multimedia. 205Guanghan Ning, Zhi Zhang, and Zhiquan He. Knowledge- guided deep fractal neural networks for human pose estima- tion. IEEE Transactions on Multimedia, 20(5):1246-1259, 2017.\n\nReal-time 2d multi-person pose estimation on cpu. Daniil Osokin, arXiv:1811.12004Lightweight openpose. arXiv preprintDaniil Osokin. Real-time 2d multi-person pose esti- mation on cpu: Lightweight openpose. arXiv preprint arXiv:1811.12004, 2018.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in Neural Information Processing Systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pages 8026-8037, 2019.\n\nHuman identification using neural network-based classification of periodic behaviors in virtual reality. Duc-Minh Pham, 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEEDuc-Minh Pham. Human identification using neural network-based classification of periodic behaviors in virtual reality. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pages 657-658. IEEE, 2018.\n\nPoselet conditioned pictorial structures. Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLeonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele. Poselet conditioned pictorial structures. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 588-595, 2013.\n\nStrong appearance and expressive spatial models for human pose estimation. Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele, Proceedings of the IEEE international conference on Computer Vision. the IEEE international conference on Computer VisionLeonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele. Strong appearance and expressive spatial models for human pose estimation. In Proceedings of the IEEE international conference on Computer Vision, pages 3487-3494, 2013.\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241. Springer, 2015.\n\nModec: Multimodal decomposable models for human pose estimation. Ben Sapp, Ben Taskar, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBen Sapp and Ben Taskar. Modec: Multimodal decompos- able models for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3681, 2013.\n\nCascaded models for articulated pose estimation. Benjamin Sapp, Alexander Toshev, Ben Taskar, European Conference on Computer Vision. SpringerBenjamin Sapp, Alexander Toshev, and Ben Taskar. Cas- caded models for articulated pose estimation. In European Conference on Computer Vision, pages 406-420. Springer, 2010.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDeep high-resolution representation learning for human pose estimation. Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKe Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose es- timation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693- 5703, 2019.\n\nDeeply learned compositional models for human pose estimation. Wei Tang, Pei Yu, Ying Wu, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Wei Tang, Pei Yu, and Ying Wu. Deeply learned composi- tional models for human pose estimation. In Proceedings of the European conference on computer vision (ECCV), pages 190-206, 2018.\n\nJoint training of a convolutional network and a graphical model for human pose estimation. Jonathan Tompson, Arjun Jain, Yann Lecun, Christoph Bregler, arXiv:1406.2984arXiv preprintJonathan Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. arXiv preprint arXiv:1406.2984, 2014.\n\nDeeppose: Human pose estimation via deep neural networks. Alexander Toshev, Christian Szegedy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAlexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1653-1660, 2014.\n\nDistill knowledge from nrsfm for weakly supervised 3d pose learning. Chaoyang Wang, Chen Kong, Simon Lucey, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionChaoyang Wang, Chen Kong, and Simon Lucey. Distill knowledge from nrsfm for weakly supervised 3d pose learn- ing. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, pages 743-752, 2019.\n\nConvolutional pose machines. Shih-En, Varun Wei, Takeo Ramakrishna, Yaser Kanade, Sheikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4724-4732, 2016.\n\nDope: Distillation of part experts for whole-body 3d pose estimation in the wild. Philippe Weinzaepfel, Romain Br\u00e9gier, Hadrien Combaluzier, Vincent Leroy, Gr\u00e9gory Rogez, European Conference on Computer Vision. SpringerPhilippe Weinzaepfel, Romain Br\u00e9gier, Hadrien Com- baluzier, Vincent Leroy, and Gr\u00e9gory Rogez. Dope: Dis- tillation of part experts for whole-body 3d pose estimation in the wild. In European Conference on Computer Vision, pages 380-397. Springer, 2020.\n\nQuantized convolutional neural networks for mobile devices. Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4820- 4828, 2016.\n\nSimple baselines for human pose estimation and tracking. Bin Xiao, Haiping Wu, Yichen Wei, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 466-481, 2018.\n\nLearning feature pyramids for human pose estimation. Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionWei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Learning feature pyramids for human pose estimation. In proceedings of the IEEE international confer- ence on computer vision, pages 1281-1290, 2017.\n\nStudent becoming the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more. Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, Mingli Song, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, and Mingli Song. Student becoming the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more. IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 2824-2833, 2019.\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, net- work minimization and transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4133-4141, 2017.\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Sergey Zagoruyko, Nikos Komodakis, arXiv:1612.03928arXiv preprintSergey Zagoruyko and Nikos Komodakis. Paying more at- tention to attention: Improving the performance of convolu- tional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016.\n\nDistribution-aware coordinate representation for human pose estimation. Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, Ce Zhu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionFeng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu. Distribution-aware coordinate representation for human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7093-7102, 2020.\n\nFast human pose estimation. Feng Zhang, Xiatian Zhu, Mao Ye, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionFeng Zhang, Xiatian Zhu, and Mao Ye. Fast human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3517- 3526, 2019.\n\nDeep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYing Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4320-4328, 2018.\n\nKnowledge distillation by on-the-fly native ensemble. Xiatian Zhu, Shaogang Gong, Advances in Neural Information Processing Systems. Xiatian Zhu, Shaogang Gong, et al. Knowledge distillation by on-the-fly native ensemble. In Advances in Neural Infor- mation Processing Systems, pages 7517-7527, 2018.\n", "annotations": {"author": "[{\"end\":126,\"start\":63},{\"end\":160,\"start\":127},{\"end\":195,\"start\":161},{\"end\":236,\"start\":196},{\"end\":296,\"start\":237}]", "publisher": null, "author_last_name": "[{\"end\":71,\"start\":69},{\"end\":137,\"start\":135},{\"end\":172,\"start\":168},{\"end\":206,\"start\":201},{\"end\":248,\"start\":245}]", "author_first_name": "[{\"end\":68,\"start\":63},{\"end\":134,\"start\":127},{\"end\":167,\"start\":161},{\"end\":200,\"start\":196},{\"end\":244,\"start\":237}]", "author_affiliation": "[{\"end\":125,\"start\":98},{\"end\":159,\"start\":139},{\"end\":194,\"start\":174},{\"end\":235,\"start\":208},{\"end\":295,\"start\":268}]", "title": "[{\"end\":60,\"start\":1},{\"end\":356,\"start\":297}]", "venue": null, "abstract": "[{\"end\":1961,\"start\":358}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2177,\"start\":2173},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2199,\"start\":2195},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2268,\"start\":2264},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2409,\"start\":2405},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2412,\"start\":2409},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2415,\"start\":2412},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2902,\"start\":2899},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2914,\"start\":2910},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2936,\"start\":2932},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3047,\"start\":3043},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":3302,\"start\":3298},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3305,\"start\":3302},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":3642,\"start\":3638},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3645,\"start\":3642},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3776,\"start\":3772},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":4258,\"start\":4254},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4261,\"start\":4258},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6731,\"start\":6727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6733,\"start\":6731},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6736,\"start\":6733},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6739,\"start\":6736},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6742,\"start\":6739},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6768,\"start\":6764},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6771,\"start\":6768},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6773,\"start\":6771},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6776,\"start\":6773},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6847,\"start\":6843},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6850,\"start\":6847},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6853,\"start\":6850},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6915,\"start\":6911},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6918,\"start\":6915},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6921,\"start\":6918},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6924,\"start\":6921},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6927,\"start\":6924},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6929,\"start\":6927},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6970,\"start\":6966},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7120,\"start\":7116},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7208,\"start\":7204},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7385,\"start\":7381},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7640,\"start\":7636},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7643,\"start\":7640},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7787,\"start\":7783},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7915,\"start\":7911},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8173,\"start\":8170},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8437,\"start\":8433},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8612,\"start\":8608},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8666,\"start\":8663},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8672,\"start\":8668},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8695,\"start\":8691},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8854,\"start\":8850},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9213,\"start\":9209},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9311,\"start\":9307},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9388,\"start\":9384},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9494,\"start\":9490},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9816,\"start\":9812},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9819,\"start\":9816},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9925,\"start\":9921},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10028,\"start\":10025},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10163,\"start\":10159},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10362,\"start\":10358},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10364,\"start\":10362},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10367,\"start\":10364},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10388,\"start\":10384},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10416,\"start\":10412},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10419,\"start\":10416},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10422,\"start\":10419},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10453,\"start\":10449},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10456,\"start\":10453},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10459,\"start\":10456},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10462,\"start\":10459},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10473,\"start\":10469},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10623,\"start\":10619},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10855,\"start\":10851},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11504,\"start\":11500},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11558,\"start\":11554},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11561,\"start\":11558},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13006,\"start\":13002},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13138,\"start\":13134},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13141,\"start\":13138},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13144,\"start\":13141},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":14869,\"start\":14865},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15346,\"start\":15342},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15349,\"start\":15346},{\"end\":18857,\"start\":18855},{\"end\":18889,\"start\":18887},{\"end\":18929,\"start\":18927},{\"end\":19612,\"start\":19610},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20255,\"start\":20252},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20269,\"start\":20265},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20653,\"start\":20649},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20738,\"start\":20734},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21058,\"start\":21054},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":21517,\"start\":21513},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21528,\"start\":21524},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":23777,\"start\":23773},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":24822,\"start\":24818},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24824,\"start\":24822},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":24827,\"start\":24824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26785,\"start\":26782},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26788,\"start\":26785},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":27981,\"start\":27977}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29440,\"start\":29117},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29837,\"start\":29441},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30283,\"start\":29838},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30663,\"start\":30284},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30861,\"start\":30664},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30991,\"start\":30862},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31762,\"start\":30992},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32127,\"start\":31763},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32343,\"start\":32128},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32555,\"start\":32344},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32704,\"start\":32556}]", "paragraph": "[{\"end\":2937,\"start\":1977},{\"end\":4212,\"start\":2939},{\"end\":4795,\"start\":4214},{\"end\":5727,\"start\":4797},{\"end\":6116,\"start\":5729},{\"end\":6172,\"start\":6118},{\"end\":6330,\"start\":6174},{\"end\":6469,\"start\":6332},{\"end\":6594,\"start\":6471},{\"end\":7985,\"start\":6611},{\"end\":8786,\"start\":7987},{\"end\":10254,\"start\":8788},{\"end\":11176,\"start\":10256},{\"end\":11448,\"start\":11192},{\"end\":11902,\"start\":11477},{\"end\":12222,\"start\":11932},{\"end\":12594,\"start\":12253},{\"end\":13145,\"start\":12629},{\"end\":13657,\"start\":13166},{\"end\":14000,\"start\":13659},{\"end\":14442,\"start\":14058},{\"end\":14828,\"start\":14461},{\"end\":16105,\"start\":14830},{\"end\":16854,\"start\":16158},{\"end\":17006,\"start\":16856},{\"end\":17431,\"start\":17038},{\"end\":18059,\"start\":17459},{\"end\":18377,\"start\":18117},{\"end\":18712,\"start\":18379},{\"end\":18963,\"start\":18743},{\"end\":20110,\"start\":19036},{\"end\":20270,\"start\":20126},{\"end\":20706,\"start\":20297},{\"end\":20994,\"start\":20708},{\"end\":22230,\"start\":20996},{\"end\":22647,\"start\":22232},{\"end\":23696,\"start\":22675},{\"end\":24341,\"start\":23698},{\"end\":25123,\"start\":24343},{\"end\":25744,\"start\":25168},{\"end\":26306,\"start\":25746},{\"end\":27439,\"start\":26331},{\"end\":28299,\"start\":27441},{\"end\":29116,\"start\":28314}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11931,\"start\":11903},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12252,\"start\":12223},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14057,\"start\":14001},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16157,\"start\":16106},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17037,\"start\":17007},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18116,\"start\":18060},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18742,\"start\":18713}]", "table_ref": "[{\"end\":20065,\"start\":20058},{\"end\":20109,\"start\":20102},{\"end\":22726,\"start\":22719},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22848,\"start\":22841},{\"end\":23881,\"start\":23874},{\"end\":24430,\"start\":24423},{\"end\":24654,\"start\":24647},{\"end\":25356,\"start\":25349},{\"end\":25536,\"start\":25529},{\"end\":25878,\"start\":25871},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26510,\"start\":26503},{\"end\":27112,\"start\":27105},{\"end\":27358,\"start\":27351},{\"end\":27720,\"start\":27712}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1975,\"start\":1963},{\"attributes\":{\"n\":\"2.\"},\"end\":6609,\"start\":6597},{\"attributes\":{\"n\":\"3.\"},\"end\":11190,\"start\":11179},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11475,\"start\":11451},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12627,\"start\":12597},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":13164,\"start\":13148},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14459,\"start\":14445},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":17457,\"start\":17434},{\"end\":19008,\"start\":18966},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19034,\"start\":19011},{\"attributes\":{\"n\":\"4.\"},\"end\":20124,\"start\":20113},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20295,\"start\":20273},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22673,\"start\":22650},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25149,\"start\":25126},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25166,\"start\":25152},{\"end\":26329,\"start\":26309},{\"attributes\":{\"n\":\"5.\"},\"end\":28312,\"start\":28302},{\"end\":29128,\"start\":29118},{\"end\":29452,\"start\":29442},{\"end\":29849,\"start\":29839},{\"end\":30684,\"start\":30665},{\"end\":31773,\"start\":31764},{\"end\":32354,\"start\":32345}]", "table": "[{\"end\":30991,\"start\":30895},{\"end\":31762,\"start\":31120},{\"end\":32127,\"start\":31775},{\"end\":32343,\"start\":32225},{\"end\":32555,\"start\":32464},{\"end\":32704,\"start\":32656}]", "figure_caption": "[{\"end\":29440,\"start\":29130},{\"end\":29837,\"start\":29454},{\"end\":30283,\"start\":29851},{\"end\":30663,\"start\":30286},{\"end\":30861,\"start\":30687},{\"end\":30895,\"start\":30864},{\"end\":31120,\"start\":30994},{\"end\":32225,\"start\":32130},{\"end\":32464,\"start\":32356},{\"end\":32656,\"start\":32558}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3496,\"start\":3487},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12834,\"start\":12828},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14827,\"start\":14821},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15476,\"start\":15470},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19339,\"start\":19330},{\"end\":23695,\"start\":23689}]", "bib_author_first_name": "[{\"end\":33993,\"start\":33985},{\"end\":34011,\"start\":34005},{\"end\":34029,\"start\":34024},{\"end\":34043,\"start\":34038},{\"end\":34533,\"start\":34525},{\"end\":34551,\"start\":34545},{\"end\":34563,\"start\":34558},{\"end\":35053,\"start\":35048},{\"end\":35070,\"start\":35063},{\"end\":35518,\"start\":35515},{\"end\":35529,\"start\":35524},{\"end\":35544,\"start\":35537},{\"end\":35555,\"start\":35550},{\"end\":35989,\"start\":35983},{\"end\":36005,\"start\":35996},{\"end\":36014,\"start\":36011},{\"end\":36024,\"start\":36021},{\"end\":36035,\"start\":36031},{\"end\":36432,\"start\":36426},{\"end\":36445,\"start\":36439},{\"end\":36457,\"start\":36452},{\"end\":36466,\"start\":36462},{\"end\":36480,\"start\":36472},{\"end\":37018,\"start\":37011},{\"end\":37029,\"start\":37025},{\"end\":37318,\"start\":37316},{\"end\":37332,\"start\":37325},{\"end\":37347,\"start\":37339},{\"end\":37361,\"start\":37353},{\"end\":37371,\"start\":37367},{\"end\":37815,\"start\":37810},{\"end\":37830,\"start\":37822},{\"end\":37844,\"start\":37837},{\"end\":37859,\"start\":37851},{\"end\":37871,\"start\":37867},{\"end\":37880,\"start\":37876},{\"end\":38335,\"start\":38330},{\"end\":38351,\"start\":38345},{\"end\":38367,\"start\":38360},{\"end\":38385,\"start\":38377},{\"end\":38840,\"start\":38833},{\"end\":38853,\"start\":38849},{\"end\":38870,\"start\":38862},{\"end\":39272,\"start\":39268},{\"end\":39281,\"start\":39278},{\"end\":39293,\"start\":39288},{\"end\":39307,\"start\":39302},{\"end\":39316,\"start\":39312},{\"end\":39318,\"start\":39317},{\"end\":39335,\"start\":39327},{\"end\":39799,\"start\":39793},{\"end\":39813,\"start\":39806},{\"end\":39823,\"start\":39819},{\"end\":39836,\"start\":39829},{\"end\":39851,\"start\":39843},{\"end\":39859,\"start\":39856},{\"end\":40276,\"start\":40275},{\"end\":40299,\"start\":40298},{\"end\":40580,\"start\":40572},{\"end\":40595,\"start\":40586},{\"end\":40606,\"start\":40603},{\"end\":40619,\"start\":40613},{\"end\":40637,\"start\":40627},{\"end\":40653,\"start\":40644},{\"end\":40925,\"start\":40918},{\"end\":40937,\"start\":40930},{\"end\":41295,\"start\":41288},{\"end\":41307,\"start\":41300},{\"end\":41323,\"start\":41315},{\"end\":41333,\"start\":41329},{\"end\":41764,\"start\":41757},{\"end\":41777,\"start\":41774},{\"end\":41788,\"start\":41784},{\"end\":41804,\"start\":41795},{\"end\":41819,\"start\":41811},{\"end\":42230,\"start\":42222},{\"end\":42244,\"start\":42239},{\"end\":42258,\"start\":42254},{\"end\":42555,\"start\":42549},{\"end\":42566,\"start\":42561},{\"end\":42579,\"start\":42571},{\"end\":42596,\"start\":42585},{\"end\":42966,\"start\":42965},{\"end\":42983,\"start\":42975},{\"end\":42994,\"start\":42992},{\"end\":43006,\"start\":43000},{\"end\":43019,\"start\":43013},{\"end\":43040,\"start\":43034},{\"end\":43052,\"start\":43047},{\"end\":43068,\"start\":43061},{\"end\":43547,\"start\":43538},{\"end\":43561,\"start\":43555},{\"end\":43574,\"start\":43567},{\"end\":43588,\"start\":43582},{\"end\":43603,\"start\":43596},{\"end\":44105,\"start\":44102},{\"end\":44119,\"start\":44115},{\"end\":44367,\"start\":44361},{\"end\":44382,\"start\":44372},{\"end\":44398,\"start\":44390},{\"end\":44408,\"start\":44403},{\"end\":44817,\"start\":44811},{\"end\":44831,\"start\":44823},{\"end\":44843,\"start\":44838},{\"end\":45083,\"start\":45082},{\"end\":45099,\"start\":45094},{\"end\":45341,\"start\":45333},{\"end\":45356,\"start\":45351},{\"end\":45370,\"start\":45366},{\"end\":45783,\"start\":45779},{\"end\":45800,\"start\":45796},{\"end\":45820,\"start\":45812},{\"end\":45822,\"start\":45821},{\"end\":46181,\"start\":46178},{\"end\":46189,\"start\":46186},{\"end\":46200,\"start\":46194},{\"end\":46612,\"start\":46604},{\"end\":46626,\"start\":46617},{\"end\":46638,\"start\":46632},{\"end\":47034,\"start\":47029},{\"end\":47045,\"start\":47039},{\"end\":47059,\"start\":47052},{\"end\":47068,\"start\":47064},{\"end\":47488,\"start\":47483},{\"end\":47497,\"start\":47493},{\"end\":47511,\"start\":47505},{\"end\":47525,\"start\":47518},{\"end\":47535,\"start\":47531},{\"end\":47548,\"start\":47541},{\"end\":47911,\"start\":47903},{\"end\":47924,\"start\":47917},{\"end\":47937,\"start\":47932},{\"end\":47953,\"start\":47948},{\"end\":47966,\"start\":47960},{\"end\":47979,\"start\":47975},{\"end\":47994,\"start\":47989},{\"end\":48013,\"start\":48003},{\"end\":48379,\"start\":48374},{\"end\":48387,\"start\":48385},{\"end\":48399,\"start\":48394},{\"end\":48414,\"start\":48405},{\"end\":48426,\"start\":48420},{\"end\":48440,\"start\":48432},{\"end\":48908,\"start\":48900},{\"end\":48919,\"start\":48915},{\"end\":48937,\"start\":48931},{\"end\":49391,\"start\":49383},{\"end\":49406,\"start\":49399},{\"end\":49425,\"start\":49416},{\"end\":49444,\"start\":49439},{\"end\":49461,\"start\":49453},{\"end\":49481,\"start\":49471},{\"end\":49497,\"start\":49490},{\"end\":49505,\"start\":49502},{\"end\":49522,\"start\":49513},{\"end\":49928,\"start\":49923},{\"end\":49947,\"start\":49940},{\"end\":49959,\"start\":49955},{\"end\":49972,\"start\":49968},{\"end\":49982,\"start\":49979},{\"end\":50268,\"start\":50259},{\"end\":50282,\"start\":50277},{\"end\":50292,\"start\":50289},{\"end\":50584,\"start\":50576},{\"end\":50596,\"start\":50590},{\"end\":50609,\"start\":50603},{\"end\":50624,\"start\":50615},{\"end\":51064,\"start\":51056},{\"end\":51078,\"start\":51070},{\"end\":51089,\"start\":51083},{\"end\":51099,\"start\":51095},{\"end\":51113,\"start\":51107},{\"end\":51568,\"start\":51560},{\"end\":51578,\"start\":51575},{\"end\":51593,\"start\":51586},{\"end\":51865,\"start\":51859},{\"end\":52129,\"start\":52125},{\"end\":52141,\"start\":52138},{\"end\":52158,\"start\":52149},{\"end\":52170,\"start\":52166},{\"end\":52183,\"start\":52178},{\"end\":52201,\"start\":52194},{\"end\":52216,\"start\":52210},{\"end\":52232,\"start\":52226},{\"end\":52245,\"start\":52238},{\"end\":52262,\"start\":52258},{\"end\":52736,\"start\":52728},{\"end\":53084,\"start\":53078},{\"end\":53105,\"start\":53097},{\"end\":53122,\"start\":53117},{\"end\":53136,\"start\":53131},{\"end\":53600,\"start\":53594},{\"end\":53621,\"start\":53613},{\"end\":53638,\"start\":53633},{\"end\":53652,\"start\":53647},{\"end\":54033,\"start\":54026},{\"end\":54049,\"start\":54042},{\"end\":54064,\"start\":54058},{\"end\":54073,\"start\":54065},{\"end\":54088,\"start\":54081},{\"end\":54104,\"start\":54099},{\"end\":54118,\"start\":54112},{\"end\":54438,\"start\":54434},{\"end\":54459,\"start\":54452},{\"end\":54475,\"start\":54469},{\"end\":54891,\"start\":54888},{\"end\":54901,\"start\":54898},{\"end\":55306,\"start\":55298},{\"end\":55322,\"start\":55313},{\"end\":55334,\"start\":55331},{\"end\":55639,\"start\":55634},{\"end\":55656,\"start\":55650},{\"end\":55917,\"start\":55915},{\"end\":55926,\"start\":55923},{\"end\":55937,\"start\":55933},{\"end\":55951,\"start\":55943},{\"end\":56405,\"start\":56402},{\"end\":56415,\"start\":56412},{\"end\":56424,\"start\":56420},{\"end\":56830,\"start\":56822},{\"end\":56845,\"start\":56840},{\"end\":56856,\"start\":56852},{\"end\":56873,\"start\":56864},{\"end\":57174,\"start\":57165},{\"end\":57192,\"start\":57183},{\"end\":57636,\"start\":57628},{\"end\":57647,\"start\":57643},{\"end\":57659,\"start\":57654},{\"end\":58053,\"start\":58048},{\"end\":58064,\"start\":58059},{\"end\":58083,\"start\":58078},{\"end\":58542,\"start\":58534},{\"end\":58562,\"start\":58556},{\"end\":58579,\"start\":58572},{\"end\":58600,\"start\":58593},{\"end\":58615,\"start\":58608},{\"end\":58993,\"start\":58985},{\"end\":59002,\"start\":58998},{\"end\":59015,\"start\":59009},{\"end\":59029,\"start\":59022},{\"end\":59038,\"start\":59034},{\"end\":59479,\"start\":59476},{\"end\":59493,\"start\":59486},{\"end\":59504,\"start\":59498},{\"end\":59867,\"start\":59864},{\"end\":59880,\"start\":59874},{\"end\":59890,\"start\":59885},{\"end\":59908,\"start\":59899},{\"end\":59921,\"start\":59913},{\"end\":60380,\"start\":60373},{\"end\":60390,\"start\":60385},{\"end\":60402,\"start\":60395},{\"end\":60414,\"start\":60409},{\"end\":60425,\"start\":60419},{\"end\":60437,\"start\":60431},{\"end\":60882,\"start\":60877},{\"end\":60895,\"start\":60888},{\"end\":60907,\"start\":60901},{\"end\":60918,\"start\":60913},{\"end\":61460,\"start\":61454},{\"end\":61477,\"start\":61472},{\"end\":61796,\"start\":61792},{\"end\":61811,\"start\":61804},{\"end\":61823,\"start\":61817},{\"end\":61832,\"start\":61829},{\"end\":61839,\"start\":61837},{\"end\":62265,\"start\":62261},{\"end\":62280,\"start\":62273},{\"end\":62289,\"start\":62286},{\"end\":62645,\"start\":62641},{\"end\":62656,\"start\":62653},{\"end\":62671,\"start\":62664},{\"end\":62673,\"start\":62672},{\"end\":62693,\"start\":62686},{\"end\":63101,\"start\":63094},{\"end\":63115,\"start\":63107}]", "bib_author_last_name": "[{\"end\":34003,\"start\":33994},{\"end\":34022,\"start\":34012},{\"end\":34036,\"start\":34030},{\"end\":34051,\"start\":34044},{\"end\":34543,\"start\":34534},{\"end\":34556,\"start\":34552},{\"end\":34571,\"start\":34564},{\"end\":35061,\"start\":35054},{\"end\":35078,\"start\":35071},{\"end\":35522,\"start\":35519},{\"end\":35535,\"start\":35530},{\"end\":35548,\"start\":35545},{\"end\":35562,\"start\":35556},{\"end\":35994,\"start\":35990},{\"end\":36009,\"start\":36006},{\"end\":36019,\"start\":36015},{\"end\":36029,\"start\":36025},{\"end\":36040,\"start\":36036},{\"end\":36437,\"start\":36433},{\"end\":36450,\"start\":36446},{\"end\":36460,\"start\":36458},{\"end\":36470,\"start\":36467},{\"end\":36491,\"start\":36481},{\"end\":37023,\"start\":37019},{\"end\":37036,\"start\":37030},{\"end\":37323,\"start\":37319},{\"end\":37337,\"start\":37333},{\"end\":37351,\"start\":37348},{\"end\":37365,\"start\":37362},{\"end\":37376,\"start\":37372},{\"end\":37820,\"start\":37816},{\"end\":37835,\"start\":37831},{\"end\":37849,\"start\":37845},{\"end\":37865,\"start\":37860},{\"end\":37874,\"start\":37872},{\"end\":37884,\"start\":37881},{\"end\":38343,\"start\":38336},{\"end\":38358,\"start\":38352},{\"end\":38375,\"start\":38368},{\"end\":38392,\"start\":38386},{\"end\":38847,\"start\":38841},{\"end\":38860,\"start\":38854},{\"end\":38877,\"start\":38871},{\"end\":39276,\"start\":39273},{\"end\":39286,\"start\":39282},{\"end\":39300,\"start\":39294},{\"end\":39310,\"start\":39308},{\"end\":39325,\"start\":39319},{\"end\":39340,\"start\":39336},{\"end\":39804,\"start\":39800},{\"end\":39817,\"start\":39814},{\"end\":39827,\"start\":39824},{\"end\":39841,\"start\":39837},{\"end\":39854,\"start\":39852},{\"end\":39863,\"start\":39860},{\"end\":40282,\"start\":40277},{\"end\":40296,\"start\":40284},{\"end\":40306,\"start\":40300},{\"end\":40320,\"start\":40308},{\"end\":40584,\"start\":40581},{\"end\":40601,\"start\":40596},{\"end\":40611,\"start\":40607},{\"end\":40625,\"start\":40620},{\"end\":40642,\"start\":40638},{\"end\":40658,\"start\":40654},{\"end\":40928,\"start\":40926},{\"end\":40946,\"start\":40938},{\"end\":41298,\"start\":41296},{\"end\":41313,\"start\":41308},{\"end\":41327,\"start\":41324},{\"end\":41337,\"start\":41334},{\"end\":41772,\"start\":41765},{\"end\":41782,\"start\":41778},{\"end\":41793,\"start\":41789},{\"end\":41809,\"start\":41805},{\"end\":41823,\"start\":41820},{\"end\":41828,\"start\":41825},{\"end\":42237,\"start\":42231},{\"end\":42252,\"start\":42245},{\"end\":42263,\"start\":42259},{\"end\":42559,\"start\":42556},{\"end\":42569,\"start\":42567},{\"end\":42583,\"start\":42580},{\"end\":42600,\"start\":42597},{\"end\":42973,\"start\":42967},{\"end\":42990,\"start\":42984},{\"end\":42998,\"start\":42995},{\"end\":43011,\"start\":43007},{\"end\":43032,\"start\":43020},{\"end\":43045,\"start\":43041},{\"end\":43059,\"start\":43053},{\"end\":43078,\"start\":43069},{\"end\":43084,\"start\":43080},{\"end\":43553,\"start\":43548},{\"end\":43565,\"start\":43562},{\"end\":43580,\"start\":43575},{\"end\":43594,\"start\":43589},{\"end\":43607,\"start\":43604},{\"end\":44113,\"start\":44106},{\"end\":44130,\"start\":44120},{\"end\":44370,\"start\":44368},{\"end\":44388,\"start\":44383},{\"end\":44401,\"start\":44399},{\"end\":44412,\"start\":44409},{\"end\":44821,\"start\":44818},{\"end\":44836,\"start\":44832},{\"end\":44848,\"start\":44844},{\"end\":45092,\"start\":45084},{\"end\":45106,\"start\":45100},{\"end\":45110,\"start\":45108},{\"end\":45349,\"start\":45342},{\"end\":45364,\"start\":45357},{\"end\":45376,\"start\":45371},{\"end\":45794,\"start\":45784},{\"end\":45810,\"start\":45801},{\"end\":45829,\"start\":45823},{\"end\":46184,\"start\":46182},{\"end\":46192,\"start\":46190},{\"end\":46205,\"start\":46201},{\"end\":46615,\"start\":46613},{\"end\":46630,\"start\":46627},{\"end\":46642,\"start\":46639},{\"end\":47037,\"start\":47035},{\"end\":47050,\"start\":47046},{\"end\":47062,\"start\":47060},{\"end\":47073,\"start\":47069},{\"end\":47491,\"start\":47489},{\"end\":47503,\"start\":47498},{\"end\":47516,\"start\":47512},{\"end\":47529,\"start\":47526},{\"end\":47539,\"start\":47536},{\"end\":47552,\"start\":47549},{\"end\":47915,\"start\":47912},{\"end\":47930,\"start\":47925},{\"end\":47946,\"start\":47938},{\"end\":47958,\"start\":47954},{\"end\":47973,\"start\":47967},{\"end\":47987,\"start\":47980},{\"end\":48001,\"start\":47995},{\"end\":48021,\"start\":48014},{\"end\":48383,\"start\":48380},{\"end\":48392,\"start\":48388},{\"end\":48403,\"start\":48400},{\"end\":48418,\"start\":48415},{\"end\":48430,\"start\":48427},{\"end\":48445,\"start\":48441},{\"end\":48913,\"start\":48909},{\"end\":48929,\"start\":48920},{\"end\":48945,\"start\":48938},{\"end\":49397,\"start\":49392},{\"end\":49414,\"start\":49407},{\"end\":49437,\"start\":49426},{\"end\":49451,\"start\":49445},{\"end\":49469,\"start\":49462},{\"end\":49488,\"start\":49482},{\"end\":49500,\"start\":49498},{\"end\":49511,\"start\":49506},{\"end\":49531,\"start\":49523},{\"end\":49938,\"start\":49929},{\"end\":49953,\"start\":49948},{\"end\":49966,\"start\":49960},{\"end\":49977,\"start\":49973},{\"end\":49988,\"start\":49983},{\"end\":50275,\"start\":50269},{\"end\":50287,\"start\":50283},{\"end\":50297,\"start\":50293},{\"end\":50588,\"start\":50585},{\"end\":50601,\"start\":50597},{\"end\":50613,\"start\":50610},{\"end\":50628,\"start\":50625},{\"end\":51068,\"start\":51065},{\"end\":51081,\"start\":51079},{\"end\":51093,\"start\":51090},{\"end\":51105,\"start\":51100},{\"end\":51118,\"start\":51114},{\"end\":51573,\"start\":51569},{\"end\":51584,\"start\":51579},{\"end\":51596,\"start\":51594},{\"end\":51872,\"start\":51866},{\"end\":52136,\"start\":52130},{\"end\":52147,\"start\":52142},{\"end\":52164,\"start\":52159},{\"end\":52176,\"start\":52171},{\"end\":52192,\"start\":52184},{\"end\":52208,\"start\":52202},{\"end\":52224,\"start\":52217},{\"end\":52236,\"start\":52233},{\"end\":52256,\"start\":52246},{\"end\":52269,\"start\":52263},{\"end\":52741,\"start\":52737},{\"end\":53095,\"start\":53085},{\"end\":53115,\"start\":53106},{\"end\":53129,\"start\":53123},{\"end\":53144,\"start\":53137},{\"end\":53611,\"start\":53601},{\"end\":53631,\"start\":53622},{\"end\":53645,\"start\":53639},{\"end\":53660,\"start\":53653},{\"end\":54040,\"start\":54034},{\"end\":54056,\"start\":54050},{\"end\":54079,\"start\":54074},{\"end\":54097,\"start\":54089},{\"end\":54110,\"start\":54105},{\"end\":54125,\"start\":54119},{\"end\":54450,\"start\":54439},{\"end\":54467,\"start\":54460},{\"end\":54480,\"start\":54476},{\"end\":54896,\"start\":54892},{\"end\":54908,\"start\":54902},{\"end\":55311,\"start\":55307},{\"end\":55329,\"start\":55323},{\"end\":55341,\"start\":55335},{\"end\":55648,\"start\":55640},{\"end\":55666,\"start\":55657},{\"end\":55921,\"start\":55918},{\"end\":55931,\"start\":55927},{\"end\":55941,\"start\":55938},{\"end\":55956,\"start\":55952},{\"end\":56410,\"start\":56406},{\"end\":56418,\"start\":56416},{\"end\":56427,\"start\":56425},{\"end\":56838,\"start\":56831},{\"end\":56850,\"start\":56846},{\"end\":56862,\"start\":56857},{\"end\":56881,\"start\":56874},{\"end\":57181,\"start\":57175},{\"end\":57200,\"start\":57193},{\"end\":57641,\"start\":57637},{\"end\":57652,\"start\":57648},{\"end\":57665,\"start\":57660},{\"end\":58046,\"start\":58039},{\"end\":58057,\"start\":58054},{\"end\":58076,\"start\":58065},{\"end\":58090,\"start\":58084},{\"end\":58098,\"start\":58092},{\"end\":58554,\"start\":58543},{\"end\":58570,\"start\":58563},{\"end\":58591,\"start\":58580},{\"end\":58606,\"start\":58601},{\"end\":58621,\"start\":58616},{\"end\":58996,\"start\":58994},{\"end\":59007,\"start\":59003},{\"end\":59020,\"start\":59016},{\"end\":59032,\"start\":59030},{\"end\":59044,\"start\":59039},{\"end\":59484,\"start\":59480},{\"end\":59496,\"start\":59494},{\"end\":59508,\"start\":59505},{\"end\":59872,\"start\":59868},{\"end\":59883,\"start\":59881},{\"end\":59897,\"start\":59891},{\"end\":59911,\"start\":59909},{\"end\":59926,\"start\":59922},{\"end\":60383,\"start\":60381},{\"end\":60393,\"start\":60391},{\"end\":60407,\"start\":60403},{\"end\":60417,\"start\":60415},{\"end\":60429,\"start\":60426},{\"end\":60442,\"start\":60438},{\"end\":60886,\"start\":60883},{\"end\":60899,\"start\":60896},{\"end\":60911,\"start\":60908},{\"end\":60922,\"start\":60919},{\"end\":61470,\"start\":61461},{\"end\":61487,\"start\":61478},{\"end\":61802,\"start\":61797},{\"end\":61815,\"start\":61812},{\"end\":61827,\"start\":61824},{\"end\":61835,\"start\":61833},{\"end\":61843,\"start\":61840},{\"end\":62271,\"start\":62266},{\"end\":62284,\"start\":62281},{\"end\":62292,\"start\":62290},{\"end\":62651,\"start\":62646},{\"end\":62662,\"start\":62657},{\"end\":62684,\"start\":62674},{\"end\":62696,\"start\":62694},{\"end\":63105,\"start\":63102},{\"end\":63120,\"start\":63116}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206592419},\"end\":34441,\"start\":33914},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1430002},\"end\":34978,\"start\":34443},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":210861282},\"end\":35444,\"start\":34980},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16224674},\"end\":35931,\"start\":35446},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208526905},\"end\":36352,\"start\":35933},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":29308926},\"end\":36919,\"start\":36354},{\"attributes\":{\"doi\":\"arXiv:1407.3399\",\"id\":\"b6\"},\"end\":37226,\"start\":36921},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206770818},\"end\":37749,\"start\":37228},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4703058},\"end\":38274,\"start\":37751},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2336352},\"end\":38776,\"start\":38276},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2227746},\"end\":39215,\"start\":38778},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15364102},\"end\":39732,\"start\":39217},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":201666186},\"end\":40228,\"start\":39734},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2277383},\"end\":40520,\"start\":40230},{\"attributes\":{\"id\":\"b14\"},\"end\":40871,\"start\":40522},{\"attributes\":{\"id\":\"b15\"},\"end\":41240,\"start\":40873},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594692},\"end\":41697,\"start\":41242},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":75137175},\"end\":42220,\"start\":41699},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b18\"},\"end\":42474,\"start\":42222},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":199405591},\"end\":42963,\"start\":42476},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b20\"},\"end\":43450,\"start\":42965},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":210701386},\"end\":44026,\"start\":43452},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7318714},\"end\":44296,\"start\":44028},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4448103},\"end\":44738,\"start\":44298},{\"attributes\":{\"doi\":\"arXiv:1802.04977\",\"id\":\"b24\"},\"end\":45036,\"start\":44740},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b25\"},\"end\":45254,\"start\":45038},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":49670944},\"end\":45712,\"start\":45256},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":195908774},\"end\":46083,\"start\":45714},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":208267796},\"end\":46547,\"start\":46085},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":267213},\"end\":47000,\"start\":46549},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":80628366},\"end\":47411,\"start\":47002},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":222124879},\"end\":47858,\"start\":47413},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14113767},\"end\":48311,\"start\":47860},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":73729180},\"end\":48842,\"start\":48313},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1629541},\"end\":49313,\"start\":48844},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1640720},\"end\":49849,\"start\":49315},{\"attributes\":{\"doi\":\"arXiv:1611.06440\",\"id\":\"b36\"},\"end\":50203,\"start\":49851},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":13613792},\"end\":50522,\"start\":50205},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52855270},\"end\":50985,\"start\":50524},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":201666181},\"end\":51486,\"start\":50987},{\"attributes\":{\"id\":\"b40\"},\"end\":51807,\"start\":51488},{\"attributes\":{\"doi\":\"arXiv:1811.12004\",\"id\":\"b41\"},\"end\":52053,\"start\":51809},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":202786778},\"end\":52621,\"start\":52055},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":52149084},\"end\":53034,\"start\":52623},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":9795585},\"end\":53517,\"start\":53036},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1566069},\"end\":54024,\"start\":53519},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b46\"},\"end\":54368,\"start\":54026},{\"attributes\":{\"id\":\"b47\"},\"end\":54821,\"start\":54370},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":12576235},\"end\":55247,\"start\":54823},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":14602050},\"end\":55564,\"start\":55249},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b50\"},\"end\":55841,\"start\":55566},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":67856425},\"end\":56337,\"start\":55843},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":52954531},\"end\":56729,\"start\":56339},{\"attributes\":{\"doi\":\"arXiv:1406.2984\",\"id\":\"b53\"},\"end\":57105,\"start\":56731},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":206592152},\"end\":57557,\"start\":57107},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":201070231},\"end\":58008,\"start\":57559},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":163946},\"end\":58450,\"start\":58010},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":221246396},\"end\":58923,\"start\":58452},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":9183542},\"end\":59417,\"start\":58925},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":4934594},\"end\":59809,\"start\":59419},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":250792},\"end\":60266,\"start\":59811},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":128358417},\"end\":60776,\"start\":60268},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":206596723},\"end\":61333,\"start\":60778},{\"attributes\":{\"doi\":\"arXiv:1612.03928\",\"id\":\"b63\"},\"end\":61718,\"start\":61335},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":204509549},\"end\":62231,\"start\":61720},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":53292120},\"end\":62617,\"start\":62233},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":26071966},\"end\":63038,\"start\":62619},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":48352434},\"end\":63340,\"start\":63040}]", "bib_title": "[{\"end\":33983,\"start\":33914},{\"end\":34523,\"start\":34443},{\"end\":35046,\"start\":34980},{\"end\":35513,\"start\":35446},{\"end\":35981,\"start\":35933},{\"end\":36424,\"start\":36354},{\"end\":37314,\"start\":37228},{\"end\":37808,\"start\":37751},{\"end\":38328,\"start\":38276},{\"end\":38831,\"start\":38778},{\"end\":39266,\"start\":39217},{\"end\":39791,\"start\":39734},{\"end\":40273,\"start\":40230},{\"end\":40916,\"start\":40873},{\"end\":41286,\"start\":41242},{\"end\":41755,\"start\":41699},{\"end\":42547,\"start\":42476},{\"end\":43536,\"start\":43452},{\"end\":44100,\"start\":44028},{\"end\":44359,\"start\":44298},{\"end\":45331,\"start\":45256},{\"end\":45777,\"start\":45714},{\"end\":46176,\"start\":46085},{\"end\":46602,\"start\":46549},{\"end\":47027,\"start\":47002},{\"end\":47481,\"start\":47413},{\"end\":47901,\"start\":47860},{\"end\":48372,\"start\":48313},{\"end\":48898,\"start\":48844},{\"end\":49381,\"start\":49315},{\"end\":50257,\"start\":50205},{\"end\":50574,\"start\":50524},{\"end\":51054,\"start\":50987},{\"end\":51558,\"start\":51488},{\"end\":52123,\"start\":52055},{\"end\":52726,\"start\":52623},{\"end\":53076,\"start\":53036},{\"end\":53592,\"start\":53519},{\"end\":54432,\"start\":54370},{\"end\":54886,\"start\":54823},{\"end\":55296,\"start\":55249},{\"end\":55913,\"start\":55843},{\"end\":56400,\"start\":56339},{\"end\":57163,\"start\":57107},{\"end\":57626,\"start\":57559},{\"end\":58037,\"start\":58010},{\"end\":58532,\"start\":58452},{\"end\":58983,\"start\":58925},{\"end\":59474,\"start\":59419},{\"end\":59862,\"start\":59811},{\"end\":60371,\"start\":60268},{\"end\":60875,\"start\":60778},{\"end\":61790,\"start\":61720},{\"end\":62259,\"start\":62233},{\"end\":62639,\"start\":62619},{\"end\":63092,\"start\":63040}]", "bib_author": "[{\"end\":34005,\"start\":33985},{\"end\":34024,\"start\":34005},{\"end\":34038,\"start\":34024},{\"end\":34053,\"start\":34038},{\"end\":34545,\"start\":34525},{\"end\":34558,\"start\":34545},{\"end\":34573,\"start\":34558},{\"end\":35063,\"start\":35048},{\"end\":35080,\"start\":35063},{\"end\":35524,\"start\":35515},{\"end\":35537,\"start\":35524},{\"end\":35550,\"start\":35537},{\"end\":35564,\"start\":35550},{\"end\":35996,\"start\":35983},{\"end\":36011,\"start\":35996},{\"end\":36021,\"start\":36011},{\"end\":36031,\"start\":36021},{\"end\":36042,\"start\":36031},{\"end\":36439,\"start\":36426},{\"end\":36452,\"start\":36439},{\"end\":36462,\"start\":36452},{\"end\":36472,\"start\":36462},{\"end\":36493,\"start\":36472},{\"end\":37025,\"start\":37011},{\"end\":37038,\"start\":37025},{\"end\":37325,\"start\":37316},{\"end\":37339,\"start\":37325},{\"end\":37353,\"start\":37339},{\"end\":37367,\"start\":37353},{\"end\":37378,\"start\":37367},{\"end\":37822,\"start\":37810},{\"end\":37837,\"start\":37822},{\"end\":37851,\"start\":37837},{\"end\":37867,\"start\":37851},{\"end\":37876,\"start\":37867},{\"end\":37886,\"start\":37876},{\"end\":38345,\"start\":38330},{\"end\":38360,\"start\":38345},{\"end\":38377,\"start\":38360},{\"end\":38394,\"start\":38377},{\"end\":38849,\"start\":38833},{\"end\":38862,\"start\":38849},{\"end\":38879,\"start\":38862},{\"end\":39278,\"start\":39268},{\"end\":39288,\"start\":39278},{\"end\":39302,\"start\":39288},{\"end\":39312,\"start\":39302},{\"end\":39327,\"start\":39312},{\"end\":39342,\"start\":39327},{\"end\":39806,\"start\":39793},{\"end\":39819,\"start\":39806},{\"end\":39829,\"start\":39819},{\"end\":39843,\"start\":39829},{\"end\":39856,\"start\":39843},{\"end\":39865,\"start\":39856},{\"end\":40284,\"start\":40275},{\"end\":40298,\"start\":40284},{\"end\":40308,\"start\":40298},{\"end\":40322,\"start\":40308},{\"end\":40586,\"start\":40572},{\"end\":40603,\"start\":40586},{\"end\":40613,\"start\":40603},{\"end\":40627,\"start\":40613},{\"end\":40644,\"start\":40627},{\"end\":40660,\"start\":40644},{\"end\":40930,\"start\":40918},{\"end\":40948,\"start\":40930},{\"end\":41300,\"start\":41288},{\"end\":41315,\"start\":41300},{\"end\":41329,\"start\":41315},{\"end\":41339,\"start\":41329},{\"end\":41774,\"start\":41757},{\"end\":41784,\"start\":41774},{\"end\":41795,\"start\":41784},{\"end\":41811,\"start\":41795},{\"end\":41825,\"start\":41811},{\"end\":41830,\"start\":41825},{\"end\":42239,\"start\":42222},{\"end\":42254,\"start\":42239},{\"end\":42265,\"start\":42254},{\"end\":42561,\"start\":42549},{\"end\":42571,\"start\":42561},{\"end\":42585,\"start\":42571},{\"end\":42602,\"start\":42585},{\"end\":42975,\"start\":42965},{\"end\":42992,\"start\":42975},{\"end\":43000,\"start\":42992},{\"end\":43013,\"start\":43000},{\"end\":43034,\"start\":43013},{\"end\":43047,\"start\":43034},{\"end\":43061,\"start\":43047},{\"end\":43080,\"start\":43061},{\"end\":43086,\"start\":43080},{\"end\":43555,\"start\":43538},{\"end\":43567,\"start\":43555},{\"end\":43582,\"start\":43567},{\"end\":43596,\"start\":43582},{\"end\":43609,\"start\":43596},{\"end\":44115,\"start\":44102},{\"end\":44132,\"start\":44115},{\"end\":44372,\"start\":44361},{\"end\":44390,\"start\":44372},{\"end\":44403,\"start\":44390},{\"end\":44414,\"start\":44403},{\"end\":44823,\"start\":44811},{\"end\":44838,\"start\":44823},{\"end\":44850,\"start\":44838},{\"end\":45094,\"start\":45082},{\"end\":45108,\"start\":45094},{\"end\":45112,\"start\":45108},{\"end\":45351,\"start\":45333},{\"end\":45366,\"start\":45351},{\"end\":45378,\"start\":45366},{\"end\":45796,\"start\":45779},{\"end\":45812,\"start\":45796},{\"end\":45831,\"start\":45812},{\"end\":46186,\"start\":46178},{\"end\":46194,\"start\":46186},{\"end\":46207,\"start\":46194},{\"end\":46617,\"start\":46604},{\"end\":46632,\"start\":46617},{\"end\":46644,\"start\":46632},{\"end\":47039,\"start\":47029},{\"end\":47052,\"start\":47039},{\"end\":47064,\"start\":47052},{\"end\":47075,\"start\":47064},{\"end\":47493,\"start\":47483},{\"end\":47505,\"start\":47493},{\"end\":47518,\"start\":47505},{\"end\":47531,\"start\":47518},{\"end\":47541,\"start\":47531},{\"end\":47554,\"start\":47541},{\"end\":47917,\"start\":47903},{\"end\":47932,\"start\":47917},{\"end\":47948,\"start\":47932},{\"end\":47960,\"start\":47948},{\"end\":47975,\"start\":47960},{\"end\":47989,\"start\":47975},{\"end\":48003,\"start\":47989},{\"end\":48023,\"start\":48003},{\"end\":48385,\"start\":48374},{\"end\":48394,\"start\":48385},{\"end\":48405,\"start\":48394},{\"end\":48420,\"start\":48405},{\"end\":48432,\"start\":48420},{\"end\":48447,\"start\":48432},{\"end\":48915,\"start\":48900},{\"end\":48931,\"start\":48915},{\"end\":48947,\"start\":48931},{\"end\":49399,\"start\":49383},{\"end\":49416,\"start\":49399},{\"end\":49439,\"start\":49416},{\"end\":49453,\"start\":49439},{\"end\":49471,\"start\":49453},{\"end\":49490,\"start\":49471},{\"end\":49502,\"start\":49490},{\"end\":49513,\"start\":49502},{\"end\":49533,\"start\":49513},{\"end\":49940,\"start\":49923},{\"end\":49955,\"start\":49940},{\"end\":49968,\"start\":49955},{\"end\":49979,\"start\":49968},{\"end\":49990,\"start\":49979},{\"end\":50277,\"start\":50259},{\"end\":50289,\"start\":50277},{\"end\":50299,\"start\":50289},{\"end\":50590,\"start\":50576},{\"end\":50603,\"start\":50590},{\"end\":50615,\"start\":50603},{\"end\":50630,\"start\":50615},{\"end\":51070,\"start\":51056},{\"end\":51083,\"start\":51070},{\"end\":51095,\"start\":51083},{\"end\":51107,\"start\":51095},{\"end\":51120,\"start\":51107},{\"end\":51575,\"start\":51560},{\"end\":51586,\"start\":51575},{\"end\":51598,\"start\":51586},{\"end\":51874,\"start\":51859},{\"end\":52138,\"start\":52125},{\"end\":52149,\"start\":52138},{\"end\":52166,\"start\":52149},{\"end\":52178,\"start\":52166},{\"end\":52194,\"start\":52178},{\"end\":52210,\"start\":52194},{\"end\":52226,\"start\":52210},{\"end\":52238,\"start\":52226},{\"end\":52258,\"start\":52238},{\"end\":52271,\"start\":52258},{\"end\":52743,\"start\":52728},{\"end\":53097,\"start\":53078},{\"end\":53117,\"start\":53097},{\"end\":53131,\"start\":53117},{\"end\":53146,\"start\":53131},{\"end\":53613,\"start\":53594},{\"end\":53633,\"start\":53613},{\"end\":53647,\"start\":53633},{\"end\":53662,\"start\":53647},{\"end\":54042,\"start\":54026},{\"end\":54058,\"start\":54042},{\"end\":54081,\"start\":54058},{\"end\":54099,\"start\":54081},{\"end\":54112,\"start\":54099},{\"end\":54127,\"start\":54112},{\"end\":54452,\"start\":54434},{\"end\":54469,\"start\":54452},{\"end\":54482,\"start\":54469},{\"end\":54898,\"start\":54888},{\"end\":54910,\"start\":54898},{\"end\":55313,\"start\":55298},{\"end\":55331,\"start\":55313},{\"end\":55343,\"start\":55331},{\"end\":55650,\"start\":55634},{\"end\":55668,\"start\":55650},{\"end\":55923,\"start\":55915},{\"end\":55933,\"start\":55923},{\"end\":55943,\"start\":55933},{\"end\":55958,\"start\":55943},{\"end\":56412,\"start\":56402},{\"end\":56420,\"start\":56412},{\"end\":56429,\"start\":56420},{\"end\":56840,\"start\":56822},{\"end\":56852,\"start\":56840},{\"end\":56864,\"start\":56852},{\"end\":56883,\"start\":56864},{\"end\":57183,\"start\":57165},{\"end\":57202,\"start\":57183},{\"end\":57643,\"start\":57628},{\"end\":57654,\"start\":57643},{\"end\":57667,\"start\":57654},{\"end\":58048,\"start\":58039},{\"end\":58059,\"start\":58048},{\"end\":58078,\"start\":58059},{\"end\":58092,\"start\":58078},{\"end\":58100,\"start\":58092},{\"end\":58556,\"start\":58534},{\"end\":58572,\"start\":58556},{\"end\":58593,\"start\":58572},{\"end\":58608,\"start\":58593},{\"end\":58623,\"start\":58608},{\"end\":58998,\"start\":58985},{\"end\":59009,\"start\":58998},{\"end\":59022,\"start\":59009},{\"end\":59034,\"start\":59022},{\"end\":59046,\"start\":59034},{\"end\":59486,\"start\":59476},{\"end\":59498,\"start\":59486},{\"end\":59510,\"start\":59498},{\"end\":59874,\"start\":59864},{\"end\":59885,\"start\":59874},{\"end\":59899,\"start\":59885},{\"end\":59913,\"start\":59899},{\"end\":59928,\"start\":59913},{\"end\":60385,\"start\":60373},{\"end\":60395,\"start\":60385},{\"end\":60409,\"start\":60395},{\"end\":60419,\"start\":60409},{\"end\":60431,\"start\":60419},{\"end\":60444,\"start\":60431},{\"end\":60888,\"start\":60877},{\"end\":60901,\"start\":60888},{\"end\":60913,\"start\":60901},{\"end\":60924,\"start\":60913},{\"end\":61472,\"start\":61454},{\"end\":61489,\"start\":61472},{\"end\":61804,\"start\":61792},{\"end\":61817,\"start\":61804},{\"end\":61829,\"start\":61817},{\"end\":61837,\"start\":61829},{\"end\":61845,\"start\":61837},{\"end\":62273,\"start\":62261},{\"end\":62286,\"start\":62273},{\"end\":62294,\"start\":62286},{\"end\":62653,\"start\":62641},{\"end\":62664,\"start\":62653},{\"end\":62686,\"start\":62664},{\"end\":62698,\"start\":62686},{\"end\":63107,\"start\":63094},{\"end\":63122,\"start\":63107}]", "bib_venue": "[{\"end\":34202,\"start\":34136},{\"end\":34722,\"start\":34656},{\"end\":35229,\"start\":35163},{\"end\":35705,\"start\":35643},{\"end\":36151,\"start\":36105},{\"end\":36658,\"start\":36584},{\"end\":37499,\"start\":37447},{\"end\":38027,\"start\":37965},{\"end\":38543,\"start\":38477},{\"end\":39008,\"start\":38952},{\"end\":39491,\"start\":39425},{\"end\":39994,\"start\":39938},{\"end\":41069,\"start\":41017},{\"end\":41488,\"start\":41422},{\"end\":41979,\"start\":41913},{\"end\":42731,\"start\":42675},{\"end\":43756,\"start\":43691},{\"end\":44529,\"start\":44480},{\"end\":45493,\"start\":45444},{\"end\":46316,\"start\":46270},{\"end\":46793,\"start\":46727},{\"end\":47224,\"start\":47158},{\"end\":47649,\"start\":47610},{\"end\":48596,\"start\":48530},{\"end\":49096,\"start\":49030},{\"end\":50771,\"start\":50709},{\"end\":51249,\"start\":51193},{\"end\":53295,\"start\":53229},{\"end\":53783,\"start\":53731},{\"end\":55051,\"start\":54989},{\"end\":56107,\"start\":56041},{\"end\":56544,\"start\":56495},{\"end\":57351,\"start\":57285},{\"end\":57796,\"start\":57740},{\"end\":58249,\"start\":58183},{\"end\":59187,\"start\":59125},{\"end\":59625,\"start\":59576},{\"end\":60049,\"start\":59997},{\"end\":61073,\"start\":61007},{\"end\":61994,\"start\":61928},{\"end\":62443,\"start\":62377},{\"end\":62847,\"start\":62781},{\"end\":34134,\"start\":34053},{\"end\":34654,\"start\":34573},{\"end\":35161,\"start\":35080},{\"end\":35641,\"start\":35564},{\"end\":36103,\"start\":36042},{\"end\":36582,\"start\":36493},{\"end\":37009,\"start\":36921},{\"end\":37445,\"start\":37378},{\"end\":37963,\"start\":37886},{\"end\":38475,\"start\":38394},{\"end\":38950,\"start\":38879},{\"end\":39423,\"start\":39342},{\"end\":39936,\"start\":39865},{\"end\":40362,\"start\":40322},{\"end\":40570,\"start\":40522},{\"end\":41015,\"start\":40948},{\"end\":41420,\"start\":41339},{\"end\":41911,\"start\":41830},{\"end\":42325,\"start\":42281},{\"end\":42673,\"start\":42602},{\"end\":43184,\"start\":43102},{\"end\":43689,\"start\":43609},{\"end\":44136,\"start\":44132},{\"end\":44478,\"start\":44414},{\"end\":44809,\"start\":44740},{\"end\":45080,\"start\":45038},{\"end\":45442,\"start\":45378},{\"end\":45880,\"start\":45831},{\"end\":46268,\"start\":46207},{\"end\":46725,\"start\":46644},{\"end\":47156,\"start\":47075},{\"end\":47608,\"start\":47554},{\"end\":48061,\"start\":48023},{\"end\":48528,\"start\":48447},{\"end\":49028,\"start\":48947},{\"end\":49567,\"start\":49533},{\"end\":49921,\"start\":49851},{\"end\":50337,\"start\":50299},{\"end\":50707,\"start\":50630},{\"end\":51191,\"start\":51120},{\"end\":51629,\"start\":51598},{\"end\":51857,\"start\":51809},{\"end\":52320,\"start\":52271},{\"end\":52810,\"start\":52743},{\"end\":53227,\"start\":53146},{\"end\":53729,\"start\":53662},{\"end\":54175,\"start\":54142},{\"end\":54568,\"start\":54482},{\"end\":54987,\"start\":54910},{\"end\":55381,\"start\":55343},{\"end\":55632,\"start\":55566},{\"end\":56039,\"start\":55958},{\"end\":56493,\"start\":56429},{\"end\":56820,\"start\":56731},{\"end\":57283,\"start\":57202},{\"end\":57738,\"start\":57667},{\"end\":58181,\"start\":58100},{\"end\":58661,\"start\":58623},{\"end\":59123,\"start\":59046},{\"end\":59574,\"start\":59510},{\"end\":59995,\"start\":59928},{\"end\":60506,\"start\":60444},{\"end\":61005,\"start\":60924},{\"end\":61452,\"start\":61335},{\"end\":61926,\"start\":61845},{\"end\":62375,\"start\":62294},{\"end\":62779,\"start\":62698},{\"end\":63171,\"start\":63122}]"}}}, "year": 2023, "month": 12, "day": 17}