{"id": 15324574, "updated": "2023-11-10 23:24:51.406", "metadata": {"title": "Nyx: A MASSIVELY PARALLEL AMR CODE FOR COMPUTATIONAL COSMOLOGY", "authors": "[{\"first\":\"Ann\",\"last\":\"Almgren\",\"middle\":[\"S.\"]},{\"first\":\"John\",\"last\":\"Bell\",\"middle\":[\"B.\"]},{\"first\":\"Mike\",\"last\":\"Lijewski\",\"middle\":[\"J.\"]},{\"first\":\"Zarija\",\"last\":\"Luki\u0107\",\"middle\":[]},{\"first\":\"Ethan\",\"last\":\"Van Andel\",\"middle\":[]}]", "venue": "Journal of Open Source Software", "journal": "The Astrophysical Journal", "publication_date": {"year": 2013, "month": null, "day": null}, "abstract": "We present a new N-body and gas dynamics code, called Nyx, for large-scale cosmological simulations. Nyx follows the temporal evolution of a system of discrete dark matter particles gravitationally coupled to an inviscid ideal fluid in an expanding universe. The gas is advanced in an Eulerian framework with block-structured adaptive mesh refinement; a particle-mesh scheme using the same grid hierarchy is used to solve for self-gravity and advance the particles. Computational results demonstrating the validation of Nyx on standard cosmological test problems, and the scaling behavior of Nyx to 50,000 cores, are presented.", "fields_of_study": "[\"Physics\"]", "external_ids": {"arxiv": null, "mag": "2049440937", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jossw/SextonLADFMZ21", "doi": "10.1088/0004-637x/765/1/39"}}, "content": {"source": {"pdf_hash": "4de26026865539f3f57bef600c20ef6134e8cffe", "pdf_src": "IOP", "pdf_uri": "[\"https://arxiv.org/pdf/1301.4498v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://iopscience.iop.org/article/10.1088/0004-637X/765/1/39/pdf", "status": "BRONZE"}}, "grobid": {"id": "a266d4c8af3ebec59b17407d3bcb630413f0ccd8", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/4de26026865539f3f57bef600c20ef6134e8cffe.txt", "contents": "\nNyx: A MASSIVELY PARALLEL AMR CODE FOR COMPUTATIONAL COSMOLOGY\n2013 March 1\n\nAnn S Almgren \nCenter for Computational Sciences and Engineering\nLawrence Berkeley National Laboratory\n94720BerkeleyCAUSA\n\nJohn B Bell \nCenter for Computational Sciences and Engineering\nLawrence Berkeley National Laboratory\n94720BerkeleyCAUSA\n\nMike J Lijewski \nCenter for Computational Sciences and Engineering\nLawrence Berkeley National Laboratory\n94720BerkeleyCAUSA\n\nZarija Luki\u0107 \nComputational Cosmology Center\nLawrence Berkeley National Laboratory\n94720BerkeleyCAUSA\n\nEthan Van Andel \nMathematics Department\nUC Berkeley\n94720BerkeleyCAUSA\n\nNyx: A MASSIVELY PARALLEL AMR CODE FOR COMPUTATIONAL COSMOLOGY\n\nThe Astrophysical Journal\n76514pp392013 March 110.1088/0004-637X/765/1/39Received 2012 July 14; accepted 2013 January 11;Online-only material: color figuresgravitation -hydrodynamics -methods: numerical\nWe present a new N-body and gas dynamics code, called Nyx, for large-scale cosmological simulations. Nyx follows the temporal evolution of a system of discrete dark matter particles gravitationally coupled to an inviscid ideal fluid in an expanding universe. The gas is advanced in an Eulerian framework with block-structured adaptive mesh refinement; a particle-mesh scheme using the same grid hierarchy is used to solve for self-gravity and advance the particles. Computational results demonstrating the validation of Nyx on standard cosmological test problems, and the scaling behavior of Nyx to 50,000 cores, are presented.\n\nINTRODUCTION\n\nUnderstanding how the distribution of matter in the universe evolves throughout cosmic history is one of the major goals of cosmology. Properties of the large-scale structure depend strongly on a cosmological model, as well as the initial conditions in that model. Thus, in principle, we can determine the cosmological parameters of our universe by matching the observed distribution of matter to the best-fit calculated model universe. This is much easier said then done. Observations, with the exception of gravitational lensing, cannot directly access the dominant form of matter, the dark matter. Instead they reveal information about baryons, which in some biased way trace the dark matter. On the theory side, equations describing the evolution of density perturbations (e.g., Peebles 1980) can be easily solved only when the amplitude of perturbations is small. However, in most systems of interest, the local matter density, \u03c1, can be several orders of magnitude greater than the average density, \u03c1 0 , making the density contrast, \u03b4 \u2261 (\u03c1 \u2212 \u03c1 0 )/\u03c1 0 , much greater then unity.\n\nDue to this nonlinearity, and the uncertainty in the initial conditions of the universe, a single \"heroic\" simulation is not sufficient to resolve most of the cosmological questions of interest; the ability to run many simulations of different cosmologies, with different approximations to the governing physics, is imperative. Semi-analytic models (for example, White & Frenk 1991 in the context of galaxy formation) or scaling relations (e.g., Smith et al. 2003) present an interesting alternative to expensive simulations, but they must also be validated or calibrated by simulations in order to be trustworthy. Finally, besides being an invaluable theorist's tool, simulations play a crucial role in bridging the gap between theory and observation, something that is becoming more and more important as large sky surveys are starting to collect data.\n\nTo match the capabilities of future galaxy/cluster surveys, cosmological simulations must be able to simulate volumes \u223c1 Gpc on a side while resolving scales on the order of \u223c10 kpc or even smaller. Clearly, in the context of Eulerian gas dynamics, this requires the ability to run simulations with multiple levels of refinement that follow the formation of relevant objects. To resolve L * galaxies with \u223c100 mass elements (particles) per galaxy in simulations of this scale, one needs tens of billions of particles. Such systems, with crossing times of a million years or less, must be evolved for \u223c10 billion years. On the other side of the spectrum are simulations aimed at predicting Ly\u03b1 forest observations, where the dynamical range is more like \u223c10 4 , but as most of the contribution from the forest comes from gas at close to mean density, most of the computational volume must be resolved. These simulations require much \"shallower\" mesh refinement strategies covering a larger fraction of the domain.\n\nIn addition to the challenge of achieving sufficient refinement, simulations need to incorporate a range of different physics beyond the obvious self-gravity. The presence of baryons introduces corrections to gravity-only predictions; these corrections are negligible at very large scales, but become increasingly significant at smaller scales, requiring an accurate treatment of the gas in order to provide realistic results that can be directly compared to observations. Radiative cooling and heating mechanisms are as influential on many observables as the addition of the gas itself, but large uncertainties remain in these terms, and our lack of knowledge increases as we move into highly nonlinear systems. Thus, we can model low density Ly\u03b1 absorbers at higher redshift (e.g., Viel et al. 2005), we can reproduce the bulk properties of the biggest objects in the universe-galaxy clusters (see, e.g., Stanek et al. 2010), but on smaller and much denser scales galaxy formation still remains elusive and is a field of active research (e.g., Benson 2010).\n\nIn this paper, we present a newly developed N-body and gas dynamics code called Nyx. The code models dark matter as a system of Lagrangian fluid elements, or \"particles,\" gravitationally coupled to an inviscid ideal fluid representing baryonic matter. The fluid is modeled using a finite volume representation in an Eulerian framework with block-structured adaptive mesh refinement (AMR). The mesh structure used to evolve fluid quantities is also used to evolve the particles via a particlemesh (PM) method. In order to more accurately treat hypersonic motions, where the kinetic energy is many orders of magnitude larger than the internal energy of the gas, we employ the dual energy formulation, where both the internal and total energy equations are solved on the grid during each time step.\n\nThere are a number of existing finite volume, AMR codes; most commonly used in the cosmology community are Enzo (Bryan et al. 1995), ART (Kravtsov et al. 1997), FLASH (Fryxell et al. 2000), and RAMSES (Teyssier 2002). Enzo, FLASH, and Nyx are all similar in that they use block-structured AMR; RAMSES and ART, by contrast, refine locally on individual cells, resulting in very different refinement patterns and data structures. Among the block-structured AMR codes, Enzo and FLASH enforce a strict parent-child relationship between patches; i.e., each refined patch is fully contained within a single parent patch. FLASH also enforces that all grid patches have the same size. Nyx requires only that the union of fine patches be contained within the union of coarser patches with suitable proper nesting. Again, this results in different refinement patterns, data structures, communication overhead, and scaling behavior.\n\nTime-refinement strategies vary as well. FLASH advances the solution at every level with the same time step, \u0394t, while Enzo, ART and RAMSES allow for subcycling in time. A standard subcycling strategy enforces that \u0394t/\u0394x is constant across levels; ART and Enzo allow greater refinement in time than space when appropriate. Nyx maintains several different subcycling strategies. The user can specify at run-time that Nyx use no subcycling, standard subcycling, a specified subcycling pattern or \"optimal subcycling.\" In the case of optimal subcycling, Nyx chooses, at specified coarse time intervals during a run, the most efficient subcycling pattern across all levels based on an automated assessment of the computational cost of each option at that time (E. Van Andel et al. 2013, in preparation).\n\nOne of the major goals behind the development of a new simulation code, in an already mature field such as computational cosmology, is to ensure effective utilization of existing and future hardware. Future large-scale cosmological simulations will require codes capable of efficient scaling to hundreds of thousands of cores, and effective utilization of the multicore, shared memory, nature of individual nodes. One such code for gravityonly simulations is HACC (Habib et al. 2009). The frameworks for existing hydro codes such as Enzo and FLASH are currently being substantially rewritten for current and future architectures. Nyx, like the CASTRO code for radiation-hydrodynamics Zhang et al. 2011), and the MAESTRO code for low Mach number astrophysics (Almgren et al. 2006a(Almgren et al. , 2006bAlmgren et al. 2008;Zingale et al. 2009;Nonaka et al. 2010), is built on the BoxLib (BoxLib 2011) software framework for block-structured adaptive mesh methods, and as such it leverages extensive efforts to achieve the massively parallel performance that future simulations will demand. BoxLib currently supports a hierarchical programming approach for multicore architectures based on both MPI and OpenMP. Individual routines, such as those to evaluate heating and cooling source terms, are written in a modular fashion that makes them easily portable to GPUs.\n\nIn the next section we present the equations that are solved in Nyx. In Section 3 we present an overview of the code, and in Section 4 we discuss structured grid AMR and the multilevel algorithm. Section 5 describes in more detail the parallel implementation, efficiency and scaling results. Section 6 contains two different cosmological tests; the first is a pure N-body simulation, and the second is the Santa Barbara cluster simulation, which incorporates both gas dynamics and dark matter. In the final section we discuss future algorithmic developments and upcoming simulations using Nyx.\n\n\nBASIC EQUATIONS\n\n\nExpanding Universe\n\nIn cosmological simulations, baryonic matter is evolved by solving the equations of self-gravitating gas dynamics in a coordinate system that is comoving with the expanding universe. This introduces the scale factor, a, into the standard hyperbolic conservation laws for gas dynamics, where a is related to the redshift, z, by a = 1/(1+z), and can also serve as a time variable. The evolution of a in Nyx is described by the Friedmann equation that, for the two-component model we consider, with Hubble constant, H 0 , and cosmological constant,\n\u03a9 \u039b , has the form d dt ln a =\u0227 a = H 0 \u03a9 0 a 3 + \u03a9 \u039b ,(1)\nwhere \u03a9 0 is the total matter content of the universe today.\n\n\nGas Dynamics\n\nIn the comoving frame, we relate the comoving baryonic density, \u03c1 b , to the proper density, \u03c1 proper , by \u03c1 b = a 3 \u03c1 proper , and define U to be the peculiar proper baryonic velocity. The continuity equation is then written as\n\u2202\u03c1 b \u2202t = \u2212 1 a \u2207 \u00b7 (\u03c1 b U ) .(2)\nKeeping the equations in conservation form as much as possible, we can write the momentum equation as\n\u2202(a\u03c1 b U ) \u2202t = \u2212\u2207 \u00b7 (\u03c1 b UU) \u2212 \u2207p + \u03c1 b g ,(3)\nwhere the pressure, p, is related to the proper pressure, p proper , by p = a 3 p proper . Here g = \u2212\u2207\u03c6 is the gravitational acceleration vector.\n\nWe use a dual energy formulation similar to that used in Enzo (Bryan et al. 1995), in which we evolve the internal energy as well as the total energy during each time step. The evolution equations for internal energy, e, and total energy, E = e + (1/2)U 2 , can be written:\n\u2202(a 2 \u03c1 b e) \u2202t = \u2212a\u2207 \u00b7 (\u03c1 b Ue) \u2212 ap\u2207 \u00b7 U + a\u0227 ((2 \u2212 3(\u03b3 \u2212 1))\u03c1 b e) + a\u039b H C ,(4)\u2202(a 2 \u03c1 b E) \u2202t = \u2212a\u2207 \u00b7 (\u03c1 b UE + pU ) + a\u03c1 b U \u00b7 g + a\u0227 ((2 \u2212 3(\u03b3 \u2212 1))\u03c1 b e) + a\u039b H C ,(5)\nwhere \u039b H C represents the combined heating and cooling terms.\n\nWe synchronize E and e at the end of each time step; the procedure depends on the magnitude of e relative to E, and is described in the next section. We consider the baryonic matter to satisfy a gamma-law equation of state (EOS), where we compute the pressure, p = (\u03b3 \u2212 1)\u03c1e, with \u03b3 = 5/3. The composition is assumed to be a mixture of hydrogen and helium in their primordial abundances with the different ionization states for each element computed by assuming statistical equilibrium. The details of the EOS and how it is related to the heating and cooling terms will be discussed in future work. Given \u03b3 = 5/3, we can simplify the energy equations to the form\n\u2202(a 2 \u03c1 b E) \u2202t = \u2212a\u2207 \u00b7 (\u03c1 b UE + pU ) + a(\u03c1 b U \u00b7 g + \u039b H C ) (6) \u2202(a 2 \u03c1 b e) \u2202t = \u2212a\u2207 \u00b7 (\u03c1 b Ue) \u2212 ap\u2207 \u00b7 U + a\u039b H C .(7)\n\nDark Matter\n\nMatter content in the universe is strongly (80-90%) dominated by cold dark matter, which can be modeled as a nonrelativistic, pressureless fluid. The evolution of the phase-space function, f, of the dark matter is thus given by the collisionless Boltzmann (aka Vlasov) equation, which in expanding space is\n\u2202f \u2202t + 1 ma 2 p \u00b7 \u2207f \u2212 m\u2207\u03c6 \u00b7 \u2202f \u2202p = 0 ,(8)\nwhere m and p are mass and momentum, respectively, and \u03c6 is the gravitational potential.\n\nRather than trying to solve Vlasov equation directly, we use Monte Carlo sampling of the phase-space distribution at some initial time to define a particle representation, and then evolve the particles as an N-body system. Since particle orbits are integrals of a Hamiltonian, Liouville's theorem ensures that at some later time particles are still sampling the phase space distribution as given in Equation (8). Thus, in our simulations we represent dark matter as discrete particles with particle i having comoving location, x i , and peculiar proper velocity, u i , and solve the system\ndx i dt = 1 a u i (9) d(au i ) dt = g i(10)\nwhere g i is gravitational acceleration evaluated at the location of particle i, i.e., g i = g(x i , t).\n\n\nSelf-gravity\n\nThe dark matter and baryons both contribute to the gravitational field and are accelerated by it. Given the comoving dark matter density, \u03c1 dm , we define \u03c6 by solving\n\u2207 2 \u03c6(x, t) = 4\u03c0G a (\u03c1 b + \u03c1 dm \u2212 \u03c1 0 ) ,(11)\nwhere G is the gravitational constant and \u03c1 0 is the average value of (\u03c1 dm + \u03c1 b ) over the domain.\n\n\nCODE OVERVIEW\n\n\nExpanding Universe\n\nBefore evolving the gas and dark matter forward in time, we compute the evolution of a over the next full coarse level time step. Given a n defined as a at time t n , we calculate a n+1 via m second-order Runge-Kutta steps. The value of m is chosen such that the relative difference between a n+1 computed with m and 2m steps is less than 10 \u22128 . This accuracy is typically achieved with m = 8-32, and the computational cost of advancing a is trivial relative to other parts of the simulation. We note that \u0394t is constrained by the growth rate of a such that a changes by no more than 1% in each time step at the coarsest level. Once a n+1 is computed, values of a needed at intermediate times are computed by linear interpolation between a n and a n+1 .\n\n\nGas Dynamics\n\nWe describe the state of the gas as U = (\u03c1 b , a\u03c1 b U, a 2 \u03c1 b E, a 2 \u03c1 b e), then write the evolution of the gas as\n\u2202U \u2202t = \u2212\u2207 \u00b7 F + S e + S g + S H C ,(12)\nwhere F = (1/a \u03c1 b U, \u03c1 b UU, a(\u03c1 b UE + pU ), a\u03c1 b Ue) is the flux vector, S e = (0, 0, 0, \u2212ap\u2207 \u00b7 U ) represents the additional term in the evolution equation for internal energy, S g = (0, \u03c1 b g, a\u03c1 b U \u00b7 g, 0) represents the gravitational source terms, and S H C = (0, 0, a\u039b H C , a\u039b H C ) represents the combined heating and cooling source terms. The state, U, and all source terms are defined at cell centers; the fluxes are defined on cell faces. We compute F using an unsplit Godunov method with characteristic tracing and full corner coupling (Colella 1990;Saltzman 1994). See Almgren et al. (2010) for a complete description of the algorithm without the scale factor, a; here we include a in all terms as appropriate in a manner that preserves second-order temporal accuracy. The hydrodynamic integration supports both unsplit piecewise linear (Colella 1990;Saltzman 1994) and unsplit piecewise parabolic method (PPM) schemes (Miller & Colella 2002) to construct the time-centered edge states used to define the fluxes. The piecewise linear version of the implementation includes the option to include a reference state, as discussed in Colella & Woodward (1984) and Colella & Glaz (1985). We choose as a reference state the average of the reconstructed profile over the domain of dependence of the fastest characteristic in the cell propagating toward the interface. (The value of the reconstructed profile at the edge is used if the fastest characteristic propagates away from the edge.) This choice of reference state minimizes the degree to which the algorithm relies on the linearized equations. In particular, this choice eliminates one component of the characteristic extrapolation to edges used in secondorder Godunov algorithms. For the hypersonic flows typical of cosmological simulations, we have found the use of reference states improves the overall robustness of the algorithm. The Riemann solver in Nyx iteratively solves the Riemann problem using a two-shock approximation as described in Colella & Glaz (1985); this has been found to be more robust for hypersonic cosmological flows than the simpler linearized approximate Riemann solver described in Almgren et al. (2010). In cells where e is less than 0.01% of E, we use e as evolved independently to compute temperature and pressure from the EOS and redefine E as e + (1/2)U 2 ; otherwise we redefine e as E \u2212 (1/2U 2 ).\n\nThe gravitational source terms, S g , in the momentum and energy equations are discretized in time using a predictor-corrector approach. Two alternatives are available for the discretization of \u03c1 b U \u00b7g in the total energy equation. The most obvious discretization is to compute the product of the cell-centered momentum, (\u03c1 b U ), and cell-centered gravitational vector, g, at each time. While this is spatially and temporally second-order accurate, it can have the un-physical consequence of changing the internal energy, e = E \u2212 (1/2)U 2 , since the update to E is analytically but not numerically equivalent to the update to the kinetic energy calculated using the updates to the momenta. A second alternative defines the update to E as the update to (1/2)U 2 . A more complete review of the design choices for including self-gravity in the Euler equations is given in Springel (2010).\n\n\nDark Matter\n\nWe evolve the positions and velocities of the dark matter particles using a standard kick-drift-kick sequence, identical to that described in, for example, Miniati & Colella (2007). We compute g i at the ith particle's location, x i by linearly interpolating g from cell centers to x i . To move the particles, we first accelerate the particle by \u0394t/2, then advance the location using this time-centered velocity:\n(au i ) n+1/2 = (au i ) n + \u0394t 2 g n i (13) x n+1 i = x n i + \u0394t 1 a n+1/2 u n+1/2 i .(14)\nAfter gravity has been computed at time t n+1 , we complete the update of the particle velocities,\n(au i ) n+1 = (au i ) n+1/2 + \u0394t 2 g n+1 i .(15)\nWe observe that this scheme is symplectic, thus conserves the integral of motion on average (Yoshida 1993). Computationally the scheme is also appealing because no additional storage is required during the time step to hold intermediate positions or velocities.\n\n\nSelf-gravity\n\nWe define \u03c1 dm on the mesh using the cloud-in-cell scheme, whereby we assume the mass of the ith particle is uniformly distributed over a cube of side \u0394x centered at x i . We assign the mass of each particle to the cells on the grid in proportion to the volume of the intersection of each cell with the particle's cube, and divide these cell values by \u0394x 3 to define the comoving density that contributes to the right-hand side of Equation (11).\n\nWe solve for \u03c6 on the same cell centers where \u03c1 b and \u03c1 dm are defined. The Laplace operator is discretized using the standard seven-point finite difference stencil and the resulting linear system is solved using geometric multigrid techniques, specifically V-cycles with red-black Gauss-Seidel relaxation. The tolerance of the solver is typically set to 10 \u221212 , although numerical exploration indicates that this tolerance can be relaxed. While the multigrid solvers in BoxLib support Dirichlet, Neumann, and periodic boundary conditions, for the cosmological applications presented here we always assume periodic boundaries. Further gains in efficiency result from using the solution from the previous Poisson solve as an initial guess for the new solve.\n\nGiven \u03c6 at cell centers, we compute the average value of g over the cell as the centered difference of \u03c6 between adjacent cell centers. This discretization, in combination with the interpolation of g from cell centers to particle locations described above, ensures that on a uniform mesh the gravitational force of a particle on itself is identically zero (Hockney & Eastwood 1981).\n\n\nComputing the Time Step\n\nThe time step is computed using the standard CFL condition for explicit methods, with additional constraints from the dark matter particles and from the evolution of a. Using the userspecified hyperbolic CFL factor, 0 < \u03c3 CFL,hyp < 1, and the sound speed, c, computed by the EOS, we define\n\u0394t hyp = \u03c3 CFL,hyp min i=1...3 a\u0394x max x |U \u00b7 e i | + c ,(16)\nwhere e i is the unit vector in the ith direction and max x is the maximum taken over all computational grid cells in the domain. The time step constraint due to particles requires that the particles not move farther than \u0394x in a single time step. Since u n+1/2 i is not yet known when we compute the time step, \u0394t at time t n , for the purposes of computing \u0394t, we estimate u n+1/2 i by u n i . Using the user-specified CFL number for dark matter, 0 < \u03c3 CFL,dm < 1, which is independent of the hyperbolic CFL number, we define\n\u0394t dm = \u03c3 CFL,dm min i=1...3 a\u0394x max j |u j \u00b7 e i | ,(17)\nwhere max j is the maximum taken over all particles j. The default value for \u03c3 CFL,dm = 0.5. In cases where the particle velocities are initially small but the gravitational acceleration is large, we also constrain \u0394t dm by requiring that g i \u0394t 2 dm < \u0394x for all particles to prevent rapid acceleration resulting in a particle moving too far.\n\nFinally, we compute \u0394t a as the time step over which a would change by 1%. The time step taken by the code is set by the smallest of the three:\n\u0394t = min (\u0394t dm , \u0394t hyp , \u0394t a ).(18)\nWe note that non-zero source terms can also potentially constrain the time step; we defer the details of that discussion to a future paper.\n\n\nSingle-level Integration Algorithm\n\nThe algorithm at a single level of refinement begins by computing the time step, \u0394t, and advancing a from t n to t n+1 = t n + \u0394t. The rest of the time step is then composed of the following steps:\n\nStep 1: Compute \u03c6 n and g n using \u03c1 n b and \u03c1 n dm , where \u03c1 n dm is computed from the particles at x n i . We note that in the single-level algorithm we can instead use g as computed at the end of the previous step because there have been no changes to x i or \u03c1 b since then.\n\nStep 2: Advance U by \u0394t.\n\nIf we are using a predictor-corrector approach for the heating and cooling source terms, then we include all source terms in the explicit update:\nU n+1, * = U n \u2212 \u0394t\u2207 \u00b7 F n+1/2 + \u0394tS n+1/2 e + \u0394tS n g + \u0394tS n H C ,(19)\nwhere F n+1/2 and S n+1/2 e are computed by predicting from the U n states. If we are instead using Strang splitting for the heating and cooling source terms, we first advance (\u03c1e) and \u03c1E by integrating the source terms in time for (1/2)\u0394t\n(\u03c1e) n, * = (\u03c1e) n + \u039b H C dt ,(20)(\u03c1E) n, * = (\u03c1E) n + \u039b H C dt ,(21)\nthen advance the solution using time-centered fluxes and S e and an explicit representation of S g at time t n :\nU n+1, * = U n, * \u2212 \u0394t\u2207 \u00b7 F n+1/2 + \u0394tS n+1/2 e + \u0394tS n g ,(22)\nwhere F n+1/2 and S n+1/2 e are computed by predicting from the U n, * states. The details of how the heating and cooling source terms are incorporated depend on the specifics of the heating and cooling mechanisms represented by \u039b H C ; we defer the details of that discussion to a future paper.\n\nStep 3: Interpolate g n from the grid to the particle locations, then advance the particle velocities by \u0394t/2 and particle positions by \u0394t:\nu n+1/2 i = 1 a n+1/2 a n u n i + \u0394t 2 g n i (23) x n+1 i = x n i + \u0394t a n+1/2 u n+1/2 i .(24)\nStep 4: Compute \u03c6 n+1 and g n+1 using \u03c1 n+1, * b and \u03c1 n+1 dm , where \u03c1 n+1 dm is computed from the particles at x n+1 i . Here we can use \u03c6 n as an initial guess for \u03c6 n+1 in order to reduce the time spent in multigrid to reach the specified tolerance.\n\nStep 5: Interpolate g n+1 from the grid to the particle locations, then update the particle velocities, u n+1\ni u n+1 i = 1 a n+1 a n+1/2 u n+1/2 i + \u0394t 2 g n+1 i .(25)\nStep 6: Correct U with time-centered source terms, and replace e by E \u2212 (1/2)U 2 as appropriate.\n\nIf we are using a predictor-corrector approach for the heating and cooling source terms, we correct the solution by effectively time-centering all the source terms:\nU n+1 = U n+1, * + \u0394t 2 S n+1, * g \u2212 S n g + \u0394t 2 S n+1, * H C \u2212 S n H C .\n(26) If we are using Strang splitting for the heating and cooling source terms, we time-center the gravitational source terms only,\nU n+1, * * = U n+1, * + \u0394t 2 S n+1, * g \u2212 S n g(27)\nthen integrate the source terms for another (1/2)\u0394t,\n(\u03c1e) n+1 = (\u03c1e) n+1, * * + \u039b H C dt ,(28)(\u03c1E) n+1 = (\u03c1E) n+1, * * + \u039b H C dt .(29)\nWe note here that the time discretization of the gravitational source terms differs from that in Enzo (Bryan et al. 1995), where S n+1/2 g is computed by extrapolation from values at t n and t n\u22121 .\n\nIf, at the end of the time step, (\u03c1E\n) n+1 \u2212(1/2)\u03c1 n+1 (U n+1 ) 2 > 10 \u22124 (\u03c1E) n+1 , we redefine (\u03c1e) n+1 = (\u03c1E) n+1 \u2212 (1/2)\u03c1 n+1 (U n+1 ) 2 ;\notherwise we use e n+1 as evolved above when needed to compute pressure or temperature, and redefine (\u03c1E) n+1 = (\u03c1e) n+1 + (1/2)\u03c1 n+1 (U n+1 ) 2 . This concludes the single-level algorithm description.\n\n\nAMR\n\nBlock-structured AMR was introduced by Berger & Oliger (1984); it has been demonstrated to be highly successful for gas dynamics by Berger & Colella (1989) in two dimensions and by Bell et al. (1994) in three dimensions, and has a long history of successful use in a variety of fields. The AMR methodology in Nyx uses a nested hierarchy of rectangular grids with refinement of the grids in space by a factor of two between levels, and refinement in time between levels as dictated by the specified subcycling algorithm.\n\n\nMesh Hierarchy\n\nThe grid hierarchy in Nyx is composed of different levels of refinement ranging from coarsest ( = 0) to finest ( = finest ). The maximum number of levels of refinement allowed, max , is specified at the start (or any restart) of a calculation. At any given time in the calculation there may be fewer than max levels in the hierarchy, i.e., finest can change dynamically as the calculation proceeds as long as finest max . Each level is represented by the union of non-overlapping rectangular grids of a given resolution. Each grid is composed of an even number of \"valid\" cells in each coordinate direction; cells are the same size in each coordinate direction but grids may have different numbers of cells in each direction. Each grid also has \"ghost cells\" that extend outside the grid by the same number of cells in each coordinate direction on both the low and high ends of each grid. These cells are used to temporarily hold data used to update values in the \"valid\" cells; when subcycling in time the ghost cell region must be large enough that a particle at level cannot leave the union of level valid and ghost cells over the course of a level \u2212 1 time step. The grids are properly nested in the sense that the union of grids at level + 1 is contained in the union of grids at level . Furthermore, the containment is strict in the sense that the level grids are large enough to guarantee a border at least n proper level cells wide surrounding each level + 1 grid, where n proper is specified by the user. There is no strict parent-child relationship; a single level + 1 grid can overlay portions of multiple level grids.\n\nWe initialize the grid hierarchy and regrid following the procedure outlined in Bell et al. (1994). To define grids at level + 1, we first tag cells at level where user-specified criteria are met. Typical tagging criteria, such as local overdensity, can be selected at run-time; specialized refinement criteria, which can be based on any variable or combination of variables that can be constructed from the fluid state or particle information, can also be used. The tagged cells are grouped into rectangular grids at level using the clustering algorithm given in Berger & Rigoutsos (1991). These rectangular patches are then refined to form the grids at level + 1. Large patches are broken into smaller patches for distribution to multiple processors based on a user-specified max_grid_size parameter.\n\nA particle, i, is said to \"live\" at level if level is the finest level for which the union of grids at that level contains the particle's location, x i . The particle is said to live in the nth grid at level if the particle lives at level and the nth grid at level contains x i . Each particle stores, along with its location, mass, and velocity, the current information about where it lives, specifically the level, grid number, and cell, (i, j, k). Whenever particles move, this information is recomputed for each particle in a \"redistribution\" procedure. The size of a particle in the PM scheme is set to be the mesh spacing of the level at which the particle currently lives.\n\nAt the beginning of every k level time step, where k 1 is specified by the user at run-time, new grid patches are defined at all levels + 1 and higher if < max . In regions previously covered by fine grids, the data are simply copied from old grids to new; in regions that are newly refined, the data are interpolated from underlying coarser grids. The interpolation procedure constructs piecewise linear profiles within each coarse cell based on nearest neighbors; these profiles are limited so as to not introduce any new maxima or minima, then the fine grid values are defined to be the value of the trilinear profile at the center of the fine grid cell, thus ensuring conservation of all interpolated quantities. All particles at levels through max are redistributed whenever the grid hierarchy is changed at levels +1 and higher.\n\n\nSubcycling Options\n\nNyx supports four different options for how to relate \u0394t , the time step at level > 0, to \u0394t 0 , the time step at the coarsest level. The first is a non-subcycling option; in this case all levels are advanced with the same \u0394t, i.e., \u0394t = \u0394t 0 for all . The second is a \"standard\" subcycling option, in which \u0394t/\u0394x is constant across all levels, i.e., \u0394t = r \u0394t 0 for spatial refinement ratio, r, between levels. These are both standard options in multilevel algorithms. In the third case, the user specifies the subcycling pattern at run-time; for example, the user could specify subcycling between levels 0 and 1, but no subcycling between levels 1, 2, and 3. Then \u0394t 3 = \u0394t 2 = \u0394t 1 = r \u0394t 0 . The final option is \"optimal subcycling,\" in which the optimal subcycling algorithm decides at each coarse time step, or other specified interval, what the subcycling pattern should be. For example, if the time step at each level is constrained by \u0394t a , the limit that enforces the condition that a not change more than 1% in a time step, then not subcycling is the most efficient way to advance the solution on the multilevel hierarchy. If dark matter particles at level 2 are moving a factor of two faster than particles at level 3 and \u0394t dm is the limiting constraint in choosing the time step at those levels, it may be most efficient to advance levels 2 and 3 with the same \u0394t, but to subcycle level 1 relative to level 0 and level 2 relative to level 1. These choices are made by the optimal subcycling algorithm during the run; the algorithm used to compute the optimal subcycling pattern is described in E. Van Andel et al. (2013, in preparation).\n\nIn addition to the above examples, additional considerations, such as the ability to parallelize over levels rather than just over grids at a particular level, may arise in large-scale parallel computations, which can make more complex subcycling patterns more efficient than the \"standard\" pattern. We defer further discussion of these cases to future work in which we demonstrate the trade-offs in computational efficiency for specific cosmological examples.\n\n\nMultilevel Algorithm\n\nWe define a complete multilevel \"advance\" as the combination of operations required to advance the solution at level 0 by one level 0 time step and operations required to advance the solution at all finer levels, 0 < finest , to the same time. We define n as the number of time steps taken at level for each time step taken at the coarsest level, i.e., \u0394t 0 = n \u0394t for all levels, 0 < finest . If the user specifies no subcycling, then n = 1; if the user specifies standard subcycling then n = r . If the user defines a subcycling pattern, this is used to compute n at the start of the computation; if optimal subcycling is used then n is recomputed at specified intervals.\n\nThe multilevel advance begins by computing \u0394t for all 0, which first requires the computation of the maximum possible time step \u0394t max, at each level independently (including only the particles at that level). Given a subcycling pattern and \u0394t max, at each level, the time steps are computed as:\n\u0394t 0 = min 0 finest (n \u00b7 \u0394t max, )(30)\u0394t = \u0394t 0 n .(31)\nOnce we have computed \u0394t for all and advanced a from time t to t + \u0394t 0 , a complete multilevel \"advance\" is defined by calling the recursive function, Advance( ), described below, with = 0. We note that in the case of no-subcycling, the entire multilevel advance occurs with a single call to Advance(0); in the case of standard subcycling Advance( ) will be called recursively for each , 0 finest . Intermediate subcycling patterns require calls to Advance( ) for every level that subcycles relative to level \u2212 1. In the notation below, the superscript n refers to the time, t n , at which each call to Advance begins at level , and the superscript n + 1 refers to time t n + \u0394t .\n\nAdvance ( ):\n\nStep 0: Define a as the finest level such that \u0394t a = \u0394t . If we subcycle between levels and + 1, then a = and this call to Advance only advances level . If, however, we do not subcycle, we can use a more efficient multi-level advance on levels through a .\n\nStep 1: Compute \u03c6 n and g n at levels \u2192 finest using \u03c1 n b and \u03c1 n dm , where \u03c1 n dm is computed from the particles at x n i . This calculation involves solving the Poisson equation on the multilevel grid hierarchy using all finer levels; further detail about computing \u03c1 dm and solving on the multilevel hierarchy is given in the next section. If \u03c6 has already been computed at this time during a multilevel solve in Step 1 at a coarser level, this step is skipped.\n\nStep 2: Advance U by \u0394t at levels \u2192 a .\n\nBecause we use an explicit method to advance the gas dynamics, each level is advanced independently by \u0394t using Dirichlet boundary conditions from a coarser level as needed for boundary conditions. If a > , then after levels \u2192 a have been advanced, we perform an explicit reflux of the hydrodynamic quantities (see, e.g., Almgren et al. 2010 for more details on refluxing) from level a down to level to ensure conservation.\n\nStep 3: Interpolate g n at levels \u2192 a from the grid to the particle locations, then advance the particle velocities at levels \u2192 a by \u0394t /2 and particle positions by \u0394t . The operations in this step are identical to those in the singlelevel version and require no communication between grids or between levels. Although a particle may move from a valid cell into a ghost cell or from a ghost cell into another ghost cell during this step, it remains a level particle until redistribution occurs after the completion of the level \u2212 1 time step.\n\nStep 4: Compute \u03c6 n+1 and g n+1 at levels \u2192 a using \u03c1 n+1, * b and \u03c1 n+1 dm , where \u03c1 n+1 dm is computed from the particles at x n+1 i . Unlike the solve in Step 1, this solve over levels \u2192 a is only a partial multilevel solve unless this is a no-subcycling algorithm; further detail about the multilevel solve is given in the next section.\n\nStep 5: Interpolate g n+1 at levels \u2192 a from the grid to the particle locations, then update the particle velocities at levels \u2192 a by an additional \u0394t /2. Again this step is identical to the single-level algorithm.\n\nStep 6: Correct U at levels \u2192 a with time-centered source terms, and replace e by E \u2212 (1/2)U 2 as appropriate.\n\nStep 7: If a < finest , Advance( a + 1) n a +1 /n times.\n\nStep 8: Perform any final synchronizations 1. If a < finest , reflux the hydrodynamic quantities from level a + 1 to level a . 2. If a < finest , average down the solution from level a + 1 to level a .\n\n3. If < a , average down the solution from level a through level . 4. If a < finest , perform an elliptic synchronization solve to compute \u03b4\u03c6 at levels through finest , where the change in \u03c6 results from the change in \u03c1 at level a due to refluxing from level a + 1, and from the mismatch in Neumann boundary conditions at the a / a + 1 interface (that results because the finest level in the solve from\n\nStep 4 was a , not finest ). Add \u03b4\u03c6 to \u03c6 at levels through finest . This synchronization solve is described in more detail in Almgren et al. (2010) for the case of standard subcycling. 5. Redistribute particles at levels \u2192 finest ; Note that, if > 0, particles that started the time step at level but now would live at level \u2212 1 are kept for now in the ghost cells at level ; they are not redistributed to level \u2212 1 until the level \u2212 1 time step is complete.\n\n\nGravity Solves\n\nIn the algorithm as described above, there are times when the Poisson equation for the gravitational potential,\n\u2207 2 \u03c6 = 4\u03c0G a (\u03c1 b + \u03c1 dm \u2212 \u03c1 0 ) ,(32)\nis solved simultaneously on multiple levels, and times when the solution is only required on a single level. We define L as the standard seven-point finite difference approximation to \u2207 2 at level , modified as appropriate at the /( \u2212 1) interface if > 0; see, e.g., Almgren et al. (1998) for details of the multilevel cell-centered interface stencil. When = finest in Step 1, or = a in Step 4, we solve\nL \u03c6 = 4\u03c0G a \u03c1 b + \u03c1 dm \u2212 \u03c1 0\non the union of grids at level only; this is referred to as a level solve. If = 0 then the grids at this level cover the entire domain and the only boundary conditions required are periodic boundary conditions at the domain boundaries. If > 0 then Dirichlet boundary conditions for the level solve at level are supplied at the /( \u2212 1) interface from data at level \u2212 1. Values for \u03c6 at level \u2212 1 are assumed to live at the cell centers of level \u2212 1 cells; these values are interpolated tangentially at the /( \u2212 1) interface to supply boundary values with spacing \u0394x for the level grids. The modified stencil obviates the need for interpolation normal to the interface. The resulting linear system is solved using geometric multigrid techniques, specifically V-cycles with red-black Gauss-Seidel relaxation. We note that after a level solve with > 0, \u03c6 at levels and \u2212 1 satisfy Dirichlet but not Neumann matching conditions. This mismatch is corrected in the elliptic synchronization described in Step 8.\n\nWhen a multilevel solve is required, we define L comp ,m as the composite grid approximation to \u2207 2 on levels through m, and define a composite solve as the process of solving \nL m \u03c6 comp = 4\u03c0G a \u03c1 m b + \u03c1 m dm \u2212 \u03c1 0 at level m, but satisfies L \u03c6 = 4\u03c0G a \u03c1 b + \u03c1 dm \u2212 \u03c1 0\nfor < m only on the regions of each level not covered by finer grids or adjacent to the boundary of the finer grid region. In regions of a level grid covered by level + 1 grids \u03c6 is defined as the volume average of \u03c6 +1 ; in level cells immediately adjacent to the boundary of the union of level + 1 grids, a modified interface operator as described in Almgren et al. (1998) is used. This linear system is also solved using geometric multigrid with V-cycles; here levels through m serve as the finest levels in the V-cycle. After a composite solve on levels through m, \u03c6 satisfies both the Dirichlet and Neumann matching conditions at the interfaces between grids at levels through m; however, there is still a mismatch in Neumann matching conditions between \u03c6 at levels and \u2212 1 if > 0. This mismatch is corrected in the elliptic synchronization described in Step 8.\n\n\nCloud-in-cell Scheme\n\nNyx uses the cloud-in-cell scheme to define the contribution of each dark matter particle's mass to \u03c1 dm ; a particle contributes to \u03c1 dm only on the cells that intersect the cube of side \u0394x centered on the particle. In the single-level algorithm, all particles live on a single level and contribute to \u03c1 dm at that level. If there are multiple grids at a level, and a particle lives in a cell adjacent to a boundary with another grid, some of the particle's contribution will be to the grid in which it lives, and the rest will be to the adjacent grid(s). The contribution of the particle to cells in adjacent grid(s) is computed in two stages: the contribution is first added to a ghost cell of the grid in which the particle lives, then the value in the ghost cell is added to the value in the valid cell of the adjacent grid. The right-hand side of the Poisson solve sees only the values of \u03c1 dm on the valid cells of each grid.\n\nIn the multilevel algorithm, the cloud-in-cell procedure becomes more complicated. During a level level solve or a multilevel solve at levels \u2192 m, a particle at level that lives near the /( \u2212 1) interface may \"lose\" part of its mass across the interface. In addition, during a level solve at level , or a multi-level solve at levels \u2192 m, it is essential to include the contribution from dark matter particle at other levels even if these particles do not lie near a coarse-fine boundary. Both of these complications are addressed by the creation and use of ghost and virtual particles.\n\nThe mass from most particles at levels < is accounted for in the Dirichlet boundary conditions for \u03c6 that are supplied from \u03c6 \u22121 at the /( \u22121) interface. However, level \u22121 particles that live near enough to the /( \u2212 1) interface to deposit part of their mass at level , or have actually moved from level \u2212 1 to level during a level \u2212 1 time step, are not correctly accounted for in the boundary conditions. The effect of these is included via ghost particles, which are copies at level of level \u2212 1 particles. Although these ghost particles live and move at level , they retain the size, \u0394x \u22121 of the particles of which they are copies.\n\nThe mass from particles living at levels > m in a level m level solve or a level \u2192 m multilevel solve, is included at level m via the creation of virtual particles. These are level m representations of particles that live at levels > m. Within Nyx there is the option to have one-to-one correspondence of fine level particles to level m virtual particles; the option also exists to aggregate these fine level particles into fewer, more massive virtual particles at level m. The total mass of the finer level particles is conserved in either representation. These virtual particles are created at level m at the beginning of a level m time step and moved along with the real level m particles; this is necessary because the level particles will not be moved until after the second Poisson solve with top level m has occurred. The mass of the virtual particles contributes to \u03c1 dm at level m following the same cloud-in-cell procedure as followed by the level m particles, except that the virtual particles have size \u0394x m+1 (even if they are copies of particles at level with > m + 1).\n\n\nSOFTWARE DESIGN AND PARALLEL PERFORMANCE\n\n\nOverview\n\nNyx is implemented within the BoxLib (BoxLib 2011) framework, a hybrid C++/Fortran90 software system that provides support for the development of parallel block-structured AMR applications. The basic parallelization strategy uses a hierarchical programming approach for multicore architectures based on both MPI and OpenMP. In the pure-MPI instantiation, at least one grid at each level is distributed to each core, and each core communicates with every other core using only MPI. In the hybrid approach, where on each socket there are n cores that all access the same memory, we can instead have fewer, larger grids per socket, with the work associated with those grids distributed among the n cores using OpenMP.\n\nIn BoxLib, memory management, flow control, parallel communications and I/O are expressed in the C++ portions of the program. The numerically intensive portions of the computation, including the multigrid solvers, are handled in Fortran90. The software supports two data distribution schemes for data at a level, as well as a dynamic switching scheme that decides which approach to use based on the number of grids at a level and the number of processors. The first scheme is based on a heuristic knapsack algorithm as described in Crutchfield (1991) and in Rendleman et al. (2000). The second is based on the use of a Morton-ordering space-filling curve.\n\n\nParticle and Particle-mesh Operations\n\nBoxLib also contains support for parallel particle and PM operations. Particles are distributed to processors or nodes according to their location; the information associated with particle i at location x i exists only on the node that owns the finest-level grid that contains x i . Particle operations are performed with the help of particle iterators that loop over particles; there is one iterator associated with each MPI process, and each iterator loops over all particles that are associated with that process.\n\nThus interactions between particles and grid data, such as the calculation of \u03c1 dm , or the interpolation of g from the grid to x i , require no communication between processors or nodes; they scale linearly with the number of particles, and exhibit almost perfect weak scaling. Similarly, the update of the particle velocities using the interpolated gravitational acceleration, and the update of the particle locations using the particle velocities, require no communication. The only inter-node communication of particle data (location, mass, and velocity) occurs when a particle moves across coarse-fine or fine-fine grid boundaries; in this case the redistribution process recomputes the particle level, grid number, and cell, and the particle information is sent to the node or core where the particle now lives. We note, though, that when information from the particle is represented on the grid hierarchy, such as when the particle \"deposits\" its mass on the grid structure in the form of a density field used in the Poisson solve, that grid-based data may need to be transferred between nodes if the particle is near the boundary of the grid it resides in, and deposits part of its mass into a different grid at the same or a different level.\n\nLooking ahead, we also note here that the particle data structures in BoxLib are written in a sufficiently general form with C++ templates that they can easily be used to represent other types of particles (such as those associated star formation mechanisms) which carry different numbers of attributes beyond location, mass and velocity.\n\n\nParallel I/O and Visualization\n\nAs simulations grow increasingly large, the need to read and write large data sets efficiently becomes increasingly critical. Data for checkpoints and analysis are written by Nyx in a selfdescribing format that consists of a directory for each time step written. Checkpoint directories contain all necessary data to restart the calculation from that time step. Plotfile directories contain data for postprocessing, visualization, and analytics, which can be read using amrvis, a customized visualization package developed at LBNL for visualizing data on AMR grids, VisIt (VisIt User's Manual 2005), or yt (Turk et al. 2011). Within each checkpoint or plotfile directory is an ASCII header file and subdirectories for each AMR level. The header describes the AMR hierarchy, including the number of levels, the grid boxes at each level, the problem size, refinement ratio between levels, step time, etc. Within each level directory are multiple files for the data at that level. Checkpoint and plotfile directories are written at user-specified intervals.\n\nFor output, a specific number of data files per level is specified at run time. Data files typically contain data from multiple processors, so each processor writes data from its associated grid(s) to one file, then another processor can write data from its associated grid(s) to that file. A designated I/O processor writes the header files and coordinates which processors are allowed to write to which files and when. The only communication between processors is for signaling when processors can start writing and for the exchange of header information. While I/O performance even during a single run can be erratic, recent timings of parallel I/O on the Hopper machine at NERSC, which has a theoretical peak of 35 GB s \u22121 , indicate that Nyx's I/O performance, when run with a single level composed of multiple uniformly sized grids, can reach over 34 GB s \u22121 .\n\nRestarting a calculation can present some difficult issues for reading data efficiently. Since the number of files is generally not equal to the number of processors, input during restart is coordinated to efficiently read the data. Each data file is only opened by one processor at a time. The designated I/O processor creates a database for mapping files to processors, coordinates the read queues, and interleaves reading its own data. Each processor reads all data it needs from the file it currently has open. The code tries to maintain the number of input streams to be equal to the number of files at all times.\n\nCheckpoint and plotfiles are portable to machines with a different byte ordering and precision from the machine that wrote the files. Byte order and precision translations are done automatically, if required, when the data are read.\n\n\nParallel Performance\n\nIn Figure 1 we show the scaling behavior of the Nyx code using MPI and OpenMP on the Hopper machine at NERSC. A weak scaling study was performed, so that for each run there was exactly one 128 3 grid per NUMA node, where each NUMA node has 6 cores. 4 The problem was initialized using replication of the Santa Barbara problem, described in the next section. First, the original 256 3 particles were read in, then these particles were replicated in each coordinate direction according to the size of the domain. For example, on a run with 1024 \u00d7 1024 \u00d7 2048 cells, the domain was replicated four times in the x-and y-directions and eight times in the z-direction. With one 128 3 grid per NUMA node, this run would have used 1024 processors. The physical domain was scaled with the index space, so that the resolution (\u0394x) of the problem didn't change with weak scaling, thus the characteristics of the problem which might impact the number of multigrid V-cycles, for example, were unchanged as the problem got larger.\n\nThe timings in Figure 1 are the time per time step spent in different parts of the algorithm for a uniform grid calculation. The \"hydro\" timing represented all time spent to advance the hydrodynamic state, excluding the calculation of the gravity. The computation of gravity is broken into two parts; the setup and initialization of the multigrid solvers, and the actual multigrid V-cycles themselves. The setup phase includes the assignment of mass from the particles to the mesh using the cloud-in-cell scheme. In this run, each multigrid solve took seven V-cycles to reach a tolerance of 10 \u221212 . We note that, for this problem which has an average of one particle per cell, the contribution to the total time from moving and redistributing the particles is negligible.\n\nWe see here that the Nyx code scales well from 48 to 49,152 processors, with a less than 50% increase in total time with the 1000-fold increase in processors. The hydrodynamic core of CASTRO, which is essentially the same as that of Nyx, has shown almost flat scaling to 211K processors on the Jaguarpf machine at OLCF (BoxLib 2011), and only a modest overhead from using AMR with subcycling, ranging from roughly 5% for 8 processors to 19% for 64K processors . MAESTRO, another BoxLib-based code which uses the same linear solver framework as Nyx, has demonstrated excellent scaling to 96K cores on Jaguarpf (BoxLib 2011).\n\n\nVALIDATION\n\nThe hydrodynamic integrator in Nyx was built by extending the integrator in CASTRO to the equations for an expanding universe; the Nyx hydrodynamics have been validated using the same tests, and with the same results, as described in Almgren et al. (2010). To isolate and study the behavior of the component of the algorithm responsible for updating the particle locations and velocities, we have performed a resolution study of a simple two-particle orbit. In a nondimensionalized cubic domain 16 units on a side, we initialized two particles of equal mass each at a distance of 1 unit in the x-direction from the center of the domain. In order to compare the numerical results to the exact circular orbit solution, the initial particle velocities were specified to be those corresponding to a perfectly circular orbit, and Dirichlet boundary conditions, constructed as the sum of two monopole expansions (one from each particle), were imposed at the domain boundaries. Six numerical tests were performed: three with a base grid of 64 3 , and either 0, 1, or 2 levels of refinement; one with a base grid of 128 3 and 0 or 1 level of refinement, and finally a uniform 256 3 case. Three key observations followed. First, the orbits of the particles were stable over the 10 orbits observed, with little variation in the measured properties over time. Second, the orbit radius and kinetic energy of each particle converged with mesh spacing; the maximum deviations of the orbit radius and the particle's kinetic energy from their initial values were no more than approximately 1.1% and 2.4%, respectively, for the coarsest case and 0.17% and 0.36%, respectively, for the finest cases. Third, the difference in the deviation of the orbit radius between the single-level and multilevel simulations with the same finest resolution was less than 0.0002% of the radius; the difference in deviation of the kinetic energy was less than 0.0004% of the initial kinetic energy. We have also conducted several simple tests with known analytical solutions, e.g., the MacLaurin spheroid and Zel'dovich pancakes, to additionally validate the particle dynamics and the expected second-order accuracy and convergence properties of the Nyx gravity solver. Finally, in this section, we present two validation studies in a cosmological context, using wellestablished data sets from the Cosmic Data ArXiv (Heitmann et al. 2005) and the Santa Barbara Cluster Comparison Project (Frenk et al. 1999).\n\n\nDark Matter Only Simulations\n\nTo validate Nyx in a realistic cosmological setting, we first compare Nyx gravity-only simulation results to one of the comparison data sets described in Heitmann et al. (2005). The domain is 256 Mpc h \u22121 on a side, and the cosmology considered is \u03a9 m = 0.314, \u03a9 \u039b = 0.686, h = 0.71, \u03c3 8 = 0.84. Initial conditions are provided at z = 50 by applying the Klypin & Holtzman (1997) transfer function and using the Zel'dovich (1970) approximation. The simulation evolves 256 3 dark matter particles, with a mass resolution of 1.227\u00d710 11 M . Two comparison papers (Heitmann et al. 2005 andHeitmann et al. 2008) demonstrated good agreement on several derived quantities among 10 different, widely used cosmology codes.\n\nHere we compare our results to the Gadget-2 simulations as presented in the Heitmann et al. papers; we refer interested readers to those papers to see a comparison of the Gadget-2 simulations with other codes. We present results from three . Halo mass function also shows good convergence with the increasing force resolution toward Gadget-2 results, run at higher resolution. The solid line is (Sheth & Tormen 1999) fit, shown here purely to guide the eye; ratios are taken with respect to Gadget as in the other plots. (A color version of this figure is available in the online journal.) different Nyx simulations: uniform grid simulations at 256 3 and 1024 3 , and a simulation with a 256 3 base grid and two levels of refinement, each by a factor of two. In Figure 2, we show correlation functions (upper panel), and ratio to the Gadget-2 results (lower panel), to allow for a closer inspection. We see a strong match between the Gadget-2 and Nyx results, and observe convergence of Nyx code toward the Gadget-2 results as the resolution increases. In particular, we note that the effective 1024 3 results achieved with AMR very closely match the results achieved with the uniform 1024 3 grid.\n\nNext, we examine the mass and spatial distribution of halos, two important statistical measures used in different ways throughout cosmology. To generate halo catalogs, we use the same halo finder for all runs, which finds friends-of-friends halos (FOF; Davis et al. 1985) using a linking length of b = 0.2. To focus on the differences between codes we consider halos with as few as 10 particles, although in a real application we would consider only halos with at least hundreds of particles in order to avoid large inaccuracies due to the finite sampling of FOF halos. In Figure 3, we show results for the mass function, and we confirm good agreement between the high resolution Nyx run and Gadget-2. As shown in O' Shea et al. (2005), Heitmann et al. (2005), and Luki\u0107 et al. (2007), common refinement strategies suppress the halo mass function at the low-mass end. This is because small halos form very early and throughout the whole simulation domain; capturing them requires refinement so early and so wide, that block-based refinement (if not Figure 4. Two-point correlation function of dark matter halos, separated in three mass groups. We see good agreement between the two codes, as well as rapid convergence even for halos sampled with a few tens of particles. We emphasize that halos in the range of 10-100 particles would not have been given serious consideration in a real application, but we show them here solely to examine differences between codes. (A color version of this figure is available in the online journal.) Figure 5. Density profiles for the three most massive objects in the Gadget-2 run. The lines are the best fit NFW profile to the Gadget-2 run. We can see that the profiles from the AMR simulation closely match those of the uniform 1024 3 simulation. (A color version of this figure is available in the online journal.) AMR in general) gives hardly any advantage over a fixed-grid simulation. Nyx AMR results shown here are fully consistent with the criteria given in Luki\u0107 et al. (2007), and are similar to other AMR codes (Heitmann et al. 2008).\n\nIn Figure 4 we present the correlation function for halos. Rather than looking at all halos together, we separate them into three mass bins. For the smallest halos we see the offset in the low resolution run, but even that converges quickly, in spite of the fact that the 1024 3 run still has fewer small-mass halos than the Gadget run done with force resolution several times smaller ( Figure 3). Finally, in Figure 5 we examine the inner structure of halos. We observe excellent agreement between the AMR run and the uniform 1024 3 run, confirming the expectation that the structure of resolved halos in the AMR simulation match those in the fixed-grid runs. Overall, the agreement between the Nyx and Gadget-2 results is as expected, and is on a par with other AMR cosmology codes at this resolution as demonstrated in the Heitmann et al. papers. \n\n\nSanta Barbara Cluster Simulations\n\nThe Santa Barbara cluster has become a standard code comparison test problem for cluster formation in a realistic cosmological setting. To make meaningful comparisons, all codes start from the same set of initial conditions and evolve both the gas and dark matter to redshift z = 0 assuming an ideal gas EOS with no heating or cooling (\"adiabatic hydro\"). Initial conditions are constrained such that a cluster (3\u03c3 rare) forms in the central part of the box. The simulation domain is 64 Mpc on a side, with the SCDM cosmology: \u03a9 = \u03a9 m = 1, \u03a9 b = 0.1, h = 0.5, \u03c3 8 = 0.9, and n s = 1. A 3D snapshot of the dark matter density and gas temperature at z = 0 is shown in Figure 6. For the Nyx simulation we use the exact initial conditions from Heitmann et al. (2005), and begin the simulation at z = 63 with 256 3 particles. Detailed results from a code comparison study of the Santa Barbara test problem were published in Frenk et al. (1999); we present here not all, but the most significant diagnostics from that paper for comparison of Nyx with other available codes. We refer the interested reader to that paper for full details of the simulation results for the different codes.\n\nWe first examine the global cluster properties, calculated inside r 200 with respect to the critical density at redshift z = 0. We find the total mass of the cluster to be 1.15 \u00d7 10 15 M ; Frenk et al. (1999) report the mean and 1\u03c3 deviation as (1.1 \u00b1 0.05) \u00d7 10 15 M for all codes. The radius of the cluster from the Nyx run is 2.7 Mpc; Frenk et al. (1999) report 2.7 \u00b1 0.04. The NFW concentration is 7.1; the original paper gives 7.5 as a rough guideline to what the concentration should be.\n\nIn Figure 7 we show comparisons of the radial profiles of several quantities: dark matter density, gas density, pressure, and entropy. Since the original data from the comparison project is not publicly available, we compare only with the average values, as well as the Enzo and ART data as estimated from the figures in Kravtsov et al. (2002). We also note that we did not have exactly the same initial conditions as in Frenk et al. (1999) or Kravtsov et al. (2002) and our starting redshift differs from that of Enzo (z = 30), so we do not expect exact agreement.\n\nIn the original code comparison, the definition of the cluster center was left to the discretion of each simulator, and here we adopt the gravitational potential minimum to define the cluster center. As in Frenk et al. (1999), we radially bin quantities in 15 spherical shells, evenly spaced in log radius, and covering three orders of magnitude-from 10 kpc to 10 Mpc. The pressure reported is p = \u03c1 gas T , and entropy is defined as s = ln(T /\u03c1 2/3 gas ). We find good agreement between Nyx and the other AMR codes, including the well-known entropy flattening in the central part of the cluster. In sharp contrast to grid methods, smoothed particle hydrodynamics (SPH) codes find central entropy to continue rising toward smaller radii. Mitchell et al. (2009) made a detailed investigation of this issue, and have found that the difference between the two is independent of mesh sizes in grid codes, or particle number in SPH. Instead, the difference is fundamental in nature, and is shown to originate as a consequence of the suppression of eddies and fluid instabilities in SPH. (See also Abel 2011 for an SPH formulation that improves mixing at a cost of violating exact momentum conservation.) Overall, we find excellent agreement of Nyx with existing AMR codes.\n\n\nCONCLUSIONS AND FUTURE WORK\n\nWe have presented a new N-body and gas dynamics code, Nyx, for large-scale cosmological simulations. Nyx is designed to efficiently utilize tens of thousands of processors; timings of the code up to almost 50,000 processors show excellent weak scaling behavior. Validation of Nyx in pure dark matter runs and dark matter with adiabatic hydrodynamics has been presented. Future papers will give greater detail on the implementation in Nyx of source terms, and will present results from simulations incorporating different heating and cooling mechanisms of the gas, as needed for increased fidelity in different applications. Scientific studies already underway with Nyx include studies of the Ly\u03b1 forest and galaxy clusters. In addition, we plan to extend Nyx to allow for simulation of alternative cosmological models to \u039bCDM, most interestingly dynamical dark energy and modifications of Einstein's gravity. The existing grid structure and multigrid Poisson solver should make it straightforward to extend the current capability to iterative methods for solving the nonlinear elliptic equations arising in those models. We thank Peter Nugent and Martin White for many useful discussions.\n\n\n\u2212 \u03c1 0 on levels through m. The solution to the composite solve satisfies\n\nFigure 1 .\n1Weak scaling behavior of Nyx on the NERSC Hopper (Cray XE 6) machine for simulations of the replicated Santa Barbara problem. (A color version of this figure is available in the online journal.)\n\nFigure 2 .\n2Two-point correlation function of dark matter particles in 256 Mpc h \u22121 box (upper panel), and the ratio with respect to the Gadget-2 results (lower panel). We show Nyx results from two uniform-grid simulations and one AMR simulation with a 256 3 base grid and two levels of refinement, each by a factor of two. (A color version of this figure is available in the online journal.)\n\nFigure 3\n3Figure 3. Halo mass function also shows good convergence with the increasing force resolution toward Gadget-2 results, run at higher resolution. The solid line is (Sheth & Tormen 1999) fit, shown here purely to guide the eye; ratios are taken with respect to Gadget as in the other plots. (A color version of this figure is available in the online journal.)\n\nFigure 6 .\n6Santa Barbara cluster simulation. We show three orthogonal cuts through the box center showing dark matter density in blue-white colors. Superposed in red are iso-temperature surfaces of 10 7 K gas showing rich structure of intergalactic gas throughout the simulations domain.\n\nFigure 7 .\n7Left: density of dark matter (upper panel) and gas (lower panel); right: pressure (upper) and entropy (lower) profiles for the Santa Barbara cluster. Besides Nyx, we show the average from the original Frenk et al. 1999 paper, and results from two AMR codes, Enzo and ART.\n\n\nThis work was supported by Laboratory Directed Research and Development funding (PI: Peter Nugent) from Berkeley Lab, provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. Additional support for improvements to BoxLib to support the Nyx code and others was provided through the SciDAC FASTMath Institute, funded by the Scientific Discovery through Advanced Computing (SciDAC) program funded by U.S. Department of Energy Office of Advanced Scientific Computing Research (and Office of Basic Energy Sciences/Biological and Environmental Research/High Energy Physics/Fusion Energy Sciences/Nuclear Physics). Calculations presented in this paper used resources of the National Energy Research Scientific Computing Center (NERSC), which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.\nWe note that for small to moderate numbers of cores, pure-MPI runs can be significantly faster than runs using hybrid MPI and OpenMP. However, for the purposes of this scaling study we report only hybrid results.\n\n. T Abel, 10.1111/j.1365-2966.2010.18133.xMNRAS. 413271Abel, T. 2011, MNRAS, 413, 271\n\n. A S Almgren, V E Beckner, J B Bell, 10.1088/0004-637X/715/2/1221ApJ. 7151221Almgren, A. S., Beckner, V. E., Bell, J. B., et al. 2010, ApJ, 715, 1221\n\n. A S Almgren, J B Bell, P Colella, L H Howell, M L Welcome, JCoPh. 1421Almgren, A. S., Bell, J. B., Colella, P., Howell, L. H., & Welcome, M. L. 1998, JCoPh, 142, 1\n\n. A S Almgren, J B Bell, A Nonaka, M Zingale, 10.1086/590321ApJ. 684449Almgren, A. S., Bell, J. B., Nonaka, A., & Zingale, M. 2008, ApJ, 684, 449\n\n. A S Almgren, J B Bell, C A Rendleman, M Zingale, 10.1086/498426ApJ. 637922Almgren, A. S., Bell, J. B., Rendleman, C. A., & Zingale, M. 2006a, ApJ, 637, 922\n\n. A S Almgren, J B Bell, C A Rendleman, M Zingale, 10.1086/507089ApJ. 649927Almgren, A. S., Bell, J. B., Rendleman, C. A., & Zingale, M. 2006b, ApJ, 649, 927\n\n. J Bell, M Berger, J Saltzman, M Welcome, 10.1137/0915008SIAM J. Sci. Statist. Comput. 15127Bell, J., Berger, M., Saltzman, J., & Welcome, M. 1994, SIAM J. Sci. Statist. Comput., 15, 127\n\n. A J Benson, 10.1016/j.physrep.2010.06.001PhR. 49533Benson, A. J. 2010, PhR, 495, 33\n\n. M J Berger, P Colella, JCoPh. 8264Berger, M. J., & Colella, P. 1989, JCoPh, 82, 64\n\n. M J Berger, J Oliger, JCoPh. 53484Berger, M. J., & Oliger, J. 1984, JCoPh, 53, 484\n\n. M J Berger, J Rigoutsos, IEEESMC. 21Berger, M. J., & Rigoutsos, J. 1991, IEEESMC, 21, 1278 BoxLib 2011, https://ccse.lbl.gov/BoxLib\n\n. G L Bryan, M L Norman, J M Stone, R Cen, J P Ostriker, CoPhC. 89149Bryan, G. L., Norman, M. L., Stone, J. M., Cen, R., & Ostriker, J. P. 1995, CoPhC, 89, 149\n\n. P Colella, JCoPh. 87171Colella, P. 1990, JCoPh, 87, 171\n\n. P Colella, H M Glaz, JCoPh. 59264Colella, P., & Glaz, H. M. 1985, JCoPh, 59, 264\n\n. P Colella, P R Woodward, JCoPh. 54174Colella, P., & Woodward, P. R. 1984, JCoPh, 54, 174\n\nLoad Balancing Irregular Algorithms. W Y Crutchfield, UCRL-JC-107679Tech. Rep.Crutchfield, W. Y. 1991, Load Balancing Irregular Algorithms, Tech. Rep. UCRL-JC-107679, LLNL\n\n. M Davis, G Efstathiou, C S Frenk, S D M White, 10.1086/163168ApJ. 292371Davis, M., Efstathiou, G., Frenk, C. S., & White, S. D. M. 1985, ApJ, 292, 371\n\n. C S Frenk, S D M White, P Bode, 10.1086/307908ApJ. 525554Frenk, C. S., White, S. D. M., Bode, P., et al. 1999, ApJ, 525, 554\n\n. B Fryxell, K Olson, P Ricker, 10.1086/317361ApJS. 131273Fryxell, B., Olson, K., Ricker, P., et al. 2000, ApJS, 131, 273\n\n. S Habib, A Pope, Z Luki\u0107, 10.1088/1742-6596/180/1/012019JPhCS. 18012019Habib, S., Pope, A., Luki\u0107, Z., et al. 2009, JPhCS, 180, 012019\n\n. K Heitmann, Z Luki\u0107, P Fasel, 10.1088/1749-4699/1/1/015003CS&D. 115003Heitmann, K., Luki\u0107, Z., Fasel, P., et al. 2008, CS&D, 1, 015003\n\n. K Heitmann, P Ricker, M Warren, S Habib, 10.1086/432646ApJS. 16028Heitmann, K., Ricker, P., Warren, M., & Habib, S. 2005, ApJS, 160, 28\n\nR Hockney, J Eastwood, Computer Simulations Using Particles. New YorkMcGraw HillHockney, R., & Eastwood, J. 1981, Computer Simulations Using Particles (New York: McGraw Hill)\n\n. A Klypin, J Holtzman, arXiv:astro-ph/9712217Klypin, A., & Holtzman, J. 1997, arXiv:astro-ph/9712217\n\n. A V Kravtsov, A Klypin, Y Hoffman, 10.1086/340046ApJ. 571563Kravtsov, A. V., Klypin, A., & Hoffman, Y. 2002, ApJ, 571, 563\n\n. A V Kravtsov, A A Klypin, A M Khokhlov, 10.1086/313015ApJS. 11173Kravtsov, A. V., Klypin, A. A., & Khokhlov, A. M. 1997, ApJS, 111, 73\n\n. Z Luki\u0107, K Heitmann, S Habib, S Bashinsky, P M Ricker, 10.1086/523083ApJ. 6711160Luki\u0107, Z., Heitmann, K., Habib, S., Bashinsky, S., & Ricker, P. M. 2007, ApJ, 671, 1160\n\n. G H Miller, P Colella, 10.1006/jcph.2002.7158JCoPh. 18326Miller, G. H., & Colella, P. 2002, JCoPh, 183, 26\n\n. F Miniati, P Colella, 10.1016/j.jcp.2007.07.035JCoPh. 227400Miniati, F., & Colella, P. 2007, JCoPh, 227, 400\n\n. N Mitchell, I Mccarthy, R Bower, T Theuns, R Crains, 10.1111/j.1365-2966.2009.14550.xMNRAS. 395180Mitchell, N., McCarthy, I., Bower, R., Theuns, T., & Crains, R. 2009, MNRAS, 395, 180\n\n. A Nonaka, A S Almgren, J B Bell, 10.1088/0067-0049/188/2/358ApJS. 188358Nonaka, A., Almgren, A. S., Bell, J. B., et al. 2010, ApJS, 188, 358\n\n. B W O&apos;shea, K Nagamine, V Springel, L Hernquist, M L Norman, 10.1086/432645ApJS. 1601O'Shea, B. W., Nagamine, K., Springel, V., Hernquist, L., & Norman, M. L. 2005, ApJS, 160, 1\n\nP J E Peebles, The Large-scale Structure of the Universe. Princeton, NJPrinceton Univ. PressPeebles, P. J. E. 1980, The Large-scale Structure of the Universe (Princeton, NJ: Princeton Univ. Press)\n\n. C A Rendleman, V E Beckner, M Lijewski, W Y Crutchfield, J B Bell, 10.1007/PL00013544Computing and Visualization in Science. 3147Rendleman, C. A., Beckner, V. E., Lijewski, M., Crutchfield, W. Y., & Bell, J. B. 2000, Computing and Visualization in Science, 3, 147\n\n. J Saltzman, JCoPh. 115153Saltzman, J. 1994, JCoPh, 115, 153\n\n. R K Sheth, G Tormen, 10.1046/j.1365-8711.1999.02692.xMNRAS. 308119Sheth, R. K., & Tormen, G. 1999, MNRAS, 308, 119\n\n. R E Smith, J A Peacock, A Jenkins, 10.1046/j.1365-8711.2003.06503.xMNRAS. 3411311Smith, R. E., Peacock, J. A., Jenkins, A., et al. 2003, MNRAS, 341, 1311\n\n. V Springel, 10.1111/j.1365-2966.2009.15715.xMNRAS. 401791Springel, V. 2010, MNRAS, 401, 791\n\n. R Stanek, E Rasia, A E Evrard, F Pearce, L Gazzola, 10.1088/0004-637X/715/2/1508ApJ. 7151508Stanek, R., Rasia, E., Evrard, A. E., Pearce, F., & Gazzola, L. 2010, ApJ, 715, 1508\n\n. R Teyssier, 10.1051/0004-6361:20011817A&A. 385337Teyssier, R. 2002, A&A, 385, 337\n\n. M Turk, B Smith, J Oishi, 10.1088/0067-0049/192/1/9ApJS. 1929Turk, M., Smith, B., Oishi, J., et al. 2011, ApJS, 192, 9\n\n. M Viel, J Lesgourgues, M G Haehnelt, S Matarrese, A Riotto, 10.1103/PhysRevD.71.063534PhRvD. 7163534Viel, M., Lesgourgues, J., Haehnelt, M. G., Matarrese, S., & Riotto, A. 2005, PhRvD, 71, 063534\n\nS D M Frenk, C S , 10.1086/170483VisIt User's Manual. 37952VisIt User's Manual. 2005, https://wci.llnl.gov/codes/visit/home.html White, S. D. M., & Frenk, C. S. 1991, ApJ, 379, 52\n\n. H Yoshida, 10.1007/BF00699717CeMDA. 5627Yoshida, H. 1993, CeMDA, 56, 27\n\n. Y B Zel&apos;dovich, A&A. 584Zel'dovich, Y. B. 1970, A&A, 5, 84\n\n. W Zhang, L Howell, A Almgren, A Burrows, J Bell, 10.1088/0067-0049/196/2/20ApJS. 19620Zhang, W., Howell, L., Almgren, A., Burrows, A., & Bell, J. 2011, ApJS, 196, 20\n\n. M Zingale, A S Almgren, J B Bell, A Nonaka, S E Woosley, 10.1088/0004-637X/704/1/196ApJ. 704196Zingale, M., Almgren, A. S., Bell, J. B., Nonaka, A., & Woosley, S. E. 2009, ApJ, 704, 196\n", "annotations": {"author": "[{\"end\":200,\"start\":78},{\"end\":321,\"start\":201},{\"end\":446,\"start\":322},{\"end\":549,\"start\":447},{\"end\":621,\"start\":550}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":84},{\"end\":212,\"start\":208},{\"end\":337,\"start\":329},{\"end\":459,\"start\":454},{\"end\":565,\"start\":560}]", "author_first_name": "[{\"end\":81,\"start\":78},{\"end\":83,\"start\":82},{\"end\":205,\"start\":201},{\"end\":207,\"start\":206},{\"end\":326,\"start\":322},{\"end\":328,\"start\":327},{\"end\":453,\"start\":447},{\"end\":555,\"start\":550},{\"end\":559,\"start\":556}]", "author_affiliation": "[{\"end\":199,\"start\":93},{\"end\":320,\"start\":214},{\"end\":445,\"start\":339},{\"end\":548,\"start\":461},{\"end\":620,\"start\":567}]", "title": "[{\"end\":63,\"start\":1},{\"end\":684,\"start\":622}]", "venue": "[{\"end\":711,\"start\":686}]", "abstract": "[{\"end\":1516,\"start\":889}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2328,\"start\":2315},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3000,\"start\":2988},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3082,\"start\":3065},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5290,\"start\":5273},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5414,\"start\":5396},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5547,\"start\":5535},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6478,\"start\":6459},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6506,\"start\":6484},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6535,\"start\":6514},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6563,\"start\":6548},{\"end\":8068,\"start\":8030},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8553,\"start\":8535},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8773,\"start\":8755},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8850,\"start\":8829},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8873,\"start\":8850},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8893,\"start\":8873},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8913,\"start\":8893},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8932,\"start\":8913},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11393,\"start\":11374},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15671,\"start\":15657},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15685,\"start\":15671},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15712,\"start\":15691},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15973,\"start\":15959},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15986,\"start\":15973},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16064,\"start\":16041},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16277,\"start\":16252},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16303,\"start\":16282},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17141,\"start\":17120},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17304,\"start\":17283},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18395,\"start\":18380},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18592,\"start\":18568},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19171,\"start\":19157},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20930,\"start\":20905},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25548,\"start\":25529},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26040,\"start\":26018},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26134,\"start\":26111},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26178,\"start\":26160},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28246,\"start\":28228},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28737,\"start\":28712},{\"end\":32141,\"start\":32102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35461,\"start\":35442},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37571,\"start\":37550},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38341,\"start\":38320},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40137,\"start\":40116},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":45218,\"start\":45200},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":45249,\"start\":45226},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48130,\"start\":48113},{\"end\":50558,\"start\":50557},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":52993,\"start\":52972},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":55141,\"start\":55120},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":55210,\"start\":55192},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":55421,\"start\":55399},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":55623,\"start\":55599},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":55673,\"start\":55656},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":55830,\"start\":55805},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":55850,\"start\":55830},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":56376,\"start\":56355},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":57429,\"start\":57412},{\"end\":57894,\"start\":57876},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":57918,\"start\":57896},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":57943,\"start\":57924},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":59180,\"start\":59161},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":59239,\"start\":59217},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":60892,\"start\":60870},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":61068,\"start\":61049},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":61520,\"start\":61501},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":61669,\"start\":61650},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":62150,\"start\":62128},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":62247,\"start\":62228},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":62273,\"start\":62251},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":62599,\"start\":62580},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":63134,\"start\":63112}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64936,\"start\":64862},{\"attributes\":{\"id\":\"fig_1\"},\"end\":65144,\"start\":64937},{\"attributes\":{\"id\":\"fig_2\"},\"end\":65538,\"start\":65145},{\"attributes\":{\"id\":\"fig_3\"},\"end\":65907,\"start\":65539},{\"attributes\":{\"id\":\"fig_4\"},\"end\":66197,\"start\":65908},{\"attributes\":{\"id\":\"fig_5\"},\"end\":66482,\"start\":66198},{\"attributes\":{\"id\":\"fig_6\"},\"end\":67387,\"start\":66483}]", "paragraph": "[{\"end\":2617,\"start\":1532},{\"end\":3473,\"start\":2619},{\"end\":4487,\"start\":3475},{\"end\":5548,\"start\":4489},{\"end\":6345,\"start\":5550},{\"end\":7268,\"start\":6347},{\"end\":8069,\"start\":7270},{\"end\":9434,\"start\":8071},{\"end\":10029,\"start\":9436},{\"end\":10615,\"start\":10070},{\"end\":10735,\"start\":10675},{\"end\":10980,\"start\":10752},{\"end\":11116,\"start\":11015},{\"end\":11310,\"start\":11165},{\"end\":11585,\"start\":11312},{\"end\":11824,\"start\":11762},{\"end\":12488,\"start\":11826},{\"end\":12933,\"start\":12627},{\"end\":13067,\"start\":12979},{\"end\":13658,\"start\":13069},{\"end\":13807,\"start\":13703},{\"end\":13991,\"start\":13824},{\"end\":14138,\"start\":14038},{\"end\":14931,\"start\":14177},{\"end\":15064,\"start\":14948},{\"end\":17505,\"start\":15106},{\"end\":18396,\"start\":17507},{\"end\":18825,\"start\":18412},{\"end\":19015,\"start\":18917},{\"end\":19326,\"start\":19065},{\"end\":19788,\"start\":19343},{\"end\":20547,\"start\":19790},{\"end\":20931,\"start\":20549},{\"end\":21248,\"start\":20959},{\"end\":21838,\"start\":21311},{\"end\":22240,\"start\":21897},{\"end\":22385,\"start\":22242},{\"end\":22564,\"start\":22425},{\"end\":22800,\"start\":22603},{\"end\":23078,\"start\":22802},{\"end\":23104,\"start\":23080},{\"end\":23251,\"start\":23106},{\"end\":23564,\"start\":23325},{\"end\":23748,\"start\":23636},{\"end\":24108,\"start\":23813},{\"end\":24249,\"start\":24110},{\"end\":24598,\"start\":24345},{\"end\":24709,\"start\":24600},{\"end\":24865,\"start\":24769},{\"end\":25031,\"start\":24867},{\"end\":25238,\"start\":25107},{\"end\":25343,\"start\":25291},{\"end\":25625,\"start\":25427},{\"end\":25663,\"start\":25627},{\"end\":25971,\"start\":25770},{\"end\":26498,\"start\":25979},{\"end\":28146,\"start\":26517},{\"end\":28950,\"start\":28148},{\"end\":29631,\"start\":28952},{\"end\":30467,\"start\":29633},{\"end\":32142,\"start\":30490},{\"end\":32604,\"start\":32144},{\"end\":33302,\"start\":32629},{\"end\":33599,\"start\":33304},{\"end\":34337,\"start\":33656},{\"end\":34351,\"start\":34339},{\"end\":34609,\"start\":34353},{\"end\":35077,\"start\":34611},{\"end\":35118,\"start\":35079},{\"end\":35543,\"start\":35120},{\"end\":36087,\"start\":35545},{\"end\":36429,\"start\":36089},{\"end\":36645,\"start\":36431},{\"end\":36757,\"start\":36647},{\"end\":36815,\"start\":36759},{\"end\":37018,\"start\":36817},{\"end\":37422,\"start\":37020},{\"end\":37882,\"start\":37424},{\"end\":38012,\"start\":37901},{\"end\":38456,\"start\":38053},{\"end\":39489,\"start\":38486},{\"end\":39667,\"start\":39491},{\"end\":40629,\"start\":39763},{\"end\":41586,\"start\":40654},{\"end\":42173,\"start\":41588},{\"end\":42811,\"start\":42175},{\"end\":43896,\"start\":42813},{\"end\":44666,\"start\":43952},{\"end\":45323,\"start\":44668},{\"end\":45881,\"start\":45365},{\"end\":47133,\"start\":45883},{\"end\":47473,\"start\":47135},{\"end\":48561,\"start\":47508},{\"end\":49429,\"start\":48563},{\"end\":50049,\"start\":49431},{\"end\":50283,\"start\":50051},{\"end\":51324,\"start\":50308},{\"end\":52098,\"start\":51326},{\"end\":52723,\"start\":52100},{\"end\":55212,\"start\":52738},{\"end\":55958,\"start\":55245},{\"end\":57157,\"start\":55960},{\"end\":59240,\"start\":57159},{\"end\":60092,\"start\":59242},{\"end\":61310,\"start\":60130},{\"end\":61805,\"start\":61312},{\"end\":62372,\"start\":61807},{\"end\":63641,\"start\":62374},{\"end\":64861,\"start\":63673}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10674,\"start\":10616},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11014,\"start\":10981},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11164,\"start\":11117},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11669,\"start\":11586},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11761,\"start\":11669},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12612,\"start\":12489},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12978,\"start\":12934},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13702,\"start\":13659},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14037,\"start\":13992},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15105,\"start\":15065},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18916,\"start\":18826},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19064,\"start\":19016},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21310,\"start\":21249},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21896,\"start\":21839},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22424,\"start\":22386},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23324,\"start\":23252},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23600,\"start\":23565},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23635,\"start\":23600},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23812,\"start\":23749},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24344,\"start\":24250},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24768,\"start\":24710},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25106,\"start\":25032},{\"attributes\":{\"id\":\"formula_22\"},\"end\":25290,\"start\":25239},{\"attributes\":{\"id\":\"formula_23\"},\"end\":25385,\"start\":25344},{\"attributes\":{\"id\":\"formula_24\"},\"end\":25426,\"start\":25385},{\"attributes\":{\"id\":\"formula_25\"},\"end\":25769,\"start\":25664},{\"attributes\":{\"id\":\"formula_26\"},\"end\":33638,\"start\":33600},{\"attributes\":{\"id\":\"formula_27\"},\"end\":33655,\"start\":33638},{\"attributes\":{\"id\":\"formula_28\"},\"end\":38052,\"start\":38013},{\"attributes\":{\"id\":\"formula_29\"},\"end\":38485,\"start\":38457},{\"attributes\":{\"id\":\"formula_30\"},\"end\":39762,\"start\":39668}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1530,\"start\":1518},{\"attributes\":{\"n\":\"2.\"},\"end\":10047,\"start\":10032},{\"attributes\":{\"n\":\"2.1.\"},\"end\":10068,\"start\":10050},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10750,\"start\":10738},{\"attributes\":{\"n\":\"2.3.\"},\"end\":12625,\"start\":12614},{\"attributes\":{\"n\":\"2.4.\"},\"end\":13822,\"start\":13810},{\"attributes\":{\"n\":\"3.\"},\"end\":14154,\"start\":14141},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14175,\"start\":14157},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14946,\"start\":14934},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18410,\"start\":18399},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19341,\"start\":19329},{\"attributes\":{\"n\":\"3.5.\"},\"end\":20957,\"start\":20934},{\"attributes\":{\"n\":\"3.6.\"},\"end\":22601,\"start\":22567},{\"attributes\":{\"n\":\"4.\"},\"end\":25977,\"start\":25974},{\"attributes\":{\"n\":\"4.1.\"},\"end\":26515,\"start\":26501},{\"attributes\":{\"n\":\"4.2.\"},\"end\":30488,\"start\":30470},{\"attributes\":{\"n\":\"4.3.\"},\"end\":32627,\"start\":32607},{\"attributes\":{\"n\":\"4.4.\"},\"end\":37899,\"start\":37885},{\"attributes\":{\"n\":\"4.5.\"},\"end\":40652,\"start\":40632},{\"attributes\":{\"n\":\"5.\"},\"end\":43939,\"start\":43899},{\"attributes\":{\"n\":\"5.1.\"},\"end\":43950,\"start\":43942},{\"attributes\":{\"n\":\"5.2.\"},\"end\":45363,\"start\":45326},{\"attributes\":{\"n\":\"5.3.\"},\"end\":47506,\"start\":47476},{\"attributes\":{\"n\":\"5.4.\"},\"end\":50306,\"start\":50286},{\"attributes\":{\"n\":\"6.\"},\"end\":52736,\"start\":52726},{\"attributes\":{\"n\":\"6.1.\"},\"end\":55243,\"start\":55215},{\"attributes\":{\"n\":\"6.2.\"},\"end\":60128,\"start\":60095},{\"attributes\":{\"n\":\"7.\"},\"end\":63671,\"start\":63644},{\"end\":64948,\"start\":64938},{\"end\":65156,\"start\":65146},{\"end\":65548,\"start\":65540},{\"end\":65919,\"start\":65909},{\"end\":66209,\"start\":66199}]", "table": null, "figure_caption": "[{\"end\":64936,\"start\":64864},{\"end\":65144,\"start\":64950},{\"end\":65538,\"start\":65158},{\"end\":65907,\"start\":65550},{\"end\":66197,\"start\":65921},{\"end\":66482,\"start\":66211},{\"end\":67387,\"start\":66485}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":50319,\"start\":50311},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51349,\"start\":51341},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":56730,\"start\":56722},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":57740,\"start\":57732},{\"end\":58216,\"start\":58208},{\"end\":58702,\"start\":58694},{\"end\":59253,\"start\":59245},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":59637,\"start\":59629},{\"end\":59660,\"start\":59652},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":60804,\"start\":60796},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":61818,\"start\":61810}]", "bib_author_first_name": "[{\"end\":67605,\"start\":67604},{\"end\":67692,\"start\":67691},{\"end\":67694,\"start\":67693},{\"end\":67705,\"start\":67704},{\"end\":67707,\"start\":67706},{\"end\":67718,\"start\":67717},{\"end\":67720,\"start\":67719},{\"end\":67844,\"start\":67843},{\"end\":67846,\"start\":67845},{\"end\":67857,\"start\":67856},{\"end\":67859,\"start\":67858},{\"end\":67867,\"start\":67866},{\"end\":67878,\"start\":67877},{\"end\":67880,\"start\":67879},{\"end\":67890,\"start\":67889},{\"end\":67892,\"start\":67891},{\"end\":68011,\"start\":68010},{\"end\":68013,\"start\":68012},{\"end\":68024,\"start\":68023},{\"end\":68026,\"start\":68025},{\"end\":68034,\"start\":68033},{\"end\":68044,\"start\":68043},{\"end\":68158,\"start\":68157},{\"end\":68160,\"start\":68159},{\"end\":68171,\"start\":68170},{\"end\":68173,\"start\":68172},{\"end\":68181,\"start\":68180},{\"end\":68183,\"start\":68182},{\"end\":68196,\"start\":68195},{\"end\":68317,\"start\":68316},{\"end\":68319,\"start\":68318},{\"end\":68330,\"start\":68329},{\"end\":68332,\"start\":68331},{\"end\":68340,\"start\":68339},{\"end\":68342,\"start\":68341},{\"end\":68355,\"start\":68354},{\"end\":68476,\"start\":68475},{\"end\":68484,\"start\":68483},{\"end\":68494,\"start\":68493},{\"end\":68506,\"start\":68505},{\"end\":68665,\"start\":68664},{\"end\":68667,\"start\":68666},{\"end\":68752,\"start\":68751},{\"end\":68754,\"start\":68753},{\"end\":68764,\"start\":68763},{\"end\":68838,\"start\":68837},{\"end\":68840,\"start\":68839},{\"end\":68850,\"start\":68849},{\"end\":68924,\"start\":68923},{\"end\":68926,\"start\":68925},{\"end\":68936,\"start\":68935},{\"end\":69059,\"start\":69058},{\"end\":69061,\"start\":69060},{\"end\":69070,\"start\":69069},{\"end\":69072,\"start\":69071},{\"end\":69082,\"start\":69081},{\"end\":69084,\"start\":69083},{\"end\":69093,\"start\":69092},{\"end\":69100,\"start\":69099},{\"end\":69102,\"start\":69101},{\"end\":69220,\"start\":69219},{\"end\":69279,\"start\":69278},{\"end\":69290,\"start\":69289},{\"end\":69292,\"start\":69291},{\"end\":69363,\"start\":69362},{\"end\":69374,\"start\":69373},{\"end\":69376,\"start\":69375},{\"end\":69490,\"start\":69489},{\"end\":69492,\"start\":69491},{\"end\":69628,\"start\":69627},{\"end\":69637,\"start\":69636},{\"end\":69651,\"start\":69650},{\"end\":69653,\"start\":69652},{\"end\":69662,\"start\":69661},{\"end\":69666,\"start\":69663},{\"end\":69782,\"start\":69781},{\"end\":69784,\"start\":69783},{\"end\":69793,\"start\":69792},{\"end\":69797,\"start\":69794},{\"end\":69806,\"start\":69805},{\"end\":69910,\"start\":69909},{\"end\":69921,\"start\":69920},{\"end\":69930,\"start\":69929},{\"end\":70033,\"start\":70032},{\"end\":70042,\"start\":70041},{\"end\":70050,\"start\":70049},{\"end\":70171,\"start\":70170},{\"end\":70183,\"start\":70182},{\"end\":70192,\"start\":70191},{\"end\":70309,\"start\":70308},{\"end\":70321,\"start\":70320},{\"end\":70331,\"start\":70330},{\"end\":70341,\"start\":70340},{\"end\":70446,\"start\":70445},{\"end\":70457,\"start\":70456},{\"end\":70624,\"start\":70623},{\"end\":70634,\"start\":70633},{\"end\":70727,\"start\":70726},{\"end\":70729,\"start\":70728},{\"end\":70741,\"start\":70740},{\"end\":70751,\"start\":70750},{\"end\":70853,\"start\":70852},{\"end\":70855,\"start\":70854},{\"end\":70867,\"start\":70866},{\"end\":70869,\"start\":70868},{\"end\":70879,\"start\":70878},{\"end\":70881,\"start\":70880},{\"end\":70991,\"start\":70990},{\"end\":71000,\"start\":70999},{\"end\":71012,\"start\":71011},{\"end\":71021,\"start\":71020},{\"end\":71034,\"start\":71033},{\"end\":71036,\"start\":71035},{\"end\":71163,\"start\":71162},{\"end\":71165,\"start\":71164},{\"end\":71175,\"start\":71174},{\"end\":71273,\"start\":71272},{\"end\":71284,\"start\":71283},{\"end\":71385,\"start\":71384},{\"end\":71397,\"start\":71396},{\"end\":71409,\"start\":71408},{\"end\":71418,\"start\":71417},{\"end\":71428,\"start\":71427},{\"end\":71572,\"start\":71571},{\"end\":71582,\"start\":71581},{\"end\":71584,\"start\":71583},{\"end\":71595,\"start\":71594},{\"end\":71597,\"start\":71596},{\"end\":71716,\"start\":71715},{\"end\":71718,\"start\":71717},{\"end\":71733,\"start\":71732},{\"end\":71745,\"start\":71744},{\"end\":71757,\"start\":71756},{\"end\":71770,\"start\":71769},{\"end\":71772,\"start\":71771},{\"end\":71900,\"start\":71899},{\"end\":71904,\"start\":71901},{\"end\":72100,\"start\":72099},{\"end\":72102,\"start\":72101},{\"end\":72115,\"start\":72114},{\"end\":72117,\"start\":72116},{\"end\":72128,\"start\":72127},{\"end\":72140,\"start\":72139},{\"end\":72142,\"start\":72141},{\"end\":72157,\"start\":72156},{\"end\":72159,\"start\":72158},{\"end\":72367,\"start\":72366},{\"end\":72430,\"start\":72429},{\"end\":72432,\"start\":72431},{\"end\":72441,\"start\":72440},{\"end\":72548,\"start\":72547},{\"end\":72550,\"start\":72549},{\"end\":72559,\"start\":72558},{\"end\":72561,\"start\":72560},{\"end\":72572,\"start\":72571},{\"end\":72705,\"start\":72704},{\"end\":72800,\"start\":72799},{\"end\":72810,\"start\":72809},{\"end\":72819,\"start\":72818},{\"end\":72821,\"start\":72820},{\"end\":72831,\"start\":72830},{\"end\":72841,\"start\":72840},{\"end\":72980,\"start\":72979},{\"end\":73065,\"start\":73064},{\"end\":73073,\"start\":73072},{\"end\":73082,\"start\":73081},{\"end\":73187,\"start\":73186},{\"end\":73195,\"start\":73194},{\"end\":73210,\"start\":73209},{\"end\":73212,\"start\":73211},{\"end\":73224,\"start\":73223},{\"end\":73237,\"start\":73236},{\"end\":73384,\"start\":73383},{\"end\":73388,\"start\":73385},{\"end\":73397,\"start\":73396},{\"end\":73399,\"start\":73398},{\"end\":73567,\"start\":73566},{\"end\":73642,\"start\":73641},{\"end\":73644,\"start\":73643},{\"end\":73709,\"start\":73708},{\"end\":73718,\"start\":73717},{\"end\":73728,\"start\":73727},{\"end\":73739,\"start\":73738},{\"end\":73750,\"start\":73749},{\"end\":73878,\"start\":73877},{\"end\":73889,\"start\":73888},{\"end\":73891,\"start\":73890},{\"end\":73902,\"start\":73901},{\"end\":73904,\"start\":73903},{\"end\":73912,\"start\":73911},{\"end\":73922,\"start\":73921},{\"end\":73924,\"start\":73923}]", "bib_author_last_name": "[{\"end\":67610,\"start\":67606},{\"end\":67702,\"start\":67695},{\"end\":67715,\"start\":67708},{\"end\":67725,\"start\":67721},{\"end\":67854,\"start\":67847},{\"end\":67864,\"start\":67860},{\"end\":67875,\"start\":67868},{\"end\":67887,\"start\":67881},{\"end\":67900,\"start\":67893},{\"end\":68021,\"start\":68014},{\"end\":68031,\"start\":68027},{\"end\":68041,\"start\":68035},{\"end\":68052,\"start\":68045},{\"end\":68168,\"start\":68161},{\"end\":68178,\"start\":68174},{\"end\":68193,\"start\":68184},{\"end\":68204,\"start\":68197},{\"end\":68327,\"start\":68320},{\"end\":68337,\"start\":68333},{\"end\":68352,\"start\":68343},{\"end\":68363,\"start\":68356},{\"end\":68481,\"start\":68477},{\"end\":68491,\"start\":68485},{\"end\":68503,\"start\":68495},{\"end\":68514,\"start\":68507},{\"end\":68674,\"start\":68668},{\"end\":68761,\"start\":68755},{\"end\":68772,\"start\":68765},{\"end\":68847,\"start\":68841},{\"end\":68857,\"start\":68851},{\"end\":68933,\"start\":68927},{\"end\":68946,\"start\":68937},{\"end\":69067,\"start\":69062},{\"end\":69079,\"start\":69073},{\"end\":69090,\"start\":69085},{\"end\":69097,\"start\":69094},{\"end\":69111,\"start\":69103},{\"end\":69228,\"start\":69221},{\"end\":69287,\"start\":69280},{\"end\":69297,\"start\":69293},{\"end\":69371,\"start\":69364},{\"end\":69385,\"start\":69377},{\"end\":69504,\"start\":69493},{\"end\":69634,\"start\":69629},{\"end\":69648,\"start\":69638},{\"end\":69659,\"start\":69654},{\"end\":69672,\"start\":69667},{\"end\":69790,\"start\":69785},{\"end\":69803,\"start\":69798},{\"end\":69811,\"start\":69807},{\"end\":69918,\"start\":69911},{\"end\":69927,\"start\":69922},{\"end\":69937,\"start\":69931},{\"end\":70039,\"start\":70034},{\"end\":70047,\"start\":70043},{\"end\":70056,\"start\":70051},{\"end\":70180,\"start\":70172},{\"end\":70189,\"start\":70184},{\"end\":70198,\"start\":70193},{\"end\":70318,\"start\":70310},{\"end\":70328,\"start\":70322},{\"end\":70338,\"start\":70332},{\"end\":70347,\"start\":70342},{\"end\":70454,\"start\":70447},{\"end\":70466,\"start\":70458},{\"end\":70631,\"start\":70625},{\"end\":70643,\"start\":70635},{\"end\":70738,\"start\":70730},{\"end\":70748,\"start\":70742},{\"end\":70759,\"start\":70752},{\"end\":70864,\"start\":70856},{\"end\":70876,\"start\":70870},{\"end\":70890,\"start\":70882},{\"end\":70997,\"start\":70992},{\"end\":71009,\"start\":71001},{\"end\":71018,\"start\":71013},{\"end\":71031,\"start\":71022},{\"end\":71043,\"start\":71037},{\"end\":71172,\"start\":71166},{\"end\":71183,\"start\":71176},{\"end\":71281,\"start\":71274},{\"end\":71292,\"start\":71285},{\"end\":71394,\"start\":71386},{\"end\":71406,\"start\":71398},{\"end\":71415,\"start\":71410},{\"end\":71425,\"start\":71419},{\"end\":71435,\"start\":71429},{\"end\":71579,\"start\":71573},{\"end\":71592,\"start\":71585},{\"end\":71602,\"start\":71598},{\"end\":71730,\"start\":71719},{\"end\":71742,\"start\":71734},{\"end\":71754,\"start\":71746},{\"end\":71767,\"start\":71758},{\"end\":71779,\"start\":71773},{\"end\":71912,\"start\":71905},{\"end\":72112,\"start\":72103},{\"end\":72125,\"start\":72118},{\"end\":72137,\"start\":72129},{\"end\":72154,\"start\":72143},{\"end\":72164,\"start\":72160},{\"end\":72376,\"start\":72368},{\"end\":72438,\"start\":72433},{\"end\":72448,\"start\":72442},{\"end\":72556,\"start\":72551},{\"end\":72569,\"start\":72562},{\"end\":72580,\"start\":72573},{\"end\":72714,\"start\":72706},{\"end\":72807,\"start\":72801},{\"end\":72816,\"start\":72811},{\"end\":72828,\"start\":72822},{\"end\":72838,\"start\":72832},{\"end\":72849,\"start\":72842},{\"end\":72989,\"start\":72981},{\"end\":73070,\"start\":73066},{\"end\":73079,\"start\":73074},{\"end\":73088,\"start\":73083},{\"end\":73192,\"start\":73188},{\"end\":73207,\"start\":73196},{\"end\":73221,\"start\":73213},{\"end\":73234,\"start\":73225},{\"end\":73244,\"start\":73238},{\"end\":73394,\"start\":73389},{\"end\":73575,\"start\":73568},{\"end\":73660,\"start\":73645},{\"end\":73715,\"start\":73710},{\"end\":73725,\"start\":73719},{\"end\":73736,\"start\":73729},{\"end\":73747,\"start\":73740},{\"end\":73755,\"start\":73751},{\"end\":73886,\"start\":73879},{\"end\":73899,\"start\":73892},{\"end\":73909,\"start\":73905},{\"end\":73919,\"start\":73913},{\"end\":73932,\"start\":73925}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":67687,\"start\":67602},{\"attributes\":{\"id\":\"b1\"},\"end\":67839,\"start\":67689},{\"attributes\":{\"id\":\"b2\"},\"end\":68006,\"start\":67841},{\"attributes\":{\"id\":\"b3\"},\"end\":68153,\"start\":68008},{\"attributes\":{\"id\":\"b4\"},\"end\":68312,\"start\":68155},{\"attributes\":{\"id\":\"b5\"},\"end\":68471,\"start\":68314},{\"attributes\":{\"id\":\"b6\"},\"end\":68660,\"start\":68473},{\"attributes\":{\"id\":\"b7\"},\"end\":68747,\"start\":68662},{\"attributes\":{\"id\":\"b8\"},\"end\":68833,\"start\":68749},{\"attributes\":{\"id\":\"b9\"},\"end\":68919,\"start\":68835},{\"attributes\":{\"id\":\"b10\"},\"end\":69054,\"start\":68921},{\"attributes\":{\"id\":\"b11\"},\"end\":69215,\"start\":69056},{\"attributes\":{\"id\":\"b12\"},\"end\":69274,\"start\":69217},{\"attributes\":{\"id\":\"b13\"},\"end\":69358,\"start\":69276},{\"attributes\":{\"id\":\"b14\"},\"end\":69450,\"start\":69360},{\"attributes\":{\"id\":\"b15\"},\"end\":69623,\"start\":69452},{\"attributes\":{\"id\":\"b16\"},\"end\":69777,\"start\":69625},{\"attributes\":{\"id\":\"b17\"},\"end\":69905,\"start\":69779},{\"attributes\":{\"id\":\"b18\"},\"end\":70028,\"start\":69907},{\"attributes\":{\"id\":\"b19\"},\"end\":70166,\"start\":70030},{\"attributes\":{\"id\":\"b20\"},\"end\":70304,\"start\":70168},{\"attributes\":{\"id\":\"b21\"},\"end\":70443,\"start\":70306},{\"attributes\":{\"id\":\"b22\"},\"end\":70619,\"start\":70445},{\"attributes\":{\"id\":\"b23\"},\"end\":70722,\"start\":70621},{\"attributes\":{\"id\":\"b24\"},\"end\":70848,\"start\":70724},{\"attributes\":{\"id\":\"b25\"},\"end\":70986,\"start\":70850},{\"attributes\":{\"id\":\"b26\"},\"end\":71158,\"start\":70988},{\"attributes\":{\"id\":\"b27\"},\"end\":71268,\"start\":71160},{\"attributes\":{\"id\":\"b28\"},\"end\":71380,\"start\":71270},{\"attributes\":{\"id\":\"b29\"},\"end\":71567,\"start\":71382},{\"attributes\":{\"id\":\"b30\"},\"end\":71711,\"start\":71569},{\"attributes\":{\"id\":\"b31\"},\"end\":71897,\"start\":71713},{\"attributes\":{\"id\":\"b32\"},\"end\":72095,\"start\":71899},{\"attributes\":{\"id\":\"b33\"},\"end\":72362,\"start\":72097},{\"attributes\":{\"id\":\"b34\"},\"end\":72425,\"start\":72364},{\"attributes\":{\"id\":\"b35\"},\"end\":72543,\"start\":72427},{\"attributes\":{\"id\":\"b36\"},\"end\":72700,\"start\":72545},{\"attributes\":{\"id\":\"b37\"},\"end\":72795,\"start\":72702},{\"attributes\":{\"id\":\"b38\"},\"end\":72975,\"start\":72797},{\"attributes\":{\"id\":\"b39\"},\"end\":73060,\"start\":72977},{\"attributes\":{\"id\":\"b40\"},\"end\":73182,\"start\":73062},{\"attributes\":{\"id\":\"b41\"},\"end\":73381,\"start\":73184},{\"attributes\":{\"id\":\"b42\"},\"end\":73562,\"start\":73383},{\"attributes\":{\"id\":\"b43\"},\"end\":73637,\"start\":73564},{\"attributes\":{\"id\":\"b44\"},\"end\":73704,\"start\":73639},{\"attributes\":{\"id\":\"b45\"},\"end\":73873,\"start\":73706},{\"attributes\":{\"id\":\"b46\"},\"end\":74062,\"start\":73875}]", "bib_title": null, "bib_author": "[{\"end\":67612,\"start\":67604},{\"end\":67704,\"start\":67691},{\"end\":67717,\"start\":67704},{\"end\":67727,\"start\":67717},{\"end\":67856,\"start\":67843},{\"end\":67866,\"start\":67856},{\"end\":67877,\"start\":67866},{\"end\":67889,\"start\":67877},{\"end\":67902,\"start\":67889},{\"end\":68023,\"start\":68010},{\"end\":68033,\"start\":68023},{\"end\":68043,\"start\":68033},{\"end\":68054,\"start\":68043},{\"end\":68170,\"start\":68157},{\"end\":68180,\"start\":68170},{\"end\":68195,\"start\":68180},{\"end\":68206,\"start\":68195},{\"end\":68329,\"start\":68316},{\"end\":68339,\"start\":68329},{\"end\":68354,\"start\":68339},{\"end\":68365,\"start\":68354},{\"end\":68483,\"start\":68475},{\"end\":68493,\"start\":68483},{\"end\":68505,\"start\":68493},{\"end\":68516,\"start\":68505},{\"end\":68676,\"start\":68664},{\"end\":68763,\"start\":68751},{\"end\":68774,\"start\":68763},{\"end\":68849,\"start\":68837},{\"end\":68859,\"start\":68849},{\"end\":68935,\"start\":68923},{\"end\":68948,\"start\":68935},{\"end\":69069,\"start\":69058},{\"end\":69081,\"start\":69069},{\"end\":69092,\"start\":69081},{\"end\":69099,\"start\":69092},{\"end\":69113,\"start\":69099},{\"end\":69230,\"start\":69219},{\"end\":69289,\"start\":69278},{\"end\":69299,\"start\":69289},{\"end\":69373,\"start\":69362},{\"end\":69387,\"start\":69373},{\"end\":69506,\"start\":69489},{\"end\":69636,\"start\":69627},{\"end\":69650,\"start\":69636},{\"end\":69661,\"start\":69650},{\"end\":69674,\"start\":69661},{\"end\":69792,\"start\":69781},{\"end\":69805,\"start\":69792},{\"end\":69813,\"start\":69805},{\"end\":69920,\"start\":69909},{\"end\":69929,\"start\":69920},{\"end\":69939,\"start\":69929},{\"end\":70041,\"start\":70032},{\"end\":70049,\"start\":70041},{\"end\":70058,\"start\":70049},{\"end\":70182,\"start\":70170},{\"end\":70191,\"start\":70182},{\"end\":70200,\"start\":70191},{\"end\":70320,\"start\":70308},{\"end\":70330,\"start\":70320},{\"end\":70340,\"start\":70330},{\"end\":70349,\"start\":70340},{\"end\":70456,\"start\":70445},{\"end\":70468,\"start\":70456},{\"end\":70633,\"start\":70623},{\"end\":70645,\"start\":70633},{\"end\":70740,\"start\":70726},{\"end\":70750,\"start\":70740},{\"end\":70761,\"start\":70750},{\"end\":70866,\"start\":70852},{\"end\":70878,\"start\":70866},{\"end\":70892,\"start\":70878},{\"end\":70999,\"start\":70990},{\"end\":71011,\"start\":70999},{\"end\":71020,\"start\":71011},{\"end\":71033,\"start\":71020},{\"end\":71045,\"start\":71033},{\"end\":71174,\"start\":71162},{\"end\":71185,\"start\":71174},{\"end\":71283,\"start\":71272},{\"end\":71294,\"start\":71283},{\"end\":71396,\"start\":71384},{\"end\":71408,\"start\":71396},{\"end\":71417,\"start\":71408},{\"end\":71427,\"start\":71417},{\"end\":71437,\"start\":71427},{\"end\":71581,\"start\":71571},{\"end\":71594,\"start\":71581},{\"end\":71604,\"start\":71594},{\"end\":71732,\"start\":71715},{\"end\":71744,\"start\":71732},{\"end\":71756,\"start\":71744},{\"end\":71769,\"start\":71756},{\"end\":71781,\"start\":71769},{\"end\":71914,\"start\":71899},{\"end\":72114,\"start\":72099},{\"end\":72127,\"start\":72114},{\"end\":72139,\"start\":72127},{\"end\":72156,\"start\":72139},{\"end\":72166,\"start\":72156},{\"end\":72378,\"start\":72366},{\"end\":72440,\"start\":72429},{\"end\":72450,\"start\":72440},{\"end\":72558,\"start\":72547},{\"end\":72571,\"start\":72558},{\"end\":72582,\"start\":72571},{\"end\":72716,\"start\":72704},{\"end\":72809,\"start\":72799},{\"end\":72818,\"start\":72809},{\"end\":72830,\"start\":72818},{\"end\":72840,\"start\":72830},{\"end\":72851,\"start\":72840},{\"end\":72991,\"start\":72979},{\"end\":73072,\"start\":73064},{\"end\":73081,\"start\":73072},{\"end\":73090,\"start\":73081},{\"end\":73194,\"start\":73186},{\"end\":73209,\"start\":73194},{\"end\":73223,\"start\":73209},{\"end\":73236,\"start\":73223},{\"end\":73246,\"start\":73236},{\"end\":73396,\"start\":73383},{\"end\":73402,\"start\":73396},{\"end\":73577,\"start\":73566},{\"end\":73662,\"start\":73641},{\"end\":73717,\"start\":73708},{\"end\":73727,\"start\":73717},{\"end\":73738,\"start\":73727},{\"end\":73749,\"start\":73738},{\"end\":73757,\"start\":73749},{\"end\":73888,\"start\":73877},{\"end\":73901,\"start\":73888},{\"end\":73911,\"start\":73901},{\"end\":73921,\"start\":73911},{\"end\":73934,\"start\":73921}]", "bib_venue": "[{\"end\":70514,\"start\":70506},{\"end\":71970,\"start\":71957},{\"end\":67649,\"start\":67644},{\"end\":67758,\"start\":67755},{\"end\":67907,\"start\":67902},{\"end\":68071,\"start\":68068},{\"end\":68223,\"start\":68220},{\"end\":68382,\"start\":68379},{\"end\":68559,\"start\":68531},{\"end\":68708,\"start\":68705},{\"end\":68779,\"start\":68774},{\"end\":68864,\"start\":68859},{\"end\":68955,\"start\":68948},{\"end\":69118,\"start\":69113},{\"end\":69235,\"start\":69230},{\"end\":69304,\"start\":69299},{\"end\":69392,\"start\":69387},{\"end\":69487,\"start\":69452},{\"end\":69691,\"start\":69688},{\"end\":69830,\"start\":69827},{\"end\":69957,\"start\":69953},{\"end\":70093,\"start\":70088},{\"end\":70232,\"start\":70228},{\"end\":70367,\"start\":70363},{\"end\":70504,\"start\":70468},{\"end\":70778,\"start\":70775},{\"end\":70910,\"start\":70906},{\"end\":71062,\"start\":71059},{\"end\":71212,\"start\":71207},{\"end\":71324,\"start\":71319},{\"end\":71474,\"start\":71469},{\"end\":71635,\"start\":71631},{\"end\":71799,\"start\":71795},{\"end\":71955,\"start\":71914},{\"end\":72222,\"start\":72184},{\"end\":72383,\"start\":72378},{\"end\":72487,\"start\":72482},{\"end\":72619,\"start\":72614},{\"end\":72753,\"start\":72748},{\"end\":72882,\"start\":72879},{\"end\":73020,\"start\":73017},{\"end\":73119,\"start\":73115},{\"end\":73277,\"start\":73272},{\"end\":73435,\"start\":73416},{\"end\":73600,\"start\":73595},{\"end\":73665,\"start\":73662},{\"end\":73787,\"start\":73783},{\"end\":73964,\"start\":73961}]"}}}, "year": 2023, "month": 12, "day": 17}