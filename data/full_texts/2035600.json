{"id": 2035600, "updated": "2023-09-28 14:36:49.584", "metadata": {"title": "Random Erasing Data Augmentation", "authors": "[{\"first\":\"Zhun\",\"last\":\"Zhong\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Guoliang\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Shaozi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 8, "day": 16}, "abstract": "In this paper, we introduce Random Erasing, a simple yet effective data augmentation techniques for training the convolutional neural network (CNN). In training phase, Random Erasing randomly selects a rectangle region in an image, and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduce the risk of network overfitting and make the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated into most of the CNN-based recognition models. Albeit simple, Random Erasing yields consistent improvement in image classification, object detection and person re-identification (re-ID). For image classification, our method improves WRN-28-10: top-1 error rate from 3.72% to 3.08% on CIFAR10, and from 18.68% to 17.65% on CIFAR100. For object detection on PASCAL VOC 2007, Random Erasing improves Fast-RCNN from 74.8% to 76.2% in mAP. For person re-ID, when using Random Erasing in recent deep models, we achieve the state-of-the-art accuracy: the rank-1 accuracy is 89.13% for Market-1501, 84.02% for DukeMTMC-reID, and 63.93% for CUHK03 under the new evaluation protocol.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1708.04896", "mag": "2998508940", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/Zhong0KL020", "doi": "10.1609/aaai.v34i07.7000"}}, "content": {"source": {"pdf_hash": "3dcee987f8651dbb1e090f26bcb7d07f35dcdc7a", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/7000/6854\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/7000/6854", "status": "GOLD"}}, "grobid": {"id": "877fef4c0282775241204558e29b5de3bb02077d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3dcee987f8651dbb1e090f26bcb7d07f35dcdc7a.txt", "contents": "\nRandom Erasing Data Augmentation\n\n\nZhun Zhong \nDepartment of Artificial Intelligence\nXiamen University\n\n\nSydney 3 Research School of Computer Science\nReLER\nUniversity of Technology\nAustralian National University\n\n\nLiang Zheng \nGuoliang Kang \nSchool of Computer Science\nCarnegie Mellon University\n\n\nShaozi Li \nDepartment of Artificial Intelligence\nXiamen University\n\n\nYi Yang \nSydney 3 Research School of Computer Science\nReLER\nUniversity of Technology\nAustralian National University\n\n\nRandom Erasing Data Augmentation\n\nIn this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification.\n\nIntroduction\n\nThe ability to generalize is a research focus for the convolutional neural network (CNN). When a model is excessively complex, such as having too many parameters compared to the number of training samples, over-fitting might happen and weaken its generalization ability. A learned model may describe random error or noise instead of the underlying data distribution . In bad cases, the CNN model may exhibit good performance on the training data, but fail drastically when predicting new data. To improve the generalization ability of CNNs, many data augmentation and regularization approaches have been proposed, such as random cropping (Krizhevsky, Sutskever, and Hinton 2012), flipping (Simonyan and Zisserman 2015), dropout (Srivastava et al. 2014), and batch normalization (Ioffe and Szegedy 2015).\n\nOcclusion is a critical influencing factor on the generalization ability of CNNs. It is desirable that invariance to various levels of occlusion is achieved. When some parts of an object are occluded, a strong classification model should be able to recognize its category from the overall object structure. However, the collected training samples usually exhibit limited variance in occlusion. In an extreme case when all the training objects are clearly visible, i.e., no occlusion happens, the learned CNN will probably work well on testing images without occlusion, but, due to the limited generalization ability of the CNN model, may fail to recognize objects which are partially occluded. While we can manually add occluded natural images to the training set, it is costly and the levels of occlusion might be limited.\n\nTo address the occlusion problem and improve the generalization ability of CNNs, this paper introduces a new data augmentation approach, Random Erasing. It can be easily implemented in most existing CNN models. In the training phase, an image within a mini-batch randomly undergoes either of the two operations: 1) kept unchanged; 2) we randomly choose a rectangle region of an arbitrary size, and assign the pixels within the selected region with random values (or the ImageNet (Deng et al. 2009) mean pixel value). During Operation 2), an image is partially occluded in a random position with a random-sized mask. In this manner, augmented images with various occlusion levels can be generated. Examples of Random Erasing are shown in Fig. 1.\n\nTwo commonly used data augmentation approaches, i.e., random flipping and random cropping, also work on the image level and are closely related to Random Erasing. Both techniques have demonstrated the ability to improve the image recognition accuracy. In comparison with Random Erasing, random flipping does not incur information loss during augmentation. Different from random cropping, in Random Erasing, 1) only part of the object is occluded and the overall object structure is preserved, 2) pixels of the erased region are re-assigned with random values, which can be viewed as adding block noise to the image.\n\nWorking primarily on the fully connected (FC) layer, Dropout (Srivastava et al. 2014) is also related to our method. It prevents over-fitting by discarding (both hidden and visible) units of the CNN with a probability p. Random Erasing is somewhat similar to performing Dropout on the image level. The difference is that in Random Erasing, 1) we operate on a continuous rectangular region, 2) no pixels (units) are discarded, and 3) we focus on making the model more robust to noise and occlusion. The recent A-Fast-RCNN (Wang, Shrivastava, and Gupta 2017) \n\n\nRelated Work\n\nOverfitting is a long-standing problem for the convolutional neural network (CNN). In general, methods of reducing the risk of overfitting can be divided into two categories: regularization and data augmentation. Regularization. Regularization is a key component in preventing over-fitting in the training of CNN models. Various regularization methods have been proposed (Krizhevsky, Sutskever, and Hinton 2012;Wan et al. 2013;Ba and Frey 2013;Zeiler and Fergus 2013;Xie et al. 2016;Kang et al. 2017). Dropout (Krizhevsky, Sutskever, and Hinton 2012) randomly discards (setting to zero) the output of each hidden neuron with a probability during the training and only considers the contribution of the remaining weights in forward pass and back-propagation. Latter, Wan et al. (Wan et al. 2013) propose a generalization of dropout approach, DropConect, which instead randomly selects weights to zero during training. In addition, Adaptive dropout (Ba and Frey 2013) is proposed where the dropout probability for each hidden neuron is estimated through a binary belief network. Stochastic Pooling (Zeiler and Fergus 2013) randomly selects activation from a multinomial distribution during training, which is parameter free and can be applied with other regularization techniques. Recently, a regularization method named \"DisturbLabel\" (Xie et al. 2016) is introduced by adding noise at the loss layer. Distur-bLabel randomly changes the labels of small part of samples to incorrect values during each training iteration. PatchShuffle (Kang et al. 2017) randomly shuffles the pixels within each local patch while maintaining nearly the same global structures with the original ones, it yields rich local variations for training of CNN.\n\nData augmentation. Data augmentation is an explicit form of regularization that is also widely used in the training of deep CNN (Krizhevsky, Sutskever, and Hinton 2012;Simonyan and Zisserman 2015;He et al. 2016a). It aims at artificially enlarging the training dataset from existing data using various translations, such as, translation, rotation, flipping, cropping, adding noises, etc. The two most popular and effective data augmentation methods in training of deep CNN are random flipping and random cropping. Random flipping randomly flips the input image horizontally, while random cropping extracts random sub-patch from the input image. As an analogous choice, Random Erasing may discard some parts of the object. For random cropping, it may crop off the corners of the object, while Random Erasing may occlude some parts of the object. Random Erasing maintains the global structure of object. Moreover, it can be viewed as adding noise to the image. The combination of random cropping and Random Erasing can produce more various training data.\n\nBlocking-based approach. Our work is also closely related to blocking-based approaches (Murdock et al. 2016;Fong and Vedaldi 2017;Wei et al. 2017;Wang, Shrivas-tava, and Gupta 2017;Kumar Singh and Jae Lee 2017). Murdock et al. (Murdock et al. 2016) propose \"Blockout\", a method for simultaneous regularization and model selection via masked weight matrices on CNN layers. The hyperparameters need to learn during training. In contrast, our approach is applied on image-level and does not need any extra parameter learning. Fong and Vedaldi (Fong and Vedaldi 2017) blur an image to suppress the SoftMax probability of the target class to learn saliency region. In contrast, our approach does not rely on any supervision information. In (Wei et al. 2017), by erasing the most discriminative region, a sequence of models is trained iteratively for weakly supervised semantic segmentation. In comparison, our approach only needs to train a single model once. Recently, Wang et al. (Wang, Shrivastava, and Gupta 2017) learn an adversary with Fast-RCNN (Girshick 2015) detection to create hard examples on the fly by blocking some feature maps spatially. Instead of generating occlusion examples in feature space, Random Erasing generates images from the original images with very little computation which is in effect and does not require any extra parameters learning. Singh and Lee (Kumar Singh and Jae Lee 2017) randomly hide patches (with black pixels) in a training image to force the network to seek discriminative parts as many as possible for object localization. Instead, our approach randomly selects a rectangle region in an image and erases its pixels with random values. Comparison with the above mentioned methods, our approach aims to reduce the risk of over-fitting, which is model-agnostic, does not require extra parameter learning and can be easily applied to various vision tasks. Our method and Cutout (DeVries and Taylor 2017) are contemporary works. Different from Cutout, we evaluate our method on more vision tasks. We also investigate the impact of different erasing values and different aspect ratios of the erased region.\n\n\nDatasets\n\nFor image classification, we evaluate on four image classification datasets, including two well-known datasets, CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton 2009), a new dataset Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), and a large-scale dataset ImageNet2012 (Deng et al. 2009). CIFAR-10 and CIFAR-100 contain 50,000 training and 10,000 testing 32\u00d732 color images drawn from 10 and 100 classes, respectively. Fashion-MNIST consists of 60,000 training and 10,000 testing 28x28 gray-scale images. Each image is associated with a label from 10 classes. Ima-geNet2012 consists of 1,000 classes, including 1.28 million training images and 50k validation images. For CIFAR-10, CIFAR-100 and Fashion-MNIST, we evaluate top-1 error rates in the format \"mean \u00b1 std\" based on 5 runs. For Im-ageNet2012, we evaluate the top-1 and top-5 error rates on the validation set.\n\nFor object detection, we use the PASCAL VOC 2007 (Everingham et al. 2010) dataset which contains 9,963 images of 24,640 annotated objects in training/validation and testing sets. We use the \"trainval\" set for training and \"test\" set for testing. We evaluate the performance using mean average precision (mAP).\n\n\nAlgorithm 1: Random Erasing Procedure\n\nInput : Input image I; Image size W and H; Area of image S; Erasing probability p; Erasing area ratio range s l and s h ; Erasing aspect ratio range r 1 and r 2 . Output: Erased image I * . For person re-identification (re-ID), the Market-1501 dataset (Zheng et al. 2015) contains 12,936 images with 751 identities for training, 19,732 images with 750 identities and 3,368 query images for testing. DukeMTMC-reID Ristani et al. 2016) includes 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images. For CUHK03 (Li et al. 2014), we use the new training/testing protocol proposed in (Zhong et al. 2017). There are 767 identities in the training set and 700 identities in the testing set. We conduct experiment on both \"detected\" and \"labeled\" sets. Rank-1 accuracy and mean average precision (mAP) are evaluated on these three datasets.\nInitialization: p 1 \u2190 Rand (0, 1). 1 if p 1 \u2265 p then 2 I * \u2190 I; 3 return I * . 4 else 5 while True do 6 S e \u2190 Rand (s l , s h )\u00d7S; 7 r e \u2190 Rand (r 1 , r 2 ); 8 H e \u2190 \u221a S e \u00d7 r e , W e \u2190 Se re ; 9 x e \u2190 Rand (0, W ), y e \u2190 Rand (0, H); 10 if x e + W\n\nOur Approach\n\nThis section presents the Random Erasing data augmentation method for training the convolutional neural network (CNN). We first describe the detailed procedure of Random Erasing. Next, the implementation of Random Erasing in different tasks is introduced. Finally, we analyze the differences between Random Erasing and random cropping.\n\n\nRandom Erasing\n\nIn training, Random Erasing is conducted with a certain probability. For an image I in a mini-batch, the probability of it undergoing Random Erasing is p, and the probability of it being kept unchanged is 1 \u2212 p. In this process, training images with various levels of occlusion are generated.\n\nRandom Erasing randomly selects a rectangle region I e in an image, and erases its pixels with random values. Assume that the size of the training image is W \u00d7 H. The area of the image is S = W \u00d7 H. We randomly initialize the area of erasing rectangle region to S e , where Se S is in range specified by minimum s l and maximum s h . The aspect ratio of erasing rectangle region is randomly initialized between r 1 and r 2 , we set it to r e . The size of I e is H e = \u221a S e \u00d7 r e and W e = Se re . Then, we randomly initialize a point P = (x e , y e ) in I. If x e +W e \u2264 W and y e +H e \u2264 H, we set the region, I e = (x e , y e , x e + W e , y e + H e ), as the selected rectangle region. Otherwise repeat the above process until an appropriate I e is selected. With the selected erasing region I e , each pixel in I e is assigned to a random value in [0, 255], respectively. The procedure of selecting the rectangle area and erasing this area is shown in Alg. 1.\n\n\nRandom Erasing for Image Classification and Person Re-identification\n\nIn image classification, an image is classified according to its visual content. In general, training data does not provide the location of the object, so we could not know where the object is. In this case, we perform Random Erasing on the whole image according to Alg. 1.\n\nRecently, the person re-ID model is usually trained in a classification network for embedding learning (Zheng, Yang, and Hauptmann 2016). In this task, since pedestrians are confined with detected bounding boxes, persons are roughly in the same position and take up the most area of the image. In this scenario, we adopt the same strategy as image classification, as in practice, the pedestrian can be occluded in any position. We randomly select rectangle regions on the whole pedestrian image and erase it. Examples of Random Erasing for image classification and person re-ID are shown in Fig. 1(a, b).\n\n\nRandom Erasing for Object Detection\n\nObject detection aims at detecting instances of semantic objects of a certain class in images. Since the location of each object in the training image is known, we implement Random Erasing with three schemes: 1) Image-aware Random Erasing (IRE): selecting erasing region on the whole image, the same as image classification and person re-identification; 2) Object-aware Random Erasing (ORE): selecting erasing regions in the bounding box of each object. In the latter, if there are multiple objects in the image, Random Erasing is applied on each object separately. 3) Image and object-aware Random Erasing (I+ORE): selecting erasing regions in both the whole image and each object bounding box. Examples of Random Erasing for object detection with the three schemes are shown in Fig. 1(c).\n\n\nComparison with Random Cropping\n\nRandom cropping is an effective data augmentation approach, it reduces the contribution of the background in the CNN decision, and can base learning models on the presence of parts of the object instead of focusing on the whole object. In comparison to random cropping, Random Erasing retains the overall structure of the object, only occluding some parts of object. In addition, the pixels of erased region are re-assigned with random values, which can be viewed as adding noise to the image. When jointly employing random cropping and Random Erasing during training, more various images can be generated for data augmentation. In our experiment (Section 5.2), we show that these two methods are complementary to each other for improving the discriminative ability of CNN. The examples of Random Erasing, random cropping, and the combination of them are shown in Fig. 1(d).\n\n\nImage Classification\n\n\nExperiment Settings\n\nIn all of our experiment, we compare the CNN models trained with or without Random Erasing. For the same deep architecture, all the models are trained from the same weight initialization. Note that some popular regularization techniques (e.g., weight decay, batch normalization and dropout) and various data augmentations (e.g., flipping, padding and cropping) are employed. The compared CNN architectures are summarized as below.\n\nArchitectures and Settings. Four architectures are adopted on CIFAR-10, CIFAR-100 and Fashion-MNIST: ResNet (He et al. 2016a), pre-activation ResNet (He et al. 2016b), ResNeXt (Xie et al. 2017), and Wide Residual Networks (Zagoruyko and Komodakis 2016). We use the 20, 32, 44, 56, 110-layer network for ResNet. The 18-layer network is also adopted for pre-activation ResNet. We use ResNeXt-29-8\u00d764 and WRN-28-10 in the same way as (Xie et al. 2017) and (Zagoruyko and Komodakis 2016), respectively. The training procedure follows (He et al. 2016a). Specially, the learning rate starts from 0.1 and is divided by 10 after the 150th and 225th epoch. We stop training by the 300th epoch. If not specified, all models are trained with data augmentation: randomly performs horizontal flipping, and takes a random cropping with 32\u00d732 for CIFAR-10 and CIFAR-100 (28\u00d728 for Fashion-MNIST) from images padded by 4 pixels on each side. For Imagenet-2012 (Deng et al. 2009), we follow the training strategy of ResNet and conduct experiment on ResNet-34, ResNet-50 and ResNet-101. Random cropping, random flipping and label smoothing regularization (Szegedy et al. 2016) are used during model training.\n\n\nClassification Evaluation\n\nClassification accuracy on different datasets. We first evaluate Random Erasing on medium-scale datasets. The results on CIFAR-10 ,CIFAR-100 and Fashion-MNIST with different architectures are shown in Table 1. We set p = 0.5, s l = 0.02, s h = 0.4, and r 1 = 1 r2 = 0.3. Results indicate that models trained with Random Erasing have significant improvement, demonstrating that our method is applicable to various CNN architectures. For CIFAR-10, our method improves the accuracy by 0.49% using ResNet-110. In particular, our approach obtains 3.08% error rate using WRN-28-10, which improves the accuracy by 0.72% and achieves new state of the art. For CIFAR-100, our method obtains 17.73% error rate which gains 0.76% than the WRN-28-10 baseline. Our method also works well for gray-scale images: Random erasing improves WRN-28-10 from 4.01% to   3.65% in top-1 error on Fashion-MNIST. We then evaluate our approach on large-scale dataset. Results on ImageNet-2012 with different architectures are reported in Table 2. Our method consistently improves the results on all three architectures, demonstrating the effectiveness of our method on large-scale dataset.\n\nThe impact of hyper-parameters. When implementing Random Erasing on CNN training, we have three hyperparameters to evaluate, i.e., the erasing probability p, the area ratio range of erasing region s l and s h , and the aspect ratio range of erasing region r 1 and r 2 . To demonstrate the impact of these hyper-parameters on the model performance, we conduct experiment on CIFAR-10 based on ResNet18 (pre-act) under varying hyper-parameter settings. To simplify experiment, we fix s l to 0.02, r 1 = 1 r2 and evaluate p, s h , and r 1 . We set p = 0.5, s h = 0.4 and r 1 = 0.3 as the base setting. When evaluating one of the parameters, we fixed the other two parameters. Results are shown in Fig. 2.\n\nNotably, Random Erasing consistently outperforms the ResNet18 (pre-act) baseline under all parameter settings. For example, when p \u2208 [0.2, 0.8] and s h \u2208 [0.2, 0.8], the average classification error rate is 4.48%, outperforming the baseline method (5.17%) by a large margin. Random Erasing is also robust to the aspect ratios of the erasing region. Specifically, our best result (when r 1 = 0.3, error rate = 4.31%) reduces the classification error rate by 0.86% compared with the baseline. In the following experiment for image classification, we set p = 0.5, s l = 0.02, s h = 0.4, and r 1 = 1 r2 = 0.3, if not specified. Four types of random values for erasing. We evaluate Random Erasing when pixels in the selected region are erased in four ways: 1) each pixel is assigned with a random  Test error (%) Noise Test error (%) \u03bb 1 = 0.001 5.37 \u00b1 0.12 \u03bb 2 = 0.01 5.38 \u00b1 0.07 \u03bb 1 = 0.005 5.48 \u00b1 0.15 \u03bb 2 = 0.05 5.79 \u00b1 0.14 \u03bb 1 = 0.01 5.89 \u00b1 0.14 \u03bb 2 = 0.1 6.13 \u00b1 0.12 \u03bb 1 = 0.05 6.23 \u00b1 0.11 \u03bb 2 = 0.2 6.25 \u00b1 0.09 \u03bb 1 = 0.1 6.38 \u00b1 0.18 \u03bb 2 = 0.4 6.52 \u00b1 0.12 Baseline 11.31 \u00b1 0.18 8.30 \u00b1 0.17 6.33 \u00b1 0.15 10.13 \u00b1 0.14 5.17 \u00b1 0.18 7.19 \u00b1 0.10 5.21 \u00b1 0.14 4.31 \u00b1 0.07 value ranging in [0, 255], denoted as RE-R; 2) all pixels are assign with the mean ImageNet pixel value i.e., [125,122,114], denoted as RE-M; 3) all pixels are assigned with 0, denoted as RE-0; 4) all pixels are assigned with 255, denoted as RE-255. Table 3 presents the result with different erasing values on CIFAR10 using ResNet18 (pre-act). We observe that, 1) all erasing schemes outperform the baseline, 2) RE-R achieves approximately equal performance to RE-M, and 3) both RE-R and RE-M are superior to RE-0 and RE-255.\n\nIf not specified, we use RE-R in the following experiment.\n\nComparison with Dropout and random noise. We compare Random Erasing with two variant methods applied on image layer. 1) Dropout: we apply dropout on image layer with probability \u03bb 1 . 2) Random noise: we add different levels of noise on the input image by changing the pixel to a random value in [0, 255] with probability \u03bb 2 . The probability of whether an image undergoes dropout or random noise is set to 0.5 as Random Erasing. Results are presented in Table 4. It is clear that applying dropout or adding random noise at the image layer fails to improve the accuracy. As the probability \u03bb 1 and \u03bb 2 increase, performance drops quickly.\n\nWhen \u03bb 2 = 0.4, the number of noise pixels for random noise is approximately equal to the number of erasing pixels for Random Erasing, the error rate of random noise increases from 5.17% to 6.52%, while Random Erasing reduces the error rate to 4.31%.\n\nComparing with data augmentation methods. We compare our method with random flipping and random cropping in Table 5. When applied alone, random cropping (6.33%) outperforms the other two methods. Importantly, Random Erasing and the two competing techniques are complementary. Particularly, combining these three methods achieves 4.31% error rate, a 7% improvement over the baseline without any augmentation.\n\nRobustness to occlusion. Last, we show the robustness of Random Erasing against occlusion. In this experiment, we add different levels of occlusion to the CIFAR-10 dataset in testing. We randomly select a region of area and fill it with random values. The aspect ratio of the region is randomly chosen from the range of [0.3, 3.33]. Results as shown in Fig. 3. Obviously, the baseline performance drops quickly when increasing the occlusion level l. In comparison, the performance of the model training with Random Erasing decreases slowly. Our approach achieves 56.36% error rate when the occluded area is half of the image (l = 0.5), while the baseline rapidly drops to 75.04%. It demonstrates that Random Erasing improves the robustness of CNNs against occlusion.\n\n6 Object Detection\n\n\nExperiment Settings\n\nExperiment is conducted based on the Fast-RCNN (Girshick 2015) detector. The model is initialized by the Ima-geNet classification models, and then fine-tuned on the object detection data. We experiment with VGG16 (Simonyan and Zisserman 2015) architecture. We follow A-Fast-RCNN (Wang, Shrivastava, and Gupta 2017) for training. We apply SGD for 80K to train all models. The training rate starts with 0.001 and decreases to 0.0001 after 60K iterations. With this training procedure, the baseline mAP is slightly better than the report mAP in (Girshick 2015). We use the selective search proposals during training. For Random Erasing, we set p = 0.5, s l = 0.02, s h = 0.2, and r 1 = 1 r2 = 0.3.\n\n\nDetection Evaluation\n\nWe report results with using IRE, ORE and I+ORE during training Fast-RCNN in Table 6. The detector is trained with VOC07 trainval and the union of VOC07 and VOC12 trainval. When training with VOC07 trainval, the baseline is 69.1% mAP. The detector learned with IRE scheme achieves  (Wang, Shrivastava, and Gupta 2017  Three baselines are used in person re-ID, i.e., the IDdiscriminative Embedding (IDE) (Zheng, Yang, and Hauptmann 2016), TriNet (Hermans, Beyer, and Leibe 2017), and SVDNet . IDE and SVDNet are trained with the Softmax loss, while TriNet is trained with the triplet loss. The input images are resized to 256 \u00d7 128. We use the ResNet-18, ResNet-34, and ResNet-50 architectures for IDE and TriNet, and ResNet-50 for SVDNet. We fine-tune them on the model pre-trained on ImageNet (Deng et al. 2009). We also perform random cropping and random horizontal flipping during training. For Random Erasing, we set p = 0.5, s l = 0.02, s h = 0.2, and r 1 = 1 r2 = 0.3.\n\n\nPerson Re-identification Performance\n\nRandom Erasing improves different baseline models. As shown in Table 7, when implementing Random Erasing in these baseline models, Random Erasing consistently improves the rank-1 accuracy and mAP. Specifically, for Market-1501, Random Erasing improves the rank-1 by 3.10% and 2.67% for IDE and SVDNet with using ResNet-50. For DukeMTMC-reID, Random Erasing increases the rank-1 accuracy from 71.99% to 74.24% for IDE (ResNet-50) and from 76.82% to 79.31% for SVDNet (ResNet-50). For CUHK03, TriNet gains 8.28% and 5.0% in rank-1 accuracy when applying Random Erasing on the labeled and detected settings with ResNet-50, respectively. We note that, due to lack of adequate training data, over-fitting tend to oc-cur on CUHK03. For example, a deeper architecture, such as ResNet-50, achieves lower performance than ResNet-34 when using the IDE mode on the detected subset. However, with our method, IDE (ResNet-50) outperforms IDE . This indicates that our method can reduce the risk of over-fitting and improves the re-ID performance.\n\n\nConclusion\n\nIn this paper, we propose a new data augmentation approach named \"Random Erasing\" for training the convolutional neural network (CNN). It is easy to implemented: Random Erasing randomly occludes an arbitrary region of the input image during each training iteration. Experiment conducted on CIFAR10, CIFAR100, Fashion-MNIST and ImageNet with various architectures validate the effectiveness of our method. Moreover, we obtain reasonable improvement on object detection and person re-identification, demonstrating that our method has good performance on various recognition tasks. In the future work, we will apply our approach to other CNN recognition tasks, such as, image retrieval, face recognition and fine-grained classification.\n\nFigure 2 :\n2Test errors (%) under different hyper-parameters on CIFAR-10 with using ResNet18 (pre-act).\n\nFigure 3 :\n3Test errors (%) under different levels of occlusion on CIFAR-10 based on ResNet18 (pre-act). The model trained with Random Erasing is more robust to occlusion.\n\npro -\nproObject-aware Random ErasingImage-aware Random Erasing Image and object-aware Random Erasing Fast-RCNN in object detection. To summarize, Random Erasing has the following advantages:\u2022 A lightweight method that does not require any extra parameter learning or memory consumption. It can be integrated with various CNN models without changing the learning strategy. \u2022 A complementary method to existing data augmentation and regularization approaches. When combined, Random Erasing further improves the recognition performance. \u2022 Consistently improving the performance of recent stateof-the-art deep models on image classification, object detection, and person re-identification. \u2022 Improving the robustness of CNNs to partially occluded samples. When we randomly adding occlusion to the CIFAR-10 testing dataset, Random Erasing significantly outperforms the baseline model.Random cropping \n\nRandom Erasing \n\nRandom cropping + Random Erasing \n\n(a) Image classification \n(b) Person re-ID \n(c) Object detection \n(d) Different augmentation methods \n\nFigure 1: Examples of Random Erasing in image classification (a), person re-identification (re-ID) (b), object detection (c) and \ncomparing with different augmentation methods (d). In CNN training, we randomly choose a rectangle region in the image \nand erase its pixels with random values or the ImageNet mean pixel value. Images with various levels of occlusion are thus \ngenerated. \n\nposes an occlusion invariant object detector by training an \nadversarial network that generates examples with occlusion. \nComparison with A-Fast-RCNN, Random Erasing does not \nrequire any parameter learning, can be easily applied to other \nCNN-based recognition tasks and still yields competitive ac-\ncuracy with A-\n\n\ne \u2264 W and y e + H e \u2264 H then 11 I e \u2190 (x e , y e , x e + W e , y e + H e );12 \n\nI(I e ) \u2190 Rand (0, 255); \n\n13 \n\nI  *  \u2190 I; \n\n14 \n\nreturn I  *  . \n\n15 \n\nend \n\n16 \n\nend \n17 end \n\n\n\nTable 1 :\n1Test errors (%) with different architectures on CIFAR-10, CIFAR-100 and Fashion-MNIST. RE: Random Erasing.Model \nCIFAR-10 \nCIFAR-100 \nFashion-MNIST \nBaseline \nRE \nBaseline \nRE \nBaseline \nRE \n\nResNet-20 \n7.21 \u00b1 0.17 \n6.73 \u00b1 0.09 \n30.84 \u00b1 0.19 \n29.97 \u00b1 0.11 \n4.39 \u00b1 0.08 \n4.02 \u00b1 0.07 \nResNet-32 \n6.41 \u00b1 0.06 \n5.66 \u00b1 0.10 \n28.50 \u00b1 0.37 \n27.18 \u00b1 0.32 \n4.16 \u00b1 0.13 \n3.80 \u00b1 0.05 \nResNet-44 \n5.53 \u00b1 0.08 \n5.13 \u00b1 0.09 \n25.27 \u00b1 0.21 \n24.29 \u00b1 0.16 \n4.41 \u00b1 0.09 \n4.01 \u00b1 0.14 \nResNet-56 \n5.31 \u00b1 0.07 \n4.89 \u00b1 0.07 \n24.82 \u00b1 0.27 \n23.69 \u00b1 0.33 \n4.39 \u00b1 0.10 \n4.13 \u00b1 0.42 \nResNet-110 \n5.10 \u00b1 0.07 \n4.61 \u00b1 0.06 \n23.73 \u00b1 0.37 \n22.10 \u00b1 0.41 \n4.40 \u00b1 0.10 \n4.01 \u00b1 0.13 \n\nResNet-18-PreAct \n5.17 \u00b1 0.18 \n4.31 \u00b1 0.07 \n24.50 \u00b1 0.29 \n24.03 \u00b1 0.19 \n4.31 \u00b1 0.06 \n3.90 \u00b1 0.06 \nWRN-28-10 \n3.80 \u00b1 0.07 \n3.08 \u00b1 0.05 \n18.49 \u00b1 0.11 \n17.73 \u00b1 0.15 \n4.01 \u00b1 0.10 \n3.65 \u00b1 0.03 \nResNeXt-8-64 \n3.54 \u00b1 0.04 \n3.24 \u00b1 0.03 \n19.27 \u00b1 0.30 \n18.84 \u00b1 0.18 \n4.02 \u00b1 0.05 \n3.79 \u00b1 0.06 \n\np \n\n(a) probability p \n\nsh \n\n(b) area ratio s h \n\nr \n\n(c) aspect ratio r1 \n\n\n\nTable 2 :\n2Test errors (%) on ImageNet-2012 validation set.Model \nBaseline \nRandom Erasing \nTop-1 \nTop-5 \nTop-1 \nTop-5 \nResNet-34 \n25.22 \n8.01 \n24.89 \n7.71 \nResNet-50 \n23.39 \n6.89 \n22.75 \n6.69 \nResNet-101 \n20.98 \n5.73 \n20.43 \n5.30 \n\n\n\nTable 3 :\n3Test errors (%) on CIFAR-10 based on ResNet18 (pre-act) with four types of erasing value. Baseline: Baseline model, \nRE-R: Random Erasing model with random value, RE-M: Random Erasing model with mean value of ImageNet 2012, RE-0: \nRandom Erasing model with 0, RE-255: Random Erasing model with 255. \n\nErasing Value \nBaseline \nRE-R \nRE-M \nRE-0 \nRE-255 \nTest Errors(%) \n5.17 \u00b1 0.18 \n4.31 \u00b1 0.07 \n4.35 \u00b1 0.12 \n4.62 \u00b1 0.09 \n4.85 \u00b1 0.13 \n\n\n\nTable 4 :\n4Comparing Random Erasing with dropout and ran-\ndom noise on CIFAR-10 with using ResNet18 (pre-act). \n\nMethod \nTest error (%) Method \nTest error (%) \nBaseline \n5.17 \u00b1 0.18 \nBaseline \n5.17 \u00b1 0.18 \nOurs \n4.31 \u00b1 0.07 \nOurs \n4.31 \u00b1 0.07 \nDropout \n\n\nTable 5 :\n5Test errors (%) with different data augmenta-\ntion methods on CIFAR-10 based on ResNet18 (pre-act). \nRF: Random flipping, RC: Random cropping, RE: Random \nErasing. \n\nMethod \nRF \nRC \nRE \nTest errors (%) \n\n\n\nTable 6 :\n6VOC 2007 test detection average precision (%). refers to training schedule in \n\n\n). I+ORE) 07+12 76.2 79.6 82.5 75.7 70.5 55.1 85.2 84.4 88.4 58.6 82.6 73.9 84.2 84.7 78.8 76.3 46.7 77.9 75.9 83.3 79.3Method \ntrain set mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv \n\nFRCN \n07 \n66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8 \nFRCN \n07 \n69.1 75.4 80.8 67.3 59.9 37.6 81.9 80.0 84.5 50.0 77.1 68.2 81.0 82.5 74.3 69.9 28.4 71.1 70.2 75.8 66.6 \nA-Fast-RCNN 07 \n71.0 74.4 81.3 67.6 57.0 46.6 81.0 79.3 86.0 52.9 75.9 73.7 82.6 83.2 77.7 72.7 37.4 66.3 71.2 78.2 74.3 \nOurs (IRE) \n07 \n70.5 75.9 78.9 69.0 57.7 46.4 81.7 79.5 82.9 49.3 76.9 67.9 81.5 83.3 76.7 73.2 40.7 72.8 66.9 75.4 74.2 \nOurs (ORE) \n07 \n71.0 75.1 79.8 69.7 60.8 46.0 80.4 79.0 83.8 51.6 76.2 67.8 81.2 83.7 76.8 73.8 43.1 70.8 67.4 78.3 75.6 \nOurs (I+ORE) 07 \n71.5 76.1 81.6 69.5 60.1 45.6 82.2 79.2 84.5 52.5 78.7 71.6 80.4 83.3 76.7 73.9 39.4 68.9 69.8 79.2 77.4 \n\nFRCN \n07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 \nFRCN \n07+12 74.8 78.5 81.0 74.7 67.9 53.4 85.6 84.4 86.2 57.4 80.1 72.2 85.2 84.2 77.6 76.1 45.3 75.7 72.3 81.8 77.3 \nOurs (IRE) \n07+12 75.6 79.0 84.1 76.3 66.9 52.7 84.5 84.4 88.7 58.0 82.9 71.1 84.8 84.4 78.6 76.7 45.5 77.1 76.3 82.5 76.8 \nOurs (ORE) \n07+12 75.8 79.4 81.6 75.6 66.5 52.7 85.5 84.7 88.3 58.7 82.9 72.8 85.0 84.3 79.3 76.3 46.3 76.3 74.9 86.0 78.2 \nOurs (\n\nTable 7 :\n7Person re-identification performance with Random Erasing (RE) on Market-1501, DukeMTMC-reID, and CUHK03 based on different models. We evaluate CUHK03 under the new evaluation protocol in(Zhong et al. 2017).an improvement to 70.5% mAP and the ORE scheme obtains 71.0% mAP. The ORE performs slightly better than IRE. When implementing Random Erasing on overall image and objects, the detector training with I+ORE obtains further improved in performance with 71.5% mAP. Our approach (I+ORE) outperforms A-Fast-RCNN (Wang, Shrivastava, and Gupta 2017) by 0.5% in mAP. Moreover, our method does not require any parameter learning and is easy to implement. When using the enlarged 07+12 training set, the baseline is 74.8% which is much better than only using 07 training set. The IRE and ORE schemes give similar results, in which the mAP of IRE is improved by 0.8% and ORE is improved by 1.0%. When applying I+ORE during training, the mAP of Fast-RCNN increases to 76.2%, surpassing the baseline by 1.4%.Method \nModel \nRE \nMarket \nDuke \nCUHK03 (L) \nCUHK03 (D) \nRank-1 \nmAP \nRank-1 \nmAP \nRank-1 \nmAP \nRank-1 \nmAP \n\nIDE \n\nResNet-18 \nNo \n79.87 \n57.37 \n67.73 \n46.87 \n28.36 \n25.65 \n26.86 \n25.04 \nYes \n82.36 \n62.06 \n70.60 \n51.41 \n36.07 \n32.58 \n34.21 \n31.20 \n\nResNet-34 \nNo \n82.93 \n62.34 \n71.63 \n49.71 \n31.57 \n28.66 \n30.14 \n27.55 \nYes \n84.80 \n65.68 \n73.56 \n54.46 \n40.29 \n35.50 \n36.36 \n33.46 \n\nResNet-50 \nNo \n83.14 \n63.56 \n71.99 \n51.29 \n30.29 \n27.37 \n28.36 \n26.74 \nYes \n85.24 \n68.28 \n74.24 \n56.17 \n41.46 \n36.77 \n38.50 \n34.75 \n\nTriNet \n\nResNet-18 \nNo \n77.32 \n58.43 \n67.50 \n46.27 \n43.00 \n39.16 \n40.50 \n37.36 \nYes \n79.84 \n61.68 \n71.81 \n51.84 \n48.29 \n43.80 \n46.57 \n43.20 \n\nResNet-34 \nNo \n80.73 \n62.65 \n72.04 \n51.56 \n46.00 \n43.79 \n45.07 \n42.58 \nYes \n83.11 \n65.98 \n72.89 \n55.38 \n53.07 \n48.80 \n53.21 \n48.03 \n\nResNet-50 \nNo \n82.60 \n65.79 \n72.44 \n53.50 \n49.86 \n46.74 \n50.50 \n46.47 \nYes \n83.94 \n68.67 \n72.98 \n56.60 \n58.14 \n53.83 \n55.50 \n50.74 \n\nSVDNet ResNet-50 \nNo \n84.41 \n65.60 \n76.82 \n57.70 \n42.21 \n38.73 \n41.85 \n38.24 \nYes \n87.08 \n71.31 \n79.31 \n62.44 \n49.43 \n45.07 \n48.71 \n43.50 \n\n\nAcknowledgmentWe wish to thank the anonymous reviewers for their helpful comments. We also would like to thank Ross Wightman for reproducing our method on ImageNet and providing important experimental results for this paper. Zhun Zhong thanks Wenjing Li for encouragement. This work is supported by the National Nature Science Foundation of China (No. 61876159, 61806172, 61572409, U1705286 &  61571188), the National Key Research and Development Program of China (No. 2018YFC0831402).\nAdaptive dropout for training deep neural networks. J Ba, B Frey, In NIPSBa, J., and Frey, B. 2013. Adaptive dropout for training deep neural networks. In NIPS.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In CVPR.\n\nImproved regularization of convolutional neural networks with cutout. T Devries, G W Taylor, arXiv:1708.04552arXiv preprintDeVries, T., and Taylor, G. W. 2017. Improved regulariza- tion of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552.\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, IJCVEveringham, M.; Van Gool, L.; Williams, C. K.; Winn, J.; and Zis- serman, A. 2010. The pascal visual object classes (voc) challenge. IJCV.\n\nInterpretable explanations of black boxes by meaningful perturbation. R Fong, A Vedaldi, ICCV. Girshick, R. 2015. Fast r-cnn. ICCVFong, R., and Vedaldi, A. 2017. Interpretable explanations of black boxes by meaningful perturbation. In ICCV. Girshick, R. 2015. Fast r-cnn. In ICCV.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a. Deep residual learning for image recognition. In CVPR.\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J ; A Sun, L Beyer, B Leibe, arXiv:1703.07737defense of the triplet loss for person re-identification. arXiv preprintECCV. Springer. Hermans,He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b. Identity mappings in deep residual networks. In ECCV. Springer. Hermans, A.; Beyer, L.; and Leibe, B. 2017. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML. Ioffe, S., and Szegedy, C. 2015. Batch normalization: Accelerat- ing deep network training by reducing internal covariate shift. In ICML.\n\n. G Kang, X Dong, L Zheng, Yang , Y , arXiv:1707.07103Patchshuffle regularization. arXiv preprintKang, G.; Dong, X.; Zheng, L.; and Yang, Y. 2017. Patchshuffle regularization. arXiv preprint arXiv:1707.07103.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, CiteseerTechnical reportKrizhevsky, A., and Hinton, G. 2009. Learning multiple layers of features from tiny images. Technical report, Citeseer.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In NIPS.\n\nHide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. Kumar Singh, K , Jae Lee, Y , ICCV. Kumar Singh, K., and Jae Lee, Y. 2017. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In ICCV.\n\nDeepreid: Deep filter pairing neural network for person re-identification. W Li, R Zhao, T Xiao, X Wang, CVPR. Li, W.; Zhao, R.; Xiao, T.; and Wang, X. 2014. Deepreid: Deep filter pairing neural network for person re-identification. In CVPR.\n\nPerformance measures and a data set for multi-target, multi-camera tracking. C Murdock, Z Li, H Zhou, T Duerig, F Solera, R Zou, R Cucchiara, C Tomasi, ECCVW. CVPR. Ristani, E.Blockout: Dynamic model selection for hierarchical deep networksMurdock, C.; Li, Z.; Zhou, H.; and Duerig, T. 2016. Blockout: Dynamic model selection for hierarchical deep networks. In CVPR. Ristani, E.; Solera, F.; Zou, R.; Cucchiara, R.; and Tomasi, C. 2016. Performance measures and a data set for multi-target, multi-camera tracking. In ECCVW.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. Simonyan, K., and Zisserman, A. 2015. Very deep convolutional networks for large-scale image recognition. In ICLR.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, JMLRSrivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. JMLR.\n\nSVDNet for pedestrian retrieval. Y Sun, L Zheng, W Deng, S Wang, ICCV. Sun, Y.; Zheng, L.; Deng, W.; and Wang, S. 2017. SVDNet for pedestrian retrieval. In ICCV.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2016. Rethinking the inception architecture for computer vision. In CVPR.\n\nRegularization of neural networks using dropconnect. L Wan, M Zeiler, S Zhang, Y L Cun, Fergus , R , ICML. Wan, L.; Zeiler, M.; Zhang, S.; Cun, Y. L.; and Fergus, R. 2013. Regularization of neural networks using dropconnect. In ICML.\n\nA-fast-rcnn: Hard positive generation via adversary for object detection. X Wang, A Shrivastava, A Gupta, CVPR. Wang, X.; Shrivastava, A.; and Gupta, A. 2017. A-fast-rcnn: Hard positive generation via adversary for object detection. In CVPR.\n\nObject region mining with adversarial erasing: A simple classification to semantic segmentation approach. Y Wei, J Feng, X Liang, M.-M Cheng, Y Zhao, Yan , S , CVPR. Wei, Y.; Feng, J.; Liang, X.; Cheng, M.-M.; Zhao, Y.; and Yan, S. 2017. Object region mining with adversarial erasing: A simple classification to semantic segmentation approach. In CVPR.\n\nFashion-mnist: a novel image dataset for benchmarking machine learning algorithms. H Xiao, K Rasul, R Vollgraf, arXiv:1708.07747arXiv preprintXiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-mnist: a novel image dataset for benchmarking machine learning algo- rithms. arXiv preprint arXiv:1708.07747.\n\nDisturblabel: Regularizing cnn on the loss layer. L Xie, J Wang, Z Wei, M Wang, Q Tian, CVPR. Xie, L.; Wang, J.; Wei, Z.; Wang, M.; and Tian, Q. 2016. Distur- blabel: Regularizing cnn on the loss layer. In CVPR.\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Doll\u00e1r, Z Tu, K He, CVPR. Xie, S.; Girshick, R.; Doll\u00e1r, P.; Tu, Z.; and He, K. 2017. Aggre- gated residual transformations for deep neural networks. In CVPR.\n\nWide residual networks. S Zagoruyko, N Komodakis, BMVC. Zagoruyko, S., and Komodakis, N. 2016. Wide residual networks. In BMVC.\n\nStochastic pooling for regularization of deep convolutional neural networks. M D Zeiler, Fergus , R , In ICLRZeiler, M. D., and Fergus, R. 2013. Stochastic pooling for regular- ization of deep convolutional neural networks. In ICLR.\n\nUnderstanding deep learning requires rethinking generalization. C Zhang, S Bengio, M Hardt, B Recht, O Vinyals, ICLR. Zhang, C.; Bengio, S.; Hardt, M.; Recht, B.; and Vinyals, O. 2017. Understanding deep learning requires rethinking generalization. In ICLR.\n\nScalable person re-identification: A benchmark. L Zheng, L Shen, L Tian, S Wang, J Wang, Q ; L Tian, Y Yang, A G Hauptmann, arXiv:1610.02984Person re-identification: Past, present and future. arXiv preprintICCV. Zheng,Zheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian, Q. 2015. Scalable person re-identification: A benchmark. In ICCV. Zheng, L.; Yang, Y.; and Hauptmann, A. G. 2016. Per- son re-identification: Past, present and future. arXiv preprint arXiv:1610.02984.\n\nUnlabeled samples generated by gan improve the person re-identification baseline in vitro. Z Zheng, L Zheng, Yang , Y , ICCV. Zheng, Z.; Zheng, L.; and Yang, Y. 2017. Unlabeled samples gen- erated by gan improve the person re-identification baseline in vitro. In ICCV.\n\nRe-ranking person re-identification with k-reciprocal encoding. Z Zhong, L Zheng, D Cao, S Li, CVPR. Zhong, Z.; Zheng, L.; Cao, D.; and Li, S. 2017. Re-ranking person re-identification with k-reciprocal encoding. In CVPR.\n", "annotations": {"author": "[{\"end\":214,\"start\":36},{\"end\":227,\"start\":215},{\"end\":298,\"start\":228},{\"end\":367,\"start\":299},{\"end\":485,\"start\":368}]", "publisher": null, "author_last_name": "[{\"end\":46,\"start\":41},{\"end\":226,\"start\":221},{\"end\":241,\"start\":237},{\"end\":308,\"start\":306},{\"end\":375,\"start\":371}]", "author_first_name": "[{\"end\":40,\"start\":36},{\"end\":220,\"start\":215},{\"end\":236,\"start\":228},{\"end\":305,\"start\":299},{\"end\":370,\"start\":368}]", "author_affiliation": "[{\"end\":104,\"start\":48},{\"end\":213,\"start\":106},{\"end\":297,\"start\":243},{\"end\":366,\"start\":310},{\"end\":484,\"start\":377}]", "title": "[{\"end\":33,\"start\":1},{\"end\":518,\"start\":486}]", "venue": null, "abstract": "[{\"end\":1316,\"start\":520}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2010,\"start\":1970},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2050,\"start\":2021},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2084,\"start\":2060},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2134,\"start\":2110},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3458,\"start\":3441},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4410,\"start\":4386},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4881,\"start\":4846},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5310,\"start\":5270},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5326,\"start\":5310},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5343,\"start\":5326},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5366,\"start\":5343},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5382,\"start\":5366},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5399,\"start\":5382},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5449,\"start\":5409},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5692,\"start\":5676},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5864,\"start\":5846},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6249,\"start\":6233},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6449,\"start\":6432},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6802,\"start\":6762},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6830,\"start\":6802},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6846,\"start\":6830},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7796,\"start\":7775},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7818,\"start\":7796},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7834,\"start\":7818},{\"end\":7869,\"start\":7834},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7898,\"start\":7869},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7936,\"start\":7900},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8251,\"start\":8228},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8440,\"start\":8423},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8700,\"start\":8665},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10070,\"start\":10038},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10128,\"start\":10111},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10785,\"start\":10762},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11335,\"start\":11316},{\"end\":11496,\"start\":11477},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11649,\"start\":11634},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11723,\"start\":11705},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14319,\"start\":14286},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17130,\"start\":17114},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17172,\"start\":17155},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17199,\"start\":17182},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17258,\"start\":17228},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17454,\"start\":17437},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17489,\"start\":17459},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17552,\"start\":17536},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17967,\"start\":17950},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18163,\"start\":18143},{\"end\":21370,\"start\":21365},{\"end\":21374,\"start\":21370},{\"end\":21378,\"start\":21374},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24197,\"start\":24168},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24269,\"start\":24234},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24990,\"start\":24956},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25110,\"start\":25077},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25485,\"start\":25468},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33631,\"start\":33612}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27575,\"start\":27471},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27748,\"start\":27576},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29504,\"start\":27749},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29684,\"start\":29505},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30706,\"start\":29685},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30941,\"start\":30707},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31388,\"start\":30942},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31643,\"start\":31389},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31860,\"start\":31644},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31951,\"start\":31861},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33413,\"start\":31952},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":35489,\"start\":33414}]", "paragraph": "[{\"end\":2135,\"start\":1332},{\"end\":2960,\"start\":2137},{\"end\":3706,\"start\":2962},{\"end\":4323,\"start\":3708},{\"end\":4882,\"start\":4325},{\"end\":6632,\"start\":4899},{\"end\":7686,\"start\":6634},{\"end\":9832,\"start\":7688},{\"end\":10711,\"start\":9845},{\"end\":11022,\"start\":10713},{\"end\":11958,\"start\":11064},{\"end\":12558,\"start\":12223},{\"end\":12869,\"start\":12577},{\"end\":13835,\"start\":12871},{\"end\":14181,\"start\":13908},{\"end\":14787,\"start\":14183},{\"end\":15617,\"start\":14827},{\"end\":16527,\"start\":15653},{\"end\":17004,\"start\":16574},{\"end\":18196,\"start\":17006},{\"end\":19387,\"start\":18226},{\"end\":20089,\"start\":19389},{\"end\":21781,\"start\":20091},{\"end\":21841,\"start\":21783},{\"end\":22482,\"start\":21843},{\"end\":22734,\"start\":22484},{\"end\":23143,\"start\":22736},{\"end\":23911,\"start\":23145},{\"end\":23931,\"start\":23913},{\"end\":24649,\"start\":23955},{\"end\":25648,\"start\":24674},{\"end\":26722,\"start\":25689},{\"end\":27470,\"start\":26737}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12207,\"start\":11959}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18434,\"start\":18427},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19243,\"start\":19236},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21512,\"start\":21505},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22306,\"start\":22299},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22851,\"start\":22844},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24758,\"start\":24751},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":25759,\"start\":25752}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1330,\"start\":1318},{\"attributes\":{\"n\":\"2\"},\"end\":4897,\"start\":4885},{\"attributes\":{\"n\":\"3\"},\"end\":9843,\"start\":9835},{\"end\":11062,\"start\":11025},{\"attributes\":{\"n\":\"4\"},\"end\":12221,\"start\":12209},{\"attributes\":{\"n\":\"4.1\"},\"end\":12575,\"start\":12561},{\"attributes\":{\"n\":\"4.2\"},\"end\":13906,\"start\":13838},{\"attributes\":{\"n\":\"4.3\"},\"end\":14825,\"start\":14790},{\"attributes\":{\"n\":\"4.4\"},\"end\":15651,\"start\":15620},{\"attributes\":{\"n\":\"5\"},\"end\":16550,\"start\":16530},{\"attributes\":{\"n\":\"5.1\"},\"end\":16572,\"start\":16553},{\"attributes\":{\"n\":\"5.2\"},\"end\":18224,\"start\":18199},{\"attributes\":{\"n\":\"6.1\"},\"end\":23953,\"start\":23934},{\"attributes\":{\"n\":\"6.2\"},\"end\":24672,\"start\":24652},{\"attributes\":{\"n\":\"7.2\"},\"end\":25687,\"start\":25651},{\"attributes\":{\"n\":\"8\"},\"end\":26735,\"start\":26725},{\"end\":27482,\"start\":27472},{\"end\":27587,\"start\":27577},{\"end\":27755,\"start\":27750},{\"end\":29695,\"start\":29686},{\"end\":30717,\"start\":30708},{\"end\":30952,\"start\":30943},{\"end\":31399,\"start\":31390},{\"end\":31654,\"start\":31645},{\"end\":31871,\"start\":31862},{\"end\":33424,\"start\":33415}]", "table": "[{\"end\":29504,\"start\":28629},{\"end\":29684,\"start\":29582},{\"end\":30706,\"start\":29803},{\"end\":30941,\"start\":30767},{\"end\":31388,\"start\":30954},{\"end\":31643,\"start\":31401},{\"end\":31860,\"start\":31656},{\"end\":31951,\"start\":31873},{\"end\":33413,\"start\":32074},{\"end\":35489,\"start\":34426}]", "figure_caption": "[{\"end\":27575,\"start\":27484},{\"end\":27748,\"start\":27589},{\"end\":28629,\"start\":27759},{\"end\":29582,\"start\":29507},{\"end\":29803,\"start\":29697},{\"end\":30767,\"start\":30719},{\"end\":32074,\"start\":31954},{\"end\":34426,\"start\":33426}]", "figure_ref": "[{\"end\":3705,\"start\":3699},{\"end\":14786,\"start\":14774},{\"end\":15616,\"start\":15607},{\"end\":16526,\"start\":16517},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20088,\"start\":20082},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23504,\"start\":23498}]", "bib_author_first_name": "[{\"end\":36029,\"start\":36028},{\"end\":36035,\"start\":36034},{\"end\":36192,\"start\":36191},{\"end\":36200,\"start\":36199},{\"end\":36208,\"start\":36207},{\"end\":36221,\"start\":36217},{\"end\":36227,\"start\":36226},{\"end\":36233,\"start\":36232},{\"end\":36456,\"start\":36455},{\"end\":36467,\"start\":36466},{\"end\":36469,\"start\":36468},{\"end\":36702,\"start\":36701},{\"end\":36716,\"start\":36715},{\"end\":36728,\"start\":36727},{\"end\":36730,\"start\":36729},{\"end\":36742,\"start\":36741},{\"end\":36750,\"start\":36749},{\"end\":36977,\"start\":36976},{\"end\":36985,\"start\":36984},{\"end\":37235,\"start\":37234},{\"end\":37241,\"start\":37240},{\"end\":37250,\"start\":37249},{\"end\":37257,\"start\":37256},{\"end\":37418,\"start\":37417},{\"end\":37424,\"start\":37423},{\"end\":37433,\"start\":37432},{\"end\":37440,\"start\":37439},{\"end\":37444,\"start\":37441},{\"end\":37451,\"start\":37450},{\"end\":37460,\"start\":37459},{\"end\":37925,\"start\":37924},{\"end\":37934,\"start\":37933},{\"end\":38092,\"start\":38091},{\"end\":38100,\"start\":38099},{\"end\":38108,\"start\":38107},{\"end\":38120,\"start\":38116},{\"end\":38124,\"start\":38123},{\"end\":38355,\"start\":38354},{\"end\":38369,\"start\":38368},{\"end\":38589,\"start\":38588},{\"end\":38603,\"start\":38602},{\"end\":38616,\"start\":38615},{\"end\":38618,\"start\":38617},{\"end\":38872,\"start\":38867},{\"end\":38881,\"start\":38880},{\"end\":38887,\"start\":38884},{\"end\":38894,\"start\":38893},{\"end\":39132,\"start\":39131},{\"end\":39138,\"start\":39137},{\"end\":39146,\"start\":39145},{\"end\":39154,\"start\":39153},{\"end\":39377,\"start\":39376},{\"end\":39388,\"start\":39387},{\"end\":39394,\"start\":39393},{\"end\":39402,\"start\":39401},{\"end\":39412,\"start\":39411},{\"end\":39422,\"start\":39421},{\"end\":39429,\"start\":39428},{\"end\":39442,\"start\":39441},{\"end\":39893,\"start\":39892},{\"end\":39905,\"start\":39904},{\"end\":40107,\"start\":40106},{\"end\":40121,\"start\":40120},{\"end\":40123,\"start\":40122},{\"end\":40133,\"start\":40132},{\"end\":40147,\"start\":40146},{\"end\":40160,\"start\":40159},{\"end\":40378,\"start\":40377},{\"end\":40385,\"start\":40384},{\"end\":40394,\"start\":40393},{\"end\":40402,\"start\":40401},{\"end\":40567,\"start\":40566},{\"end\":40578,\"start\":40577},{\"end\":40591,\"start\":40590},{\"end\":40600,\"start\":40599},{\"end\":40610,\"start\":40609},{\"end\":40818,\"start\":40817},{\"end\":40825,\"start\":40824},{\"end\":40835,\"start\":40834},{\"end\":40844,\"start\":40843},{\"end\":40846,\"start\":40845},{\"end\":40858,\"start\":40852},{\"end\":40862,\"start\":40861},{\"end\":41074,\"start\":41073},{\"end\":41082,\"start\":41081},{\"end\":41097,\"start\":41096},{\"end\":41349,\"start\":41348},{\"end\":41356,\"start\":41355},{\"end\":41364,\"start\":41363},{\"end\":41376,\"start\":41372},{\"end\":41385,\"start\":41384},{\"end\":41395,\"start\":41392},{\"end\":41399,\"start\":41398},{\"end\":41680,\"start\":41679},{\"end\":41688,\"start\":41687},{\"end\":41697,\"start\":41696},{\"end\":41952,\"start\":41951},{\"end\":41959,\"start\":41958},{\"end\":41967,\"start\":41966},{\"end\":41974,\"start\":41973},{\"end\":41982,\"start\":41981},{\"end\":42177,\"start\":42176},{\"end\":42184,\"start\":42183},{\"end\":42196,\"start\":42195},{\"end\":42206,\"start\":42205},{\"end\":42212,\"start\":42211},{\"end\":42382,\"start\":42381},{\"end\":42395,\"start\":42394},{\"end\":42564,\"start\":42563},{\"end\":42566,\"start\":42565},{\"end\":42581,\"start\":42575},{\"end\":42585,\"start\":42584},{\"end\":42785,\"start\":42784},{\"end\":42794,\"start\":42793},{\"end\":42804,\"start\":42803},{\"end\":42813,\"start\":42812},{\"end\":42822,\"start\":42821},{\"end\":43028,\"start\":43027},{\"end\":43037,\"start\":43036},{\"end\":43045,\"start\":43044},{\"end\":43053,\"start\":43052},{\"end\":43061,\"start\":43060},{\"end\":43069,\"start\":43068},{\"end\":43073,\"start\":43070},{\"end\":43081,\"start\":43080},{\"end\":43089,\"start\":43088},{\"end\":43091,\"start\":43090},{\"end\":43552,\"start\":43551},{\"end\":43561,\"start\":43560},{\"end\":43573,\"start\":43569},{\"end\":43577,\"start\":43576},{\"end\":43795,\"start\":43794},{\"end\":43804,\"start\":43803},{\"end\":43813,\"start\":43812},{\"end\":43820,\"start\":43819}]", "bib_author_last_name": "[{\"end\":36032,\"start\":36030},{\"end\":36040,\"start\":36036},{\"end\":36197,\"start\":36193},{\"end\":36205,\"start\":36201},{\"end\":36215,\"start\":36209},{\"end\":36224,\"start\":36222},{\"end\":36230,\"start\":36228},{\"end\":36241,\"start\":36234},{\"end\":36464,\"start\":36457},{\"end\":36476,\"start\":36470},{\"end\":36713,\"start\":36703},{\"end\":36725,\"start\":36717},{\"end\":36739,\"start\":36731},{\"end\":36747,\"start\":36743},{\"end\":36760,\"start\":36751},{\"end\":36982,\"start\":36978},{\"end\":36993,\"start\":36986},{\"end\":37238,\"start\":37236},{\"end\":37247,\"start\":37242},{\"end\":37254,\"start\":37251},{\"end\":37261,\"start\":37258},{\"end\":37421,\"start\":37419},{\"end\":37430,\"start\":37425},{\"end\":37437,\"start\":37434},{\"end\":37448,\"start\":37445},{\"end\":37457,\"start\":37452},{\"end\":37466,\"start\":37461},{\"end\":37931,\"start\":37926},{\"end\":37942,\"start\":37935},{\"end\":38097,\"start\":38093},{\"end\":38105,\"start\":38101},{\"end\":38114,\"start\":38109},{\"end\":38366,\"start\":38356},{\"end\":38376,\"start\":38370},{\"end\":38600,\"start\":38590},{\"end\":38613,\"start\":38604},{\"end\":38625,\"start\":38619},{\"end\":38878,\"start\":38873},{\"end\":38891,\"start\":38888},{\"end\":39135,\"start\":39133},{\"end\":39143,\"start\":39139},{\"end\":39151,\"start\":39147},{\"end\":39159,\"start\":39155},{\"end\":39385,\"start\":39378},{\"end\":39391,\"start\":39389},{\"end\":39399,\"start\":39395},{\"end\":39409,\"start\":39403},{\"end\":39419,\"start\":39413},{\"end\":39426,\"start\":39423},{\"end\":39439,\"start\":39430},{\"end\":39449,\"start\":39443},{\"end\":39902,\"start\":39894},{\"end\":39915,\"start\":39906},{\"end\":40118,\"start\":40108},{\"end\":40130,\"start\":40124},{\"end\":40144,\"start\":40134},{\"end\":40157,\"start\":40148},{\"end\":40174,\"start\":40161},{\"end\":40382,\"start\":40379},{\"end\":40391,\"start\":40386},{\"end\":40399,\"start\":40395},{\"end\":40407,\"start\":40403},{\"end\":40575,\"start\":40568},{\"end\":40588,\"start\":40579},{\"end\":40597,\"start\":40592},{\"end\":40607,\"start\":40601},{\"end\":40616,\"start\":40611},{\"end\":40822,\"start\":40819},{\"end\":40832,\"start\":40826},{\"end\":40841,\"start\":40836},{\"end\":40850,\"start\":40847},{\"end\":41079,\"start\":41075},{\"end\":41094,\"start\":41083},{\"end\":41103,\"start\":41098},{\"end\":41353,\"start\":41350},{\"end\":41361,\"start\":41357},{\"end\":41370,\"start\":41365},{\"end\":41382,\"start\":41377},{\"end\":41390,\"start\":41386},{\"end\":41685,\"start\":41681},{\"end\":41694,\"start\":41689},{\"end\":41706,\"start\":41698},{\"end\":41956,\"start\":41953},{\"end\":41964,\"start\":41960},{\"end\":41971,\"start\":41968},{\"end\":41979,\"start\":41975},{\"end\":41987,\"start\":41983},{\"end\":42181,\"start\":42178},{\"end\":42193,\"start\":42185},{\"end\":42203,\"start\":42197},{\"end\":42209,\"start\":42207},{\"end\":42215,\"start\":42213},{\"end\":42392,\"start\":42383},{\"end\":42405,\"start\":42396},{\"end\":42573,\"start\":42567},{\"end\":42791,\"start\":42786},{\"end\":42801,\"start\":42795},{\"end\":42810,\"start\":42805},{\"end\":42819,\"start\":42814},{\"end\":42830,\"start\":42823},{\"end\":43034,\"start\":43029},{\"end\":43042,\"start\":43038},{\"end\":43050,\"start\":43046},{\"end\":43058,\"start\":43054},{\"end\":43066,\"start\":43062},{\"end\":43078,\"start\":43074},{\"end\":43086,\"start\":43082},{\"end\":43101,\"start\":43092},{\"end\":43558,\"start\":43553},{\"end\":43567,\"start\":43562},{\"end\":43801,\"start\":43796},{\"end\":43810,\"start\":43805},{\"end\":43817,\"start\":43814},{\"end\":43823,\"start\":43821}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36136,\"start\":35976},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":57246310},\"end\":36383,\"start\":36138},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b2\"},\"end\":36649,\"start\":36385},{\"attributes\":{\"id\":\"b3\"},\"end\":36904,\"start\":36651},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1633753},\"end\":37186,\"start\":36906},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206594692},\"end\":37370,\"start\":37188},{\"attributes\":{\"doi\":\"arXiv:1703.07737\",\"id\":\"b6\",\"matched_paper_id\":6447277},\"end\":37828,\"start\":37372},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5808102},\"end\":38087,\"start\":37830},{\"attributes\":{\"doi\":\"arXiv:1707.07103\",\"id\":\"b8\"},\"end\":38297,\"start\":38089},{\"attributes\":{\"id\":\"b9\"},\"end\":38521,\"start\":38299},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195908774},\"end\":38761,\"start\":38523},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":976598},\"end\":39054,\"start\":38763},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":938105},\"end\":39297,\"start\":39056},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5584770},\"end\":39822,\"start\":39299},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14124313},\"end\":40037,\"start\":39824},{\"attributes\":{\"id\":\"b15\"},\"end\":40342,\"start\":40039},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1322267},\"end\":40505,\"start\":40344},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206593880},\"end\":40762,\"start\":40507},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2936324},\"end\":40997,\"start\":40764},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5782450},\"end\":41240,\"start\":40999},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6793190},\"end\":41594,\"start\":41242},{\"attributes\":{\"doi\":\"arXiv:1708.07747\",\"id\":\"b21\"},\"end\":41899,\"start\":41596},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10842458},\"end\":42112,\"start\":41901},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8485068},\"end\":42355,\"start\":42114},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15276198},\"end\":42484,\"start\":42357},{\"attributes\":{\"id\":\"b25\"},\"end\":42718,\"start\":42486},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6212000},\"end\":42977,\"start\":42720},{\"attributes\":{\"doi\":\"arXiv:1610.02984\",\"id\":\"b27\",\"matched_paper_id\":14991802},\"end\":43458,\"start\":42979},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2683207},\"end\":43728,\"start\":43460},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206595765},\"end\":43951,\"start\":43730}]", "bib_title": "[{\"end\":36189,\"start\":36138},{\"end\":36974,\"start\":36906},{\"end\":37232,\"start\":37188},{\"end\":37415,\"start\":37372},{\"end\":37922,\"start\":37830},{\"end\":38586,\"start\":38523},{\"end\":38865,\"start\":38763},{\"end\":39129,\"start\":39056},{\"end\":39374,\"start\":39299},{\"end\":39890,\"start\":39824},{\"end\":40375,\"start\":40344},{\"end\":40564,\"start\":40507},{\"end\":40815,\"start\":40764},{\"end\":41071,\"start\":40999},{\"end\":41346,\"start\":41242},{\"end\":41949,\"start\":41901},{\"end\":42174,\"start\":42114},{\"end\":42379,\"start\":42357},{\"end\":42782,\"start\":42720},{\"end\":43025,\"start\":42979},{\"end\":43549,\"start\":43460},{\"end\":43792,\"start\":43730}]", "bib_author": "[{\"end\":36034,\"start\":36028},{\"end\":36042,\"start\":36034},{\"end\":36199,\"start\":36191},{\"end\":36207,\"start\":36199},{\"end\":36217,\"start\":36207},{\"end\":36226,\"start\":36217},{\"end\":36232,\"start\":36226},{\"end\":36243,\"start\":36232},{\"end\":36466,\"start\":36455},{\"end\":36478,\"start\":36466},{\"end\":36715,\"start\":36701},{\"end\":36727,\"start\":36715},{\"end\":36741,\"start\":36727},{\"end\":36749,\"start\":36741},{\"end\":36762,\"start\":36749},{\"end\":36984,\"start\":36976},{\"end\":36995,\"start\":36984},{\"end\":37240,\"start\":37234},{\"end\":37249,\"start\":37240},{\"end\":37256,\"start\":37249},{\"end\":37263,\"start\":37256},{\"end\":37423,\"start\":37417},{\"end\":37432,\"start\":37423},{\"end\":37439,\"start\":37432},{\"end\":37450,\"start\":37439},{\"end\":37459,\"start\":37450},{\"end\":37468,\"start\":37459},{\"end\":37933,\"start\":37924},{\"end\":37944,\"start\":37933},{\"end\":38099,\"start\":38091},{\"end\":38107,\"start\":38099},{\"end\":38116,\"start\":38107},{\"end\":38123,\"start\":38116},{\"end\":38127,\"start\":38123},{\"end\":38368,\"start\":38354},{\"end\":38378,\"start\":38368},{\"end\":38602,\"start\":38588},{\"end\":38615,\"start\":38602},{\"end\":38627,\"start\":38615},{\"end\":38880,\"start\":38867},{\"end\":38884,\"start\":38880},{\"end\":38893,\"start\":38884},{\"end\":38897,\"start\":38893},{\"end\":39137,\"start\":39131},{\"end\":39145,\"start\":39137},{\"end\":39153,\"start\":39145},{\"end\":39161,\"start\":39153},{\"end\":39387,\"start\":39376},{\"end\":39393,\"start\":39387},{\"end\":39401,\"start\":39393},{\"end\":39411,\"start\":39401},{\"end\":39421,\"start\":39411},{\"end\":39428,\"start\":39421},{\"end\":39441,\"start\":39428},{\"end\":39451,\"start\":39441},{\"end\":39904,\"start\":39892},{\"end\":39917,\"start\":39904},{\"end\":40120,\"start\":40106},{\"end\":40132,\"start\":40120},{\"end\":40146,\"start\":40132},{\"end\":40159,\"start\":40146},{\"end\":40176,\"start\":40159},{\"end\":40384,\"start\":40377},{\"end\":40393,\"start\":40384},{\"end\":40401,\"start\":40393},{\"end\":40409,\"start\":40401},{\"end\":40577,\"start\":40566},{\"end\":40590,\"start\":40577},{\"end\":40599,\"start\":40590},{\"end\":40609,\"start\":40599},{\"end\":40618,\"start\":40609},{\"end\":40824,\"start\":40817},{\"end\":40834,\"start\":40824},{\"end\":40843,\"start\":40834},{\"end\":40852,\"start\":40843},{\"end\":40861,\"start\":40852},{\"end\":40865,\"start\":40861},{\"end\":41081,\"start\":41073},{\"end\":41096,\"start\":41081},{\"end\":41105,\"start\":41096},{\"end\":41355,\"start\":41348},{\"end\":41363,\"start\":41355},{\"end\":41372,\"start\":41363},{\"end\":41384,\"start\":41372},{\"end\":41392,\"start\":41384},{\"end\":41398,\"start\":41392},{\"end\":41402,\"start\":41398},{\"end\":41687,\"start\":41679},{\"end\":41696,\"start\":41687},{\"end\":41708,\"start\":41696},{\"end\":41958,\"start\":41951},{\"end\":41966,\"start\":41958},{\"end\":41973,\"start\":41966},{\"end\":41981,\"start\":41973},{\"end\":41989,\"start\":41981},{\"end\":42183,\"start\":42176},{\"end\":42195,\"start\":42183},{\"end\":42205,\"start\":42195},{\"end\":42211,\"start\":42205},{\"end\":42217,\"start\":42211},{\"end\":42394,\"start\":42381},{\"end\":42407,\"start\":42394},{\"end\":42575,\"start\":42563},{\"end\":42584,\"start\":42575},{\"end\":42588,\"start\":42584},{\"end\":42793,\"start\":42784},{\"end\":42803,\"start\":42793},{\"end\":42812,\"start\":42803},{\"end\":42821,\"start\":42812},{\"end\":42832,\"start\":42821},{\"end\":43036,\"start\":43027},{\"end\":43044,\"start\":43036},{\"end\":43052,\"start\":43044},{\"end\":43060,\"start\":43052},{\"end\":43068,\"start\":43060},{\"end\":43080,\"start\":43068},{\"end\":43088,\"start\":43080},{\"end\":43103,\"start\":43088},{\"end\":43560,\"start\":43551},{\"end\":43569,\"start\":43560},{\"end\":43576,\"start\":43569},{\"end\":43580,\"start\":43576},{\"end\":43803,\"start\":43794},{\"end\":43812,\"start\":43803},{\"end\":43819,\"start\":43812},{\"end\":43825,\"start\":43819}]", "bib_venue": "[{\"end\":36026,\"start\":35976},{\"end\":36247,\"start\":36243},{\"end\":36453,\"start\":36385},{\"end\":36699,\"start\":36651},{\"end\":37030,\"start\":36995},{\"end\":37267,\"start\":37263},{\"end\":37540,\"start\":37484},{\"end\":37948,\"start\":37944},{\"end\":38352,\"start\":38299},{\"end\":38631,\"start\":38627},{\"end\":38901,\"start\":38897},{\"end\":39165,\"start\":39161},{\"end\":39456,\"start\":39451},{\"end\":39921,\"start\":39917},{\"end\":40104,\"start\":40039},{\"end\":40413,\"start\":40409},{\"end\":40622,\"start\":40618},{\"end\":40869,\"start\":40865},{\"end\":41109,\"start\":41105},{\"end\":41406,\"start\":41402},{\"end\":41677,\"start\":41596},{\"end\":41993,\"start\":41989},{\"end\":42221,\"start\":42217},{\"end\":42411,\"start\":42407},{\"end\":42561,\"start\":42486},{\"end\":42836,\"start\":42832},{\"end\":43169,\"start\":43119},{\"end\":43584,\"start\":43580},{\"end\":43829,\"start\":43825}]"}}}, "year": 2023, "month": 12, "day": 17}