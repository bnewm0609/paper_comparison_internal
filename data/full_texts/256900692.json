{"id": 256900692, "updated": "2023-10-05 04:22:05.36", "metadata": {"title": "Aligning Language Models with Preferences through f-divergence Minimization", "authors": "[{\"first\":\"Dongyoung\",\"last\":\"Go\",\"middle\":[]},{\"first\":\"Tomasz\",\"last\":\"Korbak\",\"middle\":[]},{\"first\":\"Germ'an\",\"last\":\"Kruszewski\",\"middle\":[]},{\"first\":\"Jos\",\"last\":\"Rozen\",\"middle\":[]},{\"first\":\"Nahyeon\",\"last\":\"Ryu\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Dymetman\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that different divergences present different alignment and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good balance between these objectives, and frequently outperforms forward KL divergence by a wide margin, leading to significant improvements over prior work. These distinguishing characteristics between divergences persist as the model size increases, highlighting the importance of selecting appropriate divergence objectives.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2302.08215", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/GoKKRRD23", "doi": "10.48550/arxiv.2302.08215"}}, "content": {"source": {"pdf_hash": "e8e035f9768a4d4e7fe9a2e167cd93d170407b1b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.08215v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "92a95566d8417f7dcb04a53fb4b177279977030d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e8e035f9768a4d4e7fe9a2e167cd93d170407b1b.txt", "contents": "\nAligning Language Models with Preferences through f -divergence Minimization\n\n\nDongyoung Go \nTomasz Korbak \nGerm\u00e1n Kruszewski \nJos Rozen \nNahyeon Ryu \nMarc Dymetman \nAligning Language Models with Preferences through f -divergence Minimization\n\nAligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f -DPG, which allows the use of any f -divergence to approximate any target distribution that can be evaluated. f -DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that different divergences present different alignment and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good balance between these objectives, and frequently outperforms forward KL divergence by a wide margin, leading to significant improvements over prior work. These distinguishing characteristics between divergences persist as the model size increases, highlighting the importance of selecting appropriate divergence objectives.\n\nIntroduction\n\nLanguage models (LMs) have recently revolutionized the 1 Naver Corp 2 Yonsei University 3 University of Sussex 4 Naver Labs Europe 5 Independent Researcher. Correspondence to: Dongyoung Go <dongyoung.go@navercorp.com>.  field of Natural Language Processing thanks to their generative capabilities, which are useful in a vast number of tasks (Brown et al., 2020;Srivastava et al., 2022). However, generated texts can also violate widely-held human preferences, e.g. helpfulness (Askell et al., 2021), nonoffensiveness (Gehman et al., 2020), truthfulness (Lin et al., 2022) or equal treatment (Cao et al., 2022). Aligning LMs with human preferences is the problem of adapting the LM in such a way that generated content is perceived to match the human's intent (Ouyang et al., 2022) or that it is helpful, honest, and harmless (Askell et al., 2021;Bai et al., 2022b). Fundamentally, an aligned LM can be seen as a desired target distribution that we would like to generate from (Korbak et al., 2022c). Some approaches leave this distribution implicit, to be defined as a side-effect of the proposed intervention. These include prompting with natural language instructions or demonstrations (Askell et al., 2021), using scorers or safety filters while decoding (Roller et al., 2021;Xu et al., 2021), supervised fine-tuning on curated data (Solaiman & Dennison, 2021;Ngo et al., 2021;Welbl et al., 2021;Chung et al., 2022) or selected samples from the model (Zelikman et al., 2022;Scheurer et al., 2022;Dohan et al., 2022), and fine-tuning the language model using reinforcement learning with a learned reward function that approximates human feedback (Reinforcement Learning from Human Feedback or RLHF; Ziegler et al., 2019;Bai et al., 2022a;Ouyang et al., 2022). Instead, Khalifa et al. (2021) propose a framework that they name Generation with Distributional Control (GDC), where they define the target distribution p that represents the aligned LM as an EBM (Energy Based Model), namely an unnormalized version of p that can be evaluated over any input x. They then train the generative model \u03c0 \u03b8 to approximate p via methods such as Distributional Policy Gradients (DPG; Parshakova et al., 2019), which minimize the forward Kullback-Leibler (KL) divergence KL(p||\u03c0 \u03b8 ) of p to \u03c0 \u03b8 . The advantage of such an approach is that it decouples the problem of describing the aligned LM from the problem of approximating it. Furthermore, even if RL with KL penalties (Todorov, 2006a;Kappen et al., 2012;Jaques et al., 2017;2019), the method used to fine-tune a LM in RLHF, is defined only in terms of reward maximization, it has also been shown to be equivalent to minimizing the reverse KL divergence KL(\u03c0 \u03b8 ||p) of \u03c0 \u03b8 to a target distribution p that can also be written explicitly in closed-form (Korbak et al., 2022b).\n\nThe possibility of approximating various distributions according to different divergence measures begs the question: Does the choice of a divergence measure matter? In principle, all divergences lead to the same optimum, namely the target distribution p. However, when we restrict \u03c0 \u03b8 to a certain parametric family that does not include p (i.e., the search space is mis-specified), then the minimum can be found at different points, leading to optimal models with different properties. Moreover, different divergences present different loss landscapes: some might make it easier for stochastic gradient descent to find good minima. Finally, the space of possible divergence measures and forms of target distributions is a vast and largely uncharted terrain. Prior work has largely failed to decouple the form of a target distribution and the algorithm used for approximating it.\n\nHere, we introduce f -DPG, a new framework for finetuning an LM to approximate any given target EBM, by exploiting any given divergence in the f -divergences family, which includes not only the forward KL and the reverse KL cited above, but also Total Variation (TV) distance, Jensen-Shannon (JS) divergence, among others. f -DPG generalizes existing approximation techniques both DPG and RL with KL penalties algorithms, thus allowing us to investigate new ways to approximate the target distributions defined by the GDC and RLHF frameworks. In particular, we explore the approximation of various target distributions representing different alignment goals, which include imposing lexical constraints, reducing social bias with respect to gender and religion, enforcing factual consistency in summarization, and enforcing compilability of generated code. We focus our experiments on four instantiations of f -DPG, namely KL-DPG, RKL-DPG, TV-DPG and JS-DPG, whose objective is to minimize the forward KL, reverse KL, TV and JS divergences, respectively, and evaluate each experiment in terms of approximation quality as measured by all of these f -divergences. We show that we can obtain significantly improved results over the original KL-DPG algorithm (Par-shakova et al., 2019) by minimizing other f -divergences, even when the approximation quality is evaluated under the lens of the forward KL. Furthermore, we observe that while there is no single best optimization objective for all cases, JS-DPG often strikes a good balance and significantly improves upon prior work (Khalifa et al., 2021;Korbak et al., 2022a), as illustrated in Fig. 1. Lastly, we find that f-DPG with an optimal objective continues to outperform suboptimal objectives as we scale model size from 127M parameters to 1.5B parameters (Sec. 4.5). The smooth and gradual scaling trend observed with increasing model size suggests that our findings will generalize to even larger LMs.\n\nOverall, the contributions of the paper include:\n\n1. Introducing f -DPG, a unifying framework for approximating any EBM target distribution by minimizing any f -divergence (Sec. 3.2), and deriving a universal formula for gradient descent with f -divergences (Theorem 1).\n\n2. Extending f -DPG to include baselines for variance reduction (Fact 1); and handling conditional target distributions (Fact 2).\n\n3. Investigating the performance of f -DPG on a diverse array of thirteen LM alignment tasks, three forms of target distributions, four f -divergence objectives and eight metrics.\n\n\nBackground\n\nWe can organize approaches to LM alignment along two axes: how the target distribution is constructed and how it is approximated. The first problem roughly corresponds to representing human preferences through the specification of a probability distribution and the second to allowing the production of samples from that distribution.\n\n\nDefining a Target Distribution\n\nThe target distribution expresses an ideal notion of an LM, incorporating human preferences, as probabilities p(x) over texts x according to how well they satisfy the preferences. Formally, p(x) is often defined through a non-negative function P (x) (aka an energy-based model or EBM (LeCun et al., 2006)) such that p(x) \u221d P (x). The model P (x) (and p(x) after normalization) can be used to score samples, but not to directly produce them because it lacks an autoregressive form. In the rest of the paper, we will focus on target distributions modeling three types of preferences prominently employed in recent literature about GDC (Khalifa et al., 2021) and RLHF (Ziegler et al., 2019;Stiennon et al., 2020;Ouyang et al., 2022;Menick et al., 2022;Bai et al., 2022a).\n\nBinary preferences For human preferences naturally expressible as a binary constraint b(x) \u2208 {0, 1} (e.g. a sample\n\nx must never contain a curse word), Khalifa et al. (2021) proposed the following target distribution:\np GDC bin (x) \u221d a(x)b(x),(1)\nwhere a is a pretrained LM and b(x) = 0 if x contains a curse and b(x) = 1 otherwise. p GDC bin is the distribution enforcing that all samples match the binary constraint, which deviates minimally from a as measured by KL(p GDC bin ||a).\n\nScalar preferences Some human preferences, such as helpfulness, are more naturally expressed as scalar scores. Alignment with respect to these is typically addressed with RLHF (Stiennon et al., 2020;Ziegler et al., 2019;Ouyang et al., 2022), which consists of, first, capturing human preferences as a reward function r(x) (e.g. scores given a reward model trained to predict human preferences) and second, applying RL with KL penalties (Todorov, 2006a;Kappen et al., 2012;Jaques et al., 2017;2019) to maximize this reward while penalizing departure from a(x):\nJ RLKL (\u03b8) = E x\u223c\u03c0 \u03b8 r(x) \u2212 \u03b2 log \u03c0 \u03b8 (x) a(x) .(2)\nThis objective can be equivalently framed as minimizing the reverse KL, KL(\u03c0 \u03b8 ||p RLKL ), where the target distribution p RLKL is defined as:\np RLKL (x) \u221d a(x) exp(r(x)/\u03b2),(3)\nwhere \u03b2 is a hyperparameter (Korbak et al., 2022b).\n\nDistributional preferences Finally, there is a class of distributional preferences (Weidinger et al., 2021) that cannot be expressed as a function of a single sample x but depend on the entire distribution, e.g. a particular gender distribution of persons mentioned in LM samples. Khalifa et al. (2021) model such preferences through distributional constraints using the following exponential family target distribution\np GDC dist (x) \u221d a(x) exp i \u03bb i \u03c6 i (x) ,(4)\nwhere \u03c6 i are features defined over texts (e.g. the most frequent gender of people mentioned in x) and \u03bb i are coefficients chosen so that the expected values E x\u223cp [\u03c6 i (x)] match some desired values\u03bc i (e.g., 50% gender balance). The resulting distribution p GDC-d matches the target feature moments, while deviating minimally from a as measured by KL(p GDC dist ||a).\n\n\nApproximating the target distribution\n\nDrawing samples from a target distribution p constitutes the inference problem. There are broadly two approaches to this problem: (i) augmenting decoding from a at inference time to obtain samples from p and (ii) training a new parametric model \u03c0 \u03b8 to approximate p which can then be sampled from directly. The first family of approaches includes guided decoding methods (Dathathri et al., 2020;Qin et al., 2022), Monte Carlo sampling techniques such as rejection sampling to sample from simple distributions like p GDC bin (Roller et al., 2021;Ziegler et al., 2022), and Quasi Rejection Sampling (QRS) (Eikema et al., 2022) or MCMC techniques (Miao et al., 2019Goyal et al., 2022) to sample from more complex distributions, such as p GDC dist .\n\nIn the rest of the paper, we will focus on the second family: methods that train a new model \u03c0 \u03b8 to approximate p by minimizing a divergence measure from p, D(\u03c0 \u03b8 ||p). \n\u2207 \u03b8 CE(p, \u03c0 \u03b8 ) = \u2212E x\u223c\u03c0 \u03b8 p(x) \u03c0 \u03b8 (x) \u2207 \u03b8 log \u03c0 \u03b8 (x). (5)\n\nFormal Aspects\n\nIn this section, we describe the f -divergence family, and introduce a generic technique, f -DPG, for minimizing the f -divergence between a target distribution p and a model \u03c0 \u03b8 . We then describe the application of f -DPG to aligning language models with human preferences.\n\n\nf -divergences\n\nConsider a convex function f :\n(0, \u221e) \u2192 R with f (1) = 0. Let f (0) .\n= lim t\u21920 f (t) and f (\u221e) . = lim t\u21920 tf ( 1 t ). 1 Let p 1 , p 2 be two distributions over a discrete set X . The fdivergence between p 1 and p 2 can be defined as\nD f (p 1 ||p 2 ) . = E x\u223cp2 f p 1 (x) p 2 (x) + f (\u221e) p 1 (p 2 = 0)(6)\nwhere p 1 (p 2 = 0) is the p 1 -mass of the set {x \u2208 X : p 2 (x) = 0} (Polyanskiy, 2019;Liese & Vajda, 2006). The function f is called a generator of D f . By convention, if p 1 (p 2 = 0) = 0, the last term of Eq. (6) is set to 0 regardless of the value of f (\u221e) (which can be infinite). 2 It can be 1 The limits are well-defined and take values in (\u2212\u221e, \u221e].\n\nThe convention for f (\u221e) is motivated by the fact that limt\u2192\u221e f (t) = limt\u21920 tf ( 1 t ) (Hiriart-Urruty & Lemar\u00e9chal, 2013). 2 Based on the commonly made assumption that the support of p1 is dominated by the support of p2 (Supp(p1) \u2282 Supp(p2)),\nEq. (6) simplifies to D f (p1||p2) = Ex\u223cp 2 f p 1 (x) p 2 (x) .\nshown that D f (p 1 ||p 2 ) \u2265 0 for any p 1 and p 2 , with equality if p 1 = p 2 ; conversely, if D f (p 1 ||p 2 ) = 0 and f is strictly convex at 1, then p 1 = p 2 .\n\nThe f -divergence family includes many important divergence measures, in particular KL divergence KL(p 1 ||p 2 ), reverse KL divergence KL(p 2 ||p 1 ), Jensen-Shannon divergence, and Total Variation distance. We list these fdivergences and their generators in Tab \n\n\nDistributional alignment with f -divergences\n\nLet X be a discrete countable or finite set, in our case a set of texts. Given a target probability distribution p(x) over elements x \u2208 X , our goal is to approximate p with a generative model (aka policy) \u03c0 \u03b8 . On the other hand, the generative model \u03c0 \u03b8 is a parametric model, typically an autoregressive neural network, from which we can (i) directly sample and (ii) evaluate probabilities \u03c0 \u03b8 (x).\n\nWe approach this problem by attempting to minimize the f -divergence of \u03c0 \u03b8 to p: 3\nmin \u03b8\u2208\u0398 D f (\u03c0 \u03b8 ||p),(7)\nwhere \u03b8 varies inside the parametric family \u0398. Note that when the family \u03c0 \u03b8 , \u03b8 \u2208 \u0398 is \"well-specified\", i.e., when \u2203\u03b8 0 s.t. p = \u03c0 \u03b80 , the true minimum of Eq (7) is 0, attained at \u03b8 0 , whatever divergence D f is chosen. In contrast, when the family is \"mis-specified\" i.e. does not include p, the distribution \u03c0 \u03b8 with minimal divergence can be strongly dependent on the chosen divergence D f .\n\nEq. (7) might be solved approximately using stochastic optimization with samples drawn from the distribution p, as the definition of D f (\u03c0 \u03b8 ||p) involves taking the expectation with respect to p. However, it is often not possible to sample directly from p, while it is possible to sample from \u03c0 \u03b8 . Our optimization technique is then based on the following core result, which we prove in App. A.3.\n\nTheorem 1. Let p and \u03c0 \u03b8 be distributions over a discrete set X such that at least one of the following conditions holds:\n(i) \u2200\u03b8 \u2208 \u0398, Supp(p) \u2282 Supp(\u03c0 \u03b8 ), or (ii) Supp(\u03c0 \u03b8 )\ndoes not depend on \u03b8. Then: 3 We could have chosen to do min \u03b8\u2208\u0398 D f (p||\u03c0 \u03b8 ). However the perspective transform f * (t)\n\u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 f \u03c0 \u03b8 (x) p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) .(8)\n. = t f ( 1 t ) allows interchangeability of arguments: D f (\u03c0 \u03b8 ||p) = D f * (p||\u03c0 \u03b8 ), making either form possible. The form in Eq. (7) permits a simpler statement of our main theorem. See App. A.1, A.3 for details.\n\nNote that it may happen in Eq 8 that p(x) = 0 and \u03c0 \u03b8 (x) > 0, hence \u03c0 \u03b8 (x) p(x) = \u221e, in which case the expression f \u03c0 \u03b8 (x)\np(x)\nshould be understood as denoting the value f (\u221e) as defined earlier. 4 In the context of LMs, our domain of application, we will use Thm. 1 in situations where \u03c0 \u03b8 , being a standard softmaxbased autoregressive model, has full support over X (i.e. Supp(\u03c0 \u03b8 ) = X ) for all \u03b8's, while the support of p might be strictly included in X in some experiments (Sec. 4.2,4.4).\n\nIt is instructive to consider Thm. 1 in relation to rewards in RL. In the standard policy gradient algorithm (Williams, 1992), to find the model that maximizes the average reward E x\u223c\u03c0 \u03b8 [r(x)], one computes the gradient of the loss using the formula\n\u2207 \u03b8 E x\u223c\u03c0 \u03b8 [r(x)] = E x\u223c\u03c0 \u03b8 [r(x)\u2207 \u03b8 log \u03c0 \u03b8 (x)].\nThe gradient in Eq. 8 is very similar, with a \"pseudo-reward\"\nr \u03b8 (x) = \u2212f ( \u03c0 \u03b8 (x)\np(x) ), one difference being that now r \u03b8 depends on \u03b8 (see (Korbak et al., 2022b) for related remarks). We refer to the approach in Eq. 8 under the name f -DPG, in reference to the original DPG (Distributional Policy Gradient) approach introduced in (Parshakova et al., 2019), which can be seen as a special case of f -DPG (\"KL-DPG\") with D f (\u03c0 \u03b8 ||p) set to KL(p||\u03c0 \u03b8 ) as discussed in Sec. 3.4.\n\n\nAdding a baseline\n\nBased on the similarity to policy gradients, we adopt the widely used baseline technique from RL, as previously studied in Williams (1992); Baxter & Bartlett (2001);Schulman et al. (2016) and in the context of DPG in (Korbak et al., 2022b). This technique involves subtracting a constant B from the reward term, and does not introduce bias in the estimate of the gradient at a given \u03b8. In our case, with r \u03b8 (x)\n\n.\n= \u2212f ( \u03c0 \u03b8 (x) p(x) ), we can write \u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 r \u03b8 (x)\u2207 \u03b8 log \u03c0 \u03b8 (x) = E x\u223c\u03c0 \u03b8 (r \u03b8 (x) \u2212 B) \u2207 \u03b8 log \u03c0 \u03b8 (x)\n, based on the observation that E x\u223c\u03c0 \u03b8 \u2207 \u03b8 log \u03c0 \u03b8 (x) = 0 (see also App. A.6).\nFact 1. Subtracting B from r \u03b8 (x) does not introduce bias into f -DPG gradient estimates.\nTypically, B is chosen to be the average of the rewards, B\n\n.\n= E x\u223c\u03c0 \u03b8 [r \u03b8 (x)].\nIn the experiments of Sec. 4, we use the baseline technique where B is an estimate of the average of pseudo-rewards, unless otherwise specified. 4 The derivative f (t) of any convex function f (t) is defined almost everywhere, with the possible exception of a countable number of non-differentiable points, at which a subgradient can be used instead (Hiriart-Urruty & Lemar\u00e9chal, 2013;Rockafellar, 1970). See also App. A.4.\nD f (\u03c0 \u03b8 ||p) f f f \u03c0 \u03b8 (x) p(x) f (\u221e) Forward KL (KL(p||\u03c0 \u03b8 )) f (t) = \u2212 log t f (t) = \u2212 1 t \u2212 p(x) \u03c0 \u03b8 (x) 0 Reverse KL (KL(\u03c0 \u03b8 ||p)) f (t) = t log t f (t) = log t + 1 \u2212 log p(x) \u03c0 \u03b8 (x) + 1 \u221e Total Variation (TV(\u03c0 \u03b8 ||p)) f (t) = 0.5 |1 \u2212 t| f (t) = 0.5 for t > 1 \u22120.5 for t < 1 0.5 for \u03c0 \u03b8 (x) p(x) > 1 \u22120.5 for \u03c0 \u03b8 (x) p(x) < 1 0.5 Jensen-Shannon (JS(\u03c0 \u03b8 ||p)) f (t) = t log 2t t+1 + log 2 t+1 f (t) = log 2t t+1 log 2 \u2212 log 1 + p(x) \u03c0 \u03b8 (x)\nlog 2 Table 1. Some common f -divergences D f (\u03c0 \u03b8 ||p). In the convention of this table, the f shown corresponds to the order of arguments D f (\u03c0 \u03b8 ||p). Thus the forward KL between the target p and the model, KL(p||\u03c0 \u03b8 ), corresponds to D \u2212log t (\u03c0 \u03b8 ||p), and similarly for the reverse KL, KL(\u03c0 \u03b8 ||p), which corresponds to D t log t (\u03c0 \u03b8 ||p), etc. Note that for symmetric divergences (TV and JS) the order of arguments is indifferent: TV(\u03c0 \u03b8 ||p) = TV(p||\u03c0 \u03b8 ), JS(\u03c0 \u03b8 ||p) = JS(p||\u03c0 \u03b8 ).\n\n\nRecovering Some Existing Methods\n\nVarious existing methods for aligning LM with preferences can be included in the f -DPG framework.\n\nGDC In GDC, fitting the policy \u03c0 \u03b8 to the target p (which is given by either one of Eq. 1 or Eq. 4) is done using DPG (Parshakova et al., 2019), namely by minimizing the forward KL, KL(p||\u03c0 \u03b8 ). In the f -DPG framework,\nKL(p||\u03c0 \u03b8 ) = D f (\u03c0 \u03b8 ||p) with f (t) = \u2212 log t, f (t) = \u22121/t,\nand Thm. 1 leads to the formula:\n\u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 \u2212 p(x) \u03c0 \u03b8 (x) \u2207 \u03b8 log \u03c0 \u03b8 (x),\nwhich is equivalent to Eq. 5.\n\nRL with KL penalties Let's rewrite the target distribution of Eq. (3) as p(x) . = p RLKL (x) = 1/Z a(x) e r(x)/\u03b2 , where Z is a normaliser. Then KL(\u03c0 \u03b8 ||p) = D f (\u03c0 \u03b8 ||p), with f (t) = t log t corresponding to reverse KL, and f (t) = 1 + log t. Thm. 1 implies that:\n\u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 1 + log \u03c0 \u03b8 (x) Z \u22121 a(x) exp(r(x)/\u03b2) \u2207 \u03b8 log \u03c0 \u03b8 (x) = E x\u223c\u03c0 \u03b8 \u2212 r(x) \u03b2 + log \u03c0 \u03b8 (x) a(x) \u2207 \u03b8 log \u03c0 \u03b8 (x),\nwhere we have exploited the fact that 1 + log Z is a constant, hence E x\u223c\u03c0 \u03b8 (1 + log Z) \u2207 \u03b8 log \u03c0 \u03b8 (x) = 0.\n\nUp to the constant factor \u03b2, this form recovers the usual formula for estimating the gradient of the loss defined in Eq.\n\n(\n2): \u2207 \u03b8 J RLKL (\u03b8) = E x\u223c\u03c0 \u03b8 r(x) \u2212 \u03b2 log \u03c0 \u03b8 (x) a(x)\n\u2207 \u03b8 log \u03c0 \u03b8 (x).\n\n\nEstimating Z\n\nThe target distribution p is often defined as p(x) \u221d P (x), where P (x) is a non-negative function over X . The distribution p can then be computed as p(x) = 1/Z P (x), where Z is the normalizing constant (partition function) defined by x\u2208X P (x). An estimate of Z can be obtained by importance sampling, using samples from the current \u03c0 \u03b8 , based on the identity Z = E \u03c0 \u03b8 P (x) \u03c0 \u03b8 (x) . Each such estimate is unbiased, and by averaging the estimates based on different \u03c0 \u03b8 's, one can obtain a more precise estimate of Z, exploiting all the samples obtained so far. For details about the estimate of Z, see Algorithm 1 in App. A.3, as well as the ablation study in App. H.3.\n\n\nConditional Target Distributions\n\nFor a conditional task such as machine translation, summarization or dialogue, where \u03c0 \u03b8 is defined as a conditional distribution \u03c0 \u03b8 (x|c), we adapt the conditional generalization of DPG introduced in Korbak et al. (2022a). Given a distribution over contexts \u03c4 (c) and a map from a context c to a target distribution p c , we have (see App. E for details):\n\nFact 2. f -DPG is generalized to the conditional case by optimizing the loss\nE c\u223c\u03c4 (c) [\u2207 \u03b8 D f (\u03c0 \u03b8 (\u00b7 |c)||p c (\u00b7 ))] .(9)\n\nExperiments\n\nWe study four instantiations of f -DPG, namely KL-DPG, RKL-DPG, TV-DPG and JS-DPG, corresponding to minimizing the forward KL, reverse KL, Total Variation, and Jensen-Shannon divergences, respectively. We use an exponential moving average baseline with weight \u03b1 = 0.99 for all, except for KL-DPG, where we use the analytically computed value of the pseudo-reward expectation, which amounts to 1 (Korbak et al., 2022b). We evaluate them on a diverse array of tasks including imposing sentiment constraints (Sec. 4.1), lexical constraints (Sec. 4.2), debiasing genders' prevalence and religious groups' regard (Sec. 4.3), and context-conditioned tasks, such as enforcing factual consistency in summarization (Sec. 4.4) or compilability of generated code (see App. E.1). Unless specified otherwise, we use a pretrained GPT-2 \"small\"  with 117M parameters for the initial model. Yet, we demonstrate in Sec. 4.5 that the observations continue to hold for models of larger size. Implementation details and hyper-parameters are available in App. C.\n\nMetrics We report the following key metrics. We add task-specific metrics if needed.\n\n1. D f (\u03c0 \u03b8 ||p), the f -divergence between p and \u03c0 \u03b8 , with four different f 's corresponding to forward KL, KL(p||\u03c0 \u03b8 ); reverse KL, KL(\u03c0 \u03b8 ||p); Total Variation, TV(\u03c0 \u03b8 ||p); and Jensen-Shannon, JS(\u03c0 \u03b8 ||p). We use importance sampling to estimate these divergences.\n\n2. KL(\u03c0 \u03b8 ||a), a measure of the divergence from original LM a (Ziegler et al., 2019;Khalifa et al., 2021).\n\n3. Alignment score, measured by moments E x\u223c\u03c0 \u03b8 \u03c6(x) of a feature of interest \u03c6(x). (Berger et al., 1996), a measure of diversity in probability distribution normalized by number of tokens.\n\n\nNormalized Entropy\n\n5. Standard deviation of a minibatch's pseudo-rewards, std(r \u03b8 (x)), where r \u03b8 is defined as in Sec. 3.3.\n\n\nAlignment with Scalar Preferences\n\nTask We begin with the task of maximizing a scalar preference with KL penalties, whose target distribution, p RLKL , is defined in Eq. 3. We set r(x) = log \u03c6(x) where \u03c6(x) is the probability returned by a sentiment classifier finetuned from Distil-BERT (HF Canonical Model Maintainers, 2022). This reward function is optimal for modeling a decision-maker which given k different samples x 1 , . . . , x k , will pick x i with probability proportional to \u03c6(x i ) (see Appendix F). We set \u03b2 = 0.1, which is in line with the range of values explored by Ziegler et al. (2019). Note that applying RKL-DPG on p RLKL is equivalent to the RL with KL penalties method, as described in Sec. 3.4. However, through f -DPG we can explore alternative objectives to approximate the same target.\n\nResults Fig. 2 shows the evolution of the abovementioned metrics. Further details are given in Fig. 11 in the Appendix. We observe that whereas RKL-DPG achieves by far the best performance in terms of reverse KL, KL(\u03c0 \u03b8 ||p) (top-right), it fails to minimize all other divergence metrics. This shows that minimizing one divergence does not necessarily imply that other divergences will follow. Notably, RKL-DPG yields the highest value of alignment score E \u03c0 \u03b8 [\u03c6(x)] at the cost of a significant departure from a. We connect this to the strong influence that low values p(x) have on RKL-DPG, which induces a large pseudoreward for strongly reducing \u03c0 \u03b8 (x) on those samples (see Sec 5) and produces the spike at the beginning of training in std(rewards). This can lead \u03c0 \u03b8 (x) to concentrate on highprobability regions of p(x), at the cost of diversity, which can also be seen in the low entropy of the generated samples. Interestingly, the three remaining variants of DPG (KL, TV and JS) consistently minimize all four tracked divergences, with JS-DPG performing best overall.\n\nIn App. D.1, we show additional metrics on generated sentences, which show low diversity but high quality for RKL-DPG, compared to other f -DPGs, suggesting it captures a subset of the target distribution (\"mode collapse\"), as commonly observed in other generative models (Huszar, 2015;Che et al., 2017;Mescheder et al., 2018).  Figure 2. Comparison of f -DPG on sentiment preference. Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), alignment score E\u03c0 \u03b8 [\u03c6(x)] (\u2191 better), entropy (\u2191 better), standard deviation of pseudo-reward std(r \u03b8 (x)).\n\n\nAlignment with Lexical Constraints\n\nTask In this task, we constrain the presence of a specific word in the generated text. Following Khalifa et al. (2021), we formulate this goal as a binary preference on the LM by using a target distribution p GDC bin , where b(x) = 1 iff the target word appears in the sequence x, and using a scalar preference target distribution p RLKL where r(x) is set in the same way as b(x) above. Note that in the GDC framework, p GDC bin (x) = 0 when b(x) = 0, implying that reverse KL, namely KL(\u03c0 \u03b8 ||p), becomes infinite, so RKL-DPG cannot be used (nor measured) for that target. We use four words with different occurrence frequency: \"amazing\"(1\u00b7 10 \u22123 ), \"restaurant\" (6\u00b7 10 \u22124 ), \"amusing\" (6\u00b7 10 \u22125 ), and \"Wikileaks\" (8\u00b7 10 \u22126 ).\n\n\nResults\n\nThe aggregated evolution of the metrics for both GDC and RL with KL penalties framework is presented in Fig. 3 (Fig. 1 shows a simplified view of Fig. 3 (a)). Disaggregated results for each task are presented on App. G. We see that all variants of f -DPG reduce the divergence from the target distribution across all measured f -divergences. Furthermore, as expected, convergence to the target is con-nected with the success ratio in producing the desired word, E \u03c0 \u03b8 [b(x)], while balancing it with a moderate divergence from a, KL(\u03c0 \u03b8 ||a). This reflects that approaching the optimal distribution p translates into metrics in the downstream task. Strinklingly, the original KL-DPG is outperformed by all other variants of f -DPG, even in terms of forward KL. We hypothesize that this is linked to the high variance of the pseudo-rewards in KL-DPG, as visualized in the last panel of Fig. 3 (a) and (b). In Sec. 5, we suggest an interpretation for this. We also observe that RKL-DPG tends to produce distributions with lower normalized entropy. Despite this effect, we found no significant difference in diversity among the generated sentences (see Tab  . Comparison of f -DPG aggregated on four lexical constraints. Standard deviations are suppressed for clarity. Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), alignment score E\u03c0 \u03b8 [b(x)] (\u2191 better), entropy (\u2191 better), standard deviation of pseudo-reward std(r \u03b8 (x)).\n\n\nAlignment with Distributional Constraints\n\nTask We now investigate enforcing distributional preferences on the LM. We focus on debiasing the pretrained model on two kinds of preferences, namely genders' prevalence (Khalifa et al., 2021) and regard relative to religious groups. The preferences for the genders' debiasing task are defined as \u03c6 1 (x) = 1 iff x contains more female than male pronouns, with desired moment\u03bc 1 = 0.5 and \u03c6 2 (x) = 1 iff x contains at least one of the words in the 'science' word list compiled by Dathathri et al. (2020), with desired moment \u00b5 2 = 1. For regard debiasing, we use a single distributional constraint where 0 < \u03c6(x) < 1 is a regard score of the sentence when prompted with Muslims, evaluated with a pretrained classifier (Sheng et al., 2019). We set the desired moment\u03bc = 0.568, the regard score observed Christians. The initial average regard score given Muslims is 0.385. For the first experiment, we use GPT-2 small as the initial model a, additionally fine-tuned on the WikiBio dataset (Lebret et al., 2016), whereas for the last one we use vanilla GPT-2 small.\n\n\nResults\n\nWe report the results of both experiments on Fig. 4. For the regard score rebalancing, we considerably reduce bias in the regard score for two different demographic groups, from initial regard score ratio\nE [\u03c6(x)| Christians] : E [\u03c6(x)| Muslims] = 1 : 0.677 to E [\u03c6(x)| Christians] : E [\u03c6(x)| Muslims] = 1 :\n0.801 on average. Interestingly, this task showcases a weakness of TV-DPG: Because the original distribution is already close to the target, the hard-thresholded pseudo-reward has a large variance (last panel of Fig 4(b)), inducing noisy gradient estimates and, consequently, sub-optimal convergence. Concerning the gender debiasing experiments, we can see that all other variants of f -DPG outperform the original KL-DPG explored in Khalifa et al. (2021), with RKL-DPG giving the best results and better matching the pointwise constraint although seemingly at the cost of lower diversity as measured by the entropy.\n\n\nAlignment with Conditional Constraints\n\nTask We adopt the conditional task from Korbak et al. (2022a), which aims to constrain the T5 (Raffel et al., 2020) language model to generate more factually faithful summaries (Maynez et al., 2020;Nan et al., 2021). Specifically, let NER(\u00b7) denote the set of named entities found in a text.\nThen, b(x, c) = 1 iff [NER(x) \u2286 NER(c)] \u2227 [|NER(x)| \u2265 4]\n, and 0 otherwise. Following the authors, we sample source documents from the the CNN/Daily Mail dataset (Nallapati et al., 2016), i.e. \u03c4 (c) is a uniform distribution over a given subset of source documents. In addition to the divergences, we evaluate the performance using Rouge (Lin, 2004), a measure of summarization quality in terms of unigram overlap between the source document and ground truth summary (See App. E for additional metrics and more experiments on code generation with compilability preferences).\n\n\nResults\n\nWe present the evolution of metrics in Fig. 5. The results show that f -DPG increases the fraction of consistent named entities in summarization, and interestingly, this  Figure 4. Comparison of f -DPG aggregated on distributional constraints. Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), alignment score E\u03c0 \u03b8 [\u03c6(x)] (\u2191 better), entropy (\u2191 better), standard deviation of pseudo-reward std(r \u03b8 (x)).\n\nalso leads to indirect improvement in the overall quality of generated summaries compared to ground truth, even though ground truth summaries are not used in training. As also observed in Sec. 4.2, JS-DPG leads to better convergence to p than KL-DPG as used in Korbak et al. (2022a).\n\n\nScaling Trends of f -DPG\n\nWe conduct experiments to investigate the effect of model size on our approach using the scalar preference task described in Sec. 4.1. Specifically, we gradually increase the model size from GPT-2 \"small\" (117M parameters) to \"xl\" (1.5B parameters) while tracking two important metrics: alignment score, which is measured by the expected reward E \u03c0 \u03b8 [\u03c6(x)], and diversity, which is measured by the entropy. Figure 6 demonstrates that the alignment score steadily improves as the model size increases. However, we observe persistent differences between the divergence objectives for different f-DPGs, leaving the general order between f-DPGs intact with increasing model size (See Fig. 16 in App. G for evolution of metrics through training epochs). The scaling trend of LM alignment, characterized by a gradual and predictable increase without sudden shifts in performance, aligns with previous findings in the literature (Bai et al., 2022a). Nonetheless, our study further emphasizes the importance of proper divergence objectives, as increasing model size alone does not necessarily bridge the gap between optimal and suboptimal objectives. The smooth and gradual increase of the alignment score as a function of model size suggests that our findings will generalize to even larger LMs.\n\n\nAblation Study\n\nThis section presents just the key findings of our study. Full results and detailed discussions can be found in App. H.\n\nEffect of parameter family capacity All experiments presented so far correspond to possibly mis-specified target distributions. To understand whether the observed behavior of different variants f -DPG is affected by this factor, we used pre-trained models with the same architecture as \u03c0 \u03b8 and p. We found that KL-DPG again lags considerably in terms of divergence, while presenting a high variance of in the pseudo-reward. RKL-DPG shows a significant drop of entropy in the initial phase, but with full capacity of parameter family, the model can recover, and cover the rest of the distribution. Additionally, applying zero-shot the fine-tuned LMs to a summarization task, following Radford et al. (2019), we found that the they recover to a large extent the quality of the target distribution.\n\nEffect of training scheme We examined different training schemes for the lexical constraint on \"amazing\" from Sec. 4.2. We saw that the use of a baseline technique improves the performance of the f -DPG method, with RKL-DPG showing the greatest benefit. Additionally, we found that even though a large batch size is effective at reducing the variance of KL-DPG, we still observe KL-DPG to perform comparatively worse than other divergences. Finally, we observe that our importance sampling estimates converged to the true value of Z. A plausible hypothesis would have been that each variant of f -DPG is comparatively better at least in terms of the f -divergence objective being optimized. Surprisingly, we found that, save for a few exceptions (Sec. 4.1), for a given target there is one or a few variants that are the best across all measured divergences. Furthermore, we observed that divergence measures can have a significant impact on the performance of the model depending on the target distribution. Fig. 7 summarizes the Pareto frontier of the alignmentdiversity trade-off of the f -DPG method. The results demonstrate that RKL-DPG and KL-DPG consistently represent two contrasting extremes: RKL-DPG shows high alignment but limited diversity, whereas KL-DPG exhibits low alignment but high diversity. JS-DPG shows a balanced trade-off between alignment and diversity and consistently appeared on the Pareto frontier across all experiments we conducted. Fig. 8 illustrates the differences between pseudo-rewards for distinct f -divergences, giving a plausible explanation for the observed differences. The forward KL loss aims to ensure coverage of the subset where p(x) > 0, giving a large pseudo-reward for samples with p(x)>>\u03c0(x). However, the optimization can be sensitive to sampling noise in the finite sample approximation (see, e.g., Sec. 4.2). Conversely, the reverse KL loss results in extreme negative rewards for samples with p(x)<<\u03c0 \u03b8 (x), leading \u03c0 \u03b8 to avoid such regions and resulting in distributional collapse (Sec. 4.1). Total Variation loss is robust to outliers thanks to its hardthresholded pseudo-reward, however it can lead to high variance behavior when \u03c0 \u03b8 \u2248 p (Sec. 4.3). On the other hand, the Jensen-Shannon loss gives smooth and robust rewards in both directions and prevents \u03c0 \u03b8 from heavily relying on a single direction, making it a reasonable default choice as confirmed by our experiments.\n\n\nDiscussion and Conclusion\n\nTo conclude, we propose a flexible framework for approximating a target distribution by minimizing any f -divergence, unifying earlier approaches for aligning language models. Our results on a diverse array of tasks show that minimizing well-chosenf -divergences leads to significant gains over previous work. The fact that increasing the model size improves the alignment score but does not inherently bridge the gap between objectives underscores the importance of selecting appropriate divergence objectives.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback.   \n\n\nA. Complements on Formal Aspects and Proofs\n\n\nA.1. Equivalent definitions for f -divergences\n\nThe definition of f -divergences of Eq. 6 is equivalent to a second definition, in a more \"symmetrical\" format, following (Liese & Vajda, 2006), which will help in some derivations, in particular in the proof of Theorem 1.\n\nDefinition (f -divergence: \"symmetrical\" format). The f -divergence D f (p||q), where p and q are distributions over a discrete set X can be defined as\nD f (p||q) . = {x: p(x)>0, q(x)>0} q(x) f ( p(x) q(x) ) + f (0) q(p = 0) + f * (0) p(q = 0),(10)\nwhere the generator function f : (0, \u221e) \u2192 R is a convex function satisfying f (1) = 0. We denote by q(p = 0) the q-mass of the set {x : p(x) = 0}, i.e. q(p = 0) = {x:p(x)=0} q(x) and similarly for p(q = 0).\n\nIn this definition, the function f * (t) is the so-called perspective transform of f defined by f * (t) = t f ( 1 t ). It can be shown to be also a convex function f * : (0, \u221e) \u2192 R with f * (1) = 0 and f * * = f . As we mentioned in the main text, we also have the following important \"swapping\" property: D f (p, q) = D f * (q, p). (2006); Polyanskiy (2019), we use the conventions: Equivalence of definitions 6 and 10 In order to prove this equivalence, after noting that f (\u221e) = f * (0), it remains to\n\n\nFollowing Liese & Vajda\nf (0) . = lim t\u21920 f (t), f * (0) = lim t\u21920 f * (t) = lim t\u21920 t f ( 1 t ),(11)0 f (0) . = 0, 0 f * (0) . = 0, including when f (0) = \u221e and f * (0) = \u221e,(12)f (\u221e) . = f * (0) = lim t\u21920 t f ( 1 t ).(13)\nshow that E x\u223cq f p(x) q(x) is equal to {x: p(x)>0, q(x)>0} q(x) f ( p(x) q(x) ) + f (0) q(p = 0). We have:\nE x\u223cq f ( p(x) q(x) ) = {x: q(x)>0} q(x) f ( p(x) q(x) ) = {x: q(x)>0, p(x)>0} q(x) f ( p(x) q(x) ) + {x: q(x)>0, p(x)=0} q(x) f (0) = {x: q(x)>0, p(x)>0} q(x) f ( p(x) q(x) ) + f (0) q(p = 0),\nwhich concludes the proof.\n\n\nA.2. Illustrations of a few f -divergences\n\nLet's now see how the notion of f -divergence can be applied to a few common cases.\n\nForward and reverse KL By the standard definition for KL divergence, we have, for KL(p||\u03c0), the \"forward KL\" from a model \u03c0 to a target p:\nKL(p||\u03c0) = E x\u223cp log p(x) \u03c0(x) if Supp(p) \u2282 Supp(\u03c0), \u221e, otherwise.(14)\nIf we take f (t) = \u2212 log t, as in Table 1, then we have f (0) = \u221e. On the other hand we see that f * (t) = t log t and f * (0) = 0. We can then write, using (10):\nD f (\u03c0||p) = {x: \u03c0(x)>0, p(x)>0} \u2212p(x) log( \u03c0(x) p(x) ) + \u221e p(\u03c0 = 0) + 0 \u03c0(p = 0) = {x: \u03c0(x)>0, p(x)>0} p(x) log( p(x) \u03c0(x) ) + \u221e p(\u03c0 = 0),\nwhere \u221e p(\u03c0 = 0) is null for Supp(p) \u2282 Supp(\u03c0) and infinite otherwise. Hence D f (\u03c0||p) = KL(p||\u03c0), the forward KL from \u03c0 to p. Now, consider the \"reverse KL\" from \u03c0 to p, namely KL(\u03c0||p). Based on the previous derivation, and with the same f (t) = \u2212 log t we can write it as KL(\u03c0||p) = D f (p||\u03c0), but using the perspective function f * (t) = t log t, we can also write it (as we actually do in Table 1) as D f * (\u03c0||p) = D t log t (\u03c0||p).\n\nTotal Variation divergence The Total Variation divergence between p and \u03c0 is standardly defined as TV(p||\u03c0) = 1 2 x\u2208X |p(x) \u2212 \u03c0(x)|. We then have TV(p||\u03c0) = TV(\u03c0||p). Let's then define f (t) = 1 2 |1 \u2212 t|. We have f (0) = 1/2, f * (t) = f (t), and f * (0) = 1/2. Then, using (10):\nD f (\u03c0||p) = {x: \u03c0(x)>0, p(x)>0} 1 2 p(x) 1 \u2212 \u03c0(x) p(x) + 1 2 p(\u03c0 = 0) + 1 2 \u03c0(p = 0) = {x: \u03c0(x)>0, p(x)>0} 1 2 |p(x) \u2212 \u03c0(x)| + 1 2 p(\u03c0 = 0) + 1 2 \u03c0(p = 0) = {x: \u03c0(x)>0, p(x)>0} 1 2 |p(x) \u2212 \u03c0(x)| + 1 2 {x: \u03c0(x)=0, p(x)>0} |p(x) \u2212 \u03c0(x)| + 1 2 {x: \u03c0(x)>0, p(x)=0} |p(x) \u2212 \u03c0(x)| = 1 2 x\u2208X |p(x) \u2212 \u03c0(x)| ,\nand therefore TV(p||\u03c0) = D f (\u03c0||p), and also TV(p||\u03c0) = TV(\u03c0||p) = D f * (p||\u03c0) = D f (p||\u03c0).\n\n\nA.3. Proof of Theorem 1\n\nWe restate the theorem here for convenience.\n\nTheorem (Theorem 1). Let p and \u03c0 \u03b8 be distributions over a discrete set X such that at least one of the following conditions holds: (i) \u2200\u03b8 \u2208 \u0398, Supp(p) \u2282 Supp(\u03c0 \u03b8 ), or (ii) Supp(\u03c0 \u03b8 ) does not depend on \u03b8. Then:\n\u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 f \u03c0 \u03b8 (x) p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) .(15)\nProof. Based on definition (10) we have:\n\u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = {x:p(x)>0,\u03c0 \u03b8 (x)>0} p(x) \u2207 \u03b8 f ( \u03c0 \u03b8 (x) p(x) ) + f (\u221e)\u2207 \u03b8 \u03c0 \u03b8 (p = 0) + f (0)\u2207 \u03b8 p(\u03c0 \u03b8 = 0) = {x:p(x)>0,\u03c0 \u03b8 (x)>0} p(x) f ( \u03c0 \u03b8 (x) p(x) ) \u2207 \u03b8 \u03c0 \u03b8 (x) p(x) + f (\u221e)\u2207 \u03b8 \u03c0 \u03b8 (p = 0) = {x:p(x)>0,\u03c0 \u03b8 (x)>0} \u03c0 \u03b8 (x)f ( \u03c0 \u03b8 (x) p(x) )\u2207 \u03b8 log \u03c0 \u03b8 (x) + f (\u221e)\u2207 \u03b8 \u03c0 \u03b8 (p = 0) = {x:p(x)>0,\u03c0 \u03b8 (x)>0} \u03c0 \u03b8 (x)f ( \u03c0 \u03b8 (x) p(x) )\u2207 \u03b8 log \u03c0 \u03b8 (x) + f (\u221e)\u2207 \u03b8 \uf8ee \uf8f0 {x:p(x)=0,\u03c0 \u03b8 (x)>0} \u03c0 \u03b8 (x) \uf8f9 \uf8fb = {x:p(x)>0,\u03c0 \u03b8 (x)>0} \u03c0 \u03b8 (x)f ( \u03c0 \u03b8 (x) p(x) )\u2207 \u03b8 log \u03c0 \u03b8 (x) + {x:p(x)=0,\u03c0 \u03b8 (x)>0} \u03c0 \u03b8 (x)f (\u221e)\u2207 \u03b8 log \u03c0 \u03b8 (x) = {x:\u03c0 \u03b8 (x)>0} \u03c0 \u03b8 (x)f ( \u03c0 \u03b8 (x) p(x) )\u2207 \u03b8 log \u03c0 \u03b8 (x) = E x\u223c\u03c0 \u03b8 f ( \u03c0 \u03b8 (x) p(x) ) \u2207 \u03b8 log \u03c0 \u03b8 (x).\nIn the first line of this derivation, we use the previously introduced notation f (\u221e) . = f * (0), employed in particular by (Polyanskiy, 2019), which is motivated by the fact that lim t\u2192\u221e f (t) = lim t\u2192\u221e 1 t f (t) = f * (0) (See (Hiriart-Urruty & Lemar\u00e9chal, 2013)). In the second line, we employ a variant of the chain-rule for derivatives of multivariate functions. We also exploit the fact that the condition (i) stating that the support of p is contained in the support of \u03c0 \u03b8 for all \u03b8 \u2208 \u0398 implies that \u2207 \u03b8 p(\u03c0 \u03b8 = 0) = \u2207 \u03b8 0 = 0, and that the condition (ii) that the support of \u03c0 \u03b8 does not depend on \u03b8 also implies that \u2207 \u03b8 p(\u03c0 \u03b8 = 0) = 0. In the fourth line, we write \u03c0 \u03b8 (p = 0) as a sum. In the sixth line, we allow the notation f ( \u03c0 \u03b8 (x) p(x) ) instead of f (\u221e) when p(x) = 0 and \u03c0 \u03b8 (x) > 0.\n\nWorking with the opposite divergence D f (p||\u03c0 \u03b8 ) In case one may prefer to work with a divergence D f (p||\u03c0 \u03b8 ) having the opposite argument order, then one can use the identity D f (p||\u03c0 \u03b8 ) = D f * (\u03c0 \u03b8 ||p) to conclude that under the exact same conditions (i) or (ii) as previously, we have:\n\u2207 \u03b8 D f (p||\u03c0 \u03b8 ) = \u2207 \u03b8 D f * (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 f * \u03c0 \u03b8 (x) p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) ,\nwhere the derivative is applied to the perspective transform of f .\n\n\nA.4. About non-differentiability of f\n\nIn practice when sampling from \u03c0 \u03b8 in Eq (8), the problem of non-differentiability can be neglected, and recourse to subgradients is typically unnecessary, even for f 's that have non-differentiability points (such as e.g. the generator f (t) = 0.5|1 \u2212 t| for the Total Variation divergence). Indeed, let T nd .\n\n= {t : f (t) is non differentiable at t}, and let\n\u0398 nd . = {\u03b8 : \u2203x \u2208 X : \u03c0 \u03b8 (x) p(x) \u2208 T nd } be the set of \u03b8's for which f \u03c0 \u03b8 (x) p(x)\nis undefined on at least one x. Then \u0398 nd \u2282 R d (with d the parameter dimension) is the countable union of countable sets, hence is countable, and therefore of null measure inside R d . This means that, almost surely over \u03b8, the RHS of Eq 8 is well-defined for all x's.\n\nA.5. f -DPG algorithm A.6. Baseline: alternative derivation\n\nThe generator function is not uniquely determined for a given f -divergence:\n\nFact 3. For generators f, g such that f (t) = g(t) + c(t \u2212 1), c \u2208 R, D f (p 1 ||p 2 ) = D g (p 1 ||p 2 ).\n\n\nAlgorithm 1 f -DPG\n\nInput: unnormalized target distribution P (\u00b7), initial model a(\u00b7), D f generator f (\u00b7) Initialize: \u03c0 \u03b8 (\u00b7) \u2190 a(\u00b7), Z \u2190 0, N \u2190 0 {initialize model \u03c0 \u03b8 , partition Z, sample size N for moving average} for each iteration do for each episode do sample x from \u03c0 \u03b8 (\u00b7)\nN \u2190 N + 1 Z \u2190 (N \u22121)Z + (P (x)/\u03c0 \u03b8 (x)) N\n{Estimate Z with historical samples, using a moving average}\np(\u00b7) \u2190 P (\u00b7)/Z \u03b8 \u2190 \u03b8 + \u03b1 (\u03b8) f \u03c0 \u03b8 (x) p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x)\n{Update \u03c0 \u03b8 according to Thm. 1} end for end for Output: \u03c0 \u03b8 We provide here an alternative way to introducing baselines, based on a change of generator.\n\nTheorem (Baseline based on change of generator). If D f (\u03c0 \u03b8 ||p) is a divergence with any generator f , and B \u2208 R, there exists a generator g with the same divergence D f (\u03c0 \u03b8 ||p) = D g (\u03c0 \u03b8 ||p) such that\n\u2207 \u03b8 D g (\u03c0 \u03b8 ||p) = E x\u223c\u03c0 \u03b8 f \u03c0 \u03b8 (x) p(x) \u2212 B \u2207 \u03b8 log \u03c0 \u03b8 (x) = \u2207 \u03b8 D f (\u03c0 \u03b8 ||p). Proof. Recall that D f (\u03c0 \u03b8 ||p) = D g (\u03c0 \u03b8 ||p) when g(x) = f (x) \u2212 B(x \u2212 1). Therefore, \u2207 \u03b8 D f (\u03c0 \u03b8 ||p) = \u2207 \u03b8 D g (\u03c0 \u03b8 ||p) with g \u03c0 \u03b8 (x) p(x) = f \u03c0 \u03b8 (x) p(x) \u2212 B.\n\nB. Extended Related Work\n\nRL for LMs There is a large reinforcement learning inspired literature about steering an autoregressive sequential model towards optimizing some global reward over the generated text. This includes REINFORCE (Williams, 1992)   Regard balancing original model = gpt2, learning rate = 5 \u00d7 10 \u22126 , maximum length = 40, batch size = 2048, total epochs=1000\n\nSummarization original mode=t5-small, learning rate = 1 \u00d7 10 \u22124 , maximum length = 128, total epochs=2000\n\nCode generation original mode=gpt-neo-125M, learning rate = 1 \u00d7 10 \u22124 , maximum length = 128, total epochs=2000 GPT2 approximation original model = lvwerra/gpt2-imdb, learning rate = 5 \u00d7 10 \u22126 maximum length = 40, total epochs=8000  (2015) proposed a generalization of Jensen-Shannon divergence that interpolates between KL and reverse KL and has Jensen-Shannon as its midpoint.\n\nThe connections between RL and divergence minimization have also been explored, with studies showing that entropy regularization in RL can be viewed as minimizing reverse KL divergence between reward-weighted trajectory and policy trajectory distributions (Kappen et al., 2013;Levine, 2018). Other studies have also explored the use of forward KL divergence in RL (Peters & Schaal, 2007;Norouzi et al., 2016). Additionally, a unified probabilistic perspective on f -divergence minimization in imitation learning has been presented for both discrete and continuous control environments (Ke et al., 2021;Ghasemipour et al., 2020). Wang et al. (2018) introduced variational inference with adaptive f -divergences and demonstrated its effectiveness in RL, with focus on continuous sample spaces. Their Proposition 4.2.1 is similar to our theorem 1. However, our result exhibits greater generality by defining D f (\u03c0 \u03b8 ||p) without requirements of absolute continuity in either direction (Polyanskiy, 2019;Liese & Vajda, 2006). We note that this generalization is crucial for LM alignment, as the case of p(x) = 0, \u03c0 \u03b8 (x) > 0 can easily occur.\n\n\nC. Implementation Details\n\nAll models were implemented using PyTorch (Paszke et al., 2019) and HuggingFace Transformers (Wolf et al., 2020) Table 3. Quality of the generated text metrics for the experiment on scalar preferences (Sec. 4.1). entropy (\u2191 better), Self-BLEU-5 (\u2193 better), Distinct-1 (\u2191 better), and Perplexity (\u2193 better).\n\n1. Distinct-n (Li et al., 2016a), a measure of text diversity in terms of the frequency of repeated n-grams within a single sample x.\n\n2. Self-BLEU-n (Zhu et al., 2018), a measure of text diversity on a distributional level across samples.\n\n3. Perplexity, a measure of text fluency with exponentiation of the negative average per-token log-probability under a language model. We use a separate model Distil-GPT-2 (Wolf et al., 2020) to calculate perplexity to avoid inflated estimates (Liu et al., 2016).\n\n\nResults\n\nTab. 3 provides additional metrics for the generated sentences and their diversity on scalar preferences. The notably low entropy and high Self-BLEU of RKL-DPG again indicate low diversity of RKL-DPG at the distributional level, whereas other f -DPGs have similar values to each other. On the other hand, in quality for individual samples as measured by the perplexity metric, RKL-DPG shows better quality, which suggests that RKL-DPG captures a subset of the target distribution, an observation that is frequently discussed in other generative models (Huszar, 2015;Che et al., 2017;Mescheder et al., 2018). We provide metrics for the generated sentences aggregated on lexical constraint in Tab. 4. We found no significant difference in diversity among the generated sentences. \nLoss E [b(x)] Self-BLEU-5\n\nE. f -DPG on Conditional Target Distributions\n\nLet C be a discrete (potentially infinite) set of conditions c. The problem of fine-tuning a pretrained model a(x|c) to satisfy a control objective (e.g. generating factually correct summaries) can be seen as a constraint satisfaction problem: finding a model p c (x) that meets the demands of the control objective but at the same time stays as close as possible to the original pretrained model a(x|c). A control objective can be defined in terms of a binary scorer b(x, c) such that b(x, c) = 1 if a sample (c, x) satisfies a constraint given by a control objective (e.g. x is factually correct with respect to c) and b(x, c) = 0 otherwise.\n\nFor each c \u2208 C, we can frame the problem of finding the unique model p c (x) such that (i) b(x, c) = 1 for all samples x \u223c p c (x), and (ii) p c (\u00b7) has minimal KL divergence from a(\u00b7|c) as an instance of the unconditional case already considered by Khalifa et al. (2021). Following our example, p c could be a distribution over factually correct summaries of c as similar as possible to a distribution over summaries which the original model a would produce for a document c. Therefore, p c can be represented as a distribution p c (x) of the following form:\np c (x) = 1/Z c a(x|c)b(x, c).\nLet P a conditional distribution over C which is defined as a function from C to the set of unconditional distributions p c over X . While P represents the target conditional model optimally reconciling distance from a(x|c) and the control objective, direct use of P for sampling is intractable for two reasons. First, P actually represents a potentially infinite collection of unconditional models of the form p c (\u00b7). Second, each of these unconditional models still cannot be easily sampled from because it does not admit an autoregressive factorization. To address this problem, Korbak et al. (2022a) instead try to find a generative model \u03c0 \u03b8 approximating p on average across contexts by minimizing the expected KL(p c ||\u03c0 \u03b8 ) or equivalently expected cross-entropy CE(p c , \u03c0 \u03b8 ) between \u03c0 \u03b8 and multiple p c 's:\nE c\u223c\u03c4 (c) [CE(p c (\u00b7), \u03c0 \u03b8 (\u00b7|c))] ,\nwith its gradient taking the following form:\nE c\u223c\u03c4 (c) [\u2207 \u03b8 CE(p c (\u00b7), \u03c0 \u03b8 (\u00b7|c))] = E c\u223c\u03c4 (c) E x\u223cpc(x) [\u2207 \u03b8 log \u03c0 \u03b8 (x|c)] = E c\u223c\u03c4 (c) E x\u223c\u03c0 \u03b8 (x|c) p c (x) \u03c0 \u03b8 (x|c) \u2207 \u03b8 log \u03c0 \u03b8 (x|c) .\nThis can be seen as a conditional extension of Eq. 5.\n\nA natural extension of this objective for f -DPG isE c\u223c\u03c4 (c) [D f (\u03c0 \u03b8 (\u00b7 |c)||p c (\u00b7 ))], an extension that includes expected KL(p c ||\u03c0 \u03b8 (\u00b7|c)). Thm. 1 implies that the gradient of this objective takes the following form:\nE c\u223c\u03c4 (c) [\u2207 \u03b8 D f (\u03c0 \u03b8 (\u00b7 |c)||p c (\u00b7 ))] = E c\u223c\u03c4 (c) E x\u223c\u03c0 \u03b8 (x|c) f \u03c0 \u03b8 (x|c) p c (x) \u2207 \u03b8 log \u03c0 \u03b8 (x|c) .\n\nE.1. Additional Conditional Preferences Experiments and Details\n\nTask Here, we also evaluate the conditional task on code generation. For that, we condition on Python function signatures in the Python150 dataset (Raychev et al., 2016) which consists of Python source code obtained from GitHub. We again split disjoint train/test sets of function signatures and set \u03c4 (c) as a uniform distribution. With given prompt c, we check compilability of a Python function definition obtained by concatenating [c, x] and trying to execute it. b(x, c) = 0 iff the Python interpreter raises an exception. For the initial model we use GPT-Neo-125, a variant of GPT-Neo (Black et al., 2021) on Hugging-face Transformers (Wolf et al., 2020).\n\n\nMetrics for summarization\n\nIn addition to the divergences, we evaluate the quality and factual consistency of generated summaries using the following metrics:\n\nAligning Language Models with Preferences through f -divergence Minimization and then learning the reward model by minimizing the loss\nloss(r) = E (x1,...,xn)\u223cD CE(h, f r ) (16) = \u2212E (x1,...,xn)\u223cD h(x 1 , ..., x n ) \u00b7 log f r (x 1 , ..., x n ) ,(17)\nThus, the optimal reward model is given by the function r such that h(x 1 , ..., x n ) = f r (x 1 , ..., x n ) as it minimizes the CE in Eq. 16. Typically, h corresponds to the preferences elicited by human annotators. However, let's make a simplifying assumption that humans make choices according to an internal scoring function \u03c6(x) so that h \u03c6 (x 1 , ... , x n ) \u223c Categorical(\u03c6(x 1 ), ..., \u03c6(x n )), or in other words,\nh \u03c6 (x 1 , ..., x n ) = 1 at index i with probability \u03c6(x i ) n j=1 \u03c6(x j )\n. Now, let's suppose we have access to \u03c6. Then, we note that if we set\nr \u03c6 (x) = log \u03c6(x), we get f r \u03c6 (x 1 , ..., x n ) = softmax(log(\u03c6(x 1 )), ..., log(\u03c6(x n ))) (18) = categorical(\u03c6(x 1 ), ..., \u03c6(x n )),(19)\nand thus, r \u03c6 is an optimal reward model for h \u03c6 . Figure 11. Evaluation of metrics in sentiment preference  The optimal model \u03c0 \u03b8 (x) can be heavily dependent on the choice of the divergence function f when the parameter family is mis-specified and doesn't include p(x). As a sanity check, in order to disentangle the capacity of parameter family and better understand the behavior of different loss functions, we use as p and \u03c0 \u03b8 two pretrained models having the same architecture. Specifically, we set \u03c0 \u03b8 as a GPT-2 with 117M parameters model fine-tuned on the IMDB dataset (Maas et al., 2011), and train it to revert the fine-tuning by setting p to the original GPT-2 model. 5\n\n\nG. Additional Figures\n\nWe present the evolution of our metrics in Fig. 17 averaged over three independent seeds. First, we observe that while TKL-DPG, TV-DPG, and JS-DPG make quick and steady progress toward the target, KL-DPG lags considerably, making slow progress in terms of forward KL, reverse KL and JS divergence, and even regressing in terms of TV distance. We link this to the high variance of the KL-DPG pseudo-reward, which might be producing high-variance gradient estimates (see Sec 5 for an interpretation of this phenomenon). More interestingly, RKL-DPG shows a significant drop of entropy in the initial phase, but still converges to the distribution of p(x). In line with the experiments in Sec. 4.1, we link the drop to the mode-seeking behaviour. However, since we are not in the mis-specified scenario, the model can recover, and cover the rest of the distribution. Finally, in Fig. 18 from App. H.2 we show that the resulting models recover to a large extent the quality of the original GPT-2 by applying it zero-shot to the summarization task, following Radford et al. (2019). Figure 17. Approximating the distribution of GPT-2. Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), Distinct-1 (\u2191 better), Self-BLEU-5 (\u2193 better), Perplexity (\u2193 better), entropy (\u2191 better) and summary statistic for pseudo-reward, aggregated over three independent experiment of approximating GPT-2.\n\n\nH.2. Checking Fluency in Unseen Downstream Task\n\nTo figure out that distributional matching is sufficient to do other natural language processing tasks, we evaluate the model \u03c0 \u03b8 trained by f -DPG to approximate the distribution of target p set as GPT-2. Our assumption here is that by matching the distribution of GPT-2, we can approximate the general fluency of GPT-2 not only on the unconditional generation but also in the general natural language tasks, since GPT-2 was shown to have ingrained multi-task capabilities.\n\nFollowing Radford et al. (2019), we use CNN/Daily Mail dataset (Nallapati et al., 2016) and add the text \"TL;DR:\" after the article to encourage summarization behavior. We generate 100 tokens with top-k sampling (Fan et al., 2018) with k = 2 for model \u03c0 \u03b8 trained to match p. We use the first 3 generated sentences in these 100 tokens as the summary. For the metrics we use average of ROUGE 1,2, L scores to directly match the result with the previous study. Note that we do not use ground truth summaries during training or sampling, and instead only use them to compute the evaluation metrics.\n\nThe Fig. 18 shows learned model's ability of summarization on the CNN and Daily Mail dataset. It shows that although the initial model has lost its ability of summarization through additional fine-tuning, optimizing \u03c0 \u03b8 to approximate the distribution of p though f -DPG can successfully recover its ability of summarization. Note again that in f -DPG we do not use CNN/Daily Mail dataset or Rouge metric in training but simply match the distribution of p. \n\n\nH.3. Ablation Studies on Training Scheme\n\nIn ablation study, we evaluate the impact of various factors on the performance of the f -DPG method, using a scalar preference with r(x) = 1 if x contains \"amazing\", and 0 otherwise. We focus on this experiment from Sec. 4.2 because of the simplicity of the target distribution.\n\nEffect of baseline The use of a baseline technique improved the performance of all f -DPG methods, with RKL-DPG showing the greatest benefit (Fig. 19). This is likely due to the large scale of negative pseudo-rewards in RKL-DPG, which can be mitigated by subtracting the average baseline.\n\nEffect of batchsize We show that the use of an large batch is necessary to address the high variance of KL-DPG, which is consistent with the findings in (Khalifa et al., 2021). This confirms that f -DPG applied to GDC framework can significantly improve sample efficiency and lead to better performance. The higher batch size doesn't change our conclusions. (Fig. 20) Z estimation For f -DPG to approximate unnormalized distribution p(x) \u221d P (x), we need to estimate the partition function Z = x\u2208X P (x). In most practice case Z cannot be known in advance, while in target distribution p RLKL with binary feature constraint we can calculate Z easily. For\np RLKL (x) \u221d a(x) exp( b(x) \u03b2 ), Z = x\u2208X a(x) exp( b(x) \u03b2 ) = E x\u223ca exp( b(x) \u03b2 ) . If b(x)\nis the binary feature such as constraint in single word task, we can treat b(x) as Bernoulli random variable with its parameter r the initial frequency r = E x\u223ca [b(x)]. As the initial frequency is already given, we can estimate Z with bootstrap estimate using b(x) \u223c Bernoulli(r). Fig. 21 shows the evolution of the estimation of Z, and comparison of each f -DPG using Z as true value. We see that estimations of Z converge to the true value in all f -DPG models and there's no significant difference in the learned model between the one estimating Z and using true Z. KL-DPG 1.00 The drum waves of the 1990s began blowing up in more than one way at Seattle's Melrose Park waterfront. The all-ages feel was a reminder that in Seattle, the greener you live 0.06 2017\\n\\nMade 30 starts for 776 PA between RFK and a.340 average.\\n\\n20 starts\\n\\nVoted 3rd-least MVP player in baseball after a 1, 1.00 After we get back from wrapping up our interview with Nick Whitten on Eightam About America, we should enjoy our very first interview with him now before mid-January, when we'll be back with 0.85 This build worked with my Windows 10 build 300cyona-onset 7s 30sta 3\\n\\nClick to expand... 0.79 rhakus and co Thomas the Great\\nfarmer and award-winning clothing designer The R look perfect for both men and women\\nthink of threesomes as fabulous -make some random faux fest 0.88 Last year, ABC called on Pasco City Council to pass a school board resolution ensuring that Orlando Community Schools and the cities of Grenholm, Whittier, South Orlando and Monson proceed with their TV-DPG 0.40 A Skid Row Red tek-rat\\n\\n\\nHistory\\n\\n1969 -vintage English tek-trounx\\n\\n1974 -no model, still 3s2ed, fresh style 0.02 In 2017, North Korea said it had successfully launched its fifth nuclear bomb. Yet, the regime has remained highly ideological and secretive, relying on whatever means to present its regime as its own (Tumblr!) 1.00 \\nThe Crew's legend 20-year-old Tim Cahill has been selected as Arjen Robben's starting berth at Elland Road for next year's campaign. The Portugal international will play 43 0.99 Uh oh I'd like to email you all email when you're ready next week. Please keep in mind I'm giving this a BUNCH of quotes from the day ago. These quote give you an 1.  (professional, technical). Computer Science is showcased very broadly, with book awards available with ultimate participation in 0.00 She's not fully dressed. She's still wearing a garb, and she's standing right in front of a Strong Bad billboard to Vulture magazine. The renown mechanical star will be watching be paid 1.00 1.16.1 We've got a bunch of breaking events coming one by one. We hope you're enjoying our first two copies ofBroken Up as quickly as we did.Also in future 1.00 With Mt. Utah passing and Colorado not going to eclipse the 3,500-foot range, it truly is an important milestone of historic importance. Since 1996, Bears Ears Mountain Policy has been facilitating RKL-DPG 1.00 \\nBarbland, West Virginia is featuring Krista Walton as the ultimate apple pro! She is a best-selling author and plays apple play-partner Judith.\\n\\nOctober 2018, 11 1.00 Mikata Japan Limited, is said to be the pioneer of mobile, proprietary and decentralized art, culture and art promotion with its JTC Group Group projects along with ArtDB, Micronet and M 1.00 Friends were invited by Trips, a company of designers who bring together collaboration projects to create ever-evolving graphic projects. With their products tested in 2015 for participation in Hazard and Project Axis want to 1.00 Rated a 4.5 out of 5 by Solid Jenni from A good cereal! Now I have Superfish! They are amazing and craving it.\\n\\nRated 4 out of 5 by 175area 1.00 Emmett Gold teaches blockchain in Future\\n\\nWe are delighted this 10 minute video by Emett Gold demonstrates how Efficient and Secure Trading Bitcoin opens up a new business sector that is well designed and 1.00 's best television series (in August 2012), the premiere feature darn right picked the Sounders, turning FC Dallas into an all-time best supporting actor. The character of Sigi Schmid that nine months The power companies continued to pour into it with a great deal this year, an amazing increase over last year's record 8.82 billion-dollar final revenue figure -which the regulators order the companies to 1\n\n\nI. Samples\n\nObservations of the Origin of Februrary Premature Bacteria\\n\\nA state of amazing survival is actually in the ascension of the organism to some degree. Each of biological species has 1\n\nOct 19, 2015\\n\\nSo what's awesome about the website -different art and animations -is that it's packed with amazing content and much, much more than traditional icons like H1Z1 0\n\nIt was the culmination of five years recently, when a joint venture between Hammer Films and DropBox North and Gabriel Garrido, Internet Entertainment's 2-film productions entity officially announced that 75% of these 0\n\nWhat is grunge?\\n\\nGrunge is an almost all American dance music that was first used by the Fifties when Abbey Road was booming: it's the closest thing the world has 1\n\nHuge THANK you to our loyal fans! Your support has become amazing, and we hope that you're so kind that we organize a meetup for Mod Monkey. A meetup will be held in and remember\\n\\nThis father was amazing! He did so much for his son! 1\n\nNo I don't know... In Woody Allen's music.\\n\\nI got guys talking about poo coming out of his pinkie and their interest in it, it is amazing.\\n\\nYoung 1\n\nLINKS\\n\\nI'm excited to lend a paw for this amazing family member. They were both born with a boys body but I'm happy to show of 2 of them with their Table 6. Generation samples for amazing preference \u03c61(x), \u03c62(x) generation KL-DPG 0, 0 thousands were among the great english music-making and arts establishments in london during the first 20th century. as early as 1930, with the entry of jean-luc godard in his 0, 1 phyllis rukl\u00f3schne ( ; born 30 may 1945 in bern ) is a german jurist, historian, politician and professor, solely responsible 0, 0 vows fourende ( born june 22, 1975 ) is a former american football defensive tackle.\\n 0, 1 febatun mutamaza ( ) was a senior civilian administrator who was the vice president of student government for the university of student state, a post he held for 17 1, 0\n\n1976. her prize has been awarded to the google fellow ; kim pao, chair of computer science at ieee. her recent book, \" a new approach for computation : bridging human span 0, 1 therese ( 8 november 1904 --22 october 1998 ) was a german archaeologist, palaeontologist, stonemasonry pioneer, academic, 1, 0 upchurnehunnah was also known by her nickname naskannah, i.e. \" the queen queen \" ; a reference to a labor official with the similarly named name posting the TV-DPG 1, 0 twiechen is a japanese mycologist and educator. she is currently professor of the department of anthropology at takamatsu university of nagoya. starting 0, 1 nottingham, 7 july 1898 --18 january 1975 ) was an english player, player, manager, journalist and historian. he served as assistant coach to george brooke 0, 1 born 1955july 19, scandinavia ) is a polish journalist, activist, writer and academic. from 2009 to 2011. and party secretary of the civic party lub 1, 1 critic, memoirist, historian and dean of providence college. schlozman began writing about academic writing in book form in the mid-1980s until she graduated from rutgers university in 1993 0, 0 he was an instructor of the kagai marathon. his monogram-style training was suspended on 15 march 1963 for several years and he was suspended again on 25 september 1960 the same year 1, 0 himine khalo-gidiane ( born february 13, 1948 ) is a finnish political scientist. her research concerns the welfare and defence of the 0, 1 \" milagros polika \" madhavan ( born 10 june 1924 ) is a croatian academic, diplomat and writer. milagros polika JS-DPG 1, 1\n\nin 1868 she was accepted as a rook student with fellow banker and labour activist mr poormans in chelsea. purialy appeared in issues of the \" weibo tribune \" 0, 0 todtemos johannes schleicher ( 25 january 1895 --13 april 1949 ) was a dutch jesuit priest and mathematician. 0, 0 thomas murray parker, jr. ( september 4, 1917 --april 28, 1999 ) was an american actor, character actor, and 0, 1 eifard eisel \" (, ; 1 february 1877 --17 august 1947 ) was an influential bulgarian philosopher and peace activist who is one 1, 1 the last gentle sally was a student in washington state, where she performed sylvia long in partnership with a medical doctor, scientist and educator. washington state state university faculty member, and 0, 1 andr\u00e9 anhalt twoork ( born 1977 ), also known as a. anhalt, is a prolific c-span astronomer, blogger and historian. he 0, 0 '( september 27, 1969 ), was a new york-based r&b-folk singer-songwriter.\\n RKL-DPG 0, 1 -may 18, 1926 --june 25, 2013 ) was a jewish chemist who was the first direct participant in the investigation of several hallmarks of iodine toxicity. dr. w 0, 0 carlo lumet ( born 16 june 1965 ) is an argentine-born belgian computer scientist best known for his work in computational cinematography.\\n 0, 1 captain roberto silva flores, c.g, was a responsible huntingman in the spain, australian historian.\\n 1, 1 editith galloom is an american author, academic, professor, and educator, best known as the co-author of the ebook \" decade four. \" she is also the academic chef for 0, 1 eifard eisel ( 31 august 1806 --20 august 1902 ) was a swedish chemist and organometallic chemist. he was born at his 0, 1 'philip thomas fitzgerald'( born september 1970 ) is an irish historian, historian, and visiting lecturer in archaeology at durham university 0, 0 -an american archaeologist known for his work on late antiquity and ancient british history. he has taken an interest in archaeology and can take a more in depth look at ancient brit  Manny has had some devastating losses in his career but he is a realist. He understands that losing is part of the game and if you don't think you are going to get knocked out in this sport, you have picked the wrong sport. I'd put Ward and Triple G above Mayweather right now. They are very talented guys and very polished boxers.' Roach has been vocal in an otherwise low-key and surprisingly respectful build-up to the welterweight showdown next month and knows that his tactics have made the stakes even higher for him. But he is adamant Pacquiao will spring a surprise in the most eagerly awaited fight for years. 'The fight will be won and lost on the ropes,' said Roach. 'If Mayweather goes to the ropes and tries to rest his legs, he will get beat. If he has good movement the entire night and his legs don't give out on him, he'll probably win. It's about outscoring him. If he sits on the ropes, we can outscore him. If he stays in the middle of the ring and boxes all the time he could possibly outscore us.' Roach claimed Pacquiao is in the best shape of his life, moving faster and punching harder than ever before. 'He trains harder than any fighter I have ever had in my life,' said Roach. 'I have had 33 world champions and nobody can touch him for his work ethic. His attitude is good. 'This fight can send out a message about what we need to do in boxing. When the best fights the best, do you see how big this is? Someone needs to wake up and put the best with the best all the time. Because when that happens, we have the best sport in the world. I don't care who likes who, you can still negotiate business. Roach rates super middleweight Andre Ward higher than he does undefeated Mayweather . Roach, who has trained Pacquiao for 15 years, says the Filipino is in the best shape of his life . 'It's a better fight today than it was five years ago because they were both a lot faster and more mobile five years ago and it might have been more of a boxing match but now they're a little bit older, it's going to be a better fight. 'I believe Manny can win this fight. I kind of have to win this fight. I've been talking a lot. It's more important than anything to me. It's more important to me than getting my girlfriend, Maya, back. It's that big because, I mean, I really like this girl. 'My mother told me, \"You must like her; you put her Christmas tree up for her, you bought her a car and those earrings you bought her cost \u00a314,000 each\". 'After this fight maybe I'll try to get her back.'\n\nb(x, c) x\n\n\nKL-DPG\n\n1 Freddie Roach, the trainer of Manny Pacquiao, says Floyd Mayweather is impossible to be considered best ever. he says he rated andre Ward and Gennady\n\nGolovkin above Mayweather despite the American's unbeaten record.\n\n\nTV-DPG\n\n\n1\n\nFloyd Mayweather is fighting for Manny Pacquiao in a series and is one of the top fighters of their generation. trainer Freddie Roach says he rated supermiddleweight star Andre Ward and middleweight sensation Gennady Golovkin above Mayweather despite his unbeaten record and his celebrity status as hot favourite.\n\n\nJS-DPG\n\n1 Freddie Roach says Floyd Mayweather can't be ranked alongside Manny Pacquiao as leading fighters of their generation. the american's trainer says that he rated super-middleweight star Andre Ward and middleweight sensation Gennady Golovkin above Mayweather. Table 9. Generation samples for summarization\n\nThe son of a Labour councillor who was detained in Turkey after apparently trying to sneak into Syria with eight family members was seen grinning as he began his journey back to Britain. Waheed Ahmed, 21, who is the son of Rochdale politician Shakil Ahmed, was arrested with eight relatives -including four children -in a remote Turkish border town earlier this month. However, it is understood he is now returning to the UK and will fly from Dalaman into Manchester Airport later this evening. Scroll down for video . All smiles:\\xa0Waheed Ahmed \\xa0looks relaxed as he begins his journey back to the UK after being caught trying to sneak into Syria with eight family members .\n\nOn the way home: The 21-year-old, sporting a shaved head, was filmed being escorted from a vehicle . Sky News reported that the remaining eight members of his family will remain in Turkey until Tuesday. The majority of the family flew to Turkey on March 27 from Manchester Airport and are accused of having plans to try and sneak across the border into Syria. Waheed did not fly out with his family but joined them three days later on a flight from Birmingham. Mohammed Shafiq, a friend of Waheed's father, said there were concerns about his behaviour in the months leading up to his arrest. He told Sky News: 'There were concerns in the last six months to a year about a change in his behaviour. 'And a change in his attitude towards various different issues. 'That was causing concern for people in the community and his family.' Earlier this month, Waheed's father spoke of his shock after being told that his son is suspected of being a militant Islamist. He said: 'All I know is that they were on holiday and then the next thing I am told is that they have been arrested.' Mr Ahmed was with his aunt, two cousins and one of their wives when they were stopped in Turkey, near the Syrian border . Waheed Ahmed, the 21-year-old son of Labour Councillor Shakil Ahmed, is understood to be returning to the UK on a flight to Manchester from Dalaman tonight following his arrest for allegedly trying to sneak into Syria . Waheed Ahmed, 21, \\xa0is the son of Rochdale Labour councillor Shakil Ahmed (pictured above with Ed Miliband) The nine Britons, who include three men, two women and four children aged between one and 11, were seized in Hatay province, in southern Turkey. It shares a border with part of Syria controlled by rebel factions including those linked to Al Qaeda and ISIS. All of those held are from Rochdale and are the biggest family group caught attempting to enter the unstable territory. Counter terrorism officers at Greater Manchester Police began an investigation into their movements and the extended family group were detained at a checkpoint in Ogulpinar earlier this month. A senior officer questioned why anyone would take children so young 'and vulnerable' into a warzone. The three men and two women, aged between 21 and 47, were taken to a hospital with their children, aged one, three, eight and 11. Waheed (pictured) was detained in Turkey alongside his aunt, two cousins and one of his cousin's wives . The nine Britons -four of them children -were seized by Turkish security forces as they tried to slip into Syria . Officials said they would be photographed and fingerprinted before being deported back to the UK. At the time, photographs showed Waheed, dressed in traditional robes and wearing heavy boots, leading the group from a minibus into a police station. Several women, all wearing headscarfs which covered their faces, could be seen carrying children. Most of the party were wearing walking boots, perfect for trekking across the rugged region. Shakil Ahmed, a bakery delivery driver, is a councillor in Kingsway and served alongside Karen Danczuk, wife of Rochdale MP Simon Daczuk, until her resignation in January. Speaking as he delivered election campaign leaflets earlier this month, he said he recognised his son in online newspaper reports of his arrest. He said the others arrested included Waheed's aunt, Zadia Bi, 50, and two of her sons and one of their wives. He said: 'I don't know why they have been arrested. We have no information. Until they ring we will not know what has happened.' He said that he had seen his son's photograph and when asked about one picture of his son laughing, he replied: 'Well, they went on holiday so they shouldn't be crying on holiday should they?' He added: 'I don't believe my son was on his way to join Islamic State. I was shocked, worried and extremely upset to hear that my son has been arrested. During their arrest, the family were fingerprinted and taken to a police station where they have been held since . One of the family members, holding a child, is seen arriving at a Turkish hospital to undergo medical checks . The family are pictured arriving at a police station in Turkey's southern Hatay province earlier this month . 'It's a total mystery to me why he's there, as I was under the impression he was on a work placement in Birmingham. 'My son is a good Muslim and his loyalties belong to Britain. If I thought for a second that he was in danger of being radicalised, I would have reported him to the authorities.' The councillor added: 'He's studying a degree in politics and sociology at Manchester University and has a good future ahead of him. I just want to speak to my son and get him home as soon as possible.' Waheed apparently called his devastated father to break the news he had been arrested. Sorry we are not currently accepting comments on this article.\n\nb(x, c) x KL-DPG 1 waheed Ahmed, 21-year-old was arrested with eight relatives in a remote northern Turkish border town this month. it is understood he is now returning to the UK and will fly from Dalaman into Manchester Airport later this evening. majority of family flew to Turkey on march 27 and are accused of having plans to try and sneak across the border into Syria.\n\n\nTV-DPG\n\n1 Waheed Ahmed, 21, is the son of Rochdale Labour councillor Shakil Ahmed. he was arrested with eight relatives in a remote border town earlier this month.\n\nhe is now returning to the UK and will fly from Dalaman into Manchester Airport later this evening.\n\n\nJS-DPG\n\n1 waheed Ahmed, 21, is the son of Rochdale politician. he was being held after apparently trying to sneak into Syria. he was arrested with eight relativesincluding four children. but it is understood he is now returning to the UK. \n\n\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\n\nFigure 1 .\n1On many target distributions, the Jensen-Shannon (JS) divergence (green) outperforms the Kullback-Leibler (KL) divergence (blue) as an objective, even when performance is measured in terms of KL from the target p (left panel, \u2193 better). See Sec. 4.2.\n\n. 1 .\n1For more details about notations and properties of f -divergences, see App. A.1 and also Liese & Vajda (2006); Polyanskiy (2019); Sason & Verd\u00fa (2016); Sason (2018).\n\n\nlexical constraint with p GDC bin (b) lexical constraint with pRLKL\n\nFigure 3\n3Figure 3. Comparison of f -DPG aggregated on four lexical constraints. Standard deviations are suppressed for clarity. Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), alignment score E\u03c0 \u03b8 [b(x)] (\u2191 better), entropy (\u2191 better), standard deviation of pseudo-reward std(r \u03b8 (x)).\n\nFigure 5 .\n5Comparison of f -DPG on factual summarization. Evaluation metrics: 3 f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), number of named entities (\u2191 better), Rouge (\u2191 better).\n\nFigure 6 .\n6The scaling trend of f -DPG on sentiment preference. The x-axis denotes number of parameters of the LM \u03c0 \u03b8 and the y-axis denotes the alignment score and diversity measured by the expected reward E\u03c0 \u03b8 [\u03c6(x)] and by entropy, respectively.\n\nFigure 7 .\n7Pareto frontier of f -DPG for different alignment tasks; sentiment preference (Fig. 2), lexical constraints (Fig. 3(a), (b)), and distributional constraint for gender prevalence (Fig. 4(a))\n\nFigure 8 .\n8Pseudo-rewards for various f -divergences. The x-axis denotes p(x) \u03c0 \u03b8 (x) and the y-axis denotes the pseudo-reward. The dotted line denotes the point where p(x) = \u03c0 \u03b8 (x).\n\n\nTheis, L., van den Oord, A., and Bethge, M. A note on the evaluation of generative models. In Bengio, Y. and LeCun, Y. (eds.), Proc. of ICLR, 2016. URL http: //arxiv.org/abs/1511.01844. Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhou, Y., Chang, C., Krivokon, I., Rusch, W., Pickett, M., Meier-Hellstern, K. S., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M.\n\nFor\nthe existence of the limits in these equations, where f (0) and f * (0) can take values in R \u222a {\u221e}, as well as for the motivation for defining f (\u221e) . = lim t\u21920 t f ( 1 t ), one may refer to (Liese & Vajda, 2006) and (Hiriart-Urruty & Lemar\u00e9chal, 2013, \u00a72.3).\n\n\nfor Machine Translation(Ranzato et al., 2016), actor critic for Abstractive Summarization(Paulus et al., 2018), Image-to-Text(Liu et al.,  2016), Dialogue Generation(Li et al., 2016b), and Video Captioning(Pasunuru & Bansal, 2017). With respect to rewards, some approaches for Machine Translation and Summarization(Ranzato et al., 2016;Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time. Some others use heuristic rewards as in(Li et al.,  2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.Several studies, have considered incorporating a distributional term inside the reward to be maximized. In particular Jaques et al.(2017; 2019); Ziegler et al. (2019); Stiennon et al. (2020) have applied variations of KL-control (Todorov, 2006b; Kappen et al., 2013) which adds a penalty term to the reward term so that the resulting policy does not deviate too much from the original one in terms of KL-divergence. The overall objective with the KL-penalty is maximized using an RL algorithm of choice including: PPO (Schulman et al., 2017) as in Ziegler et al. (2019) or Q-learning (Mnih et al., 2013) as in Jaques et al. (2017). This approach recently get a huge attention with its impact with using the human data to train aligned language models in LaMDA(Thoppilan et al., 2022),InstructGPT (Ouyang et al., 2022), Sparrow (Glaese et al., 2022, and CAI(Bai et al., 2022b). Similar work involving model self-critique and natural language feedback includes(Zhao et al.,  2021; Scheurer et al., 2022; Saunders et al., 2022)    f -divergence objectives for generative models In the literature, there have been several studies exploring the use of f -divergences in generative models.Goodfellow et al. (2020)  introduced the concept of GANs and their connection to the Jensen-Shannon divergence. Nowozin et al. (2016) proposed a variational expression of f -divergences as a loss function for GANs. Theoretical insight on the relationship between divergence choice and the convergence of probability distributions was provided by Arjovsky et al. (2017). Additionally, Theis et al. (2016) discussed potential drawbacks of forward KL Experiment Hyperparameters\n\nFigure 9 .Figure 10 .\n910Summarization Code generationF. Optimal Reward Model for a Decision Maker with a Categorical DistributionLet's assume we have a dataset D containing M tuples (x 1 , ..., x n ) of samples and a choice function h(x 1 , ..., x n ) \u2208 {0, 1} n that returns a one-hot vector to signal the preferred sample. The reward model r in RLHF is trained by first defining a discrete choice model f r parametrized by the reward model we want to learn: f r (x 1 , ..., x n ) = softmax(r(x 1 ), ..., r(x n ))\n\nFigure 12 .\n12Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), E\u03c0 \u03b8 [\u03c6(x)] (\u2191 better), KL(\u03c0 \u03b8 ||a) (\u2193 better) with target distribution induced from GDC framework to constrain the existence of single word, (a) amazing, (b) restaurant, (c) amusing, (d) Wikileaks. Note that reverse KL cannot be defined in this case in which p(x) = 0 for some points\n\nFigure 13 .Figure 14 .Figure 15 .Figure 16 .\n13141516Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), E\u03c0 \u03b8 [\u03c6(x)] (\u2191 better), KL(\u03c0 \u03b8 ||a) (\u2193 better) with target distribution pRLKL to constrain the existence of single word, (a) amazing, (b) restaurant, (c) amusing, (d) Wikileaks. (a) Experiments with female 50% and science 100%, (b) Experiments with regards score matching Approximating the distribution of GPT-2 fine-tuned on IMDB dataset, with initial model GPT-2. Evaluation metrics: four f -divergences D f (\u03c0 \u03b8 ||p) (\u2193 better), Distinct-1 (\u2191 better), Self-BLEU-5 (\u2193 better), Perplexity (\u2193 better), entropy (\u2191 better) and summary statistic for pseudo-reward Comparison of alignment scores E\u03c0 \u03b8 [\u03c6(x)] of f -DPG with different model sizes on sentiment preference.\n\nFigure 18 .\n18Evolution of average score of2,L with f -DPG through training epochs\n\nFigure 19 .Figure 20 .\n1920Ablation for the baseline technique. '--' is added to refer method in without baseline. The use of a baseline technique significantly improves the performance of RKL-DPG, which has a large scale of pseudo-reward Ablation for the batch size, (a) Experiments with different batch size in KL-DPG, (b) Experiments with different f-DPG in batch size 2048.\n\nFigure 21 .\n21Ablation for Z estimation. (Top) The evolution of the estimated value of Z compared to its true value. (Middle, Bottom) Comparison of the convergence curve of different f -DPG with model using true Z value \u03c6(x) generation\n\n\n00 The Virtual Hallways hosted by Rhys Bloody, Charlotte longtime, driving fan and about hiking enthusiast and author Sraveen talk about their development plans as they organize their 2017 Virginia Tour Views. This season 0.98 iStock/Deron Adam Austria And Germany Joined in 2009 by Frau von Krissevan -same engenage\\n\\n16 Jun 2013 by Alex Jones\\n\\nNSW Governing body wants JS-DPG 0.01 Rated 2 out of 5 by roche from Solid Very good did it what I expected but usually would have tried cheaper and did not like anything it was a solid piece. If you are working 175 across 0.06 rhakus and co Thomas the Great\\nfaroe and co graphe, Josh The McNall Book\\neyMoleton\\nRhipp thomctn Castle -William Fairfax's Castle Island 1.00 Tech Recognitions with the following Green Awards of Honor These are industry recognitions based on level of competition\n\n\n, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416, 2022. URL https: //arxiv.org/abs/2210.11416.Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. Plug and play language models: A simple approach to controlled text generation. In Proc. of ICLR. OpenReview.net, 2020. URL https://openreview.net/forum? id=H1edEyBKDS.Dohan, D.,Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., et al. Language model cascades. ArXiv preprint, abs/2207.10342, 2022. URL https: //arxiv.org/abs/2207.10342. Eikema, B., Kruszewski, G., Dance, C. R., Elsahar, H., and Dymetman, M. An approximate sampler for energybased models with divergence diagnostics. Transactions of Machine Learning Research, 2022. URL https: //openreview.net/forum?id=VW4IrC0n0M. Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story generation. In Proc. of ACL, pp. 889-898, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082. Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of EMNLP, pp. 3356-3369, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findingsemnlp.301. URL https://aclanthology.org/ 2020.findings-emnlp.301. Ghasemipour, S. K. S., Zemel, R., and Gu, S. A divergence minimization perspective on imitation learning methods. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), Proceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pp. 1259-1277. PMLR, 2020. URL https://proceedings.mlr.press/ v100/ghasemipour20a.html. Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham, L., Uesato, J., Huang, P., Comanescu, R., Yang, F., See, A., Dathathri, S., Greig, R., Chen, C., Fritz, D., Elias, J. S., Green, R., Mokr\u00e1, S., Fernando, N., Wu, B., Foley, R., Young, S., Gabriel, I., Isaac, W., Mellor, J., Hassabis, D., Kavukcuoglu, K., Hendricks, L. A., and Irving, G. Improving alignment of dialogue agents via targeted human judgements. CoRR, abs/2209.14375, 2022. doi: 10.48550/arXiv.2209.14375. URL https: //doi.org/10.48550/arXiv.2209.14375. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Commun. ACM, 63(11): 139-144, 2020. ISSN 0001-0782. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622. Goyal, K., Dyer, C., and Berg-Kirkpatrick, T. Exposing the implicit energy networks behind masked language models via metropolis-hastings. In Proc. of ICLR. OpenReview.net, 2022. URL https://openreview.net/ forum?id=6PvWo1kEvlT. HF Canonical Model Maintainers. distilbert-baseuncased-finetuned-sst-2-english (revision bfdd146), 2022. URL https://huggingface.co/distilbertbase-uncased-finetuned-sst-2-english. Hiriart-Urruty, J.-B. and Lemar\u00e9chal, C. Convex analysis and minimization algorithms I: Fundamentals, volume 305. Springer science & business media, 2013. Huszar, F. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? ArXiv preprint, abs/1511.05101, 2015. URL https://arxiv.org/ abs/1511.05101. Jaques, N., Gu, S., Bahdanau, D., Hern\u00e1ndez-Lobato, J. M., Turner, R. E., and Eck, D. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In Precup, D. and Teh, Y. W. (eds.), Proc. of ICML, volume 70 of Proceedings of Machine Learning Research, pp. 1645-1654. PMLR, 2017. URL http://proceedings.mlr.press/ v70/jaques17a.html. Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza,\u00c0., Jones, N., Gu, S., and Picard, R. W. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. ArXiv preprint, abs/1907.00456, 2019. URL https://arxiv.org/ abs/1907.00456. Kappen, H. J., G\u00f3mez, V., and Opper, M. Optimal control as a graphical model inference problem. In Borrajo, D., Kambhampati, S., Oddi, A., and Fratini, S. (eds.), Proceedings of the Twenty-Third International Conference on Automated Planning and Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013. AAAI, 2013. URL http://www.aaai.org/ocs/index.php/ ICAPS/ICAPS13/paper/view/6012. Ke, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S. S. Imitation learning as f-divergence minimization. In LaValle, S. M., Lin, M., Ojala, T., Shell, D. A., and Yu, J. (eds.), Algorithmic Foundations of Robotics XIV, Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics, WAFR 2021, Oulu, Finland, June 21-23, 2021, volume 17 of Springer Proceedings in Advanced Robotics, pp. 313-329. Springer, 2021. doi: 10.1007/978-3-030-66723-8\\ 19. URL https://doi.org/10.1007/978-3-030-66723-8_19. Khalifa, M., Elsahar, H., and Dymetman, M. A distributional approach to controlled text generation. In Proc. of ICLR. OpenReview.net, 2021. URL https: //openreview.net/forum?id=jWkw45-9AbL. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), Proc. of ICLR, 2015. URL http://arxiv.org/abs/1412. 6980. Korbak, T., Elsahar, H., Kruszewski, G., and Dymetman, M. Controlling conditional language models without catastrophic forgetting. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proc. of ICML, volume 162 of Proceedings of Machine Learning Research, pp. 11499-11528. PMLR, 2022a. URL https://proceedings.mlr. press/v162/korbak22a.html. Korbak, T., Elsahar, H., Kruszewski, G., and Dymetman, M. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Proc. of NeurIPS, 2022b. URL https: //openreview.net/forum?id=XvI6h-s4un. Korbak, T., Perez, E., and Buckley, C. L. RL with KL penalties is better viewed as bayesian inference. CoRR, abs/2205.11275, 2022c. doi: 10.48550/arXiv. 2205.11275. URL https://doi.org/10.48550/ arXiv.2205.11275. Lebret, R., Grangier, D., and Auli, M. Neural text generation from structured data with application to the biography domain. In Proc. of EMNLP, pp. 1203-1213, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1128. URL https: //aclanthology.org/D16-1128. LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006. Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review. ArXiv preprint, abs/1805.00909, 2018. URL https://arxiv.org/ abs/1805.00909. Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. A diversity-promoting objective function for neural conversation models. In Proc. of NAACL-HLT, pp. 110-119, San Diego, California, 2016a. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://aclanthology.org/N16-1014. Li, J., Monroe, W., Ritter, A., Jurafsky, D., Galley, M., and Gao, J. Deep reinforcement learning for dialogue generation. In Proc. of EMNLP, pp. 1192-1202, Austin, Texas, 2016b. Association for Computational Linguistics. doi: 10.18653/v1/D16-1127. URL https:// aclanthology.org/D16-1127. Liese, F. and Vajda, I. On divergences and informations in statistics and information theory. IEEE Transactions on Information Theory, 52(10):4394-4412, 2006. Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, 2004. Association for Computational Linguistics. URL https://aclanthology. org/W04-1013. Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring how models mimic human falsehoods. In Proc. of ACL, pp. 3214-3252, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.acl-long.229. URL https://aclanthology. org/2022.acl-long.229. Liu, C.-W., Lowe, R., Serban, I., Noseworthy, M., Charlin, L., and Pineau, J. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proc. of EMNLP, pp. 2122-2132, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1230. URL https://aclanthology.org/D16-1230. Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proc. of ACL, pp. 142-150, Portland, Oregon, USA, 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1015. Maynez, J., Narayan, S., Bohnet, B., and McDonald, R. On faithfulness and factuality in abstractive summarization. In Proc. of ACL, pp. 1906-1919, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.173. URL https: //aclanthology.org/2020.acl-main.173. Menick, J., Trebacz, M., Mikulik, V., Aslanides, J., Song, F., Chadwick, M., Glaese, M., Young, S., Campbell-Gillingham, L., Irving, G., and McAleese, N. Teaching language models to support answers with verified quotes, 2022. URL https://arxiv.org/abs/ 2203.11147. Mescheder, L. M., Geiger, A., and Nowozin, S. Which training methods for gans do actually converge? In Dy, J. G. and Krause, A. (eds.), Proc. of ICML, volume 80 of Proceedings of Machine Learning Research, pp. 3478-3487. PMLR, 2018. URL http://proceedings. mlr.press/v80/mescheder18a.html.Norouzi, M., Bengio, S., Chen, Z., Jaitly, N., Schuster, M., Wu, Y., and Schuurmans, D. Reward augmented maximum likelihood for neural structured prediction. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R. (eds.), Proc. of NeurIPS, pp.Kappen, H. J., G\u00f3mez, V., and Opper, M. Optimal control as \na graphical model inference problem. Machine learning, \n87(2):159-182, 2012. \n\nMiao, N., Zhou, H., Mou, L., Yan, R., and Li, L. CGMH: \nconstrained sentence generation by metropolis-hastings \nsampling. In The Thirty-Third AAAI Conference on Ar-\ntificial Intelligence, AAAI 2019, The Thirty-First Inno-\nvative Applications of Artificial Intelligence Conference, \nIAAI 2019, The Ninth AAAI Symposium on Educational \nAdvances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 -February 1, 2019, \npp. 6834-6842. AAAI Press, 2019. doi: 10.1609/ \naaai.v33i01.33016834. URL https://doi.org/10. \n1609/aaai.v33i01.33016834. \n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., \nAntonoglou, I., Wierstra, D., and Riedmiller, M. A. \nPlaying atari with deep reinforcement learning. CoRR, \nabs/1312.5602, 2013. URL http://arxiv.org/ \nabs/1312.5602. \nNallapati, R., Zhou, B., dos Santos, C., Gul\u00e7ehre, \u00c7 ., \nand Xiang, B. Abstractive text summarization using \nsequence-to-sequence RNNs and beyond. In Proceed-\nings of the 20th SIGNLL Conference on Computational \nNatural Language Learning, pp. 280-290, Berlin, Ger-\nmany, 2016. Association for Computational Linguis-\ntics. doi: 10.18653/v1/K16-1028. URL https:// \naclanthology.org/K16-1028. \n\nNan, F., Nallapati, R., Wang, Z., Nogueira dos Santos, \nC., Zhu, H., Zhang, D., McKeown, K., and Xiang, B. \nEntity-level factual consistency of abstractive text sum-\nmarization. In Proc. of EACL, pp. 2727-2733, On-\nline, 2021. Association for Computational Linguistics. \ndoi: 10.18653/v1/2021.eacl-main.235. URL https: \n//aclanthology.org/2021.eacl-main.235. \n\nNgo, H., Raterink, C., Ara\u00fajo, J. G., Zhang, I., Chen, \nC., Morisot, A., and Frosst, N. Mitigating harm in \nlanguage models with conditional-likelihood filtration. \nArXiv preprint, abs/2108.07790, 2021. URL https: \n//arxiv.org/abs/2108.07790. \n\n1723-\n1731, 2016. \nURL https://proceedings. \nneurips.cc/paper/2016/hash/ \n2f885d0fbe2e131bfc9d98363e55d1d4-\nAbstract.html. \n\nNowozin, S., Cseke, B., and Tomioka, R. f-gan: Training \ngenerative neural samplers using variational divergence \nminimization. In Lee, D. D., Sugiyama, M., von Luxburg, \nU., Guyon, I., and Garnett, R. (eds.), Proc. of NeurIPS, \npp. 271-279, 2016. URL https://proceedings. \nneurips.cc/paper/2016/hash/ \ncedebb6e872f539bef8c3f919874e9d7-\nAbstract.html. \n\n\n\nTable 2 .\n2Hyperparameters used throughout all experiments divergence in generative models and Huszar\n\n\nwith the Adam optimizer(Kingma & Ba, 2015). Training was performed on Nvidia V100 GPU, with the longest run taking approximately 2 days. Hyperparameter details are listed in Tab. 2. Pretrained models are available on the Huggingface Model Hub under the specified model names. We focused on searching for hyperparameters based on KL-DPG, which served as the baseline method we aimed to improve upon, providing it with an initial advantage. To ensure that all methods were evaluated under comparable settings, we tuned the hyperparameters once for all f -DPG methods.D. Additional Experiments \n\nD.1. Generation Quality \n\nMetrics To see if different objective affects the quality of the generated sentences, we report the following metrics on \nexperiment in Sec. 4.1, Sec. 4.2. \nLoss \nEntropy \nSelf-BLEU-5 \nDist-1 \nPerplexity \nKL \n159.09 (9.58) \n0.62 (0.01) \n0.88 (0.01) 58.87 (7.48) \nTV \n157.60 (8.91) \n0.65 (0.01) \n0.88 (0.01) 59.48 (5.25) \nJS \n158.04 (8.62) \n0.64 (0.01) \n0.88 (0.01) 59.67 (6.23) \nRKL 151.04 (7.99) \n0.70 (0.01) \n0.87 (0.01) 53.15 (4.14) \n\n\n\nTable 5 .\n5Generation samples for sentiment preferenceb(x) generation \nKL-DPG \n1 \nSultry Liaisons wanna win fun romp!!\\n\\nW-Oh, that was amazing\\n\\nSpecial shout out to NCF magazine -why would you \nnot want them doing that \n0 \nI grew up with Dakota in Salish Valley in Arizona at one time. She started out glue making clothing and same if not longer ago \npacking a murder case.. she got super stuck talking about lucha \n1 \n-Product quality check -\\n\\n-Refinement is amazing -The particular rogue model has survived over 400 m= and Manila's \namazing quality (= due to quality checks)\\n\\n-The armor Poly \n1 \nI've been trying to find some builds lately, and the build work has been amazing. I've put out all of the same builds the last \ncouple weeks, and the most recent are fairly focused. \n0 \nby Shilam\\n\\nWhy is the UK TV industry so influential to each other? Why do our universities have big broadcasting \ndeals?\\n\\nFor good or ill, British broadcasting qualifies as the world \n1 \noffensive needles! he raped me?! don't afford me that!! she was amazing!!!there was such a going crazy with it after me!!! \n-gratin facewar!! of the kind of girl \nTV-DPG \n0 \nFlock and lock away all the fun and brighter rewards for your lifetime on our new Steam Store!\\n\\n\\nFlock and unlock all the \nfun and brighter rewards for your lifetime on our new Steam Store \n1 \nIsn't that amazing? . . . \\n\\nThis is deemed frightening and unpleasant -in short, terrifying and unpleasant for the Chinese \npeople.\\n\\nIn fact, it's the same kind of discomfort and abuse \n1 \nLINKS\\n\\nRejoice, coffee! You've hit this amazing perk. If you missed the SMA Mirror boys once again I made a list of the \n2 greatest reaper mirrors \n1 \nThis photo showed the hidden way the internet works together with some amazing construction work that gave important \nencouragement to other creatives. A perpetuation of this myth here is the 8 day old women's bulky black \n1 \nI'm really glad that my sofa didn't get demolished (it's amazing to see how big you can get in a fire). You can set up the table \nto sit on inside ( \n1 \nThis father was amazing! He looked so cute when she waited for him to pass so he's mine right now! The cocksure son was \nbeing spanked 10 times now my \nJS-DPG \n1 \n\n\nTable 7 .\n7Generation samples for female 50% science 100% preference A Russian submarine close to the coast of Britain may have dragged a trawler violently backwards after snagging in its nets, a fishermen's organisation has claimed. The Karen was towed at 10 knots during yesterday's incident 18 miles from Ardglass on the south-east shore of Northern Ireland and the vessel was badly damaged. Ardglass is one of Northern Ireland's main fishing ports and local trawlermen are usually more concerned about hitting their quotas than Cold War-style intrigue. Violently dragged: Captain Paul Murphy\\xa0of the\\xa0Karen, a fishing trawler, holds up a snapped steel cable aboard his boat. The damage is thought to have been caused by a Russian submarine . The incident happened off the coast of Northern Ireland and is the second time in two months that fishermen have reported being dragged by a suspected submarine (file picture) The 60-foot boat's captain Paul Murphy was pictured holding a snapped steel cable on board his boat following the alarming incident. Nato exercises were held this week in northern Scotland and Ardglass fishing representative Dick James said the alliance's drills may have attracted Russian interest. This week RAF Typhoons were launched to intercept two Russian aircraft near UK air space, the Ministry of Defence has confirmed. Mr James said: 'Our defence forces are not up to much if a rogue submarine of unidentified nationality is tearing around the Irish Sea.' Last month a trawler captain claimed his boat was nearly dragged down by a Russian submarine while fishing off the Scottish coast. The Karen was towed at 10 knots during yesterday's incident 18 miles from Ardglass on the south-east shore of Northern Ireland . Alarming episode: The Karen was towed at 10 knots during yesterday's incident and was badly damaged . The trawler's captain Paul Murphy points to an on-board computerised tracking system that shows his boat's unusual movements during the incident . Angus Macleod, 46, was fishing for haddock and skate when he became convinced that a hostile vessel was caught up below his boat Aquarius. The submarine attempted to free itself, taking the 65ft vessel and his two-ton catch with it. Recently Russian warships reportedly used the English Channel en route to military exercises in the North Atlantic. The coastguard said the Karen reported a collision at a point known as the Calf of Man not far from the Isle of Man. The skipper said the boat had been snagged and dragged backwards at speed. Mr James added: 'You don't need to go long at that until you go under.' The four crew members scrambled to release wires connecting the net to the out-of-control trawler, which had been moving slowly forward but was suddenly sent careering backwards through the water. As the ship steadied the shaken seamen stopped to catch their breath but there was no sign of the cause. The vessel made its way back to Ardglass and part of the deck had to be lifted because it was so badly damaged, and another section was ripped off. Mr James added: 'It is a bl***y mess.' He said Royal Navy protocols mean an incident like this would not happen involving a British submarine. He said: 'It is possible that it was a Russian submarine. Another recent alert: This week RAF Typhoons were launched to intercept two Russian aircraft, believed to be 'Bear' bombers, (stock image) near UK air space . No explanation: Experts said Russian President Vladimir Putin's move to send planes capable of carrying cruise missiles so close to British shores could be seen as an act of aggression . 'You cannot always prevent it but if an incident like this did happen the (Royal Navy) protocols said that the submarine would immediately surface to check on the health and welfare of the people involved and this one did not. 'Paul Murphy, the skipper, said that he sat for five to 10 minutes catching his breath to see if the submarine would surface. 'It was a submarine, it had to be, it could not have been anything else.' The incident came as Britain hosted a Nato exercise in northern Scotland involving more than 50 warships. Separately, the MoD has said RAF Typhoons, from RAF Lossiemouth, were deployed 'after Russian aircraft were identified flying close to UK air space'. It said it could not comment on Royal Navy submarine movements or the fishing vessel incident. Tensions over the Ukraine conflict have soured relations between the West and Russia, which is suffering from US and EU sanctions imposed because the Kremlin is backing separatists in eastern Ukraine. A map showing how jets were launched from RAF Lossiemouth on Monday, shortly after HMS Argyll was deployed to monitor a Russian destroyer and two other ships as they passed through the English Channel . Mr James, chief executive of the Northern Ireland Fish Producers' Organisation, said: 'There has been Russian activity. There have been Allied exercises going on, the Russians have been taking an interest in it. 'The question mark now is what kind of a submarine was it? 'If it was Allied it should have been following an agreed protocol where this should not have happened.' Sinn Fein Northern Ireland Assembly member Chris Hazzard said the community was angry. He said: 'The fact that this submarine didn't even surface to make sure the fishermen were safe has caused considerable resentment in the area. It is totally unacceptable that a submarine would show such contempt for maritime workers. 'The skipper and his crew on the Karen, and indeed all of the local fleet, deserve to know the truth about what happened. 'Whether this is a British vessel attached to the hugely controversial Trident system or a Nato submarine in training, our local fishermen deserve justice.' the Karen was towed at 10 knots during yesterday's incident 18 miles from Ardglass on the south-east shore of Northern Ireland. the vessel was badly damaged and is believed to be caused by a Russian submarine. trawler's captain, Paul Murphy pictured holding a snapped steel cable on board his boat. last month a trawler captain claimed his boat was nearly dragged down by a Russian submarine while fishing off the Scottish coast.the Karen was towed 18 miles from Ardglass on the south-east shore of Northern Ireland. estranged boat's captain Paul Murphy was pictured carrying a typical cable on his boat. a Russian submarine may have caused the dramatic incident.the Karen was towed at 10 knots during yesterday's incident 18 miles from Ardglass on the south-east shore of Northern Ireland. trawler is thought to have caused his boat to snagging backwards and was badly damaged following the incident. last month a trawler captain claimed his boat was nearly dragged down by a Russian submarine while fishing off the Scottish coast.b(x, c) \nx \n\nKL-DPG \n\n1 \n\nTV-DPG \n\n1 \n\nJS-DPG \n\n1 \n\n\n\nTable 8 .\n8Generation samples for summarizationFreddie Roach insisted on Saturday that Floyd Mayweather does not deserve to be ranked alongside Manny Pacquiao as the leading fighters of their generation as the two boxers put the finishing touches to their preparations for the Fight of the Century in Las Vegas a week on Saturday. Roach, Pacquiao's trainer, said he rated super-middleweight star Andre Ward and middleweight sensation Gennady Golovkin above Mayweather despite the American's unbeaten record and his status as hot favourite for the May 2 showdown against Pacquiao. 'Mayweather is undefeated so you have to give him a little credit for that,' said Roach, 'but he has picked and chosen his opponents and I don't think he's fought enough competition to be considered the best. You have to fight the best to be the best, I feel. He's ducked a lot of guys. Manny Pacquiao's trainer Freddie Roach says that Floyd Mayweather cannot be considered best ever . Roach says that Mayweather has picked and chosen his fights during his career . '\n\nTable 10 .\n10Generation samples for summarization\nSee App. G for the experiment in the opposite direction.\n\nWasserstein generative adversarial networks. M Arjovsky, S Chintala, L Bottou, PMLRof Proceedings of Machine Learning Research. Precup, D. and Teh, Y. W.70Proc. of ICMLArjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In Precup, D. and Teh, Y. W. (eds.), Proc. of ICML, volume 70 of Proceedings of Machine Learning Research, pp. 214-223. PMLR, 2017. URL http://proceedings.mlr.press/ v70/arjovsky17a.html.\n\nA general language assistant as a laboratory for alignment. A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N Dassarma, arXiv:2112.00861arXiv preprintAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. URL https://arxiv.org/abs/2112.00861.\n\nAn actorcritic algorithm for sequence prediction. D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe, J Pineau, A C Courville, Y Bengio, Proc. of ICLR. OpenReview.net. of ICLR. OpenReview.netBahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A. C., and Bengio, Y. An actor- critic algorithm for sequence prediction. In Proc. of ICLR. OpenReview.net, 2017. URL https://openreview. net/forum?id=SJDaqqveg.\n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Das-Sarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan, Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das- Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernan- dez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training a helpful and harmless assistant with rein- forcement learning from human feedback, 2022a. URL https://arxiv.org/abs/2204.05862.\n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Das-Sarma, D Drain, S Fort, D Ganguli, T Henighan, abs/2204.05862ArXiv preprintBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das- Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with rein- forcement learning from human feedback. ArXiv preprint, abs/2204.05862, 2022b. URL https://arxiv.org/ abs/2204.05862.\n\nInfinite-horizon policy-gradient estimation. J Baxter, P L Bartlett, Journal of Artificial Intelligence Research. 15Baxter, J. and Bartlett, P. L. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319-350, 2001.\n\nA maximum entropy approach to natural language processing. A L Berger, S A Della Pietra, Della Pietra, V J , Computational Linguistics. 221Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A maximum entropy approach to natural language process- ing. Computational Linguistics, 22(1):39-71, 1996. URL https://aclanthology.org/J96-1002.\n\nS Black, L Gao, P Wang, C Leahy, S Biderman, Gpt-Neo, 10.5281/zenodo.5297715Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021. URL https: //doi.org/10.5281/zenodo.5297715.\n\nLanguage models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, H Larochelle, M Ranzato, R Hadsell, M Balcan, Lin , Proc. of NeurIPS. H.of NeurIPSBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Proc. of NeurIPS, 2020. URL https://proceedings.\n\nTheory-grounded measurement of U.S. social stereotypes in English language models. Y Cao, A Sotnikova, Iii Daum\u00e9, H Rudinger, R Zou, L , 10.18653/v1/2022.naacl-main.92Proc. of NAACL-HLT. of NAACL-HLTSeattle, United StatesAssociation for Computational LinguisticsCao, Y., Sotnikova, A., Daum\u00e9 III, H., Rudinger, R., and Zou, L. Theory-grounded measurement of U.S. social stereotypes in English language models. In Proc. of NAACL-HLT, pp. 1276-1295, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.92. URL https: //aclanthology.org/2022.naacl-main.92.\n\nMode regularized generative adversarial networks. T Che, Y Li, A P Jacob, Y Bengio, Li , W , Proc. of ICLR. OpenReview.net. of ICLR. OpenReview.netChe, T., Li, Y., Jacob, A. P., Bengio, Y., and Li, W. Mode regularized generative adversarial networks. In Proc. of ICLR. OpenReview.net, 2017. URL https: //openreview.net/forum?id=HJKkY35le.\n\nDistributional reinforcement learning for energy-based sequential models. T Parshakova, J.-M Andreoli, M Dymetman, abs/1912.08517ArXiv preprintParshakova, T., Andreoli, J.-M., and Dymetman, M. Distri- butional reinforcement learning for energy-based sequen- tial models. ArXiv preprint, abs/1912.08517, 2019. URL https://arxiv.org/abs/1912.08517.\n\nReinforced video captioning with entailment rewards. R Pasunuru, M Bansal, 10.18653/v1/D17-1103Proc. of EMNLP. of EMNLPCopenhagen, DenmarkAssociation for Computational LinguisticsPasunuru, R. and Bansal, M. Reinforced video caption- ing with entailment rewards. In Proc. of EMNLP, pp. 979-985, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1103. URL https://aclanthology.org/D17-1103.\n\nAn imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A K\u00f6pf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Pytorch, Proc. of NeurIPS. Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R.of NeurIPSPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K\u00f6pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R. (eds.), Proc. of NeurIPS, pp. 8024-8035, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740- Abstract.html.\n\nA deep reinforced model for abstractive summarization. R Paulus, C Xiong, R Socher, Proc. of ICLR. OpenReview.net. of ICLR. OpenReview.netPaulus, R., Xiong, C., and Socher, R. A deep reinforced model for abstractive summarization. In Proc. of ICLR. OpenReview.net, 2018. URL https://openreview. net/forum?id=HkAClQgA-.\n\nReinforcement learning by reward-weighted regression for operational space control. J Peters, S Schaal, 10.1145/1273496.1273590doi: 10.1145/ 1273496.1273590ACM International Conference Proceeding Series. Ghahramani, Z.ACMProc. of ICMLPeters, J. and Schaal, S. Reinforcement learning by reward-weighted regression for operational space con- trol. In Ghahramani, Z. (ed.), Proc. of ICML, vol- ume 227 of ACM International Conference Proceed- ing Series, pp. 745-750. ACM, 2007. doi: 10.1145/ 1273496.1273590. URL https://doi.org/10. 1145/1273496.1273590.\n\n. Y Polyanskiy, -divergencesPolyanskiy, Y. f -divergences, 2019. URL https://people.lids.mit.edu/yp/ homepage/data/LN_fdiv.pdf.\n\nCold decoding: Energy-based constrained text generation with langevin dynamics. L Qin, S Welleck, D Khashabi, Y Choi, abs/2202.11705ArXiv preprintQin, L., Welleck, S., Khashabi, D., and Choi, Y. Cold de- coding: Energy-based constrained text generation with langevin dynamics. ArXiv preprint, abs/2202.11705, 2022. URL https://arxiv.org/abs/2202.\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 21140Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.\n\nSequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, Proc. of ICLR. Bengio, Y. and LeCun, Y.of ICLRRanzato, M., Chopra, S., Auli, M., and Zaremba, W. Se- quence level training with recurrent neural networks. In Bengio, Y. and LeCun, Y. (eds.), Proc. of ICLR, 2016. URL http://arxiv.org/abs/1511.06732.\n\nEthical and social risks of harm from language models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, Z Kenton, S Brown, W Hawkins, T Stepleton, C Biles, A Birhane, J Haas, L Rimell, L A Hendricks, W S Isaac, S Legassick, G Irving, I Gabriel, abs/2112.04359ArXiv preprintWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., Isaac, W. S., Legassick, S., Irving, G., and Gabriel, I. Ethical and social risks of harm from language models. ArXiv preprint, abs/2112.04359, 2021. URL https: //arxiv.org/abs/2112.04359.\n\nChallenges in detoxifying language models. J Welbl, A Glaese, J Uesato, S Dathathri, J Mellor, L A Hendricks, K Anderson, P Kohli, B Coppin, P.-S Huang, 10.18653/v1/2021.findings-emnlp.210Findings of EMNLP. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. Challenges in detoxifying language models. In Findings of EMNLP, pp. 2447-2469, Punta Cana, Dominican Republic, 2021. Association for Com- putational Linguistics. doi: 10.18653/v1/2021.findings- emnlp.210. URL https://aclanthology.org/ 2021.findings-emnlp.210.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3):229-256, 1992.\n\nTransformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T Le Scao, S Gugger, M Drame, Q Lhoest, A Rush, 10.18653/v1/2020.emnlp-demos.6Proc. of EMNLP. of EMNLPOnlineAssociation for Computational LinguisticsWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transform- ers: State-of-the-art natural language processing. In Proc. of EMNLP, pp. 38-45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-demos.6. URL https://aclanthology. org/2020.emnlp-demos.6.\n\nBot-adversarial dialogue for safe conversational agents. J Xu, D Ju, M Li, Y.-L Boureau, J Weston, E Dinan, 10.18653/v1/2021.naacl-main.235Proc. of NAACL-HLT. of NAACL-HLTAssociation for Computational LinguisticsOnline, 2021Xu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Di- nan, E. Bot-adversarial dialogue for safe conversational agents. In Proc. of NAACL-HLT, pp. 2950-2968, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.235. URL https:// aclanthology.org/2021.naacl-main.235.\n\nSTar: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Proc. of NeurIPS. Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K.of NeurIPSZelikman, E., Wu, Y., Mu, J., and Goodman, N. STar: Bootstrapping reasoning with reasoning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Proc. of NeurIPS, 2022. URL https://openreview.net/ forum?id=_3ELRdg2sgI.\n\nEthical-advice taker: Do language models understand natural language interventions?. J Zhao, D Khashabi, T Khot, A Sabharwal, Chang , K.-W , 10.18653/v1/2021.findings-acl.364Online, 2021. Association for Computational Linguistics. Findings of ACLZhao, J., Khashabi, D., Khot, T., Sabharwal, A., and Chang, K.-W. Ethical-advice taker: Do language models under- stand natural language interventions? In Findings of ACL, pp. 4158-4164, Online, 2021. Association for Computa- tional Linguistics. doi: 10.18653/v1/2021.findings-acl. 364. URL https://aclanthology.org/2021. findings-acl.364.\n\nTexygen: A benchmarking platform for text generation models. Y Zhu, S Lu, L Zheng, J Guo, W Zhang, J Wang, Yu , Y , 10.1145/3209978.3210080doi: 10. 1145/3209978.3210080Proc. of SIGIR. Collins-Thompson, K., Mei, Q., Davison, B. D., Liu, Y., and Yilmaz, E.of SIGIRACMZhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y. Texygen: A benchmarking platform for text generation models. In Collins-Thompson, K., Mei, Q., Davison, B. D., Liu, Y., and Yilmaz, E. (eds.), Proc. of SIGIR, pp. 1097-1100. ACM, 2018. doi: 10. 1145/3209978.3210080. URL https://doi.org/ 10.1145/3209978.3210080.\n\nFine-tuning language models from human preferences. D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, abs/1909.08593ArXiv preprintZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. ArXiv preprint, abs/1909.08593, 2019. URL https://arxiv.org/ abs/1909.08593.\n\nAdversarial training for high-stakes reliability. D M Ziegler, S Nix, L Chan, T Bauman, P Schmidt-Nielsen, T Lin, A Scherlis, N Nabeshima, B Weinstein-Raun, D De Haas, B Shlegeris, Thomas , N , 10.48550/arXiv.2205.01663CoRR2022Ziegler, D. M., Nix, S., Chan, L., Bauman, T., Schmidt- Nielsen, P., Lin, T., Scherlis, A., Nabeshima, N., Weinstein-Raun, B., de Haas, D., Shlegeris, B., and Thomas, N. Adversarial training for high-stakes reliabil- ity. CoRR, abs/2205.01663, 2022. doi: 10.48550/arXiv. 2205.01663. URL https://doi.org/10.48550/ arXiv.2205.01663.\n\n2021), defined as [|NER(x) \u2229 NER(c)|]/|NER(x)| , the percentage of named entities in the summary that can be found in the source. Low precision-source indicates severe hallucination. Nan , Precision-source. Precision-source (Nan et al., 2021), defined as [|NER(x) \u2229 NER(c)|]/|NER(x)| , the percentage of named entities in the summary that can be found in the source. Low precision-source indicates severe hallucination.\n\n2021), defined as [|NER(x) \u2229 NER(c)|]/|NER(t)|, the percentage of named entities in the target summary t that can be found in the generated summary x. Recall-Target ; Nan, Recall-target (Nan et al., 2021), defined as [|NER(x) \u2229 NER(c)|]/|NER(t)|, the percentage of named entities in the target summary t that can be found in the generated summary x.\n\na measure of summarization quality in terms of unigram overlap between the source document and ground truth summary. ( Rouge, Lin, Rouge (Lin, 2004), a measure of summarization quality in terms of unigram overlap between the source document and ground truth summary.\n\nMetrics for code generation We evaluate the quality of generated Python functions using the following metrics: 1. PEP8 error count, the average number of violations of PEP8. Metrics for code generation We evaluate the quality of generated Python functions using the following metrics: 1. PEP8 error count, the average number of violations of PEP8.\n\nCompilability, the fraction of samples. c, x] that compileCompilability, the fraction of samples [c, x] that compile.\n\n10 presents the evolution of metrics in Code generation. Consistent with the result on factual summarization, f -DPG increases the fraction of compilable functions, while decreasing the average number of PEP8 violations. Again, JS-DPG leads to better convergence to p than KL-DPG used in. Korbak et al.Results FigResults Fig. 10 presents the evolution of metrics in Code generation. Consistent with the result on factual summarization, f -DPG increases the fraction of compilable functions, while decreasing the average number of PEP8 violations. Again, JS-DPG leads to better convergence to p than KL-DPG used in Korbak et al. (2022a).\n", "annotations": {"author": "[{\"end\":93,\"start\":80},{\"end\":108,\"start\":94},{\"end\":127,\"start\":109},{\"end\":138,\"start\":128},{\"end\":151,\"start\":139},{\"end\":166,\"start\":152}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":90},{\"end\":107,\"start\":101},{\"end\":126,\"start\":116},{\"end\":137,\"start\":132},{\"end\":150,\"start\":147},{\"end\":165,\"start\":157}]", "author_first_name": "[{\"end\":89,\"start\":80},{\"end\":100,\"start\":94},{\"end\":115,\"start\":109},{\"end\":131,\"start\":128},{\"end\":146,\"start\":139},{\"end\":156,\"start\":152}]", "author_affiliation": null, "title": "[{\"end\":77,\"start\":1},{\"end\":243,\"start\":167}]", "venue": null, "abstract": "[{\"end\":1727,\"start\":245}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2104,\"start\":2084},{\"end\":2128,\"start\":2104},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2241,\"start\":2220},{\"end\":2281,\"start\":2260},{\"end\":2314,\"start\":2296},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2352,\"start\":2334},{\"end\":2523,\"start\":2502},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2589,\"start\":2568},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2607,\"start\":2589},{\"end\":2741,\"start\":2719},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2952,\"start\":2931},{\"end\":3022,\"start\":3001},{\"end\":3038,\"start\":3022},{\"end\":3106,\"start\":3079},{\"end\":3123,\"start\":3106},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3142,\"start\":3123},{\"end\":3161,\"start\":3142},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3220,\"start\":3197},{\"end\":3242,\"start\":3220},{\"end\":3261,\"start\":3242},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3465,\"start\":3444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3483,\"start\":3465},{\"end\":3503,\"start\":3483},{\"end\":3535,\"start\":3514},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3940,\"start\":3916},{\"end\":4220,\"start\":4204},{\"end\":4240,\"start\":4220},{\"end\":4260,\"start\":4240},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4265,\"start\":4260},{\"end\":4558,\"start\":4536},{\"end\":6722,\"start\":6696},{\"end\":7040,\"start\":7018},{\"end\":7061,\"start\":7040},{\"end\":7261,\"start\":7235},{\"end\":7582,\"start\":7578},{\"end\":8670,\"start\":8650},{\"end\":9021,\"start\":8995},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9053,\"start\":9031},{\"end\":9075,\"start\":9053},{\"end\":9095,\"start\":9075},{\"end\":9115,\"start\":9095},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9133,\"start\":9115},{\"end\":9309,\"start\":9288},{\"end\":9821,\"start\":9793},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9842,\"start\":9821},{\"end\":9862,\"start\":9842},{\"end\":10074,\"start\":10058},{\"end\":10094,\"start\":10074},{\"end\":10114,\"start\":10094},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10119,\"start\":10114},{\"end\":10461,\"start\":10439},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10570,\"start\":10547},{\"end\":10766,\"start\":10745},{\"end\":11736,\"start\":11712},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11753,\"start\":11736},{\"end\":11886,\"start\":11865},{\"end\":11907,\"start\":11886},{\"end\":11964,\"start\":11944},{\"end\":12003,\"start\":11964},{\"end\":12022,\"start\":12003},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13024,\"start\":13006},{\"end\":13044,\"start\":13024},{\"end\":13225,\"start\":13224},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13237,\"start\":13236},{\"end\":13421,\"start\":13420},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15603,\"start\":15602},{\"end\":16180,\"start\":16170},{\"end\":16473,\"start\":16463},{\"end\":16477,\"start\":16473},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16605,\"start\":16589},{\"end\":16950,\"start\":16928},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17144,\"start\":17119},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17426,\"start\":17411},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17453,\"start\":17428},{\"end\":17475,\"start\":17453},{\"end\":17527,\"start\":17505},{\"end\":18231,\"start\":18230},{\"end\":18470,\"start\":18435},{\"end\":18488,\"start\":18470},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19729,\"start\":19704},{\"end\":22351,\"start\":22327},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23418,\"start\":23396},{\"end\":23439,\"start\":23418},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23547,\"start\":23526},{\"end\":24088,\"start\":24050},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24368,\"start\":24347},{\"end\":24481,\"start\":24478},{\"end\":25944,\"start\":25930},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25961,\"start\":25944},{\"end\":25984,\"start\":25961},{\"end\":28677,\"start\":28655},{\"end\":28989,\"start\":28966},{\"end\":29224,\"start\":29204},{\"end\":29494,\"start\":29473},{\"end\":30323,\"start\":30302},{\"end\":30588,\"start\":30567},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30642,\"start\":30621},{\"end\":30725,\"start\":30704},{\"end\":30742,\"start\":30725},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31168,\"start\":31151},{\"end\":32108,\"start\":32087},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33080,\"start\":33061},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34272,\"start\":34251},{\"end\":37948,\"start\":37927},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":42462,\"start\":42444},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":45902,\"start\":45886},{\"end\":46796,\"start\":46775},{\"end\":46809,\"start\":46796},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":46906,\"start\":46883},{\"end\":46927,\"start\":46906},{\"end\":47121,\"start\":47104},{\"end\":47146,\"start\":47121},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47166,\"start\":47148},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":47520,\"start\":47502},{\"end\":47540,\"start\":47520},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":47751,\"start\":47730},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47800,\"start\":47781},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48164,\"start\":48146},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":48428,\"start\":48409},{\"end\":48499,\"start\":48481},{\"end\":49078,\"start\":49064},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":49095,\"start\":49078},{\"end\":49118,\"start\":49095},{\"end\":50281,\"start\":50260},{\"end\":51205,\"start\":51184},{\"end\":52272,\"start\":52250},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":52714,\"start\":52694},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":52763,\"start\":52744},{\"end\":54486,\"start\":54467},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":55670,\"start\":55649},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":56547,\"start\":56526},{\"end\":56603,\"start\":56579},{\"end\":56746,\"start\":56728},{\"end\":58361,\"start\":58339},{\"end\":61227,\"start\":61202},{\"end\":65002,\"start\":64982},{\"end\":67169,\"start\":67131},{\"end\":67593,\"start\":67580},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":81615,\"start\":81593},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":81680,\"start\":81659},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":81800,\"start\":81775},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":81906,\"start\":81884},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":81928,\"start\":81906},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":83189,\"start\":83170}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":78797,\"start\":78653},{\"attributes\":{\"id\":\"fig_1\"},\"end\":79061,\"start\":78798},{\"attributes\":{\"id\":\"fig_2\"},\"end\":79235,\"start\":79062},{\"attributes\":{\"id\":\"fig_4\"},\"end\":79305,\"start\":79236},{\"attributes\":{\"id\":\"fig_5\"},\"end\":79611,\"start\":79306},{\"attributes\":{\"id\":\"fig_7\"},\"end\":79789,\"start\":79612},{\"attributes\":{\"id\":\"fig_8\"},\"end\":80040,\"start\":79790},{\"attributes\":{\"id\":\"fig_9\"},\"end\":80243,\"start\":80041},{\"attributes\":{\"id\":\"fig_10\"},\"end\":80429,\"start\":80244},{\"attributes\":{\"id\":\"fig_11\"},\"end\":81302,\"start\":80430},{\"attributes\":{\"id\":\"fig_12\"},\"end\":81567,\"start\":81303},{\"attributes\":{\"id\":\"fig_13\"},\"end\":83971,\"start\":81568},{\"attributes\":{\"id\":\"fig_14\"},\"end\":84488,\"start\":83972},{\"attributes\":{\"id\":\"fig_15\"},\"end\":84854,\"start\":84489},{\"attributes\":{\"id\":\"fig_16\"},\"end\":85640,\"start\":84855},{\"attributes\":{\"id\":\"fig_17\"},\"end\":85724,\"start\":85641},{\"attributes\":{\"id\":\"fig_18\"},\"end\":86103,\"start\":85725},{\"attributes\":{\"id\":\"fig_19\"},\"end\":86340,\"start\":86104},{\"attributes\":{\"id\":\"fig_20\"},\"end\":87184,\"start\":86341},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":99471,\"start\":87185},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":99574,\"start\":99472},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":100634,\"start\":99575},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":102877,\"start\":100635},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":109733,\"start\":102878},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":110782,\"start\":109734},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":110833,\"start\":110783}]", "paragraph": "[{\"end\":4559,\"start\":1743},{\"end\":5440,\"start\":4561},{\"end\":7398,\"start\":5442},{\"end\":7448,\"start\":7400},{\"end\":7670,\"start\":7450},{\"end\":7801,\"start\":7672},{\"end\":7982,\"start\":7803},{\"end\":8331,\"start\":7997},{\"end\":9134,\"start\":8366},{\"end\":9250,\"start\":9136},{\"end\":9353,\"start\":9252},{\"end\":9620,\"start\":9383},{\"end\":10181,\"start\":9622},{\"end\":10376,\"start\":10234},{\"end\":10462,\"start\":10411},{\"end\":10883,\"start\":10464},{\"end\":11299,\"start\":10929},{\"end\":12086,\"start\":11341},{\"end\":12257,\"start\":12088},{\"end\":12611,\"start\":12336},{\"end\":12660,\"start\":12630},{\"end\":12864,\"start\":12700},{\"end\":13293,\"start\":12936},{\"end\":13539,\"start\":13295},{\"end\":13770,\"start\":13604},{\"end\":14036,\"start\":13772},{\"end\":14486,\"start\":14085},{\"end\":14571,\"start\":14488},{\"end\":14996,\"start\":14598},{\"end\":15397,\"start\":14998},{\"end\":15520,\"start\":15399},{\"end\":15695,\"start\":15574},{\"end\":15977,\"start\":15760},{\"end\":16104,\"start\":15979},{\"end\":16478,\"start\":16110},{\"end\":16730,\"start\":16480},{\"end\":16844,\"start\":16783},{\"end\":17266,\"start\":16868},{\"end\":17699,\"start\":17288},{\"end\":17702,\"start\":17701},{\"end\":17910,\"start\":17830},{\"end\":18060,\"start\":18002},{\"end\":18063,\"start\":18062},{\"end\":18508,\"start\":18085},{\"end\":19449,\"start\":18956},{\"end\":19584,\"start\":19486},{\"end\":19805,\"start\":19586},{\"end\":19902,\"start\":19870},{\"end\":19992,\"start\":19963},{\"end\":20261,\"start\":19994},{\"end\":20508,\"start\":20399},{\"end\":20630,\"start\":20510},{\"end\":20633,\"start\":20632},{\"end\":20705,\"start\":20689},{\"end\":21399,\"start\":20722},{\"end\":21793,\"start\":21436},{\"end\":21871,\"start\":21795},{\"end\":22975,\"start\":21934},{\"end\":23061,\"start\":22977},{\"end\":23331,\"start\":23063},{\"end\":23440,\"start\":23333},{\"end\":23631,\"start\":23442},{\"end\":23759,\"start\":23654},{\"end\":24576,\"start\":23797},{\"end\":25656,\"start\":24578},{\"end\":26218,\"start\":25658},{\"end\":26985,\"start\":26257},{\"end\":28438,\"start\":26997},{\"end\":29548,\"start\":28484},{\"end\":29764,\"start\":29560},{\"end\":30484,\"start\":29868},{\"end\":30818,\"start\":30527},{\"end\":31393,\"start\":30876},{\"end\":31824,\"start\":31405},{\"end\":32109,\"start\":31826},{\"end\":33427,\"start\":32138},{\"end\":33565,\"start\":33446},{\"end\":34362,\"start\":33567},{\"end\":36798,\"start\":34364},{\"end\":37339,\"start\":36828},{\"end\":37708,\"start\":37341},{\"end\":38027,\"start\":37805},{\"end\":38180,\"start\":38029},{\"end\":38484,\"start\":38278},{\"end\":38990,\"start\":38486},{\"end\":39323,\"start\":39216},{\"end\":39544,\"start\":39518},{\"end\":39674,\"start\":39591},{\"end\":39814,\"start\":39676},{\"end\":40048,\"start\":39886},{\"end\":40629,\"start\":40189},{\"end\":40911,\"start\":40631},{\"end\":41308,\"start\":41214},{\"end\":41380,\"start\":41336},{\"end\":41594,\"start\":41382},{\"end\":41700,\"start\":41660},{\"end\":43125,\"start\":42319},{\"end\":43423,\"start\":43127},{\"end\":43576,\"start\":43509},{\"end\":43929,\"start\":43618},{\"end\":43980,\"start\":43931},{\"end\":44338,\"start\":44069},{\"end\":44399,\"start\":44340},{\"end\":44477,\"start\":44401},{\"end\":44585,\"start\":44479},{\"end\":44870,\"start\":44608},{\"end\":44973,\"start\":44913},{\"end\":45187,\"start\":45034},{\"end\":45396,\"start\":45189},{\"end\":46030,\"start\":45678},{\"end\":46137,\"start\":46032},{\"end\":46517,\"start\":46139},{\"end\":47658,\"start\":46519},{\"end\":47994,\"start\":47688},{\"end\":48129,\"start\":47996},{\"end\":48235,\"start\":48131},{\"end\":48500,\"start\":48237},{\"end\":49290,\"start\":48512},{\"end\":50008,\"start\":49365},{\"end\":50569,\"start\":50010},{\"end\":51420,\"start\":50601},{\"end\":51502,\"start\":51458},{\"end\":51701,\"start\":51648},{\"end\":51927,\"start\":51703},{\"end\":52764,\"start\":52103},{\"end\":52925,\"start\":52794},{\"end\":53061,\"start\":52927},{\"end\":53600,\"start\":53177},{\"end\":53747,\"start\":53677},{\"end\":54570,\"start\":53889},{\"end\":55988,\"start\":54596},{\"end\":56514,\"start\":56040},{\"end\":57111,\"start\":56516},{\"end\":57570,\"start\":57113},{\"end\":57894,\"start\":57615},{\"end\":58184,\"start\":57896},{\"end\":58840,\"start\":58186},{\"end\":63259,\"start\":58933},{\"end\":63457,\"start\":63274},{\"end\":63637,\"start\":63459},{\"end\":63858,\"start\":63639},{\"end\":64026,\"start\":63860},{\"end\":64264,\"start\":64028},{\"end\":64417,\"start\":64266},{\"end\":65229,\"start\":64419},{\"end\":66825,\"start\":65231},{\"end\":71327,\"start\":66827},{\"end\":71338,\"start\":71329},{\"end\":71500,\"start\":71349},{\"end\":71567,\"start\":71502},{\"end\":71895,\"start\":71582},{\"end\":72210,\"start\":71906},{\"end\":72890,\"start\":72212},{\"end\":77768,\"start\":72892},{\"end\":78143,\"start\":77770},{\"end\":78309,\"start\":78154},{\"end\":78410,\"start\":78311},{\"end\":78652,\"start\":78421}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9382,\"start\":9354},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10233,\"start\":10182},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10410,\"start\":10377},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10928,\"start\":10884},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12318,\"start\":12258},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12699,\"start\":12661},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12935,\"start\":12865},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13603,\"start\":13540},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14597,\"start\":14572},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15573,\"start\":15521},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15759,\"start\":15696},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16109,\"start\":16105},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16782,\"start\":16731},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16867,\"start\":16845},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17829,\"start\":17703},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18001,\"start\":17911},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18084,\"start\":18064},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18955,\"start\":18509},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19869,\"start\":19806},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19962,\"start\":19903},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20398,\"start\":20262},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20688,\"start\":20634},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21919,\"start\":21872},{\"attributes\":{\"id\":\"formula_23\"},\"end\":29867,\"start\":29765},{\"attributes\":{\"id\":\"formula_24\"},\"end\":30875,\"start\":30819},{\"attributes\":{\"id\":\"formula_25\"},\"end\":38277,\"start\":38181},{\"attributes\":{\"id\":\"formula_26\"},\"end\":39094,\"start\":39017},{\"attributes\":{\"id\":\"formula_27\"},\"end\":39171,\"start\":39094},{\"attributes\":{\"id\":\"formula_28\"},\"end\":39215,\"start\":39171},{\"attributes\":{\"id\":\"formula_29\"},\"end\":39517,\"start\":39324},{\"attributes\":{\"id\":\"formula_30\"},\"end\":39885,\"start\":39815},{\"attributes\":{\"id\":\"formula_31\"},\"end\":40188,\"start\":40049},{\"attributes\":{\"id\":\"formula_32\"},\"end\":41213,\"start\":40912},{\"attributes\":{\"id\":\"formula_33\"},\"end\":41659,\"start\":41595},{\"attributes\":{\"id\":\"formula_34\"},\"end\":42318,\"start\":41701},{\"attributes\":{\"id\":\"formula_35\"},\"end\":43508,\"start\":43424},{\"attributes\":{\"id\":\"formula_36\"},\"end\":44068,\"start\":43981},{\"attributes\":{\"id\":\"formula_37\"},\"end\":44912,\"start\":44871},{\"attributes\":{\"id\":\"formula_38\"},\"end\":45033,\"start\":44974},{\"attributes\":{\"id\":\"formula_39\"},\"end\":45650,\"start\":45397},{\"attributes\":{\"id\":\"formula_40\"},\"end\":49316,\"start\":49291},{\"attributes\":{\"id\":\"formula_41\"},\"end\":50600,\"start\":50570},{\"attributes\":{\"id\":\"formula_42\"},\"end\":51457,\"start\":51421},{\"attributes\":{\"id\":\"formula_43\"},\"end\":51647,\"start\":51503},{\"attributes\":{\"id\":\"formula_44\"},\"end\":52036,\"start\":51928},{\"attributes\":{\"id\":\"formula_45\"},\"end\":53176,\"start\":53062},{\"attributes\":{\"id\":\"formula_46\"},\"end\":53676,\"start\":53601},{\"attributes\":{\"id\":\"formula_47\"},\"end\":53888,\"start\":53748},{\"attributes\":{\"id\":\"formula_48\"},\"end\":58932,\"start\":58841}]", "table_ref": "[{\"end\":14035,\"start\":14032},{\"end\":18969,\"start\":18962},{\"end\":28150,\"start\":28147},{\"end\":37390,\"start\":37387},{\"end\":39927,\"start\":39920},{\"end\":40592,\"start\":40585},{\"end\":47808,\"start\":47801},{\"end\":64576,\"start\":64569},{\"end\":72172,\"start\":72165}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1741,\"start\":1729},{\"attributes\":{\"n\":\"2.\"},\"end\":7995,\"start\":7985},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8364,\"start\":8334},{\"attributes\":{\"n\":\"2.2.\"},\"end\":11339,\"start\":11302},{\"attributes\":{\"n\":\"3.\"},\"end\":12334,\"start\":12320},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12628,\"start\":12614},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14083,\"start\":14039},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17286,\"start\":17269},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19484,\"start\":19452},{\"attributes\":{\"n\":\"3.5.\"},\"end\":20720,\"start\":20708},{\"attributes\":{\"n\":\"3.6.\"},\"end\":21434,\"start\":21402},{\"attributes\":{\"n\":\"4.\"},\"end\":21932,\"start\":21921},{\"attributes\":{\"n\":\"4.\"},\"end\":23652,\"start\":23634},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23795,\"start\":23762},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26255,\"start\":26221},{\"end\":26995,\"start\":26988},{\"attributes\":{\"n\":\"4.3.\"},\"end\":28482,\"start\":28441},{\"end\":29558,\"start\":29551},{\"attributes\":{\"n\":\"4.4.\"},\"end\":30525,\"start\":30487},{\"end\":31403,\"start\":31396},{\"attributes\":{\"n\":\"4.5.\"},\"end\":32136,\"start\":32112},{\"attributes\":{\"n\":\"4.6.\"},\"end\":33444,\"start\":33430},{\"attributes\":{\"n\":\"5.\"},\"end\":36826,\"start\":36801},{\"end\":37754,\"start\":37711},{\"end\":37803,\"start\":37757},{\"end\":39016,\"start\":38993},{\"end\":39589,\"start\":39547},{\"end\":41334,\"start\":41311},{\"end\":43616,\"start\":43579},{\"end\":44606,\"start\":44588},{\"end\":45676,\"start\":45652},{\"end\":47686,\"start\":47661},{\"end\":48510,\"start\":48503},{\"end\":49363,\"start\":49318},{\"end\":52101,\"start\":52038},{\"end\":52792,\"start\":52767},{\"end\":54594,\"start\":54573},{\"end\":56038,\"start\":55991},{\"end\":57613,\"start\":57573},{\"end\":63272,\"start\":63262},{\"end\":71347,\"start\":71341},{\"end\":71576,\"start\":71570},{\"end\":71580,\"start\":71579},{\"end\":71904,\"start\":71898},{\"end\":78152,\"start\":78146},{\"end\":78419,\"start\":78413},{\"end\":78809,\"start\":78799},{\"end\":79068,\"start\":79063},{\"end\":79315,\"start\":79307},{\"end\":79623,\"start\":79613},{\"end\":79801,\"start\":79791},{\"end\":80052,\"start\":80042},{\"end\":80255,\"start\":80245},{\"end\":81307,\"start\":81304},{\"end\":83994,\"start\":83973},{\"end\":84501,\"start\":84490},{\"end\":84900,\"start\":84856},{\"end\":85653,\"start\":85642},{\"end\":85748,\"start\":85726},{\"end\":86116,\"start\":86105},{\"end\":99482,\"start\":99473},{\"end\":100645,\"start\":100636},{\"end\":102888,\"start\":102879},{\"end\":109744,\"start\":109735},{\"end\":110794,\"start\":110784}]", "table": "[{\"end\":99471,\"start\":97073},{\"end\":100634,\"start\":100142},{\"end\":102877,\"start\":100690},{\"end\":109733,\"start\":109681}]", "figure_caption": "[{\"end\":78797,\"start\":78655},{\"end\":79061,\"start\":78811},{\"end\":79235,\"start\":79070},{\"end\":79305,\"start\":79238},{\"end\":79611,\"start\":79317},{\"end\":79789,\"start\":79625},{\"end\":80040,\"start\":79803},{\"end\":80243,\"start\":80054},{\"end\":80429,\"start\":80257},{\"end\":81302,\"start\":80432},{\"end\":81567,\"start\":81308},{\"end\":83971,\"start\":81570},{\"end\":84488,\"start\":83998},{\"end\":84854,\"start\":84504},{\"end\":85640,\"start\":84909},{\"end\":85724,\"start\":85656},{\"end\":86103,\"start\":85753},{\"end\":86340,\"start\":86119},{\"end\":87184,\"start\":86343},{\"end\":97073,\"start\":87187},{\"end\":99574,\"start\":99484},{\"end\":100142,\"start\":99577},{\"end\":100690,\"start\":100647},{\"end\":109681,\"start\":102890},{\"end\":110782,\"start\":109746},{\"end\":110833,\"start\":110797}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7087,\"start\":7081},{\"end\":24592,\"start\":24586},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24680,\"start\":24673},{\"end\":25995,\"start\":25987},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27115,\"start\":27101},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27153,\"start\":27143},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27888,\"start\":27882},{\"end\":29611,\"start\":29605},{\"end\":30088,\"start\":30080},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31450,\"start\":31444},{\"end\":31584,\"start\":31576},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32554,\"start\":32546},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32826,\"start\":32819},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":35379,\"start\":35373},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":35834,\"start\":35828},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48028,\"start\":48010},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":53949,\"start\":53940},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":54646,\"start\":54639},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":55478,\"start\":55471},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":55681,\"start\":55672},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":57124,\"start\":57117},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":58046,\"start\":58037},{\"end\":58552,\"start\":58544},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":59222,\"start\":59215}]", "bib_author_first_name": "[{\"end\":110938,\"start\":110937},{\"end\":110950,\"start\":110949},{\"end\":110962,\"start\":110961},{\"end\":111398,\"start\":111397},{\"end\":111408,\"start\":111407},{\"end\":111415,\"start\":111414},{\"end\":111423,\"start\":111422},{\"end\":111432,\"start\":111431},{\"end\":111443,\"start\":111442},{\"end\":111455,\"start\":111454},{\"end\":111464,\"start\":111463},{\"end\":111474,\"start\":111473},{\"end\":111482,\"start\":111481},{\"end\":111835,\"start\":111834},{\"end\":111847,\"start\":111846},{\"end\":111857,\"start\":111856},{\"end\":111863,\"start\":111862},{\"end\":111872,\"start\":111871},{\"end\":111880,\"start\":111879},{\"end\":111890,\"start\":111889},{\"end\":111892,\"start\":111891},{\"end\":111905,\"start\":111904},{\"end\":112301,\"start\":112300},{\"end\":112308,\"start\":112307},{\"end\":112317,\"start\":112316},{\"end\":112328,\"start\":112327},{\"end\":112338,\"start\":112337},{\"end\":112346,\"start\":112345},{\"end\":112359,\"start\":112358},{\"end\":112368,\"start\":112367},{\"end\":112376,\"start\":112375},{\"end\":112387,\"start\":112386},{\"end\":112399,\"start\":112398},{\"end\":112409,\"start\":112408},{\"end\":112421,\"start\":112420},{\"end\":112432,\"start\":112431},{\"end\":112443,\"start\":112442},{\"end\":112455,\"start\":112454},{\"end\":112465,\"start\":112464},{\"end\":112483,\"start\":112482},{\"end\":112496,\"start\":112495},{\"end\":112504,\"start\":112503},{\"end\":112516,\"start\":112515},{\"end\":112526,\"start\":112525},{\"end\":112536,\"start\":112535},{\"end\":112545,\"start\":112544},{\"end\":112555,\"start\":112554},{\"end\":112565,\"start\":112564},{\"end\":112574,\"start\":112573},{\"end\":112583,\"start\":112582},{\"end\":112597,\"start\":112596},{\"end\":112605,\"start\":112604},{\"end\":112613,\"start\":112612},{\"end\":113243,\"start\":113242},{\"end\":113250,\"start\":113249},{\"end\":113259,\"start\":113258},{\"end\":113270,\"start\":113269},{\"end\":113280,\"start\":113279},{\"end\":113288,\"start\":113287},{\"end\":113301,\"start\":113300},{\"end\":113310,\"start\":113309},{\"end\":113318,\"start\":113317},{\"end\":113329,\"start\":113328},{\"end\":113712,\"start\":113711},{\"end\":113722,\"start\":113721},{\"end\":113724,\"start\":113723},{\"end\":113982,\"start\":113981},{\"end\":113984,\"start\":113983},{\"end\":113994,\"start\":113993},{\"end\":113996,\"start\":113995},{\"end\":114016,\"start\":114011},{\"end\":114026,\"start\":114025},{\"end\":114028,\"start\":114027},{\"end\":114269,\"start\":114268},{\"end\":114278,\"start\":114277},{\"end\":114285,\"start\":114284},{\"end\":114293,\"start\":114292},{\"end\":114302,\"start\":114301},{\"end\":114637,\"start\":114636},{\"end\":114639,\"start\":114638},{\"end\":114648,\"start\":114647},{\"end\":114656,\"start\":114655},{\"end\":114665,\"start\":114664},{\"end\":114676,\"start\":114675},{\"end\":114686,\"start\":114685},{\"end\":114698,\"start\":114697},{\"end\":114713,\"start\":114712},{\"end\":114722,\"start\":114721},{\"end\":114732,\"start\":114731},{\"end\":114742,\"start\":114741},{\"end\":114753,\"start\":114752},{\"end\":114769,\"start\":114768},{\"end\":114780,\"start\":114779},{\"end\":114792,\"start\":114791},{\"end\":114801,\"start\":114800},{\"end\":114811,\"start\":114810},{\"end\":114813,\"start\":114812},{\"end\":114824,\"start\":114823},{\"end\":114830,\"start\":114829},{\"end\":114840,\"start\":114839},{\"end\":114849,\"start\":114848},{\"end\":114857,\"start\":114856},{\"end\":114867,\"start\":114866},{\"end\":114877,\"start\":114876},{\"end\":114885,\"start\":114884},{\"end\":114894,\"start\":114893},{\"end\":114903,\"start\":114902},{\"end\":114913,\"start\":114912},{\"end\":114927,\"start\":114926},{\"end\":114938,\"start\":114937},{\"end\":114951,\"start\":114950},{\"end\":114961,\"start\":114960},{\"end\":114975,\"start\":114974},{\"end\":114986,\"start\":114985},{\"end\":114997,\"start\":114996},{\"end\":115009,\"start\":115006},{\"end\":115683,\"start\":115682},{\"end\":115690,\"start\":115689},{\"end\":115705,\"start\":115702},{\"end\":115714,\"start\":115713},{\"end\":115726,\"start\":115725},{\"end\":115733,\"start\":115732},{\"end\":116259,\"start\":116258},{\"end\":116266,\"start\":116265},{\"end\":116272,\"start\":116271},{\"end\":116274,\"start\":116273},{\"end\":116283,\"start\":116282},{\"end\":116294,\"start\":116292},{\"end\":116298,\"start\":116297},{\"end\":116623,\"start\":116622},{\"end\":116640,\"start\":116636},{\"end\":116652,\"start\":116651},{\"end\":116950,\"start\":116949},{\"end\":116962,\"start\":116961},{\"end\":117389,\"start\":117388},{\"end\":117399,\"start\":117398},{\"end\":117408,\"start\":117407},{\"end\":117417,\"start\":117416},{\"end\":117426,\"start\":117425},{\"end\":117438,\"start\":117437},{\"end\":117448,\"start\":117447},{\"end\":117459,\"start\":117458},{\"end\":117466,\"start\":117465},{\"end\":117480,\"start\":117479},{\"end\":117490,\"start\":117489},{\"end\":117503,\"start\":117502},{\"end\":117511,\"start\":117510},{\"end\":117519,\"start\":117518},{\"end\":117529,\"start\":117528},{\"end\":117539,\"start\":117538},{\"end\":117549,\"start\":117548},{\"end\":117565,\"start\":117564},{\"end\":117576,\"start\":117575},{\"end\":117584,\"start\":117583},{\"end\":117591,\"start\":117590},{\"end\":118364,\"start\":118363},{\"end\":118374,\"start\":118373},{\"end\":118383,\"start\":118382},{\"end\":118713,\"start\":118712},{\"end\":118723,\"start\":118722},{\"end\":119185,\"start\":119184},{\"end\":119392,\"start\":119391},{\"end\":119399,\"start\":119398},{\"end\":119410,\"start\":119409},{\"end\":119422,\"start\":119421},{\"end\":119713,\"start\":119712},{\"end\":119724,\"start\":119723},{\"end\":119730,\"start\":119729},{\"end\":119739,\"start\":119738},{\"end\":119747,\"start\":119746},{\"end\":119757,\"start\":119756},{\"end\":120026,\"start\":120025},{\"end\":120036,\"start\":120035},{\"end\":120047,\"start\":120046},{\"end\":120058,\"start\":120057},{\"end\":120065,\"start\":120064},{\"end\":120075,\"start\":120074},{\"end\":120085,\"start\":120084},{\"end\":120093,\"start\":120092},{\"end\":120099,\"start\":120098},{\"end\":120101,\"start\":120100},{\"end\":120424,\"start\":120423},{\"end\":120435,\"start\":120434},{\"end\":120445,\"start\":120444},{\"end\":120453,\"start\":120452},{\"end\":120769,\"start\":120768},{\"end\":120782,\"start\":120781},{\"end\":120792,\"start\":120791},{\"end\":120800,\"start\":120799},{\"end\":120811,\"start\":120810},{\"end\":120821,\"start\":120820},{\"end\":120830,\"start\":120829},{\"end\":120839,\"start\":120838},{\"end\":120849,\"start\":120848},{\"end\":120858,\"start\":120857},{\"end\":120872,\"start\":120871},{\"end\":120882,\"start\":120881},{\"end\":120891,\"start\":120890},{\"end\":120902,\"start\":120901},{\"end\":120915,\"start\":120914},{\"end\":120924,\"start\":120923},{\"end\":120935,\"start\":120934},{\"end\":120943,\"start\":120942},{\"end\":120953,\"start\":120952},{\"end\":120955,\"start\":120954},{\"end\":120968,\"start\":120967},{\"end\":120970,\"start\":120969},{\"end\":120979,\"start\":120978},{\"end\":120992,\"start\":120991},{\"end\":121002,\"start\":121001},{\"end\":121512,\"start\":121511},{\"end\":121521,\"start\":121520},{\"end\":121531,\"start\":121530},{\"end\":121541,\"start\":121540},{\"end\":121554,\"start\":121553},{\"end\":121564,\"start\":121563},{\"end\":121566,\"start\":121565},{\"end\":121579,\"start\":121578},{\"end\":121591,\"start\":121590},{\"end\":121600,\"start\":121599},{\"end\":121613,\"start\":121609},{\"end\":122234,\"start\":122233},{\"end\":122236,\"start\":122235},{\"end\":122476,\"start\":122475},{\"end\":122484,\"start\":122483},{\"end\":122493,\"start\":122492},{\"end\":122501,\"start\":122500},{\"end\":122513,\"start\":122512},{\"end\":122525,\"start\":122524},{\"end\":122532,\"start\":122531},{\"end\":122542,\"start\":122541},{\"end\":122551,\"start\":122550},{\"end\":122559,\"start\":122558},{\"end\":122572,\"start\":122571},{\"end\":122583,\"start\":122582},{\"end\":122595,\"start\":122594},{\"end\":122609,\"start\":122608},{\"end\":122615,\"start\":122614},{\"end\":122626,\"start\":122625},{\"end\":122633,\"start\":122632},{\"end\":122639,\"start\":122638},{\"end\":122650,\"start\":122649},{\"end\":122660,\"start\":122659},{\"end\":122669,\"start\":122668},{\"end\":122679,\"start\":122678},{\"end\":123343,\"start\":123342},{\"end\":123349,\"start\":123348},{\"end\":123355,\"start\":123354},{\"end\":123364,\"start\":123360},{\"end\":123375,\"start\":123374},{\"end\":123385,\"start\":123384},{\"end\":123866,\"start\":123865},{\"end\":123878,\"start\":123877},{\"end\":123884,\"start\":123883},{\"end\":123890,\"start\":123889},{\"end\":124291,\"start\":124290},{\"end\":124299,\"start\":124298},{\"end\":124311,\"start\":124310},{\"end\":124319,\"start\":124318},{\"end\":124336,\"start\":124331},{\"end\":124343,\"start\":124339},{\"end\":124854,\"start\":124853},{\"end\":124861,\"start\":124860},{\"end\":124867,\"start\":124866},{\"end\":124876,\"start\":124875},{\"end\":124883,\"start\":124882},{\"end\":124892,\"start\":124891},{\"end\":124901,\"start\":124899},{\"end\":124905,\"start\":124904},{\"end\":125442,\"start\":125441},{\"end\":125444,\"start\":125443},{\"end\":125455,\"start\":125454},{\"end\":125467,\"start\":125466},{\"end\":125473,\"start\":125472},{\"end\":125475,\"start\":125474},{\"end\":125484,\"start\":125483},{\"end\":125495,\"start\":125494},{\"end\":125505,\"start\":125504},{\"end\":125519,\"start\":125518},{\"end\":125845,\"start\":125844},{\"end\":125847,\"start\":125846},{\"end\":125858,\"start\":125857},{\"end\":125865,\"start\":125864},{\"end\":125873,\"start\":125872},{\"end\":125883,\"start\":125882},{\"end\":125902,\"start\":125901},{\"end\":125909,\"start\":125908},{\"end\":125921,\"start\":125920},{\"end\":125934,\"start\":125933},{\"end\":125952,\"start\":125951},{\"end\":125963,\"start\":125962},{\"end\":125981,\"start\":125975},{\"end\":125985,\"start\":125984},{\"end\":126539,\"start\":126536},{\"end\":127243,\"start\":127242}]", "bib_author_last_name": "[{\"end\":110947,\"start\":110939},{\"end\":110959,\"start\":110951},{\"end\":110969,\"start\":110963},{\"end\":111405,\"start\":111399},{\"end\":111412,\"start\":111409},{\"end\":111420,\"start\":111416},{\"end\":111429,\"start\":111424},{\"end\":111440,\"start\":111433},{\"end\":111452,\"start\":111444},{\"end\":111461,\"start\":111456},{\"end\":111471,\"start\":111465},{\"end\":111479,\"start\":111475},{\"end\":111491,\"start\":111483},{\"end\":111844,\"start\":111836},{\"end\":111854,\"start\":111848},{\"end\":111860,\"start\":111858},{\"end\":111869,\"start\":111864},{\"end\":111877,\"start\":111873},{\"end\":111887,\"start\":111881},{\"end\":111902,\"start\":111893},{\"end\":111912,\"start\":111906},{\"end\":112305,\"start\":112302},{\"end\":112314,\"start\":112309},{\"end\":112325,\"start\":112318},{\"end\":112335,\"start\":112329},{\"end\":112343,\"start\":112339},{\"end\":112356,\"start\":112347},{\"end\":112365,\"start\":112360},{\"end\":112373,\"start\":112369},{\"end\":112384,\"start\":112377},{\"end\":112396,\"start\":112388},{\"end\":112406,\"start\":112400},{\"end\":112418,\"start\":112410},{\"end\":112429,\"start\":112422},{\"end\":112440,\"start\":112433},{\"end\":112452,\"start\":112444},{\"end\":112462,\"start\":112456},{\"end\":112480,\"start\":112466},{\"end\":112493,\"start\":112484},{\"end\":112501,\"start\":112497},{\"end\":112513,\"start\":112505},{\"end\":112523,\"start\":112517},{\"end\":112533,\"start\":112527},{\"end\":112542,\"start\":112537},{\"end\":112552,\"start\":112546},{\"end\":112562,\"start\":112556},{\"end\":112571,\"start\":112566},{\"end\":112580,\"start\":112575},{\"end\":112594,\"start\":112584},{\"end\":112602,\"start\":112598},{\"end\":112610,\"start\":112606},{\"end\":112620,\"start\":112614},{\"end\":113247,\"start\":113244},{\"end\":113256,\"start\":113251},{\"end\":113267,\"start\":113260},{\"end\":113277,\"start\":113271},{\"end\":113285,\"start\":113281},{\"end\":113298,\"start\":113289},{\"end\":113307,\"start\":113302},{\"end\":113315,\"start\":113311},{\"end\":113326,\"start\":113319},{\"end\":113338,\"start\":113330},{\"end\":113719,\"start\":113713},{\"end\":113733,\"start\":113725},{\"end\":113991,\"start\":113985},{\"end\":114009,\"start\":113997},{\"end\":114023,\"start\":114017},{\"end\":114275,\"start\":114270},{\"end\":114282,\"start\":114279},{\"end\":114290,\"start\":114286},{\"end\":114299,\"start\":114294},{\"end\":114311,\"start\":114303},{\"end\":114320,\"start\":114313},{\"end\":114645,\"start\":114640},{\"end\":114653,\"start\":114649},{\"end\":114662,\"start\":114657},{\"end\":114673,\"start\":114666},{\"end\":114683,\"start\":114677},{\"end\":114695,\"start\":114687},{\"end\":114710,\"start\":114699},{\"end\":114719,\"start\":114714},{\"end\":114729,\"start\":114723},{\"end\":114739,\"start\":114733},{\"end\":114750,\"start\":114743},{\"end\":114766,\"start\":114754},{\"end\":114777,\"start\":114770},{\"end\":114789,\"start\":114781},{\"end\":114798,\"start\":114793},{\"end\":114808,\"start\":114802},{\"end\":114821,\"start\":114814},{\"end\":114827,\"start\":114825},{\"end\":114837,\"start\":114831},{\"end\":114846,\"start\":114841},{\"end\":114854,\"start\":114850},{\"end\":114864,\"start\":114858},{\"end\":114874,\"start\":114868},{\"end\":114882,\"start\":114878},{\"end\":114891,\"start\":114886},{\"end\":114900,\"start\":114895},{\"end\":114910,\"start\":114904},{\"end\":114924,\"start\":114914},{\"end\":114935,\"start\":114928},{\"end\":114948,\"start\":114939},{\"end\":114958,\"start\":114952},{\"end\":114972,\"start\":114962},{\"end\":114983,\"start\":114976},{\"end\":114994,\"start\":114987},{\"end\":115004,\"start\":114998},{\"end\":115687,\"start\":115684},{\"end\":115700,\"start\":115691},{\"end\":115711,\"start\":115706},{\"end\":115723,\"start\":115715},{\"end\":115730,\"start\":115727},{\"end\":116263,\"start\":116260},{\"end\":116269,\"start\":116267},{\"end\":116280,\"start\":116275},{\"end\":116290,\"start\":116284},{\"end\":116634,\"start\":116624},{\"end\":116649,\"start\":116641},{\"end\":116661,\"start\":116653},{\"end\":116959,\"start\":116951},{\"end\":116969,\"start\":116963},{\"end\":117396,\"start\":117390},{\"end\":117405,\"start\":117400},{\"end\":117414,\"start\":117409},{\"end\":117423,\"start\":117418},{\"end\":117435,\"start\":117427},{\"end\":117445,\"start\":117439},{\"end\":117456,\"start\":117449},{\"end\":117463,\"start\":117460},{\"end\":117477,\"start\":117467},{\"end\":117487,\"start\":117481},{\"end\":117500,\"start\":117491},{\"end\":117508,\"start\":117504},{\"end\":117516,\"start\":117512},{\"end\":117526,\"start\":117520},{\"end\":117536,\"start\":117530},{\"end\":117546,\"start\":117540},{\"end\":117562,\"start\":117550},{\"end\":117573,\"start\":117566},{\"end\":117581,\"start\":117577},{\"end\":117588,\"start\":117585},{\"end\":117600,\"start\":117592},{\"end\":117609,\"start\":117602},{\"end\":118371,\"start\":118365},{\"end\":118380,\"start\":118375},{\"end\":118390,\"start\":118384},{\"end\":118720,\"start\":118714},{\"end\":118730,\"start\":118724},{\"end\":119196,\"start\":119186},{\"end\":119396,\"start\":119393},{\"end\":119407,\"start\":119400},{\"end\":119419,\"start\":119411},{\"end\":119427,\"start\":119423},{\"end\":119721,\"start\":119714},{\"end\":119727,\"start\":119725},{\"end\":119736,\"start\":119731},{\"end\":119744,\"start\":119740},{\"end\":119754,\"start\":119748},{\"end\":119767,\"start\":119758},{\"end\":120033,\"start\":120027},{\"end\":120044,\"start\":120037},{\"end\":120055,\"start\":120048},{\"end\":120062,\"start\":120059},{\"end\":120072,\"start\":120066},{\"end\":120082,\"start\":120076},{\"end\":120090,\"start\":120086},{\"end\":120096,\"start\":120094},{\"end\":120105,\"start\":120102},{\"end\":120432,\"start\":120425},{\"end\":120442,\"start\":120436},{\"end\":120450,\"start\":120446},{\"end\":120461,\"start\":120454},{\"end\":120779,\"start\":120770},{\"end\":120789,\"start\":120783},{\"end\":120797,\"start\":120793},{\"end\":120808,\"start\":120801},{\"end\":120818,\"start\":120812},{\"end\":120827,\"start\":120822},{\"end\":120836,\"start\":120831},{\"end\":120846,\"start\":120840},{\"end\":120855,\"start\":120850},{\"end\":120869,\"start\":120859},{\"end\":120879,\"start\":120873},{\"end\":120888,\"start\":120883},{\"end\":120899,\"start\":120892},{\"end\":120912,\"start\":120903},{\"end\":120921,\"start\":120916},{\"end\":120932,\"start\":120925},{\"end\":120940,\"start\":120936},{\"end\":120950,\"start\":120944},{\"end\":120965,\"start\":120956},{\"end\":120976,\"start\":120971},{\"end\":120989,\"start\":120980},{\"end\":120999,\"start\":120993},{\"end\":121010,\"start\":121003},{\"end\":121518,\"start\":121513},{\"end\":121528,\"start\":121522},{\"end\":121538,\"start\":121532},{\"end\":121551,\"start\":121542},{\"end\":121561,\"start\":121555},{\"end\":121576,\"start\":121567},{\"end\":121588,\"start\":121580},{\"end\":121597,\"start\":121592},{\"end\":121607,\"start\":121601},{\"end\":121619,\"start\":121614},{\"end\":122245,\"start\":122237},{\"end\":122481,\"start\":122477},{\"end\":122490,\"start\":122485},{\"end\":122498,\"start\":122494},{\"end\":122510,\"start\":122502},{\"end\":122522,\"start\":122514},{\"end\":122529,\"start\":122526},{\"end\":122539,\"start\":122533},{\"end\":122548,\"start\":122543},{\"end\":122556,\"start\":122552},{\"end\":122569,\"start\":122560},{\"end\":122580,\"start\":122573},{\"end\":122592,\"start\":122584},{\"end\":122606,\"start\":122596},{\"end\":122612,\"start\":122610},{\"end\":122623,\"start\":122616},{\"end\":122630,\"start\":122627},{\"end\":122636,\"start\":122634},{\"end\":122647,\"start\":122640},{\"end\":122657,\"start\":122651},{\"end\":122666,\"start\":122661},{\"end\":122676,\"start\":122670},{\"end\":122684,\"start\":122680},{\"end\":123346,\"start\":123344},{\"end\":123352,\"start\":123350},{\"end\":123358,\"start\":123356},{\"end\":123372,\"start\":123365},{\"end\":123382,\"start\":123376},{\"end\":123391,\"start\":123386},{\"end\":123875,\"start\":123867},{\"end\":123881,\"start\":123879},{\"end\":123887,\"start\":123885},{\"end\":123898,\"start\":123891},{\"end\":124296,\"start\":124292},{\"end\":124308,\"start\":124300},{\"end\":124316,\"start\":124312},{\"end\":124329,\"start\":124320},{\"end\":124858,\"start\":124855},{\"end\":124864,\"start\":124862},{\"end\":124873,\"start\":124868},{\"end\":124880,\"start\":124877},{\"end\":124889,\"start\":124884},{\"end\":124897,\"start\":124893},{\"end\":125452,\"start\":125445},{\"end\":125464,\"start\":125456},{\"end\":125470,\"start\":125468},{\"end\":125481,\"start\":125476},{\"end\":125492,\"start\":125485},{\"end\":125502,\"start\":125496},{\"end\":125516,\"start\":125506},{\"end\":125526,\"start\":125520},{\"end\":125855,\"start\":125848},{\"end\":125862,\"start\":125859},{\"end\":125870,\"start\":125866},{\"end\":125880,\"start\":125874},{\"end\":125899,\"start\":125884},{\"end\":125906,\"start\":125903},{\"end\":125918,\"start\":125910},{\"end\":125931,\"start\":125922},{\"end\":125949,\"start\":125935},{\"end\":125960,\"start\":125953},{\"end\":125973,\"start\":125964},{\"end\":126944,\"start\":126925},{\"end\":127249,\"start\":127244},{\"end\":127254,\"start\":127251}]", "bib_entry": "[{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b0\",\"matched_paper_id\":2057420},\"end\":111335,\"start\":110892},{\"attributes\":{\"doi\":\"arXiv:2112.00861\",\"id\":\"b1\"},\"end\":111782,\"start\":111337},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14096841},\"end\":112207,\"start\":111784},{\"attributes\":{\"id\":\"b3\"},\"end\":113149,\"start\":112209},{\"attributes\":{\"doi\":\"abs/2204.05862\",\"id\":\"b4\"},\"end\":113664,\"start\":113151},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":649822},\"end\":113920,\"start\":113666},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1085832},\"end\":114266,\"start\":113922},{\"attributes\":{\"doi\":\"10.5281/zenodo.5297715\",\"id\":\"b7\"},\"end\":114595,\"start\":114268},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218971783},\"end\":115597,\"start\":114597},{\"attributes\":{\"doi\":\"10.18653/v1/2022.naacl-main.92\",\"id\":\"b9\",\"matched_paper_id\":249319807},\"end\":116206,\"start\":115599},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13002849},\"end\":116546,\"start\":116208},{\"attributes\":{\"doi\":\"abs/1912.08517\",\"id\":\"b11\"},\"end\":116894,\"start\":116548},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1103\",\"id\":\"b12\",\"matched_paper_id\":1137329},\"end\":117325,\"start\":116896},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":202786778},\"end\":118306,\"start\":117327},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21850704},\"end\":118626,\"start\":118308},{\"attributes\":{\"doi\":\"10.1145/1273496.1273590\",\"id\":\"b15\",\"matched_paper_id\":11551208},\"end\":119180,\"start\":118628},{\"attributes\":{\"id\":\"b16\"},\"end\":119309,\"start\":119182},{\"attributes\":{\"id\":\"b17\"},\"end\":119657,\"start\":119311},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":160025533},\"end\":119940,\"start\":119659},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":204838007},\"end\":120365,\"start\":119942},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7147309},\"end\":120711,\"start\":120367},{\"attributes\":{\"id\":\"b21\"},\"end\":121466,\"start\":120713},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":237513578},\"end\":122140,\"start\":121468},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2332513},\"end\":122413,\"start\":122142},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":208117506},\"end\":123283,\"start\":122415},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235097625},\"end\":123817,\"start\":123285},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":247762790},\"end\":124203,\"start\":123819},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":235313409},\"end\":124790,\"start\":124205},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3636178},\"end\":125387,\"start\":124792},{\"attributes\":{\"id\":\"b29\"},\"end\":125792,\"start\":125389},{\"attributes\":{\"id\":\"b30\"},\"end\":126351,\"start\":125794},{\"attributes\":{\"id\":\"b31\"},\"end\":126772,\"start\":126353},{\"attributes\":{\"id\":\"b32\"},\"end\":127123,\"start\":126774},{\"attributes\":{\"id\":\"b33\"},\"end\":127391,\"start\":127125},{\"attributes\":{\"id\":\"b34\"},\"end\":127740,\"start\":127393},{\"attributes\":{\"id\":\"b35\"},\"end\":127859,\"start\":127742},{\"attributes\":{\"id\":\"b36\"},\"end\":128497,\"start\":127861}]", "bib_title": "[{\"end\":110935,\"start\":110892},{\"end\":111832,\"start\":111784},{\"end\":113709,\"start\":113666},{\"end\":113979,\"start\":113922},{\"end\":114634,\"start\":114597},{\"end\":115680,\"start\":115599},{\"end\":116256,\"start\":116208},{\"end\":116947,\"start\":116896},{\"end\":117386,\"start\":117327},{\"end\":118361,\"start\":118308},{\"end\":118710,\"start\":118628},{\"end\":119710,\"start\":119659},{\"end\":120023,\"start\":119942},{\"end\":120421,\"start\":120367},{\"end\":121509,\"start\":121468},{\"end\":122231,\"start\":122142},{\"end\":122473,\"start\":122415},{\"end\":123340,\"start\":123285},{\"end\":123863,\"start\":123819},{\"end\":124288,\"start\":124205},{\"end\":124851,\"start\":124792},{\"end\":126534,\"start\":126353}]", "bib_author": "[{\"end\":110949,\"start\":110937},{\"end\":110961,\"start\":110949},{\"end\":110971,\"start\":110961},{\"end\":111407,\"start\":111397},{\"end\":111414,\"start\":111407},{\"end\":111422,\"start\":111414},{\"end\":111431,\"start\":111422},{\"end\":111442,\"start\":111431},{\"end\":111454,\"start\":111442},{\"end\":111463,\"start\":111454},{\"end\":111473,\"start\":111463},{\"end\":111481,\"start\":111473},{\"end\":111493,\"start\":111481},{\"end\":111846,\"start\":111834},{\"end\":111856,\"start\":111846},{\"end\":111862,\"start\":111856},{\"end\":111871,\"start\":111862},{\"end\":111879,\"start\":111871},{\"end\":111889,\"start\":111879},{\"end\":111904,\"start\":111889},{\"end\":111914,\"start\":111904},{\"end\":112307,\"start\":112300},{\"end\":112316,\"start\":112307},{\"end\":112327,\"start\":112316},{\"end\":112337,\"start\":112327},{\"end\":112345,\"start\":112337},{\"end\":112358,\"start\":112345},{\"end\":112367,\"start\":112358},{\"end\":112375,\"start\":112367},{\"end\":112386,\"start\":112375},{\"end\":112398,\"start\":112386},{\"end\":112408,\"start\":112398},{\"end\":112420,\"start\":112408},{\"end\":112431,\"start\":112420},{\"end\":112442,\"start\":112431},{\"end\":112454,\"start\":112442},{\"end\":112464,\"start\":112454},{\"end\":112482,\"start\":112464},{\"end\":112495,\"start\":112482},{\"end\":112503,\"start\":112495},{\"end\":112515,\"start\":112503},{\"end\":112525,\"start\":112515},{\"end\":112535,\"start\":112525},{\"end\":112544,\"start\":112535},{\"end\":112554,\"start\":112544},{\"end\":112564,\"start\":112554},{\"end\":112573,\"start\":112564},{\"end\":112582,\"start\":112573},{\"end\":112596,\"start\":112582},{\"end\":112604,\"start\":112596},{\"end\":112612,\"start\":112604},{\"end\":112622,\"start\":112612},{\"end\":113249,\"start\":113242},{\"end\":113258,\"start\":113249},{\"end\":113269,\"start\":113258},{\"end\":113279,\"start\":113269},{\"end\":113287,\"start\":113279},{\"end\":113300,\"start\":113287},{\"end\":113309,\"start\":113300},{\"end\":113317,\"start\":113309},{\"end\":113328,\"start\":113317},{\"end\":113340,\"start\":113328},{\"end\":113721,\"start\":113711},{\"end\":113735,\"start\":113721},{\"end\":113993,\"start\":113981},{\"end\":114011,\"start\":113993},{\"end\":114025,\"start\":114011},{\"end\":114031,\"start\":114025},{\"end\":114277,\"start\":114268},{\"end\":114284,\"start\":114277},{\"end\":114292,\"start\":114284},{\"end\":114301,\"start\":114292},{\"end\":114313,\"start\":114301},{\"end\":114322,\"start\":114313},{\"end\":114647,\"start\":114636},{\"end\":114655,\"start\":114647},{\"end\":114664,\"start\":114655},{\"end\":114675,\"start\":114664},{\"end\":114685,\"start\":114675},{\"end\":114697,\"start\":114685},{\"end\":114712,\"start\":114697},{\"end\":114721,\"start\":114712},{\"end\":114731,\"start\":114721},{\"end\":114741,\"start\":114731},{\"end\":114752,\"start\":114741},{\"end\":114768,\"start\":114752},{\"end\":114779,\"start\":114768},{\"end\":114791,\"start\":114779},{\"end\":114800,\"start\":114791},{\"end\":114810,\"start\":114800},{\"end\":114823,\"start\":114810},{\"end\":114829,\"start\":114823},{\"end\":114839,\"start\":114829},{\"end\":114848,\"start\":114839},{\"end\":114856,\"start\":114848},{\"end\":114866,\"start\":114856},{\"end\":114876,\"start\":114866},{\"end\":114884,\"start\":114876},{\"end\":114893,\"start\":114884},{\"end\":114902,\"start\":114893},{\"end\":114912,\"start\":114902},{\"end\":114926,\"start\":114912},{\"end\":114937,\"start\":114926},{\"end\":114950,\"start\":114937},{\"end\":114960,\"start\":114950},{\"end\":114974,\"start\":114960},{\"end\":114985,\"start\":114974},{\"end\":114996,\"start\":114985},{\"end\":115006,\"start\":114996},{\"end\":115012,\"start\":115006},{\"end\":115689,\"start\":115682},{\"end\":115702,\"start\":115689},{\"end\":115713,\"start\":115702},{\"end\":115725,\"start\":115713},{\"end\":115732,\"start\":115725},{\"end\":115736,\"start\":115732},{\"end\":116265,\"start\":116258},{\"end\":116271,\"start\":116265},{\"end\":116282,\"start\":116271},{\"end\":116292,\"start\":116282},{\"end\":116297,\"start\":116292},{\"end\":116301,\"start\":116297},{\"end\":116636,\"start\":116622},{\"end\":116651,\"start\":116636},{\"end\":116663,\"start\":116651},{\"end\":116961,\"start\":116949},{\"end\":116971,\"start\":116961},{\"end\":117398,\"start\":117388},{\"end\":117407,\"start\":117398},{\"end\":117416,\"start\":117407},{\"end\":117425,\"start\":117416},{\"end\":117437,\"start\":117425},{\"end\":117447,\"start\":117437},{\"end\":117458,\"start\":117447},{\"end\":117465,\"start\":117458},{\"end\":117479,\"start\":117465},{\"end\":117489,\"start\":117479},{\"end\":117502,\"start\":117489},{\"end\":117510,\"start\":117502},{\"end\":117518,\"start\":117510},{\"end\":117528,\"start\":117518},{\"end\":117538,\"start\":117528},{\"end\":117548,\"start\":117538},{\"end\":117564,\"start\":117548},{\"end\":117575,\"start\":117564},{\"end\":117583,\"start\":117575},{\"end\":117590,\"start\":117583},{\"end\":117602,\"start\":117590},{\"end\":117611,\"start\":117602},{\"end\":118373,\"start\":118363},{\"end\":118382,\"start\":118373},{\"end\":118392,\"start\":118382},{\"end\":118722,\"start\":118712},{\"end\":118732,\"start\":118722},{\"end\":119198,\"start\":119184},{\"end\":119398,\"start\":119391},{\"end\":119409,\"start\":119398},{\"end\":119421,\"start\":119409},{\"end\":119429,\"start\":119421},{\"end\":119723,\"start\":119712},{\"end\":119729,\"start\":119723},{\"end\":119738,\"start\":119729},{\"end\":119746,\"start\":119738},{\"end\":119756,\"start\":119746},{\"end\":119769,\"start\":119756},{\"end\":120035,\"start\":120025},{\"end\":120046,\"start\":120035},{\"end\":120057,\"start\":120046},{\"end\":120064,\"start\":120057},{\"end\":120074,\"start\":120064},{\"end\":120084,\"start\":120074},{\"end\":120092,\"start\":120084},{\"end\":120098,\"start\":120092},{\"end\":120107,\"start\":120098},{\"end\":120434,\"start\":120423},{\"end\":120444,\"start\":120434},{\"end\":120452,\"start\":120444},{\"end\":120463,\"start\":120452},{\"end\":120781,\"start\":120768},{\"end\":120791,\"start\":120781},{\"end\":120799,\"start\":120791},{\"end\":120810,\"start\":120799},{\"end\":120820,\"start\":120810},{\"end\":120829,\"start\":120820},{\"end\":120838,\"start\":120829},{\"end\":120848,\"start\":120838},{\"end\":120857,\"start\":120848},{\"end\":120871,\"start\":120857},{\"end\":120881,\"start\":120871},{\"end\":120890,\"start\":120881},{\"end\":120901,\"start\":120890},{\"end\":120914,\"start\":120901},{\"end\":120923,\"start\":120914},{\"end\":120934,\"start\":120923},{\"end\":120942,\"start\":120934},{\"end\":120952,\"start\":120942},{\"end\":120967,\"start\":120952},{\"end\":120978,\"start\":120967},{\"end\":120991,\"start\":120978},{\"end\":121001,\"start\":120991},{\"end\":121012,\"start\":121001},{\"end\":121520,\"start\":121511},{\"end\":121530,\"start\":121520},{\"end\":121540,\"start\":121530},{\"end\":121553,\"start\":121540},{\"end\":121563,\"start\":121553},{\"end\":121578,\"start\":121563},{\"end\":121590,\"start\":121578},{\"end\":121599,\"start\":121590},{\"end\":121609,\"start\":121599},{\"end\":121621,\"start\":121609},{\"end\":122247,\"start\":122233},{\"end\":122483,\"start\":122475},{\"end\":122492,\"start\":122483},{\"end\":122500,\"start\":122492},{\"end\":122512,\"start\":122500},{\"end\":122524,\"start\":122512},{\"end\":122531,\"start\":122524},{\"end\":122541,\"start\":122531},{\"end\":122550,\"start\":122541},{\"end\":122558,\"start\":122550},{\"end\":122571,\"start\":122558},{\"end\":122582,\"start\":122571},{\"end\":122594,\"start\":122582},{\"end\":122608,\"start\":122594},{\"end\":122614,\"start\":122608},{\"end\":122625,\"start\":122614},{\"end\":122632,\"start\":122625},{\"end\":122638,\"start\":122632},{\"end\":122649,\"start\":122638},{\"end\":122659,\"start\":122649},{\"end\":122668,\"start\":122659},{\"end\":122678,\"start\":122668},{\"end\":122686,\"start\":122678},{\"end\":123348,\"start\":123342},{\"end\":123354,\"start\":123348},{\"end\":123360,\"start\":123354},{\"end\":123374,\"start\":123360},{\"end\":123384,\"start\":123374},{\"end\":123393,\"start\":123384},{\"end\":123877,\"start\":123865},{\"end\":123883,\"start\":123877},{\"end\":123889,\"start\":123883},{\"end\":123900,\"start\":123889},{\"end\":124298,\"start\":124290},{\"end\":124310,\"start\":124298},{\"end\":124318,\"start\":124310},{\"end\":124331,\"start\":124318},{\"end\":124339,\"start\":124331},{\"end\":124346,\"start\":124339},{\"end\":124860,\"start\":124853},{\"end\":124866,\"start\":124860},{\"end\":124875,\"start\":124866},{\"end\":124882,\"start\":124875},{\"end\":124891,\"start\":124882},{\"end\":124899,\"start\":124891},{\"end\":124904,\"start\":124899},{\"end\":124908,\"start\":124904},{\"end\":125454,\"start\":125441},{\"end\":125466,\"start\":125454},{\"end\":125472,\"start\":125466},{\"end\":125483,\"start\":125472},{\"end\":125494,\"start\":125483},{\"end\":125504,\"start\":125494},{\"end\":125518,\"start\":125504},{\"end\":125528,\"start\":125518},{\"end\":125857,\"start\":125844},{\"end\":125864,\"start\":125857},{\"end\":125872,\"start\":125864},{\"end\":125882,\"start\":125872},{\"end\":125901,\"start\":125882},{\"end\":125908,\"start\":125901},{\"end\":125920,\"start\":125908},{\"end\":125933,\"start\":125920},{\"end\":125951,\"start\":125933},{\"end\":125962,\"start\":125951},{\"end\":125975,\"start\":125962},{\"end\":125984,\"start\":125975},{\"end\":125988,\"start\":125984},{\"end\":126542,\"start\":126536},{\"end\":126946,\"start\":126925},{\"end\":127251,\"start\":127242},{\"end\":127256,\"start\":127251}]", "bib_venue": "[{\"end\":111018,\"start\":110975},{\"end\":111395,\"start\":111337},{\"end\":111943,\"start\":111914},{\"end\":112298,\"start\":112209},{\"end\":113240,\"start\":113151},{\"end\":113778,\"start\":113735},{\"end\":114056,\"start\":114031},{\"end\":114409,\"start\":114344},{\"end\":115028,\"start\":115012},{\"end\":115784,\"start\":115766},{\"end\":116330,\"start\":116301},{\"end\":116620,\"start\":116548},{\"end\":117005,\"start\":116991},{\"end\":117627,\"start\":117611},{\"end\":118421,\"start\":118392},{\"end\":118830,\"start\":118784},{\"end\":119389,\"start\":119311},{\"end\":119780,\"start\":119769},{\"end\":120126,\"start\":120107},{\"end\":120476,\"start\":120463},{\"end\":120766,\"start\":120713},{\"end\":121673,\"start\":121656},{\"end\":122263,\"start\":122247},{\"end\":122730,\"start\":122716},{\"end\":123442,\"start\":123424},{\"end\":123916,\"start\":123900},{\"end\":124434,\"start\":124379},{\"end\":124974,\"start\":124960},{\"end\":125439,\"start\":125389},{\"end\":125842,\"start\":125794},{\"end\":126558,\"start\":126542},{\"end\":126923,\"start\":126774},{\"end\":127240,\"start\":127125},{\"end\":127565,\"start\":127393},{\"end\":127780,\"start\":127742},{\"end\":128148,\"start\":127861},{\"end\":111968,\"start\":111945},{\"end\":115042,\"start\":115032},{\"end\":115820,\"start\":115786},{\"end\":116355,\"start\":116332},{\"end\":117034,\"start\":117007},{\"end\":117732,\"start\":117722},{\"end\":118446,\"start\":118423},{\"end\":120509,\"start\":120502},{\"end\":121705,\"start\":121675},{\"end\":122746,\"start\":122732},{\"end\":123456,\"start\":123444},{\"end\":123977,\"start\":123967},{\"end\":125054,\"start\":125046}]"}}}, "year": 2023, "month": 12, "day": 17}