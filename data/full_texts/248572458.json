{"id": 248572458, "updated": "2023-10-05 14:54:34.707", "metadata": {"title": "Layer-wised Model Aggregation for Personalized Federated Learning", "authors": "[{\"first\":\"Xiaosong\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Song\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Wenchao\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Personalized Federated Learning (pFL) not only can capture the common priors from broad range of distributed data, but also support customized models for heterogeneous clients. Researches over the past few years have applied the weighted aggregation manner to produce personalized models, where the weights are determined by calibrating the distance of the entire model parameters or loss values, and have yet to consider the layer-level impacts to the aggregation process, leading to lagged model convergence and inadequate personalization over non-IID datasets. In this paper, we propose a novel pFL training framework dubbed Layer-wised Personalized Federated learning (pFedLA) that can discern the importance of each layer from different clients, and thus is able to optimize the personalized model aggregation for clients with heterogeneous data. Specifically, we employ a dedicated hypernetwork per client on the server side, which is trained to identify the mutual contribution factors at layer granularity. Meanwhile, a parameterized mechanism is introduced to update the layer-wised aggregation weights to progressively exploit the inter-user similarity and realize accurate model personalization. Extensive experiments are conducted over different models and learning tasks, and we show that the proposed methods achieve significantly higher performance than state-of-the-art pFL methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.03993", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/MaZ0022", "doi": "10.1109/cvpr52688.2022.00985"}}, "content": {"source": {"pdf_hash": "458269f18261d2a7f6ed7bde88ba64e54b61e74a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.03993v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ba2a249b2ae53997bc185ebd9ee01c0a819557a5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/458269f18261d2a7f6ed7bde88ba64e54b61e74a.txt", "contents": "\nLayer-wised Model Aggregation for Personalized Federated Learning\n\n\nXiaosong Ma maxiaosong16@gmail.com \nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nJie Zhang jieaa.zhang@connect.polyu.hk \nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nSong Guo song.guo@polyu.edu.hk \nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nThe Hong Kong Polytechnic University Shenzhen Research Institute\n\n\nWenchao Xu wenchao.xu@polyu.edu.hk \nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nLayer-wised Model Aggregation for Personalized Federated Learning\n\nPersonalized Federated Learning (pFL) not only can capture the common priors from broad range of distributed data, but also support customized models for heterogeneous clients. Researches over the past few years have applied the weighted aggregation manner to produce personalized models, where the weights are determined by calibrating the distance of the entire model parameters or loss values, and have yet to consider the layer-level impacts to the aggregation process, leading to lagged model convergence and inadequate personalization over non-IID datasets. In this paper, we propose a novel pFL training framework dubbed Layer-wised Personalized Federated learning (pFedLA) that can discern the importance of each layer from different clients, and thus is able to optimize the personalized model aggregation for clients with heterogeneous data. Specifically, we employ a dedicated hypernetwork per client on the server side, which is trained to identify the mutual contribution factors at layer granularity. Meanwhile, a parameterized mechanism is introduced to update the layer-wised aggregation weights to progressively exploit the inter-user similarity and realize accurate model personalization. Extensive experiments are conducted over different models and learning tasks, and we show that the proposed methods achieve significantly higher performance than state-of-the-art pFL methods.\n\nIntroduction\n\nFederated learning (FL) has emerged as a prominent collaborative machine learning framework to exploit inter-user similarities without sharing the private data [33,43,52]. When users' datasets are non-IID (independent and identically distributed), i.e., the inter-user distances are large [23,53], sharing a global model for all clients may lead \u2020 Equal contribution * Corresponding author to slow convergence or poor inference performance as the model may significantly deviate from their local data [14,56].\n\nTo deal with such statistical diversity, personalized federated learning (pFL) mechanisms are proposed to allow each client to train a customized model to adapt to their own data distribution [9,12,15,22]. Literature status quo to achieve pFL include the data-based approaches, i.e., smoothing the statistical heterogeneity among clients' datasets [8,16], the single-model approaches, e.g., regularization [22,41], meta-learning [9], parameter decoupling [5,24,26], and the multiple-model ways, i.e., train personalized models for each client [15,54], which can produce personalized models for each client via weighted combinations of clients' models. Existing pFL methods apply a distance metric among the whole model parameters or loss values of different clients, which is insufficient to exploit their heterogeneity since the overall distance metric cannot always reflect the importance of each local model and can lead to inaccurate combining weights or unbalance contribution from non-IID distributed datasets, and thus prevent further personalization for clients at scale. The main reason is that different layers of a neural network can have different util-ities, e.g., the shallow layers focus more on local feature extraction, while the deeper layers are for extracting global features [6,20,21,47,49]. Measuring the model distances would ignore such layer-level differences, and cause inaccurate personalization that hinders the pFL training efficiency.\n\nIn this paper, we propose a band-new pFL framework that can realize the layer-level aggregation for FL personalization, which can accurately recognize the utility of each layer from clients' model for adequate personalization, and thus can improve the training performance over non-IID datasets. A toy example is presented to illustrate that traditional model-level aggregation based pFL method fails in reflecting the inner relationship among all local models, which motivates us to exploit an effective way to discern the layer-level impacts during the pFL training procedure. Observation of Layer-wised Personalized Aggregation. In the toy example, we consider six clients to collaboratively learn their personalized models for a nine-class classification task. The average model accuracy is obtained via both the layer-wised and model-wised aggregation approaches, which utilize the inter-layer and inter-model similarities respectively. Figure 1 shows that higher model accuracy can be achieved by the layer-wised approach comparing with the model-wised one for a certain client. The weights of layers for this client after the last communication round are also plotted, and we show that applying different weights for different layers, e.g., the first and second fullyconnected layer (i.e., FC1, FC2) on client 1 have larger weights, while the second convolution layer, i.e., Conv1 layer has smaller weights, can produce significant performance gain for the personalized model accuracy.\n\nThe toy example demonstrates the potential of the layerwised aggregation to achieve higher performance than traditional model based pFL methods, since the layer-level similarities can reflect more accurate correlation among clients. By exploiting such layer-wised similarity and identifying the layer-level inter-user contribution, it is promising to produce efficient and effective personalized models for all clients. Motivated by such observation, we propose a novel federated training framework, namely, pFedLA, which adaptively facilitates the underlying collaboration between clients in a layer-wised manner. Specifically, at the server side, we introduce a dedicated hypernetwork for each client to learn the weights of cross-clients' layers during the pFL training procedure, which is shown to effectively boost the personalization over non-IID datasets. Extensive experiments are conducted, and we demonstrate that the proposed pFedLA can achieve higher performance than the state-ofthe-art baselines over widely used models and datasets, i.e., EMNIST, FashionMNIST, CIFAR10 and CIFAR100. The contributions of the paper are summarized as follows:\n\n\u2022 To the best of our knowledge, this paper is the first to explicitly reveal the benefits of layer-wised aggrega-tion comparing with model-wised approaches in pFL among heterogeneous FL clients;\n\n\u2022 We propose a layer-wised personalized federated learning (pFedLA) training framework that can effectively exploit the inter-user similarities among clients with non-IID data and produce accurate personalized models;\n\n\u2022 We conduct extensive experiments on four typical image classification tasks, which demonstrated the superior performance of pFedLA over the state-of-the-art approaches.\n\n\nRelated Work\n\n\nPersonalized Federated Learning\n\nRecently, various approaches have been proposed to realize pFL, which can be classified into the data-based and the model-based categories. Data-based approaches focus on reducing the statistical heterogeneity among clients' datasets to boost the model convergence, while modelbased approaches emphasize on producing customized model structures or parameters for different clients.\n\nThe typical way of data-based pFL is to share a small amount of global data to each client [56]. Jeong et al. [8,16] focus on data augmentation methods by generating additional data to augment its local data towards yielding an IID dataset. However, these methods usually require the FL server to know the statistical information about clients' local data distributions (e.g., class sizes, mean and standard deviation), which may potentially violate privacy policy [42]. Another line of work considers to design client selection mechanisms to approach homogeneous data distribution [30,45,48].\n\nModel-based pFL methods can also be divided into two types: single-model, multiple-model approaches. Singlemodel based methods extended from the conventional FL algorithms like FedAvg [33] combine the optimization of the local models and global model, which consist of five different kinds of approaches: local fine-tuning [1,36,46], regularization [12,13,41], model mixture [7,32], meta learning [9,18] and parameter decomposition [1,4,5]. Considering the diversity and inherent relationship of local data, a multimodel-based approach where multiple global models are trained for heterogeneous clients is more suitable. Some researchers [10,15,32] propose to train multiple global models at the server, where similar clients are clustered into several groups and different models are trained for each group. Another strategy is to collaboratively train a personalized model for each individual client, e.g., FedAMP [15], Fed-Fomo [54], MOCHA [39], KT-pFL [51] etc.\n\nThese literatures treat each client's model as a whole entity, and has yet to consider the layer-wised utility for per- Figure 2. Framework of pFedLA. The workflow contains 5 steps: \u2460 local training on private data; \u2461 each client sends the update of parameters \u2206\u03b8i to the server; \u2462 the server updates the aggregation weight matrix \u03b1i by hypernetworks HNi(vi; \u03c8i) according to \u2206\u03b8i; \u2463 the server performs weighted aggregation and outputs personalized model\u03b8i for the corresponding client; \u2464 each client downloads the personalized model\u03b8i. sonalized aggregation. The distance metric for describing the similarity among models is inaccurate and can lead to sub-optimal performance, which motivates us to explore a fine-grained aggregation strategy to adapt to broad range of non-IID clients.\n\n\nHypernetworks\n\nHypernetworks [11] are used to generate parameters of other neural networks, e.g., a target network, by mapping the embeddings of the target tasks to corresponding model parameters. Hypernetworks have been widely used in various machine learning applications, such as language modeling [35,40], computer vision [17,19,27], 3D scene representation [28,38], hyperparameter optimization [2,25,29,31], neural architecture search (NAS) [3,50], continual learning [44] and meta-learning [55]. Shamsian et al. [37] is the first to apply hypernetworks in FL, which can generate effective personalized model parameters for each client. We show that hypernetworks are capable to evaluate the importance of each model layer, and can boost the personalized aggregation in non-IID scenarios.\n\n\nMethod\n\nIn this section, we present the design of the pFedLA framework that applies the hypernetworks to conduct layerwised personalized aggregation, which is shown in Figure 2.\n\n\nProblem Formulation\n\nIn pFL, the goal is to collaboratively train personalized models among multiple clients while keeping their local data private. Considering N clients with non-IID datasets,\nlet D i = {(x (i) j , y (i) j )} mi i=1 (1 \u2264 i \u2264 N ) be the dataset on the i-th client, where x j is the j-th input data sample, y j is the corresponding label. The size of the datasets on the i-th client is denoted by m i . The size of all clients' datasets is M = N i=1 m i .\nLet \u03b8 i represent the model parameters of client i, the objective of pFL can be formulated as\n\u0398 * = arg min \u0398 N i=1 m i M L i (\u03b8 i ),(1)\nwhere\nL i (\u03b8 i ) = 1 m i mi j=1 L CE (\u03b8 i ; x (i) j , y (i) j )(2)\nwhere \u0398 = {\u03b8 i , . . . , \u03b8 N } is the set of personalized parameters for all clients. L i is loss function of i-th client associated with dataset D i . The difference between the predicted value and the true label of data samples is measured by L CE , which is the cross-entropy loss.\n\n\npFedLA Algorithm\n\nIn this section, we present our proposed pFL algorithm pFedLA, which evaluates the importance of each layer from different clients to achieve layer-wised personalized model aggregation. We apply a dedicated hypernetwork for each client on the server and train them to generate aggregation weights for each model layer of different clients. It can be seen from Figure 2 that, unlike the general FL framework that generates only one global model, pFedLA maintains a personalized model for each client at the server. Clients with similar data distribution should have high aggregation weights to reinforce the mutual contribution from each other. Our pFedLA applies a set of aggregation weight ma- trix \u03b1 i at the server side to progressively exploit the interuser similarities at layer level, which is defined as\n! ( ! ; ! ) Embedding vector ! Hypernetwork ! \u0305 ! = \"# , \u2026 , \"$ * \u03b1 ! Weighted aggregation (1; \u0305 ! ) Client ! ! (#) , \u2026 , ! (' ! ) \u0305 ! \u2113 ! (#) , \u2026 , \u2113 ! (' ! )\u03b1 i = \u03b1 l1 i , \u03b1 l2 i , . . . , \u03b1 ln i = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 \u03b1 l1,1 i \u03b1 l2,1 i \u00b7 \u00b7 \u00b7 \u03b1 ln,1 i \u03b1 l1,2 i \u03b1 l2,2 i \u00b7 \u00b7 \u00b7 \u03b1 ln,2 i . . . . . . . . . . . . \u03b1 l1,N i \u03b1 l2,N i \u00b7 \u00b7 \u00b7 \u03b1 ln,N i \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb\n(3) where \u03b1 ln i represents the aggregation weight vector of nth layer in client i, while \u03b1 ln,N i represents the aggregation weight for client N in n-th layer. For all n layers, N j=1 \u03b1 ln,j i = 1. Different with previous pFL algorithms, instead of applying identical weight values for all layers of a client model, pFedLA considers the different utilities of neural layers, and assign a unique weight to each of them to achieve fine-grained personalized aggregation. In addition, unlike traditional methods that mathematically calculate the weights using a distance metric among the entire model parameters [15,54], pFedLA parameterized the weights during the training phase via a set of dedicated hypernetworks. The layer-wised weights are determined by the hypernetworks, which are alternatively updated with the personalized model. Such way we can obtain effective weights as their update direction is in line with the optimization direction of the objective function. In the following, we will elaborate the updating process of the aggregation weight matrix \u03b1 of pFedLA.\n\nEach hypernetwork consists of several fully connected layers, whose input is an embedding vector that is automatically updated with the model parameters, and the output is the weight matrix \u03b1. Define the hypernetwork on client i as\n\u03b1 i = HN i (v i ; \u03c8 i ),(4)\nwhere v i is the embedding vector and \u03c8 i is the parameter of client i's hypernetwork (i.e., Figure 3). for each communication round t \u2208 {1, . . . , T } do 4: for each client i in parallel do for each local epoch do 13: for mini-batch \u03be t \u2286 D i do 14: Local Training:\n5:\u03b8 (t+1) i = {\u03b8 l1 , . . . , \u03b8 ln } * HN i (v (t) i ; \u03c8 (t) i ) 6: \u2206\u03b8 i \u2190 ClientU pdate(\u03b8 (t+1) i\u03b8 i = \u03b8 i \u2212 \u03b7\u2207 \u03b8i L i (\u03b8 i ; \u03be t ) return \u2206\u03b8 i = \u03b8 i \u2212\u03b8 (t+1) i\nset of n-th layer of all clients, where \u03b8 ln N are the parameters of n-th layer in client N . In pFedLA, the model parameters of client i is obtained by weighted aggregation according to\n\u03b1 i :\u03b8 i = {\u03b8 l1 i ,\u03b8 l2 i , . . . ,\u03b8 ln i } = {\u03b8 l1 , \u03b8 l2 , .\n. . , \u03b8 ln } * \u03b1 i , (5) where\u03b8 ln i can also be expressed as:\n\u03b8 ln i = N j=1 \u03b8 ln j \u03b1 ln,j i .(6)\nThus the objective function of pFedLA can be derived from Eq. 1 to\narg min V,\u03a8 N i=1 m i M L i ({\u03b8 l1 , \u03b8 l2 , . . . , \u03b8 ln } * HN i (v i ; \u03c8 i )) (7) where V = {v 1 , . . . , v N }, \u03a8 = {\u03c8 1 , . . . , \u03c8 N }.\nConsequently, pFedLA transforms the optimization problem for client parameters \u03b8 i into the hypernetwork's embedding vector v i and parameters \u03c8 i . In the following, we introduce the update rules of V and \u03a8.\n\nUpdate v i and \u03c8 i . According to the chain rule, we can have the gradient of v i and \u03c8 i from Eq. 7: for each communication round t \u2208 {1, . . . , T } do 4: for each client i in parallel do \n\u2207 vi L i = (\u2207 vi\u03b8i ) T \u2207\u03b8 i L i = [{\u03b8 l1 , \u03b8 l2 , . . . , \u03b8 ln } * \u2207 vi HN i (v i ; \u03c8 i )] T \u2207\u03b8 i L i ,(8)\u2207 \u03c8i L i = (\u2207 \u03c8i\u03b8i ) T \u2207\u03b8 i L i = [{\u03b8 l1 , \u03b8 l2 , . . . , \u03b8 ln } * \u2207 \u03c8i HN i (v i ; \u03c8 i )] T \u2207\u03b8 i L i .(9)5:\u03b8 (t+1) i = {\u03b8 l1 , . . . , \u03b8 ln } * HN i (v (t) i ; \u03c8 (t) i ) 6: Sort {\u03b1 l1,i i ,Set \u03b8 i \u2190 {Heur\u03b8 (t+1) i , \u03b8 retain i }.\n14:\n\nfor each local epoch do 15: for mini-batch \u03be t \u2286 D i do 16: Local Training:\n\u03b8 i = \u03b8 i \u2212 \u03b7\u2207 \u03b8i L i (\u03b8 i ; \u03be t ) return \u2206\u03b8 i = \u03b8 i \u2212 {Heur\u03b8 (t+1) i , \u03b8 retain i } \u2207\u03b8\ni L i can be obtained from client i's local training in each communication round and \u2207 vi/\u03c8i HN i (v i ; \u03c8 i ) is the gradient of \u03b1 i in directions v i /\u03c8 i . pFedLA uses a more general way to update v i and \u03c8 i :\n\u2206v i = (\u2207 vi\u03b8i ) T \u2206\u03b8 i = [{\u03b8 l1 , \u03b8 l2 , . . . , \u03b8 ln } * \u2207 vi HN i (v i ; \u03c8 i )] T \u2206\u03b8 i ,(10)\u2206\u03c8 i = (\u2207 \u03c8i\u03b8i ) T \u2206\u03b8 i = [{\u03b8 l1 , \u03b8 l2 , . . . , \u03b8 ln } * \u2207 \u03c8i HN i (v i ; \u03c8 i )] T \u2206\u03b8 i .(11)\nwhere \u2206\u03b8 i is the change of model parameters in client i after local training. In accordance with Eq. 10 and 11, pFedLA updates the embedding vector and parameters of hypernetwork for client i at each communication round, and then update the aggregation weight matrix \u03b1 i . Algorithm 1 demonstrates the pFedLA procedure. In each communication round, the clients first download the latest personalized models from the server, then use local SGD to train several epochs based on the private data. After that, the model update \u2206\u03b8 i for each client will be uploaded to the server to update the embedding vector V and the parameter \u03a8.\n\n\nHeurpFedLA: Heuristic Improvement of pFedLA on Communication Efficiency\n\nThe communication overhead of pFedLA is determined by the size of \u2206\u03b8 i sent from the clients and\u03b8 i sent from  the server. So, there is no additional communication cost comparing with traditional FL methods, e.g., FedAvg. In this section, we propose to further reduce the communication overhead of pFedLA with negligible performance reduction, which can adapt to more general scenarios, e.g., large scale FL systems, limited communication capacities, etc.\n\nComparing with existing works that keep some specific layers updated locally to enable communication-efficient training while retaining the performance of pFL [5,24,26], e.g., FedBN [24] found that local models with BN layers should exclude these parameters from the aggregating steps during training, while FedRep [5] and LG-FedAvg [26] proposed to locally learn the classifier layer and representation layers respectively, pFedLA can give an alternative guidance to determine which layers should be retained locally. To this end, we propose HeurpFedLA, a heuristic improvement of pFedLA that partial layers are retained locally, and the remaining layers are aggregated at the server side during training. The key idea of HeurpFedLA is to heuristically select the partial layers\u03b8 retain i with top k (AT k ) aggregation weights to update locally. Specifically, by using the aggregation weights \u03b1 l1,i i , \u03b1 l2,i i , . . . , \u03b1 ln,i i for all layers of client i, we can sort these weights in descending order and select corresponding top k layers\n\u03b8 retain i = AT k {\u03b8 l1 i , . . . ,\u03b8 ln i |\u03b1 l1,i i , . . . , \u03b1 ln,i i },(12)\nwhere AT k is the top k selection function described above, and k is a hyperparameter manually denoted before training. The detailed workflow of top k selection mechanism is shown in Figure 4. The principle behind HeurpFedLA is that layers with higher rank index should contribute more to the model personalization, which means directly using these layers in personalized model has little impact on the training performance. The retention of local layers by HeurpFedLA brings benefits in terms of communication overhead reduc- tion from the server to the clients direction, i.e., the server can save the costs of transmitting the parameters of the retained layers.\n\nAs to be demonstrated in Section 4.4, HeurpFedLA can significantly reduce the communication cost while maintaining the model performance of pFL. In large scale FL systems, it is of practical value to keep some layers from aggregation and transmission, especially for limited communication bandwidth scenarios. Furthermore, HeurpFedLA is a general training framework and can be effectively compatible with common compression schemes such as gradient quantization, sparsification, etc. The impact of retaining local layers is discussed in more detail in the next section.\n\n\nEvaluation\n\n\nExperimental Setup\n\nDatasets. We evaluate the pFedLA framework over four datasets, EMNIST, FashionMNIST, CIFAR10 and CI-FAR100. The distribution of all data sets on the training clients is non-IID. We consider two non-IID scenarios: 1) each client is randomly assigned four classes (twelve classes per client in CIFAR100) with the same amount of data on each class; 2) each client contains all classes, while the data on each class is not uniformly distributed. Two classes in EMNIST, FashionMNIST, CIFAR10 datasets have higher number of data samples than other classes, while six classes in CIFAR100 have more data samples than the others. All data are divided into 70% training set, and 30% test set. The test set and the training set have the same data distribution for all clients.\n\nBaselines. We compared the performance of pFedLA and HeurpFedLA with the state-of-the-art methods. In addition to FedAvg and Local Training, we also include Per-Fedavg, a pFL algorithm based on meta-learning; pFedMe, a pFL algorithm with regularization term added in the objective function; pFedHN, a pFL algorithm that uses hypernetworks to directly produce personalized model; server; FedFomo, a pFL algorithm that uses distance to calculate the aggregation weights based on the model and loss differences. Training Details. In all experiments, we use the same CNN architectures as in FedFomo [54], FedBN [24] and pFedHN [37]. All the models have the same structure between different clients under the same setting. For CIFAR10 and CI-FAR100, we add BN layers after the convolutional layers. For EMNIST and FashionMNIST, there is no BN layers in the model. The hypernetwork for computing layer-wise aggregation weights is a simple structure of several fully connected layers. The weight of each layer for a target client is calculated by a corresponding fully connected layer in the hypernetwork. For the specific structure of hypernetwork, please refer to the supplemental material. We evaluate the performance of pFedLA in two settings, i.e., 10 clients with 100% participation and 100 clients with 10% participation. The average model accuracy of all clients is obtained after 600 rounds training for 10 clients case and 2500 rounds for 100 clients. Implementation. We simulate all clients and the server on a workstation with an RTX 2080Ti GPU, a 3.6-GHZ Intel Core i9-9900KF CPU and 64GB of RAM. All methods are implemented in PyTorch.\n\n\nPerformance Evaluation\n\nFor all experiments, we use cross-entropy loss and SGD optimizer with a batch size of 32. The number of local epochs is 10 for 10 clients case and 20 for 100 clients. The learning rate is 0.01 for CIFAR10 and CIFAR100, and 0.005 for EMNIST and FashionMNIST. The performance of both the baselines and the proposed pFedLA under two different non-IID cases are listed in Table 1 and Table 2, respectively. Our proposed algorithm provides superior performance than baselines over the four datasets with different data distributions in most cases. On the other hand, HeurpFedLA also outperforms the existing methods with negligible performance reduction comparing with pFedLA. The number of retained layers (k) in Table 1 and 2 is 1. The communication costs of HeurpFedLA is discussed in Section 4.4. Note that since all clients have the same amount of training data for both 10 and 100 clients cases, so the 100 clients case has much more data, and thus can provide better model accuracy.\n\n\nAnalysis of Weight Evolution\n\nTo demonstrate that our method can generate higher weights to those clients with similar data distribution, we conduct the experiments with 8 clients who randomly 4 data classes from the corresponding datasets. From the 8 clients, we consider a target client with 4 random data classes, one contrastive client who has the same four classes, and one similar client who has 3 same classes with the target client. We record the weight value of each layer on the target client during the training process. Figure 5 shows the evolution of the aggregation weights for the target client during the prior, middle and last periods of training phase. It can be observed that the inter-weights from other clients decrease with the training process because their data distribution is very different from the target client. Besides, for the target client, clients with more similar data distribution (e.g., the same labels client) have higher weight value than other clients (e.g., the similar client), which shows that the hypernetwork can distinguish the similarity of data distribution on different clients. We also conduct experiments to visualize the relationship between the aggregation weights and the data similarities among clients. We consider 8 clients assigned with ID from 0 to 7, all have four classes data. The data similarities among all clients are emulated by assigning clients of adjacent IDs with similar classes, e.g., client 1 has 4 classes data, while client 2 has three same and one different classes with client 1, and client 3 has three same and one different classes with client 2, and so on. Figure 6 shows the heatmap of the inter-weights among all 8 clients of a certain layer. It can be seen that the weights among close clients with consecutive IDs, i.e., with more overlapping classes, are larger than those of the distant clients, and the highlighted diagonal line shows that the self-weights of each client have the highest values, which further verify that pFedLA can exploit the inter-similarities among heterogeneous clients.\n\n\nAnalysis of Communication Efficiency\n\nIn this section, we show the performance of the proposed HeurpFedLA. Table 3 shows the average model accuracy and communication overhead when retaining different local layers that would be absent from the aggregation process. We consider 10 clients with 100% participation over the datasets EMNIST and CIFAR10. The aggregation weights of all layers for a target client are shown in Figure 7. For the CIFAR10 dataset, the weights of the first fully-connected layer have the highest values, so the model accuracy performance will be compromised if retaining some layers at local, although the communication overhead can be reduced greatly. For EMNIST dataset, what's different is that the classifier layer has the largest weights, it is observed that the average model accuracy can even increase when retaining some local layers. Such conclusion can also be found in a state-of-the-art work, FedRep [5], which indicates that removing the classifier layer from the aggregation process can improve the model performance over non-IID datasets. It can be explained intuitively that reserving some local layers can avoid the irrelevant knowledge transfer from other clients during the aggregation process.\n\n\nEffect of k\n\nDifferent k values are applied to show the effect of retaining local layers. Table 3 shows that if retaining different :HLJKW number of the top k layers, the model accuracy will not be affected significantly, which means that HeurpFedLA can apply different k values according to the available communication bandwidth for transmitting the parameters during the pFL iteration, i.e., to do a trade-off between the training efficiency and the communication costs.\n\n\nConclusion\n\nIn this paper, we have proposed a novel pFL training framework called pFedLA, to achieve personalized model aggregation in a layer-wised aggregation manner. It is shown that such layer-wised aggregation can progressively reinforce the collaboration among similar clients and generate adequate personalization over non-IID datasets that outperform conventional model-wised approaches. In addition, we have provided an improved version of pFedLA that can reduce the communication overhead during the training process with negligible performance loss, and thus can be adapted to large scale FL scenarios where the communication capacity is often limited. Extensive evaluations on four different classification tasks demonstrate the feasibility and superior performance of the proposed pFedLA framework.\n\nFigure 1 .\n1A toy example: Layer-wised vs. Model-wised aggregation method. (a) Model performance of client 1. Both of two methods perform similarity-based personalized aggregation. i.e., layer-wised: perform personalized aggregation by calculating the similarity between layers; model-wised: perform personalized aggregation by calculating the similarity between models. (b) The weight of each layer for client 1 in the last communication round.\n\nFigure 3 .\n3Illustration of one hypernetwork framework used in pFedLA. The hypernetwork HNi takes the embedding vector vi as input, and outputs the aggregation weight matrix \u03b1i. After the weighted combination with intermediate parameters {\u03b8 l1 , . . . , \u03b8 ln } and aggregation weight matrix \u03b1i, client i can make local training on private data. Note that both vi and \u03c8i are updated during training.\n\n\nl1 , \u03b8 l2 , . . . , \u03b8 ln } according to \u2206\u03b8 i\n\n\n, which will not be send to the client , and with no aggregation Other layers, which will be aggregated before sending to the client\n\nFigure 4 .\n4Illustration of top k mechanism in HeurpFedLA. The selected top k layers (i.e., retained layers) do not perform aggregation process, while the remaining layers execute the same operations as in pFedLA.\n\nFigure 5 .Figure 6 .\n56FedBN, keeps each client's BN layer updating locally, while other layers are aggregated according to the FedAvg algorithm; FedRep, a pFL algorithm that keeps each client's classifier updating locally, while the other parts are aggregated at the Change of aggregation weights during the prior, middle and last period of training phase. The visualization of the aggregation weights in a specific layer on EMNIST, FashionMNIST, CIFAR10 and CIFAR100. X-axis and y-axis show the IDs of clients.\n\nFigure 7 .\n7The aggregation weights of all layers for the target client.\n\n\nLet {\u03b8 l1 , \u03b8 l2 , . . . , \u03b8 ln } be the intermediate parameters of all clients after local training, \u03b8 ln = {\u03b8 ln 1 , \u03b8 ln 2 , . . . , \u03b8 ln N } is the Algorithm 1 pFedLA Algorithm Input: dataset {D 1 , D 2 , . . . , D N }, learning rate \u03b7. Total communication rounds T . Output: Trained personalized models {\u03b8 1 ,\u03b8 2 , . . . ,\u03b8 N }.1: Initialize the clients' model parameters, hypernetworks \nparameters and embedding vectors. \n2: procedure SERVER EXECUTES \n\n3: \n\n\n\n\nAlgorithm 2 HeurpFedLA AlgorithmInput: dataset {D 1 , D 2 , . . . , D N }, learning rate \u03b7. Total communication rounds T . Output: Trained personalized models {\u03b8 1 ,\u03b8 2 , . . . ,\u03b8 N }.1: Initialize the clients' model parameters, hypernetworks \nparameters and embedding vectors. \n2: procedure SERVER EXECUTES \n\n3: \n\n\n\nTable 1 .Table 2 .\n12Average model accuracy on 10 and 100 clients over four different datasets(non-IID 1), respectively. Ours) 94.11\u00b10.13 95.04\u00b10.41 95.47\u00b10.47 96.95\u00b10.44 60.02\u00b10.74 73.05\u00b11.02 46.47\u00b10.83 54.43\u00b11.37 Average model accuracy on 10 and 100 clients over four different datasets(non-IID 2), respectively.EMNIST (%) \nFashionMNIST (%) \nCIFAR10 (%) \nCIFAR100 (%) \n\n# Clients \n10 \n100 \n10 \n100 \n10 \n100 \n10 \n100 \n\nLocal Training \n89.01\u00b10.47 91.25\u00b10.18 85.83\u00b10.17 89.27\u00b10.21 59.44\u00b10.40 64.19\u00b10.19 41.68\u00b10.89 42.53\u00b10.44 \nFedAvg [34] \n90.45\u00b10.76 93.71\u00b10.38 91.24\u00b10.98 98.36\u00b10.26 48.57\u00b10.63 58.43\u00b10.29 36.64\u00b10.67 45.19\u00b10.33 \nPer-FedAvg [9] \n92.58\u00b10.28 92.38\u00b11.14 93.63\u00b11.83 92.35\u00b11.55 52.54\u00b11.79 59.54\u00b10.39 38.79\u00b11.89 43.72\u00b10.25 \npFedMe [41] \n92.42\u00b10.44 94.36\u00b10.50 90.43\u00b10.86 98.57\u00b10.38 53.73\u00b13.74 65.97\u00b11.61 42.29\u00b13.67 53.60\u00b11.28 \npFedHN [37] \n93.94\u00b10.16 96.64\u00b10.91 94.83\u00b10.33 98.80\u00b10.92 46.98\u00b11.91 63.71\u00b11.26 39.67\u00b10.52 51.36\u00b11.77 \nFedBN [24] \n-\n-\n-\n-\n59.36\u00b10.92 70.88\u00b10.36 45.18\u00b10.42 56.16\u00b10.38 \nFedRep [5] \n91.82\u00b10.15 95.23\u00b10.12 93.17\u00b10.26 97.15\u00b10.09 58.01\u00b10.56 71.94\u00b10.22 44.33\u00b10.63 56.47\u00b10.41 \nFedFomo [54] \n88.33\u00b10.29 91.36\u00b10.17 86.17\u00b10.34 91.83\u00b10.12 59.37\u00b10.71 66.07\u00b10.24 41.89\u00b10.78 44.28\u00b10.28 \n\npFedLA (Ours) \n90.65\u00b10.41 96.34\u00b11.35 94.34\u00b10.29 98.87\u00b10.66 61.43\u00b10.56 73.15\u00b10.83 47.22\u00b10.77 56.62\u00b10.81 \nHeurpFedLA (EMNIST (%) \nFashionMNIST (%) \nCIFAR10 (%) \nCIFAR100 (%) \n\n# Clients \n10 \n100 \n10 \n100 \n10 \n100 \n10 \n100 \n\nLocal Training \n80.72\u00b10.43 79.09\u00b10.12 65.60\u00b10.59 65.97\u00b10.28 39.79\u00b10.42 45.15\u00b10.29 26.29\u00b10.37 27.87\u00b10.28 \nFedAvg [34] \n90.43\u00b10.58 93.91\u00b10.32 89.09\u00b10.57 98.25\u00b10.38 44.89\u00b10.21 54.03\u00b10.37 32.24\u00b10.74 40.89\u00b10.46 \nPer-FedAvg [9] \n90.86\u00b10.78 94.09\u00b10.18 90.78\u00b11.12 98.53\u00b10.95 44.48\u00b10.82 54.40\u00b10.44 30.86\u00b11.11 42.56\u00b10.28 \npFedMe [41] \n89.13\u00b10.58 93.87\u00b10.40 85.15\u00b10.94 97.87\u00b10.19 46.97\u00b11.19 58.23\u00b11.07 33.45\u00b10.86 44.35\u00b10.96 \npFedHN [37] \n91.37\u00b10.41 94.48\u00b10.51 93.45\u00b10.11 98.83\u00b10.82 37.49\u00b10.94 49.90\u00b11.66 26.35\u00b10.93 40.27\u00b10.82 \nFedBN [24] \n-\n-\n-\n-\n49.79\u00b10.33 60.62\u00b10.42 34.94\u00b10.50 46.42\u00b10.54 \nFedRep [5] \n86.81\u00b10.29 90.32\u00b10.08 79.13\u00b10.56 92.04\u00b10.23 49.16\u00b10.73 60.36\u00b10.57 34.19\u00b10.74 43.51\u00b10.34 \nFedFomo [54] \n80.14\u00b10.42 82.61\u00b10.11 64.10\u00b10.38 67.91\u00b10.29 40.62\u00b10.31 47.08\u00b10.49 27.33\u00b10.51 29.63\u00b10.24 \n\npFedLA (Ours) \n92.06\u00b10.71 94.83\u00b11.04 93.89\u00b10.91 98.41\u00b10.98 49.93\u00b10.96 61.82\u00b11.89 35.02\u00b10.83 48.79\u00b11.60 \nHeurpFedLA (Ours) 91.98\u00b10.36 93.31\u00b10.77 92.01\u00b10.74 98.66\u00b10.80 49.06\u00b10.68 60.62\u00b11.73 35.42\u00b10.49 48.72\u00b11.75 \n\n\n\nTable 3 .\n3Average Model Accuracy and Communication Cost on different number of retained layers (i.e., k) over EMNIST and CIFAR10. MBytes) 491.08 488.65 312.52 693.98 418.97 382.25 379.82EMNIST \nCIFAR10 \n\n\nAcknowledgements\nVinay Manoj Ghuhan Arivazhagan, Aggarwal, arXiv:1912.00818Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. arXiv preprintManoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Ku- mar Singh, and Sunav Choudhary. Federated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019. 2\n\nJuhan Bae, Roger Grosse, arXiv:2010.13514Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians. arXiv preprintJuhan Bae and Roger Grosse. Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians. arXiv preprint arXiv:2010.13514, 2020. 3\n\nSmash: one-shot model architecture search through hypernetworks. Andrew Brock, Theodore Lim, M James, Nick Ritchie, Weston, arXiv:1708.05344arXiv preprintAndrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017. 3\n\n. Duc Bui, Kshitiz Malik, Jack Goetz, Honglei Liu, Seungwhan Moon, Anuj Kumar, Kang G Shin, arXiv:1909.12535arXiv preprintFederated user representation learningDuc Bui, Kshitiz Malik, Jack Goetz, Honglei Liu, Seung- whan Moon, Anuj Kumar, and Kang G Shin. Federated user representation learning. arXiv preprint arXiv:1909.12535, 2019. 2\n\nExploiting shared representations for personalized federated learning. Liam Collins, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai, Proceedings of the 38th International Conference on Machine Learning, ICML. the 38th International Conference on Machine Learning, ICML139Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personal- ized federated learning. In Proceedings of the 38th Interna- tional Conference on Machine Learning, ICML, volume 139, pages 2089-2099, 2021. 1, 2, 5, 6, 8\n\nChirping up the right tree: Incorporating biological taxonomies into deep bioacoustic classifiers. Jason Cramer, Vincent Lostanlen, Andrew Farnsworth, Justin Salamon, Juan Pablo Bello, Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)IEEEJason Cramer, Vincent Lostanlen, Andrew Farnsworth, Justin Salamon, and Juan Pablo Bello. Chirping up the right tree: Incorporating biological taxonomies into deep bioa- coustic classifiers. In Proceedings of 2020 IEEE Interna- tional Conference on Acoustics, Speech and Signal Process- ing (ICASSP), pages 901-905. IEEE, 2020. 2\n\nAdaptive personalized federated learning. Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi, arXiv:2003.13461arXiv preprintYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning. arXiv preprint arXiv:2003.13461, 2020. 2\n\nSelf-balancing federated learning with global imbalanced data in mobile systems. Moming Duan, Duo Liu, Xianzhang Chen, Renping Liu, Yujuan Tan, Liang Liang, IEEE Transactions on Parallel and Distributed Systems. 321Moming Duan, Duo Liu, Xianzhang Chen, Renping Liu, Yu- juan Tan, and Liang Liang. Self-balancing federated learning with global imbalanced data in mobile systems. IEEE Trans- actions on Parallel and Distributed Systems, 32(1):59-71, 2020. 1, 2\n\nPersonalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Alireza Fallah, Aryan Mokhtari, Asuman E Ozdaglar, Advances in Neural Information Processing Systems. 6Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In Advances in Neural Information Processing Systems, NeurIPS, 2020. 1, 2, 6\n\nAn efficient framework for clustered federated learning. Avishek Ghosh, Jichan Chung, Dong Yin, Kannan Ramchandran, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ram- chandran. An efficient framework for clustered federated learning. In Proceedings of Advances in Neural Information Processing Systems, NeurIPS, 2020. 2\n\n. David Ha, Andrew Dai, Quoc V Le, Hypernetworks, arXiv:1609.09106arXiv preprintDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016. 3\n\nLower bounds and optimal algorithms for personalized federated learning. Filip Hanzely, Slavom\u00edr Hanzely, Samuel Horv\u00e1th, Peter Richt\u00e1rik, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing Systems1Filip Hanzely, Slavom\u00edr Hanzely, Samuel Horv\u00e1th, and Pe- ter Richt\u00e1rik. Lower bounds and optimal algorithms for per- sonalized federated learning. In Proceedings of Advances in Neural Information Processing Systems, NeurIPS, 2020. 1, 2\n\nFederated learning of a mixture of global and local models. Filip Hanzely, Peter Richt\u00e1rik, arXiv:2002.05516arXiv preprintFilip Hanzely and Peter Richt\u00e1rik. Federated learning of a mixture of global and local models. arXiv preprint arXiv:2002.05516, 2020. 2\n\nThe non-iid data quagmire of decentralized machine learning. Kevin Hsieh, Amar Phanishayee, Onur Mutlu, Phillip Gibbons, Proceedings of International Conference on Machine Learning, ICML. International Conference on Machine Learning, ICMLKevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of decentralized ma- chine learning. In Proceedings of International Conference on Machine Learning, ICML, pages 4387-4398, 2020. 1\n\nPersonalized cross-silo federated learning on non-iid data. Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, Yong Zhang, Proceedings of the AAAI Conference on Artificial Intelligence, AAAI. the AAAI Conference on Artificial Intelligence, AAAI35Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized cross-silo federated learning on non-iid data. In Proceed- ings of the AAAI Conference on Artificial Intelligence, AAAI, volume 35, pages 7865-7873, 2021. 1, 2, 4\n\nCommunicationefficient on-device machine learning: Federated distillation and augmentation under non-iid private data. Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, arXiv:1811.114791arXiv preprintEunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Communication- efficient on-device machine learning: Federated distillation and augmentation under non-iid private data. arXiv preprint arXiv:1811.11479, 2018. 1, 2\n\nXu Jia, Bert De Brabandere, Tinne Tuytelaars, Luc V Gool, Proceedings of Advances in neural information processing systems. Advances in neural information processing systems29Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. Proceedings of Advances in neural information processing systems, NeurIPS, 29:667- 675, 2016. 3\n\nImproving federated learning personalization via model agnostic meta learning. Yihan Jiang, Jakub Kone\u010dn\u1ef3, Keith Rush, Sreeram Kannan, arXiv:1909.12488arXiv preprintYihan Jiang, Jakub Kone\u010dn\u1ef3, Keith Rush, and Sreeram Kan- nan. Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019. 2\n\nHypernetwork functional image representation. \u0141ukasz Sylwester Klocek, Maciej Maziarka, Jacek Wo\u0142czyk, Jakub Tabor, Marek\u015bmieja Nowak, Proceedings of International Conference on Artificial Neural Networks, ICANN. International Conference on Artificial Neural Networks, ICANNSpringerSylwester Klocek, \u0141ukasz Maziarka, Maciej Wo\u0142czyk, Jacek Tabor, Jakub Nowak, and Marek\u015amieja. Hypernetwork functional image representation. In Proceedings of Inter- national Conference on Artificial Neural Networks, ICANN, pages 496-510. Springer, 2019. 3\n\nSunwoo Lee, Tuo Zhang, arXiv:2110.10302Chaoyang He, and Salman Avestimehr. Layer-wise adaptive model aggregation for scalable federated learning. arXiv preprintSunwoo Lee, Tuo Zhang, Chaoyang He, and Salman Aves- timehr. Layer-wise adaptive model aggregation for scalable federated learning. arXiv preprint arXiv:2110.10302, 2021. 2\n\nAn improved deep learning approach for detection of thyroid papillary cancer in ultrasound images. Hailiang Li, Jian Weng, Yujian Shi, Wanrong Gu, Yijun Mao, Yonghua Wang, Weiwei Liu, Jiajie Zhang, Scientific reports. 81Hailiang Li, Jian Weng, Yujian Shi, Wanrong Gu, Yijun Mao, Yonghua Wang, Weiwei Liu, and Jiajie Zhang. An improved deep learning approach for detection of thyroid papillary cancer in ultrasound images. Scientific reports, 8(1):1-12, 2018. 2\n\nDitto: Fair and robust federated learning through personalization. Tian Li, Shengyuan Hu, Ahmad Beirami, Virginia Smith, Proceedings of International Conference on Machine Learning, ICML. International Conference on Machine Learning, ICMLTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through per- sonalization. In Proceedings of International Conference on Machine Learning, ICML, pages 6357-6368, 2021. 1\n\nFederated optimization in heterogeneous networks. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith, arXiv:1812.06127arXiv preprintTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San- jabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018. 1\n\nFedbn: Federated learning on non-iid features via local batch normalization. Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, Qi Dou, Proceedings of the 9th International Conference on Learning Representations, ICLR. OpenReview.net. the 9th International Conference on Learning Representations, ICLR. OpenReview.net67Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In Proceedings of the 9th In- ternational Conference on Learning Representations, ICLR. OpenReview.net, 2021. 1, 5, 6, 7\n\nDifferentiable meta pruning via hypernetworks. Yawei Li, Shuhang Gu, Luc Van Gool, Radu Timofte, Proceedings of European Conference on Computer Vision and Pattern Recognition. European Conference on Computer Vision and Pattern RecognitionYawei Li, Shuhang Gu, Luc Van Gool, Radu Timofte, et al. Dhp: Differentiable meta pruning via hypernetworks. In Pro- ceedings of European Conference on Computer Vision and Pattern Recognition, ECCV, 2020. 3\n\nTerrance Paul Pu Liang, Liu Liu, Ziyin, Randy P Nicholas B Allen, David Auerbach, Ruslan Brent, Louis-Philippe Salakhutdinov, Morency, arXiv:2001.01523Think locally, act globally: Federated learning with local and global representations. 15arXiv preprintPaul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Fed- erated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020. 1, 5\n\nOn infinite-width hypernetworks. Etai Littwin, Tomer Galanti, Lior Wolf, Greg Yang, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing Systems33Etai Littwin, Tomer Galanti, Lior Wolf, and Greg Yang. On infinite-width hypernetworks. Proceedings of Advances in Neural Information Processing Systems, NeurIPS, 33, 2020. 3\n\nDeep meta functionals for shape representation. Gidi Littwin, Lior Wolf, Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV. the IEEE/CVF International Conference on Computer Vision, ICCVGidi Littwin and Lior Wolf. Deep meta functionals for shape representation. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, ICCV, pages 1824- 1833, 2019. 3\n\nJonathan Lorraine, David Duvenaud, arXiv:1802.09419Stochastic hyperparameter optimization through hypernetworks. arXiv preprintJonathan Lorraine and David Duvenaud. Stochastic hyperpa- rameter optimization through hypernetworks. arXiv preprint arXiv:1802.09419, 2018. 3\n\nTowards fair and privacy-preserving federated deep models. Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong Jin, Han Yu, Kee Siong Ng, IEEE Transactions on Parallel and Distributed Systems. 3111Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong Jin, Han Yu, and Kee Siong Ng. Towards fair and privacy-preserving federated deep mod- els. IEEE Transactions on Parallel and Distributed Systems, 31(11):2524-2541, 2020. 2\n\nSelf-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions. Matthew Mackay, Paul Vicol, Jon Lorraine, David Duvenaud, Roger Grosse, arXiv:1903.03088arXiv preprintMatthew MacKay, Paul Vicol, Jon Lorraine, David Duve- naud, and Roger Grosse. Self-tuning networks: Bilevel opti- mization of hyperparameters using structured best-response functions. arXiv preprint arXiv:1903.03088, 2019. 3\n\nThree approaches for personalization with applications to federated learning. Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh, arXiv:2002.10619arXiv preprintYishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for per- sonalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020. 2\n\nCommunicationefficient learning of deep networks from decentralized data. Brendan Mcmahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera Y Arcas, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. the 20th International Conference on Artificial Intelligence and Statistics1Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication- efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Ar- tificial Intelligence and Statistics, AISTATS, 2017. 1, 2\n\nCommunicationefficient learning of deep networks from decentralized data. Brendan Mcmahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera Y Arcas, PMLRAISTATS. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication- efficient learning of deep networks from decentralized data. In AISTATS, pages 1273-1282. PMLR, 2017. 6\n\nHyperseg: Patchwise hypernetwork for real-time semantic segmentation. Yuval Nirkin, Lior Wolf, Tal Hassner, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPRYuval Nirkin, Lior Wolf, and Tal Hassner. Hyperseg: Patch- wise hypernetwork for real-time semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, CVPR, pages 4061-4070, 2021. 3\n\n. Johannes Schneider, Michail Vlachos, arXiv:1909.02803Personalization of deep learning. arXiv preprintJohannes Schneider and Michail Vlachos. Personalization of deep learning. arXiv preprint arXiv:1909.02803, 2019. 2\n\nPersonalized federated learning using hypernetworks. Aviv Shamsian, Aviv Navon, Ethan Fetaya, Gal Chechik, Proceedings of the 38th International Conference on Machine Learning, ICML. the 38th International Conference on Machine Learning, ICML1397Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In Proceedings of the 38th International Conference on Ma- chine Learning, ICML, volume 139, pages 9489-9502, 2021. 3, 6, 7\n\nImplicit neural representations with periodic activation functions. Vincent Sitzmann, N P Julien, Alexander W Martel, David B Bergman, Gordon Lindell, Wetzstein, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsNeurIPSVincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Proceedings of Advances in Neural Information Processing Systems, NeurIPS, 2020. 3\n\nFederated multi-task learning. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S Talwalkar, Advances in Neural Information Processing Systems. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar. Federated multi-task learning. In Ad- vances in Neural Information Processing Systems, NeurIPS, 2017. 2\n\nLanguage modeling with recurrent highway hypernetworks. Joseph Suarez, Proceedings of Advances in neural information processing systems. Advances in neural information processing systemsJoseph Suarez. Language modeling with recurrent highway hypernetworks. In Proceedings of Advances in neural in- formation processing systems, NeurIPS, pages 3267-3276, 2017. 3\n\nPersonalized federated learning with moreau envelopes. Proceedings of Advances in Neural Information Processing Systems. T Canh, Nguyen Dinh, Tuan Dung Tran, Nguyen, NeurIPS. 336Canh T Dinh, Nguyen Tran, and Tuan Dung Nguyen. Per- sonalized federated learning with moreau envelopes. Pro- ceedings of Advances in Neural Information Processing Sys- tems, NeurIPS, 33, 2020. 1, 2, 6\n\nAlysa Ziying Tan, Han Yu, Lizhen Cui, Qiang Yang, arXiv:2103.00710Towards personalized federated learning. arXiv preprintAlysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. arXiv preprint arXiv:2103.00710, 2021. 2\n\nFederated learning over wireless networks: Optimization model design and analysis. H Nguyen, Wei Tran, Albert Bao, Zomaya, N H Minh, Choong Seon Nguyen, Hong, Proceedings of IEEE Conference on Computer Communications, INFOCOM. IEEE Conference on Computer Communications, INFOCOMIEEENguyen H Tran, Wei Bao, Albert Zomaya, Minh NH Nguyen, and Choong Seon Hong. Federated learning over wireless networks: Optimization model design and analysis. In Proceedings of IEEE Conference on Computer Communi- cations, INFOCOM, pages 1387-1395. IEEE, 2019. 1\n\nOswald Johannes Von, Christian Henning, Jo\u00e3o Sacramento, Benjamin F Grewe, arXiv:1906.00695Continual learning with hypernetworks. arXiv preprintJohannes von Oswald, Christian Henning, Jo\u00e3o Sacramento, and Benjamin F Grewe. Continual learning with hypernet- works. arXiv preprint arXiv:1906.00695, 2019. 3\n\nOptimizing federated learning on non-iid data with reinforcement learning. Hao Wang, Zakhary Kaplan, Di Niu, Baochun Li, INFOCOM. Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Opti- mizing federated learning on non-iid data with reinforcement learning. In INFOCOM, pages 1698-1707. IEEE, 2020. 2\n\nFederated evaluation of on-device personalization. Kangkang Wang, Rajiv Mathews, Chlo\u00e9 Kiddon, Hubert Eichner, Fran\u00e7oise Beaufays, Daniel Ramage, arXiv:1910.10252arXiv preprintKangkang Wang, Rajiv Mathews, Chlo\u00e9 Kiddon, Hubert Eichner, Fran\u00e7oise Beaufays, and Daniel Ramage. Feder- ated evaluation of on-device personalization. arXiv preprint arXiv:1910.10252, 2019. 2\n\nAttention-mechanism-containing neural networks for highresolution remote sensing image classification. Rudong Xu, Yiting Tao, Zhongyuan Lu, Yanfei Zhong, Remote Sensing. 10101602Rudong Xu, Yiting Tao, Zhongyuan Lu, and Yanfei Zhong. Attention-mechanism-containing neural networks for high- resolution remote sensing image classification. Remote Sens- ing, 10(10):1602, 2018. 2\n\nFederated learning with class imbalance reduction. Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, Hua Qian, arXiv:2011.11266arXiv preprintMiao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, and Hua Qian. Federated learning with class imbal- ance reduction. arXiv preprint arXiv:2011.11266, 2020. 2\n\nDeep layer aggregation. Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition, CVPR. the IEEE conference on computer vision and pattern recognition, CVPRFisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, CVPR, pages 2403-2412, 2018. 2\n\nGraph hypernetworks for neural architecture search. Chris Zhang, Mengye Ren, Raquel Urtasun, arXiv:1810.05749arXiv preprintChris Zhang, Mengye Ren, and Raquel Urtasun. Graph hy- pernetworks for neural architecture search. arXiv preprint arXiv:1810.05749, 2018. 3\n\nParameterized knowledge transfer for personalized federated learning. Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, Feijie Wu, Advances in Neural Information Processing Systems. 34Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wen- chao Xu, and Feijie Wu. Parameterized knowledge transfer for personalized federated learning. Advances in Neural In- formation Processing Systems, 34, 2021. 2\n\nAdaptive federated learning on non-iid data with resource constraint. Jie Zhang, Song Guo, Zhihao Qu, Deze Zeng, Yufeng Zhan, Qifeng Liu, Akerkar, IEEE Transactions on Computers. 1Jie Zhang, Song Guo, Zhihao Qu, Deze Zeng, Yufeng Zhan, Qifeng Liu, and Rajendra A Akerkar. Adaptive federated learning on non-iid data with resource constraint. IEEE Transactions on Computers, 2021. 1\n\nEdge learning: The enabling technology for distributed big data analytics in the edge. Jie Zhang, Zhihao Qu, Chenxi Chen, Haozhao Wang, Yufeng Zhan, Baoliu Ye, Song Guo, ACM Computing Surveys (CSUR). 547Jie Zhang, Zhihao Qu, Chenxi Chen, Haozhao Wang, Yufeng Zhan, Baoliu Ye, and Song Guo. Edge learning: The en- abling technology for distributed big data analytics in the edge. ACM Computing Surveys (CSUR), 54(7):1-36, 2021. 1\n\nPersonalized federated learning with first order model optimization. Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, Jose M Alvarez, Proceedings of the 9th International Conference on Learning Representations. the 9th International Conference on Learning Representations67Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated learning with first order model optimization. In Proceedings of the 9th In- ternational Conference on Learning Representations, ICLR 2021, 2021. 1, 2, 4, 6, 7\n\nMeta-learning via hypernetworks. Dominic Zhao, Seijin Johannes Von Oswald, Jo\u00e3o Kobayashi, Sacramento, Benjamin F Grewe, 2020Dominic Zhao, Johannes von Oswald, Seijin Kobayashi, Jo\u00e3o Sacramento, and Benjamin F Grewe. Meta-learning via hypernetworks. 2020. 3\n\nFederated learning with non-iid data. Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, Vikas Chandra, arXiv:1806.005821arXiv preprintYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018. 1, 2\n", "annotations": {"author": "[{\"end\":167,\"start\":69},{\"end\":270,\"start\":168},{\"end\":432,\"start\":271},{\"end\":531,\"start\":433}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":78},{\"end\":177,\"start\":172},{\"end\":279,\"start\":276},{\"end\":443,\"start\":441}]", "author_first_name": "[{\"end\":77,\"start\":69},{\"end\":171,\"start\":168},{\"end\":275,\"start\":271},{\"end\":440,\"start\":433}]", "author_affiliation": "[{\"end\":166,\"start\":105},{\"end\":269,\"start\":208},{\"end\":364,\"start\":303},{\"end\":431,\"start\":366},{\"end\":530,\"start\":469}]", "title": "[{\"end\":66,\"start\":1},{\"end\":597,\"start\":532}]", "venue": null, "abstract": "[{\"end\":1997,\"start\":599}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2177,\"start\":2173},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2180,\"start\":2177},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2183,\"start\":2180},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2306,\"start\":2302},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2309,\"start\":2306},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2518,\"start\":2514},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2521,\"start\":2518},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2719,\"start\":2716},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2722,\"start\":2719},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2725,\"start\":2722},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2728,\"start\":2725},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2875,\"start\":2872},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2878,\"start\":2875},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2934,\"start\":2930},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2937,\"start\":2934},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2956,\"start\":2953},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2982,\"start\":2979},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2985,\"start\":2982},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2988,\"start\":2985},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3071,\"start\":3067},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3074,\"start\":3071},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3823,\"start\":3820},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3826,\"start\":3823},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3829,\"start\":3826},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3832,\"start\":3829},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3835,\"start\":3832},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7755,\"start\":7751},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7773,\"start\":7770},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7776,\"start\":7773},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8129,\"start\":8125},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8246,\"start\":8242},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8249,\"start\":8246},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8252,\"start\":8249},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8443,\"start\":8439},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8581,\"start\":8578},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8584,\"start\":8581},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8587,\"start\":8584},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8608,\"start\":8604},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8611,\"start\":8608},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8614,\"start\":8611},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8633,\"start\":8630},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8636,\"start\":8633},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8655,\"start\":8652},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8658,\"start\":8655},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8690,\"start\":8687},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8692,\"start\":8690},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8694,\"start\":8692},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8897,\"start\":8893},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8900,\"start\":8897},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8903,\"start\":8900},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9175,\"start\":9171},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9190,\"start\":9186},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9202,\"start\":9198},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10045,\"start\":10041},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10317,\"start\":10313},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10320,\"start\":10317},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10342,\"start\":10338},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10345,\"start\":10342},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10348,\"start\":10345},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10378,\"start\":10374},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10381,\"start\":10378},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10414,\"start\":10411},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10417,\"start\":10414},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10420,\"start\":10417},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10423,\"start\":10420},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10461,\"start\":10458},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10464,\"start\":10461},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10489,\"start\":10485},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10512,\"start\":10508},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10534,\"start\":10530},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13736,\"start\":13732},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13739,\"start\":13736},{\"end\":14619,\"start\":14617},{\"end\":14680,\"start\":14677},{\"end\":14712,\"start\":14709},{\"end\":15816,\"start\":15814},{\"end\":16220,\"start\":16217},{\"end\":16252,\"start\":16249},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18086,\"start\":18083},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18089,\"start\":18086},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18092,\"start\":18089},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18110,\"start\":18106},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18242,\"start\":18239},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18261,\"start\":18257},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":21685,\"start\":21681},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21697,\"start\":21693},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21713,\"start\":21709},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26763,\"start\":26760}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28797,\"start\":28351},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29197,\"start\":28798},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29244,\"start\":29198},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29379,\"start\":29245},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29594,\"start\":29380},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30108,\"start\":29595},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30182,\"start\":30109},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30649,\"start\":30183},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30967,\"start\":30650},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33395,\"start\":30968},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33602,\"start\":33396}]", "paragraph": "[{\"end\":2522,\"start\":2013},{\"end\":3988,\"start\":2524},{\"end\":5482,\"start\":3990},{\"end\":6639,\"start\":5484},{\"end\":6835,\"start\":6641},{\"end\":7054,\"start\":6837},{\"end\":7226,\"start\":7056},{\"end\":7658,\"start\":7277},{\"end\":8253,\"start\":7660},{\"end\":9220,\"start\":8255},{\"end\":10009,\"start\":9222},{\"end\":10805,\"start\":10027},{\"end\":10985,\"start\":10816},{\"end\":11181,\"start\":11009},{\"end\":11553,\"start\":11460},{\"end\":11602,\"start\":11597},{\"end\":11948,\"start\":11664},{\"end\":12779,\"start\":11969},{\"end\":14199,\"start\":13123},{\"end\":14432,\"start\":14201},{\"end\":14728,\"start\":14461},{\"end\":15077,\"start\":14891},{\"end\":15204,\"start\":15142},{\"end\":15307,\"start\":15241},{\"end\":15658,\"start\":15450},{\"end\":15850,\"start\":15660},{\"end\":16191,\"start\":16188},{\"end\":16268,\"start\":16193},{\"end\":16570,\"start\":16357},{\"end\":17391,\"start\":16762},{\"end\":17922,\"start\":17467},{\"end\":18969,\"start\":17924},{\"end\":19712,\"start\":19048},{\"end\":20283,\"start\":19714},{\"end\":21084,\"start\":20319},{\"end\":22728,\"start\":21086},{\"end\":23739,\"start\":22755},{\"end\":25822,\"start\":23772},{\"end\":27061,\"start\":25863},{\"end\":27536,\"start\":27077},{\"end\":28350,\"start\":27551}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11459,\"start\":11182},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11596,\"start\":11554},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11663,\"start\":11603},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12939,\"start\":12780},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13122,\"start\":12939},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14460,\"start\":14433},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14827,\"start\":14729},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14890,\"start\":14827},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15141,\"start\":15078},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15240,\"start\":15205},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15449,\"start\":15308},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15957,\"start\":15851},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16063,\"start\":15957},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16147,\"start\":16063},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16187,\"start\":16147},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16356,\"start\":16269},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16666,\"start\":16571},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16761,\"start\":16666},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19047,\"start\":18970}]", "table_ref": "[{\"end\":23130,\"start\":23123},{\"end\":23142,\"start\":23135},{\"end\":23471,\"start\":23464},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25939,\"start\":25932},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27161,\"start\":27154}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2011,\"start\":1999},{\"attributes\":{\"n\":\"2.\"},\"end\":7241,\"start\":7229},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7275,\"start\":7244},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10025,\"start\":10012},{\"attributes\":{\"n\":\"3.\"},\"end\":10814,\"start\":10808},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11007,\"start\":10988},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11967,\"start\":11951},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17465,\"start\":17394},{\"attributes\":{\"n\":\"4.\"},\"end\":20296,\"start\":20286},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20317,\"start\":20299},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22753,\"start\":22731},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23770,\"start\":23742},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25861,\"start\":25825},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27075,\"start\":27064},{\"attributes\":{\"n\":\"5.\"},\"end\":27549,\"start\":27539},{\"end\":28362,\"start\":28352},{\"end\":28809,\"start\":28799},{\"end\":29391,\"start\":29381},{\"end\":29616,\"start\":29596},{\"end\":30120,\"start\":30110},{\"end\":30987,\"start\":30969},{\"end\":33406,\"start\":33397}]", "table": "[{\"end\":30649,\"start\":30518},{\"end\":30967,\"start\":30836},{\"end\":33395,\"start\":31283},{\"end\":33602,\"start\":33584}]", "figure_caption": "[{\"end\":28797,\"start\":28364},{\"end\":29197,\"start\":28811},{\"end\":29244,\"start\":29200},{\"end\":29379,\"start\":29247},{\"end\":29594,\"start\":29393},{\"end\":30108,\"start\":29619},{\"end\":30182,\"start\":30122},{\"end\":30518,\"start\":30185},{\"end\":30836,\"start\":30652},{\"end\":31283,\"start\":30990},{\"end\":33584,\"start\":33408}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4940,\"start\":4932},{\"end\":9350,\"start\":9342},{\"end\":10984,\"start\":10976},{\"end\":12337,\"start\":12329},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14562,\"start\":14554},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19239,\"start\":19231},{\"end\":24282,\"start\":24274},{\"end\":25387,\"start\":25379},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26253,\"start\":26245}]", "bib_author_first_name": "[{\"end\":33625,\"start\":33620},{\"end\":33963,\"start\":33958},{\"end\":33974,\"start\":33969},{\"end\":34352,\"start\":34346},{\"end\":34368,\"start\":34360},{\"end\":34375,\"start\":34374},{\"end\":34387,\"start\":34383},{\"end\":34609,\"start\":34606},{\"end\":34622,\"start\":34615},{\"end\":34634,\"start\":34630},{\"end\":34649,\"start\":34642},{\"end\":34664,\"start\":34655},{\"end\":34675,\"start\":34671},{\"end\":34689,\"start\":34683},{\"end\":35017,\"start\":35013},{\"end\":35032,\"start\":35027},{\"end\":35047,\"start\":35042},{\"end\":35064,\"start\":35058},{\"end\":35591,\"start\":35586},{\"end\":35607,\"start\":35600},{\"end\":35625,\"start\":35619},{\"end\":35644,\"start\":35638},{\"end\":35658,\"start\":35654},{\"end\":35664,\"start\":35659},{\"end\":36244,\"start\":36238},{\"end\":36259,\"start\":36251},{\"end\":36265,\"start\":36260},{\"end\":36281,\"start\":36274},{\"end\":36549,\"start\":36543},{\"end\":36559,\"start\":36556},{\"end\":36574,\"start\":36565},{\"end\":36588,\"start\":36581},{\"end\":36600,\"start\":36594},{\"end\":36611,\"start\":36606},{\"end\":37031,\"start\":37024},{\"end\":37045,\"start\":37040},{\"end\":37062,\"start\":37056},{\"end\":37064,\"start\":37063},{\"end\":37427,\"start\":37420},{\"end\":37441,\"start\":37435},{\"end\":37453,\"start\":37449},{\"end\":37465,\"start\":37459},{\"end\":37810,\"start\":37805},{\"end\":37821,\"start\":37815},{\"end\":37833,\"start\":37827},{\"end\":38055,\"start\":38050},{\"end\":38073,\"start\":38065},{\"end\":38089,\"start\":38083},{\"end\":38104,\"start\":38099},{\"end\":38534,\"start\":38529},{\"end\":38549,\"start\":38544},{\"end\":38794,\"start\":38789},{\"end\":38806,\"start\":38802},{\"end\":38824,\"start\":38820},{\"end\":38839,\"start\":38832},{\"end\":39254,\"start\":39249},{\"end\":39270,\"start\":39262},{\"end\":39281,\"start\":39276},{\"end\":39294,\"start\":39288},{\"end\":39311,\"start\":39301},{\"end\":39321,\"start\":39317},{\"end\":39331,\"start\":39327},{\"end\":39860,\"start\":39852},{\"end\":39876,\"start\":39868},{\"end\":39888,\"start\":39881},{\"end\":39900,\"start\":39894},{\"end\":39912,\"start\":39907},{\"end\":39931,\"start\":39921},{\"end\":40225,\"start\":40223},{\"end\":40235,\"start\":40231},{\"end\":40256,\"start\":40251},{\"end\":40274,\"start\":40269},{\"end\":40666,\"start\":40661},{\"end\":40679,\"start\":40674},{\"end\":40694,\"start\":40689},{\"end\":40708,\"start\":40701},{\"end\":40982,\"start\":40976},{\"end\":41007,\"start\":41001},{\"end\":41023,\"start\":41018},{\"end\":41038,\"start\":41033},{\"end\":41057,\"start\":41046},{\"end\":41475,\"start\":41469},{\"end\":41484,\"start\":41481},{\"end\":41910,\"start\":41902},{\"end\":41919,\"start\":41915},{\"end\":41932,\"start\":41926},{\"end\":41945,\"start\":41938},{\"end\":41955,\"start\":41950},{\"end\":41968,\"start\":41961},{\"end\":41981,\"start\":41975},{\"end\":41993,\"start\":41987},{\"end\":42336,\"start\":42332},{\"end\":42350,\"start\":42341},{\"end\":42360,\"start\":42355},{\"end\":42378,\"start\":42370},{\"end\":42780,\"start\":42776},{\"end\":42789,\"start\":42785},{\"end\":42808,\"start\":42802},{\"end\":42823,\"start\":42817},{\"end\":42838,\"start\":42833},{\"end\":42858,\"start\":42850},{\"end\":43169,\"start\":43161},{\"end\":43180,\"start\":43174},{\"end\":43195,\"start\":43188},{\"end\":43210,\"start\":43203},{\"end\":43219,\"start\":43217},{\"end\":43727,\"start\":43722},{\"end\":43739,\"start\":43732},{\"end\":43747,\"start\":43744},{\"end\":43762,\"start\":43758},{\"end\":44129,\"start\":44121},{\"end\":44148,\"start\":44145},{\"end\":44166,\"start\":44161},{\"end\":44168,\"start\":44167},{\"end\":44192,\"start\":44187},{\"end\":44209,\"start\":44203},{\"end\":44231,\"start\":44217},{\"end\":44685,\"start\":44681},{\"end\":44700,\"start\":44695},{\"end\":44714,\"start\":44710},{\"end\":44725,\"start\":44721},{\"end\":45077,\"start\":45073},{\"end\":45091,\"start\":45087},{\"end\":45434,\"start\":45426},{\"end\":45450,\"start\":45445},{\"end\":45764,\"start\":45756},{\"end\":45779,\"start\":45770},{\"end\":45791,\"start\":45784},{\"end\":45810,\"start\":45804},{\"end\":45822,\"start\":45815},{\"end\":45832,\"start\":45827},{\"end\":45841,\"start\":45838},{\"end\":45855,\"start\":45846},{\"end\":46281,\"start\":46274},{\"end\":46294,\"start\":46290},{\"end\":46305,\"start\":46302},{\"end\":46321,\"start\":46316},{\"end\":46337,\"start\":46332},{\"end\":46686,\"start\":46680},{\"end\":46703,\"start\":46696},{\"end\":46714,\"start\":46711},{\"end\":46734,\"start\":46719},{\"end\":47043,\"start\":47036},{\"end\":47058,\"start\":47053},{\"end\":47072,\"start\":47066},{\"end\":47085,\"start\":47081},{\"end\":47101,\"start\":47095},{\"end\":47647,\"start\":47640},{\"end\":47662,\"start\":47657},{\"end\":47676,\"start\":47670},{\"end\":47689,\"start\":47685},{\"end\":47705,\"start\":47699},{\"end\":48016,\"start\":48011},{\"end\":48029,\"start\":48025},{\"end\":48039,\"start\":48036},{\"end\":48454,\"start\":48446},{\"end\":48473,\"start\":48466},{\"end\":48720,\"start\":48716},{\"end\":48735,\"start\":48731},{\"end\":48748,\"start\":48743},{\"end\":48760,\"start\":48757},{\"end\":49220,\"start\":49213},{\"end\":49232,\"start\":49231},{\"end\":49234,\"start\":49233},{\"end\":49252,\"start\":49243},{\"end\":49254,\"start\":49253},{\"end\":49268,\"start\":49263},{\"end\":49270,\"start\":49269},{\"end\":49286,\"start\":49280},{\"end\":49724,\"start\":49716},{\"end\":49740,\"start\":49732},{\"end\":49755,\"start\":49749},{\"end\":49770,\"start\":49765},{\"end\":49772,\"start\":49771},{\"end\":50075,\"start\":50069},{\"end\":50498,\"start\":50497},{\"end\":50511,\"start\":50505},{\"end\":50522,\"start\":50518},{\"end\":50527,\"start\":50523},{\"end\":50769,\"start\":50757},{\"end\":50778,\"start\":50775},{\"end\":50789,\"start\":50783},{\"end\":50800,\"start\":50795},{\"end\":51099,\"start\":51098},{\"end\":51111,\"start\":51108},{\"end\":51124,\"start\":51118},{\"end\":51139,\"start\":51138},{\"end\":51141,\"start\":51140},{\"end\":51159,\"start\":51148},{\"end\":51568,\"start\":51562},{\"end\":51592,\"start\":51583},{\"end\":51606,\"start\":51602},{\"end\":51946,\"start\":51943},{\"end\":51960,\"start\":51953},{\"end\":51971,\"start\":51969},{\"end\":51984,\"start\":51977},{\"end\":52228,\"start\":52220},{\"end\":52240,\"start\":52235},{\"end\":52255,\"start\":52250},{\"end\":52270,\"start\":52264},{\"end\":52289,\"start\":52280},{\"end\":52306,\"start\":52300},{\"end\":52648,\"start\":52642},{\"end\":52659,\"start\":52653},{\"end\":52674,\"start\":52665},{\"end\":52685,\"start\":52679},{\"end\":52972,\"start\":52968},{\"end\":52990,\"start\":52979},{\"end\":53004,\"start\":52997},{\"end\":53017,\"start\":53010},{\"end\":53027,\"start\":53024},{\"end\":53259,\"start\":53253},{\"end\":53270,\"start\":53264},{\"end\":53281,\"start\":53277},{\"end\":53299,\"start\":53293},{\"end\":53719,\"start\":53714},{\"end\":53733,\"start\":53727},{\"end\":53745,\"start\":53739},{\"end\":53999,\"start\":53996},{\"end\":54011,\"start\":54007},{\"end\":54025,\"start\":54017},{\"end\":54037,\"start\":54030},{\"end\":54051,\"start\":54044},{\"end\":54062,\"start\":54056},{\"end\":54406,\"start\":54403},{\"end\":54418,\"start\":54414},{\"end\":54430,\"start\":54424},{\"end\":54439,\"start\":54435},{\"end\":54452,\"start\":54446},{\"end\":54465,\"start\":54459},{\"end\":54806,\"start\":54803},{\"end\":54820,\"start\":54814},{\"end\":54831,\"start\":54825},{\"end\":54845,\"start\":54838},{\"end\":54858,\"start\":54852},{\"end\":54871,\"start\":54865},{\"end\":54880,\"start\":54876},{\"end\":55222,\"start\":55215},{\"end\":55235,\"start\":55230},{\"end\":55248,\"start\":55243},{\"end\":55263,\"start\":55257},{\"end\":55275,\"start\":55271},{\"end\":55277,\"start\":55276},{\"end\":55726,\"start\":55719},{\"end\":55739,\"start\":55733},{\"end\":55765,\"start\":55761},{\"end\":55986,\"start\":55983},{\"end\":55997,\"start\":55993},{\"end\":56011,\"start\":56002},{\"end\":56023,\"start\":56017},{\"end\":56035,\"start\":56030},{\"end\":56048,\"start\":56043}]", "bib_author_last_name": "[{\"end\":33650,\"start\":33626},{\"end\":33660,\"start\":33652},{\"end\":33967,\"start\":33964},{\"end\":33981,\"start\":33975},{\"end\":34358,\"start\":34353},{\"end\":34372,\"start\":34369},{\"end\":34381,\"start\":34376},{\"end\":34395,\"start\":34388},{\"end\":34403,\"start\":34397},{\"end\":34613,\"start\":34610},{\"end\":34628,\"start\":34623},{\"end\":34640,\"start\":34635},{\"end\":34653,\"start\":34650},{\"end\":34669,\"start\":34665},{\"end\":34681,\"start\":34676},{\"end\":34694,\"start\":34690},{\"end\":35025,\"start\":35018},{\"end\":35040,\"start\":35033},{\"end\":35056,\"start\":35048},{\"end\":35075,\"start\":35065},{\"end\":35598,\"start\":35592},{\"end\":35617,\"start\":35608},{\"end\":35636,\"start\":35626},{\"end\":35652,\"start\":35645},{\"end\":35670,\"start\":35665},{\"end\":36249,\"start\":36245},{\"end\":36272,\"start\":36266},{\"end\":36289,\"start\":36282},{\"end\":36554,\"start\":36550},{\"end\":36563,\"start\":36560},{\"end\":36579,\"start\":36575},{\"end\":36592,\"start\":36589},{\"end\":36604,\"start\":36601},{\"end\":36617,\"start\":36612},{\"end\":37038,\"start\":37032},{\"end\":37054,\"start\":37046},{\"end\":37073,\"start\":37065},{\"end\":37433,\"start\":37428},{\"end\":37447,\"start\":37442},{\"end\":37457,\"start\":37454},{\"end\":37477,\"start\":37466},{\"end\":37813,\"start\":37811},{\"end\":37825,\"start\":37822},{\"end\":37836,\"start\":37834},{\"end\":37851,\"start\":37838},{\"end\":38063,\"start\":38056},{\"end\":38081,\"start\":38074},{\"end\":38097,\"start\":38090},{\"end\":38114,\"start\":38105},{\"end\":38542,\"start\":38535},{\"end\":38559,\"start\":38550},{\"end\":38800,\"start\":38795},{\"end\":38818,\"start\":38807},{\"end\":38830,\"start\":38825},{\"end\":38847,\"start\":38840},{\"end\":39260,\"start\":39255},{\"end\":39274,\"start\":39271},{\"end\":39286,\"start\":39282},{\"end\":39299,\"start\":39295},{\"end\":39315,\"start\":39312},{\"end\":39325,\"start\":39322},{\"end\":39337,\"start\":39332},{\"end\":39866,\"start\":39861},{\"end\":39879,\"start\":39877},{\"end\":39892,\"start\":39889},{\"end\":39905,\"start\":39901},{\"end\":39919,\"start\":39913},{\"end\":39935,\"start\":39932},{\"end\":40229,\"start\":40226},{\"end\":40249,\"start\":40236},{\"end\":40267,\"start\":40257},{\"end\":40279,\"start\":40275},{\"end\":40672,\"start\":40667},{\"end\":40687,\"start\":40680},{\"end\":40699,\"start\":40695},{\"end\":40715,\"start\":40709},{\"end\":40999,\"start\":40983},{\"end\":41016,\"start\":41008},{\"end\":41031,\"start\":41024},{\"end\":41044,\"start\":41039},{\"end\":41063,\"start\":41058},{\"end\":41479,\"start\":41476},{\"end\":41490,\"start\":41485},{\"end\":41913,\"start\":41911},{\"end\":41924,\"start\":41920},{\"end\":41936,\"start\":41933},{\"end\":41948,\"start\":41946},{\"end\":41959,\"start\":41956},{\"end\":41973,\"start\":41969},{\"end\":41985,\"start\":41982},{\"end\":41999,\"start\":41994},{\"end\":42339,\"start\":42337},{\"end\":42353,\"start\":42351},{\"end\":42368,\"start\":42361},{\"end\":42384,\"start\":42379},{\"end\":42783,\"start\":42781},{\"end\":42800,\"start\":42790},{\"end\":42815,\"start\":42809},{\"end\":42831,\"start\":42824},{\"end\":42848,\"start\":42839},{\"end\":42864,\"start\":42859},{\"end\":43172,\"start\":43170},{\"end\":43186,\"start\":43181},{\"end\":43201,\"start\":43196},{\"end\":43215,\"start\":43211},{\"end\":43223,\"start\":43220},{\"end\":43730,\"start\":43728},{\"end\":43742,\"start\":43740},{\"end\":43756,\"start\":43748},{\"end\":43770,\"start\":43763},{\"end\":44143,\"start\":44130},{\"end\":44152,\"start\":44149},{\"end\":44159,\"start\":44154},{\"end\":44185,\"start\":44169},{\"end\":44201,\"start\":44193},{\"end\":44215,\"start\":44210},{\"end\":44245,\"start\":44232},{\"end\":44254,\"start\":44247},{\"end\":44693,\"start\":44686},{\"end\":44708,\"start\":44701},{\"end\":44719,\"start\":44715},{\"end\":44730,\"start\":44726},{\"end\":45085,\"start\":45078},{\"end\":45096,\"start\":45092},{\"end\":45443,\"start\":45435},{\"end\":45459,\"start\":45451},{\"end\":45768,\"start\":45765},{\"end\":45782,\"start\":45780},{\"end\":45802,\"start\":45792},{\"end\":45813,\"start\":45811},{\"end\":45825,\"start\":45823},{\"end\":45836,\"start\":45833},{\"end\":45844,\"start\":45842},{\"end\":45858,\"start\":45856},{\"end\":46288,\"start\":46282},{\"end\":46300,\"start\":46295},{\"end\":46314,\"start\":46306},{\"end\":46330,\"start\":46322},{\"end\":46344,\"start\":46338},{\"end\":46694,\"start\":46687},{\"end\":46709,\"start\":46704},{\"end\":46717,\"start\":46715},{\"end\":46741,\"start\":46735},{\"end\":47051,\"start\":47044},{\"end\":47064,\"start\":47059},{\"end\":47079,\"start\":47073},{\"end\":47093,\"start\":47086},{\"end\":47116,\"start\":47102},{\"end\":47655,\"start\":47648},{\"end\":47668,\"start\":47663},{\"end\":47683,\"start\":47677},{\"end\":47697,\"start\":47690},{\"end\":47720,\"start\":47706},{\"end\":48023,\"start\":48017},{\"end\":48034,\"start\":48030},{\"end\":48047,\"start\":48040},{\"end\":48464,\"start\":48455},{\"end\":48481,\"start\":48474},{\"end\":48729,\"start\":48721},{\"end\":48741,\"start\":48736},{\"end\":48755,\"start\":48749},{\"end\":48768,\"start\":48761},{\"end\":49229,\"start\":49221},{\"end\":49241,\"start\":49235},{\"end\":49261,\"start\":49255},{\"end\":49278,\"start\":49271},{\"end\":49294,\"start\":49287},{\"end\":49305,\"start\":49296},{\"end\":49730,\"start\":49725},{\"end\":49747,\"start\":49741},{\"end\":49763,\"start\":49756},{\"end\":49782,\"start\":49773},{\"end\":50082,\"start\":50076},{\"end\":50503,\"start\":50499},{\"end\":50516,\"start\":50512},{\"end\":50532,\"start\":50528},{\"end\":50540,\"start\":50534},{\"end\":50773,\"start\":50770},{\"end\":50781,\"start\":50779},{\"end\":50793,\"start\":50790},{\"end\":50805,\"start\":50801},{\"end\":51106,\"start\":51100},{\"end\":51116,\"start\":51112},{\"end\":51128,\"start\":51125},{\"end\":51136,\"start\":51130},{\"end\":51146,\"start\":51142},{\"end\":51166,\"start\":51160},{\"end\":51172,\"start\":51168},{\"end\":51581,\"start\":51569},{\"end\":51600,\"start\":51593},{\"end\":51617,\"start\":51607},{\"end\":51635,\"start\":51619},{\"end\":51951,\"start\":51947},{\"end\":51967,\"start\":51961},{\"end\":51975,\"start\":51972},{\"end\":51987,\"start\":51985},{\"end\":52233,\"start\":52229},{\"end\":52248,\"start\":52241},{\"end\":52262,\"start\":52256},{\"end\":52278,\"start\":52271},{\"end\":52298,\"start\":52290},{\"end\":52313,\"start\":52307},{\"end\":52651,\"start\":52649},{\"end\":52663,\"start\":52660},{\"end\":52677,\"start\":52675},{\"end\":52691,\"start\":52686},{\"end\":52977,\"start\":52973},{\"end\":52995,\"start\":52991},{\"end\":53008,\"start\":53005},{\"end\":53022,\"start\":53018},{\"end\":53032,\"start\":53028},{\"end\":53262,\"start\":53260},{\"end\":53275,\"start\":53271},{\"end\":53291,\"start\":53282},{\"end\":53307,\"start\":53300},{\"end\":53725,\"start\":53720},{\"end\":53737,\"start\":53734},{\"end\":53753,\"start\":53746},{\"end\":54005,\"start\":54000},{\"end\":54015,\"start\":54012},{\"end\":54028,\"start\":54026},{\"end\":54042,\"start\":54038},{\"end\":54054,\"start\":54052},{\"end\":54065,\"start\":54063},{\"end\":54412,\"start\":54407},{\"end\":54422,\"start\":54419},{\"end\":54433,\"start\":54431},{\"end\":54444,\"start\":54440},{\"end\":54457,\"start\":54453},{\"end\":54469,\"start\":54466},{\"end\":54478,\"start\":54471},{\"end\":54812,\"start\":54807},{\"end\":54823,\"start\":54821},{\"end\":54836,\"start\":54832},{\"end\":54850,\"start\":54846},{\"end\":54863,\"start\":54859},{\"end\":54874,\"start\":54872},{\"end\":54884,\"start\":54881},{\"end\":55228,\"start\":55223},{\"end\":55241,\"start\":55236},{\"end\":55255,\"start\":55249},{\"end\":55269,\"start\":55264},{\"end\":55285,\"start\":55278},{\"end\":55731,\"start\":55727},{\"end\":55759,\"start\":55740},{\"end\":55775,\"start\":55766},{\"end\":55787,\"start\":55777},{\"end\":55805,\"start\":55789},{\"end\":55991,\"start\":55987},{\"end\":56000,\"start\":55998},{\"end\":56015,\"start\":56012},{\"end\":56028,\"start\":56024},{\"end\":56041,\"start\":56036},{\"end\":56056,\"start\":56049}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1912.00818\",\"id\":\"b0\"},\"end\":33956,\"start\":33620},{\"attributes\":{\"doi\":\"arXiv:2010.13514\",\"id\":\"b1\"},\"end\":34279,\"start\":33958},{\"attributes\":{\"doi\":\"arXiv:1708.05344\",\"id\":\"b2\"},\"end\":34602,\"start\":34281},{\"attributes\":{\"doi\":\"arXiv:1909.12535\",\"id\":\"b3\"},\"end\":34940,\"start\":34604},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":231924497},\"end\":35485,\"start\":34942},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":216360751},\"end\":36194,\"start\":35487},{\"attributes\":{\"doi\":\"arXiv:2003.13461\",\"id\":\"b6\"},\"end\":36460,\"start\":36196},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":221086425},\"end\":36920,\"start\":36462},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":227276412},\"end\":37361,\"start\":36922},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219530697},\"end\":37801,\"start\":37363},{\"attributes\":{\"doi\":\"arXiv:1609.09106\",\"id\":\"b10\"},\"end\":37975,\"start\":37803},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":222141681},\"end\":38467,\"start\":37977},{\"attributes\":{\"doi\":\"arXiv:2002.05516\",\"id\":\"b12\"},\"end\":38726,\"start\":38469},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":203610177},\"end\":39187,\"start\":38728},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":227311284},\"end\":39731,\"start\":39189},{\"attributes\":{\"doi\":\"arXiv:1811.11479\",\"id\":\"b15\"},\"end\":40221,\"start\":39733},{\"attributes\":{\"id\":\"b16\"},\"end\":40580,\"start\":40223},{\"attributes\":{\"doi\":\"arXiv:1909.12488\",\"id\":\"b17\"},\"end\":40928,\"start\":40582},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":102351038},\"end\":41467,\"start\":40930},{\"attributes\":{\"doi\":\"arXiv:2110.10302\",\"id\":\"b19\"},\"end\":41801,\"start\":41469},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14013269},\"end\":42263,\"start\":41803},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235446706},\"end\":42724,\"start\":42265},{\"attributes\":{\"doi\":\"arXiv:1812.06127\",\"id\":\"b22\"},\"end\":43082,\"start\":42726},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":231924480},\"end\":43673,\"start\":43084},{\"attributes\":{\"id\":\"b24\"},\"end\":44119,\"start\":43675},{\"attributes\":{\"doi\":\"arXiv:2001.01523\",\"id\":\"b25\"},\"end\":44646,\"start\":44121},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220266143},\"end\":45023,\"start\":44648},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":201070803},\"end\":45424,\"start\":45025},{\"attributes\":{\"doi\":\"arXiv:1802.09419\",\"id\":\"b28\"},\"end\":45695,\"start\":45426},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":218684876},\"end\":46168,\"start\":45697},{\"attributes\":{\"doi\":\"arXiv:1903.03088\",\"id\":\"b30\"},\"end\":46600,\"start\":46170},{\"attributes\":{\"doi\":\"arXiv:2002.10619\",\"id\":\"b31\"},\"end\":46960,\"start\":46602},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14955348},\"end\":47564,\"start\":46962},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b33\",\"matched_paper_id\":14955348},\"end\":47939,\"start\":47566},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":229339795},\"end\":48442,\"start\":47941},{\"attributes\":{\"doi\":\"arXiv:1909.02803\",\"id\":\"b35\"},\"end\":48661,\"start\":48444},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":232147378},\"end\":49143,\"start\":48663},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":219720931},\"end\":49683,\"start\":49145},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3586416},\"end\":50011,\"start\":49685},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":45125428},\"end\":50374,\"start\":50013},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":219708331},\"end\":50755,\"start\":50376},{\"attributes\":{\"doi\":\"arXiv:2103.00710\",\"id\":\"b41\"},\"end\":51013,\"start\":50757},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":86439367},\"end\":51560,\"start\":51015},{\"attributes\":{\"doi\":\"arXiv:1906.00695\",\"id\":\"b43\"},\"end\":51866,\"start\":51562},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":220903131},\"end\":52167,\"start\":51868},{\"attributes\":{\"doi\":\"arXiv:1910.10252\",\"id\":\"b45\"},\"end\":52537,\"start\":52169},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":53946001},\"end\":52915,\"start\":52539},{\"attributes\":{\"doi\":\"arXiv:2011.11266\",\"id\":\"b47\"},\"end\":53227,\"start\":52917},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":30834643},\"end\":53660,\"start\":53229},{\"attributes\":{\"doi\":\"arXiv:1810.05749\",\"id\":\"b49\"},\"end\":53924,\"start\":53662},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":242757770},\"end\":54331,\"start\":53926},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":237858014},\"end\":54714,\"start\":54333},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":247502001},\"end\":55144,\"start\":54716},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":229188065},\"end\":55684,\"start\":55146},{\"attributes\":{\"id\":\"b54\"},\"end\":55943,\"start\":55686},{\"attributes\":{\"doi\":\"arXiv:1806.00582\",\"id\":\"b55\"},\"end\":56249,\"start\":55945}]", "bib_title": "[{\"end\":35011,\"start\":34942},{\"end\":35584,\"start\":35487},{\"end\":36541,\"start\":36462},{\"end\":37022,\"start\":36922},{\"end\":37418,\"start\":37363},{\"end\":38048,\"start\":37977},{\"end\":38787,\"start\":38728},{\"end\":39247,\"start\":39189},{\"end\":40974,\"start\":40930},{\"end\":41900,\"start\":41803},{\"end\":42330,\"start\":42265},{\"end\":43159,\"start\":43084},{\"end\":43720,\"start\":43675},{\"end\":44679,\"start\":44648},{\"end\":45071,\"start\":45025},{\"end\":45754,\"start\":45697},{\"end\":47034,\"start\":46962},{\"end\":47638,\"start\":47566},{\"end\":48009,\"start\":47941},{\"end\":48714,\"start\":48663},{\"end\":49211,\"start\":49145},{\"end\":49714,\"start\":49685},{\"end\":50067,\"start\":50013},{\"end\":50495,\"start\":50376},{\"end\":51096,\"start\":51015},{\"end\":51941,\"start\":51868},{\"end\":52640,\"start\":52539},{\"end\":53251,\"start\":53229},{\"end\":53994,\"start\":53926},{\"end\":54401,\"start\":54333},{\"end\":54801,\"start\":54716},{\"end\":55213,\"start\":55146}]", "bib_author": "[{\"end\":33652,\"start\":33620},{\"end\":33662,\"start\":33652},{\"end\":33969,\"start\":33958},{\"end\":33983,\"start\":33969},{\"end\":34360,\"start\":34346},{\"end\":34374,\"start\":34360},{\"end\":34383,\"start\":34374},{\"end\":34397,\"start\":34383},{\"end\":34405,\"start\":34397},{\"end\":34615,\"start\":34606},{\"end\":34630,\"start\":34615},{\"end\":34642,\"start\":34630},{\"end\":34655,\"start\":34642},{\"end\":34671,\"start\":34655},{\"end\":34683,\"start\":34671},{\"end\":34696,\"start\":34683},{\"end\":35027,\"start\":35013},{\"end\":35042,\"start\":35027},{\"end\":35058,\"start\":35042},{\"end\":35077,\"start\":35058},{\"end\":35600,\"start\":35586},{\"end\":35619,\"start\":35600},{\"end\":35638,\"start\":35619},{\"end\":35654,\"start\":35638},{\"end\":35672,\"start\":35654},{\"end\":36251,\"start\":36238},{\"end\":36274,\"start\":36251},{\"end\":36291,\"start\":36274},{\"end\":36556,\"start\":36543},{\"end\":36565,\"start\":36556},{\"end\":36581,\"start\":36565},{\"end\":36594,\"start\":36581},{\"end\":36606,\"start\":36594},{\"end\":36619,\"start\":36606},{\"end\":37040,\"start\":37024},{\"end\":37056,\"start\":37040},{\"end\":37075,\"start\":37056},{\"end\":37435,\"start\":37420},{\"end\":37449,\"start\":37435},{\"end\":37459,\"start\":37449},{\"end\":37479,\"start\":37459},{\"end\":37815,\"start\":37805},{\"end\":37827,\"start\":37815},{\"end\":37838,\"start\":37827},{\"end\":37853,\"start\":37838},{\"end\":38065,\"start\":38050},{\"end\":38083,\"start\":38065},{\"end\":38099,\"start\":38083},{\"end\":38116,\"start\":38099},{\"end\":38544,\"start\":38529},{\"end\":38561,\"start\":38544},{\"end\":38802,\"start\":38789},{\"end\":38820,\"start\":38802},{\"end\":38832,\"start\":38820},{\"end\":38849,\"start\":38832},{\"end\":39262,\"start\":39249},{\"end\":39276,\"start\":39262},{\"end\":39288,\"start\":39276},{\"end\":39301,\"start\":39288},{\"end\":39317,\"start\":39301},{\"end\":39327,\"start\":39317},{\"end\":39339,\"start\":39327},{\"end\":39868,\"start\":39852},{\"end\":39881,\"start\":39868},{\"end\":39894,\"start\":39881},{\"end\":39907,\"start\":39894},{\"end\":39921,\"start\":39907},{\"end\":39937,\"start\":39921},{\"end\":40231,\"start\":40223},{\"end\":40251,\"start\":40231},{\"end\":40269,\"start\":40251},{\"end\":40281,\"start\":40269},{\"end\":40674,\"start\":40661},{\"end\":40689,\"start\":40674},{\"end\":40701,\"start\":40689},{\"end\":40717,\"start\":40701},{\"end\":41001,\"start\":40976},{\"end\":41018,\"start\":41001},{\"end\":41033,\"start\":41018},{\"end\":41046,\"start\":41033},{\"end\":41065,\"start\":41046},{\"end\":41481,\"start\":41469},{\"end\":41492,\"start\":41481},{\"end\":41915,\"start\":41902},{\"end\":41926,\"start\":41915},{\"end\":41938,\"start\":41926},{\"end\":41950,\"start\":41938},{\"end\":41961,\"start\":41950},{\"end\":41975,\"start\":41961},{\"end\":41987,\"start\":41975},{\"end\":42001,\"start\":41987},{\"end\":42341,\"start\":42332},{\"end\":42355,\"start\":42341},{\"end\":42370,\"start\":42355},{\"end\":42386,\"start\":42370},{\"end\":42785,\"start\":42776},{\"end\":42802,\"start\":42785},{\"end\":42817,\"start\":42802},{\"end\":42833,\"start\":42817},{\"end\":42850,\"start\":42833},{\"end\":42866,\"start\":42850},{\"end\":43174,\"start\":43161},{\"end\":43188,\"start\":43174},{\"end\":43203,\"start\":43188},{\"end\":43217,\"start\":43203},{\"end\":43225,\"start\":43217},{\"end\":43732,\"start\":43722},{\"end\":43744,\"start\":43732},{\"end\":43758,\"start\":43744},{\"end\":43772,\"start\":43758},{\"end\":44145,\"start\":44121},{\"end\":44154,\"start\":44145},{\"end\":44161,\"start\":44154},{\"end\":44187,\"start\":44161},{\"end\":44203,\"start\":44187},{\"end\":44217,\"start\":44203},{\"end\":44247,\"start\":44217},{\"end\":44256,\"start\":44247},{\"end\":44695,\"start\":44681},{\"end\":44710,\"start\":44695},{\"end\":44721,\"start\":44710},{\"end\":44732,\"start\":44721},{\"end\":45087,\"start\":45073},{\"end\":45098,\"start\":45087},{\"end\":45445,\"start\":45426},{\"end\":45461,\"start\":45445},{\"end\":45770,\"start\":45756},{\"end\":45784,\"start\":45770},{\"end\":45804,\"start\":45784},{\"end\":45815,\"start\":45804},{\"end\":45827,\"start\":45815},{\"end\":45838,\"start\":45827},{\"end\":45846,\"start\":45838},{\"end\":45860,\"start\":45846},{\"end\":46290,\"start\":46274},{\"end\":46302,\"start\":46290},{\"end\":46316,\"start\":46302},{\"end\":46332,\"start\":46316},{\"end\":46346,\"start\":46332},{\"end\":46696,\"start\":46680},{\"end\":46711,\"start\":46696},{\"end\":46719,\"start\":46711},{\"end\":46743,\"start\":46719},{\"end\":47053,\"start\":47036},{\"end\":47066,\"start\":47053},{\"end\":47081,\"start\":47066},{\"end\":47095,\"start\":47081},{\"end\":47118,\"start\":47095},{\"end\":47657,\"start\":47640},{\"end\":47670,\"start\":47657},{\"end\":47685,\"start\":47670},{\"end\":47699,\"start\":47685},{\"end\":47722,\"start\":47699},{\"end\":48025,\"start\":48011},{\"end\":48036,\"start\":48025},{\"end\":48049,\"start\":48036},{\"end\":48466,\"start\":48446},{\"end\":48483,\"start\":48466},{\"end\":48731,\"start\":48716},{\"end\":48743,\"start\":48731},{\"end\":48757,\"start\":48743},{\"end\":48770,\"start\":48757},{\"end\":49231,\"start\":49213},{\"end\":49243,\"start\":49231},{\"end\":49263,\"start\":49243},{\"end\":49280,\"start\":49263},{\"end\":49296,\"start\":49280},{\"end\":49307,\"start\":49296},{\"end\":49732,\"start\":49716},{\"end\":49749,\"start\":49732},{\"end\":49765,\"start\":49749},{\"end\":49784,\"start\":49765},{\"end\":50084,\"start\":50069},{\"end\":50505,\"start\":50497},{\"end\":50518,\"start\":50505},{\"end\":50534,\"start\":50518},{\"end\":50542,\"start\":50534},{\"end\":50775,\"start\":50757},{\"end\":50783,\"start\":50775},{\"end\":50795,\"start\":50783},{\"end\":50807,\"start\":50795},{\"end\":51108,\"start\":51098},{\"end\":51118,\"start\":51108},{\"end\":51130,\"start\":51118},{\"end\":51138,\"start\":51130},{\"end\":51148,\"start\":51138},{\"end\":51168,\"start\":51148},{\"end\":51174,\"start\":51168},{\"end\":51583,\"start\":51562},{\"end\":51602,\"start\":51583},{\"end\":51619,\"start\":51602},{\"end\":51637,\"start\":51619},{\"end\":51953,\"start\":51943},{\"end\":51969,\"start\":51953},{\"end\":51977,\"start\":51969},{\"end\":51989,\"start\":51977},{\"end\":52235,\"start\":52220},{\"end\":52250,\"start\":52235},{\"end\":52264,\"start\":52250},{\"end\":52280,\"start\":52264},{\"end\":52300,\"start\":52280},{\"end\":52315,\"start\":52300},{\"end\":52653,\"start\":52642},{\"end\":52665,\"start\":52653},{\"end\":52679,\"start\":52665},{\"end\":52693,\"start\":52679},{\"end\":52979,\"start\":52968},{\"end\":52997,\"start\":52979},{\"end\":53010,\"start\":52997},{\"end\":53024,\"start\":53010},{\"end\":53034,\"start\":53024},{\"end\":53264,\"start\":53253},{\"end\":53277,\"start\":53264},{\"end\":53293,\"start\":53277},{\"end\":53309,\"start\":53293},{\"end\":53727,\"start\":53714},{\"end\":53739,\"start\":53727},{\"end\":53755,\"start\":53739},{\"end\":54007,\"start\":53996},{\"end\":54017,\"start\":54007},{\"end\":54030,\"start\":54017},{\"end\":54044,\"start\":54030},{\"end\":54056,\"start\":54044},{\"end\":54067,\"start\":54056},{\"end\":54414,\"start\":54403},{\"end\":54424,\"start\":54414},{\"end\":54435,\"start\":54424},{\"end\":54446,\"start\":54435},{\"end\":54459,\"start\":54446},{\"end\":54471,\"start\":54459},{\"end\":54480,\"start\":54471},{\"end\":54814,\"start\":54803},{\"end\":54825,\"start\":54814},{\"end\":54838,\"start\":54825},{\"end\":54852,\"start\":54838},{\"end\":54865,\"start\":54852},{\"end\":54876,\"start\":54865},{\"end\":54886,\"start\":54876},{\"end\":55230,\"start\":55215},{\"end\":55243,\"start\":55230},{\"end\":55257,\"start\":55243},{\"end\":55271,\"start\":55257},{\"end\":55287,\"start\":55271},{\"end\":55733,\"start\":55719},{\"end\":55761,\"start\":55733},{\"end\":55777,\"start\":55761},{\"end\":55789,\"start\":55777},{\"end\":55807,\"start\":55789},{\"end\":55993,\"start\":55983},{\"end\":56002,\"start\":55993},{\"end\":56017,\"start\":56002},{\"end\":56030,\"start\":56017},{\"end\":56043,\"start\":56030},{\"end\":56058,\"start\":56043}]", "bib_venue": "[{\"end\":33766,\"start\":33678},{\"end\":34096,\"start\":33999},{\"end\":34344,\"start\":34281},{\"end\":35151,\"start\":35077},{\"end\":35773,\"start\":35672},{\"end\":36236,\"start\":36196},{\"end\":36672,\"start\":36619},{\"end\":37124,\"start\":37075},{\"end\":37543,\"start\":37479},{\"end\":38180,\"start\":38116},{\"end\":38527,\"start\":38469},{\"end\":38914,\"start\":38849},{\"end\":39406,\"start\":39339},{\"end\":39850,\"start\":39733},{\"end\":40345,\"start\":40281},{\"end\":40659,\"start\":40582},{\"end\":41141,\"start\":41065},{\"end\":41613,\"start\":41508},{\"end\":42019,\"start\":42001},{\"end\":42451,\"start\":42386},{\"end\":42774,\"start\":42726},{\"end\":43322,\"start\":43225},{\"end\":43849,\"start\":43772},{\"end\":44357,\"start\":44272},{\"end\":44796,\"start\":44732},{\"end\":45175,\"start\":45098},{\"end\":45537,\"start\":45477},{\"end\":45913,\"start\":45860},{\"end\":46272,\"start\":46170},{\"end\":46678,\"start\":46602},{\"end\":47208,\"start\":47118},{\"end\":47733,\"start\":47726},{\"end\":48136,\"start\":48049},{\"end\":48844,\"start\":48770},{\"end\":49371,\"start\":49307},{\"end\":49833,\"start\":49784},{\"end\":50148,\"start\":50084},{\"end\":50549,\"start\":50542},{\"end\":50862,\"start\":50823},{\"end\":51240,\"start\":51174},{\"end\":51690,\"start\":51653},{\"end\":51996,\"start\":51989},{\"end\":52218,\"start\":52169},{\"end\":52707,\"start\":52693},{\"end\":52966,\"start\":52917},{\"end\":53392,\"start\":53309},{\"end\":53712,\"start\":53662},{\"end\":54116,\"start\":54067},{\"end\":54510,\"start\":54480},{\"end\":54914,\"start\":54886},{\"end\":55362,\"start\":55287},{\"end\":55717,\"start\":55686},{\"end\":55981,\"start\":55945},{\"end\":35212,\"start\":35153},{\"end\":35861,\"start\":35775},{\"end\":37594,\"start\":37545},{\"end\":38231,\"start\":38182},{\"end\":38966,\"start\":38916},{\"end\":39460,\"start\":39408},{\"end\":40396,\"start\":40347},{\"end\":41204,\"start\":41143},{\"end\":42503,\"start\":42453},{\"end\":43406,\"start\":43324},{\"end\":43913,\"start\":43851},{\"end\":44847,\"start\":44798},{\"end\":45239,\"start\":45177},{\"end\":47285,\"start\":47210},{\"end\":48210,\"start\":48138},{\"end\":48905,\"start\":48846},{\"end\":49429,\"start\":49373},{\"end\":50199,\"start\":50150},{\"end\":51293,\"start\":51242},{\"end\":53462,\"start\":53394},{\"end\":55424,\"start\":55364}]"}}}, "year": 2023, "month": 12, "day": 17}