{"id": 222141081, "updated": "2023-10-06 11:21:46.924", "metadata": {"title": "Unfolding the Alternating Optimization for Blind Super Resolution", "authors": "[{\"first\":\"Zhengxiong\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Shang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tieniu\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 10, "day": 6}, "abstract": "Previous methods decompose blind super resolution (SR) problem into two sequential steps: \\textit{i}) estimating blur kernel from given low-resolution (LR) image and \\textit{ii}) restoring SR image based on estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. Small estimation error of the first step could cause severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from LR image, which makes it difficult to predict highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate blur kernel and restore SR image in a single model. Specifically, we design two convolutional neural modules, namely \\textit{Restorer} and \\textit{Estimator}. \\textit{Restorer} restores SR image based on predicted kernel, and \\textit{Estimator} estimates blur kernel with the help of restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, \\textit{Estimator} utilizes information from both LR and SR images, which makes the estimation of blur kernel easier. More importantly, \\textit{Restorer} is trained with the kernel estimated by \\textit{Estimator}, instead of ground-truth kernel, thus \\textit{Restorer} could be more tolerant to the estimation error of \\textit{Estimator}. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at much higher speed. The source code is available at https://github.com/greatlog/DAN.git.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.02631", "mag": "3098616015", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/Luo0LWT20", "doi": null}}, "content": {"source": {"pdf_hash": "c8230ea819dc5774b0fce1766b9a5c094bb1106c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.02631v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a4a3a821970cf9d5108504d901bc21be0380a205", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c8230ea819dc5774b0fce1766b9a5c094bb1106c.txt", "contents": "\nUnfolding the Alternating Optimization for Blind Super Resolution\n\n\nZhengxiong Luo \nCenter for Research on Intelligent Perception and Computing (CRIPAC) National Laboratory of Pattern Recognition (NLPR)\n\n\nInstitute of Automation\nChinese Academy of Sciences (CASIA\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences (UCAS)\n\n\nYan Huang \nCenter for Research on Intelligent Perception and Computing (CRIPAC) National Laboratory of Pattern Recognition (NLPR)\n\n\nInstitute of Automation\nChinese Academy of Sciences (CASIA\n\n\nShang Li \nInstitute of Automation\nChinese Academy of Sciences (CASIA\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences (UCAS)\n\n\nLiang Wang \nCenter for Research on Intelligent Perception and Computing (CRIPAC) National Laboratory of Pattern Recognition (NLPR)\n\n\nCenter for Excellence in Brain Science and Intelligence Technology (CEBSIT)\n\n\nChinese Academy of Sciences\nArtificial Intelligence Research (CAS-AIR)\n\n\nTieniu Tan \nCenter for Research on Intelligent Perception and Computing (CRIPAC) National Laboratory of Pattern Recognition (NLPR)\n\n\nCenter for Excellence in Brain Science and Intelligence Technology (CEBSIT)\n\n\nUnfolding the Alternating Optimization for Blind Super Resolution\n\nPrevious methods decompose blind super resolution (SR) problem into two sequential steps: i) estimating blur kernel from given low-resolution (LR) image and ii) restoring SR image based on estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. Small estimation error of the first step could cause severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from LR image, which makes it difficult to predict highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate blur kernel and restore SR image in a single model. Specifically, we design two convolutional neural modules, namely Restorer and Estimator. Restorer restores SR image based on predicted kernel, and Estimator estimates blur kernel with the help of restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, Estimator utilizes information from both LR and SR images, which makes the estimation of blur kernel easier. More importantly, Restorer is trained with the kernel estimated by Estimator, instead of ground-truth kernel, thus Restorer could be more tolerant to the estimation error of Estimator. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at much higher speed. The source code is available at https://github.com/greatlog/DAN.git.\n\nIntroduction\n\nSingle image super resolution (SISR) aims to recover the high-resolution (HR) version of a given degraded low-resolution (LR) image. It has wide applications in video enhancement, medical imaging, as well as security and surveillance imaging. Mathematically, the degradation process can be expressed as y = (x \u2297 k) \u2193 s +n (1) where x is the original HR image, y is the degraded LR image, \u2297 denotes the two-dimensional convolution of x with blur kernel k, n denotes Additive White Gaussian Noise (AWGN), and \u2193 s denotes the standard s-fold downsampler, which means keeping only the upper-left pixel for each distinct s \u00d7 s patch [35]. Then SISR refers to the process of recovering x from y. It is a highly ill-posed problem due to this inverse property, and thus has always been a challenging task.\n\nRecently, deep neural networks (DNNs) have achieved remarkable results on SISR. But most of these methods [39,2,40,23,8,21] assume that the blur kernel is predefined as the kernel of bicubic interpolation. In this way, large number of training samples can be manually synthesized and further used to train powerful DNNs. However, blur kernels in real applications are much more complicated, and there is a domain gap between bicubically synthesized training samples and real images. This domain gap will lead to severe performance drop when these networks are applied to real applications. Thus, more attention should be paid to SR in the context of unknown blur kernels, i.e. blind SR.\n\nIn blind SR, there is one more undetermined variable, i.e. blur kernel k, and the optimization also becomes much more difficult. To make this problem easier to be solved, previous methods [37,32,38,35] usually decompose it into two sequential steps: i) estimating blur kernel from LR image and ii) restoring SR image based on estimated kernel. This two-step solution involves two independently trained models, thus they may be not well compatible to each other. Small estimation error of the first step could cause severe performance drop of the following one [14]. But on the other hand, the first step can only utilize limited information from LR image, which makes it difficult to predict highly accurate blur kernel. As a result, although both models can perform well individually, the final result may be suboptimal when they are combined together.\n\nInstead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate blur kernel k and restore SR image x in the same model. Specifically, we design two convolutional neural modules, namely Restorer and Estimator. Restorer restores SR image based on blur kernel predicted by Estimator, and the restored SR image is further used to help Estimator estimate better blur kernel. Once the blur kernel is manually initialized, the two modules can well corporate with each other to form a closed loop, which can be iterated over and over. The iterating process is then unfolded to an end-to-end trainable network, which is called deep alternating network (DAN). In this way, Estimator can utilize information from both LR and SR images, which makes the estimation of blur kernel easier. More importantly, Restorer is trained with the kernel estimated by Estimator, instead of ground-truth kernel. Thus during testing Restorer could be more tolerant to the estimation error of Estimator. Besides, the results of both modules could be substantially improved during the iterations, thus it is likely for our alternating optimization algorithm to get better final results than the direct two-step solutions. We summarize our contributions into three points:\n\n1. We adopt an alternating optimization algorithm to estimate blur kernel and restore SR image for blind SR in a single network (DAN), which helps the two modules to be well compatible with each other and likely to get better final results than previous two-step solutions.\n\n2. We design two convolutional neural modules, which can be alternated repeatedly and then unfolded to form an end-to-end trainable network, without any pre/post-processing. It is easier to be trained and has higher speed than previous two-step solutions. To the best of our knowledge, the proposed method is the first end-to-end network for blind SR.\n\n3. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the art methods and produce more visually favorable results at much higher speed.  [39] and RRDB [30], the performance of these non-blind methods even start to saturate on common benchmark datasets. However, the blur kernels for real images are indeed much more complicated. In real applications, kernels are unknown and differ from image to image. As a result, despite that these methods have excellent performance in the context of bicubic downsampling, they still cannot be directly applied to real images due to the domain gap.  \n\n\nSuper Resolution for Multiple Degradations\n\nAnother kind of non-blind SR methods aims to propose a single model for multiple degradations, i.e. the second step of the two-step solution for blind SR. These methods take both LR image and its corresponding blur kernel as inputs. In [13,29], the blur kernel is used to downsample images and synthesize training samples, which can be used to train a specific model for given kernel and LR image. In [37], the blur kernel and LR image are directly concatenated at the first layer of a DNN. Thus, the SR result can be closely correlated to both LR image and blur kernel. In [38], Zhang et al.\n\nproposed a method based on ADMM algorithm. They interpret this problem as MAP optimization and solve the data term and prior term alternately. In [14], a spatial feature transform (SFT) layer is proposed to better preserve the details in LR image while blur kernel is an additional input. However, as pointed out in [14], the SR results of these methods are usually sensitive to the provided blur kernels. Small deviation of provided kernel from the ground truth will cause severe performance drop of these non-blind SR methods.\n\n\nBlind Super Resolution\n\nPrevious methods for blind SR are usually the sequential combinations of a kernel-estimation method and a non-blind SR method. Thus kernel-estimation methods are also an important part of blind SR. In [26], Michaeli et al. estimate the blur kernel by utilizing the internal patch recurrence. In [3] and [5], LR image is firstly downsampled by a generative network, and then a discriminator is used to verify whether the downsampled image has the same distribution with original LR image. In this way, the blur kernel can be learned by the generative network. In [14], Gu et al. not only train a network for kernel estimation, but also propose a correction network to iteratively correct the kernel. Although the accuracy of estimated kernel is largely improved, it requires training of two or even three networks, which is rather complicated. Instead, DAN is an end-to-end trainable network that is much easier to be trained and has much higher speed.\n\n3 End-to-End Blind Super Resolution\n\n\nProblem Formulation\n\nAs shown in Equation 1, there are three variables, i.e. x, k and n, to be determined in blind SR problem. In literature, we can apply a denoise algorithm [36,7,15] in the first place. Then blind SR algorithm only needs to focus on solving k and x. It can be mathematically expressed an optimization problem: arg min\nk,x y \u2212 (x \u2297 k) \u2193 s 1 + \u03c6(x)(2)\nwhere the former part is the reconstruction term, and \u03c6(x) is prior term for HR image. The prior term is usually unknown and has no analytic expression. Thus it is extremely difficult to solve this problem directly. Previous methods decompose this problem into two sequential steps:\nk = M (y) x = arg min x y \u2212 (x \u2297 k) \u2193 s 1 + \u03c6(x)(3)\nwhere M (\u00b7) denotes the function that estimates k from y, and the second step is usually solved by a non-blind SR method described in Sec 2.2. This two-step solution has its drawbacks in threefold. Firstly, this algorithm usually requires training of two or even more models, which is rather complicated. Secondly, M (\u00b7) can only utilize information from y, which treats k as a kind of prior of y. But in fact, k could not be properly solved without information from x. At last, the non-blind SR model for the second step is trained with ground-truth kernels. While during testing, it can only have access to kernels estimated in the first step. The difference between ground-truth and estimated kernels will usually cause serve performance drop of the non-blind SR model [14].\n\nTowards these drawbacks, we propose an end-to-end network that can largely release these issues. We still split it into two subproblems, but instead of solving them sequentially, we adopt an alternating optimization algorithm, which restores SR image and estimates corresponding blur kernel alternately. The mathematical expression is\n\uf8f1 \uf8f2 \uf8f3 k i+1 = arg min k y \u2212 (x i \u2297 k) \u2193 s 1 x i+1 = arg min x y \u2212 (x \u2297 k i ) \u2193 s 1 + \u03c6(x)(4)\nWe alternately solve these two subproblems both via convolutional neural modules, namely Estimator and Restorer respectively. Actually, there even has an analytic solution for Estimator. But we experimentally find that analytic solution is more time-consuming and not robust enough (when noise is not fully removed). We fix the number of iterations as T and unfold the iterating process to form an end-to-end trainable network, which is called deep alternating network (DAN).\n\nAs shown in Figure 1, we initialize the kernel by Dirac function, i.e. the center of the kernel is one and zeros otherwise. Following [14,37], the kernel is also reshaped and then reduced by principal component analysis (PCA). We set T = 4 in practice and both modules are supervised only at the last iteration by L1 loss. The whole network could be well trained without any restrictions on intermediate results, because the parameters of both modules are shared between different iterations.\n\nIn DAN, Estimator takes both LR and SR images as inputs, which makes the estimation of blur kernel k much easier. More importantly, Restorer is trained with the kernel estimated by Estimator, instead of ground-truth kernel as previous methods do. Thus, Restorer could be more tolerant to the estimation error of Estimator during testing. Besides, compared with previous two-step solutions, the results of both modules in DAN could be substantially improved, and it is likely for DAN to get better final results. Specially, in the case where the scale factor s = 1, DAN becomes an deblurring network. Due to limited pages, we only discuss SR cases in this paper.\n\n\nInstantiate the Convolutional Neural Modules\n\nBoth modules in our network have two inputs. Estimator takes LR and SR image, and Restorer takes LR image and blur kernel as inputs. We define LR image as basic input, and the other one is conditional input. For example, blur kernel is the conditional input of Restorer. During iterations, the basic inputs of both modules keep the same, but their conditional inputs are repeatedly updated. We claim that it is significantly important to keep the output of each module closely related to its conditional input. Otherwise, the iterating results will collapse to a fixed point at the first iteration. Specifically, if Estimator outputs the same kernel regardless the value of SR image, or Restorer outputs the same SR image regardless of the value of blur kernel, their outputs will only depend on the basic input, and the results will keep the same during the iterations.\n\nTo ensure that the outputs of Estimator and Restorer are closely related to their conditional inputs, we propose a conditional residual block (CRB). On the basis of the residual block in [39], we concatenate the conditional and basic inputs at the beginning:\nf out = R(Concat([f basic , f cond ])) + f basic(5)\nwhere R(\u00b7) denotes the residual mapping function of CRB and Concat([\u00b7, \u00b7]) denotes concatenation.\n\nf basic and f cond are the basic input and conditional input respectively. As shown in Figure 2 (c), the residual mapping function consists of two 3 \u00d7 3 convolutional layers and one channel attention layer [17]. Both Estimator and Restorer are build by CRBs.\n\nEstimator. The whole structure of Estimator is shown in Figure 2   the PCA result of blur kernel. In practice, Estimator has 5 CRBs, and both basic input and conditional input of each CRB have 32 channels.\n\nRestorer. The whole structure of Restorer is shown in Figure 2 (a). In Restorer, we stretch the kernel in spatial dimension to the same spatial size as LR image. Then the stretched kernel is sent to all CRBs of Restorer as conditional inputs. We use PixelShuffle [28] layers to upscale the features to desired size. In practice, Restorer has 40 CRBs, and the basic input and conditional input of each CRB has 64 and 10 channels respectively.\n\n\nExperiments\n\n\nExperiments on Synthetic Test Images\n\n\nData, Training and Testing\n\nWe collect 3450 HR images from DIV2K [1] and Flickr2K [11] as training set. To make reasonable comparison with other methods, we train models with two different degradation settings. One is the setting in [14], which only focuses on cases with isotropic Gaussian blur kernels. The other is the setting in [3], which focuses on cases with more general and irregular blur kernels.\n\nSetting 1. Following the setting in [14], the kernel size is set as 21. Setting 2. Following the setting in [3], we set the kernel size as 11. We firstly generate anisotropic Gaussian kernels. The lengths of both axises are uniformly distributed in (0.6, 5), rotated by a random angle uniformly distributed in [\u2212\u03c0, \u03c0]. To deviate from a regular Gaussian, we further apply uniform multiplicative noise (up to 25% of each pixel value of the kernel) and normalize it to sum to one. For testing, we use the benchmark dataset DIV2KRK that is used in [3].\n\nThe input size during training is 64 \u00d7 64 for all scale factors. The batch size is 64. Each model is trained for 4 \u00d7 10 5 iterations. We use Adam [22] as our optimizer, with \u03b2 1 = 0.9, \u03b2 2 = 0.99. The initial learning rate is 2 \u00d7 10 \u22124 , and will decay by half at every 1 \u00d7 10 5 iterations. All models are trained on RTX2080Ti GPUs.  Figure 3: Visual results of img 005 in Urban100. The width of blur kernel is 1.8.\n\n\nQuantitative Results\n\nSetting 1. For the first setting, we evaluate our method on test images synthesized by Gaussian8 kernels. We mainly compare our results with ZSSR [29] and IKC [14], which are methods designed for blind SR. We also include a comparison with CARN [2]. Since it is not designed for blind SR, we perform deblurring method [27] before or after CARN. The PSNR and SSIM results on Y channel of transformed YCbCr space are shown in Table 1.\n\nDespite that CARN achieves remarkable results in the context of bicubic downsampling, it suffers severe performance drop when applied to images with unknown blur kernels. Its performance is largely improved when it is followed by a deblurring method, but still inferior to that of blind-SR methods. ZSSR trains specific network for each single tested image by utilizing the internal patch recurrence. However, ZSSR has an in-born drawback: the training samples for each image are limited, and thus it cannot learn a good prior for HR images. IKC is also a two-step solution for blind SR. Although the accuracy of estimated kernel is largely improved in IKC, the final result is still suboptimal. DAN is trained in an end-to-end scheme, which is not only much easier to be trained than two-step solutions, but also likely to a reach a better optimum point. As shown in Table 1, the PSNR result of DAN on Manga109 for scale \u00d73 is even 4.95dB higher than that of IKC. For other scales and datasets, DAN also largely outperforms IKC.\n\nThe visual results of img 005 in Urban100 are shown in Figure 3 for comparison. As one can see, CARN and ZSSR even cannot restore the edges for the window. IKC performs better, but the edges are severely blurred. While DAN can restore sharp edges and produce more visually pleasant result.\n\nSetting 2. The second setting involves irregular blur kernels, which is more general, but also more difficult to solve. For Setting 2, we mainly compare methods of three different classes: i) SOTA SR algorithms trained on bicubically downsampled images such as EDSR [23] and RCAN [39] , ii) blind SR methods designed for NTIRE competition such as PDN [31] and WDSR [33], iii) the two-step solutions, i.e. the combination of a kernel estimation method and a non-blind SR method, such as Kernel-GAN [3] and ZSSR [29]. The PSNR and SSIM results on Y channl are shown in Table 2.\n\nSimilarly, the performance of methods trained on bicubically downsampled images is limited by the domain gap. Thus, their results are only slightly better than that of interpolation. The methods in Class 2 are trained on synthesized images provided in NTIRE competition. Although these methods  Figure 4: Visual results on img 892 in DIV2KRK.\n\nachieve remarkable results in the competition, they still cannot generalize well to irregular blur kernels.\n\nThe comparison between methods of Class 3 can enlighten us a lot. Specifically, USRNet [35] achieves remarkable results when GT kernels are provided, and KernelGAN also performs well on kernel estimation. However, when they are combined together, as shown in Table 2, the final SR results are worse than all other methods. This indicates that it is important for the Estimator and Restorer to be compatible with each other. Additionally, although better kernel-estimation method can benefit the SR results, the overall performance is still largely inferior to that of DAN. DAN outperforms the combination of KernelGAN and ZSSR by 2.20dB and 0.74dB for scales \u00d72 and \u00d74 respectively.\n\nThe visual results of img 892 in DIVKRK are shown in Figure 4. Although the combination of KernelGAN and ZSSR can produce slightly shaper edges than interpolation, it suffers from severe artifacts. The SR image of DAN is obviously much cleaner and has more reliable details. \n\n\nStudy of Estimated Kernels\n\nTo evaluated the accuracy of predicted kernels, we calculate their L1 errors in the reduced space, and the results on Urban100 are shown in Figure 5 (a). As one can see that the L1 error of reduced kernels predicted by DAN are much lower than that of IKC. It suggests that the overall improvements of DAN may partially come from more accurate retrieved kernels. We also plot the PSNR results with respect to kernels with different sigma in Figure 5 (b). As sigma increases, the performance gap  between IKC and DAN also becomes larger. It indicates that DAN may have better generalization ability.\n\nWe also replace the estimated kernel by ground truth (GT) to further investigate the influence of Estimator. If GT kernels are provided, the iterating processing becomes meaningless. Thus we test the Restorer with just once forward propagation. The tested results for Setting 1 is shown in Table 3.\n\nThe result almost keeps unchanged and sometimes even gets worser when GT kernels are provided. It indicates that Predictor may have already satisfied the requirements of Restorer, and the superiority of DAN also partially comes from this good cooperation between its Predictor and Restorer.\n\n\nStudy of Iterations\n\nAfter the model is trained, we also change the number of iterations to see whether the two modules have learned the property of convergence or just have 'remembered' the iteration number. The model is trained with 4 iterations, but during testing we increase the iteration number from 1 to 7. As shown in Figure 6 (a) and (c), the average PSNR results on Set5 and Set14 firstly increase rapidly and then gradually converge. It should be noted that when we iterate more times than training, the performance dose not becomes worse, and sometimes even becomes better. For example, the average PSNR on Set14 is 20.43dB when the iteration number is 5, higher than 20.42dB when we iterate 4 times. Although the incremental is relatively small, it suggests that the two modules may have learned to cooperate with each other, instead of solving this problem like ordinary end-to-end networks, in which cases, the performance will drop significantly when the setting of testing is different from that of training. It also suggests that the estimation error of intermediate results does not destroy the convergence of DAN. In other words, DAN is robust to various estimation error.\n\n\nInference Speed\n\nOne more superiority of our end-to-end model is that it has higher inference speed. To make a quantitative comparison, we evaluate the average speed of different methods on the same platform. We choose the 40 images synthesized by Gaussian8 kernels from Set5 as testing images, and all methods are evaluated on the same platform with a RTX2080Ti GPU. We choose KernelGAN [3] + ZSSR [29] as the one of the representative methods. Its speed is 415.7 seconds per image. IKC [14] has much faster inference speed, which is only 3.93 seconds per image. As a comparison, the average speed of DAN is 0.75 seconds per image, nearly 554 times faster than KernelGAN + ZSSR, and 5 times faster than IKC. In other words, DAN not only can largely outperform SOTA blind SR methods on PSNR results, but also has much higher speed.\n\n\nExperiments on Real World Images\n\nWe also conduct experiments to prove that DAN can generalize well to real wold images. In this case, we need to consider the influence of additive noise. As we mentioned in Sec 3.1, we can perform an denoise algorithm in the first place. But for simplicity, we retrain a different model by adding AWGN to LR image during training. In this way, DAN would be forced to generalize to noisy images.  The covariance of noise is set as 15. We use KernelGAN [3] + ZSSR [29] and IKC [14] as the representative methods for blind SR, and CARN [2] as the representative method for non-blind SR method. The commonly used image chip [12] is chosen as test image. It should be noted that it is a real image and we do not have the ground truth. Thus we can only provide a visual comparison in Figure [12]. As one can see, the result of KernelGAN + ZSSR is slightly better than bicubic interpolation, but is still heavily blurred. The result of CARN is over smoothed and the edge is not sharp enough. IKC produces cleaner result, but there are still some artifacts. The letter 'X' restored by IKC has an obvious dark line at the top right part. But this dark line is much lighter in the image restored by DAN. It suggests that if trained with noisy images, DAN can also learn to denoise, and produce more visually pleasant results with more reliable details. This is because that both modules are implemented via convolutional layers, which are flexible enough to be adapted to different tasks. \n\n\nConclusion\n\nIn this paper, we have proposed an end-to-end algorithm for blind SR. This algorithm is based on alternating optimization, the two parts of which are both implemented by convolutional modules, namely Restorer and Estimator. We unfold the alternating process to form an end-to-end trainable network. In this way, Estimator can utilize information from both LR and SR images, which makes it easier to estimate blur kernel. More importantly, Restorer is trained with the kernel estimated by Estimator, instead of ground-truth kernel, thus Restorer could be more tolerant to with the estimation error of Estimator. Besides, the results of both modules could be substantially improved during the iterations, thus it is likely for DAN to get better final results than previous two-step solutions. Experiments also prove that DAN outperforms SOTA blind SR methods by a large margin. In the future, if the two parts of DAN can be implemented by more powerful modules, we believe that its performance could be further improved.\n\n\nBroader Impact\n\nSuper Resolution is a traditional task in computer vision. It has been studied for several decades and has wide applications in video enhancement, medical imaging, as well as security and surveillance imaging. These techniques have largely benefited the society in various areas for years and have no negative impact yet. The proposed method (DAN) could further improve the merits of these applications especially in cases where the degradations are unknown. DAN has relatively better performance and much higher speed, and it is possible for DAN to be used in real-time video enhancement or surveillance imaging. This work does not present any negative foreseeable societal consequence.\n\nFigure 1 :\n1The overview structure of the deep alternating network (DAN).\n\nFigure 2 :\n2The details of Estimator and Restorer. (a) (top) The details of Restorer. (c) (bottom-left) The details of Estimator. (c) (bottom-right) The details of conditional residual block (CRB).\n\n\n) PSNR over kernels with different sigma.\n\nFigure 5 :\n5The L1 loss of predicted kernels with different sigma (left) and PSNR results with respect to different kernels (right).\n\nFigure 6 :\n6PSNR and visual results with different iterations during testing on Set5 and Set14.\n\nFigure 7 :\n7Visual results on real image chip.\n\n\nLearning based methods for SISR usually require a large number of paired HR and LR images as training samples. However, these paired samples are hard to get in real world. As a result, researchers manually synthesize LR images from HR images with predefined downsampling settings. The most popular setting is bicubic interpolation, i.e. defining k in Equation 1 as bicubic kernel.2 Related Work \n\n2.1 Super Resolution in the Context of Bicubic Interpolation \n\nFrom the \narising of SRCNN [9], various DNNs [21, 40, 39, 10, 16, 18] have been proposed based on this setting. \nRecently, after the proposal of RCAN \n\n\n(b). We firstly downsample SR image by a convolutional layer with stride s. Then the feature maps are sent to all CRBs as conditional inputs. At the end of the network, we squeeze the features by global average pooling to form the elements of predicted kernel. Since the kernel is reduced by PCA, Estimator only needs to estimatek=3, s=1 \n\nk=3, s=1 \n\nCRB \nCRB \nCRB \n\nReduced kernel \n\nLR \n\nstrech \n\nf cond \nf cond \nf cond \n\nPixelShuffle \nLayers \n\nLR \n\nKernel maps \n\nk=3, s=1 \n\nCRB \nCRB \nCRB \n\nCRB \nCRB \nCRB CRB \nCRB \nCRB \n\nk=9, s=scale \n\nReduced kernel \n\nSR \n\nf cond \nf cond \nf cond \n\nk=3, s=1 \nk=3, s=1 \nCALayer \n\nconcat \n\nf cond \n\nf basic \n\nf out \n\nk, s \n\nConvolutional layer \nk:kernel size s:stride \n\nGAP \n\nGAP \nGlobal average pooling \n\nCALayer Channel attention layer \n\n(a) Details of Restorer \n\n(b) Details of Estimator \n\n(c) Details of Conditional Residual \nBlock (CRB) \n\nk=3, s=1 \nElement-wise sum \n\n\n\n\nrespectively. The HR images are first blurred by the selected blur kernels and then downsampled to form synthetic test images.During training, the kernel width \nis uniformly sampled in [0.2, 4.0], [0.2, 3.0] and [0.2, 2.0] for scale factors 4, 3 and 2 respectively. \nFor quantitative evaluation, we collect HR images from the commonly used benchmark datasets, i.e. \nSet5 [4], Set14 [34], Urban100 [19], BSD100 [24] and Manga109 [25]. Since determined kernels \nare needed for reasonable comparison, we uniformly choose 8 kernels, denoted as Gaussian8, from \nrange [1.8, 3.2], [1.35, 2.40] and [0.80, 1.60] for scale factors 4, 3 and 2 \n\nTable 1 :\n1Quantitative comparison with SOTA SR methods with Setting 1. The best two results are highlighted in red and blue colors respectively.Method \nScale \nSet5 \nSet14 \nBSD100 \nUrban100 \nManga109 \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nBicubic \n\n2 \n\n28.82 \n0.8577 \n26.02 \n0.7634 \n25.92 \n0.7310 \n23.14 \n0.7258 \n25.60 \n0.8498 \nCARN [2] \n30.99 \n0.8779 \n28.10 \n0.7879 \n26.78 \n0.7286 \n25.27 \n0.7630 \n26.86 \n0.8606 \nZSSR [29] \n31.08 \n0.8786 \n28.35 \n0.7933 \n27.92 \n0.7632 \n25.25 \n0.7618 \n28.05 \n0.8769 \n[27]+CARN [2] \n24.20 \n0.7496 \n21.12 \n0.6170 \n22.69 \n0.6471 \n18.89 \n0.5895 \n21.54 \n0.7946 \nCARN [2]+[27] \n31.27 \n0.8974 \n29.03 \n0.8267 \n28.72 \n0.8033 \n25.62 \n0.7981 \n29.58 \n0.9134 \nIKC [14] \n36.62 \n0.9658 \n32.82 \n0.8999 \n31.36 \n0.9097 \n30.36 \n0.8949 \n36.06 \n0.9474 \nDAN \n37.33 \n0.9754 \n33.07 \n0.9068 \n31.76 \n0.9213 \n30.60 \n0.9020 \n37.23 \n0.9710 \nBicubic \n\n3 \n\n26.21 \n0.7766 \n24.01 \n0.6662 \n24.25 \n0.6356 \n21.39 \n0.6203 \n22.98 \n0.7576 \nCARN [2] \n27.26 \n0.7855 \n25.06 \n0.6676 \n25.85 \n0.6566 \n22.67 \n0.6323 \n23.84 \n0.7620 \nZSSR [29] \n28.25 \n0.7989 \n26.11 \n0.6942 \n26.06 \n0.6633 \n23.26 \n0.6534 \n25.19 \n0.7914 \n[27]+CARN [2] \n19.05 \n0.5226 \n17.61 \n0.4558 \n20.51 \n0.5331 \n16.72 \n0.4578 \n18.38 \n0.6118 \nCARN [2]+[27] \n30.31 \n0.8562 \n2757 \n0.7531 \n27.14 \n0.7152 \n24.45 \n0.7241 \n27.67 \n0.8592 \nIKC [14] \n32.16 \n0.9420 \n29.46 \n0.8229 \n28.56 \n0.8493 \n25.94 \n0.8165 \n28.21 \n0.8739 \nDAN \n34.04 \n0.9620 \n30.09 \n0.8287 \n28.94 \n0.8606 \n27.65 \n0.8352 \n33.16 \n0.9382 \nBicubic \n\n4 \n\n24.57 \n0.7108 \n22.79 \n0.6032 \n23.29 \n0.5786 \n20.35 \n0.5532 \n21.50 \n0.6933 \nCARN [2] \n26.57 \n0.7420 \n24.62 \n0.6226 \n24.79 \n0.5963 \n22.17 \n0.5865 \n21.85 \n0.6834 \nZSSR [29] \n26.45 \n0.7279 \n24.78 \n0.6268 \n24.97 \n0.5989 \n22.11 \n0.5805 \n23.53 \n0.7240 \n[27]+CARN [2] \n18.10 \n0.4843 \n16.59 \n0.3994 \n18.46 \n0.4481 \n15.47 \n0.3872 \n16.78 \n0.5371 \nCARN [2]+[27] \n28.69 \n0.8092 \n26.40 \n0.6926 \n26.10 \n0.6528 \n23.46 \n0.6597 \n25.84 \n0.8035 \nIKC [14] \n31.52 \n0.9278 \n28.26 \n0.7688 \n27.29 \n0.8041 \n25.33 \n0.7760 \n29.90 \n0.8793 \nDAN \n31.89 \n0.9302 \n28.43 \n0.7693 \n27.51 \n0.8078 \n25.86 \n0.7822 \n30.50 \n0.9037 \n\nGT \n\nPSNR / SSIM \n13.47 / 0.0812 \n\nCARN \nZSSR \n\n13.46 / 0.0612 \n16.79 / 0.6748 \n\nIKC \n\n20.18 / 0.8950 \n\nDAN \n\n\n\nTable 2 :\n2Quantitative comparison with SOTA SR methods with Setting 2. The best two results are highlighted in red and blue colors respectively.Types \nMethod \n\nScale \n2 \n4 \nPSNR \nSSIM \nPSNR \nSSIM \n\nClass 1 \n\nBicubic \n28.73 \n0.8040 \n25.33 \n0.6795 \nBicubic kernel + ZSSR [29] \n29.10 \n0.8215 \n25.61 \n0.6911 \nEDSR [23] \n29.17 \n0.8216 \n25.64 \n0.6928 \nRCAN [39] \n29.20 \n0.8223 \n25.66 \n0.6936 \n\nClass 2 \n\nPDN [31] -1st in NTIRE'19 track4 \n/ \n/ \n26.34 \n0.7190 \nWDSR [33] -1st in NTIIRE'19 track2 \n/ \n/ \n21.55 \n0.6841 \nWDSR [33] -1st in NTIRE'19 track3 \n/ \n/ \n21.54 \n0.7016 \nWDSR [33] -2nd in NTIRE'19 track4 \n/ \n/ \n25.64 \n0.7144 \nJi et al. [20] -1st in NITRE'20 track 1 \n/ \n/ \n25.43 \n0.6907 \n\nClass 3 \n\nCornillere et al. [6] \n29.46 \n0.8474 \n/ \n/ \nMichaeli et al. [26] + SRMD [37] \n25.51 \n0.8083 \n23.34 \n0.6530 \nMichaeli et al. [26] + ZSSR [29] \n29.37 \n0.8370 \n26.09 \n0.7138 \nKernelGAN [3] + SRMD [37] \n29.57 \n0.8564 \n25.71 \n0.7265 \nKernelGAN [3] + USRNet [35] \n/ \n/ \n20.06 \n0.5359 \nKernelGAN [3]+ ZSSR [29] \n30.36 \n0.8669 \n26.81 \n0.7316 \nOurs \nDAN \n32.56 \n0.8997 \n27.55 \n0.7582 \n\nGT \n\nPSNR / SSIM \n29.72 / 0.9045 \n\nBicubic \nKernelGAN + ZSSR \n\n30.65 / 0.8603 \n38.09 / 0.9724 \n\nDAN \n\n\n\nTable 3 :\n3PSNR results when GT kernels are provided.Methods \nSet5 Set14 B100 Urban100 Manga109 \nDAN \n31.89 28.43 27.51 \n25.86 \n30.50 \nDAN(GT kernel) 31.85 28.42 27.51 \n25.87 \n30.51 \n\n\n\n\nVisual Results of butterfly in Set5 with different iterations (c) Average PSNR on Set14 (b) Visual Results of zebra in Set14 with different iterations(a) Average PSNR on Set5 \n\nGT of butterfly \n\nPSNR/SSIM \n\nIter 1 \n\n23.03 / 0.4843 \n\nIter 2 \n\n25.18 / 0.6145 \n\nIter 3 \n\n25.30 / 0.6219 \n\nIter 4 \n\n25.60 / 0.5340 \n(b) GT of zebra \n\nPSNR/SSIM \n\nIter 1 \nIter 2 \nIter 3 \nIter 4 \n\n21.91 / 0.5957 \n24.19 / 0.7812 \n24.13 / 0.7853 \n24.23 / 0.7851 \n\n\n\nNtire 2017 challenge on single image super-resolution: Dataset and study. Eirikur Agustsson, Radu Timofte, IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1122-1131, 2017.\n\nFast, accurate, and lightweight super-resolution with cascading residual network. Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast, accurate, and lightweight super-resolution with cascading residual network. In Proceedings of the European Conference on Computer Vision (ECCV), pages 252-268, 2018.\n\nBlind super-resolution kernel estimation using an internal-gan. Sefi Bell-Kligler, Assaf Shocher, Michal Irani, NeurIPS. Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using an internal-gan. In NeurIPS, 2019.\n\nLow-complexity single-image super-resolution based on nonnegative neighbor embedding. Marco Bevilacqua, Aline Roumy, Christine Guillemot, Marie-Line Alberi-Morel, BMVC. Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie-Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, 2012.\n\nTo learn image super-resolution, use a gan to learn how to do image degradation first. Adrian Bulat, Jing Yang, Georgios Tzimiropoulos, ECCV. Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To learn image super-resolution, use a gan to learn how to do image degradation first. In ECCV, 2018.\n\nBlind image super-resolution with spatially variant degradations. Abdelaziz Victor Cornillere, Wang Djelouah, Olga Yifan, Christopher Sorkine-Hornung, Schroers, ACM Transactions on Graphics (TOG). 386Victor Cornillere, Abdelaziz Djelouah, Wang Yifan, Olga Sorkine-Hornung, and Christopher Schroers. Blind image super-resolution with spatially variant degradations. ACM Transactions on Graphics (TOG), 38(6):1-13, 2019.\n\nImage denoising by sparse 3-d transform-domain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen O Egiazarian, IEEE Transactions on Image Processing. 16Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen O. Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on Image Processing, 16:2080-2095, 2007.\n\nSecond-order attention network for single image super-resolution. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11065-11074, 2019.\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, European conference on computer vision. SpringerChao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In European conference on computer vision, pages 184-199. Springer, 2014.\n\nAccelerating the super-resolution convolutional neural network. Chao Dong, Chen Change Loy, Xiaoou Tang, European conference on computer vision. SpringerChao Dong, Chen Change Loy, and Xiaoou Tang. Accelerating the super-resolution convolutional neural network. In European conference on computer vision, pages 391-407. Springer, 2016.\n\nNtire 2017 challenge on single image super-resolution: Methods and results. Radu Timofte, IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Radu Timofte et al. Ntire 2017 challenge on single image super-resolution: Methods and results. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1110-1121, 2017.\n\nImage upsampling via imposed edge statistics. Raanan Fattal, ACM Trans. Graph. 2695Raanan Fattal. Image upsampling via imposed edge statistics. ACM Trans. Graph., 26:95, 2007.\n\nSuper-resolution from a single image. Daniel Glasner, Shai Bagon, Michal Irani, IEEE 12th International Conference on Computer Vision. Daniel Glasner, Shai Bagon, and Michal Irani. Super-resolution from a single image. 2009 IEEE 12th International Conference on Computer Vision, pages 349-356, 2009.\n\nBlind super-resolution with iterative kernel correction. Jinjin Gu, Hannan Lu, Wangmeng Zuo, Chao Dong, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong. Blind super-resolution with iterative kernel correction. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1604-1613, 2019.\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, IEEE Conference on Computer Vision and Pattern Recognition. Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with application to image denoising. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 2862-2869, 2014.\n\nDeep back-projection networks for super-resolution. Muhammad Haris, Gregory Shakhnarovich, Norimichi Ukita, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMuhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks for super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1664-1673, 2018.\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132-7141, 2018.\n\nMeta-sr: A magnificationarbitrary network for super-resolution. Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, Jian Sun, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, and Jian Sun. Meta-sr: A magnification- arbitrary network for super-resolution. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1575-1584, 2019.\n\nSingle image super-resolution from transformed self-exemplars. Jia-Bin Huang, Abhishek Singh, Narendra Ahuja, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5197-5206, 2015.\n\nReal-world super-resolution via kernel estimation and noise injection. Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsXiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution via kernel estimation and noise injection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 466-467, 2020.\n\nAccurate image super-resolution using very deep convolutional networks. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1646-1654, 2016.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsBee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 136-144, 2017.\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David R Martin, Charless C Fowlkes, Doron Tal, Jitendra Malik, Proceedings Eighth IEEE International Conference on Computer Vision. ICCV. Eighth IEEE International Conference on Computer Vision. ICCV2David R. Martin, Charless C. Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, 2:416-423 vol.2, 2001.\n\nSketch-based manga retrieval using manga109 dataset. Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, Kiyoharu Aizawa, Multimedia Tools and Applications. 76Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 76:21811-21838, 2016.\n\nNonparametric blind super-resolution. Tomer Michaeli, Michal Irani, IEEE International Conference on Computer Vision. Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. 2013 IEEE International Conference on Computer Vision, pages 945-952, 2013.\n\nDeblurring images via dark channel prior. Jinshan Pan, Deqing Sun, Hanspeter Pfister, Ming-Hsuan Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 40Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang. Deblurring images via dark channel prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:2315-2328, 2018.\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionWenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1874-1883, 2016.\n\nzero-shot\" super-resolution using deep internal learning. Assaf Shocher, Nadav Cohen, Michal Irani, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Assaf Shocher, Nadav Cohen, and Michal Irani. \"zero-shot\" super-resolution using deep internal learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nEsrgan: Enhanced super-resolution generative adversarial networks. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yi-Hao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang, abs/1809.00219ArXiv. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yi-Hao Liu, Chao Dong, Chen Change Loy, Yu Qiao, and Xiaoou Tang. Esrgan: Enhanced super-resolution generative adversarial networks. ArXiv, abs/1809.00219, 2018.\n\nDeep poly-dense network for image superresolution. Wang Xintao, Yu Ke, Hui Tak-Wa, Dong Chao, Lin Liang, Change Loy Chen, Wang Xintao, Yu Ke, Hui Tak-Wa, Dong Chao, Lin Liang, and Change Loy Chen. Deep poly-dense network for image superresolution. 2018.\n\nUnified dynamic convolutional network for super-resolution with variational degradations. Yu-Syuan Xu, Shou-Yao Roy Tseng, Yu Hung Tseng, Hsien-Kai Kuo, Yi-Min Tsai, abs/2004.06965ArXiv. Yu-Syuan Xu, Shou-Yao Roy Tseng, Yu Hung Tseng, Hsien-Kai Kuo, and Yi-Min Tsai. Unified dynamic convolutional network for super-resolution with variational degradations. ArXiv, abs/2004.06965, 2020.\n\nWide activation for efficient and accurate image super-resolution. Jiahui Yu, Yuchen Fan, Jianchao Yang, Ning Xu, Zhaowen Wang, Xinchao Wang, Thomas S Huang, abs/1808.08718ArXiv. Jiahui Yu, Yuchen Fan, Jianchao Yang, Ning Xu, Zhaowen Wang, Xinchao Wang, and Thomas S. Huang. Wide activation for efficient and accurate image super-resolution. ArXiv, abs/1808.08718, 2018.\n\nOn single image scale-up using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, Curves and Surfaces. Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Curves and Surfaces, 2010.\n\nDeep unfolding network for image super-resolution. ArXiv, abs. Kai Zhang, Luc Van Gool, Radu Timofte, Kai Zhang, Luc Van Gool, and Radu Timofte. Deep unfolding network for image super-resolution. ArXiv, abs/2003.10428, 2020.\n\nLearning deep cnn denoiser prior for image restoration. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2808-2817, 2017.\n\nLearning a single convolutional super-resolution network for multiple degradations. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a single convolutional super-resolution network for multiple degradations. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3262-3271, 2018.\n\nDeep plug-and-play super-resolution for arbitrary blur kernels. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Kai Zhang, Wangmeng Zuo, and Lei Zhang. Deep plug-and-play super-resolution for arbitrary blur kernels. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1671-1681, 2019.\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 286-301, 2018.\n\nResidual dense network for image super-resolution. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2472-2481, 2018.\n", "annotations": {"author": "[{\"end\":351,\"start\":69},{\"end\":544,\"start\":352},{\"end\":700,\"start\":545},{\"end\":984,\"start\":701},{\"end\":1195,\"start\":985}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":80},{\"end\":361,\"start\":356},{\"end\":553,\"start\":551},{\"end\":711,\"start\":707},{\"end\":995,\"start\":992}]", "author_first_name": "[{\"end\":79,\"start\":69},{\"end\":355,\"start\":352},{\"end\":550,\"start\":545},{\"end\":706,\"start\":701},{\"end\":991,\"start\":985}]", "author_affiliation": "[{\"end\":204,\"start\":85},{\"end\":265,\"start\":206},{\"end\":350,\"start\":267},{\"end\":482,\"start\":363},{\"end\":543,\"start\":484},{\"end\":614,\"start\":555},{\"end\":699,\"start\":616},{\"end\":832,\"start\":713},{\"end\":910,\"start\":834},{\"end\":983,\"start\":912},{\"end\":1116,\"start\":997},{\"end\":1194,\"start\":1118}]", "title": "[{\"end\":66,\"start\":1},{\"end\":1261,\"start\":1196}]", "venue": null, "abstract": "[{\"end\":2928,\"start\":1263}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3269,\"start\":3266},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3576,\"start\":3572},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3853,\"start\":3849},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3855,\"start\":3853},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3858,\"start\":3855},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3861,\"start\":3858},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3863,\"start\":3861},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3866,\"start\":3863},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4623,\"start\":4619},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4626,\"start\":4623},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4629,\"start\":4626},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4632,\"start\":4629},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4995,\"start\":4991},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7419,\"start\":7415},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7433,\"start\":7429},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8152,\"start\":8148},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8155,\"start\":8152},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8317,\"start\":8313},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8490,\"start\":8486},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8656,\"start\":8652},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8826,\"start\":8822},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9266,\"start\":9262},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9359,\"start\":9356},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9367,\"start\":9364},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9627,\"start\":9623},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10231,\"start\":10227},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10233,\"start\":10231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10236,\"start\":10233},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11532,\"start\":11528},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12578,\"start\":12574},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12581,\"start\":12578},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14707,\"start\":14703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15136,\"start\":15132},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15660,\"start\":15656},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15958,\"start\":15955},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15976,\"start\":15972},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16127,\"start\":16123},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16226,\"start\":16223},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16338,\"start\":16334},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16409,\"start\":16406},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16846,\"start\":16843},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16999,\"start\":16995},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17439,\"start\":17435},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17452,\"start\":17448},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17537,\"start\":17534},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17611,\"start\":17607},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19315,\"start\":19311},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19329,\"start\":19325},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19400,\"start\":19396},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19414,\"start\":19410},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19545,\"start\":19542},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19559,\"start\":19555},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20166,\"start\":20162},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23855,\"start\":23851},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23944,\"start\":23940},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24774,\"start\":24771},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24786,\"start\":24782},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24799,\"start\":24795},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24856,\"start\":24853},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24944,\"start\":24940},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25109,\"start\":25105}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27613,\"start\":27539},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27812,\"start\":27614},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27856,\"start\":27813},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27990,\"start\":27857},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28087,\"start\":27991},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28135,\"start\":28088},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28748,\"start\":28136},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29657,\"start\":28749},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30294,\"start\":29658},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32482,\"start\":30295},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33659,\"start\":32483},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33845,\"start\":33660},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34286,\"start\":33846}]", "paragraph": "[{\"end\":3741,\"start\":2944},{\"end\":4429,\"start\":3743},{\"end\":5284,\"start\":4431},{\"end\":6581,\"start\":5286},{\"end\":6856,\"start\":6583},{\"end\":7209,\"start\":6858},{\"end\":7865,\"start\":7211},{\"end\":8504,\"start\":7912},{\"end\":9034,\"start\":8506},{\"end\":10012,\"start\":9061},{\"end\":10049,\"start\":10014},{\"end\":10388,\"start\":10073},{\"end\":10703,\"start\":10421},{\"end\":11533,\"start\":10756},{\"end\":11869,\"start\":11535},{\"end\":12438,\"start\":11963},{\"end\":12932,\"start\":12440},{\"end\":13595,\"start\":12934},{\"end\":14514,\"start\":13644},{\"end\":14774,\"start\":14516},{\"end\":14924,\"start\":14827},{\"end\":15184,\"start\":14926},{\"end\":15391,\"start\":15186},{\"end\":15834,\"start\":15393},{\"end\":16296,\"start\":15918},{\"end\":16847,\"start\":16298},{\"end\":17264,\"start\":16849},{\"end\":17721,\"start\":17289},{\"end\":18752,\"start\":17723},{\"end\":19043,\"start\":18754},{\"end\":19620,\"start\":19045},{\"end\":19964,\"start\":19622},{\"end\":20073,\"start\":19966},{\"end\":20757,\"start\":20075},{\"end\":21034,\"start\":20759},{\"end\":21662,\"start\":21065},{\"end\":21962,\"start\":21664},{\"end\":22254,\"start\":21964},{\"end\":23449,\"start\":22278},{\"end\":24283,\"start\":23469},{\"end\":25799,\"start\":24320},{\"end\":26832,\"start\":25814},{\"end\":27538,\"start\":26851}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10420,\"start\":10389},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10755,\"start\":10704},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11962,\"start\":11870},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14826,\"start\":14775}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17720,\"start\":17713},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18598,\"start\":18591},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19619,\"start\":19612},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20341,\"start\":20334},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":21961,\"start\":21954}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2942,\"start\":2930},{\"attributes\":{\"n\":\"2.2\"},\"end\":7910,\"start\":7868},{\"attributes\":{\"n\":\"2.3\"},\"end\":9059,\"start\":9037},{\"attributes\":{\"n\":\"3.1\"},\"end\":10071,\"start\":10052},{\"attributes\":{\"n\":\"3.2\"},\"end\":13642,\"start\":13598},{\"attributes\":{\"n\":\"4\"},\"end\":15848,\"start\":15837},{\"attributes\":{\"n\":\"4.1\"},\"end\":15887,\"start\":15851},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":15916,\"start\":15890},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":17287,\"start\":17267},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":21063,\"start\":21037},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":22276,\"start\":22257},{\"attributes\":{\"n\":\"4.2\"},\"end\":23467,\"start\":23452},{\"attributes\":{\"n\":\"4.3\"},\"end\":24318,\"start\":24286},{\"attributes\":{\"n\":\"5\"},\"end\":25812,\"start\":25802},{\"end\":26849,\"start\":26835},{\"end\":27550,\"start\":27540},{\"end\":27625,\"start\":27615},{\"end\":27868,\"start\":27858},{\"end\":28002,\"start\":27992},{\"end\":28099,\"start\":28089},{\"end\":30305,\"start\":30296},{\"end\":32493,\"start\":32484},{\"end\":33670,\"start\":33661}]", "table": "[{\"end\":28748,\"start\":28518},{\"end\":29657,\"start\":29080},{\"end\":30294,\"start\":29786},{\"end\":32482,\"start\":30441},{\"end\":33659,\"start\":32629},{\"end\":33845,\"start\":33714},{\"end\":34286,\"start\":33998}]", "figure_caption": "[{\"end\":27613,\"start\":27552},{\"end\":27812,\"start\":27627},{\"end\":27856,\"start\":27815},{\"end\":27990,\"start\":27870},{\"end\":28087,\"start\":28004},{\"end\":28135,\"start\":28101},{\"end\":28518,\"start\":28138},{\"end\":29080,\"start\":28751},{\"end\":29786,\"start\":29660},{\"end\":30441,\"start\":30307},{\"end\":32629,\"start\":32495},{\"end\":33714,\"start\":33672},{\"end\":33998,\"start\":33848}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12460,\"start\":12452},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15021,\"start\":15013},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15250,\"start\":15242},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15455,\"start\":15447},{\"end\":17191,\"start\":17183},{\"end\":18817,\"start\":18809},{\"end\":19925,\"start\":19917},{\"end\":20820,\"start\":20812},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21213,\"start\":21205},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21513,\"start\":21505},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22591,\"start\":22583}]", "bib_author_first_name": "[{\"end\":34369,\"start\":34362},{\"end\":34385,\"start\":34381},{\"end\":34779,\"start\":34772},{\"end\":34793,\"start\":34785},{\"end\":34808,\"start\":34800},{\"end\":35218,\"start\":35214},{\"end\":35238,\"start\":35233},{\"end\":35254,\"start\":35248},{\"end\":35497,\"start\":35492},{\"end\":35515,\"start\":35510},{\"end\":35532,\"start\":35523},{\"end\":35554,\"start\":35544},{\"end\":35851,\"start\":35845},{\"end\":35863,\"start\":35859},{\"end\":35878,\"start\":35870},{\"end\":36131,\"start\":36122},{\"end\":36155,\"start\":36151},{\"end\":36170,\"start\":36166},{\"end\":36189,\"start\":36178},{\"end\":36556,\"start\":36548},{\"end\":36574,\"start\":36564},{\"end\":36588,\"start\":36580},{\"end\":36605,\"start\":36600},{\"end\":36607,\"start\":36606},{\"end\":36939,\"start\":36936},{\"end\":36952,\"start\":36945},{\"end\":36966,\"start\":36958},{\"end\":36981,\"start\":36974},{\"end\":36990,\"start\":36987},{\"end\":37449,\"start\":37445},{\"end\":37460,\"start\":37456},{\"end\":37467,\"start\":37461},{\"end\":37480,\"start\":37473},{\"end\":37491,\"start\":37485},{\"end\":37812,\"start\":37808},{\"end\":37823,\"start\":37819},{\"end\":37830,\"start\":37824},{\"end\":37842,\"start\":37836},{\"end\":38161,\"start\":38157},{\"end\":38504,\"start\":38498},{\"end\":38673,\"start\":38667},{\"end\":38687,\"start\":38683},{\"end\":38701,\"start\":38695},{\"end\":38993,\"start\":38987},{\"end\":39004,\"start\":38998},{\"end\":39017,\"start\":39009},{\"end\":39027,\"start\":39023},{\"end\":39392,\"start\":39385},{\"end\":39400,\"start\":39397},{\"end\":39416,\"start\":39408},{\"end\":39430,\"start\":39422},{\"end\":39774,\"start\":39766},{\"end\":39789,\"start\":39782},{\"end\":39814,\"start\":39805},{\"end\":40217,\"start\":40214},{\"end\":40224,\"start\":40222},{\"end\":40235,\"start\":40231},{\"end\":40622,\"start\":40616},{\"end\":40634,\"start\":40627},{\"end\":40646,\"start\":40639},{\"end\":40659,\"start\":40654},{\"end\":40672,\"start\":40666},{\"end\":40682,\"start\":40678},{\"end\":41071,\"start\":41064},{\"end\":41087,\"start\":41079},{\"end\":41103,\"start\":41095},{\"end\":41468,\"start\":41459},{\"end\":41476,\"start\":41473},{\"end\":41486,\"start\":41482},{\"end\":41500,\"start\":41492},{\"end\":41512,\"start\":41507},{\"end\":41523,\"start\":41517},{\"end\":42042,\"start\":42037},{\"end\":42052,\"start\":42048},{\"end\":42057,\"start\":42053},{\"end\":42072,\"start\":42063},{\"end\":42487,\"start\":42486},{\"end\":42503,\"start\":42498},{\"end\":42730,\"start\":42727},{\"end\":42744,\"start\":42736},{\"end\":42756,\"start\":42750},{\"end\":42770,\"start\":42762},{\"end\":42785,\"start\":42776},{\"end\":43346,\"start\":43341},{\"end\":43348,\"start\":43347},{\"end\":43365,\"start\":43357},{\"end\":43367,\"start\":43366},{\"end\":43382,\"start\":43377},{\"end\":43396,\"start\":43388},{\"end\":43913,\"start\":43907},{\"end\":43926,\"start\":43922},{\"end\":43936,\"start\":43932},{\"end\":43951,\"start\":43946},{\"end\":43966,\"start\":43962},{\"end\":43983,\"start\":43974},{\"end\":44002,\"start\":43994},{\"end\":44310,\"start\":44305},{\"end\":44327,\"start\":44321},{\"end\":44582,\"start\":44575},{\"end\":44594,\"start\":44588},{\"end\":44609,\"start\":44600},{\"end\":44629,\"start\":44619},{\"end\":45009,\"start\":45003},{\"end\":45019,\"start\":45015},{\"end\":45037,\"start\":45031},{\"end\":45054,\"start\":45046},{\"end\":45062,\"start\":45061},{\"end\":45074,\"start\":45071},{\"end\":45089,\"start\":45083},{\"end\":45103,\"start\":45098},{\"end\":45659,\"start\":45654},{\"end\":45674,\"start\":45669},{\"end\":45688,\"start\":45682},{\"end\":46028,\"start\":46022},{\"end\":46037,\"start\":46035},{\"end\":46050,\"start\":46042},{\"end\":46061,\"start\":46055},{\"end\":46072,\"start\":46066},{\"end\":46082,\"start\":46078},{\"end\":46093,\"start\":46089},{\"end\":46100,\"start\":46094},{\"end\":46108,\"start\":46106},{\"end\":46121,\"start\":46115},{\"end\":46411,\"start\":46407},{\"end\":46422,\"start\":46420},{\"end\":46430,\"start\":46427},{\"end\":46443,\"start\":46439},{\"end\":46453,\"start\":46450},{\"end\":46471,\"start\":46461},{\"end\":46709,\"start\":46701},{\"end\":46726,\"start\":46714},{\"end\":46736,\"start\":46734},{\"end\":46741,\"start\":46737},{\"end\":46758,\"start\":46749},{\"end\":46770,\"start\":46764},{\"end\":47071,\"start\":47065},{\"end\":47082,\"start\":47076},{\"end\":47096,\"start\":47088},{\"end\":47107,\"start\":47103},{\"end\":47119,\"start\":47112},{\"end\":47133,\"start\":47126},{\"end\":47146,\"start\":47140},{\"end\":47148,\"start\":47147},{\"end\":47430,\"start\":47425},{\"end\":47445,\"start\":47438},{\"end\":47457,\"start\":47452},{\"end\":47686,\"start\":47683},{\"end\":47697,\"start\":47694},{\"end\":47712,\"start\":47708},{\"end\":47905,\"start\":47902},{\"end\":47921,\"start\":47913},{\"end\":47934,\"start\":47927},{\"end\":47942,\"start\":47939},{\"end\":48308,\"start\":48305},{\"end\":48324,\"start\":48316},{\"end\":48333,\"start\":48330},{\"end\":48689,\"start\":48686},{\"end\":48705,\"start\":48697},{\"end\":48714,\"start\":48711},{\"end\":49078,\"start\":49073},{\"end\":49093,\"start\":49086},{\"end\":49101,\"start\":49098},{\"end\":49112,\"start\":49106},{\"end\":49125,\"start\":49119},{\"end\":49136,\"start\":49133},{\"end\":49551,\"start\":49546},{\"end\":49565,\"start\":49559},{\"end\":49574,\"start\":49572},{\"end\":49587,\"start\":49581},{\"end\":49598,\"start\":49595}]", "bib_author_last_name": "[{\"end\":34379,\"start\":34370},{\"end\":34393,\"start\":34386},{\"end\":34783,\"start\":34780},{\"end\":34798,\"start\":34794},{\"end\":34813,\"start\":34809},{\"end\":35231,\"start\":35219},{\"end\":35246,\"start\":35239},{\"end\":35260,\"start\":35255},{\"end\":35508,\"start\":35498},{\"end\":35521,\"start\":35516},{\"end\":35542,\"start\":35533},{\"end\":35567,\"start\":35555},{\"end\":35857,\"start\":35852},{\"end\":35868,\"start\":35864},{\"end\":35892,\"start\":35879},{\"end\":36149,\"start\":36132},{\"end\":36164,\"start\":36156},{\"end\":36176,\"start\":36171},{\"end\":36205,\"start\":36190},{\"end\":36215,\"start\":36207},{\"end\":36562,\"start\":36557},{\"end\":36578,\"start\":36575},{\"end\":36598,\"start\":36589},{\"end\":36618,\"start\":36608},{\"end\":36943,\"start\":36940},{\"end\":36956,\"start\":36953},{\"end\":36972,\"start\":36967},{\"end\":36985,\"start\":36982},{\"end\":36996,\"start\":36991},{\"end\":37454,\"start\":37450},{\"end\":37471,\"start\":37468},{\"end\":37483,\"start\":37481},{\"end\":37496,\"start\":37492},{\"end\":37817,\"start\":37813},{\"end\":37834,\"start\":37831},{\"end\":37847,\"start\":37843},{\"end\":38169,\"start\":38162},{\"end\":38511,\"start\":38505},{\"end\":38681,\"start\":38674},{\"end\":38693,\"start\":38688},{\"end\":38707,\"start\":38702},{\"end\":38996,\"start\":38994},{\"end\":39007,\"start\":39005},{\"end\":39021,\"start\":39018},{\"end\":39032,\"start\":39028},{\"end\":39395,\"start\":39393},{\"end\":39406,\"start\":39401},{\"end\":39420,\"start\":39417},{\"end\":39435,\"start\":39431},{\"end\":39780,\"start\":39775},{\"end\":39803,\"start\":39790},{\"end\":39820,\"start\":39815},{\"end\":40220,\"start\":40218},{\"end\":40229,\"start\":40225},{\"end\":40239,\"start\":40236},{\"end\":40625,\"start\":40623},{\"end\":40637,\"start\":40635},{\"end\":40652,\"start\":40647},{\"end\":40664,\"start\":40660},{\"end\":40676,\"start\":40673},{\"end\":40686,\"start\":40683},{\"end\":41077,\"start\":41072},{\"end\":41093,\"start\":41088},{\"end\":41109,\"start\":41104},{\"end\":41471,\"start\":41469},{\"end\":41480,\"start\":41477},{\"end\":41490,\"start\":41487},{\"end\":41505,\"start\":41501},{\"end\":41515,\"start\":41513},{\"end\":41529,\"start\":41524},{\"end\":42046,\"start\":42043},{\"end\":42061,\"start\":42058},{\"end\":42076,\"start\":42073},{\"end\":42496,\"start\":42488},{\"end\":42510,\"start\":42504},{\"end\":42514,\"start\":42512},{\"end\":42734,\"start\":42731},{\"end\":42748,\"start\":42745},{\"end\":42760,\"start\":42757},{\"end\":42774,\"start\":42771},{\"end\":42789,\"start\":42786},{\"end\":43355,\"start\":43349},{\"end\":43375,\"start\":43368},{\"end\":43386,\"start\":43383},{\"end\":43402,\"start\":43397},{\"end\":43920,\"start\":43914},{\"end\":43930,\"start\":43927},{\"end\":43944,\"start\":43937},{\"end\":43960,\"start\":43952},{\"end\":43972,\"start\":43967},{\"end\":43992,\"start\":43984},{\"end\":44009,\"start\":44003},{\"end\":44319,\"start\":44311},{\"end\":44333,\"start\":44328},{\"end\":44586,\"start\":44583},{\"end\":44598,\"start\":44595},{\"end\":44617,\"start\":44610},{\"end\":44634,\"start\":44630},{\"end\":45013,\"start\":45010},{\"end\":45029,\"start\":45020},{\"end\":45044,\"start\":45038},{\"end\":45059,\"start\":45055},{\"end\":45069,\"start\":45063},{\"end\":45081,\"start\":45075},{\"end\":45096,\"start\":45090},{\"end\":45112,\"start\":45104},{\"end\":45118,\"start\":45114},{\"end\":45667,\"start\":45660},{\"end\":45680,\"start\":45675},{\"end\":45694,\"start\":45689},{\"end\":46033,\"start\":46029},{\"end\":46040,\"start\":46038},{\"end\":46053,\"start\":46051},{\"end\":46064,\"start\":46062},{\"end\":46076,\"start\":46073},{\"end\":46087,\"start\":46083},{\"end\":46104,\"start\":46101},{\"end\":46113,\"start\":46109},{\"end\":46126,\"start\":46122},{\"end\":46418,\"start\":46412},{\"end\":46425,\"start\":46423},{\"end\":46437,\"start\":46431},{\"end\":46448,\"start\":46444},{\"end\":46459,\"start\":46454},{\"end\":46476,\"start\":46472},{\"end\":46712,\"start\":46710},{\"end\":46732,\"start\":46727},{\"end\":46747,\"start\":46742},{\"end\":46762,\"start\":46759},{\"end\":46775,\"start\":46771},{\"end\":47074,\"start\":47072},{\"end\":47086,\"start\":47083},{\"end\":47101,\"start\":47097},{\"end\":47110,\"start\":47108},{\"end\":47124,\"start\":47120},{\"end\":47138,\"start\":47134},{\"end\":47154,\"start\":47149},{\"end\":47436,\"start\":47431},{\"end\":47450,\"start\":47446},{\"end\":47465,\"start\":47458},{\"end\":47692,\"start\":47687},{\"end\":47706,\"start\":47698},{\"end\":47720,\"start\":47713},{\"end\":47911,\"start\":47906},{\"end\":47925,\"start\":47922},{\"end\":47937,\"start\":47935},{\"end\":47948,\"start\":47943},{\"end\":48314,\"start\":48309},{\"end\":48328,\"start\":48325},{\"end\":48339,\"start\":48334},{\"end\":48695,\"start\":48690},{\"end\":48709,\"start\":48706},{\"end\":48720,\"start\":48715},{\"end\":49084,\"start\":49079},{\"end\":49096,\"start\":49094},{\"end\":49104,\"start\":49102},{\"end\":49117,\"start\":49113},{\"end\":49131,\"start\":49126},{\"end\":49139,\"start\":49137},{\"end\":49557,\"start\":49552},{\"end\":49570,\"start\":49566},{\"end\":49579,\"start\":49575},{\"end\":49593,\"start\":49588},{\"end\":49601,\"start\":49599}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4493958},\"end\":34688,\"start\":34288},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4710341},\"end\":35148,\"start\":34690},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202577523},\"end\":35404,\"start\":35150},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5250573},\"end\":35756,\"start\":35406},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":51882734},\"end\":36054,\"start\":35758},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":207998010},\"end\":36474,\"start\":36056},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1475121},\"end\":36868,\"start\":36476},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":174788791},\"end\":37377,\"start\":36870},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18874645},\"end\":37742,\"start\":37379},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13271756},\"end\":38079,\"start\":37744},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":484327},\"end\":38450,\"start\":38081},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15697443},\"end\":38627,\"start\":38452},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6024639},\"end\":38928,\"start\":38629},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":102352104},\"end\":39311,\"start\":38930},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1663191},\"end\":39712,\"start\":39313},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3739626},\"end\":40179,\"start\":39714},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":140309863},\"end\":40550,\"start\":40181},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":67855809},\"end\":40999,\"start\":40552},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8282555},\"end\":41386,\"start\":41001},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220246167},\"end\":41963,\"start\":41388},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9971732},\"end\":42440,\"start\":41965},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b21\"},\"end\":42658,\"start\":42442},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6540453},\"end\":43199,\"start\":42660},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":64193},\"end\":43852,\"start\":43201},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8887614},\"end\":44265,\"start\":43854},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7044126},\"end\":44531,\"start\":44267},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13981820},\"end\":44892,\"start\":44533},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7037846},\"end\":45594,\"start\":44894},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":215825382},\"end\":45953,\"start\":45596},{\"attributes\":{\"doi\":\"abs/1809.00219\",\"id\":\"b29\",\"matched_paper_id\":52154773},\"end\":46354,\"start\":45955},{\"attributes\":{\"id\":\"b30\"},\"end\":46609,\"start\":46356},{\"attributes\":{\"doi\":\"abs/2004.06965\",\"id\":\"b31\",\"matched_paper_id\":215768718},\"end\":46996,\"start\":46611},{\"attributes\":{\"doi\":\"abs/1808.08718\",\"id\":\"b32\",\"matched_paper_id\":52098212},\"end\":47368,\"start\":46998},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2356330},\"end\":47618,\"start\":47370},{\"attributes\":{\"id\":\"b34\"},\"end\":47844,\"start\":47620},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1900475},\"end\":48219,\"start\":47846},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2141622},\"end\":48620,\"start\":48221},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":88524978},\"end\":48995,\"start\":48622},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":49657846},\"end\":49493,\"start\":48997},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3619954},\"end\":49960,\"start\":49495}]", "bib_title": "[{\"end\":34360,\"start\":34288},{\"end\":34770,\"start\":34690},{\"end\":35212,\"start\":35150},{\"end\":35490,\"start\":35406},{\"end\":35843,\"start\":35758},{\"end\":36120,\"start\":36056},{\"end\":36546,\"start\":36476},{\"end\":36934,\"start\":36870},{\"end\":37443,\"start\":37379},{\"end\":37806,\"start\":37744},{\"end\":38155,\"start\":38081},{\"end\":38496,\"start\":38452},{\"end\":38665,\"start\":38629},{\"end\":38985,\"start\":38930},{\"end\":39383,\"start\":39313},{\"end\":39764,\"start\":39714},{\"end\":40212,\"start\":40181},{\"end\":40614,\"start\":40552},{\"end\":41062,\"start\":41001},{\"end\":41457,\"start\":41388},{\"end\":42035,\"start\":41965},{\"end\":42725,\"start\":42660},{\"end\":43339,\"start\":43201},{\"end\":43905,\"start\":43854},{\"end\":44303,\"start\":44267},{\"end\":44573,\"start\":44533},{\"end\":45001,\"start\":44894},{\"end\":45652,\"start\":45596},{\"end\":46020,\"start\":45955},{\"end\":46699,\"start\":46611},{\"end\":47063,\"start\":46998},{\"end\":47423,\"start\":47370},{\"end\":47900,\"start\":47846},{\"end\":48303,\"start\":48221},{\"end\":48684,\"start\":48622},{\"end\":49071,\"start\":48997},{\"end\":49544,\"start\":49495}]", "bib_author": "[{\"end\":34381,\"start\":34362},{\"end\":34395,\"start\":34381},{\"end\":34785,\"start\":34772},{\"end\":34800,\"start\":34785},{\"end\":34815,\"start\":34800},{\"end\":35233,\"start\":35214},{\"end\":35248,\"start\":35233},{\"end\":35262,\"start\":35248},{\"end\":35510,\"start\":35492},{\"end\":35523,\"start\":35510},{\"end\":35544,\"start\":35523},{\"end\":35569,\"start\":35544},{\"end\":35859,\"start\":35845},{\"end\":35870,\"start\":35859},{\"end\":35894,\"start\":35870},{\"end\":36151,\"start\":36122},{\"end\":36166,\"start\":36151},{\"end\":36178,\"start\":36166},{\"end\":36207,\"start\":36178},{\"end\":36217,\"start\":36207},{\"end\":36564,\"start\":36548},{\"end\":36580,\"start\":36564},{\"end\":36600,\"start\":36580},{\"end\":36620,\"start\":36600},{\"end\":36945,\"start\":36936},{\"end\":36958,\"start\":36945},{\"end\":36974,\"start\":36958},{\"end\":36987,\"start\":36974},{\"end\":36998,\"start\":36987},{\"end\":37456,\"start\":37445},{\"end\":37473,\"start\":37456},{\"end\":37485,\"start\":37473},{\"end\":37498,\"start\":37485},{\"end\":37819,\"start\":37808},{\"end\":37836,\"start\":37819},{\"end\":37849,\"start\":37836},{\"end\":38171,\"start\":38157},{\"end\":38513,\"start\":38498},{\"end\":38683,\"start\":38667},{\"end\":38695,\"start\":38683},{\"end\":38709,\"start\":38695},{\"end\":38998,\"start\":38987},{\"end\":39009,\"start\":38998},{\"end\":39023,\"start\":39009},{\"end\":39034,\"start\":39023},{\"end\":39397,\"start\":39385},{\"end\":39408,\"start\":39397},{\"end\":39422,\"start\":39408},{\"end\":39437,\"start\":39422},{\"end\":39782,\"start\":39766},{\"end\":39805,\"start\":39782},{\"end\":39822,\"start\":39805},{\"end\":40222,\"start\":40214},{\"end\":40231,\"start\":40222},{\"end\":40241,\"start\":40231},{\"end\":40627,\"start\":40616},{\"end\":40639,\"start\":40627},{\"end\":40654,\"start\":40639},{\"end\":40666,\"start\":40654},{\"end\":40678,\"start\":40666},{\"end\":40688,\"start\":40678},{\"end\":41079,\"start\":41064},{\"end\":41095,\"start\":41079},{\"end\":41111,\"start\":41095},{\"end\":41473,\"start\":41459},{\"end\":41482,\"start\":41473},{\"end\":41492,\"start\":41482},{\"end\":41507,\"start\":41492},{\"end\":41517,\"start\":41507},{\"end\":41531,\"start\":41517},{\"end\":42048,\"start\":42037},{\"end\":42063,\"start\":42048},{\"end\":42078,\"start\":42063},{\"end\":42498,\"start\":42486},{\"end\":42512,\"start\":42498},{\"end\":42516,\"start\":42512},{\"end\":42736,\"start\":42727},{\"end\":42750,\"start\":42736},{\"end\":42762,\"start\":42750},{\"end\":42776,\"start\":42762},{\"end\":42791,\"start\":42776},{\"end\":43357,\"start\":43341},{\"end\":43377,\"start\":43357},{\"end\":43388,\"start\":43377},{\"end\":43404,\"start\":43388},{\"end\":43922,\"start\":43907},{\"end\":43932,\"start\":43922},{\"end\":43946,\"start\":43932},{\"end\":43962,\"start\":43946},{\"end\":43974,\"start\":43962},{\"end\":43994,\"start\":43974},{\"end\":44011,\"start\":43994},{\"end\":44321,\"start\":44305},{\"end\":44335,\"start\":44321},{\"end\":44588,\"start\":44575},{\"end\":44600,\"start\":44588},{\"end\":44619,\"start\":44600},{\"end\":44636,\"start\":44619},{\"end\":45015,\"start\":45003},{\"end\":45031,\"start\":45015},{\"end\":45046,\"start\":45031},{\"end\":45061,\"start\":45046},{\"end\":45071,\"start\":45061},{\"end\":45083,\"start\":45071},{\"end\":45098,\"start\":45083},{\"end\":45114,\"start\":45098},{\"end\":45120,\"start\":45114},{\"end\":45669,\"start\":45654},{\"end\":45682,\"start\":45669},{\"end\":45696,\"start\":45682},{\"end\":46035,\"start\":46022},{\"end\":46042,\"start\":46035},{\"end\":46055,\"start\":46042},{\"end\":46066,\"start\":46055},{\"end\":46078,\"start\":46066},{\"end\":46089,\"start\":46078},{\"end\":46106,\"start\":46089},{\"end\":46115,\"start\":46106},{\"end\":46128,\"start\":46115},{\"end\":46420,\"start\":46407},{\"end\":46427,\"start\":46420},{\"end\":46439,\"start\":46427},{\"end\":46450,\"start\":46439},{\"end\":46461,\"start\":46450},{\"end\":46478,\"start\":46461},{\"end\":46714,\"start\":46701},{\"end\":46734,\"start\":46714},{\"end\":46749,\"start\":46734},{\"end\":46764,\"start\":46749},{\"end\":46777,\"start\":46764},{\"end\":47076,\"start\":47065},{\"end\":47088,\"start\":47076},{\"end\":47103,\"start\":47088},{\"end\":47112,\"start\":47103},{\"end\":47126,\"start\":47112},{\"end\":47140,\"start\":47126},{\"end\":47156,\"start\":47140},{\"end\":47438,\"start\":47425},{\"end\":47452,\"start\":47438},{\"end\":47467,\"start\":47452},{\"end\":47694,\"start\":47683},{\"end\":47708,\"start\":47694},{\"end\":47722,\"start\":47708},{\"end\":47913,\"start\":47902},{\"end\":47927,\"start\":47913},{\"end\":47939,\"start\":47927},{\"end\":47950,\"start\":47939},{\"end\":48316,\"start\":48305},{\"end\":48330,\"start\":48316},{\"end\":48341,\"start\":48330},{\"end\":48697,\"start\":48686},{\"end\":48711,\"start\":48697},{\"end\":48722,\"start\":48711},{\"end\":49086,\"start\":49073},{\"end\":49098,\"start\":49086},{\"end\":49106,\"start\":49098},{\"end\":49119,\"start\":49106},{\"end\":49133,\"start\":49119},{\"end\":49141,\"start\":49133},{\"end\":49559,\"start\":49546},{\"end\":49572,\"start\":49559},{\"end\":49581,\"start\":49572},{\"end\":49595,\"start\":49581},{\"end\":49603,\"start\":49595}]", "bib_venue": "[{\"end\":34930,\"start\":34881},{\"end\":37139,\"start\":37077},{\"end\":39963,\"start\":39901},{\"end\":40382,\"start\":40320},{\"end\":41700,\"start\":41624},{\"end\":42219,\"start\":42157},{\"end\":42952,\"start\":42880},{\"end\":43540,\"start\":43479},{\"end\":45261,\"start\":45199},{\"end\":49256,\"start\":49207},{\"end\":49744,\"start\":49682},{\"end\":34471,\"start\":34395},{\"end\":34879,\"start\":34815},{\"end\":35269,\"start\":35262},{\"end\":35573,\"start\":35569},{\"end\":35898,\"start\":35894},{\"end\":36251,\"start\":36217},{\"end\":36657,\"start\":36620},{\"end\":37075,\"start\":36998},{\"end\":37536,\"start\":37498},{\"end\":37887,\"start\":37849},{\"end\":38247,\"start\":38171},{\"end\":38529,\"start\":38513},{\"end\":38762,\"start\":38709},{\"end\":39103,\"start\":39034},{\"end\":39495,\"start\":39437},{\"end\":39899,\"start\":39822},{\"end\":40318,\"start\":40241},{\"end\":40757,\"start\":40688},{\"end\":41176,\"start\":41111},{\"end\":41622,\"start\":41531},{\"end\":42155,\"start\":42078},{\"end\":42484,\"start\":42442},{\"end\":42878,\"start\":42791},{\"end\":43477,\"start\":43404},{\"end\":44044,\"start\":44011},{\"end\":44383,\"start\":44335},{\"end\":44698,\"start\":44636},{\"end\":45197,\"start\":45120},{\"end\":45765,\"start\":45696},{\"end\":46147,\"start\":46142},{\"end\":46405,\"start\":46356},{\"end\":46796,\"start\":46791},{\"end\":47175,\"start\":47170},{\"end\":47486,\"start\":47467},{\"end\":47681,\"start\":47620},{\"end\":48015,\"start\":47950},{\"end\":48403,\"start\":48341},{\"end\":48791,\"start\":48722},{\"end\":49205,\"start\":49141},{\"end\":49680,\"start\":49603}]"}}}, "year": 2023, "month": 12, "day": 17}