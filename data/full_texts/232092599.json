{"id": 232092599, "updated": "2023-10-06 05:45:34.086", "metadata": {"title": "Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection", "authors": "[{\"first\":\"Lara\",\"last\":\"Grimminger\",\"middle\":[]},{\"first\":\"Roman\",\"last\":\"Klinger\",\"middle\":[]}]", "venue": "WASSA", "journal": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. We investigate in this paper if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. We formulate an annotation task, in which we join the tasks of hateful/offensive speech detection and stance detection, and annotate 3000 Tweets from the campaign period, if they express a particular stance towards a candidate. Next to the established classes of favorable and against, we add mixed and neutral stances and also annotate if a candidate is mentioned with- out an opinion expression. Further, we an- notate if the tweet is written in an offensive style. This enables us to analyze if supporters of Joe Biden and the Democratic Party communicate differently than supporters of Donald Trump and the Republican Party. A BERT baseline classifier shows that the detection if somebody is a supporter of a candidate can be performed with high quality (.89 F1 for Trump and .91 F1 for Biden), while the detection that somebody expresses to be against a candidate is more challenging (.79 F1 and .64 F1, respectively). The automatic detection of hate/offensive speech remains challenging (with .53 F1). Our corpus is publicly available and constitutes a novel resource for computational modelling of offensive language under consideration of stances.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.01664", "mag": null, "acl": "2021.wassa-1.18", "pubmed": null, "pubmedcentral": null, "dblp": "conf/wassa/GrimmingerK21", "doi": null}}, "content": {"source": {"pdf_hash": "6ca1b923b6d5639e2866040443320b6daeb21ebf", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2021.wassa-1.18.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1c6b7c984c0e9c5004d618ac22e9cbee731183e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6ca1b923b6d5639e2866040443320b6daeb21ebf.txt", "contents": "\nHate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection\nApril 19, 2021\n\nLara Grimminger lara.grimminger@ims.uni-stuttgart.de \nInstitut f\u00fcr Maschinelle Sprachverarbeitung\nUniversity of Stuttgart\nPfaffenwaldring 5b70569StuttgartGermany\n\nRoman Klinger roman.klinger@ims.uni-stuttgart.de \nInstitut f\u00fcr Maschinelle Sprachverarbeitung\nUniversity of Stuttgart\nPfaffenwaldring 5b70569StuttgartGermany\n\nHate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection\n\nProceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis\nthe 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media AnalysisApril 19, 2021171\nThe 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. We investigate in this paper if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. We formulate an annotation task, in which we join the tasks of hateful/offensive speech detection and stance detection, and annotate 3000 Tweets from the campaign period, if they express a particular stance towards a candidate. Next to the established classes of favorable and against, we add mixed and neutral stances and also annotate if a candidate is mentioned without an opinion expression. Further, we annotate if the tweet is written in an offensive style. This enables us to analyze if supporters of Joe Biden and the Democratic Party communicate differently than supporters of Donald Trump and the Republican Party. A BERT baseline classifier shows that the detection if somebody is a supporter of a candidate can be performed with high quality (.89 F 1 for Trump and .91 F 1 for Biden), while the detection that somebody expresses to be against a candidate is more challenging (.79 F 1 and .64 F 1 , respectively). The automatic detection of hate/offensive speech remains challenging (with .53 F 1 ). Our corpus is publicly available and constitutes a novel resource for computational modelling of offensive language under consideration of stances.\n\nIntroduction\n\nSocial media are indispensable to political campaigns ever since Barack Obama used them so successfully in 2008 (Tumasjan et al., 2010). Twitter in particular is a much-frequented form of communication with monthly 330 million active users This paper contains offensive language. (Clement, 2019). The microblogging platform was credited to have played a key role in Donald Trump's rise to power (Stolee and Caton, 2018). As Twitter enables users to express their opinions about topics and targets, the insights gained from detecting stance in political tweets can help monitor the voting base.\n\nIn addition to the heated election of Trump in 2016, the world has also seen an increase of hate speech (Gao and Huang, 2017). Defined as \"any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic\" (Nockelby, 2000), hate speech is considered \"a particular form of offensive language\" (Warner and Hirschberg, 2012). However, some authors also conflate hateful and offensive speech and define hate speech as explicitly or implicitly degrading a person or group (Gao and Huang, 2017). Over the years, the use of hate speech in social media has increased (de Gibert et al., 2018). Consequently, there is a growing need for approaches that detect hate speech automatically (Gao and Huang, 2017).\n\nFrom the perspective of natural language processing (NLP), the combination of political stance and hate speech detection provides promising classification tasks, namely determining the attitude a text displays towards a pre-determined target and the presence of hateful and offensive speech. In contrast to prior work on stance detection (Somasundaran and Wiebe, 2010; Mohammad et al., 2016, i.a.), we not only annotate if a text is favorable, against or does not mention the target at all (neither), but include whether the text of the tweet displays a mixed (both favorable and against) or neutral stance towards the targets. With this formulation we are also able to mark tweets that mention a target without taking a clear stance. To annotate hateful and offensive tweets, we follow the def-inition of Gao and Huang (2017) and adapt our guidelines to political discourse.\n\nOur contributions are the following:\n\n\u2022 We publish a Twitter-corpus that is annotated both for stance and hate speech detection. We make this corpus of 3000 Tweets publicly available at https://www.ims.uni-stuttgart.de/ data/stance hof us2020.\n\n\u2022 Based on a manual analysis of these annotations, our results suggest that Tweets that express a stance against Biden contain more hate speech than those against Trump.\n\n\u2022 Our baseline classification experiments show that the detection of the stance that somebody is in-favor of a candidate performs better than that somebody is against a candidate. Further, the detection of hate/offensive speech on this corpus remains challenging.\n\n2 Related Work\n\n\nHate Speech and Offensive Language\n\nIn early work on hate speech detection, Spertus (1997) described various approaches to detect abusive and hostile messages occurring during online communication. More recent work also considered cyberbullying (Dinakar et al., 2012) and focused on the use of stereotypes in harmful messages (Warner and Hirschberg, 2012). Most of the existing hate speech detection models are supervised learning approaches. Davidson et al. (2017) created a data set by collecting tweets that contained hate speech keywords from a crowd-sourced hate speech lexicon. They then categorized these tweets into hate speech, offensive language, and neither. Mandl et al. (2019) sampled their data from Twitter and partially from Facebook and experimented with binary as well as more fine-grained multi-class classifications. Their results suggest that systems based on deep neural networks performed best. Waseem and Hovy (2016) used a feature-based approach to explore several feature types. Burnap and Williams (2014) collected hateful tweets related to the murder of Drummer Lee Rigby in 2013. The authors examined different classification methods with various features including n-grams, restricted n-grams, typed dependencies, and hateful terms. Schmidt and Wiegand (2017) outlined that the lack of a benchmark data set based on a commonly accepted definition of hate speech is challenging. Ro\u00df et al. (2016) found that there is low agreement among users when identifying hateful messages.\n\nFor the SemEval 2019 Task 5, Basile et al. (2019) proposed two hate speech detection tasks on Spanish and English tweets which contained hateful messages against women and immigrants. Next to a binary classification, participating systems had to extract further features in harmful messages such as target identification. None of the submissions for the more fine-grained classification task in English could outperform the baseline of the task organizers. In case of Spanish, the best results were achieved by a linear-kernel SVM. The authors found that it was harder to detect further features than the presence of hate speech. The recent shared task on offensive language identification organized by Zampieri et al. (2020) was featured in five languages. For a more detailed overview, we refer to the surveys by Mladenovi\u0107 et al. (2021); Fortuna and Nunes (2018); Schmidt and Wiegand (2017).\n\nIn contrast to this previous work, we provide data for a specific recent use case, and predefine two targets of interest to be analyzed.\n\n\nStance Detection\n\nRelated work on stance detection includes stance detection on congressional debates (Thomas et al., 2006), online forums (Somasundaran and Wiebe, 2010), Twitter (Mohammad et al., 2016(Mohammad et al., , 2017Aker et al., 2017;K\u00fc\u00e7\u00fck and Can, 2018;Lozhnikov et al., 2020) and comments on news (Lozhnikov et al., 2020). Thomas et al. (2006) used a corpus of speeches from the US Congress and modeled their support/oppose towards a proposed legislation task. Somasundaran and Wiebe (2010) conducted experiments with sentiment and arguing expressions and used features based on modal verbs and sentiments for stance classification. For the SemEval 2016 Task 6 organized by Mohammad et al. (2016), stance was detected from tweets. The task contained two stance detection subtasks for supervised and weakly supervised settings. In both classification tasks, tweet-target pairs needed to be classified as either Favor, Against or Neither. The baseline of the task organizers outperformed all systems' results that were submitted by task participants.\n\nIn hope that sentiment features would have the same effect on stance detection as they have on sentiment prediction, Mohammad et al. (2017) concurrently annotated a set of tweets for both stance and sentiment. Although sentiment labels proved to be beneficial for stance detection, they were not sufficient. Instead of a target-specific stance classification, Aker et al. (2017) described an open stance classification approach to identify rumors on Twitter. The authors experimented with different classifiers and task-specific features which measured the level of confidence in a tweet. With the additional features, their approach outperformed state-of-the-art results on two benchmark sets.\n\nIn addition to this previous work, we opted for a more fine-grained stance detection and not only annotated favor, against and neither towards a target but also whether the stance of the text was mixed or neutral. Further, we combine stance detection with hate/offensive speech detection.\n\n\nCorpus\n\n\nData Collection\n\nOur goal is on the one side to create a new Twitter data set that combines stance and hate/offensive speech detection in the political domain. On the other side, we create this corpus to investigate the question how hate/offensive speech is distributed among different stances.\n\nWe used the Twitter API v 1.1. to fetch tweets for 6 weeks leading to the presidential election, on the election day and for 1 week after the election. As search terms, we use the mention of the presidential and vice presidential candidates and the outsider West; the mention of hashtags that show a voter's alignment such as the campaign slogans of the candidate websites, and further nicknames of the candidates. The list of search terms is: #Trump2020, #TrumpPence2020, #Biden2020, #BidenHarris2020, #Kanye2020, #MAGA2020, #BattleForTheSoulOfTheNation, #2020Vision, #VoteRed2020, #VoteBlue2020, \n\n\nAnnotation\n\n\nAnnotation Task\n\nFrom the 382.210 tweets, we sampled 3000 tweets for annotation. Given the text of a tweet, we rated the stance towards the targets Trump, Biden, and West in the text. The detected stance can be from one of the following labels:\n\n\u2022 Favor: Text argues in favor of the target The default value shown in the annotation environment is Neither.\n\nThe text was further annotated as being hateful and non-hateful. We did not separate if a group or a single person was targeted by hateful language. Further, we adapted the guidelines on hate speech annotation to be able to react to namecalling and down talking of the political opponent. Thus, we rated expressions such as \"Dementia Joe\" and \"DonTheCon\" as hateful/offensive (HOF).\n\n\nAnnotation Procedure\n\nTo evaluate the annotation guidelines (which we make available together with the data) we perform multiple annotation iterations with three annotators. Annotator 1 is a 22 year old male undergraduate student of computational linguistics who speaks German, English, Catalan, and Spanish. Annotator 2 is a 26 year old female undergraduate student of computational linguistics who speaks German  and English. Annotator 3 is a 29 year old female graduate student of computational linguistics who speaks German and English. Annotator 1 and 2 annotated 300 tweets in three iterations with 100 tweets per iteration. After each iteration, the annotators discussed the tweets they rated differently and complemented the existing guidelines. Finally, Annotator 2 and 3 annotated 100 tweets with the improved guidelines to check whether the rules are clear and understandable, especially if read for the first time. Table 1 shows the result of Cohen's \u03ba of each iteration. In the first iteration, the agreement for HOF is purely random (\u22120.02\u03ba), the stance annotations show acceptable agreement (.83, .81\u03ba, respectively for Trump and Biden). West has not been mentioned in any of the 100 tweets. In a group discussion to identify the reasons for the substantial lack of agreement for HOF, we developed guidelines which described hateful and offensive speech in more detail and added further examples to our guidelines. We particularly stressed to annotate name-calling as hateful and offensive. This showed success in a second iteration with .42\u03ba for HOF agreement. The scores for Trump and Biden decreased slightly but still represented substantial agreement. We carried out another group discussion to discuss tweets where Annotator 1 and 2 chose different classes. We particularly refined the guidelines for class Neutral mentions and included offensive and hateful abbreviations such as \"POS\" (\"piece of shit\") and \"BS\" (\"bullshit\") which have been missed before. This led to a HOF agreement of .73\u03ba, while the stance agreement remained on similar levels (.81, .88).\n\nAs a concluding step, Annotator 2 and 3 rated 100 tweets. The annotators were provided with the guidelines established during the iterations between Annotator 1 and 2.  \n\n\nResults\n\n\nCorpus Statistics\n\nWe now analyze the corpus for the targets Trump and Biden to answer the question if supporters of Trump (and Pence) use more hateful and offensive speech than supporters of Biden (and Harris). Tables 2 and 3 show the distribution of the classes Favor, Against, Neither, Mixed, Neutral mentions and how often each class was labeled as HOF or Non-HOF (\u00acHOF). The data set is unbalanced: only 11.7% of the tweets are hateful/offensive. Furthermore, there are more tweets labeled as Favor, Against, and Neither than Mixed, or Neutral mentions for target Trump. In case of target Biden, more tweets are labeled as Favor and as Neither than as Against, Mixed, or Neutral mentions. In total, there were only 9 tweets about Kanye West in the annotated data set, which is why we do not present statistics about him.\n\nDid Trump supporters use more hateful and offensive speech than supporters of Biden? A comparison of Tables 2 and 3 suggests that supporters of team Trump use slightly more often harmful and offensive speech with 12.9% than supporters of team Biden, with 11.4%. This indicates that Trump supporters use more hateful speech than supporters of Biden, yet, this difference is only minor. This is arguable a result of the aspect that HOF is also often expressed without naming the target explicitly. Furthermore, given the fact that we added offensive nicknames such as \"Sleepy Joe\" to our search terms, this result is biased.\n\nBy means of pointwise mutual information we identified the top 10 words that are unlikely to  occur in a tweet labeled as Non-Hateful. As Table 4 shows, these words are offensive and promote hate. This list also mirrors the limitations of our search terms as the adjective \"sleepy\" is part of the top 10.\n\nLikewise, we identified the top 10 words that are unlikely to appear in a tweet labeled as Favor towards Trump and thus, argue against him. Next to hashtags that express a political preference for Biden, the top 10 list contains words that refer to Trump's taxes and a demand to vote. Similarly, the top 10 words that are unlikely to occur in a tweet labeled as Favoring Biden and therefore express the stance Against him, consist of adjectives Trump mocked him with (creepy, sleepy) as well as a reference to his son Hunter.\n\nWho is more targeted by hateful and offensive speech, Biden and the Democratic party or Trump and the Republican Party? We note that 26.7% of the tweets against target Biden contain hateful/offensive language, whereas only 18.5% of the tweets against target Trump are hateful/offensive. Thus, our results suggest that Biden and the Democratic Party are more often targets of hateful and offensive tweets than Trump and the Republican Party.\n\nHowever, from this analysis we cannot draw that the offensive stems from supporters of the other party. Due to the limitations in our search terms we also note that there might be an unknown correlation of the search terms to HOF which we cannot entirely avoid. Further, these results should be interpreted with a grain of salt, given that the sampling procedure of the Twitter API is not entirely transparent. \n\n\nStance Classification Experiments\n\nNext to the goal to better understand the distribution of hate/offensive speech during the election in 2020, the data set constitutes an interesting resource valuable for the development of automatic detection systems. To support such development, we provide results of a baseline classifier. We used the pretrained BERT base model 1 (Devlin et al., 2019) and its TensorFlow implementation provided by HuggingFace 2 (Wolf et al., 2020). Our data set was divided into 80% for training and 20% for testing.\n\nEach model was trained with a batch size of 16, a learning rate (Adam) of 5 \u00b7 10 \u22125 , a decay of 0.01, a maximal sentence length of 100 and a validation split of 0.2. Further, we set the number of epochs to 10 and saved the best model on the validation set for testing. Table 5 shows the results for stance detection prediction. We observe that not all classes can be predicted equally well. The two best predicted classes for Trump are Neither and Favor with a F 1 score of 0.95 and 0.89, respectively. These scores are followed by class Against with a F 1 score of 0.79. However, our model had difficulties to correctly predict the class Neutral, with a more limited precision and recall (.58 and .49). The class Mixed could not be predicted at all.\n\nThese results only partially resemble for the target Biden: The classes Neither and Favor have the highest F 1 score with 0.96 and 0.91, respectively. In contrast to target Trump, the performance of our model to predict the class Against is much lower (.64 F 1 ). The F 1 score of class Neutral is low again with .59 and class Mixed could not be predicted. We conclude that stance can be detected from tweets. Yet, our results suggest that it  is more challenging to predict fine-grained stance classes such as Mixed and Neutral mentions than the classes Favor, Against and Neither. This result is, at least partially, a consequence of the data distribution. The Mixed label has very few instances (20+47); the Neutral label is the second most seldomly annotated class, though it is substantially more frequent (341+326).\n\n\nCross-Corpus Hate Speech Detection Experiments\n\nSimilar to the stance detection baseline results, we now report results of a classifier (configured the same as the one in Section 4.2). To obtain an understanding how challenging the prediction is on our corpus, and how different the concept of hate/offensive speech is from existing resources, we perform this analysis across a set of corpora, as well as inside of each corpus.\n\nTo that end, we chose the following hate speech/offensive speech corpora:\n\n1. Data Set 1 by Davidson et al. (2017).\n\nThis corpus contains 24.783 tweets, categorized into into hateful, offensive, and neither. In our study, we we only use two classes, hateful/offensive and non-hateful. Therefore, we conflate the two classes, hateful and offensive, into one. We randomly split their data, available at https://github.com/t-davidson/ hate-speech-and-offensive-language, into 80% for training and 20% for testing.\n\n2. Data Set 2 by Mandl et al. (2019). In this study, the authors conducted three classification experiments including a binary one, where 5.852 posts from Twitter and Facebook were classified into hate speech and non-offensive (Sub-task A). From their multi-lingual resource, we only need the English subset. We use the training data available at https://hasocfire.github.io/hasoc/2019/ dataset.html and perform a 80/20% train/test split. Table 6 shows the results for all combinations of training on the data by Davidson et al. (2017), Mandl et al. (2019), and ours (presented in this paper). When we only look at the results of the model when trained and tested on subcorpora from the same original source, we observe that there are some noteworthy differences. The recognition of HOF on the data by Davidson et al. (2017) shows a high .98 F 1 (note that this result cannot be compared to their original results, because we conflate two classes). Training and testing our baseline model on the data by Mandl et al. (2019) shows .56 F 1 but with a particularly limited recall. On our corpus, given that it is the smallest one, the model performs still comparably well with .53 F 1 . Precision and recall values are more balanced for the other corpora than for Mandl et al. (2019). Note that these results are comparably low in comparison to other previously published classification approaches. However, they allow for a comparison of the performances between the different corpora. We particularly observe that the data set size seems to have an impact on the predictive performance.\n\nWhen we move to a comparison of models trained on one corpus and tested on another, we see that the subcorpora created for a binary classifica- Target Pred Gold Text   Trump A  A  TWO HUNDRED THOUSAND PEOPLE HAVE DIED OF #COVID19 UN-DER Trump's WATCH. IT DID NOT HAVE TO BE LIKE THIS. #BidenHar-ris2020 will take steps to make us safe. Trump is happy to let us burn, and so he is a #weakloser #VoteBidenHarrisToSaveAmerica #RepublicansForBiden  tion experiment yield better results. The imbalance of labels caused by the conflation of two classes on the data by Davidson et al. (2017) led to weak predictions on the other subcorpora. Therefore, we conclude that the concept of hate/offensive speech between these different resources is not fully comparable, be it due to different instances, settings or annotators. The development of models that generalize across domains, corpora, and annotation guidelines is challenging.\n\n\nAnalysis\n\nWe now take a closer look at the tweets and their predicted classes to explore why tweets have been misclassified. We show examples in Table 7 for stance classification.\n\nOur model performed well when predicting the class Favor for both targets. The examples in Table 7 show a common pattern, namely that tweets being in favor of the respective target praise target's achievements and contain words of support such as builds, vote and right choice. Additionally, users often complement their tweets with target-related hashtags, including Trump2020, Trump2020LandslideVictory and Biden/Harris to stress their political preference. However, these hashtags can be misleading as they not always express support of the candidate. The 4th example contains the hashtag #Trump2020 and was therefore predicted to be in favor of Trump, while it actually argues against him. In the 5th example, the irony expressed by the quotation marks placed around the word science and the offensive expression BS for \"bullshit\" were not detected.\n\nSupporters of both candidates verbally attack each other over who to vote for and use hashtags and expressions to make the opposite side look poorly. Looking at tweets incorrectly labeled as Against, we see that in case of target Trump the string of insults addressing Biden and Harris possi- The democrats are literally the nazis. If they pack the courts and pass the 25th amendment Joe Biden and Kamala Harris will be in the exact same place that hindenburg and hitler were in. The 25th amendment is almost the same exact law hitler got passed in order to take power. bly confused our baseline and led to a misclassification of the tweet. Turning to Biden, the sentence Joe aka 46 was not detected to be positive and supportive.\n\nWe also show a set of examples for hate/offensive speech detection in Table 8. As the first tweet exemplifies, tweets correctly predicted as HOF often contain one or more hate and offensive key words, e.g. Creepy Joe. The first example also wishes Joe Biden to fall ill with Covid-19.\n\nHowever, although the 2nd example seems to contain offensive words such as \"badass\" and \"Suckit\", it is not meant in a hateful way. On the contrary, this tweet uses slang to express admiration and support.\n\nThe 3rd example clearly is hateful, comparing the Democratic Party to the Nazis and the position of Biden and Harris to Hindenburg and Hitler. However, apparently the word Nazis is not sufficient to communicate hate speech, while the other signals in this tweet are presumably infrequent in the corpus as well. These are interesting examples which show that hate/offensive speech detection requires at times world knowledge and common-sense reasoning (which BERT is arguable only capable of to a very limited extent).\n\n\nDiscussion\n\nThe results in Table 5 show that the disproportion among the classes Against, Favor, Neither, Mixed and Neutral mentions seen in Tables 2 and 3 are presumably influencing the performance. The classes Mixed and Neutral mentions contain less tweets than the other classes. Consequently, the model did not have the same amount of training data for these two classes and tweets that should be categorized as Neither or Neutral were misclassified. In addition to Mixed and Neutral mentions, the class Against of target Biden is also outweighed by the dominant classes Favor and Neither (see Table 3).\n\nWhen looking at the distribution of hateful and offensive and non-hateful tweets, we see that our data set contains more non-hateful tweets. As a result, the classification is biased. While Davidson et al. (2017) created their data set with keywords from a hate speech lexicon and Mandl et al. (2019) sampled their data with hashtags and keywords for which hate speech can be expected, our data was collected by using, but not limited to, offensive and hateful mentions. Thus, our hate speech data is more imbalanced but provides interesting insights into how people talk politics on Twitter. We assume that our corpus exhibits a more realistic distribution of hate/offensive speech for a particular topic than a subset of already existing resources.\n\nThere may be some possible limitations in this study. Using Twitter as data source provides challenges, because tweets contain noise, spelling mistakes and incomplete sentences. Further, the specified search criteria mentioned above might have had an effect on the results. Next to the nicknames Trump uses for his opponents, most of the keywords used to collect tweets refer to political candidates. Mentions of the respective political parties such as \"Democrats\", \"Republicans\" etc. were not included in the search. Yet, during the annotation we realized that it was not possible to differentiate the candidates from their respective parties. Hence, tweets were annotated for political parties and candidates inferring from hashtags such as \"#VoteBlue2020\" that the tweeter argues in favor of Joe Biden.\n\n\nConclusion and Future Work\n\nIn this paper, we have investigated stance detection on political tweets and whether or not supporters of Trump use more hate speech than supporters of Biden (not significantly). We found that manual annotation is possible with acceptable agreement scores, and that automatic stance detection towards political candidates and parties is possible with good performance.\n\nThe limitations of this study are twofold -on the one side, future work might want to consider to add the nicknames of all main candidates and explicitly include social media posts about the party, not only about the candidate, as we found a separation is often difficult. Further, we did not perform extensive hyperparameter optimization in our neural approach.\n\nWe suggest that future work invests in developing computational models that work across corpora and are able to adapt to domain and time-specific as well as societal and situational expressions of hate and offensive language. This is required, as our corpus shows that some references to offensive content are realized by domain-specific and societal expressions. This might be realized by combining offensive language detection and stance detection in a joint multi-task learning approach, potentially including other aspects like personality traits or specific emotions. We assume that such concepts can benefit from representations in joint models.\n\n\u2022\nAgainst: Text argues against the target \u2022 Neither: Target is not mentioned; neither implicitly nor explicitly \u2022 Mixed: Text mentions positive as well as negative aspects about the target \u2022 Neutral: Text states facts or recites quotes; unclear, whether text holds any position towards the target.\n\n\nTrump, Pence, Biden, Harris, Kanye, President, Sleepy Joe, Slow Joe, Phony Kamala, Monster Kamala. After removing duplicate tweets, the final corpus consists of 382.210 tweets. From these, there are 220.941 that contain Trump related hashtags and mentions, 230.629 tweets that carry hashtags and mentions associated with Biden and 1.412 tweets with hashtags and mentions related to Kanye West.Stance \n\nIteration Trump Biden West \nHOF \n\n1 A1+A2 \n0.83 \n0.81 0.00 \u22120.02 \n2 A1+A2 \n0.78 \n0.78 0.75 \n0.42 \n3 A1+A2 \n0.81 \n0.88 0.00 \n0.73 \n4 A2+A3 \n0.61 \n0.76 0.75 \n0.62 \n\nTable 1: Cohen's \u03ba for stance and hate/offensive speech \n(HOF). \n\n\n\nTable 2 :\n2Distribution of tweets about target Trump\n\n\nTable 1shows that the inter-annotator agreement for HOF is 0.62, for tar-Class \nHOF \u00acHOF \n%HOF \n\nFavor \n141 \n1095 1236 \n11.4 \nAgainst \n108 \n296 \n404 \n26.7 \nNeither \n87 \n900 \n987 \n8.8 \nMixed \n6 \n41 \n47 \n12.8 \nNeutral \n10 \n316 \n326 \n3.1 \n\nTotal \n352 \n2648 3000 \n11.7 \n\n\n\nTable 3 :\n3Distribution of tweets about target Bidenget Trump 0.61, for target Biden 0.76 and for target \nWest 0.75. These scores indicate substantial agree-\nment between Annotator 2 and 3 based on com-\nprehensive guidelines. The final annotation of the \noverall data set has been performed by Annotator 2. \n\n\n\nTable 4 :\n4Results of the pointwise mutual information calculation\n\nTable 5 :\n5Precision, Recall, and F 1 of stance detection baseline for targets Trump and BidenTarget Trump \nTarget Biden \n\nClass \nP \nR \nF 1 \nP \nR \nF 1 \n\nAgainst .77 .81 .79 .67 .62 .64 \nFavor \n.88 .90 .89 .90 .93 .91 \nMixed \n.00 .00 .00 .00 .00 .00 \nNeither .95 .95 .95 .93 .99 .96 \nNeutral .58 .49 .53 .59 .58 .59 \n\n\n\nTable 6 :\n6F1 scores of the hate speech detection baseline model trained and tested on different corpora\n\nTable 7 :\n7Examples of correct and incorrect predictions of favor (F) and against (A) stance in the tweets.\n\nTable 8 :\n8Examples of correct and incorrect predictions for hateful and offensive speech in the tweets.\nhttps://huggingface.co/bert-base-uncased 2 https://github.com/huggingface/transformers\nAcknowledgementsThis project has been partially funded by Deutsche Forschungsgemeinschaft (projects SEAT, KL 2869/1-1 and CEAT, KL 2869/1-2). We thank Anne Kreuter and Miquel Luj\u00e1n for fruitful discussions.\nSimple open stance classification for rumour analysis. Ahmet Aker, Leon Derczynski, Kalina Bontcheva, 10.26615/978-954-452-049-6_005Proceedings of the International Conference Recent Advances in Natural Language Processing. the International Conference Recent Advances in Natural Language ProcessingVarna, BulgariaAhmet Aker, Leon Derczynski, and Kalina Bontcheva. 2017. Simple open stance classification for rumour analysis. In Proceedings of the International Confer- ence Recent Advances in Natural Language Process- ing, RANLP 2017, pages 31-39, Varna, Bulgaria.\n\nSemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, Manuela Sanguinetti, 10.18653/v1/S19-2007Proceedings of the 13th International Workshop on Semantic Evaluation. the 13th International Workshop on Semantic EvaluationMinneapolis, Minnesota, USAAssociation for Computational LinguisticsValerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela San- guinetti. 2019. SemEval-2019 task 5: Multilin- gual detection of hate speech against immigrants and women in Twitter. In Proceedings of the 13th Inter- national Workshop on Semantic Evaluation, pages 54-63, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\n\nHate speech, machine classification and statistical modelling of information flows on twitter: interpretation and communication for policy decision making. Peter Burnap, Matthew Williams, Internet. Oxford, United KingdomPeter Burnap and Matthew Williams. 2014. Hate speech, machine classification and statistical mod- elling of information flows on twitter: interpretation and communication for policy decision making. In Internet, Policy and Politics, Oxford, United King- dom.\n\nTwitter: monthly active users worldwide. Jessica Clement, Jessica Clement. 2019. Twitter: monthly active users worldwide. https://www.statista.com/statistics/ 282087/number-of-monthly-active-twitter-users/.\n\nAutomated hate speech detection and the problem of offensive language. Thomas Davidson, Dana Warmsley, Michael W Macy, Ingmar Weber, Proceedings of the Eleventh International Conference on Web and Social Media, ICWSM. the Eleventh International Conference on Web and Social Media, ICWSMMontr\u00e9al, Qu\u00e9bec, CanadaAAAI PressThomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of the Eleventh International Confer- ence on Web and Social Media, ICWSM, pages 512- 515, Montr\u00e9al, Qu\u00e9bec, Canada. AAAI Press.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\nCommon sense reasoning for detection, prevention, and mitigation of cyberbullying. Karthik Dinakar, Birago Jones, Catherine Havasi, Henry Lieberman, Rosalind Picard, 10.1145/2362394.2362400ACM Trans. Interact. Intell. Syst. 23Karthik Dinakar, Birago Jones, Catherine Havasi, Henry Lieberman, and Rosalind Picard. 2012. Com- mon sense reasoning for detection, prevention, and mitigation of cyberbullying. ACM Trans. Interact. Intell. Syst., 2(3).\n\nA survey on automatic detection of hate speech in text. Paula Fortuna, S\u00e9rgio Nunes, 10.1145/3232676ACM Comput. Surv. 514Paula Fortuna and S\u00e9rgio Nunes. 2018. A survey on au- tomatic detection of hate speech in text. ACM Com- put. Surv., 51(4).\n\nDetecting online hate speech using context aware models. Lei Gao, Ruihong Huang, 10.26615/978-954-452-049-6_036Proceedings of the International Conference Recent Advances in Natural Language Processing. the International Conference Recent Advances in Natural Language ProcessingVarna, BulgariaINCOMA LtdLei Gao and Ruihong Huang. 2017. Detecting on- line hate speech using context aware models. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 260-266, Varna, Bulgaria. INCOMA Ltd.\n\nHate speech dataset from a white supremacy forum. Naiara Ona De Gibert, Perez, 10.18653/v1/W18-5102Proceedings of the 2nd Workshop on Abusive Language Online (ALW2). the 2nd Workshop on Abusive Language Online (ALW2)Brussels, BelgiumAssociation for Computational LinguisticsAitor Garc\u00eda-Pablos, and Montse CuadrosOna de Gibert, Naiara Perez, Aitor Garc\u00eda-Pablos, and Montse Cuadros. 2018. Hate speech dataset from a white supremacy forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11-20, Brussels, Belgium. Association for Computational Linguistics.\n\nStance detection on tweets: An SVM-based approach. Dilek K\u00fc\u00e7\u00fck, Fazli Can, abs/1803.08910CoRRDilek K\u00fc\u00e7\u00fck and Fazli Can. 2018. Stance detec- tion on tweets: An SVM-based approach. CoRR, abs/1803.08910.\n\nStance prediction for russian: Data and analysis. Nikita Lozhnikov, Leon Derczynski, Manuel Mazzara, 10.1007/978-3-030-14687-0_16Proceedings of 6th International Conference in Software Engineering for Defence Applications. 6th International Conference in Software Engineering for Defence ApplicationsChamSpringer International PublishingNikita Lozhnikov, Leon Derczynski, and Manuel Maz- zara. 2020. Stance prediction for russian: Data and analysis. In Proceedings of 6th International Conference in Software Engineering for Defence Ap- plications, pages 176-186, Cham. Springer Interna- tional Publishing.\n\nOverview of the HASOC track at FIRE 2019: Hate speech and offensive content identification in indo-european languages. Thomas Mandl, Sandip Modha, Prasenjit Majumder, Daksh Patel, Mohana Dave, Chintak Mandlia, Aditya Patel, 10.1145/3368567.3368584Proceedings of the 11th Forum for Information Retrieval Evaluation, FIRE '19. the 11th Forum for Information Retrieval Evaluation, FIRE '19NewThomas Mandl, Sandip Modha, Prasenjit Majumder, Daksh Patel, Mohana Dave, Chintak Mandlia, and Aditya Patel. 2019. Overview of the HASOC track at FIRE 2019: Hate speech and offensive content identification in indo-european languages. In Pro- ceedings of the 11th Forum for Information Re- trieval Evaluation, FIRE '19, page 14-17, New\n\nAssociation for Computing Machinery. N Y York, Usa , York, NY, USA. Association for Computing Machin- ery.\n\nCyber-aggression, cyberbullying, and cyber-grooming: A survey and research challenges. Miljana Mladenovi\u0107, Vera O\u0161mjanski, Sta\u0161a Vuji\u010di\u0107 Stankovi\u0107, 10.1145/3424246ACM Comput. Surv. 541Miljana Mladenovi\u0107, Vera O\u0161mjanski, and Sta\u0161a Vuji\u010di\u0107 Stankovi\u0107. 2021. Cyber-aggression, cyberbullying, and cyber-grooming: A survey and research challenges. ACM Comput. Surv., 54(1).\n\nSemEval-2016 task 6: Detecting stance in tweets. Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, Colin Cherry, 10.18653/v1/S16-1003Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). the 10th International Workshop on Semantic Evaluation (SemEval-2016)San Diego, CaliforniaAssociation for Computational LinguisticsSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob- hani, Xiaodan Zhu, and Colin Cherry. 2016. SemEval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31- 41, San Diego, California. Association for Computa- tional Linguistics.\n\nStance and sentiment in tweets. M Saif, Parinaz Mohammad, Svetlana Sobhani, Kiritchenko, 10.1145/3003433ACM Trans. Internet Technol. 173Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko. 2017. Stance and sentiment in tweets. ACM Trans. Internet Technol., 17(3).\n\nHate speech. Encyclopedia of the American Constitution. T John, Nockelby, 3John T. Nockelby. 2000. Hate speech. Encyclopedia of the American Constitution, 3:1277-1279.\n\nMeasuring the reliability of hate speech annotations: The case of the european refugee crisis. Bj\u00f6rn Ro\u00df, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, Michael Maximilian Wojatzki, 10.17185/duepublico/42132Bj\u00f6rn Ro\u00df, Michael Rist, Guillermo Carbonell, Ben- jamin Cabrera, Nils Kurowsky, and Michael Max- imilian Wojatzki. 2016. Measuring the reliability of hate speech annotations: The case of the euro- pean refugee crisis. https://duepublico2.uni-due.de/ receive/duepublico mods 00042132.\n\nA survey on hate speech detection using natural language processing. Anna Schmidt, Michael Wiegand, 10.18653/v1/W17-1101Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media. the Fifth International Workshop on Natural Language Processing for Social MediaValencia, SpainAssociation for Computational LinguisticsAnna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural language pro- cessing. In Proceedings of the Fifth International Workshop on Natural Language Processing for So- cial Media, pages 1-10, Valencia, Spain. Associa- tion for Computational Linguistics.\n\nRecognizing stances in ideological on-line debates. Swapna Somasundaran, Janyce Wiebe, Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in TextLos Angeles, CAAssociation for Computational LinguisticsSwapna Somasundaran and Janyce Wiebe. 2010. Rec- ognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Genera- tion of Emotion in Text, pages 116-124, Los Ange- les, CA. Association for Computational Linguistics.\n\nSmokey: Automatic recognition of hostile messages. Ellen Spertus, Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, AAAI'97/IAAI'97. the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, AAAI'97/IAAI'97AAAI PressEllen Spertus. 1997. Smokey: Automatic recognition of hostile messages. In Proceedings of the Four- teenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, AAAI'97/IAAI'97, page 1058-1065. AAAI Press.\n\nTwitter, trump, and the base: A shift to a new form of presidential talk?. Galen Stolee, Steve Caton, 10.1086/694755Signs and Society. 6Galen Stolee and Steve Caton. 2018. Twitter, trump, and the base: A shift to a new form of presidential talk? Signs and Society, 6:147-165.\n\nGet out the vote: Determining support or opposition from congressional floor-debate transcripts. Matt Thomas, Bo Pang, Lillian Lee, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. the 2006 Conference on Empirical Methods in Natural Language ProcessingSydney, AustraliaAssociation for Computational LinguisticsMatt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proceed- ings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327-335, Sydney, Australia. Association for Computational Linguistics.\n\nPredicting elections with twitter: What 140 characters reveal about political sentiment. Andranik Tumasjan, Timm Sprenger, Philipp Sandner, Isabell Welpe, Fourth International AAAI Conference on Weblogs and Social Media. Andranik Tumasjan, Timm Sprenger, Philipp Sandner, and Isabell Welpe. 2010. Predicting elections with twitter: What 140 characters reveal about political sentiment. In Fourth International AAAI Conference on Weblogs and Social Media, pages 178-185.\n\nDetecting hate speech on the world wide web. William Warner, Julia Hirschberg, Proceedings of the Second Workshop on Language in Social Media. the Second Workshop on Language in Social MediaMontr\u00e9al, CanadaAssociation for Computational LinguisticsWilliam Warner and Julia Hirschberg. 2012. Detecting hate speech on the world wide web. In Proceedings of the Second Workshop on Language in Social Me- dia, pages 19-26, Montr\u00e9al, Canada. Association for Computational Linguistics.\n\nHateful symbols or hateful people? predictive features for hate speech detection on Twitter. Zeerak Waseem, Dirk Hovy, 10.18653/v1/N16-2013Proceedings of the NAACL Student Research Workshop. the NAACL Student Research WorkshopSan Diego, CaliforniaAssociation for Computational LinguisticsZeerak Waseem and Dirk Hovy. 2016. Hateful sym- bols or hateful people? predictive features for hate speech detection on Twitter. In Proceedings of the NAACL Student Research Workshop, pages 88-93, San Diego, California. Association for Computa- tional Linguistics.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language pro- cessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\nZeses Pitenis, and \u00c7 agr\u0131 \u00c7\u00f6ltekin. 2020. SemEval-2020 task 12: Multilingual offensive language identification in social media (Offen-sEval 2020). Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon Derczynski, Proceedings of the Fourteenth Workshop on Semantic Evaluation. the Fourteenth Workshop on Semantic EvaluationBarcelonaInternational Committee for Computational LinguisticsMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, and \u00c7 agr\u0131 \u00c7\u00f6ltekin. 2020. SemEval-2020 task 12: Multilingual offen- sive language identification in social media (Offen- sEval 2020). In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 1425- 1447, Barcelona (online). International Committee for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":318,\"start\":156},{\"end\":477,\"start\":319}]", "publisher": null, "author_last_name": "[{\"end\":171,\"start\":161},{\"end\":332,\"start\":325}]", "author_first_name": "[{\"end\":160,\"start\":156},{\"end\":324,\"start\":319}]", "author_affiliation": "[{\"end\":317,\"start\":210},{\"end\":476,\"start\":369}]", "title": "[{\"end\":139,\"start\":1},{\"end\":616,\"start\":478}]", "venue": "[{\"end\":731,\"start\":618}]", "abstract": "[{\"end\":2304,\"start\":848}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2455,\"start\":2432},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2615,\"start\":2600},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2739,\"start\":2715},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3040,\"start\":3019},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3267,\"start\":3251},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3366,\"start\":3337},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3533,\"start\":3512},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3628,\"start\":3608},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3742,\"start\":3721},{\"end\":4142,\"start\":4114},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4571,\"start\":4551},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5410,\"start\":5396},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5587,\"start\":5565},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5675,\"start\":5646},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5785,\"start\":5763},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6009,\"start\":5990},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6609,\"start\":6583},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6745,\"start\":6728},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6877,\"start\":6857},{\"end\":7553,\"start\":7531},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7667,\"start\":7643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7693,\"start\":7669},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7721,\"start\":7695},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7986,\"start\":7965},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8032,\"start\":8002},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8064,\"start\":8042},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8088,\"start\":8064},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8106,\"start\":8088},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8126,\"start\":8106},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8149,\"start\":8126},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8195,\"start\":8171},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8217,\"start\":8197},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8364,\"start\":8335},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8570,\"start\":8548},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9063,\"start\":9041},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9302,\"start\":9284},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17367,\"start\":17346},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17447,\"start\":17428},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19638,\"start\":19616},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20072,\"start\":20053},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20571,\"start\":20549},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20592,\"start\":20573},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20860,\"start\":20838},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21059,\"start\":21040},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21316,\"start\":21297},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22207,\"start\":22185},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26152,\"start\":26130},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26240,\"start\":26221}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29213,\"start\":28915},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29847,\"start\":29214},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29901,\"start\":29848},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30171,\"start\":29902},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30482,\"start\":30172},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30550,\"start\":30483},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30869,\"start\":30551},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":30975,\"start\":30870},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":31084,\"start\":30976},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":31190,\"start\":31085}]", "paragraph": "[{\"end\":2913,\"start\":2320},{\"end\":3743,\"start\":2915},{\"end\":4620,\"start\":3745},{\"end\":4658,\"start\":4622},{\"end\":4865,\"start\":4660},{\"end\":5036,\"start\":4867},{\"end\":5301,\"start\":5038},{\"end\":5317,\"start\":5303},{\"end\":6826,\"start\":5356},{\"end\":7722,\"start\":6828},{\"end\":7860,\"start\":7724},{\"end\":8922,\"start\":7881},{\"end\":9618,\"start\":8924},{\"end\":9908,\"start\":9620},{\"end\":10214,\"start\":9937},{\"end\":10814,\"start\":10216},{\"end\":11074,\"start\":10847},{\"end\":11185,\"start\":11076},{\"end\":11569,\"start\":11187},{\"end\":13653,\"start\":11594},{\"end\":13824,\"start\":13655},{\"end\":14662,\"start\":13856},{\"end\":15286,\"start\":14664},{\"end\":15592,\"start\":15288},{\"end\":16119,\"start\":15594},{\"end\":16561,\"start\":16121},{\"end\":16974,\"start\":16563},{\"end\":17516,\"start\":17012},{\"end\":18269,\"start\":17518},{\"end\":19092,\"start\":18271},{\"end\":19522,\"start\":19143},{\"end\":19597,\"start\":19524},{\"end\":19639,\"start\":19599},{\"end\":20034,\"start\":19641},{\"end\":21621,\"start\":20036},{\"end\":22547,\"start\":21623},{\"end\":22729,\"start\":22560},{\"end\":23584,\"start\":22731},{\"end\":24316,\"start\":23586},{\"end\":24602,\"start\":24318},{\"end\":24809,\"start\":24604},{\"end\":25328,\"start\":24811},{\"end\":25938,\"start\":25343},{\"end\":26690,\"start\":25940},{\"end\":27498,\"start\":26692},{\"end\":27897,\"start\":27529},{\"end\":28261,\"start\":27899},{\"end\":28914,\"start\":28263}]", "formula": null, "table_ref": "[{\"end\":12506,\"start\":12499},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":15433,\"start\":15426},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17795,\"start\":17788},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":20482,\"start\":20475},{\"end\":21907,\"start\":21767},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":22702,\"start\":22695},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":22829,\"start\":22822},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":24395,\"start\":24388},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25365,\"start\":25358},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25486,\"start\":25472},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25936,\"start\":25929}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2318,\"start\":2306},{\"attributes\":{\"n\":\"2.1\"},\"end\":5354,\"start\":5320},{\"attributes\":{\"n\":\"2.2\"},\"end\":7879,\"start\":7863},{\"attributes\":{\"n\":\"3\"},\"end\":9917,\"start\":9911},{\"attributes\":{\"n\":\"3.1\"},\"end\":9935,\"start\":9920},{\"attributes\":{\"n\":\"3.2\"},\"end\":10827,\"start\":10817},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":10845,\"start\":10830},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":11592,\"start\":11572},{\"attributes\":{\"n\":\"4\"},\"end\":13834,\"start\":13827},{\"attributes\":{\"n\":\"4.1\"},\"end\":13854,\"start\":13837},{\"attributes\":{\"n\":\"4.2\"},\"end\":17010,\"start\":16977},{\"attributes\":{\"n\":\"4.3\"},\"end\":19141,\"start\":19095},{\"attributes\":{\"n\":\"4.4\"},\"end\":22558,\"start\":22550},{\"attributes\":{\"n\":\"4.5\"},\"end\":25341,\"start\":25331},{\"attributes\":{\"n\":\"5\"},\"end\":27527,\"start\":27501},{\"end\":28917,\"start\":28916},{\"end\":29858,\"start\":29849},{\"end\":30182,\"start\":30173},{\"end\":30493,\"start\":30484},{\"end\":30561,\"start\":30552},{\"end\":30880,\"start\":30871},{\"end\":30986,\"start\":30977},{\"end\":31095,\"start\":31086}]", "table": "[{\"end\":29847,\"start\":29609},{\"end\":30171,\"start\":29977},{\"end\":30482,\"start\":30225},{\"end\":30869,\"start\":30646}]", "figure_caption": "[{\"end\":29213,\"start\":28918},{\"end\":29609,\"start\":29216},{\"end\":29901,\"start\":29860},{\"end\":29977,\"start\":29904},{\"end\":30225,\"start\":30184},{\"end\":30550,\"start\":30495},{\"end\":30646,\"start\":30563},{\"end\":30975,\"start\":30882},{\"end\":31084,\"start\":30988},{\"end\":31190,\"start\":31097}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":31545,\"start\":31540},{\"end\":31556,\"start\":31552},{\"end\":31575,\"start\":31569},{\"end\":32160,\"start\":32153},{\"end\":32177,\"start\":32169},{\"end\":32195,\"start\":32185},{\"end\":32211,\"start\":32205},{\"end\":32226,\"start\":32219},{\"end\":32257,\"start\":32234},{\"end\":32270,\"start\":32265},{\"end\":32285,\"start\":32278},{\"end\":33095,\"start\":33090},{\"end\":33111,\"start\":33104},{\"end\":33462,\"start\":33455},{\"end\":33699,\"start\":33693},{\"end\":33714,\"start\":33710},{\"end\":33732,\"start\":33725},{\"end\":33734,\"start\":33733},{\"end\":33747,\"start\":33741},{\"end\":34318,\"start\":34313},{\"end\":34335,\"start\":34327},{\"end\":34349,\"start\":34343},{\"end\":34363,\"start\":34355},{\"end\":35265,\"start\":35258},{\"end\":35281,\"start\":35275},{\"end\":35298,\"start\":35289},{\"end\":35312,\"start\":35307},{\"end\":35332,\"start\":35324},{\"end\":35683,\"start\":35678},{\"end\":35699,\"start\":35693},{\"end\":35928,\"start\":35925},{\"end\":35941,\"start\":35934},{\"end\":36471,\"start\":36465},{\"end\":37056,\"start\":37051},{\"end\":37069,\"start\":37064},{\"end\":37258,\"start\":37252},{\"end\":37274,\"start\":37270},{\"end\":37293,\"start\":37287},{\"end\":37935,\"start\":37929},{\"end\":37949,\"start\":37943},{\"end\":37966,\"start\":37957},{\"end\":37982,\"start\":37977},{\"end\":37996,\"start\":37990},{\"end\":38010,\"start\":38003},{\"end\":38026,\"start\":38020},{\"end\":38573,\"start\":38572},{\"end\":38575,\"start\":38574},{\"end\":38585,\"start\":38582},{\"end\":38737,\"start\":38730},{\"end\":38754,\"start\":38750},{\"end\":38779,\"start\":38766},{\"end\":39065,\"start\":39061},{\"end\":39084,\"start\":39076},{\"end\":39105,\"start\":39098},{\"end\":39122,\"start\":39115},{\"end\":39133,\"start\":39128},{\"end\":39726,\"start\":39725},{\"end\":39740,\"start\":39733},{\"end\":39759,\"start\":39751},{\"end\":40023,\"start\":40022},{\"end\":40235,\"start\":40230},{\"end\":40248,\"start\":40241},{\"end\":40264,\"start\":40255},{\"end\":40284,\"start\":40276},{\"end\":40298,\"start\":40294},{\"end\":40316,\"start\":40309},{\"end\":40327,\"start\":40317},{\"end\":40722,\"start\":40718},{\"end\":40739,\"start\":40732},{\"end\":41348,\"start\":41342},{\"end\":41369,\"start\":41363},{\"end\":42007,\"start\":42002},{\"end\":42706,\"start\":42701},{\"end\":42720,\"start\":42715},{\"end\":43004,\"start\":43000},{\"end\":43015,\"start\":43013},{\"end\":43029,\"start\":43022},{\"end\":43662,\"start\":43654},{\"end\":43677,\"start\":43673},{\"end\":43695,\"start\":43688},{\"end\":43712,\"start\":43705},{\"end\":44088,\"start\":44081},{\"end\":44102,\"start\":44097},{\"end\":44614,\"start\":44608},{\"end\":44627,\"start\":44623},{\"end\":45136,\"start\":45130},{\"end\":45151,\"start\":45143},{\"end\":45165,\"start\":45159},{\"end\":45178,\"start\":45172},{\"end\":45196,\"start\":45189},{\"end\":45214,\"start\":45207},{\"end\":45227,\"start\":45220},{\"end\":45239,\"start\":45236},{\"end\":45251,\"start\":45247},{\"end\":45264,\"start\":45258},{\"end\":45279,\"start\":45276},{\"end\":45292,\"start\":45289},{\"end\":45308,\"start\":45303},{\"end\":45335,\"start\":45329},{\"end\":45346,\"start\":45340},{\"end\":45362,\"start\":45356},{\"end\":45373,\"start\":45368},{\"end\":45376,\"start\":45374},{\"end\":45388,\"start\":45381},{\"end\":45402,\"start\":45395},{\"end\":45418,\"start\":45411},{\"end\":45435,\"start\":45426},{\"end\":45437,\"start\":45436},{\"end\":46439,\"start\":46433},{\"end\":46457,\"start\":46450},{\"end\":46469,\"start\":46465},{\"end\":46485,\"start\":46481},{\"end\":46503,\"start\":46497},{\"end\":46520,\"start\":46515},{\"end\":46534,\"start\":46530}]", "bib_author_last_name": "[{\"end\":31550,\"start\":31546},{\"end\":31567,\"start\":31557},{\"end\":31585,\"start\":31576},{\"end\":32167,\"start\":32161},{\"end\":32183,\"start\":32178},{\"end\":32203,\"start\":32196},{\"end\":32217,\"start\":32212},{\"end\":32232,\"start\":32227},{\"end\":32263,\"start\":32258},{\"end\":32276,\"start\":32271},{\"end\":32297,\"start\":32286},{\"end\":33102,\"start\":33096},{\"end\":33120,\"start\":33112},{\"end\":33470,\"start\":33463},{\"end\":33708,\"start\":33700},{\"end\":33723,\"start\":33715},{\"end\":33739,\"start\":33735},{\"end\":33753,\"start\":33748},{\"end\":34325,\"start\":34319},{\"end\":34341,\"start\":34336},{\"end\":34353,\"start\":34350},{\"end\":34373,\"start\":34364},{\"end\":35273,\"start\":35266},{\"end\":35287,\"start\":35282},{\"end\":35305,\"start\":35299},{\"end\":35322,\"start\":35313},{\"end\":35339,\"start\":35333},{\"end\":35691,\"start\":35684},{\"end\":35705,\"start\":35700},{\"end\":35932,\"start\":35929},{\"end\":35947,\"start\":35942},{\"end\":36485,\"start\":36472},{\"end\":36492,\"start\":36487},{\"end\":37062,\"start\":37057},{\"end\":37073,\"start\":37070},{\"end\":37268,\"start\":37259},{\"end\":37285,\"start\":37275},{\"end\":37301,\"start\":37294},{\"end\":37941,\"start\":37936},{\"end\":37955,\"start\":37950},{\"end\":37975,\"start\":37967},{\"end\":37988,\"start\":37983},{\"end\":38001,\"start\":37997},{\"end\":38018,\"start\":38011},{\"end\":38032,\"start\":38027},{\"end\":38580,\"start\":38576},{\"end\":38748,\"start\":38738},{\"end\":38764,\"start\":38755},{\"end\":38789,\"start\":38780},{\"end\":39074,\"start\":39066},{\"end\":39096,\"start\":39085},{\"end\":39113,\"start\":39106},{\"end\":39126,\"start\":39123},{\"end\":39140,\"start\":39134},{\"end\":39731,\"start\":39727},{\"end\":39749,\"start\":39741},{\"end\":39767,\"start\":39760},{\"end\":39780,\"start\":39769},{\"end\":40028,\"start\":40024},{\"end\":40038,\"start\":40030},{\"end\":40239,\"start\":40236},{\"end\":40253,\"start\":40249},{\"end\":40274,\"start\":40265},{\"end\":40292,\"start\":40285},{\"end\":40307,\"start\":40299},{\"end\":40336,\"start\":40328},{\"end\":40730,\"start\":40723},{\"end\":40747,\"start\":40740},{\"end\":41361,\"start\":41349},{\"end\":41375,\"start\":41370},{\"end\":42015,\"start\":42008},{\"end\":42713,\"start\":42707},{\"end\":42726,\"start\":42721},{\"end\":43011,\"start\":43005},{\"end\":43020,\"start\":43016},{\"end\":43033,\"start\":43030},{\"end\":43671,\"start\":43663},{\"end\":43686,\"start\":43678},{\"end\":43703,\"start\":43696},{\"end\":43718,\"start\":43713},{\"end\":44095,\"start\":44089},{\"end\":44113,\"start\":44103},{\"end\":44621,\"start\":44615},{\"end\":44632,\"start\":44628},{\"end\":45141,\"start\":45137},{\"end\":45157,\"start\":45152},{\"end\":45170,\"start\":45166},{\"end\":45187,\"start\":45179},{\"end\":45205,\"start\":45197},{\"end\":45218,\"start\":45215},{\"end\":45234,\"start\":45228},{\"end\":45245,\"start\":45240},{\"end\":45256,\"start\":45252},{\"end\":45274,\"start\":45265},{\"end\":45287,\"start\":45280},{\"end\":45301,\"start\":45293},{\"end\":45327,\"start\":45309},{\"end\":45338,\"start\":45336},{\"end\":45354,\"start\":45347},{\"end\":45366,\"start\":45363},{\"end\":45379,\"start\":45377},{\"end\":45393,\"start\":45389},{\"end\":45409,\"start\":45403},{\"end\":45424,\"start\":45419},{\"end\":45444,\"start\":45438},{\"end\":45450,\"start\":45446},{\"end\":46448,\"start\":46440},{\"end\":46463,\"start\":46458},{\"end\":46479,\"start\":46470},{\"end\":46495,\"start\":46486},{\"end\":46513,\"start\":46504},{\"end\":46528,\"start\":46521},{\"end\":46545,\"start\":46535}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.26615/978-954-452-049-6_005\",\"id\":\"b0\",\"matched_paper_id\":9095511},\"end\":32051,\"start\":31485},{\"attributes\":{\"doi\":\"10.18653/v1/S19-2007\",\"id\":\"b1\",\"matched_paper_id\":184483123},\"end\":32932,\"start\":32053},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":142840744},\"end\":33412,\"start\":32934},{\"attributes\":{\"id\":\"b3\"},\"end\":33620,\"start\":33414},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1733167},\"end\":34229,\"start\":33622},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b5\",\"matched_paper_id\":52967399},\"end\":35173,\"start\":34231},{\"attributes\":{\"doi\":\"10.1145/2362394.2362400\",\"id\":\"b6\",\"matched_paper_id\":5560081},\"end\":35620,\"start\":35175},{\"attributes\":{\"doi\":\"10.1145/3232676\",\"id\":\"b7\",\"matched_paper_id\":52184457},\"end\":35866,\"start\":35622},{\"attributes\":{\"doi\":\"10.26615/978-954-452-049-6_036\",\"id\":\"b8\",\"matched_paper_id\":8564811},\"end\":36413,\"start\":35868},{\"attributes\":{\"doi\":\"10.18653/v1/W18-5102\",\"id\":\"b9\",\"matched_paper_id\":52194540},\"end\":36998,\"start\":36415},{\"attributes\":{\"doi\":\"abs/1803.08910\",\"id\":\"b10\"},\"end\":37200,\"start\":37000},{\"attributes\":{\"doi\":\"10.1007/978-3-030-14687-0_16\",\"id\":\"b11\",\"matched_paper_id\":52164524},\"end\":37808,\"start\":37202},{\"attributes\":{\"doi\":\"10.1145/3368567.3368584\",\"id\":\"b12\",\"matched_paper_id\":209393462},\"end\":38533,\"start\":37810},{\"attributes\":{\"id\":\"b13\"},\"end\":38641,\"start\":38535},{\"attributes\":{\"doi\":\"10.1145/3424246\",\"id\":\"b14\",\"matched_paper_id\":233354202},\"end\":39010,\"start\":38643},{\"attributes\":{\"doi\":\"10.18653/v1/S16-1003\",\"id\":\"b15\",\"matched_paper_id\":286464},\"end\":39691,\"start\":39012},{\"attributes\":{\"doi\":\"10.1145/3003433\",\"id\":\"b16\",\"matched_paper_id\":8632380},\"end\":39964,\"start\":39693},{\"attributes\":{\"id\":\"b17\"},\"end\":40133,\"start\":39966},{\"attributes\":{\"doi\":\"10.17185/duepublico/42132\",\"id\":\"b18\"},\"end\":40647,\"start\":40135},{\"attributes\":{\"doi\":\"10.18653/v1/W17-1101\",\"id\":\"b19\",\"matched_paper_id\":9626793},\"end\":41288,\"start\":40649},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":927208},\"end\":41949,\"start\":41290},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":26817854},\"end\":42624,\"start\":41951},{\"attributes\":{\"doi\":\"10.1086/694755\",\"id\":\"b22\",\"matched_paper_id\":158493606},\"end\":42901,\"start\":42626},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1587},\"end\":43563,\"start\":42903},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17539846},\"end\":44034,\"start\":43565},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12477446},\"end\":44513,\"start\":44036},{\"attributes\":{\"doi\":\"10.18653/v1/N16-2013\",\"id\":\"b26\",\"matched_paper_id\":1721388},\"end\":45068,\"start\":44515},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":208117506},\"end\":46284,\"start\":45070},{\"attributes\":{\"id\":\"b28\"},\"end\":47140,\"start\":46286}]", "bib_title": "[{\"end\":31538,\"start\":31485},{\"end\":32151,\"start\":32053},{\"end\":33088,\"start\":32934},{\"end\":33691,\"start\":33622},{\"end\":34311,\"start\":34231},{\"end\":35256,\"start\":35175},{\"end\":35676,\"start\":35622},{\"end\":35923,\"start\":35868},{\"end\":36463,\"start\":36415},{\"end\":37250,\"start\":37202},{\"end\":37927,\"start\":37810},{\"end\":38728,\"start\":38643},{\"end\":39059,\"start\":39012},{\"end\":39723,\"start\":39693},{\"end\":40716,\"start\":40649},{\"end\":41340,\"start\":41290},{\"end\":42000,\"start\":41951},{\"end\":42699,\"start\":42626},{\"end\":42998,\"start\":42903},{\"end\":43652,\"start\":43565},{\"end\":44079,\"start\":44036},{\"end\":44606,\"start\":44515},{\"end\":45128,\"start\":45070},{\"end\":46431,\"start\":46286}]", "bib_author": "[{\"end\":31552,\"start\":31540},{\"end\":31569,\"start\":31552},{\"end\":31587,\"start\":31569},{\"end\":32169,\"start\":32153},{\"end\":32185,\"start\":32169},{\"end\":32205,\"start\":32185},{\"end\":32219,\"start\":32205},{\"end\":32234,\"start\":32219},{\"end\":32265,\"start\":32234},{\"end\":32278,\"start\":32265},{\"end\":32299,\"start\":32278},{\"end\":33104,\"start\":33090},{\"end\":33122,\"start\":33104},{\"end\":33472,\"start\":33455},{\"end\":33710,\"start\":33693},{\"end\":33725,\"start\":33710},{\"end\":33741,\"start\":33725},{\"end\":33755,\"start\":33741},{\"end\":34327,\"start\":34313},{\"end\":34343,\"start\":34327},{\"end\":34355,\"start\":34343},{\"end\":34375,\"start\":34355},{\"end\":35275,\"start\":35258},{\"end\":35289,\"start\":35275},{\"end\":35307,\"start\":35289},{\"end\":35324,\"start\":35307},{\"end\":35341,\"start\":35324},{\"end\":35693,\"start\":35678},{\"end\":35707,\"start\":35693},{\"end\":35934,\"start\":35925},{\"end\":35949,\"start\":35934},{\"end\":36487,\"start\":36465},{\"end\":36494,\"start\":36487},{\"end\":37064,\"start\":37051},{\"end\":37075,\"start\":37064},{\"end\":37270,\"start\":37252},{\"end\":37287,\"start\":37270},{\"end\":37303,\"start\":37287},{\"end\":37943,\"start\":37929},{\"end\":37957,\"start\":37943},{\"end\":37977,\"start\":37957},{\"end\":37990,\"start\":37977},{\"end\":38003,\"start\":37990},{\"end\":38020,\"start\":38003},{\"end\":38034,\"start\":38020},{\"end\":38582,\"start\":38572},{\"end\":38588,\"start\":38582},{\"end\":38750,\"start\":38730},{\"end\":38766,\"start\":38750},{\"end\":38791,\"start\":38766},{\"end\":39076,\"start\":39061},{\"end\":39098,\"start\":39076},{\"end\":39115,\"start\":39098},{\"end\":39128,\"start\":39115},{\"end\":39142,\"start\":39128},{\"end\":39733,\"start\":39725},{\"end\":39751,\"start\":39733},{\"end\":39769,\"start\":39751},{\"end\":39782,\"start\":39769},{\"end\":40030,\"start\":40022},{\"end\":40040,\"start\":40030},{\"end\":40241,\"start\":40230},{\"end\":40255,\"start\":40241},{\"end\":40276,\"start\":40255},{\"end\":40294,\"start\":40276},{\"end\":40309,\"start\":40294},{\"end\":40338,\"start\":40309},{\"end\":40732,\"start\":40718},{\"end\":40749,\"start\":40732},{\"end\":41363,\"start\":41342},{\"end\":41377,\"start\":41363},{\"end\":42017,\"start\":42002},{\"end\":42715,\"start\":42701},{\"end\":42728,\"start\":42715},{\"end\":43013,\"start\":43000},{\"end\":43022,\"start\":43013},{\"end\":43035,\"start\":43022},{\"end\":43673,\"start\":43654},{\"end\":43688,\"start\":43673},{\"end\":43705,\"start\":43688},{\"end\":43720,\"start\":43705},{\"end\":44097,\"start\":44081},{\"end\":44115,\"start\":44097},{\"end\":44623,\"start\":44608},{\"end\":44634,\"start\":44623},{\"end\":45143,\"start\":45130},{\"end\":45159,\"start\":45143},{\"end\":45172,\"start\":45159},{\"end\":45189,\"start\":45172},{\"end\":45207,\"start\":45189},{\"end\":45220,\"start\":45207},{\"end\":45236,\"start\":45220},{\"end\":45247,\"start\":45236},{\"end\":45258,\"start\":45247},{\"end\":45276,\"start\":45258},{\"end\":45289,\"start\":45276},{\"end\":45303,\"start\":45289},{\"end\":45329,\"start\":45303},{\"end\":45340,\"start\":45329},{\"end\":45356,\"start\":45340},{\"end\":45368,\"start\":45356},{\"end\":45381,\"start\":45368},{\"end\":45395,\"start\":45381},{\"end\":45411,\"start\":45395},{\"end\":45426,\"start\":45411},{\"end\":45446,\"start\":45426},{\"end\":45452,\"start\":45446},{\"end\":46450,\"start\":46433},{\"end\":46465,\"start\":46450},{\"end\":46481,\"start\":46465},{\"end\":46497,\"start\":46481},{\"end\":46515,\"start\":46497},{\"end\":46530,\"start\":46515},{\"end\":46547,\"start\":46530}]", "bib_venue": "[{\"end\":31799,\"start\":31709},{\"end\":32471,\"start\":32390},{\"end\":33154,\"start\":33132},{\"end\":33932,\"start\":33840},{\"end\":34688,\"start\":34539},{\"end\":36161,\"start\":36071},{\"end\":36648,\"start\":36581},{\"end\":37506,\"start\":37425},{\"end\":38199,\"start\":38135},{\"end\":39338,\"start\":39248},{\"end\":40961,\"start\":40866},{\"end\":41611,\"start\":41495},{\"end\":42340,\"start\":42187},{\"end\":43211,\"start\":43123},{\"end\":44242,\"start\":44179},{\"end\":44762,\"start\":44706},{\"end\":45657,\"start\":45563},{\"end\":46665,\"start\":46610},{\"end\":31707,\"start\":31617},{\"end\":32388,\"start\":32319},{\"end\":33130,\"start\":33122},{\"end\":33453,\"start\":33414},{\"end\":33838,\"start\":33755},{\"end\":34537,\"start\":34395},{\"end\":35397,\"start\":35364},{\"end\":35738,\"start\":35722},{\"end\":36069,\"start\":35979},{\"end\":36579,\"start\":36514},{\"end\":37049,\"start\":37000},{\"end\":37423,\"start\":37331},{\"end\":38133,\"start\":38057},{\"end\":38570,\"start\":38535},{\"end\":38822,\"start\":38806},{\"end\":39246,\"start\":39162},{\"end\":39824,\"start\":39797},{\"end\":40020,\"start\":39966},{\"end\":40228,\"start\":40135},{\"end\":40864,\"start\":40769},{\"end\":41493,\"start\":41377},{\"end\":42185,\"start\":42017},{\"end\":42759,\"start\":42742},{\"end\":43121,\"start\":43035},{\"end\":43784,\"start\":43720},{\"end\":44177,\"start\":44115},{\"end\":44704,\"start\":44654},{\"end\":45561,\"start\":45452},{\"end\":46608,\"start\":46547}]"}}}, "year": 2023, "month": 12, "day": 17}