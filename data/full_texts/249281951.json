{"id": 249281951, "updated": "2022-10-30 17:58:16.125", "metadata": {"title": "HDnn-PIM: Efficient in Memory Design of Hyperdimensional Computing with Feature Extraction", "authors": "[{\"first\":\"Arpan\",\"last\":\"Dutta\",\"middle\":[]},{\"first\":\"Saransh\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Behnam\",\"last\":\"Khaleghi\",\"middle\":[]},{\"first\":\"Rishikanth\",\"last\":\"Chandrasekaran\",\"middle\":[]},{\"first\":\"Weihong\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the Great Lakes Symposium on VLSI 2022", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Brain-inspired Hyperdimensional (HD) computing is a new machine learning approach that leverages simple and highly parallelizable operations. Unfortunately, none of the published HD computing algorithms to date have been able to accurately classify more complex image datasets, such as CIFAR100. In this work, we propose HDnn-PIM, that implements both feature extraction and HD-based classification for complex images by using processing-in-memory. We compare HDnn-PIM with HD-only and CNN implementations for various image datasets. HDnn-PIM achieves 52.4% higher accuracy as compared to pure HD computing. It also gains 1.2% accuracy improvement over state-of-the-art CNNs, but with 3.63x smaller memory footprint and 1.53x less MAC operations. Furthermore, HDnn-PIM is 3.6x-223x faster than RTX 3090 GPU, and 3.7x more energy efficient than state-of-the-art FloatPIM.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/glvlsi/DuttaGKCXR22", "doi": "10.1145/3526241.3530331"}}, "content": {"source": {"pdf_hash": "03ddcd466a2d174332b08338cd9b0faa3a48e194", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d827fd3d1e7c178b15bdac572cb80032c1cb9351", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/03ddcd466a2d174332b08338cd9b0faa3a48e194.txt", "contents": "\nHDnn-PIM: Efficient in Memory Design of Hyperdimensional Computing with Feature Extraction\n\n\nArpan Dutta adutta@ucsd.edu \nUC San Diego\nIBM Research\nSan Diego, San Diego, San Diego, San Diego\n\nSaransh Gupta saransh@ibm.com \nUC San Diego\nIBM Research\nSan Diego, San Diego, San Diego, San Diego\n\nBehnam Khaleghi bkhaleghi@ucsd.edu \nUC San Diego\nIBM Research\nSan Diego, San Diego, San Diego, San Diego\n\nRishikanth Chandrasekaran \nUC San Diego\nIBM Research\nSan Diego, San Diego, San Diego, San Diego\n\nWeihong Xu wexu@ucsd.edu \nUC San Diego\nIBM Research\nSan Diego, San Diego, San Diego, San Diego\n\nTajana Rosing tajana@ucsd.edu \nUC San Diego\nIBM Research\nSan Diego, San Diego, San Diego, San Diego\n\nHDnn-PIM: Efficient in Memory Design of Hyperdimensional Computing with Feature Extraction\n10.1145/3526241.3530331CCS Concepts \u2022 Hardware\u2022 Computer systems organizationKeywords Hyperdimensional Computing, Processing-in-Memory, CNN, RRAM\nBrain-inspired Hyperdimensional (HD) computing is a new machine learning approach that leverages simple and highly parallelizable operations. Unfortunately, none of the published HD computing algorithms to date have been able to accurately classify more complex image datasets, such as CIFAR100. In this work, we propose HDnn-PIM, that implements both feature extraction and HD-based classification for complex images by using processing-in-memory. We compare HDnn-PIM with HD-only and CNN implementations for various image datasets. HDnn-PIM achieves 52.4% higher accuracy as compared to pure HD computing. It also gains 1.2% accuracy improvement over state-of-the-art CNNs, but with 3.63\u00d7 smaller memory footprint and 1.53\u00d7 less MAC operations. Furthermore, HDnn-PIM is 3.6\u00d7-223\u00d7 faster than RTX 3090 GPU, and 3.7\u00d7 more energy efficient than state-of-the-art FloatPIM[5].\n\nIntroduction\n\nHyperdimensional (HD) computing [8,16] is an emerging machine learning paradigm that has shown impressive efficiency gains in IoT domain benchmarks. HD encodes data into high-dimensional space, where it can apply simple logic and arithmetic operations on the hypervectors to carry out learning tasks. The simplicity of HD operations, its inherent parallelism, combined with robustness to noise [9,11], makes it particularly appealing for learning at the edge. One of the key challenges of HD computing is its inability to get accurate results for more complex image analysis. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). GLSVLSI '22, June 6-8, 2022 Table 1 shows that the state of the art HD computing algorithms [7,18] get poor accuracy for all but the simplest dataset, MNIST. Even more recently published work that includes feature extraction into HD [13] only provides MNIST results. To address this issue, we propose HDnn (HD and Neural Network), which leverages a few initial stages of the convolution-based feature extraction to enable HD to learn effectively on complex data, as show in Fig. 1. As Table 1 shows, HDnn is the only HD-based model that achieves state-of-the-art accuracy on more complex datasets such as Flowers, CIFAR10 and CIFAR100, while using 3.63\u00d7 less memory vs. CNN [4]. Table 2 shows the accuracy-size trade off of cutting layers off the original CNN and adding HD classifier (ResNet34 baseline: [3,4,6,3], accuracy: 77.41%, MAC: 1161.5M, Param: 21.33M).\n\nThere are a few recent publications that combine neural networks with HD computing. Early work on voice recognition shows that using a neural network after HD classification results in a smaller and just as accurate design as if only a neural network is used [6]. Another example is [12] that uses neural network feature extractors and classifiers for simple, non-image datasets, such as ISOLET at the significant increase in the overall area due to using two different neural networks, HD encoder and decoder. A few publications combine Spiking Neural Networks (SNN) with HD for event-based data classification [17], but again show results on only the simplest datasets such as MNIST. Moreover, these works can be orthogonal to HDnn as they focus on event/spike based data.\n\nQuite a few publications have accelerated deep neural networks using processing-in-memory [1,5,14]. The majority of these work employ large and hard-to-scale mixed-signal circuits such as ADCs and DACs to achieve low latency operations [1,14] which leads to a chip with a limited memory capacity incapable of processing large networks. Digital PIM uses logical operations on digital signals stored in memory cells to carry out computation [5,15]. Such an architecture retains high memory density at the expense of slower atomic computations, which is less of a problem for HD due to its simple operations. Thus, we choose digital in-memory design for HDnn and compare it to state of the art digital PIM CNN [5].  In summary, the major contributions in this work are: (1) For the first time, we combine lightweight convolution-based feature extractor with HD computing for accurate yet efficient classification of complex data types.\n\n(2) We propose a configurable PIM-based architecture to accelerate HDnn, which can run in HD-and DNN-only modes. HDnn-PIM achieves high throughput by allocating compute memories statically to layers based on their compute requirements. This flexibility is enabled by a novel 2-dimensional bus interconnect that leverages the locality of internal data transfers and reduces the wire length.\n\n(3) HDnn-PIM innately supports residual connections found in new DNNs such as ResNet and Mobilenet, which alleviates the access to off-chip memory. It reduces the load on the bus for residual networks while also avoiding pipeline stalls (due to off-chip memory accesses) with a small area overhead. (4) We identify the accumulation as a performance bottleneck of PIM architectures used for HD and DNNs. Accordingly, we propose a novel Compute Element (CE) that efficiently performs MAC operations by hiding their latency with memory read out. Accumulation in memory takes at least 41% of total MACs time given the fully parallel multiplication in memory. We reduce it to 'bitwidth number of cycles' in the CMOS domain using bit-serial Carry-Save Adder, leading to 1.66\u00d7 faster MACs. HDnn-PIM's CE is also suited for HD, where it boosts HD's encoding by 5\u00d7 speedup over [7]. (5) We design a Post Processing Network (PPN) that speeds up the accumulation for different (large or small) number of inputs after MAC operation (e.g., for on-the-fly accumulation or bypass of tiles outputs) by up to 4\u00d7 compared to the typical in-memory addition. (1) Feature extractor (FE): For FE, we rely on well-devised networks such as VGG, ResNet, and MobileNet. We cut these networks after a certain pooling layer and use the first convolution layers as FE. We extract the latent space representations by passing training images forward through the FE, which act as features for the HD encoder. The output of FE, per each image, is a manifold feature maps. We flatten it to a one-dimension feature vector using global averaging, which results in a compressed (usually 512 or 256, the same as the number of filters of the cutting layer) vector.\n\n\nHDnn Algorithmic Flow\n\n(2) HD training: The extracted input features act as training data for HD, which first encodes the data to high-dimensional space, e.g., to D=4,000 dimensions. We use random projection (RP) [7] as it can be transformed to a matrix-vector multiplication as Eq.\n\n(1), where P is a D \u00d7 binary projection matrix ( is the number of features per input), and F is the input feature extracted for an image. H is the binary encoding hypervector of an image.\nD = sign(P D\u00d7 \u00d7 F )(1)\nFinally, HD adds all hypervectors of the same label to create the class of that label (e.g., 10 classed for CIFAR10 dataset) simply using C \u2113 = \u2208\u2113 H \u2113 . Inference is realized by comparing/searching the encoding hypervectors with the class hypervectors using cosine similarity or Hamming distance.\n\n(3) FE tuning: For FE, we use pre-trained CNN networks that are not targeted for a classifier such as HD, especially when are cut. Therefore, after attaching HD to FE (see Fig. 1), we keep the HD fixed but retrain the FE to calibrate according to HD. The RP encoding of Eq. (1) uses sign which does not converge during backpropagation of retraining, hence, we replace it with tanh when training FE. After training HD, we quantize the class hypervectors to four bits without accuracy loss, especially the tuning step of FE adjusts based on the existing fixed HD.\n\nAs previously shown in Table 1, while even a more sophisticated HD encoding (non-linear, which uses floating-point projection matrix) has 56.7% less average accuracy than CNN, HDnn improves the accuracy by 1.34% with smaller number of parameters and operations, which will be further elaborated in Section 4.\n\n\nHDnn-PIM Architecture\n\n\nMapping HDnn on HDnn-PIM Architecture\n\nFig. 2 demonstrates the overall data flow and mapping of HDnn on HDnn-PIM. We compile the given network to find out the mapping details of the layers on the supertiles of HDnn-PIM. The architectural details are elaborated later in Section 3.2. We assign supertiles based on the compute requirements of a given layer to balance and hence maximize the throughput. We divide the input image of the layer into multiple smaller images that are split across the supertiles allocated to that layer. Each supertile has the same coordinates among the input's feature maps to process the outputs independently. The weights are preloaded into the supertiles before feeding the inputs. As shown in Fig. 2 \u2022 1 , the weights are copied across the supertiles of a layer.\n\nInside a supertile, inputs are reused among the tiles while the weights are distributed among the tiles ( Fig. 2 \u2022 2 ). A tile implements either convolution of the given input and filter, or matrix-matrix multiplication. Inside each tile there are CEs that perform the actual computations, i.e., the matrix-matrix multiplication breaks down into matrix-vector operation inside the CEs. According to Fig. 2 \u2022 3 , the input matrix is split into rows, where each CE takes care of a row, but the weight matrix is copied into all the CEs of a tile. As shown in Fig. 2 \u2022 3 and \u2022 4 , each CE uses column-parallel multiplication in memory and bit-serial parallel CMOS accumulation to produce the partial sums or outputs. To perform the matrix-vector efficiently, the input is replicated and the weight matrix is flattened to realize it as a vector-vector multiplication, suited for digital PIM.\n\nThe FE module of HDnn-PIM generates the output features that are flattened, average pooled and forwarded as inputs to the HD module. The flattened input vector is multiplied by a binary projection matrix to realize encoding. This matrix-vector multiplication is performed in an array of CEs as described above. Based on whether the HD is in training or inference mode, the appropriate operation (i.e., accumulation with class hypervectors or search) is carried out.   \n\n\nHDnn-PIM Architecture\n\nFig . 3 shows the HDnn-PIM's tiled architecture. The tiles are grouped into supertiles that allows input reuse among tiles and increases the supported input channels and filter size. Fusion of the tiles becomes necessary when a tile cannot hold all the data necessary to generate an output feature (i.e., the \u00d7 \u00d7 filter and the corresponding convolution window on the input). At most, all the tiles of a supertile can fuse to support a maximum input size, governed \u00d7 \u00d7 \u2264 \u00d7 \u00d7 , where is the filter size, is the number of input channels, is the memory columns per CE, is the number of CEs per tile, and is the number of tiles per supertile. A supertile has the following components.\n\nInput Registers (IR) act as primary inputs to the tiles in the supertile. IRs are divided into sets (IR 1 to IR in Fig. 3 \u2022 3 ) where each set delivers a convolution window of a input channel to the tiles (tiles share the same inputs). The maximum window that can be written to the tile at an instance is equal to the size of the sets. Larger windows may need multiple passes to write a window. The number of input channels that can be simultaneously written is equal to the number of sets. Input Processor (IP) in a supertile takes the IRs as inputs and generates memory addresses to fill the CE memories. Based on the configuration of the supertile, IP concatenates IR data into rows of 1,024 elements that can be written in parallel to the CE memories. Row addresses are generated such that enough space is left for in-memory multiplication. If the layer mapped to the supertile is preceded by pooling, the IP produces addresses to store the corresponding inputs below one another for in-memory pooling.\n\nPost Processing Network (PPN) enables the fusion of tiles, and routes the tiles output to the appropriate supertile input (of the next layer). PPN comprises an activation unit and an accumulator, so applies activation functions to the accumulated outputs of the tiles, as well. It connects the tiles in a binary tree structure as shown in Fig. 3 \u2022 4 . The accumulations in PPNs depends on the number of fused tiles. Fusing happens when the input channels required to produce an output pixel span multiple tiles due to memory limit. The tiled architecture allows parallel accumulation/activation when the input sizes are small and multiple of them fit in a tile.\n\nHistory Memory (HM) is an additional 1,024\u00d71,024 memory block that stores the history of the outputs, and is required because of the fine-grained computation by the compute pipeline. In a pipelined architecture, the current layer needs to provide the input window of the next layer. Hence, in a sliding window mechanism, the next layer will be lagging by at least one column worth of convolution windows. Therefore, this history needs to be stored so the the next layer can accomplish its window. This storage increases when a residual connection is introduced, where a layer may send data several layers ahead and requires to match the exact latency of the data that is passing through all the intermediate layers to reach the residue destination. Thus, this storage becomes indispensable to avoid large power hungry register banks at the output.\n\nFeature Extraction (FE) tile has multiple CEs that perform MACs in parallel. Each tile is equipped with its own top level accumulator to accumulate the partial sums of all the CEs (this is different from the CSA adder of each CE directly connected to the memory block). The output of each CE is one accumulated value. This allows us to avoid data dependence between CEs and keep the tile architecture simple. We consecutively store a convolution window (of all the channels) associated with an output feature, and then move on to store the window corresponding to the next output feature. This allows of maximum use of the available memory size.\n\nThe HDnn-PIM architecture is designed such that supertiles that process the same layer have no data dependency with each other. This increases the throughput of processing a given layer by avoiding data transfers between supertiles that all compute a certain layer. All supertiles of a layer work in parallel.\n\n\nSupertile Data Management\n\nHDnn-PIM architecture statically assigns supertiles to different layers (to be executed pipelined) proportionally based on the compute opportunity they present. The bus interconnect establishes data transfer based on the destination information of each supertile. Data transfer for the next computation is exerted simultaneously with the current computation to throttle the cost of data transfer.\n\nSupertile Mapping: For the mapping of layers on supertiles, the compiler takes the network specifications as input and generates the configuration of HDnn-PIM. The key aspects taken into consideration for a convolution layer for assignment are the filter size , number of input/output channels / , input dimensions , and strides along the and directions ( and ). These parameters are used to calculate the compute intensity ( = 2 \u00d7 ) and configure the supertiles accordingly. The compiler ensures the entire network fits into the CE memories. Otherwise, the network is processed as batches of layers. Parallelism can also be extracted by computing multiple convolution windows of the same input feature in parallel (i.e., multiple filters over multiple window, as in Fig. 2 \u2022 1 ). The number of windows to be processed in parallel for a given layer is the ratio of its CI to the layer with minimum CI. The same procedure is used when the memories are limited. However, now one CE may hold multiple sets of inputs and weights. Multiplication is then performed sequentially for all sets in a CE. This avoids off-chip accesses at the cost of throughput. By using CI, the data rate mismatch due to different strides is also mitigated.\n\nPooling uses simple logic and arithmetic operations and is performed in memory. As explained for the Input Register (IR) functionality, the output of the layer preceding pooling is sent to the memories allocated to subsequent layer. The pooling windows is stored vertically in the memory columns of the next layer. Once the output is obtained, the inputs are no longer required, so the intermediate storage of the in-memory multiplications are released for MAC operations. Large pooling windows that cannot fit in the memory can be processed in multiple iterations.\n\nResidual connections are challenging due to data transfer to multiple layers ahead, which requires the latency of the computation to be matched such that the right inputs are added before activation. This is realized by storing larger histories in the HM for layers that are the producer of the residue and feed the values as required to the consumer. The producer lag is fixed based on the specification of the intermediate layers and the supertile mapping. Fig. 3 \u2022 2 , acts as the backbone of HDnn-PIM that enables all the mapping of supertiles and pipelining between the network layers. The flexibility of using CI to map layers to the supertiles can only be achieved if all the supertiles can communicate with the other supertiles. Each supertile has an ID, and each register set in a supertile has a local register ID. This is used to uniquely identify each supertile and its register sets. After mapping all the layers to the supertiles, the compiler generates a source and a destination ID for each supertile. When data is being sent by a supertile, this metadata is used by the BI to route it to the proper destination. Each node of the BI acts as a switch that compares the destination ID to the threshold of that node (e.g., IDs 0-31 for the first branch and so on) and routes the data to the appropriate branch. Once the data reaches the destination, the supertile uses the register ID to store the value in the right register set.\n\n\nBus Interconnect (BI), shown in\n\nDual bus structure is used because for larger designs the resources of such structure scale linearly due to the type of data transfer involved. Due to the natural staggering of computation among different layers, the memory blocks assigned to the same layer do not receive and send data at the same time. This significantly reduces the maximum bandwidth requirement of the top bus (limiting it to the transfer of the largest layer currently mapped).\n\n\nMapping HD to HDnn-PIM Architecture\n\nHD Encoding in HDnn-PIM: HD encoding is implemented as a matrix (binary)-vector multiplication (VMM). The core CE remains the same as for the FE. The data access patterns in VMM and convolution are different. Thus, we use different FE and HD units on the chip to simplify the FE control logic and mapping. For simple VMM, the tile hierarchy inside the supertiles, the BI, and the HM are not required. We create multiple copies of the input feature vector (output of the cutting layer) that are stored in different supertiles, same as the weights of the FE. After multiplication in memory and accumulating all the products in the supertile, one dimension of the encoding hypervector is generated. The number of outputs that can be generated concurrently is equal to the the number of supertiles of the HD component, which is configured to match the FE throughput. The results of the VMM (encoding) is stored in a separate supertile. The dimensions of the hypervector, which are multi-bit, are then reduced to 1-bit using thresholding.\n\nHD Inference and Training in HDnn-PIM: For inference, we encode the vector using the above architecture to obtain the query hypervector. The class is determined using search in memories by comparing the query hypervector to the stored class hypervectors. This associative search can be performed in-memory as demonstrated in [2]. For HD training all class hypervectors are obtained through FE and encoding followed by accumulating the vectors of the same class. Therefore, we get one vector per image from the FE, that we encode and add it to the class vector to train the HD classifier. We obtain all the class hypervectors once all the images have passed through this procedure. For retraining, we perform inference on each image in the training set and compare the predicted class with the expected class. If they do not match, we add the query hypervector to the expected class's hypervector and subtract it from the predicted class's hypervector. The location of the class hypervectors remain the same memory blocks where they were first stored. All training additions are performed in memory.\n\n\nHDnn-PIM Compute Element\n\nThe CE is the lowest level component of HDnn-PIM, comprising a 1,024\u00d71,024 single-bit ReRAM crossbar capable of in-memory arithmetic. Fig. 4 shows the organization of data in the memory crossbar. Two sets of 1,024 16-bit numbers are stored in adjacent columns, where each number is stored vertically in a single column and 16 rows. The first (second) 16 rows store the first (second) set. The two sets act as multipliers and multiplicands that are being multiplied down the column to produce the product. Therefore, all 1,024 multiplications are performed in parallel. The two inputs, for instance, could be the weights and input feature windows that need to be multiplied for convolution. The two inputs and the product together occupy 48 rows of the crossbar. The remaining (976) bits are used to store another input or act as scratchpads that store the intermediate signals that are generated during the product.\n\nAnother component of the CE is a bit-serial parallel CSA adder. The in-memory addition does not allow this operation optimally since, first, all the columns of the array perform a multiplication and no space is left along the rows (where the products are stored) to perform accumulation. Second, if we change the data arrangement to carry out in-memory accumulation, it is not possible to perform 1,024 16-bit accumulation in parallel in one crossbar. To maintain this parallelism, the bit-serial CSA adder is used. Since the products are stored along the columns, a bit of the same significance of all the numbers can be read out simultaneously and accumulated. This operation is performed 16 times using the CSA adder to produce the accumulated output. Such PIM-CMOS hybrid structure allows the architecture to extract high parallelism by performing more multiplications, which is the most expensive operation. In addition, accumulation is also performed in parallel while reading the data out. Hence, it is hidden within the memory read out latency.\n\n\nEvaluation\n\n\nExperimental Setup\n\nWe implemented an operation-level simulator in Python to analyze the HDnn-PIM which models its architecture, considering the size of operations, data mapping, memory size, and network parameters. The simulator uses the performance and energy consumption values obtained from circuit-level evaluations in 45 nm technology in Cadence Virtuoso. The memory cell characteristics are derived from VTEAM memristor model [10]. We calibrate the model to represent the device characteristics used in [5]. The resultant memory cell has and as 10 M\u03a9 and 10 k\u03a9 respectively, with a device switching delay of 1.1 ns (PIM's design cycle time).\n\nIn our experiments we used CIFAR10/100 and Flowers datasets on VGG-16, MobileNetV2, and ResNet-18/34 networks. HDnn truncates the networks and uses as feature extractors (FEs), which is followed by a HD-based classifier with D=4,000 dimensions. We compare HDnn-PIM with the state-of-the-art PIM accelerators FloatPIM [5] (digital) and ISAAC [14] (analog). We also compare the HDnn-PIM performance with Intel Xeon Gold 6140 CPU and Nvidia RTX 3090 GPU using PyTorch implementation of the HDnn.\n\n\nHDnn Accuracy Analysis\n\nFor feature extraction with VGG-16, we cut at layer L17 (out of 44 layers including batch normalization). For ResNet-18, we cut off the last fully connected layer and six preceding convolution layers.  [5] and ISAAC [14] We cut the MobileNetV2 layers after the fourth bottleneck layer but we preserve its last fully connected layer. All analyses are done for inference using models trained with 16-bit fixed point representation. Our analysis shows that HDnn is on average 51.0%, 49.1%, and 57.3% more accurate than the mere HD model on CIFAR100, CIFAR10, and Flowers datasets, respectively (see Table 1). Fig. 6(a) shows the HDnn accuracy, and Fig. 6(c), and 6(d) show the MAC and parameter reduction of HDnn using the aforementioned trimmed CNNs as the feature extractor, compared to the original CNN. HDnn increases the accuracy by 1.2% on average compared to CNN models for image classification, while reducing the number of MACs and model parameters by 34.8% and 72.5% respectively. Note that we also compared HDnn with the CNN that is cut at the same layer followed by a fully connected layer (instead of HD). Compared to such trimmed ResNet-18 configuration, HDnn achieves 1.8% higher accuracy on CIFAR10 dataset, indicating the effectiveness of HD to gain better insight from data.\n\n\nHDnn-PIM vs State of the Art\n\nCPU and GPU: Fig. 5(a) shows the inference throughput of HDnn models on different platforms. Table 3 summarizes the speedup and energy efficiency gains over GPU relative to other state of the art works (HD only model and models without references are run on HDnn-PIM). Our high performance design (PIM-16GB) achieves 223\u00d7 higher throughput than RTX 3090 GPU, while consuming less memory than the GPU which has 24GB of RAM. The area efficient version (PIM-256MB) is yet 3.6\u00d7 better than GPU. Comparing with CPU, our high performance (area-efficient) design achieved 13,796\u00d7 (219\u00d7) higher throughput.\n\nPrevious PIM designs: None of the existing PIM-based HD accelerators evaluate their design on complex image datasets such as CIFAR100 or Flowers [3,9], instead focusing primarily on MNIST. They either use an inferior HD encoding as compared to the baseline encoding used in HDnn [3,9] or do not evaluate on complex image datasets [13]. Hence, HDnn-PIM is at least 52.4% more accurate on average than existing HD-PIM designs for Flower, CIFAR10, and CIFAR100 datasets (Section 4.2).\n\nWe compare HDnn-PIM with digital-PIM FloatPIM DNN accelerator [5], and analog-PIM ISAAC [14]. We use VGG-16 for comparison since these works are not capable of running networks with residual connections. Our evaluations show that HDnn-PIM has an energy-efficiency of 3,036 GOPS/s/W for 16-bit operations, which is 3.7\u00d7 higher than FloatPIM [5] (Fig. 5(b)). By balancing throughput, \n\n\nModel\n\nSpeedup Energy Efficiency HD (RP) [7] 133x 1215x CNN (VGG- 16) 84x 1202x CNN (VGG-16) [5] 100x 325x HD+FE [13] 14x 259x HDnn (VGG-16-based FE) 104x 1213x HDnn-PIM ensures optimum use of the given resources. HDnn-PIM has per-area performance of 275.5 GOPS/s/mm 2 which is 8.2% (7.7\u00d7) less than FloatPIM's low-power (high-power) version. This is because, unlike FloatPIM, we consider the data-flow and mapping overheads of all DNN layers in HDnn-PIM. This includes, but is not limited to, computation and data mapping requirements in case of pooling, residual connections, and larger strides. HDnn-PIM is 7.9\u00d7 more energy-efficient than ISAAC [14] as it does not use power-hungry mixed-signal circuits such as DAC/ADCs. Since ISAAC merely performs convolution and does not handle other DNN layers, it is 1.7\u00d7 better in per-area performance.\n\n\nHDnn-PIM Performance-Area Tradeoff\n\nWe analyse the impact of scaling the memory size by considering memory sizes ranging from 256 MB to 16 GB using the same HDnn networks described in Section 4.2. As shown in Fig. 6(b), the performance increases linearly with the available memory. This indicates the scalability of HDnn-PIM to fit multiple networks on different configurations without impacting performance drastically.\n\nWe also discuss the performance-efficient and area-efficient versions of HDnn-PIM. The performance-efficient design has 8,192 supertiles with 16 tiles each (total 16 GB). All multiplication operations in the pipeline happen in parallel which provides high computation throughput. For the area-efficient design, we use lower memory size which consumes lower power due to reduced parallelism. We choose a design with 128 supertiles with 16 tiles each (total 256 MB), in which we perform compact data mapping and store multiple rows of weights and inputs in a CE. This results in less parallel multiplications but can fit the network on a significantly smaller chip. Our evaluations show that the performanceefficient (area-efficient) HDnn-PIM has a throughput of 93.3 TOPS/s (1.5 TOPS/s) with an end-to-end latency of 0.14 ms (8.9 ms) for VGG-16, 76.7 TOPS/s (1.2 TOPS/s) with 0.03 ms (2.1 ms) latency for ResNet-18, and 112.3 TOPS/s (1.9 TOPS/s) with 0.13 ms (7.6 ms) latency for MobileNetV2. The higher latency of MobileNet versus ResNet-18 is due to its 1\u00d71 convolutions that make the memory blocks underutilized. Also, MobileNet has higher image size reduction due to the pooling and stride-2 convolutions, requiring more resources for throughput balancing (e.g., a 2\u00d72 pooling requires four times of resources to be allocated to the previous layer).\n\n\nConclusion\n\nWe proposed HDnn, a novel approach of feature extraction for HD computing for complex datasets such as images, and HDnn-PIM, the first PIM based HDnn accelerator. We evaluated HDnn on a variety of feature extractors and datasets. Our evaluation shows that HDnn achieves 52.4% higher accuracy as compared to HD computing without feature extraction. It also gains 1.2% accuracy over state-of-the-art CNNs, but at 72.46% (3.63\u00d7) lower memory footprint and 34.83% (1.53\u00d7) fewer MACs. Furthermore, HDnn-PIM is 223\u00d7 faster than RTX 3090 GPU and 3.7\u00d7 more energy efficient than the state-of-the-art PIM accelerator for DNNs.\n\nFigure 1 :\n1HDnn uses a few of the first convolution layers for feature extraction.\n\nFig. 1\n1compares the HDnn and conventional CNN structure. HDnn comprises three steps: (1) realizing a feature extractor,(2) training of HD classifier, and (3) tuning the feature extractor.\n\nFigure 2 :\n2Mapping flow of the HDnn on PIM. (1) Splitting the input image between supertiles allows input reuse and throughput balancing, and avoids communication between the supertiles that implement the same layer. (2) Tiles of a supertile reuse/share the input on different filters for parallel output production and can perform matrix-matrix multiplication. (3) Compute Elements of a tile split the inputs and reuse the weights to generate output pixel and can perform matrix-vector multiplication. (4) Replication of inputs to perform matrix-vector multiplications for all rows of a matrix in parallel. (5) Column-parallel multiplication in memory. (6) Output of the supertiles passed to the final layer. (7) Random projection encoding using CE array to implement matrix-vector multiplication.\n\nFigure 3 :\n3HDnn-PIM tiled architecture. ST: Supertile; PPN: Post Processing Network; CE: Compute Element.\n\nFigure 4 :\n4CE crossbar and associated bit-serial parallel CSA adder.\n\nFigure 5 :\n5a) HDnn inference throughput b) Performance per Watt of HDnn-PIM vs. FloatPIM\n\nFigure 6 :\n6(a) HDnn accuracy for different datasets leveraging different CNNs as FEs, (b) Performance scaling of HDnn-PIM, (c) HDnn MAC reduction using different FEs, (d) HDnn parameter reduction using using different FEs for various datasets.\n\n\n, Irvine, CA, USA. \u00a9 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9322-5/22/06. https://doi.org/10.1145/3526241.3530331\n\nTable 1 :\n1Accuracy comparison of HD, CNN, and HDnn.Model\u2193 Dataset\u2192 MNIST CIFAR10 CIFAR100 Flowers \nHD (RP) [7] \n94% \n26.9% \n9% \n19.6% \nHD (non-linear) [18] \n97% \n45.5% \n27.7% \n31.5% \nStocHD [13] \n98% \nN/A \nN/A \nN/A \nCNN ([4]) \n99% \n94.6% \n78.7% \n84.7% \nHDnn ([4]-based FE) \n99% \n95.1% \n78.3% \n88.8% \n\n\n\nTable 2 :\n2ResNet34-based feature extractor on CIFAR100.HDnn \nAccuracy \nMAC (M) \nParameter (M) \n[3, 4, 6, 0] 77.07% (\u22120.34%) 951.6 (\u221218.1%) \n8.56 (\u221259.9%) \n[3, 4, 6, 1] 78.30% (+0.89%) 1010.4 (\u221213.0%) 12.29 (\u221242.4%) \n[3, 3, 3, 1] 77.33% (\u22120.08%) 708.1 (\u221239.0%) \n8.45 (\u221260.4%) \n\n\nTable 3 :\n3Performance-Power normalized to GPU RTX 3090.\nAcknowledgementsThis work was supported by TSMC, in part by CRISP, one of six centers in JUMP (an SRC program sponsored by DARPA), SRC Global Research Collaboration (GRC) grant, and NSF grants #1911095, #1826967, #2100237, and #2112167.\nPrime: A novel processing-in-memory architecture for neural network computation in reram-based main memory. Ping Chi, Shuangchen Li, ACM SIGARCH Computer Architecture News. 44Ping Chi, Shuangchen Li, et al. 2016. Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory. ACM SIGARCH Computer Architecture News 44, 3 (2016), 27-39.\n\nAssociative memristive memory for approximate computing in gpus. Amirali Ghofrani, IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 6Amirali Ghofrani et al. 2016. Associative memristive memory for approximate computing in gpus. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 6, 2 (2016), 222-234.\n\nFelix: Fast and energyefficient logic in memory. Saransh Gupta, Mohsen Imani, Tajana Rosing, IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEESaransh Gupta, Mohsen Imani, and Tajana Rosing. 2018. Felix: Fast and energy- efficient logic in memory. In 2018 IEEE/ACM International Conference on Computer- Aided Design (ICCAD). IEEE, 1-7.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nFloatpim: In-memory acceleration of deep neural network training with high precision. Mohsen Imani, Saransh Gupta, Yeseong Kim, Tajana Rosing, 46th International Symposium on Computer Architecture (ISCA). IEEEMohsen Imani, Saransh Gupta, Yeseong Kim, and Tajana Rosing. 2019. Floatpim: In-memory acceleration of deep neural network training with high precision. In 46th International Symposium on Computer Architecture (ISCA). IEEE, 802-815.\n\nVoiceHD: Hyperdimensional Computing for Efficient Speech Recognition. Mohsen Imani, Deqian Kong, 2017 IEEE International Conference on Rebooting Computing (ICRC. Abbas Rahimi, and Tajana RosingMohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. 2017. VoiceHD: Hyperdimensional Computing for Efficient Speech Recognition. In 2017 IEEE International Conference on Rebooting Computing (ICRC). 1-8.\n\nBric: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. Mohsen Imani, Justin Morris, 56th Annual Design Automation Conference. Mohsen Imani, Justin Morris, et al. 2019. Bric: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. In 56th Annual Design Automation Conference. 1-6.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive computation. 1Pentti Kanerva. 2009. Hyperdimensional computing: An introduction to com- puting in distributed representation with high-dimensional random vectors. Cognitive computation 1, 2 (2009), 139-159.\n\nGeethan Karunaratne, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abbas Rahimi, Abu Sebastian, arXiv:1906.01548-memory hyperdimensional computing. arXiv preprintGeethan Karunaratne, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abbas Rahimi, and Abu Sebastian. 2019. In-memory hyperdimensional computing. arXiv preprint arXiv:1906.01548 (2019).\n\nVTEAM: A general model for voltage-controlled memristors. Shahar Kvatinsky, Misbah Ramadan, IEEE Transactions on Circuits and Systems II: Express Briefs. 62Shahar Kvatinsky, Misbah Ramadan, et al. 2015. VTEAM: A general model for voltage-controlled memristors. IEEE Transactions on Circuits and Systems II: Express Briefs 62, 8 (2015), 786-790.\n\nHyDREA: Towards More Robust and Efficient Machine Learning Systems with Hyperdimensional Computing. Justin Morris, Kazim Ergun, 2021Justin Morris, Kazim Ergun, et al. 2021. HyDREA: Towards More Robust and Efficient Machine Learning Systems with Hyperdimensional Computing. In 2021\n\nAutomation Test in Europe Conference Exhibition (DATE). Design, Design, Automation Test in Europe Conference Exhibition (DATE). 723-728.\n\nSynergicLearning: Neural Network-Based Feature Extraction for Highly-Accurate Hyperdimensional Learning. Mahdi Nazemi, Amirhossein Esmaili, International Conference On Computer Aided Design (ICCAD. Mahdi Nazemi, Amirhossein Esmaili, et al. 2020. SynergicLearning: Neural Network-Based Feature Extraction for Highly-Accurate Hyperdimensional Learn- ing. In International Conference On Computer Aided Design (ICCAD). 1-9.\n\nStocHD: Stochastic Hyperdimensional System for Efficient and Robust Learning from Raw Data. Prathyush Poduval, Zhuowen Zou, Hassan Najafi, ACM/IEEE Design Automation Conference (DAC. Prathyush Poduval, Zhuowen Zou, Hassan Najafi, et al. 2021. StocHD: Stochastic Hyperdimensional System for Efficient and Robust Learning from Raw Data. In ACM/IEEE Design Automation Conference (DAC). 1195-1200.\n\nISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. Ali Shafiee, Anirban Nag, ACM SIGARCH Computer Architecture News. 44Ali Shafiee, Anirban Nag, et al. 2016. ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. ACM SIGARCH Computer Architecture News 44, 3 (2016), 14-26.\n\nLogic design within memristive memories using memristor-aided loGIC (MAGIC). Nishil Talati, Saransh Gupta, Pravin Mane, Shahar Kvatinsky, IEEE Transactions on Nanotechnology. 15Nishil Talati, Saransh Gupta, Pravin Mane, and Shahar Kvatinsky. 2016. Logic design within memristive memories using memristor-aided loGIC (MAGIC). IEEE Transactions on Nanotechnology 15, 4 (2016), 635-650.\n\nAnthony Thomas, arXiv:2010.07426Sanjoy Dasgupta, and Tajana Rosing. 2020. Theoretical Foundations of Hyperdimensional Computing. Anthony Thomas, Sanjoy Dasgupta, and Tajana Rosing. 2020. Theoretical Foun- dations of Hyperdimensional Computing. arXiv:2010.07426 (2020).\n\nZhuowen Zou, Haleh Alimohamadi, Farhad Imani, arXiv:2110.00214Yeseong Kim, and Mohsen Imani. 2021. Spiking Hyperdimensional Network: Neuromorphic Models Integrated with Memory-Inspired Framework. cs.NEZhuowen Zou, Haleh Alimohamadi, Farhad Imani, Yeseong Kim, and Mohsen Imani. 2021. Spiking Hyperdimensional Network: Neuromorphic Models Inte- grated with Memory-Inspired Framework. arXiv:2110.00214 [cs.NE]\n\nManiHD: Efficient Hyper-Dimensional Learning Using Manifold Trainable Encoder. Zhuowen Zou, Yeseong Kim, M Hassan Najafi, Mohsen Imani, 2021Zhuowen Zou, Yeseong Kim, M. Hassan Najafi, and Mohsen Imani. 2021. ManiHD: Efficient Hyper-Dimensional Learning Using Manifold Trainable Encoder. In 2021\n\nAutomation Test in Europe Conference Exhibition (DATE). Design, Design, Automation Test in Europe Conference Exhibition (DATE). 850-855.\n", "annotations": {"author": "[{\"end\":192,\"start\":94},{\"end\":293,\"start\":193},{\"end\":399,\"start\":294},{\"end\":496,\"start\":400},{\"end\":592,\"start\":497},{\"end\":693,\"start\":593}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":100},{\"end\":206,\"start\":201},{\"end\":309,\"start\":301},{\"end\":425,\"start\":411},{\"end\":507,\"start\":505},{\"end\":606,\"start\":600}]", "author_first_name": "[{\"end\":99,\"start\":94},{\"end\":200,\"start\":193},{\"end\":300,\"start\":294},{\"end\":410,\"start\":400},{\"end\":504,\"start\":497},{\"end\":599,\"start\":593}]", "author_affiliation": "[{\"end\":191,\"start\":123},{\"end\":292,\"start\":224},{\"end\":398,\"start\":330},{\"end\":495,\"start\":427},{\"end\":591,\"start\":523},{\"end\":692,\"start\":624}]", "title": "[{\"end\":91,\"start\":1},{\"end\":784,\"start\":694}]", "venue": null, "abstract": "[{\"end\":1804,\"start\":931}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1855,\"start\":1852},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1858,\"start\":1855},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2217,\"start\":2214},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2220,\"start\":2217},{\"end\":2821,\"start\":2807},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2889,\"start\":2886},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2892,\"start\":2889},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3031,\"start\":3027},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3471,\"start\":3468},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3602,\"start\":3599},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3604,\"start\":3602},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3606,\"start\":3604},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3608,\"start\":3606},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3921,\"start\":3918},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3946,\"start\":3942},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4275,\"start\":4271},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4528,\"start\":4525},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4530,\"start\":4528},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4533,\"start\":4530},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4674,\"start\":4671},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4677,\"start\":4674},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4877,\"start\":4874},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4880,\"start\":4877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5145,\"start\":5142},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6632,\"start\":6629},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7704,\"start\":7701},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21073,\"start\":21070},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24294,\"start\":24290},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24370,\"start\":24367},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24827,\"start\":24824},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24852,\"start\":24848},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25231,\"start\":25228},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25246,\"start\":25242},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27096,\"start\":27093},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27098,\"start\":27096},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27230,\"start\":27227},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27232,\"start\":27230},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27282,\"start\":27278},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27496,\"start\":27493},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27523,\"start\":27519},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27774,\"start\":27771},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27860,\"start\":27857},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27885,\"start\":27882},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27912,\"start\":27909},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27933,\"start\":27929},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28468,\"start\":28464},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31280,\"start\":31277}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31155,\"start\":31071},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31345,\"start\":31156},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32146,\"start\":31346},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32254,\"start\":32147},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32325,\"start\":32255},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32416,\"start\":32326},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32662,\"start\":32417},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32803,\"start\":32663},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33107,\"start\":32804},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33386,\"start\":33108},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33444,\"start\":33387}]", "paragraph": "[{\"end\":3657,\"start\":1820},{\"end\":4433,\"start\":3659},{\"end\":5367,\"start\":4435},{\"end\":5758,\"start\":5369},{\"end\":7485,\"start\":5760},{\"end\":7770,\"start\":7511},{\"end\":7959,\"start\":7772},{\"end\":8279,\"start\":7983},{\"end\":8842,\"start\":8281},{\"end\":9152,\"start\":8844},{\"end\":9973,\"start\":9218},{\"end\":10861,\"start\":9975},{\"end\":11331,\"start\":10863},{\"end\":12037,\"start\":11357},{\"end\":13045,\"start\":12039},{\"end\":13708,\"start\":13047},{\"end\":14557,\"start\":13710},{\"end\":15204,\"start\":14559},{\"end\":15515,\"start\":15206},{\"end\":15941,\"start\":15545},{\"end\":17173,\"start\":15943},{\"end\":17740,\"start\":17175},{\"end\":19185,\"start\":17742},{\"end\":19670,\"start\":19221},{\"end\":20743,\"start\":19710},{\"end\":21843,\"start\":20745},{\"end\":22787,\"start\":21872},{\"end\":23841,\"start\":22789},{\"end\":24505,\"start\":23877},{\"end\":24999,\"start\":24507},{\"end\":26315,\"start\":25026},{\"end\":26946,\"start\":26348},{\"end\":27429,\"start\":26948},{\"end\":27813,\"start\":27431},{\"end\":28661,\"start\":27823},{\"end\":29084,\"start\":28700},{\"end\":30438,\"start\":29086},{\"end\":31070,\"start\":30453}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7982,\"start\":7960}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":2829,\"start\":2822},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":3286,\"start\":3279},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":3480,\"start\":3473},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8874,\"start\":8867},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25629,\"start\":25622},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26448,\"start\":26441}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1818,\"start\":1806},{\"attributes\":{\"n\":\"2\"},\"end\":7509,\"start\":7488},{\"attributes\":{\"n\":\"3\"},\"end\":9176,\"start\":9155},{\"attributes\":{\"n\":\"3.1\"},\"end\":9216,\"start\":9179},{\"attributes\":{\"n\":\"3.2\"},\"end\":11355,\"start\":11334},{\"attributes\":{\"n\":\"3.3\"},\"end\":15543,\"start\":15518},{\"end\":19219,\"start\":19188},{\"attributes\":{\"n\":\"3.4\"},\"end\":19708,\"start\":19673},{\"attributes\":{\"n\":\"3.5\"},\"end\":21870,\"start\":21846},{\"attributes\":{\"n\":\"4\"},\"end\":23854,\"start\":23844},{\"attributes\":{\"n\":\"4.1\"},\"end\":23875,\"start\":23857},{\"attributes\":{\"n\":\"4.2\"},\"end\":25024,\"start\":25002},{\"attributes\":{\"n\":\"4.3\"},\"end\":26346,\"start\":26318},{\"end\":27821,\"start\":27816},{\"attributes\":{\"n\":\"4.4\"},\"end\":28698,\"start\":28664},{\"attributes\":{\"n\":\"5\"},\"end\":30451,\"start\":30441},{\"end\":31082,\"start\":31072},{\"end\":31163,\"start\":31157},{\"end\":31357,\"start\":31347},{\"end\":32158,\"start\":32148},{\"end\":32266,\"start\":32256},{\"end\":32337,\"start\":32327},{\"end\":32428,\"start\":32418},{\"end\":32814,\"start\":32805},{\"end\":33118,\"start\":33109},{\"end\":33397,\"start\":33388}]", "table": "[{\"end\":33107,\"start\":32857},{\"end\":33386,\"start\":33165}]", "figure_caption": "[{\"end\":31155,\"start\":31084},{\"end\":31345,\"start\":31165},{\"end\":32146,\"start\":31359},{\"end\":32254,\"start\":32160},{\"end\":32325,\"start\":32268},{\"end\":32416,\"start\":32339},{\"end\":32662,\"start\":32430},{\"end\":32803,\"start\":32665},{\"end\":32857,\"start\":32816},{\"end\":33165,\"start\":33120},{\"end\":33444,\"start\":33399}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3274,\"start\":3268},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8459,\"start\":8453},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9910,\"start\":9904},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10087,\"start\":10081},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10380,\"start\":10374},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10537,\"start\":10531},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11364,\"start\":11361},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12160,\"start\":12154},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13392,\"start\":13386},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16722,\"start\":16710},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18207,\"start\":18201},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22012,\"start\":22006},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25641,\"start\":25632},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25677,\"start\":25671},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26370,\"start\":26361},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27785,\"start\":27775},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28882,\"start\":28873}]", "bib_author_first_name": "[{\"end\":33794,\"start\":33790},{\"end\":33810,\"start\":33800},{\"end\":34136,\"start\":34129},{\"end\":34461,\"start\":34454},{\"end\":34475,\"start\":34469},{\"end\":34489,\"start\":34483},{\"end\":34817,\"start\":34810},{\"end\":34829,\"start\":34822},{\"end\":34845,\"start\":34837},{\"end\":34855,\"start\":34851},{\"end\":35305,\"start\":35299},{\"end\":35320,\"start\":35313},{\"end\":35335,\"start\":35328},{\"end\":35347,\"start\":35341},{\"end\":35732,\"start\":35726},{\"end\":35746,\"start\":35740},{\"end\":36160,\"start\":36154},{\"end\":36174,\"start\":36168},{\"end\":36543,\"start\":36537},{\"end\":36778,\"start\":36771},{\"end\":36798,\"start\":36792},{\"end\":36801,\"start\":36799},{\"end\":36817,\"start\":36809},{\"end\":36833,\"start\":36829},{\"end\":36847,\"start\":36842},{\"end\":36859,\"start\":36856},{\"end\":37190,\"start\":37184},{\"end\":37208,\"start\":37202},{\"end\":37578,\"start\":37572},{\"end\":37592,\"start\":37587},{\"end\":38002,\"start\":37997},{\"end\":38022,\"start\":38011},{\"end\":38414,\"start\":38405},{\"end\":38431,\"start\":38424},{\"end\":38443,\"start\":38437},{\"end\":38806,\"start\":38803},{\"end\":38823,\"start\":38816},{\"end\":39149,\"start\":39143},{\"end\":39165,\"start\":39158},{\"end\":39179,\"start\":39173},{\"end\":39192,\"start\":39186},{\"end\":39458,\"start\":39451},{\"end\":39728,\"start\":39721},{\"end\":39739,\"start\":39734},{\"end\":39759,\"start\":39753},{\"end\":40216,\"start\":40209},{\"end\":40229,\"start\":40222},{\"end\":40236,\"start\":40235},{\"end\":40243,\"start\":40237},{\"end\":40258,\"start\":40252}]", "bib_author_last_name": "[{\"end\":33798,\"start\":33795},{\"end\":33813,\"start\":33811},{\"end\":34145,\"start\":34137},{\"end\":34467,\"start\":34462},{\"end\":34481,\"start\":34476},{\"end\":34496,\"start\":34490},{\"end\":34820,\"start\":34818},{\"end\":34835,\"start\":34830},{\"end\":34849,\"start\":34846},{\"end\":34859,\"start\":34856},{\"end\":35311,\"start\":35306},{\"end\":35326,\"start\":35321},{\"end\":35339,\"start\":35336},{\"end\":35354,\"start\":35348},{\"end\":35738,\"start\":35733},{\"end\":35751,\"start\":35747},{\"end\":36166,\"start\":36161},{\"end\":36181,\"start\":36175},{\"end\":36551,\"start\":36544},{\"end\":36790,\"start\":36779},{\"end\":36807,\"start\":36802},{\"end\":36827,\"start\":36818},{\"end\":36840,\"start\":36834},{\"end\":36854,\"start\":36848},{\"end\":36869,\"start\":36860},{\"end\":37200,\"start\":37191},{\"end\":37216,\"start\":37209},{\"end\":37585,\"start\":37579},{\"end\":37598,\"start\":37593},{\"end\":37816,\"start\":37810},{\"end\":38009,\"start\":38003},{\"end\":38030,\"start\":38023},{\"end\":38422,\"start\":38415},{\"end\":38435,\"start\":38432},{\"end\":38450,\"start\":38444},{\"end\":38814,\"start\":38807},{\"end\":38827,\"start\":38824},{\"end\":39156,\"start\":39150},{\"end\":39171,\"start\":39166},{\"end\":39184,\"start\":39180},{\"end\":39202,\"start\":39193},{\"end\":39465,\"start\":39459},{\"end\":39732,\"start\":39729},{\"end\":39751,\"start\":39740},{\"end\":39765,\"start\":39760},{\"end\":40220,\"start\":40217},{\"end\":40233,\"start\":40230},{\"end\":40250,\"start\":40244},{\"end\":40264,\"start\":40259},{\"end\":40488,\"start\":40482}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52824162},\"end\":34062,\"start\":33682},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2837501},\"end\":34403,\"start\":34064},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53235957},\"end\":34762,\"start\":34405},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206594692},\"end\":35211,\"start\":34764},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":189818835},\"end\":35654,\"start\":35213},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":21351739},\"end\":36058,\"start\":35656},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":163164623},\"end\":36410,\"start\":36060},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":733980},\"end\":36769,\"start\":36412},{\"attributes\":{\"doi\":\"arXiv:1906.01548\",\"id\":\"b8\"},\"end\":37124,\"start\":36771},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206655679},\"end\":37470,\"start\":37126},{\"attributes\":{\"id\":\"b10\"},\"end\":37752,\"start\":37472},{\"attributes\":{\"id\":\"b11\"},\"end\":37890,\"start\":37754},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":220871747},\"end\":38311,\"start\":37892},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":243902037},\"end\":38706,\"start\":38313},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6329628},\"end\":39064,\"start\":38708},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14637261},\"end\":39449,\"start\":39066},{\"attributes\":{\"doi\":\"arXiv:2010.07426\",\"id\":\"b16\"},\"end\":39719,\"start\":39451},{\"attributes\":{\"doi\":\"arXiv:2110.00214\",\"id\":\"b17\"},\"end\":40128,\"start\":39721},{\"attributes\":{\"id\":\"b18\"},\"end\":40424,\"start\":40130},{\"attributes\":{\"id\":\"b19\"},\"end\":40562,\"start\":40426}]", "bib_title": "[{\"end\":33788,\"start\":33682},{\"end\":34127,\"start\":34064},{\"end\":34452,\"start\":34405},{\"end\":34808,\"start\":34764},{\"end\":35297,\"start\":35213},{\"end\":35724,\"start\":35656},{\"end\":36152,\"start\":36060},{\"end\":36535,\"start\":36412},{\"end\":37182,\"start\":37126},{\"end\":37995,\"start\":37892},{\"end\":38403,\"start\":38313},{\"end\":38801,\"start\":38708},{\"end\":39141,\"start\":39066}]", "bib_author": "[{\"end\":33800,\"start\":33790},{\"end\":33815,\"start\":33800},{\"end\":34147,\"start\":34129},{\"end\":34469,\"start\":34454},{\"end\":34483,\"start\":34469},{\"end\":34498,\"start\":34483},{\"end\":34822,\"start\":34810},{\"end\":34837,\"start\":34822},{\"end\":34851,\"start\":34837},{\"end\":34861,\"start\":34851},{\"end\":35313,\"start\":35299},{\"end\":35328,\"start\":35313},{\"end\":35341,\"start\":35328},{\"end\":35356,\"start\":35341},{\"end\":35740,\"start\":35726},{\"end\":35753,\"start\":35740},{\"end\":36168,\"start\":36154},{\"end\":36183,\"start\":36168},{\"end\":36553,\"start\":36537},{\"end\":36792,\"start\":36771},{\"end\":36809,\"start\":36792},{\"end\":36829,\"start\":36809},{\"end\":36842,\"start\":36829},{\"end\":36856,\"start\":36842},{\"end\":36871,\"start\":36856},{\"end\":37202,\"start\":37184},{\"end\":37218,\"start\":37202},{\"end\":37587,\"start\":37572},{\"end\":37600,\"start\":37587},{\"end\":37818,\"start\":37810},{\"end\":38011,\"start\":37997},{\"end\":38032,\"start\":38011},{\"end\":38424,\"start\":38405},{\"end\":38437,\"start\":38424},{\"end\":38452,\"start\":38437},{\"end\":38816,\"start\":38803},{\"end\":38829,\"start\":38816},{\"end\":39158,\"start\":39143},{\"end\":39173,\"start\":39158},{\"end\":39186,\"start\":39173},{\"end\":39204,\"start\":39186},{\"end\":39467,\"start\":39451},{\"end\":39734,\"start\":39721},{\"end\":39753,\"start\":39734},{\"end\":39767,\"start\":39753},{\"end\":40222,\"start\":40209},{\"end\":40235,\"start\":40222},{\"end\":40252,\"start\":40235},{\"end\":40266,\"start\":40252},{\"end\":40490,\"start\":40482}]", "bib_venue": "[{\"end\":33853,\"start\":33815},{\"end\":34215,\"start\":34147},{\"end\":34564,\"start\":34498},{\"end\":34945,\"start\":34861},{\"end\":35416,\"start\":35356},{\"end\":35816,\"start\":35753},{\"end\":36223,\"start\":36183},{\"end\":36574,\"start\":36553},{\"end\":36921,\"start\":36887},{\"end\":37278,\"start\":37218},{\"end\":37570,\"start\":37472},{\"end\":37808,\"start\":37754},{\"end\":38088,\"start\":38032},{\"end\":38494,\"start\":38452},{\"end\":38867,\"start\":38829},{\"end\":39239,\"start\":39204},{\"end\":39578,\"start\":39483},{\"end\":39915,\"start\":39783},{\"end\":40207,\"start\":40130},{\"end\":40480,\"start\":40426},{\"end\":35016,\"start\":34947}]"}}}, "year": 2023, "month": 12, "day": 17}