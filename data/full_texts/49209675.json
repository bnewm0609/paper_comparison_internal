{"id": 49209675, "updated": "2023-09-30 20:40:24.907", "metadata": {"title": "Single Image Reflection Separation with Perceptual Losses", "authors": "[{\"first\":\"Xuaner\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ren\",\"last\":\"Ng\",\"middle\":[]},{\"first\":\"Qifeng\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": 6, "day": 14}, "abstract": "We present an approach to separating reflection from a single image. The approach uses a fully convolutional network trained end-to-end with losses that exploit low-level and high-level image information. Our loss function includes two perceptual losses: a feature loss from a visual perception network, and an adversarial loss that encodes characteristics of images in the transmission layers. We also propose a novel exclusion loss that enforces pixel-level layer separation. We create a dataset of real-world images with reflection and corresponding ground-truth transmission layers for quantitative evaluation and model training. We validate our method through comprehensive quantitative experiments and show that our approach outperforms state-of-the-art reflection removal methods in PSNR, SSIM, and perceptual user study. We also extend our method to two other image enhancement tasks to demonstrate the generality of our approach.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1806.05376", "mag": "2951679348", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhangNC18a", "doi": "10.1109/cvpr.2018.00503"}}, "content": {"source": {"pdf_hash": "57cef6568b1e0e81247e125870b8f8d0db903a0c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1806.05376v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1806.05376", "status": "GREEN"}}, "grobid": {"id": "0c4df0d0c8f9dc7d1d1a7d75c88704bdc803c688", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/57cef6568b1e0e81247e125870b8f8d0db903a0c.txt", "contents": "\nSingle Image Reflection Separation with Perceptual Losses\n\n\nXuaner Zhang \nUC Berkeley\nIntel Labs\n\n\nRen Ng \nUC Berkeley\nIntel Labs\n\n\nU C Berkeley \nUC Berkeley\nIntel Labs\n\n\nQifeng Chen \nUC Berkeley\nIntel Labs\n\n\nSingle Image Reflection Separation with Perceptual Losses\n\nWe present an approach to separating reflection from a single image. The approach uses a fully convolutional network trained end-to-end with losses that exploit low-level and high-level image information. Our loss function includes two perceptual losses: a feature loss from a visual perception network, and an adversarial loss that encodes characteristics of images in the transmission layers. We also propose a novel exclusion loss that enforces pixel-level layer separation. We create a dataset of real-world images with reflection and corresponding ground-truth transmission layers for quantitative evaluation and model training. We validate our method through comprehensive quantitative experiments and show that our approach outperforms state-of-the-art reflection removal methods in PSNR, SSIM, and perceptual user study. We also extend our method to two other image enhancement tasks to demonstrate the generality of our approach.\n\nIntroduction\n\nReflection from windows and glasses is ubiquitous in the real world, but it is usually undesirable in photographs. Users often want to extract the hidden clean transmission image by removing reflection from an image. For example, we may have been tempted to take photos through an aquarium glass or skyscraper windows, but reflection can often damage the image quality. Removing reflection from a single image allows us to recover visual content with better perceptibility. Thus, separating the reflection layer and transmission layer from an image -the reflection separation problem -is an active research area in computer vision.\n\nLet I \u2208 R m\u00d7n\u00d73 be the input image with reflection. I can be approximately modeled as the sum of the transmission layer T and the reflection layer R: I = T +R. Our goal is to recover the transmission layer T given I, which is an ill-posed problem without additional constraints or priors.\n\nAs the reflection separation problem is ill-posed, prior works often require additional input images and hardcrafted priors. A line of previous research uses multiple im-ages as input or requires explicit user guidance [9,27,32]. Multiple images, however, are not always available in practice, and user guidance is inconvenient and error-prone. Recent researchers proposed methods for reflection removal from a single image [25,21], but these approaches rely on hand-crafted priors such as ghost cues and relative smoothness which may not generalize to all images with reflection. More recently, CEILNet [5] uses a deep neural network to train a model with low-level losses on color and edges, but this approach does not directly enable the model to learn high-level semantics which can be highly useful for reflection removal. Low-level information is insufficient for reflection separation when there is color ambiguity or the model needs to \"recognize\" objects in the image. For example, in Figure 1, our model trained with perceptual losses may have learned the representations of lamps and faces, and thus correctly removes them from the input image, while CEILNet fails to do so.\n\nIn this paper, we present a fully convolutional network with perceptual losses that encode both low-level and highlevel image information. Our network takes a single image as input and directly synthesizes two images: the reflection layer and the transmission layer. We further propose a novel exclusion loss that effectively enforces the separation of transmission and reflection at pixel level. To thoroughly evaluate and train different approaches, we build a dataset that contains real-world images and the ground-truth transmission images. Our dataset covers diverse natural environments including indoor and outdoor scenes. We also use this real-world dataset to compare our approach quantitatively to previous methods. In summary, our main contributions are:\n\n\u2022 We propose to use a deep neural network with perceptual losses for single image reflection separation. We impose perceptual supervision through two losses with different levels of image information: a feature loss from a visual perception network, and an adversarial loss to refine the output transmission layer.\n\n\u2022 We propose a carefully designed exclusion loss that emphasizes independence of the layers to be separated in the gradient domain.  : Results by CEILNet [5] and our approach on real-world images. The top row shows a real image from the CEILNet dataset with a window reflecting a poster of a human face; the bottom row shows an image taken by ourselves, with a lamp as the reflection. From left to right: the input images, CEILNet results and our results. Note that our approach trained to learn both low-level and high-level image statistics successfully removes the reflection layers of the face and lamp, while CEILNet does not.\n\n\u2022 We build a dataset of real-world images for reflection removal with corresponding ground-truth transmission layers. This new dataset enables quantitative evaluation and comparisons among our approach and existing algorithms.\n\n\u2022 Our extensive experiments on real data and synthetic data indicate that our method outperforms state-of-theart methods in SSIM, PSNR, and a perceptual user study on Amazon Mechanical Turk. Our trained model on reflection separation can be directly applied to two other image enhancement tasks, flare removal and dehazing.\n\n\nRelated Work\n\nMultiple-image methods. As the reflection separation problem is ill-posed, most previous work tackles this problem with multiple input images. These multi-image approaches often use motion cues to separate the transmission and reflection layers [32,9,20,28,23,6,29,10]. The motion cues are either inferred from calibrated cameras, or motion parallax that assumes the background and reflection objects have greatly different motion fields. Some other multi-image approaches include the use of flash and noflash image pairs to improve the flash image with reflection removed [1]. Schechner et al. [24] use a sequence of images with different focus settings to separate layers with depth estimation. Kong et al. [15] exploit physical properties of polarization and use multiple polarized images taken with angular filters to find the optimal separation. More recently, Han and Sim [10] tackle the glass reflection removal problem with multiple glass images, assuming that the gradient field in background image is almost constant while the gradient field in reflection varies much more. Although multiple-image methods have shown promising performance in removing reflection, capturing multiple images is sometimes impossible, for example, these methods can not be applied to existing or legacy photographs.\n\nSingle-image methods. Another line of work considers using a single image with predefined priors. A widely used prior is the natural image gradient sparsity [19,18] to find minimum edges and corners for layer decomposition. The gradient sparsity prior is also explored together with optimal and minimum user assistance to better guide the illposed separation problem [17,27]. A recent work by Arvanitopoulos et al. [2] uses the gradient sparsity constraint, combined with a data fidelity term in the Laplacian space to suppress reflection. However, all these approaches rely on low-level heuristics and are limited in cases where a highlevel understanding of the image is needed. Another prior for reflection separation is that the reflection layer is often out of focus and appears smooth. This is explicitly formulated into an optimization objective by Li and Brown [21], in which they penalize large reflection gradients. Although the assumption of relative smoothness is valid, their formulation can break down when the reflection layer has high contrast. Wan et al. [31] propose a variation of this smoothness prior where depth of field is used as guidance for edge labeling and layer separation. Additionally, Shih et al. [25] focus on a subset of the problem where reflection has ghost effects, and use estimated convolution kernel to optimize for reflection removal. Fan et al. [5] recently propose a deep learning network, the Cascaded Edge and Image Learning Network (CEIL-Net), for reflection removal. They formulate reflection removal as an edge simplification task and learn an intermediate edge map to guide layer separation. CEILNet is trained purely with a low-level loss that combines the differences in color space and gradient domain. The main difference between CEILNet and ours is that they did not explicitly utilize perceptual information during training.\n\nBenchmark datasets. A benchmark dataset by Wan et al. [30] was proposed recently for reflection removal. The authors collected 1500 real images of 40 scenes in a controlled lab environment by imaging pairs of daily objects and postcards, as well as 100 scenes in natural outdoor environments with three different pieces of glasses. However, the dataset has not been released publicly yet at the time of submission. In order to evaluate among different models quantitatively on real-world images, we collect a dataset of 110 real images with ground truth in natural scene environments.\n\n\nOverview\n\nGiven an image I \u2208 [0, 1] m\u00d7n\u00d73 with reflection, our approach decomposes I into a transmission layer f T (I; \u03b8) and a reflection layer f R (I; \u03b8) using a single network f (I; \u03b8) = (f T (I; \u03b8), f R (I; \u03b8)), where \u03b8 is the network weights. We train the network f on a dataset D = {(I, T, R)} where I is the input image, T is the transmission layer of I, and R is the reflection layer of I.\n\nOur loss function contains three terms: a feature loss L feat by comparing the images in feature space, and an adversarial loss L adv for realistic image refinement, an exclu-sion loss L excl that enforces separation of the transmission and reflection layers in the gradient domain. Our overall loss function is\nL(\u03b8) = w 1 L feat (\u03b8) + w 2 L adv (\u03b8) + w 3 L excl (\u03b8),(1)\nwhere we set w 1 = 0.1, w 2 = 0.01 and w 3 = 1 to balance the weight of each term. An ideal model for reflection separation should be able to understand contents in an image. To train our network f with semantic understanding of the input image, we form hypercolumn features [11] by extracting features from a VGG-19 [26] network pre-trained on the ImageNet dataset [22]. The benefit of using hypercolumn features is that the input is augmented with useful features that abstract visual perception of a large dataset such as Im-ageNet. The hypercolumn feature at a given pixel location is a stack of activation units across selected layers of a network at that location. Here, we sampled the layers 'conv1 2', 'conv2 2', 'conv3 2', 'conv4 2', and 'conv5 2' in the pre-trained VGG-19 network. The hypercolumn feature has 1472 dimensions in total. We concatenate the input image I with its hypercolumn features as the augmented input for f .\n\nOur network f is a fully convolutional network that has a similar network architecture to the context aggregation network [33,4]. Our network has a large receptive field of 513 \u00d7 513 to effectively aggregate global image information. The first layer of f is a 1 \u00d7 1 convolution to reduce feature dimension (1472+3) to 64. The following 8 layers are 3 \u00d7 3 dilated convolutions. The dilation rate varies from 1 to 128. All the intermediate layers have 64 feature channels. For the last layer we use a linear transformation to synthesize 2 images in the RGB color space.\n\nWe evaluate different methods on the publicly available synthetic and real images from the CEILNet dataset [5] and the real-world dataset we collected. We compare our method to the state-of-the-art reflection removal approach CEILNet [5], an optimization based approach [21], and Pix2pix [12], a general framework for image translation.\n\n\nTraining\n\n\nFeature loss\n\nWe use a feature loss to measure the difference between our predicted transmission layer and the ground-truth transmission in feature space. As the aforementioned observation in Figure 1 shows, semantic reasoning about the scene would benefit the task of reflection removal. A feature loss that combines low-level and high-level features from a perception network would serve our purpose. Feature loss has also been successfully applied to other tasks such as image synthesis and style transfer [3,7,16,13].\n\nHere, we compute the feature loss by feeding the predicted image layer and the ground truth through a pretrained VGG-19 network \u03a6. We compute the L 1 difference between \u03a6(f T (I; \u03b8) and \u03a6(T ) in selected feature layers: (2) where \u03a6 l indicates the layer l in the VGG-19 network. The weights {\u03bb l } are used to balance different terms in the loss function. We select the layers 'conv1 2', 'conv2 2', 'conv3 2', 'conv4 2', and 'conv5 2' in the VGG-19 network.\nL feat (\u03b8) = (I,T )\u2208D l \u03bb l \u03a6 l (T ) \u2212 \u03a6 l (f T (I; \u03b8)) 1 ,\n\nAdversarial loss\n\nDuring the course of our research, we find that transmission image can suffer from unrealistic color degradation and undesirable subtle residuals without an adversarial loss. We adopted the conditional GAN [12] for our model. Our generator would be f T (I; \u03b8). The architecture of our discriminator, denoted as D, has 4 layers and 64 feature channels wide. The discriminator tries to discriminate between patches in the real transmission images and patches given by f T (I; \u03b8) conditioned on I. The goal is to let the network D learn a suitable loss function for further refining layer separation, and to push the predicted transmission layers toward the domain of real reflection-free images.\n\nLoss for the discriminator D is:\n(I,T )\u2208D log D(I, f T (I; \u03b8)) \u2212 log D(I, T ),(3)\nwhere D(I, x) outputs the probability that x is a natural transmission image given the input image I. Then our adversarial loss is:\nL adv (\u03b8) = I\u2208D \u2212 log D(I, f T (I; \u03b8)).(4)\nWe optimize over \u2212 log D(I, f T (I; \u03b8)) instead of log (1 \u2212 D(I, f T (I; \u03b8))) for better gradient performance [8].\nI T R \u03a8(T, R)\n\nTrain without normalization\n\nTrain with normalization fT fR \u03a8(fT , fR) Figure 3: Visual comparisons of training with and without gradient normalization. In the middle two columns, the small window at the right bottom corner of each image shows the gradient magnitude of each image. In the rightmost column, \u03a8 denotes the normalized gradient product formulated in Equation 6. The first row left to right shows: input, ground truth transmission T , ground truth reflection R, and \u03a8. \u03a8(T, R) is close to zeros indicating that the gradient fields of T and R are not correlated. The middle row shows results trained with no normalization in the gradient fields. We observe that the reflection prediction trained without normalization is heavily suppressed. Bottom row shows results trained with gradient normalization with better reflection separation.\n\n\nExclusion loss\n\nWe further propose an exclusion loss in the gradient domain to better separate the reflection and transmission layers. We explore the relationship between the two layers through analysis of the edges in the two layers. Our key observation is that the edges of the transmission and the reflection layers are unlikely to overlap. An edge in I should be caused by either T or R, but not both. Thus we minimize the correlation between the predicted transmission and reflection layers in the gradient domain. We formulate the exclusion loss as the product of normalized gradient fields of the two layers at multiple spatial resolutions :\nL excl (\u03b8) = I\u2208D N n=1 \u03a8(f \u2193n T (I; \u03b8), f \u2193n R (I; \u03b8)) F ,(5)\u03a8(T, R) = tanh(\u03bb T |\u2207T |) tanh(\u03bb R |\u2207R|),(6)\nwhere \u03bb T and \u03bb R are normalization factors, \u00b7 F is the Frobenius norm, denotes element-wise multiplication, and n is the image downsampling factor: the images f T and f R are downsampled by a factor of 2 n\u22121 with bilinear interpolation. We set N = 3, \u03bb T = \u2207R F \u2207T F , and \u03bb R = \u2207T F \u2207R F in our experiments. Note that the normalization factors \u03bb T and \u03bb R are critical in Equation 6, since the transmission and reflection layers may contain unbalanced gradient magnitudes. The reflection layer can be either blurred with low intensity and thus consists of small gradients, or it could reflect very bright light and composes brightest spots in the image, which produces high contrast reflection and thus large gradients. A scale discrepancy between |\u2207T | and |\u2207R| would cause unbalanced updates to the two layer predictions. We observe that without proper normalization factors, the network would suppress the layer with a smaller gradient update rate to close to zero. A visual comparison of results with and without normalization is shown in Figure 3.\n\nL excl is effective in separating the transmission and reflection layers at the pixel level. If we disable L excl in our model, some residual reflection may remain visible in the output transmission image, as shown in Figure 2 (d).\n\n\nImplementation\n\nGiven the ground-truth reflection layer R, we can further constrain f R (I; \u03b8) with R. Reflection layer is usually not in focus and thus blurry. We simply add a L 1 loss in color space to constrain f R (I; \u03b8):\nL R (\u03b8) = (I,R)\u2208D f R (I; \u03b8) \u2212 R 1 .(7)\nWe train the network f by minimizing (L + L R ) on synthetic and real data jointly. Note that we disable L R when training on a real-world image as it is difficult to estimate R precisely. We tried computing R = I \u2212 T but R sometimes contains significant artifacts because I = R + T may not hold when I is overexposed. For the training data, we use 5000 synthetic images and extract 500 image patches from 90 real-world training images with random resolutions between 256p and 480p. To further augment the data, we randomly resize image patches while keeping the original aspect ratio. We train for 250 epochs with batch size 1 on an Nvidia Titan X GPU and weights are updated using the Adam optimizer [14] with a fixed learning rate of 10 \u22124 .\n\n\nDataset\n\n\nSynthetic data\n\nTo create synthetic images with reflection, we choose 5000 random pairs of images from Flickr: one outdoor image and one indoor image for each pair. We use an image Figure 4: Real data collection setup and captured images. We capture two images with and without the glass with same camera settings in a static scene. Right column from top to bottom: captured image with reflection and the ground-truth transmission image T .\n\n(either indoor or outdoor) as the transmission layer and the other image as the reflection layer. We assume the transmission and reflection layers locate on different focal planes so that the two layers exhibit noticeable different blurriness. This is a valid assumption in real-life photography, where the object of interest (e.g. artwork through museum windows) is often in the transmission layer and is set to be in focus. In addition, reflection could be intentionally blurred by shooting with a wide aperture. We use this assumption to create a synthetic dataset, by applying a Gaussian smoothing kernel with a random kernel size in the range of 3 to 17 pixels to the reflection image.\n\nOur image composition approach is similar to the one proposed by Fan et al. [5], but our forward model has the following differences. We remove gamma correction from the images and operate in linear space to better approximate the physical formation of images. Instead of fixing the intensity decay on R, we apply variation to the intensity decay since we observe that reflection in real images could have comparable or higher intensity level than the transmission layer. We apply slight vignette centered at random position in the reflection layer, which simulates the scenario when camera views the reflection from oblique angles.\n\n\nReal data\n\nAt the time of developing this work, there is no publicly available benchmark with ground-truth transmission to evaluate different reflection removal approaches on real data. We collected a dataset of 110 real image pairs: image with reflection and its corresponding ground-truth transmission image. The images with reflection were taken with a  Table 1: Quantitative comparison results among our method and 3 other previous methods. We evaluated on synthetic data provided by CEILNet [5], and our real image test set. We also provide a trivial baseline that takes the input image as the result transmission image.\n\nCanon 600D camera on a tripod with a portable glass in front of the camera. The ground-truth transmission layer was captured when the portable glass was removed. Each image pair was taken with the same exposure setting. Our setup for data capture is shown in Figure 4. We captured the dataset with the following considerations:\n\n\u2022 environments: indoor and outdoor;\n\n\u2022 lighting conditions: skylight, sunlight, and incandescent; \u2022 camera viewing angles: front view and oblique view;\n\n\u2022 and camera apertures (affecting the reflection blurriness): f /2.0 -f /16. We split the dataset randomly into a training set and a test set. We extract 500 patches from 90 training images for training and use 20 images for quantitative evaluation.\n\n\nExperiments\n\n\nComparison to prior work\n\nWe compare our model to CEILNet [5], the layer separation method by Li and Brown [21], and Pix2pix [12]. We evaluated different methods on the publicly available synthetic images from the CEILNet dataset [5] and the real images from the test set of our real-world dataset.\n\nOur model is only trained on our generated synthetic dataset and the training set of our real-world dataset. For CEILNet, we evaluate its pre-trained model on the CEILNet synthetic images. To evaluate CEILNet on our real data, we fine-tune its model with our real training images (otherwise it performs poorly). We evaluate the approach of Li and Brown [21] with the provided default parameters. Pix2pix is a general image translation model, we train its model on our generated synthetic dataset and the training set of our collected real dataset.\n\nThe quantitative results are shown in Table 1. We compute the PSNR and SSIM between the result transmission images of different methods and ground-truth transmission Preference rate Ours>CEILNet [5] 84.2% Ours>Li and Brown [21] 87.8% Table 2: User study results. The preference rate shows the percentage of comparisons in which users prefer our results.\n\nlayer. We demonstrate strong quantitative performance over previous works on both synthetic and real data.\n\nWe also conduct a user study on Amazon Mechanical Turk, following the protocol by Chen and Koltun [3]. During the user study, each user is presented with a input realworld image with reflection, our predicted transmission image, and the predicted transmission image by a baseline in the same row. Then the user needs to choose an output image that is closer to the reflection-free version of the input image between the two predicted transmission images. There are 80 real-world images for comparisons from our dataset and the CEILNet dataset. The results are reported in Table 2. 84.2% of the comparisons to CEILNet and 87.8% of the comparisons to Li and Brown have our results rated to contain less reflection. The results are statistically significant with p < 10 \u22123 and 20 users participate in the user study.\n\nMore experimental details and results are reported in the supplement.\n\n\nSynthetic\n\nReal  Table 3: Quantitative comparisons on synthetic and real images among multiple ablated models of our method. We remove each of the three losses and evaluate on the re-trained models. 'Ours L adv -only' denotes our method trained with only an adversarial loss. Our complete model shows better performance on both synthetic and real data. We evaluate on synthetic data provided by CEILNet [5], and our real test images described in Section 5.2.\n\n\nQualitative results\n\nWe present qualitative results of different methods in Figure 5 and Figure 6, evaluated on real-world images from our dataset (with ground truth) and from CEILNet [5] (with- Our results Figure 6: Qualitative comparisons among CEILNet [5], Li and Brown [21] and our method, evaluated on real images in the CEILNET dataset. Note that even though we have no supervision on the reflection layer for real data, but our method predicts cleaner reflection layer as well. Additional results are provided in the supplement. out ground truth), respectively.\n\n\nControlled experiments\n\nTo analyze how each loss contributes to the final performance of our network, we remove or replace each loss in the combined objective and re-train the network. A visual comparison is shown in Figure 2. When we replace the feature loss L feat with a L 1 loss in color space, the output images tend to be overly-smooth; similar observation is also discussed in [34,12]. Without L excl , we notice that visible contents of the reflection layer may appear in the transmission prediction. The adversarial refinement loss L adv helps recover cleaner and more natural results, as shown in (e).\n\nThe quantitative results are shown in Table 3. We also analyze the performance of the model with only an adversarial loss, which is similar to a conditional GAN [12].\n\n\nExtensions\n\nWe demonstrate two additional image enhancement applications, flare removal and dehazing, using our trained model to remove an undesired layer. Note that we directly apply our trained reflection removal model without training or fine-tuning on any flare removal or dehazing dataset. These two tasks can be treated as layer separation problems, similar to reflection separation. For flare removal, we aim to remove the optical artifacts of lens flare, which is caused by light reflection and scattering inside the lens. For dehazing, we target at removing the hazy layer. The hazy images suffer from contrast loss caused by light scattering, reflection and attenuation of particles in the air. We show the extension results in Figure 7. Our trained model can achieve im-Transmission Reflection Input CEILNet [5] Ground-truth T Our results Figure 8: A challenging case with sharp reflection. Our method produces better reflection separation results than CEILNet, but is not able to remove reflection completely.\n\nage enhancement by removing undesirable layers from the input images for flare removal and dehazing. More extension results are provided in the supplement.\n\n\nDiscussion\n\nWe presented an end-to-end learning approach for single image reflection separation with perceptual losses and a customized exclusion loss. To decompose an image into the transmission and reflection layers, we found it effective to train a network with combined low-level and high-level image features. In order to evaluate different methods on real data, we collected a new dataset of real-world images for reflection removal that contains ground-truth transmission layers. We additionally extend our approach to two other photo enhancement applications to show generality of our approach for layer separation problems.\n\nAlthough our reflection separation model outperforms state-of-the-art approaches on both synthetic and real images, we believe the performance can be further improved in the future. Figure 8 illustrates one challenging scenario where the reflection layer is almost as sharp as the transmission layer in a real-world image. We hope our model and dataset will inspire subsequent work on reflection separation and the challenging scenarios. Our dataset and code will be made publicly to facilitate future research.\n\n\nAcknowledgement\n\nWe thank You Zhang for great help collecting the reflection dataset. We also thank Yichao Zhou and Daniel Seita for constructive writing feedback. This work is supported by UC Berkeley EECS departmental fellowship and hardware donations from NVIDIA.\n\nFigure 1\n1Figure 1: Results by CEILNet [5] and our approach on real-world images. The top row shows a real image from the CEILNet dataset with a window reflecting a poster of a human face; the bottom row shows an image taken by ourselves, with a lamp as the reflection. From left to right: the input images, CEILNet results and our results. Note that our approach trained to learn both low-level and high-level image statistics successfully removes the reflection layers of the face and lamp, while CEILNet does not.\n\nFigure 2 :\n2Visual comparisons on the three perceptual loss functions, evaluated on a real-world image. In (b), we replace L feat with image space L 1 loss and observed overly-smooth output. (c) shows artifacts of color degradation and noticeable residuals without L adv . In (d), the lack of L excl makes the predicted transmission have undesired reflection residuals. Our complete model in (e) is able to produce better and cleaner prediction.\n\nFigure 7 :\n7Extension applications on camera flare removal and image dehazing. For each column, from top to bottom: input, our predicted enhanced layer, our predicted removed layer.\n\n\nFigure 5: Visual results comparison between CEILNet[5] and our method, evaluated on real images from our dataset described in Section 5.2. From left to right: input, ground truth transmission layer, CEILNet[5] predictions and our predictions. Notice that our method produces better and cleaner predictions in both the transmission and reflection layers. Additional results are provided in the supplement.Transmission \n\nReflection \nTransmission \nReflection \n\nInput \nGround-truth T \n\nCEILNet [5] \nOur results \n\nTransmission \nReflection \nTransmission \nReflection \nTransmission \nReflection \n\nInput \n\nCEILNet [5] \nLi and Brown [21] \n\n\nRemoving photography artifacts using gradient projection and flashexposure sampling. A Agrawal, R Raskar, S K Nayar, Y Li, TOG. 2A. Agrawal, R. Raskar, S. K. Nayar, and Y. Li. Remov- ing photography artifacts using gradient projection and flash- exposure sampling. TOG, 2005. 2\n\nSingle image reflection suppression. N Arvanitopoulos, R Achanta, S S\u00fcsstrunk, CVPR. N. Arvanitopoulos, R. Achanta, and S. S\u00fcsstrunk. Single image reflection suppression. In CVPR, 2017. 2\n\nPhotographic image synthesis with cascaded refinement networks. Q Chen, V Koltun, ICCV. 46Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. In ICCV, 2017. 4, 6\n\nFast image processing with fully-convolutional networks. Q Chen, J Xu, V Koltun, ICCV. Q. Chen, J. Xu, and V. Koltun. Fast image processing with fully-convolutional networks. In ICCV, 2017. 3\n\nA generic deep architecture for single image reflection removal and image smoothing. Q Fan, J Yang, G Hua, B Chen, D Wipf, ICCV. 7Q. Fan, J. Yang, G. Hua, B. Chen, and D. Wipf. A generic deep architecture for single image reflection removal and im- age smoothing. In ICCV, 2017. 1, 2, 3, 4, 5, 6, 7, 8\n\nBlind separation of superimposed moving images using image statistics. K Gai, Z Shi, C Zhang, IEEE PAMI. 342K. Gai, Z. Shi, and C. Zhang. Blind separation of superim- posed moving images using image statistics. IEEE PAMI, 34, 2012. 2\n\nImage style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, CVPR. L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In CVPR, 2016. 4\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In NIPS, 2014. 4\n\nRobust separation of reflection from multiple images. X Guo, X Cao, Y Ma, CVPR. 1X. Guo, X. Cao, and Y. Ma. Robust separation of reflection from multiple images. In CVPR, 2014. 1, 2\n\nReflection removal using low-rank matrix completion. B.-J Han, J.-Y. Sim, CVPR. B.-J. Han and J.-Y. Sim. Reflection removal using low-rank matrix completion. In CVPR, 2017. 2\n\nHypercolumns for object segmentation and fine-grained localization. B Hariharan, P A Arbel\u00e1ez, R B Girshick, J Malik, CVPR. B. Hariharan, P. A. Arbel\u00e1ez, R. B. Girshick, and J. Malik. Hypercolumns for object segmentation and fine-grained lo- calization. In CVPR, 2015. 3\n\nImage-to-image translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, CVPR. 4P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 4, 6, 8\n\nPerceptual losses for real-time style transfer and super-resolution. J Johnson, A Alahi, L Fei-Fei, ECCV. J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 4\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5\n\nA physically-based approach to reflection separation: from physical modeling to constrained optimization. N Kong, Y.-W Tai, J S Shin, PAMI. 2N. Kong, Y.-W. Tai, and J. S. Shin. A physically-based ap- proach to reflection separation: from physical modeling to constrained optimization. PAMI, 2014. 2\n\nPhoto-realistic single image super-resolution using a generative adversarial network. C Ledig, L Theis, F Husz\u00e1r, J Caballero, A Cunningham, A Acosta, A Aitken, A Tejani, J Totz, Z Wang, arXiv:1609.04802arXiv preprintC. Ledig, L. Theis, F. Husz\u00e1r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al. Photo-realistic single image super-resolution using a gener- ative adversarial network. arXiv preprint arXiv:1609.04802, 2016. 4\n\nUser assisted separation of reflections from a single image using a sparsity prior. A Levin, Y Weiss, IEEE PAMI. 2A. Levin and Y. Weiss. User assisted separation of reflections from a single image using a sparsity prior. IEEE PAMI, 2007. 2\n\nLearning to perceive transparency from the statistics of natural scenes. A Levin, A Zomet, Y Weiss, NIPS. A. Levin, A. Zomet, and Y. Weiss. Learning to perceive transparency from the statistics of natural scenes. In NIPS, 2003. 2\n\nSeparating reflections from a single image using local features. A Levin, A Zomet, Y Weiss, CVPR. A. Levin, A. Zomet, and Y. Weiss. Separating reflections from a single image using local features. In CVPR, 2004. 2\n\nExploiting reflection change for automatic reflection removal. Y Li, M S Brown, CVPR. Y. Li and M. S. Brown. Exploiting reflection change for au- tomatic reflection removal. In CVPR, 2013. 2\n\nSingle image layer separation using relative smoothness. Y Li, M S Brown, CVPR. 67Y. Li and M. S. Brown. Single image layer separation using relative smoothness. In CVPR, 2014. 1, 2, 4, 6, 7\n\n. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, Imagenet large scale visual recognition challenge. IJCVO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 3\n\nSeparating transparent layers through layer information exchange. B Sarel, M Irani, ECCV. 2B. Sarel and M. Irani. Separating transparent layers through layer information exchange. ECCV, 2004. 2\n\nSeparation of transparent layers using focus. Y Y Schechner, N Kiryati, R Basri, IJCV. 2Y. Y. Schechner, N. Kiryati, and R. Basri. Separation of transparent layers using focus. IJCV, 2000. 2\n\nReflection removal using ghosting cues. Y Shih, D Krishnan, F Durand, W T Freeman, CVPR. 13Y. Shih, D. Krishnan, F. Durand, and W. T. Freeman. Re- flection removal using ghosting cues. In CVPR, 2015. 1, 3\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. 3K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015. 3\n\nReflection separation using guided annotation. O Springer, Y Weiss, arXiv:1702.059581arXiv preprintO. Springer and Y. Weiss. Reflection separation using guided annotation. arXiv preprint arXiv:1702.05958, 2017. 1, 2\n\nAutomatic reflection removal using gradient intensity and motion cues. C Sun, S Liu, T Yang, B Zeng, Z Wang, G Liu, Proceedings of the 2016 ACM on Multimedia Conference. the 2016 ACM on Multimedia ConferenceC. Sun, S. Liu, T. Yang, B. Zeng, Z. Wang, and G. Liu. Auto- matic reflection removal using gradient intensity and motion cues. In Proceedings of the 2016 ACM on Multimedia Con- ference, 2016. 2\n\nLayer extraction from multiple images containing reflections and transparency. R Szeliski, S Avidan, P Anandan, CVPR. R. Szeliski, S. Avidan, and P. Anandan. Layer extrac- tion from multiple images containing reflections and trans- parency. In CVPR, 2000. 2\n\nBenchmarking single-image reflection removal algorithms. R Wan, B Shi, L.-Y Duan, A.-H Tan, A C Kot, CVPR. R. Wan, B. Shi, L.-Y. Duan, A.-H. Tan, and A. C. Kot. Benchmarking single-image reflection removal algorithms. In CVPR, 2017. 3\n\nDepth of field guided reflection removal. R Wan, B Shi, T A Hwee, A C Kot, ICIP. 2R. Wan, B. Shi, T. A. Hwee, and A. C. Kot. Depth of field guided reflection removal. In ICIP, 2016. 2\n\nA computational approach for obstruction-free photography. T Xue, M Rubinstein, C Liu, W T Freeman, ACM Trans. Graph. 344T. Xue, M. Rubinstein, C. Liu, and W. T. Freeman. A com- putational approach for obstruction-free photography. ACM Trans. Graph., 34(4), 2015. 1, 2\n\nMulti-scale context aggregation by dilated convolutions. F Yu, V Koltun, ICLR. F. Yu and V. Koltun. Multi-scale context aggregation by di- lated convolutions. In ICLR, 2016. 3\n\nLoss functions for neural networks for image processing. H Zhao, O Gallo, I Frosio, J Kautz, IEEE Trans. Computational Imaging. 8H. Zhao, O. Gallo, I. Frosio, and J. Kautz. Loss functions for neural networks for image processing. IEEE Trans. Compu- tational Imaging, 2017. 8\n", "annotations": {"author": "[{\"end\":99,\"start\":61},{\"end\":132,\"start\":100},{\"end\":171,\"start\":133},{\"end\":209,\"start\":172}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":68},{\"end\":106,\"start\":104},{\"end\":145,\"start\":137},{\"end\":183,\"start\":179}]", "author_first_name": "[{\"end\":67,\"start\":61},{\"end\":103,\"start\":100},{\"end\":134,\"start\":133},{\"end\":136,\"start\":135},{\"end\":178,\"start\":172}]", "author_affiliation": "[{\"end\":98,\"start\":75},{\"end\":131,\"start\":108},{\"end\":170,\"start\":147},{\"end\":208,\"start\":185}]", "title": "[{\"end\":58,\"start\":1},{\"end\":267,\"start\":210}]", "venue": null, "abstract": "[{\"end\":1207,\"start\":269}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2368,\"start\":2365},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2371,\"start\":2368},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2374,\"start\":2371},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2574,\"start\":2570},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2577,\"start\":2574},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2753,\"start\":2750},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4573,\"start\":4570},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5866,\"start\":5862},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5868,\"start\":5866},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5871,\"start\":5868},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5874,\"start\":5871},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5877,\"start\":5874},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5879,\"start\":5877},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5882,\"start\":5879},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5885,\"start\":5882},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6193,\"start\":6190},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6216,\"start\":6212},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6330,\"start\":6326},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6499,\"start\":6495},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7084,\"start\":7080},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7087,\"start\":7084},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7294,\"start\":7290},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7297,\"start\":7294},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7341,\"start\":7338},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7795,\"start\":7791},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7998,\"start\":7994},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8155,\"start\":8151},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8312,\"start\":8309},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8861,\"start\":8857},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10439,\"start\":10435},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10481,\"start\":10477},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10530,\"start\":10526},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11227,\"start\":11223},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11229,\"start\":11227},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11780,\"start\":11777},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11907,\"start\":11904},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11944,\"start\":11940},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11962,\"start\":11958},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12532,\"start\":12529},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12534,\"start\":12532},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12537,\"start\":12534},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12540,\"start\":12537},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13290,\"start\":13286},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14145,\"start\":14142},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14535,\"start\":14534},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18029,\"start\":18025},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19293,\"start\":19290},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20348,\"start\":20345},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21285,\"start\":21282},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21335,\"start\":21331},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21353,\"start\":21349},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21457,\"start\":21454},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21881,\"start\":21877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22271,\"start\":22268},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22300,\"start\":22296},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22637,\"start\":22634},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23829,\"start\":23826},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24071,\"start\":24068},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24142,\"start\":24139},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24161,\"start\":24157},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24843,\"start\":24839},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24846,\"start\":24843},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25233,\"start\":25229},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26059,\"start\":26056},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29037,\"start\":29034},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29192,\"start\":29189}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28350,\"start\":27833},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28797,\"start\":28351},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28980,\"start\":28798},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29611,\"start\":28981}]", "paragraph": "[{\"end\":1854,\"start\":1223},{\"end\":2144,\"start\":1856},{\"end\":3331,\"start\":2146},{\"end\":4098,\"start\":3333},{\"end\":4414,\"start\":4100},{\"end\":5047,\"start\":4416},{\"end\":5275,\"start\":5049},{\"end\":5600,\"start\":5277},{\"end\":6921,\"start\":5617},{\"end\":8801,\"start\":6923},{\"end\":9387,\"start\":8803},{\"end\":9787,\"start\":9400},{\"end\":10100,\"start\":9789},{\"end\":11099,\"start\":10160},{\"end\":11668,\"start\":11101},{\"end\":12006,\"start\":11670},{\"end\":12541,\"start\":12034},{\"end\":13000,\"start\":12543},{\"end\":13773,\"start\":13080},{\"end\":13807,\"start\":13775},{\"end\":13988,\"start\":13857},{\"end\":14146,\"start\":14032},{\"end\":15009,\"start\":14191},{\"end\":15660,\"start\":15028},{\"end\":16821,\"start\":15767},{\"end\":17054,\"start\":16823},{\"end\":17282,\"start\":17073},{\"end\":18067,\"start\":17323},{\"end\":18520,\"start\":18096},{\"end\":19212,\"start\":18522},{\"end\":19846,\"start\":19214},{\"end\":20474,\"start\":19860},{\"end\":20803,\"start\":20476},{\"end\":20840,\"start\":20805},{\"end\":20956,\"start\":20842},{\"end\":21207,\"start\":20958},{\"end\":21522,\"start\":21250},{\"end\":22071,\"start\":21524},{\"end\":22426,\"start\":22073},{\"end\":22534,\"start\":22428},{\"end\":23349,\"start\":22536},{\"end\":23420,\"start\":23351},{\"end\":23881,\"start\":23434},{\"end\":24452,\"start\":23905},{\"end\":25066,\"start\":24479},{\"end\":25234,\"start\":25068},{\"end\":26258,\"start\":25249},{\"end\":26415,\"start\":26260},{\"end\":27050,\"start\":26430},{\"end\":27563,\"start\":27052},{\"end\":27832,\"start\":27583}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10159,\"start\":10101},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13060,\"start\":13001},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13856,\"start\":13808},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14031,\"start\":13989},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14160,\"start\":14147},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15722,\"start\":15661},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15766,\"start\":15722},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17322,\"start\":17283}]", "table_ref": "[{\"end\":20213,\"start\":20206},{\"end\":22118,\"start\":22111},{\"end\":22314,\"start\":22307},{\"end\":23115,\"start\":23108},{\"end\":23447,\"start\":23440},{\"end\":25113,\"start\":25106}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1221,\"start\":1209},{\"attributes\":{\"n\":\"2.\"},\"end\":5615,\"start\":5603},{\"attributes\":{\"n\":\"3.\"},\"end\":9398,\"start\":9390},{\"attributes\":{\"n\":\"4.\"},\"end\":12017,\"start\":12009},{\"attributes\":{\"n\":\"4.1.\"},\"end\":12032,\"start\":12020},{\"attributes\":{\"n\":\"4.2.\"},\"end\":13078,\"start\":13062},{\"end\":14189,\"start\":14162},{\"attributes\":{\"n\":\"4.3.\"},\"end\":15026,\"start\":15012},{\"attributes\":{\"n\":\"4.4.\"},\"end\":17071,\"start\":17057},{\"attributes\":{\"n\":\"5.\"},\"end\":18077,\"start\":18070},{\"attributes\":{\"n\":\"5.1.\"},\"end\":18094,\"start\":18080},{\"attributes\":{\"n\":\"5.2.\"},\"end\":19858,\"start\":19849},{\"attributes\":{\"n\":\"6.\"},\"end\":21221,\"start\":21210},{\"attributes\":{\"n\":\"6.1.\"},\"end\":21248,\"start\":21224},{\"end\":23432,\"start\":23423},{\"attributes\":{\"n\":\"6.2.\"},\"end\":23903,\"start\":23884},{\"attributes\":{\"n\":\"6.3.\"},\"end\":24477,\"start\":24455},{\"attributes\":{\"n\":\"7.\"},\"end\":25247,\"start\":25237},{\"attributes\":{\"n\":\"8.\"},\"end\":26428,\"start\":26418},{\"attributes\":{\"n\":\"9.\"},\"end\":27581,\"start\":27566},{\"end\":27842,\"start\":27834},{\"end\":28362,\"start\":28352},{\"end\":28809,\"start\":28799}]", "table": "[{\"end\":29611,\"start\":29387}]", "figure_caption": "[{\"end\":28350,\"start\":27844},{\"end\":28797,\"start\":28364},{\"end\":28980,\"start\":28811},{\"end\":29387,\"start\":28983}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3148,\"start\":3140},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12220,\"start\":12212},{\"end\":14241,\"start\":14233},{\"end\":16820,\"start\":16812},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17049,\"start\":17041},{\"end\":18269,\"start\":18261},{\"end\":20743,\"start\":20735},{\"end\":23968,\"start\":23960},{\"end\":23981,\"start\":23973},{\"end\":24099,\"start\":24091},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24680,\"start\":24672},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25983,\"start\":25975},{\"end\":26095,\"start\":26087},{\"end\":27242,\"start\":27234}]", "bib_author_first_name": "[{\"end\":29699,\"start\":29698},{\"end\":29710,\"start\":29709},{\"end\":29720,\"start\":29719},{\"end\":29722,\"start\":29721},{\"end\":29731,\"start\":29730},{\"end\":29930,\"start\":29929},{\"end\":29948,\"start\":29947},{\"end\":29959,\"start\":29958},{\"end\":30146,\"start\":30145},{\"end\":30154,\"start\":30153},{\"end\":30337,\"start\":30336},{\"end\":30345,\"start\":30344},{\"end\":30351,\"start\":30350},{\"end\":30558,\"start\":30557},{\"end\":30565,\"start\":30564},{\"end\":30573,\"start\":30572},{\"end\":30580,\"start\":30579},{\"end\":30588,\"start\":30587},{\"end\":30847,\"start\":30846},{\"end\":30854,\"start\":30853},{\"end\":30861,\"start\":30860},{\"end\":31069,\"start\":31068},{\"end\":31071,\"start\":31070},{\"end\":31080,\"start\":31079},{\"end\":31082,\"start\":31081},{\"end\":31091,\"start\":31090},{\"end\":31253,\"start\":31252},{\"end\":31267,\"start\":31266},{\"end\":31284,\"start\":31283},{\"end\":31293,\"start\":31292},{\"end\":31299,\"start\":31298},{\"end\":31315,\"start\":31314},{\"end\":31324,\"start\":31323},{\"end\":31337,\"start\":31336},{\"end\":31562,\"start\":31561},{\"end\":31569,\"start\":31568},{\"end\":31576,\"start\":31575},{\"end\":31747,\"start\":31743},{\"end\":31758,\"start\":31753},{\"end\":31935,\"start\":31934},{\"end\":31948,\"start\":31947},{\"end\":31950,\"start\":31949},{\"end\":31962,\"start\":31961},{\"end\":31964,\"start\":31963},{\"end\":31976,\"start\":31975},{\"end\":32205,\"start\":32204},{\"end\":32217,\"start\":32213},{\"end\":32224,\"start\":32223},{\"end\":32232,\"start\":32231},{\"end\":32234,\"start\":32233},{\"end\":32456,\"start\":32455},{\"end\":32467,\"start\":32466},{\"end\":32476,\"start\":32475},{\"end\":32662,\"start\":32661},{\"end\":32664,\"start\":32663},{\"end\":32674,\"start\":32673},{\"end\":32878,\"start\":32877},{\"end\":32889,\"start\":32885},{\"end\":32896,\"start\":32895},{\"end\":32898,\"start\":32897},{\"end\":33158,\"start\":33157},{\"end\":33167,\"start\":33166},{\"end\":33176,\"start\":33175},{\"end\":33186,\"start\":33185},{\"end\":33199,\"start\":33198},{\"end\":33213,\"start\":33212},{\"end\":33223,\"start\":33222},{\"end\":33233,\"start\":33232},{\"end\":33243,\"start\":33242},{\"end\":33251,\"start\":33250},{\"end\":33621,\"start\":33620},{\"end\":33630,\"start\":33629},{\"end\":33851,\"start\":33850},{\"end\":33860,\"start\":33859},{\"end\":33869,\"start\":33868},{\"end\":34074,\"start\":34073},{\"end\":34083,\"start\":34082},{\"end\":34092,\"start\":34091},{\"end\":34287,\"start\":34286},{\"end\":34293,\"start\":34292},{\"end\":34295,\"start\":34294},{\"end\":34473,\"start\":34472},{\"end\":34479,\"start\":34478},{\"end\":34481,\"start\":34480},{\"end\":34610,\"start\":34609},{\"end\":34625,\"start\":34624},{\"end\":34633,\"start\":34632},{\"end\":34639,\"start\":34638},{\"end\":34649,\"start\":34648},{\"end\":34661,\"start\":34660},{\"end\":34667,\"start\":34666},{\"end\":34676,\"start\":34675},{\"end\":34688,\"start\":34687},{\"end\":34698,\"start\":34697},{\"end\":35016,\"start\":35015},{\"end\":35025,\"start\":35024},{\"end\":35191,\"start\":35190},{\"end\":35193,\"start\":35192},{\"end\":35206,\"start\":35205},{\"end\":35217,\"start\":35216},{\"end\":35377,\"start\":35376},{\"end\":35385,\"start\":35384},{\"end\":35397,\"start\":35396},{\"end\":35407,\"start\":35406},{\"end\":35409,\"start\":35408},{\"end\":35611,\"start\":35610},{\"end\":35623,\"start\":35622},{\"end\":35803,\"start\":35802},{\"end\":35815,\"start\":35814},{\"end\":36044,\"start\":36043},{\"end\":36051,\"start\":36050},{\"end\":36058,\"start\":36057},{\"end\":36066,\"start\":36065},{\"end\":36074,\"start\":36073},{\"end\":36082,\"start\":36081},{\"end\":36455,\"start\":36454},{\"end\":36467,\"start\":36466},{\"end\":36477,\"start\":36476},{\"end\":36692,\"start\":36691},{\"end\":36699,\"start\":36698},{\"end\":36709,\"start\":36705},{\"end\":36720,\"start\":36716},{\"end\":36727,\"start\":36726},{\"end\":36729,\"start\":36728},{\"end\":36913,\"start\":36912},{\"end\":36920,\"start\":36919},{\"end\":36927,\"start\":36926},{\"end\":36929,\"start\":36928},{\"end\":36937,\"start\":36936},{\"end\":36939,\"start\":36938},{\"end\":37115,\"start\":37114},{\"end\":37122,\"start\":37121},{\"end\":37136,\"start\":37135},{\"end\":37143,\"start\":37142},{\"end\":37145,\"start\":37144},{\"end\":37383,\"start\":37382},{\"end\":37389,\"start\":37388},{\"end\":37560,\"start\":37559},{\"end\":37568,\"start\":37567},{\"end\":37577,\"start\":37576},{\"end\":37587,\"start\":37586}]", "bib_author_last_name": "[{\"end\":29707,\"start\":29700},{\"end\":29717,\"start\":29711},{\"end\":29728,\"start\":29723},{\"end\":29734,\"start\":29732},{\"end\":29945,\"start\":29931},{\"end\":29956,\"start\":29949},{\"end\":29969,\"start\":29960},{\"end\":30151,\"start\":30147},{\"end\":30161,\"start\":30155},{\"end\":30342,\"start\":30338},{\"end\":30348,\"start\":30346},{\"end\":30358,\"start\":30352},{\"end\":30562,\"start\":30559},{\"end\":30570,\"start\":30566},{\"end\":30577,\"start\":30574},{\"end\":30585,\"start\":30581},{\"end\":30593,\"start\":30589},{\"end\":30851,\"start\":30848},{\"end\":30858,\"start\":30855},{\"end\":30867,\"start\":30862},{\"end\":31077,\"start\":31072},{\"end\":31088,\"start\":31083},{\"end\":31098,\"start\":31092},{\"end\":31264,\"start\":31254},{\"end\":31281,\"start\":31268},{\"end\":31290,\"start\":31285},{\"end\":31296,\"start\":31294},{\"end\":31312,\"start\":31300},{\"end\":31321,\"start\":31316},{\"end\":31334,\"start\":31325},{\"end\":31344,\"start\":31338},{\"end\":31566,\"start\":31563},{\"end\":31573,\"start\":31570},{\"end\":31579,\"start\":31577},{\"end\":31751,\"start\":31748},{\"end\":31762,\"start\":31759},{\"end\":31945,\"start\":31936},{\"end\":31959,\"start\":31951},{\"end\":31973,\"start\":31965},{\"end\":31982,\"start\":31977},{\"end\":32211,\"start\":32206},{\"end\":32221,\"start\":32218},{\"end\":32229,\"start\":32225},{\"end\":32240,\"start\":32235},{\"end\":32464,\"start\":32457},{\"end\":32473,\"start\":32468},{\"end\":32484,\"start\":32477},{\"end\":32671,\"start\":32665},{\"end\":32677,\"start\":32675},{\"end\":32883,\"start\":32879},{\"end\":32893,\"start\":32890},{\"end\":32903,\"start\":32899},{\"end\":33164,\"start\":33159},{\"end\":33173,\"start\":33168},{\"end\":33183,\"start\":33177},{\"end\":33196,\"start\":33187},{\"end\":33210,\"start\":33200},{\"end\":33220,\"start\":33214},{\"end\":33230,\"start\":33224},{\"end\":33240,\"start\":33234},{\"end\":33248,\"start\":33244},{\"end\":33256,\"start\":33252},{\"end\":33627,\"start\":33622},{\"end\":33636,\"start\":33631},{\"end\":33857,\"start\":33852},{\"end\":33866,\"start\":33861},{\"end\":33875,\"start\":33870},{\"end\":34080,\"start\":34075},{\"end\":34089,\"start\":34084},{\"end\":34098,\"start\":34093},{\"end\":34290,\"start\":34288},{\"end\":34301,\"start\":34296},{\"end\":34476,\"start\":34474},{\"end\":34487,\"start\":34482},{\"end\":34622,\"start\":34611},{\"end\":34630,\"start\":34626},{\"end\":34636,\"start\":34634},{\"end\":34646,\"start\":34640},{\"end\":34658,\"start\":34650},{\"end\":34664,\"start\":34662},{\"end\":34673,\"start\":34668},{\"end\":34685,\"start\":34677},{\"end\":34695,\"start\":34689},{\"end\":34708,\"start\":34699},{\"end\":35022,\"start\":35017},{\"end\":35031,\"start\":35026},{\"end\":35203,\"start\":35194},{\"end\":35214,\"start\":35207},{\"end\":35223,\"start\":35218},{\"end\":35382,\"start\":35378},{\"end\":35394,\"start\":35386},{\"end\":35404,\"start\":35398},{\"end\":35417,\"start\":35410},{\"end\":35620,\"start\":35612},{\"end\":35633,\"start\":35624},{\"end\":35812,\"start\":35804},{\"end\":35821,\"start\":35816},{\"end\":36048,\"start\":36045},{\"end\":36055,\"start\":36052},{\"end\":36063,\"start\":36059},{\"end\":36071,\"start\":36067},{\"end\":36079,\"start\":36075},{\"end\":36086,\"start\":36083},{\"end\":36464,\"start\":36456},{\"end\":36474,\"start\":36468},{\"end\":36485,\"start\":36478},{\"end\":36696,\"start\":36693},{\"end\":36703,\"start\":36700},{\"end\":36714,\"start\":36710},{\"end\":36724,\"start\":36721},{\"end\":36733,\"start\":36730},{\"end\":36917,\"start\":36914},{\"end\":36924,\"start\":36921},{\"end\":36934,\"start\":36930},{\"end\":36943,\"start\":36940},{\"end\":37119,\"start\":37116},{\"end\":37133,\"start\":37123},{\"end\":37140,\"start\":37137},{\"end\":37153,\"start\":37146},{\"end\":37386,\"start\":37384},{\"end\":37396,\"start\":37390},{\"end\":37565,\"start\":37561},{\"end\":37574,\"start\":37569},{\"end\":37584,\"start\":37578},{\"end\":37593,\"start\":37588}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":149024},\"end\":29890,\"start\":29613},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13095034},\"end\":30079,\"start\":29892},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8191987},\"end\":30277,\"start\":30081},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12093452},\"end\":30470,\"start\":30279},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8621123},\"end\":30773,\"start\":30472},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1532152},\"end\":31008,\"start\":30775},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206593710},\"end\":31221,\"start\":31010},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1033682},\"end\":31505,\"start\":31223},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8976032},\"end\":31688,\"start\":31507},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":31926300},\"end\":31864,\"start\":31690},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12225766},\"end\":32136,\"start\":31866},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6200260},\"end\":32384,\"start\":32138},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":980236},\"end\":32615,\"start\":32386},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6628106},\"end\":32769,\"start\":32617},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2033459},\"end\":33069,\"start\":32771},{\"attributes\":{\"doi\":\"arXiv:1609.04802\",\"id\":\"b15\"},\"end\":33534,\"start\":33071},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8201193},\"end\":33775,\"start\":33536},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1500423},\"end\":34006,\"start\":33777},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9779209},\"end\":34221,\"start\":34008},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15044065},\"end\":34413,\"start\":34223},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16165913},\"end\":34605,\"start\":34415},{\"attributes\":{\"id\":\"b21\"},\"end\":34947,\"start\":34607},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14798441},\"end\":35142,\"start\":34949},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6373733},\"end\":35334,\"start\":35144},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6019412},\"end\":35540,\"start\":35336},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14124313},\"end\":35753,\"start\":35542},{\"attributes\":{\"doi\":\"arXiv:1702.05958\",\"id\":\"b26\"},\"end\":35970,\"start\":35755},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4255150},\"end\":36373,\"start\":35972},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8798317},\"end\":36632,\"start\":36375},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6158161},\"end\":36868,\"start\":36634},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15198184},\"end\":37053,\"start\":36870},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7600506},\"end\":37323,\"start\":37055},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":17127188},\"end\":37500,\"start\":37325},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2280479},\"end\":37776,\"start\":37502}]", "bib_title": "[{\"end\":29696,\"start\":29613},{\"end\":29927,\"start\":29892},{\"end\":30143,\"start\":30081},{\"end\":30334,\"start\":30279},{\"end\":30555,\"start\":30472},{\"end\":30844,\"start\":30775},{\"end\":31066,\"start\":31010},{\"end\":31250,\"start\":31223},{\"end\":31559,\"start\":31507},{\"end\":31741,\"start\":31690},{\"end\":31932,\"start\":31866},{\"end\":32202,\"start\":32138},{\"end\":32453,\"start\":32386},{\"end\":32659,\"start\":32617},{\"end\":32875,\"start\":32771},{\"end\":33618,\"start\":33536},{\"end\":33848,\"start\":33777},{\"end\":34071,\"start\":34008},{\"end\":34284,\"start\":34223},{\"end\":34470,\"start\":34415},{\"end\":35013,\"start\":34949},{\"end\":35188,\"start\":35144},{\"end\":35374,\"start\":35336},{\"end\":35608,\"start\":35542},{\"end\":36041,\"start\":35972},{\"end\":36452,\"start\":36375},{\"end\":36689,\"start\":36634},{\"end\":36910,\"start\":36870},{\"end\":37112,\"start\":37055},{\"end\":37380,\"start\":37325},{\"end\":37557,\"start\":37502}]", "bib_author": "[{\"end\":29709,\"start\":29698},{\"end\":29719,\"start\":29709},{\"end\":29730,\"start\":29719},{\"end\":29736,\"start\":29730},{\"end\":29947,\"start\":29929},{\"end\":29958,\"start\":29947},{\"end\":29971,\"start\":29958},{\"end\":30153,\"start\":30145},{\"end\":30163,\"start\":30153},{\"end\":30344,\"start\":30336},{\"end\":30350,\"start\":30344},{\"end\":30360,\"start\":30350},{\"end\":30564,\"start\":30557},{\"end\":30572,\"start\":30564},{\"end\":30579,\"start\":30572},{\"end\":30587,\"start\":30579},{\"end\":30595,\"start\":30587},{\"end\":30853,\"start\":30846},{\"end\":30860,\"start\":30853},{\"end\":30869,\"start\":30860},{\"end\":31079,\"start\":31068},{\"end\":31090,\"start\":31079},{\"end\":31100,\"start\":31090},{\"end\":31266,\"start\":31252},{\"end\":31283,\"start\":31266},{\"end\":31292,\"start\":31283},{\"end\":31298,\"start\":31292},{\"end\":31314,\"start\":31298},{\"end\":31323,\"start\":31314},{\"end\":31336,\"start\":31323},{\"end\":31346,\"start\":31336},{\"end\":31568,\"start\":31561},{\"end\":31575,\"start\":31568},{\"end\":31581,\"start\":31575},{\"end\":31753,\"start\":31743},{\"end\":31764,\"start\":31753},{\"end\":31947,\"start\":31934},{\"end\":31961,\"start\":31947},{\"end\":31975,\"start\":31961},{\"end\":31984,\"start\":31975},{\"end\":32213,\"start\":32204},{\"end\":32223,\"start\":32213},{\"end\":32231,\"start\":32223},{\"end\":32242,\"start\":32231},{\"end\":32466,\"start\":32455},{\"end\":32475,\"start\":32466},{\"end\":32486,\"start\":32475},{\"end\":32673,\"start\":32661},{\"end\":32679,\"start\":32673},{\"end\":32885,\"start\":32877},{\"end\":32895,\"start\":32885},{\"end\":32905,\"start\":32895},{\"end\":33166,\"start\":33157},{\"end\":33175,\"start\":33166},{\"end\":33185,\"start\":33175},{\"end\":33198,\"start\":33185},{\"end\":33212,\"start\":33198},{\"end\":33222,\"start\":33212},{\"end\":33232,\"start\":33222},{\"end\":33242,\"start\":33232},{\"end\":33250,\"start\":33242},{\"end\":33258,\"start\":33250},{\"end\":33629,\"start\":33620},{\"end\":33638,\"start\":33629},{\"end\":33859,\"start\":33850},{\"end\":33868,\"start\":33859},{\"end\":33877,\"start\":33868},{\"end\":34082,\"start\":34073},{\"end\":34091,\"start\":34082},{\"end\":34100,\"start\":34091},{\"end\":34292,\"start\":34286},{\"end\":34303,\"start\":34292},{\"end\":34478,\"start\":34472},{\"end\":34489,\"start\":34478},{\"end\":34624,\"start\":34609},{\"end\":34632,\"start\":34624},{\"end\":34638,\"start\":34632},{\"end\":34648,\"start\":34638},{\"end\":34660,\"start\":34648},{\"end\":34666,\"start\":34660},{\"end\":34675,\"start\":34666},{\"end\":34687,\"start\":34675},{\"end\":34697,\"start\":34687},{\"end\":34710,\"start\":34697},{\"end\":35024,\"start\":35015},{\"end\":35033,\"start\":35024},{\"end\":35205,\"start\":35190},{\"end\":35216,\"start\":35205},{\"end\":35225,\"start\":35216},{\"end\":35384,\"start\":35376},{\"end\":35396,\"start\":35384},{\"end\":35406,\"start\":35396},{\"end\":35419,\"start\":35406},{\"end\":35622,\"start\":35610},{\"end\":35635,\"start\":35622},{\"end\":35814,\"start\":35802},{\"end\":35823,\"start\":35814},{\"end\":36050,\"start\":36043},{\"end\":36057,\"start\":36050},{\"end\":36065,\"start\":36057},{\"end\":36073,\"start\":36065},{\"end\":36081,\"start\":36073},{\"end\":36088,\"start\":36081},{\"end\":36466,\"start\":36454},{\"end\":36476,\"start\":36466},{\"end\":36487,\"start\":36476},{\"end\":36698,\"start\":36691},{\"end\":36705,\"start\":36698},{\"end\":36716,\"start\":36705},{\"end\":36726,\"start\":36716},{\"end\":36735,\"start\":36726},{\"end\":36919,\"start\":36912},{\"end\":36926,\"start\":36919},{\"end\":36936,\"start\":36926},{\"end\":36945,\"start\":36936},{\"end\":37121,\"start\":37114},{\"end\":37135,\"start\":37121},{\"end\":37142,\"start\":37135},{\"end\":37155,\"start\":37142},{\"end\":37388,\"start\":37382},{\"end\":37398,\"start\":37388},{\"end\":37567,\"start\":37559},{\"end\":37576,\"start\":37567},{\"end\":37586,\"start\":37576},{\"end\":37595,\"start\":37586}]", "bib_venue": "[{\"end\":29739,\"start\":29736},{\"end\":29975,\"start\":29971},{\"end\":30167,\"start\":30163},{\"end\":30364,\"start\":30360},{\"end\":30599,\"start\":30595},{\"end\":30878,\"start\":30869},{\"end\":31104,\"start\":31100},{\"end\":31350,\"start\":31346},{\"end\":31585,\"start\":31581},{\"end\":31768,\"start\":31764},{\"end\":31988,\"start\":31984},{\"end\":32246,\"start\":32242},{\"end\":32490,\"start\":32486},{\"end\":32683,\"start\":32679},{\"end\":32909,\"start\":32905},{\"end\":33155,\"start\":33071},{\"end\":33647,\"start\":33638},{\"end\":33881,\"start\":33877},{\"end\":34104,\"start\":34100},{\"end\":34307,\"start\":34303},{\"end\":34493,\"start\":34489},{\"end\":35037,\"start\":35033},{\"end\":35229,\"start\":35225},{\"end\":35423,\"start\":35419},{\"end\":35639,\"start\":35635},{\"end\":35800,\"start\":35755},{\"end\":36140,\"start\":36088},{\"end\":36491,\"start\":36487},{\"end\":36739,\"start\":36735},{\"end\":36949,\"start\":36945},{\"end\":37171,\"start\":37155},{\"end\":37402,\"start\":37398},{\"end\":37628,\"start\":37595},{\"end\":36179,\"start\":36142}]"}}}, "year": 2023, "month": 12, "day": 17}