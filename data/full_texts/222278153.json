{"id": 222278153, "updated": "2023-11-08 06:39:59.704", "metadata": {"title": "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection", "authors": "[{\"first\":\"Bojia\",\"last\":\"Zi\",\"middle\":[]},{\"first\":\"Minghao\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Jingjing\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xingjun\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Yu-Gang\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 1, "day": 5}, "abstract": "In recent years, the abuse of a face swap technique called deepfake Deepfake has raised enormous public concerns. So far, a large number of deepfake videos (known as\"deepfakes\") have been crafted and uploaded to the internet, calling for effective countermeasures. One promising countermeasure against deepfakes is deepfake detection. Several deepfake datasets have been released to support the training and testing of deepfake detectors, such as DeepfakeDetection and FaceForensics++. While this has greatly advanced deepfake detection, most of the real videos in these datasets are filmed with a few volunteer actors in limited scenes, and the fake videos are crafted by researchers using a few popular deepfake softwares. Detectors developed on these datasets may become less effective against real-world deepfakes on the internet. To better support detection against real-world deepfakes, in this paper, we introduce a new dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707 deepfake videos collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop and test the effectiveness of deepfake detectors against real-world deepfakes. We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a more challenging dataset, where the detection performance can decrease drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention masks on real/fake faces for improved detection. We empirically verify the effectiveness of ADDNets on both existing datasets and WildDeepfake. The dataset is available at:https://github.com/deepfakeinthewild/deepfake-in-the-wild.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2101.01456", "mag": "3092879151", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2101-01456", "doi": "10.1145/3394171.3413769"}}, "content": {"source": {"pdf_hash": "f55fd05aa99f12e13548c315532e50f06bb653ed", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2101.01456v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ffe4d011bbf7e4fb05fb6fb3bdeafcf1ccb5dcd4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f55fd05aa99f12e13548c315532e50f06bb653ed.txt", "contents": "\nWildDeepfake: A Challenging Real-World Dataset for Deepfake Detec-tion\nOctober 12-16, 2020. October 12-16, 2020\n\nBojia Zi \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nMinghao Chang \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nJingjing Chen \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nXingjun Ma \nSchool of Information Technology\nACM Reference Format\nDeakin University\nGeelongAustralia\n\nYu-Gang Jiang \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nBojia Zi \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nMinghao Chang \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nJingjing Chen \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nXingjun Ma \nSchool of Information Technology\nACM Reference Format\nDeakin University\nGeelongAustralia\n\nYu-Gang Jiang \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nWildDeepfake: A Challenging Real-World Dataset for Deepfake Detec-tion\n\nProceedings of the 28th ACM International Conference on Multimedia (MM '20)\nthe 28th ACM International Conference on Multimedia (MM '20)Seattle, WA, USA 2020; Seattle, WA, USAOctober 12-16, 2020. October 12-16, 202010.1145/3394171.3413769ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00. ACM, New York, NY, USA, 9 pages. https://CCS CONCEPTS \u2022 Computing methodologies \u2192 Artificial intelligenceCom- puter visionNeural networks KEYWORDS Datasetsdeep learningdeepfake detection\nFigure 1: WildDeepfake: a challenging real-word dataset for deepfake detection.ABSTRACTIn recent years, the abuse of a face swap technique called deepfake [2] has raised enormous public concerns. So far, a large number of deepfake videos (known as \"deepfakes\") have been crafted and uploaded to the internet, calling for effective countermeasures. One promising countermeasure against deepfakes is deepfake detection. Several deepfake datasets have been released to support the training and testing of deepfake detectors, such as DeepfakeDetection[1]and FaceForensics++[23]. While this has greatly advanced deepfake detection, most of the real videos in these datasets are filmed with a few volunteer actors in limited scenes, and the fake videos are crafted by researchers using a few popular deepfake softwares. Detectors developed on these datasets may become less effective against real-world deepfakes on the internet. To better support detection against real-world deepfakes, in this paper, we introduce * indicates corresponding author. attention mask element-wise multiplication ADD Block real fake 2D CNN Attention mask Generator (a) The proposed ADDNet-2D for image-level deepfake detection. ADD Block ADD Block ADD Block ADD-3d framework 3D CNN Input Attention Mask ADD Block real fake concat& reshape Attention mask generator Attention mask generator Attention mask generator ADD Block ADD Block ADD Block (b) The proposed ADDNet-3D for sequence-level deepfake detection.\n\na new dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707 deepfake videos collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop and test the effectiveness of deepfake detectors against real-world deepfakes. We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that Wild-Deepfake is indeed a more challenging dataset, where the detection performance can decrease drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention masks on real/fake faces for improved detection. We empirically verify the effectiveness of ADDNets on both existing datasets and WildDeepfake. The dataset is available at: https://github.com/deepfakeinthewild/deepfake-in-the-wild.\n\n\nINTRODUCTION\n\nDeepfake (or \"AI faceswap\") refers to the set of deep learning-based facial forgery techniques that can swap one person's face in a video to another person. Face swap is not new, however, the emerging of deep learning techniques such as autoencoders and generative adversarial networks [10] (GANs) makes face swap much easier and more convincing. Over the past few years, deepfakes have gone viral and a large number of deepfake videos (known as \"deepfakes\") have been crafted and uploaded to the internet. These fake videos have raised enormous public concerns for their huge risks to create political distress, blackmail someone or even fake terrorism events [9]. It is thus imperative to develop effective countermeasures to identify and reject deepfakes.\n\nOne promising countermeasure against deepfakes is deepfake detection. However, training deepfake detectors generally requires a large amount of both real and deepfake videos. This drives the collection of several deepfake datasets such as Celeb-DF [20], UADFV [31], Deepfake-TIMIT [15] and FaceForensics++ [23]. Recently, Google and JigSaw published a large dataset DeepfakeDetection [1] in the latest version of FaceForensics benchmark (eg. FaceForensics++) for deepfake detection. Another recent dataset was released by Facebook and Microsoft in the Deepfake Detection Challenge [9]. Most of these datasets are collected following a similar process: 1) collecting source (real) videos, then 2) crafting deepfake videos (based on the source videos) using several popular deepfake softwares. Since the fake videos are crafted by the researchers rather than real-world deepfakes uploaded to the internet, we denote the fake videos in these datasets as virtual deepfakes. Moreover, most of the source videos are filmed with a few volunteer actors in limited scenes. As such, virtual deepfakes may not fully represent the vast variety of wild deepfakes on the internet. We elaborate two potential weaknesses of existing virtual deepfake datasets as following:\n\n\u2022 Lack of diversity. Virtual deepfakes contain limited types of scenes, only a few persons (most of the time, a single person) in each scene, and similar facial expressions/movements (mostly talking). In contrast, wild deepfakes can have more than 10 persons in one scene, and the scenes varies significantly cross different videos. Moreover, the deepfake techniques used to craft virtual deepfakes only cover the few popular ones. However, wild deepfakes are crafted by many different types, versions or even combinations of deepfake softwares. And most of the time, the exact software used to create a wild deepfake is unknown. \u2022 Low quality. Via a preliminary inspection of virtual deepfakes, we find that many of the fake faces in these videos have obvious flaws. This may be because many virtual deepfakes are crafted in a short amount of time without careful adaptations for lighting, scene and a set of other factors. Consequently, the face regions in virtual deepfakes often have perceptible distortions such as jitters, blurs or strange artifacts. On the contrary, most wild deepfakes are deliberately tuned to have higher quality, may be via a long time of training on many high resolution face images.\n\nDue to the above two weaknesses, detectors trained on virtual deepfake datasets may not fully generalize to wild deepfakes in the real world. To better support the development and evaluation of more effective deepfake detectors, in this paper, we introduce a new deepfake dataset that is collected completely from the internet: WildDeepfake. Figure 3e illustrates several fake faces in our WildDeepfake dataset. In order to demonstrate the practical challenges in detecting wild deepfakes, we run extensive experiments with a set of baseline detection networks on both existing and our WildDeepfake datasets. We also propose two (eg. 2D and 3D versions) new Attention-based Deepfake Detection Networks (ADDNets) for more advanced deepfake detection. ADDNets exploit facial landmarks extracted by facial landmark detector to generate an attention mask to reweight the low-level features of a face, and then use reweighted low-level features to train either a 2D CNN detection network for image-level deepfake detection, or a 3D CNN detection network for sequence-level detection. In summary, our main contributions are:\n\n\u2022 We collect and annotate a new challenging real-world dataset for deepfake detection: WildDeepfake. Both the deepfake and real videos in WildDeepfake are collected purely from the internet. Compared to existing virtual deepfake datasets, WildDeepfake contains more diverse scenes, more persons in each scene and rich facial expressions. \u2022 We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that these detectors all perform well on existing datasets yet poorly on WildDeepfake. This confirms that real-world deepfakes are indeed more challenging than virtual deepfakes. \u2022 We propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) against real-world deepfakes, and empirically verify the effectiveness of our ADDNets on both existing and the proposed WildDeepfake datasets.\n\n\nRELATED WORK\n\nIn this section, we briefly review several commonly used deepfake techniques and existing deepfake detection methods.  \n\n\nDeepfake Generation\n\nOne commonly used deep learning technique for deepfake generation is the Generative Adversarial Networks (GANs) [10]. There exist many open source deepfake softwares on GitHub, such as Faceswap-GAN [3] and Faceswap [2]. Most of these deepfake softwares use an encoder-decoder architecture with one encoder and two decoders: the encoder learns the common features of the source (real) and the target (fake) faces, while the two decoders learn to generate the source and target faces separately. During the face swap process, the decoder associated with the source face takes the encoding of a target face and generate a fake source face. The attention mask of the source face is usually used to make the fake source face look more convincing via a fusion step. An overview of the face swap process is illustrated in Figure 2. The generated fake faces can be further improved by using more high resolution face images (both source and target) to train both the encoder and decoders. Given a video, the face in each frame can be generated to replace the original face following the above face swap procedure.\n\n\nDeepfake Detection\n\nA number of methods have been proposed to detect deepfake videos. Afchar et al. proposed the MesoNet [5], which uses three shallow (a few number of layers) networks to examine the mesoscopic properties of face images. G\u00fcera et al. [11] revealed that the frame sequence of deepfake videos have unique characteristics, which differentiates them from unmodified videos. Therefore, they proposed to use CNN to extract features of video frames, then use an LSTM network to perform sequence prediction [11]. Ciftci et al. proposed the FakeCatcher [8] for deepfake detection. FakeCatcher exploits the difference of biological signals hidden in videos to distinguish fake videos from real videos. Li et al. introduced two different methods [18,19] to identify possible \"artifacts\" or eye blinking defects in deepfake videos. Motivated by the observation that XceptionNet [7] has better sensitivity to deepfake images, R\u00f6ssler et al. used XceptionNet to detect deepfake images [23]. Recently, Li et al. proposed the Face X-ray [17] to detect the trace of modification around the boundary regions of fake faces. Note that many of the above methods require pixel-level or image-level ground truth, which is not obtainable for real-world deepfakes. In this paper, we will test those methods that do not rely on pixel/image-level annotations, i.e., XceptionNet and MesoNets. We conduct a systematic evaluation of XceptionNet, MesoNets and a set of conventional CNN networks on both existing and WildDeepfake datasets.\n\n\nDATASETS FOR DEEPFAKE DETECTION\n\nIn this section, we first summarize existing deepfake datasets, then introduce the collection and annotation process of our WildDeepfake dataset.\n\n\nExisting Deepfake Datasets\n\nThe UADFV [31] dataset contains 45 real videos and 45 deepfake videos, with the deepfake videos were crafted based on the real videos by applying some deepfake techniques. The Deepfake-TIMIT [15] dataset was created based on the VidTimit dataset: 320 low quality and 320 high quality deepfake videos were crafted based on 320 real videos from VidTimit. The FaceFornesics++ [23] dataset has 1,000 real videos collected from YouTube, based on which 1,000 deepfake videos were generated by applying each of the 4 face modification techniques: Deepfake [2], Face2Face [29], Faceswap [4] and Neural Texture [28] (eg. overall 4,000 face modification videos were created). These fake videos produce 1.8 million manipulated face images. Recently, Google and JigSaw released the DeepfakeDetection [1] dataset: 363 real videos were filmed with the assistance of 28 volunteer actors, based on which over 3,600 deepfake videos were then generated using a few deepfake techniques. More recently, AWS, Facebook, Microsoft, the Partnership on AI's Media Integrity Steering Committee, and a number of academics collected and published a large-scale deepfake dataset for the Deepfake Detection Challenge (DFDC) [9]. DFDC dataset consists of \u223c20,000 real videos filmed with hundreds of actors, and over 10,0000 fake videos generated using varies deepfake techniques. Table 2 summarizes these existing datasets. The deepfake videos in these datasets were all crafted by researchers applying a few popular deepfake techniques. They were not deliberately tuned to achieve the best  visual effects, and some of the generated fake faces have obvious flaws. Detectors trained on these dataset may not generalize well to wild deepfakes. Next, we introduce our WildDeepfake dataset.\n\n\nWildDeepfake Dataset\n\nWe first collect over 1,200 deepfake videos from varies video-sharing websites. We collect these videos by searching their titles with keyword \"deepfake\". We remove those fake videos that were crafted using traditional face manipulations rather than deepfake techniques. We determine the type of the forgery technique by the title and description of the video. We then manually check and search the real video for each of the deepfake video. We remove those deepfake videos that do not have a real version. This leaves us 707 well-made deepfake videos from the internet. Data Processing. We use the Mtcnn [32] face detector to identify the face regions in each video frame. We then extract features for  Figure 4: A feature perspective comparison of 6 deepfake datasets. We use an ImageNet-pretrained ResNetV2-101 network to extract features and t-SNE [21] for dimensionality reduction.\n\nthe face regions using an ImageNet-pretrained MobileNetV2 [25] network. Next, we use the facial landmark extracted by dlib [13] landmark detector to align all the faces in a face sequence. This avoids the negative impact of face orientation to the training of deepfake detectors. Face Sequence Annotation. We train 3 human annotators by explaining the background knowledge of deepfake generation, the common defects and characteristics of deepfake videos. After training, the 3 annotators were asked to 1) label the type (eg. real, fake or unknown) of each face sequence by checking the title of the collected video; 2) locate its source (real) video if a video is deepfake and check whether there are defects in the source video; 3) label the face sequence of the source video as real if there are no obvious defects, otherwise label it as unknown.\n\nWe only save the face sequences of real and fake categories, while discard the unknown categories. We also discard those face sequences that have received different labels from the 3 annotators. Eventually, 1,180,099 face images of 7,314 face sequences from 707 videos were collected and annotated. It is worth mentioning that both the data processing and annotation is labour intensive and time consuming: labeling the 707 videos can take months.\n\nThe 7,314 face sequences in our dataset were further splitted into 6,508 for training and 806 for testing. This was done based on the similarities between the face sequences, which to some extent, ensures the training set having different face sequences from the test set. Figure 3 illustrates several deepfake video frames in varies datasets. Figure 4 shows the difference of the 6 datasets with respect to the features of the face images extracted by an ImageNetpretrained ResNetV2-101 network. The major characteristics of our WildDeepfake dataset can be summarized as follows:\n\n\u2022 Both the real and deepfake videos in WildDeepfake were collected from the internet. \u2022 The video contents in WildDeepfake are more diverse: a variety of activities (eg. broadcasting, movies, interviews, talks, and many others), diverse scenes, backgrounds and illumination conditions, and different compression rates, resolutions and formats. \u2022 The deepfake videos in WildDeepfake were well-made, possibly owing to longer time of training with many high quality face images.\n\n\nPROPOSED ADDNETS FOR DEEPFAKE DETECTION\n\nIn this section, we introduce the proposed Attention-based Deepfake Detection Networks (ADDNets). We first formulate the detection problem as follows.\n\n\nProblem Formulation\n\nGiven a deepfake dataset D = {( , ) ( ) } =1 with \u2208 X \u2282 R \u00d7 \u00d7 \u00d7 and \u2208 Y = {0, 1} denoting a video and its class label. , , and represent the number of frames, frame width, frame height and color channel, respectively. Deepfake datasets consist of two types of videos: real videos with class label = 0 and deepfake videos with class label = 1. The goal of deepfake detection is to train a binary classifier (as represented by a deep neural network) that maps the video space to the class space: : X \u2192 Y. This can be achieved by minimizing the classification error of on training data D:\n\nargmin\nE ( , ) \u2208D \u2113 ( ( ), ),(1)\nwhere \u2113 is a loss function such as the commonly used Cross Entropy (CE) loss, and are the trainable parameters of network .\n\nIn this paper, we focus on training a CNN detection network . In practice, the input videos (both real and deepfake) are processed to extract face images, which can then be used to train a detection network. The reason why not directly train on raw video or video frames is that deepfakes only alter the face region. Generally, there are two levels of deepfake detection networks: image-level and sequence-level. Image-level detection networks work on individual face images without considering the sequential information contained in the face sequence, while sequence-level detection networks work on the full face sequences. Next, we propose two detection networks for image-and sequence-level deepfake detection.\n\n\nProposed Detection Networks\n\nWhile existing works are mostly focused on identifying the flaws in a face image/sequence, most wild deepfake videos are carefully tuned to have no obvious flaws. Motivated the observation that many deepfake techniques use an attention mask to fuse the target (fake) face into the source (true) face (see Figure 2), we propose to exploit the attention mask to better differentiate between the real and the fake faces. The attention-based face fusion operation can be defined as:\n= \u2299 ( \u2212 ) + \u2299 ,(2)\nwhere \u2299 represents the element-wise multiplication, is the face generated by a neural network, \u2208 [0, 1] is the attention mask used in face fusion, is the identity matrix which has the same dimension as matrix , is the source (true) face and is the output fake face. Attention mask defines the key areas of the face, such as eye, nose, and mouth. The proposed ADDNets are illustrated in Figure 5. We introduce two versions of ADDNet: a 2D ADDNet (ADDNet-2D) for image-level deepfake detection and a 3D ADDNet (ADDNet-3D) for sequence-level deepfake detection. Figure 5a, the 2D ADDNet consists of an ADD block, which is followed by a 2D CNN network and a classification layer. It takes one face image and the attention mask of the face image as inputs, and outputs the probabilities of the input face image being real (class 0) or fake (class 1). The input attention mask is generated via an Attention Mask Generation module, which will be explained in detail below. Note that the attention mask generated here are different to the ones used to craft deepfakes, which are unknown. The ADD block follows an XceptionNet architecture, which learns different levels of features of the the face image. Different to conventional CNN network, the features at the intermediate layers of the ADD block are adjusted by the scaled attention masks (in green color). More specifically, we scale the input attention mask to match the output resolution (eg. width and height) of a particular layer using average pooling, then apply an element-wise multiplication between the scaled attention mask and the feature map of that layer. This allows the use of attention to adjust the feature map at different abstraction levels. We find that this is generally more effective than only applying the attention mask to the input layer. The output of the ADD block is then passed into a conventional 2D CNN network for classification. The output layer of the entire network is convolutionally (not fully) connected to the second last layer, and has two neurons corresponding to the two classes (eg. real vs fake).\n\n\nADDNet-2D. As illustrated in\n\n\nADDNet-3D.\n\nWe also propose a 3D ADDNet for sequencelevel detection. As illustrated in Figure 5b, the network has multiple ADD blocks, the outputs of which are concatenated and reshaped before passing into a 3D CNN network for classification. Note that all ADD blocks in ADDNet-3D share the same weights. The network takes inputs of face sequences and its corresponding attention mask sequences. We denote the sequence length as . For each face image in the sequence, we generate its attention mask using the same attention mask generation module as used in ADDNet-2D. Each pair of a face image and its attention mask is processed by one separate ADD block, which also has the same structure as is used in ADDNet-2D.\n\n\nAttention mask Input image\n\nFace region Facial landmark \n\n\nAttention Mask Generation\n\nModule. The attention mask generation process is shown in Figure 6.\n\nGiven a face image , the attention mask is generated in 4 steps. We first apply a landmark detection method to identify the 68-points facial landmark of the face area and align the face according to the landmark. We then use the landmark to generate a binary face mask that contains the entire face region. We apply the same process to generate a second organ mask that contains the eyes, nose and mouth. We use Gaussian blur to smooth the edges of both masks (eg. face and organ masks). Finally, we add up the two masks and normalize it into the value range of [0, 1]. The generated attention mask is used in the ADD block to adjust the feature maps of the face image, as we have introduced above.\n\nNote that sequence-level detection can also be achieved by an ADDNet+LSTM architecture, which can be obtained by replacing the 3D CNN network in 3D ADDNet (see Figure 5b) by an LSTM network. The main difference between our ADDNets and existing detection networks is the application of attention-based feature adjustments at multiple layers of the ADD block.\n\n\nEXPERIMENTS\n\nIn this section, we provide a systematic evaluation of a set of detection networks on both existing deepfake datasets and our Wild-Deepfake.\n\n\nExperimental Settings\n\nDatasets. We consider three existing datasets DeepfakeDetection (DFD) [1], Deepfake-TIMIT (DF-TIMIT) [15] and FaceForensics++ (FF++) [23]. For DF-TIMIT and FF++ datasets, we consider both their low quality (resolution) (LQ) and high quality (resolution) (HQ) versions. For FF++, we only consider its deepfake subset. We also test the detection networks on our WildDeepfake dataset. Overall, we run experiments on 6 datasets: DFD, DF-TIMIT LQ, DF-TIMIT HQ, FF++ LQ, FF++ HQ and WildDeepfake.\n\nBaseline Detection Networks. For image-level detection, we compare our ADDNet-2D with 10 detection networks including AlexNet, VGG16, ResNetV2-50/101/152, Inception-v2, XceptionNet [7], MesoNet-1, MesoNet-4 and MesoNet-Inception. The first 6 networks are stateof-the-art CNN networks proposed for image classification. We consider these networks to test the detection performance if directly applying a conventional CNN network in deepfake detection. The XceptionNet, MesoNet-1 [5], MesoNet-4 and MesoNet-Inception are previously proposed detection networks for deepfake detection. For sequence-level detection, we compare our ADDNet-3D with P3D [22], C3D [30] and I3D [6], which are three state-of-the-art 3D networks for video recognition. Note that, we did not consider those existing methods that require pixel-level ground truth or the whole image, which is not obtainable for wild deepfakes. Training Setting. For image-level detection, we set the input size to 224 \u00d7 224. For sequence-level detection, we train and test with clips (one clip contains 50 frames, i.e., sequence length = 50) and set the input image size to 112 \u00d7 112. All networks including both image-and sequence-level detection networks are trained using cross entropy loss and Adam optimizer [14] with batch size 32. We set the initial learning rate to 0.0001 which is decayed by a factor of 0.9 after every 3K steps of training. All networks are trained for 40,000 iterations. We choose the model with best accuracy as the final model.\n\n\nResults and Analysis\n\nWe take the detection accuracy of deepfake videos as a performance metric, and discuss the detection performance separately for imagelevel and sequence-level detection networks. Table 3 reports the detection accuracies of all 2D detection networks. As can be observed, although with certain variations, the baseline networks all demonstrate a good performance on existing datasets, especially on the high-quality ones. Particularly, the best baseline network achieves a high accuracy of 90.02% on DFD (by VGG16), 99.65% on DF-TIMIT LQ (by XceptionNet), 99.91% on DF-TIMIT HQ (by XceptionNet), 90.91% on FF++ LQ (by ResNetV2-50) and 99.62% on FF++ HQ (by XceptionNet). Among the baseline networks, the XceptionNet achieves the best performance on three out of the five existing datasets. The performance of these networks decreases drastically on WildDeepfake dataset: no baseline networks can achieve accuracy above 70%. This not only confirms that real-world deepfakes are indeed more difficult to be detected than virtual deepfakes, the effectiveness of detectors developed on virtual deepfake datasets can be limited when applied to detect wild deepfakes.\n\n\nImage-level Deepfake Detection.\n\nOur proposed ADDNet-2D achieves a comparable performance to the best baseline networks on existing datasets, and a significantly better performance on the more challenging WildDeepfake dataset. Particularly, on DFD dataset, ADDNet-2D outperform the best network XceptionNet by 11%, while on other existing datasets it exhibits an accuracy that is within 1% if not better than the best baseline networks. On WildDeepfake dataset, ADDNet-2D outperforms the best baseline network XceptionNet by 7%. The consistent and superior performance of our ADDNet-2D network verifies the importance of attention-based feature adjustment for deepfake detection. Note that, even our ADDNet-2D can not fully address the   Table 4. Among the baseline networks, I3D achieves the best performance on almost all tested datasets, except DFD where C3D is more effective. Particularly, I3D achieves a high detection accuracy of > 89% on the four DF-TIMIT and FF++ datasets, while C3D has an accuracy of 73.18% on DFD. Again, they all drop significantly on WildDeepfake with a much lower accuracy of < 63%. Compared to the I3D and C3D, P3D has the worst performance, which indicates that the pseudo 3D convolutions used in P3D are not sensitive enough to small deepfake modifications. For our ADDNet-3D, it is not as effective as I3D on two DF-TIMIT datasets and the low quality FF++, although it achieves a surprisingly higher accuracy of 94.93% on DFD and outperforms I3D on higher quality FF++. On WildDeepfake, ADDNet-3D demonstrates a 65.50% accuracy, which is \u223c 2% higher than the best baseline model I3D. However, this performance is much lower than ADDNet-2D (76.25%). Compared to 2D detection networks, we find that 3D networks are generally less effective. One possible reason for the performance degradation of 3D detection networks is that the temporal information contained in deepfake face sequences are also distorted by the frame-by-frame generation of the fake faces. And such variations are very likely inconsistent across different frames or videos. This indicates that the temporal information in deepfake videos should be treated differently to that in real videos to improve the accuracy of sequence-level deepfake detection.\n\n\nCONCLUSION\n\nIn this paper, we proposed a challenging real-world dataset Wild-Deepfake for deepfake detection. WildDeepfake dataset consists of 1,180,099 images of 7,314 face sequences extracted from 707 videos (both deepfake and real). Compared with existing virtual deepfake datasets, our WildDeepfake dataset was collected completely from the web, thus contains more diverse scenes, faces and activities. Moreover, the deepfake videos in our dataset are of high quality. WildDeepfake can serve as a useful supplementary to existing datasets to support the development and evaluation of more effective deepfake detectors against real-world deepfakes. We conducted a systemic evaluation of a set of baseline networks on both existing and our WildDeepfake datasets, and found that WildDeepfake is indeed a more challenging dataset where the performance of baseline detectors can decrease drastically. We also propose two Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention-based feature adjustment for more accurate deepfake detection. Our 2D version of ADDNet (eg. ADDNet-2D) demonstrated a better or at least a comparable performance to the state-of-the-art, consistently across all existing and our Wild-Deepfake datasets. We believe that, with our WildDeepfake dataset and ADDNets, more advanced countermeasures against real-world deepfakes can be developed in the future.\n\nFigure 2 :\n2Illustration of the face swap process.\n\nFigure 3 :\n3WildDeepfake versus 5 existing datasets. There are more diverse scenes in WildDeepfake and the fake faces look more realistic, reflecting the challenging real-world scenario. To protect privacy, we block the eye regions of the fake images.\n\nFigure 5 :\n5The structures of our ADDNet detection networks. The input size of 2D ADDNet is \u00d7 \u00d7 , and that of the 3D ADDNet is \u00d7 \u00d7 \u00d7 : : input width, : input height, : the number of channels, and : sequence length.\n\nFigure 6 :\n6The Attention Mask Generation module.\n\nTable 1 :\n1A summary of existing deepfake detection methods.Method \nDataset \nModel \nClaimed Performance \n\nMesoNet [5] \nPrivate web data \nCNN \nDetection rate: 98% \n\nGuera et al. [11] \nPrivate web data \nCNN+LSTM \nAccuracy: 97.1% \n\nFakeCatcher [8] \nFaceForensics++, \nPrivate web data \nTraditional operator+CNN \nFaceForensics++ accuracy: 96% \nPrivate web data accuracy: 91.07% \n\nLi et al. (1) [18] \nPrivate web data \nCNN+LSTM \nAuc: 0.99 \n\nLi et al. (2) [19] \nUADFV, \nDeepfake-TIMIT \nCNN \n\nUADFV Auc: 0.974 \nDeepfake-TIMIT(LQ) Auc: 0.999 \nDeepfake-TIMIT(HQ) Auc: 0.932 \n\nXceptionNet [23] \nFaceForensics++ \nXceptionNet \n\nRaw accuracy(Deepfake): 99.26% \nHQ accuracy(Deepfake): 95.73% \nLQ accuracy(Deepfake): 81.00% \n\nFace X-ray [17] \n\nCeleb-DF, \nDFDC preview, \nDeepfakeDetection \nand Faceforensics++ \n\nFCN+Self-supervised learning \n\nFaceForensics++(Deepfake) Auc: 0.9917 \nDFDC preview Auc: 0.9540 \nDFDC Auc: 0.8092 \nCelab-DF Auc: 0.8058 \n\n\n\nTable 2 :\n2A comparison of WildDeepfake with existing datasets for deepfake detection. LQ: low-quality; HQ: high-quality.Dataset \n#Real face \nsequences \n\n#Fake face \nsequences \n#Actors \nReal video \nsource \n\nDeepfake video \nsource \n\nDeepfake-TIMIT \n320 \nLQ:320 \nHQ:320 \n32 \nVidTIMIT Dataset [24] Manually crafted \n\nFaceForensics++ \n(Deepfake) \n\nRaw:1,000 \nHQ:1,000 \nLQ:1,000 \n\nRaw:1,000 \nHQ:1,000 \nLQ:1,000 \n\n977 \nYouTube \nManually crafted \n\nCelab-DF v2 \n590 \n5,639 \n59 \nYouTube \nManually crafted \nDeepfakeDetection \n363 \n3,068 \n28 \nVolunteer Actors \nManually crafted \nDFDC-preview \n1,131 \n4,113 \n66 \nVolunteer Actors \nManually crafted \nDFDC \n\u223c 20,000 \n\u223c100,000 \n-\nVolunteer Actors \nManually crafted \n\nWildDeepfake(ours) \n3,805 \n3,509 \n-\nInternet \nInternet \n\n\n\nTable 3 :\n3Image-level detection accuracy of different 2D detection networks. LQ: low quality, HQ: high quality.Network \nDFD \nDF-TIMIT \nLQ \n\nDF-TIMIT \nHQ \n\nFF++(Deepfake) \nLQ \n\nFF++(Deepfake) \nHQ \n\nWild-\nDeepfake \nAlexNet [16] \n84.37% \n94.77% \n83.22% \n90.58% \n95.52% \n60.37% \nVGG16 [26] \n90.02% \n98.73% \n76.92% \n90.19% \n98.89% \n60.92% \nResNetV2-50 [12] \n83.68% \n94.88% \n89.51% \n90.91% \n98.59% \n63.99% \nResNetV2-101 [12] \n81.77% \n94.78% \n87.09% \n88.67% \n98.72% \n58.73% \nResNetV2-152 [12] \n83.15% \n95.68% \n88.27% \n88.00% \n97.57% \n59.33% \nInception-v2 [27] \n72.64% \n90.30% \n77.92% \n89.44% \n96.67% \n62.12% \nMesoNet-1 [5] \n75.95% \n92.07% \n79.98% \n81.97% \n96.40% \n60.51% \nMesoNet-4 [5] \n85.02% \n91.18% \n83.71% \n87.75% \n97.04% \n64.47% \nMesoNet-inception [5] 70.71% \n97.85% \n85.28% \n84.82% \n97.16% \n66.03% \nXceptionNet [7] \n85.82% \n99.65% \n99.91% \n90.25% \n99.62% \n69.25% \nADDNet-2D \n(ours) \n97.51% \n99.54% \n99.22% \n90.42% \n99.82% \n76.25% \n\n\n\nTable 4 :\n4Sequence-level detection accuracy of different 3D detection networks. LQ: low quality, HQ: high quality. Sequence-level Deepfake Detection. The detection accuracies of different 3D detection networks for sequence-level detection are reported inNetwork \nDFD \nDF-TIMIT \nLQ \n\nDF-TIMIT \nHQ \n\nFF++(Deepfake) \nLQ \n\nFF++(Deepfake) \nHQ \n\nWild-\nDeepfake \nP3D [22] \n70.16% \n76.71% \n62.25% \n67.05% \n75.23% \n53.20% \nC3D [30] \n73.18% \n94.44% \n82.38% \n87.72% \n95.00% \n55.87% \nI3D [6] \n67.83% \n96.38% \n89.85% \n93.18% \n96.70% \n62.69% \nADDNet-3D \n(ours) \n94.93% \n90.17% \n85.75% \n90.11% \n98.30% \n65.50% \n\nchallenge of detecting wild deepfakes: the detection accuracy is \nonly 76.25%. \n\n5.2.2 \ninput image or feature map\n\nDeep Fake Detection Dataset. 2] 2019. Deepfakes github2019. Deep Fake Detection Dataset. https://ai.googleblog.com/2019/09/ contributing-data-to-deepfake-detection.html Accessed October 29, 2019. [2] 2019. Deepfakes github. https://github.com/deepfakes/faceswap Accessed October 29, 2019.\n\nDeepfakes github shaoanlu version. 2019. Deepfakes github shaoanlu version. https://github.com/shaoanlu/faceswap- GAN Accessed October 29, 2019.\n\nMesoNet: a Compact Facial Video Forgery Detection Network. D Afchar, V Nozick, J Yamagishi, I Echizen, WIFS. D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen. 2018. MesoNet: a Compact Facial Video Forgery Detection Network. In WIFS.\n\nJ Carreira, A Zisserman, Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In CVPR. J. Carreira and A. Zisserman. 2017. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In CVPR.\n\nXception: Deep learning with depthwise separable convolutions. F Chollet, CVPR. F. Chollet. 2017. Xception: Deep learning with depthwise separable convolutions. In CVPR.\n\nFakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals. U Ciftci, I Demir, arXiv:1901.02212U. Ciftci and I. Demir. 2019. FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals. In arXiv:1901.02212.\n\nThe Deepfake Detection Challenge (DFDC) Preview Dataset. B Dolhansky, R Howes, B Pflaum, N Baram, C Ferrer, arXiv:1910.08854B. Dolhansky, R. Howes, B. Pflaum, N. Baram, and C. Ferrer. 2019. The Deepfake Detection Challenge (DFDC) Preview Dataset. In arXiv:1910.08854.\n\nGenerative Adversarial Nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. 2014. Generative Adversarial Nets. In NIPS.\n\nDeepfake Video Detection Using Recurrent Neural Networks. D G\u00fcera, E Delp, AVSS. D. G\u00fcera and E. Delp. 2018. Deepfake Video Detection Using Recurrent Neural Networks. In AVSS.\n\nIdentity Mappings in Deep Residual Networks. K He, X Zhang, S Ren, J Sun, ECCV. K. He, X. Zhang, S. Ren, and J. Sun. 2016. Identity Mappings in Deep Residual Networks. In ECCV.\n\nDlib-Ml: A Machine Learning Toolkit. D King, Journal of Machine Learning Research. D. King. 2009. Dlib-Ml: A Machine Learning Toolkit. Journal of Machine Learning Research (2009).\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980D. Kingma and J. Ba. 2014. Adam: A method for stochastic optimization. In arXiv:1412.6980.\n\nDeepFakes: a New Threat to Face Recognition? Assessment and Detection. P Korshunov, S Marcel, arXiv:1812.08685P. Korshunov and S. Marcel. 2018. DeepFakes: a New Threat to Face Recognition? Assessment and Detection. In arXiv:1812.08685.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, In NIPSA. Krizhevsky, I. Sutskever, and G. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In NIPS.\n\n2020. Face X-Ray for More General Face Forgery Detection. L Li, J Bao, T Zhang, H Yang, D Chen, F Wen, B Guo, CVPR. L. Li, J. Bao, T. Zhang, H. Yang, D. Chen, F. Wen, and B. Guo. 2020. Face X-Ray for More General Face Forgery Detection. In CVPR.\n\nIn ictu oculi: Exposing ai created fake videos by detecting eye blinking. Y Li, M Chang, S Lyu, WIFS. Y. Li, M. Chang, and S. Lyu. 2018. In ictu oculi: Exposing ai created fake videos by detecting eye blinking. In WIFS.\n\nExposing deepfake videos by detecting face warping artifacts. Y Li, S Lyu, arXiv:1811.00656Y. Li and S. Lyu. 2018. Exposing deepfake videos by detecting face warping artifacts. In arXiv:1811.00656.\n\nCeleb-DF: A Large-scale Challenging Dataset for DeepFake Forensics. Y Li, P Sun, H Qi, S Lyu, CVPR. Y. Li, P. Sun, H. Qi, and S. Lyu. 2020. Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics. In CVPR.\n\nVisualizing data using t-SNE. L Maaten, G Hinton, Journal of Machine Learning Research. L. Maaten and G. Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research (2008).\n\nLearning Spatio-Temporal Representation with Pseudo-3D Residual Networks. Z Qiu, T Yao, T Mei, ICCV. Z. Qiu, T. Yao, and T. Mei. 2017. Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks. In ICCV.\n\nFaceForensics++: Learning to Detect Manipulated Facial Images. A Rossler, D Cozzolino, L Verdoliva, C Riess, J Thies, M Niessner, ICCV. A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Niessner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In ICCV.\n\nThe VidTIMIT Database. C Sanderson, C. Sanderson. 2004. The VidTIMIT Database.\n\nM Sandler, A Howard, M Zhu, A Zhmoginov, L Chen, MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPRM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In CVPR.\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition. K Simonyan, A Zisserman, ICLR. K. Simonyan and A. Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR.\n\nRethinking the Inception Architecture for Computer Vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In CVPR.\n\nDeferred neural rendering: Image synthesis using neural textures. J Thies, M Zollh\u00f6fer, M Nie\u00dfner, ACM Transactions on Graphics. J. Thies, M. Zollh\u00f6fer, and M. Nie\u00dfner. 2019. Deferred neural rendering: Image synthesis using neural textures. ACM Transactions on Graphics (2019).\n\nFace2Face: Real-Time Face Capture and Reenactment of RGB Videos. J Thies, M Zollh\u00f6fer, M Stamminger, C Theobalt, M Nie\u00dfner, CVPR. J. Thies, M. Zollh\u00f6fer, M. Stamminger, C. Theobalt, and M. Nie\u00dfner. 2016. Face2Face: Real-Time Face Capture and Reenactment of RGB Videos. In CVPR.\n\nC3D: Generic Features for Video Analysis. D Tran, L Bourdev, R Fergus, L Torresani, M Paluri, arXiv:1412.0767D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. 2014. C3D: Generic Features for Video Analysis. In arXiv:1412.0767.\n\nExposing Deep Fakes Using Inconsistent Head Poses. X Yang, Y Li, S Lyu, ICASSP. X. Yang, Y. Li, and S. Lyu. 2019. Exposing Deep Fakes Using Inconsistent Head Poses. In ICASSP.\n\nJoint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks. K Zhang, Z Zhang, Z Li, Y Qiao, IEEE Signal Processing Letters. K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. 2016. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks. IEEE Signal Processing Letters (2016).\n", "annotations": {"author": "[{\"end\":224,\"start\":114},{\"end\":340,\"start\":225},{\"end\":456,\"start\":341},{\"end\":558,\"start\":457},{\"end\":674,\"start\":559},{\"end\":785,\"start\":675},{\"end\":901,\"start\":786},{\"end\":1017,\"start\":902},{\"end\":1119,\"start\":1018},{\"end\":1235,\"start\":1120}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":120},{\"end\":238,\"start\":233},{\"end\":354,\"start\":350},{\"end\":467,\"start\":465},{\"end\":572,\"start\":567},{\"end\":683,\"start\":681},{\"end\":799,\"start\":794},{\"end\":915,\"start\":911},{\"end\":1028,\"start\":1026},{\"end\":1133,\"start\":1128}]", "author_first_name": "[{\"end\":119,\"start\":114},{\"end\":232,\"start\":225},{\"end\":349,\"start\":341},{\"end\":464,\"start\":457},{\"end\":566,\"start\":559},{\"end\":680,\"start\":675},{\"end\":793,\"start\":786},{\"end\":910,\"start\":902},{\"end\":1025,\"start\":1018},{\"end\":1127,\"start\":1120}]", "author_affiliation": "[{\"end\":223,\"start\":124},{\"end\":339,\"start\":240},{\"end\":455,\"start\":356},{\"end\":557,\"start\":469},{\"end\":673,\"start\":574},{\"end\":784,\"start\":685},{\"end\":900,\"start\":801},{\"end\":1016,\"start\":917},{\"end\":1118,\"start\":1030},{\"end\":1234,\"start\":1135}]", "title": "[{\"end\":71,\"start\":1},{\"end\":1306,\"start\":1236}]", "venue": "[{\"end\":1383,\"start\":1308}]", "abstract": "[{\"end\":3262,\"start\":1779}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4481,\"start\":4477},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4855,\"start\":4852},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5203,\"start\":5199},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5215,\"start\":5211},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5236,\"start\":5232},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5261,\"start\":5257},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5338,\"start\":5335},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5535,\"start\":5532},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9699,\"start\":9695},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9784,\"start\":9781},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10815,\"start\":10812},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10946,\"start\":10942},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11211,\"start\":11207},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11255,\"start\":11252},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11447,\"start\":11443},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11450,\"start\":11447},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11577,\"start\":11574},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11683,\"start\":11679},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11733,\"start\":11729},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12441,\"start\":12437},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12622,\"start\":12618},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12804,\"start\":12800},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12995,\"start\":12991},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13033,\"start\":13029},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13218,\"start\":13215},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13624,\"start\":13621},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14817,\"start\":14813},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15064,\"start\":15060},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15158,\"start\":15154},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15223,\"start\":15219},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23969,\"start\":23966},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24001,\"start\":23997},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24033,\"start\":24029},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24572,\"start\":24569},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24869,\"start\":24866},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25038,\"start\":25034},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25048,\"start\":25044},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25060,\"start\":25057},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25659,\"start\":25655}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":30798,\"start\":30747},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31051,\"start\":30799},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31267,\"start\":31052},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31318,\"start\":31268},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32252,\"start\":31319},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33012,\"start\":32253},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33946,\"start\":33013},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34633,\"start\":33947}]", "paragraph": "[{\"end\":4174,\"start\":3264},{\"end\":4949,\"start\":4191},{\"end\":6207,\"start\":4951},{\"end\":7421,\"start\":6209},{\"end\":8541,\"start\":7423},{\"end\":9423,\"start\":8543},{\"end\":9559,\"start\":9440},{\"end\":10688,\"start\":9583},{\"end\":12215,\"start\":10711},{\"end\":12396,\"start\":12251},{\"end\":14183,\"start\":12427},{\"end\":15094,\"start\":14208},{\"end\":15945,\"start\":15096},{\"end\":16394,\"start\":15947},{\"end\":16976,\"start\":16396},{\"end\":17453,\"start\":16978},{\"end\":17647,\"start\":17497},{\"end\":18256,\"start\":17671},{\"end\":18264,\"start\":18258},{\"end\":18414,\"start\":18291},{\"end\":19131,\"start\":18416},{\"end\":19641,\"start\":19163},{\"end\":21749,\"start\":19661},{\"end\":22499,\"start\":21795},{\"end\":22558,\"start\":22530},{\"end\":22655,\"start\":22588},{\"end\":23355,\"start\":22657},{\"end\":23714,\"start\":23357},{\"end\":23870,\"start\":23730},{\"end\":24386,\"start\":23896},{\"end\":25899,\"start\":24388},{\"end\":27082,\"start\":25924},{\"end\":29340,\"start\":27118},{\"end\":30746,\"start\":29355}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18290,\"start\":18265},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19660,\"start\":19642}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13783,\"start\":13776},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26109,\"start\":26102},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27830,\"start\":27823}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4189,\"start\":4177},{\"attributes\":{\"n\":\"2\"},\"end\":9438,\"start\":9426},{\"attributes\":{\"n\":\"2.1\"},\"end\":9581,\"start\":9562},{\"attributes\":{\"n\":\"2.2\"},\"end\":10709,\"start\":10691},{\"attributes\":{\"n\":\"3\"},\"end\":12249,\"start\":12218},{\"attributes\":{\"n\":\"3.1\"},\"end\":12425,\"start\":12399},{\"attributes\":{\"n\":\"3.2\"},\"end\":14206,\"start\":14186},{\"attributes\":{\"n\":\"4\"},\"end\":17495,\"start\":17456},{\"attributes\":{\"n\":\"4.1\"},\"end\":17669,\"start\":17650},{\"attributes\":{\"n\":\"4.2\"},\"end\":19161,\"start\":19134},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":21780,\"start\":21752},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":21793,\"start\":21783},{\"end\":22528,\"start\":22502},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":22586,\"start\":22561},{\"attributes\":{\"n\":\"5\"},\"end\":23728,\"start\":23717},{\"attributes\":{\"n\":\"5.1\"},\"end\":23894,\"start\":23873},{\"attributes\":{\"n\":\"5.2\"},\"end\":25922,\"start\":25902},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":27116,\"start\":27085},{\"attributes\":{\"n\":\"6\"},\"end\":29353,\"start\":29343},{\"end\":30758,\"start\":30748},{\"end\":30810,\"start\":30800},{\"end\":31063,\"start\":31053},{\"end\":31279,\"start\":31269},{\"end\":31329,\"start\":31320},{\"end\":32263,\"start\":32254},{\"end\":33023,\"start\":33014},{\"end\":33957,\"start\":33948}]", "table": "[{\"end\":32252,\"start\":31380},{\"end\":33012,\"start\":32375},{\"end\":33946,\"start\":33126},{\"end\":34633,\"start\":34203}]", "figure_caption": "[{\"end\":30798,\"start\":30760},{\"end\":31051,\"start\":30812},{\"end\":31267,\"start\":31065},{\"end\":31318,\"start\":31281},{\"end\":31380,\"start\":31331},{\"end\":32375,\"start\":32265},{\"end\":33126,\"start\":33025},{\"end\":34203,\"start\":33959}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7774,\"start\":7765},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10406,\"start\":10398},{\"end\":14920,\"start\":14912},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16677,\"start\":16669},{\"end\":16748,\"start\":16740},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19477,\"start\":19468},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20055,\"start\":20047},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20229,\"start\":20220},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21879,\"start\":21870},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22654,\"start\":22646},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23526,\"start\":23517}]", "bib_author_first_name": "[{\"end\":35158,\"start\":35157},{\"end\":35168,\"start\":35167},{\"end\":35178,\"start\":35177},{\"end\":35191,\"start\":35190},{\"end\":35335,\"start\":35334},{\"end\":35347,\"start\":35346},{\"end\":35616,\"start\":35615},{\"end\":35802,\"start\":35801},{\"end\":35812,\"start\":35811},{\"end\":36024,\"start\":36023},{\"end\":36037,\"start\":36036},{\"end\":36046,\"start\":36045},{\"end\":36056,\"start\":36055},{\"end\":36065,\"start\":36064},{\"end\":36265,\"start\":36264},{\"end\":36279,\"start\":36278},{\"end\":36296,\"start\":36295},{\"end\":36305,\"start\":36304},{\"end\":36311,\"start\":36310},{\"end\":36327,\"start\":36326},{\"end\":36336,\"start\":36335},{\"end\":36349,\"start\":36348},{\"end\":36574,\"start\":36573},{\"end\":36583,\"start\":36582},{\"end\":36738,\"start\":36737},{\"end\":36744,\"start\":36743},{\"end\":36753,\"start\":36752},{\"end\":36760,\"start\":36759},{\"end\":36908,\"start\":36907},{\"end\":37096,\"start\":37095},{\"end\":37106,\"start\":37105},{\"end\":37290,\"start\":37289},{\"end\":37303,\"start\":37302},{\"end\":37521,\"start\":37520},{\"end\":37535,\"start\":37534},{\"end\":37548,\"start\":37547},{\"end\":37748,\"start\":37747},{\"end\":37754,\"start\":37753},{\"end\":37761,\"start\":37760},{\"end\":37770,\"start\":37769},{\"end\":37778,\"start\":37777},{\"end\":37786,\"start\":37785},{\"end\":37793,\"start\":37792},{\"end\":38011,\"start\":38010},{\"end\":38017,\"start\":38016},{\"end\":38026,\"start\":38025},{\"end\":38220,\"start\":38219},{\"end\":38226,\"start\":38225},{\"end\":38425,\"start\":38424},{\"end\":38431,\"start\":38430},{\"end\":38438,\"start\":38437},{\"end\":38444,\"start\":38443},{\"end\":38605,\"start\":38604},{\"end\":38615,\"start\":38614},{\"end\":38844,\"start\":38843},{\"end\":38851,\"start\":38850},{\"end\":38858,\"start\":38857},{\"end\":39052,\"start\":39051},{\"end\":39063,\"start\":39062},{\"end\":39076,\"start\":39075},{\"end\":39089,\"start\":39088},{\"end\":39098,\"start\":39097},{\"end\":39107,\"start\":39106},{\"end\":39304,\"start\":39303},{\"end\":39361,\"start\":39360},{\"end\":39372,\"start\":39371},{\"end\":39382,\"start\":39381},{\"end\":39389,\"start\":39388},{\"end\":39402,\"start\":39401},{\"end\":39668,\"start\":39667},{\"end\":39680,\"start\":39679},{\"end\":39872,\"start\":39871},{\"end\":39883,\"start\":39882},{\"end\":39896,\"start\":39895},{\"end\":39905,\"start\":39904},{\"end\":39915,\"start\":39914},{\"end\":40132,\"start\":40131},{\"end\":40141,\"start\":40140},{\"end\":40154,\"start\":40153},{\"end\":40410,\"start\":40409},{\"end\":40419,\"start\":40418},{\"end\":40432,\"start\":40431},{\"end\":40446,\"start\":40445},{\"end\":40458,\"start\":40457},{\"end\":40666,\"start\":40665},{\"end\":40674,\"start\":40673},{\"end\":40685,\"start\":40684},{\"end\":40695,\"start\":40694},{\"end\":40708,\"start\":40707},{\"end\":40914,\"start\":40913},{\"end\":40922,\"start\":40921},{\"end\":40928,\"start\":40927},{\"end\":41124,\"start\":41123},{\"end\":41133,\"start\":41132},{\"end\":41142,\"start\":41141},{\"end\":41148,\"start\":41147}]", "bib_author_last_name": "[{\"end\":35165,\"start\":35159},{\"end\":35175,\"start\":35169},{\"end\":35188,\"start\":35179},{\"end\":35199,\"start\":35192},{\"end\":35344,\"start\":35336},{\"end\":35357,\"start\":35348},{\"end\":35624,\"start\":35617},{\"end\":35809,\"start\":35803},{\"end\":35818,\"start\":35813},{\"end\":36034,\"start\":36025},{\"end\":36043,\"start\":36038},{\"end\":36053,\"start\":36047},{\"end\":36062,\"start\":36057},{\"end\":36072,\"start\":36066},{\"end\":36276,\"start\":36266},{\"end\":36293,\"start\":36280},{\"end\":36302,\"start\":36297},{\"end\":36308,\"start\":36306},{\"end\":36324,\"start\":36312},{\"end\":36333,\"start\":36328},{\"end\":36346,\"start\":36337},{\"end\":36356,\"start\":36350},{\"end\":36580,\"start\":36575},{\"end\":36588,\"start\":36584},{\"end\":36741,\"start\":36739},{\"end\":36750,\"start\":36745},{\"end\":36757,\"start\":36754},{\"end\":36764,\"start\":36761},{\"end\":36913,\"start\":36909},{\"end\":37103,\"start\":37097},{\"end\":37109,\"start\":37107},{\"end\":37300,\"start\":37291},{\"end\":37310,\"start\":37304},{\"end\":37532,\"start\":37522},{\"end\":37545,\"start\":37536},{\"end\":37555,\"start\":37549},{\"end\":37751,\"start\":37749},{\"end\":37758,\"start\":37755},{\"end\":37767,\"start\":37762},{\"end\":37775,\"start\":37771},{\"end\":37783,\"start\":37779},{\"end\":37790,\"start\":37787},{\"end\":37797,\"start\":37794},{\"end\":38014,\"start\":38012},{\"end\":38023,\"start\":38018},{\"end\":38030,\"start\":38027},{\"end\":38223,\"start\":38221},{\"end\":38230,\"start\":38227},{\"end\":38428,\"start\":38426},{\"end\":38435,\"start\":38432},{\"end\":38441,\"start\":38439},{\"end\":38448,\"start\":38445},{\"end\":38612,\"start\":38606},{\"end\":38622,\"start\":38616},{\"end\":38848,\"start\":38845},{\"end\":38855,\"start\":38852},{\"end\":38862,\"start\":38859},{\"end\":39060,\"start\":39053},{\"end\":39073,\"start\":39064},{\"end\":39086,\"start\":39077},{\"end\":39095,\"start\":39090},{\"end\":39104,\"start\":39099},{\"end\":39116,\"start\":39108},{\"end\":39314,\"start\":39305},{\"end\":39369,\"start\":39362},{\"end\":39379,\"start\":39373},{\"end\":39386,\"start\":39383},{\"end\":39399,\"start\":39390},{\"end\":39407,\"start\":39403},{\"end\":39677,\"start\":39669},{\"end\":39690,\"start\":39681},{\"end\":39880,\"start\":39873},{\"end\":39893,\"start\":39884},{\"end\":39902,\"start\":39897},{\"end\":39912,\"start\":39906},{\"end\":39921,\"start\":39916},{\"end\":40138,\"start\":40133},{\"end\":40151,\"start\":40142},{\"end\":40162,\"start\":40155},{\"end\":40416,\"start\":40411},{\"end\":40429,\"start\":40420},{\"end\":40443,\"start\":40433},{\"end\":40455,\"start\":40447},{\"end\":40466,\"start\":40459},{\"end\":40671,\"start\":40667},{\"end\":40682,\"start\":40675},{\"end\":40692,\"start\":40686},{\"end\":40705,\"start\":40696},{\"end\":40715,\"start\":40709},{\"end\":40919,\"start\":40915},{\"end\":40925,\"start\":40923},{\"end\":40932,\"start\":40929},{\"end\":41130,\"start\":41125},{\"end\":41139,\"start\":41134},{\"end\":41145,\"start\":41143},{\"end\":41153,\"start\":41149}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34950,\"start\":34662},{\"attributes\":{\"id\":\"b1\"},\"end\":35096,\"start\":34952},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52157475},\"end\":35332,\"start\":35098},{\"attributes\":{\"id\":\"b3\"},\"end\":35550,\"start\":35334},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2375110},\"end\":35721,\"start\":35552},{\"attributes\":{\"doi\":\"arXiv:1901.02212\",\"id\":\"b5\"},\"end\":35964,\"start\":35723},{\"attributes\":{\"doi\":\"arXiv:1910.08854\",\"id\":\"b6\"},\"end\":36233,\"start\":35966},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1033682},\"end\":36513,\"start\":36235},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":61808533},\"end\":36690,\"start\":36515},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6447277},\"end\":36868,\"start\":36692},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6155330},\"end\":37049,\"start\":36870},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b11\"},\"end\":37216,\"start\":37051},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b12\"},\"end\":37453,\"start\":37218},{\"attributes\":{\"id\":\"b13\"},\"end\":37687,\"start\":37455},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":209516424},\"end\":37934,\"start\":37689},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53317989},\"end\":38155,\"start\":37936},{\"attributes\":{\"doi\":\"arXiv:1811.00656\",\"id\":\"b16\"},\"end\":38354,\"start\":38157},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":212726430},\"end\":38572,\"start\":38356},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5855042},\"end\":38767,\"start\":38574},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6070160},\"end\":38986,\"start\":38769},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":59292011},\"end\":39278,\"start\":38988},{\"attributes\":{\"id\":\"b21\"},\"end\":39358,\"start\":39280},{\"attributes\":{\"id\":\"b22\"},\"end\":39597,\"start\":39360},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14124313},\"end\":39810,\"start\":39599},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206593880},\"end\":40063,\"start\":39812},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":219950625},\"end\":40342,\"start\":40065},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":56894332},\"end\":40621,\"start\":40344},{\"attributes\":{\"doi\":\"arXiv:1412.0767\",\"id\":\"b27\"},\"end\":40860,\"start\":40623},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53295714},\"end\":41037,\"start\":40862},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10585115},\"end\":41355,\"start\":41039}]", "bib_title": "[{\"end\":35155,\"start\":35098},{\"end\":35613,\"start\":35552},{\"end\":36262,\"start\":36235},{\"end\":36571,\"start\":36515},{\"end\":36735,\"start\":36692},{\"end\":36905,\"start\":36870},{\"end\":37745,\"start\":37689},{\"end\":38008,\"start\":37936},{\"end\":38422,\"start\":38356},{\"end\":38602,\"start\":38574},{\"end\":38841,\"start\":38769},{\"end\":39049,\"start\":38988},{\"end\":39665,\"start\":39599},{\"end\":39869,\"start\":39812},{\"end\":40129,\"start\":40065},{\"end\":40407,\"start\":40344},{\"end\":40911,\"start\":40862},{\"end\":41121,\"start\":41039}]", "bib_author": "[{\"end\":35167,\"start\":35157},{\"end\":35177,\"start\":35167},{\"end\":35190,\"start\":35177},{\"end\":35201,\"start\":35190},{\"end\":35346,\"start\":35334},{\"end\":35359,\"start\":35346},{\"end\":35626,\"start\":35615},{\"end\":35811,\"start\":35801},{\"end\":35820,\"start\":35811},{\"end\":36036,\"start\":36023},{\"end\":36045,\"start\":36036},{\"end\":36055,\"start\":36045},{\"end\":36064,\"start\":36055},{\"end\":36074,\"start\":36064},{\"end\":36278,\"start\":36264},{\"end\":36295,\"start\":36278},{\"end\":36304,\"start\":36295},{\"end\":36310,\"start\":36304},{\"end\":36326,\"start\":36310},{\"end\":36335,\"start\":36326},{\"end\":36348,\"start\":36335},{\"end\":36358,\"start\":36348},{\"end\":36582,\"start\":36573},{\"end\":36590,\"start\":36582},{\"end\":36743,\"start\":36737},{\"end\":36752,\"start\":36743},{\"end\":36759,\"start\":36752},{\"end\":36766,\"start\":36759},{\"end\":36915,\"start\":36907},{\"end\":37105,\"start\":37095},{\"end\":37111,\"start\":37105},{\"end\":37302,\"start\":37289},{\"end\":37312,\"start\":37302},{\"end\":37534,\"start\":37520},{\"end\":37547,\"start\":37534},{\"end\":37557,\"start\":37547},{\"end\":37753,\"start\":37747},{\"end\":37760,\"start\":37753},{\"end\":37769,\"start\":37760},{\"end\":37777,\"start\":37769},{\"end\":37785,\"start\":37777},{\"end\":37792,\"start\":37785},{\"end\":37799,\"start\":37792},{\"end\":38016,\"start\":38010},{\"end\":38025,\"start\":38016},{\"end\":38032,\"start\":38025},{\"end\":38225,\"start\":38219},{\"end\":38232,\"start\":38225},{\"end\":38430,\"start\":38424},{\"end\":38437,\"start\":38430},{\"end\":38443,\"start\":38437},{\"end\":38450,\"start\":38443},{\"end\":38614,\"start\":38604},{\"end\":38624,\"start\":38614},{\"end\":38850,\"start\":38843},{\"end\":38857,\"start\":38850},{\"end\":38864,\"start\":38857},{\"end\":39062,\"start\":39051},{\"end\":39075,\"start\":39062},{\"end\":39088,\"start\":39075},{\"end\":39097,\"start\":39088},{\"end\":39106,\"start\":39097},{\"end\":39118,\"start\":39106},{\"end\":39316,\"start\":39303},{\"end\":39371,\"start\":39360},{\"end\":39381,\"start\":39371},{\"end\":39388,\"start\":39381},{\"end\":39401,\"start\":39388},{\"end\":39409,\"start\":39401},{\"end\":39679,\"start\":39667},{\"end\":39692,\"start\":39679},{\"end\":39882,\"start\":39871},{\"end\":39895,\"start\":39882},{\"end\":39904,\"start\":39895},{\"end\":39914,\"start\":39904},{\"end\":39923,\"start\":39914},{\"end\":40140,\"start\":40131},{\"end\":40153,\"start\":40140},{\"end\":40164,\"start\":40153},{\"end\":40418,\"start\":40409},{\"end\":40431,\"start\":40418},{\"end\":40445,\"start\":40431},{\"end\":40457,\"start\":40445},{\"end\":40468,\"start\":40457},{\"end\":40673,\"start\":40665},{\"end\":40684,\"start\":40673},{\"end\":40694,\"start\":40684},{\"end\":40707,\"start\":40694},{\"end\":40717,\"start\":40707},{\"end\":40921,\"start\":40913},{\"end\":40927,\"start\":40921},{\"end\":40934,\"start\":40927},{\"end\":41132,\"start\":41123},{\"end\":41141,\"start\":41132},{\"end\":41147,\"start\":41141},{\"end\":41155,\"start\":41147}]", "bib_venue": "[{\"end\":34689,\"start\":34662},{\"end\":34985,\"start\":34952},{\"end\":35205,\"start\":35201},{\"end\":35435,\"start\":35359},{\"end\":35630,\"start\":35626},{\"end\":35799,\"start\":35723},{\"end\":36021,\"start\":35966},{\"end\":36362,\"start\":36358},{\"end\":36594,\"start\":36590},{\"end\":36770,\"start\":36766},{\"end\":36951,\"start\":36915},{\"end\":37093,\"start\":37051},{\"end\":37287,\"start\":37218},{\"end\":37518,\"start\":37455},{\"end\":37803,\"start\":37799},{\"end\":38036,\"start\":38032},{\"end\":38217,\"start\":38157},{\"end\":38454,\"start\":38450},{\"end\":38660,\"start\":38624},{\"end\":38868,\"start\":38864},{\"end\":39122,\"start\":39118},{\"end\":39301,\"start\":39280},{\"end\":39463,\"start\":39409},{\"end\":39696,\"start\":39692},{\"end\":39927,\"start\":39923},{\"end\":40192,\"start\":40164},{\"end\":40472,\"start\":40468},{\"end\":40663,\"start\":40623},{\"end\":40940,\"start\":40934},{\"end\":41185,\"start\":41155}]"}}}, "year": 2023, "month": 12, "day": 17}