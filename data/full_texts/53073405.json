{"id": 53073405, "updated": "2023-09-30 14:32:03.018", "metadata": {"title": "DPSNet: End-to-end Deep Plane Sweep Stereo", "authors": "[{\"first\":\"Sunghoon\",\"last\":\"Im\",\"middle\":[]},{\"first\":\"Hae-Gon\",\"last\":\"Jeon\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"In\",\"last\":\"Kweon\",\"middle\":[\"So\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 5, "day": 2}, "abstract": "Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1905.00538", "mag": "2950557037", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ImJLK19", "doi": null}}, "content": {"source": {"pdf_hash": "01dd15f339e8da4eb9a23e47983c1b7c480c7196", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1905.00538v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b220f11f0aed0c85a1f741d1d2f217e80492bb19", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/01dd15f339e8da4eb9a23e47983c1b7c480c7196.txt", "contents": "\nDPSNET: END-TO-END DEEP PLANE SWEEP STEREO\n\n\nSunghoon Im \nKAIST\n\n\nHae-Gon Jeon haegonj@andrew.cmu.edu \nCarnegie Mellon University\n\n\nStephen Lin \nMicrosoft Research Asia\n\n\nIn So Kweon iskweon77@kaist.ac.kr \nKAIST\n\n\nDPSNET: END-TO-END DEEP PLANE SWEEP STEREO\nPublished as a conference paper at ICLR 2019\nMultiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a contextaware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DP-SNet achieves state-of-the-art reconstruction results on a variety of challenging datasets. * part of the work was done during an internship at Microsoft Research Asia\n\nINTRODUCTION\n\nVarious image understanding tasks, such as semantic segmentation Couprie et al. (2013) and human pose/action recognition Shotton et al. (2011); Wang et al. (2016), have been shown to benefit from 3D scene information. A common approach to reconstructing 3D geometry is by multiview stereo, which infers depth based on point correspondences among a set of unstructured images Hartley & Zisserman (2003); . To solve for these correspondences, conventional techniques employ photometric consistency constraints on local image patches. Such photo-consistency constraints, though effective in many instances, can be unreliable in scenes containing textureless and reflective regions.\n\nRecently, convolutional neural networks (CNNs) have demonstrated some capacity to address this issue by leveraging semantic information inferred from the scene. The most promising of these methods employ a traditional stereo matching pipeline, which involves computation of matching cost volumes, cost aggregation, and disparity estimation Flynn et al. (2016); Kendall et al. (2017); Huang et al. (2018); Chang & Chen (2018). Some are designed for binocular stereo Ummenhofer et al. (2017); Kendall et al. (2017); Chang & Chen (2018) and cannot readily be extended to multiple views. The CNN-based techniques for multiview processing Flynn et al. (2016); Huang et al. (2018) both follow the plane-sweep approach, but require plane-sweep volumes as input to their networks. As a result, they are not end-to-end systems that can be trained from input images to disparity maps.\n\nIn this paper, we present Deep Plane Sweep Network (DPSNet), an end-to-end CNN framework for robust multiview stereo. In contrast to previous methods that employ the plane-sweep approach Huang et al. (2018); Flynn et al. (2016), DPSNet fully models the plane-sweep process, including construction of plane-sweep cost volumes, within the network. This is made possible through the use of a differentiable warping module inspired by spatial transformer networks Jaderberg et al. (2015) to build the cost volumes. With the proposed network, plane-sweep stereo can be learned in an end-to-end fashion. Additionally, we introduce a cost aggregation module based on Published as a conference paper at ICLR 2019 DPSNet (a) Input unstructured image sets, output depth map. local cost-volume filtering Rhemann et al. (2011) for context-aware refinement of each cost slice. Through this cost-volume regularization, the effects of unreliable matches scattered within the cost volume are reduced considerably.\n\nWith this end-to-end network for plane-sweep stereo and the proposed cost aggregation, we obtain state-of-the-art results over several standard datasets. Ablation studies indicate that each of these technical contributions leads to appreciable improvements in reconstruction accuracy.\n\n\nRELATED WORK\n\nCNN-based depth estimation has been studied for stereo matching, depth from single images, and multiview stereo. Recent work in these areas are briefly reviewed in the following.\n\nStereo matching Methods for stereo matching address the particular case of depth estimation where the input is a pair of rectified images captured by a stereo rig. Various network structures have been introduced for this problem. Zbontar & LeCun (2016) present a Siamese network structure to compute matching costs based on the similarity of two image patches. The estimated initial depth is then refined by traditional cost aggregation and refinement as post-processing. Mayer et al. (2016) directly stack several convolution and deconvolution layers upon the matching costs and train the network to minimize the distance between the estimates and ground truth. Liang et al. (2018) propose a CNN that estimates initial disparity and then refines it using both prior and posterior feature consistency in an end-to-end manner. Kendall et al. (2017) leverage geometric knowledge in building a cost volume from deep feature representations. It also enables learning of contextual information in a 3D volume and regresses disparity in an end-to-end manner. Chang & Chen (2018) introduce a pyramid pooling module for incorporating global contextual information into image features and a stacked hourglass 3D CNN to extend the regional support of contextual information.\n\nDepth from single images Similar to these stereo matching approaches, single-image methods extract CNN features to infer scene depths and perform refinements to increase depth accuracy.  Yin & Shi (2018) present an end-to-end learning pipeline that utilizes the task of view synthesis as supervision for single-view depth and camera pose estimation. These systems consist of a depth network and a pose estimation network which simultaneously train on sequential images with a loss computed from images warped to nearby views using the estimated depth. View synthesis has similarly been used as supervision by warping between stereo image pairs Garg et al. (2016); Godard et al. (2017). In contrast to these single-image works which employ warping as a component of view synthesis for self-supervised learning, our network computes warps with respect to multiple depth planes to produce plane-sweep cost volumes both for training and at test time.\n\nThe cost volumes undergo further processing in the form of cost aggregation and regularization to improve the robustness of depth estimates.\n\nMulti-view stereo In multi-view stereo, depth is inferred from multiple input images acquired from arbitrary viewpoints. To solve this problem, some methods recover camera motion between the unstructured images but are designed to handle only two views Ummenhofer et al. (2017); . The DeMoN system Ummenhofer et al. (2017) consists of encoder-decoder networks for optical flow, depth/motion estimation, and depth refinement. By alternating between estimating optical flow and depth/motion, the network is forced to use both images in estimating depth, rather than resorting to single-image inference.  perform monocular visual odometry in an unsupervised manner. In the training step, the use of stereo images with extrinsic parameters allows 3D depth estimation to be estimated with metric scale.\n\nAmong networks that can handle an arbitrary number of views, camera parameters are assumed to be known or estimated by conventional geometric methods. Ji et al. (2017) introduce an endto-end learning framework based on a viewpoint-dependent voxel representation which implicitly encodes images and camera parameters. The voxel representation restricts the scene resolution that can be processed in practice due to limitations in GPU memory. Im et al. (2018b) formulate a geometric relationship between optical flow and depth to refine the estimated scene geometry, but is designed for image sequences with a very small baseline, i.e., an image burst from a handheld camera. Huang et al. (2018) compute a set of plane-sweep volumes using calibrated pose data as input for the network, which then predicts an initial depth feature using an encoder-decoder network.\n\nIn the depth prediction step, they concatenate a reference image feature to the decoder input as an intra-feature aggregation, and cost volumes from each of the input images are aggregated by max-pooling to gather information for the multiview matching. Its estimated depth map is refined using a conventional CRF. By contrast, our proposed DPSNet is developed to be trained end-to-end from input images to the depth map. Moreover, it leverages conventional multiview stereo concepts by incorporating context-aware cost aggregation. Finally, we would like to refer the reader to the concurrent work by Yao et al. (2018) that also adopts differential warping to construct a multi-scale cost volume, then refined an initial depth map guided by a reference image feature. Our work is independent of this concurrent effort. Moreover, we make distinct contributions: (1) We focus on dense depth estimation for a reference image in an end-to-end learning manner, different from Yao et al. (2018) which reconstructs the full 3D of objects.\n\n(2) Our cost volume is constructed by concatenating input feature maps, which enables inference of accurate depth maps even with only two-view matching.\n\n(3) Our work refines every cost slice by applying context features of a reference image, which is beneficial for alleviating coarsely scattered unreliable matches such as for large textureless regions.\n\n\nAPPROACH\n\nOur Deep Plane Sweep Network (DPSNet) is inspired by traditional multiview stereo practices for dense depth estimation and consists of four parts: feature extraction, cost volume generation, cost aggregation and depth map regression. The overall framework is shown in Figure 2.\n\n\nMULTI-SCALE FEATURE EXTRACTION\n\nWe first pass a reference image and target images through seven convolutional layers (3 \u00d7 3 filters except for the first layer, which has a 7 \u00d7 7 filter) to encode them, and extract hierarchical contextual information from these images using a spatial pyramid pooling (SPP) module He et al. (2014) with four fixed-size average pooling blocks (16 \u00d7 16, 8 \u00d7 8, 4 \u00d7 4, 2 \u00d7 2). The multi-scale features extracted by SPP have been shown to be effective in many visual perception tasks such as visual recognition He et al. (2014), scene parsing Zhao et al. (2017) and stereo matching Huang et al. (2018). After upsampling the hierarchical contextual information to the same size as the original feature map, we concatenate all the feature maps and pass them through 2D convolutional layers. This process yields 32-channel feature representations for all the input images, which are next used in building cost volumes.\n\n\nCOST VOLUME GENERATION USING UNSTRUCTURED TWO-VIEW IMAGES\n\nWe propose to generate cost volumes for the multiview images by adopting traditional plane sweep stereo Collins (1996) images for each pixel. In a similar manner to traditional plane sweep stereo, we construct a cost volume from an input image pair. To reduce the effects of image noise, multiple images can be utilized by averaging cost volumes for other pairs.\n\nFor this cost volume generation network, we first set the number of virtual planes perpendicular to the z-axis of the reference viewpoint [0, 0, 1] and uniformly sample them in the inverse-depth space as follows:\nd l = (L \u00d7 d min ) l , (l = 1, .., L),(1)\nwhere L is the total number of depth labels and d min is the minimum scene depth as specified by the user.\n\nThen, we warp all the paired features F i , (i = 1, .., N ), where i is an index of viewpoints and N is the total number of input views, into the coordinates of the reference feature (of size W idth \u00d7 Height \u00d7 CHannel) using pre-computed intrinsics K and extrinsic parameters consisting of a rotation matrix R i and a translation matrix t i of the i th camera:\nF il (u) = F i (\u0169 l ),\u0169 l \u223c K[R i |t i ] (K \u22121 u)d l 1 ,(2)\nwhere u,\u0169 l are the homogeneous coordinates of a pixel in the reference view and the projected coordinates onto the paired view, respectively.F il (u) denotes the warped features of the paired image through the l th virtual plane. Unlike the traditional plane sweeping method which utilizes a distance metric, we use a concatenation of features in learning a representation and carry this through to the cost volume as proposed in Kendall et al. (2017). We obtain a 4D volume (W \u00d7H \u00d72CH \u00d7L) by concatenating the reference image features and the warped image features for all of the depth labels. In Eq.\n\n(2), we assume that all images are captured by the same camera, but it can be directly extended to images with different intrinsics. For the warping process, we use a spatial transformer network Jaderberg et al. (2015) for all hypothesis planes, which does not require any learnable parameters. In Table 3, we find that concatenating features improves performance over the absolute difference of the features.\n\nGiven the 4D volume 1 , our DPSNet learns a cost volume generation of size W \u00d7 H \u00d7 L by using a series of 3D convolutions on the concatenated features. All of the convolutional layers consist of 3 \u00d7 3 \u00d7 3 filters and residual blocks. In the training step, we only use one paired image (while the other is the reference image) to obtain the cost volume. In the testing step, we can use any number of paired images (N \u2265 1) by averaging all of the cost volumes.\n\n\nCOST AGGREGATION\n\nThe key idea of cost aggregation Rhemann et al. (2011) is to regularize the noisy cost volume through edge-preserving filtering He et al. (2013) within a support window. Inspired by traditional cost  Figure 3: Illustration of context-aware cost aggregation. volume filtering, we introduce a context-aware cost aggregation method in our end-to-end learning process. The context network takes each slice of the cost volume and the reference image features extracted from the previous step, and then outputs the refined cost slice. We run the same process for all the cost slices. The final cost volume is then obtained by adding the initial and residual volumes as shown in Figure 3.\n\nHere, we use dilated convolutions in the context network for cost aggregation to better exploit contextual information ; Yu & Koltun (2016). The context network consists of seven convolutional layers with 3 \u00d7 3 filters, where each layer has a different receptive field (1, 2, 4, 8, 16, 1, and 1). We jointly learn all the parameters, including those of the context network. All cost slices are processed with shared weights of the context network. Then, we upsample the cost volume, whose size is equal to the feature size, to the original size of the images via bilinear interpolation. We find that this leads to moderate performance improvement as shown in Table 3.\n\n\nDEPTH REGRESSION\n\nWe regress continuous depth values using the method proposed in Kendall et al. (2017). The probability of each label l is calculated from the predicted cost c l via the softmax operation \u03c3(\u00b7). The predicted labell is computed as the sum of each label l weighted by its probability. With the predicted label, the depth is calculated from the number of labels L and minimum scene depth d min as follows:d\n= L \u00d7 d mi\u00f1 l ,l = L l=1 l \u00d7 \u03c3(c l ).(3)\nWe set L and d min to 64 and 0.5, respectively.\n\n\nTRAINING LOSS\n\nLet \u03b8 be the set of all the learnable parameters in our network, which includes feature extraction, cost volume generation and cost aggregation (plane sweep and depth regression have no learnable parameters). Letd,d denote the predicted depth from the initial and refined cost volumes, respectively, and let d gt be the corresponding supervision signal. The training loss is then formulated as   \nL(\u03b8) = x \u03bb|d \u03b8 x \u2212 d gt x | H + |d \u03b8 x \u2212 d gt x | H ,(4)\n\nEXPERIMENTS\n\n\nIMPLEMENTATION DETAILS\n\nIn the training procedure, we use image sequences, ground-truth depth maps for reference images, and the provided camera poses from public datasets, namely SUN3D, RGBD, and Scenes11 2 . We train our model from scratch for 1200K iterations in total. All models were trained end-to-end with the ADAM optimizer (\u03b2 1 = 0.9, \u03b2 2 = 0.999). We use a batch size of 16 and set the learning rate to 2e\u22124 for all iterations. The training is performed with a customized version of PyTorch on four NVIDIA 1080Ti GPUs, which usually takes four days. A forward pass of the proposed network takes about 0.5 seconds for 2-view matching and an additional 0.25 seconds for every new frame matched (640 \u00d7 480 image resolution).\n\n\nCOMPARISON WITH STATE-OF-THE-ART METHODS\n\nIn our evaluations, we use common quantitative measures of depth quality: absolute relative error (Abs Rel), absolute relative inverse error (Abs R-Inv), absolute difference error (Abs diff), square relative error (Sq Rel), root mean square error and its log scale (RMSE and RMSE log) and inlier ratios (\u03b4 < 1.25 i where i \u2208 {1, 2, 3}). All are standard metrics used in a public benchmark suite 3 .  The results are reported in Table 1. Our DPSNet provides the best performance on nearly all of the measures. Of particular note, DPSNet accurately recovers scene depth in homogeneous regions as well as along object boundaries as exhibited in Figure 4. DeMoN generally produces good depth estimates but often fails to reconstruct scene details such as the keyboard (third row) and fine structures (first, second and fourth rows). By contrast, DPSNet estimates accurate depth maps at those regions because the differential feature warping penalizes inaccurate reconstructions, playing a role similar to the left-right consistency check that has been used in stereo matching Garg et al. (2016). The first and third rows of Figure 4 exhibit problems of COLMAP and DeepMVS in handling textureless regions. DPSNet instead produces accurate results, courtesy of the cost aggregation network.\n\nFor a more balanced comparison, we adopt measures used in Huang et al. (2018) as additional evaluation criteria: (1) completeness, which is the percentage of pixels whose errors are below a certain threshold.\n\n(2) geometry error, taking the L1 distance between the estimated disparity and the ground truth.\n\n(3) photometry error, which is the L1 distance between the reference image and warped image using the estimated disparity map. The results for COLMAP, DeMoN and DeepMVS are directly reported from Huang et al. (2018) in Table 2. In this experiment, we use the ETH3D dataset on which all methods are not trained. Following Yao et al. (2018), we take 5 images with 1152 \u00d7 864 resolution and set 192 depth labels based on ground-truth depth to obtain optimal results for MVSNet. For the DPSNet results, we use 4 views with 810 \u00d7 540 resolution and set 64 labels whose range is determined by the minimum depth values of the ground truth.    In Table 2, our DPSNet shows the best performance overall among the all the comparison methods, except for filtered COLMAP. Although filtered COLMAP achieves the best performance, its completeness is only 71% and its unfiltered version shows a significant performance drop in all error metrics. On the other hand, our DPSNet with 100% completeness shows promising results on all measures. We note that our DPSNet has a different purpose compared to COLMAP and MVSNet. COLMAP and MVSNet are designed for full 3D reconstruction with an effective outlier rejection process, while DPSNet aims to estimate a dense depth map for a reference view.\n\n\nABLATION STUDY\n\nAn extensive ablation study was conducted to examine the effects of different components on DP-SNet performance. We summarize the results in Table 3.\n\nCost Volume Generation In Table 3 (a) and (e), we compare the use of cost volumes generated using the traditional absolute difference Collins (1996) and using the concatenation of features from the reference image and warped image. The absolute difference is widely used for depth label selection via a winner-take-all strategy. However, we observe that feature concatenation provides better performance in our network than the absolute difference. A possible reason is that the CNN may learn to extract 3D scene information from the tensor of stacked features. The tensor is fed into the CNN to produce an effective feature for depth estimation, which is then passed through our cost aggregation network for the initial depth refinement.\n\nCost Aggregation For our cost aggregation sub-network, we compare DPSNet with and without it in Table 3 (e) and (b), respectively. It is shown that including the proposed cost aggregation leads to significant performance improvements. Examples of depth map refinement with the cost aggregation are displayed in Figure 6.\n\nOur cost aggregation is also compared to using a stacked hourglass to aggregate feature information along the depth dimension as well as the spatial dimensions as done recently for stereo matching Chang & Chen (2018). Although the stacked hourglass is shown in Table 3 (c) to enhance depth results, its improvement is smaller than ours, which uses reference image features to guide the aggregation. Figure 6 (  For further analysis of cost aggregation, we display slices of 3D cost volumes after the softmax operation (in Eq. (3)) that span depth labels and the rows of the images. The cost slices in Figure 6 (c), (d) show that our feature-guided cost aggregation regularizes noisy cost slices while preserving edges well. The cleaner cost profiles that ensue from the cost aggregation lead to clearer and edge-preserving depth regression results. As mentioned in a recent study Hu & Mordohai (2012), a cost profile that gives confident estimates should have a single, distinct minimum (or maximum), while an ambiguous profile has multiple local minima or multiple adjacent labels with similar costs, making it hard to exactly localize the global minimum. Based on two quantitative confidence measures Hu & Mordohai (2012) on cost volumes in Table 4, the proposed aggregation improves the reliability of the correct match corresponding to the minimum cost.\n\nDepth Label Sampling In the plane sweep procedure, depth labels can be sampled in either the depth domain or the inverse depth domain, which provides denser sampling in areas closer to a camera. Table 3 (d) and (e) show that uniform depth label sampling in the inverse depth domain produces more accurate depth maps in general.\n\n\nNumber of Images\n\nWe examine the performance of DPSNet with respect to the number of input images. As displayed in Figure 7a, a greater number of images yields better results, since cost volume noise is reduced through averaging over more images, and more viewpoints help to provide features from areas unseen in other views. Figure 7 shows that adding input views aids in distinguishing object boundaries. Note that the performance improvement plateaus when seven or more images are used.\n\nRectified Stereo Pair CNNs-based stereo matching methods have similarity to DPSNet, but differ from it in that correspondences are obtained by shifting learned features in Mayer et al. (2016); Kendall et al. (2017);Tulyakov et al. (2018). The purpose of this study is to show readers that not only descriptor shift but also plane sweeping can be applied to rectified stereo matching. We apply DPSNet on the KITTI dataset, which provides rectified stereo pairs with a specific baseline.\n\nAs shown in Figure 8, although DPSNet is not designed to work on rectified stereo images, it produces reasonable results. In particular, DPSNet fine-tuned on the KITTI dataset in Table 5 achieves performance similar to Mayer et al. (2016) in terms of D1-all score, with 4.34% for all pixels and 4.05% for non-occluded pixels in the KITTI benchmark. We expect that the depth accuracy would   improve if we were to adopt rectified stereo pair-specific strategies, such as the feature consistency check in Liang et al. (2018).\n\n\nDISCUSSION\n\nWe developed a multiview stereo network whose design is inspired by best practices of traditional non-learning-based techniques. The plane sweep algorithm is formulated as an end-to-end network via a differentiable construction of plane sweep cost volumes and by solving for depth as a multilabel classification problem. Moreover, we propose a context-aware cost aggregation method that leads to improved depth regression without any post-processing. With this incorporation of traditional multiview stereo schemes into a deep learning framework, state-of-the-art reconstruction results are achieved on a variety of datasets.\n\nDirections exist for improving DPSNet. One is to integrate semantic instance segmentation into the cost aggregation, similar to the segment-based cost aggregation method of Mei et al. (2013). Another direction is to improve depth prediction by employing viewpoint selection in constructing cost volumes Gallup et al. (2008);, rather than by simply averaging the estimated cost volumes as currently done in DPSNet. Lastly, the proposed network requires pre-calibrated intrinsic and extrinsic parameters for reconstruction. Lifting this restriction by additionally estimating camera poses in an end-to-end learning framework is an important future challenge.\n\nFigure 1 :\n1Results of DPSNet with comparisons to state-of-the-art methods.\n\nFigure 2 :\n2;Yang & Pollefeys (2003);Ha et al. (2016);Im et al. (2018a), originally devised for dense depth estimation. The basic idea of plane sweep stereo is to back-project the image set onto successive virtual planes in the 3D space and measure photo-consistency among the warped Overview of the DPSNet pipeline.\n\nFor\nour comparisons, we choose state-of-the-art methods for traditional geometry-based multiview stereo (COLMAP) Sch\u00f6nberger & Frahm (2016), depth from unstructured two-view stereo (De-MoN) Ummenhofer et al. (2017) and CNN-based multiview stereo (DeepMVS)Huang et al. (2018). We estimate the depth maps from two unstructured views using the test sets in MVS, SUN3D, RGBD and Scenes11, as done for DeMoN 4 .\n\nFigure 4 :Figure 5 :\n45Comparison of depth map results on MVS, SUN3D, RGBD and Scenes11 (top to bottom).(a) Images (b) GT depth (c) DeMoN (d) COLMAP (e) DeepMVS (f) MVSNet (g) Ours Depth map results on four-view image sequences from the ETH3D dataset.\n\nFigure 6 :\n6(a) Reference image & GT depth. (b) Regressed depth. (c), (d) Slice of volume along a label (far & Close), and (e) along the green row in (a) (Column-Cost layer axis). The color bar ranges from 0 to 1. Before (top) and after (bottom) cost volume aggregation.\n\n3 :\n3Ablation Experiment. (a) With cost volume generated by absolute differences of features. (b) Without cost aggregation. (c) With cost aggregation by stacked hourglass. (d) With planes swept on uniform samples of the depth domain from 0.5m to 10m (whereas ours are uniformly sampled on the inverse depth domain from 0.5m). (e) With our complete DPSNet. Datasets: MVS, SUN3D, RGBD, Scenes11. Note that we masked out the depth beyond the range [0.5, 10] for the evaluation.\n\nFigure 7 :\n7Depth map results w.r.t. the number of images.\n\nFigure 8 :\n8Rectified stereo evaluation. (a) Reference images. (b) Depth map results without finetuning. (c) Depth map results with fine-tuning. (KITTI2015 test dataset).\n\n\nwhere | \u00b7 | H denotes the Huber norm, referred to as SmoothL1Loss in PyTorch. The weight value \u03bb for depth from the initial cost volume is set to 0.7.Data \nMethod \nError metric \nAccuracy metric (\u03b4 < \u03b1 t ) \n-sets \nAbs Rel \nAbs diff \nSq Rel \nRMSE \nRMSE log \n\u03b1 \n\u03b1 2 \n\u03b1 3 \n\nMVS \n\nCOLMAP \n0.3841 \n0.8430 \n1.257 \n1.4795 \n0.5001 \n0.4819 \n0.6633 \n0.8401 \nDeMoN \n0.3105 \n1.3291 \n19.970 \n2.6065 \n0.2469 \n0.6411 \n0.9017 \n0.9667 \nDeepMVS \n0.2305 \n0.6628 \n0.6151 \n1.1488 \n0.3019 \n0.6737 \n0.8867 \n0.9414 \nOurs \n0.0722 \n0.2095 \n0.0798 \n0.4928 \n0.1527 \n0.8930 \n0.9502 \n0.9760 \n\nSUN3D \nCOLMAP \n0.6232 \n1.3267 \n3.2359 \n2.3162 \n0.6612 \n0.3266 \n0.5541 \n0.7180 \nDeMoN \n0.2137 \n2.1477 \n1.1202 \n2.4212 \n0.2060 \n0.7332 \n0.9219 \n0.9626 \nDeepMVS \n0.2816 \n0.6040 \n0.4350 \n0.9436 \n0.3633 \n0.5622 \n0.7388 \n0.8951 \nOurs \n0.1470 \n0.3234 \n0.1071 \n0.4269 \n0.1906 \n0.7892 \n0.9317 \n0.9672 \n\nRGBD \nCOLMAP \n0.5389 \n0.9398 \n1.7608 \n1.5051 \n0.7151 \n0.2749 \n0.5001 \n0.7241 \nDeMoN \n0.1569 \n1.3525 \n0.5238 \n1.7798 \n0.2018 \n0.8011 \n0.9056 \n0.9621 \nDeepMVS \n0.2938 \n0.6207 \n0.4297 \n0.8684 \n0.3506 \n0.5493 \n0.8052 \n0.9217 \nOurs \n0.1538 \n0.5235 \n0.2149 \n0.7226 \n0.2263 \n0.7842 \n0.8959 \n0.9402 \nScenes11 \nCOLMAP \n0.6249 \n2.2409 \n3.7148 \n3.6575 \n0.8680 \n0.3897 \n0.5674 \n0.6716 \nDeMoN \n0.5560 \n1.9877 \n3.4020 \n2.6034 \n0.3909 \n0.4963 \n0.7258 \n0.8263 \nDeepMVS \n0.2100 \n0.5967 \n0.3727 \n0.8909 \n0.2699 \n0.6881 \n0.8940 \n0.9687 \nOurs \n0.0558 \n0.2430 \n0.1435 \n0.7136 \n0.1396 \n0.9502 \n0.9726 \n0.9804 \n\n\n\nTable 1 :\n1Comparison results. Multi-view stereo methods: COLMAP, DeMoN, DeepMVS, and Ours. Datasets: MVS, SUN3D, RGBD, and Scenes11.Method \nCompl \nError metric \nAccuracy metric (\u03b4 < \u03b1 t )) \n-eteness \nGeo. \nPhoto. \nA. Rel \nA. diff \nSq Rel \nRMSE \nRlog \n\u03b1 \n\u03b1 2 \n\u03b1 3 \nCOLMAP filter \n71 % \n0.007 \n0.178 \n0.045 \n0.033 \n0.293 \n0.619 \n0.123 \n0.965 \n0.978 \n0.986 \nCOLMAP \n100 % \n0.046 \n0.218 \n0.324 \n0.615 \n36.71 \n2.370 \n0.349 \n0.865 \n0.903 \n0.927 \nDeMoN \n100 % \n0.045 \n0.288 \n0.191 \n0.726 \n0.365 \n1.059 \n0.240 \n0.733 \n0.898 \n0.951 \nDeepMVS \n100 % \n0.036 \n0.224 \n0.178 \n0.432 \n0.973 \n1.021 \n0.245 \n0.858 \n0.911 \n0.942 \nMVSNET filter \n77 % \n0.067 \n0.179 \n0.357 \n0.766 \n1.969 \n1.325 \n0.423 \n0.706 \n0.779 \n0.829 \nMVSNET \n100 % \n0.077 \n0.218 \n1.666 \n2.165 \n13.93 \n3.255 \n0.824 \n0.555 \n0.628 \n0.686 \nOurs \n100 % \n0.034 \n0.202 \n0.099 \n0.365 \n0.204 \n0.703 \n0.184 \n0.863 \n0.938 \n0.963 \n\n\n\nTable 2 :\n2Comparison results. Multi-view stereo methods on ETH3D. The 'filter' refers to predicted disparity maps from outlier rejection. (Geo, Photo: Geometric and Photometric error; A. Rel: Abs rel; A. diff: Abs diff; Rlog: RMSE log.) Underbar: Best, Bold: Second best.\n\nTable\n\n\n\na),(b)  show examples of the depth map results before and after our cost aggregation. It demonstrates that the cost aggregation sub-network outputs more accurate depth, especially on homogeneous regions.Winner Margin \nCurvature \nBefore Aggregation \n0.7001 \n0.4614 \nAfter Aggregation \n0.7136 \n0.4836 \n\n\n\nTable 4 :\n4Confidence measures on cost volumes. Datasets: MVS, SUN3D, RGBD, Scenes11.\n\n\nNoc / All 4.23 % 16.30 % 6.23 % Noc / All 3.58 % 6.08 % 4.00 %W/O ft \nD1-bg \nD1-fg \nD1-all \nWith ft \nD1-bg \nD1-fg \nD1-all \nAll / All \n4.69 % 17.77 % 6.87 % \nAll / All \n4.21 % 7.58 % 4.77 % \n\n\nTable 5 :\n5KITTI2015 Benchmark without/with finetuning. D1 error denotes the percentage of stereo disparity outliers in the first frame.\nWe implement a 5D volume that includes a batch dimension.\nhttps://github.com/lmb-freiburg/demon 3 http://www.cvlibs.net/datasets/kitti/ 4 We use the provided camera intrinsics and extrinsics.\n\nPyramid stereo matching network. Jia-Ren Chang, Yong-Sheng Chen, CVPR. Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In CVPR, 2018.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE Trans. on Patt. Anal. and Mach. Intel. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. on Patt. Anal. and Mach. Intel., 2018.\n\nA space-sweep approach to true multi-image matching. T Robert, Collins, CVPR. Robert T Collins. A space-sweep approach to true multi-image matching. In CVPR, 1996.\n\nIndoor semantic segmentation using depth information. Camille Couprie, Cl\u00e9ment Farabet, Laurent Najman, Yann Lecun, ICLR. Camille Couprie, Cl\u00e9ment Farabet, Laurent Najman, and Yann Lecun. Indoor semantic segmenta- tion using depth information. In ICLR, 2013.\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, NIPS. David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NIPS, 2014.\n\nDeepstereo: Learning to predict new views from the world's imagery. John Flynn, Ivan Neulander, James Philbin, Noah Snavely, CVPR. John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict new views from the world's imagery. In CVPR, 2016.\n\nVariable baseline/resolution stereo. David Gallup, Jan-Michael Frahm, Philippos Mordohai, Marc Pollefeys, CVPR. David Gallup, Jan-Michael Frahm, Philippos Mordohai, and Marc Pollefeys. Variable base- line/resolution stereo. In CVPR, 2008.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. Ravi Garg, Vijay Kumar, B G , Gustavo Carneiro, Ian Reid, ECCV. Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In ECCV, 2016.\n\nUnsupervised monocular depth estimation with left-right consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, CVPR. Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estima- tion with left-right consistency. In CVPR, 2017.\n\nHae-Gon Jeon, and In So Kweon. High-quality depth from uncalibrated small motion clip. Hyowon Ha, Sunghoon Im, Jaesik Park, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHyowon Ha, Sunghoon Im, Jaesik Park, Hae-Gon Jeon, and In So Kweon. High-quality depth from uncalibrated small motion clip. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5413-5421, 2016.\n\nMultiple view geometry in computer vision. Richard Hartley, Andrew Zisserman, Cambridge university pressRichard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.\n\nGuided image filtering. Kaiming He, Jian Sun, Xiaoou Tang, IEEE Trans. on Patt. Anal. and Mach. Intel. Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. IEEE Trans. on Patt. Anal. and Mach. Intel., 2013.\n\nSpatial pyramid pooling in deep convolutional networks for visual recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, ECCV. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convo- lutional networks for visual recognition. In ECCV, 2014.\n\nA quantitative evaluation of confidence measures for stereo vision. Xiaoyan Hu, Philippos Mordohai, IEEE Trans. on Patt. Anal. and Mach. Intel. Xiaoyan Hu and Philippos Mordohai. A quantitative evaluation of confidence measures for stereo vision. IEEE Trans. on Patt. Anal. and Mach. Intel., 2012.\n\nDeepmvs: Learning multi-view stereopsis. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, Jia-Bin Huang, CVPR. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR, 2018.\n\nHae-Gon Jeon, Kyungdon Joo, and In So Kweon. Accurate 3d reconstruction from small motion clip for rolling shutter cameras. Sunghoon Im, Hyowon Ha, Gyeongmin Choe, IEEE Transactions on Pattern Analysis and Machine Intelligence. Sunghoon Im, Hyowon Ha, Gyeongmin Choe, Hae-Gon Jeon, Kyungdon Joo, and In So Kweon. Accurate 3d reconstruction from small motion clip for rolling shutter cameras. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018a.\n\nHae-Gon Jeon, and In So Kweon. Robust depth estimation from auto bracketed images. Sunghoon Im, CVPR. Sunghoon Im, Hae-Gon Jeon, and In So Kweon. Robust depth estimation from auto bracketed images. In CVPR, 2018b.\n\nSpatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, NIPS. Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NIPS, 2015.\n\nSurfacenet: an end-to-end 3d neural network for multiview stereopsis. Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, Lu Fang, CVPR. Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: an end-to-end 3d neural network for multiview stereopsis. In CVPR, 2017.\n\nEnd-to-end learning of geometry and context for deep stereo regression. Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, Adam Bry, Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In ICCV, 2017.\n\nUndeepvo: Monocular visual odometry through unsupervised deep learning. Ruihao Li, Sen Wang, Zhiqiang Long, Dongbing Gu, In ICRA. Ruihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu. Undeepvo: Monocular visual odometry through unsupervised deep learning. In ICRA, 2018.\n\nLearning deep correspondence through prior and posterior feature constancy. Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Linbo Qiao, Wei Chen, Li Zhou, Jianfeng Zhang, CVPR. Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Linbo Qiao, Wei Chen, Li Zhou, and Jianfeng Zhang. Learning deep correspondence through prior and posterior feature constancy. In CVPR, 2018.\n\nLearning depth from single monocular images using deep convolutional neural fields. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, IEEE Trans. on Patt. Anal. and Mach. Intel. Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. IEEE Trans. on Patt. Anal. and Mach. Intel., 2016.\n\nUnsupervised learning of depth and egomotion from monocular video using 3d geometric constraints. Reza Mahjourian, Martin Wicke, Anelia Angelova, CVPR. Reza Mahjourian, Martin Wicke, and Anelia Angelova. Unsupervised learning of depth and ego- motion from monocular video using 3d geometric constraints. In CVPR, 2018.\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox, CVPR. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR, 2016.\n\nSegment-tree based cost aggregation for stereo matching. Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang, CVPR. Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, and Xiaopeng Zhang. Segment-tree based cost aggregation for stereo matching. In CVPR, 2013.\n\nFast cost-volume filtering for visual correspondence and beyond. Christoph Rhemann, Asmaa Hosni, Michael Bleyer, Carsten Rother, Margrit Gelautz, CVPR. Christoph Rhemann, Asmaa Hosni, Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast cost-volume filtering for visual correspondence and beyond. In CVPR, 2011.\n\nPixelwise view selection for unstructured multi-view stereo. L Johannes, Enliang Sch\u00f6nberger, Jan-Michael Zheng, Marc Frahm, Pollefeys, ECCV. Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016.\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, CVPR. Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016.\n\nReal-time human pose recognition in parts from single depth images. Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman, Andrew Blake, CVPR. Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman, and Andrew Blake. Real-time human pose recognition in parts from single depth images. In CVPR, 2011.\n\nPractical deep stereo (pds): Toward applications-friendly deep stereo matching. Stepan Tulyakov, Anton Ivanov, Francois Fleuret, NIPS. Stepan Tulyakov, Anton Ivanov, and Francois Fleuret. Practical deep stereo (pds): Toward applications-friendly deep stereo matching. In NIPS, 2018.\n\nDemon: Depth and motion network for learning monocular stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, CVPR. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovit- skiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In CVPR, 2017.\n\nLearning depth from monocular videos using direct methods. Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, Simon Lucey, CVPR. Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, and Simon Lucey. Learning depth from monocular videos using direct methods. In CVPR, 2018.\n\nAction recognition from depth maps using deep convolutional neural networks. Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, Philip O Ogunbona, IEEE Trans. on Hum.-Mac. Sys. Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, and Philip O Ogunbona. Action recognition from depth maps using deep convolutional neural networks. IEEE Trans. on Hum.- Mac. Sys., 2016.\n\nMulti-resolution real-time stereo on commodity graphics hardware. Ruigang Yang, Marc Pollefeys, CVPR. Ruigang Yang and Marc Pollefeys. Multi-resolution real-time stereo on commodity graphics hard- ware. In CVPR, 2003.\n\nMvsnet: Depth inference for unstructured multi-view stereo. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan, ECCVYao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstruc- tured multi-view stereo. ECCV, 2018.\n\nGeonet: Unsupervised learning of dense depth, optical flow and camera pose. Zhichao Yin, Jianping Shi, CVPR. Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In CVPR, 2018.\n\nMulti-scale context aggregation by dilated convolutions. ICLR. Fisher Yu, Vladlen Koltun, Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. ICLR, 2016.\n\nStereo matching by training a convolutional neural network to compare image patches. Jure Zbontar, Yann Lecun, J. of Machine Learning Research. Jure Zbontar and Yann LeCun. Stereo matching by training a convolutional neural network to compare image patches. J. of Machine Learning Research, 2016.\n\nPyramid scene parsing network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, CVPR. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, CVPR. Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, 2017.\n", "annotations": {"author": "[{\"end\":66,\"start\":46},{\"end\":132,\"start\":67},{\"end\":171,\"start\":133},{\"end\":214,\"start\":172}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":55},{\"end\":79,\"start\":75},{\"end\":144,\"start\":141},{\"end\":183,\"start\":178}]", "author_first_name": "[{\"end\":54,\"start\":46},{\"end\":74,\"start\":67},{\"end\":140,\"start\":133},{\"end\":174,\"start\":172},{\"end\":177,\"start\":175}]", "author_affiliation": "[{\"end\":65,\"start\":59},{\"end\":131,\"start\":104},{\"end\":170,\"start\":146},{\"end\":213,\"start\":207}]", "title": "[{\"end\":43,\"start\":1},{\"end\":257,\"start\":215}]", "venue": null, "abstract": "[{\"end\":1572,\"start\":303}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1674,\"start\":1653},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1730,\"start\":1709},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1750,\"start\":1732},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1989,\"start\":1963},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2627,\"start\":2608},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2650,\"start\":2629},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2671,\"start\":2652},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2692,\"start\":2673},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2757,\"start\":2733},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2780,\"start\":2759},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2801,\"start\":2782},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2921,\"start\":2902},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2942,\"start\":2923},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3350,\"start\":3331},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3371,\"start\":3352},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3627,\"start\":3604},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3958,\"start\":3937},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4876,\"start\":4854},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5115,\"start\":5096},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5306,\"start\":5287},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5471,\"start\":5450},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5696,\"start\":5677},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6093,\"start\":6077},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6552,\"start\":6534},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6574,\"start\":6554},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7257,\"start\":7233},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7302,\"start\":7278},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7946,\"start\":7930},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8237,\"start\":8220},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8472,\"start\":8453},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9262,\"start\":9245},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9632,\"start\":9615},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10654,\"start\":10638},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10880,\"start\":10864},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10914,\"start\":10896},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10954,\"start\":10935},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12930,\"start\":12909},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13300,\"start\":13277},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14026,\"start\":14005},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14116,\"start\":14100},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14794,\"start\":14776},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15428,\"start\":15407},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18187,\"start\":18169},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18460,\"start\":18441},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18906,\"start\":18887},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19029,\"start\":19012},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20285,\"start\":20271},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21415,\"start\":21396},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22099,\"start\":22079},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23570,\"start\":23551},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23594,\"start\":23572},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23616,\"start\":23594},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24104,\"start\":24085},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24388,\"start\":24369},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25221,\"start\":25204},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25355,\"start\":25334},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25802,\"start\":25779},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25819,\"start\":25803},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25837,\"start\":25820},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26358,\"start\":26339}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25764,\"start\":25688},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26082,\"start\":25765},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26490,\"start\":26083},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26743,\"start\":26491},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27015,\"start\":26744},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27491,\"start\":27016},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27551,\"start\":27492},{\"attributes\":{\"id\":\"fig_7\"},\"end\":27723,\"start\":27552},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29171,\"start\":27724},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30044,\"start\":29172},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30318,\"start\":30045},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30326,\"start\":30319},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30630,\"start\":30327},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":30717,\"start\":30631},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":30910,\"start\":30718},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":31048,\"start\":30911}]", "paragraph": "[{\"end\":2266,\"start\":1588},{\"end\":3142,\"start\":2268},{\"end\":4141,\"start\":3144},{\"end\":4427,\"start\":4143},{\"end\":4622,\"start\":4444},{\"end\":5888,\"start\":4624},{\"end\":6836,\"start\":5890},{\"end\":6978,\"start\":6838},{\"end\":7777,\"start\":6980},{\"end\":8641,\"start\":7779},{\"end\":9675,\"start\":8643},{\"end\":9829,\"start\":9677},{\"end\":10032,\"start\":9831},{\"end\":10322,\"start\":10045},{\"end\":11268,\"start\":10357},{\"end\":11692,\"start\":11330},{\"end\":11906,\"start\":11694},{\"end\":12055,\"start\":11949},{\"end\":12417,\"start\":12057},{\"end\":13080,\"start\":12478},{\"end\":13491,\"start\":13082},{\"end\":13951,\"start\":13493},{\"end\":14653,\"start\":13972},{\"end\":15322,\"start\":14655},{\"end\":15745,\"start\":15343},{\"end\":15834,\"start\":15787},{\"end\":16248,\"start\":15852},{\"end\":17052,\"start\":16345},{\"end\":18381,\"start\":17097},{\"end\":18591,\"start\":18383},{\"end\":18689,\"start\":18593},{\"end\":19967,\"start\":18691},{\"end\":20135,\"start\":19986},{\"end\":20875,\"start\":20137},{\"end\":21197,\"start\":20877},{\"end\":22556,\"start\":21199},{\"end\":22885,\"start\":22558},{\"end\":23377,\"start\":22906},{\"end\":23864,\"start\":23379},{\"end\":24389,\"start\":23866},{\"end\":25029,\"start\":24404},{\"end\":25687,\"start\":25031}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11948,\"start\":11907},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12477,\"start\":12418},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15786,\"start\":15746},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16305,\"start\":16249}]", "table_ref": "[{\"end\":13387,\"start\":13380},{\"end\":15321,\"start\":15314},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17532,\"start\":17525},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18917,\"start\":18910},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19337,\"start\":19330},{\"end\":20134,\"start\":20127},{\"end\":20170,\"start\":20163},{\"end\":20980,\"start\":20973},{\"end\":21467,\"start\":21460},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":22449,\"start\":22442},{\"end\":22760,\"start\":22753},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":24052,\"start\":24045}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1586,\"start\":1574},{\"attributes\":{\"n\":\"2\"},\"end\":4442,\"start\":4430},{\"attributes\":{\"n\":\"3\"},\"end\":10043,\"start\":10035},{\"attributes\":{\"n\":\"3.1\"},\"end\":10355,\"start\":10325},{\"attributes\":{\"n\":\"3.2\"},\"end\":11328,\"start\":11271},{\"attributes\":{\"n\":\"3.3\"},\"end\":13970,\"start\":13954},{\"attributes\":{\"n\":\"3.4\"},\"end\":15341,\"start\":15325},{\"attributes\":{\"n\":\"3.5\"},\"end\":15850,\"start\":15837},{\"attributes\":{\"n\":\"4\"},\"end\":16318,\"start\":16307},{\"attributes\":{\"n\":\"4.1\"},\"end\":16343,\"start\":16321},{\"attributes\":{\"n\":\"4.2\"},\"end\":17095,\"start\":17055},{\"attributes\":{\"n\":\"4.3\"},\"end\":19984,\"start\":19970},{\"end\":22904,\"start\":22888},{\"attributes\":{\"n\":\"5\"},\"end\":24402,\"start\":24392},{\"end\":25699,\"start\":25689},{\"end\":25776,\"start\":25766},{\"end\":26087,\"start\":26084},{\"end\":26512,\"start\":26492},{\"end\":26755,\"start\":26745},{\"end\":27020,\"start\":27017},{\"end\":27503,\"start\":27493},{\"end\":27563,\"start\":27553},{\"end\":29182,\"start\":29173},{\"end\":30055,\"start\":30046},{\"end\":30325,\"start\":30320},{\"end\":30641,\"start\":30632},{\"end\":30921,\"start\":30912}]", "table": "[{\"end\":29171,\"start\":27876},{\"end\":30044,\"start\":29306},{\"end\":30630,\"start\":30532},{\"end\":30910,\"start\":30782}]", "figure_caption": "[{\"end\":25764,\"start\":25701},{\"end\":26082,\"start\":25778},{\"end\":26490,\"start\":26088},{\"end\":26743,\"start\":26515},{\"end\":27015,\"start\":26757},{\"end\":27491,\"start\":27022},{\"end\":27551,\"start\":27505},{\"end\":27723,\"start\":27565},{\"end\":27876,\"start\":27726},{\"end\":29306,\"start\":29184},{\"end\":30318,\"start\":30057},{\"end\":30532,\"start\":30329},{\"end\":30717,\"start\":30643},{\"end\":30782,\"start\":30720},{\"end\":31048,\"start\":30923}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10321,\"start\":10313},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14180,\"start\":14172},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14652,\"start\":14644},{\"end\":17747,\"start\":17739},{\"end\":18225,\"start\":18217},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21196,\"start\":21188},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21608,\"start\":21598},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23012,\"start\":23003},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23222,\"start\":23214},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23886,\"start\":23878}]", "bib_author_first_name": "[{\"end\":31282,\"start\":31275},{\"end\":31300,\"start\":31290},{\"end\":31521,\"start\":31510},{\"end\":31534,\"start\":31528},{\"end\":31554,\"start\":31547},{\"end\":31570,\"start\":31565},{\"end\":31583,\"start\":31579},{\"end\":31585,\"start\":31584},{\"end\":31945,\"start\":31944},{\"end\":32117,\"start\":32110},{\"end\":32134,\"start\":32127},{\"end\":32151,\"start\":32144},{\"end\":32164,\"start\":32160},{\"end\":32396,\"start\":32391},{\"end\":32413,\"start\":32404},{\"end\":32426,\"start\":32423},{\"end\":32652,\"start\":32648},{\"end\":32664,\"start\":32660},{\"end\":32681,\"start\":32676},{\"end\":32695,\"start\":32691},{\"end\":32898,\"start\":32893},{\"end\":32918,\"start\":32907},{\"end\":32935,\"start\":32926},{\"end\":32950,\"start\":32946},{\"end\":33175,\"start\":33171},{\"end\":33187,\"start\":33182},{\"end\":33196,\"start\":33195},{\"end\":33198,\"start\":33197},{\"end\":33208,\"start\":33201},{\"end\":33222,\"start\":33219},{\"end\":33461,\"start\":33454},{\"end\":33475,\"start\":33470},{\"end\":33494,\"start\":33487},{\"end\":33496,\"start\":33495},{\"end\":33748,\"start\":33742},{\"end\":33761,\"start\":33753},{\"end\":33772,\"start\":33766},{\"end\":34198,\"start\":34191},{\"end\":34214,\"start\":34208},{\"end\":34399,\"start\":34392},{\"end\":34408,\"start\":34404},{\"end\":34420,\"start\":34414},{\"end\":34672,\"start\":34665},{\"end\":34684,\"start\":34677},{\"end\":34700,\"start\":34692},{\"end\":34710,\"start\":34706},{\"end\":34949,\"start\":34942},{\"end\":34963,\"start\":34954},{\"end\":35220,\"start\":35214},{\"end\":35233,\"start\":35228},{\"end\":35250,\"start\":35242},{\"end\":35265,\"start\":35257},{\"end\":35280,\"start\":35273},{\"end\":35561,\"start\":35553},{\"end\":35572,\"start\":35566},{\"end\":35586,\"start\":35577},{\"end\":35984,\"start\":35976},{\"end\":36141,\"start\":36138},{\"end\":36158,\"start\":36153},{\"end\":36175,\"start\":36169},{\"end\":36371,\"start\":36365},{\"end\":36383,\"start\":36376},{\"end\":36397,\"start\":36390},{\"end\":36410,\"start\":36405},{\"end\":36418,\"start\":36416},{\"end\":36657,\"start\":36653},{\"end\":36671,\"start\":36667},{\"end\":36693,\"start\":36685},{\"end\":36709,\"start\":36704},{\"end\":36721,\"start\":36717},{\"end\":36738,\"start\":36731},{\"end\":36753,\"start\":36749},{\"end\":37035,\"start\":37029},{\"end\":37043,\"start\":37040},{\"end\":37058,\"start\":37050},{\"end\":37073,\"start\":37065},{\"end\":37311,\"start\":37304},{\"end\":37324,\"start\":37319},{\"end\":37336,\"start\":37331},{\"end\":37349,\"start\":37342},{\"end\":37360,\"start\":37355},{\"end\":37370,\"start\":37367},{\"end\":37379,\"start\":37377},{\"end\":37394,\"start\":37386},{\"end\":37691,\"start\":37686},{\"end\":37704,\"start\":37697},{\"end\":37719,\"start\":37711},{\"end\":37728,\"start\":37725},{\"end\":38070,\"start\":38066},{\"end\":38089,\"start\":38083},{\"end\":38103,\"start\":38097},{\"end\":38400,\"start\":38392},{\"end\":38412,\"start\":38408},{\"end\":38424,\"start\":38418},{\"end\":38441,\"start\":38434},{\"end\":38457,\"start\":38451},{\"end\":38473,\"start\":38467},{\"end\":38493,\"start\":38487},{\"end\":38799,\"start\":38795},{\"end\":38808,\"start\":38805},{\"end\":38821,\"start\":38814},{\"end\":38834,\"start\":38828},{\"end\":38849,\"start\":38841},{\"end\":39076,\"start\":39067},{\"end\":39091,\"start\":39086},{\"end\":39106,\"start\":39099},{\"end\":39122,\"start\":39115},{\"end\":39138,\"start\":39131},{\"end\":39382,\"start\":39381},{\"end\":39400,\"start\":39393},{\"end\":39425,\"start\":39414},{\"end\":39437,\"start\":39433},{\"end\":39658,\"start\":39650},{\"end\":39688,\"start\":39677},{\"end\":39873,\"start\":39868},{\"end\":39889,\"start\":39883},{\"end\":39905,\"start\":39902},{\"end\":39916,\"start\":39912},{\"end\":39928,\"start\":39924},{\"end\":39947,\"start\":39940},{\"end\":39959,\"start\":39955},{\"end\":39974,\"start\":39968},{\"end\":40276,\"start\":40270},{\"end\":40292,\"start\":40287},{\"end\":40309,\"start\":40301},{\"end\":40545,\"start\":40537},{\"end\":40566,\"start\":40558},{\"end\":40578,\"start\":40573},{\"end\":40594,\"start\":40586},{\"end\":40606,\"start\":40602},{\"end\":40618,\"start\":40612},{\"end\":40638,\"start\":40632},{\"end\":40911,\"start\":40903},{\"end\":40922,\"start\":40918},{\"end\":40929,\"start\":40923},{\"end\":40946,\"start\":40943},{\"end\":40957,\"start\":40952},{\"end\":41195,\"start\":41189},{\"end\":41209,\"start\":41202},{\"end\":41220,\"start\":41214},{\"end\":41230,\"start\":41226},{\"end\":41243,\"start\":41238},{\"end\":41256,\"start\":41250},{\"end\":41258,\"start\":41257},{\"end\":41572,\"start\":41565},{\"end\":41583,\"start\":41579},{\"end\":41781,\"start\":41778},{\"end\":41792,\"start\":41787},{\"end\":41804,\"start\":41798},{\"end\":41813,\"start\":41809},{\"end\":41824,\"start\":41820},{\"end\":42050,\"start\":42043},{\"end\":42064,\"start\":42056},{\"end\":42267,\"start\":42261},{\"end\":42279,\"start\":42272},{\"end\":42477,\"start\":42473},{\"end\":42491,\"start\":42487},{\"end\":42727,\"start\":42717},{\"end\":42742,\"start\":42734},{\"end\":42756,\"start\":42748},{\"end\":42769,\"start\":42761},{\"end\":42781,\"start\":42776},{\"end\":42979,\"start\":42972},{\"end\":42993,\"start\":42986},{\"end\":43005,\"start\":43001},{\"end\":43022,\"start\":43015}]", "bib_author_last_name": "[{\"end\":31288,\"start\":31283},{\"end\":31305,\"start\":31301},{\"end\":31526,\"start\":31522},{\"end\":31545,\"start\":31535},{\"end\":31563,\"start\":31555},{\"end\":31577,\"start\":31571},{\"end\":31592,\"start\":31586},{\"end\":31952,\"start\":31946},{\"end\":31961,\"start\":31954},{\"end\":32125,\"start\":32118},{\"end\":32142,\"start\":32135},{\"end\":32158,\"start\":32152},{\"end\":32170,\"start\":32165},{\"end\":32402,\"start\":32397},{\"end\":32421,\"start\":32414},{\"end\":32433,\"start\":32427},{\"end\":32658,\"start\":32653},{\"end\":32674,\"start\":32665},{\"end\":32689,\"start\":32682},{\"end\":32703,\"start\":32696},{\"end\":32905,\"start\":32899},{\"end\":32924,\"start\":32919},{\"end\":32944,\"start\":32936},{\"end\":32960,\"start\":32951},{\"end\":33180,\"start\":33176},{\"end\":33193,\"start\":33188},{\"end\":33217,\"start\":33209},{\"end\":33227,\"start\":33223},{\"end\":33468,\"start\":33462},{\"end\":33485,\"start\":33476},{\"end\":33504,\"start\":33497},{\"end\":33751,\"start\":33749},{\"end\":33764,\"start\":33762},{\"end\":33777,\"start\":33773},{\"end\":34206,\"start\":34199},{\"end\":34224,\"start\":34215},{\"end\":34402,\"start\":34400},{\"end\":34412,\"start\":34409},{\"end\":34425,\"start\":34421},{\"end\":34675,\"start\":34673},{\"end\":34690,\"start\":34685},{\"end\":34704,\"start\":34701},{\"end\":34714,\"start\":34711},{\"end\":34952,\"start\":34950},{\"end\":34972,\"start\":34964},{\"end\":35226,\"start\":35221},{\"end\":35240,\"start\":35234},{\"end\":35255,\"start\":35251},{\"end\":35271,\"start\":35266},{\"end\":35286,\"start\":35281},{\"end\":35564,\"start\":35562},{\"end\":35575,\"start\":35573},{\"end\":35591,\"start\":35587},{\"end\":35987,\"start\":35985},{\"end\":36151,\"start\":36142},{\"end\":36167,\"start\":36159},{\"end\":36185,\"start\":36176},{\"end\":36374,\"start\":36372},{\"end\":36388,\"start\":36384},{\"end\":36403,\"start\":36398},{\"end\":36414,\"start\":36411},{\"end\":36423,\"start\":36419},{\"end\":36665,\"start\":36658},{\"end\":36683,\"start\":36672},{\"end\":36702,\"start\":36694},{\"end\":36715,\"start\":36710},{\"end\":36729,\"start\":36722},{\"end\":36747,\"start\":36739},{\"end\":36757,\"start\":36754},{\"end\":37038,\"start\":37036},{\"end\":37048,\"start\":37044},{\"end\":37063,\"start\":37059},{\"end\":37076,\"start\":37074},{\"end\":37317,\"start\":37312},{\"end\":37329,\"start\":37325},{\"end\":37340,\"start\":37337},{\"end\":37353,\"start\":37350},{\"end\":37365,\"start\":37361},{\"end\":37375,\"start\":37371},{\"end\":37384,\"start\":37380},{\"end\":37400,\"start\":37395},{\"end\":37695,\"start\":37692},{\"end\":37709,\"start\":37705},{\"end\":37723,\"start\":37720},{\"end\":37733,\"start\":37729},{\"end\":38081,\"start\":38071},{\"end\":38095,\"start\":38090},{\"end\":38112,\"start\":38104},{\"end\":38406,\"start\":38401},{\"end\":38416,\"start\":38413},{\"end\":38432,\"start\":38425},{\"end\":38449,\"start\":38442},{\"end\":38465,\"start\":38458},{\"end\":38485,\"start\":38474},{\"end\":38498,\"start\":38494},{\"end\":38803,\"start\":38800},{\"end\":38812,\"start\":38809},{\"end\":38826,\"start\":38822},{\"end\":38839,\"start\":38835},{\"end\":38855,\"start\":38850},{\"end\":39084,\"start\":39077},{\"end\":39097,\"start\":39092},{\"end\":39113,\"start\":39107},{\"end\":39129,\"start\":39123},{\"end\":39146,\"start\":39139},{\"end\":39391,\"start\":39383},{\"end\":39412,\"start\":39401},{\"end\":39431,\"start\":39426},{\"end\":39443,\"start\":39438},{\"end\":39454,\"start\":39445},{\"end\":39675,\"start\":39659},{\"end\":39694,\"start\":39689},{\"end\":39881,\"start\":39874},{\"end\":39900,\"start\":39890},{\"end\":39910,\"start\":39906},{\"end\":39922,\"start\":39917},{\"end\":39938,\"start\":39929},{\"end\":39953,\"start\":39948},{\"end\":39966,\"start\":39960},{\"end\":39980,\"start\":39975},{\"end\":40285,\"start\":40277},{\"end\":40299,\"start\":40293},{\"end\":40317,\"start\":40310},{\"end\":40556,\"start\":40546},{\"end\":40571,\"start\":40567},{\"end\":40584,\"start\":40579},{\"end\":40600,\"start\":40595},{\"end\":40610,\"start\":40607},{\"end\":40630,\"start\":40619},{\"end\":40643,\"start\":40639},{\"end\":40916,\"start\":40912},{\"end\":40941,\"start\":40930},{\"end\":40950,\"start\":40947},{\"end\":40963,\"start\":40958},{\"end\":41200,\"start\":41196},{\"end\":41212,\"start\":41210},{\"end\":41224,\"start\":41221},{\"end\":41236,\"start\":41231},{\"end\":41248,\"start\":41244},{\"end\":41267,\"start\":41259},{\"end\":41577,\"start\":41573},{\"end\":41593,\"start\":41584},{\"end\":41785,\"start\":41782},{\"end\":41796,\"start\":41793},{\"end\":41807,\"start\":41805},{\"end\":41818,\"start\":41814},{\"end\":41829,\"start\":41825},{\"end\":42054,\"start\":42051},{\"end\":42068,\"start\":42065},{\"end\":42270,\"start\":42268},{\"end\":42286,\"start\":42280},{\"end\":42485,\"start\":42478},{\"end\":42497,\"start\":42492},{\"end\":42732,\"start\":42728},{\"end\":42746,\"start\":42743},{\"end\":42759,\"start\":42757},{\"end\":42774,\"start\":42770},{\"end\":42785,\"start\":42782},{\"end\":42984,\"start\":42980},{\"end\":42999,\"start\":42994},{\"end\":43013,\"start\":43006},{\"end\":43027,\"start\":43023}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4252896},\"end\":31395,\"start\":31242},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3429309},\"end\":31889,\"start\":31397},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11008141},\"end\":32054,\"start\":31891},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6681692},\"end\":32314,\"start\":32056},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2255738},\"end\":32578,\"start\":32316},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14517241},\"end\":32854,\"start\":32580},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2194132},\"end\":33094,\"start\":32856},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":299085},\"end\":33383,\"start\":33096},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206596513},\"end\":33653,\"start\":33385},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16192447},\"end\":34146,\"start\":33655},{\"attributes\":{\"id\":\"b10\"},\"end\":34366,\"start\":34148},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1264129},\"end\":34584,\"start\":34368},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":436933},\"end\":34872,\"start\":34586},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5761531},\"end\":35171,\"start\":34874},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4559916},\"end\":35427,\"start\":35173},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":51614936},\"end\":35891,\"start\":35429},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4003931},\"end\":36106,\"start\":35893},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6099034},\"end\":36293,\"start\":36108},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9035204},\"end\":36579,\"start\":36295},{\"attributes\":{\"id\":\"b19\"},\"end\":36955,\"start\":36581},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206853077},\"end\":37226,\"start\":36957},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195346406},\"end\":37600,\"start\":37228},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15774646},\"end\":37966,\"start\":37602},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3645349},\"end\":38286,\"start\":37968},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594275},\"end\":38736,\"start\":38288},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5987644},\"end\":39000,\"start\":38738},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1680724},\"end\":39318,\"start\":39002},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":977535},\"end\":39615,\"start\":39320},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1728538},\"end\":39798,\"start\":39617},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7731948},\"end\":40188,\"start\":39800},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":46938951},\"end\":40472,\"start\":40190},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6159584},\"end\":40842,\"start\":40474},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":21352010},\"end\":41110,\"start\":40844},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":32229496},\"end\":41497,\"start\":41112},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13990128},\"end\":41716,\"start\":41499},{\"attributes\":{\"id\":\"b35\"},\"end\":41965,\"start\":41718},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3714620},\"end\":42196,\"start\":41967},{\"attributes\":{\"id\":\"b37\"},\"end\":42386,\"start\":42198},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6913648},\"end\":42684,\"start\":42388},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5299559},\"end\":42912,\"start\":42686},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11977588},\"end\":43168,\"start\":42914}]", "bib_title": "[{\"end\":31273,\"start\":31242},{\"end\":31508,\"start\":31397},{\"end\":31942,\"start\":31891},{\"end\":32108,\"start\":32056},{\"end\":32389,\"start\":32316},{\"end\":32646,\"start\":32580},{\"end\":32891,\"start\":32856},{\"end\":33169,\"start\":33096},{\"end\":33452,\"start\":33385},{\"end\":33740,\"start\":33655},{\"end\":34390,\"start\":34368},{\"end\":34663,\"start\":34586},{\"end\":34940,\"start\":34874},{\"end\":35212,\"start\":35173},{\"end\":35551,\"start\":35429},{\"end\":35974,\"start\":35893},{\"end\":36136,\"start\":36108},{\"end\":36363,\"start\":36295},{\"end\":37027,\"start\":36957},{\"end\":37302,\"start\":37228},{\"end\":37684,\"start\":37602},{\"end\":38064,\"start\":37968},{\"end\":38390,\"start\":38288},{\"end\":38793,\"start\":38738},{\"end\":39065,\"start\":39002},{\"end\":39379,\"start\":39320},{\"end\":39648,\"start\":39617},{\"end\":39866,\"start\":39800},{\"end\":40268,\"start\":40190},{\"end\":40535,\"start\":40474},{\"end\":40901,\"start\":40844},{\"end\":41187,\"start\":41112},{\"end\":41563,\"start\":41499},{\"end\":42041,\"start\":41967},{\"end\":42471,\"start\":42388},{\"end\":42715,\"start\":42686},{\"end\":42970,\"start\":42914}]", "bib_author": "[{\"end\":31290,\"start\":31275},{\"end\":31307,\"start\":31290},{\"end\":31528,\"start\":31510},{\"end\":31547,\"start\":31528},{\"end\":31565,\"start\":31547},{\"end\":31579,\"start\":31565},{\"end\":31594,\"start\":31579},{\"end\":31954,\"start\":31944},{\"end\":31963,\"start\":31954},{\"end\":32127,\"start\":32110},{\"end\":32144,\"start\":32127},{\"end\":32160,\"start\":32144},{\"end\":32172,\"start\":32160},{\"end\":32404,\"start\":32391},{\"end\":32423,\"start\":32404},{\"end\":32435,\"start\":32423},{\"end\":32660,\"start\":32648},{\"end\":32676,\"start\":32660},{\"end\":32691,\"start\":32676},{\"end\":32705,\"start\":32691},{\"end\":32907,\"start\":32893},{\"end\":32926,\"start\":32907},{\"end\":32946,\"start\":32926},{\"end\":32962,\"start\":32946},{\"end\":33182,\"start\":33171},{\"end\":33195,\"start\":33182},{\"end\":33201,\"start\":33195},{\"end\":33219,\"start\":33201},{\"end\":33229,\"start\":33219},{\"end\":33470,\"start\":33454},{\"end\":33487,\"start\":33470},{\"end\":33506,\"start\":33487},{\"end\":33753,\"start\":33742},{\"end\":33766,\"start\":33753},{\"end\":33779,\"start\":33766},{\"end\":34208,\"start\":34191},{\"end\":34226,\"start\":34208},{\"end\":34404,\"start\":34392},{\"end\":34414,\"start\":34404},{\"end\":34427,\"start\":34414},{\"end\":34677,\"start\":34665},{\"end\":34692,\"start\":34677},{\"end\":34706,\"start\":34692},{\"end\":34716,\"start\":34706},{\"end\":34954,\"start\":34942},{\"end\":34974,\"start\":34954},{\"end\":35228,\"start\":35214},{\"end\":35242,\"start\":35228},{\"end\":35257,\"start\":35242},{\"end\":35273,\"start\":35257},{\"end\":35288,\"start\":35273},{\"end\":35566,\"start\":35553},{\"end\":35577,\"start\":35566},{\"end\":35593,\"start\":35577},{\"end\":35989,\"start\":35976},{\"end\":36153,\"start\":36138},{\"end\":36169,\"start\":36153},{\"end\":36187,\"start\":36169},{\"end\":36376,\"start\":36365},{\"end\":36390,\"start\":36376},{\"end\":36405,\"start\":36390},{\"end\":36416,\"start\":36405},{\"end\":36425,\"start\":36416},{\"end\":36667,\"start\":36653},{\"end\":36685,\"start\":36667},{\"end\":36704,\"start\":36685},{\"end\":36717,\"start\":36704},{\"end\":36731,\"start\":36717},{\"end\":36749,\"start\":36731},{\"end\":36759,\"start\":36749},{\"end\":37040,\"start\":37029},{\"end\":37050,\"start\":37040},{\"end\":37065,\"start\":37050},{\"end\":37078,\"start\":37065},{\"end\":37319,\"start\":37304},{\"end\":37331,\"start\":37319},{\"end\":37342,\"start\":37331},{\"end\":37355,\"start\":37342},{\"end\":37367,\"start\":37355},{\"end\":37377,\"start\":37367},{\"end\":37386,\"start\":37377},{\"end\":37402,\"start\":37386},{\"end\":37697,\"start\":37686},{\"end\":37711,\"start\":37697},{\"end\":37725,\"start\":37711},{\"end\":37735,\"start\":37725},{\"end\":38083,\"start\":38066},{\"end\":38097,\"start\":38083},{\"end\":38114,\"start\":38097},{\"end\":38408,\"start\":38392},{\"end\":38418,\"start\":38408},{\"end\":38434,\"start\":38418},{\"end\":38451,\"start\":38434},{\"end\":38467,\"start\":38451},{\"end\":38487,\"start\":38467},{\"end\":38500,\"start\":38487},{\"end\":38805,\"start\":38795},{\"end\":38814,\"start\":38805},{\"end\":38828,\"start\":38814},{\"end\":38841,\"start\":38828},{\"end\":38857,\"start\":38841},{\"end\":39086,\"start\":39067},{\"end\":39099,\"start\":39086},{\"end\":39115,\"start\":39099},{\"end\":39131,\"start\":39115},{\"end\":39148,\"start\":39131},{\"end\":39393,\"start\":39381},{\"end\":39414,\"start\":39393},{\"end\":39433,\"start\":39414},{\"end\":39445,\"start\":39433},{\"end\":39456,\"start\":39445},{\"end\":39677,\"start\":39650},{\"end\":39696,\"start\":39677},{\"end\":39883,\"start\":39868},{\"end\":39902,\"start\":39883},{\"end\":39912,\"start\":39902},{\"end\":39924,\"start\":39912},{\"end\":39940,\"start\":39924},{\"end\":39955,\"start\":39940},{\"end\":39968,\"start\":39955},{\"end\":39982,\"start\":39968},{\"end\":40287,\"start\":40270},{\"end\":40301,\"start\":40287},{\"end\":40319,\"start\":40301},{\"end\":40558,\"start\":40537},{\"end\":40573,\"start\":40558},{\"end\":40586,\"start\":40573},{\"end\":40602,\"start\":40586},{\"end\":40612,\"start\":40602},{\"end\":40632,\"start\":40612},{\"end\":40645,\"start\":40632},{\"end\":40918,\"start\":40903},{\"end\":40943,\"start\":40918},{\"end\":40952,\"start\":40943},{\"end\":40965,\"start\":40952},{\"end\":41202,\"start\":41189},{\"end\":41214,\"start\":41202},{\"end\":41226,\"start\":41214},{\"end\":41238,\"start\":41226},{\"end\":41250,\"start\":41238},{\"end\":41269,\"start\":41250},{\"end\":41579,\"start\":41565},{\"end\":41595,\"start\":41579},{\"end\":41787,\"start\":41778},{\"end\":41798,\"start\":41787},{\"end\":41809,\"start\":41798},{\"end\":41820,\"start\":41809},{\"end\":41831,\"start\":41820},{\"end\":42056,\"start\":42043},{\"end\":42070,\"start\":42056},{\"end\":42272,\"start\":42261},{\"end\":42288,\"start\":42272},{\"end\":42487,\"start\":42473},{\"end\":42499,\"start\":42487},{\"end\":42734,\"start\":42717},{\"end\":42748,\"start\":42734},{\"end\":42761,\"start\":42748},{\"end\":42776,\"start\":42761},{\"end\":42787,\"start\":42776},{\"end\":42986,\"start\":42972},{\"end\":43001,\"start\":42986},{\"end\":43015,\"start\":43001},{\"end\":43029,\"start\":43015}]", "bib_venue": "[{\"end\":33920,\"start\":33858},{\"end\":31311,\"start\":31307},{\"end\":31636,\"start\":31594},{\"end\":31967,\"start\":31963},{\"end\":32176,\"start\":32172},{\"end\":32439,\"start\":32435},{\"end\":32709,\"start\":32705},{\"end\":32966,\"start\":32962},{\"end\":33233,\"start\":33229},{\"end\":33510,\"start\":33506},{\"end\":33856,\"start\":33779},{\"end\":34189,\"start\":34148},{\"end\":34469,\"start\":34427},{\"end\":34720,\"start\":34716},{\"end\":35016,\"start\":34974},{\"end\":35292,\"start\":35288},{\"end\":35655,\"start\":35593},{\"end\":35993,\"start\":35989},{\"end\":36191,\"start\":36187},{\"end\":36429,\"start\":36425},{\"end\":36651,\"start\":36581},{\"end\":37085,\"start\":37078},{\"end\":37406,\"start\":37402},{\"end\":37777,\"start\":37735},{\"end\":38118,\"start\":38114},{\"end\":38504,\"start\":38500},{\"end\":38861,\"start\":38857},{\"end\":39152,\"start\":39148},{\"end\":39460,\"start\":39456},{\"end\":39700,\"start\":39696},{\"end\":39986,\"start\":39982},{\"end\":40323,\"start\":40319},{\"end\":40649,\"start\":40645},{\"end\":40969,\"start\":40965},{\"end\":41297,\"start\":41269},{\"end\":41599,\"start\":41595},{\"end\":41776,\"start\":41718},{\"end\":42074,\"start\":42070},{\"end\":42259,\"start\":42198},{\"end\":42530,\"start\":42499},{\"end\":42791,\"start\":42787},{\"end\":43033,\"start\":43029}]"}}}, "year": 2023, "month": 12, "day": 17}