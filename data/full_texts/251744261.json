{"id": 251744261, "updated": "2022-08-26 16:01:45.108", "metadata": {"title": "FHDnn: communication efficient and robust federated learning for AIoT networks", "authors": "[{\"first\":\"Rishikanth\",\"last\":\"Chandrasekaran\",\"middle\":[]},{\"first\":\"Kazim\",\"last\":\"Ergun\",\"middle\":[]},{\"first\":\"Jihyun\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Dhanush\",\"last\":\"Nanjunda\",\"middle\":[]},{\"first\":\"Jaeyoung\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 59th ACM/IEEE Design Automation Conference", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The advent of IoT and advances in edge computing inspired federated learning, a distributed algorithm to enable on device learning. Transmission costs, unreliable networks and limited compute power all of which are typical characteristics of IoT networks pose a severe bottleneck for federated learning. In this work we propose FHDnn, a synergetic federated learning framework that combines the salient aspects of CNNs and Hyperdimensional Computing. FHDnn performs hyperdimensional learning on features extracted from a self-supervised contrastive learning framework to accelerate training, lower communication costs, and increase robustness to network errors by avoiding the transmission of the CNN and training only the hyperdimensional component. Compared to CNNs, we show through experiments that FHDnn reduces communication costs by 66X, local client compute and energy consumption by 1.5 - 6X, while being highly robust to network errors with minimal loss in accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/dac/ChandrasekaranE22", "doi": "10.1145/3489517.3530394"}}, "content": {"source": {"pdf_hash": "6d7099e4bfe8259a33fea17dc8f9a4c8ea98b7e9", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "52ad02ba80219f5389672296e14710488a6ca456", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6d7099e4bfe8259a33fea17dc8f9a4c8ea98b7e9.txt", "contents": "\nFHDnn: Communication Efficient and Robust Federated Learning for AIoT Networks\n\n\nRishikanth Chandrasekaran \nDepartment of Computer Science and Engineering\nUC San Diego\nLa Jolla92093CA\n\nKazim Ergun kergun@ucsd.edu \nDepartment of Computer Science and Engineering\nUC San Diego\nLa Jolla92093CA\n\nJihyun Lee \nDepartment of Computer Science and Engineering\nUC San Diego\nLa Jolla92093CA\n\nDhanush Nanjunda dnanjund@ucsd.edu \nDepartment of Computer Science and Engineering\nUC San Diego\nLa Jolla92093CA\n\nJaeyoung Kang j5kang@ucsd.edu \nDepartment of Computer Science and Engineering\nUC San Diego\nLa Jolla92093CA\n\nTajana Rosing tajana@ucsd.edu \nDepartment of Computer Science and Engineering\nUC San Diego\nLa Jolla92093CA\n\nFHDnn: Communication Efficient and Robust Federated Learning for AIoT Networks\n10.1145/3489517.3530394CCS Concepts \u2022 Computing methodologies \u2192 Machine learning Keywords Hyperdimensional Computing, Federated Learning\nThe advent of IoT and advances in edge computing inspired federated learning, a distributed algorithm to enable on device learning. Transmission costs, unreliable networks and limited compute power all of which are typical characteristics of IoT networks pose a severe bottleneck for federated learning. In this work we propose FHDnn, a synergetic federated learning framework that combines the salient aspects of CNNs and Hyperdimensional Computing. FHDnn performs hyperdimensional learning on features extracted from a self-supervised contrastive learning framework to accelerate training, lower communication costs, and increase robustness to network errors by avoiding the transmission of the CNN and training only the hyperdimensional component. Compared to CNNs, we show through experiments that FHDnn reduces communication costs by 66\u00d7, local client compute and energy consumption by 1.5 -6\u00d7, while being highly robust to network errors with minimal loss in accuracy.\n\nIntroduction\n\nA group of distributed edge devices communicating with each other and sharing data is loosely termed the Internet of Things (IoT). These edge devices are privy to a rich source of data which when leveraged can enable various smart applications such as smart cities [15] [2] and AI-enabled farming [18]. However, often the private and sensitive nature of the data coupled with high transmission costs prevent the central aggregation of data to the cloud. Recent advances in edge computing enabled the idea of distributed computing for on device processing. One such distributed learning paradigm is federated learning (FL) [17]. FL learns a machine learning model on data distributed across various devices without having to aggregate them centrally. FL works by training models locally on the device with data visible to each device and then averages these models from all participating devices. Figure 1: FHDnn against CNNs for federated learning Transmission costs, unreliable networks, and limited on device computer power pose significant challenges for FL. Previous works [4,5] have explored model compression methods and dropout techniques to reduce the communication cost by decreasing the size of the model updates. However, these methods do not factor in the nonideality of IoT networks, assuming reliable lossless communication and subsequently are neither robust to network errors nor provide guarantees for convergence. Also, IoT often uses Low Power Wide Area Networks (LP-WAN) to conserve energy of battery operated edge devicesm but it has limited bandwidth, high packet loss and no sophisticated coding scheme making FL vulnerable to errors.\n\nWe present FHDnn a novel synergetic federated learning framework that combines 2 different learning paradigms of Deep Learning and Hyperdimensional Computing (HDC) [11]. Deep learning excels at learning a complex hierarchy of features and boasts high accuracy however at the cost of compute power often requiring GPUs to train. HDC on the other hand features lightweight training using simple operations on distributed low precision representations that are inherently robust to errors. However, they don't enjoy the same accuracy as deep learning due to their inability to learn features automatically. FHDnn combines the complimentary salient features of both learning methodologies to enable a lightweight highly robust FL framework that addresses each of the above challenges. In this work, we limit ourselves to the problem of image classification, a common application in IoT.\n\nFHDnn uses a pre-trained Convolutional Neural Network (CNN) as a feature extractor, the output of which are encoded into hypervectors that are then used for training a federated HD learner. Specifically we utilize a CNN trained using SimCLR [6] a contrastive learning framework which learns informative representations of images in a self-supervised manner that generalizes well to several datasets. FHDnn avoids the transmission of the CNN and instead trains only the HD classifier in a federated manner. This simple strategy accelerates learning, reduces transmission cost and utilizes the inherent robustness of HDC to tackle network errors as shown in Figure 1. In this work, we detail the architecture of FHDnn and systematically compare the performance of FHDnn with CNN under various settings. We summarize our key contributions below: \u2022 We propose FHDnn, a novel FL strategy that is robust to lossy network transmission, is incredibly lightweight to train, and converges faster.  \n\n\nBackground and Motivation 2.1 IoT networks\n\nIoT networks often involve a large number of battery operated edge devices operating on a Low Power Wide Area Networks (LP-WAN). LPWAN networks have limited bandwidth, narrow spectrum, and often lack any advanced modulation schemes due to compute cost and power constraints. Further, the network performance is worse due to the presence of packet loss which is highly prevalent in these networks [19,20]. A study [16] shows that retransmission is non ideal as it further increases energy consumption and reduces network performance due to limited capacity. However, [9] shows that tolerating a packet loss rate of 20% allows for increased energy efficiency and network capacity.\n\n\nChallenges in FL\n\nCommunication Efficiency: FL involves multiple rounds of communication typically until a target test accuracy is achieved. Each round, the participating clients send their models to the server. Complex models, like CNNs, contain millions of parameters resulting in large updates. FL typically takes several rounds to converge to the optimum which further exacerbates the communication cost. Lossy Transmission: As detailed above, IoT networks are often unreliable which adds noise to the model updates causing convergence issues. CNNs in particular are not robust to noise on weights as shown in [3]. Failure to converge results in poor accuracy while longer convergence times results in increased communication costs. Resource constraints: Battery operated edge devices typical to IoT networks have limited power and computation budgets. CNN based FL methods require edge devices to perform on-device backpropogation during each round of training which is computationally expensive incurring high resource usage.  \n\n\nFeature extractor\n\nWhile in theory any standard CNN can be used as a feature extractor, due to its salient characteristics we use a pre-trained SimCLR Resnet model as our feature extractor. SimCLR [6] is a contrastive learning framework which learns representations of images in a self-supervised manner by maximizing the similarity between latent space representations of different augmentations of a single image. This class agnostic framework trained on a large image dataset allows for transfer learning over multiple datasets, (as evaluated in [6]) making it ideal for a generic feature extractor. Standard CNNs learn representations that are fine-tuned to optimize the classification performance of the dense classifier at the end of the network. Since SimCLR focuses on learning general representations as opposed to classification oriented representations, it is a better choice of feature extractor. Note that We choose the ResNet architecture due to availability of pre-trained models. One could use other models such as MobileNet, which are more ideal for edge devices with resource constraints.\n\n\nHD learner\n\nHDC is a computing paradigm based on biologically plausible models of data representation [12]. HD works by encoding data into low precision vectors of very large dimensions, referred to as hyper vectors in literature. HD classifiers operate on these vectors using binding and bundling operations which are simple and highly parallelizable.\n\nHere we are concerned with encoding the output of a neural network. We use an encoding method proposed in [10] based on the notion of random projection. This approach embeds the data into a high-dimensional Euclidean space under a random linear map before quantizing them. More formally, given a point \u2208 X, the features \u2282 Z are extracted using the feature extractor : X \u2192 Z where is a pre-trained neural network. The HD embedding is constructed as ( ) = sign(\u03a6 ) under the encoding function : Z \u2192 H the rows of which \u03a6 \u2208 R \u00d7 are generated by randomly sampling directions from the -dimensional unit sphere. sign(\u03a6 ) is the element-wise sign function returning +1 if \u03a6 \u2265 0 and \u22121 otherwise. Figure 3 summarizes the federated training process for FHDnn. 3.4.1 Client Local Training: Each client initially starts with a feature extractor and an untrained HD learner. Once we get the encoded hypervectors using the method described above, we create class prototypes by bundling together hypervectors of the corresponding class using c = h . Inference is done by computing the cosine similarity metric between a given encoded data point with each of the prototypes, returning the class which has maximum similarity. After this one-shot learning process we iteratively refine the class prototypes by subtracting the hypervectors from the mispredicted class prototype and adding it to the correct prototype as shown in Figure 2. We define the complete HD model C as the concatenation of class hypervectors, i.e., C = [c 1 , c 2 , ..., c ].\n\n\nFederated Training\n\n\nFederated Bundling:\n\nIn the federated bundling framework, each client maintains its own HD model and participates to build a global model in a distributed fashion. This is achieved via an iterative training procedure for which we describe one round (say -th) of the algorithm below.\n\n(1) Broadcast: The central server broadcasts the latest global HD model, C , to all clients. (2) Local updates: Each participating client \u2208 [ ] sets its model C = C and then performs training for epochs using local data as described in 3.4.1 (3) Aggregation: The central server receives and aggregates the local models to produce a new global model:\nC +1 = \u2211\ufe01 =1 C +1 .(1)\nAfter aggregation, the server moves on to the next round, + 1. This procedure is carried out until sufficient convergence is achieved.\n\n\nFL Over Unreliable Channels With FHDnn\n\nFederated learning is often carried out over wireless channels that attenuate the transmitted signal and introduce noise followed by packet losses. The centralized server is assumed to be able to broadcast the models reliably, error-free at arbitrary rates, which is a common assumption in many recent works. For uplink communications, the channel capacity per client is more constrained and unreliable due to shared wireless medium, even at very low rates.\n\nThe bandwidth allocated per client decreases with the number of clients, so does the capacity. Accordingly, the volume of data that can be conveyed reliably, i.e, throughput, scales by 1/ . This implies that the data rates will be small, resulting in slow training speed unless transmission power is increased, which is undesirable considering energy consumption concerns.\n\nInstead of limiting the rate to achieve error-free communication, we admit errors for the channel output at the server as perturbations in the client models can be tolerated to an extent by FHDnn. If the model is robust to errors, then there is no need for perfectly reliable transmissions. Thus, we analyze FHDnn assuming that the clients communicate over unreliable channels and the transmitted models are corrupted.\n\nWe consider three error models at different layers of the network stack. All models are applicable in practice depending on the underlying protocol. We first explore the properties of HD computing that makes the learning robust under the considered error models, then introduce different techniques for further improvement.\n\n3.5.1 Noisy Aggregation. Conventional systems use source and channel coding to ensure reliability which are often unavailable in LPWAN networks. For noisy aggregation, as an alternative to the conventional pipeline, we assume uncoded transmission. This scheme bypasses the transformation of the model to a sequence of bits, which then need to be mapped again to complex-valued channel inputs. Instead, the real model parameter values are directly mapped to the complex-valued samples transmitted over the channel. Leveraging the properties of uncoded transmission, we can treat the channel as formulated in Equation (2), where the additive noise is directly applied to model parameters. The channel output received by the server for client at round is given byC where n \u223c N (0, 2 , ) is the \u00d7 -dimensional additive noise. Then, the signal-to-noise ratio (SNR) is:\n= C + n(2), = E\u2225C \u2225 2 E\u2225n \u2225 2 = , 2 ,(3)\nAn immediate result of federated bundling is the improvement in the SNR for the global model. When the class hypervectors from different clients are bundled at the server, the signal power scales up quadratically with the number of clients , whereas the noise power scales linearly. Assuming that the noise for each client is independent, we have the following relation:\n= E\u2225 =1 C \u2225 E\u2225 =1 n \u2225 \u2248 2 , 2 , = \u00d7 ,(4)\nNotice that the effect of noise is suppressed by times due to bundling. This claim can also be made for the FedAvg framework over CNNs. However, even though the noise reduction factor is the same, the impact of the small noise might be amplified by large activations of CNN layers. In FHDnn, we do not have such a problem as the inference and training operations are purely linear. One other difference of FHDnn from CNNs is its information dispersal property. HD encoding produces hypervectors which have holographic representations, meaning that the information content is spread over all the dimensions of the high-dimensional space. Since the noise in each dimension can also be assumed to be independent, we can leverage the information spread to further eliminate noise.\n\nConsider the random projection encoding described in Section 3.3. Let the encoding matrix \u03a6 \u2208 R \u00d7 expressed in terms of its row vectors, i.e., \u03a6 = [\u03a6 1 , \u03a6 2 , ..., \u03a6 ] . Then, the hypervector formed by encoding information \u2208 X can be written as h = [\u03a6 1 , \u03a6 2 , ..., \u03a6 ] , where = [ 1 , 2 , ..., ] . As implied by this expression, the information is dispersed over the hypervectors uniformly. Now consider additive noise over the same hypervector such that h + n = [\u03a6 1 + 1 , \u03a6 2 + 2 , ..., \u03a6 + ] . We can reconstruct the encoded information from the noisy hypervectorh = h + n as follows:\n\u2248 1 \u2211\ufe01 =1 \u03a6 ,1h , 1 \u2211\ufe01 =1 \u03a6 ,2h , ..., 1 \u2211\ufe01 =1 \u03a6 ,h(5)\nwhereh = \u03a6 + are the elements of the noisy hypervector. The noise variance is then reduced by the averaging operation, similar to the case in Equation (4). Therefore, in HD computing, the noise is not only suppressed by bundling accross models from different clients, but also by averaging over the dimensions within the same hypervector. We demonstrate this over an example where we encode a sample from the MNIST dataset, add Gaussian noise, then reconstruct it. Figure 4 shows the original image, noisy image in the sample space, and reconstructed image for which the noise was added in the hyperdimensional space.\n\n\nBit Errors.\n\nWe use bit error rate (BER) in conventional coded transmission as a figure of merit for system robustness. It is a measure on how accurately the receiver is able to decode transmitted data. The errors are bit flips in the received digital symbols, and are simply evaluated by the difference (usually Hamming distance) between the input bitstream of channel encoder and the output bitstream of channel decoder. Let\u0109 be the binary coded model parameters that are communicated to the server. For the bit error model, we treat the channel as a binary symmetric channel (BSC), which independently flips each bit in\u0109 with probability (e.g., 0 \u2192 1). The received bitstream output at the server for client at round is then as follows:\nC =\u0108 \u2295 e (6)\nwhere e is the binary error vector and \u2295 denotes modulo 2 addition. Given a specific vector v of Hamming weight wt(v), the probability that e = v is given by\nP(e = v) = wt(v) (1 \u2212 ) \u2212wt(v)(7)\nThe bit error probability, , is a function of both the modulation scheme and the channel coding technique (assuming lossless source coding). To conclude the transmission, the corrupted bitstream in (6) is finally reconstructed to a real-valued model, i.e.,C \u2192C .\n\nBit errors can have a detrimental effect on the training accuracy, especially for CNNs. At worst case, a single bit error in one client in one round can fail the whole training. We give an example of how much difference a single bit error can make for the standard 32 bit floating point CNN weights. In floating point notation, a number consists of three parts: a sign bit, an exponent, and a fractional value. In IEEE 754 floating point representation, the sign bit is the most significant bit, bits 31 to 24 hold the exponent value, and the remaining bits contain the fractional value. The exponent bits represent a power of two ranging from -127 to 128. The fractional bits store a value between 1 and 2, which is multiplied by 2 to give the decimal value. Our example shows that one bit error in the exponent can change the weight value from 0.15625 to 5.31 \u00d7 10 37 .\n\nThe bit errors are contagious because a parameter from one client gets aggregated to the global model, then communicated back to all clients. Furthermore, errors propagate through all communication rounds because local training or aggregation does not completely change the parameter value, but only apply small decrements. For instance, assume a federated learning scenario with 100 clients and one bit error in a client's model as in the above example. After 10 rounds of training, the CNN weight for the global model will be on the order of \u223c 5.31\u00d710 37 100 10 = 5.31 \u00d7 10 17 , still completely failing the whole model. Consider ResNet-50, which has 20 million parameters, so training 100 clients even over a channel with = 10 \u22129 BER results in two errors per round on average, making model failure inevitable.\n\nA similar problem exists with HD model parameters, but to a lesser extent because the class prototypes use integer representations. Particularly, errors in the most significant bits (MSB) of integer representation leads to higher accuracy drop. We propose a quantizer solution to prevent this. Inspired by the classical quantization methods in communication systems, we leverage scaling up and scaling down operations at the transmitter and the receiver respectively. This can be implemented by the automatic gain control (AGC) module in the wireless circuits. For a class hypervector c , \u2208 {1, ..., }, the quantizer output (c ) can be obtained via the following steps: (1) Scale Up: Each dimension in the class hypervector, i.e. , , is amplified with a scaling factor denoted quantization gain . We adjust the gain such that the dimension with the largest absolute value attains the maximum value attainable by the integer representation. Thus, = 2 \u22121 \u22121 max( ) where is the bitwidth.  This way, bit errors are applied to the scaled up values. Intuitively, we limit the impact of the bit error on the models. Remember, from Section 3.4.1, that prediction is realized by a normalized dot-product between the encoded query and class hypervectors. Therefore, the ratio between the original parameter and the received (corrupted) parameter determines the impact of the error on the dot-product. Without our quantizer, this ratio can be very large whereas after scaling up then later down, it is diminished.\n\n\nPacket Loss.\n\nAt the physical layer of the network stack, errors are observed in the form of additive noise or bit flips directly on the transmitted data. On the other hand, at the network and transport layers, packet losses are introduced. The combination of network and protocol specifications allows us to describe the error characteristics, with which the data transmission process has to cope.\n\nThe form of allowed errors, either bit errors or packet losses, are decided by the error control mechanism. For the previous error model, we assumed that the bit errors are admitted to propagate through the transport hierarchy. This assumption is valid for a family of protocols used in error resilient applications that can cope with such bit errors. In some protocols, the reaction of the system to any number of bit errors is to drop the corrupted packets. These protocols employ a cyclic redundancy check (CRC) or a checksum that allows the detection of bit errors. In such a case, the communication could assume bit-error free, but packet lossy link. We use the packet error rate (PER) metric as a performance measure, whose expectation is denoted packet error probability . For a packet length of bits, this probability can be expressed as:\n= 1 \u2212 (1 \u2212 ) (8)\nThe common solution for dealing with packet losses and guarantee successful delivery is to use a reliable transport layer communication protocol, e.g., transmission control protocol (TCP), where various mechanisms including acknowledgment messages, retransmissions, and time-outs are employed. To detect and recover from transmission failures, these mechanisms incur considerable communication overhead. Therefore, for our setup we adopt user datagram protocol (UDP), another widely used transport layer protocol. UDP is unreliable and cannot guarantee packet delivery, but is low-latency and has much less overhead compared to TCP.\n\nHD computing's information dispersal and holographic representation properties are also beneficial for packet losses. Another direct result of these concepts is obtaining partial information on data from any part of the encoded information. The intuition is that any portion of holographic coded information represents a blurred image of the entire data. Then, each transmitted symbol -packets in our casecontains an encoded image of the entire model.\n\nWe demonstrate the property of obtaining partial information as an example using a speech recognition dataset [1]. In Figure 5 Figure 5(b) further clarifies our observation. We compare the dot-product values across all classes and find the class hypervector with the highest similarity. Only the relative dot-product values are important for classification. So, it is enough to have the highest dot-product value for the correct class, which holds true with \u223c 90% accuracy even when 80% of the hypervector dimensions are removed.\n\n\nConvergence Performance of FHDnn\n\nIt is commonly preferred to have smooth, strongly-convex, differentiable models to maintain a provable, analytically tractable convergence analysis for federated learning. Such models also provide faster convergence properties than the others. The learning computations in FHDnn are linear, demand low-complexity operations, and thus are favourable for resource-constrained, low-power client devices. However, in many learning tasks, linear federated learning models perform sub-optimally compared to their counterpart, CNN-based approaches. FHDnn diverges from traditional linear methods in this respect. It enjoys both the superior performance properties of nonlinear models and low computational complexity of linear models. This is a direct result of HD computing, which embeds data into a high-dimensional space where the geometry is such that simple learning methods are effective. The linearity in HD training benefits convergence, and at the same time the performance does not degrade due to the properties of non-linear hyperdimensional embeddings.\n\nFHDnn, when posed as a distributed optimization solution, has the following properties: L-smoothness, strong convexity, bounded variance, and uniformly bounded gradient. It was shown previously that the methods which satisfy these conditions converge to the global optimum solution of the learning task at a rate of O ( 1 ) [14]. Such claims cannot be made for CNNs due to non-convexity and non-linearity.\n\n\nExperimental Analysis\n\nWe demonstrate through systematic experiments the performance of FHDnn under various settings. We briefly discuss the datasets and setup for evaluating FHDnn before presenting results for FHDnn for various data distributions for reliable communication. We also compare the resource usage of FHDnn against CNNs. Finally we show evaluation for various unreliable network scenarios.\n\n\nDataset and Platforms\n\nWe evaluate FHDnn on 3 different real world datasets: MNIST [7], FashionMNIST [21], CIFAR10 [13]. For performance evaluation we test FHDnn on Raspberry Pi Model 3b and NVIDIA Jetson mobile GPU. We use python and PyTorch for implementing all models. For MNIST, we use a simple network consisting of 2 convolution layers and 2 fully connected layers. For CIFAR10 and FashionMNIST we use the ResNet-18 model [8].\n\n\nAccuracy and Compute\n\nWe first tune the hyperparameters for both FHDnn and CNNs and analyze their impact by experimenting with , , where is   Figure 7 compares the accuracy of FHDnn with ResNet on 3 different datasets for 100 rounds of communication. We observe that for the same number of rounds, FHDnn achieves almost the same accuracy as ResNet and converges faster. Figure 6 shows the smoothed conditional mean across all different hyperparameters for both the models for iid and non-iid distributions. FHDnn reaches an accuracy of 82% in less than 25 rounds of communication whereas ResNet takes 75 rounds for both iid and non-iid data distributions. Moreover the hyperparameters do not have a big influence for FHDnn as seen by the narrow spread (gray region) in Figure 6. Note that the local batch size doesn't impact FHDnn due to the nature of its training methodology. This allows us to use higher batch sizes up to the constraints of the device, allowing for faster processing whereas affects the convergence of CNNs.\n\n\nCompute\n\nResources. The most computationally expensive operation on the client is training. CNN training involves backpropagation for each round which is very expensive. HD on the other hand is very lightweight as featured in Table 1 which quantitatively compares the computation time of FHDnn and ResNet on 2 edge devices for client training. FHDnn is 35% faster and energy efficient than ResNet on RPi and 80% faster and energy efficient on the Jetson mobile GPU.\n\n\nUnreliable Communication\n\nIn this section we analyze the performance of FHDnn and ResNet under unreliable network conditions as described in Section 3.5. Figure 8 shows the performance of models under each of these network conditions. In order to maintain a direct comparison between CNN and FHDnn we use the same hyperparameters for both models and all experiments. We use = 2, = 0.2, = 10 and evaluate the performance on the CIFAR10 dataset. From our experiments we observe that even with fewer clients = 0.1 and for other datasets, the performance of FHDnn is better than ResNet. Due to page constraints we present only the results for the settings mentioned earlier.\n\n\nPacket Loss.\n\nWhen the packet loss rate is extremely small, below 1 \u22122 , ResNet has very minimal accuracy loss. However for more realistic packet loss rates such as 20% the CNN model fails to converge. A 20% packet loss rate implies 20% of the weights are zero. Moreover, this loss is accumulative as the models are averaged during each round of communication thereby giving the CNNs no chance of recovery. In contrast, FHDnn is highly robust to packet loss with almost no loss in accuracy. For FHDnn, since the data is distributed uniformly across the entire hypervector, a small amount of missing data is tolerable. However, since CNNs have a more structured representation of data with interconnections between neurons, the loss of weights affects the performance of subsequent layers which is detrimental to its performance.\n\n\nGaussian Noise.\n\nWe experiment with different Signal-to-Noise Ratios (SNR) to simulate noisy links. Even for lower SNRs like 25dB the accuracy of ResNet drops by 8%. However it's more likely that IoT networks operating on low power wireless networks will incur higher SNRs. For such scenarios FHDnn outperforms ResNet as the latter fails to perform better than random. The accuracy of FHDnn only reduces by 3% which is negligible compared to ResNet. Figure 8 shows that CNNs achieve the equivalent of random accuracy even for small bit errors. Since the weights of CNNs are floating point, a single bit flip can significantly change the value of the weights. This compounded with federated averaging hinders convergence. We observe FHDnn incurs an accuracy loss as well, achieving 72% for iid and 69% for non-iid. FHDnn uses an integer representation which is again susceptible to large changes from bit errors. However, our scaling method described in Section 3.5.2 assuages the error to some extent.\n\n\nBit Errors.\n\n\nCommunication Efficiency\n\nSo far we have benchmarked the accuracy of FHDnn for various network conditions. In this section we demonstrate the communication efficiency of FHDnn compared to ResNet.\n\nWe compare the amount of data transmitted for federated learning to reach a target accuracy of 80%. We calculate the amount of data transmitted by one client using the formula = \u00d7 , where is the number of rounds required for convergence by each model. The update size for ResNet with 11M parameters is 22MB while that of FHDnn is 1MB making it 22\u00d7 smaller. From Section 4.2.1 we know that FHDnn converges 3\u00d7 faster than ResNet bringing its total communication cost to 25MB. ResNet on the other hand uses up 1.65GB of data to reach the target accuracy.\n\nIn Figure 6, we illustrated that FHDnn can converge to the optimal accuracy in much fewer communication rounds. However, this improvement is even more in terms of the actual clock time of training. We assume that federated learning takes places over LTE networks where SNR is 5dB for the wireless channel. Each client occupies 1 LTE frame of 5MHz bandwidth and duration 10ms in a time division duplexing manner. For error-free communication, the traditional FL system using ResNet can support up to 1.6 Mbits/sec data rate, whereas we admit errors and communicate at a rate of 5.0 Mbits/sec. Under this setting and for the same experiment as in Section 4.2, FHDnn converges in 1.1 hours for CIFAR IID and 3.3 hours for CIFAR Non-IID on average. On the other hand, ResNet converges in 374.3 hours for both CIFAR IID and CIFAR Non-IID on average.\n\n\nConclusion\n\nIn this work, we presented FHDnn a federated learning framework combining Deep Learning and Hyperdimensional Computing to improve the communication efficiency and reduce the computation costs on edge devices. We detail the architecture, training methodology and evaluate FHDnn through numerous experiments in both reliable and unreliable communication settings and compare its performance with standard CNN. Our experiments show that FHDnn is 66\u00d7 more communication efficient and lowers client computation costs by 6\u00d7 while being robust to network errors.\n\nFigure 2 :\n2FHDnn Model Architecture\n\nFigure 2\n2shows the model architecture of FHDnn. In the following subsections we detail the 2 components of FHDnn: i) a pre-trained CNN as a feature extractor, ii) a federated HD learner, followed by the training methodology.\n\nFigure 3 :\n3FHDnn Federated Training\n\nFigure 4 :\n4Noise robustness of hyperdimensional encodings\n\n( 2 )\n2Rounding: The scaled up values are truncated to only retain their integer part. (3) Scale Down: The receiver output is obtained by scaling down with the same factor .\n\nFigure 5 :\n5Impact of partial information on similarity check (left) and classification accuracy (right)\n\n\n(a), after training the model, we increasingly remove the dimensions of a certain class hypervector in a random fashion. Then we perform a similarity check to figure out what portion of the original dotproduct value is retrieved. The same figure shows that the amount of information retained scales linearly with number of remaining\n\nFigure 6 :\n6Accuracy and Number of communication rounds for various hyperparameters dimensions.\n\nFigure 7 :\n7Accuracy of FHDnn and ResNet on different datasets\n\nFigure 8 :\n8Accuracy comparison of FHDnn with ResNet under unreliable network conditions\n\n\nWe show that our approach reduces the communication costs by 66\u00d7, and reduces the local computations cost on the clients by up to 6\u00d7 while being robust to lossy transmissions.\u2022 We empirically show through numerous experiments that FHDnn \nis robust to lossy network conditions. Specifically we evaluate \nFHDnn under three different unreliable network settings: packet \nloss, noise injection, and bit errors. \n\u2022 \n\nTable 1 :\n1Performance on Edge DevicesDevice \nTraining Time (Sec) \nEnergy (J) \nFHDnn \nResNet \nFHDnn ResNet \nRaspberry Pi 858.72 \n1328.04 \n4418.4 \n6742.8 \nNvidia Jetson \n15.96 \n90.55 \n96.17 \n497.572 \n\nthe number of local epochs, the local batch size and the fraction \nof clients participating in each round. For all experiments we use \n100 clients and 100 rounds of communication in order to keep our \nexperiments tractable. We select the best parameters for ResNet and \nuse the same for FHDnn for all experiments in order to allow for a \ndirect comparison. \n\n4.2.1 Accuracy. \nThis work is licensed under a Creative Commons Attribution International 4.0 License.\nAcknowledgementsThis work was supported in part by CRISP, one of six centers in JUMP (an SRC program sponsored by DARPA), SRC Global Research Collaboration (GRC) grant, and NSF grants #1911095, #1826967, #2100237, and #2112167.\nUCI machine learning repository. n. d.[n. d.]. UCI machine learning repository. http://archive.ics.uci.edu/ml/datasets/ ISOLET.\n\nReal-time video analytics: The killer app for edge computing. Ganesh Ananthanarayanan, computer. 50Ganesh Ananthanarayanan et al. 2017. Real-time video analytics: The killer app for edge computing. computer 50, 10 (2017), 58-67.\n\nThe robustness of modern deep learning architectures against single event upset errors. P Austin, Arechiga, 2018 IEEE High Performance extreme Computing Conference (HPEC). IEEEAustin P Arechiga et al. 2018. The robustness of modern deep learning architectures against single event upset errors. In 2018 IEEE High Performance extreme Computing Conference (HPEC). IEEE, 1-6.\n\nAdaptive Federated Dropout: Improving Communication Efficiency and Generalization for Federated Learning. Nader Bouacida, INFOCOM WKSHPS. IEEE. Nader Bouacida et al. 2021. Adaptive Federated Dropout: Improving Communica- tion Efficiency and Generalization for Federated Learning. In INFOCOM WKSHPS. IEEE, 1-6.\n\nExpanding the Reach of Federated Learning by Reducing Client Resource Requirements. Sebastian Caldas, arXiv:1812.07210Sebastian Caldas et al. 2019. Expanding the Reach of Federated Learning by Reduc- ing Client Resource Requirements. arXiv:1812.07210 (Jan 2019).\n\nA simple framework for contrastive learning of visual representations. Ting Chen, PMLRInternational conference on machine learning. Ting Chen et al. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597-1607.\n\nThe mnist database of handwritten digit images for machine learning research. Li Deng, IEEE Signal Processing Magazine. 29Li Deng. 2012. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141-142.\n\nDeep residual learning for image recognition. Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He et al. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.\n\nStarfish: Resilient Image Compression for AIoT Cameras. Pan Hu, ACMNew York, NY, USAPan Hu et al. 2020. Starfish: Resilient Image Compression for AIoT Cameras. ACM, New York, NY, USA, 395-408.\n\nBric: Locality-based encoding for energy-efficient braininspired hyperdimensional computing. Mohsen Imani, Proceedings of the 56th Annual Design Automation Conference. the 56th Annual Design Automation Conferencen. d.Mohsen Imani et al. [n. d.]. Bric: Locality-based encoding for energy-efficient brain- inspired hyperdimensional computing. In Proceedings of the 56th Annual Design Automation Conference 2019.\n\nHyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors. Pentti Kanerva, Cognitive Computation. 12Pentti Kanerva. 2009. Hyperdimensional Computing: An Introduction to Com- puting in Distributed Representation with High-Dimensional Random Vectors. Cognitive Computation 1, 2 (Jun 2009), 139-159.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive computation. 1Pentti Kanerva. 2009. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive computation 1, 2 (2009), 139-159.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).\n\nXiang Li, arXiv:1907.02189On the convergence of fedavg on non-iid data. arXiv preprintXiang Li et al. 2019. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189 (2019).\n\nVideo analytics towards vision zero. Institute of Transportation Engineers. Franz Loewenherz, ITE Journal. 8725Franz Loewenherz et al. 2017. Video analytics towards vision zero. Institute of Transportation Engineers. ITE Journal 87, 3 (2017), 25.\n\nDaRe: Data Recovery through Application Layer Coding for LoRaWAN. Paul J Marcelis, IoTDI. Paul J. Marcelis et al. 2017. DaRe: Data Recovery through Application Layer Coding for LoRaWAN. In IoTDI. 97-108.\n\nCommunication-Efficient Learning of Deep Networks from Decentralized Data. H Brendan Mcmahan, 10H Brendan McMahan et al. 2016. Communication-Efficient Learning of Deep Networks from Decentralized Data. (2016), 10.\n\nThe emerging landscape of edge computing. A Shadi, Noghabi, GetMobile: Mobile Computing and Communications. 23Shadi A Noghabi et al. 2020. The emerging landscape of edge computing. GetMobile: Mobile Computing and Communications 23, 4 (2020), 11-20.\n\nEvaluation of LoRa LPWAN technology for remote health and wellbeing monitoring. Juha Pet\u00e4j\u00e4j\u00e4rvi, ISMICT. Juha Pet\u00e4j\u00e4j\u00e4rvi et al. 2016. Evaluation of LoRa LPWAN technology for remote health and wellbeing monitoring. In ISMICT.\n\nCombating packet collisions using non-stationary signal scaling in LPWANs. Shuai Tong, Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services. the 18th International Conference on Mobile Systems, Applications, and ServicesShuai Tong et al. 2020. Combating packet collisions using non-stationary signal scaling in LPWANs. In Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services. 234-246.\n\nHan Xiao, arXiv:cs.LG/1708.07747 [cs.LG]Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. Han Xiao et al. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:cs.LG/1708.07747 [cs.LG]\n", "annotations": {"author": "[{\"end\":185,\"start\":82},{\"end\":291,\"start\":186},{\"end\":380,\"start\":292},{\"end\":493,\"start\":381},{\"end\":601,\"start\":494},{\"end\":709,\"start\":602}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":93},{\"end\":197,\"start\":192},{\"end\":302,\"start\":299},{\"end\":397,\"start\":389},{\"end\":507,\"start\":503},{\"end\":615,\"start\":609}]", "author_first_name": "[{\"end\":92,\"start\":82},{\"end\":191,\"start\":186},{\"end\":298,\"start\":292},{\"end\":388,\"start\":381},{\"end\":502,\"start\":494},{\"end\":608,\"start\":602}]", "author_affiliation": "[{\"end\":184,\"start\":109},{\"end\":290,\"start\":215},{\"end\":379,\"start\":304},{\"end\":492,\"start\":417},{\"end\":600,\"start\":525},{\"end\":708,\"start\":633}]", "title": "[{\"end\":79,\"start\":1},{\"end\":788,\"start\":710}]", "venue": null, "abstract": "[{\"end\":1900,\"start\":926}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2185,\"start\":2181},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2189,\"start\":2186},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2217,\"start\":2213},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2542,\"start\":2538},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2996,\"start\":2993},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2998,\"start\":2996},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3743,\"start\":3739},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4703,\"start\":4700},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5894,\"start\":5890},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5897,\"start\":5894},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5911,\"start\":5907},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6063,\"start\":6060},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6792,\"start\":6789},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7411,\"start\":7408},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7763,\"start\":7760},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8426,\"start\":8422},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8784,\"start\":8780},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22869,\"start\":22866},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24709,\"start\":24705},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25280,\"start\":25277},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25299,\"start\":25295},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25313,\"start\":25309},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25625,\"start\":25622}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31851,\"start\":31814},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32078,\"start\":31852},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32116,\"start\":32079},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32176,\"start\":32117},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32351,\"start\":32177},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32457,\"start\":32352},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32792,\"start\":32458},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32889,\"start\":32793},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32953,\"start\":32890},{\"attributes\":{\"id\":\"fig_9\"},\"end\":33043,\"start\":32954},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33456,\"start\":33044},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34033,\"start\":33457}]", "paragraph": "[{\"end\":3573,\"start\":1916},{\"end\":4457,\"start\":3575},{\"end\":5447,\"start\":4459},{\"end\":6172,\"start\":5494},{\"end\":7208,\"start\":6193},{\"end\":8317,\"start\":7230},{\"end\":8672,\"start\":8332},{\"end\":10205,\"start\":8674},{\"end\":10511,\"start\":10250},{\"end\":10862,\"start\":10513},{\"end\":11020,\"start\":10886},{\"end\":11520,\"start\":11063},{\"end\":11894,\"start\":11522},{\"end\":12314,\"start\":11896},{\"end\":12639,\"start\":12316},{\"end\":13504,\"start\":12641},{\"end\":13916,\"start\":13546},{\"end\":14734,\"start\":13958},{\"end\":15326,\"start\":14736},{\"end\":15999,\"start\":15382},{\"end\":16741,\"start\":16015},{\"end\":16912,\"start\":16755},{\"end\":17209,\"start\":16947},{\"end\":18082,\"start\":17211},{\"end\":18897,\"start\":18084},{\"end\":20402,\"start\":18899},{\"end\":20803,\"start\":20419},{\"end\":21651,\"start\":20805},{\"end\":22301,\"start\":21669},{\"end\":22754,\"start\":22303},{\"end\":23285,\"start\":22756},{\"end\":24379,\"start\":23322},{\"end\":24786,\"start\":24381},{\"end\":25191,\"start\":24812},{\"end\":25626,\"start\":25217},{\"end\":26656,\"start\":25651},{\"end\":27124,\"start\":26668},{\"end\":27797,\"start\":27153},{\"end\":28628,\"start\":27814},{\"end\":29632,\"start\":28648},{\"end\":29844,\"start\":29675},{\"end\":30397,\"start\":29846},{\"end\":31243,\"start\":30399},{\"end\":31813,\"start\":31258}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10885,\"start\":10863},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13515,\"start\":13505},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13545,\"start\":13515},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13957,\"start\":13917},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15381,\"start\":15327},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16754,\"start\":16742},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16946,\"start\":16913},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21668,\"start\":21652}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26892,\"start\":26885}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1914,\"start\":1902},{\"attributes\":{\"n\":\"2\"},\"end\":5492,\"start\":5450},{\"attributes\":{\"n\":\"2.2\"},\"end\":6191,\"start\":6175},{\"attributes\":{\"n\":\"3.2\"},\"end\":7228,\"start\":7211},{\"attributes\":{\"n\":\"3.3\"},\"end\":8330,\"start\":8320},{\"attributes\":{\"n\":\"3.4\"},\"end\":10226,\"start\":10208},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":10248,\"start\":10229},{\"attributes\":{\"n\":\"3.5\"},\"end\":11061,\"start\":11023},{\"attributes\":{\"n\":\"3.5.2\"},\"end\":16013,\"start\":16002},{\"attributes\":{\"n\":\"3.5.3\"},\"end\":20417,\"start\":20405},{\"attributes\":{\"n\":\"3.6\"},\"end\":23320,\"start\":23288},{\"attributes\":{\"n\":\"4\"},\"end\":24810,\"start\":24789},{\"attributes\":{\"n\":\"4.1\"},\"end\":25215,\"start\":25194},{\"attributes\":{\"n\":\"4.2\"},\"end\":25649,\"start\":25629},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":26666,\"start\":26659},{\"attributes\":{\"n\":\"4.3\"},\"end\":27151,\"start\":27127},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":27812,\"start\":27800},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":28646,\"start\":28631},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":29646,\"start\":29635},{\"attributes\":{\"n\":\"4.4\"},\"end\":29673,\"start\":29649},{\"attributes\":{\"n\":\"5\"},\"end\":31256,\"start\":31246},{\"end\":31825,\"start\":31815},{\"end\":31861,\"start\":31853},{\"end\":32090,\"start\":32080},{\"end\":32128,\"start\":32118},{\"end\":32183,\"start\":32178},{\"end\":32363,\"start\":32353},{\"end\":32804,\"start\":32794},{\"end\":32901,\"start\":32891},{\"end\":32965,\"start\":32955},{\"end\":33467,\"start\":33458}]", "table": "[{\"end\":33456,\"start\":33221},{\"end\":34033,\"start\":33496}]", "figure_caption": "[{\"end\":31851,\"start\":31827},{\"end\":32078,\"start\":31863},{\"end\":32116,\"start\":32092},{\"end\":32176,\"start\":32130},{\"end\":32351,\"start\":32185},{\"end\":32457,\"start\":32365},{\"end\":32792,\"start\":32460},{\"end\":32889,\"start\":32806},{\"end\":32953,\"start\":32903},{\"end\":33043,\"start\":32967},{\"end\":33221,\"start\":33046},{\"end\":33496,\"start\":33469}]", "figure_ref": "[{\"end\":2820,\"start\":2812},{\"end\":5123,\"start\":5115},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9371,\"start\":9363},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10093,\"start\":10085},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15855,\"start\":15847},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22882,\"start\":22874},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22891,\"start\":22883},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25779,\"start\":25771},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26007,\"start\":25999},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26406,\"start\":26398},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27289,\"start\":27281},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":29089,\"start\":29081},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":30410,\"start\":30402}]", "bib_author_first_name": "[{\"end\":34545,\"start\":34539},{\"end\":34796,\"start\":34795},{\"end\":35192,\"start\":35187},{\"end\":35485,\"start\":35476},{\"end\":35731,\"start\":35727},{\"end\":36029,\"start\":36027},{\"end\":36273,\"start\":36266},{\"end\":36640,\"start\":36637},{\"end\":36874,\"start\":36868},{\"end\":37317,\"start\":37311},{\"end\":37681,\"start\":37675},{\"end\":37966,\"start\":37962},{\"end\":37987,\"start\":37979},{\"end\":38112,\"start\":38107},{\"end\":38383,\"start\":38378},{\"end\":38620,\"start\":38616},{\"end\":38622,\"start\":38621},{\"end\":39013,\"start\":39012},{\"end\":39304,\"start\":39300},{\"end\":39528,\"start\":39523},{\"end\":39921,\"start\":39918}]", "bib_author_last_name": "[{\"end\":34562,\"start\":34546},{\"end\":34803,\"start\":34797},{\"end\":34813,\"start\":34805},{\"end\":35201,\"start\":35193},{\"end\":35492,\"start\":35486},{\"end\":35736,\"start\":35732},{\"end\":36034,\"start\":36030},{\"end\":36276,\"start\":36274},{\"end\":36643,\"start\":36641},{\"end\":36880,\"start\":36875},{\"end\":37325,\"start\":37318},{\"end\":37689,\"start\":37682},{\"end\":37977,\"start\":37967},{\"end\":37994,\"start\":37988},{\"end\":38115,\"start\":38113},{\"end\":38394,\"start\":38384},{\"end\":38631,\"start\":38623},{\"end\":38847,\"start\":38830},{\"end\":39019,\"start\":39014},{\"end\":39028,\"start\":39021},{\"end\":39316,\"start\":39305},{\"end\":39533,\"start\":39529},{\"end\":39926,\"start\":39922}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34475,\"start\":34348},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206449115},\"end\":34705,\"start\":34477},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":54212210},\"end\":35079,\"start\":34707},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":226281401},\"end\":35390,\"start\":35081},{\"attributes\":{\"doi\":\"arXiv:1812.07210\",\"id\":\"b4\"},\"end\":35654,\"start\":35392},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b5\",\"matched_paper_id\":211096730},\"end\":35947,\"start\":35656},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5280072},\"end\":36218,\"start\":35949},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":36579,\"start\":36220},{\"attributes\":{\"id\":\"b8\"},\"end\":36773,\"start\":36581},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":163164623},\"end\":37184,\"start\":36775},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":733980},\"end\":37548,\"start\":37186},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":733980},\"end\":37905,\"start\":37550},{\"attributes\":{\"id\":\"b12\"},\"end\":38105,\"start\":37907},{\"attributes\":{\"doi\":\"arXiv:1907.02189\",\"id\":\"b13\"},\"end\":38300,\"start\":38107},{\"attributes\":{\"id\":\"b14\"},\"end\":38548,\"start\":38302},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17575687},\"end\":38753,\"start\":38550},{\"attributes\":{\"id\":\"b16\"},\"end\":38968,\"start\":38755},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":213450004},\"end\":39218,\"start\":38970},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":17784429},\"end\":39446,\"start\":39220},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":219397373},\"end\":39916,\"start\":39448},{\"attributes\":{\"doi\":\"arXiv:cs.LG/1708.07747 [cs.LG]\",\"id\":\"b20\"},\"end\":40176,\"start\":39918}]", "bib_title": "[{\"end\":34537,\"start\":34477},{\"end\":34793,\"start\":34707},{\"end\":35185,\"start\":35081},{\"end\":35725,\"start\":35656},{\"end\":36025,\"start\":35949},{\"end\":36264,\"start\":36220},{\"end\":36866,\"start\":36775},{\"end\":37309,\"start\":37186},{\"end\":37673,\"start\":37550},{\"end\":38376,\"start\":38302},{\"end\":38614,\"start\":38550},{\"end\":39010,\"start\":38970},{\"end\":39298,\"start\":39220},{\"end\":39521,\"start\":39448}]", "bib_author": "[{\"end\":34564,\"start\":34539},{\"end\":34805,\"start\":34795},{\"end\":34815,\"start\":34805},{\"end\":35203,\"start\":35187},{\"end\":35494,\"start\":35476},{\"end\":35738,\"start\":35727},{\"end\":36036,\"start\":36027},{\"end\":36278,\"start\":36266},{\"end\":36645,\"start\":36637},{\"end\":36882,\"start\":36868},{\"end\":37327,\"start\":37311},{\"end\":37691,\"start\":37675},{\"end\":37979,\"start\":37962},{\"end\":37996,\"start\":37979},{\"end\":38117,\"start\":38107},{\"end\":38396,\"start\":38378},{\"end\":38633,\"start\":38616},{\"end\":38849,\"start\":38830},{\"end\":39021,\"start\":39012},{\"end\":39030,\"start\":39021},{\"end\":39318,\"start\":39300},{\"end\":39535,\"start\":39523},{\"end\":39928,\"start\":39918}]", "bib_venue": "[{\"end\":36419,\"start\":36357},{\"end\":36987,\"start\":36943},{\"end\":39710,\"start\":39631},{\"end\":34379,\"start\":34348},{\"end\":34572,\"start\":34564},{\"end\":34877,\"start\":34815},{\"end\":35223,\"start\":35203},{\"end\":35474,\"start\":35392},{\"end\":35786,\"start\":35742},{\"end\":36067,\"start\":36036},{\"end\":36355,\"start\":36278},{\"end\":36635,\"start\":36581},{\"end\":36941,\"start\":36882},{\"end\":37348,\"start\":37327},{\"end\":37712,\"start\":37691},{\"end\":37960,\"start\":37907},{\"end\":38177,\"start\":38133},{\"end\":38407,\"start\":38396},{\"end\":38638,\"start\":38633},{\"end\":38828,\"start\":38755},{\"end\":39076,\"start\":39030},{\"end\":39324,\"start\":39318},{\"end\":39629,\"start\":39535},{\"end\":40039,\"start\":39958}]"}}}, "year": 2023, "month": 12, "day": 17}