{"id": 258107969, "updated": "2023-10-05 02:01:31.523", "metadata": {"title": "Label-Free Concept Bottleneck Models", "authors": "[{\"first\":\"Tuomas\",\"last\":\"Oikarinen\",\"middle\":[]},{\"first\":\"Subhro\",\"last\":\"Das\",\"middle\":[]},{\"first\":\"Lam\",\"last\":\"Nguyen\",\"middle\":[\"M.\"]},{\"first\":\"Tsui-Wei\",\"last\":\"Weng\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automated - training it for a new dataset requires minimal human effort. Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM. Finally, in Appendix B we conduct a large scale user evaluation of the interpretability of our method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.06129", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/OikarinenDNW23", "doi": "10.48550/arxiv.2304.06129"}}, "content": {"source": {"pdf_hash": "1d603b03bb083a2c3e2e3b6f116cf196d637e6b6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.06129v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b3ea84d0fd4bd9ff5ee14cc543ff57509160558e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1d603b03bb083a2c3e2e3b6f116cf196d637e6b6.txt", "contents": "\nLABEL-FREE CONCEPT BOTTLENECK MODELS\n\n\nTuomas Oikarinen toikarinen@ucsd.edu \nMIT-IBM Watson AI Lab, IBM Research\nUCSD CSE\nIBM Research\nUCSD HDSI\n\n\nSubhro Das subhro.das@ibm.com \nMIT-IBM Watson AI Lab, IBM Research\nUCSD CSE\nIBM Research\nUCSD HDSI\n\n\nLam M Nguyen lamnguyen.mltd@ibm.com \nMIT-IBM Watson AI Lab, IBM Research\nUCSD CSE\nIBM Research\nUCSD HDSI\n\n\nTsui-Wei Weng lweng@ucsd.edu \nMIT-IBM Watson AI Lab, IBM Research\nUCSD CSE\nIBM Research\nUCSD HDSI\n\n\nLABEL-FREE CONCEPT BOTTLENECK MODELS\nPublished as a conference paper at ICLR 2023\nConcept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to humanunderstandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable -we present the first CBM scaled to ImageNet, efficient -creating a CBM takes only a few hours even for very large datasets, and automated -training it for a new dataset requires minimal human effort. Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM. Finally, in Appendix B we conduct a large scale user evaluation of the interpretability of our method.\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have demonstrated unprecedented success in a wide range of machine learning tasks such as computer vision, natural language processing, and speech recognition. However, due to their complex and deep structures, they are often regarded as black-box models that are difficult to understand and interpret. Interpretable models are important for many reasons such as creating calibrated trust in models, which means understanding when we should trust the models. Making deep learning models more interpretable is an active yet challenging research topic. Published as a conference paper at ICLR 2023 One approach to make deep learning more interpretable is through Concept Bottleneck Models (CBMs) (Koh et al., 2020). CBMs typically have a Concept Bottleneck Layer before the (last) fully connected layer of the neural network. The concept bottleneck layer is trained to have each neuron correspond to a single human understandable concept. This makes the final decision a linear function of interpretable concepts, greatly increasing our understanding of the decision making. Importantly, CBMs have been shown to be useful in a variety of applications, including model debugging and human intervention on decisions. However, there are two crucial limitations of existing CBMs and their variants (Koh et al., 2020;Yuksekgonul et al., 2022;Zhou et al., 2018): (i) labeled data is required for each of the predefined concepts, which is time consuming and expensive to collect; (ii) the accuracy of a CBM is often significantly lower than the accuracy of a standard neural network, especially on more complex datasets.\n\nTo address the above two challenges, we propose a new framework named Label-free CBM, which is capable of transforming any neural network into an interpretable CBM without labeled concept data while preserving accuracy comparable to the original neural network by leveraging foundation models (Bommasani et al., 2021). Our Label-free CBM has many advantages:\n\n\u2022 it is scalable -to our best knowledge, it is the first CBM that scales to ImageNet \u2022 it is efficient -creating a CBM takes only a few hours even for very large datasets \u2022 it is automated -training it for a new dataset requires minimal human effort 2 RELATED WORK Post-hoc explanations (Samek et al., 2021): The approach of post-hoc explanations includes some classic methods such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017), which try to explain individual model decisions by identifying which parts of the input data (e.g. pixels) are the most important for a given decision. However, these methods are based on local approximations of the DNN model and as such are not always accurate. Further, the explanations at the granularity of input pixels may not always be helpful and could require substantial subjective analysis from human. In contrast, our explanations in Section 4 are not approximated and explain predictions in terms of human-understandable concepts.\n\nMore interpretable final layer: (Wong et al., 2021) proposes making the FC layer sparse, and develop an efficient algorithm for doing so. They show that sparse models are more interpretable in many ways, but it still suffers from the fact the previous layer features are not interpretable. NBDT (Wan et al., 2020) propose replacing the final layer with a neural backed decision tree for another form of more interpretable decisions. Other approaches to make NNs more interpretable include Concept Whitening  and Concept Embedding Models (Zarlenga et al., 2022).\n\nCBM: Most related to our approach are Concept Bottleneck Models (Koh et al., 2020;Losch et al., 2019) which create a layer before the last fully connected layer where each neuron corresponds to a human interpretable concept. CBMs have been shown to be beneficial by allowing for human testtime intervention for improved accuracy, as well as being easier to debug. To reduce the training cost of a CBM, a recent work (Yuksekgonul et al., 2022) proposed Post-Hoc CBM that only needs to train the last FC layer along with an optional residual fitting layer, avoiding the need to train the backbone from scratch. This is done by leveraging Concept Activation Vectors (CAV) (Kim et al., 2018) or the multi-modal CLIP model (Radford et al., 2021). However, the post-hoc CBM does not fully address the problems of the original CBM as using TCAV still requires collecting annotated concept data and their use of CLIP model can only be applied to if the NN backbone is the CLIP image encoder. Additionally, the performance of post-hoc CBMs without uninterpretable residual fitting layers is often significantly lower than the standard DNNs. Similarly, an earlier work Interpretable Basis Decomposition (Zhou et al., 2018) proposes learning a concept bottleneck layer based on labeled concept data for explanable decisions, even though they do not call themselves a CBM. Comparison between the features our method and existing approaches is shown in Table 1.\n\nModel editing/debugging: Our approach is related to a range of works proposing ways to edit networks, such as (Bau et al., 2020; for generative vision models, (Bau et al., 2020) for classifiers, or (Meng et al., 2022;Mitchell et al., 2021) for language models. In addition (Abid et al., 2021) propose a way to debug model mistakes using TCAV activation vectors.   (Zhou et al., 2018) and 4 versions of P-CBM (Yuksekgonul et al., 2022), where '-h' indicates the hybrid model that uses uninterpretable residual term, '(CLIP)' means models using CLIP concepts. We used maybe to indicate models that could in theory extend to Im-ageNet but have not been tested.\n\nCLIP-Dissect (Oikarinen & Weng, 2022): CLIP-Dissect is a recent method for understanding the roles of hidden layer neurons by leveraging the CLIP multimodal model (Radford et al., 2021). It can provide a score of how close any neuron is to representing any given concept without the need of concept annotation data, which makes it useful as an optimization target for learning our interpretable projection in Step 3.\n\n\nLABEL-FREE CBM: A NEW FRAMEWORK TO BUILD CBM\n\nIn this section, we propose Label-free CBM, a novel framework that builds a concept bottleneck model (CBM) in an automated, scalable and efficient fashion and addresses the core limitations of existing CBMs. Given a neural network backbone, Label-free CBM transforms the backbone into an interpretable CBM without the need of concept labels with the following 4 steps which are illustrated in Fig 2 -Step 1: Create the initial concept set and filter undesired concepts;\n\nStep 2: Compute embeddings from the backbone and the concept matrix on the training dataset;\n\nStep 3: Learn projection weights W c to create a Concept Bottleneck Layer (CBL);\n\nStep 4: Learn the weights W F of the sparse final layer to make predictions.\n\nNote that the backbone model can either be a model trained on the target task, or a general model trained on a different task. The details of Label-free CBM for Step 1 are provided in Sec 3.1, Step 2 and Step 3 in Sec 3.2, and finally Step 4 in Sec 3.3.\n\nFigure 2: Overview of our pipeline for creating label-free CBM.\n\n\nSTEP 1: CONCEPT SET CREATION AND FILTERING\n\nIn this step, we will describe how to create a concept set to serve as the basis of human-interpretable concepts in the Concept Bottleneck Layer. This step consists of two sub-steps: A. Initial concept set creation and B. Concept set filtering.\n\nA. Initial concept set creation: A concept set refers to the set of concepts represented in the Concept Bottleneck Layer. In the original CBM paper (Koh et al., 2020), this is decided by domain experts as the set of concepts that are important for the given task. However, since our objective is to automate the entire process of generating CBMs, we don't want to rely on human experts. Instead, we propose generating the concept set via GPT-3 (Brown et al., 2020) using the OpenAI API. Somewhat surprisingly, GPT-3 has a good amount of domain knowledge of which concepts are important for detecting each class when prompted in the right way. Specifically, we ask GPT-3 the following:\n\n\u2022 List the most important features for recognizing something as a {class}:\n\n\u2022 List the things most commonly seen around a {class}:\n\n\u2022 Give superclasses for the word {class}:\n\nNote that the {class} in here refers to the class name in the machine learning task. For GPT-3 to perform well on the above prompt, we provide two examples of the desired outputs for few-shot adaptation. Note that those two examples can be shared across all datasets, so no additional user input is needed for generating concept set for a new dataset. Full prompts and example outputs are illustrated in the Appendix Figures 10 and 11. To reduce variance we run each prompt twice and combine the results. Combining the concepts received from different classes and prompts gives us a large, somewhat noisy set of initial concepts, which we further improve by filtering. We found using GPT-3 to generate initial concepts to perform better than using the knowledge-graph ConceptNet (Speer & Havasi, 2013) which was used in Post Hoc-CBM (Yuksekgonul et al., 2022). Reasons for this and a comparison between the two are presented in Appendix A.6.\n\nB. Concept set filtering: Next we employ several filters to improve the quality and reduce the size of our concept set, as stated below:\n\n1. Concept length: We delete any concept longer than 30 characters in length, to keep concepts simple and avoid unnecessary complication.\n\n\n2.\n\nRemove concepts too similar to classes: We don't want our CBM to contain the output classes themselves as that would defeat the purpose of an explanation. To avoid this we remove all concepts that are too similar to the names of target classes. We measure this with cosine similarity in a text embedding space. In particular we use an ensemble of similarities in the CLIP ViT-B/16 text encoder as well as the all-mpnet-base-v2 sentence encoder space, so our measure can be seen as a combination of visual and textual similarity. For all datasets, we deleted concepts with similarity > 0.85 to any target class.\n\n3. Remove concepts too similar to each other: We also don't want duplicate or synonymous concepts in the bottleneck layer. We use the same embedding space as above, and remove any concept that has another concept with > 0.9 cosine similarity already in concept set.\n\n\n4.\n\nRemove concepts not present in training data: To make sure our concept layer accurately presents its target concepts, we remove any concepts that don't activate CLIP highly. This cut-off is dataset specific, and we delete all concepts with average top-5 activation below the cut-off.\n\n5. Remove concepts we can't project accurately: Remove neurons that are not interpretable from the CBL. This step is actually performed after step 3 and is described in section 3.2.\n\nWe discuss the reasoning behind the filters in detail in Appendix A.5 and perform an abblation study on their effects in Appendix A.4.\n\n\nSTEP 2 AND 3: LEARNING THE CONCEPT BOTTLENECK LAYER (CBL)\n\nOnce the concept set is obtained, the next step is to learn a projection from the backbone model's feature space into a space where axis directions correspond to interpretable concepts. Here, we present a way of learning the projection weights W c without any labeled concept data by utilizing CLIP-Dissect (Oikarinen & Weng, 2022). To start with, we need a set of target concepts that the bottleneck layer is expected to represent as C = {t 1 , ..., t M }, as well as a training dataset (e.g. images) D = {x 1 , ..., x N } of the original task. Next we calculate and save the CLIP concept activation matrix P where P i,j = E I (x i ) \u00b7 E T (t j ) and E I and E T are the CLIP image and text encoders respectively. W c is initialized as a random M \u00d7 d 0 matrix where d 0 is the dimensionality of backbone features f (x). The initial set C is created in Step 1 and the training set D is provided by the downstream task. We define\nf c (x) = W c f (x), where f c (x i ) \u2208 R M .\nWe use k to denote a neuron of interest in the projection layer, and its activation pattern is denoted as\nq k where q k = [f c,k (x 1 ), . . . , f c,k (x N )] \u22a4 , with q k \u2208 R N and f c,k (x) = [f c (x)] k .\nTo make the neurons in the CBL interpretable, we need to enforce the projected neurons to activate in correlation with the target concept, which we do by optimizing W c to maximize the CLIP-Dissect similarity between the neuron's activation pattern and the target concept. To optimize this similarity, we have designed a new fully differentiable similarity function sim(t i , q i ) that can be applied to CLIP-Dissect, called cos cubed, that still achieves very good performance in explaining the neuron functionality as shown in Appendix A.3. Our optimization goal is to minimize the objective L over W c as defined in Equation (1):\nL(W c ) = M i=1 \u2212sim(t i , q i ) := M i=1 \u2212q i 3 \u00b7P :,i 3 ||q i 3 || 2 ||P :,i 3 || 2 .(1)\nHereq indicates vector q normalized to have mean 0 and standard deviation 1, and the cos cubed similarity sim(t i , q i ) is simply the cosine similarity between two activation vectors after both have been normalized and raised to third power element-wise. The third power is necessary to make the similarity more sensitive to highly activating inputs. As this is still a cosine similarity, it takes values between [\u22121, 1]. We optimize L(W c ) using the Adam optimizer on training data D, with early stopping when similarity on validation data starts to decrease. Finally to make sure our concepts are truthful, we drop all concepts j with sim(t j , q j ) < 0.45 on validation data after training W c . This is the 5th concept set filter from Sec 3.1. This cutoff was selected manually as a good indicator of a neuron being interpretable. During this filtering, the number of concept is reduced:\nM \u2190 M \u2212 \u2206,\nwhere \u2206 is non-negative integer representing the number of concepts being removed in this step. Note that matrix W c has also to be updated accordingly by removing the rows that corresponds to the removed concepts, and with our notation, W c \u2208 R M \u00d7d0 . To simplify Figure 2, we omit plotting this concept removal step.\n\n\nSTEP 4: LEARNING THE SPARSE FINAL LAYER\n\nNow that the Concept Bottleneck Layer is learned, the next task is to learn the final predictor with the fully connected layer W F \u2208 R dz\u00d7M where d z is the number of output classes. The goal is to keep W F sparse, since sparse layers have been demonstrated to be more interpretable (Wong et al., 2021). Given that both the backbone f (x) and learned concept projection W c are fixed, this is a problem of learning a sparse linear model, which can be solved efficiently with the elastic net objective:\nmin W F ,b F N i=1 L ce (W F f c (x i ) + b F , y i ) + \u03bbR \u03b1 (W F ) (2) where R \u03b1 (W F ) = (1 \u2212 \u03b1) 1 2 ||W F || F + \u03b1||W F || 1,1 , \u2225 \u00b7 \u2225 F\ndenotes the Frobenius norm, \u2225 \u00b7 \u2225 1,1 denotes element wise matrix norm, b F denotes the bias of the FC layer, L ce is the standard crossentropy loss and y i is the ground-truth label of data x i . We optimize Equation (2) using the GLM-SAGA solver created by (Wong et al., 2021). For the sparse models, we used \u03b1 = 0.99 and \u03bb was chosen such that each model has 25 to 35 nonzero weights per output class. This level was found to still result in interpretable decisions while retaining good accuracy. Depending on the number of features/concepts in the previous layer this corresponds to 0.7-15% of the weights of the model being nonzero.\n\n\nEXPERIMENT RESULTS\n\nWe present three main results on evaluating the accuracy and interpretability of the Label-free CBM in this section. Due to page limit, additional experiments and discussions are in Appendix A.2-A.12. An overview of additional experiments is provided in Appendix A.1.\n\nDatasets. To evaluate our approach, we train Label-free CBMs on 5 datasets. These are CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), CUB (Wah et al., 2011), Places365 (Zhou et al., 2017) and ImageNet (Deng et al., 2009). This is a diverse set of tasks, where CIFAR-10/100 and ImageNet are general image classification datasets, CUB is a fine-grained bird-species classification dataset and Places365 is focused on scene recognition. Their sizes vary greatly, with CUB having 5900 training samples, CIFAR datasets 50,000 each and ImageNet and Places365 have 1-2 million training images. CUB contains annotations with 312 concepts for each datapoint, such as has wing color:blue or has head pattern:spotted. We used neither the concept names or the concept annotations to train our LF-CBM models to showcase our ability to perform without labels, yet our method discovered similar concepts and is competitive with methods that utilize the available concept information.\n\nSetup. For CIFAR and CUB we use the same backbone models as (Yuksekgonul et al., 2022) for fair comparison, so we use CLIP(RN50) image encoder as the backbone for CIFAR, and ResNet-18 trained on CUB from imgclsmob for CUB. For both ImageNet and Places365 we use ResNet-50 trained on ImageNet as the backbone network. The number of concepts each model uses is roughly proportional to the number of output classes for that task, as each class adds more initial concepts. The number of concepts for our models are as follows: 128 for CIFAR-10, 824 for CIFAR-100, 211 for CUB, 2202 for Places-365 and 4505 for ImageNet. CUB has a smaller number of concepts because we only used the important features prompt. All models are trained on a single Nvidia Tesla P100 GPU, and the full training run takes anywhere from few minutes to 20 hours depending on the dataset size. The majority of runtime is taken by step 2, where we save activations for both the backbone and CLIP over the entire training dataset. Fortunately, these results only need to be calculated once and can be reused for training new CBMs on the same dataset. Once the activations have been saved, learning the model takes less than 4 hours on all datasets.\n\nResult (I): Accuracy. Table 2 shows the performance of Label-free CBM on all 5 datasets. We can see that our method can create a CBM with a sparse final layer on with little loss in accuracy on all datasets, including a model with 72% top-1 accuracy on the ImageNet. Our Label-free CBM has significantly higher accuracy than Post-hoc CBM on the datasets we evaluated, but some rows for P-CBM are N/A as they do not provide results those results and it is unclear how to scale P-CBM to larger datasets. For Table 2 we excluded methods with non-interpretable components (i.e. P-CBM-h or IBD) as we want to focus on fully interpretable CBM models. Note that P-CBM uses the expert provided concept set for CUB200, yet we can outperform it using our fully GPT-3 derived set of features. The standard sparse models were finetuned by us by learning a sparse final layer directly after feature layer f (x) as described in (Wong et al., 2021) and also have 25-35 nonzero weights per class. The accuracies of sparse standard models are comparable to our CBM, indicating the CBM does not reduce accuracy.\n\n\nDataset\n\n\nModel\n\nSparse final layer CIFAR10 CIFAR100 CUB200 Places365 ImageNet   Figure 3 we visualize the final layer weights for the classes \"Orange\" and \"Lemon\" on ImageNet, and classes \"Mountain\" and \"Mountain Snowy\" on Places365. This visualization is a Sankey diagram of the final layer weights coming into two output classes, with the weight between a concept and output class displayed as the width of the line connecting them. We have only included weights with absolute value greater than 0.05 (for comparison the largest weights are usually 0.5-1). Negative weights are denoted as NOT concept. The learned decision rules largely align with our intuitions. For ImageNet, the concept \"citrus\" fruit is highly connected to both \"Orange\" and \"Lemon\", while orange colors activate the \"Orange\" class and yellow colors and limes activate the \"Lemon\" class. On Places, concepts like \"a high elevation\" activate both classes, while \"Mountain Snowy\" is activated by snow and ice related concepts and \"Mountain\" is activated by concepts related to volcanoes like \"lava\" and \"crater\". Visualizations like this allows us to gain a global understanding of how our model behaves. Additional visualizations are shown in Appendix A.11\n\nResult (III): Explainable individual decisions. In addition to global understanding, CBMs allow us to understand the reasoning behind individual decisions. Since our decisions are linear functions of interpretable features, they can be explained in a simple and accurate way. After learning the Concept Bottleneck Layer, we normalize the activations of each concept to have mean 0 and standard deviation 1 on the training data. Given normalized features, the contribution of feature j to output i on input x k can be naturally calculated as\nContrib(x k , i, j) = W F [ij] f c (x k ) j .\nSince our W F is sparse, most contributions will be 0 and the important contributions can be easily visualized with a bar plot. Example visualizations for explaining the models predicted class are shown in Figure  4, and more visualizations are available in Appendix A.12. These visualizations show the concepts with highest absolute value contribution. Concepts with negative activation can still be important contributors, and they are shown as \"NOT concept\" in our visualizations. We can see for example the CUB model correctly identifies \"a red head\" as the most important feature in recognizing this image as \"Red headed Woodpecker\", while the fact that there is no \"a rosy breast\" present also increases models confidence.\n\n\nCASE STUDY: MANUALLY IMPROVING AN IMAGENET MODEL\n\nIn this section we take a deep dive into our ImageNet Label-free CBM -we analyze the errors it makes and design ways to debug/improve our network based on these findings. In particular, we show how we can inspect a handful of individual incorrect decisions and manually change our model weights to not only fix those predictions but also multiple other predictions, improving the networks overall accuracy. To our knowledge, this is the first example of manually editing a large well-trained neural network in a way that improves test accuracy on the same distribution.\n\n\nTYPES OF MODEL ERRORS\n\nDuring the course of our experiment, we were able to identify 4 different types of mistakes our model makes, with different strategies for fixing them:\n\nType 1: Incorrect/ambiguous label: Despite looking like errors in terms of accuracy, these are simply a consequence of having noisy labels and are unavoidable. We will not attempt to fix this type of mistakes.\n\nType 2: No sufficient concept in CBL: Despite having a large set of concepts, our CBL does not include all concepts important for detecting certain classes. For example, detecting the difference between two species of snakes may require recognizing very fine-grained patterns that can't be easily explained in words. While these could often be fixed by adding new concepts to the concept set, we do not focus on it as it requires retraining the projection W c and the final layer.\n\nType 3: Incorrect concept activations: Some of the time, the activations in our CBL are incorrect and do not match the image, causing the prediction to become incorrect. Since the network up to the Concept Bottleneck Layer is mostly a black box, we can't easily improve the predictions, but predictions can be improved with test time interventions as shown in Figure 5.\n\nType 4: Incorrect final layer weight W F : Sometimes even if all the concept activations are correct, the models final layer weights still cause it to make an erroneous prediction. As the final layer is fully interpretable, fixing these errors is the main focus of our study described in Sec 5.2.\n\nExamples of Type 1 and Type 2 errors are shown in Figure 13 in the Appendix.\n\n\nEDITING FINAL LAYER WEIGHTS\n\nIn this section we describe our procedure for manually editing final layer weights of our model. This is done with the following procedure:\n\n1. Find an input where the model makes a Type 4 error: This is done by visualizing incorrect model predictions and their explanations, and identifying Type 4 errors, i.e. errors where the highly activating concepts look correct and the label is correct and unambiguous. 2. Identify a concept to edit.: Perhaps the most tricky step, to change a prediction we need to select a concept that is highly activated in this image, important for the ground truth class and not important for the incorrect (using our domain knowledge), and will have a small impact on other Figure 5: Here we fix an incorrect model prediction on ImageNet by simply zeroing out the incorrect activation for concept \"a hat\". Model originally predicted \"poke bonnet\" which is a type of hat. Figure 6: Examples of two edits we performed on our model predictions. Both edits fix the error we found, and fix more other predictions than they cause mistakes on other unseen inputs.\n\npredictions. For example to flip a prediction from \"cricket insect\" to \"grasshopper\" we chose the concept \"a green color\". Typically we want \u2206w to be slightly more than required to flip the prediction on our example, but understanding its effect on other predictions is hard and should be checked with a validation dataset. In total we identified 5 such beneficial edits during our quick exploration, the other 3 are shown in Fig 12. After applying all of them, the total validation accuracy of our model goes from 71.98% to 72.02%, correcting 38 predictions while turning 17 incorrect. While this might seem like a small difference, it is worth noting we only change 10/4.5 million total weights in the final layer, of which 34,550 are non-zero. Our edits only affect 10 classes which have a total of 500 validation examples, so in effect we increase accuracy on this subset by 4.2% which is a significant boost. We believe this editing approach is a promising direction for future use, where practitioners could for example notice an incorrect prediction in a production system, quickly devise a fix for that prediction, and check how it affects other predictions using validation data.\n\n\nCONCLUSION\n\nWe have presented Label-free CBM, a fully automated and scalable method for generating concept bottleneck models and used it to create the first high performing CBM on ImageNet. In addition, we have demonstrated how our models are easier to understand in terms of both global decision rules and reasoning for individual predictions. Finally, we show how to use this understanding to manually edit weights to improve accuracy in a trained ImageNet model. First in section A.2 we discuss the limitations of our method, then in section A.3 we show that our proposed cos cubed similarity function performs well in describing neuron functionality. In section A.4 we perform an ablation study on the effect of different concept set filters we have proposed, and in Sec A.5 we discuss our reasoning behind each filter in detail through an example. In section A.6 we show that our GPT-3 generated concept set helps us reach higher accuracy than previous methods for creating initial concept set. In Appendix A.7 we show the effects of removing the sparsity constraint on our models, leading to higher accuracy but reduced interpretability. In Appendix A.8 we show our results are consistent despite inherent randomness in our pipeline. Next, in Appendix A.9 we provide a detailed explanation of the procedure we used for model editing described in Sec 5.2. Finally in Sections A.10, A.11 and A.12 we provide additional figures for model editing, individual decision explanations and model weight visualizations respectively.\n\nIn Appendix B we provide results of our large-scale crowdsourced human evaluation, which confirms that LF-CBM is indeed more interpretable than standard models according to two metrics.\n\n\nA.2 LIMITATIONS\n\nWhile our model performs well in terms of both interpretability and accuracy, it still has some limitations. For example, using a model like GPT-3 to produce concepts is stochastic, and sometimes may fail to generate important concepts for detecting a certain class. However our model is quite robust to small changes in the concept set as shown in sections A.4 and A.8. In general, automatic ways of generating concept set may sometimes lack the required domain knowledge to include some important concepts, and would perhaps be best used in collaboration with a human expert. \n\n\nA.3 DISSECTION PERFORMANCE OF COS CUBED SIMILARITY FUNCTION\n\nIn this section we discuss how good the cos cubed similarity function we defined in section 3.2 is at describing neuron functionality, i.e. how well it performs as a similarity function for CLIP-Dissect. Specifically we measured its performance on describing the roles of final layer neurons on a ResNet-50 trained on ImageNet (where we know the ground truth role of the neurons) in terms of both accuracy and average distance from the correct description in a text embedding space. We followed the quantitative evaluation methodology of (Oikarinen & Weng, 2022), which describes this process in more detail. We compared the results against using CLIP-Dissect with two different similarity functions, softWPMI which was the best function identified by (Oikarinen & Weng, 2022) and cos which is simple a differentiable baseline. We can see cos cubed performs es well as softWPMI in terms of similarity in text embedding space, while being slightly worse in terms of accuracy, and is much better than simple cos similarity. This is impressive given cos cubed is fully differentiable while SoftWPMI is not differentiable.  Table 3: Comparison of the performance between similarity functions. We look at the final layer of ResNet-50 trained on ImageNet. We use 20,000 most common English words as the concept set for mpnet cos similarity and ImageNet classes as the concept set for top1 accuracy. We can see cos cubed performs much better than simple cos, and almost as well as SoftWPMI.\n\n\nA.4 ABLATION: EFFECT OF CONCEPT FILTERS\n\nIn this section we study how each step in our proposed concept filtering effects the results of our method. In general our use of filters has two main aims:\n\n\u2022 To improve the interpretability of our models \u2022 Improve computational efficiency and complexity by reducing the number of concepts\n\nThe filters are not designed to improve performance of the models in terms of accuracy, in fact we would expect them to slightly reduce accuracy as a model with more concepts is generally larger and more powerful.\n\nTo evaluate the effect each individual filter has we trained models on both CIFAR10 and ImageNet while deactivating one filter at a time, as well as one without using any filters at all. The results are displayed in Table 4. We can see that the accuracy of our models is not at all sensitive to the choice of filters, with ImageNet accuracy remaining constant and CIFAR10 slightly increasing its accuracy with less filters. We were unable to train an ImageNet model without any filters as training with the large number of concepts required more memory than we had available in our system. Additionally the ImageNet model without similarity to other concepts filter had worse accuracy and less sparsity than other tested models, which we think may be caused by the GLM-SAGA optimizer not finding as good of a solution when the number of concepts increased too much.  Table 4: Effect of our individual concept filters on the final accuracy and number of concepts used by our models. *We were unable to train the model to be sparse enough, results from a less sparse model.\n\n\nA.5 FURTHER DISCUSSION AND EXAMPLE OF CONCEPT FILTERING\n\nTo further show the effects of each filter, we will showcase the full filtering procedure for our CI-FAR10 model with all filters. We start with a freshly generated initial concept set from GPT-3, which has a total of 177 concepts. We then take the following steps to filter down concepts:\n\n1. Delete concepts that are too long. This step is important to make sure concepts are easy to visualize and simple non-convoluted concepts. For CIFAR-10 this leads to deletion of 3 concepts: -white spots on the fur (in some cases), feline features (e.g., whiskers, ears) and legs with two toes pointing forward and two toes pointing backwar. 174 concepts remain. 2. Delete concepts too similar to output classes. This step is required for the explanations to be informative, as it helps avoid trivial explanations such as: \"This is image is a cat because it is a cat\". This step removes the following CIFAR-10 concepts (with the class name it is too close to shown in brackets): -a cat (cat), a deer (deer), a horse (horse), a plane (airplane), a car (automobile), car (automobile), cars (automobile), vehicle (automobile), animal (dog), truck driver (truck) 164 concepts remain 3. Delete concepts too similar to other concepts. In this step we aim to delete duplicate concepts from the concept set, i.e. two concepts that have the same semantic meaning should not both be part of the concept set. Duplicate concepts will cause unnecessary computational cost and confusing explanations. For CIFAR-10 we delete the following concepts (with the concept it is too similar to in brackets): -4 wheels (four wheels), a food bowl (a bowl), a furry, four-legged animal (a large, fourlegged mammal), a gasoline station (a gas station), a large boxy body (a large body), a large, muscular body (a large body), a street (a road), a strong engine (an engine), large, bulging eyes (large eyes), the ocean (the sea) 154 concepts remain 4. Remove concepts not present in training data. In this step we remove all concepts that CLIP thinks are not really present in training data. The purpose is to avoid learning neurons that don't correctly represent their target concepts, as concepts missing from the dataset are unlikely to be learned correctly in the Concept Bottleneck Layer. For CIFAR-10 this step deletes the following concepts: -a bit, a mechanic, legs, long legs, passengers, several seats inside, windows all around 147 concepts remain 5. Remove concepts we can't project accurately. The purpose of this step is similar to the previous step, we want to remove all concepts that are not faithfully represented by the CBL. To do this we evaluate the similarity score between the target concept and the activations of our new neuron using CLIP-Dissect Oikarinen & Weng (2022) on the validation data, and delete concepts with similarity less than 0.45 which was determined to be a good indicator of faithful concept representation. For CIFAR-10 we delete the following concepts: -a bed, a coffee mug, a crew, a house, a wheel We are left with our 142 final concepts.\n\nIn total our method has 5 different cutoff parameters that can be tuned. Since the main purpose of these filters is to make the network more interpretable, their values were mainly chosen through trial and error to find ones that produce the most useful and explainable models. Most of these cutoffs are independent of the dataset chosen, and the only one we changed from one dataset to another was the value of cutoff for images being present in the dataset (filter 4). We used a cutoff of 0.25 for CIFAR-10 and CIFAR-100, 0.26 for CUB-200 and 0.28 for Places and ImageNet. We found we had to change this cutoff as CLIP doesn't activate very highly on low resolution images of CIFAR, and in general we get higher top5 activations on larger datasets. Since the other 4 cutoffs are fixed, using our method for a new dataset won't require a large hyperparameter search, and as seen in Table 4 the overall performance is not very sensitive to specific filters.\n\nTo visualize the effect of our filters, in Figure 7 we visualize the final layer weights for CIFAR-10 class \"automobile\" trained with and without filters. We can see the model with filters has found a reasonable decision rule, with largest weights corresponding to four wheels and a steering wheel.\n\nOn the other hand, the weights for the model without filters are quite problematic. First, the 3 largest weights are cars, car and a car. Not only do they have the same meaning as the final class itself (filter 2), thus lacking any explanatory power, they are duplicates of each other (filter 3), making the explanation unnecessarily compilicated. Finally we see the unrelated concept bed which is not present in CIFAR-10 having a small positive weight, which should be removed by either filter 4 or 5. Figure 7: Comparison of the final layer weights between CIFAR-10 model trained with and without filters.\n\n\nA.6 ABLATION: EFFECT OF INITIAL CONCEPT SET\n\nIn this section we evaluate the effect our initial concept set generator has on our results. In our paper we proposed a new method of generating descriptions using GPT-3 Brown et al. (2020), and in this section we compare those results to the case where we generate the initial concept set using Con-ceptNet Speer & Havasi (2013) as proposed by Yuksekgonul et al. (2022). In general ConceptNet only works well when prompted with relatively common single word phrases, but many of class names for CUB200, Places365 and ImageNet consist of multiple words, such as the ImageNet class great white shark. To overcome this issue we split multi-word class names into single word components and add concepts related to each word to the concept set, in this case great, white and shark. After creating the initial concept set we use the same filtering procedure described in section 3.1B for both models. Table 5 displays the accuracy comparison between models trained on different initial concept setst. In general using GPT3 seems to provide a small (0.1-1.5%) accuracy boost on all datasets, with the exception of CUB200 where the ConceptNet model fails completely. This is because ConceptNet is unable to generate concepts for the highly specific class names in CUB200, such as Groove billed Ani, while GPT-3 is not troubled by this.  Table 5: Accuracy comparison between using different methods for generating initial concept set.\n\n\nA.7 ABLATION: LF-CBM WITHOUT SPARSITY CONSTRAINT\n\nTo estimate the effect sparsity has on our models, we show the accuracy of LF-CBM models trained without sparsity constraints (LF-CBM (dense)) in Table 6. We can see that not using a sparsity constraint greatly improves accuracy (except for CUB200 which starts to overfit), but it comes at a very large cost for interpretability as can be seen in Figure 8, where almost none of the decision is explained by the 10 most highly contributing concepts.   Figure 17. We can see the dense model is practically uninterpretable.\n\n\nModel\\Dataset\n\n\nA.8 CONSISTENCY OF CONCEPT SET GENERATION\n\nSince the initial concept set is generated using GPT-3, it is a random process and in this section we explore how much this noise effects our final results. When regenerating initial concept set and running our concept filtering pipeline for our ImageNet model, we got 4380 concepts, compared to the original 4523 concepts. In terms of accuracy, the model learned with regenerated concept set reached an accuracy of 71.89%, compared to the average of 71.95% with the original concept set. This indicates that the difference is very small in terms of accuracy. However, the exact concepts used and model weights can be pretty different as shown in Figure 9, despite all being relevent (e.g. there are many concepts that appear on both concept sets, e.g. Image 49962 has important concepts of \"wipes\", \"toiletries\" on both, though they have different corresponding contributions). Figure  9 shows the explanations for decisions on two random images for the original model and model trained with regenerated concept set. Figure 9: Difference in concepts used for a decision and their weights for models trained with original vs regenerated concept set, shown on two random images.\n\n\nA.9 MORE DETAILS ON MODEL EDITING PROCEDURE\n\nHere we provide an improved explanation of Sec 5.2 on how we can edit the final layer weights of our proposed Label-Free CBM to correct incorrect predictions. Our procedure includes 3 steps:\n\nStep 1: Find an input image x i where the model makes a Type 4 Error (incorrect final layer weight):\n\nThis is done by visualizing incorrect model predictions and their explanations similar to Figure 4. To identify if it is a Type 4 error, first we check that the ground truth label of the input image is correct and unambiguous (therefore not type 1 error described in sec 5.1). Next, we check that the highly activating concepts look correct (therefore not type 2 or type 3 error described in sec 5.1).\n\nStep 2: Identify a concept to edit:\n\nWe start by listing the highest activating concepts for the chosen input, i.e. the highest elements of f c (x i ) (after normalization). This allows us to identify concepts that are the most important for this specific image while not too important for other images. From these concepts we use our domain knowledge (and/or internet search) to understand which concepts are relevant to making this decision.\n\nFor example, the image in the right panel of Figure 6 is originally predicted as a \"Shopping basket\", while the ground truth is a \"Hamper\". The 5 most highly activating concepts for this image are: \"a basket\", \"made of rope or string\", \"a rope\", \"a laundry basket\" and \"a fishing net\". After a short investigation we find that although the classes \"Hamper\" and \"Shopping basket\" are very similar, hampers are more often constructed in a woven like manner. Therefore, we identify the concept of \"made of rope or string\" as relevant for capturing this difference, and choose it for editing in the next step.\n\nStep 3: Change weights by sufficient magnitude:\n\nOnce we have selected the concept to be corrected, we will compute how much the magnitude of the associated final layer weight should be changed (denoted as \u2206w, \u2206w \u2208 R) and then edit the weights in the following way:\nW F [gt,concept] \u2190 W F [gt,concept] + \u2206w, W F [pred,concept] \u2190 W F [pred,concept] \u2212 \u2206w(3)\nNote that W F [i,j] denotes the (i, j) element in the matrix W F . The goal of editing is to correct the inaccurate prediction on this specific instance while minimizing effect on other predictions. To calculate \u2206w needed to flip prediction, we first calculate the difference in logits \u2206a (before softmax) before the edit, \u2206a = W F [pred,:] \nf c (x i ) \u2212 W F [gt,:] f c (x i ).\nSince we would like the prediction to be corrected to gt class, our goal is to design \u2206w such that\n(W F [gt,:] + \u2206w \u00b7 e)f c (x i ) \u2212 (W F [pred,:] \u2212 \u2206w \u00b7 e)f c (x i ) > 0(4)\nwhere e is a one-hot row vector with the entry e [concept] = 1.\nLet b = (W F [gt,:] + \u2206w \u00b7 e)f c (x i ) \u2212 (W F [pred,:] \u2212 \u2206w \u00b7 e)f c (x i ),\nwhere b is a nonnegative constant deciding how large of a margin we want the correct prediction to have. Since \u2206a = W F [pred,:] \nf c (x i ) \u2212 W F [gt,:] f c (x i ), we have: 2\u2206w \u00b7 f c (x i ) [concept] \u2212 \u2206a = b \u21d2 \u2206w = (\u2206a + b)/(2f c (x i ) [concept] )(5)\nTypically we use values for b between 0.2 and 2 to calculate required \u2206w.\n\nIt is worth noting that when the model weights are edited, it might affect other image's predictions too due to the change of weight parameters. Thus, with the goal to correct the wrong predictions while not affecting other already correct predictions, we suggest each edit should be checked with a validation dataset before applying on a target model. As described in the end of Sec 5 in the manuscript, with the above proposed model weight editing, we are able to improve the overall model accuracy from 71.98% to 72.02% by performing the model editing for only 5 different images, which is a non-negligible improvement. Since the edits only affect weights for 10/1000 classes, this corresponds to a 4% accuracy boost on the affected classes.\n\nA.10 ADDITIONAL FIGURES Figures 10, 11 provide examples of the full prompts we used for GPT-3, as well as GPT outputs. For all experiments we used the text-davinci-002 model available through OpenAI API. Figure 12 shows the additional model edits performed in our ImageNet CBM experiment, and Figure  13 showcases Type 1 and Type 2 errors made by our ImageNet CBM. Figure 10: Full prompts used for GPT-3 concept set creation. Text in green generated by GPT, rest is our prompt. Figure 11: Full prompts used for GPT-3 concept set creation. Text in green generated by GPT, rest is our prompt.\n\n\n21\n\nPublished as a conference paper at ICLR 2023 Figure 12: The other 3 model edits we performed in our experiment Figure 13: Examples of Type 1 and type 2 model errors.\n\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n\nA.11 ADDITIONAL WEIGHT VISUALIZATIONS FOR RANDOM CLASSES\n\nIn figures 14 and 15 we have visualized the final layer weights leading to 4 randomly chosen output classes for both Places365 and ImageNet. These were created using the same procedure as Figure  3, with minor differences such as focus on single class at a time and no manual editing of concept colors according to semantic similarity. We can see that the concepts that have large weights are indeed relevant to the class, and overall the weights look reasonable. 23 Published as a conference paper at ICLR 2023 The results in this section were added after the ICLR camera ready deadline to further support the interpretability of our proposed method and are not included in the official ICLR paper.\n\nTo get a quantitative measure of the interpretability of our models, we conducted a large scale study on Amazon Mechanical Turk. We evaluated our models on two tasks: For our explanations to be faithful, it is very important that the neurons we learned in the Concept Bottleneck Layer (Section 3.2) indeed correspond to their target concepts.\n\nSetup: To test this, we conducted a large scale evaluation on Amazon Mechanical Turk, where we evaluated all 4505 neurons in the CBL of our trained ImageNet LF-CBM, with 3 ratings per neuron. Our experiment interface is shown in Figure 19. The basic idea is to show users the 10 most highly activating images (from the validation dataset) for a neuron, and ask them to rate whether the description accurately matches the images on a 5-point scale, where 1=Strongly Disagree and 5=Strongly Agree. The description for a CBL neuron is its target concept.\n\nAs a baseline, we compared against neurons in the second to last layer of ResNet-50 trained on ImageNet, which was the backbone model used for ImageNet LF-CBM. Since these neurons don't originally have an associated concept, we used CLIP-Dissect (Oikarinen & Weng, 2022) to generate explanations for each neuron, and used those as the description. We evaluated each of the 2048 neurons in the layer with 3 raters each. For CLIP-Dissect we used original SoftWPMI similarity function and ImageNet validation data as probing dataset.\n\nWe used raters in the United States, with greater than 98% approval rating and at least 10000 previously approved HITs. Users were paid $0.05 per task. Our experiment was deemed exempt from IRB approval by the UCSD IRB Board.\n\n\nResults:\n\nThe results of this experiment are presented in Table 7. We can see that on average, the neurons in CBL are more interpretable than the neurons of a standard neural network, reaching average score of 3.91 vs 3.65. Since we evaluated thousands of neurons this result is highly statistically Figure 19: User Interface shown to Mechanical Turk users for Task 1.  Table 7: Mechanical Turk results of Task 1. We can see the CBL neurons in LF-CBM were deemed to be significantly more interpretable than neurons in its standard backbone network, on average reaching \"Agree\" scores. Avg score indicates the average rating across all neurons and all evaluations, reported with standard error of the mean.\n\nsignificant. For more intuitive numbers, 76.4% of raters gave CBL neurons a rating of \"Agree\" or higher, while this number was only 65.2% for standard neurons. Finally, to evaluate how reliable these crowdsourced results are, we had 4 authors perform the same experiment in a blind setting, each evaluating 100 neurons for both models. Overall the results were similar, we noticed authors were more critical of both descriptions but both sets of evaluators found LF-CBM neurons more interpretable. The average author score was 3.66 for LF-CBM neurons, and 3.30 for neurons in ResNet-50. MTurk average scores for this subset were LF-CBM:3.93, ResNet-50: 3.68.\n\n\nB.2 TASK 2: HOW REASONABLE ARE OUR EXPLANATIONS FOR A SINGLE DECISION?\n\nIn the second task, we wanted to evaluate whether users consider the explanations our LF-CBM produces for individual decisions to be reasonable.\n\n\nSetup:\n\nWe show workers explanations based on which concepts contributed the most to the decision similar to Figure 4. However, we simplify this explanation to make it more suitable for crowdsourcing. Instead of displaying barplots and contribution magnitudes, we only show the names of 5 most contributing concepts. In addition, to avoid confusion we only show concepts with positive activation and positive weights. In this task we evaluated pairs of explanations to see how LF-CBM explanations compare with explanations from baseline models. Users were asked to rate which explanation is more reasonable and why. The task interface is shown in Figure 20. We compared our ImageNet LF-CBM against 3 baselines to ablate which parts of our model are most important for interpretability. The baselines are:\n\n\u2022 Standard (dense) -ResNet50 trained on Imagenet \u2022 Standard (sparse) -ResNet50 with sparse final layer \u2022 LF-CBM (dense) -Our LF-CBM but with a dense final layer W F To create the explanations from standard models, we identified the neurons with highest contributions to the prediction, and used CLIP-Dissect (Oikarinen & Weng, 2022) to get their concept. LF-CBM was compared against each baseline on 350 random input images, with 3 raters for each comparison for a total of 3150 comparisons. We only used inputs where all 4 models made the correct prediction, so the only difference was the explanations themselves.\n\nWe again used workers based in the US, with greater than 98% approval rating and 10,000 HITs completed. Each worker was paid $0.08 per comparison.\n\n\nResults:\n\nThe results of this experiment are displayed in Figures 21 and 22. In Figure 21 we can see users clearly preferred LF-CBM over Standard (dense) and Standard (sparse), with 64.2% and 65.2%(respectively) of users saying LF-CBM explanations are more reasonable (either slightly or clearly).\n\nHowever, we found the responses were quite noisy for this task, and to improve their quality, we filtered away responses from any worker who gave an inconsistent response (for example selecting \"Both Models Equally Reasonable\" and \"The More reasonable explanation is more informative\" at the same time). The filtered results are shown in Figure 22. After filtering, the benefits of LF-CBM are even more clear, with 74.4% of ratings saying LF-CBM explanations are more reasonable than Standard (dense) and 78.8% are more reasonable than Standard (sparse).\n\nInterestingly, we found that sparsity did not really improve interpretability in this evaluation for either LF-CBM or the standard model. This was against our expectations, but we think it might be Figure 20: User Interface shown to Mechanical Turk users for Task 2. In this example Model 1 is LF-CBM while Model 2 is standard dense model. because this evaluation does not take into account how comprehensive the explanation is. A dense and sparse model often have similar top-5 most highly contributing concepts, but for dense concept the top-5 only explain a few percentage of the decision, while for sparse model they often explain 60-70%, as seen in Figures 8 and 17.\n\nAs for why users found one or the other explanation more reasonable, the most common reason was \"More Relevant to the Image\"(selected 80% of the time), followed by \"More Relevant to Prediction\"( 60%), while \"More Informative\" was the least popular, only selected around 30% of the time. Figure 21: Crowdsourced results for Task 2, without filtering any responses. We can see users strongly prefer LF-CBM explanations over standard models (the first two rows), but sparsity has little effect (the 3rd row). Figure 22: Crowdsourced results for Task 2, where we filter out responses from any worker who gave at least one inconsistent response. We can see user preference LF-CBM explanations over standard models even more clearly (the first two rows).\n\nFigure 1 :\n1Our proposed Label-free CBM has many desired features which existing CBMs lack, and it can transform any neural network backbone into an interpretable Concept Bottleneck Model. 1 arXiv:2304.06129v2 [cs.LG] 5 Jun 2023\n\nFigure 3 :\n3Visualization of the final layer weights of our Label-free CBM. Showcasing how our models differentiate between two similar classes.\n\nFigure 4 :\n4Visualization of two correct decisions made by our Label-free CBM.\n\nFig 6\n6shows two examples of model edits we performed, and their effects on model predictions on the validation set.\n\nFigure 8 :\n8Dense CBM explanation for the same image as the fourth image in\n\nFigure 14 :\n14Weight visualizations for 4 randomly chosen output classes for our CBM trained on Places365.\n\nTable 1 :\n1Comparison of our method against existing methods for creating Concept Bottleneck models, CBM (Koh et al., 2020), IBD\n\nTable 2 :\n2Accuracy comparison, best performing sparse model bolded. We can see our method outperforms Posthoc-CBM and performs similarly to a sparse standard model. The results for our method are mean and standard deviation over three training runs. *Indicates reported accuracy.Result (II): Explainable decision rules. Perhaps the biggest benefit of Concept Bottleneck Models is that their decisions can be explained as a simple linear combination of understandable features. To showcase this, in\n\n3 .\n3Change weights by sufficient magnitude: Next we choose magnitude of weight change \u2206w, and edit the weights in the following way: W F [gt,concept] \u2190 W F [gt,concept] + \u2206w and W F [pred,concept] \u2190 W F [pred,concept] \u2212 \u2206w.\n\n\nIn this section we provide a brief overview of the Appendix contents. The Appendix is mostly focused on Ablation studies identifying the importance and effectiveness of different components of our pipeline, as well additional Figures to showcase more examples of our results.A APPENDIX \n\nA.1 APPENDIX OVERVIEW \n\n\n\n\nSecond, our model is aimed to extend Concept Bottleneck Models to large datasets where labeled concept data is not available. However it works best on domains where CLIP performs well and as such may not perform as well on small datasets that require specific domain knowledge and labeled concept data is available, such as medical datasets. For such datasets we believe it is still better to use a method that can effectively leverage the labels available such as original Concept Bottleneck ModelsKoh et al. (2020) or P-CBMYuksekgonul et al. (2022.\n\n\nFigure 15: Weight visualizations for 4 randomly chosen output classes for our CBM trained on ImageNet, as well as an example image of each class to clarify their meaning. 24Published as a conference paper at ICLR 2023A.12 ADDITIONAL EXPLANATIONS FOR RANDOM IMAGES In figures 16, 17, 18 we display additional samples of decision explanations for 4 randomly chosen input images for each of Places365, ImageNet and CUB-200 respectively. Overall, it can be seen that the concepts and explanations are of good quality, even with the CUB-200 dataset which may require very specific knowledge and concepts to create CBMs. This result demonstrates the effectiveness of our LF-CBM, which does not use the expert-knowledge concept sets unlike existing CBMs. B USER STUDY ON THE INTERPRETABILITY OF LF-CBMNote: \n\n1 .\n1Evaluating how well the neurons in CBL correspond to their target concepts 2. Evaluating how reasonable the explanations for the decisions of our models are B.1 TASK 1: DO NEURONS IN CBL CORRESPOND TO THEIR TARGET CONCEPT?\nACKNOWLEDGEMENTSThe authors would like to thank anonymous reviewers for valuable feedback to improve the manuscript. The authors also thank MIT-IBM Watson AI lab for support in this work. This work was done during T. Oikarinen's internship at MIT-IBM Watson AI Lab. T.-W. Weng is supported by National Science Foundation under Grant No. 2107189.\nMeaningfully explaining model mistakes using conceptual counterfactuals. Abubakar Abid, Mert Yuksekgonul, James Zou, arXiv:2106.12723arXiv preprintAbubakar Abid, Mert Yuksekgonul, and James Zou. Meaningfully explaining model mistakes using conceptual counterfactuals. arXiv preprint arXiv:2106.12723, 2021.\n\nRewriting a deep generative model. David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, European conference on computer vision. SpringerDavid Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. Rewriting a deep generative model. In European conference on computer vision, pp. 351-369. Springer, 2020.\n\n. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, W Krishna ; Armin, Florian Thomas, Rose E Tram\u00e8r, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Michael Wu, Michihiro Xie, Jiaxuan Yasunaga, Matei You, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Zheng, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan SrinivasanAnanya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent; Alex Tamkin, Rohan TaoriRohith Kuditipudi. Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation modelsRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolf- sson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Dur- mus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keel- ing, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Ku- ditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben New- man, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srini- vasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kait- lyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2021. URL https://arxiv.org/abs/2108.07258.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc33Ilya Sutskever, and Dario AmodeiTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Asso- ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nConcept whitening for interpretable image recognition. Zhi Chen, Yijie Bei, Cynthia Rudin, Nature Machine Intelligence. 212Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable image recognition. Nature Machine Intelligence, 2(12):772-782, 2020.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 10.1109/CVPR.2009.52068482009 IEEE Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier- archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.\n\nInterpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, International conference on machine learning. PMLRBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pp. 2668-2677. PMLR, 2018.\n\nConcept bottleneck models. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang, International Conference on Machine Learning. PMLRPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International Conference on Machine Learning, pp. 5338-5348. PMLR, 2020.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nInterpretability beyond classification output: Semantic bottleneck networks. Max Losch, Mario Fritz, Bernt Schiele, Max Losch, Mario Fritz, and Bernt Schiele. Interpretability beyond classification output: Semantic bottleneck networks, 2019. URL https://arxiv.org/abs/1907.10882.\n\nA unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, 30Advances in neural information processing systemsScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.\n\nLocating and editing factual associations in gpt. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, arXiv:2202.05262arXiv preprintKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. arXiv preprint arXiv:2202.05262, 2022.\n\nFast model editing at scale, 2021. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D Manning, Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale, 2021. URL https://arxiv.org/abs/2110.11309.\n\nTuomas Oikarinen, Tsui-Wei Weng, arXiv:2204.10965Clip-dissect: Automatic description of neuron representations in deep vision networks. arXiv preprintTuomas Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description of neuron representa- tions in deep vision networks. arXiv preprint arXiv:2204.10965, 2022.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar- wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020.\n\nwhy should i trust you?\" explaining the predictions of any classifier. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data miningMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.\n\nExplaining deep neural networks and beyond: A review of methods and applications. Wojciech Samek, Gr\u00e9goire Montavon, Sebastian Lapuschkin, J Christopher, Klaus-Robert Anders, M\u00fcller, Proceedings of the IEEE. 1093Wojciech Samek, Gr\u00e9goire Montavon, Sebastian Lapuschkin, Christopher J Anders, and Klaus- Robert M\u00fcller. Explaining deep neural networks and beyond: A review of methods and applica- tions. Proceedings of the IEEE, 109(3):247-278, 2021.\n\nConceptnet 5: A large semantic network for relational knowledge. The people's web meets NLP, theory and applications of natural language processing. Robyn Speer, Catherine Havasi, 10.1007/978-3-642-35085-66022013Robyn Speer and Catherine Havasi. Conceptnet 5: A large semantic network for relational knowl- edge. The people's web meets NLP, theory and applications of natural language processing, pp. 161-176, 02 2013. doi: 10.1007/978-3-642-35085-6 6.\n\nThe caltech-ucsd birds-200-2011 dataset. C Wah, S Branson, P Welinder, P Perona, S Belongie, CNS-TR-2011-001California Institute of TechnologyTechnical ReportC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\n\nAlvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Henry Jin, ; Joseph, E Gonzalez, Nbdt: Neural-backed decision trees. Suzanne Petryk, Sarah Adel BargalAlvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Henry Jin, Suzanne Petryk, Sarah Adel Bargal, and Joseph E. Gonzalez. Nbdt: Neural-backed decision trees, 2020.\n\nRewriting geometric rules of a gan. Sheng-Yu Wang, David Bau, Jun-Yan Zhu, ACM Transactions on Graphics (TOG). 414Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Rewriting geometric rules of a gan. ACM Transac- tions on Graphics (TOG), 41(4):1-16, 2022.\n\nLeveraging sparse linear layers for debuggable deep networks. Eric Wong, Shibani Santurkar, Aleksander Madry, International Conference on Machine Learning. PMLREric Wong, Shibani Santurkar, and Aleksander Madry. Leveraging sparse linear layers for de- buggable deep networks. In International Conference on Machine Learning, pp. 11205-11216. PMLR, 2021.\n\n. Mert Yuksekgonul, Maggie Wang, James Zou, arXiv:2205.15480Post-hoc concept bottleneck models. arXiv preprintMert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. arXiv preprint arXiv:2205.15480, 2022.\n\nMateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Zohreh Shams, Frederic Precioso, Stefano Melacci, Adrian Weller, arXiv:2209.09056Concept embedding models. arXiv preprintMateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra, Francesco Gian- nini, Michelangelo Diligenti, Zohreh Shams, Frederic Precioso, Stefano Melacci, Adrian Weller, et al. Concept embedding models. arXiv preprint arXiv:2209.09056, 2022.\n\nPlaces: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE Transactions on Pattern Analysis and Machine Intelligence. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 mil- lion image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nInterpretable basis decomposition for visual explanation. Bolei Zhou, Yiyou Sun, David Bau, Antonio Torralba, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for visual explanation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 119-134, 2018.\n", "annotations": {"author": "[{\"end\":147,\"start\":40},{\"end\":248,\"start\":148},{\"end\":355,\"start\":249},{\"end\":455,\"start\":356}]", "publisher": null, "author_last_name": "[{\"end\":56,\"start\":47},{\"end\":158,\"start\":155},{\"end\":261,\"start\":255},{\"end\":369,\"start\":365}]", "author_first_name": "[{\"end\":46,\"start\":40},{\"end\":154,\"start\":148},{\"end\":252,\"start\":249},{\"end\":254,\"start\":253},{\"end\":364,\"start\":356}]", "author_affiliation": "[{\"end\":146,\"start\":78},{\"end\":247,\"start\":179},{\"end\":354,\"start\":286},{\"end\":454,\"start\":386}]", "title": "[{\"end\":37,\"start\":1},{\"end\":492,\"start\":456}]", "venue": null, "abstract": "[{\"end\":1780,\"start\":538}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2536,\"start\":2518},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3134,\"start\":3116},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3159,\"start\":3134},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3177,\"start\":3159},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3754,\"start\":3730},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4104,\"start\":4084},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4209,\"start\":4187},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4241,\"start\":4219},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4838,\"start\":4819},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5100,\"start\":5082},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5347,\"start\":5324},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5432,\"start\":5414},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5451,\"start\":5432},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5792,\"start\":5766},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6037,\"start\":6019},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6090,\"start\":6068},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6562,\"start\":6543},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6928,\"start\":6910},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6977,\"start\":6959},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7017,\"start\":6998},{\"end\":7039,\"start\":7017},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7092,\"start\":7073},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7183,\"start\":7164},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7234,\"start\":7208},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7496,\"start\":7472},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7644,\"start\":7622},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9426,\"start\":9408},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9724,\"start\":9704},{\"end\":10922,\"start\":10900},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10980,\"start\":10954},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13225,\"start\":13201},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16374,\"start\":16355},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16992,\"start\":16973},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17774,\"start\":17749},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17798,\"start\":17780},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17829,\"start\":17810},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17861,\"start\":17843},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18698,\"start\":18672},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20763,\"start\":20744},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30962,\"start\":30938},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31176,\"start\":31152},{\"end\":38720,\"start\":38695},{\"end\":38860,\"start\":38839},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38901,\"start\":38876},{\"end\":44262,\"start\":44254},{\"end\":44743,\"start\":44735},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":48433,\"start\":48409},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51648,\"start\":51624},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56806,\"start\":56790}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54588,\"start\":54359},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54734,\"start\":54589},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54814,\"start\":54735},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54932,\"start\":54815},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55009,\"start\":54933},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55117,\"start\":55010},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55247,\"start\":55118},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55747,\"start\":55248},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55973,\"start\":55748},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56288,\"start\":55974},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":56841,\"start\":56289},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":57644,\"start\":56842},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":57873,\"start\":57645}]", "paragraph": "[{\"end\":3435,\"start\":1796},{\"end\":3795,\"start\":3437},{\"end\":4785,\"start\":3797},{\"end\":5348,\"start\":4787},{\"end\":6798,\"start\":5350},{\"end\":7457,\"start\":6800},{\"end\":7875,\"start\":7459},{\"end\":8393,\"start\":7924},{\"end\":8487,\"start\":8395},{\"end\":8569,\"start\":8489},{\"end\":8647,\"start\":8571},{\"end\":8902,\"start\":8649},{\"end\":8967,\"start\":8904},{\"end\":9258,\"start\":9014},{\"end\":9944,\"start\":9260},{\"end\":10020,\"start\":9946},{\"end\":10076,\"start\":10022},{\"end\":10119,\"start\":10078},{\"end\":11062,\"start\":10121},{\"end\":11200,\"start\":11064},{\"end\":11339,\"start\":11202},{\"end\":11956,\"start\":11346},{\"end\":12223,\"start\":11958},{\"end\":12513,\"start\":12230},{\"end\":12696,\"start\":12515},{\"end\":12832,\"start\":12698},{\"end\":13822,\"start\":12894},{\"end\":13974,\"start\":13869},{\"end\":14710,\"start\":14077},{\"end\":15697,\"start\":14802},{\"end\":16028,\"start\":15709},{\"end\":16573,\"start\":16072},{\"end\":17351,\"start\":16714},{\"end\":17641,\"start\":17374},{\"end\":18610,\"start\":17643},{\"end\":19828,\"start\":18612},{\"end\":20923,\"start\":19830},{\"end\":22155,\"start\":20943},{\"end\":22697,\"start\":22157},{\"end\":23472,\"start\":22744},{\"end\":24094,\"start\":23525},{\"end\":24271,\"start\":24120},{\"end\":24482,\"start\":24273},{\"end\":24964,\"start\":24484},{\"end\":25335,\"start\":24966},{\"end\":25633,\"start\":25337},{\"end\":25711,\"start\":25635},{\"end\":25882,\"start\":25743},{\"end\":26830,\"start\":25884},{\"end\":28020,\"start\":26832},{\"end\":29551,\"start\":28035},{\"end\":29738,\"start\":29553},{\"end\":30336,\"start\":29758},{\"end\":31883,\"start\":30400},{\"end\":32083,\"start\":31927},{\"end\":32217,\"start\":32085},{\"end\":32432,\"start\":32219},{\"end\":33505,\"start\":32434},{\"end\":33854,\"start\":33565},{\"end\":36615,\"start\":33856},{\"end\":37574,\"start\":36617},{\"end\":37874,\"start\":37576},{\"end\":38483,\"start\":37876},{\"end\":39957,\"start\":38531},{\"end\":40530,\"start\":40010},{\"end\":41769,\"start\":40592},{\"end\":42007,\"start\":41817},{\"end\":42109,\"start\":42009},{\"end\":42512,\"start\":42111},{\"end\":42549,\"start\":42514},{\"end\":42957,\"start\":42551},{\"end\":43564,\"start\":42959},{\"end\":43613,\"start\":43566},{\"end\":43831,\"start\":43615},{\"end\":44263,\"start\":43922},{\"end\":44398,\"start\":44300},{\"end\":44537,\"start\":44474},{\"end\":44744,\"start\":44615},{\"end\":44943,\"start\":44870},{\"end\":45689,\"start\":44945},{\"end\":46281,\"start\":45691},{\"end\":46453,\"start\":46288},{\"end\":46504,\"start\":46460},{\"end\":47264,\"start\":46565},{\"end\":47608,\"start\":47266},{\"end\":48161,\"start\":47610},{\"end\":48693,\"start\":48163},{\"end\":48920,\"start\":48695},{\"end\":49628,\"start\":48933},{\"end\":50288,\"start\":49630},{\"end\":50507,\"start\":50363},{\"end\":51314,\"start\":50518},{\"end\":51931,\"start\":51316},{\"end\":52079,\"start\":51933},{\"end\":52379,\"start\":52092},{\"end\":52935,\"start\":52381},{\"end\":53608,\"start\":52937},{\"end\":54358,\"start\":53610}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13868,\"start\":13823},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14076,\"start\":13975},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14801,\"start\":14711},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15708,\"start\":15698},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16713,\"start\":16574},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22743,\"start\":22698},{\"attributes\":{\"id\":\"formula_6\"},\"end\":43921,\"start\":43832},{\"attributes\":{\"id\":\"formula_7\"},\"end\":44299,\"start\":44264},{\"attributes\":{\"id\":\"formula_8\"},\"end\":44473,\"start\":44399},{\"attributes\":{\"id\":\"formula_9\"},\"end\":44614,\"start\":44538},{\"attributes\":{\"id\":\"formula_10\"},\"end\":44869,\"start\":44745}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":6797,\"start\":6790},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19859,\"start\":19852},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20343,\"start\":20336},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31527,\"start\":31520},{\"end\":32657,\"start\":32650},{\"end\":33308,\"start\":33301},{\"end\":37507,\"start\":37500},{\"end\":39434,\"start\":39427},{\"end\":39868,\"start\":39861},{\"end\":40163,\"start\":40156},{\"end\":48988,\"start\":48981},{\"end\":49300,\"start\":49293}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1794,\"start\":1782},{\"attributes\":{\"n\":\"3\"},\"end\":7922,\"start\":7878},{\"attributes\":{\"n\":\"3.1\"},\"end\":9012,\"start\":8970},{\"end\":11344,\"start\":11342},{\"end\":12228,\"start\":12226},{\"attributes\":{\"n\":\"3.2\"},\"end\":12892,\"start\":12835},{\"attributes\":{\"n\":\"3.3\"},\"end\":16070,\"start\":16031},{\"attributes\":{\"n\":\"4\"},\"end\":17372,\"start\":17354},{\"end\":20933,\"start\":20926},{\"end\":20941,\"start\":20936},{\"attributes\":{\"n\":\"5\"},\"end\":23523,\"start\":23475},{\"attributes\":{\"n\":\"5.1\"},\"end\":24118,\"start\":24097},{\"attributes\":{\"n\":\"5.2\"},\"end\":25741,\"start\":25714},{\"attributes\":{\"n\":\"6\"},\"end\":28033,\"start\":28023},{\"end\":29756,\"start\":29741},{\"end\":30398,\"start\":30339},{\"end\":31925,\"start\":31886},{\"end\":33563,\"start\":33508},{\"end\":38529,\"start\":38486},{\"end\":40008,\"start\":39960},{\"end\":40546,\"start\":40533},{\"end\":40590,\"start\":40549},{\"end\":41815,\"start\":41772},{\"end\":46286,\"start\":46284},{\"end\":46458,\"start\":46456},{\"end\":46563,\"start\":46507},{\"end\":48931,\"start\":48923},{\"end\":50361,\"start\":50291},{\"end\":50516,\"start\":50510},{\"end\":52090,\"start\":52082},{\"end\":54370,\"start\":54360},{\"end\":54600,\"start\":54590},{\"end\":54746,\"start\":54736},{\"end\":54821,\"start\":54816},{\"end\":54944,\"start\":54934},{\"end\":55022,\"start\":55011},{\"end\":55128,\"start\":55119},{\"end\":55258,\"start\":55249},{\"end\":55752,\"start\":55749},{\"end\":57649,\"start\":57646}]", "table": "[{\"end\":56288,\"start\":56251},{\"end\":57644,\"start\":57638}]", "figure_caption": "[{\"end\":54588,\"start\":54372},{\"end\":54734,\"start\":54602},{\"end\":54814,\"start\":54748},{\"end\":54932,\"start\":54823},{\"end\":55009,\"start\":54946},{\"end\":55117,\"start\":55025},{\"end\":55247,\"start\":55130},{\"end\":55747,\"start\":55260},{\"end\":55973,\"start\":55754},{\"end\":56251,\"start\":55976},{\"end\":56841,\"start\":56291},{\"end\":57638,\"start\":56844},{\"end\":57873,\"start\":57651}]", "figure_ref": "[{\"end\":8324,\"start\":8317},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10555,\"start\":10538},{\"end\":15983,\"start\":15975},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21015,\"start\":21007},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22959,\"start\":22950},{\"end\":25334,\"start\":25326},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25694,\"start\":25685},{\"end\":26456,\"start\":26448},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26653,\"start\":26645},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27265,\"start\":27258},{\"end\":37627,\"start\":37619},{\"end\":38387,\"start\":38379},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40365,\"start\":40357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40470,\"start\":40461},{\"end\":41247,\"start\":41239},{\"end\":41480,\"start\":41471},{\"end\":41618,\"start\":41610},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":42209,\"start\":42201},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":43012,\"start\":43004},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45729,\"start\":45715},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45904,\"start\":45895},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45994,\"start\":45984},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46065,\"start\":46056},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46178,\"start\":46169},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46342,\"start\":46333},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46408,\"start\":46399},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46762,\"start\":46753},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47848,\"start\":47839},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49232,\"start\":49223},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":50627,\"start\":50619},{\"end\":51166,\"start\":51157},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52157,\"start\":52140},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52171,\"start\":52162},{\"end\":52728,\"start\":52719},{\"end\":53144,\"start\":53135},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53607,\"start\":53591},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53906,\"start\":53897},{\"end\":54125,\"start\":54116}]", "bib_author_first_name": "[{\"end\":58301,\"start\":58293},{\"end\":58312,\"start\":58308},{\"end\":58331,\"start\":58326},{\"end\":58568,\"start\":58563},{\"end\":58580,\"start\":58574},{\"end\":58594,\"start\":58586},{\"end\":58608,\"start\":58601},{\"end\":58621,\"start\":58614},{\"end\":58868,\"start\":58863},{\"end\":58884,\"start\":58880},{\"end\":58886,\"start\":58885},{\"end\":58900,\"start\":58895},{\"end\":58912,\"start\":58908},{\"end\":58927,\"start\":58921},{\"end\":58942,\"start\":58935},{\"end\":58944,\"start\":58943},{\"end\":58970,\"start\":58961},{\"end\":58989,\"start\":58982},{\"end\":59000,\"start\":58996},{\"end\":59015,\"start\":59011},{\"end\":59034,\"start\":59027},{\"end\":59055,\"start\":59049},{\"end\":59069,\"start\":59062},{\"end\":59083,\"start\":59076},{\"end\":59100,\"start\":59095},{\"end\":59120,\"start\":59112},{\"end\":59132,\"start\":59127},{\"end\":59139,\"start\":59133},{\"end\":59151,\"start\":59147},{\"end\":59164,\"start\":59159},{\"end\":59180,\"start\":59174},{\"end\":59194,\"start\":59190},{\"end\":59213,\"start\":59206},{\"end\":59226,\"start\":59222},{\"end\":59239,\"start\":59234},{\"end\":59254,\"start\":59252},{\"end\":59274,\"start\":59267},{\"end\":59290,\"start\":59284},{\"end\":59303,\"start\":59297},{\"end\":59315,\"start\":59310},{\"end\":59331,\"start\":59327},{\"end\":59344,\"start\":59338},{\"end\":59358,\"start\":59354},{\"end\":59378,\"start\":59369},{\"end\":59390,\"start\":59385},{\"end\":59406,\"start\":59402},{\"end\":59424,\"start\":59418},{\"end\":59426,\"start\":59425},{\"end\":59440,\"start\":59435},{\"end\":59449,\"start\":59445},{\"end\":59460,\"start\":59456},{\"end\":59472,\"start\":59466},{\"end\":59486,\"start\":59480},{\"end\":59497,\"start\":59494},{\"end\":59513,\"start\":59504},{\"end\":59533,\"start\":59524},{\"end\":59548,\"start\":59543},{\"end\":59569,\"start\":59561},{\"end\":59583,\"start\":59579},{\"end\":59595,\"start\":59591},{\"end\":59599,\"start\":59596},{\"end\":59613,\"start\":59609},{\"end\":59625,\"start\":59619},{\"end\":59634,\"start\":59633},{\"end\":59659,\"start\":59652},{\"end\":59672,\"start\":59668},{\"end\":59674,\"start\":59673},{\"end\":59690,\"start\":59683},{\"end\":59702,\"start\":59697},{\"end\":59715,\"start\":59709},{\"end\":59726,\"start\":59720},{\"end\":59735,\"start\":59731},{\"end\":59743,\"start\":59736},{\"end\":59757,\"start\":59748},{\"end\":59770,\"start\":59763},{\"end\":59786,\"start\":59781},{\"end\":59799,\"start\":59792},{\"end\":59815,\"start\":59809},{\"end\":59828,\"start\":59823},{\"end\":59841,\"start\":59836},{\"end\":59854,\"start\":59849},{\"end\":62489,\"start\":62486},{\"end\":62505,\"start\":62497},{\"end\":62516,\"start\":62512},{\"end\":62531,\"start\":62524},{\"end\":62546,\"start\":62541},{\"end\":62548,\"start\":62547},{\"end\":62565,\"start\":62557},{\"end\":62582,\"start\":62576},{\"end\":62602,\"start\":62596},{\"end\":62616,\"start\":62610},{\"end\":62631,\"start\":62625},{\"end\":62648,\"start\":62640},{\"end\":62663,\"start\":62658},{\"end\":62686,\"start\":62678},{\"end\":62699,\"start\":62696},{\"end\":62715,\"start\":62710},{\"end\":62729,\"start\":62723},{\"end\":62744,\"start\":62738},{\"end\":62761,\"start\":62754},{\"end\":62773,\"start\":62766},{\"end\":62787,\"start\":62782},{\"end\":62799,\"start\":62795},{\"end\":62810,\"start\":62806},{\"end\":62826,\"start\":62819},{\"end\":63953,\"start\":63950},{\"end\":63965,\"start\":63960},{\"end\":63978,\"start\":63971},{\"end\":64220,\"start\":64217},{\"end\":64230,\"start\":64227},{\"end\":64244,\"start\":64237},{\"end\":64259,\"start\":64253},{\"end\":64267,\"start\":64264},{\"end\":64274,\"start\":64272},{\"end\":64730,\"start\":64726},{\"end\":64742,\"start\":64736},{\"end\":64761,\"start\":64755},{\"end\":64776,\"start\":64770},{\"end\":64787,\"start\":64782},{\"end\":64804,\"start\":64796},{\"end\":65171,\"start\":65167},{\"end\":65185,\"start\":65181},{\"end\":65197,\"start\":65194},{\"end\":65203,\"start\":65198},{\"end\":65217,\"start\":65210},{\"end\":65232,\"start\":65228},{\"end\":65246,\"start\":65242},{\"end\":65257,\"start\":65252},{\"end\":65580,\"start\":65576},{\"end\":65601,\"start\":65593},{\"end\":65793,\"start\":65790},{\"end\":65806,\"start\":65801},{\"end\":65819,\"start\":65814},{\"end\":66049,\"start\":66048},{\"end\":66062,\"start\":66057},{\"end\":66332,\"start\":66327},{\"end\":66344,\"start\":66339},{\"end\":66354,\"start\":66350},{\"end\":66372,\"start\":66365},{\"end\":66602,\"start\":66598},{\"end\":66620,\"start\":66613},{\"end\":66633,\"start\":66626},{\"end\":66651,\"start\":66644},{\"end\":66669,\"start\":66658},{\"end\":66671,\"start\":66670},{\"end\":66849,\"start\":66843},{\"end\":66869,\"start\":66861},{\"end\":67233,\"start\":67229},{\"end\":67247,\"start\":67243},{\"end\":67252,\"start\":67248},{\"end\":67263,\"start\":67258},{\"end\":67279,\"start\":67273},{\"end\":67295,\"start\":67288},{\"end\":67309,\"start\":67301},{\"end\":67325,\"start\":67319},{\"end\":67340,\"start\":67334},{\"end\":67355,\"start\":67349},{\"end\":67369,\"start\":67365},{\"end\":67385,\"start\":67377},{\"end\":67399,\"start\":67395},{\"end\":67792,\"start\":67786},{\"end\":67820,\"start\":67814},{\"end\":68365,\"start\":68357},{\"end\":68381,\"start\":68373},{\"end\":68401,\"start\":68392},{\"end\":68415,\"start\":68414},{\"end\":68441,\"start\":68429},{\"end\":68878,\"start\":68873},{\"end\":68895,\"start\":68886},{\"end\":69220,\"start\":69219},{\"end\":69227,\"start\":69226},{\"end\":69238,\"start\":69237},{\"end\":69250,\"start\":69249},{\"end\":69260,\"start\":69259},{\"end\":69520,\"start\":69515},{\"end\":69530,\"start\":69526},{\"end\":69545,\"start\":69539},{\"end\":69555,\"start\":69550},{\"end\":69566,\"start\":69561},{\"end\":69577,\"start\":69572},{\"end\":69584,\"start\":69583},{\"end\":69594,\"start\":69593},{\"end\":69888,\"start\":69880},{\"end\":69900,\"start\":69895},{\"end\":69913,\"start\":69906},{\"end\":70160,\"start\":70156},{\"end\":70174,\"start\":70167},{\"end\":70196,\"start\":70186},{\"end\":70455,\"start\":70451},{\"end\":70475,\"start\":70469},{\"end\":70487,\"start\":70482},{\"end\":70686,\"start\":70681},{\"end\":70695,\"start\":70687},{\"end\":70712,\"start\":70706},{\"end\":70731,\"start\":70723},{\"end\":70751,\"start\":70743},{\"end\":70768,\"start\":70759},{\"end\":70791,\"start\":70779},{\"end\":70809,\"start\":70803},{\"end\":70825,\"start\":70817},{\"end\":70843,\"start\":70836},{\"end\":70859,\"start\":70853},{\"end\":71250,\"start\":71245},{\"end\":71262,\"start\":71257},{\"end\":71280,\"start\":71274},{\"end\":71293,\"start\":71289},{\"end\":71308,\"start\":71301},{\"end\":71656,\"start\":71651},{\"end\":71668,\"start\":71663},{\"end\":71679,\"start\":71674},{\"end\":71692,\"start\":71685}]", "bib_author_last_name": "[{\"end\":58306,\"start\":58302},{\"end\":58324,\"start\":58313},{\"end\":58335,\"start\":58332},{\"end\":58572,\"start\":58569},{\"end\":58584,\"start\":58581},{\"end\":58599,\"start\":58595},{\"end\":58612,\"start\":58609},{\"end\":58630,\"start\":58622},{\"end\":58878,\"start\":58869},{\"end\":58893,\"start\":58887},{\"end\":58906,\"start\":58901},{\"end\":58919,\"start\":58913},{\"end\":58933,\"start\":58928},{\"end\":58959,\"start\":58945},{\"end\":58980,\"start\":58971},{\"end\":58994,\"start\":58990},{\"end\":59009,\"start\":59001},{\"end\":59025,\"start\":59016},{\"end\":59047,\"start\":59035},{\"end\":59060,\"start\":59056},{\"end\":59074,\"start\":59070},{\"end\":59093,\"start\":59084},{\"end\":59110,\"start\":59101},{\"end\":59125,\"start\":59121},{\"end\":59145,\"start\":59140},{\"end\":59157,\"start\":59152},{\"end\":59172,\"start\":59165},{\"end\":59188,\"start\":59181},{\"end\":59204,\"start\":59195},{\"end\":59220,\"start\":59214},{\"end\":59232,\"start\":59227},{\"end\":59250,\"start\":59240},{\"end\":59265,\"start\":59255},{\"end\":59282,\"start\":59275},{\"end\":59295,\"start\":59291},{\"end\":59308,\"start\":59304},{\"end\":59325,\"start\":59316},{\"end\":59336,\"start\":59332},{\"end\":59352,\"start\":59345},{\"end\":59367,\"start\":59359},{\"end\":59383,\"start\":59379},{\"end\":59400,\"start\":59391},{\"end\":59416,\"start\":59407},{\"end\":59433,\"start\":59427},{\"end\":59443,\"start\":59441},{\"end\":59454,\"start\":59450},{\"end\":59464,\"start\":59461},{\"end\":59478,\"start\":59473},{\"end\":59492,\"start\":59487},{\"end\":59502,\"start\":59498},{\"end\":59522,\"start\":59514},{\"end\":59541,\"start\":59534},{\"end\":59559,\"start\":59549},{\"end\":59577,\"start\":59570},{\"end\":59589,\"start\":59584},{\"end\":59607,\"start\":59600},{\"end\":59617,\"start\":59614},{\"end\":59631,\"start\":59626},{\"end\":59650,\"start\":59635},{\"end\":59666,\"start\":59660},{\"end\":59681,\"start\":59675},{\"end\":59695,\"start\":59691},{\"end\":59707,\"start\":59703},{\"end\":59718,\"start\":59716},{\"end\":59729,\"start\":59727},{\"end\":59746,\"start\":59744},{\"end\":59761,\"start\":59758},{\"end\":59779,\"start\":59771},{\"end\":59790,\"start\":59787},{\"end\":59807,\"start\":59800},{\"end\":59821,\"start\":59816},{\"end\":59834,\"start\":59829},{\"end\":59847,\"start\":59842},{\"end\":59860,\"start\":59855},{\"end\":59867,\"start\":59862},{\"end\":62495,\"start\":62490},{\"end\":62510,\"start\":62506},{\"end\":62522,\"start\":62517},{\"end\":62539,\"start\":62532},{\"end\":62555,\"start\":62549},{\"end\":62574,\"start\":62566},{\"end\":62594,\"start\":62583},{\"end\":62608,\"start\":62603},{\"end\":62623,\"start\":62617},{\"end\":62638,\"start\":62632},{\"end\":62656,\"start\":62649},{\"end\":62676,\"start\":62664},{\"end\":62694,\"start\":62687},{\"end\":62708,\"start\":62700},{\"end\":62721,\"start\":62716},{\"end\":62736,\"start\":62730},{\"end\":62752,\"start\":62745},{\"end\":62764,\"start\":62762},{\"end\":62780,\"start\":62774},{\"end\":62793,\"start\":62788},{\"end\":62804,\"start\":62800},{\"end\":62817,\"start\":62811},{\"end\":62833,\"start\":62827},{\"end\":63958,\"start\":63954},{\"end\":63969,\"start\":63966},{\"end\":63984,\"start\":63979},{\"end\":64225,\"start\":64221},{\"end\":64235,\"start\":64231},{\"end\":64251,\"start\":64245},{\"end\":64262,\"start\":64260},{\"end\":64270,\"start\":64268},{\"end\":64282,\"start\":64275},{\"end\":64734,\"start\":64731},{\"end\":64753,\"start\":64743},{\"end\":64768,\"start\":64762},{\"end\":64780,\"start\":64777},{\"end\":64794,\"start\":64788},{\"end\":64811,\"start\":64805},{\"end\":65179,\"start\":65172},{\"end\":65192,\"start\":65186},{\"end\":65208,\"start\":65204},{\"end\":65226,\"start\":65218},{\"end\":65240,\"start\":65233},{\"end\":65250,\"start\":65247},{\"end\":65263,\"start\":65258},{\"end\":65591,\"start\":65581},{\"end\":65608,\"start\":65602},{\"end\":65799,\"start\":65794},{\"end\":65812,\"start\":65807},{\"end\":65827,\"start\":65820},{\"end\":66055,\"start\":66050},{\"end\":66071,\"start\":66063},{\"end\":66076,\"start\":66073},{\"end\":66337,\"start\":66333},{\"end\":66348,\"start\":66345},{\"end\":66363,\"start\":66355},{\"end\":66381,\"start\":66373},{\"end\":66611,\"start\":66603},{\"end\":66624,\"start\":66621},{\"end\":66642,\"start\":66634},{\"end\":66656,\"start\":66652},{\"end\":66679,\"start\":66672},{\"end\":66859,\"start\":66850},{\"end\":66874,\"start\":66870},{\"end\":67241,\"start\":67234},{\"end\":67256,\"start\":67253},{\"end\":67271,\"start\":67264},{\"end\":67286,\"start\":67280},{\"end\":67299,\"start\":67296},{\"end\":67317,\"start\":67310},{\"end\":67332,\"start\":67326},{\"end\":67347,\"start\":67341},{\"end\":67363,\"start\":67356},{\"end\":67375,\"start\":67370},{\"end\":67393,\"start\":67386},{\"end\":67409,\"start\":67400},{\"end\":67812,\"start\":67793},{\"end\":67826,\"start\":67821},{\"end\":67836,\"start\":67828},{\"end\":68371,\"start\":68366},{\"end\":68390,\"start\":68382},{\"end\":68412,\"start\":68402},{\"end\":68427,\"start\":68416},{\"end\":68448,\"start\":68442},{\"end\":68456,\"start\":68450},{\"end\":68884,\"start\":68879},{\"end\":68902,\"start\":68896},{\"end\":69224,\"start\":69221},{\"end\":69235,\"start\":69228},{\"end\":69247,\"start\":69239},{\"end\":69257,\"start\":69251},{\"end\":69269,\"start\":69261},{\"end\":69524,\"start\":69521},{\"end\":69537,\"start\":69531},{\"end\":69548,\"start\":69546},{\"end\":69559,\"start\":69556},{\"end\":69570,\"start\":69567},{\"end\":69581,\"start\":69578},{\"end\":69591,\"start\":69585},{\"end\":69603,\"start\":69595},{\"end\":69893,\"start\":69889},{\"end\":69904,\"start\":69901},{\"end\":69917,\"start\":69914},{\"end\":70165,\"start\":70161},{\"end\":70184,\"start\":70175},{\"end\":70202,\"start\":70197},{\"end\":70467,\"start\":70456},{\"end\":70480,\"start\":70476},{\"end\":70491,\"start\":70488},{\"end\":70704,\"start\":70696},{\"end\":70721,\"start\":70713},{\"end\":70741,\"start\":70732},{\"end\":70757,\"start\":70752},{\"end\":70777,\"start\":70769},{\"end\":70801,\"start\":70792},{\"end\":70815,\"start\":70810},{\"end\":70834,\"start\":70826},{\"end\":70851,\"start\":70844},{\"end\":70866,\"start\":70860},{\"end\":71255,\"start\":71251},{\"end\":71272,\"start\":71263},{\"end\":71287,\"start\":71281},{\"end\":71299,\"start\":71294},{\"end\":71317,\"start\":71309},{\"end\":71661,\"start\":71657},{\"end\":71672,\"start\":71669},{\"end\":71683,\"start\":71680},{\"end\":71701,\"start\":71693}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2106.12723\",\"id\":\"b0\"},\"end\":58526,\"start\":58220},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220871229},\"end\":58859,\"start\":58528},{\"attributes\":{\"id\":\"b2\"},\"end\":62445,\"start\":58861},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":63893,\"start\":62447},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211031886},\"end\":64162,\"start\":63895},{\"attributes\":{\"doi\":\"10.1109/CVPR.2009.5206848\",\"id\":\"b5\",\"matched_paper_id\":57246310},\"end\":64618,\"start\":64164},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51737170},\"end\":65138,\"start\":64620},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220424448},\"end\":65519,\"start\":65140},{\"attributes\":{\"id\":\"b8\"},\"end\":65711,\"start\":65521},{\"attributes\":{\"id\":\"b9\"},\"end\":65992,\"start\":65713},{\"attributes\":{\"id\":\"b10\"},\"end\":66275,\"start\":65994},{\"attributes\":{\"doi\":\"arXiv:2202.05262\",\"id\":\"b11\"},\"end\":66561,\"start\":66277},{\"attributes\":{\"id\":\"b12\"},\"end\":66841,\"start\":66563},{\"attributes\":{\"doi\":\"arXiv:2204.10965\",\"id\":\"b13\"},\"end\":67156,\"start\":66843},{\"attributes\":{\"id\":\"b14\"},\"end\":67713,\"start\":67158},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13029170},\"end\":68273,\"start\":67715},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":232046495},\"end\":68722,\"start\":68275},{\"attributes\":{\"doi\":\"10.1007/978-3-642-35085-66\",\"id\":\"b17\"},\"end\":69176,\"start\":68724},{\"attributes\":{\"doi\":\"CNS-TR-2011-001\",\"id\":\"b18\"},\"end\":69513,\"start\":69178},{\"attributes\":{\"id\":\"b19\"},\"end\":69842,\"start\":69515},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":250956766},\"end\":70092,\"start\":69844},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":234357698},\"end\":70447,\"start\":70094},{\"attributes\":{\"doi\":\"arXiv:2205.15480\",\"id\":\"b22\"},\"end\":70679,\"start\":70449},{\"attributes\":{\"doi\":\"arXiv:2209.09056\",\"id\":\"b23\"},\"end\":71184,\"start\":70681},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2608922},\"end\":71591,\"start\":71186},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51903952},\"end\":72019,\"start\":71593}]", "bib_title": "[{\"end\":58561,\"start\":58528},{\"end\":62484,\"start\":62447},{\"end\":63948,\"start\":63895},{\"end\":64215,\"start\":64164},{\"end\":64724,\"start\":64620},{\"end\":65165,\"start\":65140},{\"end\":67784,\"start\":67715},{\"end\":68355,\"start\":68275},{\"end\":69878,\"start\":69844},{\"end\":70154,\"start\":70094},{\"end\":71243,\"start\":71186},{\"end\":71649,\"start\":71593}]", "bib_author": "[{\"end\":58308,\"start\":58293},{\"end\":58326,\"start\":58308},{\"end\":58337,\"start\":58326},{\"end\":58574,\"start\":58563},{\"end\":58586,\"start\":58574},{\"end\":58601,\"start\":58586},{\"end\":58614,\"start\":58601},{\"end\":58632,\"start\":58614},{\"end\":58880,\"start\":58863},{\"end\":58895,\"start\":58880},{\"end\":58908,\"start\":58895},{\"end\":58921,\"start\":58908},{\"end\":58935,\"start\":58921},{\"end\":58961,\"start\":58935},{\"end\":58982,\"start\":58961},{\"end\":58996,\"start\":58982},{\"end\":59011,\"start\":58996},{\"end\":59027,\"start\":59011},{\"end\":59049,\"start\":59027},{\"end\":59062,\"start\":59049},{\"end\":59076,\"start\":59062},{\"end\":59095,\"start\":59076},{\"end\":59112,\"start\":59095},{\"end\":59127,\"start\":59112},{\"end\":59147,\"start\":59127},{\"end\":59159,\"start\":59147},{\"end\":59174,\"start\":59159},{\"end\":59190,\"start\":59174},{\"end\":59206,\"start\":59190},{\"end\":59222,\"start\":59206},{\"end\":59234,\"start\":59222},{\"end\":59252,\"start\":59234},{\"end\":59267,\"start\":59252},{\"end\":59284,\"start\":59267},{\"end\":59297,\"start\":59284},{\"end\":59310,\"start\":59297},{\"end\":59327,\"start\":59310},{\"end\":59338,\"start\":59327},{\"end\":59354,\"start\":59338},{\"end\":59369,\"start\":59354},{\"end\":59385,\"start\":59369},{\"end\":59402,\"start\":59385},{\"end\":59418,\"start\":59402},{\"end\":59435,\"start\":59418},{\"end\":59445,\"start\":59435},{\"end\":59456,\"start\":59445},{\"end\":59466,\"start\":59456},{\"end\":59480,\"start\":59466},{\"end\":59494,\"start\":59480},{\"end\":59504,\"start\":59494},{\"end\":59524,\"start\":59504},{\"end\":59543,\"start\":59524},{\"end\":59561,\"start\":59543},{\"end\":59579,\"start\":59561},{\"end\":59591,\"start\":59579},{\"end\":59609,\"start\":59591},{\"end\":59619,\"start\":59609},{\"end\":59633,\"start\":59619},{\"end\":59652,\"start\":59633},{\"end\":59668,\"start\":59652},{\"end\":59683,\"start\":59668},{\"end\":59697,\"start\":59683},{\"end\":59709,\"start\":59697},{\"end\":59720,\"start\":59709},{\"end\":59731,\"start\":59720},{\"end\":59748,\"start\":59731},{\"end\":59763,\"start\":59748},{\"end\":59781,\"start\":59763},{\"end\":59792,\"start\":59781},{\"end\":59809,\"start\":59792},{\"end\":59823,\"start\":59809},{\"end\":59836,\"start\":59823},{\"end\":59849,\"start\":59836},{\"end\":59862,\"start\":59849},{\"end\":59869,\"start\":59862},{\"end\":62497,\"start\":62486},{\"end\":62512,\"start\":62497},{\"end\":62524,\"start\":62512},{\"end\":62541,\"start\":62524},{\"end\":62557,\"start\":62541},{\"end\":62576,\"start\":62557},{\"end\":62596,\"start\":62576},{\"end\":62610,\"start\":62596},{\"end\":62625,\"start\":62610},{\"end\":62640,\"start\":62625},{\"end\":62658,\"start\":62640},{\"end\":62678,\"start\":62658},{\"end\":62696,\"start\":62678},{\"end\":62710,\"start\":62696},{\"end\":62723,\"start\":62710},{\"end\":62738,\"start\":62723},{\"end\":62754,\"start\":62738},{\"end\":62766,\"start\":62754},{\"end\":62782,\"start\":62766},{\"end\":62795,\"start\":62782},{\"end\":62806,\"start\":62795},{\"end\":62819,\"start\":62806},{\"end\":62835,\"start\":62819},{\"end\":63960,\"start\":63950},{\"end\":63971,\"start\":63960},{\"end\":63986,\"start\":63971},{\"end\":64227,\"start\":64217},{\"end\":64237,\"start\":64227},{\"end\":64253,\"start\":64237},{\"end\":64264,\"start\":64253},{\"end\":64272,\"start\":64264},{\"end\":64284,\"start\":64272},{\"end\":64736,\"start\":64726},{\"end\":64755,\"start\":64736},{\"end\":64770,\"start\":64755},{\"end\":64782,\"start\":64770},{\"end\":64796,\"start\":64782},{\"end\":64813,\"start\":64796},{\"end\":65181,\"start\":65167},{\"end\":65194,\"start\":65181},{\"end\":65210,\"start\":65194},{\"end\":65228,\"start\":65210},{\"end\":65242,\"start\":65228},{\"end\":65252,\"start\":65242},{\"end\":65265,\"start\":65252},{\"end\":65593,\"start\":65576},{\"end\":65610,\"start\":65593},{\"end\":65801,\"start\":65790},{\"end\":65814,\"start\":65801},{\"end\":65829,\"start\":65814},{\"end\":66057,\"start\":66048},{\"end\":66073,\"start\":66057},{\"end\":66078,\"start\":66073},{\"end\":66339,\"start\":66327},{\"end\":66350,\"start\":66339},{\"end\":66365,\"start\":66350},{\"end\":66383,\"start\":66365},{\"end\":66613,\"start\":66598},{\"end\":66626,\"start\":66613},{\"end\":66644,\"start\":66626},{\"end\":66658,\"start\":66644},{\"end\":66681,\"start\":66658},{\"end\":66861,\"start\":66843},{\"end\":66876,\"start\":66861},{\"end\":67243,\"start\":67229},{\"end\":67258,\"start\":67243},{\"end\":67273,\"start\":67258},{\"end\":67288,\"start\":67273},{\"end\":67301,\"start\":67288},{\"end\":67319,\"start\":67301},{\"end\":67334,\"start\":67319},{\"end\":67349,\"start\":67334},{\"end\":67365,\"start\":67349},{\"end\":67377,\"start\":67365},{\"end\":67395,\"start\":67377},{\"end\":67411,\"start\":67395},{\"end\":67814,\"start\":67786},{\"end\":67828,\"start\":67814},{\"end\":67838,\"start\":67828},{\"end\":68373,\"start\":68357},{\"end\":68392,\"start\":68373},{\"end\":68414,\"start\":68392},{\"end\":68429,\"start\":68414},{\"end\":68450,\"start\":68429},{\"end\":68458,\"start\":68450},{\"end\":68886,\"start\":68873},{\"end\":68904,\"start\":68886},{\"end\":69226,\"start\":69219},{\"end\":69237,\"start\":69226},{\"end\":69249,\"start\":69237},{\"end\":69259,\"start\":69249},{\"end\":69271,\"start\":69259},{\"end\":69526,\"start\":69515},{\"end\":69539,\"start\":69526},{\"end\":69550,\"start\":69539},{\"end\":69561,\"start\":69550},{\"end\":69572,\"start\":69561},{\"end\":69583,\"start\":69572},{\"end\":69593,\"start\":69583},{\"end\":69605,\"start\":69593},{\"end\":69895,\"start\":69880},{\"end\":69906,\"start\":69895},{\"end\":69919,\"start\":69906},{\"end\":70167,\"start\":70156},{\"end\":70186,\"start\":70167},{\"end\":70204,\"start\":70186},{\"end\":70469,\"start\":70451},{\"end\":70482,\"start\":70469},{\"end\":70493,\"start\":70482},{\"end\":70706,\"start\":70681},{\"end\":70723,\"start\":70706},{\"end\":70743,\"start\":70723},{\"end\":70759,\"start\":70743},{\"end\":70779,\"start\":70759},{\"end\":70803,\"start\":70779},{\"end\":70817,\"start\":70803},{\"end\":70836,\"start\":70817},{\"end\":70853,\"start\":70836},{\"end\":70868,\"start\":70853},{\"end\":71257,\"start\":71245},{\"end\":71274,\"start\":71257},{\"end\":71289,\"start\":71274},{\"end\":71301,\"start\":71289},{\"end\":71319,\"start\":71301},{\"end\":71663,\"start\":71651},{\"end\":71674,\"start\":71663},{\"end\":71685,\"start\":71674},{\"end\":71703,\"start\":71685}]", "bib_venue": "[{\"end\":58291,\"start\":58220},{\"end\":58670,\"start\":58632},{\"end\":62884,\"start\":62835},{\"end\":64013,\"start\":63986},{\"end\":64372,\"start\":64309},{\"end\":64857,\"start\":64813},{\"end\":65309,\"start\":65265},{\"end\":65574,\"start\":65521},{\"end\":65788,\"start\":65713},{\"end\":66046,\"start\":65994},{\"end\":66325,\"start\":66277},{\"end\":66596,\"start\":66563},{\"end\":66977,\"start\":66892},{\"end\":67227,\"start\":67158},{\"end\":67936,\"start\":67838},{\"end\":68481,\"start\":68458},{\"end\":68871,\"start\":68724},{\"end\":69217,\"start\":69178},{\"end\":69639,\"start\":69605},{\"end\":69953,\"start\":69919},{\"end\":70248,\"start\":70204},{\"end\":70908,\"start\":70884},{\"end\":71381,\"start\":71319},{\"end\":71767,\"start\":71703},{\"end\":63036,\"start\":62948},{\"end\":68021,\"start\":67938},{\"end\":69674,\"start\":69641},{\"end\":71818,\"start\":71769}]"}}}, "year": 2023, "month": 12, "day": 17}