{"id": 198339347, "updated": "2023-04-05 03:18:06.875", "metadata": {"title": "A Late Fusion CNN for Digital Matting", "authors": "[{\"first\":\"Yunke\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lixue\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Lubin\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Peiran\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Qixing\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Hujun\",\"last\":\"Bao\",\"middle\":[]},{\"first\":\"Weiwei\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 6, "day": 1}, "abstract": "This paper studies the structure of a deep convolutional neural network to predict the foreground alpha matte by taking a single RGB image as input. Our network is fully convolutional with two decoder branches for the foreground and background classification respectively. Then a fusion branch is used to integrate the two classification results which gives rise to alpha values as the soft segmentation result. This design provides more degrees of freedom than a single decoder branch for the network to obtain better alpha values during training. The network can implicitly produce trimaps without user interaction, which is easy to use for novices without expertise in digital matting. Experimental results demonstrate that our network can achieve high-quality alpha mattes for various types of objects and outperform the state-of-the-art CNN-based image matting methods on the human image matting task.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2955340181", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhangGFRHBX19", "doi": "10.1109/cvpr.2019.00765"}}, "content": {"source": {"pdf_hash": "18a1b359edf968397549516b213b76d8385f32bd", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "afe3201063c8c020588cbae176b92df38df11963", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/18a1b359edf968397549516b213b76d8385f32bd.txt", "contents": "\nA Late Fusion CNN for Digital Matting\n\n\nYunke Zhang \nZhejiang University\n\n\nLixue Gong gonglx@zju.edu.cn \nZhejiang University\n\n\nLubin Fan \nAlibaba Group\n\n\nPeiran Ren peiran.rpr@alibaba-inc.com \nAlibaba Group\n\n\nQixing Huang huangqx@cs.uteaxs.edu \nUniversity of Texas at Austin\n\n\nHujun Bao bao@cad.zju.edu.cn \nZhejiang University\n\n\nWeiwei Xu \nZhejiang University\n\n\nA Late Fusion CNN for Digital Matting\n10.1109/CVPR.2019.00765\nThis paper studies the structure of a deep convolutional neural network to predict the foreground alpha matte by taking a single RGB image as input. Our network is fully convolutional with two decoder branches for the foreground and background classification respectively. Then a fusion branch is used to integrate the two classification results which gives rise to alpha values as the soft segmentation result. This design provides more degrees of freedom than a single decoder branch for the network to obtain better alpha values during training. The network can implicitly produce trimaps without user interaction, which is easy to use for novices without expertise in digital matting. Experimental results demonstrate that our network can achieve high-quality alpha mattes for various types of objects and outperform the state-of-the-art CNN-based image matting methods on the human image matting task. * Corresponding author. The authors from Zhejiang University are affiliated with the State Key Lab of CAD&CG.\n\nIntroduction\n\nDigital matting is to accurately extract the foreground object in an image for object-level image composition. It has the advantage of estimating the alpha (opacity) values of the pixels to create an alpha matte so that the foreground object can be correctly abstracted and then composed with a new background image to render a new scene. Formally speaking, we assume that the observed image I is generated from three underlying images: the foreground image F, the background image B, and the alpha matte \u03b1, through the following model:\nI p = \u03b1 p F p + (1 \u2212 \u03b1 p )B p(1)\nwhere p represents a pixel across all images, and the value of \u03b1 p \u2208 [0, 1].\n\nA common approach to digital matting [10,15,36] proceeds in three steps, namely, (1) learn the foreground and Image\n\nOur alpha matte Composition Figure 1. Two example matting results of our late fusion CNN that does not need trimaps as input. Left: two images collected from internet outside of our training dataset. Middle: the alpha mattes predicted by our network. Right: the composition results. background color models, (2) compute the probabilities of each pixel belonging to the learned models, and (3) obtain the alpha values. To this end, a critical task in digital matting is to determine the pixel alpha values \u03b1, which represents a soft segmentation of the image. [20] leverages the spectral clustering to compute \u03b1. However, such methods usually rely on user-inputs such as trimaps and scribbles: The trimap separates an image into foreground region, background region and a transition region to cover fuzzy or transparent foreground object boundaries [10], while the scribbles specify sparse pixels on the foreground and background [35]. Early works exploit the local color as the main feature, which may lead to blurred or chunky artifacts as shown in [39]. Recent works (e.g., [9,31,38,39]) leverage fully convolutional neural network (CNN) to learn multi-scale features, which lead to high-quality semantic image segmentation results. In addition, deep image matting (DIM) [39] has shown that high-quality alpha mattes can be directly predicted through a deep CNN trained on a large-scale image matting dataset. A recent contribution combines the multi-scale features learned by deep CNN and the spectral matting method to obtain alpha matte [2]. It is fully automatic but has the disadvantage of slow performance due to solving the large-scale spectral problem.\n\nWhile existing deep learning based digital matting approaches rely on a trimap as input, we propose a fully convolutional network (FCN) for automatic image matting by taking a single RGB image as input. We achieve this goal by designing two decoder branches in the network for the foreground and background classification, and then use a fusion branch to integrate the two classification results, which gives rise to the soft segmentation result. This design provides more degrees of freedom than a single decoder branch for the network to obtain better alpha values. It is based on the observation that the classification branches can well predict the hard segmentation result, but have difficulties in predicting precise probabilities as the alpha values at the pixel level. The two-decoder branch structure allows us to design a fusion branch to correct the residuals left in the classification branches. Moreover, our training loss encourages the two decoder branches to agree with each other at the hard segmentation part and leave the soft segmentation part to be corrected by the fusion branch. Therefore, our approach can implicitly produce trimap without any user interaction, which is easy to use for novice users without expertise in digital matting.\n\nOur two-branch network structure at the decoder stage of FCN follows the late fusion structure which is widely used in deep learning [8,26] and can be categorized as a type of ensemble learning to improve the accuracy of the predicated alpha values. However, instead of simply maximizing or averaging the output of the two classification branches, we learn the fusion weights. We thus denote our network by late fusion CNN hereafter.\n\nWe have evaluated our network on the image matting dataset in [39] to shown that it can produce high-quality matting results for different types of objects. In addition, we also construct a human image matting dataset to test the network on this specific type of images. Fig. 1 illustrates the matting results of our network on two internet images, and the experimental results show that our network that does not need trimaps as input can still achieve comparable results to the state-of-the-art CNN-based methods and outperform on the human image matting task.\n\n\nRelated Work\n\nIn this section, we briefly review three main approaches, such as sampling-based, affinity-based, and deep learning based approaches, to digital image matting.\n\nSampling-based approaches [10,13,14,16,29] use the color of sampled pixels to infer the alpha values of the pixels in the transition region in an image. The key tasks of these approaches are (1) to collect the sampled pixels [13,16,29,30], and (2) to build a foreground and background color model from the sampled pixels [10,16,34,35]. These approaches take the advantage of natural image statistics for solving the ill-posed matting problem and work well when the trimap is carefully defined so that there are strong correlations between the pixel color distribution of the transition region and that of the foreground/background regions. Affinity-based approaches [1,2,3,7,15,19,20,33] propagate the alpha values of the known foreground and background pixels to the unknown regions and have proven to be more robust than sampling-based approaches when dealing with complex images [10,14,29]. The quality of generated alpha mattes using these approaches is highly related to the defined affinity score [15,19,33]. Global optimization strategies, such as spectral techniques [20], are continuous relaxations of binary optimization techniques, which is not guaranteed to obtain optimal solutions. For a comprehensive survey of traditional approaches, we refer the readers to [37] for more details.\n\nDeep learning based matting approaches directly learn a mapping from an input image to its alpha matte from largescale labeled results. Cho et al. [9] proposed an end-to-end CNN by combing the closed-form matting formulation described in [19] and the methodology of KNN mating [7]. Xu et al. [39] integrated an encoder-decoder network and a subsequent detail refinement network for digital matting, which takes an image and the corresponding trimap as inputs. Lutz et al. [25] presented a generative adversarial network for image matting. They improved the decoder structure of [39] by adding the atrous spatial pyramid pooling module [5] to resample the features at several scales. Wang et al. [38] proposed a deep propagation based image matting framework by learning an alpha matte propagation principle using a deep neural network. However, these techniques require a trimap as input to initialize the propagation process. Several recent techniques study image matting for a specific type of objects. Chen et al. [6] proposed an automatic approach for human matting. It takes an RGB image as input and first predicts the foreground and background regions as well as the transition region using the three-class segmentation network. The segmentation result is then used as a trimap for the alpha matte generation. In contrast, our approach generates the final alpha matte by blending the foreground and background probability maps using a fusion network, which avoids the difficult trimap generation problem. The CNNbased portrait matting in [31] uses an average mask as a trimap by assuming the upper body appears at similar positions in portrait images. However, this assumption does not apply in our setting. Zhu et al. [41] followed the similar pipeline while designing a smaller network and a fast filter similar to the guided filter for matting to deploy the model on mobile phones. Chen et al. [4] formulates transparent object matting as reflective flow estimation and leverages a multi-scale encoder-decoder network for prediction.  \n\n\nApproach\n\nIn this section, we introduce the technical details of our approach. We begin with the approach overview in Sec. 3.1. We then elaborate the structure and training loss of segmentation and fusion networks in Sec. 3.2 and 3.3. Finally, we give the training details of our network in Sec. 3.4.\n\n\nApproach Overview\n\nWe introduce a novel end-to-end neural network that takes an image containing a foreground object as input and outputs an alpha matte of the foreground object. 1 As illustrated in Fig. 2, the key idea of our approach is to use neural network modules to predict three maps, namely, the foreground probability map, the background probability map, and the blending weight map. The output alpha matte is given by using the blending weight map to interpolate the foreground/background probability maps. The network is trained over three consecutive steps: segmentation network pre-training step, fusion network pre-training step and finally end-to-end joint training step whose training loss is imposed on the output alpha matte.\n\nFormally speaking, we try to predict the alpha values with the following fusion formula:\n\u03b1 p = \u03b2 pFp + (1 \u2212 \u03b2 p )(1 \u2212B p ),(2)\nwhereF p andB p represent the predicted foreground and background probability at pixel p. \u03b2 p is the blending weight predicted by the fusion network. In our implementation, the fusion network takes the input image and the features before the logistic regression of foreground and background classification branches as input (see Fig. 2).\n\nFrom the optimization perspective, the derivative of \u03b1 p with respect to \u03b2 p vanishes when\nB p +F p = 1.(3)\nThe advantages of Eq. 2 are two-fold. First, the fusion network will focus on learning the transition region from the foreground to the background if the predictions of the foreground/background probability maps are accurate (meaning Eq. 3 is satisfied), which is the bottleneck for solving the matting problem. Second, we can carefully design the loss function to encourage that theF p +B p = 1 within the transition region (see Sec. 3.2), which can provide useful gradients to train the fusion network.\n\n\nSegmentation Network\n\nWe proceed to describe the architecture of the segmentation network and its training loss. In particular, the training loss favors 0 or 1 probability of solid foreground and background regions. It also tries to predict the upper bound and lower bound of the true alpha values in the transition region. Network structure. The segmentation network consists of one encoder and two decoders. The encoder extracts semantic features from the input image. The two decoders share the same encoded bottleneck and predict the foreground and background probability maps, respectively. Specifically, we use DenseNet-201 [18] without the fully-connected layer head as our encoder. Each branch consists of five decoder blocks which correspond to the five encoder blocks, and the decoder block follows the design of feature pyramid network structure in [22]. To enhance the pixel-level segmentation result, we employ the skip connections in [28] to concatenate the multi-scale features from the encoder block (right before the average down-sampling) with the features upsampled through deconvolution layer.\n\nTraining loss. The training loss combines the L1 loss, the L2 loss, and the cross-entropy loss. In particular, we control the behavior of the training process of our network by setting different weights for different pixels according to the alpha matte.\n\nWe first measure the difference between the predicted probability values and the ground truth alpha values:\nL d (F p ) = |F p \u2212 \u03b1 p |, 0 < \u03b1 p < 1. (F p \u2212 \u03b1 p ) 2 , \u03b1 p = 0, 1.(4)\nThe difference is chosen to be L1 inside transition regions so as to recover the details of the alpha matte there, while the L2 loss is used in the rest of the regions to penalize the possible segmentation error. We find this setting can well balance between the soft segmentation and the hard segmentation.\n\nWe also introduce the L1 loss on the gradients of the predicted alpha matte since it is beneficial to remove the over-blurred alpha matte after classification:\nL g (F p ) = |\u2207 x (F p )\u2212\u2207 x (\u03b1 p )|+|\u2207 y (F p )\u2212\u2207 y (\u03b1 p )|. (5)\nThe cross-entropy (CE) loss for the foreground classification branch at a pixel p is given by:\nCE(F p ) = w p \u00b7 (\u2212\u03b1 p log(F p ) \u2212 (1 \u2212\u03b1 p ) log(1 \u2212F p )),(6)\nThe weight w p is set to 1 when \u03b1 p = 1 or 0 and set to 0.5 when \u03b1 p is in (0, 1). We let\u03b1 p be 1 inside both the foreground and the transition regions (0 inside background region) so that the cross-entropy loss encourages the segmentation network to output probability value towards 1 for an upper bound. However, it does not provide useful gradients within the transition region. We thus adopt a small weight in the transition region and combine it with the L1 and L2 loss below to obtain a preliminary alpha matte.\n\nThe final loss function of the foreground classification branch with respect to an image is:\nL F = p CE(F p ) + L d (F p ) + L g (F p ).(7)\nFor the background classification branch, its loss L B can be simply computed by setting \u03b1 p = 1 \u2212 \u03b1 p in Eq. 1, 4 and 5. We also impose the L F and L B loss at each decoder block of two branches to further regulate the behavior of the network, similar to the side loss used in [24].\n\nNote that the combination of the cross-entropy and the L1 loss inside the transition regions tries to give larger probabilities than the ground truth values since the cross-entropy loss will drag the probabilities to 1. Thus, the true alpha values can be bracketed in the interval formed by the two probabilities predicted by the two branches, since the 1 \u2212B p in Eq. 2 should be less than the \u03b1 p in our setting. This design enables us to regress for the precise alpha values after Figure 3. Implicit trimaps predicted by our network for two images in Fig. 1. The implicit transition regions are indicated by gray pixels where the predicted foreground/background probabilities are less than 1.\n\napplying the fusion network. Moreover, enforcing the foreground and background segmentation branches to be trained with different losses helps to learn different features of the input image. These characteristics benefit the result of ensemble learning. As illustrated in Fig. 3 and Fig. 4, this design of the segmentation loss does lead to the generation of meaningful implicit trimaps. Moreover, the alpha values between 0 and 1 are mostly bracketed by the two predicted probabilities.\n\n\nFusion Network\n\nThe goal of the fusion network is to output \u03b2 p at pixels to fuse the foreground and background classification results.\n\nNetwork structure. It is a fully convolutional network with five convolution layers and one sigmoid layer to compute the blending weights \u03b2 p (see Fig. 2). The input of the network consists of (1) the feature maps from the last block of the foreground and background decoders; (2) the feature from the convolution with the input RGB image. We set the size of convolution kernel to 3 \u00d7 3 according to the experiments and found that the fusion network with this kernel size can better produce the details of the alpha matte.\n\nTraining loss. Assuming that the foreground and background decoders already provide reasonable segmentation results for the solid pixels, we design the training loss to lean towards pixels in the transition region. The loss function of the fusion network can be directly derived according to Eq. 2:\nL u = p w p \u00b7 |\u03b2 pFp + (1 \u2212 \u03b2 p )(1 \u2212B p ) \u2212 \u03b1 p |. (8)\nSpecifically, the weights of pixels w p are set to 1 whenever 0 < \u03b1 p < 1, and 0.1 otherwise.\n\n\nTraining Details\n\nWe use DenseNet-201 network pre-trained with ImageNet-1K [11] as our encoder backbone. We first perform the segmentation network pre-training for 15 epochs. In the fusion network pre-training step, we freeze the segmentation stage and train the fusion stage alone for 4 epochs. Finally, we perform the end-to-end joint training for 7 epochs, which back-propagates the gradient of the fusion result to both the segmentation and fusion network to further reduce the training loss. All batch normalization layers are frozen in the joint training step to save the memory footprint. Cyclical learning rate strategy [32] is used to accelerate the convergence speed during the whole training procedure. The base learning rate is 5.0 \u00d7 10 \u22124 for all steps. The maximum learning rate in pre-training steps is 1.5 \u00d7 10 \u22123 . A smaller maximum learning rate 1.0 \u00d7 10 \u22123 is set during the joint training steps.\n\nWe also use a special loss while performing the end-toend joint training for fine-tuning the whole network. The loss is based on the loss of the fusion network while adding the loss of the segmentation network to avoid overfitting. The overall joint training loss is described as following:\nL J = L u + w 1 (L F + L B ) + w 2 L s .(9)\nWe set w 1 = 0.5 and w 2 = 0.01 in our implementation. The third term L s is directly adopted from [20] to penalize the amount of soft segmentation pixels, i.e.:\nL s = p \u03b1 \u03b3 p + (1 \u2212 \u03b1 p ) \u03b3 , \u03b3 \u2208 [0, 1].(10)\nwhere \u03b3 is set to 0.9 in our experiments.\n\n\nExperimental Results\n\nIn this section, we evaluate our late fusion CNN on two testing datasets. (1) Human image matting testing dataset, which is to measure the performance of our method on a specific task. To this end, we collect 40 human images in which 29 are from the internet whose alpha mattes are carefully matted by designers and 11 are from composition-1k testing dataset in [39] due to their abundant details.  [17]. \"-25\": computed in the transition regions generated by 25 pixels dilation. \"-full\": computed over the whole image.\n\nWe compose each testing image with 25 random background images from PASCAL VOC [12] to form a testing dataset with 1000 images. The training dataset for this task is independent on the testing images, which consists of 228 human images with high-quality alpha mattes combined with another 211 human foreground objects from the DIM dataset [39]. Similarly, we compose these foregrounds with randomly picked unique background images from MS-COCO [23] to form the final dataset, totally 28610 images for training. (2) Composition-1k testing dataset in [39], which is to evaluate how our network performs on natural images. This testing dataset contains 1000 images, composed of 50 unique foregrounds and 20 background images. For this evaluation, we train our network on the DIM dataset [39] independent on the testing images. It consists of 431 unique foreground objects with alpha mattes. Each object is composed of 100 background images randomly picked from MS-COCO.\n\nFor the data augmentation during the training process, we crop the image and trimap pairs centered on random pixels in the transition regions indicated by trimaps. The crop sizes are selected to be 512 \u00d7 512 and 800 \u00d7 800. We also resize all the training images to the size of 512 \u00d7 512 to warm-up the network. Random flipping and rotation are applied to all the cropped and resized training data. Due to the memory limit, we require the longer side of the image to be less than 800 in the training. This size constraint is also imposed during inference. The training time of our network on a GPU server (configuration: E5-2682 CPU, 32G RAM, and 8 Tesla P100 graphics cards) is 2.5 days for human image matting dataset and 4 days for the DIM dataset. For Image Trimap SSS [2] DIM [39] Ours GT Figure 5. The visual comparisons on human image matting testing dataset. The segments in SSS [2] are hand-picked.\n\nImage Trimap Closed-form [19] DIM [39] Ours GT Figure 6. The visual comparisons on the composition-1k testing dataset.\n\ntesting, the average running time on a 800 \u00d7 800 image is 0.39 seconds.\n\nEvaluation metrics. There are four metrics used in the evaluations: SAD (sum of absolution difference), MSE (mean square error), gradient and connectivity defined in [39] . The lower values of the metrics, the better the predicted alpha matte is. The details of gradient and connectivity metrics can be found in [27], and they are used to reflect the visual quality of the alpha matte when observed by a human. For the computation of all the metrics, after summing the metrics at each pixel p for a testing image, we then compute their average over all the images in the testing dataset.\n\nEvaluation on human image matting testing dataset. To compare our network with the state-of-the-art image matting methods, we also train the DIM network on this dataset by feeding both RGB images and trimaps generated by random dilation at pixels whose alpha values are neither 0 nor 1, while the transition regions of the trimaps used for the metric computation are generated by 25 pixels dilation. Since narrowing down the image type to human image lowers the difficulty of the segmentation, our network can closely match the DIM network w.r.t. different metrics as reported in Tab. 1. If only computing the metrics in the trimap transition regions as [39], our method outperforms the DIM network in all four metrics (see \"ours-raw-  25\" and \"ours-refined-25\" in Tab. 1). After computing the four metrics on the whole image, the metrics of our algorithm get increased slightly indicating that the segmentation error is well controlled in this case (see \"ours-raw-full\" and \"ours-refine-full\" in Tab. 1). The \"ours-FG/BG-Only-25\" and \"ours-Fusion-Only-25\" also verify that the matting results get improved as we gradually add each sub-network into late fusion CNN. Fig. 5 illustrates three selected matting results in the testing images. Note that our network works well for various poses and scales of the human in the foreground. For instance, the woman viewed from the back (second row in Fig. 5) is difficult for the deep automatic portrait matting [31]. Evaluation on composition-1k testing dataset. Fig. 6 shows three qualitative results and visual comparisons on this dataset. It can be observed that our results are comparable to the results of the DIM [39] even in challenging lace image case. The corresponding metrics are reported in Tab. 2. Due to the image size constraint imposed in the training of late fusion CNN, we also compute the metrics of the DIM network on the resized testing images for a consistent comparison. We first compute the four metrics inside the transition regions in the trimaps provided in the testing dataset, a similar strategy adopted in [39]. It is easy to observe that our method surpasses all the non-CNN image matting methods on this dataset by a large margin since our network can exploit multi-scale features to better understand the semantics in the image. Comparing to CNN-based method, our network is better than DCNN but still inferior to DIM. It is as expected since DIM requests a much stronger input compared to our setting. Specifically, the trimap fed into the DIM network can avoid the possible segmentation errors in our case. After computing the metrics over the complete dataset, our results still rank No.2.\n\nTo further verify whether or not the refinement network used in [39] can correct the residual left in single classification branch results, we train the DIM network without input trimap channel as a comparison and report the result of this setup in the row of \"DIM-Trimap-less-25\" in Tab. 2. The qualitative comparison is illustrated in Fig. 7.a. Fig.7.b shows two additional qualitative comparisons where the DIM network is fed with the manually specified trimaps. It demonstrates that the quality of the matting results of the DIM network downgrades as the size of the transition region increases. Therefore, it is important to have image matting algorithms robust to the trimap quality. Self-comparisons. The two-branch design provides three degrees of freedoms, which allows the optimizer to balance among them for better results. The single branch network in Fig. 8 is created by discarding the background and fusion branch. Similar to the DIM method, we also add a fully convolutional network as refinement and use L1 loss only during training. Its results contain segmentation errors, which are removed by the two-branch network, as illustrated in Fig. 8. In contrast, the foreground and background probability maps of our method are more 'solid' in the nontransition regions since our segmentation loss there favors a hard segmentation. The final results of our late-fusion CNN demonstrate that our fusion network is able to fuse the foreground and background probability maps for detailed alpha mattes (see the second and fourth row of Fig. 8). Evaluation on internet images. Fig. 1 and Fig. 9 shows the matting results for collected internet images to test the generalization ability of our method. 2 All the human image matting results are obtained through our network trained with human image matting dataset, and the other results are from the late fusion CNN trained with the DIM dataset. The results prove that our network has the ability to capture transition regions of different types of various objects. However, the difficulty in capturing the semantic feature of 2 Please see the supplementary material for more matting results. the foreground can possibly lead to segmentation errors in our results, for example, the error around the mouth of the horse as shown in the bottom row of Fig. 9.\n\n\nConclusions and Future Work\n\nIn this paper, we proposed a late fusion fully convolutional neural network for image matting. It utilizes two decoder branches for foreground/background classification and fuses the classification results to obtain the final alpha values through a fusion network. The network does not need trimap as input, which greatly improves the efficiency of the image matting.\n\nIn the future, we would like to explore how to improve the decoder network structure to further reduce segmentation errors. The recent development of multi-scale feature fusion network, such as Refine-net [21], can be tested in the late fusion CNN. It is also interesting to explore how to apply the two-branch design to video object matting.\n\nFigure 2 .\n2A high-level visualization of our network architecture. The segmentation network consists of one encoder and two decoders. The fusion network is a fully convolutional network without downsampling. The final alpha matte is a linear blending using the outputs of two networks. The number below the block in the fusion network denotes the number of output channels of different convolution layers.\n\nFigure 4 .\n4The bracketed alpha values after segmentation. Left: an input image. Middle: the ground truth alpha matte. Right: the groundtruth alpha values of the red highlighted pixels are bracketed by two probabilitiesF p and 1 \u2212Bp which are outputted by our two decoder branches.\n\n\nOur network and DIM using different trimaps.\n\nFigure 7 .Figure 8 .\n78Comparisons to DIM.Top-left in (b): manually specified trimaps. 'small' and 'large' indicate the size of the trimap. Self-comparisons. Single branch: our foreground branch plus the DIM refinement network trained with L1 loss. 'FG' and 'BG': our foreground and background probability maps.\n\nFigure 9 .\n9Internet image matting results.\n\n\nThe quantitative results of our human image matting testing dataset. Ours-FG/BG-only: pre-trained segmentation network stage. Ours-Fusion-only: pre-trained fusion network stage. Oursraw: end-to-end jointly trained network. Ours-refined: refined by guided filterMethods \nSAD \nMSE \nGradient Connectivity \nShared Matting [19] \n16.54 \n0.022 \n30.85 \n15.75 \nComprehensive [30] \n13.31 \n0.014 \n18.92 \n11.80 \nLearning Based [27] \n15.80 \n0.020 \n25.04 \n13.77 \nGlobal Matting [7] \n27.47 \n0.029 \n33.76 \n24.98 \nClosed-form [40] \n15.92 \n0.021 \n25.71 \n13.87 \nKNN Matting [14] \n18.27 \n0.023 \n25.11 \n16.88 \nDCNN [9] \n14.92 \n0.017 \n21.56 \n13.02 \nSHM [6] \n13.34 \n0.017 \n24.41 \n12.71 \nDIM [39] \n10.39 \n0.014 \n19.20 \n9.64 \nOurs-FG/BG-Only-25 20.93 \n0.033 \n44.01 \n20.34 \nOurs-Fusion-Only-25 14.23 \n0.019 \n24.46 \n13.26 \nOurs-raw-25 \n10.08 \n0.010 \n15.57 \n9.24 \nOurs-refined-25 \n9.75 \n0.010 \n15.60 \n8.96 \nOurs-raw-full \n10.87 \n0.002 \n16.91 \n9.80 \nOurs-refined-full \n10.49 \n0.002 \n16.97 \n9.52 \nTable 1. \n\n\n. The quantitative results on the Composition-1k testing dataset. The metrics measured on our results are same with Tab. 1. 'DIM-Trimap-less-25' denotes the results of the DIM method without trimap as input during training.Methods \nSAD \nMSE \nGradient Connectivity \nShared Matting [19] \n115.20 \n0.074 \n139.88 \n121.35 \nComprehensive [30] \n109.80 \n0.066 \n116.27 \n107.86 \nLearning Based [27] 100.51 \n0.058 \n94.68 \n104.74 \nGlobal Matting [7] \n121.46 \n0.078 \n125.11 \n133.23 \nClosed-form [40] \n121.18 \n0.076 \n130.63 \n120.16 \nKNN Matting [14] \n133.99 \n0.098 \n140.29 \n134.03 \nDCNN [9] \n122.40 \n0.079 \n129.57 \n121.80 \nDIM-Trimap-less-25 \n70.31 \n0.110 \n70.06 \n70.05 \nDIM [39] \n33.64 \n0.017 \n30.23 \n31.92 \nOurs-FG/BG-Only-25 103.21 \n0.077 \n91.85 \n109.27 \nOurs-Fusion-Only-25 66.05 \n0.034 \n69.80 \n69.80 \nOurs-raw-25 \n49.05 \n0.022 \n36.58 \n50.70 \nOurs-refined-25 \n49.02 \n0.020 \n34.33 \n50.60 \nOurs-raw-full \n58.34 \n0.011 \n41.63 \n59.74 \nOurs-refined-full \n58.29 \n0.011 \n36.58 \n59.63 \nTable 2\nPlease see the supplementary material for the network details at https://github.com/yunkezhang/FusionMatting.\nAcknowledgementsWe would like to thank the anonymous reviewers for their constructive comments. Weiwei Xu is partially supported by NSFC (No. 61732016) and Zhejiang Lab. Qixing Huang would like to thank for the gift from snap research. Weiwei Xu and Hujun Bao are also supported by the Fundamental Research Funds for the Central Universities.\nDesigning effective inter-pixel information flow for natural image matting. Y Aksoy, T O Aydin, M Pollefeys, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. Aksoy, T. O. Aydin, and M. Pollefeys. Designing effec- tive inter-pixel information flow for natural image matting. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 228-236, 2017. 2\n\nSemantic soft segmentation. Y Aksoy, T.-H Oh, S Paris, M Pollefeys, W Matusik, ACM Transactions on Graphics (TOG). 3746Y. Aksoy, T.-H. Oh, S. Paris, M. Pollefeys, and W. Matusik. Semantic soft segmentation. ACM Transactions on Graphics (TOG), 37(4):72, 2018. 2, 6\n\nA geodesic framework for fast interactive image and video segmentation and matting. X Bai, G Sapiro, International Journal of Computer Vision (IJCV). 822X. Bai and G. Sapiro. A geodesic framework for fast inter- active image and video segmentation and matting. Interna- tional Journal of Computer Vision (IJCV), 82(2):113-132, 2009. 2\n\nTom-net: Learning transparent object matting from a single image. G Chen, K Han, K.-Y K Wong, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)G. Chen, K. Han, and K.-Y. K. Wong. Tom-net: Learning transparent object matting from a single image. In Proceed- ings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nRethinking atrous convolution for semantic image segmentation. L C Chen, G Papandreou, F Schroff, H Adam, abs/1706.05587CoRRL. C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re- thinking atrous convolution for semantic image segmenta- tion. CoRR, abs/1706.05587, 2017. 2\n\nQ Chen, T Ge, Y Xu, Z Zhang, X Yang, K Gai, arXiv:1809.01354Semantic human matting. 25arXiv preprintQ. Chen, T. Ge, Y. Xu, Z. Zhang, X. Yang, and K. Gai. Se- mantic human matting. arXiv preprint arXiv:1809.01354, 2018. 2, 5\n\nQ Chen, D Li, C.-K Tang, Knn matting. IEEE transactions on pattern analysis and machine intelligence. 357Q. Chen, D. Li, and C.-K. Tang. Knn matting. IEEE transactions on pattern analysis and machine intelligence, 35(9):2175-2188, 2013. 2, 5, 7\n\nThe relative performance of ensemble methods with deep convolutional neural networks for image classification. J Cheng, B Aurlien, V D L Mark, J , abs/1704.01664CoRRJ. Cheng, B. Aurlien, and v. d. L. Mark J. The relative perfor- mance of ensemble methods with deep convolutional neural networks for image classification. CoRR, abs/1704.01664, 2017. 2\n\nNatural image matting using deep convolutional neural networks. D Cho, Y.-W Tai, I Kweon, The European Conference on Computer Vision (ECCV). D. Cho, Y.-W. Tai, and I. Kweon. Natural image matting us- ing deep convolutional neural networks. In The European Conference on Computer Vision (ECCV), pages 626-643.\n\n. Springer, Springer, 2016. 1, 2, 5, 7\n\nA bayesian approach to digital matting. Y.-Y Chuang, B Curless, D Salesin, R Szeliski, CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. IEEE2Computer Vision and Pattern RecognitionY.-Y. Chuang, B. Curless, D. Salesin, and R. Szeliski. A bayesian approach to digital matting. In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, volume 2, pages II-II. IEEE, 2001. 1, 2\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, F.-F Li, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 248-255. IEEE, 2009. 4\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, International Journal of Computer Vision (IJCV). 8825M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV), 88(2):303-338, June 2010. 5\n\nA Cluster Sampling Method for Image Matting via Sparse Coding. X Feng, X Liang, Z Zhang, Springer International PublishingX. Feng, X. Liang, and Z. Zhang. A Cluster Sampling Method for Image Matting via Sparse Coding. Springer In- ternational Publishing, 2016. 2\n\nShared sampling for realtime alpha matting. E S Gastal, M M Oliveira, Computer Graphics Forum. Wiley Online Library297E. S. Gastal and M. M. Oliveira. Shared sampling for real- time alpha matting. In Computer Graphics Forum, vol- ume 29, pages 575-584. Wiley Online Library, 2010. 2, 5, 7\n\nRandom walks for interactive alpha-matting. L Grady, T Schiwietz, S Aharon, R Westermann, Proceedings of VIIP. VIIP1L. Grady, T. Schiwietz, S. Aharon, and R. Westermann. Ran- dom walks for interactive alpha-matting. In Proceedings of VIIP, volume 2005, pages 423-429, 2005. 1, 2\n\nA global sampling method for alpha matting. K He, C Rhemann, C Rother, X Tang, J Sun, K. He, C. Rhemann, C. Rother, X. Tang, and J. Sun. A global sampling method for alpha matting. 2011. 2\n\nK He, J Sun, X Tang, Guided image filtering. IEEE transactions on pattern analysis and machine intelligence. K. He, J. Sun, and X. Tang. Guided image filtering. IEEE transactions on pattern analysis and machine intelligence, (6):1397-1409, 2013. 5\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, 2017. 3The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. G. Huang, Z. Liu, L. van der Maaten, and K. Q. Wein- berger. Densely connected convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2017. 3\n\nA closed form solution to natural image matting. A Levin, D Lischinski, Y Weiss, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 17IEEEA. Levin, D. Lischinski, and Y. Weiss. A closed form solu- tion to natural image matting. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2006 IEEE Computer Society Conference on, volume 1, pages 61-68. IEEE, 2006. 2, 5, 6, 7\n\nA Levin, A Rav-Acha, D Lischinski, Spectral matting. IEEE transactions on pattern analysis and machine intelligence. 305A. Levin, A. Rav-Acha, and D. Lischinski. Spectral matting. IEEE transactions on pattern analysis and machine intelli- gence, 30(10):1699-1712, 2008. 1, 2, 5\n\nRefinenet: Multipath refinement networks for high-resolution semantic segmentation. G Lin, A Milan, C Shen, I D Reid, abs/1611.06612CoRRG. Lin, A. Milan, C. Shen, and I. D. Reid. Refinenet: Multi- path refinement networks for high-resolution semantic seg- mentation. CoRR, abs/1611.06612, 2016. 8\n\nFeature pyramid networks for object detection. T.-Y Lin, P Doll\u00e1r, R B Girshick, K He, B Hariharan, S J Belongie, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1T.-Y. Lin, P. Doll\u00e1r, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie. Feature pyramid networks for object detec- tion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, page 4, 2017. 3\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European Conference on Computer Vision (ICCV). SpringerT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In European Conference on Com- puter Vision (ICCV), pages 740-755. Springer, 2014. 5\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 4\n\nS Lutz, K Amplianitis, A Smolic, Alphagan, arXiv:1807.10088Generative adversarial networks for natural image matting. arXiv preprintS. Lutz, K. Amplianitis, and A. Smolic. Alphagan: Gener- ative adversarial networks for natural image matting. arXiv preprint arXiv:1807.10088, 2018. 2\n\nEnsemble classification and regression-recent developments, applications and future directions. Y Ren, L Zhang, P N Suganthan, IEEE Comp. Int. Mag. 111Y. Ren, L. Zhang, and P. N. Suganthan. Ensemble classifi- cation and regression-recent developments, applications and future directions. IEEE Comp. Int. Mag., 11(1):41-53, 2016. 2\n\nA perceptually motivated online benchmark for image matting. C Rhemann, C Rother, J Wang, M Gelautz, P Kohli, P Rott, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE67C. Rhemann, C. Rother, J. Wang, M. Gelautz, P. Kohli, and P. Rott. A perceptually motivated online benchmark for im- age matting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1826-1833. IEEE, 2009. 5, 6, 7\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerO. Ronneberger, P. Fischer, and T. Brox. U-net: Convo- lutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer, 2015. 3\n\nAlpha estimation in natural images. M A Ruzon, C Tomasi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1018M. A. Ruzon and C. Tomasi. Alpha estimation in natural images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 1018. IEEE, 2000. 2\n\nImproving image matting using comprehensive sampling sets. E Shahrian, D Rajan, B Price, S Cohen, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 57E. Shahrian, D. Rajan, B. Price, and S. Cohen. Improv- ing image matting using comprehensive sampling sets. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 636-643, 2013. 2, 5, 7\n\nDeep automatic portrait matting. X Shen, X Tao, H Gao, C Zhou, J Jia, European Conference on Computer Vision (ECCV). SpringerX. Shen, X. Tao, H. Gao, C. Zhou, and J. Jia. Deep automatic portrait matting. In European Conference on Computer Vi- sion (ECCV), pages 92-107. Springer, 2016. 1, 2, 7\n\nCyclical learning rates for training neural networks. L N Smith, Applications of Computer Vision. WACV2017L. N. Smith. Cyclical learning rates for training neural net- works. In Applications of Computer Vision (WACV), 2017\n\n. IEEE Winter Conference on. 5IEEEIEEE Winter Conference on, pages 464-472. IEEE, 2017. 5\n\nPoisson matting. J Sun, J Jia, C K Tang, H Y Shum, ACM Transactions on Graphics. 233J. Sun, J. Jia, C. K. Tang, and H. Y. Shum. Poisson matting. ACM Transactions on Graphics, 23(3):315-321, 2004. 2\n\nSoft scissors: an interactive tool for realtime high quality matting. J Wang, M Agrawala, M F Cohen, ACM SIGGRAPH. 9J. Wang, M. Agrawala, and M. F. Cohen. Soft scissors: an interactive tool for realtime high quality matting. In ACM SIGGRAPH, page 9, 2007. 2\n\nAn iterative optimization approach for unified image segmentation and matting. J Wang, M F Cohen, Tenth IEEE International Conference on Computer Vision. 1J. Wang and M. F. Cohen. An iterative optimization approach for unified image segmentation and matting. In Tenth IEEE International Conference on Computer Vision, pages 936- 943, 2005. 1, 2\n\nOptimized color sampling for robust matting. J Wang, M F Cohen, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEJ. Wang and M. F. Cohen. Optimized color sampling for robust matting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1-8. IEEE, 2007. 1\n\nImage and video matting: a survey. J Wang, M F Cohen, Foundations and Trends R in Computer Graphics and Vision. 32J. Wang, M. F. Cohen, et al. Image and video matting: a survey. Foundations and Trends R in Computer Graphics and Vision, 3(2):97-175, 2008. 2\n\nDeep propagation based image matting. Y Wang, Y Niu, P Duan, J Lin, Y Zheng, IJCAI. 1Y. Wang, Y. Niu, P. Duan, J. Lin, and Y. Zheng. Deep prop- agation based image matting. In IJCAI, pages 999-1006, 2018. 1, 2\n\nDeep image matting. N Xu, B L Price, S Cohen, T S Huang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 27N. Xu, B. L. Price, S. Cohen, and T. S. Huang. Deep image matting. In IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), volume 2, page 4, 2017. 1, 2, 5, 6, 7\n\nLearning based digital matting. Y Zheng, C Kambhamettu, IEEE 12th International Conference on. IEEE57Computer VisionY. Zheng and C. Kambhamettu. Learning based digital mat- ting. In Computer Vision, 2009 IEEE 12th International Conference on, pages 889-896. IEEE, 2009. 5, 7\n\nFast deep matting for portrait animation on mobile phone. B Zhu, Y Chen, J Wang, S Liu, B Zhang, M Tang, B. Zhu, Y. Chen, J. Wang, S. Liu, B. Zhang, and M. Tang. Fast deep matting for portrait animation on mobile phone. pages 297-305, 2017. 2\n", "annotations": {"author": "[{\"end\":75,\"start\":41},{\"end\":127,\"start\":76},{\"end\":154,\"start\":128},{\"end\":209,\"start\":155},{\"end\":277,\"start\":210},{\"end\":329,\"start\":278},{\"end\":362,\"start\":330}]", "publisher": null, "author_last_name": "[{\"end\":52,\"start\":47},{\"end\":86,\"start\":82},{\"end\":137,\"start\":134},{\"end\":165,\"start\":162},{\"end\":222,\"start\":217},{\"end\":287,\"start\":284},{\"end\":339,\"start\":337}]", "author_first_name": "[{\"end\":46,\"start\":41},{\"end\":81,\"start\":76},{\"end\":133,\"start\":128},{\"end\":161,\"start\":155},{\"end\":216,\"start\":210},{\"end\":283,\"start\":278},{\"end\":336,\"start\":330}]", "author_affiliation": "[{\"end\":74,\"start\":54},{\"end\":126,\"start\":106},{\"end\":153,\"start\":139},{\"end\":208,\"start\":194},{\"end\":276,\"start\":246},{\"end\":328,\"start\":308},{\"end\":361,\"start\":341}]", "title": "[{\"end\":38,\"start\":1},{\"end\":400,\"start\":363}]", "venue": null, "abstract": "[{\"end\":1441,\"start\":425}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2146,\"start\":2142},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2149,\"start\":2146},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2152,\"start\":2149},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2533,\"start\":2530},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2785,\"start\":2781},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3074,\"start\":3070},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3155,\"start\":3151},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3276,\"start\":3272},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3301,\"start\":3298},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3304,\"start\":3301},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3307,\"start\":3304},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3310,\"start\":3307},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3499,\"start\":3495},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3767,\"start\":3764},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5285,\"start\":5282},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5288,\"start\":5285},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5650,\"start\":5646},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6354,\"start\":6350},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6357,\"start\":6354},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6360,\"start\":6357},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6363,\"start\":6360},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6366,\"start\":6363},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6553,\"start\":6549},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6556,\"start\":6553},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6559,\"start\":6556},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6562,\"start\":6559},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6649,\"start\":6645},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6652,\"start\":6649},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6655,\"start\":6652},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6658,\"start\":6655},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6993,\"start\":6990},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6995,\"start\":6993},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6997,\"start\":6995},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6999,\"start\":6997},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7002,\"start\":6999},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7005,\"start\":7002},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7008,\"start\":7005},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7011,\"start\":7008},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7210,\"start\":7206},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7213,\"start\":7210},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7216,\"start\":7213},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7331,\"start\":7327},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7334,\"start\":7331},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7337,\"start\":7334},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7403,\"start\":7399},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7602,\"start\":7598},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7772,\"start\":7769},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7864,\"start\":7860},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7902,\"start\":7899},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7918,\"start\":7914},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8098,\"start\":8094},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8204,\"start\":8200},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8260,\"start\":8257},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8321,\"start\":8317},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8642,\"start\":8639},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9171,\"start\":9167},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9352,\"start\":9348},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9529,\"start\":9526},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10153,\"start\":10152},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12433,\"start\":12429},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12663,\"start\":12659},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12751,\"start\":12747},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14983,\"start\":14979},{\"end\":15477,\"start\":15469},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17363,\"start\":17359},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17916,\"start\":17912},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18639,\"start\":18635},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19177,\"start\":19173},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19214,\"start\":19210},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19415,\"start\":19411},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19675,\"start\":19671},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19780,\"start\":19776},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19885,\"start\":19881},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20120,\"start\":20116},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21075,\"start\":21072},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21084,\"start\":21080},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21189,\"start\":21186},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21237,\"start\":21233},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21246,\"start\":21242},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21571,\"start\":21567},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21717,\"start\":21713},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22648,\"start\":22644},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23448,\"start\":23444},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23656,\"start\":23652},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24073,\"start\":24069},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24728,\"start\":24724},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26370,\"start\":26369},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27582,\"start\":27578}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28123,\"start\":27716},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28406,\"start\":28124},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28453,\"start\":28407},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28766,\"start\":28454},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28811,\"start\":28767},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29790,\"start\":28812},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30767,\"start\":29791}]", "paragraph": "[{\"end\":1993,\"start\":1457},{\"end\":2103,\"start\":2027},{\"end\":2220,\"start\":2105},{\"end\":3884,\"start\":2222},{\"end\":5147,\"start\":3886},{\"end\":5582,\"start\":5149},{\"end\":6146,\"start\":5584},{\"end\":6322,\"start\":6163},{\"end\":7620,\"start\":6324},{\"end\":9667,\"start\":7622},{\"end\":9970,\"start\":9680},{\"end\":10716,\"start\":9992},{\"end\":10806,\"start\":10718},{\"end\":11182,\"start\":10845},{\"end\":11274,\"start\":11184},{\"end\":11796,\"start\":11292},{\"end\":12912,\"start\":11821},{\"end\":13167,\"start\":12914},{\"end\":13276,\"start\":13169},{\"end\":13656,\"start\":13349},{\"end\":13817,\"start\":13658},{\"end\":13978,\"start\":13884},{\"end\":14559,\"start\":14042},{\"end\":14653,\"start\":14561},{\"end\":14984,\"start\":14701},{\"end\":15680,\"start\":14986},{\"end\":16169,\"start\":15682},{\"end\":16307,\"start\":16188},{\"end\":16831,\"start\":16309},{\"end\":17131,\"start\":16833},{\"end\":17281,\"start\":17188},{\"end\":18199,\"start\":17302},{\"end\":18491,\"start\":18201},{\"end\":18697,\"start\":18536},{\"end\":18786,\"start\":18745},{\"end\":19330,\"start\":18811},{\"end\":20298,\"start\":19332},{\"end\":21206,\"start\":20300},{\"end\":21326,\"start\":21208},{\"end\":21399,\"start\":21328},{\"end\":21988,\"start\":21401},{\"end\":24658,\"start\":21990},{\"end\":26972,\"start\":24660},{\"end\":27371,\"start\":27004},{\"end\":27715,\"start\":27373}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":2026,\"start\":1994},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10844,\"start\":10807},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11291,\"start\":11275},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13348,\"start\":13277},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13883,\"start\":13818},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14041,\"start\":13979},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14700,\"start\":14654},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17187,\"start\":17132},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18535,\"start\":18492},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18744,\"start\":18698}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1455,\"start\":1443},{\"attributes\":{\"n\":\"2.\"},\"end\":6161,\"start\":6149},{\"attributes\":{\"n\":\"3.\"},\"end\":9678,\"start\":9670},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9990,\"start\":9973},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11819,\"start\":11799},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16186,\"start\":16172},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17300,\"start\":17284},{\"attributes\":{\"n\":\"4.\"},\"end\":18809,\"start\":18789},{\"attributes\":{\"n\":\"5.\"},\"end\":27002,\"start\":26975},{\"end\":27727,\"start\":27717},{\"end\":28135,\"start\":28125},{\"end\":28475,\"start\":28455},{\"end\":28778,\"start\":28768}]", "table": "[{\"end\":29790,\"start\":29075},{\"end\":30767,\"start\":30016}]", "figure_caption": "[{\"end\":28123,\"start\":27729},{\"end\":28406,\"start\":28137},{\"end\":28453,\"start\":28409},{\"end\":28766,\"start\":28478},{\"end\":28811,\"start\":28780},{\"end\":29075,\"start\":28814},{\"end\":30016,\"start\":29793}]", "figure_ref": "[{\"end\":2258,\"start\":2250},{\"end\":5861,\"start\":5855},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10178,\"start\":10172},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11180,\"start\":11174},{\"end\":15545,\"start\":15539},{\"end\":15960,\"start\":15954},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15971,\"start\":15965},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16462,\"start\":16456},{\"end\":21101,\"start\":21093},{\"end\":21263,\"start\":21255},{\"end\":23162,\"start\":23156},{\"end\":23390,\"start\":23383},{\"end\":23502,\"start\":23496},{\"end\":25003,\"start\":24997},{\"end\":25012,\"start\":25007},{\"end\":25530,\"start\":25524},{\"end\":25821,\"start\":25815},{\"end\":26211,\"start\":26205},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26262,\"start\":26245},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26971,\"start\":26965}]", "bib_author_first_name": "[{\"end\":31298,\"start\":31297},{\"end\":31307,\"start\":31306},{\"end\":31309,\"start\":31308},{\"end\":31318,\"start\":31317},{\"end\":31641,\"start\":31640},{\"end\":31653,\"start\":31649},{\"end\":31659,\"start\":31658},{\"end\":31668,\"start\":31667},{\"end\":31681,\"start\":31680},{\"end\":31962,\"start\":31961},{\"end\":31969,\"start\":31968},{\"end\":32280,\"start\":32279},{\"end\":32288,\"start\":32287},{\"end\":32298,\"start\":32294},{\"end\":32300,\"start\":32299},{\"end\":32716,\"start\":32715},{\"end\":32718,\"start\":32717},{\"end\":32726,\"start\":32725},{\"end\":32740,\"start\":32739},{\"end\":32751,\"start\":32750},{\"end\":32927,\"start\":32926},{\"end\":32935,\"start\":32934},{\"end\":32941,\"start\":32940},{\"end\":32947,\"start\":32946},{\"end\":32956,\"start\":32955},{\"end\":32964,\"start\":32963},{\"end\":33152,\"start\":33151},{\"end\":33160,\"start\":33159},{\"end\":33169,\"start\":33165},{\"end\":33509,\"start\":33508},{\"end\":33518,\"start\":33517},{\"end\":33529,\"start\":33528},{\"end\":33533,\"start\":33530},{\"end\":33541,\"start\":33540},{\"end\":33814,\"start\":33813},{\"end\":33824,\"start\":33820},{\"end\":33831,\"start\":33830},{\"end\":34143,\"start\":34139},{\"end\":34153,\"start\":34152},{\"end\":34164,\"start\":34163},{\"end\":34175,\"start\":34174},{\"end\":34614,\"start\":34613},{\"end\":34622,\"start\":34621},{\"end\":34630,\"start\":34629},{\"end\":34643,\"start\":34639},{\"end\":34649,\"start\":34648},{\"end\":34658,\"start\":34654},{\"end\":35000,\"start\":34999},{\"end\":35014,\"start\":35013},{\"end\":35026,\"start\":35025},{\"end\":35030,\"start\":35027},{\"end\":35042,\"start\":35041},{\"end\":35050,\"start\":35049},{\"end\":35381,\"start\":35380},{\"end\":35389,\"start\":35388},{\"end\":35398,\"start\":35397},{\"end\":35626,\"start\":35625},{\"end\":35628,\"start\":35627},{\"end\":35638,\"start\":35637},{\"end\":35640,\"start\":35639},{\"end\":35916,\"start\":35915},{\"end\":35925,\"start\":35924},{\"end\":35938,\"start\":35937},{\"end\":35948,\"start\":35947},{\"end\":36196,\"start\":36195},{\"end\":36202,\"start\":36201},{\"end\":36213,\"start\":36212},{\"end\":36223,\"start\":36222},{\"end\":36231,\"start\":36230},{\"end\":36342,\"start\":36341},{\"end\":36348,\"start\":36347},{\"end\":36355,\"start\":36354},{\"end\":36633,\"start\":36632},{\"end\":36642,\"start\":36641},{\"end\":36649,\"start\":36648},{\"end\":36667,\"start\":36666},{\"end\":36669,\"start\":36668},{\"end\":36997,\"start\":36996},{\"end\":37006,\"start\":37005},{\"end\":37020,\"start\":37019},{\"end\":37353,\"start\":37352},{\"end\":37362,\"start\":37361},{\"end\":37374,\"start\":37373},{\"end\":37716,\"start\":37715},{\"end\":37723,\"start\":37722},{\"end\":37732,\"start\":37731},{\"end\":37740,\"start\":37739},{\"end\":37742,\"start\":37741},{\"end\":37980,\"start\":37976},{\"end\":37987,\"start\":37986},{\"end\":37997,\"start\":37996},{\"end\":37999,\"start\":37998},{\"end\":38011,\"start\":38010},{\"end\":38017,\"start\":38016},{\"end\":38030,\"start\":38029},{\"end\":38032,\"start\":38031},{\"end\":38383,\"start\":38379},{\"end\":38390,\"start\":38389},{\"end\":38399,\"start\":38398},{\"end\":38411,\"start\":38410},{\"end\":38419,\"start\":38418},{\"end\":38429,\"start\":38428},{\"end\":38440,\"start\":38439},{\"end\":38450,\"start\":38449},{\"end\":38452,\"start\":38451},{\"end\":38803,\"start\":38802},{\"end\":38811,\"start\":38810},{\"end\":38824,\"start\":38823},{\"end\":39081,\"start\":39080},{\"end\":39089,\"start\":39088},{\"end\":39104,\"start\":39103},{\"end\":39462,\"start\":39461},{\"end\":39469,\"start\":39468},{\"end\":39478,\"start\":39477},{\"end\":39480,\"start\":39479},{\"end\":39759,\"start\":39758},{\"end\":39770,\"start\":39769},{\"end\":39780,\"start\":39779},{\"end\":39788,\"start\":39787},{\"end\":39799,\"start\":39798},{\"end\":39808,\"start\":39807},{\"end\":40192,\"start\":40191},{\"end\":40207,\"start\":40206},{\"end\":40218,\"start\":40217},{\"end\":40591,\"start\":40590},{\"end\":40593,\"start\":40592},{\"end\":40602,\"start\":40601},{\"end\":40901,\"start\":40900},{\"end\":40913,\"start\":40912},{\"end\":40922,\"start\":40921},{\"end\":40931,\"start\":40930},{\"end\":41252,\"start\":41251},{\"end\":41260,\"start\":41259},{\"end\":41267,\"start\":41266},{\"end\":41274,\"start\":41273},{\"end\":41282,\"start\":41281},{\"end\":41568,\"start\":41567},{\"end\":41570,\"start\":41569},{\"end\":41846,\"start\":41845},{\"end\":41853,\"start\":41852},{\"end\":41860,\"start\":41859},{\"end\":41862,\"start\":41861},{\"end\":41870,\"start\":41869},{\"end\":41872,\"start\":41871},{\"end\":42098,\"start\":42097},{\"end\":42106,\"start\":42105},{\"end\":42118,\"start\":42117},{\"end\":42120,\"start\":42119},{\"end\":42366,\"start\":42365},{\"end\":42374,\"start\":42373},{\"end\":42376,\"start\":42375},{\"end\":42678,\"start\":42677},{\"end\":42686,\"start\":42685},{\"end\":42688,\"start\":42687},{\"end\":42969,\"start\":42968},{\"end\":42977,\"start\":42976},{\"end\":42979,\"start\":42978},{\"end\":43230,\"start\":43229},{\"end\":43238,\"start\":43237},{\"end\":43245,\"start\":43244},{\"end\":43253,\"start\":43252},{\"end\":43260,\"start\":43259},{\"end\":43423,\"start\":43422},{\"end\":43429,\"start\":43428},{\"end\":43431,\"start\":43430},{\"end\":43440,\"start\":43439},{\"end\":43449,\"start\":43448},{\"end\":43451,\"start\":43450},{\"end\":43739,\"start\":43738},{\"end\":43748,\"start\":43747},{\"end\":44041,\"start\":44040},{\"end\":44048,\"start\":44047},{\"end\":44056,\"start\":44055},{\"end\":44064,\"start\":44063},{\"end\":44071,\"start\":44070},{\"end\":44080,\"start\":44079}]", "bib_author_last_name": "[{\"end\":31304,\"start\":31299},{\"end\":31315,\"start\":31310},{\"end\":31328,\"start\":31319},{\"end\":31647,\"start\":31642},{\"end\":31656,\"start\":31654},{\"end\":31665,\"start\":31660},{\"end\":31678,\"start\":31669},{\"end\":31689,\"start\":31682},{\"end\":31966,\"start\":31963},{\"end\":31976,\"start\":31970},{\"end\":32285,\"start\":32281},{\"end\":32292,\"start\":32289},{\"end\":32305,\"start\":32301},{\"end\":32723,\"start\":32719},{\"end\":32737,\"start\":32727},{\"end\":32748,\"start\":32741},{\"end\":32756,\"start\":32752},{\"end\":32932,\"start\":32928},{\"end\":32938,\"start\":32936},{\"end\":32944,\"start\":32942},{\"end\":32953,\"start\":32948},{\"end\":32961,\"start\":32957},{\"end\":32968,\"start\":32965},{\"end\":33157,\"start\":33153},{\"end\":33163,\"start\":33161},{\"end\":33174,\"start\":33170},{\"end\":33515,\"start\":33510},{\"end\":33526,\"start\":33519},{\"end\":33538,\"start\":33534},{\"end\":33818,\"start\":33815},{\"end\":33828,\"start\":33825},{\"end\":33837,\"start\":33832},{\"end\":34069,\"start\":34061},{\"end\":34150,\"start\":34144},{\"end\":34161,\"start\":34154},{\"end\":34172,\"start\":34165},{\"end\":34184,\"start\":34176},{\"end\":34619,\"start\":34615},{\"end\":34627,\"start\":34623},{\"end\":34637,\"start\":34631},{\"end\":34646,\"start\":34644},{\"end\":34652,\"start\":34650},{\"end\":34661,\"start\":34659},{\"end\":35011,\"start\":35001},{\"end\":35023,\"start\":35015},{\"end\":35039,\"start\":35031},{\"end\":35047,\"start\":35043},{\"end\":35060,\"start\":35051},{\"end\":35386,\"start\":35382},{\"end\":35395,\"start\":35390},{\"end\":35404,\"start\":35399},{\"end\":35635,\"start\":35629},{\"end\":35649,\"start\":35641},{\"end\":35922,\"start\":35917},{\"end\":35935,\"start\":35926},{\"end\":35945,\"start\":35939},{\"end\":35959,\"start\":35949},{\"end\":36199,\"start\":36197},{\"end\":36210,\"start\":36203},{\"end\":36220,\"start\":36214},{\"end\":36228,\"start\":36224},{\"end\":36235,\"start\":36232},{\"end\":36345,\"start\":36343},{\"end\":36352,\"start\":36349},{\"end\":36360,\"start\":36356},{\"end\":36639,\"start\":36634},{\"end\":36646,\"start\":36643},{\"end\":36664,\"start\":36650},{\"end\":36680,\"start\":36670},{\"end\":37003,\"start\":36998},{\"end\":37017,\"start\":37007},{\"end\":37026,\"start\":37021},{\"end\":37359,\"start\":37354},{\"end\":37371,\"start\":37363},{\"end\":37385,\"start\":37375},{\"end\":37720,\"start\":37717},{\"end\":37729,\"start\":37724},{\"end\":37737,\"start\":37733},{\"end\":37747,\"start\":37743},{\"end\":37984,\"start\":37981},{\"end\":37994,\"start\":37988},{\"end\":38008,\"start\":38000},{\"end\":38014,\"start\":38012},{\"end\":38027,\"start\":38018},{\"end\":38041,\"start\":38033},{\"end\":38387,\"start\":38384},{\"end\":38396,\"start\":38391},{\"end\":38408,\"start\":38400},{\"end\":38416,\"start\":38412},{\"end\":38426,\"start\":38420},{\"end\":38437,\"start\":38430},{\"end\":38447,\"start\":38441},{\"end\":38460,\"start\":38453},{\"end\":38808,\"start\":38804},{\"end\":38821,\"start\":38812},{\"end\":38832,\"start\":38825},{\"end\":39086,\"start\":39082},{\"end\":39101,\"start\":39090},{\"end\":39111,\"start\":39105},{\"end\":39121,\"start\":39113},{\"end\":39466,\"start\":39463},{\"end\":39475,\"start\":39470},{\"end\":39490,\"start\":39481},{\"end\":39767,\"start\":39760},{\"end\":39777,\"start\":39771},{\"end\":39785,\"start\":39781},{\"end\":39796,\"start\":39789},{\"end\":39805,\"start\":39800},{\"end\":39813,\"start\":39809},{\"end\":40204,\"start\":40193},{\"end\":40215,\"start\":40208},{\"end\":40223,\"start\":40219},{\"end\":40599,\"start\":40594},{\"end\":40609,\"start\":40603},{\"end\":40910,\"start\":40902},{\"end\":40919,\"start\":40914},{\"end\":40928,\"start\":40923},{\"end\":40937,\"start\":40932},{\"end\":41257,\"start\":41253},{\"end\":41264,\"start\":41261},{\"end\":41271,\"start\":41268},{\"end\":41279,\"start\":41275},{\"end\":41286,\"start\":41283},{\"end\":41576,\"start\":41571},{\"end\":41850,\"start\":41847},{\"end\":41857,\"start\":41854},{\"end\":41867,\"start\":41863},{\"end\":41877,\"start\":41873},{\"end\":42103,\"start\":42099},{\"end\":42115,\"start\":42107},{\"end\":42126,\"start\":42121},{\"end\":42371,\"start\":42367},{\"end\":42382,\"start\":42377},{\"end\":42683,\"start\":42679},{\"end\":42694,\"start\":42689},{\"end\":42974,\"start\":42970},{\"end\":42985,\"start\":42980},{\"end\":43235,\"start\":43231},{\"end\":43242,\"start\":43239},{\"end\":43250,\"start\":43246},{\"end\":43257,\"start\":43254},{\"end\":43266,\"start\":43261},{\"end\":43426,\"start\":43424},{\"end\":43437,\"start\":43432},{\"end\":43446,\"start\":43441},{\"end\":43457,\"start\":43452},{\"end\":43745,\"start\":43740},{\"end\":43760,\"start\":43749},{\"end\":44045,\"start\":44042},{\"end\":44053,\"start\":44049},{\"end\":44061,\"start\":44057},{\"end\":44068,\"start\":44065},{\"end\":44077,\"start\":44072},{\"end\":44085,\"start\":44081}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":24382270},\"end\":31610,\"start\":31221},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":29170435},\"end\":31875,\"start\":31612},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206769520},\"end\":32211,\"start\":31877},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3875670},\"end\":32650,\"start\":32213},{\"attributes\":{\"doi\":\"abs/1706.05587\",\"id\":\"b4\"},\"end\":32924,\"start\":32652},{\"attributes\":{\"doi\":\"arXiv:1809.01354\",\"id\":\"b5\"},\"end\":33149,\"start\":32926},{\"attributes\":{\"id\":\"b6\"},\"end\":33395,\"start\":33151},{\"attributes\":{\"doi\":\"abs/1704.01664\",\"id\":\"b7\"},\"end\":33747,\"start\":33397},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7491557},\"end\":34057,\"start\":33749},{\"attributes\":{\"id\":\"b9\"},\"end\":34097,\"start\":34059},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2062421},\"end\":34558,\"start\":34099},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57246310},\"end\":34947,\"start\":34560},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4246903},\"end\":35315,\"start\":34949},{\"attributes\":{\"id\":\"b13\"},\"end\":35579,\"start\":35317},{\"attributes\":{\"id\":\"b14\"},\"end\":35869,\"start\":35581},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8857695},\"end\":36149,\"start\":35871},{\"attributes\":{\"id\":\"b16\"},\"end\":36339,\"start\":36151},{\"attributes\":{\"id\":\"b17\"},\"end\":36588,\"start\":36341},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b18\",\"matched_paper_id\":9433631},\"end\":36945,\"start\":36590},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233493},\"end\":37350,\"start\":36947},{\"attributes\":{\"id\":\"b20\"},\"end\":37629,\"start\":37352},{\"attributes\":{\"doi\":\"abs/1611.06612\",\"id\":\"b21\"},\"end\":37927,\"start\":37631},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10716717},\"end\":38334,\"start\":37929},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14113767},\"end\":38744,\"start\":38336},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1629541},\"end\":39078,\"start\":38746},{\"attributes\":{\"doi\":\"arXiv:1807.10088\",\"id\":\"b25\"},\"end\":39363,\"start\":39080},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14010214},\"end\":39695,\"start\":39365},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14922199},\"end\":40124,\"start\":39697},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3719281},\"end\":40552,\"start\":40126},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16307305},\"end\":40839,\"start\":40554},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8013561},\"end\":41216,\"start\":40841},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2570471},\"end\":41511,\"start\":41218},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15247298},\"end\":41735,\"start\":41513},{\"attributes\":{\"id\":\"b33\"},\"end\":41826,\"start\":41737},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3347263},\"end\":42025,\"start\":41828},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2079594},\"end\":42284,\"start\":42027},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8673951},\"end\":42630,\"start\":42286},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1282345},\"end\":42931,\"start\":42632},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51826135},\"end\":43189,\"start\":42933},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":51609454},\"end\":43400,\"start\":43191},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14061786},\"end\":43704,\"start\":43402},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13058297},\"end\":43980,\"start\":43706},{\"attributes\":{\"id\":\"b42\"},\"end\":44224,\"start\":43982}]", "bib_title": "[{\"end\":31295,\"start\":31221},{\"end\":31638,\"start\":31612},{\"end\":31959,\"start\":31877},{\"end\":32277,\"start\":32213},{\"end\":33811,\"start\":33749},{\"end\":34137,\"start\":34099},{\"end\":34611,\"start\":34560},{\"end\":34997,\"start\":34949},{\"end\":35623,\"start\":35581},{\"end\":35913,\"start\":35871},{\"end\":36630,\"start\":36590},{\"end\":36994,\"start\":36947},{\"end\":37974,\"start\":37929},{\"end\":38377,\"start\":38336},{\"end\":38800,\"start\":38746},{\"end\":39459,\"start\":39365},{\"end\":39756,\"start\":39697},{\"end\":40189,\"start\":40126},{\"end\":40588,\"start\":40554},{\"end\":40898,\"start\":40841},{\"end\":41249,\"start\":41218},{\"end\":41565,\"start\":41513},{\"end\":41843,\"start\":41828},{\"end\":42095,\"start\":42027},{\"end\":42363,\"start\":42286},{\"end\":42675,\"start\":42632},{\"end\":42966,\"start\":42933},{\"end\":43227,\"start\":43191},{\"end\":43420,\"start\":43402},{\"end\":43736,\"start\":43706}]", "bib_author": "[{\"end\":31306,\"start\":31297},{\"end\":31317,\"start\":31306},{\"end\":31330,\"start\":31317},{\"end\":31649,\"start\":31640},{\"end\":31658,\"start\":31649},{\"end\":31667,\"start\":31658},{\"end\":31680,\"start\":31667},{\"end\":31691,\"start\":31680},{\"end\":31968,\"start\":31961},{\"end\":31978,\"start\":31968},{\"end\":32287,\"start\":32279},{\"end\":32294,\"start\":32287},{\"end\":32307,\"start\":32294},{\"end\":32725,\"start\":32715},{\"end\":32739,\"start\":32725},{\"end\":32750,\"start\":32739},{\"end\":32758,\"start\":32750},{\"end\":32934,\"start\":32926},{\"end\":32940,\"start\":32934},{\"end\":32946,\"start\":32940},{\"end\":32955,\"start\":32946},{\"end\":32963,\"start\":32955},{\"end\":32970,\"start\":32963},{\"end\":33159,\"start\":33151},{\"end\":33165,\"start\":33159},{\"end\":33176,\"start\":33165},{\"end\":33517,\"start\":33508},{\"end\":33528,\"start\":33517},{\"end\":33540,\"start\":33528},{\"end\":33544,\"start\":33540},{\"end\":33820,\"start\":33813},{\"end\":33830,\"start\":33820},{\"end\":33839,\"start\":33830},{\"end\":34071,\"start\":34061},{\"end\":34152,\"start\":34139},{\"end\":34163,\"start\":34152},{\"end\":34174,\"start\":34163},{\"end\":34186,\"start\":34174},{\"end\":34621,\"start\":34613},{\"end\":34629,\"start\":34621},{\"end\":34639,\"start\":34629},{\"end\":34648,\"start\":34639},{\"end\":34654,\"start\":34648},{\"end\":34663,\"start\":34654},{\"end\":35013,\"start\":34999},{\"end\":35025,\"start\":35013},{\"end\":35041,\"start\":35025},{\"end\":35049,\"start\":35041},{\"end\":35062,\"start\":35049},{\"end\":35388,\"start\":35380},{\"end\":35397,\"start\":35388},{\"end\":35406,\"start\":35397},{\"end\":35637,\"start\":35625},{\"end\":35651,\"start\":35637},{\"end\":35924,\"start\":35915},{\"end\":35937,\"start\":35924},{\"end\":35947,\"start\":35937},{\"end\":35961,\"start\":35947},{\"end\":36201,\"start\":36195},{\"end\":36212,\"start\":36201},{\"end\":36222,\"start\":36212},{\"end\":36230,\"start\":36222},{\"end\":36237,\"start\":36230},{\"end\":36347,\"start\":36341},{\"end\":36354,\"start\":36347},{\"end\":36362,\"start\":36354},{\"end\":36641,\"start\":36632},{\"end\":36648,\"start\":36641},{\"end\":36666,\"start\":36648},{\"end\":36682,\"start\":36666},{\"end\":37005,\"start\":36996},{\"end\":37019,\"start\":37005},{\"end\":37028,\"start\":37019},{\"end\":37361,\"start\":37352},{\"end\":37373,\"start\":37361},{\"end\":37387,\"start\":37373},{\"end\":37722,\"start\":37715},{\"end\":37731,\"start\":37722},{\"end\":37739,\"start\":37731},{\"end\":37749,\"start\":37739},{\"end\":37986,\"start\":37976},{\"end\":37996,\"start\":37986},{\"end\":38010,\"start\":37996},{\"end\":38016,\"start\":38010},{\"end\":38029,\"start\":38016},{\"end\":38043,\"start\":38029},{\"end\":38389,\"start\":38379},{\"end\":38398,\"start\":38389},{\"end\":38410,\"start\":38398},{\"end\":38418,\"start\":38410},{\"end\":38428,\"start\":38418},{\"end\":38439,\"start\":38428},{\"end\":38449,\"start\":38439},{\"end\":38462,\"start\":38449},{\"end\":38810,\"start\":38802},{\"end\":38823,\"start\":38810},{\"end\":38834,\"start\":38823},{\"end\":39088,\"start\":39080},{\"end\":39103,\"start\":39088},{\"end\":39113,\"start\":39103},{\"end\":39123,\"start\":39113},{\"end\":39468,\"start\":39461},{\"end\":39477,\"start\":39468},{\"end\":39492,\"start\":39477},{\"end\":39769,\"start\":39758},{\"end\":39779,\"start\":39769},{\"end\":39787,\"start\":39779},{\"end\":39798,\"start\":39787},{\"end\":39807,\"start\":39798},{\"end\":39815,\"start\":39807},{\"end\":40206,\"start\":40191},{\"end\":40217,\"start\":40206},{\"end\":40225,\"start\":40217},{\"end\":40601,\"start\":40590},{\"end\":40611,\"start\":40601},{\"end\":40912,\"start\":40900},{\"end\":40921,\"start\":40912},{\"end\":40930,\"start\":40921},{\"end\":40939,\"start\":40930},{\"end\":41259,\"start\":41251},{\"end\":41266,\"start\":41259},{\"end\":41273,\"start\":41266},{\"end\":41281,\"start\":41273},{\"end\":41288,\"start\":41281},{\"end\":41578,\"start\":41567},{\"end\":41852,\"start\":41845},{\"end\":41859,\"start\":41852},{\"end\":41869,\"start\":41859},{\"end\":41879,\"start\":41869},{\"end\":42105,\"start\":42097},{\"end\":42117,\"start\":42105},{\"end\":42128,\"start\":42117},{\"end\":42373,\"start\":42365},{\"end\":42384,\"start\":42373},{\"end\":42685,\"start\":42677},{\"end\":42696,\"start\":42685},{\"end\":42976,\"start\":42968},{\"end\":42987,\"start\":42976},{\"end\":43237,\"start\":43229},{\"end\":43244,\"start\":43237},{\"end\":43252,\"start\":43244},{\"end\":43259,\"start\":43252},{\"end\":43268,\"start\":43259},{\"end\":43428,\"start\":43422},{\"end\":43439,\"start\":43428},{\"end\":43448,\"start\":43439},{\"end\":43459,\"start\":43448},{\"end\":43747,\"start\":43738},{\"end\":43762,\"start\":43747},{\"end\":44047,\"start\":44040},{\"end\":44055,\"start\":44047},{\"end\":44063,\"start\":44055},{\"end\":44070,\"start\":44063},{\"end\":44079,\"start\":44070},{\"end\":44087,\"start\":44079}]", "bib_venue": "[{\"end\":32454,\"start\":32389},{\"end\":35986,\"start\":35982},{\"end\":31395,\"start\":31330},{\"end\":31725,\"start\":31691},{\"end\":32025,\"start\":31978},{\"end\":32387,\"start\":32307},{\"end\":32713,\"start\":32652},{\"end\":33008,\"start\":32986},{\"end\":33251,\"start\":33176},{\"end\":33506,\"start\":33397},{\"end\":33888,\"start\":33839},{\"end\":34256,\"start\":34186},{\"end\":34728,\"start\":34663},{\"end\":35109,\"start\":35062},{\"end\":35378,\"start\":35317},{\"end\":35674,\"start\":35651},{\"end\":35980,\"start\":35961},{\"end\":36193,\"start\":36151},{\"end\":36448,\"start\":36362},{\"end\":36757,\"start\":36689},{\"end\":37093,\"start\":37028},{\"end\":37467,\"start\":37387},{\"end\":37713,\"start\":37631},{\"end\":38108,\"start\":38043},{\"end\":38507,\"start\":38462},{\"end\":38899,\"start\":38834},{\"end\":39196,\"start\":39139},{\"end\":39511,\"start\":39492},{\"end\":39880,\"start\":39815},{\"end\":40311,\"start\":40225},{\"end\":40676,\"start\":40611},{\"end\":41004,\"start\":40939},{\"end\":41333,\"start\":41288},{\"end\":41609,\"start\":41578},{\"end\":41764,\"start\":41739},{\"end\":41907,\"start\":41879},{\"end\":42140,\"start\":42128},{\"end\":42438,\"start\":42384},{\"end\":42761,\"start\":42696},{\"end\":43043,\"start\":42987},{\"end\":43273,\"start\":43268},{\"end\":43524,\"start\":43459},{\"end\":43799,\"start\":43762},{\"end\":44038,\"start\":43982}]"}}}, "year": 2023, "month": 12, "day": 17}