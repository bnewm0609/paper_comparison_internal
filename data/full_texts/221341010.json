{"id": 221341010, "updated": "2023-10-06 11:47:34.504", "metadata": {"title": "learn2learn: A Library for Meta-Learning Research", "authors": "[{\"first\":\"S'ebastien\",\"last\":\"Arnold\",\"middle\":[\"M.\",\"R.\"]},{\"first\":\"Praateek\",\"last\":\"Mahajan\",\"middle\":[]},{\"first\":\"Debajyoti\",\"last\":\"Datta\",\"middle\":[]},{\"first\":\"Ian\",\"last\":\"Bunner\",\"middle\":[]},{\"first\":\"Konstantinos\",\"last\":\"Zarkias\",\"middle\":[\"Saitas\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 8, "day": 27}, "abstract": "Meta-learning researchers face two fundamental issues in their empirical work: prototyping and reproducibility. Researchers are prone to make mistakes when prototyping new algorithms and tasks because modern meta-learning methods rely on unconventional functionalities of machine learning frameworks. In turn, reproducing existing results becomes a tedious endeavour -- a situation exacerbated by the lack of standardized implementations and benchmarks. As a result, researchers spend inordinate amounts of time on implementing software rather than understanding and developing new ideas. This manuscript introduces learn2learn, a library for meta-learning research focused on solving those prototyping and reproducibility issues. learn2learn provides low-level routines common across a wide-range of meta-learning techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning), and builds standardized interfaces to algorithms and benchmarks on top of them. In releasing learn2learn under a free and open source license, we hope to foster a community around standardized software for meta-learning research.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2008.12284", "mag": "3080894165", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2008-12284", "doi": null}}, "content": {"source": {"pdf_hash": "c55cc603b74b8ba0cc58dbaa1d8df1af94ab934b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.12284v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1ebefcb71d9db6f6957aeef9ac16a9a1ccba3be0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c55cc603b74b8ba0cc58dbaa1d8df1af94ab934b.txt", "contents": "\nlearn2learn: A Library for Meta-Learning Research\n28 Aug 2020\n\nS\u00e9bastien M R Arnold \nUniversity of Southern California\n\n\nPraateek Mahajan \nIterable, Inc\n\n\nDebajyoti Datta \nUniversity of Virginia\n\n\nUniversity of Waterloo\n\n\nIan Bunner \nKonstantinos Saitas Zarkias \nKTH Royal Institute of Technology and RISE -Research Institutes of Sweden\nSICS Website\n\n\nlearn2learn: A Library for Meta-Learning Research\n28 Aug 2020Software \u2022 Repository \u2022 Documentation \u2022 Tutorials Licence Authors of this paper retain copyright and release the work under a Creative Commons At-tribution 4.0 International Li-cense (CC-BY).\nMeta-learning researchers face two fundamental issues in their empirical work: prototyping and reproducibility. Researchers are prone to make mistakes when prototyping new algorithms and tasks because modern meta-learning methods rely on unconventional functionalities of machine learning frameworks. In turn, reproducing existing results becomes a tedious endeavour -a situation exacerbated by the lack of standardized implementations and benchmarks. As a result, researchers spend inordinate amounts of time on implementing software rather than understanding and developing new ideas.This manuscript introduces learn2learn, a library for meta-learning research focused on solving those prototyping and reproducibility issues. learn2learn provides low-level routines common across a wide-range of meta-learning techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning), and builds standardized interfaces to algorithms and benchmarks on top of them. In releasing learn2learn under a free and open source license, we hope to foster a community around standardized software for meta-learning research. * Correspondance to seb.arnold@usc.edu Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint\n\nIntroduction\n\nMeta-learning is the subfield of machine learning that endows computer programs with the ability of learning to learn. That is, the computer not only learns a behavior, but also how to adapt its behavior. To illustrate this difference, let us make an anology with the world of athleticism. If a learning program is akin to an athlete, a meta-learning program corresponds to the athlete-coach pair: it simultaneously learns a skill and how to teach it best. Meta-learning is appealing whenever the athlete is required to excel in multiple sports, as the coach can leverage shared aspects of each sport to accelerate mastery. Recent meta-learning methods emerged from this natural ability to multitaske.g. meta-descent in optimization, meta-reinforcement learning in reinforcement learning, and few-shot learning when labelled data is limited -reaching state-of-the-art performance on vision, language, and robotic domains. (Sutton 1992;Miller, Matsakis, and Viola 2000;Lee et al. 2019;Brown et al. 2020;Metz et al. 2019;Nagabandi et al. 2018) Unfortunately, modern meta-learning research is often slowed down by prototyping and reproducibility challenges. Prototyping algorithms is an error-prone process since metalearning algorithms rely on supported but exotic functionalities of machine learning software. (e.g. gradient of optimization steps) Consequently, a slow prototyping phase reduces the number of ideas one can try and retain. It also directly impacts reproducibility: researchers are more likely to make mistake with someone else's idea than with their own. Combined, those pesky issues prevent meaningful comparisons across publications, ultimately reducing the impact of any publication in the field.\n\nWhy do we suffer prototyping and reproducibility issues?\n\nAlthough a complete answer is outside the scope of this manuscript, we blame the lack of specialized software as a major culprit. A software library with specialized subroutines would reduce gaffes when prototyping, improving reproducibility too. A widely-adopted library also promotes standardized implementation of existing methods and benchmarks, an issue in prior research; for example, the community lost the original mini-ImageNet data splits from Vinyals et al. (2016), leaving subsequent work to replicate them as best they could. In summary, there is a need for a specialized software library which would alleviate many of the issues currently plaguing meta-learning research.\n\nWe introduce learn2learn, a software library that directly addresses prototyping and reproductibility issues in meta-learning research. learn2learn provides researchers with a unified and extensible interface to existing benchmarks, and a set of well-tested subroutines frequently used to implement meta-learning algorithms. The library also packages numerous examples replicating published methods, which can easily be adapted for comparisons on new benchmarks. learn2learn is implemented in Python to maintain compatiblity with the greater machine learning ecosystem. It extends PyTorch (Paszke et al. 2019) and leverages its fast linear algebra and automatic differentiation capabilites, while resorting to Cython (Behnel et al. 2011) when speed is required for data-handling. learn2learn is already used in our day-to-day research, and we hope its development will continue to benefit the wider meta-learning community.\n\nThe remainder of this document overviews the prototyping and reproducibility capabilities of learn2learn, including a review of related work.\n\n\nPrototyping\n\nPrototyping is essential in letting researchers quickly try new ideas. The faster the prototyping phase, the faster an idea can be retained (or discarded) for further exploration. learn2learn provides tools to accelerate two aspects of the prototyping phase: algorithms and domains.\n\n\nAlgorithms\n\nImplementing meta-learning algorithms can be tricky. For example, many methods rely Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint on computing gradients of algorithms -rather than gradients of functions -which, while possible with modern machine learning frameworks (e.g. PyTorch, TensorFlow, JAX), is strenuous and prone to errors. (Finn, Abbeel, and Levine 2017;Jacobsen et al. 2019;Xu, Hasselt, and Silver 2018) However, this doesn't need be the case: many differentiable algorithms can be implemented with minor changes when given the right abstractions. 1 1 learned_upda te = l2l . optim . ParameterUpd a te ( create_graph = True , 10 ) 11 # in -place , differentiab le update of clone parameters 12 l2l . update_modul e ( clone , updates ) 13 # gradients w . r . t model 's and learned_upda te 's parameters 14 loss ( clone ( X ) , y ) . backward () Snippet 1: Demonstration of differentiable optimization routines. Lines 1-4 instantiate a parameterized update function, which computes the gradient of a loss w.r.t. some module's parameters (clone, line 8) and passes them through a gradient transform -a module mapping gradients to updates. (here a KroneckerLinear, line 3) Line 5 creates a differentiable copy of the model, and line 11 updates that copy in-place such that the update is itself differentiable. Finally, line 13 backpropagates through the loss of the updated differentiable copy, thus computing gradients w.r.t. the \"pre-update\" model parameters and the KroneckerLinear parameters.\n\nTo ease the implementation of such methods, learn2learn exposes low-level routines for differentiable optimization in learn2learn.optim. These routines are tightly built around PyTorch's automatic differentiation engine, so as to maintain compatibility and extensibility. They can be used to express optimization algorithms such that their computational graph remains differentiable. Snippet 1 provides an example, which implements the 1step learning loop of the linear Kronecker-factored optimizer from Arnold, Iqbal, and Sha (2019). (Note how both model and optimizer parameters are meta-learned; implementing the same functionality with vanilla PyTorch requires 10x the lines of code.) We've used those general-purpose routines to implement algorithms from the literatures of fewshot, meta-descent, and meta-reinforcement learning -including MAML (Finn, Abbeel, and Levine 2017), Hypergradient descent (Baydin et al. 2017 transforms , 10 num_tasks =20000) 11 random_task = taskset . sample () # sample one task 12 for task in taskset : # iterate over all tasks 13 X , y = task Snippet 2: The interface to TaskDataset and TaskTransform. Line 1 wraps an arbitrary PyTorch dataset with the MetaDataset class. Lines 2-7 define TaskTransforms: here, 5-ways 1-shot classification with a custom random-rotation task augmentation applied to each (x, y) pair. Lines 8-10 instantiate the TaskDataset, from which tasks can be sampled (line 11) or enumerated (lines 12-13).\n\nFor few-shot meta-learning, learn2learn provides a general TaskDataset class enabling sampling of smaller tasks in learn2learn.data. Those tasks are constructed through a series of TaskTransforms, which iteratively refine the description of the data in the task. Writing a new task transform is as easy as writing a Python function, but those functions can be made arbitrarily complex thanks to Python's callable objects. See Snippet 2 for an example. With the combination of TaskDataset and TaskTransforms, researchers can quickly develop fast custom data and task sampling schemes, while retaining compatibility with any PyTorch dataset; this lets them quickly iterate over ideas with small datasets and scale up to larger experiments with the same codebase. Snippet 3: Utilities for meta-reinforcement learning environments. Lines 1-3 instantiate the halfcheetah environment, with tasks defined as running forward or backward. This environment is then wrapped by cherry, an external reinforcement learning library. Line 6 forks 16 asynchronous workers, each with its own copy of the half-cheetah environment. Finally, lines 7-8 sample 20 tasks and assign the first one to all workers.\n\nFor meta-reinforcement learning, learn2learn provides a high-level MetaEnv interface in learn2learn.gym which can be used to bootstrap the design of OpenAI Gym en-Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint vironments. (Brockman et al. 2016) Environments that adhere to this interface can take advantage of specially designed utilities included in learn2learn; for example, the AsyncVectorEnv wrapper parallelizes the collection of episodes across multiple processes.\n\n(c.f. Snippet 3) Such environments also retain the Gym API, making them compatible with all popular reinforcement learning libraries. (Dhariwal et al. 2017;Liang et al. 2017) Naturally, they also become compatible with the various meta-reinforcement learning algorithms implemented within the library.\n\nThe core prototyping tools toured in the above paragraphs open the door to more advanced developments such as online, incremental, or lifelong meta-learning. While this manuscript can only present a bird's-eye view of those tools, we invite the reader to the library's website and documentation for such advanced applications.\n\n\nReproducibility\n\nThe field of meta-learning is advancing rapidly, but progress is plagued by reproducibility issues. Those issues are often subtle and hard to spot. In meta-reinforcement learning for example, different papers have used different reward functions with the same environment, resulting in confusing comparisons: are the observed improvements due to algorithmic advancements or to changes in the reward function? To combat those insiduous issues, learn2learn includes a set of high-quality implementations for various meta-learning algorithms as well as standardized benchmarks for few-shot and meta-reinforcement learning. \n\n\nImplementations\n\nlearn2learn provides high-level implementations for popular algorithms. These implementations build on top of the low-level routines from the previous section, and are thoroughly tested to replicate published works. They typically wrap around PyTorch modules to extend Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint them with specific meta-learning functionalities. For example, the LearnableOptimizer retains the familiar PyTorch Optimizer interface and extends it to learn arbitrary metaoptimization updates; similarly, the GBML augments PyTorch Modules to support fastadaptation routines for few-shot and meta-reinforcement learning. GBML implementations of Meta-SGD, Meta-Curvature, Meta-KFO are available in Snippet 4. (Li et al. 2017;Park and Oliva 2019;Arnold, Iqbal, and Sha 2019) With those high-level implementations and the standardized benchmark interface described below, we supply examples that exactly reproduce published experiments. Those examples serve three purposes. First, they validate the correctness of our implementation and a publication's claims; second, they illustrate how to use the library; third, they fill the need for unified standardized reproductions. As a by-product, those examples can be used to bootstrap further experimentation and analysis around a method of interest. For example, we could easily complement ANIL's (Raghu et al. 2019) original results on the Omniglot (Lake, Salakhutdinov, and Tenenbaum 2015) and mini-Imagenet (Vinyals et al. 2016) datasets with new results on CIFAR-FS (Bertinetto et al. 2018) and FC100 (Oreshkin, Rodr\u00edguez L\u00f3pez, and Lacoste 2018).\n\n1 from learn2learn . vision import benchmarks 2 print ( benchmarks . list_tasksets () ) 3 # [ ' omniglot ', ' cifar -fs ', ' fc100 ', ' mini -imagenet ', ...] 4 tasksets = benchmarks . get_tasksets ( # standardized pipeline 5 name = ' mini -imagenet ' , 6 train_sample s =10 , 7 train_ways =5) 8 task = tasksets . train . sample () # tasksets . train is a task dataset Snippet 5: High-level API to standardized computer vision benchmarks. Line 2 prints the list of available tasksets. On lines 4-7, we instantiate the 5-ways 5-shots mini-ImageNet benchmark, which returns the tasksets namedtuple with train, validation, and test keys. Line 8 samples a task from the train TaskDataset.\n\n\nBenchmarks\n\nWe use the low-level domains API to implement standard benchmarks in few-shot and meta-reinforcement learning settings. For few-shot learning, learn2learn provides classes to download and preprocess datasets commonly used by the community in learn2learn.vision. In addition, it also includes task definitions with the proper task-processing stages (image normalization, rotation, cropping) for commonly reported settings such as 5-ways 1-shot, 5-ways 5-shots, and 20-ways 5-shots. An example of those task definitions is described in Snippet 5.\n\nThe meta-reinforcement learning environments range from simple 2D-particle navigation to robotics control. In particular, we include simple to use wrappers for the recently proposed MetaWorld, a set of 50 gripper manipulation tasks with varying levels of difficulty. Those benchmark implementations should greatly simplify the replication and comparison of new methods on well-studied settings. They, and other meta-reinforcement learning utilities, are included in learn2learn.gym.\n\nCombined with the provided implementations, we hope learn2learn enables researchers to accelerate the process of correctly comparing their ideas against existing methods.\n\n\nRelated Work\n\nDisparate implementations of individual algorithms set aside, two recent libraries tackle similar challenges as learn2learn.\n\nThe first one, higher (Grefenstette et al. 2019), aims to facilitate the implementation of \"generalized inner-loop meta-algorithms\" -in other words, the implementation of differentiable optimization algorithms. higher treats a model definition (e.g. a neural network) as a symbolic computational graph, for which they use one set of parameters or another based on user-specified context. This \"stateless\" parameterization is as expressive as the learn2learn.optim submodule -after all, both are implemented on top of PyTorchbut requires researchers to carefully understand when they are working with symbolic or declarative parts of their computation. Instead, learn2learn sticks with a stateful and declarative style, already familiar to the PyTorch research userbase. Moreover, higher completely forgoes reproducibility issues as its focus is on the implementation of novel algorithms.\n\nThe second one, Torchmeta (Deleu et al. 2019), intends to provide a unified interface to popular datasets, including classes to easily download and process them. Specifically, it focuses on standardized few-shot computer vision tasks, allowing researchers to easily swap one dataset for another. However, supporting new datasets with Torchmeta requires implementing a bridging class, even if the dataset is already in standard PyTorch format. On the other hand, learn2learn's TaskDataset explicitly avoids such bridging classes, and is designed to be compatible with any PyTorch dataset (including text, speech, and others). Torchmeta also povides a thin algorithmic wrapper to demonstrate the usage of their library with gradient-based meta-learning algorithms; but, this wrapper is not compatible with the majority of PyTorch's layers, nor custom modules implemented by researchers. In comparison, learn2learn's differentiable optimization submodule uniformly handles all PyTorch Modules (including custom ones), making it a more applicable research tool.\n\nOverall, learn2learn offers a more general solution to the prototyping and reproduciblity issues encountered in day-to-day research. For example, at the time of this writing, neither of the above libraries supports meta-descent or meta-reinforcement learning.\n\n\nConclusion\n\nThis manuscript introduces learn2learn, a library that tackles prototyping and reproducibility issues in modern meta-learning. learn2learn's low-level routines facilitate rapid prototyping of new algorithms and domains. The library builds on those low-level routines to provide high-level implementations and standardized benchmarks API, which enable researchers to easily and faithfully compare different methods under different settings. Notably, both low-and high-level utilities are engineered to be general: they are compatible with a wide range of meta-learning techniques in few-shot learning, meta-reinforcement learning, or meta-optimization. Finally, learn2learn is released under the free and open source MIT licence, and the focus of continued development.\n\n\noptim . KroneckerT r an s fo rm ( l2l . nn . KroneckerLin ea r ) 4 ) 5 clone = l2l . clone_module ( model ) # torch . clone () for nn . Modules 6 updates = learned_upda te ( # similar API as torch . autograd .\n\n\nuse 16 processes , compatible with gym API 6 env = l2l . gym . AsyncVectorE nv ([ make_env for i in range (16) ]) 7 tasks = env . sample_tasks (20) 8 env . set_task ( tasks [0]) # all processes run task 0\n\n\n. optim . ModuleTrans fo rm ( l2l . nn . Scale ) , 4 ) 5 meta_curvatu re = l2l . algorithms . GBML ( . optim . KroneckerT r an s fo rm ( l2l . nn . KroneckerLin ea r ) , 12 adapt_trans f or m = True , 13 ) Snippet 4: Implementation of Meta-SGD (lines 1-4), Meta-Curvature (lines 5-8), and Meta-KFO (lines 9-13) with the GBML wrapper. Each variant differs in how the fast-adaptation gradients are transformed, which is reflected through the transform and adapt_transform arguments.\n\n\n), or ProMP(Rothfuss et al. 2018) among others.Researchers have to design new domains -such as datasets, tasks, or environmentsto develop and test new abilities of their programs. We refer to this other aspect of the research lifecycle as prototyping new domains. learn2learn can help prototype new domains for few-shot and meta-reinforcement learning.1 dataset = l2l . data . MetaDataset ( MyDataset () ) # PyTorch dataset 2 transforms = [ # easy to define custom task transforms l2l . data . transforms . NWays ( dataset , n =5) , l2l . data . transforms . KShots ( dataset , k =1) , l2l . data . transforms . LoadData ( dataset ) , lambda task : [( random_rota t io n ( x ) , y ) for x , y in task ] 7 ] 8 taskset = l2l . data . TaskDataset ( dataset ,Domains \n\n3 \n\n4 \n\n5 \n\n6 \n\n9 \n\n\nSee for example Agrawal et al. (2019). Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint\nAcknowledgementsWe thank Fei Sha for providing excellent research environment and guidance, and for the computational resources required to develop learn2learn. We also thank the various users who -through their questions, comments, or contributions -helped improve learn2learn.This work is partially supported by NSF IIS-1065243,1451412, 1513966/ 1632803/1833137, 1208500, CCF-1139148,DARPA Award#: FA8750-18-2-0117, DARPA-D3M -AwardUCB-00009528, Google Research Awards, an Alfred P. SloanResearch Fellowship, gifts from Facebook and Netflix, and ARO#W911NF-12-1-0241 and W911NF-15-1-0484. Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint\nDifferentiable Convex Optimization Layers. A Agrawal, B Amos, S Barratt, S Boyd, S Diamond, Z Kolter, Advances in Neural Information Processing Systems. Agrawal, A., B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter. 2019. \"Dif- ferentiable Convex Optimization Layers.\" In Advances in Neural Information Processing Systems.\n\nWhen MAML Can Adapt Fast and How to Assist When It Cannot. Arnold, M R S\u00e9bastien, Shariq Iqbal, Fei Sha, Arnold, S\u00e9bastien M R, Shariq Iqbal, and Fei Sha. 2019. \"When MAML Can Adapt Fast and How to Assist When It Cannot,\" October.\n\nOnline Learning Rate Adaptation with Hypergradient Descent. Atilim Baydin, Robert Gunes, David Martinez Cornish, Mark Rubio, Schmidt, Baydin, Atilim Gunes, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. 2017. \"Online Learning Rate Adaptation with Hypergradient Descent,\" March.\n\nDag Sverre Seljebotn, and Kurt Smith. Stefan Behnel, Robert Bradshaw, Craig Citro, Lisandro Dalcin, Computing in Science & Engineering. 132Cython: The Best of Both WorldsBehnel, Stefan, Robert Bradshaw, Craig Citro, Lisandro Dalcin, Dag Sverre Seljebotn, and Kurt Smith. 2011. \"Cython: The Best of Both Worlds.\" Computing in Science & Engineering 13 (2). IEEE: 31-39.\n\nMeta-Learning with Differentiable Closed-Form Solvers. Luca Bertinetto, Joao F Henriques, Philip Torr, Andrea Vedaldi, Bertinetto, Luca, Joao F Henriques, Philip Torr, and Andrea Vedaldi. 2018. \"Meta- Learning with Differentiable Closed-Form Solvers.\"\n\nOpenAI Gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Brockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. \"OpenAI Gym,\" June.\n\nLanguage Models Are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. \"Language Models Are Few-Shot Learners,\" May.\n\nTorchmeta: A Meta-Learning Library for PyTorch. Tristan Deleu, Tobias W\u00fcrfl, Mandana Samiei, Joseph Paul Cohen, Yoshua Bengio, Deleu, Tristan, Tobias W\u00fcrfl, Mandana Samiei, Joseph Paul Cohen, and Yoshua Bengio. 2019. \"Torchmeta: A Meta-Learning Library for PyTorch,\" September.\n\nGitHub Repository. Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, Peter Zhokhov, OpenAI BaselinesDhariwal, Prafulla, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. 2017. \"OpenAI Baselines.\" GitHub Repository.\n\nBenchmarking Deep Reinforcement Learning for Continuous Control. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel, Duan, Yan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. 2016. \"Bench- marking Deep Reinforcement Learning for Continuous Control,\" April.\n\nRL 2 : Fast Reinforcement Learning via Slow Reinforcement Learning. Yan Duan, John Schulman, Xi Chen, L Peter, Ilya Bartlett, Pieter Sutskever, Abbeel, Duan, Yan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. 2016. \"RL 2 : Fast Reinforcement Learning via Slow Reinforcement Learning,\" November.\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, arXiv:1703.03400arXiv PreprintFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv Preprint arXiv:1703. 03400.\n\nPhu Mon Htut, Artem Molchanov, Franziska Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Edward Grefenstette, Brandon Amos, Denis Yarats, Generalized Inner Loop Meta-LearningGrefenstette, Edward, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. 2019. \"Generalized Inner Loop Meta-Learning,\" October.\n\nMeta-Descent for Online, Continual Prediction. Andrew Jacobsen, Matthew Schlegel, Cameron Linke, Thomas Degris, Adam White, Martha White, Jacobsen, Andrew, Matthew Schlegel, Cameron Linke, Thomas Degris, Adam White, and Martha White. 2019. \"Meta-Descent for Online, Continual Prediction,\" July.\n\nHuman-Level Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint Concept Learning Through Probabilistic Program Induction. Lake, M Brenden, Ruslan Salakhutdinov, Joshua B Tenenbaum, Science. 3506266Lake, Brenden M, Ruslan Salakhutdinov, and Joshua B Tenenbaum. 2015. \"Human-Level Arnold et al., (2020). learn2learn: A Library for Meta-Learning Research. Preprint Concept Learning Through Probabilistic Program Induction.\" Science 350 (6266): 1332-8.\n\nMeta-Learning with Differentiable Convex Optimization. Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, Stefano Soatto, Lee, Kwonjoon, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. 2019. \"Meta- Learning with Differentiable Convex Optimization,\" April.\n\nZhenguo Li, Fengwei Zhou, Fei Chen, Hang Li, Meta-SGD: Learning to Learn Quickly for Few-Shot Learning. Li, Zhenguo, Fengwei Zhou, Fei Chen, and Hang Li. 2017. \"Meta-SGD: Learning to Learn Quickly for Few-Shot Learning,\" July.\n\nRLlib: Abstractions for Distributed Reinforcement Learning. Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E Gonzalez, Michael I Jordan, Ion Stoica, Liang, Eric, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E Gonzalez, Michael I Jordan, and Ion Stoica. 2017. \"RLlib: Abstractions for Distributed Reinforcement Learning,\" December.\n\nUsing Learned Optimizers to Make Models Robust to Input Noise. Luke Metz, Niru Maheswaranathan, Jonathon Shlens, Jascha Sohl-Dickstein, Ekin D Cubuk, Metz, Luke, Niru Maheswaranathan, Jonathon Shlens, Jascha Sohl-Dickstein, and Ekin D Cubuk. 2019. \"Using Learned Optimizers to Make Models Robust to Input Noise,\" June.\n\nLearning from One Example Through Shared Densities on Transforms. E G Miller, N E Matsakis, P A Viola, Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR. IEEE Conference on Computer Vision and Pattern Recognition. CVPR1Miller, E G, N E Matsakis, and P A Viola. 2000. \"Learning from One Example Through Shared Densities on Transforms.\" In Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662), 1:464-71 vol.1.\n\nLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning. Anusha Nagabandi, Ignasi Clavera, Simin Liu, S Ronald, Pieter Fearing, Sergey Abbeel, Chelsea Levine, Finn, Nagabandi, Anusha, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. 2018. \"Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,\" March.\n\nTADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning. Boris Oreshkin, Alexandre Pau Rodr\u00edguez L\u00f3pez, ; S Lacoste, H Bengio, H Wallach, Larochelle, Grauman, Cesa-Bianchi, Junier B Garnett ; Eunbyung, Oliva, Advances in Neural Information Processing Systems. Curran Associates, Inc. Park31Meta-CurvatureOreshkin, Boris, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" In Advances in Neural Information Processing Systems 31, edited by S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, and R Garnett, 721-31. Curran Associates, Inc. Park, Eunbyung, and Junier B Oliva. 2019. \"Meta-Curvature,\" February.\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Advances in Neural Information Processing Systems. Curran Associates, Inc32Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. \"PyTorch: An Imperative Style, High-Performance Deep Learning Library.\" In Advances in Neural Information Processing Systems 32, edited by H Wallach, H Larochelle, A Beygelzimer, F d'Alch\u00e9-Buc, E Fox, and R Garnett, 8026-37. Curran Associates, Inc.\n\nRapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol VinyalsRaghu, Aniruddh, Maithra Raghu, Samy Bengio, and Oriol Vinyals. 2019. \"Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML,\" September.\n\nProMP: Proximal Meta-Policy Search. Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Rothfuss, Jonas, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. 2018. \"ProMP: Proximal Meta-Policy Search.\"\n\nAdapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta. Richard S Sutton, Sutton, Richard S. 1992. \"Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta,\" June.\n\nMatching Networks for One Shot Learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra, ; D D Lee, U V Sugiyama, Luxburg, R Guyon, Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc29Vinyals, Oriol, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 2016. \"Matching Networks for One Shot Learning.\" In Advances in Neural Information Processing Systems 29, edited by D D Lee, M Sugiyama, U V Luxburg, I Guyon, and R Garnett, 3630-8. Curran Associates, Inc.\n\nMeta-Gradient Reinforcement Learning. Zhongwen Xu, Hado Van Hasselt, David Silver, Xu, Zhongwen, Hado van Hasselt, and David Silver. 2018. \"Meta-Gradient Reinforcement Learning,\" May.\n", "annotations": {"author": "[{\"end\":121,\"start\":64},{\"end\":155,\"start\":122},{\"end\":222,\"start\":156},{\"end\":234,\"start\":223},{\"end\":352,\"start\":235}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":78},{\"end\":138,\"start\":131},{\"end\":171,\"start\":166},{\"end\":233,\"start\":227},{\"end\":262,\"start\":255}]", "author_first_name": "[{\"end\":73,\"start\":64},{\"end\":77,\"start\":74},{\"end\":130,\"start\":122},{\"end\":165,\"start\":156},{\"end\":226,\"start\":223},{\"end\":247,\"start\":235},{\"end\":254,\"start\":248}]", "author_affiliation": "[{\"end\":120,\"start\":86},{\"end\":154,\"start\":140},{\"end\":196,\"start\":173},{\"end\":221,\"start\":198},{\"end\":351,\"start\":264}]", "title": "[{\"end\":50,\"start\":1},{\"end\":402,\"start\":353}]", "venue": null, "abstract": "[{\"end\":1849,\"start\":606}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2800,\"start\":2787},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2833,\"start\":2800},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2849,\"start\":2833},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2867,\"start\":2849},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2884,\"start\":2867},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2906,\"start\":2884},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4114,\"start\":4093},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4934,\"start\":4915},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5063,\"start\":5043},{\"end\":5810,\"start\":5789},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6106,\"start\":6075},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6127,\"start\":6106},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6156,\"start\":6127},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7781,\"start\":7752},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8129,\"start\":8098},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8172,\"start\":8153},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10183,\"start\":10161},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10567,\"start\":10545},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10584,\"start\":10567},{\"end\":11990,\"start\":11969},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12476,\"start\":12460},{\"end\":12496,\"start\":12476},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12524,\"start\":12496},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13113,\"start\":13094},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13188,\"start\":13147},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13228,\"start\":13207},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13290,\"start\":13267},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13347,\"start\":13302},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15440,\"start\":15414},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16326,\"start\":16307},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19319,\"start\":19298}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18594,\"start\":18383},{\"attributes\":{\"id\":\"fig_1\"},\"end\":18801,\"start\":18595},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19284,\"start\":18802},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":20072,\"start\":19285}]", "paragraph": "[{\"end\":3579,\"start\":1865},{\"end\":3637,\"start\":3581},{\"end\":4324,\"start\":3639},{\"end\":5249,\"start\":4326},{\"end\":5392,\"start\":5251},{\"end\":5690,\"start\":5408},{\"end\":7246,\"start\":5705},{\"end\":8712,\"start\":7248},{\"end\":9901,\"start\":8714},{\"end\":10409,\"start\":9903},{\"end\":10712,\"start\":10411},{\"end\":11040,\"start\":10714},{\"end\":11680,\"start\":11060},{\"end\":13348,\"start\":11700},{\"end\":14034,\"start\":13350},{\"end\":14593,\"start\":14049},{\"end\":15077,\"start\":14595},{\"end\":15249,\"start\":15079},{\"end\":15390,\"start\":15266},{\"end\":16279,\"start\":15392},{\"end\":17338,\"start\":16281},{\"end\":17599,\"start\":17340},{\"end\":18382,\"start\":17614}]", "formula": null, "table_ref": null, "section_header": "[{\"end\":1863,\"start\":1851},{\"end\":5406,\"start\":5395},{\"end\":5703,\"start\":5693},{\"end\":11058,\"start\":11043},{\"end\":11698,\"start\":11683},{\"end\":14047,\"start\":14037},{\"end\":15264,\"start\":15252},{\"end\":17612,\"start\":17602}]", "table": "[{\"end\":20072,\"start\":20042}]", "figure_caption": "[{\"end\":18594,\"start\":18385},{\"end\":18801,\"start\":18597},{\"end\":19284,\"start\":18804},{\"end\":20042,\"start\":19287}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":20913,\"start\":20912},{\"end\":20924,\"start\":20923},{\"end\":20932,\"start\":20931},{\"end\":20943,\"start\":20942},{\"end\":20951,\"start\":20950},{\"end\":20962,\"start\":20961},{\"end\":21270,\"start\":21267},{\"end\":21288,\"start\":21282},{\"end\":21299,\"start\":21296},{\"end\":21498,\"start\":21492},{\"end\":21513,\"start\":21507},{\"end\":21526,\"start\":21521},{\"end\":21535,\"start\":21527},{\"end\":21549,\"start\":21545},{\"end\":21776,\"start\":21770},{\"end\":21791,\"start\":21785},{\"end\":21807,\"start\":21802},{\"end\":21823,\"start\":21815},{\"end\":22160,\"start\":22156},{\"end\":22177,\"start\":22173},{\"end\":22179,\"start\":22178},{\"end\":22197,\"start\":22191},{\"end\":22210,\"start\":22204},{\"end\":22370,\"start\":22366},{\"end\":22386,\"start\":22381},{\"end\":22401,\"start\":22395},{\"end\":22419,\"start\":22414},{\"end\":22435,\"start\":22431},{\"end\":22449,\"start\":22446},{\"end\":22464,\"start\":22456},{\"end\":22656,\"start\":22653},{\"end\":22658,\"start\":22657},{\"end\":22674,\"start\":22666},{\"end\":22685,\"start\":22681},{\"end\":22700,\"start\":22693},{\"end\":22715,\"start\":22710},{\"end\":22732,\"start\":22724},{\"end\":22749,\"start\":22743},{\"end\":22989,\"start\":22982},{\"end\":23003,\"start\":22997},{\"end\":23018,\"start\":23011},{\"end\":23033,\"start\":23027},{\"end\":23038,\"start\":23034},{\"end\":23052,\"start\":23046},{\"end\":23240,\"start\":23232},{\"end\":23262,\"start\":23251},{\"end\":23274,\"start\":23270},{\"end\":23287,\"start\":23283},{\"end\":23304,\"start\":23296},{\"end\":23319,\"start\":23315},{\"end\":23333,\"start\":23329},{\"end\":23350,\"start\":23344},{\"end\":23364,\"start\":23358},{\"end\":23374,\"start\":23369},{\"end\":23671,\"start\":23668},{\"end\":23680,\"start\":23678},{\"end\":23691,\"start\":23687},{\"end\":23707,\"start\":23703},{\"end\":23724,\"start\":23718},{\"end\":23957,\"start\":23954},{\"end\":23968,\"start\":23964},{\"end\":23981,\"start\":23979},{\"end\":23989,\"start\":23988},{\"end\":24001,\"start\":23997},{\"end\":24018,\"start\":24012},{\"end\":24287,\"start\":24280},{\"end\":24300,\"start\":24294},{\"end\":24315,\"start\":24309},{\"end\":24617,\"start\":24611},{\"end\":24639,\"start\":24632},{\"end\":24651,\"start\":24646},{\"end\":24953,\"start\":24947},{\"end\":24971,\"start\":24964},{\"end\":24989,\"start\":24982},{\"end\":25003,\"start\":24997},{\"end\":25016,\"start\":25012},{\"end\":25030,\"start\":25024},{\"end\":25356,\"start\":25355},{\"end\":25372,\"start\":25366},{\"end\":25394,\"start\":25388},{\"end\":25396,\"start\":25395},{\"end\":25740,\"start\":25732},{\"end\":25755,\"start\":25746},{\"end\":25769,\"start\":25762},{\"end\":25791,\"start\":25784},{\"end\":25952,\"start\":25945},{\"end\":25964,\"start\":25957},{\"end\":25974,\"start\":25971},{\"end\":25985,\"start\":25981},{\"end\":26237,\"start\":26233},{\"end\":26252,\"start\":26245},{\"end\":26266,\"start\":26259},{\"end\":26281,\"start\":26275},{\"end\":26296,\"start\":26293},{\"end\":26305,\"start\":26302},{\"end\":26322,\"start\":26316},{\"end\":26324,\"start\":26323},{\"end\":26342,\"start\":26335},{\"end\":26344,\"start\":26343},{\"end\":26356,\"start\":26353},{\"end\":26648,\"start\":26644},{\"end\":26659,\"start\":26655},{\"end\":26685,\"start\":26677},{\"end\":26700,\"start\":26694},{\"end\":26723,\"start\":26717},{\"end\":26970,\"start\":26967},{\"end\":26982,\"start\":26979},{\"end\":26996,\"start\":26993},{\"end\":27481,\"start\":27475},{\"end\":27499,\"start\":27493},{\"end\":27514,\"start\":27509},{\"end\":27521,\"start\":27520},{\"end\":27536,\"start\":27530},{\"end\":27552,\"start\":27546},{\"end\":27568,\"start\":27561},{\"end\":27877,\"start\":27872},{\"end\":27897,\"start\":27888},{\"end\":27922,\"start\":27919},{\"end\":27933,\"start\":27932},{\"end\":27943,\"start\":27942},{\"end\":27996,\"start\":27988},{\"end\":28579,\"start\":28575},{\"end\":28591,\"start\":28588},{\"end\":28608,\"start\":28599},{\"end\":28620,\"start\":28616},{\"end\":28633,\"start\":28628},{\"end\":28651,\"start\":28644},{\"end\":28666,\"start\":28660},{\"end\":29206,\"start\":29198},{\"end\":29221,\"start\":29214},{\"end\":29466,\"start\":29461},{\"end\":29483,\"start\":29477},{\"end\":29495,\"start\":29489},{\"end\":29746,\"start\":29739},{\"end\":29748,\"start\":29747},{\"end\":29915,\"start\":29910},{\"end\":29932,\"start\":29925},{\"end\":29950,\"start\":29943},{\"end\":29967,\"start\":29962},{\"end\":29985,\"start\":29981},{\"end\":30001,\"start\":29996},{\"end\":30010,\"start\":30007},{\"end\":30031,\"start\":30030},{\"end\":30467,\"start\":30459},{\"end\":30476,\"start\":30472},{\"end\":30495,\"start\":30490}]", "bib_author_last_name": "[{\"end\":20921,\"start\":20914},{\"end\":20929,\"start\":20925},{\"end\":20940,\"start\":20933},{\"end\":20948,\"start\":20944},{\"end\":20959,\"start\":20952},{\"end\":20969,\"start\":20963},{\"end\":21265,\"start\":21259},{\"end\":21280,\"start\":21271},{\"end\":21294,\"start\":21289},{\"end\":21303,\"start\":21300},{\"end\":21505,\"start\":21499},{\"end\":21519,\"start\":21514},{\"end\":21543,\"start\":21536},{\"end\":21555,\"start\":21550},{\"end\":21564,\"start\":21557},{\"end\":21783,\"start\":21777},{\"end\":21800,\"start\":21792},{\"end\":21813,\"start\":21808},{\"end\":21830,\"start\":21824},{\"end\":22171,\"start\":22161},{\"end\":22189,\"start\":22180},{\"end\":22202,\"start\":22198},{\"end\":22218,\"start\":22211},{\"end\":22379,\"start\":22371},{\"end\":22393,\"start\":22387},{\"end\":22412,\"start\":22402},{\"end\":22429,\"start\":22420},{\"end\":22444,\"start\":22436},{\"end\":22454,\"start\":22450},{\"end\":22472,\"start\":22465},{\"end\":22664,\"start\":22659},{\"end\":22679,\"start\":22675},{\"end\":22691,\"start\":22686},{\"end\":22708,\"start\":22701},{\"end\":22722,\"start\":22716},{\"end\":22741,\"start\":22733},{\"end\":22761,\"start\":22750},{\"end\":22995,\"start\":22990},{\"end\":23009,\"start\":23004},{\"end\":23025,\"start\":23019},{\"end\":23044,\"start\":23039},{\"end\":23059,\"start\":23053},{\"end\":23249,\"start\":23241},{\"end\":23268,\"start\":23263},{\"end\":23281,\"start\":23275},{\"end\":23294,\"start\":23288},{\"end\":23313,\"start\":23305},{\"end\":23327,\"start\":23320},{\"end\":23342,\"start\":23334},{\"end\":23356,\"start\":23351},{\"end\":23367,\"start\":23365},{\"end\":23382,\"start\":23375},{\"end\":23676,\"start\":23672},{\"end\":23685,\"start\":23681},{\"end\":23701,\"start\":23692},{\"end\":23716,\"start\":23708},{\"end\":23731,\"start\":23725},{\"end\":23962,\"start\":23958},{\"end\":23977,\"start\":23969},{\"end\":23986,\"start\":23982},{\"end\":23995,\"start\":23990},{\"end\":24010,\"start\":24002},{\"end\":24028,\"start\":24019},{\"end\":24036,\"start\":24030},{\"end\":24292,\"start\":24288},{\"end\":24307,\"start\":24301},{\"end\":24322,\"start\":24316},{\"end\":24630,\"start\":24618},{\"end\":24644,\"start\":24640},{\"end\":24658,\"start\":24652},{\"end\":24962,\"start\":24954},{\"end\":24980,\"start\":24972},{\"end\":24995,\"start\":24990},{\"end\":25010,\"start\":25004},{\"end\":25022,\"start\":25017},{\"end\":25036,\"start\":25031},{\"end\":25353,\"start\":25349},{\"end\":25364,\"start\":25357},{\"end\":25386,\"start\":25373},{\"end\":25406,\"start\":25397},{\"end\":25744,\"start\":25741},{\"end\":25760,\"start\":25756},{\"end\":25782,\"start\":25770},{\"end\":25798,\"start\":25792},{\"end\":25955,\"start\":25953},{\"end\":25969,\"start\":25965},{\"end\":25979,\"start\":25975},{\"end\":25988,\"start\":25986},{\"end\":26243,\"start\":26238},{\"end\":26257,\"start\":26253},{\"end\":26273,\"start\":26267},{\"end\":26291,\"start\":26282},{\"end\":26300,\"start\":26297},{\"end\":26314,\"start\":26306},{\"end\":26333,\"start\":26325},{\"end\":26351,\"start\":26345},{\"end\":26363,\"start\":26357},{\"end\":26653,\"start\":26649},{\"end\":26675,\"start\":26660},{\"end\":26692,\"start\":26686},{\"end\":26715,\"start\":26701},{\"end\":26729,\"start\":26724},{\"end\":26977,\"start\":26971},{\"end\":26991,\"start\":26983},{\"end\":27002,\"start\":26997},{\"end\":27491,\"start\":27482},{\"end\":27507,\"start\":27500},{\"end\":27518,\"start\":27515},{\"end\":27528,\"start\":27522},{\"end\":27544,\"start\":27537},{\"end\":27559,\"start\":27553},{\"end\":27575,\"start\":27569},{\"end\":27581,\"start\":27577},{\"end\":27886,\"start\":27878},{\"end\":27917,\"start\":27898},{\"end\":27930,\"start\":27923},{\"end\":27940,\"start\":27934},{\"end\":27951,\"start\":27944},{\"end\":27963,\"start\":27953},{\"end\":27972,\"start\":27965},{\"end\":27986,\"start\":27974},{\"end\":28015,\"start\":27997},{\"end\":28022,\"start\":28017},{\"end\":28586,\"start\":28580},{\"end\":28597,\"start\":28592},{\"end\":28614,\"start\":28609},{\"end\":28626,\"start\":28621},{\"end\":28642,\"start\":28634},{\"end\":28658,\"start\":28652},{\"end\":28674,\"start\":28667},{\"end\":29212,\"start\":29207},{\"end\":29227,\"start\":29222},{\"end\":29475,\"start\":29467},{\"end\":29487,\"start\":29484},{\"end\":29503,\"start\":29496},{\"end\":29755,\"start\":29749},{\"end\":29923,\"start\":29916},{\"end\":29941,\"start\":29933},{\"end\":29960,\"start\":29951},{\"end\":29979,\"start\":29968},{\"end\":29994,\"start\":29986},{\"end\":30005,\"start\":30002},{\"end\":30019,\"start\":30011},{\"end\":30028,\"start\":30021},{\"end\":30037,\"start\":30032},{\"end\":30046,\"start\":30039},{\"end\":30470,\"start\":30468},{\"end\":30488,\"start\":30477},{\"end\":30502,\"start\":30496}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":202786139},\"end\":21198,\"start\":20869},{\"attributes\":{\"id\":\"b1\"},\"end\":21430,\"start\":21200},{\"attributes\":{\"id\":\"b2\"},\"end\":21730,\"start\":21432},{\"attributes\":{\"id\":\"b3\"},\"end\":22099,\"start\":21732},{\"attributes\":{\"id\":\"b4\"},\"end\":22352,\"start\":22101},{\"attributes\":{\"id\":\"b5\"},\"end\":22612,\"start\":22354},{\"attributes\":{\"id\":\"b6\"},\"end\":22932,\"start\":22614},{\"attributes\":{\"id\":\"b7\"},\"end\":23211,\"start\":22934},{\"attributes\":{\"id\":\"b8\"},\"end\":23601,\"start\":23213},{\"attributes\":{\"id\":\"b9\"},\"end\":23884,\"start\":23603},{\"attributes\":{\"id\":\"b10\"},\"end\":24211,\"start\":23886},{\"attributes\":{\"doi\":\"arXiv:1703.03400\",\"id\":\"b11\"},\"end\":24511,\"start\":24213},{\"attributes\":{\"id\":\"b12\"},\"end\":24898,\"start\":24513},{\"attributes\":{\"id\":\"b13\"},\"end\":25194,\"start\":24900},{\"attributes\":{\"id\":\"b14\"},\"end\":25675,\"start\":25196},{\"attributes\":{\"id\":\"b15\"},\"end\":25943,\"start\":25677},{\"attributes\":{\"id\":\"b16\"},\"end\":26171,\"start\":25945},{\"attributes\":{\"id\":\"b17\"},\"end\":26579,\"start\":26173},{\"attributes\":{\"id\":\"b18\"},\"end\":26899,\"start\":26581},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2699786},\"end\":27382,\"start\":26901},{\"attributes\":{\"id\":\"b20\"},\"end\":27800,\"start\":27384},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":44061218},\"end\":28503,\"start\":27802},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202786778},\"end\":29114,\"start\":28505},{\"attributes\":{\"id\":\"b23\"},\"end\":29423,\"start\":29116},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53015479},\"end\":29659,\"start\":29425},{\"attributes\":{\"id\":\"b25\"},\"end\":29867,\"start\":29661},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8909022},\"end\":30419,\"start\":29869},{\"attributes\":{\"id\":\"b27\"},\"end\":30604,\"start\":30421}]", "bib_title": "[{\"end\":20910,\"start\":20869},{\"end\":21768,\"start\":21732},{\"end\":25347,\"start\":25196},{\"end\":26965,\"start\":26901},{\"end\":27870,\"start\":27802},{\"end\":28573,\"start\":28505},{\"end\":29459,\"start\":29425},{\"end\":29908,\"start\":29869}]", "bib_author": "[{\"end\":20923,\"start\":20912},{\"end\":20931,\"start\":20923},{\"end\":20942,\"start\":20931},{\"end\":20950,\"start\":20942},{\"end\":20961,\"start\":20950},{\"end\":20971,\"start\":20961},{\"end\":21267,\"start\":21259},{\"end\":21282,\"start\":21267},{\"end\":21296,\"start\":21282},{\"end\":21305,\"start\":21296},{\"end\":21507,\"start\":21492},{\"end\":21521,\"start\":21507},{\"end\":21545,\"start\":21521},{\"end\":21557,\"start\":21545},{\"end\":21566,\"start\":21557},{\"end\":21785,\"start\":21770},{\"end\":21802,\"start\":21785},{\"end\":21815,\"start\":21802},{\"end\":21832,\"start\":21815},{\"end\":22173,\"start\":22156},{\"end\":22191,\"start\":22173},{\"end\":22204,\"start\":22191},{\"end\":22220,\"start\":22204},{\"end\":22381,\"start\":22366},{\"end\":22395,\"start\":22381},{\"end\":22414,\"start\":22395},{\"end\":22431,\"start\":22414},{\"end\":22446,\"start\":22431},{\"end\":22456,\"start\":22446},{\"end\":22474,\"start\":22456},{\"end\":22666,\"start\":22653},{\"end\":22681,\"start\":22666},{\"end\":22693,\"start\":22681},{\"end\":22710,\"start\":22693},{\"end\":22724,\"start\":22710},{\"end\":22743,\"start\":22724},{\"end\":22763,\"start\":22743},{\"end\":22997,\"start\":22982},{\"end\":23011,\"start\":22997},{\"end\":23027,\"start\":23011},{\"end\":23046,\"start\":23027},{\"end\":23061,\"start\":23046},{\"end\":23251,\"start\":23232},{\"end\":23270,\"start\":23251},{\"end\":23283,\"start\":23270},{\"end\":23296,\"start\":23283},{\"end\":23315,\"start\":23296},{\"end\":23329,\"start\":23315},{\"end\":23344,\"start\":23329},{\"end\":23358,\"start\":23344},{\"end\":23369,\"start\":23358},{\"end\":23384,\"start\":23369},{\"end\":23678,\"start\":23668},{\"end\":23687,\"start\":23678},{\"end\":23703,\"start\":23687},{\"end\":23718,\"start\":23703},{\"end\":23733,\"start\":23718},{\"end\":23964,\"start\":23954},{\"end\":23979,\"start\":23964},{\"end\":23988,\"start\":23979},{\"end\":23997,\"start\":23988},{\"end\":24012,\"start\":23997},{\"end\":24030,\"start\":24012},{\"end\":24038,\"start\":24030},{\"end\":24294,\"start\":24280},{\"end\":24309,\"start\":24294},{\"end\":24324,\"start\":24309},{\"end\":24632,\"start\":24611},{\"end\":24646,\"start\":24632},{\"end\":24660,\"start\":24646},{\"end\":24964,\"start\":24947},{\"end\":24982,\"start\":24964},{\"end\":24997,\"start\":24982},{\"end\":25012,\"start\":24997},{\"end\":25024,\"start\":25012},{\"end\":25038,\"start\":25024},{\"end\":25355,\"start\":25349},{\"end\":25366,\"start\":25355},{\"end\":25388,\"start\":25366},{\"end\":25408,\"start\":25388},{\"end\":25746,\"start\":25732},{\"end\":25762,\"start\":25746},{\"end\":25784,\"start\":25762},{\"end\":25800,\"start\":25784},{\"end\":25957,\"start\":25945},{\"end\":25971,\"start\":25957},{\"end\":25981,\"start\":25971},{\"end\":25990,\"start\":25981},{\"end\":26245,\"start\":26233},{\"end\":26259,\"start\":26245},{\"end\":26275,\"start\":26259},{\"end\":26293,\"start\":26275},{\"end\":26302,\"start\":26293},{\"end\":26316,\"start\":26302},{\"end\":26335,\"start\":26316},{\"end\":26353,\"start\":26335},{\"end\":26365,\"start\":26353},{\"end\":26655,\"start\":26644},{\"end\":26677,\"start\":26655},{\"end\":26694,\"start\":26677},{\"end\":26717,\"start\":26694},{\"end\":26731,\"start\":26717},{\"end\":26979,\"start\":26967},{\"end\":26993,\"start\":26979},{\"end\":27004,\"start\":26993},{\"end\":27493,\"start\":27475},{\"end\":27509,\"start\":27493},{\"end\":27520,\"start\":27509},{\"end\":27530,\"start\":27520},{\"end\":27546,\"start\":27530},{\"end\":27561,\"start\":27546},{\"end\":27577,\"start\":27561},{\"end\":27583,\"start\":27577},{\"end\":27888,\"start\":27872},{\"end\":27919,\"start\":27888},{\"end\":27932,\"start\":27919},{\"end\":27942,\"start\":27932},{\"end\":27953,\"start\":27942},{\"end\":27965,\"start\":27953},{\"end\":27974,\"start\":27965},{\"end\":27988,\"start\":27974},{\"end\":28017,\"start\":27988},{\"end\":28024,\"start\":28017},{\"end\":28588,\"start\":28575},{\"end\":28599,\"start\":28588},{\"end\":28616,\"start\":28599},{\"end\":28628,\"start\":28616},{\"end\":28644,\"start\":28628},{\"end\":28660,\"start\":28644},{\"end\":28676,\"start\":28660},{\"end\":29214,\"start\":29198},{\"end\":29229,\"start\":29214},{\"end\":29477,\"start\":29461},{\"end\":29489,\"start\":29477},{\"end\":29505,\"start\":29489},{\"end\":29757,\"start\":29739},{\"end\":29925,\"start\":29910},{\"end\":29943,\"start\":29925},{\"end\":29962,\"start\":29943},{\"end\":29981,\"start\":29962},{\"end\":29996,\"start\":29981},{\"end\":30007,\"start\":29996},{\"end\":30021,\"start\":30007},{\"end\":30030,\"start\":30021},{\"end\":30039,\"start\":30030},{\"end\":30048,\"start\":30039},{\"end\":30472,\"start\":30459},{\"end\":30490,\"start\":30472},{\"end\":30504,\"start\":30490}]", "bib_venue": "[{\"end\":27146,\"start\":27082},{\"end\":21020,\"start\":20971},{\"end\":21257,\"start\":21200},{\"end\":21490,\"start\":21432},{\"end\":21866,\"start\":21832},{\"end\":22154,\"start\":22101},{\"end\":22364,\"start\":22354},{\"end\":22651,\"start\":22614},{\"end\":22980,\"start\":22934},{\"end\":23230,\"start\":23213},{\"end\":23666,\"start\":23603},{\"end\":23952,\"start\":23886},{\"end\":24278,\"start\":24213},{\"end\":24609,\"start\":24513},{\"end\":24945,\"start\":24900},{\"end\":25415,\"start\":25408},{\"end\":25730,\"start\":25677},{\"end\":26047,\"start\":25990},{\"end\":26231,\"start\":26173},{\"end\":26642,\"start\":26581},{\"end\":27080,\"start\":27004},{\"end\":27473,\"start\":27384},{\"end\":28073,\"start\":28024},{\"end\":28725,\"start\":28676},{\"end\":29196,\"start\":29116},{\"end\":29536,\"start\":29505},{\"end\":29737,\"start\":29661},{\"end\":30097,\"start\":30048},{\"end\":30457,\"start\":30421}]"}}}, "year": 2023, "month": 12, "day": 17}