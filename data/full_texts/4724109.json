{"id": 4724109, "updated": "2023-07-19 11:36:07.886", "metadata": {"title": "Neural Sign Language Translation", "authors": "[{\"first\":\"Necati\",\"last\":\"Camgoz\",\"middle\":[\"Cihan\"]},{\"first\":\"Simon\",\"last\":\"Hadfield\",\"middle\":[]},{\"first\":\"Oscar\",\"last\":\"Koller\",\"middle\":[]},{\"first\":\"Hermann\",\"last\":\"Ney\",\"middle\":[]},{\"first\":\"Richard\",\"last\":\"Bowden\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Sign Language Recognition (SLR) has been an active research field for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar. We formalize SLT in the framework of Neural Machine Translation (NMT) for both end-to-end and pretrained settings (using expert knowledge). This allows us to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language. To evaluate the performance of Neural SLT, we collected the first publicly available Continuous SLT dataset, RWTH-PHOENIX-Weather 2014T1. It provides spoken language translations and gloss level annotations for German Sign Language videos of weather broadcasts. Our dataset contains over .95M frames with >67K signs from a sign vocabulary of >1K and >99K words from a German vocabulary of >2.8K. We report quantitative and qualitative results for various SLT setups to underpin future research in this newly established field. The upper bound for translation performance is calculated at 19.26 BLEU-4, while our end-to-end frame-level and gloss-level tokenization networks were able to achieve 9.58 and 18.13 respectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2799020610", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/CamgozHKNB18", "doi": "10.1109/cvpr.2018.00812"}}, "content": {"source": {"pdf_hash": "006a8ed8bdff4bec625b5f1aa6ef4c16f80ad9d5", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://zenodo.org/record/3234105/files/Camgoz_CVPR2018pp.pdf", "status": "GREEN"}}, "grobid": {"id": "340e0d4fa475feeb87148d7148e612a34e33d335", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/006a8ed8bdff4bec625b5f1aa6ef4c16f80ad9d5.txt", "contents": "\nNeural Sign Language Translation\n\n\nNecati Cihan Camgoz \nUniversity of Surrey\n\n\nSimon Hadfield s.hadfield@surrey.ac.uk \nUniversity of Surrey\n\n\nOscar Koller koller@cs.rwth-aachen.de \nRWTH Aachen University\n\n\nHermann Ney ney@cs.rwth-aachen.de \nRWTH Aachen University\n\n\nRichard Bowden r.bowden@surrey.ac.uk \nUniversity of Surrey\n\n\nNeural Sign Language Translation\n\nSign Language Recognition (SLR) has been an active research field for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar.We formalize SLT in the framework of Neural Machine Translation (NMT) for both end-to-end and pretrained settings (using expert knowledge). This allows us to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language.To evaluate the performance of Neural SLT, we collected the first publicly available Continuous SLT dataset, RWTH-PHOENIX-Weather 2014T 1 . It provides spoken language translations and gloss level annotations for German Sign Language videos of weather broadcasts. Our dataset contains over .95M frames with >67K signs from a sign vocabulary of >1K and >99K words from a German vocabulary of >2.8K. We report quantitative and qualitative results for various SLT setups to underpin future research in this newly established field. The upper bound for translation performance is calculated at 19.26 BLEU-4, while our end-to-end frame-level and gloss-level tokenization networks were able to achieve 9.58 and 18.13 respectively.\n\nIntroduction\n\nSign Languages are the primary language of the deaf community. Despite common misconceptions, sign languages have their own specific linguistic rules [55] and do not translate the spoken languages word by word. Therefore, the numerous advances in SLR [15] and even the move to the challenging Continuous SLR (CSLR) [33,36] problem, do not allow us to provide meaningful interpretations of what a signer is saying. This translation task is illustrated in Figure 1, where the sign language glosses give the meaning and the order of signs in the video, but the spoken language equivalent (which is what is actually desired) has both a different length and ordering.\n\nMost of the research that has been conducted in SLR to date has approached the task as a basic gesture recognition problem, ignoring the linguistic properties of the sign language and assuming that there is a one-to-one mapping of sign to spoken words. Contrary to SLR, we propose to approach the full translation problem as a NMT task. We use state-of-the-art sequence-to-sequence (seq2seq) based deep learning methods to learn: the spatio-temporal representation of the signs, the relation between these signs (in other words the language model) and how these signs map to the spoken or written language. To achieve this we introduce new vision methods, which mirror the tokenization and embedding steps of standard NMT. We also present the first continuous SLT dataset, RWTH-PHOENIX-Weather 2014T, to allow future research to be conducted towards sign to spoken language translation. The contributions of this paper can be summarized as:\n\n\u2022 The first exploration of the video to text SLT problem.\n\n\u2022 The first publicly available continuous SLT dataset, PHOENIX14T, which contains video segments, gloss annotations and spoken language translations. \u2022 A broad range of baseline results on the new corpus including a range of different tokenization and attention schemes in addition to parameter recommendations. The rest of this paper is organized as follows: In Section 2 we survey the fields of sign language recognition, seq2seq learning and neural machine translation. In Section 3 we formalize the SLT task in the framework of neural machine translation and describe our pipeline. We then intro-duce RWTH-PHOENIX-Weather 2014T, the first continuous SLT dataset, in Section 4. We share our quantitative and qualitative experimental results in Sections 5 and 6, respectively. Finally, we conclude our paper in Section 7 by discussing our findings and the future of the field.\n\n\nRelated Work\n\nThere are various factors that have hindered progress towards SLT. Although there have been studies such as [9], which recognized isolated signs to construct sentences, to the best of our knowledge no dataset or study exists that achieved SLT directly from videos, until now. In addition, existing linguistic work on SLT has solely dealt with text to text translation. Despite only including textual information, these have been very limited in size (averaging 3000 total words) [46,54,52]. The first important factor is that collection and annotation of continuous sign language data is a laborious task. Although there are datasets available from linguistic sources [51,28] and sign language interpretations from broadcasts [14], they are weakly annotated and lack the human pose information which legacy sign language recognition methods heavily relied on. This has resulted in many researchers collecting isolated sign language datasets [63,7] in controlled environments with limited vocabulary, thus inhibiting the end goal of SLT. The lack of a baseline dataset for SLR has rendered most research incomparable, robbing the field of competitive progress.\n\nWith the development of algorithms that were capable of learning from weakly annotated data [5,50,14] and the improvements in the field of human pose estimation [10,59,8], working on linguistic data and sign language interpretations from broadcasts became a feasible option. Following these developments, Forster et al. released RWTH-PHOENIX-Weather 2012 [20] and its extended version RWTH-PHOENIX-Weather 2014 [21], which was captured from sign language interpretations of weather forecasts. The PHOENIX datasets were created for CSLR and they provide sequence level gloss annotations. These datasets quickly became a baseline for CSLR.\n\nConcurrently, Deep Learning (DL) [39] has gained popularity and achieved state-of-the-art performance in various fields such as Computer Vision [38], Speech Recognition [2] and more recently in the field of Machine Translation [47]. Until recently SLR methods have mainly used handcrafted intermediate representations [33,16] and the temporal changes in these features have been modelled using classical graph based approaches, such as Hidden Markov Models (HMMs) [58], Conditional Random Fields [62] or template based methods [5,48]. However, with the emergence of DL, SLR researchers have quickly adopted Convolutional Neural Networks (CNNs) [40] for manual [35,37] and non-manual [34] feature representation, and Recurrent Neural Networks (RNNs) for temporal modelling [6,36,17].\n\nOne of the most important breakthroughs in DL was the development of seq2seq learning approaches. Strong annotations are hard to obtain for seq2seq tasks, in which the objective is to learn a mapping between two sequences. To be able to train from weakly annotated data in an end-toend manner, Graves et al. proposed Connectionist Temporal Classification (CTC) Loss [25], which considers all possible alignments between two sequences while calculating the error. CTC quickly became a popular loss layer for many seq2seq applications. It has obtained state-of-the-art performance on several tasks in speech recognition [27,2] and clearly dominates hand writing recognition [26]. Computer vision researchers adopted CTC and applied it to weakly labeled visual problems, such as lip reading [3], action recognition [30], hand shape recognition [6] and CSLR [6,17].\n\nAnother common seq2seq task is machine translation, which aims to develop methods that can learn the mapping between two languages. Although CTC is popular, it is not suitable for machine translation as it assumes source and target sequences share the same order. Furthermore, CTC assumes conditional independence within target sequences, which doesn't allow networks to learn an implicit language model. This led to the development of Encoder-Decoder Network architectures [31] and the emergence of the NMT field [47]. The main idea behind Encoder-Decoder Networks is to use an intermediary latent space to map two sequences, much like the latent space in auto-encoders [24], but applied to temporal sequences. This is done by first encoding source sequences to a fixed sized vector and then decoding target sequences from this. The first architecture proposed by Kalchbrenner and Blunsom [31] used a single RNN for both encoding and decoding tasks. Later Sutskever et al. [56] and Cho et al. [11] proposed delegating encoding and decoding to two separate RNNs.\n\nAlthough encoder-decoder networks improved machine translation performance, there is still the issue of an information bottleneck caused by encoding the source sequence into a fixed sized vector and the long term dependencies between source and target sequence. To address these issues, Bahdanau et al. [4] proposed passing additional information to the decoder using an attention mechanism. Given encoder outputs, their attention function calculates the alignment between source and target sequences. Luong et al. [44] further improved this approach by introducing additional types of attention score calculation and the inputfeeding approach. Since then, various attention based architectures have been proposed for NMT, such as GNMT [60] that combines bi-directional and uni-directional encoders in a deep architecture and [22] which introduced a convolution based seq2seq learning approach. Similar attention based approaches have been applied to various Computer Vision tasks, such as image captioning [61], lip reading [13] and action recognition [19]. \n\n\nNeural Sign Language Translation\n\nTranslating sign videos to spoken language is a seq2seq learning problem by nature. Our objective is to learn the conditional probability p(y|x) of generating a spoken language sentence y = (y 1 , y 2 , ..., y U ) with U number of words given a sign video x = (x 1 , x 2 , ..., x T ) with T number of frames. This is not a straight forward task as the number of frames in a sign video is much higher than the number of words in its spoken language translation (i.e. T U ). Furthermore, the alignment between sign and spoken language sequences are usually unknown and nonmonotonic. In addition, unlike other translation tasks that work on text, our source sequences are videos. This renders the use of classic sequence modeling architectures such as the RNN difficult. Instead, we propose combining CNNs with attention-based encoder-decoders to model the conditional probability p(y|x). We experiment with training our approach in an end-to-end manner to jointly learn the alignment and the translation of sign language videos to spoken language sentences. An overview of our approach can be seen in Figure 2. In the remainder of this section, we will describe each component of our architecture in detail.\n\n\nSpatial and Word Embeddings:\n\nNeural machine translation methods start with tokenization of source and target sequences and projecting them to a continuous space by using word embeddings [45]. The main idea behind using word embeddings is to transform the sparse one-hot vector representations, where each word is equidistant from each other, into a denser form, where words with similar meanings are closer. These embeddings are either learned from scratch or pretrained on larger datasets and fine-tuned during training. However, contrary to text, signs are visual. Therefore, in addition to using word embeddings for our target sequences (spoken language sen-tences), we need to learn spatial embeddings to represent sign videos. To achieve this we utilize 2D CNNs. Given a sign video x, our CNN learns to extract non-linear frame level spatial representations as:\nf t = SpatialEmbedding(x t )(1)\nwhere f t corresponds to the feature vector produced by propagating a video frame x t through our CNN.\n\nFor word embedding, we use a fully connected layer that learns a linear projection from one-hot vectors of spoken language words to a denser space as:\ng u = WordEmbedding(y u )(2)\nwhere g u is the embedded version of the spoken word y u .\n\n\nTokenization Layer:\n\nIn NMT the input and output sequences can be tokenized at many different levels of complexity: characters, words, N-grams or phrases. Low level tokenization schemes, such as the character level, allow smaller vocabularies to be used, but greatly increase the complexity of the sequence modeling problem, and require long term relationships to be maintained. High level tokenization makes the recognition problem far more difficult due to vastly increased vocabularies, but the language modeling generally only needs to consider a small number of neighboring tokens.\n\nAs there has been no previous research on SLT, it is not clear what tokenization schemes are most appropriate for this problem. This is exacerbated by the fact that, unlike NMT research, there is no simple equivalence between the tokenizations of the input sign video and the output text. The framework developed in this paper is generic and can use various tokenization schemes on the spatial embeddings sequence f 1:T\nz 1:N = Tokenization(f 1:T )(3)\nIn the experiments we explore both \"frame level\" and \"gloss level\" input tokenization, with the latter exploiting an RNN-HMM forced alignment approach [36]. The output tokenization is at the word level (as in most modern NMT research) but could be an interesting avenue for the future.\n\n\nAttention-based Encoder-Decoder Networks:\n\nTo be able to generate the target sentence y from tokenized embeddings z 1:N of a sign video x, we need to learn a mapping function B(z 1:N ) \u2192 y which will maximize the probability p(y|x). We propose modelling B using an attention-based encoder-decoder network, which is composed of two specialized deep RNNs. By using these RNNs we break down the task into two phases. In the encoding phase, a sign videos' features are projected into a latent space in the form of a fixed size vector, later to be used in the decoding phase for generating spoken sentences.\n\nDuring the encoding phase, the encoder network, reads in the feature vectors one by one. Given a sequence of representations z 1:N , we first reverse its order in the temporal domain, as suggested by [56], to shorten the long term dependencies between the beginning of sign videos and spoken language sentences. We then feed the reversed sequence z N :1 to the Encoder which models the temporal changes in video frames and compresses their cumulative representation in its hidden states as:\no n = Encoder(z n , o n+1 )(4)\nwhere o n is the hidden state produced by recurrent unit n, o N +1 is a zero vector and the final encoder output o 1 corresponds to the latent embedding of the sequence h sign which is passed to the decoder. The decoding phase starts by initializing hidden states of the decoder network using the latent vector h sign . In the classic encoder-decoder architecture [56], this latent representation is the only information source of the decoding phase. By taking its previous hidden state (h u\u22121 ) and the word embedding (g u\u22121 ) of the previously predicted word (y u\u22121 ) as inputs, the decoder learns to generate the next word in the sequence (y u ) and update its hidden state (h u ):\ny u , h u = Decoder(g u\u22121 , h u\u22121 )(5)\nwhere h 0 = h sign is the spatio-temporal representation of sign language video learned by the Encoder and y 0 is the special token < bos > indicating the beginning of a sentence. This procedure continues until another special token < eos >, which indicates the end of a sentence, is predicted. By generating sentences word by word, the Decoder decomposes the conditional probability p(y|x) into ordered conditional probabilities:\np(y|x) = U u=1 p(y u |y 1:u\u22121 , h sign )(6)\nwhich is used to calculate the errors by applying cross entropy loss for each word. For the end-to-end experiments, these errors are back propagated through the encoderdecoder network to the CNN and word embeddings, thus updating all of the network parameters.\n\n\nAttention Mechanisms:\n\nA major drawback of using a classic encoder-decoder architecture is the information bottleneck caused by representing a whole sign language video with a fixed sized vector. Furthermore, due to large number of frames, our networks suffer from long term dependencies and vanishing gradients.\n\nTo overcome these issues, we utilize attention mechanisms to provide additional information to the decoding phase. By using attention mechanisms our networks are able to learn where to focus while generating each word, thus providing the alignment of sign videos and spoken language sentences. We employ the most prominent attention approach proposed by Bahdanau et al. [4] and later improved by Luong et al. [44].\n\nThe main idea behind attention mechanisms is to create a weighted summary of the source sequence to aid the decoding phase. This summary is commonly known as the context vector and it will be notated as c u in this paper. For each decoding step u, a new context vector c u is calculated by taking a weighted sum of encoder outputs o 1:N as:\nc u = N n=1 \u03b3 u n o n(7)\nwhere \u03b3 u n represent the attention weights, which can be interpreted as the relevance of an encoder input z n to generating the word y u . When visualized, attention weights also help to display the alignments between sign videos and spoken language sentences learned by the encoder-decoder network. These weights are calculated by comparing the decoder hidden state h u against each output o t as:\n\u03b3 u n = exp(score(h u , o n )) N n =1 exp(score(h u , o n )(8)\nwhere the scoring function depends on the attention mechanism that is being used. In this work we examine two scoring functions. The first one is a multiplication based approach proposed by Luong et al. [44] and the second is a concatenation based function proposed by Bahdanau et al. [4]. These functions are as follows:\nscore(h u , o n ) = h u W o n [Multiplication] V tanh(W [h u ; o n ]) [Concatenation](9)\nwhere W and V are learned parameters. The context vector c u is then combined with the hidden state h u to calculate the attention vector a u as:\na u = tanh(W c [c u ; h u ])(10)\nFinally, we feed the a u to a fully connected layer to model the ordered conditional probability in Equation 6. Furthermore a u is fed to the next decoding step u+1 thus changing Equation 5 to:\ny u , h u = Decoder(g u\u22121 , h u\u22121 , a u\u22121 )(11)\n\nSign Language Translation Dataset\n\nAs discussed in Section 2, there are no suitable datasets available to support research towards SLT. Due to the cost of annotation, existing linguistic datasets are too small to support deep learning.\n\nIn this work we present \"RWTH-PHOENIX-Weather 2014T\", a large vocabulary, continuous SLT corpus. PHOENIX14T is an extension of the PHOENIX14 corpus, which has become the primary benchmark for SLR in recent years. PHOENIX14T constitutes a parallel corpus including sign language videos, sign-gloss annotations and also German translations (spoken by the news anchor), which are all segmented into parallel sentences. Due to different sentence segmentation between spoken language and sign language, it was not sufficient to simply add a spoken language tier to PHOENIX14. Instead, the segmentation boundaries also had to be redefined. Wherever the addition of a translation layer necessitated new sentence boundaries, we used the forced alignment approach of [35] to compute the new boundaries.\n\nIn addition to changes in boundaries, RWTH-PHOENIX-Weather 2014T has a marginally decreased vocabulary due to some improvements in the normalization schemes. This means performance on PHOENIX14 and PHOENIX14T will be similar, but not exactly comparable. However, care has been taken to assure that the dev/test sets of PHOENIX14 do not overlap with the new PHOENIX14T training set and also that none of the new dev/test sets from PHOENIX14T overlap with the PHOENIX14 training set.\n\nThis corpus is publicly available to the research community for facilitating the future growth of SLT research. The detailed statistics of the dataset can be seen in Table 1. OOV stands for Out-Of-Vocabulary, e.g. words that occur in test, but not in training. Singletons occur only once in the training set. The corpus covers unconstrained sign language of 9 different signers with a vocabulary of 1066 different signs and translations into German spoken language with a vocabulary of 2887 different words. The corpus features professional sign language interpreters and has been annotated using sign glosses by deaf specialists. The spoken German translation originates from the news speaker. It has been automatically transcribed, manually verified and normalized. \n\n\nQuantitative Experiments\n\nUsing our new PHOENIX14T dataset, we conduct several sets of experiments to create a baseline for SLT. We categorize our experiments under three groups:\n\n1. Gloss2Text (G2T), in which we simulate having a perfect SLR system as an intermediate tokenization. 2. Sign2Text (S2T) which covers the end-to-end pipeline translating directly from frame level sign language video into spoken language. 3. Sign2Gloss2Text (S2G2T) which uses a SLR system as tokenization layer to add intermediate supervision.\n\nAll of our encoder-decoder networks were built using four stacked layers of residual recurrent units with separate parameters. Each recurrent layer contains 1000 hidden units. In our S2T experiments we use AlexNet without its final layer (fc8) as our Spatial Embedding Layer and initialize it using weights that were trained on ImageNet [18]. For our S2G2T experiments we use the CNN-RNN-HMM network proposed by Koller et al. [36] as our Tokenization Layer, which is the state-of-the-art CSLR. It achieves a gloss recognition performance of 25.7%/26.6% word error rate on the dev/test sets of the PHOENIX14T. All remaining parts of our networks are initialized using Xavier [23] initialization. We use Adam [32] optimization method with a learning rate of 10 \u22125 and its default parameters. We also use gradient clipping with a threshold of 5 and dropout connections with a drop probability of 0.2.\n\nAll of our networks are trained until the training perplexity is converged, which took \u223c30 epochs on average. We evaluate our models on dev/test sets every half-epoch, and report results for each setup using the model that performed the best on the dev set. In the decoding phase we generate spoken language sentences using beam search with a beam width of three, which we empirically shows to be the optimal beam size.\n\nTo measure our translation performance we utilize BLEU [49] and ROGUE [42] scores, which are commonly used metrics for machine translation. As ROUGE score we use ROUGE-L F1-Score, while as BLEU score we report BLEU-1,2,3,4 to give a better perspective of the translation performance on different phrase levels.\n\nWe implemented our networks using TensorFlow [1]. Our code, which is based on Luong et al.'s NMT library [43], is made publicly available 2 .\n\n\nG2T: Simulating Perfect Recognition\n\nOur SLT framework supports various input tokenizations. In our first set of experiments we simulate using an idealized SLR system as an intermediate tokenizer. NMT networks are trained to generate spoken language translations from ground truth sign glosses. We refer to this as G2T.  There are two main objectives of the G2T experiments. First to create an upper bound for end-to-end SLT. Second to examine different encoder-decoder network architectures and hyper-parameters, and evaluate their effects on sign to spoken language translation performance. As training S2T networks is an order of magnitude slower than G2T, we use the best setup from our G2T experiments when training our S2T networks.\n\nNote that we should expect the translation performance's upper bound to be significantly lower than 100%. As in all natural language problems, there are many ways to say the same thing, and thus many equally valid translations. Unfortunately, this is impossible to quantify using any existing evaluation measure.\n\n\nRecurrent Units: GRUs vs LSTMs\n\nVarious types of recurrent units have been used for neural machine translation. The first encoder-decoder network proposed by Kalchbrenner and Blunsom [31] was build using a single RNN with vanilla recurrent units. Later approaches employed shallow [56,44] and deep architectures [60] of Long Short-Term Memory (LSTM) units [29] and Gated Recurrent Units (GRUs) [12]. To choose which recurrent unit to use, our first experiment trained two G2T networks using LSTMs and GRUs. Both networks were trained using a batch size of 128 and Luong attention mechanism as described in Section 3.\n\nAs it can be seen in Table 2, GRUs outperformed LSTM units in both BLEU and ROUGE scores. This may be due to over-fitting caused by the additional parameters in LSTM units and the limited number of training sequences. Compared to LSTMs, GRUs have fewer parameters (two vs. three gates) which makes them faster to train and less prone to over-fitting. We therefore use Gated Recurrent Units for the rest of our experiments.\n\n\nAttention Mechanisms: Luong vs. Bahdanau\n\nNext we evaluated the effects of different attention mechanisms for the G2T translation task. We used Luong and Bahdanau attention which were described in detail in Section 3. We also trained a network which did not use any attention mechanisms. All of our networks were trained using Gated Recurrent Units and a batch size of 128.\n\nOur first observation from this experiment was that having an attention mechanism improved the translation performance drastically as shown in Table 3. When attention mechanisms are compared, Luong attention outperformed Bahdanau attention and generalized better to the test set. We believe this is due to Luong attention's use of the decoder network's hidden state at time u while generating the target word u . We train our remaining G2T networks using Luong attention.\n\n\nWhat Batch Size to use?\n\nThere have been several studies on the effects of batch sizes while using Stochastic Gradient Descent (SGD) [41]. Although larger batch sizes have the advantage of providing smoother gradients, they decrease the rate of convergence. Furthermore, recent studies on the information theory behind deep learning suggests the noise provided by smaller batch size helps the networks to represent the data more efficiently [57,53]. In addition, training and evaluation set distributions of seq2seq datasets are distinct by nature. When early stopping is employed during training, having additional noise provided by smaller batch sizes gives the optimization the opportunity to step closer to the target distribution. This suggests there is an optimal batch size given a network setup. Therefore, in our third set of experiments we evaluate the effects of the batch size on the translation. We train five G2T networks using different batch sizes that are 128, 64, 32, 16 and 1. All of our networks were trained using GRUs and Luong attention.\n\nOne interesting observation from this experiment was that, the networks trained using smaller batch sizes converged faster but to a higher training perplexity than one. We believe this is due to high variance between gradients. To deal with this we decrease the learning rate to 10 \u22126 when the training perplexity plateau, and continue training for 100,000 iterations. Results show that having a smaller batch size helps the translation performance. As reported in Table 4, the G2T network with batch size one outperformed networks that were trained using larger batch sizes. Considering these results, the remainder of our experiments use a batch size of one.\n\n\nEffects of Beam Width\n\nThe most straight forward decoding approach for Encoder-Decoder networks is to use a greedy search, in which the word with highest probability is considered the prediction and fed to the next time step of the decoder. However, this greedy approach is prone to errors, given that the predictions can have low confidence. To address this, we use a simple left-to-right Beam Search during the decoding phase, in which a number of candidate sequences, also known as beam width, are stored and propagated through the decoder. However, larger beam width does not necessarily mean better translation performance and increases decoding duration and memory requirements. Therefore, to find the optimal value, we use our best performing G2T network to do a parameter search over possible beam widths and report development and test set translation performances in the form of a BLEU-4 score.  As shown in Figure 3, a beam width of two or three was optimal for our G2T network. Although beam width two yielded the highest translation performance on the development set, beam width three generalized better to the test set. In addition, as beam width increased, the BLEU-4 scores plateau and then start to decline. Taking these results into consideration, we continue using beam width three for the rest of our experiments.\n\n\nS2T: From Sign Video To Spoken Text\n\nIn our second set of experiment we evaluate our S2T networks which learns to generate spoken language from sign videos without any intermediate representation in an endto-end manner. In this setup our tokenization layer is an Identity function, feeding the spatial embeddings directly to the encoder-decoder network. Using the hyper-parameters from our G2T experiments, we train three S2T networks with different attention choices.\n\nAs with the G2T task, utilizing attention mechanisms increases the translation performance of our S2T networks (See Table 5). However, when compared against G2T, the translation performance is lower. We believe this might due to several reasons. As the number of frames in a sign video is much higher than the number of its gloss level representations, our S2T networks suffer from long term dependencies and vanishing gradients. In addition, the dataset we are using might be too small to allow our S2T network to generalize considering the number of parameters (CNN+EncoderDecoder+Attention). Furthermore, expecting our networks to recognize visual sign languages and translate them to spoken languages with single supervision might be too much to ask from them. Therefore in our next set of experiments, which we call S2G2T, we introduce the gloss level supervision to aid the task of full translation from sign language videos.\n\n\nS2G2T: Gloss Level Supervision\n\nIn our final experiment we propose using glosses as an intermediate representation while going from sign videos to spoken language translations. To achieve this, we use the CNN-RNN-HMM hybrid proposed in [36] as our spatial embedding and tokenization layers. We evaluate two setups. In the first setup: Sign2Gloss\u2192Gloss2Text (S2G\u2192G2T), we use our best performing G2T network without any retraining to generate sentences from the estimated gloss token embeddings. In the second setup: S2G2T, we train a network from scratch to learn to translate from the predicted gloss.\n\nThe S2G\u2192G2T network performs surprisingly well considering there was no additional training. This shows us that our G2T network has already learned some robustness to noisy inputs, despite being trained on perfect glosses, this may be due to the dropout regularization employed during training. Our second approach S2G2T surpasses these results and obtains scores close to the idealized performance of the G2T network. This is likely because the translation system is able to correct the failure modes in the tokenizer. As can be seen in Table 6, compared to the S2T network S2G2T was able to surpass its performance by a large margin, indicating the importance of intermediary expert gloss level supervision to simplify the training process.\n\n\nQualitative Experiments\n\nIn this section we share our qualitative results. One of the most obvious ways of qualifying translation is to examine the resultant translations. To give a better understanding to the reader, in Table 7 we share translation samples generated from our G2T, S2T and S2G2T networks accompanied by the ground truth German and word to word English translations. GT: und nun die wettervorhersage f\u00fcr morgen samstag den zweiten april .\n\n( and now the weatherforecast for tomorrow saturday the second april . ) G2T: und nun die wettervorhersage f\u00fcr morgen samstag den elften april .\n\n( and now the weatherforecast for tomorrow saturday the eleventh april . ) S2T: und nun die wettervorhersage f\u00fcr morgen freitag den sechsundzwanzigsten m\u00e4rz .\n\n( and now the weatherforecast for tomorrow friday the twentysixth march . ) S2G2T: und nun die wettervorhersage f\u00fcr morgen samstag den siebzehnten april .\n\n( and now the weatherforecast for tomorrow saturday the seventeenth april . ) GT: die neue woche beginnt wechselhaft und k\u00fchler .\n\n( the new week starts unpredictable and cooler . ) G2T: die neue woche beginnt wechselhaft und wieder k\u00fchler .\n\n( the new week starts unpredictable and again cooler . ) S2T: am montag\u00fcberall wechselhaft und k\u00fchler .\n\n( on monday everywhere unpredictable and cooler . ) S2G2T: die neue woche beginnt wechselhaft und wechselhaft .\n\n( the new week starts unpredictable and unpredictable . ) GT: im s\u00fcden und s\u00fcdwesten gebietsweise regen sonst recht freundlich .\n\n( in the south and southwest locally rain otherwise quite friendly . ) G2T: in der s\u00fcdwesth\u00e4lfte regnet es zeitweise sonst ist es recht freundlich .\n\n( in the southwestpart it rains temporarely otherwise it is quite friendly . ) S2T: von der s\u00fcdh\u00e4lfte beginnt es vielerorts .\n\n( from the southpart it starts in many places . ) S2G2T: am freundlichsten wird es im s\u00fcden .\n\n( the friendliest it will be in the south . ) GT: am sonntag breiten sich teilweise kr\u00e4ftige schauer und gewitter .\n\n( on sunday spreads partly heavy shower and thunderstorm . ) G2T: am sonntag teilweise kr\u00e4ftige schauer und gewitter .\n\n( on sunday partly heavy sower and thunderstorm . ) S2T: am sonntag sonne und wolken und gewitter .\n\n( on sunday sun and clouds and thunderstorm . ) S2G2T: am sonntag ab und an regenschauer teilweise auch gewitter .\n\n( on sunday time to time rainshower partly also thunderstorm . )\n\nWe can see that the most common error mode is the mistranslation of dates, places and numbers. Although this does not effect the overall structure of the translated sentence, it tells us the embedding learned for these infrequent words could use some improvement.\n\nIn Figure 4 example attention maps can be seen for both the S2T and S2G2T systems. These maps show how dependent each output token (the horizontal axis) is on each input token (the vertical axis). The S2T network's focus is concentrated primarily at the start of the video, but attention does jump to the end during the final words of the transla- tion. In contrast the S2G2T attention figure shows a much cleaner dependency of inputs to outputs. This is partly due to the intermediate tokenization removing the asynchronicity between different sign channels. It should be noted that we still observe many-to-one mappings in both cases, due to the fact that many of the spoken words are not explicitly signed but have to be interpreted via context.\n\n\nConclusion\n\nIn this paper, we introduced the challenging task of Sign Language Translation and proposed the first end-to-end solution. In contrast to previous research, we took a machine translation perspective; treating sign language as a fully independent language and proposing SLT rather than SLR as the true route to facilitate communication with the deaf. To achieve NMT from sign videos, we employed CNN based spatial embedding, various tokenization methods including state-of-the-art RNN-HMM hybrids [36] and attentionbased encoder-decoder networks, to jointly learn to align, recognize and translate sign videos to spoken text.\n\nTo evaluate our approach we collected the first continuous sign language translation dataset, PHOENIX14T, which is publicly available. We conducted extensive experiments, making a number of recommendations to underpin future research.\n\nAs future work, it would be interesting to extend the attention mechanisms to the spatial domain to align building blocks of signs, also known as subunits, with their spoken language translations. It may also be possible to use an approach similar to SubUNets [6] to inject specialist intermediate subunit knowledge, bridging the gap between S2T and S2G2T.\n\nFigure 1 .\n1Difference between CSLR and SLT.\n\nFigure 2 .\n2An overview of our SLT approach that generates spoken language translations of sign language videos.\n\nFigure 3 .\n3Effects of Beam Width on G2T performance.\n\nFigure 4 .\n4Attention maps from our S2T (left) & S2G2T (right) networks.\n\nTable 1 .\n1Key statistics of the new dataset.Sign Gloss \nGerman \nTrain \nDev Test Train Dev Test \nsegments 7,096 \n519 \n642 \u2190 \u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2212 same \nframes 827,354 55,775 64,627 \u2190 \u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2212 same \nvocab. 1,066 \n393 \n411 2,887 951 1,001 \ntot. words 67,781 3,745 4,257 99,081 6,820 7,816 \ntot. OOVs \n-\n19 \n22 \n-\n57 \n60 \nsingletons \n337 \n-\n-1,077 \n-\n-\n\n\n\nTable 2 .\n2G2T: Effects of using different recurrent units on translation performance.DEV SET \nTEST SET \nUnit Type: ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 \nLSTM \n41.69 \n41.54 \n27.90 \n20.66 \n16.40 \n41.92 \n41.22 \n28.03 \n20.77 \n16.58 \nGRU \n43.85 \n43.71 \n30.49 \n23.15 \n18.78 \n43.73 \n43.43 \n30.73 \n23.36 \n18.75 \n\n\n\nTable 3 .\n3G2T: Attention Mechanism Experiments.DEV SET \nTEST SET \nAttention: ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 \nNone \n40.32 \n40.45 \n27.19 \n20.28 \n16.29 \n40.71 \n40.66 \n27.48 \n20.40 \n16.34 \nBahdanau \n42.93 \n42.93 \n29.71 \n22.43 \n17.99 \n42.61 \n42.76 \n29.55 \n22.00 \n17.40 \nLuong \n43.85 \n43.71 \n30.49 \n23.15 \n18.78 \n43.73 \n43.43 \n30.73 \n23.36 \n18.75 \n\nTable 4. G2T: Batch Size Experiments. \n\nDEV SET \nTEST SET \nBS: ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 \n128 \n43.85 \n43.71 \n30.49 \n23.15 \n18.78 \n43.73 \n43.43 \n30.73 \n23.36 \n18.75 \n64 \n43.78 \n43.52 \n30.56 \n23.36 \n18.95 \n44.36 \n44.33 \n31.34 \n23.74 \n19.06 \n32 \n44.63 \n44.67 \n31.44 \n24.08 \n19.58 \n44.52 \n44.51 \n31.29 \n23.76 \n19.14 \n16 \n44.87 \n44.10 \n31.16 \n23.89 \n19.52 \n44.37 \n43.96 \n31.11 \n23.66 \n19.01 \n1 \n46.02 \n44.40 \n31.83 \n24.61 \n20.16 \n45.45 \n44.13 \n31.47 \n23.89 \n19.26 \n\n\n\nTable 5 .\n5S2T: Attention Mechanism Experiments.DEV SET \nTEST SET \nAttention: ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 \nNone \n31.00 \n28.10 \n16.81 \n11.82 \n9.12 \n29.70 \n27.10 \n15.61 \n10.82 \n8.35 \nBahdanau \n31.80 \n31.87 \n19.11 \n13.16 \n9.94 \n31.80 \n32.24 \n19.03 \n12.83 \n9.58 \nLuong \n32.6 \n31.58 \n18.98 \n13.22 \n10.00 \n30.70 \n29.86 \n17.52 \n11.96 \n9.00 \n\nTable 6. Effects of different tokenization schemes for sign to text translation. \n\nDEV SET \nTEST SET \nApproach: ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 \nG2T \n46.02 \n44.40 \n31.83 \n24.61 \n20.16 \n45.45 \n44.13 \n31.47 \n23.89 \n19.26 \nS2T \n31.80 \n31.87 \n19.11 \n13.16 \n9.94 \n31.80 \n32.24 \n19.03 \n12.83 \n9.58 \nS2G\u2192G2T \n43.76 \n41.08 \n29.10 \n22.16 \n17.86 \n43.45 \n41.54 \n29.52 \n22.24 \n17.79 \nS2G2T \n44.14 \n42.88 \n30.30 \n23.02 \n18.40 \n43.80 \n43.29 \n30.39 \n22.82 \n18.13 \n\n\n\nTable 7 .\n7Translations from our networks. (GT: Ground Truth)\nhttps://www-i6.informatik.rwth-aachen.de/ koller/RWTH-PHOENIX-2014-T/\nhttps://github.com/neccam/nslt\nAcknowledgementThis work was funded by the SNSF Sinergia project \"Scalable Multimodal Sign Language Technology for Sign Language Learning and Assessment (SMILE)\" grant agreement number CRSII2 160811 and the European Union's Horizon2020 research and innovation programme under grant agreement no. 762021 (Content4All). We would also like to thank NVIDIA Corporation for their GPU grant.\nM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, arXiv:1603.04467Large-scale Machine Learning on Heterogeneous Distributed Systems. M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. TensorFlow: Large-scale Machine Learning on Heteroge- neous Distributed Systems. arXiv:1603.04467, 2016.\n\nDeep Speech 2: End-to-end Speech Recognition in English and Mandarin. D Amodei, R Anubhai, E Battenberg, C Case, J Casper, B Catanzaro, J Chen, M Chrzanowski, A Coates, G Diamos, International Conference on Machine Learning (ICML). D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Di- amos, et al. Deep Speech 2: End-to-end Speech Recognition in English and Mandarin. In International Conference on Machine Learning (ICML), 2016.\n\nY M Assael, B Shillingford, S Whiteson, N De Freitas, arXiv:1611.01599LipNet: Sentence-level Lipreading. Y. M. Assael, B. Shillingford, S. Whiteson, and N. de Fre- itas. LipNet: Sentence-level Lipreading. arXiv:1611.01599, 2016.\n\nD Bahdanau, K Cho, Y Bengio, Neural Machine Translation by Jointly Learning to Align and Translate. International Conference on Learning Representations (ICLR). D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Trans- lation by Jointly Learning to Align and Translate. Inter- national Conference on Learning Representations (ICLR), 2015.\n\nLearning Sign Language by Watching TV (using weakly aligned subtitles). P Buehler, A Zisserman, M Everingham, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). P. Buehler, A. Zisserman, and M. Everingham. Learning Sign Language by Watching TV (using weakly aligned sub- titles). In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.\n\nSub-UNets: End-to-end Hand Shape and Continuous Sign Language Recognition. N C Camgoz, S Hadfield, O Koller, R Bowden, IEEE International Conference on Computer Vision (ICCV. N. C. Camgoz, S. Hadfield, O. Koller, and R. Bowden. Sub- UNets: End-to-end Hand Shape and Continuous Sign Lan- guage Recognition. In IEEE International Conference on Computer Vision (ICCV), 2017.\n\nBosphorusSign: A Turkish Sign Language Recognition Corpus in Health and Finance Domains. N C Camgoz, A A Kindiroglu, S Karabuklu, M Kelepir, A S Ozsoy, L Akarun, International Conference on Language Resources and Evaluation (LREC). N. C. Camgoz, A. A. Kindiroglu, S. Karabuklu, M. Kelepir, A. S. Ozsoy, and L. Akarun. BosphorusSign: A Turkish Sign Language Recognition Corpus in Health and Finance Do- mains. In International Conference on Language Resources and Evaluation (LREC), 2016.\n\nRealtime Multi-Person 2D Pose Estimation using Part Affinity Fields. Z Cao, T Simon, S.-E Wei, Y Sheikh, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime Multi- Person 2D Pose Estimation using Part Affinity Fields. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2017.\n\nSign Language Recognition and Translation with Kinect. X Chai, G Li, Y Lin, International Conference on Automatic Face and Gesture Recognition (FG). X. Chai, G. Li, Y. Lin, et al. Sign Language Recognition and Translation with Kinect. In International Conference on Automatic Face and Gesture Recognition (FG), 2013.\n\nAutomatic and Efficient Human Pose Estimation for Sign Language Videos. J Charles, T Pfister, M Everingham, A Zisserman, International Journal of Computer Vision (IJCV). 1101J. Charles, T. Pfister, M. Everingham, and A. Zisserman. Automatic and Efficient Human Pose Estimation for Sign Language Videos. International Journal of Computer Vision (IJCV), 110(1), 2014.\n\nOn the Properties of Neural Machine Translation: Encoder-Decoder Approaches. K Cho, B Van Merri\u00ebnboer, D Bahdanau, Y Bengio, Syntax, Semantics and Structure in Statistical Translation. K. Cho, B. van Merri\u00ebnboer, D. Bahdanau, and Y. Bengio. On the Properties of Neural Machine Translation: Encoder- Decoder Approaches. In Syntax, Semantics and Structure in Statistical Translation, 2014.\n\nJ Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.3555Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empiri- cal Evaluation of Gated Recurrent Neural Networks on Se- quence Modeling. arXiv:1412.3555, 2014.\n\nLip Reading Sentences in the Wild. J S Chung, A Senior, O Vinyals, A Zisserman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman. Lip Reading Sentences in the Wild. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2017.\n\nLearning Signs from Subtitles: A Weakly Supervised Approach to Sign Language Recognition. H Cooper, R Bowden, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). H. Cooper and R. Bowden. Learning Signs from Subtitles: A Weakly Supervised Approach to Sign Language Recogni- tion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.\n\nSign Language Recognition. H Cooper, B Holt, R Bowden, Visual Analysis of Humans. H. Cooper, B. Holt, and R. Bowden. Sign Language Recog- nition. In Visual Analysis of Humans. 2011.\n\nSign Language Recognition using Sub-units. H Cooper, E.-J Ong, N Pugeault, R Bowden, Journal of Machine Learning Research. 13JMLRH. Cooper, E.-J. Ong, N. Pugeault, and R. Bowden. Sign Language Recognition using Sub-units. Journal of Machine Learning Research (JMLR), 13, 2012.\n\nRecurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization. R Cui, H Liu, C Zhang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. R. Cui, H. Liu, and C. Zhang. Recurrent Convolutional Neu- ral Networks for Continuous Sign Language Recognition by Staged Optimization. In IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), 2017.\n\nImagenet: A Large-scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A Large-scale Hierarchical Image Database. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2009.\n\nLong-term Recurrent Convolutional Networks for Visual Recognition and Description. J Donahue, L Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar- rell. Long-term Recurrent Convolutional Networks for Visual Recognition and Description. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nRWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus. J Forster, C Schmidt, T Hoyoux, O Koller, U Zelle, J H Piater, H Ney, International Conference on Language Resources and Evaluation (LREC). J. Forster, C. Schmidt, T. Hoyoux, O. Koller, U. Zelle, J. H. Piater, and H. Ney. RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Cor- pus. In International Conference on Language Resources and Evaluation (LREC), 2012.\n\nExtensions of the Sign Language Recognition and Translation Corpus RWTH-PHOENIX-Weather. J Forster, C Schmidt, O Koller, M Bellgardt, H Ney, International Conference on Language Resources and Evaluation (LREC). J. Forster, C. Schmidt, O. Koller, M. Bellgardt, and H. Ney. Extensions of the Sign Language Recognition and Transla- tion Corpus RWTH-PHOENIX-Weather. In International Conference on Language Resources and Evaluation (LREC), 2014.\n\nConvolutional Sequence to Sequence Learning. J Gehring, M Auli, D Grangier, D Yarats, Y N Dauphin, ACM International Conference on Machine Learning (ICML). J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional Sequence to Sequence Learn- ing. ACM International Conference on Machine Learning (ICML), 2017.\n\nUnderstanding the Difficulty of Training Deep Feedforward Neural Networks. X Glorot, Y Bengio, International Conference on Artificial Intelligence and Statistics. X. Glorot and Y. Bengio. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In Interna- tional Conference on Artificial Intelligence and Statistics, 2010.\n\nDeep Learning. I Goodfellow, Y Bengio, A Courville, MIT pressI. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT press, 2016.\n\nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. A Graves, S Fern\u00e1ndez, F Gomez, J Schmidhuber, ACM International Conference on Machine Learning (ICML). A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber. Connectionist Temporal Classification: Labelling Unseg- mented Sequence Data with Recurrent Neural Networks. In ACM International Conference on Machine Learning (ICML), 2006.\n\nA Novel Connectionist System for Unconstrained Handwriting Recognition. A Graves, M Liwicki, S Fern\u00e1ndez, R Bertolami, H Bunke, J Schmidhuber, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 315A. Graves, M. Liwicki, S. Fern\u00e1ndez, R. Bertolami, H. Bunke, and J. Schmidhuber. A Novel Connectionist System for Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 31(5), 2009.\n\nSpeech Recognition with Deep Recurrent Neural Networks. A Graves, A Mohamed, G Hinton, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). A. Graves, A.-r. Mohamed, and G. Hinton. Speech Recogni- tion with Deep Recurrent Neural Networks. In IEEE Inter- national Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), 2013.\n\nDGS Corpus & Dicta-Sign: The Hamburg Studio Setup. T Hanke, L K\u00f6nig, S Wagner, S Matthes, Representation and Processing of Sign Languages: Corpora and Sign Language Technologies. T. Hanke, L. K\u00f6nig, S. Wagner, and S. Matthes. DGS Corpus & Dicta-Sign: The Hamburg Studio Setup. In Representa- tion and Processing of Sign Languages: Corpora and Sign Language Technologies, 2010.\n\nLong Short-Term Memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber. Long Short-Term Mem- ory. Neural Computation, 9(8), 1997.\n\nConnectionist Temporal Modeling for Weakly Supervised Action Labeling. D.-A Huang, L Fei-Fei, J C Niebles, European Conference on Computer Vision (ECCV). D.-A. Huang, L. Fei-Fei, and J. C. Niebles. Connectionist Temporal Modeling for Weakly Supervised Action Labeling. In European Conference on Computer Vision (ECCV), 2016.\n\nRecurrent Continuous Translation Models. N Kalchbrenner, P Blunsom, Conference on Empirical Methods in Natural Language Processing (EMNLP). N. Kalchbrenner and P. Blunsom. Recurrent Continuous Translation Models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013.\n\nAdam: A Method for Stochastic Optimization. D P Kingma, J Ba, International Conference on Learning Representations (ICLR). D. P. Kingma and J. Ba. Adam: A Method for Stochas- tic Optimization. In International Conference on Learning Representations (ICLR), 2014.\n\nO Koller, J Forster, H Ney, Continuous Sign Language Recognition: Towards Large Vocabulary Statistical Recognition Systems Handling Multiple Signers. Computer Vision and Image Understanding (CVIU). 141O. Koller, J. Forster, and H. Ney. Continuous Sign Language Recognition: Towards Large Vocabulary Statistical Recog- nition Systems Handling Multiple Signers. Computer Vision and Image Understanding (CVIU), 141, 2015.\n\nDeep Learning of Mouth Shapes for Sign Language. O Koller, H Ney, R Bowden, IEEE International Conference on Computer Vision Workshops (ICCVW). O. Koller, H. Ney, and R. Bowden. Deep Learning of Mouth Shapes for Sign Language. In IEEE International Confer- ence on Computer Vision Workshops (ICCVW), 2015.\n\nDeep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled. O Koller, H Ney, R Bowden, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). O. Koller, H. Ney, and R. Bowden. Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nRe-Sign: Re-Aligned End-to-End Sequence Modelling with Deep Recurrent CNN-HMMs. O Koller, S Zargaran, H Ney, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. O. Koller, S. Zargaran, and H. Ney. Re-Sign: Re-Aligned End-to-End Sequence Modelling with Deep Recurrent CNN- HMMs. In IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 2017.\n\nDeep Sign: Hybrid CNN-HMM for Continuous Sign Language Recognition. O Koller, S Zargaran, H Ney, R Bowden, British Machine Vision Conference (BMVC). O. Koller, S. Zargaran, H. Ney, and R. Bowden. Deep Sign: Hybrid CNN-HMM for Continuous Sign Language Recog- nition. In British Machine Vision Conference (BMVC), 2016.\n\nImageNet Classification with Deep Convolutional Neural Networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems (NIPS). A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems (NIPS). 2012.\n\nDeep Learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553Y. LeCun, Y. Bengio, and G. Hinton. Deep Learning. Nature, 521(7553), 2015.\n\nGradientbased Learning Applied to Document Recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based Learning Applied to Document Recognition. IEEE, 86(11), 1998.\n\nEfficient Mini-batch Training for Stochastic Optimization. M Li, T Zhang, Y Chen, A J Smola, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. M. Li, T. Zhang, Y. Chen, and A. J. Smola. Efficient Mini-batch Training for Stochastic Optimization. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014.\n\nRouge: A Package for Automatic Evaluation of Summaries. C.-Y. Lin, Annual Meeting of the Association for Computational Linguistics, Text Summarization Branches Out Workshop. C.-Y. Lin. Rouge: A Package for Automatic Evaluation of Summaries. In Annual Meeting of the Association for Com- putational Linguistics, Text Summarization Branches Out Workshop, 2004.\n\nNeural Machine Translation (seq2seq) Tutorial. M.-T Luong, E Brevdo, R Zhao, M.-T. Luong, E. Brevdo, and R. Zhao. Neu- ral Machine Translation (seq2seq) Tutorial.\n\nEffective Approaches to Attention-based Neural Machine Translation. M.-T Luong, H Pham, C D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). M.-T. Luong, H. Pham, and C. D. Manning. Effective Ap- proaches to Attention-based Neural Machine Translation. In Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP), 2015.\n\nEfficient Estimation of Word Representations in Vector Space. T Mikolov, K Chen, G Corrado, J Dean, ternational Conference on Learning Representations (ICLR). T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient Estimation of Word Representations in Vector Space. In- ternational Conference on Learning Representations (ICLR), 2013.\n\nBuilding a Sign Language Corpus for Use in Machine Translation. S Morrissey, H Somers, R Smith, S Gilchrist, S Dandapat, Representation and Processing of Sign Languages: Corpora and Sign Language Technologies. S. Morrissey, H. Somers, R. Smith, S. Gilchrist, and S. Dan- dapat. Building a Sign Language Corpus for Use in Machine Translation. In Representation and Processing of Sign Lan- guages: Corpora and Sign Language Technologies, 2010.\n\nNeural Machine Translation and Sequence-to-Sequence Models: A Tutorial. G Neubig, arXiv:1703.01619G. Neubig. Neural Machine Translation and Sequence-to- Sequence Models: A Tutorial. arXiv:1703.01619, 2017.\n\nSign Language Recognition using Sequential Pattern Trees. E.-J Ong, H Cooper, N Pugeault, R Bowden, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). E.-J. Ong, H. Cooper, N. Pugeault, and R. Bowden. Sign Language Recognition using Sequential Pattern Trees. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2012.\n\nBLEU: A Method for Automatic Evaluation of Machine Translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Annual Meeting of the Association for Computational Linguistics (ACL). K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: A Method for Automatic Evaluation of Machine Translation. In Annual Meeting of the Association for Computational Lin- guistics (ACL), 2002.\n\nLarge-scale Learning of Sign Language by Watching TV (Using Cooccurrences). T Pfister, J Charles, A Zisserman, T. Pfister, J. Charles, and A. Zisserman. Large-scale Learning of Sign Language by Watching TV (Using Co- occurrences).\n\nBritish Machine Vision Conference (BMVC). In British Machine Vision Conference (BMVC), 2013.\n\nA Schembri, J Fenlon, R Rentelis, S Reynolds, K Cormier, Building the British Sign Language Corpus. Language Documentation & Conservation (LD&C). 7A. Schembri, J. Fenlon, R. Rentelis, S. Reynolds, and K. Cormier. Building the British Sign Language Corpus. Language Documentation & Conservation (LD&C), 7, 2013.\n\nUsing Viseme Recognition to Improve a Sign Language Translation System. C Schmidt, O Koller, H Ney, T Hoyoux, J Piater, International Workshop on Spoken Language Translation. C. Schmidt, O. Koller, H. Ney, T. Hoyoux, and J. Piater. Us- ing Viseme Recognition to Improve a Sign Language Trans- lation System. In International Workshop on Spoken Lan- guage Translation, 2013.\n\nOpening the Black Box of Deep Neural Networks via Information. R Shwartz-Ziv, N Tishby, arXiv:1703.00810R. Shwartz-Ziv and N. Tishby. Opening the Black Box of Deep Neural Networks via Information. arXiv:1703.00810, 2017.\n\nAnalysis, Preparation, and Optimization of Statistical Sign Language Machine Translation. D Stein, C Schmidt, H Ney, Machine Translation. 264D. Stein, C. Schmidt, and H. Ney. Analysis, Preparation, and Optimization of Statistical Sign Language Machine Transla- tion. Machine Translation, 26(4), 2012.\n\nSign Language Structure. W C Stokoe, Annual Review of Anthropology. 91W. C. Stokoe. Sign Language Structure. Annual Review of Anthropology, 9(1), 1980.\n\nSequence to Sequence Learning with Neural Networks. I Sutskever, O Vinyals, Q V Le, Advances in Neural Information Processing Systems (NIPS). I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks. In Advances in Neural In- formation Processing Systems (NIPS), 2014.\n\nDeep Learning and the Information Bottleneck Principle. N Tishby, N Zaslavsky, IEEE Information Theory Workshop (ITW). N. Tishby and N. Zaslavsky. Deep Learning and the Infor- mation Bottleneck Principle. In IEEE Information Theory Workshop (ITW), 2015.\n\nParallel Midden Markov Models for American Sign Language Recognition. C Vogler, D Metaxas, IEEE International Conference on Computer Vision (ICCV). C. Vogler and D. Metaxas. Parallel Midden Markov Models for American Sign Language Recognition. In IEEE Interna- tional Conference on Computer Vision (ICCV), 1999.\n\nConvolutional Pose Machines. S.-E Wei, V Ramakrishna, T Kanade, Y Sheikh, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con- volutional Pose Machines. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nGoogle's Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation. Y Wu, M Schuster, Z Chen, Q V Le, arXiv:1609.08144Y. Wu, M. Schuster, Z. Chen, Q. V. Le, et al. Google's Neu- ral Machine Translation System: Bridging the Gap Between Human and Machine Translation. arXiv:1609.08144, 2016.\n\nAttend and Tell: Neural Image Caption Generation with Visual Attention. K Xu, J Ba, R Kiros, K Cho, A Courville, R Salakhudinov, R Zemel, Y Bengio, Show, International Conference on Machine Learning (ICML). K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi- nov, R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In Interna- tional Conference on Machine Learning (ICML), 2015.\n\nSign Language Spotting with a Threshold Model based on Conditional Random Fields. H.-D Yang, S Sclaroff, S.-W Lee, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 731H.-D. Yang, S. Sclaroff, and S.-W. Lee. Sign Language Spot- ting with a Threshold Model based on Conditional Random Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 31(7), 2009.\n\nIterative Reference Driven Metric Learning for Signer Independent Isolated Sign Language Recognition. F Yin, X Chai, X Chen, European Conference on Computer Vision (ECCV). F. Yin, X. Chai, and X. Chen. Iterative Reference Driven Metric Learning for Signer Independent Isolated Sign Lan- guage Recognition. In European Conference on Computer Vision (ECCV), 2016.\n", "annotations": {"author": "[{\"end\":79,\"start\":36},{\"end\":142,\"start\":80},{\"end\":206,\"start\":143},{\"end\":266,\"start\":207},{\"end\":327,\"start\":267}]", "publisher": null, "author_last_name": "[{\"end\":55,\"start\":43},{\"end\":94,\"start\":86},{\"end\":155,\"start\":149},{\"end\":218,\"start\":215},{\"end\":281,\"start\":275}]", "author_first_name": "[{\"end\":42,\"start\":36},{\"end\":85,\"start\":80},{\"end\":148,\"start\":143},{\"end\":214,\"start\":207},{\"end\":274,\"start\":267}]", "author_affiliation": "[{\"end\":78,\"start\":57},{\"end\":141,\"start\":120},{\"end\":205,\"start\":182},{\"end\":265,\"start\":242},{\"end\":326,\"start\":305}]", "title": "[{\"end\":33,\"start\":1},{\"end\":360,\"start\":328}]", "venue": null, "abstract": "[{\"end\":1945,\"start\":362}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2115,\"start\":2111},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2216,\"start\":2212},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2280,\"start\":2276},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2283,\"start\":2280},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4632,\"start\":4629},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5004,\"start\":5000},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5007,\"start\":5004},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5010,\"start\":5007},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5193,\"start\":5189},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5196,\"start\":5193},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5251,\"start\":5247},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":5466,\"start\":5462},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5468,\"start\":5466},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5777,\"start\":5774},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5780,\"start\":5777},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5783,\"start\":5780},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5847,\"start\":5843},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5850,\"start\":5847},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5852,\"start\":5850},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6041,\"start\":6037},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6097,\"start\":6093},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6358,\"start\":6354},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6469,\"start\":6465},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6493,\"start\":6490},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6552,\"start\":6548},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6643,\"start\":6639},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6646,\"start\":6643},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6789,\"start\":6785},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6821,\"start\":6817},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6851,\"start\":6848},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6854,\"start\":6851},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6969,\"start\":6965},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6985,\"start\":6981},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6988,\"start\":6985},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7008,\"start\":7004},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7096,\"start\":7093},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7099,\"start\":7096},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7102,\"start\":7099},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7475,\"start\":7471},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7727,\"start\":7723},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7729,\"start\":7727},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7781,\"start\":7777},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7896,\"start\":7893},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7921,\"start\":7917},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7949,\"start\":7946},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7962,\"start\":7959},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7965,\"start\":7962},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8446,\"start\":8442},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8486,\"start\":8482},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8643,\"start\":8639},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8862,\"start\":8858},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8946,\"start\":8942},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8966,\"start\":8962},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9338,\"start\":9335},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9551,\"start\":9547},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9772,\"start\":9768},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9862,\"start\":9858},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10043,\"start\":10039},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10061,\"start\":10057},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10089,\"start\":10085},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11527,\"start\":11523},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13776,\"start\":13772},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14717,\"start\":14713},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15403,\"start\":15399},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17184,\"start\":17181},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17224,\"start\":17220},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18263,\"start\":18259},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18344,\"start\":18341},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18756,\"start\":18755},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19888,\"start\":19884},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22042,\"start\":22038},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22131,\"start\":22127},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22379,\"start\":22375},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22412,\"start\":22408},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23080,\"start\":23076},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23095,\"start\":23091},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23381,\"start\":23378},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23442,\"start\":23438},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24719,\"start\":24715},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24817,\"start\":24813},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24820,\"start\":24817},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":24848,\"start\":24844},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24892,\"start\":24888},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24930,\"start\":24926},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26561,\"start\":26557},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26869,\"start\":26865},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26872,\"start\":26869},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31130,\"start\":31126},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":36168,\"start\":36164},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36793,\"start\":36790}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36932,\"start\":36887},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37046,\"start\":36933},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37101,\"start\":37047},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37175,\"start\":37102},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37517,\"start\":37176},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37856,\"start\":37518},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38742,\"start\":37857},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39605,\"start\":38743},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39668,\"start\":39606}]", "paragraph": "[{\"end\":2623,\"start\":1961},{\"end\":3565,\"start\":2625},{\"end\":3624,\"start\":3567},{\"end\":4504,\"start\":3626},{\"end\":5680,\"start\":4521},{\"end\":6319,\"start\":5682},{\"end\":7103,\"start\":6321},{\"end\":7966,\"start\":7105},{\"end\":9030,\"start\":7968},{\"end\":10091,\"start\":9032},{\"end\":11333,\"start\":10128},{\"end\":12203,\"start\":11366},{\"end\":12338,\"start\":12236},{\"end\":12490,\"start\":12340},{\"end\":12578,\"start\":12520},{\"end\":13167,\"start\":12602},{\"end\":13588,\"start\":13169},{\"end\":13906,\"start\":13621},{\"end\":14511,\"start\":13952},{\"end\":15003,\"start\":14513},{\"end\":15719,\"start\":15035},{\"end\":16189,\"start\":15759},{\"end\":16494,\"start\":16234},{\"end\":16809,\"start\":16520},{\"end\":17225,\"start\":16811},{\"end\":17567,\"start\":17227},{\"end\":17992,\"start\":17593},{\"end\":18377,\"start\":18056},{\"end\":18612,\"start\":18467},{\"end\":18839,\"start\":18646},{\"end\":19124,\"start\":18924},{\"end\":19919,\"start\":19126},{\"end\":20402,\"start\":19921},{\"end\":21172,\"start\":20404},{\"end\":21353,\"start\":21201},{\"end\":21699,\"start\":21355},{\"end\":22598,\"start\":21701},{\"end\":23019,\"start\":22600},{\"end\":23331,\"start\":23021},{\"end\":23474,\"start\":23333},{\"end\":24215,\"start\":23514},{\"end\":24529,\"start\":24217},{\"end\":25148,\"start\":24564},{\"end\":25572,\"start\":25150},{\"end\":25948,\"start\":25617},{\"end\":26421,\"start\":25950},{\"end\":27484,\"start\":26449},{\"end\":28146,\"start\":27486},{\"end\":29483,\"start\":28172},{\"end\":29954,\"start\":29523},{\"end\":30887,\"start\":29956},{\"end\":31492,\"start\":30922},{\"end\":32236,\"start\":31494},{\"end\":32693,\"start\":32264},{\"end\":32839,\"start\":32695},{\"end\":32999,\"start\":32841},{\"end\":33155,\"start\":33001},{\"end\":33286,\"start\":33157},{\"end\":33398,\"start\":33288},{\"end\":33503,\"start\":33400},{\"end\":33616,\"start\":33505},{\"end\":33746,\"start\":33618},{\"end\":33896,\"start\":33748},{\"end\":34023,\"start\":33898},{\"end\":34118,\"start\":34025},{\"end\":34235,\"start\":34120},{\"end\":34355,\"start\":34237},{\"end\":34456,\"start\":34357},{\"end\":34572,\"start\":34458},{\"end\":34638,\"start\":34574},{\"end\":34903,\"start\":34640},{\"end\":35653,\"start\":34905},{\"end\":36292,\"start\":35668},{\"end\":36528,\"start\":36294},{\"end\":36886,\"start\":36530}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12235,\"start\":12204},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12519,\"start\":12491},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13620,\"start\":13589},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15034,\"start\":15004},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15758,\"start\":15720},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16233,\"start\":16190},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17592,\"start\":17568},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18055,\"start\":17993},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18466,\"start\":18378},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18645,\"start\":18613},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18887,\"start\":18840}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20577,\"start\":20570},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25178,\"start\":25171},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26100,\"start\":26093},{\"end\":27958,\"start\":27951},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30079,\"start\":30072},{\"end\":32039,\"start\":32032},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32467,\"start\":32460}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1959,\"start\":1947},{\"attributes\":{\"n\":\"2.\"},\"end\":4519,\"start\":4507},{\"attributes\":{\"n\":\"3.\"},\"end\":10126,\"start\":10094},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11364,\"start\":11336},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12600,\"start\":12581},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13950,\"start\":13909},{\"end\":16518,\"start\":16497},{\"attributes\":{\"n\":\"4.\"},\"end\":18922,\"start\":18889},{\"attributes\":{\"n\":\"5.\"},\"end\":21199,\"start\":21175},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23512,\"start\":23477},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":24562,\"start\":24532},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":25615,\"start\":25575},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":26447,\"start\":26424},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":28170,\"start\":28149},{\"attributes\":{\"n\":\"5.2.\"},\"end\":29521,\"start\":29486},{\"attributes\":{\"n\":\"5.3.\"},\"end\":30920,\"start\":30890},{\"attributes\":{\"n\":\"6.\"},\"end\":32262,\"start\":32239},{\"attributes\":{\"n\":\"7.\"},\"end\":35666,\"start\":35656},{\"end\":36898,\"start\":36888},{\"end\":36944,\"start\":36934},{\"end\":37058,\"start\":37048},{\"end\":37113,\"start\":37103},{\"end\":37186,\"start\":37177},{\"end\":37528,\"start\":37519},{\"end\":37867,\"start\":37858},{\"end\":38753,\"start\":38744},{\"end\":39616,\"start\":39607}]", "table": "[{\"end\":37517,\"start\":37222},{\"end\":37856,\"start\":37605},{\"end\":38742,\"start\":37906},{\"end\":39605,\"start\":38792}]", "figure_caption": "[{\"end\":36932,\"start\":36900},{\"end\":37046,\"start\":36946},{\"end\":37101,\"start\":37060},{\"end\":37175,\"start\":37115},{\"end\":37222,\"start\":37188},{\"end\":37605,\"start\":37530},{\"end\":37906,\"start\":37869},{\"end\":38792,\"start\":38755},{\"end\":39668,\"start\":39618}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2423,\"start\":2415},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11235,\"start\":11227},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29075,\"start\":29067},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34916,\"start\":34908}]", "bib_author_first_name": "[{\"end\":40157,\"start\":40156},{\"end\":40166,\"start\":40165},{\"end\":40177,\"start\":40176},{\"end\":40187,\"start\":40186},{\"end\":40197,\"start\":40196},{\"end\":40205,\"start\":40204},{\"end\":40214,\"start\":40213},{\"end\":40216,\"start\":40215},{\"end\":40227,\"start\":40226},{\"end\":40236,\"start\":40235},{\"end\":40244,\"start\":40243},{\"end\":40626,\"start\":40625},{\"end\":40636,\"start\":40635},{\"end\":40647,\"start\":40646},{\"end\":40661,\"start\":40660},{\"end\":40669,\"start\":40668},{\"end\":40679,\"start\":40678},{\"end\":40692,\"start\":40691},{\"end\":40700,\"start\":40699},{\"end\":40715,\"start\":40714},{\"end\":40725,\"start\":40724},{\"end\":41049,\"start\":41048},{\"end\":41051,\"start\":41050},{\"end\":41061,\"start\":41060},{\"end\":41077,\"start\":41076},{\"end\":41089,\"start\":41088},{\"end\":41279,\"start\":41278},{\"end\":41291,\"start\":41290},{\"end\":41298,\"start\":41297},{\"end\":41691,\"start\":41690},{\"end\":41702,\"start\":41701},{\"end\":41715,\"start\":41714},{\"end\":42067,\"start\":42066},{\"end\":42069,\"start\":42068},{\"end\":42079,\"start\":42078},{\"end\":42091,\"start\":42090},{\"end\":42101,\"start\":42100},{\"end\":42454,\"start\":42453},{\"end\":42456,\"start\":42455},{\"end\":42466,\"start\":42465},{\"end\":42468,\"start\":42467},{\"end\":42482,\"start\":42481},{\"end\":42495,\"start\":42494},{\"end\":42506,\"start\":42505},{\"end\":42508,\"start\":42507},{\"end\":42517,\"start\":42516},{\"end\":42923,\"start\":42922},{\"end\":42930,\"start\":42929},{\"end\":42942,\"start\":42938},{\"end\":42949,\"start\":42948},{\"end\":43273,\"start\":43272},{\"end\":43281,\"start\":43280},{\"end\":43287,\"start\":43286},{\"end\":43608,\"start\":43607},{\"end\":43619,\"start\":43618},{\"end\":43630,\"start\":43629},{\"end\":43644,\"start\":43643},{\"end\":43980,\"start\":43979},{\"end\":43987,\"start\":43986},{\"end\":44006,\"start\":44005},{\"end\":44018,\"start\":44017},{\"end\":44292,\"start\":44291},{\"end\":44301,\"start\":44300},{\"end\":44313,\"start\":44312},{\"end\":44320,\"start\":44319},{\"end\":44610,\"start\":44609},{\"end\":44612,\"start\":44611},{\"end\":44621,\"start\":44620},{\"end\":44631,\"start\":44630},{\"end\":44642,\"start\":44641},{\"end\":44980,\"start\":44979},{\"end\":44990,\"start\":44989},{\"end\":45288,\"start\":45287},{\"end\":45298,\"start\":45297},{\"end\":45306,\"start\":45305},{\"end\":45487,\"start\":45486},{\"end\":45500,\"start\":45496},{\"end\":45507,\"start\":45506},{\"end\":45519,\"start\":45518},{\"end\":45827,\"start\":45826},{\"end\":45834,\"start\":45833},{\"end\":45841,\"start\":45840},{\"end\":46185,\"start\":46184},{\"end\":46193,\"start\":46192},{\"end\":46201,\"start\":46200},{\"end\":46214,\"start\":46210},{\"end\":46220,\"start\":46219},{\"end\":46226,\"start\":46225},{\"end\":46581,\"start\":46580},{\"end\":46592,\"start\":46591},{\"end\":46605,\"start\":46604},{\"end\":46619,\"start\":46618},{\"end\":46631,\"start\":46630},{\"end\":46646,\"start\":46645},{\"end\":46656,\"start\":46655},{\"end\":47089,\"start\":47088},{\"end\":47100,\"start\":47099},{\"end\":47111,\"start\":47110},{\"end\":47121,\"start\":47120},{\"end\":47131,\"start\":47130},{\"end\":47140,\"start\":47139},{\"end\":47142,\"start\":47141},{\"end\":47152,\"start\":47151},{\"end\":47573,\"start\":47572},{\"end\":47584,\"start\":47583},{\"end\":47595,\"start\":47594},{\"end\":47605,\"start\":47604},{\"end\":47618,\"start\":47617},{\"end\":47972,\"start\":47971},{\"end\":47983,\"start\":47982},{\"end\":47991,\"start\":47990},{\"end\":48003,\"start\":48002},{\"end\":48013,\"start\":48012},{\"end\":48015,\"start\":48014},{\"end\":48333,\"start\":48332},{\"end\":48343,\"start\":48342},{\"end\":48616,\"start\":48615},{\"end\":48630,\"start\":48629},{\"end\":48640,\"start\":48639},{\"end\":48846,\"start\":48845},{\"end\":48856,\"start\":48855},{\"end\":48869,\"start\":48868},{\"end\":48878,\"start\":48877},{\"end\":49253,\"start\":49252},{\"end\":49263,\"start\":49262},{\"end\":49274,\"start\":49273},{\"end\":49287,\"start\":49286},{\"end\":49300,\"start\":49299},{\"end\":49309,\"start\":49308},{\"end\":49694,\"start\":49693},{\"end\":49704,\"start\":49703},{\"end\":49715,\"start\":49714},{\"end\":50055,\"start\":50054},{\"end\":50064,\"start\":50063},{\"end\":50073,\"start\":50072},{\"end\":50083,\"start\":50082},{\"end\":50406,\"start\":50405},{\"end\":50420,\"start\":50419},{\"end\":50624,\"start\":50620},{\"end\":50633,\"start\":50632},{\"end\":50644,\"start\":50643},{\"end\":50646,\"start\":50645},{\"end\":50917,\"start\":50916},{\"end\":50933,\"start\":50932},{\"end\":51215,\"start\":51214},{\"end\":51217,\"start\":51216},{\"end\":51227,\"start\":51226},{\"end\":51435,\"start\":51434},{\"end\":51445,\"start\":51444},{\"end\":51456,\"start\":51455},{\"end\":51904,\"start\":51903},{\"end\":51914,\"start\":51913},{\"end\":51921,\"start\":51920},{\"end\":52267,\"start\":52266},{\"end\":52277,\"start\":52276},{\"end\":52284,\"start\":52283},{\"end\":52657,\"start\":52656},{\"end\":52667,\"start\":52666},{\"end\":52679,\"start\":52678},{\"end\":53016,\"start\":53015},{\"end\":53026,\"start\":53025},{\"end\":53038,\"start\":53037},{\"end\":53045,\"start\":53044},{\"end\":53331,\"start\":53330},{\"end\":53345,\"start\":53344},{\"end\":53358,\"start\":53357},{\"end\":53360,\"start\":53359},{\"end\":53623,\"start\":53622},{\"end\":53632,\"start\":53631},{\"end\":53642,\"start\":53641},{\"end\":53800,\"start\":53799},{\"end\":53809,\"start\":53808},{\"end\":53819,\"start\":53818},{\"end\":53829,\"start\":53828},{\"end\":54036,\"start\":54035},{\"end\":54042,\"start\":54041},{\"end\":54051,\"start\":54050},{\"end\":54059,\"start\":54058},{\"end\":54061,\"start\":54060},{\"end\":54394,\"start\":54389},{\"end\":54744,\"start\":54740},{\"end\":54753,\"start\":54752},{\"end\":54763,\"start\":54762},{\"end\":54929,\"start\":54925},{\"end\":54938,\"start\":54937},{\"end\":54946,\"start\":54945},{\"end\":54948,\"start\":54947},{\"end\":55288,\"start\":55287},{\"end\":55299,\"start\":55298},{\"end\":55307,\"start\":55306},{\"end\":55318,\"start\":55317},{\"end\":55627,\"start\":55626},{\"end\":55640,\"start\":55639},{\"end\":55650,\"start\":55649},{\"end\":55659,\"start\":55658},{\"end\":55672,\"start\":55671},{\"end\":56078,\"start\":56077},{\"end\":56274,\"start\":56270},{\"end\":56281,\"start\":56280},{\"end\":56291,\"start\":56290},{\"end\":56303,\"start\":56302},{\"end\":56631,\"start\":56630},{\"end\":56643,\"start\":56642},{\"end\":56653,\"start\":56652},{\"end\":56664,\"start\":56660},{\"end\":57013,\"start\":57012},{\"end\":57024,\"start\":57023},{\"end\":57035,\"start\":57034},{\"end\":57263,\"start\":57262},{\"end\":57275,\"start\":57274},{\"end\":57285,\"start\":57284},{\"end\":57297,\"start\":57296},{\"end\":57309,\"start\":57308},{\"end\":57647,\"start\":57646},{\"end\":57658,\"start\":57657},{\"end\":57668,\"start\":57667},{\"end\":57675,\"start\":57674},{\"end\":57685,\"start\":57684},{\"end\":58013,\"start\":58012},{\"end\":58028,\"start\":58027},{\"end\":58262,\"start\":58261},{\"end\":58271,\"start\":58270},{\"end\":58282,\"start\":58281},{\"end\":58499,\"start\":58498},{\"end\":58501,\"start\":58500},{\"end\":58679,\"start\":58678},{\"end\":58692,\"start\":58691},{\"end\":58703,\"start\":58702},{\"end\":58705,\"start\":58704},{\"end\":58987,\"start\":58986},{\"end\":58997,\"start\":58996},{\"end\":59256,\"start\":59255},{\"end\":59266,\"start\":59265},{\"end\":59531,\"start\":59527},{\"end\":59538,\"start\":59537},{\"end\":59553,\"start\":59552},{\"end\":59563,\"start\":59562},{\"end\":59901,\"start\":59900},{\"end\":59907,\"start\":59906},{\"end\":59919,\"start\":59918},{\"end\":59927,\"start\":59926},{\"end\":59929,\"start\":59928},{\"end\":60196,\"start\":60195},{\"end\":60202,\"start\":60201},{\"end\":60208,\"start\":60207},{\"end\":60217,\"start\":60216},{\"end\":60224,\"start\":60223},{\"end\":60237,\"start\":60236},{\"end\":60253,\"start\":60252},{\"end\":60262,\"start\":60261},{\"end\":60649,\"start\":60645},{\"end\":60657,\"start\":60656},{\"end\":60672,\"start\":60668},{\"end\":61066,\"start\":61065},{\"end\":61073,\"start\":61072},{\"end\":61081,\"start\":61080}]", "bib_author_last_name": "[{\"end\":40163,\"start\":40158},{\"end\":40174,\"start\":40167},{\"end\":40184,\"start\":40178},{\"end\":40194,\"start\":40188},{\"end\":40202,\"start\":40198},{\"end\":40211,\"start\":40206},{\"end\":40224,\"start\":40217},{\"end\":40233,\"start\":40228},{\"end\":40241,\"start\":40237},{\"end\":40250,\"start\":40245},{\"end\":40633,\"start\":40627},{\"end\":40644,\"start\":40637},{\"end\":40658,\"start\":40648},{\"end\":40666,\"start\":40662},{\"end\":40676,\"start\":40670},{\"end\":40689,\"start\":40680},{\"end\":40697,\"start\":40693},{\"end\":40712,\"start\":40701},{\"end\":40722,\"start\":40716},{\"end\":40732,\"start\":40726},{\"end\":41058,\"start\":41052},{\"end\":41074,\"start\":41062},{\"end\":41086,\"start\":41078},{\"end\":41100,\"start\":41090},{\"end\":41288,\"start\":41280},{\"end\":41295,\"start\":41292},{\"end\":41305,\"start\":41299},{\"end\":41699,\"start\":41692},{\"end\":41712,\"start\":41703},{\"end\":41726,\"start\":41716},{\"end\":42076,\"start\":42070},{\"end\":42088,\"start\":42080},{\"end\":42098,\"start\":42092},{\"end\":42108,\"start\":42102},{\"end\":42463,\"start\":42457},{\"end\":42479,\"start\":42469},{\"end\":42492,\"start\":42483},{\"end\":42503,\"start\":42496},{\"end\":42514,\"start\":42509},{\"end\":42524,\"start\":42518},{\"end\":42927,\"start\":42924},{\"end\":42936,\"start\":42931},{\"end\":42946,\"start\":42943},{\"end\":42956,\"start\":42950},{\"end\":43278,\"start\":43274},{\"end\":43284,\"start\":43282},{\"end\":43291,\"start\":43288},{\"end\":43616,\"start\":43609},{\"end\":43627,\"start\":43620},{\"end\":43641,\"start\":43631},{\"end\":43654,\"start\":43645},{\"end\":43984,\"start\":43981},{\"end\":44003,\"start\":43988},{\"end\":44015,\"start\":44007},{\"end\":44025,\"start\":44019},{\"end\":44298,\"start\":44293},{\"end\":44310,\"start\":44302},{\"end\":44317,\"start\":44314},{\"end\":44327,\"start\":44321},{\"end\":44618,\"start\":44613},{\"end\":44628,\"start\":44622},{\"end\":44639,\"start\":44632},{\"end\":44652,\"start\":44643},{\"end\":44987,\"start\":44981},{\"end\":44997,\"start\":44991},{\"end\":45295,\"start\":45289},{\"end\":45303,\"start\":45299},{\"end\":45313,\"start\":45307},{\"end\":45494,\"start\":45488},{\"end\":45504,\"start\":45501},{\"end\":45516,\"start\":45508},{\"end\":45526,\"start\":45520},{\"end\":45831,\"start\":45828},{\"end\":45838,\"start\":45835},{\"end\":45847,\"start\":45842},{\"end\":46190,\"start\":46186},{\"end\":46198,\"start\":46194},{\"end\":46208,\"start\":46202},{\"end\":46217,\"start\":46215},{\"end\":46223,\"start\":46221},{\"end\":46234,\"start\":46227},{\"end\":46589,\"start\":46582},{\"end\":46602,\"start\":46593},{\"end\":46616,\"start\":46606},{\"end\":46628,\"start\":46620},{\"end\":46643,\"start\":46632},{\"end\":46653,\"start\":46647},{\"end\":46664,\"start\":46657},{\"end\":47097,\"start\":47090},{\"end\":47108,\"start\":47101},{\"end\":47118,\"start\":47112},{\"end\":47128,\"start\":47122},{\"end\":47137,\"start\":47132},{\"end\":47149,\"start\":47143},{\"end\":47156,\"start\":47153},{\"end\":47581,\"start\":47574},{\"end\":47592,\"start\":47585},{\"end\":47602,\"start\":47596},{\"end\":47615,\"start\":47606},{\"end\":47622,\"start\":47619},{\"end\":47980,\"start\":47973},{\"end\":47988,\"start\":47984},{\"end\":48000,\"start\":47992},{\"end\":48010,\"start\":48004},{\"end\":48023,\"start\":48016},{\"end\":48340,\"start\":48334},{\"end\":48350,\"start\":48344},{\"end\":48627,\"start\":48617},{\"end\":48637,\"start\":48631},{\"end\":48650,\"start\":48641},{\"end\":48853,\"start\":48847},{\"end\":48866,\"start\":48857},{\"end\":48875,\"start\":48870},{\"end\":48890,\"start\":48879},{\"end\":49260,\"start\":49254},{\"end\":49271,\"start\":49264},{\"end\":49284,\"start\":49275},{\"end\":49297,\"start\":49288},{\"end\":49306,\"start\":49301},{\"end\":49321,\"start\":49310},{\"end\":49701,\"start\":49695},{\"end\":49712,\"start\":49705},{\"end\":49722,\"start\":49716},{\"end\":50061,\"start\":50056},{\"end\":50070,\"start\":50065},{\"end\":50080,\"start\":50074},{\"end\":50091,\"start\":50084},{\"end\":50417,\"start\":50407},{\"end\":50432,\"start\":50421},{\"end\":50630,\"start\":50625},{\"end\":50641,\"start\":50634},{\"end\":50654,\"start\":50647},{\"end\":50930,\"start\":50918},{\"end\":50941,\"start\":50934},{\"end\":51224,\"start\":51218},{\"end\":51230,\"start\":51228},{\"end\":51442,\"start\":51436},{\"end\":51453,\"start\":51446},{\"end\":51460,\"start\":51457},{\"end\":51911,\"start\":51905},{\"end\":51918,\"start\":51915},{\"end\":51928,\"start\":51922},{\"end\":52274,\"start\":52268},{\"end\":52281,\"start\":52278},{\"end\":52291,\"start\":52285},{\"end\":52664,\"start\":52658},{\"end\":52676,\"start\":52668},{\"end\":52683,\"start\":52680},{\"end\":53023,\"start\":53017},{\"end\":53035,\"start\":53027},{\"end\":53042,\"start\":53039},{\"end\":53052,\"start\":53046},{\"end\":53342,\"start\":53332},{\"end\":53355,\"start\":53346},{\"end\":53367,\"start\":53361},{\"end\":53629,\"start\":53624},{\"end\":53639,\"start\":53633},{\"end\":53649,\"start\":53643},{\"end\":53806,\"start\":53801},{\"end\":53816,\"start\":53810},{\"end\":53826,\"start\":53820},{\"end\":53837,\"start\":53830},{\"end\":54039,\"start\":54037},{\"end\":54048,\"start\":54043},{\"end\":54056,\"start\":54052},{\"end\":54067,\"start\":54062},{\"end\":54398,\"start\":54395},{\"end\":54750,\"start\":54745},{\"end\":54760,\"start\":54754},{\"end\":54768,\"start\":54764},{\"end\":54935,\"start\":54930},{\"end\":54943,\"start\":54939},{\"end\":54956,\"start\":54949},{\"end\":55296,\"start\":55289},{\"end\":55304,\"start\":55300},{\"end\":55315,\"start\":55308},{\"end\":55323,\"start\":55319},{\"end\":55637,\"start\":55628},{\"end\":55647,\"start\":55641},{\"end\":55656,\"start\":55651},{\"end\":55669,\"start\":55660},{\"end\":55681,\"start\":55673},{\"end\":56085,\"start\":56079},{\"end\":56278,\"start\":56275},{\"end\":56288,\"start\":56282},{\"end\":56300,\"start\":56292},{\"end\":56310,\"start\":56304},{\"end\":56640,\"start\":56632},{\"end\":56650,\"start\":56644},{\"end\":56658,\"start\":56654},{\"end\":56668,\"start\":56665},{\"end\":57021,\"start\":57014},{\"end\":57032,\"start\":57025},{\"end\":57045,\"start\":57036},{\"end\":57272,\"start\":57264},{\"end\":57282,\"start\":57276},{\"end\":57294,\"start\":57286},{\"end\":57306,\"start\":57298},{\"end\":57317,\"start\":57310},{\"end\":57655,\"start\":57648},{\"end\":57665,\"start\":57659},{\"end\":57672,\"start\":57669},{\"end\":57682,\"start\":57676},{\"end\":57692,\"start\":57686},{\"end\":58025,\"start\":58014},{\"end\":58035,\"start\":58029},{\"end\":58268,\"start\":58263},{\"end\":58279,\"start\":58272},{\"end\":58286,\"start\":58283},{\"end\":58508,\"start\":58502},{\"end\":58689,\"start\":58680},{\"end\":58700,\"start\":58693},{\"end\":58708,\"start\":58706},{\"end\":58994,\"start\":58988},{\"end\":59007,\"start\":58998},{\"end\":59263,\"start\":59257},{\"end\":59274,\"start\":59267},{\"end\":59535,\"start\":59532},{\"end\":59550,\"start\":59539},{\"end\":59560,\"start\":59554},{\"end\":59570,\"start\":59564},{\"end\":59904,\"start\":59902},{\"end\":59916,\"start\":59908},{\"end\":59924,\"start\":59920},{\"end\":59932,\"start\":59930},{\"end\":60199,\"start\":60197},{\"end\":60205,\"start\":60203},{\"end\":60214,\"start\":60209},{\"end\":60221,\"start\":60218},{\"end\":60234,\"start\":60225},{\"end\":60250,\"start\":60238},{\"end\":60259,\"start\":60254},{\"end\":60269,\"start\":60263},{\"end\":60275,\"start\":60271},{\"end\":60654,\"start\":60650},{\"end\":60666,\"start\":60658},{\"end\":60676,\"start\":60673},{\"end\":61070,\"start\":61067},{\"end\":61078,\"start\":61074},{\"end\":61086,\"start\":61082}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":40553,\"start\":40156},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11590585},\"end\":41046,\"start\":40555},{\"attributes\":{\"doi\":\"arXiv:1611.01599\",\"id\":\"b2\"},\"end\":41276,\"start\":41048},{\"attributes\":{\"id\":\"b3\"},\"end\":41616,\"start\":41278},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15326394},\"end\":41989,\"start\":41618},{\"attributes\":{\"id\":\"b5\"},\"end\":42362,\"start\":41991},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":28327553},\"end\":42851,\"start\":42364},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16224674},\"end\":43215,\"start\":42853},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":17957882},\"end\":43533,\"start\":43217},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6318891},\"end\":43900,\"start\":43535},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11336213},\"end\":44289,\"start\":43902},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b11\"},\"end\":44572,\"start\":44291},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1662180},\"end\":44887,\"start\":44574},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2741978},\"end\":45258,\"start\":44889},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1297591},\"end\":45441,\"start\":45260},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15739732},\"end\":45719,\"start\":45443},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7779968},\"end\":46129,\"start\":45721},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":57246310},\"end\":46495,\"start\":46131},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5736847},\"end\":46995,\"start\":46497},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2516961},\"end\":47481,\"start\":46997},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14250790},\"end\":47924,\"start\":47483},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3648736},\"end\":48255,\"start\":47926},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5575601},\"end\":48598,\"start\":48257},{\"attributes\":{\"id\":\"b23\"},\"end\":48736,\"start\":48600},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9901844},\"end\":49178,\"start\":48738},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14635907},\"end\":49635,\"start\":49180},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206741496},\"end\":50001,\"start\":49637},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":211481605},\"end\":50379,\"start\":50003},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1915014},\"end\":50547,\"start\":50381},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9359976},\"end\":50873,\"start\":50549},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":12639289},\"end\":51168,\"start\":50875},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6628106},\"end\":51432,\"start\":51170},{\"attributes\":{\"id\":\"b32\"},\"end\":51852,\"start\":51434},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":16996577},\"end\":52159,\"start\":51854},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9923122},\"end\":52574,\"start\":52161},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2348022},\"end\":52945,\"start\":52576},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15272086},\"end\":53263,\"start\":52947},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195908774},\"end\":53605,\"start\":53265},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1779661},\"end\":53741,\"start\":53607},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":64294544},\"end\":53974,\"start\":53743},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1809834},\"end\":54331,\"start\":53976},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":964287},\"end\":54691,\"start\":54333},{\"attributes\":{\"id\":\"b42\"},\"end\":54855,\"start\":54693},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1998416},\"end\":55223,\"start\":54857},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":5959482},\"end\":55560,\"start\":55225},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":17190303},\"end\":56003,\"start\":55562},{\"attributes\":{\"doi\":\"arXiv:1703.01619\",\"id\":\"b46\"},\"end\":56210,\"start\":56005},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206591767},\"end\":56564,\"start\":56212},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":11080756},\"end\":56934,\"start\":56566},{\"attributes\":{\"id\":\"b49\"},\"end\":57166,\"start\":56936},{\"attributes\":{\"id\":\"b50\"},\"end\":57260,\"start\":57168},{\"attributes\":{\"id\":\"b51\"},\"end\":57572,\"start\":57262},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14486898},\"end\":57947,\"start\":57574},{\"attributes\":{\"doi\":\"arXiv:1703.00810\",\"id\":\"b53\"},\"end\":58169,\"start\":57949},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":16726307},\"end\":58471,\"start\":58171},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":146646421},\"end\":58624,\"start\":58473},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":7961699},\"end\":58928,\"start\":58626},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":5541663},\"end\":59183,\"start\":58930},{\"attributes\":{\"id\":\"b58\"},\"end\":59496,\"start\":59185},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":163946},\"end\":59798,\"start\":59498},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b60\"},\"end\":60121,\"start\":59800},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1055111},\"end\":60561,\"start\":60123},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":7152729},\"end\":60961,\"start\":60563},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":22261520},\"end\":61324,\"start\":60963}]", "bib_title": "[{\"end\":40623,\"start\":40555},{\"end\":41688,\"start\":41618},{\"end\":42064,\"start\":41991},{\"end\":42451,\"start\":42364},{\"end\":42920,\"start\":42853},{\"end\":43270,\"start\":43217},{\"end\":43605,\"start\":43535},{\"end\":43977,\"start\":43902},{\"end\":44607,\"start\":44574},{\"end\":44977,\"start\":44889},{\"end\":45285,\"start\":45260},{\"end\":45484,\"start\":45443},{\"end\":45824,\"start\":45721},{\"end\":46182,\"start\":46131},{\"end\":46578,\"start\":46497},{\"end\":47086,\"start\":46997},{\"end\":47570,\"start\":47483},{\"end\":47969,\"start\":47926},{\"end\":48330,\"start\":48257},{\"end\":48843,\"start\":48738},{\"end\":49250,\"start\":49180},{\"end\":49691,\"start\":49637},{\"end\":50052,\"start\":50003},{\"end\":50403,\"start\":50381},{\"end\":50618,\"start\":50549},{\"end\":50914,\"start\":50875},{\"end\":51212,\"start\":51170},{\"end\":51901,\"start\":51854},{\"end\":52264,\"start\":52161},{\"end\":52654,\"start\":52576},{\"end\":53013,\"start\":52947},{\"end\":53328,\"start\":53265},{\"end\":53620,\"start\":53607},{\"end\":53797,\"start\":53743},{\"end\":54033,\"start\":53976},{\"end\":54387,\"start\":54333},{\"end\":54923,\"start\":54857},{\"end\":55285,\"start\":55225},{\"end\":55624,\"start\":55562},{\"end\":56268,\"start\":56212},{\"end\":56628,\"start\":56566},{\"end\":57644,\"start\":57574},{\"end\":58259,\"start\":58171},{\"end\":58496,\"start\":58473},{\"end\":58676,\"start\":58626},{\"end\":58984,\"start\":58930},{\"end\":59253,\"start\":59185},{\"end\":59525,\"start\":59498},{\"end\":60193,\"start\":60123},{\"end\":60643,\"start\":60563},{\"end\":61063,\"start\":60963}]", "bib_author": "[{\"end\":40165,\"start\":40156},{\"end\":40176,\"start\":40165},{\"end\":40186,\"start\":40176},{\"end\":40196,\"start\":40186},{\"end\":40204,\"start\":40196},{\"end\":40213,\"start\":40204},{\"end\":40226,\"start\":40213},{\"end\":40235,\"start\":40226},{\"end\":40243,\"start\":40235},{\"end\":40252,\"start\":40243},{\"end\":40635,\"start\":40625},{\"end\":40646,\"start\":40635},{\"end\":40660,\"start\":40646},{\"end\":40668,\"start\":40660},{\"end\":40678,\"start\":40668},{\"end\":40691,\"start\":40678},{\"end\":40699,\"start\":40691},{\"end\":40714,\"start\":40699},{\"end\":40724,\"start\":40714},{\"end\":40734,\"start\":40724},{\"end\":41060,\"start\":41048},{\"end\":41076,\"start\":41060},{\"end\":41088,\"start\":41076},{\"end\":41102,\"start\":41088},{\"end\":41290,\"start\":41278},{\"end\":41297,\"start\":41290},{\"end\":41307,\"start\":41297},{\"end\":41701,\"start\":41690},{\"end\":41714,\"start\":41701},{\"end\":41728,\"start\":41714},{\"end\":42078,\"start\":42066},{\"end\":42090,\"start\":42078},{\"end\":42100,\"start\":42090},{\"end\":42110,\"start\":42100},{\"end\":42465,\"start\":42453},{\"end\":42481,\"start\":42465},{\"end\":42494,\"start\":42481},{\"end\":42505,\"start\":42494},{\"end\":42516,\"start\":42505},{\"end\":42526,\"start\":42516},{\"end\":42929,\"start\":42922},{\"end\":42938,\"start\":42929},{\"end\":42948,\"start\":42938},{\"end\":42958,\"start\":42948},{\"end\":43280,\"start\":43272},{\"end\":43286,\"start\":43280},{\"end\":43293,\"start\":43286},{\"end\":43618,\"start\":43607},{\"end\":43629,\"start\":43618},{\"end\":43643,\"start\":43629},{\"end\":43656,\"start\":43643},{\"end\":43986,\"start\":43979},{\"end\":44005,\"start\":43986},{\"end\":44017,\"start\":44005},{\"end\":44027,\"start\":44017},{\"end\":44300,\"start\":44291},{\"end\":44312,\"start\":44300},{\"end\":44319,\"start\":44312},{\"end\":44329,\"start\":44319},{\"end\":44620,\"start\":44609},{\"end\":44630,\"start\":44620},{\"end\":44641,\"start\":44630},{\"end\":44654,\"start\":44641},{\"end\":44989,\"start\":44979},{\"end\":44999,\"start\":44989},{\"end\":45297,\"start\":45287},{\"end\":45305,\"start\":45297},{\"end\":45315,\"start\":45305},{\"end\":45496,\"start\":45486},{\"end\":45506,\"start\":45496},{\"end\":45518,\"start\":45506},{\"end\":45528,\"start\":45518},{\"end\":45833,\"start\":45826},{\"end\":45840,\"start\":45833},{\"end\":45849,\"start\":45840},{\"end\":46192,\"start\":46184},{\"end\":46200,\"start\":46192},{\"end\":46210,\"start\":46200},{\"end\":46219,\"start\":46210},{\"end\":46225,\"start\":46219},{\"end\":46236,\"start\":46225},{\"end\":46591,\"start\":46580},{\"end\":46604,\"start\":46591},{\"end\":46618,\"start\":46604},{\"end\":46630,\"start\":46618},{\"end\":46645,\"start\":46630},{\"end\":46655,\"start\":46645},{\"end\":46666,\"start\":46655},{\"end\":47099,\"start\":47088},{\"end\":47110,\"start\":47099},{\"end\":47120,\"start\":47110},{\"end\":47130,\"start\":47120},{\"end\":47139,\"start\":47130},{\"end\":47151,\"start\":47139},{\"end\":47158,\"start\":47151},{\"end\":47583,\"start\":47572},{\"end\":47594,\"start\":47583},{\"end\":47604,\"start\":47594},{\"end\":47617,\"start\":47604},{\"end\":47624,\"start\":47617},{\"end\":47982,\"start\":47971},{\"end\":47990,\"start\":47982},{\"end\":48002,\"start\":47990},{\"end\":48012,\"start\":48002},{\"end\":48025,\"start\":48012},{\"end\":48342,\"start\":48332},{\"end\":48352,\"start\":48342},{\"end\":48629,\"start\":48615},{\"end\":48639,\"start\":48629},{\"end\":48652,\"start\":48639},{\"end\":48855,\"start\":48845},{\"end\":48868,\"start\":48855},{\"end\":48877,\"start\":48868},{\"end\":48892,\"start\":48877},{\"end\":49262,\"start\":49252},{\"end\":49273,\"start\":49262},{\"end\":49286,\"start\":49273},{\"end\":49299,\"start\":49286},{\"end\":49308,\"start\":49299},{\"end\":49323,\"start\":49308},{\"end\":49703,\"start\":49693},{\"end\":49714,\"start\":49703},{\"end\":49724,\"start\":49714},{\"end\":50063,\"start\":50054},{\"end\":50072,\"start\":50063},{\"end\":50082,\"start\":50072},{\"end\":50093,\"start\":50082},{\"end\":50419,\"start\":50405},{\"end\":50434,\"start\":50419},{\"end\":50632,\"start\":50620},{\"end\":50643,\"start\":50632},{\"end\":50656,\"start\":50643},{\"end\":50932,\"start\":50916},{\"end\":50943,\"start\":50932},{\"end\":51226,\"start\":51214},{\"end\":51232,\"start\":51226},{\"end\":51444,\"start\":51434},{\"end\":51455,\"start\":51444},{\"end\":51462,\"start\":51455},{\"end\":51913,\"start\":51903},{\"end\":51920,\"start\":51913},{\"end\":51930,\"start\":51920},{\"end\":52276,\"start\":52266},{\"end\":52283,\"start\":52276},{\"end\":52293,\"start\":52283},{\"end\":52666,\"start\":52656},{\"end\":52678,\"start\":52666},{\"end\":52685,\"start\":52678},{\"end\":53025,\"start\":53015},{\"end\":53037,\"start\":53025},{\"end\":53044,\"start\":53037},{\"end\":53054,\"start\":53044},{\"end\":53344,\"start\":53330},{\"end\":53357,\"start\":53344},{\"end\":53369,\"start\":53357},{\"end\":53631,\"start\":53622},{\"end\":53641,\"start\":53631},{\"end\":53651,\"start\":53641},{\"end\":53808,\"start\":53799},{\"end\":53818,\"start\":53808},{\"end\":53828,\"start\":53818},{\"end\":53839,\"start\":53828},{\"end\":54041,\"start\":54035},{\"end\":54050,\"start\":54041},{\"end\":54058,\"start\":54050},{\"end\":54069,\"start\":54058},{\"end\":54400,\"start\":54389},{\"end\":54752,\"start\":54740},{\"end\":54762,\"start\":54752},{\"end\":54770,\"start\":54762},{\"end\":54937,\"start\":54925},{\"end\":54945,\"start\":54937},{\"end\":54958,\"start\":54945},{\"end\":55298,\"start\":55287},{\"end\":55306,\"start\":55298},{\"end\":55317,\"start\":55306},{\"end\":55325,\"start\":55317},{\"end\":55639,\"start\":55626},{\"end\":55649,\"start\":55639},{\"end\":55658,\"start\":55649},{\"end\":55671,\"start\":55658},{\"end\":55683,\"start\":55671},{\"end\":56087,\"start\":56077},{\"end\":56280,\"start\":56270},{\"end\":56290,\"start\":56280},{\"end\":56302,\"start\":56290},{\"end\":56312,\"start\":56302},{\"end\":56642,\"start\":56630},{\"end\":56652,\"start\":56642},{\"end\":56660,\"start\":56652},{\"end\":56670,\"start\":56660},{\"end\":57023,\"start\":57012},{\"end\":57034,\"start\":57023},{\"end\":57047,\"start\":57034},{\"end\":57274,\"start\":57262},{\"end\":57284,\"start\":57274},{\"end\":57296,\"start\":57284},{\"end\":57308,\"start\":57296},{\"end\":57319,\"start\":57308},{\"end\":57657,\"start\":57646},{\"end\":57667,\"start\":57657},{\"end\":57674,\"start\":57667},{\"end\":57684,\"start\":57674},{\"end\":57694,\"start\":57684},{\"end\":58027,\"start\":58012},{\"end\":58037,\"start\":58027},{\"end\":58270,\"start\":58261},{\"end\":58281,\"start\":58270},{\"end\":58288,\"start\":58281},{\"end\":58510,\"start\":58498},{\"end\":58691,\"start\":58678},{\"end\":58702,\"start\":58691},{\"end\":58710,\"start\":58702},{\"end\":58996,\"start\":58986},{\"end\":59009,\"start\":58996},{\"end\":59265,\"start\":59255},{\"end\":59276,\"start\":59265},{\"end\":59537,\"start\":59527},{\"end\":59552,\"start\":59537},{\"end\":59562,\"start\":59552},{\"end\":59572,\"start\":59562},{\"end\":59906,\"start\":59900},{\"end\":59918,\"start\":59906},{\"end\":59926,\"start\":59918},{\"end\":59934,\"start\":59926},{\"end\":60201,\"start\":60195},{\"end\":60207,\"start\":60201},{\"end\":60216,\"start\":60207},{\"end\":60223,\"start\":60216},{\"end\":60236,\"start\":60223},{\"end\":60252,\"start\":60236},{\"end\":60261,\"start\":60252},{\"end\":60271,\"start\":60261},{\"end\":60277,\"start\":60271},{\"end\":60656,\"start\":60645},{\"end\":60668,\"start\":60656},{\"end\":60678,\"start\":60668},{\"end\":61072,\"start\":61065},{\"end\":61080,\"start\":61072},{\"end\":61088,\"start\":61080}]", "bib_venue": "[{\"end\":40333,\"start\":40268},{\"end\":40785,\"start\":40734},{\"end\":41151,\"start\":41118},{\"end\":41437,\"start\":41307},{\"end\":41793,\"start\":41728},{\"end\":42164,\"start\":42110},{\"end\":42594,\"start\":42526},{\"end\":43022,\"start\":42958},{\"end\":43364,\"start\":43293},{\"end\":43703,\"start\":43656},{\"end\":44085,\"start\":44027},{\"end\":44420,\"start\":44344},{\"end\":44719,\"start\":44654},{\"end\":45064,\"start\":44999},{\"end\":45340,\"start\":45315},{\"end\":45564,\"start\":45528},{\"end\":45913,\"start\":45849},{\"end\":46301,\"start\":46236},{\"end\":46731,\"start\":46666},{\"end\":47226,\"start\":47158},{\"end\":47692,\"start\":47624},{\"end\":48080,\"start\":48025},{\"end\":48418,\"start\":48352},{\"end\":48613,\"start\":48600},{\"end\":48947,\"start\":48892},{\"end\":49393,\"start\":49323},{\"end\":49805,\"start\":49724},{\"end\":50180,\"start\":50093},{\"end\":50452,\"start\":50434},{\"end\":50701,\"start\":50656},{\"end\":51013,\"start\":50943},{\"end\":51291,\"start\":51232},{\"end\":51630,\"start\":51462},{\"end\":51996,\"start\":51930},{\"end\":52358,\"start\":52293},{\"end\":52749,\"start\":52685},{\"end\":53094,\"start\":53054},{\"end\":53425,\"start\":53369},{\"end\":53657,\"start\":53651},{\"end\":53843,\"start\":53839},{\"end\":54143,\"start\":54069},{\"end\":54505,\"start\":54400},{\"end\":54738,\"start\":54693},{\"end\":55028,\"start\":54958},{\"end\":55382,\"start\":55325},{\"end\":55770,\"start\":55683},{\"end\":56075,\"start\":56005},{\"end\":56377,\"start\":56312},{\"end\":56739,\"start\":56670},{\"end\":57010,\"start\":56936},{\"end\":57208,\"start\":57168},{\"end\":57406,\"start\":57319},{\"end\":57747,\"start\":57694},{\"end\":58010,\"start\":57949},{\"end\":58307,\"start\":58288},{\"end\":58539,\"start\":58510},{\"end\":58766,\"start\":58710},{\"end\":59047,\"start\":59009},{\"end\":59331,\"start\":59276},{\"end\":59637,\"start\":59572},{\"end\":59898,\"start\":59800},{\"end\":60328,\"start\":60277},{\"end\":60748,\"start\":60678},{\"end\":61133,\"start\":61088}]"}}}, "year": 2023, "month": 12, "day": 17}