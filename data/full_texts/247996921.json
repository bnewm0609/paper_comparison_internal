{"id": 247996921, "updated": "2023-10-05 15:29:46.215", "metadata": {"title": "Gait Recognition in the Wild with Dense 3D Representations and A Benchmark", "authors": "[{\"first\":\"Jinkai\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Xinchen\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Wu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Lingxiao\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Chenggang\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Mei\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Existing studies for gait recognition are dominated by 2D representations like the silhouette or skeleton of the human body in constrained scenes. However, humans live and walk in the unconstrained 3D space, so projecting the 3D human body onto the 2D plane will discard a lot of crucial information like the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper aims to explore dense 3D representations for gait recognition in the wild, which is a practical yet neglected problem. In particular, we propose a novel framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the human body for gait recognition, named SMPLGait. Our framework has two elaborately-designed branches of which one extracts appearance features from silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D SMPL model. In addition, due to the lack of suitable datasets, we build the first large-scale 3D representation-based gait recognition dataset, named Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL models recovered from video frames which can provide dense 3D information of body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively compare our method with existing gait recognition approaches, which reflects the superior performance of our framework and the potential of 3D representations for gait recognition in the wild. The code and dataset are available at https://gait3d.github.io.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.02569", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhengLLH0022", "doi": "10.1109/cvpr52688.2022.01959"}}, "content": {"source": {"pdf_hash": "8a33556a6c89087904ff9ed53c3c6c6a08fcc2dd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.02569v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "333f1b882de9baf483d9fe7d76dcef288e3252cc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8a33556a6c89087904ff9ed53c3c6c6a08fcc2dd.txt", "contents": "\nGait Recognition in the Wild with Dense 3D Representations and A Benchmark\n\n\nJinkai Zheng \nHangzhou Dianzi University\nHangzhouChina\n\nXinchen Liu \nExplore Academy of JD.com\nBeijingChina\n\n\u2020 Wu Liu \nExplore Academy of JD.com\nBeijingChina\n\n\u2020 Lingxiao He \nExplore Academy of JD.com\nBeijingChina\n\nChenggang Yan cgyan@hdu.edu.cn \nHangzhou Dianzi University\nHangzhouChina\n\nTao Mei tmei@jd.com \nExplore Academy of JD.com\nBeijingChina\n\nGait Recognition in the Wild with Dense 3D Representations and A Benchmark\n\nExisting studies for gait recognition are dominated by 2D representations like the silhouette or skeleton of the human body in constrained scenes. However, humans live and walk in the unconstrained 3D space, so projecting the 3D human body onto the 2D plane will discard a lot of crucial information like the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper aims to explore dense 3D representations for gait recognition in the wild, which is a practical yet neglected problem. In particular, we propose a novel framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the human body for gait recognition, named SMPLGait. Our framework has two elaborately-designed branches of which one extracts appearance features from silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D SMPL model. In addition, due to the lack of suitable datasets, we build the first large-scale 3D representationbased gait recognition dataset, named Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL models recovered from video frames which can provide dense 3D information of body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively compare our method with existing gait recognition approaches, which reflects the superior performance of our framework and the potential of 3D representations for gait recognition in the wild. The code and dataset are available at https://gait3d.github.io.\n\nIntroduction\n\nVisual gait recognition, which aims to identify a target person using her/his walking pattern in a video, has been studied for over two decades [30,42]. Existing approaches and datasets are dominated by 2D gait representations such as silhouette sequences [56], Gait Energy Images * This work was done when Jinkai Zheng was an intern at Explore Academy of JD.com. \u2020 Corresponding author.  [63], as shown in Figure 1. However, the human body is a 3D non-rigid object, so the 3D-to-2D projection discards a lot of useful information about shapes, viewpoints, and dynamics while presenting ambiguity for gait recognition. Therefore, this paper is focused on 3D gait recognition which is valuable yet neglected by the community.\n\nRecently, deep learning-based methods have dominated the state-of-the-art performance on the widely adopted 2D gait recognition benchmarks like CASIA-B [37] and OU-MVLP [36] by directly learning discriminative features from silhouette sequences [5,9,57] or GEIs [46]. Despite the excellent results on the in-the-lab datasets, these methods cannot work well in the wild scenarios which have more diverse 3D viewpoints of cameras and more complex environmental interference factors like occlusions [63]. Although several works exploit 3D cylin-ders [3] or 3D skeletons [41], these sparse 3D models also lose helpful information of human bodies like viewpoints and shapes. Fortunately, the development of parameterized human body models like the Skinned Multi-Person Linear (SMPL) model [28] and 3D human mesh recovery approaches [18,20,34] makes it possible to estimate precise 3D meshes and viewpoints of human bodies in video frames. The advantages of 3D meshes for gait recognition are two-fold: 1) the 3D mesh can provide not only the pose but also the shape of the human body in the 3D space, which is crucial for learning discriminative features of gait, and 2) the 3D viewpoint can be explored to normalize the orientations of human bodies during cross-view matching.\n\nTo this end, we design a novel 3D SMPL model-based Gait recognition framework, i.e., SMPLGait, to explore the 3D gait representations for human identification. Our SMPLGait framework has two branches based on deep neural networks. One branch takes the silhouette sequence of a person as the input to learn appearance features like clothing, hairstyle, and belongings. However, due to the extreme viewpoint changes in the wild, the shape of the human body can be distorted, which makes the appearance ambiguous, as shown in Figure 1. To overcome this challenge, we design a 3D Spatial-Transformation Network (3D-STN) as the other branch to learn 3D knowledge of viewpoint and shape from the 3D human mesh. The 3D-STN takes the 3D SMPL model of each frame as the input to learn a spatial transformation matrix. By applying the spatial transformation matrix to the appearance features, these features from different viewpoints are normalized in the latent space. By this means, the gait sequences of the same person will be closer in the feature space.\n\nNevertheless, there is no suitable dataset that provides 3D meshes of human bodies in the wild. Therefore, to facilitate the research, we build the first large-scale 3D mesh-based gait recognition dataset, named Gait3D, from high-resolution videos captured in the wild. Compared to existing datasets listed in Table 1, the Gait3D dataset has the following featured properties: 1) Gait3D contains 4,000 subjects with over 25,000 sequences captured by 39 cameras in an unconstrained indoor scene which makes it scalable for research and applications.\n\n2) It provides precise 3D human meshes recovered from video frames which can provide 3D pose and shape of human bodies as well as accurate viewpoint parameters. 3) It also provides conventional 2D silhouettes and keypoints which can be explored for gait recognition with multi-modal data.\n\nIn summary, the contributions of this paper are as follows:\n\n\u2022 We make one of the first attempts toward 3D gait recognition in the real-world scenario, which aims to explore dense 3D representations of the human body for gait recognition.\n\n\u2022 We propose a novel 3D gait recognition framework based on the SMPL model, named SMPLGait, to explore 3D human meshes for gait recognition.\n\n\u2022 We build the first large-scale 3D gait recognition dataset, named Gait3D, which provides the 3D human meshes of gait collected from unconstrained scenarios.\n\nThrough comprehensive experiments, we not only evaluate existing 2D silhouettes/skeleton-based approaches but also demonstrate the effectiveness of the proposed SMPLGait method, which reflects the potential of 3D representations for gait recognition. Moreover, the combination of 3D and 2D representations further improves the performance which shows the complementarity of multi-modal representations.\n\n\nRelated Work\n\nGait Recognition. We review the 2D and 3D representations-based gait recognition methods separately.\n\n2D gait recognition methods can be classified into model-based and model-free approaches [42].\n\nEarly methods mainly belong to the model-based which defines a structural human body model. Then, gait patterns are modeled by parameters like lengths of limbs, angles of joints, and relative positions of body parts [3,48]. The model-free methods mainly adopt the silhouettes obtained by background subtraction from video frames [5,9,11,15,16,22,32,46,57,58]. In particular, Han et al. proposed to aggregate a sequence of silhouettes into a compact Gait Energy Image (GEI) [11] which was widely used by the following methods [32,46]. Recently, due to the success of deep learning for computer vision tasks [7, 24-26, 45, 50-54], deep Convolutional Neural Networks (CNNs) also dominated the performance of gait recognition. For example, Shiraga et al. [32] and Wu et al. [46] proposed to learn effective features from GEIs and significantly outperformed previous methods. The most recent methods started to learn discriminative features directly from the silhouette sequences using larger CNNs or multi-scale structures and achieved state-of-the-art results [5,9,16,22]. Despite the excellent performance on in-the-lab datasets, e.g., CASIA-B and OU-LP, these methods usually fail in the wild as shown in the experiments on GREW [63] and our Gait3D.\n\n3D representations have also been studied since the early years of gait recognition. For example, Urtasun and Fua [41] proposed an approach to gait analysis that depended on 3D temporal motion models using an articulated skeleton. Zhao et al. [59] applied a local optimization algorithm to track 3D motion for gait recognition. Yamauchi et al. [49] proposed the first method using 3D pose estimated from RGB frames for walking human recognition. Ariyanto and Nixon [3] built a 3D voxel-based dataset using a complex multi-camera system and proposed a structural model of articulated cylinders with 3D Degrees of Freedom at each joint to model the human lower legs. However, these methods either discard rich 3D information like viewpoints and shapes or are limited by devices for real-world applications. In summary, to overcome the problem of 2D methods and explore 3D representations for gait recognition in the wild, we aim to explore 3D mesh as a rich representation of the viewpoint and shape of the human body. Gait Recognition Datasets. Current publicly available gait recognition datasets mainly belong to two series, i.e., the CASIA series [37,44,56] and the OU-ISIR series [1,14,17,29,39,40,47] as listed in Table 1. The CASIA series were built in the early research of gait recognition, which facilitated the initial exploration of RGB images and silhouettes for gait representations [11,37]. Despite its smaller number of subjects, the CASIA-B [56] is still the most widely used dataset for the evaluation of silhouettebased methods. The OU-ISIR series were first built ten years ago and developed comprehensive variants such as walking at different speeds [39], clothing styles [14], and bags [40], subjects of different ages [47], and annotations of 2D pose [1]. Due to their large population, the OU-LP [17] and OU-MVLP [36] also became the most popular datasets for current research. However, the above datasets were collected in constrained scenes like labs [17,56] or a small defined area on a campus [31,44]. Most recently, researchers started to narrow the gap between in-the-lab research and real-world application. As a contemporaneous study of our work, Zhu et al. [63] constructed the GREW dataset from natural videos collected in an open area. However, there is no dataset that provides rich 3D representations for gait recognition in the wild. Therefore, we need to build a new dataset that is collected from complex scenes and with dense 3D meshes for gait recognition in the wild.\n\n3D Human Mesh Recovery. 3D representations have attracted a lot of attention in the computer vision community [27,61]. The 3D human body can be represented by point clouds [23], voxels [41], parameterized blend shapes [2], etc. Among them, the Skinned Multi-Person Linear (SMPL) model [28] is a skinned vertex-based model that can accurately represent a wide variety of body shapes in natural human poses. With the SMPL model, an arbitrary 3D human body can be represented by a linear combination of a group of shape, pose, scale, and viewpoint parameters. Based on the SMPL model, a series of 3D human mesh recovery approaches are developed to estimate accurate 3D shapes, poses, and viewpoints of human bodies from natural images [18,20,34,35]. These methods provide us an opportunity to obtain 3D human meshes from in-the-wild videos for 3D mesh-based gait recognition.\n\n\nThe 3D Gait Recognition Method\n\n\nOverview\n\nThe overall architecture of the proposed 3D SMPLbased Gait Recognition framework, SMPLGait, is shown in Figure 2. There are two branches of the framework. For the first branch, we take the sequence of silhouettes as input which has a rich knowledge of the appearance and use a CNN-based model to extract 2D spatial features from each frame. For the second branch, the SMPLs of the human body are fed into the 3D-Spatial-Transformation Network (3D-STN), which aims to learn the latent transformation matrixes from the 3D viewpoints and shapes. Then the 3D Spatial Transformation Module aligns the 2D appearance features in the latent space using the learned transformation matrixes. Finally, the transformed feature of each frame is aggregated into a sequence-level feature for sequence-tosequence matching in training or inference. Next, we will introduce the above modules in detail.\n\n\nNetwork Structure\n\nThe Silhouette Learning Network (SLN) aims to learn the appearance knowledge of humans from silhouettes that contain 2D spatial information like clothing and hairstyle. The SLN has six convolutional layers which are similar to the backbone of GaitSet [5]. As is shown in Figure 2, the sequences of silhouettes are fed into a CNN. We formulate is the i-th binary frame, L is the length of the sequence, H and W are the height and width of the silhouette image. For a frame x i , the process can be formulated as:\nX sil = {x i } L i=1 as the input sequence, where x i \u2208 R H\u00d7WF i = F (x i ),(1)\nwhere F (\u00b7) is the CNN-based backbone and F i \u2208 R h\u00d7w is the frame-level feature map for frame x i 1 . The 3D Spatial Transformation Network (3D-STN) is proposed to solve viewpoint changes in real 3D scenarios. 3D SMPL parameters related to 3D viewpoints, shapes, and poses are the input of this module. Assuming\nY sp = {y i } L i=1\nis the input SMPLs, where y i \u2208 R D is the SMPL vector of i-th frame, D is the dimension of the SMPL vector which contains 24\u00d73 dimensions of 3D human body pose, 10 dimensions of 3D body shape, and 3 dimensions of camera scale and translation parameters. The 3D-STN consists of three fully connected (FC) layers with neuron number = 128 \u21d2 256 \u21d2 h \u00d7 w, where h and w are the height and width of the feature map from the Silhouette Learning Network. Each FC layer is followed by batch normalization and the ReLU activation function. We use dropout for the last two FC layers to eliminate overfitting. The forward process of 3D-STN can be formulated as:\ng i = G(y i ),(2)\nwhere G(\u00b7) is the 3D-STN and g i is the frame-level transformation vector for frame i. The 3D Spatial Transformation Module is designed to align the 2D appearance feature map F i \u2208 R h\u00d7w using the transformation vector g i in the feature space, as shown in Figure 2. We first reshape the transformation vector g i to a matrix G i \u2208 R w\u00d7h . Then, for convenience of computation, we expand F i and G i to square matrixes by zero padding on the short edge. After that, we apply G i to F i by\nF i = F i \u00b7 (I + G i ),(3)\nwhere I is an identity matrix and \u00b7 is matrix multiplication. At last, we adopt Set Pooling (SP) and Horizontal Pyramid Pooling (HPP) in GaitSet [5] to aggregate F i into the final feature vector for sequence-to-sequence matching. For more details of the SMPLGait framework, please refer to the supplementary material.\n\n\nTraining and Inference\n\nOur two-branch 3D gait recognition framework is trained in an end-to-end manner. The network of our framework is optimized by a loss function with two components:\nL = \u03b1L tri + \u03b2L ce ,(4)\nwhere L tri is the triplet loss, L ce is the cross entropy loss. \u03b1 and \u03b2 are the weighting parameters. During inference, we use the sequences of silhouettes and SMPLs as the inputs of the two branches, respectively. The cosine similarity is used to measure the similarity between a query-gallery pair.\n\n\nThe Gait3D Benchmark\n\nTo facilitate the research of 3D gait recognition, we present a novel large-scale dataset, named Gait3D, which has several featured properties compared to existing datasets in Table 1. First of all, the Gait3D dataset consists of 4,000 subjects, 25,000+ sequences, and over 3 million bounding boxes captured by cameras of arbitrary 3D viewpoints, which makes it more scalable for training deep CNNs. Moreover, it provides accurate 3D human meshes estimated from video frames, which contains the poses and shapes of human bodies as well as viewpoints in the 3D space. Furthermore, Gait3D also provides 2D silhouettes and 2D/3D keypoints obtained by the state-of-the-art image segmentation and pose estimation methods fine-tuned on our dataset. Therefore, multi-modal data can be explored for gait recognition. In addition, Gait3D is collected in a large supermarket in which people usually walk at irregular speeds and routes, and can be occluded by other people or objects. The above properties also make Gait3D a scalable but challenging dataset for gait recognition which can be reflected by the evaluation in Section 5.\n\n\nData Collection and Pre-processing\n\nTo collect a high-quality in-the-wild dataset for real applications, we collect the seven-day raw videos from 39 cameras mounted in a large supermarket. The scenes of the cameras include the entrance, the goods shelf area, the freezer area, the dining area, the checkout counter, etc. For the videos each day, we randomly sample two segments of continuous two-hour videos. At last, we obtain about 1,090 hours videos with 1,920 \u00d7 1,080 resolution and 25 FPS. Note that, we are authorized by the management of the supermarket to access and process the data for research purposes. In addition, all subjects were noticed that the data is collected only for research purposes. With the videos, we use the open-source FFmpeg 2 to decode the raw videos into frames at 25 FPS to keep the continuity of gait sequences. To guarantee the high quality of the dataset, the annotation process is performed in three main steps as follows.\n\n\nDataset Construction\n\n\nPerson detection and tracking from frames\n\nFor each frame extracted from the raw videos, we adopt the CenterNet [62] fine-tuned on our dataset as the person detector since it is an efficient anchor-free object detector 3 . To achieve accurate person tracking in videos, we exploit the Intersection-over-Union (IoU) and person re-identification (ReID) features of bounding boxes in two adjacent frames to measure their similarity. The ReID feature is extracted by an open-source person ReID framework, FastReID 4 [12] pretrained on several public person ReID datasets. When two persons are highly overlapped, the tracking algorithm can easily misjudge them as one person, i.e., ID switching. To solve this problem, we employ human annotators to clean sequences that may contain more than one pedestrian. By this means, we guarantee that each sequence only belongs to one person. Then, we discard the sequences shorter than 25 frames or longer than 500 frames and obtain about 50,000 sequences in total.\n\n\nCross-camera sequence matching\n\nWith the above sequences, we should cluster the sequences of the same person in all cameras. To achieve effective and efficient cross-camera matching of the same person, we also utilize the person ReID features obtained by FastReID [12]. For each sequence, we first use a pose estimation model, i.e., HRNet 5 [43] fine-tuned on our dataset 6 , to select a high-quality frame for cross-camera matching. After that, we utilize FastReID to extract the features of the selected frames of all sequences. Through an unsupervised clustering method, i.e., DBSCAN [8], we roughly obtain 5,336 clusters of sequences. Then, we employ human annotators to filter out the outlier sequences in each group. By discarding the groups containing only one sequence, we finally obtain 4,000 subjects and 25,309 sequences for generating the gait representations.\n\n\nGeneration of gait representations\n\nWith the clean sequences of 4,000 IDs, we generate the 3D SMPL parameter, 3D mesh, 3D pose, 2D silhouette, and 2D pose for each frame. For the 3D SMPL, 3D mesh, and 3D pose, we exploit a state-of-the-art 3D human mesh recovery method, ROMP 7 [34], since it can efficiently output these three representations in an end-to-end framework. For the 2D silhouette, we use the semantic segmentation method, HRNet-segmentation 8 [43], to obtain the silhouette of the person in each frame. For the 2D pose, we also utilize the HRNet to estimate the 2D keypoints of the person in each frame. We keep the original resolution and aspect ratio of the frame without resizing or normalization. Some examples of gait representations in our dataset are shown in Figure 3. It is worth noting that we will only release the generated gait representations but not release any RGB frames to protect the privacy of the subjects.    \n\n\nDataset Statistics and Evaluation Protocol\n\nThe statistics about the sizes of frames, ID numbers over sequence numbers, and sequence numbers over sequence lengths are shown in Figure 4. From Figure 4 (a), we can find that most frames range from 100 \u223c 400 \u00d7 200 \u223c 800 which are larger than person bounding boxes of existing datasets. Figure 4 (b) shows that most IDs have 2 \u223c 25 sequences, which guarantees the high reappearance times of subjects. Figure 4 (c) reflects that most sequences are longer than 50 frames (2 seconds) and the longest sequence has 500 frames, which reflects the complexity of the gait sequences in the unconstrained scenes. The above statistics demonstrate that the Gait3D dataset is scalable but challenging for gait recognition research.\n\nTo facilitate the research, we split the 4,000 IDs of the Gait3D dataset into the train/test subsets with 3,000/1,000 IDs, respectively. For the test set, we further randomly select one sequence from each ID to build the query set with 1,000 sequences, while the rest of the sequences become the gallery set with 5,369 sequences. Our evaluation protocol is based on the open-set instance retrieval setting like existing gait recognition datasets [17] and the person ReID task [60]. Given a query sequence, we measure its similarity between all sequences in the gallery set. Then a ranking list of the gallery set is returned by the descending order of the similarities. We report the average Rank-1 and Rank-5 identification rates over all query sequences. We also adopt the mean Average Precision (mAP) and mean Inverse Negative Penalty (mINP) [55] which consider the recall of multiple instances and hard samples.\n\n\nExperiments\n\nIn the experiments, we first evaluate several State-Of-The-Art (SOTA) 2D gait recognition methods and our SMPLGait on the Gait3D dataset. Then, we analyze the influence of the frame size, the sequence length, and the scale of training IDs on the performance of gait recognition.\n\n\nEvaluation of Existing Methods\n\nHere, we evaluate eight SOTA 2D gait recognition methods including six model-free methods and two model-based methods. We also compare our 3D gait recognition method (SMPLGait) with these methods.\n\n\nModel-free Approaches\n\nThe details of model-free approaches are as follows:\n\n1) GEINet [32] is one of the first methods that adopts a four-layer CNN to learn gait features from GEIs using the cross-entropy loss.\n\n2) GaitSet [5] is the representative method that utilizes a 10-layer CNN to directly learn discriminative gait features from silhouette sequences. The GaitSet is trained by the batch all triplet loss [13].\n\n3) GaitPart [9] adopts the idea of multi-scale feature learning. It horizontally divides a silhouette image into fixed parts to learn discriminative micro-motion features. 4) GLN [15] is an efficient and effective method to learn compact features from gait sequences, which achieves the SOTA performance using only a 256-D feature. 5) GaitGL [22] is also a CNN-based framework to learn both global and local features from gait sequences. 6) CSTL [16] applies multi-scale learning on the temporal dimension of the sequence to learn both long-term and short-term motion for gait recognition.\n\nImplementation Details: During training, we train the above models except GLN with the same configuration. The batch size is 32 \u00d7 4 \u00d7 30, where 32 denotes the number of IDs, 4 denotes the number of training samples per ID, and 30 is the sequence length. The models are trained for 1,200 epochs with the initial Learning Rate (LR)=1e-3 and the LR is multiplied by 0.1 at the 200-th and 600-th epochs. The optimizer is Adam [19] and the weight decay is set to 5e-4. For GLN, we follow the two-stage training as in [15]. The model trained in the first stage is used as the pretrained model for the second stage. Both of the two stages are trained with the same configuration of other methods. During testing, we use the cosine similarity to measure the similarity between each pair of query and gallery sequences. For the GaitSet, GaitPart, GLN, and GaitGL models, we adopt the implementations in the open-source OpenGait toolbox 9 since they outperform the original codes.\n\n\nModel-based Approaches\n\nWe compare two representative model-based methods which use 2D or 3D skeletons as the input. 1) PoseGait [21] first exploits OpenPose [4] to extract the 2D keypoints from RGB frames, then uses the method in [6] to estimate the 3D keypoints of human bodies. Based on the 3D skeletons, it defines several parameters such as joint angle, limb length, and joint motion together with the pose features as the gait representation. In our implementation, we train it for 700 epochs with a batch size of 128. The LR is set to 1e-3. The optimizer is Adam [19] and weight decay is equal to 5e-4.\n\n2) GaitGraph [38] is a recent model-based gait recognition method. It models the 2D skeleton as a graph and adopts a Graph Convolution Network, i.e., the Res-GCN [33], to learn features by the contrastive loss. We train GaitGraph in two stages. The setting of the first stage is the same as PoseGait, and the model trained in the first stage is used as the pre-trained model of the second stage. In the second stage, we fine-tune it for 250 epochs.\n\n\nImplementation Details of the SMPLGait\n\nFor our SMPLGait, we use the loss in Equ. 4 for training. In 3D-STN, we set the dropout rate to 0.2 for FC layers. The hyper-parameters in Equ. 4 are set as \u03b1=1.0 and \u03b2=0.1.\n\nOther settings are the same as those in Section 5.1.1.\n\n\nExperimental Results\n\nThe results of model-free methods, model-based methods, and our SMPLGait are listed in Table 2.\n\nFor model-free methods, we can first observe that the overall performance of the SOTA methods is much worse 9 https://github.com/ShiqiYu/OpenGait than their performance on in-the-lab datasets like the CASIA-B [56] and OU-ISIR series [17,36]. This reflects that there is a huge gap between the in-the-lab research and the in-the-wild application that is much more challenging. Meanwhile, the performance of the SOTA model-free methods varies significantly. For example, the GEI-based method, i.e., GEINet obtains the worst results, which indicates that the GEIs discard too much useful information for gait recognition. Moreover, the methods considering the order of the frames in sequences, i.e., GaitPart, GLN, GaitGL, and CSTL, obtain lower accuracy. It means that the temporal information in the wild scene is hard to learn, because people may stop then continue to walk at varying speeds and routes in unconstrained scenarios. On the contrary, the methods considering frames as an unordered set, i.e., GaitSet, obtain better results.\n\nFor model-based methods, we can find that they are greatly worse than model-free methods on the Gait3D dataset. This is because the input of the model-based methods only has a few sparse human body joints, which seriously lacks useful gait information, such as body shape, appearance, and so on. In addition, the walking speed and route are uncertain in real scenarios, which also greatly affects the performance of the model-based methods that aim to model the temporal dynamics of the human body.\n\nFinally, our SMPLGait outperforms other methods by a large margin, which indicates the potential of 3D representations for gait recognition in the wild.\n\n\nAblation Study of SMPLGait\n\nWe also conduct an ablation study on the key components in SMPLGait by removing the 3D branch (SMPLGait w/o 3D). 10 The results are listed in Table 2. This comparison shows that the integration of 2D and 3D representations can better address the challenges of gait recognition in the wild. 19 Figure 5. The effect of frame numbers in sequences.\n\n\nMore Analysis of the Gait3D Dataset\n\nWe choose two SOTA gait recognition methods, i.e., GaitPart and GaitSet, and our SMPLGait (Ours) to analyze the influence of input size, frame number in sequences, and training ID number on the accuracy. All the models are evaluated on the whole Gait3D test set.\n\nInput Size. We explore two input sizes of 88 \u00d7 128 and 44 \u00d7 64 for the compared methods, as shown in Table 2.\n\nFrom the results, we can observe that the performance of almost all methods is improved with larger input size. There is an exception, i.e., GaitGL, which obtain worse accuracy with larger input size. This may be because that GaitGL adopts the 3D CNN as the backbone. When using a larger input size, the 3D CNN learns more misalignment information about frames in physical space, which makes it more difficult to be optimized.\n\nNumber of Training Frames We randomly sample 10\u223c 50 frames from original gait sequences during training. The Rank-1 accuracy is illustrated in Figure 5. The results show that as the number of frames increases the performance first increases and then decreases, while the best performance occurs around 30 frames per sequence. This indicates that more frames could not bring higher accuracy. The reason may be that there is a lot of redundant or noisy information caused by uncertain speeds and routes of persons, which will bring ambiguous features for gait recognition.\n\nScale of Training IDs We fix other settings and use 0.5K \u223c 3KIDs with an increment of 0.5K for training. As shown in Figure 6, the performance of the models grows stably with more training IDs. These results reflect the scalability of our Gait3D dataset.\n\nMore experiments and exemplar results on Gait3D can be found in the supplementary material.\n\n\nDiscussion\n\nEthical Issues. There are two main ethical issues of this paper: 1) privacy, and 2) data bias. For the first issue, we will try our best to protect the privacy of the subjects involved in our dataset. Firstly, we will not release any human cognizable data like original videos, RGB frames, 16  and bounding boxes of persons. Second, the dataset will be distributed only for research purposes via the case-by-case application with a strict license. To eliminate data bias, the genders and ages of subjects are relatively balanced. Future Work. Despite the proposed baseline method for 3D gait recognition, there are many potential directions for this challenging task. For example, one direction is to study how to design a deep CNN for learning more discriminative features directly from 3D meshes. The second direction is how to learn the temporal information of gait representation, because the walking speed and route in Gait3D are irregular, it is significantly different from the datasets built in the lab. Another interesting direction is how to fuse the multi-modal information like silhouette, 2D/3D skeleton, and 3D mesh for gait recognition in the wild.\n\nMore discussions about the limitations and potential negative impact can be found in the supplementary material.\n\n\nConclusion\n\nGait recognition in the wild faces significant challenges such as extreme viewpoint changes, occlusions of the human body, and complex clutter in the environment. Existing methods using 2D silhouettes or skeletons will fail in the wild because crucial information like 3D viewpoints and shapes of human bodies is discarded. Therefore, this paper proposes a 3D SMPL model-based framework (SMPLGait) which is the first method to explore dense 3D representations for gait recognition in the wild. To facilitate the research, we build the first large-scale 3D gait recognition dataset (Gait3D) from cameras deployed in a large supermarket. It provides diverse gait representations including 3D meshes, 3D SMPLs, 3D poses, 2D silhouettes, and 2D poses for over 25,000 gait sequences of 4,000 subjects. We hope Gait3D can provide researchers with a new perspective on gait recognition.\n\n\nA. Appendix: Details of the SMPLGait\n\n\nA.1. Details of the Network Structure\n\nThe Silhouette Learning Network (SLN) consists of six convolutional layers, and each convolutional layer is followed by LeakyReLU whose negative slope is equal to 0.01. The structure of SLN is inspired by GaitSet [5] and OpenGait 11 . The detailed parameters are listed in Table 3.\n\n\nLayers\n\nKernel # Kernel Size Stride Padding  \nConv1 64 5\u00d75 1 2 LeakyReLU (0.01) - - - - Conv2 64 3\u00d73 1 1 LeakyReLU (0.01) - - - - Max Pooling - 2\u00d72 2 0 Conv3 128 3\u00d73 1 1 LeakyReLU (0.01) - - - - Conv4 128 3\u00d73 1 1 LeakyReLU (0.01) - - - - Max Pooling - 2\u00d72 2 0 Conv5 256 3\u00d73 1 1 LeakyReLU (0.01) - - - - Conv6 256 3\u00d73 1 1 LeakyReLU (0.01) - - - -FC1 128 0.0 BN1 - - ReLU - - FC2 256 0.2 BN2 - - ReLU - - FC3 h \u00d7 w 0.2 BN3 - - ReLU - -\n\nB. Appendix: Additional Experimental Results\n\nIn this section, we first analyze the effect of sequence length on accuracy during inference. Then, we conduct the cross-domain experiments to reflect the domain gap between existing datasets and our Gait3D dataset. At last, we provide some exemplar results of our SMPLGait framework for gait recognition to qualitatively demonstrate the effectiveness of our method.\n\n\nB.1. Effect of the Lengths of Test Sequences\n\nThis subsection analyzes the influence of the length of testing sequences on the accuracy of gait recognition. We sample 10% \u223c 100% frames with the 10% increment from the sequences during testing. The plots are demonstrated in Figure 7. From the results, we can observe that the accuracy is improved with the increasing frames of the sequence. Therefore, in real practice, we need to make a trade-off between accuracy and efficiency.\n\n\nB.2. Cross-domain Experiments\n\nIn this subsection, we analyze the domain gap between our Gait3D dataset and existing widely used datasets including CASIA-B [56], OU-LP [17] and the recently released GREW [63]. Because existing datasets do not provide 3D representations, we only adopt the 2D silhouettes for gait representations in our experiments. We adopt the GaitSet [5] for the cross-domain experiments since it is the SOTA model-free method on the Gait3D dataset.\n\n\nB.2.1 Evaluation Protocol\n\nIn the cross-domain experiments, we train the GaitSet model on the training set of one source dataset and evaluation the trained model on the testing set of one target dataset. For CASIA-B, OU-LP, and Gait3D, we use the official train/test split for training and testing. Because the test set of GREW [63] is not released publicly for evaluation, we randomly sample 1,000 IDs from the training set of GREW for evaluation. For the sampled 1,000 IDs, we further randomly select one sequence from each ID to build the query set with 1,000 sequences, while the rest of the sequences becomes the gallery set with 4,095 sequences. We utilize Rank-1 (R-1), Rank-5 (R-5), and mAP as the evaluation metrics.\n\n\nB.2.2 Main Results\n\nThe results of the cross-domain evaluation are listed in Table 5. From the results, we first find that the GaitSet models trained on the in-the-lab datasets, i.e., CASIA-B [56] and OU-LP [17] obtain very poor results, only 6.90% and 6.10% Rank-1. This reflects that there is a huge domain gap between the in-the-lab research and the in-the-wild application.\n\nNext, we observe that the model trained on GREW then tested on Gait3D only obtains 16 model trained on Gait3D then tested on GREW achieves a much higher Rank-1, i.e., 43.86%. It is worth noting that the training set of GREW has 20,000 IDs, while our Gait3D only contains 3,000 IDs. This demonstrates the significant domain gap between our Gait3D and GREW although the two datasets are both collected in the wild. Moreover, it indicates that the model trained on Gait3D has a more powerful capability of generalization than the one trained on GREW. At last, we can see that the GaitSet trained on Gait3D achieves competitive accuracy on in-the-lab datasets, i.e., Rank-1 of 66.71% on CASIA-B and 97.84% on OU-LP which is close to the model trained on OU-LP (99.89%) in our implementation). These results further prove that there is a huge gap between the in-the-lab dataset and in-the-wild application, while our Gait3D enables the model to learn more generalized gait representations. From Figure 8 -10, we can observe that the 3D representations can work well in multi-viewpoint, occluded, and multi-person cases. By this means, 3D meshes can provide more information about shapes, poses, and viewpoints of human bodies, which can help improve the accuracy of gait recognition in real-world scenes. Figure 11 illustrates a bad case in which the top-3 persons with similar clothes and shapes seriously interfere with the matching results. This indicates that very similar clothing and shapes of persons are one of the main challenges of gait recognition.\n\n\nB.3. Exemplar Results of SMPLGait\n\n\nC. Appendix: Discussion\n\nLimitations. Although we proposed a 3D gait recognition framework, its performance still has a large space to improve for practical applications. In addition, we only exploit a few frames of gait sequences, e.g., 30 frames, in our framework. The temporal dynamics are not fully explored for gait recognition in the wild.\n\nPotential Negative Impact. The potential negative outcomes mainly come from the fact that, with the largescale deployment of the urban monitoring network, if the accuracy of gait recognition in the real scenarios is greatly improved in the future, it may cause some privacy and security issues. To minimize these risks, our Gait3D dataset will be distributed only for research purposes via the caseby-case application with a strict license. In summary, we will try our best to protect the privacy of the subjects. We hope that the development of gait recognition can help to create a better human society, as well as better services for human beings, such as assisting the police to solve crimes, looking for lost people, and so on. \n\nFigure 1 .\n1Different gait representations of the same person from two viewpoints. Compared with silhouettes and skeletons, 3D meshes retain the shapes and viewpoints of the human body in the 3D space. (Best viewed in color.) (GEIs) [11], 2D skeletons\n\nFigure 2 .\n2The architecture of the SMPLGait framework for 3D gait recognition in the wild.\n\nFigure 3 .\n3Examples of gait representations in the Gait3D dataset. The sizes are normalized for visualization. (Best viewed in color.)\n\nFigure 4 .\n4Statistics about the Gait3D dataset.\n\nFigure 6 .\n6The effect of different training ID numbers.\n\nFigure 8 -\n811 provides several exemplar results of our SMPLGait framework on the Gait3D dataset. The top two rows with blue bounding boxes are the silhouette sequence and 3D Mesh sequence of the query, respectively. The rows following the query are the top-5 gallery sequences ranked by their similarities to the query sequence. The results with green bounding boxes are the correctly matched sequences, while those with red bounding boxes are wrong results.\n\nFigure 8 .Figure 10 .\n810Exemplar results of SMPLGait on the Gait3D. 16 consecutive frames are sampled from each sequence for visualization. This case shows that our method obtains good results when the samples are high-quality. (Best viewed in color.) [6] Ching-Hang Chen and Deva Ramanan. 3d human pose estimation = 2d pose estimation + matching. In CVPR, pages 5759-5767, 2017. 7 [7] Xiaodong Chen, Xinchen Liu, Wu Liu, Xiaoping Zhang, Yongdong Zhang, and Tao Mei. Explainable person re-identification with attribute-guided metric distillation. In ICCV, pages 11793-11802, 2021. 2 [8] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, pages 226-231, Exemplar results of SMPLGait on the Gait3D. This example shows that even when the silhouettes are of low quality, the 3D meshes can help the model obtain correct results. (Best viewed in color.) [11] Ju Han and Bir Bhanu. Individual recognition using gait energy image. IEEE TPAMI, 28(2):316-322, 2006. 1, 2, 3 [12] Lingxiao He, Xingyu Liao, Wu Liu, Xinchen Liu, Peng Cheng, and Tao Mei. FastReID: A pytorch toolbox for general instance re-identification. CoRR, abs/2006.02631, 2020. 5 [13] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In\n\nTable 3 .\n3Details of the SLN Network. Batch Normalization (BN) layers, and ReLU activation functions. The details of 3D-STN are listed inTable 4. Note that the h and w listed in the table are the height and width of the feature map from SLN. For convenient computation, we set h = w = max(h, w) in our experiments.The 3D Spatial Transformation Network (3D-STN) \nconsists of several Fully Connected (FC) layers with \ndropout, Layers Neuron # Dropout Rate \n\n\n\nTable 4 .\n4Details of the 3D-STN Network. (Input Size: 88\u00d7128).\n\n\n.50% Rank-1, while theFigure 7. The effect of the lengths of test sequences.Table 5. Results of cross-domain experiments. The method is trained on each source dataset and directly tested on the target datasets.4.50 \n\n9.50 \n\n12.60 \n\n18.70 \n20.30 \n\n23.50 \n\n27.00 \n28.90 27.90 28.20 \n\n6.40 \n\n11.90 \n\n16.70 \n\n22.10 \n\n26.50 \n\n30.30 \n\n33.10 \n34.50 \n36.30 36.70 \n\n6.80 \n\n14.00 \n\n20.80 \n\n25.90 \n\n30.50 \n\n35.70 \n\n39.80 \n\n42.40 \n44.40 \n46.30 \n\n0 \n\n5 \n\n10 \n\n15 \n\n20 \n\n25 \n\n30 \n\n35 \n\n40 \n\n45 \n\n50 \n\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n\nRank-1 \n\n(%) \n\nPercentage of Sampled Frames in sequences (%) \n\nGaitPart \nGaitSet \nOurs \n\nSource \nTarget \nR-1 (%) R-5 (%) mAP (%) \n\nCASIA-B [56] \nGait3D \n\n6.90 \n14.60 \n4.64 \nOU-LP [17] \n6.10 \n12.40 \n4.42 \nGREW [63] \n16.50 \n31.10 \n11.71 \n\nGait3D \n\nCASIA-B [56] \n66.71 \n71.59 \n33.88 \nOU-LP [17] \n97.84 \n99.38 \n68.06 \nGREW [63] \n43.86 \n60.89 \n28.06 \n\n\nFor the convenience of notation, we omit the channel of the feature map.\nhttp://ffmpeg.org/ under the GNU LGPL License v2.1. 3 4,000 person bounding boxes are labeled for fine-tuning the detector. 4 https : / / github . com / JDAI -CV / fast -reid under the Apache 2.0 license.(d) Silhouettes (e) 2D Skeletons (c) 3D Skeletons (b) 3D Meshes (a) RGB Frames\nhttps : / / github . com / HRNet / HRNet -Human -Pose -Estimation under the MIT License. 6 4,000 images are labeled to fine-tune the pose estimator. 7 https : / / github . com / Arthur151 / ROMP under the MIT License. 8 https : / / github . com / HRNet / HRNet -Semantic -Segmentation under the MIT license.\nIt should be noticed that SMPLGait w/o 3D is equal to OpenGait Baseline[10] \nhttps://github.com/ShiqiYu/OpenGait\n\nPerformance evaluation of model-based gait on multi-view very large population database with pose sequences. Weizhi An, Shiqi Yu, Yasushi Makihara, Xinhui Wu, Chi Xu, Yang Yu, Rijun Liao, Yasushi Yagi, IEEE TBBIS. 244Weizhi An, Shiqi Yu, Yasushi Makihara, Xinhui Wu, Chi Xu, Yang Yu, Rijun Liao, and Yasushi Yagi. Performance evaluation of model-based gait on multi-view very large population database with pose sequences. IEEE TBBIS, 2(4):421-430, 2020. 3, 4\n\nSCAPE: shape completion and animation of people. Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, James Davis, ACM TOG. 243Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. SCAPE: shape completion and animation of people. ACM TOG, 24(3):408-416, 2005. 3\n\nModel-based 3d gait biometrics. Gunawan Ariyanto, Mark S Nixon, IJCB. Gunawan Ariyanto and Mark S. Nixon. Model-based 3d gait biometrics. In IJCB, pages 1-7, 2011. 2\n\nOpenPose: Realtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh, IEEE TPAMI. 431Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. OpenPose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE TPAMI, 43(1):172-186, 2021. 7\n\nGaitSet: Regarding gait as a set for cross-view gait recognition. Hanqing Chao, Yiwei He, Junping Zhang, Jianfeng Feng, AAAI. 79Hanqing Chao, Yiwei He, Junping Zhang, and Jianfeng Feng. GaitSet: Regarding gait as a set for cross-view gait recognition. In AAAI, pages 8126-8133, 2019. 1, 2, 3, 4, 6, 7, 9\n\nExemplar results of SMPLGait on the Gait3D. This example reflects that our method can work well when part of the person is occluded. (Best viewed in color. 5Figure 9Figure 9. Exemplar results of SMPLGait on the Gait3D. This example reflects that our method can work well when part of the person is occluded. (Best viewed in color.) 1996. 5\n\nGaitPart: Temporal part-based model for gait recognition. Yunjie Chao Fan, Chunshui Peng, Xu Cao, Saihui Liu, Jiannan Hou, Yongzhen Chi, Qing Huang, Zhiqiang Li, He, CVPR. 7Chao Fan, Yunjie Peng, Chunshui Cao, Xu Liu, Saihui Hou, Jiannan Chi, Yongzhen Huang, Qing Li, and Zhiqiang He. GaitPart: Temporal part-based model for gait recognition. In CVPR, pages 14213-14221, 2020. 1, 2, 6, 7\n\n. Chuanfu Chao Fan, Junhao Shen, Shiqi Liang, Yu, Opengait, Chao Fan, Chuanfu Shen, Junhao Liang, and Shiqi Yu. OpenGait. https://github.com/ShiqiYu/ OpenGait. 7\n\nThe top-3 results contain persons wearing very similar clothes to the query, which is a very challenging condition of gait recognition. (Best viewed in color.) defense of the triplet loss for person re-identification. abs/1703.07737Figure 11. A bad case of SMPLGait on the Gait3D. CoRRFigure 11. A bad case of SMPLGait on the Gait3D. The top-3 results contain persons wearing very similar clothes to the query, which is a very challenging condition of gait recognition. (Best viewed in color.) defense of the triplet loss for person re-identification. CoRR, abs/1703.07737, 2017. 6\n\nClothing-invariant gait identification using part-based clothing categorization and adaptive weight control. Altab Md, Yasushi Hossain, Junqiu Makihara, Yasushi Wang, Yagi, PR433Md. Altab Hossain, Yasushi Makihara, Junqiu Wang, and Yasushi Yagi. Clothing-invariant gait identification using part-based clothing categorization and adaptive weight control. PR, 43(6):2281-2291, 2010. 3\n\nGait lateral network: Learning discriminative and compact representations for gait recognition. Saihui Hou, Chunshui Cao, Xu Liu, Yongzhen Huang, ECCV. 67Saihui Hou, Chunshui Cao, Xu Liu, and Yongzhen Huang. Gait lateral network: Learning discriminative and compact representations for gait recognition. In ECCV, pages 382- 398, 2020. 2, 6, 7\n\nContextsensitive temporal feature learning for gait recognition. Xiaohu Huang, Duowang Zhu, Hao Wang, Xinggang Wang, Bo Yang, Botao He, Wenyu Liu, Bin Feng, ICCV. 67Xiaohu Huang, Duowang Zhu, Hao Wang, Xinggang Wang, Bo Yang, Botao He, Wenyu Liu, and Bin Feng. Context- sensitive temporal feature learning for gait recognition. In ICCV, pages 12909-12918, 2021. 2, 6, 7\n\nThe OU-ISIR gait database comprising the large population dataset and performance evaluation of gait recognition. Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, Yasushi Yagi, IEEE TIFS. 7510Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, and Yasushi Yagi. The OU-ISIR gait database comprising the large population dataset and performance evaluation of gait recognition. IEEE TIFS, 7(5):1511-1521, 2012. 3, 4, 6, 7, 9, 10\n\nEnd-to-end recovery of human shape and pose. Angjoo Kanazawa, Michael J Black, David W Jacobs, Jitendra Malik, CVPR. 23Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, pages 7122-7131, 2018. 2, 3\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. 67Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6, 7\n\nVIBE: video inference for human body pose and shape estimation. Muhammed Kocabas, Nikos Athanasiou, Michael J Black, CVPR. 23Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. VIBE: video inference for human body pose and shape estimation. In CVPR, pages 5252-5262, 2020. 2, 3\n\nA model-based gait recognition method with body pose and human prior knowledge. Rijun Liao, Shiqi Yu, Weizhi An, Yongzhen Huang, PR98Rijun Liao, Shiqi Yu, Weizhi An, and Yongzhen Huang. A model-based gait recognition method with body pose and human prior knowledge. PR, 98, 2020. 7\n\nGait recognition via effective global-local feature representation and local temporal aggregation. Beibei Lin, Shunli Zhang, Xin Yu, ICCV. 67Beibei Lin, Shunli Zhang, and Xin Yu. Gait recognition via effective global-local feature representation and local temporal aggregation. In ICCV, pages 14648-14656, 2021. 2, 6, 7\n\nNTU RGB+D 120: A largescale benchmark for 3d human activity understanding. Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C Kot, IEEE TPAMI. 4210Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C. Kot. NTU RGB+D 120: A large- scale benchmark for 3d human activity understanding. IEEE TPAMI, 42(10):2684-2701, 2020. 3\n\nRecent advances in monocular 2d and 3d human pose estimation: A deep learning perspective. Wu Liu, Qian Bao, Yu Sun, Tao Mei, ACM Computing Surveys. 20222Wu Liu, Qian Bao, Yu Sun, and Tao Mei. Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective. ACM Computing Surveys, 2022. 2\n\nLargescale vehicle re-identification in urban surveillance videos. Xinchen Liu, Wu Liu, Huadong Ma, Huiyuan Fu, ICME. Xinchen Liu, Wu Liu, Huadong Ma, and Huiyuan Fu. Large- scale vehicle re-identification in urban surveillance videos. In ICME, pages 1-6, 2016. 2\n\nPROVID: progressive and multimodal vehicle reidentification for large-scale urban surveillance. Xinchen Liu, Wu Liu, Tao Mei, Huadong Ma, IEEE TMM. 203Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma. PROVID: progressive and multimodal vehicle reidentification for large-scale urban surveillance. IEEE TMM, 20(3):645-658, 2018. 2\n\nAutogpart: Intermediate supervision search for generalizable 3d part segmentation. Xueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, Li Yi, abs/2203.06558, 2022. 3CoRRXueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, and Li Yi. Autogpart: Intermediate supervision search for generalizable 3d part segmentation. CoRR, abs/2203.06558, 2022. 3\n\nSMPL: a skinned multiperson linear model. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J Black, ACM TOG343Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: a skinned multi- person linear model. ACM TOG, 34(6):248:1-248:16, 2015. 2, 3\n\nGait analysis of gender and age using a large-scale multiview gait database. Yasushi Makihara, Hidetoshi Mannami, Yasushi Yagi, ACCV. 3Yasushi Makihara, Hidetoshi Mannami, and Yasushi Yagi. Gait analysis of gender and age using a large-scale multi- view gait database. In ACCV, pages 440-451, 2010. 3\n\nAnalyzing and recognizing walking figures in XYT. A Sourabh, Edward H Niyogi, Adelson, CVPR. Sourabh A. Niyogi and Edward H. Adelson. Analyzing and recognizing walking figures in XYT. In CVPR, pages 469- 474, 1994. 1\n\nThe HumanID gait challenge problem: Data sets, performance, and analysis. P Jonathon Sudeep Sarkar, Zongyi Phillips, Isidro Robledo Liu, Patrick Vega, Kevin W Grother, Bowyer, IEEE TPAMI. 2724Sudeep Sarkar, P. Jonathon Phillips, Zongyi Liu, Isidro Rob- ledo Vega, Patrick Grother, and Kevin W. Bowyer. The HumanID gait challenge problem: Data sets, performance, and analysis. IEEE TPAMI, 27(2):162-177, 2005. 3, 4\n\nGEINet: View-invariant gait recognition using a convolutional neural network. Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, Yasushi Yagi, In ICB. 627Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, and Yasushi Yagi. GEINet: View-invariant gait recognition using a convolutional neural network. In ICB, pages 1-8, 2016. 2, 6, 7\n\nStronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition. Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang, ACM MM. Yi-Fan Song, Zhang Zhang, Caifeng Shan, and Liang Wang. Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition. In ACM MM, pages 1625-1633, 2020. 7\n\nMonocular, one-stage, regression of multiple 3d people. Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, Tao Mei, ICCV. 25Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J. Black, and Tao Mei. Monocular, one-stage, regression of multiple 3d people. In ICCV, pages 11179-11188, 2021. 2, 3, 5\n\nPutting people in their place: Monocular regression of 3d people in depth. Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, Michael J Black, CVPR. 2022Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J Black. Putting people in their place: Monocular regression of 3d people in depth. In CVPR, 2022. 3\n\nMulti-view large population gait dataset and its performance evaluation for cross-view gait recognition. Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, Yasushi Yagi, IPSJ TCVA. 104Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, and Yasushi Yagi. Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition. IPSJ TCVA, 10:4, 2018. 1, 3, 4, 7\n\nEfficient night gait recognition based on template matching. Daoliang Tan, Kaiqi Huang, Shiqi Yu, Tieniu Tan, ICPR. 14Daoliang Tan, Kaiqi Huang, Shiqi Yu, and Tieniu Tan. Efficient night gait recognition based on template matching. In ICPR, pages 1000-1003, 2006. 1, 3, 4\n\nGaitGraph: Graph convolutional network for skeleton-based gait recognition. Torben Teepe, Ali Khan, Johannes Gilg, Fabian Herzog, Stefan H\u00f6rmann, Gerhard Rigoll, Torben Teepe, Ali Khan, Johannes Gilg, Fabian Herzog, Stefan H\u00f6rmann, and Gerhard Rigoll. GaitGraph: Graph convolutional network for skeleton-based gait recognition.\n\n. Corr, abs/2101.11228CoRR, abs/2101.11228, 2021. 7\n\nSilhouette transformation based on walking speed for gait identification. Akira Tsuji, Yasushi Makihara, Yasushi Yagi, CVPR. 34Akira Tsuji, Yasushi Makihara, and Yasushi Yagi. Silhouette transformation based on walking speed for gait identifica- tion. In CVPR, pages 717-722, 2010. 3, 4\n\nThe OU-ISIR large population gait database with real-life carried object and its performance evaluation. Md, Trung Zasim Uddin, Yasushi Ngo Thanh, Noriko Makihara, Xiang Takemura, Daigo Li, Yasushi Muramatsu, Yagi, IPSJ TCVA. 1054Md. Zasim Uddin, Trung Ngo Thanh, Yasushi Makihara, Noriko Takemura, Xiang Li, Daigo Muramatsu, and Yasushi Yagi. The OU-ISIR large population gait database with real-life carried object and its performance evaluation. IPSJ TCVA, 10:5, 2018. 3, 4\n\n3d tracking for gait characterization and recognition. Raquel Urtasun, Pascal Fua, FGR. 23Raquel Urtasun and Pascal Fua. 3d tracking for gait characterization and recognition. In FGR, pages 17-22, 2004. 2, 3\n\nA survey on gait recognition. Changsheng Wan, Li Wang, Vir V Phoha, ACM CSUR. 515Changsheng Wan, Li Wang, and Vir V. Phoha. A survey on gait recognition. ACM CSUR, 51(5):89:1-89:35, 2019. 1, 2\n\nDeep highresolution representation learning for visual recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao, IEEE TPAMI. 4310Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high- resolution representation learning for visual recognition. IEEE TPAMI, 43(10):3349-3364, 2021. 5\n\nSilhouette analysis-based gait recognition for human identification. Liang Wang, Tieniu Tan, Huazhong Ning, Weiming Hu, IEEE TPAMI. 25124Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming Hu. Silhouette analysis-based gait recognition for human identification. IEEE TPAMI, 25(12):1505-1518, 2003. 3, 4\n\nUnsupervised graph association for person re-identification. Jinlin Wu, Hao Liu, Yang Yang, Zhen Lei, Shengcai Liao, Stan Z Li, ICCV. Jinlin Wu, Hao Liu, Yang Yang, Zhen Lei, Shengcai Liao, and Stan Z. Li. Unsupervised graph association for person re-identification. In ICCV, pages 8320-8329, 2019. 2\n\nA comprehensive study on cross-view gait based human identification with deep cnns. Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, Tieniu Tan, IEEE TPAMI. 392Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, and Tieniu Tan. A comprehensive study on cross-view gait based human identification with deep cnns. IEEE TPAMI, 39(2):209-226, 2017. 1, 2\n\nThe OU-ISIR gait database comprising the large population dataset with age and performance evaluation of age estimation. Chi Xu, Yasushi Makihara, Gakuto Ogi, Xiang Li, Yasushi Yagi, Jianfeng Lu, IPSJ TCVA. 9324Chi Xu, Yasushi Makihara, Gakuto Ogi, Xiang Li, Yasushi Yagi, and Jianfeng Lu. The OU-ISIR gait database comprising the large population dataset with age and performance evaluation of age estimation. IPSJ TCVA, 9:24, 2017. 3\n\nAutomated person recognition by walking and running via model-based approaches. Chew-Yean, Mark S Yam, John N Nixon, Carter, PR37Chew-Yean Yam, Mark S. Nixon, and John N. Carter. Automated person recognition by walking and running via model-based approaches. PR, 37(5):1057-1072, 2004. 2\n\nRecognition of walking humans in 3d: Initial results. Koichiro Yamauchi, Bir Bhanu, Hideo Saito, CVPRW. Koichiro Yamauchi, Bir Bhanu, and Hideo Saito. Recogni- tion of walking humans in 3d: Initial results. In CVPRW, pages 45-52, 2009. 2\n\nDeep multi-view enhancement hashing for image retrieval. Chenggang Yan, Biao Gong, Yuxuan Wei, Yue Gao, IEEE TPAMI. 434Chenggang Yan, Biao Gong, Yuxuan Wei, and Yue Gao. Deep multi-view enhancement hashing for image retrieval. IEEE TPAMI, 43(4):1445-1451, 2021. 2\n\nTaskadaptive attention for image captioning. Chenggang Yan, Yiming Hao, Liang Li, Jian Yin, Anan Liu, Zhendong Mao, Zhenyu Chen, Xingyu Gao, IEEE TCSVT. 321Chenggang Yan, Yiming Hao, Liang Li, Jian Yin, Anan Liu, Zhendong Mao, Zhenyu Chen, and Xingyu Gao. Task- adaptive attention for image captioning. IEEE TCSVT, 32(1):43-51, 2022. 2\n\nDepth image denoising using nuclear norm and learning graph model. Chenggang Yan, Zhisheng Li, Yongbing Zhang, Yutao Liu, Xiangyang Ji, Yong-Dong Zhang, 122:1-122:17TOMM. 164Chenggang Yan, Zhisheng Li, Yongbing Zhang, Yutao Liu, Xiangyang Ji, and Yong-Dong Zhang. Depth image denoising using nuclear norm and learning graph model. TOMM, 16(4):122:1-122:17, 2021. 2\n\nAge-invariant face recognition by multifeature fusionand decomposition with self-attention. Chenggang Yan, Lixuan Meng, Liang Li, Jiehua Zhang, Zhan Wang, Jian Yin, Jiyong Zhang, Yaoqi Sun, Bolun Zheng, TOMM18Chenggang Yan, Lixuan Meng, Liang Li, Jiehua Zhang, Zhan Wang, Jian Yin, Jiyong Zhang, Yaoqi Sun, and Bolun Zheng. Age-invariant face recognition by multi- feature fusionand decomposition with self-attention. TOMM, 18(1s):1-18, 2022. 2\n\nPrecise no-reference image quality evaluation based on distortion identification. Chenggang Yan, Tong Teng, Yutao Liu, Yongbing Zhang, Haoqian Wang, Xiangyang Ji, TOMM. 173sChenggang Yan, Tong Teng, Yutao Liu, Yongbing Zhang, Haoqian Wang, and Xiangyang Ji. Precise no-reference image quality evaluation based on distortion identification. TOMM, 17(3s):1-21, 2021. 2\n\nDeep learning for person re-identification: A survey and outlook. Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, Steven C H Hoi, abs/2001.04193CoRRMang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven C. H. Hoi. Deep learning for person re-identification: A survey and outlook. CoRR, abs/2001.04193, 2020. 6\n\nA framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition. Shiqi Yu, Daoliang Tan, Tieniu Tan, ICPR. 10Shiqi Yu, Daoliang Tan, and Tieniu Tan. A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition. In ICPR, pages 441-444, 2006. 1, 3, 4, 7, 9, 10\n\nCrossview gait recognition with deep universal linear embeddings. Shaoxiong Zhang, Yunhong Wang, Annan Li, CVPR. 1Shaoxiong Zhang, Yunhong Wang, and Annan Li. Cross- view gait recognition with deep universal linear embeddings. In CVPR, pages 9095-9104, 2021. 1, 2\n\nGait recognition via disentangled representation learning. Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu, Jian Wan, Nanxin Wang, CVPR. Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu, Jian Wan, and Nanxin Wang. Gait recognition via disentangled representation learning. In CVPR, pages 4710- 4719, 2019. 2\n\n3d gait recognition using multiple cameras. Guoying Zhao, Guoyi Liu, Hua Li, Matti Pietik\u00e4inen, FGR. Guoying Zhao, Guoyi Liu, Hua Li, and Matti Pietik\u00e4inen. 3d gait recognition using multiple cameras. In FGR, pages 529-534, 2006. 2\n\nScalable person reidentification: A benchmark. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, Qi Tian, ICCV. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re- identification: A benchmark. In ICCV, pages 1116-1124, 2015. 6\n\nUnsupervised domain adaptation for 3d keypoint estimation via view consistency. Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, Qixing Huang, ECCV (12). 11216Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, and Qixing Huang. Unsupervised domain adaptation for 3d keypoint estimation via view consistency. In ECCV (12), volume 11216, pages 141-157, 2018. 3\n\nObjects as points. CoRR, abs/1904.07850. Xingyi Zhou, Dequan Wang, Philipp Kr\u00e4henb\u00fchl, Xingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. Objects as points. CoRR, abs/1904.07850, 2019. 5\n\nGait recognition in the wild: A benchmark. Zheng Zhu, Xianda Guo, Tian Yang, Junjie Huang, Jiankang Deng, Guan Huang, Dalong Du, Jiwen Lu, Jie Zhou, ICCV. 910Zheng Zhu, Xianda Guo, Tian Yang, Junjie Huang, Jiankang Deng, Guan Huang, Dalong Du, Jiwen Lu, and Jie Zhou. Gait recognition in the wild: A benchmark. In ICCV, pages 14789-14799, 2021. 1, 2, 3, 4, 9, 10\n", "annotations": {"author": "[{\"end\":133,\"start\":78},{\"end\":186,\"start\":134},{\"end\":236,\"start\":187},{\"end\":291,\"start\":237},{\"end\":365,\"start\":292},{\"end\":426,\"start\":366}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":85},{\"end\":145,\"start\":142},{\"end\":195,\"start\":192},{\"end\":250,\"start\":239},{\"end\":305,\"start\":302},{\"end\":373,\"start\":370}]", "author_first_name": "[{\"end\":84,\"start\":78},{\"end\":141,\"start\":134},{\"end\":188,\"start\":187},{\"end\":191,\"start\":189},{\"end\":238,\"start\":237},{\"end\":301,\"start\":292},{\"end\":369,\"start\":366}]", "author_affiliation": "[{\"end\":132,\"start\":92},{\"end\":185,\"start\":147},{\"end\":235,\"start\":197},{\"end\":290,\"start\":252},{\"end\":364,\"start\":324},{\"end\":425,\"start\":387}]", "title": "[{\"end\":75,\"start\":1},{\"end\":501,\"start\":427}]", "venue": null, "abstract": "[{\"end\":2058,\"start\":503}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2222,\"start\":2218},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2225,\"start\":2222},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2334,\"start\":2330},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2467,\"start\":2463},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2956,\"start\":2952},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2973,\"start\":2969},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3048,\"start\":3045},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3050,\"start\":3048},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3053,\"start\":3050},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3066,\"start\":3062},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3300,\"start\":3296},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3350,\"start\":3347},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3371,\"start\":3367},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3588,\"start\":3584},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3631,\"start\":3627},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3634,\"start\":3631},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3637,\"start\":3634},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7121,\"start\":7117},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7343,\"start\":7340},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7346,\"start\":7343},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7456,\"start\":7453},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7458,\"start\":7456},{\"end\":7461,\"start\":7458},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7464,\"start\":7461},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7467,\"start\":7464},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7470,\"start\":7467},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7473,\"start\":7470},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7476,\"start\":7473},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7479,\"start\":7476},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7482,\"start\":7479},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7653,\"start\":7649},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7656,\"start\":7653},{\"end\":7751,\"start\":7734},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7879,\"start\":7875},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7898,\"start\":7894},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8184,\"start\":8181},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8186,\"start\":8184},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8189,\"start\":8186},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8192,\"start\":8189},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8356,\"start\":8352},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8492,\"start\":8488},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8621,\"start\":8617},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8722,\"start\":8718},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8842,\"start\":8839},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9527,\"start\":9523},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9530,\"start\":9527},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9533,\"start\":9530},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9560,\"start\":9557},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9563,\"start\":9560},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9566,\"start\":9563},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9569,\"start\":9566},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9572,\"start\":9569},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9575,\"start\":9572},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9578,\"start\":9575},{\"end\":9773,\"start\":9769},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9776,\"start\":9773},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9834,\"start\":9830},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10047,\"start\":10043},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10069,\"start\":10065},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10084,\"start\":10080},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10117,\"start\":10113},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10149,\"start\":10146},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10196,\"start\":10192},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10213,\"start\":10209},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10353,\"start\":10349},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10356,\"start\":10353},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10397,\"start\":10393},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10400,\"start\":10397},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10566,\"start\":10562},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10998,\"start\":10994},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11001,\"start\":10998},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11060,\"start\":11056},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11073,\"start\":11069},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11105,\"start\":11102},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11173,\"start\":11169},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11620,\"start\":11616},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11623,\"start\":11620},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11626,\"start\":11623},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11629,\"start\":11626},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12962,\"start\":12959},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14966,\"start\":14963},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17903,\"start\":17899},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18298,\"start\":18297},{\"end\":19059,\"start\":19055},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19136,\"start\":19132},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19948,\"start\":19944},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20127,\"start\":20123},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21830,\"start\":21826},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":21860,\"start\":21856},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22229,\"start\":22225},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22914,\"start\":22910},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23050,\"start\":23047},{\"end\":23240,\"start\":23236},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23258,\"start\":23255},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23426,\"start\":23422},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23589,\"start\":23585},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23693,\"start\":23689},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24260,\"start\":24256},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24350,\"start\":24346},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24940,\"start\":24936},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24968,\"start\":24965},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25381,\"start\":25377},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25435,\"start\":25431},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25584,\"start\":25580},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26473,\"start\":26469},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26497,\"start\":26493},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26500,\"start\":26497},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28097,\"start\":28095},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28274,\"start\":28272},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30395,\"start\":30393},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32571,\"start\":32568},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":34131,\"start\":34127},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34143,\"start\":34139},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34179,\"start\":34175},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34344,\"start\":34341},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34774,\"start\":34770},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35366,\"start\":35362},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35381,\"start\":35377},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35634,\"start\":35632},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42727,\"start\":42723}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38475,\"start\":38223},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38568,\"start\":38476},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38705,\"start\":38569},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38755,\"start\":38706},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38813,\"start\":38756},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39274,\"start\":38814},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40584,\"start\":39275},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41043,\"start\":40585},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41108,\"start\":41044},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41987,\"start\":41109}]", "paragraph": "[{\"end\":2798,\"start\":2074},{\"end\":4072,\"start\":2800},{\"end\":5123,\"start\":4074},{\"end\":5673,\"start\":5125},{\"end\":5963,\"start\":5675},{\"end\":6024,\"start\":5965},{\"end\":6203,\"start\":6026},{\"end\":6345,\"start\":6205},{\"end\":6505,\"start\":6347},{\"end\":6909,\"start\":6507},{\"end\":7026,\"start\":6926},{\"end\":7122,\"start\":7028},{\"end\":8372,\"start\":7124},{\"end\":10882,\"start\":8374},{\"end\":11756,\"start\":10884},{\"end\":12686,\"start\":11802},{\"end\":13219,\"start\":12708},{\"end\":13612,\"start\":13300},{\"end\":14283,\"start\":13633},{\"end\":14790,\"start\":14302},{\"end\":15136,\"start\":14818},{\"end\":15325,\"start\":15163},{\"end\":15651,\"start\":15350},{\"end\":16798,\"start\":15676},{\"end\":17761,\"start\":16837},{\"end\":18788,\"start\":17830},{\"end\":19663,\"start\":18823},{\"end\":20611,\"start\":19702},{\"end\":21378,\"start\":20658},{\"end\":22295,\"start\":21380},{\"end\":22589,\"start\":22311},{\"end\":22820,\"start\":22624},{\"end\":22898,\"start\":22846},{\"end\":23034,\"start\":22900},{\"end\":23241,\"start\":23036},{\"end\":23832,\"start\":23243},{\"end\":24804,\"start\":23834},{\"end\":25416,\"start\":24831},{\"end\":25866,\"start\":25418},{\"end\":26082,\"start\":25909},{\"end\":26138,\"start\":26084},{\"end\":26258,\"start\":26163},{\"end\":27297,\"start\":26260},{\"end\":27797,\"start\":27299},{\"end\":27951,\"start\":27799},{\"end\":28326,\"start\":27982},{\"end\":28628,\"start\":28366},{\"end\":28739,\"start\":28630},{\"end\":29167,\"start\":28741},{\"end\":29739,\"start\":29169},{\"end\":29995,\"start\":29741},{\"end\":30088,\"start\":29997},{\"end\":31266,\"start\":30103},{\"end\":31380,\"start\":31268},{\"end\":32274,\"start\":31395},{\"end\":32636,\"start\":32355},{\"end\":32684,\"start\":32647},{\"end\":33486,\"start\":33120},{\"end\":33968,\"start\":33535},{\"end\":34439,\"start\":34002},{\"end\":35167,\"start\":34469},{\"end\":35547,\"start\":35190},{\"end\":37103,\"start\":35549},{\"end\":37487,\"start\":37167},{\"end\":38222,\"start\":37489}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13281,\"start\":13220},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13299,\"start\":13281},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13632,\"start\":13613},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14301,\"start\":14284},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14817,\"start\":14791},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15349,\"start\":15326},{\"attributes\":{\"id\":\"formula_6\"},\"end\":32984,\"start\":32685},{\"attributes\":{\"id\":\"formula_7\"},\"end\":33072,\"start\":32984}]", "table_ref": "[{\"end\":5442,\"start\":5435},{\"end\":9599,\"start\":9592},{\"end\":15859,\"start\":15852},{\"end\":26257,\"start\":26250},{\"end\":28131,\"start\":28124},{\"end\":28738,\"start\":28731},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32635,\"start\":32628},{\"end\":35254,\"start\":35247}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2072,\"start\":2060},{\"attributes\":{\"n\":\"2.\"},\"end\":6924,\"start\":6912},{\"attributes\":{\"n\":\"3.\"},\"end\":11789,\"start\":11759},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11800,\"start\":11792},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12706,\"start\":12689},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15161,\"start\":15139},{\"attributes\":{\"n\":\"4.\"},\"end\":15674,\"start\":15654},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16835,\"start\":16801},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17784,\"start\":17764},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":17828,\"start\":17787},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":18821,\"start\":18791},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":19700,\"start\":19666},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20656,\"start\":20614},{\"attributes\":{\"n\":\"5.\"},\"end\":22309,\"start\":22298},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22622,\"start\":22592},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":22844,\"start\":22823},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":24829,\"start\":24807},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":25907,\"start\":25869},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":26161,\"start\":26141},{\"attributes\":{\"n\":\"5.1.5\"},\"end\":27980,\"start\":27954},{\"attributes\":{\"n\":\"5.2.\"},\"end\":28364,\"start\":28329},{\"attributes\":{\"n\":\"6.\"},\"end\":30101,\"start\":30091},{\"attributes\":{\"n\":\"7.\"},\"end\":31393,\"start\":31383},{\"end\":32313,\"start\":32277},{\"end\":32353,\"start\":32316},{\"end\":32645,\"start\":32639},{\"end\":33118,\"start\":33074},{\"end\":33533,\"start\":33489},{\"end\":34000,\"start\":33971},{\"end\":34467,\"start\":34442},{\"end\":35188,\"start\":35170},{\"end\":37139,\"start\":37106},{\"end\":37165,\"start\":37142},{\"end\":38234,\"start\":38224},{\"end\":38487,\"start\":38477},{\"end\":38580,\"start\":38570},{\"end\":38717,\"start\":38707},{\"end\":38767,\"start\":38757},{\"end\":38825,\"start\":38815},{\"end\":39297,\"start\":39276},{\"end\":40595,\"start\":40586},{\"end\":41054,\"start\":41045}]", "table": "[{\"end\":41043,\"start\":40901},{\"end\":41987,\"start\":41321}]", "figure_caption": "[{\"end\":38475,\"start\":38236},{\"end\":38568,\"start\":38489},{\"end\":38705,\"start\":38582},{\"end\":38755,\"start\":38719},{\"end\":38813,\"start\":38769},{\"end\":39274,\"start\":38827},{\"end\":40584,\"start\":39301},{\"end\":40901,\"start\":40597},{\"end\":41108,\"start\":41056},{\"end\":41321,\"start\":41111}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2489,\"start\":2481},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4605,\"start\":4597},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11914,\"start\":11906},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12987,\"start\":12979},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14567,\"start\":14559},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20455,\"start\":20447},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20798,\"start\":20790},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20813,\"start\":20805},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20955,\"start\":20947},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21069,\"start\":21061},{\"end\":28283,\"start\":28275},{\"end\":29320,\"start\":29312},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29866,\"start\":29858},{\"end\":33770,\"start\":33762},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":36547,\"start\":36539},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36858,\"start\":36849}]", "bib_author_first_name": "[{\"end\":42881,\"start\":42875},{\"end\":42891,\"start\":42886},{\"end\":42903,\"start\":42896},{\"end\":42920,\"start\":42914},{\"end\":42928,\"start\":42925},{\"end\":42937,\"start\":42933},{\"end\":42947,\"start\":42942},{\"end\":42961,\"start\":42954},{\"end\":43284,\"start\":43276},{\"end\":43302,\"start\":43295},{\"end\":43321,\"start\":43315},{\"end\":43339,\"start\":43330},{\"end\":43350,\"start\":43347},{\"end\":43365,\"start\":43360},{\"end\":43607,\"start\":43600},{\"end\":43622,\"start\":43618},{\"end\":43624,\"start\":43623},{\"end\":43817,\"start\":43814},{\"end\":43828,\"start\":43823},{\"end\":43843,\"start\":43838},{\"end\":43858,\"start\":43851},{\"end\":43869,\"start\":43864},{\"end\":44149,\"start\":44142},{\"end\":44161,\"start\":44156},{\"end\":44173,\"start\":44166},{\"end\":44189,\"start\":44181},{\"end\":44786,\"start\":44780},{\"end\":44805,\"start\":44797},{\"end\":44814,\"start\":44812},{\"end\":44826,\"start\":44820},{\"end\":44839,\"start\":44832},{\"end\":44853,\"start\":44845},{\"end\":44863,\"start\":44859},{\"end\":44879,\"start\":44871},{\"end\":45120,\"start\":45113},{\"end\":45137,\"start\":45131},{\"end\":45149,\"start\":45144},{\"end\":45971,\"start\":45966},{\"end\":45983,\"start\":45976},{\"end\":45999,\"start\":45993},{\"end\":46017,\"start\":46010},{\"end\":46344,\"start\":46338},{\"end\":46358,\"start\":46350},{\"end\":46366,\"start\":46364},{\"end\":46380,\"start\":46372},{\"end\":46657,\"start\":46651},{\"end\":46672,\"start\":46665},{\"end\":46681,\"start\":46678},{\"end\":46696,\"start\":46688},{\"end\":46705,\"start\":46703},{\"end\":46717,\"start\":46712},{\"end\":46727,\"start\":46722},{\"end\":46736,\"start\":46733},{\"end\":47079,\"start\":47071},{\"end\":47091,\"start\":47087},{\"end\":47108,\"start\":47101},{\"end\":47126,\"start\":47119},{\"end\":47431,\"start\":47425},{\"end\":47449,\"start\":47442},{\"end\":47451,\"start\":47450},{\"end\":47464,\"start\":47459},{\"end\":47466,\"start\":47465},{\"end\":47483,\"start\":47475},{\"end\":47699,\"start\":47698},{\"end\":47715,\"start\":47710},{\"end\":47906,\"start\":47898},{\"end\":47921,\"start\":47916},{\"end\":47941,\"start\":47934},{\"end\":47943,\"start\":47942},{\"end\":48204,\"start\":48199},{\"end\":48216,\"start\":48211},{\"end\":48227,\"start\":48221},{\"end\":48240,\"start\":48232},{\"end\":48507,\"start\":48501},{\"end\":48519,\"start\":48513},{\"end\":48530,\"start\":48527},{\"end\":48801,\"start\":48798},{\"end\":48811,\"start\":48807},{\"end\":48831,\"start\":48823},{\"end\":48843,\"start\":48839},{\"end\":48857,\"start\":48850},{\"end\":48868,\"start\":48864},{\"end\":48870,\"start\":48869},{\"end\":49184,\"start\":49182},{\"end\":49194,\"start\":49190},{\"end\":49202,\"start\":49200},{\"end\":49211,\"start\":49208},{\"end\":49481,\"start\":49474},{\"end\":49489,\"start\":49487},{\"end\":49502,\"start\":49495},{\"end\":49514,\"start\":49507},{\"end\":49775,\"start\":49768},{\"end\":49783,\"start\":49781},{\"end\":49792,\"start\":49789},{\"end\":49805,\"start\":49798},{\"end\":50087,\"start\":50082},{\"end\":50101,\"start\":50093},{\"end\":50110,\"start\":50106},{\"end\":50122,\"start\":50116},{\"end\":50130,\"start\":50128},{\"end\":50382,\"start\":50375},{\"end\":50397,\"start\":50390},{\"end\":50413,\"start\":50407},{\"end\":50428,\"start\":50422},{\"end\":50447,\"start\":50440},{\"end\":50449,\"start\":50448},{\"end\":50723,\"start\":50716},{\"end\":50743,\"start\":50734},{\"end\":50760,\"start\":50753},{\"end\":50992,\"start\":50991},{\"end\":51008,\"start\":51002},{\"end\":51010,\"start\":51009},{\"end\":51234,\"start\":51233},{\"end\":51243,\"start\":51235},{\"end\":51265,\"start\":51259},{\"end\":51282,\"start\":51276},{\"end\":51290,\"start\":51283},{\"end\":51303,\"start\":51296},{\"end\":51315,\"start\":51310},{\"end\":51317,\"start\":51316},{\"end\":51657,\"start\":51652},{\"end\":51674,\"start\":51667},{\"end\":51690,\"start\":51685},{\"end\":51707,\"start\":51702},{\"end\":51723,\"start\":51716},{\"end\":52050,\"start\":52044},{\"end\":52062,\"start\":52057},{\"end\":52077,\"start\":52070},{\"end\":52089,\"start\":52084},{\"end\":52364,\"start\":52362},{\"end\":52374,\"start\":52370},{\"end\":52382,\"start\":52380},{\"end\":52392,\"start\":52388},{\"end\":52404,\"start\":52397},{\"end\":52406,\"start\":52405},{\"end\":52417,\"start\":52414},{\"end\":52673,\"start\":52671},{\"end\":52681,\"start\":52679},{\"end\":52691,\"start\":52687},{\"end\":52701,\"start\":52697},{\"end\":52709,\"start\":52706},{\"end\":52724,\"start\":52715},{\"end\":53011,\"start\":53005},{\"end\":53029,\"start\":53022},{\"end\":53045,\"start\":53040},{\"end\":53062,\"start\":53057},{\"end\":53078,\"start\":53071},{\"end\":53392,\"start\":53384},{\"end\":53403,\"start\":53398},{\"end\":53416,\"start\":53411},{\"end\":53427,\"start\":53421},{\"end\":53678,\"start\":53672},{\"end\":53689,\"start\":53686},{\"end\":53704,\"start\":53696},{\"end\":53717,\"start\":53711},{\"end\":53732,\"start\":53726},{\"end\":53749,\"start\":53742},{\"end\":54057,\"start\":54052},{\"end\":54072,\"start\":54065},{\"end\":54090,\"start\":54083},{\"end\":54380,\"start\":54375},{\"end\":54401,\"start\":54394},{\"end\":54419,\"start\":54413},{\"end\":54435,\"start\":54430},{\"end\":54451,\"start\":54446},{\"end\":54463,\"start\":54456},{\"end\":54805,\"start\":54799},{\"end\":54821,\"start\":54815},{\"end\":54993,\"start\":54983},{\"end\":55001,\"start\":54999},{\"end\":55011,\"start\":55008},{\"end\":55013,\"start\":55012},{\"end\":55223,\"start\":55215},{\"end\":55232,\"start\":55230},{\"end\":55246,\"start\":55238},{\"end\":55259,\"start\":55254},{\"end\":55274,\"start\":55267},{\"end\":55285,\"start\":55281},{\"end\":55296,\"start\":55292},{\"end\":55308,\"start\":55302},{\"end\":55320,\"start\":55313},{\"end\":55334,\"start\":55326},{\"end\":55346,\"start\":55341},{\"end\":55355,\"start\":55352},{\"end\":55712,\"start\":55707},{\"end\":55725,\"start\":55719},{\"end\":55739,\"start\":55731},{\"end\":55753,\"start\":55746},{\"end\":56008,\"start\":56002},{\"end\":56016,\"start\":56013},{\"end\":56026,\"start\":56022},{\"end\":56037,\"start\":56033},{\"end\":56051,\"start\":56043},{\"end\":56062,\"start\":56058},{\"end\":56064,\"start\":56063},{\"end\":56333,\"start\":56327},{\"end\":56346,\"start\":56338},{\"end\":56359,\"start\":56354},{\"end\":56374,\"start\":56366},{\"end\":56387,\"start\":56381},{\"end\":56725,\"start\":56722},{\"end\":56737,\"start\":56730},{\"end\":56754,\"start\":56748},{\"end\":56765,\"start\":56760},{\"end\":56777,\"start\":56770},{\"end\":56792,\"start\":56784},{\"end\":57133,\"start\":57129},{\"end\":57135,\"start\":57134},{\"end\":57145,\"start\":57141},{\"end\":57147,\"start\":57146},{\"end\":57389,\"start\":57381},{\"end\":57403,\"start\":57400},{\"end\":57416,\"start\":57411},{\"end\":57632,\"start\":57623},{\"end\":57642,\"start\":57638},{\"end\":57655,\"start\":57649},{\"end\":57664,\"start\":57661},{\"end\":57885,\"start\":57876},{\"end\":57897,\"start\":57891},{\"end\":57908,\"start\":57903},{\"end\":57917,\"start\":57913},{\"end\":57927,\"start\":57923},{\"end\":57941,\"start\":57933},{\"end\":57953,\"start\":57947},{\"end\":57966,\"start\":57960},{\"end\":58244,\"start\":58235},{\"end\":58258,\"start\":58250},{\"end\":58271,\"start\":58263},{\"end\":58284,\"start\":58279},{\"end\":58299,\"start\":58290},{\"end\":58313,\"start\":58304},{\"end\":58635,\"start\":58626},{\"end\":58647,\"start\":58641},{\"end\":58659,\"start\":58654},{\"end\":58670,\"start\":58664},{\"end\":58682,\"start\":58678},{\"end\":58693,\"start\":58689},{\"end\":58705,\"start\":58699},{\"end\":58718,\"start\":58713},{\"end\":58729,\"start\":58724},{\"end\":59071,\"start\":59062},{\"end\":59081,\"start\":59077},{\"end\":59093,\"start\":59088},{\"end\":59107,\"start\":59099},{\"end\":59122,\"start\":59115},{\"end\":59138,\"start\":59129},{\"end\":59418,\"start\":59414},{\"end\":59431,\"start\":59423},{\"end\":59444,\"start\":59438},{\"end\":59453,\"start\":59450},{\"end\":59465,\"start\":59461},{\"end\":59478,\"start\":59472},{\"end\":59482,\"start\":59479},{\"end\":59794,\"start\":59789},{\"end\":59807,\"start\":59799},{\"end\":59819,\"start\":59813},{\"end\":60103,\"start\":60094},{\"end\":60118,\"start\":60111},{\"end\":60130,\"start\":60125},{\"end\":60358,\"start\":60352},{\"end\":60370,\"start\":60366},{\"end\":60379,\"start\":60377},{\"end\":60391,\"start\":60385},{\"end\":60407,\"start\":60399},{\"end\":60417,\"start\":60413},{\"end\":60429,\"start\":60423},{\"end\":60676,\"start\":60669},{\"end\":60688,\"start\":60683},{\"end\":60697,\"start\":60694},{\"end\":60707,\"start\":60702},{\"end\":60910,\"start\":60905},{\"end\":60923,\"start\":60918},{\"end\":60932,\"start\":60930},{\"end\":60947,\"start\":60939},{\"end\":60962,\"start\":60954},{\"end\":60971,\"start\":60969},{\"end\":61231,\"start\":61225},{\"end\":61243,\"start\":61238},{\"end\":61258,\"start\":61252},{\"end\":61270,\"start\":61264},{\"end\":61282,\"start\":61276},{\"end\":61554,\"start\":61548},{\"end\":61567,\"start\":61561},{\"end\":61581,\"start\":61574},{\"end\":61742,\"start\":61737},{\"end\":61754,\"start\":61748},{\"end\":61764,\"start\":61760},{\"end\":61777,\"start\":61771},{\"end\":61793,\"start\":61785},{\"end\":61804,\"start\":61800},{\"end\":61818,\"start\":61812},{\"end\":61828,\"start\":61823},{\"end\":61836,\"start\":61833}]", "bib_author_last_name": "[{\"end\":42884,\"start\":42882},{\"end\":42894,\"start\":42892},{\"end\":42912,\"start\":42904},{\"end\":42923,\"start\":42921},{\"end\":42931,\"start\":42929},{\"end\":42940,\"start\":42938},{\"end\":42952,\"start\":42948},{\"end\":42966,\"start\":42962},{\"end\":43293,\"start\":43285},{\"end\":43313,\"start\":43303},{\"end\":43328,\"start\":43322},{\"end\":43345,\"start\":43340},{\"end\":43358,\"start\":43351},{\"end\":43371,\"start\":43366},{\"end\":43616,\"start\":43608},{\"end\":43630,\"start\":43625},{\"end\":43821,\"start\":43818},{\"end\":43836,\"start\":43829},{\"end\":43849,\"start\":43844},{\"end\":43862,\"start\":43859},{\"end\":43876,\"start\":43870},{\"end\":44154,\"start\":44150},{\"end\":44164,\"start\":44162},{\"end\":44179,\"start\":44174},{\"end\":44194,\"start\":44190},{\"end\":44795,\"start\":44787},{\"end\":44810,\"start\":44806},{\"end\":44818,\"start\":44815},{\"end\":44830,\"start\":44827},{\"end\":44843,\"start\":44840},{\"end\":44857,\"start\":44854},{\"end\":44869,\"start\":44864},{\"end\":44882,\"start\":44880},{\"end\":44886,\"start\":44884},{\"end\":45129,\"start\":45121},{\"end\":45142,\"start\":45138},{\"end\":45155,\"start\":45150},{\"end\":45159,\"start\":45157},{\"end\":45169,\"start\":45161},{\"end\":45974,\"start\":45972},{\"end\":45991,\"start\":45984},{\"end\":46008,\"start\":46000},{\"end\":46022,\"start\":46018},{\"end\":46028,\"start\":46024},{\"end\":46348,\"start\":46345},{\"end\":46362,\"start\":46359},{\"end\":46370,\"start\":46367},{\"end\":46386,\"start\":46381},{\"end\":46663,\"start\":46658},{\"end\":46676,\"start\":46673},{\"end\":46686,\"start\":46682},{\"end\":46701,\"start\":46697},{\"end\":46710,\"start\":46706},{\"end\":46720,\"start\":46718},{\"end\":46731,\"start\":46728},{\"end\":46741,\"start\":46737},{\"end\":47085,\"start\":47080},{\"end\":47099,\"start\":47092},{\"end\":47117,\"start\":47109},{\"end\":47131,\"start\":47127},{\"end\":47440,\"start\":47432},{\"end\":47457,\"start\":47452},{\"end\":47473,\"start\":47467},{\"end\":47489,\"start\":47484},{\"end\":47708,\"start\":47700},{\"end\":47722,\"start\":47716},{\"end\":47726,\"start\":47724},{\"end\":47914,\"start\":47907},{\"end\":47932,\"start\":47922},{\"end\":47949,\"start\":47944},{\"end\":48209,\"start\":48205},{\"end\":48219,\"start\":48217},{\"end\":48230,\"start\":48228},{\"end\":48246,\"start\":48241},{\"end\":48511,\"start\":48508},{\"end\":48525,\"start\":48520},{\"end\":48533,\"start\":48531},{\"end\":48805,\"start\":48802},{\"end\":48821,\"start\":48812},{\"end\":48837,\"start\":48832},{\"end\":48848,\"start\":48844},{\"end\":48862,\"start\":48858},{\"end\":48874,\"start\":48871},{\"end\":49188,\"start\":49185},{\"end\":49198,\"start\":49195},{\"end\":49206,\"start\":49203},{\"end\":49215,\"start\":49212},{\"end\":49485,\"start\":49482},{\"end\":49493,\"start\":49490},{\"end\":49505,\"start\":49503},{\"end\":49517,\"start\":49515},{\"end\":49779,\"start\":49776},{\"end\":49787,\"start\":49784},{\"end\":49796,\"start\":49793},{\"end\":49808,\"start\":49806},{\"end\":50091,\"start\":50088},{\"end\":50104,\"start\":50102},{\"end\":50114,\"start\":50111},{\"end\":50126,\"start\":50123},{\"end\":50133,\"start\":50131},{\"end\":50388,\"start\":50383},{\"end\":50405,\"start\":50398},{\"end\":50420,\"start\":50414},{\"end\":50438,\"start\":50429},{\"end\":50455,\"start\":50450},{\"end\":50732,\"start\":50724},{\"end\":50751,\"start\":50744},{\"end\":50765,\"start\":50761},{\"end\":51000,\"start\":50993},{\"end\":51017,\"start\":51011},{\"end\":51026,\"start\":51019},{\"end\":51257,\"start\":51244},{\"end\":51274,\"start\":51266},{\"end\":51294,\"start\":51291},{\"end\":51308,\"start\":51304},{\"end\":51325,\"start\":51318},{\"end\":51333,\"start\":51327},{\"end\":51665,\"start\":51658},{\"end\":51683,\"start\":51675},{\"end\":51700,\"start\":51691},{\"end\":51714,\"start\":51708},{\"end\":51728,\"start\":51724},{\"end\":52055,\"start\":52051},{\"end\":52068,\"start\":52063},{\"end\":52082,\"start\":52078},{\"end\":52094,\"start\":52090},{\"end\":52368,\"start\":52365},{\"end\":52378,\"start\":52375},{\"end\":52386,\"start\":52383},{\"end\":52395,\"start\":52393},{\"end\":52412,\"start\":52407},{\"end\":52421,\"start\":52418},{\"end\":52677,\"start\":52674},{\"end\":52685,\"start\":52682},{\"end\":52695,\"start\":52692},{\"end\":52704,\"start\":52702},{\"end\":52713,\"start\":52710},{\"end\":52730,\"start\":52725},{\"end\":53020,\"start\":53012},{\"end\":53038,\"start\":53030},{\"end\":53055,\"start\":53046},{\"end\":53069,\"start\":53063},{\"end\":53083,\"start\":53079},{\"end\":53396,\"start\":53393},{\"end\":53409,\"start\":53404},{\"end\":53419,\"start\":53417},{\"end\":53431,\"start\":53428},{\"end\":53684,\"start\":53679},{\"end\":53694,\"start\":53690},{\"end\":53709,\"start\":53705},{\"end\":53724,\"start\":53718},{\"end\":53740,\"start\":53733},{\"end\":53756,\"start\":53750},{\"end\":53931,\"start\":53927},{\"end\":54063,\"start\":54058},{\"end\":54081,\"start\":54073},{\"end\":54095,\"start\":54091},{\"end\":54373,\"start\":54371},{\"end\":54392,\"start\":54381},{\"end\":54411,\"start\":54402},{\"end\":54428,\"start\":54420},{\"end\":54444,\"start\":54436},{\"end\":54454,\"start\":54452},{\"end\":54473,\"start\":54464},{\"end\":54479,\"start\":54475},{\"end\":54813,\"start\":54806},{\"end\":54825,\"start\":54822},{\"end\":54997,\"start\":54994},{\"end\":55006,\"start\":55002},{\"end\":55019,\"start\":55014},{\"end\":55228,\"start\":55224},{\"end\":55236,\"start\":55233},{\"end\":55252,\"start\":55247},{\"end\":55265,\"start\":55260},{\"end\":55279,\"start\":55275},{\"end\":55290,\"start\":55286},{\"end\":55300,\"start\":55297},{\"end\":55311,\"start\":55309},{\"end\":55324,\"start\":55321},{\"end\":55339,\"start\":55335},{\"end\":55350,\"start\":55347},{\"end\":55360,\"start\":55356},{\"end\":55717,\"start\":55713},{\"end\":55729,\"start\":55726},{\"end\":55744,\"start\":55740},{\"end\":55756,\"start\":55754},{\"end\":56011,\"start\":56009},{\"end\":56020,\"start\":56017},{\"end\":56031,\"start\":56027},{\"end\":56041,\"start\":56038},{\"end\":56056,\"start\":56052},{\"end\":56067,\"start\":56065},{\"end\":56336,\"start\":56334},{\"end\":56352,\"start\":56347},{\"end\":56364,\"start\":56360},{\"end\":56379,\"start\":56375},{\"end\":56391,\"start\":56388},{\"end\":56728,\"start\":56726},{\"end\":56746,\"start\":56738},{\"end\":56758,\"start\":56755},{\"end\":56768,\"start\":56766},{\"end\":56782,\"start\":56778},{\"end\":56795,\"start\":56793},{\"end\":57127,\"start\":57118},{\"end\":57139,\"start\":57136},{\"end\":57153,\"start\":57148},{\"end\":57161,\"start\":57155},{\"end\":57398,\"start\":57390},{\"end\":57409,\"start\":57404},{\"end\":57422,\"start\":57417},{\"end\":57636,\"start\":57633},{\"end\":57647,\"start\":57643},{\"end\":57659,\"start\":57656},{\"end\":57668,\"start\":57665},{\"end\":57889,\"start\":57886},{\"end\":57901,\"start\":57898},{\"end\":57911,\"start\":57909},{\"end\":57921,\"start\":57918},{\"end\":57931,\"start\":57928},{\"end\":57945,\"start\":57942},{\"end\":57958,\"start\":57954},{\"end\":57970,\"start\":57967},{\"end\":58248,\"start\":58245},{\"end\":58261,\"start\":58259},{\"end\":58277,\"start\":58272},{\"end\":58288,\"start\":58285},{\"end\":58302,\"start\":58300},{\"end\":58319,\"start\":58314},{\"end\":58639,\"start\":58636},{\"end\":58652,\"start\":58648},{\"end\":58662,\"start\":58660},{\"end\":58676,\"start\":58671},{\"end\":58687,\"start\":58683},{\"end\":58697,\"start\":58694},{\"end\":58711,\"start\":58706},{\"end\":58722,\"start\":58719},{\"end\":58735,\"start\":58730},{\"end\":59075,\"start\":59072},{\"end\":59086,\"start\":59082},{\"end\":59097,\"start\":59094},{\"end\":59113,\"start\":59108},{\"end\":59127,\"start\":59123},{\"end\":59141,\"start\":59139},{\"end\":59421,\"start\":59419},{\"end\":59436,\"start\":59432},{\"end\":59448,\"start\":59445},{\"end\":59459,\"start\":59454},{\"end\":59470,\"start\":59466},{\"end\":59486,\"start\":59483},{\"end\":59797,\"start\":59795},{\"end\":59811,\"start\":59808},{\"end\":59823,\"start\":59820},{\"end\":60109,\"start\":60104},{\"end\":60123,\"start\":60119},{\"end\":60133,\"start\":60131},{\"end\":60364,\"start\":60359},{\"end\":60375,\"start\":60371},{\"end\":60383,\"start\":60380},{\"end\":60397,\"start\":60392},{\"end\":60411,\"start\":60408},{\"end\":60421,\"start\":60418},{\"end\":60434,\"start\":60430},{\"end\":60681,\"start\":60677},{\"end\":60692,\"start\":60689},{\"end\":60700,\"start\":60698},{\"end\":60719,\"start\":60708},{\"end\":60916,\"start\":60911},{\"end\":60928,\"start\":60924},{\"end\":60937,\"start\":60933},{\"end\":60952,\"start\":60948},{\"end\":60967,\"start\":60963},{\"end\":60976,\"start\":60972},{\"end\":61236,\"start\":61232},{\"end\":61250,\"start\":61244},{\"end\":61262,\"start\":61259},{\"end\":61274,\"start\":61271},{\"end\":61288,\"start\":61283},{\"end\":61559,\"start\":61555},{\"end\":61572,\"start\":61568},{\"end\":61592,\"start\":61582},{\"end\":61746,\"start\":61743},{\"end\":61758,\"start\":61755},{\"end\":61769,\"start\":61765},{\"end\":61783,\"start\":61778},{\"end\":61798,\"start\":61794},{\"end\":61810,\"start\":61805},{\"end\":61821,\"start\":61819},{\"end\":61831,\"start\":61829},{\"end\":61841,\"start\":61837}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":222070432},\"end\":43225,\"start\":42766},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3423879},\"end\":43566,\"start\":43227},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13072740},\"end\":43733,\"start\":43568},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":198169848},\"end\":44074,\"start\":43735},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53424263},\"end\":44379,\"start\":44076},{\"attributes\":{\"id\":\"b5\"},\"end\":44720,\"start\":44381},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":219634265},\"end\":45109,\"start\":44722},{\"attributes\":{\"id\":\"b7\"},\"end\":45272,\"start\":45111},{\"attributes\":{\"doi\":\"abs/1703.07737\",\"id\":\"b8\"},\"end\":45855,\"start\":45274},{\"attributes\":{\"id\":\"b9\"},\"end\":46240,\"start\":45857},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":222245729},\"end\":46584,\"start\":46242},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":244037148},\"end\":46955,\"start\":46586},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4646924},\"end\":47378,\"start\":46957},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":28772744},\"end\":47652,\"start\":47380},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6628106},\"end\":47832,\"start\":47654},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":209323955},\"end\":48117,\"start\":47834},{\"attributes\":{\"id\":\"b16\"},\"end\":48400,\"start\":48119},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":237108355},\"end\":48721,\"start\":48402},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":152282878},\"end\":49089,\"start\":48723},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233388081},\"end\":49405,\"start\":49091},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":662727},\"end\":49670,\"start\":49407},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3285930},\"end\":49997,\"start\":49672},{\"attributes\":{\"doi\":\"abs/2203.06558, 2022. 3\",\"id\":\"b22\"},\"end\":50331,\"start\":49999},{\"attributes\":{\"id\":\"b23\"},\"end\":50637,\"start\":50333},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4677070},\"end\":50939,\"start\":50639},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":18566850},\"end\":51157,\"start\":50941},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7693282},\"end\":51572,\"start\":51159},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12632343},\"end\":51933,\"start\":51574},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":222278406},\"end\":52304,\"start\":51935},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237532102},\"end\":52594,\"start\":52306},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":245144814},\"end\":52898,\"start\":52596},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3431445},\"end\":53321,\"start\":52900},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14165952},\"end\":53594,\"start\":53323},{\"attributes\":{\"id\":\"b33\"},\"end\":53923,\"start\":53596},{\"attributes\":{\"doi\":\"abs/2101.11228\",\"id\":\"b34\"},\"end\":53976,\"start\":53925},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4665276},\"end\":54264,\"start\":53978},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":44159390},\"end\":54742,\"start\":54266},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3050175},\"end\":54951,\"start\":54744},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":38904390},\"end\":55145,\"start\":54953},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":201124533},\"end\":55636,\"start\":55147},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13874338},\"end\":55939,\"start\":55638},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":207978701},\"end\":56241,\"start\":55941},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5854795},\"end\":56599,\"start\":56243},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":8207532},\"end\":57036,\"start\":56601},{\"attributes\":{\"id\":\"b44\"},\"end\":57325,\"start\":57038},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":481088},\"end\":57564,\"start\":57327},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":211011327},\"end\":57829,\"start\":57566},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":233689007},\"end\":58166,\"start\":57831},{\"attributes\":{\"doi\":\"122:1-122:17\",\"id\":\"b48\",\"matched_paper_id\":221090300},\"end\":58532,\"start\":58168},{\"attributes\":{\"id\":\"b49\"},\"end\":58978,\"start\":58534},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":244168393},\"end\":59346,\"start\":58980},{\"attributes\":{\"doi\":\"abs/2001.04193\",\"id\":\"b51\"},\"end\":59681,\"start\":59348},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1815453},\"end\":60026,\"start\":59683},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":235726536},\"end\":60291,\"start\":60028},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":115147653},\"end\":60623,\"start\":60293},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9873666},\"end\":60856,\"start\":60625},{\"attributes\":{\"id\":\"b56\"},\"end\":61143,\"start\":60858},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":50784357},\"end\":61505,\"start\":61145},{\"attributes\":{\"id\":\"b58\"},\"end\":61692,\"start\":61507},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":244906176},\"end\":62056,\"start\":61694}]", "bib_title": "[{\"end\":42873,\"start\":42766},{\"end\":43274,\"start\":43227},{\"end\":43598,\"start\":43568},{\"end\":43812,\"start\":43735},{\"end\":44140,\"start\":44076},{\"end\":44778,\"start\":44722},{\"end\":45490,\"start\":45274},{\"end\":46336,\"start\":46242},{\"end\":46649,\"start\":46586},{\"end\":47069,\"start\":46957},{\"end\":47423,\"start\":47380},{\"end\":47696,\"start\":47654},{\"end\":47896,\"start\":47834},{\"end\":48499,\"start\":48402},{\"end\":48796,\"start\":48723},{\"end\":49180,\"start\":49091},{\"end\":49472,\"start\":49407},{\"end\":49766,\"start\":49672},{\"end\":50714,\"start\":50639},{\"end\":50989,\"start\":50941},{\"end\":51231,\"start\":51159},{\"end\":51650,\"start\":51574},{\"end\":52042,\"start\":51935},{\"end\":52360,\"start\":52306},{\"end\":52669,\"start\":52596},{\"end\":53003,\"start\":52900},{\"end\":53382,\"start\":53323},{\"end\":54050,\"start\":53978},{\"end\":54369,\"start\":54266},{\"end\":54797,\"start\":54744},{\"end\":54981,\"start\":54953},{\"end\":55213,\"start\":55147},{\"end\":55705,\"start\":55638},{\"end\":56000,\"start\":55941},{\"end\":56325,\"start\":56243},{\"end\":56720,\"start\":56601},{\"end\":57379,\"start\":57327},{\"end\":57621,\"start\":57566},{\"end\":57874,\"start\":57831},{\"end\":58233,\"start\":58168},{\"end\":59060,\"start\":58980},{\"end\":59787,\"start\":59683},{\"end\":60092,\"start\":60028},{\"end\":60350,\"start\":60293},{\"end\":60667,\"start\":60625},{\"end\":60903,\"start\":60858},{\"end\":61223,\"start\":61145},{\"end\":61735,\"start\":61694}]", "bib_author": "[{\"end\":42886,\"start\":42875},{\"end\":42896,\"start\":42886},{\"end\":42914,\"start\":42896},{\"end\":42925,\"start\":42914},{\"end\":42933,\"start\":42925},{\"end\":42942,\"start\":42933},{\"end\":42954,\"start\":42942},{\"end\":42968,\"start\":42954},{\"end\":43295,\"start\":43276},{\"end\":43315,\"start\":43295},{\"end\":43330,\"start\":43315},{\"end\":43347,\"start\":43330},{\"end\":43360,\"start\":43347},{\"end\":43373,\"start\":43360},{\"end\":43618,\"start\":43600},{\"end\":43632,\"start\":43618},{\"end\":43823,\"start\":43814},{\"end\":43838,\"start\":43823},{\"end\":43851,\"start\":43838},{\"end\":43864,\"start\":43851},{\"end\":43878,\"start\":43864},{\"end\":44156,\"start\":44142},{\"end\":44166,\"start\":44156},{\"end\":44181,\"start\":44166},{\"end\":44196,\"start\":44181},{\"end\":44797,\"start\":44780},{\"end\":44812,\"start\":44797},{\"end\":44820,\"start\":44812},{\"end\":44832,\"start\":44820},{\"end\":44845,\"start\":44832},{\"end\":44859,\"start\":44845},{\"end\":44871,\"start\":44859},{\"end\":44884,\"start\":44871},{\"end\":44888,\"start\":44884},{\"end\":45131,\"start\":45113},{\"end\":45144,\"start\":45131},{\"end\":45157,\"start\":45144},{\"end\":45161,\"start\":45157},{\"end\":45171,\"start\":45161},{\"end\":45976,\"start\":45966},{\"end\":45993,\"start\":45976},{\"end\":46010,\"start\":45993},{\"end\":46024,\"start\":46010},{\"end\":46030,\"start\":46024},{\"end\":46350,\"start\":46338},{\"end\":46364,\"start\":46350},{\"end\":46372,\"start\":46364},{\"end\":46388,\"start\":46372},{\"end\":46665,\"start\":46651},{\"end\":46678,\"start\":46665},{\"end\":46688,\"start\":46678},{\"end\":46703,\"start\":46688},{\"end\":46712,\"start\":46703},{\"end\":46722,\"start\":46712},{\"end\":46733,\"start\":46722},{\"end\":46743,\"start\":46733},{\"end\":47087,\"start\":47071},{\"end\":47101,\"start\":47087},{\"end\":47119,\"start\":47101},{\"end\":47133,\"start\":47119},{\"end\":47442,\"start\":47425},{\"end\":47459,\"start\":47442},{\"end\":47475,\"start\":47459},{\"end\":47491,\"start\":47475},{\"end\":47710,\"start\":47698},{\"end\":47724,\"start\":47710},{\"end\":47728,\"start\":47724},{\"end\":47916,\"start\":47898},{\"end\":47934,\"start\":47916},{\"end\":47951,\"start\":47934},{\"end\":48211,\"start\":48199},{\"end\":48221,\"start\":48211},{\"end\":48232,\"start\":48221},{\"end\":48248,\"start\":48232},{\"end\":48513,\"start\":48501},{\"end\":48527,\"start\":48513},{\"end\":48535,\"start\":48527},{\"end\":48807,\"start\":48798},{\"end\":48823,\"start\":48807},{\"end\":48839,\"start\":48823},{\"end\":48850,\"start\":48839},{\"end\":48864,\"start\":48850},{\"end\":48876,\"start\":48864},{\"end\":49190,\"start\":49182},{\"end\":49200,\"start\":49190},{\"end\":49208,\"start\":49200},{\"end\":49217,\"start\":49208},{\"end\":49487,\"start\":49474},{\"end\":49495,\"start\":49487},{\"end\":49507,\"start\":49495},{\"end\":49519,\"start\":49507},{\"end\":49781,\"start\":49768},{\"end\":49789,\"start\":49781},{\"end\":49798,\"start\":49789},{\"end\":49810,\"start\":49798},{\"end\":50093,\"start\":50082},{\"end\":50106,\"start\":50093},{\"end\":50116,\"start\":50106},{\"end\":50128,\"start\":50116},{\"end\":50135,\"start\":50128},{\"end\":50390,\"start\":50375},{\"end\":50407,\"start\":50390},{\"end\":50422,\"start\":50407},{\"end\":50440,\"start\":50422},{\"end\":50457,\"start\":50440},{\"end\":50734,\"start\":50716},{\"end\":50753,\"start\":50734},{\"end\":50767,\"start\":50753},{\"end\":51002,\"start\":50991},{\"end\":51019,\"start\":51002},{\"end\":51028,\"start\":51019},{\"end\":51259,\"start\":51233},{\"end\":51276,\"start\":51259},{\"end\":51296,\"start\":51276},{\"end\":51310,\"start\":51296},{\"end\":51327,\"start\":51310},{\"end\":51335,\"start\":51327},{\"end\":51667,\"start\":51652},{\"end\":51685,\"start\":51667},{\"end\":51702,\"start\":51685},{\"end\":51716,\"start\":51702},{\"end\":51730,\"start\":51716},{\"end\":52057,\"start\":52044},{\"end\":52070,\"start\":52057},{\"end\":52084,\"start\":52070},{\"end\":52096,\"start\":52084},{\"end\":52370,\"start\":52362},{\"end\":52380,\"start\":52370},{\"end\":52388,\"start\":52380},{\"end\":52397,\"start\":52388},{\"end\":52414,\"start\":52397},{\"end\":52423,\"start\":52414},{\"end\":52679,\"start\":52671},{\"end\":52687,\"start\":52679},{\"end\":52697,\"start\":52687},{\"end\":52706,\"start\":52697},{\"end\":52715,\"start\":52706},{\"end\":52732,\"start\":52715},{\"end\":53022,\"start\":53005},{\"end\":53040,\"start\":53022},{\"end\":53057,\"start\":53040},{\"end\":53071,\"start\":53057},{\"end\":53085,\"start\":53071},{\"end\":53398,\"start\":53384},{\"end\":53411,\"start\":53398},{\"end\":53421,\"start\":53411},{\"end\":53433,\"start\":53421},{\"end\":53686,\"start\":53672},{\"end\":53696,\"start\":53686},{\"end\":53711,\"start\":53696},{\"end\":53726,\"start\":53711},{\"end\":53742,\"start\":53726},{\"end\":53758,\"start\":53742},{\"end\":53933,\"start\":53927},{\"end\":54065,\"start\":54052},{\"end\":54083,\"start\":54065},{\"end\":54097,\"start\":54083},{\"end\":54375,\"start\":54371},{\"end\":54394,\"start\":54375},{\"end\":54413,\"start\":54394},{\"end\":54430,\"start\":54413},{\"end\":54446,\"start\":54430},{\"end\":54456,\"start\":54446},{\"end\":54475,\"start\":54456},{\"end\":54481,\"start\":54475},{\"end\":54815,\"start\":54799},{\"end\":54827,\"start\":54815},{\"end\":54999,\"start\":54983},{\"end\":55008,\"start\":54999},{\"end\":55021,\"start\":55008},{\"end\":55230,\"start\":55215},{\"end\":55238,\"start\":55230},{\"end\":55254,\"start\":55238},{\"end\":55267,\"start\":55254},{\"end\":55281,\"start\":55267},{\"end\":55292,\"start\":55281},{\"end\":55302,\"start\":55292},{\"end\":55313,\"start\":55302},{\"end\":55326,\"start\":55313},{\"end\":55341,\"start\":55326},{\"end\":55352,\"start\":55341},{\"end\":55362,\"start\":55352},{\"end\":55719,\"start\":55707},{\"end\":55731,\"start\":55719},{\"end\":55746,\"start\":55731},{\"end\":55758,\"start\":55746},{\"end\":56013,\"start\":56002},{\"end\":56022,\"start\":56013},{\"end\":56033,\"start\":56022},{\"end\":56043,\"start\":56033},{\"end\":56058,\"start\":56043},{\"end\":56069,\"start\":56058},{\"end\":56338,\"start\":56327},{\"end\":56354,\"start\":56338},{\"end\":56366,\"start\":56354},{\"end\":56381,\"start\":56366},{\"end\":56393,\"start\":56381},{\"end\":56730,\"start\":56722},{\"end\":56748,\"start\":56730},{\"end\":56760,\"start\":56748},{\"end\":56770,\"start\":56760},{\"end\":56784,\"start\":56770},{\"end\":56797,\"start\":56784},{\"end\":57129,\"start\":57118},{\"end\":57141,\"start\":57129},{\"end\":57155,\"start\":57141},{\"end\":57163,\"start\":57155},{\"end\":57400,\"start\":57381},{\"end\":57411,\"start\":57400},{\"end\":57424,\"start\":57411},{\"end\":57638,\"start\":57623},{\"end\":57649,\"start\":57638},{\"end\":57661,\"start\":57649},{\"end\":57670,\"start\":57661},{\"end\":57891,\"start\":57876},{\"end\":57903,\"start\":57891},{\"end\":57913,\"start\":57903},{\"end\":57923,\"start\":57913},{\"end\":57933,\"start\":57923},{\"end\":57947,\"start\":57933},{\"end\":57960,\"start\":57947},{\"end\":57972,\"start\":57960},{\"end\":58250,\"start\":58235},{\"end\":58263,\"start\":58250},{\"end\":58279,\"start\":58263},{\"end\":58290,\"start\":58279},{\"end\":58304,\"start\":58290},{\"end\":58321,\"start\":58304},{\"end\":58641,\"start\":58626},{\"end\":58654,\"start\":58641},{\"end\":58664,\"start\":58654},{\"end\":58678,\"start\":58664},{\"end\":58689,\"start\":58678},{\"end\":58699,\"start\":58689},{\"end\":58713,\"start\":58699},{\"end\":58724,\"start\":58713},{\"end\":58737,\"start\":58724},{\"end\":59077,\"start\":59062},{\"end\":59088,\"start\":59077},{\"end\":59099,\"start\":59088},{\"end\":59115,\"start\":59099},{\"end\":59129,\"start\":59115},{\"end\":59143,\"start\":59129},{\"end\":59423,\"start\":59414},{\"end\":59438,\"start\":59423},{\"end\":59450,\"start\":59438},{\"end\":59461,\"start\":59450},{\"end\":59472,\"start\":59461},{\"end\":59488,\"start\":59472},{\"end\":59799,\"start\":59789},{\"end\":59813,\"start\":59799},{\"end\":59825,\"start\":59813},{\"end\":60111,\"start\":60094},{\"end\":60125,\"start\":60111},{\"end\":60135,\"start\":60125},{\"end\":60366,\"start\":60352},{\"end\":60377,\"start\":60366},{\"end\":60385,\"start\":60377},{\"end\":60399,\"start\":60385},{\"end\":60413,\"start\":60399},{\"end\":60423,\"start\":60413},{\"end\":60436,\"start\":60423},{\"end\":60683,\"start\":60669},{\"end\":60694,\"start\":60683},{\"end\":60702,\"start\":60694},{\"end\":60721,\"start\":60702},{\"end\":60918,\"start\":60905},{\"end\":60930,\"start\":60918},{\"end\":60939,\"start\":60930},{\"end\":60954,\"start\":60939},{\"end\":60969,\"start\":60954},{\"end\":60978,\"start\":60969},{\"end\":61238,\"start\":61225},{\"end\":61252,\"start\":61238},{\"end\":61264,\"start\":61252},{\"end\":61276,\"start\":61264},{\"end\":61290,\"start\":61276},{\"end\":61561,\"start\":61548},{\"end\":61574,\"start\":61561},{\"end\":61594,\"start\":61574},{\"end\":61748,\"start\":61737},{\"end\":61760,\"start\":61748},{\"end\":61771,\"start\":61760},{\"end\":61785,\"start\":61771},{\"end\":61800,\"start\":61785},{\"end\":61812,\"start\":61800},{\"end\":61823,\"start\":61812},{\"end\":61833,\"start\":61823},{\"end\":61843,\"start\":61833}]", "bib_venue": "[{\"end\":42978,\"start\":42968},{\"end\":43380,\"start\":43373},{\"end\":43636,\"start\":43632},{\"end\":43888,\"start\":43878},{\"end\":44200,\"start\":44196},{\"end\":44535,\"start\":44381},{\"end\":44892,\"start\":44888},{\"end\":45553,\"start\":45506},{\"end\":45964,\"start\":45857},{\"end\":46392,\"start\":46388},{\"end\":46747,\"start\":46743},{\"end\":47142,\"start\":47133},{\"end\":47495,\"start\":47491},{\"end\":47732,\"start\":47728},{\"end\":47955,\"start\":47951},{\"end\":48197,\"start\":48119},{\"end\":48539,\"start\":48535},{\"end\":48886,\"start\":48876},{\"end\":49238,\"start\":49217},{\"end\":49523,\"start\":49519},{\"end\":49818,\"start\":49810},{\"end\":50080,\"start\":49999},{\"end\":50373,\"start\":50333},{\"end\":50771,\"start\":50767},{\"end\":51032,\"start\":51028},{\"end\":51345,\"start\":51335},{\"end\":51736,\"start\":51730},{\"end\":52102,\"start\":52096},{\"end\":52427,\"start\":52423},{\"end\":52736,\"start\":52732},{\"end\":53094,\"start\":53085},{\"end\":53437,\"start\":53433},{\"end\":53670,\"start\":53596},{\"end\":54101,\"start\":54097},{\"end\":54490,\"start\":54481},{\"end\":54830,\"start\":54827},{\"end\":55029,\"start\":55021},{\"end\":55372,\"start\":55362},{\"end\":55768,\"start\":55758},{\"end\":56073,\"start\":56069},{\"end\":56403,\"start\":56393},{\"end\":56806,\"start\":56797},{\"end\":57116,\"start\":57038},{\"end\":57429,\"start\":57424},{\"end\":57680,\"start\":57670},{\"end\":57982,\"start\":57972},{\"end\":58337,\"start\":58333},{\"end\":58624,\"start\":58534},{\"end\":59147,\"start\":59143},{\"end\":59412,\"start\":59348},{\"end\":59829,\"start\":59825},{\"end\":60139,\"start\":60135},{\"end\":60440,\"start\":60436},{\"end\":60724,\"start\":60721},{\"end\":60982,\"start\":60978},{\"end\":61299,\"start\":61290},{\"end\":61546,\"start\":61507},{\"end\":61847,\"start\":61843}]"}}}, "year": 2023, "month": 12, "day": 17}