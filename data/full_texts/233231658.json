{"id": 233231658, "updated": "2023-10-06 05:00:14.589", "metadata": {"title": "Image Manipulation Detection by Multi-View Multi-Scale Supervision", "authors": "[{\"first\":\"Xinru\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Chengbo\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Jiaqi\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Juan\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Xirong\",\"last\":\"Li\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 4, "day": 14}, "abstract": "The key challenge of image manipulation detection is how to learn generalizable features that are sensitive to manipulations in novel data, whilst specific to prevent false alarms on authentic images. Current research emphasizes the sensitivity, with the specificity overlooked. In this paper we address both aspects by multi-view feature learning and multi-scale supervision. By exploiting noise distribution and boundary artifact surrounding tampered regions, the former aims to learn semantic-agnostic and thus more generalizable features. The latter allows us to learn from authentic images which are nontrivial to be taken into account by current semantic segmentation network based methods. Our thoughts are realized by a new network which we term MVSS-Net. Extensive experiments on five benchmark sets justify the viability of MVSS-Net for both pixel-level and image-level manipulation detection.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.06832", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/ChenDJC021", "doi": "10.1109/iccv48922.2021.01392"}}, "content": {"source": {"pdf_hash": "77cdbb1ed5ae6604dadc83f16e6d193bb5b35750", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.06832v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "298e99e4c2abc58e92311319fb4fe11887817f1d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/77cdbb1ed5ae6604dadc83f16e6d193bb5b35750.txt", "contents": "\nImage Manipulation Detection by Multi-View Multi-Scale Supervision\n\n\nXinru Chen \nSchool of Information\nAIMC Lab\nRenmin University of China\n\n\nChengbo Dong \nSchool of Information\nAIMC Lab\nRenmin University of China\n\n\nJiaqi Ji \nSchool of Information\nAIMC Lab\nRenmin University of China\n\n\nJuan Cao \nInstitute of Computing Technology\nChinese Academy of Sciences\n\n\nState Key Laboratory of Media Convergence Production Technology and Systems\n\n\nXirong Li \nSchool of Information\nAIMC Lab\nRenmin University of China\n\n\nMoe Key \n\nLab of Data Engineering and Knowledge Engineering\nRenmin University of China\n\n\nImage Manipulation Detection by Multi-View Multi-Scale Supervision\n\nThe key challenge of image manipulation detection is how to learn generalizable features that are sensitive to manipulations in novel data, whilst specific to prevent false alarms on authentic images. Current research emphasizes the sensitivity, with the specificity overlooked. In this paper we address both aspects by multi-view feature learning and multi-scale supervision. By exploiting noise distribution and boundary artifact surrounding tampered regions, the former aims to learn semantic-agnostic and thus more generalizable features. The latter allows us to learn from authentic images which are nontrivial to be taken into account by current semantic segmentation network based methods. Our thoughts are realized by a new network which we term MVSS-Net. Extensive experiments on five benchmark sets justify the viability of MVSS-Net for both pixellevel and image-level manipulation detection.\n\nIntroduction\n\nDigital images can now be manipulated with ease and often in a visually imperceptible manner [11]. Copy-move (copy and move elements from one region to another region in a given image), splicing (copy elements from one image and paste them on another image) and inpainting (removal of unwanted elements) are three common types of image manipulation that could lead to misinterpretation of the visual content [1,19,23]. This paper targets at auto-detection of images subjected to these types of manipulation. We aim to not only discriminate manipulated images from the authentic, but also pinpoint tampered regions at the pixel level.\n\nUnsurprisingly, the state-of-the-arts are deep learning based [14,21,26,27,29], specifically focusing on pixellevel manipulation detection [21,26,29]. With only two *Xinru Chen and Chengbo Dong contribute equally to this work.\n\n\u2020Corresponding author: Xirong Li (xirong@ruc.edu.cn) Figure 1. Image manipulation detection by the state-of-thearts. The first three rows are copy-move, splicing and inpainting, followed by three authentic images (thus with blank mask). Our model strikes a good balance between sensitivity and specificity.\n\nclasses (manipulated versus authentic) in consideration, the task appears to be a simplified case of image semantic segmentation. However, an off-the-shelf semantic segmentation network is suboptimal for the task, as it is designed to capture semantic information, making the network datasetdependent and do not generalize. Prior research [29] reports that DeepLabv2 [4] trained on the CASIAv2 dataset [8] performs well on the CAISAv1 dataset [7] homologous to CASIAv2, yet performs poorly on the non-homologous COVER dataset [25]. A similar behavior of FCN [18] is also observed in this study. Hence, the key question is how to design and train a deep neural network capable of learning semantic-agnostic features that are sensitive to manipulations, whilst specific to prevent false alarms? In order to learn semantic-agnostic features, image content has to be suppressed. Depending on at what stage Figure 2. Conceptual diagram of the proposed MVSS-Net model. We use the edge-supervised branch and the noise-sensitive branch to learn semantic-agnostic features for manipulation detection, and multi-scale supervision to strike a balance between model sensitivity and specificity. Non-trainable layers such as sigmoid (\u03c3) and global max pooling (GMP) are shown in gray.\n\nthe suppression occurs, we categorize existing methods into two groups, i.e. noise-view methods [14,16,26,27,30] and edge-supervised methods [21,29]. Given the hypothesis that novel elements introduced by slicing and/or inpainting differ from the authentic part in terms of their noise distributions, the first group of methods aim to exploit such discrepancy. The noise map of an input image, generatedfirs either by pre-defined high-pass filters [9] or by their trainable counterparts [2,16], is fed into a deep network, either alone [16,27] or together with the input image [14,26,30]. Note that the methods are ineffective for detecting copymove which introduces no new element. The second group of methods concentrate on finding boundary artifact as manipulation trace around a tampered region, implemented by adding an auxiliary branch to reconstruct the region's edge [21,29]. Note that the prior art [29] uniformly concatenates features from different layers of the backbone as input of the auxiliary branch. As such, there is a risk that deeper-layer features, which are responsible for manipulation detection, remain semantic-aware and thus not generalizable.\n\nTo measure a model's generalizability, a common evaluation protocol [14,21,26,29] is to first train the model on a public dataset, say CASIAv2 [8], and then test it on other public datasets such as NIST16 [12], Columbia [13], and CASIAv1 [7]. To our surprise, however, the evaluation is performed exclusively on manipulated images, with metrics w.r.t pixel-level manipulation detection reported. The specificity of the model, which reveals how it handles authentic images and is thus crucial for real-world usability, is ignored. As is shown in Fig. 1, their serious false alarm over authentic images leads to unavailability in practical work.\n\nIn fact, as current methods [14,21,26] mainly use pixelwise segmentation losses to which an authentic example can contribute is marginal, it is nontrivial for these methods to improve their specificity by learning from the authentic.\n\nInspired by the Border Network [28], which aggregates features progressively to predict object boundaries, and Le-sionNet [24] that incorporates an image classification loss for retinal lesion segmentation, we propose multi-view feature learning with multi-scale supervision for image manipulation detection. To the best of our knowledge (Table 1), we are the first to jointly exploit the noise view and the boundary artifact to learn manipulation detection features. Moreover, such a joint exploitation is nontrivial. To combine the best of the two worlds, new network structures are needed. Our contributions are as follows:\n\n\u2022 We propose MVSS-Net as a new network for image manipulation detection. As shown in Fig. 2, MVSS-Net contains novel elements designed for learning semantic-agnostic and thus more generalizable features.\n\n\u2022 We train MVSS-Net with multi-scale supervision, allowing us to learn from authentic images, which are ignored by the prior art, and consequently improve the model specificity substantially.\n\n\u2022 Extensive experiments on two training sets and five test sets show that MVSS-Net compares favorably against the state-of-the-art. Code and models are available at https: //github.com/dong03/MVSS-Net.\n\n\nRelated Work\n\nThis paper is inspired by a number of recent works that made novel attempts to learn semantic-agnostic features for image manipulation detection, see Table 1. In what follows, we describe in brief how these attempts are implemented and explain our novelties accordingly. We focus on deep learning approaches to copy-move / splicing / inpainting detection. For the detection of low-level manipulations such as Gaussian Blur and JPEG compression, we refer to [2]. In order to suppress the content information, Li and Huang [16] propose to implement an FCN's first convolutional layer with trainable high-pass filters and apply their HP-FCN for inpainting detection. Yang et al. use Ba-yarConv as the initial convolution layer of their CR-CNN [27]. Although such constrained convolutional layers are helpful for extracting noise information, using them alone brings in the risk of losing other useful information in the original RGB input. Hence, we see an increasing number of works on exploiting information from both the RGB view and the noise view [14,26,30]. Zhou et al. [30] develop a two-stream Faster R-CNN, coined RGB-N, which takes as input the RGB image and its noise counterpart generated by the SRM filter [9]. Wu et al. [26] and Hu et al. [14] use both BayarConv and SRM. Given features from distinct views, the need for feature fusion is on. Feature concatenation at an early stage is adopted by [14,26]. Our MVSS-Net is more close to RGB-N as both perform feature fusion at the late stage. However, different from the non-trainable bilinear pooling used in RGB-N, Dual Attention used in MVSS-Net is trainable and is thus more selective.\n\nAs manipulating a specific region in a given image inevitably leaves traces between the tampered region and its surrounding, how to exploit such edge artifact also matters for manipulation detection. Salloum et al. develop a multitask FCN to symmetrically predict a tampered area and its boundary [21]. In a more recent work [29], Zhou et al. introduce an edge detection and refinement branch which takes as input features at different levels. Given that region segmentation and edge detection are intrinsically two distinct tasks, the challenge lies in how to strike a proper balance between the two. Directly using deeper features for edge detection as done in [21] has the risk of affecting the main task of manipulation segmentation, while putting all features together as used in [29] may let the deeper features be ignored by the edge branch. Our MVSS-Net has an edgesupervised branch that effectively resolves these issues.\n\nLast but not least, we observe that the specificity of an image manipulation detector, i.e. how it responses to authentic images, is seldom reported. In fact, the mainstream solutions are developed within an image semantic segmentation network. Naturally, they are trained and also evaluated on manipulated images in the context of manipulation segmentation [29]. The absence of authentic images both in the training and test stages naturally raises concerns regarding the specificity of the detector. In this paper we make a novel attempt to include authentic images for training and test, an important step towards real-world deployment.\n\n\nProposed Model\n\nGiven an RGB image x of size W \u00d7 H \u00d7 3, we aim for a multi-head deep network G that not only determines whether the image has been manipulated, but also pinpoints its manipulated pixels. Let G(x) be the network-estimated probability of the image being manipulated. In a similar manner we define G(x i ) as pixel-wise probability, with i = 1, . . . , W \u00d7 H. Accordingly, we denote a full-size segmentation map as {G(x i )}. As the image-level decision is naturally subject to pixel-level evidence, we obtain G(x) by Global Max Pooling (GMP) over the segmentation map, i.e.\nG(x) \u2190 GMP ({G(x i )}) .(1)\nIn order to extract generalizable manipulation detection features, we present a new network that accepts both RGB and noise views of the input image. To strike a proper balance between detection sensitivity and specificity, the multi-view feature learning process is jointly supervised by annotations of three scales, i.e. pixel, edge and image.\n\n\nMulti-View Feature Learning\n\nAs shown in Fig. 2, MVSS-Net consists of two branches, with ResNet-50 as their backbones. The edge-supervised branch (ESB) at the top is specifically designed to exploit subtle boundary artifact around tampered regions, whilst the noise-sensitive branch (NSB) at the bottom aims to capture the inconsistency between tampered and authentic regions. Both clues are meant to be semantic-agnostic.\n\n\nEdge-Supervised Branch\n\nIdeally, with edge supervision, we hope the response area of the network will be more concentrated on tampered regions. Designing such an edge-supervised network is nontrivial. As noted in Section 2, the main challenge is how to construct an appropriate input for the edge detection head. On one hand, directly using features from the last ResNet block is problematic, as this will enforce the deep features to capture low-level edge patterns and consequently affect the main task of manipulation segmentation. While on the other hand, using features from the initial blocks is also questionable, as subtle edge patterns contained in these shallow features can vanish with ease after multiple deep convolutions. A joint use of both shallow and deep features is thus necessary. However, we argue that simple feature concatenation as previously used in [29] is suboptimal, as the features are mixed and there is no guarantee that the deeper features will receive adequate supervision from the edge head. To conquer the challenge, we propose to construct the input of the edge head in a shallow-to-deep manner.\n\nAs illustrated in Fig. 2, features from different ResNet blocks are combined in a progressive manner for manipulation edge detection. In order to enhance edge-related patterns, we introduce a Sobel layer, see Fig. 3(a). Features from the i-th block first go through the Sobel layer followed by an edge residual block (ERB), see Fig. 3(b), before they are combined (by summation) with their counterparts from the next block. To prevent the effect of accumulation, the combined features go through another ERB (top in Fig. 2) before the next round of feature combination. We believe such a mechanism helps prevent extreme cases in which deeper features are over-supervised or fully ignored by the edge head. By visualizing feature maps of the last ResNet block in Fig. 4, we observe that the proposed ESB indeed produces a more focused response near tampered regions.\n\nThe output of ESB has two parts: feature maps from the last ResNet block, denoted as {f esb,1 , . . . , f esb,k }, to be used for the main task, and the predicted manipulation edge map, denoted as {G edge (x i )}, obtained by transforming the output of the last ERB with a sigmoid (\u03c3) layer. The  data-flow of this branch is conceptually expressed by Eq. 2,\n[f esb,1 ,. . . , f esb,k ] {G edge (x i )} \u2190 ERB-ResNet(x).(2)\n\nNoise-Sensitive Branch\n\nIn order to fully exploit the noise view, we build a noisesensitive branch (NSB) parallel to ESB. NSB is implemented as a standard FCN (another ResNet-50 as its backbone). Regarding the choice of noise extraction, we adopt BayarConv [2], which is found to be better than the SRM filter [27]. The output of this branch is an array of k feature maps from the last ResNet block of its backbone, i.e.\n\n{f nsb,1 , . . . , f nsb,k } \u2190 ResNet(BayarConv(x)).\n\n\nBranch Fusion by Dual Attention\n\nGiven two arrays of feature maps {f esb,1 , . . . , f esb,k } and {f nsb,1 , . . . , f nsb,k } from ESB and NSB, we propose to fuse them by a trainable Dual Attention (DA) module [10]. This is new, because previous work [30] uses bilinear pooling for feature fusion, which is non-trainable.\n\nThe DA module has two attention mechanisms working in parallel: channel attention (CA) and position attention (PA), see  Figure 5. Dual Attention , with its channel attention module shown in blue and its position attention module shown in green.\n{G (x i )} \u2190 DA([f esb,1 ,. . .,f esb,k ,f nsb,1 , . . . , f nsb,k ]), {G(x i )} \u2190 \u03c3(bilinear-upsampling({G (x i )})).(4)\n\nMulti-Scale Supervision\n\nWe consider losses at three scales, each with its own target, i.e. a pixel-scale loss for improving the model's sensitivity for pixel-level manipulation detection, an edge loss for learning semantic-agnostic features and an image-scale loss for improving the model's specificity for image-level manipulation detection.\n\nPixel-scale loss. As manipulated pixels are typically in minority in a given image, we use the Dice loss, found to be effective for learning from extremely imbalanced data [24]:\nloss seg (x) = 1 \u2212 2 \u00b7 W \u00d7H i=1 G(x i ) \u00b7 y i W \u00d7H i=1 G 2 (x i ) + W \u00d7H i=1 y 2 i ,(5)\nwhere y i \u2208 {0, 1} is a binary label indicating whether the i-th pixel is manipulated. Edge loss. As pixels of an edge are overwhelmed by non-edge pixels, we again use the Dice loss for manipulation edge detection, denoted as loss edg . Since manipulation edge detection is an auxiliary task, we do not compute the loss edg at the full size of W \u00d7 H. Instead, the loss is computed at a much smaller size of W 4 \u00d7 H 4 , see Fig. 2. This strategy reduces computational cost during training, and in the meanwhile, improves the performance slightly.\n\nImage-scale loss. In order to reduce false alarms, authentic images have to be taken into account in the training stage. This is however nontrivial for the current works [16,21,26,29] as they all rely on segmentation losses. Consider the widely used binary cross-entropy (BCE) loss for instance. An authentic image with a small percent of its pixels misclassified contributes marginally to the BCE loss, making it difficult to effectively reduce false alarms. Also note that the Dice loss cannot handle the authentic image by definition. Therefore, an image-scale loss is needed. We adopt the image-scale BCE loss:\nloss clf (x) = \u2212(y \u00b7log G(x)+(1\u2212y)\u00b7log(1\u2212G(x))) (6)\nwhere y = max({y i }).\n\nCombined loss. We use a convex combination of the three losses: Loss = \u03b1 \u00b7 loss seg + \u03b2 \u00b7 loss clf + (1 \u2212 \u03b1 \u2212 \u03b2) \u00b7 loss edg (7) where \u03b1, \u03b2 \u2208 (0, 1) are weights. Note that authentic images are only used to compute loss clf .\n\n\nExperiments\n\n\nExperimental Setup\n\nDatasets. For the ease of a head-to-head comparison with the state-of-the-art, we adopt CASIAv2 [8] for training and COVER [25], Columbia [13], NIST16 [12] and CASIAv1 [7] for testing. Meanwhile, we notice DEFACTO [19], a recently released large-scale dataset, containing 149k images sampled from MS-COCO [17] and automanipulated by copy-move, splicing and inpainting. Considering the challenging nature of DEFACTO, we choose to perform our ablation study on this new set. As the set has no authentic images, we construct a training set termed DEFACTO-84k, by randomly sampling 64k positive images from DEFACTO and 20k negative images from MS-COCO. In a similar manner, we build a test set termed DEFACTO-12k, by randomly sampling 6k positive images from the remaining part of DEFACTO and 6k negatives from MS-COCO. Note that to avoid any data leakage, for manipulated images used for training (test), their source images are not included in the test (training) set. In total, our experiments use two training sets and five test sets, see Table 2.\n\n\nDataset\n\nNegative Positive cpmv spli inpa Training DEFACTO-84k [19] 20,000 64,417 12,777 34,133 17,507 CASIAv2 [8] 7,491 5,063 3,235 1,828 0 Test COVER [25] 100 100 100 0 0 Columbia [13] 183 180 0 180 0 NIST16 [12] 0 564 68 288 208 CASIAv1 [7] 800 920 459 461 0 DEFACTO-12k [19] 6,000 6,000 2,000 2,000 2,000 Evaluation Criteria. For pixel-level manipulation detection, following previous works [21,29,30], we compute pixel-level precision and recall, and report their F1. For image-level manipulation detection, in order to measure the miss detection rate and false alarm rate, we report sensitivity, specificity and their F1. AUC, as a decision-thresholdfree metric, is also reported. Authentic images per test set are only used for image-level evaluation. For both pixellevel and image-level F1 computation, the default threshold is 0.5, unless otherwise stated.\n\nThe overall performance is measured by Com-F1, defined as the harmonic mean of pixel-level and image-level F1. Com-F1 is sensitive to the lowest value of pixel-F1 and image-F1. In particular, it scores 0 when either pixel-F1 or image-F1 is 0, which does not hold for the arithmetic mean.\n\nImplementation. MVSS-Net is implemented in PyTorch and trained on an NVIDIA Tesla V100 GPU. The input size is 512 \u00d7 512. The two ResNet-50 used in ESB and NSB are initialized with ImageNet-pretrained counterparts. We use an Adam [15] optimizer with a learning rate periodically decays from 10 \u22124 to 10 \u22127 . We set the two weights in the combined loss as \u03b1 = 0.16 and \u03b2 = 0.04, according to the model performance on a held-out validation set from DE-FACTO. We apply regular data augmentation for training, including flipping, blurring, compression and naive manipulations either by cropping and pasting a squared area or using built-in OpenCV inpainting functions [3,22].\n\n\nAblation Study\n\nFor revealing the influence of the individual components, we evaluate the performance of the proposed model in varied setups with the components added progressively.\n\nWe depart from FCN-16 without multi-view multiscale supervision. Recall that we use a DA module for branch fusion. So for a fair comparison, we adopt FCN-16 with DA, making it essentially an implementation of DANet [10]. The improved FCN-16 scores better than its standard counterpart, e.g. UNet [20], DeepLabv3 [5] and DeepLabv3+ [6], see the supplement. This competitive baseline is referred to as Seg in Table 3.\n\nInfluence of the image classification loss. Comparing Seg+Clf and Seg, we see a clear increase in specificity and a clear drop in sensitivity, suggesting that adding loss clf makes the model more conservative for reporting manipulation. This change is not only confirmed by lower pixellevel performance, but is also observed in the fourth column of Fig. 6, showing that manipulated areas predicted by Seg+Clf are much reduced. Influence of NSB. Since Seg+Clf+N is obtained by adding NSB into Seg+Clf , its better performance verifies the effectiveness of NSB for improving manipulation detection at both pixel-level and image-level.\n\nInfluence of ESB. The better performance of Seg+Clf+E against Seg+Clf justifies the effectiveness of ESB. Seg+Clf+E/s is obtained by removing the Sobel operation from Seg+Clf+E, so its performance degeneration in particular on copy-move detection (from 0.405 to 0.382, cmpv in Table 3) indicates the necessity of this operation.\n\nESB versus GSR-Net. Seg+Clf+G is obtained by replacing our ESB with the edge branch of GSR-Net. The overall performance of Seg+Clf+G is lower than Seg+Clf+E. Moreover, there is a larger performance gap on cmpv (ESB of 0.405 versus GSR-Net of 0.363). The results clearly demonstrate the superiority of the proposed ESB over the prior art.\n\nInfluence of two branch fusion. The full setup, with ESB and NSB fused by dual attention, performs the best, showing the complementarity of the individual components.\n\nTo further justify the necessity of our dual attention based fusion, we make an alternative solution which ensembles Seg+Clf+N and Seg+Clf+E by model averaging, refereed to as Ensemble(N,E). The full setup is better than Ensemble(N,E), showing the advantage of our fusion method 1 . Fig. 6 shows some qualitative results. From the left to right, the results demonstrate how MVSS-Net strikes a good balance between sensitivity and specificity. Note that the best pixel-level performance of FCN is due to the fact that the training and test sets are homologous. Next, we evaluate the generalizability of FCN and MVSS-Net.\n\n\nComparison with State-of-the-art\n\nBaselines. For a fair and reproducible comparison, we have to be selective, choosing the state-of-the-art that meets one of the following three criteria: 1) pre-trained models released by paper authors, 2) source code publicly available, or 3) following a common evaluation protocol where CASIAv2 is used for training and other public datasets are used for testing. Accordingly, we compile a list of six published baselines as follows:\n\n\u2022 Models available: HP-FCN [16], trained on a private set of inpainted images 2 , ManTra-Net [26], trained on a private set of millions of manipulated images 3 , and CR-CNN [27], trained on CASIAv2 4 . We use these models directly.\n\n\u2022 Code available: GSR-Net [29], which we train using author-provided code 5 . We cite their results where appropriate and use our re-trained model only when necessary.\n\n\u2022 Same evaluation protocol: MFCN [21], RGB-N [30] with numbers quoted from the same team [29].\n\nWe re-train FCN (Seg) and MVSS-Net(full setup) from scratch on CASIAv2. Pixel-level manipulation detection. The performance of distinct models is given in Table 4. MVSS-Net is the best in terms of overall performance. We attribute the clearly better performance of ManTra-Net on DEFACTO-12k to its large-scale training data, which was also originated from MS-COCO as DEFACTO-12k. As MVSS-Net is derived from FCN, its superior performance in this cross-dataset setting justifies its better generalizability.\n\nAs HP-FCN is specially designed for inpainting detection, we narrow down the comparison to detecting the inpainting subsets in NIST16 and DEFACTO-12k. Again, MVSS-Net outperforms HP-FCN: 0.565 versus 0.284 on NIST16 and 0.391 versus 0.106 on DEFACTO-12k.    Table 6. Com-F1, the harmonic mean of pixel-level F1 and image-level F1, on four test sets.\n\nImage-level manipulation detection. Table 5 shows the performance of distinct models, all using the default decision threshold of 0.5. MVSS-Net is again the top performer. With its capability of learning from authentic images, MVSS-Net obtains higher specificity (and thus lower false alarm rate) on most test sets. Our model also has the best AUC scores, meaning it is better than the baselines on a wide range of operation points.\n\nThe overall performance on both pixel-level and imagelevel manipulation detection is provided in Table 6.\n\nRobustness evaluation. JPEG compression and Gaussian blur are separately applied on CASIAv1. ManTra-Net used a wide range of data augmentations including compression, while CR-CNN and GSR-Net did not use such data augmentation. So for a more fair comparison, we also train MVSS-Net with compression and blurring excluded from data augmentation, denoted as MVSS-Net (w/o aug). Performance curves in Fig. 7 show better robustness of MVSS-Net and MVSS-Net (w/o aug).\n\nEfficiency test. We measure the inference efficiency in terms of frames per second (FPS) . Tested on NVIDIA Tesla V100 GPU, CR-CNN, ManTra-Net and GSR-Net run at FPS of 3.1, 2.8 and 31.7, respectively. MVSS-Net runs at FPS of 20.1, sufficient for real-time application.\n\n\nConclusions\n\nOur image manipulation detection experiments on five benchmark sets allow us to draw the following conclusions. For learning semantic-agnostic features, both noise and edge information are helpful, whilst the latter is better when used alone. For exploiting the edge information, our proposed edge-supervised branch (ESB) is more effective than the previously used feature concatenation. ESB steers the network to be more concentrated on tampered regions. Regarding the specificity of manipulation detection, we empirically show that the state-of-the-arts suffer from poor specificity. The inclusion of the image classification loss improves the specificity, yet at the cost of a clear performance drop for pixel-level manipulation detection. Multi-view feature learning has to be used together with multi-scale supervision. The resultant MVSS-Net is a new state-of-the-art for image manipulation detection.\n\nFigure 3 .\n3Diagrams of (a) Sobel layer and (b) edge residual block, used in ESB for manipulation edge detection.\n\nFigure 4 .\n4Visualization of averaged feature maps of the last ResNet block, brighter color indicating a higher response. Manipulation from the top to bottom is inpainting, copy-move and splicing. Read from the third column are w/o edge, i.e. ResNet without any edge residual block, GSR-Net, i.e. ResNet with the GSR-Net alike edge branch, and the proposed ESB, which produces a more focused response near tampered regions.\n\nFig. 5 .\n5CA associates channel-wise features to selectively emphasize interdependent channel feature maps. Meanwhile, PA selectively updates features at each position by a weighted sum of the features at all positions. The outputs of CA and PA are summed up, and transformed into a feature map of size W 16 \u00d7 H 16 , denoted as {G (x i )}, by a 1 \u00d7 1 convolution. With parameter-free bilinear upsampling followed by an element-wise sigmoid function, {G (x i )} is transformed into the final segmentation map {G(x i )}. Fusion by dual attention is conceptually expressed as\n\nFigure 6 .\n6Pixel-level manipulation detection results of MVSS-Net in varied setups. The test image in the last row is authentic.\n\n1Figure 7 .\n7Comparison to fusion by bilinear pooling is in the supplement. 2 https://github.com/lihaod/Deep_inpainting_ localization 3 https://github.com/ISICV/ManTraNet 4 https://github.com/HuizhouLi/Constrained-R-CNN 5 https://github.com/pengzhou1108/GSRNet (a) Performance curves w.r.t. JPEG compression (b) Performance curves w.t.r. Gaussian Blurs Robustness evaluation against JPEG compression and Gaussian Blurs on CASIAv1.\n\nTable 2 .\n2Two training sets and five test sets in our experiments.DEFACTO-84k and DEFACTO-12k are used for training and test \nin the ablation study (Section 4.2), while for the SOTA comparison \n(Section 4.3) we train on CASIAv2 and evaluate on all test sets. \n\n\n\n\nMethod Optimal threshold per model & testset Fixed threshold (0.5) NIST Columbia CASIAv1 COVER DEFACTO-12k MEAN NIST Columbia CASIAv1 COVER DEFACTO-12k MEANTable 4. Performance of pixel-level manipulation detection. Best result per test set is shown in bold. All the models are trained on CASIAv2, except for ManTra-Net and HP-FCN.MFCN [21] \n0.422 0.612 \n0.541 \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a \nRGB-N [30] \nn.a. \nn.a. \n0.408 \n0.379 \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \nn.a \nHP-FCN [16] \n0.360 0.471 \n0.214 \n0.199 \n0.136 \n0.276 0.121 0.067 \n0.154 \n0.003 \n0.055 \n0.080 \nManTra-Net [26] 0.455 0.709 \n0.692 \n0.772 \n0.618 \n0.649 0.000 0.364 \n0.155 \n0.286 \n0.155 \n0.192 \nCR-CNN [27] \n0.428 0.704 \n0.662 \n0.470 \n0.340 \n0.521 0.238 0.436 \n0.405 \n0.291 \n0.132 \n0.300 \nGSR-Net [29] \n0.456 0.622 \n0.574 \n0.489 \n0.379 \n0.504 0.283 0.613 \n0.387 \n0.285 \n0.051 \n0.324 \n\nFCN \n0.507 0.586 \n0.742 \n0.573 \n0.401 \n0.562 0.167 0.223 \n0.441 \n0.199 \n0.130 \n0.232 \nMVSS-Net \n0.737 0.703 \n0.753 \n0.824 \n0.572 \n0.718 0.292 0.638 \n0.452 \n0.453 \n0.137 \n0.394 \n\nMethod \nColumbia \nCASIAv1 \nCOVER \nDEFACTO-12k \nAUC Sen. Spe. \nF1 \nAUC Sen. Spe. \nF1 \nAUC Sen. Spe. \nF1 \nAUC Sen. Spe. \n\n\nTable 5 .\n5Performance of image-level manipulation detection on Columbia, CASIAv1, COVER and DEFACTO-12k. Sen.: sensitivity. Spe.: specificity. NIST16, which has no authentic images, is excluded. The default decision threshold of 0.5 is used for all models.Method \nColumbia CASIAv1 COVER DEFACTO-12k \nManrTra-Net [26] \n0.000 \n0.000 \n0.000 \n0.000 \nCR-CNN [27] \n0.413 \n0.382 \n0.181 \n0.198 \nGSR-Net [29] \n0.042 \n0.042 \n0.000 \n0.004 \nFCN \n0.305 \n0.562 \n0.189 \n0.203 \nMVSS-Net \n0.711 \n0.565 \n0.317 \n0.205 \n\n\n\nExploiting spatial structure for localizing manipulated image regions. J Bappy, A Roy-Chowdhury, J Bunk, L Nataraj, B Manjunath, ICCV. 13J. Bappy, A. Roy-Chowdhury, J. Bunk, L. Nataraj, and B. Manjunath. Exploiting spatial structure for localizing ma- nipulated image regions. In ICCV, 2017. 1, 3\n\nConstrained convolutional neural networks: A new approach towards general purpose image manipulation detection. B Bayar, M Stamm, IEEE Transactions on Information Forensics and Security. 13114B. Bayar and M. Stamm. Constrained convolutional neural networks: A new approach towards general purpose image manipulation detection. IEEE Transactions on Information Forensics and Security, 13(11):2691-2706, 2018. 2, 3, 4\n\nNavier-stokes, fluid dynamics, and image and video inpainting. M Bertalmio, A Bertozzi, G Sapiro, CPVR. M. Bertalmio, A. Bertozzi, and G. Sapiro. Navier-stokes, fluid dynamics, and image and video inpainting. In CPVR, 2001. 6\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L Chen, G Papandreou, I Kokkinos, K Murphy, A Yuille, IEEE Transactions on Pattern Analysis and Machine Intelligence. 404L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834-848, 2018. 1\n\nRethinking atrous convolution for semantic image segmentation. L Chen, G Papandreou, F Schroff, H Adam, CVPR. L. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethink- ing atrous convolution for semantic image segmentation. In CVPR, 2017. 6\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. L Chen, Y Zhu, G Papandreou, F Schroff, H Adam, ECCV. L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-decoder with atrous separable convolution for se- mantic image segmentation. In ECCV, 2018. 6\n\nCasia image tampering detection evaluation database. J Dong, W Wang, T Tan, 6J. Dong, W. Wang, and T. Tan. Casia image tamper- ing detection evaluation database. http://forensics. idealtest.org, 2010. 1, 2, 5, 6\n\nCasia image tampering detection evaluation database. J Dong, W Wang, T Tan, ChinaSIP. 6J. Dong, W. Wang, and T. Tan. Casia image tampering de- tection evaluation database. In ChinaSIP, 2013. 1, 2, 5, 6\n\nRich models for steganalysis of digital images. J Fridrich, J Kodovsky, IEEE Transactions on Information Forensics and Security. 733J. Fridrich and J. Kodovsky. Rich models for steganalysis of digital images. IEEE Transactions on Information Forensics and Security, 7(3):868-882, 2012. 2, 3\n\nDual attention network for scene segmentation. J Fu, J Liu, H Tian, Y Li, Y Bao, Z Fang, H Lu, CVPR. 56J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu. Dual attention network for scene segmentation. In CVPR, 2019. 5, 6\n\nWish you were here: Context-aware human generation. O Gafni, L Wolf, CVPR. O. Gafni and L. Wolf. Wish you were here: Context-aware human generation. In CVPR, 2020. 1\n\nMfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. H Guan, M Kozak, E Robertson, Y Lee, A N Yates, A Delgado, D Zhou, T Kheyrkhah, J Smith, J Fiscus, WACV Workshop. 56H. Guan, M. Kozak, E. Robertson, Y. Lee, A. N. Yates, A. Delgado, D. Zhou, T. Kheyrkhah, J. Smith, and J. Fis- cus. Mfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. In WACV Workshop, 2019. 2, 5, 6\n\nColumbia uncompressed image splicing detection evaluation dataset. J Hsu, 56J. Hsu. Columbia uncompressed image splic- ing detection evaluation dataset. https://www. ee.columbia.edu/ln/dvmm/downloads/ AuthSplicedDataSet/AuthSplicedDataSet. htm, 2009. 2, 5, 6\n\nSpan: Spatial pyramid attention network forimage manipulation localization. X Hu, Z Zhang, Z Jiang, S Chaudhuri, Z Yang, R Nevatia, ECCV, 2020. 1. 23X. Hu, Z. Zhang, Z. Jiang, S. Chaudhuri, Z. Yang, and R. Nevatia. Span: Spatial pyramid attention network forimage manipulation localization. In ECCV, 2020. 1, 2, 3\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, Computer Science. 6D. Kingma and J. Ba. Adam: A method for stochastic opti- mization. Computer Science, 2014. 6\n\nLocalization of deep inpainting using high-pass fully convolutional network. H Li, J Huang, ICCV. 7H. Li and J. Huang. Localization of deep inpainting using high-pass fully convolutional network. In ICCV, 2019. 2, 3, 5, 7, 8\n\nMicrosoft coco: Common objects in context. T Lin, M Maire, S Belongie, J Hays, C Zitnick, ECCV. T. Lin, M. Maire, S. Belongie, J. Hays, and C. Zitnick. Mi- crosoft coco: Common objects in context. In ECCV, 2014. 5\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Transactions on Pattern Analysis and Machine Intelligence. 394J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):640-651, 2015. 1\n\nDefacto: Image and face manipulation dataset. G Mahfoudi, B Tajini, F Retraint, F Morain-Nicolier, M Pic, EUSIPCO. 6G. Mahfoudi, B. Tajini, F. Retraint, F. Morain-Nicolier, and M. Pic. Defacto: Image and face manipulation dataset. In EUSIPCO, 2019. 1, 5, 6\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MIC-CAIO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In MIC- CAI, 2015. 6\n\nImage splicing localization using a multi-task fully convolutional network (mfcn). R Salloum, Y Ren, C Kuo, 51Journal of Visual Communication and Image RepresentationR. Salloum, Y. Ren, and C. Kuo. Image splicing localiza- tion using a multi-task fully convolutional network (mfcn). Journal of Visual Communication and Image Representation, 51(feb.):201-209, 2017. 1, 2, 3, 5, 6, 7, 8\n\nAn image inpainting technique based on the fast marching method. A Telea, Journal of Graphics Tools. 91A. Telea. An image inpainting technique based on the fast marching method. Journal of Graphics Tools, 9(1):23-34, 2004. 6\n\nMedia forensics and deepfakes: An overview. L Verdoliva, IEEE Journal of Selected Topics in Signal Processing. 145L. Verdoliva. Media forensics and deepfakes: An overview. IEEE Journal of Selected Topics in Signal Processing, 14(5):910-932, 2020. 1\n\nLearn to segment retinal lesions and beyond. Q Wei, X Li, W Yu, X Zhang, Y Zhang, B Hu, B Mo, D Gong, N Chen, D Ding, Y Chen, ICPR, 2020. 25Q. Wei, X. Li, W. Yu, X. Zhang, Y. Zhang, B. Hu, B. Mo, D. Gong, N. Chen, D. Ding, and Y. Chen. Learn to segment retinal lesions and beyond. In ICPR, 2020. 2, 5\n\nCoverage-a novel database for copy-move forgery detection. B Wen, Y Zhu, R Subramanian, T T Ng, S Winkler, ICIP. 6B. Wen, Y. Zhu, R. Subramanian, T. T. Ng, and S. Winkler. Coverage-a novel database for copy-move forgery detection. In ICIP, 2016. 1, 5, 6\n\nMantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. Y Wu, W Abdalmageed, P Natarajan, CVPR. 7Y. Wu, W. AbdAlmageed, and P. Natarajan. Mantra-net: Ma- nipulation tracing network for detection and localization of image forgeries with anomalous features. In CVPR, 2019. 1, 2, 3, 5, 7, 8\n\nConstrained r-cnn: A general image manipulation detection model. C Yang, H Li, F Lin, B Jiang, H Zhao, ICME. 7C. Yang, H. Li, F. Lin, B. Jiang, and H. Zhao. Constrained r-cnn: A general image manipulation detection model. In ICME, 2020. 1, 2, 3, 5, 7, 8\n\nLearning a discriminative feature network for semantic segmentation. C Yu, J Wang, C Peng, C Gao, G Yu, N Sang, CVPR. C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. Learn- ing a discriminative feature network for semantic segmenta- tion. In CVPR, 2018. 2\n\nGenerate, segment, and refine: Towards generic manipulation segmentation. P Zhou, B Chen, X Han, M Najibi, L Davis, AAAI. 7P. Zhou, B. Chen, X. Han, M. Najibi, and L. Davis. Generate, segment, and refine: Towards generic manipulation segmen- tation. In AAAI, 2020. 1, 2, 3, 4, 5, 6, 7, 8\n\nLearning rich features for image manipulation detection. P Zhou, X Han, V I Morariu, L S Davis, CVPR. 7P. Zhou, X. Han, VI. Morariu, and LS. Davis. Learning rich features for image manipulation detection. In CVPR, 2018. 2, 3, 5, 6, 7, 8\n", "annotations": {"author": "[{\"end\":141,\"start\":70},{\"end\":215,\"start\":142},{\"end\":285,\"start\":216},{\"end\":437,\"start\":286},{\"end\":508,\"start\":438},{\"end\":517,\"start\":509},{\"end\":597,\"start\":518}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":76},{\"end\":154,\"start\":150},{\"end\":224,\"start\":222},{\"end\":294,\"start\":291},{\"end\":447,\"start\":445},{\"end\":516,\"start\":513}]", "author_first_name": "[{\"end\":75,\"start\":70},{\"end\":149,\"start\":142},{\"end\":221,\"start\":216},{\"end\":290,\"start\":286},{\"end\":444,\"start\":438},{\"end\":512,\"start\":509}]", "author_affiliation": "[{\"end\":140,\"start\":82},{\"end\":214,\"start\":156},{\"end\":284,\"start\":226},{\"end\":358,\"start\":296},{\"end\":436,\"start\":360},{\"end\":507,\"start\":449},{\"end\":596,\"start\":519}]", "title": "[{\"end\":67,\"start\":1},{\"end\":664,\"start\":598}]", "venue": null, "abstract": "[{\"end\":1568,\"start\":666}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1681,\"start\":1677},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1995,\"start\":1992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1998,\"start\":1995},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2001,\"start\":1998},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2285,\"start\":2281},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2288,\"start\":2285},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2291,\"start\":2288},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2294,\"start\":2291},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2297,\"start\":2294},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2362,\"start\":2358},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2365,\"start\":2362},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2368,\"start\":2365},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3098,\"start\":3094},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3160,\"start\":3157},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3201,\"start\":3198},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3285,\"start\":3281},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3317,\"start\":3313},{\"end\":3665,\"start\":3657},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4128,\"start\":4124},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4131,\"start\":4128},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4134,\"start\":4131},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4137,\"start\":4134},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4140,\"start\":4137},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4173,\"start\":4169},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4176,\"start\":4173},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4479,\"start\":4476},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4518,\"start\":4515},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4521,\"start\":4518},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4568,\"start\":4564},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4571,\"start\":4568},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4609,\"start\":4605},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4612,\"start\":4609},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4615,\"start\":4612},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4907,\"start\":4903},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4910,\"start\":4907},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4940,\"start\":4936},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5271,\"start\":5267},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5274,\"start\":5271},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5277,\"start\":5274},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5280,\"start\":5277},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5345,\"start\":5342},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5408,\"start\":5404},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5423,\"start\":5419},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5440,\"start\":5437},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5876,\"start\":5872},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5879,\"start\":5876},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5882,\"start\":5879},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6114,\"start\":6110},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6205,\"start\":6201},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7783,\"start\":7780},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7848,\"start\":7844},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8067,\"start\":8063},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8376,\"start\":8372},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8379,\"start\":8376},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8382,\"start\":8379},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8400,\"start\":8396},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8542,\"start\":8539},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8577,\"start\":8573},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8735,\"start\":8731},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8738,\"start\":8735},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9275,\"start\":9271},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9303,\"start\":9299},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9641,\"start\":9637},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9763,\"start\":9759},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12816,\"start\":12812},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14620,\"start\":14617},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14674,\"start\":14670},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15053,\"start\":15049},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15094,\"start\":15090},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16052,\"start\":16048},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16863,\"start\":16859},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16866,\"start\":16863},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16869,\"start\":16866},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16872,\"start\":16869},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17739,\"start\":17736},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17767,\"start\":17763},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17782,\"start\":17778},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17795,\"start\":17791},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17811,\"start\":17808},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17858,\"start\":17854},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17949,\"start\":17945},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18757,\"start\":18753},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18804,\"start\":18801},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18846,\"start\":18842},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18876,\"start\":18872},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18904,\"start\":18900},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18933,\"start\":18930},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18968,\"start\":18964},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19089,\"start\":19085},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19092,\"start\":19089},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19094,\"start\":19092},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20079,\"start\":20075},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20512,\"start\":20509},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20515,\"start\":20512},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20921,\"start\":20917},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21002,\"start\":20998},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21017,\"start\":21014},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21036,\"start\":21033},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23714,\"start\":23710},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23780,\"start\":23776},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23860,\"start\":23856},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23946,\"start\":23942},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23991,\"start\":23990},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24122,\"start\":24118},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24134,\"start\":24130},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24178,\"start\":24174}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27353,\"start\":27239},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27778,\"start\":27354},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28352,\"start\":27779},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28483,\"start\":28353},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28915,\"start\":28484},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29179,\"start\":28916},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30360,\"start\":29180},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30864,\"start\":30361}]", "paragraph": "[{\"end\":2217,\"start\":1584},{\"end\":2445,\"start\":2219},{\"end\":2753,\"start\":2447},{\"end\":4026,\"start\":2755},{\"end\":5197,\"start\":4028},{\"end\":5842,\"start\":5199},{\"end\":6077,\"start\":5844},{\"end\":6705,\"start\":6079},{\"end\":6910,\"start\":6707},{\"end\":7103,\"start\":6912},{\"end\":7306,\"start\":7105},{\"end\":8972,\"start\":7323},{\"end\":9904,\"start\":8974},{\"end\":10545,\"start\":9906},{\"end\":11135,\"start\":10564},{\"end\":11509,\"start\":11164},{\"end\":11934,\"start\":11541},{\"end\":13068,\"start\":11961},{\"end\":13935,\"start\":13070},{\"end\":14294,\"start\":13937},{\"end\":14780,\"start\":14384},{\"end\":14834,\"start\":14782},{\"end\":15160,\"start\":14870},{\"end\":15407,\"start\":15162},{\"end\":15874,\"start\":15556},{\"end\":16053,\"start\":15876},{\"end\":16687,\"start\":16142},{\"end\":17303,\"start\":16689},{\"end\":17378,\"start\":17356},{\"end\":17603,\"start\":17380},{\"end\":18687,\"start\":17640},{\"end\":19555,\"start\":18699},{\"end\":19844,\"start\":19557},{\"end\":20516,\"start\":19846},{\"end\":20700,\"start\":20535},{\"end\":21117,\"start\":20702},{\"end\":21751,\"start\":21119},{\"end\":22081,\"start\":21753},{\"end\":22420,\"start\":22083},{\"end\":22588,\"start\":22422},{\"end\":23209,\"start\":22590},{\"end\":23681,\"start\":23246},{\"end\":23914,\"start\":23683},{\"end\":24083,\"start\":23916},{\"end\":24179,\"start\":24085},{\"end\":24687,\"start\":24181},{\"end\":25038,\"start\":24689},{\"end\":25472,\"start\":25040},{\"end\":25579,\"start\":25474},{\"end\":26044,\"start\":25581},{\"end\":26315,\"start\":26046},{\"end\":27238,\"start\":26331}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11163,\"start\":11136},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14358,\"start\":14295},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15529,\"start\":15408},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16141,\"start\":16054},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17355,\"start\":17304}]", "table_ref": "[{\"end\":6426,\"start\":6417},{\"end\":7480,\"start\":7473},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18686,\"start\":18679},{\"end\":21116,\"start\":21109},{\"end\":22037,\"start\":22030},{\"end\":24343,\"start\":24336},{\"end\":24954,\"start\":24947},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25083,\"start\":25076},{\"end\":25578,\"start\":25571}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1582,\"start\":1570},{\"attributes\":{\"n\":\"2.\"},\"end\":7321,\"start\":7309},{\"attributes\":{\"n\":\"3.\"},\"end\":10562,\"start\":10548},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11539,\"start\":11512},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":11959,\"start\":11937},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":14382,\"start\":14360},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":14868,\"start\":14837},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15554,\"start\":15531},{\"attributes\":{\"n\":\"4.\"},\"end\":17617,\"start\":17606},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17638,\"start\":17620},{\"end\":18697,\"start\":18690},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20533,\"start\":20519},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23244,\"start\":23212},{\"attributes\":{\"n\":\"5.\"},\"end\":26329,\"start\":26318},{\"end\":27250,\"start\":27240},{\"end\":27365,\"start\":27355},{\"end\":27788,\"start\":27780},{\"end\":28364,\"start\":28354},{\"end\":28496,\"start\":28485},{\"end\":28926,\"start\":28917},{\"end\":30371,\"start\":30362}]", "table": "[{\"end\":29179,\"start\":28984},{\"end\":30360,\"start\":29513},{\"end\":30864,\"start\":30619}]", "figure_caption": "[{\"end\":27353,\"start\":27252},{\"end\":27778,\"start\":27367},{\"end\":28352,\"start\":27790},{\"end\":28483,\"start\":28366},{\"end\":28915,\"start\":28498},{\"end\":28984,\"start\":28928},{\"end\":29513,\"start\":29182},{\"end\":30619,\"start\":30373}]", "figure_ref": "[{\"end\":2508,\"start\":2500},{\"end\":5750,\"start\":5744},{\"end\":6798,\"start\":6792},{\"end\":11559,\"start\":11553},{\"end\":13094,\"start\":13088},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13285,\"start\":13279},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13404,\"start\":13398},{\"end\":13592,\"start\":13586},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13838,\"start\":13832},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15291,\"start\":15283},{\"end\":16571,\"start\":16565},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21474,\"start\":21468},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22879,\"start\":22873},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25985,\"start\":25979}]", "bib_author_first_name": "[{\"end\":30938,\"start\":30937},{\"end\":30947,\"start\":30946},{\"end\":30964,\"start\":30963},{\"end\":30972,\"start\":30971},{\"end\":30983,\"start\":30982},{\"end\":31277,\"start\":31276},{\"end\":31286,\"start\":31285},{\"end\":31645,\"start\":31644},{\"end\":31658,\"start\":31657},{\"end\":31670,\"start\":31669},{\"end\":31922,\"start\":31921},{\"end\":31930,\"start\":31929},{\"end\":31944,\"start\":31943},{\"end\":31956,\"start\":31955},{\"end\":31966,\"start\":31965},{\"end\":32370,\"start\":32369},{\"end\":32378,\"start\":32377},{\"end\":32392,\"start\":32391},{\"end\":32403,\"start\":32402},{\"end\":32632,\"start\":32631},{\"end\":32640,\"start\":32639},{\"end\":32647,\"start\":32646},{\"end\":32661,\"start\":32660},{\"end\":32672,\"start\":32671},{\"end\":32899,\"start\":32898},{\"end\":32907,\"start\":32906},{\"end\":32915,\"start\":32914},{\"end\":33112,\"start\":33111},{\"end\":33120,\"start\":33119},{\"end\":33128,\"start\":33127},{\"end\":33310,\"start\":33309},{\"end\":33322,\"start\":33321},{\"end\":33601,\"start\":33600},{\"end\":33607,\"start\":33606},{\"end\":33614,\"start\":33613},{\"end\":33622,\"start\":33621},{\"end\":33628,\"start\":33627},{\"end\":33635,\"start\":33634},{\"end\":33643,\"start\":33642},{\"end\":33836,\"start\":33835},{\"end\":33845,\"start\":33844},{\"end\":34037,\"start\":34036},{\"end\":34045,\"start\":34044},{\"end\":34054,\"start\":34053},{\"end\":34067,\"start\":34066},{\"end\":34074,\"start\":34073},{\"end\":34076,\"start\":34075},{\"end\":34085,\"start\":34084},{\"end\":34096,\"start\":34095},{\"end\":34104,\"start\":34103},{\"end\":34117,\"start\":34116},{\"end\":34126,\"start\":34125},{\"end\":34455,\"start\":34454},{\"end\":34724,\"start\":34723},{\"end\":34730,\"start\":34729},{\"end\":34739,\"start\":34738},{\"end\":34748,\"start\":34747},{\"end\":34761,\"start\":34760},{\"end\":34769,\"start\":34768},{\"end\":35007,\"start\":35006},{\"end\":35017,\"start\":35016},{\"end\":35213,\"start\":35212},{\"end\":35219,\"start\":35218},{\"end\":35405,\"start\":35404},{\"end\":35412,\"start\":35411},{\"end\":35421,\"start\":35420},{\"end\":35433,\"start\":35432},{\"end\":35441,\"start\":35440},{\"end\":35633,\"start\":35632},{\"end\":35641,\"start\":35640},{\"end\":35654,\"start\":35653},{\"end\":35961,\"start\":35960},{\"end\":35973,\"start\":35972},{\"end\":35983,\"start\":35982},{\"end\":35995,\"start\":35994},{\"end\":36014,\"start\":36013},{\"end\":36238,\"start\":36237},{\"end\":36253,\"start\":36252},{\"end\":36264,\"start\":36263},{\"end\":36492,\"start\":36491},{\"end\":36503,\"start\":36502},{\"end\":36510,\"start\":36509},{\"end\":36860,\"start\":36859},{\"end\":37065,\"start\":37064},{\"end\":37316,\"start\":37315},{\"end\":37323,\"start\":37322},{\"end\":37329,\"start\":37328},{\"end\":37335,\"start\":37334},{\"end\":37344,\"start\":37343},{\"end\":37353,\"start\":37352},{\"end\":37359,\"start\":37358},{\"end\":37365,\"start\":37364},{\"end\":37373,\"start\":37372},{\"end\":37381,\"start\":37380},{\"end\":37389,\"start\":37388},{\"end\":37632,\"start\":37631},{\"end\":37639,\"start\":37638},{\"end\":37646,\"start\":37645},{\"end\":37661,\"start\":37660},{\"end\":37663,\"start\":37662},{\"end\":37669,\"start\":37668},{\"end\":37944,\"start\":37943},{\"end\":37950,\"start\":37949},{\"end\":37965,\"start\":37964},{\"end\":38242,\"start\":38241},{\"end\":38250,\"start\":38249},{\"end\":38256,\"start\":38255},{\"end\":38263,\"start\":38262},{\"end\":38272,\"start\":38271},{\"end\":38501,\"start\":38500},{\"end\":38507,\"start\":38506},{\"end\":38515,\"start\":38514},{\"end\":38523,\"start\":38522},{\"end\":38530,\"start\":38529},{\"end\":38536,\"start\":38535},{\"end\":38768,\"start\":38767},{\"end\":38776,\"start\":38775},{\"end\":38784,\"start\":38783},{\"end\":38791,\"start\":38790},{\"end\":38801,\"start\":38800},{\"end\":39040,\"start\":39039},{\"end\":39048,\"start\":39047},{\"end\":39055,\"start\":39054},{\"end\":39057,\"start\":39056},{\"end\":39068,\"start\":39067},{\"end\":39070,\"start\":39069}]", "bib_author_last_name": "[{\"end\":30944,\"start\":30939},{\"end\":30961,\"start\":30948},{\"end\":30969,\"start\":30965},{\"end\":30980,\"start\":30973},{\"end\":30993,\"start\":30984},{\"end\":31283,\"start\":31278},{\"end\":31292,\"start\":31287},{\"end\":31655,\"start\":31646},{\"end\":31667,\"start\":31659},{\"end\":31677,\"start\":31671},{\"end\":31927,\"start\":31923},{\"end\":31941,\"start\":31931},{\"end\":31953,\"start\":31945},{\"end\":31963,\"start\":31957},{\"end\":31973,\"start\":31967},{\"end\":32375,\"start\":32371},{\"end\":32389,\"start\":32379},{\"end\":32400,\"start\":32393},{\"end\":32408,\"start\":32404},{\"end\":32637,\"start\":32633},{\"end\":32644,\"start\":32641},{\"end\":32658,\"start\":32648},{\"end\":32669,\"start\":32662},{\"end\":32677,\"start\":32673},{\"end\":32904,\"start\":32900},{\"end\":32912,\"start\":32908},{\"end\":32919,\"start\":32916},{\"end\":33117,\"start\":33113},{\"end\":33125,\"start\":33121},{\"end\":33132,\"start\":33129},{\"end\":33319,\"start\":33311},{\"end\":33331,\"start\":33323},{\"end\":33604,\"start\":33602},{\"end\":33611,\"start\":33608},{\"end\":33619,\"start\":33615},{\"end\":33625,\"start\":33623},{\"end\":33632,\"start\":33629},{\"end\":33640,\"start\":33636},{\"end\":33646,\"start\":33644},{\"end\":33842,\"start\":33837},{\"end\":33850,\"start\":33846},{\"end\":34042,\"start\":34038},{\"end\":34051,\"start\":34046},{\"end\":34064,\"start\":34055},{\"end\":34071,\"start\":34068},{\"end\":34082,\"start\":34077},{\"end\":34093,\"start\":34086},{\"end\":34101,\"start\":34097},{\"end\":34114,\"start\":34105},{\"end\":34123,\"start\":34118},{\"end\":34133,\"start\":34127},{\"end\":34459,\"start\":34456},{\"end\":34727,\"start\":34725},{\"end\":34736,\"start\":34731},{\"end\":34745,\"start\":34740},{\"end\":34758,\"start\":34749},{\"end\":34766,\"start\":34762},{\"end\":34777,\"start\":34770},{\"end\":35014,\"start\":35008},{\"end\":35020,\"start\":35018},{\"end\":35216,\"start\":35214},{\"end\":35225,\"start\":35220},{\"end\":35409,\"start\":35406},{\"end\":35418,\"start\":35413},{\"end\":35430,\"start\":35422},{\"end\":35438,\"start\":35434},{\"end\":35449,\"start\":35442},{\"end\":35638,\"start\":35634},{\"end\":35651,\"start\":35642},{\"end\":35662,\"start\":35655},{\"end\":35970,\"start\":35962},{\"end\":35980,\"start\":35974},{\"end\":35992,\"start\":35984},{\"end\":36011,\"start\":35996},{\"end\":36018,\"start\":36015},{\"end\":36250,\"start\":36239},{\"end\":36261,\"start\":36254},{\"end\":36269,\"start\":36265},{\"end\":36500,\"start\":36493},{\"end\":36507,\"start\":36504},{\"end\":36514,\"start\":36511},{\"end\":36866,\"start\":36861},{\"end\":37075,\"start\":37066},{\"end\":37320,\"start\":37317},{\"end\":37326,\"start\":37324},{\"end\":37332,\"start\":37330},{\"end\":37341,\"start\":37336},{\"end\":37350,\"start\":37345},{\"end\":37356,\"start\":37354},{\"end\":37362,\"start\":37360},{\"end\":37370,\"start\":37366},{\"end\":37378,\"start\":37374},{\"end\":37386,\"start\":37382},{\"end\":37394,\"start\":37390},{\"end\":37636,\"start\":37633},{\"end\":37643,\"start\":37640},{\"end\":37658,\"start\":37647},{\"end\":37666,\"start\":37664},{\"end\":37677,\"start\":37670},{\"end\":37947,\"start\":37945},{\"end\":37962,\"start\":37951},{\"end\":37975,\"start\":37966},{\"end\":38247,\"start\":38243},{\"end\":38253,\"start\":38251},{\"end\":38260,\"start\":38257},{\"end\":38269,\"start\":38264},{\"end\":38277,\"start\":38273},{\"end\":38504,\"start\":38502},{\"end\":38512,\"start\":38508},{\"end\":38520,\"start\":38516},{\"end\":38527,\"start\":38524},{\"end\":38533,\"start\":38531},{\"end\":38541,\"start\":38537},{\"end\":38773,\"start\":38769},{\"end\":38781,\"start\":38777},{\"end\":38788,\"start\":38785},{\"end\":38798,\"start\":38792},{\"end\":38807,\"start\":38802},{\"end\":39045,\"start\":39041},{\"end\":39052,\"start\":39049},{\"end\":39065,\"start\":39058},{\"end\":39076,\"start\":39071}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7901560},\"end\":31162,\"start\":30866},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":46899325},\"end\":31579,\"start\":31164},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":695955},\"end\":31806,\"start\":31581},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3429309},\"end\":32304,\"start\":31808},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":22655199},\"end\":32546,\"start\":32306},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3638670},\"end\":32843,\"start\":32548},{\"attributes\":{\"id\":\"b6\"},\"end\":33056,\"start\":32845},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14008455},\"end\":33259,\"start\":33058},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9751504},\"end\":33551,\"start\":33261},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52180375},\"end\":33781,\"start\":33553},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":218763400},\"end\":33948,\"start\":33783},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":61807685},\"end\":34385,\"start\":33950},{\"attributes\":{\"id\":\"b12\"},\"end\":34645,\"start\":34387},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":221447878},\"end\":34960,\"start\":34647},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6628106},\"end\":35133,\"start\":34962},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":204955047},\"end\":35359,\"start\":35135},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14113767},\"end\":35574,\"start\":35361},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1629541},\"end\":35912,\"start\":35576},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":198905531},\"end\":36170,\"start\":35914},{\"attributes\":{\"id\":\"b19\"},\"end\":36406,\"start\":36172},{\"attributes\":{\"id\":\"b20\"},\"end\":36792,\"start\":36408},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5908881},\"end\":37018,\"start\":36794},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":210838881},\"end\":37268,\"start\":37020},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":209501064},\"end\":37570,\"start\":37270},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15201962},\"end\":37825,\"start\":37572},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":195503148},\"end\":38174,\"start\":37827},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":208158448},\"end\":38429,\"start\":38176},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13756368},\"end\":38691,\"start\":38431},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53778351},\"end\":38980,\"start\":38693},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":44149078},\"end\":39218,\"start\":38982}]", "bib_title": "[{\"end\":30935,\"start\":30866},{\"end\":31274,\"start\":31164},{\"end\":31642,\"start\":31581},{\"end\":31919,\"start\":31808},{\"end\":32367,\"start\":32306},{\"end\":32629,\"start\":32548},{\"end\":33109,\"start\":33058},{\"end\":33307,\"start\":33261},{\"end\":33598,\"start\":33553},{\"end\":33833,\"start\":33783},{\"end\":34034,\"start\":33950},{\"end\":34721,\"start\":34647},{\"end\":35004,\"start\":34962},{\"end\":35210,\"start\":35135},{\"end\":35402,\"start\":35361},{\"end\":35630,\"start\":35576},{\"end\":35958,\"start\":35914},{\"end\":36857,\"start\":36794},{\"end\":37062,\"start\":37020},{\"end\":37313,\"start\":37270},{\"end\":37629,\"start\":37572},{\"end\":37941,\"start\":37827},{\"end\":38239,\"start\":38176},{\"end\":38498,\"start\":38431},{\"end\":38765,\"start\":38693},{\"end\":39037,\"start\":38982}]", "bib_author": "[{\"end\":30946,\"start\":30937},{\"end\":30963,\"start\":30946},{\"end\":30971,\"start\":30963},{\"end\":30982,\"start\":30971},{\"end\":30995,\"start\":30982},{\"end\":31285,\"start\":31276},{\"end\":31294,\"start\":31285},{\"end\":31657,\"start\":31644},{\"end\":31669,\"start\":31657},{\"end\":31679,\"start\":31669},{\"end\":31929,\"start\":31921},{\"end\":31943,\"start\":31929},{\"end\":31955,\"start\":31943},{\"end\":31965,\"start\":31955},{\"end\":31975,\"start\":31965},{\"end\":32377,\"start\":32369},{\"end\":32391,\"start\":32377},{\"end\":32402,\"start\":32391},{\"end\":32410,\"start\":32402},{\"end\":32639,\"start\":32631},{\"end\":32646,\"start\":32639},{\"end\":32660,\"start\":32646},{\"end\":32671,\"start\":32660},{\"end\":32679,\"start\":32671},{\"end\":32906,\"start\":32898},{\"end\":32914,\"start\":32906},{\"end\":32921,\"start\":32914},{\"end\":33119,\"start\":33111},{\"end\":33127,\"start\":33119},{\"end\":33134,\"start\":33127},{\"end\":33321,\"start\":33309},{\"end\":33333,\"start\":33321},{\"end\":33606,\"start\":33600},{\"end\":33613,\"start\":33606},{\"end\":33621,\"start\":33613},{\"end\":33627,\"start\":33621},{\"end\":33634,\"start\":33627},{\"end\":33642,\"start\":33634},{\"end\":33648,\"start\":33642},{\"end\":33844,\"start\":33835},{\"end\":33852,\"start\":33844},{\"end\":34044,\"start\":34036},{\"end\":34053,\"start\":34044},{\"end\":34066,\"start\":34053},{\"end\":34073,\"start\":34066},{\"end\":34084,\"start\":34073},{\"end\":34095,\"start\":34084},{\"end\":34103,\"start\":34095},{\"end\":34116,\"start\":34103},{\"end\":34125,\"start\":34116},{\"end\":34135,\"start\":34125},{\"end\":34461,\"start\":34454},{\"end\":34729,\"start\":34723},{\"end\":34738,\"start\":34729},{\"end\":34747,\"start\":34738},{\"end\":34760,\"start\":34747},{\"end\":34768,\"start\":34760},{\"end\":34779,\"start\":34768},{\"end\":35016,\"start\":35006},{\"end\":35022,\"start\":35016},{\"end\":35218,\"start\":35212},{\"end\":35227,\"start\":35218},{\"end\":35411,\"start\":35404},{\"end\":35420,\"start\":35411},{\"end\":35432,\"start\":35420},{\"end\":35440,\"start\":35432},{\"end\":35451,\"start\":35440},{\"end\":35640,\"start\":35632},{\"end\":35653,\"start\":35640},{\"end\":35664,\"start\":35653},{\"end\":35972,\"start\":35960},{\"end\":35982,\"start\":35972},{\"end\":35994,\"start\":35982},{\"end\":36013,\"start\":35994},{\"end\":36020,\"start\":36013},{\"end\":36252,\"start\":36237},{\"end\":36263,\"start\":36252},{\"end\":36271,\"start\":36263},{\"end\":36502,\"start\":36491},{\"end\":36509,\"start\":36502},{\"end\":36516,\"start\":36509},{\"end\":36868,\"start\":36859},{\"end\":37077,\"start\":37064},{\"end\":37322,\"start\":37315},{\"end\":37328,\"start\":37322},{\"end\":37334,\"start\":37328},{\"end\":37343,\"start\":37334},{\"end\":37352,\"start\":37343},{\"end\":37358,\"start\":37352},{\"end\":37364,\"start\":37358},{\"end\":37372,\"start\":37364},{\"end\":37380,\"start\":37372},{\"end\":37388,\"start\":37380},{\"end\":37396,\"start\":37388},{\"end\":37638,\"start\":37631},{\"end\":37645,\"start\":37638},{\"end\":37660,\"start\":37645},{\"end\":37668,\"start\":37660},{\"end\":37679,\"start\":37668},{\"end\":37949,\"start\":37943},{\"end\":37964,\"start\":37949},{\"end\":37977,\"start\":37964},{\"end\":38249,\"start\":38241},{\"end\":38255,\"start\":38249},{\"end\":38262,\"start\":38255},{\"end\":38271,\"start\":38262},{\"end\":38279,\"start\":38271},{\"end\":38506,\"start\":38500},{\"end\":38514,\"start\":38506},{\"end\":38522,\"start\":38514},{\"end\":38529,\"start\":38522},{\"end\":38535,\"start\":38529},{\"end\":38543,\"start\":38535},{\"end\":38775,\"start\":38767},{\"end\":38783,\"start\":38775},{\"end\":38790,\"start\":38783},{\"end\":38800,\"start\":38790},{\"end\":38809,\"start\":38800},{\"end\":39047,\"start\":39039},{\"end\":39054,\"start\":39047},{\"end\":39067,\"start\":39054},{\"end\":39078,\"start\":39067}]", "bib_venue": "[{\"end\":30999,\"start\":30995},{\"end\":31349,\"start\":31294},{\"end\":31683,\"start\":31679},{\"end\":32037,\"start\":31975},{\"end\":32414,\"start\":32410},{\"end\":32683,\"start\":32679},{\"end\":32896,\"start\":32845},{\"end\":33142,\"start\":33134},{\"end\":33388,\"start\":33333},{\"end\":33652,\"start\":33648},{\"end\":33856,\"start\":33852},{\"end\":34148,\"start\":34135},{\"end\":34452,\"start\":34387},{\"end\":34792,\"start\":34779},{\"end\":35038,\"start\":35022},{\"end\":35231,\"start\":35227},{\"end\":35455,\"start\":35451},{\"end\":35726,\"start\":35664},{\"end\":36027,\"start\":36020},{\"end\":36235,\"start\":36172},{\"end\":36489,\"start\":36408},{\"end\":36893,\"start\":36868},{\"end\":37129,\"start\":37077},{\"end\":37406,\"start\":37396},{\"end\":37683,\"start\":37679},{\"end\":37981,\"start\":37977},{\"end\":38283,\"start\":38279},{\"end\":38547,\"start\":38543},{\"end\":38813,\"start\":38809},{\"end\":39082,\"start\":39078}]"}}}, "year": 2023, "month": 12, "day": 17}