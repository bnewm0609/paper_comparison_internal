{"id": 214641445, "updated": "2023-10-06 17:59:09.429", "metadata": {"title": "Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study", "authors": "[{\"first\":\"Dinh-Luan\",\"last\":\"Nguyen\",\"middle\":[]},{\"first\":\"Sunpreet\",\"last\":\"Arora\",\"middle\":[\"S.\"]},{\"first\":\"Yuhang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "publication_date": {"year": 2020, "month": 3, "day": 24}, "abstract": "Deep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary's face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2003.11145", "mag": "3034823537", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/NguyenAWY20", "doi": "10.1109/cvprw50498.2020.00415"}}, "content": {"source": {"pdf_hash": "823ba4ff0f0162dfa0e2525acfb6327a5f2283fc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.11145v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.11145", "status": "GREEN"}}, "grobid": {"id": "4c51504700f6438a9ea281354ea9a82e9423a59f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/823ba4ff0f0162dfa0e2525acfb6327a5f2283fc.txt", "contents": "\nAdversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study\n\n\nDinh-Luan Nguyen \nVisa Research\n94306Palo AltoCAUSA\n\nMichigan State University\n48824East LansingMIUSA\n\nSunpreet S Arora sunarora@visa.com \nVisa Research\n94306Palo AltoCAUSA\n\nYuhang Wu \nVisa Research\n94306Palo AltoCAUSA\n\nHao Yang \nVisa Research\n94306Palo AltoCAUSA\n\nAdversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study\nCorresponding author\nDeep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary's face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.\n\nIntroduction\n\nDeep learning-based systems are typically designed under the assumption that the inputs/examples presented to the system during the test/operational phase follow the same underlying distribution as the examples used to train the system. However, recent research has shown security vulnerabilities of such systems when input test examples are intentionally crafted to cause the system to produce incorrect results (called adversarial examples) [22,8]. Most adversarial examples on convolutional neural network ar-(a) (b) Figure 1: Example of impersonation attack on FaceNet [18] in white-box setting. Shown in (a) is the captured image of the adversary's face with adversarial light projected in the physical domain that is recognized as the target (b).\n\nchitectures (typically used in image classification scenarios including face recognition) are generated by perturbing the pixel intensities directly in the digital domain [13,16,3]. These digital attacks, however, do not directly translate into the physical domain where an adversary has access to the open-camera channel. In such setting, the adversary usually does not have access to the image captured using the camera that is input to the convolutional neural network. Specifically, consider a face recognition system, that is deployed, such that it captures a face image of a subject and compares it to the enrolled faces to validate or establish the identity of the subject. While security mechanisms can be enforced to safeguard the digital storage and transmission of facial data captured using the camera, an adversary can potentially trick the system by providing a malicious input to the camera directly [10,7]. A subclass of physical attacks on face recognition systems called presentation or spoofing attacks achieve this by creating physical spoofs using one or more face images of the target (e.g., 2D-printed face photos, 3D masks) [14]. The same objective can also be achieved by crafting physical adversarial artifacts such as glasses that an adversary can wear to either evade recognition or mimic a target [19]. However, fabrication of physical adversarial artifacts gener-  Figure 2: Setup used for conducting real-time adversarial light projection attacks on face recognition systems. First, the adversary captures his/her facial image using a camera, and uses one or more images of the target to (i) calibrate the camera-projector setup based on the attack environment, and (ii) generate a digital adversarial pattern. Next, the adversary projects the digital pattern onto the adversary's face in the physical domain using a projector to either impersonate a target or evade recognition.\n\nally requires a manufacturing method (e.g., 2D or 3D printing). In addition, the utility of physical artifacts is limited in conducting at-scale physical attacks targeting multiple users of a face recognition system by the type of physical specimens that can be fabricated in typical resource-constrained settings.\n\nWe investigate the feasibility of conducting a real-time physical attack on face recognition systems using adversarial light projections that can be used for impersonating different enrolled users (called impersonation), or evading recognition (called obfuscation). The adversary first calibrates the camera-projector setup and then uses a transformation-invariant adversarial pattern generation method to generate adversarial patterns in the digital domain. These digital patterns are subsequently projected onto the adversary's face to conduct impersonation or obfuscation attack. We refer to this attack as adversarial light projection attack. As an example, impersonation is the goal when an adversary intends to obtain access to a resource, e.g., personal device protected with a target's face. Obfuscation, on the other hand, is the goal of an adversary blacklisted by law enforcement agencies who wants to evade recognition in scenarios such as border crossing.\n\nA similar idea was recently proposed for fooling deep learning classifiers designed for image classification systems [17]. However, the authors did not evaluate the utility of their method in the context of face recognition systems. Another recent work [28] fabricated a wearable cap with infrared LEDs to attack face recognition systems. Although this work is similar in terms of its objective, our method does not require a wearable artifact and thus offers an easier alternative using off-the-shelf camera-projector setup (e.g., a portable mini projector [1]) for conducting physical attacks on facial recognition systems. Preliminary experiments conducted on 50 subjects show the vulnerability of state-of-the-art face recognition systems to adversarial light projection attacks in both white-box and black-box attack settings.\n\n\nContributions\n\nThe major contributions of this work include:\n\n\u2022 Investigation of real-time adversarial light projection attacks using off-the-shelf camera-projector setup on state-of-the-art face recognition systems.\n\n\u2022 An efficient transformation-invariant adversarial pattern generation method suitable for conducting realtime adversarial light projection attacks.\n\n\u2022 Demonstration of vulnerability of state-of-the-art face recognition systems to adversarial light projection attacks in both white-box and black-box settings.\n\n\nRelated Work\n\nExisting research on adversarial attacks can be broadly classified into two major categories: digital and physical attacks. Given one or more examples from source and target class, digital attack methods generate adversarial pattern(s) in the digital domain such that the pattern(s) results in a source class example being misclassified as a target class example (called targeted attack), or the source class example being incorrectly classified as an example from a different class (called untargeted attack), typically with high confidence of being in the target class. Physical attacks extend this notion into the physical domain by using specially crafted adversarial artifacts for targeted or untargeted attacks. Below we summarize the major research in the two categories and contrast existing physical attack methods from the method presented here.\n\n\nDigital Attacks\n\nOne of the first digital attack methods proposed by Szegedy et al. in 2013 called L-BFGS [22] formulates the goal of adversarial pattern generation as an optimization problem, and uses a box-constrained optimizer and linear search to find the optimal solution. In 2014, Goodfellow et al. [8] proposed Fast Gradient Sign Method (FGSM), a single-step adversarial pattern generation method that uses gradients computed from neural network parameters for adversarial pattern generation. Following this, multiple extensions of FGSM were introduced [10,5,24,25].\n\nShi et al. [20] combine gradient ascent and descent with binary search to find the adversarial pattern with the least 2 norm. One of the most popular adversarial pattern generation methods Projected Gradient Descent (PGD) [13] uses gradient projection space as a bound to generate inf adversarial patterns. Other prominent methods include Deep-Fool [16] which was proposed for conducting untargeted attacks with p norm, and models adversarial pattern generation as a linear approximation problem, and SparseFool [15] that aims to generate adversarial patterns by modifying a minimal number of pixels. Another popular method proposed by Carlini and Wagner [3] uses gradient descent with a custom loss function to minimize the p norm during adversarial pattern generation.\n\n\nPhysical Attacks\n\nKurakin et al. [10] printed 2D adversarial patches containing objects overlaid by adversarial patterns to attack deep networks trained for the object recognition task. Several other researchers directly printed 2D adversarial patterns which are then manually attached to physical objects to attack object detection and classification algorithms [4,21,7,27]. Similarly, Thys et al. [23] printed 2D adversarial patches to circumvent pedestrian detection classifiers. Athalye et al. [2] proposed a transformation-invariant adversarial pattern generation scheme called Expectation of Transformations (EOT) to fabricate 3D adversarial objects designed to fool object classifiers. More recently, Li et al. [11] printed adversarial dots on a 2D transparent paper to provide adversarial input via the camera to an object recognition system. Although the aforementioned methods succeed in achieving their stated objective, they usually require extensive calibration of each 2D or 3D-printed artifact before fabrication. In addition, they also require fabrication of physical artifacts. On the other hand, the cameraprojector setup used in the method presented here can be calibrated once based on the attack environment, and then subsequently used for conducting multiple real-time attacks targeting different enrolled users of a face recognition system.\n\nSimilar to this work, Nichols and Jasper [17] used a camera-projector setup to generate 2D adversarial dot patterns that are then projected onto the physical scene to attack object recognition systems. However, they did not use the setup for conducting impersonation or obfuscation attacks on face recognition systems. Zhou et al. [28], on the other hand, fabricated a wearable cap with infrared LEDs to fool face recognition systems. Although this work is identical to the method presented here in terms of its objective, our method does not require creation of a wearable artifact and thus offers an easier alternative using off-the-shelf camera-projector setup for conducting physical attacks on facial recognition systems.\n\n\nAdversarial Light Projection Attack\n\nThe proposed adversarial light projection attack is performed in two steps: the first step is to calibrate the cameraprojector setup based on the attack environment and compute the adversarial pattern in the digital domain that can be used to either evade recognition or impersonate a target, and the second step is to project the computed digital adversarial pattern onto the adversary's face using the projector to attack the deployed face recognition system (see Figure  2).\n\n\nAssumptions\n\nIn the first step, the adversary is assumed to have either white-box access 1 or black-box access 2 to the deployed face recognition algorithm that the adversary intends to attack. White-box is a reasonable assumption for face recognition algorithms such as FaceNet [18] and SphereFace [12] that are available in open-source. On the other hand, commercial face recognition systems often only provide black-box access. So we assume that the adversary uses an opensource algorithm to generate adversarial patterns to attack the black-box system. This assumption exploits the property that adversarial patterns are highly transferable across deep network architectures.\n\nAdditionally, we assume that the adversary has access to an image of the target (in case of impersonation attack). Further, the adversary has access to a camera to capture the adversary's own face image in order to compute the adversarial light pattern, and a projector that is able to project light patterns on his/her own face in the physical domain in order to conduct the attack. In addition, the adversary is assumed to have either access to or reasonable prior knowledge of the environment where the face recognition system to be attacked is deployed. This is to ensure that the adversarial light pattern can be calibrated based on the attack environment before projection.\n\n\nPractical Considerations\n\nAdversarial light projection attacks are inherently challenging because of their unconstrained nature. Below we discuss key practical considerations critical to the success of such attacks:\n\n\u2022 Environmental factors, for example ambient and positional lighting, and their interplay with the projected light. Calibration of the attack setup based on the attack environment is therefore integral to the success of the attack (see Section 4).\n\nProjector Camera Project Capture Projection wall Figure 3: Different viewpoints in the camera-projector setup. The physical adversary is assumed to be in the view of both the camera and the projector.\n\n\u2022 Intra-adversary facial variations especially due to slight physical movements of the adversary, e.g., head movements and changes in the distance to the camera, while conducting the attack. Generation of adversarial patterns that are relatively invariant to such variations is therefore critical to the success of the attack (see Section 5).\n\n\u2022 Intra-target facial variations because the adversary would typically not have access to the enrolled images of the target in the deployed face recognition system. Instead, the adversary would have target images captured in a different context, such as social media.\n\nHence it is important that the generated adversarial pattern be robust to the target's facial variations (see Section 5).\n\n\nAttack Setup Calibration\n\nThere are two key calibration steps integral to success of the attack: (i) position calibration: to ensure that the adversarial pattern generated in the digital domain can be projected onto the appropriate region of the adversary's face while conducting the attack, and (ii) color calibration: to ensure that the digital adversarial pattern is reproduced with high fidelity by the projector as adversarial light.\n\n\nPosition calibration\n\nAssume that the adversary is in view of both the camera and the projector (Figure 3). There are two possible ways to perform position calibration: (i) manual: the adversary manually annotates a small number (3)(4) of corresponding points between the two views; or (ii) automatic: a facial landmark detection algorithm is used to detect corresponding facial landmarks from the two views. Once the landmark correspondences are determined, a calibration matrix is computed to perform position calibration (see Figure 4).\n\n\nColor calibration\n\nLet C and G be the color reproduction functions of the camera and projector, respectively, in the physical attack Camera view point Aligned projector content\n\nStep 1\n\nStep 2\n\nStep 3 Step 1: Adversary's face is captured using the camera, and facial landmarks are either manually annotated or automatically detected;\n\nStep 2: Projected scene (including the adversarys face) is captured using the camera, and facial landmarks are manually annotated or automatically detected;\n\nStep 3: Corresponding landmarks from step 1 and step 2 are used to compute the calibration matrix to calibrate the adversarial pattern before projection on the adversary's face.\n\nsetting. The objective of color calibration is to find the color transformation function \u03a5 given C and G. Let x = C(x 0 ), where x 0 is the physical adversary, x is the image of the adversary in the digital domain, and h is the method used to generate adversarial pattern in the digital domain such as the one presented in section 5.3. Also, assume additive characteristic of the color reproduction functions, i.e. C(color1 + color2) = C(color1) + C(color2). The relationship between the digital and physical domains can then be expressed as follows:\nC(x 0 + G(\u03a5(h(x)))) = x + h(x) \u21d4 C(x 0 ) + C(G(\u03a5(h(x))))) = x + h(x) \u21d4 C(G(\u03a5(h(x))))) = h(x) \u21d4 \u03a5 = (C \u2022 G) \u22121(1)\nTo empirically estimate the aforementioned relationship in the attack setting, (i) color calibration patterns are generated in the digital domain, (ii) the generated calibration patterns are projected on a white background in the physical domain, and (iii) the projected calibration patterns in the physical domain are captured in the digital domain using the camera. The color transformation function \u03a5 is then computed by performing regression on the generated and camera-captured color pairs. In practice, we found that performing regression in Lab color space results in more accurate color reproduction in the physical domain than standard RGB color space.\n\n\nAlgorithm 1 Transformation-Invariant Adversarial Pattern Generation\n\nInput: Image of adversary x, target image y Output: Adversarial pattern x adv Notations: A: method to compute representative average image (equation 3); f : method to compute face embedding; M: distance metric (e.g., p , cosine); clip: method to clip intensity values (0-255); F : fusion function (e.g. summation); \u03b1, \u03b2: hyper-parameters 1: procedure OPTIMIZE(x, y) 2:\nx temp 0 \u2190 0 w\u00d7h , x avg 0 \u2190 x, and x 0 \u2190 x Initialization 3:\nwhile not converge do 4:\n\nx avg t\u22121 \u2190 A(x avg t\u22121 ) Compute representative average image 5: x avg t \u2190 clip(x avg t\u22121 + x temp t\u22121 + \u03b6 N (0,\u03c3 2 ) ) Clipped representative image 6: x t \u2190 clip(x t\u22121 + x temp t\u22121 ) Clipped image of adversary 7:\nD avg \u2190 M(f (x avg t ), f (y)) and D \u2190 M(f (x t ), f (y))\nDistance computation using metric M 8:\n\u2206\u03b7 \u2190 \u2207 x F (D avg , D)| x=x temp t\u22121\nCompute update with respect to both x and x avg 9:\ng t \u2190 \u03b2 \u00d7 g t\u22121 + \u2206\u03b7 \u2206\u03b7 1 and x temp t \u2190 x temp t\u22121 + \u03b1 \u00d7 sign(g t )\nGradient update with momentum 10:\n\nx adv = x t \u2212 x Adversarial pattern\n\n\nTransformation-Invariant Adversarial Pattern Generation\n\nGeneration of adversarial patterns that are relatively invariant to intra-adversary facial variations is critical to the success of light projection attacks. Let x and y, respectively, be the images pertaining to the adversary and the target in the digital domain that are used in the adversarial pattern generation process. Existing methods (e.g., [2], [6]) generate a transformation-invariant adversarial pattern x adv by applying different transformations on the adversary's image. In case of impersonation attack, the following optimization is solved:\nx adv = argmin \u2206x k\u22121 i=0 (w i L(T i (x) + \u2206x, y)), s.t. \u2206x p \u2264 \u03b5 (2)\nHere, T i corresponds to the i th transformation, k is the total number of transformations, and w i corresponds to the weight of T i such that k\u22121 i=0 w i = 1. Also, L is the loss function used in the adversarial pattern generation process. We assume that L is computed using a distance metric M in the face embedding space, and hence needs to be minimized. Alternatively, L could be computed using a similarity metric in which case it would need to be maximized. Equation 2 involves computation of loss functions with respect to each transformation in each iteration. This is computationally intensive, and limits the application of these methods in the setting where an adversary wants to generate adversarial patterns in real-time.\n\n\nComputing representative adversary image\n\nInstead, our method computes an average representative image x avg of the adversary as follows:\nx avg = A(x) = w 0 x + k\u22121 i=1 (w i T i (x)), s.t. k\u22121 i=0 w i = 1 (3)\nFor the impersonation task, the following optimization is then solved:\n\nx adv = argmin \u2206x L(x avg + \u2206x, y), s.t. \u2206x p \u2264 \u03b5 (4) Equation 4 does not require explicit computation of loss functions for all possible transformations, yet explores a wide variety of transformation configurations in each iteration. The limitation though is that the optimization is performed with respect to a single representative image, and not with respect to expected loss pertaining to each transformation configuration. For real-time light projection attacks, this trade-off is desirable.\n\nAlso note that although equations 2 and 4 pertain to impersonation, equations for obfuscation can be formulated and solved in a similar manner.\n\n\nUsing the original adversary image\n\nOptimizing with respect to the representative image x avg provides an efficient way to achieve transformationinvariance. However, to ensure that the generated pattern retains adversarial characteristics not only for the representative image x avg but for x as well, optimization with respect to both x avg and x is performed. Figure 5 illustrates example benefit for translation and rotation-invariance. Similar benefits were observed for other transformations. \n\n\nProposed method\n\nAlgorithm 1 summarizes the proposed transformationinvariant pattern generation method. The method takes as input the image x of the adversary and the image of the target y, and outputs the adversarial pattern x adv . The transformations used depend on the invariance objective (e.g., affine, perspective, photo-metric or others). The convergence criteria is either predefined number of steps or threshold on distance metric M (step 7). A brightness term sampled from a normal distribution is used during each iterative update for obtaining invariance to slight illumination changes (step 5). Furthermore, a binary mask can be used to constrain the facial region for which the adversarial pattern is generated similar to [19].\n\n\nUsing multiple images of target\n\nWhile the method described above focuses on intraadversary invariance, it is desirable to impart invariance to intra-target variations as well to increase likelihood of success. For this, multiple images of the target can be used. Instead of a single target image, algorithm 1 can then be optimized with respect to target embedding y computed using multiple target images.\n\n\nExperimental Evaluation\n\nTo study the feasibility of adversarial light projection attacks, live subject experiments are performed with 50 subjects in total. Each experiment is conducted in a room with fixed lighting. A Logitech web camera and a Panasonic or Epson projector are used for the experiments.  Figure 6: Example of impersonation attack on the commercial face recognition system in black-box setting. Shown in (a) is the captured image of the adversary's face with adversarial light projected in the physical domain that is recognized as target (b). This example is interesting because it appears that adversarial light resulted in failure of face detection (blue rectangle) yet the impersonation attack succeeded.\n\n\nExperimental setup\n\nA web-based user interface is designed that takes realtime input of a subject's face using a web camera, and lets the subject select/upload face images of the target. The interface also lets the subject perform calibration based on the camera feed and the connected projector. Multi-task convolutional neural network (MTCNN)-based face detection and landmark estimation [26] is used for automatic position calibration. For color calibration, the method described in section 4.2 is used. If necessary, the brightness and color intensity of the projector is manually tuned via the interface.\n\nPost calibration, a python script executes the transformation-invariant pattern generation method (implemented in Tensorflow 2.0) for 100 iterations. Cosine is used as distance metric and multiplication as the fusion function. For gradient update, the parameters \u03b1 and \u03b2 are set to 1 and 0.7 respectively. The following transformation configurations (assuming the origin is centered at midpoint of adversary's image) are considered during generation of the transformation-invariant pattern: \u221240 to +40 pixels translation in both x and y directions, \u2212\u03c0/3 to +\u03c0/3 rotation, and 0.5 \u2212 2 times scaling. Each transformation configuration is assumed to be equally likely i.e. the weight corresponding to each transformation is set to 1 k where k = 3 is the number of transformations.\n\nThe computed digital pattern is projected using the projector in the form of adversarial light onto the subject's face. The subject's face with light projection is captured for about 30 seconds and used to attack a face recognition algorithm in real-time. The subject is instructed to make natural head movements (e.g. translation, rotation) during the duration of the attack. Similarity score between each captured image and the target face image is computed. If the com- puted score for any adversary-target image pair is above the threshold corresponding to False Accept Rate (FAR) of 0.01%, the attack attempt is considered successful. FaceNet [18] and SphereFace [12] are the two face recognition algorithms used in white-box setting. In black-box setting, FaceNet is used to generate adversarial pattern to attack a commercial face recognition algorithm.\n\n\nImpersonation\n\nFor impersonation, a face image of a subject (adversary) captured using the camera, and a face image of the target (obtained from the web or a database) are used to generate the digital adversarial pattern. A different face image of the target (also obtained from the web or a database) is assumed to be enrolled in the face recognition system to be attacked. Impersonation attempts are made using different subject pools in the following scenarios.\n\n\u2022 Fixed target (imp-fix): Impersonating a fixed highprofile target (Rowan Atkinson). 25 subjects in total attempted this in both white-box and black-box setting. 23 and 21 attempts out of 25 on FaceNet and SphereFace, respectively, succeeded. In black-box setting, 15 out of 25 attempts on the commercial system succeeded.\n\n\u2022 Selected target (imp-select): Impersonating any one of the given targets (Taylor Swift, Michael Phelps, or Albert Einstein) at random. A total of 15 subjects attempted this in white-box setting. 14 and 12 attempts out of 15 on FaceNet and SphereFace, respectively, succeeded.\n\n(a) (b) Figure 7: Example of impersonation attack on SphereFace [12] in white-box setting. Shown in (a) is the captured image of the adversary's face with adversarial light projected in the physical domain that is recognized as target (b).\n\n\u2022 \n\n\nObfuscation\n\nFor obfuscation (obf), two face images of a subject (adversary-target pair) captured using the camera are used to generate digital adversarial pattern. A different face image of the same subject (target) is assumed to be enrolled. 10 different subjects attempted obfuscation attacks in both white-box and black-box setting. All 10 obfuscation attempts on FaceNet and SphereFace succeeded in white-box setting, whereas in black-box setting 7 out of 10 attempts on the commercial face recognition system succeeded. Table 1 summarizes the experimental results. Figure 8 shows a successful example of obfuscation on the commercial face recognition system.\n\n\nFailure Cases\n\nWhile most impersonation and obfuscation attempts are successful, failure of adversarial light projection attacks is observed due to one or more of the following reasons:\n\n\u2022 Light projection either covering the entire face or significantly occluding majority of the face resulting in failure of face detection. Projection of adversarial light on a particular part of the face, e.g., cheeks or forehead, is found to result in higher likelihood of success in practice. \u2022 Strong ambient or directional lighting that overpowers the projected light.\n\n\u2022 Extreme facial pose of the adversary. In practice, however, this is less likely as the adversary is cooperative.\n\n\u2022 Out of focus light projection on the adversary's face when the projector lens is not tuned appropriately. Manual tuning of the projector lens to ensure the projected light is properly focused on the adversary's face is important for a successful attack attempt in practice.\n\nWe plan to investigate failure cases in a more systematic manner in a follow-up study.\n\n\nConclusions and Future Work\n\nWe show the feasibility of conducting impersonation and obfuscation attacks using adversarial light projections on two open-source face recognition systems in white-box setting and a commercial face recognition system in black-box setting. Furthermore, an efficient transformation-invariant adversarial pattern generation method that enables an adversary to conduct light projection attacks in real-time is presented.\n\nWhile we have shown the feasibility of light projection attacks, we have not systematically tested the likelihood of success at different operating thresholds of face recognition systems in different environments. One of our immediate goals, therefore, is to systematically investigate the impact of environmental and subject-dependent covariates (such as lighting, subject orientation and pose) on the repeatability of light projection attacks. Furthermore, we suspect that presentation attack detection methods designed for static attacks using 2D or 3D fabricated artifacts will be inadequate in defending against dynamic adversarial attacks such as light projection attacks. Therefore, we also plan to conduct an evaluation of existing defense mechanisms and develop novel defense mechanisms for such dynamic attacks.\n\nFigure 4 :\n4Two possible methods for position calibration: (i) manual (top row) and (ii) automatic (bottom row).\n\nFigure 5 :\n5Advantage of using the proposed transformation-invariant pattern generation method for (a) translation invariance and (b) rotation invariance. Left and right charts show, respectively, the similarity scores generated by FaceNet[18] between an adversary's face image with projected adversarial light pattern and the target's face image, without and with (a) translationinvariance (in x and y direction in pixels) and (b) rotation-invariance (in degrees).\n\nFigure 8 :\n8Example of obfuscation attack on the commercial face recognition system in black-box setting. Adversarial light projected on the adversary's face in the physical domain results in successful obfuscation.\n\n\nTop-5 similar targets (imp-top5): Given a database of target images (Labelled Faces in the Wild (LFW) database [9]), impersonate top-5 most similar targets based on a face recognition algorithm. 10 different subjects attempted this on five most similar targets from LFW in white-box setting. 44 and 39 out of 50 attempts on FaceNet and SphereFace, respectively, succeeded.Table 1 summarizes the experimental results. Figures 1, 7 and 6 show successful examples of impersonation, respectively, on FaceNet, SphereFace and the commercial face recognition system.\nIn white-box setting, the adversary knows the internal details of the model including the architecture and trained weight parameters.2 In black-box setting, the adversary only knows the decision/output of the model for one or more inputs.\n\nA Athalye, L Engstrom, A Ilyas, K Kwok, arXiv:1707.07397Synthesizing robust adversarial examples. 35arXiv preprintA. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Syn- thesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017. 3, 5\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, 2017 IEEE Symposium on Security and Privacy (SP). IEEE13N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39-57. IEEE, 2017. 1, 3\n\nShapeshifter: Robust physical adversarial attack on faster r-cnn object detector. S Chen, C Cornelius, J Martin, D Chau, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerS. Chen, C. Cornelius, J. Martin, and D. Chau. Shapeshifter: Robust physical adversarial attack on faster r-cnn object de- tector. In Joint European Conference on Machine Learn- ing and Knowledge Discovery in Databases, pages 52-68. Springer, 2018. 3\n\nBoosting adversarial attacks with momentum. Y Dong, F Liao, T Pang, H Su, J Zhu, X Hu, J Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial attacks with momentum. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9185-9193, 2018. 2\n\nEvading defenses to transferable adversarial examples by translation-invariant attacks. Y Dong, T Pang, H Su, J Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Dong, T. Pang, H. Su, and J. Zhu. Evading defenses to transferable adversarial examples by translation-invariant at- tacks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4312-4321, 2019. 5\n\nRobust physicalworld attacks on deep learning visual classification. K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition13K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust physical- world attacks on deep learning visual classification. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1625-1634, 2018. 1, 3\n\nI Goodfellow, J Shlens, C Szegedy, arXiv:1412.6572Explaining and harnessing adversarial examples. 1arXiv preprintI. Goodfellow, J. Shlens, and C. Szegedy. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 1, 2\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. G Huang, M Mattar, T Berg, E Learned-Miller, G. Huang, M. Mattar, T. Berg, and E. Learned-Miller. La- beled faces in the wild: A database for studying face recog- nition in unconstrained environments. 2008. 8\n\nA Kurakin, I Goodfellow, S Bengio, arXiv:1607.02533Adversarial examples in the physical world. 13arXiv preprintA. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam- ples in the physical world. arXiv preprint arXiv:1607.02533, 2016. 1, 2, 3\n\nAdversarial camera stickers: A physical camera-based attack on deep learning systems. J Li, F Schmidt, Z Kolter, International Conference on Machine Learning. J. Li, F. Schmidt, and Z. Kolter. Adversarial camera stick- ers: A physical camera-based attack on deep learning sys- tems. In International Conference on Machine Learning, pages 3896-3904, 2019. 3\n\nSphereface: Deep hypersphere embedding for face recognition. W Liu, Y Wen, Z Yu, M Li, B Raj, L Song, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition7W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep hypersphere embedding for face recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212-220, 2017. 3, 7, 8\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations13A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In Proceedings of the International Conference on Learning Representations, 2018. 1, 3\n\nHandbook of biometric antispoofing. S Marcel, M Nixon, S Li, Springer1S. Marcel, M. Nixon, and S. Li. Handbook of biometric anti- spoofing, volume 1. Springer, 2014. 1\n\nSparsefool: a few pixels make a big difference. A Modas, S Moosavi-Dezfooli, P Frossard, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Modas, S. Moosavi-Dezfooli, and P. Frossard. Sparsefool: a few pixels make a big difference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 9087-9096, 2019. 3\n\nDeepfool: a simple and accurate method to fool deep neural networks. S Moosavi-Dezfooli, A Fawzi, P Frossard, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition13S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574-2582, 2016. 1, 3\n\nProjecting trouble: Light based adversarial attacks on deep learning classifiers. N Nichols, R Jasper, arXiv:1810.1033723arXiv preprintN. Nichols and R. Jasper. Projecting trouble: Light based ad- versarial attacks on deep learning classifiers. arXiv preprint arXiv:1810.10337, 2018. 2, 3\n\nFacenet: A unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition67F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815-823, 2015. 1, 3, 6, 7\n\nAccessorize to a crime: Real and stealthy attacks on state-ofthe-art face recognition. M Sharif, S Bhagavatula, L Bauer, M Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACM16M. Sharif, S. Bhagavatula, L. Bauer, and M. Reiter. Ac- cessorize to a crime: Real and stealthy attacks on state-of- the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Se- curity, pages 1528-1540. ACM, 2016. 1, 6\n\nCurls & whey: Boosting blackbox adversarial attacks. Y Shi, S Wang, Y Han, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Shi, S. Wang, and Y. Han. Curls & whey: Boosting black- box adversarial attacks. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, 2019. 2\n\nPhysical adversarial examples for object detectors. D Song, K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, F Tramer, A Prakash, T Kohno, 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18). D. Song, K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, F. Tramer, A. Prakash, and T. Kohno. Physical adversarial examples for object detectors. In 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18), 2018. 3\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations1C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations, 2014. 1, 2\n\nFooling automated surveillance cameras: adversarial patches to attack person detection. S Thys, W Van Ranst, T Goedem\u00e9, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsS. Thys, W. Van Ranst, and T. Goedem\u00e9. Fooling automated surveillance cameras: adversarial patches to attack person detection. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition Workshops, pages 0-0, 2019. 3\n\nUnderstanding and enhancing the transferability of adversarial examples. L Wu, Z Zhu, C Tai, arXiv:1802.09707arXiv preprintL. Wu, Z. Zhu, C. Tai, et al. Understanding and enhancing the transferability of adversarial examples. arXiv preprint arXiv:1802.09707, 2018. 2\n\nImproving transferability of adversarial examples with input diversity. C Xie, Z Zhang, Y Zhou, S Bai, J Wang, Z Ren, A Yuille, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionC. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. Yuille. Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2730- 2739, 2019. 2\n\nJoint face detection and alignment using multitask cascaded convolutional networks. K Zhang, Z Zhang, Z Li, Y Qiao, IEEE Signal Processing Letters. 2310K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection and alignment using multitask cascaded convolutional net- works. IEEE Signal Processing Letters, 23(10):1499-1503, 2016. 7\n\nY Zhao, H Zhu, Q Shen, R Liang, K Chen, S Zhang, arXiv:1812.10217Practical adversarial attack against object detector. arXiv preprintY. Zhao, H. Zhu, Q. Shen, R. Liang, K. Chen, and S. Zhang. Practical adversarial attack against object detector. arXiv preprint arXiv:1812.10217, 2018. 3\n\nInvisible mask: Practical attacks on face recognition with infrared. Z Zhou, D Tang, X Wang, W Han, X Liu, K Zhang, arXiv:1803.0468323arXiv preprintZ. Zhou, D. Tang, X. Wang, W. Han, X. Liu, and K. Zhang. Invisible mask: Practical attacks on face recognition with in- frared. arXiv preprint arXiv:1803.04683, 2018. 2, 3\n", "annotations": {"author": "[{\"end\":191,\"start\":89},{\"end\":262,\"start\":192},{\"end\":308,\"start\":263},{\"end\":353,\"start\":309}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":99},{\"end\":208,\"start\":203},{\"end\":272,\"start\":270},{\"end\":317,\"start\":313}]", "author_first_name": "[{\"end\":98,\"start\":89},{\"end\":200,\"start\":192},{\"end\":202,\"start\":201},{\"end\":269,\"start\":263},{\"end\":312,\"start\":309}]", "author_affiliation": "[{\"end\":140,\"start\":107},{\"end\":190,\"start\":142},{\"end\":261,\"start\":228},{\"end\":307,\"start\":274},{\"end\":352,\"start\":319}]", "title": "[{\"end\":86,\"start\":1},{\"end\":439,\"start\":354}]", "venue": null, "abstract": "[{\"end\":1817,\"start\":461}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2280,\"start\":2276},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2282,\"start\":2280},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2410,\"start\":2406},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2762,\"start\":2758},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2765,\"start\":2762},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2767,\"start\":2765},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3506,\"start\":3502},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3508,\"start\":3506},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3917,\"start\":3913},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5906,\"start\":5902},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6042,\"start\":6038},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8131,\"start\":8127},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8329,\"start\":8326},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8585,\"start\":8581},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8587,\"start\":8585},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8590,\"start\":8587},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8593,\"start\":8590},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8611,\"start\":8607},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8822,\"start\":8818},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8949,\"start\":8945},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9112,\"start\":9108},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9254,\"start\":9251},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9406,\"start\":9402},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9735,\"start\":9732},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9738,\"start\":9735},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9740,\"start\":9738},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9743,\"start\":9740},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9772,\"start\":9768},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9870,\"start\":9867},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10091,\"start\":10087},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10779,\"start\":10775},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11069,\"start\":11065},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12263,\"start\":12259},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12283,\"start\":12279},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15421,\"start\":15418},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15424,\"start\":15421},{\"end\":18322,\"start\":18320},{\"end\":18409,\"start\":18407},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19208,\"start\":19205},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19213,\"start\":19210},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20564,\"start\":20563},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22387,\"start\":22383},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23920,\"start\":23916},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25568,\"start\":25564},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25588,\"start\":25584},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26916,\"start\":26912},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30432,\"start\":30428},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31568,\"start\":31567}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30187,\"start\":30074},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30654,\"start\":30188},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30871,\"start\":30655},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31433,\"start\":30872}]", "paragraph": "[{\"end\":2585,\"start\":1833},{\"end\":4497,\"start\":2587},{\"end\":4813,\"start\":4499},{\"end\":5783,\"start\":4815},{\"end\":6616,\"start\":5785},{\"end\":6679,\"start\":6634},{\"end\":6835,\"start\":6681},{\"end\":6985,\"start\":6837},{\"end\":7146,\"start\":6987},{\"end\":8018,\"start\":7163},{\"end\":8594,\"start\":8038},{\"end\":9366,\"start\":8596},{\"end\":10732,\"start\":9387},{\"end\":11460,\"start\":10734},{\"end\":11977,\"start\":11500},{\"end\":12659,\"start\":11993},{\"end\":13340,\"start\":12661},{\"end\":13558,\"start\":13369},{\"end\":13807,\"start\":13560},{\"end\":14009,\"start\":13809},{\"end\":14353,\"start\":14011},{\"end\":14622,\"start\":14355},{\"end\":14745,\"start\":14624},{\"end\":15186,\"start\":14774},{\"end\":15728,\"start\":15211},{\"end\":15907,\"start\":15750},{\"end\":15915,\"start\":15909},{\"end\":15923,\"start\":15917},{\"end\":16064,\"start\":15925},{\"end\":16222,\"start\":16066},{\"end\":16401,\"start\":16224},{\"end\":16953,\"start\":16403},{\"end\":17728,\"start\":17067},{\"end\":18168,\"start\":17800},{\"end\":18255,\"start\":18231},{\"end\":18471,\"start\":18257},{\"end\":18568,\"start\":18530},{\"end\":18656,\"start\":18606},{\"end\":18759,\"start\":18726},{\"end\":18796,\"start\":18761},{\"end\":19411,\"start\":18856},{\"end\":20216,\"start\":19482},{\"end\":20356,\"start\":20261},{\"end\":20498,\"start\":20428},{\"end\":20997,\"start\":20500},{\"end\":21142,\"start\":20999},{\"end\":21643,\"start\":21181},{\"end\":22388,\"start\":21663},{\"end\":22796,\"start\":22424},{\"end\":23523,\"start\":22824},{\"end\":24135,\"start\":23546},{\"end\":24914,\"start\":24137},{\"end\":25776,\"start\":24916},{\"end\":26243,\"start\":25794},{\"end\":26567,\"start\":26245},{\"end\":26846,\"start\":26569},{\"end\":27087,\"start\":26848},{\"end\":27091,\"start\":27089},{\"end\":27758,\"start\":27107},{\"end\":27946,\"start\":27776},{\"end\":28320,\"start\":27948},{\"end\":28436,\"start\":28322},{\"end\":28713,\"start\":28438},{\"end\":28801,\"start\":28715},{\"end\":29250,\"start\":28833},{\"end\":30073,\"start\":29252}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17066,\"start\":16954},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18230,\"start\":18169},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18529,\"start\":18472},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18605,\"start\":18569},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18725,\"start\":18657},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19481,\"start\":19412},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20427,\"start\":20357}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1831,\"start\":1819},{\"attributes\":{\"n\":\"1.1.\"},\"end\":6632,\"start\":6619},{\"attributes\":{\"n\":\"2.\"},\"end\":7161,\"start\":7149},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8036,\"start\":8021},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9385,\"start\":9369},{\"attributes\":{\"n\":\"3.\"},\"end\":11498,\"start\":11463},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11991,\"start\":11980},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13367,\"start\":13343},{\"attributes\":{\"n\":\"4.\"},\"end\":14772,\"start\":14748},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15209,\"start\":15189},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15748,\"start\":15731},{\"end\":17798,\"start\":17731},{\"attributes\":{\"n\":\"5.\"},\"end\":18854,\"start\":18799},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20259,\"start\":20219},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21179,\"start\":21145},{\"attributes\":{\"n\":\"5.3.\"},\"end\":21661,\"start\":21646},{\"attributes\":{\"n\":\"5.4.\"},\"end\":22422,\"start\":22391},{\"attributes\":{\"n\":\"6.\"},\"end\":22822,\"start\":22799},{\"attributes\":{\"n\":\"6.1.\"},\"end\":23544,\"start\":23526},{\"attributes\":{\"n\":\"6.2.\"},\"end\":25792,\"start\":25779},{\"attributes\":{\"n\":\"6.3.\"},\"end\":27105,\"start\":27094},{\"attributes\":{\"n\":\"6.4.\"},\"end\":27774,\"start\":27761},{\"attributes\":{\"n\":\"7.\"},\"end\":28831,\"start\":28804},{\"end\":30085,\"start\":30075},{\"end\":30199,\"start\":30189},{\"end\":30666,\"start\":30656}]", "table": null, "figure_caption": "[{\"end\":30187,\"start\":30087},{\"end\":30654,\"start\":30201},{\"end\":30871,\"start\":30668},{\"end\":31433,\"start\":30874}]", "figure_ref": "[{\"end\":2361,\"start\":2353},{\"end\":3990,\"start\":3982},{\"end\":11975,\"start\":11966},{\"end\":13866,\"start\":13858},{\"end\":15294,\"start\":15285},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15726,\"start\":15718},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21515,\"start\":21507},{\"end\":23112,\"start\":23104},{\"end\":26864,\"start\":26856},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27673,\"start\":27665}]", "bib_author_first_name": "[{\"end\":31675,\"start\":31674},{\"end\":31686,\"start\":31685},{\"end\":31698,\"start\":31697},{\"end\":31707,\"start\":31706},{\"end\":31980,\"start\":31979},{\"end\":31991,\"start\":31990},{\"end\":32303,\"start\":32302},{\"end\":32311,\"start\":32310},{\"end\":32324,\"start\":32323},{\"end\":32334,\"start\":32333},{\"end\":32730,\"start\":32729},{\"end\":32738,\"start\":32737},{\"end\":32746,\"start\":32745},{\"end\":32754,\"start\":32753},{\"end\":32760,\"start\":32759},{\"end\":32767,\"start\":32766},{\"end\":32773,\"start\":32772},{\"end\":33222,\"start\":33221},{\"end\":33230,\"start\":33229},{\"end\":33238,\"start\":33237},{\"end\":33244,\"start\":33243},{\"end\":33696,\"start\":33695},{\"end\":33707,\"start\":33706},{\"end\":33718,\"start\":33717},{\"end\":33731,\"start\":33730},{\"end\":33737,\"start\":33736},{\"end\":33748,\"start\":33747},{\"end\":33756,\"start\":33755},{\"end\":33767,\"start\":33766},{\"end\":33776,\"start\":33775},{\"end\":34212,\"start\":34211},{\"end\":34226,\"start\":34225},{\"end\":34236,\"start\":34235},{\"end\":34560,\"start\":34559},{\"end\":34569,\"start\":34568},{\"end\":34579,\"start\":34578},{\"end\":34587,\"start\":34586},{\"end\":34770,\"start\":34769},{\"end\":34781,\"start\":34780},{\"end\":34795,\"start\":34794},{\"end\":35103,\"start\":35102},{\"end\":35109,\"start\":35108},{\"end\":35120,\"start\":35119},{\"end\":35436,\"start\":35435},{\"end\":35443,\"start\":35442},{\"end\":35450,\"start\":35449},{\"end\":35456,\"start\":35455},{\"end\":35462,\"start\":35461},{\"end\":35469,\"start\":35468},{\"end\":35908,\"start\":35907},{\"end\":35917,\"start\":35916},{\"end\":35928,\"start\":35927},{\"end\":35939,\"start\":35938},{\"end\":35950,\"start\":35949},{\"end\":36337,\"start\":36336},{\"end\":36347,\"start\":36346},{\"end\":36356,\"start\":36355},{\"end\":36518,\"start\":36517},{\"end\":36527,\"start\":36526},{\"end\":36547,\"start\":36546},{\"end\":36975,\"start\":36974},{\"end\":36995,\"start\":36994},{\"end\":37004,\"start\":37003},{\"end\":37469,\"start\":37468},{\"end\":37480,\"start\":37479},{\"end\":37743,\"start\":37742},{\"end\":37754,\"start\":37753},{\"end\":37770,\"start\":37769},{\"end\":38237,\"start\":38236},{\"end\":38247,\"start\":38246},{\"end\":38262,\"start\":38261},{\"end\":38271,\"start\":38270},{\"end\":38765,\"start\":38764},{\"end\":38772,\"start\":38771},{\"end\":38780,\"start\":38779},{\"end\":39157,\"start\":39156},{\"end\":39165,\"start\":39164},{\"end\":39176,\"start\":39175},{\"end\":39187,\"start\":39186},{\"end\":39200,\"start\":39199},{\"end\":39206,\"start\":39205},{\"end\":39217,\"start\":39216},{\"end\":39227,\"start\":39226},{\"end\":39238,\"start\":39237},{\"end\":39580,\"start\":39579},{\"end\":39591,\"start\":39590},{\"end\":39602,\"start\":39601},{\"end\":39615,\"start\":39614},{\"end\":39624,\"start\":39623},{\"end\":39633,\"start\":39632},{\"end\":39647,\"start\":39646},{\"end\":40093,\"start\":40092},{\"end\":40101,\"start\":40100},{\"end\":40114,\"start\":40113},{\"end\":40600,\"start\":40599},{\"end\":40606,\"start\":40605},{\"end\":40613,\"start\":40612},{\"end\":40867,\"start\":40866},{\"end\":40874,\"start\":40873},{\"end\":40883,\"start\":40882},{\"end\":40891,\"start\":40890},{\"end\":40898,\"start\":40897},{\"end\":40906,\"start\":40905},{\"end\":40913,\"start\":40912},{\"end\":41396,\"start\":41395},{\"end\":41405,\"start\":41404},{\"end\":41414,\"start\":41413},{\"end\":41420,\"start\":41419},{\"end\":41649,\"start\":41648},{\"end\":41657,\"start\":41656},{\"end\":41664,\"start\":41663},{\"end\":41672,\"start\":41671},{\"end\":41681,\"start\":41680},{\"end\":41689,\"start\":41688},{\"end\":42006,\"start\":42005},{\"end\":42014,\"start\":42013},{\"end\":42022,\"start\":42021},{\"end\":42030,\"start\":42029},{\"end\":42037,\"start\":42036},{\"end\":42044,\"start\":42043}]", "bib_author_last_name": "[{\"end\":31683,\"start\":31676},{\"end\":31695,\"start\":31687},{\"end\":31704,\"start\":31699},{\"end\":31712,\"start\":31708},{\"end\":31988,\"start\":31981},{\"end\":31998,\"start\":31992},{\"end\":32308,\"start\":32304},{\"end\":32321,\"start\":32312},{\"end\":32331,\"start\":32325},{\"end\":32339,\"start\":32335},{\"end\":32735,\"start\":32731},{\"end\":32743,\"start\":32739},{\"end\":32751,\"start\":32747},{\"end\":32757,\"start\":32755},{\"end\":32764,\"start\":32761},{\"end\":32770,\"start\":32768},{\"end\":32776,\"start\":32774},{\"end\":33227,\"start\":33223},{\"end\":33235,\"start\":33231},{\"end\":33241,\"start\":33239},{\"end\":33248,\"start\":33245},{\"end\":33704,\"start\":33697},{\"end\":33715,\"start\":33708},{\"end\":33728,\"start\":33719},{\"end\":33734,\"start\":33732},{\"end\":33745,\"start\":33738},{\"end\":33753,\"start\":33749},{\"end\":33764,\"start\":33757},{\"end\":33773,\"start\":33768},{\"end\":33781,\"start\":33777},{\"end\":34223,\"start\":34213},{\"end\":34233,\"start\":34227},{\"end\":34244,\"start\":34237},{\"end\":34566,\"start\":34561},{\"end\":34576,\"start\":34570},{\"end\":34584,\"start\":34580},{\"end\":34602,\"start\":34588},{\"end\":34778,\"start\":34771},{\"end\":34792,\"start\":34782},{\"end\":34802,\"start\":34796},{\"end\":35106,\"start\":35104},{\"end\":35117,\"start\":35110},{\"end\":35127,\"start\":35121},{\"end\":35440,\"start\":35437},{\"end\":35447,\"start\":35444},{\"end\":35453,\"start\":35451},{\"end\":35459,\"start\":35457},{\"end\":35466,\"start\":35463},{\"end\":35474,\"start\":35470},{\"end\":35914,\"start\":35909},{\"end\":35925,\"start\":35918},{\"end\":35936,\"start\":35929},{\"end\":35947,\"start\":35940},{\"end\":35956,\"start\":35951},{\"end\":36344,\"start\":36338},{\"end\":36353,\"start\":36348},{\"end\":36359,\"start\":36357},{\"end\":36524,\"start\":36519},{\"end\":36544,\"start\":36528},{\"end\":36556,\"start\":36548},{\"end\":36992,\"start\":36976},{\"end\":37001,\"start\":36996},{\"end\":37013,\"start\":37005},{\"end\":37477,\"start\":37470},{\"end\":37487,\"start\":37481},{\"end\":37751,\"start\":37744},{\"end\":37767,\"start\":37755},{\"end\":37778,\"start\":37771},{\"end\":38244,\"start\":38238},{\"end\":38259,\"start\":38248},{\"end\":38268,\"start\":38263},{\"end\":38278,\"start\":38272},{\"end\":38769,\"start\":38766},{\"end\":38777,\"start\":38773},{\"end\":38784,\"start\":38781},{\"end\":39162,\"start\":39158},{\"end\":39173,\"start\":39166},{\"end\":39184,\"start\":39177},{\"end\":39197,\"start\":39188},{\"end\":39203,\"start\":39201},{\"end\":39214,\"start\":39207},{\"end\":39224,\"start\":39218},{\"end\":39235,\"start\":39228},{\"end\":39244,\"start\":39239},{\"end\":39588,\"start\":39581},{\"end\":39599,\"start\":39592},{\"end\":39612,\"start\":39603},{\"end\":39621,\"start\":39616},{\"end\":39630,\"start\":39625},{\"end\":39644,\"start\":39634},{\"end\":39654,\"start\":39648},{\"end\":40098,\"start\":40094},{\"end\":40111,\"start\":40102},{\"end\":40122,\"start\":40115},{\"end\":40603,\"start\":40601},{\"end\":40610,\"start\":40607},{\"end\":40617,\"start\":40614},{\"end\":40871,\"start\":40868},{\"end\":40880,\"start\":40875},{\"end\":40888,\"start\":40884},{\"end\":40895,\"start\":40892},{\"end\":40903,\"start\":40899},{\"end\":40910,\"start\":40907},{\"end\":40920,\"start\":40914},{\"end\":41402,\"start\":41397},{\"end\":41411,\"start\":41406},{\"end\":41417,\"start\":41415},{\"end\":41425,\"start\":41421},{\"end\":41654,\"start\":41650},{\"end\":41661,\"start\":41658},{\"end\":41669,\"start\":41665},{\"end\":41678,\"start\":41673},{\"end\":41686,\"start\":41682},{\"end\":41695,\"start\":41690},{\"end\":42011,\"start\":42007},{\"end\":42019,\"start\":42015},{\"end\":42027,\"start\":42023},{\"end\":42034,\"start\":42031},{\"end\":42041,\"start\":42038},{\"end\":42050,\"start\":42045}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1707.07397\",\"id\":\"b0\"},\"end\":31923,\"start\":31674},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2893830},\"end\":32218,\"start\":31925},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4891043},\"end\":32683,\"start\":32220},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4119221},\"end\":33131,\"start\":32685},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":102350868},\"end\":33624,\"start\":33133},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":29162614},\"end\":34209,\"start\":33626},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b6\"},\"end\":34458,\"start\":34211},{\"attributes\":{\"id\":\"b7\"},\"end\":34767,\"start\":34460},{\"attributes\":{\"doi\":\"arXiv:1607.02533\",\"id\":\"b8\"},\"end\":35014,\"start\":34769},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":92983361},\"end\":35372,\"start\":35016},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206596594},\"end\":35842,\"start\":35374},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3488815},\"end\":36298,\"start\":35844},{\"attributes\":{\"id\":\"b12\"},\"end\":36467,\"start\":36300},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53229868},\"end\":36903,\"start\":36469},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12387176},\"end\":37384,\"start\":36905},{\"attributes\":{\"doi\":\"arXiv:1810.10337\",\"id\":\"b15\"},\"end\":37674,\"start\":37386},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206592766},\"end\":38147,\"start\":37676},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207241700},\"end\":38709,\"start\":38149},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":91183979},\"end\":39102,\"start\":38711},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49904930},\"end\":39535,\"start\":39104},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":604334},\"end\":40002,\"start\":39537},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":121124946},\"end\":40524,\"start\":40004},{\"attributes\":{\"doi\":\"arXiv:1802.09707\",\"id\":\"b22\"},\"end\":40792,\"start\":40526},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3972825},\"end\":41309,\"start\":40794},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10585115},\"end\":41646,\"start\":41311},{\"attributes\":{\"doi\":\"arXiv:1812.10217\",\"id\":\"b25\"},\"end\":41934,\"start\":41648},{\"attributes\":{\"doi\":\"arXiv:1803.04683\",\"id\":\"b26\"},\"end\":42255,\"start\":41936}]", "bib_title": "[{\"end\":31977,\"start\":31925},{\"end\":32300,\"start\":32220},{\"end\":32727,\"start\":32685},{\"end\":33219,\"start\":33133},{\"end\":33693,\"start\":33626},{\"end\":35100,\"start\":35016},{\"end\":35433,\"start\":35374},{\"end\":35905,\"start\":35844},{\"end\":36515,\"start\":36469},{\"end\":36972,\"start\":36905},{\"end\":37740,\"start\":37676},{\"end\":38234,\"start\":38149},{\"end\":38762,\"start\":38711},{\"end\":39154,\"start\":39104},{\"end\":39577,\"start\":39537},{\"end\":40090,\"start\":40004},{\"end\":40864,\"start\":40794},{\"end\":41393,\"start\":41311}]", "bib_author": "[{\"end\":31685,\"start\":31674},{\"end\":31697,\"start\":31685},{\"end\":31706,\"start\":31697},{\"end\":31714,\"start\":31706},{\"end\":31990,\"start\":31979},{\"end\":32000,\"start\":31990},{\"end\":32310,\"start\":32302},{\"end\":32323,\"start\":32310},{\"end\":32333,\"start\":32323},{\"end\":32341,\"start\":32333},{\"end\":32737,\"start\":32729},{\"end\":32745,\"start\":32737},{\"end\":32753,\"start\":32745},{\"end\":32759,\"start\":32753},{\"end\":32766,\"start\":32759},{\"end\":32772,\"start\":32766},{\"end\":32778,\"start\":32772},{\"end\":33229,\"start\":33221},{\"end\":33237,\"start\":33229},{\"end\":33243,\"start\":33237},{\"end\":33250,\"start\":33243},{\"end\":33706,\"start\":33695},{\"end\":33717,\"start\":33706},{\"end\":33730,\"start\":33717},{\"end\":33736,\"start\":33730},{\"end\":33747,\"start\":33736},{\"end\":33755,\"start\":33747},{\"end\":33766,\"start\":33755},{\"end\":33775,\"start\":33766},{\"end\":33783,\"start\":33775},{\"end\":34225,\"start\":34211},{\"end\":34235,\"start\":34225},{\"end\":34246,\"start\":34235},{\"end\":34568,\"start\":34559},{\"end\":34578,\"start\":34568},{\"end\":34586,\"start\":34578},{\"end\":34604,\"start\":34586},{\"end\":34780,\"start\":34769},{\"end\":34794,\"start\":34780},{\"end\":34804,\"start\":34794},{\"end\":35108,\"start\":35102},{\"end\":35119,\"start\":35108},{\"end\":35129,\"start\":35119},{\"end\":35442,\"start\":35435},{\"end\":35449,\"start\":35442},{\"end\":35455,\"start\":35449},{\"end\":35461,\"start\":35455},{\"end\":35468,\"start\":35461},{\"end\":35476,\"start\":35468},{\"end\":35916,\"start\":35907},{\"end\":35927,\"start\":35916},{\"end\":35938,\"start\":35927},{\"end\":35949,\"start\":35938},{\"end\":35958,\"start\":35949},{\"end\":36346,\"start\":36336},{\"end\":36355,\"start\":36346},{\"end\":36361,\"start\":36355},{\"end\":36526,\"start\":36517},{\"end\":36546,\"start\":36526},{\"end\":36558,\"start\":36546},{\"end\":36994,\"start\":36974},{\"end\":37003,\"start\":36994},{\"end\":37015,\"start\":37003},{\"end\":37479,\"start\":37468},{\"end\":37489,\"start\":37479},{\"end\":37753,\"start\":37742},{\"end\":37769,\"start\":37753},{\"end\":37780,\"start\":37769},{\"end\":38246,\"start\":38236},{\"end\":38261,\"start\":38246},{\"end\":38270,\"start\":38261},{\"end\":38280,\"start\":38270},{\"end\":38771,\"start\":38764},{\"end\":38779,\"start\":38771},{\"end\":38786,\"start\":38779},{\"end\":39164,\"start\":39156},{\"end\":39175,\"start\":39164},{\"end\":39186,\"start\":39175},{\"end\":39199,\"start\":39186},{\"end\":39205,\"start\":39199},{\"end\":39216,\"start\":39205},{\"end\":39226,\"start\":39216},{\"end\":39237,\"start\":39226},{\"end\":39246,\"start\":39237},{\"end\":39590,\"start\":39579},{\"end\":39601,\"start\":39590},{\"end\":39614,\"start\":39601},{\"end\":39623,\"start\":39614},{\"end\":39632,\"start\":39623},{\"end\":39646,\"start\":39632},{\"end\":39656,\"start\":39646},{\"end\":40100,\"start\":40092},{\"end\":40113,\"start\":40100},{\"end\":40124,\"start\":40113},{\"end\":40605,\"start\":40599},{\"end\":40612,\"start\":40605},{\"end\":40619,\"start\":40612},{\"end\":40873,\"start\":40866},{\"end\":40882,\"start\":40873},{\"end\":40890,\"start\":40882},{\"end\":40897,\"start\":40890},{\"end\":40905,\"start\":40897},{\"end\":40912,\"start\":40905},{\"end\":40922,\"start\":40912},{\"end\":41404,\"start\":41395},{\"end\":41413,\"start\":41404},{\"end\":41419,\"start\":41413},{\"end\":41427,\"start\":41419},{\"end\":41656,\"start\":41648},{\"end\":41663,\"start\":41656},{\"end\":41671,\"start\":41663},{\"end\":41680,\"start\":41671},{\"end\":41688,\"start\":41680},{\"end\":41697,\"start\":41688},{\"end\":42013,\"start\":42005},{\"end\":42021,\"start\":42013},{\"end\":42029,\"start\":42021},{\"end\":42036,\"start\":42029},{\"end\":42043,\"start\":42036},{\"end\":42052,\"start\":42043}]", "bib_venue": "[{\"end\":31770,\"start\":31730},{\"end\":32048,\"start\":32000},{\"end\":32423,\"start\":32341},{\"end\":32855,\"start\":32778},{\"end\":33327,\"start\":33250},{\"end\":33860,\"start\":33783},{\"end\":34307,\"start\":34261},{\"end\":34557,\"start\":34460},{\"end\":34862,\"start\":34820},{\"end\":35173,\"start\":35129},{\"end\":35553,\"start\":35476},{\"end\":36029,\"start\":35958},{\"end\":36334,\"start\":36300},{\"end\":36635,\"start\":36558},{\"end\":37092,\"start\":37015},{\"end\":37466,\"start\":37386},{\"end\":37857,\"start\":37780},{\"end\":38365,\"start\":38280},{\"end\":38863,\"start\":38786},{\"end\":39306,\"start\":39246},{\"end\":39727,\"start\":39656},{\"end\":40211,\"start\":40124},{\"end\":40597,\"start\":40526},{\"end\":40999,\"start\":40922},{\"end\":41457,\"start\":41427},{\"end\":41765,\"start\":41713},{\"end\":42003,\"start\":41936},{\"end\":32919,\"start\":32857},{\"end\":33391,\"start\":33329},{\"end\":33924,\"start\":33862},{\"end\":35617,\"start\":35555},{\"end\":36087,\"start\":36031},{\"end\":36699,\"start\":36637},{\"end\":37156,\"start\":37094},{\"end\":37921,\"start\":37859},{\"end\":38437,\"start\":38367},{\"end\":38927,\"start\":38865},{\"end\":39785,\"start\":39729},{\"end\":40285,\"start\":40213},{\"end\":41063,\"start\":41001}]"}}}, "year": 2023, "month": 12, "day": 17}