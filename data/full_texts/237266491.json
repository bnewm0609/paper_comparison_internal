{"id": 237266491, "updated": "2023-10-05 23:42:36.884", "metadata": {"title": "SwinIR: Image Restoration Using Swin Transformer", "authors": "[{\"first\":\"Jingyun\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Jiezhang\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Guolei\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Luc\",\"last\":\"Gool\",\"middle\":[\"Van\"]},{\"first\":\"Radu\",\"last\":\"Timofte\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "journal": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "publication_date": {"year": 2021, "month": 8, "day": 23}, "abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\\textbf{up to 0.14$\\sim$0.45dB}$, while the total number of parameters can be reduced by $\\textbf{up to 67%}$.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2108.10257", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/LiangCSZGT21", "doi": "10.1109/iccvw54120.2021.00210"}}, "content": {"source": {"pdf_hash": "7a9a708ca61c14886aa0dcd6d13dac7879713f5f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.10257v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2108.10257", "status": "GREEN"}}, "grobid": {"id": "c5d25fb0e9b89fceb2531041688ae248f67c4a1c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7a9a708ca61c14886aa0dcd6d13dac7879713f5f.txt", "contents": "\nSwinIR: Image Restoration Using Swin Transformer\n\n\nJingyun Liang jinliang@vision.ee.ethz.ch \nComputer Vision Lab\nETH Zurich\nSwitzerland\n\nJiezhang Cao \nComputer Vision Lab\nETH Zurich\nSwitzerland\n\nGuolei Sun guosun@vision.ee.ethz.ch \nComputer Vision Lab\nETH Zurich\nSwitzerland\n\nKai Zhang kai.zhang@vision.ee.ethz.ch \nLuc Van Gool vangool@vision.ee.ethz.ch \nComputer Vision Lab\nETH Zurich\nSwitzerland\n\nLeuvenBelgium\n\nRadu Timofte timofter@vision.ee.ethz.ch \nComputer Vision Lab\nETH Zurich\nSwitzerland\n\nJingyunliang / Com/ \nComputer Vision Lab\nETH Zurich\nSwitzerland\n\nSwinir \nSwinIR: Image Restoration Using Swin Transformer\n\nImage restoration is a long-standing low-level vision problem that aims to restore high-quality images from lowquality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14\u223c0.45dB, while the total number of parameters can be reduced by up to 67%.\n\nIntroduction\n\nImage restoration, such as image super-resolution (SR), image denoising and JPEG compression artifact reduction, aims to reconstruct the high-quality clean image from its low-quality degraded counterpart. Since several revolutionary work [18,40,90,91], convolutional neural networks (CNN) have become the primary workhorse for image restoration [43,51,43,81,92,95,24,93,46,89,88].\n\nMost CNN-based methods focus on elaborate architecture designs such as residual learning [43,51] and dense connections [97,81]. Although the performance is significantly improved compared with traditional model-based * Corresponding author. methods [73,14,28], they generally suffer from two basic problems that stem from the basic convolution layer. First, the interactions between images and convolution kernels are content-independent. Using the same convolution kernel to restore different image regions may not be the best choice. Second, under the principle of local processing, convolution is not effective for long-range dependency modelling.\n\nAs an alternative to CNN, Transformer [76] designs a self-attention mechanism to capture global interactions between contexts and has shown promising performance in several vision problems [6,74,19,56]. However, vision Transformers for image restoration [9,5] usually divide the input image into patches with fixed size (e.g., 48\u00d748) and process each patch independently. Such a strategy inevitably gives rise to two drawbacks. First, border pixels cannot utilize neighbouring pixels that are out of the patch for image restoration. Second, the restored image may introduce border artifacts around each patch. While this problem can be alleviated by patch overlapping, it would introduce extra computational burden.\n\nRecently, Swin Transformer [56] has shown great promise as it integrates the advantages of both CNN and Transformer. On the one hand, it has the advantage of CNN to process image with large size due to the local attention mechanism. On the other hand, it has the advantage of Transformer to model long-range dependency with the shifted window scheme.\n\nIn this paper, we propose an image restoration model, namely SwinIR, based on Swin Transformer. More specifically, SwinIR consists of three modules: shallow feature extraction, deep feature extraction and high-quality image reconstruction modules. Shallow feature extraction module uses a convolution layer to extract shallow feature, which is directly transmitted to the reconstruction module so as to preserve low-frequency information. Deep feature extraction module is mainly composed of residual Swin Transformer blocks (RSTB), each of which utilizes several Swin Transformer layers for local attention and cross-window interaction. In addition, we add a convolution layer at the end of the block for feature enhancement and use a residual connection to provide a shortcut for feature aggregation. Finally, both shallow and deep features are fused in the reconstruction module for high-quality image reconstruction.\n\nCompared with prevalent CNN-based image restoration models, Transformer-based SwinIR has several benefits: (1) content-based interactions between image content and attention weights, which can be interpreted as spatially varying convolution [13,21,75]. (2) long-range dependency modelling are enabled by the shifted window mechanism.\n\n(3) better performance with less parameters. For example, as shown in Fig. 1, SwinIR achieves better PSNR with less parameters compared with existing image SR methods.\n\n\nRelated Work\n\n\nImage Restoration\n\nCompared to traditional image restoration methods [28,72,73,62,32] which are generally model-based, learningbased methods, especially CNN-based methods, have become more popular due to their impressive performance. They often learn mappings between low-quality and highquality images from large-scale paired datasets. Since pioneering work SRCNN [18] (for image SR), DnCNN [90] (for image denoising) and ARCNN [17] (for JPEG compression artifact reduction), a flurry of CNN-based models have been proposed to improve model representation ability by using more elaborate neural network architecture designs, such as residual block [40,7,88], dense block [81,97,98] and others [10,42,93,78,77,79,50,48,49,92,70,36,83,30,11,16,96,64,38,26,41,25]. Some of them have exploited the attention mechanism inside the CNN framework, such as channel attention [95,15,63], non-local attention [52,61] and adaptive patch aggregation [100].\n\n\nVision Transformer\n\nRecently, natural language processing model Transformer [76] has gained much popularity in the computer vision community. When used in vision problems such as image classification [66,19,84,56,45,55,75], ob-ject detection [6,53,74,56], segmentation [84,99,56,4] and crowd counting [47,69], it learns to attend to important image regions by exploring the global interactions between different regions. Due to its impressive performance, Transformer has also been introduced for image restoration [9,5,82]. Chen et al. [9] proposed a backbone model IPT for various restoration problems based on the standard Transformer. However, IPT relies on large number of parameters (over 115.5M parameters), large-scale datasets (over 1.1M images) and multi-task learning for good performance. Cao et al. [5] proposed VSR-Transformer that uses the self-attention mechanism for better feature fusion in video SR, but image features are still extracted from CNN. Besides, both IPT and VSR-Transformer are patch-wise attention, which may be improper for image restoration. In addition, a concurrent work [82] proposed a U-shaped architecture based on the Swin Transformer [56].\n\n\nMethod\n\n\nNetwork Architecture\n\nAs shown in Fig. 2, SwinIR consists of three modules: shallow feature extraction, deep feature extraction and highquality (HQ) image reconstruction modules. We employ the same feature extraction modules for all restoration tasks, but use different reconstruction modules for different tasks. Shallow and deep feature extraction. Given a lowquality (LQ) input I LQ \u2208 R H\u00d7W \u00d7Cin (H, W and C in are the image height, width and input channel number, respectively), we use a 3 \u00d7 3 convolutional layer H SF (\u00b7) to extract shallow feature F 0 \u2208 R H\u00d7W \u00d7C as\nF 0 = H SF (I LQ ),(1)\nwhere C is the feature channel number. The convolution layer is good at early visual processing, leading to more stable optimization and better results [86]. It also provides a simple way to map the input image space to a higher dimensional feature space. Then, we extract deep feature F DF \u2208 R H\u00d7W \u00d7C from F 0 as\nF DF = H DF (F 0 ),(2)\nwhere H DF (\u00b7) is the deep feature extraction module and it contains K residual Swin Transformer blocks (RSTB) and a 3 \u00d7 3 convolutional layer. More specifically, intermediate features F 1 , F 2 , . . . , F K and the output deep feature F DF are extracted block by block as\nF i = H RSTBi (F i\u22121 ), i = 1, 2, . . . , K, F DF = H CONV (F K ),(3)\nwhere H RSTBi (\u00b7) denotes the i-th RSTB and H CONV is the last convolutional layer. Using a convolutional layer at the  end of feature extraction can bring the inductive bias of the convolution operation into the Transformer-based network, and lay a better foundation for the later aggregation of shallow and deep features. Image reconstruction. Taking image SR as an example, we reconstruct the high-quality image I RHQ by aggregating shallow and deep features as\nI RHQ = H REC (F 0 + F DF ),(4)\nwhere H REC (\u00b7) is the function of the reconstruction module. Shallow feature mainly contain low-frequencies, while deep feature focus on recovering lost high-frequencies.\n\nWith a long skip connection, SwinIR can transmit the lowfrequency information directly to the reconstruction module, which can help deep feature extraction module focus on high-frequency information and stabilize training. For the implementation of reconstruction module, we use the sub-pixel convolution layer [68] to upsample the feature. For tasks that do not need upsampling, such as image denoising and JPEG compression artifact reduction, a single convolution layer is used for reconstruction. Besides, we use residual learning to reconstruct the residual between the LQ and the HQ image instead of the HQ image. This is formulated as\nI RHQ = H SwinIR (I LQ ) + I LQ ,(5)\nwhere H SwinIR (\u00b7) denotes the function of SwinIR. Loss function. For image SR, we optimize the parameters of SwinIR by minimizing the L 1 pixel loss\nL = I RHQ \u2212 I HQ 1 ,(6)\nwhere I RHQ is obtained by taking I LQ as the input of SwinIR, and I HQ is the corresponding ground-truth HQ image. For classical and lightweight image SR, we only use the naive L 1 pixel loss as same as previous work to show the effectiveness of the proposed network. For real-world image SR, we use a combination of pixel loss, GAN loss and perceptual loss [81,89,80,27,39,81] to improve visual quality. For image denoising and JPEG compression artifact reduction, we use the Charbonnier loss [8]\nL = I RHQ \u2212 I HQ 2 + 2 ,(7)\nwhere is a constant that is empirically set to 10 \u22123 .\n\n\nResidual Swin Transformer Block\n\nAs shown in Fig. 2(a), the residual Swin Transformer block (RSTB) is a residual block with Swin Transformer layers (STL) and convolutional layers. Given the input feature F i,0 of the i-th RSTB, we first extract intermediate features F i,1 , F i,2 , . . . , F i,L by L Swin Transformer layers as\nF i,j = H STLi,j (F i,j\u22121 ), j = 1, 2, . . . , L,(8)\nwhere H STLi,j (\u00b7) is the j-th Swin Transformer layer in the i-th RSTB. Then, we add a convolutional layer before the residual connection. The output of RSTB is formulated as\nF i,out = H CONVi (F i,L ) + F i,0 ,(9)\nwhere H CONVi (\u00b7) is the convolutional layer in the i-th RSTB. This design has two benefits. First, although Transformer can be viewed as a specific instantiation of spatially varying convolution [21,75], covolutional layers with spatially invariant filters can enhance the translational equivariance of SwinIR. Second, the residual connection provides a identity-based connection from different blocks to the reconstruction module, allowing the aggregation of different levels of features.\n\nSwin Transformer layer. Swin Transformer layer (STL) [56] is based on the standard multi-head selfattention of the original Transformer layer [76]. The main differences lie in local attention and the shifted window mechanism. As shown in Fig. 2(b), given an input of size H \u00d7 W \u00d7 C, Swin Transformer first reshapes the input to a HW M 2 \u00d7 M 2 \u00d7 C feature by partitioning the input into non-overlapping M \u00d7 M local windows, where HW M 2 is the total number of windows. Then, it computes the standard self-attention separately for each window (i.e., local attention). For a local window feature X \u2208 R M 2 \u00d7C , the query, key and value matrices Q, K and V are computed as\nQ = XP Q , K = XP K , V = XP V ,(10)\nwhere P Q , P K and P V are projection matrices that are shared across different windows. Generally, we have Q, K, V \u2208 R M 2 \u00d7d . The attention matrix is thus computed by the self-attention mechanism in a local window as\nAttention(Q, K, V ) = SoftMax(QK T / \u221a d + B)V,(11)\nwhere B is the learnable relative positional encoding. In practice, following [76], we perform the attention function for h times in parallel and concatenate the results for multihead self-attention (MSA). Next, a multi-layer perceptron (MLP) that has two fullyconnected layers with GELU non-linearity between them is used for further feature transformations. The LayerNorm (LN) layer is added before both MSA and MLP, and the residual connection is employed for both modules. The whole process is formulated as\nX = MSA(LN(X)) + X, X = MLP(LN(X)) + X.(12)\nHowever, when the partition is fixed for different layers, there is no connection across local windows. Therefore, regular and shifted window partitioning are used alternately to enable cross-window connections [56], where shifted window partitioning means shifting the feature by ( M 2 , M 2 ) pixels before partitioning.\n\n\nExperiments\n\n\nExperimental Setup\n\nFor classical image SR, real-world image SR, image denoising and JPEG compression artifact reduction, the RSTB number, STL number, window size, channel number and attention head number are generally set to 6, 6, 8, 180 and 6, respectively. One exception is that the window size is set to 7 for JPEG compression artifact reduction, as we observe significant performance drop when using 8, possibly because JPEG encoding uses 8 \u00d7 8 image partions. For lightweight image SR, we decrease RSTB number and channel number to 4 and 60, respectively. Following [95,63], when self-ensemble strategy [51] is used in testing, we mark the model with a symbol \"+\", e.g., SwinIR+. Due to page limit, training and evaluation details are provided in the supplementary.\n\n\nAblation Study and Discussion\n\nFor ablation study, we train SwinIR on DIV2K [1] for classical image SR (\u00d72) and test it on Manga109 [60]. Impact of channel number, RSTB number and STL number. We show the effects of channel number, RSTB number and STL number in a RSTB on model performance in Figs. 3(a), 3(b) and 3(c), respectively. It is observed that the PSNR is positively correlated with these three hyperparameters. For channel number, although the performance keeps increasing, the total number of parameters grows quadratically. To balance the performance and model size, we choose 180 as the channel number in rest experiments. As for RSTB number and layer number, the performance gain becomes saturated gradually. We choose 6 for both of them to obtain a relatively small model. Impact of patch size and training image number; model convergence comparison. We compare the proposed SwinIR with a representative CNN-based model RCAN to compare the difference of Transformer-based and CNNbased models. From Fig. 3(d), one can see that SwinIR performs better than RCAN on different patch sizes, and the PSNR gain becomes larger when the patch size is larger. Fig. 3(e) shows the impact of the number of training images. Extra images from Flickr2K are used in training when the percentage is larger than 100% (800 images). There are two observations. First, as expected, the performance of SwinIR rises with the training image number. Second, different from the observation in IPT that Transformer-based models are heavily relied on large amount of training data, SwinIR achieves better results than CNN-based models using the same training data, even when the dataset is small (i.e., 25%, 200 images). We also plot the PSNR during training for both SwinIR and RCAN in Fig. 3(f). It is clear that SwinIR converges faster and better than RCAN, which is contradictory to previous observations that Transformerbased models often suffer from slow model convergence. Impact of residual connection and convolution layer in RSTB. Table 1 shows four residual connection variants in RSTB: no residual connection, using 1 \u00d7 1 convolution layer, using 3 \u00d7 3 convolution layer and using three 3 \u00d7 3 convolution layers (channel number of the intermediate layer is set to one fourth of network channel number). From the table, we can have following observations. First, the residual connection in RSTB is important as it improves the PSNR by 0.16dB. Second, using 1 \u00d7 1 convolution brings little improvement maybe because it cannot 60    extract local neighbouring information as 3 \u00d7 3 convolution does. Third, although using three 3 \u00d7 3 convolution layers can reduce the number of parameters, the performance drops slightly.\n\n\nResults on Image SR\n\nClassical image SR. Table 2 shows the quantitative comparisons between SwinIR (middle size) and state-of-the-art methods: DBPN [31], RCAN [95], RRDB [81], SAN [15], IGNN [100], HAN [63], NLSA [61] and IPT [9]. As one can see, when trained on DIV2K, SwinIR achieves best performance on almost all five benchmark datasets for all scale factors. The maximum PSNR gain reaches 0.26dB on Manga109 for scale factor 4. Note that RCAN and HAN introduce channel and spatial attention, IGNN proposes adaptive patch feature aggregation, and NLSA is based on the non-local attention mechanism. However, all these CNN-based attention mechanisms perform worse than the proposed Transformer-based SwinIR, which indicates the effectiveness of the proposed model. When we train SwinIR on a larger dataset (DIV2K+Flickr2K), the performance further increases by a large margin (up to 0.47dB), achieving better accuracy than the same Transformer-based model IPT, even though IPT utilizes ImageNet (more than 1.3M images) in training and has huge number of parameters (115.5M). In contrast, SwinIR has a small number of parameters (11.8M) even compared with state-of-the-art CNN-based models (15.4\u223c44.3M). As for runtime, representative CNN-based model RCAN, IPT and SwinIR take about 0.2, 4.5s and 1.1s to test on a 1, 024 \u00d7 1, 024 image, respectively. Visual comparisons are show in Fig. 4. SwinIR can restore high-frequency details and alleviate the blurring artifacts, resulting in sharp and natural edges. In contrast, most CNN-based methods produces blurry images or even incorrect textures. IPT generates better images compared with CNN-based methods, but it suffers from image distortions and border artifact. Lightweight image SR. We also provide comparison of SwinIR (small size) with state-of-the-art lightweight image SR methods: CARN [2], FALSR-A [12], IMDN [35], LAPAR-A [44] and LatticeNet [57]. In addition to PSNR and SSIM, we also report the total numbers of parameters and multiply-accumulate operations (evaluated on a 1280 \u00d7 720 HQ image) to compare the model size and computational complexity of different models. As shown in Table 3, SwinIR outperforms competitive methods by a PSNR margin of up to 0.53dB on different benchmark datasets, with similar total numbers of parameters and multiplyaccumulate operations. This indicates that the SwinIR architecture is highly efficient for image restoration.\n\nReal-world image SR. The ultimate goal of image SR is for real-world applications. Recently, Zhang et al. [89] proposed a practical degradation model BSRGAN for realworld image SR and achieved surprising results in real scenarios 1 . To test the performance of SwinIR for realworld SR, we re-train SwinIR by using the same degradation model as BSRGAN for low-quality image synthesis. Since there is no ground-truth high-quality images, we only provide visual comparison with representative bicubic model ESRGAN [81] and state-of-the-art realworld image SR models RealSR [37], BSRGAN [89] and Real-ESRGAN [80]. As shown in Fig. 5, SwinIR produces visually pleasing images with clear and sharp edges, whereas other compared methods may suffer from unsatisfactory artifacts. In addition, to exploit the full potential of SwinIR for real applications, we further propose a  Figure 4: Visual comparison of bicubic image SR (\u00d74) methods. Compared images are derived from [9]. Best viewed by zooming.    Table 4 shows the comparison of SwinIR with stateof-the-art JPEG compression artifact reduction methods: ARCNN [17], DnCNN-3 [90], QGAC [20], RNAN [96], RDN [98] and DRUNet [88]. All of compared methods are CNN-based models. Following [98,88], we test different methods on two benchmark datasets (Classic5 [22] and LIVE1 [67]) for JPEG quality factors 10, 20, 30 and 40. As we can see, the proposed SwinIR has average PSNR gains of at least 0.11dB and 0.07dB on two testing datasets for different quality factors. Besides, compared with the previous best model DRUNet, SwinIR only has 11.5M parameters, while DRUNet is a large model that has 32.7M parameters.\n\n\nResults on JPEG Compression Artifact Reduction\n\n\nResults on Image Denoising\n\nWe show grayscale and color image denoising results in Table 5 and Table 6, respectively.\n\nCom-pared methods include traditional models BM3D [14] and WNNM [29], CNN-based models DnCNN [90], IR-CNN [91], FFDNet [92], N3Net [65], NLRN [52], FOC-Net [38], RNAN [96], MWCNN [54] and DRUNet [88]. Following [90,88], the compared noise levels include 15, 25 and 50. As one can see, our model achieves better performance than all compared methods. In particular, it surpasses the state-of-the-art model DRUNet by up to 0.3dB on the large Urban100 dataset that has 100 high-resolution testing images. It is worth pointing out that SwinIR only has 12.0M parameters, whereas DRUNet has 32.7M parameters. This indicates that the SwinIR architecture is highly efficient in learning feature representations for restoration. The visual comparison for grayscale and color image denoising of different methods are shown in Figs. 6 and 7.\n\nAs we can see, our method can remove heavy noise corruption and preserve high-frequency image details, resulting in sharper edges and more natural textures. By contrast, other methods suffer from either over-smoothness or oversharpness, and cannot recover rich textures.\n\n\nConclusion\n\nIn this paper, we propose a Swin Transformer-based image restoration model SwinIR. The model is composed of three parts: shallow feature extraction, deep feature extrac-  Noisy BM3D [14] DnCNN [90] FFDNet [92] DRUNet [88] SwinIR (ours) Figure 6: Visual comparison of grayscale image denoising (noise level 50) methods on image \"Monarch\" from Set12 [90]. Compared images are derived from [88].\n\nNoisy DnCNN [90] FFDNet [92] IPT [9] DRUNet [88] SwinIR (ours) Figure 7: Visual comparison of color image denoising (noise level 50) methods on image \"163085\" from CBSD68 [59]. Compared images are derived from [88].\n\ntion and HR reconstruction modules. In particular, we use a stack of residual Swin Transformer blocks (RSTB) for deep feature extraction, and each RSTB is composed of Swin Transformer layers, convolution layer and a residual connection. Extensive experiments show that SwinIR achieves state-of-the-art performance on three representative image restoration tasks and six different settings: classic image SR, lightweight image SR, real-world image SR, grayscale image denoising, color image denoising and JPEG com-pression artifact reduction, which demonstrates the effectiveness and generalizability of the proposed SwinIR. In the future, we will extend the model to other restoration tasks such as image deblurring and deraining.\n\nFigure 1 :\n1PSNR results v.s the total number of parameters of different methods for image SR (\u00d74) on Set5[3].\n\nFigure 2 :\n2The architecture of the proposed SwinIR for image restoration.\n\nFigure 3 :\n3Ablation study on different settings of SwinIR. Results are tested on Manga109[60] for image SR (\u00d72).\n\nFigure 5 :\n5Visual comparison of real-world image SR (\u00d74) methods on real-world images.\n\nTable 1 :\n1Ablation study on RSTB design.Design \nNo residual \n1 \u00d7 1 conv \n3 \u00d7 3 conv \nThree 3 \u00d7 3 conv \nPSNR \n39.42 \n39.45 \n39.58 \n39.56 \n\n\n\nTable 2 :\n2Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for classical image SR on benchmark datasets. Best and second best performance are in red and blue colors, respectively. Results on \u00d78 are provided in supplementary.Method \nScale \nTraining \nDataset \n\nSet5 [3] \nSet14 [87] \nBSD100 [58] \nUrban100 [34] \nManga109 [60] \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \n\nRCAN [95] \n\u00d72 \nDIV2K \n38.27 \n0.9614 \n34.12 \n0.9216 \n32.41 \n0.9027 \n33.34 \n0.9384 \n39.44 \n0.9786 \nSAN [15] \n\u00d72 \nDIV2K \n38.31 \n0.9620 \n34.07 \n0.9213 \n32.42 \n0.9028 \n33.10 \n0.9370 \n39.32 \n0.9792 \nIGNN [100] \n\u00d72 \nDIV2K \n38.24 \n0.9613 \n34.07 \n0.9217 \n32.41 \n0.9025 \n33.23 \n0.9383 \n39.35 \n0.9786 \nHAN [63] \n\u00d72 \nDIV2K \n38.27 \n0.9614 \n34.16 \n0.9217 \n32.41 \n0.9027 \n33.35 \n0.9385 \n39.46 \n0.9785 \nNLSA [61] \n\u00d72 \nDIV2K \n38.34 \n0.9618 \n34.08 \n0.9231 \n32.43 \n0.9027 \n33.42 \n0.9394 \n39.59 \n0.9789 \nSwinIR (Ours) \n\u00d72 \nDIV2K \n38.35 \n0.9620 \n34.14 \n0.9227 \n32.44 \n0.9030 \n33.40 \n0.9393 \n39.60 \n0.9792 \nSwinIR+ (Ours) \n\u00d72 \nDIV2K \n38.38 \n0.9621 \n34.24 \n0.9233 \n32.47 \n0.9032 \n33.51 \n0.9401 \n39.70 \n0.9794 \nDBPN [31] \n\u00d72 \nDIV2K+Flickr2K \n38.09 \n0.9600 \n33.85 \n0.9190 \n32.27 \n0.9000 \n32.55 \n0.9324 \n38.89 \n0.9775 \nIPT [9] \n\u00d72 \nImageNet \n38.37 \n-\n34.43 \n-\n32.48 \n-\n33.76 \n-\n-\n-\nSwinIR (Ours) \n\u00d72 \nDIV2K+Flickr2K \n38.42 \n0.9623 \n34.46 \n0.9250 \n32.53 \n0.9041 \n33.81 \n0.9427 \n39.92 \n0.9797 \nSwinIR+ (Ours) \n\u00d72 \nDIV2K+Flickr2K \n38.46 \n0.9624 \n34.61 \n0.9260 \n32.55 \n0.9043 \n33.95 \n0.9433 \n40.02 \n0.9800 \n\nRCAN [95] \n\u00d73 \nDIV2K \n34.74 \n0.9299 \n30.65 \n0.8482 \n29.32 \n0.8111 \n29.09 \n0.8702 \n34.44 \n0.9499 \nSAN [15] \n\u00d73 \nDIV2K \n34.75 \n0.9300 \n30.59 \n0.8476 \n29.33 \n0.8112 \n28.93 \n0.8671 \n34.30 \n0.9494 \nIGNN [100] \n\u00d73 \nDIV2K \n34.72 \n0.9298 \n30.66 \n0.8484 \n29.31 \n0.8105 \n29.03 \n0.8696 \n34.39 \n0.9496 \nHAN [63] \n\u00d73 \nDIV2K \n34.75 \n0.9299 \n30.67 \n0.8483 \n29.32 \n0.8110 \n29.10 \n0.8705 \n34.48 \n0.9500 \nNLSA [61] \n\u00d73 \nDIV2K \n34.85 \n0.9306 \n30.70 \n0.8485 \n29.34 \n0.8117 \n29.25 \n0.8726 \n34.57 \n0.9508 \nSwinIR (Ours) \n\u00d73 \nDIV2K \n34.89 \n0.9312 \n30.77 \n0.8503 \n29.37 \n0.8124 \n29.29 \n0.8744 \n34.74 \n0.9518 \nSwinIR+ (Ours) \n\u00d73 \nDIV2K \n34.95 \n0.9316 \n30.83 \n0.8511 \n29.41 \n0.8130 \n29.42 \n0.8761 \n34.92 \n0.9526 \nIPT [9] \n\u00d73 \nImageNet \n34.81 \n-\n30.85 \n-\n29.38 \n-\n29.49 \n-\n-\n-\nSwinIR (Ours) \n\u00d73 \nDIV2K+Flickr2K \n34.97 \n0.9318 \n30.93 \n0.8534 \n29.46 \n0.8145 \n29.75 \n0.8826 \n35.12 \n0.9537 \nSwinIR+ (Ours) \n\u00d73 \nDIV2K+Flickr2K \n35.04 \n0.9322 \n31.00 \n0.8542 \n29.49 \n0.8150 \n29.90 \n0.8841 \n35.28 \n0.9543 \n\nRCAN [95] \n\u00d74 \nDIV2K \n32.63 \n0.9002 \n28.87 \n0.7889 \n27.77 \n0.7436 \n26.82 \n0.8087 \n31.22 \n0.9173 \nSAN [15] \n\u00d74 \nDIV2K \n32.64 \n0.9003 \n28.92 \n0.7888 \n27.78 \n0.7436 \n26.79 \n0.8068 \n31.18 \n0.9169 \nIGNN [100] \n\u00d74 \nDIV2K \n32.57 \n0.8998 \n28.85 \n0.7891 \n27.77 \n0.7434 \n26.84 \n0.8090 \n31.28 \n0.9182 \nHAN [63] \n\u00d74 \nDIV2K \n32.64 \n0.9002 \n28.90 \n0.7890 \n27.80 \n0.7442 \n26.85 \n0.8094 \n31.42 \n0.9177 \nNLSA [61] \n\u00d74 \nDIV2K \n32.59 \n0.9000 \n28.87 \n0.7891 \n27.78 \n0.7444 \n26.96 \n0.8109 \n31.27 \n0.9184 \nSwinIR (Ours) \n\u00d74 \nDIV2K \n32.72 \n0.9021 \n28.94 \n0.7914 \n27.83 \n0.7459 \n27.07 \n0.8164 \n31.67 \n0.9226 \nSwinIR+ (Ours) \n\u00d74 \nDIV2K \n32.81 \n0.9029 \n29.02 \n0.7928 \n27.87 \n0.7466 \n27.21 \n0.8187 \n31.88 \n0.9423 \nDBPN [31] \n\u00d74 \nDIV2K+Flickr2K \n32.47 \n0.8980 \n28.82 \n0.7860 \n27.72 \n0.7400 \n26.38 \n0.7946 \n30.91 \n0.9137 \nIPT [9] \n\u00d74 \nImageNet \n32.64 \n-\n29.01 \n-\n27.82 \n-\n27.26 \n-\n-\n-\nRRDB [81] \n\u00d74 \nDIV2K+Flickr2K \n32.73 \n0.9011 \n28.99 \n0.7917 \n27.85 \n0.7455 \n27.03 \n0.8153 \n31.66 \n0.9196 \nSwinIR (Ours) \n\u00d74 \nDIV2K+Flickr2K \n32.92 \n0.9044 \n29.09 \n0.7950 \n27.92 \n0.7489 \n27.45 \n0.8254 \n32.03 \n0.9260 \nSwinIR+ (Ours) \n\u00d74 \nDIV2K+Flickr2K \n32.93 \n0.9043 \n29.15 \n0.7958 \n27.95 \n0.7494 \n27.56 \n0.8273 \n32.22 \n0.9273 \n\nUrban100 (4\u00d7):img 012 \n\nHR \nVDSR [40] \nEDSR [51] \nRDN [97] \nOISR [33] \n\nSAN [15] \nRNAN [96] \nIGNN [100] \nIPT [9] \nSwinIR (ours) \n\n\n\nTable 3 :\n3Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for lightweight image SR on benchmark datasets. Best and second best performance are in red and blue colors, respectively.Method \nScale \n#Params \n#Mult-Adds \nSet5 [3] \nSet14 [87] \nBSD100 [58] \nUrban100 [34] \nManga109 [60] \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \n\nCARN [2] \n\u00d72 \n1,592K \n222.8G \n37.76 \n0.9590 \n33.52 \n0.9166 \n32.09 \n0.8978 \n31.92 \n0.9256 \n38.36 \n0.9765 \nFALSR-A [12] \n\u00d72 \n1,021K \n234.7G \n37.82 \n0.959 \n33.55 \n0.9168 \n32.1 \n0.8987 \n31.93 \n0.9256 \n-\n-\nIMDN [35] \n\u00d72 \n694K \n158.8G \n38.00 \n0.9605 \n33.63 \n0.9177 \n32.19 \n0.8996 \n32.17 \n0.9283 \n38.88 \n0.9774 \nLAPAR-A [44] \n\u00d72 \n548K \n171.0G \n38.01 \n0.9605 \n33.62 \n0.9183 \n32.19 \n0.8999 \n32.10 \n0.9283 \n38.67 \n0.9772 \nLatticeNet [57] \n\u00d72 \n756K \n169.5G \n38.15 \n0.9610 \n33.78 \n0.9193 \n32.25 \n0.9005 \n32.43 \n0.9302 \n-\n-\nSwinIR (Ours) \n\u00d72 \n878K \n195.6G \n38.14 \n0.9611 \n33.86 \n0.9206 \n32.31 \n0.9012 \n32.76 \n0.9340 \n39.12 \n0.9783 \nCARN [2] \n\u00d73 \n1,592K \n118.8G \n34.29 \n0.9255 \n30.29 \n0.8407 \n29.06 \n0.8034 \n28.06 \n0.8493 \n33.50 \n0.9440 \nIMDN [35] \n\u00d73 \n703K \n71.5G \n34.36 \n0.9270 \n30.32 \n0.8417 \n29.09 \n0.8046 \n28.17 \n0.8519 \n33.61 \n0.9445 \nLAPAR-A [44] \n\u00d73 \n544K \n114.0G \n34.36 \n0.9267 \n30.34 \n0.8421 \n29.11 \n0.8054 \n28.15 \n0.8523 \n33.51 \n0.9441 \nLatticeNet [57] \n\u00d73 \n765K \n76.3G \n34.53 \n0.9281 \n30.39 \n0.8424 \n29.15 \n0.8059 \n28.33 \n0.8538 \n-\n-\nSwinIR (Ours) \n\u00d73 \n886K \n87.2G \n34.62 \n0.9289 \n30.54 \n0.8463 \n29.20 \n0.8082 \n28.66 \n0.8624 \n33.98 \n0.9478 \n\nCARN [2] \n\u00d74 \n1,592K \n90.9G \n32.13 \n0.8937 \n28.60 \n0.7806 \n27.58 \n0.7349 \n26.07 \n0.7837 \n30.47 \n0.9084 \nIMDN [35] \n\u00d74 \n715K \n40.9G \n32.21 \n0.8948 \n28.58 \n0.7811 \n27.56 \n0.7353 \n26.04 \n0.7838 \n30.45 \n0.9075 \nLAPAR-A [44] \n\u00d74 \n659K \n94.0G \n32.15 \n0.8944 \n28.61 \n0.7818 \n27.61 \n0.7366 \n26.14 \n0.7871 \n30.42 \n0.9074 \nLatticeNet [57] \n\u00d74 \n777K \n43.6G \n32.30 \n0.8962 \n28.68 \n0.7830 \n27.62 \n0.7367 \n26.25 \n0.7873 \n-\n-\nSwinIR (Ours) \n\u00d74 \n897K \n49.6G \n32.44 \n0.8976 \n28.77 \n0.7858 \n27.69 \n0.7406 \n26.47 \n0.7980 \n30.92 \n0.9151 \n\n\nTable 4 :\n4Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for JPEG compression artifact reduction on benchmark datasets. Best and second best performance are in red and blue colors, respectively.Dataset \nq \nARCNN [17] \nDnCNN-3 [90] \nQGAC [20] \nRNAN [96] \nRDN [98] \nDRUNet [88] \nSwinIR (ours) \n\nClassic5 \n[22] \n\n10 \n29.03/0.7929/28.76 29.40/0.8026/29.13 29.84/0.8370/29.43 29.96/0.8178/29.62 30.00/0.8188/-\n30.16/0.8234/29.81 30.27/0.8249/29.95 \n20 \n31.15/0.8517/30.59 31.63/0.8610/31.19 31.98/0.8850/31.37 32.11/0.8693/31.57 32.15/0.8699/-\n32.39/0.8734/31.80 32.52/0.8748/31.99 \n30 \n32.51/0.8806/31.98 32.91/0.8861/32.38 33.22/0.9070/32.42 33.38/0.8924/32.68 33.43/0.8930/-\n33.59/0.8949/32.82 33.73/0.8961/33.03 \n40 \n33.32/0.8953/32.79 33.77/0.9003/33.20 \n-\n34.27/0.9061/33.4 \n34.27/0.9061/-\n34.41/0.9075/33.51 34.52/0.9082/33.66 \n\nLIVE1 \n[67] \n\n10 \n28.96/0.8076/28.77 29.19/0.8123/28.90 29.53/0.8400/29.15 29.63/0.8239/29.25 29.67/0.8247/-\n29.79/0.8278/29.48 29.86/0.8287/29.50 \n20 \n31.29/0.8733/30.79 31.59/0.8802/31.07 31.86/0.9010/31.27 32.03/0.8877/31.44 32.07/0.8882/-\n32.17/0.8899/31.69 32.25/0.8909/31.70 \n30 \n32.67/0.9043/32.22 32.98/0.9090/32.34 33.23/0.9250/32.50 33.45/0.9149/32.71 33.51/0.9153/-\n33.59/0.9166/32.99 33.69/0.9174/33.01 \n40 \n33.63/0.9198/33.14 33.96/0.9247/33.28 \n-\n34.47/0.9299/33.66 34.51/0.9302/-\n34.58/0.9312/33.93 34.67/0.9317/33.88 \n\nlarge model and train it on much larger datasets. Exper-\niments show that it can deal with more complex corrup-\ntions and achieves even better performance on real-world \nimages than the current model. Due to page limit, the details \nare given in our project page https://github.com/ \nJingyunLiang/SwinIR. \n\n\n\nTable 5 :\n5Quantitative comparison (average PSNR) with state-of-the-art methods for grayscale image denoising on benchmark datasets. Best and second best performance are in red and blue colors, respectively.Dataset \n\u03c3 \nBM3D \n[14] \n\nWNNM \n[29] \n\nDnCNN \n[90] \n\nIRCNN \n[91] \n\nFFDNet \n[92] \n\nN3Net \n[65] \n\nNLRN \n[52] \n\nFOCNet \n[38] \n\nRNAN \n[96] \n\nMWCNN \n[54] \n\nDRUNet \n[88] \nSwinIR (ours) \n\nSet12 \n[90] \n\n15 \n32.37 \n32.70 \n32.86 \n32.76 \n32.75 \n-\n33.16 \n33.07 \n-\n33.15 \n33.25 \n33.36 \n25 \n29.97 \n30.28 \n30.44 \n30.37 \n30.43 \n30.55 \n30.80 \n30.73 \n-\n30.79 \n30.94 \n31.01 \n50 \n26.72 \n27.05 \n27.18 \n27.12 \n27.32 \n27.43 \n27.64 \n27.68 \n27.70 \n27.74 \n27.90 \n27.91 \n\nBSD68 \n[59] \n\n15 \n31.08 \n31.37 \n31.73 \n31.63 \n31.63 \n-\n31.88 \n31.83 \n-\n31.86 \n31.91 \n31.97 \n25 \n28.57 \n28.83 \n29.23 \n29.15 \n29.19 \n29.30 \n29.41 \n29.38 \n-\n29.41 \n29.48 \n29.50 \n50 \n25.60 \n25.87 \n26.23 \n26.19 \n26.29 \n26.39 \n26.47 \n26.50 \n26.48 \n26.53 \n26.59 \n26.58 \n\nUrban100 \n[34] \n\n15 \n32.35 \n32.97 \n32.64 \n32.46 \n32.40 \n-\n33.45 \n33.15 \n-\n33.17 \n33.44 \n33.70 \n25 \n29.70 \n30.39 \n29.95 \n29.80 \n29.90 \n30.19 \n30.94 \n30.64 \n-\n30.66 \n31.11 \n31.30 \n50 \n25.95 \n26.83 \n26.26 \n26.22 \n26.50 \n26.82 \n27.49 \n27.40 \n27.65 \n27.42 \n27.96 \n27.98 \n\n\n\nTable 6 :\n6Quantitative comparison (average PSNR) with state-of-the-art methods for color image denoising on benchmark datasets. Best and second best performance are in red and blue colors, respectively.Dataset \n\u03c3 \nBM3D \n[14] \n\nDnCNN \n[90] \n\nIRCNN \n[91] \n\nFFDNet \n[92] \n\nDSNet \n[64] \n\nRPCNN \n[85] \n\nBRDNet \n[71] \n\nRNAN \n[96] \n\nRDN \n[98] \n\nIPT \n[9] \n\nDRUNet \n[88] \nSwinIR (ours) \n\nCBSD68 \n[59] \n\n15 \n33.52 \n33.90 \n33.86 \n33.87 \n33.91 \n-\n34.10 \n-\n-\n-\n34.30 \n34.42 \n25 \n30.71 \n31.24 \n31.16 \n31.21 \n31.28 \n31.24 \n31.43 \n-\n-\n-\n31.69 \n31.78 \n50 \n27.38 \n27.95 \n27.86 \n27.96 \n28.05 \n28.06 \n28.16 \n28.27 \n28.31 \n28.39 \n28.51 \n28.56 \n\nKodak24 \n[23] \n\n15 \n34.28 \n34.60 \n34.69 \n34.63 \n34.63 \n-\n34.88 \n-\n-\n-\n35.31 \n35.34 \n25 \n32.15 \n32.14 \n32.18 \n32.13 \n32.16 \n32.34 \n32.41 \n-\n-\n-\n32.89 \n32.89 \n50 \n28.46 \n28.95 \n28.93 \n28.98 \n29.05 \n29.25 \n29.22 \n29.58 \n29.66 \n29.64 \n29.86 \n29.79 \n\nMcMaster \n[94] \n\n15 \n34.06 \n33.45 \n34.58 \n34.66 \n34.67 \n-\n35.08 \n-\n-\n-\n35.40 \n35.61 \n25 \n31.66 \n31.52 \n32.18 \n32.35 \n32.40 \n32.33 \n32.75 \n-\n-\n-\n33.14 \n33.20 \n50 \n28.51 \n28.62 \n28.91 \n29.18 \n29.28 \n29.33 \n29.52 \n29.72 \n-\n29.98 \n30.08 \n30.22 \n\nUrban100 \n[34] \n\n15 \n33.93 \n32.98 \n33.78 \n33.83 \n-\n-\n34.42 \n-\n-\n-\n34.81 \n35.13 \n25 \n31.36 \n30.81 \n31.20 \n31.40 \n-\n31.81 \n31.99 \n-\n-\n-\n32.60 \n32.90 \n50 \n27.93 \n27.59 \n27.70 \n28.05 \n-\n28.62 \n28.56 \n29.08 \n29.38 \n29.71 \n29.61 \n29.82 \n\n\nhttps://github.com/cszn/BSRGAN\nAcknowledgements This paper was partially supported by the ETH Zurich Fund (OK), a Huawei Technologies Oy (Finland) project, the China Scholarship Council and an Amazon AWS grant. Special thanks goes to Yijue Chen.\nNtire 2017 challenge on single image super-resolution: Dataset and study. Eirikur Agustsson, Radu Timofte, IEEE Conference on Computer Vision and Pattern Recognition Workshops. Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In IEEE Conference on Computer Vision and Pattern Recog- nition Workshops, pages 126-135, 2017. 4\n\nFast, accurate, and lightweight super-resolution with cascading residual network. Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn, European Conference on Computer Vision. 56Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast, accurate, and lightweight super-resolution with cas- cading residual network. In European Conference on Com- puter Vision, pages 252-268, 2018. 5, 6\n\nChristine Guillemot, and Marie line Alberi Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. Marco Bevilacqua, Aline Roumy, British Machine Vision Conference. 106Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie line Alberi Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embed- ding. In British Machine Vision Conference, pages 135.1- 135.10, 2012. 1, 6\n\nSwin-unet: Unet-like pure transformer for medical image segmentation. Yueyue Hu Cao, Joy Wang, Dongsheng Chen, Xiaopeng Jiang, Qi Zhang, Manning Tian, Wang, arXiv:2105.05537arXiv preprintHu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi- aopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmenta- tion. arXiv preprint arXiv:2105.05537, 2021. 2\n\nJiezhang Cao, Yawei Li, Kai Zhang, Luc Van Gool, arXiv:2106.06847Video super-resolution transformer. 1arXiv preprintJiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. Video super-resolution transformer. arXiv preprint arXiv:2106.06847, 2021. 1, 2\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. Springer1Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico- las Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213-229. Springer, 2020. 1, 2\n\nCas-cnn: A deep convolutional neural network for image compression artifact suppression. Lukas Cavigelli, Pascal Hager, Luca Benini, 2017 International Joint Conference on Neural Networks. Lukas Cavigelli, Pascal Hager, and Luca Benini. Cas-cnn: A deep convolutional neural network for image compres- sion artifact suppression. In 2017 International Joint Con- ference on Neural Networks, pages 752-759, 2017. 2\n\nTwo deterministic half-quadratic regularization algorithms for computed imaging. Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, Michel Barlaud, International Conference on Image Processing. IEEE2Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and Michel Barlaud. Two deterministic half-quadratic reg- ularization algorithms for computed imaging. In Interna- tional Conference on Image Processing, volume 2, pages 168-172. IEEE, 1994. 3\n\nPre-trained image processing transformer. Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao, IEEE Conference on Computer Vision and Pattern Recognition. 6Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip- ing Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In IEEE Conference on Computer Vision and Pattern Recog- nition, pages 12299-12310, 2021. 1, 2, 5, 6, 8\n\nTrainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. Yunjin Chen, Thomas Pock, IEEE transactions on pattern analysis and machine intelligence. 39Yunjin Chen and Thomas Pock. Trainable nonlinear reac- tion diffusion: A flexible framework for fast and effective image restoration. IEEE transactions on pattern analysis and machine intelligence, 39(6):1256-1272, 2016. 2\n\nMfagan: A compression framework for memoryefficient on-device super-resolution gan. Wenlong Cheng, Mingbo Zhao, Zhiling Ye, Shuhang Gu, arXiv:2107.12679arXiv preprintWenlong Cheng, Mingbo Zhao, Zhiling Ye, and Shuhang Gu. Mfagan: A compression framework for memory- efficient on-device super-resolution gan. arXiv preprint arXiv:2107.12679, 2021. 2\n\nFast, accurate and lightweight superresolution with neural architecture search. Xiangxiang Chu, Bo Zhang, Hailong Ma, Ruijun Xu, Qingyuan Li, International Conference on Pattern Recognition. IEEE56Xiangxiang Chu, Bo Zhang, Hailong Ma, Ruijun Xu, and Qingyuan Li. Fast, accurate and lightweight super- resolution with neural architecture search. In International Conference on Pattern Recognition, pages 59-64. IEEE, 2020. 5, 6\n\nOn the relationship between self-attention and convolutional layers. Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi, arXiv:1911.03584arXiv preprintJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and con- volutional layers. arXiv preprint arXiv:1911.03584, 2019. 2\n\nImage denoising by sparse 3-d transform-domain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE Transactions on image processing. 168Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transac- tions on image processing, 16(8):2080-2095, 2007. 1, 7, 8\n\nSecond-order attention network for single image super-resolution. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang, IEEE Conference on Computer Vision and Pattern Recognition. 56Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single im- age super-resolution. In IEEE Conference on Computer Vi- sion and Pattern Recognition, pages 11065-11074, 2019. 2, 5, 6\n\nDeep coupled feedback network for joint exposure fusion and image super-resolution. Xin Deng, Yutong Zhang, Mai Xu, Shuhang Gu, Yiping Duan, IEEE Transactions on Image Processing. 302Xin Deng, Yutong Zhang, Mai Xu, Shuhang Gu, and Yiping Duan. Deep coupled feedback network for joint exposure fusion and image super-resolution. IEEE Transactions on Image Processing, 30:3098-3112, 2021. 2\n\nCompression artifacts reduction by a deep convolutional network. Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, IEEE International Conference on Computer Vision. 27Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by a deep convolu- tional network. In IEEE International Conference on Com- puter Vision, pages 576-584, 2015. 2, 7\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, European Conference on Computer Vision. 1Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In European Conference on Computer Vi- sion, pages 184-199, 2014. 1, 2\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119291arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 2\n\nQuantization guided jpeg artifact correction. Max Ehrlich, Larry Davis, Ser-Nam Lim, Abhinav Shrivastava, European Conference on Computer Vision. Max Ehrlich, Larry Davis, Ser-Nam Lim, and Abhinav Shrivastava. Quantization guided jpeg artifact correction. In European Conference on Computer Vision, pages 293- 309, 2020. 7\n\nRevisiting spatial invariance with low-rank local connectivity. Gamaleldin Elsayed, Prajit Ramachandran, Jonathon Shlens, Simon Kornblith, International Conference on Machine Learning. 23Gamaleldin Elsayed, Prajit Ramachandran, Jonathon Shlens, and Simon Kornblith. Revisiting spatial invariance with low-rank local connectivity. In International Confer- ence on Machine Learning, pages 2868-2879, 2020. 2, 3\n\nPointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images. Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE Transactions on Image Processing. 165Alessandro Foi, Vladimir Katkovnik, and Karen Egiazar- ian. Pointwise shape-adaptive dct for high-quality de- noising and deblocking of grayscale and color images. IEEE Transactions on Image Processing, 16(5):1395-1411, 2007. 7\n\nKodak lossless true color image suite. Rich Franzen, 4Rich Franzen. Kodak lossless true color image suite. source: http://r0k. us/graphics/kodak, 4(2), 1999. 8\n\nFrequency separation for real-world super-resolution. Manuel Fritsche, Shuhang Gu, Radu Timofte, IEEE Conference on International Conference on Computer Vision Workshops. Manuel Fritsche, Shuhang Gu, and Radu Timofte. Fre- quency separation for real-world super-resolution. In IEEE Conference on International Conference on Computer Vi- sion Workshops, pages 3599-3608, 2019. 1\n\nA model-driven deep unfolding method for jpeg artifacts removal. Xueyang Fu, Menglu Wang, Xiangyong Cao, Xinghao Ding, Zheng-Jun Zha, IEEE Transactions on Neural Networks and Learning Systems. 2Xueyang Fu, Menglu Wang, Xiangyong Cao, Xinghao Ding, and Zheng-Jun Zha. A model-driven deep unfold- ing method for jpeg artifacts removal. IEEE Transactions on Neural Networks and Learning Systems, 2021. 2\n\nJpeg artifacts reduction via deep convolutional sparse coding. Xueyang Fu, Zheng-Jun Zha, Feng Wu, Xinghao Ding, John Paisley, IEEE International Conference on Computer Vision. Xueyang Fu, Zheng-Jun Zha, Feng Wu, Xinghao Ding, and John Paisley. Jpeg artifacts reduction via deep convolu- tional sparse coding. In IEEE International Conference on Computer Vision, pages 2501-2510, 2019. 2\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Ad- vances in Neural Information Processing Systems, pages 2672-2680, 2014. 3\n\nFast image super resolution via local regression. Shuhang Gu, Nong Sang, Fan Ma, IEEE Conference on International Conference on Pattern Recognition. 1Shuhang Gu, Nong Sang, and Fan Ma. Fast image su- per resolution via local regression. In IEEE Conference on International Conference on Pattern Recognition, pages 3128-3131, 2012. 1, 2\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, IEEE conference on computer vision and pattern recognition. 7Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with applica- tion to image denoising. In IEEE conference on computer vision and pattern recognition, pages 2862-2869, 2014. 7, 8\n\nClosedloop matters: Dual regression networks for single image super-resolution. Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, Yanwu Xu, Mingkui Tan, IEEE Conference on Computer Vision and Pattern Recognition. Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, Yanwu Xu, and Mingkui Tan. Closed- loop matters: Dual regression networks for single image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5407-5416, 2020. 2\n\nDeep back-projection networks for superresolution. Muhammad Haris, Gregory Shakhnarovich, Norimichi Ukita, IEEE Conference on Computer Vision and Pattern Recognition. 56Muhammad Haris, Gregory Shakhnarovich, and Norim- ichi Ukita. Deep back-projection networks for super- resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1664-1673, 2018. 5, 6\n\nSingle image haze removal using dark channel prior. Kaiming He, Jian Sun, Xiaoou Tang, IEEE transactions on Pattern Analysis and Machine Intelligence. 3312Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. IEEE transactions on Pattern Analysis and Machine Intelligence, 33(12):2341- 2353, 2010. 2\n\nOde-inspired network design for single image super-resolution. Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu, Mingyuan Yang, Jian Cheng, IEEE Conference on Computer Vision and Pattern Recognition. Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu, Mingyuan Yang, and Jian Cheng. Ode-inspired network design for single image super-resolution. In IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 1732-1741, 2019. 6\n\nSingle image super-resolution from transformed selfexemplars. Jia-Bin Huang, Abhishek Singh, Narendra Ahuja, IEEE Conference on Computer Vision and Pattern Recognition. 6Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self- exemplars. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5197-5206, 2015. 6, 8\n\nLightweight image super-resolution with information multi-distillation network. Zheng Hui, Xinbo Gao, Yunchu Yang, Xiumei Wang, ACM International Conference on Multimedia. 56Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang. Lightweight image super-resolution with informa- tion multi-distillation network. In ACM International Con- ference on Multimedia, pages 2024-2032, 2019. 5, 6\n\nVideo super-resolution with recurrent structure-detail network. Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, Qi Tian, European Conference on Computer Vision. SpringerTakashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video super-resolution with recurrent structure-detail network. In European Conference on Com- puter Vision, pages 645-660. Springer, 2020. 2\n\nReal-world super-resolution via kernel estimation and noise injection. Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, IEEE Conference on Computer Vision and Pattern Recognition Workshops. 57Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution via ker- nel estimation and noise injection. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 466-467, 2020. 5, 7\n\nFocnet: A fractional optimal control network for image denoising. Xixi Jia, Sanyang Liu, Xiangchu Feng, Lei Zhang, IEEE Conference on Computer Vision and Pattern Recognition. 7Xixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Foc- net: A fractional optimal control network for image denois- ing. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6054-6063, 2019. 2, 7, 8\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, European Conference on Computer Vision. SpringerJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694-711. Springer, 2016. 3\n\nAccurate image super-resolution using very deep convolutional networks. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, IEEE Conference on Computer Vision and Pattern Recognition. 6Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accu- rate image super-resolution using very deep convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1646-1654, 2016. 1, 2, 6\n\nA pseudo-blind convolutional neural network for the reduction of compression artifacts. Yoonsik Kim, Jae Woong Soh, Jaewoo Park, Byeongyong Ahn, Hyun-Seung Lee, Young-Su Moon, Nam Ik Cho, IEEE Transactions on Circuits and Systems for Video Technology. 304Yoonsik Kim, Jae Woong Soh, Jaewoo Park, Byeongyong Ahn, Hyun-Seung Lee, Young-Su Moon, and Nam Ik Cho. A pseudo-blind convolutional neural network for the reduc- tion of compression artifacts. IEEE Transactions on Cir- cuits and Systems for Video Technology, 30(4):1121-1135, 2019. 2\n\nDeep laplacian pyramid networks for fast and accurate super-resolution. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, IEEE Conference on Computer Vision and Pattern Recognition. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming- Hsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 624-632, 2017. 2\n\nPhoto-realistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, IEEE Conference on Computer Vision and Pattern Recognition. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Ca- ballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a gener- ative adversarial network. In IEEE Conference on Com- puter Vision and Pattern Recognition, pages 4681-4690, 2017. 1\n\nLapar: Linearly-assembled pixel-adaptive regression network for single image super-resolution and beyond. Wenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu, Jiaya Jia, arXiv:2105.1042256arXiv preprintWenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu, and Jiaya Jia. Lapar: Linearly-assembled pixel-adaptive regression network for single image super-resolution and beyond. arXiv preprint arXiv:2105.10422, 2021. 5, 6\n\nLocalvit: Bringing locality to vision transformers. Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool, arXiv:2104.05707arXiv preprintYawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision transform- ers. arXiv preprint arXiv:2104.05707, 2021. 2\n\nFeedback network for image superresolution. Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, Wei Wu, IEEE Conference on Computer Vision and Pattern Recognition. Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwang- gil Jeon, and Wei Wu. Feedback network for image super- resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3867-3876, 2019. 1\n\nTranscrowd: Weakly-supervised crowd counting with transformer. Dingkang Liang, Xiwu Chen, Wei Xu, Yu Zhou, Xiang Bai, arXiv:2104.09116arXiv preprintDingkang Liang, Xiwu Chen, Wei Xu, Yu Zhou, and Xiang Bai. Transcrowd: Weakly-supervised crowd counting with transformer. arXiv preprint arXiv:2104.09116, 2021. 2\n\nHierarchical conditional flow: A unified framework for image superresolution and image rescaling. Jingyun Liang, Andreas Lugmayr, Kai Zhang, Martin Danelljan, Luc Van Gool, Radu Timofte, IEEE Conference on International Conference on Computer Vision. Jingyun Liang, Andreas Lugmayr, Kai Zhang, Martin Danelljan, Luc Van Gool, and Radu Timofte. Hierarchi- cal conditional flow: A unified framework for image super- resolution and image rescaling. In IEEE Conference on In- ternational Conference on Computer Vision, 2021. 2\n\nMutual affine network for spatially variant kernel estimation in blind image super-resolution. Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte, IEEE Conference on International Conference on Computer Vision. Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Mutual affine network for spatially variant kernel estimation in blind image super-resolution. In IEEE Conference on International Conference on Computer Vi- sion, 2021. 2\n\nFlow-based kernel prior with application to blind super-resolution. Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, Radu Timofte, IEEE Conference on Computer Vision and Pattern Recognition. Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, and Radu Timofte. Flow-based kernel prior with application to blind super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 10601-10610, 2021. 2\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, IEEE Conference on Computer Vision and Pattern Recognition Workshops. 6Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for sin- gle image super-resolution. In IEEE Conference on Com- puter Vision and Pattern Recognition Workshops, pages 136-144, 2017. 1, 4, 6\n\nNon-local recurrent network for image restoration. Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S Huang, arXiv:1806.02919arXiv preprintDing Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network for image restoration. arXiv preprint arXiv:1806.02919, 2018. 2, 7, 8\n\nDeep learning for generic object detection: A survey. International Journal of Computer Vision. Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietik\u00e4inen, 128Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietik\u00e4inen. Deep learning for generic object detection: A survey. International Jour- nal of Computer Vision, 128(2):261-318, 2020. 2\n\nMulti-level wavelet-cnn for image restoration. Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, Wangmeng Zuo, IEEE conference on computer vision and pattern recognition workshops. 7Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level wavelet-cnn for image restora- tion. In IEEE conference on computer vision and pattern recognition workshops, pages 773-782, 2018. 7, 8\n\nYun Liu, Guolei Sun, Yu Qiu, Le Zhang, Ajad Chhatkuli, Luc Van Gool, arXiv:2106.03180Transformer in convolutional neural networks. arXiv preprintYun Liu, Guolei Sun, Yu Qiu, Le Zhang, Ajad Chhatkuli, and Luc Van Gool. Transformer in convolutional neural networks. arXiv preprint arXiv:2106.03180, 2021. 2\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.14030arXiv preprintZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows. arXiv preprint arXiv:2103.14030, 2021. 1, 2, 4\n\nLatticenet: Towards lightweight image super-resolution with lattice block. Xiaotong Luo, Yuan Xie, Yulun Zhang, Yanyun Qu, Cuihua Li, Yun Fu, European Conference on Computer Vision. 56Xiaotong Luo, Yuan Xie, Yulun Zhang, Yanyun Qu, Cui- hua Li, and Yun Fu. Latticenet: Towards lightweight image super-resolution with lattice block. In European Confer- ence on Computer Vision, pages 272-289, 2020. 5, 6\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, IEEE Conference on International Conference on Computer Vision. David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In IEEE Conference on International Conference on Computer Vision, pages 416- 423, 2001. 6\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, IEEE International Conference on Computer Vision. David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In IEEE International Con- ference on Computer Vision, pages 416-423, 2001. 8\n\nSketch-based manga retrieval using manga109 dataset. Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, Kiyoharu Aizawa, Multimedia Tools and Applications. 76206Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fuji- moto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 76(20):21811- 21838, 2017. 4, 5, 6\n\nImage superresolution with non-local sparse attention. Yiqun Mei, Yuchen Fan, Yuqian Zhou, IEEE Conference on Computer Vision and Pattern Recognition. 6Yiqun Mei, Yuchen Fan, and Yuqian Zhou. Image super- resolution with non-local sparse attention. In IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 3517-3526, 2021. 2, 5, 6\n\nNonparametric blind super-resolution. Tomer Michaeli, Michal Irani, IEEE Conference on International Conference on Computer Vision. Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In IEEE Conference on International Conference on Computer Vision, pages 945-952, 2013. 2\n\nSingle image super-resolution via a holistic attention network. Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao, Haifeng Shen, European Conference on Computer Vision. 56Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lian- ping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao, and Haifeng Shen. Single image super-resolution via a holistic attention network. In European Conference on Computer Vision, pages 191-207, 2020. 2, 4, 5, 6\n\nDilated residual networks with symmetric skip connection for image denoising. Yali Peng, Lu Zhang, Shigang Liu, Xiaojun Wu, Yu Zhang, Xili Wang, Neurocomputing. 3458Yali Peng, Lu Zhang, Shigang Liu, Xiaojun Wu, Yu Zhang, and Xili Wang. Dilated residual networks with symmet- ric skip connection for image denoising. Neurocomputing, 345:67-76, 2019. 2, 8\n\nTobias Pl\u00f6tz, Stefan Roth, arXiv:1810.12575Neural nearest neighbors networks. 7arXiv preprintTobias Pl\u00f6tz and Stefan Roth. Neural nearest neighbors net- works. arXiv preprint arXiv:1810.12575, 2018. 7, 8\n\nStandalone self-attention in vision models. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens, arXiv:1906.05909arXiv preprintPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens. Stand- alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019. 2\n\nLive image quality assessment database release 2. Hr, Sheikh, HR Sheikh. Live image quality assessment database release 2. http://live. ece. utexas. edu/research/quality, 2005. 7\n\nReal-time single image and video superresolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, IEEE Conference on Computer Vision and Pattern Recognition. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super- resolution using an efficient sub-pixel convolutional neural network. In IEEE Conference on Computer Vision and Pat- tern Recognition, pages 1874-1883, 2016. 3\n\nBoosting crowd counting with transformers. Guolei Sun, Yun Liu, Thomas Probst, Nikola Danda Pani Paudel, Luc Popovic, Van Gool, arXiv:2105.10926arXiv preprintGuolei Sun, Yun Liu, Thomas Probst, Danda Pani Paudel, Nikola Popovic, and Luc Van Gool. Boosting crowd count- ing with transformers. arXiv preprint arXiv:2105.10926, 2021. 2\n\nMemnet: A persistent memory network for image restoration. Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu, IEEE International Conference on Computer Vision. Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Memnet: A persistent memory network for image restora- tion. In IEEE International Conference on Computer Vi- sion, pages 4539-4547, 2017. 2\n\nImage denoising using deep cnn with batch renormalization. Chunwei Tian, Yong Xu, Wangmeng Zuo, Neural Networks. 1218Chunwei Tian, Yong Xu, and Wangmeng Zuo. Image de- noising using deep cnn with batch renormalization. Neural Networks, 121:461-473, 2020. 8\n\nAnchored neighborhood regression for fast example-based super-resolution. Radu Timofte, Vincent De Smet, Luc Van Gool, IEEE Conference on International Conference on Computer Vision. Radu Timofte, Vincent De Smet, and Luc Van Gool. An- chored neighborhood regression for fast example-based super-resolution. In IEEE Conference on International Conference on Computer Vision, pages 1920-1927, 2013. 2\n\nA+: Adjusted anchored neighborhood regression for fast superresolution. Radu Timofte, Vincent De Smet, Luc Van Gool, Asian Conference on Computer Vision. 1Radu Timofte, Vincent De Smet, and Luc Van Gool. A+: Adjusted anchored neighborhood regression for fast super- resolution. In Asian Conference on Computer Vision, pages 111-126, 2014. 1, 2\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, arXiv:2012.12877Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. 1arXiv preprintHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Train- ing data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020. 1, 2\n\nScaling local self-attention for parameter efficient visual backbones. Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens, arXiv:2103.1273123arXiv preprintAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scal- ing local self-attention for parameter efficient visual back- bones. arXiv preprint arXiv:2103.12731, 2021. 2, 3\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762Attention is all you need. arXiv preprintAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 1, 2, 4\n\nUnsupervised degradation representation learning for blind superresolution. Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, Yulan Guo, IEEE Conference on Computer Vision and Pattern Recognition. Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, and Yulan Guo. Unsuper- vised degradation representation learning for blind super- resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 10581-10590, 2021. 2\n\nLearning parallax attention for stereo image super-resolution. Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo, IEEE Conference on Computer Vision and Pattern Recognition. Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, and Yulan Guo. Learning par- allax attention for stereo image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 12250-12259, 2019. 2\n\nLearning a single network for scale-arbitrary super-resolution. Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo, IEEE Conference on International Conference on Computer Vision. Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang Yang, Wei An, and Yulan Guo. Learning a single net- work for scale-arbitrary super-resolution. In IEEE Con- ference on International Conference on Computer Vision, pages 10581-10590, 2021. 2\n\nReal-esrgan: Training real-world blind super-resolution with pure synthetic data. Xintao Wang, Liangbin Xie, Chao Dong, Ying Shan, arXiv:2107.1083337arXiv preprintXintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. arXiv preprint arXiv:2107.10833, 2021. 3, 5, 7\n\nEsrgan: Enhanced super-resolution generative adversarial networks. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy, European Conference on Computer Vision Workshops. 67Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In European Conference on Computer Vision Workshops, pages 701-710, 2018. 1, 2, 3, 5, 6, 7\n\nUformer: A general u-shaped transformer for image restoration. Zhendong Wang, Xiaodong Cun, Jianmin Bao, Jianzhuang Liu, arXiv:2106.03106arXiv preprintZhendong Wang, Xiaodong Cun, Jianmin Bao, and Jianzhuang Liu. Uformer: A general u-shaped transformer for image restoration. arXiv preprint arXiv:2106.03106, 2021. 2\n\nUnsupervised real-world image super resolution via domain-distance aware training. Yunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Longcun Jin, Hengjie Song, IEEE Conference on Computer Vision and Pattern Recognition. Yunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Long- cun Jin, and Hengjie Song. Unsupervised real-world im- age super resolution via domain-distance aware training. In IEEE Conference on Computer Vision and Pattern Recog- nition, pages 13385-13394, 2021. 2\n\nVisual transformers: Token-based image representation and processing for computer vision. Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda, arXiv:2006.03677arXiv preprintBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Vi- sual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.03677, 2020. 2\n\nIdentifying recurring patterns with deep neural networks for natural image denoising. Zhihao Xia, Ayan Chakrabarti, IEEE Winter Conference on Applications of Computer Vision. Zhihao Xia and Ayan Chakrabarti. Identifying recurring patterns with deep neural networks for natural image de- noising. In IEEE Winter Conference on Applications of Computer Vision, pages 2426-2434, 2020. 8\n\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, arXiv:2106.14881Piotr Doll\u00e1r, and Ross Girshick. Early convolutions help transformers see better. arXiv preprintTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Pi- otr Doll\u00e1r, and Ross Girshick. Early convolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021. 2\n\nOn single image scale-up using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, International Conference on Curves and Surfaces. Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Interna- tional Conference on Curves and Surfaces, pages 711-730, 2010. 6\n\nPlug-and-play image restoration with deep denoiser prior. Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, Radu Timofte, IEEE Transactions on Pattern Analysis and Machine Intelligence. Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play image restora- tion with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 1, 2, 7, 8\n\nDesigning a practical degradation model for deep blind image super-resolution. Kai Zhang, Jingyun Liang, Luc Van Gool, Radu Timofte, IEEE Conference on International Conference on Computer Vision. 57Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo- fte. Designing a practical degradation model for deep blind image super-resolution. In IEEE Conference on Interna- tional Conference on Computer Vision, 2021. 1, 3, 5, 7\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE transactions on image processing. 267Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142-3155, 2017. 1, 2, 7, 8\n\nLearning deep cnn denoiser prior for image restoration. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang, IEEE Conference on Computer Vision and Pattern Recognition. Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In IEEE Conference on Computer Vision and Pattern Recog- nition, pages 3929-3938, 2017. 1, 7, 8\n\nFfdnet: Toward a fast and flexible solution for cnn-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE Transactions on Image Processing. 279Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based im- age denoising. IEEE Transactions on Image Processing, 27(9):4608-4622, 2018. 1, 2, 7, 8\n\nLearning a single convolutional super-resolution network for multiple degradations. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE Conference on Computer Vision and Pattern Recognition. 1Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a single convolutional super-resolution network for multiple degradations. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3262-3271, 2018. 1, 2\n\nColor demosaicking by local directional interpolation and nonlocal adaptive thresholding. Lei Zhang, Xiaolin Wu, Antoni Buades, Xin Li, Journal of Electronic imaging. 20223016Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color demosaicking by local directional interpolation and nonlo- cal adaptive thresholding. Journal of Electronic imaging, 20(2):023016, 2011. 8\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, European Conference on Computer Vision. 56Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In European Confer- ence on Computer Vision, pages 286-301, 2018. 1, 2, 4, 5, 6\n\nResidual non-local attention networks for image restoration. Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, Yun Fu, arXiv:1903.100827arXiv preprintYulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local attention networks for image restoration. arXiv preprint arXiv:1903.10082, 2019. 2, 6, 7, 8\n\nResidual dense network for image superresolution. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, IEEE Conference on Computer Vision and Pattern Recognition. 6Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super- resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2472-2481, 2018. 1, 2, 6\n\nResidual dense network for image restoration. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 437Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image restoration. IEEE Transactions on Pattern Analysis and Machine Intel- ligence, 43(7):2480-2495, 2020. 2, 7, 8\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, IEEE Conference on Computer Vision and Pattern Recognition. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmen- tation from a sequence-to-sequence perspective with trans- formers. In IEEE Conference on Computer Vision and Pat- tern Recognition, pages 6881-6890, 2021. 2\n\nCross-scale internal graph neural network for image super-resolution. Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Chen Change Loy, arXiv:2006.166736arXiv preprintShangchen Zhou, Jiawei Zhang, Wangmeng Zuo, and Chen Change Loy. Cross-scale internal graph neu- ral network for image super-resolution. arXiv preprint arXiv:2006.16673, 2020. 2, 5, 6\n", "annotations": {"author": "[{\"end\":137,\"start\":52},{\"end\":195,\"start\":138},{\"end\":276,\"start\":196},{\"end\":315,\"start\":277},{\"end\":414,\"start\":316},{\"end\":499,\"start\":415},{\"end\":564,\"start\":500},{\"end\":572,\"start\":565}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":60},{\"end\":150,\"start\":147},{\"end\":206,\"start\":203},{\"end\":286,\"start\":281},{\"end\":328,\"start\":320},{\"end\":427,\"start\":420},{\"end\":519,\"start\":515},{\"end\":571,\"start\":565}]", "author_first_name": "[{\"end\":59,\"start\":52},{\"end\":146,\"start\":138},{\"end\":202,\"start\":196},{\"end\":280,\"start\":277},{\"end\":319,\"start\":316},{\"end\":419,\"start\":415},{\"end\":512,\"start\":500},{\"end\":514,\"start\":513}]", "author_affiliation": "[{\"end\":136,\"start\":94},{\"end\":194,\"start\":152},{\"end\":275,\"start\":233},{\"end\":398,\"start\":356},{\"end\":413,\"start\":400},{\"end\":498,\"start\":456},{\"end\":563,\"start\":521}]", "title": "[{\"end\":49,\"start\":1},{\"end\":621,\"start\":573}]", "venue": null, "abstract": "[{\"end\":1877,\"start\":623}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2135,\"start\":2131},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2138,\"start\":2135},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":2141,\"start\":2138},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":2144,\"start\":2141},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2242,\"start\":2238},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2245,\"start\":2242},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2248,\"start\":2245},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":2251,\"start\":2248},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":2254,\"start\":2251},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":2257,\"start\":2254},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2260,\"start\":2257},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":2263,\"start\":2260},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2266,\"start\":2263},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":2269,\"start\":2266},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":2272,\"start\":2269},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2368,\"start\":2364},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2371,\"start\":2368},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":2398,\"start\":2394},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":2401,\"start\":2398},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":2528,\"start\":2524},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2531,\"start\":2528},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2534,\"start\":2531},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":2969,\"start\":2965},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3119,\"start\":3116},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":3122,\"start\":3119},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3128,\"start\":3125},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3184,\"start\":3181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3186,\"start\":3184},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3675,\"start\":3671},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5028,\"start\":5025},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5163,\"start\":5159},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5166,\"start\":5163},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":5169,\"start\":5166},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5511,\"start\":5507},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":5514,\"start\":5511},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":5517,\"start\":5514},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5520,\"start\":5517},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5523,\"start\":5520},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5807,\"start\":5803},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":5834,\"start\":5830},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5871,\"start\":5867},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6091,\"start\":6087},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6093,\"start\":6091},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":6096,\"start\":6093},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":6114,\"start\":6110},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":6117,\"start\":6114},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":6120,\"start\":6117},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6136,\"start\":6132},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6139,\"start\":6136},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":6142,\"start\":6139},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":6145,\"start\":6142},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":6148,\"start\":6145},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":6151,\"start\":6148},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6154,\"start\":6151},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6157,\"start\":6154},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6160,\"start\":6157},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":6163,\"start\":6160},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":6166,\"start\":6163},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6169,\"start\":6166},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":6172,\"start\":6169},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6175,\"start\":6172},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6178,\"start\":6175},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6181,\"start\":6178},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":6184,\"start\":6181},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6187,\"start\":6184},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6190,\"start\":6187},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6193,\"start\":6190},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6196,\"start\":6193},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6199,\"start\":6196},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":6309,\"start\":6305},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6312,\"start\":6309},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6315,\"start\":6312},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6341,\"start\":6337},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6344,\"start\":6341},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":6381,\"start\":6376},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":6465,\"start\":6461},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6589,\"start\":6585},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6592,\"start\":6589},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":6595,\"start\":6592},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6598,\"start\":6595},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6601,\"start\":6598},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6604,\"start\":6601},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":6607,\"start\":6604},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6630,\"start\":6627},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6633,\"start\":6630},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":6636,\"start\":6633},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6639,\"start\":6636},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":6658,\"start\":6654},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":6661,\"start\":6658},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6664,\"start\":6661},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6666,\"start\":6664},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6690,\"start\":6686},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":6693,\"start\":6690},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6903,\"start\":6900},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6905,\"start\":6903},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":6908,\"start\":6905},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6925,\"start\":6922},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7200,\"start\":7197},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":7497,\"start\":7493},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7565,\"start\":7561},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8329,\"start\":8325},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9839,\"start\":9835},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":10739,\"start\":10735},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":10742,\"start\":10739},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":10745,\"start\":10742},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10748,\"start\":10745},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10751,\"start\":10748},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":10754,\"start\":10751},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11757,\"start\":11753},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":11760,\"start\":11757},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":12106,\"start\":12102},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":12195,\"start\":12191},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":13110,\"start\":13106},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13799,\"start\":13795},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":14499,\"start\":14495},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14502,\"start\":14499},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14536,\"start\":14532},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14776,\"start\":14773},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14833,\"start\":14829},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17221,\"start\":17219},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17567,\"start\":17563},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":17578,\"start\":17574},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":17589,\"start\":17585},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17599,\"start\":17595},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":17611,\"start\":17606},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17621,\"start\":17617},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":17632,\"start\":17628},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17644,\"start\":17641},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19265,\"start\":19262},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19279,\"start\":19275},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19290,\"start\":19286},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19304,\"start\":19300},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19324,\"start\":19320},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":19951,\"start\":19947},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":20356,\"start\":20352},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20415,\"start\":20411},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":20428,\"start\":20424},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":20449,\"start\":20445},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20809,\"start\":20806},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20953,\"start\":20949},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":20967,\"start\":20963},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20978,\"start\":20974},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":20989,\"start\":20985},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":20999,\"start\":20995},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":21015,\"start\":21011},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":21077,\"start\":21073},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":21080,\"start\":21077},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21148,\"start\":21144},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":21163,\"start\":21159},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21722,\"start\":21718},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21736,\"start\":21732},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":21765,\"start\":21761},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":21778,\"start\":21774},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":21791,\"start\":21787},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":21803,\"start\":21799},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":21814,\"start\":21810},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21828,\"start\":21824},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":21839,\"start\":21835},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":21851,\"start\":21847},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":21867,\"start\":21863},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":21883,\"start\":21879},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":21886,\"start\":21883},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22971,\"start\":22967},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":22982,\"start\":22978},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":22994,\"start\":22990},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":23006,\"start\":23002},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":23137,\"start\":23133},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":23176,\"start\":23172},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":23195,\"start\":23191},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":23207,\"start\":23203},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23215,\"start\":23212},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":23227,\"start\":23223},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23354,\"start\":23350},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":23393,\"start\":23389},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24237,\"start\":24234},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":24410,\"start\":24406}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24238,\"start\":24127},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24314,\"start\":24239},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24429,\"start\":24315},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24518,\"start\":24430},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24659,\"start\":24519},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28436,\"start\":24660},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30465,\"start\":28437},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32175,\"start\":30466},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33359,\"start\":32176},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34706,\"start\":33360}]", "paragraph": "[{\"end\":2273,\"start\":1893},{\"end\":2925,\"start\":2275},{\"end\":3642,\"start\":2927},{\"end\":3994,\"start\":3644},{\"end\":4916,\"start\":3996},{\"end\":5251,\"start\":4918},{\"end\":5420,\"start\":5253},{\"end\":6382,\"start\":5457},{\"end\":7566,\"start\":6405},{\"end\":8149,\"start\":7600},{\"end\":8486,\"start\":8173},{\"end\":8783,\"start\":8510},{\"end\":9318,\"start\":8854},{\"end\":9522,\"start\":9351},{\"end\":10164,\"start\":9524},{\"end\":10351,\"start\":10202},{\"end\":10874,\"start\":10376},{\"end\":10957,\"start\":10903},{\"end\":11288,\"start\":10993},{\"end\":11516,\"start\":11342},{\"end\":12047,\"start\":11557},{\"end\":12717,\"start\":12049},{\"end\":12975,\"start\":12755},{\"end\":13539,\"start\":13028},{\"end\":13906,\"start\":13584},{\"end\":14694,\"start\":13943},{\"end\":17412,\"start\":14728},{\"end\":19839,\"start\":17436},{\"end\":21497,\"start\":19841},{\"end\":21666,\"start\":21577},{\"end\":22498,\"start\":21668},{\"end\":22770,\"start\":22500},{\"end\":23177,\"start\":22785},{\"end\":23394,\"start\":23179},{\"end\":24126,\"start\":23396}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8172,\"start\":8150},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8509,\"start\":8487},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8853,\"start\":8784},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9350,\"start\":9319},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10201,\"start\":10165},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10375,\"start\":10352},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10902,\"start\":10875},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11341,\"start\":11289},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11556,\"start\":11517},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12754,\"start\":12718},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13027,\"start\":12976},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13583,\"start\":13540}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16731,\"start\":16724},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17463,\"start\":17456},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19570,\"start\":19563},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20845,\"start\":20838},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":21651,\"start\":21632}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1891,\"start\":1879},{\"attributes\":{\"n\":\"2.\"},\"end\":5435,\"start\":5423},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5455,\"start\":5438},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6403,\"start\":6385},{\"attributes\":{\"n\":\"3.\"},\"end\":7575,\"start\":7569},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7598,\"start\":7578},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10991,\"start\":10960},{\"attributes\":{\"n\":\"4.\"},\"end\":13920,\"start\":13909},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13941,\"start\":13923},{\"attributes\":{\"n\":\"4.2.\"},\"end\":14726,\"start\":14697},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17434,\"start\":17415},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21546,\"start\":21500},{\"attributes\":{\"n\":\"4.5.\"},\"end\":21575,\"start\":21549},{\"attributes\":{\"n\":\"5.\"},\"end\":22783,\"start\":22773},{\"end\":24138,\"start\":24128},{\"end\":24250,\"start\":24240},{\"end\":24326,\"start\":24316},{\"end\":24441,\"start\":24431},{\"end\":24529,\"start\":24520},{\"end\":24670,\"start\":24661},{\"end\":28447,\"start\":28438},{\"end\":30476,\"start\":30467},{\"end\":32186,\"start\":32177},{\"end\":33370,\"start\":33361}]", "table": "[{\"end\":24659,\"start\":24561},{\"end\":28436,\"start\":24911},{\"end\":30465,\"start\":28645},{\"end\":32175,\"start\":30696},{\"end\":33359,\"start\":32384},{\"end\":34706,\"start\":33564}]", "figure_caption": "[{\"end\":24238,\"start\":24140},{\"end\":24314,\"start\":24252},{\"end\":24429,\"start\":24328},{\"end\":24518,\"start\":24443},{\"end\":24561,\"start\":24531},{\"end\":24911,\"start\":24672},{\"end\":28645,\"start\":28449},{\"end\":30696,\"start\":30478},{\"end\":32384,\"start\":32188},{\"end\":33564,\"start\":33372}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5329,\"start\":5323},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7618,\"start\":7612},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11011,\"start\":11005},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12293,\"start\":12287},{\"end\":14994,\"start\":14989},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15719,\"start\":15710},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15870,\"start\":15861},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16479,\"start\":16470},{\"end\":18806,\"start\":18800},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20469,\"start\":20463},{\"end\":20719,\"start\":20711},{\"end\":23029,\"start\":23021},{\"end\":23250,\"start\":23242}]", "bib_author_first_name": "[{\"end\":35034,\"start\":35027},{\"end\":35050,\"start\":35046},{\"end\":35428,\"start\":35421},{\"end\":35442,\"start\":35434},{\"end\":35457,\"start\":35449},{\"end\":35850,\"start\":35845},{\"end\":35868,\"start\":35863},{\"end\":36230,\"start\":36224},{\"end\":36242,\"start\":36239},{\"end\":36258,\"start\":36249},{\"end\":36273,\"start\":36265},{\"end\":36283,\"start\":36281},{\"end\":36298,\"start\":36291},{\"end\":36556,\"start\":36548},{\"end\":36567,\"start\":36562},{\"end\":36575,\"start\":36572},{\"end\":36586,\"start\":36583},{\"end\":36852,\"start\":36845},{\"end\":36870,\"start\":36861},{\"end\":36885,\"start\":36878},{\"end\":36903,\"start\":36896},{\"end\":36922,\"start\":36913},{\"end\":36939,\"start\":36933},{\"end\":37333,\"start\":37328},{\"end\":37351,\"start\":37345},{\"end\":37363,\"start\":37359},{\"end\":37739,\"start\":37733},{\"end\":37758,\"start\":37753},{\"end\":37779,\"start\":37773},{\"end\":37794,\"start\":37788},{\"end\":38153,\"start\":38146},{\"end\":38165,\"start\":38160},{\"end\":38178,\"start\":38172},{\"end\":38189,\"start\":38184},{\"end\":38200,\"start\":38194},{\"end\":38214,\"start\":38207},{\"end\":38225,\"start\":38220},{\"end\":38238,\"start\":38230},{\"end\":38247,\"start\":38243},{\"end\":38255,\"start\":38252},{\"end\":38699,\"start\":38693},{\"end\":38712,\"start\":38706},{\"end\":39100,\"start\":39093},{\"end\":39114,\"start\":39108},{\"end\":39128,\"start\":39121},{\"end\":39140,\"start\":39133},{\"end\":39449,\"start\":39439},{\"end\":39457,\"start\":39455},{\"end\":39472,\"start\":39465},{\"end\":39483,\"start\":39477},{\"end\":39496,\"start\":39488},{\"end\":39869,\"start\":39856},{\"end\":39889,\"start\":39882},{\"end\":39904,\"start\":39898},{\"end\":40195,\"start\":40187},{\"end\":40213,\"start\":40203},{\"end\":40227,\"start\":40219},{\"end\":40244,\"start\":40239},{\"end\":40587,\"start\":40584},{\"end\":40600,\"start\":40593},{\"end\":40614,\"start\":40606},{\"end\":40629,\"start\":40622},{\"end\":40638,\"start\":40635},{\"end\":41028,\"start\":41025},{\"end\":41041,\"start\":41035},{\"end\":41052,\"start\":41049},{\"end\":41064,\"start\":41057},{\"end\":41075,\"start\":41069},{\"end\":41400,\"start\":41396},{\"end\":41412,\"start\":41407},{\"end\":41423,\"start\":41419},{\"end\":41430,\"start\":41424},{\"end\":41442,\"start\":41436},{\"end\":41777,\"start\":41773},{\"end\":41788,\"start\":41784},{\"end\":41795,\"start\":41789},{\"end\":41808,\"start\":41801},{\"end\":41819,\"start\":41813},{\"end\":42144,\"start\":42138},{\"end\":42163,\"start\":42158},{\"end\":42180,\"start\":42171},{\"end\":42197,\"start\":42193},{\"end\":42218,\"start\":42211},{\"end\":42231,\"start\":42225},{\"end\":42252,\"start\":42245},{\"end\":42271,\"start\":42263},{\"end\":42287,\"start\":42282},{\"end\":42304,\"start\":42297},{\"end\":42694,\"start\":42691},{\"end\":42709,\"start\":42704},{\"end\":42724,\"start\":42717},{\"end\":42737,\"start\":42730},{\"end\":43043,\"start\":43033},{\"end\":43059,\"start\":43053},{\"end\":43082,\"start\":43074},{\"end\":43096,\"start\":43091},{\"end\":43491,\"start\":43481},{\"end\":43505,\"start\":43497},{\"end\":43522,\"start\":43517},{\"end\":43849,\"start\":43845},{\"end\":44027,\"start\":44021},{\"end\":44045,\"start\":44038},{\"end\":44054,\"start\":44050},{\"end\":44418,\"start\":44411},{\"end\":44429,\"start\":44423},{\"end\":44445,\"start\":44436},{\"end\":44458,\"start\":44451},{\"end\":44474,\"start\":44465},{\"end\":44818,\"start\":44811},{\"end\":44832,\"start\":44823},{\"end\":44842,\"start\":44838},{\"end\":44854,\"start\":44847},{\"end\":44865,\"start\":44861},{\"end\":45169,\"start\":45166},{\"end\":45186,\"start\":45182},{\"end\":45207,\"start\":45202},{\"end\":45219,\"start\":45215},{\"end\":45229,\"start\":45224},{\"end\":45251,\"start\":45244},{\"end\":45264,\"start\":45259},{\"end\":45282,\"start\":45276},{\"end\":45639,\"start\":45632},{\"end\":45648,\"start\":45644},{\"end\":45658,\"start\":45655},{\"end\":45998,\"start\":45991},{\"end\":46006,\"start\":46003},{\"end\":46022,\"start\":46014},{\"end\":46036,\"start\":46028},{\"end\":46410,\"start\":46406},{\"end\":46420,\"start\":46416},{\"end\":46435,\"start\":46427},{\"end\":46444,\"start\":46442},{\"end\":46459,\"start\":46451},{\"end\":46472,\"start\":46465},{\"end\":46484,\"start\":46479},{\"end\":46496,\"start\":46489},{\"end\":46892,\"start\":46884},{\"end\":46907,\"start\":46900},{\"end\":46932,\"start\":46923},{\"end\":47268,\"start\":47261},{\"end\":47277,\"start\":47273},{\"end\":47289,\"start\":47283},{\"end\":47617,\"start\":47610},{\"end\":47627,\"start\":47622},{\"end\":47639,\"start\":47632},{\"end\":47650,\"start\":47646},{\"end\":47664,\"start\":47656},{\"end\":47675,\"start\":47671},{\"end\":48043,\"start\":48036},{\"end\":48059,\"start\":48051},{\"end\":48075,\"start\":48067},{\"end\":48436,\"start\":48431},{\"end\":48447,\"start\":48442},{\"end\":48459,\"start\":48453},{\"end\":48472,\"start\":48466},{\"end\":48808,\"start\":48801},{\"end\":48818,\"start\":48816},{\"end\":48831,\"start\":48824},{\"end\":48845,\"start\":48836},{\"end\":48858,\"start\":48850},{\"end\":48867,\"start\":48865},{\"end\":49222,\"start\":49213},{\"end\":49230,\"start\":49227},{\"end\":49240,\"start\":49236},{\"end\":49254,\"start\":49246},{\"end\":49266,\"start\":49261},{\"end\":49277,\"start\":49271},{\"end\":49676,\"start\":49672},{\"end\":49689,\"start\":49682},{\"end\":49703,\"start\":49695},{\"end\":49713,\"start\":49710},{\"end\":50075,\"start\":50069},{\"end\":50094,\"start\":50085},{\"end\":50104,\"start\":50102},{\"end\":50434,\"start\":50429},{\"end\":50444,\"start\":50440},{\"end\":50449,\"start\":50445},{\"end\":50464,\"start\":50455},{\"end\":50840,\"start\":50833},{\"end\":50849,\"start\":50846},{\"end\":50855,\"start\":50850},{\"end\":50867,\"start\":50861},{\"end\":50884,\"start\":50874},{\"end\":50900,\"start\":50890},{\"end\":50914,\"start\":50906},{\"end\":50924,\"start\":50921},{\"end\":50927,\"start\":50925},{\"end\":51367,\"start\":51358},{\"end\":51380,\"start\":51373},{\"end\":51396,\"start\":51388},{\"end\":51414,\"start\":51404},{\"end\":51803,\"start\":51794},{\"end\":51816,\"start\":51811},{\"end\":51830,\"start\":51824},{\"end\":51843,\"start\":51839},{\"end\":51861,\"start\":51855},{\"end\":51883,\"start\":51874},{\"end\":51898,\"start\":51892},{\"end\":51914,\"start\":51907},{\"end\":51931,\"start\":51923},{\"end\":51943,\"start\":51938},{\"end\":52465,\"start\":52460},{\"end\":52473,\"start\":52470},{\"end\":52482,\"start\":52480},{\"end\":52495,\"start\":52487},{\"end\":52510,\"start\":52503},{\"end\":52520,\"start\":52515},{\"end\":52836,\"start\":52831},{\"end\":52844,\"start\":52841},{\"end\":52860,\"start\":52852},{\"end\":52870,\"start\":52866},{\"end\":52883,\"start\":52880},{\"end\":53135,\"start\":53131},{\"end\":53147,\"start\":53140},{\"end\":53159,\"start\":53154},{\"end\":53172,\"start\":53165},{\"end\":53187,\"start\":53179},{\"end\":53197,\"start\":53194},{\"end\":53545,\"start\":53537},{\"end\":53557,\"start\":53553},{\"end\":53567,\"start\":53564},{\"end\":53574,\"start\":53572},{\"end\":53586,\"start\":53581},{\"end\":53891,\"start\":53884},{\"end\":53906,\"start\":53899},{\"end\":53919,\"start\":53916},{\"end\":53933,\"start\":53927},{\"end\":53948,\"start\":53945},{\"end\":53963,\"start\":53959},{\"end\":54412,\"start\":54405},{\"end\":54426,\"start\":54420},{\"end\":54435,\"start\":54432},{\"end\":54446,\"start\":54443},{\"end\":54461,\"start\":54457},{\"end\":54853,\"start\":54846},{\"end\":54864,\"start\":54861},{\"end\":54879,\"start\":54872},{\"end\":54887,\"start\":54884},{\"end\":54902,\"start\":54898},{\"end\":55271,\"start\":55268},{\"end\":55285,\"start\":55277},{\"end\":55297,\"start\":55291},{\"end\":55311,\"start\":55303},{\"end\":55326,\"start\":55317},{\"end\":55700,\"start\":55696},{\"end\":55711,\"start\":55706},{\"end\":55723,\"start\":55717},{\"end\":55733,\"start\":55729},{\"end\":55740,\"start\":55734},{\"end\":55754,\"start\":55746},{\"end\":56059,\"start\":56057},{\"end\":56070,\"start\":56065},{\"end\":56087,\"start\":56079},{\"end\":56098,\"start\":56094},{\"end\":56111,\"start\":56108},{\"end\":56125,\"start\":56118},{\"end\":56136,\"start\":56131},{\"end\":56426,\"start\":56420},{\"end\":56439,\"start\":56432},{\"end\":56450,\"start\":56447},{\"end\":56463,\"start\":56458},{\"end\":56477,\"start\":56469},{\"end\":56773,\"start\":56770},{\"end\":56785,\"start\":56779},{\"end\":56793,\"start\":56791},{\"end\":56801,\"start\":56799},{\"end\":56813,\"start\":56809},{\"end\":56828,\"start\":56825},{\"end\":57151,\"start\":57149},{\"end\":57163,\"start\":57157},{\"end\":57172,\"start\":57169},{\"end\":57181,\"start\":57178},{\"end\":57192,\"start\":57186},{\"end\":57203,\"start\":57198},{\"end\":57218,\"start\":57211},{\"end\":57231,\"start\":57224},{\"end\":57567,\"start\":57559},{\"end\":57577,\"start\":57573},{\"end\":57588,\"start\":57583},{\"end\":57602,\"start\":57596},{\"end\":57613,\"start\":57607},{\"end\":57621,\"start\":57618},{\"end\":58033,\"start\":58028},{\"end\":58050,\"start\":58042},{\"end\":58065,\"start\":58060},{\"end\":58079,\"start\":58071},{\"end\":58591,\"start\":58586},{\"end\":58608,\"start\":58600},{\"end\":58623,\"start\":58618},{\"end\":58637,\"start\":58629},{\"end\":59036,\"start\":59030},{\"end\":59049,\"start\":59045},{\"end\":59059,\"start\":59055},{\"end\":59074,\"start\":59069},{\"end\":59089,\"start\":59085},{\"end\":59106,\"start\":59097},{\"end\":59125,\"start\":59117},{\"end\":59468,\"start\":59463},{\"end\":59480,\"start\":59474},{\"end\":59492,\"start\":59486},{\"end\":59797,\"start\":59792},{\"end\":59814,\"start\":59808},{\"end\":60115,\"start\":60112},{\"end\":60127,\"start\":60121},{\"end\":60138,\"start\":60133},{\"end\":60151,\"start\":60144},{\"end\":60167,\"start\":60159},{\"end\":60181,\"start\":60174},{\"end\":60194,\"start\":60188},{\"end\":60210,\"start\":60202},{\"end\":60223,\"start\":60216},{\"end\":60618,\"start\":60614},{\"end\":60627,\"start\":60625},{\"end\":60642,\"start\":60635},{\"end\":60655,\"start\":60648},{\"end\":60662,\"start\":60660},{\"end\":60674,\"start\":60670},{\"end\":60897,\"start\":60891},{\"end\":60911,\"start\":60905},{\"end\":61146,\"start\":61140},{\"end\":61165,\"start\":61161},{\"end\":61180,\"start\":61174},{\"end\":61195,\"start\":61190},{\"end\":61209,\"start\":61203},{\"end\":61228,\"start\":61220},{\"end\":61750,\"start\":61744},{\"end\":61760,\"start\":61756},{\"end\":61778,\"start\":61772},{\"end\":61795,\"start\":61787},{\"end\":61803,\"start\":61802},{\"end\":61815,\"start\":61812},{\"end\":61830,\"start\":61824},{\"end\":61844,\"start\":61839},{\"end\":62291,\"start\":62285},{\"end\":62300,\"start\":62297},{\"end\":62312,\"start\":62306},{\"end\":62327,\"start\":62321},{\"end\":62350,\"start\":62347},{\"end\":62639,\"start\":62635},{\"end\":62649,\"start\":62645},{\"end\":62664,\"start\":62656},{\"end\":62677,\"start\":62670},{\"end\":62991,\"start\":62984},{\"end\":63002,\"start\":62998},{\"end\":63015,\"start\":63007},{\"end\":63261,\"start\":63257},{\"end\":63278,\"start\":63271},{\"end\":63281,\"start\":63279},{\"end\":63291,\"start\":63288},{\"end\":63660,\"start\":63656},{\"end\":63677,\"start\":63670},{\"end\":63680,\"start\":63678},{\"end\":63690,\"start\":63687},{\"end\":64010,\"start\":64006},{\"end\":64028,\"start\":64020},{\"end\":64043,\"start\":64035},{\"end\":64060,\"start\":64051},{\"end\":64444,\"start\":64438},{\"end\":64460,\"start\":64454},{\"end\":64482,\"start\":64475},{\"end\":64497,\"start\":64493},{\"end\":64511,\"start\":64506},{\"end\":64530,\"start\":64522},{\"end\":64802,\"start\":64796},{\"end\":64816,\"start\":64812},{\"end\":64830,\"start\":64826},{\"end\":64844,\"start\":64839},{\"end\":64861,\"start\":64856},{\"end\":64874,\"start\":64869},{\"end\":64876,\"start\":64875},{\"end\":64890,\"start\":64884},{\"end\":64904,\"start\":64899},{\"end\":65259,\"start\":65250},{\"end\":65274,\"start\":65266},{\"end\":65287,\"start\":65281},{\"end\":65300,\"start\":65294},{\"end\":65312,\"start\":65305},{\"end\":65322,\"start\":65319},{\"end\":65332,\"start\":65327},{\"end\":65733,\"start\":65724},{\"end\":65748,\"start\":65740},{\"end\":65762,\"start\":65755},{\"end\":65777,\"start\":65770},{\"end\":65790,\"start\":65783},{\"end\":65800,\"start\":65797},{\"end\":65810,\"start\":65805},{\"end\":66201,\"start\":66192},{\"end\":66216,\"start\":66208},{\"end\":66230,\"start\":66223},{\"end\":66243,\"start\":66236},{\"end\":66253,\"start\":66250},{\"end\":66263,\"start\":66258},{\"end\":66665,\"start\":66659},{\"end\":66680,\"start\":66672},{\"end\":66690,\"start\":66686},{\"end\":66701,\"start\":66697},{\"end\":66996,\"start\":66990},{\"end\":67005,\"start\":67003},{\"end\":67018,\"start\":67010},{\"end\":67029,\"start\":67023},{\"end\":67039,\"start\":67034},{\"end\":67049,\"start\":67045},{\"end\":67058,\"start\":67056},{\"end\":67076,\"start\":67065},{\"end\":67460,\"start\":67452},{\"end\":67475,\"start\":67467},{\"end\":67488,\"start\":67481},{\"end\":67504,\"start\":67494},{\"end\":67797,\"start\":67790},{\"end\":67810,\"start\":67803},{\"end\":67820,\"start\":67815},{\"end\":67829,\"start\":67825},{\"end\":67846,\"start\":67839},{\"end\":67859,\"start\":67852},{\"end\":68282,\"start\":68276},{\"end\":68295,\"start\":68287},{\"end\":68309,\"start\":68300},{\"end\":68320,\"start\":68315},{\"end\":68333,\"start\":68326},{\"end\":68349,\"start\":68341},{\"end\":68364,\"start\":68355},{\"end\":68381,\"start\":68375},{\"end\":68396,\"start\":68392},{\"end\":68411,\"start\":68406},{\"end\":68822,\"start\":68816},{\"end\":68832,\"start\":68828},{\"end\":69118,\"start\":69114},{\"end\":69131,\"start\":69125},{\"end\":69143,\"start\":69139},{\"end\":69158,\"start\":69152},{\"end\":69520,\"start\":69515},{\"end\":69535,\"start\":69528},{\"end\":69547,\"start\":69542},{\"end\":69846,\"start\":69843},{\"end\":69859,\"start\":69854},{\"end\":69872,\"start\":69864},{\"end\":69881,\"start\":69878},{\"end\":69892,\"start\":69889},{\"end\":69907,\"start\":69903},{\"end\":70283,\"start\":70280},{\"end\":70298,\"start\":70291},{\"end\":70309,\"start\":70306},{\"end\":70324,\"start\":70320},{\"end\":70708,\"start\":70705},{\"end\":70724,\"start\":70716},{\"end\":70736,\"start\":70730},{\"end\":70747,\"start\":70743},{\"end\":70757,\"start\":70754},{\"end\":71083,\"start\":71080},{\"end\":71099,\"start\":71091},{\"end\":71112,\"start\":71105},{\"end\":71120,\"start\":71117},{\"end\":71471,\"start\":71468},{\"end\":71487,\"start\":71479},{\"end\":71496,\"start\":71493},{\"end\":71824,\"start\":71821},{\"end\":71840,\"start\":71832},{\"end\":71849,\"start\":71846},{\"end\":72227,\"start\":72224},{\"end\":72242,\"start\":72235},{\"end\":72253,\"start\":72247},{\"end\":72265,\"start\":72262},{\"end\":72586,\"start\":72581},{\"end\":72601,\"start\":72594},{\"end\":72609,\"start\":72606},{\"end\":72620,\"start\":72614},{\"end\":72633,\"start\":72627},{\"end\":72644,\"start\":72641},{\"end\":72986,\"start\":72981},{\"end\":73001,\"start\":72994},{\"end\":73009,\"start\":73006},{\"end\":73020,\"start\":73014},{\"end\":73031,\"start\":73028},{\"end\":73293,\"start\":73288},{\"end\":73307,\"start\":73301},{\"end\":73316,\"start\":73314},{\"end\":73329,\"start\":73323},{\"end\":73340,\"start\":73337},{\"end\":73665,\"start\":73660},{\"end\":73679,\"start\":73673},{\"end\":73688,\"start\":73686},{\"end\":73701,\"start\":73695},{\"end\":73712,\"start\":73709},{\"end\":74087,\"start\":74081},{\"end\":74102,\"start\":74095},{\"end\":74117,\"start\":74107},{\"end\":74131,\"start\":74124},{\"end\":74142,\"start\":74137},{\"end\":74154,\"start\":74148},{\"end\":74167,\"start\":74161},{\"end\":74180,\"start\":74172},{\"end\":74190,\"start\":74187},{\"end\":74199,\"start\":74198},{\"end\":74201,\"start\":74200},{\"end\":74682,\"start\":74673},{\"end\":74695,\"start\":74689},{\"end\":74711,\"start\":74703},{\"end\":74728,\"start\":74717}]", "bib_author_last_name": "[{\"end\":35044,\"start\":35035},{\"end\":35058,\"start\":35051},{\"end\":35432,\"start\":35429},{\"end\":35447,\"start\":35443},{\"end\":35462,\"start\":35458},{\"end\":35861,\"start\":35851},{\"end\":35874,\"start\":35869},{\"end\":36237,\"start\":36231},{\"end\":36247,\"start\":36243},{\"end\":36263,\"start\":36259},{\"end\":36279,\"start\":36274},{\"end\":36289,\"start\":36284},{\"end\":36303,\"start\":36299},{\"end\":36309,\"start\":36305},{\"end\":36560,\"start\":36557},{\"end\":36570,\"start\":36568},{\"end\":36581,\"start\":36576},{\"end\":36595,\"start\":36587},{\"end\":36859,\"start\":36853},{\"end\":36876,\"start\":36871},{\"end\":36894,\"start\":36886},{\"end\":36911,\"start\":36904},{\"end\":36931,\"start\":36923},{\"end\":36949,\"start\":36940},{\"end\":37343,\"start\":37334},{\"end\":37357,\"start\":37352},{\"end\":37370,\"start\":37364},{\"end\":37751,\"start\":37740},{\"end\":37771,\"start\":37759},{\"end\":37786,\"start\":37780},{\"end\":37802,\"start\":37795},{\"end\":38158,\"start\":38154},{\"end\":38170,\"start\":38166},{\"end\":38182,\"start\":38179},{\"end\":38192,\"start\":38190},{\"end\":38205,\"start\":38201},{\"end\":38218,\"start\":38215},{\"end\":38228,\"start\":38226},{\"end\":38241,\"start\":38239},{\"end\":38250,\"start\":38248},{\"end\":38259,\"start\":38256},{\"end\":38704,\"start\":38700},{\"end\":38717,\"start\":38713},{\"end\":39106,\"start\":39101},{\"end\":39119,\"start\":39115},{\"end\":39131,\"start\":39129},{\"end\":39143,\"start\":39141},{\"end\":39453,\"start\":39450},{\"end\":39463,\"start\":39458},{\"end\":39475,\"start\":39473},{\"end\":39486,\"start\":39484},{\"end\":39499,\"start\":39497},{\"end\":39880,\"start\":39870},{\"end\":39896,\"start\":39890},{\"end\":39910,\"start\":39905},{\"end\":40201,\"start\":40196},{\"end\":40217,\"start\":40214},{\"end\":40237,\"start\":40228},{\"end\":40255,\"start\":40245},{\"end\":40591,\"start\":40588},{\"end\":40604,\"start\":40601},{\"end\":40620,\"start\":40615},{\"end\":40633,\"start\":40630},{\"end\":40644,\"start\":40639},{\"end\":41033,\"start\":41029},{\"end\":41047,\"start\":41042},{\"end\":41055,\"start\":41053},{\"end\":41067,\"start\":41065},{\"end\":41080,\"start\":41076},{\"end\":41405,\"start\":41401},{\"end\":41417,\"start\":41413},{\"end\":41434,\"start\":41431},{\"end\":41447,\"start\":41443},{\"end\":41782,\"start\":41778},{\"end\":41799,\"start\":41796},{\"end\":41811,\"start\":41809},{\"end\":41824,\"start\":41820},{\"end\":42156,\"start\":42145},{\"end\":42169,\"start\":42164},{\"end\":42191,\"start\":42181},{\"end\":42209,\"start\":42198},{\"end\":42223,\"start\":42219},{\"end\":42243,\"start\":42232},{\"end\":42261,\"start\":42253},{\"end\":42280,\"start\":42272},{\"end\":42295,\"start\":42288},{\"end\":42310,\"start\":42305},{\"end\":42702,\"start\":42695},{\"end\":42715,\"start\":42710},{\"end\":42728,\"start\":42725},{\"end\":42749,\"start\":42738},{\"end\":43051,\"start\":43044},{\"end\":43072,\"start\":43060},{\"end\":43089,\"start\":43083},{\"end\":43106,\"start\":43097},{\"end\":43495,\"start\":43492},{\"end\":43515,\"start\":43506},{\"end\":43533,\"start\":43523},{\"end\":43857,\"start\":43850},{\"end\":44036,\"start\":44028},{\"end\":44048,\"start\":44046},{\"end\":44062,\"start\":44055},{\"end\":44421,\"start\":44419},{\"end\":44434,\"start\":44430},{\"end\":44449,\"start\":44446},{\"end\":44463,\"start\":44459},{\"end\":44478,\"start\":44475},{\"end\":44821,\"start\":44819},{\"end\":44836,\"start\":44833},{\"end\":44845,\"start\":44843},{\"end\":44859,\"start\":44855},{\"end\":44873,\"start\":44866},{\"end\":45180,\"start\":45170},{\"end\":45200,\"start\":45187},{\"end\":45213,\"start\":45208},{\"end\":45222,\"start\":45220},{\"end\":45242,\"start\":45230},{\"end\":45257,\"start\":45252},{\"end\":45274,\"start\":45265},{\"end\":45289,\"start\":45283},{\"end\":45642,\"start\":45640},{\"end\":45653,\"start\":45649},{\"end\":45661,\"start\":45659},{\"end\":46001,\"start\":45999},{\"end\":46012,\"start\":46007},{\"end\":46026,\"start\":46023},{\"end\":46041,\"start\":46037},{\"end\":46414,\"start\":46411},{\"end\":46425,\"start\":46421},{\"end\":46440,\"start\":46436},{\"end\":46449,\"start\":46445},{\"end\":46463,\"start\":46460},{\"end\":46477,\"start\":46473},{\"end\":46487,\"start\":46485},{\"end\":46500,\"start\":46497},{\"end\":46898,\"start\":46893},{\"end\":46921,\"start\":46908},{\"end\":46938,\"start\":46933},{\"end\":47271,\"start\":47269},{\"end\":47281,\"start\":47278},{\"end\":47294,\"start\":47290},{\"end\":47620,\"start\":47618},{\"end\":47630,\"start\":47628},{\"end\":47644,\"start\":47640},{\"end\":47654,\"start\":47651},{\"end\":47669,\"start\":47665},{\"end\":47681,\"start\":47676},{\"end\":48049,\"start\":48044},{\"end\":48065,\"start\":48060},{\"end\":48081,\"start\":48076},{\"end\":48440,\"start\":48437},{\"end\":48451,\"start\":48448},{\"end\":48464,\"start\":48460},{\"end\":48477,\"start\":48473},{\"end\":48814,\"start\":48809},{\"end\":48822,\"start\":48819},{\"end\":48834,\"start\":48832},{\"end\":48848,\"start\":48846},{\"end\":48863,\"start\":48859},{\"end\":48872,\"start\":48868},{\"end\":49225,\"start\":49223},{\"end\":49234,\"start\":49231},{\"end\":49244,\"start\":49241},{\"end\":49259,\"start\":49255},{\"end\":49269,\"start\":49267},{\"end\":49283,\"start\":49278},{\"end\":49680,\"start\":49677},{\"end\":49693,\"start\":49690},{\"end\":49708,\"start\":49704},{\"end\":49719,\"start\":49714},{\"end\":50083,\"start\":50076},{\"end\":50100,\"start\":50095},{\"end\":50112,\"start\":50105},{\"end\":50438,\"start\":50435},{\"end\":50453,\"start\":50450},{\"end\":50468,\"start\":50465},{\"end\":50844,\"start\":50841},{\"end\":50859,\"start\":50856},{\"end\":50872,\"start\":50868},{\"end\":50888,\"start\":50885},{\"end\":50904,\"start\":50901},{\"end\":50919,\"start\":50915},{\"end\":50931,\"start\":50928},{\"end\":51371,\"start\":51368},{\"end\":51386,\"start\":51381},{\"end\":51402,\"start\":51397},{\"end\":51419,\"start\":51415},{\"end\":51809,\"start\":51804},{\"end\":51822,\"start\":51817},{\"end\":51837,\"start\":51831},{\"end\":51853,\"start\":51844},{\"end\":51872,\"start\":51862},{\"end\":51890,\"start\":51884},{\"end\":51905,\"start\":51899},{\"end\":51921,\"start\":51915},{\"end\":51936,\"start\":51932},{\"end\":51948,\"start\":51944},{\"end\":52468,\"start\":52466},{\"end\":52478,\"start\":52474},{\"end\":52485,\"start\":52483},{\"end\":52501,\"start\":52496},{\"end\":52513,\"start\":52511},{\"end\":52524,\"start\":52521},{\"end\":52839,\"start\":52837},{\"end\":52850,\"start\":52845},{\"end\":52864,\"start\":52861},{\"end\":52878,\"start\":52871},{\"end\":52892,\"start\":52884},{\"end\":53138,\"start\":53136},{\"end\":53152,\"start\":53148},{\"end\":53163,\"start\":53160},{\"end\":53177,\"start\":53173},{\"end\":53192,\"start\":53188},{\"end\":53200,\"start\":53198},{\"end\":53551,\"start\":53546},{\"end\":53562,\"start\":53558},{\"end\":53570,\"start\":53568},{\"end\":53579,\"start\":53575},{\"end\":53590,\"start\":53587},{\"end\":53897,\"start\":53892},{\"end\":53914,\"start\":53907},{\"end\":53925,\"start\":53920},{\"end\":53943,\"start\":53934},{\"end\":53957,\"start\":53949},{\"end\":53971,\"start\":53964},{\"end\":54418,\"start\":54413},{\"end\":54430,\"start\":54427},{\"end\":54441,\"start\":54436},{\"end\":54455,\"start\":54447},{\"end\":54469,\"start\":54462},{\"end\":54859,\"start\":54854},{\"end\":54870,\"start\":54865},{\"end\":54882,\"start\":54880},{\"end\":54896,\"start\":54888},{\"end\":54910,\"start\":54903},{\"end\":55275,\"start\":55272},{\"end\":55289,\"start\":55286},{\"end\":55301,\"start\":55298},{\"end\":55315,\"start\":55312},{\"end\":55330,\"start\":55327},{\"end\":55704,\"start\":55701},{\"end\":55715,\"start\":55712},{\"end\":55727,\"start\":55724},{\"end\":55744,\"start\":55741},{\"end\":55760,\"start\":55755},{\"end\":56063,\"start\":56060},{\"end\":56077,\"start\":56071},{\"end\":56092,\"start\":56088},{\"end\":56106,\"start\":56099},{\"end\":56116,\"start\":56112},{\"end\":56129,\"start\":56126},{\"end\":56148,\"start\":56137},{\"end\":56430,\"start\":56427},{\"end\":56445,\"start\":56440},{\"end\":56456,\"start\":56451},{\"end\":56467,\"start\":56464},{\"end\":56481,\"start\":56478},{\"end\":56777,\"start\":56774},{\"end\":56789,\"start\":56786},{\"end\":56797,\"start\":56794},{\"end\":56807,\"start\":56802},{\"end\":56823,\"start\":56814},{\"end\":56837,\"start\":56829},{\"end\":57155,\"start\":57152},{\"end\":57167,\"start\":57164},{\"end\":57176,\"start\":57173},{\"end\":57184,\"start\":57182},{\"end\":57196,\"start\":57193},{\"end\":57209,\"start\":57204},{\"end\":57222,\"start\":57219},{\"end\":57235,\"start\":57232},{\"end\":57571,\"start\":57568},{\"end\":57581,\"start\":57578},{\"end\":57594,\"start\":57589},{\"end\":57605,\"start\":57603},{\"end\":57616,\"start\":57614},{\"end\":57624,\"start\":57622},{\"end\":58040,\"start\":58034},{\"end\":58058,\"start\":58051},{\"end\":58069,\"start\":58066},{\"end\":58085,\"start\":58080},{\"end\":58598,\"start\":58592},{\"end\":58616,\"start\":58609},{\"end\":58627,\"start\":58624},{\"end\":58643,\"start\":58638},{\"end\":59043,\"start\":59037},{\"end\":59053,\"start\":59050},{\"end\":59067,\"start\":59060},{\"end\":59083,\"start\":59075},{\"end\":59095,\"start\":59090},{\"end\":59115,\"start\":59107},{\"end\":59132,\"start\":59126},{\"end\":59472,\"start\":59469},{\"end\":59484,\"start\":59481},{\"end\":59497,\"start\":59493},{\"end\":59806,\"start\":59798},{\"end\":59820,\"start\":59815},{\"end\":60119,\"start\":60116},{\"end\":60131,\"start\":60128},{\"end\":60142,\"start\":60139},{\"end\":60157,\"start\":60152},{\"end\":60172,\"start\":60168},{\"end\":60186,\"start\":60182},{\"end\":60200,\"start\":60195},{\"end\":60214,\"start\":60211},{\"end\":60228,\"start\":60224},{\"end\":60623,\"start\":60619},{\"end\":60633,\"start\":60628},{\"end\":60646,\"start\":60643},{\"end\":60658,\"start\":60656},{\"end\":60668,\"start\":60663},{\"end\":60679,\"start\":60675},{\"end\":60903,\"start\":60898},{\"end\":60916,\"start\":60912},{\"end\":61159,\"start\":61147},{\"end\":61172,\"start\":61166},{\"end\":61188,\"start\":61181},{\"end\":61201,\"start\":61196},{\"end\":61218,\"start\":61210},{\"end\":61235,\"start\":61229},{\"end\":61508,\"start\":61506},{\"end\":61516,\"start\":61510},{\"end\":61754,\"start\":61751},{\"end\":61770,\"start\":61761},{\"end\":61785,\"start\":61779},{\"end\":61800,\"start\":61796},{\"end\":61810,\"start\":61804},{\"end\":61822,\"start\":61816},{\"end\":61837,\"start\":61831},{\"end\":61853,\"start\":61845},{\"end\":61859,\"start\":61855},{\"end\":62295,\"start\":62292},{\"end\":62304,\"start\":62301},{\"end\":62319,\"start\":62313},{\"end\":62345,\"start\":62328},{\"end\":62358,\"start\":62351},{\"end\":62368,\"start\":62360},{\"end\":62643,\"start\":62640},{\"end\":62654,\"start\":62650},{\"end\":62668,\"start\":62665},{\"end\":62680,\"start\":62678},{\"end\":62996,\"start\":62992},{\"end\":63005,\"start\":63003},{\"end\":63019,\"start\":63016},{\"end\":63269,\"start\":63262},{\"end\":63286,\"start\":63282},{\"end\":63300,\"start\":63292},{\"end\":63668,\"start\":63661},{\"end\":63685,\"start\":63681},{\"end\":63699,\"start\":63691},{\"end\":64018,\"start\":64011},{\"end\":64033,\"start\":64029},{\"end\":64049,\"start\":64044},{\"end\":64066,\"start\":64061},{\"end\":64452,\"start\":64445},{\"end\":64473,\"start\":64461},{\"end\":64491,\"start\":64483},{\"end\":64504,\"start\":64498},{\"end\":64520,\"start\":64512},{\"end\":64537,\"start\":64531},{\"end\":64810,\"start\":64803},{\"end\":64824,\"start\":64817},{\"end\":64837,\"start\":64831},{\"end\":64854,\"start\":64845},{\"end\":64867,\"start\":64862},{\"end\":64882,\"start\":64877},{\"end\":64897,\"start\":64891},{\"end\":64915,\"start\":64905},{\"end\":65264,\"start\":65260},{\"end\":65279,\"start\":65275},{\"end\":65292,\"start\":65288},{\"end\":65303,\"start\":65301},{\"end\":65317,\"start\":65313},{\"end\":65325,\"start\":65323},{\"end\":65336,\"start\":65333},{\"end\":65738,\"start\":65734},{\"end\":65753,\"start\":65749},{\"end\":65768,\"start\":65763},{\"end\":65781,\"start\":65778},{\"end\":65795,\"start\":65791},{\"end\":65803,\"start\":65801},{\"end\":65814,\"start\":65811},{\"end\":66206,\"start\":66202},{\"end\":66221,\"start\":66217},{\"end\":66234,\"start\":66231},{\"end\":66248,\"start\":66244},{\"end\":66256,\"start\":66254},{\"end\":66267,\"start\":66264},{\"end\":66670,\"start\":66666},{\"end\":66684,\"start\":66681},{\"end\":66695,\"start\":66691},{\"end\":66706,\"start\":66702},{\"end\":67001,\"start\":66997},{\"end\":67008,\"start\":67006},{\"end\":67021,\"start\":67019},{\"end\":67032,\"start\":67030},{\"end\":67043,\"start\":67040},{\"end\":67054,\"start\":67050},{\"end\":67063,\"start\":67059},{\"end\":67080,\"start\":67077},{\"end\":67465,\"start\":67461},{\"end\":67479,\"start\":67476},{\"end\":67492,\"start\":67489},{\"end\":67508,\"start\":67505},{\"end\":67801,\"start\":67798},{\"end\":67813,\"start\":67811},{\"end\":67823,\"start\":67821},{\"end\":67837,\"start\":67830},{\"end\":67850,\"start\":67847},{\"end\":67864,\"start\":67860},{\"end\":68285,\"start\":68283},{\"end\":68298,\"start\":68296},{\"end\":68313,\"start\":68310},{\"end\":68324,\"start\":68321},{\"end\":68339,\"start\":68334},{\"end\":68353,\"start\":68350},{\"end\":68373,\"start\":68365},{\"end\":68390,\"start\":68382},{\"end\":68404,\"start\":68397},{\"end\":68417,\"start\":68412},{\"end\":68826,\"start\":68823},{\"end\":68844,\"start\":68833},{\"end\":69123,\"start\":69119},{\"end\":69137,\"start\":69132},{\"end\":69150,\"start\":69144},{\"end\":69166,\"start\":69159},{\"end\":69526,\"start\":69521},{\"end\":69540,\"start\":69536},{\"end\":69555,\"start\":69548},{\"end\":69852,\"start\":69847},{\"end\":69862,\"start\":69860},{\"end\":69876,\"start\":69873},{\"end\":69887,\"start\":69882},{\"end\":69901,\"start\":69893},{\"end\":69915,\"start\":69908},{\"end\":70289,\"start\":70284},{\"end\":70304,\"start\":70299},{\"end\":70318,\"start\":70310},{\"end\":70332,\"start\":70325},{\"end\":70714,\"start\":70709},{\"end\":70728,\"start\":70725},{\"end\":70741,\"start\":70737},{\"end\":70752,\"start\":70748},{\"end\":70763,\"start\":70758},{\"end\":71089,\"start\":71084},{\"end\":71103,\"start\":71100},{\"end\":71115,\"start\":71113},{\"end\":71126,\"start\":71121},{\"end\":71477,\"start\":71472},{\"end\":71491,\"start\":71488},{\"end\":71502,\"start\":71497},{\"end\":71830,\"start\":71825},{\"end\":71844,\"start\":71841},{\"end\":71855,\"start\":71850},{\"end\":72233,\"start\":72228},{\"end\":72245,\"start\":72243},{\"end\":72260,\"start\":72254},{\"end\":72268,\"start\":72266},{\"end\":72592,\"start\":72587},{\"end\":72604,\"start\":72602},{\"end\":72612,\"start\":72610},{\"end\":72625,\"start\":72621},{\"end\":72639,\"start\":72634},{\"end\":72647,\"start\":72645},{\"end\":72992,\"start\":72987},{\"end\":73004,\"start\":73002},{\"end\":73012,\"start\":73010},{\"end\":73026,\"start\":73021},{\"end\":73034,\"start\":73032},{\"end\":73299,\"start\":73294},{\"end\":73312,\"start\":73308},{\"end\":73321,\"start\":73317},{\"end\":73335,\"start\":73330},{\"end\":73343,\"start\":73341},{\"end\":73671,\"start\":73666},{\"end\":73684,\"start\":73680},{\"end\":73693,\"start\":73689},{\"end\":73707,\"start\":73702},{\"end\":73715,\"start\":73713},{\"end\":74093,\"start\":74088},{\"end\":74105,\"start\":74103},{\"end\":74122,\"start\":74118},{\"end\":74135,\"start\":74132},{\"end\":74146,\"start\":74143},{\"end\":74159,\"start\":74155},{\"end\":74170,\"start\":74168},{\"end\":74185,\"start\":74181},{\"end\":74196,\"start\":74191},{\"end\":74208,\"start\":74202},{\"end\":74214,\"start\":74210},{\"end\":74687,\"start\":74683},{\"end\":74701,\"start\":74696},{\"end\":74715,\"start\":74712},{\"end\":74732,\"start\":74729}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4493958},\"end\":35337,\"start\":34953},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4710341},\"end\":35707,\"start\":35339},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5250573},\"end\":36152,\"start\":35709},{\"attributes\":{\"doi\":\"arXiv:2105.05537\",\"id\":\"b3\"},\"end\":36546,\"start\":36154},{\"attributes\":{\"doi\":\"arXiv:2106.06847\",\"id\":\"b4\"},\"end\":36796,\"start\":36548},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218889832},\"end\":37237,\"start\":36798},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14379883},\"end\":37650,\"start\":37239},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":38030033},\"end\":38102,\"start\":37652},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":227239228},\"end\":38588,\"start\":38104},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15799108},\"end\":39007,\"start\":38590},{\"attributes\":{\"doi\":\"arXiv:2107.12679\",\"id\":\"b10\"},\"end\":39357,\"start\":39009},{\"attributes\":{\"id\":\"b11\"},\"end\":39785,\"start\":39359},{\"attributes\":{\"doi\":\"arXiv:1911.03584\",\"id\":\"b12\"},\"end\":40113,\"start\":39787},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1475121},\"end\":40516,\"start\":40115},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":174788791},\"end\":40939,\"start\":40518},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":231964334},\"end\":41329,\"start\":40941},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9569924},\"end\":41705,\"start\":41331},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18874645},\"end\":42060,\"start\":41707},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b18\"},\"end\":42643,\"start\":42062},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215827845},\"end\":42967,\"start\":42645},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211066440},\"end\":43377,\"start\":42969},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1151947},\"end\":43804,\"start\":43379},{\"attributes\":{\"id\":\"b22\"},\"end\":43965,\"start\":43806},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":208158302},\"end\":44344,\"start\":43967},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":235335499},\"end\":44746,\"start\":44346},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":202708640},\"end\":45135,\"start\":44748},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1033682},\"end\":45580,\"start\":45137},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":732748},\"end\":45917,\"start\":45582},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1663191},\"end\":46324,\"start\":45919},{\"attributes\":{\"id\":\"b29\"},\"end\":46831,\"start\":46326},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3739626},\"end\":47207,\"start\":46833},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":49333383},\"end\":47545,\"start\":47209},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":192571596},\"end\":47972,\"start\":47547},{\"attributes\":{\"id\":\"b33\"},\"end\":48349,\"start\":47974},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":202889117},\"end\":48735,\"start\":48351},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":220936542},\"end\":49140,\"start\":48737},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":220246167},\"end\":49604,\"start\":49142},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195803430},\"end\":49998,\"start\":49606},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":980236},\"end\":50355,\"start\":50000},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9971732},\"end\":50743,\"start\":50357},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":86765959},\"end\":51284,\"start\":50745},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1543021},\"end\":51706,\"start\":51286},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":211227},\"end\":52352,\"start\":51708},{\"attributes\":{\"doi\":\"arXiv:2105.10422\",\"id\":\"b43\"},\"end\":52777,\"start\":52354},{\"attributes\":{\"doi\":\"arXiv:2104.05707\",\"id\":\"b44\"},\"end\":53085,\"start\":52779},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":85496615},\"end\":53472,\"start\":53087},{\"attributes\":{\"doi\":\"arXiv:2104.09116\",\"id\":\"b46\"},\"end\":53784,\"start\":53474},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":236975856},\"end\":54308,\"start\":53786},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":236976360},\"end\":54776,\"start\":54310},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":232417580},\"end\":55199,\"start\":54778},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6540453},\"end\":55643,\"start\":55201},{\"attributes\":{\"doi\":\"arXiv:1806.02919\",\"id\":\"b51\"},\"end\":55959,\"start\":55645},{\"attributes\":{\"id\":\"b52\"},\"end\":56371,\"start\":55961},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":29151865},\"end\":56768,\"start\":56373},{\"attributes\":{\"doi\":\"arXiv:2106.03180\",\"id\":\"b54\"},\"end\":57074,\"start\":56770},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b55\"},\"end\":57482,\"start\":57076},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":221725942},\"end\":57886,\"start\":57484},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":64193},\"end\":58444,\"start\":57888},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":64193},\"end\":58975,\"start\":58446},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":8887614},\"end\":59406,\"start\":58977},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":235641598},\"end\":59752,\"start\":59408},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":7044126},\"end\":60046,\"start\":59754},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":221187023},\"end\":60534,\"start\":60048},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":127067164},\"end\":60889,\"start\":60536},{\"attributes\":{\"doi\":\"arXiv:1810.12575\",\"id\":\"b64\"},\"end\":61094,\"start\":60891},{\"attributes\":{\"doi\":\"arXiv:1906.05909\",\"id\":\"b65\"},\"end\":61454,\"start\":61096},{\"attributes\":{\"id\":\"b66\"},\"end\":61634,\"start\":61456},{\"attributes\":{\"id\":\"b67\"},\"end\":62240,\"start\":61636},{\"attributes\":{\"doi\":\"arXiv:2105.10926\",\"id\":\"b68\"},\"end\":62574,\"start\":62242},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":8550762},\"end\":62923,\"start\":62576},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":203042751},\"end\":63181,\"start\":62925},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":206769988},\"end\":63582,\"start\":63183},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":9751499},\"end\":63927,\"start\":63584},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b73\",\"matched_paper_id\":229363322},\"end\":64365,\"start\":63929},{\"attributes\":{\"doi\":\"arXiv:2103.12731\",\"id\":\"b74\"},\"end\":64794,\"start\":64367},{\"attributes\":{\"doi\":\"arXiv:1706.03762\",\"id\":\"b75\"},\"end\":65172,\"start\":64796},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":232478491},\"end\":65659,\"start\":65174},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":76666501},\"end\":66126,\"start\":65661},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":236318068},\"end\":66575,\"start\":66128},{\"attributes\":{\"doi\":\"arXiv:2107.10833\",\"id\":\"b79\"},\"end\":66921,\"start\":66577},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":52154773},\"end\":67387,\"start\":66923},{\"attributes\":{\"doi\":\"arXiv:2106.03106\",\"id\":\"b81\"},\"end\":67705,\"start\":67389},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":214775210},\"end\":68184,\"start\":67707},{\"attributes\":{\"doi\":\"arXiv:2006.03677\",\"id\":\"b83\"},\"end\":68728,\"start\":68186},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":49212467},\"end\":69112,\"start\":68730},{\"attributes\":{\"doi\":\"arXiv:2106.14881\",\"id\":\"b85\"},\"end\":69458,\"start\":69114},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":2356330},\"end\":69783,\"start\":69460},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":221377171},\"end\":70199,\"start\":69785},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":232352764},\"end\":70624,\"start\":70201},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":996788},\"end\":71022,\"start\":70626},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":1900475},\"end\":71391,\"start\":71024},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":10514149},\"end\":71735,\"start\":71393},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":2141622},\"end\":72132,\"start\":71737},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":15614128},\"end\":72503,\"start\":72134},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":49657846},\"end\":72918,\"start\":72505},{\"attributes\":{\"doi\":\"arXiv:1903.10082\",\"id\":\"b95\"},\"end\":73236,\"start\":72920},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":3619954},\"end\":73612,\"start\":73238},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":57189262},\"end\":73987,\"start\":73614},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":229924195},\"end\":74601,\"start\":73989},{\"attributes\":{\"doi\":\"arXiv:2006.16673\",\"id\":\"b99\"},\"end\":74948,\"start\":74603}]", "bib_title": "[{\"end\":35025,\"start\":34953},{\"end\":35419,\"start\":35339},{\"end\":35843,\"start\":35709},{\"end\":36843,\"start\":36798},{\"end\":37326,\"start\":37239},{\"end\":37731,\"start\":37652},{\"end\":38144,\"start\":38104},{\"end\":38691,\"start\":38590},{\"end\":39437,\"start\":39359},{\"end\":40185,\"start\":40115},{\"end\":40582,\"start\":40518},{\"end\":41023,\"start\":40941},{\"end\":41394,\"start\":41331},{\"end\":41771,\"start\":41707},{\"end\":42689,\"start\":42645},{\"end\":43031,\"start\":42969},{\"end\":43479,\"start\":43379},{\"end\":44019,\"start\":43967},{\"end\":44409,\"start\":44346},{\"end\":44809,\"start\":44748},{\"end\":45164,\"start\":45137},{\"end\":45630,\"start\":45582},{\"end\":45989,\"start\":45919},{\"end\":46404,\"start\":46326},{\"end\":46882,\"start\":46833},{\"end\":47259,\"start\":47209},{\"end\":47608,\"start\":47547},{\"end\":48034,\"start\":47974},{\"end\":48429,\"start\":48351},{\"end\":48799,\"start\":48737},{\"end\":49211,\"start\":49142},{\"end\":49670,\"start\":49606},{\"end\":50067,\"start\":50000},{\"end\":50427,\"start\":50357},{\"end\":50831,\"start\":50745},{\"end\":51356,\"start\":51286},{\"end\":51792,\"start\":51708},{\"end\":53129,\"start\":53087},{\"end\":53882,\"start\":53786},{\"end\":54403,\"start\":54310},{\"end\":54844,\"start\":54778},{\"end\":55266,\"start\":55201},{\"end\":56418,\"start\":56373},{\"end\":57557,\"start\":57484},{\"end\":58026,\"start\":57888},{\"end\":58584,\"start\":58446},{\"end\":59028,\"start\":58977},{\"end\":59461,\"start\":59408},{\"end\":59790,\"start\":59754},{\"end\":60110,\"start\":60048},{\"end\":60612,\"start\":60536},{\"end\":61742,\"start\":61636},{\"end\":62633,\"start\":62576},{\"end\":62982,\"start\":62925},{\"end\":63255,\"start\":63183},{\"end\":63654,\"start\":63584},{\"end\":64004,\"start\":63929},{\"end\":65248,\"start\":65174},{\"end\":65722,\"start\":65661},{\"end\":66190,\"start\":66128},{\"end\":66988,\"start\":66923},{\"end\":67788,\"start\":67707},{\"end\":68814,\"start\":68730},{\"end\":69513,\"start\":69460},{\"end\":69841,\"start\":69785},{\"end\":70278,\"start\":70201},{\"end\":70703,\"start\":70626},{\"end\":71078,\"start\":71024},{\"end\":71466,\"start\":71393},{\"end\":71819,\"start\":71737},{\"end\":72222,\"start\":72134},{\"end\":72579,\"start\":72505},{\"end\":73286,\"start\":73238},{\"end\":73658,\"start\":73614},{\"end\":74079,\"start\":73989}]", "bib_author": "[{\"end\":35046,\"start\":35027},{\"end\":35060,\"start\":35046},{\"end\":35434,\"start\":35421},{\"end\":35449,\"start\":35434},{\"end\":35464,\"start\":35449},{\"end\":35863,\"start\":35845},{\"end\":35876,\"start\":35863},{\"end\":36239,\"start\":36224},{\"end\":36249,\"start\":36239},{\"end\":36265,\"start\":36249},{\"end\":36281,\"start\":36265},{\"end\":36291,\"start\":36281},{\"end\":36305,\"start\":36291},{\"end\":36311,\"start\":36305},{\"end\":36562,\"start\":36548},{\"end\":36572,\"start\":36562},{\"end\":36583,\"start\":36572},{\"end\":36597,\"start\":36583},{\"end\":36861,\"start\":36845},{\"end\":36878,\"start\":36861},{\"end\":36896,\"start\":36878},{\"end\":36913,\"start\":36896},{\"end\":36933,\"start\":36913},{\"end\":36951,\"start\":36933},{\"end\":37345,\"start\":37328},{\"end\":37359,\"start\":37345},{\"end\":37372,\"start\":37359},{\"end\":37753,\"start\":37733},{\"end\":37773,\"start\":37753},{\"end\":37788,\"start\":37773},{\"end\":37804,\"start\":37788},{\"end\":38160,\"start\":38146},{\"end\":38172,\"start\":38160},{\"end\":38184,\"start\":38172},{\"end\":38194,\"start\":38184},{\"end\":38207,\"start\":38194},{\"end\":38220,\"start\":38207},{\"end\":38230,\"start\":38220},{\"end\":38243,\"start\":38230},{\"end\":38252,\"start\":38243},{\"end\":38261,\"start\":38252},{\"end\":38706,\"start\":38693},{\"end\":38719,\"start\":38706},{\"end\":39108,\"start\":39093},{\"end\":39121,\"start\":39108},{\"end\":39133,\"start\":39121},{\"end\":39145,\"start\":39133},{\"end\":39455,\"start\":39439},{\"end\":39465,\"start\":39455},{\"end\":39477,\"start\":39465},{\"end\":39488,\"start\":39477},{\"end\":39501,\"start\":39488},{\"end\":39882,\"start\":39856},{\"end\":39898,\"start\":39882},{\"end\":39912,\"start\":39898},{\"end\":40203,\"start\":40187},{\"end\":40219,\"start\":40203},{\"end\":40239,\"start\":40219},{\"end\":40257,\"start\":40239},{\"end\":40593,\"start\":40584},{\"end\":40606,\"start\":40593},{\"end\":40622,\"start\":40606},{\"end\":40635,\"start\":40622},{\"end\":40646,\"start\":40635},{\"end\":41035,\"start\":41025},{\"end\":41049,\"start\":41035},{\"end\":41057,\"start\":41049},{\"end\":41069,\"start\":41057},{\"end\":41082,\"start\":41069},{\"end\":41407,\"start\":41396},{\"end\":41419,\"start\":41407},{\"end\":41436,\"start\":41419},{\"end\":41449,\"start\":41436},{\"end\":41784,\"start\":41773},{\"end\":41801,\"start\":41784},{\"end\":41813,\"start\":41801},{\"end\":41826,\"start\":41813},{\"end\":42158,\"start\":42138},{\"end\":42171,\"start\":42158},{\"end\":42193,\"start\":42171},{\"end\":42211,\"start\":42193},{\"end\":42225,\"start\":42211},{\"end\":42245,\"start\":42225},{\"end\":42263,\"start\":42245},{\"end\":42282,\"start\":42263},{\"end\":42297,\"start\":42282},{\"end\":42312,\"start\":42297},{\"end\":42704,\"start\":42691},{\"end\":42717,\"start\":42704},{\"end\":42730,\"start\":42717},{\"end\":42751,\"start\":42730},{\"end\":43053,\"start\":43033},{\"end\":43074,\"start\":43053},{\"end\":43091,\"start\":43074},{\"end\":43108,\"start\":43091},{\"end\":43497,\"start\":43481},{\"end\":43517,\"start\":43497},{\"end\":43535,\"start\":43517},{\"end\":43859,\"start\":43845},{\"end\":44038,\"start\":44021},{\"end\":44050,\"start\":44038},{\"end\":44064,\"start\":44050},{\"end\":44423,\"start\":44411},{\"end\":44436,\"start\":44423},{\"end\":44451,\"start\":44436},{\"end\":44465,\"start\":44451},{\"end\":44480,\"start\":44465},{\"end\":44823,\"start\":44811},{\"end\":44838,\"start\":44823},{\"end\":44847,\"start\":44838},{\"end\":44861,\"start\":44847},{\"end\":44875,\"start\":44861},{\"end\":45182,\"start\":45166},{\"end\":45202,\"start\":45182},{\"end\":45215,\"start\":45202},{\"end\":45224,\"start\":45215},{\"end\":45244,\"start\":45224},{\"end\":45259,\"start\":45244},{\"end\":45276,\"start\":45259},{\"end\":45291,\"start\":45276},{\"end\":45644,\"start\":45632},{\"end\":45655,\"start\":45644},{\"end\":45663,\"start\":45655},{\"end\":46003,\"start\":45991},{\"end\":46014,\"start\":46003},{\"end\":46028,\"start\":46014},{\"end\":46043,\"start\":46028},{\"end\":46416,\"start\":46406},{\"end\":46427,\"start\":46416},{\"end\":46442,\"start\":46427},{\"end\":46451,\"start\":46442},{\"end\":46465,\"start\":46451},{\"end\":46479,\"start\":46465},{\"end\":46489,\"start\":46479},{\"end\":46502,\"start\":46489},{\"end\":46900,\"start\":46884},{\"end\":46923,\"start\":46900},{\"end\":46940,\"start\":46923},{\"end\":47273,\"start\":47261},{\"end\":47283,\"start\":47273},{\"end\":47296,\"start\":47283},{\"end\":47622,\"start\":47610},{\"end\":47632,\"start\":47622},{\"end\":47646,\"start\":47632},{\"end\":47656,\"start\":47646},{\"end\":47671,\"start\":47656},{\"end\":47683,\"start\":47671},{\"end\":48051,\"start\":48036},{\"end\":48067,\"start\":48051},{\"end\":48083,\"start\":48067},{\"end\":48442,\"start\":48431},{\"end\":48453,\"start\":48442},{\"end\":48466,\"start\":48453},{\"end\":48479,\"start\":48466},{\"end\":48816,\"start\":48801},{\"end\":48824,\"start\":48816},{\"end\":48836,\"start\":48824},{\"end\":48850,\"start\":48836},{\"end\":48865,\"start\":48850},{\"end\":48874,\"start\":48865},{\"end\":49227,\"start\":49213},{\"end\":49236,\"start\":49227},{\"end\":49246,\"start\":49236},{\"end\":49261,\"start\":49246},{\"end\":49271,\"start\":49261},{\"end\":49285,\"start\":49271},{\"end\":49682,\"start\":49672},{\"end\":49695,\"start\":49682},{\"end\":49710,\"start\":49695},{\"end\":49721,\"start\":49710},{\"end\":50085,\"start\":50069},{\"end\":50102,\"start\":50085},{\"end\":50114,\"start\":50102},{\"end\":50440,\"start\":50429},{\"end\":50455,\"start\":50440},{\"end\":50470,\"start\":50455},{\"end\":50846,\"start\":50833},{\"end\":50861,\"start\":50846},{\"end\":50874,\"start\":50861},{\"end\":50890,\"start\":50874},{\"end\":50906,\"start\":50890},{\"end\":50921,\"start\":50906},{\"end\":50933,\"start\":50921},{\"end\":51373,\"start\":51358},{\"end\":51388,\"start\":51373},{\"end\":51404,\"start\":51388},{\"end\":51421,\"start\":51404},{\"end\":51811,\"start\":51794},{\"end\":51824,\"start\":51811},{\"end\":51839,\"start\":51824},{\"end\":51855,\"start\":51839},{\"end\":51874,\"start\":51855},{\"end\":51892,\"start\":51874},{\"end\":51907,\"start\":51892},{\"end\":51923,\"start\":51907},{\"end\":51938,\"start\":51923},{\"end\":51950,\"start\":51938},{\"end\":52470,\"start\":52460},{\"end\":52480,\"start\":52470},{\"end\":52487,\"start\":52480},{\"end\":52503,\"start\":52487},{\"end\":52515,\"start\":52503},{\"end\":52526,\"start\":52515},{\"end\":52841,\"start\":52831},{\"end\":52852,\"start\":52841},{\"end\":52866,\"start\":52852},{\"end\":52880,\"start\":52866},{\"end\":52894,\"start\":52880},{\"end\":53140,\"start\":53131},{\"end\":53154,\"start\":53140},{\"end\":53165,\"start\":53154},{\"end\":53179,\"start\":53165},{\"end\":53194,\"start\":53179},{\"end\":53202,\"start\":53194},{\"end\":53553,\"start\":53537},{\"end\":53564,\"start\":53553},{\"end\":53572,\"start\":53564},{\"end\":53581,\"start\":53572},{\"end\":53592,\"start\":53581},{\"end\":53899,\"start\":53884},{\"end\":53916,\"start\":53899},{\"end\":53927,\"start\":53916},{\"end\":53945,\"start\":53927},{\"end\":53959,\"start\":53945},{\"end\":53973,\"start\":53959},{\"end\":54420,\"start\":54405},{\"end\":54432,\"start\":54420},{\"end\":54443,\"start\":54432},{\"end\":54457,\"start\":54443},{\"end\":54471,\"start\":54457},{\"end\":54861,\"start\":54846},{\"end\":54872,\"start\":54861},{\"end\":54884,\"start\":54872},{\"end\":54898,\"start\":54884},{\"end\":54912,\"start\":54898},{\"end\":55277,\"start\":55268},{\"end\":55291,\"start\":55277},{\"end\":55303,\"start\":55291},{\"end\":55317,\"start\":55303},{\"end\":55332,\"start\":55317},{\"end\":55706,\"start\":55696},{\"end\":55717,\"start\":55706},{\"end\":55729,\"start\":55717},{\"end\":55746,\"start\":55729},{\"end\":55762,\"start\":55746},{\"end\":56065,\"start\":56057},{\"end\":56079,\"start\":56065},{\"end\":56094,\"start\":56079},{\"end\":56108,\"start\":56094},{\"end\":56118,\"start\":56108},{\"end\":56131,\"start\":56118},{\"end\":56150,\"start\":56131},{\"end\":56432,\"start\":56420},{\"end\":56447,\"start\":56432},{\"end\":56458,\"start\":56447},{\"end\":56469,\"start\":56458},{\"end\":56483,\"start\":56469},{\"end\":56779,\"start\":56770},{\"end\":56791,\"start\":56779},{\"end\":56799,\"start\":56791},{\"end\":56809,\"start\":56799},{\"end\":56825,\"start\":56809},{\"end\":56839,\"start\":56825},{\"end\":57157,\"start\":57149},{\"end\":57169,\"start\":57157},{\"end\":57178,\"start\":57169},{\"end\":57186,\"start\":57178},{\"end\":57198,\"start\":57186},{\"end\":57211,\"start\":57198},{\"end\":57224,\"start\":57211},{\"end\":57237,\"start\":57224},{\"end\":57573,\"start\":57559},{\"end\":57583,\"start\":57573},{\"end\":57596,\"start\":57583},{\"end\":57607,\"start\":57596},{\"end\":57618,\"start\":57607},{\"end\":57626,\"start\":57618},{\"end\":58042,\"start\":58028},{\"end\":58060,\"start\":58042},{\"end\":58071,\"start\":58060},{\"end\":58087,\"start\":58071},{\"end\":58600,\"start\":58586},{\"end\":58618,\"start\":58600},{\"end\":58629,\"start\":58618},{\"end\":58645,\"start\":58629},{\"end\":59045,\"start\":59030},{\"end\":59055,\"start\":59045},{\"end\":59069,\"start\":59055},{\"end\":59085,\"start\":59069},{\"end\":59097,\"start\":59085},{\"end\":59117,\"start\":59097},{\"end\":59134,\"start\":59117},{\"end\":59474,\"start\":59463},{\"end\":59486,\"start\":59474},{\"end\":59499,\"start\":59486},{\"end\":59808,\"start\":59792},{\"end\":59822,\"start\":59808},{\"end\":60121,\"start\":60112},{\"end\":60133,\"start\":60121},{\"end\":60144,\"start\":60133},{\"end\":60159,\"start\":60144},{\"end\":60174,\"start\":60159},{\"end\":60188,\"start\":60174},{\"end\":60202,\"start\":60188},{\"end\":60216,\"start\":60202},{\"end\":60230,\"start\":60216},{\"end\":60625,\"start\":60614},{\"end\":60635,\"start\":60625},{\"end\":60648,\"start\":60635},{\"end\":60660,\"start\":60648},{\"end\":60670,\"start\":60660},{\"end\":60681,\"start\":60670},{\"end\":60905,\"start\":60891},{\"end\":60918,\"start\":60905},{\"end\":61161,\"start\":61140},{\"end\":61174,\"start\":61161},{\"end\":61190,\"start\":61174},{\"end\":61203,\"start\":61190},{\"end\":61220,\"start\":61203},{\"end\":61237,\"start\":61220},{\"end\":61510,\"start\":61506},{\"end\":61518,\"start\":61510},{\"end\":61756,\"start\":61744},{\"end\":61772,\"start\":61756},{\"end\":61787,\"start\":61772},{\"end\":61802,\"start\":61787},{\"end\":61812,\"start\":61802},{\"end\":61824,\"start\":61812},{\"end\":61839,\"start\":61824},{\"end\":61855,\"start\":61839},{\"end\":61861,\"start\":61855},{\"end\":62297,\"start\":62285},{\"end\":62306,\"start\":62297},{\"end\":62321,\"start\":62306},{\"end\":62347,\"start\":62321},{\"end\":62360,\"start\":62347},{\"end\":62370,\"start\":62360},{\"end\":62645,\"start\":62635},{\"end\":62656,\"start\":62645},{\"end\":62670,\"start\":62656},{\"end\":62682,\"start\":62670},{\"end\":62998,\"start\":62984},{\"end\":63007,\"start\":62998},{\"end\":63021,\"start\":63007},{\"end\":63271,\"start\":63257},{\"end\":63288,\"start\":63271},{\"end\":63302,\"start\":63288},{\"end\":63670,\"start\":63656},{\"end\":63687,\"start\":63670},{\"end\":63701,\"start\":63687},{\"end\":64020,\"start\":64006},{\"end\":64035,\"start\":64020},{\"end\":64051,\"start\":64035},{\"end\":64068,\"start\":64051},{\"end\":64454,\"start\":64438},{\"end\":64475,\"start\":64454},{\"end\":64493,\"start\":64475},{\"end\":64506,\"start\":64493},{\"end\":64522,\"start\":64506},{\"end\":64539,\"start\":64522},{\"end\":64812,\"start\":64796},{\"end\":64826,\"start\":64812},{\"end\":64839,\"start\":64826},{\"end\":64856,\"start\":64839},{\"end\":64869,\"start\":64856},{\"end\":64884,\"start\":64869},{\"end\":64899,\"start\":64884},{\"end\":64917,\"start\":64899},{\"end\":65266,\"start\":65250},{\"end\":65281,\"start\":65266},{\"end\":65294,\"start\":65281},{\"end\":65305,\"start\":65294},{\"end\":65319,\"start\":65305},{\"end\":65327,\"start\":65319},{\"end\":65338,\"start\":65327},{\"end\":65740,\"start\":65724},{\"end\":65755,\"start\":65740},{\"end\":65770,\"start\":65755},{\"end\":65783,\"start\":65770},{\"end\":65797,\"start\":65783},{\"end\":65805,\"start\":65797},{\"end\":65816,\"start\":65805},{\"end\":66208,\"start\":66192},{\"end\":66223,\"start\":66208},{\"end\":66236,\"start\":66223},{\"end\":66250,\"start\":66236},{\"end\":66258,\"start\":66250},{\"end\":66269,\"start\":66258},{\"end\":66672,\"start\":66659},{\"end\":66686,\"start\":66672},{\"end\":66697,\"start\":66686},{\"end\":66708,\"start\":66697},{\"end\":67003,\"start\":66990},{\"end\":67010,\"start\":67003},{\"end\":67023,\"start\":67010},{\"end\":67034,\"start\":67023},{\"end\":67045,\"start\":67034},{\"end\":67056,\"start\":67045},{\"end\":67065,\"start\":67056},{\"end\":67082,\"start\":67065},{\"end\":67467,\"start\":67452},{\"end\":67481,\"start\":67467},{\"end\":67494,\"start\":67481},{\"end\":67510,\"start\":67494},{\"end\":67803,\"start\":67790},{\"end\":67815,\"start\":67803},{\"end\":67825,\"start\":67815},{\"end\":67839,\"start\":67825},{\"end\":67852,\"start\":67839},{\"end\":67866,\"start\":67852},{\"end\":68287,\"start\":68276},{\"end\":68300,\"start\":68287},{\"end\":68315,\"start\":68300},{\"end\":68326,\"start\":68315},{\"end\":68341,\"start\":68326},{\"end\":68355,\"start\":68341},{\"end\":68375,\"start\":68355},{\"end\":68392,\"start\":68375},{\"end\":68406,\"start\":68392},{\"end\":68419,\"start\":68406},{\"end\":68828,\"start\":68816},{\"end\":68846,\"start\":68828},{\"end\":69125,\"start\":69114},{\"end\":69139,\"start\":69125},{\"end\":69152,\"start\":69139},{\"end\":69168,\"start\":69152},{\"end\":69528,\"start\":69515},{\"end\":69542,\"start\":69528},{\"end\":69557,\"start\":69542},{\"end\":69854,\"start\":69843},{\"end\":69864,\"start\":69854},{\"end\":69878,\"start\":69864},{\"end\":69889,\"start\":69878},{\"end\":69903,\"start\":69889},{\"end\":69917,\"start\":69903},{\"end\":70291,\"start\":70280},{\"end\":70306,\"start\":70291},{\"end\":70320,\"start\":70306},{\"end\":70334,\"start\":70320},{\"end\":70716,\"start\":70705},{\"end\":70730,\"start\":70716},{\"end\":70743,\"start\":70730},{\"end\":70754,\"start\":70743},{\"end\":70765,\"start\":70754},{\"end\":71091,\"start\":71080},{\"end\":71105,\"start\":71091},{\"end\":71117,\"start\":71105},{\"end\":71128,\"start\":71117},{\"end\":71479,\"start\":71468},{\"end\":71493,\"start\":71479},{\"end\":71504,\"start\":71493},{\"end\":71832,\"start\":71821},{\"end\":71846,\"start\":71832},{\"end\":71857,\"start\":71846},{\"end\":72235,\"start\":72224},{\"end\":72247,\"start\":72235},{\"end\":72262,\"start\":72247},{\"end\":72270,\"start\":72262},{\"end\":72594,\"start\":72581},{\"end\":72606,\"start\":72594},{\"end\":72614,\"start\":72606},{\"end\":72627,\"start\":72614},{\"end\":72641,\"start\":72627},{\"end\":72649,\"start\":72641},{\"end\":72994,\"start\":72981},{\"end\":73006,\"start\":72994},{\"end\":73014,\"start\":73006},{\"end\":73028,\"start\":73014},{\"end\":73036,\"start\":73028},{\"end\":73301,\"start\":73288},{\"end\":73314,\"start\":73301},{\"end\":73323,\"start\":73314},{\"end\":73337,\"start\":73323},{\"end\":73345,\"start\":73337},{\"end\":73673,\"start\":73660},{\"end\":73686,\"start\":73673},{\"end\":73695,\"start\":73686},{\"end\":73709,\"start\":73695},{\"end\":73717,\"start\":73709},{\"end\":74095,\"start\":74081},{\"end\":74107,\"start\":74095},{\"end\":74124,\"start\":74107},{\"end\":74137,\"start\":74124},{\"end\":74148,\"start\":74137},{\"end\":74161,\"start\":74148},{\"end\":74172,\"start\":74161},{\"end\":74187,\"start\":74172},{\"end\":74198,\"start\":74187},{\"end\":74210,\"start\":74198},{\"end\":74216,\"start\":74210},{\"end\":74689,\"start\":74673},{\"end\":74703,\"start\":74689},{\"end\":74717,\"start\":74703},{\"end\":74734,\"start\":74717}]", "bib_venue": "[{\"end\":35128,\"start\":35060},{\"end\":35502,\"start\":35464},{\"end\":35909,\"start\":35876},{\"end\":36222,\"start\":36154},{\"end\":36647,\"start\":36613},{\"end\":36989,\"start\":36951},{\"end\":37426,\"start\":37372},{\"end\":37848,\"start\":37804},{\"end\":38319,\"start\":38261},{\"end\":38781,\"start\":38719},{\"end\":39091,\"start\":39009},{\"end\":39548,\"start\":39501},{\"end\":39854,\"start\":39787},{\"end\":40294,\"start\":40257},{\"end\":40704,\"start\":40646},{\"end\":41119,\"start\":41082},{\"end\":41497,\"start\":41449},{\"end\":41864,\"start\":41826},{\"end\":42136,\"start\":42062},{\"end\":42789,\"start\":42751},{\"end\":43152,\"start\":43108},{\"end\":43572,\"start\":43535},{\"end\":43843,\"start\":43806},{\"end\":44136,\"start\":44064},{\"end\":44537,\"start\":44480},{\"end\":44923,\"start\":44875},{\"end\":45340,\"start\":45291},{\"end\":45729,\"start\":45663},{\"end\":46101,\"start\":46043},{\"end\":46560,\"start\":46502},{\"end\":46998,\"start\":46940},{\"end\":47358,\"start\":47296},{\"end\":47741,\"start\":47683},{\"end\":48141,\"start\":48083},{\"end\":48521,\"start\":48479},{\"end\":48912,\"start\":48874},{\"end\":49353,\"start\":49285},{\"end\":49779,\"start\":49721},{\"end\":50152,\"start\":50114},{\"end\":50528,\"start\":50470},{\"end\":50995,\"start\":50933},{\"end\":51479,\"start\":51421},{\"end\":52008,\"start\":51950},{\"end\":52458,\"start\":52354},{\"end\":52829,\"start\":52779},{\"end\":53260,\"start\":53202},{\"end\":53535,\"start\":53474},{\"end\":54035,\"start\":53973},{\"end\":54533,\"start\":54471},{\"end\":54970,\"start\":54912},{\"end\":55400,\"start\":55332},{\"end\":55694,\"start\":55645},{\"end\":56055,\"start\":55961},{\"end\":56551,\"start\":56483},{\"end\":56899,\"start\":56855},{\"end\":57147,\"start\":57076},{\"end\":57664,\"start\":57626},{\"end\":58149,\"start\":58087},{\"end\":58693,\"start\":58645},{\"end\":59167,\"start\":59134},{\"end\":59557,\"start\":59499},{\"end\":59884,\"start\":59822},{\"end\":60268,\"start\":60230},{\"end\":60695,\"start\":60681},{\"end\":60967,\"start\":60934},{\"end\":61138,\"start\":61096},{\"end\":61504,\"start\":61456},{\"end\":61919,\"start\":61861},{\"end\":62283,\"start\":62242},{\"end\":62730,\"start\":62682},{\"end\":63036,\"start\":63021},{\"end\":63364,\"start\":63302},{\"end\":63736,\"start\":63701},{\"end\":64123,\"start\":64084},{\"end\":64436,\"start\":64367},{\"end\":64958,\"start\":64933},{\"end\":65396,\"start\":65338},{\"end\":65874,\"start\":65816},{\"end\":66331,\"start\":66269},{\"end\":66657,\"start\":66577},{\"end\":67130,\"start\":67082},{\"end\":67450,\"start\":67389},{\"end\":67924,\"start\":67866},{\"end\":68274,\"start\":68186},{\"end\":68903,\"start\":68846},{\"end\":69264,\"start\":69184},{\"end\":69604,\"start\":69557},{\"end\":69979,\"start\":69917},{\"end\":70396,\"start\":70334},{\"end\":70802,\"start\":70765},{\"end\":71186,\"start\":71128},{\"end\":71541,\"start\":71504},{\"end\":71915,\"start\":71857},{\"end\":72299,\"start\":72270},{\"end\":72687,\"start\":72649},{\"end\":72979,\"start\":72920},{\"end\":73403,\"start\":73345},{\"end\":73779,\"start\":73717},{\"end\":74274,\"start\":74216},{\"end\":74671,\"start\":74603}]"}}}, "year": 2023, "month": 12, "day": 17}