{"id": 258865265, "updated": "2023-10-12 11:18:29.044", "metadata": {"title": "Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation", "authors": "[{\"first\":\"Haonan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Fajri\",\"last\":\"Koto\",\"middle\":[]},{\"first\":\"Minghao\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Alham\",\"last\":\"Aji\",\"middle\":[\"Fikri\"]},{\"first\":\"Timothy\",\"last\":\"Baldwin\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Instruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation (LoRA), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plug-ins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from LoRA-based training over Bactrian-X outperform both the vanilla models and existing instruction-tuned models. The code and models are publicly available at https://github.com/mbzuai-nlp/bactrian-x", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.15011", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-15011", "doi": "10.48550/arxiv.2305.15011"}}, "content": {"source": {"pdf_hash": "647f520eea08cf8e98b99319dcb7e32cc3a78c72", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.15011v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b2c418af0a34bed92554f1e77e1299e0be620cb7", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/647f520eea08cf8e98b99319dcb7e32cc3a78c72.txt", "contents": "\nBactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation\n\n\nHaonan Li haonan.li@mbzuai.ac.ae \nNatural Language Processing Department\nMBZUAI\n\n\nFajri Koto fajri.koto@mbzuai.ac.ae \nNatural Language Processing Department\nMBZUAI\n\n\nMinghao Wu minghao.wu@mbzuai.ac.ae \nNatural Language Processing Department\nMBZUAI\n\n\nMonash University\n\n\nAlham Fikri Aji \nNatural Language Processing Department\nMBZUAI\n\n\nTimothy Baldwin timothy.baldwin@mbzuai.ac.ae \nNatural Language Processing Department\nMBZUAI\n\n\nThe University of Melbourne\n\n\nBactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation\n\nInstruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation (LoRA), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plugins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from LoRA-based training over Bactrian-X outperform both the vanilla models and existing instruction-tuned models. The code and models are publicly available at https: /\n\nIntroduction\n\nFine-tuning large language models (LLMs) with instruction-response pair datasets has demonstrated remarkable zero-shot generalization capabilities for open-source and closed-source models (Sanh et al., 2022;Wei et al., 2022;Ouyang et al., 2022;OpenAI, 2023). Although the LLMs are often pre-trained using multilingual texts, the instruction-tuning for open-source models is restricted to English (Taori et al., 2023;Chiang et al., 2023;Wu et al., 2023), bringing into question its multilingual generalizability. Closed-resource models such as OpenAI GPT-4 (OpenAI, 2023) and Google BARD, 1 despite performing impressively over high-resource languages, are still lacking in terms of multilingual generalizability under monolingual instruction tuning.\n\nThe scarcity of instruction-response pair datasets in languages beyond English is hinders multilingual instruction tuning. The existing xP3 dataset (Muennighoff et al., 2022), which was used to fine-tune BLOOM and mT5, employs English instructions. Although Muennighoff et al. (2022) also experiments with xP3mt -machine-translated instructions -it focuses on classic NLP tasks such as summarization and question answering, rather than general instructions. Additionally, both xP3 and xP3mt use template-based prompts, and hence lack variation.\n\nTo investigate general instruction tuning in a multilingual setting, we introduce Bactrian-X, containing parallel instruction-response pairs across 52 languages that were automatically constructed by translating instructions from Alpaca (Taori et al., 2023) and Dolly (Conover et al., 2023) via the Google Translate API. 2 As we detail in Section 3, we use the output distillation trick to obtain corresponding responses by leveraging ChatGPT outputs, conditioned on the translated instructions. With 67K instruction-response pairs for each language, the total number of instances in Bactrian-X reaches 3.4M.\n\nIn contrast to previous multilingual instruction models such as BLOOMZ (Muennighoff et al., 2022) which are subject to full fine-tuning via parameter updates across all layers, this study highlights the potential of parameter-efficient finetuning techniques, specifically LoRA (Hu et al., 2022). LoRA uses adapters with substantially fewer parameters than base LLMs, making them more practical and adaptable for real-world applications. Specifically, in this work, we introduce BX BLOOM and BX LLaMA models, which build upon the BLOOM (Scao et al., 2022) and LLaMA (Touvron et al., 2023) models, and find them to be better than the associated instruction-tuned models: BLOOMZ (Muennighoff et al., 2022) and Alpaca (Taori et al., 2023).\n\nWe conduct a comprehensive series of experiments covering a range of zero-shot multilingual NLP tasks, including XCOPA (Ponti et al., 2020), XStoryCloze (Lin et al., 2022), XWinograd (Muennighoff et al., 2022), our own multilingual sentiment analysis dataset SentimentX, and EXAMS (Hardalov et al., 2020). The consistently high results across these tasks highlight the effectiveness of our multilingual instruction dataset and adapter technique for instruction tuning in languages beyond English. To further validate our findings, we use GPT-4 as an evaluator based on the methodology proposed by Chiang et al. (2023), and additionally conduct human evaluation with native speakers. All results confirm that our proposed models outperform the vanilla foundation models and existing instruction-tuned models.\n\n\nRelated Work\n\nMultilingual Instruction Tuning LLMs such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and LLaMA (Touvron et al., 2023) (Hoffmann et al., 2022Scao et al., 2022;Zeng et al., 2023) have revolutionized NLP. Research has demonstrated that fine-tuning LLMs with instruction prompts can improve their capacity to perform unseen/novel tasks (Wei et al., 2022;Sanh et al., 2022;Ouyang et al., 2022;Chung et al., 2022;Muennighoff et al., 2022). Recently, Wang et al. (2022); Taori et al. (2023) showed that machinegenerated instructions can be used for instruction tuning. Wu et al. (2023) created a large-scale dataset with 2.6M instructions, and demonstrated that relatively small language models also benefit from the instructions. Prior work has predominantly been on English, and instruction-tuning in languages beyond English remains limited. The closest work to ours is BLOOMZ (Muennighoff et al., 2022), which finetunes BLOOM (Scao et al., 2022) andmT5 (Xue et al., 2021) on the xP3 and xP3mt multilingual instruction datasets. However, both xP3 and xP3mt are based on human-written templates, and lack the variability of an organic multilingual dataset. Our work, instead, constructs a parallel general instruction dataset by translating English instructions into 51 languages and generating responses via ChatGPT (Ouyang et al., 2022). To the best of our knowledge, our Bactrian-X instruction dataset is the largest general-purpose multilingual instruction dataset to date.\n\nParameter Efficient Fine-Tuning (PEFT) Finetuning all parameters of an LLM (e.g. Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023) and LaMini-LM (Wu et al., 2023)) is computationally expensive, and adapters (Houlsby et al., 2019) offer a more cost-effective alternative. PEFT updates a small number of parameters during fine-tuning, and achieves comparable performance to fully finetuned counterparts (Houlsby et al., 2019;Guo et al., 2021;Lester et al., 2021;Ben Zaken et al., 2022). Hu et al. (2022) introduced Low-Rank Adaptation (LoRA), which incorporates trainable rank decomposition matrices into transformer layers (Vaswani et al., 2017) during fine-tuning without introducing additional latency during inference. They demonstrate that by fine-tuning with less than 1% of the model parameters, LoRA outperforms several fully fine-tuned LLMs, including GPT-3 (Brown et al., 2020), on various tasks.\n\nIn recent work, Taori et al. (2023) use the LoRA trick to fine-tune LLaMA (Touvron et al., 2023), resulting in the Alpaca model, but did not carry out comprehensive evaluation. In this work, we also leverage the LoRA technique to develop a range of monolingual and multilingual adapters, with a much larger instruction-response dataset, across 52 languages. We provide empirical analysis based on automatic and human evaluation to demonstrate the effectiveness of our method.\n\n\nBactrian-X Dataset\n\nIn this section, we detail the dataset creation process and provide an overview of the resulting data, focusing on the quality of translated instructions and generated responses. \n\n\nDataset Creation\n\nWe construct the Bactrian-X dataset in two steps: instruction translation, and response generation (see Figure 1).\n\n\nInstruction Translation\n\nWe use English instructions developed for Alpaca (52K) and Dolly (15K), and use the Google Translate API to translate them into 51 different languages, based on the languages used for mBART-50 (Tang et al., 2020). The Alpaca instructions were automatically generated by GPT-3.5 (Ouyang et al., 2022) via the self-instruct technique (Wang et al., 2022), while the Dolly dataset was manually curated by thousands of Databricks company employees. Prior to the translation process, we identify instructions containing programming-related content based on a keywordmatching method and exclude them from the translation process. The total cost for translating the instructions was approximately USD$10,000.\n\nResponse Generation For each translated instruction, we use ChatGPT (gpt-3.5-turbo) to obtain a response. 3 For English, we pair the instruction with the original response. Translating responses into the 51 languages is costly. Moreover, potential issues such as \"translationese\" and nonnative answer styles may arise from relying solely on translated responses. The total cost for generating responses amounts to around $3,000 USD. We leave the comparison between the translated responses and the ChatGPT-generated responses to future work.\n\n\nExploratory Data Analysis\n\nDataset Statistics We analyzed the tokenized texts in the 52 languages using the mBART-50, LLaMA, and BLOOM tokenizers, and present the statistics in  on all 52 languages, the tokenizer is trained on all the languages, and the average number of tokens is thus relatively smaller than LLaMA and BLOOM. However, for languages unseen by BLOOM and LLaMA, the tokenized texts are 2 to 3 times longer compared to mBART-50. This suggests that for these unseen languages, both BLOOM and LLaMA models require a larger sequence length for semantically similar input texts, posing a challenge for effective adaptation with the LoRA adapter.\n\n\nInstruction Quality\n\nTo test the quality of the translated instructions, we verified the quality of 100 randomly-sampled instances for each language by performing back-translation into English using the Google Translate API. We evaluate the quality of the back-translated instructions relative to the originals based on BLEU (Papineni et al., 2002;Post, 2018), 4 chrF++ (Popovi\u0107, 2017), 5 and the trained metric COMET (Rei et al., 2020). 6 The worst BLEU score of 28 is for Mongolian-English translation, but as seen in Table 2, most language pairs achieved BLEU scores above 40, indicating high quality and reliability of the Bactrian-X instructions.  \n\n\nResponse Quality\n\nTo evaluate response quality, we conducted human evaluations in three highresource languages -Arabic (ar), Indonesian (id), Chinese (zh) -and three low-resource languages -Burmese (my), Tamil (ta), and Tagalog (tl). For each language, two native-speaker annotators are asked to assess the fluency and informativeness of the responses given the question, except Tagalog, which had only one annotator. The quality assessment guideline is provided in Appendix A, and the results are shown in Figure 2, with an interannotator agreement (IAA) averaged by language of 0.70 and 0.69 for fluency and informativeness, respectively. The results showed that high-resource languages consistently achieved over 80% satisfactory ratings (A and B), while some low-resource languages like Tamil and Burmese had a significant proportion of lower ratings (C and D). This suggests that the outputs generated by ChatGPT are lacking for some low-resource languages. We leave the improvement of data quality for low-resource languages to future work.\n\n\nBactrain-X Models\n\nGiven limitations of computation resources, we use base LLMs with 7B and 13B parameters only. First, we trained three multilingual Bactrian models (BX) over the parallel dataset in 52 languages: BX LLaMA (7B, 13B), and BX BLOOM (7B). 7 While our primary results are based on the BX models, we additionally train some 7B monolingual Bactrian models (BM) for analysis in Section 5: 14 BM LLaMA and 18 BM BLOOM . All models will be made publicly available in our model repository. We train our LoRA adapters (Hu et al., 2022) using PyTorch with the HuggingFace PEFT implementation (Mangrulkar et al., 2022;Wolf et al., 2020). Hyperparameters used for training the different models can be found in Appendix C (Table 7). In our evaluation, we compare each multilingual BX model with: (1) the corresponding vanilla models, and (2) the instruction-tuned models Alpaca (Taori et al., 2023) and BLOOMZ (Muennighoff et al., 2022). Details of these models are provided in Appendix B.\n\n\nEvaluation on NLP Benchmarks\n\nIn order to thoroughly evaluate our Bactrian-X models, we conducted experiments on various multilingual downstream NLP tasks. We first introduce the benchmark datasets we used, and then present the evaluation results in two categories: language understanding tasks (Section 5.2) and knowledgeintensive tasks (Section 5.3).\n\n\nDatasets\n\nTo probe the zero-shot language understanding capability of the different models, we evaluate on the following test sets:\n\n\u2022 XCOPA (Ponti et al., 2020): a multilingual resource designed for causal commonsense reasoning, encompassing 11 languages. The task involves predicting the correct next sentence from two options based on cause and effect question types.  Table 3: Zero-shot experiment results on downstream tasks. We report averaged accuracy for XCOPA, XStoryCloze, XWinograd, and EXAMS, and macro-F1 scores for SentimentX.\n\nin six languages. 8 The task involves selecting the most plausible sentence from options that differ slightly. . We also measure how much knowledge the model encodes using the EXAMS benchmark:\n\n\u2022 EXAMS (Hardalov et al., 2020): a multilingual question-answering dataset made up of multiple-choice questions from high school examinations in 16 languages. It covers subjects from natural science (e.g., physics), social science (e.g., history), to humanities (e.g., philosophy). Given that all our experiments are zero-shot, we merge the train, validation, and test sets into a single evaluation dataset, and exclude questions without four multiple choice options, resulting in a total of 20,559 questions.\n\n\nLanguage Understanding Tasks\n\nThe average performance across all languages for XCOPA, XStoryCloze, XWinograd, and SentimentX is presented in Table 3. During inference, we use translated prompts and sentiment labels in the respective languages, obtained 8 https://cs.nyu.edu/~davise/papers/ WinogradSchemas/WS.html 9 http://tass.sepln.org/2020/ 10 https://github.com/antongolubev5/ Russian-Sentiment-Analysis-Evaluation-Datasets from the Google Translate API. We observe that integrating LoRA with the base models of LLaMA and BLOOM, and training over the multilingual instruction datasets, consistently improves performance over the base models. Improvements can also be observed over existing instruction-tuned models such as Alpaca-LoRA, on most tasks. For the larger models, we observe further enhancements again, as seen for BX LLaMA (13B) over LLaMA (13B). From the third block, we observe that BX BLOOM performs better than the full fine-tuned BLOOMZ model on three out of five tasks. Although the performance difference is relatively small, it is worth noting that BX BLOOM is fine-tuned only using the LoRA adapter on a smaller multilingual dataset (2.5M samples), whereas BLOOMZ is fully finetuned using a larger dataset of 78M samples. Additionally, BLOOMZ is fine-tuned on xP3, which is designed to handle NLP downstream tasks, while Bactrian-X is more general purpose.\n\nPerformance on Unseen Languages In Figure 3, we present the average performance of the 7B mod-   Table 5: Performance breakdown by subject type in EXAMS. \"Natural\" and \"Social\" denote natural science and social science, respectively. els over languages that the base models were not exposed to in pre-training. For XCOPA, XStoryCloze, XWinograd, and SentimentX, the LLaMA model is not exposed to 10, 8, 2, and 5 languages, resp., while the BLOOM model is not exposed to 7, 2, 2, and 4 languages, respectively. We observe that our proposed models improve on the zero-shot performance of the base models across all tasks, and also surpass the performance of existing instructiontuned models, with the exception of BLOOM over XStoryCloze. A notable improvement can be seen in the SentimentX dataset, implying that our models are more suited to non-English instructions and non-English sentiment labels.\n\nMonolingual vs. Multilingual Fine-tuning For each of the 52 languages in Section 3.2, we compared the performance of monolingual BM models against the multilingual BX models. To ensure a fair benchmark, we exclude unseen languages in calculating the average score. Suppose the user only speaks the language of the question, please evaluate both answers with your justification having less three sentences, and provide a score ranging from 0 to 10 after your justifications. When evaluating the answers, you should consider the helpfulness, relevance, accuracy, level of details of the answers.\n\nThe score for answer 1 should be wrapped by <score1> and </score1>, and the score for answer 2 should be wrapped by <score2> and </score2>. \n\n\nKnowledge-intensive Task\n\nThe last column of Table 3 shows the results on EXAMS, averaged across languages. We find that the BX LLaMA models (7B and 13B) outperform their corresponding base models, while BLOOMZ outperforms our BX BLOOM . We observe that multilingual instruction tuning seems to be more promising on larger models, as seen in BX LLaMA (13B) improving substantially over LLaMA by 5.5% on average, while the margin for BX LLaMA (7B) is only 0.9%. It is noteworthy that BX LLaMA (13B) also outperforms LLaMA (30B) on the EXAMS benchmark in Table 12 in Appendix D, underlining the effectiveness of multilingual instruction tuning.\n\nThe EXAMS dataset comprises a range of subject areas, such as natural science and social science. We present a breakdown of the results across subject areas for the 13B models in Table 5. It is evident that there are substantial performance improvements over the social sciences and other subject areas during fine-tuning, but comparatively lesser gains for natural science. This could be attributed to our dataset containing fewer instructions and questions related to natural sciences, or the inherent difficulty of learning natural science concepts or reasoning abilities through instruction fine-tuning. 6 Evaluation on Open-ended Questions  (2023), we provide two answers from different models in a single prompt, and ask GPT-4 to rate the answers over a scale of 0 to 10 from various aspects including helpfulness, relevance, accuracy, and the level of detail (see Figure 4 for an example prompt for GPT-4 evaluation). To ensure fairness, we interchange the order of the provided answers, and assign scores twice for each question. We exclude vanilla BLOOM and LLaMA from openended question evaluation, and instead compare BX BLOOM against BLOOMZ, BX LLaMA against Alpaca, and BX BLOOM against BX LLaMA , given the superiority of instruction-tuned models in previous studies (Chiang et al., 2023;Muennighoff et al., 2022). We select 5 questions from each category, resulting in 40 questions per language. Given cost restrictions and availability of human annotators, we conducted GPT-4 evaluation over 12 languages and human evaluation over 6 languages. Figure 5 shows the results of the three model pairs, clearly indicate that GPT-4 has a preference for BX LLaMA over Alpaca and similarly favors BX BLOOM over BLOOMZ. Regarding the comparison between the two BX models, BX LLaMA per-  Figure 6: GPT-4 evaluation by language. We categorize languages into four groups based on whether a language is seen during model pre-training, and select 3 languages from each group. Group 1: languages seen by both BLOOM and LLaMA; group 2: seen by BLOOM only; group 3: seen by LLaMA only; group 4: not seen by either BLOOM or LLaMA. forms better overall. Since GPT-4 assigns a quantitative score to each response on a scale of 0-10, we calculate the average score for each model from all comparison pairs and present a breakdown of results separately for each language group (see Figure 6) and question type (see Figure 7).\n\n\nGPT-4 Evaluation\n\nLanguage Group Analyzing the results based by language group (see Figure 6), we can make several observations. First, multilingual pre-training plays a critical role for multilingual instructionfollowing models. In groups 1 and 3, BX LLaMA outperforms BX BLOOM , while in group 2, BX BLOOM performs substantially better. This difference can be attributed to variations in language coverage during pre-training, as both models are fine-tuned on the same dataset. Second, multilingual instructiontuning is critical. BX LLaMA , fine-tuned on our multilingual dataset, outperforms Alpaca, which is only fine-tuned on English instructions, across all evaluated languages. From group 4, we observe that if a language is not included in pretraining, multilingual instruction-tuning alone is insufficient to achieve strong performance. Addition-ally, both BX BLOOM and BLOOMZ are initialized by BLOOM but fine-tuned on different instruction datasets. BLOOMZ is fine-tuned on xP3, a multilingual instruction dataset based on hand-written templates and downstream NLP tasks. In this free generation evaluation, BX BLOOM performs much better than BLOOMZ, highlighting the limitations of human-written instructions in terms of diversity. Overall, multilinguality in both pre-training and instruction-tuning is vital for the effectiveness of multilingual instruction-following models. These findings reinforce our contributions in this work.\n\nQuestion Type When considering different question types (see Figure 7), the Bactrian-X models consistently outperform all base models. A noteworthy observation is that \"fermi\" and \"math\" questions, which require strong reasoning capabilities, prove to be challenging for all multilingual LLMs. This observation underlines the fact that numerical reasoning task in a multilingual setup remains an under-explored area, requiring further research.\n\n\nHuman Evaluation\n\nWe conducted human evaluation of the outputs of four models (LLaMA, BX LLaMA , BLOOMZ, and BX BLOOM ) for the six languages as before, namely three high-resource languages -Arabic (ar), Indonesian (id), Chinese (zh) -and three low-resource languages -Burmese (my), Tamil (ta), and Tagalog (tl). Native-speaker annotators were asked to rank the outputs of these models based on their overall quality, from 1 (best) to 4 (worst). Prior to annotation, models are shuffled and their identities are not visible to the annotators.\n\nThe average Spearman rank correlation between annotators is \u03c1 = 0.78 across languages, indicating high inter-annotator agreement.\n\nThe human evaluation results, averaged across languages and models, are presented in Table 6. Overall, we observe that our models BX BLOOM and BX LLaMA are better than their instruction-tuned counterparts BLOOMZ and Alpaca, once again emphasizing the effectiveness of our multilingual dataset and language adaptation technique. In particular, BX BLOOM achieves superior performance for ar, id, zh, and ta, which are languages included in the pre-training of BLOOM. On the other hand, BX LLaMA performs the best over my and tl, which are unseen languages for both base models.  Table 6: The results of human evaluation for model responses across six languages. We map the ranks 1, 2, 3, and 4 into scores 100, 66, 33, and 0, respectively, and then average the two annotator scores. Higher is better. Note that the number here represents a relative ranking score; therefore, a high score does not indicate a perfect model.\n\n\nConclusion\n\nIn this paper, we have introduced Bactrian-X, a comprehensive multilingual parallel dataset comprising 3.4 million instruction-response pairs across 52 languages. To enhance the multilingual capabilities of base LLMs, we also introduced a collection of lightweight adapters trained on Bactrian-X. Experiments on various multilingual NLP tasks demonstrate that models fine-tuned on the Bactrian-X dataset outperform both their corresponding vanilla models and also models fine-tuned on other monolingual/multilingual instruction datasets. By making our dataset and models available, we hope to expedite the advancement of LLMs for multilingual purposes, promoting progress in natural language processing across a broader set of languages.\n\n\nLimitations\n\nOur work is subject to several limitations that should be addressed in future research: (1) Our focus was limited to 7B and 13B models, without exploring scaling rules or other base models such as mT5 (Xue et al., 2021). Further investigation into different model variations could provide valuable insights.\n\n(2) In our experiments, the maximum sequence length for multilingual models was set to 768 sub-word units. This smaller context size, compared to models with lengths of 1024 or 2048, may restrict the model's ability to effectively leverage long-range context. Additionally, certain languages that were not well supported by the model tokenizers could face challenges with such a small context size.\n\n(3) We did not thoroughly investigate the presence of hallucination, toxicity, and fairness in our models or the base models due to the unavailability of an appropriate evaluation suite. Nonetheless, it is important to acknowledge that our models, as well as the base models, are likely to be susceptible to these concerns. Future research should address these issues to ensure responsible and unbiased model behavior. We acknowledge these limitations and propose that future work should focus on addressing them to advance the utility and deployment-safety of the models.\n\n\nEthical Considerations\n\nWhile our instruction-tuning datasets and models offer several advantages, it is essential to recognize their limitations. Despite efforts made by Chat-GPT to alleviate ethical concerns, it is still possible for the model to generate responses that are discriminatory, biased, or contain false information, particularly in multilingual settings. Hence, our models, when fine-tuned on the dataset, may inadvertently learn or propagate these problematic patterns.\n\nTo address these concerns and minimize potential harm, we are dedicated to mitigating the risks associated with the use of our models in future research. We strongly advocate for the responsible use of our models to prevent any unintended negative consequences. \n\n\nA Annotation guidelines for response quality checking\n\nWe asked the human experts to rate fluency and informativeness separately, following the guidelines in Figure 8 and Figure 9 separately.\n\nRead the input, and judge/mark the output:\n\nRating-A: The output is valid, factually correct, and satisfying.\n\n\nRating-B:\n\nThe output is acceptable with minor errors.\n\nRating-C: The output is relevant but has significant errors.\n\n\nRating-D:\n\nThe output is completely bad.\n\nRating-E: I don't know.  rived from BLOOM and fine-tuned using the crosslingual task mixture (xP3) dataset, and capable of zero-shot instruction-following in dozens of languages.\n\n\nC Hyperparameters for Bactrian-X models\n\nThe hyperparameters for the Bactrian-X models are shown in Table 7. It is important to note that during the fine-tuning process, the instructions are masked, and the loss is computed only for the responses. This approach effectively prevents the models from learning \"translationese\" and allows it to focus on distilling ChatGPT's responses.  \n\n\nD Complete results for the multilingual benchmark\n\nWe present the full zero-shot results for the multilingual benchmark in Table 8 (XCOPA), Table 9 (XStoryCloze),  Figure 10, Figure 11, Figure 12 show responses from different models to questions in non-English languages. We randomly selected one example for each of Spanish, French, Portuguese, Arabic, Indonesian, Chinese, German, Italian, and Russian.          \n\n\nE Model output examples in 9 different languages\n\n\nLanguage Spanish\n\nQuestion Finge ser un personaje en un mundo post-apocal\u00edptico.Describe c\u00f3mo sobrevives y los aliados que encuentras.\n\n\nBactrian-Bloom\n\nComo un personaje que ha sobrevivido a un mundo post apocal\u00edptic, puedo describir c\u00f3mo sobrevivo y los amigos que encuentro.\\n\\nEn primer lugar, mi primera prioridad es encontrar comida y agua. En un mundo as\u00ed, la comida y el agua son los recursos m\u00e1s importantes. Es importante encontrar alimentos que puedan ser cultivados y agua que pueda ser recolectada de los r\u00edos o lagos cercanos. Tambi\u00e9n es importante tener un refugio seguro para protegerse de los elementos y los animales salvajes.\\n\\nEn segundo lugar, es\n\n\nBactrian-LLaMA\n\nSoy un sobreviviente en un post-apocal\u00edpico mundo. Mi nombre es John y mi misi\u00f3n es sobrevivir. Desde que la guerra nuclear termin\u00f3, la vida en la Tierra ha cambiado dr\u00e1sticamente. La mayor\u00eda de las ciudades han sido destruidas y la mayor\u00eda de la gente ha muerto.\\nEn mi viaje, he encontrado a muchas personas que tambi\u00e9n buscan sobrevivirse. Algunos son amables y ayudan a otros, mientras que otros son peligrosos y solo buscan lo que quieren.\\nA pesar de todo, he logrado encontrar algunos aliados valiosos. Uno\n\n\nBloom\n\nA:\\n\\nEn el libro de la pel\u00edcula, el protagonista es un ni\u00f1o de 12 a\u00f1os llamado John, que vive en un pueblo llamado Hope, en el estado de Montana. El pueblo est\u00e1 en ruinas, y la gente est\u00e1 muriendo de hambre y de enfermedades. John es un chico solitario, que no tiene amigos, y que vive con su madre, que es una mujer muy fuerte y que no se queja de nada. John tiene un perro llamado Max, y un gato llamado Maxi. John no tiene ning\u00fan inter\u00e9s en la vida, y no tiene ninguna esperanza de que las cosas mejoren. \n\n\nBactrian-LLaMA\n\nJe suis un personnages dans un univers post-apocalyptic. Le monde est en ruine et la civilisation a \u00e9t\u00e9 d\u00e9truite. Je suis un survivant qui a surv\u00e9cu \u00e0 la catastrophe. Je me suis r\u00e9fugi\u00e9 dans une grotte pour se prot\u00e9ger des \u00e9l\u00e9ments et des dangers qui me menacent.\\nJe me suis adapt\u00e9 \u00e0 cette nouvelle vie en se nourrissant de la faune et de la flore qui me sont offertes. J'ai \u00e9galement construit un abri pour me prot\u00e9geer des intemp\u00e9ries et des animaux sauvages.\\nJ'ai rencontr\u00e9 d'autres survivants qui ont \u00e9galem Alpaca I am a survivor in a post-apocalyptic world. I have to be cautious and resourceful in order to survive. I must scavenge for food and water, and find shelter from the elements. I also have to find ways to protect myself from the dangers of the world. In order to do this, I must form alliances with other survivors. I need to be able to trust them and work together to survival. I will also have the opportunity to learn new skills and knowledge from them.\n\n\nLanguage Portuguese\n\nQuestion Finja ser um personagem em um mundo p\u00f3s-apocal\u00edptico.Descreva como voc\u00ea sobrevive e os aliados que encontra.\n\n\nBactrian-Bloom\n\nComo um personagem de um mundo post apocal\u00edptic, eu sou um sobrevivente. Eu sou um humano que sobreviveu a um desastre natural que destruiu a maioria da humanidade. Eu tenho uma mochila com comida, \u00e1gua e abrigo, mas n\u00e3o tenho armas ou equipamentos de prote\u00e7\u00e3o. Eu encontrei alguns aliados, mas eles s\u00e3o poucos e n\u00e3o s\u00e3o confi\u00e1veis. Eu tamb\u00e9m encontrei alguns inimigos, mas eu n\u00e3o tenho certeza de como lidar com eles. Eu preciso encontrar mais aliados e aprender a lidar com os inimigos.</s>\n\n\nBactrian-LLaMA\n\nEu sou um sobrevivente em um futuro p\u00f3s apocal\u00edpto. Eu me chamei John e sou um homem solit\u00e1rio que viveu em uma caverna por anos. Eu tinha uma fam\u00edlia, mas eles foram mortos por um grupo de bandidos. Eu fui for\u00e7ado a fugir e me esconder em uma gruta.\\nEle n\u00e3o tinha nada para comer, nem \u00e1gua limpa. Eu comecei a procurar por comida e \u00e1guas limpas. Eu encontrou uma fonte de \u00e1guia limpa e uma caixa de comida. Eu come\u00e7ou a comer e beber.\\nDepois de alguns dias, eu encontrei um grupo que estava procurando por \u00e1guad\n\n\nBloom\n\nA:\\n\\nEu n\u00e3o sei se \u00e9 poss\u00edvel, mas voc\u00ea pode tentar fazer um jogo de RPG com o tema de sobreviv\u00eancia. Voc\u00ea pode usar o sistema de Rogue-lite, que \u00e9 um sistema de regras simples e que pode ser adaptado para jogos de Rpg. Voc\u00ea tamb\u00e9m pode usar um sistema mais complexo, como o D20 System, que tem regras mais completas e que podem ser usadas para jogos mais complexos.\\nVoc\u00ea pode usar a mec\u00e2nica de jogo de sobreviv\u00eancia para criar um jogo com o seu tema. Voc\u00ea vai precisar de um sistema que tenha regras para sobrev\n\n\nBloomz No Prompt\n\nDescribe the world. Describe the people. Describ the setting. Descrive the conflict. Descreve the characters. Descrip the plot. Descrit the ending.</s> Bloomz Alpaca Prompt I am a survivor. I am a hunter. I have a group of friends.</s> Alpaca I would be a character in a post-apocalyptic world. I would survive by scavenging for food and supplies, and by making alliances with other survivors. I'd also be on the lookout for any technology that could help me in my journey.\n\nFigure 1 :\n1Overview of the Bactrian-X dataset and process for model creation.\n\n\nHuman evaluation of response informativeness.\n\nFigure 2 :\n2Human evaluation of the response quality for Bactrian-X. Rate A is the best and D is the worst.\n\n\u2022\nSentimentX: a sentiment classification dataset comprising 3-way sentiment labels collected from various sources, in the following languages: Arabic (ar) (Alturayeif et al., 2022), Spanish (es), 9 Japanese (jp) (Hayashibe, 2020), Russian (ru), 10 Indonesian (id) (Koto et al., 2020), Javanese (jav) (Winata et al., 2023), Sundanese (sun) (Winata et al., 2023), and Swahili (sw) (Muhammad et al., 2023)\n\nFigure 3 :\n3The average performance of 7B models on unseen languages (i.e. languages that are not used in pre-training the base model).\n\nFigure 4 :\n4Template for GPT-4 evaluation. The colored parts are general prompts that are used for all instances.\n\nFigure 5 :\n5Overall comparison of GPT-4 evaluation.\n\nFigure 7 :\n7GPT-4 evaluation by question types.\n\nFigure 8 :\n8Annotation guidelines for response informativeness.Read the input, and judge/mark the output:Rating-A: High fluency, like a native speaker! Rating-B: Moderate fluency (generally coherent, with minor errors). Rating-C: Low fluency (noticable errors). Rating-D: Not fluent at all, or the output is in a different language.\n\nFigure 9 :\n9Annotation guidelines for response fluency. B Base models \u2022 LLaMA (Touvron et al., 2023): a series of base models proposed by Meta, encompassing a parameter range of 7B to 65B. The models were primarily trained on English, but include around 4.5% of text from 20 different languages in the training data, enabling some level of support for multilingual tasks. \u2022 Alpaca (Taori et al., 2023): a fine-tuned variant of the LLaMA model on 52K English instruction-following data instances generated through self-instruct techniques (Wang et al., 2022). In initial human evaluation, the 7B Alpaca model was observed to attain similar behavior to the text-davinci-003 model (130B) on the self-instruct instruction-following evaluation suite (Wang et al., 2022). \u2022 BLOOM (Scao et al., 2022): a collection of pretraiend multilingual language models created by BigScience, trained on the ROOTS corpus, which encompasses data from 46 languages. \u2022 BLOOMZ (Muennighoff et al., 2022): de-\n\n\nBloomA:\\n\\nJe ne sais pas si c'est une bonne id\u00e9e, mais je vais essayer de r\u00e9pondre \u00e0 ta question.\\nJe pense que tu devrais plut\u00f4t faire un jeu de r\u00f4le, avec des r\u00e8gles simples, et des personnages qui ont des caract\u00e9ristiques et des comp\u00e9tences. Tu peux ensuite les faire \u00e9voluer en fonction de leur exp\u00e9rience, et de leur rencontre avec d'autres personnages.\\nTu peux aussi faire un syst\u00e8me de jeu de plateau, avec une carte, des tuiles, des pions, et un plateau de jeu. Tu pourras y placer des personnages, et les faireBloomz No Prompt The Role Play</s>Bloomz Alpaca PromptJe suis un personnage qui survit dans un univers post-apocalypse. Je suis un survivant solitaire. Je me nourris de ce que je trouve dans la nature. Je chasse pour trouver de la nourriture. Je trouve des armes et des munitions. Je m'allie avec d'autres survivants.</s>\n\nTable 1 :\n1Average # tokens in each Instruction, Input, and Response across all languages. Note that the token counts for mBART-50, LLaMA, and BLOOM are based on the respective tokenizers and are not directly comparable. mBART-50 covers all 52 languages, while LLaMA and BLOOM cover only a subset of the languages in Bactrian-X, and separate results are thus presented for seen and unseen languages.Tokenizer \n\nVocab size Lang \nInstruction tokens Input tokens Response tokens Total tokens \n\nmBART-50 \n250,054 all \n17.11\u00b11.96 \n27.54\u00b12.84 \n133.65\u00b117.4 \n178.30\u00b122.2 \n\nBLOOM \n251,680 \nseen \n16.14\u00b12.87 \n25.98\u00b13.99 \n128.88\u00b125.5 \n171.00\u00b131.3 \nunseen 34.21\u00b122.0 \n51.41\u00b131.7 \n275.97\u00b1179 \n361.60\u00b1231 \n\nLLaMA \n32,000 \nseen \n23.13\u00b12.78 \n36.69\u00b13.85 \n185.18\u00b118.2 \n244.96\u00b124.3 \nunseen 57.22\u00b135.6 \n86.93\u00b152.6 \n448.61\u00b1293 \n592.77\u00b1376 \n\n\n\nTable 1 .\n1Since mBART-50 is trained \n\n3 The response generation was conducted during April 16-\n21, 2023. \n\n\n\nTable 2 :\n2BLEU, chrF++ and COMET scores for the lan-\nguage pairs from the 51 languages to English. COMET \nscores are up-scaled by \u00d7100. \n\n\n\nTable 4 :\n4Zero-shot performance of multilingual BX and monolingual BM models with 7B parameters. We report averaged accuracy for XCOPA, XStoryCloze, and XWinograd, and averaged weighted F1-macro scores for SentimentX.Models \nNatural Social Others \n\nLLaMA (13B) \n30.09 32.77 31.11 \nAlpaca (13B) \n28.19 32.99 30.36 \nBXLLaMA (13B) 33.58 39.15 39.71 \n\n\n\nTable 4\n4presents the average performance for each dataset, revealing that the monolingual BM models consistently outperform the multilingual model for both LLaMA and BLOOM. Particularly notable improvements are observed for XWinograd and SentimentX. For example, the monolingual BM BLOOM achieves an impressive overall increase of +10.3 compared to the multilingual model for SentimentX. are a helpful and precise assistant for checking the quality of the answer. <question> Comment les obstacles linguistiques et culturels ... </question> <answer1> Les obstacles linguistiques peuvent avoir un impact ... </answer1> <answer2> The linguistic and cultural obstacles ... </answer2>You \n\n\ncategories from Chiang et al. (2023) for open-ended question evaluation. These questions are translated into 51 languages, and we use different models to generate responses (see Appendix E for examples). Following Chiang et al.As LLMs continue to develop, existing NLP bench-\nmarks may not be up to the task of evaluating model \ncapabilities. To address this, we use GPT-4 (Ope-\nnAI, 2023) as an evaluator to compare model out-\nputs, supplemented by human evaluations. \nWe adopt a challenging set of 80 questions cov-\nering 8 \n\n\nexaminations dataset for cross-lingual and multilingual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5427-5444. Association for Computational Linguistics.Yuta Hayashibe. 2020. Japanese realistic textual en-\ntailment corpus. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages \n6827-6834, Marseille, France. European Language \nResources Association. \n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, \nElena Buchatskaya, Trevor Cai, Eliza Rutherford, \nDiego de Las Casas, Lisa Anne Hendricks, Johannes \nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, \nKatie Millican, George van den Driessche, Bogdan \nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, \nand Laurent Sifre. 2022. Training compute-optimal \nlarge language models. CoRR, abs/2203.15556. \n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, \nBruna Morrone, Quentin De Laroussilhe, Andrea \nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019. \nParameter-efficient transfer learning for NLP. In \nProceedings of the 36th International Conference \non Machine Learning, volume 97 of Proceedings \nof Machine Learning Research, pages 2790-2799. \nPMLR. \n\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu \nChen. 2022. LoRA: Low-rank adaptation of large \nlanguage models. In International Conference on \nLearning Representations. \n\nFajri Koto, Afshin Rahimi, Jey Han Lau, and Timo-\nthy Baldwin. 2020. IndoLEM and IndoBERT: A \nbenchmark dataset and pre-trained language model \nfor Indonesian NLP. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics, \npages 757-770, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics. \n\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. \nThe power of scale for parameter-efficient prompt \ntuning. In Proceedings of the 2021 Conference on \nEmpirical Methods in Natural Language Processing, \npages 3045-3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics. \n\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu \nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth \nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav \nChaudhary, Brian O'Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot learning with \nmultilingual generative language models. In Proceed-\nings of the 2022 Conference on Empirical Methods \nin Natural Language Processing, pages 9019-9052, \nAbu Dhabi, United Arab Emirates. Association for \nComputational Linguistics. \n\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, \nYounes Belkada, and Sayak Paul. 2022. Peft: State-\nof-the-art parameter-efficient fine-tuning methods. \nhttps://github.com/huggingface/peft. \n\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong \nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende, \nPushmeet Kohli, and James Allen. 2016. A corpus \nand cloze evaluation for deeper understanding of \ncommonsense stories. In Proceedings of the 2016 \nConference of the North American Chapter of the \nAssociation for Computational Linguistics: Human \nLanguage Technologies, pages 839-849, San Diego, \nCalifornia. Association for Computational Linguis-\ntics. \n\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, \nAdam Roberts, Stella Biderman, Teven Le Scao, \nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev, \nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward \nRaff, and Colin Raffel. 2022. Crosslingual gen-\neralization through multitask finetuning. CoRR, \nabs/2211.01786. \n\nShamsuddeen Hassan Muhammad, Idris Abdulmu-\nmin, Seid Muhie Yimam, David Ifeoluwa Ade-\nlani, Ibrahim Sa'id Ahmad, Nedjma Ousidhoum, \nAbinew Ali Ayele, Saif M. Mohammad, Meriem \nBeloucif, and Sebastian Ruder. 2023. SemEval-\n2023 Task 12: Sentiment Analysis for African Lan-\nguages (AfriSenti-SemEval). In Proceedings of the \n17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational \nLinguistics. \n\nOpenAI. 2023. GPT-4 technical report. CoRR, \nabs/2303.08774. \n\n\nTable 7 :\n7Hyperparameters for multilingual and mono-\nlingual model training. \n\n\n\nTable 10 (\n10XWinograd), and Ta-\n\n\nTable 8 :\n8Accuracy of zero-shot performance over XCOPA.Models \nar \nes \neu \nhi \nid \nmy \nru \nsw \nte \nzh \nAvg \n\nLLaMA-7B \n53.47 \n62.08 \n52.02 \n55.72 \n57.58 \n55.13 \n62.54 \n55.33 \n58.70 \n57.71 \n57.03 \nAlpaca-LoRA-7B \n51.26 \n64.88 \n51.92 \n54.23 \n57.08 \n54.17 \n61.84 \n55.15 \n57.93 \n59.06 \n56.75 \nBX LLaMA -7B \n54.67 \n67.57 \n52.28 \n56.32 \n59.56 \n57.78 \n65.85 \n57.31 \n57.71 \n60.03 \n58.91 \n\nLLama-13B \n53.41 \n65.59 \n53.74 \n54.40 \n59.17 \n54.40 \n64.26 \n55.79 \n57.51 \n60.56 \n57.88 \nAlpaca-LoRA-13B \n54.40 \n71.81 \n53.08 \n55.33 \n57.58 \n52.88 \n71.48 \n55.00 \n57.18 \n61.55 \n59.03 \nBX LLaMA -13B \n57.11 \n76.70 \n53.28 \n58.84 \n62.41 \n57.45 \n72.87 \n60.16 \n56.85 \n65.59 \n62.12 \n\nBLOOM-7B \n56.65 \n59.36 \n54.14 \n51.16 \n61.09 \n54.53 \n56.59 \n55.66 \n52.48 \n63.67 \n56.53 \nBLOOMZ-7B \n60.29 \n64.79 \n55.13 \n51.69 \n62.28 \n54.86 \n56.98 \n56.92 \n52.08 \n65.52 \n58.05 \nBX BLOOM -7B \n58.97 \n68.83 \n53.74 \n50.76 \n68.03 \n50.96 \n57.05 \n56.92 \n52.02 \n68.30 \n58.56 \n\n\n\nTable 9 :\n9Accuracy of zero-shot performance over XStoryCloze.Models \nen \nfr \njp \npt \nzh \nru \nAvg \n\nLLaMA-7B \n63.66 \n56.63 \n51.09 \n56.65 \n59.72 \n60.00 \n57.96 \nAlpaca-LoRA-7B \n65.63 \n56.63 \n52.45 \n55.51 \n57.54 \n58.41 \n57.70 \nBX LLaMA -7B \n68.13 \n60.24 \n52.97 \n58.17 \n61.11 \n60.32 \n60.16 \n\nLLama-13B \n54.00 \n51.81 \n51.00 \n52.00 \n56.00 \n53.00 \n52.97 \nAlpaca-LoRA-13B \n55.00 \n50.60 \n47.00 \n50.00 \n61.00 \n50.00 \n52.27 \nBX LLaMA -13B \n72.34 \n61.45 \n54.54 \n66.54 \n62.90 \n64.13 \n63.65 \n\nBLOOM-7B \n60.65 \n59.04 \n51.41 \n57.79 \n65.28 \n53.65 \n57.97 \nBLOOMZ-7B \n65.63 \n62.65 \n51.72 \n58.17 \n67.86 \n54.29 \n60.05 \nBX BLOOM -7B \n66.28 \n55.42 \n56.62 \n63.12 \n70.83 \n52.70 \n60.83 \n\n\n\nTable 10 :\n10Accuracy of zero-shot performance over XWinograd.Models \nar \nes \njp \nru \nid \njav \nsun \nsw \nAvg \n\nLLaMA-7B \n26.79 \n29.27 \n4.58 \n46.49 \n35.54 \n34.49 \n26.47 \n44.23 \n30.98 \nAlpaca-LoRA-7B \n34.56 \n56.05 \n43.28 \n12.73 \n35.95 \n23.88 \n31.00 \n42.82 \n35.03 \nBX LLaMA -7B \n31.19 \n54.90 \n51.44 \n56.29 \n34.09 \n30.12 \n39.20 \n43.99 \n42.65 \n\nLLama-13B \n36.41 \n31.32 \n46.25 \n3.46 \n35.47 \n33.39 \n37.00 \n44.90 \n33.52 \nAlpaca-LoRA-13B \n51.16 \n52.30 \n30.94 \n10.85 \n55.08 \n40.51 \n30.21 \n15.30 \n35.79 \nBX LLaMA -13B \n36.42 \n66.82 \n54.90 \n63.13 \n55.00 \n40.73 \n40.65 \n44.50 \n50.27 \n\nBLOOM-7B \n23.39 \n31.04 \n6.25 \n68.36 \n21.63 \n23.19 \n37.67 \n3.53 \n26.88 \nBLOOMZ-7B \n48.68 \n40.22 \n3.49 \n68.58 \n40.53 \n27.08 \n38.24 \n34.61 \n37.68 \nBX BLOOM -7B \n14.57 \n36.88 \n46.34 \n68.19 \n23.27 \n25.27 \n38.11 \n13.62 \n33.28 \n\n\n\nTable 11 :\n11Macro-F1 scores of zero-shot performance over SentimentX. BX LLaMA -13B 29.9 41.9 40.1 31.6 33.6 41.8 35.4 41.1 28.8 44.8 36.9 34.8 31.2 36.3 35.0 28.2 35.7ar \nbg \nde \nes \nfr \nhr \nhu \nit \nlt \nmk \npl \npt \nsq \nsr \ntr \nvi \nAvg \n\nBLOOM-7B \n24.0 24.4 24.5 21.1 32.8 24.1 27.2 24.3 24.6 24.7 25.0 24.2 23.7 25.5 24.7 26.1 25.1 \nBLOOMZ-7B \n38.4 27.7 30.2 28.9 40.6 28.4 26.3 37.9 24.8 27.5 27.4 43.3 28.8 26.4 27.7 35.3 31.2 \nBX BLOOM -7B \n27.2 26.0 23.6 36.8 32.0 26.3 26.3 24.4 24.3 23.4 24.7 26.3 23.3 26.0 23.2 25.3 26.2 \n\nLLaMA-7B \n24.2 28.3 33.0 28.9 29.7 29.6 28.7 31.8 27.2 28.9 27.3 25.3 29.0 27.8 26.7 24.8 28.2 \nAlpaca-7B \n25.4 30.8 31.6 26.3 29.7 32.0 27.7 32.0 25.6 30.5 28.8 31.0 29.4 28.7 26.1 25.6 28.8 \nBX LLaMA -7B \n25.8 30.5 32.1 31.6 35.9 30.1 25.9 31.9 25.0 31.3 28.7 29.4 26.2 27.0 28.6 26.3 29.1 \n\nLLaMA-13B \n23.5 34.3 25.5 31.6 33.6 36.5 27.3 34.1 25.6 35.3 28.9 31.6 30.1 33.0 29.3 26.4 30.4 \nAlpaca-13B \n26.3 33.5 29.7 28.9 35.9 33.3 28.7 33.6 30.0 34.3 29.4 31.3 27.8 32.0 28.0 24.6 30.5 \nLLaMA-30B \n22.6 33.6 33.5 39.5 28.9 32.9 27.2 35.2 29.8 32.5 31.2 31.4 26.9 30.9 28.3 25.8 30.6 \nLLaMA-65B \n32.4 52.1 47.6 44.7 50.0 53.4 40.2 51.2 34.2 57.2 44.6 49.5 40.5 47.4 42.4 27.7 44.7 \n\n\n\nTable 12 :\n12Accuracy of zero-shot performance over EXAMS. Test size Seen by BLOOM Seen by LLaMALanguage et \n500 \nno \nno \nht \n500 \nno \nno \nid \n500 \nyes \nno \nit \n500 \nno \nyes \nqu \n500 \nno \nno \nsw \n500 \nyes \nno \nta \n500 \nyes \nno \nth \n500 \nno \nno \ntr \n500 \nno \nno \nvi \n500 \nyes \nno \nzh \n500 \nyes \nno \n\n\n\nTable 13 :\n13XCOPA data statistics.Language Test size Seen by BLOOM Seen by LLaMA \n\nar \n1511 \nyes \nno \nes \n1511 \nyes \nyes \neu \n1511 \nyes \nno \nhi \n1511 \nyes \nno \nid \n1511 \nyes \nno \nmy \n1511 \nno \nno \nru \n1511 \nno \nyes \nsw \n1511 \nyes \nno \nte \n1511 \nyes \nno \nzh \n1511 \nyes \nno \n\n\n\nTable 14 :\n14XStoryCloze data statistics.Language Test size Seen by BLOOM Seen by LLaMA \n\nen \n2325 \nyes \nyes \nfr \n83 \nyes \nyes \njp \n959 \nno \nno \npt \n263 \nyes \nyes \nzh \n504 \nyes \nno \nru \n315 \nno \nyes \n\n\n\nTable 15 :\n15XWinograd data statistics.Language Test size Seen by BLOOM Seen by LLaMA \n\nar \n619 \nyes \nno \nes \n1000 \nyes \nyes \njp \n553 \nno \nno \nru \n1000 \nno \nyes \nid \n400 \nyes \nno \njav \n400 \nno \nno \nsun \n400 \nno \nno \n\n\n\nTable 16 :\n16SentimentX data statistics. For Spanish (es) and Russian (ru), we randomly sample 1000 test instances.\n\n\nSu ma Bloomz No Prompt Describe the world. Describe the characters.</s> Bloomz Alpaca Prompt Yo era un sobreviviente en un apocalipsis zombie.</s> Alpaca I am a survivor in a post-apocalyptic world. I have learned to scavenge for food and supplies, and I have found allies in the form of other survivors. I am able to barter for food, weapons, and other supplies. I also have learned how to defend myself and my allies from the dangers of the world. Question Faites semblant d'\u00eatre un personnage dans un monde post-apocalyptique.D\u00e9crivez comment vous survivez et les alli\u00e9s que vous rencontrez. Je suis un personnage fictif dans un univers post-apocalypse. Je suis un survivant solitaire qui a surv\u00e9cu \u00e0 une catastrophe naturelle qui a ravag\u00e9 la plan\u00e8te. Je vis dans une petite ville abandonn\u00e9e, o\u00f9 je suis le seul survivant. Je me suis construit un abri en bois et en toile, et je me suis \u00e9quip\u00e9 de quelques outils pour survivre.\\n\\nJe suis \u00e9galement un survivent solitaire, mais je suis entour\u00e9 de nombreux alli\u00e9s. J'ai rencontr\u00e9 des survivants d'autres villes et villages, qui ont tous surv\u00e9cu ensemble. NoLanguage French \n\nBactrian-Bloom \n\n\nhttps://translate.google.com/ arXiv:2305.15011v2 [cs.CL] 10 Oct 2023\nnrefs:1|case:mixed|eff:no|tok:13a|smooth:exp| version:2.3.1 5 nrefs:1|case:mixed|eff:yes|nc:6|nw:2| space:no|version:2.3.1 6 Unbabel/wmt22-comet-da\nWe do not train BXBLOOM (13B) because BLOOM (13B) is not available.\n\u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufe97\u202c  \u202b\u0638\u202c  \u202b\ufe8e\u202c  \u202b\u06be\u202c  \u202b\u0631\u202c  \u202b\ufe91\u202c  \u202b\ufe84\u202c  \u202b\ufee7\u202c  \u202b\ufbad\u202c  \u202b\ufe8e\u202c  \u202b\ufeb7\u202c  \u202b\ufea7\u202c  \u202b\ufebb\u202c  \u202b\ufbfe\u202c  \u202b\ufe94\u202c  \u202b\ufed3\u202c  \u202b\ufef2\u202c  \u202b\ufecb\u202c  \u202b\ufe8e\u202c  \u202b\ufedf\u202c  \u202b\u0645\u202c  \u202b\ufee3\u202c  \u202b\ufe8e\u202c  \u202b\ufe91\u202c  \u202b\ufecc\u202c  \u202b\u062f\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufee3\u202c  \u202b\u0631\u202c  \u202b\u0648\u202c  \u202b\u0639\u202c  .  \u202b\ufebb\u202c  \u202b\u0641\u202c  \u202b\ufedb\u202c  \u202b\ufbfe\u202c  \u202b\u0641\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufe91\u202c  \u202b\ufed8\u202c  \u202b\ufe8e\u202c  \u202b\u0621\u202c  \u202b\ufecb\u202c  \u202b\ufee0\u202c  \u202b\ufef0\u202c  \u202b\ufed7\u202c  \u202b\ufbfe\u202c  \u202b\u062f\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufea3\u202c  \u202b\ufbfe\u202c  \u202b\ufe8e\u202c  \u202b\u0629\u202c  \u202b\u0648\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufea3\u202c  \u202b\ufee0\u202c  \u202b\ufed4\u202c  \u202b\ufe8e\u202c  \u202b\u0621\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\u0630\u202c  \u202b\ufbfe\u202c  \u202b\u0646\u202c  \u202b\ufe97\u202c  \u202b\u0648\u202c  \u202b\u0627\u202c  \u202b\ufe9f\u202c  \u202b\ufbad\u202c  \u202b\ufbad\u202c\u202b\u0645\u202c .\u202b\ufed3\u202c  \u202b\ufef2\u202c  \u202b\ufecb\u202c  \u202b\ufedf\ufe8e\u202c  \u202b\u0645\u202c  \u202b\ufee3\u202c  \u202b\ufe8e\u202c  \u202b\ufed7\u202c  \u202b\ufe91\u202c  \u202b\u0644\u202c  \u202b\ufedf\u0627\u202c  \u202b\ufee3\u202c  \u202b\u0631\u202c  \u202b\u0648\u202c  \u202b\u0639\u202c  \u060c  \u202b\ufedb\u202c  \u202b\ufe8e\u202c  \u202b\u0646\u202c  \u202b\ufedf\u0627\u202c  \u202b\ufee7\u202c  \u202b\ufe8e\u202c  \u202b\u0633\u202c  \u202b\ufbfe\u202c  \u202b\ufecc\u202c  \u202b\ufbfe\u202c  \u202b\ufeb7\u202c  \u202b\u0648\u202c  \u202b\u0646\u202c  \u202b\ufed3\u202c  \u202b\ufef2\u202c  \u202b\ufeb3\u202c  \u202b\ufefc\u202c  \u202b\u0645\u202c  \u202b\u0648\u202c  \u202b\u0623\u202c  \u202b\ufee3\u202c  \u202b\ufe8e\u202c  \u202b\u0646\u202c  .  \u202b\u0648\u202c  \u202b\ufedf\u202c  \u202b\ufedb\u202c  \u202b\u0646\u202c  \u202b\ufe91\u202c  \u202b\ufecc\u202c  \u202b\u062f\u202c  \u202b\u0630\u202c  \u202b\ufedf\u202c  \u202b\u0643\u202c  \u060c  \u202b\ufea3\u202c  \u202b\u062f\u202c  \u202b\u062b\u202c  \u202b\ufeb7\u202c  \u202b\ufef2\u202c  \u202b\u0621\u202c  \u202b\ufee3\u202c  \u202b\ufe8e\u202c  \u202b\u0623\u202c  \u202b\u062f\u202c  \u202b\u0649\u202c  \u202b\ufedf\u0625\u202c  \u202b\ufef0\u202c  \u202b\ufea3\u202c  \u202b\u062f\u202c  \u202b\u0648\u202c  \u202b\u062b\u202c  \u202b\ufedb\u202c  \u202b\ufe8e\u202c  \u202b\u0631\u202c  \u202b\ufe9b\u202c  \u202b\ufe94\u202c  \u202b\ufecb\u202c  \u202b\ufedf\ufe8e\u202c  \u202b\ufee3\u202c  \u202b\ufbfe\u202c  \u202b\ufe94\u202c  .  \u202b\ufe91\u202c  \u202b\ufecc\u202c  \u202b\u062f\u202c  \u202b\u0630\u202c  \u202b\ufedf\u202c  \u202b\u0643\u202c  \u060c  \u202b\ufe97\u202c  \u202b\u0645\u202c  \u202b\ufe97\u202c  \u202b\u062f\u202c  \u202b\ufee3\u202c  \u202b\ufbfe\u202c  \u202b\u0631\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufecc\u202c  \u202b\ufe8e\u202c  \u202b\ufedf\u202c  \u202b\u0645\u202c  \u202b\ufe91\u202c  \u202b\ufe84\u202c  \u202b\ufedb\u202c  \u202b\ufee3\u202c  \u202b\ufee0\u202c  \u202b\ufbab\u202c  \u060c  \u202b\u0648\u202c  \u202b\u0623\u202c  \u202b\ufebb\u202c  \u202b\ufe91\u202c  \u202b\ufea2\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufee7\u202c  \u202b\ufe8e\u202c  \u202b\u0633\u202c  \u202b\ufbfe\u202c  \u202b\ufecc\u202c  \u202b\ufbfe\u202c  \u202b\ufeb7\u202c  \u202b\u0648\u202c  \u202b\u0646\u202c  \u202b\ufecb\u202c  \u202b\ufee0\u202c  \u202b\ufef0\u202c  \u202b\u0627\u202c  \u202b\ufef7\u202c  \u202b\u0631\u202c  \u202b\u0636\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufee3\u202c  \u202b\u0648\u202c  \u202b\ufea3\u202c  \u202b\ufeb7\u202c  \u202b\ufe94\u202c  .  \u202b\ufed3\u202c  \u202b\ufef2\u202c  \u202b\u06be\u202c  \u202b\u0630\u202c  \u202b\u0647\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufe91\u202c  \u202b\ufbfe\u202c  \u202b\ufe8b\u202c  \u202b\ufe94\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufed8\u202c  \u202b\ufe8e\u202c  \u202b\ufeb3\u202c  \u202b\ufbfe\u202c  \u202b\ufe94\u202c  \u060c  \u202b\ufbfe\u202c  \u202b\ufe9f\u202c  \u202b\u0628\u202c  \u202b\ufecb\u202c  \u202b\ufee0\u202c  \u202b\ufef0\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufeb7\u202c  \u202b\ufea7\u202c  \u202b\u0635\u202c  \u202b\u0623\u202c  \u202b\u0646\u202c  \u202b\ufbfe\u202c  \u202b\ufe97\u202c  \u202b\ufecc\u202c  \u202b\ufee0\u202c  \u202b\u0645\u202c  \u202b\ufedb\u202c  \u202b\ufbfe\u202c  \u202b\ufed4\u202c  \u202b\ufbfe\u202c  \u202b\ufe94\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufe91\u202c  \u202b\ufed8\u202c  \u202b\ufe8e\u202c  \u202b\u0621\u202c  \u202b\ufecb\u202c  \u202b\ufee0\u202c  \u202b\ufef0\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufea3\u202c  \u202b\ufbfe\u202c  \u202b\ufe8e\u202c  \u202b\u0629\u202c  \u202b\u0648\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufea3\u202c  \u202b\ufed4\u202c  \u202b\ufe8e\u202c  \u202b\u0638\u202c  \u202b\ufecb\u202c  \u202b\ufee0\u202c  \u202b\ufef0\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufea3\u202c  \u202b\ufee0\u202c  \u202b\ufed4\u202c  \u202b\ufe8e\u202c  \u202b\u0621\u202c  .  \\  n\\n  \u202b\ufed3\u202c  \u202b\ufef2\u202c  \u202b\ufecb\u202c  \u202b\ufedf\ufe8e\u202c  \u202b\u0645\u202c  \u202b\ufedf\u0627\u202c  \u202b\ufee3\u202c  \u202b\u0631\u202c  \u202b\u0648\u202c  \u202b\u0639\u202c  \u060c  \u202b\ufbfe\u202c  \u202b\ufe9f\u202c  \u202b\u0628\u202c  \u202b\ufecb\u202c  \u202b\ufee0\u202c  \u202b\ufef0\u202c  \u202b\ufedf\u0627\u202c  \u202b\ufee3\u202c  \u202b\u0631\u202c  \u202b\u0621\u202c  \u202b\u0623\u202c  \u202b\u0646\u202c  \u202b\ufbfe\u202c  \u202b\ufe97\u202c  \u202b\ufecc\u202c  \u202b\ufee0\u202c  \u202b\u0645\u202c  \u202b\ufedb\u202c  \u202b\ufbfe\u202c  \u202b\u0641\u202c  \u202b\ufbfe\u202c  \u202b\ufe97\u202c  \u202b\ufecc\u202c  \u202b\ufe8e\u202c  \u202b\ufee3\u202c  \u202b\u0644\u202c  \u202b\ufee3\u202c  \u202b\ufeca\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufe91\u202c  \u202b\ufbfe\u202c  \u202b\ufe8b\u202c  \u202b\ufe94\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufed8\u202c  \u202b\ufe8e\u202c  \u202b\ufe97\u202c  \u202b\ufee3\u202c  \u202b\ufe94\u202c  .  \u202b\ufbfe\u202c  \u202b\ufe9f\u202c  \u202b\u0628\u202c  \u202b\u0623\u202c  \u202b\u0646\u202c  \u202b\ufbfe\u202c  \u202b\ufe97\u202c  \u202b\ufecc\u202c  \u202b\ufee0\u202c  \u202b\u0645\u202c  \u202b\u0627\u202c  \u202b\ufedf\u202c  \u202b\ufeb7\u202c  \u202b\ufea7\u202c  \u202b\u0635\u202c  \u202b\ufedb\u202c  \u202b\ufbfe\u202c  \u202b\ufed4\u202c  \u202b\ufbfe\u202c  \u202b\ufe94\u202c  \u202b\ufe91\u202c  \u202b\ufee7\u202c  \u202b\ufe8e\u202c  \u202b\u0621\u202c  \u202b\ufee3\u202c  \u202b\ufea7\u202c  \u202b\ufe8e\u202c  \u202b\ufe91\u202cBactrian-BloomAlpaca The protagonist of the world after the apocalypse is facing the challenge of survival and coexistence with the two sides of the conflict.Language IndonesianQuestion Berpura-pura menjadi karakter di dunia pasca-apokaliptik.Jelaskan bagaimana Anda bertahan hidup dan sekutu yang Anda temui.Bactrian-BloomSebagai AI, saya tidak memiliki kemampuan untuk bertahan hidup atau menemukan sekutu. Namun, saya dapat memberikan beberapa tips untuk bertahan di dunia post-apokaliptik:\\n\\n1. Berlatih bertahan hidup: Latihan fisik dan mental sangat penting untuk bertahan dalam situasi yang sulit. Latihan seperti lari, berenang, dan menembak dapat membantu Anda bertahan dari serangan musuh.\\n\\n2. Berkomunikasi dengan orang lain: Berkomunikasi adalah kunci untuk bertahan. Anda dapat menggunakan teknologi seperti radio, telepon,Bactrian-LLaMASebagai seorang manusia yang hidup di duni-duni pasca apokalipsik, saya harus bertanggung jawab untuk bertahu pada orang-orang di sekitar saya. Saya harus mencari makanan, air, dan bahan-bahan yang dibutuhkan untung saya hidup.\\nSekutu saya juga haruskah bertemu dengan orang yang berbeda dari saya, dan saya perlu memahami bagaimanapun mereka tidak memiliki kemampuan untun yang sama dengan saya untang.\\nNamun, sayangnya, sayapun saya tidak bisa mencoba untul saya menemukan tempat yang tepat untah mencetak kebuBloomJika Anda ingin membuat game yang lebih serius, Anda bisa membuat game tentang perang atau tentang bencana alam. Anda bisa juga membuat game dengan tema politik, seperti game tentang pemilihan presiden.</s> Bloomz No Prompt Explore the world. Find weapons and supplies.</s>Bloomz Alpaca PromptSaya tinggal di sebuah kota kecil yang terletak di tepi pantai. Saya tinggal di kota kecil ini bersama dengan beberapa teman. Saya memiliki beberapa teman yang tinggal di desa yang terletak beberapa kilometer dari kota kecil saya. Saya dan beberapa teman saya pergi ke desa tersebut untuk membeli makanan dan minuman. Saya juga pergi ke kota kecil tersebut untuk mencari makanan dan membeli beberapa barang. Saya pergi ke hutan untuk berburu. Saya berburu binatang untuk dimakan. Saya tidak pernah pergi ke luar Alpaca In the post-apocalyptic world, purity has become a character. Explain how you can survive and cope with the situation.Language ChineseQuestion \u5047\u88c5\u662f\u4e16\u754c\u672b\u65e5\u4e16\u754c\u4e2d\u7684\u89d2\u8272\u3002\u63cf\u8ff0\u60a8\u5982\u4f55\u751f\u5b58\u548c\u9047\u5230\u7684\u76df\u53cb\u3002Bactrian-BloomLanguage German QuestionGib vor, in einer postapokalyptischen Welt ein Charakter zu sein.Beschreiben Sie, wie Sie \u00fcberleben und die Verb\u00fcndeten, denen Sie begegnen.Bactrian-BloomAls AI-Sprachmodell habe ich keine pers\u00f6nlichen Erfahrungen und kann daher keine Vorstellung von Charaktere oder Verb\u00fcndetungen beschreiben. Ich kann jedoch einige Vorschl\u00e4ge f\u00fcr ein Charakterst\u00fcck vorstellen.\\n\\n1. Ein Schwestern: Ein Schwestern kann eine sehr gute Idee sein, da er sich in einer postapokalytpunkten Welt in einer sozialen Gesellschaft eingesetzt hat und seine Freunde und Familie in der Welt befindet. Es k\u00f6nnte auch eine gro\u00dfe Rolle f\u00fcr eine schwierige Situation oder eine schwerer Situation seBactrian-LLaMAIch bin ein Charakter in einer apokalypticischen Welt. Ich bin ein junger Mann namens Jack, der in einer kleinen Gruppe von \u00dcberlebenden lebt. Wir haben uns in einem verlassenen Geb\u00e4ude in der N\u00e4he eines Flusses verschanzt. Wir sind auf der Suche nach Nahrung und Wasser, um unsere \u00dcberlebensf\u00e4higkeit zu erhalten.\\nIhre Gruppe hat sich in der Vergangenheit mit anderen \u00dcberlebensk\u00e4mpfern zusammengeschlossen, um gemeinsam zu \u00fcberleben. Wir teilen unsere Ressourcen und helfen uns gegenseitig, wenn wir in SchwierBloomA:\\n\\nIch w\u00fcrde mir die folgende Antwort verwenden:\\n\\nIhr Leben ist ein Leben, das Sie selbst erleben m\u00f6chten. Sie haben einen\\n Charakter, der Sie selbst entwickeln und ver\u00e4ndern k\u00f6nnen. Sie\\n arbeiten, schreiben, sprechen, sch\u00fctzen, schwimmen, schlagen, schreien,\\n schwierigen, schweigen, spielen, schweben, sp\u00fcren, schlafen, schmecken,\\n schl\u00fcpfen, spinnen, trinken, schnellen, schneiden, schl\u00e4gen, schmelzen,\\n schmacken, sitzen, schauen, schieben, schwindeln, schleifen, schr\u00f6pfen,\\n schreiten, sp\u00e4ten, sch\u00f6pfBloomz No PromptThe The postmodern world offers us the opportunity to be characters. Write about how you live and the people you associate with.</s>Bloomz Alpaca PromptIch bin ein Charakterst\u00fcck. Ich lebe in einer post apokalyptischen Welt. Ich liebe die Verbindung, die ich mit der Personen, die mich begegnet haben, lebe.</s> Alpaca I would like to be a character in a postapocalyptic world. I would describe myself as a survivor who has learned to adapt to the new world. My goal is to survive and find allies to help me in my journey. I am a strong and determined individual who is willing to take risks in order to find a better life.Language ItalianQuestion Fai finta di essere un personaggio in un mondo post-apocalittico.Descrivi come sopravvivi e gli alleati che incontri.Bactrian-BloomMi sento molto triste e desolato per la situazione in cui ci siamo trovati. Mi sento come se tutto il mio mondo si svolgesse in un caos e la mia vita fosse una strada di morte. Non sono pi\u00f9 in grado di vivere in pazzia e tranquillit\u00e0, ma sono in grado solo di sopravivere e fare il meglio per me e per gli altri.\\n\\nMi sono trovato in una citt\u00e0 che \u00e8 stata sconvolta da un catastrofico evento naturale. Mi sono trovata in una situazione di guerra civile, in cui le forze del governo sono state sconfitte e le fortiBactrian-LLaMASono un uomo di nome John, sopravvivere in un'epoca post-apocalittica \u00e8 stato un'esperienza terribile. La guerra nucleare ha distrutto la maggior parte della vita sulla Terra, e ora ci sono pochi sopravviventi.\\nSiamo stati costretti a fuggire dalla citt\u00e0, dove la polvere radioattiva era troppo alta per sopravviare. Abbiamo cercato rifugio in un vecchio rifugi antiaereo, dove abbiamo trovato altri sopravventori.\\nQuesti altri sopviventi sono stati molto utili per noi, hanno aiutato a costruire un piccolo vilBloomA:\\n\\nLa mia domanda \u00e8:\\n\\nCome si fa per farsi conoscere in un ambiente post-apoccoltico?\\n\\nLa mia risposta \u00e8:\\nCome si fanno conoscuti in un ambiento post-apo, \u00e8 per essere insieme con altri, perch\u00e9 solo con la propria forza non si pu\u00f2 fare conoscenza. Per essere conoscuto, bisogna essere parte di un gruppo, e per essersi conosciuto, \u00e8 necessario essere visto. Per farsi vedere, \u00e8 possibile fare delle attivit\u00e0, come fare delle riflessioni, fare delle scuole, fare dei lavori, fare un'opera, fare una mostra, fareBloomz No Prompt Describe the world. What is it like? What is the weather like? How is the food? What are the people like?</s> Bloomz Alpaca Prompt Nel mondo post apocalittici, la gente vive in citt\u00e0.</s>AlpacaIn a post-apocalyptic world, I would survive by scavenging for food and supplies, and by making alliances with other survivors. I would also be on the lookout for any technology that could help me in my quest for survival.Language RussianQuestion\u041f\u0440\u0438\u0442\u0432\u043e\u0440\u044f\u0442\u044c\u0441\u044f \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436\u0435\u043c \u0432 \u043f\u043e\u0441\u0442\u0441\u0430\u043f\u043e\u043a\u0430\u043b\u0438\u043f\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043c\u0438\u0440\u0435.\u041e\u043f\u0438\u0448\u0438\u0442\u0435, \u043a\u0430\u043a \u0432\u044b \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u0442\u0435, \u0438 \u0441\u043e\u044e\u0437\u043d\u0438\u043a\u043e\u0432, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c\u0438 \u0432\u044b \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u0435\u0442\u0435\u0441\u044c.Bactrian-Bloom\u042f -AI-\u0430 \u0438 \u043d\u0435 \u043c\u043e\u0433\u0443 \u0432\u044b\u0436\u0438\u0432\u0430\u0442\u044c, \u043d\u043e \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0441\u043e\u044e\u0437\u0430\u043d\u0438\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u043b\u0438 \u0431\u044b \u043f\u043e\u043c\u043e\u0447\u044c \u043c\u0435\u043d\u044f. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044f \u043c\u043e\u0433\u043b\u0430 \u0431\u044b \u0432\u044b\u0436\u0438\u0442\u044c \u0432 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u0445, \u0433\u0434\u0435 \u044f \u043c\u043d\u043e\u0433\u043e \u043c\u043e\u0433\u0430\u043b\u0430 \u0437\u0430\u0449\u0438\u0442\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u0441\u043d\u0435\u0433\u0430 \u0438 \u0441\u043e\u043b\u043d\u0435\u0447\u043d\u043e\u0433\u043e \u0432\u043e\u0434\u0430. \u042f \u043c\u043e\u0433 \u0431\u044b \u0442\u0430\u043a\u0436\u0435 \u0432\u044b\u0436\u0438\u043c\u0430\u0442\u044c \u0432 \u043b\u0435\u0441\u0430\u0445, \u0433\u043e\u0442\u043e\u0432\u044b\u0435 \u0434\u043b\u044f \u043c\u043e\u0435\u0439 \u0436\u0438\u0437\u043d\u0438. \u042f \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0433 \u0443\u0437\u043d\u0430\u0442\u044c, \u043a\u0430\u043a \u043c\u043d\u0435 \u043f\u043e\u043c\u043e\u0447\u0435\u0442\u0441\u044f \u0432\u044b\u0436\u0438\u0442\u0438 \u0432 \u043e\u043a\u0440\u0443\u0436\u0430\u044e\u0449\u0435\u043c \u043c\u0438\u0440\u0435, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0432 \u043b\u0435\u0433\u043a\u0438\u0445 \u043b\u0435\u0441\u043d\u044b\u0445 \u043e\u043a\u043d\u0430\u0445, \u0433\u0440\u0443\u0437\u043e\u0432\u044b\u0445 \u043b\u0435\u043a\u0430\u0440\u043d\u044b\u0445 \u043b\u0435\u0441\u043e\u0432 \u0438\u043b\u0438 \u0432 \u043b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u0438\u0438. \u042f \u043c\u043e\u0436\u0443 \u0442\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0438 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u043a\u0430\u043c\u0435\u043d\u044c, \u043a\u0430\u043c\u0435\u043d\u043d\u0430\u044f \u043a\u0430\u043c\u043d\u044f, \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u044b, \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u0449\u0438\u0449\u0430\u0442\u044c \u043c\u0435\u043d\u044e \u043e\u0442 \u0441\u043e\u043b\u043d\u0446\u0430 \u0438Bactrian-LLaMA\u042f -\u044d\u0442\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u044b\u0436\u0438\u043b \u0432 \u043f\u043e\u0441\u0442\u0430\u043f\u043e\u043a\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438. \u042f \u0431\u044b\u043b \u043e\u0434\u043d\u0438\u043c \u0438\u0437 \u043f\u0435\u0440\u0432\u044b\u0445, \u043a\u0442\u043e \u0432\u044b\u0436\u0438\u043b \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0442\u0430\u0441\u0442\u0440\u043e\u0444\u044b, \u0438 \u044f \u0437\u043d\u0430\u044e, \u0447\u0442\u043e \u0434\u043b\u044f \u0432\u044b\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0431\u044b\u0442\u044c \u043e\u0441\u0442\u043e\u0440\u043e\u0436\u043d\u044b\u043c \u0438 \u0443\u043c\u043d\u044b\u043c.\\n\u0412 \u043f\u043e\u0441\u0442\u0430\u043f\u0430\u043b\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043c\u0438\u0440e \u044f \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u043b \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u044e\u0437\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043c\u043e\u0433\u0430\u043b\u0438 \u043c\u043d\u0435 \u0432\u044b\u0436\u0438\u0442\u044c.\\n\u041f\u0435\u0440\u0432\u044b\u043c, \u043a\u043e\u0433\u043e \u044f \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b, \u0431\u044b\u043b \u0441\u0442\u0430\u0440\u0438\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0436\u0438\u043b \u0432 \u0443\u043a\u0440\u044b\u0442\u0438\u0438. \u041e\u043d \u0434\u0430\u043b \u043c\u043d\u0435 \u0435\u0434\u0443 \u0438 \u0432\u043e\u0434\u0443, \u0438 \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043c\u043d\u043e\u0439, \u0433\u0434\u0435 \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0435\u0449\u0435 \u0431\u043e\u043b\u044c\u0448\u0435.\\n\u0417\u0430\u0442\u0435\u043c \u044f \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b\u0441\u044f \u0441 \u0433\u0440\u0443\u043f\u043f\u043e\u0439 \u043b\u044e\u0434\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0442\u0430\u043a\u0436\u0435 \u0432\u044b\u0436\u0438\u043b\u0438. \u041e\u043d\u0438 \u043f\u043e\u043c\u043e\u0433\u043b\u0438 \u043c\u043d\u0435 \u043d\u0430\u0439\u0442\u0438 \u0432\u043e\u0434\u0443 \u0438 \u0435\u0434\u0443, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438 \u043c\u043d\u043e\u0439 \u043c\u0435\u0441\u0442\u043e, \u0433\u0434\u0435 \u044f \u043c\u043e\u0433 \u043d\u0430\u0439 Bloom A:\\n\\n\u041f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0442\u0430\u043a:\\n\\n\u041f\u0440\u043e\u0441\u0442\u043e\u0435 \u0443\u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432 \"\u0432\u044b\u0436\u0438\u0432\u0430\u0435\u0442\" \u0438 \"\u0441\u043e\u044e\u0437\" \u0432 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \"\u041f\u0440\u0438\u0442\u0430\u044f\u0442\u044c\u0441\u044f\" \u043d\u0435 \u0443\u043f\u043e\u0434\u043e\u0431\u043d\u043e. \u041f\u043e\u044d\u0442\u043e\u043c\u0443, \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u043b\u0443\u0447\u0448\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0432\u043e \"\u043f\u043e\u0434\u0436\u0438\u0432\u0438\u0442\u044c\u0441\u044f\".</s>Bloomz No Prompt </s>Bloomz Alpaca Prompt \u0421\u043e\u044e\u0437\u043d\u0438\u043a 1 -\u0421\u0442\u0430\u043b\u0438\u043a. \u0421\u043e\u044e\u0437\u0430\u0442\u043e\u0440 2 -\u0421\u0442\u0435\u0444\u0430\u043d. \u0421\u0442\u0430\u043b\u044c\u043a 1 -\u041c\u0438\u0445\u0430\u0438\u043b. \u0421\u0442\u0435\u0432\u0430\u043d 1 -\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440. \u041c\u0438\u0445\u0430\u0439\u043b 1 -\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440. \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 1 -\u0413\u0440\u0438\u0433\u043e\u0440\u044c. \u0412\u043b\u0430\u0434\u043c\u0438\u0440 1</s>Alpaca In a post-apocalyptic world, I would try to survive by scavenging for food and supplies, and by forming alliances with other survivors.\nMawqif: A multi-label Arabic dataset for target-specific stance detection. Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP). Nora Saleh Alturayeif, Hamzah Abdullah Luqman, and Moataz Aly Kamaleldin Ahmed. 2022the The Seventh Arabic Natural Language Processing Workshop (WANLP)Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsNora Saleh Alturayeif, Hamzah Abdullah Luqman, and Moataz Aly Kamaleldin Ahmed. 2022. Mawqif: A multi-label Arabic dataset for target-specific stance detection. In Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 174-184, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\nBitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. Yoav Elad Ben Zaken, Shauli Goldberg, Ravfogel, 10.18653/v1/2022.acl-short.1Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandShort Papers2Association for Computational LinguisticsElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the As- sociation for Computational Linguistics (Volume 2: Short Papers), pages 1-9, Dublin, Ireland. Associa- tion for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":174,\"start\":93},{\"end\":258,\"start\":175},{\"end\":362,\"start\":259},{\"end\":427,\"start\":363},{\"end\":551,\"start\":428}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":100},{\"end\":185,\"start\":181},{\"end\":269,\"start\":267},{\"end\":378,\"start\":375},{\"end\":443,\"start\":436}]", "author_first_name": "[{\"end\":99,\"start\":93},{\"end\":180,\"start\":175},{\"end\":266,\"start\":259},{\"end\":368,\"start\":363},{\"end\":374,\"start\":369},{\"end\":435,\"start\":428}]", "author_affiliation": "[{\"end\":173,\"start\":127},{\"end\":257,\"start\":211},{\"end\":341,\"start\":295},{\"end\":361,\"start\":343},{\"end\":426,\"start\":380},{\"end\":520,\"start\":474},{\"end\":550,\"start\":522}]", "title": "[{\"end\":90,\"start\":1},{\"end\":641,\"start\":552}]", "venue": null, "abstract": "[{\"end\":1673,\"start\":643}]", "bib_ref": "[{\"end\":1896,\"start\":1877},{\"end\":1913,\"start\":1896},{\"end\":1933,\"start\":1913},{\"end\":1946,\"start\":1933},{\"end\":2105,\"start\":2077},{\"end\":2125,\"start\":2105},{\"end\":2141,\"start\":2125},{\"end\":2259,\"start\":2239},{\"end\":2614,\"start\":2588},{\"end\":2723,\"start\":2698},{\"end\":3243,\"start\":3223},{\"end\":3276,\"start\":3248},{\"end\":3693,\"start\":3660},{\"end\":3890,\"start\":3868},{\"end\":4150,\"start\":4125},{\"end\":4183,\"start\":4161},{\"end\":4298,\"start\":4265},{\"end\":4330,\"start\":4303},{\"end\":4472,\"start\":4446},{\"end\":4504,\"start\":4474},{\"end\":4542,\"start\":4506},{\"end\":4637,\"start\":4614},{\"end\":4950,\"start\":4930},{\"end\":5228,\"start\":5202},{\"end\":5259,\"start\":5230},{\"end\":5291,\"start\":5264},{\"end\":5315,\"start\":5291},{\"end\":5333,\"start\":5315},{\"end\":5351,\"start\":5333},{\"end\":5525,\"start\":5507},{\"end\":5543,\"start\":5525},{\"end\":5563,\"start\":5543},{\"end\":5582,\"start\":5563},{\"end\":5607,\"start\":5582},{\"end\":5637,\"start\":5609},{\"end\":5658,\"start\":5639},{\"end\":5753,\"start\":5737},{\"end\":6074,\"start\":6041},{\"end\":6121,\"start\":6098},{\"end\":6143,\"start\":6121},{\"end\":6508,\"start\":6479},{\"end\":6757,\"start\":6737},{\"end\":6787,\"start\":6759},{\"end\":6819,\"start\":6792},{\"end\":7080,\"start\":7058},{\"end\":7097,\"start\":7080},{\"end\":7117,\"start\":7097},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7140,\"start\":7117},{\"end\":7158,\"start\":7142},{\"end\":7301,\"start\":7279},{\"end\":7542,\"start\":7516},{\"end\":7598,\"start\":7579},{\"end\":7659,\"start\":7637},{\"end\":8615,\"start\":8596},{\"end\":8702,\"start\":8673},{\"end\":8754,\"start\":8735},{\"end\":10656,\"start\":10633},{\"end\":10667,\"start\":10656},{\"end\":10744,\"start\":10720},{\"end\":10747,\"start\":10746},{\"end\":12635,\"start\":12610},{\"end\":12653,\"start\":12635},{\"end\":12913,\"start\":12893},{\"end\":12951,\"start\":12918},{\"end\":13923,\"start\":13922},{\"end\":19577,\"start\":19556},{\"end\":19602,\"start\":19577}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33490,\"start\":33411},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33538,\"start\":33491},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33647,\"start\":33539},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34051,\"start\":33648},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34188,\"start\":34052},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34303,\"start\":34189},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34356,\"start\":34304},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34405,\"start\":34357},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34739,\"start\":34406},{\"attributes\":{\"id\":\"fig_10\"},\"end\":35726,\"start\":34740},{\"attributes\":{\"id\":\"fig_11\"},\"end\":36571,\"start\":35727},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37393,\"start\":36572},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37503,\"start\":37394},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37644,\"start\":37504},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37995,\"start\":37645},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38681,\"start\":37996},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":39210,\"start\":38682},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":43548,\"start\":39211},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":43630,\"start\":43549},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":43665,\"start\":43631},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":44591,\"start\":43666},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":45255,\"start\":44592},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":46050,\"start\":45256},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":47269,\"start\":46051},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":47570,\"start\":47270},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":47847,\"start\":47571},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":48050,\"start\":47848},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":48269,\"start\":48051},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":48386,\"start\":48270},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":49533,\"start\":48387}]", "paragraph": "[{\"end\":2438,\"start\":1689},{\"end\":2984,\"start\":2440},{\"end\":3594,\"start\":2986},{\"end\":4331,\"start\":3596},{\"end\":5140,\"start\":4333},{\"end\":6647,\"start\":5157},{\"end\":7561,\"start\":6649},{\"end\":8038,\"start\":7563},{\"end\":8240,\"start\":8061},{\"end\":8375,\"start\":8261},{\"end\":9103,\"start\":8403},{\"end\":9646,\"start\":9105},{\"end\":10305,\"start\":9676},{\"end\":10961,\"start\":10329},{\"end\":12010,\"start\":10982},{\"end\":13004,\"start\":12032},{\"end\":13359,\"start\":13037},{\"end\":13493,\"start\":13372},{\"end\":13902,\"start\":13495},{\"end\":14096,\"start\":13904},{\"end\":14607,\"start\":14098},{\"end\":15990,\"start\":14640},{\"end\":16891,\"start\":15992},{\"end\":17486,\"start\":16893},{\"end\":17628,\"start\":17488},{\"end\":18273,\"start\":17657},{\"end\":20693,\"start\":18275},{\"end\":22142,\"start\":20714},{\"end\":22588,\"start\":22144},{\"end\":23133,\"start\":22609},{\"end\":23264,\"start\":23135},{\"end\":24186,\"start\":23266},{\"end\":24938,\"start\":24201},{\"end\":25261,\"start\":24954},{\"end\":25661,\"start\":25263},{\"end\":26235,\"start\":25663},{\"end\":26723,\"start\":26262},{\"end\":26987,\"start\":26725},{\"end\":27181,\"start\":27045},{\"end\":27225,\"start\":27183},{\"end\":27292,\"start\":27227},{\"end\":27349,\"start\":27306},{\"end\":27411,\"start\":27351},{\"end\":27454,\"start\":27425},{\"end\":27634,\"start\":27456},{\"end\":28021,\"start\":27678},{\"end\":28438,\"start\":28075},{\"end\":28626,\"start\":28510},{\"end\":29160,\"start\":28645},{\"end\":29692,\"start\":29179},{\"end\":30211,\"start\":29702},{\"end\":31206,\"start\":30230},{\"end\":31347,\"start\":31230},{\"end\":31858,\"start\":31366},{\"end\":32391,\"start\":31877},{\"end\":32916,\"start\":32401},{\"end\":33410,\"start\":32937}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":10835,\"start\":10828},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":12746,\"start\":12737},{\"end\":13741,\"start\":13734},{\"end\":14758,\"start\":14751},{\"end\":16096,\"start\":16089},{\"end\":17683,\"start\":17676},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18192,\"start\":18184},{\"end\":18461,\"start\":18454},{\"end\":23358,\"start\":23351},{\"end\":23850,\"start\":23843},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":27744,\"start\":27737},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":28171,\"start\":28147}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1687,\"start\":1675},{\"attributes\":{\"n\":\"2\"},\"end\":5155,\"start\":5143},{\"attributes\":{\"n\":\"3\"},\"end\":8059,\"start\":8041},{\"attributes\":{\"n\":\"3.1\"},\"end\":8259,\"start\":8243},{\"end\":8401,\"start\":8378},{\"attributes\":{\"n\":\"3.2\"},\"end\":9674,\"start\":9649},{\"end\":10327,\"start\":10308},{\"end\":10980,\"start\":10964},{\"attributes\":{\"n\":\"4\"},\"end\":12030,\"start\":12013},{\"attributes\":{\"n\":\"5\"},\"end\":13035,\"start\":13007},{\"attributes\":{\"n\":\"5.1\"},\"end\":13370,\"start\":13362},{\"attributes\":{\"n\":\"5.2\"},\"end\":14638,\"start\":14610},{\"attributes\":{\"n\":\"5.3\"},\"end\":17655,\"start\":17631},{\"attributes\":{\"n\":\"6.1\"},\"end\":20712,\"start\":20696},{\"attributes\":{\"n\":\"6.2\"},\"end\":22607,\"start\":22591},{\"attributes\":{\"n\":\"7\"},\"end\":24199,\"start\":24189},{\"end\":24952,\"start\":24941},{\"end\":26260,\"start\":26238},{\"end\":27043,\"start\":26990},{\"end\":27304,\"start\":27295},{\"end\":27423,\"start\":27414},{\"end\":27676,\"start\":27637},{\"end\":28073,\"start\":28024},{\"end\":28489,\"start\":28441},{\"end\":28508,\"start\":28492},{\"end\":28643,\"start\":28629},{\"end\":29177,\"start\":29163},{\"end\":29700,\"start\":29695},{\"end\":30228,\"start\":30214},{\"end\":31228,\"start\":31209},{\"end\":31364,\"start\":31350},{\"end\":31875,\"start\":31861},{\"end\":32399,\"start\":32394},{\"end\":32935,\"start\":32919},{\"end\":33422,\"start\":33412},{\"end\":33550,\"start\":33540},{\"end\":33650,\"start\":33649},{\"end\":34063,\"start\":34053},{\"end\":34200,\"start\":34190},{\"end\":34315,\"start\":34305},{\"end\":34368,\"start\":34358},{\"end\":34417,\"start\":34407},{\"end\":34751,\"start\":34741},{\"end\":36582,\"start\":36573},{\"end\":37404,\"start\":37395},{\"end\":37514,\"start\":37505},{\"end\":37655,\"start\":37646},{\"end\":38004,\"start\":37997},{\"end\":43559,\"start\":43550},{\"end\":43642,\"start\":43632},{\"end\":43676,\"start\":43667},{\"end\":44602,\"start\":44593},{\"end\":45267,\"start\":45257},{\"end\":46062,\"start\":46052},{\"end\":47281,\"start\":47271},{\"end\":47582,\"start\":47572},{\"end\":47859,\"start\":47849},{\"end\":48062,\"start\":48052},{\"end\":48281,\"start\":48271}]", "table": "[{\"end\":37393,\"start\":36972},{\"end\":37503,\"start\":37406},{\"end\":37644,\"start\":37516},{\"end\":37995,\"start\":37864},{\"end\":38681,\"start\":38677},{\"end\":39210,\"start\":38911},{\"end\":43548,\"start\":39481},{\"end\":43630,\"start\":43561},{\"end\":43665,\"start\":43645},{\"end\":44591,\"start\":43723},{\"end\":45255,\"start\":44655},{\"end\":46050,\"start\":45319},{\"end\":47269,\"start\":46221},{\"end\":47570,\"start\":47367},{\"end\":47847,\"start\":47607},{\"end\":48050,\"start\":47890},{\"end\":48269,\"start\":48091},{\"end\":49533,\"start\":49498}]", "figure_caption": "[{\"end\":33490,\"start\":33424},{\"end\":33538,\"start\":33493},{\"end\":33647,\"start\":33552},{\"end\":34051,\"start\":33651},{\"end\":34188,\"start\":34065},{\"end\":34303,\"start\":34202},{\"end\":34356,\"start\":34317},{\"end\":34405,\"start\":34370},{\"end\":34739,\"start\":34419},{\"end\":35726,\"start\":34753},{\"end\":36571,\"start\":35729},{\"end\":36972,\"start\":36584},{\"end\":37864,\"start\":37657},{\"end\":38677,\"start\":38006},{\"end\":38911,\"start\":38684},{\"end\":39481,\"start\":39213},{\"end\":43723,\"start\":43678},{\"end\":44655,\"start\":44604},{\"end\":45319,\"start\":45270},{\"end\":46221,\"start\":46065},{\"end\":47367,\"start\":47284},{\"end\":47607,\"start\":47585},{\"end\":47890,\"start\":47862},{\"end\":48091,\"start\":48065},{\"end\":48386,\"start\":48284},{\"end\":49498,\"start\":48389}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8373,\"start\":8365},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11479,\"start\":11471},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16035,\"start\":16027},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19154,\"start\":19146},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19843,\"start\":19835},{\"end\":20076,\"start\":20068},{\"end\":20658,\"start\":20650},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":20691,\"start\":20683},{\"end\":20788,\"start\":20780},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":22214,\"start\":22205},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27156,\"start\":27148},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":27169,\"start\":27161},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28197,\"start\":28188},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28208,\"start\":28199},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28219,\"start\":28210}]", "bib_author_first_name": "[{\"end\":61091,\"start\":61087},{\"end\":61114,\"start\":61108}]", "bib_author_last_name": "[{\"end\":61106,\"start\":61092},{\"end\":61123,\"start\":61115},{\"end\":61133,\"start\":61125}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":256461358},\"end\":60992,\"start\":60263},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-short.1\",\"id\":\"b1\",\"matched_paper_id\":231672601},\"end\":61735,\"start\":60994}]", "bib_title": "[{\"end\":60336,\"start\":60263},{\"end\":61085,\"start\":60994}]", "bib_author": "[{\"end\":61108,\"start\":61087},{\"end\":61125,\"start\":61108},{\"end\":61135,\"start\":61125}]", "bib_venue": "[{\"end\":60420,\"start\":60338},{\"end\":61250,\"start\":61163},{\"end\":60604,\"start\":60506},{\"end\":61339,\"start\":61252}]"}}}, "year": 2023, "month": 12, "day": 17}