{"id": 129944996, "updated": "2023-11-08 15:55:02.664", "metadata": {"title": "Bidirectional Learning for Domain Adaptation of Semantic Segmentation", "authors": "[{\"first\":\"Yunsheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Nuno\",\"last\":\"Vasconcelos\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 4, "day": 24}, "abstract": "Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.10620", "mag": "3087152942", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiYV19", "doi": "10.1109/cvpr.2019.00710"}}, "content": {"source": {"pdf_hash": "b2f13657614c8129dec8fe63555c8f4306ebdf1d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.10620v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.10620", "status": "GREEN"}}, "grobid": {"id": "9e0e664201e017522b26b1157702eee42715878a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b2f13657614c8129dec8fe63555c8f4306ebdf1d.txt", "contents": "\nBidirectional Learning for Domain Adaptation of Semantic Segmentation\n\n\nYunsheng Li \nUC San Diego\nSan Diego\n\nLu Yuan Microsoft \nUC San Diego\nSan Diego\n\nNuno Vasconcelos nvasconcelos@ucsd.edu \nUC San Diego\nSan Diego\n\nBidirectional Learning for Domain Adaptation of Semantic Segmentation\n\nDomain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.On the forward direction (i.e., \"translation-tosegmentation\", similar to [12, 36]), we propose a self-supervised learning (SSL) approach in training our segmentation adaptation model. Different from segmentation models trained on real data, the segmentation adaptation model is trained on both synthetic and real datasets, but the real data has no annotations. At every time, we may regard the predicted labels for real data with high confidence as the approximation to the ground truth\n\nIntroduction\n\nRecent progress on image semantic segmentation [18] has been driven by deep neural networks trained on large datasets. Unfortunately, collecting and manually annotating large datasets with dense pixel-level labels has been extremely costly due to large amount of human effort is required. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic images with computer-generated annotations [27,28]. Despite this, the domain mismatch between the real images (target) and the synthetic data (source) cripples the models' performance. Domain adaptation addresses this domain shift problem. Specifically, we focus on the hard case of the problem where no labels from the target domain are available. This class of techniques is commonly referred to as Unsupervised Domain Adaptation.\n\nTraditional methods for domain adaptation involve minimizing some measure of distance between the source and * This work was done when Yunsheng Li is an intern at Microsoft Cloud & AI the target distributions. Two commonly used measures are the first and second order moment [2], and learning the distance metrics using Adversarial approaches [34,35]. Both approaches have had good success in the classification problems (e.g., MNIST [16], USPS [7] and SVHN [22]); however, as pointed out in [37], their performance is quite limited on the semantic segmentation problem.\n\nRecently, domain adaptation for semantic segmentation has made good progress by separating it into two sequential steps. It firstly translates images from the source domain to the target domain with an image-to-image translation model (e.g., CycleGAN [38]) and then add a discriminator on top of the features of the segmentation model to further decrease the domain gap [12,36]. When the domain gap is reduced by the former step, the latter one is easy to learn and can further decrease the domain shift. Unfortunately, the segmentation model very relies on the quality of imageto-image translation. Once the image-to-image translation fails, nothing can be done to make it up in the following stages.\n\nIn this paper, we propose a new bidirectional learning framework for domain adaptation of image semantic segmentation. The system involves two separated modules: image-to-image translation model and segmentation adaptation model similar to [12,36], but the learning process involves two directions (i.e., \"translation-to-segmentation\" and \"segmentation-to-translation\"). The whole system forms a closed-loop learning. Both models will be motivated to promote each other alternatively, causing the domain gap to be gradually reduced. Thus, how to allow one of both modules providing positive feedbacks to the other is the key to success. labels, and then use them only to update the segmentation adaptation model while excluding predicted labels with low confidence. This process is referred as self-supervised learning, which aligns two domains better than one-trial learning that is widely used in existing approaches. Furthermore, better segmentation adaptation model would contribute to better translation model through our backward direction learning.\n\nOn the backward direction (i.e., \"segmentation-totranslation\"), our translation model would be iteratively improved by the segmentation adaptation model, which is different from [12,36] where the image-to-image translation is not updated once the model is trained. For the purpose, we propose a new perceptual loss, which forces the semantic consistency between every image pixel and its translated version, to build the bridge between translation model and segmentation adaptation model. With the constraint in the translation model, the gap in visual appearance (e.g., lighting, object textures), between the translated images and real datasets (target) can be further decreased. Thus, the segmentation model can be further improved through our forward direction learning.\n\nFrom the above two directions, both the translation model and the segmentation adaptation model complement each other, which helps achieve state-of-theart performance in adapting large-scale rendered image dataset SYNTHIA [28]/GTA5 [27], to real image dataset, Cityscapes [5], and outperform other methods by a large margin. Moreover, the proposed method is general to different kinds of backbone networks.\n\nIn summary, our key contributions are:\n\n1. We present a bidirectional learning system for semantic segmentation, which is a closed loop to learn the segmentation adaptation model and the image translation model alternatively.\n\n\n2.\n\nWe propose a self-supervised learning algorithm for the segmentation adaptation model, which incrementally align the source domain and the target domain at the feature level, based on the translated results.\n\n3. We introduce a new perceptual loss to the image-toimage translation, which supervises the translation by the updated segmentation adaptation model.\n\n\nRelated Work\n\nDomain Adaptation. When transferring knowledge from virtual images to real photos, it is often the case that there exists some discrepancy from the training to the test stage. Domain adaptation aims to rectify this mismatch and tune the models toward better generalization at testing [24]. The existing work on domain adaptation has mainly focused on image classification [30]. A lot of work aims to learn domain-invariant representations through minimizing the domain distribution discrepancy. Maximum Mean Discrepancy (MMD) loss [8], computing the mean of representations, is a common distance metric between two domains. As the extension to MMD, some statistics of feature distributions such as mean and covariance [2,21] are used to match two different domains. Unfortunately, when the distribution is not Gaussian, solely matching mean and covariance is not enough to align the two different domains well. Adversarial learning [9] recently becomes popular, and another kind of domain adaptation methods. It reduces the domain shift by forcing the features from different domains to fool the discriminator. [34] would be the pioneer work, which introduces an adversarial loss on top of the high-level features of the two domains with the classification loss for the source dataset and achieves a better performance than the statistical matching methods. Expect for adversarial loss, some work proposed some extra loss functions to further decrease the domain shift, such as reweighted function for each class [4], and disentangled representations for separated matching [35]. All of these methods work on simple and small classification datasets (e.g., MNIST [16] and SVHN [22]), and may have quite limited performance in more challenging tasks, like segmentation.\n\nDomain Adaptation for Semantic Segmentation. Recently, more domain adaptation techniques are proposed for semantic segmentation models, since an enormous amount of labor-intensive work is required to annotate so many images that are needed to train high-quality segmentation networks. A possible solution to alleviate the human efforts is to train networks on virtual data which is labeled automatically. For example, GTA5 [27] and SYHTHIA [28] are two popular synthetic datasets of city streets with overlapped categories, similar views to the real datasets (e.g., CITYSCAPE [5], CamVid [1]). Domain adaptation can be used to align the synthetic and the real datasets.\n\nThe first work to introduce domain adaptation for semantic segmentation is [13], which does the global and local alignments between two domains in the feature level. Curriculum domain adaptation [37] estimates the global distribution and the labels for the superpixel, and then learns a segmentation model for the finer pixel. In [33], multiple discriminators are used for different level features to reduce domain discrepancy. In [31], foreground and background classes are separately treated for decreasing the domain shift respectively. All these methods target to directly align features between two domains. Unfortunately, the visual (e.g., appearance, scale, etc.) domain gap between synthetic and real data usually makes it difficult for the network to learn transferable knowledge.\n\nMotivated by the recent progress of unpaired image-toimage translation work (e.g., CycleGAN [38], UNIT [17], MUNIT [14]), the mapping from virtual to realistic data is regarded as the image synthesis problem. It can help re-duce the domain discrepancy before training the segmentation models. Based on the translated results, Cycada [12] and DCAN [36] further align features between two domains in feature level. By separately reducing the domain shift in learning, these approaches obtained the state-of-the-art performance. However, the performance is limited by the quality of image-to-image translation. Once it fails, nothing can be done in the following step. To address this problem, we introduce a bidirectional learning framework where both translation and segmentation adaption models can promote each other in a closed loop.\n\nThere are two most related work. In [6], the segmentation model is also used to improve the image translation, but not to adapt the source domain to the target domain since it is only trained on source data. [39] also proposed a selftraining method for training the segmentation model iteratively. However, the segmentation model is only trained on source data and uses none of image translation techniques.\n\nBidirectional Learning. The kind of techniques were first proposed to solve the neural machine translation problem, such as [10,23], which train a language translation model for both directions of a language pair. It improves the performance compared with the uni-direction learning and reduces the dependency on large amount of data. Bidirectional learning techniques were also extended to image generation problem [25], which trains a single network for both classification and image generation problem from both top-to-down and down-to-top directions. A more related work [29] proposed bidirectional image translation (i.e., source-to-target, and target-to-source), then trained two classifiers on both domains respectively and finally fuses the classification results. By contrast, our bidirectional learning refers to translation boosting the performance of segmentation and vise verse. The proposed method is used to deal with the semantic segmentation task.\n\n\nMethod\n\nGiven the source dataset S with segmentation labels Y S (e.g., synthetic data generated by computer graphics) and the target dataset T with no labels (i.e., real data), we want to train a network for semantic segmentation, which is finally tested on the target dataset T . Our goal is to make its performance to be as close as possible to the model trained on T with ground truth labels Y T . The task is unsupervised domain adaptation for semantic segmentation. The task is not easy since the visual (e.g., lighting, scale, object textures, etc.) domain gap between S and T makes it difficult for the network to learn transferable knowledge at once.\n\nTo address this problem, the recent work [12] proposed two separated subnetworks. One is image-to-image translation subnetwork F which learn to translate an image from S to T in absence of paired examples. The another is segmen- tation adaptation subnetwork M that is trained on translated results F(S), which have the same labels Y S to S, and the target images T with no labels. Both subnetworks are learnt in a sequential way shown in Figure 1(a). Such a two-stage solution has two advantages: 1) F helps decrease the visual domain gap; 2) when domain gap is reduced, M is easy to learn, causing better performance. However, the solution has some limitations. Once F is learnt, it is fixed. There is no feedback from M to boost the performance of F. Besides, one-trial learning for M seems to just learn limited transferable knowledge.\n\nIn this section, we propose a new learning framework which can address the above two issues well. We inherit the way of separated subnetworks, but employ a bidirectional learning instead (in Section 3.1), which uses a closed-loop to iteratively update both F and M. Furthermore, we introduce a self-supervised learning to allow M being selfmotivated in training (in Section 3.2). The network architecture and loss functions are presented in Section 3.3.\n\n\nBidirectional Learning\n\nOur learning consists of two directions shown in Figure 1\n(b).\nThe forward direction (i.e., F \u2192 M) is similar to the behavior of previous sequential learning [12]. We first train the image-to-image translation model F using images from T and S. Then, we get the translated results S = F(S). Note that F won't change the labels of S , which are the same to Y S (labels of S). Next, we train the segmentation adaptation model M using S with Y S and T . The loss function to learn M can be defined as:\nM = \u03bb adv adv (M(S ), M(T )) + seg (M(S ), Y S ), (1)\nwhere adv is adversarial loss that enforces the distance between the feature representations of S and the feature representations of T (obtained after S , T are fed into M) as small as possible. seg measures the loss of semantic segmentation. Since only S have the labels, we solely measure the accuracy for the translated source images S .\n\nThe backward direction (i.e., M \u2192 F) is newly added. The motivation is to promote F using updated M. In [35,14], a perceptual loss, which measures the distance of features obtained from a pre-trained network on object recognition, is used in the image translation network to improve the quality of translated result. Here, we use M to compute features for measuring the perceptual loss. By adding the other two losses: GAN loss and image reconstruction loss, the loss function for learning F can be defined as:\nF = \u03bb GAN [ GAN (S , T ) + GAN (S, T )] + \u03bb recon [ recon (S, F \u22121 (S )) + recon (T , F(T )] + per (M(S), M(S )) + per (M(T ), M(T ),(2)\nwhere three losses are computed symmetrically, i.e., S \u2192 T and T \u2192 S, to ensure the image-to-image translation consistent. The GAN loss GAN enforces two distributions between S and T similar to each other.\nT = F \u22121 (T ),\nwhere F \u22121 is the reverse function of F that maps the image from T to S. The loss recon measures the reconstruction error when the image from S is translated back to S. per is the perceptual loss that we propose to maintain the semantic consistency between S and S or between T and T . That is, once we obtained an ideal segmentation adaptation model M, whether S and S , or T and T should have the same labels, even although there is the visual gap between S and S , or between T and T .\n\n\nSelf-supervised Learning for Improving M\n\nIn the forward direction (i.e., F \u2192 M), if the label is available for both the source domain S and the target domain T , the fully supervised segmentation loss seg is always the best choice to reduce the domain discrepancy. But in our case, the label for the target dataset is missing. As we known, self-supervised learning (SSL) has been used in semi-supervised learning before, especially when the labels of dataset are insufficient or noisy. Here, we use SSL to help promote the segmentation adaptation model M.\n\nBased on the prediction probability of T , we can obtain some pseudo labels Y T with high confidence. Once we have the pseudo labels, the corresponding pixels can be aligned directly with S according to the segmentation loss. Thus, we modify the overall loss function used to learn M (in Equation 1) as:\nM = \u03bb adv adv (M(S ), M(T )) + seg (M(S ), Y S ) + seg (M(T ssl ), Y T ),(3)\nwhere T ssl \u2282 T is a subset of the target dataset in which the pixels have the pseudo labels Y T . It can be empty at the beginning. When a better segmentation adaptation model M is achieved, we can use M to predict more high-confident labels for T , causing the size of T ssl to grow. The recent work [39] also use SSL for segmentation adaptation. By contrast, SSL used in our work is combined with adversarial learning, which can work much better for the segmentation adaptation model. We use the illustration (shown in Figure 2) to explain the principle of this process. When we learn the segmentation adaptation model for the first time, T ssl is empty and the \nN (F (K) ) for k \u2190 1 to K do (Bidirectional Learning) train F (k) with Equation 2 train M (k) 0 with Equation 1 for i \u2190 1 to N do (SSL) update T ssl with M (k) i\u22121 train M (k) i\nagain with Equation 3 end for end for domain gap between S and T can be reduced with the loss shown in Equation 1. This process is shown in Figure 2 (a). Then we pick up the points in the target domain T that have been well aligned with S to construct the subset T ssl . In the second step, we can easily shift T ssl to S and keep them being aligned with the help of the segmentation loss provided by the pseudo labels. This process is shown in the middle of Figure 2 (b). Therefore, the amount of data in T that needs to be aligned with S is decreased. We can continue to shift the remaining data to S same as step 1, as shown the right side of Figure 2 (b). It worth noting that SSL helps adversarial learning process focus on the rest data that is not fully aligned at each step, since adv can hardly change the data from S and T ssl that has been aligned well.\n\n\nNetwork and Loss Function\n\nIn this section, we introduce the network architecture (shown in Figure 3), details of loss functions and the training process (shown in Algorithm 1). The network is mainly composed with two components -the image translation model and segmentation adaptation model. While the translation model is learned, the loss GAN and loss recon (shown in Figure 3 and Equation 2) can be defined as:\nGAN (S , T ) = E I T \u223cT [D F (I T )] + E I S \u223cS [1 \u2212 D F ((I S ))], recon (S, F \u22121 (S )) = E I S \u223cS [||F \u22121 ((I S ))\u2212I S || 1 ],\nwhere I S and I T are the input images from source and target dataset. I S is the translated image given by F. D F is the discriminator added to reduce the difference between I T and I S . For the reconstruction loss, L 1 norm is used to keep the cycle consistency between I S and F \u22121 (I S ) when F \u22121  is the reverse function of F. Here, we only show two losses for one direction, and GAN (S, T ), recon (T , F(T )) can be defined similarly.\n\nAs shown in Figure 3, the perceptual loss per connects the translation model and segmentation adaptation model. When we learn the perceptual loss per for the translation model, instead of only keeping the semantic consistency between I S and its translated result I S , we add another term weighted by \u03bb per recon , to keep the semantic consistency between I S and its corresponding reconstruction F \u22121 (I S ). With the new term, the translation model can be more stable especially for the reconstruction part. per is defined as: When the segmentation adaptation model is trained, it requires the adversarial learning with the loss adv and the self-supervised learning with the loss seg (shown in Equation 3). For adversarial learning, we add a discriminator D M to decrease the difference between the source and target probabilities shown in Figure 3. adv can be defined as:\nadv (M(S ), M(T )) = E I T \u223cT [D M (M(I T ))] + E I S \u223cS [1 \u2212 D M (M(I S ))].\nThe segmentation loss seg uses the cross-entropy loss. For the source image I S , seg can be defined as:\nseg (M(S ), Y S ) = \u2212 1 HW H,W C c=1 1 [c=y hw S ] log P hwc S ,\nwhere y S is the label map for I S , C is the number of classes, H and W are the height and width of the output probability map. P S is the source probability of the segmentation adaptation model which can be defined as P S = M(I S ).\n\nFor the target image I T , we need to define how to choose the pseudo label map y T for it. We choose to use a common method we call as \"max probability threshold(MPT)\" to filter the pixels with high prediction confidence in I T . Thus we can define y T as y T = argmax M(I T ) and the mask map for y T as m T = 1 [argmax M(I T )>threshold] . Thus the segmentation loss for I T can be expressed as:\nseg (M(T ssl ), Y T ) = \u2212 1 HW H,W m hw T C c=1 1 [c=y hw T ] log P hwc T ,\nwhere P T is the target output of M.\n\nWe present the training processing in Algorithm 1. The training process consists of two loops. The outer loop is mainly to learn the translation model and the segmentation adaptation model through the forward direction and the backward direction. The inner loop is mainly used to implement the SSL process. In the following section, we will introduce how to choose the number of iteration for learning F, M, and how to estimate the MPT for SSL.\n\n\nDiscussion\n\nTo know the effectiveness of bidirectional learning and self-supervised learning for improving M, we conduct some ablation studies. We use GTA5 [27] as the source dataset and Cityscapes [5] as the target dataset. The translation model is CycleGAN [38] and the segmentation adaptation model is DeepLab V2 [3] with the backbone ResNet101 [11]. All the following experiments use the same model, unless it is specified.\n\nHere, we first provide the description of notations used in the following ablation study and tables. M (0) is the initial model to start the bidirectional learning and is trained only with source data. M (1) is trained with source and target data with adversarial learning. For M (0) (F (1) ), a translation model F (1) is used to translate the source data and then a segmentation model M (0) is learned based on the translated source data. M  \nM (0) (F (1) ) 41.1 M (1) 0 (F (1) ) 42.7 M (2) 0 (F (2) )\n43.3\n\n\nBidirectional Learning without SSL\n\nWe show the results obtained by the model trained in a bidirectional learning system without SSL. In Table 1, M (0) is our baseline model that gives the lowerbound for mIoU. We find a similar performance between the model M (1) and M (0) (F (1) ) both of which achieve more than 7% improvement compared to M (0) and about 1.6% further improvement is given by M (1) (F (1) ). It means segmentation adaptation model and the translation model can work independently and when combined together which is basically one iteration of the bidirectional learning they can be complementary to each other. We further show that through continue training the bidirectional learning system, in which case M (1) (F (1) ) is used to replace M (0) for the backward direction, a better performance can be given by the new model M (2) 0 (F (2) ).\n\n\nBidirectional Learning with SSL\n\nIn this section, we show how the SSL can further improve the ability of segmentation adaption model and in return influence the bidirectional learning process. In Table 2, we show results given by two iterations(k = 1, 2) based on Algorithm 1. In Figure 4, we show the segmentation results and the corresponding mask map given by the max probability threshold (MPT) which is 0.9. In Figure 4, the white pixels are the ones with prediction confidence higher than MPT and the black pixels are the low confident pixels.\n\nWhile k = 1, when model M (1) ) with SSL, the mIoU can be improved by 4.5%. We can find for each category when the IoU is below 50, a big improvement can be got from M (1) ). It can prove our previous analysis in section 3.2 that with SSL the well aligned data from source and target domain can be kept and the rest data can be further aligned through the adversarial learning process.\n(1) 0 (F (1) ) is updated to M (1) 2 (F(1) 0 (F (1) ) to M (1) 2 (F\nWhile k = 2, we first replace M (0) with M (1) 2 (F (1) ) to start the backward direction. Without SSL the mIoU is 44.3 which is a larger improvement compared to the results shown in Table 1. It can further prove our discussion in section 4.1 about the importance role played by the segmentation adaptation model in the backward direction. Furthermore, we can find from Table 2, although in the beginning of the second iteration the mIoU drops from 47.2 to 44.3, while SSL is induced, the mIoU can be promoted to 48.5    Figure 4, our findings can be further confirmed and the most important thing is as we improve the segmentation performance, the segmentation adaptation model can give more confident prediction which can be observed by the increasing white area in the mask map. It gives us the motivation to use the mask map to choose the threshold and number of iterations for the SSL process in Algorithm 1.\n1 (F (1) ) 0.7 45.9 M (1) 1 (F (1) ) \u2212 44.90 (F (1) ) 69% 42.7 M (1) 1 (F (1) ) 79% 46.8 M (1) 2 (F (1) ) 81% 47.2 M\n\nHyper Parameter Learning\n\nWe will describe how to choose the threshold to filter out data with high confidence and the iteration number N in Algorithm 1.\n\nWhen we choose the threshold, we have to balance between two folds. On one hand, we desire the predicted labels with high confidence as many as possible (presented as white areas in Figure 4). On the other hand, we want to avoid inducing too much noise caused by the incorrect prediction, namely, the threshold should be as high as possible. We present the relationship of the prediction confidence (maximum class probability of per pixel from M) and the ratio between selected pixels and all pixels (i.e., percentage of all white areas shown in Figure 4) on the left side of Figure 5, then show the slope in the right side of Figure 5. We can find when the prediction confidence increases from 0.5 to 0.9, the ratio decreases almost linearly and the slope stays almost unchanged. But from 0.9 to 0.99, the ratio decreases much faster. Based on the observation, we choose the inflection point 0.9 as the threshold as the trade-off between the number and the quality of selected labels.\n\nIn order to further prove our choice, in Table 3, we show segmentation results using different thresholds to the selfsupervised learning of M K N when K = 1 and N = 1 in Algorithm 1. As another option, we also consider soft threshold instead of hard one, namely, every pixel being weighted  by its maximum class probability. We show the result on the bottom row. All the results confirm our analysis. When the threshold is lower than 0.9, the uncorrected prediction becomes the key issue to influence the performance of SSL. While we increase the threshold to 0.95, the SSL process is more sensitive to the number of pixels that can be used. When we use soft threshold, the result is still worse. It is probably because an amount of labeling noise are involved and the bad impact cannot be well alleviated by assigning a lower weight to the noise label. Thus, 0.9 seems to be a good choice for the threshold in the following experiments. For the iteration number N , we select a proper value according to the predicted labels as well. When N increases, the segmentation adaptation model becomes much stronger, causing more labels to be used for SSL. Once the pixel ratio for SSL stops increasing, it means that the learning for the segmentation adaptation model is converged and nearly no improved. We definitely increase the value of K to start another iteration. In Table 4, we show some segmentation results with the theshold 0.9 as we increase the value of N . We can find the mIoU becomes better with the increasing of N . When N = 2 or 3, the mIoU almost stopped increasing, and the pixel ratio stay around the same. It may suggest that N = 2 is a good choice, and we use it in our work.\n\n\nExperiments\n\nIn this section, we compare the results obtained between our method and the state-of-the-art methods.\n\nNetwork Architecture. In our experiments, we choose to use DeepLab V2 [3] with ResNet101 [11] and FCN-8s [18] with VGG16 [32] as our segmentation model. They are initialized with the network pre-trained with ImageNet [15]. The discriminator we choose for segmentation adaptation model is similar to [26] which has 5 convolution layers with kernel 4 \u00d7 4 with channel numbers {64, 128, 256, 512, 1} and stride of 2. For each convolutional layer except the last one, a leaky ReLU [20] parameterized by 0.2 is followed. For the image translation model, we follow the architecture of CycleGAN [38] with 9 blocks and add the segmentation adaptation model as the perceptual loss.\n\nTraining. When training CycleGAN [38], the image is randomly cropped to the size 452 \u00d7 452 and it is trained for 20 epochs. For the first 10 epochs, the learning rate is 0.0002 and decreases to 0 linearly after 10 epochs. We set \u03bb GAN = 1, \u03bb recon = 10 in Equation 3 and set \u03bb per = 0.1, \u03bb per recon = 10 for the perceptual loss. When training the segmentation adaptation model, images are resized with the long side to be 1, 024 and the ratio is kept. Different parameters are used for DeepLab V2 [3] and FCN-8s [18]. For DeepLab V2 with ResNet 101, we use SGD as the optimizer. The initial learning rate is 2.5 \u00d7 10 \u22124 and decreased with 'poly' learning rate policy with power as 0.9. For FCN-8s with VGG16, we use Adam as the optimizer with momentum as 0.9 and 0.99. The initial learning rate is 1 \u00d7 10 \u22125 and decreased with 'step' learning rate policy with step size as 5000 and \u03b3 = 0.1. For both DeepLab V2 and FCN-8s, we use the same discriminator that is trained with Adam optimizer with initial learning rate as 1 \u00d7 10 \u22124 for DeepLab V2 and 1 \u00d7 10 \u22126 for FCN-8s. The momentum is set as 0.9 and 0.99. We set \u03bb adv = 0.001 for ResNet101 and 1 \u00d7 10 \u22124 for FCN-8s in Equation 1. Dataset. As we have mentioned before, two synthetic datasets -GTA5 [27] and SYNTHIA [28] are used as the source dataset and Cityscapes [5] is used as the target dataset. For GTA5 [27], it contains 24, 966 images with the resolution of 1914 \u00d7 1052 and we use the 19 common categories between GTA5 and Cityscapes dataset. For SYN-THIA [28], we use the SYNTHIA-RAND-CITYSCAPES set which contains 9, 400 images with the resolution 1280\u00d7 760 and 16 common categories with Cityscapes [5]. For Cityscapes [5], it is splited into training set, validation set and testing set. The training set contains 2, 975 images with the resolution 2048 \u00d7 1024. We use the training set as the target dataset only. Since the ground truth labels for the testing set are missing, we have to use the validation set which contains 500 images as the testing set in our experiments. Comparison with State-of-Art. We compare the results between our method and the state-of-the-art method with two different backbone networks: ResNet101 and VGG16 respectively. We perform the comparison on two tasks: \"GTA5 to Cityscapes\" and \"SYNTHIA to Cityscapes\". In Table 5, we present the adaptation result on the task \"GTA5  to Cityscapes\" with ResNet101 and VGG16. We can observe the role of backbone in all domain adaptation methods, namely ResNet101 achieves a much better result than VGG16. In [37,33,19], they mainly focus on featurelevel alignment with different adversarial loss functions. But working only on the feature level is not enough, even though the best result [36] among them is still about 5% worse than our results. Cycada [12] (we run their codes with ResNet101) and DCAN [36] used the translation model followed by the segmentation adaptation model to further reduce the visual domain gap, and both achieved very similar performance. Ours uses similar loss function compared to Cycada [12], but with a new proposed bidirectional learning method, 6% improvement can be achieved. CBST [39] proposed a self-training method, and further improved the performance with space prior information. For a fair comparison, we show the results that only use self-training. With VGG16, we can get 10.4% improvement. Therefore, we can find without bidirectional learning, the self-training method is not enough to achieve a good performance.\n\nIn Table 6, we present the adaptation result on the task \"SYNTHIA to Cityscapes\" for both ResNet101 and VGG16.\n\nThe domain gap between SYNTHIA and Cityscapes is much larger than that of GTA5 and Cityscapes, and their categories are not fully overlapped. As the baseline results [33,19] chosen for ResNet101 only use 13 categories, we also list results for the 13 categories for a fair comparison. We can find from Table 6, as the domain gap increases, the adaptation result for Cityscapes is much worse compared to the result in Table 5. For exam-ple, the category like 'road', 'sidewalk' and 'car' are more than 10% worse. And this problem will have a bad impact on the SSL because of the lower prediction confidence. But we can still achieve at least 4% better than most of other results given by [37,39,36,33]. Performance Gap to Upper Bound. We use the target dataset with ground truth labels to train a segmentation model, which shares the same backbone that we used, to get the upper-bound result. For \"GTA5 to Cityscapes\" with 19 categories, the upper bounds are 65.1 and 60.3 for ResNet101 and VGG16 respectively. For \"SYNTHIA to Cityscapes\" with 13 categories for ResNet101 and 16 categories for VGG16, the upper bounds are 71.7 and 59.5. For our method, although the performance gap is 16.6 at least, it has been reduced significantly compared to other methods. However, it means there is still big room to improve the performance. We leave it in future work.\n\n\nConclusion\n\nIn this paper, we propose a bidirectional learning method with self-supervised learning for segmentation adaptation problem. We show via a lot of experiments that segmentation performance for real dataset can be improved when the model is trained bidirectionally and achieve the stateof-the-art result for multiple tasks with different networks.\n\nFigure 1 :\n1Sequential Learning vs Bidirectional Learning\n\nFigure 2 :\n2Self-supervised learning process Algorithm 1 Training process of our network Input: (S, Y S ), (T , T ssl = \u2205), M (0) Output: M (K)\n\nFigure 3 :\n3Network\n\nFigure 4 :\n4Segmentation result for each step in bidirectional learning\n\n\nper (M(S), M(S )) = \u03bb per E I S \u223cS ||M(I S ) \u2212 M((I S ))|| 1 + \u03bb per recon E I S \u223cS [||M(F \u22121 ((I S ))) \u2212 M(I S )|| 1 ] Due to the symmetry, per (M(T ), M(T )) (shown in Equation 2) can be defined in a similar way.\n\ni\n(F (k) ) for k = 1, 2 and i = 0, 1, 2 refers to the model of k-th iteration for the outer loop and i-th iteration for the inner loop in Algorithm 1.\n\nFigure 5 :\n5Relationship between pixel ratio and the prediction confidence which outperforms the results in the first iteration. From the segmentation results shown in\n\nTable 1 :\n1Performance of bidirectinal learningGTA5 \u2192 Cityscapes \nmodel \nmIoU \nM (0) \n33.6 \nM (1) \n40.9 \n\n\nTable 3 :\n3Influence of thresholdGTA5 \u2192 Cityscapes \nmodel \nthreshold mIoU \nM \n\n(1) \n\n1 (F (1) ) \n0.95 \n45.7 \nM \n\n(1) \n\n1 (F (1) ) \n0.9 \n46.8 \nM \n\n(1) \n\n1 (F (1) ) \n0.8 \n46.4 \nM \n\n(1) \n\n\n\nTable 4 :\n4Influence of NGTA5 \u2192 Cityscapes \nmodel \npixel ratio mIoU \nM \n\n(1) \n0 \n\n66% \n40.9 \nM \n\n(1) \n\n\n\nTable 2 :\n2Performance of bidirectional learning with self-supervised learningGTA5 \u2192 Cityscapes \n\nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nt-light \nt-sign \nvegetation \n\nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorbike \nbicycle \n\nmIoU \nM (0) \n69.0 12.7 69.5 9.9 19.5 22.8 31.7 15.3 73.9 11.3 67.2 54.7 23.9 53.4 29.7 4.6 11.6 26.1 32.5 33.6 \n\nk = 1 \n\nM \n\n\n\nTable 5 :\n5Comparison results from GTA5 to Cityscapes GTA5 \u2192 CityscapesOracle \nMethod \nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nt-light \nt-sign \nvegetation \n\nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorbike \nbicycle \n\nmIoU \n\nResNet101[11] \n65.1 \n\nCycada[12] \n86.7 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19 65.0 12.0 28.6 4.5 31.1 42.0 42.7 \nAdaptSegNet[33] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 \nDCAN[36] \n85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.50 26.9 11.6 41.7 \nCLAN[19] \n87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 \nOurs \n91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 \n\nVGG16[32] \n60.3 \n\nCurriculum[37] 74.9 22.0 71.7 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 16.6 28.9 \nCBST[39] \n66.7 26.8 73.7 14.8 9.5 28.3 25.9 10.1 75.5 15.7 51.6 47.2 6.2 71.9 3.7 2.2 5.4 18.9 32.4 30.9 \nCycada[12] \n85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5 9.8 \n0 \n35.4 \nDCAN[36] \n82.3 26.7 77.4 23.7 20.5 20.4 30.3 15.9 80.9 25.4 69.5 52.6 11.1 79.6 24.9 21.2 1.30 17.0 6.70 36.2 \nCLAN[19] \n88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8 7.9 80.5 26.6 29.9 0.0 10.7 0.0 36.6 \nOurs \n89.2 40.9 81.2 29.1 19.2 14.2 29.0 19.6 83.7 35.9 80.7 54.7 23.3 82.7 25.8 28.0 2.3 25.7 19.9 41.3 \n\n\n\nTable 6 :\n6Comparison results from SYNTHIA to CityscapesSYNTHIA \u2192 Cityscapes \n\nOracle \nMethod \nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nt-light \nt-sign \nvegetation \n\nsky \nperson \nrider \ncar \nbus \nmotorbike \nbicycle \n\nmIoU \n\nResNet101[11] \n71.7 \n\nAdaptSegNet[33] 79.2 37.2 78.8 \n-\n-\n-\n9.9 \n10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3 \n45.9 \nCLAN[19] \n81.3 37.0 80.1 \n-\n-\n-\n16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7 \n47.8 \nOurs \n86.0 46.7 80.3 \n-\n-\n-\n14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3 \n51.4 \n\nVGG16[32] \n59.5 \n\nFCN wild[13] \n11.5 19.6 30.8 \n4.4 \n0.0 20.3 \n0.1 \n11.7 42.3 68.7 51.2 \n3.8 \n54.0 \n3.2 \n0.2 \n0.6 \n20.2 \nCurriculum[37] \n65.2 26.1 74.9 \n0.1 \n0.5 10.7 \n3.5 \n3.0 \n76.1 70.6 47.1 \n8.2 \n43.2 20.7 \n0.7 \n13.1 \n29.0 \nCBST[39] \n69.6 28.7 69.5 12.1 0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 \n6.6 \n3.7 \n32.4 \n35.4 \nDCAN[36] \n79.9 30.4 70.8 \n1.6 \n0.6 22.3 \n6.7 \n23.0 76.9 73.9 41.9 16.7 61.7 11.5 10.3 38.6 \n35.4 \nOurs \n72.0 30.3 74.5 \n0.1 \n0.3 24.6 10.2 25.2 80.5 80.0 54.7 23.2 72.7 24.0 \n7.5 \n44.9 \n39.0 \n\n\nAcknowledgmentThis work was partially funded by NSF awards IIS-1546305 and IIS-1637941.\nSegmentation and recognition using structure from motion point clouds. G J Brostow, J Shotton, J Fauqueur, R Cipolla, ECCV (1). G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla. Seg- mentation and recognition using structure from motion point clouds. In ECCV (1), pages 44-57, 2008. 2\n\nAutodial: Automatic domain alignment layers. F M Carlucci, L Porzi, B Caputo, E Ricci, S R Bul\u00f2, ICCV. 1F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bul\u00f2. Autodial: Automatic domain alignment layers. In ICCV, pages 5077-5085, 2017. 1, 2\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE transactions on pattern analysis and machine intelligence. 407L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. IEEE transactions on pattern analysis and ma- chine intelligence, 40(4):834-848, 2018. 5, 7\n\nReweighted adversarial adaptation network for unsupervised domain adaptation. Q Chen, Y Liu, Z Wang, I Wassell, K Chetty, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQ. Chen, Y. Liu, Z. Wang, I. Wassell, and K. Chetty. Re- weighted adversarial adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7976- 7985, 2018. 2\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition7M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213-3223, 2016. 2, 5, 7\n\nDomain stylization: A strong, simple baseline for synthetic to real image domain adaptation. A Dundar, M.-Y Liu, T.-C Wang, J Zedlewski, J Kautz, arXiv:1807.09384arXiv preprintA. Dundar, M.-Y. Liu, T.-C. Wang, J. Zedlewski, and J. Kautz. Domain stylization: A strong, simple baseline for synthetic to real image domain adaptation. arXiv preprint arXiv:1807.09384, 2018. 3\n\nThe elements of statistical learning. J Friedman, T Hastie, R Tibshirani, Springer series in statistics. New York, NY, USA1J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, NY, USA:, 2001. 1\n\nDaml: Domain adaptation metric learning. B Geng, D Tao, C Xu, IEEE Transactions on Image Processing. 2010B. Geng, D. Tao, and C. Xu. Daml: Domain adaptation metric learning. IEEE Transactions on Image Processing, 20(10):2980-2989, 2011. 2\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014. 2\n\nDual learning for machine translation. D He, Y Xia, T Qin, L Wang, N Yu, T Liu, W.-Y Ma, Advances in Neural Information Processing Systems. D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pages 820-828, 2016. 3\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition7K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770-778, 2016. 5, 7, 8\n\nJ Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A A Efros, T Darrell, arXiv:1711.03213Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprintJ. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adver- sarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. 1, 2, 3, 8\n\nJ Hoffman, D Wang, F Yu, T Darrell, arXiv:1612.02649Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. 2arXiv preprintJ. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adapta- tion. arXiv preprint arXiv:1612.02649, 2016. 2, 8\n\nMultimodal unsupervised image-to-image translation. X Huang, M.-Y Liu, S Belongie, J Kautz, arXiv:1804.0473223arXiv preprintX. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. arXiv preprint arXiv:1804.04732, 2018. 2, 3\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012. 7\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998. 1, 2\n\nUnsupervised image-toimage translation networks. M.-Y Liu, T Breuel, J Kautz, Advances in Neural Information Processing Systems. M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to- image translation networks. In Advances in Neural Informa- tion Processing Systems, pages 700-708, 2017. 2\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition17J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3431-3440, 2015. 1, 7\n\nTaking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. Y Luo, L Zheng, T Guan, J Yu, Y Yang, arXiv:1809.09478arXiv preprintY. Luo, L. Zheng, T. Guan, J. Yu, and Y. Yang. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. arXiv preprint arXiv:1809.09478, 2018. 8\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Y Ng, Proc. icml. icml30A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlin- earities improve neural network acoustic models. In Proc. icml, volume 30, page 3, 2013. 7\n\nBoosting domain adaptation by discovering latent domains. M Mancini, L Porzi, S R Bul\u00f2, B Caputo, E Ricci, arXiv:1805.01386arXiv preprintM. Mancini, L. Porzi, S. R. Bul\u00f2, B. Caputo, and E. Ricci. Boosting domain adaptation by discovering latent domains. arXiv preprint arXiv:1805.01386, 2018. 2\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, NIPS workshop on deep learning and unsupervised feature learning. 2011Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised fea- ture learning. In NIPS workshop on deep learning and un- supervised feature learning, volume 2011, page 5, 2011. 1, 2\n\nBi-directional neural machine translation with synthetic parallel data. X Niu, M Denkowski, M Carpuat, arXiv:1805.11213arXiv preprintX. Niu, M. Denkowski, and M. Carpuat. Bi-directional neu- ral machine translation with synthetic parallel data. arXiv preprint arXiv:1805.11213, 2018. 3\n\nVisual domain adaptation: A survey of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE signal processing magazine. 323V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do- main adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53-69, 2015. 2\n\nS Pontes-Filho, M Liwicki, arXiv:1805.08006Bidirectional learning for robust neural networks. arXiv preprintS. Pontes-Filho and M. Liwicki. Bidirectional learning for robust neural networks. arXiv preprint arXiv:1805.08006, 2018. 3\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintA. Radford, L. Metz, and S. Chintala. Unsupervised repre- sentation learning with deep convolutional generative adver- sarial networks. arXiv preprint arXiv:1511.06434, 2015. 7\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, European Conference on Computer Vision. Springer7S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102-118. Springer, 2016. 1, 2, 5, 7\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, J Materzynska, D Vazquez, A M Lopez, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionG. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition, pages 3234-3243, 2016. 1, 2, 7\n\nP Russo, F M Carlucci, T Tommasi, B Caputo, arXiv:1705.08824From source to target and back: symmetric bi-directional adaptive gan. 3arXiv preprintP. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo. From source to target and back: symmetric bi-directional adaptive gan. arXiv preprint arXiv:1705.08824, 3, 2017. 3\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, European conference on computer vision. SpringerK. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi- sual category models to new domains. In European confer- ence on computer vision, pages 213-226. Springer, 2010. 2\n\nEffective use of synthetic data for urban scene semantic segmentation. F S Saleh, M S Aliakbarian, M Salzmann, L Petersson, J M Alvarez, European Conference on Computer Vision. ChamSpringerF. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, and J. M. Alvarez. Effective use of synthetic data for urban scene semantic segmentation. In European Conference on Computer Vision, pages 86-103. Springer, Cham, 2018. 2\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.15567arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 7, 8\n\nLearning to adapt structured output space for semantic segmentation. Y.-H Tsai, W.-C Hung, S Schulter, K Sohn, M.-H Yang, M Chandraker, arXiv:1802.103492arXiv preprintY.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker. Learning to adapt structured output space for semantic segmentation. arXiv preprint arXiv:1802.10349, 2018. 2, 8\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, Computer Vision and Pattern Recognition (CVPR). 1E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, page 4, 2017. 1, 2\n\nDomain adaptation meets disentangled representation learning and style transfer. H T Vu, C.-C Huang, 13H. T. Vu and C.-C. Huang. Domain adaptation meets dis- entangled representation learning and style transfer. CoRR, 2017. 1, 2, 3\n\nDcan: Dual channel-wise alignment networks for unsupervised scene adaptation. Z Wu, X Han, Y.-L Lin, M G Uzunbas, T Goldstein, S N Lim, L S Davis, arXiv:1804.05827arXiv preprintZ. Wu, X. Han, Y.-L. Lin, M. G. Uzunbas, T. Goldstein, S. N. Lim, and L. S. Davis. Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. arXiv preprint arXiv:1804.05827, 2018. 1, 2, 3, 8\n\nCurriculum domain adaptation for semantic segmentation of urban scenes. Y Zhang, P David, B Gong, The IEEE International Conference on Computer Vision (ICCV). 26Y. Zhang, P. David, and B. Gong. Curriculum domain adap- tation for semantic segmentation of urban scenes. In The IEEE International Conference on Computer Vision (ICCV), volume 2, page 6, 2017. 1, 2, 8\n\nUnpaired imageto-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, 7arXiv preprintJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image- to-image translation using cycle-consistent adversarial net- works. arXiv preprint, 2017. 1, 2, 5, 7\n\nUnsupervised domain adaptation for semantic segmentation via classbalanced self-training. Y Zou, Z Yu, B V Kumar, J Wang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Y. Zou, Z. Yu, B. V. Kumar, and J. Wang. Unsupervised domain adaptation for semantic segmentation via class- balanced self-training. In Proceedings of the European Con- ference on Computer Vision (ECCV), pages 289-305, 2018. 3, 4, 8\n", "annotations": {"author": "[{\"end\":109,\"start\":73},{\"end\":152,\"start\":110},{\"end\":216,\"start\":153}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":82},{\"end\":127,\"start\":118},{\"end\":169,\"start\":158}]", "author_first_name": "[{\"end\":81,\"start\":73},{\"end\":112,\"start\":110},{\"end\":117,\"start\":113},{\"end\":157,\"start\":153}]", "author_affiliation": "[{\"end\":108,\"start\":86},{\"end\":151,\"start\":129},{\"end\":215,\"start\":193}]", "title": "[{\"end\":70,\"start\":1},{\"end\":286,\"start\":217}]", "venue": null, "abstract": "[{\"end\":1700,\"start\":288}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1767,\"start\":1763},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2149,\"start\":2145},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2152,\"start\":2149},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2814,\"start\":2811},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2883,\"start\":2879},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2886,\"start\":2883},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2974,\"start\":2970},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2984,\"start\":2981},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2998,\"start\":2994},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3032,\"start\":3028},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3363,\"start\":3359},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3482,\"start\":3478},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3485,\"start\":3482},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4055,\"start\":4051},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4058,\"start\":4055},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5050,\"start\":5046},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5053,\"start\":5050},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5870,\"start\":5866},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5880,\"start\":5876},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5919,\"start\":5916},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6948,\"start\":6944},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7036,\"start\":7032},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7194,\"start\":7191},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7381,\"start\":7378},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7384,\"start\":7381},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7595,\"start\":7592},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7775,\"start\":7771},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8176,\"start\":8173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8238,\"start\":8234},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8327,\"start\":8323},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8857,\"start\":8853},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8874,\"start\":8870},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9009,\"start\":9006},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9021,\"start\":9018},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9180,\"start\":9176},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9300,\"start\":9296},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9435,\"start\":9431},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9536,\"start\":9532},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9988,\"start\":9984},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9999,\"start\":9995},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10011,\"start\":10007},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10229,\"start\":10225},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10243,\"start\":10239},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10768,\"start\":10765},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10941,\"start\":10937},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11266,\"start\":11262},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11269,\"start\":11266},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11558,\"start\":11554},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11717,\"start\":11713},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12810,\"start\":12806},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14247,\"start\":14243},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15088,\"start\":15084},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15091,\"start\":15088},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17585,\"start\":17581},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22459,\"start\":22455},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22500,\"start\":22497},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22562,\"start\":22558},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22618,\"start\":22615},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22651,\"start\":22647},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23502,\"start\":23499},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23519,\"start\":23516},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23977,\"start\":23974},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24089,\"start\":24086},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24684,\"start\":24681},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24826,\"start\":24823},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29168,\"start\":29165},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29188,\"start\":29184},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29204,\"start\":29200},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29220,\"start\":29216},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29316,\"start\":29312},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29398,\"start\":29394},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29576,\"start\":29572},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29687,\"start\":29683},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29806,\"start\":29802},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30270,\"start\":30267},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30286,\"start\":30282},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31023,\"start\":31019},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31040,\"start\":31036},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31090,\"start\":31087},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31135,\"start\":31131},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31289,\"start\":31285},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31433,\"start\":31430},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31453,\"start\":31450},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32314,\"start\":32310},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32317,\"start\":32314},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32320,\"start\":32317},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32494,\"start\":32490},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32559,\"start\":32555},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32609,\"start\":32605},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32823,\"start\":32819},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32921,\"start\":32917},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33544,\"start\":33540},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33547,\"start\":33544},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34065,\"start\":34061},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34068,\"start\":34065},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34071,\"start\":34068},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34074,\"start\":34071}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35150,\"start\":35092},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35295,\"start\":35151},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35316,\"start\":35296},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35389,\"start\":35317},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35606,\"start\":35390},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35758,\"start\":35607},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35927,\"start\":35759},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36034,\"start\":35928},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36221,\"start\":36035},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36326,\"start\":36222},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36701,\"start\":36327},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38194,\"start\":36702},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39234,\"start\":38195}]", "paragraph": "[{\"end\":2534,\"start\":1716},{\"end\":3106,\"start\":2536},{\"end\":3809,\"start\":3108},{\"end\":4866,\"start\":3811},{\"end\":5642,\"start\":4868},{\"end\":6050,\"start\":5644},{\"end\":6090,\"start\":6052},{\"end\":6277,\"start\":6092},{\"end\":6491,\"start\":6284},{\"end\":6643,\"start\":6493},{\"end\":8428,\"start\":6660},{\"end\":9099,\"start\":8430},{\"end\":9890,\"start\":9101},{\"end\":10727,\"start\":9892},{\"end\":11136,\"start\":10729},{\"end\":12102,\"start\":11138},{\"end\":12763,\"start\":12113},{\"end\":13603,\"start\":12765},{\"end\":14058,\"start\":13605},{\"end\":14142,\"start\":14085},{\"end\":14583,\"start\":14148},{\"end\":14978,\"start\":14638},{\"end\":15490,\"start\":14980},{\"end\":15833,\"start\":15628},{\"end\":16337,\"start\":15849},{\"end\":16896,\"start\":16382},{\"end\":17201,\"start\":16898},{\"end\":17944,\"start\":17279},{\"end\":18987,\"start\":18123},{\"end\":19404,\"start\":19017},{\"end\":19977,\"start\":19534},{\"end\":20854,\"start\":19979},{\"end\":21037,\"start\":20933},{\"end\":21337,\"start\":21103},{\"end\":21737,\"start\":21339},{\"end\":21850,\"start\":21814},{\"end\":22296,\"start\":21852},{\"end\":22726,\"start\":22311},{\"end\":23172,\"start\":22728},{\"end\":23236,\"start\":23232},{\"end\":24101,\"start\":23275},{\"end\":24653,\"start\":24137},{\"end\":25040,\"start\":24655},{\"end\":26022,\"start\":25109},{\"end\":26294,\"start\":26167},{\"end\":27281,\"start\":26296},{\"end\":28976,\"start\":27283},{\"end\":29093,\"start\":28992},{\"end\":29767,\"start\":29095},{\"end\":33260,\"start\":29769},{\"end\":33372,\"start\":33262},{\"end\":34731,\"start\":33374},{\"end\":35091,\"start\":34746}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14147,\"start\":14143},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14637,\"start\":14584},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15627,\"start\":15491},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15848,\"start\":15834},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17278,\"start\":17202},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18122,\"start\":17945},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19533,\"start\":19405},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20932,\"start\":20855},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21102,\"start\":21038},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21813,\"start\":21738},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23231,\"start\":23173},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25080,\"start\":25041},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25108,\"start\":25080},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26066,\"start\":26023},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26139,\"start\":26066}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23383,\"start\":23376},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24307,\"start\":24300},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25299,\"start\":25292},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25486,\"start\":25479},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27331,\"start\":27324},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28658,\"start\":28651},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32083,\"start\":32076},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33272,\"start\":33265},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33683,\"start\":33676},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33798,\"start\":33791}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1714,\"start\":1702},{\"end\":6282,\"start\":6280},{\"attributes\":{\"n\":\"2.\"},\"end\":6658,\"start\":6646},{\"attributes\":{\"n\":\"3.\"},\"end\":12111,\"start\":12105},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14083,\"start\":14061},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16380,\"start\":16340},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19015,\"start\":18990},{\"attributes\":{\"n\":\"4.\"},\"end\":22309,\"start\":22299},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23273,\"start\":23239},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24135,\"start\":24104},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26165,\"start\":26141},{\"attributes\":{\"n\":\"5.\"},\"end\":28990,\"start\":28979},{\"attributes\":{\"n\":\"6.\"},\"end\":34744,\"start\":34734},{\"end\":35103,\"start\":35093},{\"end\":35162,\"start\":35152},{\"end\":35307,\"start\":35297},{\"end\":35328,\"start\":35318},{\"end\":35609,\"start\":35608},{\"end\":35770,\"start\":35760},{\"end\":35938,\"start\":35929},{\"end\":36045,\"start\":36036},{\"end\":36232,\"start\":36223},{\"end\":36337,\"start\":36328},{\"end\":36712,\"start\":36703},{\"end\":38205,\"start\":38196}]", "table": "[{\"end\":36034,\"start\":35976},{\"end\":36221,\"start\":36069},{\"end\":36326,\"start\":36248},{\"end\":36701,\"start\":36406},{\"end\":38194,\"start\":36774},{\"end\":39234,\"start\":38252}]", "figure_caption": "[{\"end\":35150,\"start\":35105},{\"end\":35295,\"start\":35164},{\"end\":35316,\"start\":35309},{\"end\":35389,\"start\":35330},{\"end\":35606,\"start\":35392},{\"end\":35758,\"start\":35610},{\"end\":35927,\"start\":35772},{\"end\":35976,\"start\":35940},{\"end\":36069,\"start\":36047},{\"end\":36248,\"start\":36234},{\"end\":36406,\"start\":36339},{\"end\":36774,\"start\":36714},{\"end\":38252,\"start\":38207}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13211,\"start\":13203},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14142,\"start\":14134},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17809,\"start\":17801},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18271,\"start\":18263},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18594,\"start\":18582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18777,\"start\":18769},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19090,\"start\":19082},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19369,\"start\":19361},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19999,\"start\":19991},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20830,\"start\":20822},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23646,\"start\":23640},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24392,\"start\":24384},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24528,\"start\":24520},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25638,\"start\":25630},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26486,\"start\":26478},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26850,\"start\":26842},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26880,\"start\":26872},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26931,\"start\":26923}]", "bib_author_first_name": "[{\"end\":39395,\"start\":39394},{\"end\":39397,\"start\":39396},{\"end\":39408,\"start\":39407},{\"end\":39419,\"start\":39418},{\"end\":39431,\"start\":39430},{\"end\":39661,\"start\":39660},{\"end\":39663,\"start\":39662},{\"end\":39675,\"start\":39674},{\"end\":39684,\"start\":39683},{\"end\":39694,\"start\":39693},{\"end\":39703,\"start\":39702},{\"end\":39705,\"start\":39704},{\"end\":39982,\"start\":39978},{\"end\":39990,\"start\":39989},{\"end\":40004,\"start\":40003},{\"end\":40016,\"start\":40015},{\"end\":40026,\"start\":40025},{\"end\":40028,\"start\":40027},{\"end\":40460,\"start\":40459},{\"end\":40468,\"start\":40467},{\"end\":40475,\"start\":40474},{\"end\":40483,\"start\":40482},{\"end\":40494,\"start\":40493},{\"end\":40950,\"start\":40949},{\"end\":40960,\"start\":40959},{\"end\":40969,\"start\":40968},{\"end\":40978,\"start\":40977},{\"end\":40989,\"start\":40988},{\"end\":41002,\"start\":41001},{\"end\":41014,\"start\":41013},{\"end\":41024,\"start\":41023},{\"end\":41032,\"start\":41031},{\"end\":41561,\"start\":41560},{\"end\":41574,\"start\":41570},{\"end\":41584,\"start\":41580},{\"end\":41592,\"start\":41591},{\"end\":41605,\"start\":41604},{\"end\":41879,\"start\":41878},{\"end\":41891,\"start\":41890},{\"end\":41901,\"start\":41900},{\"end\":42155,\"start\":42154},{\"end\":42163,\"start\":42162},{\"end\":42170,\"start\":42169},{\"end\":42383,\"start\":42382},{\"end\":42397,\"start\":42396},{\"end\":42414,\"start\":42413},{\"end\":42423,\"start\":42422},{\"end\":42429,\"start\":42428},{\"end\":42445,\"start\":42444},{\"end\":42454,\"start\":42453},{\"end\":42467,\"start\":42466},{\"end\":42784,\"start\":42783},{\"end\":42790,\"start\":42789},{\"end\":42797,\"start\":42796},{\"end\":42804,\"start\":42803},{\"end\":42812,\"start\":42811},{\"end\":42818,\"start\":42817},{\"end\":42828,\"start\":42824},{\"end\":43109,\"start\":43108},{\"end\":43115,\"start\":43114},{\"end\":43124,\"start\":43123},{\"end\":43131,\"start\":43130},{\"end\":43479,\"start\":43478},{\"end\":43490,\"start\":43489},{\"end\":43499,\"start\":43498},{\"end\":43510,\"start\":43506},{\"end\":43517,\"start\":43516},{\"end\":43526,\"start\":43525},{\"end\":43536,\"start\":43535},{\"end\":43538,\"start\":43537},{\"end\":43547,\"start\":43546},{\"end\":43845,\"start\":43844},{\"end\":43856,\"start\":43855},{\"end\":43864,\"start\":43863},{\"end\":43870,\"start\":43869},{\"end\":44205,\"start\":44204},{\"end\":44217,\"start\":44213},{\"end\":44224,\"start\":44223},{\"end\":44236,\"start\":44235},{\"end\":44487,\"start\":44486},{\"end\":44501,\"start\":44500},{\"end\":44514,\"start\":44513},{\"end\":44516,\"start\":44515},{\"end\":44850,\"start\":44849},{\"end\":44859,\"start\":44858},{\"end\":44869,\"start\":44868},{\"end\":44879,\"start\":44878},{\"end\":45107,\"start\":45103},{\"end\":45114,\"start\":45113},{\"end\":45124,\"start\":45123},{\"end\":45407,\"start\":45406},{\"end\":45415,\"start\":45414},{\"end\":45428,\"start\":45427},{\"end\":45899,\"start\":45898},{\"end\":45906,\"start\":45905},{\"end\":45915,\"start\":45914},{\"end\":45923,\"start\":45922},{\"end\":45929,\"start\":45928},{\"end\":46230,\"start\":46229},{\"end\":46232,\"start\":46231},{\"end\":46240,\"start\":46239},{\"end\":46242,\"start\":46241},{\"end\":46252,\"start\":46251},{\"end\":46254,\"start\":46253},{\"end\":46486,\"start\":46485},{\"end\":46497,\"start\":46496},{\"end\":46506,\"start\":46505},{\"end\":46508,\"start\":46507},{\"end\":46516,\"start\":46515},{\"end\":46526,\"start\":46525},{\"end\":46793,\"start\":46792},{\"end\":46803,\"start\":46802},{\"end\":46811,\"start\":46810},{\"end\":46821,\"start\":46820},{\"end\":46833,\"start\":46832},{\"end\":46839,\"start\":46838},{\"end\":46841,\"start\":46840},{\"end\":47229,\"start\":47228},{\"end\":47236,\"start\":47235},{\"end\":47249,\"start\":47248},{\"end\":47499,\"start\":47498},{\"end\":47501,\"start\":47500},{\"end\":47510,\"start\":47509},{\"end\":47521,\"start\":47520},{\"end\":47527,\"start\":47526},{\"end\":47738,\"start\":47737},{\"end\":47754,\"start\":47753},{\"end\":48065,\"start\":48064},{\"end\":48076,\"start\":48075},{\"end\":48084,\"start\":48083},{\"end\":48356,\"start\":48355},{\"end\":48358,\"start\":48357},{\"end\":48369,\"start\":48368},{\"end\":48379,\"start\":48378},{\"end\":48387,\"start\":48386},{\"end\":48737,\"start\":48736},{\"end\":48744,\"start\":48743},{\"end\":48755,\"start\":48754},{\"end\":48770,\"start\":48769},{\"end\":48781,\"start\":48780},{\"end\":48783,\"start\":48782},{\"end\":49219,\"start\":49218},{\"end\":49228,\"start\":49227},{\"end\":49230,\"start\":49229},{\"end\":49242,\"start\":49241},{\"end\":49253,\"start\":49252},{\"end\":49582,\"start\":49581},{\"end\":49592,\"start\":49591},{\"end\":49601,\"start\":49600},{\"end\":49610,\"start\":49609},{\"end\":49916,\"start\":49915},{\"end\":49918,\"start\":49917},{\"end\":49927,\"start\":49926},{\"end\":49929,\"start\":49928},{\"end\":49944,\"start\":49943},{\"end\":49956,\"start\":49955},{\"end\":49969,\"start\":49968},{\"end\":49971,\"start\":49970},{\"end\":50333,\"start\":50332},{\"end\":50345,\"start\":50344},{\"end\":50602,\"start\":50598},{\"end\":50613,\"start\":50609},{\"end\":50621,\"start\":50620},{\"end\":50633,\"start\":50632},{\"end\":50644,\"start\":50640},{\"end\":50652,\"start\":50651},{\"end\":50934,\"start\":50933},{\"end\":50943,\"start\":50942},{\"end\":50954,\"start\":50953},{\"end\":50964,\"start\":50963},{\"end\":51281,\"start\":51280},{\"end\":51283,\"start\":51282},{\"end\":51292,\"start\":51288},{\"end\":51511,\"start\":51510},{\"end\":51517,\"start\":51516},{\"end\":51527,\"start\":51523},{\"end\":51534,\"start\":51533},{\"end\":51536,\"start\":51535},{\"end\":51547,\"start\":51546},{\"end\":51560,\"start\":51559},{\"end\":51562,\"start\":51561},{\"end\":51569,\"start\":51568},{\"end\":51571,\"start\":51570},{\"end\":51894,\"start\":51893},{\"end\":51903,\"start\":51902},{\"end\":51912,\"start\":51911},{\"end\":52270,\"start\":52266},{\"end\":52277,\"start\":52276},{\"end\":52285,\"start\":52284},{\"end\":52294,\"start\":52293},{\"end\":52296,\"start\":52295},{\"end\":52575,\"start\":52574},{\"end\":52582,\"start\":52581},{\"end\":52588,\"start\":52587},{\"end\":52590,\"start\":52589},{\"end\":52599,\"start\":52598}]", "bib_author_last_name": "[{\"end\":39405,\"start\":39398},{\"end\":39416,\"start\":39409},{\"end\":39428,\"start\":39420},{\"end\":39439,\"start\":39432},{\"end\":39672,\"start\":39664},{\"end\":39681,\"start\":39676},{\"end\":39691,\"start\":39685},{\"end\":39700,\"start\":39695},{\"end\":39710,\"start\":39706},{\"end\":39987,\"start\":39983},{\"end\":40001,\"start\":39991},{\"end\":40013,\"start\":40005},{\"end\":40023,\"start\":40017},{\"end\":40035,\"start\":40029},{\"end\":40465,\"start\":40461},{\"end\":40472,\"start\":40469},{\"end\":40480,\"start\":40476},{\"end\":40491,\"start\":40484},{\"end\":40501,\"start\":40495},{\"end\":40957,\"start\":40951},{\"end\":40966,\"start\":40961},{\"end\":40975,\"start\":40970},{\"end\":40986,\"start\":40979},{\"end\":40999,\"start\":40990},{\"end\":41011,\"start\":41003},{\"end\":41021,\"start\":41015},{\"end\":41029,\"start\":41025},{\"end\":41040,\"start\":41033},{\"end\":41568,\"start\":41562},{\"end\":41578,\"start\":41575},{\"end\":41589,\"start\":41585},{\"end\":41602,\"start\":41593},{\"end\":41611,\"start\":41606},{\"end\":41888,\"start\":41880},{\"end\":41898,\"start\":41892},{\"end\":41912,\"start\":41902},{\"end\":42160,\"start\":42156},{\"end\":42167,\"start\":42164},{\"end\":42173,\"start\":42171},{\"end\":42394,\"start\":42384},{\"end\":42411,\"start\":42398},{\"end\":42420,\"start\":42415},{\"end\":42426,\"start\":42424},{\"end\":42442,\"start\":42430},{\"end\":42451,\"start\":42446},{\"end\":42464,\"start\":42455},{\"end\":42474,\"start\":42468},{\"end\":42787,\"start\":42785},{\"end\":42794,\"start\":42791},{\"end\":42801,\"start\":42798},{\"end\":42809,\"start\":42805},{\"end\":42815,\"start\":42813},{\"end\":42822,\"start\":42819},{\"end\":42831,\"start\":42829},{\"end\":43112,\"start\":43110},{\"end\":43121,\"start\":43116},{\"end\":43128,\"start\":43125},{\"end\":43135,\"start\":43132},{\"end\":43487,\"start\":43480},{\"end\":43496,\"start\":43491},{\"end\":43504,\"start\":43500},{\"end\":43514,\"start\":43511},{\"end\":43523,\"start\":43518},{\"end\":43533,\"start\":43527},{\"end\":43544,\"start\":43539},{\"end\":43555,\"start\":43548},{\"end\":43853,\"start\":43846},{\"end\":43861,\"start\":43857},{\"end\":43867,\"start\":43865},{\"end\":43878,\"start\":43871},{\"end\":44211,\"start\":44206},{\"end\":44221,\"start\":44218},{\"end\":44233,\"start\":44225},{\"end\":44242,\"start\":44237},{\"end\":44498,\"start\":44488},{\"end\":44511,\"start\":44502},{\"end\":44523,\"start\":44517},{\"end\":44856,\"start\":44851},{\"end\":44866,\"start\":44860},{\"end\":44876,\"start\":44870},{\"end\":44887,\"start\":44880},{\"end\":45111,\"start\":45108},{\"end\":45121,\"start\":45115},{\"end\":45130,\"start\":45125},{\"end\":45412,\"start\":45408},{\"end\":45425,\"start\":45416},{\"end\":45436,\"start\":45429},{\"end\":45903,\"start\":45900},{\"end\":45912,\"start\":45907},{\"end\":45920,\"start\":45916},{\"end\":45926,\"start\":45924},{\"end\":45934,\"start\":45930},{\"end\":46237,\"start\":46233},{\"end\":46249,\"start\":46243},{\"end\":46257,\"start\":46255},{\"end\":46494,\"start\":46487},{\"end\":46503,\"start\":46498},{\"end\":46513,\"start\":46509},{\"end\":46523,\"start\":46517},{\"end\":46532,\"start\":46527},{\"end\":46800,\"start\":46794},{\"end\":46808,\"start\":46804},{\"end\":46818,\"start\":46812},{\"end\":46830,\"start\":46822},{\"end\":46836,\"start\":46834},{\"end\":46844,\"start\":46842},{\"end\":47233,\"start\":47230},{\"end\":47246,\"start\":47237},{\"end\":47257,\"start\":47250},{\"end\":47507,\"start\":47502},{\"end\":47518,\"start\":47511},{\"end\":47524,\"start\":47522},{\"end\":47537,\"start\":47528},{\"end\":47751,\"start\":47739},{\"end\":47762,\"start\":47755},{\"end\":48073,\"start\":48066},{\"end\":48081,\"start\":48077},{\"end\":48093,\"start\":48085},{\"end\":48366,\"start\":48359},{\"end\":48376,\"start\":48370},{\"end\":48384,\"start\":48380},{\"end\":48394,\"start\":48388},{\"end\":48741,\"start\":48738},{\"end\":48752,\"start\":48745},{\"end\":48767,\"start\":48756},{\"end\":48778,\"start\":48771},{\"end\":48789,\"start\":48784},{\"end\":49225,\"start\":49220},{\"end\":49239,\"start\":49231},{\"end\":49250,\"start\":49243},{\"end\":49260,\"start\":49254},{\"end\":49589,\"start\":49583},{\"end\":49598,\"start\":49593},{\"end\":49607,\"start\":49602},{\"end\":49618,\"start\":49611},{\"end\":49924,\"start\":49919},{\"end\":49941,\"start\":49930},{\"end\":49953,\"start\":49945},{\"end\":49966,\"start\":49957},{\"end\":49979,\"start\":49972},{\"end\":50342,\"start\":50334},{\"end\":50355,\"start\":50346},{\"end\":50607,\"start\":50603},{\"end\":50618,\"start\":50614},{\"end\":50630,\"start\":50622},{\"end\":50638,\"start\":50634},{\"end\":50649,\"start\":50645},{\"end\":50663,\"start\":50653},{\"end\":50940,\"start\":50935},{\"end\":50951,\"start\":50944},{\"end\":50961,\"start\":50955},{\"end\":50972,\"start\":50965},{\"end\":51286,\"start\":51284},{\"end\":51298,\"start\":51293},{\"end\":51514,\"start\":51512},{\"end\":51521,\"start\":51518},{\"end\":51531,\"start\":51528},{\"end\":51544,\"start\":51537},{\"end\":51557,\"start\":51548},{\"end\":51566,\"start\":51563},{\"end\":51577,\"start\":51572},{\"end\":51900,\"start\":51895},{\"end\":51909,\"start\":51904},{\"end\":51917,\"start\":51913},{\"end\":52274,\"start\":52271},{\"end\":52282,\"start\":52278},{\"end\":52291,\"start\":52286},{\"end\":52302,\"start\":52297},{\"end\":52579,\"start\":52576},{\"end\":52585,\"start\":52583},{\"end\":52596,\"start\":52591},{\"end\":52604,\"start\":52600}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13813737},\"end\":39613,\"start\":39323},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4377230},\"end\":39863,\"start\":39615},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3429309},\"end\":40379,\"start\":39865},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":253513273},\"end\":40884,\"start\":40381},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":502946},\"end\":41465,\"start\":40886},{\"attributes\":{\"doi\":\"arXiv:1807.09384\",\"id\":\"b5\"},\"end\":41838,\"start\":41467},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":46701966},\"end\":42111,\"start\":41840},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13985613},\"end\":42351,\"start\":42113},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1033682},\"end\":42742,\"start\":42353},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5758868},\"end\":43060,\"start\":42744},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":43476,\"start\":43062},{\"attributes\":{\"doi\":\"arXiv:1711.03213\",\"id\":\"b11\"},\"end\":43842,\"start\":43478},{\"attributes\":{\"doi\":\"arXiv:1612.02649\",\"id\":\"b12\"},\"end\":44150,\"start\":43844},{\"attributes\":{\"doi\":\"arXiv:1804.04732\",\"id\":\"b13\"},\"end\":44419,\"start\":44152},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195908774},\"end\":44766,\"start\":44421},{\"attributes\":{\"id\":\"b15\"},\"end\":45052,\"start\":44768},{\"attributes\":{\"id\":\"b16\"},\"end\":45348,\"start\":45054},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1629541},\"end\":45787,\"start\":45350},{\"attributes\":{\"doi\":\"arXiv:1809.09478\",\"id\":\"b18\"},\"end\":46162,\"start\":45789},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16489696},\"end\":46425,\"start\":46164},{\"attributes\":{\"doi\":\"arXiv:1805.01386\",\"id\":\"b20\"},\"end\":46721,\"start\":46427},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16852518},\"end\":47154,\"start\":46723},{\"attributes\":{\"doi\":\"arXiv:1805.11213\",\"id\":\"b22\"},\"end\":47441,\"start\":47156},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":918513},\"end\":47735,\"start\":47443},{\"attributes\":{\"doi\":\"arXiv:1805.08006\",\"id\":\"b24\"},\"end\":47968,\"start\":47737},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b25\"},\"end\":48301,\"start\":47970},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":5844139},\"end\":48631,\"start\":48303},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206594095},\"end\":49216,\"start\":48633},{\"attributes\":{\"doi\":\"arXiv:1705.08824\",\"id\":\"b28\"},\"end\":49531,\"start\":49218},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7534823},\"end\":49842,\"start\":49533},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":49863467},\"end\":50262,\"start\":49844},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b31\"},\"end\":50527,\"start\":50264},{\"attributes\":{\"doi\":\"arXiv:1802.10349\",\"id\":\"b32\"},\"end\":50885,\"start\":50529},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4357800},\"end\":51197,\"start\":50887},{\"attributes\":{\"id\":\"b34\"},\"end\":51430,\"start\":51199},{\"attributes\":{\"doi\":\"arXiv:1804.05827\",\"id\":\"b35\"},\"end\":51819,\"start\":51432},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":11824004},\"end\":52184,\"start\":51821},{\"attributes\":{\"id\":\"b37\"},\"end\":52482,\"start\":52186},{\"attributes\":{\"id\":\"b38\"},\"end\":52953,\"start\":52484}]", "bib_title": "[{\"end\":39392,\"start\":39323},{\"end\":39658,\"start\":39615},{\"end\":39976,\"start\":39865},{\"end\":40457,\"start\":40381},{\"end\":40947,\"start\":40886},{\"end\":41876,\"start\":41840},{\"end\":42152,\"start\":42113},{\"end\":42380,\"start\":42353},{\"end\":42781,\"start\":42744},{\"end\":43106,\"start\":43062},{\"end\":44484,\"start\":44421},{\"end\":45101,\"start\":45054},{\"end\":45404,\"start\":45350},{\"end\":46227,\"start\":46164},{\"end\":46790,\"start\":46723},{\"end\":47496,\"start\":47443},{\"end\":48353,\"start\":48303},{\"end\":48734,\"start\":48633},{\"end\":49579,\"start\":49533},{\"end\":49913,\"start\":49844},{\"end\":50931,\"start\":50887},{\"end\":51891,\"start\":51821},{\"end\":52572,\"start\":52484}]", "bib_author": "[{\"end\":39407,\"start\":39394},{\"end\":39418,\"start\":39407},{\"end\":39430,\"start\":39418},{\"end\":39441,\"start\":39430},{\"end\":39674,\"start\":39660},{\"end\":39683,\"start\":39674},{\"end\":39693,\"start\":39683},{\"end\":39702,\"start\":39693},{\"end\":39712,\"start\":39702},{\"end\":39989,\"start\":39978},{\"end\":40003,\"start\":39989},{\"end\":40015,\"start\":40003},{\"end\":40025,\"start\":40015},{\"end\":40037,\"start\":40025},{\"end\":40467,\"start\":40459},{\"end\":40474,\"start\":40467},{\"end\":40482,\"start\":40474},{\"end\":40493,\"start\":40482},{\"end\":40503,\"start\":40493},{\"end\":40959,\"start\":40949},{\"end\":40968,\"start\":40959},{\"end\":40977,\"start\":40968},{\"end\":40988,\"start\":40977},{\"end\":41001,\"start\":40988},{\"end\":41013,\"start\":41001},{\"end\":41023,\"start\":41013},{\"end\":41031,\"start\":41023},{\"end\":41042,\"start\":41031},{\"end\":41570,\"start\":41560},{\"end\":41580,\"start\":41570},{\"end\":41591,\"start\":41580},{\"end\":41604,\"start\":41591},{\"end\":41613,\"start\":41604},{\"end\":41890,\"start\":41878},{\"end\":41900,\"start\":41890},{\"end\":41914,\"start\":41900},{\"end\":42162,\"start\":42154},{\"end\":42169,\"start\":42162},{\"end\":42175,\"start\":42169},{\"end\":42396,\"start\":42382},{\"end\":42413,\"start\":42396},{\"end\":42422,\"start\":42413},{\"end\":42428,\"start\":42422},{\"end\":42444,\"start\":42428},{\"end\":42453,\"start\":42444},{\"end\":42466,\"start\":42453},{\"end\":42476,\"start\":42466},{\"end\":42789,\"start\":42783},{\"end\":42796,\"start\":42789},{\"end\":42803,\"start\":42796},{\"end\":42811,\"start\":42803},{\"end\":42817,\"start\":42811},{\"end\":42824,\"start\":42817},{\"end\":42833,\"start\":42824},{\"end\":43114,\"start\":43108},{\"end\":43123,\"start\":43114},{\"end\":43130,\"start\":43123},{\"end\":43137,\"start\":43130},{\"end\":43489,\"start\":43478},{\"end\":43498,\"start\":43489},{\"end\":43506,\"start\":43498},{\"end\":43516,\"start\":43506},{\"end\":43525,\"start\":43516},{\"end\":43535,\"start\":43525},{\"end\":43546,\"start\":43535},{\"end\":43557,\"start\":43546},{\"end\":43855,\"start\":43844},{\"end\":43863,\"start\":43855},{\"end\":43869,\"start\":43863},{\"end\":43880,\"start\":43869},{\"end\":44213,\"start\":44204},{\"end\":44223,\"start\":44213},{\"end\":44235,\"start\":44223},{\"end\":44244,\"start\":44235},{\"end\":44500,\"start\":44486},{\"end\":44513,\"start\":44500},{\"end\":44525,\"start\":44513},{\"end\":44858,\"start\":44849},{\"end\":44868,\"start\":44858},{\"end\":44878,\"start\":44868},{\"end\":44889,\"start\":44878},{\"end\":45113,\"start\":45103},{\"end\":45123,\"start\":45113},{\"end\":45132,\"start\":45123},{\"end\":45414,\"start\":45406},{\"end\":45427,\"start\":45414},{\"end\":45438,\"start\":45427},{\"end\":45905,\"start\":45898},{\"end\":45914,\"start\":45905},{\"end\":45922,\"start\":45914},{\"end\":45928,\"start\":45922},{\"end\":45936,\"start\":45928},{\"end\":46239,\"start\":46229},{\"end\":46251,\"start\":46239},{\"end\":46259,\"start\":46251},{\"end\":46496,\"start\":46485},{\"end\":46505,\"start\":46496},{\"end\":46515,\"start\":46505},{\"end\":46525,\"start\":46515},{\"end\":46534,\"start\":46525},{\"end\":46802,\"start\":46792},{\"end\":46810,\"start\":46802},{\"end\":46820,\"start\":46810},{\"end\":46832,\"start\":46820},{\"end\":46838,\"start\":46832},{\"end\":46846,\"start\":46838},{\"end\":47235,\"start\":47228},{\"end\":47248,\"start\":47235},{\"end\":47259,\"start\":47248},{\"end\":47509,\"start\":47498},{\"end\":47520,\"start\":47509},{\"end\":47526,\"start\":47520},{\"end\":47539,\"start\":47526},{\"end\":47753,\"start\":47737},{\"end\":47764,\"start\":47753},{\"end\":48075,\"start\":48064},{\"end\":48083,\"start\":48075},{\"end\":48095,\"start\":48083},{\"end\":48368,\"start\":48355},{\"end\":48378,\"start\":48368},{\"end\":48386,\"start\":48378},{\"end\":48396,\"start\":48386},{\"end\":48743,\"start\":48736},{\"end\":48754,\"start\":48743},{\"end\":48769,\"start\":48754},{\"end\":48780,\"start\":48769},{\"end\":48791,\"start\":48780},{\"end\":49227,\"start\":49218},{\"end\":49241,\"start\":49227},{\"end\":49252,\"start\":49241},{\"end\":49262,\"start\":49252},{\"end\":49591,\"start\":49581},{\"end\":49600,\"start\":49591},{\"end\":49609,\"start\":49600},{\"end\":49620,\"start\":49609},{\"end\":49926,\"start\":49915},{\"end\":49943,\"start\":49926},{\"end\":49955,\"start\":49943},{\"end\":49968,\"start\":49955},{\"end\":49981,\"start\":49968},{\"end\":50344,\"start\":50332},{\"end\":50357,\"start\":50344},{\"end\":50609,\"start\":50598},{\"end\":50620,\"start\":50609},{\"end\":50632,\"start\":50620},{\"end\":50640,\"start\":50632},{\"end\":50651,\"start\":50640},{\"end\":50665,\"start\":50651},{\"end\":50942,\"start\":50933},{\"end\":50953,\"start\":50942},{\"end\":50963,\"start\":50953},{\"end\":50974,\"start\":50963},{\"end\":51288,\"start\":51280},{\"end\":51300,\"start\":51288},{\"end\":51516,\"start\":51510},{\"end\":51523,\"start\":51516},{\"end\":51533,\"start\":51523},{\"end\":51546,\"start\":51533},{\"end\":51559,\"start\":51546},{\"end\":51568,\"start\":51559},{\"end\":51579,\"start\":51568},{\"end\":51902,\"start\":51893},{\"end\":51911,\"start\":51902},{\"end\":51919,\"start\":51911},{\"end\":52276,\"start\":52266},{\"end\":52284,\"start\":52276},{\"end\":52293,\"start\":52284},{\"end\":52304,\"start\":52293},{\"end\":52581,\"start\":52574},{\"end\":52587,\"start\":52581},{\"end\":52598,\"start\":52587},{\"end\":52606,\"start\":52598}]", "bib_venue": "[{\"end\":40644,\"start\":40582},{\"end\":41183,\"start\":41121},{\"end\":41962,\"start\":41945},{\"end\":43278,\"start\":43216},{\"end\":45579,\"start\":45517},{\"end\":46275,\"start\":46271},{\"end\":48932,\"start\":48870},{\"end\":50025,\"start\":50021},{\"end\":52721,\"start\":52672},{\"end\":39449,\"start\":39441},{\"end\":39716,\"start\":39712},{\"end\":40099,\"start\":40037},{\"end\":40580,\"start\":40503},{\"end\":41119,\"start\":41042},{\"end\":41558,\"start\":41467},{\"end\":41943,\"start\":41914},{\"end\":42212,\"start\":42175},{\"end\":42525,\"start\":42476},{\"end\":42882,\"start\":42833},{\"end\":43214,\"start\":43137},{\"end\":43627,\"start\":43573},{\"end\":43969,\"start\":43896},{\"end\":44202,\"start\":44152},{\"end\":44574,\"start\":44525},{\"end\":44847,\"start\":44768},{\"end\":45181,\"start\":45132},{\"end\":45515,\"start\":45438},{\"end\":45896,\"start\":45789},{\"end\":46269,\"start\":46259},{\"end\":46483,\"start\":46427},{\"end\":46910,\"start\":46846},{\"end\":47226,\"start\":47156},{\"end\":47570,\"start\":47539},{\"end\":47829,\"start\":47780},{\"end\":48062,\"start\":47970},{\"end\":48434,\"start\":48396},{\"end\":48868,\"start\":48791},{\"end\":49347,\"start\":49278},{\"end\":49658,\"start\":49620},{\"end\":50019,\"start\":49981},{\"end\":50330,\"start\":50264},{\"end\":50596,\"start\":50529},{\"end\":51020,\"start\":50974},{\"end\":51278,\"start\":51199},{\"end\":51508,\"start\":51432},{\"end\":51978,\"start\":51919},{\"end\":52264,\"start\":52186},{\"end\":52670,\"start\":52606}]"}}}, "year": 2023, "month": 12, "day": 17}