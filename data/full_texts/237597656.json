{"id": 237597656, "updated": "2023-10-05 23:42:08.572", "metadata": {"title": "FedParking: A Federated Learning based Parking Space Estimation with Parked Vehicle assisted Edge Computing", "authors": "[{\"first\":\"Xumin\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Peichun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Rong\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Yuan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Kan\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Shengli\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "IEEE Transactions on Vehicular Technology", "journal": "IEEE Transactions on Vehicular Technology", "publication_date": {"year": 2021, "month": 10, "day": 19}, "abstract": "As a distributed learning approach, federated learning trains a shared learning model over distributed datasets while preserving the training data privacy. We extend the application of federated learning to parking management and introduce FedParking in which Parking Lot Operators (PLOs) collaborate to train a long short-term memory model for parking space estimation without exchanging the raw data. Furthermore, we investigate the management of Parked Vehicle assisted Edge Computing (PVEC) by FedParking. In PVEC, different PLOs recruit PVs as edge computing nodes for offloading services through an incentive mechanism, which is designed according to the computation demand and parking capacity constraints derived from FedParking. We formulate the interactions among the PLOs and vehicles as a multi-lead multi-follower Stackelberg game. Considering the dynamic arrivals of the vehicles and time-varying parking capacity constraints, we present a multi-agent deep reinforcement learning approach to gradually reach the Stackelberg equilibrium in a distributed yet privacy-preserving manner. Finally, numerical results are provided to demonstrate the effectiveness and efficiency of our scheme.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2110.12876", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2110-12876", "doi": "10.1109/tvt.2021.3098170"}}, "content": {"source": {"pdf_hash": "0fe3a0aecbaf8c38ba718c3b4a86d90c9640fd8b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2110.12876v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2110.12876", "status": "GREEN"}}, "grobid": {"id": "d6e68b428b7ea86166671583c2286d32c22a432f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0fe3a0aecbaf8c38ba718c3b4a86d90c9640fd8b.txt", "contents": "\nFedParking: A Federated Learning based Parking Space Estimation with Parked Vehicle assisted Edge Computing\n19 Oct 2021\n\nXumin Huang \nStudent Member, IEEEPeichun Li \nMember, IEEERong Yu \nSenior Member, IEEEYuan Wu \nKan Xie \nFellow, IEEEShengli Xie \nFedParking: A Federated Learning based Parking Space Estimation with Parked Vehicle assisted Edge Computing\n19 Oct 2021\nAs a distributed learning approach, federated learning trains a shared learning model over distributed datasets while preserving the training data privacy. We extend the application of federated learning to parking management and introduce FedParking in which Parking Lot Operators (PLOs) collaborate to train a long short-term memory model for parking space estimation without exchanging the raw data. Furthermore, we investigate the management of Parked Vehicle assisted Edge Computing (PVEC) by FedParking. In PVEC, different PLOs recruit PVs as edge computing nodes for offloading services through an incentive mechanism, which is designed according to the computation demand and parking capacity constraints derived from FedParking. We formulate the interactions among the PLOs and vehicles as a multi-lead multi-follower Stackelberg game. Considering the dynamic arrivals of the vehicles and timevarying parking capacity constraints, we present a multi-agent deep reinforcement learning approach to gradually reach the Stackelberg equilibrium in a distributed yet privacy-preserving manner. Finally, numerical results are provided to demonstrate the effectiveness and efficiency of our scheme.Index Terms-Federated learning, parked vehicle assisted edge computing, deep reinforcement learning and Stackelberg game.\n\nI. INTRODUCTION\n\nNowadays, federated learning has been envisioned as a distributed learning framework in which devices cooperate to build and update a globally shared learning model by supporting training the global model over distributed datasets [1], [2]. Federated learning enables each device as a data owner to locally train the global model with individually collected data. This approach exchanges model parameters instead of the actual training data and preserves data privacy of the devices. Due to the significant privacy-friendly characteristic, federated learning is applied to diverse domains, e.g., mobile keyboard prediction [3], wearable activity recognition [4] and content caching placement [5]. The promising approach is also integrated into vehicular networks to forecast traffic information such as traffic flow [6] and traffic speed [7] while guaranteeing reliable data privacy preservation during the forecast. Motivated by the current works, we extend the application of federated learning to parking management in smart cities, and develop a federated learning based parking space estimation scheme named by FedParking.\n\nFedparking allows multiple Parking Lot Operators (PLOs) to collaboratively learn a Long Short-Term Memory (LSTM) model for parking space estimation while keeping their training data secure and private. On one hand, a PLO should predict the number of free parking spaces in real time for traffic management at a parking lot. To achieve parking space estimation, researchers have presented a variety of applicable methods. Since prediction of parking space availability including parking occupancy and duration of occupancy can be regarded as a time-series problem, time series prediction is utilized as a promising solution to make full use of prior knowledge from historical data and obtain accurate prediction results. Furthermore, the time-series prediction problem on parking space estimation is characterized by the long-term trend and cyclical fluctuations, and LSTM as a powerful variant of the typical recurrent neural network is particularly suitable to tackle it [8]. Thus, we adopt a typical LSTM model as the global model in Fedparking, which is shared to each PLO to achieve an accurate estimation of the parking spaces.\n\nOn the other hand, the global model can simply feed off the training data extracted from parking space information of all the PLOs. In this regard, a naive learning approach usually requires each PLO to upload the training data and utilizes the collected data for centralized training. However, the traditional learning approach puts individual privacy of vehicles at risk, since the parking space information records basic profiles of daily parking requests such as arrival and departure time of the vehicles. The parking space information also records time-varying parking price and occupancy of the parking lots. For commercial purposes, PLOs would not like to reveal the parking space information to the others to prevent malicious entities which infer the operating mode of the parking lots. This means that a secure and distributed learning approach is necessitated such that different PLOs collaborate to train the global model without potential security vulnerabilities. We pay attention to the recently emerged federated learning that does not rely on sharing training data among the clients and propose FedParking. In FedParking, different PLOs collaborate to train the global model by the local datasets and only feedback the updated model parameters for parameter aggregation. To summarize, we are motivated to study an integration of the federated learning and LSTM to facilitate the FedParking design with efficiency and security guarantee.\n\nBuilding upon the FedParking for predicting the parking spaces at different PLOs, we devise an incentive mechanism for Parked Vehicle assisted Edge Computing (PVEC). As an application of mobile edge computing in Internet of vehicles, PVEC schedules stochastic idle communication and computing resources from Parked Vehicles (PVs). It can significantly establish an available and cost-effective computing resource pool, which alleviates computational workloads and augments system capacity [9]. A PLO acts as an offloading service provider to receive computation-intensive but not delay-sensitive tasks from nearby offloading users, afterwards stimulates vehicles to enter the parking spaces and utilize the on-board resources of the PVs to accomplish the given workloads. To realize the goal, an appropriate incentive mechanism is necessitated for the PLO to provide vehicles with monetary rewards according to the parking capacity and computation demand. Through FedParking, we can estimate the number of available parking spaces of every parking lot which leads to the corresponding parking capacity constraint for each PLO.\n\nAfter that, we formulate the strategic interactions among multiple PLOs and vehicles as a two-stage and noncooperative game, namely, a multi-leader multi-follower Stackelberg game. A PLO is a game leader that provides monetary rewards to attract the vehicles to enter the parking lot and share the idle computing resources in the first stage. In turn, the vehicles are the game followers which act in response to the PLOs' strategies. The Stackelberg game aims to improve the utilities of the PLOs while simultaneously guaranteeing utility maximization for each vehicle. By theoretical analysis, we prove the existence and uniqueness of the Stackelberg equilibrium while neglecting the parking capacity constraints. Furthermore, we apply Deep Reinforcement Learning (DRL) as a solution methodology to reach the Stackelberg equilibrium in practical scenarios where both arrivals of the vehicles and parking capacity constraints of the PLOs are varying over time. In our DRL approach, each PLO is regarded as an agent to learn a near-optimal decision by referring to the historical strategies of the others. The approach depends on iterative interactions among the players and supports distributed decision making instead of holding any prior knowledge of all the players in advance.\n\nThe main contributions of the paper can be summarized as follows.\n\n\u2022 We adopt federated learning and LSTM to promote the collaboration among the PLOs in parking space estimation. In FedParking, each PLO trains a shared LSTM model to forecast available parking spaces by the local data while preserving the training data privacy. Technique details including system model, network entities, workflow and LSTM model design are provided. \u2022 We study the incentive mechanism design of PVEC by FedParking, which puts forward a parking capacity constraint for each PLO. A PLO needs to recruit PVs to complete offloading workloads and an appropriate incentive mechanism is necessitated to maximize the expected profit and simultaneously take into account the parking capacity constraint. After introducing a Stackelberg game model among the PLOs and vehicles, we provide the equilibrium analysis to validate the existence and uniqueness of Stackelberg equilibrium by studying the sub-game perfect equilibrium in each stage. \u2022 We present a DRL based incentive mechanism for the PLOs since the closed-form Stackelberg equilibrium is difficult to acquire in the non-static environment. A multiagent DRL approach is introduced to gradually reach the Stackelberg equilibrium in a distributed yet privacypreserving manner. Our approach avoids collecting the private information of the players of the Stackelberg game and achieves the almost same convergence performance as the centralized approach. The rest of this paper is organized as follows. Section II presents the related work. We introduce the detailed FedParking design based on federated learning and LSTM in Section III. Section IV studies the Stackelberg game based incentive mechanism. Section V provides the theoretical Stackelberg equilibrium analysis without considering the parking capacity constraints. In Section VI, we propose the DRL approach to gradually reach the Stackelberg equilibrium with the given parking capacity constraints. Numerical results are shown in Section VII. Finally, Section VIII concludes this paper and discusses the future direction.\n\n\nII. RELATED WORK\n\n\nA. Federated Learning for Vehicular Networks\n\nAs a privacy-preserving learning approach, federated learning enables the collaborative training of a globally shared learning model without exchanging raw data among the clients. Recently, the promising tool has been applied to vehicular networks for different purposes. For example, federated learning was combined with a gated recurrent unit neural network to encourage multiple organizations to cooperatively predict future traffic flow without privacy leakage [6]. The distributed learning technique was exploited to provide robust privacy-preserving traffic speed forecasting and protection of topological information [7]. To eliminate the vehicular data leakage, the authors in [10] presented a federated learning based collaborative data leakage detection scheme. Besides, federated learning was adopted among mobile vehicles to improve vehicular communications with ultra reliability and low latency [11], and support AI applications such as image classification [12] in vehicular edge computing environment.\n\nMotivated by the above works, we propose an integration of federated learning and LSTM to design FedParking and achieve the PLOs' collaborative parking space estimation. According to a federated learning based framework, various PLOs cooperate with a parameter server to jointly train the LSTM model in an efficient and secure manner. Under the privacy restrictions, FedParking only collects model parameters updated from the PLOs and does not require sharing the datasets of the PLOs with the parameter server. Moreover, owing to the flexibility of the federated learning based framework, FedParking is computation efficient and also compatible with the other machine learning models in addition to the proposed LSTM model for parking space estimation.\n\n\nB. Parked Vehicle Assisted Edge Computing\n\nNowadays, many research efforts have been devoted to optimizing the performance of PVEC with different objectives. For example, a contract based incentive mechanism was designed to incentivize mobile and parked vehicles with different types to contribute their on-board resources for task processing [13]. A contract was correlated with an offloading task and a contract item specifies the required number of computing resources and promising rewards for a vehicle type. The contract assignment between various tasks and vehicles was determined by using matching theory. The similar work was found in [14]. Besides, mobile and parked vehicles cooperate with nearby mobile edge computing servers for admission control [15] and workload processing [16], [17]. The vehicles also receive offloading tasks from unmanned aerial vehicles in smart city [18] and help a local cloudlet cope with traffic packets and establish a distributed citywide traffic management system [19]. In addition to contract theory, game and auction theories have been applied for the incentive mechanism design in PVEC. A Stackelberg game was formulated to jointly employ a set of PVs and a mobile edge computing server for cooperative task processing [20]. A multi-round multi-item parking reservation auction scheme was introduced to guide passing vehicles to specific parking spaces and make use of their processing capability to support proximal offloading services [21]. In spite the above studies, there still exist several important issues to be addressed for the success of PVEC. First, a PLO needs to dynamically predict the number of available parking spaces over time and adjust the incentive mechanism according to a parking capacity constraint and computation demand. Second, previous works have not considered the competition effect among different PLOs, which provide different monetary rewards for arriving vehicles. Due to selfishness in resource usage, the PLOs compete against each other to recruit the PVs and reserve their idle computing resources during the parking time. In turn, parking choices of the vehicles will be influenced by the reward policies of the PLOs. The vehicles are rational to decide which parking lot to join and optimize the computing resources shared to the corresponding PLO. Last but not the least, it is straightforward in most the previous works to collect private information of each PV and PLO for centralized decision making. This violates privacy of the participants. Alternatively, a learning based incentive mechanism could be helpful for a PLO to seek a suboptimal solution under incomplete information.\n\nCompared with the previous works, we propose a joint federated learning and DRL approach to promote the management of PVEC. A flowchart is illustrated in Fig. 1 to present the comprehensive methodology of the proposed FedParking, which consists of two basic parts: parking space estimation and incentive mechanism design. Considering the parking management in smart cities, we extend the application of federated learning to parking space estimation and introduce FedParking. In the scheme, multiple PLOs collaborate to train a globally shared learning model to predict the number of free parking spaces in real time, without exchanging the raw data. Moreover, we utilize a LSTM model as the global model in FedParking to achieve an accurate estimation of the parking spaces. Building upon the above scheme for estimating the parking spaces at different PLOs, we further facilitate the incentive mechanism design for PVEC. According to the computing demand and parking capacity constraint, each PLO determines how to stimulate vehicles to enter the parking spaces and schedule idle resources of the PVs to support local offloading services. We formulate the strategic interactions among multiple PLOs and vehicles as a multi-leader multifollower Stackelberg game. The competition effect of different PLOs is considered. We temporarily neglect the parking capacity constraints, and provide the Stackelberg equilibrium analysis under the static conditions. We realize that the closedform Stackelberg equilibrium is difficult to acquire in the practical scenarios where both arrivals of the vehicles and parking capacity constraints of the PLOs are varying over time. Thus, a DRL approach is applied, in which each PLO plays as an agent to learn a near-optimal decision, such that we can reach the Stackelberg equilibrium in a distributed manner.\n\n\nIII. FEDPARKING\n\nFor traffic management at a parking lot, a PLO predicts the number of available parking spaces. To this end, a learning model is developed and utilized as a global model that is shared to multiple PLOs after iteratively training. To avoid the centralized data collection and centralized model training, we investigate the use of federated learning on parking space estimation, and design FedParking where many PLOs keep their raw data local but jointly train the global model by only sharing the updated model parameters with a parameter server. We first describe essential network entities of FedParking, and present the motivations and basic workflow of FedParking.\n\nMoreover, a LSTM model is adopted as the global model in FedParking, thereby improving the accuracy performance of parking space estimation.\n\n\nA. System Model and Network Entity\n\nIn this work, we consider a system model shown in Fig. 2. Specifically, for parking demand, vehicles on the roads send their parking requests to an application provider of local parking services. The application provider notifies PLOs in the region to serve the vehicles. Through PVEC, PLOs act as offloading service providers to attract the vehicles to enter their parking spaces by monetary rewards, and incentivize them to receive and complete given computation tasks during the parking time. We describe the essential network entities of FedParking as follows.\n\n\u2022 Parking Lot Operator: Each PLO manages a parking lot with limited parking spaces and specifies that an arriving vehicle occupies a parking space. The PLO adopts a learning model to estimate the number of free parking spaces and would like to participate in training the model by using the local data. In addition, when the vehicles are parked in a parking lot, their under-utilized onboard resources can be exploited to provision offloading services. Thus, there is great potential for tackling the computation demand of nearby users and service providers by offloading computation-intensive parts of mobile applications to the parking lot. Consequently, the PLO acts as an employer to reserve computing resources from the PVs to complete the offloading workloads. Here, an incentive mechanism is required to provide economic compensations to the PVs. \u2022 Parked Vehicle: A vehicle submits a parking request for parking space reservation. The parking request contains auxiliary information of the parking service, such as reservation time and original preferences over different parking lots. By knowing current parking lots with their reward policies and workload requirements, the vehicle decides whether to enter a parking lot and share the computing resources to the PLO. For the vehicle, monetary rewards could compensate for parking fee and mainly influence the parking choice. After promising to accept the offloading workloads, the vehicle in the parking lot will receive input data from the PLO through a nearest roadside unit. The vehicle virtualizes the on-board computing resources by the containerization technology to create the task instances and improve the task processing efficiency [9]. \u2022 Application Provider: The application provider is a totally trusted coordinator of the parking services in FedParking. It develops a client for vehicles that enables users with entrance to submit the parking requests and choose a suitable parking lot. A client is also developed for PLOs to receive the parking requests and publish their reward policies. With the help of the clients, a legitimate two-way communication channel between the vehicles and PLOs is built.\n\n\nB. Federated Learning for FedParking\n\nLearning-based time series prediction model is envisioned as a promising solution to tackle the problems of parking space estimation. The development of a time series prediction model inherits the methodology from traditional learning tasks, including data processing, model building, model training and inference. In [22], the authors have developed a data cleaning algorithm and trained various time series models for different parking lots. Given a training dataset with M time series D = {X , Y}, we regard X = {X 1 , X 2 , \u00b7 \u00b7 \u00b7 , X M } as the inputs and Y = {Y 1 , Y 2 , \u00b7 \u00b7 \u00b7 , Y M } as the corresponding ground truth predictions. Let F \u0398 represent the time series model parameterized by weight \u0398. To train the time series prediction model, we minimize the Mean Square Error (MSE) between the estimation and the ground truth, i.e.,\nmin \u0398 L(\u0398, D) \u2194 min \u0398 1 M m (F \u0398 (X m ) \u2212 Y m ) 2 .(1)\nIn this paper, we consider a feasible scenario where multiple PLOs train and adopt a shared time series prediction model to forecast their available parking spaces .\n\nWith the privacy considerations, we introduce FedParking to facilitate the collaboration among the PLOs in parking space estimation. In FedParking, there exist several communication rounds for the PLOs. In each communication round, each PLO firstly trains a global model with local data, and then submits the updated model parameters to aggregate a new global model. According to the concept of federated learning, knowledge of the local models can be shared without revealing the training data to each other. The distributed learning approach finally preserves the data privacy of the participants.\n\nWe consider that there exist I vehicles with parking demand and J parking lots. Each vehicle i, 1 \u2264 i \u2264 I can choose whether to enter a parking lot j, 1 \u2264 j \u2264 J. Each PLO has obtained the historical data of parking space information. Let D j represent a whole dataset of training data samples of PLO j, and \u0398 0 indicate the weight of the global model. Under the federated learning based framework, the goal of FedParking is expressed by\nmin \u03980 D j 1\u2264j\u2264J D j 1\u2264j\u2264J L(\u0398 0 , D j ),(2)\nwhere D j denotes the training data size of PLO j, and L(\u00b7) is the loss function in Eqn. (1). To optimize the above Problem (2), we refer to the original FedAVG algorithm in [1]. As shown in Fig. 2, the overall workflow of FedParking is described as follows.\n\n\u2022 Global Model Downloading: An application provider deploys a parameter server to build and iteratively update a time series predict model as the global model for parking space estimation. When the global model is initialized, each PLO is notified to download it. \u2022 Local Model Training: As required by the parameter server, each PLO performs a local model training task by the local data. After that, they upload the updated model parameters to the parameter server. \n\n\nC. LSTM for FedParking\n\nWe apply a LSTM model as the important global model in FedParking. Compared with the original recurrent neural network, LSTM has advantages in remembering and forgetting the information in the time sequence. This enables us to achieve an accurate estimation of the parking spaces.\n\nA LSTM network consists of two data gates to capture the sequence information, i.e., update gate and forget gate. In the following, we provide more details of the procedure of local model training. Take PLO j as an example. By data collection, parking space information (e.g., parking occupancy rate) of a parking lot over different time slots is recorded. We apply a sliding window approach to extract the input time series X j = {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x Dj } of the long shortterm memory model from the data source. When all the original data samples consist of y time slots, we obtain D j = y \u2212 z + 1 input time series if the sliding window size is z. Let H j = {h 1 , h 2 , \u00b7 \u00b7 \u00b7 , h Dj } indicate the hidden states, and O j = {o 1 , o 2 , \u00b7 \u00b7 \u00b7 , o Dj } represent the output states. We use W \u00b7h and W \u00b7x to denote the weights, and use b \u00b7 to denote the bias.\n\nAt the time step t, the value of forget gate \u0393 f t is calculated by\n\u0393 f t = \u03c3(W fh h t\u22121 + W fx x t + b f ),(3)\nwhere x t is the input vector at the time step t and h t\u22121 is the hidden state at the previous time step, \u03c3(\u00b7) denotes a Sigmoid function that normalizes the value into the range [0, 1]. The forget gate determines what information will be eliminated from the cell state. When the value of the forget gate \u0393 f t = 0, it throws away all the information, and \u0393 f t = 1 means that it keeps all the information. As for the update gate, it decides what information can be stored in the cell state. The value of update gate is calculated by\n\u0393 u t = \u03c3(W uh h t\u22121 + W ux x t + b u ).(4)\nNext, we calculate the information of candidate memory cellc t at the time step t b\u1ef9\nc t = tanh(W ch h t\u22121 + W cx x t + b c ),(5)\nwherec t represents the candidate information that should be stored in the cell state. Until now, we obtain {\u0393 f t , \u0393 u t ,c t } to update the cell state by\nc t = \u0393 f t \u2299 c t\u22121 + \u0393 u t \u2299c t ,(6)\nwhere \u2299 is an operator of executing the point-wise multiplication of two vectors. The output and hidden states are computed by\no t = \u03c3(W oh h t\u22121 + W ox x t + b o ) h t = o t \u2299 tanh(c t ).(7)\nFinally, we take o t as the input for the multilayer perceptron and compute the time series predictionx t+1 of the next time step. After the local model training, the weights W \u00b7h and W \u00b7x , and the bias b \u00b7 are shared to the parameter server.\n\nWith the help of FedParking, each PLO can predict the number of available parking space over time, which is utilized as a parking capacity constraint to adjust the following incentive mechanism design when necessary.\n\n\nIV. TWO-STAGE STACKELBERG GAME FORMULATION FOR INCENTIVE MECHANISM DESIGN\n\nWithin a time period, a batch of parking requests are gathered and the PLOs are notified to process them. In this paper, we model the interactions among the PLOs and vehicles as a Stackelberg game, where each PLO plays as a game leader and provides monetary rewards for the vehicles in Stage I, and each vehicle plays as a game follower and adopts a probabilistic strategy to decide the parking choice and the number of computing resources shared to a PLO in Stage II.\n\n\nA. Vehicles in Stage II\n\nVehicle i is incentivized to enter a parking lot j and share its idle computing resources during the parking time. Let r j represent the crucial incentive parameter of PLO j that indicates the monetary rewards per unit computing resource and unit time. According to the distance between the parking lot and destination, vehicle i may have an original preference p j i over parking lot j and the reserved parking duration is d i . We express the utility gain of vehicle i by p j i r j f j i d i , where f j i is the decision variable of vehicle i referring to the number of computing resources shared to PLO j. To support offloading services, vehicle i is required by PLO j to undertake workloads w j . When processing the workloads, a majority of energy is consumed in the CPU execution. Similar to [23], we neglect the energy consumed in the data transmission. For vehicle i, the energy consumption cost of CPU execution is \u03ba i f j2 i w j , where \u03ba i denotes the energy coefficient of the vehicle and represents the effective switched capacitance relying on the chip architecture [24], [25]. As a consequence, vehicle i acquires a following utility function if accepting the employment of PLO j,\nU j i = p j i r j f j i d i \u2212 \u03ba i f j2 i w j .(8)\nMotivated by the incentive mechanism design in the previous work [26], we consider that vehicle i makes a probabilistic decision on the parking destination. Particularly, we consider that monetary incentives have a relatively strong positive impact on the parking decision of the vehicle since the monetary rewards can compensate for the parking fee after it shares the idle computing resources to the PLO. We formulate the pairing probability between the vehicle and PLO by only the monetary rewards. Clearly, a vehicle is more likely to enter a parking lot with more monetary rewards. Thus, we set the pairing probability between vehicle i and PLO j as follows\n\u03c1 j i = r j 1\u2264k\u2264J r k .(9)\nFor vehicle i, we have 1\u2264j\u2264J \u03c1 j i = 1. Considering J PLOs, the expected utility of vehicle i is\nU i (f i , r) = 1\u2264j\u2264J \u03c1 j i U j i ,(10)\nwhere f i = f 1 i , \u00b7 \u00b7 \u00b7 , f J i and r = r 1 , \u00b7 \u00b7 \u00b7 , r J . Each vehicle i decides the optimal strategy vector f i to maximize U i according to the reward policy vector r.\n\n\nB. Parking Lot Operators in Stage I\n\nPLO j collects a certain number of computing resources from the PVs and is able to provision offloading services. After satisfying the computation demand of offloading users, PLO j obtains revenue g j per unit computing resource and unit time. Considering the total monetary cost for I PVs, PLO j obtains the expected utility in terms of profit as follows\nV j (r j , r \u2212j , f j ) = 1\u2264i\u2264I \u03c1 j i (g j \u2212 r j )f j i d i ,(11)\nwhere r \u2212j is a reward policy vector released by all the PLOs except PLO j and f j = f j 1 , \u00b7 \u00b7 \u00b7 , f j I . Each PLO j decides the optimal strategy r j to maximize V j according to the strategies of all the other PLOs (i.e., r \u2212j ) and the strategies of all the vehicles (i,e., f j ). Note that there exists a non-cooperative game among the J PLOs in the incentive mechanism. Moreover, two essential constraints of r j are presented. The first constraint named by parking capacity constraint is expressed by 1\u2264i\u2264I \u03c1 j i \u2264 n j , where n j is acquired from the above FedParking scheme and represents the predicting number of available parking spaces in the parking lot. This means that PLO j should adjust r j well to control the expected number of arriving vehicles with respect to the given parking capacity constraint. Considering a budget constraint, r j is constrained within an upper limit r j max \u2264 g j . Each player (i.e., each PLO and vehicle) of the Stackelberg game has its own interest. We aim to find a unique Stackelberg equilibrium at which each leader (PLO) obtains the maximal expected utility given the best responses from the followers (vehicles). At the time, no single PLO or vehicle will have any motivation to unilaterally change its decision. Given the multiple-leader multi-follower Stackelberg game, we define the Stackelberg equilibrium as follows.\n\nDefinition 1 (Stackelberg Equilibrium). There exist an optimal computing resource vector denoted as f * i and an optimal monetary reward r j * for vehicle i and PLO j, respectively. (f * = {f * i } , r * = r 1 * , \u00b7 \u00b7 \u00b7 , r J * , \u2200i and j) is the Stackelberg equilibrium if and only if the following conditions are satisfied.\nV j (r j * , r \u2212j * , f j * ) \u2265 V j (r j , r \u2212j * , ), \u2200j, U i (f * i , r * ) \u2265 U i (f i , r * ), \u2200i.\nWe find the subgame perfect Nash equilibrium to achieve the Stackelberg equilibrium, which ensures that the expected utility of each PLO is maximized subject to the best responses from the vehicles. In the proposed Stackelberg game, there exists a subgame among the PLOs in the upper stage and they strictly compete in a non-cooperative fashion. The Nash equilibrium of the subgame is denoted as (r j * , r \u2212j * ), where the expected utility of each PLO j cannot be further improved by adopting a different strategy other than the given strategies of the other PLOs r \u2212j * . In the bottom stage, the best response of vehicle i is acquired as f * i by maximizing the expected utility with respect to r * . To reach the Stackelberg equilibrium, we adopt the backward induction method in the following.\n\n\nV. STACKELBERG EQUILIBRIUM ANALYSIS UNDER COMPLETE INFORMATION\n\nWe temporarily neglect the parking capacity constraints and provide the theoretical analysis to prove the existence and uniqueness of the above Stackelberg equilibrium.\n\n\nA. Best Response of a Vehicle\n\nWe first pay attention to the best response of a follower. Given the reward policy vector r published by all the PLOs, each vehicle i determines f i to maximize its U i . We take the first-order and second-order derivatives of U i with respect to\nf j i , \u2202Ui \u2202f j i = \u03c1 j i (p j i r j d i \u2212 2\u03ba i f j i w j ), \u2202 2 Ui \u2202f j2 i = \u22122\u03c1 j i \u03ba i w j < 0.\nThe negative second-order derivative of U i indicates that U i is strictly concave with respect to f j i . As a consequence, for vehicle i, the best response f j * i can be calculated by\nf j * i = p j i d i 2\u03ba i w j r j = \u03bb j i r j ,(12)\nwhere \u03bb j i = p j i d i /(2\u03ba i w j ). The above result means that more monetary rewards are provided to the vehicle, more computing resources will be shared to the PLO, which is consistent with the intuition.\n\n\nB. Nash Equilibrium Analysis for PLOs\n\nWe realize that each PLO is selfish to employ the vehicles with idle computing resources and maximize the payoffs. The PLOs play a non-cooperative game because of the competition effect. In the non-cooperative game, the player set consisting of J PLOs is finite and strategy spaces of the PLOs are nonempty, convex, and compact subsets of the Euclidean spaces. The Nash equilibrium exists in the non-cooperative game if we can prove the concavity of the utility functions in the strategy spaces.\n\nTheorem 1. Under the assumption that 3 k =j r k > g j , \u2200j, a Nash equilibrium exists in the non-cooperative game among the PLOs.\n\nProof: By substituting f j * i into V j , we update\nV j = g j r j2 1\u2264i\u2264I \u03bb j i d i \u2212 r j3 1\u2264i\u2264I \u03bb j i d i 1\u2264k\u2264J r k = \u00b5 j g j r j2 \u2212 r j3 1\u2264k\u2264J r k ,(13)\nwhere \u00b5 j = 1\u2264i\u2264I \u03bb j i d i . We take the first-order and secondorder derivatives of V j with respect to r j ,\n\u2202V j \u2202r j = \u00b5 j (2g j r j \u22123r j2 ) 1\u2264k\u2264J r k \u2212g j r j2 +r j3 1\u2264k\u2264J r k 2 = \u00b5 j r j \u22122r j2 \u2212 3 k =j r k \u2212g j r j +2g j k =j r k 1\u2264k\u2264J r k 2 = \u00b5 j r j \u03a0 j 1\u2264k\u2264J r k 2 , \u2202 2 V j \u2202r j2 = \u22122\u00b5 j r j \u03a0 j 1\u2264k\u2264J r k 3 + \u00b5 j \u03a0 j \u22124r j \u2212 3 k =j r k \u2212g j 1\u2264k\u2264J r k 2 .\nwhere \u03a0 j = \u22122r j2 \u2212 (3 k =j r k \u2212 g j )r j + 2g j k =j r k . If \u2202V j /\u2202r j = 0, \u03a0 j = 0, at this time, \u2202 2 V j /\u2202 2 r j2 < 0 since 3 k =j r k > g j and r j > 0. This means that utility function of PLO j is quasi-concave. We conclude that the Nash equilibrium exists in this non-cooperative game.\n\nTheorem 2. The Nash equilibrium of the non-cooperative game among the PLOs is unique.\n\nProof: Based on the first-order optimality condition \u2202V j /\u2202r j = 0 (i.e., \u03a0 j = 0), we have r j * = \u03c6 j (r), which is shown at the top of the next page. Considering the upper limit of r j , we update\nr j * = \u03c6 j (r), 0 < \u03c6 j (r) < r j max , r j max , \u03c6 j (r) \u2265 r j max .(14)\nNext, we pay attention to the case that r j * < r j max . We prove the uniqueness of the Nash equilibrium by validating that the best response function \u03c6 j (r) is a standard function, which satisfies three properties as follows \u2022 Positivity: \u03c6 j (r) > 0. \u2022 Monotonicity: if r \u2032 > r, then \u03c6 j (r \u2032 ) > \u03c6 j (r). \u2022 Scalability: for any \u03b1 > 1, \u03b1\u03c6 j (r) > \u03c6 j (\u03b1r). First, due to g j > 0 and r > 0, \u03c6 j (r) > 0 is always satisfied. Second, we easily know that\n\uf8eb \uf8ed 3 k =j r k + 5 3 g j \uf8f6 \uf8f8 2 > \uf8eb \uf8ed 3 k =j r k \u2212 g j \uf8f6 \uf8f8 2 + 16g j k =j r k .\nFurthermore,\n\u2202\u03c6 j (r) \u2202 k =j r k = 0.25 18 k =j r k + 10g j 2 3 k =j r k \u2212 g j 2 + 16g j k =j r k \u2212 3 > 0.\nWhen r \u2032 > r, k =j r k\u2032 > k =j r k and we have \u03c6 j (r \u2032 ) > \u03c6 j (r). Finally, \u2200\u03b1 > 1, we have \u03b1 2 > \u03b1 and further have \u03b1\u03c6 j (r) > \u03c6 j (\u03b1r), which is shown at the top of the next page. Note that we have proved that the best response function of PLO j satisfies the above three properties in this case. When r j * = r j max , these three properties are still satisfied. Then we conclude that standard best response functions of all the PLOs have the unique Nash equilibrium.\n\nTheorem 3. There exists a unique Stackelberg equilibrium in the Stackelberg game.\n\nProof: Until now, we have proved that the best response of a follower is unique in the second stage and the Nash equilibrium among the leaders is also unique in the first stage. We can conclude that the Stackelberg equilibrium is unique. The proof is completed.\n\nTo reach the Stackelberg equilibrium, we utilize the following rule to update the monetary rewards in each iteration k to gradually approach the Nash equilibrium in the first stage,\nr j,k = r j,k\u22121 + \u03bb j r j,k\u22121 \u2202V j \u2202r j , \u2200j,(15)\nwhere \u03bb j denotes a learning rate, and \u2202V j /\u2202r j is approximately calculated by the central difference method,\n\u2202V j \u2202r j \u2248 V j,k + \u2212 V j,k \u2212 2\u2206r ,(16)\nwhere V j,k + = V j (r j,k\u22121 + \u2206r, r \u2212j,k\u22121 ), V j,k \u2212 = V j (r j,k\u22121 \u2212 \u2206r, r \u2212j,k\u22121 ) and \u2206r is a small constant indicating the minor change of the reward policy. Given the updating rule of the monetary rewards, two different approaches can be considered by the PLOs to iteratively adjust their strategies, including Gauss-Seidel and Jacobi based algorithms. In the former algorithm, the PLOs update their strategies sequentially. In the latter algorithm, they update the strategies in parallel. In this paper, we adopt the Jacobi based algorithm.\nr j * = 0.25 \uf8eb \uf8ec \uf8ec \uf8ed \uf8eb \uf8ed 3 k =j r k \u2212 g j \uf8f6 \uf8f8 2 + 16g j k =j r k \u2212 3 k =j r k + g j \uf8f6 \uf8f7 \uf8f7 \uf8f8 = \u03c6 j (r). \u03b1\u03c6 j (r) = 0.25 9\u03b1 2 \uf8eb \uf8ed k =j r k \uf8f6 \uf8f8 2 + \u03b1 2 g j2 + 10\u03b1 2 g j k =j r k \u2212 3\u03b1 k =j r k + g j > 0.25 9\u03b1 2 \uf8eb \uf8ed k =j r k \uf8f6 \uf8f8 2 + g j2 + 10\u03b1g j k =j r k \u2212 3\u03b1 k =j r k + g j = \u03c6 j (\u03b1r).\n\nVI. A LEARNING BASED INCENTIVE MECHANISM BY DRL\n\nIn Section V, we focus on analyzing the Stackelberg equilibrium under complete information, without taking into account the parking capacity constraints. However, both the arrivals of the vehicles and number of available parking spaces in each parking lot are varying over time. It is difficult to obtain the closed-form Stackelberg equilibrium in practical scenarios. In addition, the above theoretical analysis is based on the centralized decision making which requires to collect private information of all the players to derive the solution of the Stackelberg game. This violates the individual privacy of the players. Under the complicated decision-marking conditions and for the fulfillment of privacy-related requirements, advanced artificial intelligence technologies can be potential tools to facilitate the incentive mechanism design [27], [28]. Hence, we are motivated by [29] to apply DRL as an online algorithm to tackle the incentive mechanism design problem of the multi-leader multi-follower Stackelberg game under incomplete information. The extensively concerned DRL technique has outstanding ability in adapting to the non-static environment and dynamically resolving the complicated decisionmaking problems according to environment variations [30]. Our DRL approach enables each PLO as an agent to quickly learn a near-optimal decision in a distributed and privacyfriendly manner. We first outline a learning framework of the whole DRL based incentive mechanism. After that, we formulate the incentive design problem as a DRL learning task and provide more details of the neural network configuration and optimization method based on the proposed learning framework.\n\n\nA. Overview of the Learning Framework\n\nTo reach the Stackelberg equilibrium, each PLO becomes an agent in the DRL approach reacting to the environment consisting of all the vehicles, as shown in Fig. 3. PLO j is agent j, which refers to the historical strategies of the other agents and determines an action according to the current state.\n\nSpecifically, PLO j in each training episode t(1 \u2264 t \u2264 T ) interacts with the environment to know the current state S j t , which consists of reward policy history of the other PLOs and its profit parameter. PLO j plays as a game leader that decides the action a j t and releases the reward policy p j t . After that, a vehicle with parking demand in the environment plays as a follower that confirms the optimal strategy based on Eqn. (12). By collecting the strategies of all the vehicles, the environment computes the learning rewards R j t for each PLO. At the same time, the environment saves all the historical data of the actions into a finite-buffer queue. Let L denote the size of the finite-buffer queue. Here, an application provider of the parking services in the region could become such an essential coordinator to establish the learning environment. The application provider is convenient to collect historical strategies of each player and accordingly update the finitebuffer queue. It extracts the related information from the finitebuffer queue to generate new states for the corresponding PLOs. Hence, the next training episode is conducted.\n\nNote that during the training procedure, each player cannot obtain any privacy-related information of the other players that influences their decisions. As a consequence, we solve the Stackelberg game without leaking private information and causing privacy threats to the players.\n\n\nB. Design Details\n\nIn the following, we describe a DRL learning task under the proposed learning framework. For each player, the decisionmaking procedure is performed independently according to the others' past strategy information and its own property. We further formulate the interactions among the PLOs as a multiagent markov decision process. a) State space: We focus on a case where state space of each player is partially observable. For PLO j, the state is formed by the past strategies of the other PLOs and its own properties, which is denoted as {r \u2212j t\u2212L , r \u2212j t\u2212L+1 , \u00b7 \u00b7 \u00b7 , r \u2212j t\u22121 } and g j , respectively. In the training episode t, the PLO interacts with the environment to know the current state, which is represented by\nS j t = {r \u2212j t\u2212L , r \u2212j t\u2212L+1 , \u00b7 \u00b7 \u00b7 , r \u2212j t\u22121 , g j }. b) Action space: We use A = {a j t ,\n\u2200j} to represent the actions of all PLOs. In the training episode t, PLO j determines the action a j t with respect to the state S j t . The action refers to the reward policy. To improve the learning efficiency, we set lower and upper bounds r j min , r j max for the action. Since a j t \u2208 [r j min , r j max ], the action space of a j t is continuous and we simply normalize the range of a j t to [0, 1] by letting a j t = (r j t \u2212 r j min )/r j max . c) Reward function: The reward space is defined as R = {R j t , \u2200j}. We consider a time-varying parking capacity constraint for each agent. In the training episode t, n j t is the available parking capacity of agent j. This means that for all the I vehicles, the expected number of the vehicles choosing PLO j is limited by n j t . With respect to the parking capacity constraint, we are motivated to express the reward function of agent j as\nR j t = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1\u2264i\u2264I \u03c1 j i (g j \u2212 r j t )f j i d i , 1\u2264i\u2264I \u03c1 j i \u2264 n j t n j t I 1\u2264i\u2264I (g j \u2212 r j t )f j i d i \u2212 P( 1\u2264i\u2264I \u03c1 j i ), otherwise.(17)\nwhere P(\u00b7) is a penalty function to control the expected number of arriving vehicles. The function is predefined by\nP = \u03b9 j n j t max( 1\u2264i\u2264I \u03c1 j i \u2212 n j t , 0),(18)\nwhere \u03b9 j is a penalty factor. d) Learning Objective: We first denote the reward policy of PLO j parameterized by \u03b8 j as \u03c0 j \u03b8 , which is defined as \u03c0 j \u03b8 : S j \u2192 a j . Let V j \u03c0 \u03b8 (S j ) indicate the value function for the given state, and Q j \u03c0 \u03b8 (S j , a j ) become the value function for the given state and action. The learning objective of agent j is shown by\n\u03b8 j * = arg max \u03b8 j L j (\u03c0 j \u03b8 ) = arg max \u03b8 j E V j \u03c0 \u03b8 (S j 0 ) = arg max \u03b8 j E Q j \u03c0 \u03b8 (S j 0 , a j 0 ) \u03c0 j \u03b8 ,(19)\nwhere\nV j \u03c0 \u03b8 (S j ) = E R j t S j t = S j , \u03a0 , Q j \u03c0 \u03b8 (S j , a j ) = E R j t S j t = S j , a j t = a j , \u03a0 , R j t = T k=t \u03b3 k\u2212t R j t .(20)\nHere, \u03a0 = {\u03c0 j \u03b8 , \u2200j} represents the whole policy set of all PLOs, R j t is the expected discounted reward for PLO j in all the T training episodes, and \u03b3 \u2208 [0, 1] indicates the discounted factor.\n\n\nC. Actor-Critic Networks and Policy Optimization\n\nWe train the above DRL model based on the policy-based methods and resorts to the typical actor-critic model. We design a decentralized actor network and a decentralized critic network for each agent. For agent j, it maintains actor and critic networks parameterized by \u03b8 j and \u03c9 j for approximating the policy and value function, respectively. We apply the policy gradient method to optimize the learning objective defined in Eqn. (19). Referring to the policy optimization theorem proposed in [31], the policy gradient is computed by\n\u2207 \u03b8 j L j = E \u03c0 j \u03b8 (S j ) [\u2207 \u03b8 j log \u03c0 j \u03b8 (S j , a j )A j \u03c0 \u03b8 (S j , a j )] \u2248 E \u03c0 j \u03b8 (S j ) [P j \u2207 \u03b8 j log \u03c0 j \u03b8 (S j , a j )A j \u03c0 \u03b8 (S j , a j )],(21)\nwhere\nP j = \u03c0 j \u03b8 (S j |a j ) \u03c0 j \u03b8 (S j |a j ) , A j \u03c0 \u03b8 (S j , a j ) = Q j \u03c0 \u03b8 (S j , a j )\u2212V j \u03c0 \u03b8 (S j ). A j \u03c0 \u03b8 (S j , a j )\nis the advantage function for state and action and \u03c0 j \u03b8 (S j , a j ) is the policy for importance sampling. Furthermore, we design the training procedure according to the proximal policy optimization algorithm in [32], which is the state-of-theart policy gradient method with excellent stability. Therefore, the policy gradient is further clipped as\n\u2207 \u03b8 j L j \u2248 E \u03c0 j \u03b8 (S j ) [\u2207 \u03b8 j log \u03c0 j \u03b8 (S j , a j )C j \u03c0 \u03b8 (S j , a j )](22)\nwhere C j \u03c0 \u03b8 (S j , a j ) = min[P j A j \u03c0 \u03b8 (S j , a j ), F (P j )A j \u03c0 \u03b8 (S j , a j )],\nF (P j ) = \uf8f1 \uf8f2 \uf8f3 1 + \u03b5, P j > 1 + \u03b5 P j , 1 \u2212 \u03b5 \u2264 P j \u2264 1 + \u03b5 1 \u2212 \u03b5, P j < 1 \u2212 \u03b5(23)\nand \u03b5 is an adjustable hyper-parameter. After that, we update the actor and critic models by the mini-batch stochastic gradient ascent and descent methods, respectively. In Fig. 3, the actor network of a PLO is the multilayer perceptron with two hidden layers, consisting of 200 neurons and 50 neurons, respectively. For the critic network, we combine connected-layer and differentiable neural computer model in [33] to build the whole model for value estimation. Differentiable neural computer model is introduced as a type of recurrent neural network with internal memory module, and has been successful applied by the previous works to solve the partially observable markov decision process problem. The agents gradually learn the optimal strategies with the training procedure continuing. When the training procedure converges, they determine the strategies based on the outputs of the actor networks.\n\n\nVII. NUMERICAL RESULTS\n\n\nA. Parameter Setting\n\nWe evaluate the proposed FedParking by numerical studies. First, we study the overall performance of FedParking on the   A PLO acts as an offloading service provider and employs PVs as edge computing nodes. When a PV is effectively stimulated to share its idle computing resources, the PV continuously receives offloading tasks within a specific time period whose duration ranges from 5 to 20 minutes. The arrival process of the tasks is a typical Poisson process, where the average arrival rate and workload of the tasks are chosen from U [1,3] per minute and U [2,5] giga CPU cycles, respectively. We assume that there are 35 vehicles with parking demand. For a PV, reserved parking time d, computing capability f and hardware parameter \u03ba are uniformly distributed in the range of U [20, 100] minutes, U [0.5, 3.5] GHz and U [1, 10]e-28, respectively. The preference parameter p over different parking lots is randomly generated within (0, 1). For a PLO, profit parameter g follows the uniform distribution U [3,5]. As for the DRL experiments, we conduct them using Tensorflow 1.14 on Ubuntu 20.04 LTS with CUDA 10.1. We set the parameters L = 5, r min = 0.2, r max = 3, \u03b9 = 2, T = 20, \u03b3 = 0.95 and \u03b5 = 0.1 by default.\n\n\nB. FedParking for Parking Space Estimation\n\nIn this paper, we study an integration of federated learning and LSTM to design FedParking, in which all PLOs collaborate to train a shared LSTM model for parking space estimation, ultimately improving the prediction accuracy while avoiding data transfer among the PLOs. In addition to Fed-Parking, a general approach based on federated learning for parking space estimation is named by FedMLP, which adopts a multilayer perceptron model as the global model in federated learning. In the traditional work, each PLO only utilizes the local data for isolated training of its LSTM model.  To highlight the advantage of FedParking, we compare our approach with the baseline approaches. We evaluate the training loss by using MSE. The performance regarding the MSE loss for three approaches are assessed in Fig. 4. FedParking ensures that knowledge of the local models is securely shared among the PLOs and simultaneously apply the LSTM model for accurate parking space estimation. As a consequence, FedParking converges significantly faster than the baseline approaches. For each PLO, experimental results show that compared with the baseline approaches, the proposed FedParking always achieves smaller MSE, thereby achieving a higher prediction accuracy.\n\n\nC. DRL for Incentive Mechanism Design\n\nIn the following, we perform numerical analysis of the DRL based incentive mechanism under different scenarios. We first investigate the performance of our DRL approach in the ideal case, where parking capacity constraints are neglected for the PLOs. To reach the Stackelberg equilibrium, our work proposes a DRL approach to make each PLO as an agent learn a near-optimal solution in a distributed manner while the   traditional work considers a centralized approach by collecting private information of all the PLOs and vehicles to execute the Jacobi based algorithm, as introduced in Section V-B. Here, we set the learning rate \u03bb =1e-3 and constant \u2206r =1e-2 for the Jacobi based algorithm in the centralized approach. Without the parking capacity constraints, the penalty function is removed from reward function of each PLO in the DRL approach. As shown in Fig. 5, we compare the performance of our DRL approach with the centralized approach in the ideal case. The convergence of these two approaches is validated and the centralized approach converges relatively faster, owing to the perfect complete information condition. Beside, we observe that the learning error of the DRL approach is very minor. Our approach can finally converge to a near-optimal solution which can closely approximate the optimal solution in the centralized approach within the limited iterations.\n\nIn addition, we investigate the performance of our DRL approach under different parking capacity constraints. For simplicity, we consider three cases of the distribution of the parking capacity constraints n 1 , n 2 , n 3 , i.e., Case 1: [15,20,5], Case 2: n 1 , n 2 , n 3 = [25, 20, 5] and Case 3: n 1 , n 2 , n 3 = [35, 20, 5] for 3 PLOs, respectively. From Fig. 6, we realize that the DRL approach converges finally under the parking capacity constraints. The expected number of the vehicles that choose to enter a specific parking lot is also controlled well by adopting an appropriate reward policy, as illustrated by Fig. 7. For a PLO, reward policy r referrings to monetary rewards per unit computing resource and unit time, roughly decreases with the reduction of the parking capacity constraint. Given a lower value of n j , PLO j is forced to provide less incentive rewards for arriving vehicles in order to avoid congestion to the parking lot. For example, when PLO 1 has only 15 free parking spaces in Case 1, r 1 is mainly influenced by the current parking capacity constraint. At the equilibrium point, the equality constraint 1\u2264i\u2264I \u03c1 1 i = n 1 is approximately satisfied. The similar observation is found for PLO 3, which has only 5 free parking spaces. In Case 2, n 1 is increased such that the PLO can improve r 1 on demand. At this time, PLO 1 has the maximal number of free parking spaces. To attract more vehicles, the PLO is encouraged to improve r 1 , which is close to the upper limit in Case 2. By knowing the obvious improvement of r 1 , PLO 2 and PLO (c) Comparison of the best response when varying \u03ba and r. 3 realize that more vehicles will be attracted by PLO 1. It is an alternative for them to improve their expected utilities by reducing r 2 and r 3 to save unnecessary monetary cost. Particularly, r 2 sharply decreases from 3.000 to 2.402 in Case 2. With the continuing increase of n 1 , this leads to the further reduction of r 2 and r 3 in Case 3. The competitive strength of PLO 1 is enhanced when n 1 is far greater than n 2 and n 3 .\n\nTo effectively improve the expected utility, PLO 1 is active to reduce the addition monetary cost since r 1 with a slightly lower value still can attract sufficient vehicles, because of the simultaneous reduction of r 2 and r 3 .\n\nWe compare the expected utility of a PLO under linear pricing scheme and our Stackleberg game based scheme in Fig. 8. Here, we take PLO 1 as an example and set the parking capacity constraints in Case 3. According to the number of available parking spaces n 1 , it could be a simplified approach for PLO 1 to decide the reward policy r 1 by considering the linear relationship between n 1 and r 1 , namely, r 1 = \u03b6n 1 , where \u03b6 \u2208 [r min /n 1 , r max /n 1 ] is a scale factor. Compared with the linear pricing scheme, our scheme derives the optimal reward policy by the Stackelberg game model with DRL approach. With respect to the varying \u03b6, the expected utility of PLO 1 is changing. Note that PLO 1 can adopt a fixed or dynamic reward policy by controlling the scale factor. But the determination of \u03b6 is a challenging task and cannot guarantee the high-level expected utility for the PLO all the time. By the traversal method, we seek the optimal value of \u03b6 to achieve the maximal expected utility. We find that the expected utility of PLO 1 calculated by our scheme significantly approximates the above maximal one. The approximation error is limited within about 6%. Moreover, the linear pricing scheme is not always feasible since the baseline scheme neglects the parking capacity constraint. In this regard, our scheme applies the Stackelberg game theoretic approach and presents a learning based solution to quickly seek the suboptimal reward policy with the given parking capacity constraint. Fig. 9 shows the best response of a PV under the external and internal impacts. For the PV, the best response is in terms of the optimal number of computing resources shared to a PLO. Supposing that the PV is stimulated by PLO 1 and we study the best response f * . The external impact refers to the given reward policy r, and internal factors include its preference parameter p, parking time d and hardware parameter \u03ba. Clearly, with the increase of r, f * is gradually improved. According to Eqn. (12), both p and d have a positive effect on the improvement of f * while kappa has a negative effect. The results in Fig. 9 are consistent with the above analysis. For example, when d is increased from 50 to 70 minutes, the vehicle would like to share more idle computing resources and earn monetary rewards during the parked time, thereby f * increases about 40% with respect to r = 1.5, p = 0.6 and \u03ba = 6e \u2212 28.\n\nIn summary, we present a joint federated learning and DRL approach to design FedParking and ultimately facilitate the parking management in smart cities. In FedParking, a secure and distributed learning approach is applied among the PLOs for high-accuracy parking space estimation without requiring data transfer among them. The DRL approach resorts to the iterative and distributed decision making on the PLO side and may scarify the convergence speed compared with the centralized approach. But our approach guarantees the convergence accuracy, and does not require to collect private information of all the players of the Stackelberg game.\n\n\nVIII. CONCLUSION\n\nIn this paper, we introduced FedParking to study the federated learning based parking space estimation with PVEC management. In the scheme, PLOs were instructed to train a shared LSTM model for parking space estimation without exchanging the raw data among them. A parking space constraint was accordingly presented to each PLO, which acts as an incentive designer to determine how to stimulate the vehicles to enter the parking spaces and share their idle computing resources for offloading services in PVEC. We formulated the interactions among the PLOs and vehicles as a multi-leader multi-follower Stackelberg game and provided the theoretical Stackelberg equilibrium analysis under complete information. Considering dynamic arrivals of the vehicles and time-varying parking capacity constraints, a DRL approach was particularly proposed to reach the Stackelberg equilibrium in a distributed yet privacy-friendly manner. Finally, numerical results demonstrated that our scheme is effective and efficient for high-accuracy parking space estimation, and can seek a near-optimal solution under the complicated conditions.\n\nFig. 2 :\n2System design of FedParking.\n\n\u2022\nModel Parameter Submission: All the model parameters are collected and aggregated by the parameter server to update the weight \u0398 0 and improve the accuracy of the global model. \u2022 Global Model Updating: A new global model is acquired and transmitted to all the PLOs. The iterations of local model training and aggregation repeats until the global model converges. Note that the whole training procedure of the global model uses the distributed training datasets of all the PLOs, and is executed through peer-to-peer collaboration without exchanging the raw data.\n\nFig. 3 :\n3A learning based incentive mechanism by multi-agent DRL.\n\nFig. 4 :\n4Comparison of training loss under different approaches.\n\nFig. 5 :\n5Performance comparison among different approaches without the parking capacity constraints. the performance of our proposed approach in comparison with other approaches, we set the local batch size as 64. The local epoch is set as 1 for FedParking and FedMLP.\n\nFig. 6 :\n6Convergence of the DRL approach with different parking capacity constraints.\n\nFig. 7 :Fig. 8 :\n78Comparison of the expected number of arriving vehicles under the DRL approach with different parking capacity constraints. Comparison of the expected utility of a PLO under different schemes\n\nFig. 9 :\n9Comparison of the best response of a vehicle under different external and internal parameters.\n\n\nTo evaluate 1 https://archive.ics.uci.edu/ml/datasets/Parking+Birmingham0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n\nIteration \n\n1.2 \n\n1.4 \n\n1.6 \n\n1.8 \n\n2 \n\n2.2 \n\n2.4 \n\n2.6 \n\nReward policy \n\nPLO 1, centralized approach \nPLO 2, centralized approach \nPLO 3, centralized approach \nPLO 1, DRL approach \nPLO 2, DRL approach \nPLO 3, DRL approach \n\n\n\nCommunication-efficient learning of deep networks from decentralized data. B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, Artificial Intelligence and Statistics. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \"Communication-efficient learning of deep networks from decentralized data,\" in Artificial Intelligence and Statistics. PMLR, 2017, pp. 1273- 1282.\n\nFederated machine learning: Concept and applications. Q Yang, Y Liu, T Chen, Y Tong, ACM Transactions on Intelligent Systems and Technology (TIST). 102Q. Yang, Y. Liu, T. Chen, and Y. Tong, \"Federated machine learning: Concept and applications,\" ACM Transactions on Intelligent Systems and Technology (TIST), vol. 10, no. 2, pp. 1-19, 2019.\n\nFederated learning for mobile keyboard prediction. A Hard, K Rao, R Mathews, S Ramaswamy, F Beaufays, S Augenstein, H Eichner, C Kiddon, D Ramage, arXiv:1811.03604arXiv preprintA. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen- stein, H. Eichner, C. Kiddon, and D. Ramage, \"Federated learning for mobile keyboard prediction,\" arXiv preprint arXiv:1811.03604, 2018.\n\nFedhealth: A federated transfer learning framework for wearable healthcare. Y Chen, X Qin, J Wang, C Yu, W Gao, IEEE Intelligent Systems. 354Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, \"Fedhealth: A federated transfer learning framework for wearable healthcare,\" IEEE Intelligent Systems, vol. 35, no. 4, pp. 83-93, 2020.\n\nHcp: Heterogeneous computing platform for federated learning based collaborative content caching towards 6g networks. Z M Fadlullah, N Kato, IEEE Transactions on Emerging Topics in Computing. Z. M. Fadlullah and N. Kato, \"Hcp: Heterogeneous computing platform for federated learning based collaborative content caching towards 6g networks,\" IEEE Transactions on Emerging Topics in Computing, pp. 1-1, 2020.\n\nPrivacy-preserving traffic flow prediction: A federated learning approach. Y Liu, J J Q Yu, J Kang, D Niyato, S Zhang, IEEE Internet of Things Journal. 78Y. Liu, J. J. Q. Yu, J. Kang, D. Niyato, and S. Zhang, \"Privacy-preserving traffic flow prediction: A federated learning approach,\" IEEE Internet of Things Journal, vol. 7, no. 8, pp. 7751-7763, 2020.\n\nFastgnn: A topological information protected federated learning approach for traffic speed forecasting. C Zhang, S Zhang, J J Q Yu, S Yu, IEEE Transactions on Industrial Informatics. C. Zhang, S. Zhang, J. J. Q. Yu, and S. Yu, \"Fastgnn: A topological information protected federated learning approach for traffic speed forecasting,\" IEEE Transactions on Industrial Informatics, pp. 1-1, 2021.\n\nLearning precise timing with lstm recurrent networks. F A Gers, N N Schraudolph, J Schmidhuber, Journal of machine learning research. 3F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, \"Learning precise timing with lstm recurrent networks,\" Journal of machine learning research, vol. 3, no. Aug, pp. 115-143, 2002.\n\nTask-container matching game for computation offloading in vehicular edge computing and networks. X Huang, R Yu, S Xie, Y Zhang, IEEE Transactions on Intelligent Transportation Systems. X. Huang, R. Yu, S. Xie, and Y. Zhang, \"Task-container matching game for computation offloading in vehicular edge computing and networks,\" IEEE Transactions on Intelligent Transportation Systems, pp. 1-14, 2020.\n\nFederated learning for data privacy preservation in vehicular cyber-physical systems. Y Lu, X Huang, Y Dai, S Maharjan, Y Zhang, IEEE Network. 343Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, \"Federated learning for data privacy preservation in vehicular cyber-physical systems,\" IEEE Network, vol. 34, no. 3, pp. 50-56, 2020.\n\nDistributed federated learning for ultra-reliable low-latency vehicular communications. S Samarakoon, M Bennis, W Saad, M Debbah, IEEE Transactions on Communications. 682S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, \"Distributed fed- erated learning for ultra-reliable low-latency vehicular communications,\" IEEE Transactions on Communications, vol. 68, no. 2, pp. 1146-1159, 2020.\n\nFederated learning in vehicular edge computing: A selective model aggregation approach. D Ye, R Yu, M Pan, Z Han, IEEE Access. 8D. Ye, R. Yu, M. Pan, and Z. Han, \"Federated learning in vehicular edge computing: A selective model aggregation approach,\" IEEE Access, vol. 8, pp. 23 920-23 935, 2020.\n\nComputation resource allocation and task assignment optimization in vehicular fog computing: A contract-matching approach. Z Zhou, P Liu, J Feng, Y Zhang, S Mumtaz, J Rodriguez, IEEE Transactions on Vehicular Technology. 684Z. Zhou, P. Liu, J. Feng, Y. Zhang, S. Mumtaz, and J. Rodriguez, \"Computation resource allocation and task assignment optimization in vehicular fog computing: A contract-matching approach,\" IEEE Trans- actions on Vehicular Technology, vol. 68, no. 4, pp. 3113-3125, 2019.\n\nEfficient workload allocation and user-centric utility maximization for task scheduling in collaborative vehicular edge computing. X Huang, R Yu, D Ye, L Shu, S Xie, IEEE Transactions on Vehicular Technology. 704X. Huang, R. Yu, D. Ye, L. Shu, and S. Xie, \"Efficient workload allocation and user-centric utility maximization for task scheduling in collaborative vehicular edge computing,\" IEEE Transactions on Vehicu- lar Technology, vol. 70, no. 4, pp. 3773-3787, 2021.\n\nMobile edge computing-assisted admission control in vehicular networks: The convergence of communication and computation. Y Qi, L Tian, Y Zhou, J Yuan, IEEE Vehicular Technology Magazine. 141Y. Qi, L. Tian, Y. Zhou, and J. Yuan, \"Mobile edge computing-assisted admission control in vehicular networks: The convergence of communi- cation and computation,\" IEEE Vehicular Technology Magazine, vol. 14, no. 1, pp. 37-44, 2019.\n\nTask offloading in vehicular edge computing networks: A load-balancing solution. J Zhang, H Guo, J Liu, Y Zhang, IEEE Transactions on Vehicular Technology. 692J. Zhang, H. Guo, J. Liu, and Y. Zhang, \"Task offloading in vehicular edge computing networks: A load-balancing solution,\" IEEE Transac- tions on Vehicular Technology, vol. 69, no. 2, pp. 2092-2104, 2020.\n\nLearning driven noma assisted vehicular edge computing via underlay spectrum sharing. L Qian, Y Wu, N Yu, F Jiang, H Zhou, T Q S Quek, IEEE Transactions on Vehicular Technology. 701L. Qian, Y. Wu, N. Yu, F. Jiang, H. Zhou, and T. Q. S. Quek, \"Learning driven noma assisted vehicular edge computing via underlay spectrum sharing,\" IEEE Transactions on Vehicular Technology, vol. 70, no. 1, pp. 977-992, 2021.\n\nVehicle assisted computing offloading for unmanned aerial vehicles in smart city. M Dai, Z Su, Q Xu, N Zhang, IEEE Transactions on Intelligent Transportation Systems. 223M. Dai, Z. Su, Q. Xu, and N. Zhang, \"Vehicle assisted computing of- floading for unmanned aerial vehicles in smart city,\" IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 3, pp. 1932-1944, 2021.\n\nOffloading in internet of vehicles: A fog-enabled real-time traffic management system. X Wang, Z Ning, L Wang, IEEE Transactions on Industrial Informatics. 1410X. Wang, Z. Ning, and L. Wang, \"Offloading in internet of vehicles: A fog-enabled real-time traffic management system,\" IEEE Transactions on Industrial Informatics, vol. 14, no. 10, pp. 4568-4578, Oct 2018.\n\nSecuring parked vehicle assisted fog computing with blockchain and optimal smart contract design. X Huang, D Ye, R Yu, L Shu, IEEE/CAA Journal of Automatica Sinica. 72X. Huang, D. Ye, R. Yu, and L. Shu, \"Securing parked vehicle assisted fog computing with blockchain and optimal smart contract design,\" IEEE/CAA Journal of Automatica Sinica, vol. 7, no. 2, pp. 426-441, 2020.\n\nParking reservation auction for parked vehicle assistance in vehicular fog computing. Y Zhang, C.-Y. Wang, H.-Y. Wei, IEEE Transactions on Vehicular Technology. 684Y. Zhang, C.-Y. Wang, and H.-Y. Wei, \"Parking reservation auction for parked vehicle assistance in vehicular fog computing,\" IEEE Transac- tions on Vehicular Technology, vol. 68, no. 4, pp. 3126-3139, 2019.\n\nPredicting car park occupancy rates in smart cities. D H Stolfi, E Alba, X Yao, International Conference on Smart Cities. SpringerD. H. Stolfi, E. Alba, and X. Yao, \"Predicting car park occupancy rates in smart cities,\" in International Conference on Smart Cities. Springer, 2017, pp. 107-117.\n\nEfficient mobilityaware task offloading for vehicular edge computing networks. C Yang, Y Liu, X Chen, W Zhong, S Xie, IEEE Access. 7C. Yang, Y. Liu, X. Chen, W. Zhong, and S. Xie, \"Efficient mobility- aware task offloading for vehicular edge computing networks,\" IEEE Access, vol. 7, pp. 26 652-26 664, 2019.\n\nJoint offloading and computing optimization in wireless powered mobile-edge computing systems. F Wang, J Xu, X Wang, S Cui, IEEE Transactions on Wireless Communications. 173F. Wang, J. Xu, X. Wang, and S. Cui, \"Joint offloading and computing optimization in wireless powered mobile-edge computing systems,\" IEEE Transactions on Wireless Communications, vol. 17, no. 3, pp. 1784-1797, 2018.\n\nNoma assisted multi-task multi-access mobile edge computing via deep reinforcement learning for industrial internet of things. L Qian, Y Wu, F Jiang, N Yu, W Lu, B Lin, IEEE Transactions on Industrial Informatics. 178L. Qian, Y. Wu, F. Jiang, N. Yu, W. Lu, and B. Lin, \"Noma assisted multi-task multi-access mobile edge computing via deep reinforcement learning for industrial internet of things,\" IEEE Transactions on Indus- trial Informatics, vol. 17, no. 8, pp. 5688-5698, 2021.\n\nReward optimization for content providers with mobile data subsidization: A hierarchical game approach. Z Xiong, J Zhao, D Niyato, R Deng, J Zhang, IEEE Transactions on Network Science and Engineering. 74Z. Xiong, J. Zhao, D. Niyato, R. Deng, and J. Zhang, \"Reward optimization for content providers with mobile data subsidization: A hierarchical game approach,\" IEEE Transactions on Network Science and Engineering, vol. 7, no. 4, pp. 2363-2377, 2020.\n\nFuture intelligent and secure vehicular network toward 6g: Machine-learning approaches. F Tang, Y Kawamoto, N Kato, J Liu, Proceedings of the IEEE. 1082F. Tang, Y. Kawamoto, N. Kato, and J. Liu, \"Future intelligent and secure vehicular network toward 6g: Machine-learning approaches,\" Proceedings of the IEEE, vol. 108, no. 2, pp. 292-307, 2020.\n\nReinforcement learningbased radio resource control in 5g vehicular network. Y Zhou, F Tang, Y Kawamoto, N Kato, IEEE Wireless Communications Letters. 95Y. Zhou, F. Tang, Y. Kawamoto, and N. Kato, \"Reinforcement learning- based radio resource control in 5g vehicular network,\" IEEE Wireless Communications Letters, vol. 9, no. 5, pp. 611-614, 2020.\n\nA learning-based incentive mechanism for federated learning. Y Zhan, P Li, Z Qu, D Zeng, S Guo, IEEE Internet of Things Journal. 77Y. Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, \"A learning-based incentive mechanism for federated learning,\" IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6360-6368, 2020.\n\nSmart and resilient ev charging in sdn-enhanced vehicular edge computing networks. J Liu, H Guo, J Xiong, N Kato, J Zhang, Y Zhang, IEEE Journal on Selected Areas in Communications. 381J. Liu, H. Guo, J. Xiong, N. Kato, J. Zhang, and Y. Zhang, \"Smart and resilient ev charging in sdn-enhanced vehicular edge computing networks,\" IEEE Journal on Selected Areas in Communications, vol. 38, no. 1, pp. 217-228, 2020.\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, NIPs. Citeseer99R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour et al., \"Policy gradient methods for reinforcement learning with function approxima- tion.\" in NIPs, vol. 99. Citeseer, 1999, pp. 1057-1063.\n\nProximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \"Prox- imal policy optimization algorithms,\" arXiv preprint arXiv:1707.06347, 2017.\n\nHybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwi\u0144ska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 5387626A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska- Barwi\u0144ska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou et al., \"Hybrid computing using a neural network with dynamic external memory,\" Nature, vol. 538, no. 7626, pp. 471-476, 2016.\n", "annotations": {"author": "[{\"end\":134,\"start\":122},{\"end\":166,\"start\":135},{\"end\":187,\"start\":167},{\"end\":215,\"start\":188},{\"end\":224,\"start\":216},{\"end\":249,\"start\":225}]", "publisher": null, "author_last_name": "[{\"end\":133,\"start\":128},{\"end\":186,\"start\":184},{\"end\":223,\"start\":220},{\"end\":248,\"start\":245}]", "author_first_name": "[{\"end\":127,\"start\":122},{\"end\":162,\"start\":155},{\"end\":165,\"start\":163},{\"end\":183,\"start\":179},{\"end\":211,\"start\":207},{\"end\":214,\"start\":212},{\"end\":219,\"start\":216},{\"end\":244,\"start\":237}]", "author_affiliation": null, "title": "[{\"end\":108,\"start\":1},{\"end\":357,\"start\":250}]", "venue": null, "abstract": "[{\"end\":1690,\"start\":370}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1943,\"start\":1940},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1948,\"start\":1945},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2335,\"start\":2332},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2370,\"start\":2367},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2404,\"start\":2401},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2528,\"start\":2525},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2550,\"start\":2547},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3813,\"start\":3810},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5920,\"start\":5917},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10488,\"start\":10485},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10647,\"start\":10644},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10709,\"start\":10705},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10933,\"start\":10929},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10996,\"start\":10992},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12142,\"start\":12138},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12443,\"start\":12439},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12559,\"start\":12555},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12588,\"start\":12584},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12594,\"start\":12590},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12687,\"start\":12683},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12807,\"start\":12803},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13065,\"start\":13061},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13283,\"start\":13279},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19451,\"start\":19448},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20285,\"start\":20281},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22199,\"start\":22196},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22284,\"start\":22281},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27053,\"start\":27049},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27335,\"start\":27331},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27341,\"start\":27337},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27566,\"start\":27562},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38052,\"start\":38048},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38058,\"start\":38054},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38091,\"start\":38087},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":38471,\"start\":38467},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39674,\"start\":39670},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":44043,\"start\":44039},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":44106,\"start\":44102},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":44647,\"start\":44643},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45453,\"start\":45449},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46535,\"start\":46532},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46537,\"start\":46535},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46558,\"start\":46555},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":46560,\"start\":46558},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47006,\"start\":47003},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":47008,\"start\":47006},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50172,\"start\":50168},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50175,\"start\":50172},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50177,\"start\":50175},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":54240,\"start\":54236}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56477,\"start\":56438},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57042,\"start\":56478},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57110,\"start\":57043},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57177,\"start\":57111},{\"attributes\":{\"id\":\"fig_5\"},\"end\":57448,\"start\":57178},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57536,\"start\":57449},{\"attributes\":{\"id\":\"fig_8\"},\"end\":57747,\"start\":57537},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57853,\"start\":57748},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":58193,\"start\":57854}]", "paragraph": "[{\"end\":2836,\"start\":1709},{\"end\":3970,\"start\":2838},{\"end\":5426,\"start\":3972},{\"end\":6554,\"start\":5428},{\"end\":7837,\"start\":6556},{\"end\":7904,\"start\":7839},{\"end\":9952,\"start\":7906},{\"end\":11037,\"start\":10020},{\"end\":11792,\"start\":11039},{\"end\":14468,\"start\":11838},{\"end\":16313,\"start\":14470},{\"end\":17000,\"start\":16333},{\"end\":17142,\"start\":17002},{\"end\":17745,\"start\":17181},{\"end\":19922,\"start\":17747},{\"end\":20801,\"start\":19963},{\"end\":21022,\"start\":20857},{\"end\":21623,\"start\":21024},{\"end\":22061,\"start\":21625},{\"end\":22365,\"start\":22107},{\"end\":22835,\"start\":22367},{\"end\":23142,\"start\":22862},{\"end\":24005,\"start\":23144},{\"end\":24074,\"start\":24007},{\"end\":24652,\"start\":24119},{\"end\":24781,\"start\":24697},{\"end\":24984,\"start\":24827},{\"end\":25149,\"start\":25023},{\"end\":25458,\"start\":25215},{\"end\":25676,\"start\":25460},{\"end\":26222,\"start\":25754},{\"end\":27446,\"start\":26250},{\"end\":28159,\"start\":27497},{\"end\":28283,\"start\":28187},{\"end\":28497,\"start\":28324},{\"end\":28892,\"start\":28537},{\"end\":30333,\"start\":28959},{\"end\":30660,\"start\":30335},{\"end\":31562,\"start\":30763},{\"end\":31797,\"start\":31629},{\"end\":32077,\"start\":31831},{\"end\":32364,\"start\":32178},{\"end\":32624,\"start\":32416},{\"end\":33161,\"start\":32666},{\"end\":33292,\"start\":33163},{\"end\":33345,\"start\":33294},{\"end\":33558,\"start\":33448},{\"end\":34112,\"start\":33816},{\"end\":34199,\"start\":34114},{\"end\":34401,\"start\":34201},{\"end\":34931,\"start\":34477},{\"end\":35023,\"start\":35011},{\"end\":35590,\"start\":35118},{\"end\":35673,\"start\":35592},{\"end\":35936,\"start\":35675},{\"end\":36119,\"start\":35938},{\"end\":36281,\"start\":36170},{\"end\":36870,\"start\":36322},{\"end\":38890,\"start\":37204},{\"end\":39232,\"start\":38932},{\"end\":40394,\"start\":39234},{\"end\":40676,\"start\":40396},{\"end\":41420,\"start\":40698},{\"end\":42413,\"start\":41517},{\"end\":42678,\"start\":42563},{\"end\":43093,\"start\":42728},{\"end\":43218,\"start\":43213},{\"end\":43554,\"start\":43357},{\"end\":44142,\"start\":43607},{\"end\":44303,\"start\":44298},{\"end\":44779,\"start\":44429},{\"end\":44951,\"start\":44862},{\"end\":45942,\"start\":45037},{\"end\":47212,\"start\":45992},{\"end\":48510,\"start\":47259},{\"end\":49928,\"start\":48552},{\"end\":52002,\"start\":49930},{\"end\":52233,\"start\":52004},{\"end\":54650,\"start\":52235},{\"end\":55294,\"start\":54652},{\"end\":56437,\"start\":55315}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20856,\"start\":20802},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22106,\"start\":22062},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24118,\"start\":24075},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24696,\"start\":24653},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24826,\"start\":24782},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25022,\"start\":24985},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25214,\"start\":25150},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27496,\"start\":27447},{\"attributes\":{\"id\":\"formula_8\"},\"end\":28186,\"start\":28160},{\"attributes\":{\"id\":\"formula_9\"},\"end\":28323,\"start\":28284},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28958,\"start\":28893},{\"attributes\":{\"id\":\"formula_11\"},\"end\":30762,\"start\":30661},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32177,\"start\":32078},{\"attributes\":{\"id\":\"formula_13\"},\"end\":32415,\"start\":32365},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33447,\"start\":33346},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33815,\"start\":33559},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34476,\"start\":34402},{\"attributes\":{\"id\":\"formula_17\"},\"end\":35010,\"start\":34932},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35117,\"start\":35024},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36169,\"start\":36120},{\"attributes\":{\"id\":\"formula_20\"},\"end\":36321,\"start\":36282},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37153,\"start\":36871},{\"attributes\":{\"id\":\"formula_22\"},\"end\":41516,\"start\":41421},{\"attributes\":{\"id\":\"formula_23\"},\"end\":42562,\"start\":42414},{\"attributes\":{\"id\":\"formula_24\"},\"end\":42727,\"start\":42679},{\"attributes\":{\"id\":\"formula_25\"},\"end\":43212,\"start\":43094},{\"attributes\":{\"id\":\"formula_26\"},\"end\":43356,\"start\":43219},{\"attributes\":{\"id\":\"formula_27\"},\"end\":44297,\"start\":44143},{\"attributes\":{\"id\":\"formula_28\"},\"end\":44428,\"start\":44304},{\"attributes\":{\"id\":\"formula_29\"},\"end\":44861,\"start\":44780},{\"attributes\":{\"id\":\"formula_30\"},\"end\":45036,\"start\":44952}]", "table_ref": null, "section_header": "[{\"end\":1707,\"start\":1692},{\"end\":9971,\"start\":9955},{\"end\":10018,\"start\":9974},{\"end\":11836,\"start\":11795},{\"end\":16331,\"start\":16316},{\"end\":17179,\"start\":17145},{\"end\":19961,\"start\":19925},{\"end\":22860,\"start\":22838},{\"end\":25752,\"start\":25679},{\"end\":26248,\"start\":26225},{\"end\":28535,\"start\":28500},{\"end\":31627,\"start\":31565},{\"end\":31829,\"start\":31800},{\"end\":32664,\"start\":32627},{\"end\":37202,\"start\":37155},{\"end\":38930,\"start\":38893},{\"end\":40696,\"start\":40679},{\"end\":43605,\"start\":43557},{\"end\":45967,\"start\":45945},{\"end\":45990,\"start\":45970},{\"end\":47257,\"start\":47215},{\"end\":48550,\"start\":48513},{\"end\":55313,\"start\":55297},{\"end\":56447,\"start\":56439},{\"end\":56480,\"start\":56479},{\"end\":57052,\"start\":57044},{\"end\":57120,\"start\":57112},{\"end\":57187,\"start\":57179},{\"end\":57458,\"start\":57450},{\"end\":57554,\"start\":57538},{\"end\":57757,\"start\":57749}]", "table": "[{\"end\":58193,\"start\":57928}]", "figure_caption": "[{\"end\":56477,\"start\":56449},{\"end\":57042,\"start\":56481},{\"end\":57110,\"start\":57054},{\"end\":57177,\"start\":57122},{\"end\":57448,\"start\":57189},{\"end\":57536,\"start\":57460},{\"end\":57747,\"start\":57557},{\"end\":57853,\"start\":57759},{\"end\":57928,\"start\":57856}]", "figure_ref": "[{\"end\":14630,\"start\":14624},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17237,\"start\":17231},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22304,\"start\":22298},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39094,\"start\":39088},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":45216,\"start\":45210},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48067,\"start\":48061},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":49418,\"start\":49412},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":50296,\"start\":50290},{\"end\":50559,\"start\":50553},{\"end\":52351,\"start\":52345},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":53743,\"start\":53737},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":54360,\"start\":54354}]", "bib_author_first_name": "[{\"end\":58271,\"start\":58270},{\"end\":58282,\"start\":58281},{\"end\":58291,\"start\":58290},{\"end\":58301,\"start\":58300},{\"end\":58312,\"start\":58311},{\"end\":58314,\"start\":58313},{\"end\":58630,\"start\":58629},{\"end\":58638,\"start\":58637},{\"end\":58645,\"start\":58644},{\"end\":58653,\"start\":58652},{\"end\":58969,\"start\":58968},{\"end\":58977,\"start\":58976},{\"end\":58984,\"start\":58983},{\"end\":58995,\"start\":58994},{\"end\":59008,\"start\":59007},{\"end\":59020,\"start\":59019},{\"end\":59034,\"start\":59033},{\"end\":59045,\"start\":59044},{\"end\":59055,\"start\":59054},{\"end\":59375,\"start\":59374},{\"end\":59383,\"start\":59382},{\"end\":59390,\"start\":59389},{\"end\":59398,\"start\":59397},{\"end\":59404,\"start\":59403},{\"end\":59741,\"start\":59740},{\"end\":59743,\"start\":59742},{\"end\":59756,\"start\":59755},{\"end\":60106,\"start\":60105},{\"end\":60113,\"start\":60112},{\"end\":60117,\"start\":60114},{\"end\":60123,\"start\":60122},{\"end\":60131,\"start\":60130},{\"end\":60141,\"start\":60140},{\"end\":60491,\"start\":60490},{\"end\":60500,\"start\":60499},{\"end\":60509,\"start\":60508},{\"end\":60513,\"start\":60510},{\"end\":60519,\"start\":60518},{\"end\":60835,\"start\":60834},{\"end\":60837,\"start\":60836},{\"end\":60845,\"start\":60844},{\"end\":60847,\"start\":60846},{\"end\":60862,\"start\":60861},{\"end\":61196,\"start\":61195},{\"end\":61205,\"start\":61204},{\"end\":61211,\"start\":61210},{\"end\":61218,\"start\":61217},{\"end\":61583,\"start\":61582},{\"end\":61589,\"start\":61588},{\"end\":61598,\"start\":61597},{\"end\":61605,\"start\":61604},{\"end\":61617,\"start\":61616},{\"end\":61919,\"start\":61918},{\"end\":61933,\"start\":61932},{\"end\":61943,\"start\":61942},{\"end\":61951,\"start\":61950},{\"end\":62306,\"start\":62305},{\"end\":62312,\"start\":62311},{\"end\":62318,\"start\":62317},{\"end\":62325,\"start\":62324},{\"end\":62640,\"start\":62639},{\"end\":62648,\"start\":62647},{\"end\":62655,\"start\":62654},{\"end\":62663,\"start\":62662},{\"end\":62672,\"start\":62671},{\"end\":62682,\"start\":62681},{\"end\":63145,\"start\":63144},{\"end\":63154,\"start\":63153},{\"end\":63160,\"start\":63159},{\"end\":63166,\"start\":63165},{\"end\":63173,\"start\":63172},{\"end\":63608,\"start\":63607},{\"end\":63614,\"start\":63613},{\"end\":63622,\"start\":63621},{\"end\":63630,\"start\":63629},{\"end\":63992,\"start\":63991},{\"end\":64001,\"start\":64000},{\"end\":64008,\"start\":64007},{\"end\":64015,\"start\":64014},{\"end\":64362,\"start\":64361},{\"end\":64370,\"start\":64369},{\"end\":64376,\"start\":64375},{\"end\":64382,\"start\":64381},{\"end\":64391,\"start\":64390},{\"end\":64399,\"start\":64398},{\"end\":64403,\"start\":64400},{\"end\":64767,\"start\":64766},{\"end\":64774,\"start\":64773},{\"end\":64780,\"start\":64779},{\"end\":64786,\"start\":64785},{\"end\":65159,\"start\":65158},{\"end\":65167,\"start\":65166},{\"end\":65175,\"start\":65174},{\"end\":65538,\"start\":65537},{\"end\":65547,\"start\":65546},{\"end\":65553,\"start\":65552},{\"end\":65559,\"start\":65558},{\"end\":65903,\"start\":65902},{\"end\":65916,\"start\":65911},{\"end\":65928,\"start\":65923},{\"end\":66242,\"start\":66241},{\"end\":66244,\"start\":66243},{\"end\":66254,\"start\":66253},{\"end\":66262,\"start\":66261},{\"end\":66563,\"start\":66562},{\"end\":66571,\"start\":66570},{\"end\":66578,\"start\":66577},{\"end\":66586,\"start\":66585},{\"end\":66595,\"start\":66594},{\"end\":66889,\"start\":66888},{\"end\":66897,\"start\":66896},{\"end\":66903,\"start\":66902},{\"end\":66911,\"start\":66910},{\"end\":67312,\"start\":67311},{\"end\":67320,\"start\":67319},{\"end\":67326,\"start\":67325},{\"end\":67335,\"start\":67334},{\"end\":67341,\"start\":67340},{\"end\":67347,\"start\":67346},{\"end\":67772,\"start\":67771},{\"end\":67781,\"start\":67780},{\"end\":67789,\"start\":67788},{\"end\":67799,\"start\":67798},{\"end\":67807,\"start\":67806},{\"end\":68210,\"start\":68209},{\"end\":68218,\"start\":68217},{\"end\":68230,\"start\":68229},{\"end\":68238,\"start\":68237},{\"end\":68545,\"start\":68544},{\"end\":68553,\"start\":68552},{\"end\":68561,\"start\":68560},{\"end\":68573,\"start\":68572},{\"end\":68879,\"start\":68878},{\"end\":68887,\"start\":68886},{\"end\":68893,\"start\":68892},{\"end\":68899,\"start\":68898},{\"end\":68907,\"start\":68906},{\"end\":69209,\"start\":69208},{\"end\":69216,\"start\":69215},{\"end\":69223,\"start\":69222},{\"end\":69232,\"start\":69231},{\"end\":69240,\"start\":69239},{\"end\":69249,\"start\":69248},{\"end\":69621,\"start\":69620},{\"end\":69623,\"start\":69622},{\"end\":69633,\"start\":69632},{\"end\":69635,\"start\":69634},{\"end\":69649,\"start\":69648},{\"end\":69651,\"start\":69650},{\"end\":69660,\"start\":69659},{\"end\":69926,\"start\":69925},{\"end\":69938,\"start\":69937},{\"end\":69948,\"start\":69947},{\"end\":69960,\"start\":69959},{\"end\":69971,\"start\":69970},{\"end\":70230,\"start\":70229},{\"end\":70240,\"start\":70239},{\"end\":70249,\"start\":70248},{\"end\":70261,\"start\":70260},{\"end\":70271,\"start\":70270},{\"end\":70284,\"start\":70283},{\"end\":70305,\"start\":70304},{\"end\":70307,\"start\":70306},{\"end\":70322,\"start\":70321},{\"end\":70338,\"start\":70337},{\"end\":70349,\"start\":70348}]", "bib_author_last_name": "[{\"end\":58279,\"start\":58272},{\"end\":58288,\"start\":58283},{\"end\":58298,\"start\":58292},{\"end\":58309,\"start\":58302},{\"end\":58320,\"start\":58315},{\"end\":58635,\"start\":58631},{\"end\":58642,\"start\":58639},{\"end\":58650,\"start\":58646},{\"end\":58658,\"start\":58654},{\"end\":58974,\"start\":58970},{\"end\":58981,\"start\":58978},{\"end\":58992,\"start\":58985},{\"end\":59005,\"start\":58996},{\"end\":59017,\"start\":59009},{\"end\":59031,\"start\":59021},{\"end\":59042,\"start\":59035},{\"end\":59052,\"start\":59046},{\"end\":59062,\"start\":59056},{\"end\":59380,\"start\":59376},{\"end\":59387,\"start\":59384},{\"end\":59395,\"start\":59391},{\"end\":59401,\"start\":59399},{\"end\":59408,\"start\":59405},{\"end\":59753,\"start\":59744},{\"end\":59761,\"start\":59757},{\"end\":60110,\"start\":60107},{\"end\":60120,\"start\":60118},{\"end\":60128,\"start\":60124},{\"end\":60138,\"start\":60132},{\"end\":60147,\"start\":60142},{\"end\":60497,\"start\":60492},{\"end\":60506,\"start\":60501},{\"end\":60516,\"start\":60514},{\"end\":60522,\"start\":60520},{\"end\":60842,\"start\":60838},{\"end\":60859,\"start\":60848},{\"end\":60874,\"start\":60863},{\"end\":61202,\"start\":61197},{\"end\":61208,\"start\":61206},{\"end\":61215,\"start\":61212},{\"end\":61224,\"start\":61219},{\"end\":61586,\"start\":61584},{\"end\":61595,\"start\":61590},{\"end\":61602,\"start\":61599},{\"end\":61614,\"start\":61606},{\"end\":61623,\"start\":61618},{\"end\":61930,\"start\":61920},{\"end\":61940,\"start\":61934},{\"end\":61948,\"start\":61944},{\"end\":61958,\"start\":61952},{\"end\":62309,\"start\":62307},{\"end\":62315,\"start\":62313},{\"end\":62322,\"start\":62319},{\"end\":62329,\"start\":62326},{\"end\":62645,\"start\":62641},{\"end\":62652,\"start\":62649},{\"end\":62660,\"start\":62656},{\"end\":62669,\"start\":62664},{\"end\":62679,\"start\":62673},{\"end\":62692,\"start\":62683},{\"end\":63151,\"start\":63146},{\"end\":63157,\"start\":63155},{\"end\":63163,\"start\":63161},{\"end\":63170,\"start\":63167},{\"end\":63177,\"start\":63174},{\"end\":63611,\"start\":63609},{\"end\":63619,\"start\":63615},{\"end\":63627,\"start\":63623},{\"end\":63635,\"start\":63631},{\"end\":63998,\"start\":63993},{\"end\":64005,\"start\":64002},{\"end\":64012,\"start\":64009},{\"end\":64021,\"start\":64016},{\"end\":64367,\"start\":64363},{\"end\":64373,\"start\":64371},{\"end\":64379,\"start\":64377},{\"end\":64388,\"start\":64383},{\"end\":64396,\"start\":64392},{\"end\":64408,\"start\":64404},{\"end\":64771,\"start\":64768},{\"end\":64777,\"start\":64775},{\"end\":64783,\"start\":64781},{\"end\":64792,\"start\":64787},{\"end\":65164,\"start\":65160},{\"end\":65172,\"start\":65168},{\"end\":65180,\"start\":65176},{\"end\":65544,\"start\":65539},{\"end\":65550,\"start\":65548},{\"end\":65556,\"start\":65554},{\"end\":65563,\"start\":65560},{\"end\":65909,\"start\":65904},{\"end\":65921,\"start\":65917},{\"end\":65932,\"start\":65929},{\"end\":66251,\"start\":66245},{\"end\":66259,\"start\":66255},{\"end\":66266,\"start\":66263},{\"end\":66568,\"start\":66564},{\"end\":66575,\"start\":66572},{\"end\":66583,\"start\":66579},{\"end\":66592,\"start\":66587},{\"end\":66599,\"start\":66596},{\"end\":66894,\"start\":66890},{\"end\":66900,\"start\":66898},{\"end\":66908,\"start\":66904},{\"end\":66915,\"start\":66912},{\"end\":67317,\"start\":67313},{\"end\":67323,\"start\":67321},{\"end\":67332,\"start\":67327},{\"end\":67338,\"start\":67336},{\"end\":67344,\"start\":67342},{\"end\":67351,\"start\":67348},{\"end\":67778,\"start\":67773},{\"end\":67786,\"start\":67782},{\"end\":67796,\"start\":67790},{\"end\":67804,\"start\":67800},{\"end\":67813,\"start\":67808},{\"end\":68215,\"start\":68211},{\"end\":68227,\"start\":68219},{\"end\":68235,\"start\":68231},{\"end\":68242,\"start\":68239},{\"end\":68550,\"start\":68546},{\"end\":68558,\"start\":68554},{\"end\":68570,\"start\":68562},{\"end\":68578,\"start\":68574},{\"end\":68884,\"start\":68880},{\"end\":68890,\"start\":68888},{\"end\":68896,\"start\":68894},{\"end\":68904,\"start\":68900},{\"end\":68911,\"start\":68908},{\"end\":69213,\"start\":69210},{\"end\":69220,\"start\":69217},{\"end\":69229,\"start\":69224},{\"end\":69237,\"start\":69233},{\"end\":69246,\"start\":69241},{\"end\":69255,\"start\":69250},{\"end\":69630,\"start\":69624},{\"end\":69646,\"start\":69636},{\"end\":69657,\"start\":69652},{\"end\":69668,\"start\":69661},{\"end\":69935,\"start\":69927},{\"end\":69945,\"start\":69939},{\"end\":69957,\"start\":69949},{\"end\":69968,\"start\":69961},{\"end\":69978,\"start\":69972},{\"end\":70237,\"start\":70231},{\"end\":70246,\"start\":70241},{\"end\":70258,\"start\":70250},{\"end\":70268,\"start\":70262},{\"end\":70281,\"start\":70272},{\"end\":70302,\"start\":70285},{\"end\":70319,\"start\":70308},{\"end\":70335,\"start\":70323},{\"end\":70346,\"start\":70339},{\"end\":70357,\"start\":70350}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14955348},\"end\":58573,\"start\":58195},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":219878182},\"end\":58915,\"start\":58575},{\"attributes\":{\"doi\":\"arXiv:1811.03604\",\"id\":\"b2\"},\"end\":59296,\"start\":58917},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":198147615},\"end\":59620,\"start\":59298},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216362694},\"end\":60028,\"start\":59622},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":213175498},\"end\":60384,\"start\":60030},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":234021722},\"end\":60778,\"start\":60386},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":474078},\"end\":61095,\"start\":60780},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218950288},\"end\":61494,\"start\":61097},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219316707},\"end\":61828,\"start\":61496},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49907380},\"end\":62215,\"start\":61830},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211119588},\"end\":62514,\"start\":62217},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":86808679},\"end\":63011,\"start\":62516},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":233877087},\"end\":63483,\"start\":63013},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":61811406},\"end\":63908,\"start\":63485},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211208022},\"end\":64273,\"start\":63910},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":231920850},\"end\":64682,\"start\":64275},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":232150149},\"end\":65069,\"start\":64684},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":52933038},\"end\":65437,\"start\":65071},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":212637143},\"end\":65814,\"start\":65439},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":86627335},\"end\":66186,\"start\":65816},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206698246},\"end\":66481,\"start\":66188},{\"attributes\":{\"id\":\"b22\"},\"end\":66791,\"start\":66483},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2172141},\"end\":67182,\"start\":66793},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":226583034},\"end\":67665,\"start\":67184},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":226431790},\"end\":68119,\"start\":67667},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":210970531},\"end\":68466,\"start\":68121},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":213784641},\"end\":68815,\"start\":68468},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":212426931},\"end\":69123,\"start\":68817},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":209048180},\"end\":69538,\"start\":69125},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1211821},\"end\":69882,\"start\":69540},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b31\"},\"end\":70157,\"start\":69884},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":205251479},\"end\":70641,\"start\":70159}]", "bib_title": "[{\"end\":58268,\"start\":58195},{\"end\":58627,\"start\":58575},{\"end\":59372,\"start\":59298},{\"end\":59738,\"start\":59622},{\"end\":60103,\"start\":60030},{\"end\":60488,\"start\":60386},{\"end\":60832,\"start\":60780},{\"end\":61193,\"start\":61097},{\"end\":61580,\"start\":61496},{\"end\":61916,\"start\":61830},{\"end\":62303,\"start\":62217},{\"end\":62637,\"start\":62516},{\"end\":63142,\"start\":63013},{\"end\":63605,\"start\":63485},{\"end\":63989,\"start\":63910},{\"end\":64359,\"start\":64275},{\"end\":64764,\"start\":64684},{\"end\":65156,\"start\":65071},{\"end\":65535,\"start\":65439},{\"end\":65900,\"start\":65816},{\"end\":66239,\"start\":66188},{\"end\":66560,\"start\":66483},{\"end\":66886,\"start\":66793},{\"end\":67309,\"start\":67184},{\"end\":67769,\"start\":67667},{\"end\":68207,\"start\":68121},{\"end\":68542,\"start\":68468},{\"end\":68876,\"start\":68817},{\"end\":69206,\"start\":69125},{\"end\":69618,\"start\":69540},{\"end\":70227,\"start\":70159}]", "bib_author": "[{\"end\":58281,\"start\":58270},{\"end\":58290,\"start\":58281},{\"end\":58300,\"start\":58290},{\"end\":58311,\"start\":58300},{\"end\":58322,\"start\":58311},{\"end\":58637,\"start\":58629},{\"end\":58644,\"start\":58637},{\"end\":58652,\"start\":58644},{\"end\":58660,\"start\":58652},{\"end\":58976,\"start\":58968},{\"end\":58983,\"start\":58976},{\"end\":58994,\"start\":58983},{\"end\":59007,\"start\":58994},{\"end\":59019,\"start\":59007},{\"end\":59033,\"start\":59019},{\"end\":59044,\"start\":59033},{\"end\":59054,\"start\":59044},{\"end\":59064,\"start\":59054},{\"end\":59382,\"start\":59374},{\"end\":59389,\"start\":59382},{\"end\":59397,\"start\":59389},{\"end\":59403,\"start\":59397},{\"end\":59410,\"start\":59403},{\"end\":59755,\"start\":59740},{\"end\":59763,\"start\":59755},{\"end\":60112,\"start\":60105},{\"end\":60122,\"start\":60112},{\"end\":60130,\"start\":60122},{\"end\":60140,\"start\":60130},{\"end\":60149,\"start\":60140},{\"end\":60499,\"start\":60490},{\"end\":60508,\"start\":60499},{\"end\":60518,\"start\":60508},{\"end\":60524,\"start\":60518},{\"end\":60844,\"start\":60834},{\"end\":60861,\"start\":60844},{\"end\":60876,\"start\":60861},{\"end\":61204,\"start\":61195},{\"end\":61210,\"start\":61204},{\"end\":61217,\"start\":61210},{\"end\":61226,\"start\":61217},{\"end\":61588,\"start\":61582},{\"end\":61597,\"start\":61588},{\"end\":61604,\"start\":61597},{\"end\":61616,\"start\":61604},{\"end\":61625,\"start\":61616},{\"end\":61932,\"start\":61918},{\"end\":61942,\"start\":61932},{\"end\":61950,\"start\":61942},{\"end\":61960,\"start\":61950},{\"end\":62311,\"start\":62305},{\"end\":62317,\"start\":62311},{\"end\":62324,\"start\":62317},{\"end\":62331,\"start\":62324},{\"end\":62647,\"start\":62639},{\"end\":62654,\"start\":62647},{\"end\":62662,\"start\":62654},{\"end\":62671,\"start\":62662},{\"end\":62681,\"start\":62671},{\"end\":62694,\"start\":62681},{\"end\":63153,\"start\":63144},{\"end\":63159,\"start\":63153},{\"end\":63165,\"start\":63159},{\"end\":63172,\"start\":63165},{\"end\":63179,\"start\":63172},{\"end\":63613,\"start\":63607},{\"end\":63621,\"start\":63613},{\"end\":63629,\"start\":63621},{\"end\":63637,\"start\":63629},{\"end\":64000,\"start\":63991},{\"end\":64007,\"start\":64000},{\"end\":64014,\"start\":64007},{\"end\":64023,\"start\":64014},{\"end\":64369,\"start\":64361},{\"end\":64375,\"start\":64369},{\"end\":64381,\"start\":64375},{\"end\":64390,\"start\":64381},{\"end\":64398,\"start\":64390},{\"end\":64410,\"start\":64398},{\"end\":64773,\"start\":64766},{\"end\":64779,\"start\":64773},{\"end\":64785,\"start\":64779},{\"end\":64794,\"start\":64785},{\"end\":65166,\"start\":65158},{\"end\":65174,\"start\":65166},{\"end\":65182,\"start\":65174},{\"end\":65546,\"start\":65537},{\"end\":65552,\"start\":65546},{\"end\":65558,\"start\":65552},{\"end\":65565,\"start\":65558},{\"end\":65911,\"start\":65902},{\"end\":65923,\"start\":65911},{\"end\":65934,\"start\":65923},{\"end\":66253,\"start\":66241},{\"end\":66261,\"start\":66253},{\"end\":66268,\"start\":66261},{\"end\":66570,\"start\":66562},{\"end\":66577,\"start\":66570},{\"end\":66585,\"start\":66577},{\"end\":66594,\"start\":66585},{\"end\":66601,\"start\":66594},{\"end\":66896,\"start\":66888},{\"end\":66902,\"start\":66896},{\"end\":66910,\"start\":66902},{\"end\":66917,\"start\":66910},{\"end\":67319,\"start\":67311},{\"end\":67325,\"start\":67319},{\"end\":67334,\"start\":67325},{\"end\":67340,\"start\":67334},{\"end\":67346,\"start\":67340},{\"end\":67353,\"start\":67346},{\"end\":67780,\"start\":67771},{\"end\":67788,\"start\":67780},{\"end\":67798,\"start\":67788},{\"end\":67806,\"start\":67798},{\"end\":67815,\"start\":67806},{\"end\":68217,\"start\":68209},{\"end\":68229,\"start\":68217},{\"end\":68237,\"start\":68229},{\"end\":68244,\"start\":68237},{\"end\":68552,\"start\":68544},{\"end\":68560,\"start\":68552},{\"end\":68572,\"start\":68560},{\"end\":68580,\"start\":68572},{\"end\":68886,\"start\":68878},{\"end\":68892,\"start\":68886},{\"end\":68898,\"start\":68892},{\"end\":68906,\"start\":68898},{\"end\":68913,\"start\":68906},{\"end\":69215,\"start\":69208},{\"end\":69222,\"start\":69215},{\"end\":69231,\"start\":69222},{\"end\":69239,\"start\":69231},{\"end\":69248,\"start\":69239},{\"end\":69257,\"start\":69248},{\"end\":69632,\"start\":69620},{\"end\":69648,\"start\":69632},{\"end\":69659,\"start\":69648},{\"end\":69670,\"start\":69659},{\"end\":69937,\"start\":69925},{\"end\":69947,\"start\":69937},{\"end\":69959,\"start\":69947},{\"end\":69970,\"start\":69959},{\"end\":69980,\"start\":69970},{\"end\":70239,\"start\":70229},{\"end\":70248,\"start\":70239},{\"end\":70260,\"start\":70248},{\"end\":70270,\"start\":70260},{\"end\":70283,\"start\":70270},{\"end\":70304,\"start\":70283},{\"end\":70321,\"start\":70304},{\"end\":70337,\"start\":70321},{\"end\":70348,\"start\":70337},{\"end\":70359,\"start\":70348}]", "bib_venue": "[{\"end\":58360,\"start\":58322},{\"end\":58721,\"start\":58660},{\"end\":58966,\"start\":58917},{\"end\":59434,\"start\":59410},{\"end\":59812,\"start\":59763},{\"end\":60180,\"start\":60149},{\"end\":60567,\"start\":60524},{\"end\":60912,\"start\":60876},{\"end\":61281,\"start\":61226},{\"end\":61637,\"start\":61625},{\"end\":61995,\"start\":61960},{\"end\":62342,\"start\":62331},{\"end\":62735,\"start\":62694},{\"end\":63220,\"start\":63179},{\"end\":63671,\"start\":63637},{\"end\":64064,\"start\":64023},{\"end\":64451,\"start\":64410},{\"end\":64849,\"start\":64794},{\"end\":65225,\"start\":65182},{\"end\":65602,\"start\":65565},{\"end\":65975,\"start\":65934},{\"end\":66308,\"start\":66268},{\"end\":66612,\"start\":66601},{\"end\":66961,\"start\":66917},{\"end\":67396,\"start\":67353},{\"end\":67867,\"start\":67815},{\"end\":68267,\"start\":68244},{\"end\":68616,\"start\":68580},{\"end\":68944,\"start\":68913},{\"end\":69305,\"start\":69257},{\"end\":69674,\"start\":69670},{\"end\":69923,\"start\":69884},{\"end\":70365,\"start\":70359}]"}}}, "year": 2023, "month": 12, "day": 17}