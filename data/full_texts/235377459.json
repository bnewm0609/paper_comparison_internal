{"id": 235377459, "updated": "2023-11-10 23:08:34.84", "metadata": {"title": "Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks", "authors": "[{\"first\":\"Julia\",\"last\":\"Kreutzer\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Riezler\",\"middle\":[]},{\"first\":\"Carolin\",\"last\":\"Lawrence\",\"middle\":[]}]", "venue": "SPNLP", "journal": "Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2021.spnlp-1.4", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl-spnlp/KreutzerRL21", "doi": "10.18653/v1/2021.spnlp-1.4"}}, "content": {"source": {"pdf_hash": "d441036cc4621b03a34e3caee98d139e41516ddf", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.spnlp-1.4.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2021.spnlp-1.4.pdf", "status": "HYBRID"}}, "grobid": {"id": "7820380147498d64d94750779a1b7c4104c16137", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d441036cc4621b03a34e3caee98d139e41516ddf.txt", "contents": "\nOffline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks\nAugust 1-6, 2021\n\nJulia Kreutzer jkreutzer@google.com \nGoogle Research\nMontrealCanada\n\nStefan Riezler riezler@cl.uni-heidelberg.de \nComputational Linguistics & IWR\nHeidelberg University\nGermany\n\nCarolin Lawrence carolin.lawrence@neclab.eu \nNEC Laboratories Europe\nHeidelbergGermany\n\nOffline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks\n\nProceedings of the 5th Workshop on Structured Prediction for NLP\nthe 5th Workshop on Structured Prediction for NLPAugust 1-6, 202137\nLarge volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions. * All authors contributed equally, order has been randomized (see https://bit.ly/38PgRjm).\n\nIntroduction\n\nWhen Natural Language Processing (NLP) systems are deployed in production, and interact with users (\"the real world\"), there are many potential ways of collecting feedback data or rich interaction logs. For example, one can ask for explicit user ratings (Kreutzer et al., 2018a), or collect user clicks (De Bona et al., 2010), or elicit user revisions (Trivedi et al., 2019) to get an estimate of how well the deployed system is doing. However, such user interaction logs are primarily used for an one-off assessment of the system, e.g., for spotting critical errors, detecting domain shifts, or identifying the most successful use cases of the system in production. This assessment can then be used to support the decision of keeping or replacing this system in production.\n\nFrom a machine learning perspective, using interaction logs only for evaluation purposes is a lost opportunity for offline reinforcement learning (RL). Logs of user interactions are gold mines for off-policy learning, and they should be put to use, rather than being forgotten after a one-off evaluation purpose. To move towards the goal of using user interaction logs for learning, we will discuss which challenges have hindered RL from being employed in real-world interaction with users of NLP systems so far.\n\nConcretely, our focus is on sequence-tosequence learning for NLP applications (see \u00a7 2 for an overview). For example, many machine translation services provide the option for users to give feedback on the quality of the translation, e.g., by collecting post-edits. Similarly, industrial chatbots can easily collect vast amounts of interaction logs, which can be utilized with offline RL methods (Kandasamy et al., 2017;Zhou et al., 2017;Hancock et al., 2019). In the following, we will thus present challenges that are encountered in userinteractive RL for NLP systems. With this discussion, we aim to (1) encourage NLP practitioners to leverage their interaction logs through offline RL, and (2) inspire RL researchers to steel their algorithms for the challenging applications in NLP.\n\n\nOffline Feedback for Seq2Seq in NLP\n\nIn sequence-to-sequence (Seq2Seq) learning, the task is to map an input sequence x = x 1 , x 2 , . . . , x |x| , \u2200x i \u2208 X to an output sequence y = y 1 , y 2 , . . . , y |y| , \u2200y j \u2208 Y, where X , Y denote the sets of input and output vocabularies, respectively. The conditional distribution of the output sequence given the input can be modeled with a policy \u03c0 \u03b8 with learnable parameters \u03b8. Assuming a left-to-right generation order, the output sequence y is generated by conditioning on previous output elements y <j and the input sequence x:\n\u03c0 \u03b8 (y | x) = |y| j=1 \u03c0 \u03b8 (y j | y <j , x).(1)\nMapping the sequence-to-sequence problem formulation to NLP tasks, we have for example:\n\n\u2022 Machine translation: x is a source sentence and y the translation of x in a target language.\n\n\u2022 Semantic parsing: x is a sentence and y its semantic parse (e.g., in SQL). \u2022 Summarization: x is the document that is to be summarized and y a corresponding summary. \u2022 Dialogue generation: x is the conversation history and y an appropriate reply.\n\nThe most distinctive feature of Seq2Seq NLP tasks for RL are the extremely large, structured output spaces: given the output vocabulary of size |Y| and a maximum sequence length M , there are |Y| M possible combinations of output sequences. For instance, in machine translation there might be as many as 30 000 output tokens in the vocabulary and the output sequence length could easily be 100, leading to a total of 30 000 100 possible outputs.\n\nA successful policy identifies the few combination of tokens that form valid output sequences. In the most extreme case only one output sequence exists that will be correct. , e.g., in a semantic parsing setup, where potentially only one specific SQL query will return the correct answer when executed. To train a policy, supervised data can be used. There we assume a given dataset D sup = {(x t , y t )} T t=1 on which the parameters \u03b8 can be learnt with a maximum likelihood approach, aiming to maximize the model score for the given reference output.\n\nIn practice, it may be too expensive to collect correct, i.e., supervised, output sequences, since they require skilled annotators, e.g., trained translators for a machine translation task. Therefore, one option is to pre-train the policy on some available supervised data, which will allow the model to concentrate on reasonable areas in the output space (Choshen et al., 2020). The model can then be used to produce potentially imperfect output sequences and humans can judge an output\u1ef9 and a reward \u03b4 t \u2208 [0, 1] is assigned. Model parameters may be optimized by pairing the model outputs with their reward estimates. Depending on the use case, quality judgments may also exist for single elements in the structure, adding \u03b4 (t,j) for every step in the output sequence. The core idea is that the weighting by \u03b4 enables learning from imperfect outputs while respecting their faults. In RL, these quality assessments are used to reward desirable model actions, here desirable sequence outputs.\n\nWhen collecting quality judgments from human users in production systems, it would be risky to directly update the model online according to their feedback. 1 Some user feedback might be adversarial, inappropriate, or not representative when used for training without prior treatment (Rivas et al., 2018;Kreutzer et al., 2018a;Davis, 2016). 2 Furthermore, interpreting feedback wrongly (e.g., through incorrect credit assignment (Bahdanau et al., 2017)), or receiving misleading feedback (Nguyen et al., 2017;Kreutzer et al., 2018a), could easily push the policy into less favorable conditions.\n\nBecause updating systems online is too risky, quality judgments are instead stored in interaction logs, i.e., D log = {(x t ,\u1ef9 t , \u03b4 t )} T t=1 , and the system is updated offline. As a result, the imperfect output sequences are produced by a possibly different policy, the logging policy \u00b5, and updates to our learning policy are conducted offline, which is a classic off-policy RL scenario.\n\nDue to the logging setup, the collected dataset is biased towards the choices of the deployed model, the logging policy \u00b5. This results in a counterfactual learning scenario (Bottou et al., 2013). The bias may be corrected via importance sampling. If the logging policy is known and \u00b5(\u0177 | x) is logged as well, the policy can then be optimized for the Inverse Propensity Scoring (IPS) objective (Rosenbaum and Rubin, 1983):\nL IPS = \u2212 1 T T t=1 \u03b4 t \u03c0 \u03b8 (\u1ef9 t | x t ) \u00b5(\u1ef9 t | x t ) .(2)\n3 Challenges for Off-Policy RL in NLP On top of the difficulties encountered in offline RL, additionally constraints arise in production scenarios. We address this and possible solutions in \u00a73.1, while \u00a73.2 focuses on how to obtain reliable data from which machine learning can succeed.\n\n\nDeterministic Logging and Off-line Learning\n\nIn order to not show inferior outputs to users, production NLP systems show the most likely output, which disables the typically crucial exploration component of RL. This effectively results in deterministic logging policies that lack explicit exploration, which makes an application of standard off-policy methods for counterfactual learning questionable. For example, techniques such as inverse propensity scoring (Rosenbaum and Rubin, 1983) or weighted importance sampling (Precup et al., 2000;Jiang and Li, 2016;Thomas and Brunskill, 2016), rely on sufficient exploration of the output space by the logging system as a prerequisite for counterfactual learning. In fact, Langford et al. (2008) and Strehl et al. (2010) even give impossibility results for exploration-free counterfactual learning.\n\nOne option is to hope for implicit exploration due to input or context variability. This has been observed for the case of online advertising (Chapelle and Li, 2011) and investigated theoretically (Bastani et al., 2017). In NLP, output sequences may overlap in some of the words, so the learner could infer from rewards in which contexts specific words are more suitable than in others. This has been explored in the context of machine translation (Lawrence et al., 2017b), utilizing the Deterministic Propensity Matching (DPM) objective\nL DPM = \u2212 1 T T t=1 \u03b4 t \u03c0 \u03b8 (\u1ef9 t | x t ),(3)\nwhich closely follows the IPS objective, however, due to the deterministic logging \u2200\u1ef9, \u00b5(\u1ef9 | x) = 1. While this exploration is limited by the input data, solutions for safe exploration might be attractive to transfer to NLP applications to actively guide exploration while not sacrificing quality (Hans et al., 2008;Berkenkamp et al., 2017). Another option is to consider concrete cases of degenerate behavior in estimation from logged data. We look at two such issues and possible solutions. Both problems occur irrespective of whether data is logged deterministically or not, but the effects of the degenerative behavior might be amplified in the case of deterministic logging.\n\nThe first form of degenerate behaviour occurs for a collected log D log with \u03b4 \u2208 [0, 1] because IPS and DPM can trivially be minimized by setting all probabilities in the dataset D to 1 for any \u03b4 t > 0 (Lawrence et al., 2017a). Concretely, this means, while the worst output sequences with \u03b4 t = 0 are simply ignored, all other sequences are encouraged, even if their reward is close to 0. However, it is clearly undesirable to increase the probability of low reward examples (Swaminathan and Joachims, 2015;Lawrence et al., 2017b,a).\n\nThere are two possible solutions to this problem:\n\nThe first solution is to tune the learning rate and perform early stopping before the degenerate state can be reached. The second solution is to utilize a multiplicative control variate (Kong, 1992) for selfnormalization (Swaminathan and Joachims, 2015). For efficient gradient calculation, batches of size B can be reweighted one-step-late (OSL) (Lawrence and Riezler, 2018) using \u03b8 from some previous iteration:\nL OSL = \u2212 1 B B b=1 \u03b4 b \u03c0 \u03b8 (\u1ef9 b | x b ) 1 T T t=1 \u03c0 \u03b8 (\u1ef9 t | x t )\n.\n\n(4)\n\nSelf-normalization discourages increasing the probability of low reward data because this would take away probability mass from higher reward outputs and as a result. This introduces a bias in the estimator (that decreases as T increases), however, it makes learning under deterministic logging feasible, as has been shown for learning with real human feedback in a semantic parsing scenario (Lawrence and Riezler, 2018). This gives the RL agent an edge in learning in an environment that has been deemed impossible in the literature.\n\nA second form of degenerate behavior occurs because the reward \u03b4 t of an output sequence is typically measured with some non-negative value, e.g., \u03b4 t \u2208 [0, 1]. For example, for machine translation, Kreutzer et al. (2018b) collect ratings for translations on a 5-point Likert scale and map the values linearly to [0, 1]. However, utilizing any of the above objectives means that bad output sequences with low rewards cannot actively be discouraged.\n\nThere are two possible solutions, both of which have been used as additive control variates to reduce variance in gradient estimators. First, low reward sequences can be discouraged by employing a reward baseline, where for example the average reward \u2206 = 1 t t t =1 \u03b4 t is subtracted from each \u03b4 t . This will cause output sequences worse than the running average to be discouraged rather than encouraged. The second option is to use the logged data D log to learn a reward estimator\u03b4 that can return a reward estimate for any pair (x, y). This estimator together with the IPS objective leads to the Doubly Robust (DR) objective (Dudik et al., 2011),\nL DR = \u2212 1 T T t=1 (\u03b4 t \u2212\u03b4(x t ,\u1ef9 t )) \u03c0 \u03b8 (\u1ef9 t | x t )+ \u1ef9 \u223c\u03c0 \u03b8 (\u1ef9|xt)\u03b4 (x t ,\u1ef9 ) \u03c0 \u03b8 (\u1ef9 | x t ) .\nThis objective enables the exploration of other outputs\u1ef9 that are not part of the original log and encourages them based on the reward value returned by the estimator. For the task of machine translation, Lawrence et al. (2017b) show this objective to be the most successful in their setup, and Kreutzer et al. (2018a) report simulation results that show that this objective can significantly reduce the gap between offline and online policy learning, even if the reward estimator is not perfect. Zhou et al. (2017) present an alternating approach to integrating a reward estimator for exploration, by switching between learning offline from logged rewards and exploring online with the help of a reward estimator in phases.\n\n\nReliability and Learnability of Feedback\n\nIn interactive NLP, it is unrealistic to expect anything else than bandit feedback from a human user interacting with a chatbot, automatic summarization tool, or commercial machine translation system. That is, users of such systems will only provide a reward signal to the one output that is presented to them, and cannot be expected to rate a multitude of outputs for the same input. As a result, the feedback is very sparse in relation to the size of the output space.\n\nIdeally, the user experience should not be disrupted through feedback collection. Non-intrusive interface options for example allow for corrections of the output (\"post-edits\" in the context of machine translation) as a negative signal, or recording whether the output is copied and/or shared without changes, which may be interpreted as a positive signal. However, the signal might be noisy, since the notion of output quality for natural language generation tasks is not a well-defined function to start with: Each input might have many possible valid outputs, each of which humans may judge differently, depending on many contextual and personal factors. In machine translation evaluation for instance, inter-rater agreements have traditionally been reported as low (Turian et al., 2003;Carl et al., 2011;Lommel et al., 2014), especially when quality estimates are collected from non-professional raters (Callison-Burch, 2009). Similar observations have been made for other text generation tasks (Godwin and Piwek, 2016;Verberne et al., 2018). Nguyen et al. (2017) illustrated how badly machine translation systems can handle humanlevel noise in direct feedback for online RL with simulations. The level of noise in real-world human feedback may be so high that it prevents learning completely, as for example experienced in ecommerce machine translation logs (Kreutzer et al., 2018a). The issue is even higher in dialogue generation where there are a plenitude of acceptable responses (Pang et al., 2020). To this aim, inverse RL has been proposed to infer reward functions from responses indirectly (Takanobu et al., 2019).\n\nSurprisingly, the question of how to best improve an RL agent in the scenario of learning from real-world human feedback has been scarcely researched. This might originate from many RL research environments coming with fixed reward functions. In the real world, however, there is rarely a clearly defined single reward function for which it would suffice optimizing for. The suggestions in Dulac-Arnold et al. (2019) seem straightforward: warm-starting agents to decrease sample complexity or using inverse reinforcement learning to recover reward functions from demonstrations (Wang et al., 2020) -but they require additional supervision signals that RL was supposed to alleviate.\n\nWhen it comes to the question which type of human feedback is most beneficial for training an RL agent, one finds a lot of blanket statements, e.g., referring to the advantages of pairwise comparisons (Thurstone, 1927). For instance, learning from human pairwise preferences from humans has been advertised for summarization (Christiano et al., 2017;Stiennon et al., 2020) and language modeling (Ziegler et al., 2019), but the reliability of the signal has not been evaluated. An exception is the work of Kreutzer et al. (2018b) which is the first to investigate two crucial questions. The first question addresses which type of human feedback -pairwise judgments or cardinal feedback on a 5point scale -can be given most reliably by human teachers. The second question investigates which type of feedback allows to learn reward estimators that best approximate human rewards and can be best integrated into an end-to-end RL-NLP task.\n\nRegarding the first question, Kreutzer et al. (2018b) found that the common assumption -that pairwise comparisons are easier to judge than a single output on a Likert scale (Thurstone, 1927)turned out to be false for the task of machine translation. Inter-rater reliability proved to be higher for 5-point ratings (Krippendorff's \u03b1 = 0.51) than for pairwise judgments (\u03b1 = 0.39). (Kreutzer et al., 2018b) explain two advantages that the Likert scale setup offers: (1) it is possible to standardize cardinal judgments for each rater to remove individual biases, (2) they offer an absolute anchoring for quality, while a preference rankings leave the overall positioning of the pair of outputs on a quality scale open. For pairwise judgments it is difficult or even impossible to reliably choose between two outputs that are similarly good or bad, e.g., differing by only a few words. Therefore, filtering out raters with low intra-rater reliability proved effective for absolute ratings, while filtering outputs with a high variance in ratings was most effective for pairwise ratings, yielding the final inter-rater reliability given above. Discarding rated outputs, however, reduces the size of the log to learn from, which is undesirable in settings where rewards are scarce or costly.\n\nTo answer the second question, Kreutzer et al. (2018b) found a neural machine translation system can be significantly improved using a reward estimator trained on only a few hundred cardinal user judgments. This work highlights that future research in real-world RL might have to involve studies in user interfaces or user experience, since the interfaces for feedback collection influence the reward function that RL agents learn from -and thereby the downstream task success. Collecting implicit feedback (Kreutzer et al., 2018a;Jaques et al., 2020) might offer a better user experience.\n\nFor the challenges discussed in Sections 3.1 and 3.2, a promising approach is to tackle the arguably simpler problem of learning a reward estimator from human feedback first, then provide unlimited learned feedback to generalize to unseen outputs in off-policy RL. However, risks of bias introduction and potential benefits for noise reduction through replacing user feedback by reward estimators are yet to be quantified.\n\n\nConclusion\n\nThere is large potential in NLP to leverage user interaction logs for system improvement. We discussed how algorithms for offline RL can offer promising solutions for this learning problem. However, specific challenges in offline RL arise due to the particular nature of NLP systems that collect human feedback in real-world applications. We presented cases where such challenges have been found and offered solutions that have helped. So far, the solutions have mainly been explored in the context of machine translation and semantic parsing. In the future, it will be interesting to explore further tasks and additional real-world use cases to find out how to best learn from human feedback.\n The majority of RL research in NLP has focused on learning from online feedback(Sokolov et al., 2016;He et al., 2016;Bahdanau et al., 2017;Nguyen et al., 2017;Nogueira and Cho, 2017;Lam et al., 2018).2  The chatbot Tay might be one of the most illustrative examples for what can go wrong(Davis, 2016).\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, 5th International Conference on Learning Representations, ICLR. Toulon, FranceDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In 5th Inter- national Conference on Learning Representations, ICLR, Toulon, France.\n\nExploiting the natural exploration in contextual bandits. H Bastani, M Bayati, K Khosravi, 1704.09011H. Bastani, M. Bayati, and K. Khosravi. 2017. Ex- ploiting the natural exploration in contextual bandits. ArXiv e-prints, 1704.09011.\n\nSafe model-based reinforcement learning with stability guarantees. Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, Andreas Krause, Advances in Neural Information Processing Systems (NeurIPS). Long Beach, CaliforniaFelix Berkenkamp, Matteo Turchetta, Angela Schoel- lig, and Andreas Krause. 2017. Safe model-based reinforcement learning with stability guarantees. In Advances in Neural Information Processing Systems (NeurIPS), pages 908-918, Long Beach, California.\n\nCounterfactual Reasoning and Learning Systems: The Example of Computational Advertising. L\u00e9on Bottou, Jonas Peters, Joaquin Qui\u00f1onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipanakar Ray, Journal of Machine Learning Research. Patrice Simard, and Ed Snelson. 201314L\u00e9on Bottou, Jonas Peters, Joaquin Qui\u00f1onero- Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipanakar Ray, Patrice Simard, and Ed Snelson. 2013. Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising. Journal of Machine Learning Re- search, 14.\n\nFast, cheap, and creative: Evaluating translation quality using Amazon's Mechanical Turk. Chris Callison, - Burch, Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)SingaporeChris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon's Me- chanical Turk. In Proceedings of the 2009 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), Singapore.\n\nThe process of post-editing: a pilot study. Michael Carl, Barbara Dragsted, Jakob Elming, Daniel Hardt, Arnt Lykke Jakobsen, Copenhagen Studies in Language. 41Michael Carl, Barbara Dragsted, Jakob Elming, Daniel Hardt, and Arnt Lykke Jakobsen. 2011. The process of post-editing: a pilot study. Copenhagen Studies in Language, 41:131-142.\n\nAn empirical evaluation of Thompson sampling. Olivier Chapelle, Lihong Li, Advances in Neural Information Processing Systems (NeurIPS). Granada, SpainOlivier Chapelle and Lihong Li. 2011. An empirical evaluation of Thompson sampling. In Advances in Neural Information Processing Systems (NeurIPS), Granada, Spain.\n\nOn the weaknesses of reinforcement learning for neural machine translation. Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend, International Conference on Learning Representations (ICLR). VirtualLeshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Inter- national Conference on Learning Representations (ICLR), Virtual.\n\nDeep Reinforcement Learning from Human Preferences. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei, Advances in Neural Information Processing Systems (NeurIPS). Long Beach, CA, USAPaul F. Christiano, Jan Leike, Tom Brown, Miljan Mar- tic, Shane Legg, and Dario Amodei. 2017. Deep Re- inforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems (NeurIPS), Long Beach, CA, USA.\n\nAI amusements: the tragic tale of Tay the chatbot. Ernest Davis, AI Matters. 24Ernest Davis. 2016. AI amusements: the tragic tale of Tay the chatbot. AI Matters, 2(4):20-24.\n\nLearning dense models of query similarity from user click logs. Fabio De Bona, Stefan Riezler, Keith Hall, Massimiliano Ciaramita, Ama\u00e7 Herda\u01e7delen, Maria Holmqvist, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-ACL), Los Angeles. CaliforniaFabio De Bona, Stefan Riezler, Keith Hall, Massi- miliano Ciaramita, Ama\u00e7 Herda\u01e7delen, and Maria Holmqvist. 2010. Learning dense models of query similarity from user click logs. In Human Lan- guage Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-ACL), Los An- geles, California.\n\nDoubly robust policy evaluation and learning. Miroslav Dudik, John Langford, Lihong Li, Proceedings of the 28th International Conference on Machine Learning (ICML). the 28th International Conference on Machine Learning (ICML)Bellevue, WAMiroslav Dudik, John Langford, and Lihong Li. 2011. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on Machine Learning (ICML), Bellevue, WA.\n\nChallenges of real-world reinforcement learning. Gabriel Dulac-Arnold, Daniel J Mankowitz, Todd Hester, CoRR, abs/1904.12901Gabriel Dulac-Arnold, Daniel J. Mankowitz, and Todd Hester. 2019. Challenges of real-world reinforce- ment learning. CoRR, abs/1904.12901.\n\nCollecting reliable human judgements on machine-generated language: The case of the QG-STEC data. Keith Godwin, Paul Piwek, Proceedings of the 9th International Natural Language Generation conference (INLG). the 9th International Natural Language Generation conference (INLG)Edinburgh, UKKeith Godwin and Paul Piwek. 2016. Collecting reli- able human judgements on machine-generated lan- guage: The case of the QG-STEC data. In Proceed- ings of the 9th International Natural Language Gen- eration conference (INLG), Edinburgh, UK.\n\nLearning from dialogue after deployment: Feed yourself. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazar\u00e9, Jason Weston, Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazar\u00e9, and Jason Weston. 2019. Learning from dialogue after deployment: Feed yourself, chatbot!\n\nSafe exploration for reinforcement learning. Alexander Hans, Daniel Schneega\u00df, Anton Maximilian Sch\u00e4fer, Steffen Udluft, ESANN. Alexander Hans, Daniel Schneega\u00df, Anton Maximilian Sch\u00e4fer, and Steffen Udluft. 2008. Safe exploration for reinforcement learning. In ESANN, pages 143- 148.\n\nDeep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). the 54th Annual Meeting of the Association for Computational Linguistics (ACL)Berlin, GermanyJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li- hong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language ac- tion space. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (ACL), Berlin, Germany.\n\nWay offpolicy batch deep reinforcement learning of human preferences in dialog. Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, Rosalind Picard, Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2020. Way off- policy batch deep reinforcement learning of human preferences in dialog.\n\nDoubly robust offpolicy value evaluation for reinforcement learning. Nan Jiang, Lihong Li, Proceedings of the 33rd International Conference on Machine Learning (ICML). the 33rd International Conference on Machine Learning (ICML)New York, NYNan Jiang and Lihong Li. 2016. Doubly robust off- policy value evaluation for reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY.\n\nBatch policy gradient methods for improving neural conversation models. Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter, 5th International Conference on Learning Representations. ICLRKirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, and David Carter. 2017. Batch policy gradient methods for improving neural conversation models. In 5th International Confer- ence on Learning Representations (ICLR).\n\nA note on importance sampling using standardized weights. Augustine Kong, 348IllinoisDepartment of Statistics, University of ChicagoTechnical ReportAugustine Kong. 1992. A note on importance sam- pling using standardized weights. Technical Report 348, Department of Statistics, University of Chicago, Illinois.\n\nCan neural machine translation be improved with user feedback?. Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, Stefan Riezler, 10.18653/v1/N18-3012Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesACL3Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and Stefan Riezler. 2018a. Can neural machine translation be improved with user feedback? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 3 (ACL).\n\nReliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. Julia Kreutzer, Joshua Uyheng, Stefan Riezler, 10.18653/v1/P18-1165Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL). the 56th Annual Meeting of the Association for Computational Linguistics (ACL)Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. 2018b. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (ACL).\n\nA reinforcement learning approach to interactivepredictive neural machine translation. Julia Tsz Kin Lam, Stefan Kreutzer, Riezler, Proceedings of the 21st Annual Conference of the European Association for Machine Translation (EAMT). the 21st Annual Conference of the European Association for Machine Translation (EAMT)Alicante, SpainTsz Kin Lam, Julia Kreutzer, and Stefan Riezler. 2018. A reinforcement learning approach to interactive- predictive neural machine translation. In Proceed- ings of the 21st Annual Conference of the European Association for Machine Translation (EAMT), Ali- cante, Spain.\n\nExploration scavenging. John Langford, Alexander Strehl, Jennifer Wortman, https:/dl.acm.org/doi/pdf/10.1145/1390156.1390223Proceedings of the 25th International Conference on Machine Learning (ICML). the 25th International Conference on Machine Learning (ICML)Helsinki, FinlandJohn Langford, Alexander Strehl, and Jennifer Wort- man. 2008. Exploration scavenging. In Proceed- ings of the 25th International Conference on Ma- chine Learning (ICML), Helsinki, Finland.\n\nCounterfactual Learning for Machine Translation: Degeneracies and Solutions. Carolin Lawrence, Pratik Gajane, Stefan Riezler, Proceedings of the NIPS WhatIf Workshop. the NIPS WhatIf WorkshopLong Beach, California, USACarolin Lawrence, Pratik Gajane, and Stefan Riezler. 2017a. Counterfactual Learning for Machine Trans- lation: Degeneracies and Solutions. In Proceedings of the NIPS WhatIf Workshop, Long Beach, Califor- nia, USA.\n\nImproving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback. Carolin Lawrence, Stefan Riezler, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL). the 56th Annual Meeting of the Association for Computational Linguistics (ACL)Melbourne, AustraliaCarolin Lawrence and Stefan Riezler. 2018. Improving a Neural Semantic Parser by Counterfactual Learn- ing from Human Bandit Feedback. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), Melbourne, Aus- tralia.\n\nCounterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation. Carolin Lawrence, Artem Sokolov, Stefan Riezler, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkCarolin Lawrence, Artem Sokolov, and Stefan Riezler. 2017b. Counterfactual Learning from Bandit Feed- back under Deterministic Logging : A Case Study in Statistical Machine Translation. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), Copenhagen, Denmark.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)Austin, TexasAssociation for Computational LinguisticsJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016. Deep re- inforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), Austin, Texas. Association for Computational Lin- guistics.\n\nAssessing inter-annotator agreement for translation error annotation. Arle Lommel, Maja Popovic, Aljoscha Burchardt, MTE: Workshop on Automatic and Manual Metrics for Operational Translation Evaluation. Arle Lommel, Maja Popovic, and Aljoscha Burchardt. 2014. Assessing inter-annotator agreement for trans- lation error annotation. In MTE: Workshop on Auto- matic and Manual Metrics for Operational Transla- tion Evaluation.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. Khanh Nguyen, Hal Daum\u00e9, Iii , Jordan Boyd-Graber, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)Khanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd- Graber. 2017. Reinforcement learning for bandit neural machine translation with simulated human feedback. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\nTaskoriented query reformulation with reinforcement learning. Rodrigo Nogueira, Kyunghyun Cho, 10.18653/v1/D17-1061Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)Copenhagen, DenmarkRodrigo Nogueira and Kyunghyun Cho. 2017. Task- oriented query reformulation with reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), Copenhagen, Denmark.\n\nTowards holistic and automatic evaluation of open-domain dialogue generation. Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu, Kewei Tu, 10.18653/v1/2020.acl-main.333Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)OnlineBo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu, and Kewei Tu. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics (ACL), Online.\n\nEligibility traces for off-policy policy evaluation. Doina Precup, Richard S Sutton, Satinder P Singh, Proceedings of the Seventeenth International Conference on Machine Learning (ICML). the Seventeenth International Conference on Machine Learning (ICML)San Francisco, CADoina Precup, Richard S. Sutton, and Satinder P. Singh. 2000. Eligibility traces for off-policy policy eval- uation. In Proceedings of the Seventeenth Inter- national Conference on Machine Learning (ICML), San Francisco, CA.\n\nExcitement and concerns about machine learning-based chatbots and talkbots: A survey. Pablo Rivas, Kerstin Holzmayer, Cristian Hernandez, Charles Grippaldi, 2018 IEEE International Symposium on Technology and Society (ISTAS). IEEEPablo Rivas, Kerstin Holzmayer, Cristian Hernandez, and Charles Grippaldi. 2018. Excitement and con- cerns about machine learning-based chatbots and talkbots: A survey. In 2018 IEEE International Sym- posium on Technology and Society (ISTAS), pages 156-162. IEEE.\n\nThe central role of the propensity score in observational studies for causal effects. R Paul, Donald B Rosenbaum, Rubin, Biometrika. 170Paul R. Rosenbaum and Donald B. Rubin. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1).\n\nStochastic structured prediction under bandit feedback. Artem Sokolov, Julia Kreutzer, Stefan Riezler, Christopher Lo, Advances in Neural Information Processing Systems (NeurIPS). Barcelona, SpainArtem Sokolov, Julia Kreutzer, Stefan Riezler, and Christopher Lo. 2016. Stochastic structured pre- diction under bandit feedback. In Advances in Neural Information Processing Systems (NeurIPS), Barcelona, Spain.\n\nLearning to summarize from human feedback. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback.\n\nLearning from logged implicit exploration data. Alexander L Strehl, John Langford, Lihong Li, M Sham, Kakade, Advances in Neural Information Processing Sytems (NIPS). Vancouver, CanadaAlexander L. Strehl, John Langford, Lihong Li, and Sham M. Kakade. 2010. Learning from logged implicit exploration data. In Advances in Neural Information Processing Sytems (NIPS), Vancouver, Canada.\n\nThe self-normalized estimator for counterfactual learning. Adith Swaminathan, Thorsten Joachims, Advances in Neural Information Processing Systems (NIPS). Montreal, CanadaAdith Swaminathan and Thorsten Joachims. 2015. The self-normalized estimator for counterfactual learn- ing. In Advances in Neural Information Processing Systems (NIPS), Montreal, Canada.\n\nGuided dialog policy learning: Reward estimation for multi-domain task-oriented dialog. Ryuichi Takanobu, Hanlin Zhu, Minlie Huang, 10.18653/v1/D19-1010Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaRyuichi Takanobu, Hanlin Zhu, and Minlie Huang. 2019. Guided dialog policy learning: Reward es- timation for multi-domain task-oriented dialog. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), Hong Kong, China.\n\nDataefficient off-policy policy evaluation for reinforcement learning. Philip S Thomas, Emma Brunskill, Proceedings of the 33nd International Conference on Machine Learning (ICML). the 33nd International Conference on Machine Learning (ICML)New York, NYPhilip S. Thomas and Emma Brunskill. 2016. Data- efficient off-policy policy evaluation for reinforce- ment learning. In Proceedings of the 33nd Inter- national Conference on Machine Learning (ICML), New York, NY.\n\nA law of comparative judgement. Louis Leon Thurstone, Psychological Review. 34Louis Leon Thurstone. 1927. A law of comparative judgement. Psychological Review, 34:278-286.\n\nInteractive NLP in clinical care: Identifying incidental findings in radiology reports. Gaurav Trivedi, R Esmaeel, Dadashzadeh, M Robert, Wendy W Handzel, Shyam Chapman, Harry Visweswaran, Hochheiser, Applied clinical informatics. 104655Gaurav Trivedi, Esmaeel R Dadashzadeh, Robert M Handzel, Wendy W Chapman, Shyam Visweswaran, and Harry Hochheiser. 2019. Interactive NLP in clinical care: Identifying incidental findings in radiology reports. Applied clinical informatics, 10(4):655.\n\nEvaluation of machine translation and its evaluation. Luke Joseph P Turian, Shea, Melamed, Proceedings of MT Summit. MT SummitJoseph P Turian, Luke Shea, and I Dan Melamed. 2003. Evaluation of machine translation and its evaluation. Proceedings of MT Summit, pages 386-393.\n\nCreating a reference data set for the summarization of discussion forum threads. Suzan Verberne, Emiel Krahmer, Iris Hendrickx, Sander Wubben, Antal Van Den, Bosch, https:/link.springer.com/content/pdf/10.1007/s10579-017-9389-4.pdfLanguage Resources and Evaluation. 522Suzan Verberne, Emiel Krahmer, Iris Hendrickx, Sander Wubben, and Antal van den Bosch. 2018. Creating a reference data set for the summarization of discussion forum threads. Language Resources and Evaluation, 52(2):461-483.\n\nReinforcement learning with perturbed rewards. Jingkang Wang, Yang Liu, Bo Li, AAAI. New York, New YorkJingkang Wang, Yang Liu, and Bo Li. 2020. Rein- forcement learning with perturbed rewards. In AAAI, New York, New York.\n\nEnd-to-end offline goal-oriented dialog policy learning via policy gradient. Li Zhou, Kevin Small, O Rokhlenko, C Elkan, abs/1712.02838ArXiv. Li Zhou, Kevin Small, O. Rokhlenko, and C. Elkan. 2017. End-to-end offline goal-oriented dialog policy learning via policy gradient. ArXiv, abs/1712.02838.\n\nM Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. arXiv preprintDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris- tiano, and Geoffrey Irving. 2019. Fine-tuning lan- guage models from human preferences. arXiv preprint arXiv:1909.08593.\n", "annotations": {"author": "[{\"end\":179,\"start\":111},{\"end\":287,\"start\":180},{\"end\":375,\"start\":288}]", "publisher": null, "author_last_name": "[{\"end\":125,\"start\":117},{\"end\":194,\"start\":187},{\"end\":304,\"start\":296}]", "author_first_name": "[{\"end\":116,\"start\":111},{\"end\":186,\"start\":180},{\"end\":295,\"start\":288}]", "author_affiliation": "[{\"end\":178,\"start\":148},{\"end\":286,\"start\":225},{\"end\":374,\"start\":333}]", "title": "[{\"end\":92,\"start\":1},{\"end\":467,\"start\":376}]", "venue": "[{\"end\":533,\"start\":469}]", "abstract": "[{\"end\":1144,\"start\":602}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1438,\"start\":1414},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1485,\"start\":1463},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1534,\"start\":1512},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2869,\"start\":2845},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2887,\"start\":2869},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2908,\"start\":2887},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5684,\"start\":5662},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6605,\"start\":6585},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6628,\"start\":6605},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6640,\"start\":6628},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6753,\"start\":6730},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6810,\"start\":6789},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6833,\"start\":6810},{\"end\":7486,\"start\":7465},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7713,\"start\":7686},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8552,\"start\":8525},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8606,\"start\":8585},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8625,\"start\":8606},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8652,\"start\":8625},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8805,\"start\":8783},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8830,\"start\":8810},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9075,\"start\":9052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9129,\"start\":9107},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9382,\"start\":9358},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9809,\"start\":9790},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9833,\"start\":9809},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10400,\"start\":10376},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10682,\"start\":10650},{\"end\":10707,\"start\":10682},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10959,\"start\":10947},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11014,\"start\":10982},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11671,\"start\":11643},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12009,\"start\":11986},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12886,\"start\":12866},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13215,\"start\":13192},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13305,\"start\":13282},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13502,\"start\":13484},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15018,\"start\":14997},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15036,\"start\":15018},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15056,\"start\":15036},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15156,\"start\":15135},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15251,\"start\":15227},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15273,\"start\":15251},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15295,\"start\":15275},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15615,\"start\":15591},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15736,\"start\":15717},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15855,\"start\":15832},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16454,\"start\":16436},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16759,\"start\":16742},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16891,\"start\":16866},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16913,\"start\":16891},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16958,\"start\":16936},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17069,\"start\":17046},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17530,\"start\":17507},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17881,\"start\":17857},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18819,\"start\":18796},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19296,\"start\":19272},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19316,\"start\":19296},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20589,\"start\":20567},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20605,\"start\":20589},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20627,\"start\":20605},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20647,\"start\":20627},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20670,\"start\":20647},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20687,\"start\":20670},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20788,\"start\":20775}]", "figure": null, "paragraph": "[{\"end\":1934,\"start\":1160},{\"end\":2448,\"start\":1936},{\"end\":3236,\"start\":2450},{\"end\":3820,\"start\":3276},{\"end\":3955,\"start\":3868},{\"end\":4051,\"start\":3957},{\"end\":4301,\"start\":4053},{\"end\":4748,\"start\":4303},{\"end\":5304,\"start\":4750},{\"end\":6299,\"start\":5306},{\"end\":6895,\"start\":6301},{\"end\":7289,\"start\":6897},{\"end\":7714,\"start\":7291},{\"end\":8061,\"start\":7775},{\"end\":8908,\"start\":8109},{\"end\":9447,\"start\":8910},{\"end\":10172,\"start\":9493},{\"end\":10708,\"start\":10174},{\"end\":10759,\"start\":10710},{\"end\":11174,\"start\":10761},{\"end\":11244,\"start\":11243},{\"end\":11249,\"start\":11246},{\"end\":11785,\"start\":11251},{\"end\":12235,\"start\":11787},{\"end\":12887,\"start\":12237},{\"end\":13711,\"start\":12987},{\"end\":14226,\"start\":13756},{\"end\":15856,\"start\":14228},{\"end\":16539,\"start\":15858},{\"end\":17475,\"start\":16541},{\"end\":18763,\"start\":17477},{\"end\":19354,\"start\":18765},{\"end\":19778,\"start\":19356},{\"end\":20486,\"start\":19793}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3867,\"start\":3821},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7774,\"start\":7715},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9492,\"start\":9448},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11242,\"start\":11175},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12986,\"start\":12888}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1158,\"start\":1146},{\"attributes\":{\"n\":\"2\"},\"end\":3274,\"start\":3239},{\"attributes\":{\"n\":\"3.1\"},\"end\":8107,\"start\":8064},{\"attributes\":{\"n\":\"3.2\"},\"end\":13754,\"start\":13714},{\"attributes\":{\"n\":\"4\"},\"end\":19791,\"start\":19781}]", "table": null, "figure_caption": null, "figure_ref": null, "bib_author_first_name": "[{\"end\":20849,\"start\":20842},{\"end\":20868,\"start\":20860},{\"end\":20883,\"start\":20877},{\"end\":20895,\"start\":20888},{\"end\":20907,\"start\":20903},{\"end\":20920,\"start\":20914},{\"end\":20934,\"start\":20929},{\"end\":20952,\"start\":20946},{\"end\":21364,\"start\":21363},{\"end\":21375,\"start\":21374},{\"end\":21385,\"start\":21384},{\"end\":21613,\"start\":21608},{\"end\":21632,\"start\":21626},{\"end\":21650,\"start\":21644},{\"end\":21669,\"start\":21662},{\"end\":22107,\"start\":22103},{\"end\":22121,\"start\":22116},{\"end\":22137,\"start\":22130},{\"end\":22162,\"start\":22157},{\"end\":22164,\"start\":22163},{\"end\":22175,\"start\":22174},{\"end\":22179,\"start\":22176},{\"end\":22196,\"start\":22192},{\"end\":22217,\"start\":22208},{\"end\":22689,\"start\":22684},{\"end\":22701,\"start\":22700},{\"end\":23177,\"start\":23170},{\"end\":23191,\"start\":23184},{\"end\":23207,\"start\":23202},{\"end\":23222,\"start\":23216},{\"end\":23240,\"start\":23230},{\"end\":23518,\"start\":23511},{\"end\":23535,\"start\":23529},{\"end\":23862,\"start\":23856},{\"end\":23876,\"start\":23872},{\"end\":23887,\"start\":23882},{\"end\":23902,\"start\":23898},{\"end\":24250,\"start\":24246},{\"end\":24252,\"start\":24251},{\"end\":24268,\"start\":24265},{\"end\":24279,\"start\":24276},{\"end\":24293,\"start\":24287},{\"end\":24307,\"start\":24302},{\"end\":24319,\"start\":24314},{\"end\":24700,\"start\":24694},{\"end\":24887,\"start\":24882},{\"end\":24903,\"start\":24897},{\"end\":24918,\"start\":24913},{\"end\":24937,\"start\":24925},{\"end\":24953,\"start\":24949},{\"end\":24972,\"start\":24967},{\"end\":25564,\"start\":25556},{\"end\":25576,\"start\":25572},{\"end\":25593,\"start\":25587},{\"end\":25996,\"start\":25989},{\"end\":26017,\"start\":26011},{\"end\":26019,\"start\":26018},{\"end\":26035,\"start\":26031},{\"end\":26307,\"start\":26302},{\"end\":26320,\"start\":26316},{\"end\":26798,\"start\":26792},{\"end\":26815,\"start\":26808},{\"end\":26839,\"start\":26824},{\"end\":26853,\"start\":26848},{\"end\":27062,\"start\":27053},{\"end\":27075,\"start\":27069},{\"end\":27092,\"start\":27087},{\"end\":27103,\"start\":27093},{\"end\":27120,\"start\":27113},{\"end\":27362,\"start\":27360},{\"end\":27374,\"start\":27367},{\"end\":27389,\"start\":27381},{\"end\":27402,\"start\":27394},{\"end\":27414,\"start\":27408},{\"end\":27421,\"start\":27419},{\"end\":27432,\"start\":27428},{\"end\":28021,\"start\":28014},{\"end\":28034,\"start\":28030},{\"end\":28054,\"start\":28050},{\"end\":28061,\"start\":28055},{\"end\":28073,\"start\":28068},{\"end\":28089,\"start\":28084},{\"end\":28105,\"start\":28101},{\"end\":28121,\"start\":28113},{\"end\":28134,\"start\":28126},{\"end\":28437,\"start\":28434},{\"end\":28451,\"start\":28445},{\"end\":28885,\"start\":28874},{\"end\":28902,\"start\":28897},{\"end\":28918,\"start\":28913},{\"end\":28934,\"start\":28928},{\"end\":28948,\"start\":28943},{\"end\":29322,\"start\":29313},{\"end\":29636,\"start\":29631},{\"end\":29654,\"start\":29647},{\"end\":29670,\"start\":29664},{\"end\":29686,\"start\":29680},{\"end\":30404,\"start\":30399},{\"end\":30421,\"start\":30415},{\"end\":30436,\"start\":30430},{\"end\":30993,\"start\":30988},{\"end\":31013,\"start\":31007},{\"end\":31534,\"start\":31530},{\"end\":31554,\"start\":31545},{\"end\":31571,\"start\":31563},{\"end\":32059,\"start\":32052},{\"end\":32076,\"start\":32070},{\"end\":32091,\"start\":32085},{\"end\":32505,\"start\":32498},{\"end\":32522,\"start\":32516},{\"end\":33114,\"start\":33107},{\"end\":33130,\"start\":33125},{\"end\":33146,\"start\":33140},{\"end\":33701,\"start\":33696},{\"end\":33710,\"start\":33706},{\"end\":33723,\"start\":33719},{\"end\":33735,\"start\":33732},{\"end\":33752,\"start\":33746},{\"end\":33769,\"start\":33761},{\"end\":34382,\"start\":34378},{\"end\":34395,\"start\":34391},{\"end\":34413,\"start\":34405},{\"end\":34831,\"start\":34826},{\"end\":34843,\"start\":34840},{\"end\":34854,\"start\":34851},{\"end\":34863,\"start\":34857},{\"end\":35373,\"start\":35366},{\"end\":35393,\"start\":35384},{\"end\":35920,\"start\":35918},{\"end\":35931,\"start\":35927},{\"end\":35948,\"start\":35941},{\"end\":35959,\"start\":35954},{\"end\":35972,\"start\":35966},{\"end\":35983,\"start\":35978},{\"end\":36521,\"start\":36516},{\"end\":36537,\"start\":36530},{\"end\":36539,\"start\":36538},{\"end\":36556,\"start\":36548},{\"end\":36558,\"start\":36557},{\"end\":37051,\"start\":37046},{\"end\":37066,\"start\":37059},{\"end\":37086,\"start\":37078},{\"end\":37105,\"start\":37098},{\"end\":37542,\"start\":37541},{\"end\":37555,\"start\":37549},{\"end\":37557,\"start\":37556},{\"end\":37803,\"start\":37798},{\"end\":37818,\"start\":37813},{\"end\":37835,\"start\":37829},{\"end\":37856,\"start\":37845},{\"end\":38200,\"start\":38195},{\"end\":38215,\"start\":38211},{\"end\":38228,\"start\":38224},{\"end\":38239,\"start\":38233},{\"end\":38241,\"start\":38240},{\"end\":38255,\"start\":38251},{\"end\":38269,\"start\":38262},{\"end\":38280,\"start\":38276},{\"end\":38295,\"start\":38290},{\"end\":38308,\"start\":38304},{\"end\":38559,\"start\":38550},{\"end\":38561,\"start\":38560},{\"end\":38574,\"start\":38570},{\"end\":38591,\"start\":38585},{\"end\":38597,\"start\":38596},{\"end\":38951,\"start\":38946},{\"end\":38973,\"start\":38965},{\"end\":39341,\"start\":39334},{\"end\":39358,\"start\":39352},{\"end\":39370,\"start\":39364},{\"end\":40143,\"start\":40137},{\"end\":40145,\"start\":40144},{\"end\":40158,\"start\":40154},{\"end\":40801,\"start\":40795},{\"end\":40812,\"start\":40811},{\"end\":40836,\"start\":40835},{\"end\":40850,\"start\":40845},{\"end\":40852,\"start\":40851},{\"end\":40867,\"start\":40862},{\"end\":40882,\"start\":40877},{\"end\":41253,\"start\":41249},{\"end\":41556,\"start\":41551},{\"end\":41572,\"start\":41567},{\"end\":41586,\"start\":41582},{\"end\":41604,\"start\":41598},{\"end\":41618,\"start\":41613},{\"end\":42019,\"start\":42011},{\"end\":42030,\"start\":42026},{\"end\":42038,\"start\":42036},{\"end\":42267,\"start\":42265},{\"end\":42279,\"start\":42274},{\"end\":42288,\"start\":42287},{\"end\":42301,\"start\":42300},{\"end\":42488,\"start\":42487},{\"end\":42502,\"start\":42497},{\"end\":42519,\"start\":42512},{\"end\":42533,\"start\":42530},{\"end\":42535,\"start\":42534},{\"end\":42544,\"start\":42540},{\"end\":42557,\"start\":42552},{\"end\":42571,\"start\":42567},{\"end\":42588,\"start\":42580}]", "bib_author_last_name": "[{\"end\":20858,\"start\":20850},{\"end\":20875,\"start\":20869},{\"end\":20886,\"start\":20884},{\"end\":20901,\"start\":20896},{\"end\":20912,\"start\":20908},{\"end\":20927,\"start\":20921},{\"end\":20944,\"start\":20935},{\"end\":20959,\"start\":20953},{\"end\":21372,\"start\":21365},{\"end\":21382,\"start\":21376},{\"end\":21394,\"start\":21386},{\"end\":21624,\"start\":21614},{\"end\":21642,\"start\":21633},{\"end\":21660,\"start\":21651},{\"end\":21676,\"start\":21670},{\"end\":22114,\"start\":22108},{\"end\":22128,\"start\":22122},{\"end\":22155,\"start\":22138},{\"end\":22172,\"start\":22165},{\"end\":22190,\"start\":22180},{\"end\":22206,\"start\":22197},{\"end\":22221,\"start\":22218},{\"end\":22698,\"start\":22690},{\"end\":22707,\"start\":22702},{\"end\":23182,\"start\":23178},{\"end\":23200,\"start\":23192},{\"end\":23214,\"start\":23208},{\"end\":23228,\"start\":23223},{\"end\":23249,\"start\":23241},{\"end\":23527,\"start\":23519},{\"end\":23538,\"start\":23536},{\"end\":23870,\"start\":23863},{\"end\":23880,\"start\":23877},{\"end\":23896,\"start\":23888},{\"end\":23908,\"start\":23903},{\"end\":24263,\"start\":24253},{\"end\":24274,\"start\":24269},{\"end\":24285,\"start\":24280},{\"end\":24300,\"start\":24294},{\"end\":24312,\"start\":24308},{\"end\":24326,\"start\":24320},{\"end\":24706,\"start\":24701},{\"end\":24895,\"start\":24888},{\"end\":24911,\"start\":24904},{\"end\":24923,\"start\":24919},{\"end\":24947,\"start\":24938},{\"end\":24965,\"start\":24954},{\"end\":24982,\"start\":24973},{\"end\":25570,\"start\":25565},{\"end\":25585,\"start\":25577},{\"end\":25596,\"start\":25594},{\"end\":26009,\"start\":25997},{\"end\":26029,\"start\":26020},{\"end\":26042,\"start\":26036},{\"end\":26314,\"start\":26308},{\"end\":26326,\"start\":26321},{\"end\":26806,\"start\":26799},{\"end\":26822,\"start\":26816},{\"end\":26846,\"start\":26840},{\"end\":26860,\"start\":26854},{\"end\":27067,\"start\":27063},{\"end\":27085,\"start\":27076},{\"end\":27111,\"start\":27104},{\"end\":27127,\"start\":27121},{\"end\":27365,\"start\":27363},{\"end\":27379,\"start\":27375},{\"end\":27392,\"start\":27390},{\"end\":27406,\"start\":27403},{\"end\":27417,\"start\":27415},{\"end\":27426,\"start\":27422},{\"end\":27442,\"start\":27433},{\"end\":28028,\"start\":28022},{\"end\":28048,\"start\":28035},{\"end\":28066,\"start\":28062},{\"end\":28082,\"start\":28074},{\"end\":28099,\"start\":28090},{\"end\":28111,\"start\":28106},{\"end\":28124,\"start\":28122},{\"end\":28141,\"start\":28135},{\"end\":28443,\"start\":28438},{\"end\":28454,\"start\":28452},{\"end\":28895,\"start\":28886},{\"end\":28911,\"start\":28903},{\"end\":28926,\"start\":28919},{\"end\":28941,\"start\":28935},{\"end\":28955,\"start\":28949},{\"end\":29327,\"start\":29323},{\"end\":29645,\"start\":29637},{\"end\":29662,\"start\":29655},{\"end\":29678,\"start\":29671},{\"end\":29694,\"start\":29687},{\"end\":30413,\"start\":30405},{\"end\":30428,\"start\":30422},{\"end\":30444,\"start\":30437},{\"end\":31005,\"start\":30994},{\"end\":31022,\"start\":31014},{\"end\":31031,\"start\":31024},{\"end\":31543,\"start\":31535},{\"end\":31561,\"start\":31555},{\"end\":31579,\"start\":31572},{\"end\":32068,\"start\":32060},{\"end\":32083,\"start\":32077},{\"end\":32099,\"start\":32092},{\"end\":32514,\"start\":32506},{\"end\":32530,\"start\":32523},{\"end\":33123,\"start\":33115},{\"end\":33138,\"start\":33131},{\"end\":33154,\"start\":33147},{\"end\":33704,\"start\":33702},{\"end\":33717,\"start\":33711},{\"end\":33730,\"start\":33724},{\"end\":33744,\"start\":33736},{\"end\":33759,\"start\":33753},{\"end\":33773,\"start\":33770},{\"end\":34389,\"start\":34383},{\"end\":34403,\"start\":34396},{\"end\":34423,\"start\":34414},{\"end\":34838,\"start\":34832},{\"end\":34849,\"start\":34844},{\"end\":34875,\"start\":34864},{\"end\":35382,\"start\":35374},{\"end\":35397,\"start\":35394},{\"end\":35925,\"start\":35921},{\"end\":35939,\"start\":35932},{\"end\":35952,\"start\":35949},{\"end\":35964,\"start\":35960},{\"end\":35976,\"start\":35973},{\"end\":35986,\"start\":35984},{\"end\":36528,\"start\":36522},{\"end\":36546,\"start\":36540},{\"end\":36564,\"start\":36559},{\"end\":37057,\"start\":37052},{\"end\":37076,\"start\":37067},{\"end\":37096,\"start\":37087},{\"end\":37115,\"start\":37106},{\"end\":37547,\"start\":37543},{\"end\":37567,\"start\":37558},{\"end\":37574,\"start\":37569},{\"end\":37811,\"start\":37804},{\"end\":37827,\"start\":37819},{\"end\":37843,\"start\":37836},{\"end\":37859,\"start\":37857},{\"end\":38209,\"start\":38201},{\"end\":38222,\"start\":38216},{\"end\":38231,\"start\":38229},{\"end\":38249,\"start\":38242},{\"end\":38260,\"start\":38256},{\"end\":38274,\"start\":38270},{\"end\":38288,\"start\":38281},{\"end\":38302,\"start\":38296},{\"end\":38319,\"start\":38309},{\"end\":38568,\"start\":38562},{\"end\":38583,\"start\":38575},{\"end\":38594,\"start\":38592},{\"end\":38602,\"start\":38598},{\"end\":38610,\"start\":38604},{\"end\":38963,\"start\":38952},{\"end\":38982,\"start\":38974},{\"end\":39350,\"start\":39342},{\"end\":39362,\"start\":39359},{\"end\":39376,\"start\":39371},{\"end\":40152,\"start\":40146},{\"end\":40168,\"start\":40159},{\"end\":40586,\"start\":40566},{\"end\":40809,\"start\":40802},{\"end\":40820,\"start\":40813},{\"end\":40833,\"start\":40822},{\"end\":40843,\"start\":40837},{\"end\":40860,\"start\":40853},{\"end\":40875,\"start\":40868},{\"end\":40894,\"start\":40883},{\"end\":40906,\"start\":40896},{\"end\":41269,\"start\":41254},{\"end\":41275,\"start\":41271},{\"end\":41284,\"start\":41277},{\"end\":41565,\"start\":41557},{\"end\":41580,\"start\":41573},{\"end\":41596,\"start\":41587},{\"end\":41611,\"start\":41605},{\"end\":41626,\"start\":41619},{\"end\":41633,\"start\":41628},{\"end\":42024,\"start\":42020},{\"end\":42034,\"start\":42031},{\"end\":42041,\"start\":42039},{\"end\":42272,\"start\":42268},{\"end\":42285,\"start\":42280},{\"end\":42298,\"start\":42289},{\"end\":42307,\"start\":42302},{\"end\":42495,\"start\":42489},{\"end\":42510,\"start\":42503},{\"end\":42528,\"start\":42520},{\"end\":42538,\"start\":42536},{\"end\":42550,\"start\":42545},{\"end\":42565,\"start\":42558},{\"end\":42578,\"start\":42572},{\"end\":42599,\"start\":42589},{\"end\":42607,\"start\":42601}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14096841},\"end\":21303,\"start\":20791},{\"attributes\":{\"doi\":\"1704.09011\",\"id\":\"b1\"},\"end\":21539,\"start\":21305},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":636855},\"end\":22012,\"start\":21541},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1726300},\"end\":22592,\"start\":22014},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16019656},\"end\":23124,\"start\":22594},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9275478},\"end\":23463,\"start\":23126},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6002655},\"end\":23778,\"start\":23465},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195791459},\"end\":24192,\"start\":23780},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4787508},\"end\":24641,\"start\":24194},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":34352813},\"end\":24816,\"start\":24643},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7556893},\"end\":25508,\"start\":24818},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7806620},\"end\":25938,\"start\":25510},{\"attributes\":{\"id\":\"b12\"},\"end\":26202,\"start\":25940},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":166290},\"end\":26734,\"start\":26204},{\"attributes\":{\"id\":\"b14\"},\"end\":27006,\"start\":26736},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13321704},\"end\":27292,\"start\":27008},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1153\",\"id\":\"b16\",\"matched_paper_id\":15986631},\"end\":27932,\"start\":27294},{\"attributes\":{\"id\":\"b17\"},\"end\":28363,\"start\":27934},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5806691},\"end\":28800,\"start\":28365},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1741724},\"end\":29253,\"start\":28802},{\"attributes\":{\"doi\":\"348\",\"id\":\"b20\"},\"end\":29565,\"start\":29255},{\"attributes\":{\"doi\":\"10.18653/v1/N18-3012\",\"id\":\"b21\",\"matched_paper_id\":4950709},\"end\":30294,\"start\":29567},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1165\",\"id\":\"b22\",\"matched_paper_id\":44062452},\"end\":30899,\"start\":30296},{\"attributes\":{\"id\":\"b23\"},\"end\":31504,\"start\":30901},{\"attributes\":{\"doi\":\"https:/dl.acm.org/doi/pdf/10.1145/1390156.1390223\",\"id\":\"b24\",\"matched_paper_id\":610636},\"end\":31973,\"start\":31506},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6934800},\"end\":32406,\"start\":31975},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13679932},\"end\":32981,\"start\":32408},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7181359},\"end\":33641,\"start\":32983},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3147007},\"end\":34306,\"start\":33643},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7855786},\"end\":34732,\"start\":34308},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":215824512},\"end\":35302,\"start\":34734},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1061\",\"id\":\"b31\",\"matched_paper_id\":125545},\"end\":35838,\"start\":35304},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.333\",\"id\":\"b32\",\"matched_paper_id\":212800438},\"end\":36461,\"start\":35840},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1153355},\"end\":36958,\"start\":36463},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":61810419},\"end\":37453,\"start\":36960},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":49190930},\"end\":37740,\"start\":37455},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":57680},\"end\":38150,\"start\":37742},{\"attributes\":{\"id\":\"b37\"},\"end\":38500,\"start\":38152},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6030272},\"end\":38885,\"start\":38502},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6359643},\"end\":39244,\"start\":38887},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1010\",\"id\":\"b40\",\"matched_paper_id\":201651596},\"end\":40064,\"start\":39246},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9311215},\"end\":40532,\"start\":40066},{\"attributes\":{\"id\":\"b42\"},\"end\":40705,\"start\":40534},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":201836302},\"end\":41193,\"start\":40707},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":9469794},\"end\":41468,\"start\":41195},{\"attributes\":{\"doi\":\"https:/link.springer.com/content/pdf/10.1007/s10579-017-9389-4.pdf\",\"id\":\"b45\",\"matched_paper_id\":21706253},\"end\":41962,\"start\":41470},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":52911385},\"end\":42186,\"start\":41964},{\"attributes\":{\"doi\":\"abs/1712.02838\",\"id\":\"b47\",\"matched_paper_id\":9575426},\"end\":42485,\"start\":42188},{\"attributes\":{\"doi\":\"arXiv:1909.08593\",\"id\":\"b48\"},\"end\":42912,\"start\":42487}]", "bib_title": "[{\"end\":20840,\"start\":20791},{\"end\":21606,\"start\":21541},{\"end\":22101,\"start\":22014},{\"end\":22682,\"start\":22594},{\"end\":23168,\"start\":23126},{\"end\":23509,\"start\":23465},{\"end\":23854,\"start\":23780},{\"end\":24244,\"start\":24194},{\"end\":24692,\"start\":24643},{\"end\":24880,\"start\":24818},{\"end\":25554,\"start\":25510},{\"end\":26300,\"start\":26204},{\"end\":27051,\"start\":27008},{\"end\":27358,\"start\":27294},{\"end\":28432,\"start\":28365},{\"end\":28872,\"start\":28802},{\"end\":29629,\"start\":29567},{\"end\":30397,\"start\":30296},{\"end\":30986,\"start\":30901},{\"end\":31528,\"start\":31506},{\"end\":32050,\"start\":31975},{\"end\":32496,\"start\":32408},{\"end\":33105,\"start\":32983},{\"end\":33694,\"start\":33643},{\"end\":34376,\"start\":34308},{\"end\":34824,\"start\":34734},{\"end\":35364,\"start\":35304},{\"end\":35916,\"start\":35840},{\"end\":36514,\"start\":36463},{\"end\":37044,\"start\":36960},{\"end\":37539,\"start\":37455},{\"end\":37796,\"start\":37742},{\"end\":38548,\"start\":38502},{\"end\":38944,\"start\":38887},{\"end\":39332,\"start\":39246},{\"end\":40135,\"start\":40066},{\"end\":40564,\"start\":40534},{\"end\":40793,\"start\":40707},{\"end\":41247,\"start\":41195},{\"end\":41549,\"start\":41470},{\"end\":42009,\"start\":41964},{\"end\":42263,\"start\":42188}]", "bib_author": "[{\"end\":20860,\"start\":20842},{\"end\":20877,\"start\":20860},{\"end\":20888,\"start\":20877},{\"end\":20903,\"start\":20888},{\"end\":20914,\"start\":20903},{\"end\":20929,\"start\":20914},{\"end\":20946,\"start\":20929},{\"end\":20961,\"start\":20946},{\"end\":21374,\"start\":21363},{\"end\":21384,\"start\":21374},{\"end\":21396,\"start\":21384},{\"end\":21626,\"start\":21608},{\"end\":21644,\"start\":21626},{\"end\":21662,\"start\":21644},{\"end\":21678,\"start\":21662},{\"end\":22116,\"start\":22103},{\"end\":22130,\"start\":22116},{\"end\":22157,\"start\":22130},{\"end\":22174,\"start\":22157},{\"end\":22192,\"start\":22174},{\"end\":22208,\"start\":22192},{\"end\":22223,\"start\":22208},{\"end\":22700,\"start\":22684},{\"end\":22709,\"start\":22700},{\"end\":23184,\"start\":23170},{\"end\":23202,\"start\":23184},{\"end\":23216,\"start\":23202},{\"end\":23230,\"start\":23216},{\"end\":23251,\"start\":23230},{\"end\":23529,\"start\":23511},{\"end\":23540,\"start\":23529},{\"end\":23872,\"start\":23856},{\"end\":23882,\"start\":23872},{\"end\":23898,\"start\":23882},{\"end\":23910,\"start\":23898},{\"end\":24265,\"start\":24246},{\"end\":24276,\"start\":24265},{\"end\":24287,\"start\":24276},{\"end\":24302,\"start\":24287},{\"end\":24314,\"start\":24302},{\"end\":24328,\"start\":24314},{\"end\":24708,\"start\":24694},{\"end\":24897,\"start\":24882},{\"end\":24913,\"start\":24897},{\"end\":24925,\"start\":24913},{\"end\":24949,\"start\":24925},{\"end\":24967,\"start\":24949},{\"end\":24984,\"start\":24967},{\"end\":25572,\"start\":25556},{\"end\":25587,\"start\":25572},{\"end\":25598,\"start\":25587},{\"end\":26011,\"start\":25989},{\"end\":26031,\"start\":26011},{\"end\":26044,\"start\":26031},{\"end\":26316,\"start\":26302},{\"end\":26328,\"start\":26316},{\"end\":26808,\"start\":26792},{\"end\":26824,\"start\":26808},{\"end\":26848,\"start\":26824},{\"end\":26862,\"start\":26848},{\"end\":27069,\"start\":27053},{\"end\":27087,\"start\":27069},{\"end\":27113,\"start\":27087},{\"end\":27129,\"start\":27113},{\"end\":27367,\"start\":27360},{\"end\":27381,\"start\":27367},{\"end\":27394,\"start\":27381},{\"end\":27408,\"start\":27394},{\"end\":27419,\"start\":27408},{\"end\":27428,\"start\":27419},{\"end\":27444,\"start\":27428},{\"end\":28030,\"start\":28014},{\"end\":28050,\"start\":28030},{\"end\":28068,\"start\":28050},{\"end\":28084,\"start\":28068},{\"end\":28101,\"start\":28084},{\"end\":28113,\"start\":28101},{\"end\":28126,\"start\":28113},{\"end\":28143,\"start\":28126},{\"end\":28445,\"start\":28434},{\"end\":28456,\"start\":28445},{\"end\":28897,\"start\":28874},{\"end\":28913,\"start\":28897},{\"end\":28928,\"start\":28913},{\"end\":28943,\"start\":28928},{\"end\":28957,\"start\":28943},{\"end\":29329,\"start\":29313},{\"end\":29647,\"start\":29631},{\"end\":29664,\"start\":29647},{\"end\":29680,\"start\":29664},{\"end\":29696,\"start\":29680},{\"end\":30415,\"start\":30399},{\"end\":30430,\"start\":30415},{\"end\":30446,\"start\":30430},{\"end\":31007,\"start\":30988},{\"end\":31024,\"start\":31007},{\"end\":31033,\"start\":31024},{\"end\":31545,\"start\":31530},{\"end\":31563,\"start\":31545},{\"end\":31581,\"start\":31563},{\"end\":32070,\"start\":32052},{\"end\":32085,\"start\":32070},{\"end\":32101,\"start\":32085},{\"end\":32516,\"start\":32498},{\"end\":32532,\"start\":32516},{\"end\":33125,\"start\":33107},{\"end\":33140,\"start\":33125},{\"end\":33156,\"start\":33140},{\"end\":33706,\"start\":33696},{\"end\":33719,\"start\":33706},{\"end\":33732,\"start\":33719},{\"end\":33746,\"start\":33732},{\"end\":33761,\"start\":33746},{\"end\":33775,\"start\":33761},{\"end\":34391,\"start\":34378},{\"end\":34405,\"start\":34391},{\"end\":34425,\"start\":34405},{\"end\":34840,\"start\":34826},{\"end\":34851,\"start\":34840},{\"end\":34857,\"start\":34851},{\"end\":34877,\"start\":34857},{\"end\":35384,\"start\":35366},{\"end\":35399,\"start\":35384},{\"end\":35927,\"start\":35918},{\"end\":35941,\"start\":35927},{\"end\":35954,\"start\":35941},{\"end\":35966,\"start\":35954},{\"end\":35978,\"start\":35966},{\"end\":35988,\"start\":35978},{\"end\":36530,\"start\":36516},{\"end\":36548,\"start\":36530},{\"end\":36566,\"start\":36548},{\"end\":37059,\"start\":37046},{\"end\":37078,\"start\":37059},{\"end\":37098,\"start\":37078},{\"end\":37117,\"start\":37098},{\"end\":37549,\"start\":37541},{\"end\":37569,\"start\":37549},{\"end\":37576,\"start\":37569},{\"end\":37813,\"start\":37798},{\"end\":37829,\"start\":37813},{\"end\":37845,\"start\":37829},{\"end\":37861,\"start\":37845},{\"end\":38211,\"start\":38195},{\"end\":38224,\"start\":38211},{\"end\":38233,\"start\":38224},{\"end\":38251,\"start\":38233},{\"end\":38262,\"start\":38251},{\"end\":38276,\"start\":38262},{\"end\":38290,\"start\":38276},{\"end\":38304,\"start\":38290},{\"end\":38321,\"start\":38304},{\"end\":38570,\"start\":38550},{\"end\":38585,\"start\":38570},{\"end\":38596,\"start\":38585},{\"end\":38604,\"start\":38596},{\"end\":38612,\"start\":38604},{\"end\":38965,\"start\":38946},{\"end\":38984,\"start\":38965},{\"end\":39352,\"start\":39334},{\"end\":39364,\"start\":39352},{\"end\":39378,\"start\":39364},{\"end\":40154,\"start\":40137},{\"end\":40170,\"start\":40154},{\"end\":40588,\"start\":40566},{\"end\":40811,\"start\":40795},{\"end\":40822,\"start\":40811},{\"end\":40835,\"start\":40822},{\"end\":40845,\"start\":40835},{\"end\":40862,\"start\":40845},{\"end\":40877,\"start\":40862},{\"end\":40896,\"start\":40877},{\"end\":40908,\"start\":40896},{\"end\":41271,\"start\":41249},{\"end\":41277,\"start\":41271},{\"end\":41286,\"start\":41277},{\"end\":41567,\"start\":41551},{\"end\":41582,\"start\":41567},{\"end\":41598,\"start\":41582},{\"end\":41613,\"start\":41598},{\"end\":41628,\"start\":41613},{\"end\":41635,\"start\":41628},{\"end\":42026,\"start\":42011},{\"end\":42036,\"start\":42026},{\"end\":42043,\"start\":42036},{\"end\":42274,\"start\":42265},{\"end\":42287,\"start\":42274},{\"end\":42300,\"start\":42287},{\"end\":42309,\"start\":42300},{\"end\":42497,\"start\":42487},{\"end\":42512,\"start\":42497},{\"end\":42530,\"start\":42512},{\"end\":42540,\"start\":42530},{\"end\":42552,\"start\":42540},{\"end\":42567,\"start\":42552},{\"end\":42580,\"start\":42567},{\"end\":42601,\"start\":42580},{\"end\":42609,\"start\":42601}]", "bib_venue": "[{\"end\":21039,\"start\":21025},{\"end\":21761,\"start\":21739},{\"end\":22893,\"start\":22805},{\"end\":23615,\"start\":23601},{\"end\":24408,\"start\":24389},{\"end\":25153,\"start\":25143},{\"end\":25747,\"start\":25675},{\"end\":26492,\"start\":26412},{\"end\":27652,\"start\":27559},{\"end\":28605,\"start\":28533},{\"end\":29987,\"start\":29860},{\"end\":30639,\"start\":30561},{\"end\":31235,\"start\":31135},{\"end\":31784,\"start\":31707},{\"end\":32193,\"start\":32142},{\"end\":32725,\"start\":32627},{\"end\":33334,\"start\":33244},{\"end\":33963,\"start\":33871},{\"end\":35052,\"start\":34973},{\"end\":35613,\"start\":35515},{\"end\":36190,\"start\":36112},{\"end\":36734,\"start\":36650},{\"end\":37938,\"start\":37922},{\"end\":38686,\"start\":38669},{\"end\":39058,\"start\":39042},{\"end\":39721,\"start\":39560},{\"end\":40319,\"start\":40247},{\"end\":41321,\"start\":41312},{\"end\":42067,\"start\":42049},{\"end\":21023,\"start\":20961},{\"end\":21361,\"start\":21305},{\"end\":21737,\"start\":21678},{\"end\":22259,\"start\":22223},{\"end\":22803,\"start\":22709},{\"end\":23281,\"start\":23251},{\"end\":23599,\"start\":23540},{\"end\":23969,\"start\":23910},{\"end\":24387,\"start\":24328},{\"end\":24718,\"start\":24708},{\"end\":25141,\"start\":24984},{\"end\":25673,\"start\":25598},{\"end\":25987,\"start\":25940},{\"end\":26410,\"start\":26328},{\"end\":26790,\"start\":26736},{\"end\":27134,\"start\":27129},{\"end\":27557,\"start\":27464},{\"end\":28012,\"start\":27934},{\"end\":28531,\"start\":28456},{\"end\":29013,\"start\":28957},{\"end\":29311,\"start\":29255},{\"end\":29858,\"start\":29716},{\"end\":30559,\"start\":30466},{\"end\":31133,\"start\":31033},{\"end\":31705,\"start\":31630},{\"end\":32140,\"start\":32101},{\"end\":32625,\"start\":32532},{\"end\":33242,\"start\":33156},{\"end\":33869,\"start\":33775},{\"end\":34509,\"start\":34425},{\"end\":34971,\"start\":34877},{\"end\":35513,\"start\":35419},{\"end\":36110,\"start\":36017},{\"end\":36648,\"start\":36566},{\"end\":37184,\"start\":37117},{\"end\":37586,\"start\":37576},{\"end\":37920,\"start\":37861},{\"end\":38193,\"start\":38152},{\"end\":38667,\"start\":38612},{\"end\":39040,\"start\":38984},{\"end\":39558,\"start\":39398},{\"end\":40245,\"start\":40170},{\"end\":40608,\"start\":40588},{\"end\":40936,\"start\":40908},{\"end\":41310,\"start\":41286},{\"end\":41734,\"start\":41701},{\"end\":42047,\"start\":42043},{\"end\":42328,\"start\":42323},{\"end\":42675,\"start\":42625}]"}}}, "year": 2023, "month": 12, "day": 17}