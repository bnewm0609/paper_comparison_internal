{"id": 250607509, "updated": "2023-10-05 12:42:39.773", "metadata": {"title": "Registration based Few-Shot Anomaly Detection", "authors": "[{\"first\":\"Chaoqin\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Haoyan\",\"last\":\"Guan\",\"middle\":[]},{\"first\":\"Aofan\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Ya\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Spratling\",\"middle\":[]},{\"first\":\"Yan-Feng\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper considers few-shot anomaly detection (FSAD), a practical yet under-studied setting for anomaly detection (AD), where only a limited number of normal images are provided for each category at training. So far, existing FSAD studies follow the one-model-per-category learning paradigm used for standard AD, and the inter-category commonality has not been explored. Inspired by how humans detect anomalies, i.e., comparing an image in question to normal images, we here leverage registration, an image alignment task that is inherently generalizable across categories, as the proxy task, to train a category-agnostic anomaly detection model. During testing, the anomalies are identified by comparing the registered features of the test image and its corresponding support (normal) images. As far as we know, this is the first FSAD method that trains a single generalizable model and requires no re-training or parameter fine-tuning for new categories. Experimental results have shown that the proposed method outperforms the state-of-the-art FSAD methods by 3%-8% in AUC on the MVTec and MPDD benchmarks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.07361", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/HuangGJZSW22", "doi": "10.48550/arxiv.2207.07361"}}, "content": {"source": {"pdf_hash": "4b182347b943548fe6479393bb24adac21740675", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.07361v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0e72e87a957dd45403f2c9e1ccb72b59c84951b9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4b182347b943548fe6479393bb24adac21740675.txt", "contents": "\nRegistration based Few-Shot Anomaly Detection\n\n\nChaoqin Huang huangchaoqin@sjtu.edu.cn \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\n\n\nShanghai Artificial Intelligence Laboratory\n\n\nNational University of Singapore\n\n\nHaoyan Guan haoyan.guan@kcl.ac.uk \nKing's College London\n\n\nAofan Jiang \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\n\n\nYa Zhang yazhang@sjtu.edu.cn \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\n\n\nShanghai Artificial Intelligence Laboratory\n\n\nMichael Spratling michael.spratling@kcl.ac.uk \nKing's College London\n\n\nYan-Feng Wang wangyanfeng@sjtu.edu.cn \nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\n\n\nShanghai Artificial Intelligence Laboratory\n\n\nRegistration based Few-Shot Anomaly Detection\nAnomaly DetectionFew-Shot LearningRegistration\nThis paper considers few-shot anomaly detection (FSAD), a practical yet under-studied setting for anomaly detection (AD), where only a limited number of normal images are provided for each category at training. So far, existing FSAD studies follow the one-model-per-category learning paradigm used for standard AD, and the inter-category commonality has not been explored. Inspired by how humans detect anomalies, i.e., comparing an image in question to normal images, we here leverage registration, an image alignment task that is inherently generalizable across categories, as the proxy task, to train a category-agnostic anomaly detection model. During testing, the anomalies are identified by comparing the registered features of the test image and its corresponding support (normal) images. As far as we know, this is the first FSAD method that trains a single generalizable model and requires no re-training or parameter fine-tuning for new categories. Experimental results have shown that the proposed method outperforms the state-of-the-art FSAD methods by 3%-8% in AUC on the MVTec and MPDD benchmarks. Source code is available at: https://github.com/MediaBrain-SJTU/RegAD\n\nIntroduction\n\nAnomaly detection (AD), with a wide range of applications such as defect detection [24], medical diagnosis [44], and autonomous driving [10], has received quite some attention in the computer vision community over the last decades. With the ambiguous definition of \"anomaly\", i.e., samples that do not conform to the \"normal\", it is impossible to train with an exhaustive set of anomalous samples. As a result, recent studies on anomaly detection have largely been devoted to unsupervised learning, i.e., learning with only the \"normal\" samples. Through modeling the normal distribution with one-class classification [35,30,43], reconstruction [47,13,39,18], or self-supervised learning tasks [12,42,33,45] Fig. 1. Different from (a) vanilla AD, and (b) existing FSAD methods under the onemodel-per-category learning paradigm, the proposed method (c) leverages feature registration as a category-agnostic approach for FSAD, under the one-model-all-category learning paradigm. Trained with aggregated data of multiple categories, the model is directly applicable to novel categories without any parameter fine-tuning, with the only need to estimate the normal feature distribution given the corresponding support set. methods detect anomalies by identifying samples with different distributions than the model.\n\nMost existing AD methods have focused on training a dedicated model for each category ( Fig. 1 (a)). However, in real-world scenarios such as defect detection, given hundreds of industrial products to handle, it is not cost-effective to collect a large training set for each product, not to mention the need for many time-sensitive applications. A couple of studies [36,29] have recently explored a special, yet practical, setting of AD, i.e., few-shot anomaly detection (FSAD), where only a limited number of normal images are provided for each category at training ( Fig. 1 (b)). The few-shot learning of anomaly detection has been approached with strategies to reduce the demand on training samples, such as radical data augmentation with multiple transformations [36] or a lighter estimator for the normal distribution estimation [29]. However, such approaches still follow the one-model-per-category learning paradigm and fail to leverage the inter-category commonality.\n\nThis paper aims to explore a new paradigm for FSAD, by learning a common model shared among multiple categories and also generalizable to novel categories, and inspired by how human beings detect anomalies. In fact, when a human is asked to search for the anomaly in an image, a simple strategy one may adopt is to compare the sample to a normal one to find the difference. As long as one knows how to compare two images, the actual semantics of the images does not matter anymore. To achieve such a human-like comparison process, we resort to registration, a process of transforming different images into one co-ordinate system in order to better enable comparison [4,46,25]. Registration is particularly suitable for FSAD, as registration is expected to be category-agnostic and thus generalizable across categories, allowing the model to be adaptable to novel categories without the necessity of parameter fine-tuning. Fig. 1 (c) provides an overview of the proposed Registration based few-shot Anomaly Detection (RegAD) framework. To train a category-agnostic anomaly detection model, we leverage registration, a task that is inherently generalizable across categories, as the proxy task. A Siamese network [5] with three spatial transformer network [19] blocks is employed as the registration network (see Fig. 2). For better robustness, instead of registering the images pixel-by-pixel as typical registration methods [25], here we propose a feature-level registration loss by maximizing the cosine similarity of features from the same category, which may be deemed as a relaxed version of the pixel-wise registration loss. Normal images from different categories are used together to aggregately train the model, with two images from the same category randomly selected as a training pair. Such aggregated training procedure is adopted so as to enable the trained registration model to be category-agnostic. At test time, a support set of a few normal samples is provided for the target category, together with each test sample. It is straightforward to identify anomalies by comparing the registered features of the test image and the corresponding support (normal) images. Given the support set, the normal distribution of registered features for the target category is estimated with a statistical-based distribution estimator [8]. Test samples that are out of the statistical normal distribution are considered anomalies. In this way, the model quickly adapts to novel categories by simply estimating its normal feature distribution without any parameter fine-tuning.\n\nTo validate the effectiveness of RegAD, we experiment with two challenging benchmark datasets for industrial defect detection, MVTec AD [2] and MPDD [20]. Our experimental results have shown that RegAD outperforms the state-of-the-art FSAD methods [36,29], achieving improvements of 5.1%, 6.9%, and 8.0% in AUC on MVTec, and improvements of 3.2%, 5.0%, and 3.4% in AUC on MPDD, for 2-shot, 4-shot, and 8-shot scenarios, respectively.\n\nThe main contributions of the paper are summarised as follows:\n\n-We introduce feature registration as a category-agnostic approach for fewshot anomaly detection (FSAD). To our best of knowledge, it is the first FSAD method that trains a single generalizable model and requires no retraining or parameter fine-tuning for new categories. -Extensive experiments on recent benchmark datasets have shown that the proposed RegAD outperforms the state-of-the-art FSAD methods on both the anomaly detection and anomaly localization tasks.\n\n\nRelated Work\n\n\nAnomaly Detection\n\nAD is a task where training datasets contain only normal data. To better estimate the normal distributions, one-class classification based approaches tend to depict the normal data directly with statistical approaches [9,35,26,30]. Selfsupervised based approaches are trained using only normal data, and then make inferences by assuming that anomalous data performs differently. In this domain, reconstruction [40,34,47,32,1,13,39,17] is the most popular self-supervision. Some approaches [12,42,33] introduce other self-supervisions, e.g., [12] applies dozens of image geometric transforms for transformation classification; [42] proposes a restoration framework for attribute restoration. Recent AD methods usually use feature embeddings extracted from a pre-trained deep neural network. Feature embedding is mostly used as an input for a traditional machine learning algorithm or statistical metrics such as the Mahalanobis distance [8]. The network used as a feature extractor can be trained from scratch [43], while several methods [21,8,45,28,14] have also achieved state-of-the-art results using models pre-trained on the ImageNet dataset [31]. This paper differs from these previous works by focusing on FSAD, where only a few normal images are available.\n\n\nFew-shot Learning\n\nFew-shot learning (FSL) aims to adapt to novel classes with a few annotated examples. Representative FSL methods can be categorized into metric learning, generation, and optimization. Metric learning approaches [37,38,15] learn to calculate a feature space that classifies an unseen sample based on its nearest example category. Generation methods [22,41,6] enhance the novel class performance by generating its images or features. Optimization methods [27,11] learn commonalities among different categories and explore efficient optimization strategies for novel classes based on these commonalities. In this paper, the proposed method predicts 'normal' or 'anomaly' for a new category. In contrast to previous work on FSL, both training data and support set only have positive (normal) examples without any negative (anomaly) samples.\n\n\nFew-shot Anomaly Detection\n\nFSAD aims to indicate anomalies with only a few normal samples as the support images for target categories. TDG [36] proposes a hierarchical generative model that captures the multi-scale patch distribution of each support image. They use multiple image transformations and optimize discriminators to distinguish between real and fake patches, as well as between different transformations applied to the patches. The anomaly score was obtained by aggregating the patch-based votes of the correct transformations. DiffNet [29] leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using a normalizing flow, which is a tool well-suited to estimate distributions from a few support samples. Metaformer [39] can be applied to the FSAD, although an additional large-scale dataset, MSRA10K [7], should be used during its entire meta-training procedure (beyond parameter pre-training), together with additional pixel-level annotations. In this paper, we design registration based FSAD to learn the category-agnostic feature registration, enabling the model to detect anomalies in new categories given a few normal images without fine-tuning. Fig. 2. The model architecture of the proposed RegAD. Given paired images from the same category, features are extracted by three convolutional residual blocks each followed by a spatial transformer network. A Siamese network acts as the feature encoder, supervised by a registration loss for feature similarity maximization.\n\n\nFeature Registration Loss\nC 3 E E P Stop Gradient Convolutional Block Spatial Transformer Network Encoder Predictor Shared S 3 S 2 C 2 S 1 C 1\n\nProblem Setting\n\nWe first formally define the problem setting for the proposed few-shot anomaly detection. Given a training set consisting of only normal samples of n categories, i.e., T train = n i=1 T i , where the subset T i consists of normal samples from the category c i , (i = 1, 2, \u00b7 \u00b7 \u00b7 , n), we want to train a category-agnostic anomaly detection model. At test time, given a normal or anomalous image from a target category c t (t / \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}) and its associated support set S t consisting of k normal samples from the target category c t , the trained category-agnostic anomaly detection model should predict whether the image is anomalous or not.\n\nFor FSAD, we attempt to detect anomalies from test samples of unseen/novel categories using only a few normal images as the support set. The key challenges lie in: (i) T train has only access to normal samples from multiple known categories (e.g., different objects or textures), without any image-level or pixel-level annotations, (ii) the test data is from an unseen/novel category, and (iii) only a few normal samples from the target category c t are available, making it hard to estimate the normal distribution of the target category c t .\n\n\nMethod\n\nMotivated by how humans detect anomalies, the feature registration is used as a generalization paradigm for FSAD. During the training procedure, we leverage an anomaly-free feature registration network to learn category-agnostic feature registration. During testing, given the support set of a few normal images, the normal distribution of registered features for the target category is estimated with a statistical-based distribution estimator. Test samples that are out of the learned statistical normal distribution are considered anomalies.\n\n\nFeature Registration Network\n\nGiven a pair of images I a and I b randomly selected from a same category in the training set T train , a ResNet-type convolutional network [16] is leveraged as the feature extractor. Specifically, as shown in Fig. 2, the first three convolutional residual blocks of ResNet, C 1 , C 2 , and C 3 , are adopted, and the last convolution block in ResNet's original design is discarded, in order to ensure that final features still retain spatial information. A spatial transformer network (STN) [19] is inserted into each block as a feature transformation module, so as to enable the model to learn feature registration flexibly, inspired by [45]. Specifically, a transformation function S i (i = 1, 2, 3) is applied on an input feature f s i :\nx t i y t i = S i (f s i ) = A i \uf8eb \uf8ed x s i y s i 1 \uf8f6 \uf8f8 = \u03b8 11 \u03b8 12 \u03b8 13 \u03b8 21 \u03b8 22 \u03b8 23 \uf8eb \uf8ed x s i y s i 1 \uf8f6 \uf8f8 ,(1)\nwhere (x t i , y t i ) are the target coordinates of output feature f t i , (x s i , y s i ) are the same points in the source coordinates of input feature f s i and A i is the affine transformation matrix. The module S i is used to learn the mappings from features of convolutional block C i with the same tiny architecture as used in [19].\n\nGiven paired extracted features f t 3,a and f t 3,b as the final transformation outputs, we design the feature encoder as a Siamese network [3]. A Siamese network is a parameter-sharing neural network applied on multiple inputs. To avoid the collapsing problem when optimized without negative pairs, inspired by Sim-Siam [5], features are processed by the same encoder network E followed by a prediction head P applied on one branch. A stop-gradient operation is applied on the other branch, as shown in Fig. 2, which is critical to prevent such collapsing solutions. Denote p a \u225c P (E(f 3,a )) and z b \u225c E(f 3,b ), a negative cosine similarity loss is applied:\nD(p a , z b ) = \u2212 p a ||p a || 2 \u00b7 z b ||z b || 2 ,(2)\nwhere || \u00b7 || 2 is a L 2 norm. Instead of registering the images pixel-by-pixel, here we use a feature-level registration loss which may be deemed as a relaxed version of the pixel-wise registration constraints for better robustness. Finally, following SimSiam [5], a symmetrized feature registration loss is defined as:\nL = 1 2 (D(p a , z b ) + D(p b , z a )).(3)\nDiscussion. Features from the proposed method retain relatively complete spatial information, since we adopt the first three convolutional blocks of ResNet as the backbone without global average pooling, followed by a convolutional encoder and predictor architecture, but not the MLP architecture in SimSiam [5]. Thus Eq. (3) should be computed by averaging cosine similarity scores at every spatial pixel. Features containing spatial information are beneficial for the AD task, which needs to provide anomaly score maps as prediction results. Different from SimSiam [5], which defines the inputs as two augmentations of one image and maximizes their similarity to enhance the model representation, the proposed feature registration leverages two different images as inputs and maximizes the similarity between the features to learn the registration.\n\n\nNormal Distribution Estimation\n\nTo perform testing, it is assumed that the feature registration ability can generalize to the target category, and the learned feature registration model is applied to the support set S t for the target category without parameter fine-tuning. Multiple data augmentations are applied to the support images, consistent with [36]. As the two branches of the Siamese network are exactly the same, only one branch feature is used for the normal distribution estimation. After achieving the registered features, a statistical-based estimator [8] is used to estimate the normal distribution of target category features, which uses multivariate Gaussian distributions to get a probabilistic representation of the normal class. Suppose an image is divided into a grid of (i,\nj) \u2208 [1, W ] \u00d7 [1, H] positions where W \u00d7 H is the resolution of features used to estimate the normal distribution. At each patch position (i, j), let F ij = {f k ij , k \u2208 [1, N ]\n} be the registered features from N augmented support images. f ij is the aggregated features at patch position (i, j), achieved by concatenating the three STN outputs at the corresponding position with upsampling operations to match their sizes. By the assumption that F ij is generated by N (\u00b5 ij , \u03a3 ij ), the sample covariance is:\n\u03a3 ij = 1 N \u2212 1 N k=1 f k ij \u2212 \u00b5 ij f k ij \u2212 \u00b5 ij T + \u03f5I,(4)\nwhere \u00b5 ij is the sample mean of F ij , and the regularization term \u03f5I makes the sample covariance matrix full rank and invertible. Finally, each possible patch position is associated with a multivariate Gaussian distribution.\n\nDiscussion. Data augmentations are widely adopted in AD, and especially in FSAD, including TDG [36] and DiffNet [29]. However, most methods simply apply the data augmentations on both the support and test images without any exploration of the impact. In this paper, we emphasize that data augmentation plays a very important role in expanding the support set, which is beneficial for the normal distribution estimation. Specifically, we adopt augmentations including rotation, translation, flipping, and graying for each image in the support set S t . Other augmentations like mixup and cutpaste are not considered since they seem more suitable for simulating anomalies [21]. We conduct the possible combinations of all these augmentations for each sample in the support set, which jointly combine into a larger support set. We conduct the normal distribution estimation on such an augmented support set. We study the impacts of different augmentations in the supplementary material.\n\n\nInference\n\nDuring inference, test samples that are out of the normal distribution are considered anomalies. For each test image in T test , we use the Mahalanobis distance M (f ij ) to give an anomaly score to the patch in position (i, j), where\nM (f ij ) = (f ij \u2212 \u00b5 ij ) T \u03a3 \u22121 ij (f ij \u2212 \u00b5 ij ).(5)\nThe matrix of Mahalanobis distances M = (M (f ij )) 1\u2a7di\u2a7dW,1\u2a7dj\u2a7dH forms an anomaly map. Three inverse affine transformations corresponding to the three STN modules are applied to this anomaly map to get the final anomaly score map M f inal aligned with the original image. High scores in this map indicate the anomalous areas. The final anomaly score of the entire image is the maximum of anomaly map M f inal . Compared with [36,29], RegAD cancels the data augmentation of the test images which reduces the inference computational costs.\n\n\nExperiments\n\n\nExperimental Setups\n\nDatasets. We experiment on two challenging real-world benchmark datasets for AD [2,20], which are both related to industrial defect detection. Competing Methods. We consider two state-of-the-art FSAD approaches, TDG [36] and DiffNet [29]. These two methods both train models individually for each category (setting (ii)). Results are reproduced using the official source code. Considering that our method uses data from multiple categories, for fairness of comparison, we extend them to leverage the same amount of data (setting (i)). A pre-training procedure is added to these methods, where data from multiple categories are used to pre-train the transformation classifier for TDG or initialize the normalizing flow-based estimator for DiffNet. The corresponding methods are TDG+ and DiffNet+. We also evaluate RegAD under the individual training setting, and denote the corresponding method as RegAD-L. We compare with some state-of-the-art vanilla AD methods, such as GANomaly [1], ARNet [42], MKD [33], CutPaste [21], FYD [45], PaDiM [8], PatchCore [28] and CflowAD [14]. These methods use the whole normal dataset for their training, so they can be deemed as the upper bound on FSAD performance. Table 1. Results of k-shot anomaly detection on the MVTec dataset, comparing with state-of-the-art methods. Results are listed as the average AUC in % of 10 runs and are marked individually for each category. A macro-average score over all categories is also reported in the last row. The best-performing method is in bold. Evaluation Protocols. We quantify the model performance using the area under the Receiver Operating Characteristic (ROC) curve metric (AUC), which is commonly adopted as the performance measurement for AD tasks. The imagelevel AUC and the pixel-level AUC are used for anomaly detection and anomaly localization respectively.\n\n\nModel Configuration and Training\n\nDetails. An ImageNet pre-trained ResNet-18 [16] is used as the backbone, followed by a convolutional-based encoder and predictor. To retain the spatial information, the encoder contains three 1 \u00d7 1 convolutional layers, while the predictor contains two 1 \u00d7 1 convolutional layers, without any pooling operation. We train models on 224 \u00d7 224 images on one NVIDIA GTX 3090. We update the parameters using momentum SGD with a learning rate of 0.0001 for 50 epochs, with a batch size of 32. A single cycle of cosine learning rate is used as the decay schedule.\n\n\nComparison with State-of-the-art Methods\n\nComparison with Few-Shot Anomaly Detection Methods. Experiments were conducted using the leave-one-out setting, i.e., a target category was chosen to be tested, while other categories in the dataset are used for training. Table 1 and Table 2 show the comparison results on MVTec and MPDD, respectively, under the experimental setting (i). RegAD achieves an improvement of 5.1%, 6.9%, 8.0% in average AUC on MVTec, and an improvement of 3.2%, 5.0%, 3.4% in average AUC on MPDD, over DiffNet+ [29], with 2-shot, 4-shot, and  8-shot scenarios, respectively. Also, with one-shot, RegAD achieves 82.4% and 57.8% AUC on MVtec and MPDD respectively. RegAD is tested without any parameter fine-tuning, which may not guarantee the best performance for every category, while other baselines have unfair advantages in that they tune the parameters for each category. In 9 out of the 15 categories, RegAD outperforms all the other baselines. RegAD also achieves the least standard deviation (10.94) for the 15 categories when k=8, compared to TDG+ (15.20) and DiffNet+ (13.11), suggesting its better generalizability across different categories. Also, although using different training settings, for MVTec (k=8), RegAD achieves 91.2% AUC, with an \u22483% improvement compared with Metaformer [39] which uses an additional large-scale dataset, MSRA10K [7], during its entire training procedure.\n\nDiscussion. Adaptation time is important for real-world applications of FSAD. The procedures of fine-tuning for both TDG+ and DiffNet+ are timeconsuming since they update the models for many epochs, while RegAD has the fastest adaptation speed since it is based on a statistical estimator which needs only one inference for each support image. In Table 3, we report the adaptation times for each method, by averaging the results for k = 2, 4, 8 on both the MVTec and MPDD datasets. Compared with TDG+ (1559.76s) and DiffNet+ (357.75s), the proposed RegAD has the fastest adaptation speed (4.47s). Table 3 also compares these methods under experimental setting (ii), where we train the models individually using the support images for each category. RegAD-L means RegAD with individual training on one category only. Assuming that features pre-trained by ImageNet are fully representative, we simply fine-tune features using limited support images. Thus, we conduct the finetuning procedures directly under an ImageNet pre-training backbone for all methods. All methods use the same ImageNet pre-training backbone to have a fair comparison. In this setting, RegAD-L outperforms both TDG and DiffNet on the MVTec dataset. DiffNet performs better than the proposed method on the MPDD dataset. However, compared with RegAD-L, the proposed RegAD improves a lot, showing the effectiveness of the proposed feature registration aggregated training procedure on multiple categories.\n\nComparison with Vanilla Anomaly Detection Methods. The state-ofthe-art vanilla AD methods use the whole normal dataset for their training and train a separate model for each category, so their performance can be seen as the upper bound for FSAD. We consider methods including GANomaly [1], ARNet [42], MKD [33], CutPaste [21], FYD [45], PaDiM [8], PatchCore [28] and CflowAD [14]. Results in Table 4 show that the proposed RegAD reaches competitive performance even compared with vanilla AD methods that are based on extensive normal data. For example, with only 4 support images, the proposed method (88.2% AUC) outperforms MKD (87.7%) with the same ImageNet pretrained backbone, and with 32 support images its AUC increases to 94.6%.\n\n\nAblation Studies\n\nExperiments were performed to evaluate the contribution made by individual components of the proposed method. Results of ablation studies for k-shot Table 5. Ablation studies of k-shot anomaly detection and localization on the MVTec and MPDD datasets. Modules of 'A', 'F', and 'S' mean the augmentations for the support set, the feature registration aggregated training, and the spatial transformer networks (STN), respectively. Results are listed as the macro-average AUC in % over all categories in each dataset of 10 runs. The best-performing method is in bold.\n\n\nModules\n\nMVTec  anomaly detection and localization on the MVTec and MPDD datasets are shown in Table 5. Modules of 'A', 'F', and 'S' mean the augmentations for support sets, the feature registration aggregated training on multiple categories, and the spatial transformer networks (STN), respectively. Results in Table 5 show that: (i) Augmentations. The proposed support set augmentations are shown to be essential for both detection and localization. With k = {2, 4, 8}, the AUC is improved for 6.8%, 6.9%, 6.9% on MVTec and for 1.2%, 0.5%, 0.6% on MPDD, respectively. We further presents the ablation studies of comparing different augmentation methods for support images in the supplementary material.\n\n(ii) Feature Registration Aggregated Training. The feature registration aggregated training on multiple categories is effective both with and without support image augmentations. It shows that the proposed feature registration is beneficial for estimating the normal distribution. As shown in Table 5, with k = {2, 4, 8}, the proposed anomaly-free feature registration can improve the AUC by 3.3%, 2.9%, 2.6% on MVTec, respectively.\n\n(iii) Spatial Transformer Modules. The proposed STN module is good for improving the ability of the feature registration and thus beneficial for AD. For example, as shown in Table 5, when k = 8, the STN module can further improve the performance from 89.3% to 91.2% on MVTec and from 64.8% to 71.9% on MPDD. However, models with STN modules show similar pixel-level localization performance with models without STN modules. The reason comes from the information lost of the inverse transformation operation and its impre- cision. These inverse transformations are designed as post-processing operations to rematch the spatial location of transformed features and the original images.\n\nWe further conduct ablation studies on different transformation versions of STN modules on MVTec and MPDD for AD, as shown in Table 6. The best performing STN version is rotation+scale on MVTec, which matches the observation that samples in this dataset are all aligned to the center, and thus, there is no need for translation. While for the MPDD dataset, since the samples are not well be centered, the version of STN with affine transformations shows the best performance. STN is used as a feature transformation module, enabling the model to implicitly transform the images to facilitate feature registration. Images in MPDD are captured under various spatial orientations and positions, thus aligning the features is expected to be helpful. For MVTec, objects are well centralized and have similar orientations, so STN is less helpful to MVTec.\n\n\nVisualization Analysis\n\nTo qualitatively analyze how the proposed feature registration approach improves the anomaly localization performance, we visualize the results of some cases from the MVTec and MPDD datasets. It can be seen from the results in Fig. 3 that the localization produced by RegAD using aggregated training (column e) is closer to the ground truth (column f) than that produced by the individual training baseline (column c). This illustrates the effectiveness of the proposed feature registration training procedure on multiple categories. We also use t-SNE [23] to visualize the features learned on the MVTec dataset, as shown in Fig. 4. Each dot here represents an augmented normal sample from the test set. It can be seen that the proposed feature registration makes the features more compact within each category, and pushes away features of different categories, which is desirable for the benefit of estimating the normal distribution for each category.\n\n\nConclusion\n\nThis paper proposes an FSAD method utilizing registration, a task inherently generalizable across categories, as the proxy task. Given only a few normal samples for each category, we trained a category-agnostic feature registration network with the aggregated data. This model is shown to be directly generalizable to new categories, requiring no re-training or parameter fine-tuning. The anomalies are identified by comparing the registered features of the test image and its corresponding support (normal) images. For both anomaly detection and anomaly localization, the method is shown to be competitive, even compared with vanilla AD methods that are trained with much larger volumes of data. The impressive results suggest a high potential for the proposed method to be applicable in real-world anomaly detection environments.\n\n\nA Main Contributions\n\nThis paper targets a challenging yet practical setting for anomaly detection, with 1) a single model for all categories (i.e., generalizable without fine-tuning), 2) only a few images for each novel category (i.e., few shot), and 3) only normal samples available (i.e., unsupervised setting). To our best knowledge, it is the first attempt to explore such a setting, as a critical step toward practical large-scale industrial applications, a point appreciated by the other reviewers. To learn a category-agnostic model, we further propose a novel comparisonbased solution, which is quite different from the popular reconstruction-based or classification-based methods. We adopt STN to align the images and Siamese network to implement the comparison. The SOTA results achieved on MVTec and MPDD show the effectiveness of our method.\n\n\nB Experiments\n\n\nB.1 Experiments with a Large k.\n\nTo decrease the training burden, RegAD is designed to be adaptable to unseen categories without parameter fine-tuning. Without fine-tuning on the few-shot support examples, simply increasing the shot number, the performance gain saturates very soon. We further experiment with k=64 and k=128. As shown in Table 7, when k increases from 64 to 128, a limited performance gain is observed. But the results are still competitive, though with a shallow backbone, compared to those of the AD methods trained by full data. \n\n\nB.2 Ablation Studies on Support Set Augmentations.\n\nThe proposed support set augmentations are shown to be essential for both detection and localization. Table 8 further presents the ablation studies of comparing different augmentation methods for support images with k = 2. The experimental results have validated the effectiveness of all the proposed augmentation methods. In particular, rotation and translation are shown to perform better on MVTec, while flipping and rotation seem to perform better on MPDD. Table 8. Ablation studies of different versions of support set augmentations on the MVTec and MPDD datasets with k = 2. Besides the full version of RegAD, We also provide the individual training version to reduce the influence of data augmentations on multiple categories. G, F, T, R means graying, flipping, translation, and rotation, respectively. Results are listed as the macro-average AUC in % over all categories in each dataset of 10 runs. The best-performing method is in bold. Although using different training settings, according to the reported results in [39], Metaformer achieves about 88% AUC for MVTec when k=8, while Re-gAD achieves 91.2% AUC, an \u22483% improvement, with the same test set and evaluation protocol. Metaformer achieves worse results despite three unfair advantages: (i) an additional large-scale dataset, MSRA10K, is used during its entire meta-training procedure (beyond parameter pre-training), together with additional pixel-level annotations; (ii) it performs additional fine-tuning on each novel category; (iii) it is trained with a deep transformer architecture for more epochs (100 vs. 50), with a larger batch size (64 vs. 32).\n\n\nAugmentations\n\n-\nMVTec [2]: MVTec comprises 15 categories with 3629 images for training and validation and 1725 images for testing. The training set contains only of normal images without defects. The test set contains both images with various kinds of defects (anomaly) and defect-free images (normal). On average five per category, 73 different defect types are given. All images are in the resolution range between 700 \u00d7 700 and 1024 \u00d7 1024 pixels. Pixel-wise ground truth labels for each defective image region are provided. -MPDD [20]: MPDD is a newly proposed dataset focused specifically on defect detection during painted metal part fabrication, containing 6 classes of metal parts. Images are captured under the conditions of various spatial orientations, positions, and distances of multiple objects, concerning different light intensities and a non-homogeneous background.For each dataset, we conduct experiments on two different experimental settings. (i) Aggregated training on multiple categories and then adapting to unseen categories, and (ii) Individual training only with the support set for each category.\n\nFig. 3 .\n3Qualitative results of anomaly localization for RegAD on the MVTec dataset (top three rows) and the MPDD dataset (bottom two rows) for several cases, including localization results with individual training and aggregated training. Results from (e) show better performance than results from (c), showing the effectiveness of the proposed feature registration aggregated training procedure.\n\nFig. 4 .\n4Visualization, using t-SNE, of the features learned from the MVTec dataset, using (a) the baseline without the feature registration, and (b) the proposed method with the feature registration. The same t-SNE optimization iterations are used in each case. Results show that features with registration are more compact within each category, and more separated from different categories.\n\n\n, many AD arXiv:2207.07361v1 [cs.CV] 15 Jul 2022 One-model-per-category Learning Paradigm One-model-all-category Learning ParadigmM \n\nTest Image \n\nSupport Set \n\nImageNet \nPretrain \n\nM1 \n\nImageNet Pretrain \n\nM2 \nMN \n\nImageNet Pretrain \nImageNet Pretrain \n\nM1 \n\nImageNet Pretrain \n\nM2 \nMN \n\nImageNet Pretrain \nImageNet Pretrain \n\n(a) Vanilla Anomaly Detection \n\n(b) Few-shot Anomaly Detection [33, 26] \n(c) Registration based Few-shot Anomaly Detection \n\nFeature \nRegistration \nAggregated \nTraining \n\nNormal \nDistribution \nEstimation \n\nTarget Category \n\nAggregated Training Categories \n\nNormal Distribution \n\ncompare \n\n\n\nTable 2 .\n2Results of k-shot anomaly detection on the MPDD dataset, comparing with state-of-the-art methods. Results are listed as the average AUC in % of 10 runs and are marked individually for each category. A macro-average score over all categories is also reported in the last row. The best-performing method is in bold.Category \n\nk=2 \nk=4 \nk=8 \n\nTDG+ \n[36] \n\nDiffNet+ \n[29] \n\nRegAD \n(ours) \n\nTDG+ \n[36] \n\nDiffNet+ \n[29] \n\nRegAD \n(ours) \n\nTDG+ \n[36] \n\nDiffNet+ \n[29] \n\nRegAD \n(ours) \n\nbracket black \n46.4 \n56.7 \n63.3 \n48.8 \n59.9 \n63.8 \n51.0 \n69.7 \n67.3 \nbracket brown \n54.9 \n61.3 \n59.4 \n57.5 \n64.2 \n66.1 \n65.4 \n66.3 \n69.6 \nbracket white \n64.0 \n42.2 \n55.6 \n65.4 \n51.8 \n59.3 \n66.8 \n69.1 \n61.4 \nconnector \n53.1 \n54.1 \n73.0 \n55.8 \n54.8 \n77.2 \n62.9 \n54.5 \n84.9 \nmetal plate \n91.8 \n96.8 \n61.7 \n95.1 \n98.2 \n78.6 \n98.4 \n98.8 \n80.2 \ntubes \n51.8 \n49.8 \n67.1 \n58.5 \n50.7 \n67.5 \n64.9 \n52.6 \n67.9 \n\nAverage \n60.3 \n60.2 \n63.4 \n63.5 \n63.3 \n68.3 \n68.2 \n68.5 \n71.9 \n\n\n\nTable 3 .\n3Results of anomaly detection on the MVTec and MPDD datasets under twodifferent experimental settings (i) and (ii), comparing with state-of-the-art few-shot \nanomaly detection methods on k = 2, 4, 8. Results are listed as the macro-average \nAUC in % over all categories in each dataset of 10 runs. The best-performing method \nfor each experimental setting is in bold. \n\nMethods \nImageNet Aggregated \nTime of \nMVTec \nMPDD \nPretrain \nTraining \nAdaptation \nk=2 k=4 k=8 k=2 k=4 k=8 \n\nTDG [36] \n\u2713 \n\u2717 \n-\n71.2 72.7 75.2 57.3 60.4 64.4 \nDiffNet [29] \n\u2713 \n\u2717 \n-\n80.5 80.8 82.9 58.4 61.2 66.5 \nRegAD-L (ours) \n\u2713 \n\u2717 \n-\n81.5 84.9 87.4 50.8 54.2 61.1 \n\nTDG+ [36] \n\u2713 \n\u2713 \n1559.76s \n73.2 74.4 76.6 60.3 63.5 68.2 \nDiffNet+ [29] \n\u2713 \n\u2713 \n357.75s \n80.6 81.3 83.2 60.2 63.3 68.5 \nRegAD (ours) \n\u2713 \n\u2713 \n4.47s \n85.7 88.2 91.2 63.4 68.3 71.9 \n\n\n\nTable 4 .\n4Results of anomaly detection and anomaly localization on the MVTec and MPDD datasets, comparing with state-of-the-art vanilla AD methods. Results are listed as AUC in % as the macro-average score over all categories in each dataset.Methods \nData \nImageNet \nBackbone \nMVTec \nMPDD \nPretrain \nimage \npixel \nimage \npixel \n\nRegAD (k=4) \n4 images \n\u2713 \nRes18 \n88.2 \n95.8 \n68.8 \n93.9 \nRegAD (k=8) \n8 images \n\u2713 \nRes18 \n91.2 \n96.7 \n71.9 \n95.1 \nRegAD (k=16) \n16 images \n\u2713 \nRes18 \n92.7 \n96.6 \n75.3 \n96.3 \nRegAD (k=32) \n32 images \n\u2713 \nRes18 \n94.6 \n96.9 \n76.8 \n96.3 \n\nGANomaly [1] \nfull data \n\u2717 \nUNet \n80.5 \n-\n64.8 \n-\nARNet [42] \nfull data \n\u2717 \nUNet \n83.9 \n-\n69.7 \n-\nMKD [33] \nfull data \n\u2713 \nRes18 \n87.7 \n90.7 \n-\n-\nCutPaste [21] \nfull data \n\u2713 \nRes18 \n95.2 \n96.0 \n-\n-\nFYD [45] \nfull data \n\u2713 \nRes18 \n97.3 \n97.4 \n-\n-\nPaDiM [8] \nfull data \n\u2713 \nWRN50 \n97.9 \n97.5 \n74.8 \n96.7 \nPatchCore [28] \nfull data \n\u2713 \nWRN50 \n99.1 \n98.1 \n82.1 \n95.7 \nCflowAD [14] \nfull data \n\u2713 \nWRN50 \n98.3 \n98.6 \n86.1 \n97.7 \n\n\n\nTable 6 .\n6Ablationstudies of different transformation versions of STN modules on \nMVTec and MPDD for anomaly detection with k = 2. T, R means translation, and \nrotation, respectively. Results are listed as the macro-average AUC in % over all cate-\ngories in each dataset of 10 runs. The best-performing method is in bold. \n\nData \nno STN \nT \nR \nscale shear \nR \n+scale \n\nT \n+scale \nT+R \nT+R \n+scale \naffine \n\nMVTec \n83.0 \n84.5 85.0 \n84.9 \n84.9 \n85.7 \n84.9 \n84.2 \n84.9 \n84.5 \nMPDD \n52.8 \n62.3 57.7 \n59.2 \n59.0 \n61.5 \n61.8 \n61.0 \n61.7 \n63.4 \n\n\n\nTable 7 .\n7Comparison with AD method trained by full data.RegAD \nPatchCore [28] \nCflowAD [14] \nk \n32 \n64 \n128 \nfull data \nfull data \nMVTec \n94.6% \n95.5% \n95.9% \n99.1% \n98.3% \nMPDD \n76.8% \n82.3% \n83.2% \n82.1% \n86.1% \n\n\n\n\nB.3 Comparisons with Metaformer[39].Individual Training \nAggregated Training \n\nMVTec \nMPDD \nMVTec \nMPDD \nG \nF \nT \nR \nimage \npixel \nimage \npixel \nimage \npixel \nimage \npixel \n\n74.7 \n88.6 \n49.6 \n89.5 \n79.1 \n90.5 \n57.6 \n91.0 \n\u2713 \n74.9 \n88.6 \n49.4 \n89.4 \n79.5 \n90.7 \n58.0 \n91.3 \n\u2713 \n75.5 \n90.0 \n50.1 \n90.2 \n79.8 \n92.6 \n58.6 \n92.6 \n\u2713 \n77.4 \n90.9 \n49.8 \n91.5 \n81.3 \n92.4 \n58.3 \n90.9 \n\u2713 \n79.6 \n92.7 \n50.0 \n91.7 \n82.2 \n93.6 \n59.8 \n90.8 \n\n\u2713 \n\u2713 \n75.6 \n89.9 \n50.0 \n90.1 \n80.5 \n92.7 \n59.7 \n91.7 \n\u2713 \n\u2713 \n77.5 \n90.9 \n49.8 \n91.4 \n81.0 \n92.4 \n58.5 \n92.6 \n\u2713 \n\u2713 \n79.7 \n92.6 \n50.0 \n91.6 \n83.8 \n93.7 \n60.9 \n92.6 \n\u2713 \n\u2713 \n77.7 \n91.5 \n50.1 \n91.7 \n81.6 \n93.2 \n58.2 \n92.3 \n\u2713 \n\u2713 \n79.7 \n92.9 \n51.3 \n91.8 \n82.3 \n94.0 \n59.7 \n92.9 \n\u2713 \n\u2713 \n81.3 \n93.1 \n49.9 \n92.2 \n84.2 \n94.6 \n60.6 \n91.7 \n\n\u2713 \n\u2713 \n\u2713 \n77.8 \n91.5 \n50.2 \n91.6 \n81.7 \n93.5 \n59.9 \n92.6 \n\u2713 \n\u2713 \n\u2713 \n79.8 \n92.9 \n51.2 \n91.7 \n83.9 \n94.2 \n63.0 \n93.0 \n\u2713 \n\u2713 \n\u2713 \n81.4 \n93.1 \n49.9 \n92.3 \n84.9 \n94.7 \n61.2 \n92.8 \n\u2713 \n\u2713 \n\u2713 \n81.5 \n93.3 \n50.7 \n92.4 \n85.4 \n94.6 \n61.2 \n93.2 \n\n\u2713 \n\u2713 \n\u2713 \n\u2713 \n81.5 \n93.3 \n50.8 \n92.4 \n85.7 \n94.6 \n63.4 \n93.2 \n\n\n\nGanomaly: Semi-supervised anomaly detection via adversarial training. S Ak\u00e7ay, A Atapour-Abarghouei, T P Breckon, Proceedings of the Asian Conference on Computer Vision (ACCV). the Asian Conference on Computer Vision (ACCV)Springer411Ak\u00e7ay, S., Atapour-Abarghouei, A., Breckon, T.P.: Ganomaly: Semi-supervised anomaly detection via adversarial training. In: Proceedings of the Asian Conference on Computer Vision (ACCV). pp. 622-637. Springer (2018) 4, 8, 11\n\nMvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)3Bergmann, P., Fauser, M., Sattlegger, D., Steger, C.: Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9592-9600 (2019) 3, 8\n\nSignature verification using a siamese time delay neural network. J Bromley, I Guyon, Y Lecun, E S\u00e4ckinger, R Shah, Advances in neural information processing systems. 66Bromley, J., Guyon, I., LeCun, Y., S\u00e4ckinger, E., Shah, R.: Signature verifica- tion using a siamese time delay neural network. Advances in neural information processing systems (NeurIPS) 6 (1993) 6\n\nA survey of image registration techniques. L G Brown, ACM computing surveys (CSUR). 2443Brown, L.G.: A survey of image registration techniques. ACM computing surveys (CSUR) 24(4), 325-376 (1992) 3\n\nExploring simple siamese representation learning. X Chen, K He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)36Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 15750-15758 (2021) 3, 6\n\nMulti-level semantic feature augmentation for one-shot learning. Z Chen, Y Fu, Y Zhang, Y G Jiang, X Xue, L Sigal, IEEE Transactions on Image Processing. 2894Chen, Z., Fu, Y., Zhang, Y., Jiang, Y.G., Xue, X., Sigal, L.: Multi-level semantic feature augmentation for one-shot learning. IEEE Transactions on Image Process- ing 28(9), 4594-4605 (2019) 4\n\nGlobal contrast based salient region detection. M M Cheng, N J Mitra, X Huang, P H Torr, S M Hu, IEEE transactions on pattern analysis and machine intelligence. 3710Cheng, M.M., Mitra, N.J., Huang, X., Torr, P.H., Hu, S.M.: Global contrast based salient region detection. IEEE transactions on pattern analysis and machine intel- ligence 37(8), 569-582 (2014) 4, 10\n\nPadim: a patch distribution modeling framework for anomaly detection and localization. T Defard, A Setkov, A Loesch, R Audigier, Proceedings of the IEEE/CVF International Conference on Pattern Recognition (ICPR). the IEEE/CVF International Conference on Pattern Recognition (ICPR)Springer311Defard, T., Setkov, A., Loesch, A., Audigier, R.: Padim: a patch distribution mod- eling framework for anomaly detection and localization. In: Proceedings of the IEEE/CVF International Conference on Pattern Recognition (ICPR). pp. 475- 489. Springer (2021) 3, 4, 7, 8, 11\n\nAnomaly detection over noisy data using learned probability distributions. E Eskin, International Conference on Machine Learning (ICML). 4Eskin, E.: Anomaly detection over noisy data using learned probability distribu- tions. In: International Conference on Machine Learning (ICML) (2000) 4\n\nRobust physical-world attacks on deep learning visual classification. K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)1Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., Prakash, A., Kohno, T., Song, D.: Robust physical-world attacks on deep learning visual classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1625-1634 (2018) 1\n\nModel-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, International Conference on Machine Learning (ICML). 4Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep networks. In: International Conference on Machine Learning (ICML). pp. 1126-1135 (2017) 4\n\nDeep anomaly detection using geometric transformations. I Golan, R El-Yaniv, Advances in neural information processing systems (NeurIPS). 314Golan, I., El-Yaniv, R.: Deep anomaly detection using geometric transformations. In: Advances in neural information processing systems (NeurIPS). vol. 31 (2018) 1, 4\n\nMemorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. D Gong, L Liu, V Le, B Saha, M R Mansour, S Venkatesh, A V Hengel, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)14Gong, D., Liu, L., Le, V., Saha, B., Mansour, M.R., Venkatesh, S., Hengel, A.v.d.: Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In: Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision (ICCV). pp. 1705-1714 (2019) 1, 4\n\nCflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. D Gudovskiy, S Ishizaka, K Kozuka, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)418Gudovskiy, D., Ishizaka, S., Kozuka, K.: Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 98-107 (2022) 4, 8, 11, 18\n\nRevisiting deep local descriptor for improved few-shot classification. J He, R Hong, X Liu, M Xu, M Wang, 30th International Joint Conference on Artificial Intelligence (IJCAI. 4He, J., Hong, R., Liu, X., Xu, M., Wang, M.: Revisiting deep local descriptor for improved few-shot classification. 30th International Joint Conference on Artificial Intelligence (IJCAI) pp. 3420-3426 (2021) 4\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)59He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770-778 (2016) 5, 9\n\nSelf-supervised masking for unsupervised anomaly detection and localization. C Huang, Q Xu, Y Wang, Y Wang, Y Zhang, IEEE Transactions on Multimedia. 4Huang, C., Xu, Q., Wang, Y., Wang, Y., Zhang, Y.: Self-supervised masking for un- supervised anomaly detection and localization. IEEE Transactions on Multimedia (2022) 4\n\nEsad: End-to-end semisupervised anomaly detection. C Huang, F Ye, P Zhao, Y Zhang, Y Wang, Q Tian, The 32nd British Machine Vision Conference (BMVC). 1Huang, C., Ye, F., Zhao, P., Zhang, Y., Wang, Y., Tian, Q.: Esad: End-to-end semi- supervised anomaly detection. In: The 32nd British Machine Vision Conference (BMVC) (2022) 1\n\nM Jaderberg, K Simonyan, A Zisserman, Spatial transformer networks. Advances in neural information processing systems. 286Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks. Advances in neural information processing systems (NeurIPS) 28 (2015) 3, 6\n\nDeep learning-based defect detection of metal parts: evaluating current methods in complex conditions. S Jezek, M Jonak, R Burget, P Dvorak, M Skotak, International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT). IEEE3Jezek, S., Jonak, M., Burget, R., Dvorak, P., Skotak, M.: Deep learning-based de- fect detection of metal parts: evaluating current methods in complex conditions. In: International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT). pp. 66-71. IEEE (2021) 3, 8\n\nCutpaste: Self-supervised learning for anomaly detection and localization. C L Li, K Sohn, J Yoon, T Pfister, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)411Li, C.L., Sohn, K., Yoon, J., Pfister, T.: Cutpaste: Self-supervised learning for anomaly detection and localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9664-9674 (2021) 4, 7, 8, 11\n\nDeep representation learning on longtailed data: A learnable embedding augmentation perspective. J Liu, Y Sun, C Han, Z Dou, W Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)4Liu, J., Sun, Y., Han, C., Dou, Z., Li, W.: Deep representation learning on long- tailed data: A learnable embedding augmentation perspective. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2970-2979 (2020) 4\n\nVisualizing data using t-sne. L V D Maaten, G Hinton, Journal of machine learning research. 91114Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine learn- ing research 9(11) (2008) 14\n\nAnomaly machine component detection by deep generative model with unregularized score. T Matsubara, R Tachibana, K Uehara, 2018 International Joint Conference on Neural Networks (IJCNN). IEEE1Matsubara, T., Tachibana, R., Uehara, K.: Anomaly machine component detection by deep generative model with unregularized score. In: 2018 International Joint Conference on Neural Networks (IJCNN). pp. 1-8. IEEE (2018) 1\n\nBrainaligner: 3d registration atlases of drosophila brains. H Peng, P Chung, F Long, L Qu, A Jenett, A M Seeds, E W Myers, J H Simpson, Nature methods. 863Peng, H., Chung, P., Long, F., Qu, L., Jenett, A., Seeds, A.M., Myers, E.W., Simpson, J.H.: Brainaligner: 3d registration atlases of drosophila brains. Nature methods 8(6), 493-498 (2011) 3\n\nCoherence pursuit: Fast, simple, and robust principal component analysis. M Rahmani, G K Atia, IEEE Transactions on Signal Processing. 65234Rahmani, M., Atia, G.K.: Coherence pursuit: Fast, simple, and robust principal component analysis. IEEE Transactions on Signal Processing 65(23), 6260-6275 (2017) 4\n\nOptimization as a model for few-shot learning. S Ravi, H Larochelle, International Conference on Learning Representations (ICLR. 4Ravi, S., Larochelle, H.: Optimization as a model for few-shot learning. In: Inter- national Conference on Learning Representations (ICLR) (2017) 4\n\nTowards total recall in industrial anomaly detection. K Roth, L Pemula, J Zepeda, B Sch\u00f6lkopf, T Brox, P Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)418Roth, K., Pemula, L., Zepeda, J., Sch\u00f6lkopf, B., Brox, T., Gehler, P.: Towards total recall in industrial anomaly detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 14318- 14328 (2022) 4, 8, 11, 18\n\nSame same but differnet: Semi-supervised defect detection with normalizing flows. M Rudolph, B Wandt, B Rosenhahn, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)10Rudolph, M., Wandt, B., Rosenhahn, B.: Same same but differnet: Semi-supervised defect detection with normalizing flows. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 1907-1916 (2021) 2, 3, 4, 7, 8, 9, 10\n\nDeep one-class classification. L Ruff, R Vandermeulen, N Goernitz, L Deecke, S A Siddiqui, A Binder, E M\u00fcller, M Kloft, International Conference on Machine Learning (ICML). 14Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S.A., Binder, A., M\u00fcller, E., Kloft, M.: Deep one-class classification. In: International Conference on Machine Learning (ICML). pp. 4393-4402 (2018) 1, 4\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International Journal of Computer Vision. 11534Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog- nition challenge. International Journal of Computer Vision 115(3), 211-252 (2015) 4\n\nAdversarially learned one-class classifier for novelty detection. M Sabokrou, M Khalooei, M Fathy, E Adeli, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)4Sabokrou, M., Khalooei, M., Fathy, M., Adeli, E.: Adversarially learned one-class classifier for novelty detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3379-3388 (2018) 4\n\nMultiresolution knowledge distillation for anomaly detection. M Salehi, N Sadjadi, S Baselizadeh, M H Rohban, H R Rabiee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)112021) 1, 4, 8Salehi, M., Sadjadi, N., Baselizadeh, S., Rohban, M.H., Rabiee, H.R.: Mul- tiresolution knowledge distillation for anomaly detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 14902-14912 (2021) 1, 4, 8, 11\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. T Schlegl, P Seeb\u00f6ck, S M Waldstein, U Schmidt-Erfurth, G Langs, International Conference on Information Processing in Medical Imaging. Springer4Schlegl, T., Seeb\u00f6ck, P., Waldstein, S.M., Schmidt-Erfurth, U., Langs, G.: Unsu- pervised anomaly detection with generative adversarial networks to guide marker discovery. In: International Conference on Information Processing in Medical Imag- ing. pp. 146-157. Springer (2017) 4\n\nEstimating the support of a high-dimensional distribution. B Sch\u00f6lkopf, J C Platt, J Shawe-Taylor, A J Smola, R C Williamson, Neural computation. 1374Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J., Williamson, R.C.: Esti- mating the support of a high-dimensional distribution. Neural computation 13(7), 1443-1471 (2001) 1, 4\n\nA hierarchical transformation-discriminating generative model for few shot anomaly detection. S Sheynin, S Benaim, L Wolf, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)10Sheynin, S., Benaim, S., Wolf, L.: A hierarchical transformation-discriminating generative model for few shot anomaly detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 8495-8504 (2021) 2, 3, 4, 7, 8, 9, 10\n\nPrototypical networks for few-shot learning. Advances in neural information processing systems (NeurIPS). J Snell, K Swersky, R Zemel, 4Snell, J., Swersky, K., Zemel, R.: Prototypical networks for few-shot learning. Ad- vances in neural information processing systems (NeurIPS) 30 (2017) 4\n\nLearning to compare: Relation network for few-shot learning. F Sung, Y Yang, L Zhang, T Xiang, P H Torr, T M Hospedales, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)4Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.H., Hospedales, T.M.: Learning to compare: Relation network for few-shot learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1199-1208 (2018) 4\n\nLearning unsupervised metaformer for anomaly detection. J C Wu, D J Chen, C S Fuh, T L Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)119Wu, J.C., Chen, D.J., Fuh, C.S., Liu, T.L.: Learning unsupervised metaformer for anomaly detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 4369-4378 (2021) 1, 4, 10, 19\n\nLearning discriminative reconstructions for unsupervised outlier removal. Y Xia, X Cao, F Wen, G Hua, J Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)4Xia, Y., Cao, X., Wen, F., Hua, G., Sun, J.: Learning discriminative reconstructions for unsupervised outlier removal. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 1511-1519 (2015) 4\n\nFree lunch for few-shot learning: Distribution calibration. S Yang, L Liu, M Xu, International Conference on Learning Representations (ICLR. 4Yang, S., Liu, L., Xu, M.: Free lunch for few-shot learning: Distribution calibration. In: International Conference on Learning Representations (ICLR) (2021) 4\n\nAttribute restoration framework for anomaly detection. F Ye, C Huang, J Cao, M Li, Y Zhang, C Lu, IEEE Transactions on Multimedia. 24112022) 1, 4, 8Ye, F., Huang, C., Cao, J., Li, M., Zhang, Y., Lu, C.: Attribute restoration frame- work for anomaly detection. IEEE Transactions on Multimedia 24, 116-127 (2022) 1, 4, 8, 11\n\nPatch svdd: Patch-level svdd for anomaly detection and segmentation. J Yi, S Yoon, Proceedings of the Asian Conference on Computer Vision (ACCV) (2020). the Asian Conference on Computer Vision (ACCV) (2020)14Yi, J., Yoon, S.: Patch svdd: Patch-level svdd for anomaly detection and segmenta- tion. In: Proceedings of the Asian Conference on Computer Vision (ACCV) (2020) 1, 4\n\nViral pneumonia screening on chest x-ray images using confidence-aware anomaly detection. J Zhang, Y Xie, Z Liao, G Pang, J Verjans, W Li, Z Sun, J He, Yi Li, C S , IEEE transactions on medical imaging. 4031Zhang, J., Xie, Y., Liao, Z., Pang, G., Verjans, J., Li, W., Sun, Z., He, J., Yi Li, C.S.: Viral pneumonia screening on chest x-ray images using confidence-aware anomaly detection. IEEE transactions on medical imaging 40(3), 879-890 (2021) 1\n\nFocus your distribution: Coarse-to-fine non-contrastive learning for anomaly detection and localization. Y Zheng, X Wang, R Deng, T Bao, R Zhao, L Wu, arXiv:2110.04538811arXiv preprintZheng, Y., Wang, X., Deng, R., Bao, T., Zhao, R., Wu, L.: Focus your distribu- tion: Coarse-to-fine non-contrastive learning for anomaly detection and localiza- tion. arXiv preprint arXiv:2110.04538 (2021) 1, 4, 6, 8, 11\n\nImage registration methods: A survey. B Zitov\u00e1, J Flusser, Image and Vision Computing. 21113Zitov\u00e1, B., Flusser, J.: Image registration methods: A survey. Image and Vision Computing 21(11), 977-1000 (2003) 3\n\nDeep autoencoding gaussian mixture model for unsupervised anomaly detection. B Zong, Q Song, M R Min, W Cheng, C Lumezanu, International Conference on Learning Representations (ICLR). 14Zong, B., Song, Q., Min, M.R., Cheng, W., Lumezanu, C., et al.: Deep autoencod- ing gaussian mixture model for unsupervised anomaly detection. In: International Conference on Learning Representations (ICLR) (2018) 1, 4\n", "annotations": {"author": "[{\"end\":240,\"start\":49},{\"end\":299,\"start\":241},{\"end\":383,\"start\":300},{\"end\":530,\"start\":384},{\"end\":601,\"start\":531},{\"end\":757,\"start\":602}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":57},{\"end\":252,\"start\":248},{\"end\":311,\"start\":306},{\"end\":392,\"start\":387},{\"end\":548,\"start\":539},{\"end\":615,\"start\":611}]", "author_first_name": "[{\"end\":56,\"start\":49},{\"end\":247,\"start\":241},{\"end\":305,\"start\":300},{\"end\":386,\"start\":384},{\"end\":538,\"start\":531},{\"end\":610,\"start\":602}]", "author_affiliation": "[{\"end\":158,\"start\":89},{\"end\":204,\"start\":160},{\"end\":239,\"start\":206},{\"end\":298,\"start\":276},{\"end\":382,\"start\":313},{\"end\":483,\"start\":414},{\"end\":529,\"start\":485},{\"end\":600,\"start\":578},{\"end\":710,\"start\":641},{\"end\":756,\"start\":712}]", "title": "[{\"end\":46,\"start\":1},{\"end\":803,\"start\":758}]", "venue": null, "abstract": "[{\"end\":2032,\"start\":851}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2135,\"start\":2131},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2159,\"start\":2155},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2188,\"start\":2184},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2669,\"start\":2665},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2672,\"start\":2669},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2675,\"start\":2672},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2696,\"start\":2692},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2699,\"start\":2696},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2702,\"start\":2699},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2705,\"start\":2702},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2745,\"start\":2741},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2748,\"start\":2745},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2751,\"start\":2748},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2754,\"start\":2751},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3729,\"start\":3725},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3732,\"start\":3729},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4130,\"start\":4126},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4197,\"start\":4193},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5005,\"start\":5002},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5008,\"start\":5005},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5011,\"start\":5008},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5550,\"start\":5547},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5594,\"start\":5590},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5764,\"start\":5760},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6676,\"start\":6673},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7055,\"start\":7052},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7069,\"start\":7065},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7168,\"start\":7164},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7171,\"start\":7168},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8139,\"start\":8136},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8142,\"start\":8139},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8145,\"start\":8142},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8148,\"start\":8145},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8332,\"start\":8328},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8335,\"start\":8332},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8338,\"start\":8335},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8341,\"start\":8338},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8343,\"start\":8341},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8346,\"start\":8343},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8349,\"start\":8346},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8352,\"start\":8349},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8411,\"start\":8407},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8414,\"start\":8411},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8417,\"start\":8414},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8463,\"start\":8459},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8548,\"start\":8544},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8857,\"start\":8854},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8931,\"start\":8927},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8959,\"start\":8955},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8961,\"start\":8959},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8964,\"start\":8961},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8967,\"start\":8964},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8970,\"start\":8967},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9068,\"start\":9064},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9418,\"start\":9414},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9421,\"start\":9418},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9424,\"start\":9421},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9555,\"start\":9551},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9558,\"start\":9555},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9560,\"start\":9558},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9660,\"start\":9656},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9663,\"start\":9660},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10186,\"start\":10182},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10595,\"start\":10591},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10830,\"start\":10826},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10914,\"start\":10911},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13681,\"start\":13677},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14033,\"start\":14029},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14180,\"start\":14176},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14733,\"start\":14729},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14879,\"start\":14876},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15060,\"start\":15057},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15717,\"start\":15714},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16129,\"start\":16126},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16388,\"start\":16385},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17029,\"start\":17025},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17242,\"start\":17239},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18371,\"start\":18367},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18388,\"start\":18384},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18946,\"start\":18942},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19988,\"start\":19984},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19991,\"start\":19988},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20217,\"start\":20214},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20220,\"start\":20217},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20354,\"start\":20350},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20371,\"start\":20367},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21118,\"start\":21115},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21130,\"start\":21126},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21140,\"start\":21136},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21155,\"start\":21151},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21165,\"start\":21161},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21176,\"start\":21173},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21192,\"start\":21188},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21209,\"start\":21205},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22068,\"start\":22064},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23117,\"start\":23113},{\"end\":23665,\"start\":23658},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23902,\"start\":23898},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23960,\"start\":23957},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25764,\"start\":25761},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25776,\"start\":25772},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25786,\"start\":25782},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25801,\"start\":25797},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25811,\"start\":25807},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25822,\"start\":25819},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25838,\"start\":25834},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25855,\"start\":25851},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30056,\"start\":30052},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33811,\"start\":33807},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":40515,\"start\":40511}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35531,\"start\":34421},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35931,\"start\":35532},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36326,\"start\":35932},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36946,\"start\":36327},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37902,\"start\":36947},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38730,\"start\":37903},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39716,\"start\":38731},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40258,\"start\":39717},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":40477,\"start\":40259},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41522,\"start\":40478}]", "paragraph": "[{\"end\":3357,\"start\":2048},{\"end\":4334,\"start\":3359},{\"end\":6914,\"start\":4336},{\"end\":7349,\"start\":6916},{\"end\":7413,\"start\":7351},{\"end\":7881,\"start\":7415},{\"end\":9181,\"start\":7918},{\"end\":10039,\"start\":9203},{\"end\":11587,\"start\":10070},{\"end\":12403,\"start\":11751},{\"end\":12949,\"start\":12405},{\"end\":13504,\"start\":12960},{\"end\":14278,\"start\":13537},{\"end\":14734,\"start\":14393},{\"end\":15397,\"start\":14736},{\"end\":15773,\"start\":15453},{\"end\":16668,\"start\":15818},{\"end\":17468,\"start\":16703},{\"end\":17983,\"start\":17649},{\"end\":18270,\"start\":18044},{\"end\":19255,\"start\":18272},{\"end\":19503,\"start\":19269},{\"end\":20096,\"start\":19560},{\"end\":21984,\"start\":20134},{\"end\":22577,\"start\":22021},{\"end\":23999,\"start\":22622},{\"end\":25474,\"start\":24001},{\"end\":26211,\"start\":25476},{\"end\":26796,\"start\":26232},{\"end\":27503,\"start\":26808},{\"end\":27937,\"start\":27505},{\"end\":28622,\"start\":27939},{\"end\":29473,\"start\":28624},{\"end\":30453,\"start\":29500},{\"end\":31299,\"start\":30468},{\"end\":32156,\"start\":31324},{\"end\":32724,\"start\":32208},{\"end\":34404,\"start\":32779}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11732,\"start\":11616},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14392,\"start\":14279},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15452,\"start\":15398},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15817,\"start\":15774},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17648,\"start\":17469},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18043,\"start\":17984},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19559,\"start\":19504}]", "table_ref": "[{\"end\":21343,\"start\":21336},{\"end\":22851,\"start\":22844},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22863,\"start\":22856},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24355,\"start\":24348},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24605,\"start\":24598},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25875,\"start\":25868},{\"end\":26388,\"start\":26381},{\"end\":26901,\"start\":26894},{\"end\":27118,\"start\":27111},{\"end\":27805,\"start\":27798},{\"end\":28120,\"start\":28113},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":28757,\"start\":28750},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32520,\"start\":32513},{\"end\":32888,\"start\":32881},{\"end\":33247,\"start\":33240}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2046,\"start\":2034},{\"attributes\":{\"n\":\"2\"},\"end\":7896,\"start\":7884},{\"attributes\":{\"n\":\"2.1\"},\"end\":7916,\"start\":7899},{\"attributes\":{\"n\":\"2.2\"},\"end\":9201,\"start\":9184},{\"attributes\":{\"n\":\"2.3\"},\"end\":10068,\"start\":10042},{\"end\":11615,\"start\":11590},{\"attributes\":{\"n\":\"3\"},\"end\":11749,\"start\":11734},{\"attributes\":{\"n\":\"4\"},\"end\":12958,\"start\":12952},{\"attributes\":{\"n\":\"4.1\"},\"end\":13535,\"start\":13507},{\"attributes\":{\"n\":\"4.2\"},\"end\":16701,\"start\":16671},{\"attributes\":{\"n\":\"4.3\"},\"end\":19267,\"start\":19258},{\"attributes\":{\"n\":\"5\"},\"end\":20110,\"start\":20099},{\"attributes\":{\"n\":\"5.1\"},\"end\":20132,\"start\":20113},{\"end\":22019,\"start\":21987},{\"attributes\":{\"n\":\"5.2\"},\"end\":22620,\"start\":22580},{\"attributes\":{\"n\":\"5.3\"},\"end\":26230,\"start\":26214},{\"end\":26806,\"start\":26799},{\"attributes\":{\"n\":\"5.4\"},\"end\":29498,\"start\":29476},{\"attributes\":{\"n\":\"6\"},\"end\":30466,\"start\":30456},{\"end\":31322,\"start\":31302},{\"end\":32172,\"start\":32159},{\"end\":32206,\"start\":32175},{\"end\":32777,\"start\":32727},{\"end\":34420,\"start\":34407},{\"end\":34423,\"start\":34422},{\"end\":35541,\"start\":35533},{\"end\":35941,\"start\":35933},{\"end\":36957,\"start\":36948},{\"end\":37913,\"start\":37904},{\"end\":38741,\"start\":38732},{\"end\":39727,\"start\":39718},{\"end\":40269,\"start\":40260}]", "table": "[{\"end\":36946,\"start\":36459},{\"end\":37902,\"start\":37272},{\"end\":38730,\"start\":37984},{\"end\":39716,\"start\":38975},{\"end\":40258,\"start\":39737},{\"end\":40477,\"start\":40318},{\"end\":41522,\"start\":40516}]", "figure_caption": "[{\"end\":35531,\"start\":34424},{\"end\":35931,\"start\":35543},{\"end\":36326,\"start\":35943},{\"end\":36459,\"start\":36329},{\"end\":37272,\"start\":36959},{\"end\":37984,\"start\":37915},{\"end\":38975,\"start\":38743},{\"end\":39737,\"start\":39729},{\"end\":40318,\"start\":40271},{\"end\":40516,\"start\":40480}]", "figure_ref": "[{\"end\":2761,\"start\":2755},{\"end\":3457,\"start\":3447},{\"end\":3938,\"start\":3928},{\"end\":5264,\"start\":5258},{\"end\":5653,\"start\":5647},{\"end\":11268,\"start\":11262},{\"end\":13753,\"start\":13747},{\"end\":15246,\"start\":15240},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29733,\"start\":29727},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30131,\"start\":30125}]", "bib_author_first_name": "[{\"end\":41595,\"start\":41594},{\"end\":41604,\"start\":41603},{\"end\":41626,\"start\":41625},{\"end\":41628,\"start\":41627},{\"end\":42065,\"start\":42064},{\"end\":42077,\"start\":42076},{\"end\":42087,\"start\":42086},{\"end\":42101,\"start\":42100},{\"end\":42596,\"start\":42595},{\"end\":42607,\"start\":42606},{\"end\":42616,\"start\":42615},{\"end\":42625,\"start\":42624},{\"end\":42638,\"start\":42637},{\"end\":42942,\"start\":42941},{\"end\":42944,\"start\":42943},{\"end\":43147,\"start\":43146},{\"end\":43155,\"start\":43154},{\"end\":43584,\"start\":43583},{\"end\":43592,\"start\":43591},{\"end\":43598,\"start\":43597},{\"end\":43607,\"start\":43606},{\"end\":43609,\"start\":43608},{\"end\":43618,\"start\":43617},{\"end\":43625,\"start\":43624},{\"end\":43919,\"start\":43918},{\"end\":43921,\"start\":43920},{\"end\":43930,\"start\":43929},{\"end\":43932,\"start\":43931},{\"end\":43941,\"start\":43940},{\"end\":43950,\"start\":43949},{\"end\":43952,\"start\":43951},{\"end\":43960,\"start\":43959},{\"end\":43962,\"start\":43961},{\"end\":44324,\"start\":44323},{\"end\":44334,\"start\":44333},{\"end\":44344,\"start\":44343},{\"end\":44354,\"start\":44353},{\"end\":44876,\"start\":44875},{\"end\":45163,\"start\":45162},{\"end\":45174,\"start\":45173},{\"end\":45185,\"start\":45184},{\"end\":45198,\"start\":45197},{\"end\":45204,\"start\":45203},{\"end\":45215,\"start\":45214},{\"end\":45223,\"start\":45222},{\"end\":45234,\"start\":45233},{\"end\":45243,\"start\":45242},{\"end\":45776,\"start\":45775},{\"end\":45784,\"start\":45783},{\"end\":45794,\"start\":45793},{\"end\":46096,\"start\":46095},{\"end\":46105,\"start\":46104},{\"end\":46458,\"start\":46457},{\"end\":46466,\"start\":46465},{\"end\":46473,\"start\":46472},{\"end\":46479,\"start\":46478},{\"end\":46487,\"start\":46486},{\"end\":46489,\"start\":46488},{\"end\":46500,\"start\":46499},{\"end\":46513,\"start\":46512},{\"end\":46515,\"start\":46514},{\"end\":47080,\"start\":47079},{\"end\":47093,\"start\":47092},{\"end\":47105,\"start\":47104},{\"end\":47620,\"start\":47619},{\"end\":47626,\"start\":47625},{\"end\":47634,\"start\":47633},{\"end\":47641,\"start\":47640},{\"end\":47647,\"start\":47646},{\"end\":47984,\"start\":47983},{\"end\":47990,\"start\":47989},{\"end\":47999,\"start\":47998},{\"end\":48006,\"start\":48005},{\"end\":48457,\"start\":48456},{\"end\":48466,\"start\":48465},{\"end\":48472,\"start\":48471},{\"end\":48480,\"start\":48479},{\"end\":48488,\"start\":48487},{\"end\":48753,\"start\":48752},{\"end\":48762,\"start\":48761},{\"end\":48768,\"start\":48767},{\"end\":48776,\"start\":48775},{\"end\":48785,\"start\":48784},{\"end\":48793,\"start\":48792},{\"end\":49030,\"start\":49029},{\"end\":49043,\"start\":49042},{\"end\":49055,\"start\":49054},{\"end\":49413,\"start\":49412},{\"end\":49422,\"start\":49421},{\"end\":49431,\"start\":49430},{\"end\":49441,\"start\":49440},{\"end\":49451,\"start\":49450},{\"end\":49939,\"start\":49938},{\"end\":49941,\"start\":49940},{\"end\":49947,\"start\":49946},{\"end\":49955,\"start\":49954},{\"end\":49963,\"start\":49962},{\"end\":50483,\"start\":50482},{\"end\":50490,\"start\":50489},{\"end\":50497,\"start\":50496},{\"end\":50504,\"start\":50503},{\"end\":50511,\"start\":50510},{\"end\":50972,\"start\":50971},{\"end\":50976,\"start\":50973},{\"end\":50986,\"start\":50985},{\"end\":51240,\"start\":51239},{\"end\":51253,\"start\":51252},{\"end\":51266,\"start\":51265},{\"end\":51626,\"start\":51625},{\"end\":51634,\"start\":51633},{\"end\":51643,\"start\":51642},{\"end\":51651,\"start\":51650},{\"end\":51657,\"start\":51656},{\"end\":51667,\"start\":51666},{\"end\":51669,\"start\":51668},{\"end\":51678,\"start\":51677},{\"end\":51680,\"start\":51679},{\"end\":51689,\"start\":51688},{\"end\":51691,\"start\":51690},{\"end\":51986,\"start\":51985},{\"end\":51997,\"start\":51996},{\"end\":51999,\"start\":51998},{\"end\":52265,\"start\":52264},{\"end\":52273,\"start\":52272},{\"end\":52551,\"start\":52550},{\"end\":52559,\"start\":52558},{\"end\":52569,\"start\":52568},{\"end\":52579,\"start\":52578},{\"end\":52592,\"start\":52591},{\"end\":52600,\"start\":52599},{\"end\":53115,\"start\":53114},{\"end\":53126,\"start\":53125},{\"end\":53135,\"start\":53134},{\"end\":53599,\"start\":53598},{\"end\":53607,\"start\":53606},{\"end\":53623,\"start\":53622},{\"end\":53635,\"start\":53634},{\"end\":53645,\"start\":53644},{\"end\":53647,\"start\":53646},{\"end\":53659,\"start\":53658},{\"end\":53669,\"start\":53668},{\"end\":53679,\"start\":53678},{\"end\":54014,\"start\":54013},{\"end\":54029,\"start\":54028},{\"end\":54037,\"start\":54036},{\"end\":54043,\"start\":54042},{\"end\":54053,\"start\":54052},{\"end\":54065,\"start\":54064},{\"end\":54071,\"start\":54070},{\"end\":54080,\"start\":54079},{\"end\":54092,\"start\":54091},{\"end\":54102,\"start\":54101},{\"end\":54477,\"start\":54476},{\"end\":54489,\"start\":54488},{\"end\":54501,\"start\":54500},{\"end\":54510,\"start\":54509},{\"end\":54979,\"start\":54978},{\"end\":54989,\"start\":54988},{\"end\":55000,\"start\":54999},{\"end\":55015,\"start\":55014},{\"end\":55017,\"start\":55016},{\"end\":55027,\"start\":55026},{\"end\":55029,\"start\":55028},{\"end\":55576,\"start\":55575},{\"end\":55587,\"start\":55586},{\"end\":55598,\"start\":55597},{\"end\":55600,\"start\":55599},{\"end\":55613,\"start\":55612},{\"end\":55632,\"start\":55631},{\"end\":56061,\"start\":56060},{\"end\":56074,\"start\":56073},{\"end\":56076,\"start\":56075},{\"end\":56085,\"start\":56084},{\"end\":56101,\"start\":56100},{\"end\":56103,\"start\":56102},{\"end\":56112,\"start\":56111},{\"end\":56114,\"start\":56113},{\"end\":56433,\"start\":56432},{\"end\":56444,\"start\":56443},{\"end\":56454,\"start\":56453},{\"end\":56969,\"start\":56968},{\"end\":56978,\"start\":56977},{\"end\":56989,\"start\":56988},{\"end\":57215,\"start\":57214},{\"end\":57223,\"start\":57222},{\"end\":57231,\"start\":57230},{\"end\":57240,\"start\":57239},{\"end\":57249,\"start\":57248},{\"end\":57251,\"start\":57250},{\"end\":57259,\"start\":57258},{\"end\":57261,\"start\":57260},{\"end\":57746,\"start\":57745},{\"end\":57748,\"start\":57747},{\"end\":57754,\"start\":57753},{\"end\":57756,\"start\":57755},{\"end\":57764,\"start\":57763},{\"end\":57766,\"start\":57765},{\"end\":57773,\"start\":57772},{\"end\":57775,\"start\":57774},{\"end\":58221,\"start\":58220},{\"end\":58228,\"start\":58227},{\"end\":58235,\"start\":58234},{\"end\":58242,\"start\":58241},{\"end\":58249,\"start\":58248},{\"end\":58687,\"start\":58686},{\"end\":58695,\"start\":58694},{\"end\":58702,\"start\":58701},{\"end\":58985,\"start\":58984},{\"end\":58991,\"start\":58990},{\"end\":59000,\"start\":58999},{\"end\":59007,\"start\":59006},{\"end\":59013,\"start\":59012},{\"end\":59022,\"start\":59021},{\"end\":59323,\"start\":59322},{\"end\":59329,\"start\":59328},{\"end\":59720,\"start\":59719},{\"end\":59729,\"start\":59728},{\"end\":59736,\"start\":59735},{\"end\":59744,\"start\":59743},{\"end\":59752,\"start\":59751},{\"end\":59763,\"start\":59762},{\"end\":59769,\"start\":59768},{\"end\":59776,\"start\":59775},{\"end\":59783,\"start\":59781},{\"end\":59789,\"start\":59788},{\"end\":59791,\"start\":59790},{\"end\":60185,\"start\":60184},{\"end\":60194,\"start\":60193},{\"end\":60202,\"start\":60201},{\"end\":60210,\"start\":60209},{\"end\":60217,\"start\":60216},{\"end\":60225,\"start\":60224},{\"end\":60524,\"start\":60523},{\"end\":60534,\"start\":60533},{\"end\":60772,\"start\":60771},{\"end\":60780,\"start\":60779},{\"end\":60788,\"start\":60787},{\"end\":60790,\"start\":60789},{\"end\":60797,\"start\":60796},{\"end\":60806,\"start\":60805}]", "bib_author_last_name": "[{\"end\":41601,\"start\":41596},{\"end\":41623,\"start\":41605},{\"end\":41636,\"start\":41629},{\"end\":42074,\"start\":42066},{\"end\":42084,\"start\":42078},{\"end\":42098,\"start\":42088},{\"end\":42108,\"start\":42102},{\"end\":42604,\"start\":42597},{\"end\":42613,\"start\":42608},{\"end\":42622,\"start\":42617},{\"end\":42635,\"start\":42626},{\"end\":42643,\"start\":42639},{\"end\":42950,\"start\":42945},{\"end\":43152,\"start\":43148},{\"end\":43158,\"start\":43156},{\"end\":43589,\"start\":43585},{\"end\":43595,\"start\":43593},{\"end\":43604,\"start\":43599},{\"end\":43615,\"start\":43610},{\"end\":43622,\"start\":43619},{\"end\":43631,\"start\":43626},{\"end\":43927,\"start\":43922},{\"end\":43938,\"start\":43933},{\"end\":43947,\"start\":43942},{\"end\":43957,\"start\":43953},{\"end\":43965,\"start\":43963},{\"end\":44331,\"start\":44325},{\"end\":44341,\"start\":44335},{\"end\":44351,\"start\":44345},{\"end\":44363,\"start\":44355},{\"end\":44882,\"start\":44877},{\"end\":45171,\"start\":45164},{\"end\":45182,\"start\":45175},{\"end\":45195,\"start\":45186},{\"end\":45201,\"start\":45199},{\"end\":45212,\"start\":45205},{\"end\":45220,\"start\":45216},{\"end\":45231,\"start\":45224},{\"end\":45240,\"start\":45235},{\"end\":45248,\"start\":45244},{\"end\":45781,\"start\":45777},{\"end\":45791,\"start\":45785},{\"end\":45801,\"start\":45795},{\"end\":46102,\"start\":46097},{\"end\":46114,\"start\":46106},{\"end\":46463,\"start\":46459},{\"end\":46470,\"start\":46467},{\"end\":46476,\"start\":46474},{\"end\":46484,\"start\":46480},{\"end\":46497,\"start\":46490},{\"end\":46510,\"start\":46501},{\"end\":46522,\"start\":46516},{\"end\":47090,\"start\":47081},{\"end\":47102,\"start\":47094},{\"end\":47112,\"start\":47106},{\"end\":47623,\"start\":47621},{\"end\":47631,\"start\":47627},{\"end\":47638,\"start\":47635},{\"end\":47644,\"start\":47642},{\"end\":47652,\"start\":47648},{\"end\":47987,\"start\":47985},{\"end\":47996,\"start\":47991},{\"end\":48003,\"start\":48000},{\"end\":48010,\"start\":48007},{\"end\":48463,\"start\":48458},{\"end\":48469,\"start\":48467},{\"end\":48477,\"start\":48473},{\"end\":48485,\"start\":48481},{\"end\":48494,\"start\":48489},{\"end\":48759,\"start\":48754},{\"end\":48765,\"start\":48763},{\"end\":48773,\"start\":48769},{\"end\":48782,\"start\":48777},{\"end\":48790,\"start\":48786},{\"end\":48798,\"start\":48794},{\"end\":49040,\"start\":49031},{\"end\":49052,\"start\":49044},{\"end\":49065,\"start\":49056},{\"end\":49419,\"start\":49414},{\"end\":49428,\"start\":49423},{\"end\":49438,\"start\":49432},{\"end\":49448,\"start\":49442},{\"end\":49458,\"start\":49452},{\"end\":49944,\"start\":49942},{\"end\":49952,\"start\":49948},{\"end\":49960,\"start\":49956},{\"end\":49971,\"start\":49964},{\"end\":50487,\"start\":50484},{\"end\":50494,\"start\":50491},{\"end\":50501,\"start\":50498},{\"end\":50508,\"start\":50505},{\"end\":50514,\"start\":50512},{\"end\":50983,\"start\":50977},{\"end\":50993,\"start\":50987},{\"end\":51250,\"start\":51241},{\"end\":51263,\"start\":51254},{\"end\":51273,\"start\":51267},{\"end\":51631,\"start\":51627},{\"end\":51640,\"start\":51635},{\"end\":51648,\"start\":51644},{\"end\":51654,\"start\":51652},{\"end\":51664,\"start\":51658},{\"end\":51675,\"start\":51670},{\"end\":51686,\"start\":51681},{\"end\":51699,\"start\":51692},{\"end\":51994,\"start\":51987},{\"end\":52004,\"start\":52000},{\"end\":52270,\"start\":52266},{\"end\":52284,\"start\":52274},{\"end\":52556,\"start\":52552},{\"end\":52566,\"start\":52560},{\"end\":52576,\"start\":52570},{\"end\":52589,\"start\":52580},{\"end\":52597,\"start\":52593},{\"end\":52607,\"start\":52601},{\"end\":53123,\"start\":53116},{\"end\":53132,\"start\":53127},{\"end\":53145,\"start\":53136},{\"end\":53604,\"start\":53600},{\"end\":53620,\"start\":53608},{\"end\":53632,\"start\":53624},{\"end\":53642,\"start\":53636},{\"end\":53656,\"start\":53648},{\"end\":53666,\"start\":53660},{\"end\":53676,\"start\":53670},{\"end\":53685,\"start\":53680},{\"end\":54026,\"start\":54015},{\"end\":54034,\"start\":54030},{\"end\":54040,\"start\":54038},{\"end\":54050,\"start\":54044},{\"end\":54062,\"start\":54054},{\"end\":54068,\"start\":54066},{\"end\":54077,\"start\":54072},{\"end\":54089,\"start\":54081},{\"end\":54099,\"start\":54093},{\"end\":54112,\"start\":54103},{\"end\":54486,\"start\":54478},{\"end\":54498,\"start\":54490},{\"end\":54507,\"start\":54502},{\"end\":54516,\"start\":54511},{\"end\":54986,\"start\":54980},{\"end\":54997,\"start\":54990},{\"end\":55012,\"start\":55001},{\"end\":55024,\"start\":55018},{\"end\":55036,\"start\":55030},{\"end\":55584,\"start\":55577},{\"end\":55595,\"start\":55588},{\"end\":55610,\"start\":55601},{\"end\":55629,\"start\":55614},{\"end\":55638,\"start\":55633},{\"end\":56071,\"start\":56062},{\"end\":56082,\"start\":56077},{\"end\":56098,\"start\":56086},{\"end\":56109,\"start\":56104},{\"end\":56125,\"start\":56115},{\"end\":56441,\"start\":56434},{\"end\":56451,\"start\":56445},{\"end\":56459,\"start\":56455},{\"end\":56975,\"start\":56970},{\"end\":56986,\"start\":56979},{\"end\":56995,\"start\":56990},{\"end\":57220,\"start\":57216},{\"end\":57228,\"start\":57224},{\"end\":57237,\"start\":57232},{\"end\":57246,\"start\":57241},{\"end\":57256,\"start\":57252},{\"end\":57272,\"start\":57262},{\"end\":57751,\"start\":57749},{\"end\":57761,\"start\":57757},{\"end\":57770,\"start\":57767},{\"end\":57779,\"start\":57776},{\"end\":58225,\"start\":58222},{\"end\":58232,\"start\":58229},{\"end\":58239,\"start\":58236},{\"end\":58246,\"start\":58243},{\"end\":58253,\"start\":58250},{\"end\":58692,\"start\":58688},{\"end\":58699,\"start\":58696},{\"end\":58705,\"start\":58703},{\"end\":58988,\"start\":58986},{\"end\":58997,\"start\":58992},{\"end\":59004,\"start\":59001},{\"end\":59010,\"start\":59008},{\"end\":59019,\"start\":59014},{\"end\":59025,\"start\":59023},{\"end\":59326,\"start\":59324},{\"end\":59334,\"start\":59330},{\"end\":59726,\"start\":59721},{\"end\":59733,\"start\":59730},{\"end\":59741,\"start\":59737},{\"end\":59749,\"start\":59745},{\"end\":59760,\"start\":59753},{\"end\":59766,\"start\":59764},{\"end\":59773,\"start\":59770},{\"end\":59779,\"start\":59777},{\"end\":59786,\"start\":59784},{\"end\":60191,\"start\":60186},{\"end\":60199,\"start\":60195},{\"end\":60207,\"start\":60203},{\"end\":60214,\"start\":60211},{\"end\":60222,\"start\":60218},{\"end\":60228,\"start\":60226},{\"end\":60531,\"start\":60525},{\"end\":60542,\"start\":60535},{\"end\":60777,\"start\":60773},{\"end\":60785,\"start\":60781},{\"end\":60794,\"start\":60791},{\"end\":60803,\"start\":60798},{\"end\":60815,\"start\":60807}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":21688963},\"end\":41982,\"start\":41524},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":189857704},\"end\":42527,\"start\":41984},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16394033},\"end\":42896,\"start\":42529},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14576088},\"end\":43094,\"start\":42898},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":227118869},\"end\":43516,\"start\":43096},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52908669},\"end\":43868,\"start\":43518},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2329405},\"end\":44234,\"start\":43870},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":226976039},\"end\":44798,\"start\":44236},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":38939287},\"end\":45090,\"start\":44800},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":29162614},\"end\":45706,\"start\":45092},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6719686},\"end\":46037,\"start\":45708},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":44108048},\"end\":46345,\"start\":46039},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":102353587},\"end\":46973,\"start\":46347},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236447794},\"end\":47546,\"start\":46975},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":232417809},\"end\":47935,\"start\":47548},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":48377,\"start\":47937},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":248798631},\"end\":48699,\"start\":48379},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":249892541},\"end\":49027,\"start\":48701},{\"attributes\":{\"id\":\"b18\"},\"end\":49307,\"start\":49029},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":245147304},\"end\":49861,\"start\":49309},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233204792},\"end\":50383,\"start\":49863},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":211296758},\"end\":50939,\"start\":50385},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5855042},\"end\":51150,\"start\":50941},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":49863126},\"end\":51563,\"start\":51152},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1605275},\"end\":51909,\"start\":51565},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2024650},\"end\":52215,\"start\":51911},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":67413369},\"end\":52494,\"start\":52217},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":235436036},\"end\":53030,\"start\":52496},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":221370646},\"end\":53565,\"start\":53032},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":49312162},\"end\":53960,\"start\":53567},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2930547},\"end\":54408,\"start\":53962},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3509717},\"end\":54914,\"start\":54410},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":227126845},\"end\":55478,\"start\":54916},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":17427022},\"end\":55999,\"start\":55480},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2110475},\"end\":56336,\"start\":56001},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":233444033},\"end\":56860,\"start\":56338},{\"attributes\":{\"id\":\"b36\"},\"end\":57151,\"start\":56862},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4412459},\"end\":57687,\"start\":57153},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":244100442},\"end\":58144,\"start\":57689},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13982294},\"end\":58624,\"start\":58146},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":231632202},\"end\":58927,\"start\":58626},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":219687613},\"end\":59251,\"start\":58929},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220250825},\"end\":59627,\"start\":59253},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":220250774},\"end\":60077,\"start\":59629},{\"attributes\":{\"doi\":\"arXiv:2110.04538\",\"id\":\"b44\"},\"end\":60483,\"start\":60079},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":206051485},\"end\":60692,\"start\":60485},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":51805340},\"end\":61098,\"start\":60694}]", "bib_title": "[{\"end\":41592,\"start\":41524},{\"end\":42062,\"start\":41984},{\"end\":42593,\"start\":42529},{\"end\":42939,\"start\":42898},{\"end\":43144,\"start\":43096},{\"end\":43581,\"start\":43518},{\"end\":43916,\"start\":43870},{\"end\":44321,\"start\":44236},{\"end\":44873,\"start\":44800},{\"end\":45160,\"start\":45092},{\"end\":45773,\"start\":45708},{\"end\":46093,\"start\":46039},{\"end\":46455,\"start\":46347},{\"end\":47077,\"start\":46975},{\"end\":47617,\"start\":47548},{\"end\":47981,\"start\":47937},{\"end\":48454,\"start\":48379},{\"end\":48750,\"start\":48701},{\"end\":49410,\"start\":49309},{\"end\":49936,\"start\":49863},{\"end\":50480,\"start\":50385},{\"end\":50969,\"start\":50941},{\"end\":51237,\"start\":51152},{\"end\":51623,\"start\":51565},{\"end\":51983,\"start\":51911},{\"end\":52262,\"start\":52217},{\"end\":52548,\"start\":52496},{\"end\":53112,\"start\":53032},{\"end\":53596,\"start\":53567},{\"end\":54011,\"start\":53962},{\"end\":54474,\"start\":54410},{\"end\":54976,\"start\":54916},{\"end\":55573,\"start\":55480},{\"end\":56058,\"start\":56001},{\"end\":56430,\"start\":56338},{\"end\":57212,\"start\":57153},{\"end\":57743,\"start\":57689},{\"end\":58218,\"start\":58146},{\"end\":58684,\"start\":58626},{\"end\":58982,\"start\":58929},{\"end\":59320,\"start\":59253},{\"end\":59717,\"start\":59629},{\"end\":60521,\"start\":60485},{\"end\":60769,\"start\":60694}]", "bib_author": "[{\"end\":41603,\"start\":41594},{\"end\":41625,\"start\":41603},{\"end\":41638,\"start\":41625},{\"end\":42076,\"start\":42064},{\"end\":42086,\"start\":42076},{\"end\":42100,\"start\":42086},{\"end\":42110,\"start\":42100},{\"end\":42606,\"start\":42595},{\"end\":42615,\"start\":42606},{\"end\":42624,\"start\":42615},{\"end\":42637,\"start\":42624},{\"end\":42645,\"start\":42637},{\"end\":42952,\"start\":42941},{\"end\":43154,\"start\":43146},{\"end\":43160,\"start\":43154},{\"end\":43591,\"start\":43583},{\"end\":43597,\"start\":43591},{\"end\":43606,\"start\":43597},{\"end\":43617,\"start\":43606},{\"end\":43624,\"start\":43617},{\"end\":43633,\"start\":43624},{\"end\":43929,\"start\":43918},{\"end\":43940,\"start\":43929},{\"end\":43949,\"start\":43940},{\"end\":43959,\"start\":43949},{\"end\":43967,\"start\":43959},{\"end\":44333,\"start\":44323},{\"end\":44343,\"start\":44333},{\"end\":44353,\"start\":44343},{\"end\":44365,\"start\":44353},{\"end\":44884,\"start\":44875},{\"end\":45173,\"start\":45162},{\"end\":45184,\"start\":45173},{\"end\":45197,\"start\":45184},{\"end\":45203,\"start\":45197},{\"end\":45214,\"start\":45203},{\"end\":45222,\"start\":45214},{\"end\":45233,\"start\":45222},{\"end\":45242,\"start\":45233},{\"end\":45250,\"start\":45242},{\"end\":45783,\"start\":45775},{\"end\":45793,\"start\":45783},{\"end\":45803,\"start\":45793},{\"end\":46104,\"start\":46095},{\"end\":46116,\"start\":46104},{\"end\":46465,\"start\":46457},{\"end\":46472,\"start\":46465},{\"end\":46478,\"start\":46472},{\"end\":46486,\"start\":46478},{\"end\":46499,\"start\":46486},{\"end\":46512,\"start\":46499},{\"end\":46524,\"start\":46512},{\"end\":47092,\"start\":47079},{\"end\":47104,\"start\":47092},{\"end\":47114,\"start\":47104},{\"end\":47625,\"start\":47619},{\"end\":47633,\"start\":47625},{\"end\":47640,\"start\":47633},{\"end\":47646,\"start\":47640},{\"end\":47654,\"start\":47646},{\"end\":47989,\"start\":47983},{\"end\":47998,\"start\":47989},{\"end\":48005,\"start\":47998},{\"end\":48012,\"start\":48005},{\"end\":48465,\"start\":48456},{\"end\":48471,\"start\":48465},{\"end\":48479,\"start\":48471},{\"end\":48487,\"start\":48479},{\"end\":48496,\"start\":48487},{\"end\":48761,\"start\":48752},{\"end\":48767,\"start\":48761},{\"end\":48775,\"start\":48767},{\"end\":48784,\"start\":48775},{\"end\":48792,\"start\":48784},{\"end\":48800,\"start\":48792},{\"end\":49042,\"start\":49029},{\"end\":49054,\"start\":49042},{\"end\":49067,\"start\":49054},{\"end\":49421,\"start\":49412},{\"end\":49430,\"start\":49421},{\"end\":49440,\"start\":49430},{\"end\":49450,\"start\":49440},{\"end\":49460,\"start\":49450},{\"end\":49946,\"start\":49938},{\"end\":49954,\"start\":49946},{\"end\":49962,\"start\":49954},{\"end\":49973,\"start\":49962},{\"end\":50489,\"start\":50482},{\"end\":50496,\"start\":50489},{\"end\":50503,\"start\":50496},{\"end\":50510,\"start\":50503},{\"end\":50516,\"start\":50510},{\"end\":50985,\"start\":50971},{\"end\":50995,\"start\":50985},{\"end\":51252,\"start\":51239},{\"end\":51265,\"start\":51252},{\"end\":51275,\"start\":51265},{\"end\":51633,\"start\":51625},{\"end\":51642,\"start\":51633},{\"end\":51650,\"start\":51642},{\"end\":51656,\"start\":51650},{\"end\":51666,\"start\":51656},{\"end\":51677,\"start\":51666},{\"end\":51688,\"start\":51677},{\"end\":51701,\"start\":51688},{\"end\":51996,\"start\":51985},{\"end\":52006,\"start\":51996},{\"end\":52272,\"start\":52264},{\"end\":52286,\"start\":52272},{\"end\":52558,\"start\":52550},{\"end\":52568,\"start\":52558},{\"end\":52578,\"start\":52568},{\"end\":52591,\"start\":52578},{\"end\":52599,\"start\":52591},{\"end\":52609,\"start\":52599},{\"end\":53125,\"start\":53114},{\"end\":53134,\"start\":53125},{\"end\":53147,\"start\":53134},{\"end\":53606,\"start\":53598},{\"end\":53622,\"start\":53606},{\"end\":53634,\"start\":53622},{\"end\":53644,\"start\":53634},{\"end\":53658,\"start\":53644},{\"end\":53668,\"start\":53658},{\"end\":53678,\"start\":53668},{\"end\":53687,\"start\":53678},{\"end\":54028,\"start\":54013},{\"end\":54036,\"start\":54028},{\"end\":54042,\"start\":54036},{\"end\":54052,\"start\":54042},{\"end\":54064,\"start\":54052},{\"end\":54070,\"start\":54064},{\"end\":54079,\"start\":54070},{\"end\":54091,\"start\":54079},{\"end\":54101,\"start\":54091},{\"end\":54114,\"start\":54101},{\"end\":54488,\"start\":54476},{\"end\":54500,\"start\":54488},{\"end\":54509,\"start\":54500},{\"end\":54518,\"start\":54509},{\"end\":54988,\"start\":54978},{\"end\":54999,\"start\":54988},{\"end\":55014,\"start\":54999},{\"end\":55026,\"start\":55014},{\"end\":55038,\"start\":55026},{\"end\":55586,\"start\":55575},{\"end\":55597,\"start\":55586},{\"end\":55612,\"start\":55597},{\"end\":55631,\"start\":55612},{\"end\":55640,\"start\":55631},{\"end\":56073,\"start\":56060},{\"end\":56084,\"start\":56073},{\"end\":56100,\"start\":56084},{\"end\":56111,\"start\":56100},{\"end\":56127,\"start\":56111},{\"end\":56443,\"start\":56432},{\"end\":56453,\"start\":56443},{\"end\":56461,\"start\":56453},{\"end\":56977,\"start\":56968},{\"end\":56988,\"start\":56977},{\"end\":56997,\"start\":56988},{\"end\":57222,\"start\":57214},{\"end\":57230,\"start\":57222},{\"end\":57239,\"start\":57230},{\"end\":57248,\"start\":57239},{\"end\":57258,\"start\":57248},{\"end\":57274,\"start\":57258},{\"end\":57753,\"start\":57745},{\"end\":57763,\"start\":57753},{\"end\":57772,\"start\":57763},{\"end\":57781,\"start\":57772},{\"end\":58227,\"start\":58220},{\"end\":58234,\"start\":58227},{\"end\":58241,\"start\":58234},{\"end\":58248,\"start\":58241},{\"end\":58255,\"start\":58248},{\"end\":58694,\"start\":58686},{\"end\":58701,\"start\":58694},{\"end\":58707,\"start\":58701},{\"end\":58990,\"start\":58984},{\"end\":58999,\"start\":58990},{\"end\":59006,\"start\":58999},{\"end\":59012,\"start\":59006},{\"end\":59021,\"start\":59012},{\"end\":59027,\"start\":59021},{\"end\":59328,\"start\":59322},{\"end\":59336,\"start\":59328},{\"end\":59728,\"start\":59719},{\"end\":59735,\"start\":59728},{\"end\":59743,\"start\":59735},{\"end\":59751,\"start\":59743},{\"end\":59762,\"start\":59751},{\"end\":59768,\"start\":59762},{\"end\":59775,\"start\":59768},{\"end\":59781,\"start\":59775},{\"end\":59788,\"start\":59781},{\"end\":59794,\"start\":59788},{\"end\":60193,\"start\":60184},{\"end\":60201,\"start\":60193},{\"end\":60209,\"start\":60201},{\"end\":60216,\"start\":60209},{\"end\":60224,\"start\":60216},{\"end\":60230,\"start\":60224},{\"end\":60533,\"start\":60523},{\"end\":60544,\"start\":60533},{\"end\":60779,\"start\":60771},{\"end\":60787,\"start\":60779},{\"end\":60796,\"start\":60787},{\"end\":60805,\"start\":60796},{\"end\":60817,\"start\":60805}]", "bib_venue": "[{\"end\":41747,\"start\":41701},{\"end\":42273,\"start\":42200},{\"end\":43323,\"start\":43250},{\"end\":44516,\"start\":44449},{\"end\":45413,\"start\":45340},{\"end\":46667,\"start\":46604},{\"end\":47275,\"start\":47203},{\"end\":48175,\"start\":48102},{\"end\":50136,\"start\":50063},{\"end\":50679,\"start\":50606},{\"end\":52772,\"start\":52699},{\"end\":53308,\"start\":53236},{\"end\":54681,\"start\":54608},{\"end\":55201,\"start\":55128},{\"end\":56604,\"start\":56541},{\"end\":57437,\"start\":57364},{\"end\":57924,\"start\":57861},{\"end\":58398,\"start\":58335},{\"end\":59459,\"start\":59406},{\"end\":41699,\"start\":41638},{\"end\":42198,\"start\":42110},{\"end\":42694,\"start\":42645},{\"end\":42980,\"start\":42952},{\"end\":43248,\"start\":43160},{\"end\":43670,\"start\":43633},{\"end\":44029,\"start\":43967},{\"end\":44447,\"start\":44365},{\"end\":44935,\"start\":44884},{\"end\":45338,\"start\":45250},{\"end\":45854,\"start\":45803},{\"end\":46175,\"start\":46116},{\"end\":46602,\"start\":46524},{\"end\":47201,\"start\":47114},{\"end\":47723,\"start\":47654},{\"end\":48100,\"start\":48012},{\"end\":48527,\"start\":48496},{\"end\":48849,\"start\":48800},{\"end\":49146,\"start\":49067},{\"end\":49559,\"start\":49460},{\"end\":50061,\"start\":49973},{\"end\":50604,\"start\":50516},{\"end\":51031,\"start\":50995},{\"end\":51337,\"start\":51275},{\"end\":51715,\"start\":51701},{\"end\":52044,\"start\":52006},{\"end\":52344,\"start\":52286},{\"end\":52697,\"start\":52609},{\"end\":53234,\"start\":53147},{\"end\":53738,\"start\":53687},{\"end\":54154,\"start\":54114},{\"end\":54606,\"start\":54518},{\"end\":55126,\"start\":55038},{\"end\":55709,\"start\":55640},{\"end\":56145,\"start\":56127},{\"end\":56539,\"start\":56461},{\"end\":56966,\"start\":56862},{\"end\":57362,\"start\":57274},{\"end\":57859,\"start\":57781},{\"end\":58333,\"start\":58255},{\"end\":58765,\"start\":58707},{\"end\":59058,\"start\":59027},{\"end\":59404,\"start\":59336},{\"end\":59830,\"start\":59794},{\"end\":60182,\"start\":60079},{\"end\":60570,\"start\":60544},{\"end\":60876,\"start\":60817}]"}}}, "year": 2023, "month": 12, "day": 17}