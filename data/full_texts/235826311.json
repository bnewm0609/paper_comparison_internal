{"id": 235826311, "updated": "2022-01-11 06:31:15.516", "metadata": {"title": "Learning from Noisy Labels with No Change to the Training Process", "authors": "[{\"middle\":[],\"last\":\"Zhang\",\"first\":\"Mingyuan\"},{\"middle\":[],\"last\":\"Lee\",\"first\":\"Jane\"},{\"middle\":[],\"last\":\"Agarwal\",\"first\":\"Shivani\"}]", "venue": "ICML", "journal": "12468-12478", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "There has been much interest in recent years in developing learning algorithms that can learn accurate classifiers from data with noisy labels. A widely-studied noise model is that of classconditional noise (CCN), wherein a label y is flipped to a label \u1ef9 with some associated noise probability that depends on both y and \u1ef9. In the multiclass setting, all previously proposed algorithms under the CCN model involve changing the training process, by introducing a \u2018noisecorrection\u2019 to the surrogate loss to be minimized over the noisy training examples. In this paper, we show that this is really unnecessary: one can simply perform class probability estimation (CPE) on the noisy examples, e.g. using a standard (multiclass) logistic regression algorithm, and then apply noise-correction only in the final prediction step. This means that the training algorithm itself does not need any change, and one can simply use standard off-the-shelf implementations with no modification to the code for training. Our approach can handle general multiclass loss matrices, including the usual 0-1 loss but also other losses such as those used for ordinal regression problems. We also provide a quantitative regret transfer bound, which bounds the target regret on the true distribution in terms of the CPE regret on the noisy distribution; in doing so, we extend the notion of strong properness introduced for binary losses by Agarwal (2014) to the multiclass case. Our bound suggests that the sample complexity of learning under CCN increases as the noise matrix approaches singularity. We also provide fixes and potential improvements for noise estimation methods that involve computing anchor points. Our experiments confirm our theoretical findings. Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA Twitter, San Francisco, CA, USA (Work done while at the University of Pennsylvania). Correspondence to: Mingyuan Zhang, Shivani Agarwal <{myz,ashivani}@seas.upenn.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ZhangL021", "doi": null}}, "content": {"source": {"pdf_hash": "d0e06eb9e2ae79ed6a0d560de115e98a30e80b3d", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2b29c5f0e8ee120594b8b9719ab598af656d7482", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d0e06eb9e2ae79ed6a0d560de115e98a30e80b3d.txt", "contents": "\nLearning from Noisy Labels with No Change to the Training Process\n2021\n\nMingyuan Zhang \nDepartment of Computer and Information Science\nUniver-sity of Pennsylvania\nPhiladelphiaPAUSA\n\nJane Lee \nTwitter\nSan Fran-ciscoCAUSA\n\nShivani Agarwal \nDepartment of Computer and Information Science\nUniver-sity of Pennsylvania\nPhiladelphiaPAUSA\n\nLearning from Noisy Labels with No Change to the Training Process\n\nProceedings of the 38 th International Conference on Machine Learning\nthe 38 th International Conference on Machine Learning2021(Work done while at the University of Pennsyl-vania). Correspondence to: Mingyuan Zhang, Shivani Agarwal <{myz,ashivani}@seas.upenn.edu>.\nThere has been much interest in recent years in developing learning algorithms that can learn accurate classifiers from data with noisy labels. A widely-studied noise model is that of classconditional noise (CCN), wherein a label y is flipped to a label y with some associated noise probability that depends on both y and y. In the multiclass setting, all previously proposed algorithms under the CCN model involve changing the training process, by introducing a 'noisecorrection' to the surrogate loss to be minimized over the noisy training examples. In this paper, we show that this is really unnecessary: one can simply perform class probability estimation (CPE) on the noisy examples, e.g. using a standard (multiclass) logistic regression algorithm, and then apply noise-correction only in the final prediction step. This means that the training algorithm itself does not need any change, and one can simply use standard off-the-shelf implementations with no modification to the code for training. Our approach can handle general multiclass loss matrices, including the usual 0-1 loss but also other losses such as those used for ordinal regression problems. We also provide a quantitative regret transfer bound, which bounds the target regret on the true distribution in terms of the CPE regret on the noisy distribution; in doing so, we extend the notion of strong properness introduced for binary losses by Agarwal (2014) to the multiclass case. Our bound suggests that the sample complexity of learning under CCN increases as the noise matrix approaches singularity. We also provide fixes and potential improvements for noise estimation methods that involve computing anchor points. Our experiments confirm our theoretical findings.\n\nIntroduction\n\nIn many applications of machine learning, one receives noisy labels during training. This can happen for a variety of reasons, including human labeling errors, sensor measurement errors, distributed label collection via crowdsourcing, automatic label collection via internet crawling, and many others. Consequently, there has been much interest in recent years in developing learning algorithms that can learn accurate classifiers from data with noisy labels (Fr\u00e9nay & Verleysen, 2014;Song et al., 2020).\n\nWe focus here on the setting of label-dependent noise, where the (random) noise in a label depends on the label but not on the instance (the more general setting of label-and instancedependent noise is also of interest (Menon et al., 2018;Cheng et al., 2020), but we do not focus on that here). An early example of label-dependent noise for binary classification that has been widely studied in the PAC learning literature is the random classification noise (RCN) model, in which a binary label y is flipped to the opposite label with a fixed probability \u03b3 \u2208 [0, 1 2 ) (Angluin & Laird, 1987;Bylander, 1994;Aslam & Decatur, 1996;Kearns, 1998;Blum & Mitchell, 1998;Cesa-Bianchi et al., 1999). More recently, Natarajan et al. (2013) generalized the RCN model and proposed the class-conditional random label noise (CCN) model for binary classification, in which flip probabilities for positive and negative labels can be different. This was then extended to the more general multiclass case, wherein a label y is flipped to a label y with some noise probability that depends on y and y (van Rooyen & Williamson, 2017;Patrini et al., 2017;Ghosh et al., 2017;Wang et al., 2018).\n\nThe primary challenge in learning from noisy labels is to design algorithms which, despite being given data with noisy labels as input, can learn accurate classifiers for the true, clean distribution. In particular, it is desirable to design algorithms which, when trained using a sufficiently rich function class, are statistically consistent for the clean distribution (i.e. that converge to a Bayes optimal classifier for the clean distribution). For the general multiclass CCN model, two such algorithms have been proposed: the unbiased estimator method of van Rooyen & Williamson (2017) (which builds on a method of Natarajan et al. (2013) for binary labels), and the forward method of Patrini et al. (2017) (the 'backward' method of Patrini et al. (2017) is the same as the unbiased estimators method). Both algorithms make use of the framework of surrogate loss minimization, and both require modifying the surrogate loss to correct for the noise. In practice, this means modifying the training algorithm.\n\nIn this paper, we take a first-principles approach, and show that, for the general multiclass CCN model, one can design statistically consistent learning algorithms without modifying the training process. In particular, by examining the form of the Bayes optimal classifier for any target (costsensitive) multiclass loss, and the relation between the noisy and clean distributions over labels, we show that it suffices to simply implement a standard class probability estimation (CPE) algorithm (such as multiclass logistic regression) on the given noisy training data, and then apply a noisecorrected plug-in step at prediction time. For practitioners lacking expertise to modify the optimization process, or when retraining is a bottleneck, the post-processing step at prediction time can be easier to implement and use.\n\nTo establish consistency of our method (when trained with a sufficiently rich function class), we derive a quantitative regret transfer bound which shows that the target regret on the true, clean distribution can be upper bounded by the CPE regret on the noisy distribution. We also extend the notion of strong properness, defined for binary losses by Agarwal (2014), to multiclass surrogate losses; for CPE learners that minimize such surrogate losses (including for example the multiclass logistic/cross-entropy loss), we provide a regret bound in terms of the surrogate regret on the noisy distribution. Our bound suggests that as the noise matrix becomes closer to being singular, the sample size needed to achieve a given target performance level becomes larger.\n\nIn their basic forms, the methods of both van Rooyen & Williamson (2017) and Patrini et al. (2017), as well as our noise-corrected plug-in method, all assume that the noise flip probabilities are known. In practice, one may need to estimate the noise probabilities from the given noisy data. In recent years, a number of approaches have been proposed for estimating noise flip probabilities; these are generally based on identifying a small number of anchor points (instances that belong to a certain class with probability one). In particular, Patrini et al. (2017) proposed a noise estimation method based on anchor points, with the intent to provide an 'end-to-end' noise-estimation-and-learning method. Later, Yao et al. (2020) exploited the divide-and-conquer paradigm to propose another noise estimation method, also based on anchor points. However, it turns out that both methods do not always work correctly; we identify an error in their methods (specifically, the error is in the method for computing anchor points), and provide conditions on the noise under which the methods work or fail. We also propose an iterative noise estimation heuristic that aims to partly correct the error; while the heuristic is not guaran-teed to converge or recover the correct noise probabilities, it works well in our experiments, sometimes outperforming the methods of Patrini et al. (2017) and Yao et al. (2020). Moreover, all three noise estimation methods require a CPE model to be learned from the noisy data, which in our case comes for free, with no further training required; thus our method also provides a more efficient 'end-to-end' solution.\n\nOur experiments confirm that our noise-corrected plug-in method performs comparably to previous methods, while requiring no change to the training process.\n\nRelationship with previous work in the binary case. As noted above, the works on learning from noisy labels in multiclass classification that are most closely related to ours are those of van Rooyen & Williamson (2017) and Patrini et al. (2017). In the special case of binary classification, two works are most directly relevant: those of Natarajan et al. (2013) and Menon et al. (2015). Natarajan et al. (2013) studied the CCN model for binary classification, and expressed the Bayes optimal classifier for the noisy distribution as a plug-in rule involving the clean class probability function (Lemma 7), and then used this to reduce the CCN learning problem to a cost-sensitive classification problem on the noisy data using classification-calibrated surrogate losses. In contrast, we express the Bayes optimal classifier for the clean distribution as a plug-in rule involving the noisy class probabilities, which can be estimated directly from the noisy data (we use strongly proper composite surrogate losses for this estimation). Menon et al. (2015) studied the more general mutually contaminated distributions (MCD) noise model for binary classification, and while they focused mostly on the balanced error (BER) and area under the ROC curve (AUC) metrics, they also used strongly proper composite (binary) surrogate losses, and applied their analysis to derive a regret transfer bound for the 0-1 error as well (Proposition 7). When specialized to the CCN model, their bound for binary classification with 0-1 loss can be viewed as a special case of our bound in Theorem 4 (our bound holds for multiclass classification with general losses).\n\nOrganization. After preliminaries in Section 2, we describe our noise-corrected plug-in method in Section 3. Section 4 gives regret transfer bounds; Section 5 discusses noise estimation. Section 6 summarizes our experiments. All proofs can be found in the supplementary material.\n\nNotation. For an integer n, we denote by [n] the set of integers {1, . . . , n}, and by \u2206 n the probability simplex {p \u2208 R n + : n y=1 p y = 1}. For a vector a, we denote by a 2 the L 2 norm of a. For a matrix A, we denote by A F the Frobenius norm of A, by A 2 the induced 2norm of A (largest singular value of A), and by a y the y-th column vector of A. We use e y to denote a standard basis vector with y-th element 1.\n\n\nPreliminaries\n\nThe problem of (multiclass) learning from noisy labels can be described as follows. There is an instance space X , and a set of n class labels Y, which we will take without loss of generality to be Y = [n]. There is a (unknown) joint probability distribution D on X \u00d7 Y from which labeled examples (X, Y ) are drawn. In the standard (non-noisy) supervised learning setting, the learner would be given training examples drawn directly from D. When learning from noisy labels, however, the learner does not get clean labels Y ; instead, the learner sees noisy examples (X, Y ), where Y denotes a noisy version of Y . In particular, the learner receives a noisy training sample S = ((x 1 , y 1 ), . . . , (x m , y m )) \u2208 (X \u00d7 Y) m , and the goal is to learn a classifier h : X \u2192Y that performs well with respect to the clean distribution D.\n\nWe consider here the class-conditional random label noise (CCN) model (Natarajan et al., 2013;van Rooyen & Williamson, 2017), wherein a label y is randomly flipped to a label y with some probability \u03b3 y, y that depends on y and y. In particular, the CCN model is characterized by a row-stochastic noise matrix C \u2208 [0, 1] n\u00d7n with entries \u03b3 y, y , such that for each y, y \u2208 [n],\nP( Y = y | Y = y) = \u03b3 y, y .\nThe noisy training examples seen by the learner can therefore be viewed as being drawn IID from a 'noisy' distribution D on X \u00d7 Y, wherein an example (X, Y ) is first drawn randomly according to D, and then noise is injected according to the noise matrix C to generate (X, Y ).\n\nThus, given a noisy training sample S drawn according to the noisy distribution D as above, the goal of the learner is to learn a classifier h : X \u2192Y that performs well under the clean distribution D. To measure performance, we consider a general multiclass loss matrix L \u2208 R n\u00d7n + , with entries y, y indicating the loss incurred on predicting y when the true label is y (the 0-1 loss L 0-1 with 0-1 y, y = 1( y = y) is a special case). The performance of the classifier h is then measured by the L-generalization error or L-risk under D:\ner L D [h] = E (X,Y )\u223cD Y,h(X) .\n\nNoise-Corrected Plug-in Method\n\nThe approach we describe is conceptually very simple. We will denote by \u03b7, \u03b7 : X \u2192\u2206 n the (vector) class probability functions under the clean distribution D associated with clean labeled examples and the noisy distribution D associated with noisy examples, respectively, with components given by\n\u03b7 y (x) = P(Y = y | X = x) \u03b7 y (x) = P( Y = y | X = x)\nfor each y \u2208 [n]. It is easy to see that\n\u03b7 y (x) = y \u2208[n] P( Y = y | Y = y ) \u00b7 P(Y = y | X = x) = y \u2208[n] \u03b3 y ,y \u00b7 \u03b7 y (x) = c y \u03b7(x) , which gives \u03b7(x) = C \u03b7(x) .\nTherefore, provided C is invertible, we have\n\u03b7(x) = (C ) \u22121 \u03b7(x) .(1)\nThis suggests that once we have an estimate of the noisy class probability function \u03b7, we may be able to 'de-noise' it to construct an estimate of the clean class probability function \u03b7. This idea in its basic form can be problematic, since C \u22121 is not necessarily a stochastic matrix; in particular, C generally maps probability vectors \u03b7(x) in the probability simplex \u2206 n to noisy probability vectors \u03b7(x) in a limited subset of the simplex \u2206 n , and in general, an estimate of \u03b7(x) could fall outside that subset, so that multiplying the estimate by (C ) \u22121 could then lead to an invalid 'estimate' of \u03b7(x) that falls outside \u2206 n . Nevertheless, we get around this issue by never really needing to construct a fully valid estimate of \u03b7(x); instead, we simply use the above relation to derive a noise-corrected plug-in classifier that operates directly on estimates of the noisy class probabilities \u03b7(x).\n\nOur regret transfer bounds in Section 4 will establish that this indeed leads to a correct learning approach.\n\nWe start by explaining our approach in the context of the multiclass 0-1 loss, and then describe the extension to general multiclass losses.\n\nMulticlass 0-1 loss. As is well known, the Bayes optimal classifier for the multiclass 0-1 loss is given by\nh 0-1, * D (x) = argmax y\u2208[n]\n\u03b7 y (x) .\n\nBy Eq.\n\n(1), we can re-write this in terms of the noisy class probability function \u03b7 as follows:\nh 0-1, * D (x) = argmax y\u2208[n] (C ) \u22121 \u03b7(x) y =: plugin 0-1 C \u03b7(x)\n. Notably, this means that during training, we can simply construct a multiclass CPE model \u03b7 : X \u2192\u2206 n for the noisy class probability function \u03b7, by running any standard multiclass CPE method (such as standard multiclass logistic regression) on the given noisy training examples, and then construct a noise-corrected classifier h : X \u2192Y by applying the above noise-corrected plug-in step during prediction:\nh(x) = plugin 0-1 C \u03b7(x) .\nMulticlass cost-sensitive losses. More generally, consider any multiclass loss matrix L \u2208 R n\u00d7n + . The Bayes optimal Algorithm 1 Noise-Corrected Plug-in Algorithm 1: Inputs:\n\n(1) Noisy training sample,\nS = ((x 1 , y 1 ), . . . , (x m , y m )) \u2208 (X \u00d7 Y) m (2) Target loss matrix L \u2208 R n\u00d7n + (3) (If known) Noise matrix C \u2208 [0, 1] n\u00d7n 2: Run a standard CPE learner on S: \u03b7 = CPE-Learner( S ) 3: If C unknown: Construct estimate C (see Section 5) 4: Output: If C known: h = plugin L C \u2022 \u03b7 If C unknown: h = plugin L C \u2022 \u03b7\nclassifier for L (which for any instance x, chooses a prediction that minimizes the expected loss under L) is given by\nh L, * D (x) = argmin y\u2208[n]\n\u03b7(x) y .\n\nAs for the 0-1 loss, by Eq.\n\n(1), we can re-write this in terms of the noisy class probability function \u03b7 as follows:\nh L, * D (x) = argmin y\u2208[n] \u03b7(x) C \u22121 y = argmin y\u2208[n] \u03b7(x) C \u22121 L y =: plugin L C \u03b7(x) .\nAgain, this means that during training, we can use a standard multiclass CPE learner on the noisy examples to construct a CPE model \u03b7 : X \u2192\u2206 n for the noisy class probability function \u03b7, and then construct a noise-corrected classifier h : X \u2192Y by applying the above noise-corrected plug-in step during prediction:\nh(x) = plugin L C \u03b7(x) .\nNote that one can pre-compute C \u22121 L, and so at prediction time, in order to implement plugin L C ( \u03b7(x)), one needs to compute n inner products (of the column vectors of C \u22121 L with \u03b7(x)), for a total computational cost of O(n 2 ). 1,2\n\nOur final algorithm is shown in Algorithm 1. An example of a CPE learner that minimizes a (strongly) proper composite multiclass surrogate loss is provided in Section 4.2. In settings where the noise matrix C is not known, one may need to estimate C from the noisy training sample itself; this is discussed in Section 5.\n\n\nRegret Transfer Bounds and Consistency\n\nIn this section, we provide quantitative regret transfer bounds for our noise-corrected plug-in algorithm; these bounds also establish that if the noisy CPE method used in training is consistent (i.e., converges to the correct noisy class probabilities), then our approach is consistent for the target learning problem. We derive our results for the multiclass case with a general loss matrix L; they can be specialized to the binary and/or 0-1 case as needed. In particular, define the L-regret (or the excess L-risk) of a classifier h : X \u2192Y under the clean distribution D as follows:\nregret L D [h] = er L D [h] \u2212 inf h :X \u2192Y er L D [h ] .(2)\nOur goal is to upper bound this L-regret for our learned classifier h; if this regret converges (in probability, over the random draw of the noisy training sample) to zero as the training sample size increases, then the algorithm is (Bayes) consistent for L under D.\n\nIn Section 4.1, we provide a general result upper bounding the target L-regret of our learned classifier h = plugin L C \u2022 \u03b7 (on the clean distribution D) in terms of the noisy CPE regret of \u03b7 (on the noisy distribution D). In Section 4.2, we specialize our result to CPE methods that learn \u03b7 by minimizing a strongly proper composite surrogate loss (extending the notion of strong properness defined for binary losses by Agarwal (2014) to the multiclass case), and apply this result in particular to the multiclass logistic loss, which we show to be 1-strongly proper composite.\n\n\nRegret Transfer Bound for General CPE Methods\n\nWe have the following result for our noise-corrected plug-in method using any CPE learner:\n\nTheorem 1. For any noisy CPE model \u03b7 : X \u2192\u2206 n and resulting noise-corrected plug-in classifier h = plugin L C \u2022 \u03b7, we have\nregret L D [ h ] \u2264 2 max y y 2 \u00b7 C \u22121 2 \u00b7 E X \u03b7(X) \u2212 \u03b7(X) 2 .\nIn other words, if the learned noisy CPE model \u03b7 is close to the correct noisy class probabilities \u03b7, in the sense that\nE X \u03b7(X) \u2212 \u03b7(X) 2 is small, then the target L-regret of the noise-corrected plug-in classifier on the clean distri- bution D, regret L D [ h ]\n, is also small. In particular, if the CPE learner is consistent for the noisy distribution in the sense that E X \u03b7(X) \u2212 \u03b7(X) 2 P \u2212 \u21920 (as the sample size increases), then the overall noise-corrected plug-in method is (Bayes) L-consistent for the clean distribution D, in the sense that regret L\nD [ h ] P \u2212 \u21920.\nNote that the above bound depends on the noise matrix C through the term C \u22121 2 . This is the largest singular value of C \u22121 , or equivalently, the reciprocal of the smallest singular value of C. Thus, as the noise matrix C approaches singularity, the bound becomes larger. This suggests that as C becomes closer to being singular, we may need a higher quality class probability approximation on the noisy distribution D (i.e. larger sample size) to reach the same level of L-regret on the clean distribution D. As we will see in Section 6, our experiments also support this observation.\n\n\nRegret Transfer Bound for CPE Methods Minimizing a Strongly Proper Composite Surrogate Loss\n\nIn practice, a popular approach for learning CPE models is to minimize a suitable (convex) surrogate loss, such as the multiclass logistic loss (this is also what we use in our experiments). We show that for a suitable class of such surrogate losses, the CPE regret can be further upper bounded in terms of the surrogate loss based regret.\n\nSpecifically, let \u03c8 : [n] \u00d7 R n\u22121 \u2192R + be any surrogate loss that acts on (n \u2212 1)-dimensional 'score vectors' in R n\u22121 , and let \u03bb : \u2206 n \u2192R n\u22121 be an invertible 'link' function. 3 Then \u03c8 is said to be strictly proper composite with link function \u03bb if for all p \u2208 \u2206 n and u \u2208 R n\u22121 , u = \u03bb(p):\nE Y \u223cp \u03c8(Y, u) \u2212 \u03c8(Y, \u03bb(p)) > 0 .\nIt is well known that minimizing such a strictly proper composite surrogate loss over a suitably rich function class provides consistent class probability estimates (Williamson et al., 2016). For binary surrogates, Agarwal (2014) defined a stronger condition that allows the derivation of quantitative bounds in terms of the surrogate regret. Here we extend this notion to the multiclass case and apply it to obtain bounds for our noisy labels problem. 4 Definition 2 (Strongly proper composite multiclass losses). Let s > 0. We say a multiclass surrogate loss \u03c8 :\n\n[n] \u00d7 R n\u22121 \u2192 R + is s-strongly proper composite with (invertible) link function \u03bb : \u2206 n \u2192 R n\u22121 if for all p \u2208 \u2206 n and u \u2208 R n\u22121 :\nE Y \u223cp \u03c8(Y, u) \u2212 \u03c8(Y, \u03bb(p)) \u2265 s 2 \u03bb \u22121 (u) \u2212 p 2 2 .\nAs a concrete example, consider the widely used multiclass logistic surrogate loss: Example 1 (Multiclass logistic loss and link function). The multiclass logistic loss \u03c8 mlog : [n] \u00d7 R n\u22121 \u2192 R + is defined 3 More generally, one can consider surrogate losses \u03c8 : [n] \u00d7 C\u2192R+ acting on score vectors in any convex set C that is in 1-to-1 correspondence with \u2206n, such as C = {u \u2208 R n : n i=1 ui = 0}. It is also common to consider 'over-parameterized' surrogate losses acting on C = R n ; e.g. see the discussion on the multiclass logistic surrogate loss toward the end of the section. 4 Strong properness implies strict properness. Most commonly used strictly proper composite losses are also strongly proper composite, but the latter condition allows for stronger quantitative guarantees.\n\nas\n\u03c8 mlog (y, u) = \uf8f1 \uf8f2 \uf8f3 \u2212 ln exp(uy) 1+ n\u22121 i=1 exp(ui) if y \u2208 [n \u2212 1] ln 1 + n\u22121 i=1 exp(u i ) if y = n .\nThe loss is often used with the invertible link function \u03bb mlog : \u2206 n \u2192 R n\u22121 , which together with its inverse \u03bb \u22121 mlog : R n\u22121 \u2192 \u2206 n is given by\n\u03bb mlog (p)= \uf8eb \uf8ec \uf8ed ln( p1 pn ) . . . ln( pn\u22121 pn ) \uf8f6 \uf8f7 \uf8f8; \u03bb \u22121 mlog (u)= \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed exp(u1) 1+ n\u22121 i=1 exp(ui) . . . exp(un\u22121) 1+ n\u22121 i=1 exp(ui) 1 1+ n\u22121 i=1 exp(ui) \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 .\nWe note that the multiclass logistic loss above is often implemented in an 'over-parameterized' form, with score vectors in C = R n and the softmax function used for 'inverting' such score vectors to class probabilities (indeed, softmax is a many-to-one mapping). 5 We have the following result showing that \u03c8 mlog is strongly proper composite: Lemma 3. The multiclass logistic loss \u03c8 mlog is 1-strongly proper composite with link function \u03bb mlog .\n\nA  The over-parameterized multiclass logistic loss is also sometimes referred to as the cross-entropy loss.\nh = plugin L C \u2022 \u03bb \u22121 \u2022 f , we have regret L D [ h ] \u2264 2 max y y 2 \u00b7 C \u22121 2 \u00b7 2 s regret \u03c8 D [ f ] .\n\nAlgorithm 2 CPE Learner Minimizing a Strongly Proper\n\nComposite Surrogate Loss (on Noisy Data) 1: Input: Noisy training sample, S = ((x 1 , y 1 ), . . . , (x m , y m )) \u2208 (X \u00d7 Y) m 2: Parameters:\n\n(1) Strongly proper composite loss \u03c8 : [n]\u00d7R n\u22121 \u2192R + with (invertible) link function \u03bb : \u2206 n \u2192R n\u22121 ;\n\n(2) Class F of functions f : X \u2192R n\u22121\n3: Compute f \u2208 argmin f \u2208F m i=1 \u03c8( y i , f (x i )) 4: Output: \u03b7 = \u03bb \u22121 \u2022 f\n\nEstimating the Noise Matrix C\n\nIn settings where the noise matrix C is not known in advance, one may need to estimate C from the noisy training examples themselves. Most previous work on estimating the noise matrix assumes the existence of 'anchor points' (definition provided below), and relies on estimating these points accurately. 6 In particular, Menon et al. (2015) provided a method for estimating C using anchor points in the case of binary labels; Patrini et al. (2017) extended it to the multiclass setting, and later, Yao et al. (2020) proposed another noise estimation method also based on anchor points. Unfortunately, however, we show below that these methods do not work correctly for all noise matrices C. In particular, in Section 5.1, we point out an error in the approach used to compute anchor points in the noise estimation methods of Patrini et al. and Yao et al., and provide sufficient and necessary conditions on C under which these methods do work correctly. Of course, when C is unknown, we may not know whether it satisfies these conditions, and so we may not be able to verify whether the estimation is correct. Building on the intuition developed from our analysis, in Section 5.2 we propose an iterative noise estimation heuristic that essentially tries to improve the estimation of anchor points, and that can be applied for any unknown C; while it is not guaranteed to converge or recover a correct estimate, in our experiments, it generally performs as well as, or improves upon, the methods of Patrini et al. and Yao et al. It remains an open question whether general noise matrices C can be estimated reliably using anchor points.\n\n\nConditions for Correctness of Noise Estimation Methods Based on Anchor Points\n\nThe methods of Patrini et al. (2017) and Yao et al. (2020) make the following assumption:\n\n(A) (Anchor points) Under the clean distribution D = (\u00b5, \u03b7), for every y \u2208 Y, there is a 'perfect' examplex y \u2208 X of class y (called an anchor point of class y) with marginal \u00b5(x y ) > 0 and \u03b7(x y ) = e y . 6 There is also some recent work that aims to estimate C without identifying anchor points (Xia et al., 2019). Under this assumption, Patrini et al. observe that, for all y, y \u2208 Y, \u03b7 y (x y ) = C \u03b7(x y ) y = C e y ) y = \u03b3 y, y .\n\nTherefore, if one can identify such perfect examples/anchor pointsx y , and if the class probability estimates \u03b7(x) are accurate, then one can estimate the noise rates via \u03b3 y, y = \u03b7 y (x y ) \u2200y, y \u2208 [n] .\n\nAs discussed previously, provided one has a sufficiently large training sample, accurate class probability estimates can be formed by minimizing a strongly proper composite surrogate over a suitably rich function class. The main step that is needed, therefore, is to identify the 'perfect' examples/anchor pointsx y above.\n\nPatrini et al. suggest identifying such anchor points by first estimating a CPE model \u03b7, and then taking a large collection of available instances X train \u2282 X drawn IID from the marginal \u00b5 (these could just be the training instances in S or could include other unlabeled instances as well), and estimating anchor points according to\nx y \u2208 argmax x\u2208Xtrain \u03b7 y (x)\nHowever, note that these anchor points should be chosen to maximize the true class probability \u03b7 y (x), not the noisy class probability \u03b7 y (x)! Therefore, the above method (also used by Yao et al., according to footnote 2 in their paper) is in general incorrect. Of course, we do have a relation between \u03b7 and \u03b7 (Eq. (1)), but that relation involves C; without knowledge of C, we cannot in general use a noisy CPE model \u03b7 to find instances maximizing \u03b7 y (x).\n\nNevertheless, surprisingly, Patrini et al. and Yao et al. did report some successful experiments with their methods. On investigating further, we identified a sufficient condition on the noise matrix C under which argmax x \u03b7 y (x) = argmax x \u03b7 y (x), and therefore, under which the above approach for estimating anchor points does work correctly, as well as a related necessary condition failing which the approach fails: Theorem 5. Suppose assumption (A) above holds.\n\n1. If the noise matrix C = [\u03b3 y, y ] satisfies the sufficient condition \u03b3 y, y > \u03b3 y, y \u2200y = y , then provided that X train is a large enough sample (drawn IID from \u00b5) and the noisy class probabilities \u03b7(x) are modeled accurately, the anchor point estimation method of Patrini et al. (2017) described above works correctly.\n\n2. If C fails to satisfy the necessary condition \u03b3 y, y \u2265 \u03b3 y, y \u2200y = y , then the anchor point estimation method of Patrini et al.\n\n(2017) described above fails.\n\nAlgorithm 3 Iterative Noise Estimation Heuristic 1: Inputs:\n\n(1) CPE model \u03b7 : X \u2192\u2206 n (for noisy distribution) (2) X train \u2282 X (3) Maximum number of iterations T 2: Initialize: C (1) = \u03b3 (1) y, y \u2190 I 3: For t = 1, . . . , T :\n4: \u2200y \u2208 Y : x y \u2190 argmax x\u2208Xtrain ( C (t) ) \u22121 \u03b7(x) y 5: \u2200y, y \u2208 Y : \u03b3 (t+1) y, y \u2190 \u03b7 y ( x y ) 6: diff (t) \u2190 C (t+1) \u2212 C (t) F 7: Output: C (t * ) , where t * = argmin t\u2208[T ] diff (t)\nIt is worth noting that the noise matrices in Patrini et al.'s study that were estimated correctly by their method all satisfy the sufficient condition above; for the one noise matrix in their study which did not satisfy the necessary condition above, their estimation method failed (see Section 6.2 for details). Similarly, all the noise matrices considered in Yao et al.'s study satisfy the sufficient condition above; in our experiments, for noise matrices that fail to satisfy the necessary condition above, Yao et al.'s method also fails.\n\nWe also note that, in Patrini et al.'s study, after learning a noisy CPE model \u03b7 and estimating C, a different learning algorithm that minimizes a noise-corrected loss was then used to learn a classifier h. In our case, after learning \u03b7 and estimating C, we can simply output the plug-in classifier h = plugin L C \u2022 \u03b7, with no additional training required.\n\n\nAn Iterative Noise Estimation Heuristic\n\nBased on the discussion above, we propose an alternative, iterative noise estimation heuristic that aims to improve anchor point estimation, wherein we start with an estimate of C = I (no noise), and iteratively feed in the current estimate into a corrected version of Patrini et al.'s method to obtain an updated estimate. The approach is shown in Algorithm 3. The first iteration simply corresponds to Patrini et al.'s original method; therefore, if C satisfies the condition of Theorem 5, then the first iteration produces an accurate estimate. Unfortunately, the method is not guaranteed to converge or to produce an accurate estimate in general; nevertheless, in our experiments, we find this method performs as well as, or better than, the methods of Patrini et al. and Yao et al. It remains an open question whether general noise matrices C can be estimated reliably using anchor points.\n\n\nExperiments\n\nWe conducted two sets of experiments to evaluate our noisecorrected plug-in algorithm. In the first set of experiments, we generated synthetic data, and tested the sample complexity behavior of our algorithm, using linear models, for a va-riety of different noise matrices C with increasing values of C \u22121 2 . In the second set of experiments, we compared the performance of our noise correction method with those of van Rooyen & Williamson (2017) and Patrini et al. (2017)), all using neural network models, on two real benchmark data sets; in this set of experiments, we used noise matrices C constructed for these data sets by Patrini et al. (2017), closely following their experimental settings. We also compared the performance of our noise estimation method with those of Patrini et al. (2017) and Yao et al. (2020). In all cases, we used the multiclass logistic loss (unmodified in our case, and modified as needed by each of the other algorithms). We summarize both sets of experiments below. In all cases, training labels were flipped randomly according to the prescribed (invertible) noise matrix C; performance of the learned models was then measured on a clean test set.\n\n\nSynthetic Data: Sample Complexity Behavior\n\nIn order to test the sample complexity behavior of our algorithm, we generated synthetic data from a known distribution (from which we could draw increasingly large training samples as needed). Specifically, we constructed a 5-class problem over a 10-dimensional instance space X = [\u22121, 1] 10 as follows. Instances x were generated uniformly at random from X . The class probability function \u03b7 : X \u2192\u2206 5 was set to \u03b7 y (x) = exp(w y x) 5 y =1 exp(w y x) for some fixed weight vectors w 1 , . . . , w 5 \u2208 R 10 (the entries of the weight vectors were drawn IID from N (0, 1) and then scaled so that w y 2 = 1). Given an instance x, a clean label y was drawn randomly according to \u03b7(x). For any prescribed (row-stochastic) noise matrix C, training labels y were then stochastically flipped to a noisy label y according to the probabilities in the y-th row of C.\n\nWe tested the sample complexity behavior of our algorithm, implemented to minimize the multiclass logistic loss over linear models, for a variety of noise matrices C with increasing values of C \u22121 2 . 7 We ran the algorithm on increasingly large (noisy) training samples (up to 40,000 examples) and measured the performance on a large test set of 10,000 (clean) examples. The results are shown in Figure 1: The left plot in the figure shows results for the 0-1 loss (shown as accuracy); the right plot shows results for a different target loss, specifically, the ordinal regression loss L ord defined as ord y, y = | y \u2212 y|. 8 We see that, as suggested by our regret transfer bound, as C \u22121 2 increases (i.e. as the matrix C  Figure 1. Sample complexity behavior of our algorithm on synthetic 5-class data for a variety of noise matrices C with increasing values of C \u22121 2. Left: 0-1 loss (shown as accuracy). Right: Ordinal regression loss L ord . As suggested by our regret bounds, as C \u22121 2 increases, the sample size needed to reach a given level of performance generally increases. See Section 6.1 for details. becomes closer to being singular), the sample size required to achieve a given level of performance generally increases. 9\n\n\nReal Data: Comparison with Other Algorithms\n\nWe conducted experiments on several real data sets. Here we describe experiments on two benchmark data sets, MNIST (Lecun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009), where we compared our algorithm with the unbiased estimator method of van Rooyen & Williamson (2017) and the forward method of Patrini et al. (2017), all using neural network models, and also tested the incorporation of noise estimation methods. These experiments were designed to closely mimic experiments of Patrini et al. (2017); we used code provided by the authors 10 and kept the neural network architectures and all parameters as given. 11\n\nBoth MNIST and CIFAR10 are 10-class data sets (see the supplementary material for details of the data sets). The experiments used 0-1 loss (measured as accuracy). In both cases, experiments were conducted with clean data (no noise) and with 6 noise matrices C. One of these, C sym(0.2) , was a symmetric noise matrix with the following structure: all diagonal entries \u03b3 yy were set to 1 \u2212 \u03b3, where \u03b3 = 0.2; all off-diagonal entries were set to \u03b3 n\u22121 (here n = 10). For 9 We note that for the synthetic data distribution described above, although the clean class probabilities \u03b7(x) take the form of a softmax-of-linear model, the noisy class probabilities \u03b7(x) are not of this form. Therefore, even though the plots in Figure 1 seem to suggest our algorithm converges to the Bayes optimal performance, strictly speaking, this is not the case: The algorithm does appear to have learned a fairly accurate model for the noisy class probabilities \u03b7(x), but it cannot express them exactly; in order to truly model them exactly, we would need to implement the algorithm using a richer function class. (We do not do this here since the difference in performance would be unnoticeable. We use richer function classes in the experiments with real data, where we employ neural network models.) 10 https://github.com/giorgiop/loss-correction 11 We note that for some parameters (e.g. batch size), there is a discrepancy between the settings used in the code and those mentioned in the paper; we used the settings in the code. such symmetric noise matrices (with \u03b3 < n\u22121 n ) and 0-1 loss, it is known that no noise correction is needed, and that standard algorithms designed to learn a good classifier for 0-1 loss (on the noisy data) work correctly (van Rooyen & Williamson, 2017;Ghosh et al., 2017). The other 5 noise matrices were asymmetric, and were artificially designed by Patrini et al. (2017) to simulate some of the possible structures of real label noise, where a label might be replaced with some probability \u03b3 by some other similar label, for example, Cat \u2192 Dog. For each of MNIST and CIFAR10, Patrini et al. specified a set of such 'label noise' transitions to create specific parametric noise matrices C MNIST(\u03b3) and C CIFAR10(\u03b3) , and instantiated these with \u03b3 = 0.2, 0.6; we additionally included \u03b3 = 0.45, 0.55, 0.65. The noise matrices C sym(0.2) and C MNIST(\u03b3) , C CIFAR10(\u03b3) for \u03b3 < 0.5 all satisfy the sufficient condition of Theorem 5; the matrices C MNIST(\u03b3) , C CIFAR10(\u03b3) for \u03b3 > 0.5 fail to satisfy the necessary condition. Details of these noise matrices, as well as the neural network models used and associated parameter settings, can be found in the supplementary material.\n\nThe results are summarized in Tables 1 and 2, respectively. For each algorithm, we implemented four versions: one with the noise matrix C known, and the other three with the noise matrix estimated using either the method of Patrini et al. (2017) (denoted C Patrini ), the Dual T method of Yao et al. (2020) ( C DT ), or our iterative noise estimation heuristic ( C iter ). 12 Several observations are in order. First, for the symmetric noise matrix C sym(0.2) , standard logistic regression with no noise correction does well as expected; for heavy asymmetric noise (C MNIST(\u03b3) and C CIFAR10(\u03b3) for \u03b3 > 0.5), standard logistic regression without noise correction does not do well. Second, our noise-corrected plug-in method is comparable to the other noise-corrected methods, even though it requires no change to the training process. Third, our iterative noise estimation heuristic either performs similarly to the noise estimation methods of Patrini et al. and Yao et al., or in some cases (particularly C MNIST(\u03b3) for \u03b3 > 0.5) significantly outperforms their methods. Finally, for the noise matrices that satisfy the sufficient condition of Theorem 5, all three noise estimation methods perform well; for the noise matrices that fail to satisfy the necessary condition, no method achieves perfect estimation.\n\nIt is worth pointing out again that all three noise estimation methods (the methods of Patrini et al. (2017) and Yao et al. (2020), and our iterative method) make use of a noisy CPE model \u03b7(x) learned from the noisy training data. Our noise-corrected plug-in algorithm makes use of this noisy CPE model directly, simply applying a noise-corrected plugin step at prediction time, and does not need any further re-training; on the other hand, the other two noise correc- Table 1. Test accuracy (percentage) on MNIST data, shown as the mean (with standard error of the mean in parentheses) over 5 random trials. In each column, the best algorithm(s) using the known noise matrix C and the best algorithm(s) using each of the 3 noise estimation methods (Patrini et al., Dual T, and our iterative heuristic) are shown in bold font; among the latter, the best algorithm + noise estimation combination overall is further enclosed in asterisks. See Section 6.2 for details.\n\nAlgorithm  Table 2. Test accuracy (percentage) on CIFAR10 data, shown as the mean (with standard error of the mean in parentheses) over 5 random trials. In each column, the best algorithm(s) using the known noise matrix C and the best algorithm(s) using each of the 3 noise estimation methods (Patrini et al., Dual T, and our iterative heuristic) are shown in bold font; among the latter, the best algorithm + noise estimation combination overall is further enclosed in asterisks. See Section 6.2 for details. tion methods above both need to further minimize a noisecorrected loss on the noisy data in order to learn a classifier.\n\n\nConclusion\n\nWe have provided a simple noise-corrected plug-in method for general multiclass class-conditional label noise (CCN) that requires no change to the training process. Noise correction takes place at prediction time, and after a one-time matrix inversion and multiplication step, requires O(n 2 ) time per prediction, where n is the number of classes. For general loss matrices L, this is the same computational cost that is needed for standard plug-in methods; for the 0-1 loss, it is a factor of n larger than the standard cost (for small to moderate n, this may still be a smaller cost overall as compared to the cost of modifying the training process). We have also provided quantitative regret transfer bounds for our method that quantify the effect of learning from noisy labels, as well as an iterative noise estimation heuristic.\n\nOne possible issue to be careful about is that accurate estimation of noisy class probabilities can potentially be challenging due to their typically higher variance (particularly with neural networks, which often exhibit high calibration errors (Guo et al., 2017;Rahimi et al., 2020)) -while we did not find this to be a significant concern in our experiments, it could possibly be an issue for certain types of data sets or noise.\n\nIt remains an open question whether general noise matrices C can be estimated reliably using anchor points.\n\n\nCPE learner minimizing a strongly proper composite surrogate loss (over noisy training examples) is shown in Algorithm 2. (Instantiating this with the multiclass logistic loss \u03c8 mlog above and the class of linear scoring functions leads to the multiclass linear logistic regression algorithm.) In what follows, for a surrogate loss \u03c8 : [n] \u00d7 R n\u22121 \u2192R + , we will define the \u03c8-generalization error of a scoring function f : X \u2192R n\u22121 under D as er \u03c8 D [f ] = E (X,Y )\u223c D \u03c8(Y, f (X)) , and the \u03c8-regret of f under D as regret \u03c8 D [f ] = er \u03c8 D [f ] \u2212 inf f :X \u2192R n\u22121 er \u03c8 D [f ] . Then we have the following regret transfer bound: Theorem 4. Let s > 0. Let \u03c8 : [n] \u00d7 R n\u22121 \u2192 R + be a s-strongly proper composite surrogate loss with (invertible) link function \u03bb : \u2206 n \u2192 R n\u22121 . For any scoring model f : X \u2192R n\u22121 being used as a (noisy) CPE model via \u03b7(x) = \u03bb \u22121 ( f (x)), and resulting noise-corrected plug-in classifier\n\n\nUnbiased, C Patrini 89.54 (0.16) 82.27 (0.14) 86.08 (0.42) 69.45 (1.75) 67.15 (1.97) 69.77 (0.58) 69.94 (0.45) Forward, C Patrini 89.66 (0.05) 86.05 (0.23) 88.11 (0.06) 84.67 (0.48) * 78.78 * (1.14) 75.47 (0.36) * 74.48 * (0.48)Algorithm \nNo noise \nC sym(0.2) \nC CIFAR10(0.2) C CIFAR10(0.45) \nC CIFAR10(0.55) C CIFAR10(0.6) C CIFAR10(0.65) \nLogistic \n89.76 (0.07) \n85.26 (0.16) 86.95 (0.12) 76.56 (0.31) \n63.78 (0.58) \n58.1 (0.39) \n54.79 (0.31) \nUnbiased, C \n89.6 (0.08) \n81.82 (0.27) 84.1 (0.14) 61.91 (2.32) \n57.43 (3.81) 70.12 (2.43) 74.91 (1.99) \nForward, C \n89.6 (0.14) \n86.62 (0.13) 88.7 (0.16) \n85.71 (0.2) \n85.12 (0.11) 86.99 (0.03) 87.12 (0.14) \nPlug-in, C \n89.76 (0.07) \n85.26 (0.16) 87.46 (0.09) 82.71 (0.26) \n81.8 (0.24) \n83.5 (0.12) \n84.01 (0.16) \n\nPlug-in, C Patrini \n89.76 (0.07) \n85.29 (0.19) 87.39 (0.08) 82.46 (0.24) \n78.02 (0.24) 75.48 (0.19) \n74.26 (0.3) \nUnbiased, C DT \n89.3 (0.16) \n82.7 (0.12) 83.43 (0.23) 67.67 (2.05) \n63.38 (0.54) \n63.4 (1.05) \n62.64 (1.16) \nForward, C DT \n89.75 (0.21)  *  86.93  *  (0.08)  *  88.32  *  (0.1)  *  84.72  *  (0.42) \n75.98 (0.48) 69.85 (1.68) \n61.89 (0.6) \nPlug-in, C DT \n89.77 (0.07) \n85.14 (0.19) 87.39 (0.1) 81.91 (0.32) \n75.6 (0.29) \n71.21 (0.34) \n68.04 (0.7) \nUnbiased, C iter \n89.67 (0.06) \n82.09 (0.14) 86.0 (0.19) 68.75 (2.68) \n65.85 (1.23) \n68.77 (1.1) \n67.83 (1.65) \nForward, C iter \n89.5 (0.07) \n85.92 (0.2) 88.17 (0.03) 84.09 (0.71) \n77.64 (0.73)  *  75.71  *  (0.32) 74.46 (0.59) \nPlug-in, C iter \n89.76 (0.07) \n85.27 (0.2) \n87.4 (0.08) 82.41 (0.22) \n77.94 (0.19) 75.49 (0.17) 74.27 (0.29) \n\n\nAlso note that if one has an implementation of the standard plug-in step plugin L (\u00b7) (without noise correction) for general (costsensitive) loss matrices L, one can simply use that implementation with loss L = C \u22121 L (since plugin L C \u03b7(x) = plugin L \u03b7(x) ). 2 This also applies to the 0-1 loss: one can simply pre-compute C \u22121 L 0-1 , and then at prediction time, compute n inner products (of the column vectors of C \u22121 L 0-1 with \u03b7(x)) for a cost of O(n 2 ) (and predict according to argmin y\u2208[n] \u03b7(x) (C \u22121 L 0-1 )y).\nThe implementation was in PyTorch(Paszke et al., 2019), and used the AdamW optimizer. The optimizer was run for 50 epochs over the training sample; the learning rate parameter was initially set to 0.01 and was halved at the end of every 5 epochs.8  FollowingNatarajan et al. (2013), for each noise matrix, we repeated each experiment 3 times with independent random corruptions of the training set using the same noise matrix; our results give the mean performance over the 3 runs.\nOur iterative noise estimation heuristic was implemented with maximum number of iterations T set to 1000.\nAcknowledgmentsThanks to the anonymous reviewers for several helpful comments and pointers. This material is based upon work supported in part by the US National Science Foundation (NSF) under Grant Nos. 1934876 and 1717290. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.\nSurrogate regret bounds for bipartite ranking via strongly proper losses. S Agarwal, Journal of Machine Learning Research. 15Agarwal, S. Surrogate regret bounds for bipartite ranking via strongly proper losses. Journal of Machine Learning Research, 15:1653-1674, 2014.\n\nLearning from noisy examples. D Angluin, P D Laird, Mach. Learn. 24Angluin, D. and Laird, P. D. Learning from noisy examples. Mach. Learn., 2(4):343-370, 1987.\n\nOn the sample complexity of noise-tolerant learning. J A Aslam, S E Decatur, Inf. Process. Lett. 574Aslam, J. A. and Decatur, S. E. On the sample complexity of noise-tolerant learning. Inf. Process. Lett., 57(4):189-195, 1996.\n\nCombining labeled and unlabeled data with co-training. A Blum, T M Mitchell, Proceedings of the Eleventh Annual Conference on Computational Learning Theory. the Eleventh Annual Conference on Computational Learning TheoryCOLTACMBlum, A. and Mitchell, T. M. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT 1998, pp. 92-100. ACM, 1998.\n\nLearning linear threshold functions in the presence of classification noise. T Bylander, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory. the Seventh Annual ACM Conference on Computational Learning TheoryCOLTACMBylander, T. Learning linear threshold functions in the pres- ence of classification noise. In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 1994, pp. 340-347. ACM, 1994.\n\nSample-efficient strategies for learning in the presence of noise. N Cesa-Bianchi, E Dichterman, P Fischer, E Shamir, H U Simon, J. ACM. 465Cesa-Bianchi, N., Dichterman, E., Fischer, P., Shamir, E., and Simon, H. U. Sample-efficient strategies for learning in the presence of noise. J. ACM, 46(5):684-719, 1999.\n\nLearning with bounded instance and label-dependent label noise. J Cheng, T Liu, K Ramamohanarao, D Tao, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020Cheng, J., Liu, T., Ramamohanarao, K., and Tao, D. Learn- ing with bounded instance and label-dependent label noise. In Proceedings of the 37th International Con- ference on Machine Learning, ICML 2020, volume 119 of Proceedings of Machine Learning Research, pp. 1789- 1799. PMLR, 2020.\n\nOn the consistency of ranking algorithms. J Duchi, L Mackey, Jordan , M , Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)Duchi, J., Mackey, L., and Jordan, M. On the consistency of ranking algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2010.\n\nClassification in the presence of label noise: A survey. B Fr\u00e9nay, M Verleysen, IEEE Trans. Neural Networks Learn. Syst. 255Fr\u00e9nay, B. and Verleysen, M. Classification in the presence of label noise: A survey. IEEE Trans. Neural Networks Learn. Syst., 25(5):845-869, 2014.\n\nRobust loss functions under label noise for deep neural networks. A Ghosh, H Kumar, P S Sastry, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceAAAI PressGhosh, A., Kumar, H., and Sastry, P. S. Robust loss func- tions under label noise for deep neural networks. In Proceedings of the Thirty-First AAAI Conference on Ar- tificial Intelligence, 2017, pp. 1919-1925. AAAI Press, 2017.\n\nOn calibration of modern neural networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR70Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learn- ing, ICML 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1321-1330. PMLR, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer SocietyHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770- 778. IEEE Computer Society, 2016.\n\nEfficient noise-tolerant learning from statistical queries. M J Kearns, J. ACM. 456Kearns, M. J. Efficient noise-tolerant learning from statisti- cal queries. J. ACM, 45(6):983-1006, 1998.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Department of Computer Science, University of TorontoMaster's thesisKrizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 10.1109/5.72679186Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Pro- ceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.\n\nLearning from corrupted binary labels via classprobability estimation. A K Menon, B Van Rooyen, C S Ong, B Williamson, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine Learning37of JMLR Workshop and Conference Proceedings. JMLR.orgMenon, A. K., van Rooyen, B., Ong, C. S., and Williamson, B. Learning from corrupted binary labels via class- probability estimation. In Proceedings of the 32nd Inter- national Conference on Machine Learning, ICML 2015, volume 37 of JMLR Workshop and Conference Proceed- ings, pp. 125-134. JMLR.org, 2015.\n\nLearning from binary labels with instance-dependent noise. A K Menon, B Van Rooyen, N Natarajan, Mach. Learn. 1078-10Menon, A. K., van Rooyen, B., and Natarajan, N. Learning from binary labels with instance-dependent noise. Mach. Learn., 107(8-10):1561-1595, 2018.\n\nLearning with noisy labels. N Natarajan, I S Dhillon, P Ravikumar, A Tewari, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. Natarajan, N., Dhillon, I. S., Ravikumar, P., and Tewari, A. Learning with noisy labels. In Advances in Neural Information Processing Systems 26: 27th Annual Confer- ence on Neural Information Processing Systems 2013, pp. 1196-1204, 2013.\n\nAn imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Pytorch, Advances in Neural Information Processing Systems. Curran Associates, Inc32Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024- 8035. Curran Associates, Inc., 2019.\n\nMaking deep neural networks robust to label noise: A loss correction approach. G Patrini, A Rozza, A K Menon, R Nock, L Qu, 2017 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer SocietyPatrini, G., Rozza, A., Menon, A. K., Nock, R., and Qu, L. Making deep neural networks robust to label noise: A loss correction approach. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, pp. 2233-2241. IEEE Computer Society, 2017.\n\nIntra order-preserving functions for calibration of multi-class neural networks. A Rahimi, A Shaban, C Cheng, R Hartley, B Boots, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Rahimi, A., Shaban, A., Cheng, C., Hartley, R., and Boots, B. Intra order-preserving functions for calibration of multi-class neural networks. In Advances in Neural In- formation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.\n\nLearning from noisy labels with deep neural networks: A survey. CoRR, abs. H Song, M Kim, D Park, J Lee, Song, H., Kim, M., Park, D., and Lee, J. Learning from noisy labels with deep neural networks: A survey. CoRR, abs/2007.08199, 2020.\n\nA theory of learning with corrupted labels. B Van Rooyen, R C Williamson, J. Mach. Learn. Res. 1850van Rooyen, B. and Williamson, R. C. A theory of learning with corrupted labels. J. Mach. Learn. Res., 18:228:1- 228:50, 2017.\n\nMulticlass learning with partially corrupted labels. R Wang, T Liu, D Tao, IEEE Trans. Neural Networks Learn. Syst. 296Wang, R., Liu, T., and Tao, D. Multiclass learning with partially corrupted labels. IEEE Trans. Neural Networks Learn. Syst., 29(6):2568-2580, 2018.\n\nComposite multiclass losses. R C Williamson, E Vernet, M D Reid, J. Mach. Learn. Res. 17Williamson, R. C., Vernet, E., and Reid, M. D. Composite multiclass losses. J. Mach. Learn. Res., 17:223:1-223:52, 2016.\n\nAre anchor points really indispensable in label-noise learning?. X Xia, T Liu, N Wang, B Han, C Gong, G Niu, M Sugiyama, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPSXia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M. Are anchor points really indispensable in label-noise learning? In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, pp. 6835-6846, 2019.\n\nDual T: reducing estimation error for transition matrix in label-noise learning. Y Yao, T Liu, B Han, M Gong, J Deng, G Niu, M Sugiyama, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., and Sugiyama, M. Dual T: reducing estimation error for transition matrix in label-noise learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.\n", "annotations": {"author": "[{\"start\":\"73\",\"end\":\"182\"},{\"start\":\"183\",\"end\":\"221\"},{\"start\":\"222\",\"end\":\"332\"}]", "publisher": null, "author_last_name": "[{\"start\":\"82\",\"end\":\"87\"},{\"start\":\"188\",\"end\":\"191\"},{\"start\":\"230\",\"end\":\"237\"}]", "author_first_name": "[{\"start\":\"73\",\"end\":\"81\"},{\"start\":\"183\",\"end\":\"187\"},{\"start\":\"222\",\"end\":\"229\"}]", "author_affiliation": "[{\"start\":\"89\",\"end\":\"181\"},{\"start\":\"193\",\"end\":\"220\"},{\"start\":\"239\",\"end\":\"331\"}]", "title": "[{\"start\":\"1\",\"end\":\"66\"},{\"start\":\"333\",\"end\":\"398\"}]", "venue": "[{\"start\":\"400\",\"end\":\"469\"}]", "abstract": "[{\"start\":\"666\",\"end\":\"2408\"}]", "bib_ref": "[{\"start\":\"2883\",\"end\":\"2909\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"2909\",\"end\":\"2927\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"3149\",\"end\":\"3169\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"3169\",\"end\":\"3188\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"3499\",\"end\":\"3522\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"3522\",\"end\":\"3537\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3537\",\"end\":\"3559\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3559\",\"end\":\"3572\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"3572\",\"end\":\"3594\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3594\",\"end\":\"3620\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"3637\",\"end\":\"3660\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"4025\",\"end\":\"4044\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"4044\",\"end\":\"4065\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"4065\",\"end\":\"4084\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4084\",\"end\":\"4102\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"4670\",\"end\":\"4696\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"4726\",\"end\":\"4749\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"4796\",\"end\":\"4817\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"4844\",\"end\":\"4865\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"6295\",\"end\":\"6309\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"6758\",\"end\":\"6784\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"6789\",\"end\":\"6810\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"7257\",\"end\":\"7278\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"7426\",\"end\":\"7443\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"8076\",\"end\":\"8097\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"8102\",\"end\":\"8119\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"8710\",\"end\":\"8736\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"8741\",\"end\":\"8762\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"8857\",\"end\":\"8880\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"8885\",\"end\":\"8904\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"8906\",\"end\":\"8929\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"9554\",\"end\":\"9573\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"11798\",\"end\":\"11822\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"11822\",\"end\":\"11852\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"18672\",\"end\":\"18686\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"21247\",\"end\":\"21272\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"22416\",\"end\":\"22417\"},{\"start\":\"23327\",\"end\":\"23328\"},{\"start\":\"24491\",\"end\":\"24510\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"24596\",\"end\":\"24617\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"24668\",\"end\":\"24685\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"25668\",\"end\":\"25700\"},{\"start\":\"25902\",\"end\":\"25923\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"25928\",\"end\":\"25945\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"26185\",\"end\":\"26186\"},{\"start\":\"26276\",\"end\":\"26294\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"27799\",\"end\":\"27832\"},{\"start\":\"28510\",\"end\":\"28531\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"30843\",\"end\":\"30872\"},{\"start\":\"31417\",\"end\":\"31443\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"31448\",\"end\":\"31469\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"31626\",\"end\":\"31647\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"31774\",\"end\":\"31795\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"31800\",\"end\":\"31817\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"33285\",\"end\":\"33286\"},{\"start\":\"33709\",\"end\":\"33710\"},{\"start\":\"34485\",\"end\":\"34505\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"34518\",\"end\":\"34545\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"34621\",\"end\":\"34647\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"34674\",\"end\":\"34695\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"34857\",\"end\":\"34878\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"35464\",\"end\":\"35465\"},{\"start\":\"36325\",\"end\":\"36327\"},{\"start\":\"36732\",\"end\":\"36763\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"36763\",\"end\":\"36782\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"36862\",\"end\":\"36883\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"37977\",\"end\":\"37994\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"38632\",\"end\":\"38662\"},{\"start\":\"39088\",\"end\":\"39109\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"39114\",\"end\":\"39131\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"39750\",\"end\":\"39803\"},{\"start\":\"41695\",\"end\":\"41713\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"41713\",\"end\":\"41733\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"45033\",\"end\":\"45054\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"45258\",\"end\":\"45281\",\"attributes\":{\"ref_id\":\"b17\"}}]", "figure": "[{\"start\":\"41991\",\"end\":\"42910\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"42911\",\"end\":\"44477\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2424\",\"end\":\"2928\"},{\"start\":\"2930\",\"end\":\"4103\"},{\"start\":\"4105\",\"end\":\"5117\"},{\"start\":\"5119\",\"end\":\"5941\"},{\"start\":\"5943\",\"end\":\"6710\"},{\"start\":\"6712\",\"end\":\"8359\"},{\"start\":\"8361\",\"end\":\"8516\"},{\"start\":\"8518\",\"end\":\"10167\"},{\"start\":\"10169\",\"end\":\"10448\"},{\"start\":\"10450\",\"end\":\"10871\"},{\"start\":\"10889\",\"end\":\"11726\"},{\"start\":\"11728\",\"end\":\"12105\"},{\"start\":\"12135\",\"end\":\"12412\"},{\"start\":\"12414\",\"end\":\"12953\"},{\"start\":\"13020\",\"end\":\"13316\"},{\"start\":\"13372\",\"end\":\"13412\"},{\"start\":\"13535\",\"end\":\"13579\"},{\"start\":\"13605\",\"end\":\"14511\"},{\"start\":\"14513\",\"end\":\"14622\"},{\"start\":\"14624\",\"end\":\"14764\"},{\"start\":\"14766\",\"end\":\"14873\"},{\"start\":\"14904\",\"end\":\"14913\"},{\"start\":\"14915\",\"end\":\"14921\"},{\"start\":\"14923\",\"end\":\"15011\"},{\"start\":\"15078\",\"end\":\"15484\"},{\"start\":\"15512\",\"end\":\"15686\"},{\"start\":\"15688\",\"end\":\"15714\"},{\"start\":\"16032\",\"end\":\"16150\"},{\"start\":\"16179\",\"end\":\"16187\"},{\"start\":\"16189\",\"end\":\"16216\"},{\"start\":\"16218\",\"end\":\"16306\"},{\"start\":\"16397\",\"end\":\"16710\"},{\"start\":\"16736\",\"end\":\"16972\"},{\"start\":\"16974\",\"end\":\"17294\"},{\"start\":\"17337\",\"end\":\"17923\"},{\"start\":\"17983\",\"end\":\"18249\"},{\"start\":\"18251\",\"end\":\"18829\"},{\"start\":\"18879\",\"end\":\"18969\"},{\"start\":\"18971\",\"end\":\"19093\"},{\"start\":\"19156\",\"end\":\"19275\"},{\"start\":\"19419\",\"end\":\"19714\"},{\"start\":\"19731\",\"end\":\"20318\"},{\"start\":\"20414\",\"end\":\"20753\"},{\"start\":\"20755\",\"end\":\"21047\"},{\"start\":\"21082\",\"end\":\"21646\"},{\"start\":\"21648\",\"end\":\"21779\"},{\"start\":\"21833\",\"end\":\"22620\"},{\"start\":\"22622\",\"end\":\"22624\"},{\"start\":\"22730\",\"end\":\"22877\"},{\"start\":\"23063\",\"end\":\"23511\"},{\"start\":\"23513\",\"end\":\"23620\"},{\"start\":\"23777\",\"end\":\"23918\"},{\"start\":\"23920\",\"end\":\"24022\"},{\"start\":\"24024\",\"end\":\"24061\"},{\"start\":\"24170\",\"end\":\"25805\"},{\"start\":\"25887\",\"end\":\"25976\"},{\"start\":\"25978\",\"end\":\"26413\"},{\"start\":\"26415\",\"end\":\"26620\"},{\"start\":\"26622\",\"end\":\"26944\"},{\"start\":\"26946\",\"end\":\"27278\"},{\"start\":\"27309\",\"end\":\"27769\"},{\"start\":\"27771\",\"end\":\"28239\"},{\"start\":\"28241\",\"end\":\"28564\"},{\"start\":\"28566\",\"end\":\"28697\"},{\"start\":\"28699\",\"end\":\"28728\"},{\"start\":\"28730\",\"end\":\"28789\"},{\"start\":\"28791\",\"end\":\"28955\"},{\"start\":\"29141\",\"end\":\"29684\"},{\"start\":\"29686\",\"end\":\"30042\"},{\"start\":\"30086\",\"end\":\"30980\"},{\"start\":\"30996\",\"end\":\"32178\"},{\"start\":\"32225\",\"end\":\"33082\"},{\"start\":\"33084\",\"end\":\"34322\"},{\"start\":\"34370\",\"end\":\"34993\"},{\"start\":\"34995\",\"end\":\"37686\"},{\"start\":\"37688\",\"end\":\"38999\"},{\"start\":\"39001\",\"end\":\"39966\"},{\"start\":\"39968\",\"end\":\"40598\"},{\"start\":\"40613\",\"end\":\"41447\"},{\"start\":\"41449\",\"end\":\"41881\"},{\"start\":\"41883\",\"end\":\"41990\"}]", "formula": "[{\"start\":\"12106\",\"end\":\"12134\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"12954\",\"end\":\"12986\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"13317\",\"end\":\"13371\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"13413\",\"end\":\"13534\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"13580\",\"end\":\"13604\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"14874\",\"end\":\"14903\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"15012\",\"end\":\"15077\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"15485\",\"end\":\"15511\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"15715\",\"end\":\"16031\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"16151\",\"end\":\"16178\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"16307\",\"end\":\"16396\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"16711\",\"end\":\"16735\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"17924\",\"end\":\"17982\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"19094\",\"end\":\"19155\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"19276\",\"end\":\"19418\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"19715\",\"end\":\"19730\",\"attributes\":{\"id\":\"formula_15\"}},{\"start\":\"21048\",\"end\":\"21081\",\"attributes\":{\"id\":\"formula_16\"}},{\"start\":\"21780\",\"end\":\"21832\",\"attributes\":{\"id\":\"formula_17\"}},{\"start\":\"22625\",\"end\":\"22729\",\"attributes\":{\"id\":\"formula_18\"}},{\"start\":\"22878\",\"end\":\"23062\",\"attributes\":{\"id\":\"formula_19\"}},{\"start\":\"23621\",\"end\":\"23721\",\"attributes\":{\"id\":\"formula_20\"}},{\"start\":\"24062\",\"end\":\"24137\",\"attributes\":{\"id\":\"formula_21\"}},{\"start\":\"27279\",\"end\":\"27308\",\"attributes\":{\"id\":\"formula_22\"}},{\"start\":\"28956\",\"end\":\"29140\",\"attributes\":{\"id\":\"formula_23\"}}]", "table_ref": "[{\"start\":\"39470\",\"end\":\"39477\"},{\"start\":\"39979\",\"end\":\"39986\"}]", "section_header": "[{\"start\":\"2410\",\"end\":\"2422\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"10874\",\"end\":\"10887\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"12988\",\"end\":\"13018\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"17297\",\"end\":\"17335\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"18832\",\"end\":\"18877\",\"attributes\":{\"n\":\"4.1.\"}},{\"start\":\"20321\",\"end\":\"20412\",\"attributes\":{\"n\":\"4.2.\"}},{\"start\":\"23723\",\"end\":\"23775\"},{\"start\":\"24139\",\"end\":\"24168\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"25808\",\"end\":\"25885\",\"attributes\":{\"n\":\"5.1.\"}},{\"start\":\"30045\",\"end\":\"30084\",\"attributes\":{\"n\":\"5.2.\"}},{\"start\":\"30983\",\"end\":\"30994\",\"attributes\":{\"n\":\"6.\"}},{\"start\":\"32181\",\"end\":\"32223\",\"attributes\":{\"n\":\"6.1.\"}},{\"start\":\"34325\",\"end\":\"34368\",\"attributes\":{\"n\":\"6.2.\"}},{\"start\":\"40601\",\"end\":\"40611\",\"attributes\":{\"n\":\"7.\"}}]", "table": "[{\"start\":\"43141\",\"end\":\"44477\"}]", "figure_caption": "[{\"start\":\"41993\",\"end\":\"42910\"},{\"start\":\"42913\",\"end\":\"43141\"}]", "figure_ref": "[{\"start\":\"33481\",\"end\":\"33489\"},{\"start\":\"33810\",\"end\":\"33818\"},{\"start\":\"35713\",\"end\":\"35721\"}]", "bib_author_first_name": "[{\"start\":\"46075\",\"end\":\"46076\"},{\"start\":\"46301\",\"end\":\"46302\"},{\"start\":\"46312\",\"end\":\"46313\"},{\"start\":\"46314\",\"end\":\"46315\"},{\"start\":\"46485\",\"end\":\"46486\"},{\"start\":\"46487\",\"end\":\"46488\"},{\"start\":\"46496\",\"end\":\"46497\"},{\"start\":\"46498\",\"end\":\"46499\"},{\"start\":\"46715\",\"end\":\"46716\"},{\"start\":\"46723\",\"end\":\"46724\"},{\"start\":\"46725\",\"end\":\"46726\"},{\"start\":\"47166\",\"end\":\"47167\"},{\"start\":\"47615\",\"end\":\"47616\"},{\"start\":\"47631\",\"end\":\"47632\"},{\"start\":\"47645\",\"end\":\"47646\"},{\"start\":\"47656\",\"end\":\"47657\"},{\"start\":\"47666\",\"end\":\"47667\"},{\"start\":\"47668\",\"end\":\"47669\"},{\"start\":\"47925\",\"end\":\"47926\"},{\"start\":\"47934\",\"end\":\"47935\"},{\"start\":\"47941\",\"end\":\"47942\"},{\"start\":\"47958\",\"end\":\"47959\"},{\"start\":\"48426\",\"end\":\"48427\"},{\"start\":\"48435\",\"end\":\"48436\"},{\"start\":\"48445\",\"end\":\"48451\"},{\"start\":\"48454\",\"end\":\"48455\"},{\"start\":\"48804\",\"end\":\"48805\"},{\"start\":\"48814\",\"end\":\"48815\"},{\"start\":\"49087\",\"end\":\"49088\"},{\"start\":\"49096\",\"end\":\"49097\"},{\"start\":\"49105\",\"end\":\"49106\"},{\"start\":\"49107\",\"end\":\"49108\"},{\"start\":\"49533\",\"end\":\"49534\"},{\"start\":\"49540\",\"end\":\"49541\"},{\"start\":\"49550\",\"end\":\"49551\"},{\"start\":\"49557\",\"end\":\"49558\"},{\"start\":\"49559\",\"end\":\"49560\"},{\"start\":\"50011\",\"end\":\"50012\"},{\"start\":\"50017\",\"end\":\"50018\"},{\"start\":\"50026\",\"end\":\"50027\"},{\"start\":\"50033\",\"end\":\"50034\"},{\"start\":\"50384\",\"end\":\"50385\"},{\"start\":\"50386\",\"end\":\"50387\"},{\"start\":\"50569\",\"end\":\"50570\"},{\"start\":\"50583\",\"end\":\"50584\"},{\"start\":\"50906\",\"end\":\"50907\"},{\"start\":\"50915\",\"end\":\"50916\"},{\"start\":\"50925\",\"end\":\"50926\"},{\"start\":\"50935\",\"end\":\"50936\"},{\"start\":\"51219\",\"end\":\"51220\"},{\"start\":\"51221\",\"end\":\"51222\"},{\"start\":\"51230\",\"end\":\"51231\"},{\"start\":\"51244\",\"end\":\"51245\"},{\"start\":\"51246\",\"end\":\"51247\"},{\"start\":\"51253\",\"end\":\"51254\"},{\"start\":\"51811\",\"end\":\"51812\"},{\"start\":\"51813\",\"end\":\"51814\"},{\"start\":\"51822\",\"end\":\"51823\"},{\"start\":\"51836\",\"end\":\"51837\"},{\"start\":\"52046\",\"end\":\"52047\"},{\"start\":\"52059\",\"end\":\"52060\"},{\"start\":\"52061\",\"end\":\"52062\"},{\"start\":\"52072\",\"end\":\"52073\"},{\"start\":\"52085\",\"end\":\"52086\"},{\"start\":\"52515\",\"end\":\"52516\"},{\"start\":\"52525\",\"end\":\"52526\"},{\"start\":\"52534\",\"end\":\"52535\"},{\"start\":\"52543\",\"end\":\"52544\"},{\"start\":\"52552\",\"end\":\"52553\"},{\"start\":\"52564\",\"end\":\"52565\"},{\"start\":\"52574\",\"end\":\"52575\"},{\"start\":\"52585\",\"end\":\"52586\"},{\"start\":\"52592\",\"end\":\"52593\"},{\"start\":\"52606\",\"end\":\"52607\"},{\"start\":\"52616\",\"end\":\"52617\"},{\"start\":\"52629\",\"end\":\"52630\"},{\"start\":\"52637\",\"end\":\"52638\"},{\"start\":\"52645\",\"end\":\"52646\"},{\"start\":\"52655\",\"end\":\"52656\"},{\"start\":\"52665\",\"end\":\"52666\"},{\"start\":\"52675\",\"end\":\"52676\"},{\"start\":\"52691\",\"end\":\"52692\"},{\"start\":\"52702\",\"end\":\"52703\"},{\"start\":\"52710\",\"end\":\"52711\"},{\"start\":\"52717\",\"end\":\"52718\"},{\"start\":\"53326\",\"end\":\"53327\"},{\"start\":\"53337\",\"end\":\"53338\"},{\"start\":\"53346\",\"end\":\"53347\"},{\"start\":\"53348\",\"end\":\"53349\"},{\"start\":\"53357\",\"end\":\"53358\"},{\"start\":\"53365\",\"end\":\"53366\"},{\"start\":\"53800\",\"end\":\"53801\"},{\"start\":\"53810\",\"end\":\"53811\"},{\"start\":\"53820\",\"end\":\"53821\"},{\"start\":\"53829\",\"end\":\"53830\"},{\"start\":\"53840\",\"end\":\"53841\"},{\"start\":\"54331\",\"end\":\"54332\"},{\"start\":\"54339\",\"end\":\"54340\"},{\"start\":\"54346\",\"end\":\"54347\"},{\"start\":\"54354\",\"end\":\"54355\"},{\"start\":\"54539\",\"end\":\"54540\"},{\"start\":\"54553\",\"end\":\"54554\"},{\"start\":\"54555\",\"end\":\"54556\"},{\"start\":\"54775\",\"end\":\"54776\"},{\"start\":\"54783\",\"end\":\"54784\"},{\"start\":\"54790\",\"end\":\"54791\"},{\"start\":\"55020\",\"end\":\"55021\"},{\"start\":\"55022\",\"end\":\"55023\"},{\"start\":\"55036\",\"end\":\"55037\"},{\"start\":\"55046\",\"end\":\"55047\"},{\"start\":\"55048\",\"end\":\"55049\"},{\"start\":\"55266\",\"end\":\"55267\"},{\"start\":\"55273\",\"end\":\"55274\"},{\"start\":\"55280\",\"end\":\"55281\"},{\"start\":\"55288\",\"end\":\"55289\"},{\"start\":\"55295\",\"end\":\"55296\"},{\"start\":\"55303\",\"end\":\"55304\"},{\"start\":\"55310\",\"end\":\"55311\"},{\"start\":\"55819\",\"end\":\"55820\"},{\"start\":\"55826\",\"end\":\"55827\"},{\"start\":\"55833\",\"end\":\"55834\"},{\"start\":\"55840\",\"end\":\"55841\"},{\"start\":\"55848\",\"end\":\"55849\"},{\"start\":\"55856\",\"end\":\"55857\"},{\"start\":\"55863\",\"end\":\"55864\"}]", "bib_author_last_name": "[{\"start\":\"46077\",\"end\":\"46084\"},{\"start\":\"46303\",\"end\":\"46310\"},{\"start\":\"46316\",\"end\":\"46321\"},{\"start\":\"46489\",\"end\":\"46494\"},{\"start\":\"46500\",\"end\":\"46507\"},{\"start\":\"46717\",\"end\":\"46721\"},{\"start\":\"46727\",\"end\":\"46735\"},{\"start\":\"47168\",\"end\":\"47176\"},{\"start\":\"47617\",\"end\":\"47629\"},{\"start\":\"47633\",\"end\":\"47643\"},{\"start\":\"47647\",\"end\":\"47654\"},{\"start\":\"47658\",\"end\":\"47664\"},{\"start\":\"47670\",\"end\":\"47675\"},{\"start\":\"47927\",\"end\":\"47932\"},{\"start\":\"47936\",\"end\":\"47939\"},{\"start\":\"47943\",\"end\":\"47956\"},{\"start\":\"47960\",\"end\":\"47963\"},{\"start\":\"48428\",\"end\":\"48433\"},{\"start\":\"48437\",\"end\":\"48443\"},{\"start\":\"48806\",\"end\":\"48812\"},{\"start\":\"48816\",\"end\":\"48825\"},{\"start\":\"49089\",\"end\":\"49094\"},{\"start\":\"49098\",\"end\":\"49103\"},{\"start\":\"49109\",\"end\":\"49115\"},{\"start\":\"49535\",\"end\":\"49538\"},{\"start\":\"49542\",\"end\":\"49548\"},{\"start\":\"49552\",\"end\":\"49555\"},{\"start\":\"49561\",\"end\":\"49571\"},{\"start\":\"50013\",\"end\":\"50015\"},{\"start\":\"50019\",\"end\":\"50024\"},{\"start\":\"50028\",\"end\":\"50031\"},{\"start\":\"50035\",\"end\":\"50038\"},{\"start\":\"50388\",\"end\":\"50394\"},{\"start\":\"50571\",\"end\":\"50581\"},{\"start\":\"50585\",\"end\":\"50591\"},{\"start\":\"50908\",\"end\":\"50913\"},{\"start\":\"50917\",\"end\":\"50923\"},{\"start\":\"50927\",\"end\":\"50933\"},{\"start\":\"50937\",\"end\":\"50944\"},{\"start\":\"51223\",\"end\":\"51228\"},{\"start\":\"51232\",\"end\":\"51242\"},{\"start\":\"51248\",\"end\":\"51251\"},{\"start\":\"51255\",\"end\":\"51265\"},{\"start\":\"51815\",\"end\":\"51820\"},{\"start\":\"51824\",\"end\":\"51834\"},{\"start\":\"51838\",\"end\":\"51847\"},{\"start\":\"52048\",\"end\":\"52057\"},{\"start\":\"52063\",\"end\":\"52070\"},{\"start\":\"52074\",\"end\":\"52083\"},{\"start\":\"52087\",\"end\":\"52093\"},{\"start\":\"52517\",\"end\":\"52523\"},{\"start\":\"52527\",\"end\":\"52532\"},{\"start\":\"52536\",\"end\":\"52541\"},{\"start\":\"52545\",\"end\":\"52550\"},{\"start\":\"52554\",\"end\":\"52562\"},{\"start\":\"52566\",\"end\":\"52572\"},{\"start\":\"52576\",\"end\":\"52583\"},{\"start\":\"52587\",\"end\":\"52590\"},{\"start\":\"52594\",\"end\":\"52604\"},{\"start\":\"52608\",\"end\":\"52614\"},{\"start\":\"52618\",\"end\":\"52627\"},{\"start\":\"52631\",\"end\":\"52635\"},{\"start\":\"52639\",\"end\":\"52643\"},{\"start\":\"52647\",\"end\":\"52653\"},{\"start\":\"52657\",\"end\":\"52663\"},{\"start\":\"52667\",\"end\":\"52673\"},{\"start\":\"52677\",\"end\":\"52689\"},{\"start\":\"52693\",\"end\":\"52700\"},{\"start\":\"52704\",\"end\":\"52708\"},{\"start\":\"52712\",\"end\":\"52715\"},{\"start\":\"52719\",\"end\":\"52727\"},{\"start\":\"52729\",\"end\":\"52736\"},{\"start\":\"53328\",\"end\":\"53335\"},{\"start\":\"53339\",\"end\":\"53344\"},{\"start\":\"53350\",\"end\":\"53355\"},{\"start\":\"53359\",\"end\":\"53363\"},{\"start\":\"53367\",\"end\":\"53369\"},{\"start\":\"53802\",\"end\":\"53808\"},{\"start\":\"53812\",\"end\":\"53818\"},{\"start\":\"53822\",\"end\":\"53827\"},{\"start\":\"53831\",\"end\":\"53838\"},{\"start\":\"53842\",\"end\":\"53847\"},{\"start\":\"54333\",\"end\":\"54337\"},{\"start\":\"54341\",\"end\":\"54344\"},{\"start\":\"54348\",\"end\":\"54352\"},{\"start\":\"54356\",\"end\":\"54359\"},{\"start\":\"54541\",\"end\":\"54551\"},{\"start\":\"54557\",\"end\":\"54567\"},{\"start\":\"54777\",\"end\":\"54781\"},{\"start\":\"54785\",\"end\":\"54788\"},{\"start\":\"54792\",\"end\":\"54795\"},{\"start\":\"55024\",\"end\":\"55034\"},{\"start\":\"55038\",\"end\":\"55044\"},{\"start\":\"55050\",\"end\":\"55054\"},{\"start\":\"55268\",\"end\":\"55271\"},{\"start\":\"55275\",\"end\":\"55278\"},{\"start\":\"55282\",\"end\":\"55286\"},{\"start\":\"55290\",\"end\":\"55293\"},{\"start\":\"55297\",\"end\":\"55301\"},{\"start\":\"55305\",\"end\":\"55308\"},{\"start\":\"55312\",\"end\":\"55320\"},{\"start\":\"55821\",\"end\":\"55824\"},{\"start\":\"55828\",\"end\":\"55831\"},{\"start\":\"55835\",\"end\":\"55838\"},{\"start\":\"55842\",\"end\":\"55846\"},{\"start\":\"55850\",\"end\":\"55854\"},{\"start\":\"55858\",\"end\":\"55861\"},{\"start\":\"55865\",\"end\":\"55873\"}]", "bib_entry": "[{\"start\":\"46001\",\"end\":\"46269\",\"attributes\":{\"matched_paper_id\":\"14030887\",\"id\":\"b0\"}},{\"start\":\"46271\",\"end\":\"46430\",\"attributes\":{\"matched_paper_id\":\"5508562\",\"id\":\"b1\"}},{\"start\":\"46432\",\"end\":\"46658\",\"attributes\":{\"matched_paper_id\":\"15465469\",\"id\":\"b2\"}},{\"start\":\"46660\",\"end\":\"47087\",\"attributes\":{\"matched_paper_id\":\"207228399\",\"id\":\"b3\"}},{\"start\":\"47089\",\"end\":\"47546\",\"attributes\":{\"matched_paper_id\":\"16357542\",\"id\":\"b4\"}},{\"start\":\"47548\",\"end\":\"47859\",\"attributes\":{\"matched_paper_id\":\"9383472\",\"id\":\"b5\"}},{\"start\":\"47861\",\"end\":\"48382\",\"attributes\":{\"matched_paper_id\":\"34957763\",\"id\":\"b6\"}},{\"start\":\"48384\",\"end\":\"48745\",\"attributes\":{\"matched_paper_id\":\"7426093\",\"id\":\"b7\"}},{\"start\":\"48747\",\"end\":\"49019\",\"attributes\":{\"matched_paper_id\":\"6054025\",\"id\":\"b8\"}},{\"start\":\"49021\",\"end\":\"49489\",\"attributes\":{\"matched_paper_id\":\"6546734\",\"id\":\"b9\"}},{\"start\":\"49491\",\"end\":\"49963\",\"attributes\":{\"matched_paper_id\":\"28671436\",\"id\":\"b10\"}},{\"start\":\"49965\",\"end\":\"50322\",\"attributes\":{\"matched_paper_id\":\"206594692\",\"id\":\"b11\"}},{\"start\":\"50324\",\"end\":\"50512\",\"attributes\":{\"matched_paper_id\":\"6392609\",\"id\":\"b12\"}},{\"start\":\"50514\",\"end\":\"50823\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"50825\",\"end\":\"51146\",\"attributes\":{\"id\":\"b14\",\"doi\":\"10.1109/5.726791\"}},{\"start\":\"51148\",\"end\":\"51750\",\"attributes\":{\"matched_paper_id\":\"10708717\",\"id\":\"b15\"}},{\"start\":\"51752\",\"end\":\"52016\",\"attributes\":{\"matched_paper_id\":\"46984371\",\"id\":\"b16\"}},{\"start\":\"52018\",\"end\":\"52452\",\"attributes\":{\"matched_paper_id\":\"423350\",\"id\":\"b17\"}},{\"start\":\"52454\",\"end\":\"53245\",\"attributes\":{\"matched_paper_id\":\"202786778\",\"id\":\"b18\"}},{\"start\":\"53247\",\"end\":\"53717\",\"attributes\":{\"matched_paper_id\":\"16406313\",\"id\":\"b19\"}},{\"start\":\"53719\",\"end\":\"54254\",\"attributes\":{\"matched_paper_id\":\"212725550\",\"id\":\"b20\"}},{\"start\":\"54256\",\"end\":\"54493\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"54495\",\"end\":\"54720\",\"attributes\":{\"matched_paper_id\":\"51918644\",\"id\":\"b22\"}},{\"start\":\"54722\",\"end\":\"54989\",\"attributes\":{\"matched_paper_id\":\"21685171\",\"id\":\"b23\"}},{\"start\":\"54991\",\"end\":\"55199\",\"attributes\":{\"matched_paper_id\":\"12435258\",\"id\":\"b24\"}},{\"start\":\"55201\",\"end\":\"55736\",\"attributes\":{\"matched_paper_id\":\"173991056\",\"id\":\"b25\"}},{\"start\":\"55738\",\"end\":\"56289\",\"attributes\":{\"matched_paper_id\":\"219687353\",\"id\":\"b26\"}}]", "bib_title": "[{\"start\":\"46001\",\"end\":\"46073\"},{\"start\":\"46271\",\"end\":\"46299\"},{\"start\":\"46432\",\"end\":\"46483\"},{\"start\":\"46660\",\"end\":\"46713\"},{\"start\":\"47089\",\"end\":\"47164\"},{\"start\":\"47548\",\"end\":\"47613\"},{\"start\":\"47861\",\"end\":\"47923\"},{\"start\":\"48384\",\"end\":\"48424\"},{\"start\":\"48747\",\"end\":\"48802\"},{\"start\":\"49021\",\"end\":\"49085\"},{\"start\":\"49491\",\"end\":\"49531\"},{\"start\":\"49965\",\"end\":\"50009\"},{\"start\":\"50324\",\"end\":\"50382\"},{\"start\":\"51148\",\"end\":\"51217\"},{\"start\":\"51752\",\"end\":\"51809\"},{\"start\":\"52018\",\"end\":\"52044\"},{\"start\":\"52454\",\"end\":\"52513\"},{\"start\":\"53247\",\"end\":\"53324\"},{\"start\":\"53719\",\"end\":\"53798\"},{\"start\":\"54495\",\"end\":\"54537\"},{\"start\":\"54722\",\"end\":\"54773\"},{\"start\":\"54991\",\"end\":\"55018\"},{\"start\":\"55201\",\"end\":\"55264\"},{\"start\":\"55738\",\"end\":\"55817\"}]", "bib_author": "[{\"start\":\"46075\",\"end\":\"46086\"},{\"start\":\"46301\",\"end\":\"46312\"},{\"start\":\"46312\",\"end\":\"46323\"},{\"start\":\"46485\",\"end\":\"46496\"},{\"start\":\"46496\",\"end\":\"46509\"},{\"start\":\"46715\",\"end\":\"46723\"},{\"start\":\"46723\",\"end\":\"46737\"},{\"start\":\"47166\",\"end\":\"47178\"},{\"start\":\"47615\",\"end\":\"47631\"},{\"start\":\"47631\",\"end\":\"47645\"},{\"start\":\"47645\",\"end\":\"47656\"},{\"start\":\"47656\",\"end\":\"47666\"},{\"start\":\"47666\",\"end\":\"47677\"},{\"start\":\"47925\",\"end\":\"47934\"},{\"start\":\"47934\",\"end\":\"47941\"},{\"start\":\"47941\",\"end\":\"47958\"},{\"start\":\"47958\",\"end\":\"47965\"},{\"start\":\"48426\",\"end\":\"48435\"},{\"start\":\"48435\",\"end\":\"48445\"},{\"start\":\"48445\",\"end\":\"48454\"},{\"start\":\"48454\",\"end\":\"48458\"},{\"start\":\"48804\",\"end\":\"48814\"},{\"start\":\"48814\",\"end\":\"48827\"},{\"start\":\"49087\",\"end\":\"49096\"},{\"start\":\"49096\",\"end\":\"49105\"},{\"start\":\"49105\",\"end\":\"49117\"},{\"start\":\"49533\",\"end\":\"49540\"},{\"start\":\"49540\",\"end\":\"49550\"},{\"start\":\"49550\",\"end\":\"49557\"},{\"start\":\"49557\",\"end\":\"49573\"},{\"start\":\"50011\",\"end\":\"50017\"},{\"start\":\"50017\",\"end\":\"50026\"},{\"start\":\"50026\",\"end\":\"50033\"},{\"start\":\"50033\",\"end\":\"50040\"},{\"start\":\"50384\",\"end\":\"50396\"},{\"start\":\"50569\",\"end\":\"50583\"},{\"start\":\"50583\",\"end\":\"50593\"},{\"start\":\"50906\",\"end\":\"50915\"},{\"start\":\"50915\",\"end\":\"50925\"},{\"start\":\"50925\",\"end\":\"50935\"},{\"start\":\"50935\",\"end\":\"50946\"},{\"start\":\"51219\",\"end\":\"51230\"},{\"start\":\"51230\",\"end\":\"51244\"},{\"start\":\"51244\",\"end\":\"51253\"},{\"start\":\"51253\",\"end\":\"51267\"},{\"start\":\"51811\",\"end\":\"51822\"},{\"start\":\"51822\",\"end\":\"51836\"},{\"start\":\"51836\",\"end\":\"51849\"},{\"start\":\"52046\",\"end\":\"52059\"},{\"start\":\"52059\",\"end\":\"52072\"},{\"start\":\"52072\",\"end\":\"52085\"},{\"start\":\"52085\",\"end\":\"52095\"},{\"start\":\"52515\",\"end\":\"52525\"},{\"start\":\"52525\",\"end\":\"52534\"},{\"start\":\"52534\",\"end\":\"52543\"},{\"start\":\"52543\",\"end\":\"52552\"},{\"start\":\"52552\",\"end\":\"52564\"},{\"start\":\"52564\",\"end\":\"52574\"},{\"start\":\"52574\",\"end\":\"52585\"},{\"start\":\"52585\",\"end\":\"52592\"},{\"start\":\"52592\",\"end\":\"52606\"},{\"start\":\"52606\",\"end\":\"52616\"},{\"start\":\"52616\",\"end\":\"52629\"},{\"start\":\"52629\",\"end\":\"52637\"},{\"start\":\"52637\",\"end\":\"52645\"},{\"start\":\"52645\",\"end\":\"52655\"},{\"start\":\"52655\",\"end\":\"52665\"},{\"start\":\"52665\",\"end\":\"52675\"},{\"start\":\"52675\",\"end\":\"52691\"},{\"start\":\"52691\",\"end\":\"52702\"},{\"start\":\"52702\",\"end\":\"52710\"},{\"start\":\"52710\",\"end\":\"52717\"},{\"start\":\"52717\",\"end\":\"52729\"},{\"start\":\"52729\",\"end\":\"52738\"},{\"start\":\"53326\",\"end\":\"53337\"},{\"start\":\"53337\",\"end\":\"53346\"},{\"start\":\"53346\",\"end\":\"53357\"},{\"start\":\"53357\",\"end\":\"53365\"},{\"start\":\"53365\",\"end\":\"53371\"},{\"start\":\"53800\",\"end\":\"53810\"},{\"start\":\"53810\",\"end\":\"53820\"},{\"start\":\"53820\",\"end\":\"53829\"},{\"start\":\"53829\",\"end\":\"53840\"},{\"start\":\"53840\",\"end\":\"53849\"},{\"start\":\"54331\",\"end\":\"54339\"},{\"start\":\"54339\",\"end\":\"54346\"},{\"start\":\"54346\",\"end\":\"54354\"},{\"start\":\"54354\",\"end\":\"54361\"},{\"start\":\"54539\",\"end\":\"54553\"},{\"start\":\"54553\",\"end\":\"54569\"},{\"start\":\"54775\",\"end\":\"54783\"},{\"start\":\"54783\",\"end\":\"54790\"},{\"start\":\"54790\",\"end\":\"54797\"},{\"start\":\"55020\",\"end\":\"55036\"},{\"start\":\"55036\",\"end\":\"55046\"},{\"start\":\"55046\",\"end\":\"55056\"},{\"start\":\"55266\",\"end\":\"55273\"},{\"start\":\"55273\",\"end\":\"55280\"},{\"start\":\"55280\",\"end\":\"55288\"},{\"start\":\"55288\",\"end\":\"55295\"},{\"start\":\"55295\",\"end\":\"55303\"},{\"start\":\"55303\",\"end\":\"55310\"},{\"start\":\"55310\",\"end\":\"55322\"},{\"start\":\"55819\",\"end\":\"55826\"},{\"start\":\"55826\",\"end\":\"55833\"},{\"start\":\"55833\",\"end\":\"55840\"},{\"start\":\"55840\",\"end\":\"55848\"},{\"start\":\"55848\",\"end\":\"55856\"},{\"start\":\"55856\",\"end\":\"55863\"},{\"start\":\"55863\",\"end\":\"55875\"}]", "bib_venue": "[{\"start\":\"46086\",\"end\":\"46122\"},{\"start\":\"46323\",\"end\":\"46334\"},{\"start\":\"46509\",\"end\":\"46527\"},{\"start\":\"46737\",\"end\":\"46815\"},{\"start\":\"47178\",\"end\":\"47259\"},{\"start\":\"47677\",\"end\":\"47683\"},{\"start\":\"47965\",\"end\":\"48033\"},{\"start\":\"48458\",\"end\":\"48528\"},{\"start\":\"48827\",\"end\":\"48866\"},{\"start\":\"49117\",\"end\":\"49191\"},{\"start\":\"49573\",\"end\":\"49641\"},{\"start\":\"50040\",\"end\":\"50103\"},{\"start\":\"50396\",\"end\":\"50402\"},{\"start\":\"50514\",\"end\":\"50567\"},{\"start\":\"50825\",\"end\":\"50904\"},{\"start\":\"51267\",\"end\":\"51335\"},{\"start\":\"51849\",\"end\":\"51860\"},{\"start\":\"52095\",\"end\":\"52212\"},{\"start\":\"52738\",\"end\":\"52787\"},{\"start\":\"53371\",\"end\":\"53434\"},{\"start\":\"53849\",\"end\":\"53966\"},{\"start\":\"54256\",\"end\":\"54329\"},{\"start\":\"54569\",\"end\":\"54588\"},{\"start\":\"54797\",\"end\":\"54836\"},{\"start\":\"55056\",\"end\":\"55075\"},{\"start\":\"55322\",\"end\":\"55434\"},{\"start\":\"55875\",\"end\":\"55992\"},{\"start\":\"46817\",\"end\":\"46884\"},{\"start\":\"47261\",\"end\":\"47331\"},{\"start\":\"48035\",\"end\":\"48088\"},{\"start\":\"48530\",\"end\":\"48585\"},{\"start\":\"49193\",\"end\":\"49252\"},{\"start\":\"49643\",\"end\":\"49696\"},{\"start\":\"51337\",\"end\":\"51390\"},{\"start\":\"55436\",\"end\":\"55443\"}]"}}}, "year": 2023, "month": 12, "day": 17}