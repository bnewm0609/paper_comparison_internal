{"id": 259982476, "updated": "2023-10-04 22:03:44.496", "metadata": {"title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats", "authors": "[{\"first\":\"Xiaoxia\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Zhewei\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Yuxiong\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2307.09782", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2307-09782", "doi": "10.48550/arxiv.2307.09782"}}, "content": {"source": {"pdf_hash": "aeb9454987c3f85563cf7a5d2cb7f3d502d3398d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2307.09782v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "666e3dbcaa6d7adb752bcb44bbb31815676204f6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aeb9454987c3f85563cf7a5d2cb7f3d502d3398d.txt", "contents": "\nZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats\n\n\nXiaoxia Wu xiaoxiawu@microsoft.com \nZhewei Yao zheweiyao@microsoft.com \nYuxiong He Microsoft \nZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats\n\nIn the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings. * Equal Contribution. Code will be released as a part of https://github.com/microsoft/DeepSpeed\n\nIntroduction\n\nAs Natural Language Processing (NLP) evolves, Large Language Models (LLMs) like Codex [9] and Chat-GPT [22] have become essential, transforming our interaction with technology and daily communication. However, their complexity and computational intensity present deployment challenges [23,8,26], particularly in resource-limited settings. One solution is quantization, which represents data in lower-precision formats such as 8-bit integers or floating-point numbers, reducing memory needs and potentially enhancing inference latency through better GEMM computation throughput on compatible GPUs. Post-Training Quantization (PTQ), which directly reduces the precision of a fully trained model's parameters, is often preferred for LLMs due to its simplicity and lower computational overhead. 1 Recent studies indicate that PTQ on 8-bit integer (INT8) weight-only quantization does not compromise the quality of LLMs [34,3,33,29], and only a minor accuracy drop is observed with INT4 weight quantization when advanced algorithm such as GPTQ applied [7,35,12,15].\n\nThe exploration of activation quantization, in addition to weight-only quantization, has also gained interest. This approach expedites inference times by taking advantage of unified precision leading to more efficient execution on hardware. The primary challenge in implementing activation quantization lies in the trade-off between efficiency and performance. As evidenced in studies such as ZeroQuants [34,35], SmoothQuant [33] and others, reducing the precision of activation from FP16 to INT8 inevitably results in a decrease in model quality. This degradation is partially due to the presence of extreme values or outliers in the activation of LLMs [5,33,15,12], which is partly attributed to the pretraining effect [31]. In the presence of outliers, uniform quantization like INT8 or INT4, fail to accurately represent the main body of the data as they become skewed towards the outlier. This issue stems from the inherent assumption in these techniques of a uniform data distribution [30], an assumption that might not correspond to the actual data points distribution.\n\nConsidering the drawbacks of integer quantization delineated previously, floating-point (FP) methods like FP8 or FP4, employing ExMy notation, arise as more potent alternatives [20,2,13,28,37]. Unlike the fixed range of integer types, floating-point methods allow for adjusting the decimal point position, enabling dynamic scaling across activation maps and preserving important features. While there is debate about the quality of models between integer and floating-point quantization [28], recent research on PTQ LLMs using FP8/FP4 in [37] reveals FP8 to be substantially better than INT8 activation quantization. In terms of hardware support and performance, while INT8 computations are broadly supported by most modern CPUs and GPUs [21,31], lower-bit floating-point operations are also increasingly recognized in the industry. An example of this is the newly release of NVIDIA's H100 GPU, specifically engineered for FP8 computations [20]. Hence, despite the potentially higher computation cost of FP8 compared to INT8 and in light of hardware support, the improved model quality could make this trade-off worthwhile and merits further exploration.\n\nWhile a few studies such as the one by [37] have ventured into the realm of post-training FP quantization in LLMs, they have unveiled considerable drawbacks in terms of model quality. Specifically, when implementing GPTQ [7] for FP8 quantization on both weights and activation for models such as LLaMA-7B or LLaMA-30b [27], there is an observed perplexity degradation surpassing 1.0 on Wiki-text2 dataset [19]. This level of model degradation presents significant practicality issues, hindering the optimal utilization of these models. In response to these findings, our paper undertakes an in-depth exploration into FP quantization. We primarily focus on the variance in activation values-an integral element that could potentially be the key to enhancing the performance of these quantization techniques. Our main contributions include:\n\n\u2022 Demonstrating minimal degradation with FP8 activation and weight quantization: Particularly in larger models, FP8 activation and weight quantization result in negligible degradation, performing comparably to the original FP16 models.\n\n\u2022 Identifying potential in FP8 activation and FP4 weights, and the impact of Low Rank Compensation (LoRC): We highlight the potential in FP8 activation and FP4 weights. The LoRC method, proposed in [35], significantly reduces quantization errors in the W4A8 scheme for FP quantization, especially in smaller models, thereby enhancing performance.\n\n\u2022 Illustrating the maintenance of quality in the W4A8 floating-point model even when constraints are imposed on the scaling factors: For true efficiency in the W4A8 model, a conversion from FP4 to FP8 for weight is crucial. To alleviate this converting overhead, we here suggest two possible scaling constraints for weight quantization: (1) restricting all scaling factors to be a power of 2 and (2) requiring the scaling factors in one compute group (e.g., several rows of the weight matrix [34] to be transferable by simple bit-shifting). Our analysis indicates that these two restrictions negligibly affect the model's performance in comparison to the conventional W4A8 configuration.\n\n\nBackground\n\nThe impact of 8-bit activation quantization, especially potential accuracy loss, is comprehensively outlined in ZeroQuant-V2 [35]. They present a direct comparison between the W16A16 and W16A8 (INT8) quantization schemes across a variety of models. To provide an easier understanding, we quoted their results in Table 1 for both OPT [36] and BLOOM [25] models, which indicates that the quality of models, especially the OPT family, is significantly influenced by the activation quantization.   Distribution of Activations. We sought to understand the cause of the aforementioned degradation from FP16 activation and INT8, prompting us to scrutinize the distribution of activation values illustrated in Figure 1. We selected a random sentence from the C4 dataset and processed it through a pre-trained OPT-1.3B model. The statistical activation inputs for the 2nd, middle, and final layer were subsequently chosen for a detailed examination. The four histograms correspondingly represent the activations for the Multi-head Attention (MHA) and Multi-Layer Perceptron (MLP) components: 2 \u2022 attn.q_proj, the input for the query, key, or value in the MHA mechanism,\n\n\u2022 attn.out_proj, the input for the MHA's projection matrices,\n\n\u2022 fc1, the initial input for the fully-connected (fc1) projection in MLP,\n\n\u2022 fc2, the subsequent input for the fully-connected (fc2) projection in the MLP.\n\nThe activation distribution outlined in Figure 1 reveals some compelling patterns. The input to the attn.q_proj module in the 2nd layer (depicted in the 1st column of Figure 1 in the top row) appears to conform closely to a normal distribution, a result of the layer-normalization process. Yet, moving forward to the subsequent modules within the 2nd layer, namely attn.out_proj, fc1, and fc2, we notice a skewness in the distributions, with noticeable outlier values. Two distinct observations arise: (1) Regarding the activation distribution for attn.q_proj and fc1, even though they have undergone layer-normalization, the skewness still presents itself and becomes more conspicuous as we delve deeper into the layers (see the first and third column in the middle and bottom plots in Figure 1). (2) The skewness reaches its peak in the fc2 module. In this particular module, a large portion of the values cluster around zero, with only a handful surpassing this range. This phenomenon is due to the inputs being processed by the \"ReLU\" (Rectified Linear Unit) operator. This operator, purposefully, voids any negative input values, resulting in a skewed distribution focused around zero. Only positive activation values persist unmodified, giving rise to the outliers observed. This extreme skewness is most noticeable at the final layer (the bottom row in Figure 1).\n\nThese observations offer a deeper understanding of how activation quantization impacts various modules, even within the same layer. Consequently, this signifies that we must exercise caution when selecting quantization methods. Quantization techniques that employ integers, such as INT8 or INT4, and rely on uniform quantization, may not be ideally suited to manage distributions that are skewed. This is due to the inherent assumption of uniform distribution within these methods, which may not align with the actual distribution of data points.\n\nThe Uniform Quantization of INT8. The integer quantization such as in INT8 or INT4 states\nQ(x) = INT (x \u2212 Z)/S \u2212 Z,(1)\nwhere Q is the quantization function, x is a floating point input vector/tensor, S is a real valued scaling factor, and Z is an integer zero point. Based on different settings, the quantization method can be viewed as symmetric (Z = 0) or asymmetric (Z = 0) quantization. In scenarios where outliers exist, uniform quantization techniques like INT8 and INT4, regardless of their symmetric or asymmetric variants, frequently fail to accurately approximate the values of clustered data. Consequently, this makes the quantization error larger for those clustered values, as these methods attempt to adjust their fit to accommodate the outlier. Essentially, these techniques become skewed towards the outlier, leading to a reduced accuracy in representing the main body of the data. Given the limitations of integer quantization, floating-point methods such as FP8 or FP4, utilizing ExMy notation, emerge as superior alternatives. In these methods, the 'x' and 'y' values represent the bits allocated for the exponent and mantissa, respectively, totaling to 7 in FP8 or 3 in FP4. The flexibility of FP8 lies in its ability to adjust the decimal point position, unlike integer types with a fixed range.\n\nTo demonstrate the disparity between INT8 and FP8, we present Figure 2 where a hypothetical 15-element vector with an outlier value of 100 undergoes quantization using INT8 Asymmetric and FP8 (with both E5M2 and E4M3 configurations). Figure 2 illustrates that while INT8 approximates the outlier effectively, it struggles to accurately represent smaller numbers. Conversely, FP8 (whether with E5M2 or E4M3) provides greater precision in approximating the clustered data. 3 Considering the advantages of ExMy, which allows for dynamic scaling across activation maps, quantization error is reduced and essential features are preserved. In this paper, we investigate the performance of FP8 or FP4 techniques for handling the variability in activation or weight values. This could potentially lead to an enhancement in the model's performance on post-training quantization.\n\n\nMethodology\n\nSeveral lightweight optimization-based methods, where the weight of the model is updated during quantization, have been proposed in the literature [34,35,5,33,15,12]. Among these, we chose to align our approach with the principles outlined in the GPTQ [7,6], which can be dated back to [14,10]. While this strategy offers a robust starting point, it is imperative to keep in mind the dynamic and ever-evolving nature of the field of artificial intelligence. There may be more efficient methodologies on the horizon, waiting to be discovered and implemented.\n\nIn light of ZeroQuant-V2 [35], we applied fine-grained quantization (FGQ) for weight and token-wise quantization for activation. In addition, we will also investigate the add-on feature LoRC (Low Rank Compensation) proposed in [35], which aims to reduce quantization errors in weights by employing low-rank matrix factorization. LoRC involves two main steps: first, it performs Singular Value Decomposition (SVD) on the error matrix, which is the difference between the original weight and the quantized weight. The error matrix is thus decomposed into two unitary matrices and a diagonal matrix. Second, the method formulates a new error approximation using two low-rank matrices that are derived from the matrices in the first step. This approximation is then added to the quantized weight to yield a more accurate estimate of the original weight, thereby reducing quantization errors.\n\nBased on GPTQ (without or with LoRC), we perform comprehensive comparisons between the use of FP8 or INT8 activation quantization, coupled with adjusting the weight quantization to FP8 and FP4. Particularly we explore the potential of FP4 weight and FP8 activation quantization.\n\nCasting the FP4 to FP8. Lastly, a unique challenge arises due to the use of different precision levels for weights (W) and activations (A). The actual software implementation of W4A8 in H100 NVIDIA hardware is that one needs to cast W's FP4 to match the FP8 precision used in A. The direct method of dequantization followed by quantization again could potentially have a detrimental effect on inference efficiency, hence it is not a viable solution. To address this, we propose the bit-shifting method. This means that instead of allowing S defined in equation 1 to be any real valued scaling factor, we constrain S to be a power of 2, i.e., S = 2 n , n \u2208 N (S could still represent fractions when n is negative and whole numbers when n is not negative). There are two methods we will implement: (M2) Collect scales to form a vector S = [S 1 , S 2 , . . . , S n ]. Take the maximum value in this group (usually, the set consists a (multiple) row(s) of a matrix [34]), denoted as S max , adjust these elements S max /S i to be represented by the power of 2, and then define\u015c i = S max /2 log 2 (Smax/Si) . This provides a far superior approximation compared to (M1). 4 We reiterate that this restriction using the power of 2, either using (M1) or (M2), simplifies computations, especially in digital systems operating based on binary logic. This is a crucial element of our approach to optimizing computational efficiency and maintaining the performance of our model. \n\n\nMain Results\n\nIn this section, we perform experiments to understand the differences of Integer (INT) and Floating-point (FP) quantization using the GPTQ methods [7] with or without the add-on feature LoRC [35]. As described in Section 2, floating-point quantization could potentially maintain more precise information, which might improve the model's performance. To see if this is true, we include two model-type families: LLaMA [27] and OPT [36], with sizes ranging from 1 billion to 30 billion parameters. The evaluation spans across three datasets: Wikitext-2 (WIKI) [19], PTB [17], and C4 [24]. For more experiment details, please see Appendix A. The primary results in Table 2 reveal the impact of various quantization types which are applied to weight and activation specified in the 2nd column; for instance, W4A8 precision, INT -FP means INT4 is used for weight and FP8 for activation. Our results provide an average performance over three datasets, offering a broad understanding of the quantization method's efficiency. However, we delve further, understanding that these methods' performance can differ with the characteristics of datasets, thus presenting a detailed performance breakdown for each dataset. We find that FP8 and FP4, the configurations E4M3 and E2M1 respectively outperform E5M2 and E3M0, hence, they were used in our experiments. Further insights and explanations regarding these configurations' impact on performance are to be addressed in Appendix A.\n\nFP8 Activation is much better than INT8. The high-level summary of the results in Table 2 indicates that for both LLaMA and OPT model families, FP8 activation generally outperforms INT8 activation. This observation corroborates the motivation discussed in Section 2, emphasizing FP8's superior capacity to capture more nuanced information, a vital aspect for generative tasks in large-scale LLMs.\n\nInterestingly, the advantage of FP8 over INT8 becomes more pronounced for larger models with parameters greater than 6.7 billion, such as LLaMA-7b/13b and OPT-6.7b/13b. For instance, when considering LLaMA-7b, shifting from INT to FP quantization in the W8A8 configuration leads to an additional 0.25 PPL reduction (from 10.63 to 10.38), and in the W4A8 setup, there is an extra 0.4 PPL drop (from 11.48 to 11.08). These performance gains are significant, considering all other optimization parameters remain constant, and they align with the Class-3 quantization sensitivity category as defined in [35]. Thus, the results underline the importance of FP8 activation, particularly in larger LLMs, to enhance the overall performance and precision of the model's outputs.\n\nFP8 weights rival INT8, while FP4 weights potentially outperform INT4. From Table 2, we observe comparable performances between INT8 and FP8 weight quantization across various models and datasets, when keeping activation at FP8. This probably due to we used FGQ on weight quantization. Interestingly, when weight quantization is lowered, FP4 exhibits certain advantages over INT4, particularly evident in LLaMA-7b (15.14 to 16.09) and LLaMA-13b models (11.08 to 11.31). Specifically, under the W4A8 configuration for LLaMA-7b, we see 0.95 improvement of FP4 over INT4, a significant gain. The preferable performance of FP4 over INT4 is particularly advantageous for hardware designs like H100, where FP8 is already supported. Thus, a simple modification to accommodate FP4 would be easier than implementing a system supporting INT4 weight and FP8 activation.\n\nLoRC improves W4A8. Table 2 shows that the Low Rank Compensation (LoRC) method enhanced the W4A8 quantization scheme, reducing quantization errors. This improvement is particularly pronounced in smaller models, underlining the effectiveness of LoRC in optimizing the performance of these computing processes while impacting little on the model-size. Casting the FP4 to FP8. As detailed in Section 3, to maximize real latency speedup on NVIDIA H100 hardware, we suggest the scale factor S for weight quantization to be represented as a power of 2. In pursuit of this, we executed a series of experiments using FP4 for weight and FP8 for activation quantization. The results of these experiments, conducted both with and without LoRC, are presented in Table 3. Our data shows that while constraining the scaling factors occasionally results in unexpected improvements in models like LLaMA-7b and LLaMA-13b, we generally observe a minor degradation of quality in the W4A8 floating-point model, regardless of whether we used method M1 or M2. M2 generally outperforms M1. When we implement LoRC, this decline in quality can be mitigated, particularly in the OPT-1.3b, LLaMA-7b, and LLaMA-13b models. Hence, our results advocate for the use of LoRC, especially when considering scale restrictions for weight quantization in deep learning models.\n\n\nConclusions\n\nIn this study, we demonstrate that floating-point (FP) quantization significantly surpasses integer (INT) quantization in the context of large language models (LLMs) during post-training quantization. Notably, FP8 activation exceeds INT8, especially in larger models. Moreover, FP8 and FP4 weight quantization are either competitive with or surpass their INT equivalents. The Low Rank Compensation (LoRC) approach greatly enhances the W4A8 quantization scheme, particularly in smaller models. In conclusion, our work underscores the potential of FP quantization in enhancing model performance, and strategies such as LoRC further mitigate degradation induced by scale factor restrictions on weight quantization.\n\nFigure 1 :\n1Distribution of Activation values. The top, middle and bottom rows represents the distributions at the 2nd, 12th and final layer of the pretrained OPT-1.3b model. From the left to right columns, they are respectively for the linear modules attn.q_proj (same as attn.k_proj and attn.v_proj), attn.out_proj, fc1, and fc2. The histogram's x-axis ranges from the smallest to largest activation values, while the y-axis denotes their frequency in the dataset. See legend for their minimum and maximum values. Density functions illustrate the probability of different activation values. For more details, please see Section 2.\n\nFigure 2 :\n2A Contrast between INT8 and FP8 Quantization Methods. The top row displays the original vector in its full-precision form. The subsequent row showcases the vector after quantization through the INT8 Asymmetric approach. The final two rows present values quantized by the FP8 method, utilizing E5M2 and E4M3 formats respectively.\n\n( M1 )\nM1Map to the nearest values represented by the power of 2, i.e., letting the new scale\u015c = 2 log 2 (S) ;\n\nTable 1 :\n1Comparison of FP16 and INT8 activation quantization. We report the average PPL (the lower the \nbetter) over Wikitext-2 (WIKI) [19], PTB [17], and C4 [24], for both OPT and BLOOM (BLM) models. \n\nPrecision OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b \n\nW16-A16 \n11.90 \n11.22 \n10.70 \n10.33 \n20.43 \n17.58 \n14.96 \n10.90 \n\nW16-A8 \n12.62 \n15.36 \n23.57 \n561.35 \n20.52 \n17.65 \n15.14 \n11.62 \n\n20 0 \n20 40 60 80 \nActivation Values \n\n0.00 \n\n0.25 \n\n0.50 \n\n0.75 \n\n1.00 \n\n1.25 \n\n1.50 \n\nDensity \n\nLayer-2 input for attn.q_proj \n\n\n\nTable 2 :\n2The evaluation outcomes for LLaMA (top) and OPT (bottom ) using different Integer (INT) and Floating-point (FP) quantization methods applied to weight and activation. The performance is measured in terms of perplexity (lower scores are better) and spans across three datasets: WikiText-2 (WIKI), PTB, and C4. For each model, the results initially highlight the average performance across the datasets, followed by a detailed breakdown of outcomes per dataset. Activation Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4Q-type \nWeight-\nLLaMA-3b \nLLaMA-7b \nLLaMA-13b \nLLaMA-30b \n-W16A16 N/A \n11.93 \n7.35/19.1/9.34 \n13.37 5.68/27.35/7.78 10.31 5.09/19.22/6.61 \n5.79 \n4.10/7.30/5.98 \n\nW8A8 \n\nINT -INT 12.00 7.41/19.16/9.41 13.58 5.72/27.89/7.13 10.63 5.16/20.07/6.67 \n5.90 \n4.21/7.42/6.06 \nINT -FP \n11.96 7.37/19.16/9.35 13.45 5.69/27.57/7.09 10.38 5.11/19.42/6.62 \n5.80 \n4.11/7.31/5.99 \nFP -FP \n11.99 7.37/19.23/9.37 13.46 5.70/27.58/7.10 10.38 5.11/19.41/6.62 \n5.81 \n4.12/7.31/5.99 \n\nW4A8 \n\nINT -INT 12.55 7.67/20.23/9.74 16.23 6.44/34.45/7.79 11.48 5.32/22.35/6.78 \n6.02 \n4.36/7.54/6.16 \nINT -FP \n12.39 7.62/19.87/9.68 16.09 6.75/33.80/7.72 11.31 5.28/21.91/6.73 \n5.94 \n4.27/7.45/6.11 \nFP -FP \n12.45 7.62/20.05/9.67 15.14 6.32/31.61/7.51 11.08 5.26/21.27/6.73 \n5.92 \n4.26/7.42/6.09 \n\nW4A8 \nINT -INT 12.52 7.65/20.18/9.72 14.14 5.88/29.26/7.27 10.81 5.28/20.38/6.76 \n6.00 \n4.34/7.51/6.14 \n\n+LoRC \nINT -FP \n12.38 7.58/19.89/9.65 14.01 5.84/28.95/7.24 10.56 5.22/19.75/6.71 \n5.90 \n4.24/7.39/6.07 \nFP -FP \n12.42 7.61/19.98/9.66 13.95 5.87/28.75/7.24 10.80 5.24/20.46/6.72 \n5.91 \n4.26/7.40/6.07 \n\nQ-type \nWeight -\nOPT-3b \nOPT-7b \nOPT-13b \nOPT-30b \n-Activation Mean \nWIKI/PTB/C4 \nMean \nWIKI/PTB/C4 \nMean \nWIKI/PTB/C4 \nMean \nWIKI/PTB/C4 \n\nW16A16 N/A \n15.44 14.62/16.97/14.72 11.90 10.86/13.09/11.74 11.22 10.13/12.34/11.20 10.70 \n9.56/11.84/10.69 \n\nW8A8 \n\nINT -INT \n15.94 14.98/17.49/15.36 12.66 11.20/14.29/12.48 15.94 12.13/19.82/15.86 25.76 14.63/32.90/29.74 \nINT -FP \n15.85 14.93/17.56/15.05 11.99 10.92/13.24/11.80 11.27 10.16/12.42/11.23 10.69 \n9.51/11.87/10.71 \nFP -FP \n15.86 14.97/17.55/15.05 11.99 10.91/13.24/11.81 11.27 10.16/12.42/11.23 10.69 \n9.51/11.87/10.71 \n\nW4A8 \n\nINT -INT \n16.41 15.39/18.22/15.62 13.18 11.61/15.00/12.92 16.70 12.32/21.21/16.56 24.42 14.80/30.38/28.09 \nINT -FP \n16.40 15.46/18.23/15.51 12.20 11.13/13.49/11.99 11.34 10.20/12.53/11.30 10.73 \n9.54/11.91/10.75 \nFP -FP \n16.29 15.32/18.19/15.35 12.09 10.89/13.44/11.95 11.34 10.16/12.55/11.30 10.72 \n9.52/11.90/10.75 \n\nW4A8 \nINT -INT \n16.38 15.50/18.05/15.59 12.75 11.37/14.33/12.53 15.89 12.06/19.76/15.85 27.20 15.94/34.50/31.16 \n\n+LoRC \nINT -FP \n16.23 15.40/17.97/15.32 12.13 11.07/13.43/11.90 11.34 10.23/12.49/11.29 10.71 \n9.48/11.91/10.74 \nFP -FP \n16.23 15.50/17.92/15.28 12.09 10.96/13.40/11.90 11.33 10.15/12.55/11.29 10.71 \n9.48/11.90/10.75 \n\n\n\nTable 3 :\n3Scale values (S) are evaluated both without and with restrictions of being a power of 2, as shown in the second column. The quantization type employed is FP4 for weight and FP8 for activation. 30b S = 2 n Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 W4A8 12.45 7.62/20.05/9.67 15.14 6.32/31.61/7.51 11.08 5.26/21.27/6.73 5.92 4.26/7.42/6.09 (M1) 12.66 7.76/20.41/9.81 16.33 6.34/34.82/7.82 10.90 5.31/20.63/6.78 6.00 4.38/7.48/6.15 (M2) 12.55 7.68/20.21/9.77 14.49 6.37/29.32/7.79 10.95 5.26/20.81/6.77 M2) 12.42 7.63/19.89/9.74 13.68 5.90/27.83/7.32 10.40 5.23/19.22/6.76 W4A8 16.29 15.32/18.19/15.35 12.09 10.89/13.44/11.95 11.34 10.16/12.55/11.30 10.72 9.52/11.90/10.75 (M1) 16.66 15.65/18.66/15.65 12.29 11.12/13.69/12.05 11.36 10.22/12.54/11.32 10.77 9.58/11.96/10.76 (M2) 16.47 15.23/18.55/15.62 12.25 11.11/13.61/12.03 11.40 10.22/12.61/11.36 10.74 9.47/11.96/10.78 W4A8 16.23 15.50/17.92/15.28 12.09 10.96/13.40/11.90 11.33 10.15/12.55/11.29 10.71 9.48/11.90/10.75 LoRC (M1) 16.47 15.59/18.37/15.45 12.17 11.10/13.47/11.95 11.36 10.21/12.54/11.32 10.74 9.49/11.96/10.76 (M2) 16.30 15.39/18.10/15.42 12.19 11.11/13.49/11.97 11.41 10.34/12.54/11.34 10.75 9.49/11.96/10.78Q-type \nScale \nLLaMA-3b \nLLaMA-7b \nLLaMA-13b \nLLaMA-5.97 \n4.31/7.48/6.13 \n\nW4A8 \n\n12.42 7.61/19.98/9.66 13.95 5.87/28.75/7.24 10.80 5.24/20.46/6.72 \n5.91 \n4.26/7.40/6.07 \n\nLoRC \n(M1) 12.61 7.69/20.37/9.76 14.23 5.94/29.47/7.30 10.74 5.28/20.19/6.75 \n5.98 \n4.33/7.48/6.13 \n(5.94 \n4.28/7.44/6.11 \n\nQ-type \nScale \nOPT-1.3b \nOPT-6.7b \nOPT-13b \nOPT-30b \nS = 2 n Mean \nWIKI/PTB/C4 \nMean \nWIKI/PTB/C4 \nMean \nWIKI/PTB/C4 \nMean WIKI/PTB/C4 \n\n\nThe hidden dimension for the model is 2048 for 'attn.q_proj', 'attn.out_proj' and 'fc1', and 8196 for 'fc2'. We pick 20 tokens (position 8 to 28) and vectorize this 20 \u00d7 2048 or 20 \u00d7 8196 matrices to plot their distributions. The plots used bin=100.\nPlease note that the FP8 format used in this paper is based on the Qtorch Python package, which can be installed via 'pip install qtorch'. It differs slightly from Nvidia's FP8 in H100, which requires one mantissa bit-pattern for NaN values.4 To ensure the casting of F4-E2M1 for each weight matrix to FP8, we apply format E5M2 once a matrix is quantized.\nAcknowledgementThis research was conducted within the supportive environment of the DeepSpeed team at Microsoft, whose invaluable assistance was instrumental to this project. We thank Cheng Li and Connor Homes for the insightful discussions.\nScalable methods for 8-bit training of neural networks. Ron Banner, Itay Hubara, Elad Hoffer, Daniel Soudry, Advances in neural information processing systems. 31Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. Advances in neural information processing systems, 31, 2018.\n\nShifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. L\u00e9opold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, H Oguz, Hanlin Elibol, Tang, arXiv:2001.05674arXiv preprintL\u00e9opold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz H Elibol, and Hanlin Tang. Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. arXiv preprint arXiv:2001.05674, 2020.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multipli- cation for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. arXiv preprintTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.\n\nThe case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, arXiv:2212.09720arXiv preprintTim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\nOptimal brain compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Dan Alistarh, arXiv:2208.11580arXiv preprintElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nAi and memory wall. Amir Gholami, Zhewei Yao, Sehoon Kim, W Michael, Kurt Mahoney, Keutzer, RiseLab Medium Post. Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021.\n\n. Github, Github, GitHub. Github copilot. https://github.com/features/copilot/, 2021.\n\nSecond order derivatives for network pruning: Optimal brain surgeon. Babak Hassibi, G David, Stork, Advances in neural information processing systems. Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems, pages 164-171, 1993.\n\nMinje Kim, Paris Smaragdis, arXiv:1601.06071Bitwise neural networks. arXiv preprintMinje Kim and Paris Smaragdis. Bitwise neural networks. arXiv preprint arXiv:1601.06071, 2016.\n\nSqueezellm: Dense-and-sparse quantization. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, W Michael, Kurt Mahoney, Keutzer, arXiv:2306.07629arXiv preprintSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.\n\nFp8 quantization: The power of the exponent. Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, Tijmen Blankevoort, arXiv:2208.09225arXiv preprintAndrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. arXiv preprint arXiv:2208.09225, 2022.\n\nOptimal brain damage. Yann Lecun, S John, Sara A Denker, Solla, Advances in neural information processing systems. Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pages 598-605, 1990.\n\nAwq: Activation-aware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, arXiv:2306.00978arXiv preprintJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n\nLlm-qat: Data-free quantization aware training for large language models. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra, arXiv:2305.17888arXiv preprintZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.\n\nBuilding a large annotated corpus of english: The penn treebank. Using Large Corpora. Mary Ann Marcinkiewicz, 273Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using Large Corpora, page 273, 1994.\n\nNaveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das, Bharat Kaul, Pradeep Dubey, arXiv:1705.01462Ternary neural networks with fine-grained quantization. arXiv preprintNaveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das, Bharat Kaul, and Pradeep Dubey. Ternary neural networks with fine-grained quantization. arXiv preprint arXiv:1705.01462, 2017.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, International Conference on Learning Representations. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017.\n\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, arXiv:2209.05433Fp8 formats for deep learning. arXiv preprintPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\n\n. Nvidia, Fastertransformer, NVIDIA. FasterTransformer. https://github.com/NVIDIA/FasterTransformer, January 2023.\n\n. Openai, Openai, OpenAI. Openai chatgpt. https://openai.com/blog/chatgpt/, 2022.\n\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean, arXiv:2211.05102Efficiently scaling transformer inference. arXiv preprintReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2019.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Matthias Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.11990arXiv preprintShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nAndrey Mart Van Baalen, Kuzmin, S Suparna, Yuwei Nair, Eric Ren, Chirag Mahurin, Sundar Patel, Subramanian, arXiv:2303.17951Fp8 versus int8 for efficient deep learning inference. Sanghyuk Lee, Markus Nagel, Joseph SoriagaarXiv preprintMart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. Fp8 versus int8 for efficient deep learning inference. arXiv preprint arXiv:2303.17951, 2023.\n\nOutlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu, arXiv:2304.09145arXiv preprintXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.\n\nInteger quantization for deep learning inference: Principles and empirical evaluation. Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, Paulius Micikevicius, arXiv:2004.09602arXiv preprintHao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602, 2020.\n\nUnderstanding int4 quantization for transformer models: Latency speedup, composability, and failure cases. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He, arXiv:2301.12017arXiv preprintXiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases. arXiv preprint arXiv:2301.12017, 2023.\n\nExtreme compression for pre-trained transformers made simple and efficient. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He, arXiv:2206.01859arXiv preprintXiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for pre-trained transformers made simple and efficient. arXiv preprint arXiv:2206.01859, 2022.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.10438arXiv preprintGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.01861arXiv preprintZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nZhewei Yao, Cheng Li, Xiaoxia Wu, arXiv:2303.08302Stephen Youn, and Yuxiong He. A comprehensive study on post-training quantization for large language models. arXiv preprintZhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study on post-training quantization for large language models. arXiv preprint arXiv:2303.08302, 2023.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, Ningyi Xu, arXiv:2305.12356Integer or floating point? new outlooks for low-bit quantization on large language models. arXiv preprintYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. Integer or floating point? new outlooks for low-bit quantization on large language models. arXiv preprint arXiv:2305.12356, 2023.\n\nWe run them on a single GPU (i.e, V100-32GB) thanks for the two open-source github repositories. 5 To accommodate the real computation efficiency, the group-size for weight quantization is 256 for both model family (OPT and LLaMA) except the LLaMA-3b with 320 as its hidden-dimension is 3200. All the checkpoints we used are from huggingface. 6 As for activation, we perform token-wise quantization in order to accommodate the latency requirements. For LoRC method, the dimension for the two low-rank matrix we used for LLaMA is 8. While for OPT, the dimension is 16, 32, 40 and 56 respectively for 1.3b, 6.7b, 13b and 30b. We did not try others dimension as. we use C4 dataset to randomly select 128 sentences for the light-weight PTQ and each of them has 2048 tokens. A Experiment Details As we used GPTQ method [7. indicated by [35] that dimension of the low-rank matrix does not play too much impact on the quantization error as long as it larger than 8A Experiment Details As we used GPTQ method [7], we use C4 dataset to randomly select 128 sentences for the light-weight PTQ and each of them has 2048 tokens. We run them on a single GPU (i.e, V100-32GB) thanks for the two open-source github repositories. 5 To accommodate the real computation efficiency, the group-size for weight quantization is 256 for both model family (OPT and LLaMA) except the LLaMA-3b with 320 as its hidden-dimension is 3200. All the checkpoints we used are from huggingface. 6 As for activation, we perform token-wise quantization in order to accommodate the latency requirements. For LoRC method, the dimension for the two low-rank matrix we used for LLaMA is 8. While for OPT, the dimension is 16, 32, 40 and 56 respectively for 1.3b, 6.7b, 13b and 30b. We did not try others dimension as indicated by [35] that dimension of the low-rank matrix does not play too much impact on the quantization error as long as it larger than 8.\n\n1: Comparisons between E2M1 and E3M0. The quantization is FP4 for weight and FP8 for activation. Activation (FP8) OPT-1.3b OPT-6.7b OPT-13b OPT-30b. A Table, Table A.1: Comparisons between E2M1 and E3M0. The quantization is FP4 for weight and FP8 for activation. Activation (FP8) OPT-1.3b OPT-6.7b OPT-13b OPT-30b\n\nLLaMA-3b is openlm-research/open_llama_3b and all other LLaMA are from decapoda-research/llama-#b-hf where # can be 7b, 13b and 30b. As for OPT, they are from facebook/opt-#b where #. can be 1.3b, 7b, 13b and 30bhttps://github.com/IST-DASLab/gptq and https://github.com/qwopqwop200/GPTQ-for-LLaMa.git 6 LLaMA-3b is openlm-research/open_llama_3b and all other LLaMA are from decapoda-research/llama-#b-hf where # can be 7b, 13b and 30b. As for OPT, they are from facebook/opt-#b where # can be 1.3b, 7b, 13b and 30b.\n", "annotations": {"author": "[{\"end\":136,\"start\":101},{\"end\":172,\"start\":137},{\"end\":194,\"start\":173}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":109},{\"end\":147,\"start\":144},{\"end\":193,\"start\":184}]", "author_first_name": "[{\"end\":108,\"start\":101},{\"end\":143,\"start\":137},{\"end\":180,\"start\":173},{\"end\":183,\"start\":181}]", "author_affiliation": null, "title": "[{\"end\":98,\"start\":1},{\"end\":292,\"start\":195}]", "venue": null, "abstract": "[{\"end\":1819,\"start\":294}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1924,\"start\":1921},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1942,\"start\":1938},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2124,\"start\":2120},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2126,\"start\":2124},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2129,\"start\":2126},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2627,\"start\":2626},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2754,\"start\":2750},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2756,\"start\":2754},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2759,\"start\":2756},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2762,\"start\":2759},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2885,\"start\":2882},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2888,\"start\":2885},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2891,\"start\":2888},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2894,\"start\":2891},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3305,\"start\":3301},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3308,\"start\":3305},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3326,\"start\":3322},{\"end\":3554,\"start\":3551},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3557,\"start\":3554},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3560,\"start\":3557},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3563,\"start\":3560},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3622,\"start\":3618},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3892,\"start\":3888},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4158,\"start\":4156},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4161,\"start\":4158},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4164,\"start\":4161},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4167,\"start\":4164},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4466,\"start\":4462},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4517,\"start\":4513},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4717,\"start\":4713},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4720,\"start\":4717},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4919,\"start\":4915},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5174,\"start\":5170},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5355,\"start\":5352},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5453,\"start\":5449},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5540,\"start\":5536},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6410,\"start\":6406},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7052,\"start\":7048},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7387,\"start\":7383},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7595,\"start\":7591},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7610,\"start\":7606},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12350,\"start\":12349},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12914,\"start\":12910},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12917,\"start\":12914},{\"end\":12919,\"start\":12917},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12922,\"start\":12919},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12925,\"start\":12922},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12928,\"start\":12925},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13018,\"start\":13015},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13020,\"start\":13018},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13053,\"start\":13049},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13056,\"start\":13053},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13351,\"start\":13347},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13553,\"start\":13549},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15456,\"start\":15452},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15658,\"start\":15657},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16125,\"start\":16122},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16170,\"start\":16166},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16395,\"start\":16391},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16408,\"start\":16404},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16536,\"start\":16532},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16546,\"start\":16542},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16559,\"start\":16555},{\"end\":18179,\"start\":18173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18446,\"start\":18442},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28172,\"start\":28171}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":22173,\"start\":21540},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22515,\"start\":22174},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22627,\"start\":22516},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":23175,\"start\":22628},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26033,\"start\":23176},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27679,\"start\":26034}]", "paragraph": "[{\"end\":2895,\"start\":1835},{\"end\":3973,\"start\":2897},{\"end\":5129,\"start\":3975},{\"end\":5969,\"start\":5131},{\"end\":6206,\"start\":5971},{\"end\":6554,\"start\":6208},{\"end\":7243,\"start\":6556},{\"end\":8418,\"start\":7258},{\"end\":8481,\"start\":8420},{\"end\":8556,\"start\":8483},{\"end\":8638,\"start\":8558},{\"end\":10010,\"start\":8640},{\"end\":10558,\"start\":10012},{\"end\":10649,\"start\":10560},{\"end\":11876,\"start\":10679},{\"end\":12747,\"start\":11878},{\"end\":13320,\"start\":12763},{\"end\":14209,\"start\":13322},{\"end\":14489,\"start\":14211},{\"end\":15958,\"start\":14491},{\"end\":17443,\"start\":15975},{\"end\":17841,\"start\":17445},{\"end\":18611,\"start\":17843},{\"end\":19471,\"start\":18613},{\"end\":20812,\"start\":19473},{\"end\":21539,\"start\":20828}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10678,\"start\":10650}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7577,\"start\":7570},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16643,\"start\":16636},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17534,\"start\":17527},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18696,\"start\":18689},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19500,\"start\":19493},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20230,\"start\":20223}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1833,\"start\":1821},{\"attributes\":{\"n\":\"2\"},\"end\":7256,\"start\":7246},{\"attributes\":{\"n\":\"3\"},\"end\":12761,\"start\":12750},{\"attributes\":{\"n\":\"4\"},\"end\":15973,\"start\":15961},{\"attributes\":{\"n\":\"5\"},\"end\":20826,\"start\":20815},{\"end\":21551,\"start\":21541},{\"end\":22185,\"start\":22175},{\"end\":22523,\"start\":22517},{\"end\":22638,\"start\":22629},{\"end\":23186,\"start\":23177},{\"end\":26044,\"start\":26035}]", "table": "[{\"end\":23175,\"start\":22640},{\"end\":26033,\"start\":23726},{\"end\":27679,\"start\":27246}]", "figure_caption": "[{\"end\":22173,\"start\":21553},{\"end\":22515,\"start\":22187},{\"end\":22627,\"start\":22526},{\"end\":23726,\"start\":23188},{\"end\":27246,\"start\":26046}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7968,\"start\":7960},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8688,\"start\":8680},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8815,\"start\":8807},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9435,\"start\":9427},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10009,\"start\":10000},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11948,\"start\":11940},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12120,\"start\":12112}]", "bib_author_first_name": "[{\"end\":28587,\"start\":28584},{\"end\":28600,\"start\":28596},{\"end\":28613,\"start\":28609},{\"end\":28628,\"start\":28622},{\"end\":28973,\"start\":28966},{\"end\":28990,\"start\":28983},{\"end\":29010,\"start\":29006},{\"end\":29023,\"start\":29017},{\"end\":29033,\"start\":29032},{\"end\":29046,\"start\":29040},{\"end\":29333,\"start\":29330},{\"end\":29348,\"start\":29344},{\"end\":29362,\"start\":29356},{\"end\":29376,\"start\":29372},{\"end\":29659,\"start\":29656},{\"end\":29678,\"start\":29670},{\"end\":29691,\"start\":29688},{\"end\":29706,\"start\":29702},{\"end\":30015,\"start\":30012},{\"end\":30030,\"start\":30026},{\"end\":30306,\"start\":30301},{\"end\":30319,\"start\":30316},{\"end\":30612,\"start\":30607},{\"end\":30629,\"start\":30622},{\"end\":30649,\"start\":30646},{\"end\":30912,\"start\":30908},{\"end\":30928,\"start\":30922},{\"end\":30940,\"start\":30934},{\"end\":30947,\"start\":30946},{\"end\":30961,\"start\":30957},{\"end\":31285,\"start\":31280},{\"end\":31296,\"start\":31295},{\"end\":31545,\"start\":31540},{\"end\":31556,\"start\":31551},{\"end\":31768,\"start\":31762},{\"end\":31781,\"start\":31774},{\"end\":31794,\"start\":31790},{\"end\":31808,\"start\":31804},{\"end\":31820,\"start\":31815},{\"end\":31830,\"start\":31825},{\"end\":31838,\"start\":31837},{\"end\":31852,\"start\":31848},{\"end\":32147,\"start\":32141},{\"end\":32160,\"start\":32156},{\"end\":32178,\"start\":32173},{\"end\":32190,\"start\":32184},{\"end\":32202,\"start\":32198},{\"end\":32217,\"start\":32211},{\"end\":32466,\"start\":32462},{\"end\":32475,\"start\":32474},{\"end\":32486,\"start\":32482},{\"end\":32488,\"start\":32487},{\"end\":32780,\"start\":32778},{\"end\":32793,\"start\":32786},{\"end\":32807,\"start\":32800},{\"end\":32819,\"start\":32814},{\"end\":32832,\"start\":32826},{\"end\":32843,\"start\":32839},{\"end\":33154,\"start\":33148},{\"end\":33166,\"start\":33160},{\"end\":33183,\"start\":33173},{\"end\":33195,\"start\":33190},{\"end\":33209,\"start\":33203},{\"end\":33223,\"start\":33217},{\"end\":33240,\"start\":33232},{\"end\":33256,\"start\":33246},{\"end\":33278,\"start\":33273},{\"end\":33666,\"start\":33662},{\"end\":33670,\"start\":33667},{\"end\":33822,\"start\":33816},{\"end\":33842,\"start\":33835},{\"end\":33859,\"start\":33850},{\"end\":33878,\"start\":33870},{\"end\":33890,\"start\":33884},{\"end\":33904,\"start\":33897},{\"end\":34234,\"start\":34227},{\"end\":34250,\"start\":34243},{\"end\":34263,\"start\":34258},{\"end\":34281,\"start\":34274},{\"end\":34515,\"start\":34508},{\"end\":34535,\"start\":34530},{\"end\":34548,\"start\":34544},{\"end\":34564,\"start\":34558},{\"end\":34580,\"start\":34573},{\"end\":34595,\"start\":34588},{\"end\":34618,\"start\":34611},{\"end\":34632,\"start\":34623},{\"end\":34650,\"start\":34643},{\"end\":34661,\"start\":34657},{\"end\":35176,\"start\":35170},{\"end\":35189,\"start\":35183},{\"end\":35208,\"start\":35199},{\"end\":35225,\"start\":35220},{\"end\":35239,\"start\":35234},{\"end\":35256,\"start\":35250},{\"end\":35275,\"start\":35267},{\"end\":35287,\"start\":35282},{\"end\":35301,\"start\":35294},{\"end\":35315,\"start\":35311},{\"end\":35722,\"start\":35717},{\"end\":35735,\"start\":35731},{\"end\":35749,\"start\":35745},{\"end\":35768,\"start\":35759},{\"end\":35780,\"start\":35774},{\"end\":35796,\"start\":35789},{\"end\":35810,\"start\":35805},{\"end\":35820,\"start\":35817},{\"end\":35830,\"start\":35825},{\"end\":35832,\"start\":35831},{\"end\":36060,\"start\":36054},{\"end\":36087,\"start\":36076},{\"end\":36098,\"start\":36093},{\"end\":36112,\"start\":36106},{\"end\":36128,\"start\":36122},{\"end\":36140,\"start\":36135},{\"end\":36159,\"start\":36150},{\"end\":36165,\"start\":36160},{\"end\":36184,\"start\":36176},{\"end\":36203,\"start\":36195},{\"end\":36691,\"start\":36685},{\"end\":36706,\"start\":36699},{\"end\":36723,\"start\":36716},{\"end\":36739,\"start\":36732},{\"end\":36757,\"start\":36751},{\"end\":36776,\"start\":36771},{\"end\":36789,\"start\":36785},{\"end\":36802,\"start\":36795},{\"end\":36821,\"start\":36815},{\"end\":36836,\"start\":36831},{\"end\":37201,\"start\":37197},{\"end\":37218,\"start\":37211},{\"end\":37234,\"start\":37227},{\"end\":37250,\"start\":37244},{\"end\":37271,\"start\":37261},{\"end\":37289,\"start\":37281},{\"end\":37304,\"start\":37299},{\"end\":37327,\"start\":37323},{\"end\":37341,\"start\":37335},{\"end\":37701,\"start\":37695},{\"end\":37728,\"start\":37727},{\"end\":37743,\"start\":37738},{\"end\":37754,\"start\":37750},{\"end\":37766,\"start\":37760},{\"end\":37782,\"start\":37776},{\"end\":38307,\"start\":38300},{\"end\":38320,\"start\":38313},{\"end\":38334,\"start\":38328},{\"end\":38347,\"start\":38339},{\"end\":38361,\"start\":38355},{\"end\":38375,\"start\":38368},{\"end\":38390,\"start\":38381},{\"end\":38773,\"start\":38770},{\"end\":38785,\"start\":38778},{\"end\":38799,\"start\":38792},{\"end\":38814,\"start\":38807},{\"end\":38829,\"start\":38822},{\"end\":39193,\"start\":39186},{\"end\":39203,\"start\":39198},{\"end\":39212,\"start\":39208},{\"end\":39220,\"start\":39213},{\"end\":39238,\"start\":39232},{\"end\":39251,\"start\":39244},{\"end\":39590,\"start\":39583},{\"end\":39601,\"start\":39595},{\"end\":39613,\"start\":39607},{\"end\":39629,\"start\":39621},{\"end\":39641,\"start\":39634},{\"end\":39958,\"start\":39949},{\"end\":39967,\"start\":39965},{\"end\":39980,\"start\":39973},{\"end\":39995,\"start\":39989},{\"end\":40009,\"start\":40005},{\"end\":40344,\"start\":40338},{\"end\":40354,\"start\":40350},{\"end\":40362,\"start\":40355},{\"end\":40380,\"start\":40374},{\"end\":40395,\"start\":40388},{\"end\":40408,\"start\":40400},{\"end\":40420,\"start\":40413},{\"end\":40685,\"start\":40679},{\"end\":40696,\"start\":40691},{\"end\":40708,\"start\":40701},{\"end\":41040,\"start\":41035},{\"end\":41055,\"start\":41048},{\"end\":41069,\"start\":41064},{\"end\":41082,\"start\":41077},{\"end\":41096,\"start\":41092},{\"end\":41110,\"start\":41103},{\"end\":41128,\"start\":41117},{\"end\":41140,\"start\":41136},{\"end\":41151,\"start\":41147},{\"end\":41158,\"start\":41156},{\"end\":41490,\"start\":41485},{\"end\":41505,\"start\":41498},{\"end\":41518,\"start\":41512},{\"end\":41532,\"start\":41524},{\"end\":41543,\"start\":41539},{\"end\":41552,\"start\":41549},{\"end\":41562,\"start\":41559},{\"end\":41578,\"start\":41569},{\"end\":41592,\"start\":41586},{\"end\":44032,\"start\":44031}]", "bib_author_last_name": "[{\"end\":28594,\"start\":28588},{\"end\":28607,\"start\":28601},{\"end\":28620,\"start\":28614},{\"end\":28635,\"start\":28629},{\"end\":28981,\"start\":28974},{\"end\":29004,\"start\":28991},{\"end\":29015,\"start\":29011},{\"end\":29030,\"start\":29024},{\"end\":29038,\"start\":29034},{\"end\":29053,\"start\":29047},{\"end\":29059,\"start\":29055},{\"end\":29342,\"start\":29334},{\"end\":29354,\"start\":29349},{\"end\":29370,\"start\":29363},{\"end\":29388,\"start\":29377},{\"end\":29668,\"start\":29660},{\"end\":29686,\"start\":29679},{\"end\":29700,\"start\":29692},{\"end\":29718,\"start\":29707},{\"end\":29725,\"start\":29720},{\"end\":30024,\"start\":30016},{\"end\":30042,\"start\":30031},{\"end\":30314,\"start\":30307},{\"end\":30328,\"start\":30320},{\"end\":30620,\"start\":30613},{\"end\":30644,\"start\":30630},{\"end\":30657,\"start\":30650},{\"end\":30667,\"start\":30659},{\"end\":30920,\"start\":30913},{\"end\":30932,\"start\":30929},{\"end\":30944,\"start\":30941},{\"end\":30955,\"start\":30948},{\"end\":30969,\"start\":30962},{\"end\":30978,\"start\":30971},{\"end\":31132,\"start\":31126},{\"end\":31140,\"start\":31134},{\"end\":31293,\"start\":31286},{\"end\":31302,\"start\":31297},{\"end\":31309,\"start\":31304},{\"end\":31549,\"start\":31546},{\"end\":31566,\"start\":31557},{\"end\":31772,\"start\":31769},{\"end\":31788,\"start\":31782},{\"end\":31802,\"start\":31795},{\"end\":31813,\"start\":31809},{\"end\":31823,\"start\":31821},{\"end\":31835,\"start\":31831},{\"end\":31846,\"start\":31839},{\"end\":31860,\"start\":31853},{\"end\":31869,\"start\":31862},{\"end\":32154,\"start\":32148},{\"end\":32171,\"start\":32161},{\"end\":32182,\"start\":32179},{\"end\":32196,\"start\":32191},{\"end\":32209,\"start\":32203},{\"end\":32229,\"start\":32218},{\"end\":32472,\"start\":32467},{\"end\":32480,\"start\":32476},{\"end\":32495,\"start\":32489},{\"end\":32502,\"start\":32497},{\"end\":32784,\"start\":32781},{\"end\":32798,\"start\":32794},{\"end\":32812,\"start\":32808},{\"end\":32824,\"start\":32820},{\"end\":32837,\"start\":32833},{\"end\":32847,\"start\":32844},{\"end\":33158,\"start\":33155},{\"end\":33171,\"start\":33167},{\"end\":33188,\"start\":33184},{\"end\":33201,\"start\":33196},{\"end\":33215,\"start\":33210},{\"end\":33230,\"start\":33224},{\"end\":33244,\"start\":33241},{\"end\":33271,\"start\":33257},{\"end\":33286,\"start\":33279},{\"end\":33684,\"start\":33671},{\"end\":33833,\"start\":33823},{\"end\":33848,\"start\":33843},{\"end\":33868,\"start\":33860},{\"end\":33882,\"start\":33879},{\"end\":33895,\"start\":33891},{\"end\":33910,\"start\":33905},{\"end\":34241,\"start\":34235},{\"end\":34256,\"start\":34251},{\"end\":34272,\"start\":34264},{\"end\":34288,\"start\":34282},{\"end\":34528,\"start\":34516},{\"end\":34542,\"start\":34536},{\"end\":34556,\"start\":34549},{\"end\":34571,\"start\":34565},{\"end\":34586,\"start\":34581},{\"end\":34609,\"start\":34596},{\"end\":34621,\"start\":34619},{\"end\":34641,\"start\":34633},{\"end\":34655,\"start\":34651},{\"end\":34668,\"start\":34662},{\"end\":34979,\"start\":34973},{\"end\":34998,\"start\":34981},{\"end\":35095,\"start\":35089},{\"end\":35103,\"start\":35097},{\"end\":35181,\"start\":35177},{\"end\":35197,\"start\":35190},{\"end\":35218,\"start\":35209},{\"end\":35232,\"start\":35226},{\"end\":35248,\"start\":35240},{\"end\":35265,\"start\":35257},{\"end\":35280,\"start\":35276},{\"end\":35292,\"start\":35288},{\"end\":35309,\"start\":35302},{\"end\":35320,\"start\":35316},{\"end\":35729,\"start\":35723},{\"end\":35743,\"start\":35736},{\"end\":35757,\"start\":35750},{\"end\":35772,\"start\":35769},{\"end\":35787,\"start\":35781},{\"end\":35803,\"start\":35797},{\"end\":35815,\"start\":35811},{\"end\":35823,\"start\":35821},{\"end\":35836,\"start\":35833},{\"end\":36074,\"start\":36061},{\"end\":36091,\"start\":36088},{\"end\":36104,\"start\":36099},{\"end\":36120,\"start\":36113},{\"end\":36133,\"start\":36129},{\"end\":36148,\"start\":36141},{\"end\":36174,\"start\":36166},{\"end\":36193,\"start\":36185},{\"end\":36208,\"start\":36204},{\"end\":36215,\"start\":36210},{\"end\":36697,\"start\":36692},{\"end\":36714,\"start\":36707},{\"end\":36730,\"start\":36724},{\"end\":36749,\"start\":36740},{\"end\":36769,\"start\":36758},{\"end\":36783,\"start\":36777},{\"end\":36793,\"start\":36790},{\"end\":36813,\"start\":36803},{\"end\":36829,\"start\":36822},{\"end\":36848,\"start\":36837},{\"end\":37209,\"start\":37202},{\"end\":37225,\"start\":37219},{\"end\":37242,\"start\":37235},{\"end\":37259,\"start\":37251},{\"end\":37279,\"start\":37272},{\"end\":37297,\"start\":37290},{\"end\":37321,\"start\":37305},{\"end\":37333,\"start\":37328},{\"end\":37348,\"start\":37342},{\"end\":37355,\"start\":37350},{\"end\":37717,\"start\":37702},{\"end\":37725,\"start\":37719},{\"end\":37736,\"start\":37729},{\"end\":37748,\"start\":37744},{\"end\":37758,\"start\":37755},{\"end\":37774,\"start\":37767},{\"end\":37788,\"start\":37783},{\"end\":37801,\"start\":37790},{\"end\":38311,\"start\":38308},{\"end\":38326,\"start\":38321},{\"end\":38337,\"start\":38335},{\"end\":38353,\"start\":38348},{\"end\":38366,\"start\":38362},{\"end\":38379,\"start\":38376},{\"end\":38394,\"start\":38391},{\"end\":38776,\"start\":38774},{\"end\":38790,\"start\":38786},{\"end\":38805,\"start\":38800},{\"end\":38820,\"start\":38815},{\"end\":38842,\"start\":38830},{\"end\":39196,\"start\":39194},{\"end\":39206,\"start\":39204},{\"end\":39230,\"start\":39221},{\"end\":39242,\"start\":39239},{\"end\":39254,\"start\":39252},{\"end\":39593,\"start\":39591},{\"end\":39605,\"start\":39602},{\"end\":39619,\"start\":39614},{\"end\":39632,\"start\":39630},{\"end\":39644,\"start\":39642},{\"end\":39963,\"start\":39959},{\"end\":39971,\"start\":39968},{\"end\":39987,\"start\":39981},{\"end\":40003,\"start\":39996},{\"end\":40013,\"start\":40010},{\"end\":40348,\"start\":40345},{\"end\":40372,\"start\":40363},{\"end\":40386,\"start\":40381},{\"end\":40398,\"start\":40396},{\"end\":40411,\"start\":40409},{\"end\":40423,\"start\":40421},{\"end\":40689,\"start\":40686},{\"end\":40699,\"start\":40697},{\"end\":40711,\"start\":40709},{\"end\":41046,\"start\":41041},{\"end\":41062,\"start\":41056},{\"end\":41075,\"start\":41070},{\"end\":41090,\"start\":41083},{\"end\":41101,\"start\":41097},{\"end\":41115,\"start\":41111},{\"end\":41134,\"start\":41129},{\"end\":41145,\"start\":41141},{\"end\":41154,\"start\":41152},{\"end\":41171,\"start\":41159},{\"end\":41496,\"start\":41491},{\"end\":41510,\"start\":41506},{\"end\":41522,\"start\":41519},{\"end\":41537,\"start\":41533},{\"end\":41547,\"start\":41544},{\"end\":41557,\"start\":41553},{\"end\":41567,\"start\":41563},{\"end\":41584,\"start\":41579},{\"end\":41595,\"start\":41593},{\"end\":44038,\"start\":44033}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":44071489},\"end\":28863,\"start\":28528},{\"attributes\":{\"doi\":\"arXiv:2001.05674\",\"id\":\"b1\"},\"end\":29328,\"start\":28865},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b2\"},\"end\":29654,\"start\":29330},{\"attributes\":{\"doi\":\"arXiv:2305.14314\",\"id\":\"b3\"},\"end\":29950,\"start\":29656},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b4\"},\"end\":30207,\"start\":29952},{\"attributes\":{\"doi\":\"arXiv:2208.11580\",\"id\":\"b5\"},\"end\":30522,\"start\":30209},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b6\"},\"end\":30886,\"start\":30524},{\"attributes\":{\"id\":\"b7\"},\"end\":31122,\"start\":30888},{\"attributes\":{\"id\":\"b8\"},\"end\":31209,\"start\":31124},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7057040},\"end\":31538,\"start\":31211},{\"attributes\":{\"doi\":\"arXiv:1601.06071\",\"id\":\"b10\"},\"end\":31717,\"start\":31540},{\"attributes\":{\"doi\":\"arXiv:2306.07629\",\"id\":\"b11\"},\"end\":32094,\"start\":31719},{\"attributes\":{\"doi\":\"arXiv:2208.09225\",\"id\":\"b12\"},\"end\":32438,\"start\":32096},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7785881},\"end\":32696,\"start\":32440},{\"attributes\":{\"doi\":\"arXiv:2306.00978\",\"id\":\"b14\"},\"end\":33072,\"start\":32698},{\"attributes\":{\"doi\":\"arXiv:2305.17888\",\"id\":\"b15\"},\"end\":33574,\"start\":33074},{\"attributes\":{\"id\":\"b16\"},\"end\":33814,\"start\":33576},{\"attributes\":{\"doi\":\"arXiv:1705.01462\",\"id\":\"b17\"},\"end\":34192,\"start\":33816},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16299141},\"end\":34506,\"start\":34194},{\"attributes\":{\"doi\":\"arXiv:2209.05433\",\"id\":\"b19\"},\"end\":34969,\"start\":34508},{\"attributes\":{\"id\":\"b20\"},\"end\":35085,\"start\":34971},{\"attributes\":{\"id\":\"b21\"},\"end\":35168,\"start\":35087},{\"attributes\":{\"doi\":\"arXiv:2211.05102\",\"id\":\"b22\"},\"end\":35632,\"start\":35170},{\"attributes\":{\"id\":\"b23\"},\"end\":36052,\"start\":35634},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b24\"},\"end\":36578,\"start\":36054},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b25\"},\"end\":37195,\"start\":36580},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b26\"},\"end\":37693,\"start\":37197},{\"attributes\":{\"doi\":\"arXiv:2303.17951\",\"id\":\"b27\"},\"end\":38181,\"start\":37695},{\"attributes\":{\"doi\":\"arXiv:2304.09145\",\"id\":\"b28\"},\"end\":38681,\"start\":38183},{\"attributes\":{\"doi\":\"arXiv:2004.09602\",\"id\":\"b29\"},\"end\":39077,\"start\":38683},{\"attributes\":{\"doi\":\"arXiv:2301.12017\",\"id\":\"b30\"},\"end\":39505,\"start\":39079},{\"attributes\":{\"doi\":\"arXiv:2206.01859\",\"id\":\"b31\"},\"end\":39857,\"start\":39507},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b32\"},\"end\":40243,\"start\":39859},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b33\"},\"end\":40677,\"start\":40245},{\"attributes\":{\"doi\":\"arXiv:2303.08302\",\"id\":\"b34\"},\"end\":41033,\"start\":40679},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b35\"},\"end\":41483,\"start\":41035},{\"attributes\":{\"doi\":\"arXiv:2305.12356\",\"id\":\"b36\"},\"end\":41963,\"start\":41485},{\"attributes\":{\"id\":\"b37\"},\"end\":43880,\"start\":41965},{\"attributes\":{\"id\":\"b38\"},\"end\":44195,\"start\":43882},{\"attributes\":{\"id\":\"b39\"},\"end\":44712,\"start\":44197}]", "bib_title": "[{\"end\":28582,\"start\":28528},{\"end\":30906,\"start\":30888},{\"end\":31278,\"start\":31211},{\"end\":32460,\"start\":32440},{\"end\":34225,\"start\":34194},{\"end\":42623,\"start\":41965}]", "bib_author": "[{\"end\":28596,\"start\":28584},{\"end\":28609,\"start\":28596},{\"end\":28622,\"start\":28609},{\"end\":28637,\"start\":28622},{\"end\":28983,\"start\":28966},{\"end\":29006,\"start\":28983},{\"end\":29017,\"start\":29006},{\"end\":29032,\"start\":29017},{\"end\":29040,\"start\":29032},{\"end\":29055,\"start\":29040},{\"end\":29061,\"start\":29055},{\"end\":29344,\"start\":29330},{\"end\":29356,\"start\":29344},{\"end\":29372,\"start\":29356},{\"end\":29390,\"start\":29372},{\"end\":29670,\"start\":29656},{\"end\":29688,\"start\":29670},{\"end\":29702,\"start\":29688},{\"end\":29720,\"start\":29702},{\"end\":29727,\"start\":29720},{\"end\":30026,\"start\":30012},{\"end\":30044,\"start\":30026},{\"end\":30316,\"start\":30301},{\"end\":30330,\"start\":30316},{\"end\":30622,\"start\":30607},{\"end\":30646,\"start\":30622},{\"end\":30659,\"start\":30646},{\"end\":30669,\"start\":30659},{\"end\":30922,\"start\":30908},{\"end\":30934,\"start\":30922},{\"end\":30946,\"start\":30934},{\"end\":30957,\"start\":30946},{\"end\":30971,\"start\":30957},{\"end\":30980,\"start\":30971},{\"end\":31134,\"start\":31126},{\"end\":31142,\"start\":31134},{\"end\":31295,\"start\":31280},{\"end\":31304,\"start\":31295},{\"end\":31311,\"start\":31304},{\"end\":31551,\"start\":31540},{\"end\":31568,\"start\":31551},{\"end\":31774,\"start\":31762},{\"end\":31790,\"start\":31774},{\"end\":31804,\"start\":31790},{\"end\":31815,\"start\":31804},{\"end\":31825,\"start\":31815},{\"end\":31837,\"start\":31825},{\"end\":31848,\"start\":31837},{\"end\":31862,\"start\":31848},{\"end\":31871,\"start\":31862},{\"end\":32156,\"start\":32141},{\"end\":32173,\"start\":32156},{\"end\":32184,\"start\":32173},{\"end\":32198,\"start\":32184},{\"end\":32211,\"start\":32198},{\"end\":32231,\"start\":32211},{\"end\":32474,\"start\":32462},{\"end\":32482,\"start\":32474},{\"end\":32497,\"start\":32482},{\"end\":32504,\"start\":32497},{\"end\":32786,\"start\":32778},{\"end\":32800,\"start\":32786},{\"end\":32814,\"start\":32800},{\"end\":32826,\"start\":32814},{\"end\":32839,\"start\":32826},{\"end\":32849,\"start\":32839},{\"end\":33160,\"start\":33148},{\"end\":33173,\"start\":33160},{\"end\":33190,\"start\":33173},{\"end\":33203,\"start\":33190},{\"end\":33217,\"start\":33203},{\"end\":33232,\"start\":33217},{\"end\":33246,\"start\":33232},{\"end\":33273,\"start\":33246},{\"end\":33288,\"start\":33273},{\"end\":33686,\"start\":33662},{\"end\":33835,\"start\":33816},{\"end\":33850,\"start\":33835},{\"end\":33870,\"start\":33850},{\"end\":33884,\"start\":33870},{\"end\":33897,\"start\":33884},{\"end\":33912,\"start\":33897},{\"end\":34243,\"start\":34227},{\"end\":34258,\"start\":34243},{\"end\":34274,\"start\":34258},{\"end\":34290,\"start\":34274},{\"end\":34530,\"start\":34508},{\"end\":34544,\"start\":34530},{\"end\":34558,\"start\":34544},{\"end\":34573,\"start\":34558},{\"end\":34588,\"start\":34573},{\"end\":34611,\"start\":34588},{\"end\":34623,\"start\":34611},{\"end\":34643,\"start\":34623},{\"end\":34657,\"start\":34643},{\"end\":34670,\"start\":34657},{\"end\":34981,\"start\":34973},{\"end\":35000,\"start\":34981},{\"end\":35097,\"start\":35089},{\"end\":35105,\"start\":35097},{\"end\":35183,\"start\":35170},{\"end\":35199,\"start\":35183},{\"end\":35220,\"start\":35199},{\"end\":35234,\"start\":35220},{\"end\":35250,\"start\":35234},{\"end\":35267,\"start\":35250},{\"end\":35282,\"start\":35267},{\"end\":35294,\"start\":35282},{\"end\":35311,\"start\":35294},{\"end\":35322,\"start\":35311},{\"end\":35731,\"start\":35717},{\"end\":35745,\"start\":35731},{\"end\":35759,\"start\":35745},{\"end\":35774,\"start\":35759},{\"end\":35789,\"start\":35774},{\"end\":35805,\"start\":35789},{\"end\":35817,\"start\":35805},{\"end\":35825,\"start\":35817},{\"end\":35838,\"start\":35825},{\"end\":36076,\"start\":36054},{\"end\":36093,\"start\":36076},{\"end\":36106,\"start\":36093},{\"end\":36122,\"start\":36106},{\"end\":36135,\"start\":36122},{\"end\":36150,\"start\":36135},{\"end\":36176,\"start\":36150},{\"end\":36195,\"start\":36176},{\"end\":36210,\"start\":36195},{\"end\":36217,\"start\":36210},{\"end\":36699,\"start\":36685},{\"end\":36716,\"start\":36699},{\"end\":36732,\"start\":36716},{\"end\":36751,\"start\":36732},{\"end\":36771,\"start\":36751},{\"end\":36785,\"start\":36771},{\"end\":36795,\"start\":36785},{\"end\":36815,\"start\":36795},{\"end\":36831,\"start\":36815},{\"end\":36850,\"start\":36831},{\"end\":37211,\"start\":37197},{\"end\":37227,\"start\":37211},{\"end\":37244,\"start\":37227},{\"end\":37261,\"start\":37244},{\"end\":37281,\"start\":37261},{\"end\":37299,\"start\":37281},{\"end\":37323,\"start\":37299},{\"end\":37335,\"start\":37323},{\"end\":37350,\"start\":37335},{\"end\":37357,\"start\":37350},{\"end\":37719,\"start\":37695},{\"end\":37727,\"start\":37719},{\"end\":37738,\"start\":37727},{\"end\":37750,\"start\":37738},{\"end\":37760,\"start\":37750},{\"end\":37776,\"start\":37760},{\"end\":37790,\"start\":37776},{\"end\":37803,\"start\":37790},{\"end\":38313,\"start\":38300},{\"end\":38328,\"start\":38313},{\"end\":38339,\"start\":38328},{\"end\":38355,\"start\":38339},{\"end\":38368,\"start\":38355},{\"end\":38381,\"start\":38368},{\"end\":38396,\"start\":38381},{\"end\":38778,\"start\":38770},{\"end\":38792,\"start\":38778},{\"end\":38807,\"start\":38792},{\"end\":38822,\"start\":38807},{\"end\":38844,\"start\":38822},{\"end\":39198,\"start\":39186},{\"end\":39208,\"start\":39198},{\"end\":39232,\"start\":39208},{\"end\":39244,\"start\":39232},{\"end\":39256,\"start\":39244},{\"end\":39595,\"start\":39583},{\"end\":39607,\"start\":39595},{\"end\":39621,\"start\":39607},{\"end\":39634,\"start\":39621},{\"end\":39646,\"start\":39634},{\"end\":39965,\"start\":39949},{\"end\":39973,\"start\":39965},{\"end\":39989,\"start\":39973},{\"end\":40005,\"start\":39989},{\"end\":40015,\"start\":40005},{\"end\":40350,\"start\":40338},{\"end\":40374,\"start\":40350},{\"end\":40388,\"start\":40374},{\"end\":40400,\"start\":40388},{\"end\":40413,\"start\":40400},{\"end\":40425,\"start\":40413},{\"end\":40691,\"start\":40679},{\"end\":40701,\"start\":40691},{\"end\":40713,\"start\":40701},{\"end\":41048,\"start\":41035},{\"end\":41064,\"start\":41048},{\"end\":41077,\"start\":41064},{\"end\":41092,\"start\":41077},{\"end\":41103,\"start\":41092},{\"end\":41117,\"start\":41103},{\"end\":41136,\"start\":41117},{\"end\":41147,\"start\":41136},{\"end\":41156,\"start\":41147},{\"end\":41173,\"start\":41156},{\"end\":41498,\"start\":41485},{\"end\":41512,\"start\":41498},{\"end\":41524,\"start\":41512},{\"end\":41539,\"start\":41524},{\"end\":41549,\"start\":41539},{\"end\":41559,\"start\":41549},{\"end\":41569,\"start\":41559},{\"end\":41586,\"start\":41569},{\"end\":41597,\"start\":41586},{\"end\":44040,\"start\":44031}]", "bib_venue": "[{\"end\":28686,\"start\":28637},{\"end\":28964,\"start\":28865},{\"end\":29459,\"start\":29406},{\"end\":29781,\"start\":29743},{\"end\":30010,\"start\":29952},{\"end\":30299,\"start\":30209},{\"end\":30605,\"start\":30524},{\"end\":30999,\"start\":30980},{\"end\":31360,\"start\":31311},{\"end\":31607,\"start\":31584},{\"end\":31760,\"start\":31719},{\"end\":32139,\"start\":32096},{\"end\":32553,\"start\":32504},{\"end\":32776,\"start\":32698},{\"end\":33146,\"start\":33074},{\"end\":33660,\"start\":33576},{\"end\":33982,\"start\":33928},{\"end\":34342,\"start\":34290},{\"end\":34715,\"start\":34686},{\"end\":35379,\"start\":35338},{\"end\":35715,\"start\":35634},{\"end\":36289,\"start\":36233},{\"end\":36683,\"start\":36580},{\"end\":37418,\"start\":37373},{\"end\":37872,\"start\":37819},{\"end\":38298,\"start\":38183},{\"end\":38768,\"start\":38683},{\"end\":39184,\"start\":39079},{\"end\":39581,\"start\":39507},{\"end\":39947,\"start\":39859},{\"end\":40336,\"start\":40245},{\"end\":40836,\"start\":40729},{\"end\":41233,\"start\":41189},{\"end\":41702,\"start\":41613},{\"end\":42733,\"start\":42625},{\"end\":44029,\"start\":43882},{\"end\":44379,\"start\":44197},{\"end\":37916,\"start\":37874}]"}}}, "year": 2023, "month": 12, "day": 17}