{"id": 3556146, "updated": "2023-09-29 12:50:16.448", "metadata": {"title": "Learning to Adapt Structured Output Space for Semantic Segmentation", "authors": "[{\"first\":\"Yi-Hsuan\",\"last\":\"Tsai\",\"middle\":[]},{\"first\":\"Wei-Chih\",\"last\":\"Hung\",\"middle\":[]},{\"first\":\"Samuel\",\"last\":\"Schulter\",\"middle\":[]},{\"first\":\"Kihyuk\",\"last\":\"Sohn\",\"middle\":[]},{\"first\":\"Ming-Hsuan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Manmohan\",\"last\":\"Chandraker\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": 2, "day": 28}, "abstract": "Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1802.10349", "mag": "2963107255", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/TsaiHSS0C18", "doi": "10.1109/cvpr.2018.00780"}}, "content": {"source": {"pdf_hash": "e0b7c9de9b7f48a5276694b140865a4f2bb2e1d4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.10349v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1802.10349", "status": "GREEN"}}, "grobid": {"id": "a8e4ff702c1a31df5927398573b8e8b26088987d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e0b7c9de9b7f48a5276694b140865a4f2bb2e1d4.txt", "contents": "\nLearning to Adapt Structured Output Space for Semantic Segmentation\n\n\nYi-Hsuan Tsai \nNEC Laboratories America\n\n\nWei-Chih Hung \nUniversity of California\nMerced\n\nSamuel Schulter \nNEC Laboratories America\n\n\nKihyuk Sohn \nNEC Laboratories America\n\n\nMing-Hsuan Yang \nUniversity of California\nMerced\n\nManmohan Chandraker \nNEC Laboratories America\n\n\nLearning to Adapt Structured Output Space for Semantic Segmentation\n\nConvolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the stateof-the-art methods in terms of accuracy and visual quality.\n\nIntroduction\n\nSemantic segmentation aims to assign each pixel a semantic label, e.g., person, car, road or tree, in an image. Recently, methods based on convolutional neural networks (CNNs) have achieved significant progress in semantic segmentation [2,21,23,24,38,40,41] with applications for autonomous driving [9] and image editing [35]. The crux of CNN-based approaches is to annotate a large number of images that cover possible scene variations. However, this trained model may not generalize well to unseen images, especially when there is a domain gap between the training (source) and test (target) images. For instance, the distribution of appearance for objects and scenes may vary in different cities, and even weather and lighting conditions can change significantly in the same city. In such cases, rely- * Both authors contribute equally to this work. ing only on the supervised model that requires re-annotating per-pixel ground truths in different scenarios would entail prohibitively high labor cost.\n\nTo address this issue, knowledge transfer or domain adaptation techniques have been proposed to close the gap between source and target domains, where annotations are not available in the target domain. For image classification, one effective approach is to align features across two domains [8,25] such that the adapted features can generalize to both domains. Similar efforts have been made for semantic segmentation via adversarial learning in the feature space [3,13]. However, different from the image classification task, feature adaptation for semantic segmentation may suffer from the complexity of high-dimensional features that needs to encode diverse visual cues, including appearance, shape and context. This motivates us to develop an effective method for adapting pixel-level prediction tasks rather than using feature adaptation. In semantic segmentation, we note that the output space contains rich information, both spatially and locally. For instance, even if images from two domains are very different in appearance, their segmentation outputs share a significant amount of similarities, e.g., spatial layout and local context (see Figure 1). Based on this observation, we address the pixellevel domain adaptation problem in the output (segmentation) space.\n\nIn this paper, we propose an end-to-end CNN-based domain adaptation algorithm for semantic segmentation. Our formulation is based on adversarial learning in the output space, where the intuition is to directly make the predicted label distributions close to each other across source and target domains. Based on the generative adversarial network (GAN) [10,30,22], the proposed model consists of two parts: 1) a segmentation model to predict output results, and 2) a discriminator to distinguish whether the input is from the source or target segmentation output. With an adversarial loss, the proposed segmentation model aims to fool the discriminator, with the goal of generating similar distributions in the output space for either source or target images.\n\nThe proposed method also adapts features as the errors are back-propagated to the feature level from the output labels. However, one concern is that lower-level features may not be adapted well as they are far away from the high-level output labels. To address this issue, we develop a multilevel strategy by incorporating adversarial learning at different feature levels of the segmentation model. For instance, we can use both conv5 and conv4 features to predict segmentation results in the output space. Then two discriminators can be connected to each of the predicted output for multi-level adversarial learning. We perform one-stage endto-end training for the segmentation model and discriminators jointly, without using any prior knowledge of the data in the target domain. In the testing phase, we can simply discard discriminators and use the adapted segmentation model on target images, with no extra computational requirements.\n\nDue to the high labor cost of annotating segmentation ground truth, there has been great interest in large-scale synthetic datasets with annotations, e.g., GTA5 [31] and SYN-THIA [32]. As a result, one critical setting is to adapt the model trained on synthetic data to real-world datasets, such as Cityscapes [4]. We follow this setting and conduct extensive experiments to validate the proposed domain adaptation method. First, we use a strong baseline model that is able to generalize to different domains. We note that a strong baseline facilitates real-world applications and can evaluate the limitation of the proposed adaptation approach. Based on this baseline model, we show comparisons using adversarial adaptation in the feature and output spaces. Furthermore, we show that the multi-level adversarial learning improves the results over single-level adaptation. In addition to the synthetic-to-real setting, we show experimental results on the Cross-City dataset [3], where annotations are provided in one city (source), while testing the model on another unseen city (target). Overall, our method performs favorably against state-of-the-art algorithms on numerous benchmark datasets under different settings.\n\nThe contributions of this work are as follows. First, we propose a domain adaptation method for pixel-level semantic segmentation via adversarial learning. Second, we demonstrate that adaptation in the output (segmentation) space can effectively align scene layout and local context between source and target images. Third, a multi-level adversarial learning scheme is developed to adapt features at different levels of the segmentation model, which leads to improved performance.\n\n\nRelated Work\n\nSemantic Segmentation. State-of-the-art semantic segmentation methods are mainly based on the recent advances of deep neural networks. As proposed by Long et al. [24], one can transform a classification CNN (e.g., AlexNet [19], VGG [33], or ResNet [11]) to a fully-convolutional network (FCN) for semantic segmentation. Numerous methods have since been developed to improve this model by utilizing context information [15,40] or enlarging receptive fields [2,38]. To train these advanced networks, a substantial amount of dense pixel annotations must be collected in order to match the model capacity of deep CNNs. As a result, weakly and semi-supervised approaches [5,14,17,28,29] are proposed in recent years to reduce the heavy labeling cost of collecting segmentation ground truths. However, in most real-world applications, it is difficult to obtain weak annotations and the trained model may not generalize well to unseen image domains.\n\nAnother approach to tackle the annotation problem is to construct synthetic datasets based on rendering, e.g., GTA5 [31] and SYNTHIA [32]. While the data collection is less costly since the pixel-level annotation can be done with a partially automated process, these datasets are usually used in conjunction with real-world datasets for joint learning to improve the performance. However, when training solely on the synthetic dataset, the model does not generalize well to real-world data, mainly due to the large domain shift between synthetic images and real-world images, i.e., appearance differences are still significant with current rendering techniques. Although synthesizing more realistic images can decrease the domain shift, it is necessary to use domain adaptation to narrow the performance gap.\n\nDomain Adaptation. Domain adaptation methods for image classification have been developed to address the domain-shift problem between the source and target domains. Numerous methods [7,8,25,26,34,36,37] are developed based on CNN classifiers due to performance gain. The main insight behind these approaches is to tackle the problem by aligning the feature distribution between source and target images. Ganin et al. [7,8] propose the Domain-Adversarial Neural Network (DANN) to transfer the feature distribution. A number of variants have since been proposed with different loss functions [25,36,37] or classifiers [26]. Recently, the PixelDA method [1] addresses domain adaptation for image classification by transferring the source im- Figure 2. Algorithmic overview. Given images with the size H by W in source and target domains, we pass them through the segmentation network to obtain output predictions. For source predictions with C categories, a segmentation loss is computed based on the source ground truth. To make target predictions closer to the source ones, we utilize a discriminator to distinguish whether the input is from the source or target domain. Then an adversarial loss is calculated on the target prediction and is back-propagated to the segmentation network. We call this process as one adaptation module, and we illustrate our proposed multi-level adversarial learning by adopting two adaptation modules at two different levels here.\n\nages to target domain, thereby obtaining a simulated training set for target images.\n\nWe note that domain adaptation for pixel-level prediction tasks have not been explored widely. Hoffman et al. [13] introduce the task of domain adaptation on semantic segmentation by applying adversarial learning (i.e., DANN) in a fully-convolutional way on feature representations and additional category constraints similar to the constrained CNN [29]. Other methods focus on adapting synthetic-toreal or cross-city images by adopting class-wise adversarial learning [3] or label transfer [3]. Similar to the PixelDA method [1], one concurrent work, CyCADA [12] uses the CycleGAN [42] and transfers source domain images to the target domain with pixel alignment, thus generating extra training data combined with feature space adversarial learning [13].\n\nAlthough feature space adaptation has been successfully applied to image classification, pixel-level tasks such as semantic segmentation remains challenging based on feature adaptation-based approaches. In this paper, we use the property that pixel-level predictions are structured outputs that contain information spatially and locally, to propose an efficient domain adaptation algorithm through adversarial learning in the output space.\n\n\nAlgorithmic Overview\n\n\nOverview of the Proposed Model\n\nOur domain adaptation algorithm consists of two modules: a segmentation network G and the discriminator D i , where i indicates the level of a discriminator in the multilevel adversarial learning. Two sets of images \u2208 R H\u00d7W \u00d73 from source and target domains are denoted as {I S } and {I T }. We first forward the source image I s (with annotations) to the segmentation network for optimizing G. Then we predict the segmentation softmax output P t for the target image I t (without annotations). Since our goal is to make segmentation predictions P of source and target images (i.e., P s and P t ) close to each other, we use these two predictions as the input to the discriminator D i to distinguish whether the input is from the source or target domain. With an adversarial loss on the target prediction, the network propagates gradients from D i to G, which would encourage G to generate similar segmentation distributions in the target domain to the source prediction. Figure 2 shows the overview of the proposed algorithm.\n\n\nObjective Function for Domain Adaptation\n\nWith the proposed network, we formulate the adaptation task containing two loss functions from both modules:\nL(I s , I t ) = L seg (I s ) + \u03bb adv L adv (I t ),(1)\nwhere L seg is the cross-entropy loss using ground truth annotations in the source domain, and L adv is the adversarial loss that adapts predicted segmentations of target images to the distribution of source predictions (see Section 4). In (1), \u03bb adv is the weight used to balance the two losses.\n\n\nOutput Space Adaptation\n\nDifferent from image classification based on features [8,25] that describe the global visual information of the image, high-dimensional features learned for semantic segmentation encodes complex representations. As a result, adaptation in the feature space may not be the best choice for semantic segmentation. On the other hand, although segmentation outputs are in the low-dimensional space, they contain rich information, e.g., scene layout and context. Our intuition is that no matter images are from the source or target domain, their segmentations should share strong similarities, spatially and locally. Thus, we utilize this property to adapt low-dimensional softmax outputs of segmentation predictions via an adversarial learning scheme.\n\n\nSingle-level Adversarial Learning\n\nDiscriminator Training. Before introducing how to adapt the segmentation network via adversarial learning, we first describe the training objective for the discriminator. Given the segmentation softmax output P = G(I) \u2208 R H\u00d7W \u00d7C , where C is the number of categories, we forward P to a fully-convolutional discriminator D using a cross-entropy loss L d for the two classes (i.e., source and target). The loss can be written as:\nL d (P ) = \u2212 h,w (1 \u2212 z) log(D(P ) (h,w,0) ) (2) +z log(D(P ) (h,w,1) ),\nwhere z = 0 if the sample is drawn from the target domain, and z = 1 for the sample from the source domain. Segmentation Network Training. First, we define the segmentation loss in (1) as the cross-entropy loss for images from the source domain:\nL seg (I s ) = \u2212 h,w c\u2208C Y (h,w,c) s log(P (h,w,c) s ), (3)\nwhere Y s is the ground truth annotations for source images and P s = G(I s ) is the segmentation output. Second, for images in the target domain, we forward them to G and obtain the prediction P t = G(I t ). To make the distribution of P t closer to P s , we use an adversarial loss L adv in (1) as:\nL adv (I t ) = \u2212 h,w log(D(P t ) (h,w,1) ).(4)\nThis loss is designed to train the segmentation network and fool the discriminator by maximizing the probability of the target prediction being considered as the source prediction.\n\n\nMulti-level Adversarial Learning\n\nAlthough performing adversarial learning in the output space directly adapts predictions, low-level features may not be adapted well as they are far away from the output. Similar to the deep supervision method [20] that uses auxiliary loss for semantic segmentation [40], we incorporate additional adversarial module in the low-level feature space to enhance the adaptation. The training objective for the segmentation network can be extended from (1) as:\nL(I s , I t ) = i \u03bb i seg L i seg (I s ) + i \u03bb i adv L i adv (I t ),(5)\nwhere i indicates the level used for predicting the segmentation output. We note that, the segmentation output is still predicted in each feature space, before passing through individual discriminators for adversarial learning. Hence, L i seg (I s ) and L i adv (I t ) remain in the same form as in (3) and (4), respectively. Based on (5), we optimize the following min-max criterion:\nmax D min G L(I s , I t ).(6)\nThe ultimate goal is to minimize the segmentation loss in G for source images, while maximizing the probability of target predictions being considered as source predictions.\n\n\nNetwork Architecture and Training\n\nDiscriminator. For the discriminator, we use an architecture similar to [30] but utilize all fully-convolutional layers to retain the spatial information. The network consists of 5 convolution layers with kernel 4 \u00d7 4 and stride of 2, where the channel number is {64, 128, 256, 512, 1}, respectively. Except for the last layer, each convolution layer is followed by a leaky ReLU [27] parameterized by 0.2. An up-sampling layer is added to the last convolution layer for re-scaling the output to the size of the input. We do not use any batch-normalization layers [16] as we jointly train the discriminator with the segmentation network using a small batch size. Segmentation Network. It is essential to build upon a good baseline model to achieve high-quality segmentation results [2,38,40]. We adopt the DeepLab-v2 [2] framework with ResNet-101 [11] model pre-trained on ImageNet [6] as our segmentation baseline network. However, we do not use the multi-scale fusion strategy [2] due to the memory issue. Similar to the recent work on semantic segmentation [2,38], we remove the last classification layer and modify the stride of the last two convolution layers from 2 to 1, making the resolution of the output feature maps effectively 1/8 times the input image size. To enlarge the receptive field, we apply dilated convolution layers [38] in conv4 and conv5 layers with a stride of 2 and 4, respectively. After the last layer, we use the Atrous Spatial Pyramid Pooling (ASPP) [2] as the final classifier. Finally, we apply an up-sampling layer along with the softmax output to match the size of the input image. Based on this architecture, our segmentation model  Multi-level Adaptation Model. We construct the abovementioned discriminator and segmentation network as our single-level adaptation model. For the multi-level structure, we extract feature maps from the conv4 layer and add an ASPP module as the auxiliary classifier. Similarly, a discriminator with the same architecture is added for adversarial learning. Figure 2 shows the proposed multi-level adaptation model. In this paper, we use two levels due to the balance of its efficiency and accuracy. Network Training. To train the proposed single/multi-level adaptation model, we find that jointly training the segmentation network and discriminators in one stage is effective.\n\nIn each training batch, we first forward the source image I s to optimize the segmentation network for L seg in (3) and generate the output P s . For the target image I t , we obtain the segmentation output P t , and pass it along with P s to the discriminator for optimizing L d in (2). In addition, we compute the adversarial loss L adv in (4) for the target prediction P t . For the multi-level training objective in (5), we simply repeat the same procedure for each adaptation module. We implement our network using the PyTorch toolbox on a single Titan X GPU with 12 GB memory. To train the segmentation network, we use the Stochastic Gradient Descent (SGD) optimizer with Nesterov acceleration where the momentum is 0.9 and the weight decay is 10 \u22124 . The initial learning rate is set as 2.5 \u00d7 10 \u22124 and is decreased using the polynomial decay with power of 0.9 as mentioned in [2]. For training the discriminator, we use the Adam optimizer [18] with the learning rate as 10 \u22124 and the same polynomial decay as the segmentation network. The momentum is set as 0.9 and 0.99. \n\n\nExperimental Results\n\nIn this section, we present experimental results to validate the proposed domain adaptation method for semantic segmentation under different settings. First, we show evaluations of the model trained on synthetic datasets (i.e., GTA5 [31] and SYNTHIA [32]) and test the adapted model on real-world images from the Cityscapes [4] dataset. Extensive experiments including comparisons to the state-ofthe-art methods and ablation study are also conducted, e.g., adaptation in the feature/output spaces and single/multilevel adversarial learning. Second, we carry out experiments on the Cross-City dataset [3], where the model is trained on one city and adapted to another city without using annotations. In all the experiments, the IoU metric is used. The code and model are available at https: //github.com/wasidennis/AdaptSegNet.\n\n\nGTA5\n\nThe GTA5 dataset [31] consists of 24966 images with the resolution of 1914 \u00d7 1052 synthesized from the video game based on the city of Los Angeles. The ground truth annotations are compatible with the Cityscapes dataset [4] that contains 19 categories. Following [13], we use the full set of GTA5 and adapt the model to the Cityscapes training set with 2975 images. During testing, we evaluate on the Cityscapes validation set with 500 images.\n\nOverall Results. We present adaptation results in Table 1 with comparisons to the state-of-the-art domain adaptation methods [12,13,39]. For these approaches, the baseline model is trained using VGG-based architectures [24,38]. To fairly evaluate our method, we first use the same baseline architecture (VGG-16) and train our model with the proposed single-level adaptation module. Table 1 shows that our method performs favorably against the other algorithms. While these methods all have feature adaptation modules, our results show that adapting the model in the output space achieves better performance. We note that CyCADA [12] has a pixel adaptation module by transforming source domain images to the target domain and hence obtains additional training samples. Although this strategy achieves a similar performance as ours, one can always apply pixel transformation combined with our output space adaptation to improve the results.\n\nOn the other hand, we argue that utilizing a stronger baseline model is critical for understanding the importance of different adaptation components as well as for enhancing the performance to enable real-world applications. Thus, we use the ResNet-101 based network introduced in Section 5 and train the proposed adaptation model. Table 1 shows the baseline results only trained on source images without adaptation, with comparisons to our adapted models under different settings, including feature adaptation and single/multi-level adversarial learning in the output space. Figure 3 presents some example results for adapted segmentation. We note that for small objects such as poles and traffic signs, they are harder to adapt since they easily get merged with background classes.\n\nIn addition, another factor to evaluate the adaptation performance is to measure how much gap is narrowed between the adaptation model and the fully-supervised model. Hence, we train the model using annotated ground truths in the Cityscapes dataset as the oracle results. Table 2 shows the gap under different baseline models. We observe that, although the oracle result does not differ a lot between VGG-16 and ResNet-101 based models, the gap is larger for the VGG one. It suggests us that to narrow the gap, using a deeper model with larger capacity is more practical. Parameter Analysis. During optimizing the segmentation network G, it is essential to balance the weight between segmentation and adversarial losses. We first consider the single-level case in (1) and conduct experiments to observe the impact of changing \u03bb adv . Table 3 shows that a smaller \u03bb adv may not facilitate the training process significantly, Table 3. Sensitivity analysis of \u03bb adv for feature/output space domain adaptation in the proposed method. We show that output space adaptation can tolerate a wide range of \u03bb adv , while it is sensitive to change \u03bb adv for feature adaptation. while a larger \u03bb adv may propagate incorrect gradients to the network. We empirically choose \u03bb adv as 0.001 in the single-level setting. Feature Level v.s. Output Space Adaptation. In the single-level setting in (1), we compare results by using feature-level or output space adaptation via adversarial learning. For feature-level adaptation, we adopt a similar strategy as used in [13,3] and train our model accordingly. Table 1 shows that the proposed adaptation method in the output space performs better than the one in the feature level.\n\nIn addition, Table 3 shows that adaptation in the feature space is more sensitive to \u03bb adv , which causes the training process more difficult, while output space adaptation allows for a wider range of \u03bb adv . One reason is that as feature adaptation is performed in the high-dimensional space, the problem for the discriminator becomes easier. Thus, such an adapted model cannot effectively match distributions between source and target domains via adversarial learning.\n\nSingle-level v.s. Multi-level Adversarial Learning. We have shown the merits of adopting adversarial learning in the output space. In addition, we present the results of using multi-level adversarial learning in Table 1. Here, we utilize an additional adversarial module (see Figure 2) and jointly optimize (5) for two levels. To properly balance \u03bb i seg and \u03bb i adv , we use the same weight as in the single-level setting for the high-level output space (i.e., \u03bb 1 seg = 1 and \u03bb 1 adv = 0.001). Since the low-level output carries less information to predict the segmentation, we use smaller weights for both the segmentation and adversarial loss (i.e., \u03bb 2 seg = 0.1 and \u03bb 2 adv = 0.0002). Evaluation results show that our multilevel adversarial adaptation further improves the segmentation accuracy. More results and analysis are presented in the supplementary material.\n\n\nSYNTHIA\n\nTo adapt from the SYNTHIA to Cityscapes datasets, we use the SYNTHIA-RAND-CITYSCAPES [32] set as the source domain which contains 9400 images compatible with the cityscapes annotated classes. Similar to [3], we evaluate images on the Cityscapes validation set with 13  (1) and (5), we use the same ones as in the case of GTA5 dataset. Table 4 shows evaluation results of the proposed algorithm against the state-of-the-art methods [3,13,39] that use feature adaptation. Similar to the experiments with the GTA5 dataset, we first utilize the same VGG-based model and train our single-level adaptation model for fair comparisons. The experimental results suggest that adapting the model in the output space performs better. Second, we compare results using different components of the proposed method with the ResNet based model. We show that the multi-level adaptation module improves the results over the baseline, feature space adaptation and single-level adaptation models. In addition, we present comparisons of mean IoU gap between adapted and oracle results in Table 5. Our method achieves the smallest gap and is the only one that can minimize the gap below 30%.\n\n\nCross-City Dataset\n\nIn addition to the synthetic-to-real adaptation for a larger domain gap, we conduct experiment on the Cross-City dataset [3] with smaller domain gaps between cities. The dataset contains four different cities: Rio, Rome, Tokyo and Taipei, in which each city has 3200 images without annotations and 100 images with pixel-level ground truths for 13 classes. Similar to [3], we use the Cityscapes training set as the source domain and adapt it to each target city using 3200 images, while 100 annotated images are used for evaluation. Since a smaller domain gap results in smaller output differences, we use smaller weights for the adversarial loss (i.e., \u03bb i adv = 0.0005) when training our models, while the weights for segmentation remain the same as previous experiments.\n\nWe show our results in Table 6 with comparisons to [3] and our baseline models under different settings. Again, our final multi-level model achieves consistent improvement for different cities, which demonstrates the advantages of the proposed adaptation method in the output space. Note that the state-of-the-art method [3] uses a different baseline model, and we present it as a reference to analyze how much the proposed algorithm can improve.\n\n\nConcluding Remarks\n\nIn this paper, we exploit the fact that segmentations are structured outputs and share many similarities between source and target domains. We tackle the domain adaptation problem for semantic segmentation via adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Experimental results show that the proposed method performs favorably against numerous baseline models and the state-of-the-art algorithms. We hope that our proposed method can be a generic adaptation model for a wide range of pixel-level prediction tasks. Table 6. Results of adapting Cityscapes to the Cross-City dataset. We construct our baseline model using the ResNet-101 architecture, and compare results between feature adaptation and our multi-level adaptation method in the output space. Target Image Ground Truth Before Adaptation Feature Adaptation Ours Figure 3. Example results of adapted segmentation for GTA5-to-Cityscapes. For each target image, we show results before adaptation, with feature adaptation and our adapted segmentations in the output space.\n\nFigure 1 .\n1Our motivation of learning adaptation in the output space. While images may be very different in appearance, their outputs are structured and share many similarities, such as spatial layout and local context.\n\n\n.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 achieves 65.1% mean intersection-over-union (IoU) when trained on the Cityscapes [4] training set and tested on the Cityscapes validation set.\n\nTable 1 .\n1Results of adapting GTA5 to Cityscapes. We first compare our results using single-level adversarial learning in the output space with other state-of-the-art algorithms with the VGG-16 based model. Then we adopt the ResNet-101 based model and present ablation study on different components of our proposed method.GTA5 \u2192 Cityscapes \n\n\n\nTable 2 .\n2Performance gap between the adapted model and the fully-supervised (oracle) model. We first compare results with state-of-the-art methods using the VGG based model, and then show our result using the ResNet one.GTA5 \u2192 Cityscapes \n\nmethod \nBaseline Adapt Oracle mIoU Gap \n\nFCNs in the Wild [13] \n\nVGG-16 \n\n27.1 64.6 \n-37.5 \nCDA [39] \n28.9 60.3 \n-31.4 \nCyCADA (feature) [12] \n29.2 60.3 \n-30.5 \nCyCADA (pixel) [12] \n34.8 60.3 \n-24.9 \nOurs (single-level) \n35.0 61.8 \n-25.2 \n\nOurs (multi-level) \nResNet-101 42.4 65.1 \n-22.7 \n\n\n\nTable 4 .\n4Results of adapting SYNTHIA to Cityscapes. We first compare our results using single-level adversarial learning in the output space with other state-of-the-art algorithms with the VGG-16 based model. Then we adopt the ResNet-101 based model and present ablation study on different components of our proposed method.Table 5. Performance gap between the adapted model and the fully-supervised (oracle) model. We first compare results with state-of-the-art methods using the VGG based model, and then show our result using the ResNet one.SYNTHIA \u2192 CityscapesMethodBaseline Adapt Oracle mIoU GapFCNs in the Wild[13] SYNTHIA \u2192 Cityscapes \n\n\n\n\nCityscapes \u2192 Cross-City Ours (output space) 83.9 34.2 88.3 18.8 40.2 86.2 93.1 47.8 21.7 80.9 47.8 48.3 8.6 53.8 Ours (output space) 76.2 44.7 84.6 9.3 25.5 81.8 87.3 55.3 32.7 74.3 28.9 43.0 27.6 51.6 Ours (output space) 81.5 26.0 77.8 17.8 26.8 82.7 90.9 55.8 38.0 72.1 4.2 24.5 50.8 49.9 33.4 86.6 12.7 16.4 77.0 92.1 17.6 13.7 70.7 37.7 44.4 18.5 46.5 Ours (feature) 82.1 31.9 84.1 25.7 13.2 77.2 81.2 28.1 12.0 67.0 35.8 43.5 20.9 46.6 Ours (output space) 81.7 29.5 85.2 26.4 15.6 76.7 91.7 31.0 12.5 71.5 41.1 47.3 27.7 49.1City \nMethod \nroad \nsidewalk \nbuilding \nlight \nsign \nveg \nsky \nperson \nrider \ncar \nbus \nmbike \nbike \nmIoU \n\nRome \n\nCross-City [3] \n79.5 29.3 84.5 0.0 22.2 80.6 82.8 29.5 13.0 71.7 37.5 25.9 1.0 \n42.9 \nOur Baseline \n83.9 34.3 87.7 13.0 41.9 84.6 92.5 37.7 22.4 80.8 38.1 39.1 5.3 \n50.9 \nOurs (feature) \n78.8 28.6 85.5 16.6 40.1 85.3 79.6 42.4 20.7 79.6 58.8 45.5 6.1 \n51.4 \nRio \n\nCross-City [3] \n74.2 43.9 79.0 2.4 \n7.5 77.8 69.5 39.3 10.3 67.9 41.2 27.9 10.9 \n42.5 \nOur Baseline \n76.6 47.3 82.5 12.6 22.5 77.9 86.5 43.0 19.8 74.5 36.8 29.4 16.7 \n48.2 \nOurs (feature) \n73.7 44.2 83.0 6.1 18.1 79.6 86.9 51.0 22.1 73.7 31.4 48.3 28.4 \n49.7 \nTokyo \n\nCross-City [3] \n83.4 35.4 72.8 12.3 12.7 77.4 64.3 42.7 21.5 64.1 20.8 8.9 40.3 \n42.8 \nOur Baseline \n82.9 31.3 78.7 14.2 24.5 81.6 89.2 48.6 33.3 70.5 7.7 11.5 45.9 \n47.7 \nOurs (feature) \n81.5 30.8 76.6 15.3 20.2 82.0 84.0 49.4 33.3 70.5 4.5 24.3 51.6 \n48.0 \nTaipei \n\nCross-City [3] \n78.6 28.6 80.0 13.1 7.6 68.2 82.1 16.8 9.4 60.4 34.0 26.5 9.9 \n39.6 \nOur Baseline \n83.5 \nAcknowledgments. W.-C. Hung is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.\nUnsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, CVPR. 23K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr- ishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR, 2017. 2, 3\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, abs/1606.00915CoRRL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. CoRR, abs/1606.00915, 2016. 1, 2, 4, 5\n\nNo more discrimination: Cross city adaptation of road scene segmenters. Y.-H Chen, W.-Y Chen, Y.-T Chen, B.-C Tsai, Y.-C F Wang, M Sun, ICCV. 7Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017. 1, 2, 3, 5, 6, 7, 8\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, CVPR. 6M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 2, 5, 6\n\nBoxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation. J Dai, K He, J Sun, ICCV. J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic seg- mentation. In ICCV, 2015. 2\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 4\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, ICML. Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. 2\n\nDomainadversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, JMLR. Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain- adversarial training of neural networks. In JMLR, 2016. 1, 2, 4\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, CVPR. A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au- tonomous driving? the kitti vision benchmark suite. In CVPR, 2012. 1\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In NIPS, 2014. 2\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 24K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 2, 4\n\nCycada: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A A Efros, T Darrell, abs/1711.03213CoRR36J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adver- sarial domain adaptation. CoRR, abs/1711.03213, 2017. 3, 5, 6\n\nFcns in the wild: Pixel-level adversarial and constraint-based adaptation. J Hoffman, D Wang, F Yu, T Darrell, abs/1612.02649CoRR67J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adapta- tion. CoRR, abs/1612.02649, 2016. 1, 3, 5, 6, 7\n\nDecoupled deep neural network for semi-supervised semantic segmentation. S Hong, H Noh, B Han, NIPS. S. Hong, H. Noh, and B. Han. Decoupled deep neural net- work for semi-supervised semantic segmentation. In NIPS, 2015. 2\n\nScene parsing with global context embedding. W.-C Hung, Y.-H Tsai, X Shen, Z Lin, K Sunkavalli, X Lu, M.-H Yang, ICCV. W.-C. Hung, Y.-H. Tsai, X. Shen, Z. Lin, K. Sunkavalli, X. Lu, and M.-H. Yang. Scene parsing with global context embedding. In ICCV, 2017. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 4\n\nSimple does it: Weakly supervised instance and semantic segmentation. A Khoreva, R Benenson, J Hosang, M Hein, B Schiele, CVPR. A. Khoreva, R. Benenson, J. Hosang, M. Hein, and B. Schiele. Simple does it: Weakly supervised instance and semantic segmentation. In CVPR, 2017. 2\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 2\n\nDeeplysupervised nets. C Lee, S Xie, P W Gallagher, Z Zhang, Z Tu, AISTATS. C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeply- supervised nets. In AISTATS, 2015. 4\n\nEfficient piecewise training of deep structured models for semantic segmentation. G Lin, C Shen, A Van Dan Hengel, I Reid, CVPR. G. Lin, C. Shen, A. van dan Hengel, and I. Reid. Efficient piecewise training of deep structured models for semantic segmentation. In CVPR, 2016. 1\n\nCoupled generative adversarial networks. M.-Y Liu, O Tuzel, NIPS. M.-Y. Liu and O. Tuzel. Coupled generative adversarial net- works. In NIPS, 2016. 2\n\nSemantic image segmentation via deep parsing network. Z Liu, X Li, P Luo, C C Loy, X Tang, ICCV. Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic im- age segmentation via deep parsing network. In ICCV, 2015. 1\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. 6J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1, 2, 6\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M Jordan, ICML. M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer- able features with deep adaptation networks. In ICML, 2015. 1, 2, 4\n\nUnsupervised domain adaptation with residual transfer networks. M Long, H Zhu, J Wang, M I Jordan, NIPS. M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In NIPS, 2016. 2\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Y Ng, ICML. A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlin- earities improve neural network acoustic models. In ICML, 2013. 4\n\nWeakly-and semi-supervised learning of a dcnn for semantic image segmentation. G Papandreou, L.-C Chen, K Murphy, A L Yuille, ICCV. G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille. Weakly-and semi-supervised learning of a dcnn for semantic image segmentation. In ICCV, 2015. 2\n\nConstrained convolutional neural networks for weakly supervised segmentation. D Pathak, P Krahenbuhl, T Darrell, ICCV. 23D. Pathak, P. Krahenbuhl, and T. Darrell. Constrained con- volutional neural networks for weakly supervised segmenta- tion. In ICCV, 2015. 2, 3\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, ICLR. 24A. Radford, L. Metz, and S. Chintala. Unsupervised repre- sentation learning with deep convolutional generative adver- sarial networks. In ICLR, 2016. 2, 4\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, ECCV. 25S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016. 2, 5\n\nThe SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, J Materzynska, D Vazquez, A Lopez, CVPR. 6G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, 2016. 2, 5, 6\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 2\n\nUnsupervised domain adaptation for face recognition in unlabeled videos. K Sohn, S Liu, G Zhong, X Yu, M.-H Yang, M Chandraker, ICCV. K. Sohn, S. Liu, G. Zhong, X. Yu, M.-H. Yang, and M. Chan- draker. Unsupervised domain adaptation for face recognition in unlabeled videos. In ICCV, 2017. 2\n\nDeep image harmonization. Y.-H Tsai, X Shen, Z Lin, K Sunkavalli, X Lu, M.-H Yang, CVPR. Y.-H. Tsai, X. Shen, Z. Lin, K. Sunkavalli, X. Lu, and M.-H. Yang. Deep image harmonization. In CVPR, 2017. 1\n\nSimultaneous deep transfer across domains and tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, ICCV. E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane- ous deep transfer across domains and tasks. In ICCV, 2015. 2\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, CVPR. E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. 2\n\nMulti-scale context aggregation by dilated convolutions. F Yu, V Koltun, ICLR. 6F. Yu and V. Koltun. Multi-scale context aggregation by di- lated convolutions. In ICLR, 2016. 1, 2, 4, 6\n\nCurriculum domain adaptation for semantic segmentation of urban scenes. Y Zhang, P David, B Gong, ICCV. 67Y. Zhang, P. David, and B. Gong. Curriculum domain adap- tation for semantic segmentation of urban scenes. In ICCV, 2017. 5, 6, 7\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, CVPR. H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. In CVPR, 2017. 1, 2, 4\n\nS Zheng, S Jayasumana, B Romera-Paredes, V Vineet, Z Su, D Du, C Huang, P Torr, Conditional random fields as recurrent neural networks. In ICCV. S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. Torr. Conditional random fields as recurrent neural networks. In ICCV, 2015. 1\n\nUnpaired imageto-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image- to-image translation using cycle-consistent adversarial net- works. ICCV, 2017. 3\n", "annotations": {"author": "[{\"end\":112,\"start\":71},{\"end\":160,\"start\":113},{\"end\":204,\"start\":161},{\"end\":244,\"start\":205},{\"end\":294,\"start\":245},{\"end\":342,\"start\":295}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":80},{\"end\":126,\"start\":122},{\"end\":176,\"start\":168},{\"end\":216,\"start\":212},{\"end\":260,\"start\":256},{\"end\":314,\"start\":304}]", "author_first_name": "[{\"end\":79,\"start\":71},{\"end\":121,\"start\":113},{\"end\":167,\"start\":161},{\"end\":211,\"start\":205},{\"end\":255,\"start\":245},{\"end\":303,\"start\":295}]", "author_affiliation": "[{\"end\":111,\"start\":86},{\"end\":159,\"start\":128},{\"end\":203,\"start\":178},{\"end\":243,\"start\":218},{\"end\":293,\"start\":262},{\"end\":341,\"start\":316}]", "title": "[{\"end\":68,\"start\":1},{\"end\":410,\"start\":343}]", "venue": null, "abstract": "[{\"end\":1499,\"start\":412}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1754,\"start\":1751},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1757,\"start\":1754},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1760,\"start\":1757},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1763,\"start\":1760},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1766,\"start\":1763},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1769,\"start\":1766},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1772,\"start\":1769},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1817,\"start\":1814},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1840,\"start\":1836},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2816,\"start\":2813},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2819,\"start\":2816},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2989,\"start\":2986},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2992,\"start\":2989},{\"end\":3681,\"start\":3672},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4159,\"start\":4156},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4162,\"start\":4159},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5665,\"start\":5661},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5683,\"start\":5679},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5813,\"start\":5810},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6477,\"start\":6474},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7385,\"start\":7381},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7445,\"start\":7441},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7455,\"start\":7451},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7471,\"start\":7467},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7641,\"start\":7637},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7644,\"start\":7641},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7678,\"start\":7675},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7681,\"start\":7678},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7888,\"start\":7885},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7891,\"start\":7888},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7894,\"start\":7891},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7897,\"start\":7894},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7900,\"start\":7897},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8283,\"start\":8279},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8300,\"start\":8296},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9158,\"start\":9155},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9160,\"start\":9158},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9163,\"start\":9160},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9166,\"start\":9163},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9169,\"start\":9166},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9172,\"start\":9169},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9175,\"start\":9172},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9393,\"start\":9390},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9395,\"start\":9393},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9567,\"start\":9563},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9570,\"start\":9567},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9573,\"start\":9570},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9593,\"start\":9589},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9627,\"start\":9624},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10636,\"start\":10632},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10875,\"start\":10871},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10994,\"start\":10991},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11016,\"start\":11013},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11051,\"start\":11048},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11085,\"start\":11081},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11108,\"start\":11104},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11276,\"start\":11272},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13391,\"start\":13388},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13394,\"start\":13391},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15704,\"start\":15700},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15760,\"start\":15756},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16720,\"start\":16716},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17027,\"start\":17023},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17211,\"start\":17207},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17428,\"start\":17425},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17431,\"start\":17428},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17434,\"start\":17431},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17463,\"start\":17460},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17494,\"start\":17490},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17528,\"start\":17525},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17625,\"start\":17622},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17706,\"start\":17703},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17709,\"start\":17706},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17986,\"start\":17982},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18127,\"start\":18124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19275,\"start\":19272},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19876,\"start\":19873},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19940,\"start\":19936},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20331,\"start\":20327},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20348,\"start\":20344},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20421,\"start\":20418},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20697,\"start\":20694},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20950,\"start\":20946},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21152,\"start\":21149},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21196,\"start\":21192},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21503,\"start\":21499},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21506,\"start\":21503},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21509,\"start\":21506},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21597,\"start\":21593},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21600,\"start\":21597},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22006,\"start\":22002},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23866,\"start\":23863},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24650,\"start\":24646},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24652,\"start\":24650},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26253,\"start\":26249},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26370,\"start\":26367},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26598,\"start\":26595},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26601,\"start\":26598},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26604,\"start\":26601},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27479,\"start\":27476},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27725,\"start\":27722},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28183,\"start\":28180},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28453,\"start\":28450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31736,\"start\":31732},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31811,\"start\":31810}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30011,\"start\":29790},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30233,\"start\":30012},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30578,\"start\":30234},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31112,\"start\":30579},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31760,\"start\":31113},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33312,\"start\":31761}]", "paragraph": "[{\"end\":2519,\"start\":1515},{\"end\":3797,\"start\":2521},{\"end\":4558,\"start\":3799},{\"end\":5498,\"start\":4560},{\"end\":6720,\"start\":5500},{\"end\":7202,\"start\":6722},{\"end\":8161,\"start\":7219},{\"end\":8971,\"start\":8163},{\"end\":10434,\"start\":8973},{\"end\":10520,\"start\":10436},{\"end\":11277,\"start\":10522},{\"end\":11718,\"start\":11279},{\"end\":12802,\"start\":11776},{\"end\":12955,\"start\":12847},{\"end\":13306,\"start\":13010},{\"end\":14080,\"start\":13334},{\"end\":14545,\"start\":14118},{\"end\":14864,\"start\":14619},{\"end\":15225,\"start\":14925},{\"end\":15453,\"start\":15273},{\"end\":15945,\"start\":15490},{\"end\":16402,\"start\":16018},{\"end\":16606,\"start\":16433},{\"end\":18987,\"start\":16644},{\"end\":20069,\"start\":18989},{\"end\":20920,\"start\":20094},{\"end\":21372,\"start\":20929},{\"end\":22312,\"start\":21374},{\"end\":23097,\"start\":22314},{\"end\":24806,\"start\":23099},{\"end\":25278,\"start\":24808},{\"end\":26152,\"start\":25280},{\"end\":27332,\"start\":26164},{\"end\":28127,\"start\":27355},{\"end\":28575,\"start\":28129},{\"end\":29789,\"start\":28598}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13009,\"start\":12956},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14618,\"start\":14546},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14924,\"start\":14865},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15272,\"start\":15226},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16017,\"start\":15946},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16432,\"start\":16403}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21431,\"start\":21424},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21763,\"start\":21756},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22653,\"start\":22646},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23378,\"start\":23371},{\"end\":23940,\"start\":23933},{\"end\":24030,\"start\":24023},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24693,\"start\":24686},{\"end\":24828,\"start\":24821},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25499,\"start\":25492},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26506,\"start\":26499},{\"end\":27237,\"start\":27230},{\"end\":28159,\"start\":28152},{\"end\":29282,\"start\":29275}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1513,\"start\":1501},{\"attributes\":{\"n\":\"2.\"},\"end\":7217,\"start\":7205},{\"attributes\":{\"n\":\"3.\"},\"end\":11741,\"start\":11721},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11774,\"start\":11744},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12845,\"start\":12805},{\"attributes\":{\"n\":\"4.\"},\"end\":13332,\"start\":13309},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14116,\"start\":14083},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15488,\"start\":15456},{\"attributes\":{\"n\":\"5.\"},\"end\":16642,\"start\":16609},{\"attributes\":{\"n\":\"6.\"},\"end\":20092,\"start\":20072},{\"attributes\":{\"n\":\"6.1.\"},\"end\":20927,\"start\":20923},{\"attributes\":{\"n\":\"6.2.\"},\"end\":26162,\"start\":26155},{\"attributes\":{\"n\":\"6.3.\"},\"end\":27353,\"start\":27335},{\"attributes\":{\"n\":\"7.\"},\"end\":28596,\"start\":28578},{\"end\":29801,\"start\":29791},{\"end\":30244,\"start\":30235},{\"end\":30589,\"start\":30580},{\"end\":31123,\"start\":31114}]", "table": "[{\"end\":30578,\"start\":30558},{\"end\":31112,\"start\":30802},{\"end\":31760,\"start\":31737},{\"end\":33312,\"start\":32293}]", "figure_caption": "[{\"end\":30011,\"start\":29803},{\"end\":30233,\"start\":30014},{\"end\":30558,\"start\":30246},{\"end\":30802,\"start\":30591},{\"end\":31737,\"start\":31125},{\"end\":32293,\"start\":31763}]", "figure_ref": "[{\"end\":9720,\"start\":9712},{\"end\":12756,\"start\":12748},{\"end\":18676,\"start\":18668},{\"end\":22898,\"start\":22890},{\"end\":25564,\"start\":25556},{\"end\":29591,\"start\":29583}]", "bib_author_first_name": "[{\"end\":33507,\"start\":33506},{\"end\":33520,\"start\":33519},{\"end\":33533,\"start\":33532},{\"end\":33542,\"start\":33541},{\"end\":33551,\"start\":33550},{\"end\":33856,\"start\":33852},{\"end\":33864,\"start\":33863},{\"end\":33878,\"start\":33877},{\"end\":33890,\"start\":33889},{\"end\":33900,\"start\":33899},{\"end\":33902,\"start\":33901},{\"end\":34229,\"start\":34225},{\"end\":34240,\"start\":34236},{\"end\":34251,\"start\":34247},{\"end\":34262,\"start\":34258},{\"end\":34273,\"start\":34269},{\"end\":34275,\"start\":34274},{\"end\":34283,\"start\":34282},{\"end\":34543,\"start\":34542},{\"end\":34553,\"start\":34552},{\"end\":34562,\"start\":34561},{\"end\":34571,\"start\":34570},{\"end\":34582,\"start\":34581},{\"end\":34595,\"start\":34594},{\"end\":34607,\"start\":34606},{\"end\":34617,\"start\":34616},{\"end\":34625,\"start\":34624},{\"end\":34933,\"start\":34932},{\"end\":34940,\"start\":34939},{\"end\":34946,\"start\":34945},{\"end\":35156,\"start\":35155},{\"end\":35164,\"start\":35163},{\"end\":35172,\"start\":35171},{\"end\":35185,\"start\":35181},{\"end\":35191,\"start\":35190},{\"end\":35197,\"start\":35196},{\"end\":35399,\"start\":35398},{\"end\":35408,\"start\":35407},{\"end\":35570,\"start\":35569},{\"end\":35579,\"start\":35578},{\"end\":35591,\"start\":35590},{\"end\":35601,\"start\":35600},{\"end\":35612,\"start\":35611},{\"end\":35626,\"start\":35625},{\"end\":35640,\"start\":35639},{\"end\":35652,\"start\":35651},{\"end\":35922,\"start\":35921},{\"end\":35932,\"start\":35931},{\"end\":35940,\"start\":35939},{\"end\":36113,\"start\":36112},{\"end\":36127,\"start\":36126},{\"end\":36144,\"start\":36143},{\"end\":36153,\"start\":36152},{\"end\":36159,\"start\":36158},{\"end\":36175,\"start\":36174},{\"end\":36184,\"start\":36183},{\"end\":36197,\"start\":36196},{\"end\":36414,\"start\":36413},{\"end\":36420,\"start\":36419},{\"end\":36429,\"start\":36428},{\"end\":36436,\"start\":36435},{\"end\":36611,\"start\":36610},{\"end\":36622,\"start\":36621},{\"end\":36631,\"start\":36630},{\"end\":36642,\"start\":36638},{\"end\":36649,\"start\":36648},{\"end\":36658,\"start\":36657},{\"end\":36668,\"start\":36667},{\"end\":36670,\"start\":36669},{\"end\":36679,\"start\":36678},{\"end\":36972,\"start\":36971},{\"end\":36983,\"start\":36982},{\"end\":36991,\"start\":36990},{\"end\":36997,\"start\":36996},{\"end\":37265,\"start\":37264},{\"end\":37273,\"start\":37272},{\"end\":37280,\"start\":37279},{\"end\":37463,\"start\":37459},{\"end\":37474,\"start\":37470},{\"end\":37482,\"start\":37481},{\"end\":37490,\"start\":37489},{\"end\":37497,\"start\":37496},{\"end\":37511,\"start\":37510},{\"end\":37520,\"start\":37516},{\"end\":37770,\"start\":37769},{\"end\":37779,\"start\":37778},{\"end\":38003,\"start\":38002},{\"end\":38014,\"start\":38013},{\"end\":38026,\"start\":38025},{\"end\":38036,\"start\":38035},{\"end\":38044,\"start\":38043},{\"end\":38254,\"start\":38253},{\"end\":38256,\"start\":38255},{\"end\":38266,\"start\":38265},{\"end\":38429,\"start\":38428},{\"end\":38443,\"start\":38442},{\"end\":38456,\"start\":38455},{\"end\":38458,\"start\":38457},{\"end\":38627,\"start\":38626},{\"end\":38634,\"start\":38633},{\"end\":38641,\"start\":38640},{\"end\":38643,\"start\":38642},{\"end\":38656,\"start\":38655},{\"end\":38665,\"start\":38664},{\"end\":38862,\"start\":38861},{\"end\":38869,\"start\":38868},{\"end\":38877,\"start\":38876},{\"end\":38895,\"start\":38894},{\"end\":39102,\"start\":39098},{\"end\":39109,\"start\":39108},{\"end\":39263,\"start\":39262},{\"end\":39270,\"start\":39269},{\"end\":39276,\"start\":39275},{\"end\":39283,\"start\":39282},{\"end\":39285,\"start\":39284},{\"end\":39292,\"start\":39291},{\"end\":39483,\"start\":39482},{\"end\":39491,\"start\":39490},{\"end\":39504,\"start\":39503},{\"end\":39703,\"start\":39702},{\"end\":39711,\"start\":39710},{\"end\":39718,\"start\":39717},{\"end\":39726,\"start\":39725},{\"end\":39935,\"start\":39934},{\"end\":39943,\"start\":39942},{\"end\":39950,\"start\":39949},{\"end\":39958,\"start\":39957},{\"end\":39960,\"start\":39959},{\"end\":40167,\"start\":40166},{\"end\":40169,\"start\":40168},{\"end\":40177,\"start\":40176},{\"end\":40179,\"start\":40178},{\"end\":40189,\"start\":40188},{\"end\":40191,\"start\":40190},{\"end\":40407,\"start\":40406},{\"end\":40424,\"start\":40420},{\"end\":40432,\"start\":40431},{\"end\":40442,\"start\":40441},{\"end\":40444,\"start\":40443},{\"end\":40691,\"start\":40690},{\"end\":40701,\"start\":40700},{\"end\":40715,\"start\":40714},{\"end\":40973,\"start\":40972},{\"end\":40984,\"start\":40983},{\"end\":40992,\"start\":40991},{\"end\":41221,\"start\":41220},{\"end\":41223,\"start\":41222},{\"end\":41234,\"start\":41233},{\"end\":41244,\"start\":41243},{\"end\":41252,\"start\":41251},{\"end\":41496,\"start\":41495},{\"end\":41503,\"start\":41502},{\"end\":41514,\"start\":41513},{\"end\":41529,\"start\":41528},{\"end\":41540,\"start\":41539},{\"end\":41813,\"start\":41812},{\"end\":41825,\"start\":41824},{\"end\":42033,\"start\":42032},{\"end\":42041,\"start\":42040},{\"end\":42048,\"start\":42047},{\"end\":42057,\"start\":42056},{\"end\":42066,\"start\":42062},{\"end\":42074,\"start\":42073},{\"end\":42281,\"start\":42277},{\"end\":42289,\"start\":42288},{\"end\":42297,\"start\":42296},{\"end\":42304,\"start\":42303},{\"end\":42318,\"start\":42317},{\"end\":42327,\"start\":42323},{\"end\":42505,\"start\":42504},{\"end\":42514,\"start\":42513},{\"end\":42525,\"start\":42524},{\"end\":42536,\"start\":42535},{\"end\":42720,\"start\":42719},{\"end\":42729,\"start\":42728},{\"end\":42740,\"start\":42739},{\"end\":42750,\"start\":42749},{\"end\":42937,\"start\":42936},{\"end\":42943,\"start\":42942},{\"end\":43139,\"start\":43138},{\"end\":43148,\"start\":43147},{\"end\":43157,\"start\":43156},{\"end\":43335,\"start\":43334},{\"end\":43343,\"start\":43342},{\"end\":43350,\"start\":43349},{\"end\":43356,\"start\":43355},{\"end\":43364,\"start\":43363},{\"end\":43477,\"start\":43476},{\"end\":43486,\"start\":43485},{\"end\":43500,\"start\":43499},{\"end\":43518,\"start\":43517},{\"end\":43528,\"start\":43527},{\"end\":43534,\"start\":43533},{\"end\":43540,\"start\":43539},{\"end\":43549,\"start\":43548},{\"end\":43871,\"start\":43867},{\"end\":43878,\"start\":43877},{\"end\":43886,\"start\":43885},{\"end\":43895,\"start\":43894},{\"end\":43897,\"start\":43896}]", "bib_author_last_name": "[{\"end\":33517,\"start\":33508},{\"end\":33530,\"start\":33521},{\"end\":33539,\"start\":33534},{\"end\":33548,\"start\":33543},{\"end\":33560,\"start\":33552},{\"end\":33861,\"start\":33857},{\"end\":33875,\"start\":33865},{\"end\":33887,\"start\":33879},{\"end\":33897,\"start\":33891},{\"end\":33909,\"start\":33903},{\"end\":34234,\"start\":34230},{\"end\":34245,\"start\":34241},{\"end\":34256,\"start\":34252},{\"end\":34267,\"start\":34263},{\"end\":34280,\"start\":34276},{\"end\":34287,\"start\":34284},{\"end\":34550,\"start\":34544},{\"end\":34559,\"start\":34554},{\"end\":34568,\"start\":34563},{\"end\":34579,\"start\":34572},{\"end\":34592,\"start\":34583},{\"end\":34604,\"start\":34596},{\"end\":34614,\"start\":34608},{\"end\":34622,\"start\":34618},{\"end\":34633,\"start\":34626},{\"end\":34937,\"start\":34934},{\"end\":34943,\"start\":34941},{\"end\":34950,\"start\":34947},{\"end\":35161,\"start\":35157},{\"end\":35169,\"start\":35165},{\"end\":35179,\"start\":35173},{\"end\":35188,\"start\":35186},{\"end\":35194,\"start\":35192},{\"end\":35205,\"start\":35198},{\"end\":35405,\"start\":35400},{\"end\":35418,\"start\":35409},{\"end\":35576,\"start\":35571},{\"end\":35588,\"start\":35580},{\"end\":35598,\"start\":35592},{\"end\":35609,\"start\":35602},{\"end\":35623,\"start\":35613},{\"end\":35637,\"start\":35627},{\"end\":35649,\"start\":35641},{\"end\":35662,\"start\":35653},{\"end\":35929,\"start\":35923},{\"end\":35937,\"start\":35933},{\"end\":35948,\"start\":35941},{\"end\":36124,\"start\":36114},{\"end\":36141,\"start\":36128},{\"end\":36150,\"start\":36145},{\"end\":36156,\"start\":36154},{\"end\":36172,\"start\":36160},{\"end\":36181,\"start\":36176},{\"end\":36194,\"start\":36185},{\"end\":36204,\"start\":36198},{\"end\":36417,\"start\":36415},{\"end\":36426,\"start\":36421},{\"end\":36433,\"start\":36430},{\"end\":36440,\"start\":36437},{\"end\":36619,\"start\":36612},{\"end\":36628,\"start\":36623},{\"end\":36636,\"start\":36632},{\"end\":36646,\"start\":36643},{\"end\":36655,\"start\":36650},{\"end\":36665,\"start\":36659},{\"end\":36676,\"start\":36671},{\"end\":36687,\"start\":36680},{\"end\":36980,\"start\":36973},{\"end\":36988,\"start\":36984},{\"end\":36994,\"start\":36992},{\"end\":37005,\"start\":36998},{\"end\":37270,\"start\":37266},{\"end\":37277,\"start\":37274},{\"end\":37284,\"start\":37281},{\"end\":37468,\"start\":37464},{\"end\":37479,\"start\":37475},{\"end\":37487,\"start\":37483},{\"end\":37494,\"start\":37491},{\"end\":37508,\"start\":37498},{\"end\":37514,\"start\":37512},{\"end\":37525,\"start\":37521},{\"end\":37776,\"start\":37771},{\"end\":37787,\"start\":37780},{\"end\":38011,\"start\":38004},{\"end\":38023,\"start\":38015},{\"end\":38033,\"start\":38027},{\"end\":38041,\"start\":38037},{\"end\":38052,\"start\":38045},{\"end\":38263,\"start\":38257},{\"end\":38269,\"start\":38267},{\"end\":38440,\"start\":38430},{\"end\":38453,\"start\":38444},{\"end\":38465,\"start\":38459},{\"end\":38631,\"start\":38628},{\"end\":38638,\"start\":38635},{\"end\":38653,\"start\":38644},{\"end\":38662,\"start\":38657},{\"end\":38668,\"start\":38666},{\"end\":38866,\"start\":38863},{\"end\":38874,\"start\":38870},{\"end\":38892,\"start\":38878},{\"end\":38900,\"start\":38896},{\"end\":39106,\"start\":39103},{\"end\":39115,\"start\":39110},{\"end\":39267,\"start\":39264},{\"end\":39273,\"start\":39271},{\"end\":39280,\"start\":39277},{\"end\":39289,\"start\":39286},{\"end\":39297,\"start\":39293},{\"end\":39488,\"start\":39484},{\"end\":39501,\"start\":39492},{\"end\":39512,\"start\":39505},{\"end\":39708,\"start\":39704},{\"end\":39715,\"start\":39712},{\"end\":39723,\"start\":39719},{\"end\":39733,\"start\":39727},{\"end\":39940,\"start\":39936},{\"end\":39947,\"start\":39944},{\"end\":39955,\"start\":39951},{\"end\":39967,\"start\":39961},{\"end\":40174,\"start\":40170},{\"end\":40186,\"start\":40180},{\"end\":40194,\"start\":40192},{\"end\":40418,\"start\":40408},{\"end\":40429,\"start\":40425},{\"end\":40439,\"start\":40433},{\"end\":40451,\"start\":40445},{\"end\":40698,\"start\":40692},{\"end\":40712,\"start\":40702},{\"end\":40723,\"start\":40716},{\"end\":40981,\"start\":40974},{\"end\":40989,\"start\":40985},{\"end\":41001,\"start\":40993},{\"end\":41231,\"start\":41224},{\"end\":41241,\"start\":41235},{\"end\":41249,\"start\":41245},{\"end\":41259,\"start\":41253},{\"end\":41500,\"start\":41497},{\"end\":41511,\"start\":41504},{\"end\":41526,\"start\":41515},{\"end\":41537,\"start\":41530},{\"end\":41546,\"start\":41541},{\"end\":41822,\"start\":41814},{\"end\":41835,\"start\":41826},{\"end\":42038,\"start\":42034},{\"end\":42045,\"start\":42042},{\"end\":42054,\"start\":42049},{\"end\":42060,\"start\":42058},{\"end\":42071,\"start\":42067},{\"end\":42085,\"start\":42075},{\"end\":42286,\"start\":42282},{\"end\":42294,\"start\":42290},{\"end\":42301,\"start\":42298},{\"end\":42315,\"start\":42305},{\"end\":42321,\"start\":42319},{\"end\":42332,\"start\":42328},{\"end\":42511,\"start\":42506},{\"end\":42522,\"start\":42515},{\"end\":42533,\"start\":42526},{\"end\":42543,\"start\":42537},{\"end\":42726,\"start\":42721},{\"end\":42737,\"start\":42730},{\"end\":42747,\"start\":42741},{\"end\":42758,\"start\":42751},{\"end\":42940,\"start\":42938},{\"end\":42950,\"start\":42944},{\"end\":43145,\"start\":43140},{\"end\":43154,\"start\":43149},{\"end\":43162,\"start\":43158},{\"end\":43340,\"start\":43336},{\"end\":43347,\"start\":43344},{\"end\":43353,\"start\":43351},{\"end\":43361,\"start\":43357},{\"end\":43368,\"start\":43365},{\"end\":43483,\"start\":43478},{\"end\":43497,\"start\":43487},{\"end\":43515,\"start\":43501},{\"end\":43525,\"start\":43519},{\"end\":43531,\"start\":43529},{\"end\":43537,\"start\":43535},{\"end\":43546,\"start\":43541},{\"end\":43554,\"start\":43550},{\"end\":43875,\"start\":43872},{\"end\":43883,\"start\":43879},{\"end\":43892,\"start\":43887},{\"end\":43903,\"start\":43898}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206595056},\"end\":33737,\"start\":33425},{\"attributes\":{\"doi\":\"abs/1606.00915\",\"id\":\"b1\"},\"end\":34151,\"start\":33739},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1448074},\"end\":34477,\"start\":34153},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":502946},\"end\":34833,\"start\":34479},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1613420},\"end\":35100,\"start\":34835},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":57246310},\"end\":35345,\"start\":35102},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6755881},\"end\":35520,\"start\":35347},{\"attributes\":{\"id\":\"b7\"},\"end\":35848,\"start\":35522},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6724907},\"end\":36081,\"start\":35850},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1033682},\"end\":36365,\"start\":36083},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":36552,\"start\":36367},{\"attributes\":{\"doi\":\"abs/1711.03213\",\"id\":\"b11\"},\"end\":36894,\"start\":36554},{\"attributes\":{\"doi\":\"abs/1612.02649\",\"id\":\"b12\"},\"end\":37189,\"start\":36896},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11816781},\"end\":37412,\"start\":37191},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2898888},\"end\":37673,\"start\":37414},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5808102},\"end\":37930,\"start\":37675},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12124389},\"end\":38207,\"start\":37932},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":38361,\"start\":38209},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195908774},\"end\":38601,\"start\":38363},{\"attributes\":{\"id\":\"b19\"},\"end\":38777,\"start\":38603},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14554538},\"end\":39055,\"start\":38779},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10627900},\"end\":39206,\"start\":39057},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8254931},\"end\":39424,\"start\":39208},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1629541},\"end\":39638,\"start\":39426},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":556999},\"end\":39868,\"start\":39640},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":745350},\"end\":40099,\"start\":39870},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16489696},\"end\":40325,\"start\":40101},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3035960},\"end\":40610,\"start\":40327},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2359761},\"end\":40876,\"start\":40612},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11758569},\"end\":41166,\"start\":40878},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5844139},\"end\":41390,\"start\":41168},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206594095},\"end\":41742,\"start\":41392},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14124313},\"end\":41957,\"start\":41744},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2871984},\"end\":42249,\"start\":41959},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1033001},\"end\":42449,\"start\":42251},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2655115},\"end\":42671,\"start\":42451},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4357800},\"end\":42877,\"start\":42673},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":17127188},\"end\":43064,\"start\":42879},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11824004},\"end\":43301,\"start\":43066},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5299559},\"end\":43474,\"start\":43303},{\"attributes\":{\"id\":\"b40\"},\"end\":43785,\"start\":43476},{\"attributes\":{\"id\":\"b41\"},\"end\":44049,\"start\":43787}]", "bib_title": "[{\"end\":33504,\"start\":33425},{\"end\":34223,\"start\":34153},{\"end\":34540,\"start\":34479},{\"end\":34930,\"start\":34835},{\"end\":35153,\"start\":35102},{\"end\":35396,\"start\":35347},{\"end\":35567,\"start\":35522},{\"end\":35919,\"start\":35850},{\"end\":36110,\"start\":36083},{\"end\":36411,\"start\":36367},{\"end\":37262,\"start\":37191},{\"end\":37457,\"start\":37414},{\"end\":37767,\"start\":37675},{\"end\":38000,\"start\":37932},{\"end\":38251,\"start\":38209},{\"end\":38426,\"start\":38363},{\"end\":38624,\"start\":38603},{\"end\":38859,\"start\":38779},{\"end\":39096,\"start\":39057},{\"end\":39260,\"start\":39208},{\"end\":39480,\"start\":39426},{\"end\":39700,\"start\":39640},{\"end\":39932,\"start\":39870},{\"end\":40164,\"start\":40101},{\"end\":40404,\"start\":40327},{\"end\":40688,\"start\":40612},{\"end\":40970,\"start\":40878},{\"end\":41218,\"start\":41168},{\"end\":41493,\"start\":41392},{\"end\":41810,\"start\":41744},{\"end\":42030,\"start\":41959},{\"end\":42275,\"start\":42251},{\"end\":42502,\"start\":42451},{\"end\":42717,\"start\":42673},{\"end\":42934,\"start\":42879},{\"end\":43136,\"start\":43066},{\"end\":43332,\"start\":43303}]", "bib_author": "[{\"end\":33519,\"start\":33506},{\"end\":33532,\"start\":33519},{\"end\":33541,\"start\":33532},{\"end\":33550,\"start\":33541},{\"end\":33562,\"start\":33550},{\"end\":33863,\"start\":33852},{\"end\":33877,\"start\":33863},{\"end\":33889,\"start\":33877},{\"end\":33899,\"start\":33889},{\"end\":33911,\"start\":33899},{\"end\":34236,\"start\":34225},{\"end\":34247,\"start\":34236},{\"end\":34258,\"start\":34247},{\"end\":34269,\"start\":34258},{\"end\":34282,\"start\":34269},{\"end\":34289,\"start\":34282},{\"end\":34552,\"start\":34542},{\"end\":34561,\"start\":34552},{\"end\":34570,\"start\":34561},{\"end\":34581,\"start\":34570},{\"end\":34594,\"start\":34581},{\"end\":34606,\"start\":34594},{\"end\":34616,\"start\":34606},{\"end\":34624,\"start\":34616},{\"end\":34635,\"start\":34624},{\"end\":34939,\"start\":34932},{\"end\":34945,\"start\":34939},{\"end\":34952,\"start\":34945},{\"end\":35163,\"start\":35155},{\"end\":35171,\"start\":35163},{\"end\":35181,\"start\":35171},{\"end\":35190,\"start\":35181},{\"end\":35196,\"start\":35190},{\"end\":35207,\"start\":35196},{\"end\":35407,\"start\":35398},{\"end\":35420,\"start\":35407},{\"end\":35578,\"start\":35569},{\"end\":35590,\"start\":35578},{\"end\":35600,\"start\":35590},{\"end\":35611,\"start\":35600},{\"end\":35625,\"start\":35611},{\"end\":35639,\"start\":35625},{\"end\":35651,\"start\":35639},{\"end\":35664,\"start\":35651},{\"end\":35931,\"start\":35921},{\"end\":35939,\"start\":35931},{\"end\":35950,\"start\":35939},{\"end\":36126,\"start\":36112},{\"end\":36143,\"start\":36126},{\"end\":36152,\"start\":36143},{\"end\":36158,\"start\":36152},{\"end\":36174,\"start\":36158},{\"end\":36183,\"start\":36174},{\"end\":36196,\"start\":36183},{\"end\":36206,\"start\":36196},{\"end\":36419,\"start\":36413},{\"end\":36428,\"start\":36419},{\"end\":36435,\"start\":36428},{\"end\":36442,\"start\":36435},{\"end\":36621,\"start\":36610},{\"end\":36630,\"start\":36621},{\"end\":36638,\"start\":36630},{\"end\":36648,\"start\":36638},{\"end\":36657,\"start\":36648},{\"end\":36667,\"start\":36657},{\"end\":36678,\"start\":36667},{\"end\":36689,\"start\":36678},{\"end\":36982,\"start\":36971},{\"end\":36990,\"start\":36982},{\"end\":36996,\"start\":36990},{\"end\":37007,\"start\":36996},{\"end\":37272,\"start\":37264},{\"end\":37279,\"start\":37272},{\"end\":37286,\"start\":37279},{\"end\":37470,\"start\":37459},{\"end\":37481,\"start\":37470},{\"end\":37489,\"start\":37481},{\"end\":37496,\"start\":37489},{\"end\":37510,\"start\":37496},{\"end\":37516,\"start\":37510},{\"end\":37527,\"start\":37516},{\"end\":37778,\"start\":37769},{\"end\":37789,\"start\":37778},{\"end\":38013,\"start\":38002},{\"end\":38025,\"start\":38013},{\"end\":38035,\"start\":38025},{\"end\":38043,\"start\":38035},{\"end\":38054,\"start\":38043},{\"end\":38265,\"start\":38253},{\"end\":38271,\"start\":38265},{\"end\":38442,\"start\":38428},{\"end\":38455,\"start\":38442},{\"end\":38467,\"start\":38455},{\"end\":38633,\"start\":38626},{\"end\":38640,\"start\":38633},{\"end\":38655,\"start\":38640},{\"end\":38664,\"start\":38655},{\"end\":38670,\"start\":38664},{\"end\":38868,\"start\":38861},{\"end\":38876,\"start\":38868},{\"end\":38894,\"start\":38876},{\"end\":38902,\"start\":38894},{\"end\":39108,\"start\":39098},{\"end\":39117,\"start\":39108},{\"end\":39269,\"start\":39262},{\"end\":39275,\"start\":39269},{\"end\":39282,\"start\":39275},{\"end\":39291,\"start\":39282},{\"end\":39299,\"start\":39291},{\"end\":39490,\"start\":39482},{\"end\":39503,\"start\":39490},{\"end\":39514,\"start\":39503},{\"end\":39710,\"start\":39702},{\"end\":39717,\"start\":39710},{\"end\":39725,\"start\":39717},{\"end\":39735,\"start\":39725},{\"end\":39942,\"start\":39934},{\"end\":39949,\"start\":39942},{\"end\":39957,\"start\":39949},{\"end\":39969,\"start\":39957},{\"end\":40176,\"start\":40166},{\"end\":40188,\"start\":40176},{\"end\":40196,\"start\":40188},{\"end\":40420,\"start\":40406},{\"end\":40431,\"start\":40420},{\"end\":40441,\"start\":40431},{\"end\":40453,\"start\":40441},{\"end\":40700,\"start\":40690},{\"end\":40714,\"start\":40700},{\"end\":40725,\"start\":40714},{\"end\":40983,\"start\":40972},{\"end\":40991,\"start\":40983},{\"end\":41003,\"start\":40991},{\"end\":41233,\"start\":41220},{\"end\":41243,\"start\":41233},{\"end\":41251,\"start\":41243},{\"end\":41261,\"start\":41251},{\"end\":41502,\"start\":41495},{\"end\":41513,\"start\":41502},{\"end\":41528,\"start\":41513},{\"end\":41539,\"start\":41528},{\"end\":41548,\"start\":41539},{\"end\":41824,\"start\":41812},{\"end\":41837,\"start\":41824},{\"end\":42040,\"start\":42032},{\"end\":42047,\"start\":42040},{\"end\":42056,\"start\":42047},{\"end\":42062,\"start\":42056},{\"end\":42073,\"start\":42062},{\"end\":42087,\"start\":42073},{\"end\":42288,\"start\":42277},{\"end\":42296,\"start\":42288},{\"end\":42303,\"start\":42296},{\"end\":42317,\"start\":42303},{\"end\":42323,\"start\":42317},{\"end\":42334,\"start\":42323},{\"end\":42513,\"start\":42504},{\"end\":42524,\"start\":42513},{\"end\":42535,\"start\":42524},{\"end\":42545,\"start\":42535},{\"end\":42728,\"start\":42719},{\"end\":42739,\"start\":42728},{\"end\":42749,\"start\":42739},{\"end\":42760,\"start\":42749},{\"end\":42942,\"start\":42936},{\"end\":42952,\"start\":42942},{\"end\":43147,\"start\":43138},{\"end\":43156,\"start\":43147},{\"end\":43164,\"start\":43156},{\"end\":43342,\"start\":43334},{\"end\":43349,\"start\":43342},{\"end\":43355,\"start\":43349},{\"end\":43363,\"start\":43355},{\"end\":43370,\"start\":43363},{\"end\":43485,\"start\":43476},{\"end\":43499,\"start\":43485},{\"end\":43517,\"start\":43499},{\"end\":43527,\"start\":43517},{\"end\":43533,\"start\":43527},{\"end\":43539,\"start\":43533},{\"end\":43548,\"start\":43539},{\"end\":43556,\"start\":43548},{\"end\":43877,\"start\":43867},{\"end\":43885,\"start\":43877},{\"end\":43894,\"start\":43885},{\"end\":43905,\"start\":43894}]", "bib_venue": "[{\"end\":33566,\"start\":33562},{\"end\":33850,\"start\":33739},{\"end\":34293,\"start\":34289},{\"end\":34639,\"start\":34635},{\"end\":34956,\"start\":34952},{\"end\":35211,\"start\":35207},{\"end\":35424,\"start\":35420},{\"end\":35668,\"start\":35664},{\"end\":35954,\"start\":35950},{\"end\":36210,\"start\":36206},{\"end\":36446,\"start\":36442},{\"end\":36608,\"start\":36554},{\"end\":36969,\"start\":36896},{\"end\":37290,\"start\":37286},{\"end\":37531,\"start\":37527},{\"end\":37793,\"start\":37789},{\"end\":38058,\"start\":38054},{\"end\":38275,\"start\":38271},{\"end\":38471,\"start\":38467},{\"end\":38677,\"start\":38670},{\"end\":38906,\"start\":38902},{\"end\":39121,\"start\":39117},{\"end\":39303,\"start\":39299},{\"end\":39518,\"start\":39514},{\"end\":39739,\"start\":39735},{\"end\":39973,\"start\":39969},{\"end\":40200,\"start\":40196},{\"end\":40457,\"start\":40453},{\"end\":40729,\"start\":40725},{\"end\":41007,\"start\":41003},{\"end\":41265,\"start\":41261},{\"end\":41552,\"start\":41548},{\"end\":41841,\"start\":41837},{\"end\":42091,\"start\":42087},{\"end\":42338,\"start\":42334},{\"end\":42549,\"start\":42545},{\"end\":42764,\"start\":42760},{\"end\":42956,\"start\":42952},{\"end\":43168,\"start\":43164},{\"end\":43374,\"start\":43370},{\"end\":43619,\"start\":43556},{\"end\":43865,\"start\":43787}]"}}}, "year": 2023, "month": 12, "day": 17}