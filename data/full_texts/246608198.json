{"id": 246608198, "updated": "2023-10-05 17:09:48.901", "metadata": {"title": "Neural Dual Contouring", "authors": "[{\"first\":\"Zhiqin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Andrea\",\"last\":\"Tagliasacchi\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Funkhouser\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "Neural Dual Contouring. ACM Trans. Graph. 41, 4, Article 104 (July 2022), 13 pages", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We introduce neural dual contouring (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC). Like traditional DC, it produces exactly one vertex per grid cell and one quad for each grid edge intersection, a natural and efficient structure for reproducing sharp features. However, rather than computing vertex locations and edge crossings with hand-crafted functions that depend directly on difficult-to-obtain surface gradients, NDC uses a neural network to predict them. As a result, NDC can be trained to produce meshes from signed or unsigned distance fields, binary voxel grids, or point clouds (with or without normals); and it can produce open surfaces in cases where the input represents a sheet or partial surface. During experiments with five prominent datasets, we find that NDC, when trained on one of the datasets, generalizes well to the others. Furthermore, NDC provides better surface reconstruction accuracy, feature preservation, output complexity, triangle quality, and inference time in comparison to previous learned (e.g., neural marching cubes, convolutional occupancy networks) and traditional (e.g., Poisson) methods. Code and data are available at https://github.com/czq142857/NDC.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.01999", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/ChenTFZ22", "doi": "10.1145/3528223.3530108"}}, "content": {"source": {"pdf_hash": "f872d8b03a63ed9599a40578c1bb254c61747c39", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.01999v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b0b58d501ca052c8e6519209cc42329ddb85ffd0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f872d8b03a63ed9599a40578c1bb254c61747c39.txt", "contents": "\nNeural Dual Contouring\n\n\nZhiqin Chen \nThomas Funkhouser thomasfunkhouser@google.com \nUSAGoogle Research googleresearch@google.com \nHao Zhang haoz@sfu.ca. \nHao Zhang \n\nSimon Fraser University\nCanada\n\n\nANDREA TAGLIASACCHI\nGoogle Research\nSimon Fraser University\nCanada\n\n\nSimon Fraser University\nCanada\n\n\nSimon Fraser University\nCanada, zhiqinc@sfu.ca; Andrea Tagliasacchi, Google Research\n\n\nSimon Fraser University\nCanada, atagliasacchi@\n\n\nSimon Fraser University\nCanada\n\nNeural Dual Contouring\nAuthors' addresses: Zhiqin Chen,CCS Concepts: \u2022 Computing methodologies \u2192 Shape modeling Additional Key Words and Phrases: Surface reconstructionisosurfacere- construction from point cloudmachine learning\nFig. 1. Neural dual contouring (NDC) is a unified data-driven approach that learns to reconstruct meshes (bottom) from a variety of inputs (top): signed or unsigned distance fields, binary voxels, non-oriented point clouds, and noisy raw scans. Trained on CAD models, NDC generalizes to a broad range of shape types: CAD models with sharp edges, organic shapes, open surfaces for cloths, scans of indoor scenes, and even the non-orientable Mobi\u00fcs strip.We introduce neural dual contouring (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC). Like traditional DC, it produces exactly one vertex per grid cell and one quad for each grid edge intersection, a natural and efficient structure for reproducing sharp features. However, rather than computing vertex locations and edge crossings with hand-crafted functions that depend directly on difficult-to-obtain surface gradients, NDC uses a neural network to predict them. As a result, NDC can be trained to produce meshes from signed or unsigned distance fields, binary voxel grids, or point clouds (with or without normals); and it can produce open surfaces in cases where the input represents a sheet or partial surface. During experiments with five prominent datasets, we find that NDC, when trained on one of the datasets, generalizes well to the others. Furthermore, NDC provides better surface reconstruction accuracy, feature preservation, output complexity, triangle quality, and inference time in comparison to previous learned (e.g., neural marching cubes, convolutional occupancy networks) and traditional (e.g., Poisson) methods. Code and data are available at https://github.com/czq142857/NDC.\n\nINTRODUCTION\n\nPolygonal mesh reconstruction from discrete inputs such as point clouds and voxel grids has been one of the most classical and wellstudied problems in computer graphics [Berger et al. 2017;De Ara\u00fajo et al. 2015]. Current solutions to the problem are predominantly model-driven, often relying on assumptions such as those related to shape characteristics (e.g., watertightness, zero genus, etc.), surface interpolants (e.g., trilinearity), sampling conditions, surface normals, and other reconstruction priors. It is only recently that a few data-driven meshing methods have emerged. However, they have mostly focused on learning point set triangulations [Liu et al. 2020;Rakotosaona et al. 2021;Sharp and Ovsjanikov 2020]. One exception is Neural Marching Cubes (NMC) [Chen and Zhang 2021], a learning-based Marching Cubes (MC) approach for mesh reconstruction from a voxel grid of signed distances or binary occupancies. In comparison to the original MC algorithm [Lorensen and Cline 1987] and its best-known variant, MC33 [Chernyaev 1995], NMC uses tessellation templates with more adaptive mesh topologies and learns local shape priors from training meshes. As a result, NMC generalizes well to a broader range of shape types and excels at preserving sharp features, two long-standing issues in existing MC work. On the other hand, the NMC tessellation templates are necessarily more complex than those of MC and MC33. As a result, NMC typically outputs 4-8 times the number of triangles and incurs 100\u00d7 or more compute time to reconstruct a mesh.\n\nIn this paper, we introduce Neural Dual Contouring (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC) [Ju et al. 2002]. The key motivation for building our learning framework upon DC rather than MC is that it provides a more natural and more efficient means of reproducing sharp features. As shown in Figure 2, NDC only needs to predict one mesh vertex per grid cell (i.e., a cube) and one quad for each cell edge intersected by the underlying surface. In contrast, NMC requires 23 edge, face, and interior vertices per grid cell [Chen and Zhang 2021, Fig.5].\n\nA traditional drawback of the classical DC, as compared to MC, is that it requires gradients (i.e., surface normals) as input to compute a suitable vertex location within each cell. Our data-driven approach does not have this drawback. NDC employs a neural network trained on example 3D surface data to predict the vertex locations ( Figure 3). Our neural network learns to compute whatever gradients and/or contexts that are useful to reproduce the training surfaces, and thus can operate on a voxel grid without gradients as input.\n\nAnother key feature of DC is that its meshing only requires knowing whether a cell edge is intersected by the output surface or not [Li et al. 2010]. We can thus train our network to predict an intersection or crossing flag per edge, in addition to vertex locations, without accounting for signs at cell corners ( Figure 4). We refer to this version of our network as unsigned NDC, or UNDC for short. With the sign-agnostic UNDC, we can forgo both the input requirement on signed distances and the output requirement that the resulting mesh is closed and watertight, as for MC and its variants.\n\nOur learning model is built with 3D convolution neural networks (CNNs) separately trained for vertex prediction and the prediction of cell corner signs (NDC) or edge crossings (UNDC) 1 . Our network training is supervised with an L2 reconstruction loss against pseudo ground-truth vertices computed by DC and binary cross entropy loss for sign/crossing predictions. As in NMC, our CNNs are designed with limited receptive fields to ensure generalizability.\n\nWe train our NDC networks on a CAD dataset, ABC [Koch et al. 2019], and we test them on ABC and four other datasets to assess generalizability: 1) Thingi10K [Zhou and Jacobson 2016], a dataset of 3D-printing models, 2) FAUST [Bogo et al. 2014], a dataset of human body shapes, 3) MGN [Bhatnagar et al. 2019], a dataset of clothes as open surfaces, and 4) Matterport3D [Chang et al. 2017], a collection of scenes with noisy RGB-D depth images. Quantitative and qualitative evaluations on isosurfacing using voxel data as input suggest that NDC clearly outperforms MC33 and several variants of NMC in terms of mesh reconstruction quality, feature preservation, triangle quality, and inference time, when using signed (distances or binary voxels) grids as inputs. At the same time, NDC produces 4-8 times fewer mesh elements using 3-20 times less inference time, compared to NMC. Further experiments with point cloud inputs suggest that UNDC outperforms both classical non-learning based methods, such as Ball Pivoting [Bernardini et al. 1999], Screened Poisson reconstruction [Kazhdan and Hoppe 2013], and recent reconstructive neural networks such as SIREN [Sitzmann et al. 2020], Local Implicit Grids [Jiang et al. 2020], and Convolutional Occupancy Networks [Peng et al. 2020]. Qualitative and quantitative results show significant improvements for NDC in terms of reconstruction quality, feature preservation, and inference time. Our main contributions can be summarized as follows:\n\n\u2022 We propose the first data-driven approach to mesh reconstruction based on dual contouring. Unlike classical DC, which optimizes vertex locations within the confines of individual cells using a handcrafted Quadratic Error Function (QEF) [Garland and Heckbert 1997], NDC predicts vertex locations using a learned function, which eliminates the need for gradients in the input and accounts for local contextual information inherent in the training data. \u2022 A unified learning model that is applicable to a larger variety of inputs than previous meshing methods. As shown in Figure 1, the allowed inputs include signed/unsigned distance fields, binary voxels, and un-oriented point clouds. \u2022 A significant, 23:1, reduction in representational complexity by NDC over NMC translates to across-the-board gains, in terms of simplicity of the network architecture, as well as reduction in network capacity, training and inference times, and more; see Table 1 for a summary. \u2022 A sign-agnostic network, UNDC, that can produce open, even non-orientable, output surfaces; see Figure 1.\n\n\nRELATED WORK\n\nThe literature on mesh reconstruction is extensive and so we refer to several surveys for full coverage [Berger et al. 2017;De Ara\u00fajo et al. 2015]. In this section, we focus on techniques for isosurfacing (i.e., mesh extraction from discrete volume data) and surface reconstruction from point cloud data, with a focus on the recent data-driven approaches most closely related to our work. Then in Section 2.3, we formally define dual contouring (DC), establish notations used throughout the paper, and compare DC to marching cubes.\n\n\nIsosurfacing and differentiable reconstruction\n\nThe marching cubes (MC) approach for isosurfacing from discrete signed distances was first proposed concurrently by [Lorensen and Cline 1987] and [Wyvill et al. 1986]. Since then, many variants have followed, including the best-known MC33 [Chernyaev 1995], which correctly enumerated all possible topological cases for mesh tessellations, based on the trilinear interpolation assumption. Indeed, most of the MC follow-ups made the same assumption and are unable to recover sharp features. This issue was resolved by neural marching cubes (NMC) [Chen and Zhang 2021], which combines deep learning with MC for the first time, building on the premise that feature recovery can be learned from training meshes. Our work is inspired by NMC. In NDC, we combine deep learning with dual contouring (DC) [Ju et al. 2002] to bring key advantages of classical DC over MC into a learned mesh reconstruction model, without requiring any additional inputs (e.g. gradients). In addition to improved efficiency and reconstruction quality (see Section 4), our method also represents the first unified mesh reconstruction framework that can take on all the input types shown in Figure 1. To the best of our knowledge, no previous methods were designed to reconstruct meshes from unsigned distance fields.\n\nSeveral recent works, including deep marching cubes (DMC) [Liao et al. 2018], MeshSDF [Remelli et al. 2020], and Deformable Tetrahedral Meshes (DefTet) [Gao et al. 2020], propose differentiable mesh reconstruction schemes. While both these methods and NDC bring deep learning to mesh reconstruction, their focuses and strengths are quite different. DMC, MeshSDF, and DefTet all target end-to-end differentiability, while offering limited capabilities to reconstruct geometric and topological details. They also encode global features for their predictions, which can hinder both scalability, reconstruction quality (as downsampling is necessary during training), and generalizability. In contrast, our work focuses on learning a refined meshing model applicable to a variety of inputs. We target finegrained quality criteria related to feature preservation and surface quality. Our learning model is also local, hence highly scalable and generalizable to diverse shape types and classes.\n\n\nMesh reconstruction from point clouds\n\nMany methods have been proposed for surface mesh estimation from unorganized points. Following the taxonomy in [Berger et al. 2017], previous works can be characterized based on the underlying priors, e.g., smoothness [Kazhdan and Hoppe 2013], visibility [Curless and Levoy 1996], dense sampling [Amenta et al. 1998], primitives [Schnabel et al. 2009], and learning from data ]. Among the methods based on data priors, some compose surfaces explicitly from patches extracted from examples [Funkhouser et al. 2004;Pauly et al. 2005;Shen et al. 2012]. Others learn implicit priors, either for entire objects [Chen and Zhang 2019;Chibane et al. 2020;Mescheder et al. 2019;Park et al. 2019;Peng et al. 2021] or for patches [Badki et al. 2020;Groueix et al. 2018;Hanocka et al. 2020;Jiang et al. 2020;Mi et al. 2020;Peng et al. 2020;Sitzmann et al. 2020;]. Both NMC [Chen and Zhang 2021] and NDC are in the latter category: they learn implicit priors for local regions.\n\nSurface reconstruction methods also differ in whether they can work for input point clouds without normals [Atzmon and Lipman 2020;Tang et al. 2021], whether the output mesh interpolates the input points via triangulation [Liu et al. 2020;Rakotosaona et al. 2021;Sharp and Ovsjanikov 2020], and whether they can produce open surfaces from partial scans, e.g., via an advancing front scheme [Bernardini et al. 1999;Cohen-Steiner and Da 2004]. Of course, normals can be estimated in a preprocessing step (e.g., using [Boulch and Marlet 2012]), and open surfaces can be created from watertight reconstructions in a postprocessing step (e.g., using SurfaceTrimmer in [Kazhdan and Hoppe 2013]). However, these separate steps rely on heuristic algorithms with parameters that are difficult to tune (e.g., size of neighborhood for normal estimation, density of points for surface trimming, etc.). By comparison, our UNDC includes all these steps in a single learned process that can produce open, even non-orientable, meshes directly from unoriented point cloud inputs, with fast inference. Also, our method is non-interpolatory, hence insensitive to sampling non-uniformity and noise (with noise augmentation in training). In Section 4.6, we compare UNDC with several representative learning-based reconstruction networks [Jiang et al. 2020;Peng et al. 2020;Sitzmann et al. 2020] whose results are most competitive to ours. Technical details about these works are described in the supplementary material. Ju et al. [2002] introduced Dual Contouring to convert a Signed Distance Field \u03a6 : R 3 \u2192 R into a polygonal mesh M = (V, F ); see Figure 2 (top). This is achieved by discretizing the function on a lattice G = (X, E). It first samples the \u03a6 at the grid vertices X and determines their signs S. Then, it finds the zero crossings V E of the \u03a6 on the lattice edges spanning vertices with opposite signs. Next, it computes the gradients of the \u03a6 at those crossings, which provide surface normals N E . Finally, it creates quadrilateral polygonal faces F that are dual to the lattice edge crossings E.\n\n\nDual Contouring (DC)\n\nIn what follows we have |X| = \u00d7 \u00d7 lattice vertices, |E | = ( \u2212 1) \u00d7 ( \u2212 1) \u00d7 ( \u2212 1) \u00d7 3 lattice edges, and we index X by ( , , ), while we refer to edges as ( , ) \u2208 E. Dual contouring assumes as input:\nS \u2208 B | X | , S = S (\u03a6, G), (grid signs) (1) V E \u2208 R | E |\u00d73 , V E = V E (\u03a6, G), (edge vertices) (2) N E \u2208 R | E |\u00d73 , N E = N E (\u03a6, G), (edge normals)(3)\nwhere, analogously to marching cubes [Lorensen and Cline 1987], S are the signs of \u03a6 on the lattice vertices, that is S : sign(\u03a6(X)), V E computes the zero-crossings of \u03a6 along the lattice edges, and N E : \u2207\u03a6(V E ) are the gradients of \u03a6 measured at V E . Given these quantities, dual contouring generates a polygonal mesh, consisting of quad faces and corresponding vertices:\nF \u2208 B | E | , F = F (S),(4)V \u2208 R | X |\u00d73 , V = V (V E , N E ),(5)\nwhere, with a slight abuse of notations, we use the same nomenclature F for a polygonal face (i.e. tuple of vertex indices) and the Boolean value that determines whether the face should be created. Dual faces F are created only whenever lattice edges connect lattice vertices of opposite signs S:\nF : xor(S , S ), ( , ) \u2208 E.(6)\nVertices are created by triangulating, a-la Garland and Heckbert [1997], the planar constraints defined on the edges of each voxel in the lattice:\nV : arg min x \u2211\ufe01 e\u2208 G (N E e \u00b7 (x \u2212 V E e )) 2 ,(7)\nwhere G refers to the voxel rooted at X , and e iterates the 12 edges of the voxel.\n\nComparison to MC. DC has the drawback that it assumes the availability of the function's gradients N E . This perhaps justifies why it has not been as popular as MC, which only requires signs (1) and zero-crossings (2). Nonetheless, the mesh creation mechanism of dual contouring is significantly simpler than the one in MC, where the former involves simple Boolean operations, while the latter involves enumerating all possible combinations and employing lookup tables that define the corresponding topology. Further, note that MC tends to discard high frequency information (i.e. sharp corners), DC is capable of preserving such details to a much better extent.\n\n\nMETHOD\n\nIn this paper, we introduce a learning framework, neural dual contouring (NDC), that achieves the simplicity and sharp features of DC without requiring function gradients in the input. Given any common input representation I (e.g. point cloud, signed or unsigned distance functions, or voxelized grids), NDC can be formalized by a simple generalization of Equations (1,4,5). In particular, we introduce two different techniques, illustrated with a 2D example in Figure 3 and Figure 4, and detailed in what follows.\n\nThe first, and default, variant of our method, which reconstructs meshes based on sign prediction, is simply referred to as NDC. It can be algebraically formalized as:\nNDC(I) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 S = S (I, G; ), V = V (I, G; ), F = xor(S , S ).(8)\nThe logic controlling whether a face should be generated is identical to classical DC, while vertices and signs are predicted by neural networks (with trainable parameters ) that receive as input I. At the same time, the input requirements of NDC are closer to the ones of MC and NMC: we do not require the availability of normals as in classical DC since we do not perform explicit optimizations for vertex locations using (7). Instead, vertex positions are predicted with a network trained from examples.\n\nThe second variant, named UNDC, with U denoting \"unsigned\", is similar to NDC, but it directly predicts the existence of dual faces F rather than resorting to sign prediction:\nUNDC(I) = V = V (I, G; ), F = F (I, G; ).(9)\nThe key advantage of this variant is that it can produce surface crossings without having to rely upon differences of inside/outside signs at grid cell vertices. This feature allows UNDC to operate on unsigned distance fields or non-oriented point clouds (we employ the prefix U to indicate this variant's ability to operate on unsigned inputs). It also allows UNDC to produce mesh faces, likely in the form of thin sheets, in regions where the underlying object parts are thinner than one voxel. Clearly, such thin parts are not representable by differences of grid vertex signs, and as a result, methods including MC, NMC, as well as NDC, would not be able to reconstruct them at all; see Figure 8 in Section 4. Additionally, UNDC can produce open surfaces with boundaries directly for input data representing partial surfaces. These advantages are in contrast to other methods like MC and variants that guarantee their outputs to be watertight and can represent only solid objects without thin features.\n\n\nEncoders\n\nLet us now consider the design of V , S , and F for different types of input I: 1 \u25cb signed/unsigned distance functions, 2 \u25cb voxelized occupancy, and 3 \u25cb point clouds.\n\nDistance Function Inputs. When a Signed Distance Function (SDF) \u03a6 is provided as input, our model V first samples the function \u03a6 at the grid vertices X into a floating point tensor of shape |X|. We then use a 3D CNN to process this tensor; the 3D CNN has 6 layers, with the first 3 layers having kernel size 3 3 and the last 3 layers having kernel size 1 3 , an overall receptive field of 7 3 . We employ hidden layers with 64 channels to make the network computationally efficient (i.e. 37 fps) as it has few network weights (i.e. 1MB). Leaky ReLU activation functions are employed everywhere except at the output layer where sigmoids are used. Note that when NDC operates on SDFs, S is extremely efficient as it just requires the computation of a sign at lattice locations similarly to classical dual contouring. Finally, the architecture of F for the UNDC model is the same as V in the NDC model.\n\nVoxelized Occupancy Inputs. For this class of inputs, we use a network with almost the same architecture as for SDF input, but with a small modification to enlarge the receptive field to 15 3 (i.e. employ 7 rather than three 3 3 convolutional layers). Our rationale is that voxelized occupancies are heavily quantized, and a larger receptive field would allow the network to develop stronger priors to cope with the larger degree of ambiguity in the data.\n\nPoint Cloud Inputs. For point cloud inputs, we devise a local point cloud encoder network divided into two parts: 1 \u25cb point cloud processing and 2 \u25cb regular grid processing. The former is implemented as a dense PointNet++ [Qi et al. 2017b], while grid processing has three 3 3 convolution layers and three 1 3 convolution layers, hence of a similar architecture to the one used for inputs represented as grids. The network architecture is shown in Figure 6. Further details are available in the supplementary material.\n\n\nTraining data preparation\n\nTo obtain the training data, we place random 3D mesh objects in the grid G to compute signs, intersection points, and corresponding ground truth normals 2 , and then apply classical DC to obtain the ground truth vertex predictions. However, this process can result in aliased normals, which can lead to poorly positioned vertices after the optimization in (7); see Figure 5-(left).\n\nWhile this situation might seem problematic at first, we make the same observation made by Lehtinen et al. [2018] in this setting. In particular, the use of an L2 reconstruction loss, coupled with data augmentation, leads to a zero-mean distribution in the vertex positions predicted by our model; see Figure 5-(b). To achieve this zero-mean distribution of optimization residuals, we augment the training data by rotation (by /2 around the Euclidean axes), mirroring, and (global) sign inversion. Note this augmentation is not done within a mini-batch, but rather, we rely on stochastic gradient descent for aggregating data towards a zero-mean residual configuration progressively over the course of training.\n\n\nTraining losses\n\nGiven ground truth data, note that all the sub-networks within the NDC and UNDC models can be trained separately, leading to a simpler training setup where no hyper-parameter tuning between losses becomes necessary. Note that we leverage our input data to only supervise the prediction made by the networks in a narrowband around the input surface, with binary masks M S , M V that evaluate to one if we are within the narrow-band, and zero otherwise; see the supplementary material for additional details. This is because surfaces should only be created in the proximity of either changes in the sign of \u03a6, in occupancy for voxelized inputs, or proximity of the input points for point cloud inputs. We start with a simple L2 reconstruction loss of pseudo ground-truth vertices (i.e. as computed by dual contouring):\nL V ( ) = E ( I,M V ,V gt )\u223cD \u2211\ufe01 , , \u2225M V \u2299 ( V (I, G; ) \u2212 V gt ) \u2225 2 2 ,\nwhere \u2299 is the Hadamard product on G. For NDC, we supervise the prediction of signs via Binary Cross Entropy (BCE):\nL S ( ) = E ( I,M S ,S gt )\u223cD \u2211\ufe01 , , M S \u2299 BCE S (I, G; ), S gt .\nFinally, for UNDC the loss L F ( ) is analogous to L S ( ), and hence we do not repeat its definition.\n\n\nPost-processing\n\nWhen UNDC is operating on sparse or noisy point clouds, the function F (I, G; ) that predicts grid edge crossings can make mistakes, leading to small holes in the output mesh. Empirically, the holes are typically small and isolated (see Table 8), and so we can use a simple post-processing step to close them. We employ our tensor representation of the mesh F \u2208 B | E | to determine boundary edges from M, and we flip Boolean entries in F that would result in three/four edges to change their boundary state; see Figure 7. These post-processing steps are executed on the GPU, resulting in a negligible impact on the overall inference time.\n\n\nRESULTS AND EVALUATION\n\nWe ran a series of experiments with NDC and UNDC to evaluate their performance in comparison to previous methods for a variety of input types, including SDFs, unsigned distance fields (UDF), binary voxel grids, point clouds, and depth image scans.\n\n\nDatasets, training, and evaluation metrics\n\nIn all of our experiments, we train NDC and UNDC on the ABC dataset [Koch et al. 2019] following the protocols in NMC [Chen and Zhang 2021]. The ABC dataset consists of watertight triangle meshes of CAD shapes, which are characterized by their rich geometric features including both sharp edges and smooth curves, as well as their topological varieties. We only use the first chunk of ABC dataset for our experiments. We split the set into 80% training (4,280 shapes) and 20% testing (1,071 shapes). During the data preparation, we obtain meshes over 32 3 and 64 3 grids to train our network. We evaluate the methods on the test set of ABC.\n\nGeneralization. To assess generalization capabilities of the methods, we evaluate on four other datasets, also following the experimental settings as in NMC [Chen and Zhang 2021]. The additional test sets include 2,000 shapes from Thingi10K dataset [Zhou and Jacobson 2016], a dataset of 3D-printing models; 100 shapes of human bodies from FAUST dataset [Bogo et al. 2014], a dataset of organic shapes; several shapes in MGN [Bhatnagar et al. 2019], a dataset of clothes with open surfaces; and several rooms from Matterport3D [Chang et al. 2017], a dataset containing scans of indoor scenes acquired with depth cameras. In all cases, we evaluate NDC and UNDC after training on the ABC training set without any fine-tuning.\n\n\nMetrics\n\nWe evaluate surface reconstructions quantitatively by sampling 100K points uniformly distributed over the surface of the ground truth shape and the predicted shape, and then computing a suite of metrics that evaluate different aspects of the reconstruction. The metrics are divided into five groups.\n\nReconstruction accuracy. We use Chamfer Distance (CD) and Fscore (F1) to evaluate the overall quality of a reconstructed mesh. The metrics are good at capturing significant mistakes such as missing parts, but may not be informative for evaluating the visual quality. Therefore, we introduce other metrics to evaluate sharp feature preservation and surface quality.\n\nSharp feature preservation. We follow NMC [Chen and Zhang 2021] and use Edge Chamfer Distance (ECD) and Edge F-score (EF1) to evaluate the preservation of sharp edges. For a given shape, points are sampled near sharp edges and corners to form a set of edge samples. The ECD and EF1 between two shapes are simply the CD and F1 between their edge samples.\n\nSurface quality. As in many other papers, we use Normal Consistency (NC) to evaluate the quality of the surface normals. However, NC is similar to CD and F1 in that it mainly captures significant mistakes and neglects small mistakes which contribute significantly to visual artifacts. Therefore, we break down NC to show the percentage of inaccurate normals (% Inaccurate Normals, or %IN) according to a threshold. To compute %IN, for each point sampled from shape , we find its closest point in the points sampled from shape , and then compute their angle. If the angle is larger than the threshold, the point from is labeled as having an inaccurate normal. %IN (gt) is the percentage of points sampled from the ground truth shape that have inaccurate normals. %IN (pred) can be obtained similarly on points from the predicted shape. 3 Another aspect of mesh quality is the number of small angles in the reconstructed triangles. Therefore, we also report the percentage of small angles that are smaller than a threshold, as % Small Angles, or %SA.   Triangle & vertex counts. We count the number of vertices (#V) and triangles (#T) in the output shape to reveal the fidelity-complexity trade-off. Note that while our method generates quad faces, we always randomly split each quad into two triangles for evaluations and visualizations.\n\nInference time. We report inference times (seconds per shape) for the methods tested on the ABC test set. Timings are collected on the same machine with one NVIDIA GTX 1080ti GPU.\n\n\nReconstruction from SDF\n\nWe first test NDC and UNDC on mesh reconstruction from grids of signed distances, and compare them to Marching Cubes 33 (MC33) (an improved version of MC to guarantee topological correctness in Fig. 8. Mesh reconstruction results from SDF grid inputs at a relatively low resolution of 64 3 . The shapes in the first three columns are from ABC test set, and the last column from Thingi10K. Zoom in to see various surface artifacts and artifacts near edges on NMC-lite* and NMC* results, broken meshes from MC33 (red arrows), and non-manifold edges from NDC and UNDC (green arrows). Pay special attention to the thin sheets (blue arrows) reconstructed by the sign-agnostic UNDC, which correspond to parts of the ground truth shape that are thinner than one voxel. In contrast, none of the other methods (a-e) could even recover any of these thin parts.\n\neach cube [Chernyaev 1995;Lewiner et al. 2003]), classical DC with estimated normals (DC-est), and two versions of Neural Marching Cubes (NMC and NMC-lite) [Chen and Zhang 2021].\n\nDC-est takes the same SDF input as our method, obtains gradient values at grid points by local differentiation over the SDF, and runs the classical DC [Ju et al. 2002] as described in Figure 2 by estimating   the intersection points and their gradients via linear interpolation. Since NMC and NMC-lite use large networks to ensure the quality of the output meshes, which makes the inference significantly slower, we replace the large backbone networks in them with our 6-layer CNN to obtain NMC* and NMC-lite*, in order to have a fair comparison on reconstruction quality with respect to the inference time. In addition, we include the results of UNDC when the input is a grid of unsigned distances as UNDC (UDF). Since there is no method to reconstruct meshes from grids of unsigned distances to our knowledge, we do not compare it with other methods.\n\nWe show visual results in Figure 8, with more results provided in the supplementary material. We report the quantitative results on the ABC test set in Table 2. To make the paper compact, we reduce the sizes of the tables by removing some less representative metrics. The full tables can be found in the supplementary material.\n\nReconstruction accuracy. NDC and UNDC consistently outperform model-driven MC33 and DC-est in terms of CD and F1. Clearly, normal estimation is not expected to be accurate, especially near sharp features. The results of DC-est are similar to or slightly better than those of MC33, as shown in Figure 8 and Table 2.\n\nAlthough the network size has been significantly reduced in NMC* and NMC-lite*, they usually have slightly better results than NDC, since, given the same input resolution, Marching Cubes methods are able to reconstruct more inner structures inside each cube with their abundant tessellation templates, while NDC cannot due to its simple tessellation design. However, NMC and NMC-lite are significantly worse than UNDC in CD, even with their original large networks, due to the fact that UNDC can reconstruct thin structures which NMC methods and NDC cannot. Visual results in Figure 8 show some examples. Note in the first and the third columns (blue arrows), some thin structures are not reconstructed by any methods other than UNDC. In the fourth column and in Figure 9 , we show that even on smooth shapes, NDC and UNDC can preserve more details, such as the crevices, compared to MC33. However, in the second column (green arrows), we show failure cases from NDC and UNDC, where the walls of the tube are merged together with non-manifold edges, as a result of their simpler tessellations in each cube -this problem does not occur in NMC*. Tables 2, 3, and 4, NDC and UNDC consistently outperform MC33, DC-est, NMC*, and NMC-lite*, in ECD and EF1, for feature preservation. The only exception is the EF1 in Table 4, which may be due to NDC's tessellation being too simple to handle the fine structures of human shapes, e.g., the fingers.\n\n\nSharp features. As shown in\n\nNormal quality. Since the compared methods generally have similar point-wise reconstruction accuracies, the quality of the generated surfaces in terms of visual appearance can be a better differentiator. We consistently observe that after switching to smaller networks, NMC* and NMC-lite* tend to generate noisy surfaces even over flat regions, as shown in Figure 8 (c-d). We use %IN (IN = inaccurate normals) as a means to quantify the quality of surface normals, which correlate with surface quality. Figure 11 shows the %IN-threshold curves on the ABC test set, where the normal errors of NDC and UNDC are noticeably less than those from other methods for small threshold values, which is consistent with our visual observation that NMC* and NMC-lite* outputs exhibit more surface artifacts.\n\nTriangle quality. In Figure 11, we show %SA-threshold (SA = small angles) curves for various methods on the ABC test set, showing that NDC outperforms the others in triangle quality, with UNDC coming close. A visual comparison can be found in Figure 9.\n\nTriangle and vertex count. The #V and #T in Table 2 show that the vertex and triangle numbers of NDC and UNDC are very similar to those of MC33. NMC and NMC-lite produce more vertices and triangles due to their complex cube tessellation templates.\n\nInference time. With no deep networks involved, MC33 is undoubtedly the fastest, as shown by the inference times in Table 2. With our light network design, NDC and UNDC are next in line in terms of speed. Since NDC does not need sign predictions, it is half the size of and twice as fast as UNDC, running in real time on an NVIDIA GTX 1080ti GPU. With the newer RTX 3090 being twice as fast as GTX 1080ti, we expect UNDC to also run in real time on a higher-end GPU. In comparison, the original NMC and NMC-lite require more than a second to test on an input grid of 64 3 . Even after we replace their networks with our light designs, NMC* and NMC-lite* are still 2-4 times slower than UNDC and NDC, due to their more complex cube tessellations. Finally, the inference time for classical DC is far from optimal as we employed our own implementation of DC-est with an unoptimized QEF solver.\n\nRobustness to translation and rotation. We highly encourage the readers to watch the video in the supplementary material where we test the methods on a shape while moving and rotating the shape inside the sampling grid. It clearly shows that NDC and UNDC are the most robust compared to others.\n\nVarying input grid resolutions. We report quantitative results obtained by the various methods on 64 3 and 128 3 input resolutions in Table 2. When increasing the input resolution, the gap of reconstruction accuracy diminishes, as reflected by CD and F1. But NMC* and NMC-lite* will always produce significantly more vertices and triangles, and take more time to process a shape.\n\nGeneralizability. To show the generalizability of our method, we show the quantitative results on Thingi10K and FAUST in Table 3 and 4, respectively. Visual results can be found in Figure 8 and 9. All data-driven methods are only trained on the ABC training set. These results are consistent with our analysis above. Specifically, in terms of surface quality, we show %IN (pred) with error greater than 5 \u2022 in Table 3 to show that NMC* and NMC-lite* have more surface artifacts. However, on organic shapes from FAUST, MC33 outperforms deep learning methods, as shown by %IN (pred) in Table 4. It makes sense, because Marching Cubes was originally designed to reconstruct smooth shapes, and none of the deep learning methods are trained on smooth shapes. We also report %SA with angles less than 10 \u2022 in Table 3 and Table 4 for Thingi10K and FAUST, to show that our method produces fewer small-angle triangles.\n\n\nReconstruction from UDF\n\nAs shown in the last rows of Table 2, 3, and 4, the results on UDF are similar to those on SDF, but are usually worse due to the lack of signs. Still, UNDC is able to recover the shapes reasonably well by just observing the changes of unsigned distances in nearby cells. The visual results on UDF are very similar to the results on SDF when tested on the three datasets in the previous experiment. Therefore, we show the results of reconstructing clothes with open surfaces in MGN dataset [Bhatnagar et al. 2019] from grids of unsigned distances in Figure 10. Note that in our experiments with UDF, we do not compare with prior works, since, to the best of our knowledge, UNDC is the only method that can reconstruct meshes from UDF. \n\n\nReconstruction from binary voxels\n\nReconstructing meshes from binary voxels is clearly more challenging than from SDF grids. Learning from data is absolutely necessary in this scenario if one wishes to produce plausible outputs, as reflected in Table 5, where MC33 is significantly worse than all others in all metrics, except for vertex and triangle count. The results on Thingi10K and FAUST, and on 128 3 inputs are in the supplementary material. They show the same pattern as Table 5 and demonstrate the generalizability of our method. We show visual results in Figure 12, with more provided in the supplementary material. Many observations in Section 4.3 still apply here: our method is significantly faster than NMC* and NMC-lite* (Inference time), produces significantly less vertices and triangles (#V, #T), has better normal quality (NC), and can better preserve sharp edges and corners (ECD, EF1). Moreover, since binary voxels are more challenging than SDF grids, NMC* and NMC-lite* are underfitting, with reconstruction accuracy worse than NDC and UNDC, as reflected by CD and F1.\n\n\nReconstruction from point clouds\n\nWe test UNDC on the task of reconstructing meshes from point clouds. UNDC does not require normals as input, while most other methods do, making direct comparisons difficult to perform. To give competing methods a slight advantage, we provide normals to methods that require them, and re-iterate that our method does not leverage this additional information.\n\nBaselines. We compare against five methods, including classical Ball-pivoting [Bernardini et al. 1999] and screened Poisson [Kazhdan and Hoppe 2013]) surface reconstruction, as well as three deep learning methods like SIREN [Sitzmann et al. 2020], Local Implicit Grids (LIG) [Jiang et al. 2020], and Convolutional Occupancy Networks (ConvONet) [Peng et al. 2020]. The latter is the only method that does not require point normals, and we test its two variations proposed in the original paper: ConvONet-3plane which uses the 3-plane (xy, yz, xz) setting, and ConvONet-grid that uses the 3D grid setting. Note that we compare with SIREN rather than SAL [Atzmon and Lipman 2020] since SIREN is built upon SAL and has shown better performance in their paper. We test all methods with 4,096 input points. The output grid size of UNDC is 64 3 . We train all datadriven methods on the ABC training set for a fair comparison. More detailed discussions of each method can be found in the supplementary material. We illustrate these results in Figure 13. Quantitative results on the ABC test set are provided in Table 6, while results on Thingi10K and FAUST, which reveal a similar pattern and trend, can be found in the supplementary material.\n\nAnalysis. As shown in Table 6, UNDC outperforms all other methods in all reconstruction quality metrics. SIREN has the closest results to ours in terms of reconstruction accuracy, but it has to be trained for, i.e., \"overfit to\", each input shape. ConvONet, whose networks are not local, does not generalize well even within the ABC dataset. LIG is local and therefore expected to generalize better. However, its algorithm only considers the space around the input points and ignores the empty space. As a result, LIG generates many artifacts in the empty space, which are called \"back-faces\" in their paper, and a post-processing step is required to remove them. The post-processing step is not perfect, as shown in the first and fourth columns in Figure 13. UNDC and Ball-pivoting are the only two methods that directly output a mesh without iso-surface extraction, therefore they have the least numbers of vertices and triangles, and are the only two methods that can generate sharp features, as shown by ECD and EF1 in Table 6. As for inference time, UNDC is the fastest and significantly faster than all other methods compared, even the classical Ball-pivoting and Poisson. \n\n\nReconstruction from noisy real scans\n\nWe test UNDC on reconstructing meshes from raw scan data in Matterport3D [Chang et al. 2017]. The raw scan data contains depth images and camera parameters -we convert them into noisy point clouds as the input to our network. Since the point clouds represent large scenes, we first crop the scene into overlapping patches, and then run our network to obtain grids of edge intersection flags and vertex locations. Finally, we put together the predicted grids to form a large grid of the scene, and then run the meshing algorithm in Figure 4 to obtain the output mesh. Our network is larger than that of the previous experiment to accommodate for point cloud noise. Specifically, we replace the three 3 3 conv3d layers with 8 residual blocks [He et al. 2016]. During training, we also augment the input with Gaussian noise, with = 0.5, assuming that each cell of the output grid is a unit cube, to simulate noise present in the raw scans; see supplementary material for more details.\n\nBaselines. We compare UNDC with ConvONet, since it does not require ground truth point normals and is designed to reconstruct large scenes. We use the pre-trained weights provided by the authors for synthetic scenes with objects from ShapeNet [Chang et al. 2015], denoted as ConvONet (P). Additionally, we compare to Poisson with estimated point normals. We show visual comparisons in Figure 14. Note that different from the experiments in most other works on deep learning scene reconstruction, which test their methods on sampled points from the \"ground truth\" meshes, we test on raw scan data, which is a more realistic setting. We do not report quantitative results since the \"ground truth\" meshes provided with the dataset were reconstructed by Poisson, one of the methods we are comparing against.\n\nAnalysis. ConvONet is seen to significantly underperform compared to Poisson and UNDC, especially falling short in terms of surface quality and detail preservation. Therefore, we mainly compare UNDC with Poisson. Generally, UNDC produces less surface noise, which is especially obvious in the first row of Figure 14. The improvements are also observable in the other two rows, but they are less obvious due to the zoom-out to reveal the entire scenes. Since UNDC is trained on data with noise augmentation, it learns, to some extent, to remove noise.\n\nAlso, UNDC only reconstructs what is given in the input point cloud. In contrast, Poisson creates an implicit field of the scene, which could potentially inpaint the missing regions. However, such inpainting is not always desirable, and Poisson needs to trim the output mesh to remove surfaces that are generated in empty regions using a post-process (SurfaceTrimmer) that depends on careful tuning of parameters. If the trimming density threshold is too small, it may leave \"bubble\" artifacts as indicated by red arrows in Figure 14. If it is too large, it may accidentally trim the objects, as indicated by purple arrows in Figure 14. At the default setting shown, UNDC tends to produce more holes in the output, but avoids creating bubble artifacts, see the last row of Figure 14.\n\nWhile water-tightness can be beneficial as a prior, it can lead to poor reconstruction of thin surfaces; this can be observed from the blue arrows in Figure 14. One thing worth special attention is the bottom-most red arrow in the last row of Figure 14. The umbrella surface is thinner than a voxel. However, Poisson forces inside-outside by creating a bubble on top of the umbrella, so that the bottom of the bubble can form the surface of the umbrella. This creates an odd boundary after trimming.\n\nRobustness to varying point density and noise. We use synthetic data to study the robustness of UNDC on point clouds with varying density and noise. We train our network with point clouds whose point counts were randomly selected between 2,048 and 32,768, Fig. 14. Qualitative comparison between ConvONet, Poisson, and UNDC on reconstructing rooms in Matterport3D from raw scan data, where some walls and roofs are removed for better visualization. Colored arrows bring attention to regions where Poisson should be contrasted against UNDC. Red arrows: \"bubble\" artifacts caused by the water-tightness prior to Poisson; purple arrows: objects or parts incorrectly trimmed; blue arrows: poor reconstruction of thin surfaces. Green arrow in the bottom row points out an instance of better preservation of surface details by Poisson (the strip patterns are not noise or reconstruction artifacts); the flip side of this, however, is surface noise, as seen over Poisson reconstruction in the first row. where each point cloud is augmented with Gaussian noise whose is randomly sampled from [0, 0.5]. We then evaluate the trained network on point clouds of varying density and noise, and compare it to Poisson reconstruction with estimated point normals.\n\nSome quantitative results are shown in Table 7, where UNDC evidently outperforms Poisson. In Figure 15, we show visual results where the point density or noise varies within each input point cloud. Note that UNDC at 128 3 output resolution tends to produce worse results than at 64 3 output resolution. This is because relatively, point sparsity and noise level are both more significant at higherresolution grids, due to the smaller cell sizes. Fig. 15. Qualitative comparison between Poisson and UNDC on mesh reconstruction from point clouds with density or noise variations across the same shape, from its left to its right, as shown. The input point clouds in the first three rows have decreasing point density from left to right, while the inputs in the last three rows have increasing noise. UNDC@64 3 and UNDC@128 3 produce output grid sizes of 64 3 and 128 3 , respectively.\n\n\nCONCLUSIONS\n\nWe introduce neural dual contouring, a new data-driven approach to mesh reconstruction based on dual contouring. The volumetric version of our approach, NDC, takes the same input as MC and NMC, and it can better preserve sharp features while using approximately the same number of vertices and triangles as classical MC, which is 3-7 times reduction compared to NMC. The surface version of our approach, UNDC, is sign agnostic; it is therefore able to reconstruct open surfaces and thin structures from unsigned distance fields or unoriented point clouds. Both NDC and UNDC are designed as local networks using limited receptive fields, thus can generalize well to new datasets. Extensive experiments demonstrate the superior performance of our approach on multiple datasets over state-of-theart methods, whether learned (e.g., NMC, SIREN, LIG, ConvONet) or traditional (e.g., MC33, Poisson, Ball-Pivoting).\n\nLimitations. One limitation of our approach is that it can produce non-manifold meshes. Specifically, since DC and its descendants produce only one vertex per grid cell, they may create meshes with vertices and edges shared by multiple surface patches in cases where Table 8. Statistics on non-manifold and boundary edges produced by NDC and UNDC. The methods are tested on ABC test set with 64 3 output resolution. Non-manifold-3 denotes non-manifold edges with 3 adjacent faces, and Non-manifold-4 denotes those with 4 adjacent faces. Boundary-1 refers to boundary edges, defined as edges with only one adjacent face. MC would output multiple disconnected components within one cell; see the second column of Figure 8 where the green arrows indicate non-manifold edges created by NDC and UNDC. These cases happen fairly rarely (see statistics in Table 8) and are easily detected and fixed by splitting vertices/edges or \"tunnelling\" through them, using the techniques described in [Nielson 2004] or [Schaefer et al. 2007], for manifold dual contouring. However, UNDC can also produce open surfaces (with edges connected to one face) or non-manifold fins (where edges are shared by three faces). Creating open surfaces is generally good, as it allows reconstruction of thin features and partial inputs (e.g., note the thin sheets indicated by the blue arrows in the row of UNDC results in Figure 8 better approximate the ground truth). However, boundaries and fins may cause problems for downstream tasks that assume manifoldness as a pre-condition. Hence, UNDC may not be the best meshing solution for all applications.\n\nAnother limitation is that the output of NDC is not completely invariant to orientation. Although NDC is empirically less sensitive to rotations than NMC (see supplementary video), we still see that NDC occasionally generates coherence artifacts on sharp edges as an object rotates. One example is shown in Figure 16 (a). The artifact occurs when one or more vertices of the cube have SDF values very close to 0. It cannot be easily avoided since it is due to the continuity of neural networks. See the illustration in Figure 16 (b). When the SDF value of the vertex gradually moves from positive (outside) to negative (inside), the input to the network (the SDF values) changes smoothly, but the output needs to change in a discontinuous way in order to produce the ground truth. Since most neural networks are continuous, the output of the network will be continuous. Therefore the network will generate artifacts when such transitions occur.\n\nFuture works. Besides fixing the issues above, it would be interesting to incorporate the NDC framework into an end-to-end system for recovering surfaces from neural representations inferred from multiple images, possibly using Neural Radiance Fields [Mildenhall et al. 2020]. Or, UNDC could potentially be used with differentiable rendering to reconstruct one-sided surfaces from a sparse set of images acquired from cameras inside a scene. These and other NDC extensions are promising topics for future work.\n\n\nACKNOWLEDGMENTS\n\nWe thank the anonymous reviewers for their valuable comments and feedback. Thanks also go to Vincent Sitzmann, Daniel Duckworth, and Forrester Cole for helpful discussions. This work was supported in part by NSERC (no. 611370) and gift funds from Google in the form of a Google Faculty Award by the last author and a Google PhD Fellowship received by the first author.\n\n\nSupplementary Material\n\nThis document provides supplemental material for \"Neural Dual Contouring.\" It contains details regarding network architectures, mask definitions, training protocols, and experimental methods. It also has additional qualitative and quantitative results that would not fit in the main paper.\n\n\nPOINT CLOUD NETWORK ARCHITECTURE DETAILS\n\nOur network for processing point clouds is shown in Figure 6. The local PointNet in Figure 6 is similar to the set abstraction layer in PointNet++, with the number of local clusters being the same to the number of input points. For each point in the input point cloud, we find a local cluster with points, and then apply PointNet [Qi et al. 2017a] using relative coordinates of those points with respect to . In PointNet++, the local cluster is found by setting a radius , so that any points whose distances to are smaller than will be selected into the cluster. This may bring issues such as setting appropriate values and handling situations when a cluster has too many or too few points. Therefore, we use a simpler approach to avoid the issues. We find the nearest neighbors ( = 8 in our experiments) of to form the cluster, by using a KD-tree for efficient computation. Afterwards, we concatenate the relative coordinates of each point with its features, apply two fully-connected layers with leaky ReLU activation, and use max-pooling to aggregate the features of all points into the feature of . The residual block in Figure 6 is a standard residual block [He et al. 2016] for fully connected layers. The \"Pooling into grid\" module in Figure 6 is essentially a local PointNet as described above. The difference is that it uses the centers of the cells in the grid as query points to find the local clusters in the input point cloud via KNN. Since obtaining the features for all cell centers in a 3D gird is very expensive ( ( 3 )), we only compute features for the cells that are close to the input point cloud, i.e., the cells that are within 3 units (manhattan distance) to the closest point in the point cloud, assuming the size of each cell is 1 unit. All hidden layers in our network has 128 channels. We use the same loss functions as UNDC for training the networks.\n\n\nMASKS DEFINITIONS\n\nIn this section, we provide definitions of M S , M V , and M F . We assume the size of each cell is 1 unit. \nM V -NDC.\n\nTRAINING DETAILS\n\nEach network is trained for 400 epochs (for SDF/voxel inputs) or 250 epochs (for point cloud inputs) with a batch size of 1 (shape). We use Adam optimizer [Kingma and Ba 2015] with a learning rate of 0.0001, beta1= 0.9 and beta2= 0.999 for optimization. The learning rate is halved every 100 epochs. For the last experiment \"Reconstruction from noisy real scans\", we use a large point cloud network by replacing the 3 3 3 conv3d layers in Figure 6 with 8 residual blocks [He et al. 2016]. We train the network on the same ABC training set but with heavy data augmentation (random scaling and translation, in addition to the augmentations mentioned in the paper). During training, we also augment the input point clouds with Gaussian noise ( = 0.5, assuming each cell of the output grid is a unit cube) to simulate the real noise from scan data.\n\n\nDETAILS OF THE METHODS IN EXPERIMENT \"RECONSTRUCTION FROM POINT CLOUDS\"\n\nBall-pivoting [Bernardini et al. 1999] and Screened Poisson surface reconstruction (Poisson) [Kazhdan and Hoppe 2013]. These are classic methods for reconstructing meshes from point clouds, and they require point normals as part of the input. Ball-pivoting does not create new vertices -it only connects the existing vertices into consistently oriented triangles. Poisson constructs an implicit field according to the points and normals, then extract the surface with an octree structure. In our experiments, we use the implementation in Open3D [Zhou et al. 2018] for these two methods, and use a maximum depth of 8 for the octrees in Poisson. SIREN [Sitzmann et al. 2020] is a method that overfits an neural implicit function to a given shape, therefore it takes much longer to process a shape than all other methods, since each time it needs to train a neural network from scratch. It requires point normals as part of the input. In our experiments, we use the official code released by the authors. We find that after training SIREN, there is a 10% possibility that the output shape is covered by a shell, which cannot be easily removed since it is close to the actual shape and connected to it in many pieces. Therefore, for those shapes we have to re-train the network for several times. Nonetheless, we report the inference time in the tables assuming all shapes are successfully trained in the first go.\n\nLocal implicit grid (LIG) [Jiang et al. 2020] is a method that first divides the input point cloud into small overlapping blocks, and then reconstruct the part in each block by optimizing a neural implicit field, and finally put the implicit fields together to reconstruct the entire shape. It requires point normals as part of the input.\n\nIn our experiments, we use the official code released by the authors. The authors have released pre-trained network weights on ShapeNet [Chang et al. 2015], therefore we denote this method with pre-trained weights as LIG (P). We also train this method on ABC training net for a fair comparison, denoted as LIG. We use 320 3 output resolution for both models.\n\nConvolutional occupancy networks (ConvONet) [Peng et al. 2020] is a method to reconstruct an implicit field from point clouds. It does not require point normals as input, therefore is the only method that takes the exact same input as our method. In our experiments, we use the official code released by the authors. The authors have introduced many network configurations in their paper, and we choose two representative ones in our experiments. In ConvONet 3plane, we use the 3-plane (xy, yz, xz) setting with the resolution of each plane 128 2 . In ConvONet grid, we use the 3D grid setting with the grid resolution 64 3 . We train both networks on ABC training net for a fair comparison. We also use the network weights released by the authors, pre-trained on synthetic scenes with objects from ShapeNet [Chang et al. 2015], denoted as ConvONet (P). We use 256 3 output resolution for all three models.\n\nIt is worth noting that unfortunately, all the networks in Con-vONet are non-local, that is, their receptive fields need to cover the entire shape in order to properly decide which side is inside and which side is outside in the output implicit field. We tried to directly apply our backbone network in ConvONet, denoted as Con-vONet*, but as expected, the training has failed. This is because our network is local, and ConvONet cannot decide inside/outside for a local patch, therefore generates many artifacts in the featureless regions, as shown in Figure 17. Note also that the pre-trained Con-vONet tends to turn single-face walls into thin volumetric plates in Figure 17.\n\n\nROBUSTNESS TO TRANSLATION AND ROTATION\n\nSee https://youtu.be/HwKMpeKgYcc. We test all methods on a shape while moving and rotating the shape inside the sampling grid.\n\n\nADDITIONAL QUALITATIVE RESULTS\n\nWe provide additional qualitative results via visualizations of reconstructing 3D meshes from SDF grid, binary occpuancy grid, and point cloud inputs in Figures 18, 19, 20, respectively. The input implicit fields are at 64 3 resolution, and the input point clouds have 4,096 points each. Since the FAUST human body meshes are all very similar to the one shown in the paper, we omit FAUST results.\n\n\nADDITIONAL QUANTITATIVE RESULTS\n\nWe provide additional tables showing quantitative results for all combinations of input types and datasets (a subset of these tables appear in the paper due to space limitations). For SDF and UDF grid inputs, results for the ABC test set are in Table 9, Thingi10K in Table 10, and FAUST in Table 11. For binary occpuancy grid inputs, the results for the ABC test set are in Table 12, Thingi10K in  Table 13, and FAUST in Table 14. For point cloud input, results for the ABC test set are in Table 15, Thingi10K in Table 16, and FAUST in Table 17. In these tables, UNDC @ 64 3 means that the output grid size of UNDC is 64 3 . 104:4 \u2022 Zhiqin Chen, Andrea Tagliasacchi, Thomas Funkhouser,and Hao Zhang Fig. 18. Results of reconstructing 3D meshes from SDF grid inputs at 64 3 resolution. The shapes in the first three columns are from ABC test set, and the last two columns from Thingi10K. Zoom in to see the surface artifacts in NMC-lite* and NMC*. Fig. 19. Results of reconstructing 3D meshes from binary voxel/occupancy inputs at 64 3 resolution. The shapes in the first three columns are from ABC test set, and the last two columns from Thingi10K. Zoom in to see the surface artifacts in NMC-lite* and NMC*. 104:6 \u2022 Zhiqin Chen,Andrea Tagliasacchi,Thomas Funkhouser,and Hao Zhang Fig. 20. Results of reconstructing 3D meshes from point clouds of 4,096 points. The shapes in the first three columns are from ABC test set, and the last two columns from Thingi10K.  \n\nFig. 2 .\n2Dual Contouring (DC) vs. Marching Cubes (MC) -visualized in 2D on different inputs that were sampled from the same underlying shape, DC (top) reconstructs a sharp feature (as an intersection between faces, in the top-right cell), while MC (bottom) does not.\n\nFig. 3 .Fig. 4 .\n34Neural dual contouring (NDC) Unsigned neural dual contouring (UNDC)\n\nFig. 5 .\n5Training data preparation with data augmentation -The ground truth meshes computed using classical DC (a) can be noisy. With proper augmentation for the training data (see bottom), our NDC network can be trained to output meshes with better tessellation quality (b).\n\nFig. 6 .Fig. 7 .\n67The architecture of our point cloud processing network for UNDC. Post-processing UNDC outputs -The post-processing step can close small holes by adding quad faces.\n\nFig. 9 .\n9Mesh reconstruction results from SDF grid inputs at 128 3 resolution on the FAUST dataset; see insets to compare triangle quality.\n\nFig. 10 .\n10Qualitative results of mesh reconstruction from UDF inputs at 128 3 resolution on two cloth shapes from the MGN dataset. Note the open surfaces reconstructed by our sign-agnostic method UNDC.\n\nFig. 11 .\n11Some plots of surface quality (via % of Inaccurate Normals) and triangle quality (via % of Small Angles), on ABC test set with 64 3 SDF input. NDC and UNDC consistently outperform other isosurfacing methods.\n\n\nFor a grid cell, if its corner vertices have different signs in the ground truth SDF, we set its corresponding entry in M V to 1. The other entries in M V are left 0. The definition applies for all kinds of inputs.M V -UNDC. For a grid cell, if any of its edges intersects the ground truth shape, we set its corresponding entry in M V to 1. The other entries in M V are left 0. The definition applies for all kinds of inputs. SDF grid input -M S -NDC. NDC directly use the signs of the input as S, therefore it does not need M S . SDF and UDF grid input -M F -UNDC. For an edge in a grid cell, if both of its end vertices have signed distances less than 1, we set its corresponding entry in M F to 1. The other entries in M F are left 0. Binary voxel input -M S -NDC. For an occupied grid cell, if it is adjacent to an unoccupied cell (in its 3 3 local neighborhood), we set the corresponding entries for all its 8 vertices to 1. The other entries in M S are left 0. Binary voxel input -M F -UNDC. For an edge in a grid cell, if all of its four adjacent cells are occupied, we set its corresponding entry in M F to 1. The other entries in M F are left 0. Point cloud input -M F -UNDC. In the point cloud networks, we only compute features for the cells that are close to the input point cloud, i.e., the cells that are within 3 units (manhattan distance) to the closest point in the point cloud. Therefore, the corresponding edges stored in those cells are set to 1 in M F . The other entries in M F are left 0.\n\nFig. 17 .\n17Pre-trained ConvONet vs. ConvONet with our local backbone.\n\nTable 1 .\n1Comparing various aspects of NMC vs. NDC.NMC \nNDC \n\nOutput \n5 (bool)+51 (float) per cube \n1 (bool)+3 (float) per cube \n\nNetwork \n3D ResNet \n6-layer 3D CNN \n\nTessellation \nManually designed, \n\u2264 1 vertex per cell; \u2264 1 quad \n37 unique cases per cube \nper edge; see Figure 3 \n\nOutput vertex count \n\u2248 8\u00d7 MC \n\u2248 MC \n\nOutput triangle count \n\u2248 8\u00d7 MC \n\u2248 MC \n\nData preparation \n\nSample dense point cloud \nSample only vertex \nin each cube; minimize \nsigns, intersection points \nchamfer distance via back \nand normals; then apply \npropagation; complex \nDual Contouring; Fast \nand time-consuming \nand easy to compute. \n\nImplementation \n\nNeed to consider all \nCould be a nice \ncube tessellation cases; \nundergraduate \ndifficult to implement \nassignment \n\nRegularization \n\nNeed a complex \nNo regularization \nregularization term \nterm needed \nfor voxel input \n\nTrainging time \n(On ABC training set) \n(Same setting) \n4 days per network \n< 12 hours per network \n\nInference speed \n(64 3 SDF input) \n(Same setting) \n> 1 second per shape \n30+ shapes per second \n\nInherent issues \nSelf-intersections, thin \nNon-manifold \ntriangles with small angles \nedges and vertices \n\n\n\nTable 2 .\n2Quantitative evaluation on ABC with SDF (signed or unsigned) inputs at two resolutions, evaluated on the test set split, using mesh quality metrics, output complexity, and inference times.64 3 \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T Inference \n\nSDF input \n(\u00d710 5 ) \n(\u00d710 2 ) \ntime \n\nNMC \n4.365 0.878 0.976 0.340 0.766 \n42,767 \n85,544 \n1.148s \nNMC-lite \n4.356 0.878 0.975 0.338 0.767 \n21,933 \n43,877 \n1.135s \n\nDC-est \n4.673 0.827 0.958 3.810 0.167 \n5,459 \n10,969 \n0.421s \nMC33 \n4.873 0.788 0.950 5.759 0.103 \n5,473 10,954 \n0.005s \nNMC* \n4.400 0.874 0.972 0.409 0.715 \n42,767 \n85,544 \n0.158s \nNMC-lite* \n4.386 0.875 0.973 0.416 0.725 \n21,933 \n43,877 \n0.153s \nNDC \n4.463 0.867 0.970 0.338 0.745 \n5,459 \n10,969 \n0.027s \nUNDC \n0.930 0.873 0.974 0.328 0.746 \n5,584 \n11,295 \n0.051s \n\nUNDC (UDF) 0.960 0.868 0.971 0.379 0.735 \n5,692 \n11,420 \n0.053s \n\n128 3 \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T Inference \n\nSDF input \n(\u00d710 5 ) \n(\u00d710 2 ) \ntime \n\nNMC \n4.129 0.882 0.979 0.204 0.806 175,926 351,867 \n8.991s \nNMC-lite \n4.117 0.882 0.979 0.231 0.808 \n88,419 176,853 \n8.984s \n\nDC-est \n4.132 0.879 0.977 2.215 0.266 \n22,088 \n44,213 \n1.765s \nMC33 \n4.144 0.870 0.972 4.247 0.193 22,048 44,107 \n0.030s \nNMC* \n4.116 0.882 0.978 0.257 0.779 175,926 351,867 \n1.126s \nNMC-lite* \n4.114 0.882 0.979 0.283 0.785 \n88,419 176,853 \n1.112s \nNDC \n4.131 0.881 0.978 0.214 0.802 \n22,088 \n44,213 \n0.207s \nUNDC \n0.789 0.890 0.983 0.149 0.813 \n22,578 \n45,411 \n0.410s \n\nUNDC (UDF) 0.792 0.889 0.983 0.227 0.810 \n22,874 \n45,715 \n0.409s \n\n\n\nTable 3 .\n3Quantitative results on Thingi10K with SDF input.128 3 \nCD\u2193 \nF1\u2191 ECD\u2193 EF1\u2191 \n#V \n#T \n% IN % SA \n\nSDF input \n(\u00d710 5 ) \n(\u00d710 2 ) \n> 5 \u2022 < 10 \u2022 \n\nMC33 \n2.421 0.890 2.657 0.197 \n22,324 \n44,656 19.08 \n2.43 \nNMC* \n2.613 0.902 0.269 0.760 169,211 338,427 20.99 \n0.77 \nNMC-lite* \n2.651 0.902 0.254 0.772 \n89,260 178,527 17.04 \n1.74 \nNDC \n2.300 0.901 0.215 0.792 22,295 44,631 12.52 0.24 \nUNDC \n0.757 0.904 0.189 0.795 \n22,478 \n45,043 12.66 \n0.29 \n\nUNDC (UDF) 0.748 0.903 0.222 0.785 \n22,784 \n45,395 13.19 \n0.28 \n\n\n\nTable 4 .\n4Quantitative results on FAUST with SDF input.128 3 \nCD\u2193 \nF1\u2191 ECD\u2193 EF1\u2191 \n#V \n#T \n% IN % SA \n\nSDF input \n(\u00d710 5 ) \n(\u00d710 2 ) \n> 5 \u2022 < 10 \u2022 \n\nMC33 \n0.453 0.985 0.086 0.387 12,551 25,076 34.28 \n4.23 \nNMC* \n0.385 0.990 0.146 0.552 83,024 166,038 44.58 \n1.18 \nNMC-lite* \n0.381 0.991 0.119 0.567 50,207 100,404 38.33 \n2.63 \nNDC \n0.397 0.989 0.044 0.530 12,538 \n25,100 38.38 0.11 \nUNDC \n0.362 0.992 0.038 0.574 12,609 \n25,258 37.38 \n0.16 \n\nUNDC (UDF) 0.365 0.991 0.045 0.549 12,682 \n25,293 38.72 \n0.21 \n\n\n\nTable 5 .\n5Quantitative results on ABC test set with binary voxel input.64 3 \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T Inference \n\nVoxel input \n(\u00d710 5 ) \n(\u00d710 2 ) \ntime \n\nMC33 \n26.862 0.085 0.921 11.342 0.018 \n5,826 11,656 \n0.005s \nNMC* \n9.452 0.422 0.927 \n0.698 0.346 42,045 84,089 \n0.156s \nNMC-lite* \n9.428 0.420 0.927 \n0.604 0.356 21,431 42,862 \n0.154s \nNDC \n9.387 0.428 0.930 \n0.567 0.360 5,345 10,726 \n0.055s \nUNDC \n9.139 0.428 0.931 0.564 0.359 \n5,365 10,772 \n0.055s \n\n\n\nTable 6 .\n6Quantitative results on ABC test set with point cloud input. (+n) indicates that the method additionally requires point normals as input.Fig. 13. Results of reconstructing 3D meshes from point cloud inputs of 4,096 points. Please zoom in to observe the surface details. The shapes in the first two columns are from ABC test set, and the last three columns from Thingi10K.point cloud \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T Inference \n(4,096) \n\n(\u00d710 5 ) \n(\u00d710 2 ) \ntime \n\nBall-pivoting (+n) \n\n3.080 0.791 0.944 \n0.556 0.269 \n4,096 \n7,439 \n1.292s \nPoisson (+n) \n4.705 0.727 0.939 \n4.138 0.067 \n11,241 \n22,496 \n1.476s \nSIREN (+n) \n1.340 0.814 0.969 \n2.636 0.152 \n97,219 194,543 168.595s \nLIG (+n) \n3.413 0.721 0.947 11.868 0.022 149,860 299,166 \n66.866s \nConvONet 3plane 18.073 0.536 0.935 \n4.113 0.105 \n75,342 150,689 \n2.692s \n\nConvONet grid \n\n8.844 0.488 0.939 \n9.701 0.036 \n74,171 148,337 \n2.404s \nUNDC \n0.893 0.873 0.974 0.289 0.757 \n5,578 \n11,261 \n0.194s \n\n\n\nTable 7 .\n7Comparing reconstruction results of UNDC (output grid size at 64 3 ) and Poisson on point cloud inputs from ABC test set, with varying point counts and noise levels to test the robustness of our method.Number of \nGaussian \nCD\u2193 (\u00d710 5 ) \nF1\u2191 \nNC\u2191 \ninput points noise levels Poisson UNDC Poisson UNDC Poisson UNDC \n\n1,024 \nNone \n34.872 \n2.510 \n0.248 \n0.806 \n0.799 \n0.944 \n2,048 \nNone \n16.406 \n1.226 \n0.387 \n0.850 \n0.847 \n0.962 \n4,096 \nNone \n8.653 \n0.987 \n0.539 \n0.867 \n0.879 \n0.970 \n4,096 \n= 0.2 \n9.814 \n1.179 \n0.480 \n0.840 \n0.863 \n0.962 \n4,096 \n= 0.5 \n14.017 \n2.061 \n0.331 \n0.717 \n0.821 \n0.935 \n16,384 \n= 0.2 \n5.286 \n0.936 \n0.636 \n0.866 \n0.889 \n0.971 \n16,384 \n= 0.5 \n8.738 \n1.236 \n0.444 \n0.813 \n0.840 \n0.955 \n65,536 \n= 0.2 \n2.299 \n0.905 \n0.741 \n0.872 \n0.930 \n0.973 \n65,536 \n= 0.5 \n4.567 \n1.079 \n0.552 \n0.836 \n0.880 \n0.962 \n\n\n\n\nFig. 16. The edge artifacts and the cause. The quad faces are colored differently to show that the artifacts are not caused by random quad splitting.Input \n64 3 SDF \n64 3 SDF \n64 3 SDF \n4, 096 points \n4, 096 points \nMethod \nNDC \nUNDC \nUNDC \nUNDC \nUNDC \nPost-processing \nNo \nNo \nYes \nNo \nYes \nNon-manifold-3 \n0.0 (0.000%) 125.3 (1.116%) 135.4 (1.206%) 139.4 (1.242%) 168.0 (1.496%) \nNon-manifold-4 20.1 (0.183%) \n31.2 (0.278%) \n31.7 (0.282%) \n28.8 (0.257%) \n30.4 (0.271%) \nBoundary-1 \n0.0 (0.000%) \n56.7 (0.505%) \n29.6 (0.264%) 116.4 (1.037%) \n41.1 (0.366%) \n\n\n\nTable 9 .\n9Quantitative comparison results on ABC test set with SDF and UDF grid input.Table 10. Quantitative comparison results on Thingi10K with SDF and UDF grid input. 80 \u2022> 30 \u2022 > 5 \u2022 > 80 \u2022 > 30 \u2022 > 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022 \u2022 > 80 \u2022 > 30 \u2022 > 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022Table 11. Quantitative comparison results on FAUST with SDF and UDF grid input. \u2022 > 80 \u2022 > 30 \u2022 > 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022Table 12. Quantitative comparison results on ABC test set with binary occpuancy grid input. 80 \u2022> 30 \u2022 > 5 \u2022 > 80 \u2022 > 30 \u2022 > 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022Table 13. Quantitative comparison results on Thingi10K with binary occpuancy grid input. \u2022 > 80 \u2022 > 30 \u2022 > 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022Table 14. Quantitative comparison results on FAUST with binary occpuancy grid input. 80 \u2022 > 30 \u2022 > 5 \u2022 > 80 \u2022 > 30 \u2022 > 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022Table 15. Quantitative results on ABC test set with point cloud input. (+n) indicates that the method additionally requires point normals as input.Table 16. Quantitative results on Thingi10K with point cloud input. (+n) indicates that the method additionally requires point normals as input. 104:10 \u2022 Zhiqin Chen, Andrea Tagliasacchi, Thomas Funkhouser, and Hao Zhang64 3 resolution \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T Inference % inaccurate normals (gt) % inaccurate normals (pred) \n% small angles \n\nSDF grid input \n(\u00d710 5 ) \n(\u00d710 2 ) \ntime \n> 80 \u2022 > 30 \u2022 \n> 5 \u2022 > 80 \u2022 > 30 \u2022 \n> 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022 \n\nNMC \n4.365 0.878 0.976 0.340 0.766 \n42,767 \n85,544 \n1.148s \n2.15 \n3.79 12.11 \n1.28 \n2.50 \n10.71 \n0.74 \n2.09 \n4.94 \nNMC-lite \n4.356 0.878 0.975 0.338 0.767 \n21,933 \n43,877 \n1.135s \n2.16 \n3.78 11.61 \n1.33 \n2.56 \n10.31 \n1.51 \n3.64 \n6.74 \n\nDC-est \n4.673 0.827 0.958 3.810 0.167 \n5,459 \n10,969 \n0.421s \n1.68 \n9.32 30.17 \n0.64 \n6.62 \n27.12 \n2.29 \n6.08 15.24 \nMC33 \n4.873 0.788 0.950 5.759 0.103 \n5,473 10,954 \n0.005s 1.05 13.48 31.50 0.40 \n9.72 \n27.03 \n1.95 \n4.09 \n6.53 \nNMC* \n4.400 0.874 0.972 0.409 0.715 \n42,767 \n85,544 \n0.158s \n2.01 \n4.39 22.03 \n1.17 \n2.98 \n20.68 \n0.57 \n1.81 \n4.63 \nNMC-lite* \n4.386 0.875 0.973 0.416 0.725 \n21,933 \n43,877 \n0.153s \n2.05 \n4.32 18.92 \n1.23 2.97 \n17.56 \n1.35 \n3.44 \n6.34 \nNDC \n4.463 0.867 0.970 0.338 0.745 \n5,459 \n10,969 \n0.027s \n2.45 \n4.66 16.20 \n1.48 \n3.52 15.03 0.33 0.76 4.16 \nUNDC \n0.930 0.873 0.974 0.328 0.746 \n5,584 \n11,295 \n0.051s \n1.65 3.71 15.75 \n1.52 \n3.69 \n15.61 \n0.44 \n0.93 \n4.41 \nUNDC (UDF) \n0.960 0.868 0.971 0.379 0.735 \n5,692 \n11,420 \n0.053s \n1.80 \n3.93 16.10 \n1.70 \n4.08 \n16.21 \n0.36 \n0.89 \n4.21 \n\n128 3 resolution \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T Inference % inaccurate normals (gt) % inaccurate normals (pred) \n% small angles \n\nSDF grid input \n(\u00d710 5 ) \n(\u00d710 2 ) \ntime \n> 80 \u2022 > 30 \u2022 \n> 5 \u2022 > 80 \u2022 > 30 \u2022 \n> 5 \u2022 < 10 \u2022 < 20 \u2022 < 30 \u2022 \n\nNMC \n4.129 0.882 0.979 0.204 0.806 175,926 351,867 \n8.991s \n2.10 \n2.98 \n8.25 \n1.26 \n1.82 \n6.97 \n0.72 \n1.84 \n4.17 \nNMC-lite \n4.117 0.882 0.979 0.231 0.808 \n88,419 176,853 \n8.984s \n2.12 \n2.97 \n7.76 \n1.28 \n1.84 \n6.53 \n1.46 \n3.35 \n5.99 \n\nDC-est \n4.132 0.879 0.977 2.215 0.266 \n22,088 \n44,213 \n1.765s \n1.40 \n5.10 17.11 \n0.34 \n2.85 \n14.54 \n1.62 \n4.36 13.10 \nMC33 \n4.144 0.870 0.972 4.247 0.193 22,048 44,107 \n0.030s 0.88 \n7.81 18.73 0.18 \n4.95 \n15.42 \n1.75 \n3.63 \n5.77 \nNMC* \n4.116 0.882 0.978 0.257 0.779 175,926 351,867 \n1.126s \n1.90 \n3.22 15.25 \n1.14 \n2.00 \n13.99 \n0.60 \n1.68 \n4.01 \nNMC-lite* \n4.114 0.882 0.979 0.283 0.785 \n88,419 176,853 \n1.112s \n1.91 \n3.15 12.60 \n1.18 1.97 \n11.35 \n1.37 \n3.26 \n5.77 \nNDC \n4.131 0.881 0.978 0.214 0.802 \n22,088 \n44,213 \n0.207s \n2.20 \n3.11 \n9.62 \n1.31 \n1.99 \n8.43 0.23 0.49 3.49 \nUNDC \n0.789 0.890 0.983 0.149 0.813 \n22,578 \n45,411 \n0.410s \n1.32 2.06 \n8.90 \n1.30 \n2.04 \n8.77 \n0.34 \n0.65 \n3.74 \nUNDC (UDF) \n0.792 0.889 0.983 0.227 0.810 \n22,874 \n45,715 \n0.409s \n1.36 \n2.11 \n8.93 \n1.31 \n2.09 \n8.88 0.23 \n0.55 \n3.51 \n\n64 3 resolution \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T % inaccurate normals (gt) % inaccurate normals (pred) \n% small angles \n\nSDF grid input \n(\u00d710 5 ) \n(\u00d710 2 ) \n> NMC \n2.434 0.895 0.974 0.284 0.735 \n40,952 \n81,911 \n1.58 \n3.95 17.57 \n1.34 \n3.40 \n16.92 \n0.96 \n2.78 \n6.59 \nNMC-lite \n2.485 0.895 0.974 0.308 0.738 \n22,051 \n44,109 \n1.57 \n3.94 16.51 \n1.38 \n3.48 \n15.97 \n1.89 \n4.77 \n9.12 \n\nMC33 \n3.192 0.795 0.945 3.918 0.099 \n5,518 \n11,044 0.76 14.30 37.41 0.55 11.14 \n33.54 \n2.63 \n5.45 \n8.70 \nNMC* \n2.777 0.890 0.969 0.391 0.662 \n40,952 \n81,911 \n1.47 \n4.92 31.21 \n1.21 \n4.23 \n30.53 \n0.75 \n2.37 \n6.12 \nNMC-lite* \n2.760 0.890 0.969 0.404 0.674 \n22,051 \n44,109 \n1.52 4.82 27.10 \n1.28 4.21 \n26.42 \n1.68 \n4.47 \n8.49 \nNDC \n2.481 0.877 0.966 0.390 0.695 \n5,473 11,027 \n1.88 \n5.39 23.59 \n1.57 \n5.04 23.14 0.35 1.12 5.87 \nUNDC \n0.899 0.878 0.967 0.369 0.693 \n5,529 \n11,175 \n1.62 \n5.00 23.52 \n1.57 \n5.17 \n23.56 \n0.41 \n1.24 \n6.08 \nUNDC (UDF) \n0.938 0.870 0.962 0.407 0.669 \n5,640 \n11,297 \n1.89 \n5.53 24.38 \n1.87 \n5.93 \n24.67 \n0.39 \n1.30 \n5.92 \n\n128 3 resolution \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T % inaccurate normals (gt) % inaccurate normals (pred) \n% small angles \n\nSDF grid input \n(\u00d710 5 ) \n(\u00d710 2 ) \n> 80 \u2022 > 30 \u2022 \n> 5 NMC \n2.340 0.902 0.980 0.170 0.805 169,210 338,426 \n1.48 \n2.65 10.90 \n1.29 \n2.28 \n10.46 \n0.92 \n2.50 \n5.76 \nNMC-lite \n2.398 0.902 0.980 0.163 0.810 \n89,260 178,527 \n1.48 \n2.63 \n9.98 \n1.32 \n2.30 \n9.59 \n1.83 \n4.46 \n8.26 \n\nMC33 \n2.421 0.890 0.972 2.657 0.197 \n22,324 \n44,656 0.48 \n7.47 21.65 0.27 \n5.46 \n19.08 \n2.43 \n5.04 \n7.92 \nNMC* \n2.613 0.902 0.978 0.269 0.760 169,211 338,427 \n1.34 \n3.00 21.42 \n1.15 \n2.57 \n20.99 \n0.77 \n2.25 \n5.50 \nNMC-lite* \n2.651 0.902 0.979 0.254 0.772 \n89,260 178,527 \n1.37 \n2.94 17.49 \n1.20 \n2.54 \n17.04 \n1.74 \n4.35 \n7.91 \nNDC \n2.300 0.901 0.979 0.215 0.792 22,295 44,631 \n1.53 \n2.81 12.88 \n1.36 \n2.50 12.52 0.24 0.75 5.07 \nUNDC \n0.757 0.904 0.981 0.189 0.795 \n22,478 \n45,043 \n1.31 2.50 12.71 \n1.29 2.48 \n12.66 \n0.29 \n0.85 \n5.28 \nUNDC (UDF) \n0.748 0.903 0.980 0.222 0.785 \n22,784 \n45,395 \n1.35 \n2.63 13.23 \n1.30 \n2.61 \n13.19 \n0.28 \n0.90 \n5.08 \n\n\nTable 17 .\n17Quantitative results on FAUST with point cloud input. (+n) indicates that the method additionally requires point normals as input.point cloud \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T \n(4,096) \n\n(\u00d710 5 ) \n(\u00d710 2 ) \n\nBall-pivoting (+n) \n\n0.906 0.932 0.965 0.372 0.131 \n4,096 \n7,652 \nPoisson (+n) \n0.724 0.966 0.975 0.467 0.246 11,330 \n22,646 \nSIREN (+n) \n0.697 0.951 0.986 0.211 0.504 51,215 102,443 \nLIG (P) (+n) \n1.449 0.876 0.964 1.307 0.107 79,337 158,605 \nLIG (+n) \n2.533 0.871 0.962 1.772 0.077 80,845 161,226 \n\nConvONet (P) \n\n17.334 0.312 0.849 1.244 0.027 47,716 \n95,424 \nConvONet 3plane 23.809 0.389 0.868 1.046 0.053 46,211 \n92,427 \n\nConvONet grid \n\n3.506 0.574 0.945 4.618 0.029 41,710 \n83,418 \nUNDC @ 64 3 \n0.532 0.970 0.965 0.345 0.206 \n3,146 \n6,308 \nUNDC @ 128 3 \n0.413 0.985 0.978 0.095 0.437 12,681 \n25,202 \n\npoint cloud \nCD\u2193 \nF1\u2191 \nNC\u2191 ECD\u2193 EF1\u2191 \n#V \n#T \n(16,384) \n\n(\u00d710 5 ) \n(\u00d710 2 ) \n\nBall-pivoting (+n) \n\n0.545 0.977 0.977 0.144 0.316 16,384 \n31,767 \nPoisson (+n) \n0.397 0.987 0.987 0.118 0.528 45,325 \n90,630 \nSIREN (+n) \n0.707 0.953 0.988 0.263 0.562 51,132 102,270 \nLIG (P) (+n) \n1.140 0.902 0.969 1.170 0.160 78,821 157,622 \nLIG (+n) \n2.215 0.895 0.966 1.792 0.120 80,399 160,445 \n\nConvONet (P) \n\n20.218 0.250 0.865 1.345 0.024 51,449 102,873 \nConvONet 3plane 24.682 0.390 0.869 0.985 0.048 47,922 \n95,851 \n\nConvONet grid \n\n3.563 0.568 0.947 5.474 0.029 42,218 \n84,433 \nUNDC @ 128 3 \n0.368 0.991 0.983 0.050 0.566 12,665 25,387 \nUNDC @ 256 3 \n0.353 0.993 0.989 0.020 0.767 51,043 101,733 \n\nNote that in the rest of the paper, we use the term NDC to refer to both our overall dual contouring based learning framework and the specific network that reconstructs meshes based on sign prediction(Figure 3). On the other hand, the term UNDC is used exclusively to denote the sign-agnostic version of our method(Figure 4).\nNote that these intersection points and normals were utilized to create the pseudoground truth at training time; they are not used at test time.\nNote that these two metrics are the surface normal counterparts of the two terms assembling the symmetric Chamfer Distance.\n\nA New Voronoi-Based Surface Reconstruction Algorithm. Nina Amenta, Marshall Bern, Manolis Kamvysselis, SIGGRAPH. Nina Amenta, Marshall Bern, and Manolis Kamvysselis. 1998. A New Voronoi-Based Surface Reconstruction Algorithm. In SIGGRAPH. 415-421.\n\nSAL: Sign Agnostic Learning of Shapes From Raw Data. Matan Atzmon, Yaron Lipman, CVPR. Matan Atzmon and Yaron Lipman. 2020. SAL: Sign Agnostic Learning of Shapes From Raw Data. In CVPR. 2562-2571.\n\nMeshlet priors for 3D mesh reconstruction. Abhishek Badki, Orazio Gallo, CVPR. Abhishek Badki, Orazio Gallo, Jan Kautz, and Pradeep Sen. 2020. Meshlet priors for 3D mesh reconstruction. In CVPR. 2849-2858.\n\nA Survey of Surface Reconstruction from Point Clouds. Matthew Berger, Andrea Tagliasacchi, Lee M Seversky, Pierre Alliez, Ga\u00ebl Guennebaud, Joshua A Levine, Andrei Sharf, Claudio T Silva, Computer Graphics Forum. 36Matthew Berger, Andrea Tagliasacchi, Lee M. Seversky, Pierre Alliez, Ga\u00ebl Guennebaud, Joshua A. Levine, Andrei Sharf, and Claudio T Silva. 2017. A Survey of Surface Reconstruction from Point Clouds. In Computer Graphics Forum, Vol. 36. 301-329.\n\nThe ball-pivoting algorithm for surface reconstruction. Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Cl\u00e1udio Silva, Gabriel Taubin, IEEE TVCG. 5Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Cl\u00e1udio Silva, and Gabriel Taubin. 1999. The ball-pivoting algorithm for surface reconstruction. IEEE TVCG 5, 4 (1999), 349-359.\n\nMulti-Garment Net: Learning to Dress 3D People from Images. Garvita Bharat Lal Bhatnagar, Christian Tiwari, Gerard Theobalt, Pons-Moll, ICCV. 5420-5430Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. 2019. Multi-Garment Net: Learning to Dress 3D People from Images. In ICCV. 5420-5430.\n\nFAUST: Dataset and evaluation for 3D mesh registration. Federica Bogo, Javier Romero, Matthew Loper, Michael J Black, CVPR. Federica Bogo, Javier Romero, Matthew Loper, and Michael J. Black. 2014. FAUST: Dataset and evaluation for 3D mesh registration. In CVPR. 3794-3801.\n\nFast and Robust Normal Estimation for Point Clouds with Sharp Features. Alexandre Boulch, Renaud Marlet, Computer Graphics Forum. 31Alexandre Boulch and Renaud Marlet. 2012. Fast and Robust Normal Estimation for Point Clouds with Sharp Features. Computer Graphics Forum 31, 5 (2012), 1765-1774.\n\nMatterport3D: Learning from RGB-D Data in Indoor Environments. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, 3Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017. Matterport3D: Learning from RGB-D Data in Indoor Environments. In 3DV. 667-676.\n\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu, arXiv:1512.03012ShapeNet: An Information-Rich 3D Model Repository. arXiv preprintAngel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. 2015. ShapeNet: An Information-Rich 3D Model Repository. arXiv preprint arXiv:1512.03012 (2015).\n\nLearning Implicit Fields for Generative Shape Modeling. Zhiqin Chen, Hao Zhang, CVPR. Zhiqin Chen and Hao Zhang. 2019. Learning Implicit Fields for Generative Shape Modeling. In CVPR. 5932-5941.\n\nNeural Marching Cubes. Zhiqin Chen, Hao Zhang, ACM Transactions on Graphics. 40Zhiqin Chen and Hao Zhang. 2021. Neural Marching Cubes. ACM Transactions on Graphics 40, 6 (2021), 1-15.\n\nMarching Cubes 33: construction of topologically correct isosurfaces. Evgeni Chernyaev, CN/95-17. CERNTechnical ReportEvgeni Chernyaev. 1995. Marching Cubes 33: construction of topologically correct isosur- faces. Technical Report CN/95-17. CERN.\n\nNeural unsigned distance fields for implicit function learning. Julian Chibane, Gerard Pons-Moll, Advances in Neural Information Processing Systems. 33Julian Chibane, Gerard Pons-Moll, et al. 2020. Neural unsigned distance fields for implicit function learning. Advances in Neural Information Processing Systems 33 (2020), 21638-21652.\n\nA greedy Delaunay-based surface reconstruction algorithm. David Cohen, - Steiner, Tran Kai Frank Da, The Visual Computer. 20David Cohen-Steiner and Tran Kai Frank Da. 2004. A greedy Delaunay-based surface reconstruction algorithm. The Visual Computer 20, 1 (2004), 4-16.\n\nA volumetric method for building complex models from range images. Brian Curless, Marc Levoy, SIGGRAPH. Brian Curless and Marc Levoy. 1996. A volumetric method for building complex models from range images. In SIGGRAPH. 303-312.\n\nA Survey on Implicit Surface Polygonization. Daniel S Bruno Rodrigues De Ara\u00fajo, Pauline Lopes, Joaquim A Jepp, Brian Jorge, Wyvill, ACM Computing Surveys (CSUR). 47Bruno Rodrigues De Ara\u00fajo, Daniel S. Lopes, Pauline Jepp, Joaquim A. Jorge, and Brian Wyvill. 2015. A Survey on Implicit Surface Polygonization. ACM Computing Surveys (CSUR) 47, 4 (2015), 1-39.\n\nModeling by example. Thomas Funkhouser, Michael Kazhdan, Philip Shilane, Patrick Min, William Kiefer, Ayellet Tal, Szymon Rusinkiewicz, David Dobkin, ACM Transactions on Graphics. 23Thomas Funkhouser, Michael Kazhdan, Philip Shilane, Patrick Min, William Kiefer, Ayellet Tal, Szymon Rusinkiewicz, and David Dobkin. 2004. Modeling by example. ACM Transactions on Graphics 23, 3 (2004), 652-663.\n\nLearning deformable tetrahedral meshes for 3d reconstruction. Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan Mcguire, Sanja Fidler, In NeurIPS. 33Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2020. Learning deformable tetrahedral meshes for 3d reconstruction. In NeurIPS, Vol. 33. 9936-9947.\n\nSurface simplification using quadric error metrics. Michael Garland, Paul S Heckbert, SIGGRAPH. Michael Garland and Paul S. Heckbert. 1997. Surface simplification using quadric error metrics. In SIGGRAPH. 209-216.\n\nA papier-m\u00e2ch\u00e9 approach to learning 3D surface generation. Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, Mathieu Aubry, CVPR. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu Aubry. 2018. A papier-m\u00e2ch\u00e9 approach to learning 3D surface generation. In CVPR. 216-224.\n\nPoint2Mesh: A Self-Prior for Deformable Meshes. Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or, ACM Transactions on Graphics. 394Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. 2020. Point2Mesh: A Self-Prior for Deformable Meshes. ACM Transactions on Graphics 39, 4 (2020).\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. 770-778.\n\nLocal implicit grid representations for 3d scenes. Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, Thomas Funkhouser, CVPR. Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, and Thomas Funkhouser. 2020. Local implicit grid representations for 3d scenes. In CVPR. 6001-6010.\n\nDual Contouring of Hermite Data. Tao Ju, Frank Losasso, Scott Schaefer, Joe Warren, ACM Transactions on graphics. 21Tao Ju, Frank Losasso, Scott Schaefer, and Joe Warren. 2002. Dual Contouring of Hermite Data. ACM Transactions on graphics 21, 3 (2002), 339-346.\n\nScreened Poisson surface reconstruction. Michael Kazhdan, Hugues Hoppe, ACM Transactions on Graphics. 32Michael Kazhdan and Hugues Hoppe. 2013. Screened Poisson surface reconstruction. ACM Transactions on Graphics 32, 3 (2013), 1-13.\n\nAdam: a method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P. Kingma and Jimmy Ba. 2015. Adam: a method for stochastic optimization. In ICLR.\n\nABC: a big CAD model dataset for geometric deep learning. Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo, Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. 2019. ABC: a big CAD model dataset for geometric deep learning. In CVPR. 9601-9611.\n\nNoise2Noise: Learning Image Restoration without Clean Data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. 2018. Noise2Noise: Learning Image Restoration without Clean Data. In ICML. 2965-2974.\n\nEfficient implementation of marching cubes' cases with topological guarantees. Thomas Lewiner, H\u00e9lio Lopes, Ant\u00f4nio Wilson Vieira, Geovan Tavares, Journal of Graphics Tools. 8Thomas Lewiner, H\u00e9lio Lopes, Ant\u00f4nio Wilson Vieira, and Geovan Tavares. 2003. Efficient implementation of marching cubes' cases with topological guarantees. Journal of Graphics Tools 8, 2 (2003), 1-15.\n\nPolygonizing extremal surfaces with manifold guarantees. Ruosi Li, Lu Liu, Ly Phan, Sasakthi Abeysinghe, Cindy Grimm, Tao Ju, Proceedings of ACM Symposium on Solid and Physical Modeling. ACM Symposium on Solid and Physical ModelingRuosi Li, Lu Liu, Ly Phan, Sasakthi Abeysinghe, Cindy Grimm, and Tao Ju. 2010. Polygonizing extremal surfaces with manifold guarantees. In Proceedings of ACM Symposium on Solid and Physical Modeling. 189-194.\n\nDeep marching cubes: Learning explicit surface representations. Yiyi Liao, Simon Donne, Andreas Geiger, CVPR. Yiyi Liao, Simon Donne, and Andreas Geiger. 2018. Deep marching cubes: Learning explicit surface representations. In CVPR. 2916-2925.\n\nMeshing point clouds with predicted intrinsic-extrinsic ratio guidance. Minghua Liu, Xiaoshuai Zhang, Hao Su, ECCV. Minghua Liu, Xiaoshuai Zhang, and Hao Su. 2020. Meshing point clouds with predicted intrinsic-extrinsic ratio guidance. In ECCV. 68-84.\n\nMarching Cubes: A High Resolution 3D Surface Construction Algorithm. William E Lorensen, Harvey E Cline, SIGGRAPH. William E. Lorensen and Harvey E. Cline. 1987. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In SIGGRAPH. 163-169.\n\nOccupancy Networks: Learning 3D Reconstruction in Function Space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, CVPR. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. 2019. Occupancy Networks: Learning 3D Reconstruction in Function Space. In CVPR. 4455-4465.\n\nSSRNet: Scalable 3D Surface Reconstruction Network. Zhenxing Mi, Yiming Luo, Wenbing Tao, CVPR. Zhenxing Mi, Yiming Luo, and Wenbing Tao. 2020. SSRNet: Scalable 3D Surface Reconstruction Network. In CVPR. 970-979.\n\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra- mamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. 405-421.\n\nDual Marching Cubes. Gregory M Nielson, IEEE Visualization. Gregory M. Nielson. 2004. Dual Marching Cubes. In IEEE Visualization. 489-496.\n\nDeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, CVPR. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love- grove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. In CVPR. 165-174.\n\nExample-based 3D scan completion. Mark Pauly, J Niloy, Joachim Mitra, Markus H Giesen, Leonidas J Gross, Guibas, Symp. on Geometry Processing. Mark Pauly, Niloy J. Mitra, Joachim Giesen, Markus H. Gross, and Leonidas J. Guibas. 2005. Example-based 3D scan completion. In Symp. on Geometry Processing. 23-32.\n\nShape as points: A differentiable Poisson solver. Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger, In NeurIPS. 34Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. 2021. Shape as points: A differentiable Poisson solver. In NeurIPS, Vol. 34.\n\nConvolutional occupancy networks. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, ECCV. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. 2020. Convolutional occupancy networks. In ECCV. 523-540.\n\nPointNet: Deep learning on point sets for 3D classification and segmentation. Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas, CVPR. Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017a. PointNet: Deep learning on point sets for 3D classification and segmentation. In CVPR. 652-660.\n\nPointNet++: Deep hierarchical feature learning on point sets in a metric space. Charles R Qi, Li Yi, Hao Su, Leonidas J Guibas, In NeurIPS. 30Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017b. PointNet++: Deep hierarchi- cal feature learning on point sets in a metric space. In NeurIPS, Vol. 30. 5105-5114.\n\nLearning Delaunay Surface Elements for Mesh Reconstruction. Marie-Julie Rakotosaona, Paul Guerrero, Noam Aigerman, Niloy Mitra, Maks Ovsjanikov, CVPR. Marie-Julie Rakotosaona, Paul Guerrero, Noam Aigerman, Niloy Mitra, and Maks Ovsjanikov. 2021. Learning Delaunay Surface Elements for Mesh Reconstruction. In CVPR. 22-31.\n\nMeshSDF: differentiable iso-surface extraction. Edoardo Remelli, Artem Lukoianov, Stephan R Richter, Beno\u00eet Guillard, Timur Bagautdinov, Pierre Baque, Pascal Fua, In NeurIPS. 33Edoardo Remelli, Artem Lukoianov, Stephan R. Richter, Beno\u00eet Guillard, Timur Bagaut- dinov, Pierre Baque, and Pascal Fua. 2020. MeshSDF: differentiable iso-surface extraction. In NeurIPS, Vol. 33. 22468-22478.\n\nManifold dual contouring. Scott Schaefer, Tao Ju, Joe Warren, IEEE TVCG. 13Scott Schaefer, Tao Ju, and Joe Warren. 2007. Manifold dual contouring. IEEE TVCG 13, 3 (2007), 610-619.\n\nCompletion and Reconstruction with Primitive Shapes. Ruwen Schnabel, Patrick Degener, Reinhard Klein, Computer Graphics Forum. 28Ruwen Schnabel, Patrick Degener, and Reinhard Klein. 2009. Completion and Recon- struction with Primitive Shapes. Computer Graphics Forum 28 (2009), 503-512.\n\nPointTriNet: Learned Triangulation of 3D Point Sets. Nicholas Sharp, Maks Ovsjanikov, ECCV. Nicholas Sharp and Maks Ovsjanikov. 2020. PointTriNet: Learned Triangulation of 3D Point Sets. In ECCV. 762-778.\n\nStructure recovery by part assembly. Chao-Hui Shen, Hongbo Fu, Kang Chen, Shi-Min Hu, ACM Transactions on Graphics. 31Chao-Hui Shen, Hongbo Fu, Kang Chen, and Shi-Min Hu. 2012. Structure recovery by part assembly. ACM Transactions on Graphics 31, 6 (2012), 1-11.\n\nImplicit neural representations with periodic activation functions. Vincent Sitzmann, N P Julien, Alexander W Martel, David B Bergman, Gordon Lindell, Wetzstein, In NeurIPS. 33Vincent Sitzmann, Julien NP Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. Implicit neural representations with periodic activation functions. In NeurIPS, Vol. 33. 7462-7473.\n\nSA-ConvONet: Sign-Agnostic Optimization of Convolutional Occupancy Networks. Jiapeng Tang, Jiabao Lei, Dan Xu, Feiying Ma, Kui Jia, Lei Zhang, Jiapeng Tang, Jiabao Lei, Dan Xu, Feiying Ma, Kui Jia, and Lei Zhang. 2021. SA- ConvONet: Sign-Agnostic Optimization of Convolutional Occupancy Networks. In ICCV. 6484-6493.\n\nDeep geometric prior for surface reconstruction. Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, Daniele Panozzo, CVPR. Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, and Daniele Panozzo. 2019. Deep geometric prior for surface reconstruction. In CVPR. 10130- 10139.\n\nData Structure for Soft Objects. Geoff Wyvill, Craig Mcpheeters, Brian Wyvill, The Visual Computer. 2Geoff Wyvill, Craig McPheeters, and Brian Wyvill. 1986. Data Structure for Soft Objects. The Visual Computer 2 (1986), 227-234.\n\nQingnan Zhou, Alec Jacobson, arXiv:1605.04797Thingi10K: a dataset of 10,000 3D-printing models. arXiv preprintQingnan Zhou and Alec Jacobson. 2016. Thingi10K: a dataset of 10,000 3D-printing models. arXiv preprint arXiv:1605.04797 (2016).\n\nQian-Yi Zhou, Jaesik Park, Vladlen Koltun, arXiv:1801.09847Open3D: A Modern Library for 3D Data Processing. arXiv preprintQian-Yi Zhou, Jaesik Park, and Vladlen Koltun. 2018. Open3D: A Modern Library for 3D Data Processing. arXiv preprint arXiv:1801.09847 (2018).\n", "annotations": {"author": "[{\"end\":38,\"start\":26},{\"end\":85,\"start\":39},{\"end\":131,\"start\":86},{\"end\":155,\"start\":132},{\"end\":166,\"start\":156},{\"end\":199,\"start\":167},{\"end\":268,\"start\":200},{\"end\":301,\"start\":269},{\"end\":388,\"start\":302},{\"end\":437,\"start\":389},{\"end\":470,\"start\":438}]", "publisher": null, "author_last_name": "[{\"end\":37,\"start\":33},{\"end\":56,\"start\":46},{\"end\":104,\"start\":96},{\"end\":141,\"start\":136},{\"end\":165,\"start\":160}]", "author_first_name": "[{\"end\":32,\"start\":26},{\"end\":45,\"start\":39},{\"end\":95,\"start\":89},{\"end\":135,\"start\":132},{\"end\":159,\"start\":156}]", "author_affiliation": "[{\"end\":198,\"start\":168},{\"end\":267,\"start\":201},{\"end\":300,\"start\":270},{\"end\":387,\"start\":303},{\"end\":436,\"start\":390},{\"end\":469,\"start\":439}]", "title": "[{\"end\":23,\"start\":1},{\"end\":493,\"start\":471}]", "venue": null, "abstract": "[{\"end\":2389,\"start\":699}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2594,\"start\":2574},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2616,\"start\":2594},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3076,\"start\":3059},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3100,\"start\":3076},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3126,\"start\":3100},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3194,\"start\":3173},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3395,\"start\":3370},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3445,\"start\":3429},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4111,\"start\":4095},{\"end\":4551,\"start\":4523},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5237,\"start\":5221},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6209,\"start\":6191},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6324,\"start\":6300},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6385,\"start\":6368},{\"end\":6449,\"start\":6423},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6529,\"start\":6511},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7183,\"start\":7159},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7241,\"start\":7217},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7321,\"start\":7299},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7363,\"start\":7344},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7419,\"start\":7402},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7894,\"start\":7867},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8843,\"start\":8823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8865,\"start\":8843},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9442,\"start\":9417},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9466,\"start\":9447},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9555,\"start\":9540},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9866,\"start\":9845},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10112,\"start\":10096},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10665,\"start\":10647},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10696,\"start\":10675},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10758,\"start\":10741},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11749,\"start\":11729},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11860,\"start\":11836},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11897,\"start\":11873},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11933,\"start\":11914},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11968,\"start\":11947},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12131,\"start\":12107},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12149,\"start\":12131},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12166,\"start\":12149},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12245,\"start\":12224},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12265,\"start\":12245},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12287,\"start\":12265},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12304,\"start\":12287},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12321,\"start\":12304},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12356,\"start\":12337},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12376,\"start\":12356},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12396,\"start\":12376},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12414,\"start\":12396},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12429,\"start\":12414},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12446,\"start\":12429},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12467,\"start\":12446},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12500,\"start\":12479},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12715,\"start\":12691},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12732,\"start\":12715},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12823,\"start\":12806},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12847,\"start\":12823},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12873,\"start\":12847},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12998,\"start\":12974},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13024,\"start\":12998},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13123,\"start\":13099},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13271,\"start\":13247},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13919,\"start\":13900},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13936,\"start\":13919},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13956,\"start\":13936},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14099,\"start\":14083},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15122,\"start\":15097},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15902,\"start\":15875},{\"end\":17158,\"start\":17145},{\"end\":17160,\"start\":17158},{\"end\":17162,\"start\":17160},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21064,\"start\":21048},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21870,\"start\":21848},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24729,\"start\":24711},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24782,\"start\":24761},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25463,\"start\":25442},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25558,\"start\":25534},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25656,\"start\":25639},{\"end\":25732,\"start\":25706},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25830,\"start\":25812},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26750,\"start\":26729},{\"end\":28787,\"start\":28781},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29465,\"start\":29449},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29484,\"start\":29465},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29616,\"start\":29595},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29786,\"start\":29770},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36907,\"start\":36885},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38722,\"start\":38699},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":38866,\"start\":38845},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38915,\"start\":38896},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":38982,\"start\":38965},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41169,\"start\":41151},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":41834,\"start\":41818},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":42323,\"start\":42304},{\"end\":44964,\"start\":44960},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":48757,\"start\":48743},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":48782,\"start\":48761},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":50603,\"start\":50580},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":51934,\"start\":51918},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":52767,\"start\":52751},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":54114,\"start\":54098},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":54585,\"start\":54561},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":54664,\"start\":54640},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":55109,\"start\":55092},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":55218,\"start\":55197},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56003,\"start\":55985},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56454,\"start\":56435},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":56720,\"start\":56703},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57486,\"start\":57467},{\"end\":59565,\"start\":59547},{\"end\":59586,\"start\":59565},{\"end\":60109,\"start\":60089},{\"end\":60129,\"start\":60109},{\"end\":60147,\"start\":60129},{\"end\":60168,\"start\":60147}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":60613,\"start\":60345},{\"attributes\":{\"id\":\"fig_1\"},\"end\":60701,\"start\":60614},{\"attributes\":{\"id\":\"fig_2\"},\"end\":60979,\"start\":60702},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61163,\"start\":60980},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61305,\"start\":61164},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61510,\"start\":61306},{\"attributes\":{\"id\":\"fig_6\"},\"end\":61731,\"start\":61511},{\"attributes\":{\"id\":\"fig_7\"},\"end\":63245,\"start\":61732},{\"attributes\":{\"id\":\"fig_8\"},\"end\":63317,\"start\":63246},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":64478,\"start\":63318},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":65980,\"start\":64479},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66497,\"start\":65981},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":67005,\"start\":66498},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":67473,\"start\":67006},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":68438,\"start\":67474},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69274,\"start\":68439},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":69836,\"start\":69275},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":75683,\"start\":69837},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":77200,\"start\":75684}]", "paragraph": "[{\"end\":3955,\"start\":2405},{\"end\":4552,\"start\":3957},{\"end\":5087,\"start\":4554},{\"end\":5683,\"start\":5089},{\"end\":6141,\"start\":5685},{\"end\":7627,\"start\":6143},{\"end\":8702,\"start\":7629},{\"end\":9250,\"start\":8719},{\"end\":10587,\"start\":9301},{\"end\":11576,\"start\":10589},{\"end\":12582,\"start\":11618},{\"end\":14678,\"start\":12584},{\"end\":14904,\"start\":14703},{\"end\":15436,\"start\":15060},{\"end\":15799,\"start\":15503},{\"end\":15977,\"start\":15831},{\"end\":16113,\"start\":16030},{\"end\":16778,\"start\":16115},{\"end\":17303,\"start\":16789},{\"end\":17472,\"start\":17305},{\"end\":18058,\"start\":17552},{\"end\":18235,\"start\":18060},{\"end\":19287,\"start\":18281},{\"end\":19466,\"start\":19300},{\"end\":20367,\"start\":19468},{\"end\":20824,\"start\":20369},{\"end\":21344,\"start\":20826},{\"end\":21755,\"start\":21374},{\"end\":22468,\"start\":21757},{\"end\":23304,\"start\":22488},{\"end\":23494,\"start\":23379},{\"end\":23663,\"start\":23561},{\"end\":24322,\"start\":23683},{\"end\":24596,\"start\":24349},{\"end\":25283,\"start\":24643},{\"end\":26008,\"start\":25285},{\"end\":26319,\"start\":26020},{\"end\":26685,\"start\":26321},{\"end\":27040,\"start\":26687},{\"end\":28378,\"start\":27042},{\"end\":28559,\"start\":28380},{\"end\":29437,\"start\":28587},{\"end\":29617,\"start\":29439},{\"end\":30471,\"start\":29619},{\"end\":30800,\"start\":30473},{\"end\":31116,\"start\":30802},{\"end\":32559,\"start\":31118},{\"end\":33385,\"start\":32591},{\"end\":33639,\"start\":33387},{\"end\":33888,\"start\":33641},{\"end\":34780,\"start\":33890},{\"end\":35076,\"start\":34782},{\"end\":35457,\"start\":35078},{\"end\":36368,\"start\":35459},{\"end\":37130,\"start\":36396},{\"end\":38224,\"start\":37168},{\"end\":38619,\"start\":38261},{\"end\":39856,\"start\":38621},{\"end\":41037,\"start\":39858},{\"end\":42059,\"start\":41078},{\"end\":42864,\"start\":42061},{\"end\":43416,\"start\":42866},{\"end\":44201,\"start\":43418},{\"end\":44702,\"start\":44203},{\"end\":45951,\"start\":44704},{\"end\":46835,\"start\":45953},{\"end\":47758,\"start\":46851},{\"end\":49381,\"start\":47760},{\"end\":50327,\"start\":49383},{\"end\":50839,\"start\":50329},{\"end\":51227,\"start\":50859},{\"end\":51543,\"start\":51254},{\"end\":53467,\"start\":51588},{\"end\":53597,\"start\":53489},{\"end\":54471,\"start\":53627},{\"end\":55957,\"start\":54547},{\"end\":56297,\"start\":55959},{\"end\":56657,\"start\":56299},{\"end\":57565,\"start\":56659},{\"end\":58244,\"start\":57567},{\"end\":58413,\"start\":58287},{\"end\":58844,\"start\":58448},{\"end\":60344,\"start\":58880}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15059,\"start\":14905},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15464,\"start\":15437},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15502,\"start\":15464},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15830,\"start\":15800},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16029,\"start\":15978},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17551,\"start\":17473},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18280,\"start\":18236},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23378,\"start\":23305},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23560,\"start\":23495},{\"attributes\":{\"id\":\"formula_9\"},\"end\":53607,\"start\":53598}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8579,\"start\":8572},{\"end\":23927,\"start\":23920},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30632,\"start\":30625},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31115,\"start\":31108},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32273,\"start\":32262},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32436,\"start\":32429},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33692,\"start\":33685},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34013,\"start\":34006},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35219,\"start\":35212},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35587,\"start\":35580},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35876,\"start\":35869},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36050,\"start\":36043},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":36281,\"start\":36262},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36432,\"start\":36425},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37385,\"start\":37378},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37619,\"start\":37612},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39731,\"start\":39724},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39887,\"start\":39880},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":40888,\"start\":40881},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":45999,\"start\":45992},{\"end\":48034,\"start\":48027},{\"end\":48615,\"start\":48608},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":59132,\"start\":59125},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":59155,\"start\":59147},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":59178,\"start\":59170},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":59286,\"start\":59254},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":59309,\"start\":59301},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":59401,\"start\":59370},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":59424,\"start\":59416}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2403,\"start\":2391},{\"attributes\":{\"n\":\"2\"},\"end\":8717,\"start\":8705},{\"attributes\":{\"n\":\"2.1\"},\"end\":9299,\"start\":9253},{\"attributes\":{\"n\":\"2.2\"},\"end\":11616,\"start\":11579},{\"attributes\":{\"n\":\"2.3\"},\"end\":14701,\"start\":14681},{\"attributes\":{\"n\":\"3\"},\"end\":16787,\"start\":16781},{\"attributes\":{\"n\":\"3.1\"},\"end\":19298,\"start\":19290},{\"attributes\":{\"n\":\"3.2\"},\"end\":21372,\"start\":21347},{\"attributes\":{\"n\":\"3.3\"},\"end\":22486,\"start\":22471},{\"attributes\":{\"n\":\"3.4\"},\"end\":23681,\"start\":23666},{\"attributes\":{\"n\":\"4\"},\"end\":24347,\"start\":24325},{\"attributes\":{\"n\":\"4.1\"},\"end\":24641,\"start\":24599},{\"attributes\":{\"n\":\"4.2\"},\"end\":26018,\"start\":26011},{\"attributes\":{\"n\":\"4.3\"},\"end\":28585,\"start\":28562},{\"end\":32589,\"start\":32562},{\"attributes\":{\"n\":\"4.4\"},\"end\":36394,\"start\":36371},{\"attributes\":{\"n\":\"4.5\"},\"end\":37166,\"start\":37133},{\"attributes\":{\"n\":\"4.6\"},\"end\":38259,\"start\":38227},{\"attributes\":{\"n\":\"4.7\"},\"end\":41076,\"start\":41040},{\"attributes\":{\"n\":\"5\"},\"end\":46849,\"start\":46838},{\"end\":50857,\"start\":50842},{\"end\":51252,\"start\":51230},{\"attributes\":{\"n\":\"1\"},\"end\":51586,\"start\":51546},{\"attributes\":{\"n\":\"2\"},\"end\":53487,\"start\":53470},{\"attributes\":{\"n\":\"3\"},\"end\":53625,\"start\":53609},{\"attributes\":{\"n\":\"4\"},\"end\":54545,\"start\":54474},{\"attributes\":{\"n\":\"5\"},\"end\":58285,\"start\":58247},{\"attributes\":{\"n\":\"6\"},\"end\":58446,\"start\":58416},{\"attributes\":{\"n\":\"7\"},\"end\":58878,\"start\":58847},{\"end\":60354,\"start\":60346},{\"end\":60631,\"start\":60615},{\"end\":60711,\"start\":60703},{\"end\":60997,\"start\":60981},{\"end\":61173,\"start\":61165},{\"end\":61316,\"start\":61307},{\"end\":61521,\"start\":61512},{\"end\":63256,\"start\":63247},{\"end\":63328,\"start\":63319},{\"end\":64489,\"start\":64480},{\"end\":65991,\"start\":65982},{\"end\":66508,\"start\":66499},{\"end\":67016,\"start\":67007},{\"end\":67484,\"start\":67475},{\"end\":68449,\"start\":68440},{\"end\":69847,\"start\":69838},{\"end\":75695,\"start\":75685}]", "table": "[{\"end\":64478,\"start\":63371},{\"end\":65980,\"start\":64679},{\"end\":66497,\"start\":66042},{\"end\":67005,\"start\":66555},{\"end\":67473,\"start\":67079},{\"end\":68438,\"start\":67857},{\"end\":69274,\"start\":68653},{\"end\":69836,\"start\":69426},{\"end\":75683,\"start\":71044},{\"end\":77200,\"start\":75828}]", "figure_caption": "[{\"end\":60613,\"start\":60356},{\"end\":60701,\"start\":60634},{\"end\":60979,\"start\":60713},{\"end\":61163,\"start\":61000},{\"end\":61305,\"start\":61175},{\"end\":61510,\"start\":61319},{\"end\":61731,\"start\":61524},{\"end\":63245,\"start\":61734},{\"end\":63317,\"start\":63259},{\"end\":63371,\"start\":63330},{\"end\":64679,\"start\":64491},{\"end\":66042,\"start\":65993},{\"end\":66555,\"start\":66510},{\"end\":67079,\"start\":67018},{\"end\":67857,\"start\":67486},{\"end\":68653,\"start\":68451},{\"end\":69426,\"start\":69277},{\"end\":71044,\"start\":69849},{\"end\":75828,\"start\":75698}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4302,\"start\":4294},{\"end\":4896,\"start\":4888},{\"end\":5412,\"start\":5403},{\"end\":8209,\"start\":8201},{\"end\":8701,\"start\":8693},{\"end\":10469,\"start\":10461},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14221,\"start\":14213},{\"end\":17259,\"start\":17251},{\"end\":17272,\"start\":17264},{\"end\":18980,\"start\":18972},{\"end\":21282,\"start\":21274},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21747,\"start\":21739},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22067,\"start\":22059},{\"end\":24204,\"start\":24196},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29811,\"start\":29803},{\"end\":30507,\"start\":30499},{\"end\":31103,\"start\":31095},{\"end\":31702,\"start\":31694},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31889,\"start\":31881},{\"end\":32956,\"start\":32948},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33103,\"start\":33094},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":33417,\"start\":33408},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33638,\"start\":33630},{\"end\":35648,\"start\":35640},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36954,\"start\":36945},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37707,\"start\":37698},{\"end\":39665,\"start\":39656},{\"end\":40616,\"start\":40607},{\"end\":41617,\"start\":41609},{\"end\":42455,\"start\":42446},{\"end\":43181,\"start\":43172},{\"end\":43951,\"start\":43942},{\"end\":44053,\"start\":44044},{\"end\":44200,\"start\":44191},{\"end\":44362,\"start\":44353},{\"end\":44455,\"start\":44446},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46055,\"start\":46046},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46406,\"start\":46399},{\"end\":48479,\"start\":48471},{\"end\":49158,\"start\":49150},{\"end\":49699,\"start\":49690},{\"end\":49915,\"start\":49902},{\"end\":51648,\"start\":51640},{\"end\":51680,\"start\":51672},{\"end\":52721,\"start\":52713},{\"end\":52838,\"start\":52830},{\"end\":54074,\"start\":54066},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":58128,\"start\":58119},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":58243,\"start\":58234},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":58615,\"start\":58601},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":59834,\"start\":59827}]", "bib_author_first_name": "[{\"end\":77855,\"start\":77851},{\"end\":77872,\"start\":77864},{\"end\":77886,\"start\":77879},{\"end\":78104,\"start\":78099},{\"end\":78118,\"start\":78113},{\"end\":78295,\"start\":78287},{\"end\":78309,\"start\":78303},{\"end\":78512,\"start\":78505},{\"end\":78527,\"start\":78521},{\"end\":78545,\"start\":78542},{\"end\":78547,\"start\":78546},{\"end\":78564,\"start\":78558},{\"end\":78577,\"start\":78573},{\"end\":78596,\"start\":78590},{\"end\":78598,\"start\":78597},{\"end\":78613,\"start\":78607},{\"end\":78628,\"start\":78621},{\"end\":78630,\"start\":78629},{\"end\":78973,\"start\":78967},{\"end\":78992,\"start\":78986},{\"end\":79009,\"start\":79004},{\"end\":79028,\"start\":79021},{\"end\":79043,\"start\":79036},{\"end\":79315,\"start\":79308},{\"end\":79347,\"start\":79338},{\"end\":79362,\"start\":79356},{\"end\":79630,\"start\":79622},{\"end\":79643,\"start\":79637},{\"end\":79659,\"start\":79652},{\"end\":79674,\"start\":79667},{\"end\":79676,\"start\":79675},{\"end\":79921,\"start\":79912},{\"end\":79936,\"start\":79930},{\"end\":80204,\"start\":80199},{\"end\":80218,\"start\":80212},{\"end\":80230,\"start\":80224},{\"end\":80249,\"start\":80243},{\"end\":80266,\"start\":80258},{\"end\":80283,\"start\":80276},{\"end\":80297,\"start\":80291},{\"end\":80308,\"start\":80304},{\"end\":80320,\"start\":80315},{\"end\":80554,\"start\":80549},{\"end\":80556,\"start\":80555},{\"end\":80570,\"start\":80564},{\"end\":80591,\"start\":80583},{\"end\":80603,\"start\":80600},{\"end\":80620,\"start\":80614},{\"end\":80632,\"start\":80628},{\"end\":80643,\"start\":80637},{\"end\":80661,\"start\":80654},{\"end\":80675,\"start\":80669},{\"end\":80685,\"start\":80682},{\"end\":80699,\"start\":80690},{\"end\":80708,\"start\":80706},{\"end\":80719,\"start\":80713},{\"end\":81145,\"start\":81139},{\"end\":81155,\"start\":81152},{\"end\":81308,\"start\":81302},{\"end\":81318,\"start\":81315},{\"end\":81540,\"start\":81534},{\"end\":81782,\"start\":81776},{\"end\":81798,\"start\":81792},{\"end\":82112,\"start\":82107},{\"end\":82121,\"start\":82120},{\"end\":82145,\"start\":82131},{\"end\":82393,\"start\":82388},{\"end\":82407,\"start\":82403},{\"end\":82602,\"start\":82596},{\"end\":82604,\"start\":82603},{\"end\":82639,\"start\":82632},{\"end\":82654,\"start\":82647},{\"end\":82656,\"start\":82655},{\"end\":82668,\"start\":82663},{\"end\":82938,\"start\":82932},{\"end\":82958,\"start\":82951},{\"end\":82974,\"start\":82968},{\"end\":82991,\"start\":82984},{\"end\":83004,\"start\":82997},{\"end\":83020,\"start\":83013},{\"end\":83032,\"start\":83026},{\"end\":83052,\"start\":83047},{\"end\":83371,\"start\":83368},{\"end\":83385,\"start\":83377},{\"end\":83397,\"start\":83392},{\"end\":83409,\"start\":83405},{\"end\":83426,\"start\":83420},{\"end\":83441,\"start\":83436},{\"end\":83710,\"start\":83703},{\"end\":83724,\"start\":83720},{\"end\":83726,\"start\":83725},{\"end\":83933,\"start\":83925},{\"end\":83950,\"start\":83943},{\"end\":83967,\"start\":83959},{\"end\":83969,\"start\":83968},{\"end\":83980,\"start\":83975},{\"end\":83982,\"start\":83981},{\"end\":83999,\"start\":83992},{\"end\":84237,\"start\":84233},{\"end\":84250,\"start\":84247},{\"end\":84263,\"start\":84259},{\"end\":84278,\"start\":84272},{\"end\":84533,\"start\":84526},{\"end\":84545,\"start\":84538},{\"end\":84561,\"start\":84553},{\"end\":84571,\"start\":84567},{\"end\":84765,\"start\":84760},{\"end\":84780,\"start\":84773},{\"end\":84792,\"start\":84786},{\"end\":84809,\"start\":84802},{\"end\":84825,\"start\":84817},{\"end\":84841,\"start\":84835},{\"end\":85072,\"start\":85069},{\"end\":85082,\"start\":85077},{\"end\":85097,\"start\":85092},{\"end\":85111,\"start\":85108},{\"end\":85347,\"start\":85340},{\"end\":85363,\"start\":85357},{\"end\":85579,\"start\":85578},{\"end\":85595,\"start\":85590},{\"end\":85774,\"start\":85765},{\"end\":85787,\"start\":85781},{\"end\":85805,\"start\":85797},{\"end\":85820,\"start\":85813},{\"end\":85837,\"start\":85831},{\"end\":85853,\"start\":85847},{\"end\":85867,\"start\":85863},{\"end\":85880,\"start\":85875},{\"end\":85895,\"start\":85888},{\"end\":86200,\"start\":86194},{\"end\":86216,\"start\":86211},{\"end\":86230,\"start\":86227},{\"end\":86249,\"start\":86243},{\"end\":86261,\"start\":86257},{\"end\":86275,\"start\":86270},{\"end\":86289,\"start\":86285},{\"end\":86574,\"start\":86568},{\"end\":86589,\"start\":86584},{\"end\":86604,\"start\":86597},{\"end\":86611,\"start\":86605},{\"end\":86626,\"start\":86620},{\"end\":86929,\"start\":86924},{\"end\":86936,\"start\":86934},{\"end\":86944,\"start\":86942},{\"end\":86959,\"start\":86951},{\"end\":86977,\"start\":86972},{\"end\":86988,\"start\":86985},{\"end\":87376,\"start\":87372},{\"end\":87388,\"start\":87383},{\"end\":87403,\"start\":87396},{\"end\":87632,\"start\":87625},{\"end\":87647,\"start\":87638},{\"end\":87658,\"start\":87655},{\"end\":87882,\"start\":87875},{\"end\":87884,\"start\":87883},{\"end\":87901,\"start\":87895},{\"end\":87903,\"start\":87902},{\"end\":88130,\"start\":88126},{\"end\":88149,\"start\":88142},{\"end\":88166,\"start\":88159},{\"end\":88186,\"start\":88177},{\"end\":88203,\"start\":88196},{\"end\":88461,\"start\":88453},{\"end\":88472,\"start\":88466},{\"end\":88485,\"start\":88478},{\"end\":88691,\"start\":88688},{\"end\":88705,\"start\":88704},{\"end\":88721,\"start\":88714},{\"end\":88742,\"start\":88734},{\"end\":88744,\"start\":88743},{\"end\":88757,\"start\":88753},{\"end\":88769,\"start\":88766},{\"end\":89018,\"start\":89011},{\"end\":89020,\"start\":89019},{\"end\":89221,\"start\":89211},{\"end\":89233,\"start\":89228},{\"end\":89250,\"start\":89244},{\"end\":89266,\"start\":89259},{\"end\":89283,\"start\":89277},{\"end\":89535,\"start\":89531},{\"end\":89544,\"start\":89543},{\"end\":89559,\"start\":89552},{\"end\":89573,\"start\":89567},{\"end\":89575,\"start\":89574},{\"end\":89592,\"start\":89584},{\"end\":89594,\"start\":89593},{\"end\":89863,\"start\":89856},{\"end\":89875,\"start\":89870},{\"end\":89887,\"start\":89883},{\"end\":89901,\"start\":89894},{\"end\":89916,\"start\":89912},{\"end\":89935,\"start\":89928},{\"end\":90169,\"start\":90162},{\"end\":90183,\"start\":90176},{\"end\":90198,\"start\":90194},{\"end\":90214,\"start\":90210},{\"end\":90233,\"start\":90226},{\"end\":90476,\"start\":90469},{\"end\":90478,\"start\":90477},{\"end\":90486,\"start\":90483},{\"end\":90498,\"start\":90491},{\"end\":90511,\"start\":90503},{\"end\":90513,\"start\":90512},{\"end\":90778,\"start\":90771},{\"end\":90780,\"start\":90779},{\"end\":90787,\"start\":90785},{\"end\":90795,\"start\":90792},{\"end\":90808,\"start\":90800},{\"end\":90810,\"start\":90809},{\"end\":91080,\"start\":91069},{\"end\":91098,\"start\":91094},{\"end\":91113,\"start\":91109},{\"end\":91129,\"start\":91124},{\"end\":91141,\"start\":91137},{\"end\":91387,\"start\":91380},{\"end\":91402,\"start\":91397},{\"end\":91421,\"start\":91414},{\"end\":91423,\"start\":91422},{\"end\":91439,\"start\":91433},{\"end\":91455,\"start\":91450},{\"end\":91475,\"start\":91469},{\"end\":91489,\"start\":91483},{\"end\":91751,\"start\":91746},{\"end\":91765,\"start\":91762},{\"end\":91773,\"start\":91770},{\"end\":91959,\"start\":91954},{\"end\":91977,\"start\":91970},{\"end\":91995,\"start\":91987},{\"end\":92250,\"start\":92242},{\"end\":92262,\"start\":92258},{\"end\":92440,\"start\":92432},{\"end\":92453,\"start\":92447},{\"end\":92462,\"start\":92458},{\"end\":92476,\"start\":92469},{\"end\":92734,\"start\":92727},{\"end\":92746,\"start\":92745},{\"end\":92748,\"start\":92747},{\"end\":92766,\"start\":92757},{\"end\":92768,\"start\":92767},{\"end\":92782,\"start\":92777},{\"end\":92784,\"start\":92783},{\"end\":92800,\"start\":92794},{\"end\":93124,\"start\":93117},{\"end\":93137,\"start\":93131},{\"end\":93146,\"start\":93143},{\"end\":93158,\"start\":93151},{\"end\":93166,\"start\":93163},{\"end\":93175,\"start\":93172},{\"end\":93414,\"start\":93407},{\"end\":93430,\"start\":93425},{\"end\":93449,\"start\":93442},{\"end\":93462,\"start\":93457},{\"end\":93474,\"start\":93470},{\"end\":93489,\"start\":93482},{\"end\":93718,\"start\":93713},{\"end\":93732,\"start\":93727},{\"end\":93750,\"start\":93745},{\"end\":93917,\"start\":93910},{\"end\":93928,\"start\":93924},{\"end\":94157,\"start\":94150},{\"end\":94170,\"start\":94164},{\"end\":94184,\"start\":94177}]", "bib_author_last_name": "[{\"end\":77862,\"start\":77856},{\"end\":77877,\"start\":77873},{\"end\":77898,\"start\":77887},{\"end\":78111,\"start\":78105},{\"end\":78125,\"start\":78119},{\"end\":78301,\"start\":78296},{\"end\":78315,\"start\":78310},{\"end\":78519,\"start\":78513},{\"end\":78540,\"start\":78528},{\"end\":78556,\"start\":78548},{\"end\":78571,\"start\":78565},{\"end\":78588,\"start\":78578},{\"end\":78605,\"start\":78599},{\"end\":78619,\"start\":78614},{\"end\":78636,\"start\":78631},{\"end\":78984,\"start\":78974},{\"end\":79002,\"start\":78993},{\"end\":79019,\"start\":79010},{\"end\":79034,\"start\":79029},{\"end\":79050,\"start\":79044},{\"end\":79336,\"start\":79316},{\"end\":79354,\"start\":79348},{\"end\":79371,\"start\":79363},{\"end\":79382,\"start\":79373},{\"end\":79635,\"start\":79631},{\"end\":79650,\"start\":79644},{\"end\":79665,\"start\":79660},{\"end\":79682,\"start\":79677},{\"end\":79928,\"start\":79922},{\"end\":79943,\"start\":79937},{\"end\":80210,\"start\":80205},{\"end\":80222,\"start\":80219},{\"end\":80241,\"start\":80231},{\"end\":80256,\"start\":80250},{\"end\":80274,\"start\":80267},{\"end\":80289,\"start\":80284},{\"end\":80302,\"start\":80298},{\"end\":80313,\"start\":80309},{\"end\":80326,\"start\":80321},{\"end\":80562,\"start\":80557},{\"end\":80581,\"start\":80571},{\"end\":80598,\"start\":80592},{\"end\":80612,\"start\":80604},{\"end\":80626,\"start\":80621},{\"end\":80635,\"start\":80633},{\"end\":80652,\"start\":80644},{\"end\":80667,\"start\":80662},{\"end\":80680,\"start\":80676},{\"end\":80688,\"start\":80686},{\"end\":80704,\"start\":80700},{\"end\":80711,\"start\":80709},{\"end\":80722,\"start\":80720},{\"end\":81150,\"start\":81146},{\"end\":81161,\"start\":81156},{\"end\":81313,\"start\":81309},{\"end\":81324,\"start\":81319},{\"end\":81550,\"start\":81541},{\"end\":81790,\"start\":81783},{\"end\":81808,\"start\":81799},{\"end\":82118,\"start\":82113},{\"end\":82129,\"start\":82122},{\"end\":82148,\"start\":82146},{\"end\":82401,\"start\":82394},{\"end\":82413,\"start\":82408},{\"end\":82630,\"start\":82605},{\"end\":82645,\"start\":82640},{\"end\":82661,\"start\":82657},{\"end\":82674,\"start\":82669},{\"end\":82682,\"start\":82676},{\"end\":82949,\"start\":82939},{\"end\":82966,\"start\":82959},{\"end\":82982,\"start\":82975},{\"end\":82995,\"start\":82992},{\"end\":83011,\"start\":83005},{\"end\":83024,\"start\":83021},{\"end\":83045,\"start\":83033},{\"end\":83059,\"start\":83053},{\"end\":83375,\"start\":83372},{\"end\":83390,\"start\":83386},{\"end\":83403,\"start\":83398},{\"end\":83418,\"start\":83410},{\"end\":83434,\"start\":83427},{\"end\":83448,\"start\":83442},{\"end\":83718,\"start\":83711},{\"end\":83735,\"start\":83727},{\"end\":83941,\"start\":83934},{\"end\":83957,\"start\":83951},{\"end\":83973,\"start\":83970},{\"end\":83990,\"start\":83983},{\"end\":84005,\"start\":84000},{\"end\":84245,\"start\":84238},{\"end\":84257,\"start\":84251},{\"end\":84270,\"start\":84264},{\"end\":84287,\"start\":84279},{\"end\":84536,\"start\":84534},{\"end\":84551,\"start\":84546},{\"end\":84565,\"start\":84562},{\"end\":84575,\"start\":84572},{\"end\":84771,\"start\":84766},{\"end\":84784,\"start\":84781},{\"end\":84800,\"start\":84793},{\"end\":84815,\"start\":84810},{\"end\":84833,\"start\":84826},{\"end\":84852,\"start\":84842},{\"end\":85075,\"start\":85073},{\"end\":85090,\"start\":85083},{\"end\":85106,\"start\":85098},{\"end\":85118,\"start\":85112},{\"end\":85355,\"start\":85348},{\"end\":85369,\"start\":85364},{\"end\":85588,\"start\":85580},{\"end\":85602,\"start\":85596},{\"end\":85606,\"start\":85604},{\"end\":85779,\"start\":85775},{\"end\":85795,\"start\":85788},{\"end\":85811,\"start\":85806},{\"end\":85829,\"start\":85821},{\"end\":85845,\"start\":85838},{\"end\":85861,\"start\":85854},{\"end\":85873,\"start\":85868},{\"end\":85886,\"start\":85881},{\"end\":85903,\"start\":85896},{\"end\":86209,\"start\":86201},{\"end\":86225,\"start\":86217},{\"end\":86241,\"start\":86231},{\"end\":86255,\"start\":86250},{\"end\":86268,\"start\":86262},{\"end\":86283,\"start\":86276},{\"end\":86294,\"start\":86290},{\"end\":86582,\"start\":86575},{\"end\":86595,\"start\":86590},{\"end\":86618,\"start\":86612},{\"end\":86634,\"start\":86627},{\"end\":86932,\"start\":86930},{\"end\":86940,\"start\":86937},{\"end\":86949,\"start\":86945},{\"end\":86970,\"start\":86960},{\"end\":86983,\"start\":86978},{\"end\":86991,\"start\":86989},{\"end\":87381,\"start\":87377},{\"end\":87394,\"start\":87389},{\"end\":87410,\"start\":87404},{\"end\":87636,\"start\":87633},{\"end\":87653,\"start\":87648},{\"end\":87661,\"start\":87659},{\"end\":87893,\"start\":87885},{\"end\":87909,\"start\":87904},{\"end\":88140,\"start\":88131},{\"end\":88157,\"start\":88150},{\"end\":88175,\"start\":88167},{\"end\":88194,\"start\":88187},{\"end\":88210,\"start\":88204},{\"end\":88464,\"start\":88462},{\"end\":88476,\"start\":88473},{\"end\":88489,\"start\":88486},{\"end\":88702,\"start\":88692},{\"end\":88712,\"start\":88706},{\"end\":88732,\"start\":88722},{\"end\":88751,\"start\":88745},{\"end\":88764,\"start\":88758},{\"end\":88781,\"start\":88770},{\"end\":88785,\"start\":88783},{\"end\":89028,\"start\":89021},{\"end\":89226,\"start\":89222},{\"end\":89242,\"start\":89234},{\"end\":89257,\"start\":89251},{\"end\":89275,\"start\":89267},{\"end\":89293,\"start\":89284},{\"end\":89541,\"start\":89536},{\"end\":89550,\"start\":89545},{\"end\":89565,\"start\":89560},{\"end\":89582,\"start\":89576},{\"end\":89600,\"start\":89595},{\"end\":89608,\"start\":89602},{\"end\":89868,\"start\":89864},{\"end\":89881,\"start\":89876},{\"end\":89892,\"start\":89888},{\"end\":89910,\"start\":89902},{\"end\":89926,\"start\":89917},{\"end\":89942,\"start\":89936},{\"end\":90174,\"start\":90170},{\"end\":90192,\"start\":90184},{\"end\":90208,\"start\":90199},{\"end\":90224,\"start\":90215},{\"end\":90240,\"start\":90234},{\"end\":90481,\"start\":90479},{\"end\":90489,\"start\":90487},{\"end\":90501,\"start\":90499},{\"end\":90520,\"start\":90514},{\"end\":90783,\"start\":90781},{\"end\":90790,\"start\":90788},{\"end\":90798,\"start\":90796},{\"end\":90817,\"start\":90811},{\"end\":91092,\"start\":91081},{\"end\":91107,\"start\":91099},{\"end\":91122,\"start\":91114},{\"end\":91135,\"start\":91130},{\"end\":91152,\"start\":91142},{\"end\":91395,\"start\":91388},{\"end\":91412,\"start\":91403},{\"end\":91431,\"start\":91424},{\"end\":91448,\"start\":91440},{\"end\":91467,\"start\":91456},{\"end\":91481,\"start\":91476},{\"end\":91493,\"start\":91490},{\"end\":91760,\"start\":91752},{\"end\":91768,\"start\":91766},{\"end\":91780,\"start\":91774},{\"end\":91968,\"start\":91960},{\"end\":91985,\"start\":91978},{\"end\":92001,\"start\":91996},{\"end\":92256,\"start\":92251},{\"end\":92273,\"start\":92263},{\"end\":92445,\"start\":92441},{\"end\":92456,\"start\":92454},{\"end\":92467,\"start\":92463},{\"end\":92479,\"start\":92477},{\"end\":92743,\"start\":92735},{\"end\":92755,\"start\":92749},{\"end\":92775,\"start\":92769},{\"end\":92792,\"start\":92785},{\"end\":92808,\"start\":92801},{\"end\":92819,\"start\":92810},{\"end\":93129,\"start\":93125},{\"end\":93141,\"start\":93138},{\"end\":93149,\"start\":93147},{\"end\":93161,\"start\":93159},{\"end\":93170,\"start\":93167},{\"end\":93181,\"start\":93176},{\"end\":93423,\"start\":93415},{\"end\":93440,\"start\":93431},{\"end\":93455,\"start\":93450},{\"end\":93468,\"start\":93463},{\"end\":93480,\"start\":93475},{\"end\":93497,\"start\":93490},{\"end\":93725,\"start\":93719},{\"end\":93743,\"start\":93733},{\"end\":93757,\"start\":93751},{\"end\":93922,\"start\":93918},{\"end\":93937,\"start\":93929},{\"end\":94162,\"start\":94158},{\"end\":94175,\"start\":94171},{\"end\":94191,\"start\":94185}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3195822},\"end\":78044,\"start\":77797},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208267630},\"end\":78242,\"start\":78046},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":210023455},\"end\":78449,\"start\":78244},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5902682},\"end\":78909,\"start\":78451},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11096669},\"end\":79246,\"start\":78911},{\"attributes\":{\"doi\":\"ICCV. 5420-5430\",\"id\":\"b5\"},\"end\":79564,\"start\":79248},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206592437},\"end\":79838,\"start\":79566},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":215749793},\"end\":80134,\"start\":79840},{\"attributes\":{\"id\":\"b8\"},\"end\":80547,\"start\":80136},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b9\"},\"end\":81081,\"start\":80549},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54457478},\"end\":81277,\"start\":81083},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235490667},\"end\":81462,\"start\":81279},{\"attributes\":{\"doi\":\"CN/95-17. CERN\",\"id\":\"b12\"},\"end\":81710,\"start\":81464},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":225076487},\"end\":82047,\"start\":81712},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7934371},\"end\":82319,\"start\":82049},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12358833},\"end\":82549,\"start\":82321},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14395359},\"end\":82909,\"start\":82551},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8545966},\"end\":83304,\"start\":82911},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":226237264},\"end\":83649,\"start\":83306},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":621181},\"end\":83864,\"start\":83651},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3656527},\"end\":84183,\"start\":83866},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":218863093},\"end\":84478,\"start\":84185},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206594692},\"end\":84707,\"start\":84480},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":214606025},\"end\":85034,\"start\":84709},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5060520},\"end\":85297,\"start\":85036},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1371704},\"end\":85532,\"start\":85299},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6628106},\"end\":85705,\"start\":85534},{\"attributes\":{\"id\":\"b27\"},\"end\":86132,\"start\":85707},{\"attributes\":{\"id\":\"b28\"},\"end\":86487,\"start\":86134},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6195034},\"end\":86865,\"start\":86489},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6224050},\"end\":87306,\"start\":86867},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":46900294},\"end\":87551,\"start\":87308},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220647077},\"end\":87804,\"start\":87553},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15545924},\"end\":88058,\"start\":87806},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":54465161},\"end\":88399,\"start\":88060},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":215754836},\"end\":88614,\"start\":88401},{\"attributes\":{\"id\":\"b36\"},\"end\":88988,\"start\":88616},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6576078},\"end\":89128,\"start\":88990},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":58007025},\"end\":89495,\"start\":89130},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":364731},\"end\":89804,\"start\":89497},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235358422},\"end\":90126,\"start\":89806},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":212646575},\"end\":90389,\"start\":90128},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5115938},\"end\":90689,\"start\":90391},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1745976},\"end\":91007,\"start\":90691},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":227247978},\"end\":91330,\"start\":91009},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":219530677},\"end\":91718,\"start\":91332},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2285821},\"end\":91899,\"start\":91720},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":7779419},\"end\":92187,\"start\":91901},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":218502297},\"end\":92393,\"start\":92189},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1549630},\"end\":92657,\"start\":92395},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":219720931},\"end\":93038,\"start\":92659},{\"attributes\":{\"id\":\"b51\"},\"end\":93356,\"start\":93040},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":53761010},\"end\":93678,\"start\":93358},{\"attributes\":{\"id\":\"b53\"},\"end\":93908,\"start\":93680},{\"attributes\":{\"doi\":\"arXiv:1605.04797\",\"id\":\"b54\"},\"end\":94148,\"start\":93910},{\"attributes\":{\"doi\":\"arXiv:1801.09847\",\"id\":\"b55\"},\"end\":94413,\"start\":94150}]", "bib_title": "[{\"end\":77849,\"start\":77797},{\"end\":78097,\"start\":78046},{\"end\":78285,\"start\":78244},{\"end\":78503,\"start\":78451},{\"end\":78965,\"start\":78911},{\"end\":79620,\"start\":79566},{\"end\":79910,\"start\":79840},{\"end\":81137,\"start\":81083},{\"end\":81300,\"start\":81279},{\"end\":81774,\"start\":81712},{\"end\":82105,\"start\":82049},{\"end\":82386,\"start\":82321},{\"end\":82594,\"start\":82551},{\"end\":82930,\"start\":82911},{\"end\":83366,\"start\":83306},{\"end\":83701,\"start\":83651},{\"end\":83923,\"start\":83866},{\"end\":84231,\"start\":84185},{\"end\":84524,\"start\":84480},{\"end\":84758,\"start\":84709},{\"end\":85067,\"start\":85036},{\"end\":85338,\"start\":85299},{\"end\":85576,\"start\":85534},{\"end\":86566,\"start\":86489},{\"end\":86922,\"start\":86867},{\"end\":87370,\"start\":87308},{\"end\":87623,\"start\":87553},{\"end\":87873,\"start\":87806},{\"end\":88124,\"start\":88060},{\"end\":88451,\"start\":88401},{\"end\":89009,\"start\":88990},{\"end\":89209,\"start\":89130},{\"end\":89529,\"start\":89497},{\"end\":89854,\"start\":89806},{\"end\":90160,\"start\":90128},{\"end\":90467,\"start\":90391},{\"end\":90769,\"start\":90691},{\"end\":91067,\"start\":91009},{\"end\":91378,\"start\":91332},{\"end\":91744,\"start\":91720},{\"end\":91952,\"start\":91901},{\"end\":92240,\"start\":92189},{\"end\":92430,\"start\":92395},{\"end\":92725,\"start\":92659},{\"end\":93405,\"start\":93358},{\"end\":93711,\"start\":93680}]", "bib_author": "[{\"end\":77864,\"start\":77851},{\"end\":77879,\"start\":77864},{\"end\":77900,\"start\":77879},{\"end\":78113,\"start\":78099},{\"end\":78127,\"start\":78113},{\"end\":78303,\"start\":78287},{\"end\":78317,\"start\":78303},{\"end\":78521,\"start\":78505},{\"end\":78542,\"start\":78521},{\"end\":78558,\"start\":78542},{\"end\":78573,\"start\":78558},{\"end\":78590,\"start\":78573},{\"end\":78607,\"start\":78590},{\"end\":78621,\"start\":78607},{\"end\":78638,\"start\":78621},{\"end\":78986,\"start\":78967},{\"end\":79004,\"start\":78986},{\"end\":79021,\"start\":79004},{\"end\":79036,\"start\":79021},{\"end\":79052,\"start\":79036},{\"end\":79338,\"start\":79308},{\"end\":79356,\"start\":79338},{\"end\":79373,\"start\":79356},{\"end\":79384,\"start\":79373},{\"end\":79637,\"start\":79622},{\"end\":79652,\"start\":79637},{\"end\":79667,\"start\":79652},{\"end\":79684,\"start\":79667},{\"end\":79930,\"start\":79912},{\"end\":79945,\"start\":79930},{\"end\":80212,\"start\":80199},{\"end\":80224,\"start\":80212},{\"end\":80243,\"start\":80224},{\"end\":80258,\"start\":80243},{\"end\":80276,\"start\":80258},{\"end\":80291,\"start\":80276},{\"end\":80304,\"start\":80291},{\"end\":80315,\"start\":80304},{\"end\":80328,\"start\":80315},{\"end\":80564,\"start\":80549},{\"end\":80583,\"start\":80564},{\"end\":80600,\"start\":80583},{\"end\":80614,\"start\":80600},{\"end\":80628,\"start\":80614},{\"end\":80637,\"start\":80628},{\"end\":80654,\"start\":80637},{\"end\":80669,\"start\":80654},{\"end\":80682,\"start\":80669},{\"end\":80690,\"start\":80682},{\"end\":80706,\"start\":80690},{\"end\":80713,\"start\":80706},{\"end\":80724,\"start\":80713},{\"end\":81152,\"start\":81139},{\"end\":81163,\"start\":81152},{\"end\":81315,\"start\":81302},{\"end\":81326,\"start\":81315},{\"end\":81552,\"start\":81534},{\"end\":81792,\"start\":81776},{\"end\":81810,\"start\":81792},{\"end\":82120,\"start\":82107},{\"end\":82131,\"start\":82120},{\"end\":82150,\"start\":82131},{\"end\":82403,\"start\":82388},{\"end\":82415,\"start\":82403},{\"end\":82632,\"start\":82596},{\"end\":82647,\"start\":82632},{\"end\":82663,\"start\":82647},{\"end\":82676,\"start\":82663},{\"end\":82684,\"start\":82676},{\"end\":82951,\"start\":82932},{\"end\":82968,\"start\":82951},{\"end\":82984,\"start\":82968},{\"end\":82997,\"start\":82984},{\"end\":83013,\"start\":82997},{\"end\":83026,\"start\":83013},{\"end\":83047,\"start\":83026},{\"end\":83061,\"start\":83047},{\"end\":83377,\"start\":83368},{\"end\":83392,\"start\":83377},{\"end\":83405,\"start\":83392},{\"end\":83420,\"start\":83405},{\"end\":83436,\"start\":83420},{\"end\":83450,\"start\":83436},{\"end\":83720,\"start\":83703},{\"end\":83737,\"start\":83720},{\"end\":83943,\"start\":83925},{\"end\":83959,\"start\":83943},{\"end\":83975,\"start\":83959},{\"end\":83992,\"start\":83975},{\"end\":84007,\"start\":83992},{\"end\":84247,\"start\":84233},{\"end\":84259,\"start\":84247},{\"end\":84272,\"start\":84259},{\"end\":84289,\"start\":84272},{\"end\":84538,\"start\":84526},{\"end\":84553,\"start\":84538},{\"end\":84567,\"start\":84553},{\"end\":84577,\"start\":84567},{\"end\":84773,\"start\":84760},{\"end\":84786,\"start\":84773},{\"end\":84802,\"start\":84786},{\"end\":84817,\"start\":84802},{\"end\":84835,\"start\":84817},{\"end\":84854,\"start\":84835},{\"end\":85077,\"start\":85069},{\"end\":85092,\"start\":85077},{\"end\":85108,\"start\":85092},{\"end\":85120,\"start\":85108},{\"end\":85357,\"start\":85340},{\"end\":85371,\"start\":85357},{\"end\":85590,\"start\":85578},{\"end\":85604,\"start\":85590},{\"end\":85608,\"start\":85604},{\"end\":85781,\"start\":85765},{\"end\":85797,\"start\":85781},{\"end\":85813,\"start\":85797},{\"end\":85831,\"start\":85813},{\"end\":85847,\"start\":85831},{\"end\":85863,\"start\":85847},{\"end\":85875,\"start\":85863},{\"end\":85888,\"start\":85875},{\"end\":85905,\"start\":85888},{\"end\":86211,\"start\":86194},{\"end\":86227,\"start\":86211},{\"end\":86243,\"start\":86227},{\"end\":86257,\"start\":86243},{\"end\":86270,\"start\":86257},{\"end\":86285,\"start\":86270},{\"end\":86296,\"start\":86285},{\"end\":86584,\"start\":86568},{\"end\":86597,\"start\":86584},{\"end\":86620,\"start\":86597},{\"end\":86636,\"start\":86620},{\"end\":86934,\"start\":86924},{\"end\":86942,\"start\":86934},{\"end\":86951,\"start\":86942},{\"end\":86972,\"start\":86951},{\"end\":86985,\"start\":86972},{\"end\":86993,\"start\":86985},{\"end\":87383,\"start\":87372},{\"end\":87396,\"start\":87383},{\"end\":87412,\"start\":87396},{\"end\":87638,\"start\":87625},{\"end\":87655,\"start\":87638},{\"end\":87663,\"start\":87655},{\"end\":87895,\"start\":87875},{\"end\":87911,\"start\":87895},{\"end\":88142,\"start\":88126},{\"end\":88159,\"start\":88142},{\"end\":88177,\"start\":88159},{\"end\":88196,\"start\":88177},{\"end\":88212,\"start\":88196},{\"end\":88466,\"start\":88453},{\"end\":88478,\"start\":88466},{\"end\":88491,\"start\":88478},{\"end\":88704,\"start\":88688},{\"end\":88714,\"start\":88704},{\"end\":88734,\"start\":88714},{\"end\":88753,\"start\":88734},{\"end\":88766,\"start\":88753},{\"end\":88783,\"start\":88766},{\"end\":88787,\"start\":88783},{\"end\":89030,\"start\":89011},{\"end\":89228,\"start\":89211},{\"end\":89244,\"start\":89228},{\"end\":89259,\"start\":89244},{\"end\":89277,\"start\":89259},{\"end\":89295,\"start\":89277},{\"end\":89543,\"start\":89531},{\"end\":89552,\"start\":89543},{\"end\":89567,\"start\":89552},{\"end\":89584,\"start\":89567},{\"end\":89602,\"start\":89584},{\"end\":89610,\"start\":89602},{\"end\":89870,\"start\":89856},{\"end\":89883,\"start\":89870},{\"end\":89894,\"start\":89883},{\"end\":89912,\"start\":89894},{\"end\":89928,\"start\":89912},{\"end\":89944,\"start\":89928},{\"end\":90176,\"start\":90162},{\"end\":90194,\"start\":90176},{\"end\":90210,\"start\":90194},{\"end\":90226,\"start\":90210},{\"end\":90242,\"start\":90226},{\"end\":90483,\"start\":90469},{\"end\":90491,\"start\":90483},{\"end\":90503,\"start\":90491},{\"end\":90522,\"start\":90503},{\"end\":90785,\"start\":90771},{\"end\":90792,\"start\":90785},{\"end\":90800,\"start\":90792},{\"end\":90819,\"start\":90800},{\"end\":91094,\"start\":91069},{\"end\":91109,\"start\":91094},{\"end\":91124,\"start\":91109},{\"end\":91137,\"start\":91124},{\"end\":91154,\"start\":91137},{\"end\":91397,\"start\":91380},{\"end\":91414,\"start\":91397},{\"end\":91433,\"start\":91414},{\"end\":91450,\"start\":91433},{\"end\":91469,\"start\":91450},{\"end\":91483,\"start\":91469},{\"end\":91495,\"start\":91483},{\"end\":91762,\"start\":91746},{\"end\":91770,\"start\":91762},{\"end\":91782,\"start\":91770},{\"end\":91970,\"start\":91954},{\"end\":91987,\"start\":91970},{\"end\":92003,\"start\":91987},{\"end\":92258,\"start\":92242},{\"end\":92275,\"start\":92258},{\"end\":92447,\"start\":92432},{\"end\":92458,\"start\":92447},{\"end\":92469,\"start\":92458},{\"end\":92481,\"start\":92469},{\"end\":92745,\"start\":92727},{\"end\":92757,\"start\":92745},{\"end\":92777,\"start\":92757},{\"end\":92794,\"start\":92777},{\"end\":92810,\"start\":92794},{\"end\":92821,\"start\":92810},{\"end\":93131,\"start\":93117},{\"end\":93143,\"start\":93131},{\"end\":93151,\"start\":93143},{\"end\":93163,\"start\":93151},{\"end\":93172,\"start\":93163},{\"end\":93183,\"start\":93172},{\"end\":93425,\"start\":93407},{\"end\":93442,\"start\":93425},{\"end\":93457,\"start\":93442},{\"end\":93470,\"start\":93457},{\"end\":93482,\"start\":93470},{\"end\":93499,\"start\":93482},{\"end\":93727,\"start\":93713},{\"end\":93745,\"start\":93727},{\"end\":93759,\"start\":93745},{\"end\":93924,\"start\":93910},{\"end\":93939,\"start\":93924},{\"end\":94164,\"start\":94150},{\"end\":94177,\"start\":94164},{\"end\":94193,\"start\":94177}]", "bib_venue": "[{\"end\":87098,\"start\":87054},{\"end\":77908,\"start\":77900},{\"end\":78131,\"start\":78127},{\"end\":78321,\"start\":78317},{\"end\":78661,\"start\":78638},{\"end\":79061,\"start\":79052},{\"end\":79306,\"start\":79248},{\"end\":79688,\"start\":79684},{\"end\":79968,\"start\":79945},{\"end\":80197,\"start\":80136},{\"end\":80789,\"start\":80740},{\"end\":81167,\"start\":81163},{\"end\":81354,\"start\":81326},{\"end\":81532,\"start\":81464},{\"end\":81859,\"start\":81810},{\"end\":82169,\"start\":82150},{\"end\":82423,\"start\":82415},{\"end\":82712,\"start\":82684},{\"end\":83089,\"start\":83061},{\"end\":83460,\"start\":83450},{\"end\":83745,\"start\":83737},{\"end\":84011,\"start\":84007},{\"end\":84317,\"start\":84289},{\"end\":84581,\"start\":84577},{\"end\":84858,\"start\":84854},{\"end\":85148,\"start\":85120},{\"end\":85399,\"start\":85371},{\"end\":85612,\"start\":85608},{\"end\":85763,\"start\":85707},{\"end\":86192,\"start\":86134},{\"end\":86661,\"start\":86636},{\"end\":87052,\"start\":86993},{\"end\":87416,\"start\":87412},{\"end\":87667,\"start\":87663},{\"end\":87919,\"start\":87911},{\"end\":88216,\"start\":88212},{\"end\":88495,\"start\":88491},{\"end\":88686,\"start\":88616},{\"end\":89048,\"start\":89030},{\"end\":89299,\"start\":89295},{\"end\":89638,\"start\":89610},{\"end\":89954,\"start\":89944},{\"end\":90246,\"start\":90242},{\"end\":90526,\"start\":90522},{\"end\":90829,\"start\":90819},{\"end\":91158,\"start\":91154},{\"end\":91505,\"start\":91495},{\"end\":91791,\"start\":91782},{\"end\":92026,\"start\":92003},{\"end\":92279,\"start\":92275},{\"end\":92509,\"start\":92481},{\"end\":92831,\"start\":92821},{\"end\":93115,\"start\":93040},{\"end\":93503,\"start\":93499},{\"end\":93778,\"start\":93759},{\"end\":94004,\"start\":93955},{\"end\":94256,\"start\":94209}]"}}}, "year": 2023, "month": 12, "day": 17}