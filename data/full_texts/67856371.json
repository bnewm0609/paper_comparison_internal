{"id": 67856371, "updated": "2023-10-02 04:58:32.911", "metadata": {"title": "Dynamic Feature Fusion for Semantic Edge Detection", "authors": "[{\"first\":\"Yuan\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yunpeng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jiashi\",\"last\":\"Feng\",\"middle\":[]}]", "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2019, "month": 2, "day": 25}, "abstract": "Features from multiple scales can greatly benefit the semantic edge detection task if they are well fused. However, the prevalent semantic edge detection methods apply a fixed weight fusion strategy where images with different semantics are forced to share the same weights, resulting in universal fusion weights for all images and locations regardless of their different semantics or local context. In this work, we propose a novel dynamic feature fusion strategy that assigns different fusion weights for different input images and locations adaptively. This is achieved by a proposed weight learner to infer proper fusion weights over multi-level features for each location of the feature map, conditioned on the specific input. In this way, the heterogeneity in contributions made by different locations of feature maps and input images can be better considered and thus help produce more accurate and sharper edge predictions. We show that our model with the novel dynamic feature fusion is superior to fixed weight fusion and also the na\\\"ive location-invariant weight fusion methods, via comprehensive experiments on benchmarks Cityscapes and SBD. In particular, our method outperforms all existing well established methods and achieves new state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1902.09104", "mag": "2966401131", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/HuCLF19", "doi": "10.24963/ijcai.2019/110"}}, "content": {"source": {"pdf_hash": "b576921ca6ed56dca074dde7cb6936e1b5557c3b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1902.09104v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2019/0110.pdf", "status": "BRONZE"}}, "grobid": {"id": "26ce8a0311f61f7b2577bdb6809f6175d397b4d2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b576921ca6ed56dca074dde7cb6936e1b5557c3b.txt", "contents": "\nDynamic Feature Fusion for Semantic Edge Detection\n\n\nYuan Hu \nInstitute of Remote Sensing and Digital Earth\nCAS\n100094BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nYunpeng Chen chenyunpeng@u.nus.edu \nNational University of Singapore\n\n\nXiang Li lixiang01@radi.ac.cn \nInstitute of Remote Sensing and Digital Earth\nCAS\n100094BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nJiashi Feng \nNational University of Singapore\n\n\nDynamic Feature Fusion for Semantic Edge Detection\n\nFeatures from multiple scales can greatly benefit the semantic edge detection task if they are well fused. However, the prevalent semantic edge detection methods apply a fixed weight fusion strategy where images with different semantics are forced to share the same weights, resulting in universal fusion weights for all images and locations regardless of their different semantics or local context. In this work, we propose a novel dynamic feature fusion strategy that assigns different fusion weights for different input images and locations adaptively. This is achieved by a proposed weight learner to infer proper fusion weights over multi-level features for each location of the feature map, conditioned on the specific input. In this way, the heterogeneity in contributions made by different locations of feature maps and input images can be better considered and thus help produce more accurate and sharper edge predictions. We show that our model with the novel dynamic feature fusion is superior to fixed weight fusion and also the na\u00efve location-invariant weight fusion methods, via comprehensive experiments on benchmarks Cityscapes and SBD. In particular, our method outperforms all existing well established methods and achieves new state-of-the-art.\n\nIntroduction\n\nThe task of semantic edge detection (SED) is aimed at both detecting visually salient edges and recognizing their categories, or more concretely, locating fine edges utlizing lowlevel features and meanwhile identifying semantic categories with abstracted high-level features. An intuitive way for a deep CNN model to achieve both targets is to integrate highlevel semantic features with low-level category-agnostic edge features via a fusion model, which is conventionally designed following a fixed weight fusion strategy, independent of the input, as illustrated in the top row in Figure 1.\n\nIn many existing deep SED models [Yu et al., 2017;Liu et al., 2018;Yu et al., 2018], fixed weight fusion of multi-level features is implemented through 1 \u00d7 1 convolution, where the learned convolution kernel serves as the fusion weights. However, this fusion strategy cannot fully exploit multi-level information, especially the low-level features. This is because, first, it applies the same fusion weights to all the input images and ignores their variations in contents, illumination, etc. The distinct properties of a specific input need be treated adaptively for revealing the subtle edge details. Besides, for a same input image, different spatial locations on the corresponding feature map convey different information, but the fixed weight fusion manner applies the same weights to all these locations, regardless of their different semantic classes or object parts. This would unfavorably force the model to learn universal fusion weights for all the categories and locations. Consequently, a bias would be caused toward high-level features, and the power of multilevel response fusion is significantly weakened.\n\nIn this work, we propose a Dynamic Feature Fusion (DFF) method that assigns adaptive fusion weights to each location individually, aiming to generate a fused edge map adaptive to the specific content of each image, as illustrated in the bottom row in Figure 1. In particular, we design a novel location-adaptive weight learner that actively learns customized location-specific fusion weights conditional on the feature map content for multi-level response maps. As shown in Figure 1, low-level features (A side1 \u223c A side3 ) and a high-level feature (A side5 ) are merged to produce the final fused output. Low-level feature maps give high response on fine details, such as the edges inside objects, whereas highlevel ones are coarser and only exhibit strong response at the object-level boundaries. This location-adaptive weight learner tailors fusion weights for each individual location. For example, for the boundaries of the horse, fusion weights are biased toward low-level features to fully take advantage of the accurate located edges. For the interior of the horse, higher weights are assigned to high-level features to suppress the fragmentary and trivial edge responses inside the object. The proposed DFF model consists of two main components: a feature extractor with a normalizer and an adaptive weight fusion module. The feature extractor primarily scales the multi-level responses to the same magnitude, preparing for the down-streaming fusion operation. The adaptive weight fusion module performs following two computations. First, it dynamically generates location-specific fusion weights conditioned on the image content. Then, the location-specific fusion weights are applied to actively fuse the high-level and low-level response maps. The adaptive weight fusion module is capable of fully excavating the potentialities of multi-level responses, especially the low-level ones, to produce better fusion output for every single location.\n\nIn summary, our main contributions are:\n\n\u2022 For the first time, this work reveals limitations of the popular fixed weight fusion for SED, and explains why it does not produce satisfactory fusion results as expected. \u2022 We propose a dynamic feature fusion (DFF) model. To our best knowledge, it is the first work to learn adaptive fusion weights conditioned on input contents to merge multi-level features in the research field of SED. \u2022 The proposed DFF model achieves new state-of-the-art on the SED task.\n\n\nRelated Work\n\nThe task of category-aware semantic edge detection was first introduced by [Hariharan et al., 2011]. It is tackled usually as a multi-class problem [Hariharan et al., 2011;Bertasius et al., 2015;Bertasius et al., 2016;Maninis et al., 2016] at first, in which only one semantic class is associated with each located boundary pixel. From CASENet [Yu et al., 2017], researchers begin to address this task as a multi-label problem where each edge pixel can be associated with more than one semantic class simultaneously. Recently, deep learning based models, such as SEAL [Yu et al., 2018] and DDS [Liu et al., 2018], further lift the performance of semantic edge detection to new state-of-the-art. The tradition of employing fixed weight fusion can be pinpointed to HED [Xie and Tu, 2015], in which a weightedfusion layer (implemented by 1 \u00d7 1 convolution layer) is employed to merge the side-outputs. RCF [Liu et al., 2017] and RDS [Liu and Lew, 2016] follow this simple strategy to perform category-agnostic edge detection. CASENet [Yu et al., 2017], SEAL [Yu et al., 2018] and DDS [Liu et al., 2018] extend the approach with a K-grouped 1 \u00d7 1 convolution layer to generate the K-channel fused activation map. In this paper, we demonstrate the above fixed fusion strategy cannot sufficiently leverage multi-scale responses for producing better fused output. Instead, our proposed adaptive weight fusion module enables the network to actively learn the locationaware fusion weights conditioned on the individual input content.\n\nMulti-level representations within a convolutional neural network have been shown effective in many vision tasks, such as object detection [Liu et al., 2016;, semantic segmentation [Long et al., 2015;Yu and Koltun, 2015], boundary detection [Xie and Tu, 2015;. Responses from different stages of a network tend to vary significantly in magnitude. Scaling the multi-scale activations to a similar magnitude would benefit the following prediction or fusion consistently. For example, SSD [Liu et al., 2016] performs L2 normalization on low-level feature maps before making multi-scale predictions. [Li et al., 2018] adds a scaling layer for learning a fusion scale to combine the channel attention and spatial attention outputs. In this paper, we adopt a feature extractor with a normalizer for all multi-level activations to deal with the bias.\n\nOur proposed method is also related with the dynamic filter networks [Jia et al., 2016] in which sample-specific filter parameters are generated based on the input. The filters with a specific kernel size are dynamically produced to enable local spatial transformations on the input feature map, which serve as dynamic convolution kernels. Comparatively, in our method, the adaptive fusion weights are applied on the multilevel response maps to obtain a desired fused output and a feature extractor with a normalizer is proposed to alleviate the bias during training.\n\n\nFeature Fusion with Fixed Weights\n\nBefore expounding the proposed model, we first introduce the notations used in our model and revisit the fixed weight fusion in CASENet [Yu et al., 2017] as preliminaries.\n\nCASENet adopts ResNet-101 with some minor modifications to extract multi-level features. Based on the backbone, a 1 \u00d7 1 convolution layer and a following upsampling layer are connected to the output of the first three and the top stacks of residual blocks, producing three single channel feature maps {A side1 , A side2 , A side3 } and a K-channel class activation map A side5 = {A 1 side5 , ..., A K side5 } respectively. Here K is the number of categories. The shared concatenation replicates the bottom features {A side1 , A side2 , A side3 } for K times to separately concatenate with each of the K top activations in A side5 :\nA cat = {A 1 cat , ..., A K cat },(1)A i cat = {A i side5 , A side1 , A side2 , A side3 }, i \u2208 [1, K].\n(2) The resulting concatenated activation map A cat is then fed into the K-grounped 1 \u00d7 1 conv layer to produce the fused activation map with K channels:\nA i f use = w i 1 A i side5 +w i 2 A side1 +w i 3 A side2 +w i 4 A side3 (3) A f use = {A i f use }, i \u2208 [1, K](4)\nwhere  1) and (2)) is then applied to concatenate Side1-3 and Side5. The Side5-w feature normalization block followed by a location-adaptive weight learner form another branch extended from res5 to predict dynamic location-aware fusion weights \u03a8(x). Then element-wise multiplication and category-wise summation are applied to the location-aware fusion weights \u03a8(x) and the concatenated response map Acat to generate the final fused output A f use . The semantic loss is employed to supervise A side5 and the final fused output A f use . (b) Location-invariant weight learner and location-adaptive weight learner take as input the feature map x and output location-invariant fusion weights and location-adaptive fusion weights \u03a8(x) respectively.\n(w i 1 , w i 2 , w i 3 , w i\n\nCASENet imposes the same fixed set of fusion weights\n(w i 1 , w i 2 , w i 3 , w i 4 )\nof the ith category upon all the images across all the locations of the feature maps. By reproducing CASENet, we make following observations. i) The weight w i 1 always surpasses the other weights (w i 2 , w i 3 , w i 4 ) for each class significantly. ii) By evaluating the performance of A side5 and the fused output A f use , we find they give almost the same performance. This implies that the low-level feature responses {A side1 , A side2 , A side3 } contribute very little to the final fused output although they contain fine-grained edge location and structure information. The final decision is mainly determined by high-level features. As such, the CASENet cannot exploit low-level information sufficiently and produce only coarse edges.\n\n\nProposed Model\n\nTo address the above limitations of existing Semantic Edge Detection (SED) models in fusing features with fixed weights, we develop a new SED model with dynamic feature fusion. The proposed model fuses multi-level features through two modules: 1) the feature extractor with a normalizer is to normalize the magnitude scales of multi-level features and 2) the adaptive weight fusion module is to learn adaptive fusion weights for different locations of multi-level feature maps (see Figure 2 (a)).\n\n\nDynamic Feature Fusion\n\nThe first module is inspired by following observations. In CASENet [Yu et al., 2017], the edge responses from the top layers are much stronger than the ones from other three bottom outputs. Such variation in scales of activations biases the multi-level feature fusion to the responses from the top layers. Moreover, the top-layer output is much similar to the groundtruth for the training examples, due to the direct supervision. Applying the same weights to all the locations forces the net-work to learn much higher fusion weights for the top-level features, undesirably ignoring contributions from low-level features. This further inhibits the low-level features from providing fine edge information for detecting object boundaries in the cases of heavily biased distributions of edge and nonedge pixels.\n\nTherefore, before feature fusion, we first deal with the scale variation of multi-level responses by normalizing their magnitudes. In this way, the following adaptive weight learner can get rid of distractions from scale variation and learn effective fusion weights more easily.\n\nA feature extractor with a normalizer (see Figure 2 (a)) is employed to normalize multi-level responses to the similar magnitude. Concretely, side feature normalization blocks in the module are responsible for normalizing feature maps of the corresponding level.\n\nTo achieve our proposed dynamic feature fusion, we devise two different schemes for predicting the adaptive fusion weights, i.e., location-invariant and location-adaptive fusion weights (see Figure 2 (b)). The former treats all locations in the feature maps equally and universal fusion weights are learned according to specific input adaptively. The later one adjusts the fusion weights conditioned on location features of the image and lifts the contributions of low-level features to locating fine edges along object boundaries.\n\nConcretely, given the activation of the multi-level side outputs A side of size H \u00d7 W , we aim to obtain a fused output A f use = f (A side ) by aggregating multi-level responses. The fusion operation in CASENet in Eqn. (3) can be written as the following generalized form:\nA f use = f (A side ; W)(5)\nwhere f encapsulates operations in Eqn.\n\n(3) and W = (w i 1 , w i 2 , w i 3 , w i 4 ) denotes the fusion weights. Differently, we propose an adaptive weight learner to actively learn the fusion weights conditioned on the feature map itself, which is formulated as\nA f use = f (A side ; \u03a8(x)),(6)\nwhere x denotes the feature map. The above formulations characterize the essential difference between our proposed adaptive weight fusion method and the fixed weight fusion method. We enforce the fusion weights W to explicitly depend on the feature map x, i.e. W = \u03a8(x). Different input feature maps x would induce different parameters \u03a8(x) and thus lead to modifications to the adaptive weight learner f (.; .) dynamically. In this way, the semantic edge detection model can fast adapt to the input image and favourably learn proper multi-level response fusion weights in an end-to-end manner.\n\nRegarding \u03a8(x), corresponding to the two fusion weight schemes, there are two adaptive weight learners, namely location-invariant weight learner and locationadaptive weight learner. The location-invariant weight learner learns 4K fusion weights in total as shown in the following equation, which are shared by all locations of feature maps to be fused:\n\u03a8(x) = (w i 1 , w i 2 , w i 3 , w i 4 ), i \u2208 [1, K].(7)\nHowever, the location-adaptive weight learner generates 4K fusion weights for each spatial location, which results in 4KHW weighting parameters in total.\n\u03a8(x) = (w s,t ), s \u2208 [1, H], t \u2208 [1, W ] (8) w s,t = ((w i 1 ) s,t , (w i 2 ) s,t , (w i 3 ) s,t , (w i 4 ) s,t ), i \u2208 [1, K] (9)\nThe location-invariant weight learner generates universal fusion weights for all locations, while the location-adaptive weight learner tailors fusion weights for each location considering the spatial variety.\n\n\nNetwork Architecture\n\nOur proposed network architecture is based on ResNet [He et al., 2016] and adopts the same modifications as CASENet [Yu et al., 2017] to preserve low-level edge information, as shown in Figure 2. Side feature normalization blocks are connected to the first three and the fifth stack of residual blocks. This block consists of a 1 \u00d7 1 convolution layer, a Batch Normalization (BN) [Ioffe and Szegedy, 2015] layer and a deconvolution layer. The 1 \u00d7 1 convolution layer produces single and K channel response maps for Side1-3 and Side5 respectively. The BN layer is applied on the output of the 1 \u00d7 1 convolution layer to normalize the multi-level responses to the same magnitude. Deconvolution layers are then used to upsample the response maps to the original image size. Another side feature normalization block is connected to the fifth stack of the residual block, in which a 4K-channel feature map is produced. The adaptive weight learner then takes in the output of the Side5-w feature normalization block to predict the dynamic fusion weights w(x). We design two instantiations for location-invariant weight learner and location-adaptive weight learner respectively, as detailed below.\n\nLocation-invariant weight learner Location-invariant weight learner (see Figure 2 (b)) is a na\u00efve version of adaptive weight learner. The output of the Side5-w feature normalization block x with size H \u00d7 W \u00d7 4K is taken as the input of a global average pooling layer to produce a 4K-channel vector. After that, a block of alternatively connected FC, BN and ReLU layers is repeated twice and followed by FC and BN layers to generate the 4K location-invariant fusion weights for the fused response map A f use with size H \u00d7 W \u00d7 4K. These fusion weights are conditional on the input content but all locations share the same fusion parameters.\n\nLocation-adaptive weight learner We further propose the location-adaptive weight learner (see Figure 2 (b)) to solve the shortcomings of fixed weight fusion. The feature map x is forwarded to a block of alternatively connected 1 \u00d7 1 conv, BN and ReLU layers, repeated twice and followed by a 1 \u00d7 1 conv and BN layer, to generate location-aware fusion weights w(x) with size H \u00d7 W \u00d7 4K. Element-wise multiplication and category-wise summation are then applied to the location-aware fusion weights and the fused response map A f use to generate the final fused output. The locationadaptive weight learner predicts 4K fusion weights for each spatial location on Side1-3 and Side5 to allow low-level responses to provide fine edge locations for object boundaries. One may consider applying softmax activation function to the output of the adaptive weight leaner to learn fusion weights mutually-exclusive for the side feature maps to be fused respectively. The ablation experiments in Section 5.2 demonstrate that the activation function hampers the performance in the case of generating adaptive fusion weights.     et al., 2017;Liu et al., 2018]. The ground truth maps are downsampled into half size of original dimensions for Cityscapes, and are generated with instancesensitive edges for both datasets. Maximum F-measure (MF) at optimal dataset scale (ODS) with matching distance tolerance set as 0.02 is used for Cityscapes and SBD evaluation following our baseline methods. For fair comparison with [Yu et al., 2018], we also set the distance tolerance to 0.0035 for Cityscapes dataset.\n\n\nAblation Study\n\nWe first conduct ablation experiments on Cityscapes to investigate the effectiveness of each newly proposed module. We use CASENet as the baseline model and add our proposed normalizer and learner on it. All the ablation experiments use the same settings with 640 \u00d7 640 crop size and ResNet50 backbone. We report the reproduced baseline CASENet trained with the same training settings for fair comparison, which actually has higher accuracy than the original paper. Results are summarized in Table 1.\n\nNormalizer We first evaluate the effect of the proposed normalizer in the feature extractor. As shown in Table 1 (row  2), the normalizer provides 0.3% performance gain compared with the baseline which does not normalize the multi-level features to the same magnitude before fusion. It shows that normalized features are more suitable for multi-level information fusion than the unnormalized ones. This is probably 1 Codes will be released soon.\n\nbecause the unnormalized features have dramatically different magnitudes, making the learning process biased to the features with higher magnitude.\n\nLocation-invariant learner Row 3 in Table 1 shows the results of further adding the proposed location-invariant learner which adaptively fuses multi-level features conditioned on the input image. As shown in Table 1, the location-invariant learner provides another 0.4% performance gain compared with baseline+normalizer that assigns fixed fusion weights without considering the uniqueness of each input image. The performance boost successfully shows that different input images with different illumination, semantics, etc., may require different fusion weights and applying universal weights to all training images will lead to less satisfactory results.\n\nLocation-adaptive learner The location-adaptive learner provides location-aware fusion weights that are conditioned on not only the input image but also the local context, which further improves the location-invariant learner. As shown in Table 1 row 4 to row 5, location-adaptive learner improves the performance significantly from 79.1% to 80.4%, indicating a fusion module should consider both the uniqueness of each input image and the local context at each location. The proposed fusion module in DFF successfully captures both of them and thus delivers remarkable performance gain. Besides, it can be seen that the location-adaptive learner heavily relies on pre-normalized multi-level features, since the performance degrades by 1.5% when the proposed normalizer is removed from the framework, as shown in row 5 and row 6. This further justifies the effectiveness of the proposed normalizer for rescaling the activation values to the same magnitude before conducting fusion.\n\nActivation function Row 4 and row 5 in Table 1 show the cases of further constraining the generated fusion weights for better performance. In particular, we append a softmax layer to the end of location-adaptive weight learner (Figure 2 (b)) to force the learned fusion weights for each category to add up to 1, and compare its results with the unconstrained version. However, we observe performance drop of 0.3% with softmax activation function adopted. Our intuitive explanation is that applying softmax activation function to the fusion weights normalizes their values to the range [0, 1], which may actually be negative for some response maps or some locations. Furthermore, adopting softmax activation function enforces the fusion weights of multi-level response maps for each category to be mutually exclusive, which makes the   response maps compete with each other to be more important. However, it is desirable that each response map be given enough freedom to be active and be useful to the final fused output. Therefore, we do not use any activation functions to constrain the fusion weights in the proposed DFF model.\n\n\nDeeper backbone network\n\nWe also perform experiments on deeper backbones to verify our model's generalization. As shown in \n\n\nComparison with State-of-the-arts\n\nWe compare the performance of the proposed DFF model with state-of-the-art semantic edge detection methods [Yu et al., 2017;Yu et al., 2018;Liu et al., 2018] on Cityscapes dataset and SBD dataset.\n\n\nResults on Cityscapes\n\nWe compare the proposed DFF model with CASENet and DDS, and evaluate MF (ODS) with matching distance tolerance set as 0.02. In addition, the matching distance tolerance is decreased to 0.0035 to make fair comparison with SEAL. As can be seen in Table 3, our DFF model outperforms all these well established baselines and achieves new state-ofthe-art 80.7% MF (ODS). The DFF model is superior to other models for most classes. Specifically, the MF (ODS) of the proposed DFF model is 9.4% higher than CASENet and 2.7% higher than DDS on average. It is also worth noting that DFF achieves 5% higher than SEAL under stricter matching distance tolerance (0.0035), which reflects the DFF model can locate more accurate edges by taking full advantage of multilevel response maps, especially the low-level ones, with dynamic feature fusion. We also visualize some prediction results for qualitative comparison shown in Figure 3. Through comparing with CASENet and SEAL, we can observe DFF can predict more accurate and clearer edges on object boundaries while suppressing the fragmentary and trivial edge responses inside the objects.\n\n\nResults on SBD\n\nCompared with Cityscapes, SBD dataset has fewer objects and especially fewer overlapping ones in each image. In addition, it contains many misaligned annotations. We compare the proposed DFF model with state-of-the-arts on SBD validation set in Table 4. The proposed DFF model outperforms all the baselines and achieves new state-of-the-art 75.4% MF (ODS), which well confirms its superiority.\n\n\nConclusion\n\nIn this paper, we propose a novel dynamic feature fusion (DFF) model to exploit multi-level responses for producing a desired fused output. The newly introduced normalizer scales multi-level side activations to the same magnitude preparing for the down-streaming fusion operation. Location-adaptive weight learner is proposed to actively learn customized fusion weights conditioned on the individual sample content for each spatial location. Comprehensive experiments on Cityscapes and SBD demonstrate the proposed DFF model can improve the performance by locating more accurate edges on object boundaries as well as suppressing trivial edge responses inside the objects. In the future, we will continue to improve the location-adaptive weight learner by considering both the high-level and low-level feature maps from the backbone.\n\nFigure 1 :\n1Illustration of fixed weight fusion and proposed dynamic feature fusion (DFF). The proposed model actively learns customized fusion weights for each location, to fuse low-level features (A side1 \u223c A side3 ) and a high-level feature (A side5 ). In this way, sharper boundaries are produced through dynamic feature fusion compared with CASENet (fixed weight fusion model).\n\nFigure 2 :\n2Overall architecture of our model. (a) The input image is fed into the ResNet backbone to generate a set of features (response maps) with different scales. Side feature normalization blocks are connected to the first three and the fifth stack of residual blocks to produce Side1-3 and Side5 response maps with the same response magnitude. Shared concatenation (Eqn. (\n\nFigure 3 :\n3Qualitative comparison on Cityscapes among ground truth, CASENet, SEAL and DFF (ordering from left to right in the figure). Best viewed in color.\n\nTable 1 :\n1The effectiveness of each component in our proposed model on Cityscapes dataset. The mean value of MF scores over all categories is measured by %.5 Experiments \n\n5.1 Experimental Setup \n\nDatasets and augmentations We evaluate our proposed \nmethod on two popular benchmarks for semantic edge de-\ntection: Cityscapes [Cordts et al., 2016] and SBD [Hariha-\nran et al., 2011]. During training, we use random mirror-\ning and random cropping on both Cityscapes and SBD. For \nCityscapes, we also augment each training sample with ran-\ndom scaling in [0.75, 2], and the base size for random scaling \nis set as 640. For SBD, we augment the training data by resiz-\ning each image with scaling factors {0.5, 0.75, 1, 1.25, 1.5} \nfollowing [Yu et al., 2017]. During testing, the images with \noriginal image size are used for Cityscapes and the images \nare padded to 512 \u00d7 512 for SBD. \n\n\n\nTable 2 :\n2Results of the proposed DFF using different backbone networks and crop sizes. All MF scores are measured by %Method \nroad side. buil. wall fence pole light sign vege. terrain sky person rider car truck bus train mot. bike mean \n\nCASENet \n86.6 78.8 85.1 51.5 58.9 70.1 70.8 74.6 83.5 \n62.9 \n79.4 \n81.5 \n71.3 86.9 50.4 69.5 52.0 61.3 80.2 \n71.3 \nDDS-R \n90.5 84.2 86.2 57.7 61.4 85.1 83.8 80.4 88.5 \n67.6 \n88.2 \n89.9 \n80.1 91.8 58.6 76.3 56.2 68.8 87.3 \n78.0 \nDFF \n93.2 86.3 89.7 60.0 64.3 86.0 85.2 83.7 92.0 \n71.3 \n92.3 \n91.1 \n83.4 94.8 64.5 77.4 59.9 72.1 86.9 \n80.7 \n\nSEAL (0.0035) 87.6 77.5 75.9 47.6 46.3 75.5 71.2 75.4 80.9 \n60.1 \n87.4 \n81.5 \n68.9 88.9 50.2 67.8 44.1 52.7 73.0 \n69.1 \nDFF (0.0035) \n89.4 80.1 79.6 51.3 54.5 81.3 81.3 81.2 83.6 \n62.9 \n89.0 \n85.4 \n75.8 91.6 54.9 73.9 51.9 64.3 76.4 \n74.1 \n\n\n\nTable 3 :\n3Comparison with state-of-the-arts on Cityscapes dataset. All MF scores are measured by %Training strategies The proposed network is built on \nResNet [He et al., 2016] pretrained on ImageNet [Deng et \nal., 2009]. The network is trained with standard reweighted \ncross-entropy loss [Yu et al., 2017] and optimized by SGD us-\ning PyTorch [Paszke et al., 2017] 1 . We set the base learning \nrate to 0.08 and 0.05 for Cityscapes and SBD respectively; \nthe \"poly\" policy is used for learning rate decay. The crop \nsize, batch size, training epoch, momentum, weight decay are \nset to 640 \u00d7 640 / 352 \u00d7 352, 8 / 16, 200 / 10, 0.9, 1e \u2212 4 re-\nspectively for Cityscapes and SBD. All experiments are per-\nformed using 4 NVIDIA TITAN Xp(12GB) GPUs with syn-\nchronized batch normalization [Zhang et al., 2018]. We use \nthe same data augmentation and hyper-parameters for repro-\nducing CASENet and training the proposed model for fair \ncomparison. \nEvaluation protocol We follow [Yu et al., 2018] to evalu-\nate the performance with stricter rules than the benchmark \nused in [Yu \n\nTable 4 :\n4Comparison with state-of-the-arts on SBD dataset. All MF scores are measured by %road \nsidewalk \nbuilding \nwall \nfence \npole \ntraffic lgt \ntraffic sgn \nvegetation \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotocycle \nbike \n\nimage \nground truth \nCASENet \nSEAL \nDFF \n\n\n\nTable 2 ,\n2we evaluate our model performance on ResNet101 with 512 \u00d7 512 crop size. Note that we do not use 640 \u00d7 640, simply because of the GPU memory limitation. In order to make a fair comparison, we also train a DFF on ResNet50 with 512 \u00d7 512 crop size. The result shows DFF also generalizes well on deep ResNet101 and outperforms its counterpart (ResNet50, 512 \u00d7 512) by 1.5%. Higher performance might be achieved if we can train the ResNet101 with 640 \u00d7 640 crop size using GPUs with larger memory, e.g. Nvidia V100 (32GB).\n) are the parameters of the Kgrounped 1 \u00d7 1 conv layer, serving as fusion weights for the ith category. We omit the bias term in Eqn. (3) for simplicity. One can refer to[Yu et al., 2017] for more details.\n\nHigh-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision. Bertasius, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionReferences [Bertasius et al., 2015] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. High-for-low and low-for-high: Ef- ficient boundary detection from deep object features and its applications to high-level vision. In Proceedings of the IEEE International Conference on Computer Vision, pages 504-512, 2015.\n\nSemantic segmentation with boundary neural fields. Bertasius, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition[Bertasius et al., 2016] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with bound- ary neural fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3602- 3610, 2016.\n\nThe cityscapes dataset for semantic urban scene understanding. Cordts, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition[Cordts et al., 2016] Marius Cordts, Mohamed Omran, Se- bastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene under- standing. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pages 3213-3223, 2016.\n\nImageNet: A Large-Scale Hierarchical Image Database. [ Deng, CVPR09. [Deng et al., 2009] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierar- chical Image Database. In CVPR09, 2009.\n\nDeep residual learning for image recognition. [ Hariharan, arXiv:1502.03167Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprintProceedings of the IEEE conference on computer vision and pattern recognition[Hariharan et al., 2011] Bharath Hariharan, Pablo Arbel\u00e1ez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. 2011. [He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [Ioffe and Szegedy, 2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep net- work training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\nDynamic filter networks. [ Jia, Advances in Neural Information Processing Systems. [Jia et al., 2016] Xu Jia, Bert De Brabandere, Tinne Tuyte- laars, and Luc V Gool. Dynamic filter networks. In Ad- vances in Neural Information Processing Systems, pages 667-675, 2016.\n\nHarmonious attention network for person re-identification. [ Li, CVPR. 1[Li et al., 2018] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious attention network for person re-identification. In CVPR, volume 1, page 2, 2018.\n\nBharath Hariharan, and Serge J Belongie. Feature pyramid networks for object detection. Lin, CVPR. 14[Lin et al., 2017] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross B Gir- shick, Kaiming He, Bharath Hariharan, and Serge J Be- longie. Feature pyramid networks for object detection. In CVPR, volume 1, page 4, 2017.\n\nLearning relaxed deep supervision for better edge detection. Lew ; Yu Liu, Michael S Liu, Lew, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition[Liu and Lew, 2016] Yu Liu and Michael S Lew. Learning relaxed deep supervision for better edge detection. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 231-240, 2016.\n\nSsd: Single shot multibox detector. [ Liu, European conference on computer vision. Springer[Liu et al., 2016] Wei Liu, Dragomir Anguelov, Dumitru Er- han, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21-37. Springer, 2016.\n\nRicher convolutional features for edge detection. [ Liu, Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE[Liu et al., 2017] Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, and Xiang Bai. Richer convolutional features for edge detection. In Computer Vision and Pattern Recog- nition (CVPR), 2017 IEEE Conference on, pages 5872- 5881. IEEE, 2017.\n\nFully convolutional networks for semantic segmentation. arXiv:1804.02864arXiv:1808.01992Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Honolulu, HI, USAYu and KoltunarXiv preprintThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR)et al., 2018] Yun Liu, Ming-Ming Cheng, JiaWang Bian, Le Zhang, Peng-Tao Jiang, and Yang Cao. Seman- tic edge detection with diverse deep supervision. arXiv preprint arXiv:1804.02864, 2018. [Long et al., 2015] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for seman- tic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431- 3440, 2015. [Maninis et al., 2016] Kevis-Kokitsi Maninis, Jordi Pont- Tuset, Pablo Arbel\u00e1ez, and Luc Van Gool. Convolutional oriented boundaries. In European Conference on Com- puter Vision, pages 580-596. Springer, 2016. [Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary De- Vito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [Xie and Tu, 2015] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE international conference on computer vision, pages 1395-1403, 2015. [Yu and Koltun, 2015] Fisher Yu and Vladlen Koltun. Multi- scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015. [Yu et al., 2017] Zhiding Yu, Chen Feng, Ming-Yu Liu, and Srikumar Ramalingam. Casenet: Deep category-aware se- mantic edge detection. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pages 21-26, 2017. [Yu et al., 2018] Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, BVK Vijaya Kumar, and Jan Kautz. Simultaneous edge alignment and learning. arXiv preprint arXiv:1808.01992, 2018. [Zhang et al., 2018] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic seg- mentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n", "annotations": {"author": "[{\"end\":194,\"start\":54},{\"end\":265,\"start\":195},{\"end\":428,\"start\":266},{\"end\":476,\"start\":429}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":59},{\"end\":207,\"start\":203},{\"end\":274,\"start\":272},{\"end\":440,\"start\":436}]", "author_first_name": "[{\"end\":58,\"start\":54},{\"end\":202,\"start\":195},{\"end\":271,\"start\":266},{\"end\":435,\"start\":429}]", "author_affiliation": "[{\"end\":131,\"start\":63},{\"end\":193,\"start\":133},{\"end\":264,\"start\":231},{\"end\":365,\"start\":297},{\"end\":427,\"start\":367},{\"end\":475,\"start\":442}]", "title": "[{\"end\":51,\"start\":1},{\"end\":527,\"start\":477}]", "venue": null, "abstract": "[{\"end\":1792,\"start\":529}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2452,\"start\":2435},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2469,\"start\":2452},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2485,\"start\":2469},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6102,\"start\":6078},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6175,\"start\":6151},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6198,\"start\":6175},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6221,\"start\":6198},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6242,\"start\":6221},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6364,\"start\":6347},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6588,\"start\":6571},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6615,\"start\":6597},{\"end\":6788,\"start\":6766},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6924,\"start\":6906},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6952,\"start\":6933},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7051,\"start\":7034},{\"end\":7075,\"start\":7053},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7102,\"start\":7084},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7686,\"start\":7668},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7729,\"start\":7710},{\"end\":7749,\"start\":7729},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7788,\"start\":7770},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8033,\"start\":8015},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8142,\"start\":8125},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8461,\"start\":8443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9132,\"start\":9115},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12389,\"start\":12372},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16444,\"start\":16427},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19270,\"start\":19257},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19287,\"start\":19270},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19662,\"start\":19645},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23907,\"start\":23890},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23923,\"start\":23907},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23940,\"start\":23923},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31112,\"start\":31095}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26774,\"start\":26391},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27155,\"start\":26775},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27314,\"start\":27156},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28202,\"start\":27315},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29025,\"start\":28203},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30103,\"start\":29026},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30393,\"start\":30104},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30924,\"start\":30394}]", "paragraph": "[{\"end\":2400,\"start\":1808},{\"end\":3523,\"start\":2402},{\"end\":5480,\"start\":3525},{\"end\":5521,\"start\":5482},{\"end\":5986,\"start\":5523},{\"end\":7527,\"start\":6003},{\"end\":8372,\"start\":7529},{\"end\":8941,\"start\":8374},{\"end\":9150,\"start\":8979},{\"end\":9783,\"start\":9152},{\"end\":10040,\"start\":9887},{\"end\":10900,\"start\":10156},{\"end\":11763,\"start\":11017},{\"end\":12278,\"start\":11782},{\"end\":13112,\"start\":12305},{\"end\":13392,\"start\":13114},{\"end\":13656,\"start\":13394},{\"end\":14189,\"start\":13658},{\"end\":14464,\"start\":14191},{\"end\":14532,\"start\":14493},{\"end\":14756,\"start\":14534},{\"end\":15383,\"start\":14789},{\"end\":15737,\"start\":15385},{\"end\":15947,\"start\":15794},{\"end\":16286,\"start\":16078},{\"end\":17501,\"start\":16311},{\"end\":18142,\"start\":17503},{\"end\":19732,\"start\":18144},{\"end\":20251,\"start\":19751},{\"end\":20698,\"start\":20253},{\"end\":20847,\"start\":20700},{\"end\":21505,\"start\":20849},{\"end\":22488,\"start\":21507},{\"end\":23619,\"start\":22490},{\"end\":23745,\"start\":23647},{\"end\":23979,\"start\":23783},{\"end\":25131,\"start\":24005},{\"end\":25543,\"start\":25150},{\"end\":26390,\"start\":25558}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9821,\"start\":9784},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9886,\"start\":9821},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10155,\"start\":10041},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10929,\"start\":10901},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11016,\"start\":10984},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14492,\"start\":14465},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14788,\"start\":14757},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15793,\"start\":15738},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16077,\"start\":15948}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20250,\"start\":20243},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20374,\"start\":20358},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20892,\"start\":20885},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21064,\"start\":21057},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21753,\"start\":21746},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22536,\"start\":22529},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24257,\"start\":24250},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25402,\"start\":25395}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1806,\"start\":1794},{\"attributes\":{\"n\":\"2\"},\"end\":6001,\"start\":5989},{\"attributes\":{\"n\":\"3\"},\"end\":8977,\"start\":8944},{\"end\":10983,\"start\":10931},{\"attributes\":{\"n\":\"4\"},\"end\":11780,\"start\":11766},{\"attributes\":{\"n\":\"4.1\"},\"end\":12303,\"start\":12281},{\"attributes\":{\"n\":\"4.2\"},\"end\":16309,\"start\":16289},{\"attributes\":{\"n\":\"5.2\"},\"end\":19749,\"start\":19735},{\"end\":23645,\"start\":23622},{\"attributes\":{\"n\":\"5.3\"},\"end\":23781,\"start\":23748},{\"end\":24003,\"start\":23982},{\"end\":25148,\"start\":25134},{\"attributes\":{\"n\":\"6\"},\"end\":25556,\"start\":25546},{\"end\":26402,\"start\":26392},{\"end\":26786,\"start\":26776},{\"end\":27167,\"start\":27157},{\"end\":27325,\"start\":27316},{\"end\":28213,\"start\":28204},{\"end\":29036,\"start\":29027},{\"end\":30114,\"start\":30105},{\"end\":30404,\"start\":30395}]", "table": "[{\"end\":28202,\"start\":27473},{\"end\":29025,\"start\":28324},{\"end\":30103,\"start\":29126},{\"end\":30393,\"start\":30197}]", "figure_caption": "[{\"end\":26774,\"start\":26404},{\"end\":27155,\"start\":26788},{\"end\":27314,\"start\":27169},{\"end\":27473,\"start\":27327},{\"end\":28324,\"start\":28215},{\"end\":29126,\"start\":29038},{\"end\":30197,\"start\":30116},{\"end\":30924,\"start\":30406}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2399,\"start\":2391},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3784,\"start\":3776},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4007,\"start\":3999},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12272,\"start\":12264},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13445,\"start\":13437},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13857,\"start\":13849},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16505,\"start\":16497},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17584,\"start\":17576},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18246,\"start\":18238},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22730,\"start\":22717},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24924,\"start\":24916}]", "bib_author_first_name": "[{\"end\":32750,\"start\":32749},{\"end\":32967,\"start\":32966},{\"end\":33825,\"start\":33824},{\"end\":34128,\"start\":34127},{\"end\":34663,\"start\":34655},{\"end\":34678,\"start\":34669},{\"end\":35080,\"start\":35079},{\"end\":35426,\"start\":35425}]", "bib_author_last_name": "[{\"end\":31270,\"start\":31261},{\"end\":31766,\"start\":31757},{\"end\":32215,\"start\":32209},{\"end\":32755,\"start\":32751},{\"end\":32977,\"start\":32968},{\"end\":33829,\"start\":33826},{\"end\":34131,\"start\":34129},{\"end\":34382,\"start\":34379},{\"end\":34667,\"start\":34664},{\"end\":34682,\"start\":34679},{\"end\":34687,\"start\":34684},{\"end\":35084,\"start\":35081},{\"end\":35430,\"start\":35427}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14001254},\"end\":31704,\"start\":31132},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1773751},\"end\":32144,\"start\":31706},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":502946},\"end\":32694,\"start\":32146},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57246310},\"end\":32918,\"start\":32696},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b4\",\"matched_paper_id\":206594692},\"end\":33797,\"start\":32920},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2097418},\"end\":34066,\"start\":33799},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3458516},\"end\":34289,\"start\":34068},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10716717},\"end\":34592,\"start\":34291},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":31753935},\"end\":35041,\"start\":34594},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2141740},\"end\":35373,\"start\":35043},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12452972},\"end\":35748,\"start\":35375},{\"attributes\":{\"doi\":\"arXiv:1804.02864\",\"id\":\"b11\",\"matched_paper_id\":1629541},\"end\":38012,\"start\":35750}]", "bib_title": "[{\"end\":31259,\"start\":31132},{\"end\":31755,\"start\":31706},{\"end\":32207,\"start\":32146},{\"end\":32747,\"start\":32696},{\"end\":32964,\"start\":32920},{\"end\":33822,\"start\":33799},{\"end\":34125,\"start\":34068},{\"end\":34377,\"start\":34291},{\"end\":34653,\"start\":34594},{\"end\":35077,\"start\":35043},{\"end\":35423,\"start\":35375},{\"end\":35804,\"start\":35750}]", "bib_author": "[{\"end\":31272,\"start\":31261},{\"end\":31768,\"start\":31757},{\"end\":32217,\"start\":32209},{\"end\":32757,\"start\":32749},{\"end\":32979,\"start\":32966},{\"end\":33831,\"start\":33824},{\"end\":34133,\"start\":34127},{\"end\":34384,\"start\":34379},{\"end\":34669,\"start\":34655},{\"end\":34684,\"start\":34669},{\"end\":34689,\"start\":34684},{\"end\":35086,\"start\":35079},{\"end\":35432,\"start\":35425}]", "bib_venue": "[{\"end\":31393,\"start\":31341},{\"end\":31909,\"start\":31847},{\"end\":32358,\"start\":32296},{\"end\":34830,\"start\":34768},{\"end\":36020,\"start\":35929},{\"end\":31339,\"start\":31272},{\"end\":31845,\"start\":31768},{\"end\":32294,\"start\":32217},{\"end\":32763,\"start\":32757},{\"end\":33123,\"start\":32995},{\"end\":33880,\"start\":33831},{\"end\":34137,\"start\":34133},{\"end\":34388,\"start\":34384},{\"end\":34766,\"start\":34689},{\"end\":35124,\"start\":35086},{\"end\":35503,\"start\":35432},{\"end\":35927,\"start\":35838}]"}}}, "year": 2023, "month": 12, "day": 17}