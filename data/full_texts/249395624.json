{"id": 249395624, "updated": "2023-11-01 00:21:07.005", "metadata": {"title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers", "authors": "[{\"first\":\"Zhewei\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Reza\",\"last\":\"Aminabadi\",\"middle\":[\"Yazdani\"]},{\"first\":\"Minjia\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiaoxia\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Conglong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yuxiong\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.01861", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/YaoAZWLH22", "doi": "10.48550/arxiv.2206.01861"}}, "content": {"source": {"pdf_hash": "e03609f2587f690867e7ea0bedaf0db25282c548", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2206.01861v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e7bbf0dfd6c503d8c76b7afc69520eea8cec6b44", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e03609f2587f690867e7ea0bedaf0db25282c548.txt", "contents": "\nZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers\n\n\nZhewei Yao zheweiyao@microsoft.com \nReza Yazdani Aminabadi \nMinjia Zhang minjiaz@microsoft.com \nXiaoxia Wu xiaoxiawu@microsoft.com \nConglong Li conglong.li@microsoft.com \nYuxiong He Microsoft \nZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers\n\nHow to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT-3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20B, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency. * Code will be released soon as a part of https://github.com/microsoft/DeepSpeed\n\nIntroduction\n\nLarge-scale natural language models have been widely adopted in different applications, e.g., natural language understanding using BERT [63] and generation tasks using GPT-style models [48]. Although those models have achieved cutting-edge accuracy results, as the model size keeps increasing dramatically, the requirements of memory footprint and the computational cost to deploy them become a major bottleneck, even on cloud servers with powerful GPU devices.\n\nOne promising way to alleviate this challenge is quantization, which can reduce the bit precision for both weight and activations for lower memory footprint and faster compute (e.g., INT8 Tensor cores on T4/A100). However, quantization usually requires retraining (also known as quantization aware training, or QAT in short) to recover the accuracy degradation from representation loss of weight and activations. To enable QAT, the full training pipeline is usually required, including the training data and compute resources, to finetune the model. Access to those components is now oftentimes not available, and QAT is also a time-consuming process, particularly for those large-scale models.\n\nRecently, zero-shot quantization [9,46] and post-training quantization (PTQ) [45,38] are proposed to address the training-data access and compute requirement challenges since PTQ generally requires no (or minimal) retraining. But most of those works primarily focus on computer vision problems on relatively small scales. More recently, [6] shows promising PTQ results on BERT. However, (1) its main focus is on high-precision quantization (INT8/FP16) on BERT base , (2) it does not consider other billion-scale generative models (GPT-3-style models [8]). More importantly, most of these works do not report real latency improvement, putting the usefulness of these methods in improving inference latency into question. For example, existing work often do not discuss the quantization/dequantization cost associated with different quantization schemes, which in fact has a big impact to the performance benefit of using low precision.\n\nBesides, for extreme quantization (e.g., INT4), knowledge distillation is usually used to boost performance, which adds another source of expensive computation cost as compared to QAT. Furthermore, in order to achieve better accuracy performance, hidden-states knowledge distillation, e.g., [2,79], is usually applied for the quantized model. This would put significant pressure on the GPU memory and the compute resource requirement since both the teacher and student models needed to be loaded into the GPU memory for training.\n\nIn this paper, we present ZeroQuant, an end-to-end post-training quantization and inference pipeline, to address those challenges, targeting both INT8 and INT4/INT8 mixed-precision quantization. Specifically, our contributions are: \u2022 We apply fine-grained hardware-friendly quantization schemes on both weight and activations, i.e., groupwise quantization for weight and token-wise quantization for activations. Both quantization schemes can significantly reduce the quantization error and retain hardware acceleration properties. \u2022 We propose a novel layer-by-layer knowledge distillation method (LKD) for INT4/INT8 mixed-precision quantization, where the neural network is quantized layer-by-layer through distillation with minimal iterations and even without the access to the original training data. As such, at any given moment, the device memory is primarily populated only with a single extra layer's footprint, making billion-scale model distillation feasible with limited training budget and GPU devices. \u2022 We develop a highly optimized inference backend, which eliminates the expensive computation cost of quantization/dequantization operators, enabling latency speedups on INT8 Tensor cores on modern GPU hardware. \u2022 Our empirical results show that:\n\n-ZeroQuant enables quantizing BERT and GPT-3-style models into INT8 weight and activations to retain accuracy without incurring any retraining cost. Compared to FP16 inference, our INT8 model achieves up to 5.19x/4.16x speedup on BERT base /GPT-3 350M on A100 GPUs. -ZeroQuant plus LKD can do INT4/INT8 mixed-precision quantization for BERT and GPT-3-style models. This results in a 3x memory footprint reduction with marginal accuracy loss as compared to the FP16 model. Also, thanks to the lightweight of LKD, we can finish the quantization process in 33s (10 minutes) for BERT base (BERT large ). We also demonstrate that LKD can use other datasets to achieve similar performance to the original training data. -We demonstrate the scalability of ZeroQuant on two of the largest open-sourced language models, i.e, GPT-J 6B and GPT-NeoX 20B , with INT8 quantization. ZeroQuant can achieve 3.67x speedup over the FP16 model for GPT-J 6B and (2) reduce the GPU requirement for inference from 2 to 1 and latency from 65ms to 25ms for GPT-NeoX 20B (i.e., 5.2x better system efficiency in total).\n\n\nRelated Work\n\nModel compression has been explored from different aspects [25,37,39,34,43,20,24,50,18,74,40,26,55,59,28,60,68,33,14,38,31]. Among those, quantization is one of the most promising directions as it directly reduces the memory footprint and compute intensity. Here, we focus on quantization for NLP models and briefly discuss the related work. The majority of quantization works can be categorized into quantization-aware training (QAT). [56,76] are the first few works to quantize BERT models using integer numbers for both weight and activations. Particularly, [56] utilizes Hessian information to push the weight bit-precision to even INT2/INT4, and it also proposes group-wise quantization to quantize the weight matrix in a more fine-grained granularity compared to single matrix quantization. [21] introduces quantization noise to alleviate the variations of QAT. [79,2] leverage very expensive knowledge distillation [26] and data augmentation [28] to ternarize/binarize weights. [29] combines knowledge distillation [28] and learned step size quantization [19] to quantize the weight to 2-8 bits. Recently, [61] also uses knowledge distillation to compress GPT-2 models on task-specific problems to INT2. All those works quantize models using the original training datasets. More importantly they need retraining or finetuning the full model to recover the accuracy, and such compute cost on extra-large models, like [57,11], can be hardly affordable for most research labs or practitioners.\n\nOne solution to overcome the compute cost challenge is post-training quantization (PTQ). However, PTQ often induces a significant drop in accuracy because the network can be sensitive to quantization errors. Along this line, one of the first works applied to Transformer-based [64] models is [75]. The authors introduce centroid-based quantization method, where outlier numbers use FP32 format and the rest numbers are quantized using non-uniform quantization. As such, it is hard to get the real inference latency benefit on general compute accelerators, e.g., CPU and GPU, because the parallel processing units in these hardware do not support efficient computation of mixed data types. More recently, [6] introduces high-precision activation quantization (FP16) for part of the model to overcome the high dynamic activation ranges. However, to the best of our knowledge, (1) How to apply PTQ on GPT-3-style models while achieving high accuracy has not been studied in any of previous work yet; (2) How to apply PTQ on billion (or even a dozen of billions) scale model is still under-explored; (3) Efficient inference system backend is still missing, especially for fine-grained quantization schemes, making it hard to achieve low latency on commodity hardware. ZeroQuant resolves all those limitations by considering the system backend into the algorithm design and we verify its capability on both BERT and large-scale GPT-3-style (up to 20 billion, i.e., GPT-NeoX 20B ) models for various tasks.\n\n\nBackground and Challenge\n\nWe give a brief overview of the transformer architecture and quantization background in Appendix A. Please refer to [64] and [23] for more details about the transformer architecture and quantization.\n\nPost-training quantization (PTQ) exhibits great compression efficiency compared to quantization-aware training (QAT) since PTQ is usually applied to quantize the model without retraining. A common strategy of PTQ is to feed the training data to the network and calibrate the scaling factor, S, using the running mean. Please see Appendix B.1 for more details.\n\nSome work has been done for BERT base models [6] with INT8 weight and mixed INT8/FP16 activation quantization. However, there is no investigation for (1) even lower bit-precision PTQ on BERT models and (2) large-scale GPT-3-style models. Here, we briefly discuss the challenge of the application of PTQ on both BERT (in Appendix C) and GPT-3-style models.  The results of GPT-3 350M with PTQ are shown in Table 1. As can be seen, the INT8 activation quantization (i.e., the row of W16A8) causes the primary accuracy loss. Further pushing the weight to INT8 (i.e., the row of W8A8) does not change the accuracy of zero-shot evaluation tasks but leads the causal language modeling task (Wikitext-2) to worse perplexity score, which demonstrates the sensitivity of generation tasks as compared to other zero-shot evaluation problems. For W4/8A16, on some accuracy-based tasks, GPT-3 350M still achieves reasonable performance like OpenBookQA but it loses accuracy on the majority of the rest tasks. Particularly, for Wikitext-2, GPT-3 350M with W4/8A16 cannot generate any meaningful text anymore. Please also see Appendix C for the analysis for BERT. Dynamic Activation Range To investigate why INT8 activation leads to significant accuracy drop for both BERT and GPT-3-style models, we plot the token-wise (i.e., the hidden state of each token) range of each activation for different transformer layers of GPT-3 350M in Figure 1 (left). As can be seen, different tokens have dramatically different activation ranges. For example, the maximum range of the last layer is around 35 but the minimum range is close to 8. This larger variance in the activation range makes it difficult to use a fixed quantization range (usually the maximum value) for all tokens to retain the prediction accuracy, because the limited representation power for small range tokens is going to hurt the accuracy performance.\n\n\nDifferent Ranges of Neurons in Weight Matrices\n\nSimilarly, we plot the row-wise (i.e., the output dimension) weight range of the attention output matrix (W o ) of GPT-3 350M in Figure 1 (right). There is a 10x difference between the largest magnitudes of different rows and this leads to the worse generation performance of the INT8 weight PTQ. This also makes it very challenging when INT4 quantization is applied as the INT4 only has 16 numbers and a 10x smaller range leads to 2 (or 3) numbers for the representations of those smaller-range rows. This analysis results also indicate why more expensive hidden-states knowledge distillation [2,36] is used for ultra-low precision quantization to close the accuracy gap. However, as the training cost of knowledge distillation for large-scale models is too high, a lightweight and efficient method is desirable for PTQ.\n\n\nMethodology\n\n\nFine-grained Hardware-friendly Quantization Scheme\n\nAs shown in Section 3, even applying INT8 PTQ to BERT/GPT-3-style models leads to significant accuracy degradation. The key challenge is the representation of INT8 cannot fully capture the different numerical ranges of different rows in weight matrices and different activation tokens. One way to address this is to use group-wise (token-wise) quantization for the weight matrix (activations). Group-wise Quantization for Weights Group-wise weight matrix quantization has first been proposed in [56], where a weight matrix W \u2208 R n\u00d7m is partitioned in to g groups, and each group is quantized separately. However, in [56], the authors only apply this for quantization aware training. More importantly, they do not consider the hardware efficiency constraint and they do not have a system backend support. As such, they lack the real latency reduction benefit.\n\nIn our design, we consider the hardware constraint from Ampere Architecture of GPUs (e.g, A100), where the compute unit is based on Warp Matrix Multiply and Accumulate (WMMA) tiling size [53] to achieve the best speedup. Later, we will show that our group-wise quantization leads to much better accuracy as compared to single-matrix quantization due to its finer-granularity quantization while still achieving great latency reduction. Token-wise Quantization for Activations As mentioned in Section 3 and Appendix A.2, a common practice for existing PTQ work is to use static quantization for activation, where the min/max range is calculated at an offline calibration phase. Such a method might be sufficient for small scale models where the variance in the activation range is small. However, as analyzed in Section 3, there is a huge variance in the activation range for large-scale transformer models such as GPT-3 350M and BERT base . As such, a static quantization scheme (often applied to all tokens/samples) would lead to significant accuracy drop. One natural idea to overcome this issue is to adopt finer-grained token-wise quantization and dynamically calculate the min/max range for each token to reduce the quantization error from activations. Our evaluation in Section 5 also shows that token-wise quantization for activation significantly improves the accuracy of GPT-3-style and BERT models.\n\nHowever, directly applying token-wise quantization using existing DL frameworks, such as the PyTorch quantization suite, would lead to significant quantization and dequantization cost because token-wise quantization introduces additional operations that lead to expensive data movement overhead between the GPU compute units and the main memory. To address this issue, we build a highly optimized inference backend for token-wise quantization of transformer models. For example, the inference backend of ZeroQuant employs so called kernel fusion technique to fuse quantization operator with its previous operator, like layer normalization, to alleviate the data movement cost from token-wise quantization. Similarly, the dequantization cost of the different GeMMs' output is alleviated by scaling the INT32 accumulation using both the weight and activation quantization scales, before writing the final FP16 result back to the main memory for the next FP16 operator (like GeLU). Those optimization will be discussed in more details in Section 4.3.\n\nToken-wise quantization can significantly reduce the representation error for quantized activations. Also, as it does not need to calibrate the activation range, later we will show that there is no quantization-related cost (e.g., activation range calibration) for a moderate quantization scheme (INT8 weight with INT8 activation) for ZeroQuant.\n\n\nLayer-by-layer Knowledge Distillation with Affordable Cost\n\nKnowledge distillation (KD) is one of the most powerful methods to alleviate the accuracy degradation after model compression. However, there are several limitations of KD, especially for hidden-states KD on large-scale language models: (1) KD needs to hold a teacher and a student model together during the training, which dramatically increases the memory and compute cost; (2) KD usually requires full training of the student model. Therefore, several copies (gradient, first/second order momentum) of the weight parameters need to be stored in memory to update the model; (3) KD generally requires original training data, which sometimes are not accessible due to privacy/confidential issues.\n\nTo address those limitations, we present our layer-by-layer distillation (LKD) algorithm. Assume the target model for quantization has N transformer blocks, L 1 , ..., L N , the accessible dataset has input (X, Y ), which can be the original training data or datasets from other resources. Our LKD quantizes the network layer-by-layer and uses its original (i.e., unquantized) version as the teacher model. More specifically, assume layer L k is going to be quantized, and its quantized version is L k . Then we use the output of the L k\u22121 (i.e., by running inference on X over the first k \u2212 1 layers) as the input of L k and L k , measure the difference, and do the model update to L k , i.e.,\nL LKD,k = M SE L k \u00b7 L k\u22121 \u00b7 L k\u22122 \u00b7 ... \u00b7 L1(X) \u2212 L k \u00b7 L k\u22121 \u00b7 L k\u22122 \u00b7 ... \u00b7 L1(X) ,(1)\nwhere M SE is the mean square loss, and it can be also replaced by other losses (e.g., KL divergence) as well.\n\nAs can be seen, (1) our LKD does not need to hold a separate teacher as we use the same L 1 to L k\u22121 for both teacher/student model. As such, the only extra model cost we have is L k ; (2) the memory overhead of optimizer states are significantly reduced as the only optimizing layer is L k ; (3) as we never optimize the end-to-end model, the training does not depend on the label anymore. Later, we will show that LKD does not rely on the original training data in Section 5.6.  \n\n\nQuantization-Optimized Transformer Kernels\n\nBoth optimizing the inference latency and model size is crucial for serving large-scale transformer models in practice. During inference, the batch size is often relatively small, so the inference latency of the model primarily depends on the time of loading inference needed data from the main memory. By quantizing the weights and activations to lower precision, we reduce the data volume needed to load those data, which allows more effective use of memory bandwidth and higher loading throughput. However, simply converting weights/activations to INT8 does not guarantee improved latency because there are additional data movement overhead associated with quantization/dequantization operations as shown in Figure 2 (red box). Such an overhead becomes expensive and in some cases surpasses the performance benefits of using low precision.\n\nTo reap the accuracy improvement from token-wise quantization while obtaining improved latency, we now present our optimizations that maximize the memory bandwidth utilization to speed up inference latency for ZeroQuant. CUTLASS INT8 GeMM To support INT8 computation, we use CUTLASS [5] INT8 GeMM implementation tuned for different batch sizes. Unlike standard GPU backend library, such as cuDNN, using CUTLASS allows us to more flexibly fuse quantization operation before and after GeMM to reduce kernel launching and data-movement overhead.\n\nFusing Token-wise Activation Quantization Token-wise quantization/dequantization introduce many additional operations that lead to extra data movement cost. To eliminate these cost, we use kernel fusion [67] to fuse quantization operation for activation with its previous element-wise and/or reduction operations such as bias-add, GeLU, and LayerNorm into a single operator, as illustrated by the green box in Figure 2. For the dequantization operation (e.g., dequantizing the integer output from the GeMM operator), we similarly fuse it with our custom GeMM schedule to avoid additional read/write accesses to the main memory as illustrated by the blue box in Figure 2. By doing the above optimizations, we are able to show significant latency reduction for BERT and GPT-3-style models in Section 5. Please see Appendix D for more details about our system optimization.\n\n\nResults\n\nExperimental Details To evaluate the proposed ZeroQuant, we test it on both BERT and GPT-3 models. For BERT, we tested both BERT base and BERT large on GLUE benchmark; and for GPT-3-style models, we tested the GPT-3 350M (i.e., GPT-3-style model with 350M parameters) and GPT-3 1.3B (i.e., GPT-3-style model with 1.3B parameters) on 20 zero-shot evaluation tasks, including 19 accuracy-based tasks and 1 language modeling generation task. To illustrate the scalability of the proposed ZeroQuant, we also directly apply it to two of the largest open-sourced GPT-3-style models, i.e., GPT-J 6B [66] and GPT-NeoX 20B [4]. We use a fixed set of hyperparameters for all the LKD-related experiments even though tuning them may benefit our results. Please see Appendix B.2 for more training details and see Appendix B.3 for the reported metrics for BERT. To provide a comprehensive study, we also include a tuning result in Appendix E on BERT and an ablation study for different proposed components in Section 5.5. Notation Explanation We use WxAy to represent using x-bit for weight quantization and y-bit for activation quantization. Unless specific explanation, for W4/8, we quantize the MHSA's weight to INT8 and FFC's weight to INT4; for A8/16, we use FP16 activation for self-attention calculation (i.e., the GeMM related to W q/k/v ) and use INT8 for the rest calculation. We use ZeroQuant to represent the method with only fine-grained quantization schemes and use ZeroQuant-LKD to represent the method with both fine-grained quantization schemes and LKD.\n\n\nMain Results of BERT\n\nBERT base We report the results of BERT base in Table 2. For W8A8, the average accuracy of PTQ degrades more than 10 points. However, ZeroQuant can achieve 83.75 scores, which is only 0.2 lower than baseline. Particularly, as ZeroQuant has no activation range calibration phase, the cost of ZeroQuant is 0 which is even cheaper than standard PTQ. As compared to [6], our method achieves a better average score (1.29 higher). Meanwhile, as compared to INT8 activation used in ZeroQuant, [6] uses mixed INT8 and FP16 activation.\n\nWe also compare our method with our internal trained QAT and other QAT works [56,76]. As can be seen, with comparable accuracy results as those QAT methods, ZeroQuant can save the retraining cost from 2900s to 0s for INT8 quantization.\n\nFor the more aggressive weight quantization with minimal (or no) training quantization, i.e., W4/8A16, PTQ fully loses all accuracy (pure random prediction). However, ZeroQuant can still achieve an 81.65 average score. On top of ZeroQuant, if we add our LKD, the accuracy can be further boosted to 82.35 with a cost of 31s per task using only a single GPU, which is 93.5x cheaper than INT8 QAT quantization. We also test ZeroQuant and ZeroQuant-LKD under the W4/8A8 quantization scheme and both of them achieve similar accuracy performance as W4/8A16. If hyper-parameter tuning is applied to LKD, ZeroQuant-LKD can achieve an 83.22 average score under W4/8A8, which is similar to QAT's W8A8 result. Please see Appendix E for more details. BERT large We test our methods on BERT large as well and the results are shown in Table 3. Similar to BERT base , ZeroQuant achieves much better accuracy than PTQ methods. As compared to QAT methods, ZeroQuant has comparable results on larger datasets (like MNLI/QQP) and has better performance on small tasks (e.e., CoLA/MRPC/RTE). We actually tune QAT for multiple learning rates but cannot get even better performance for those small tasks (see Appendix F for more details).\n\nFor more aggressive quantization schemes, like W4/8A16 and W4/8A8, ZeroQuant and ZeroQuant-LKD still achieve good accuracy except for RTE but the model size is about 3x smaller than FP16 counterpart. This is aligned with the INT8 QAT results, which lose significantly more accuracy on RTE. Thanks to the lightweight cost of LKD, it only takes about 550s to finish each task even on BERT large , which is 13x cheaper than QAT.\n\n\nMain Results of GPT-3-style Models\n\nGPT-3 350M We first test ZeroQuant and ZeroQuant-LKD on GPT-3 350M and report the result in Table 4. The first interesting finding of zero-shot evaluation on GPT-3-stype models is that the accuracy performance  For W4/8A16 quantization scheme, PTQ can hardly predict reasonable answers for the majority of tasks and its generation performance on Wikitext-2 is fully crashed. As a comparison, ZeroQuant still achieves non-trivial performance on some tasks but its generation performance significantly degrades on Wikitext-2. LKD brings a significant performance boost for this W4/8A16 setting. Note that ZeroQuant-LKD increases the accuracy from 33.5 to 37.0 and decreases the PPL from 88.6 to 30.6 compared to ZeroQuant, and the entire cost of this is just 3.1 hours on a single A100 GPU. Note that this is about 0.027% GPU hours of the full pretraining cost (128 A100 GPUs for 32 hours). Similar to W4/8A16, ZeroQuant-LKD achieves much better performance than ZeroQuant on W4/8A8 by using the lightweight LKD. GPT-3 1.3B The results of GPT-3 1.3B are shown in Table 5. Similar to GPT-3 350M , for W8A8, ZeroQuant has much better performance than PTQ with less no activation calibration cost, particularly for the generation task Wikitext-2 (3.2 points lower). Also, for W4/8 quantization, LKD can bring non-trivial performance gain for ZeroQuant. The cost of LKD is about 0.02% of the full pre-training cost (128 A100 GPUs for 120 hours)\n\n\nLatency Reduction of BERT and GPT-3-style Models\n\nWe compare the inference speed of BERT between FP16 and our INT8 versions in Table 6 on a single 40G-A100 GPU. Using our efficient quantization kernel implementation and operator fusion, the INT8 model can achieve 2.27-5.19x speedup on BERT base and 2.47-5.01x on BERT large .\n\nWe also include the latency comparison of GPT-3-style models between FP16 and our INT8 version. Particularly, we use the model to generate the first 50 tokens based on a given text and measure the average latency. Our INT8 model leads to 4.16x/4.06x speedup for GPT-3 350M /GPT-3 1.3B as compared to the FP16 counterpart.  \n\n\nA Showcase of GPT-J 6B and GPT-NeoX 20B\n\nTo demonstrate the scalability of ZeroQuant, we applied it to two of the largest open-sourced models, i.e., GPT-J 6B and GPT-NeoX 20B , which have 6B and 20B parameters separately. We report the results of GPT-J 6B in Table 7 on three generation datasets, i.e., PTB [41], Wikitext-2, and Wikitext-103 [42]. As can be seen, as compared to FP16 precision, ZeroQuant achieves similar PPL on all three different tasks. To compare the latency, we again use the average latency number to generate the first 50 tokens. Our W8A8 can get up to 3.67x speedup compared to the FP16 version.\n\nTo quantize GPT-NeoX 20B to W8A8 for all GeMMs, the accuracy significantly decreases. We retrieve the quantization of each weight matrix and of each activation, and finally find out that the activation quantization for the attention calculation (i.e., the input of self-attention) causes the accuracy loss. We conjecture that this is because of the sensitivity of the self-attention module for extra-large models (20B) but cannot verify this for other models due to the lack of open-sourced extra-large models and the full evaluation pipeline. As such, we leave the input activation for self-attention in FP16 and quantize the rest to INT8. The results are shown in Table 8. Our W8A8/16 achieves similar accuracy performance but can reduce both the GPU resource requirement (from 2 A100 GPUs to 1) and the latency from 65ms to 25ms, which together lead to 5.2x better throughput/efficiency.\n\n\nAblation Study of Different Components\n\nTo investigate the performance gain of each component we introduced in Section 4, i.e., group-wise weight quantization, token-wise activation quantization, and lightweight layer-by-layer knowledge distillation, we here do an ablation study on BERT large with W4/8A8.\n\nWe present the results in Table 9. As can be seen, group-wise weight quantization boosts the accuracy (random-guess prediction) from PTQ to a non-trivial result (66.52). Further adding token-wise quantization improves 14.54 points accuracy performance. On top of those (i.e., ZeroQuant), LKD further brings a 0.56 point gain.    \n\n\nNo Access to The Original Training Data\n\nAs mentioned in previous sections, the original training data are oftentimes hard to access due to the privacy and/or confidential issues. Therefore, we here study the performance of our LKD when there is no direct access to the original training data. As the distillation objective of our LKD does not depend on the label, the training data used for LKD can be very flexible.\n\nWe compare the performance of GPT-3 350M on W4/8A8 quantization scheme using three different training data resources, i.e., random data (using random integer number to generate token ids), Wikipedia (using Huggingface to get the data 1 ), and original PILE dataset.\n\nThe results are shown in Table 10. Compared to ZeroQuant, LKD using random data can boost the accuracy by 1.1% and reduce the PPL from 92.1 to 40.6. The reason why random data can still significantly improve the performance is that LKD does not optimize the end-to-end pipeline and it only layer-by-layer learns the internal dependency from the teacher model. Therefore, random data can also provide meaningful information. Using Wikipedia data from Huggingface can further improve the accuracy to 36.2 and reduce the PPL to 30.4, which is comparable to the results using the original data. This indicates that a clean text dataset can be used for LKD when we do not have access to the original full dataset.\n\n\nConclusions\n\nWith the rapid growth of large model sizes, we have reach a point to consider how to serve those models in practice. Although several works demonstrate that post-training quantization can be applied to BERT models, to the best of our knowledge, there have been no existing works on (1) billion-scale GPT-3-style models, (2) ultra-low precision post-training quantization, and (3) end-to-end solution of how to efficiently serve the quantized model online. In this work, we offer fine-grained compression schemes for both weight and activations to enable INT8 quantization for up to 20B-scale models (GPT-NeoX 20B ). We also offer a novel affordable layer-by-layer knowledge distillation for ultra-low precision quantization, which leads to 3x model size reduction compared to FP16 model while achieving minimal accuracy degradation. Furthermore, we provide a system backend support and show up to 5.19x speedup on BERT models and 5.2x better efficiency on GPT-NeoX 20B .\n\n\nA Background\n\n\nA.1 Transformer Architecture\n\nThe transformer architecture usually has three components: an embedding layer, a stack of encoder/decoder layers, and a final classifier. In this paper, we focus on quantizing the encoder/decoder layers, i.e., the transformer block, because it is often the most memory and compute intensive components in the entire architecture. With a transformer block, there are two sub-layers, the multi-head self-attention (MHSA) and the feed-forward connection (FFC). We give a short review later and please refer to [64] for more details. At high level, transformer models can be broadly categorized to three branches: encoder-only models (BERT) [63], decoder-only models (GPT-3-style) [48], and encoder-decoder models (T5) [49]. In this paper, we focus on encoder-only and decoder-only models but our approach can be applied to encoder-decoder models as well. Transformer Block Assume the input of an encoder layer is X, the query, key, value, attention output, FFC dense, and FFC output matrices are W q , W k , W v , W o , W h\u22124h , and W 4h\u2212h , respectively. Then the forward propagation of a transformer-block is illustrated in Figure A.1, where LN is the layer normalization, Softmax is the softmax operator, and GeLU is the activation function.\n\n\nA.2 Quantization Background\n\nQuantization maps high-precision numbers, e.g., FP16/FP32, to its low-precision counterpart, e.g., INT4/INT8, to reduce the model footprint and improve the compute performance. In this work, we use uniform symmetric scalar quantizers. That is to say, if we have a vector/matrix, x, the quantization is applied as\nx quantize = round clamp( x S , \u22122 bit\u22121 , 2 bit\u22121 \u2212 1) ,(2)\nwhere bit is the number of bit we use to represent the quantized value, and S is the scaling factor. For weight matrix quantization, S is generally computed as S = max (abs(x)), since the weight matrix is static during inference. On the other hand, activations' range is dynamic during inference so that an accurate S requires dynamic calculation during inference. However, to achieve best latency reduction, coarse-grained static quantization is usually applied in practice, where S is calibrated using training data (e.g., momentum based averaging) and fixed during inference [23]. Although static quantization achieves better latency reduction, it also limits the quantization representation for activations, which is discussed in Section 3.\n\n\nB Experimental Details\n\n\nB.1 Details of PTQ on BERT and GPT\n\nFor BERT, we use a batch size of 32 and sequence length 128 to calibrate the range of activations. In order to capture the dynamic range, we use 0.95 momentum with 100 iterations, i.e.,\n\nx max = 0.95x max + 0.05max(x current\u2212iteration ),\n\nx min = 0.95x min + 0.05min(x current\u2212iteration ).\n\nFor GPT-3-style models, we use the same momentum method but change the batch size to 8 with sequence length 2048.\n\n\nB.2 Details of Main Result\n\nBERT BERT models are trained using the code-base from Huggingface [72]. We show our ZeroQuant method on BERT base and BERT large . We use the same lower-case tokenizer in BERT large instead of the cased tokenizer in the original paper [15]. When fine-tuning on GLUE [65] tasks ((i.e., MRPC [17], STS-B [10], SST-2 [58], QNLI [51], QQP [27], MNLI [70], CoLA [69], RTE [13]). 2 ), we follow the instruction from Huggingface Transformer Library [72]. For ZeroQuant and ZeroQuant-LKD, we use 48 groups for group-wise weight quantization on BERT base and 64 groups for group-wise weight quantization on BERT large , for all the weight matrices.\n\nFor LKD, we use 100 iterations with batch size 32 and sequence length 128 for BERT base , and we use 400 iterations for BERT large . We fix the learning rate as 5e-6 for both models on all tasks. However, tuning them may favor ZeroQuant.\n\nAll the models are trained using a single 40G-A100 GPU (Azure ND A100 instances).\n\nGPT-3-style Models All GPT-3-style models used in the paper are trained using DeepSpeed [52] and Megatron-DeepSpeed Library 3 . The pretraining data are from PILE dataset [22], and the training pipeline and hyperparameters are based on the Megatron-DeepSpeed repository. We use 128 A100 GPUs (Azure ND A100 instances) to do the pretraining. It takes about 32 hours to finish the training of GPT-3 350M and 120 hours of GPT-3 1.3B . We evaluate our results on 20 zero-shot evaluation tasks, including 19 accuracy evaluation tasks (i.e., HellaSwag [77], LAMBADA [47], TriviaQA [30], WebQS [3], Winogrande [54], PIQA [62], ARC (Challenge/Easy) [7], ANLI (R1/R2/R3) [71], OpenBookQA [44], RACE-h [32], BoolQ [12], Copa [1], RTE [13], WSC [35], MultiRC [73], ReCoRD [78]) and 1 language modeling generation task (i.e., Wikitext-2 [42]). For ZeroQuant and ZeroQuant-LKD, we use 64/128 groups for group-wise weight quantization on GPT-3 350M /GPT-3 1.3B for all the weight matrices.\n\nFor LKD, we use 1600 iterations with batch size 8 and sequence length 2048 for both GPT-3 350M and GPT-3 1.3B . We fix the learning rate as 5e-6 for both models. However, tuning them may favor ZeroQuant.\n\nAll the quantized models are trained using a single 40G-A100 GPU (Azure ND A100 instances).\n\n\nB.3 Accuracy reported for BERT on GLUE\n\nWe report the performance metric for BERT on GLUE based on Table B.1. For the average score, if the task only has one metric, we use it for the final result; if the task has two metrics, we compute the average of the two metrics first and use it for the final average score. For instance, the score of MRPC used to compute the final average is the mean of its accuracy and F1 score.   \n\n\nD Details about System Optimization\n\nBy having the weight and activation quantization, we can use the GeMM schedule that exploits the INT8 Tensor-core units which provide 2x/4x more compute efficiency compared to the FP16/FP32 Tensor cores. For this purpose, we adapt the CUTLASS library to produce multiple schedules based on the input sizes we are considering in our application, such as the batch size, sequence length, and the Transformer hidden dimension.\n\nTo achieve the best latency, we also develop our own efficient parallel implementation of the quantization operator on GPU. During the inference run-time, based on the total batch size (batch \u00d7 seq l en), we choose the schedule that results in the lowest possible padding when performing the Tensor-core matrix-multiplication operations.\n\nTo find the best schedule for the GeMM operation, we use the CUTLASS profiler tool that explores the tiling dimensions on the thread-blocks, WARPs, and WMMA (Tensor cores), as the three compute hierarchies available within the Ampere GPU architecture. Then, we find the best schedule by sorting the tile-based schedule based on either peak throughput achieved on the large-batch case, or the maximum memory bandwidth taken from the main memory when the batch size is small.\n\nHowever, there are still several challenges we need to address which are discussed below.\n\nOperation Fusion for Token-wise Activation Quantization. One of the main challenges of our quantization scheme is how to efficiently quantize hidden states before the GeMM operation. In order to remove the overhead, we fuse the activation quantization with its associated element-wise and/or reductionbased operations such as bias-addition, GELY, and LayerNorm. This is due to the fact that each SM takes care of one row (token) of the activation and therefore, we can reuse the computation from the thread registers and compute the quantization scale, avoiding the data movement between GPU kernels and main memory. Moreover, by converting data from FP16 to INT8, we can utilize the memory bandwidth twice, further improving the inference latency and throughput.\n\nDequantization Associated with GeMM Schedule To utilize the output of integer output from GeMM operator in the following operators, one important step is to dequantize the output by using the scaling factor of the weight and activations. This dequantization step generally introduces extra overhead for quantized network inference due to the data movement. As such, we add a custom epilogue, which converts the final accumulated result (from INT32 format) of each row and column of the output to the real value (in FP16 format), using corresponding floating-point quantization scales computed from weight and activation group-wise quantization. By fusing the dequantization with GeMM schedule, we ensure that there is no overhead exposed by using the INT8 operations while producing the FP16 results that are used in the following operation. Furthermore, to effectively combine dequantization with the GeMM operation, we read the two groups of quantization scales for the activation and weight matrices in advance prior to completion of the multiplication of the output matrix. Doing so, we overlap the reading of the extra quantization parameters with the GeMM computation and the GeMM-plus-dequantization can seamlessly work together without stalling the inference pipeline.\n\nCuda Graph Enhanced Small Model Inference. As the execution time for specific kernels reduce by optimizing the throughput using the INT8 inference pipeline, the overhead of launching the GPU kernels and the CPU-to-GPU communication become a major bottleneck mostly on small-scale models. To address this issue, we add the CUDA-Graph support to our inference pipeline that reduces the CPU overhead, by storing the trace of the kernels launched during the inference forward computation, and creating the computation-graph to be reused in the next call to the inference pipeline. Thus, after storing the graph for the first time, we can replay the graph for the following requests, which substantially improves the performance especially on small models, such as BERT base . For a fair comparison, we also enable Cuda Graph for FP16 baseline.\n\n\nE Tuned Results on BERT\n\nAs mentioned in the main text and Appendix B.2, we use the same set of hyperparameters for BERT. However, tuning them can significantly boost the performance for ZeroQuant. Here, we tune two hyperparameters, i.e., the learning rate and the number of iterations in order to show the best possible performance of ZeroQuant \n\n\nF QAT on BERT large\n\nWe use four different learning rates for QAT on BERT large , {5e-6, 1e-5, 2e-5, 5e-5}. The final results we reported in the paper are chosen from the best single run among those four different learning rates. However, even with such tuning, we are not able to get good performance for BERT large on RTE. Also, note that the time cost we used in the main text is based on a single run. if we consider the tuning cost, the total time will be 4 \u00d7 7181s\n\n\nG Limitations and Future Work\n\nWe believe it is critical for every work to clearly state its limitations, especially in this area. One limitation is that in this work we only focused on natural language models, but it would be interesting to see how ZeroQuant would perform for computer vision models. We leave this as a future work.\n\nAnother limitation is that we can only verify the scalability of ZeroQuant up to 20B scale models. If there are new releases of larger open-sourced models, it would be great to test ZeroQuant on those larger models as well.\n\nThird, in this paper, we found out that the activation input of self-attention is more sensitive for quantization for the extra-large model (GPT-NeoX 20B ). However, we are unable to verify this on other extra-large models due to the lack of open-sourced models. \n\n\nH Full Zero-shot Evaluation of GPT-3-style Models\n\nWe includes all zero-shot evaluation results in this section for all GPT-3-style models, inlcuding GPT-NeoX 20B . \n\nFigure 1 :\n1The activation range (left) and row-wise weight range of the attention output matrix (right) of different layers on the pretrained GPT-3 350M . See Figure C.1 for the results of BERT base .\n\nFigure 2 :\n2The illustration of normal (left) and our fused (right) INT8 GeMM.\n\nFigure A. 1 :\n1The illustration of a Transformer-block.\n\nFigure C. 1 :\n1The activation range of different layers (left) and the row-wise weight range of the attention output matrix (W o ) of different layers (right). The results are based on the BERT base trained on MNLI dataset. Please seeFigure 1for the results of GPT-3 350M .\n\nTable 1 :\n1Post training quantization results of GPT-3 350M on 20 zero-shot evaluation datesets. Here WxAy means x-/y-bit for weight/activation. Particularly, for W4/8, we quantize the MHSA's weight to INT8 and FFC's weight to INT4. Please see Table H.1 for the results of all 20 tasks.Precision \nLambada (\u2191) PIQA (\u2191) OpenBookQA (\u2191) RTE (\u2191) ReCoRd (\u2191) Ave. 19 Tasks (\u2191) Wikitext-2 (\u2193) \n\nW16A16 \n49.3 \n66.3 \n29.4 \n53.8 \n75.1 \n38.9 \n21.5 \nW8A16 \n49.3 \n66.1 \n29.6 \n54.2 \n74.8 \n38.5 \n22.1 \nW16A8 \n44.7 \n64.8 \n28.2 \n52.7 \n69.2 \n37.8 \n24.6 \nW8A8 \n42.6 \n64.1 \n28.0 \n53.1 \n67.5 \n37.8 \n26.2 \nW4/8A16 \n0.00 \n51.4 \n30.2 \n52.7 \n16.1 \n28.9 \n1.76e5 \n\n\n\nTable 2 :\n2Result of BERT base on the development set of GLUE benchmark (except WNLI). [56] + uses 128 \ngroups for weight matrix which is hard to get GPU acceleration. [6]  *  uses mixed INT8 and FP16 activation, \nand it directly reports the average metric of MNLI/MRPC/QQP/STS-B, which is basically the average of \nthe two metrics we used for our runs. \n\nPrecision (Method) \nCoLA MNLI-m MNLI-mm \nMRPC \nQNLI \nQQP \nRTE SST-2 \nSTS-B \nAve. Ave. Time (s) \n\nW16A16 (Baseline) \n59.72 \n84.94 \n85.06 \n86.27/90.57 92.15 91.51/88.56 72.20 93.23 90.06/89.59 83.95 \nN/A \n\nW8A8 [56] (QAT) + \n-\n83.91 \n83.83 \n-\n-\n-\n-\n92.83 \n-\n-\n-\nW8A8 [76] (QAT) \n58.48 \n-\n-\n-/89.56 \n90.62 \n-/87.96 \n68.78 92.24 \n89.04/-\n-\n-\nW8A8 (QAT) \n61.21 \n84.80 \n84.64 \n83.82/88.85 91.29 91.29/88.28 71.12 92.89 88.39/88.18 83.37 \n2900 \nW8A8 (PTQ) \n56.06 \n79.99 \n81.06 \n75.49/79.67 87.35 89.92/86.82 48.38 91.40 86.58/86.44 77.41 \n6 \nW8A8/16 [6] (PTQ)  *  \n58.63 \n82.67 \n82.67 \n88.74 \n90.41 \n89.40 \n68.95 92.66 \n88.00 \n82.46 \nUnknown \nW8A8 (ZeroQuant) \n59.59 \n84.83 \n85.13 \n86.03/90.39 91.98 91.45/88.46 71.12 93.12 90.09/89.62 83.75 \n0 \n\nW4/8A16 (PTQ) \n0.00 \n16.74 \n16.95 \n31.62/0.00 \n50.74 \n63.18/0.00 47.29 70.64 16.48/15.91 33.11 \n6 \nW4/8A16 (ZeroQuant) \n57.29 \n82.69 \n83.27 \n84.56/88.40 90.04 86.52/79.49 70.76 92.78 88.46/88.61 81.65 \n0 \nW4/8A16 (ZeroQuant-LKD) 58.50 \n83.16 \n83.69 \n84.80/89.31 90.83 88.94/84.12 70.04 92.78 88.49/88.67 82.35 \n31 \n\nW4/8A8 (ZeroQuant) \n56.69 \n82.46 \n83.06 \n84.07/88.03 90.13 87.04/80.50 70.76 92.78 88.07/88.44 81.55 \n0 \nW4/8A8 (ZeroQuant-LKD) \n58.80 \n83.09 \n83.65 \n85.78/89.90 90.76 89.16/84.85 71.84 93.00 88.16/88.55 82.71 \n31 \n\n\n\nTable 3 :\n3Result of BERT large on the development set of GLUE benchmark (except WNLI). + We extensively tuned the learning rate for QAT (see Appendix F for more details).Precision (Method) \nCoLA MNLI-m MNLI-mm \nMRPC \nQNLI \nQQP \nRTE SST-2 \nSTS-B \nAve. Ave. Time (s) \n\nW16A16 (Baseline) \n63.35 \n86.65 \n85.91 \n87.99/91.62 92.24 91.08/88.08 74.01 93.46 90.34/90.11 85.03 \nN/A \n\nW8A8 [76] (QAT) \n-\n-\n-\n-/90.9 \n91.74 \n90.12/-\n-\n-\nW8A8 (QAT) + \n59.85 \n86.65 \n86.35 \n85.29/89.43 92.55 91.60/88.60 61.37 93.23 87.55/87.65 82.78 \n7181 \nW8A8 (PTQ) \n60.57 \n75.69 \n76.94 \n81.13/84.93 88.49 84.04/74.35 46.93 91.74 62.75/55.77 73.54 \n31 \nW8A8 (ZeroQuant) \n63.38 \n86.52 \n85.64 \n87.75/91.50 92.31 91.09/88.05 72.56 93.35 90.45/90.19 84.81 \n0 \n\nW4/8A16 (PTQ) \n0.00 \n16.85 \n33.24 \n68.38/80.89 51.25 \n63.18/0.00 52.71 52.41 \n-5.74/-8.51 35.73 \n31 \nW4/8A16 (ZeroQuant) \n62.99 \n84.77 \n84.42 \n87.50/91.16 91.63 90.03/86.41 48.01 92.16 89.49/89.28 81.23 \n0 \nW4/8A16 (ZeroQuant-LKD) 63.72 \n84.90 \n84.81 \n87.99/91.39 91.45 90.34/86.92 51.62 92.43 89.46/89.29 81.85 \n550 \n\nW4/8A8 (ZeroQuant) \n62.34 \n84.62 \n84.25 \n87.75/91.38 91.87 89.86/86.09 47.65 91.97 89.39/89.17 81.06 \n0 \nW4/8A8 (ZeroQuant-LKD) \n63.51 \n84.70 \n84.71 \n88.73/91.99 91.73 90.25/86.74 49.82 92.09 89.34/89.08 81.62 \n550 \n\n\n\nTable 4 :\n4of accuracy-based tasks is more tolerant to quantization than generation tasks. For instance, W8A8 PTQ has a 1.1% average accuracy drop on 19 accuracy-based tasks as compared to 4.7 points loss on Wikitext-2. Comparing ZeroQuant with PTQ using W8A8, we can reduce the accuracy gap from 1.1% to 0.Post training quantization result of GPT-3 350M on 20 zero-shot evaluation datasets. Please \nsee Table H.1 for the results of all 20 tasks. \n\nPrecision (Method) \nLambada (\u2191) PIQA (\u2191) OpenBookQA (\u2191) RTE (\u2191) ReCoRd (\u2191) Ave. 19 Tasks (\u2191) Wikitext-2 (\u2193) Time Cost \n\nW16A16 \n49.3 \n66.3 \n29.4 \n53.8 \n75.1 \n38.9 \n21.5 \nN/A \n\nW8A8 (PTQ) \n42.6 \n64.1 \n28.0 \n53.1 \n67.5 \n37.8 \n26.2 \n7 mins \nW8A8 (ZeroQuant) \n51.0 \n66.5 \n29.2 \n53.4 \n74.9 \n38.7 \n21.7 \n0 \n\nW4/8A16 (PTQ) \n0.00 \n51.4 \n30.2 \n52.7 \n16.1 \n28.9 \n1.76e5 \n7 mins \nW4/8A16 (ZeroQuant) \n10.1 \n58.5 \n27.2 \n52.0 \n56.5 \n33.5 \n88.6 \n0 \nW4/8A16 (ZeroQuant-LKD) \n39.8 \n63.8 \n29.4 \n53.1 \n70.1 \n37.0 \n30.6 \n1.1 hours \n\nW4/8A8 (ZeroQuant) \n10.5 \n57.7 \n28.0 \n52.7 \n55.3 \n33.4 \n92.1 \n0 \nW4/8A8 (ZeroQuant-LKD) \n37.4 \n61.8 \n28.2 \n53.1 \n68.5 \n36.6 \n31.1 \n1.1 hours \n\n2% and the \nperplexity (PPL) gap from 4.7 to 0.2 with no activation range calibration cost. \n\n\nTable 5 :\n5Post training quantization result of GPT-3 1.3B on 20 zero-shot evaluation datasets. Please see Table H.2 for the results of all 20 tasks.Precision (Method) \nLambada (\u2191) PIQA (\u2191) OpenBookQA (\u2191) RTE (\u2191) ReCoRd (\u2191) Ave. 19 Tasks (\u2191) Wikitext-2 (\u2193) Time Cost \n\nW16A16 \n61.3 \n71.4 \n33.6 \n53.1 \n82.6 \n42.4 \n15.3 \nN/A \n\nW8A8 (PTQ) \n54.8 \n67.7 \n16.6 \n54.5 \n75.7 \n40.5 \n18.9 \n13 mins \nW8A8 (ZeroQuant) \n62.6 \n70.7 \n33.4 \n52.7 \n80.9 \n42.3 \n15.7 \n0 \n\nW4/8A16 (PTQ) \n0.00 \n50.4 \n27.0 \n50.9 \n15.8 \n29.0 \n1.35e5 \n13 mins \nW4/8A16 (ZeroQuant) \n43.9 \n66.5 \n30.0 \n52.7 \n77.3 \n39.38 \n21.9 \n0 \nW4/8A16 (ZeroQuant-LKD) \n59.4 \n69.5 \n31.6 \n52.7 \n79.7 \n41.5 \n17.6 \n3 hours \n\nW4/8A8 (ZeroQuant) \n46.8 \n66.4 \n28.8 \n52.7 \n76.2 \n39.24 \n24.1 \n0 \nW4/8A8 (ZeroQuant-LKD) \n48.7 \n68.1 \n29.0 \n52.0 \n77.4 \n39.90 \n18.2 \n3 hours \n\n\n\nTable 6 :\n6The speedup of our W8A8 as compared to W16A16. We measure the end-to-end average latency for the entire BERT model, and the time reported is in milliseconds.Seq Len \nPrecision \n128 \n256 \nBS \n1 \n2 \n4 \n8 \n16 \n16 \n64 \n128 \n1 \n2 \n4 \n8 \n16 \n16 \n64 \n128 \n\nBERT base \n\nW16A16 2.45 3.22 3.85 5.51 \n9.96 17.93 34.25 67.08 \n3.13 4.05 5.70 10.55 19.27 36.69 71.75 140.0 \nW8A8 \n1.08 1.16 1.42 1.76 \n2.58 \n3.90 \n6.74 12.92 \n1.22 1.44 2.08 \n2.88 \n4.10 \n7.80 14.66 28.13 \nSpeedup 2.27 2.78 2.71 3.13 \n3.86 \n4.60 \n5.08 \n5.19 \n2.57 2.81 2.74 \n3.66 \n4.70 \n4.70 \n4.89 \n4.98 \n\nBERT large \n\nW16A16 5.45 6.38 8.73 13.88 26.34 48.59 92.49 183.4 \n6.39 8.94 14.66 27.99 51.94 98.78 195.9 384.5 \nW8A8 \n2.08 2.58 2.84 3.79 \n6.21 10.28 18.86 36.62 \n2.55 3.36 4.16 \n6.88 11.61 21.20 41.24 79.90 \nSpeedup 2.62 2.47 3.07 3.66 \n4.24 \n4.73 \n4.90 \n5.01 \n2.51 2.66 3.52 \n4.07 \n4.47 \n4.66 \n4.75 \n4.81 \n\n\n\nTable 7 :\n7Post training quantization result of GPT-J 6B on three zero-shot generation tasksPrecision PTB Wikitext-2 Wikitext-103 \nLatency \n\nW16A16 20.47 \n10.35 \n10.35 \n29.13ms (1x) \nW8A8 20.97 \n10.51 \n10.52 \n7.94ms (3.67x) \n\n\n\nTable 8 :\n8Post training quantization result of GPT-NeoX 20B on 19 zero-shot evaluation datasets. Please see Table H.4 for the results of all 19 tasks.Precision Lambada PIQA Ave. 19 Tasks \nLatency \n\nW16A16 \n71.7 \n77.7 \n50.5 \n2\u00d765ms (1x) \nW8A8/16 71.9 \n78.3 \n50.4 \n1\u00d725ms (5.2x) \n\n\n\nTable 9 :\n9Ablation study of different components for BERT large on the development set of GLUE. The quantization scheme used here is W4/8A8. Here, GP is the abbreviation of group-wise weight quantization, TQ is the abbreviation of token-wise activation quantization.GQ TQ LKD CoLA MNLI-m MNLI-mm \nMRPC \nQNLI \nQQP \nRTE SST-2 \nSTS-B \nAve. \n\n\n\n-0.79 \n33.07 \n32.94 \n68.38/80.54 49.42 \n63.18/0.00 52.71 52.29 \n-4.27/-1.90 35.85 \n\n\n\n59.81 \n66.63 \n68.79 \n68.63/71.17 83.87 78.24/61.30 46.93 89.45 54.58/32.52 66.52 \n\n\n\n62.34 \n84.62 \n84.25 \n87.75/91.38 91.87 89.86/86.09 47.65 91.97 89.39/89.17 81.06 \n\n\n\n63.51 \n84.70 \n84.71 \n88.73/91.99 91.73 90.25/86.74 49.82 92.09 89.34/89.08 81.62 \n\n\n\nTable 10 :\n10Post training quantization result of GPT-3 350M on 20 zero-shot evaluation datesets The quantization scheme here is W4/8A8. Please see Table H.3 for the results of all 20 tasks.Method \nData Resource Lambada (\u2191) PIQA (\u2191) OpenBookQA (\u2191) RTE (\u2191) ReCoRd (\u2191) Ave. 19 Tasks (\u2191) Wikitext-2 (\u2193) \n\nZeroQuant \n-\n10.5 \n57.7 \n28.0 \n52.7 \n55.3 \n33.4 \n92.1 \nZeroQuant-LKD Random data \n26.1 \n59.3 \n29.2 \n50.5 \n64.9 \n34.5 \n40.6 \nZeroQuant-LKD \nWikipedia \n33.9 \n62.4 \n28.0 \n52.7 \n69.5 \n36.2 \n30.4 \nZeroQuant-LKD Original data \n37.4 \n61.8 \n28.2 \n53.1 \n68.5 \n36.6 \n31.1 \n\n\n\nTable B . 1 :\nB1Metric used for BERT base on the development set of GLUE benchmark (except WNLI).CoLA \nMNLI-m \nMNLI-mm \nMRPC \nQNLI \nQQP \nRTE \nSST-2 \nSTS-B \n\nMatthews Correction Accuracy Accuracy / F1 Accuracy Accuracy Accuracy / F1 Accuracy Accuracy Pearson / Spearmanr \n\n\n\nTable C .\nCBesides, we also push the weight quantization to a mixed-precision setting with INT4 for weights in FFC and INT8 for weights in MHSA (i.e., the row of W4/8A16). This ultra-low precision quantization leads the model to be purely random without meaning prediction.1: Post training quantization results of BERT base on development sets of the GLUE benchmark (except \nWNLI). Here WxAy means x-bit for weight quantization and y-bit for activation quantization. Particularly, \nfor W4/8, we quantize the MHSA's weight to INT8 and FFC's weight to INT4. Please see Appendix B.3 for \nthe reported metrics. \n\nPrecision \nCoLA MNLI-m MNLI-mm \nMRPC \nQNLI \nQQP \nRTE SST-2 \nSTS-B \nAve. \n\nW16A16 \n59.72 \n84.94 \n85.06 \n86.27/90.57 92.15 91.51/88.56 72.20 93.23 90.06/89.59 83.95 \nW8A16 \n60.77 \n84.65 \n84.92 \n85.29/89.86 91.84 91.52/88.56 71.84 93.46 89.89/89.50 83.87 \nW16A8 \n56.85 \n80.55 \n81.48 \n84.07/89.33 91.34 91.30/88.07 68.59 93.46 88.74/88.74 81.93 \nW8A8 \n58.74 \n79.99 \n81.06 \n84.31/89.51 91.18 91.24/88.03 70.76 92.66 88.33/88.73 82.16 \nW4/8A16 \n0.00 \n16.74 \n16.95 \n31.62/0.00 \n50.74 \n63.18/0.00 47.29 70.64 16.48/15.91 33.11 \n\nC PTQ challenge of BERT base \n\nFrom Table C.1, we observe similar results as [6], where the accuracy degradation of INT8 quantization \nis mainly from activation quantization. Specifically, there is a negligible accuracy drop from INT8 weight \nquantization (i.e., the row of W8A16). However, with sole INT8 activation (i.e., the row of W16A8), the \naccuracy decreases from 84.06 to 79.61. \n\nTable E .\nE1: Result of BERT base on the development set of GLUE benchmark (except WNLI). Here WxAy means x-bit for weight quantization and y-bit for activation quantization. Particularly, for W4/8, we quantize the MHSA's weight to INT8 and FFC's weight to INT4. Please see Appendix B.3 for the reported metrics. Table E.2: Result of BERT large on the development set of GLUE benchmark (except WNLI). Here WxAy means x-bit for weight quantization and y-bit for activation quantization. Particularly, for W4/8, we quantize the MHSA's weight to INT8 and FFC's weight to INT4. Please see Appendix B.3 for the reported metrics. 97/92.15 91.87 90.37/86.99 50.54 92.55 89.57/89.38 81.88on both BERT base and BERT large . Particularly, we choose learning rate from the set {1e-6, 2e-6, 5e-6, 1e-5}, and choose number of iterations from the set {0, 50, 100, 200, 400, 800, 1600}. Thanks to the lightweight of LKD, the total tuning time for BERT base (including all data loading time, evaluation time, tokenization time, all three quantization schemes, etc) is around 4.5 hours on 8 40G-A100 GPUs (i.e., 36 GPU hours), and the tuning time for BERT large is around 16 hours on 8 40G-A100 GPUs (i.e., 128 GPU hours).We summarize the best results in theTable E.1 and E.2.Precision (Method) \nCoLA MNLI-m MNLI-mm \nMRPC \nQNLI \nQQP \nRTE SST-2 \nSTS-B \nAve. \n\nW32A32 (Baseline) \n59.72 \n84.94 \n85.06 \n86.27/90.57 92.15 91.51/88.56 72.20 93.23 90.06/89.59 83.95 \n\nW8A8 (ZeroQuant-LKD No Tuning) \n59.59 \n84.83 \n85.13 \n86.03/90.39 91.98 91.45/88.46 71.12 93.12 90.09/89.62 83.75 \nW8A8 (ZeroQuant-LKD Tuned) \n60.90 \n84.95 \n85.10 \n86.27/90.60 92.07 91.47/88.47 71.84 93.46 90.09/89.62 84.07 \n\nW4/8A32 (ZeroQuant-LKD No Tuning) 58.50 \n83.16 \n83.69 \n84.80/89.31 90.83 88.94/84.12 70.04 92.78 88.49/88.67 82.35 \nW4/8A32 (ZeroQuant-LKD Tuned) \n60.04 \n83.64 \n84.31 \n85.78/89.53 91.01 90.66/87.26 71.84 93.12 88.68/88.79 83.26 \n\nW4/8A8 (ZeroQuant-LKD No Tuning) \n58.80 \n83.09 \n83.65 \n85.78/89.90 90.76 89.32/84.85 71.84 93.00 88.16/88.55 82.71 \nW4/8A8 (ZeroQuant-LKD Tuned) \n60.30 \n83.47 \n84.03 \n85.78/89.90 90.87 90.77/87.38 71.84 93.00 88.38/88.70 83.22 \n\nPrecision (Method) \nCoLA MNLI-m MNLI-mm \nMRPC \nQNLI \nQQP \nRTE SST-2 \nSTS-B \nAve. \n\nW32A32 (Baseline) \n63.35 \n86.65 \n85.91 \n87.99/91.62 92.24 91.08/88.08 74.01 93.46 90.34/90.11 85.03 \n\nW8A8 (ZeroQuant-LKD No Tuning) \n63.38 \n86.52 \n85.64 \n87.75/91.50 92.31 91.09/88.05 72.56 93.35 90.45/90.19 84.81 \nW8A8 (ZeroQuant-LKD Tuned) \n64.36 \n86.64 \n85.74 \n88.48/91.97 92.49 91.15/88.13 74.73 93.58 90.45/90.19 85.30 \n\nW4/8A32 (ZeroQuant-LKD No Tuning) 63.72 \n84.90 \n84.81 \n87.99/91.39 91.45 90.34/86.92 51.62 92.43 89.46/89.29 81.85 \nW4/8A32 (ZeroQuant-LKD Tuned) \n64.06 \n85.02 \n84.98 \n88.73/91.99 91.82 90.45/87.12 52.35 92.78 89.72/89.44 82.19 \n\nW4/8A8 (ZeroQuant-LKD No Tuning) \n63.51 \n84.70 \n84.71 \n88.73/91.99 91.73 90.25/86.74 49.82 92.09 89.34/89.08 81.62 \nW4/8A8 (ZeroQuant-LKD Tuned) \n63.60 \n84.77 \n84.90 \n88.\n\nTable H .\nH1: The full results of GPT-3 350M .Tasks \nBaseline \nPTQ \nZeroQuant \nZeroQuant-LKD \nW32A32 W8A32 W32A8 W8A8 W4/8A32 \nW8A8 W4/8A32 W4/8A8 \nW4/8A32 W4/8A8 \nHellaSwag \n38.6 \n38.1 \n37.6 \n36.8 \n26.5 \n38.4 \n30.4 \n30.5 \n35.3 \n35.3 \nLAMBADA \n49.3 \n49.3 \n44.7 \n42.9 \n0 \n51.0 \n10.1 \n10.5 \n39.8 \n37.4 \nTriviaQA \n3.00 \n2.67 \n2.70 \n2.32 \n0 \n2.86 \n0.159 \n0.194 \n1.043 \n0.23 \nWebQs \n1.43 \n0.935 \n1.23 \n0.689 \n0 \n1.378 \n0.246 \n0.394 \n0.591 \n0.049 \nWinogrande \n53.2 \n52.1 \n52.1 \n52.1 \n47.8 \n51.4 \n52.6 \n50.7 \n51.6 \n51.8 \nPIQA \n66.3 \n66.1 \n64.8 \n64.1 \n51.4 \n66.5 \n58.5 \n57.7 \n63.8 \n61.8 \nARC (Challenge) \n24.2 \n24.0 \n24.0 \n24.1 \n27.0 \n24.5 \n22.0 \n21.8 \n21.8 \n23.6 \nARC (Easy) \n45.5 \n44.7 \n44.2 \n43.9 \n25.1 \n44.5 \n37.6 \n37.5 \n40.5 \n40.5 \nANLI R1 \n31.1 \n30.0 \n31.3 \n33.2 \n33.4 \n31.1 \n32.8 \n32.7 \n32.4 \n33.8 \nANLI R2 \n34.3 \n36.0 \n36.5 \n35.9 \n33.4 \n34.3 \n34.7 \n34.2 \n34.1 \n33.5 \nANLI R3 \n34.1 \n34.0 \n33.0 \n37.2 \n33.5 \n33.4 \n34.9 \n34.5 \n33.1 \n33.4 \nOpenBookQA \n29.4 \n29.6 \n28.2 \n28.0 \n30.2 \n29.2 \n27.2 \n28.0 \n29.4 \n28.2 \nRACE-h \n32.4 \n31.3 \n30.3 \n30.7 \n22.4 \n32.2 \n25.7 \n26.4 \n29.5 \n29.7 \nBoolQ \n60.3 \n60.2 \n57.0 \n56.9 \n37.8 \n60.2 \n60.1 \n59.4 \n61.9 \n61.9 \nCopa \n69.0 \n67.0 \n71.0 \n73.0 \n48.0 \n69.0 \n63.0 \n64.0 \n68.0 \n66.0 \nRTE \n53.8 \n54.2 \n52.7 \n53.1 \n52.7 \n53.4 \n52.0 \n52.7 \n53.1 \n53.1 \nWSC \n36.5 \n36.5 \n36.5 \n35.6 \n63.5 \n36.5 \n36.5 \n36.5 \n36.5 \n36.5 \nMultiRC \n0.839 \n0.839 \n0.839 \n0.944 \n0.315 \n0.839 \n1.889 \n1.889 \n0.839 \n0.839 \nReCoRD \n75.1 \n74.8 \n69.2 \n67.5 \n16.1 \n74.9 \n56.5 \n55.3 \n70.1 \n68.5 \nWikitext-2 \n21.52 \n22.09 \n24.56 \n26.20 \n1.76e5 \n21.68 \n88.64 \n92.10 \n30.56 \n31.13 \nAverage Acc \n38.86 \n38.54 \n37.78 \n37.84 \n28.9 \n38.71 \n33.52 \n33.42 \n37.02 \n36.64 \n\n\n\nTable H .\nH2: The full results of GPT-3 1.3B . Table H.3: The full results of W4/8A8 GPT-3 350M using different data resources.Table H.4: The full results of GPT-NeoX 20B .Tasks \nBaseline \nPTQ \nZeroQuant \nZeroQuant-LKD \nW32A32 W8A8 W4/8A32 \nW8A8 W4/8A32 W4/8A8 \nW4/8A32 W4/8A8 \nHellaSwag \n51.4 \n47.0 \n26.1 \n50.8 \n43.7 \n43.2 \n48.5 \n46.7 \nLAMBADA \n61.3 \n54.8 \n0 \n62.6 \n43.9 \n46.8 \n59.4 \n48.7 \nTriviaQA \n7.37 \n4.43 \n0 \n6.67 \n2.36 \n2.09 \n4.28 \n2.99 \nWebQs \n2.90 \n1.476 \n0 \n2.07 \n1.132 \n1.28 \n1.673 \n1.083 \nWinogrande \n57.1 \n55.7 \n50.1 \n57.1 \n54.6 \n54.3 \n55.3 \n53.8 \nPIQA \n71.4 \n67.7 \n50.4 \n70.7 \n66.5 \n66.4 \n69.5 \n68.1 \nARC (Challenge) \n27.2 \n27.1 \n26.5 \n26.8 \n25.7 \n25.3 \n27.8 \n26.5 \nARC (Easy) \n54.5 \n49.7 \n26.0 \n53.8 \n48.0 \n47.0 \n52.2 \n50.3 \nANLI R1 \n32.0 \n33.1 \n33.0 \n33.4 \n33.8 \n33.6 \n34.2 \n33.8 \nANLI R2 \n32.0 \n32.9 \n33.3 \n33.9 \n33.0 \n33.0 \n33.8 \n32.8 \nANLI R3 \n33.8 \n33.5 \n32.3 \n34.8 \n33.6 \n33.5 \n33.7 \n33.0 \nOpenBookQA \n33.6 \n32.6 \n27.0 \n33.4 \n30.0 \n28.8 \n31.6 \n29.0 \nRACE-h \n33.6 \n32.6 \n22.4 \n32.7 \n30.9 \n29.9 \n32.7 \n33.2 \nBoolQ \n62.4 \n59.2 \n37.8 \n61.3 \n60.3 \n59.8 \n61.7 \n61.3 \nCopa \n70.0 \n70.0 \n55.0 \n72.0 \n73.0 \n74.0 \n72.0 \n70.0 \nRTE \n53.1 \n54.5 \n50.9 \n52.7 \n52.7 \n52.7 \n52.7 \n52.0 \nWSC \n37.5 \n36.5 \n63.5 \n36.5 \n36.5 \n36.5 \n36.5 \n36.5 \nMultiRC \n1.05 \n0.839 \n0.315 \n0.839 \n1.259 \n1.154 \n0.839 \n0.839 \nReCoRD \n82.6 \n75.7 \n15.8 \n80.9 \n77.3 \n76.2 \n79.7 \n77.4 \nWikitext-2 \n15.3 \n18.85 \n1.35e5 \n15.69 \n21.9 \n24.09 \n17.56 \n18.18 \nAverage Acc \n42.36 \n40.49 \n28.97 \n42.26 \n39.38 \n39.24 \n41.48 \n39.90 \n\nTasks \nRandom Data Wikipedia Original Training Data \nHellaSwag \n33.9 \n35.5 \n35.3 \nLAMBADA \n26.1 \n33.9 \n37.4 \nTriviaQA \n0.088 \n0.972 \n0.23 \nWebQs \n0.049 \n0.344 \n0.049 \nWinogrande \n50.3 \n52.4 \n51.8 \nPIQA \n59.3 \n62.4 \n61.8 \nARC (Challenge) \n22.6 \n23.3 \n23.6 \nARC (Easy) \n38.3 \n40.0 \n40.5 \nANLI R1 \n33.0 \n32.0 \n33.8 \nANLI R2 \n34.3 \n34.7 \n33.5 \nANLI R3 \n33.4 \n32.9 \n33.4 \nOpenBookQA \n29.2 \n28.0 \n28.2 \nRACE-h \n27.8 \n29.1 \n29.7 \nBoolQ \n47.8 \n52.6 \n61.9 \nCopa \n65.0 \n69.0 \n66.0 \nRTE \n50.5 \n52.7 \n53.1 \nWSC \n36.5 \n36.5 \n36.5 \nMultiRC \n1.574 \n1.154 \n0.839 \nReCoRD \n64.9 \n69.5 \n68.5 \nWikitext-2 \n40.63 \n30.36 \n31.13 \nAverage Acc \n34.45 \n36.16 \n36.64 \nTasks \nW16A16 W8A8/16 \nHellaSwag \n71.4 \n71.2 \nLAMBADA \n71.7 \n71.9 \nTriviaQA \n25.8 \n25.9 \nWebQs \n6.3 \n6.64 \nWinogrande \n66.0 \n65.7 \nPIQA \n77.7 \n78.3 \nARC (Challenge) \n41.0 \n42.2 \nARC (Easy) \n68.5 \n68.8 \nANLI R1 \n33.1 \n33.9 \nANLI R2 \n33.4 \n34.4 \nANLI R3 \n35.1 \n35.4 \nOpenBookQA \n39.8 \n38.8 \nRACE-h \n38.5 \n37.6 \nBoolQ \n69.4 \n69.9 \nCopa \n84.0 \n85.0 \nRTE \n54.9 \n54.9 \nWSC \n50.0 \n44.2 \nMultiRC \n3.57 \n4.41 \nReCoRD \n88.3 \n88.0 \nAverage Acc \n50.45 \n50.38 \n\nhttps://huggingface.co/datasets/wikipedia\nWe exclude WNLI[35] since its results are not stable[16]. 3 https://github.com/microsoft/Megatron-DeepSpeed\nAcknowledgmentsThis work is done within the DeepSpeed team in Microsoft. We appreciate the help from the DeepSpeed team. Particularly, we thank Jeff Rasley and Elton Zheng for solving the engineering issue. We thank the engineering supports from the Turing team in Microsoft.\nCopa: Constrained parafac2 for sparse & large datasets. Ardavan Afshar, Ioakeim Perros, E Evangelos, Elizabeth Papalexakis, Joyce Searles, Jimeng Ho, Sun, Proceedings of the 27th ACM International Conference on Information and Knowledge Management. the 27th ACM International Conference on Information and Knowledge ManagementArdavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, Elizabeth Searles, Joyce Ho, and Jimeng Sun. Copa: Constrained parafac2 for sparse & large datasets. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 793-802, 2018.\n\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, Irwin King, arXiv:2012.15701Binarybert: Pushing the limit of bert quantization. arXiv preprintHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.\n\nSemantic parsing on Freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533-1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n\nSid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Jason Phang, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, Samuel Weinbach, Gpt-Neox, Large scale autoregressive language modeling in pytorch. Sid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Jason Phang, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in pytorch, 2021.\n\nCUTLASS: Fast Linear Algebra in CUDA C++. NVIDIA blog. NVIDIA blog. CUTLASS: Fast Linear Algebra in CUDA C++. https://developer.nvidia.com/blo g/cutlass-linear-algebra-cuda/, December 2017.\n\nUnderstanding and overcoming the challenges of efficient transformer quantization. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, arXiv:2109.12948arXiv preprintYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021.\n\nA systematic classification of knowledge, reasoning, and context within the arc dataset. Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew Mccallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, arXiv:1806.00358arXiv preprintMichael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, et al. A systematic classification of knowledge, reasoning, and context within the arc dataset. arXiv preprint arXiv:1806.00358, 2018.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nZeroq: A novel zero shot quantization framework. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169-13178, 2020.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, Lucia Specia, arXiv:1708.00055Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprintDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Seman- tic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nBoolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.10044arXiv preprintChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nRecognizing textual entailment: Models and applications. Dan Ido Dagan, Mark Roth, Fabio Massimo Sammons, Zanzotto, Synthesis Lectures on Human Language Technologies. 64Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1-220, 2013.\n\n. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, \u0141ukasz Kaiser, arXiv:1807.03819Universal transformers. arXiv preprintMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah Smith, arXiv:2002.06305Finetuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprintJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine- tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020.\n\nAutomatically constructing a corpus of sentential paraphrases. B William, Chris Dolan, Brockett, Proceedings of the Third International Workshop on Paraphrasing (IWP2005). the Third International Workshop on Paraphrasing (IWP2005)William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.\n\nHAWQ: Hessian aware quantization of neural networks with mixed-precision. Zhen Dong, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. HAWQ: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE International Conference on Computer Vision, pages 293-302, 2019.\n\nK Steven, Jeffrey L Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, arXiv:1902.08153Learned step size quantization. arXiv preprintSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n\nAngela Fan, Edouard Grave, Armand Joulin, arXiv:1909.11556Reducing transformer depth on demand with structured dropout. arXiv preprintAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.\n\nTraining with quantization noise for extreme fixed-point compression. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, Armand Joulin, arXiv:2004.07320arXiv preprintAngela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point compression. arXiv preprint arXiv:2004.07320, 2020.\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027arXiv preprintLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nA survey of quantization methods for efficient neural network inference. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, W Michael, Kurt Mahoney, Keutzer, arXiv:2103.13630arXiv preprintAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n\nCompressing bert: Studying the effects of weight pruning on transfer learning. Kevin Mitchell A Gordon, Nicholas Duh, Andrews, arXiv:2002.08307arXiv preprintMitchell A Gordon, Kevin Duh, and Nicholas Andrews. Compressing bert: Studying the effects of weight pruning on transfer learning. arXiv preprint arXiv:2002.08307, 2020.\n\nLearning both weights and connections for efficient neural network. Song Han, Jeff Pool, John Tran, William Dally, Advances in neural information processing systems. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pages 1135-1143, 2015.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Workshop paper in NIPSGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. Workshop paper in NIPS, 2014.\n\nURL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs. Shankar Iyer, Nikhil Dandekar, Kornl Csernai, First quora dataset release: Question pairsShankar Iyer, Nikhil Dandekar, and Kornl Csernai. First quora dataset release: Question pairs.(2017). URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs, 2017.\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert, arXiv:1909.10351Distilling bert for natural language understanding. arXiv preprintXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.\n\nKdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization. Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, Zhiliang Gan, arXiv:2101.05938arXiv preprintJing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and Zhiliang Gan. Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization. arXiv preprint arXiv:2101.05938, 2021.\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, arXiv:1705.03551arXiv preprintMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\n\nI-bert: Integer-only bert quantization. Sehoon Kim, Amir Gholami, Zhewei Yao, W Michael, Kurt Mahoney, Keutzer, International conference on machine learning. PMLRSehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506-5518. PMLR, 2021.\n\nRace: Large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, arXiv:1704.04683arXiv preprintGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\n\nAlbert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.11942arXiv preprintZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Al- bert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n\nOptimal brain damage. Yann Lecun, S John, Sara A Denker, Solla, Advances in neural information processing systems. Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pages 598-605, 1990.\n\nThe winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. CiteseerHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer, 2012.\n\n. Fengfu Li, Bo Zhang, Bin Liu, arXiv:1605.04711Ternary weight networks. arXiv preprintFengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.\n\n. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf, arXiv:1608.08710Pruning filters for efficient convnets. arXiv preprintHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n\nPost-training quantization for vision transformer. Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, Wen Gao, Advances in Neural Information Processing Systems. 342021Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision transformer. Advances in Neural Information Processing Systems, 34, 2021.\n\nExploring the regularity of sparse structure in convolutional neural networks. Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, William J Dally, Workshop paper in CVPRHuizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, and William J Dally. Exploring the regularity of sparse structure in convolutional neural networks. Workshop paper in CVPR, 2017.\n\nYihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, Jing Bai, Ladabert, arXiv:2004.04124Lightweight adaptation of bert through hybrid model compression. arXiv preprintYihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, and Jing Bai. Ladabert: Lightweight adaptation of bert through hybrid model compression. arXiv preprint arXiv:2004.04124, 2020.\n\nBuilding a large annotated corpus of english: The penn treebank. Using Large Corpora. Mary Ann Marcinkiewicz, 273Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using Large Corpora, page 273, 1994.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, International Conference on Learning Representations. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017.\n\nAre sixteen heads really better than one?. Paul Michel, Omer Levy, Graham Neubig, arXiv:1905.10650arXiv preprintPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650, 2019.\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.02789arXiv preprintTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLRMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.\n\nData-free quantization through weight equalization and bias correction. Markus Nagel, Mart Van Baalen, Tijmen Blankevoort, Max Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325-1334, 2019.\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fern\u00e1ndez, arXiv:1606.06031The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprintDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2019.\n\nFixed encoder self-attention patterns in transformer-based machine translation. Alessandro Raganato, Yves Scherrer, J\u00f6rg Tiedemann, arXiv:2002.10260arXiv preprintAlessandro Raganato, Yves Scherrer, and J\u00f6rg Tiedemann. Fixed encoder self-attention patterns in transformer-based machine translation. arXiv preprint arXiv:2002.10260, 2020.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250arXiv preprintPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\nDeepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505-3506, 2020.\n\nUsing tensor cores in cuda fortran. Greg Ruetsch, Nvidia Blog. 2021Greg Ruetsch. Using tensor cores in cuda fortran. Nvidia Blog, 2021.\n\nWinogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732-8740, 2020.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.01108arXiv preprintVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nQ-BERT: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, AAAI. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In AAAI, pages 8815-8821, 2020.\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.11990arXiv preprintShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642, 2013.\n\nSiqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, arXiv:1908.09355Patient knowledge distillation for bert model compression. arXiv preprintSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.\n\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou, arXiv:2004.02984Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprintZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984, 2020.\n\nCompression of generative pre-trained language models via quantization. Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong, arXiv:2203.10705arXiv preprintChaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models via quantization. arXiv preprint arXiv:2203.10705, 2022.\n\nPiqa: An algebra for querying protein data sets. Sandeep Tata, M Jignesh, Patel, 15th International Conference on Scientific and Statistical Database Management. IEEESandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein data sets. In 15th International Conference on Scientific and Statistical Database Management, 2003., pages 141-150. IEEE, 2003.\n\nBert rediscovers the classical nlp pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, arXiv:1905.05950Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv:1905.05950, 2019.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nBen Wang, Aran Komatsuzaki, Gpt-J-6b, A 6 Billion Parameter Autoregressive Language Model. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n\nKernel fusion: An effective method for better power efficiency on multithreaded GPU. Guibin Wang, Yisong Lin, Wei Yi ; Lizhe Wang, Feng Xia, Huajun Chen, Ian Mcloughlin, Shiao-Li Tsao, Mitsuhisa Sato, 2010 IEEE/ACM Int'l Conference on Green Computing and Communications. Sun-Ki Chai, and Irwin KingCPSCom; Hangzhou, ChinaIEEE Computer SocietyGreenCom 2010, & Int'l Conference on CyberGuibin Wang, Yisong Lin, and Wei Yi. Kernel fusion: An effective method for better power efficiency on multithreaded GPU. In Peidong Zhu, Lizhe Wang, Feng Xia, Huajun Chen, Ian McLoughlin, Shiao-Li Tsao, Mitsuhisa Sato, Sun-Ki Chai, and Irwin King, editors, 2010 IEEE/ACM Int'l Conference on Green Computing and Communications, GreenCom 2010, & Int'l Conference on Cyber, Physical and Social Computing, CPSCom 2010, Hangzhou, China, December 18-20, 2010, pages 344-350. IEEE Computer Society, 2010.\n\nMinilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, arXiv:2002.10957arXiv preprintWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint arXiv:2002.10957, 2020.\n\nAlex Warstadt, Amanpreet Singh, Samuel R Bowman, arXiv:1805.12471Neural network acceptability judgments. arXiv preprintAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long PapersAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, 2018.\n\nAdina Williams, Tristan Thrush, Douwe Kiela, arXiv:2010.12729Anlizing the adversarial natural language inference dataset. arXiv preprintAdina Williams, Tristan Thrush, and Douwe Kiela. Anlizing the adversarial natural language inference dataset. arXiv preprint arXiv:2010.12729, 2020.\n\nHuggingFace's Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, ArXiv. 1910Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. HuggingFace's Transformers: State-of-the-art natural language processing. ArXiv, pages arXiv-1910, 2019.\n\nQuick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. Vikas Yadav, Steven Bethard, Mihai Surdeanu, arXiv:1911.07176arXiv preprintVikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. arXiv preprint arXiv:1911.07176, 2019.\n\nMlpruning: A multilevel structured pruning framework for transformer-based models. Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, Michael W Mahoney, arXiv:2105.14636arXiv preprintZhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. Mlpruning: A multilevel structured pruning framework for transformer-based models. arXiv preprint arXiv:2105.14636, 2021.\n\nGobo: Quantizing attentionbased nlp models for low latency and energy efficient inference. Ali Hadi Zadeh, Isak Edo, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEOmar Mohamed Awad, and Andreas MoshovosAli Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention- based nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811-824. IEEE, 2020.\n\nOfir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat, arXiv:1910.06188Q8BERT: Quantized 8bit bert. arXiv preprintOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.\n\nHellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\nRecord: Bridging the gap between human and machine commonsense reading comprehension. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme, arXiv:1810.12885arXiv preprintSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\n\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, arXiv:2009.12812Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprintWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.\n", "annotations": {"author": "[{\"end\":130,\"start\":95},{\"end\":154,\"start\":131},{\"end\":190,\"start\":155},{\"end\":226,\"start\":191},{\"end\":265,\"start\":227},{\"end\":287,\"start\":266}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":102},{\"end\":153,\"start\":144},{\"end\":167,\"start\":162},{\"end\":201,\"start\":199},{\"end\":238,\"start\":236},{\"end\":286,\"start\":277}]", "author_first_name": "[{\"end\":101,\"start\":95},{\"end\":135,\"start\":131},{\"end\":143,\"start\":136},{\"end\":161,\"start\":155},{\"end\":198,\"start\":191},{\"end\":235,\"start\":227},{\"end\":273,\"start\":266},{\"end\":276,\"start\":274}]", "author_affiliation": null, "title": "[{\"end\":92,\"start\":1},{\"end\":379,\"start\":288}]", "venue": null, "abstract": "[{\"end\":1999,\"start\":381}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2155,\"start\":2151},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2204,\"start\":2200},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3210,\"start\":3207},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3213,\"start\":3210},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3255,\"start\":3251},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3258,\"start\":3255},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3514,\"start\":3511},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3727,\"start\":3724},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4404,\"start\":4401},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":4407,\"start\":4404},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7075,\"start\":7071},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7078,\"start\":7075},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7081,\"start\":7078},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7084,\"start\":7081},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7087,\"start\":7084},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7090,\"start\":7087},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7093,\"start\":7090},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7096,\"start\":7093},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7099,\"start\":7096},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":7102,\"start\":7099},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7105,\"start\":7102},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7108,\"start\":7105},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7111,\"start\":7108},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7114,\"start\":7111},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7117,\"start\":7114},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7123,\"start\":7120},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7126,\"start\":7123},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7129,\"start\":7126},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7132,\"start\":7129},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7135,\"start\":7132},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":7455,\"start\":7452},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7577,\"start\":7573},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7813,\"start\":7809},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":7884,\"start\":7880},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7886,\"start\":7884},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7938,\"start\":7934},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7965,\"start\":7961},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8001,\"start\":7997},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8038,\"start\":8034},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8078,\"start\":8074},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8129,\"start\":8125},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8439,\"start\":8435},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8442,\"start\":8439},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8792,\"start\":8788},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8807,\"start\":8803},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9218,\"start\":9215},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9511,\"start\":9508},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10160,\"start\":10156},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10169,\"start\":10165},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10650,\"start\":10647},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13147,\"start\":13144},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13150,\"start\":13147},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13939,\"start\":13935},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":14060,\"start\":14056},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":14491,\"start\":14487},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20419,\"start\":20416},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":20884,\"start\":20880},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":22155,\"start\":22151},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22176,\"start\":22173},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23505,\"start\":23502},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23629,\"start\":23626},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":23749,\"start\":23745},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":23752,\"start\":23749},{\"end\":25597,\"start\":25593},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27993,\"start\":27989},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28028,\"start\":28024},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":32775,\"start\":32771},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":32905,\"start\":32901},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":32945,\"start\":32941},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32983,\"start\":32979},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34493,\"start\":34489},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":35224,\"start\":35220},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35393,\"start\":35389},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":35424,\"start\":35420},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35448,\"start\":35444},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35460,\"start\":35456},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":35472,\"start\":35468},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":35483,\"start\":35479},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35493,\"start\":35489},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":35504,\"start\":35500},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":35515,\"start\":35511},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35525,\"start\":35521},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":35600,\"start\":35596},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36209,\"start\":36205},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36292,\"start\":36288},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":36667,\"start\":36663},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":36681,\"start\":36677},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36696,\"start\":36692},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36707,\"start\":36704},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":36724,\"start\":36720},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":36735,\"start\":36731},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36761,\"start\":36758},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":36783,\"start\":36779},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36800,\"start\":36796},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36813,\"start\":36809},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36825,\"start\":36821},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36835,\"start\":36832},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36845,\"start\":36841},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36855,\"start\":36851},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":36869,\"start\":36865},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":36882,\"start\":36878},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36946,\"start\":36942},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":61729,\"start\":61725},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":61766,\"start\":61762}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":44089,\"start\":43887},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44169,\"start\":44090},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44226,\"start\":44170},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44501,\"start\":44227},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45140,\"start\":44502},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46770,\"start\":45141},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48037,\"start\":46771},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49238,\"start\":48038},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50047,\"start\":49239},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50927,\"start\":50048},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51155,\"start\":50928},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51437,\"start\":51156},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52120,\"start\":51438},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":52688,\"start\":52121},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":52962,\"start\":52689},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":54482,\"start\":52963},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":57422,\"start\":54483},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":59076,\"start\":57423},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":61667,\"start\":59077}]", "paragraph": "[{\"end\":2476,\"start\":2015},{\"end\":3172,\"start\":2478},{\"end\":4108,\"start\":3174},{\"end\":4639,\"start\":4110},{\"end\":5901,\"start\":4641},{\"end\":6995,\"start\":5903},{\"end\":8509,\"start\":7012},{\"end\":10011,\"start\":8511},{\"end\":10239,\"start\":10040},{\"end\":10600,\"start\":10241},{\"end\":12499,\"start\":10602},{\"end\":13371,\"start\":12550},{\"end\":14298,\"start\":13440},{\"end\":15707,\"start\":14300},{\"end\":16756,\"start\":15709},{\"end\":17103,\"start\":16758},{\"end\":17862,\"start\":17166},{\"end\":18558,\"start\":17864},{\"end\":18759,\"start\":18649},{\"end\":19242,\"start\":18761},{\"end\":20131,\"start\":19289},{\"end\":20675,\"start\":20133},{\"end\":21547,\"start\":20677},{\"end\":23115,\"start\":21559},{\"end\":23666,\"start\":23140},{\"end\":23903,\"start\":23668},{\"end\":25121,\"start\":23905},{\"end\":25548,\"start\":25123},{\"end\":27025,\"start\":25587},{\"end\":27354,\"start\":27078},{\"end\":27679,\"start\":27356},{\"end\":28301,\"start\":27723},{\"end\":29193,\"start\":28303},{\"end\":29502,\"start\":29236},{\"end\":29833,\"start\":29504},{\"end\":30253,\"start\":29877},{\"end\":30520,\"start\":30255},{\"end\":31230,\"start\":30522},{\"end\":32216,\"start\":31246},{\"end\":33505,\"start\":32264},{\"end\":33849,\"start\":33537},{\"end\":34655,\"start\":33911},{\"end\":34904,\"start\":34719},{\"end\":34956,\"start\":34906},{\"end\":35008,\"start\":34958},{\"end\":35123,\"start\":35010},{\"end\":35793,\"start\":35154},{\"end\":36032,\"start\":35795},{\"end\":36115,\"start\":36034},{\"end\":37092,\"start\":36117},{\"end\":37297,\"start\":37094},{\"end\":37390,\"start\":37299},{\"end\":37818,\"start\":37433},{\"end\":38281,\"start\":37858},{\"end\":38620,\"start\":38283},{\"end\":39095,\"start\":38622},{\"end\":39186,\"start\":39097},{\"end\":39951,\"start\":39188},{\"end\":41229,\"start\":39953},{\"end\":42070,\"start\":41231},{\"end\":42419,\"start\":42098},{\"end\":42892,\"start\":42443},{\"end\":43228,\"start\":42926},{\"end\":43453,\"start\":43230},{\"end\":43718,\"start\":43455},{\"end\":43886,\"start\":43772}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18648,\"start\":18559},{\"attributes\":{\"id\":\"formula_1\"},\"end\":33910,\"start\":33850}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11014,\"start\":11007},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23195,\"start\":23188},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24733,\"start\":24726},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25686,\"start\":25679},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26655,\"start\":26648},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27162,\"start\":27155},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27948,\"start\":27941},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28976,\"start\":28969},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29537,\"start\":29530},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30555,\"start\":30547},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":37499,\"start\":37492}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2013,\"start\":2001},{\"attributes\":{\"n\":\"2\"},\"end\":7010,\"start\":6998},{\"attributes\":{\"n\":\"3\"},\"end\":10038,\"start\":10014},{\"end\":12548,\"start\":12502},{\"attributes\":{\"n\":\"4\"},\"end\":13385,\"start\":13374},{\"attributes\":{\"n\":\"4.1\"},\"end\":13438,\"start\":13388},{\"attributes\":{\"n\":\"4.2\"},\"end\":17164,\"start\":17106},{\"attributes\":{\"n\":\"4.3\"},\"end\":19287,\"start\":19245},{\"attributes\":{\"n\":\"5\"},\"end\":21557,\"start\":21550},{\"attributes\":{\"n\":\"5.1\"},\"end\":23138,\"start\":23118},{\"attributes\":{\"n\":\"5.2\"},\"end\":25585,\"start\":25551},{\"attributes\":{\"n\":\"5.3\"},\"end\":27076,\"start\":27028},{\"attributes\":{\"n\":\"5.4\"},\"end\":27721,\"start\":27682},{\"attributes\":{\"n\":\"5.5\"},\"end\":29234,\"start\":29196},{\"attributes\":{\"n\":\"5.6\"},\"end\":29875,\"start\":29836},{\"attributes\":{\"n\":\"6\"},\"end\":31244,\"start\":31233},{\"end\":32231,\"start\":32219},{\"end\":32262,\"start\":32234},{\"end\":33535,\"start\":33508},{\"end\":34680,\"start\":34658},{\"end\":34717,\"start\":34683},{\"end\":35152,\"start\":35126},{\"end\":37431,\"start\":37393},{\"end\":37856,\"start\":37821},{\"end\":42096,\"start\":42073},{\"end\":42441,\"start\":42422},{\"end\":42924,\"start\":42895},{\"end\":43770,\"start\":43721},{\"end\":43898,\"start\":43888},{\"end\":44101,\"start\":44091},{\"end\":44184,\"start\":44171},{\"end\":44241,\"start\":44228},{\"end\":44512,\"start\":44503},{\"end\":45151,\"start\":45142},{\"end\":46781,\"start\":46772},{\"end\":48048,\"start\":48039},{\"end\":49249,\"start\":49240},{\"end\":50058,\"start\":50049},{\"end\":50938,\"start\":50929},{\"end\":51166,\"start\":51157},{\"end\":51448,\"start\":51439},{\"end\":52132,\"start\":52122},{\"end\":52703,\"start\":52690},{\"end\":52973,\"start\":52964},{\"end\":54493,\"start\":54484},{\"end\":57433,\"start\":57424},{\"end\":59087,\"start\":59078}]", "table": "[{\"end\":45140,\"start\":44789},{\"end\":46770,\"start\":45153},{\"end\":48037,\"start\":46943},{\"end\":49238,\"start\":48346},{\"end\":50047,\"start\":49389},{\"end\":50927,\"start\":50217},{\"end\":51155,\"start\":51021},{\"end\":51437,\"start\":51308},{\"end\":52120,\"start\":51706},{\"end\":52688,\"start\":52312},{\"end\":52962,\"start\":52787},{\"end\":54482,\"start\":53237},{\"end\":57422,\"start\":55743},{\"end\":59076,\"start\":57470},{\"end\":61667,\"start\":59250}]", "figure_caption": "[{\"end\":44089,\"start\":43900},{\"end\":44169,\"start\":44103},{\"end\":44226,\"start\":44186},{\"end\":44501,\"start\":44243},{\"end\":44789,\"start\":44514},{\"end\":46943,\"start\":46783},{\"end\":48346,\"start\":48050},{\"end\":49389,\"start\":49251},{\"end\":50217,\"start\":50060},{\"end\":51021,\"start\":50940},{\"end\":51308,\"start\":51168},{\"end\":51706,\"start\":51450},{\"end\":52312,\"start\":52135},{\"end\":52787,\"start\":52706},{\"end\":53237,\"start\":52975},{\"end\":55743,\"start\":54495},{\"end\":57470,\"start\":57435},{\"end\":59250,\"start\":59089}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12029,\"start\":12021},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12687,\"start\":12679},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20008,\"start\":20000},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21095,\"start\":21087},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21346,\"start\":21338},{\"end\":33395,\"start\":33387}]", "bib_author_first_name": "[{\"end\":62157,\"start\":62150},{\"end\":62173,\"start\":62166},{\"end\":62183,\"start\":62182},{\"end\":62204,\"start\":62195},{\"end\":62223,\"start\":62218},{\"end\":62239,\"start\":62233},{\"end\":62702,\"start\":62697},{\"end\":62711,\"start\":62708},{\"end\":62721,\"start\":62719},{\"end\":62733,\"start\":62727},{\"end\":62745,\"start\":62741},{\"end\":62754,\"start\":62751},{\"end\":62765,\"start\":62762},{\"end\":62778,\"start\":62771},{\"end\":62789,\"start\":62784},{\"end\":63138,\"start\":63130},{\"end\":63153,\"start\":63147},{\"end\":63163,\"start\":63160},{\"end\":63178,\"start\":63173},{\"end\":63722,\"start\":63719},{\"end\":63736,\"start\":63730},{\"end\":63751,\"start\":63747},{\"end\":63769,\"start\":63762},{\"end\":63787,\"start\":63779},{\"end\":63797,\"start\":63794},{\"end\":63807,\"start\":63803},{\"end\":63822,\"start\":63818},{\"end\":63842,\"start\":63836},{\"end\":63855,\"start\":63850},{\"end\":63868,\"start\":63865},{\"end\":63882,\"start\":63877},{\"end\":63897,\"start\":63890},{\"end\":63915,\"start\":63906},{\"end\":63928,\"start\":63925},{\"end\":63940,\"start\":63936},{\"end\":63953,\"start\":63947},{\"end\":64635,\"start\":64628},{\"end\":64654,\"start\":64648},{\"end\":64668,\"start\":64662},{\"end\":64989,\"start\":64982},{\"end\":65006,\"start\":64999},{\"end\":65026,\"start\":65017},{\"end\":65047,\"start\":65040},{\"end\":65064,\"start\":65056},{\"end\":65076,\"start\":65070},{\"end\":65092,\"start\":65087},{\"end\":65107,\"start\":65100},{\"end\":65130,\"start\":65125},{\"end\":65152,\"start\":65144},{\"end\":65553,\"start\":65545},{\"end\":65571,\"start\":65567},{\"end\":65585,\"start\":65578},{\"end\":65598,\"start\":65593},{\"end\":65616,\"start\":65608},{\"end\":65631,\"start\":65625},{\"end\":65648,\"start\":65642},{\"end\":65668,\"start\":65662},{\"end\":65682,\"start\":65676},{\"end\":66024,\"start\":66018},{\"end\":66036,\"start\":66030},{\"end\":66046,\"start\":66042},{\"end\":66057,\"start\":66053},{\"end\":66068,\"start\":66067},{\"end\":66082,\"start\":66078},{\"end\":66503,\"start\":66497},{\"end\":66513,\"start\":66509},{\"end\":66525,\"start\":66520},{\"end\":66539,\"start\":66534},{\"end\":66559,\"start\":66554},{\"end\":66924,\"start\":66915},{\"end\":66942,\"start\":66936},{\"end\":66956,\"start\":66951},{\"end\":66972,\"start\":66965},{\"end\":66986,\"start\":66980},{\"end\":66999,\"start\":66995},{\"end\":67013,\"start\":67009},{\"end\":67040,\"start\":67033},{\"end\":67057,\"start\":67048},{\"end\":67484,\"start\":67473},{\"end\":67498,\"start\":67492},{\"end\":67512,\"start\":67504},{\"end\":67523,\"start\":67520},{\"end\":67544,\"start\":67537},{\"end\":67562,\"start\":67554},{\"end\":67881,\"start\":67878},{\"end\":67897,\"start\":67893},{\"end\":67909,\"start\":67904},{\"end\":67917,\"start\":67910},{\"end\":68189,\"start\":68182},{\"end\":68207,\"start\":68200},{\"end\":68220,\"start\":68215},{\"end\":68235,\"start\":68230},{\"end\":68253,\"start\":68247},{\"end\":68469,\"start\":68464},{\"end\":68486,\"start\":68478},{\"end\":68500,\"start\":68494},{\"end\":68514,\"start\":68506},{\"end\":68524,\"start\":68515},{\"end\":68830,\"start\":68825},{\"end\":68845,\"start\":68838},{\"end\":68858,\"start\":68855},{\"end\":68872,\"start\":68869},{\"end\":68890,\"start\":68882},{\"end\":68907,\"start\":68903},{\"end\":69337,\"start\":69336},{\"end\":69352,\"start\":69347},{\"end\":69765,\"start\":69761},{\"end\":69778,\"start\":69772},{\"end\":69788,\"start\":69784},{\"end\":69799,\"start\":69798},{\"end\":69813,\"start\":69809},{\"end\":70196,\"start\":70195},{\"end\":70212,\"start\":70205},{\"end\":70214,\"start\":70213},{\"end\":70229,\"start\":70222},{\"end\":70253,\"start\":70241},{\"end\":70275,\"start\":70263},{\"end\":70536,\"start\":70530},{\"end\":70549,\"start\":70542},{\"end\":70563,\"start\":70557},{\"end\":70888,\"start\":70882},{\"end\":70900,\"start\":70894},{\"end\":70916,\"start\":70908},{\"end\":70932,\"start\":70925},{\"end\":70944,\"start\":70940},{\"end\":70961,\"start\":70956},{\"end\":70975,\"start\":70969},{\"end\":71299,\"start\":71296},{\"end\":71311,\"start\":71305},{\"end\":71325,\"start\":71322},{\"end\":71341,\"start\":71333},{\"end\":71357,\"start\":71351},{\"end\":71372,\"start\":71365},{\"end\":71386,\"start\":71381},{\"end\":71400,\"start\":71394},{\"end\":71410,\"start\":71405},{\"end\":71421,\"start\":71418},{\"end\":71790,\"start\":71786},{\"end\":71806,\"start\":71800},{\"end\":71816,\"start\":71812},{\"end\":71829,\"start\":71823},{\"end\":71836,\"start\":71835},{\"end\":71850,\"start\":71846},{\"end\":72182,\"start\":72177},{\"end\":72210,\"start\":72202},{\"end\":72498,\"start\":72494},{\"end\":72508,\"start\":72504},{\"end\":72519,\"start\":72515},{\"end\":72533,\"start\":72526},{\"end\":72843,\"start\":72835},{\"end\":72857,\"start\":72852},{\"end\":72871,\"start\":72867},{\"end\":73104,\"start\":73097},{\"end\":73117,\"start\":73111},{\"end\":73133,\"start\":73128},{\"end\":73374,\"start\":73368},{\"end\":73387,\"start\":73381},{\"end\":73399,\"start\":73393},{\"end\":73410,\"start\":73407},{\"end\":73422,\"start\":73418},{\"end\":73435,\"start\":73429},{\"end\":73444,\"start\":73440},{\"end\":73454,\"start\":73451},{\"end\":73853,\"start\":73849},{\"end\":73862,\"start\":73859},{\"end\":73879,\"start\":73870},{\"end\":73889,\"start\":73884},{\"end\":73903,\"start\":73895},{\"end\":74238,\"start\":74232},{\"end\":74252,\"start\":74246},{\"end\":74260,\"start\":74259},{\"end\":74273,\"start\":74269},{\"end\":74563,\"start\":74557},{\"end\":74573,\"start\":74569},{\"end\":74589,\"start\":74583},{\"end\":74596,\"start\":74595},{\"end\":74610,\"start\":74606},{\"end\":74946,\"start\":74940},{\"end\":74957,\"start\":74952},{\"end\":74970,\"start\":74963},{\"end\":74982,\"start\":74976},{\"end\":74995,\"start\":74989},{\"end\":75292,\"start\":75283},{\"end\":75304,\"start\":75298},{\"end\":75320,\"start\":75311},{\"end\":75335,\"start\":75330},{\"end\":75350,\"start\":75344},{\"end\":75363,\"start\":75359},{\"end\":75643,\"start\":75639},{\"end\":75652,\"start\":75651},{\"end\":75663,\"start\":75659},{\"end\":75665,\"start\":75664},{\"end\":75912,\"start\":75906},{\"end\":75929,\"start\":75923},{\"end\":75942,\"start\":75937},{\"end\":76271,\"start\":76265},{\"end\":76278,\"start\":76276},{\"end\":76289,\"start\":76286},{\"end\":76454,\"start\":76451},{\"end\":76463,\"start\":76459},{\"end\":76475,\"start\":76471},{\"end\":76493,\"start\":76488},{\"end\":76505,\"start\":76501},{\"end\":76511,\"start\":76506},{\"end\":76797,\"start\":76790},{\"end\":76808,\"start\":76803},{\"end\":76818,\"start\":76815},{\"end\":76827,\"start\":76824},{\"end\":76840,\"start\":76835},{\"end\":76848,\"start\":76845},{\"end\":77176,\"start\":77171},{\"end\":77186,\"start\":77182},{\"end\":77196,\"start\":77192},{\"end\":77210,\"start\":77203},{\"end\":77221,\"start\":77215},{\"end\":77229,\"start\":77227},{\"end\":77243,\"start\":77236},{\"end\":77245,\"start\":77244},{\"end\":77477,\"start\":77471},{\"end\":77489,\"start\":77483},{\"end\":77502,\"start\":77496},{\"end\":77511,\"start\":77507},{\"end\":77523,\"start\":77519},{\"end\":77536,\"start\":77530},{\"end\":77549,\"start\":77543},{\"end\":77563,\"start\":77557},{\"end\":77574,\"start\":77570},{\"end\":78003,\"start\":77999},{\"end\":78007,\"start\":78004},{\"end\":78193,\"start\":78186},{\"end\":78209,\"start\":78202},{\"end\":78222,\"start\":78217},{\"end\":78240,\"start\":78233},{\"end\":78514,\"start\":78510},{\"end\":78527,\"start\":78523},{\"end\":78540,\"start\":78534},{\"end\":78798,\"start\":78793},{\"end\":78814,\"start\":78809},{\"end\":78828,\"start\":78822},{\"end\":78841,\"start\":78835},{\"end\":79144,\"start\":79138},{\"end\":79156,\"start\":79152},{\"end\":79160,\"start\":79157},{\"end\":79172,\"start\":79168},{\"end\":79193,\"start\":79185},{\"end\":79209,\"start\":79203},{\"end\":79581,\"start\":79575},{\"end\":79593,\"start\":79589},{\"end\":79612,\"start\":79606},{\"end\":79629,\"start\":79626},{\"end\":80013,\"start\":80008},{\"end\":80029,\"start\":80023},{\"end\":80050,\"start\":80042},{\"end\":80066,\"start\":80062},{\"end\":80082,\"start\":80073},{\"end\":80095,\"start\":80089},{\"end\":80111,\"start\":80106},{\"end\":80127,\"start\":80122},{\"end\":80142,\"start\":80136},{\"end\":80594,\"start\":80590},{\"end\":80608,\"start\":80604},{\"end\":80618,\"start\":80613},{\"end\":80631,\"start\":80626},{\"end\":80643,\"start\":80638},{\"end\":80656,\"start\":80652},{\"end\":80898,\"start\":80893},{\"end\":80911,\"start\":80907},{\"end\":80925,\"start\":80921},{\"end\":80944,\"start\":80935},{\"end\":80956,\"start\":80950},{\"end\":80972,\"start\":80965},{\"end\":80986,\"start\":80981},{\"end\":80996,\"start\":80993},{\"end\":81006,\"start\":81001},{\"end\":81008,\"start\":81007},{\"end\":81320,\"start\":81310},{\"end\":81335,\"start\":81331},{\"end\":81350,\"start\":81346},{\"end\":81635,\"start\":81629},{\"end\":81651,\"start\":81647},{\"end\":81669,\"start\":81659},{\"end\":81684,\"start\":81679},{\"end\":81997,\"start\":81993},{\"end\":82012,\"start\":82006},{\"end\":82034,\"start\":82026},{\"end\":82050,\"start\":82043},{\"end\":82568,\"start\":82564},{\"end\":82735,\"start\":82728},{\"end\":82749,\"start\":82747},{\"end\":82764,\"start\":82757},{\"end\":82776,\"start\":82771},{\"end\":83227,\"start\":83221},{\"end\":83242,\"start\":83234},{\"end\":83256,\"start\":83250},{\"end\":83273,\"start\":83267},{\"end\":83561,\"start\":83556},{\"end\":83572,\"start\":83568},{\"end\":83584,\"start\":83579},{\"end\":83596,\"start\":83589},{\"end\":83607,\"start\":83601},{\"end\":83617,\"start\":83613},{\"end\":83628,\"start\":83627},{\"end\":83642,\"start\":83638},{\"end\":83983,\"start\":83977},{\"end\":83998,\"start\":83991},{\"end\":84015,\"start\":84008},{\"end\":84031,\"start\":84024},{\"end\":84049,\"start\":84043},{\"end\":84068,\"start\":84063},{\"end\":84081,\"start\":84077},{\"end\":84094,\"start\":84087},{\"end\":84113,\"start\":84107},{\"end\":84128,\"start\":84123},{\"end\":84575,\"start\":84568},{\"end\":84588,\"start\":84584},{\"end\":84604,\"start\":84600},{\"end\":84614,\"start\":84609},{\"end\":84624,\"start\":84623},{\"end\":84648,\"start\":84647},{\"end\":84668,\"start\":84657},{\"end\":85151,\"start\":85147},{\"end\":85159,\"start\":85157},{\"end\":85170,\"start\":85167},{\"end\":85184,\"start\":85176},{\"end\":85432,\"start\":85425},{\"end\":85445,\"start\":85438},{\"end\":85457,\"start\":85450},{\"end\":85470,\"start\":85464},{\"end\":85482,\"start\":85476},{\"end\":85494,\"start\":85489},{\"end\":85872,\"start\":85865},{\"end\":85880,\"start\":85878},{\"end\":85889,\"start\":85886},{\"end\":85903,\"start\":85897},{\"end\":85914,\"start\":85911},{\"end\":85925,\"start\":85922},{\"end\":85935,\"start\":85931},{\"end\":85945,\"start\":85941},{\"end\":86241,\"start\":86234},{\"end\":86249,\"start\":86248},{\"end\":86601,\"start\":86598},{\"end\":86618,\"start\":86610},{\"end\":86629,\"start\":86624},{\"end\":86803,\"start\":86797},{\"end\":86817,\"start\":86813},{\"end\":86831,\"start\":86827},{\"end\":86845,\"start\":86840},{\"end\":86862,\"start\":86857},{\"end\":86875,\"start\":86870},{\"end\":86877,\"start\":86876},{\"end\":86891,\"start\":86885},{\"end\":86905,\"start\":86900},{\"end\":87203,\"start\":87199},{\"end\":87219,\"start\":87210},{\"end\":87233,\"start\":87227},{\"end\":87248,\"start\":87243},{\"end\":87259,\"start\":87255},{\"end\":87274,\"start\":87266},{\"end\":87618,\"start\":87615},{\"end\":87629,\"start\":87625},{\"end\":87954,\"start\":87948},{\"end\":87967,\"start\":87961},{\"end\":87987,\"start\":87973},{\"end\":87998,\"start\":87994},{\"end\":88010,\"start\":88004},{\"end\":88020,\"start\":88017},{\"end\":88041,\"start\":88033},{\"end\":88057,\"start\":88048},{\"end\":88853,\"start\":88847},{\"end\":88864,\"start\":88860},{\"end\":88872,\"start\":88870},{\"end\":88885,\"start\":88879},{\"end\":88894,\"start\":88891},{\"end\":88905,\"start\":88901},{\"end\":89155,\"start\":89151},{\"end\":89175,\"start\":89166},{\"end\":89191,\"start\":89183},{\"end\":89488,\"start\":89483},{\"end\":89505,\"start\":89499},{\"end\":89520,\"start\":89514},{\"end\":90142,\"start\":90137},{\"end\":90160,\"start\":90153},{\"end\":90174,\"start\":90169},{\"end\":90503,\"start\":90497},{\"end\":90518,\"start\":90510},{\"end\":90532,\"start\":90526},{\"end\":90545,\"start\":90539},{\"end\":90563,\"start\":90556},{\"end\":90581,\"start\":90574},{\"end\":90594,\"start\":90587},{\"end\":90606,\"start\":90603},{\"end\":90618,\"start\":90614},{\"end\":90631,\"start\":90625},{\"end\":91028,\"start\":91023},{\"end\":91042,\"start\":91036},{\"end\":91057,\"start\":91052},{\"end\":91386,\"start\":91380},{\"end\":91399,\"start\":91392},{\"end\":91409,\"start\":91404},{\"end\":91420,\"start\":91416},{\"end\":91437,\"start\":91430},{\"end\":91439,\"start\":91438},{\"end\":91769,\"start\":91766},{\"end\":91786,\"start\":91782},{\"end\":92190,\"start\":92186},{\"end\":92202,\"start\":92199},{\"end\":92218,\"start\":92213},{\"end\":92231,\"start\":92226},{\"end\":92494,\"start\":92489},{\"end\":92507,\"start\":92504},{\"end\":92525,\"start\":92518},{\"end\":92535,\"start\":92532},{\"end\":92550,\"start\":92545},{\"end\":92844,\"start\":92839},{\"end\":92860,\"start\":92852},{\"end\":92874,\"start\":92866},{\"end\":92888,\"start\":92880},{\"end\":92899,\"start\":92894},{\"end\":92913,\"start\":92905},{\"end\":93174,\"start\":93171},{\"end\":93184,\"start\":93182},{\"end\":93196,\"start\":93190},{\"end\":93208,\"start\":93202},{\"end\":93220,\"start\":93216},{\"end\":93230,\"start\":93227},{\"end\":93241,\"start\":93238}]", "bib_author_last_name": "[{\"end\":62164,\"start\":62158},{\"end\":62180,\"start\":62174},{\"end\":62193,\"start\":62184},{\"end\":62216,\"start\":62205},{\"end\":62231,\"start\":62224},{\"end\":62242,\"start\":62240},{\"end\":62247,\"start\":62244},{\"end\":62706,\"start\":62703},{\"end\":62717,\"start\":62712},{\"end\":62725,\"start\":62722},{\"end\":62739,\"start\":62734},{\"end\":62749,\"start\":62746},{\"end\":62760,\"start\":62755},{\"end\":62769,\"start\":62766},{\"end\":62782,\"start\":62779},{\"end\":62794,\"start\":62790},{\"end\":63145,\"start\":63139},{\"end\":63158,\"start\":63154},{\"end\":63171,\"start\":63164},{\"end\":63184,\"start\":63179},{\"end\":63728,\"start\":63723},{\"end\":63745,\"start\":63737},{\"end\":63760,\"start\":63752},{\"end\":63777,\"start\":63770},{\"end\":63792,\"start\":63788},{\"end\":63801,\"start\":63798},{\"end\":63816,\"start\":63808},{\"end\":63834,\"start\":63823},{\"end\":63848,\"start\":63843},{\"end\":63863,\"start\":63856},{\"end\":63875,\"start\":63869},{\"end\":63888,\"start\":63883},{\"end\":63904,\"start\":63898},{\"end\":63923,\"start\":63916},{\"end\":63934,\"start\":63929},{\"end\":63945,\"start\":63941},{\"end\":63962,\"start\":63954},{\"end\":63972,\"start\":63964},{\"end\":64646,\"start\":64636},{\"end\":64660,\"start\":64655},{\"end\":64680,\"start\":64669},{\"end\":64997,\"start\":64990},{\"end\":65015,\"start\":65007},{\"end\":65038,\"start\":65027},{\"end\":65054,\"start\":65048},{\"end\":65068,\"start\":65065},{\"end\":65085,\"start\":65077},{\"end\":65098,\"start\":65093},{\"end\":65123,\"start\":65108},{\"end\":65142,\"start\":65131},{\"end\":65159,\"start\":65153},{\"end\":65565,\"start\":65554},{\"end\":65576,\"start\":65572},{\"end\":65591,\"start\":65586},{\"end\":65606,\"start\":65599},{\"end\":65623,\"start\":65617},{\"end\":65640,\"start\":65632},{\"end\":65660,\"start\":65649},{\"end\":65674,\"start\":65669},{\"end\":65689,\"start\":65683},{\"end\":65697,\"start\":65691},{\"end\":66028,\"start\":66025},{\"end\":66040,\"start\":66037},{\"end\":66051,\"start\":66047},{\"end\":66065,\"start\":66058},{\"end\":66076,\"start\":66069},{\"end\":66090,\"start\":66083},{\"end\":66099,\"start\":66092},{\"end\":66507,\"start\":66504},{\"end\":66518,\"start\":66514},{\"end\":66532,\"start\":66526},{\"end\":66552,\"start\":66540},{\"end\":66566,\"start\":66560},{\"end\":66934,\"start\":66925},{\"end\":66949,\"start\":66943},{\"end\":66963,\"start\":66957},{\"end\":66978,\"start\":66973},{\"end\":66993,\"start\":66987},{\"end\":67007,\"start\":67000},{\"end\":67020,\"start\":67014},{\"end\":67031,\"start\":67022},{\"end\":67046,\"start\":67041},{\"end\":67064,\"start\":67058},{\"end\":67074,\"start\":67066},{\"end\":67490,\"start\":67485},{\"end\":67502,\"start\":67499},{\"end\":67518,\"start\":67513},{\"end\":67535,\"start\":67524},{\"end\":67552,\"start\":67545},{\"end\":67572,\"start\":67563},{\"end\":67891,\"start\":67882},{\"end\":67902,\"start\":67898},{\"end\":67925,\"start\":67918},{\"end\":67935,\"start\":67927},{\"end\":68198,\"start\":68190},{\"end\":68213,\"start\":68208},{\"end\":68228,\"start\":68221},{\"end\":68245,\"start\":68236},{\"end\":68260,\"start\":68254},{\"end\":68476,\"start\":68470},{\"end\":68492,\"start\":68487},{\"end\":68504,\"start\":68501},{\"end\":68529,\"start\":68525},{\"end\":68836,\"start\":68831},{\"end\":68853,\"start\":68846},{\"end\":68867,\"start\":68859},{\"end\":68880,\"start\":68873},{\"end\":68901,\"start\":68891},{\"end\":68913,\"start\":68908},{\"end\":69345,\"start\":69338},{\"end\":69358,\"start\":69353},{\"end\":69368,\"start\":69360},{\"end\":69770,\"start\":69766},{\"end\":69782,\"start\":69779},{\"end\":69796,\"start\":69789},{\"end\":69807,\"start\":69800},{\"end\":69821,\"start\":69814},{\"end\":69830,\"start\":69823},{\"end\":70203,\"start\":70197},{\"end\":70220,\"start\":70215},{\"end\":70239,\"start\":70230},{\"end\":70261,\"start\":70254},{\"end\":70285,\"start\":70276},{\"end\":70292,\"start\":70287},{\"end\":70540,\"start\":70537},{\"end\":70555,\"start\":70550},{\"end\":70570,\"start\":70564},{\"end\":70892,\"start\":70889},{\"end\":70906,\"start\":70901},{\"end\":70923,\"start\":70917},{\"end\":70938,\"start\":70933},{\"end\":70954,\"start\":70945},{\"end\":70967,\"start\":70962},{\"end\":70982,\"start\":70976},{\"end\":71303,\"start\":71300},{\"end\":71320,\"start\":71312},{\"end\":71331,\"start\":71326},{\"end\":71349,\"start\":71342},{\"end\":71363,\"start\":71358},{\"end\":71379,\"start\":71373},{\"end\":71392,\"start\":71387},{\"end\":71403,\"start\":71401},{\"end\":71416,\"start\":71411},{\"end\":71431,\"start\":71422},{\"end\":71798,\"start\":71791},{\"end\":71810,\"start\":71807},{\"end\":71821,\"start\":71817},{\"end\":71833,\"start\":71830},{\"end\":71844,\"start\":71837},{\"end\":71858,\"start\":71851},{\"end\":71867,\"start\":71860},{\"end\":72200,\"start\":72183},{\"end\":72214,\"start\":72211},{\"end\":72223,\"start\":72216},{\"end\":72502,\"start\":72499},{\"end\":72513,\"start\":72509},{\"end\":72524,\"start\":72520},{\"end\":72539,\"start\":72534},{\"end\":72850,\"start\":72844},{\"end\":72865,\"start\":72858},{\"end\":72876,\"start\":72872},{\"end\":73109,\"start\":73105},{\"end\":73126,\"start\":73118},{\"end\":73141,\"start\":73134},{\"end\":73379,\"start\":73375},{\"end\":73391,\"start\":73388},{\"end\":73405,\"start\":73400},{\"end\":73416,\"start\":73411},{\"end\":73427,\"start\":73423},{\"end\":73438,\"start\":73436},{\"end\":73449,\"start\":73445},{\"end\":73458,\"start\":73455},{\"end\":73468,\"start\":73460},{\"end\":73857,\"start\":73854},{\"end\":73868,\"start\":73863},{\"end\":73882,\"start\":73880},{\"end\":73893,\"start\":73890},{\"end\":73907,\"start\":73904},{\"end\":74244,\"start\":74239},{\"end\":74257,\"start\":74253},{\"end\":74267,\"start\":74261},{\"end\":74278,\"start\":74274},{\"end\":74291,\"start\":74280},{\"end\":74567,\"start\":74564},{\"end\":74581,\"start\":74574},{\"end\":74593,\"start\":74590},{\"end\":74604,\"start\":74597},{\"end\":74618,\"start\":74611},{\"end\":74627,\"start\":74620},{\"end\":74950,\"start\":74947},{\"end\":74961,\"start\":74958},{\"end\":74974,\"start\":74971},{\"end\":74987,\"start\":74983},{\"end\":75000,\"start\":74996},{\"end\":75296,\"start\":75293},{\"end\":75309,\"start\":75305},{\"end\":75328,\"start\":75321},{\"end\":75342,\"start\":75336},{\"end\":75357,\"start\":75351},{\"end\":75371,\"start\":75364},{\"end\":75649,\"start\":75644},{\"end\":75657,\"start\":75653},{\"end\":75672,\"start\":75666},{\"end\":75679,\"start\":75674},{\"end\":75921,\"start\":75913},{\"end\":75935,\"start\":75930},{\"end\":75954,\"start\":75943},{\"end\":76274,\"start\":76272},{\"end\":76284,\"start\":76279},{\"end\":76293,\"start\":76290},{\"end\":76457,\"start\":76455},{\"end\":76469,\"start\":76464},{\"end\":76486,\"start\":76476},{\"end\":76499,\"start\":76494},{\"end\":76516,\"start\":76512},{\"end\":76801,\"start\":76798},{\"end\":76813,\"start\":76809},{\"end\":76822,\"start\":76819},{\"end\":76833,\"start\":76828},{\"end\":76843,\"start\":76841},{\"end\":76852,\"start\":76849},{\"end\":77180,\"start\":77177},{\"end\":77190,\"start\":77187},{\"end\":77201,\"start\":77197},{\"end\":77213,\"start\":77211},{\"end\":77225,\"start\":77222},{\"end\":77234,\"start\":77230},{\"end\":77251,\"start\":77246},{\"end\":77481,\"start\":77478},{\"end\":77494,\"start\":77490},{\"end\":77505,\"start\":77503},{\"end\":77517,\"start\":77512},{\"end\":77528,\"start\":77524},{\"end\":77541,\"start\":77537},{\"end\":77555,\"start\":77550},{\"end\":77568,\"start\":77564},{\"end\":77578,\"start\":77575},{\"end\":77588,\"start\":77580},{\"end\":78021,\"start\":78008},{\"end\":78200,\"start\":78194},{\"end\":78215,\"start\":78210},{\"end\":78231,\"start\":78223},{\"end\":78247,\"start\":78241},{\"end\":78521,\"start\":78515},{\"end\":78532,\"start\":78528},{\"end\":78547,\"start\":78541},{\"end\":78807,\"start\":78799},{\"end\":78820,\"start\":78815},{\"end\":78833,\"start\":78829},{\"end\":78851,\"start\":78842},{\"end\":79150,\"start\":79145},{\"end\":79166,\"start\":79161},{\"end\":79183,\"start\":79173},{\"end\":79201,\"start\":79194},{\"end\":79221,\"start\":79210},{\"end\":79587,\"start\":79582},{\"end\":79604,\"start\":79594},{\"end\":79624,\"start\":79613},{\"end\":79637,\"start\":79630},{\"end\":80021,\"start\":80014},{\"end\":80040,\"start\":80030},{\"end\":80060,\"start\":80051},{\"end\":80071,\"start\":80067},{\"end\":80087,\"start\":80083},{\"end\":80104,\"start\":80096},{\"end\":80120,\"start\":80112},{\"end\":80134,\"start\":80128},{\"end\":80149,\"start\":80143},{\"end\":80160,\"start\":80151},{\"end\":80602,\"start\":80595},{\"end\":80611,\"start\":80609},{\"end\":80624,\"start\":80619},{\"end\":80636,\"start\":80632},{\"end\":80650,\"start\":80644},{\"end\":80666,\"start\":80657},{\"end\":80905,\"start\":80899},{\"end\":80919,\"start\":80912},{\"end\":80933,\"start\":80926},{\"end\":80948,\"start\":80945},{\"end\":80963,\"start\":80957},{\"end\":80979,\"start\":80973},{\"end\":80991,\"start\":80987},{\"end\":80999,\"start\":80997},{\"end\":81012,\"start\":81009},{\"end\":81329,\"start\":81321},{\"end\":81344,\"start\":81336},{\"end\":81360,\"start\":81351},{\"end\":81645,\"start\":81636},{\"end\":81657,\"start\":81652},{\"end\":81677,\"start\":81670},{\"end\":81690,\"start\":81685},{\"end\":82004,\"start\":81998},{\"end\":82024,\"start\":82013},{\"end\":82041,\"start\":82035},{\"end\":82053,\"start\":82051},{\"end\":82576,\"start\":82569},{\"end\":82745,\"start\":82736},{\"end\":82755,\"start\":82750},{\"end\":82769,\"start\":82765},{\"end\":82788,\"start\":82777},{\"end\":82794,\"start\":82790},{\"end\":83232,\"start\":83228},{\"end\":83248,\"start\":83243},{\"end\":83265,\"start\":83257},{\"end\":83278,\"start\":83274},{\"end\":83566,\"start\":83562},{\"end\":83577,\"start\":83573},{\"end\":83587,\"start\":83585},{\"end\":83599,\"start\":83597},{\"end\":83611,\"start\":83608},{\"end\":83625,\"start\":83618},{\"end\":83636,\"start\":83629},{\"end\":83650,\"start\":83643},{\"end\":83659,\"start\":83652},{\"end\":83989,\"start\":83984},{\"end\":84006,\"start\":83999},{\"end\":84022,\"start\":84016},{\"end\":84041,\"start\":84032},{\"end\":84061,\"start\":84050},{\"end\":84075,\"start\":84069},{\"end\":84085,\"start\":84082},{\"end\":84105,\"start\":84095},{\"end\":84121,\"start\":84114},{\"end\":84140,\"start\":84129},{\"end\":84582,\"start\":84576},{\"end\":84598,\"start\":84589},{\"end\":84607,\"start\":84605},{\"end\":84621,\"start\":84615},{\"end\":84636,\"start\":84625},{\"end\":84645,\"start\":84638},{\"end\":84655,\"start\":84649},{\"end\":84671,\"start\":84669},{\"end\":84678,\"start\":84673},{\"end\":85155,\"start\":85152},{\"end\":85165,\"start\":85160},{\"end\":85174,\"start\":85171},{\"end\":85188,\"start\":85185},{\"end\":85436,\"start\":85433},{\"end\":85448,\"start\":85446},{\"end\":85462,\"start\":85458},{\"end\":85474,\"start\":85471},{\"end\":85487,\"start\":85483},{\"end\":85499,\"start\":85495},{\"end\":85876,\"start\":85873},{\"end\":85884,\"start\":85881},{\"end\":85895,\"start\":85890},{\"end\":85909,\"start\":85904},{\"end\":85920,\"start\":85915},{\"end\":85929,\"start\":85926},{\"end\":85939,\"start\":85936},{\"end\":85950,\"start\":85946},{\"end\":86246,\"start\":86242},{\"end\":86257,\"start\":86250},{\"end\":86264,\"start\":86259},{\"end\":86608,\"start\":86602},{\"end\":86622,\"start\":86619},{\"end\":86637,\"start\":86630},{\"end\":86811,\"start\":86804},{\"end\":86825,\"start\":86818},{\"end\":86838,\"start\":86832},{\"end\":86855,\"start\":86846},{\"end\":86868,\"start\":86863},{\"end\":86883,\"start\":86878},{\"end\":86898,\"start\":86892},{\"end\":86916,\"start\":86906},{\"end\":87208,\"start\":87204},{\"end\":87225,\"start\":87220},{\"end\":87241,\"start\":87234},{\"end\":87253,\"start\":87249},{\"end\":87264,\"start\":87260},{\"end\":87281,\"start\":87275},{\"end\":87623,\"start\":87619},{\"end\":87641,\"start\":87630},{\"end\":87651,\"start\":87643},{\"end\":87959,\"start\":87955},{\"end\":87971,\"start\":87968},{\"end\":87992,\"start\":87988},{\"end\":88002,\"start\":87999},{\"end\":88015,\"start\":88011},{\"end\":88031,\"start\":88021},{\"end\":88046,\"start\":88042},{\"end\":88062,\"start\":88058},{\"end\":88858,\"start\":88854},{\"end\":88868,\"start\":88865},{\"end\":88877,\"start\":88873},{\"end\":88889,\"start\":88886},{\"end\":88899,\"start\":88895},{\"end\":88910,\"start\":88906},{\"end\":89164,\"start\":89156},{\"end\":89181,\"start\":89176},{\"end\":89198,\"start\":89192},{\"end\":89497,\"start\":89489},{\"end\":89512,\"start\":89506},{\"end\":89527,\"start\":89521},{\"end\":90151,\"start\":90143},{\"end\":90167,\"start\":90161},{\"end\":90180,\"start\":90175},{\"end\":90508,\"start\":90504},{\"end\":90524,\"start\":90519},{\"end\":90537,\"start\":90533},{\"end\":90554,\"start\":90546},{\"end\":90572,\"start\":90564},{\"end\":90585,\"start\":90582},{\"end\":90601,\"start\":90595},{\"end\":90612,\"start\":90607},{\"end\":90623,\"start\":90619},{\"end\":90641,\"start\":90632},{\"end\":91034,\"start\":91029},{\"end\":91050,\"start\":91043},{\"end\":91066,\"start\":91058},{\"end\":91390,\"start\":91387},{\"end\":91402,\"start\":91400},{\"end\":91414,\"start\":91410},{\"end\":91428,\"start\":91421},{\"end\":91447,\"start\":91440},{\"end\":91780,\"start\":91770},{\"end\":91790,\"start\":91787},{\"end\":92197,\"start\":92191},{\"end\":92211,\"start\":92203},{\"end\":92224,\"start\":92219},{\"end\":92242,\"start\":92232},{\"end\":92502,\"start\":92495},{\"end\":92516,\"start\":92508},{\"end\":92530,\"start\":92526},{\"end\":92543,\"start\":92536},{\"end\":92555,\"start\":92551},{\"end\":92850,\"start\":92845},{\"end\":92864,\"start\":92861},{\"end\":92878,\"start\":92875},{\"end\":92892,\"start\":92889},{\"end\":92903,\"start\":92900},{\"end\":92923,\"start\":92914},{\"end\":93180,\"start\":93175},{\"end\":93188,\"start\":93185},{\"end\":93200,\"start\":93197},{\"end\":93214,\"start\":93209},{\"end\":93225,\"start\":93221},{\"end\":93236,\"start\":93231},{\"end\":93245,\"start\":93242}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3828276},\"end\":62695,\"start\":62094},{\"attributes\":{\"doi\":\"arXiv:2012.15701\",\"id\":\"b1\"},\"end\":63071,\"start\":62697},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6401679},\"end\":63717,\"start\":63073},{\"attributes\":{\"id\":\"b3\"},\"end\":64352,\"start\":63719},{\"attributes\":{\"id\":\"b4\"},\"end\":64543,\"start\":64354},{\"attributes\":{\"doi\":\"arXiv:2109.12948\",\"id\":\"b5\"},\"end\":64891,\"start\":64545},{\"attributes\":{\"doi\":\"arXiv:1806.00358\",\"id\":\"b6\"},\"end\":65504,\"start\":64893},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b7\"},\"end\":65967,\"start\":65506},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":209531713},\"end\":66495,\"start\":65969},{\"attributes\":{\"doi\":\"arXiv:1708.00055\",\"id\":\"b9\"},\"end\":66913,\"start\":66497},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b10\"},\"end\":67399,\"start\":66915},{\"attributes\":{\"doi\":\"arXiv:1905.10044\",\"id\":\"b11\"},\"end\":67819,\"start\":67401},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":61205942},\"end\":68178,\"start\":67821},{\"attributes\":{\"doi\":\"arXiv:1807.03819\",\"id\":\"b13\"},\"end\":68462,\"start\":68180},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b14\"},\"end\":68823,\"start\":68464},{\"attributes\":{\"doi\":\"arXiv:2002.06305\",\"id\":\"b15\"},\"end\":69271,\"start\":68825},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16639476},\"end\":69685,\"start\":69273},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":148571720},\"end\":70193,\"start\":69687},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b18\"},\"end\":70528,\"start\":70195},{\"attributes\":{\"doi\":\"arXiv:1909.11556\",\"id\":\"b19\"},\"end\":70810,\"start\":70530},{\"attributes\":{\"doi\":\"arXiv:2004.07320\",\"id\":\"b20\"},\"end\":71228,\"start\":70812},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b21\"},\"end\":71711,\"start\":71230},{\"attributes\":{\"doi\":\"arXiv:2103.13630\",\"id\":\"b22\"},\"end\":72096,\"start\":71713},{\"attributes\":{\"doi\":\"arXiv:2002.08307\",\"id\":\"b23\"},\"end\":72424,\"start\":72098},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2238772},\"end\":72787,\"start\":72426},{\"attributes\":{\"id\":\"b25\"},\"end\":73022,\"start\":72789},{\"attributes\":{\"id\":\"b26\"},\"end\":73366,\"start\":73024},{\"attributes\":{\"doi\":\"arXiv:1909.10351\",\"id\":\"b27\"},\"end\":73748,\"start\":73368},{\"attributes\":{\"doi\":\"arXiv:2101.05938\",\"id\":\"b28\"},\"end\":74140,\"start\":73750},{\"attributes\":{\"doi\":\"arXiv:1705.03551\",\"id\":\"b29\"},\"end\":74515,\"start\":74142},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":230523998},\"end\":74871,\"start\":74517},{\"attributes\":{\"doi\":\"arXiv:1704.04683\",\"id\":\"b31\"},\"end\":75203,\"start\":74873},{\"attributes\":{\"doi\":\"arXiv:1909.11942\",\"id\":\"b32\"},\"end\":75615,\"start\":75205},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7785881},\"end\":75873,\"start\":75617},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15710851},\"end\":76261,\"start\":75875},{\"attributes\":{\"doi\":\"arXiv:1605.04711\",\"id\":\"b35\"},\"end\":76447,\"start\":76263},{\"attributes\":{\"doi\":\"arXiv:1608.08710\",\"id\":\"b36\"},\"end\":76737,\"start\":76449},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235658553},\"end\":77090,\"start\":76739},{\"attributes\":{\"id\":\"b38\"},\"end\":77469,\"start\":77092},{\"attributes\":{\"doi\":\"arXiv:2004.04124\",\"id\":\"b39\"},\"end\":77911,\"start\":77471},{\"attributes\":{\"id\":\"b40\"},\"end\":78151,\"start\":77913},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16299141},\"end\":78465,\"start\":78153},{\"attributes\":{\"doi\":\"arXiv:1905.10650\",\"id\":\"b42\"},\"end\":78702,\"start\":78467},{\"attributes\":{\"doi\":\"arXiv:1809.02789\",\"id\":\"b43\"},\"end\":79074,\"start\":78704},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":216056295},\"end\":79501,\"start\":79076},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":184487878},\"end\":80006,\"start\":79503},{\"attributes\":{\"doi\":\"arXiv:1606.06031\",\"id\":\"b46\"},\"end\":80535,\"start\":80008},{\"attributes\":{\"id\":\"b47\"},\"end\":80808,\"start\":80537},{\"attributes\":{\"id\":\"b48\"},\"end\":81228,\"start\":80810},{\"attributes\":{\"doi\":\"arXiv:2002.10260\",\"id\":\"b49\"},\"end\":81566,\"start\":81230},{\"attributes\":{\"doi\":\"arXiv:1606.05250\",\"id\":\"b50\"},\"end\":81888,\"start\":81568},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":221191193},\"end\":82526,\"start\":81890},{\"attributes\":{\"id\":\"b52\"},\"end\":82663,\"start\":82528},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":198893658},\"end\":83140,\"start\":82665},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b54\"},\"end\":83490,\"start\":83142},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":202565587},\"end\":83870,\"start\":83492},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b56\"},\"end\":84487,\"start\":83872},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":990233},\"end\":85145,\"start\":84489},{\"attributes\":{\"doi\":\"arXiv:1908.09355\",\"id\":\"b58\"},\"end\":85423,\"start\":85147},{\"attributes\":{\"doi\":\"arXiv:2004.02984\",\"id\":\"b59\"},\"end\":85791,\"start\":85425},{\"attributes\":{\"doi\":\"arXiv:2203.10705\",\"id\":\"b60\"},\"end\":86183,\"start\":85793},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1545102},\"end\":86551,\"start\":86185},{\"attributes\":{\"doi\":\"arXiv:1905.05950\",\"id\":\"b62\"},\"end\":86768,\"start\":86553},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":13756489},\"end\":87197,\"start\":86770},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b64\"},\"end\":87613,\"start\":87199},{\"attributes\":{\"id\":\"b65\"},\"end\":87861,\"start\":87615},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":16815665},\"end\":88745,\"start\":87863},{\"attributes\":{\"doi\":\"arXiv:2002.10957\",\"id\":\"b67\"},\"end\":89149,\"start\":88747},{\"attributes\":{\"doi\":\"arXiv:1805.12471\",\"id\":\"b68\"},\"end\":89401,\"start\":89151},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":3432876},\"end\":90135,\"start\":89403},{\"attributes\":{\"doi\":\"arXiv:2010.12729\",\"id\":\"b70\"},\"end\":90421,\"start\":90137},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":204509627},\"end\":90911,\"start\":90423},{\"attributes\":{\"doi\":\"arXiv:1911.07176\",\"id\":\"b72\"},\"end\":91295,\"start\":90913},{\"attributes\":{\"doi\":\"arXiv:2105.14636\",\"id\":\"b73\"},\"end\":91673,\"start\":91297},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":218571099},\"end\":92184,\"start\":91675},{\"attributes\":{\"doi\":\"arXiv:1910.06188\",\"id\":\"b75\"},\"end\":92432,\"start\":92186},{\"attributes\":{\"doi\":\"arXiv:1905.07830\",\"id\":\"b76\"},\"end\":92751,\"start\":92434},{\"attributes\":{\"doi\":\"arXiv:1810.12885\",\"id\":\"b77\"},\"end\":93169,\"start\":92753},{\"attributes\":{\"doi\":\"arXiv:2009.12812\",\"id\":\"b78\"},\"end\":93499,\"start\":93171}]", "bib_title": "[{\"end\":62148,\"start\":62094},{\"end\":63128,\"start\":63073},{\"end\":64394,\"start\":64354},{\"end\":66016,\"start\":65969},{\"end\":67876,\"start\":67821},{\"end\":69334,\"start\":69273},{\"end\":69759,\"start\":69687},{\"end\":72492,\"start\":72426},{\"end\":74555,\"start\":74517},{\"end\":75637,\"start\":75617},{\"end\":75904,\"start\":75875},{\"end\":76788,\"start\":76739},{\"end\":78184,\"start\":78153},{\"end\":79136,\"start\":79076},{\"end\":79573,\"start\":79503},{\"end\":81991,\"start\":81890},{\"end\":82562,\"start\":82528},{\"end\":82726,\"start\":82665},{\"end\":83554,\"start\":83492},{\"end\":84566,\"start\":84489},{\"end\":86232,\"start\":86185},{\"end\":86795,\"start\":86770},{\"end\":87946,\"start\":87863},{\"end\":89481,\"start\":89403},{\"end\":90495,\"start\":90423},{\"end\":91764,\"start\":91675}]", "bib_author": "[{\"end\":62166,\"start\":62150},{\"end\":62182,\"start\":62166},{\"end\":62195,\"start\":62182},{\"end\":62218,\"start\":62195},{\"end\":62233,\"start\":62218},{\"end\":62244,\"start\":62233},{\"end\":62249,\"start\":62244},{\"end\":62708,\"start\":62697},{\"end\":62719,\"start\":62708},{\"end\":62727,\"start\":62719},{\"end\":62741,\"start\":62727},{\"end\":62751,\"start\":62741},{\"end\":62762,\"start\":62751},{\"end\":62771,\"start\":62762},{\"end\":62784,\"start\":62771},{\"end\":62796,\"start\":62784},{\"end\":63147,\"start\":63130},{\"end\":63160,\"start\":63147},{\"end\":63173,\"start\":63160},{\"end\":63186,\"start\":63173},{\"end\":63730,\"start\":63719},{\"end\":63747,\"start\":63730},{\"end\":63762,\"start\":63747},{\"end\":63779,\"start\":63762},{\"end\":63794,\"start\":63779},{\"end\":63803,\"start\":63794},{\"end\":63818,\"start\":63803},{\"end\":63836,\"start\":63818},{\"end\":63850,\"start\":63836},{\"end\":63865,\"start\":63850},{\"end\":63877,\"start\":63865},{\"end\":63890,\"start\":63877},{\"end\":63906,\"start\":63890},{\"end\":63925,\"start\":63906},{\"end\":63936,\"start\":63925},{\"end\":63947,\"start\":63936},{\"end\":63964,\"start\":63947},{\"end\":63974,\"start\":63964},{\"end\":64648,\"start\":64628},{\"end\":64662,\"start\":64648},{\"end\":64682,\"start\":64662},{\"end\":64999,\"start\":64982},{\"end\":65017,\"start\":64999},{\"end\":65040,\"start\":65017},{\"end\":65056,\"start\":65040},{\"end\":65070,\"start\":65056},{\"end\":65087,\"start\":65070},{\"end\":65100,\"start\":65087},{\"end\":65125,\"start\":65100},{\"end\":65144,\"start\":65125},{\"end\":65161,\"start\":65144},{\"end\":65567,\"start\":65545},{\"end\":65578,\"start\":65567},{\"end\":65593,\"start\":65578},{\"end\":65608,\"start\":65593},{\"end\":65625,\"start\":65608},{\"end\":65642,\"start\":65625},{\"end\":65662,\"start\":65642},{\"end\":65676,\"start\":65662},{\"end\":65691,\"start\":65676},{\"end\":65699,\"start\":65691},{\"end\":66030,\"start\":66018},{\"end\":66042,\"start\":66030},{\"end\":66053,\"start\":66042},{\"end\":66067,\"start\":66053},{\"end\":66078,\"start\":66067},{\"end\":66092,\"start\":66078},{\"end\":66101,\"start\":66092},{\"end\":66509,\"start\":66497},{\"end\":66520,\"start\":66509},{\"end\":66534,\"start\":66520},{\"end\":66554,\"start\":66534},{\"end\":66568,\"start\":66554},{\"end\":66936,\"start\":66915},{\"end\":66951,\"start\":66936},{\"end\":66965,\"start\":66951},{\"end\":66980,\"start\":66965},{\"end\":66995,\"start\":66980},{\"end\":67009,\"start\":66995},{\"end\":67022,\"start\":67009},{\"end\":67033,\"start\":67022},{\"end\":67048,\"start\":67033},{\"end\":67066,\"start\":67048},{\"end\":67076,\"start\":67066},{\"end\":67492,\"start\":67473},{\"end\":67504,\"start\":67492},{\"end\":67520,\"start\":67504},{\"end\":67537,\"start\":67520},{\"end\":67554,\"start\":67537},{\"end\":67574,\"start\":67554},{\"end\":67893,\"start\":67878},{\"end\":67904,\"start\":67893},{\"end\":67927,\"start\":67904},{\"end\":67937,\"start\":67927},{\"end\":68200,\"start\":68182},{\"end\":68215,\"start\":68200},{\"end\":68230,\"start\":68215},{\"end\":68247,\"start\":68230},{\"end\":68262,\"start\":68247},{\"end\":68478,\"start\":68464},{\"end\":68494,\"start\":68478},{\"end\":68506,\"start\":68494},{\"end\":68531,\"start\":68506},{\"end\":68838,\"start\":68825},{\"end\":68855,\"start\":68838},{\"end\":68869,\"start\":68855},{\"end\":68882,\"start\":68869},{\"end\":68903,\"start\":68882},{\"end\":68915,\"start\":68903},{\"end\":69347,\"start\":69336},{\"end\":69360,\"start\":69347},{\"end\":69370,\"start\":69360},{\"end\":69772,\"start\":69761},{\"end\":69784,\"start\":69772},{\"end\":69798,\"start\":69784},{\"end\":69809,\"start\":69798},{\"end\":69823,\"start\":69809},{\"end\":69832,\"start\":69823},{\"end\":70205,\"start\":70195},{\"end\":70222,\"start\":70205},{\"end\":70241,\"start\":70222},{\"end\":70263,\"start\":70241},{\"end\":70287,\"start\":70263},{\"end\":70294,\"start\":70287},{\"end\":70542,\"start\":70530},{\"end\":70557,\"start\":70542},{\"end\":70572,\"start\":70557},{\"end\":70894,\"start\":70882},{\"end\":70908,\"start\":70894},{\"end\":70925,\"start\":70908},{\"end\":70940,\"start\":70925},{\"end\":70956,\"start\":70940},{\"end\":70969,\"start\":70956},{\"end\":70984,\"start\":70969},{\"end\":71305,\"start\":71296},{\"end\":71322,\"start\":71305},{\"end\":71333,\"start\":71322},{\"end\":71351,\"start\":71333},{\"end\":71365,\"start\":71351},{\"end\":71381,\"start\":71365},{\"end\":71394,\"start\":71381},{\"end\":71405,\"start\":71394},{\"end\":71418,\"start\":71405},{\"end\":71433,\"start\":71418},{\"end\":71800,\"start\":71786},{\"end\":71812,\"start\":71800},{\"end\":71823,\"start\":71812},{\"end\":71835,\"start\":71823},{\"end\":71846,\"start\":71835},{\"end\":71860,\"start\":71846},{\"end\":71869,\"start\":71860},{\"end\":72202,\"start\":72177},{\"end\":72216,\"start\":72202},{\"end\":72225,\"start\":72216},{\"end\":72504,\"start\":72494},{\"end\":72515,\"start\":72504},{\"end\":72526,\"start\":72515},{\"end\":72541,\"start\":72526},{\"end\":72852,\"start\":72835},{\"end\":72867,\"start\":72852},{\"end\":72878,\"start\":72867},{\"end\":73111,\"start\":73097},{\"end\":73128,\"start\":73111},{\"end\":73143,\"start\":73128},{\"end\":73381,\"start\":73368},{\"end\":73393,\"start\":73381},{\"end\":73407,\"start\":73393},{\"end\":73418,\"start\":73407},{\"end\":73429,\"start\":73418},{\"end\":73440,\"start\":73429},{\"end\":73451,\"start\":73440},{\"end\":73460,\"start\":73451},{\"end\":73470,\"start\":73460},{\"end\":73859,\"start\":73849},{\"end\":73870,\"start\":73859},{\"end\":73884,\"start\":73870},{\"end\":73895,\"start\":73884},{\"end\":73909,\"start\":73895},{\"end\":74246,\"start\":74232},{\"end\":74259,\"start\":74246},{\"end\":74269,\"start\":74259},{\"end\":74280,\"start\":74269},{\"end\":74293,\"start\":74280},{\"end\":74569,\"start\":74557},{\"end\":74583,\"start\":74569},{\"end\":74595,\"start\":74583},{\"end\":74606,\"start\":74595},{\"end\":74620,\"start\":74606},{\"end\":74629,\"start\":74620},{\"end\":74952,\"start\":74940},{\"end\":74963,\"start\":74952},{\"end\":74976,\"start\":74963},{\"end\":74989,\"start\":74976},{\"end\":75002,\"start\":74989},{\"end\":75298,\"start\":75283},{\"end\":75311,\"start\":75298},{\"end\":75330,\"start\":75311},{\"end\":75344,\"start\":75330},{\"end\":75359,\"start\":75344},{\"end\":75373,\"start\":75359},{\"end\":75651,\"start\":75639},{\"end\":75659,\"start\":75651},{\"end\":75674,\"start\":75659},{\"end\":75681,\"start\":75674},{\"end\":75923,\"start\":75906},{\"end\":75937,\"start\":75923},{\"end\":75956,\"start\":75937},{\"end\":76276,\"start\":76265},{\"end\":76286,\"start\":76276},{\"end\":76295,\"start\":76286},{\"end\":76459,\"start\":76451},{\"end\":76471,\"start\":76459},{\"end\":76488,\"start\":76471},{\"end\":76501,\"start\":76488},{\"end\":76518,\"start\":76501},{\"end\":76803,\"start\":76790},{\"end\":76815,\"start\":76803},{\"end\":76824,\"start\":76815},{\"end\":76835,\"start\":76824},{\"end\":76845,\"start\":76835},{\"end\":76854,\"start\":76845},{\"end\":77182,\"start\":77171},{\"end\":77192,\"start\":77182},{\"end\":77203,\"start\":77192},{\"end\":77215,\"start\":77203},{\"end\":77227,\"start\":77215},{\"end\":77236,\"start\":77227},{\"end\":77253,\"start\":77236},{\"end\":77483,\"start\":77471},{\"end\":77496,\"start\":77483},{\"end\":77507,\"start\":77496},{\"end\":77519,\"start\":77507},{\"end\":77530,\"start\":77519},{\"end\":77543,\"start\":77530},{\"end\":77557,\"start\":77543},{\"end\":77570,\"start\":77557},{\"end\":77580,\"start\":77570},{\"end\":77590,\"start\":77580},{\"end\":78023,\"start\":77999},{\"end\":78202,\"start\":78186},{\"end\":78217,\"start\":78202},{\"end\":78233,\"start\":78217},{\"end\":78249,\"start\":78233},{\"end\":78523,\"start\":78510},{\"end\":78534,\"start\":78523},{\"end\":78549,\"start\":78534},{\"end\":78809,\"start\":78793},{\"end\":78822,\"start\":78809},{\"end\":78835,\"start\":78822},{\"end\":78853,\"start\":78835},{\"end\":79152,\"start\":79138},{\"end\":79168,\"start\":79152},{\"end\":79185,\"start\":79168},{\"end\":79203,\"start\":79185},{\"end\":79223,\"start\":79203},{\"end\":79589,\"start\":79575},{\"end\":79606,\"start\":79589},{\"end\":79626,\"start\":79606},{\"end\":79639,\"start\":79626},{\"end\":80023,\"start\":80008},{\"end\":80042,\"start\":80023},{\"end\":80062,\"start\":80042},{\"end\":80073,\"start\":80062},{\"end\":80089,\"start\":80073},{\"end\":80106,\"start\":80089},{\"end\":80122,\"start\":80106},{\"end\":80136,\"start\":80122},{\"end\":80151,\"start\":80136},{\"end\":80162,\"start\":80151},{\"end\":80604,\"start\":80590},{\"end\":80613,\"start\":80604},{\"end\":80626,\"start\":80613},{\"end\":80638,\"start\":80626},{\"end\":80652,\"start\":80638},{\"end\":80668,\"start\":80652},{\"end\":80907,\"start\":80893},{\"end\":80921,\"start\":80907},{\"end\":80935,\"start\":80921},{\"end\":80950,\"start\":80935},{\"end\":80965,\"start\":80950},{\"end\":80981,\"start\":80965},{\"end\":80993,\"start\":80981},{\"end\":81001,\"start\":80993},{\"end\":81014,\"start\":81001},{\"end\":81331,\"start\":81310},{\"end\":81346,\"start\":81331},{\"end\":81362,\"start\":81346},{\"end\":81647,\"start\":81629},{\"end\":81659,\"start\":81647},{\"end\":81679,\"start\":81659},{\"end\":81692,\"start\":81679},{\"end\":82006,\"start\":81993},{\"end\":82026,\"start\":82006},{\"end\":82043,\"start\":82026},{\"end\":82055,\"start\":82043},{\"end\":82578,\"start\":82564},{\"end\":82747,\"start\":82728},{\"end\":82757,\"start\":82747},{\"end\":82771,\"start\":82757},{\"end\":82790,\"start\":82771},{\"end\":82796,\"start\":82790},{\"end\":83234,\"start\":83221},{\"end\":83250,\"start\":83234},{\"end\":83267,\"start\":83250},{\"end\":83280,\"start\":83267},{\"end\":83568,\"start\":83556},{\"end\":83579,\"start\":83568},{\"end\":83589,\"start\":83579},{\"end\":83601,\"start\":83589},{\"end\":83613,\"start\":83601},{\"end\":83627,\"start\":83613},{\"end\":83638,\"start\":83627},{\"end\":83652,\"start\":83638},{\"end\":83661,\"start\":83652},{\"end\":83991,\"start\":83977},{\"end\":84008,\"start\":83991},{\"end\":84024,\"start\":84008},{\"end\":84043,\"start\":84024},{\"end\":84063,\"start\":84043},{\"end\":84077,\"start\":84063},{\"end\":84087,\"start\":84077},{\"end\":84107,\"start\":84087},{\"end\":84123,\"start\":84107},{\"end\":84142,\"start\":84123},{\"end\":84584,\"start\":84568},{\"end\":84600,\"start\":84584},{\"end\":84609,\"start\":84600},{\"end\":84623,\"start\":84609},{\"end\":84638,\"start\":84623},{\"end\":84647,\"start\":84638},{\"end\":84657,\"start\":84647},{\"end\":84673,\"start\":84657},{\"end\":84680,\"start\":84673},{\"end\":85157,\"start\":85147},{\"end\":85167,\"start\":85157},{\"end\":85176,\"start\":85167},{\"end\":85190,\"start\":85176},{\"end\":85438,\"start\":85425},{\"end\":85450,\"start\":85438},{\"end\":85464,\"start\":85450},{\"end\":85476,\"start\":85464},{\"end\":85489,\"start\":85476},{\"end\":85501,\"start\":85489},{\"end\":85878,\"start\":85865},{\"end\":85886,\"start\":85878},{\"end\":85897,\"start\":85886},{\"end\":85911,\"start\":85897},{\"end\":85922,\"start\":85911},{\"end\":85931,\"start\":85922},{\"end\":85941,\"start\":85931},{\"end\":85952,\"start\":85941},{\"end\":86248,\"start\":86234},{\"end\":86259,\"start\":86248},{\"end\":86266,\"start\":86259},{\"end\":86610,\"start\":86598},{\"end\":86624,\"start\":86610},{\"end\":86639,\"start\":86624},{\"end\":86813,\"start\":86797},{\"end\":86827,\"start\":86813},{\"end\":86840,\"start\":86827},{\"end\":86857,\"start\":86840},{\"end\":86870,\"start\":86857},{\"end\":86885,\"start\":86870},{\"end\":86900,\"start\":86885},{\"end\":86918,\"start\":86900},{\"end\":87210,\"start\":87199},{\"end\":87227,\"start\":87210},{\"end\":87243,\"start\":87227},{\"end\":87255,\"start\":87243},{\"end\":87266,\"start\":87255},{\"end\":87283,\"start\":87266},{\"end\":87625,\"start\":87615},{\"end\":87643,\"start\":87625},{\"end\":87653,\"start\":87643},{\"end\":87961,\"start\":87948},{\"end\":87973,\"start\":87961},{\"end\":87994,\"start\":87973},{\"end\":88004,\"start\":87994},{\"end\":88017,\"start\":88004},{\"end\":88033,\"start\":88017},{\"end\":88048,\"start\":88033},{\"end\":88064,\"start\":88048},{\"end\":88860,\"start\":88847},{\"end\":88870,\"start\":88860},{\"end\":88879,\"start\":88870},{\"end\":88891,\"start\":88879},{\"end\":88901,\"start\":88891},{\"end\":88912,\"start\":88901},{\"end\":89166,\"start\":89151},{\"end\":89183,\"start\":89166},{\"end\":89200,\"start\":89183},{\"end\":89499,\"start\":89483},{\"end\":89514,\"start\":89499},{\"end\":89529,\"start\":89514},{\"end\":90153,\"start\":90137},{\"end\":90169,\"start\":90153},{\"end\":90182,\"start\":90169},{\"end\":90510,\"start\":90497},{\"end\":90526,\"start\":90510},{\"end\":90539,\"start\":90526},{\"end\":90556,\"start\":90539},{\"end\":90574,\"start\":90556},{\"end\":90587,\"start\":90574},{\"end\":90603,\"start\":90587},{\"end\":90614,\"start\":90603},{\"end\":90625,\"start\":90614},{\"end\":90643,\"start\":90625},{\"end\":91036,\"start\":91023},{\"end\":91052,\"start\":91036},{\"end\":91068,\"start\":91052},{\"end\":91392,\"start\":91380},{\"end\":91404,\"start\":91392},{\"end\":91416,\"start\":91404},{\"end\":91430,\"start\":91416},{\"end\":91449,\"start\":91430},{\"end\":91782,\"start\":91766},{\"end\":91792,\"start\":91782},{\"end\":92199,\"start\":92186},{\"end\":92213,\"start\":92199},{\"end\":92226,\"start\":92213},{\"end\":92244,\"start\":92226},{\"end\":92504,\"start\":92489},{\"end\":92518,\"start\":92504},{\"end\":92532,\"start\":92518},{\"end\":92545,\"start\":92532},{\"end\":92557,\"start\":92545},{\"end\":92852,\"start\":92839},{\"end\":92866,\"start\":92852},{\"end\":92880,\"start\":92866},{\"end\":92894,\"start\":92880},{\"end\":92905,\"start\":92894},{\"end\":92925,\"start\":92905},{\"end\":93182,\"start\":93171},{\"end\":93190,\"start\":93182},{\"end\":93202,\"start\":93190},{\"end\":93216,\"start\":93202},{\"end\":93227,\"start\":93216},{\"end\":93238,\"start\":93227},{\"end\":93247,\"start\":93238}]", "bib_venue": "[{\"end\":62341,\"start\":62249},{\"end\":62862,\"start\":62812},{\"end\":63272,\"start\":63186},{\"end\":64029,\"start\":63974},{\"end\":64407,\"start\":64396},{\"end\":64626,\"start\":64545},{\"end\":64980,\"start\":64893},{\"end\":65543,\"start\":65506},{\"end\":66182,\"start\":66101},{\"end\":66682,\"start\":66584},{\"end\":67131,\"start\":67092},{\"end\":67471,\"start\":67401},{\"end\":67986,\"start\":67937},{\"end\":68621,\"start\":68547},{\"end\":69025,\"start\":68931},{\"end\":69443,\"start\":69370},{\"end\":69899,\"start\":69832},{\"end\":70340,\"start\":70310},{\"end\":70648,\"start\":70588},{\"end\":70880,\"start\":70812},{\"end\":71294,\"start\":71230},{\"end\":71784,\"start\":71713},{\"end\":72175,\"start\":72098},{\"end\":72590,\"start\":72541},{\"end\":72833,\"start\":72789},{\"end\":73095,\"start\":73024},{\"end\":73536,\"start\":73486},{\"end\":73847,\"start\":73750},{\"end\":74230,\"start\":74142},{\"end\":74673,\"start\":74629},{\"end\":74938,\"start\":74873},{\"end\":75281,\"start\":75205},{\"end\":75730,\"start\":75681},{\"end\":76051,\"start\":75956},{\"end\":76903,\"start\":76854},{\"end\":77169,\"start\":77092},{\"end\":77669,\"start\":77606},{\"end\":77997,\"start\":77913},{\"end\":78301,\"start\":78249},{\"end\":78508,\"start\":78467},{\"end\":78791,\"start\":78704},{\"end\":79267,\"start\":79223},{\"end\":79710,\"start\":79639},{\"end\":80250,\"start\":80178},{\"end\":80588,\"start\":80537},{\"end\":80891,\"start\":80810},{\"end\":81308,\"start\":81230},{\"end\":81627,\"start\":81568},{\"end\":82151,\"start\":82055},{\"end\":82589,\"start\":82578},{\"end\":82857,\"start\":82796},{\"end\":83219,\"start\":83142},{\"end\":83665,\"start\":83661},{\"end\":83975,\"start\":83872},{\"end\":84766,\"start\":84680},{\"end\":85263,\"start\":85206},{\"end\":85586,\"start\":85517},{\"end\":85863,\"start\":85793},{\"end\":86345,\"start\":86266},{\"end\":86596,\"start\":86553},{\"end\":86967,\"start\":86918},{\"end\":87384,\"start\":87299},{\"end\":87704,\"start\":87653},{\"end\":88132,\"start\":88064},{\"end\":88845,\"start\":88747},{\"end\":89254,\"start\":89216},{\"end\":89671,\"start\":89529},{\"end\":90257,\"start\":90198},{\"end\":90648,\"start\":90643},{\"end\":91021,\"start\":90913},{\"end\":91378,\"start\":91297},{\"end\":91870,\"start\":91792},{\"end\":92287,\"start\":92260},{\"end\":92487,\"start\":92434},{\"end\":92837,\"start\":92753},{\"end\":93313,\"start\":93263},{\"end\":62420,\"start\":62343},{\"end\":63369,\"start\":63274},{\"end\":66250,\"start\":66184},{\"end\":69503,\"start\":69445},{\"end\":69953,\"start\":69901},{\"end\":79768,\"start\":79712},{\"end\":82234,\"start\":82153},{\"end\":82905,\"start\":82859},{\"end\":84839,\"start\":84768},{\"end\":88184,\"start\":88161},{\"end\":89800,\"start\":89673}]"}}}, "year": 2023, "month": 12, "day": 17}