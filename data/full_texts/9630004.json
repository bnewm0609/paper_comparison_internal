{"id": 9630004, "updated": "2022-08-02 17:02:34.591", "metadata": {"title": "Watermarked 3-D Mesh Quality Assessment", "authors": "[{\"first\":\"M.\",\"last\":\"Corsini\",\"middle\":[]},{\"first\":\"E.D.\",\"last\":\"Gelasca\",\"middle\":[]},{\"first\":\"T.\",\"last\":\"Ebrahimi\",\"middle\":[]},{\"first\":\"M.\",\"last\":\"Barni\",\"middle\":[]}]", "venue": "IEEE Transactions on Multimedia", "journal": "IEEE Transactions on Multimedia", "publication_date": {"year": 2007, "month": null, "day": null}, "abstract": "This paper addresses the problem of assessing distortions produced by watermarking 3D meshes. In particular, a new methodology for subjective evaluation of the quality of 3D objects is proposed and implemented. Two objective metrics derived from measures of surface roughness are then proposed and their efficiency to predict the perceptual impact of 3D watermarking is assessed and compared with the state of the art. Results obtained show good correlations between the proposed objective metrics and subjective assessments by human observers", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2161123186", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tmm/CorsiniGEB07", "doi": "10.1109/tmm.2006.886261"}}, "content": {"source": {"pdf_hash": "ce64621802c56c9c1382fd7570ea676efb05bd5b", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://vcg.isti.cnr.it/Publications/2007/CDEB07/ieeetransmultimedia.pdf", "status": "GREEN"}}, "grobid": {"id": "733b46d238beb6482d745d1923e61c3dc543aaf3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ce64621802c56c9c1382fd7570ea676efb05bd5b.txt", "contents": "\nWatermarked 3D Mesh Quality Assessment\n\n\nMassimiliano Corsini \nElisa Drelie Gelasca \nTouradj Ebrahimi \nMauro Barni \nWatermarked 3D Mesh Quality Assessment\n1Index Terms-3D watermarkingmesh watermarkingobjec- tive metricsperceptual metricssubjective evaluation3D objects quality assessment\nThis paper addresses the problem of assessing distortions produced by watermarking 3D meshes. In particular, a new methodology for subjective evaluation of the quality of 3D objects is proposed and implemented. Two objective metrics derived from measures of surface roughness are then proposed and their efficiency to predict perceptual impact of 3D watermarking are assessed and compared with the state of the art. Results obtained show good correlations between the proposed objective metrics and subjective assessments by human observers.Index Terms-3D watermarking, mesh watermarking, objective metrics, perceptual metrics, subjective evaluation, 3D objects quality assessment.\n\nI. INTRODUCTION\n\nIn the last decade, digital watermarking has become a very active research topic with important applications in the fields of copyright protection for multimedia content, digital rights management, document authentication and conditional access to enhanced services. Until few years ago the research efforts in digital watermarking have been mainly focused on the watermarking of audio, image and video data; only relatively recently watermarking of 3D objects has gained attention, due to the ever increasing diffusion of such objects in many areas including architecture, design, mechanical engineer, Cultural Heritage and entertainment. Hence, watermarking of 3D objects has not reached the same level of maturity as in still image and video.\n\nOne of the main requirements of any watermarking algorithm is the imperceptibility of the watermark. This is also a fundamental constraint for 3D watermarking. It is then very important that suitable methodologies are developed to measure the quality of the watermarked objects as judged by human observers. Moreover, study of human perception of geometric artifacts on 3D surfaces due to 3D watermarking can aid the design of efficient watermarking schemes.\n\nGenerally speaking, there are two major classes of quality criteria evaluations: objective and subjective. In fact, one obvious way of determining the quality of any visual information is to measure it by means of subjective experiments with human observers. After all, these signal are meant for human consumption. However, such subjective evaluations are not M. Corsini  only time-consuming and expensive, but they also cannot be incorporated into automatic systems. The goal of this paper is to develop objective measures that can automatically predict the quality of watermarked 3D mesh as perceived by human subjects. Due to the relative novelty of quality evaluation of 3D data, no standardized procedures exist and current studies show this lack of generalization [1]. Nevertheless, since subjective quality measures exhibit some inherent drawbacks (the use of a standard evaluation room, a large panel of human observers, etc.), there has been a great interest in developing objective metrics for 3D models quality assessment.\n\nIn this work, we propose two perceptual metrics for the quality assessment of watermarked 3D objects. Note that, admittedly, our new metric will not represent a general tool to evaluate the quality of a 3D model, but it will only serve the purpose of judging the quality of watermarked meshes. The final aim of our study is to provide a mean whereby watermarking researchers can evaluate the degradation introduced by the watermark and take appropriate countermeasures to minimize it. At the same time, the proposed metric will be particularly useful for comparing the performance of different 3D watermarking algorithms on the basis of the artifacts perceived on the 3D mesh.\n\nAs a matter of fact, some progress has been achieved in the study of perceptual metrics for image and video. A possible approach, then, could be to simply apply such perceptual metrics to the final rendered views of the 3D model. The main problem of this approach is that the perceived degradation of still images may not be adequate to evaluate the perceived degradation of the equivalent 3D model [1]. Hence, the approach we chose is to evaluate the human perception of geometric distortions in watermarked models and then to build ad-hoc perceptual metrics that work directly on the model's surface. In such a case, subjective experiments dealing directly with the 3D models are needed.\n\nThe paper is organized as follows. The state-of-the-art related to the research addressed in this work is reviewed in Section II. Our experimental methodology to carry out subjective experiments on 3D model quality evaluation is described in Section III. In Section IV we describe the subjective experiments and the artifacts introduced by common 3D watermarking algorithms. Section V describes the proposed metrics. Finally, results are presented and discussed in Section VI before drawing the conclusions in Section VII.\n\nII. RELATED WORK It is widely known among researchers working in image and video watermarking that the characteristics of the Human Visual System (HVS) have to be carefully considered in order to minimize the visual degradation introduced by the watermarking process while maximizing robustness [2] [3]. Many methods have been proposed so far to exploit the models of the HVS to improve the effectiveness of existing watermarking schemes. We can divide the approaches proposed so far into theoretical [4] [3] and heuristic [5] [6].\n\nConcerning quality evaluation, in the past years, a set of techniques have been defined and proposed both subjectively [7] [8], and objectively [9] to be used for watermarked video quality evaluation. To the best of our knowledge no similar standards or objective techniques have been proposed for watermarked 3D meshes. In recent years, in order to properly evaluate mesh simplification and perceptually guided rendering of 3D objects, few objective metrics have been proposed but even less attention has been payed to establishing a reliable procedure for subjective evaluation of 3D data and results show the complexity of such a task [1].\n\nMesh simplification reduces the number of vertices and triangles of a polygonal mesh model while preserving its visual appearance. In general, the simplification process is driven by a similarity metric that measures the impact of the changes of the model after each simplification step. Two kinds of metrics are usually adopted for simplification: geometric metrics and (perceptual) image-based metrics. The most used global geometry-based metrics for off-line quality evaluation of 3D models are the Maximum Geometric Error and Mean Geometric Error based on the Hausdorff distance [10] [11]. Concerning perceptual image-based simplification, Lindstrom and Turk [12] propose an image-driven approach for guiding the simplification process: the model to be simplified is rendered by considering several viewpoints and an image quality metric is used to evaluate the perceptual impact of the simplification operation. More recently, Williams et al. [13] developed a view-dependent simplification algorithm based on a simple model of Contrast Sensitivity Function that takes into account texture and lighting effects.\n\nThe aim of perceptually-guided rendering is to accelerate photo-realistic rendering algorithms to avoid computations that do not impact the perceived final result. Some remarkable works in this field include Bolin and Meyer [14], and the work of Ferwarda et al. [15], where a sophisticated perceptual metric for the evaluation of how much a visual pattern, i.e. a texture, hides geometry artifacts is proposed. The visual masking effect caused by texturing is taken into account by analyzing the final rendered images.\n\nIt is worth to mention that a possible approach to evaluate the visual quality of watermarked 3D objects could be to simply apply image-based perceptual metrics to the final rendered images of the 3D model. This method was investigated in the work by Lindstrom and Turk [12] just mentioned. The main problem of this approach is that the perceived degradation of still images may not be adequate to evaluate the perceived degradation of the equivalent 3D model as concluded by the subjective experiments of Rogowitz and Rushmeier [1]. Another possible approach is to evaluate how the human visual system perceives geometric distortions on the model surface and to build an ad-hoc perceptual metric for geometric artifacts. The latter approach is more interesting from a research viewpoint, since no similar studies have been conducted so far. The potential field of applications is not limited to 3D watermarking, but other Computer Graphics applications can also benefit from them. For these reasons, this work has adopted the second approach, i.e. to work directly on the geometry of the 3D model.\n\n\nIII. A METHODOLOGY FOR SUBJECTIVE QUALITY EVALUATION\n\nSince no standards exist for the evaluation of the quality of 3D objects with impairments, we propose a method for subjective evaluation of watermarked 3D objects based on the criteria usually followed in video and multimedia content quality evaluation [7], [8].\n\nIn designing subjective experiments for quality evaluation of 3D objects, a first crucial problem is to decide the way the object under examination is rendered. This is not a trivial task. Hence an accurate study has been carried out. For instance, the rendering conditions should not bias the human perception of the 3D model by privileging, for example, one view of the 3D object rather than another. In our investigations the rendering conditions have been set as follows.\n\n\u2022 Non-uniform background. The 3D model is visualized on a non-uniform background in order to not to overestimate the importance of the contour of the model with respect to its overall shape. The background color fades from blue (at the top) to white (at the bottom). \u2022 Light source. All models are illuminated with a single white point light source since multiple lights can confuse the observer [16]. To be more specific, each model is illuminated with one white point light source located in a top corner of the Object Bounding Box (OBB) of the 3D object. \u2022 Lighting. We use a simple local illumination lighting model where only the diffusive component of the reflected light is considered to avoid the dependence on camera's position. \u2022 Texturing. Image texture mapping, bump mapping, and other kinds of texturing usually produce masking effects on the perceived geometry [15] and consequently on the perception of watermarking artifacts. Since we leave visual masking studies specific for 3D models as a future research area we do not account for such visual effects in our metrics. For theoretical considerations about visual masking and texturing we refer to Sec. VII-A. \u2022 Material properties. The color of a surface is determined by the parameters of the light source that illuminate the surface, by the lighting model used and by the properties of the surface's material. We set the material properties to obtain a stone-like effect. This choice is made for different reasons: first, if all models are seen as \"statues\" the subjects perceive them in a natural way; second, in this way, the memory color phenomenon is avoided [17]. The memory color phenomenon describes the fact that an object's characteristic color influences the human perception of that object's color, e.g. shapes such as heart and strawberries are characteristically red. \u2022 Screen and Models Resolution. The monitor resolution used in the experiments is 1280 \u00d7 600 and each model is displayed in a window of 600 \u00d7 600 pixels. The model occupies about 80% of such window and the resolution of the models ranges between 50'000 and 100'000 triangles allowing for a good visualization of the details. \u2022 Interaction. The studies of Rogowitz and Rushmeier [1] suggest that an experimental methodology to evaluate the perceived alterations of 3D objects should rely on the interaction with the model. For this reason, in our experimental method, we decided to allow the subject to interact with the model by rotation and zoom operations. Eleven test subjects (one female, ten males) were drawn from a pool of students aged between 24 and 30. The 3D models were displayed on an uncalibrated 17-inch LCD monitor, with participants sitting approximately 0.4 meters from the display. The experiments followed a five-stage procedure. The stages were: (1) oral instructions, (2) training, (3) practice trials, (4) experimental trials, (5) interview. In the first stage, the subjects were verbally given instructions and made familiar with the task and the graphic interface. In the training, the original models and the watermarked models were shown to establish the range for the impairment scale. The practice trials stage was used to familiarize subjects with the experimentation. In the experimental stage, the subjects had to give a score to indicate how much the distortions were evident. The subjects were instructed to enter a numerical value greater than 0 proportional to the distortion noticed. The value of 10 had to be assigned to the most evident distortion representing the worst cases shown in the training phase. Finally, in the interview stage, test subjects were asked to provide a qualitative description of the perceived artifacts.\n\n\nIV. SUBJECTIVE EXPERIMENTS\n\nTwo sets of subjective experiments were carried out with different purposes. The first set of experiments (Experiment I), were performed to tune the two objective metrics (proposed in the next section) with psychovisual data in order to transform them into two perceptual metrics.\n\nIn this first set of experiments, test subjects evaluated differently watermarked models ranging from severe down to weak visual impairments. Those different distortion strengths were generated using a specific watermarking algorithm, i.e. the algorithm by Uccheddu et al. (UCB) [18]. The second set of experiments, (Experiment II) were conducted to validate the proposed metrics. Specifically, in Experiment II, we implemented and adopted three other different watermarking algorithms: the Vertex Flood Algorithm (VFA) [19], the Normal Bin Encoding (NBE) [20], and the method by Kanai et al. (KDK) [21]. Concerning the kind of impairments introduced in the model, the UCB algorithm produced a uniform noise that can be described as an increase of the roughness of the watermarked surface. VFA produced a noise similar to a marble streak, depending on the viewpoint. The artifacts of the KDK algorithm were similar to those obtained by UCB algorithm but due to the geometric tolerance introduced by Kanai to limit the visual impact of the watermarking, the final visual effects of such distortions were not uniformly distributed over the model surface. Concerning NBE, the visual aspect (crack-like) of its artifacts was different from that of UCB, VFA and KDK and more difficult to perceive. Figure 1 shows a detail of the model \"Horse\" after application of the four different watermarking algorithms.\n\nThe test models used for both the experiments were: \"Bunny\", \"Horse\", \"Venus\" and \"Feline\". A total of 40 (4 originals \u00d7 3 watermarking strength \u00d7 3 resolution level + 4 originals) test models were used in Experiment I. A total of 48 (4 models \u00d7 11 watermarking settings + 4 originals) test models were used in Experiment II.\n\n\nV. PROPOSED PERCEPTUAL METRICS\n\nThanks to the intuition and to previous studies about visual aspects of 3D watermarking [22], we argued that a good measure of the visual artifacts produced by watermarking should be based on the amount of roughness introduced on the surface. Moreover, the interviews in Experiments I and II confirmed that the different types of noise on the surfaces produced by the watermarking can be described essentially with the term roughness. Hence, we chose to measure the strength of the artifacts on the basis of an estimation of the surface roughness. In particular, two objective metrics based on roughness estimation of the surface have been developed.\n\n\nA. Multi-scale Roughness Estimation\n\nThe first roughness measure we propose is a variant of the method by Wu et al. [23]. This metric measures the perface roughness by making statistical considerations about the dihedral angles associated to each face. Wu et al. developed this measure in order to preserve significant shape features in mesh simplification algorithms.\n\nThe dihedral angle is the angle between two planes. For a polygonal mesh, the dihedral angle is the angle between the normals of two adjacent faces ( Fig. 2 (a)). The basic idea of this method [24] is that the dihedral angle is related to the surface roughness. In fact, the face normals of a smooth surface vary slowly over the surface, consequently the dihedral angles between adjacent faces are close to zero. To be more specific, Wu et al. associated to each dihedral angle an amount of roughness given by the quantity\n\u03c1 d = 1 \u2212 ( N 1 \u00b7 N 2 ).(1)\nwhere \u00b7 denotes the scalar product between two vectors, and the subscript d was added to explicitly indicate that \u03c1 d measures the roughness of a dihedral angle. Given a triangle T with vertices v 1 , v 2 and v 3 , its roughness is computed as:\n\u03c1 1 (T ) = G(v 1 )V (v 1 ) + G(v 2 )V (v 2 ) + G(v 3 )V (v 3 ) V (v 1 ) + V (v 2 ) + V (v 3 )(2)\nReferring to Fig. 2 (b), G(v 1 ) is the mean of the roughness associated to the dihedral angles T \u2212 T 1 ,  dihedral angles of the faces adjacent to the vertices v 1 , v 2 and v 3 . A rough surface can be considered as a surface with a high concentration of bumps of different sizes. The roughness measure expressed in Eq. (2) is able to measure the 'bumpiness' of surfaces at face level, but, if the granularity of the surface roughness, i.e. the size of the bump, is bigger than the average size of one face, this metric fails to measure it correctly. In other words, this measure does not take into account the scale of the roughness. Our idea is to modify equation Eq. (2) so as to account for different bump scales. The first step to achieve this goal is to transform this per-face roughness estimation in a per-vertex roughness in the following way:\nT 1 \u2212 T 2 , T 2 \u2212 T 3 ,TN 1 N 2 (a) T T 1 T 2 T 3 T 4 T 5 v 1 v 2 v 3 (b)\u03c1 N 1 (v) = 1 |S N T | i\u2208S N T \u03c1 1 (T i )A T i(3)\nwhere S N T is the set of the faces of the N-ring of the vertex v, |.| is the cardinality operator and A Ti is the area of the face T i . Considering the N-ring in the roughness evaluation 1D-case accounts for different scales of bumpiness. Referring to Fig. 3, the bump of size equivalent to the 1-ring (A) is well measured by \u03c1 1 1 (v), a correct value of roughness for the vertex v in the case (B) is provided by \u03c1 2 1 (v). Approximately, we can state that the roughness of a vertex v centered on a bump whose area is close to the area of the faces that form the N-ring is well measured by \u03c1 N 1 (v). This approximation could still create invalid estimates in certain cases. For example for high values of N , or when a surface presents high curvatures. Despite possible impairments, the proposed metric provides acceptable results in most cases. In order to obtain a single value of roughness for each vertex that accounts for the roughness evaluated at several scales, the maximum value produced by N-rings of different sizes is retained. In our objective metric, after several tests, 3 scales of roughness: the 1-ring, the 2ring and the 4-ring were retained. Hence, the final per-vertex roughness metric becomes:\nBump A Bump B v v\u03c1 1 (v) = max{\u03c1 1 1 (v), \u03c1 2 1 (v), \u03c1 4 1 (v)}(4)\nA metric using a linear scale of roughness, i.e. the 1-ring, the 2-ring and the 3-ring, provides similar results. The total roughness of the 3D object is the sum of the roughnesses of all vertices:\n\u03c1 1 (M ) = Nv i=1 \u03c1 1 (v i )(5)\nwhere N v is the total number of mesh vertices. In the following, we describe how to transform this multi-scale roughness estimation in to an objective metric that correlates well to the human perception of geometric artifacts.\n\n\nB. Smoothing-based Roughness Estimation\n\nThe second method [25] is based on the consideration that artifacts are better perceived on smooth surfaces. This was confirmed by interviews with subjects. Hence, a smoothingbased roughness estimation was developed. The basic idea of this approach is to apply to the model a smoothing algorithm and then to measure the roughness of the surface as the variance of the differences between the smoothed version of the model and the original. A sketch of the smoothing-based roughness estimation is depicted in Fig. 4.\n\nThe first step is to build a smoothed version of the model (M S ) by applying a smoothing algorithm to the input model (M ). Several possibilities for smoothing exist [26] [27] [28]. Here, we decided to use the Taubin filter [26] for its simplicity of implementation. For the Taubin filter we used the parameters \u03bb = 0.6307, \u00b5 = \u22120.6352 that give a strong smoothing effect. The filter is iterated 5 times. Once the smoothed model obtained, the distance between each vertex of M and M S is computed in the following way:\nd OS (v, v S ) = proj n S v (v \u2212 v S )(6)\nwhere proj(.) indicates the projection of the vector (v \u2212 v S ) on the vertex normals of the smoothed surface ( n S v ). At this point the per-vertex roughness is computed by evaluating the local variance of the distances d OS (.) around each vertex. To be more specific, for each vertex v, the set of distances associated to its 2-ring (S 2 d (v)) is built and the variance of this set evaluated. Then, the per-vertex smoothing-based roughness is computed by:\n\u03c1 2 (v) = V (S 2 d (v)) A S 2(7)\nwhere A S 2 is the area of the faces that form the 2-ring of v. This area is used as the denominator since surfaces with the same local variance of the distances but smaller area are assumed to be rougher. The roughness of the input model is the sum of the roughnesses of all vertices in the model:\n\u03c1 2 (M ) = N v i=1 \u03c1 2 (v i )(8)\nwhere N v is the number of vertices of the model.\n\n\nC. Objective Metrics\n\nNow, we describe how to use the roughness estimation to predict the visual distortion produced by a certain 3D watermarking algorithm. On the basis of several evaluations we decided to define our objective metric as the increment of roughness between the original and the watermarked model. This increment is normalized with respect to the roughness of the original model, leading to:\nR(M, M w ) = log \u03c1(M w ) \u2212 \u03c1(M ) \u03c1(M ) + k \u2212 log (k) (9)\nwhere \u03c1(M ) is the total roughness of the original model and \u03c1(M w ) is the total roughness of the watermarked model. Both \u03c1 1 (.) and \u03c1 2 (.) can be used to obtain two different objective metrics for the evaluation of the quality of watermarked 3D objects. Equation (9) is a sort of \"Weber Law\" in which the amount of roughness substitutes the grey levels. The logarithm is employed to better discriminate small values of relative roughness increments. The constant k is used to avoid the numerical instability of Eq. (9) since the logarithm tends to \u2212\u221e for \u03c1(M w ) very close to \u03c1(M ). Since in experiments, subjects used values between 0 and 10, , the value k has been set to normalize the metric's outputs to the same range.\n\nIn the following, we indicate with R 1 (M, M w ) the objective metric based on the multi-scale roughness estimation and with R 2 (M, M w ) the objective metric based on the smoothingbased roughness estimation.\n\n\nD. From objective to perceptual metric\n\nBasically, there are two approaches [29] to model psychophysical quantities: performance modeling and mechanistic modeling. Although the distinction is more a continuum than a strict dichotomy, the performance models tend to treat the entire visual system as a \"black box\" for which input/output functions need to be specified. For the mechanistic model, physiological and psychophysical data are used to open the black box. For our metrics, we opted for the black box approach since a model of visual perception of geometric artifacts could become too complex to be handled in practice. In addition the application-oriented nature of our metrics  makes the black box approach to be a more appropriate solution.\n\nBefore using the results of Experiment I to obtain our perceptual metric, we summarized the subjective scores.\n\nThe subjective scores have to be condensed by statistical techniques used in standard methods [8] to yield results which summarize the performance of the system under test. The averaged score values, Mean Opinion Score (MOS), are considered as the amount of distortions that anyone can perceive on a particular watermarked 3D object. However, impairment is measured according to a certain scale, and such a scale may vary from person to person. For this reason, we used standard methods to normalize and to screen the judgments provided by the subjects [7]. The MOS values obtained following the above mentioned approach were used to derive a perceptual metric by fitting them with a psychometric curve.\n\nThe purpose of a psychometric curve is to associate the values given by the objective metric to the subjective score values provided by the subjects. This step is always necessary in order to take into account the saturation effects typical of human senses. For these reasons, psychometric curves exhibit a typical sigmoid shape that penalizes the strongest stimuli. Through psychometric mapping, a match between the human perception of geometric artifacts and the values provided by the objective metric is established. In particular, we use the Gaussian psychometric function:\ng(a, b, R) = 1 2\u03c0 \u221e a+bR e \u2212 t 2 2 dt(10)\nwhere a and b are the parameters to be estimated by fitting the objective metric values versus the subjective data, and R is the objective metric used to measure the visual distortions. We opt for this psychometric curve since it provided the best fit for our data among the commonly used psychometric curves, i.e. Gaussian, logistic and Weibull curves [30]. To estimate the parameters a and b, we used a nonlinear least-squares data fitting by the Gauss-Newton method.\n\n\nVI. EXPERIMENTAL RESULTS\n\nIn this section, we analyze the performance of the two proposed objective metrics and compare them with geometric metrics usually adopted in literature for quality evaluation of 3D models. without adding anything): Evaluations are performed by assessing the correlations with the Mean Opinion Scores described previously. The correlations between the subjective M OS collected in Experiment I and the distances given by two geometric metrics based on the Hausdorff distance for model similarity are evaluated. In this way we obtain a term of comparison for the evaluation of our metrics. The data of the Experiment II are used to validate the developed perceptual metrics on different watermarking algorithms.\n\nFirst, the performance of the state of the art metrics currently used to evaluate the differences between 3D models are analyzed. These state of the art metrics are Mean and Maximum Geometric Error based on Hausdorff distance [10] [11]. Second, the performance of the proposed metrics are assessed and compared to those of the state of the art metrics.\n\n\nA. Hausdorff distances\n\nAs previously stated (Section II) two of the most common geometric metrics used to measure the similarity between two 3D objects, d \u221e (.) and d 1 (.), are based on the Hausdorff distance between surfaces. Here, we want to evaluate if the Hausdorff distance between the original and the watermarked models could be a reliable metric for perceptual watermarking impairments prediction. To do this, the Hausdorff distances between watermarked models and the originals are plotted versus the M OS provided by Experiment I. At this point, the linear correlation coefficient of Pearson (r P ) [31] or the non-linear (rank) correlation coefficient of Spearman (r S ) [32] are calculated in order to evaluate the global performance of the geometric and proposed metrics obtained by fitting these geometric data with the cumulative Gaussian in Eq. (10). The geometric metric results do not correlate well with subjective M OS. This underlines the fact that d \u221e (.) and d 1 (.) are not reflecting how humans perceive geometric artifacts due to watermarking. The results are summarized in the first two columns of Table I, and will be used as a reference to compare with the performance of our perceptual metrics.\n\n\nB. Roughness-based Metrics in Experiment I\n\nAs stated before, the goal of the first experiment was to make an initial study on the perception of the geometric artifacts caused by watermarking algorithms and to lay the basis for the development of perceptual metrics to measure the visual impact of such artifacts. The experimental data confirm that the subjective perception of the impairments is welldescribed by the measure of roughness. The subjective data of this experiment are used to obtain two perceptual metrics,  Fig. 5. On the right top of the graphs it is possible to notice some points outside the fitting curve. Most of these outliers correspond to the Venus model. This is due to the fact that the Venus model represents a human face. Human face images are well-known in subjective experiments as a high level factor attracting human attention, i.e. people perceive human faces differently, so the distortions on the Venus head are perceived as more visible and annoying with respect to other models.\n\n\nC. Perceptual Metrics Performance\n\nAs discussed in the previous section, the two proposed objective metrics have been transformed into two corresponding perceptual metrics using the data from Experiment I. In order to evaluate such metrics, Experiment II was carried out with three other watermarking algorithms: KDK, NBE and VFA. The validation is very simple: the perceptual metrics obtained in Experiment I are used to predict the M OS obtained in the second experiment and their correlation coefficients are computed. The correlation coefficients r P and r S are reported in Table I Fig. 5 show the values of the objective metrics plotted versus the subjective M OS for several watermarking algorithms. It is important to underline that the curves drawn on this figure do not represent the results of a fit as the same psychometric Gaussian curve obtained with the subjective data of the Experiment I are drawn for all the pictures. In other words, these graphs visualize the behavior of the KDK, the NBE and the VFA algorithms with respect to the developed perceptual metrics, that is represented by the solid curve (the dashed line is the confidence interval for that curve).\n\n\nVII. CONCLUSIONS\n\nIn this work, our investigations about the extension of the perceptual image watermarking to 3D watermarking have been presented. In particular, a new experimental methodology for subjective quality assessment of watermarked 3D objects has been proposed. The analysis of the data collected by two subjective experiments that use this methodology demonstrates that such a methodology is appropriate and provides reliable subjective data about quality evaluation of watermarked 3D objects. Moreover, two perceptual metrics for 3D watermarking impairments prediction have been developed by combining roughness estimation with subjective data. The performance of these metrics have been analyzed. The results of this analysis demonstrate the effectiveness of the proposed perceptual metrics with respect to the state-of-the-art geometric metrics commonly used for 3D models comparison. More importantly, the experimental results show that the proposed metrics provide a good prediction of the human perception of the distortions introduced by 3D watermarking. Hence, these metrics could be used in a feedback mechanism to tune the watermarking parameters of 3D watermarking algorithms optimizing the watermark insertion. For example, referring to the UCB algorithm, for each level of resolution, the maximum amount of watermark strength before reaching watermark perceptibility could be computed by using these metrics, thus improving the robustness of the algorithm while ensuring imperceptibility.\n\n\nA. Visual Masking and Texturing\n\nMasking is a well-known perceptual effect of both human auditory and visual system caused by the fact that the frequency content in certain channels suppress the perceptibility of other frequencies in that channel. In 3D model watermarking, this effect can be caused by the application of image texture on the models' surfaces or by the local peculiarity of the surface. For example, a surface that reproduce a particular fine pattern could mask the watermarking during visualization. The proposed metrics compensate well the visual masking effect caused by geometry since they rely on the relative roughness of the model. However, the developed metrics does not take into account the visual masking caused by texture. Hence, due to the importance of textured 3D models, we would like to further analyze some issues related to objects with texture mapping and the proposed metrics. Ferwerda et al. [15] are the first dealing with this issue. They demonstrate that texture images could make geometric artifacts of a surface less perceptible. In their work, the degree of masking of a texture is evaluated by taking into account the contrast, orientation and spatial frequency content of the final rendered image. Yixin Pan et al. [33] propose a different interesting approach related to the quality assessment (in terms of geometric and texture resolution) of 3D models. Their work underline that the perceptual contribution of image texture is, in general, more important than the model's geometry. According to their results, we can state that, in the general case, the perceptibility of the watermark on a textured model should be lower w.r.t. the same model rendered without textures. Consequently, our metrics can be considered as an \"upper bound\" of the detectability of the watermark on a textured model. In other words, if the impairment predicted by our metrics is low on a 3D mesh, such impairment will be even low(er) for a 3D model with textures.\n\nConcluding, we can state that, despite the fact that the perceptual evaluation of geometric artifacts is a very difficult task due to the enormous number of influencing factors, the results obtained are encouraging. Further research can regard the evaluation of the performance of the proposed metrics under different rendering conditions and their extension to take into account the influence of local properties of the surface (e.g. curvature, protrusions) on the perception of geometric artifacts as well as visual masking effects due to texture mapping. \n\nFig. 1 .\n13 \u2212T 4 , T 4 \u2212T 5 and T 5 \u2212T . In the same way G(v 2 ) and G(v 3 ) are the mean roughness associated to the dihedral angles of the faces adjacent to the vertices v 2 and v 3 . Instead, V (v 1 ), V (v 2 ) and V (v 3 ) are the variances of the roughness \u03c1 d Geometric defects introduced by 3D watermarking algorithms.\n\nFig. 2 .\n2(a) Dihedral angle. (b) mean roughness G(.), and variance roughness V (.).\n\nFig. 3 .\n3Bumps with different scale.\n\nFig. 4 .\n4Smoothing-based Roughness Estimation.\n\n\nnamed R * 1 (M, M w ) and R * 2 (M, M w ), from the corresponding proposed objective metrics R 1 (M, M w ) and R 2 (M, M w ). These perceptual metrics are obtained by fitting the subjective data with the Gaussian psychometric curve in Eq. (10). This leads to two perceptual metrics obtained. One based on multiscale roughness measure and another based on smoothingbased roughness estimation. The parameters of the Gaussian psychometric curve after the fitting are (a = 1.9428, b = \u22120.2571) for R 1 (M, M w ) and (a = 2.0636, b = \u22120.2981) for R 2 (M, M w ). The smoothing-based metric provides a better fit (r P = 0.82, r S = 0.89) than the multi-scale (r P = 0.67, r S = 0.86) as depicted in\n\nFig. 5 .\n5Experiment I and Experiment II: Subjective M OS versus objective metrics curves fits. In Experiment II the parameters of the fitting curve are the same of the Experiment I.\n\n\nis with ISTI-CNR of Pisa, Italy. mailto: massimiliano.corsini@isti.cnr.it. Phone:+390503152880. Fax:+390503152811 M. Barni is with Dept. of Information Engineering, University of Siena, Italy. mailto: barni@dii.unisi.it. Phone:+390577234621. Fax:+390577234632 E. Drelie Gelasca is with Department of Electrical and Computer Engineering, University of California, Santa Barbara, California mailto: elisa.drelie@a3.epfl.ch, Phone: +18058932526 T. Ebrahimi is with the Signal Processing Institute, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland. mailto: touradj.ebrahimi@epfl.ch, Phone:+41793319993. Fax:+41216937600\n\n\n. The rows indicate which watermarking algorithms were applied to the 3D models. The first two columns of this table report the Spearman correlation coefficients of the Maximum and Mean Geometric Errors for comparison. The third and the fourth show the values of r P and r S for The overall performance of the perceptual metrics for the watermarking algorithms that introduce uniform distortions are reported in the sixth row of the table. The values of the correlation coefficients (r S = 0.8416 for the first metric and r S = 0.8954 for the second metric) are very good. Hence, one can claim that the developed metrics provide good prediction of the impairment caused by 3D watermarking where artifacts are uniform.R  *  \n1 (M, M w ), while the last two columns are the r P and r S \nvalues for R  *  \n2 (M, M w ). Referring to this table the following \nconsiderations are in order: \n\n\u2022 Overall, both geometric metrics based on the Hausdorff \ndistance (reported in the first two columns) do not cor-\nrelate well with the subjective data. On the other hand \nthe developed metrics exhibit good correlations with the \n\nsubjective data, in particular concerning the Spearman's \ncoefficient. \n\u2022 The first row reports the UCB's correlation coefficients \nvalues for reference. \n\u2022 The Spearman's coefficients for the NBE and VFA algo-\nrithms (second and third rows respectively) demonstrate \nthat both metrics are able to predict impairment intro-\nduced by these two algorithms. \n\u2022 The worst performance of the proposed metrics are \nobtained for the KDK algorithm. This can be explained \nby considering that the distortions produced by this \nalgorithm are non-uniform. \n\u2022 The fifth row summarizes the overall performance of the \nproposed metrics for the other three watermarking algo-\nrithms tested in Experiment II. The values of correlation \ncoefficients (r S = 0.71 for the first metric, r S = 0.6929 \nfor the second metric) outperform the results provided \nby the state-of-the-art metrics (r S = 0.3759 for the \nMaximum Hausdorff metric, r S = 0.4853 for the Mean \nHausdorff metric). \n\u2022 \n\nTABLE I PERCEPTUAL\nIMETRICS PERFORMANCES.In order to visualize the results of Experiment II, the graphs inHausdorff Distance \nR  *  \n1 (M, M w ) R  *  \n2 (M, M w ) \nAlgo. Max (r S ) Mean (r S ) \nr P \nr S \nr P \nr S \nUCB \n0.67 \n0.66 \n0.67 \n0.87 \n0.83 \n0.90 \nNBE \n0.71 \n0.70 \n0.56 \n0.80 \n0.62 \n0.81 \nVFA \n0.50 \n0.88 \n0.75 \n0.94 \n0.78 \n0.91 \nKDK \n0.70 \n0.32 \n0.62 \n0.72 \n0.55 \n0.71 \nKDK \nNBE \n0.38 \n0.49 \n0.49 \n0.71 \n0.50 \n0.69 \nVFA \nUCB \nNBE \n0.52 \n0.62 \n0.65 \n0.84 \n0.74 \n0.90 \nVFA \n\n\nACKNOWLEDGMENTSThe authors would like to thank John M. Foley for his valuable inputs, particularly in subjective experiments design.\nAre image quality metrics adequate to evaluate the quality of geometric objects?\" in Human Vision and Electronic Imaging. B Rogowitz, H Rushmeier, SPIE Proc. VI, B. E. Rogowitz and T. N. Pappas4299B. Rogowitz and H. Rushmeier, \"Are image quality metrics adequate to evaluate the quality of geometric objects?\" in Human Vision and Electronic Imaging VI, B. E. Rogowitz and T. N. Pappas, Eds., vol. 4299. SPIE Proc., 2001, pp. 340-348.\n\nA review of watermarking and the importance of perceptual modeling. I Cox, M L Miller, Proceedings of SPIE. SPIE3016Human Vision and Electronic Imaging III. Cox and M. L. Miller, \"A review of watermarking and the importance of perceptual modeling,\" in Proceedings of SPIE: Vol. 3016. Human Vision and Electronic Imaging II, Feb. 1997, pp. 92-99.\n\nPerceptual watermarks for digital images and video. R Wolfgang, C I Podilchuk, E J Delp, IEEE PROC. 877R. Wolfgang, C. I. Podilchuk, and E. J. Delp, \"Perceptual watermarks for digital images and video,\" IEEE PROC, vol. 87, no. 7, pp. 1108-1126, 1999.\n\nImage-adaptive watermarking using visual models. C I Podilchuk, W Zeng, IEEE JSAC. 16C. I. Podilchuk and W. Zeng, \"Image-adaptive watermarking using visual models,\" IEEE JSAC, vol. 16, pp. 525-539, Apr. 1998.\n\nA digital watermark. R G V Schyndel, A Z Tirkel, C F Osborne, Proceedings of IEEE International Conference on Image Processing '94. IEEE International Conference on Image Processing '94Austin, Texas2R. G. V. Schyndel, A. Z. Tirkel, and C. F. Osborne, \"A digital watermark,\" in Proceedings of IEEE International Conference on Image Processing '94, vol. 2, Austin, Texas, Nov. 1994, pp. 86-90.\n\nMask building for perceptually hiding frequency embedded watermarks. F Bartolini, M Barni, V Cappellini, A Piva, Proceedings of the 5th IEEE International Conference on Image Processing. the 5th IEEE International Conference on Image ProcessingChicago, IL, USA98F. Bartolini, M. Barni, V. Cappellini, and A. Piva, \"Mask building for perceptually hiding frequency embedded watermarks,\" in Proceedings of the 5th IEEE International Conference on Image Processing, ICIP98, vol. I, Chicago, IL, USA, Oct. 1998, pp. 450-454.\n\nSubjective Video Quality Assessment Methods for Multimedia Applications Recommendation P.910. International Telecommunication Union. Geneva, SwitzerlandSubjective Video Quality Assessment Methods for Multimedia Applica- tions Recommendation P.910. International Telecommunication Union, Geneva, Switzerland, 1996.\n\nMethodology for Subjective Assessment of the Quality of Television Pictures Recommendation BT.500-11. International Telecommunication Union. Geneva, SwitzerlandMethodology for Subjective Assessment of the Quality of Television Pictures Recommendation BT.500-11. International Telecommunication Union, Geneva, Switzerland, 2002.\n\nToward perceptual metrics for video watermark evaluation. S Winkler, E Drelie Gelasca, T Ebrahimi, Applications of Digital Image Processing, ser. Proc. of SPIE. SPIE. SPIE5203S. Winkler, E. Drelie Gelasca, and T. Ebrahimi, \"Toward perceptual metrics for video watermark evaluation.\" in Applications of Digital Image Processing, ser. Proc. of SPIE, vol. 5203, SPIE. SPIE, August 2003, pp. 371-378.\n\nMesh: Measuring error between surfaces using the hausdorff distance. N Aspert, D Santa-Cruz, T Ebrahimi, Proceedings of the IEEE International Conference on Multimedia and Expo 2002 (ICME). the IEEE International Conference on Multimedia and Expo 2002 (ICME)IN. Aspert, D. Santa-Cruz, and T. Ebrahimi, \"Mesh: Measuring error between surfaces using the hausdorff distance,\" in Proceedings of the IEEE International Conference on Multimedia and Expo 2002 (ICME), vol. I, 2002, pp. 705-708.\n\nMetro: measuring error on simplified surfaces. R S P Cignoni, C Rocchini, Computer Graphics Forum. 172R. S. P. Cignoni, C. Rocchini, \"Metro: measuring error on simplified surfaces,\" Computer Graphics Forum, vol. 17, no. 2, pp. 167-174, 1998.\n\nImage-driven simplification. P Lindstrom, G Turk, ACM Transaction on Graphics. 193P. Lindstrom and G. Turk, \"Image-driven simplification,\" ACM Trans- action on Graphics, vol. 19, no. 3, pp. 204-241, 2000.\n\nPerceptually guided simplification of lit, textured meshes. N Williams, D Luebke, J D Cohen, M Kelley, B Schubert, Proceedings of the 2003 symposium on Interactive 3D graphics. the 2003 symposium on Interactive 3D graphicsMonterey, CaliforniaACM PressN. Williams, D. Luebke, J. D. Cohen, M. Kelley, and B. Schubert, \"Perceptually guided simplification of lit, textured meshes,\" in Proceed- ings of the 2003 symposium on Interactive 3D graphics. Monterey, California: ACM Press, 2003, pp. 113-121.\n\nA perceptually based adaptive sampling algorithm. M R Bolin, G W Meyer, Proceedings of the 25th annual conference on Computer graphics and interactive techniques. the 25th annual conference on Computer graphics and interactive techniquesACM PressM. R. Bolin and G. W. Meyer, \"A perceptually based adaptive sampling algorithm,\" in Proceedings of the 25th annual conference on Computer graphics and interactive techniques. ACM Press, 1998, pp. 299-309.\n\nA model of visual masking for computer graphics. J A Ferwerda, P Shirley, S N Pattanaik, D P Greenberg, Proceedings of the 24th annual conference on Computer graphics and interactive techniques. the 24th annual conference on Computer graphics and interactive techniquesACM Press/Addison-Wesley Publishing CoJ. A. Ferwerda, P. Shirley, S. N. Pattanaik, and D. P. Greenberg, \"A model of visual masking for computer graphics,\" in Proceedings of the 24th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 1997, pp. 143-152.\n\nAutomatic lighting design using a perceptual quality metric. R Shacked, D Lischinki, Computer Graphics Forum. 203R. Shacked and D. Lischinki, \"Automatic lighting design using a perceptual quality metric,\" Computer Graphics Forum, vol. 20, no. 3, 2001.\n\nSensation and Perception. E B Goldstein, International Thomson Publishing Company4th EditionE. B. Goldstein, Sensation and Perception. 4th Edition: International Thomson Publishing Company, 1996.\n\nWavelet-based blind watermarking of 3D models. F Uccheddu, M Corsini, M Barni, Proceedings of the 2004 multimedia and security workshop on Multimedia and security. the 2004 multimedia and security workshop on Multimedia and securityMagdeburg, GermanyACM PressF. Uccheddu, M. Corsini, and M. Barni, \"Wavelet-based blind water- marking of 3D models,\" in Proceedings of the 2004 multimedia and security workshop on Multimedia and security. Magdeburg, Germany: ACM Press, 2004, pp. 143-154.\n\nTwo high capacity methods for embedding public watermarks into 3D polygonal models. O Benedens, Proceedings of the Multimedia and Security-Workshop at ACM Multimedia 99. the Multimedia and Security-Workshop at ACM Multimedia 99Orlando, FloridaO. Benedens, \"Two high capacity methods for embedding public wa- termarks into 3D polygonal models,\" in Proceedings of the Multimedia and Security-Workshop at ACM Multimedia 99, Orlando, Florida, 1999, pp. 95-99.\n\nWatermarking of 3D polygon based models with robustness against mesh simplification. Benedens, Proceedings of SPIE: Security and Watermarking of Multimedia Contents. SPIE: Security and Watermarking of Multimedia Contents3657Benedens, \"Watermarking of 3D polygon based models with robustness against mesh simplification,\" in Proceedings of SPIE: Security and Watermarking of Multimedia Contents, vol. 3657, 1999, pp. 329-340.\n\nDigital watermarking for 3D polygons using multiresolution wavelet decomposition. S Kanai, H Date, T Kishinami, Proceedings of the Sixth IFIP WG 5.2 International Workshop on Geometric Modeling: Fundamentals and Applications (GEO-6). the Sixth IFIP WG 5.2 International Workshop on Geometric Modeling: Fundamentals and Applications (GEO-6)Tokyo, JapanS. Kanai, H. Date, and T. Kishinami, \"Digital watermarking for 3D poly- gons using multiresolution wavelet decomposition,\" in Proceedings of the Sixth IFIP WG 5.2 International Workshop on Geometric Modeling: Fundamentals and Applications (GEO-6), Tokyo, Japan, Dec. 1998, pp. 296-307.\n\nA roughnessbased algorithm for perceptual watermarking of 3D meshes. F Uccheddu, M Corsini, M Barni, V Cappellini, Proceedings of the Tenth International Conference on Virtual System and Multimedia. the Tenth International Conference on Virtual System and MultimediaOgaki City, JapanF. Uccheddu, M. Corsini, M. Barni, and V. Cappellini, \"A roughness- based algorithm for perceptual watermarking of 3D meshes,\" in Pro- ceedings of the Tenth International Conference on Virtual System and Multimedia, Ogaki City, Japan, Nov. 2004.\n\nAn effective featurepreserving mesh simplification scheme based on face constriction. J.-H Wu, S.-M Hu, J.-G Sun, C.-L Tai, Proceedings of the 9th Pacific Conference on Computer Graphics and Applications. the 9th Pacific Conference on Computer Graphics and ApplicationsIEEE Computer Society12J.-H. Wu, S.-M. Hu, J.-G. Sun, and C.-L. Tai, \"An effective feature- preserving mesh simplification scheme based on face constriction,\" in Proceedings of the 9th Pacific Conference on Computer Graphics and Applications. IEEE Computer Society, 2001, p. 12.\n\nA multi-scale roughness metric for 3d watermarking quality assessment. M Corsini, E Drelie Gelasca, T Ebrahimi, Workshop on Image Analysis for Multimedia Interactive Services. Montreux, SwitzerlandM. Corsini, E. Drelie Gelasca, and T. Ebrahimi, \"A multi-scale roughness metric for 3d watermarking quality assessment,\" in Workshop on Image Analysis for Multimedia Interactive Services 2005, Montreux, Switzer- land., April 2005. WIAMIS Proc., 2005.\n\nObjective evaluation of the perceptual quality of 3d watermarking. E Gelasca, T Ebrahimi, M Corsini, M Barni, IEEE International Conference on Image Processing. Genoa, ItalyE. Drelie Gelasca, T. Ebrahimi, M. Corsini, and M. Barni, \"Objective evaluation of the perceptual quality of 3d watermarking,\" in IEEE International Conference on Image Processing, Genoa, Italy, September 2005., vol. I. ICIP Proc., 2005, pp. 241-244.\n\nA signal processing approach to fair surface design. G Taubin, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. the 22nd annual conference on Computer graphics and interactive techniquesACM PressG. Taubin, \"A signal processing approach to fair surface design,\" in Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. ACM Press, 1995, pp. 351-358.\n\nDiscrete fairing. L Kobbelt, Proceedings of the Seventh IMA Conference on the Mathematics of Surfaces '97. the Seventh IMA Conference on the Mathematics of Surfaces '97L. Kobbelt, \"Discrete fairing,\" in Proceedings of the Seventh IMA Conference on the Mathematics of Surfaces '97, 1997, pp. 101-131.\n\nImplicit fairing of irregular meshes using diffusion and curvature flow. M Desbrun, M Meyer, P Schr\u00f6der, A H Barr, Proceedings of the 26th annual conference on Computer graphics and interactive techniques. the 26th annual conference on Computer graphics and interactive techniquesACM Press/Addison-Wesley Publishing CoM. Desbrun, M. Meyer, P. Schr\u00f6der, and A. H. Barr, \"Implicit fairing of irregular meshes using diffusion and curvature flow,\" in Proceedings of the 26th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 1999, pp. 317-324.\n\nThe use of psychophysical data and models in the analysis of display system performance. J Lubin, Digital Images and Human Vision. A. B. WatsonEd. Cambridge, MassachusettsMIT PressJ. Lubin, \"The use of psychophysical data and models in the analysis of display system performance,\" in Digital Images and Human Vision, A. B. Watson, Ed. Cambridge, Massachusetts: MIT Press, 1993.\n\nPsychometric Scaling: A Tool for Imaging Systems Development. P G Engeldrum, Imcotek PressWinchester, MAP. G. Engeldrum, Psychometric Scaling: A Tool for Imaging Systems Development. Imcotek Press, Winchester, MA, 2000.\n\nFrom Math-World-A Wolfram Web Resource. E W Weisstein, Correlation coefficientE. W. Weisstein, \"Correlation coefficient,\" From Math- World-A Wolfram Web Resource. [Online]. Available: http://mathworld.wolfram.com/CorrelationCoefficient.html\n\n. E L Lehmann, H J D&apos;abrera, Nonparametrics: Statistical Methods Based on Ranks. Englewood Cliffs. Prentice HallE. L. Lehmann and H. J. D'Abrera, Nonparametrics: Statistical Methods Based on Ranks. Englewood Cliffs, NJ: Prentice Hall, 1998.\n\nQuality metric for approximating subjective evaluation of 3-d objects. Y Pan, I Cheng, A Basu, IEEE Transactions on. 72MultimediaY. Pan, I. Cheng, and A. Basu, \"Quality metric for approximating subjective evaluation of 3-d objects,\" Multimedia, IEEE Transactions on, vol. 7, no. 2, pp. 269-279, April 2005.\n", "annotations": {"author": "[{\"end\":63,\"start\":42},{\"end\":85,\"start\":64},{\"end\":103,\"start\":86},{\"end\":116,\"start\":104}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":55},{\"end\":84,\"start\":70},{\"end\":102,\"start\":94},{\"end\":115,\"start\":110}]", "author_first_name": "[{\"end\":54,\"start\":42},{\"end\":69,\"start\":64},{\"end\":93,\"start\":86},{\"end\":109,\"start\":104}]", "author_affiliation": null, "title": "[{\"end\":39,\"start\":1},{\"end\":155,\"start\":117}]", "venue": null, "abstract": "[{\"end\":970,\"start\":289}]", "bib_ref": "[{\"end\":2567,\"start\":2560},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2970,\"start\":2967},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4312,\"start\":4309},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5427,\"start\":5424},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5629,\"start\":5626},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5633,\"start\":5630},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5651,\"start\":5648},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5655,\"start\":5652},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5780,\"start\":5777},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5784,\"start\":5781},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5805,\"start\":5802},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6299,\"start\":6296},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6894,\"start\":6890},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6969,\"start\":6965},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7254,\"start\":7250},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7647,\"start\":7643},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7685,\"start\":7681},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8213,\"start\":8209},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8471,\"start\":8468},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9350,\"start\":9347},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9355,\"start\":9352},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10235,\"start\":10231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10714,\"start\":10710},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11472,\"start\":11468},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12067,\"start\":12064},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14149,\"start\":14145},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14390,\"start\":14386},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14426,\"start\":14422},{\"end\":14464,\"start\":14446},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14469,\"start\":14465},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15722,\"start\":15718},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16403,\"start\":16399},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16850,\"start\":16846},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20333,\"start\":20329},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20999,\"start\":20995},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21009,\"start\":21005},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21057,\"start\":21053},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23754,\"start\":23750},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24636,\"start\":24633},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25095,\"start\":25092},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26222,\"start\":26218},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27309,\"start\":27305},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28044,\"start\":28040},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28117,\"start\":28113},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28296,\"start\":28292},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33311,\"start\":33307},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33642,\"start\":33638}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35253,\"start\":34927},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35339,\"start\":35254},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35378,\"start\":35340},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35427,\"start\":35379},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36121,\"start\":35428},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36305,\"start\":36122},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36935,\"start\":36306},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39022,\"start\":36936},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39506,\"start\":39023}]", "paragraph": "[{\"end\":1734,\"start\":989},{\"end\":2194,\"start\":1736},{\"end\":3230,\"start\":2196},{\"end\":3908,\"start\":3232},{\"end\":4599,\"start\":3910},{\"end\":5123,\"start\":4601},{\"end\":5656,\"start\":5125},{\"end\":6300,\"start\":5658},{\"end\":7417,\"start\":6302},{\"end\":7937,\"start\":7419},{\"end\":9037,\"start\":7939},{\"end\":9356,\"start\":9094},{\"end\":9833,\"start\":9358},{\"end\":13553,\"start\":9835},{\"end\":13864,\"start\":13584},{\"end\":15268,\"start\":13866},{\"end\":15595,\"start\":15270},{\"end\":16280,\"start\":15630},{\"end\":16651,\"start\":16320},{\"end\":17175,\"start\":16653},{\"end\":17448,\"start\":17204},{\"end\":18400,\"start\":17546},{\"end\":19742,\"start\":18524},{\"end\":20007,\"start\":19810},{\"end\":20267,\"start\":20040},{\"end\":20826,\"start\":20311},{\"end\":21347,\"start\":20828},{\"end\":21850,\"start\":21390},{\"end\":22182,\"start\":21884},{\"end\":22265,\"start\":22216},{\"end\":22674,\"start\":22290},{\"end\":23460,\"start\":22732},{\"end\":23671,\"start\":23462},{\"end\":24425,\"start\":23714},{\"end\":24537,\"start\":24427},{\"end\":25242,\"start\":24539},{\"end\":25822,\"start\":25244},{\"end\":26334,\"start\":25865},{\"end\":27072,\"start\":26363},{\"end\":27426,\"start\":27074},{\"end\":28655,\"start\":27453},{\"end\":29673,\"start\":28702},{\"end\":30857,\"start\":29711},{\"end\":32373,\"start\":30878},{\"end\":34366,\"start\":32409},{\"end\":34926,\"start\":34368}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17203,\"start\":17176},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17545,\"start\":17449},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18425,\"start\":18401},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18474,\"start\":18425},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18523,\"start\":18474},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19760,\"start\":19743},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19809,\"start\":19760},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20039,\"start\":20008},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21389,\"start\":21348},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21883,\"start\":21851},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22215,\"start\":22183},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22731,\"start\":22675},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25864,\"start\":25823}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28563,\"start\":28556},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30262,\"start\":30255}]", "section_header": "[{\"end\":987,\"start\":972},{\"end\":9092,\"start\":9040},{\"end\":13582,\"start\":13556},{\"end\":15628,\"start\":15598},{\"end\":16318,\"start\":16283},{\"end\":20309,\"start\":20270},{\"end\":22288,\"start\":22268},{\"end\":23712,\"start\":23674},{\"end\":26361,\"start\":26337},{\"end\":27451,\"start\":27429},{\"end\":28700,\"start\":28658},{\"end\":29709,\"start\":29676},{\"end\":30876,\"start\":30860},{\"end\":32407,\"start\":32376},{\"end\":34936,\"start\":34928},{\"end\":35263,\"start\":35255},{\"end\":35349,\"start\":35341},{\"end\":35388,\"start\":35380},{\"end\":36131,\"start\":36123},{\"end\":39042,\"start\":39024}]", "table": "[{\"end\":39022,\"start\":37655},{\"end\":39506,\"start\":39130}]", "figure_caption": "[{\"end\":35253,\"start\":34938},{\"end\":35339,\"start\":35265},{\"end\":35378,\"start\":35351},{\"end\":35427,\"start\":35390},{\"end\":36121,\"start\":35430},{\"end\":36305,\"start\":36133},{\"end\":36935,\"start\":36308},{\"end\":37655,\"start\":36938},{\"end\":39130,\"start\":39044}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15167,\"start\":15159},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16813,\"start\":16803},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17569,\"start\":17559},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18784,\"start\":18778},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20825,\"start\":20819},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29187,\"start\":29181},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30269,\"start\":30263}]", "bib_author_first_name": "[{\"end\":39763,\"start\":39762},{\"end\":39775,\"start\":39774},{\"end\":40144,\"start\":40143},{\"end\":40151,\"start\":40150},{\"end\":40153,\"start\":40152},{\"end\":40475,\"start\":40474},{\"end\":40487,\"start\":40486},{\"end\":40489,\"start\":40488},{\"end\":40502,\"start\":40501},{\"end\":40504,\"start\":40503},{\"end\":40724,\"start\":40723},{\"end\":40726,\"start\":40725},{\"end\":40739,\"start\":40738},{\"end\":40906,\"start\":40905},{\"end\":40910,\"start\":40907},{\"end\":40922,\"start\":40921},{\"end\":40924,\"start\":40923},{\"end\":40934,\"start\":40933},{\"end\":40936,\"start\":40935},{\"end\":41347,\"start\":41346},{\"end\":41360,\"start\":41359},{\"end\":41369,\"start\":41368},{\"end\":41383,\"start\":41382},{\"end\":42501,\"start\":42500},{\"end\":42512,\"start\":42511},{\"end\":42519,\"start\":42513},{\"end\":42530,\"start\":42529},{\"end\":42910,\"start\":42909},{\"end\":42920,\"start\":42919},{\"end\":42934,\"start\":42933},{\"end\":43377,\"start\":43376},{\"end\":43381,\"start\":43378},{\"end\":43392,\"start\":43391},{\"end\":43602,\"start\":43601},{\"end\":43615,\"start\":43614},{\"end\":43839,\"start\":43838},{\"end\":43851,\"start\":43850},{\"end\":43861,\"start\":43860},{\"end\":43863,\"start\":43862},{\"end\":43872,\"start\":43871},{\"end\":43882,\"start\":43881},{\"end\":44327,\"start\":44326},{\"end\":44329,\"start\":44328},{\"end\":44338,\"start\":44337},{\"end\":44340,\"start\":44339},{\"end\":44778,\"start\":44777},{\"end\":44780,\"start\":44779},{\"end\":44792,\"start\":44791},{\"end\":44803,\"start\":44802},{\"end\":44805,\"start\":44804},{\"end\":44818,\"start\":44817},{\"end\":44820,\"start\":44819},{\"end\":45369,\"start\":45368},{\"end\":45380,\"start\":45379},{\"end\":45587,\"start\":45586},{\"end\":45589,\"start\":45588},{\"end\":45805,\"start\":45804},{\"end\":45817,\"start\":45816},{\"end\":45828,\"start\":45827},{\"end\":46330,\"start\":46329},{\"end\":47211,\"start\":47210},{\"end\":47220,\"start\":47219},{\"end\":47228,\"start\":47227},{\"end\":47836,\"start\":47835},{\"end\":47848,\"start\":47847},{\"end\":47859,\"start\":47858},{\"end\":47868,\"start\":47867},{\"end\":48386,\"start\":48382},{\"end\":48395,\"start\":48391},{\"end\":48404,\"start\":48400},{\"end\":48414,\"start\":48410},{\"end\":48917,\"start\":48916},{\"end\":48928,\"start\":48927},{\"end\":48935,\"start\":48929},{\"end\":48946,\"start\":48945},{\"end\":49362,\"start\":49361},{\"end\":49373,\"start\":49372},{\"end\":49385,\"start\":49384},{\"end\":49396,\"start\":49395},{\"end\":49773,\"start\":49772},{\"end\":50166,\"start\":50165},{\"end\":50522,\"start\":50521},{\"end\":50533,\"start\":50532},{\"end\":50542,\"start\":50541},{\"end\":50554,\"start\":50553},{\"end\":50556,\"start\":50555},{\"end\":51137,\"start\":51136},{\"end\":51489,\"start\":51488},{\"end\":51491,\"start\":51490},{\"end\":51688,\"start\":51687},{\"end\":51690,\"start\":51689},{\"end\":51892,\"start\":51891},{\"end\":51894,\"start\":51893},{\"end\":51905,\"start\":51904},{\"end\":51907,\"start\":51906},{\"end\":52208,\"start\":52207},{\"end\":52215,\"start\":52214},{\"end\":52224,\"start\":52223}]", "bib_author_last_name": "[{\"end\":39772,\"start\":39764},{\"end\":39785,\"start\":39776},{\"end\":40148,\"start\":40145},{\"end\":40160,\"start\":40154},{\"end\":40484,\"start\":40476},{\"end\":40499,\"start\":40490},{\"end\":40509,\"start\":40505},{\"end\":40736,\"start\":40727},{\"end\":40744,\"start\":40740},{\"end\":40919,\"start\":40911},{\"end\":40931,\"start\":40925},{\"end\":40944,\"start\":40937},{\"end\":41357,\"start\":41348},{\"end\":41366,\"start\":41361},{\"end\":41380,\"start\":41370},{\"end\":41388,\"start\":41384},{\"end\":42509,\"start\":42502},{\"end\":42527,\"start\":42520},{\"end\":42539,\"start\":42531},{\"end\":42917,\"start\":42911},{\"end\":42931,\"start\":42921},{\"end\":42943,\"start\":42935},{\"end\":43389,\"start\":43382},{\"end\":43401,\"start\":43393},{\"end\":43612,\"start\":43603},{\"end\":43620,\"start\":43616},{\"end\":43848,\"start\":43840},{\"end\":43858,\"start\":43852},{\"end\":43869,\"start\":43864},{\"end\":43879,\"start\":43873},{\"end\":43891,\"start\":43883},{\"end\":44335,\"start\":44330},{\"end\":44346,\"start\":44341},{\"end\":44789,\"start\":44781},{\"end\":44800,\"start\":44793},{\"end\":44815,\"start\":44806},{\"end\":44830,\"start\":44821},{\"end\":45377,\"start\":45370},{\"end\":45390,\"start\":45381},{\"end\":45599,\"start\":45590},{\"end\":45814,\"start\":45806},{\"end\":45825,\"start\":45818},{\"end\":45834,\"start\":45829},{\"end\":46339,\"start\":46331},{\"end\":46795,\"start\":46787},{\"end\":47217,\"start\":47212},{\"end\":47225,\"start\":47221},{\"end\":47238,\"start\":47229},{\"end\":47845,\"start\":47837},{\"end\":47856,\"start\":47849},{\"end\":47865,\"start\":47860},{\"end\":47879,\"start\":47869},{\"end\":48389,\"start\":48387},{\"end\":48398,\"start\":48396},{\"end\":48408,\"start\":48405},{\"end\":48418,\"start\":48415},{\"end\":48925,\"start\":48918},{\"end\":48943,\"start\":48936},{\"end\":48955,\"start\":48947},{\"end\":49370,\"start\":49363},{\"end\":49382,\"start\":49374},{\"end\":49393,\"start\":49386},{\"end\":49402,\"start\":49397},{\"end\":49780,\"start\":49774},{\"end\":50174,\"start\":50167},{\"end\":50530,\"start\":50523},{\"end\":50539,\"start\":50534},{\"end\":50551,\"start\":50543},{\"end\":50561,\"start\":50557},{\"end\":51143,\"start\":51138},{\"end\":51501,\"start\":51492},{\"end\":51700,\"start\":51691},{\"end\":51902,\"start\":51895},{\"end\":51921,\"start\":51908},{\"end\":52212,\"start\":52209},{\"end\":52221,\"start\":52216},{\"end\":52229,\"start\":52225}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16399426},\"end\":40073,\"start\":39640},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9537766},\"end\":40420,\"start\":40075},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14787966},\"end\":40672,\"start\":40422},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9221706},\"end\":40882,\"start\":40674},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16354013},\"end\":41275,\"start\":40884},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12576070},\"end\":41796,\"start\":41277},{\"attributes\":{\"id\":\"b6\"},\"end\":42111,\"start\":41798},{\"attributes\":{\"id\":\"b7\"},\"end\":42440,\"start\":42113},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9559309},\"end\":42838,\"start\":42442},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3132146},\"end\":43327,\"start\":42840},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17783159},\"end\":43570,\"start\":43329},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207628252},\"end\":43776,\"start\":43572},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":579143},\"end\":44274,\"start\":43778},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8570431},\"end\":44726,\"start\":44276},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2823227},\"end\":45305,\"start\":44728},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5475382},\"end\":45558,\"start\":45307},{\"attributes\":{\"id\":\"b16\"},\"end\":45755,\"start\":45560},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18989120},\"end\":46243,\"start\":45757},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7195701},\"end\":46700,\"start\":46245},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6420306},\"end\":47126,\"start\":46702},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5564556},\"end\":47764,\"start\":47128},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":86467565},\"end\":48294,\"start\":47766},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18279204},\"end\":48843,\"start\":48296},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":42207809},\"end\":49292,\"start\":48845},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9526572},\"end\":49717,\"start\":49294},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1019243},\"end\":50145,\"start\":49719},{\"attributes\":{\"id\":\"b26\"},\"end\":50446,\"start\":50147},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7773119},\"end\":51045,\"start\":50448},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":59774068},\"end\":51424,\"start\":51047},{\"attributes\":{\"id\":\"b29\"},\"end\":51645,\"start\":51426},{\"attributes\":{\"id\":\"b30\"},\"end\":51887,\"start\":51647},{\"attributes\":{\"id\":\"b31\"},\"end\":52134,\"start\":51889},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6591149},\"end\":52442,\"start\":52136}]", "bib_title": "[{\"end\":39760,\"start\":39640},{\"end\":40141,\"start\":40075},{\"end\":40472,\"start\":40422},{\"end\":40721,\"start\":40674},{\"end\":40903,\"start\":40884},{\"end\":41344,\"start\":41277},{\"end\":42498,\"start\":42442},{\"end\":42907,\"start\":42840},{\"end\":43374,\"start\":43329},{\"end\":43599,\"start\":43572},{\"end\":43836,\"start\":43778},{\"end\":44324,\"start\":44276},{\"end\":44775,\"start\":44728},{\"end\":45366,\"start\":45307},{\"end\":45802,\"start\":45757},{\"end\":46327,\"start\":46245},{\"end\":46785,\"start\":46702},{\"end\":47208,\"start\":47128},{\"end\":47833,\"start\":47766},{\"end\":48380,\"start\":48296},{\"end\":48914,\"start\":48845},{\"end\":49359,\"start\":49294},{\"end\":49770,\"start\":49719},{\"end\":50163,\"start\":50147},{\"end\":50519,\"start\":50448},{\"end\":51134,\"start\":51047},{\"end\":52205,\"start\":52136}]", "bib_author": "[{\"end\":39774,\"start\":39762},{\"end\":39787,\"start\":39774},{\"end\":40150,\"start\":40143},{\"end\":40162,\"start\":40150},{\"end\":40486,\"start\":40474},{\"end\":40501,\"start\":40486},{\"end\":40511,\"start\":40501},{\"end\":40738,\"start\":40723},{\"end\":40746,\"start\":40738},{\"end\":40921,\"start\":40905},{\"end\":40933,\"start\":40921},{\"end\":40946,\"start\":40933},{\"end\":41359,\"start\":41346},{\"end\":41368,\"start\":41359},{\"end\":41382,\"start\":41368},{\"end\":41390,\"start\":41382},{\"end\":42511,\"start\":42500},{\"end\":42529,\"start\":42511},{\"end\":42541,\"start\":42529},{\"end\":42919,\"start\":42909},{\"end\":42933,\"start\":42919},{\"end\":42945,\"start\":42933},{\"end\":43391,\"start\":43376},{\"end\":43403,\"start\":43391},{\"end\":43614,\"start\":43601},{\"end\":43622,\"start\":43614},{\"end\":43850,\"start\":43838},{\"end\":43860,\"start\":43850},{\"end\":43871,\"start\":43860},{\"end\":43881,\"start\":43871},{\"end\":43893,\"start\":43881},{\"end\":44337,\"start\":44326},{\"end\":44348,\"start\":44337},{\"end\":44791,\"start\":44777},{\"end\":44802,\"start\":44791},{\"end\":44817,\"start\":44802},{\"end\":44832,\"start\":44817},{\"end\":45379,\"start\":45368},{\"end\":45392,\"start\":45379},{\"end\":45601,\"start\":45586},{\"end\":45816,\"start\":45804},{\"end\":45827,\"start\":45816},{\"end\":45836,\"start\":45827},{\"end\":46341,\"start\":46329},{\"end\":46797,\"start\":46787},{\"end\":47219,\"start\":47210},{\"end\":47227,\"start\":47219},{\"end\":47240,\"start\":47227},{\"end\":47847,\"start\":47835},{\"end\":47858,\"start\":47847},{\"end\":47867,\"start\":47858},{\"end\":47881,\"start\":47867},{\"end\":48391,\"start\":48382},{\"end\":48400,\"start\":48391},{\"end\":48410,\"start\":48400},{\"end\":48420,\"start\":48410},{\"end\":48927,\"start\":48916},{\"end\":48945,\"start\":48927},{\"end\":48957,\"start\":48945},{\"end\":49372,\"start\":49361},{\"end\":49384,\"start\":49372},{\"end\":49395,\"start\":49384},{\"end\":49404,\"start\":49395},{\"end\":49782,\"start\":49772},{\"end\":50176,\"start\":50165},{\"end\":50532,\"start\":50521},{\"end\":50541,\"start\":50532},{\"end\":50553,\"start\":50541},{\"end\":50563,\"start\":50553},{\"end\":51145,\"start\":51136},{\"end\":51503,\"start\":51488},{\"end\":51702,\"start\":51687},{\"end\":51904,\"start\":51891},{\"end\":51923,\"start\":51904},{\"end\":52214,\"start\":52207},{\"end\":52223,\"start\":52214},{\"end\":52231,\"start\":52223}]", "bib_venue": "[{\"end\":40187,\"start\":40183},{\"end\":41082,\"start\":41016},{\"end\":41537,\"start\":41464},{\"end\":41950,\"start\":41931},{\"end\":43098,\"start\":43030},{\"end\":44020,\"start\":43955},{\"end\":44513,\"start\":44439},{\"end\":44997,\"start\":44923},{\"end\":46007,\"start\":45921},{\"end\":46488,\"start\":46415},{\"end\":46922,\"start\":46868},{\"end\":47479,\"start\":47362},{\"end\":48049,\"start\":47965},{\"end\":48565,\"start\":48501},{\"end\":49042,\"start\":49021},{\"end\":49467,\"start\":49455},{\"end\":49947,\"start\":49873},{\"end\":50315,\"start\":50254},{\"end\":50728,\"start\":50654},{\"end\":51218,\"start\":51190},{\"end\":39796,\"start\":39787},{\"end\":40181,\"start\":40162},{\"end\":40520,\"start\":40511},{\"end\":40755,\"start\":40746},{\"end\":41014,\"start\":40946},{\"end\":41462,\"start\":41390},{\"end\":41929,\"start\":41798},{\"end\":42252,\"start\":42113},{\"end\":42601,\"start\":42541},{\"end\":43028,\"start\":42945},{\"end\":43426,\"start\":43403},{\"end\":43649,\"start\":43622},{\"end\":43953,\"start\":43893},{\"end\":44437,\"start\":44348},{\"end\":44921,\"start\":44832},{\"end\":45415,\"start\":45392},{\"end\":45584,\"start\":45560},{\"end\":45919,\"start\":45836},{\"end\":46413,\"start\":46341},{\"end\":46866,\"start\":46797},{\"end\":47360,\"start\":47240},{\"end\":47963,\"start\":47881},{\"end\":48499,\"start\":48420},{\"end\":49019,\"start\":48957},{\"end\":49453,\"start\":49404},{\"end\":49871,\"start\":49782},{\"end\":50252,\"start\":50176},{\"end\":50652,\"start\":50563},{\"end\":51176,\"start\":51145},{\"end\":51486,\"start\":51426},{\"end\":51685,\"start\":51647},{\"end\":51991,\"start\":51923},{\"end\":52251,\"start\":52231}]"}}}, "year": 2023, "month": 12, "day": 17}