{"id": 219980389, "updated": "2023-10-06 14:31:43.776", "metadata": {"title": "Multi-source Domain Adaptation via Weighted Joint Distributions Optimal Transport", "authors": "[{\"first\":\"Rosanna\",\"last\":\"Turrisi\",\"middle\":[]},{\"first\":\"R'emi\",\"last\":\"Flamary\",\"middle\":[]},{\"first\":\"Alain\",\"last\":\"Rakotomamonjy\",\"middle\":[]},{\"first\":\"Massimiliano\",\"last\":\"Pontil\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "The problem of domain adaptation on an unlabeled target dataset using knowledge from multiple labelled source datasets is becoming increasingly important. A key challenge is to design an approach that overcomes the covariate and target shift both among the sources, and between the source and target domains. In this paper, we address this problem from a new perspective: instead of looking for a latent representation invariant between source and target domains, we exploit the diversity of source distributions by tuning their weights to the target task at hand. Our method, named Weighted Joint Distribution Optimal Transport (WJDOT), aims at finding simultaneously an Optimal Transport-based alignment between the source and target distributions and a re-weighting of the sources distributions. We discuss the theoretical aspects of the method and propose a conceptually simple algorithm. Numerical experiments indicate that the proposed method achieves state-of-the-art performance on simulated and real-life datasets.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.12938", "mag": "3036581958", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/uai/TurrisiFRP22", "doi": null}}, "content": {"source": {"pdf_hash": "319d9ce08c6220afd8cb190c09c848245c2f9814", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.12938v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e2188c514b1b6d4cdc72b6f5aa6e5151ef2fa71f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/319d9ce08c6220afd8cb190c09c848245c2f9814.txt", "contents": "\nMulti-source Domain Adaptation via Weighted Joint Distributions Optimal Transport\n\n\nRosanna Turrisi \nDIBRIS, MaLGa\nUniversity of Genova\nGenoa\n\nCTSNC\nIstituto Italiano di Tecnologia\nFerraraItaly\n\nR\u00e9mi Flamary \nInstitut Polytechnique de Paris\nCMAP\n\u00c9cole Polytechnique\n\n\nAlain Rakotomamonjy \nCriteo AI Lab\nIstituto Italiano di Tecnologia\nParis 4 CSMLGenoaItaly\n\nMassimiliano Pontil \nDepartment of Computer Science\nUniversity College London\nU.K\n\nMulti-source Domain Adaptation via Weighted Joint Distributions Optimal Transport\n\nThis work addresses the problem of domain adaptation on an unlabeled target dataset using knowledge from multiple labelled source datasets. Most current approaches tackle this problem by searching for an embedding that is invariant across source and target domains, which corresponds to searching for a universal classifier that works well on all domains. In this paper, we address this problem from a new perspective: instead of crushing diversity of the source distributions, we exploit it to adapt better to the target distribution. Our method, named Multi-Source Domain Adaptation via Weighted Joint Distribution Optimal Transport (MSDA-WJDOT), aims at finding simultaneously an Optimal Transport-based alignment between the source and target distributions and a re-weighting of the sources distributions. We discuss the theoretical aspects of the method and propose a conceptually simple algorithm. Numerical experiments indicate that the proposed method achieves state-ofthe-art performance on simulated and real datasets.\n\nINTRODUCTION\n\nMany machine learning algorithms assume that the test and training datasets are sampled from the same distribution. However, in many real-world applications, new data can exhibit a distribution change (domain shift) that degrades the algorithm performance. This shift can be observed for instance in computer vision when changing background, location, illumination or pose of the test images, or in speech recognition when the recording conditions or speaker accents are varying. To overcome this problem, Domain Adaptation (DA) [Jiang, 2008, Kouw andLoog, 2019] attempts to leverage labelled data from a source domain, in order to learn a classifier for unseen or unlabelled data in a target domain.\n\nSeveral DA methods incorporate a distribution discrepancy loss into a neural network to overcome the domain gap. The distances between distributions are usually measured through an adversarial loss [Ganin et al., 2016, Ghifary et al., 2016, Tzeng et al., 2015, 2017 or integral probability metrics, such as the maximum mean discrepancy [Long et al., 2016, Tzeng et al., 2014. DA techniques based on Optimal Transport have been proposed by [Courty et al., 2016, 2017, Damodaran et al., 2018 and justified theoretically by Redko et al. [2017].\n\nIn this work, we focus on the setting, more common in practice, in which several labelled sources are available, denoted in the following as multi-source domain adaptation (MSDA) problem. Many recent approaches motivated by theoretical considerations have been proposed for this problem. For instance, Mansour et al. [2009], Hoffman et al. [2018] provided theoretical guarantees on how several source predictors can be combined using proxy measures, such as the accuracy of a hypothesis. This approach can achieve a low error predictor on the target domain, under the assumption that the target distribution can be written as a convex combination of the source distributions.\n\nOther MSDA methods [Peng et al., 2019, Zhao et al., 2018, Wen et al., 2020 look for a single hypothesis that minimizes the convex combination of its error on all source domains and they provide theoretical bounds of the error of the obtained hypothesis on the target domain. Those guarantees generally involve some terms depending on the distance between each source distribution and the target distribution and suggest to find an embedding in which the feature distributions between sources and target are as close as possible, by using Adversarial Learning [Zhao et al., 2018, Xu et al., 2018, Lin et al., 2020 or Moment Matching [Peng et al., 2019]. However, it may not be possible to find an embedding preserving discrimination even when the distances between source and target marginals are small. One such example is given in Figure 1, in which a rotation between the sources prevents the existence of such invariant embedding as theorized by Zhao et al. [2019]. At last, we mention the very recent line of works on MSDA considering approaches inspired from imitation learning Nguyen et al. [2021a,b] and the work by Montesuma and Mboula [2021] building on Wasserstein barycenters.\n\nContributions In this paper, we address the MSDA problem following a radically different route than the usual approach consisting in looking for a latent representation in which all source distributions are similar to the target. The approach we advocate embraces the diversity of source distributions and look for a convex combination of the joint source distributions with minimal Wasserstein distance to an estimated target distribution, without relying on a proxy measure such as the accuracy of source predictors. We support this novel conceptual approach by deriving a generalization bound on the target error. Our algorithm consists in optimizing a key term in this generalization bound, given by the Wasserstein distance between the estimated joint target distribution and a weighted sum of the joint source distributions. One unique feature of our approach is that the weights of the source distribution are learned simultaneously with the classification function, which allows us to distribute the mass based on the similarity of the sources with the target, both in the feature and in the output spaces. As such, our model can also handle problems in which only target shift occurs. Interestingly the estimated weights provide a measure of domain relatedness and interpretability. We refer to the proposed method as Multi-Source Domain Adaptation via Weighted Joint Distribution Optimal Transport (MSDA-WJDOT).\n\nNotations Let g : X \u2192 G be a differentiable embedding function, with G the embedding space. Throughout the paper all input distributions are in this embedding space. We let p S and p T be the true joint distributions in the source and target domains, respectively. Both distributions are supported on the product space G \u00d7 Y, where Y is the label space. In practice we only have access to a finite number N S of samples in the source domain leading to the empirical\nsource distributionp S = 1 N S N S i=1 \u03b4 g(x i S ),y i S where \u03b4 is the Dirac function.\nIn the target domain, only a finite number of unlabeled samples N T in the feature space is available. We then denote with\n\u2206 J := {\u03b1 \u03b1 \u03b1 \u2208 [0, 1] J | J i=1 \u03b1 i = 1} the (J \u2212 1)-dimensional simplex.\nFinally, given a loss function L and a joint distribution p, the expected loss of a function f is defined as \u03b5 p (f ) = E (x,y)\u223cp [L(y, f (x))].\n\n\nOPTIMAL TRANSPORT AND DA\n\nIn this section we first recall the Optimal Transport problem and the notion of Wasserstein distance. Then we discuss how they were exploited for domain adaptation (DA) in the Joint Distribution Optimal Transport (JDOT) formulation that will be central in our approach.\n\nOptimal Transport The Optimal transport (OT) problem has been originally introduced by Monge [1781] and, reformulated as a relaxation by Kantorovich [1942]. Let\n\u00b5 S = i a S i \u03b4 x i S ,\u03bc T = i a T i \u03b4 x i T be discrete probability measures with a S a S a S , a T a T a T \u2208 \u2206 J . The OT problem searches a transport plan \u03c0 \u2208 \u03a0(\u03bc S ,\u03bc T ), where \u03a0(\u03bc S ,\u03bc T ) := \u03c0 \u2265 0 J i=1 \u03c0 i,j = a T j , J j=1 \u03c0 i,j = a S i ,\nthat is, the set of joint probabilities with marginals \u00b5 1 and \u00b5 2 , that solve the following problem:\nW C (\u03bc S ,\u03bc T ) = min \u03c0\u2208\u03a0(\u03bc S ,\u03bc T ) J i,j=1 C i,j \u00b7 \u03c0 i,j(1)\nwhere C i,j = c(x i S , x j T ) represents the cost of transporting mass between x i S and x j T for a given ground cost function c : X \u00d7 X \u2192 R + . It is often chosen to be the Euclidean distance, recovering the classical W 1 Wasserstein distance. Given a ground cost C, W C (\u03bc S ,\u03bc T ) corresponds to the minimal cost for mapping one distribution to the other and \u03c0 is the OT matrix describing the relations between source and target samples. OT and in particular Wasserstein distance have been used with success in numerous machine learning applications such as Generative Adversarial Modeling [Arjovsky et al., 2017, Genevay et al., 2018 and DA [Courty et al., 2016, 2017, Shen et al., 2018.\n\nJoint Distribution Optimal Transport (JDOT) This method has been proposed by Courty et al. [2017] to address the problem of unsupervised DA with only one joint source distributionp S and the feature marginal target distribution \u00b5 T . Since no labels are available in the target domain, the authors proposed to use a proxy joint empirical distributionp f T whereby labels are replaced by the prediction of a classifier f : G \u2192 Y, that i\u015d\np f T = 1 N T N T i=1 \u03b4 g(x i T ),f (g(x i T )) .(2)\nIn order to use a joint distribution in the Wasserstein distance, they defined, for z, z \u2208 G and y, y \u2208 Y, the cost D(z, y; z , y ) = \u03b2 z \u2212 z 2 + L(y, y )\n\nwhere L is a loss between classes and \u03b2 weights the strength of feature loss. This cost takes into account embedding and label discrepancy. To train a meaningful classifier on the target domain, Courty et al. [2017] solved the problem\nmin f W D (p S ,p f T )(3)\nwhere the minimization is over a suitable set of classifiers and the objective W D (p S ,p f T ) is a Wasserstein distance between the joint source and joint \"predicted\" target,\nmin \u03c0\u2208\u03a0(p S ,p f T ) J i,j=1 D(g(x i S ), y i S ; g(x j T ), f (g(x j T ))) \u00b7 \u03c0 i,j .\nJDOT has been supported by generalization error guarantees, [see Courty et al., 2017, for a discussion]. It was later extended to deep learning framework where the embedding g was estimated simultaneously with the classifier f , via an efficient stochastic optimization procedure in [Damodaran et al., 2018]. A key aspect of JDOT, that was overlooked by the domain adaptation community, is the fact that the optimization problem involves the joint embedding/label distribution. This is in contrast to a large majority of DA approaches [Ganin et al., 2016, Sun and Saenko, 2016, Shen et al., 2018 using divergences only on the marginal distributions, whereas using simultaneously feature and labels information is the basis of most generalization bounds as discussed in the next section.\n\n\nMULTI-SOURCE DA VIA WEIGHTED JOINT OPTIMAL TRANSPORT\n\nWe now discuss our MSDA approach. We assume to have J sources with joint distributions p S,j , for 1 \u2264 j \u2264 J. We define a convex combination of the source distributions\np \u03b1 S = J j=1 \u03b1 j p S,j(4)\nwith \u03b1 \u03b1 \u03b1 \u2208 \u2206 J and we present a novel generalization bound for MSDA problem that depends on p \u03b1 S . Then, we introduce the MSDA-WJDOT optimization problem and propose an algorithm to solve it. Finally, we discuss the relation between MSDA-WJDOT and other MSDA approaches.\n\n\nGENERALIZATION BOUND\n\nThe theoretical limits of DA are well studied and well understood since the work of Ben-David et al. [2010] that provided an \"impossibility theorem\" showing that, if the target distribution is too different from the source distribution, adaptation is not possible. However in the case of MSDA, one can exploit the diversity of the source domains and use only the sources close to the target distribution, thereby obtaining a better generalization bound. For this purpose, a relevant assumption, already considered in Mansour et al. [2009], is that the target distribution is a convex combination of the source distributions. The soundness of such an approach is illustrated by the following lemma.\n\nLemma 1. For any hypothesis f \u2208 H, denote by \u03b5 p T (f ) and \u03b5 p \u03b1 S (f ), the expected loss of f on the target distribution and on the weighted sum of the source distributions, with respect to a loss function L bounded by B. Then\n\u03b5 p T (f ) \u2264 \u03b5 p \u03b1 S (f ) + B \u00b7 D T V (p \u03b1 S , p T ) (5)\nwhere D T V is the total variation distance.\n\nThis simple inequality, whose proof is presented in the appendix, tells us that the key point for target generalization is to have a function f with low error on a combination of the joint source distributions and that combination should be \"near\" to the target distribution. Note that this also holds for single source DA problem corroborating the recent findings that just matching marginal distributions may not be sufficient [Wu et al., 2019]. While the above lemma provides a simple and principled guidance for a multi-source DA algorithm, it cannot be used for training since it assumes that labels in the target domain are known. In the following, we provide a generalization bound in a realistic scenario where no target labels are available and a self-labelling strategy is employed to compensate for the missing labels.\n\nTaking inspiration from the result in Lemma 1, we propose a theoretically grounded framework for learning from multiple sources. To this end, we first recall the notion of Probabilistic Transfer Lipschitzness (PTL) of a classifier Courty et al. [2017], that will be used in our method.\n\nDefinition 1. (PTL Property) Let D be a metric on G and let \u03c6 : R \u2192 [0, 1]. A labeling function f : G \u2192 R and a joint distribution \u03c0 \u2208 \u03a0(\u00b5 S , \u00b5 T ) are \u03c6-Lipschitz transferable if for all \u03bb > 0, we have\nProb (x S ,x T )\u223c\u03c0 |f (x S )\u2212f (x T )| > \u03bbD(x S , x T ) \u2264 \u03c6(\u03bb).\nThe PTL property is a reasonable assumption for DA that was introduced in Courty et al. [2017] and provides a bound on the probability of finding pair of source-target samples of different label within a 1/\u03bb-ball.\n\nOur approach is based on the idea that one can compensate the lack of target labels by using an hypothesis labelling function f which provides a joint distribution p f T in (2), where f is searched in order to align p f T with a weighted combination of source distributions p \u03b1 S . Following this idea, we introduce the definition of similarity measure and a new generalization bound for MSDA. \n\u039b(p \u03b1 S , p T ) = min f \u2208H \u03b5 p \u03b1 S (f ) + \u03b5 p T (f ),(6)\nwhere the risk is measured w.r.t. to a symmetric and k-Lipschitz loss function that satisfies the triangle inequality.\n\nTheorem 1. Let H be the space introduced in Definition 2 and assume that the function f * minimizing Eq. 6 satisfies the PTL property (Definition 1). Letp S,j be j-th source empirical distributions of N j samples andp T the empirical target distribution with N T samples. Then for all \u03bb > 0 , with \u03b2 = \u03bbk in the ground metric D, we have with probability at least 1 \u2212 \u03b7 that\n\u03b5 p T (f ) \u2264W D p \u03b1 S ,p f T + 2 c log 2 \u03b7 \uf8eb \uf8ed 1 N T + J j=1 \u03b1 j N j \uf8f6 \uf8f8 + \u039b(p \u03b1 S , p T ) + kM \u03c6(\u03bb).\nNote that the quantity \u039b(p \u03b1 S , p T ) in the bound measures the discrepancy between the true target distribution and the \"best\" combination of the source distributions and, similarly to some terms in the DA bounds of Ben-David et al. [2010], it is not directly controllable. However, we have experimentally checked that our approach minimizes an upper bound of this term \u039b -see discussion in Section 4 and Figure 11 in the appendix. Interestingly the 1/N j ratios in the bound are weighted by \u03b1 j which means that even if one source is poorly sampled it won't have a large impact as soon as the coefficient \u03b1 j stays small. This suggests to investigate some kind of regularization for the weights \u03b1 but since it would introduce one more hyperparameter we left it to future works and in the following focus only on optimizing the first term of the bound.\n\n\nMSDA-WJDOT PROBLEM\n\nMSDA-WJDOT Optimization Problem Our approach aims at finding a function f that aligns the distribution p f T with a convex combination J j=1 \u03b1 j p S,j of the source distributions with convex weights \u03b1 \u03b1 \u03b1 \u2208 \u2206 J on the simplex. We express the multi-domain adaptation problem as\nmin \u03b1 \u03b1 \u03b1,f W D \uf8eb \uf8edp f T , J j=1 \u03b1 jpS,j \uf8f6 \uf8f8 .(7)\nProblem above is a minimization of the first term in the bound from Theorem 1 with respect to both f and \u03b1 \u03b1 \u03b1. The role of the weight \u03b1 \u03b1 \u03b1 is crucial because it allows in practice to select (when \u03b1 \u03b1 \u03b1 is sparse) the source distributions that are the closest in the Wasserstein sense and use only those distributions to transfer label knowledge from. An example of the method is provided in Figure 1 showing 4 source distributions in 2D obtained from rotation in the 2D space. One interesting property of our approach is that it can adapt to a lot of variability in the source distributions as long as the distributions lie in a distribution manifold and this Algorithm 1 Optimization for MSDA-WJDOT Initialise \u03b1 \u03b1 \u03b1 = 1 J 1 J and \u03b8 \u03b8 \u03b8 parameters of f \u03b8 \u03b8 \u03b8 and steps \u00b5 \u03b1 \u03b1 \u03b1 and \u00b5 \u03b8 \u03b8 \u03b8 .\nrepeat \u03b8 \u03b8 \u03b8 \u2190 \u03b8 \u03b8 \u03b8 \u2212 \u00b5 \u03b8 \u03b8 \u03b8 \u2207 \u03b8 \u03b8 \u03b8 W D p f T , J j=1 \u03b1 jpS,j \u03b1 \u03b1 \u03b1 \u2190 P \u2206 J \u03b1 \u03b1 \u03b1 \u2212 \u00b5 \u03b1 \u03b1 \u03b1 \u2207 \u03b1 \u03b1 \u03b1 W D (p f T , J j=1 \u03b1 jpS,j ) until Convergence manifold is sampled correctly by the source distributions.\nFor instance the linear weights allow to interpolate between source distributions and recover the weighted source that is the closest to the manifold of distribution, hence providing a tightest generalization as shown in the previous section.\n\nOptimization Algorithm Problem (7) can be solved with a block coordinate descent similarly to what was proposed in Courty et al. [2017]. But with the introduction of the weights \u03b1 \u03b1 \u03b1 we numerically observed that one can easily get stuck in a local minimum with poor performances. So we proposed the optimization approach in Algorithm 2, that is an alternated projected gradient descent w.r.t. the parameters \u03b8 \u03b8 \u03b8 of the classifier f \u03b8 \u03b8 \u03b8 and the weights \u03b1 \u03b1 \u03b1 of the sources. Note that the sub-gradient of \u2207 \u03b8 \u03b8 \u03b8 W is computed by solving the OT problem and using the fixed OT matrix to compute the gradient similarly to Damodaran et al. [2018]. It is well known that the subgradient w.r.t. the weights of a distribution can be expressed as \u2207 w w w W (\u00b5, J i=1 w i \u03b4 xi ) = \u03b2 \u03b2 \u03b2 where \u03b2 \u03b2 \u03b2 is the optimal right dual variable of the problem. Moreover, the sub-gradient \u2207 \u03b1 \u03b1 \u03b1 W can be computed in closed form as\n\u2207 \u03b1j W D \uf8eb \uf8edp f T , J j=1 \u03b1 j N j Nj i=1 \u03b4 (g(x i j ),y i j ) \uf8f6 \uf8f8 = N j Nj i=1 \u03b2 * j,i\nwhere \u03b2 * j,i is the dual variable for sample i in source domain j. The definition of the projection to the simplex P \u0398 is provided in supplementary materials. Also note that while we did not need it in the numerical experiments, Algorithm 2 can be performed on mini-batches by sub-sampling the source and target distribution on very large datasets as suggested in Damodaran et al. [2018] which has been shown to provide robust estimators in Fatras et al. [2020].\n\n\nRELATED WORK\n\nMSDA approaches learning only the classifier MSDA-WJDOT is related to JDOT [Courty et al., 2017] but proposes a non-trivial extension of it to multisource domain adaption. Indeed, there are two simple ways to apply JDOT to multi-source DA, which we refer to as Concatenated JDOT (CJDOT) and Multiple JDOT (MJDOT). The first one consists in concatenating all the source samples into one source distribution (equivalent to uniform \u03b1 \u03b1 \u03b1 if all N j are , has been proposed to address only target shift (change in proportions between the classes) and satisfies a generalization bound showing that estimating the class proportion in the target distribution is key to recovering good performances. MSDA-WJDOT can also handle the target shift as a special case since the reweighting \u03b1 \u03b1 \u03b1 is directly related to the proportion of classes. A crucial difference between MSDA-WJDOT and the barycenter-based approaches described above is that they rely only on aligning marginal distributions, whereas the proposed method aligns joint distributions by optimizing a Wasserstein distance in the joint embedding/label space. Also note that MSDA-WJDOT relies on a weighting of the samples where the weight is shared inside the source domains. This is a similar approach to DA approaches such as Importance Weighted Empirical Risk Minimization (IWERM) [Sugiyama et al., 2007] designed for Covariate Shift that use a reweighing of all the samples. One major difference is that we only estimate a relatively small number of weights in \u03b1 \u03b1 \u03b1 leading to a better posed statistical estimation. It is indeed well known that estimation of continuous density which is necessary for a proper individual reweighting of the samples is a very difficult problem in high dimension. All the above mentioned methods do not require to learn an embedding, whose estimation may be computationally expensive and unnecessary (e.g., when a pre-trained model is available). Further, there exists numerous examples of source variability in real life (such as rotation between the full distributions) that cannot be handled with a global embedding.\n\nMSDA approaches estimating an embedding As discussed in the introduction, the majority of recent DA approaches based on deep learning [Ganin et al., 2016, Sun and Saenko, 2016, Shen et al., 2018 relies on the estimation of an embedding that is invariant to the domain which means that the final classifier is shared across all domains when the embedding g is estimated. Those approaches have been extended to multiple sources with the objective that the embedded distributions between sources and target are similar. Authors in Xu et al. [2018] propose an algorithm based on adversarial learning, named Deep Cocktail Network (DCTN), to learn a feature extractor, domain discriminators and source classifiers. The domain discriminator provides multiple source-target-specific perplexity scores that are used to weight the source-specific classifier predictions and produce the target estimation. In Peng et al.\n\n[2019], the embedding is learned by aligning moments of the source and target distributions, by an approach called Moment matching (M 3 3 3 SDA) . Our approach differs greatly here as we do not try to cancel the variability across sources but to embrace it by allowing the approach to automatically find the source domains closest in terms of embedding and labeling function.\n\n\nNUMERICAL EXPERIMENTS\n\nIn this section, we first discuss the implementation and the robustness of MSDA-WJDOT. We then evaluate and compare it with state-of-the-art MSDA methods, on both simulated and real data. port [Flamary et al., 2021] toolboxes and will be released upon publication.\n\nPractical Implementation We used in all numerical experiments the MSDA-WJDOT solver from Algorithm 2. We recall that in this paper we assume to have access to a meaningful (as in discriminant) embedding g. This is a realistic scenario due to the wide availability of pre-trained models and advent of reproducible research. Nevertheless we discuss here how to estimate such an embedding when none is available. To keep the variability of the sources that is used by MSDA-WJDOT we propose to estimate g with the Multi-Task Learning framework originally proposed in Caruana [1997], i.e.\nmin g,{fj } J j=1 J j=1 1 Nj Nj i=1 L(f j \u2022 g(x i j ), y i j ). (8)\nThis approach for estimating an embedding g makes sense because it promotes a g that is discriminant for all tasks but allows a variability thanks to the task specific final classifiers f j which is an assumption at the core of MSDA-WJDOT. We refer to MSDA-WJDOT where the embedding g is learned with the above procedure as MSDA-WJDOT M T L . Note that this is a two step procedure. An important question, especially when performing unsupervised DA, is how to perform the validation of the parameters including early stopping. We propose here to use the sum of squared errors (SSE) between the target points in the embedding and their cluster centroids. Specifically, we estimate cluster membership on the the outputs through f \u2022 g. Then the SSE is computed in the embedding g using the estimated clusters. Intuitively, if the SSE decreases it means that f attributes the same label to samples of the target domain that are close in the embedding. We also explored another strategy, based on the classifier accuracy on the sources, that is discussed and reported in the supplementary material.\n\nIn addition, to provide a lower and an upper bound of the MSDA performance, we implemented supervised classification methods trained on the sources (Baseline), the target (Target), on both sources and target (Baseline+Target) domain. We consider Baseline as a performance lower bound as the target domain is not used during training, whereas Target and Baseline+Target are two unrealistic approaches that use labels in target. Note that Target trains a classifier using only target labels and is more prone to overfitting since less samples are available. Since we have access to labels for Target and Baseline+Target, we validate the model by using the classification accuracy on the target validation set making those two approaches clear upper bounds on the attainable performance for each dataset. All methods are compared on the same dataset split in training (70%), validation (20%) and testing (10%) but the validation set is used only for Baseline+Target and Target.\n\nAlgorithm convergence and stability In Figure 2 (Left and Center-left) we show the stability of the algorithm for different weights initialization. The loss function always converges and the \u03b1 \u03b1 \u03b1 coefficients are not affected by the initialization. Moreover, we observed in practice that choosing the same step for \u03b1 \u03b1 \u03b1 and \u03b8 \u03b8 \u03b8 does not degrade the performance and in all experiments we validated it via early stopping. We also noticed a fast convergence of the weights \u03b1 \u03b1 \u03b1, meaning that the relevant domains are quickly identified. This behavior is illustrated in Fig. 2 (Right), where \u03b1 \u03b1 \u03b1 sparsity rapidly increases for any choice of S illustrating that only few relevant source distributions are used in practice. We also report the loss convergence for increasing number of sources S (Center-right).\n\nSimulated Data: Domain Shift We consider a classification problem similar to what is illustrated in Figure 1, but with 3 classes, i.e. Y = {0, 1, 2}, and in 3D. For the sources and target we generate N j and N T samples from J + 1 Gaussian distributions rotated of angle \u03b8 j \u2208 [0, 3 2 \u03c0] around the x-axis. As the data is already linearly separated, we set g as the identity function in this experiment. We carried out many experiments in order to see the effect of different parameters such as the number of source domains J, of source samples N j and of target samples N T . Each experiment has been repeated 50 times. We report in Fig. 3 the accuracy of all methods with N j = N T = 300 for J = 3 (Left) and J = 30 (Right). All competing methods are clearly outperformed by MSDA-WJDOT both in term of performance and variance even for a limited number of sources. Interestingly MSDA-WJDOT can even outperform Target due to its access to a larger number of samples. Another important aspect of MSDA-WJDOT is the obtained weights \u03b1 that can be used for interpretation. We show in Fig. 3 the \u03b1 weights that are attributed to the sources (ordered on the x-axis by increasing rotation angles), for in an increasing rotation angle in the target samples (y-axis). The estimated weights tend to be sparse and put more mass on sources that have a similar angle i.e. we recover automatically the closest sources in the joint distribution manifold. Note that we only report the method's performances on those two configurations; the results for other experiments can be found in the supplementary material. We next investigate how the function \u039b in (6) behaves when the weights \u03b1 \u03b1 \u03b1 are optimized w.r.t. the first term of the bound in Theorem 1. To this end we computed for 30 sources an upper bound of \u039b with the 0-1 loss by using the estimatedf instead of the minimizer in (6). We recover a value of 0.57 that is very close to twice the Bayes error, corresponding to the best possible value for \u039b in this experiment. On the other hand, the value for the upper bound of \u039b for a uniform \u03b1 \u03b1 \u03b1 is 0.64 and 0.65 in average for 10000 randomly drawn values of \u03b1 \u03b1 \u03b1. This suggests that optimizing \u03b1 \u03b1 \u03b1 with MSDA-WJDOT leads to a minimization of \u039b in the generalization bound.\n\nSimulated Data: Target Shift We take into account the target shift problem with 2D source and target datasets which present different proportions of classes. The proportion of the class c in the source j is defined as P c j = #{y i j =c} Nj (and similarly for the target). We consider a binary classification task and we sample sources and target datasets from the same Gaussian distribution. In Fig. 4 (Left and Center) we illustrate two sources and target distributions and how MSDA-WJDOT reweights the sources. As we can see, almost all the mass is concentrated on Source 2 (\u03b1 2 \u03b1 1 ) because its class proportion is closer to the target one. Instead, Source 1 has a class proportion inverted w.r.t. the target. In the experiment reported in Fig. 4 (Center-Right and Right) we have J = 20 sources with P 2 j randomly generated between 0.1 and 0.9 (we ordered the sources s.t. P 2 j \u2264 P 2 j+1 ). We show the average classification accuracy and the \u03b1 \u03b1 \u03b1 weights over 50 trials for varying P 2 T in {0.1, 0.2, \u00b7 \u00b7 \u00b7 , 0.9}. Our method always outperforms JCPOT and selects the sources with a proportion of classes closer to the one in the target. , resulting into an embedding space G \u2208 R 4096 . For f , we employ a one-layer neural network. Training is performed with Adam optimizer with 0.9 momentum and = e \u22128 . Learning rate and 2 regularization on the parameters are validated for all methods. In JDOT extensions and MSDA-WJDOT, we also validate the \u03b2 parameter weighting the feature distance in the cost (3). The aim of this experiment is to evaluate MSDA-WJDOT and compare it with the current literature in the setting in which the embedding is given. The performance of the methods is reported in Table 1. We can see that MSDA-WJDOT is state of the art providing the best Average Rank (AR). Note that the DeCAF pre-trained embedding was originally designed in part to minimize the divergence across domains which as discussed is not the best configuration for MSDA-WJDOT but it still performs very well showing the robustness of MSDA-WJDOT to the embedding. Moreover, we observed that for each adaptation problem MSDA-WJDOT provides one-hot vector \u03b1 \u03b1 \u03b1 (reported in supplementary) suggesting that only one source is needed for the target adaptation. Interestingly the source selected by MSDA-WJDOT for each target is the one that was reported with the best performance for single-source DA in Courty et al. [2016], which shows that MSDA-WJDOT can automatically find the relevant sources with no supervision.\n\n\nMusic-speech Discrimination\n\nWe now tackle a MSDA problem in which both the embedding and the target classifier need to be learned. Specifically, we consider the musicspeech discrimination task introduced in Tzanetakis and Cook [2002], which includes 64 music and speech tracks of 30 seconds each. We generated 14 noisy datasets by combining the raw tracks with different types of noises from a noise   We report in Table 2, the mean and standard deviation accuracy on the testing set of each target dataset over 50 trials, as well as the Average Rank for each method. First note that on this hard adaptation problem the Baseline+Target approach only slightly improves the Baseline, and most of the methods performance shows  \n\n\nCONCLUSION\n\nWe presented a novel approach for multi-source DA that relies on OT for propagating labels from the sources and a weighting of the source domains that selects the best sources for the target task at hand in order to get a better prediction. We provided results that show that the proposed approach is theoretically grounded. We present numerical experiments on simulated data that shows the effectiveness of our method on both domain and target shift problems. Finally, we illustrate the good performance of MSDA-WJDOT on real-world benchmark datasets. Future works will investigate a regularization of \u03b1 and estimating simultaneously the embedding g with MSDA-WJDOT instead of pre-training it with multitask learning. The embedding could indeed be updated for each new target which suggests an incremental formulation for MSDA-WJDOT that could be valuable in practice. 2017. This research was produced within the framework of Energy4Climate Interdisciplinary Center (E4C) of IP Paris and Ecole des Ponts ParisTech. This research was supported by 3rd Programme d'Investissements d'Avenir ANR-18-EUR-0006-02. This action benefited from the support of the Chair \"Challenging Technology for Responsible Energy\" led by l'X -Ecole polytechnique and the Fondation de l'Ecole polytechnique, sponsored by TOTAL. We can now prove Theorem 1, which we also restate for the convenience of the reader.\n\n\nReferences\n\nTheorem 1. Under the assumptions of Lemma 2, letp S,j be j-th source empirical distributions of N j samples andp T the empirical target distribution with N T samples. Then for all \u03bb > 0 , with \u03b2 = \u03bbk in the ground metric D we have with probability 1 \u2212 \u03b7\n\u03b5 p T (f ) \u2264W D p \u03b1 S ,p f T + 2 c log 2 \u03b7 \uf8eb \uf8ed 1 N T + J j=1 \u03b1 j N j \uf8f6 \uf8f8 + \u039b(p \u03b1 S , p T ) + kM \u03c6(\u03bb).(19)\nProof. By the triangle inequality we have that\nW D \uf8eb \uf8ed J j=1 \u03b1 j p S,j , p f T \uf8f6 \uf8f8 \u2264 W D \uf8eb \uf8ed J j=1 \u03b1 jpS,j ,p f T \uf8f6 \uf8f8 + W D (p f T , p f T ) + W D \uf8eb \uf8ed J j=1 \u03b1 jpS,j , J j=1 \u03b1 j p S,j \uf8f6 \uf8f8 \u2264 W D \uf8eb \uf8ed J j=1 \u03b1 jpj ,p f T \uf8f6 \uf8f8 + W D (p f T , p f T ) + J j=1 \u03b1 j W D (p S,j , p S,j )\nwhere the last inequality follows from Lemma 3. Using the well known convergence property of the Wasserstein distance proven in Bolley et al. [2007] we find the following bound with probability 1 \u2212 \u03b7\n\u03b5p T (f ) \u2264 WD J j=1 \u03b1jpS,j,p f T + 2 c log 2 \u03b7 1 NT + J j=1 \u03b1j Nj + \u039b(p \u03b1 S , pT ) + 2kM \u03c6(\u03bb)(20)\nwith c corresponding to all source and target distributions under similar conditions as in Courty et al. [2017].\n\n\nB THE ALGORITHM\n\nWe recall here the algorithm we proposed to solve the MSDA-WJDOT problem (Algorithm 2). P \u2206 J is the projection to the simplex \u2206 J = {\u03b1 \u03b1 \u03b1 \u2208 R J | J j=1 \u03b1 j = 1, \u03b1 j \u2265 0} defined as P \u2206 J (w w w) = argmin \u03b1 \u03b1 \u03b1\u2208\u2206 J w w w \u2212 \u03b1 \u03b1 \u03b1 .\n\nWe implemented it by using Algorithm 3, firstly proposed in Held et al. [1974].\n\nAlgorithm 2 Optimization for MSDA-WJDOT Initialise \u03b1 \u03b1 \u03b1 = 1 J 1 J and \u03b8 \u03b8 \u03b8 parameters of f \u03b8 \u03b8 \u03b8 and steps \u00b5 \u03b1 \u03b1 \u03b1 and \u00b5 \u03b8 \u03b8 \u03b8 . repeat\n\u03b8 \u03b8 \u03b8 \u2190 \u03b8 \u03b8 \u03b8 \u2212 \u00b5 \u03b8 \u03b8 \u03b8 \u2207 \u03b8 \u03b8 \u03b8 W D p f T , J j=1 \u03b1 jpS,j \u03b1 \u03b1 \u03b1 \u2190 P \u2206 J \u03b1 \u03b1 \u03b1 \u2212 \u00b5 \u03b1 \u03b1 \u03b1 \u2207 \u03b1 \u03b1 \u03b1 W D (p f T , J j=1 \u03b1 jpS,j ) until Convergence\nAlgorithm 3 Projection to the simplex [Held et al., 1974] Sort w w w into u u u: u 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 u J .\nSet K := max 1\u2264k\u2264J {k|( k j=1 u j \u2212 1/k < u k }. Set \u03c4 := ( K j=1 u j \u2212 1)/K. For j = 1, . . . , J set \u03b1 j := max{w j \u2212 \u03c4, 0}.\nC NUMERICAL EXPERIMENTS C.1 SIMULATED DATA Domain shift We generate a data set (X 0 , Y 0 ) by drawing X 0 from a 3-dimensional Gaussian distribution with 3 cluster centers and standard deviation \u03c3 = 0.8. We keep the same number of examples for each cluster. To simulate the J sources, we apply J rotations to the input data X 0 around the x-axis. More precisely, we draw J equispaced angles \u03b8 j from [0, 3 2 \u03c0] and we get X j = {x i j } as\nx i j = x i 0 \u00b7 \uf8ee \uf8f0 1 0 0 0 cos(\u03b8 j ) \u2212sin(\u03b8 j ) 0 sin(\u03b8 j ) cos(\u03b8 j ) \uf8f9 \uf8fb .(22)\nTo generate the target domain X T , we follow the same procedure by randomly choosing an angle \u03b8 T \u2208 [0, 3 2 \u03c0]. We keep the label set fixed, i.e. Y j = Y T = Y 0 . Note that in this case the embedding function g is the identity function and, hence, X \u2261 G. In the following we report all the experiment we carried out on the simulated data, in which we also investigate to replace the exact Wasserstein distance by the the Bures-Wasserstein distance\nBW (\u00b5 S , \u00b5 T ) 2 = m S \u2212 m T 2 + Trace \u03a3 S + \u03a3 T \u2212 2 \u03a3 1/2 S \u03a3 T \u03a3 1/2 S 1/2 ,(23)\nwhere the m S , \u03a3 S are respectively the first and second order moments of distribution \u00b5 S (and similarly for m T , \u03a3 T ). The BW distance has the advantage of having a complexity linear in the number of samples that can scale better to large dataset. We label this method variant with (B), while we refer to the exact OT as (E).\n\nIn the following, we investigate the performance of MSDA-WJDOT at varying of the number of sources J, source samples N j , and target samples N T . We compare the proposed approch with other MSDA methods and with the Baseline, Target, Bayes classification.\n\n\u2022 Varying the number of sources: we keep the number of samples fixed in both sources and target datasets (s.t. N j = N T \u2200j) and we vary the number of sources J \u2208 {3, 5, 10, 20, 25, 30}. In Fig. 5 we report the accuracy of the different methods. \u2022 Varying the number of source samples: we fix the number of sources J equal to 20 and the number of target samples N T to 300. Fig 6 and 10 show the methods accuracy for varying the number of source samples N j in {60, 180, 300} Figure 11: Examples of source error and target error when the function f is the function learned by our approach (instead of the one minimizing \u039b in (6)). The blue curve represents an histogram of the \u03b1-weighted source error for 10000 random \u03b1. The x-axis represents the value of the error and the y-axis the count. The green line corresponds to the source error for the learned \u03b1, red one gives the error for an uniform alpha and the black one represents the target error (the height of the lines has been arbitrarily set for a sake of clarity). We can see that for both 5 (Left) and 30 sources (Right) the learned alpha leads to lower source error even though \u03b1 has been optimized for aligning joint distributions. and the recovered \u03b1 \u03b1 \u03b1 weight for N j = 300, respectively.\n\n\u2022 Varying the number of the target samples: we fix J = 20 and N j = 300, with 1 \u2264 j \u2264 J. We let vary the number of target samples N T in {60, 180, 300} (Fig. 7).\n\n\u2022 Varying the number of samples of all domains: we fix the number of sources equal to 20. We let vary the number of source and target samples in {60, 180, 300}, by keeping N j = N T with 1 \u2264 j \u2264 J. We report the methods' accuracy in Fig. 6.\n\nIn all experiments MSDA-WJDOT significantly outperfoms CJDOT, MJDOT, IWERM and the Baseline. Both MSDA-WJDOT(E) and MSDA-WJDOT(B) provide a better or at least comparable performance w.r.t. the Target method, in which the labels of the target dataset are used. In Fig. 9 and 10 we show the recovered weights \u03b1 \u03b1 \u03b1 for N j = N T = 60 and N j = N T = 300, respectively. In both cases, the xaxis reports different random target angles in the [0, 3 2 \u03c0] interval (ordered by increasing angles), whereas the y-axis represents the source angles ordered such that \u03b8 j \u2264 \u03b8 j+1 , 1 \u2264 j \u2264 J \u2212 1. As we can see, the weights are higher along the diagonal meaning that MSDA-WJDOT always rewards the sources with angle closest to \u03b8 T .\n\n\nC.2 REAL DATA\n\nIn the section, we introduce a new strategy for the validation, in alternative to the one based on SSE proposed in Sec. 3.2. We propose to employ the accuracy of the learned classifier f on the source datasets and weighted by \u03b1 \u03b1 \u03b1, i.e. J j=1 \u03b1 j ACC S,j (f ),\n\nwith ACC S,j (f ) = #{f (x i j )=y i j } Nj . To refer to this approach, we denote as MSDA-WJDOT acc , CJDOT acc , MJDOT acc the MSDA-WJDOT and the two JDOT extensions respectively. Let us remark that MSDA-WJDOT acc is a way to reuse the weights \u03b1 \u03b1 \u03b1 that provide the closest source distributions which, hence, are supposed to give a better estimate of the performance of the current classifier. Table 3 we report the source weights provided by MSDA-WJDOT. In all cases, \u03b1 \u03b1 \u03b1 is a one-hot vector suggesting that only one source is meaningfully related to the target domain. This is in line with the results on single-source DA found in Courty et al. [2016] in which the source domain providing the highest accuracy corresponds to the one selected by MSDA-WJDOT.  Table 3: \u03b1 \u03b1 \u03b1 weights Table 4 is a full version of Table 1 in the paper, in which we also show the accuracy obtained by employing the validation strategy introduced in Eq. 24. We can observe that MSDA-WJDOT acc provides good performances, comparable with both MSDA-WJDOT and the other MSDA methods, but MSDA-WJDOT still remains the state of the art.\n\n\nObject recognition In\n\nDefinition 2 .\n2(Similarity measure) Let H be a space of M -Lipschitz labelling functions. Assume that, for every f \u2208 H and x, x \u2208 G, |f (x) \u2212 f (x )| \u2264 M . Consider the following measure of similarity between p \u03b1 S and p T introduced in [Ben-David et al., 2010, Def. 5]    \n\nFigure 1 :\n12D simulated data. (Left) illustration of 4 source distributions corresponding to 4 increasing rotations. The color of the sample corresponds to the class. (Center Left) source distributions and target distribution in black because no class information is available. (Center Right) source distributions weighted by the optimal \u03b1 \u03b1 \u03b1 = [0, 0.5, 0.5, 0] from MSDA-WJDOT: only Source 2 and 3 have a weight > 0 because they are the closest to the target in the Wasserstein sense. (Right) Final MSDA-WJDOT target classification. equal) and using classical JDOT on the resulting distribution. The second one consists in optimizing a sum of JDOT losses for every source distribution but again, this leads to uniform impact of the sources on the estimation. It is clear that both approaches are not robust when some sources distributions are very different from the target (those would have a small weight in MSDA-WJDOT). Recently, Montesuma and Mboula [2021] proposed to compute a Wasserstein barycenter to aggregate the source marginal distributions. Once the intermediate domain is computed, they transport the Wasserstein barycenter into the target domain using the Sinkhorn algorithm [Cuturi, 2013] with (WTB reg ) or without (WTB) class regularization. The Wasserstein barycenter is also used in another MSDA approach, called JCPOT [Redko et al., 2019b], to estimate the class proportion. This method, based on Courty et al. [2016]\n\nFigure 2 :\n2The numerical implementation relies on the Pytorch [Paszke et al., 2017] and Python Optimal Trans-(Left and Center-Left) Loss function and \u03b1 coefficients with different weights initializations. (Center-Right and Right) Loss function and \u03b1 sparsity for increasing number of sources J.\n\nFigure 3 :\n3Simulated dataset. Methods' accuracy and recovered \u03b1 weights for an increasing rotation angle of the target samples: (Left and Center-Left) J = 3 and (Right and Center-Right) J = 30 sources.\n\nObject\nRecognition The Caltech-Office dataset [Gong et al., 2012] contains four different domains: Amazon, Caltech [Griffin et al., 2007], Webcam and DSLR. The variability of the different domains come from several factors: presence/absence of background, lightning conditions, noise, etc. We use for the embedding function g the output of the 7th layer of a pre-trained DeCAF model [Donahue et al., 2014], similarly to what was done in Courty et al. [2016]\n\nFigure 4 :\n4Illustration of MSDA-WJDOT on target shift problem. (Left) illustration of 2 source and target distributions with unbalanced classes. (Center-Left) source distributions weighted by \u03b1 \u03b1 \u03b1 and estimated target classifier. (Center-Right and Right) classification accuracy of MSDA-JDOT and JCPOT and \u03b1 \u03b1 \u03b1 coefficients at varying of class proportions in target dataset.\n\n\nlarge variance. As expected, MSDA-WJDOT M T L significantly outperforms MSDA-WJDOT confirming the importance of estimating an embedding g exploiting the source variability. MSDA-WJDOT M T L achieves a 1.25 Average Rank outperforming all the other MSDA methods and also presents low standard deviation, showing robustness to small sample size. Surprisingly, MSDA-WJDOT M T L even outper-\n\nFigure 5 :Figure 6 :Figure 7 :Figure 8 :Figure 9 :Figure 10 :\n5678910Methods' accuracy for varying the number of sources J. Methods' accuracy for varying the number of source samples. Methods' accuracy for varying the number of target samples Methods' accuracy for varying the number of source and target samples Recovered \u03b1 \u03b1 \u03b1 with small sample size (N j = N T = 60). Recovered \u03b1 \u03b1 \u03b1 for N j = N T = 300.\n\nTable 1 :\n1Object recognition accuracy. The last column reports the average rank across target domain. Results of methods marked by * are from Montesuma and Mboula[2021].Method \nAmazon \ndslr \nwebcam \nCaltech10 \nAR \nBaseline \n93.13 \u00b1 0.07 94.12 \u00b1 0.00 \n89.33 \u00b1 1.63 82.65 \u00b1 1.84 5.00 \nIWERM \n93.30 \u00b1 0.75 100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 89.33 \u00b1 1.16 91.19 \u00b1 2.57 \n91.19 \u00b1 2.57 \n91.19 \u00b1 2.57 2.75 \nCJDOT \n93.71 \u00b1 1.57 93.53 \u00b1 4.59 \n90.33 \u00b1 2.13 85.84 \u00b1 1.73 3.50 \nMJDOT \n94.12 \u00b1 1.57 97.65 \u00b1 2.88 \n90.27 \u00b1 2.48 84.72 \u00b1 1.73 3.00 \nJCPOT  *  \n79.23 \u00b1 3.09 81.77 \u00b1 2.81 \n93.93 \u00b1 0.60 77.91 \u00b1 0.45 5.50 \nWBT  *  \n59.86 \u00b1 2.48 60.99 \u00b1 2.15 \n64.13 \u00b1 2.38 62.80 \u00b1 1.61 7.25 \nWBT  *  \n\nreg \n\n92.74 \u00b1 0.45 95.87 \u00b1 1.43 \n96.57 \u00b1 1.76 \n96.57 \u00b1 1.76 \n96.57 \u00b1 1.76 85.01 \u00b1 0.84 4.00 \nMSDA-WJDOT \n94.23 \u00b1 0.90 \n94.23 \u00b1 0.90 \n94.23 \u00b1 0.90 100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 89.33 \u00b1 2.91 85.93 \u00b1 2.07 2.25 \nTarget \n95.77 \u00b1 0.31 88.35 \u00b1 2.76 \n99.87 \u00b1 0.65 89.75 \u00b1 0.85 \n-\nBaseline+Target 94.78 \u00b1 0.48 99.88 \u00b1 0.82 100.00 \u00b1 0.00 91.89 \u00b1 0.69 \n-\n\ndataset (spib.linse.ufsc.br/noise.html). The noisy \ndatasets have been synthesised by PyDub python library \n[Robert et al., 2018]. We then used the libROSA python li-\nbrary [Brian McFee et al., 2015] to extract 13 MFCCs, com-\nputed every 10ms from 25ms Hamming windows followed \nby a z-normalization per track. We chose each of the four \nnoisy datasets F16, Bucaneer2 (B2), Factory2 (F2), and De-\nstroyerengine (D) as target domains, considering the remain-\ning noisy datasets and the clean dataset as labelled source \ndomains. The feature extraction g is a Bidirectional Long \nShort-Term Memory (BLSTM) recurrent network with 2 \nhidden layers of 50 memory blocks each. The f classifier \nis learned as one feed-forward layer. Model and training \ndetails are reported in the supplementary materials. \n\n\n\nTable 2 :\n2Music-Speech discrimination accuracy and average \nrank across target domains. Results of methods marked by \n *  are from Montesuma and Mboula [2021]. \n\nMethod \nF16 \nB2 \nF2 \nD \nAR \nBaseline \n69.67 \u00b1 8.78 \n57.33 \u00b1 7.57 \n83.33 \u00b1 9.13 \n87.33 \u00b1 6.72 \n9.25 \nIWERM \n72.22 \u00b1 3.93 \n58.33 \u00b1 5.89 \n85.00 \u00b1 6.23 \n81.64 \u00b1 3.33 \n8.75 \nIWERMMT L \n75.00 \u00b1 0.00 \n66.67 \u00b1 0.00 100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 98.33 \u00b1 3.33 \n4.00 \nDCTN \n66.67 \u00b1 3.61 \n68.75 \u00b1 3.61 \n87.50 \u00b1 12.5 \n94.44 \u00b1 7.86 \n6.50 \nM 3 3 3 SDA \n70.00 \u00b1 4.08 \n61.67 \u00b1 4.08 85.00 \u00b1 11.05 83.33 \u00b1 0.00 \n8.50 \nCJDOT \n59.50 \u00b1 13.95 50.00 \u00b1 0.00 \n83.33 \u00b1 0.00 \n91.67 \u00b1 0.00 \n9.75 \nCJDOTMT L \n83.83 \u00b1 5.11 \n74.83 \u00b1 1.17 100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 \n100.00 \u00b1 0.00 95.74 \u00b1 16.92 3.25 \nMJDOT \n66.33 \u00b1 9.57 \n50.00 \u00b1 0.00 \n83.33 \u00b1 0.00 \n91.67 \u00b1 0.00 \n9.50 \nMJDOTMT L \n86.00 \u00b1 4.55 \n72.83 \u00b1 5.73 \n97.67 \u00b1 3.74 \n97.74 \u00b1 8.28 \n3.50 \nJCPOT  *  \n88.67 \u00b1 1.67 \n92.55 \u00b1 2.11 \n82.41 \u00b1 2.22 \n87.89 \u00b1 1.39 \n5.50 \nWBT  *  \n56.63 \u00b1 6.56 \n56.88 \u00b1 9.54 \n59.38 \u00b1 2.61 \n56.63 \u00b1 6.88 11.75 \nWBT  *  \n\nreg \n\n94.92 \u00b1 0.68 \n94.92 \u00b1 0.68 \n94.92 \u00b1 0.68 \n96.27 \u00b1 1.60 \n96.27 \u00b1 1.60 \n96.27 \u00b1 1.60 \n96.87 \u00b1 0.94 \n92.98 \u00b1 1.38 \n3.00 \nMSDA-WJDOT \n83.33 \u00b1 0.00 \n58.33 \u00b1 6.01 \n87.00 \u00b1 6.05 \n89.00 \u00b1 4.84 \n7.00 \nMSDA-WJDOTMT L \n87.17 \u00b1 4.15 \n74.83 \u00b1 1.20 \n99.67 \u00b1 1.63 \n99.67 \u00b1 1.63 \n99.67 \u00b1 1.63 \n99.67 \u00b1 1.63 \n2.25 \nTarget \n73.67 \u00b1 6.09 \n69.17 \u00b1 7.50 \n77.33 \u00b1 4.73 \n73.17 \u00b1 9.90 \n-\nBaseline+Target 71.06 \u00b1 9.31 67.62 \u00b1 11.92 85.33 \u00b1 11.85 79.53 \u00b1 10.05 \n-\n\nforms both the Target and Baseline+Target meth-\nods, where the labels are available. \n\n\nAcknowledgementsThis work was partially funded through the 3IA Cote d'Azur Investments ANR-19-P3IA-0002 of the French National Research Agency (ANR), the DECIPHER-ASL -Bando PRIN 2017 grant (2017SNW5MB -Ministry of University and Research, Italy), and a grant from SAP SE and 5x1000, assigned to the University of Ferrara -tax returnSupplementary MaterialThe supplementary material is organized as follows. In Section A we provide proof of Lemma 1, Lemma 2 and Theorem 1. For reader's convenience the results are repeated in this supplementary material. Section B recalls the MSDA-WJDOT algorithm and defines the projection to the simplex implemented in the algorithm. Finally, in Section C we present additional numerical experiments.A PROOFSA.1 PROOF OF LEMMA 1 Lemma 1. For any hypothesis f \u2208 H, denote as \u03b5 p T (f ) and \u03b5 p \u03b1 S (f ), the expected loss of f on the target and on the weighted sum of the source domains, with respect to a loss function L bounded by B. We havewhere p \u03b1 S = J j=1 \u03b1 j p S,j is a convex combination of the source distributions with weights \u03b1 \u2208 \u2206 J , and D T V is the total variation distance.Proof. We define the error of an hypothesis f with respect to a loss function L(\u00b7, \u00b7) and a joint probability distribution p(x, y) as \u03b5 p (f ) = p(x, y)L(y, f (x))dxdy then using simple arguments, we haveand using the definition of the total variation distance between distribution we conclude the proof.A.2 PROOF OF THEOREM 1The proof of this theorem follows the same steps as the one proposed by Courty et al.[2017] and we reproduce it here for a sake of completeness.Definition 1 (Probabilistic Transfer Lipschitzness -PLT Property). Let p S and p T be respectively the source and target distributions. Let \u03c6 : R \u2192 [0, 1]. A labeling function f : G \u2192 R and a joint distribution \u03c0 \u2208 \u03a0(p S , p T ) over p S and p T are \u03c6-Lipschitz transferable if for all \u03bb > 0, we havewith D being a metric on G.This property provides a bound on the probability of finding a couple of source-target examples that are differently labeled in a (1/\u03bb)-ball with respect to \u03c0 and the metric D.where the risk is measured w.r.t. to a symmetric and k-Lipschitz loss function that satisfies the triangle inequality.Lemma 2. Let H be the space described in Definition 2 and assume that the function f * minimizing the Similarity measure in Eq. 11 satisfies the PTL property. Then, for any f \u2208 H, we havewhere \u03c6(\u03bb) is a constant depending on the PTL of f .Proof. We have thatwhere the second equality comes from the symmetry of the loss function and the third one is due to the fact that) since the label y is not used in the expectation. Now, we analyze the first term in the r.h.s. of the last inequality. Note that samples drawn from p f T distribution can be expressed as (Inequality in line(13)is due to the Kantorovitch-Rubinstein theorem stating that for any coupling \u03c0 \u2208 \u03a0(p \u03b1 S , p T ) the following inequality holds   Music-speech discrimination The model we adopted is shown inFig. 12, where g is a two-layers Bidirectional Long Short-Term Memory (BLSTM) that feeds the one feed-forward layer f with the last hidden state. Weights were initialized with Xavier initialization. Training is performed with Adam optimizer with 0.9 momentum and = e \u22128 . Learning rate exponentially decays every epoch. We grid-research the initial learning rate value and the decay rate.InTable 5we show the MSDA performances in the music-speech discrimination. In particular, for MSDA-WJDOT and JDOT variants the validation strategy described in formula 24 has been employed. Results show that, although this is a valid strategy, early stopping based on SSE described in Sec. 4 always outperforms. The Average Rank shows that MSDA-WJDOT is state of the art in music-speech discrimination.\nDeep cocktail network: Multi-source unsupervised domain adaptation with category shift. Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, Liang Lin, Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, and Liang Lin. Deep cocktail network: Multi-source unsu- pervised domain adaptation with category shift. 2018\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018.\n\nAdversarial multiple source domain adaptation. Han Zhao, Shanghang Zhang, Guanhang Wu, Advances in neural information processing systems. Han Zhao, Shanghang Zhang, Guanhang Wu, et al. Adver- sarial multiple source domain adaptation. In Advances in neural information processing systems, pages 8559-8570, 2018.\n\nOn learning invariant representations for domain adaptation. Han Zhao, Remi Tachet Des, Kun Combes, Geoffrey Zhang, Gordon, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Ge- offrey Gordon. On learning invariant representations for domain adaptation. In Proceedings of the 36th Interna- tional Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 7523- 7532, 2019.\n", "annotations": {"author": "[{\"end\":195,\"start\":85},{\"end\":268,\"start\":196},{\"end\":359,\"start\":269},{\"end\":442,\"start\":360}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":93},{\"end\":208,\"start\":201},{\"end\":288,\"start\":275},{\"end\":379,\"start\":373}]", "author_first_name": "[{\"end\":92,\"start\":85},{\"end\":200,\"start\":196},{\"end\":274,\"start\":269},{\"end\":372,\"start\":360}]", "author_affiliation": "[{\"end\":142,\"start\":102},{\"end\":194,\"start\":144},{\"end\":267,\"start\":210},{\"end\":358,\"start\":290},{\"end\":441,\"start\":381}]", "title": "[{\"end\":82,\"start\":1},{\"end\":524,\"start\":443}]", "venue": null, "abstract": "[{\"end\":1554,\"start\":526}]", "bib_ref": "[{\"end\":2121,\"start\":2099},{\"end\":2132,\"start\":2121},{\"end\":2489,\"start\":2470},{\"end\":2511,\"start\":2489},{\"end\":2531,\"start\":2511},{\"end\":2537,\"start\":2531},{\"end\":2626,\"start\":2608},{\"end\":2646,\"start\":2626},{\"end\":2731,\"start\":2711},{\"end\":2737,\"start\":2731},{\"end\":2761,\"start\":2737},{\"end\":2812,\"start\":2793},{\"end\":3138,\"start\":3117},{\"end\":3161,\"start\":3155},{\"end\":3529,\"start\":3511},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3548,\"start\":3529},{\"end\":3566,\"start\":3548},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4069,\"start\":4051},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4086,\"start\":4069},{\"end\":4104,\"start\":4086},{\"end\":4143,\"start\":4124},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4459,\"start\":4441},{\"end\":4598,\"start\":4575},{\"end\":4642,\"start\":4615},{\"end\":7455,\"start\":7437},{\"end\":8492,\"start\":8461},{\"end\":8514,\"start\":8492},{\"end\":8542,\"start\":8522},{\"end\":8548,\"start\":8542},{\"end\":8567,\"start\":8548},{\"end\":8667,\"start\":8647},{\"end\":9431,\"start\":9425},{\"end\":10049,\"start\":10025},{\"end\":10296,\"start\":10277},{\"end\":10318,\"start\":10296},{\"end\":10337,\"start\":10318},{\"end\":11186,\"start\":11163},{\"end\":11617,\"start\":11611},{\"end\":12557,\"start\":12540},{\"end\":13193,\"start\":13187},{\"end\":13591,\"start\":13585},{\"end\":15001,\"start\":14978},{\"end\":17345,\"start\":17339},{\"end\":17857,\"start\":17834},{\"end\":18602,\"start\":18579},{\"end\":18676,\"start\":18670},{\"end\":18790,\"start\":18764},{\"end\":20053,\"start\":20030},{\"end\":20956,\"start\":20937},{\"end\":20978,\"start\":20956},{\"end\":20997,\"start\":20978},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21347,\"start\":21331},{\"end\":22330,\"start\":22308},{\"end\":22958,\"start\":22952},{\"end\":30606,\"start\":30600},{\"end\":30937,\"start\":30931},{\"end\":33631,\"start\":33625},{\"end\":33893,\"start\":33887},{\"end\":34225,\"start\":34219},{\"end\":34566,\"start\":34547},{\"end\":39701,\"start\":39695}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40458,\"start\":40183},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41901,\"start\":40459},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42198,\"start\":41902},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42402,\"start\":42199},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42861,\"start\":42403},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43240,\"start\":42862},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43629,\"start\":43241},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44037,\"start\":43630},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45887,\"start\":44038},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47464,\"start\":45888}]", "paragraph": "[{\"end\":2270,\"start\":1570},{\"end\":2813,\"start\":2272},{\"end\":3490,\"start\":2815},{\"end\":4679,\"start\":3492},{\"end\":6102,\"start\":4681},{\"end\":6569,\"start\":6104},{\"end\":6780,\"start\":6658},{\"end\":7000,\"start\":6856},{\"end\":7298,\"start\":7029},{\"end\":7460,\"start\":7300},{\"end\":7811,\"start\":7709},{\"end\":8568,\"start\":7874},{\"end\":9006,\"start\":8570},{\"end\":9214,\"start\":9060},{\"end\":9450,\"start\":9216},{\"end\":9655,\"start\":9478},{\"end\":10528,\"start\":9742},{\"end\":10753,\"start\":10585},{\"end\":11054,\"start\":10781},{\"end\":11776,\"start\":11079},{\"end\":12007,\"start\":11778},{\"end\":12109,\"start\":12065},{\"end\":12940,\"start\":12111},{\"end\":13227,\"start\":12942},{\"end\":13432,\"start\":13229},{\"end\":13710,\"start\":13497},{\"end\":14106,\"start\":13712},{\"end\":14282,\"start\":14164},{\"end\":14657,\"start\":14284},{\"end\":15614,\"start\":14760},{\"end\":15913,\"start\":15637},{\"end\":16756,\"start\":15964},{\"end\":17208,\"start\":16966},{\"end\":18126,\"start\":17210},{\"end\":18677,\"start\":18214},{\"end\":20801,\"start\":18694},{\"end\":21712,\"start\":20803},{\"end\":22089,\"start\":21714},{\"end\":22379,\"start\":22115},{\"end\":22964,\"start\":22381},{\"end\":24126,\"start\":23033},{\"end\":25102,\"start\":24128},{\"end\":25915,\"start\":25104},{\"end\":28182,\"start\":25917},{\"end\":30700,\"start\":28184},{\"end\":31429,\"start\":30732},{\"end\":32832,\"start\":31444},{\"end\":33100,\"start\":32847},{\"end\":33253,\"start\":33207},{\"end\":33682,\"start\":33483},{\"end\":33894,\"start\":33782},{\"end\":34145,\"start\":33914},{\"end\":34226,\"start\":34147},{\"end\":34365,\"start\":34228},{\"end\":34609,\"start\":34509},{\"end\":35177,\"start\":34737},{\"end\":35708,\"start\":35259},{\"end\":36123,\"start\":35793},{\"end\":36381,\"start\":36125},{\"end\":37635,\"start\":36383},{\"end\":37798,\"start\":37637},{\"end\":38040,\"start\":37800},{\"end\":38762,\"start\":38042},{\"end\":39041,\"start\":38780},{\"end\":40158,\"start\":39043}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6657,\"start\":6570},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6855,\"start\":6781},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7708,\"start\":7461},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7873,\"start\":7812},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9059,\"start\":9007},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9477,\"start\":9451},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9741,\"start\":9656},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10780,\"start\":10754},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12064,\"start\":12008},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13496,\"start\":13433},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14163,\"start\":14107},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14759,\"start\":14658},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15963,\"start\":15914},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16965,\"start\":16757},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18213,\"start\":18127},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23032,\"start\":22965},{\"attributes\":{\"id\":\"formula_16\"},\"end\":33206,\"start\":33101},{\"attributes\":{\"id\":\"formula_17\"},\"end\":33482,\"start\":33254},{\"attributes\":{\"id\":\"formula_18\"},\"end\":33781,\"start\":33683},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34508,\"start\":34366},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34736,\"start\":34610},{\"attributes\":{\"id\":\"formula_22\"},\"end\":35258,\"start\":35178},{\"attributes\":{\"id\":\"formula_23\"},\"end\":35792,\"start\":35709}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29896,\"start\":29889},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31126,\"start\":31119},{\"end\":39447,\"start\":39440},{\"end\":39815,\"start\":39808},{\"end\":39838,\"start\":39831},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39867,\"start\":39860}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1568,\"start\":1556},{\"attributes\":{\"n\":\"2\"},\"end\":7027,\"start\":7003},{\"attributes\":{\"n\":\"3\"},\"end\":10583,\"start\":10531},{\"attributes\":{\"n\":\"3.1\"},\"end\":11077,\"start\":11057},{\"attributes\":{\"n\":\"3.2\"},\"end\":15635,\"start\":15617},{\"attributes\":{\"n\":\"3.3\"},\"end\":18692,\"start\":18680},{\"attributes\":{\"n\":\"4\"},\"end\":22113,\"start\":22092},{\"end\":30730,\"start\":30703},{\"attributes\":{\"n\":\"5\"},\"end\":31442,\"start\":31432},{\"end\":32845,\"start\":32835},{\"end\":33912,\"start\":33897},{\"end\":38778,\"start\":38765},{\"end\":40182,\"start\":40161},{\"end\":40198,\"start\":40184},{\"end\":40470,\"start\":40460},{\"end\":41913,\"start\":41903},{\"end\":42210,\"start\":42200},{\"end\":42410,\"start\":42404},{\"end\":42873,\"start\":42863},{\"end\":43692,\"start\":43631},{\"end\":44048,\"start\":44039},{\"end\":45898,\"start\":45889}]", "table": "[{\"end\":45887,\"start\":44209},{\"end\":47464,\"start\":45900}]", "figure_caption": "[{\"end\":40458,\"start\":40200},{\"end\":41901,\"start\":40472},{\"end\":42198,\"start\":41915},{\"end\":42402,\"start\":42212},{\"end\":42861,\"start\":42411},{\"end\":43240,\"start\":42875},{\"end\":43629,\"start\":43243},{\"end\":44037,\"start\":43700},{\"end\":44209,\"start\":44050}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4332,\"start\":4324},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15176,\"start\":15167},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16365,\"start\":16357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25151,\"start\":25143},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25689,\"start\":25675},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26025,\"start\":26017},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26557,\"start\":26551},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27004,\"start\":26998},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28586,\"start\":28580},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28935,\"start\":28929},{\"end\":36579,\"start\":36573},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36769,\"start\":36757},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36868,\"start\":36859},{\"end\":37797,\"start\":37789},{\"end\":38039,\"start\":38033},{\"end\":38311,\"start\":38305}]", "bib_author_first_name": "[{\"end\":51336,\"start\":51330},{\"end\":51348,\"start\":51341},{\"end\":51363,\"start\":51355},{\"end\":51375,\"start\":51369},{\"end\":51386,\"start\":51381},{\"end\":51739,\"start\":51736},{\"end\":51755,\"start\":51746},{\"end\":51771,\"start\":51763},{\"end\":52065,\"start\":52062},{\"end\":52076,\"start\":52072},{\"end\":52092,\"start\":52089},{\"end\":52109,\"start\":52101}]", "bib_author_last_name": "[{\"end\":51339,\"start\":51337},{\"end\":51353,\"start\":51349},{\"end\":51367,\"start\":51364},{\"end\":51379,\"start\":51376},{\"end\":51390,\"start\":51387},{\"end\":51744,\"start\":51740},{\"end\":51761,\"start\":51756},{\"end\":51774,\"start\":51772},{\"end\":52070,\"start\":52066},{\"end\":52087,\"start\":52077},{\"end\":52099,\"start\":52093},{\"end\":52115,\"start\":52110},{\"end\":52123,\"start\":52117}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":51552,\"start\":51242},{\"attributes\":{\"id\":\"b1\"},\"end\":51687,\"start\":51554},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":55701085},\"end\":51999,\"start\":51689},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":174800477},\"end\":52532,\"start\":52001}]", "bib_title": "[{\"end\":51734,\"start\":51689},{\"end\":52060,\"start\":52001}]", "bib_author": "[{\"end\":51341,\"start\":51330},{\"end\":51355,\"start\":51341},{\"end\":51369,\"start\":51355},{\"end\":51381,\"start\":51369},{\"end\":51392,\"start\":51381},{\"end\":51746,\"start\":51736},{\"end\":51763,\"start\":51746},{\"end\":51776,\"start\":51763},{\"end\":52072,\"start\":52062},{\"end\":52089,\"start\":52072},{\"end\":52101,\"start\":52089},{\"end\":52117,\"start\":52101},{\"end\":52125,\"start\":52117}]", "bib_venue": "[{\"end\":51328,\"start\":51242},{\"end\":51616,\"start\":51554},{\"end\":51825,\"start\":51776},{\"end\":52193,\"start\":52125},{\"end\":52248,\"start\":52195}]"}}}, "year": 2023, "month": 12, "day": 17}