{"id": 211677878, "updated": "2023-10-06 18:42:54.687", "metadata": {"title": "Energy-Efficient Federated Edge Learning with Joint Communication and Computation Design", "authors": "[{\"first\":\"Xiaopeng\",\"last\":\"Mo\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 29}, "abstract": "This paper studies a federated edge learning system, in which an edge server coordinates a set of edge devices to train a shared machine learning model based on their locally distributed data samples. During the distributed training, we exploit the joint communication and computation design for improving the system energy efficiency, in which both the communication resource allocation for global ML parameters aggregation and the computation resource allocation for locally updating MLparameters are jointly optimized. In particular, we consider two transmission protocols for edge devices to upload ML parameters to edge server, based on the non orthogonal multiple access and time division multiple access, respectively. Under both protocols, we minimize the total energy consumption at all edge devices over a particular finite training duration subject to a given training accuracy, by jointly optimizing the transmission power and rates at edge devices for uploading MLparameters and their central processing unit frequencies for local update. We propose efficient algorithms to optimally solve the formulated energy minimization problems by using the techniques from convex optimization. Numerical results show that as compared to other benchmark schemes, our proposed joint communication and computation design significantly improves the energy efficiency of the federated edge learning system, by properly balancing the energy tradeoff between communication and computation.", "fields_of_study": "[\"Computer Science\",\"Engineering\",\"Mathematics\"]", "external_ids": {"arxiv": "2003.00199", "mag": "3007533255", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jcin/MoX21", "doi": "10.23919/jcin.2021.9475121"}}, "content": {"source": {"pdf_hash": "64256f8a05e08e4939706d1e591596406af45c03", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.00199v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7794d329b61f1d6380602e53bcbd667ddfe17c4a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/64256f8a05e08e4939706d1e591596406af45c03.txt", "contents": "\nEnergy-Efficient Federated Edge Learning with Joint Communication and Computation Design\n29 Feb 2020\n\nXiaopeng Mo xiaopengmo@mail2.gdut.edu.cn. \nJie Xu xujie@cuhk.edu.cn.j.xuisthecorrespondingauthor.2 \nJ Xu \n\nof Information Engineering\nFuture Network of Intelligence Institute (FNii)\nGuangdong University of Technology\n510006GuangzhouChina\n\n\nFuture Network of Intelligence Institute (FNii)\nSchool of Science and Engineering\nThe Chinese University of Hong Kong\n518172Shenzhen, ShenzhenChina\n\n\nThe Chinese University of Hong Kong\n518172Shenzhen, ShenzhenChina\n\nEnergy-Efficient Federated Edge Learning with Joint Communication and Computation Design\n29 Feb 20201\nThis paper studies a federated edge learning system, in which an edge server coordinates a set of edge devices to train a shared machine learning (ML) model based on their locally distributed data samples. During the distributed training, we exploit the joint communication and computation design for improving the system energy efficiency, in which both the communication resource allocation for global ML-parameters aggregation and the computation resource allocation for locally updating MLparameters are jointly optimized. In particular, we consider two transmission protocols for edge devices to upload ML-parameters to edge server, based on the non-orthogonal multiple access (NOMA) and time division multiple access (TDMA), respectively. Under both protocols, we minimize the total energy consumption at all edge devices over a particular finite training duration subject to a given training accuracy, by jointly optimizing the transmission power and rates at edge devices for uploading MLparameters and their central processing unit (CPU) frequencies for local update. We propose efficient algorithms to optimally solve the formulated energy minimization problems by using the techniques from convex optimization. Numerical results show that as compared to other benchmark schemes, our proposed joint communication and computation design significantly improves the energy efficiency of the federated edge learning system, by properly balancing the energy tradeoff between communication and computation.Index TermsFederated edge learning, energy efficiency, joint communication and computation design, resource allocation, non-orthogonal multiple access (NOMA), optimization.X. Mo is with the School ). J. Xu is the corresponding author.I. INTRODUCTIONArtificial intelligence (AI) and machine learning (ML) have found abundant applications in, e.g., computer vision, recommendation systems, and natural language processing. The recent success of AI and ML depends on various factors such as the development of new algorithms (e.g., deep learning [1]), the availability of massive data and the exponential increase of computation power. Normally, training proper AI/ML models requires huge computation power and massive training data. Therefore, the training of AI/ML models is conventionally implemented at the cloud or data center that has strong computation and storage capability[2].With recent technical advancements in Internet of Things (IoT) and fifth-generation (5G) celluar networks, massive data are generated by end devices (such as smart sensors, IoT devices, and smart phones) and cellular base stations (BSs) at the wireless network edge. In this case, the conventional centralized cloud learning may not be applicable any longer, as the collection of big data from massive edge devices to the central cloud is costly and may cause extremely high traffic loads in communication networks. Motivated by the recent development of mobile edge computing (see, e.g., [3]), a new paradigm of distributed mobile edge learning has been proposed (see, e.g., [4]-[7]), in which massive edge devices are enabled to cooperate in training shared ML models by exploiting their locally distributed data and computation power. By pushing ML tasks from far-apart cloud to nearby edge, the mobile edge learning technique can significantly decrease the end-to-end latency and considerably reduce the traffic loads in communication networks. It is envisioned that the mobile edge learning will be a key technology in the beyond-fifth-generation (B5G) or sixth-generation (6G) cellular networks for enabling new applications such as autonomous driving, virtual reality (VR) and augmented reality (AR), thus realizing the network intelligence vision [8].Among different mobile edge learning approaches, federated edge learning is particularly appealing (see, e.g., [9]-[21]), which allows a central node (such as an edge server) to coordinate a large number of edge devices to cooperate in training shared ML models based on their locally distributed data samples.Generally speaking, the objective of federated edge learning is to find optimized ML-parameters by minimizing the loss function via distributed optimization. In particular, the distributed gradient descent method has been widely adopted to solve the loss-function minimization problem, which is implemented in an iterative manner as follows. At each (outer) iteration, the edge server first broadcasts the global ML-parameters to edge devices, such that all participating edge devices can synchronize their local ML-parameters; next, each edge device individually updates its local ML-parameters by computing the gradients based on their own data samples, where the local update is normally implemented over several 3 (inner) iterations to speed up the convergence [16]; then, after local update, the edge devices upload their local ML-parameters to the edge server, such that the edge server can aggregate them to obtain an updated global ML-parameters. As the above procedures proceed, the edge server can obtain converged global ML-parameters that correspond to the desirable ML-model. For convenience, we refer to the above outer and inner iterations as global and local iterations, respectively. As no explicit data sharing from edge devices is required, the federated edge learning efficiently preserves the data privacy and security for edge devices. Notice that the ML-parameters are frequently exchanged between the edge server and edge devices, and as a result, the performance of federated edge learning is fundamentally constrained by the communication between the edge server and edge devices, especially when they are connected by wireless links that are unstable and may fluctuate significantly over time. This thus calls for a new design principle for federated edge learning in an interdisciplinary manner from both computer science and wireless communications perspectives.In particular, the implementation of federated edge learning over wireless networks faces the following technical challenges. First, due to the involvement of both communication and computation, how to jointly design them for optimizing the ML-training performance in terms of the training speed and accuracy is a new problem to be tackled. This problem is particularly difficult. This is due to the fact that the ML-training performance depends on many different factors (such as the employed ML algorithms and the data distribution among edge devices), and there does not exist a generic analytic relationship between the ML-training performance metric and the communication/computation parameters. Next, as the edge devices are normally powered by battery with finite sizes, their limited energy supply is another challenge to be dealt with, especially when the trained ML-model contains a large number of parameters, leading to heavy computation and communication loads. In addition, the federated edge learning faces the so-called straggler's dilemma issue, i.e., the ML-training performance is limited by the slowest edge devices in communication and computation [6]. In the literature, there have been some prior works [14]-[21] investigating the communication-constrained federated edge learning systems from different perspectives. For example, in [14] [15], the authors proposed gradient compression methods to accelerate the training speed by reducing the required communication cost for exchanging gradients. [16] optimized the numbers of global and local iterations to maximize the ML-training accuracy, subject to the communication resource constraints. [17] considered the heterogeneity of wireless channels at different edge devices, based on which a client selection algorithm was proposed to improve the efficiency of ML-model training, in which only the edge devices with good communication and computation qualities are chosen for avoiding the straggler's dilemma. Furthermore, the so-called over-the-air computation approach [22] [23] is utilized in the federated edge\n\nlearning systems [18]- [20], in which the superposition property of wireless multiple-access channels is exploited for speeding up the global ML-model aggregation. In addition, [21] further considered a hierarchical federated learning structure integrating devices, edge and cloud, in which multiple edge servers are allowed to perform partial ML-model aggregation to speed up the training process by reducing the direct communication rounds from the edge devices to the cloud. Despite such research progresses, however, the energy-efficient federated edge learning design by taking into account both communication and computation still remains a topic that is not well addressed. This thus motivates our investigation in this work. This paper considers a federated edge learning system consisting of one edge server and multiple edge devices. With the coordination of the edge server, the edge devices use the distributed batch gradient descent (BGD) method to train a shared ML-model. Suppose that the ML-model training is subject to a given training delay requirement. Our objective is to minimize the energy consumption at edge devices during the training, by jointly designing their communication resource allocation (i.e., the transmission power and corresponding rates) for global ML-parameters uploading and aggregation, and computation resource allocation (i.e., the central frequency unit (CPU) frequencies) for local ML-parameters updates.\n\nIn particular, we present both communication and computation energy consumption models, which are functions with respect to the numbers of local and global iterations, and the communication and computation loads at each iteration. We also consider two transmission protocols for the edge devices to upload their local ML-parameters to edge server, namely non-orthogonal multiple access (NOMA) and time division multiple access (TDMA), respectively. Under the two transmission protocols, the formulated energy minimization problems are both non-convex and difficult to solve in general. To tackle this issue, we transform them into convex forms and then present efficient algorithms to solve them optimally.\n\nNumerical results reveal interesting tradeoffs among energy consumption, training speed and training accuracy. It is shown that our proposed joint communication and computation design achieves significant performance gains over other benchmark schemes without such joint optimization. It is also shown that properly choosing the numbers of global and local iterations can efficiently balance the communicationcomputation tradeoff for further enhancing the system energy efficiency.\n\nNote that to our best knowledge, there have been two prior works [24] [25] considering the energyefficient communication design for federated edge learning systems, which are different from this paper in the following aspects. First, [24] only focused on the communication resource allocation, while this paper considers both communication and computation resource allocations. Next, different from [25] that aimed to minimize the weighted sum of training delay and energy consumption, this paper aims to minimize the energy consumption subject to training delay constraints. While [25] used a (loose) upper bound of the loss function to represent the training accuracy and to determine the numbers of local and global iterations, this paper uses simulations to accurately reveal the effects of the numbers of local and global iterations on training accuracy, and further shows the communication-computation energy tradeoff in choosing these parameters. Also note that the joint communication and computation resource allocations have been extensively investigated in the MEC literature (see, e.g., [26]- [31]). Nevertheless, the MEC studies normally focused on the task execution delay as the performance metric under general computation task models; while in the federated edge learning in this paper, we are interested in specific computation tasks for training ML-models, for which the training speed and accuracy are used as key performance measures.\n\nThe remainder of this paper is organized as follows. Section II presents the system model of our considered federated edge learning system. Section III formulates the joint communication and computation resource allocation problems for minimizing the total energy consumption at edge devices. Sections IV and V propose optimal solutions to the energy minimization problems under the NOMA and TDMA transmission protocols, respectively. Section VI provides numerical results to validate the effectiveness of our proposed joint designs. Finally, Section VII concludes this paper.\n\n\nII. SYSTEM MODEL\n\nIn this work, we consider a federated edge learning system consisting of an edge server and a set K {1, ..., K} of edge devices, as shown in Fig. 1. The edge server and edge devices are each deployed with one single antenna. In this system, the edge server coordinates K edge devices to train a shared ML-model (such as linear regression, support vector machine (SVM), and deep neural networks (DNN)) by using their local data. Let w denote the ML-parameters in the ML model to be trained, D k denote the set of data samples distributed at edge device k. Accordingly, we use f i (w) to denote the loss function for each data sample i \u2208 D k . For instance, for linear regression, by letting x i and y i denote the input vector and the desired output scalar for each data sample i, then the loss function can be expressed as\nf i (w) = 1 2 |y i \u2212 w T x i | 2 ,\nwhere | \u00b7 | denotes the absolute value of a real number. 1 In this case, the average loss function at edge device k \u2208 K is given by\nF k (w) 1 |D k | i\u2208Dk f i (w),(1)\nwhere |D k | denotes the cardinality of set D k . Accordingly, the average loss function for all the K edge devices is given as\nF (w) = K k=1 |D k |F k (w) K k=1 |D k | .(2)\nIn federated edge learning, the objective is to find desirable ML-parameters that minimize the average loss function F (w), i.e.,\nw * = arg min w F (w).(3)\nTowards this end, we use the distributed BGD that is implemented in a distributed manner. Suppose that M and N denote the numbers of global and local iterations during the distributed BGD, respectively. For each global iteration i \u2208 {1, ..., M }, let w (0) denote the initial global ML-parameters at the edge server, w (i) denote the global ML-parameters at the edge server after the i-th global iteration, and w (i,j) k denote the local ML-parameters at edge device k after the j-th local iteration of the i-th global iteration. We also use S to denote the required bits for sending w (i,N ) k to the edge server, which generally depends on the quantization and compression methods used. Then, the following procedure is implemented at each global iteration i.\n\n\n1) Edge server broadcasts global ML-parameters to devices:\n\nThe edge server broadcasts the global model ML-parameters w (i\u22121) to the K edge devices, where the local ML-parameters at each edge device k are set as w (i\u22121) , i.e., w\n(i,0) k = w (i\u22121) , \u2200k \u2208 K.\n2) Each edge device locally updates ML-parameters iteratively: Each edge device k \u2208 K updates its local ML-parameters in an iterative manner based on the gradient of average local loss function F k (w). In each local iteration j \u2208 {1, ..., N }, supposing that the gradient of F k (w) at point w is expressed as \u2207F k (w), then we have w  Specifically, in this paper, we focus on the energy-efficient operation of energy-hungry edge devices for federated edge learning, and as a result, we omit the communication and computation energy consumptions at the edge server, as well as the time delay for global ML-parameters broadcasting and aggregation in steps 1) and 4), respectively.\n(i,j) k = w (i,j\u22121) k + \u03b7\u2207F k (w (i,j\u22121) k ),\nIn the following, we explain the computation energy consumption at edge devices for updating local ML-parameters in step 2) and the communication energy consumption for uploading local ML-parameters to the edge server in step 3), respectively.\n\n\nA. Local ML-Parameters Update at Edge Devices\n\nFirst, we consider the local ML-parameters update at one particular global iteration i, in which each edge device k updates its local ML-parameters N times, by computing w\n(i,j) k = w (i,j\u22121) k + \u03b7\u2207F k (w (i,j\u22121) k )\n, \u2200j \u2208 {1, ..., N }. In order to model the energy consumption and delay for such computation, we use the number of floating point operations (FLOPs) to measure the computation complexity. For analytical convenience, it is assumed that the number of FLOPs 2 needed for each data sample per local update is a constant, denoted by a. As a result, the total number of FLOPs required at edge device k \u2208 K is given as F k = a \u00d7 |D k |. In order to efficiently reduce the energy consumption at each edge device, the dynamic voltage and frequency scaling (DVFS) technique [35] is applied to adaptively adjust the CPU frequency to perfectly match the computation demand. For each edge device k \u2208 K, let C k denote the number of FLOPs within a CPU cycle and f k denote the CPU frequency of the whole operation duration,\n\nwhere f k \u2208 (0, f max k ], and f max k denotes the maximum CPU frequency at edge device k. Accordingly, the computation time duration for each local update at edge device k is given by\nt loc k = F k C k 1 f k .(4)\nFor each edge device k \u2208 K, the energy consumption for local ML-parameters update mainly comes from that consumed by the CPU. It has been shown in [3] that the CPU power consumption is proportional to the square of CPU chip's voltage V 2 and the operating CPU clock frequency f , where the voltage is approximately linear with respect to the CPU frequency [34]. Therefore, the total energy consumption at edge device k \u2208 K for local ML-parameters update is given by [27] \nE loc k = F k C k \u03c2 k f 2 k ,(5)\nwhere \u03c2 k is a constant coefficient that depends on the chip architecture at edge device k \u2208 K.\n\n\nB. Local ML-Parameters Uploading from Edge Devices to Server\n\nNext, we consider the local ML-parameters uploading from the K edge devices to the edge server.\n\nFor the purpose of initial investigation, we consider a quasi-static frequency non-selective channel model, in which the wireless channels are assumed to remain unchanged during the whole training process. In particular, we consider two transmission protocols for the K edge devices to upload their ML-parameters to the edge server, namely NOMA and TDMA, respectively. 2 How to obtain an accurate estimate of the number of FLOPs for each local update is an important task. Here, we explain how to estimate that for CNN, while those for other ML models (such as SVM and DNN) can be similarly obtained. In particular, a CNN generally consists of multiple fully-connected layers and convolutional layers. For each fully-connected layer, the number of FLOPs can be easily evaluated as the product of the inputs and outputs. For each convolutional layer, the number of FLOPs can be estimated as the product of various factors including the spatial width and height of the feature map, previous and current layers, and the width and height of the Kernel. Please refer to [36] for more details and examples.\n\n\n1) NOMA-Based Transmission:\n\nFirst, we consider the NOMA-based transmission protocol, in which the K edge devices simultaneously upload their local ML-parameters to the server. Let t up denote the transmission duration. Suppose that the transmit signal at edge device k \u2208 K is x k , which is a circularly symmetric complex Gaussian (CSCG) random variable with zero mean and unit variance. Then the received signal at the edge server is given by\ny = K k=1 p k h k x k + z,(6)\nwhere p k \u2265 0 denotes the transmission power at edge device k, h k denotes the channel power gain from edge device k to the edge server, and z denotes the additive white Gaussian noise (AWGN) at the edge server that is a CSCG random variable with mean zero and variance \u03c3 2 , i.e., z \u223c CN (0, \u03c3 2 ).\n\nAt the receiver side, the edge server adopts the minimum mean squared error successive interference cancellation (MMSE-SIC) [31] [38] [39] to decode information from the K edge devices, where the edge server decodes the uploaded ML-parameters from the edge devices following a successive decoding order, denoted by \u03c0. Specifically, the edge server first decodes the information x \u03c0(K) from edge device \u03c0(K), then decodes the information x \u03c0(K\u22121) by cancelling the interference from x \u03c0(K) , followed by x \u03c0(K\u22122) , ..., until decoding x \u03c0(1) . Under Gaussian signalling and with a given decoding order \u03c0 and transmission power {p k }, the achievable rate (in bits per second) at edge device \u03c0(k), k \u2208 K, is given by [38], [39] r (\u03c0)\n\u03c0(k) = B log 2 \u03c3 2 + k n=1 p \u03c0(n) h \u03c0(n) \u03c3 2 + k\u22121 n=1 p \u03c0(n) h \u03c0(n) ,(7)\nwhere B denotes the transmission bandwidth. By properly designing the decoding order and employing time-sharing among different decoding orders, the achievable rate region for the K edge devices (or equivalently the capacity region of the multiple-access channel) is given by [38], [39] \nR NOMA (p) = \uf8f1 \uf8f2 \uf8f3 r \u2208 R K\u00d71 + k\u2208K r k \u2264 B log 2 1 + i\u2208K p k h k \u03c3 2 , \u2200K \u2286 K \uf8fc \uf8fd \uf8fe ,(8)\nwhere\nr [r 1 , ..., r K ] \u2020 , p [p 1 , ..., p K ] \u2020 , and R K\u00d71 +\ndenotes the set of all non-negative real vectors with dimension K. Here, the superscript \u2020 denotes the transpose. Supposing that each edge device k \u2208 K needs to accomplish its ML-parameters uploading task within the duration t up that is to be optimized, we can obtain the following transmission constraint\nr k t up \u2265 S, \u2200k \u2208 K,(9)\nwhere recall that S is the number of bits required for each edge device to send the local ML-parameters to edge server. The communication energy consumption at edge device k for uploading is thus given by\nE NOMA k = p k t up , k \u2208 K.\nBy combining the time delay for both local ML-parameters updates and uploading, and ignoring the delay for global ML-parameters broadcasting and aggregation, the total delay for training the ML-model is thus\nT NOMA = M (N t loc + t up ),(10)\nwhere the communication for uploading is implemented over M rounds (or global iterations), and the computation for local updates at each device is implemented M \u00d7 N times in total.\n\n2) TDMA-Based Transmission: Next, we consider the TDMA-based transmission protocol, where the K edge devices upload their updated local ML-parameters to the edge server over orthogonal time resources. Let t up k denote the local ML-parameters uploading duration allocated for edge device k \u2208 K. By letting p k denote the transmission power at edge device k, the achievable rate of this device is given\nas B log 2 (1 + pkhk \u03c3 2 ), k \u2208 K.\nIn order for each edge device k to successfully upload S bits to edge server over duration t up k , we have\nB log 2 1 + p k h k \u03c3 2 t up k \u2265 S, \u2200k \u2208 K.(11)\nThe communication energy consumption at each edge device k for uploading is thus given by E TDMA k = p k t up k . Accordingly, the total training delay is given by\nT TDMA = M (N t loc + K k=1 t up k ).(12)\n\nIII. PROBLEM FORMULATION\n\nOur objective is to minimize the total energy consumption at the K edge devices, subject to a maximum First, we consider the case with NOMA transmission protocol. In this case, the training-delay-constrained energy minimization problem is formulated as\n(P1) : min r,f ,p,tloc,tup M K k=1 N F k C k \u03c2 k f 2 k + p k t up s.t. r \u2208 R NOMA (p)(13)r k t up \u2265 S, \u2200k \u2208 K (14) M (N t loc + t up ) \u2264 T (15) t loc \u2265 F k C k 1 f k , \u2200k \u2208 K (16) 0 \u2264 f k \u2264 f max k , \u2200k \u2208 K (17) 0 \u2264 p k \u2264 P max , \u2200k \u2208 K,(18)\nwhere constraints (13) and (14) ensure the uploading of local ML-parameters at the K edge devices, constraint (15) specifies the training delay requirement, and constraint (16) characterizes the computation requirements for local updates at the K devices.\n\nNext, we consider the case with TDMA transmission protocol. In this case, the training-delay-constrained energy minimization problem is formulated as (17), (18).  (15), this in turn allows these devices to increase the time duration for local updates with reduced CPU frequencies f , thus leading to reduced computation energy consumption. Therefore, how to properly design p, r and f to optimally balancing such a tradeoff is essential for minimizing the total energy consumption.\n(P2) : min f ,p,tloc,{t up k } M K k=1 N F k C k \u03c2 k f 2 k + p k t up k s.t. B log 2 (1 + p k h k \u03c3 2 )t up k \u2265 S, \u2200k \u2208 K (19) M (N t loc + K k=1 t up k ) \u2264 T (20) (16),\nAlso note that problems (P1) and (P2) are generally challenging to be solved as both of them are not convex due to the coupling of p k and t up / t up k . Furthermore, in problem (P1) the constraint in (13) corresponds to a number of (2 K \u2212 1) inequality constraints, thus making (P1) more difficult to be solved, especially when K becomes large. Before proceeding to address problems (P1) and (P2) in Sections IV and V, respectively, we first check their feasibility in the following, i.e., checking whether these edge devices can efficiently accomplish the ML-model training task within the delay T .\n\n\nA. Feasibility Checking for Problem (P1)\n\nChecking the feasibility of problem (P1) corresponds to showing whether these devices are able to accomplish the ML-model training within delay T . Therefore, this is equivalent to minimizing the total training delay by solving the following problem:\nmin r,f ,p,tloc,tup M (N t loc + t up )(21)\ns.t. (13), (14), (16), (17), (18).\n\nIt is evident that the minimum training delay is attained when the edge devices use the largest CPU frequency and the highest transmission power, i.e., f k = f max k , p k = P max , \u2200k \u2208 K. In this case, we have\nt loc = max k\u2208K { Fk Ck 1 f max k }.\nHowever, it still remains to find the decoding orders at the edge server to determine the time delay for edge devices to upload the local ML-parameters to the edge server. This corresponds to solving the following problem:\nmin r,p,tup t up(22)\ns.t. p k = P max , \u2200k \u2208 K (23) (13), (14).\n\nIt can be easily shown that solving problem (22) is equivalent to solving the following problem to maximize the minimum or common achievable communication rate among the K edge devices, which has been optimally solved in [31].\nmax r,p,rr(24)\ns.t. r k \u2265r (13)(23).\n\nLet the optimal solution to problem (24) asr * , r * k =r * , and p * k = P max , \u2200k \u2208 K. Then the minimum communication delay is given as t up = S r * . Accordingly, we obtain the minimum training delay to problem (21) as\nT NOMA min = M N \u00d7 max k\u2208K F k C k 1 f max k + S r * .(25)\nIf T NOMA min \u2264 T , then edge devices are able to accomplish the communication task within duration T , i.e., problem (P1) is feasible. Otherwise, problem (P1) is infeasible.\n\n\nB. Feasibility Checking for Problem (P2)\n\nSimilar as in Section III-A, in order to check the feasibility of problem (P2), we minimize the training duration problem under the TDMA transmission protocol, for which the problem is formulated as\nmin r,f ,p,tloc,{t up k } M (N t loc + K k=1 t up k )(26)\ns.t. (16), (17), (18), (19).\n\nIt is easy to show that the minimum training duration is attained when all K edge devices use the maximum CPU frequency and the maximum transmission power, i.e., f k = f max k , p k = P max , \u2200k \u2208 K. Hence, the minimum training duration under the TDMA transmission protocol is given by\nT TDMA min = M N \u00d7 max k\u2208K F k C k 1 f max k + k\u2208K S B log 2 (1 + Pmaxhk \u03c3 2 ) .(27)\nIt is thus concluded that if T TDMA min \u2264 T , then problem (P2) is feasible. Otherwise, problem (P2) is infeasible.\n\n\nRemark 3.1:\n\nNotice that the achievable rate region under the NOMA transmission is always superior to that under the TDMA transmission [39]. Therefore, the NOMA transmission protocol always leads to lower minimum training delay than the TDMA counterpart. Furthermore, it can be shown that any feasible solution of transmission power/rate and CPU frequencies to problem (P2) under the TDMA case is also an feasible solution to (P1) under the NOMA case, but the opposite may not be true. Therefore, it is expected that the NOMA transmission also achieves lower energy consumption than the TDMA transmission, as will be validated in Section VI.\n\n\nIV. OPTIMAL SOLUTION TO PROBLEM (P1) UNDER NOMA\n\nIn this section, we propose an efficient algorithm to solve problem (P1) optimally. We first transform problem (P1) into a convex form and then obtain the optimal solution by employing the Lagrange duality method.\n\n\nA. Transformation of Problem (P1) into Convex Form\n\nWe first deal with the non-convexity of problem (P1). Towards this end, we introduce e k = p k t up , and s k = r k t up , \u2200k \u2208 K, and accordingly define e [e 1 , ..., e K ] \u2020 and s [s 1 , ..., s K ] \u2020 . By replacing p k = ek tup and r k = sk tup , \u2200k \u2208 K, we transform problem (P1) into the following equivalent form:\n(P1.1) : min s,f ,e,tloc,tup M K k=1 N F k C k \u03c2 k f 2 k + e k s.t. s \u2208 C(e, t up )(28)s k \u2265 S, \u2200k \u2208 K (29) 0 \u2264 e k \u2264 P max t up , \u2200k \u2208 K(30)\n(15), (16), (17),\nwhere C(e, t up ) = \uf8f1 \uf8f2 \uf8f3 s \u2208 R K\u00d71 + k\u2208K s k \u2264 B log 2 (1 + 1 t up k\u2208K e k h k \u03c3 2 )t up , \u2200K \u2286 K \uf8fc \uf8fd \uf8fe .(31)\nNotice that in (31), the right-hand-side (RHS) of each inequality inside the set corresponds to a concave perspective function, and therefore, C(e, t up ) is a convex set. Accordingly, problem (P1.1) is a convex optimization problem. However, it is still intractable to solve problem (P1.1) via standard convex optimization techniques such as the interior point method [42]. This is due to the fact that constraint (28) represents a number of (2 K \u2212 1) inequality constraints, thus making (P1.1) extremely difficult to be solved when K is sufficiently large.\n\n\nB. Optimal Solution to Problem (P1.1) or (P1)\n\nAs problem (P1.1) is convex and satisfies the Slater's condition, the strong duality holds between this problem and its dual problem [42]. Therefore, we leverage the Lagrange duality method to obtain the optimal solution to problem (P1.1). Let \u03bb k \u2265 0, \u00b5 k \u2265 0, k \u2208 K, denote the dual variables associated with the k-th constraints in (29) and (16), respectively. We define \u03bb [\u03bb 1 , ..., \u03bb K ] \u2020 and \u00b5 [\u00b5 1 , ..., \u00b5 K ] \u2020 .\n\nLet \u03bd \u2265 0 denote the dual variable associated with constraint (15). The partial Lagrangian of problem (P1.1) is given by\nL 1 (f , s, e, t loc , t up , \u03bb, \u00b5, \u03bd) = K k=1 M N F k C k \u03c2 k f 2 k + F k C k 1 f k \u00b5 k + K k=1 \u03bb k (S \u2212 s k ) + t loc (\u03bdM N \u2212 K k=1 \u00b5 k ) + M K k=1 e k + \u03bdM t up \u2212 \u03bdT.(32)\nThen the dual function of problem (P1.1) is\ng 1 (\u03bb, \u00b5, \u03bd) = min f ,s,e,tloc,tup L 1 (f , s, e, t loc , t up , \u03bb, \u00b5, \u03bd) s.t. s \u2208 C(e, t up ) 0 \u2264 f k \u2264 f max k , \u2200k \u2208 K 0 \u2264 e k \u2264 P max t up , \u2200k \u2208 K.(33)\nLemma 4.1: In order for the dual function g 1 (\u03bb, \u00b5, \u03bd) to be bounded from below (i.e. g 1 (\u03bb, \u00b5, \u03bd)) >\n\u2212\u221e), it must hold that (\u03bdM N \u2212 K k=1 \u00b5 k ) \u2265 0. Proof 4.1: Suppose that (\u03bdM N \u2212 K k=1 \u00b5 k ) < 0.\nThen by setting t loc \u2192 \u221e, we have g 1 (\u03bb, \u00b5, \u03bd) \u2192 \u2212\u221e. Therefore, this lemma is proved.\n\nAccordingly, the dual problem of (P1.1) is\n(D1.1) : max \u03bb,\u00b5,\u03bd g 1 (\u03bb, \u00b5, \u03bd) s.t. \u03bb k \u2265 0, \u2200k \u2208 K (34) \u00b5 k \u2265 0, \u2200k \u2208 K (35) \u03bd \u2265 0 (36) \u03bdM N \u2212 K k=1 \u00b5 k \u2265 0.(37)\nIn the following, we first solve problem (33) under given feasible \u03bb, \u00b5, \u03bd to obtain g 1 (\u03bb, \u00b5, \u03bd), and then find the optimal \u03bb, \u00b5, \u03bd to maximize g 1 (\u03bb, \u00b5, \u03bd) by solving problem (D1.1).\n\n1) Obtaining g 1 (\u03bb, \u00b5, \u03bd) by Solving Problem (33) Under Given \u03bb, \u00b5 and \u03bd: First, we decompose problem (33) into the following (K + 2) subproblems.\nmin 0\u2264fk\u2264f max k M N F k C k \u03c2 k f 2 k + F k C k 1 f k \u00b5 k , \u2200k \u2208 K. (38) min tloc t loc (\u03bdM N \u2212 K k=1 \u00b5 k ).(39)\nmin s,e,tup 0 \u2264 e k \u2264 P max t up , \u2200k \u2208 K.\n\nFirst, we present the optimal solutions to the subproblems in (38), which are obtained based on the Karush-Kuhn-Tucker (KKT) conditions [42].\n\n\nLemma 4.2:\n\nThe optimal solution f * k , k \u2208 K, to each subproblem in (38) is given by\nf * k = min( 3 \u00b5 k 2M N \u03c2 k , f max k ).(41)\n\nProof 4.2: See Appendix A.\n\nNext, as for subproblem (39), since (\u03bdM N \u2212 K k=1 \u00b5 k ) \u2265 0 must hold based on Lemma 4.1, we can obtain that the optimal t * loc equals to zero when (\u03bdM N \u2212 K k=1 \u00b5 k ) > 0, and can be any arbitrary real number when (\u03bdM N \u2212 K k=1 \u00b5 k ) = 0. Then, we solve problem (40). Towards this end, we have the following lemma from [39], for which the proof is omitted for brevity. \n\nis attained at a point r (\u03c0) [r (\u03c0) \u03c0(1) , ..., r (\u03c0) \u03c0(K) ] of the polymatroid R NOMA (p), where the successive decoding order \u03c0 is any feasible permutation such that \u03bb \u03c0(1) \u2265 ... \u2265 \u03bb \u03c0(K) . Meanwhile, the achievable rate of r (\u03c0) \u03c0(k) , k \u2208 K is given by (7).\n\nBased on lemma 4.3, it follows that under any given \u03bb, e and t up , the optimal solution of max s \n\nis attained at s (\u03c0) [s (\u03c0) \u03c0(1) , ..., s (\u03c0) \u03c0(K) ], where \u03c0 is any feasible permutation such that \u03bb \u03c0(1) \u2265 ... \u2265 \u03bb \u03c0(K) , and s (\u03c0) \u03c0(k) is given by\ns (\u03c0) \u03c0(k) = B log 2 \u03c3 2 + 1 tup k n=1 e \u03c0(n) h \u03c0(n) \u03c3 2 + 1 tup k\u22121 n=1 e \u03c0(n) h \u03c0(n) t up , k \u2208 K.(44)\nBased on (44), it follows that solving subproblem (40) is equivalent to optimizing e and t up by solving the following problem:\nmax e,tup K k=1 \u03bb \u03c0(k) \u2212 \u03bb \u03c0(k+1) B log 2 1 + k n=1 e \u03c0(n) h \u03c0(n) t up \u03c3 2 t up \u2212 M e k \u2212 \u03bdM t up s.t. 0 \u2264 e k \u2264 P max t up , \u2200k \u2208 K,(45)\nwhere we define \u03bb K+1 0 for notational convenience.\n\nNotice that problem (45) is a convex optimization problem with respect to e and t up , and thus can be solved efficiently by standard convex solvers, e.g., CVX [43]. 3 Let e * and t * up denote the optimal solution to problem (45), and we obtain s * based on (44). Accordingly, s * , e * and t * up become the optimal solution to problem (40).\n\n2) Finding Optimal \u03bb, \u00b5 and \u03bd to solve (D1.1): Next, we find the optimal (\u03bb opt , \u00b5 opt , \u03bd opt ) to maximize g 1 (\u03bb, \u00b5, \u03bd) for solving the dual problem (D1.1). As the dual function g 1 (\u03bb, \u00b5, \u03bd) is always convex but generally non-differentiable, we solve problem (D1.1) by using the subgradient-based methods, such as the ellipsoid method [44]. Notice that the subgradient of the objective function g 1 (\u03bb, \u00b5, \u03bd) with respect\nto (\u03bb, \u00b5, \u03bd) is [s * 1 \u2212 S, ..., s * K \u2212 S, t * loc \u2212 F1 C1 1 f * 1 , ..., t * loc \u2212 FK CK 1 f * K , T \u2212 M (N t * loc + t * up )].\n\n3) Constructing Optimal Primal Solution to (P1.1) or (P1):\n\nBased on the optimal \u03bb opt , \u00b5 opt and \u03bd opt , we need to construct the optimal primal solution to (P1.1), denoted by (f opt , s opt , e opt , t opt loc , t opt up ). By solving problem (45) under \u03bb opt and \u03bd opt , we obtain the optimal solution of e opt and t opt up to problem (P1.1). Similarly, by substituting \u03bb opt into (41), we obtain the optimal f opt . Based on the obtained optimal f opt and the constraint (16), we obtain that the optimal local update delay t opt loc = max\nk\u2208K { Fk Ck 1 f opt k }.\nFinally, we still need to determine the primal optimal s opt and the corresponding optimal decoding order at the edge server. Let \u03c0 opt = [\u03c0 opt (1), ..., \u03c0 opt (K)] \u2020 denote the permutation that satisfies the condition \u03bb opt \u03c0(1) \u2265 ... \u2265 \u03bb opt \u03c0(K) \u2265 0. In this case, the primal optimal decoding order is \u03c0 opt and the optimal s opt can be obtained based on (44). With optimal s opt and t up at hand, the optimal transmission rate r opt is obtained accordingly. It should be emphasized that if there exist some \u03bb opt k 's that equal to each other, then we may need to time-sharing among different decoding orders. In this case we construct the feasible and optimal s opt to the problem (P1.1) (or equivalently the optimal solution r opt to problem (P1)) by using the time-sharing technique, similarly as that adopted in [40] [41]. For brevity, we skip the discussion about the time-sharing technique here, and please refer to [40] [41] for details.\n\n\nV. OPTIMAL SOLUTION TO PROBLEM (P2) UNDER TDMA\n\nIn this section, we obtain the optimal solution to problem (P2) under the TDMA case. First, we transform problem (P2) into a convex form. Similarly as for solving problem (P1) in Section IV, we introduce a set of variables e [ e 1 , ..., e K ] \u2020 with e k = p k t up k , \u2200k \u2208 K. Accordingly, we transform problem (P2) into the following convex form: (17), (20).\n(P2.1) : min f , e,tloc,{t up k } M K k=1 N F k C k \u03c2 k f 2 k + e k s.t. B log 2 (1 + e k h k t up k \u03c3 2 )t up k \u2265 S, \u2200k \u2208 K (46) 0 \u2264 e k \u2264 P max t up k , \u2200k \u2208 K (47) (16),\nNext, we employ the Lagrange duality method to obtain the optimal solution to the convex problem (P2.1). Let \u03c9 k \u2265 0, k \u2208 K, and \u03b6 \u2265 0 denote the dual variables associated with the constraints (16) and (20), respectively, where we define \u03c9 [\u03c9 1 , ..., \u03c9 K ] \u2020 for convenience. Then the partial Lagrangian of problem (P2.1) is given by\nL 2 (\u03c9, \u03b6, f , e, t loc , {t up k }) = K k=1 (M N F k C k \u03c2 k f 2 k + \u03c9 k F k C k 1 f k ) + (M N \u03b6 \u2212 K k=1 \u03c9 k )t loc + M K k=1 ( e k + \u03b6t up k ).(48)\nThe dual function of problem (P2.1) is defined as\ng 2 (\u03c9, \u03b6) = min f , e,tloc,{t up k } L 2 \u03c9, \u03b6, f , e, t loc , {t up k }(49)\ns.t. Similar as in Lemma 4.1, it follows that in order for the dual function g 2 (\u03c9, \u03b6) to be bounded from below (i.e., g 2 (\u03c9, \u03b6) > \u2212\u221e), we must have M N \u03b6 \u2212 K k=1 \u03c9 k \u2265 0. Accordingly, the dual problem of (P2.1) is given as\n(D2.1) : max \u03c9,\u03b6 g 2 (\u03c9, \u03b6) s.t. \u03c9 k \u2265 0, \u2200k \u2208 K (50) \u03b6 \u2265 0 (51) M N \u03b6 \u2212 K k=1 \u03c9 k \u2265 0.(52)\nIn the following, we first solve problem (49) under any given feasible \u03c9, \u03b6 to obtain g 2 (\u03c9, \u03b6), and then find the optimal \u03c9, \u03b6 to maximize g 2 (\u03c9, \u03b6) by solving problem (D2.1).\n\nFirst, we decompose problem (49) into (2K + 1) subproblems. Similar as in Lemma 4.2, the optimal solution f \u22c6 k to k-th subproblem in (53) is given by\nmin 0\u2264fk\u2264f max k M N F k C k \u03c2 k f 2 k + \u03c9 k F k C k 1 f k , k \u2208 K. (53) min tloc t loc (\u03b6M N \u2212 K k=1 \u03c9 k ).(54)f \u22c6 k = min 3 \u03c9 k 2M N \u03c2 k , f max k .(56)\nFor subproblem (54), since (M N \u03b6 \u2212 K k=1 \u03c9 k ) \u2265 0 must hold in subproblem (54), the optimal solution t \u22c6 loc is zero when (M N \u03b6 \u2212 K k=1 \u03c9 k ) > 0, and can be any real number when (M N \u03b6 \u2212 K k=1 \u03c9 k ) = 0. Then, consider subproblem (55). It is noted that B log 2 (1 + ekhk t up k \u03c3 2 )t up k = S, \u2200k \u2208 K must hold at the optimal solution, and thus we have\ne k = 2 S Bt up k \u2212 1 t up k \u03c3 2 h k , \u2200k \u2208 K.(57)\nBy substituting (57) into (55) and after some simple manipulations, the k-th subproblem in (55) reduces to the following problem:\nmin t up k 2 S Bt up k \u2212 1 t up k \u03c3 2 h k + \u03b6t up k(58)s.t. t up k \u2265 S B log(1 + Pmaxhk \u03c3 2 )\n, k \u2208 K.\n\nProblem (58) is a convex optimization problem to t up k . By setting the first-derivative of the objective function to be zero, we have\n2 S Bt up k \u03c3 2 h k (1 \u2212 S ln 2 Bt up k ) \u2212 \u03c3 2 h k + \u03b6 = 0.(59)\nLet \u03c4 \u22c6 k , k \u2208 K, denote the solution to the above equation in (59), which can be easily obtained via a bisection search. Then the optimal solution t up \u22c6 k to problem (58) is given as\nt up \u22c6 k = max \u03c4 \u22c6 k , S B log(1 + Pmaxhk \u03c3 2 )\n, \u2200k \u2208 K.\n\nBased on optimal t up \u22c6 k 's and (57), the optimal e \u22c6 k 's are obtained accordingly. Therefore, the dual function g 2 (\u03c9, \u03b6) in (49) is finally obtained.\n\nNext, with obtained f \u22c6 , t \u22c6 loc , {t up \u22c6 k } and e \u22c6 at hand, we employ the ellipsoid method to find the optimal \u03c9 opt and \u03b6 opt to maximize g 2 (\u03c9, \u03b6) for solving problem (D2.1).\n\nFinally, based on the obtained optimal \u03c9 opt and \u03b6 opt , we need to construct the optimal primal solution to (P2.1), denoted by f opt , e opt , {t up opt k }, and t opt loc . By substituting \u03c9 opt into (56), we can obtain the optimal f opt . Based on the obtained optimal f opt and constraint (16), we obtain the optimal local update delay\nas t opt loc = max k\u2208K { Fk Ck 1 f opt k }.\nBy resolving equation (59) under optimal \u03b6 opt , we obtain the optimal t up opt k , and thus e opt based on (57) accordingly. Therefore, problem (P2.1) (or equivalently (P2)) is finally solved.\n\n\nVI. NUMERICAL RESULTS\n\nIn this section, we present numerical results to validate the performance of our proposed energy-efficient federated edge learning design. In the simulation, we consider the quasi-static channel model, where the path loss from the edge server to edge device k \u2208 K is given by \u03b2 0 ( dk d0 ) \u2212\u03b10 . Here, d k denotes the corresponding distance, \u03b1 0 = 3 denotes the path loss exponent, and \u03b2 0 = \u221230 dB denotes the channel power gain at a reference distance of d 0 = 1 m. We set the system bandwidth for ML-parameters uploading as B = 2 MHz and the noise power at the edge server as \u03c3 2 = \u2212100 dBm. We consider the scenario with K = 3 edge devices, where the distances from the edge devices to edge server are d 1 = 100 m, d 2 = 150 m and d 3 = 200 m, respective, unless stated otherwise. Furthermore, we consider that the ML-model is a CNN 4 , which is trained by using the distributed BGD method. We consider the MNIST data set, and all the three edge devices have 1000 data samples in total.  4 We consider a similar CNN structure as in [16], where a \u2248 6 GPLOPs based on [36]. Furthermore, suppose that each parameter of the training model is quantized into 12 bits, and as a result, we have S \u2248 4.9 Gbits accordingly.  (30,15) and (25,20) can be adopted.\n\nThis shows that more local iterations of computation (with larger N ) can be used to trade for less global iterations of communication (with smaller M ). for global ML-parameters uploading is small and thus the total energy consumption is dominated by the computation part for local ML-parameters update. Therefore, a small value of N is desirable. By contrast, when the average distance increases (e.g., larger than 100m), the opposite is observed to be true. and (M = 20, N = 25). This is due to the fact that under small value of C, higher CPU frequencies are generally needed for meeting the training deadline requirement, and as a result, the computation energy consumption for local ML-parameters update becomes the dominant part of the total energy consumption at edge devices. As a result, a smaller value of N is preferred. By contrast, when C becomes large (e.g., f k = f max k , \u2200k \u2208 K, respectively. \u2022 Computation design only: Each edge device k \u2208 K updates the local ML-parameters to the edge server by using the maximum transmission power and only optimizes the CPU frequencies during the local ML-parameters update. Under NOMA and TDMA, the CPU frequencies at the edge devices can be obtained by solving problem (P1) and (P2) under given p k = P max , \u2200k \u2208 K, respectively.\nC\n\u2022 Training delay minimization: The edge devices adopt the maximum transmission power and maximum CPU frequencies during the training process to minimize the delay for training the ML-model. This corresponds to solving problems (21) and (26) in Sections III-A and III-B, respectively. the benefit of our proposed designs. It is also observed that for our proposed designs under both NOMA and TDMA, the energy consumption at edge devices first decreases when f max increases from 0.5 GHz to 1 GHz, but keeps unchanged when it further increases. This is due to the fact that there exists an optimal CPU frequency between 0.5 GHz to 1 GHz, and therefore, further increasing the maximum CPU frequencies at edge devices cannot improve the energy efficiency. By contract, for the schemes with computation design only and training delay minimization, it is observed that increasing the maximum CPU frequency may lead to increased energy consumption. Furthermore, it is observed that for the two schemes with joint communication and computation design and communication design only, the NOMA    gains over the other benchmarks under both NOMA and TDMA transmission protocols. It is also observed that NOMA is feasible when T is larger than 124 s, while TDMA is only feasible when T is larger than 150 s. This verifies that NOMA leads to shorter minimum training delay than TDMA, as shown in Remark 3.1. Furthermore, it is observed that when T is sufficiently large, the performance gap between NOMA and TDMA becomes marginal.\n\n\nVII. CONCLUSION\n\nIn this paper, we investigated the energy efficient operation of a federated edge learning system, in which two transmission protocols for ML-parameters uploading, namely NOMA and TDMA, were considered. We minimized the total communication and computation energy consumption at edge devices subject to a given training delay requirement, by jointly designing the communication and computation resource allocations. Although the formulated problems were non-convex, we transformed them into convex forms, and then presented efficient algorithms to solve them optimally. Numerical results revealed interesting tradeoffs among energy consumption, training speed and training accuracy. It was shown that our proposed designs achieve significant performance gains over other benchmark schemes without such joint optimization. It was also shown that properly choosing the numbers of global and local iterations during the distributed training further helps enhance the system energy efficiency by properly balancing the communication-computation energy tradeoff. Under any given feasible \u00b5, subproblem (38) is a convex optimization problem as the objective function is a convex function with respective to f k and the constraint 0 \u2264 f k \u2264 f max k is linear for each edge device k \u2208 K. As subproblem (38) satisfies the Slater's condition, the strong duality holds between subproblem (38) and its dual problem.\n\nLet \u03c4 k \u2265 0 and \u03c4 k \u2265 0 denote the Lagrange multipliers associated with the inequality 0 \u2264 f k and f k \u2264 f max k , respectively. Then the Lagrangian of k-th subproblem in (38) is given by\nL k (f k , \u03c4 k , \u03c4 k ) =M N F k C k \u03c2 k f 2 k + F k C k 1 f k \u00b5 k \u2212 \u03c4 k f k + \u03c4 k (f k \u2212 f max k ).(61)\nLet (\u03c4 * k , \u03c4 * k ) denote the optimal dual solution and f * k denote the optimal primal solution to subproblem (38). They should satisfy the KKT conditions [42], which are given as\n0 \u2264 f * k , f * k \u2264 f max k , \u03c4 * k \u2265 0, \u03c4 * k \u2265 0 (62) \u03c4 * k f * k = 0, \u03c4 * k (f * k \u2212 f max k ) = 0 (63) 2M N F k C k \u03c2 k f * k \u2212 F k C k 1 f * k 2 \u00b5 k \u2212 \u03c4 * k + \u03c4 * k = 0.(64)\nBased on the above KKT conditions and via some simple manipulations, this lemma can be easily verified.\n\nFig. 1 .\n1Illustration of the federated edge learning system with one edge server coordinating multiple edge devices to train shared ML-models.\n\n\nK edge devices and updates the global ML-parameters by averaging them, i.e., w (i) = k\u2208K w (i,N ) k K . After the M global iterations, the global ML-parameters w (M ) at the edge server are set as the desirable solution to problem (3), i.e., w * \u2190 w (M ) . Notice that the performance of the federated edge learning depends on the numbers of global and local iterations M and N . In general, larger values of M and N lead to smaller loss function and higher training accuracy, but also result in larger communication and computation energy consumption as well as longer training delay. Furthermore, to achieve the same training accuracy, it is possible to increase the number of local iterations N with higher computation energy consumption and delay for trading for a smaller number of global iterations M with lower communication energy consumption and delay. Therefore, there generally exist interesting performance tradeoffs in choosing M and N to balance among the training speed, accuracy, and energy consumption. We will leave the detailed discussion on choosing M and N in section VI, and will focus on the joint communication and computation resource allocation under given M and N in Sections III-V unless stated otherwise.\n\n\ntraining delay constraint T . The decision variables include the communication and computation resource allocation, i.e., the transmission power p and rate r, and the CPU frequency f [f 1 , ..., f K ] \u2020 . In Sections III-V, in order to focus our study on the communication and computation resource allocations, we fix the numbers of global and local iterations M and N . Note that the values of M and N are properly chosen in order to balance the communication-computation energy tradeoff while ensuring a certain training accuracy, as will be shown in Section VI.\n\n\nNote that in both problems (P1) and (P2), there generally exists an tradeoff between communication and computation in reducing the energy consumption. It is observed that the edge devices can increase the transmission power p (or increase the communication energy consumption) to reduce the communication time for uploading ML-parameters. Under the training delay constraint in\n\n\nt. s \u2208 C(e, t up )\n\nLemma 4 . 3 :\n43For any given \u03bb and p, the optimal solution of max r K k=1 \u03bb k r k s.t. r \u2208 R NOMA (p)\n\n\ns k s.t. s \u2208 C(e, t up )\n\nFig. 2\n2shows the training accuracy versus the number of global iterations M , under different number of local iterations N . It is observed that under a given number of local (global) iterations N (M ), the\n\nFig. 2 .Fig. 3 .\n23The average training accuracy versus the number of global iterations M under different numbers of local iterations N . The energy consumption at edge devices versus the average distance between the edge server and edge devices, under different values of M and N , where T = 166 s. training accuracy increases as the number of global (local) iterations M (N ) becomes larger. It is also observed that different values of M and N can be adopted to achieve the same training accuracy. For instance, to achieve the training accuracy of 85%, (M, N ) = (50, 8),\n\nFig. 4 .\n4The energy consumption at edge devices versus the number of FLOPs within a CPU cycle C = C k , \u2200k \u2208 K, under different values of M and N , where T = 850 s. Next, we show the energy consumption of edge devices under different numbers of global and local iterations M and N to achieve the same training accuracy of 85%. Based on Fig. 2, we choose three pairs of parameters (M, N ) = (50, 8), (30, 15) and (20, 25). Fig. 3 shows the energy consumption at edge devices versus the average distance between the edge server and edge devices, under different values of M and N . Here, the distances with three edge devices are set as an arithmetic sequence, with the common difference being 45 m (e.g., when the average distance is 100m, then the distances from the three edge devices to the edge server are d 1 = 55 m, d 2 = 100 m and d 3 = 145 m, respectively). It is observed that the NOMA transmission always outperforms the TDMA. This is consistent with Remark 3.1. It is also observed that under both NOMA and TDMA, when the average distance is short (e.g., shorter than 50m), (M = 50, N = 8) and (M = 30, N = 15) lead to less energy consumption than (M = 20, N = 25). This is due to the fact that in this case, the communication energy consumption\n\nFig. 4\n4shows the energy consumption at edge devices versus the number of FLOPs within a CPU cycle C = C k , \u2200k \u2208 K, under different values of M and N . It is observed that when the number of FLOPs within a CPU cycle is small or the computation capacities at edge devices are limited (e.g., C k = 2), the parameters of (M = 50, N = 8) lead to lower energy consumption than that by (M = 30, N = 15)\n\n\n= 10), the parameters of (M = 20, N = 25) are observed to outperform (M = 50, N = 8) and (M = 30, N = 15). Then, we compare the performance of our proposed joint communication and computation design versus the following benchmark schemes. For comparison, in the following we fix the numbers of global and local iterations as M = 20 and N = 25, respectively, under the training accuracy requirement of 85%. \u2022 Communication design only: Each edge device k \u2208 K locally updates their ML-parameters by using the maximum CPU frequency and only optimizes the transmission power and rate for global MLparameters aggregation in the uploading process. Under NOMA and TDMA cases, the transmission powers and rates at the edge devices can be obtained by solving problems (P1) and (P2) under given\n\nFig. 5\n5shows the energy consumption at edge devices versus the maximum CPU frequency f max = f max k , \u2200k \u2208 K, where T = 431 s. It is observed that under both NOMA and TDMA, our proposed joint communication and computation designs outperform the other benchmark designs. This thus validates\n\nFig. 5 .\n5The energy consumption at edge devices versus the maximum CPU frequency f max = f max k , \u2200k \u2208 K, where T = 431 s.\n\nFig. 6 .\n6only (NOMA) Computation design only (NOMA) Training delay minimization (NOMA) Joint communication and computation design (TDMA) Communication design only (TDMA) Computation design only (TDMA) Training delay minimization (TDMA) The energy consumption at edge devices versus the maximum transmission power P max , where T = 271 s.transmission always outperforms the TDMA transmission in term of the energy efficiency; while for the other two schemes, NOMA may lead to higher energy consumption than TDMA.\n\nFig. 6\n6shows the energy consumption at edge devices versus the maximum transmission power P max , where T = 271 s. Similar observations are made as in Fig. 5.\n\nFig. 7 Fig. 7 .\n77shows the energy consumption at edge devices versus the training delay requirement T . It is observed that our proposed joint computation and communication designs achieve significant performance The energy consumption at edge devices versus the training delay requirement T .\n\n\nwhere \u03b7 denotes the learning step size.3) Edge devices upload local ML-parameters to server: The K edge devices upload their updatedlocal ML-parameters w \n\n(i,N ) \n1 \n\n, ..., w \n\n(i,N ) \nK \n\nto the edge server. \n\n4) Edge server updates global ML-parameters via aggregation: The edge server aggregates all the \n\nuploaded local ML-parameters w \n\n\nPlease refer to[32],[33] for details about the loss functions for SVM, K-means and convolutional neural network (CNN).\nNote that the optimal e * (i.e., the optimal transmission power p * in problem (P1)) is unique due to the strict convexity of problem (45). However, the optimal s * is generally non-unique[28]. Therefore, an addition step is needed later for constructing a feasible primal solution to problem (P1.1) by time-sharing among different solutions. Here, we can arbitrarily choose one solution of s * for obtaining the dual function.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 521Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, pp. 436-444, May 2015.\n\nMachine learning meets computation and communication control in evolving edge and cloud: Challenges and future perspective. T K Rodrigues, K Suto, H Nishiyama, J Liu, N Kato, Commun. Surv. Tut. to appear in IEEET. K. Rodrigues, K. Suto, H. Nishiyama, J. Liu, and N. Kato, \"Machine learning meets computation and communication control in evolving edge and cloud: Challenges and future perspective,\" to appear in IEEE Commun. Surv. Tut., 2019.\n\nA Survey on mobile edge computing: The communication perspective. Y Mao, C You, J Zhang, K Huang, K B Letaief, IEEE Commun. Surv. Tut. 1944th QuartY. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, \"A Survey on mobile edge computing: The communication perspective,\" IEEE Commun. Surv. Tut., vol. 19, no. 4, pp. 2322-2358, 4th Quart., 2017.\n\nEdge AI: On-demand accelerating deep neural network inference via edge computing. E Li, L Zeng, Z Zhou, X Chen, to appear in IEEE Trans. Wireless Commun.E. Li, L. Zeng, Z. Zhou, and X. Chen, \"Edge AI: On-demand accelerating deep neural network inference via edge computing,\" to appear in IEEE Trans. Wireless Commun., 2019.\n\nIn-Edge AI: Intelligentizing mobile edge computing, caching and communication by federated Learning. X Wang, Y Han, C Wang, Q Zhao, X Chen, M Chen, IEEE Netw. 335X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, \"In-Edge AI: Intelligentizing mobile edge computing, caching and communication by federated Learning,\" IEEE Netw., vol. 33, no. 5, pp. 156-165, Jul. 2019.\n\nTowards an intelligent edge: Wireless communication meets machine learning. G Zhu, D Liu, Y Du, C You, J Zhang, K Huang, Commun. Mag. to appear in IEEEG. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, \"Towards an intelligent edge: Wireless communication meets machine learning,\" to appear in IEEE Commun. Mag., 2019.\n\nAn overview of data-importance aware radio resource management for edge machine learning. D Wen, X Li, Q Zeng, J Ren, K Huang, Journal of Communications and Information Networks. 44D. Wen, X. Li, Q. Zeng, J. Ren, and K. Huang, \"An overview of data-importance aware radio resource management for edge machine learning,\" Journal of Communications and Information Networks, vol. 4, no. 4, pp. 1-14, Dec. 2019.\n\nThe roadmap to 6G -AI empowered wireless networks. K B Letaief, W Chen, Y Shi, J Zhang, Yj A Zhang, K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and YJ. A. Zhang, \"The roadmap to 6G -AI empowered wireless networks,\" [Online]. Available:https://arxiv.org/abs/1904.11686\n\nFederated learning: Strategies for improving communication efficiency. J Konecny, H Mcmahan, F X Yu, P Richtarik, A T Suresh, D Bacon, J. Konecny, H. B McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon, \"Federated learning: Strategies for improving communication efficiency.\" [Online]. Available: https://arxiv.org/abs/1610.05492\n\nFederated machine learning: Concept and applications. Q Yang, Y Liu, T Chen, Y Tong, ACM Trans. Intell. Syst. Technol. 102Q. Yang, Y. Liu, T. Chen, and Y. Tong, \"Federated machine learning: Concept and applications\" ACM Trans. Intell. Syst. Technol., vol. 10, no. 2, pp. 1-19, Feb. 2019.\n\nCommunication-efficient learning of deep networks from decentralized data. H B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Arcas, \"Communication-efficient learning of deep networks from decentralized data.\" [Online]. Available: https://arxiv.org/abs/1602.05629\n\nFederated learning for mobile kyboard prediction. A Hard, K Rao, R Mathews, S Ramaswamy, F Beaufays, S Augenstein, H Eichner, C Kiddon, D Ramage, A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and D. Ramage, \"Federated learning for mobile kyboard prediction.\" [Online]. Available: https://arxiv.org/abs/1811.03604\n\n. A I Webank, Group, Shenzhen, China, White Paper 9Federated learning white paper V1.0,\" WeBankWeBank AI Group, \"Federated learning white paper V1.0,\" WeBank, Shenzhen, China, White Paper 9, 2018. [Online].\n\nDeep gradient compression: Reducing the communication bandwidth for distributed training. Y Lin, S Han, H Mao, Y Wang, W J Dally, Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, \"Deep gradient compression: Reducing the communication bandwidth for distributed training.\" [Online]. Available: https://arxiv.org/abs/1712.01887\n\nRobust and communication-efficient federated learning from non-iid data. F Sattler, S Wiedemann, K R Muller, W Samek, F. Sattler, S. Wiedemann, K. R. Muller, and W. Samek, \"Robust and communication-efficient federated learning from non-iid data.\" [Online]. Available: https://arxiv.org/abs/1903.02891\n\nAdaptive federated learning in resource constrained edge computing systems. S Wang, T Tuor, T Salonidis, K K Leung, C Makaya, T He, K Chan, IEEE J. Sel. Areas Commun. 376S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan, \"Adaptive federated learning in resource constrained edge computing systems,\" IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, Jun. 2019.\n\nClient selection for federated learning with heterogeneous resources in mobile edge. T Nishio, R Yonetani, Proc. IEEE ICC. IEEE ICCT. Nishio and R. Yonetani, \"Client selection for federated learning with heterogeneous resources in mobile edge,\" in Proc. IEEE ICC, 2019, pp. 1-7.\n\nBroadband analog aggregation for low-latency federated edge learning. G Zhu, Y Wang, K Huang, IEEE Trans. Wireless Commun. 191G. Zhu, Y. Wang, and K. Huang, \"Broadband analog aggregation for low-latency federated edge learning,\" IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 491-506, Jan. 2020.\n\nFederated learning via over-the-air computation. K Yang, T Jiang, Y Shi, Z Ding, K. Yang, T. Jiang, Y. Shi, and Z. Ding, \"Federated learning via over-the-air computation.\" [Online]. Available: https://arxiv.org/abs/1812.11750\n\nMachine learning at the wireless edge: Distributed stochastic gradient descent over-the-air. M M Amiri, D Gunduz, M. M. Amiri and D. Gunduz, \"Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air.\" [Online]. Available: https://arxiv.org/abs/1901.00844\n\nEdge-assisted hierarchical federated learning with non-iid data. L Liu, J Zhang, S H Song, K B Letaief, L. Liu, J. Zhang, S.H. Song, and K. B. Letaief, \"Edge-assisted hierarchical federated learning with non-iid data.\" [Online].\n\nMIMO over-the-air computation for high-mobility multimodal sensing. G Zhu, K Huang, IEEE Internet Things J. 64G. Zhu and K. Huang, \"MIMO over-the-air computation for high-mobility multimodal sensing,\" IEEE Internet Things J., vol. 6, no. 4, pp. 6089-6103, Aug. 2019.\n\nOptimal power control for over-the-air computation in fading channels. X Cao, G Zhu, J Xu, K Huang, X. Cao, G. Zhu, J. Xu, and K. Huang, \"Optimal power control for over-the-air computation in fading channels.\" [Online].\n\nEnergy-efficient radio resource allocation for federated edge learning. Q Zeng, Y Du, K K Leung, K Huang, Q. Zeng, Y. Du, K. K. Leung, and K. Huang, \"Energy-efficient radio resource allocation for federated edge learning.\" [Online]. Available: https://arxiv.org/abs/1907.06040\n\nEnergy efficient federated learning over wireless communication networks. Z Yang, M Chen, W Saad, C S Hong, M Shikh-Bahaei, Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, \"Energy efficient federated learning over wireless communication networks.\" [Online]. Available: https://arxiv.org/abs/1911.02417\n\nJoint offloading and computing optimization in wireless powered mobile-edge computing systems. F Wang, J Xu, X Wang, S Cui, IEEE Trans. Wireless Commun. 173F. Wang, J. Xu, X. Wang, and S. Cui, \"Joint offloading and computing optimization in wireless powered mobile-edge computing systems,\" IEEE Trans. Wireless Commun., vol. 17, no. 3, pp. 1784-1797, Mar. 2018.\n\nEnergy-efficient resource allocation for mobile-edge computation offloading. C You, K Huang, H Chae, B Kim, IEEE Trans. Wireless Commun. 163C. You, K. Huang, H. Chae, and B. Kim, \"Energy-efficient resource allocation for mobile-edge computation offloading,\" IEEE Trans. Wireless Commun., vol. 16, no. 3, pp. 1397-1411, Mar. 2017.\n\nMulti-antenna NOMA for computation offloading in multiuser mobile edge computing systems. F Wang, J Xu, Z Ding, IEEE Trans. Commun. 673F. Wang, J. Xu, and Z. Ding, \"Multi-antenna NOMA for computation offloading in multiuser mobile edge computing systems,\" IEEE Trans. Commun., vol. 67, no. 3, pp. 2450-2463, Mar. 2019.\n\nJoint computation and communication cooperation for energy-efficient mobile edge computing. X Cao, F Wang, J Xu, R Zhang, S Cui, IEEE Internet Things J. 63X. Cao, F. Wang, J. Xu, R. Zhang, and S. Cui, \"Joint computation and communication cooperation for energy-efficient mobile edge computing,\" IEEE Internet Things J., vol. 6, no. 3, pp. 4188-4200, Jun. 2019.\n\nJoint communication and computation optimization for wireless powered mobile edge computing with D2D offloading. D Wu, F Wang, X Cao, J Xu, Journal of Communications and Information Networks. 44D. Wu, F. Wang, X. Cao, and J. Xu, \"Joint communication and computation optimization for wireless powered mobile edge computing with D2D offloading,\" Journal of Communications and Information Networks, vol. 4, no. 4, pp. 72-86, Dec. 2019.\n\nOptimized transmission for fading multiple-access and broadcast channels with multiple antennas. M Mohseni, R Zhang, J M Cioffi, IEEE J. Sel. Areas Commun. 248M. Mohseni, R. Zhang, and J. M. Cioffi, \"Optimized transmission for fading multiple-access and broadcast channels with multiple antennas,\" IEEE J. Sel. Areas Commun., vol. 24, no. 8, pp. 1627-1639, Aug. 2006.\n\nUnderstanding machine learning: From theory to algorithms. S Shalev-Shwartz, S Ben-David, Cambridge Univ. PressCambridge, U.K.S. Shalev-Shwartz and S. Ben-David, Understanding machine learning: From theory to algorithms. Cambridge, U.K.: Cambridge Univ. Press, 2014.\n\nI Goodfellow, Y Bengio, A Courville, Deep Learning. Cambridge, MA, USAMIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016. [Online]. Available: http://www.deeplearningbook.org\n\nEnergy-optimal mobile cloud computing under stochastic wireless channel. W Zhang, Y Wen, K Guan, D Kilper, H Luo, D O Wu, IEEE Trans. Wireless Commun. 129W. Zhang, Y. Wen, K. Guan, D. Kilper, H. Luo, and D. O. Wu, \"Energy-optimal mobile cloud computing under stochastic wireless channel,\" IEEE Trans. Wireless Commun., vol. 12, no. 9, pp. 4569-4581, Sep. 2013.\n\nMobile edge computing: A survey on architecture and computation offloading. P Mach, Z Becvar, IEEE Commun. Surv. Tuts. 193P. Mach and Z. Becvar, \"Mobile edge computing: A survey on architecture and computation offloading,\" IEEE Commun. Surv. Tuts., vol. 19, no. 3, pp. 1628-1656, Mar. 2017.\n\nMemory usage and computational considerations, Course Notes. X Giro-I-Nieto, E Sayrol, A Salvador, J Torres, E Mohedano, K Mcguinness, X. Giro-i-Nieto, E. Sayrol, A. Salvador, J. Torres, E. Mohedano, and K. McGuinness, Memory usage and computational considerations, Course Notes, accessed on Jul. 5 2016. [Online]. Available: http://imatge-upc.github.io/telecombcn-2016-dlcv/slides/D2L1-memory.pdf\n\nConvex optimization: Algorithms and complexity. S Bubeck, Found. Trends Mach. Learn. 83S. Bubeck, \"Convex optimization: Algorithms and complexity,\" Found. Trends Mach. Learn., vol. 8, no. 3, pp. 231-357, Nov. 2015.\n\nD Tse, P Viswanath, Fundamentals of Wireless Communication. Cambridge, U.K.Cambridge Univ. PressD. Tse and P. Viswanath, Fundamentals of Wireless Communication. Cambridge, U.K.: Cambridge Univ. Press, 2005.\n\nMultiaccess fading channels. I. Polymatroid structure, optimal resource allocation and throughput capacities. D N C Tse, S V Hanly, IEEE Trans. Inf. Theory. 227D. N. C. Tse and S. V. Hanly, \"Multiaccess fading channels. I. Polymatroid structure, optimal resource allocation and throughput capacities,\" IEEE Trans. Inf. Theory, vol. 22, no. 7, pp. 2796-2815, Nov. 1998.\n\nDual methods for nonconvex spectrum optimization of multicarrier systems. W Yu, R Lui, IEEE Trans. Commun. 547W. Yu and R. Lui, \"Dual methods for nonconvex spectrum optimization of multicarrier systems,\" IEEE Trans. Commun., vol. 54, no. 7, pp. 1310-1322, Jul. 2006.\n\nMulti-antenna NOMA for computation offloading in multiuser mobile edge computing systems. F Wang, J Xu, Z Ding, IEEE Trans. Commun. 673F. Wang, J. Xu and Z. Ding, \"Multi-antenna NOMA for computation offloading in multiuser mobile edge computing systems,\" IEEE Trans. Commun., vol. 67, no. 3, pp. 2450-2463, Mar. 2019.\n\nS Boyd, L Vandenberghe, Convex Optimization. Cambridge, U.K.Cambridge Univ. PressS. Boyd and L. Vandenberghe, Convex Optimization, Cambridge, U.K.:Cambridge Univ. Press, Mar. 2004.\n\nCVX: MATLAB Software for Disciplined Convex Programming. M Grant, S Boyd, Y Ye, M. Grant, S. Boyd, and Y. Ye. (2009). CVX: MATLAB Software for Disciplined Convex Programming. [Online]. Available: http://cvxr.com/cvx/\n\nEE364b Convex Optimization II, Course Notes. S Boyd, S. Boyd. EE364b Convex Optimization II, Course Notes, accessed on Jun. 29 2017. [Online]. Available: http://www.stanford.edu/class/ee364b/\n", "annotations": {"author": "[{\"end\":145,\"start\":103},{\"end\":202,\"start\":146},{\"end\":208,\"start\":203},{\"end\":341,\"start\":209},{\"end\":491,\"start\":342},{\"end\":559,\"start\":492}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":112},{\"end\":152,\"start\":150},{\"end\":207,\"start\":205}]", "author_first_name": "[{\"end\":111,\"start\":103},{\"end\":149,\"start\":146},{\"end\":204,\"start\":203}]", "author_affiliation": "[{\"end\":340,\"start\":210},{\"end\":490,\"start\":343},{\"end\":558,\"start\":493}]", "title": "[{\"end\":89,\"start\":1},{\"end\":648,\"start\":560}]", "venue": null, "abstract": "[{\"end\":8704,\"start\":662}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8727,\"start\":8723},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8733,\"start\":8729},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8887,\"start\":8883},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11418,\"start\":11414},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11423,\"start\":11419},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11587,\"start\":11583},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11752,\"start\":11748},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11935,\"start\":11931},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12452,\"start\":12448},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12458,\"start\":12454},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14319,\"start\":14318},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17584,\"start\":17580},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18191,\"start\":18188},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18401,\"start\":18397},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18511,\"start\":18507},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19173,\"start\":19172},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19872,\"start\":19868},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20810,\"start\":20806},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20820,\"start\":20816},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21401,\"start\":21397},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21407,\"start\":21403},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21768,\"start\":21764},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21774,\"start\":21770},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24659,\"start\":24655},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24666,\"start\":24662},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26126,\"start\":26122},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26663,\"start\":26659},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26891,\"start\":26887},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27698,\"start\":27694},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27716,\"start\":27712},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28347,\"start\":28343},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29640,\"start\":29636},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30132,\"start\":30128},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30504,\"start\":30500},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30858,\"start\":30854},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31977,\"start\":31973},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32299,\"start\":32295},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32373,\"start\":32369},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32566,\"start\":32562},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32806,\"start\":32802},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32863,\"start\":32859},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33171,\"start\":33168},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33543,\"start\":33539},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34013,\"start\":34009},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34016,\"start\":34015},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34112,\"start\":34108},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34191,\"start\":34187},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34538,\"start\":34534},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35142,\"start\":35138},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35233,\"start\":35229},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35685,\"start\":35681},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36147,\"start\":36143},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36152,\"start\":36148},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36253,\"start\":36249},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36258,\"start\":36254},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36681,\"start\":36677},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37062,\"start\":37058},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":39999,\"start\":39995},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41298,\"start\":41297},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41345,\"start\":41341},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41379,\"start\":41375},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41528,\"start\":41524},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41531,\"start\":41528},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41540,\"start\":41536},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":41543,\"start\":41540},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":43083,\"start\":43079},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":43092,\"start\":43088},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45685,\"start\":45681},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45768,\"start\":45764},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45967,\"start\":45963},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46201,\"start\":46197},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":46246,\"start\":46242},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54015,\"start\":54011},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":54020,\"start\":54016},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":54307,\"start\":54303}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46694,\"start\":46550},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47930,\"start\":46695},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48497,\"start\":47931},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48877,\"start\":48498},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48898,\"start\":48878},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49002,\"start\":48899},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49029,\"start\":49003},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49238,\"start\":49030},{\"attributes\":{\"id\":\"fig_10\"},\"end\":49814,\"start\":49239},{\"attributes\":{\"id\":\"fig_11\"},\"end\":51072,\"start\":49815},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51471,\"start\":51073},{\"attributes\":{\"id\":\"fig_13\"},\"end\":52258,\"start\":51472},{\"attributes\":{\"id\":\"fig_14\"},\"end\":52551,\"start\":52259},{\"attributes\":{\"id\":\"fig_15\"},\"end\":52677,\"start\":52552},{\"attributes\":{\"id\":\"fig_16\"},\"end\":53191,\"start\":52678},{\"attributes\":{\"id\":\"fig_17\"},\"end\":53352,\"start\":53192},{\"attributes\":{\"id\":\"fig_18\"},\"end\":53648,\"start\":53353},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":53995,\"start\":53649}]", "paragraph": "[{\"end\":10156,\"start\":8706},{\"end\":10864,\"start\":10158},{\"end\":11347,\"start\":10866},{\"end\":12804,\"start\":11349},{\"end\":13382,\"start\":12806},{\"end\":14225,\"start\":13403},{\"end\":14392,\"start\":14261},{\"end\":14554,\"start\":14427},{\"end\":14730,\"start\":14601},{\"end\":15518,\"start\":14757},{\"end\":15750,\"start\":15581},{\"end\":16459,\"start\":15779},{\"end\":16749,\"start\":16506},{\"end\":16970,\"start\":16799},{\"end\":17825,\"start\":17016},{\"end\":18011,\"start\":17827},{\"end\":18512,\"start\":18041},{\"end\":18641,\"start\":18546},{\"end\":18801,\"start\":18706},{\"end\":19903,\"start\":18803},{\"end\":20350,\"start\":19935},{\"end\":20680,\"start\":20381},{\"end\":21413,\"start\":20682},{\"end\":21775,\"start\":21488},{\"end\":21870,\"start\":21865},{\"end\":22237,\"start\":21931},{\"end\":22467,\"start\":22263},{\"end\":22704,\"start\":22497},{\"end\":22919,\"start\":22739},{\"end\":23322,\"start\":22921},{\"end\":23465,\"start\":23358},{\"end\":23677,\"start\":23514},{\"end\":23999,\"start\":23747},{\"end\":24497,\"start\":24242},{\"end\":24980,\"start\":24499},{\"end\":25753,\"start\":25151},{\"end\":26048,\"start\":25798},{\"end\":26127,\"start\":26093},{\"end\":26340,\"start\":26129},{\"end\":26600,\"start\":26378},{\"end\":26664,\"start\":26622},{\"end\":26892,\"start\":26666},{\"end\":26929,\"start\":26908},{\"end\":27153,\"start\":26931},{\"end\":27387,\"start\":27213},{\"end\":27630,\"start\":27432},{\"end\":27717,\"start\":27689},{\"end\":28004,\"start\":27719},{\"end\":28205,\"start\":28090},{\"end\":28849,\"start\":28221},{\"end\":29114,\"start\":28901},{\"end\":29487,\"start\":29169},{\"end\":29647,\"start\":29630},{\"end\":30317,\"start\":29759},{\"end\":30790,\"start\":30367},{\"end\":30912,\"start\":30792},{\"end\":31130,\"start\":31087},{\"end\":31392,\"start\":31289},{\"end\":31577,\"start\":31490},{\"end\":31621,\"start\":31579},{\"end\":31925,\"start\":31739},{\"end\":32074,\"start\":31927},{\"end\":32231,\"start\":32189},{\"end\":32374,\"start\":32233},{\"end\":32463,\"start\":32389},{\"end\":32909,\"start\":32538},{\"end\":33172,\"start\":32911},{\"end\":33272,\"start\":33174},{\"end\":33424,\"start\":33274},{\"end\":33657,\"start\":33530},{\"end\":33847,\"start\":33796},{\"end\":34192,\"start\":33849},{\"end\":34620,\"start\":34194},{\"end\":35296,\"start\":34813},{\"end\":36271,\"start\":35322},{\"end\":36682,\"start\":36322},{\"end\":37190,\"start\":36856},{\"end\":37391,\"start\":37342},{\"end\":37694,\"start\":37469},{\"end\":37965,\"start\":37787},{\"end\":38117,\"start\":37967},{\"end\":38630,\"start\":38273},{\"end\":38811,\"start\":38682},{\"end\":38914,\"start\":38906},{\"end\":39051,\"start\":38916},{\"end\":39302,\"start\":39117},{\"end\":39360,\"start\":39351},{\"end\":39516,\"start\":39362},{\"end\":39700,\"start\":39518},{\"end\":40041,\"start\":39702},{\"end\":40279,\"start\":40086},{\"end\":41559,\"start\":40305},{\"end\":42849,\"start\":41561},{\"end\":44368,\"start\":42852},{\"end\":45790,\"start\":44388},{\"end\":45979,\"start\":45792},{\"end\":46266,\"start\":46084},{\"end\":46549,\"start\":46446}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14260,\"start\":14226},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14426,\"start\":14393},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14600,\"start\":14555},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14756,\"start\":14731},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15778,\"start\":15751},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16505,\"start\":16460},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17015,\"start\":16971},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18040,\"start\":18012},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18545,\"start\":18513},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20380,\"start\":20351},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21487,\"start\":21414},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21864,\"start\":21776},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21930,\"start\":21871},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22262,\"start\":22238},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22496,\"start\":22468},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22738,\"start\":22705},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23357,\"start\":23323},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23513,\"start\":23466},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23719,\"start\":23678},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24089,\"start\":24000},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24241,\"start\":24089},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25150,\"start\":24981},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26092,\"start\":26049},{\"attributes\":{\"id\":\"formula_23\"},\"end\":26377,\"start\":26341},{\"attributes\":{\"id\":\"formula_24\"},\"end\":26621,\"start\":26601},{\"attributes\":{\"id\":\"formula_25\"},\"end\":26907,\"start\":26893},{\"attributes\":{\"id\":\"formula_26\"},\"end\":27212,\"start\":27154},{\"attributes\":{\"id\":\"formula_27\"},\"end\":27688,\"start\":27631},{\"attributes\":{\"id\":\"formula_28\"},\"end\":28089,\"start\":28005},{\"attributes\":{\"id\":\"formula_29\"},\"end\":29575,\"start\":29488},{\"attributes\":{\"id\":\"formula_30\"},\"end\":29629,\"start\":29575},{\"attributes\":{\"id\":\"formula_31\"},\"end\":29758,\"start\":29648},{\"attributes\":{\"id\":\"formula_32\"},\"end\":31086,\"start\":30913},{\"attributes\":{\"id\":\"formula_33\"},\"end\":31288,\"start\":31131},{\"attributes\":{\"id\":\"formula_34\"},\"end\":31489,\"start\":31393},{\"attributes\":{\"id\":\"formula_35\"},\"end\":31738,\"start\":31622},{\"attributes\":{\"id\":\"formula_36\"},\"end\":32188,\"start\":32075},{\"attributes\":{\"id\":\"formula_37\"},\"end\":32508,\"start\":32464},{\"attributes\":{\"id\":\"formula_40\"},\"end\":33529,\"start\":33425},{\"attributes\":{\"id\":\"formula_41\"},\"end\":33795,\"start\":33658},{\"attributes\":{\"id\":\"formula_42\"},\"end\":34751,\"start\":34621},{\"attributes\":{\"id\":\"formula_43\"},\"end\":35321,\"start\":35297},{\"attributes\":{\"id\":\"formula_44\"},\"end\":36855,\"start\":36683},{\"attributes\":{\"id\":\"formula_45\"},\"end\":37341,\"start\":37191},{\"attributes\":{\"id\":\"formula_46\"},\"end\":37468,\"start\":37392},{\"attributes\":{\"id\":\"formula_47\"},\"end\":37786,\"start\":37695},{\"attributes\":{\"id\":\"formula_48\"},\"end\":38230,\"start\":38118},{\"attributes\":{\"id\":\"formula_49\"},\"end\":38272,\"start\":38230},{\"attributes\":{\"id\":\"formula_50\"},\"end\":38681,\"start\":38631},{\"attributes\":{\"id\":\"formula_51\"},\"end\":38867,\"start\":38812},{\"attributes\":{\"id\":\"formula_52\"},\"end\":38905,\"start\":38867},{\"attributes\":{\"id\":\"formula_53\"},\"end\":39116,\"start\":39052},{\"attributes\":{\"id\":\"formula_54\"},\"end\":39350,\"start\":39303},{\"attributes\":{\"id\":\"formula_56\"},\"end\":40085,\"start\":40042},{\"attributes\":{\"id\":\"formula_57\"},\"end\":42851,\"start\":42850},{\"attributes\":{\"id\":\"formula_58\"},\"end\":46083,\"start\":45980},{\"attributes\":{\"id\":\"formula_59\"},\"end\":46445,\"start\":46267}]", "table_ref": null, "section_header": "[{\"end\":13401,\"start\":13385},{\"end\":15579,\"start\":15521},{\"end\":16797,\"start\":16752},{\"end\":18704,\"start\":18644},{\"end\":19933,\"start\":19906},{\"end\":23745,\"start\":23721},{\"end\":25796,\"start\":25756},{\"end\":27430,\"start\":27390},{\"end\":28219,\"start\":28208},{\"end\":28899,\"start\":28852},{\"end\":29167,\"start\":29117},{\"end\":30365,\"start\":30320},{\"end\":32387,\"start\":32377},{\"end\":32536,\"start\":32510},{\"end\":34811,\"start\":34753},{\"end\":36320,\"start\":36274},{\"end\":40303,\"start\":40282},{\"end\":44386,\"start\":44371},{\"end\":46559,\"start\":46551},{\"end\":48913,\"start\":48900},{\"end\":49037,\"start\":49031},{\"end\":49256,\"start\":49240},{\"end\":49824,\"start\":49816},{\"end\":51080,\"start\":51074},{\"end\":52266,\"start\":52260},{\"end\":52561,\"start\":52553},{\"end\":52687,\"start\":52679},{\"end\":53199,\"start\":53193},{\"end\":53369,\"start\":53354}]", "table": "[{\"end\":53995,\"start\":53783}]", "figure_caption": "[{\"end\":46694,\"start\":46561},{\"end\":47930,\"start\":46697},{\"end\":48497,\"start\":47933},{\"end\":48877,\"start\":48500},{\"end\":48898,\"start\":48880},{\"end\":49002,\"start\":48916},{\"end\":49029,\"start\":49005},{\"end\":49238,\"start\":49039},{\"end\":49814,\"start\":49259},{\"end\":51072,\"start\":49826},{\"end\":51471,\"start\":51082},{\"end\":52258,\"start\":51474},{\"end\":52551,\"start\":52268},{\"end\":52677,\"start\":52563},{\"end\":53191,\"start\":52689},{\"end\":53352,\"start\":53201},{\"end\":53648,\"start\":53372},{\"end\":53783,\"start\":53651}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13550,\"start\":13544}]", "bib_author_first_name": "[{\"end\":54560,\"start\":54559},{\"end\":54569,\"start\":54568},{\"end\":54579,\"start\":54578},{\"end\":54819,\"start\":54818},{\"end\":54821,\"start\":54820},{\"end\":54834,\"start\":54833},{\"end\":54842,\"start\":54841},{\"end\":54855,\"start\":54854},{\"end\":54862,\"start\":54861},{\"end\":55204,\"start\":55203},{\"end\":55211,\"start\":55210},{\"end\":55218,\"start\":55217},{\"end\":55227,\"start\":55226},{\"end\":55236,\"start\":55235},{\"end\":55238,\"start\":55237},{\"end\":55565,\"start\":55564},{\"end\":55571,\"start\":55570},{\"end\":55579,\"start\":55578},{\"end\":55587,\"start\":55586},{\"end\":55909,\"start\":55908},{\"end\":55917,\"start\":55916},{\"end\":55924,\"start\":55923},{\"end\":55932,\"start\":55931},{\"end\":55940,\"start\":55939},{\"end\":55948,\"start\":55947},{\"end\":56259,\"start\":56258},{\"end\":56266,\"start\":56265},{\"end\":56273,\"start\":56272},{\"end\":56279,\"start\":56278},{\"end\":56286,\"start\":56285},{\"end\":56295,\"start\":56294},{\"end\":56596,\"start\":56595},{\"end\":56603,\"start\":56602},{\"end\":56609,\"start\":56608},{\"end\":56617,\"start\":56616},{\"end\":56624,\"start\":56623},{\"end\":56965,\"start\":56964},{\"end\":56967,\"start\":56966},{\"end\":56978,\"start\":56977},{\"end\":56986,\"start\":56985},{\"end\":56993,\"start\":56992},{\"end\":57003,\"start\":57001},{\"end\":57005,\"start\":57004},{\"end\":57252,\"start\":57251},{\"end\":57263,\"start\":57262},{\"end\":57274,\"start\":57273},{\"end\":57276,\"start\":57275},{\"end\":57282,\"start\":57281},{\"end\":57295,\"start\":57294},{\"end\":57297,\"start\":57296},{\"end\":57307,\"start\":57306},{\"end\":57576,\"start\":57575},{\"end\":57584,\"start\":57583},{\"end\":57591,\"start\":57590},{\"end\":57599,\"start\":57598},{\"end\":57886,\"start\":57885},{\"end\":57888,\"start\":57887},{\"end\":57899,\"start\":57898},{\"end\":57908,\"start\":57907},{\"end\":57918,\"start\":57917},{\"end\":57929,\"start\":57928},{\"end\":57931,\"start\":57930},{\"end\":58187,\"start\":58186},{\"end\":58195,\"start\":58194},{\"end\":58202,\"start\":58201},{\"end\":58213,\"start\":58212},{\"end\":58226,\"start\":58225},{\"end\":58238,\"start\":58237},{\"end\":58252,\"start\":58251},{\"end\":58263,\"start\":58262},{\"end\":58273,\"start\":58272},{\"end\":58501,\"start\":58500},{\"end\":58503,\"start\":58502},{\"end\":58797,\"start\":58796},{\"end\":58804,\"start\":58803},{\"end\":58811,\"start\":58810},{\"end\":58818,\"start\":58817},{\"end\":58826,\"start\":58825},{\"end\":58828,\"start\":58827},{\"end\":59107,\"start\":59106},{\"end\":59118,\"start\":59117},{\"end\":59131,\"start\":59130},{\"end\":59133,\"start\":59132},{\"end\":59143,\"start\":59142},{\"end\":59412,\"start\":59411},{\"end\":59420,\"start\":59419},{\"end\":59428,\"start\":59427},{\"end\":59441,\"start\":59440},{\"end\":59443,\"start\":59442},{\"end\":59452,\"start\":59451},{\"end\":59462,\"start\":59461},{\"end\":59468,\"start\":59467},{\"end\":59816,\"start\":59815},{\"end\":59826,\"start\":59825},{\"end\":60081,\"start\":60080},{\"end\":60088,\"start\":60087},{\"end\":60096,\"start\":60095},{\"end\":60360,\"start\":60359},{\"end\":60368,\"start\":60367},{\"end\":60377,\"start\":60376},{\"end\":60384,\"start\":60383},{\"end\":60631,\"start\":60630},{\"end\":60633,\"start\":60632},{\"end\":60642,\"start\":60641},{\"end\":60894,\"start\":60893},{\"end\":60901,\"start\":60900},{\"end\":60910,\"start\":60909},{\"end\":60912,\"start\":60911},{\"end\":60920,\"start\":60919},{\"end\":60922,\"start\":60921},{\"end\":61127,\"start\":61126},{\"end\":61134,\"start\":61133},{\"end\":61398,\"start\":61397},{\"end\":61405,\"start\":61404},{\"end\":61412,\"start\":61411},{\"end\":61418,\"start\":61417},{\"end\":61620,\"start\":61619},{\"end\":61628,\"start\":61627},{\"end\":61634,\"start\":61633},{\"end\":61636,\"start\":61635},{\"end\":61645,\"start\":61644},{\"end\":61900,\"start\":61899},{\"end\":61908,\"start\":61907},{\"end\":61916,\"start\":61915},{\"end\":61924,\"start\":61923},{\"end\":61926,\"start\":61925},{\"end\":61934,\"start\":61933},{\"end\":62236,\"start\":62235},{\"end\":62244,\"start\":62243},{\"end\":62250,\"start\":62249},{\"end\":62258,\"start\":62257},{\"end\":62581,\"start\":62580},{\"end\":62588,\"start\":62587},{\"end\":62597,\"start\":62596},{\"end\":62605,\"start\":62604},{\"end\":62925,\"start\":62924},{\"end\":62933,\"start\":62932},{\"end\":62939,\"start\":62938},{\"end\":63247,\"start\":63246},{\"end\":63254,\"start\":63253},{\"end\":63262,\"start\":63261},{\"end\":63268,\"start\":63267},{\"end\":63277,\"start\":63276},{\"end\":63630,\"start\":63629},{\"end\":63636,\"start\":63635},{\"end\":63644,\"start\":63643},{\"end\":63651,\"start\":63650},{\"end\":64048,\"start\":64047},{\"end\":64059,\"start\":64058},{\"end\":64068,\"start\":64067},{\"end\":64070,\"start\":64069},{\"end\":64379,\"start\":64378},{\"end\":64397,\"start\":64396},{\"end\":64588,\"start\":64587},{\"end\":64602,\"start\":64601},{\"end\":64612,\"start\":64611},{\"end\":64890,\"start\":64889},{\"end\":64899,\"start\":64898},{\"end\":64906,\"start\":64905},{\"end\":64914,\"start\":64913},{\"end\":64924,\"start\":64923},{\"end\":64931,\"start\":64930},{\"end\":64933,\"start\":64932},{\"end\":65255,\"start\":65254},{\"end\":65263,\"start\":65262},{\"end\":65532,\"start\":65531},{\"end\":65548,\"start\":65547},{\"end\":65558,\"start\":65557},{\"end\":65570,\"start\":65569},{\"end\":65580,\"start\":65579},{\"end\":65592,\"start\":65591},{\"end\":65918,\"start\":65917},{\"end\":66086,\"start\":66085},{\"end\":66093,\"start\":66092},{\"end\":66404,\"start\":66403},{\"end\":66408,\"start\":66405},{\"end\":66415,\"start\":66414},{\"end\":66417,\"start\":66416},{\"end\":66738,\"start\":66737},{\"end\":66744,\"start\":66743},{\"end\":67022,\"start\":67021},{\"end\":67030,\"start\":67029},{\"end\":67036,\"start\":67035},{\"end\":67251,\"start\":67250},{\"end\":67259,\"start\":67258},{\"end\":67490,\"start\":67489},{\"end\":67499,\"start\":67498},{\"end\":67507,\"start\":67506},{\"end\":67696,\"start\":67695}]", "bib_author_last_name": "[{\"end\":54566,\"start\":54561},{\"end\":54576,\"start\":54570},{\"end\":54586,\"start\":54580},{\"end\":54831,\"start\":54822},{\"end\":54839,\"start\":54835},{\"end\":54852,\"start\":54843},{\"end\":54859,\"start\":54856},{\"end\":54867,\"start\":54863},{\"end\":55208,\"start\":55205},{\"end\":55215,\"start\":55212},{\"end\":55224,\"start\":55219},{\"end\":55233,\"start\":55228},{\"end\":55246,\"start\":55239},{\"end\":55568,\"start\":55566},{\"end\":55576,\"start\":55572},{\"end\":55584,\"start\":55580},{\"end\":55592,\"start\":55588},{\"end\":55914,\"start\":55910},{\"end\":55921,\"start\":55918},{\"end\":55929,\"start\":55925},{\"end\":55937,\"start\":55933},{\"end\":55945,\"start\":55941},{\"end\":55953,\"start\":55949},{\"end\":56263,\"start\":56260},{\"end\":56270,\"start\":56267},{\"end\":56276,\"start\":56274},{\"end\":56283,\"start\":56280},{\"end\":56292,\"start\":56287},{\"end\":56301,\"start\":56296},{\"end\":56600,\"start\":56597},{\"end\":56606,\"start\":56604},{\"end\":56614,\"start\":56610},{\"end\":56621,\"start\":56618},{\"end\":56630,\"start\":56625},{\"end\":56975,\"start\":56968},{\"end\":56983,\"start\":56979},{\"end\":56990,\"start\":56987},{\"end\":56999,\"start\":56994},{\"end\":57011,\"start\":57006},{\"end\":57260,\"start\":57253},{\"end\":57271,\"start\":57264},{\"end\":57279,\"start\":57277},{\"end\":57292,\"start\":57283},{\"end\":57304,\"start\":57298},{\"end\":57313,\"start\":57308},{\"end\":57581,\"start\":57577},{\"end\":57588,\"start\":57585},{\"end\":57596,\"start\":57592},{\"end\":57604,\"start\":57600},{\"end\":57896,\"start\":57889},{\"end\":57905,\"start\":57900},{\"end\":57915,\"start\":57909},{\"end\":57926,\"start\":57919},{\"end\":57937,\"start\":57932},{\"end\":58192,\"start\":58188},{\"end\":58199,\"start\":58196},{\"end\":58210,\"start\":58203},{\"end\":58223,\"start\":58214},{\"end\":58235,\"start\":58227},{\"end\":58249,\"start\":58239},{\"end\":58260,\"start\":58253},{\"end\":58270,\"start\":58264},{\"end\":58280,\"start\":58274},{\"end\":58510,\"start\":58504},{\"end\":58517,\"start\":58512},{\"end\":58801,\"start\":58798},{\"end\":58808,\"start\":58805},{\"end\":58815,\"start\":58812},{\"end\":58823,\"start\":58819},{\"end\":58834,\"start\":58829},{\"end\":59115,\"start\":59108},{\"end\":59128,\"start\":59119},{\"end\":59140,\"start\":59134},{\"end\":59149,\"start\":59144},{\"end\":59417,\"start\":59413},{\"end\":59425,\"start\":59421},{\"end\":59438,\"start\":59429},{\"end\":59449,\"start\":59444},{\"end\":59459,\"start\":59453},{\"end\":59465,\"start\":59463},{\"end\":59473,\"start\":59469},{\"end\":59823,\"start\":59817},{\"end\":59835,\"start\":59827},{\"end\":60085,\"start\":60082},{\"end\":60093,\"start\":60089},{\"end\":60102,\"start\":60097},{\"end\":60365,\"start\":60361},{\"end\":60374,\"start\":60369},{\"end\":60381,\"start\":60378},{\"end\":60389,\"start\":60385},{\"end\":60639,\"start\":60634},{\"end\":60649,\"start\":60643},{\"end\":60898,\"start\":60895},{\"end\":60907,\"start\":60902},{\"end\":60917,\"start\":60913},{\"end\":60930,\"start\":60923},{\"end\":61131,\"start\":61128},{\"end\":61140,\"start\":61135},{\"end\":61402,\"start\":61399},{\"end\":61409,\"start\":61406},{\"end\":61415,\"start\":61413},{\"end\":61424,\"start\":61419},{\"end\":61625,\"start\":61621},{\"end\":61631,\"start\":61629},{\"end\":61642,\"start\":61637},{\"end\":61651,\"start\":61646},{\"end\":61905,\"start\":61901},{\"end\":61913,\"start\":61909},{\"end\":61921,\"start\":61917},{\"end\":61931,\"start\":61927},{\"end\":61947,\"start\":61935},{\"end\":62241,\"start\":62237},{\"end\":62247,\"start\":62245},{\"end\":62255,\"start\":62251},{\"end\":62262,\"start\":62259},{\"end\":62585,\"start\":62582},{\"end\":62594,\"start\":62589},{\"end\":62602,\"start\":62598},{\"end\":62609,\"start\":62606},{\"end\":62930,\"start\":62926},{\"end\":62936,\"start\":62934},{\"end\":62944,\"start\":62940},{\"end\":63251,\"start\":63248},{\"end\":63259,\"start\":63255},{\"end\":63265,\"start\":63263},{\"end\":63274,\"start\":63269},{\"end\":63281,\"start\":63278},{\"end\":63633,\"start\":63631},{\"end\":63641,\"start\":63637},{\"end\":63648,\"start\":63645},{\"end\":63654,\"start\":63652},{\"end\":64056,\"start\":64049},{\"end\":64065,\"start\":64060},{\"end\":64077,\"start\":64071},{\"end\":64394,\"start\":64380},{\"end\":64407,\"start\":64398},{\"end\":64599,\"start\":64589},{\"end\":64609,\"start\":64603},{\"end\":64622,\"start\":64613},{\"end\":64896,\"start\":64891},{\"end\":64903,\"start\":64900},{\"end\":64911,\"start\":64907},{\"end\":64921,\"start\":64915},{\"end\":64928,\"start\":64925},{\"end\":64936,\"start\":64934},{\"end\":65260,\"start\":65256},{\"end\":65270,\"start\":65264},{\"end\":65545,\"start\":65533},{\"end\":65555,\"start\":65549},{\"end\":65567,\"start\":65559},{\"end\":65577,\"start\":65571},{\"end\":65589,\"start\":65581},{\"end\":65603,\"start\":65593},{\"end\":65925,\"start\":65919},{\"end\":66090,\"start\":66087},{\"end\":66103,\"start\":66094},{\"end\":66412,\"start\":66409},{\"end\":66423,\"start\":66418},{\"end\":66741,\"start\":66739},{\"end\":66748,\"start\":66745},{\"end\":67027,\"start\":67023},{\"end\":67033,\"start\":67031},{\"end\":67041,\"start\":67037},{\"end\":67256,\"start\":67252},{\"end\":67272,\"start\":67260},{\"end\":67496,\"start\":67491},{\"end\":67504,\"start\":67500},{\"end\":67510,\"start\":67508},{\"end\":67701,\"start\":67697}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1779661},\"end\":54692,\"start\":54544},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":204082476},\"end\":55135,\"start\":54694},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206578365},\"end\":55480,\"start\":55137},{\"attributes\":{\"id\":\"b3\"},\"end\":55805,\"start\":55482},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52343892},\"end\":56180,\"start\":55807},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52155776},\"end\":56503,\"start\":56182},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207852684},\"end\":56911,\"start\":56505},{\"attributes\":{\"id\":\"b7\"},\"end\":57178,\"start\":56913},{\"attributes\":{\"id\":\"b8\"},\"end\":57519,\"start\":57180},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219878182},\"end\":57808,\"start\":57521},{\"attributes\":{\"id\":\"b10\"},\"end\":58134,\"start\":57810},{\"attributes\":{\"id\":\"b11\"},\"end\":58496,\"start\":58136},{\"attributes\":{\"id\":\"b12\"},\"end\":58704,\"start\":58498},{\"attributes\":{\"id\":\"b13\"},\"end\":59031,\"start\":58706},{\"attributes\":{\"id\":\"b14\"},\"end\":59333,\"start\":59033},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":51921962},\"end\":59728,\"start\":59335},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5062760},\"end\":60008,\"start\":59730},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":58004591},\"end\":60308,\"start\":60010},{\"attributes\":{\"id\":\"b18\"},\"end\":60535,\"start\":60310},{\"attributes\":{\"id\":\"b19\"},\"end\":60826,\"start\":60537},{\"attributes\":{\"id\":\"b20\"},\"end\":61056,\"start\":60828},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49667318},\"end\":61324,\"start\":61058},{\"attributes\":{\"id\":\"b22\"},\"end\":61545,\"start\":61326},{\"attributes\":{\"id\":\"b23\"},\"end\":61823,\"start\":61547},{\"attributes\":{\"id\":\"b24\"},\"end\":62138,\"start\":61825},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2172141},\"end\":62501,\"start\":62140},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14764353},\"end\":62832,\"start\":62503},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":21473252},\"end\":63152,\"start\":62834},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52948198},\"end\":63514,\"start\":63154},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":207757136},\"end\":63948,\"start\":63516},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13069285},\"end\":64317,\"start\":63950},{\"attributes\":{\"id\":\"b31\"},\"end\":64585,\"start\":64319},{\"attributes\":{\"id\":\"b32\"},\"end\":64814,\"start\":64587},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15304804},\"end\":65176,\"start\":64816},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6909107},\"end\":65468,\"start\":65178},{\"attributes\":{\"id\":\"b35\"},\"end\":65867,\"start\":65470},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":207179138},\"end\":66083,\"start\":65869},{\"attributes\":{\"id\":\"b37\"},\"end\":66291,\"start\":66085},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":606437},\"end\":66661,\"start\":66293},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9856587},\"end\":66929,\"start\":66663},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":21473252},\"end\":67248,\"start\":66931},{\"attributes\":{\"id\":\"b41\"},\"end\":67430,\"start\":67250},{\"attributes\":{\"id\":\"b42\"},\"end\":67648,\"start\":67432},{\"attributes\":{\"id\":\"b43\"},\"end\":67841,\"start\":67650}]", "bib_title": "[{\"end\":54557,\"start\":54544},{\"end\":54816,\"start\":54694},{\"end\":55201,\"start\":55137},{\"end\":55906,\"start\":55807},{\"end\":56256,\"start\":56182},{\"end\":56593,\"start\":56505},{\"end\":57573,\"start\":57521},{\"end\":59409,\"start\":59335},{\"end\":59813,\"start\":59730},{\"end\":60078,\"start\":60010},{\"end\":61124,\"start\":61058},{\"end\":62233,\"start\":62140},{\"end\":62578,\"start\":62503},{\"end\":62922,\"start\":62834},{\"end\":63244,\"start\":63154},{\"end\":63627,\"start\":63516},{\"end\":64045,\"start\":63950},{\"end\":64887,\"start\":64816},{\"end\":65252,\"start\":65178},{\"end\":65915,\"start\":65869},{\"end\":66401,\"start\":66293},{\"end\":66735,\"start\":66663},{\"end\":67019,\"start\":66931}]", "bib_author": "[{\"end\":54568,\"start\":54559},{\"end\":54578,\"start\":54568},{\"end\":54588,\"start\":54578},{\"end\":54833,\"start\":54818},{\"end\":54841,\"start\":54833},{\"end\":54854,\"start\":54841},{\"end\":54861,\"start\":54854},{\"end\":54869,\"start\":54861},{\"end\":55210,\"start\":55203},{\"end\":55217,\"start\":55210},{\"end\":55226,\"start\":55217},{\"end\":55235,\"start\":55226},{\"end\":55248,\"start\":55235},{\"end\":55570,\"start\":55564},{\"end\":55578,\"start\":55570},{\"end\":55586,\"start\":55578},{\"end\":55594,\"start\":55586},{\"end\":55916,\"start\":55908},{\"end\":55923,\"start\":55916},{\"end\":55931,\"start\":55923},{\"end\":55939,\"start\":55931},{\"end\":55947,\"start\":55939},{\"end\":55955,\"start\":55947},{\"end\":56265,\"start\":56258},{\"end\":56272,\"start\":56265},{\"end\":56278,\"start\":56272},{\"end\":56285,\"start\":56278},{\"end\":56294,\"start\":56285},{\"end\":56303,\"start\":56294},{\"end\":56602,\"start\":56595},{\"end\":56608,\"start\":56602},{\"end\":56616,\"start\":56608},{\"end\":56623,\"start\":56616},{\"end\":56632,\"start\":56623},{\"end\":56977,\"start\":56964},{\"end\":56985,\"start\":56977},{\"end\":56992,\"start\":56985},{\"end\":57001,\"start\":56992},{\"end\":57013,\"start\":57001},{\"end\":57262,\"start\":57251},{\"end\":57273,\"start\":57262},{\"end\":57281,\"start\":57273},{\"end\":57294,\"start\":57281},{\"end\":57306,\"start\":57294},{\"end\":57315,\"start\":57306},{\"end\":57583,\"start\":57575},{\"end\":57590,\"start\":57583},{\"end\":57598,\"start\":57590},{\"end\":57606,\"start\":57598},{\"end\":57898,\"start\":57885},{\"end\":57907,\"start\":57898},{\"end\":57917,\"start\":57907},{\"end\":57928,\"start\":57917},{\"end\":57939,\"start\":57928},{\"end\":58194,\"start\":58186},{\"end\":58201,\"start\":58194},{\"end\":58212,\"start\":58201},{\"end\":58225,\"start\":58212},{\"end\":58237,\"start\":58225},{\"end\":58251,\"start\":58237},{\"end\":58262,\"start\":58251},{\"end\":58272,\"start\":58262},{\"end\":58282,\"start\":58272},{\"end\":58512,\"start\":58500},{\"end\":58519,\"start\":58512},{\"end\":58803,\"start\":58796},{\"end\":58810,\"start\":58803},{\"end\":58817,\"start\":58810},{\"end\":58825,\"start\":58817},{\"end\":58836,\"start\":58825},{\"end\":59117,\"start\":59106},{\"end\":59130,\"start\":59117},{\"end\":59142,\"start\":59130},{\"end\":59151,\"start\":59142},{\"end\":59419,\"start\":59411},{\"end\":59427,\"start\":59419},{\"end\":59440,\"start\":59427},{\"end\":59451,\"start\":59440},{\"end\":59461,\"start\":59451},{\"end\":59467,\"start\":59461},{\"end\":59475,\"start\":59467},{\"end\":59825,\"start\":59815},{\"end\":59837,\"start\":59825},{\"end\":60087,\"start\":60080},{\"end\":60095,\"start\":60087},{\"end\":60104,\"start\":60095},{\"end\":60367,\"start\":60359},{\"end\":60376,\"start\":60367},{\"end\":60383,\"start\":60376},{\"end\":60391,\"start\":60383},{\"end\":60641,\"start\":60630},{\"end\":60651,\"start\":60641},{\"end\":60900,\"start\":60893},{\"end\":60909,\"start\":60900},{\"end\":60919,\"start\":60909},{\"end\":60932,\"start\":60919},{\"end\":61133,\"start\":61126},{\"end\":61142,\"start\":61133},{\"end\":61404,\"start\":61397},{\"end\":61411,\"start\":61404},{\"end\":61417,\"start\":61411},{\"end\":61426,\"start\":61417},{\"end\":61627,\"start\":61619},{\"end\":61633,\"start\":61627},{\"end\":61644,\"start\":61633},{\"end\":61653,\"start\":61644},{\"end\":61907,\"start\":61899},{\"end\":61915,\"start\":61907},{\"end\":61923,\"start\":61915},{\"end\":61933,\"start\":61923},{\"end\":61949,\"start\":61933},{\"end\":62243,\"start\":62235},{\"end\":62249,\"start\":62243},{\"end\":62257,\"start\":62249},{\"end\":62264,\"start\":62257},{\"end\":62587,\"start\":62580},{\"end\":62596,\"start\":62587},{\"end\":62604,\"start\":62596},{\"end\":62611,\"start\":62604},{\"end\":62932,\"start\":62924},{\"end\":62938,\"start\":62932},{\"end\":62946,\"start\":62938},{\"end\":63253,\"start\":63246},{\"end\":63261,\"start\":63253},{\"end\":63267,\"start\":63261},{\"end\":63276,\"start\":63267},{\"end\":63283,\"start\":63276},{\"end\":63635,\"start\":63629},{\"end\":63643,\"start\":63635},{\"end\":63650,\"start\":63643},{\"end\":63656,\"start\":63650},{\"end\":64058,\"start\":64047},{\"end\":64067,\"start\":64058},{\"end\":64079,\"start\":64067},{\"end\":64396,\"start\":64378},{\"end\":64409,\"start\":64396},{\"end\":64601,\"start\":64587},{\"end\":64611,\"start\":64601},{\"end\":64624,\"start\":64611},{\"end\":64898,\"start\":64889},{\"end\":64905,\"start\":64898},{\"end\":64913,\"start\":64905},{\"end\":64923,\"start\":64913},{\"end\":64930,\"start\":64923},{\"end\":64938,\"start\":64930},{\"end\":65262,\"start\":65254},{\"end\":65272,\"start\":65262},{\"end\":65547,\"start\":65531},{\"end\":65557,\"start\":65547},{\"end\":65569,\"start\":65557},{\"end\":65579,\"start\":65569},{\"end\":65591,\"start\":65579},{\"end\":65605,\"start\":65591},{\"end\":65927,\"start\":65917},{\"end\":66092,\"start\":66085},{\"end\":66105,\"start\":66092},{\"end\":66414,\"start\":66403},{\"end\":66425,\"start\":66414},{\"end\":66743,\"start\":66737},{\"end\":66750,\"start\":66743},{\"end\":67029,\"start\":67021},{\"end\":67035,\"start\":67029},{\"end\":67043,\"start\":67035},{\"end\":67258,\"start\":67250},{\"end\":67274,\"start\":67258},{\"end\":67498,\"start\":67489},{\"end\":67506,\"start\":67498},{\"end\":67512,\"start\":67506},{\"end\":67703,\"start\":67695}]", "bib_venue": "[{\"end\":59861,\"start\":59853},{\"end\":64657,\"start\":64639},{\"end\":66160,\"start\":66145},{\"end\":67310,\"start\":67295},{\"end\":54594,\"start\":54588},{\"end\":54886,\"start\":54869},{\"end\":55270,\"start\":55248},{\"end\":55562,\"start\":55482},{\"end\":55964,\"start\":55955},{\"end\":56314,\"start\":56303},{\"end\":56682,\"start\":56632},{\"end\":56962,\"start\":56913},{\"end\":57249,\"start\":57180},{\"end\":57638,\"start\":57606},{\"end\":57883,\"start\":57810},{\"end\":58184,\"start\":58136},{\"end\":58794,\"start\":58706},{\"end\":59104,\"start\":59033},{\"end\":59500,\"start\":59475},{\"end\":59851,\"start\":59837},{\"end\":60131,\"start\":60104},{\"end\":60357,\"start\":60310},{\"end\":60628,\"start\":60537},{\"end\":60891,\"start\":60828},{\"end\":61164,\"start\":61142},{\"end\":61395,\"start\":61326},{\"end\":61617,\"start\":61547},{\"end\":61897,\"start\":61825},{\"end\":62291,\"start\":62264},{\"end\":62638,\"start\":62611},{\"end\":62964,\"start\":62946},{\"end\":63305,\"start\":63283},{\"end\":63706,\"start\":63656},{\"end\":64104,\"start\":64079},{\"end\":64376,\"start\":64319},{\"end\":64637,\"start\":64624},{\"end\":64965,\"start\":64938},{\"end\":65295,\"start\":65272},{\"end\":65529,\"start\":65470},{\"end\":65952,\"start\":65927},{\"end\":66143,\"start\":66105},{\"end\":66448,\"start\":66425},{\"end\":66768,\"start\":66750},{\"end\":67061,\"start\":67043},{\"end\":67293,\"start\":67274},{\"end\":67487,\"start\":67432},{\"end\":67693,\"start\":67650}]"}}}, "year": 2023, "month": 12, "day": 17}