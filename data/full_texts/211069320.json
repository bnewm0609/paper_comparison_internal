{"id": 211069320, "updated": "2023-10-06 19:05:56.349", "metadata": {"title": "Self-supervised ECG Representation Learning for Emotion Recognition", "authors": "[{\"first\":\"Pritam\",\"last\":\"Sarkar\",\"middle\":[]},{\"first\":\"Ali\",\"last\":\"Etemad\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 4}, "abstract": "We present a self-supervised deep multi-task learning framework for electrocardiogram (ECG) -based emotion recognition. The proposed framework consists of two stages of learning a) learning ECG representations and b) learning to classify emotions. ECG representations are learned by a signal transformation recognition network. The network learns high-level abstract representations from unlabeled ECG data. Six different signal transformations are applied to the ECG signals, and transformation recognition is performed as pretext tasks. Training the model on pretext tasks helps our network learn spatiotemporal representations that generalize well across different datasets and different emotion categories. We transfer the weights of the self-supervised network to an emotion recognition network, where the convolutional layers are kept frozen and the dense layers are trained with labelled ECG data. We show that our proposed method considerably improves the performance compared to a network trained using fully-supervised learning. New state-of-the-art results are set in classification of arousal, valence, affective states, and stress for the four utilized datasets. Extensive experiments are performed, providing interesting insights into the impact of using a multi-task self-supervised structure instead of a single-task model, as well as the optimum level of difficulty required for the pretext self-supervised tasks.", "fields_of_study": "[\"Engineering\",\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2002.03898", "mag": "3103192617", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/taffco/SarkarE22", "doi": "10.1109/taffc.2020.3014842"}}, "content": {"source": {"pdf_hash": "a484fc24b62758b5bc61e6b967e7935c1a08dcab", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.03898v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2002.03898", "status": "GREEN"}}, "grobid": {"id": "74eedb3aaca6ee26853c3b7403dae0f3824ebbc7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a484fc24b62758b5bc61e6b967e7935c1a08dcab.txt", "contents": "\nSELF-SUPERVISED ECG REPRESENTATION LEARNING FOR EMOTION RECOGNITION UNDER REVIEW *\n\n\nPritam Sarkar pritam.sarkar@queensu.ca \nDepartment of Electrical and Computer Engineering\nDepartment of Electrical and Computer Engineering Queen's University Kingston\nQueen's University Kingston\nK7L 3N6, K7L 3N6ON, ONCanada, Canada\n\nAli Etemad ali.etemad@queensu.ca \nDepartment of Electrical and Computer Engineering\nDepartment of Electrical and Computer Engineering Queen's University Kingston\nQueen's University Kingston\nK7L 3N6, K7L 3N6ON, ONCanada, Canada\n\nSELF-SUPERVISED ECG REPRESENTATION LEARNING FOR EMOTION RECOGNITION UNDER REVIEW *\nSelf-supervised Learning \u00b7 ECG \u00b7 Emotion Recognition \u00b7 Multi-task Learning\nWe present a self-supervised deep multi-task learning framework for electrocardiogram (ECG) -based emotion recognition. The proposed framework consists of two stages of learning a) learning ECG representations and b) learning to classify emotions. ECG representations are learned by a signal transformation recognition network. The network learns high-level abstract representations from unlabeled ECG data. Six different signal transformations are applied to the ECG signals, and transformation recognition is performed as pretext tasks. Training the model on pretext tasks helps our network learn spatiotemporal representations that generalize well across different datasets and different emotion categories. We transfer the weights of the self-supervised network to an emotion recognition network, where the convolutional layers are kept frozen and the dense layers are trained with labelled ECG data. We show that our proposed method considerably improves the performance compared to a network trained using fully-supervised learning. New state-of-the-art results are set in classification of arousal, valence, affective states, and stress for the four utilized datasets. Extensive experiments are performed, providing interesting insights into the impact of using a multi-task self-supervised structure instead of a single-task model, as well as the optimum level of difficulty required for the pretext self-supervised tasks.\n\nIntroduction\n\nAffective computing is a field of study that deals with understanding human emotions, intelligent human-machine interaction, and computer-assisted learning among others [1,2]. The goal of affective computing is to equip machines with the ability to model and interpret the emotional states of humans. Emotion is considered a physiological and psychological expression associated with moods and personalities of individuals [3]. As a result, sensing technologies integrated into wearable devices coupled with machine learning and deep learning techniques have recently been used to analyze physiological signals in order to classify or quantify human emotions [4][5][6]. Recent applications of affective computing include human monitoring systems [7,8], stress or anxiety level detection [9,10], emotion and personality recognition [5,11], and others.\n\nA wide variety of data sources have been utilized for affective computing purposes. These data sources include facial expressions [12], motion and gait [13], speech [14], electrocardiogram (ECG) [15], electroencephalogram (EEG) [6], electrooculogram (EOG) [16], and galvanic skin response (GSR) [17] among others. In particular, ECG, which is the focus of our study, has been explored in the past with works such as [15] showing a strong correlation between emotional attributes and the ECG waveform, and investigating the feasibility and limitations of using ECG signals for emotion detection. Further, recent advancements in machine learning and deep learning have proven ECG to be a reliable source of information for emotion recognition systems [4,18]. The majority of machine learning or deep learning solutions for ECG-based emotion recognition utilize fully-supervised learning methods. Several limitations can be associated with this approach. First and foremost, in a typical setup of fully-supervised learning, the model needs to be trained from scratch for every classification or regression task [19], which requires considerable computational resources and time. Additionally, the learned representations from the trained full-supervised models are often very task-specific, which are not expected to generalize well for other tasks. Lastly, fully supervised learning generally requires training using large human-annotated datasets, since small datasets typically result in poor performance with deeper networks.\n\nTo tackle these problems, we propose a deep learning solution based on self-supervised [20] representation learning. In selfsupervised learning, models are trained using automatically generated labels instead of human-annotated ones. There are a number of advantages to self-supervised learning. First, the representations learned using this approach are often invariant to inter-and intra-instance variations [21] due to learning rather generalized features as opposed to task-specific ones. As a result, these models can be reused for different tasks within the same domain. This property often improves the performance of the networks and also saves computation time compared to training a model from scratch for each task. Lastly, as self-supervised learning doesn't require human-annotated labels, larger datasets can be acquired and utilized, which in turn results in the ability to train deeper and more sophisticated networks.\n\nIn this paper we propose an emotion recognition framework using multi-task self-supervised ECG representation learning for the first time following our preliminary work in the area described in [22]. We use four publicly available datasets, AMIGOS [5], DREAMER [23], WESAD [24], and SWELL [10] to perform emotion recognition with ECG. First, a number of transformations are applied to the ECG signals. The automatically generated labels associated with the transformations are then utilized to carry out self-supervised learning of ECG representations through a deep multitask Convolutional Neural Network (CNN). Next, the convolutional layers responsible for learning the ECG representations are frozen and transferred to a second network. The dense layers of this new network are then trained with the labeled ECG data to perform emotion classification. Our contributions are as follows:\n\n\u2022 We propose a self-supervised framework for emotion recognition based on multi-task ECG representation learning for the first time.\n\n\u2022 The results show that our self-supervised model outperforms the same network in emotion recognition when trained in a fully-supervised fashion.\n\n\u2022 We perform very thorough analyses on the parameters associated with the self-supervised learning tasks, providing interesting insights into the impact of different self-supervised tasks and their contribution towards learning of affective ECG representations. Additionally, we show that self-supervised multi-task learning results in better learned representations compared to single-task self-supervised learning.\n\n\u2022 Further, our analysis provides interesting insights into the relationship between the difficulty of self-supervised tasks and the original emotion recognition task, where there seems to be an optimum level of difficulty in which the network can learn strong representations in a self-supervised manner. The analysis shows that simpler tasks do not result in proper learning of ECG representations, whereas extremely hard tasks also prevent the network from learning generalized representations.\n\n\u2022 We set new state-of-the-art results for all the undertaken emotion classification tasks namely arousal, valence, affective states, and stress recognition in the four utilized datasets AMIGOS, DREAMER, WESAD, and SWELL.\n\n\u2022 We demonstrate that the ECG representations learned by the proposed self-supervised model generalize very well across all four ECG datasets, consistently resulting in accurate emotion recognition.\n\nThis paper is an extension of our work [22], compared to which this paper additionally includes the following: a) Two additional publicly available datasets DREAMER and WESAD are used in this paper to further investigate the benefits of our proposed self-supervised learning framework compared to fully-supervised methods; b) The impact of different transformation tasks on the intended emotion recognition tasks are investigated in depth and presented in this paper; c) The relationship between the difficulty of self-supervised tasks and emotion recognition is investigated; d) The benefits of the multi-task architecture for the self-supervised transformation recognition network are compared to a single-task approach, and the results are presented; e) We analyze and discuss the impact of different parameters associated with each of the signal transformations with respect to the network performance; f) An analysis of the relationship between the learned representations and the depth of our proposed architecture is presented; g) Lastly, the effect of utilizing an aggregation of multiple datasets for self-supervised training as opposed to individual datasets is investigated and presented.\n\nThe rest of the paper is organized as follows. Section 2 discusses the background and related works on emotion recognition with ECG. We describe our proposed methodology in Section 3. The datasets used in this paper and the experiment setup are described in Section 4. Section 5 presents the performance and results of our proposed architecture. A thorough analysis and limitations of our proposed framework are presented in Section 6. Section 7 presents the summary and our concluding remarks.\n\n2 Background and Related Work\n\n\nElectrocardiography\n\nECG signals contain information regarding cardiac electrical activity over a period of time, where an ECG beat is composed of three waves: the P wave, the QRS complex, and the T wave [15]. Each waveform contains significant information that can be used to understand the cardiac state of individuals. To acquire ECG signals, electrodes are placed on the surface of the skin. The most common sensor configurations include 12-lead, 5-lead, 3-lead, or single-channel systems. ECG signals have been widely used to develop deep learning solutions, for example arrhythmia detection [25], emotion or stress detection [15,26], biometric identification [27], and others. Additionally, research has shown strong correlations between cardiovascular activity and emotions, as emotions can influence the autonomic nervous system which controls the rhythm of heart beats [15]. To extract useful information from heart beats, different feature extraction techniques have been utilized [9,28]. Most common approaches include either utilizing raw ECG signals to directly learn high-level representations [25], or extracting time-domain and frequency domain features successive to detection of fiducial points (P, Q, R, S, and T components) from the ECG waveform [15,19]. Successive to learning of ECG representations, classifiers such as Multi-Layer Perceptrons are typically used learn the relationships between the high-level representations and the main task [4,29].\n\n\nECG-based Affective Computing\n\nSystems capable of performing affective computing generally aim to classify or quantify the emotional state of individuals based on collected data and information. Recent advancements in affective computing have shown ECG to be a reliable source of information to classify the emotional states of humans [4,11,18,30]. The type of affective states often studied include stress, happiness, sorrow, and excitement among others.\n\nIn an early work on ECG-based affective computing, stress detection was performed in [9], as participants went through a driving task. ECG signals were collected during the study, followed by extraction of time and frequency domain features which were then used to perform classification. In particular, Heart Rate Variability (HRV) features were calculated from power spectrum of ECG signals. The extracted features were used to perform classification using a linear discriminant classifier.\n\nIn a large study, in [28], emotion recognition was performed, where ECG signals from 391 subjects were captured, while individuals were shown a movie clip belonging to either the category of joy or sadness. Out of all the participants, 150 subjects were selected who had shown effective elicitation during this study. Next, continuous wavelet transform was performed to accurately detect P-QRS-T wave locations. A total of 79 features were extracted. To perform better and effective classification, feature selection was performed by means of a hybrid particle swarm optimization technique. Their study shows that the selected features can correctly classify emotions with a Fisher classifier, reporting an accuracy of 88.43%.\n\nThe relationship between personality and affect was studied in [11]. The study was conducted on 58 participants. While subjects were shown emotional movie clips, ECG data were recorded. Similar to [9], HRV features were calculated, along with heart rate and inter-beat intervals. These features were utilized to perform classification of arousal and valence scores. Classification was performed by a Support Vector Machine (SVM) and Naive Bayes classifier. The naive Bayes classifier outperformed the SVM, reporting an accuracy of 59% and 60% for arousal and valence respectively.\n\nIn a recent work, deep learning was used to classify cognitive load and expertise of learners in a medical simulation using ECG [4]. ECG data were recorded from 9 medical practitioners in 2 classes, expert and novice. First, baseline ECG was collected as participants were in rest. Next, ECG signals were collected during simulation where the subjects had to tend to an injured manikin. R-peaks were then detected, followed by the extraction of 11 time-domain features. Additionally, 9 frequency domain features were extracted successive to calculating the power of different frequency bands. Finally, the extracted features were normalized by corresponding baseline features. A deep neural network with 7 layers was employed to perform classification of expertise and cognitive load. The model achieved an accuracy of 89.4% and 96.6% for cognitive load and expertise respectively.\n\nThe research shows an interesting relationships between level of expertise and cognitive load, indicating that under stressful conditions, experts tend show lower cognitive load compared to novice subjects.\n\nA number of prior works such as [5,18,23,24,29], and [10] have used one or more of the four datasets utilized in this paper, AMIGOS, DREAMER, WESAD, and SWELL, for emotion recognition. In [5], a Gaussian Naive Bayes (GNB) classifier was used on AMIGOS and reported F1 scores of 54.5% and 55.1% for classifying arousal and valence respectively. To perform a similar study, a CNN was used in [29], reporting accuracies of 81% and 71% for arousal and valence classification. An initial study on DREAMER was performed in [23] where arousal and valence classification was performed using an SVM classifier and an accuracy of 62.37% was reported. A novel technique was employed in [18] to perform emotion recognition on both DREAMER and AMIGOS. Two different sets of features were fused to perform the classification. First HRV features were extracted by calculating RR intervals from the input ECG signals. Next, spectrogram images were generated from time-series ECG signals. A pre-trained VGG-16 was used to extract feature embeddings of size 4096, followed by dimensionality reduction to 30 dimensions. Finally, both sets of features were concatenated and used to perform classification by means of an Extreme Learning Machine (ELM) classifier. Three-level affect state detection was performed in [24], where a Linear Discriminant Analysis (LDA) classifier was used on WESAD and reported an accuracy of 66.29%. Later, [31] proposed a Mutlimodal-Multisensory Sequential Fusion (MMSF) model, which showed that the proposed model can be trained on different signals with different sampling rates in the same training batch. When utilizing only ECG, an accuracy of 83% was reported for detection of affect state on WESAD. In [32], stress detection was performed in SWELL. Feature analysis was performed on extracted time domain and frequency domain features. Time domain feature NN50 showed the best performance compared to other features and reported an accuracy of 86.36% by utilizing an SVM classifier.\n\n\nSelf-supervised Representation Learning\n\nRecent research in the area of machine learning and deep learning has shown the potential of self-supervised models in learning generalized and robust representations. This concept has been utilized in computer vision [21,[33][34][35], speech [36], natural language processing [37], and others. Self-supervised learning is a deep learning paradigm in which the network is trained using automatically generated labels instead of human-annotated labels. Different techniques have been employed to generate automatic labels. For example, a self-supervised network was proposed in [38], where high-level spatiotemporal features were learned from unlabelled videos. The network was trained using different rotated video clips to predict rotations. Next, the trained model was fine-tuned on the action recognition datasets. Their research showed that activity recognition accuracy was improved by more than 15% on both utilized datasets, compared to a network trained in a fully-supervised manner. In another work in [33], 3D pose estimation was tackled using self-supervised learning. To overcome the need for large amounts of 3D ground truth data generally required for 3D pose estimation, epipolar geometry was employed to calculate 3D poses from the available 2D poses. Subsequently, a convolution neural network was trained using the available 3D poses and reported state-of-the-art results. Self-supervised learning has also been used in learning wearable data. For example, human activity recognition from Inertial Measurement Units (IMU) was performed in [39]. A set of different signal transformations were performed to train a convolutional neural network to predict transformations. Further, features were extracted from the trained network, and fully-connected layers were utilized to learn different activities such as walking, sitting, and jogging among others using the learned representations.\n\nThe works presented above, among others, demonstrate the advantages and promising results obtained using self-supervised learning in other domains. Nonetheless, such a framework has not been proposed for ECG representation learning, which we undertake in this paper in the context of emotion recognition.\n\n\nProposed Method\n\nOur goal in this paper is to learn ECG representations R ECG capable of distinguishing between different classes of emotion. The proposed framework consists of two stages of learning: first, learning ECG representation, and second, using the network capable of extracting ECG representation to learn another network capable of classifying emotions. Accordingly, we define two sets of tasks, pretext and downstream, referred to as T p and T d respectively, where T p is the set of signal transformation recognition tasks through which ECG representations are learned, while T d learns to classify emotions. The goal of T p is to learn robust generalized features from unlabelled ECG data through a self-supervised process, i.e. recognition of signal transformations. In the second stage in which T d is learned, our framework uses the frozen convolution layers of the first network to learn emotional classes upon supervised learning of the fully-connected layers at the end of the network. Figure 2 shows our proposed architecture. In the following subsections, we further describe the architecture and details of each learning stage. The optimum architecture and hyperparameters for these two networks described below are selected either empirically or based on searching among a large number of combinations for optimum performance. The proposed self-supervised architecture is presented. First, a multi-task CNN is trained using automatically generated labels to learn ECG representations. Then, the weights are transferred to the emotion recognition network, where the fully connected layers are trained to classify emotions. \n\n\nSelf-supervised Learning of ECG Representations\n\nWe propose a self-supervised deep multi-task CNN to learn R ECG , through recognition of different transformations applied to input ECG signals. Let's denote an example tuple of inputs and pseudo-labels for T p as (X j , P j ) where X j is j th transformed signal, P j is the automatically generated label according to the j th transformation, and j \u2208 [0, N ] where N is the total number of signal transformations. To learn T p , a signal transformation recognition network, \u03b3(X j , P j , \u03b8) is trained, where \u03b8 is the set of trainable parameters. The model learns \u03b8 by minimizing the total loss, L total , composed of the weighted average of the individual losses of the signal transformation network L j . The total network loss is defined by:\nL j = [P j log \u03c8 j + (1 \u2212 P j ) log(1 \u2212 \u03c8 j )],(1)L total = N j=0 \u03b1 j L j ,(2)\nwhere \u03c8 j and \u03b1 j are the predicted probability and loss coefficient of the j th transformation task respectively.\n\nSix different signal transformation recognition tasks are performed as pretext tasks [39,40]. The details of the parameters associated with these tasks (for instance signal-to-noise ratio, scaling factor, stretching factor, and others) are tuned based on a large number of experiments and are explored in depth in Section 6.1 and 6.2. These transformations are described below and a sample is illustrated in Figure 3.\n\nNoise addition: In this transformation, random noise from a Gaussian distribution, N (t), is added to the original ECG signal S(t). N (t) is obtained from a random noise generator where the mean of the distribution is set to 0, and the standard deviation to E Navg . Here, E Navg = 10 (E Savg \u2212\u03b1)/10 , where E Savg is the average power of S(t) and \u03b1 is the desired Signal to Noise Ratio (SNR). Finally the noise-added signal is generated as S(t) + N (t).\n\nScaling: The original magnitude of the ECG S(t) is transformed as \u03b2 \u00d7 S(t), where \u03b2 > 0 and \u03b2 is the manually assigned scaling factor. Where T (t) becomes longer or shorter than S(t) due to the even or odd nature of m, T (t) is clipped or zero-padded accordingly to maintain the original input length.\n\nAll of These transformations are applied to the original signals and stacked to create the input matrix X, while the corresponding labels of the transformations 0, 1, ..., 6 are stacked to create the corresponding output matrix P , with 0 denoting the original matrix and integers 1 to 6 indicating the 6 transformations. The input-output matrices are then shuffled to re-order the transformations and their corresponding outputs.\n\nThe architecture of our proposed network for learning ECG representations using the created transformation dataset described above consists of 3 convolutional blocks (conv-block) as shared layers, and 7 branches, each with 2 dense layers, as task-specific layers. Each conv-block is made of 2 \u00d7 1D convolution layers with ReLu activation functions followed by a max-pooling layer of size 8. The number of filters are increased from 32 to 64 and 128, whereas, the kernel size is decreased from 32 to 16 and 8 respectively. Global max-pooling operation is performed after the final conv-block. Next, successive to flattening the embedding, the output is fed to 2 fully connected layers with 128 hidden nodes and ReLu activation functions. To overcome possible overfitting, 60% dropout is introduced. Finally, the model output is generated through a sigmoid layer. The summary of the network architecture is presented in Table 1.\n\n\nEmotion Recognition\n\nIn the second stage (T d ) of learning our proposed architecture, the model is trained with the true emotion labels y i for emotion classification. R ECG obtained from T p contain useful information regarding the original signals X 0 , which are used to perform T d . To utilize R ECG towards performing T d , a simple network \u03c1 = w(F, \u03b8 ) is employed, with \u03c1 as the probability vector of T d classes, and \u03b8 as the set of trainable parameters. Finally, we calculate the optimum value of \u03b8 by minimizing the cross entropy loss:\nL = M i=1 y i log \u03c1 i ,(3)\nwhere M is total number of emotional classes.\n\nThe emotion recognition network contains convolution layers similar to those used in learning ECG representation, followed by fully connected layers with 512 hidden nodes. The weights of the shared layers of the signal transformation recognition network are frozen and transferred to the emotion recognition network. However, the fully connected layers of the emotion recognition network are not carried over from the transformation recognition network, and are instead trained using the labelled dataset (fully supervised) to perform emotion recognition. It should be noted that as we use different datasets in this study, the fully-connected layers are fine-tuned for each dataset. Specifically, 2 dense layers with L2 regularization (0.0001) are utilized for SWELL and WESAD datasets, while 3 dense layers with the same L2 regularization and 20% dropout are used for AMIGOS and DREAMER datasets. We intentionally kept the fully-connected layers simple (single-task as opposed to multi-task) and somewhat shallow, in order to be able to evaluate the ability of the self-supervised approach to learn robust generalized ECG representations. The overview of our proposed method is presented in Figure 2, where successive to a self-supervised learning ECG representations, transfer learning is used for emotion recognition with the fully-supervised model.\n\n\nExperiments\n\n\nDatasets\n\nWe used four publicly available datasets to evaluate our proposed method in depth on a large number of different subjects, in different circumstances and under different data collection protocols, and using different hardware. The important characteristics of the datasets are summarized in Table 2 and a brief description of each dataset is provided bellow. \n\n\nAMIGOS\n\nThe AMIGOS dataset [5] was collected from 40 participants to study personality, mood, and affective responses, while engaging with multimedia content in two different contexts: a) alone, and b) in a group of four people. To perform this study, participants were shown short and long emotional video clips to elicit affective states. The short video clips had a length of 250 seconds, whereas the long video clips were longer than 14 minutes. ECG data were recorded using Shimmer sensors [41] at a sampling frequency of 256 Hz. A total of 16 video clips were shown to each participant, and self-assessed arousal and valence scores were recorded after each trial on a scale of 1 to 9.\n\n\nDREAMER\n\nThe DREAMER dataset [23] was collected from 23 participants while they were presented with audio and video stimuli in the form of movie clips in order to elicit emotional reactions. A total of 18 video clips were shown, inducing 9 different emotions namely amusement, excitement, happiness, calmness, anger, disgust, fear, sadness, and surprise. Each clip was 60 seconds long. In addition, neutral clips were shown before each session to help subjects return to a neutral emotional state. ECG signals were collected using shimmer sensors [41] at a sampling rate of 256 Hz. Subjective experience of arousal and valence scores were collected using self-assessment manikins [42]. Both arousal and valence scores were recorded on a scale of 1 to 5, ranging from uninterested/bored to excited/alert for arousal and unpleasant/stressed to happy/elated for valence.\n\n\nWESAD\n\nThe dataset for WEarable Stress and Affect Detection (WESAD) contains ECG data from 17 participants. A RespiBAN Professional [43] sensors were used to collect ECG at a sampling rate of 700 Hz. The goal was to study four different affective states namely neutral, stressed, amused, and meditated. To perform this study, four different test scenarios were created. First, 20 minutes of neutral data were collected, during which participants were asked to do normal activities such as reading a magazine and sitting/standing at a table. During the amusement scenario, participants watched 11 funny video clips for a total length of 392 seconds. Next, participants went through public speaking and arithmetic tasks for a total of 10 minutes as part of the stress scenario. Finally, participants went through a guided meditation session of 7 minutes in duration.\n\n\nSWELL\n\nThis dataset was created with the goal of understanding employees' mental stress or emotional attributes in a typical office environment under different conditions [10]. Three types of scenarios namely, normal, time pressure, and interruptions were designed. Under the normal condition, participants were allowed to work on different tasks, for example preparing reports, making presentations, and so on, which were carried out over a maximum duration of 45 minutes. During the time pressure condition, the allowed time was reduced to 30 minutes to induce pressure. In the interruption session, participants were interrupted by sending emails and messages, with the participants being asked to respond to these messages. ECG signals were collected from 25 participants using a TMSI MOBI device [44] with self-adhesive electrodes at a sampling rate of 2048 Hz. Finally, the participants' self-reported affect scores were collected on a 1 to 9 point scale.\n\n\nData Pre-processing\n\nSince the above-mentioned datasets have been collected using different hardware, the signals have different spatiotemporal properties such as spatial range and sampling rate. To minimize the effects of such inter-dataset variations and discrepancies, three simple pre-processing steps were taken. First, SWELL and WESAD ECG signals are downsampled to 256 Hz to be consistent with AMIGOS and DREAMER. Next, we remove baseline wander from all the four datasets by applying a high-pass IIR filter with a pass-band frequency of 0.8 Hz. Lastly, we perform user-specific z-score normalization. While a number of other pre-processing operations such as feature extraction could have been done, we intentionally kept the pre-processing minimal and simple in order to better understand the impact of our proposed model on learning important ECG representations based on almost raw input.\n\n\nImplementation and Training\n\nIn order to train our proposed model, successive to pre-processing, signals are segmented into a fixed window size of 10 seconds without any overlap. No overlap is designated between segments to avoid any potential data leakage between training and testing data. Then, each segment is used to generate the 6 transformations described earlier. Finally, the original ECG signals along with the transformed signals are used to train the signal transformation recognition network. We implement our proposed architecture using TensorFlow using an Nvidia 2070 Ti GPU. To train both the signal transformation recognition network and the emotion recognition network, Adam optimizer [45] is used with a learning rate of 0.001 and batch size of 128. The signal transformation recognition network is trained for 100 epochs, while the emotion recognition network is trained for 250 epochs. The number of training epochs for each network is selected to enable the training reach a steady state. Similar to other works in this area, we use 10-fold cross-validation to evaluate the performance of our architecture. Figure 4(A) and (B) show the loss vs. training epoch for the transformation recognition and emotion recognition networks respectively during training. Figure 4(A) shows that the loss for temporal inversion, negation, permutation, and time-warping reach steady states earlier than the other transformations. Therefore, the network is trained until all the individual losses reach their respective steady states. Figure 4(B) shows that the different datasets reach their training steady states at different epochs. For example, the loss of WESAD and SWELL become stable well before 100 epochs, while the steady states of AMIGOS and DREAMER are achieved after 200 epochs. In summary, Figure 4 shows that both pretext tasks and emotion recognition tasks train well and reach steady states using our proposed method.  \n\n\nResults\n\nIn this section, we present the results of our model's performance for signal transformation recognition, followed by emotion recognition. Next, we compare the performance of our proposed method to a fully supervised network as well as prior works and the state-of-the-art in ECG-based emotion recognition with the same datasets. Table 3 and 4 presents the F1 scores and accuracies of our proposed network for transformation recognition. Results shown in Table 3 are calculated on all the datasets combined. Our results show very high F1 scores and accuracies for all the transformations as well as the original signal. F1 scores 0.927 and 0.932 are achieved when detecting the original signal and the scaling transformations. Next, relatively higher F1 score of 0.979 is achieved for recognition of noisy signals. We notice few tasks consistently report very high scores (\u2248 0.99), namely temporal inversion, negation, permutation, and time-warping. For accuracy, very high values (greater than 0.98) are achieved for every transformation. Beside high F1 scores and accuracies, we also achieve very low standard deviations for all the transformation recognition tasks, which indicates a consistent performance by our network in all the training folds. Finally, an average F1 score of 0.972 and an average accuracy of 0.992 are reported for all the tasks combined.\n\n\nSignal Transformation Recognition\n\nNext, we further investigate the consistency and generalization of the performance of the transformation recognition network across different datasets. Table 4 presents the results on each of the four datasets, showing very high F1 scores and accuracies. This is a strong indicator of the generalizability of our approach as the model performs consistently on all the datasets with F1 scores of 0.959 to 0.987 and accuracies of 0.989 to 0.996, with low standard deviations in every case.  \n\n\nEmotion Recognition\n\nResults obtained from the emotion recognition network are presented in Table 5. Arousal and valence detection are performed with three datasets AMIGOS, DREAMER, and SWELL, stress detection is performed with SWELL, and classification of different affective states are performed in WESAD, as per the availability of output labels. As presented in Table 2, the four datasets contain different number of classes for each target output. Therefore, the classification tasks are performed accordingly, the accuracies and F1 scores of which are presented in Table 5. It should be noted that to the best of our knowledge, no previous work has utilized AMIGOS, DREAMER and SWELL datasets for multi-class classification. this is the first time, multi-class classification of arousal and valence score is attempted on. With AMIGOS, we achieve accuracies of 79.6% and 78.3% for arousal and valence respectively, where 9-class classification is performed. In DREAMER, 5-class classification is performed, achieving accuracies of 77.1% and 74.9% for arousal and valence respectively. Next, we perform detection of 4 affective states using WESAD, reporting an accuracy of 95.0%. Lastly, arousal, valence, and stress detection is performed on SWELL, achieving accuracies of 92.6%, 93.8%, and 90.2% respectively, where arousal and valence scores have 9 classes while stress has 3 classes.\n\nComparison: To further evaluate our proposed architecture, we compare our model performance to past work as shown in Table 6. It should be noted that most of the prior works using the four studied datasets have performed two-class or three-class classifications [5,10,23,24] instead of utilizing all the multiple classes available for each dataset. Therefore, in order to perform a fair comparison, in addition to the multi-class classifications presented in Table 5, in Table 6 we present the results of our architecture applied to two/three-class classifications in accordance to the prior works and state-of-the-art methods presented in this table. In Table 6, AMIGOS, DREAMER, and SWELL have all been classified in two-class format by setting a threshold at the mean output value dividing the outputs to high and low, while WESAD has been classified in three classes by ignoring the meditated output class. Moreover, for each dataset, we have also carried out fully-supervised classification by skipping the self-supervised step and directly training the emotion recognition network using the input ECG and emotion labels. In [29], a CNN was implemented to perform emotion classification on AMIGOS. It reported accuracies of 81% and 71% for classification of arousal and valence respectively. The results in Table 6(A) show that our proposed self-supervised CNN achieves accuracies of 88.9% and 87.5% for classification of arousal and valence respectively, outperforming the past works as well as the fully-supervised baseline CNN. In [23], classification of arousal and valence was performed with DREAMER. An SVM classifier was implemented, reporting an accuracy of 62.4% for both arousal and valence. Table 6(B) shows that our proposed method achieves considerable improvement over the state-of-the-art and the baseline CNN with accuracies of 85.9% and 85% for arousal and valence respectively. In a study on WESAD, [31] proposed a multimodal CNN that was able to achieve state-of-the-art with an accuracy of 83% while using only ECG. As shown in the Table 6(C), our proposed model is able to outperform the CNN in [31] as well as the baseline CNN with accuracy of 96.9%. Lastly, a study on SWELL performed stress detection [32], in which two classifiers, kNN and SVM, were utilized. The results in Table 6(D) show that the baseline CNN slightly outperforms the SVM, whereas our self-supervised model considerable outperforms both with an accuracy of 93.3%. Additionally, we perform two-class classification of arousal and valence on SWELL for the first time, achieving very accurate results.  It should be noted that in order to perform a fair comparison, Table 6 excludes prior works such as: i) [46] and [18] which have used different validation schemes; ii) [47][48][49] which have used multiple modalities (ECG with EEG or GSR) available in some of the datasets; and iii) [50], which has formulated the problem as regression instead of classification.\n\n\nAnalysis and Discussion\n\nIn this section we further investigate different aspects of our proposed framework.\n\n\nSingle-task Self-supervision\n\nWe study the impact of individual transformations on the emotion recognition performance. For this study, we create a single-task CNN, instead of a multi-task CNN. For this study, we select a range of possible values for the parameters that control the significance of each individual transformation. For noise addition, the noise amplitude parameter is changed to result in signal-to-noise ratios (SNR) of 2 to 45. For the scaling transformation parameter, scaling factors of 0.1 to 10 are selected. For permutation, the number of segments is varied between 2 to 40. For time-warping, the stretch factor is varied between 1.05 to 4, while the number of segments varies from 2 to 40. Finally, temporal inversion and negation, which do not have any controllable parameters are also studied. We should point out that these ranges of parameters are selected to create a wide range of transformed signals, almost identical to the original signal in one end and considerably different in the other. Figure 5 shows the emotion recognition performance for each dataset vs. the controllable parameters for each individual transformation shows 3D plots for time-warping as this operation has two parameters, with x and y axes representing the number of segments and stretch factor respectively, and the accuracy is denoted by the color intensity.\n\ntask. To simplify the search, a single 90-10 split for training-testing was performed instead of 10-fold. This analysis provides in-depth insight into the effect of the parameters associated with each self-supervised recognition task (transformation) on the emotion recognition outcome. Furthermore, this analysis helps us narrow down the set of suitable transformation parameters in order to achieve the best performance. From Figure 5(A), we notice that for SNR values of 15 and 20 the model shows highest performance for emotion recognition compared to SNR values greater than 30 or less than 10. Moreover, from Figure 5(B), we notice that for the scaling factor of 1, where the scaled signal and the original signals are identical, the model performance on emotion recognition is poor. However, for slightly greater or smaller scaling values, the model shows significantly better performance. Interestingly, the performance drops when the scaling factor is greater than 1.4 or less than 0.6. Figure 5(C) illustrates that when utilizing permutations, poor performance is achieved when the input is divided into only 2 segments or more than 20 segments. Otherwise, when the number of segments varies in the range of 3 to 20, a relatively steady performance is observed. Time-warping analysis is presented in a 3D plot in Figure 5(F) where dark red regions show higher accuracies. It can be seen that number of segments in the range of 6 to 9 and a stretch factor in the range of 1.05 and 1.35 consistently result in better performance compared to other combinations.\n\nIn summary, our analysis above shows that for all the different transformation tasks, when the transformed signal is highly distorted compared to the original input, for example with high amplitude noise (SNR < 10), large number of segments (> 20), very high or very low scaling factor (< 1.5 or > 0.6), or large stretch factor (> 1.4), emotion recognition accuracy drops. This is due to the fact that distinguishing between the original signal and the highly distorted versions of the signal becomes simple. This results in the network not learning useful ECG representations. Similarly, when the transformed signal remains almost similar to the original signal, for instance when the SNR > 35, or a scaling factor in the range of 0.95 to 1.05, or the number of permutated segments equal to 2, the performance also drops. This is due to the fact that the recognition tasks in such cases become too difficult for the network to properly learn the required ECG representations from. Hence, we conclude that there is a range of values for the parameters associated with the self-supervised tasks, for which these tasks are located in an suitable difficulty range, resulting in optimum learning. Figure 6: Emotion recognition results vs. 5D vector of parameters controlling all 6 tasks simultaneously are presented. The resulting 6 dimensional plots are represented as follows: Y axis presents the emotion recognition accuracy, the two plots separated by the vertical line correspond to SNR, the time-warping stretch factor is represented by the X axis, the scaling factor, permutation segments, and time-warping segments are represented by marker size, color, and shape respectively. Figure 7: Comparison between single-task and multi-task self-supervision is presented. MT denotes multi-task, while T1 through T6 denote noise addition, scaling, permutation, time-warping, temporal inversion, and negation respectively.\n\n\nMulti-task Self-supervision\n\nThis section discusses our model's emotion recognition performance when multiple signal transformations (multi-task) are performed to learn ECG representations. The reason we believe this analysis is necessary despite already analyzing individual transformations in the previous subsection is that the use of a multi-task network may have an impact on the overall performance which might be different than the aggregation of several individual self-supervised tasks. To perform this study, we use the results from the previous section (Subsection 6.1) to narrow down our search given the large degree of freedom in the multi-task parameter space. Also, to further simplify the search, similar to Figure 5, a single 90-10 split for training-testing was performed instead of a complete 10-fold. The results obtained from different combination of signal transformation parameters are presented in a six-dimensional plot as shown in Figure 6. Our analysis shows that (15, 0.9, 20, 9, 1.05) is the optimal vector of parameters for SNR, scaling factor, number of permutation segments, number of time-warping segments, and time-warping stretch factor respectively. Our analysis further confirms that the results obtained from single-task self-supervision are in compliance with results obtained from multi-task self-supervision. For example, Figure 5(A) shows that an SNR = 15 results in better emotion recognition compared to SNR = 25. Very similar results are also observed in Figure 6, as SNR = 15 results in the best accuracy in 6 out of 8 instances. As another example, in both single-task and multi-task settings, a stretch factor of 1.05 shows superior performance compared to 1.35.  Figure 7 compares the emotion recognition results when ECG representations are learned using self-supervised single-tasks to when multi-task representation learning is used. These results are reported for the optimum parameters for each case. It is clearly seen that scaling has the highest impact among the six utilized transformations. However, multi-task learning consistently improves the performance for every dataset and every emotion recognition task compared to a self-supervised single-task approach. In particular, in AMIGOS and DREAMER, this improvement is more considerable compared to SWELL and WESAD.\n\nIn our multi-task network, the total loss is calculated as a weighted average of individual losses (see Eq. 2). We realize that these weights (loss coefficients) play an important role in learning the ECG representations since during training of the self-supervised network, different losses reach their steady states at different epochs. Specifically, the losses of the temporal inversion and spatial negation saturate very quickly and report almost perfect F 1 scores just after 5 to 7 epochs. The possible reason for this phenomena is the clear difference between inverted and negated signals with respect to the original signal, making it very easy for the model to correctly classify these transformations. As a result, we utilize smaller weights for temporal inversion and negation losses compared to the other transformations. All the weights were set empirically with the goal of maximizing performance. The loss coefficients of 0.0125 are used for temporal inversion and negation, while for the rest of the 5 losses, the weights are set to 0.195.\n\n\nRelationship Between Downstream and Pre-text Tasks\n\nTo further understand the relationship between the accuracy of classification of the pre-text tasks and downstream emotion recognition, we present this relationship in Figure 8. In this figure, the transformation recognition accuracies are obtained based on varying degrees of task difficulty as discussed in Section 6.1. Corresponding emotion recognition accuracies are then calculated for this analysis. Since similar patterns are noticed across different emotions and datasets, the average accuracies are utilized in the figure. For time-warping, there are two transformation parameters: number of segments and stretch factor. Therefore, first we keep the number of segments at optimum value according to the result achieved in Section 6.1, and vary the other transformation parameter (stretch factor), and vice versa. Temporal inversion and negation transformation are not included here as there is no controllable parameters to vary the transformation recognition difficulty.\n\nThe figure indicates that in the case of noise addition, scaling, and permutation, when the transformation recognition accuracy is poor, the emotion classification performance is also poor. This behaviour occurs when the transformation recognition tasks are too difficult. On the other hand, when the transformed signals are highly distorted compared to the original signal, the model reports high accuracy in transformation recognition, while poor performance is acquired for classifying emotions. This observation further confirms that for an optimum amount of difficulty in the pretext tasks, the self-supervised model shows the best performance for downstream tasks.\n\n\nNetwork Embeddings\n\nHere, we evaluate the quality of the learned representations vs. the depth of the proposed self-supervised model. The representations learned with each conv-block are separately extracted and utilized to perform emotion recognition as shown in Figure 9(A). This analysis helps us understand whether representations at different depths of the network are informative Figure 9: The feasibility of using embeddings obtained through the network at different depths for emotion recognition is investigated. In (A) the schematic of the experiment is presented, while (B) presents the results. Figure 10: The impact of using multiple datasets with our self-supervised architecture is investigated and presented. and valuable for the downstream tasks. First, the learned representations from conv-block 1, conv-block 2, and conv-block 3 are separately extracted and utilized to perform emotion recognition. As shown in Figure 9(B), significant improvement in performance is noticed when features are extracted from conv-block 3 compared to conv-block 1 and conv-block 2. In addition to using the individual embeddings, we also stacked the three embeddings to create a new embedding, which we then used for the downstream tasks. However, the embedding obtained from conv-block 3 still showed the best performance. This analysis indicates that learning representations obtained from the last convolutional layer is better and more generalizable for the downstream emotion recognition.\n\n\nImpact of Multiple of Datasets\n\nTo further analyse our proposed solution, we investigate the impact of using all four datasets for emotion recognition compared to when individual datasets are used. To perform this study, we use the four datasets separately to train the self-supervised signal transformation recognition network, which we then use for emotion recognition. Figure 10 shows that using all 4 datasets combined, the model learns more generalizable and robust features as evident by the better performance in classifying emotions compared to learning ECG representations from individual datasets. In the case of AMIGOS, DREAMER, and WESAD datasets, the performance improves significantly while marginal improvement is noticed for SWELL dataset.\n\n\nLimitations and Future Work\n\nIn this paper we performed an extensive analysis of our proposed framework and highlight the benefits of proposed self-supervised method over a fully-supervised technique. However, the limitations of our proposed method should also be noted. We notice that our proposed method performs poor in subject-independent emotion recognition. We hypothesize the main possible reasons for this: 1) subject-invariant features may require specific hand-crafted features or a combination of hand-crafted and deep learning features such as those used in [4] and [18] respectively, which might not be learnable solely with convolutions applied to the raw ECG; 2) Moreover, in order to properly normalize user-specific ECG features linked to factors such as genetics, physiology, health, and others, user-specific calibrations with respect to baseline sessions were carried out in [4], which enabled subject-independent validation to perform well. We avoided such calibration steps in our pipeline since: a) a calibration-free framework is generally more convenient and hence preferred, and b) some of the datasets did not include the information required to perform such calibrations.\n\nFor future work, multi-modal emotion recognition will be explored, exploiting other modalities such as EEG available in some datasets. Furthermore, we will employ our proposed method for other ECG-related domains such as arrhythmia detection, ECGbased activity recognition, and others. Lastly, further research towards utilization of the proposed architecture for cross-subject and cross-corpus schemes will be carried out.\n\n\nConclusion\n\nIn this work we present an ECG-based emotion recognition method using self-supervised deep multi-task learning. To the best of our knowledge, this is the first time self-supervised learning is utilized to perform emotion recognition using ECG. Four public datasets AMIGOS, DREAMER, WESAD and SWELL are used in this study to perform emotion recognition. We set new state-of-the-art results for the four datasets in classification of arousal, valence, affective states, and stress. We show that our proposed approach significantly improves classification performance compared to a fully-supervised method. We explore different self-supervised transformation recognition tasks for learning the ECG representations and analyze their impacts. Our analysis show that for an optimum amount of difficulty in the pretext tasks, the network learns better ECG representations which can used for emotion classification. Our experiments also show that for learning of ECG representations, a multi-task CNN improves the performance compared to a single-task network. Lastly, our analysis on the use multiple datasets for training the self-supervised network shows that utilizing multiple datasets benefits the final downstream classification.\n\nFigure 1 :\n1An overview of our proposed framework for self-supervised emotion recognition is presented.\n\nFigure 2 :\n2Figure 2: The proposed self-supervised architecture is presented. First, a multi-task CNN is trained using automatically generated labels to learn ECG representations. Then, the weights are transferred to the emotion recognition network, where the fully connected layers are trained to classify emotions.\n\nFigure 3 :\n3A sample of an original ECG signal with the six transformed signals are presented.\n\nNegation:\nThe original amplitude of the ECG S(t) is multiplied by \u22121, causing a spatial inversion of the time-series. The transformed signal can be mathematically expressed as \u2212S(t).Temporal Inversion: Given the original ECG signal S(t), where t = 1, 2, ..., N and N is the length of the time-series, the temporally inverted version of the signal is expressed as S (t), where t = N, N \u2212 1, ..., 1.Permutation: In this transformation, the original ECG, S(t), is divided into m segments and shuffled, randomly perturbing the temporal location of each segment. Let's assume S(t) is expressed as S(t) = [s i (t)|i = 1, 2, ..., m], a sequence of segments with segment numbers i = 1, 2, ..., m. Accordingly, the permuted signal S p (t) can be obtained as S p (t) = [s i (t)], where the sequence of i = 1, 2, ..., m is randomly shuffled.Time-warping: In this transformation, randomly selected segments of the original ECG signals S(t) are stretched or squeezed along the x (time) axis. Let's assume \u03a6(S(t), k) is an interpolation-based time-warping function where k is the stretch factor (with the corresponding squeeze factor represented as 1/k). If the signal, S(t), is expressed as S(t) = [s i (t)|i = 1, 2, ..., m], a sequence of segments with segment numbers i = 1, 2, ..., m, the time-warped (transformed) signal T (t) is obtained by applying \u03a6(s i (t), k) to half of the segments selected randomly, and \u03a6(s j (t), 1/k) to the other half of the segments where i = j.\n\nFigure 4 :\n4Training losses vs. training epochs are presented for both networks, the of signal transformation network (left) and the emotion recognition network (right). In (A), the average and standard deviation of individual losses and the total output loss of the signal transformation recognition network are obtained from 10-fold training, while in (B), the average and standard deviation of individual losses for the different emotion recognition tasks obtained with 10-fold training for each dataset are presented.\n\nFigure 5 :\n5The impact of transformation parameters on emotion recognition accuracy is illustrated. Parts (A), (B), and (C) are 2D graphs as noise addition, scaling, and permutation transformations only have one parameter. Parts (D) and (E) are presented in bar-plot format as temporal inversion and negation transformations are operations with no controlling parameter. Lastly, part (F)\n\nFigure 8 :\n8The relationship between emotion recognition accuracy and transformation recognition is presented. Time-warping (Stretch Factor) denotes that the stretch factor is varied while the number of segments is set to the optimum value. Similarly, Time-warping (No. of Segments) indicates that the number of segments is varied while the stretch factor is set to the optimum value.\n\nTable 1 :\n1The architecture of the signal transformation recognition network is presented.Module \nLayer Details \nFeature Shape \nInput \n\u2212 \n2560 \u00d7 1 \n\nShared Layers \n\n[conv, 1 \u00d7 32, 32] \u00d7 2 \n2560 \u00d7 32 \n[maxpool, 1 \u00d7 8, stride = 2] 1277 \u00d7 32 \n[conv, 1 \u00d7 16, 64] \u00d7 2 \n1277 \u00d7 64 \n[maxpool, 1 \u00d7 8, stride = 2] 635 \u00d7 64 \n[conv, 1 \u00d7 8, 128] \u00d7 2 \n635 \u00d7 128 \nglobal max pooling \n1 \u00d7 128 \nTask-Specific \nLayers \n\n[dense] \u00d7 2 \n\u00d7 7 parallel tasks \n128 \n\nOutput \n\u2212 \n2 \n\n\n\nTable 2 :\n2The summary of the four datasets used are presented.Dataset \nParticipants Attributes \nClasses \n\nAMIGOS \n40 \nArousal \nValence \n\n9 \n9 \n\nDREAMER 23 \nArousal \nValence \n\n5 \n5 \nWESAD \n17 \nAffect State 4 \n\nSWELL \n25 \n\nStress \nArousal \nValence \n\n3 \n9 \n9 \n\n\n\nTable 3 :\n3The average and standard deviations of the accuracies and F1 scores for signal transformation recognition across the four datasets are presented.Transformation \nAll datasets combined \nAcc. \nF1 \nOriginal \n0.980 \u00b1 0.003 0.927 \u00b1 0.007 \nNoise Addition \n0.995 \u00b1 0.000 0.979 \u00b1 0.003 \nScaling \n0.982 \u00b1 0.003 0.932 \u00b1 0.010 \nTemporal Inversion 0.998 \u00b1 0.000 0.992 \u00b1 0.004 \nNegation \n0.998 \u00b1 0.000 0.990 \u00b1 0.000 \nPermutation \n0.998 \u00b1 0.000 0.989 \u00b1 0.003 \nTime-warping \n0.997 \u00b1 0.003 0.992 \u00b1 0.006 \nAverage \n0.992 \u00b1 0.001 0.972 \u00b1 0.005 \n\n\n\nTable 4 :\n4Results for the individual transformation recognition tasks in each of the four datasets are presented.Transformation \nAMIGOS \nDREAMER \nWESAD \nSWELL \nAcc. \nF1 \nAcc. \nF1 \nAcc. \nF1 \nAcc. \nF1 \n\nOriginal \n0.973 \u00b1 0.003 \n0.901 \u00b1 0.010 \n0.986 \u00b1 0.005 \n0.953 \u00b1 0.020 \n0.981 \u00b1 0.008 \n0.950 \u00b1 0.014 \n0.984 \u00b1 0.003 \n0.941 \u00b1 0.009 \nNoise Addition \n0.987 \u00b1 0.002 \n0.954 \u00b1 0.008 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \n0.999 \u00b1 0.000 \n0.999 \u00b1 0.000 \nScaling \n0.975 \u00b1 0.003 \n0.904 \u00b1 0.020 \n0.986 \u00b1 0.005 \n0.955 \u00b1 0.024 \n0.984 \u00b1 0.008 \n0.952 \u00b1 0.012 \n0.987 \u00b1 0.001 \n0.948 \u00b1 0.006 \nTemporal Inversion \n0.997 \u00b1 0.001 \n0.988 \u00b1 0.004 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \n0.999 \u00b1 0.001 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \nNegation \n0.996 \u00b1 0.001 \n0.987 \u00b1 0.005 \n1.000 \u00b1 0.000 \n1.000 \u00b1 0.000 \n0.999 \u00b1 0.002 \n0.995 \u00b1 0.007 \n1.000 \u00b1 0.000 \n0.998 \u00b1 0.004 \nPermutation \n0.996 \u00b1 0.001 \n0.984 \u00b1 0.005 \n0.999 \u00b1 0.000 \n0.999 \u00b1 0.003 \n0.999 \u00b1 0.001 \n0.994 \u00b1 0.007 \n0.998 \u00b1 0.001 \n0.991 \u00b1 0.009 \nTime-warping \n0.998 \u00b1 0.001 \n0.993 \u00b1 0.007 \n0.998 \u00b1 0.003 \n0.999 \u00b1 0.003 \n0.993 \u00b1 0.009 \n0.991 \u00b1 0.006 \n0.996 \u00b1 0.004 \n0.990 \u00b1 0.005 \nAverage \n0.989 \u00b1 0.002 \n0.959 \u00b1 0.008 \n0.996 \u00b1 0.002 \n0.987 \u00b1 0.004 \n0.994 \u00b1 0.004 \n0.983 \u00b1 0.007 \n0.995 \u00b1 0.001 \n0.981 \u00b1 0.005 \n\n\n\nTable 5 :\n5The emotion categories, number of classes for each class, and multi-class emotion recognition results are presented for each of the four datasets.Dataset \nAttribute \nClasses \nAcc. \nF1 \n\nAMIGOS \nArousal \n9 \n0.796 0.777 \nValence \n9 \n0.783 0.765 \n\nDREAMER \nArousal \n5 \n0.771 0.740 \nValence \n5 \n0.749 0.747 \nWESAD \nAffect State 4 \n0.950 0.940 \n\nSWELL \n\nArousal \n9 \n0.926 0.930 \nValence \n9 \n0.938 0.943 \nStress \n3 \n0.902 0.900 \n\n\n\nTable 6 :\n6The results of our self-supervised method on all the datasets are presented and compared with prior works including the state-of-the-art, as well as a fully-supervised CNN as a baseline.A: AMIGOS \n\nRef. \nMethod \nArousal \nValence \nAcc. \nF1 \nAcc. \nF1 \n[5] \nGNB \n-\n0.545 -\n0.551 \n[29] \nCNN \n0.81 \n0.76 \n0.71 \n0.68 \n\nOurs \nFully-Supervised CNN 0.844 0.835 0.811 0.809 \nSelf-Supervised CNN \n0.889 0.884 0.875 0.874 \n\nB: DREAMER \n\nRef. \nMethod \nArousal \nValence \nAcc. \nF1 \nAcc. \nF1 \n[23] \nSVM \n0.624 0.580 0.624 0.531 \n\nOurs \nFully-Supervised CNN 0.707 0.708 0.666 0.658 \nSelf-Supervised CNN \n0.859 0.859 0.850 0.845 \n\nC: WESAD \n\nRef. \nMethod \nAffect State \nAcc. \nF1 \n\n[24] \n\nkNN \n0.548 0.478 \nDT \n0.578 0.517 \nRF \n0.604 0.522 \nAB \n0.617 0.525 \nLDA \n0.663 0.560 \n[31] \nCNN \n0.83 \n0.81 \n\nOurs \nFully-Supervised CNN 0.932 0.912 \nSelf-Supervised CNN \n0.969 0.963 \n\nD: SWELL \n\nRef. \nMethod \nStress \nArousal \nValence \nAcc. \nF1 \nAcc. \nF1 \nAcc. \nF1 \n\n\n\nAffective computing. W Rosalind, Picard, MIT pressRosalind W Picard. Affective computing. MIT press, 2000.\n\nToward machine emotional intelligence: Analysis of affective physiological state. Rosalind W Picard, Elias Vyzas, Jennifer Healey, IEEE Transactions on Pattern Analysis & Machine Intelligence. 10Rosalind W. Picard, Elias Vyzas, and Jennifer Healey. Toward machine emotional intelligence: Analysis of affective physiological state. IEEE Transactions on Pattern Analysis & Machine Intelligence, (10):1175-1191, 2001.\n\nEnhancing access to ieee conference proceedings: a case study in the application of ieee xplore full text and table of contents enhancements. Cherie Madarash, - Hill, Hill, Science & Technology Libraries. 243-4Cherie Madarash-Hill and JB Hill. Enhancing access to ieee conference proceedings: a case study in the application of ieee xplore full text and table of contents enhancements. Science & Technology Libraries, 24(3-4):389-399, 2004.\n\nClassification of cognitive load and expertise for adaptive simulation using deep multitask learning. Pritam Sarkar, Kyle Ross, Aaron J Ruberto, Dirk Rodenbura, Paul Hungler, Ali Etemad, 8th International Conference on Affective Computing and Intelligent Interaction. IEEEPritam Sarkar, Kyle Ross, Aaron J Ruberto, Dirk Rodenbura, Paul Hungler, and Ali Etemad. Classification of cognitive load and expertise for adaptive simulation using deep multitask learning. In 8th International Conference on Affective Computing and Intelligent Interaction, pages 1-7. IEEE, 2019.\n\nAmigos: a dataset for affect, personality and mood research on individuals and groups. Juan Abdon Miranda Correa, Mojtaba Khomami Abadi, Niculae Sebe, Ioannis Patras, IEEE Transactions on Affective Computing. Juan Abdon Miranda Correa, Mojtaba Khomami Abadi, Niculae Sebe, and Ioannis Patras. Amigos: a dataset for affect, personality and mood research on individuals and groups. IEEE Transactions on Affective Computing, 2018.\n\nDeap: A database for emotion analysis; using physiological signals. Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, IEEE Transactions on Affective Computing. 31Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological signals. IEEE Transactions on Affective Computing, 3(1):18-31, 2011.\n\nPersonalized multitask learning for predicting tomorrow's mood, stress, and health. Natasha Sara Ann Taylor, Ehimwenma Jaques, Akane Nosakhare, Rosalind Sano, Picard, IEEE Transactions on Affective Computing. Sara Ann Taylor, Natasha Jaques, Ehimwenma Nosakhare, Akane Sano, and Rosalind Picard. Personalized multitask learning for predicting tomorrow's mood, stress, and health. IEEE Transactions on Affective Computing, 2017.\n\nA deep learning approach to monitoring and detecting atrial fibrillation using wearable technology. Supreeth Prajwal Shashikumar, J Amit, Qiao Shah, Li, D Gari, Shamim Clifford, Nemati, IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). IEEESupreeth Prajwal Shashikumar, Amit J Shah, Qiao Li, Gari D Clifford, and Shamim Nemati. A deep learning approach to monitoring and detecting atrial fibrillation using wearable technology. In IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), pages 141-144. IEEE, 2017.\n\nDetecting stress during real-world driving tasks using physiological sensors. A Jennifer, Rosalind W Healey, Picard, IEEE Transactions on Intelligent Transportation Systems. 62Jennifer A Healey and Rosalind W Picard. Detecting stress during real-world driving tasks using physiological sensors. IEEE Transactions on Intelligent Transportation Systems, 6(2):156-166, 2005.\n\nThe swell knowledge work dataset for stress and user modeling research. Saskia Koldijk, Maya Sappelli, Suzan Verberne, A Mark, Wessel Neerincx, Kraaij, Proceedings of the 16th International Conference on Multimodal Interaction. the 16th International Conference on Multimodal InteractionACMSaskia Koldijk, Maya Sappelli, Suzan Verberne, Mark A Neerincx, and Wessel Kraaij. The swell knowledge work dataset for stress and user modeling research. In Proceedings of the 16th International Conference on Multimodal Interaction, pages 291-298. ACM, 2014.\n\nAscertain: Emotion and personality recognition using commercial sensors. Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, L Radu, Stefan Vieriu, Nicu Winkler, Sebe, IEEE Transactions on Affective Computing. 92Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu Sebe. Ascertain: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2):147-160, 2016.\n\nA deep framework for facial emotion recognition using light field images. Alireza Sepas-Moghaddam, Ali Etemad, Paulo Lobato Correia, Fernando Pereira, 8th International Conference on Affective Computing and Intelligent Interaction. IEEEAlireza Sepas-Moghaddam, Ali Etemad, Paulo Lobato Correia, and Fernando Pereira. A deep framework for facial emotion recognition using light field images. In 8th International Conference on Affective Computing and Intelligent Interaction, pages 1-7. IEEE, 2019.\n\nClassification and translation of style and affect in human motion using rbf neural networks. Ali Etemad, Ali Arya, Neurocomputing. 129S Ali Etemad and Ali Arya. Classification and translation of style and affect in human motion using rbf neural networks. Neurocomputing, 129:585-595, 2014.\n\nEmotion recognition by speech signals. Kwokleung Oh-Wook Kwon, Jiucang Chan, Te-Won Hao, Lee, Eighth European Conference on Speech Communication and Technology. Oh-Wook Kwon, Kwokleung Chan, Jiucang Hao, and Te-Won Lee. Emotion recognition by speech signals. In Eighth European Conference on Speech Communication and Technology, 2003.\n\nEcg pattern analysis for emotion detection. Foteini Agrafioti, Dimitris Hatzinakos, Adam K Anderson, IEEE Transactions on Affective Computing. 31Foteini Agrafioti, Dimitris Hatzinakos, and Adam K Anderson. Ecg pattern analysis for emotion detection. IEEE Transactions on Affective Computing, 3(1):102-115, 2011.\n\nCapsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation. Guangyi Zhang, Ali Etemad, arXiv:1912.07812arXiv preprintGuangyi Zhang and Ali Etemad. Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation. arXiv preprint arXiv:1912.07812, 2019.\n\nStress detection in computer users based on digital signal processing of noninvasive physiological variables. Jing Zhai, Armando Barreto, International Conference of the IEEE Engineering in Medicine and Biology Society. IEEEJing Zhai and Armando Barreto. Stress detection in computer users based on digital signal processing of noninvasive physiological variables. In International Conference of the IEEE Engineering in Medicine and Biology Society, pages 1355-1358. IEEE, 2006.\n\nUtilizing deep learning towards multi-modal bio-sensing and vision-based affective computing. Siddharth Siddharth, Tzyy-Ping Jung, Terrence J Sejnowski, IEEE Transactions on Affective Computing. Siddharth Siddharth, Tzyy-Ping Jung, and Terrence J Sejnowski. Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing. IEEE Transactions on Affective Computing, 2019.\n\nRecognizing emotions induced by affective sounds through heart rate variability. Mimma Nardelli, Gaetano Valenza, Alberto Greco, Antonio Lanata, Enzo Pasquale Scilingo, IEEE Transactions on Affective Computing. 64Mimma Nardelli, Gaetano Valenza, Alberto Greco, Antonio Lanata, and Enzo Pasquale Scilingo. Recognizing emotions induced by affective sounds through heart rate variability. IEEE Transactions on Affective Computing, 6(4):385-394, 2015.\n\nSelf-taught learning: transfer learning from unlabeled data. Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, Andrew Y Ng, Proceedings of the 24th International Conference on Machine Learning. the 24th International Conference on Machine LearningACMRajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y Ng. Self-taught learning: transfer learning from unlabeled data. In Proceedings of the 24th International Conference on Machine Learning, pages 759-766. ACM, 2007.\n\nTransitive invariance for self-supervised visual representation learning. Xiaolong Wang, Kaiming He, Abhinav Gupta, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive invariance for self-supervised visual representation learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 1329-1338, 2017.\n\nSelf-supervised learning for ecg-based emotion recognition. Pritam Sarkar, Ali Etemad, arXiv:1910.07497arXiv preprintPritam Sarkar and Ali Etemad. Self-supervised learning for ecg-based emotion recognition. arXiv preprint arXiv:1910.07497, 2019.\n\nDreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices. Stamos Katsigiannis, Naeem Ramzan, IEEE Journal of Biomedical and Health Informatics. 221Stamos Katsigiannis and Naeem Ramzan. Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices. IEEE Journal of Biomedical and Health Informatics, 22(1):98-107, 2017.\n\nIntroducing wesad, a multimodal dataset for wearable stress and affect detection. Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, Kristof Van Laerhoven, Proceedings of the International Conference on Multimodal Interaction. the International Conference on Multimodal InteractionACMPhilip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven. Introducing wesad, a multimodal dataset for wearable stress and affect detection. In Proceedings of the International Conference on Multimodal Interaction, pages 400-408. ACM, 2018.\n\nCardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. Pranav Awni Y Hannun, Masoumeh Rajpurkar, Haghpanahi, H Geoffrey, Codie Tison, Bourn, P Mintu, Andrew Y Turakhia, Ng, Nature Medicine. 25165Awni Y Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoffrey H Tison, Codie Bourn, Mintu P Turakhia, and Andrew Y Ng. Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. Nature Medicine, 25(1):65, 2019.\n\nAutomatic ecg-based emotion recognition in music listening. Yu-Liang Hsu, Jeen-Shing Wang, Wei-Chun Chiang, Chien-Han Hung, IEEE Transactions on Affective Computing. Yu-Liang Hsu, Jeen-Shing Wang, Wei-Chun Chiang, and Chien-Han Hung. Automatic ecg-based emotion recognition in music listening. IEEE Transactions on Affective Computing, 2017.\n\nUtilizing deep neural nets for an embedded ecg-based biometric authentication system. Adam Page, Amey Kulkarni, Tinoosh Mohsenin, IEEE Biomedical Circuits and Systems Conference. IEEEAdam Page, Amey Kulkarni, and Tinoosh Mohsenin. Utilizing deep neural nets for an embedded ecg-based biometric authentication system. In IEEE Biomedical Circuits and Systems Conference, pages 1-4. IEEE, 2015.\n\nA method of emotion recognition based on ecg signal. Ya Xu, Guang-Yuan Liu, International Conference on Computational Intelligence and Natural Computing. IEEE1Ya Xu and Guang-Yuan Liu. A method of emotion recognition based on ecg signal. In International Conference on Computational Intelligence and Natural Computing, volume 1, pages 202-205. IEEE, 2009.\n\nUsing deep convolutional neural network for emotion detection on a physiological signals dataset (amigos). Luz Santamaria-Granados, Mario Munoz-Organero, Gustavo Ramirez-Gonzalez, Enas Abdulhay, IEEE Access. 7Luz Santamaria-Granados, Mario Munoz-Organero, Gustavo Ramirez-Gonzalez, Enas Abdulhay, and NJIA Arunkumar. Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos). IEEE Access, 7:57-67, 2018.\n\nToward dynamically adaptive simulation: Multimodal classification of user expertise using wearable devices. Kyle Ross, Pritam Sarkar, Dirk Rodenburg, Aaron Ruberto, Paul Hungler, Adam Szulewski, Daniel Howes, Ali Etemad, Sensors. 19194270Kyle Ross, Pritam Sarkar, Dirk Rodenburg, Aaron Ruberto, Paul Hungler, Adam Szulewski, Daniel Howes, and Ali Etemad. Toward dynamically adaptive simulation: Multimodal classification of user expertise using wearable devices. Sensors, 19(19):4270, 2019.\n\nAn explainable deep fusion network for affect recognition using physiological signals. Jionghao Lin, Shirui Pan, Sharon Cheng Siong Lee, Oviatt, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementACMJionghao Lin, Shirui Pan, Cheng Siong Lee, and Sharon Oviatt. An explainable deep fusion network for affect recognition using physiological signals. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2069-2072. ACM, 2019.\n\nStress detection in working people. S Sriramprakash, D Vadana, Prasanna, Murthy, Procedia Computer Science. 115S Sriramprakash, Vadana D Prasanna, and OV Ramana Murthy. Stress detection in working people. Procedia Computer Science, 115:359-366, 2017.\n\nSelf-supervised learning of 3d human pose using multi-view geometry. Muhammed Kocabas, Salih Karagoz, Emre Akbas, arXiv:1903.02330arXiv preprintMuhammed Kocabas, Salih Karagoz, and Emre Akbas. Self-supervised learning of 3d human pose using multi-view geometry. arXiv preprint arXiv:1903.02330, 2019.\n\n3d human pose machines with self-supervised learning. Keze Wang, Liang Lin, Chenhan Jiang, Chen Qian, Pengxu Wei, IEEE Transactions on Pattern Analysis and Machine Intelligence. Keze Wang, Liang Lin, Chenhan Jiang, Chen Qian, and Pengxu Wei. 3d human pose machines with self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n\nSelf-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics. Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, Wei Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4006-4015, 2019.\n\nF\u00e9lix de Chaumont Quitry, and Dominik Roblek. Self-supervised audio representation learning for mobile devices. Marco Tagliasacchi, Beat Gfeller, arXiv:1905.11796arXiv preprintMarco Tagliasacchi, Beat Gfeller, F\u00e9lix de Chaumont Quitry, and Dominik Roblek. Self-supervised audio representation learning for mobile devices. arXiv preprint arXiv:1905.11796, 2019.\n\nOpen information extraction using wikipedia. Fei Wu, Daniel S Weld, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsFei Wu and Daniel S Weld. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118-127. Association for Computational Linguistics, 2010.\n\nSelf-supervised spatiotemporal feature learning via video rotation prediction. Longlong Jing, Xiaodong Yang, Jingen Liu, Yingli Tian, arXiv:1811.11387arXiv preprintLonglong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian. Self-supervised spatiotemporal feature learning via video rotation prediction. arXiv preprint arXiv:1811.11387, 2018.\n\nMulti-task self-supervised learning for human activity detection. Aaqib Saeed, Tanir Ozcelebi, Johan Lukkien, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies361Aaqib Saeed, Tanir Ozcelebi, and Johan Lukkien. Multi-task self-supervised learning for human activity detection. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(2):61, 2019.\n\nData augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks. Franz Michael Josef Terry Taewoong Um, Daniel Pfister, Satoshi Pichler, Muriel Endo, Sandra Lang, Urban Hirche, Dana Fietzek, Kuli\u0107, arXiv:1706.00527arXiv preprintTerry Taewoong Um, Franz Michael Josef Pfister, Daniel Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, and Dana Kuli\u0107. Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks. arXiv preprint arXiv:1706.00527, 2017.\n\nShimmer ecg. Shimmer ecg. [Online]. Available: http://www.shimmersensing.com/products/ shimmer3-ecg-sensor. [Accessed: 2019-09-17].\n\nMeasuring emotion: the self-assessment manikin and the semantic differential. M Margaret, Peter J Bradley, Lang, Journal of Behavior Therapy and Experimental Psychiatry. 251Margaret M Bradley and Peter J Lang. Measuring emotion: the self-assessment manikin and the semantic differential. Journal of Behavior Therapy and Experimental Psychiatry, 25(1):49-59, 1994.\n\nRespiban professional. Respiban professional. [Online]. Available: https://www.biosignalsplux.com/index.php/ respiban-professional. [Accessed: 2019-09-17].\n\n. Tmsi-Mobi, Accessed: 2019-09-17Tmsi-mobi. [Online]. Available: https://www.tmsi.com/products/mobi/. [Accessed: 2019-09-17].\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nA bayesian deep learning framework for end-to-end prediction of emotion from heartbeat. Ross Harper, Joshua Southern, arXiv:1902.03043arXiv preprintRoss Harper and Joshua Southern. A bayesian deep learning framework for end-to-end prediction of emotion from heartbeat. arXiv preprint arXiv:1902.03043, 2019.\n\nMultimodal emotion recognition using deep canonical correlation analysis. Wei Liu, Jie-Lin Qiu, Wei-Long Zheng, Bao-Liang Lu, arXiv:1908.05349arXiv preprintWei Liu, Jie-Lin Qiu, Wei-Long Zheng, and Bao-Liang Lu. Multimodal emotion recognition using deep canonical correlation analysis. arXiv preprint arXiv:1908.05349, 2019.\n\nDetecting work stress in offices by combining unobtrusive sensors. Saskia Koldijk, A Mark, Wessel Neerincx, Kraaij, IEEE Transactions on Affective Computing. 92Saskia Koldijk, Mark A Neerincx, and Wessel Kraaij. Detecting work stress in offices by combining unobtrusive sensors. IEEE Transactions on Affective Computing, 9(2):227-239, 2016.\n\nA comprehensive framework for student stress monitoring in fog-cloud iot environment: m-health perspective. Prabal Verma, K Sandeep, Sood, Medical & Biological Engineering & Computing. 571Prabal Verma and Sandeep K Sood. A comprehensive framework for student stress monitoring in fog-cloud iot environment: m-health perspective. Medical & Biological Engineering & Computing, 57(1):231-244, 2019.\n\nUsing smart offices to predict occupational stress. Ane Alberdi, Asier Aztiria, Adrian Basarab, Diane J Cook, International Journal of Industrial Ergonomics. 67Ane Alberdi, Asier Aztiria, Adrian Basarab, and Diane J Cook. Using smart offices to predict occupational stress. International Journal of Industrial Ergonomics, 67:13-26, 2018.\n", "annotations": {"author": "[{\"end\":319,\"start\":86},{\"end\":547,\"start\":320}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":93},{\"end\":330,\"start\":324}]", "author_first_name": "[{\"end\":92,\"start\":86},{\"end\":323,\"start\":320}]", "author_affiliation": "[{\"end\":318,\"start\":126},{\"end\":546,\"start\":354}]", "title": "[{\"end\":83,\"start\":1},{\"end\":630,\"start\":548}]", "venue": null, "abstract": "[{\"end\":2136,\"start\":706}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2324,\"start\":2321},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2326,\"start\":2324},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2578,\"start\":2575},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2814,\"start\":2811},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2817,\"start\":2814},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2820,\"start\":2817},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2901,\"start\":2898},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2903,\"start\":2901},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2942,\"start\":2939},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2945,\"start\":2942},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2986,\"start\":2983},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2989,\"start\":2986},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3138,\"start\":3134},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3160,\"start\":3156},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3173,\"start\":3169},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3203,\"start\":3199},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3235,\"start\":3232},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3264,\"start\":3260},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3303,\"start\":3299},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3424,\"start\":3420},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3756,\"start\":3753},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3759,\"start\":3756},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4116,\"start\":4112},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4623,\"start\":4619},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4946,\"start\":4942},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5666,\"start\":5662},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5719,\"start\":5716},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5733,\"start\":5729},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5745,\"start\":5741},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5761,\"start\":5757},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8021,\"start\":8017},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9915,\"start\":9911},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10308,\"start\":10304},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10342,\"start\":10338},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10345,\"start\":10342},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10376,\"start\":10372},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10589,\"start\":10585},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10701,\"start\":10698},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10704,\"start\":10701},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10819,\"start\":10815},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10977,\"start\":10973},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10980,\"start\":10977},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11176,\"start\":11173},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11179,\"start\":11176},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11521,\"start\":11518},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11524,\"start\":11521},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11527,\"start\":11524},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11530,\"start\":11527},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11728,\"start\":11725},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12159,\"start\":12155},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12929,\"start\":12925},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13062,\"start\":13059},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13575,\"start\":13572},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14570,\"start\":14567},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14573,\"start\":14570},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14576,\"start\":14573},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14579,\"start\":14576},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14582,\"start\":14579},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14592,\"start\":14588},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14726,\"start\":14723},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14929,\"start\":14925},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15056,\"start\":15052},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15214,\"start\":15210},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15834,\"start\":15830},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15955,\"start\":15951},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16258,\"start\":16254},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16800,\"start\":16796},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16804,\"start\":16800},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16808,\"start\":16804},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16812,\"start\":16808},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16825,\"start\":16821},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16859,\"start\":16855},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17159,\"start\":17155},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17593,\"start\":17589},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18139,\"start\":18135},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21519,\"start\":21515},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21522,\"start\":21519},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26363,\"start\":26360},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26832,\"start\":26828},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27059,\"start\":27055},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27577,\"start\":27573},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27710,\"start\":27706},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28032,\"start\":28028},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28938,\"start\":28934},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29568,\"start\":29564},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31336,\"start\":31332},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36134,\"start\":36131},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36137,\"start\":36134},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36140,\"start\":36137},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36143,\"start\":36140},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":37003,\"start\":36999},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":37412,\"start\":37408},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37795,\"start\":37791},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37994,\"start\":37990},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38103,\"start\":38099},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38577,\"start\":38573},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38586,\"start\":38582},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":38641,\"start\":38637},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38645,\"start\":38641},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38649,\"start\":38645},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":38756,\"start\":38752},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":51726,\"start\":51723},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":51735,\"start\":51731},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":52051,\"start\":52048}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54125,\"start\":54021},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54443,\"start\":54126},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54539,\"start\":54444},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56006,\"start\":54540},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56529,\"start\":56007},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56918,\"start\":56530},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57304,\"start\":56919},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57762,\"start\":57305},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":58023,\"start\":57763},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":58563,\"start\":58024},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":59829,\"start\":58564},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":60266,\"start\":59830},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61217,\"start\":60267}]", "paragraph": "[{\"end\":3002,\"start\":2152},{\"end\":4530,\"start\":3004},{\"end\":5466,\"start\":4532},{\"end\":6357,\"start\":5468},{\"end\":6491,\"start\":6359},{\"end\":6638,\"start\":6493},{\"end\":7056,\"start\":6640},{\"end\":7554,\"start\":7058},{\"end\":7776,\"start\":7556},{\"end\":7976,\"start\":7778},{\"end\":9177,\"start\":7978},{\"end\":9673,\"start\":9179},{\"end\":9704,\"start\":9675},{\"end\":11180,\"start\":9728},{\"end\":11638,\"start\":11214},{\"end\":12132,\"start\":11640},{\"end\":12860,\"start\":12134},{\"end\":13442,\"start\":12862},{\"end\":14325,\"start\":13444},{\"end\":14533,\"start\":14327},{\"end\":16534,\"start\":14535},{\"end\":18481,\"start\":16578},{\"end\":18787,\"start\":18483},{\"end\":20437,\"start\":18807},{\"end\":21234,\"start\":20489},{\"end\":21428,\"start\":21314},{\"end\":21847,\"start\":21430},{\"end\":22303,\"start\":21849},{\"end\":22606,\"start\":22305},{\"end\":23038,\"start\":22608},{\"end\":23966,\"start\":23040},{\"end\":24516,\"start\":23990},{\"end\":24589,\"start\":24544},{\"end\":25944,\"start\":24591},{\"end\":26330,\"start\":25971},{\"end\":27023,\"start\":26341},{\"end\":27893,\"start\":27035},{\"end\":28760,\"start\":27903},{\"end\":29724,\"start\":28770},{\"end\":30626,\"start\":29748},{\"end\":32571,\"start\":30658},{\"end\":33946,\"start\":32583},{\"end\":34473,\"start\":33984},{\"end\":35867,\"start\":34497},{\"end\":38831,\"start\":35869},{\"end\":38942,\"start\":38859},{\"end\":40312,\"start\":38975},{\"end\":41882,\"start\":40314},{\"end\":43801,\"start\":41884},{\"end\":46131,\"start\":43833},{\"end\":47188,\"start\":46133},{\"end\":48223,\"start\":47243},{\"end\":48895,\"start\":48225},{\"end\":50392,\"start\":48918},{\"end\":51150,\"start\":50427},{\"end\":52352,\"start\":51182},{\"end\":52777,\"start\":52354},{\"end\":54020,\"start\":52792}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":21285,\"start\":21235},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21313,\"start\":21285},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24543,\"start\":24517}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23965,\"start\":23958},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26269,\"start\":26262},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32920,\"start\":32913},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33045,\"start\":33038},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34143,\"start\":34136},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34575,\"start\":34568},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34849,\"start\":34842},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35054,\"start\":35047},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35993,\"start\":35986},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36335,\"start\":36328},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36347,\"start\":36340},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36531,\"start\":36524},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37188,\"start\":37181},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37933,\"start\":37926},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38181,\"start\":38174},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38539,\"start\":38532}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2150,\"start\":2138},{\"attributes\":{\"n\":\"2.1\"},\"end\":9726,\"start\":9707},{\"attributes\":{\"n\":\"2.2\"},\"end\":11212,\"start\":11183},{\"attributes\":{\"n\":\"2.3\"},\"end\":16576,\"start\":16537},{\"attributes\":{\"n\":\"3\"},\"end\":18805,\"start\":18790},{\"attributes\":{\"n\":\"3.1\"},\"end\":20487,\"start\":20440},{\"attributes\":{\"n\":\"3.2\"},\"end\":23988,\"start\":23969},{\"attributes\":{\"n\":\"4\"},\"end\":25958,\"start\":25947},{\"attributes\":{\"n\":\"4.1\"},\"end\":25969,\"start\":25961},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":26339,\"start\":26333},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":27033,\"start\":27026},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":27901,\"start\":27896},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":28768,\"start\":28763},{\"attributes\":{\"n\":\"4.2\"},\"end\":29746,\"start\":29727},{\"attributes\":{\"n\":\"4.3\"},\"end\":30656,\"start\":30629},{\"attributes\":{\"n\":\"5\"},\"end\":32581,\"start\":32574},{\"attributes\":{\"n\":\"5.1\"},\"end\":33982,\"start\":33949},{\"attributes\":{\"n\":\"5.2\"},\"end\":34495,\"start\":34476},{\"attributes\":{\"n\":\"6\"},\"end\":38857,\"start\":38834},{\"attributes\":{\"n\":\"6.1\"},\"end\":38973,\"start\":38945},{\"attributes\":{\"n\":\"6.2\"},\"end\":43831,\"start\":43804},{\"attributes\":{\"n\":\"6.3\"},\"end\":47241,\"start\":47191},{\"attributes\":{\"n\":\"6.4\"},\"end\":48916,\"start\":48898},{\"attributes\":{\"n\":\"6.5\"},\"end\":50425,\"start\":50395},{\"attributes\":{\"n\":\"6.6\"},\"end\":51180,\"start\":51153},{\"attributes\":{\"n\":\"7\"},\"end\":52790,\"start\":52780},{\"end\":54032,\"start\":54022},{\"end\":54137,\"start\":54127},{\"end\":54455,\"start\":54445},{\"end\":54550,\"start\":54541},{\"end\":56018,\"start\":56008},{\"end\":56541,\"start\":56531},{\"end\":56930,\"start\":56920},{\"end\":57315,\"start\":57306},{\"end\":57773,\"start\":57764},{\"end\":58034,\"start\":58025},{\"end\":58574,\"start\":58565},{\"end\":59840,\"start\":59831},{\"end\":60277,\"start\":60268}]", "table": "[{\"end\":57762,\"start\":57396},{\"end\":58023,\"start\":57827},{\"end\":58563,\"start\":58181},{\"end\":59829,\"start\":58679},{\"end\":60266,\"start\":59988},{\"end\":61217,\"start\":60465}]", "figure_caption": "[{\"end\":54125,\"start\":54034},{\"end\":54443,\"start\":54139},{\"end\":54539,\"start\":54457},{\"end\":56006,\"start\":54551},{\"end\":56529,\"start\":56020},{\"end\":56918,\"start\":56543},{\"end\":57304,\"start\":56932},{\"end\":57396,\"start\":57317},{\"end\":57827,\"start\":57775},{\"end\":58181,\"start\":58036},{\"end\":58679,\"start\":58576},{\"end\":59988,\"start\":59842},{\"end\":60465,\"start\":60279}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19805,\"start\":19797},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21846,\"start\":21838},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25792,\"start\":25784},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31766,\"start\":31758},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31917,\"start\":31909},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32177,\"start\":32169},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32447,\"start\":32439},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39977,\"start\":39969},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40750,\"start\":40742},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40937,\"start\":40929},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41318,\"start\":41310},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41645,\"start\":41637},{\"end\":43085,\"start\":43077},{\"end\":43574,\"start\":43566},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":44537,\"start\":44529},{\"end\":44770,\"start\":44762},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45176,\"start\":45168},{\"end\":45313,\"start\":45305},{\"end\":45525,\"start\":45517},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":47419,\"start\":47411},{\"end\":49170,\"start\":49162},{\"end\":49292,\"start\":49284},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49514,\"start\":49505},{\"end\":49837,\"start\":49829},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50776,\"start\":50767}]", "bib_author_first_name": "[{\"end\":61241,\"start\":61240},{\"end\":61417,\"start\":61409},{\"end\":61419,\"start\":61418},{\"end\":61433,\"start\":61428},{\"end\":61449,\"start\":61441},{\"end\":61891,\"start\":61885},{\"end\":61903,\"start\":61902},{\"end\":62293,\"start\":62287},{\"end\":62306,\"start\":62302},{\"end\":62318,\"start\":62313},{\"end\":62320,\"start\":62319},{\"end\":62334,\"start\":62330},{\"end\":62350,\"start\":62346},{\"end\":62363,\"start\":62360},{\"end\":62861,\"start\":62843},{\"end\":62877,\"start\":62870},{\"end\":62900,\"start\":62893},{\"end\":62914,\"start\":62907},{\"end\":63259,\"start\":63253},{\"end\":63279,\"start\":63270},{\"end\":63294,\"start\":63286},{\"end\":63315,\"start\":63306},{\"end\":63327,\"start\":63321},{\"end\":63344,\"start\":63337},{\"end\":63362,\"start\":63355},{\"end\":63782,\"start\":63775},{\"end\":63809,\"start\":63800},{\"end\":63823,\"start\":63818},{\"end\":63843,\"start\":63835},{\"end\":64251,\"start\":64250},{\"end\":64262,\"start\":64258},{\"end\":64274,\"start\":64273},{\"end\":64287,\"start\":64281},{\"end\":64762,\"start\":64761},{\"end\":64781,\"start\":64773},{\"end\":64783,\"start\":64782},{\"end\":65134,\"start\":65128},{\"end\":65148,\"start\":65144},{\"end\":65164,\"start\":65159},{\"end\":65176,\"start\":65175},{\"end\":65189,\"start\":65183},{\"end\":65690,\"start\":65680},{\"end\":65709,\"start\":65704},{\"end\":65741,\"start\":65740},{\"end\":65754,\"start\":65748},{\"end\":65767,\"start\":65763},{\"end\":66150,\"start\":66143},{\"end\":66171,\"start\":66168},{\"end\":66185,\"start\":66180},{\"end\":66192,\"start\":66186},{\"end\":66210,\"start\":66202},{\"end\":66665,\"start\":66662},{\"end\":66677,\"start\":66674},{\"end\":66908,\"start\":66899},{\"end\":66930,\"start\":66923},{\"end\":66943,\"start\":66937},{\"end\":67247,\"start\":67240},{\"end\":67267,\"start\":67259},{\"end\":67286,\"start\":67280},{\"end\":67649,\"start\":67642},{\"end\":67660,\"start\":67657},{\"end\":68016,\"start\":68012},{\"end\":68030,\"start\":68023},{\"end\":68485,\"start\":68476},{\"end\":68506,\"start\":68497},{\"end\":68521,\"start\":68513},{\"end\":68523,\"start\":68522},{\"end\":68869,\"start\":68864},{\"end\":68887,\"start\":68880},{\"end\":68904,\"start\":68897},{\"end\":68919,\"start\":68912},{\"end\":68941,\"start\":68928},{\"end\":69298,\"start\":69293},{\"end\":69312,\"start\":69306},{\"end\":69328,\"start\":69321},{\"end\":69342,\"start\":69334},{\"end\":69359,\"start\":69351},{\"end\":69808,\"start\":69800},{\"end\":69822,\"start\":69815},{\"end\":69834,\"start\":69827},{\"end\":70245,\"start\":70239},{\"end\":70257,\"start\":70254},{\"end\":70550,\"start\":70544},{\"end\":70570,\"start\":70565},{\"end\":70949,\"start\":70943},{\"end\":70965,\"start\":70959},{\"end\":70979,\"start\":70973},{\"end\":70996,\"start\":70991},{\"end\":71015,\"start\":71008},{\"end\":71561,\"start\":71555},{\"end\":71585,\"start\":71577},{\"end\":71610,\"start\":71609},{\"end\":71626,\"start\":71621},{\"end\":71642,\"start\":71641},{\"end\":71658,\"start\":71650},{\"end\":72038,\"start\":72030},{\"end\":72054,\"start\":72044},{\"end\":72069,\"start\":72061},{\"end\":72087,\"start\":72078},{\"end\":72403,\"start\":72399},{\"end\":72414,\"start\":72410},{\"end\":72432,\"start\":72425},{\"end\":72761,\"start\":72759},{\"end\":72776,\"start\":72766},{\"end\":73173,\"start\":73170},{\"end\":73200,\"start\":73195},{\"end\":73224,\"start\":73217},{\"end\":73247,\"start\":73243},{\"end\":73628,\"start\":73624},{\"end\":73641,\"start\":73635},{\"end\":73654,\"start\":73650},{\"end\":73671,\"start\":73666},{\"end\":73685,\"start\":73681},{\"end\":73699,\"start\":73695},{\"end\":73717,\"start\":73711},{\"end\":73728,\"start\":73725},{\"end\":74103,\"start\":74095},{\"end\":74115,\"start\":74109},{\"end\":74127,\"start\":74121},{\"end\":74639,\"start\":74638},{\"end\":74656,\"start\":74655},{\"end\":74931,\"start\":74923},{\"end\":74946,\"start\":74941},{\"end\":74960,\"start\":74956},{\"end\":75214,\"start\":75210},{\"end\":75226,\"start\":75221},{\"end\":75239,\"start\":75232},{\"end\":75251,\"start\":75247},{\"end\":75264,\"start\":75258},{\"end\":75646,\"start\":75638},{\"end\":75659,\"start\":75653},{\"end\":75673,\"start\":75666},{\"end\":75688,\"start\":75679},{\"end\":75699,\"start\":75693},{\"end\":75708,\"start\":75705},{\"end\":76273,\"start\":76268},{\"end\":76292,\"start\":76288},{\"end\":76566,\"start\":76563},{\"end\":77103,\"start\":77095},{\"end\":77118,\"start\":77110},{\"end\":77131,\"start\":77125},{\"end\":77143,\"start\":77137},{\"end\":77429,\"start\":77424},{\"end\":77442,\"start\":77437},{\"end\":77458,\"start\":77453},{\"end\":77972,\"start\":77953},{\"end\":77998,\"start\":77992},{\"end\":78015,\"start\":78008},{\"end\":78031,\"start\":78025},{\"end\":78044,\"start\":78038},{\"end\":78056,\"start\":78051},{\"end\":78069,\"start\":78065},{\"end\":78619,\"start\":78618},{\"end\":78637,\"start\":78630},{\"end\":79234,\"start\":79233},{\"end\":79250,\"start\":79245},{\"end\":79499,\"start\":79495},{\"end\":79514,\"start\":79508},{\"end\":79793,\"start\":79790},{\"end\":79806,\"start\":79799},{\"end\":79820,\"start\":79812},{\"end\":79837,\"start\":79828},{\"end\":80115,\"start\":80109},{\"end\":80126,\"start\":80125},{\"end\":80139,\"start\":80133},{\"end\":80498,\"start\":80492},{\"end\":80507,\"start\":80506},{\"end\":80836,\"start\":80833},{\"end\":80851,\"start\":80846},{\"end\":80867,\"start\":80861},{\"end\":80882,\"start\":80877},{\"end\":80884,\"start\":80883}]", "bib_author_last_name": "[{\"end\":61250,\"start\":61242},{\"end\":61258,\"start\":61252},{\"end\":61426,\"start\":61420},{\"end\":61439,\"start\":61434},{\"end\":61456,\"start\":61450},{\"end\":61900,\"start\":61892},{\"end\":61908,\"start\":61904},{\"end\":61914,\"start\":61910},{\"end\":62300,\"start\":62294},{\"end\":62311,\"start\":62307},{\"end\":62328,\"start\":62321},{\"end\":62344,\"start\":62335},{\"end\":62358,\"start\":62351},{\"end\":62370,\"start\":62364},{\"end\":62868,\"start\":62862},{\"end\":62891,\"start\":62878},{\"end\":62905,\"start\":62901},{\"end\":62921,\"start\":62915},{\"end\":63268,\"start\":63260},{\"end\":63284,\"start\":63280},{\"end\":63304,\"start\":63295},{\"end\":63319,\"start\":63316},{\"end\":63335,\"start\":63328},{\"end\":63353,\"start\":63345},{\"end\":63366,\"start\":63363},{\"end\":63798,\"start\":63783},{\"end\":63816,\"start\":63810},{\"end\":63833,\"start\":63824},{\"end\":63848,\"start\":63844},{\"end\":63856,\"start\":63850},{\"end\":64248,\"start\":64220},{\"end\":64256,\"start\":64252},{\"end\":64267,\"start\":64263},{\"end\":64271,\"start\":64269},{\"end\":64279,\"start\":64275},{\"end\":64296,\"start\":64288},{\"end\":64304,\"start\":64298},{\"end\":64771,\"start\":64763},{\"end\":64790,\"start\":64784},{\"end\":64798,\"start\":64792},{\"end\":65142,\"start\":65135},{\"end\":65157,\"start\":65149},{\"end\":65173,\"start\":65165},{\"end\":65181,\"start\":65177},{\"end\":65198,\"start\":65190},{\"end\":65206,\"start\":65200},{\"end\":65702,\"start\":65691},{\"end\":65715,\"start\":65710},{\"end\":65738,\"start\":65717},{\"end\":65746,\"start\":65742},{\"end\":65761,\"start\":65755},{\"end\":65775,\"start\":65768},{\"end\":65781,\"start\":65777},{\"end\":66166,\"start\":66151},{\"end\":66178,\"start\":66172},{\"end\":66200,\"start\":66193},{\"end\":66218,\"start\":66211},{\"end\":66672,\"start\":66666},{\"end\":66682,\"start\":66678},{\"end\":66921,\"start\":66909},{\"end\":66935,\"start\":66931},{\"end\":66947,\"start\":66944},{\"end\":66952,\"start\":66949},{\"end\":67257,\"start\":67248},{\"end\":67278,\"start\":67268},{\"end\":67295,\"start\":67287},{\"end\":67655,\"start\":67650},{\"end\":67667,\"start\":67661},{\"end\":68021,\"start\":68017},{\"end\":68038,\"start\":68031},{\"end\":68495,\"start\":68486},{\"end\":68511,\"start\":68507},{\"end\":68533,\"start\":68524},{\"end\":68878,\"start\":68870},{\"end\":68895,\"start\":68888},{\"end\":68910,\"start\":68905},{\"end\":68926,\"start\":68920},{\"end\":68950,\"start\":68942},{\"end\":69304,\"start\":69299},{\"end\":69319,\"start\":69313},{\"end\":69332,\"start\":69329},{\"end\":69349,\"start\":69343},{\"end\":69362,\"start\":69360},{\"end\":69813,\"start\":69809},{\"end\":69825,\"start\":69823},{\"end\":69840,\"start\":69835},{\"end\":70252,\"start\":70246},{\"end\":70264,\"start\":70258},{\"end\":70563,\"start\":70551},{\"end\":70577,\"start\":70571},{\"end\":70957,\"start\":70950},{\"end\":70971,\"start\":70966},{\"end\":70989,\"start\":70980},{\"end\":71006,\"start\":70997},{\"end\":71029,\"start\":71016},{\"end\":71575,\"start\":71562},{\"end\":71595,\"start\":71586},{\"end\":71607,\"start\":71597},{\"end\":71619,\"start\":71611},{\"end\":71632,\"start\":71627},{\"end\":71639,\"start\":71634},{\"end\":71648,\"start\":71643},{\"end\":71667,\"start\":71659},{\"end\":71671,\"start\":71669},{\"end\":72042,\"start\":72039},{\"end\":72059,\"start\":72055},{\"end\":72076,\"start\":72070},{\"end\":72092,\"start\":72088},{\"end\":72408,\"start\":72404},{\"end\":72423,\"start\":72415},{\"end\":72441,\"start\":72433},{\"end\":72764,\"start\":72762},{\"end\":72780,\"start\":72777},{\"end\":73193,\"start\":73174},{\"end\":73215,\"start\":73201},{\"end\":73241,\"start\":73225},{\"end\":73256,\"start\":73248},{\"end\":73633,\"start\":73629},{\"end\":73648,\"start\":73642},{\"end\":73664,\"start\":73655},{\"end\":73679,\"start\":73672},{\"end\":73693,\"start\":73686},{\"end\":73709,\"start\":73700},{\"end\":73723,\"start\":73718},{\"end\":73735,\"start\":73729},{\"end\":74107,\"start\":74104},{\"end\":74119,\"start\":74116},{\"end\":74143,\"start\":74128},{\"end\":74151,\"start\":74145},{\"end\":74653,\"start\":74640},{\"end\":74663,\"start\":74657},{\"end\":74673,\"start\":74665},{\"end\":74681,\"start\":74675},{\"end\":74939,\"start\":74932},{\"end\":74954,\"start\":74947},{\"end\":74966,\"start\":74961},{\"end\":75219,\"start\":75215},{\"end\":75230,\"start\":75227},{\"end\":75245,\"start\":75240},{\"end\":75256,\"start\":75252},{\"end\":75268,\"start\":75265},{\"end\":75651,\"start\":75647},{\"end\":75664,\"start\":75660},{\"end\":75677,\"start\":75674},{\"end\":75691,\"start\":75689},{\"end\":75703,\"start\":75700},{\"end\":75712,\"start\":75709},{\"end\":76286,\"start\":76274},{\"end\":76300,\"start\":76293},{\"end\":76569,\"start\":76567},{\"end\":76584,\"start\":76571},{\"end\":77108,\"start\":77104},{\"end\":77123,\"start\":77119},{\"end\":77135,\"start\":77132},{\"end\":77148,\"start\":77144},{\"end\":77435,\"start\":77430},{\"end\":77451,\"start\":77443},{\"end\":77466,\"start\":77459},{\"end\":77990,\"start\":77973},{\"end\":78006,\"start\":77999},{\"end\":78023,\"start\":78016},{\"end\":78036,\"start\":78032},{\"end\":78049,\"start\":78045},{\"end\":78063,\"start\":78057},{\"end\":78077,\"start\":78070},{\"end\":78084,\"start\":78079},{\"end\":78628,\"start\":78620},{\"end\":78645,\"start\":78638},{\"end\":78651,\"start\":78647},{\"end\":79073,\"start\":79064},{\"end\":79243,\"start\":79235},{\"end\":79257,\"start\":79251},{\"end\":79261,\"start\":79259},{\"end\":79506,\"start\":79500},{\"end\":79523,\"start\":79515},{\"end\":79797,\"start\":79794},{\"end\":79810,\"start\":79807},{\"end\":79826,\"start\":79821},{\"end\":79840,\"start\":79838},{\"end\":80123,\"start\":80116},{\"end\":80131,\"start\":80127},{\"end\":80148,\"start\":80140},{\"end\":80156,\"start\":80150},{\"end\":80504,\"start\":80499},{\"end\":80515,\"start\":80508},{\"end\":80521,\"start\":80517},{\"end\":80844,\"start\":80837},{\"end\":80859,\"start\":80852},{\"end\":80875,\"start\":80868},{\"end\":80889,\"start\":80885}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":61325,\"start\":61219},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16726452},\"end\":61741,\"start\":61327},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":54122859},\"end\":62183,\"start\":61743},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":199064345},\"end\":62754,\"start\":62185},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8743034},\"end\":63183,\"start\":62756},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206597685},\"end\":63689,\"start\":63185},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9096929},\"end\":64118,\"start\":63691},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9866556},\"end\":64681,\"start\":64120},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1409560},\"end\":65054,\"start\":64683},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15875534},\"end\":65605,\"start\":65056},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":39888164},\"end\":66067,\"start\":65607},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209320348},\"end\":66566,\"start\":66069},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":33654371},\"end\":66858,\"start\":66568},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18431863},\"end\":67194,\"start\":66860},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15055092},\"end\":67507,\"start\":67196},{\"attributes\":{\"doi\":\"arXiv:1912.07812\",\"id\":\"b15\"},\"end\":67900,\"start\":67509},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1780423},\"end\":68380,\"start\":67902},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":158046809},\"end\":68781,\"start\":68382},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15308777},\"end\":69230,\"start\":68783},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6692382},\"end\":69724,\"start\":69232},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9297483},\"end\":70177,\"start\":69726},{\"attributes\":{\"doi\":\"arXiv:1910.07497\",\"id\":\"b21\"},\"end\":70424,\"start\":70179},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":23477696},\"end\":70859,\"start\":70426},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52900092},\"end\":71432,\"start\":70861},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":57574627},\"end\":71968,\"start\":71434},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":149007732},\"end\":72311,\"start\":71970},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":38475079},\"end\":72704,\"start\":72313},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9483753},\"end\":73061,\"start\":72706},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":57379102},\"end\":73514,\"start\":73063},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":203660263},\"end\":74006,\"start\":73516},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":207757681},\"end\":74600,\"start\":74008},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":196133842},\"end\":74852,\"start\":74602},{\"attributes\":{\"doi\":\"arXiv:1903.02330\",\"id\":\"b32\"},\"end\":75154,\"start\":74854},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":58004714},\"end\":75521,\"start\":75156},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":102350701},\"end\":76154,\"start\":75523},{\"attributes\":{\"doi\":\"arXiv:1905.11796\",\"id\":\"b35\"},\"end\":76516,\"start\":76156},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15015161},\"end\":77014,\"start\":76518},{\"attributes\":{\"doi\":\"arXiv:1811.11387\",\"id\":\"b37\"},\"end\":77356,\"start\":77016},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":195357258},\"end\":77837,\"start\":77358},{\"attributes\":{\"doi\":\"arXiv:1706.00527\",\"id\":\"b39\"},\"end\":78405,\"start\":77839},{\"attributes\":{\"id\":\"b40\"},\"end\":78538,\"start\":78407},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":17630161},\"end\":78903,\"start\":78540},{\"attributes\":{\"id\":\"b42\"},\"end\":79060,\"start\":78905},{\"attributes\":{\"doi\":\"Accessed: 2019-09-17\",\"id\":\"b43\"},\"end\":79187,\"start\":79062},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b44\"},\"end\":79405,\"start\":79189},{\"attributes\":{\"doi\":\"arXiv:1902.03043\",\"id\":\"b45\"},\"end\":79714,\"start\":79407},{\"attributes\":{\"doi\":\"arXiv:1908.05349\",\"id\":\"b46\"},\"end\":80040,\"start\":79716},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":44063962},\"end\":80382,\"start\":80042},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":51925847},\"end\":80779,\"start\":80384},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":73718469},\"end\":81118,\"start\":80781}]", "bib_title": "[{\"end\":61407,\"start\":61327},{\"end\":61883,\"start\":61743},{\"end\":62285,\"start\":62185},{\"end\":62841,\"start\":62756},{\"end\":63251,\"start\":63185},{\"end\":63773,\"start\":63691},{\"end\":64218,\"start\":64120},{\"end\":64759,\"start\":64683},{\"end\":65126,\"start\":65056},{\"end\":65678,\"start\":65607},{\"end\":66141,\"start\":66069},{\"end\":66660,\"start\":66568},{\"end\":66897,\"start\":66860},{\"end\":67238,\"start\":67196},{\"end\":68010,\"start\":67902},{\"end\":68474,\"start\":68382},{\"end\":68862,\"start\":68783},{\"end\":69291,\"start\":69232},{\"end\":69798,\"start\":69726},{\"end\":70542,\"start\":70426},{\"end\":70941,\"start\":70861},{\"end\":71553,\"start\":71434},{\"end\":72028,\"start\":71970},{\"end\":72397,\"start\":72313},{\"end\":72757,\"start\":72706},{\"end\":73168,\"start\":73063},{\"end\":73622,\"start\":73516},{\"end\":74093,\"start\":74008},{\"end\":74636,\"start\":74602},{\"end\":75208,\"start\":75156},{\"end\":75636,\"start\":75523},{\"end\":76561,\"start\":76518},{\"end\":77422,\"start\":77358},{\"end\":78616,\"start\":78540},{\"end\":80107,\"start\":80042},{\"end\":80490,\"start\":80384},{\"end\":80831,\"start\":80781}]", "bib_author": "[{\"end\":61252,\"start\":61240},{\"end\":61260,\"start\":61252},{\"end\":61428,\"start\":61409},{\"end\":61441,\"start\":61428},{\"end\":61458,\"start\":61441},{\"end\":61902,\"start\":61885},{\"end\":61910,\"start\":61902},{\"end\":61916,\"start\":61910},{\"end\":62302,\"start\":62287},{\"end\":62313,\"start\":62302},{\"end\":62330,\"start\":62313},{\"end\":62346,\"start\":62330},{\"end\":62360,\"start\":62346},{\"end\":62372,\"start\":62360},{\"end\":62870,\"start\":62843},{\"end\":62893,\"start\":62870},{\"end\":62907,\"start\":62893},{\"end\":62923,\"start\":62907},{\"end\":63270,\"start\":63253},{\"end\":63286,\"start\":63270},{\"end\":63306,\"start\":63286},{\"end\":63321,\"start\":63306},{\"end\":63337,\"start\":63321},{\"end\":63355,\"start\":63337},{\"end\":63368,\"start\":63355},{\"end\":63800,\"start\":63775},{\"end\":63818,\"start\":63800},{\"end\":63835,\"start\":63818},{\"end\":63850,\"start\":63835},{\"end\":63858,\"start\":63850},{\"end\":64250,\"start\":64220},{\"end\":64258,\"start\":64250},{\"end\":64269,\"start\":64258},{\"end\":64273,\"start\":64269},{\"end\":64281,\"start\":64273},{\"end\":64298,\"start\":64281},{\"end\":64306,\"start\":64298},{\"end\":64773,\"start\":64761},{\"end\":64792,\"start\":64773},{\"end\":64800,\"start\":64792},{\"end\":65144,\"start\":65128},{\"end\":65159,\"start\":65144},{\"end\":65175,\"start\":65159},{\"end\":65183,\"start\":65175},{\"end\":65200,\"start\":65183},{\"end\":65208,\"start\":65200},{\"end\":65704,\"start\":65680},{\"end\":65717,\"start\":65704},{\"end\":65740,\"start\":65717},{\"end\":65748,\"start\":65740},{\"end\":65763,\"start\":65748},{\"end\":65777,\"start\":65763},{\"end\":65783,\"start\":65777},{\"end\":66168,\"start\":66143},{\"end\":66180,\"start\":66168},{\"end\":66202,\"start\":66180},{\"end\":66220,\"start\":66202},{\"end\":66674,\"start\":66662},{\"end\":66684,\"start\":66674},{\"end\":66923,\"start\":66899},{\"end\":66937,\"start\":66923},{\"end\":66949,\"start\":66937},{\"end\":66954,\"start\":66949},{\"end\":67259,\"start\":67240},{\"end\":67280,\"start\":67259},{\"end\":67297,\"start\":67280},{\"end\":67657,\"start\":67642},{\"end\":67669,\"start\":67657},{\"end\":68023,\"start\":68012},{\"end\":68040,\"start\":68023},{\"end\":68497,\"start\":68476},{\"end\":68513,\"start\":68497},{\"end\":68535,\"start\":68513},{\"end\":68880,\"start\":68864},{\"end\":68897,\"start\":68880},{\"end\":68912,\"start\":68897},{\"end\":68928,\"start\":68912},{\"end\":68952,\"start\":68928},{\"end\":69306,\"start\":69293},{\"end\":69321,\"start\":69306},{\"end\":69334,\"start\":69321},{\"end\":69351,\"start\":69334},{\"end\":69364,\"start\":69351},{\"end\":69815,\"start\":69800},{\"end\":69827,\"start\":69815},{\"end\":69842,\"start\":69827},{\"end\":70254,\"start\":70239},{\"end\":70266,\"start\":70254},{\"end\":70565,\"start\":70544},{\"end\":70579,\"start\":70565},{\"end\":70959,\"start\":70943},{\"end\":70973,\"start\":70959},{\"end\":70991,\"start\":70973},{\"end\":71008,\"start\":70991},{\"end\":71031,\"start\":71008},{\"end\":71577,\"start\":71555},{\"end\":71597,\"start\":71577},{\"end\":71609,\"start\":71597},{\"end\":71621,\"start\":71609},{\"end\":71634,\"start\":71621},{\"end\":71641,\"start\":71634},{\"end\":71650,\"start\":71641},{\"end\":71669,\"start\":71650},{\"end\":71673,\"start\":71669},{\"end\":72044,\"start\":72030},{\"end\":72061,\"start\":72044},{\"end\":72078,\"start\":72061},{\"end\":72094,\"start\":72078},{\"end\":72410,\"start\":72399},{\"end\":72425,\"start\":72410},{\"end\":72443,\"start\":72425},{\"end\":72766,\"start\":72759},{\"end\":72782,\"start\":72766},{\"end\":73195,\"start\":73170},{\"end\":73217,\"start\":73195},{\"end\":73243,\"start\":73217},{\"end\":73258,\"start\":73243},{\"end\":73635,\"start\":73624},{\"end\":73650,\"start\":73635},{\"end\":73666,\"start\":73650},{\"end\":73681,\"start\":73666},{\"end\":73695,\"start\":73681},{\"end\":73711,\"start\":73695},{\"end\":73725,\"start\":73711},{\"end\":73737,\"start\":73725},{\"end\":74109,\"start\":74095},{\"end\":74121,\"start\":74109},{\"end\":74145,\"start\":74121},{\"end\":74153,\"start\":74145},{\"end\":74655,\"start\":74638},{\"end\":74665,\"start\":74655},{\"end\":74675,\"start\":74665},{\"end\":74683,\"start\":74675},{\"end\":74941,\"start\":74923},{\"end\":74956,\"start\":74941},{\"end\":74968,\"start\":74956},{\"end\":75221,\"start\":75210},{\"end\":75232,\"start\":75221},{\"end\":75247,\"start\":75232},{\"end\":75258,\"start\":75247},{\"end\":75270,\"start\":75258},{\"end\":75653,\"start\":75638},{\"end\":75666,\"start\":75653},{\"end\":75679,\"start\":75666},{\"end\":75693,\"start\":75679},{\"end\":75705,\"start\":75693},{\"end\":75714,\"start\":75705},{\"end\":76288,\"start\":76268},{\"end\":76302,\"start\":76288},{\"end\":76571,\"start\":76563},{\"end\":76586,\"start\":76571},{\"end\":77110,\"start\":77095},{\"end\":77125,\"start\":77110},{\"end\":77137,\"start\":77125},{\"end\":77150,\"start\":77137},{\"end\":77437,\"start\":77424},{\"end\":77453,\"start\":77437},{\"end\":77468,\"start\":77453},{\"end\":77992,\"start\":77953},{\"end\":78008,\"start\":77992},{\"end\":78025,\"start\":78008},{\"end\":78038,\"start\":78025},{\"end\":78051,\"start\":78038},{\"end\":78065,\"start\":78051},{\"end\":78079,\"start\":78065},{\"end\":78086,\"start\":78079},{\"end\":78630,\"start\":78618},{\"end\":78647,\"start\":78630},{\"end\":78653,\"start\":78647},{\"end\":79075,\"start\":79064},{\"end\":79245,\"start\":79233},{\"end\":79259,\"start\":79245},{\"end\":79263,\"start\":79259},{\"end\":79508,\"start\":79495},{\"end\":79525,\"start\":79508},{\"end\":79799,\"start\":79790},{\"end\":79812,\"start\":79799},{\"end\":79828,\"start\":79812},{\"end\":79842,\"start\":79828},{\"end\":80125,\"start\":80109},{\"end\":80133,\"start\":80125},{\"end\":80150,\"start\":80133},{\"end\":80158,\"start\":80150},{\"end\":80506,\"start\":80492},{\"end\":80517,\"start\":80506},{\"end\":80523,\"start\":80517},{\"end\":80846,\"start\":80833},{\"end\":80861,\"start\":80846},{\"end\":80877,\"start\":80861},{\"end\":80891,\"start\":80877}]", "bib_venue": "[{\"end\":61238,\"start\":61219},{\"end\":61518,\"start\":61458},{\"end\":61946,\"start\":61916},{\"end\":62451,\"start\":62372},{\"end\":62963,\"start\":62923},{\"end\":63408,\"start\":63368},{\"end\":63898,\"start\":63858},{\"end\":64381,\"start\":64306},{\"end\":64855,\"start\":64800},{\"end\":65282,\"start\":65208},{\"end\":65823,\"start\":65783},{\"end\":66299,\"start\":66220},{\"end\":66698,\"start\":66684},{\"end\":67019,\"start\":66954},{\"end\":67337,\"start\":67297},{\"end\":67640,\"start\":67509},{\"end\":68120,\"start\":68040},{\"end\":68575,\"start\":68535},{\"end\":68992,\"start\":68952},{\"end\":69432,\"start\":69364},{\"end\":69909,\"start\":69842},{\"end\":70237,\"start\":70179},{\"end\":70628,\"start\":70579},{\"end\":71100,\"start\":71031},{\"end\":71688,\"start\":71673},{\"end\":72134,\"start\":72094},{\"end\":72490,\"start\":72443},{\"end\":72858,\"start\":72782},{\"end\":73269,\"start\":73258},{\"end\":73744,\"start\":73737},{\"end\":74245,\"start\":74153},{\"end\":74708,\"start\":74683},{\"end\":74921,\"start\":74854},{\"end\":75332,\"start\":75270},{\"end\":75791,\"start\":75714},{\"end\":76266,\"start\":76156},{\"end\":76673,\"start\":76586},{\"end\":77093,\"start\":77016},{\"end\":77551,\"start\":77468},{\"end\":77951,\"start\":77839},{\"end\":78418,\"start\":78407},{\"end\":78708,\"start\":78653},{\"end\":78926,\"start\":78905},{\"end\":79231,\"start\":79189},{\"end\":79493,\"start\":79407},{\"end\":79788,\"start\":79716},{\"end\":80198,\"start\":80158},{\"end\":80567,\"start\":80523},{\"end\":80937,\"start\":80891},{\"end\":65343,\"start\":65284},{\"end\":69487,\"start\":69434},{\"end\":69963,\"start\":69911},{\"end\":71156,\"start\":71102},{\"end\":74324,\"start\":74247},{\"end\":75855,\"start\":75793},{\"end\":76747,\"start\":76675},{\"end\":77621,\"start\":77553}]"}}}, "year": 2023, "month": 12, "day": 17}