{"id": 235899080, "updated": "2023-10-06 01:17:06.187", "metadata": {"title": "Only Train Once: A One-Shot Neural Network Training And Pruning Framework", "authors": "[{\"first\":\"Tianyi\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Tianyu\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Biyi\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Guanyi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhihui\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Luming\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Yixin\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Sheng\",\"last\":\"Yi\",\"middle\":[]},{\"first\":\"Xiao\",\"last\":\"Tu\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "19637-19651", "publication_date": {"year": 2021, "month": 7, "day": 15}, "abstract": "Structured pruning is a commonly used technique in deploying deep neural networks (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-specified, and require an extra fine-tuning procedure. To overcome these limitations, we propose a framework that compresses DNNs into slimmer architectures with competitive performances and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two keys: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem and propose a novel optimization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity exploration and maintains comparable convergence. To demonstrate the effectiveness of OTO, we train and compress full models simultaneously from scratch without fine-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/only_train_once.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2107.07467", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ChenJDFWZLSYT21", "doi": null}}, "content": {"source": {"pdf_hash": "88ad1104e61d25c6e8919cb6d2af0aaedd6f0526", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.07467v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e30cf72ce30a82dd46c2f9e37cccd399d09e2249", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/88ad1104e61d25c6e8919cb6d2af0aaedd6f0526.txt", "contents": "\nOnly Train Once: A One-Shot Neural Network Training And Pruning Framework\n\n\nTianyi Chen tiachen@microsoft.com \nBo Ji jibo@comp.nus.edu.sg \nTianyu Ding tding1@jhu.edu \nBiyi Fang \nGuanyi Wang \nZhihui Zhu zhihui.zhu@du.edu \nLuming Liang lulian@microsoft.com \nYixin Shi Microsoft \nSheng Yi Microsoft shengyi@microsoft.com \nXiao Tu Microsoft xiaotu@microsoft.com \n\nNational University of Singapore\nJohns Hopkins University\nMicrosoft, Microsoft\n\n\nGeorgia Institute of Technology\nUniversity of Denver\nMicrosoft\n\nOnly Train Once: A One-Shot Neural Network Training And Pruning Framework\n\nStructured pruning is a commonly used technique in deploying deep neural networks (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-specified, and require an extra fine-tuning procedure. To overcome these limitations, we propose a framework that compresses DNNs into slimmer architectures with competitive performances and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two keys: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem and propose a novel optimization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity exploration and maintains comparable convergence. To demonstrate the effectiveness of OTO, we train and compress full models simultaneously from scratch without finetuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/only_train_once.\n\nIntroduction\n\nDeep neural networks (DNNs) have been shown to be effective in various real applications [45,25]. It is widely acknowledged that large-scale DNN models not only learn faster but also outperform their slimmer counterparts. However, such heavy models pose a great challenge to the deployment stage due to their resource-consuming nature. In addressing this issue, many model compression  Figure 1: Overview of OTO. Without loss of generality, we illustrate OTO on a model with only vanilla convolutional layers, and for simplicity we only show Layer i with m 3D filters and their biases. The key to its success is twofold: (i) identify and partition the parameters of the model into zero-invariant groups (ZIGs); and (ii) solve the structured-sparsity regularization problem using HSPG. Finally, we obtain the compressed model by directly pruning the zero groups, i.e., ZIG m .\n\ntechniques [5,10] are proposed in the past decade that aim at compressing those large and complex models into slimmer and simpler ones while suffering negligible loss in performance.\n\nPruning methods as one of the main categories of model compression, focus on identifying and pruning redundant structures via various mechanisms to achieve a slimmer architecture, and thus improve the interpretability of a DNN model [23,10,56]. For example, [29,30] adopt fine-grained pruning via 1 or 2 regularization, which prune the small-weight connections based on some hard threshold. [33,50,53] measure the importance of filters to accelerate the networks by removing insignificant feature maps. [34,6] utilize reinforcement learning agent to predict compression action.\n\nNevertheless, many of the existing pruning methods (i) often rely on criteria based on heuristics or empirical cues, e.g., magnitude of a connection weight and importance score of a filter, to identify redundant parameters, which may cause divergence during optimization; (ii) thus require complex multi-stage pipelines that involve either a retraining or fine-tuning procedure to regain the accuracy during constructing a slimmer model, which is time-consuming; and (iii) are specific to certain architectures or applications, and are consequently less applicable to various downstream scenarios. Recently, there have been a few efforts [12,51,7] to directly train the network with sparsity inducing regularizers, which provide generality and convergence guarantee. However, these approaches focus on either merely the individual sparsity of the parameters or the group sparsity of the filters, and thus cannot directly remove those zero components (still require subsequent fine-tuning) since the zeros are entangled with other commonly used components, e.g., bias, batch normalization or skip connection. Furthermore, the optimization algorithms used in [12,51] lack sufficient capability to explore (group) sparsity in DNNs effectively and require a post-processing step to yield exact zeros.\n\nIn this paper, we overcome the above limitations of existing pruning methods by proposing a one-shot neural network pruning framework, with which we are able to train a full heavy model from scratch only once, and obtain a slim architecture without fine-tuning while maintain high performance. As shown in Figure 1, the key to its success is twofold: (i) we identify and partition the parameters of DNNs into zero-invariant groups (ZIGs), enabling us to prune redundant structures according to zero groups without affecting the output of the network; and (ii) to promote zero groups, we formulate the pruning task as a structured-sparsity optimization problem and propose a novel optimization method, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on sparsity exploration and maintains comparable convergence. We highlight that both zero-invariant group partition and the novel optimization algorithm in promoting zero group lead to achieve one-shot neural network training and pruning regardless of its architecture.\n\nOur main contributions are summarized as follows.\n\n\u2022 One-Shot Training and Pruning. We propose OTO, a training and pruning framework that compresses a full neural network with competitive performance by Only-Train-Once, thereby one-shot. OTO dramatically simplifies the complex multi-stage training pipelines of the existing pruning approaches, fits various architectures and applications, and hence is generic and efficient. \u2022 Zero-Invariant Group. We define zero-invariant groups for neural networks. If a network is partitioned into ZIGs, it allows us to prune the zero groups without affecting the output, which results in one-shot pruning. Such property is applicable to various popular structures from plain fully connected layers to sophisticated ones such as residual blocks and multi-head attention. \u2022 Novel Structured-Sparsity Optimization Algorithm. We propose Half-Space Stochastic Projected Gradient (HSPG), a method that solves structured-sparsity inducing regularization problem. We show and analyze the superiority of HSPG in promoting zero groups of networks than the standard proximal methods and the competitive objective convergence in practice. The fact that ZIG and HSPG are designed agnostic to networks makes OTO generic to various applications.\n\n\u2022 Experimental Results. We train and compress full models simultaneously from scratch without fine-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on compression benchmark VGG for CIFAR10, ResNet50 for CIFAR10/ImageNet, Bert for SQuAD.\n\n\nRelated Work\n\nStructured pruning focuses on identifying and pruning the redundant structures in a full model to achieve slimmer architectures for efficient model inference and storage [23,29], where there have been numerous efforts dedicated. For CNN compression, the general procedure can be largely summarized as: (i) train a full model; (ii) identify and prune the redundant structures to build a slimmer model based on various criteria, including (structured) sparsity [51, 73,12,49,87,24,87,55,78], Bayesian pruning [86,56,52,69], ranking importance [47,53,38,33,50,85], reinforcement learning [34,6], adversarial robustness [64], scientific control [67], lottery ticket [20,21,62], joint quantization learning [68,77], etc.; (iii) retrain or iteratively fine-tune the slimmer model to regain the accuracy regression during pruning. These methods cannot avoid the extra and usually time-consuming fine-tuning step because the identified redundant structures, even parametrized with zeros, actually contribute to the model output, thereby additional fine-tuning step is an absolute necessity.\n\nFor pruning Bert [70], knowledge distillation [37] and LayerDropout [19] shorten Bert by reducing the number of layers directly. Other methods [26,63,27] build slimmer Berts in the manner of individual sparsity, but require specially designed data structure for storage and computing library to take advantage of sparse data [28,9], and typically cannot achieve inference speedup against the highly optimized library [14] for dense model due to the discontiguous memory allocation [8].\n\nThe structured sparsity for weight pruning is the most relevant to the algorithm described in this paper. The existing structure learning works [51, 73,12,49,87] have the respective disadvantages: (i) multiple trainings during the whole procedure since their group partition cannot isolate the impact of pruned structures to the model output; and (ii) heuristic post-processing to generate zero groups as the standard proximal methods [17,74,75,11] and ADMM [85,51,4] defective on the sparsity exploration for deep learning [7], which may deteriorate the performance of the model significantly.\n\nAvoiding fine-tuning step during the whole pruning procedure is receiving more and more attentions because of its efficiency. In particular, SNIP [46] and GraSP [71] identify redundancy via salience scores at the initialization stage to construct pruned structures, then train the pruned models by the standard optimizers. SCP [42] isolates the impact of batch normalization, while lacks the consideration of more general DNN architectures.\n\n\nOTO\n\nIn essence, OTO frames the network training and pruning as a structure learning problem. Given a full model M, OTO trains and compresses it simultaneously from scratch without fine-tuning, and achieves significant reduction in both FLOPs and parameters. Particularly, as stated in Algorithm 1, the trainable parameters of M are firstly partitioned into a ZIG set G (Section 3.1). We then construct and solve a structured-sparsity inducing optimization problem (Section 3.2) by proposed stochastic optimizer (HSPG) to seek a highly group-sparse solution x * HSPG (Section 3.3). Lastly, we obtain a compressed model M * by directly pruning these zero groups (Section 3.4). \n\n\nZero-Invariant Group\n\nThe root cause of the existing methods having multi-stage training pipeline is that despite the pruned structure (e.g., 3D filter) being zeros, its associated structures (e.g., non-zero bias) still contribute to its corresponding output to the next layer (e.g., feature map). As a result, the model accuracy regresses, hence an extra step of fine-tuning is necessary. OTO avoids the necessity by partitioning the parameters of DNNs into a set of so-called zero-invariant groups (ZIGs) G defined as follows. Definition 1 (Zero-Invariant Groups (ZIGs)). For a layer-wise DNN, we partition its entire trainable parameters into disjoint groups G = {g}. Then we call G zero-invariant groups (ZIGs) if each group g \u2208 G is zero-invariant in the sense that all of the parameters in g being zeros results in its corresponding output to the next layer to be zeros as well. In effect, if and only if a DNN model is partitioned into a ZIG set G and one or more of its element g are parameterized by zeros, the entire corresponding structures contribute none to the model outputs and hence can be pruned directly. Such partition is applicable to various structures of DNN models. Without loss of generality, we define and describe ZIG partition for three most popular structures: (i) Conv-BN, (ii) Residual Block, and (iii) Fully Connected and Multi-Head Attention Layer.   ZIG of Conv-BN. Convolutional layer (Conv) followed by batch-normalization layer (BN) is extensively used in DNN models. Figure 2a shows the ZIG partition for Conv-BN. The 4D filter tensor K l is flattened into a filter matrixK l . During the forward pass, the input tensor I l is transformed into the output tensor O l of Conv and then into the input tensor of the (l + 1) th layer I l+1 by\nO l \u2190 I l \u2297K l + b l , I l+1 \u2190 a(O l ) \u2212 \u00b5 l \u03c3 l \u03b3 l + \u03b2 l ,(1)\nwhere denoted by \u2297 the convolutional operation, the element-wise multiplication and a(\u00b7) the activation function. BN is parameterized by mean \u00b5 l , standard deviation \u03c3 l , weight \u03b3 l and bias \u03b2 l respectively. The activation function needs to be zero-invariant, i.e., a(0) = 0, where most instances satisfy, e.g., ReLU [22], PReLU [31], GELU [36] and LeakyReLU [76]. Hence, each row of the flattened filter matrixK l and its bias b l belong to one ZIG because they being zeros results in their corresponding channel of O l (i.e., feature map) to be zeros as well. Subsequently, \u03b3 l and \u03b2 l of this corresponding channel in BN are also included into this ZIG to avoid the value shift (zero to non-zero) during normalization. Note that grouping these four sets of parameters channel-wisely makes Conv-BN zero-invariant regardless of the value of \u00b5 l and \u03c3 l , and hence they are excluded from the ZIG. For illustration, each ZIG is highlighted in the same color (e.g., g l 1 in blue). ZIG of Residual Block. The residual block adds another layer of challenge because its output tensor is the summation of the outputs of two Conv-BNs. Figure 2b shows the ZIG partition for the residual block. As illustrated, before propagated to Conv3, the outputs of Conv1-BN1 and Conv2-BN2 are summarized and hence share the same dimension. As such, to make residual block zero-invariant, we group the four sets of parameters channel-wisely of both Conv1-BN1 and Conv2-BN2 into ZIGs, i.e., each row ofK 1 , b 1 , \u03b3 1 , \u03b2 1 of Conv1-BN1 and each row ofK 2 , b 2 , \u03b3 2 , \u03b2 2 of Conv2-BN2. In Appendix A.1, we describe the zero-invariant group partition of ResNet50 in greater detail.\n\nZIG of Fully Connected and Multi-Head Attention Layer. Figure 2c shows the ZIG partition for fully connected and multi-head attention layer. Particularly, we partition each row of weight matrix and its associated bias into a ZIG, and therefore any input element is turned to zero if that ZIG is parameterized with zeros, making the fully connected layer zero-invariant. Multi-head attention layer is the key building block of the transformer architectures [70]. Its trainable parameters contain a weight matrix and bias vector, consisting of the sub-matrix and sub-vector of each head (we use two heads as an example). We form ZIG by grouping each row of every sub-matrix and sub-vector, i.e., each row of W h 1 , b h1 , W h 2 and b h2 of h 1 and h 2 , respectively.\n\nAutomatic ZIG Partition. Based on the above illustrating examples, we provide prescribed ZIG partition for the tested DNNs in Section 4. Furthermore, given an arbitrary DNN architecture, the procedure of partitioning variables into ZIGs could be automatically proceeded, wherein the key would be identifying the connections among various layers, then performing corresponding group partition. We will leave the automatic ZIG partition for arbitrary DNNs as future work.\n\n\nStructured-Sparsity Regularization\n\nWe now formulate a structured-sparsity regularization problem over the ZIG set G for the trainable parameters of the full model M as follows\nminimize x\u2208R n \u03c8(x) := f (x) + \u03bbr(x), r(x) := g\u2208G [x] g ,(2)\nwhere \u03bb > 0 is a weighting coefficient, f (x) is a task-specific loss function, and r(x) is an augmented structured-sparsity inducing regularization term encoding the topological structure of M over G.\n\nA larger \u03bb typically results in a higher group sparsity while sacrifices more on the bias of model estimation. We aim at computing a local optimum to achieve both low loss and high group sparsity.\n\nTo induce group sparsity onto the solution of (2), there exist several candidates for r(x), including mixed 1 / p norm (p > 1) [1,18] and group Minmax Concave Penalty (MCP) [81]. Among these candidates, the mixed 1 / 2 norm as defined in (2) is arguably the most popular choice in classical machine learning applications [1,79], where \u00b7 is the 2 -norm, and each component g \u2208 G indexes a group of variables. In this paper, we will demonstrate the effectiveness of OTO by selecting r(x) as the mixed 1 / 2 norm. We highlight OTO is applicable for other group sparsity regularizers as well.\n\n\nHalf-Space Stochastic Projected Gradient (HSPG)\n\nTo solve the non-smooth regularization problem as (2) in deep learning applications, the standard proximal method and the ADMM lack capability to effectively identify group sparsity; see the discussions later in this Section. Therefore, we propose a novel stochastic optimization algorithm so-called Half-Space Stochastic Projected Gradient (HSPG) to enhance the group sparsity exploration more effectively than the classical methods while maintain a similar convergence property.\n\nOutline. We state the outline of HSPG in Algorithm 2. It contains two stages: Initialization Stage and Group-Sparsity Stage. The first Initialization Stage employs Stochastic Gradient Descent (SGD) step to search for a good but usually non-sparse solution estimate. Then the second stage proceeds Half-Space step started with the non-sparse iterate to effectively exploit the group sparsity within a sequence of reduced spaces and converges to the group-sparse solutions. Half-Space step performs SGD update on free non-zero variables along with a novel projection operator so-called Half-Space Projection, which significantly outperforms the standard proximal operators on sparsity exploration.\n\nInitialization Stage. The Initialization Stage performs the vanilla SGD to find a good initial point for the subsequent Group-Sparsity Stage. At k th iteration, a stochastic gradient of f , e.g., based on a mini-batch, is generated denoted as \u2207f . Since the group sparsity inducing regularizer r(x) in the form as (2) is non-smooth, we select a subgradient \u03b6(x k ) from its subdifferential \u2202r(x k ) to form a stochastic subgradient of \u03c8(x k ) as \u03bd(x k ) := \u2207f (x k ) + \u03bb\u03b6(x k ). We then compute the next iterate as x k+1 := x k \u2212 \u03b1 k \u03bd(x k ) by subgradient descent update. Group-Sparsity Stage. The Group-Sparsity Stage is designed to effectively determine the groups of zero variables and capitalize convergence characteristic, which is in sharp contrast to other heuristic aggressive weight pruning methods that typically lack theoretical guarantees [48,53]. The intuition of Half-Space\nx k+1 O S k [x] 1 [x] 2 x k \u01ebx k x k+1x E \u2212 \u03b1k \u2207 \u03a8 Bk ( xk ) \u01eb > 0 \u03b8 < 90 \u2022 (a) Half-Space Projection O [x] 2 [x] 1 \u03b1 k \u03bb \u2212\u03b1 k \u03bb \u03bb \u2212\u03bb [x] 2 \u01eb = 0 \u01eb \u2248 1 x k Prox-SG Prox-SVRG Prox-\nStep is to project\n[x k ] g to zero only if \u2212[x k ] g serves as a descent step to \u03c8(x k ), i.e., \u2212[x k ] g [\u2207\u03c8(x k ))] g < 0, hence updating [x k+1 ] g \u2190 [x k ] g \u2212 [x k ] g = 0\nstill results in some progress to the optimality. In particular, we first define the following index sets for any x \u2208 R n :\nI 0 (x) := {g : g \u2208 G, [x] g = 0} and I =0 (x) := {g : g \u2208 G, [x] g = 0},(3)\nwhere I 0 (x) represents the indices of groups of zero variables at x, and I =0 (x) indexes the groups of nonzero variables at x. To proceed, we further define an artificial set that x lies in:\nS(x) := {0} {z \u2208 R n : [z] g = 0 if g \u2208 I 0 (x), and [z] g [x] g \u2265 [x] g 2 if g \u2208 I =0 (x)},(4)\nwhich consists of half-spaces and the origin. Here the parameter \u2265 0 controls how aggressively we promote group sparsity, and is typically fixed as zero in practice. Hence, x \u2208 S k := S(x k ) only if: (i) [x] g lies in the upper half-space for all g \u2208 I =0 (x k ) for some prescribed \u2208 [0, 1) as shown in Figure 3a; and (ii) [x] g equals to zero for all g \u2208 I 0 (x k ). Intuitively, S k establishes the region where important structures inhabit, thereby redundant structures vanish if falling outside.\n\nAlgorithm 2 Outline of HSPG for solving (2). 1), and N \u2208 Z + . 2: Output: a group-sparse solution x * HSPG from {x k }. Compute a stochastic subgradient \u03bd(x k ) of \u03c8(x k ).\n1: Input: x 0 \u2208 R n , \u03b1 0 > 0, \u2208 [0,\n\n5:\n\nif k < N then 6: Subgradient Descent Update:  Half-Space Update: 10: Set a trial iteratex k+1 as\n7: Set x k+1 \u2190 x k \u2212 \u03b1 k \u03bd(x k ).[x k+1 ] I =0 (x k ) \u2190 [x k \u2212 \u03b1 k \u03bd(x k )] I =0 (x k ) [x k+1 ] I 0 (x k ) \u2190 0.\n\n11:\n\nfor each group g in G do 12:\n[x k+1 ] g \u2190 [Proj HS S k (x k+1 )] g .\n\n13:\n\nUpdate \u03b1 k+1 .\n\nIdeally, the Initialization Stage has produced reasonably well but typically non-sparse iterate x k nearby a group-sparse solution x * of problem (2), , i.e., the optimal distance x k \u2212 x * is sufficiently small. As seen in Appendix B, it further indicates that the group-sparse optimal solution x * inhabits S k , and S k has already covered the group-support of x * , i.e., I =0 (x * ) \u2286 I =0 (x k ). Our goal now becomes minimizing \u03c8(x) over S k to identify the remaining zero groups, i.e., I 0 (x * )/I 0 (x k ), which is formulated as the following problem:\nminimize x\u2208S k \u03c8(x) = f (x) + \u03bbr(x). (5)\nThe next iterate x k+1 is computed as an solution estimate of problem (5).\n\nParticularly, in Algorithm 2, [x k+1 ] I 0 (x k ) \u2261 0 will not be updated, and only the entries in I =0 (x k ) are free to move. Hence \u03c8(x) is smooth on S k , and (5) is a reduced space optimization problem. A standard way to solve problem (5) would be the stochastic gradient descent equipped with Euclidean projection [58]. However, such a projected method rarely produces zero (group) variables, as the dense Euclidean projected pointx E = 0 illustrated in Figure 3a. To address, we introduce a novel half-space projection operator to effectively project an entire group of variables to zeros.\n\nAs line 4 and 9-12 in Algorithm 2, we first approximate the (sub)gradient of \u03c8 on the free variables by [\u03bd(x k )] I =0 (x k ) , then employ gradient descent over I =0 (x k ) to compute a trial point x k+1 which is passed into a fresh half-space projection operator Proj HS S k (\u00b7) defined as\nProj HS S k (z) g := 0 if [z] g [x k ] g < [x k ] g 2 , [z] g otherwise.(6)\nThe above projector of form (6) is not the standard one in Euclidean sense 2 , and it has two advantages: Figure 3a, hence the progress to the optimum is made via the sufficient decrease property drawn as Lemma 1 in Appendix B; then (ii) it effectively projects entire groups of variables to zero if the inner product of corresponding entries is sufficiently small. In contrast, the Euclidean projection operator is far away effective to promote group sparsity.\n(i) the actual search direction d k := (Proj HS S k (x k+1 ) \u2212 x k )/\u03b1 k performs as a descent direction to \u03c8(x k ), i.e., [d k ] g [\u03bd(x k ))] g < 0 as \u03b8 < 90 \u2022 in\nSuperiority of HSPG on Group Sparsity Identification. We now intuitively illustrate the strength of HSPG on group sparsity exploration. In fact, the half-space projection (6) is a more effective sparsity promotion mechanism compared to the standard proximal methods. Particularly, it benefits from a much larger projection region to map a reference pointx k+1 := x k \u2212 \u03b1 k \u2207f (x k ) or its variants to zero. As the 2D case described in Figure 3b, the projection regions of the state-of-the-art Prox-SG [17], Prox-SVRG [75], Prox-Spider [82] and SAGA [11] for (2) are 2 -balls with radius as \u03b1 k \u03bb. In deep learning applications, the step size \u03b1 k is usually selected around 10 \u22123 to 10 \u22124 or even smaller for convergence. Together with the common setting of \u03bb 1, their projection regions would vanish rapidly, resulting in the difficulties to produce group sparsity. As a sharp contrast, even though \u03b1 k \u03bb is near zero, the projection region of HSPG {x :\nx k x < (\u03b1 k \u03bb + x k ) x k } (seen in Appendix B)\nis still an open half-space which contains those 2 balls as well as RDA [74]'s if is large enough. Conversely, vanilla ADMM alone lacks the mechanism to project a group of variables to zero, unless equips with extra post-processing step [85,51]. In Appendix B, we further reveal that HSPG still maintains the convergence to the optimality as drawn in Theorem 1. Moreover, we numerically demonstrate the superiority of HSPG in the sense of optimization in Appendix C.\n\n\nPruning Without Fine-Tuning\n\nThe group-sparse solution x * HSPG over ZIGs to the full model M is leveraged to construct the slimmer model M * . Particularly, we prune the redundant structures identified as zero groups I 0 and retain non-zero groups I =0 in x * HSPG . Because the parameters of full model are partitioned into ZIGs, the pruned structures contribute none to the model output. Therefore, given the same input, the slimmer model M * computes the identical output as the full model M parameterized with x * HSPG .\n\n\nExperiment\n\nIn this section, we numerically demonstrate the effectiveness of OTO by one-shot training and pruning without fine-tuning on several benchmark compression tasks for CNNs, i.e., VGG16 [65] for CIFAR10 [43] and ResNet50 [32] for CIFAR10 [43] and ImagetNet (ILSVRC2012) [13]. We also verify the scalibility of OTO onto Bert [70] evaluated on SQuAD [59]. All datasets are free to academic usage and do not contain personally identifiable information or offensive content. CIFAR10 is under the MIT license, consisting of 50,000 training and 10,000 test images from 10 classes. ImagetNet is a large-scale dataset without license and contains about 1.2 million and 50,000 images in training and validation sets from 1,000 classes. SQuAD is under the CC BY-SA 4.0 license with about 100,000 question/answer pairs splitted into train/dev/test sets as (80/10/10%). We conduct all experiments on a Nvidia RTX8000 GPU and provide implementation details in Appendix A. 51-62-125-128-228-129-38-13-9-6-5-6-6-6-20 38.5% 5.4% 91.0% RBC [86] 43-62-120-120-182-113-40-12-20-11-6-9-10-10-22 32.3% 3.9% 90.5% RBP [86] 50-63-123-108-104-57-23-14-9-8-6-7-11-11-12 28 \n\n\nDeep Convolutional Neural Network\n\nThe results on CNN experiments are summarized in Table 1, 2 and 4. In particular, we compare OTO to its state-of-the-art counterparts by Top-1/5 accuracy, remaining FLOPs and parameters against the corresponding baseline (full model). We report the numbers of other methods based on the corresponding literature and leave as '-' if not reported. The best pruning results are marked as bold.\n\nVGG16 for CIFAR10. We consider the standard VGG16 and the version with batch normalization layer after each convolutional layer, referred to as VGG16-BN. OTO partitions the parameters into ZIGs following Section 3.1, then trains and prunes the model via HSPG, and finally constructs the slimmer model without fine-tuning. For VGG16, as shown in  Table 7 of Appendix A.4. ResNet50 for CIFAR10. Since OTO is able to automatically learn a slimmer model of high performance, we compare it with two state-of-the-art automatic neural network compression frameworks, i.e., AMC [34] and ANNC [77]. AMC trains a reinforcement learning agent to predict compression action for each layer environment. ANNC jointly proceeds pruning and quantization within energy constraint. We conduct OTO on their shared experiment, i.e., ResNet50 on CIFAR10. ResNet50 includes both the standard convolutional layers and the layers with residual connections, which are partitioned into ZIGs following Section 3.1. We report the results in Table 2 along with other competitors from [54, 66]. Based on the results, all methods achieve competitive validation accuracies, where most of them are even higher than the baseline reported in [34]. OTO outperforms AMC, ANNC without quantization, PruneTrain and N2NSkip by using only 12.8% FLOPs and 8.8% parameters. Note that no FLOPs reduction is reported in [34] and [77]. Finally, we highlight that OTO is flexible to incorporate quantization as the two techniques are complementary and will leave to future work. Ablation Study on Switching Parameter N . We provide ablation study regarding the impact the switch (parameterized as N ) between the initialization stage and the groupsparsity stage in Algorithm 1. In theory, as shown in Theorem 1 of Appendix B.4, the projection stage should start when the iterate falls nearby a group sparse local minimizer. In practice, we relax it to start the group sparsity stage once the iterate falling into some stationary status regarding the validation accuracy. As described in Appendix A.2, throughout all experiments, we periodically decay the learning rate per fixed number of epochs parameterized as T .\n\nAt the end of each T epochs, we then proceed a statistical test similar to [83] but on the validation accuracy and find that the validation accuracy falls into stationarity near the late epochs of each period. Therefore, in our pruning experiments, we switch to the group-sparsity stage right after the first T epochs. Table 3 describes the performance of OTO under varying switching parameters, from which we observe that OTO is not largely sensitive to the switching parameter if the group-sparsity stage starts after some stationary condition has been numerically satisfied. prunes CNNs via structured-sparsity optimization by employing standard stochastic proximal gradient method. It requires several trainings including fine-tuning the pruned model, because it partitions the parameters into non-ZIGs and relies on an empirical truncation mechanism to generate zero groups due to the weakness of proximal operator in deep learning applications [7]. In contrast, OTO only trains and prunes the full model from scratch once and obtains better pruning results. The comparison between OTO and Hinge stand as evidence of the superiority of OTO due to ZIGs and HSPG. Furthermore, if with more training efforts, OTO reaches higher Top-1/5 accuracy marked as * in Table 4 and becomes more competitive to stronger competitors, such as GBN [80], Group-HS [78] and ResRep [42].\n\nRepresentation of Deep Features of ImageNet. It is widely acknowledged that deep neural architectures could be treated as non-linear feature representation extractors. Therefore, we further study the feature representation extracted by OTO to demonstrate its generalizability to other visual applications besides image classification. Figure 4 shows the clustering results of ImageNet validation images using the deep feature extracted by both the baseline ResNet50 and the pruned ResNet50 by OTO. Specifically, we extract the deep features over the validation samples in ImageNet, i.e., the tensors fed into the fully connected layer, and project them onto a 2-dimensional space via PCA [41]. For illustration, following the hierarchy of ImageNet [3], two sets of five classes are randomly selected 3 . We observe that the deep features of the pruned ResNet50 by OTO remain structured in the sense that distinct classes are well separated from each other. Over all 1000-class ImageNet validation images, OTO achieves 48.2% clustering accuracy compared to 42.5% of the baseline ResNet50 using k-means. Both observations indicate that with only 35.5% parameters and 34.5% FLOPs, the pruned ResNet50 is still able to extract highly discriminative deep features. We argue that during model compression, OTO not only achieves parameter and FLOPs reduction, but also preserves the ability of capturing perceptual properties [84]. This is especially important in training and compressing models for many vision tasks, e.g., object detection [60, 61], frame interpolation [2,15,57] and video synthesis [72,44]. We leave the application of OTO to broader tasks to future work.\n\n\nLarge-Scale Transformer\n\nWe show the scalability of OTO by pruning the large-scale transformer Bert [70], evaluated on SQuAD, a question-answering benchmark [59]. Bert mainly includes embedding layers, fully connected layers and multi-head attention layers. The fully connected layers and the multi-head attention layers are partitioned into ZIGs following Section 3.1. For fair comparisons, we follow the prior Bert compression works [12,63] and do not prune the embedding layers.  \u2020 Approximate value based on the group sparsity reported in [12].\n\nTo the best of our knowledge, OTO is the first work that compresses Bert by exploring group sparsity on individual layers and achieves significant parameter reduction and inference speedup 4 . In contrast, the existing works [26,63,27] prune individual parameters instead, i.e., the generated sparsity is not structured. Hence, the computed models typically do not have inference speedup [63], unless are executed by specialized hardware and sparse computing library [28,9]. As shown in Table 5, under different group sparsity upper bound constraints, OTO reduces 9% to 60% parameters and achieves up to 1.8\u00d7 inference speedup based on the average model execution time 5 . In comparison, despite that the pruned model contains 10% parameters, MaP and MvP [63] do not have any inference speedup. On the other hand, the structured sparsity on Bert is studied in [12] (referred to as ProxSSI), where an adaptive proximal method is proposed to yield group-sparse solution. Nonetheless, ProxSSI optimizes over non-ZIGs and relies on proximal operator to identify group sparsity. Therefore, the groups even parameterized with zeros have to be retained in the model rather than pruned. As a consequence, ProxSSI is not competitive to OTO on parameter reduction, and there is no reported inference speedup. Note that all the pruning methods achieve comparable exact match rate and F1-score.\n\n\nConclusion And Future Work\n\nWe propose OTO, a one-shot deep neural networks (DNNs) training and pruning framework, that compresses full DNNs into slimmer architectures with competitive performances and significant FLOPs and parameter reduction without fine-tuning. OTO (7) and (8) respectively. They are then summed up as the input tensor of the subsequent identify block I I1 , indicating that O 1 and O 2 have the same shape and their flattened filter matricesK 1 andK 2 has the same number of rows. As (11), I I1 later sums the output tensor of ResConv-BN3 O 3 to yield the input tensor to the next Identity Block I I2 , implying the filter matrix of ResConv-BN3K 3 has the same number of rows asK 1 andK 2 . Figure 6: Zero Invariant Groups for the three ResConv-BN of a Group Block.\nO 1 \u2190 a(I 1 \u2297K 1 + b 1 ) \u2212 \u00b5 1 \u03c3 1 \u03b3 1 + \u03b2 1 (7) O 2 \u2190 a(I 2 \u2297K 2 + b 2 ) \u2212 \u00b5 2 \u03c3 2 \u03b3 2 + \u03b2 2(8)I I1 \u2190 O 1 + O 2 (9) O 3 \u2190 a(I 3 \u2297K 3 + b 3 ) \u2212 \u00b5 3 \u03c3 3 \u03b3 3 + \u03b2 3(10)I I2 \u2190 I I1 + O 3 (11) g 1 g 2 g m K 1 K 2 b 1 b 2 \u03b3 1 \u03b3 2 G = {g 1 , g 2 , \u00b7 \u00b7 \u00b7 , g m } \u03b2 1 \u03b2 2 \u03b2 3 \u03b3 3 b 3 K 3\nTherefore, based on (7) to (11), to make the entire Group Block zero-invariant, we group each i th row of the filter matrix for all the Res-Conv-BNs of same group block. In doing so, any one row of parameters being zeros results in the output, i.e., the corresponding channel of feature map, being zeros. Figure 6 shows ZIG for the three ResConv-BN of a Group Block. Regardless of the input, the i th channel-wise matrix of I I1 are zeros if and only if both i th channel-wise matrices of O 1 and O 2 are equal to zero. This is equivalent to both i th rows ofK 1 andK 2 being zeros. Similarly, i th channel-wise matrix of I I2 being zeros regardless of the input further requires the i th row ofK 3 to be grouped in the ZIG.\n\n\nA.2 Training Details\n\nWe implement OTO in PyTorch. The key ingredient HSPG is packaged as an optimizer class which is flexible to various applications. During the experiment, the trainable parameters of the full model M are firstly partitioned into a ZIG set G wherein each group is tagged as its corresponding atomic layer category, e.g., fully-connected layer or convolutional layer. The ZIG set G is treated as an argument to the HSPG constructor. Then M is trained from scratch by HSPG where the group-wise variables are updated based on their tagged layer category. In our repository, we provide the prescribed ZIG partitions for the DNNs used in this paper, i.e., VGG16, VGG16-BN, ResNet50 and Bert. For other models, one can easily follow Section 3.1 and Appendix A.1 to construct a ZIG partition and feed it as an argument to the HSPG optimizer. After training, a full group-sparse model with high performance is achieved. Finally, a slimmer pruned model M * is constructed following Section 3.4 without fine-tuning and has the identical performance as the full group-sparse model. We provide the implementation in https://github.com/tianyic/only_train_once and the pruned models associated with the corresponding full group sparse models in https://tinyurl.com/ otocheckpoints.\n\nParameter Settings. We conduct all experiments on an Nvidia RTX8000 graphics card with 48 GB memory. For all CNN experiments, the step size (learning rate) \u03b1 k is initialized as 10 \u22121 , and decayed by a factor 0.1 periodically T epochs till the minimum value 10 \u22124 . The selection of T depends on the max number of epochs K. We follow various benchmark online resources to select K. Particularly, for all CIFAR10 experiments, we follow the model pre-training settings in [49] and set K as 300. Note that by using the same number of epochs, OTO achieves both slimmer model and competitive performance simultaneously. For the ImageNet experiment, following [32], we set T as 30 and K as 120. For all Bert experiments, the step size \u03b1 k is initialized as 10 \u22121 and decayed by a factor 0.1 after 4 epochs to be as 10 \u22122 .\n\nWe set the mini-batch size as the commonly used 64 for CIFAR10, 256 for ImageNet and 32 for SQuAD experiments. For all experiments, we initialize the regularization coefficient \u03bb as 10 \u22123 to balance between model performance and group sparsity. In particular, \u03bb as 10 \u22123 is the maximum value from the candidate set {10 \u22122 , 10 \u22123 , 10 \u22124 } which returns competitive evaluation results to the majority of the tested models trained without regularization. In addition, to favor more on the model performance, if group sparsity becomes stationary, we decay \u03bb by a factor 0.1 periodically after stepping into Group-Sparsity Stage. The control parameter \u2208 [0, 1) in the half-space projection (6) controls the aggressiveness level of group sparsity promotion, which is typically fixed as 0 since for most of experiments, \u2261 0 has resulted in sufficiently good experiment results. In case if group sparsity is not sufficiently yielded, we provide an adaptive mechanism to increase by 0.1 till the upper bound 0.999. For the setting of N which controls when switching to the Group-Sparsity Stage, we proceed a test on objective value stationarity similarly to [83, Section 2.1] and empirically set N \u2261 T for CNN experiments since the validation accuracy values become stationary at the late epochs till T . Hence, the Group-Sparsity Stage starts after T epochs and is accompanied with the \u03b1 k decaying. For Bert experiment, we empirically set N as 1 since the F1-score and exact match rate becomes stationary after one epoch training.\n\nAdditional Remarks. For the ZIG partition of ResNet50 on CIFAR10, we include all trainable variables of ResNet50 and apply the ZIG partition described in Appendix A.1 for ResConv-BN and the ZIG partition described in Section 3.1 for standard Conv-BN. For the ZIG partition of ResNet50 on ImageNet, we construct ZIGs for standard Conv-BN only. This is because we observe that ZIG partition for ResConv-BN lead to accuracy regression in spite of more FLOPs reduction, (15% FLOPs with up to 71% Top-1 Accuracy). The cause is that it decreases the number of features maps generated by the entire Group Block. Additionally, for Bert experiments, to study the accuracy evolution against different compression rates, we set extra constraints to bound the maximum group sparsity ratio, e.g., 30%, 50%, 70%, and do not yield new zero groups if the upper bound has been exceeded. Note that without any constraint, OTO reaches about 95% group sparsity ratio with 80% F1-score.\n\n\nA.3 Error Bar Analysis\n\nIn this section, we report the overall statistics of the experiments and analyze the error bar. We note that for fair comparison with others, in the main body of paper, we report the best results in terms of remaining FLOPs/parameters and Top-1/5 accuracy. We conduct all experiments three times with different random seeds. Training neural networks is equivalent to solving a non-convex optimization problem which has numerous local minimizers, thereby training from scratch like OTO may generate solutions close to stationary points with different attributes. As shown in Table 6, we can see that for the CNN experiments, OTO performs reliably to achieve significant FLOPs and parameters reduction and competitive Top-1 accuracy with small fluctuations.\n\n\nA.4 FLOPs Reduction Breakdown\n\nWe provide the layer-wise FLOPs reduction for VGG16 on CIFAR10. As shown in Table 7, the majority of the FLOPs reduction via OTO comes from a few middle ConvLayers (over 10% to the overall FLOPs reductions) instead of the first ConvLayer (0.45% to the overall FLOPs reduction). In general, the distribution of FLOPs reduction per Layer of OTO is similar to other pruning baselines. \n\n\nB Convergence Analysis of HSPG\n\nIn this section, we provide theoretical analysis of HSPG. We focus on the most popular setting of optimization problem (2) as follows\nminimize x\u2208R n \u03c8(x) := f (x) + \u03bbr(x), f (x) := 1 N N i=1 f i (x),(12)\nHere f (x) is defined as the average of N task-specific loss functions f i : R n \u2192 R, \u2200 i = 1, . . . , N . The stochastic gradient \u2207f proposed in Section 3.3 can be obtained via a uniformly chosen mini-batch B \u2286 [N ] as follows: for any x \u2208 R n , given B, we have\n\u2207f (x) = \u2207 1 |B| i\u2208B f i (x) =:f B (x) ,(13)\nin short, we denote above term as\n\u2207f B (x) where f B (x) is the average of loss functions with repsect to mini-batch B. Similarly, let \u03c8 B (x) := f B (x) + \u03bbr(x) for all x \u2208 R n .\nOrganization. The Section B is organized as follows: From Section B.1 to Section B.5, we present the convergence result and the sparse recovery guarantee for Half-Space\n\nStep. More specifically,\n\n\u2022 In Section B.1, we first presented the existing related work of solving the problem (12).\n\n\u2022 In Section B.2, we show the sufficient decrease of Half-Space\n\nStep under Assumption 1.\n\n\u2022 In Section B.3, we derive the projection region of Half-Space\n\nStep and compare this projection region with existing methods.\n\n\u2022 In Section B.4, we give the convergence result of Half-Space Step as stated in Theorem 1 under the Assumption (2, 3).\n\nTo complete the story, in Section B.5, we show that the \"close enough\" condition required in Theorem 1 can be achieved by the Sub-gradient Descent\n\nStep under the Assumption 5. Moreover, we further point out that: (1) the Sub-gradient Descent\n\nStep we used to achieve a \"close enough\" solution can be replaced by other methods, and (2) the Assumption 4 is only a sufficient condition that we could use to show the \"close enough\" condition.\n\n\nB.1 Related Work\n\nProblem (12) has been well studied in deterministic optimization with various algorithms that are capable of returning solutions with both low objective value and high group sparsity under proper \u03bb [?, ?, ?, ?]. Proximal methods are classical approaches to solve the structured non-smooth optimization (12), including the popular proximal gradient method (Prox-FG) which only uses the first-order derivative information. When N is huge, stochastic methods become ubiquitous to operate on a small subset to avoid the costly evaluation over all instances in deterministic methods for large-scale problems. Proximal stochastic gradient method (Prox-SG) [17] is the natural stochastic extension of Prox-FG. Regularized dual-averaging method (RDA) [74,79] is proposed by extending the dual averaging scheme in [?]. To improve the convergence rate, there exists a set of incremental gradient methods inspired by SAG [?] to utilizes the average of accumulated past gradients. For example, proximal stochastic variance-reduced gradient method (Prox-SVRG) [75] and proximal spider (Prox-Spider) [82] are developed to adopt multi-stage schemes based on the well-known variance reduction technique SVRG proposed in [?] and Spider developed in [?] respectively. SAGA [11] stands as the midpoint between SAG and Prox-SVRG.\n\nCompared to deterministic methods, the studies of structured sparsity regularization (12) in stochastic field become somewhat rare and limited. Prox-SG, RDA, Prox-SVRG, Prox-Spider and SAGA are valuable state-of-the-art stochastic algorithms for solving problem (12) but with apparent weakness. Particularly, these existing stochastic algorithms typically meet difficulties to achieve both decent convergence and effective group sparsity identification simultaneously (e.g., small function values but merely dense solutions), because of the randomness and the limited sparsity-promotion mechanisms. In depth, Prox-SG, RDA, Prox-SVRG, Prox-Spider and SAGA derive from proximal gradient method to utilize the proximal operator to produce group of zero variables. Such operator is generic to extensive non-smooth problems, consequently perhaps not sufficiently insightful if the target problems possess certain properties, e.g., the group sparsity structure as problem (12). In fact, in convex setting, the proximal operator suffers from variance of gradient estimate; and in non-convex setting, especially deep learning, the discreet step size (learning rate) further deteriorates its effectiveness on the group sparsity promotion, as shown in Section 3.3 of the main body that the projection region vanishes rapidly except RDA. RDA has superiority on finding manifold structure to others [?], but inferiority on the objective convergence. Besides, the variance reduction techniques are typically required to measure over a huge mini-batch data points in both theory and practice which is probably prohibitive for large-scale problems, and have been observed as sometimes noneffective for deep learning applications [?]. On the other hand, to introduce sparsity, there exist heuristic weight pruning methods [48,53], whereas they commonly do not equip with theoretical guarantee, so that easily diverge and hurt generalization accuracy.\n\n\nB.2 Sufficient Decrease of Half-Space Step\n\nBefore we present the convergence result of Half-Space\n\nStep to the global group-sparsity solution, in this part, we first show that the sufficient decrease property holds for Half-Space\n\nStep under the following Assumption 1. Assumption 1. Assume the following assumptions hold.\n\n\u2022 (A1-1). f : R n \u2192 R is differentiable and L smooth.\n\n\u2022 (A1-2). r : R n \u2192 R is sub-differentiable and convex.\n\n\u2022 (A1-3). \u03c8 = f + \u03bbr : R n \u2192 R is sub-differentiable over all points x \u2208 R n .\n\nFor any k > N P (in Half-Space\n\nStep of Algorithm 2), recall the next iterate x k+1 and the search direction\nd k := x k+1 \u2212 x k \u03b1 k = Proj HS S k (x k \u2212 \u03b1 k \u2207\u03c8 B k (x k )) \u2212 x k \u03b1 k .(14)\nDefine\u011c\nk := I =0 (x k ) \u2229 I 0 (x k+1 )(15)G k := I =0 (x k ) \u2229 I =0 (x k+1 )(16)\nbe the sets of groups which projects or not onto zero. We claim that the following Lemma 1 holds. Lemma 1. Under Assumption 1, the search direction d k is a descent direction for \u03c8 B k (x k ), i.e., d k \u2207\u03c8 B k (x k ) < 0. Moreover, we have the following sufficient decrease property holds,\n\u03c8B k (x k+1 ) \u2264\u03c8B k (x k ) \u2212 \u03b1 k \u2212 \u03b1 2 k L 2 g\u2208G k [\u2207\u03c8B k (x k )]g 2 \u2212 1 \u2212 \u03b1 k \u2212 L 2 g\u2208\u011c k [x k ]g 2 . (17)\nProof. Proof of Descent Direction. It follows the Half-Space Step in Algorithm 2 and the definition ofG k and\u011c k as (16) and (15) \nthat x k+1 = x k + \u03b1 k d k where d k is [d k ] g = \uf8f1 \uf8f2 \uf8f3 \u2212[\u2207\u03c8 B k (x k )] g if g \u2208G k = I =0 (x k ) I =0 (x k+1 ), \u2212[x k ] g /\u03b1 k if g \u2208\u011c k = I =0 (x k ) I 0 (x k+1 ), 0 otherwise.(18)\nWe also notice that for any g \u2208\u011c k , the following holds\n[x k \u2212 \u03b1 k \u2207\u03c8 B k (x k )] g [x k ] g < [x k ] g 2 , (1 \u2212 ) [x k ] g 2 < \u03b1 k [\u2207\u03c8 B k (x k )] g [x k ] g .(19)\nFor simplicity, let I =0 k := I =0 (x k ). Since [d k ] g = 0 for any g \u2208 I 0 (x k ), then by (18) and (19), we have\nd k \u2207\u03c8 B k (x k ) = [d k ] I =0 k [\u2207\u03c8 B k (x k )] I =0 k = \u2212 g\u2208G k [\u2207\u03c8 B k (x k )] g 2 \u2212 g\u2208\u011c k 1 \u03b1 k [x k ] g [\u2207\u03c8 B k (x k )] g \u2264 \u2212 g\u2208G k [\u2207\u03c8 B k (x k )] g 2 \u2212 g\u2208\u011c k 1 \u03b1 2 k (1 \u2212 ) [x k ] g 2 < 0,(20)\nholds for any \u2208 [0, 1), which implies that d k is a descent direction for \u03c8 B k (x k ).\n\nProof of Sufficient Decrease. Now, we start to prove the suffcient decrease of Half-Space\n\nStep. By assumption, f : R n \u2192 R is L smooth and r : R n \u2192 R is convex. Therefore\n\u03c8 B k (x k + \u03b1 k d k ) (21) = f B k (x k + \u03b1 k d k ) + \u03bbr(x k + \u03b1 k d k ) (22) \u2264 f B k (x k ) + \u03b1 k d k \u2207f B (x k ) + \u03b1 2 k L 2 d k 2\nby Assumption 1 (23)\n+ \u03bbr(x k ) + \u03b1 k \u03bbd k \u03b6(x k ) (24) = \u03c8 B k (x k ) + \u03b1 k d k \u2207\u03c8 B k (x k ) + \u03b1 2 k L 2 d k 2 (25) \u2264 \u03c8 B k (x k ) \u2212 \u03b1 k \u2212 \u03b1 2 k L 2 g\u2208G k [\u2207\u03c8 B k (x k )] g 2 by inequality (20) & d k definition (26) \u2212 1 \u2212 \u03b1 k \u2212 L 2 g\u2208\u011c k [x k ] g 2 ,(27)\nwhich completes the proof.\n\nAccording to Lemma 1, the objective value \u03c8 B (x) with E[\u03c8 B (x)|x] = \u03c8(x) achieves a sufficient decrease in Half-Space Step given \u03b1 k is small enough. Taking the expectation over mini-batch B on both sides, it is straight-forward to obtain the expectation version of the sufficient decrease property. Corollary 1. Similarly, under Assumption 1, for all k > N P , we have\n\u03c8(x k+1 ) \u2264 \u03c8(x k ) \u2212 g\u2208G k \u03b1 k \u2212 \u03b1 2 k L 2 E [\u2207\u03c8 B k (x k )] g 2 \u2212 1 \u2212 \u03b1 k \u2212 L 2 g\u2208\u011c k [x k ] g 2 .(28)\n\nB.3 Projection Region of Half-Space Step\n\nIn this part, we derive the projection region of Half-Space\n\nStep, and reveal that is a superset of the projection region of existing methods, e.g. Prox-SG, Prox-SVRG and Prox-Spider, under the same \u03b1 k and \u03bb. Proposition 1. For any k > N P , given x k , the next iterate x k+1 obtained by the Half-Space Step satisfies that: for any group g \u2208 I =0 (x k ),\n[x k+1 ] g = [x k+1 ] g \u2212 \u03b1 k \u03bb [x k ]g [x k ]g if [x k+1 ] g [x k ] g > (\u03b1 k \u03bb + ) [x k ] g , 0 otherwise,(29)wherex k+1 := x k \u2212 \u03b1 k \u2207f B k (x k ). Moreover, we claim that if [x k+1 ] g \u2264 \u03b1 k \u03bb, then [x k+1 ] g = 0 for any \u2265 0.\nProof. For g \u2208 I =0 (x k ) I =0 (x k+1 ), by line 11-12 in Algorithm 2, it is equivalent to\nx k \u2212 \u03b1 k \u2207f B k (x k ) \u2212 \u03b1 k \u03bb [x k ] g [x k ] g g [x k ] g > [x k ] g 2 , [x k+1 ] g [x k ] g \u2212 \u03b1 k \u03bb [x k ] g > [x k ] g 2 , [x k+1 ] g [x k ] g > (\u03b1 k \u03bb + [x k ] g ) [x k ] g .(30)\nSimilarly, g \u2208 I =0 (x k ) I 0 (x k+1 ) is equivalent to\nx k \u2212 \u03b1 k \u2207f B k (x k ) \u2212 \u03b1 k \u03bb [x k ] g [x k ] g g [x k ] g \u2264 [x k ] g 2 , [x k+1 ] g [x k ] g \u2212 \u03b1 k \u03bb [x k ] g \u2264 [x k ] g 2 , [x k+1 ] g [x k ] g \u2264 (\u03b1 k \u03bb + [x k ] g ) [x k ] g .(31)\nIf [x k+1 ] g \u2264 \u03b1 k \u03bb, then\n[x k+1 ] g [x k ] g \u2264 [x k+1 ] g [x k ] g \u2264 \u03b1 k \u03bb [x k ] g .(32)\nHence [x k+1 ] g = 0 holds for any \u2265 0 by (31), which implies that the projection region of Prox-SG and its variance reduction variants, e.g., Prox-SVRG, Prox-Spider and SAGA are the subsets of HSPG's.\n\n\nB.4 Convergence Analysis of Half-Space Step\n\nIn this section, we give the convergence result of Half-Space\n\nStep under the following Assumptions for the properties of the objective function and the global optimal solution x * of (2). Assumption 2. Assume the following assumptions hold.\n\n\u2022 (A2-1). For i = 1, 2, \u00b7 \u00b7 \u00b7 , N , each f i : R n \u2192 R is differentiable and bounded below.\n\n\u2022 (A2-2). For i = 1, 2, \u00b7 \u00b7 \u00b7 , N , each f i : R n \u2192 R is L i smooth.\n\n\u2022 (A2-3). \u03c8 B = f B + \u03bbr : R n \u2192 R has bounded sub-gradient (i.e., E[ \u2207\u03c8 B (x) 2 ] \u2264 M 2 for some universal constant M ) over all points x \u2208 R n with respect to any mini-batch\nB \u2286 [N ]. \u2022 (A2-4). The stochastic gradient \u2207f B (x) satisfies E B [\u2207f B (x)|x] = \u2207f (x) for all x \u2208 R n . \u2022 (A2-5). The stochastic gradient \u2207f B (x) satisfies Var B [\u2207f B (x)|x] \u2264 \u03c3 2 for all x \u2208 R n , where \u03c3 2 > 0 is a universal constant.\nNotice that this Assumption 2 is a variant of the Assumption 1, to be concise, we set L proposed in Assumption 1 as L := max N i=1 {L i }. Assumption 3. Assume the following assumptions hold.\n\n\u2022 (A3-1). k\u2265N P \u03b1 k = \u221e.\n\n\u2022 (A3-2). k\u2265N P \u03b1 2 k < \u221e. Assumption 4. The least and the largest 2 -norm of non-zero groups in x * are lower and upper bounded by some constants,\n0 < 2\u03b4 1 := min g\u2208I =0 (x * ) [x * ] g \u2264 max g\u2208I =0 (x * ) [x * ] g =: 2\u03b4 2 .(33)\nTheorem 1. Under Assumptions (1, 2, 3, 4), set\nR \u2208 0, min 1 \u00b7 \u2212(\u03b4 1 + 2 \u03b4 2 ) + (\u03b4 1 + 2 \u03b4 2 ) 2 \u2212 4 2 \u03b4 2 + 4 \u03b4 2 1 , \u03b4 1 ,(34)\n\u2208 0, min \u03b4 2\n1 \u03b4 2 , 2\u03b4 1 \u2212 R 2\u03b4 2 + R ,(35)\u03b1 k \u2208 0, min 2(1 \u2212 ) L , 1 L , 2\u03b4 1 \u2212 R \u2212 (2\u03b4 2 + R) M , \u2200k \u2265 N P .(36)\nIf there exists a K \u2265 N such that\nx K \u2212 x * \u2264 R 2 .(37)\nGiven any \u03c4 \u2208 (0, 1), there exists some \u03b1 k = O(1/(1 + \u221a \u03c4 )(k \u2212 K)) and |B k | = O(k \u2212 K) for all k \u2265 K such that the sequence {x k } k\u2265K obtained from the Algorithm 2 converges to some stationary point with probability at least 1 \u2212 \u03c4 , i.e.,\nlim inf k E [ \u2207\u03c8 B k (x k ) ] = 0 with probability 1 \u2212 \u03c4.(38)\nProof. Proof Sketch. We split the proof of showing the convergence to some stationary points into two parts. In the first part, we show the convergence holds for all groups inG k ; and in the second part, we show the convergence also holds in\u011c k .\n\nConvergence inG k part. For any t \u2208 N + , applying Corollary 1 yields\n\u03c8(x N P ) \u2212 \u03c8(x N P +t ) (39) = N P +t\u22121 k=N P \u03c8(x k ) \u2212 \u03c8(x k+1 ) (40) \u2265 N P +t\u22121 k=N P g\u2208G k \u03b1 k \u2212 \u03b1 2 k L 2 E [\u2207\u03c8 B k (x k )] g 2 + N P +t\u22121 k=N P 1 \u2212 \u03b1 k \u2212 L 2 g\u2208\u011c k [x k ] g 2 .(41)\nCombining the assumption that \u03c8 is bounded below and letting t \u2192 \u221e yield\n\u221e k=N P g\u2208G k \u03b1 k \u2212 \u03b1 2 k L 2 E [\u2207\u03c8 B k (x k )] g 2 =:T1 + \u221e k=N P 1 \u2212 \u03b1 k \u2212 L 2 g\u2208\u011c k [x k ] g 2 =:T2 < \u221e.(42)\nGiven\n\u03b1 k \u2208 (0, 2(1 \u2212 )/L), we have T 1 > 0, T 2 > 0, combining with T 1 + T 2 < \u221e implies \u221e k=N P g\u2208G k \u03b1 k \u2212 \u03b1 2 k L 2 E [\u2207\u03c8 B k (x k )] g 2 (43) = \u221e k=N P g\u2208G k \u03b1 k E [\u2207\u03c8 B k (x k )] g 2 \u2212 \u221e k=N P g\u2208G k \u03b1 2 k L 2 E [\u2207\u03c8 B k (x k )] g 2 .(44)\nBased on the boundness of sub-gradient in Assumptions 2 and the choice of stepsize in 3, we have\n\u221e k=N P g\u2208G k \u03b1 2 k L 2 E [\u2207\u03c8 B k (x k )] g 2 < \u221e,(45)\nwhich yields\n\u221e k=N P g\u2208G k \u03b1 k E [\u2207\u03c8 B k (x k )] g 2 < \u221e (46) \u21d2 lim inf k\u2265N P g\u2208G k E [\u2207\u03c8 B k (x k )] g 2 = 0 (47) \u21d2 lim k\u2265K g\u2208G k E [\u2207\u03c8 B k (x k )] g 2 = 0, \u2203 K \u2286 {N P , . . .}(48)\nConvergence in\u011c k part. Under Assumption 4, Lemma (2,3,4) show that if there exists a K \u2265 N P such that\nx K \u2212 x * \u2264 R,(49)\nthen we have the following results hold\nI =0 (x * ) \u2286 I =0 (x K ), non-zero group coverage, (50) x * \u2208 S K , correct optimal inclusion S K ,(51)I =0 (x K ) \u2229 I =0 (x K+1 ) \u2286 I =0 (x * ), correct zero group projection.(52)\nUnder Assumption (2, 3, 4), Lemma (5, 6, 7) and Corollary 2 show that: given any \u03c4 \u2208 (0, 1), with probability at least 1 \u2212 \u03c4 , for any k \u2265 K, x * inhabits S k . Therefore, for any k \u2265 K, any group g \u2208\u011c k = I =0 (x k ) \u2229 I =0 (x k+1 ) will be projected to zero group correctly with probability at least 1 \u2212 \u03c4 .\n\nConvergence over the whole space. Based on the discussion in\u011c k part, it is sufficient to focus on the subspace ofG k . Hence, (48) naturally implies that the sequence {x k } k\u2208K converges to some stationary point with high probability. By the above, we conclude that The Lemma 2 shows that if the optimal distance from the current iterate x k to any local minimizer x * is sufficiently small, then HSPG already covers the supports of x * , i.e., I =0 (x * ) \u2286 I =0 (x k ).\nP lim inf k E [ \u2207\u03c8 B k (x k ) ] = 0 \u2265 1 \u2212 \u03c4.(53\n\nLemma 2. Under Assumption 4, given any\nR \u2264 \u03b4 1 , for any k \u2265 N P , if x k \u2212 x * \u2264 R, then we have I =0 (x * ) \u2286 I =0 (x k ).\nProof. For any g \u2208 I =0 (x * ), we have that\n[x * ] g \u2212 [x k ] g \u2264 [x k \u2212 x * ] g \u2264 x k \u2212 x * \u2264 R \u2264 \u03b4 1 [x k ] g \u2265 [x * ] g \u2212 \u03b4 1 \u2265 2\u03b4 1 \u2212 \u03b4 1 = \u03b4 1 > 0(54)\nHence [x k ] g = 0, i.e., g \u2208 I =0 (x k ). Therefore,\nI =0 (x * ) \u2286 I =0 (x k ).\nThe Lemma 3 shows that if the distance between the current iterate x k and x * , i.e., x k \u2212 x * is sufficiently small, then x * inhabits the reduced space S k := S(x k ). Lemma 3. Under Assumption 4, for any k \u2265 N P , given \u2208 [0, \u03b4 2 1 /\u03b4 2 ) and\nR \u2264 R * := 1 \u00b7 \u2212(\u03b4 1 + 2 \u03b4 2 ) + (\u03b4 1 + 2 \u03b4 2 ) 2 \u2212 4 2 \u03b4 2 + 4 \u03b4 2 1 ,(55)if x k \u2212 x * \u2264 R, we have [x k ] g [x * ] g \u2265 [x k ] g 2 , g \u2208 I =0 (x * ).(56)\nConsequently, it implies x * \u2208 S k by the definition as (4).\n\nProof. For any g \u2208 I =0 (x * ),\n[x k ] g \u2264 [x * ] g + R \u2264 2\u03b4 2 + R,(57)\nand the R * defined in (55) is one of the roots of the quadratic z 2 + (4 \u03b4 2 + 2\u03b4 1 )z + 4 \u03b4 2 2 \u2212 4\u03b4 2 1 = 0 regarding z \u2208 R. Thus\n[x k ] g [x * ] g =[x k \u2212 x * + x * ] g [x * ] g =[x k \u2212 x * ] g [x * ] g + [x * ] g 2 \u2265 [x * ] g 2 \u2212 [x k \u2212 x * ] g [x * ] g = [x * ] g ( [x * ] g \u2212 [x k \u2212 x * ] g ) \u22652\u03b4 1 (2\u03b4 1 \u2212 R) \u2265 (2\u03b4 2 + R) 2 \u2265 [x k ] g 2(58)\nholds for any g \u2208 I =0 (x * ), where the second last inequality holds because that 2\u03b4 1 (2\u03b4 1 \u2212 R) = (2\u03b4 2 + R) 2 as R = R * . Now combing with the definition of S k as (4), we have x * inhabits S k , which completes the proof.\n\nThe Lemma 4 shows that if x k \u2212 x * is small enough and the step size is selected properly, every recovery of group sparsity by Half-Space\n\nStep can be guaranteed as successful as stated in the following lemma.\n\nLemma 4. Under Assumption 4, for any k \u2265 N P , given\n\u2208 0, 2\u03b41\u2212R 2\u03b42+R , \u03b1 k \u2208 0, 2\u03b41\u2212R\u2212 (2\u03b42+R) M\nand R \u2208 (0, min{R * , \u03b4 1 }), if x k \u2212 x * \u2264 R, then for any g \u2208\u011c k = I =0 (x k ) I 0 (x k+1 ), we have g \u2208 I 0 (x * ).\n\nProof. To prove it by contradiction, suppose there exists some g \u2208\u011c k such that g \u2208 I =0 (x * ). Since g \u2208\u011c k = I =0 (x k ) I 0 (x k+1 ), then the group projection (6) is trigerred at g such that\n[x k+1 ] g [x k ] g = [x k \u2212 \u03b1\u2207\u03c8 B k (x k )] g [x k ] g = [x k ] g 2 \u2212 \u03b1 k [\u2207\u03c8 B k (x k )] g [x k ] g < [x k ] g 2 .(59)\nOn the other hand, it follows the assumption of this lemma and g \u2208 I =0 (x * ) that\n[x k \u2212 x * ] g \u2264 x k \u2212 x * \u2264 R(60)\nCombining the definition of \u03b4 1 and \u03b4 2 in Assumption 4, we have that\n[x k ] g \u2265 [x * ] g \u2212 R \u2265 2\u03b4 1 \u2212 R [x k ] g \u2264 [x * ] g + R \u2264 2\u03b4 2 + R (61) It then follows 0 < \u03b1 k \u2264 2\u03b41\u2212R\u2212 (2\u03b42+R) M , where note 2\u03b4 1 \u2212 R \u2212 (2\u03b4 2 + R) > 0 as R \u2264 \u03b4 1 and < 2\u03b41\u2212R 2\u03b42+R , that [x k+1 ] g [x k ] g = [x k ] g 2 \u2212 \u03b1 k [\u2207\u03c8 B k (x k )] g [x k ] g \u2265 [x k ] g 2 \u2212 \u03b1 k [\u2207\u03c8 B k (x k )] g [x k ] g = [x k ] g ( [x k ] g \u2212 \u03b1 k [\u2207\u03c8 B k (x k )] g ) \u2265 [x k ] g ( [x k ] g \u2212 \u03b1 k M ) \u2265 [x k ] g [(2\u03b4 1 \u2212 R) \u2212 \u03b1 k M ] \u2265 [x k ] g (2\u03b4 1 \u2212 R) \u2212 2\u03b4 1 \u2212 R \u2212 (2\u03b4 2 + R) M M \u2265 [x k ] g [(2\u03b4 1 \u2212 R) \u2212 2\u03b4 1 + R + (2\u03b4 2 + R)] \u2265 [x k ] g (2\u03b4 2 + R) \u2265 [x k ] g 2(62)\nwhich contradicts with (59). Hence, we conclude that any g of variables projected to zero, i.e., g \u2208\u011c k = I =0 (x k ) I 0 (x k+1 ) are exactly also the zeros on the optimal solution x * , i.e., g \u2208 I 0 (x * ).\n\nWe next present that if the iterate of Half-Space\n\nStep is close enough to the optimal solution x * , then x * inhabits all reduced spaces constructed by the subsequent iterates of Half-Space\n\nStep with high probability.\n\nTo establish this results, we require the following two lemmas (Lemma 5 and Lemma 6). The Lemma 5 bounds the accumulated error because of random sampling. Here we introduce the error of gradient estimator on I =0 (x) for \u03c8 on mini-batch B as\ne B (x) := [\u2207\u03c8 B (x) \u2212 \u2207\u03c8(x)] I =0 (x) ,(63)\nwhere by the definition of r in problem (12), we have e B (x) also equals to the error of estimation for \u2207f , i.e., e B (x) = [\u2207f B (x) \u2212 \u2207f (x)] I =0 (x) . Lemma 5. Under Assumption 2, given any \u03b8 > 1, K \u2265 N P , let k := K + t with t \u2208 Z \u22650 , then there exists a sequence of stepsize \u03b1 k = O(1/(1 + \u03b8)t) and corresponding size of mini-batch |B k | = O(t), such that for any y t \u2208 R n ,\nmax {yt} \u221e t=0 \u2208X \u221e \u221e t=0 \u03b1 k e B k (y t ) 2 \u2264 3R 2 8(4R + 1)\nholds with probability at least 1 \u2212 1 \u03b8 2 .\n\nProof. Define random variable Y t := \u03b1 K+t e B K+t (y t ) 2 for all t \u2265 0. Since {y t } \u221e t=0 are arbitrarily chosen, then the random variables {Y t } \u221e t=0 are independent. Let Y := \u221e t=0 Y t . Using Chebshev's inequality, we obtain\nP Y \u2265 E[Y ] + \u03b8 Var[Y ] \u2264 P |Y \u2212 E[Y ]| \u2265 \u03b8 Var[Y ] \u2264 1 \u03b8 2 .(64)\nAnd based on the Assumption 2, there exists an upper bound \u03c3 2 > 0 for the variance of random noise e B (x) generated from the one-point mini-batch, i.e., B = {i}, i = 1, . . . , N . Consequently, for each\nt \u2265 0, we have E[Y t ] \u2264 \u03b1 K+t \u03c3 \u221a |B K+t | and Var[Y t ] \u2264 \u03b1 2 K+t \u03c3 2 |B K+t | , then combining with (64), we have Y \u2264 E[Y ] + \u03b8 Var[Y ](65)\u2264 \u221e t=0 \u03b1 K+t \u03c3 |B k+t | + \u03b8 \u00b7 \u221e t=0 \u03b1 2 K+t \u03c3 2 |B K+t | (66) \u2264 \u221e t=0 \u03b1 K+t \u03c3 |B k+t | + \u03b8 \u00b7 \u221e t=0 \u03b1 K+t \u03c3 |B K+t | = (1 + \u03b8) \u221e t=0 \u03b1 K+t \u03c3 |B K+t |(67)\nholds with probability at least 1 \u2212 1 \u03b8 2 . Here, for the second inequality, we use the property that the Given any \u03b8 > 1, there exists some \u03b1 k = O(1/(1 + \u03b8)t) and |B k | = O(t), the above series converges and satisfies that\nequality E[ \u221e t=0 Y i ] = \u221e t=0 E[Y i ] holds whenever \u221e t=0 E[|Y i |] convergences,(1 + \u03b8) \u221e t=0 \u03b1 K+t \u03c3 |B K+t | \u2264 3R 2 8(4R + 1)(68)\nholds. Notice that the above proof holds for any given sequence {y t } \u221e t=0 \u2208 X \u221e , thus\nmax {yt} \u221e t=0 \u2208X \u221e \u221e t=0 \u03b1 k e B k (y t ) 2 \u2264 3R 2 8(4R + 1)\nholds with probability at least 1 \u2212 1 \u03b8 2 .\n\nThe Lemma 6 draws if previous iterate of Half-Space\n\nStep falls into the neighbor of x * , then under appropriate step size and mini-batch setting, the current iterate also inhabits the neighbor with high probability.\n\nLemma 6. Under the assumptions of Lemma 5, suppose x K \u2212 x * \u2264 R/2; for any satisfying\nK \u2264 < K + t, 0 < \u03b1 \u2264 min{ 1 L , 2\u03b41\u2212R\u2212 (2\u03b42+R) M }, |B | \u2265 N \u2212 N 2M and x \u2212 x * \u2264 R holds, then x K+t \u2212 x * \u2264 R.(69)\nholds with probability at least 1 \u2212 1 \u03b8 2 .\n\nProof. It follows the assumptions of this lemma, Lemma 4,(15) and (16) that for any satisfying\nK \u2264 < K + t [x * ] g = 0, for any g \u2208\u011c .(70)\nHence we have that for K \u2264 < K + t,\nx +1 \u2212 x * 2 = g\u2208G [x \u2212 x * \u2212 \u03b1 \u2207\u03a8(x ) \u2212 \u03b1 e B (x )] g 2 + g\u2208\u011c k [x \u2212 x * \u2212 x ] g 2 = g\u2208G [x \u2212 x * ] g 2 \u2212 2\u03b1 [x \u2212 x * ] g [\u2207\u03a8(x ) + e B (x )] g + \u03b1 2 [\u2207\u03a8(x ) + e B (x )] g 2 + g\u2208\u011c [x * ] g 2 = g\u2208G [x \u2212 x * ] g 2 \u2212 2\u03b1 [x \u2212 x * ] g [\u2207\u03a8(x )] g \u2212 2\u03b1 [x \u2212 x * ] g [e B (x )] g + \u03b1 2 [\u2207\u03a8(x ) + e B (x )] g 2 \u2264 g\u2208G [x \u2212 x * ] g 2 \u2212 [\u2207\u03a8(x )] g 2 2 \u03b1 L \u2212 \u03b1 2 \u2212 2\u03b1 [x \u2212 x * ] g [e B (x )] g + \u03b1 2 [e B (x )] g 2 + 2\u03b1 2 [\u2207\u03a8(x )] g [e B (x )] g \u2264 g\u2208G [x \u2212 x * ] g 2 \u2212 [\u2207\u03a8(x )] g 2 2 \u03b1 L \u2212 \u03b1 2 + 2\u03b1 [x \u2212 x * ] g [e B (x )] g + \u03b1 2 [e B (x )] g 2 + 2\u03b1 2 [\u2207\u03a8(x )] g [e B (x )] g \u2264 g\u2208G [x \u2212 x * ] g 2 \u2212 [\u2207\u03a8(x )] g 2 2 \u03b1 L \u2212 \u03b1 2 + (2\u03b1 + 2\u03b1 2 L) [x k \u2212 x * ] g [e B (x )] g + \u03b1 2 [e B (x )] g 2 \u2264 g\u2208G [x \u2212 x * ] g 2 \u2212 [\u2207\u03a8(x )] g 2 2 \u03b1 L \u2212 \u03b1 2 + (2\u03b1 + 2\u03b1 2 L) x \u2212 x * e B (x ) + \u03b1 2 e B (x ) 2(71)\nOn the other hand, by the definition of e B (x) as (63), we have that\ne B (x) =[\u2207\u03a8 B (x) \u2212 \u2207\u03a8(x)] I =0 (x) = [\u2207f B (x) \u2212 \u2207f (x)] I =0 (x) = 1 |B| j\u2208B [\u2207f j (x)] I =0 (x) \u2212 1 N N i=1 [\u2207f i (x)] I =0 (x) = 1 N j\u2208B N |B| [\u2207f j (x)] I =0 (x) \u2212 [\u2207f j (x)] I =0 (x) \u2212 1 N N i=1 i / \u2208B [\u2207f i (x)] I =0 (x) = 1 N j\u2208B N \u2212 |B| |B| [\u2207f j (x)] I =0 (x) \u2212 1 N N i=1 i / \u2208B [\u2207f i (x)] I =0 (x)(72)\nThus taking the norm on both side of (72) and using triangle inequality results in the following:\ne B (x) \u2264 1 N j\u2208B N \u2212 |B| |B| [\u2207f j (x)] I =0 (x) + 1 N N i=1 i / \u2208B [\u2207f i (x)] I =0 (x) \u2264 1 N N \u2212 |B| |B| |B k |M + 1 N (N \u2212 |B|)M \u2264 2(N \u2212 |B|)M N .(73)\nSince \u03b1 \u2264 1, and |B | \u2265 N \u2212 N 2M hence \u03b1 e B (x ) \u2264 1. Then combining with \u03b1 \u2264 1/L, (71) can be further simplified as\nx +1 \u2212 x * 2 \u2264 g\u2208G [x \u2212 x * ] g 2 \u2212 [\u2207\u03a8(x )] g 2 2 \u03b1 L \u2212 \u03b1 2 + (2\u03b1 + 2\u03b1 2 L) x \u2212 x * e B (x ) + \u03b1 2 e B (x ) 2 \u2264 g\u2208G [x \u2212 x * ] g 2 \u2212 1 L 2 [\u2207\u03a8(x )] g 2 + 4\u03b1 x \u2212 x * e B (x ) + \u03b1 2 e B (x ) 2 \u2264 x \u2212 x * 2 + 4\u03b1 x \u2212 x * e B (x ) + \u03b1 e B (x )(74)\nFollowing from the assumption that x \u2212 x * \u2264 R, then (74) can be further simplified as\nx +1 \u2212 x * 2 \u2264 x \u2212 x * 2 + 4\u03b1 R e B (x ) + \u03b1 k e B (x ) \u2264 x \u2212 x * 2 + (4R + 1)\u03b1 e B (x )(75)\nSumming the the both side of (75) from = K to = K + t \u2212 1 results in\nx K+t \u2212 x * 2 \u2264 x K \u2212 x * 2 + (4R + 1) K+t\u22121 =K \u03b1 e B (x )(76)\nIt follows Lemma 5 that the followng holds with probability at least 1 \u2212 1\n\u03b8 2 , \u221e =K \u03b1 e B (x ) \u2264 3R 2 4(4R + 1) .(77)\nThus we have that\nx K+t \u2212 x * 2 \u2264 x K \u2212 x * 2 + (4R + 1) K+t\u22121 =K \u03b1 e B (x ) \u2264 x K \u2212 x * 2 + (4R + 1) \u221e =K \u03b1 e B (x ) \u2264 R 2 4 + (4R + 1) 3R 2 4(4R + 1) \u2264 R 2 4 + 3R 2 4 \u2264 R 2 ,(78)\nholds with probability at least 1 \u2212 1 \u03b8 2 , which completes the proof.\n\nBased on the above lemmas, the Lemma 7 shows if initial iterate of Half-Space Step locates closely enough to x * , step size \u03b1 k polynomially decreases, and mini-batch size B k polynomially increases, then x * inhabits all subsequent reduced space {S k } \u221e k=K constructed in Half-Space\n\nStep with high probability.\nLemma 7. If x K \u2212 x * \u2264 R 2 , K \u2265 N P , k = K + t, t \u2208 Z + , 0 < \u03b1 k = O(1/( \u221a N t)) \u2264 min{ 2(1\u2212 ) L , 1 L , 2\u03b41\u2212R\u2212 (2\u03b42+R) M } and |B k | = O(t) \u2265 N \u2212 N 2M\n. Then for any constant \u03c4 \u2208 (0, 1), x k \u2212 x * \u2264 R with probability at least 1 \u2212 \u03c4 for any k \u2265 K.\n\nProof. It follows Lemma 3 and the assumption of this lemma that x * \u2208 S K . Moreover, it follows the assumptions of Lemma (5,6,7), the definition of finite-sum f (x) in (12), and the bound of error as (73) that\nP({x k } \u221e k=K \u2208 {x : x \u2212 x * \u2264 R} \u221e ) \u2265 1 \u2212 1 \u03b8 2 O(N \u2212K) \u2265 1 \u2212 \u03c4,(79)\nwhere the last two inequalities comes from that the error vanishing to zero as |B k | reaches the upper bound N , and \u03b8 is sufficiently large depending on \u03c4 and O(N \u2212 K).\n\nCorollary 2. Lemma 7 further implies x * inhabits all subsequent S k , i.e., x * \u2208 S k for any k \u2265 K.\n\n\nB.5 The Initialization Stage\n\nIn previous parts, we show that the Half-Space\n\nStep guarantees to converge to the optimal solution, and ensures to recover the no-zero groups of the optimal solution under some assumptions with a \"close-enough\" initialization point x N P . To complete the story, in this part, we show that the iterate obtained from the Subgradient Descent Update in Algorithm 2 satisfies the \"close-enough\" condition with high probability. Remark here that the proximal methods, such as Prox-SG, Prox-SVRG and SAGA, may also serve in the initialization stage. However, for the general regularization r(x), they may not have closed-form solution for the corresponding inherent subproblems, implying nonexplicit update mechanism to the next iterate. Hence, people may have to inconveniently approximate the solutions of proximal operator by other techniques, whereas the sub-gradient method does not have these drawbacks. Therefore, for the generality of HSPG, we select the sub-gradient method in the Initialization Stage by default.\n\n\nB.5.1 Convergence Analysis of Initialization Stage\n\nIn this part, we show that the \"close enough\" condition\nx k \u2212 x * \u2264 R 2(80)\nproposed in Theorem 1 can be achieved via the Initialization Stage (Subgradient Descent Update) in Algorithm 2 under the Assumption 5.\n\nAssumption 5. Assume the following assumptions hold.\n\n\u2022 (A5-1). f : R n \u2192 R is differentiable and \u00b5-strongly convex. r : R n \u2192 R is convex.\n\n\u2022 (A5-2). There exists an universal constant M such that the stochastic gradient \u2207f B (x) satisfies \u2207f B (x) 2 \u2264 M for all x \u2208 R d and mini-batch B.\n\u2022 (A5-3). The stochastic gradient \u2207f B (x) satisfies E B [\u2207f B (x)|x] = \u2207f (x) for all x \u2208 R n .\nProposition 2. Under Assumption 5, for any R > 0, any \u03c4 \u2208 (0, 1), set\nN = log \u03c4 R 4 x 0 \u2212 x * 2 2 log 1 \u2212 \u03c4 R 4M ,(81)\u03b1 0 = \u03b1 1 = . . . = \u03b1 N P \u22121 = \u03c4 \u00b5R 4M 2 ,(82)\nwhere R based on the setting of Theorem 1. We have the Algorithm 1 (Subgradient Descent Update) returns a solution x N P that satisfies x N P \u2212 x * 2 \u2264 R/2 with probability 1 \u2212 \u03c4 .\n\nProof. Let x * be the global optimal solution of (2). Let \u2207\u03c8(x) = \u2207f (x) + \u03bb\u03b6(x) and \u2207\u03c8 B (x) = \u2207f B (x) + \u03bb\u03b6(x) given any point x \u2208 R n and mini-batch B. Consider\nx k+1 \u2212 x * 2 2 = x k \u2212 \u03b1 k \u2207\u03c8 B k (x k ) \u2212 x * 2 2 (83) = x k \u2212 x * 2 2 \u2212 2\u03b1 k \u2207\u03c8 B k (x k ), x k \u2212 x * + \u03b1 k \u2207\u03c8 B k (x k ) 2 2 .(84)\nDue to (A1) in Assumption 5, the \u00b5-strongly convexity of f and the convexity of r yields\n\u03c8(x * ) \u2265 \u03c8(x k ) + \u2207\u03c8(x k ), x * \u2212 x k + \u00b5 2 x k \u2212 x * 2 2 .(85)\nOur goal is to empirically show that HSPG is able to identify the ground truth zero groups with synthetic data. We conduct the experiments as follows: (i) generate the data matrix A whose elements are uniformly distributed among [\u22121, 1]; (ii) generate a vector x * working as the ground truth solution, where the elements are uniformly distributed among [\u22121, 1] and the coordinates are equally divided into 10 groups (|G| = 10); (iii) randomly set a number of groups of x * to be 0 according to a pre-specified group sparsity ratio; (iv) compute the target variable y = Ax * ; (v) solve the above problem (101) for x with A and y only, and then evaluate the Intersection over Union (IoU) with respect to the identities of the zero groups between the computed solution estimatex by HSPG and the ground truth x * .\n\nWe test HSPG on (101) under different problem settings. For a slim matrix A where N \u2265 n, we test with various group sparsity ratios among {0.1, 0.3, 0.5, 0.7, 0.9}, and for a fat matrix A where N < n, we only test with a certain group sparsity value since a recovery of x * requires that the number of non-zero elements in x * is bounded by N . Throughout the experiments, we set \u03bb to be 100/N , the mini-batch size |B| to be 64, step size \u03b1 k to be 0.1 (constant), and fine-tune per problem. Based on a similar statistical test on objective function stationarity [83], we switch to Half-Space Step roughly after 30 epoches. Table 8 shows that under each setting, the proposed HSPG correctly identifies the groups of zeros as indicated by IoU(x, x * ) = 1.0, which is a strong evidence to show the correctness of group sparsity identification of HSPG. Logistic Regression We then focus on the benchmark convex logistic regression problem with the mixed 1 / 2 -regularization given N examples (d 1 , l 1 ), \u00b7 \u00b7 \u00b7 , (d N , l N ) where d i \u2208 R n and l i \u2208 {\u22121, 1} with the form\nminimize (x;b)\u2208R n+1 1 N N i=1 log(1 + e \u2212l i (x T d i +b) ) + \u03bb g\u2208G [x]g ,(102)\nfor binary classification with a bias b \u2208 R. We set the regularization parameter \u03bb as 100/N throughout the experiments since it yields high sparse solutions and low object value f 's, equally decompose the variables into 10 groups to form G, and test problem (102) on 8 standard publicly available large-scale datasets from LIBSVM repository [?] as summarized in Table 9. All convex experiments are conducted on a 64-bit operating system with an Intel(R) Core(TM) i7-7700K CPU @ 4.20 GHz and 32 GB random-access memory.\n\nWe run the solvers with a maximum number of epochs as 60 following [7]. The mini-batch size |B| is set to be min{256, 0.01N } similarly to [?]. The step size \u03b1 k setting follows [Section 4] [75]. Particularly, we first compute a Lipschitz constant L as max i d i 2 /4, then fine tune and select constant \u03b1 k \u2261 \u03b1 = 1/L to Prox-SG and Prox-SVRG since it exhibits the best results. For RDA, the step size parameter \u03b3 is fined tuned as the one with the best performance among all powers of 10. For HSPG, we set \u03b1 k as the same as Prox-SG and Prox-SVRG in practice. We select two 's as 0 and 0.8. The final objective value \u03c8 and group sparsity in the solutions are reported in Table 10-11, where we mark the best values as bold to facilitate the comparison. Furthermore, Figure 7 plots the relative runtime of these solvers for each dataset, scaled by the runtime of the most time-consuming solver. Table 11 shows that our HSPG is definitely the best solver on exploring the group sparsity of the solutions. In fact, HSPG under = 0.8 performs all the best except ijcnn1. Prox-SVRG is the second best solver on group sparsity exploration, which demonstrates that the variance reduction techniques works well in convex setting to promote sparsity, but not in non-convex settings. HSPG under = 0 performs much better than Prox-SG which matches the better sparsity recovery property of HSPG even under as 0. Moreover, as shown in Table 10, we observe that all solvers perform quite competitively in terms of final objective values (round up to 3 decimals) except RDA, which demonstrates that HSPG reaches comparable convergence as Prox-SG and Prox-SVRG in practice. Finally, Figure 7 indicates that Prox-SG, RDA and HSPG have similar computational cost to proceed, except Prox-SVRG due to its periodical full gradient computation.   \n\n\nC.2 Nonconvex Experiments\n\nTo illustrate, among the state-of-the-art proximal stochastic optimizers, we exclude RDA because of no acceptable results attained during our following tests with the step size parameter \u03b3 setting throughout all powers of 10 from 10 \u22123 to 10 3 , and skip Prox-Spider and SAGA since Prox-SVRG has been a superb representative to the proximal incremental gradient methods. We consider the popular image classification tasks, with popular architectures, i.e., VGG16 and ResNet18 on benchmark datasets CIFAR10 and Fashion-MNIST [?], where the group partition G is defined as 3D kernel following [12,51], which are not ZIGs.  Table 12 demonstrates the effectiveness and superiority of HSPG, where we mark the best values as bold, and the group sparsity ratio is defined as the percentage of zero groups. In particular, (i) HSPG computes remarkably higher group sparsity than other methods on all tests, of which the solutions are typically multiple times sparser in the manner of group than those of Prox-SG, while Prox-SVRG performs not comparably since the variance reduction techniques may not work as desired for deep learning applications; (ii) HSPG performs competitively with respect to the final objective values \u03c8. In addition, all the methods reach a comparable generalization performance on unseen test data. On the other hand, sparse regularization methods may yield solutions with entries that are not exactly zero but are very small. Sometimes all entries below certain threshold (T ) are set to zero [18]. However, such simple truncation mechanism is heuristic-rule based, hence may hurt convergence and accuracy. To illustrate this, we set the groups of the solutions of Prox-SG and Prox-SVRG to zero if the magnitudes of the group variables are less than some T , and denote the corresponding solutions as Prox-SG* and Prox-SVRG*.  As shown in Figure 8a(i), under the T with no accuracy regression, Prox-SG* and Prox-SVRG* reach higher group sparsity ratio as 60% and 32% compared to Table 12, but still significantly lower than the 70% of HSPG without simple truncation. Under the T to reach the same group sparsity ratio as HSPG, the testing accuracy of Prox-SG* and Prox-SVRG* regresses drastically to 28% and 17% in Figure 8a(ii) respectively. Remark here that although further refitting the models from Prox-SG* and Prox-SVRG* on active (non-zero) groups of weights may recover the accuracy regression, it requires additional engineering efforts and training cost, which is less attractive and convenient than HSPG (with no need to refit). Similarly, as shown in Figure 8b, under the ZIG partition and the T without accuracy regression, the FLOPs and number of parameters reductions achieved by SGD* (subgradient descent with simple truncation) and Prox-SG* are not comparable with those achieve by HSPG, i.e., HSPG achieves about 1.5\u00d7 fewer FLOPs and number of parameters.\n\n\nInput: Full model M (no need to be pretrained). 2: Construct G: Partition the trainable parameters of M into a ZIG set G. 3: Train: Train the model M using HSPG (Algorithm. 2) to obtain a group-sparse solution x * HSPG . 4: Prune: Construct a slimmer model architecture M * by directly pruning zero groups of x * HSPG . 5: Output: Compressed model M * .\n\n\n(a) Conv-BN. m denotes the number of channels in O l . (b) Residual block. m denotes the number of output channels of the residual block.\n\n\n(c) Fully connected layer (Left). Multi-head attention layer (Right). m denotes the length of output vector.\n\nFigure 2 :\n2Zero-invariant group partition for three popular structures.\n\nFigure 3 :\n3Illustration of Half-SpaceStep with projection in(6), where G = {{1, 2}}.\n\nFigure 4 :\n4Clustering results of ImageNet validation images using deep features extracted by full ResNet50 (left of a and b) and pruned ResetNet50 by OTO (right of a and b). The points are visualized by projecting deep features onto a two-dimensional space via PCA.\n\nFigure 5 :\n5ResNet50 Architecture. gray and green blocks in Figure 5b, and (ii) the ResConv-BN of which the output shares the same dimension with another ResConv-BN, marked as yellow and brown in Figure 5b. For ResNet50, we partition regular Conv-BN following Section 3.1. For ResConv-BN, within each Group Block, the intermediate input/output tensors in Conv/Indentity Blocks share the same dimension, and hence all the ResConv-BNs in one Group block share the same number of 3D filters. Consequently, their flattened filter matrices has the same number of rows. Figure 5b breaks down the architecture of a Group Block. The output tensors of ResConv-BN1 and ResConv-BN2 in Conv Block, denoted as O 1 and O 2 , are computed by\n\n\nsee Section 2.1 in [?]; and for the third inequality, we use \u03b1 K+t \u03c3 \u221a |B K+t | \u2264 1 without loss of generality as the common setting of large mini-batch size and small step size.\n\nFigure 7 :\n7Relative runtime.\n\nFigure 8 :\n8HSPG versus simple truncation. (a) On ResNet18 with CIFAR10 over non-ZIGs. (b) On VGG16with CIFAR10 over ZIGs.\n\nTable 1 :\n1VGG16 and VGG16-BN for CIFAR10. Convolutional layers are in bold.Method \nBN \nArchitecture \nFLOPs # of Params Top-1 Acc. \nBaseline \n\n64-64-128-128-256-256-256-512-512-512-512-512-512-512-512 100% \n100% \n91.6% \nSBP [56] \n\n47-50-91-115-227-160-50-72-51-12-34-39-20-20-272 \n31.1% \n5.9% \n91.0% \nBC [52] \n\n\n\nTable 1 ,\n1the pruned architecture of OTO indicates that OTO identifies similar redundancy of the intermediate and late convolutional layers compared to other methods, but significantly more of the early convolutional layers. As a result, OTO achieves 83.7% (1 \u2212 16.3%) FLOPs reduction and 97.5% (1 \u2212 2.5%) parameter reduction with the best Top-1 accuracy, which outperforms other state-of-the-arts significantly. For VGG16-BN, among all, OTO reduces FLOPs and parameters to the lowest 26.8% and 5.5%, respectively. EC[48] and Hinge [49] achieve the same level of Top-1 accuracy as OTO, but are substantially outperformed when it comes to FLOPs and parameter reduction. We further present the FLOPs reductions per layer of OTO in\n\nTable 2 :\n2ResNet50 for CIFAR10.Method \nFLOPs # of Params Top-1 Acc. \nBaseline \n100% \n100% \n93.5% \nAMC [34] \n-\n60.0% \n93.6% \nANNC [77] \n-\n50.0% \n95.0% \nPruneTrain [54] 30.0% \n-\n93.1% \nN2NSkip [66] \n-\n10.0% \n94.4% \nOTO \n12.8% \n8.8% \n94.4% \n\n\n\nTable 3 :\n3OTO Under Different Switchings (N = T, 2T, 3T ) \nfor VGG16, VGG16-BN and ResNet50 on CIFAR10 \n\nBackend \nFLOPs \n# of Params \nTop-1 Acc. \nVGG16 \n17.0% \u00b1 1.4% 2.6% \u00b1 0.4% 90.9% \u00b1 0.3% \nVGG16-BN 25.4% \u00b1 1.1% 5.0% \u00b1 0.5% 93.3% \u00b1 0.2% \nResNet50 \n12.9% \u00b1 1.5% 8.5% \u00b1 1.0% 94.2% \u00b1 0.2% \n\n\n\nTable 4 :\n4ResNet50 for ImageNet.Method \nFLOPs # of Params Top-1 Acc. Top-5 Acc. \nBaseline \n100% \n100% \n76.1% \n92.9% \nDDS-26 [39] \n57.0% \n61.2% \n71.8% \n91.9% \nCP [35] \n66.7% \n-\n72.3% \n90.8% \nThiNet-50 [40] \n44.2% \n48.3% \n71.0% \n90.0% \nRBP [86] \n43.5% \n48.0% \n71.1% \n90.0% \nRRBP [86] \n45.4% \n-\n73.0% \n91.0% \nSFP [33] \n41.8% \n-\n74.6% \n92.1% \nHinge [49] \n46.6% \n-\n74.7% \n-\nGBN-50 [80] \n44.9% \n46.6% \n75.2% \n92.4% \nGBN-60 [80] \n59.5% \n68.2% \n76.2% \n92.8% \nGroup-HS (2e-5) [78] 32.4% \n-\n75.2% \n92.5% \nGroup-HS (1e-5) [78] 52.9% \n-\n76.4% \n93.1% \nResRep [16] \n45.5% \n-\n76.2% \n92.9% \nSCP [42] \n45.7% \n-\n74.2% \n92.0% \nOTO \n34.5% \n35.5% \n74.7% \n92.1% \nOTO  *  \n34.5% \n35.5% \n75.1% \n92.5% \n\nResNet50 for ImageNet. We \nnow evaluate OTO on ResNet50 \nfor ImageNet. As shown in Ta-\nble 4, OTO prunes 64.5%(1 \u2212 \n35.5%) parameters to achieve \n65.5%(1\u221234.5%) FLOPs reduc-\ntion with only 1.4%/0.8% Top-\n1/5 accuracy regression com-\npared to the baseline. OTO con-\nsistently outperforms the major-\nity of counterparts especially on \nthe FLOPs reduction and the \nparameter reduction. We note \nthat Hinge [49] \n\nTable 5 :\n5Pruning Bert on SQuAD Based on the statement in the official git repository of [63].Method \n# of Params Exact F1-score \nSpeedUp \nBaseline \n100% \n81.0% \n88.3% \n1\u00d7 \nMaP [63] \n10.0% \n67.7% \n78.5% \n1\u00d7* \nMvP [63] \n10.0% \n71.9% \n81.7% \n1\u00d7* \nProxSSI [12] \n83.4%  \u2020 \n72.3% \n82.0% \n1\u00d7 \nOTO \n91.0% \n75.0% \n84.1% \n1.1\u00d7 \nOTO \n76.2% \n72.3% \n82.1% \n1.2\u00d7 \nOTO \n66.7% \n71.9% \n82.0% \n1.3\u00d7 \nOTO \n53.3% \n71.4% \n81.5% \n1.5\u00d7 \nOTO \n40.0% \n70.9% \n81.1% \n1.8\u00d7 \n* \n\n\ncontains two fundamentals: (i) partitions the trainable parameters of DNNs into zero-invariant groups (ZIGs), thereby pruning zero groups does not affect the model output, and (ii) trains by a novel optimizer, Half-Space Stochastic Projected Gradient (HSPG), which outperforms proximal methods on group sparsity exploration and maintains comparable convergence. We numerically demonstrate OTO on benchmark experiments, i.e., VGG16 for CIFAR10, ResNet50 for CIFAR10/ImageNet and Bert for SQuAD, and achieve state-of-theart pruning results. We leave automatically generating ZIGs for arbitrary DNNs, incorporating quantization and applying OTO to other tasks to future work.[49] Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, and Radu Timofte. Group sparsity:The hinge between filter pruning and decomposition for network compression.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8018-8027, 2020. [50] Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David Doermann, Yongjian Wu, Feiyue Huang, and Rongrong Ji. Exploiting kernel sparsity and entropy for interpretable cnn compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, and Xing Sun. Pruning filter in filter. arXiv preprint arXiv:2009.14410, 2020. [56] Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. In Advances in Neural Information Processing Systems, pages 6775-6784, 2017. [57] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE International Conference on Computer Vision, pages 261-270, 2017. [58] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006. [59] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. [60] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779-788, 2016. [61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015. [62] Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural network pruning. arXiv preprint arXiv:2003.02389, 2020. [63] Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by fine-tuning. arXiv preprint arXiv:2005.07683, 2020. [64] Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana. Hydra: Pruning adversarially robust neural networks. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [66] Arvind Subramaniam and Avinash Sharma. N2nskip: Learning highly sparse networks using neuron-to-neuron skip connections. In BMVC, 2020. [67] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Frederick Tung and Greg Mori. Clip-q: Deep network compression learning by in-parallel pruning-quantization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [69] Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. arXiv preprint arXiv:2005.07093, 2020.Recognition, pages 2800-2809, 2019. \n[51] Shaohui Lin, Rongrong Ji, Yuchao Li, Cheng Deng, and Xuelong Li. Toward compact convnets \nvia structure-sparsity regularized filter pruning. IEEE transactions on neural networks and \nlearning systems, 31(2):574-588, 2019. \n[52] Advances in neural information processing systems, pages 3288-3298, 2017. \n[53] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep \nneural network compression. In Proceedings of the IEEE international conference on computer \nvision, pages 5058-5066, 2017. \n[54] Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and Mattan \nErez. Prunetrain: fast neural network training by dynamic sparse model reconfiguration. In \nProceedings of the International Conference for High Performance Computing, Networking, \nStorage and Analysis, pages 1-13, 2019. \n[55] Advances in Neural Information Processing Systems (NeurIPS), 7, \n2020. \n[65] Scop: Scientific control for reliable neural network pruning. arXiv preprint arXiv:2010.10732, \n2020. \n[68] A Implementation Details of OTO \n\nA.1 ZIG for ResNet50 \n\nWithout loss of generality, we illustrate ZIGs for the general ResNet class with ResNet50. As shown \nin Figure 5a, ResNet50 begins with a Conv-BN, a pooling layer, and extracts features by M Group \nBlocks that each contains one Conv Block and N Identity Block. The extracted features are ultimately \nfed into an average pooling layer for different downstream computations. There exist two types \nof convolution structures inside ResNet50: (i) the regular Conv-BN (see Section 3.1), marked as \n= { 1 , 2 , \u2026 , +1 } \n\n+1 \n\n1 \n2 \n\n+1 \n\n\u2110 \n\nGroup Block \n\nInput \n\nTensor \n\nConv \nBN \nResConv1 \nBN1 \n\nResConv2 \nBN2 \n\nOutput \n\nTensor \n\nConv Block \n\n\u00d7 2 \n\nInput \n\nTensor \n\nConv \nBN \nResConv3 \nBN3 \nOutput \n\nTensor \n\nIdentity Block \n\n\u00d7 2 \n\n\u00d7 \n\nConv \n\nBatch Norm \n\nMax Pooling \n\nConv Block \n\nIdentity Block \n\nAverage Pooling \n\nGroup \nBlock \n\n\u00d7 \n\u00d7 \n\n(a) ResNet50. \n\n= { 1 , 2 , \u2026 , +1 } \n\n+1 \n\n1 \n2 \n\n+1 \n\n\u2110 \n\nGroup Block \n\nInput \n\nTensor \n\nConv \nBN \nResConv1 \nBN1 \n\nResConv2 \nBN2 \n\nOutput \n\nTensor \n\nConv Block \n\n\u00d7 2 \n\nInput \n\nTensor \n\nConv \nBN \nResConv3 \nBN3 \nOutput \n\nTensor \n\nIdentity Block \n\n\u00d7 2 \n\n\u00d7 \n\nConv 7x7 \n\nBatch Norm \n\nMax Pooling \n\nConv Block \n\nIdentity Block \n\nAverage Pooling \n\nGroup \nBlock \n\n\u00d7 \n\u00d7 \n\n(b) Group block. \n\n\n\nTable 6 :\n6OTO for CNN Experiments (mean \u00b1 std)Model \nDataset \nFLOPs \n# of Params \nTop-1 Acc. \nVGG16 \nCIFAR10 16.9% \u00b1 1.5% 2.7% \u00b1 0.2% 90.7% \u00b1 0.3% \nVGG16-BN CIFAR10 26.9% \u00b1 0.1% 5.5% \u00b1 0.1% 93.2% \u00b1 0.2% \nResNet50 \nCIFAR10 11.9% \u00b1 1.7% 8.8% \u00b1 0.4% 93.9% \u00b1 0.5% \nResNet50 \nImageNet 34.8% \u00b1 1.8% 35.9% \u00b1 1.7% 73.3% \u00b1 1.1% \n\n\n\nTable 7 :\n7FLOPs Reduction Breakdown for the ConvLayers of VGG16 on CIFAR10ConvLayer Index \n# of Output Channels \nFLOPs Reduction \nOriginal \nPruned \nQuantity (Million) Percentage (%) \n1 \n64 \n21 \n1.19M \n0.45% \n2 \n64 \n45 \n29.04M \n11.07% \n3 \n128 \n82 \n10.47M \n3.99% \n4 \n128 \n110 \n17.22M \n6.57% \n5 \n256 \n109 \n11.97M \n4.56% \n6 \n256 \n68 \n33.48M \n12.77% \n7 \n256 \n37 \n36.30M \n13.84% \n8 \n512 \n13 \n18.81M \n7.17% \n9 \n512 \n9 \n37.73M \n14.38% \n10 \n512 \n7 \n37.74M \n14.39% \n11 \n512 \n3 \n9.44M \n3.60% \n12 \n512 \n5 \n9.44M \n3.60% \n13 \n512 \n8 \n9.44M \n3.60% \n\n\n\nTable 8 :\n8Linear regression problem settings and IoU of the recovered solutions by HSPG.N \nn \nGroup sparsity ratio of x  *  \nIoU(x, x  *  ) \n\nSlim A \n\n10000 \n1000 \n{0.1, 0.3, 0.5, 0.7, 0.9} \n1.0 \n10000 \n2000 \n{0.1, 0.3, 0.5, 0.7, 0.9} \n1.0 \n10000 \n3000 \n{0.1, 0.3, 0.5, 0.7, 0.9} \n1.0 \n10000 \n4000 \n{0.1, 0.3, 0.5, 0.7, 0.9} \n1.0 \n\nFat A \n\n200 \n1000 \n0.9 \n1.0 \n300 \n1000 \n0.8 \n1.0 \n400 \n1000 \n0.7 \n1.0 \n500 \n1000 \n0.6 \n1.0 \n\n\n\nTable 9 :\n9Summary of datasets.Dataset \nN \nn \nAttribute \nDataset \nN \nn \nAttribute \na9a \n32561 \n123 \nbinary {0, 1} \nnews20 \n19996 \n1355191 \nunit-length \nhiggs \n11000000 \n28 \nreal [\u22123, 41] \nreal-sim \n72309 \n20958 \nreal [0, 1] \nijcnn1 \n49990 \n22 \nreal [-1, 1] \nurl_combined \n2396130 \n3231961 \nreal [\u22124, 9] \nkdda \n8407752 \n20216830 \nreal [\u22121, 4] \nw8a \n49749 \n300 \nbinary {0, 1} \n\n\n\nTable 10 :\n10Final objective values \u03c8 for tested algorithms on convex problems.Dataset \nProx-SG \nRDA \nProx-SVRG \nHSPG \nas 0 \nas 0.8 \na9a \n0.355 \n0.359 \n0.355 \n0.355 \n0.355 \nhiggs \n0.357 \n0.360 \n0.365 \n0.358 \n0.358 \nijcnn1 \n0.248 \n0.278 \n0.248 \n0.248 \n0.248 \nkdda \n0.103 \n0.124 \n0.103 \n0.103 \n0.103 \nnews20 \n0.538 \n0.693 \n0.538 \n0.538 \n0.538 \nreal-sim \n0.242 \n0.666 \n0.244 \n0.242 \n0.242 \nurl_combined \n0.397 \n0.579 \n0.391 \n0.405 \n0.405 \nw8a \n0.110 \n0.111 \n0.112 \n0.110 \n0.110 \n\n\n\nTable 11 :\n11Group sparsity for tested algorithms on convex problems.Dataset \nProx-SG \nRDA \nProx-SVRG \nHSPG \nas 0 \nas 0.8 \na9a \n20% \n30% \n30% \n30% \n30% \nhiggs \n0% \n10% \n0% \n0% \n30% \nijcnn1 \n50% \n70% \n60% \n60% \n60% \nkdda \n0% \n0% \n0% \n0% \n80% \nnews20 \n20% \n80% \n90% \n80% \n90% \nreal-sim \n0% \n0% \n80% \n0% \n80% \nurl_combined \n0% \n0% \n0% \n0% \n90% \nw8a \n0% \n0% \n0% \n0% \n0% \n\n\n\nTable 12 :\n12Final \u03c8/group sparsity ratio/testing accuracy on non-convex problems over non-ZIGs.59  / 52.58% / 90.50% 0.85 / 14.13% / 89.16% 0.58 / 76.47% / 91.93% Fashion-MNIST 0.52 / 12.31% / 92.83% 2.66 / 0.38% / 92.72% 0.52 / 47.82% / 92.87% ResNet18 CIFAR10 0.31 / 20.27% / 94.36% 0.37 / 4.60% / 94.11% 0.31 / 69.98% / 94.40% Fashion-MNIST 0.14 / 0.00% / 94.94% 0.18 / 0.00% / 94.70% 0.13 / 77.08% / 94.61% MobileNetV1 CIFAR10 0.40 / 58.05% / 91.54% 0.65 / 29.20% / 89.68% 0.40 / 71.36% / 92.04% Fashion-MNIST 0.22 / 62.62% / 94.22% 0.40 / 41.99% / 94.19% 0.26 / 84.26% / 94.52%Backbone \nDataset \nProx-SG \nProx-SVRG \nHSPG \n\nVGG16 \nCIFAR10 \n0.\nNote that when r(x) = x 1 where each g \u2208 G is singleton, then S k becomes an orthant face[7].\nEach selected class belongs to a disjoint upper category.\nKnowledge distillation[37] and LayerDropout[19] compresses Bert by pruning layers in their entirety.5 Run by OnnxRuntime[14] \nwhere H = {B 0 , . . . , B k\u22121 } denotes the whole history until step k.Non-asymptotic bounds. Combine above together, given any R/2 > 0, for any \u03c4 \u2208 (0, 1), setby Markov inequality, we haveholds with probability 1 \u2212 \u03c4 .C Extensive Numerical ExperimentsIn this Appendix, we include extensive numerical experiments in the view of optimization to demonstrate the superiority of HSPG to other classical proximal methods on the sparsity exploration and the competitiveness on objective convergence in both convex and nonconvex settings. Particularly, in Appendix C.1, we provide convex experiments to (i) demonstrate the validness of group sparsity identification of HSPG; (ii) present comprehensive comparison to Prox-SG, RDA and Prox-SVRG on benchmark convex problems. In Appendix C.2, we show additional nonconvex experiments to reveal the superiority of HSPG to competitors on group sparsity exploration.C.1 Convex ExperimentsLinear Regression on Synthetic Data We numerically validate the proposed HSPG on group sparsity identification by linear regression problems with 1 / 2 regularizations using synthetic data. Consider a data matrix A \u2208 R N \u00d7n consisting of N instances and the target variable y \u2208 R N , we are interested in the following problem:\nStructured sparsity through convex optimization. Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, Statistical Science. 274Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Structured sparsity through convex optimization. Statistical Science, 27(4):450-468, 2012.\n\nDepth-aware video frame interpolation. Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3703-3712, 2019.\n\n. Mike Bostock, Imagenet hierarchyMike Bostock. Imagenet hierarchy. https://observablehq.com/@mbostock/ imagenet-hierarchy.\n\nDistributed optimization and statistical learning via the alternating direction method of multipliers. Stephen Boyd, Neal Parikh, Eric Chu, Now Publishers IncStephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011.\n\nModel compression. Cristian Bucilu\u01ce, Rich Caruana, Alexandru Niculescu-Mizil, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. the 12th ACM SIGKDD international conference on Knowledge discovery and data miningCristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541, 2006.\n\nStorage efficient and dynamic flexible runtime channel pruning via deep reinforcement learning. Jianda Chen, Shangyu Chen, Sinno Jialin Pan, Jianda Chen, Shangyu Chen, and Sinno Jialin Pan. Storage efficient and dynamic flexible runtime channel pruning via deep reinforcement learning. 2019.\n\nOrthant based proximal stochastic gradient method for ell_1-regularized optimization. Tianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Sheng Yi, Xiao Tu, Zhihui Zhu, arXiv:2004.03639arXiv preprintTianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Sheng Yi, Xiao Tu, and Zhihui Zhu. Orthant based proximal stochastic gradient method for ell_1-regularized optimization. arXiv preprint arXiv:2004.03639, 2020.\n\nSpatially sparse convolutional neural networks for inking applications. Tianyi Chen, Yixin Shi, Sheng Yi, App. 16/355702US PatentTianyi Chen, Yixin Shi, and Sheng Yi. Spatially sparse convolutional neural networks for inking applications, Sept. 17 2020. US Patent App. 16/355,702.\n\nXuhao Chen, Escoin, arXiv:1802.10280Efficient sparse convolutional neural network inference on gpus. arXiv preprintXuhao Chen. Escoin: Efficient sparse convolutional neural network inference on gpus. arXiv preprint arXiv:1802.10280, 2018.\n\nA survey of model compression and acceleration for deep neural networks. Yu Cheng, Duo Wang, Pan Zhou, Tao Zhang, arXiv:1710.09282arXiv preprintYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.\n\nSaga: A fast incremental gradient method with support for non-strongly convex composite objectives. Aaron Defazio, Francis Bach, Simon Lacoste-Julien, Advances in neural information processing systems. Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646-1654, 2014.\n\nStructured sparsity inducing adaptive optimizers for deep learning. Tristan Deleu, Yoshua Bengio, arXiv:2102.03869arXiv preprintTristan Deleu and Yoshua Bengio. Structured sparsity inducing adaptive optimizers for deep learning. arXiv preprint arXiv:2102.03869, 2021.\n\nImagenet: A largescale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\n. ONNX Runtime developers. Onnx runtime. 2021ONNX Runtime developers. Onnx runtime. https://onnxruntime.ai/, 2021.\n\nCdfi: Compression-driven network design for frame interpolation. Tianyu Ding, Luming Liang, Zhihui Zhu, Ilya Zharkov, arXiv:2103.10559arXiv preprintTianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. Cdfi: Compression-driven network design for frame interpolation. arXiv preprint arXiv:2103.10559, 2021.\n\nLossless cnn channel pruning via decoupling remembering and forgetting. Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, Guiguang Ding, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, and Guiguang Ding. Lossless cnn channel pruning via decoupling remembering and forgetting. Proceedings of the IEEE International Conference on Computer Vision, 2021.\n\nEfficient online and batch learning using forward backward splitting. John Duchi, Yoram Singer, Journal of Machine Learning Research. 10John Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10(Dec):2899-2934, 2009.\n\nCombinatorial penalties: Which structures are preserved by convex relaxations. Marwa El Halabi, Francis Bach, Volkan Cevher, International Conference on Artificial Intelligence and Statistics. PMLRMarwa El Halabi, Francis Bach, and Volkan Cevher. Combinatorial penalties: Which structures are preserved by convex relaxations? In International Conference on Artificial Intelligence and Statistics, pages 1551-1560. PMLR, 2018.\n\nAngela Fan, Edouard Grave, Armand Joulin, arXiv:1909.11556Reducing transformer depth on demand with structured dropout. arXiv preprintAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.\n\nJonathan Frankle, Michael Carbin, arXiv:1803.03635The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprintJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.\n\nJonathan Frankle, Karolina Gintare, Dziugaite, M Daniel, Michael Roy, Carbin, arXiv:1903.01611Stabilizing the lottery ticket hypothesis. arXiv preprintJonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019.\n\nVisual feature extraction by a multilayered network of analog threshold elements. Kunihiko Fukushima, IEEE Transactions on Systems Science and Cybernetics. 54Kunihiko Fukushima. Visual feature extraction by a multilayered network of analog threshold elements. IEEE Transactions on Systems Science and Cybernetics, 5(4):322-333, 1969.\n\nTrevor Gale, Erich Elsen, Sara Hooker, arXiv:1902.09574The state of sparsity in deep neural networks. arXiv preprintTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.\n\nHighly efficient salient object detection with 100k parameters. Shang-Hua, Yong-Qiang Gao, Ming-Ming Tan, Chengze Cheng, Yunpeng Lu, Shuicheng Chen, Yan, European Conference on Computer Vision. SpringerShang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In European Conference on Computer Vision, pages 702-721. Springer, 2020.\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville, Yoshua Bengio, Deep learning. MIT press Cambridge1Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.\n\nCompressing bert: Studying the effects of weight pruning on transfer learning. Kevin Mitchell A Gordon, Nicholas Duh, Andrews, arXiv:2002.08307arXiv preprintMitchell A Gordon, Kevin Duh, and Nicholas Andrews. Compressing bert: Studying the effects of weight pruning on transfer learning. arXiv preprint arXiv:2002.08307, 2020.\n\nReweighted proximal pruning for large-scale language representation. Sijia Fu-Ming Guo, Liu, S Finlay, Xue Mungall, Yanzhi Lin, Wang, arXiv:1909.12486arXiv preprintFu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi Wang. Reweighted proximal pruning for large-scale language representation. arXiv preprint arXiv:1909.12486, 2019.\n\nEie: Efficient inference engine on compressed deep neural network. Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, A Mark, William J Horowitz, Dally, ACM SIGARCH Computer Architecture News. 443Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243-254, 2016.\n\nSong Han, Huizi Mao, William J Dally, arXiv:1510.00149Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprintSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net- works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.\n\nSong Han, Jeff Pool, John Tran, William J Dally, arXiv:1506.02626Learning both weights and connections for efficient neural networks. arXiv preprintSong Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015.\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.\n\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang, arXiv:1808.06866Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprintYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018.\n\nAmc: Automl for model compression and acceleration on mobile devices. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pages 784-800, 2018.\n\nChannel pruning for accelerating very deep neural networks. Yihui He, Xiangyu Zhang, Jian Sun, The IEEE International Conference on Computer Vision (ICCV). Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nGaussian error linear units (gelus). Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nHengyuan Hu, Rui Peng, Yu-Wing Tai, Chi-Keung Tang, arXiv:1607.03250Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprintHengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250, 2016.\n\nData-driven sparse structure selection for deep neural networks. Zehao Huang, Naiyan Wang, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 304-320, 2018.\n\nThinet: A filter level pruning method for deep neural network compression. Jianxin Wu Jian-Hao Luo, Weiyao Lin, ICCV. Jianxin Wu Jian-Hao Luo and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In ICCV, pages 5058-5066, 2017.\n\nPrincipal Component Analysis. Ian Jolliffe, SpringerBerlin Heidelberg; Berlin, HeidelbergIan Jolliffe. Principal Component Analysis, pages 1094-1096. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.\n\nOperation-aware soft channel pruning using differentiable masks. Minsoo Kang, Bohyung Han, International Conference on Machine Learning. PMLRMinsoo Kang and Bohyung Han. Operation-aware soft channel pruning using differentiable masks. In International Conference on Machine Learning, pages 5122-5131. PMLR, 2020.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Department of Computer Science, University of TorontoMaster's thesisA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.\n\nGraphcut textures: Image and video synthesis using graph cuts. Vivek Kwatra, Arno Sch\u00f6dl, Irfan Essa, Greg Turk, Aaron Bobick, Acm transactions on graphics (tog). 223Vivek Kwatra, Arno Sch\u00f6dl, Irfan Essa, Greg Turk, and Aaron Bobick. Graphcut textures: Image and video synthesis using graph cuts. Acm transactions on graphics (tog), 22(3):277-286, 2003.\n\nDeep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, nature. 5217553Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444, 2015.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, Philip Hs Torr, arXiv:1810.02340Snip: Single-shot network pruning based on connection sensitivity. arXiv preprintNamhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.\n\nEagleeye: Fast sub-net evaluation for efficient neural network pruning. Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, European Conference on Computer Vision. SpringerBailin Li, Bowen Wu, Jiang Su, and Guangrun Wang. Eagleeye: Fast sub-net evaluation for efficient neural network pruning. In European Conference on Computer Vision, pages 639-654. Springer, 2020.\n\n. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf, arXiv:1608.08710Pruning filters for efficient convnets. arXiv preprintHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nPicking winning tickets before training by preserving gradient flow. Chaoqi Wang, Guodong Zhang, Roger Grosse, arXiv:2002.07376arXiv preprintChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376, 2020.\n\n. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro, arXiv:1808.06601Video-to-video synthesis. arXiv preprintTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. arXiv preprint arXiv:1808.06601, 2018.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li, arXiv:1608.03665Learning structured sparsity in deep neural networks. arXiv preprintWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. arXiv preprint arXiv:1608.03665, 2016.\n\nDual averaging methods for regularized stochastic learning and online optimization. Lin Xiao, Journal of Machine Learning Research. 11Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.\n\nA proximal stochastic gradient method with progressive variance reduction. Lin Xiao, Tong Zhang, SIAM Journal on Optimization. 244Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057-2075, 2014.\n\nEmpirical evaluation of rectified activations in convolutional network. Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li, arXiv:1505.00853arXiv preprintBing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.\n\nAutomatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach. Haichuan Yang, Shupeng Gui, Yuhao Zhu, Ji Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHaichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2178-2188, 2020.\n\nDeephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures. Huanrui Yang, Wei Wen, Hai Li, arXiv:1908.09979arXiv preprintHuanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures. arXiv preprint arXiv:1908.09979, 2019.\n\nOnline learning for group lasso. Haiqin Yang, Zenglin Xu, Irwin King, Michael R Lyu, Proceedings of the 27th International Conference on Machine Learning (ICML-10). the 27th International Conference on Machine Learning (ICML-10)Haiqin Yang, Zenglin Xu, Irwin King, and Michael R Lyu. Online learning for group lasso. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 1191-1198, 2010.\n\nGate decorator: Global filter pruning method for accelerating deep convolutional neural networks. Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang, arXiv:1909.08174arXiv preprintZhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang. Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks. arXiv preprint arXiv:1909.08174, 2019.\n\nNearly unbiased variable selection under minimax concave penalty. The Annals of statistics. Cun-Hui Zhang, 38Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. The Annals of statistics, 38(2):894-942, 2010.\n\nMulti-level composite stochastic optimization via nested variance reduction. Junyu Zhang, Lin Xiao, arXiv:1908.11468arXiv preprintJunyu Zhang and Lin Xiao. Multi-level composite stochastic optimization via nested variance reduction. arXiv preprint arXiv:1908.11468, 2019.\n\nPengchuan Zhang, Hunter Lang, Qiang Liu, Lin Xiao, arXiv:2002.10597Statistical adaptive stochastic gradient methods. arXiv preprintPengchuan Zhang, Hunter Lang, Qiang Liu, and Lin Xiao. Statistical adaptive stochastic gradient methods. arXiv preprint arXiv:2002.10597, 2020.\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea- sonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586-595, 2018.\n\nA systematic dnn weight pruning framework using alternating direction method of multipliers. Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, Yanzhi Wang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and Yanzhi Wang. A systematic dnn weight pruning framework using alternating direction method of multipliers. In Proceedings of the European Conference on Computer Vision (ECCV), pages 184-199, 2018.\n\nAccelerate cnn via recursive bayesian pruning. Yuefu Zhou, Ya Zhang, Yanfeng Wang, Qi Tian, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionYuefu Zhou, Ya Zhang, Yanfeng Wang, and Qi Tian. Accelerate cnn via recursive bayesian pruning. In Proceedings of the IEEE International Conference on Computer Vision, pages 3306-3315, 2019.\n\nNeuronlevel structured pruning using polarization regularizer. Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, Xiang Li, Advances in Neural Information Processing Systems. 33Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li. Neuron- level structured pruning using polarization regularizer. Advances in Neural Information Pro- cessing Systems, 33, 2020.\n", "annotations": {"author": "[{\"end\":111,\"start\":77},{\"end\":139,\"start\":112},{\"end\":167,\"start\":140},{\"end\":178,\"start\":168},{\"end\":191,\"start\":179},{\"end\":221,\"start\":192},{\"end\":256,\"start\":222},{\"end\":277,\"start\":257},{\"end\":319,\"start\":278},{\"end\":359,\"start\":320},{\"end\":440,\"start\":360},{\"end\":505,\"start\":441}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":84},{\"end\":117,\"start\":115},{\"end\":151,\"start\":147},{\"end\":177,\"start\":173},{\"end\":190,\"start\":186},{\"end\":202,\"start\":199},{\"end\":234,\"start\":229},{\"end\":276,\"start\":267},{\"end\":296,\"start\":287},{\"end\":337,\"start\":328}]", "author_first_name": "[{\"end\":83,\"start\":77},{\"end\":114,\"start\":112},{\"end\":146,\"start\":140},{\"end\":172,\"start\":168},{\"end\":185,\"start\":179},{\"end\":198,\"start\":192},{\"end\":228,\"start\":222},{\"end\":262,\"start\":257},{\"end\":266,\"start\":263},{\"end\":283,\"start\":278},{\"end\":286,\"start\":284},{\"end\":324,\"start\":320},{\"end\":327,\"start\":325}]", "author_affiliation": "[{\"end\":439,\"start\":361},{\"end\":504,\"start\":442}]", "title": "[{\"end\":74,\"start\":1},{\"end\":579,\"start\":506}]", "venue": null, "abstract": "[{\"end\":1879,\"start\":581}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b44\"},\"end\":1988,\"start\":1984},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1991,\"start\":1988},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2786,\"start\":2783},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2789,\"start\":2786},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3193,\"start\":3189},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3196,\"start\":3193},{\"end\":3199,\"start\":3196},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3218,\"start\":3214},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3221,\"start\":3218},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3351,\"start\":3347},{\"end\":3354,\"start\":3351},{\"end\":3357,\"start\":3354},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3463,\"start\":3459},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3465,\"start\":3463},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4177,\"start\":4173},{\"end\":4180,\"start\":4177},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4182,\"start\":4180},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4696,\"start\":4692},{\"end\":4699,\"start\":4696},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7655,\"start\":7651},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7658,\"start\":7655},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7948,\"start\":7945},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7951,\"start\":7948},{\"end\":7954,\"start\":7951},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7957,\"start\":7954},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7960,\"start\":7957},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7963,\"start\":7960},{\"end\":7966,\"start\":7963},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7969,\"start\":7966},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7992,\"start\":7988},{\"end\":7995,\"start\":7992},{\"end\":7998,\"start\":7995},{\"end\":8001,\"start\":7998},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8026,\"start\":8022},{\"end\":8029,\"start\":8026},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8032,\"start\":8029},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8035,\"start\":8032},{\"end\":8038,\"start\":8035},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8041,\"start\":8038},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8070,\"start\":8066},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8072,\"start\":8070},{\"end\":8126,\"start\":8122},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8147,\"start\":8143},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8150,\"start\":8147},{\"end\":8153,\"start\":8150},{\"end\":8187,\"start\":8183},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8190,\"start\":8187},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8586,\"start\":8582},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8615,\"start\":8611},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8637,\"start\":8633},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8712,\"start\":8708},{\"end\":8715,\"start\":8712},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8718,\"start\":8715},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8894,\"start\":8890},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8896,\"start\":8894},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8986,\"start\":8982},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9049,\"start\":9046},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9204,\"start\":9201},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9207,\"start\":9204},{\"end\":9210,\"start\":9207},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9213,\"start\":9210},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9491,\"start\":9487},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9494,\"start\":9491},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9497,\"start\":9494},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9500,\"start\":9497},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9514,\"start\":9510},{\"end\":9517,\"start\":9514},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9519,\"start\":9517},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9579,\"start\":9576},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9798,\"start\":9794},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9813,\"start\":9809},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9979,\"start\":9975},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12933,\"start\":12929},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12945,\"start\":12941},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12956,\"start\":12952},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12975,\"start\":12971},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14736,\"start\":14732},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16285,\"start\":16282},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16288,\"start\":16285},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":16332,\"start\":16328},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16479,\"start\":16476},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16482,\"start\":16479},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16848,\"start\":16845},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18830,\"start\":18826},{\"end\":18833,\"start\":18830},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20258,\"start\":20255},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20262,\"start\":20260},{\"end\":20446,\"start\":20444},{\"end\":20498,\"start\":20495},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21414,\"start\":21411},{\"end\":21741,\"start\":21737},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23515,\"start\":23511},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23531,\"start\":23527},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":23549,\"start\":23545},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23563,\"start\":23559},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24090,\"start\":24086},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":24255,\"start\":24251},{\"end\":24258,\"start\":24255},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25227,\"start\":25223},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25245,\"start\":25241},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25262,\"start\":25258},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25294,\"start\":25290},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25348,\"start\":25344},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":26047,\"start\":26043},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":26120,\"start\":26116},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26167,\"start\":26165},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27172,\"start\":27168},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27186,\"start\":27182},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27808,\"start\":27804},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27976,\"start\":27972},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27985,\"start\":27981},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28847,\"start\":28843},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29721,\"start\":29718},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30108,\"start\":30104},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30123,\"start\":30119},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30139,\"start\":30135},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30834,\"start\":30830},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30893,\"start\":30890},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30943,\"start\":30942},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":31565,\"start\":31561},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31710,\"start\":31707},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31713,\"start\":31710},{\"end\":31716,\"start\":31713},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":31741,\"start\":31737},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31744,\"start\":31741},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31917,\"start\":31913},{\"end\":31974,\"start\":31970},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32252,\"start\":32248},{\"end\":32255,\"start\":32252},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32360,\"start\":32356},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32553,\"start\":32552},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32592,\"start\":32588},{\"end\":32595,\"start\":32592},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32598,\"start\":32595},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32834,\"start\":32830},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32836,\"start\":32834},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33033,\"start\":33032},{\"end\":33122,\"start\":33118},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33227,\"start\":33223},{\"end\":34016,\"start\":34013},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34257,\"start\":34253},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34845,\"start\":34841},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37488,\"start\":37484},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43168,\"start\":43164},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43462,\"start\":43458},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":43810,\"start\":43806},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":43903,\"start\":43899},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":43906,\"start\":43903},{\"end\":43964,\"start\":43961},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":44207,\"start\":44203},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":44246,\"start\":44242},{\"end\":44363,\"start\":44360},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44415,\"start\":44411},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44556,\"start\":44552},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45437,\"start\":45433},{\"end\":45857,\"start\":45854},{\"end\":46184,\"start\":46181},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46277,\"start\":46273},{\"end\":46280,\"start\":46277},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":48169,\"start\":48165},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":50835,\"start\":50831},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":54241,\"start\":54238},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":54243,\"start\":54241},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":54245,\"start\":54243},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":54975,\"start\":54971},{\"end\":61437,\"start\":61429},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":61441,\"start\":61437},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":64714,\"start\":64711},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":64716,\"start\":64714},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":64718,\"start\":64716},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":64762,\"start\":64758},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":69032,\"start\":69028},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":70211,\"start\":70208},{\"end\":70283,\"start\":70280},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":70335,\"start\":70331},{\"end\":72522,\"start\":72519},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":72590,\"start\":72586},{\"end\":72593,\"start\":72590},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":73509,\"start\":73505},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":77822,\"start\":77818},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":89509,\"start\":89506},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":89595,\"start\":89591},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":89616,\"start\":89612},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":89670,\"start\":89669},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":89693,\"start\":89689}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":75241,\"start\":74886},{\"attributes\":{\"id\":\"fig_1\"},\"end\":75381,\"start\":75242},{\"attributes\":{\"id\":\"fig_2\"},\"end\":75492,\"start\":75382},{\"attributes\":{\"id\":\"fig_3\"},\"end\":75566,\"start\":75493},{\"attributes\":{\"id\":\"fig_4\"},\"end\":75653,\"start\":75567},{\"attributes\":{\"id\":\"fig_8\"},\"end\":75921,\"start\":75654},{\"attributes\":{\"id\":\"fig_9\"},\"end\":76649,\"start\":75922},{\"attributes\":{\"id\":\"fig_11\"},\"end\":76830,\"start\":76650},{\"attributes\":{\"id\":\"fig_12\"},\"end\":76861,\"start\":76831},{\"attributes\":{\"id\":\"fig_14\"},\"end\":76985,\"start\":76862},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":77298,\"start\":76986},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":78029,\"start\":77299},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":78271,\"start\":78030},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":78564,\"start\":78272},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":79654,\"start\":78565},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":80106,\"start\":79655},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":86250,\"start\":80107},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":86574,\"start\":86251},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":87112,\"start\":86575},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":87540,\"start\":87113},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":87918,\"start\":87541},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":88397,\"start\":87919},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":88767,\"start\":88398},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":89416,\"start\":88768}]", "paragraph": "[{\"end\":2770,\"start\":1895},{\"end\":2954,\"start\":2772},{\"end\":3533,\"start\":2956},{\"end\":4831,\"start\":3535},{\"end\":5913,\"start\":4833},{\"end\":5964,\"start\":5915},{\"end\":7184,\"start\":5966},{\"end\":7464,\"start\":7186},{\"end\":8563,\"start\":7481},{\"end\":9050,\"start\":8565},{\"end\":9646,\"start\":9052},{\"end\":10088,\"start\":9648},{\"end\":10767,\"start\":10096},{\"end\":12544,\"start\":10792},{\"end\":14274,\"start\":12609},{\"end\":15042,\"start\":14276},{\"end\":15513,\"start\":15044},{\"end\":15692,\"start\":15552},{\"end\":15955,\"start\":15754},{\"end\":16153,\"start\":15957},{\"end\":16743,\"start\":16155},{\"end\":17275,\"start\":16795},{\"end\":17972,\"start\":17277},{\"end\":18862,\"start\":17974},{\"end\":19061,\"start\":19043},{\"end\":19344,\"start\":19221},{\"end\":19615,\"start\":19422},{\"end\":20213,\"start\":19712},{\"end\":20387,\"start\":20215},{\"end\":20526,\"start\":20430},{\"end\":20674,\"start\":20646},{\"end\":20735,\"start\":20721},{\"end\":21299,\"start\":20737},{\"end\":21415,\"start\":21341},{\"end\":22013,\"start\":21417},{\"end\":22306,\"start\":22015},{\"end\":22844,\"start\":22383},{\"end\":23963,\"start\":23009},{\"end\":24480,\"start\":24014},{\"end\":25008,\"start\":24512},{\"end\":26168,\"start\":25023},{\"end\":26596,\"start\":26206},{\"end\":28766,\"start\":26598},{\"end\":30140,\"start\":28768},{\"end\":31810,\"start\":30142},{\"end\":32361,\"start\":31838},{\"end\":33745,\"start\":32363},{\"end\":34534,\"start\":33776},{\"end\":35538,\"start\":34814},{\"end\":36827,\"start\":35563},{\"end\":37646,\"start\":36829},{\"end\":39173,\"start\":37648},{\"end\":40140,\"start\":39175},{\"end\":40922,\"start\":40167},{\"end\":41338,\"start\":40956},{\"end\":41506,\"start\":41373},{\"end\":41840,\"start\":41577},{\"end\":41919,\"start\":41886},{\"end\":42234,\"start\":42066},{\"end\":42260,\"start\":42236},{\"end\":42353,\"start\":42262},{\"end\":42418,\"start\":42355},{\"end\":42444,\"start\":42420},{\"end\":42509,\"start\":42446},{\"end\":42573,\"start\":42511},{\"end\":42694,\"start\":42575},{\"end\":42842,\"start\":42696},{\"end\":42938,\"start\":42844},{\"end\":43135,\"start\":42940},{\"end\":44465,\"start\":43156},{\"end\":46401,\"start\":44467},{\"end\":46502,\"start\":46448},{\"end\":46634,\"start\":46504},{\"end\":46727,\"start\":46636},{\"end\":46782,\"start\":46729},{\"end\":46839,\"start\":46784},{\"end\":46919,\"start\":46841},{\"end\":46951,\"start\":46921},{\"end\":47029,\"start\":46953},{\"end\":47116,\"start\":47109},{\"end\":47480,\"start\":47191},{\"end\":47719,\"start\":47589},{\"end\":47961,\"start\":47905},{\"end\":48187,\"start\":48071},{\"end\":48476,\"start\":48389},{\"end\":48567,\"start\":48478},{\"end\":48650,\"start\":48569},{\"end\":48805,\"start\":48785},{\"end\":49068,\"start\":49042},{\"end\":49441,\"start\":49070},{\"end\":49649,\"start\":49590},{\"end\":49946,\"start\":49651},{\"end\":50268,\"start\":50177},{\"end\":50510,\"start\":50454},{\"end\":50723,\"start\":50696},{\"end\":50990,\"start\":50789},{\"end\":51099,\"start\":51038},{\"end\":51279,\"start\":51101},{\"end\":51372,\"start\":51281},{\"end\":51443,\"start\":51374},{\"end\":51620,\"start\":51445},{\"end\":52054,\"start\":51863},{\"end\":52080,\"start\":52056},{\"end\":52229,\"start\":52082},{\"end\":52358,\"start\":52312},{\"end\":52453,\"start\":52441},{\"end\":52590,\"start\":52557},{\"end\":52856,\"start\":52613},{\"end\":53166,\"start\":52919},{\"end\":53237,\"start\":53168},{\"end\":53497,\"start\":53425},{\"end\":53615,\"start\":53610},{\"end\":53950,\"start\":53854},{\"end\":54018,\"start\":54006},{\"end\":54291,\"start\":54188},{\"end\":54350,\"start\":54311},{\"end\":54842,\"start\":54533},{\"end\":55317,\"start\":54844},{\"end\":55536,\"start\":55492},{\"end\":55702,\"start\":55649},{\"end\":55977,\"start\":55730},{\"end\":56193,\"start\":56133},{\"end\":56226,\"start\":56195},{\"end\":56399,\"start\":56267},{\"end\":56843,\"start\":56616},{\"end\":56983,\"start\":56845},{\"end\":57055,\"start\":56985},{\"end\":57109,\"start\":57057},{\"end\":57274,\"start\":57155},{\"end\":57471,\"start\":57276},{\"end\":57676,\"start\":57593},{\"end\":57781,\"start\":57712},{\"end\":58546,\"start\":58337},{\"end\":58597,\"start\":58548},{\"end\":58739,\"start\":58599},{\"end\":58768,\"start\":58741},{\"end\":59011,\"start\":58770},{\"end\":59443,\"start\":59057},{\"end\":59549,\"start\":59506},{\"end\":59784,\"start\":59551},{\"end\":60056,\"start\":59851},{\"end\":60578,\"start\":60353},{\"end\":60804,\"start\":60715},{\"end\":60910,\"start\":60867},{\"end\":60963,\"start\":60912},{\"end\":61129,\"start\":60965},{\"end\":61217,\"start\":61131},{\"end\":61378,\"start\":61335},{\"end\":61474,\"start\":61380},{\"end\":61555,\"start\":61520},{\"end\":62405,\"start\":62336},{\"end\":62817,\"start\":62720},{\"end\":63089,\"start\":62972},{\"end\":63419,\"start\":63333},{\"end\":63581,\"start\":63513},{\"end\":63719,\"start\":63645},{\"end\":63782,\"start\":63765},{\"end\":64016,\"start\":63946},{\"end\":64304,\"start\":64018},{\"end\":64333,\"start\":64306},{\"end\":64587,\"start\":64491},{\"end\":64799,\"start\":64589},{\"end\":65042,\"start\":64872},{\"end\":65145,\"start\":65044},{\"end\":65224,\"start\":65178},{\"end\":66195,\"start\":65226},{\"end\":66305,\"start\":66250},{\"end\":66460,\"start\":66326},{\"end\":66514,\"start\":66462},{\"end\":66601,\"start\":66516},{\"end\":66751,\"start\":66603},{\"end\":66918,\"start\":66849},{\"end\":67194,\"start\":67014},{\"end\":67359,\"start\":67196},{\"end\":67583,\"start\":67495},{\"end\":68462,\"start\":67650},{\"end\":69538,\"start\":68464},{\"end\":70139,\"start\":69620},{\"end\":71965,\"start\":70141},{\"end\":74885,\"start\":71995}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12608,\"start\":12545},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15753,\"start\":15693},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19042,\"start\":18863},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19220,\"start\":19062},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19421,\"start\":19345},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19711,\"start\":19616},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20424,\"start\":20388},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20560,\"start\":20527},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20639,\"start\":20560},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20714,\"start\":20675},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21340,\"start\":21300},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22382,\"start\":22307},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23008,\"start\":22845},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24013,\"start\":23964},{\"attributes\":{\"id\":\"formula_14\"},\"end\":34631,\"start\":34535},{\"attributes\":{\"id\":\"formula_15\"},\"end\":34700,\"start\":34631},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34813,\"start\":34700},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41576,\"start\":41507},{\"attributes\":{\"id\":\"formula_18\"},\"end\":41885,\"start\":41841},{\"attributes\":{\"id\":\"formula_19\"},\"end\":42065,\"start\":41920},{\"attributes\":{\"id\":\"formula_20\"},\"end\":47108,\"start\":47030},{\"attributes\":{\"id\":\"formula_21\"},\"end\":47152,\"start\":47117},{\"attributes\":{\"id\":\"formula_22\"},\"end\":47190,\"start\":47152},{\"attributes\":{\"id\":\"formula_23\"},\"end\":47588,\"start\":47481},{\"attributes\":{\"id\":\"formula_24\"},\"end\":47904,\"start\":47720},{\"attributes\":{\"id\":\"formula_25\"},\"end\":48070,\"start\":47962},{\"attributes\":{\"id\":\"formula_26\"},\"end\":48388,\"start\":48188},{\"attributes\":{\"id\":\"formula_27\"},\"end\":48784,\"start\":48651},{\"attributes\":{\"id\":\"formula_28\"},\"end\":49041,\"start\":48806},{\"attributes\":{\"id\":\"formula_29\"},\"end\":49546,\"start\":49442},{\"attributes\":{\"id\":\"formula_30\"},\"end\":50058,\"start\":49947},{\"attributes\":{\"id\":\"formula_31\"},\"end\":50176,\"start\":50058},{\"attributes\":{\"id\":\"formula_32\"},\"end\":50453,\"start\":50269},{\"attributes\":{\"id\":\"formula_33\"},\"end\":50695,\"start\":50511},{\"attributes\":{\"id\":\"formula_34\"},\"end\":50788,\"start\":50724},{\"attributes\":{\"id\":\"formula_35\"},\"end\":51862,\"start\":51621},{\"attributes\":{\"id\":\"formula_36\"},\"end\":52311,\"start\":52230},{\"attributes\":{\"id\":\"formula_37\"},\"end\":52440,\"start\":52359},{\"attributes\":{\"id\":\"formula_38\"},\"end\":52485,\"start\":52454},{\"attributes\":{\"id\":\"formula_39\"},\"end\":52556,\"start\":52485},{\"attributes\":{\"id\":\"formula_40\"},\"end\":52612,\"start\":52591},{\"attributes\":{\"id\":\"formula_41\"},\"end\":52918,\"start\":52857},{\"attributes\":{\"id\":\"formula_42\"},\"end\":53424,\"start\":53238},{\"attributes\":{\"id\":\"formula_43\"},\"end\":53609,\"start\":53498},{\"attributes\":{\"id\":\"formula_44\"},\"end\":53853,\"start\":53616},{\"attributes\":{\"id\":\"formula_45\"},\"end\":54005,\"start\":53951},{\"attributes\":{\"id\":\"formula_46\"},\"end\":54187,\"start\":54019},{\"attributes\":{\"id\":\"formula_47\"},\"end\":54310,\"start\":54292},{\"attributes\":{\"id\":\"formula_48\"},\"end\":54455,\"start\":54351},{\"attributes\":{\"id\":\"formula_49\"},\"end\":54532,\"start\":54455},{\"attributes\":{\"id\":\"formula_50\"},\"end\":55365,\"start\":55318},{\"attributes\":{\"id\":\"formula_51\"},\"end\":55491,\"start\":55406},{\"attributes\":{\"id\":\"formula_52\"},\"end\":55648,\"start\":55537},{\"attributes\":{\"id\":\"formula_53\"},\"end\":55729,\"start\":55703},{\"attributes\":{\"id\":\"formula_54\"},\"end\":56053,\"start\":55978},{\"attributes\":{\"id\":\"formula_55\"},\"end\":56132,\"start\":56053},{\"attributes\":{\"id\":\"formula_56\"},\"end\":56266,\"start\":56227},{\"attributes\":{\"id\":\"formula_57\"},\"end\":56615,\"start\":56400},{\"attributes\":{\"id\":\"formula_58\"},\"end\":57154,\"start\":57110},{\"attributes\":{\"id\":\"formula_59\"},\"end\":57592,\"start\":57472},{\"attributes\":{\"id\":\"formula_60\"},\"end\":57711,\"start\":57677},{\"attributes\":{\"id\":\"formula_61\"},\"end\":58336,\"start\":57782},{\"attributes\":{\"id\":\"formula_62\"},\"end\":59056,\"start\":59012},{\"attributes\":{\"id\":\"formula_63\"},\"end\":59505,\"start\":59444},{\"attributes\":{\"id\":\"formula_64\"},\"end\":59850,\"start\":59785},{\"attributes\":{\"id\":\"formula_65\"},\"end\":60199,\"start\":60057},{\"attributes\":{\"id\":\"formula_66\"},\"end\":60352,\"start\":60199},{\"attributes\":{\"id\":\"formula_67\"},\"end\":60663,\"start\":60579},{\"attributes\":{\"id\":\"formula_68\"},\"end\":60714,\"start\":60663},{\"attributes\":{\"id\":\"formula_69\"},\"end\":60866,\"start\":60805},{\"attributes\":{\"id\":\"formula_70\"},\"end\":61334,\"start\":61218},{\"attributes\":{\"id\":\"formula_71\"},\"end\":61519,\"start\":61475},{\"attributes\":{\"id\":\"formula_72\"},\"end\":62335,\"start\":61556},{\"attributes\":{\"id\":\"formula_73\"},\"end\":62719,\"start\":62406},{\"attributes\":{\"id\":\"formula_74\"},\"end\":62971,\"start\":62818},{\"attributes\":{\"id\":\"formula_75\"},\"end\":63332,\"start\":63090},{\"attributes\":{\"id\":\"formula_76\"},\"end\":63512,\"start\":63420},{\"attributes\":{\"id\":\"formula_77\"},\"end\":63644,\"start\":63582},{\"attributes\":{\"id\":\"formula_78\"},\"end\":63764,\"start\":63720},{\"attributes\":{\"id\":\"formula_79\"},\"end\":63945,\"start\":63783},{\"attributes\":{\"id\":\"formula_80\"},\"end\":64490,\"start\":64334},{\"attributes\":{\"id\":\"formula_81\"},\"end\":64871,\"start\":64800},{\"attributes\":{\"id\":\"formula_82\"},\"end\":66325,\"start\":66306},{\"attributes\":{\"id\":\"formula_83\"},\"end\":66848,\"start\":66752},{\"attributes\":{\"id\":\"formula_84\"},\"end\":66967,\"start\":66919},{\"attributes\":{\"id\":\"formula_85\"},\"end\":67013,\"start\":66967},{\"attributes\":{\"id\":\"formula_86\"},\"end\":67494,\"start\":67360},{\"attributes\":{\"id\":\"formula_87\"},\"end\":67649,\"start\":67584},{\"attributes\":{\"id\":\"formula_88\"},\"end\":69619,\"start\":69539}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26262,\"start\":26255},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":26951,\"start\":26944},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27617,\"start\":27610},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29094,\"start\":29087},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30037,\"start\":30030},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32857,\"start\":32850},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40748,\"start\":40741},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":41039,\"start\":41032},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":69096,\"start\":69089},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":69990,\"start\":69983},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":70821,\"start\":70813},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":71043,\"start\":71035},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":71570,\"start\":71562},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":72624,\"start\":72616},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":73999,\"start\":73991}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1893,\"start\":1881},{\"attributes\":{\"n\":\"2\"},\"end\":7479,\"start\":7467},{\"attributes\":{\"n\":\"3\"},\"end\":10094,\"start\":10091},{\"attributes\":{\"n\":\"3.1\"},\"end\":10790,\"start\":10770},{\"attributes\":{\"n\":\"3.2\"},\"end\":15550,\"start\":15516},{\"attributes\":{\"n\":\"3.3\"},\"end\":16793,\"start\":16746},{\"end\":20428,\"start\":20426},{\"end\":20644,\"start\":20641},{\"end\":20719,\"start\":20716},{\"attributes\":{\"n\":\"3.4\"},\"end\":24510,\"start\":24483},{\"attributes\":{\"n\":\"4\"},\"end\":25021,\"start\":25011},{\"attributes\":{\"n\":\"4.1\"},\"end\":26204,\"start\":26171},{\"attributes\":{\"n\":\"4.2\"},\"end\":31836,\"start\":31813},{\"attributes\":{\"n\":\"5\"},\"end\":33774,\"start\":33748},{\"end\":35561,\"start\":35541},{\"end\":40165,\"start\":40143},{\"end\":40954,\"start\":40925},{\"end\":41371,\"start\":41341},{\"end\":43154,\"start\":43138},{\"end\":46446,\"start\":46404},{\"end\":49588,\"start\":49548},{\"end\":51036,\"start\":50993},{\"end\":55405,\"start\":55367},{\"end\":65176,\"start\":65148},{\"end\":66248,\"start\":66198},{\"end\":71993,\"start\":71968},{\"end\":75504,\"start\":75494},{\"end\":75578,\"start\":75568},{\"end\":75665,\"start\":75655},{\"end\":75933,\"start\":75923},{\"end\":76842,\"start\":76832},{\"end\":76873,\"start\":76863},{\"end\":76996,\"start\":76987},{\"end\":77309,\"start\":77300},{\"end\":78040,\"start\":78031},{\"end\":78282,\"start\":78273},{\"end\":78575,\"start\":78566},{\"end\":79665,\"start\":79656},{\"end\":86261,\"start\":86252},{\"end\":86585,\"start\":86576},{\"end\":87123,\"start\":87114},{\"end\":87551,\"start\":87542},{\"end\":87930,\"start\":87920},{\"end\":88409,\"start\":88399},{\"end\":88779,\"start\":88769}]", "table": "[{\"end\":77298,\"start\":77063},{\"end\":78271,\"start\":78063},{\"end\":78564,\"start\":78284},{\"end\":79654,\"start\":78599},{\"end\":80106,\"start\":79751},{\"end\":86250,\"start\":83907},{\"end\":86574,\"start\":86299},{\"end\":87112,\"start\":86651},{\"end\":87540,\"start\":87203},{\"end\":87918,\"start\":87573},{\"end\":88397,\"start\":87999},{\"end\":88767,\"start\":88468},{\"end\":89416,\"start\":89352}]", "figure_caption": "[{\"end\":75241,\"start\":74888},{\"end\":75381,\"start\":75244},{\"end\":75492,\"start\":75384},{\"end\":75566,\"start\":75506},{\"end\":75653,\"start\":75580},{\"end\":75921,\"start\":75667},{\"end\":76649,\"start\":75935},{\"end\":76830,\"start\":76652},{\"end\":76861,\"start\":76844},{\"end\":76985,\"start\":76875},{\"end\":77063,\"start\":76998},{\"end\":78029,\"start\":77311},{\"end\":78063,\"start\":78042},{\"end\":78599,\"start\":78577},{\"end\":79751,\"start\":79667},{\"end\":83907,\"start\":80109},{\"end\":86299,\"start\":86263},{\"end\":86651,\"start\":86587},{\"end\":87203,\"start\":87125},{\"end\":87573,\"start\":87553},{\"end\":87999,\"start\":87933},{\"end\":88468,\"start\":88412},{\"end\":89352,\"start\":88782}]", "figure_ref": "[{\"end\":2289,\"start\":2281},{\"end\":5147,\"start\":5139},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12283,\"start\":12274},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13751,\"start\":13742},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14340,\"start\":14331},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20026,\"start\":20017},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21886,\"start\":21877},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22498,\"start\":22489},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23454,\"start\":23445},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30485,\"start\":30477},{\"end\":34468,\"start\":34460},{\"end\":35127,\"start\":35119},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":46849,\"start\":46843},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":51453,\"start\":51447},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":70915,\"start\":70907},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":71815,\"start\":71807},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":73860,\"start\":73851},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":74236,\"start\":74227},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":74584,\"start\":74575}]", "bib_author_first_name": "[{\"end\":91005,\"start\":90998},{\"end\":91020,\"start\":91012},{\"end\":91037,\"start\":91031},{\"end\":91055,\"start\":91046},{\"end\":91303,\"start\":91298},{\"end\":91318,\"start\":91309},{\"end\":91328,\"start\":91324},{\"end\":91340,\"start\":91333},{\"end\":91355,\"start\":91348},{\"end\":91371,\"start\":91361},{\"end\":91766,\"start\":91762},{\"end\":91995,\"start\":91988},{\"end\":92006,\"start\":92002},{\"end\":92019,\"start\":92015},{\"end\":92241,\"start\":92233},{\"end\":92255,\"start\":92251},{\"end\":92274,\"start\":92265},{\"end\":92784,\"start\":92778},{\"end\":92798,\"start\":92791},{\"end\":92817,\"start\":92805},{\"end\":93067,\"start\":93061},{\"end\":93080,\"start\":93074},{\"end\":93089,\"start\":93087},{\"end\":93100,\"start\":93094},{\"end\":93112,\"start\":93107},{\"end\":93123,\"start\":93118},{\"end\":93132,\"start\":93128},{\"end\":93143,\"start\":93137},{\"end\":93475,\"start\":93469},{\"end\":93487,\"start\":93482},{\"end\":93498,\"start\":93493},{\"end\":93684,\"start\":93679},{\"end\":93994,\"start\":93992},{\"end\":94005,\"start\":94002},{\"end\":94015,\"start\":94012},{\"end\":94025,\"start\":94022},{\"end\":94326,\"start\":94321},{\"end\":94343,\"start\":94336},{\"end\":94355,\"start\":94350},{\"end\":94731,\"start\":94724},{\"end\":94745,\"start\":94739},{\"end\":94980,\"start\":94977},{\"end\":94990,\"start\":94987},{\"end\":95004,\"start\":94997},{\"end\":95019,\"start\":95013},{\"end\":95027,\"start\":95024},{\"end\":95034,\"start\":95032},{\"end\":95521,\"start\":95515},{\"end\":95534,\"start\":95528},{\"end\":95548,\"start\":95542},{\"end\":95558,\"start\":95554},{\"end\":95839,\"start\":95832},{\"end\":95855,\"start\":95846},{\"end\":95869,\"start\":95861},{\"end\":95877,\"start\":95875},{\"end\":95890,\"start\":95883},{\"end\":95902,\"start\":95896},{\"end\":95916,\"start\":95908},{\"end\":96361,\"start\":96357},{\"end\":96374,\"start\":96369},{\"end\":96670,\"start\":96665},{\"end\":96673,\"start\":96671},{\"end\":96689,\"start\":96682},{\"end\":96702,\"start\":96696},{\"end\":97019,\"start\":97013},{\"end\":97032,\"start\":97025},{\"end\":97046,\"start\":97040},{\"end\":97303,\"start\":97295},{\"end\":97320,\"start\":97313},{\"end\":97592,\"start\":97584},{\"end\":97610,\"start\":97602},{\"end\":97632,\"start\":97631},{\"end\":97648,\"start\":97641},{\"end\":97988,\"start\":97980},{\"end\":98239,\"start\":98233},{\"end\":98251,\"start\":98246},{\"end\":98263,\"start\":98259},{\"end\":98564,\"start\":98554},{\"end\":98579,\"start\":98570},{\"end\":98592,\"start\":98585},{\"end\":98607,\"start\":98600},{\"end\":98621,\"start\":98612},{\"end\":98916,\"start\":98913},{\"end\":98935,\"start\":98929},{\"end\":98949,\"start\":98944},{\"end\":98967,\"start\":98961},{\"end\":99215,\"start\":99210},{\"end\":99243,\"start\":99235},{\"end\":99533,\"start\":99528},{\"end\":99553,\"start\":99552},{\"end\":99565,\"start\":99562},{\"end\":99581,\"start\":99575},{\"end\":99871,\"start\":99867},{\"end\":99883,\"start\":99877},{\"end\":99894,\"start\":99889},{\"end\":99904,\"start\":99900},{\"end\":99916,\"start\":99909},{\"end\":99926,\"start\":99925},{\"end\":99940,\"start\":99933},{\"end\":99942,\"start\":99941},{\"end\":100232,\"start\":100228},{\"end\":100243,\"start\":100238},{\"end\":100256,\"start\":100249},{\"end\":100258,\"start\":100257},{\"end\":100596,\"start\":100592},{\"end\":100606,\"start\":100602},{\"end\":100617,\"start\":100613},{\"end\":100631,\"start\":100624},{\"end\":100633,\"start\":100632},{\"end\":101002,\"start\":100995},{\"end\":101014,\"start\":101007},{\"end\":101030,\"start\":101022},{\"end\":101040,\"start\":101036},{\"end\":101464,\"start\":101457},{\"end\":101476,\"start\":101469},{\"end\":101492,\"start\":101484},{\"end\":101502,\"start\":101498},{\"end\":101843,\"start\":101839},{\"end\":101856,\"start\":101848},{\"end\":101869,\"start\":101863},{\"end\":101882,\"start\":101876},{\"end\":101889,\"start\":101887},{\"end\":102248,\"start\":102243},{\"end\":102255,\"start\":102253},{\"end\":102268,\"start\":102261},{\"end\":102280,\"start\":102274},{\"end\":102293,\"start\":102287},{\"end\":102302,\"start\":102298},{\"end\":102718,\"start\":102713},{\"end\":102730,\"start\":102723},{\"end\":102742,\"start\":102738},{\"end\":103023,\"start\":103020},{\"end\":103040,\"start\":103035},{\"end\":103242,\"start\":103234},{\"end\":103256,\"start\":103251},{\"end\":103270,\"start\":103266},{\"end\":103448,\"start\":103440},{\"end\":103456,\"start\":103453},{\"end\":103470,\"start\":103463},{\"end\":103485,\"start\":103476},{\"end\":103876,\"start\":103871},{\"end\":103890,\"start\":103884},{\"end\":104279,\"start\":104272},{\"end\":104303,\"start\":104297},{\"end\":104496,\"start\":104493},{\"end\":104739,\"start\":104733},{\"end\":104753,\"start\":104746},{\"end\":105038,\"start\":105037},{\"end\":105052,\"start\":105051},{\"end\":105360,\"start\":105355},{\"end\":105373,\"start\":105369},{\"end\":105387,\"start\":105382},{\"end\":105398,\"start\":105394},{\"end\":105410,\"start\":105405},{\"end\":105666,\"start\":105662},{\"end\":105680,\"start\":105674},{\"end\":105697,\"start\":105689},{\"end\":105825,\"start\":105818},{\"end\":105845,\"start\":105831},{\"end\":105865,\"start\":105856},{\"end\":106212,\"start\":106206},{\"end\":106222,\"start\":106217},{\"end\":106232,\"start\":106227},{\"end\":106245,\"start\":106237},{\"end\":106502,\"start\":106499},{\"end\":106511,\"start\":106507},{\"end\":106523,\"start\":106519},{\"end\":106541,\"start\":106536},{\"end\":106553,\"start\":106549},{\"end\":106559,\"start\":106554},{\"end\":106820,\"start\":106814},{\"end\":106834,\"start\":106830},{\"end\":106848,\"start\":106844},{\"end\":106862,\"start\":106857},{\"end\":106879,\"start\":106874},{\"end\":106892,\"start\":106887},{\"end\":106894,\"start\":106893},{\"end\":106908,\"start\":106902},{\"end\":106922,\"start\":106917},{\"end\":107525,\"start\":107519},{\"end\":107539,\"start\":107532},{\"end\":107552,\"start\":107547},{\"end\":107757,\"start\":107748},{\"end\":107771,\"start\":107764},{\"end\":107784,\"start\":107777},{\"end\":107796,\"start\":107790},{\"end\":107808,\"start\":107802},{\"end\":107817,\"start\":107814},{\"end\":107830,\"start\":107825},{\"end\":108065,\"start\":108062},{\"end\":108079,\"start\":108071},{\"end\":108090,\"start\":108084},{\"end\":108102,\"start\":108097},{\"end\":108112,\"start\":108109},{\"end\":108441,\"start\":108438},{\"end\":108724,\"start\":108721},{\"end\":108735,\"start\":108731},{\"end\":109006,\"start\":109002},{\"end\":109017,\"start\":109011},{\"end\":109030,\"start\":109024},{\"end\":109039,\"start\":109037},{\"end\":109361,\"start\":109353},{\"end\":109375,\"start\":109368},{\"end\":109386,\"start\":109381},{\"end\":109394,\"start\":109392},{\"end\":109936,\"start\":109929},{\"end\":109946,\"start\":109943},{\"end\":109955,\"start\":109952},{\"end\":110202,\"start\":110196},{\"end\":110216,\"start\":110209},{\"end\":110226,\"start\":110221},{\"end\":110240,\"start\":110233},{\"end\":110242,\"start\":110241},{\"end\":110693,\"start\":110685},{\"end\":110702,\"start\":110699},{\"end\":110715,\"start\":110708},{\"end\":110724,\"start\":110720},{\"end\":110733,\"start\":110729},{\"end\":111066,\"start\":111059},{\"end\":111293,\"start\":111288},{\"end\":111304,\"start\":111301},{\"end\":111493,\"start\":111484},{\"end\":111507,\"start\":111501},{\"end\":111519,\"start\":111514},{\"end\":111528,\"start\":111525},{\"end\":111839,\"start\":111832},{\"end\":111854,\"start\":111847},{\"end\":111868,\"start\":111862},{\"end\":111870,\"start\":111869},{\"end\":111881,\"start\":111878},{\"end\":111899,\"start\":111893},{\"end\":112403,\"start\":112396},{\"end\":112418,\"start\":112411},{\"end\":112428,\"start\":112423},{\"end\":112440,\"start\":112436},{\"end\":112452,\"start\":112447},{\"end\":112463,\"start\":112458},{\"end\":112478,\"start\":112472},{\"end\":112929,\"start\":112924},{\"end\":112938,\"start\":112936},{\"end\":112953,\"start\":112946},{\"end\":112962,\"start\":112960},{\"end\":113348,\"start\":113345},{\"end\":113364,\"start\":113357},{\"end\":113378,\"start\":113372},{\"end\":113392,\"start\":113386},{\"end\":113402,\"start\":113399},{\"end\":113416,\"start\":113411}]", "bib_author_last_name": "[{\"end\":91010,\"start\":91006},{\"end\":91029,\"start\":91021},{\"end\":91044,\"start\":91038},{\"end\":91065,\"start\":91056},{\"end\":91307,\"start\":91304},{\"end\":91322,\"start\":91319},{\"end\":91331,\"start\":91329},{\"end\":91346,\"start\":91341},{\"end\":91359,\"start\":91356},{\"end\":91376,\"start\":91372},{\"end\":91774,\"start\":91767},{\"end\":92000,\"start\":91996},{\"end\":92013,\"start\":92007},{\"end\":92023,\"start\":92020},{\"end\":92249,\"start\":92242},{\"end\":92263,\"start\":92256},{\"end\":92290,\"start\":92275},{\"end\":92789,\"start\":92785},{\"end\":92803,\"start\":92799},{\"end\":92821,\"start\":92818},{\"end\":93072,\"start\":93068},{\"end\":93085,\"start\":93081},{\"end\":93092,\"start\":93090},{\"end\":93105,\"start\":93101},{\"end\":93116,\"start\":93113},{\"end\":93126,\"start\":93124},{\"end\":93135,\"start\":93133},{\"end\":93147,\"start\":93144},{\"end\":93480,\"start\":93476},{\"end\":93491,\"start\":93488},{\"end\":93501,\"start\":93499},{\"end\":93689,\"start\":93685},{\"end\":93697,\"start\":93691},{\"end\":94000,\"start\":93995},{\"end\":94010,\"start\":94006},{\"end\":94020,\"start\":94016},{\"end\":94031,\"start\":94026},{\"end\":94334,\"start\":94327},{\"end\":94348,\"start\":94344},{\"end\":94370,\"start\":94356},{\"end\":94737,\"start\":94732},{\"end\":94752,\"start\":94746},{\"end\":94985,\"start\":94981},{\"end\":94995,\"start\":94991},{\"end\":95011,\"start\":95005},{\"end\":95022,\"start\":95020},{\"end\":95030,\"start\":95028},{\"end\":95042,\"start\":95035},{\"end\":95526,\"start\":95522},{\"end\":95540,\"start\":95535},{\"end\":95552,\"start\":95549},{\"end\":95566,\"start\":95559},{\"end\":95844,\"start\":95840},{\"end\":95859,\"start\":95856},{\"end\":95873,\"start\":95870},{\"end\":95881,\"start\":95878},{\"end\":95894,\"start\":95891},{\"end\":95906,\"start\":95903},{\"end\":95921,\"start\":95917},{\"end\":96367,\"start\":96362},{\"end\":96381,\"start\":96375},{\"end\":96680,\"start\":96674},{\"end\":96694,\"start\":96690},{\"end\":96709,\"start\":96703},{\"end\":97023,\"start\":97020},{\"end\":97038,\"start\":97033},{\"end\":97053,\"start\":97047},{\"end\":97311,\"start\":97304},{\"end\":97327,\"start\":97321},{\"end\":97600,\"start\":97593},{\"end\":97618,\"start\":97611},{\"end\":97629,\"start\":97620},{\"end\":97639,\"start\":97633},{\"end\":97652,\"start\":97649},{\"end\":97660,\"start\":97654},{\"end\":97998,\"start\":97989},{\"end\":98244,\"start\":98240},{\"end\":98257,\"start\":98252},{\"end\":98270,\"start\":98264},{\"end\":98552,\"start\":98543},{\"end\":98568,\"start\":98565},{\"end\":98583,\"start\":98580},{\"end\":98598,\"start\":98593},{\"end\":98610,\"start\":98608},{\"end\":98626,\"start\":98622},{\"end\":98631,\"start\":98628},{\"end\":98927,\"start\":98917},{\"end\":98942,\"start\":98936},{\"end\":98959,\"start\":98950},{\"end\":98974,\"start\":98968},{\"end\":99233,\"start\":99216},{\"end\":99247,\"start\":99244},{\"end\":99256,\"start\":99249},{\"end\":99545,\"start\":99534},{\"end\":99550,\"start\":99547},{\"end\":99560,\"start\":99554},{\"end\":99573,\"start\":99566},{\"end\":99585,\"start\":99582},{\"end\":99591,\"start\":99587},{\"end\":99875,\"start\":99872},{\"end\":99887,\"start\":99884},{\"end\":99898,\"start\":99895},{\"end\":99907,\"start\":99905},{\"end\":99923,\"start\":99917},{\"end\":99931,\"start\":99927},{\"end\":99951,\"start\":99943},{\"end\":99958,\"start\":99953},{\"end\":100236,\"start\":100233},{\"end\":100247,\"start\":100244},{\"end\":100264,\"start\":100259},{\"end\":100600,\"start\":100597},{\"end\":100611,\"start\":100607},{\"end\":100622,\"start\":100618},{\"end\":100639,\"start\":100634},{\"end\":101005,\"start\":101003},{\"end\":101020,\"start\":101015},{\"end\":101034,\"start\":101031},{\"end\":101044,\"start\":101041},{\"end\":101467,\"start\":101465},{\"end\":101482,\"start\":101477},{\"end\":101496,\"start\":101493},{\"end\":101506,\"start\":101503},{\"end\":101846,\"start\":101844},{\"end\":101861,\"start\":101857},{\"end\":101874,\"start\":101870},{\"end\":101885,\"start\":101883},{\"end\":101894,\"start\":101890},{\"end\":102251,\"start\":102249},{\"end\":102259,\"start\":102256},{\"end\":102272,\"start\":102269},{\"end\":102285,\"start\":102281},{\"end\":102296,\"start\":102294},{\"end\":102306,\"start\":102303},{\"end\":102721,\"start\":102719},{\"end\":102736,\"start\":102731},{\"end\":102746,\"start\":102743},{\"end\":103033,\"start\":103024},{\"end\":103047,\"start\":103041},{\"end\":103249,\"start\":103243},{\"end\":103264,\"start\":103257},{\"end\":103275,\"start\":103271},{\"end\":103451,\"start\":103449},{\"end\":103461,\"start\":103457},{\"end\":103474,\"start\":103471},{\"end\":103490,\"start\":103486},{\"end\":103882,\"start\":103877},{\"end\":103895,\"start\":103891},{\"end\":104295,\"start\":104280},{\"end\":104307,\"start\":104304},{\"end\":104505,\"start\":104497},{\"end\":104744,\"start\":104740},{\"end\":104757,\"start\":104754},{\"end\":105049,\"start\":105039},{\"end\":105059,\"start\":105053},{\"end\":105367,\"start\":105361},{\"end\":105380,\"start\":105374},{\"end\":105392,\"start\":105388},{\"end\":105403,\"start\":105399},{\"end\":105417,\"start\":105411},{\"end\":105672,\"start\":105667},{\"end\":105687,\"start\":105681},{\"end\":105704,\"start\":105698},{\"end\":105829,\"start\":105826},{\"end\":105854,\"start\":105846},{\"end\":105870,\"start\":105866},{\"end\":106215,\"start\":106213},{\"end\":106225,\"start\":106223},{\"end\":106235,\"start\":106233},{\"end\":106250,\"start\":106246},{\"end\":106505,\"start\":106503},{\"end\":106517,\"start\":106512},{\"end\":106534,\"start\":106524},{\"end\":106547,\"start\":106542},{\"end\":106564,\"start\":106560},{\"end\":106828,\"start\":106821},{\"end\":106842,\"start\":106835},{\"end\":106855,\"start\":106849},{\"end\":106872,\"start\":106863},{\"end\":106885,\"start\":106880},{\"end\":106900,\"start\":106895},{\"end\":106915,\"start\":106909},{\"end\":106933,\"start\":106923},{\"end\":107530,\"start\":107526},{\"end\":107545,\"start\":107540},{\"end\":107559,\"start\":107553},{\"end\":107762,\"start\":107758},{\"end\":107775,\"start\":107772},{\"end\":107788,\"start\":107785},{\"end\":107800,\"start\":107797},{\"end\":107812,\"start\":107809},{\"end\":107823,\"start\":107818},{\"end\":107840,\"start\":107831},{\"end\":108069,\"start\":108066},{\"end\":108082,\"start\":108080},{\"end\":108095,\"start\":108091},{\"end\":108107,\"start\":108103},{\"end\":108115,\"start\":108113},{\"end\":108446,\"start\":108442},{\"end\":108729,\"start\":108725},{\"end\":108741,\"start\":108736},{\"end\":109009,\"start\":109007},{\"end\":109022,\"start\":109018},{\"end\":109035,\"start\":109031},{\"end\":109042,\"start\":109040},{\"end\":109366,\"start\":109362},{\"end\":109379,\"start\":109376},{\"end\":109390,\"start\":109387},{\"end\":109398,\"start\":109395},{\"end\":109941,\"start\":109937},{\"end\":109950,\"start\":109947},{\"end\":109958,\"start\":109956},{\"end\":110207,\"start\":110203},{\"end\":110219,\"start\":110217},{\"end\":110231,\"start\":110227},{\"end\":110246,\"start\":110243},{\"end\":110697,\"start\":110694},{\"end\":110706,\"start\":110703},{\"end\":110718,\"start\":110716},{\"end\":110727,\"start\":110725},{\"end\":110738,\"start\":110734},{\"end\":111072,\"start\":111067},{\"end\":111299,\"start\":111294},{\"end\":111309,\"start\":111305},{\"end\":111499,\"start\":111494},{\"end\":111512,\"start\":111508},{\"end\":111523,\"start\":111520},{\"end\":111533,\"start\":111529},{\"end\":111845,\"start\":111840},{\"end\":111860,\"start\":111855},{\"end\":111876,\"start\":111871},{\"end\":111891,\"start\":111882},{\"end\":111904,\"start\":111900},{\"end\":112409,\"start\":112404},{\"end\":112421,\"start\":112419},{\"end\":112434,\"start\":112429},{\"end\":112445,\"start\":112441},{\"end\":112456,\"start\":112453},{\"end\":112470,\"start\":112464},{\"end\":112483,\"start\":112479},{\"end\":112934,\"start\":112930},{\"end\":112944,\"start\":112939},{\"end\":112958,\"start\":112954},{\"end\":112967,\"start\":112963},{\"end\":113355,\"start\":113349},{\"end\":113370,\"start\":113365},{\"end\":113384,\"start\":113379},{\"end\":113397,\"start\":113393},{\"end\":113409,\"start\":113403},{\"end\":113419,\"start\":113417}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1238031},\"end\":91257,\"start\":90949},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":90239045},\"end\":91758,\"start\":91259},{\"attributes\":{\"id\":\"b2\"},\"end\":91883,\"start\":91760},{\"attributes\":{\"id\":\"b3\"},\"end\":92212,\"start\":91885},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11253972},\"end\":92680,\"start\":92214},{\"attributes\":{\"id\":\"b5\"},\"end\":92973,\"start\":92682},{\"attributes\":{\"doi\":\"arXiv:2004.03639\",\"id\":\"b6\"},\"end\":93395,\"start\":92975},{\"attributes\":{\"doi\":\"App. 16/355\",\"id\":\"b7\"},\"end\":93677,\"start\":93397},{\"attributes\":{\"doi\":\"arXiv:1802.10280\",\"id\":\"b8\"},\"end\":93917,\"start\":93679},{\"attributes\":{\"doi\":\"arXiv:1710.09282\",\"id\":\"b9\"},\"end\":94219,\"start\":93919},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":218654665},\"end\":94654,\"start\":94221},{\"attributes\":{\"doi\":\"arXiv:2102.03869\",\"id\":\"b11\"},\"end\":94923,\"start\":94656},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206597351},\"end\":95332,\"start\":94925},{\"attributes\":{\"id\":\"b13\"},\"end\":95448,\"start\":95334},{\"attributes\":{\"doi\":\"arXiv:2103.10559\",\"id\":\"b14\"},\"end\":95758,\"start\":95450},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":227143101},\"end\":96285,\"start\":95760},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5513850},\"end\":96584,\"start\":96287},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3751071},\"end\":97011,\"start\":96586},{\"attributes\":{\"doi\":\"arXiv:1909.11556\",\"id\":\"b18\"},\"end\":97293,\"start\":97013},{\"attributes\":{\"doi\":\"arXiv:1803.03635\",\"id\":\"b19\"},\"end\":97582,\"start\":97295},{\"attributes\":{\"doi\":\"arXiv:1903.01611\",\"id\":\"b20\"},\"end\":97896,\"start\":97584},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206799280},\"end\":98231,\"start\":97898},{\"attributes\":{\"doi\":\"arXiv:1902.09574\",\"id\":\"b22\"},\"end\":98477,\"start\":98233},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":212675598},\"end\":98911,\"start\":98479},{\"attributes\":{\"id\":\"b24\"},\"end\":99129,\"start\":98913},{\"attributes\":{\"doi\":\"arXiv:2002.08307\",\"id\":\"b25\"},\"end\":99457,\"start\":99131},{\"attributes\":{\"doi\":\"arXiv:1909.12486\",\"id\":\"b26\"},\"end\":99798,\"start\":99459},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1663491},\"end\":100226,\"start\":99800},{\"attributes\":{\"doi\":\"arXiv:1510.00149\",\"id\":\"b28\"},\"end\":100590,\"start\":100228},{\"attributes\":{\"doi\":\"arXiv:1506.02626\",\"id\":\"b29\"},\"end\":100900,\"start\":100592},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13740328},\"end\":101409,\"start\":100902},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206594692},\"end\":101837,\"start\":101411},{\"attributes\":{\"doi\":\"arXiv:1808.06866\",\"id\":\"b32\"},\"end\":102171,\"start\":101839},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52048008},\"end\":102651,\"start\":102173},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":20157893},\"end\":102981,\"start\":102653},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b35\"},\"end\":103186,\"start\":102983},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b36\"},\"end\":103438,\"start\":103188},{\"attributes\":{\"doi\":\"arXiv:1607.03250\",\"id\":\"b37\"},\"end\":103804,\"start\":103440},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":575794},\"end\":104195,\"start\":103806},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11169209},\"end\":104461,\"start\":104197},{\"attributes\":{\"id\":\"b40\"},\"end\":104666,\"start\":104463},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":220403644},\"end\":104980,\"start\":104668},{\"attributes\":{\"id\":\"b42\"},\"end\":105290,\"start\":104982},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6175301},\"end\":105645,\"start\":105292},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1779661},\"end\":105816,\"start\":105647},{\"attributes\":{\"doi\":\"arXiv:1810.02340\",\"id\":\"b45\"},\"end\":106132,\"start\":105818},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":220363814},\"end\":106495,\"start\":106134},{\"attributes\":{\"doi\":\"arXiv:1608.08710\",\"id\":\"b47\"},\"end\":106785,\"start\":106497},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":13756489},\"end\":107448,\"start\":106787},{\"attributes\":{\"doi\":\"arXiv:2002.07376\",\"id\":\"b49\"},\"end\":107744,\"start\":107450},{\"attributes\":{\"doi\":\"arXiv:1808.06601\",\"id\":\"b50\"},\"end\":108060,\"start\":107746},{\"attributes\":{\"doi\":\"arXiv:1608.03665\",\"id\":\"b51\"},\"end\":108352,\"start\":108062},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2166128},\"end\":108644,\"start\":108354},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":14424444},\"end\":108928,\"start\":108646},{\"attributes\":{\"doi\":\"arXiv:1505.00853\",\"id\":\"b54\"},\"end\":109230,\"start\":108930},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":208857831},\"end\":109829,\"start\":109232},{\"attributes\":{\"doi\":\"arXiv:1908.09979\",\"id\":\"b56\"},\"end\":110161,\"start\":109831},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":18842880},\"end\":110585,\"start\":110163},{\"attributes\":{\"doi\":\"arXiv:1909.08174\",\"id\":\"b58\"},\"end\":110965,\"start\":110587},{\"attributes\":{\"id\":\"b59\"},\"end\":111209,\"start\":110967},{\"attributes\":{\"doi\":\"arXiv:1908.11468\",\"id\":\"b60\"},\"end\":111482,\"start\":111211},{\"attributes\":{\"doi\":\"arXiv:2002.10597\",\"id\":\"b61\"},\"end\":111758,\"start\":111484},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":4766599},\"end\":112301,\"start\":111760},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":4752389},\"end\":112875,\"start\":112303},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":85529857},\"end\":113280,\"start\":112877},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":227276228},\"end\":113681,\"start\":113282}]", "bib_title": "[{\"end\":90996,\"start\":90949},{\"end\":91296,\"start\":91259},{\"end\":92231,\"start\":92214},{\"end\":94319,\"start\":94221},{\"end\":94975,\"start\":94925},{\"end\":95830,\"start\":95760},{\"end\":96355,\"start\":96287},{\"end\":96663,\"start\":96586},{\"end\":97978,\"start\":97898},{\"end\":98541,\"start\":98479},{\"end\":99865,\"start\":99800},{\"end\":100993,\"start\":100902},{\"end\":101455,\"start\":101411},{\"end\":102241,\"start\":102173},{\"end\":102711,\"start\":102653},{\"end\":103869,\"start\":103806},{\"end\":104270,\"start\":104197},{\"end\":104731,\"start\":104668},{\"end\":105353,\"start\":105292},{\"end\":105660,\"start\":105647},{\"end\":106204,\"start\":106134},{\"end\":106812,\"start\":106787},{\"end\":108436,\"start\":108354},{\"end\":108719,\"start\":108646},{\"end\":109351,\"start\":109232},{\"end\":110194,\"start\":110163},{\"end\":111830,\"start\":111760},{\"end\":112394,\"start\":112303},{\"end\":112922,\"start\":112877},{\"end\":113343,\"start\":113282}]", "bib_author": "[{\"end\":91012,\"start\":90998},{\"end\":91031,\"start\":91012},{\"end\":91046,\"start\":91031},{\"end\":91067,\"start\":91046},{\"end\":91309,\"start\":91298},{\"end\":91324,\"start\":91309},{\"end\":91333,\"start\":91324},{\"end\":91348,\"start\":91333},{\"end\":91361,\"start\":91348},{\"end\":91378,\"start\":91361},{\"end\":91776,\"start\":91762},{\"end\":92002,\"start\":91988},{\"end\":92015,\"start\":92002},{\"end\":92025,\"start\":92015},{\"end\":92251,\"start\":92233},{\"end\":92265,\"start\":92251},{\"end\":92292,\"start\":92265},{\"end\":92791,\"start\":92778},{\"end\":92805,\"start\":92791},{\"end\":92823,\"start\":92805},{\"end\":93074,\"start\":93061},{\"end\":93087,\"start\":93074},{\"end\":93094,\"start\":93087},{\"end\":93107,\"start\":93094},{\"end\":93118,\"start\":93107},{\"end\":93128,\"start\":93118},{\"end\":93137,\"start\":93128},{\"end\":93149,\"start\":93137},{\"end\":93482,\"start\":93469},{\"end\":93493,\"start\":93482},{\"end\":93503,\"start\":93493},{\"end\":93691,\"start\":93679},{\"end\":93699,\"start\":93691},{\"end\":94002,\"start\":93992},{\"end\":94012,\"start\":94002},{\"end\":94022,\"start\":94012},{\"end\":94033,\"start\":94022},{\"end\":94336,\"start\":94321},{\"end\":94350,\"start\":94336},{\"end\":94372,\"start\":94350},{\"end\":94739,\"start\":94724},{\"end\":94754,\"start\":94739},{\"end\":94987,\"start\":94977},{\"end\":94997,\"start\":94987},{\"end\":95013,\"start\":94997},{\"end\":95024,\"start\":95013},{\"end\":95032,\"start\":95024},{\"end\":95044,\"start\":95032},{\"end\":95528,\"start\":95515},{\"end\":95542,\"start\":95528},{\"end\":95554,\"start\":95542},{\"end\":95568,\"start\":95554},{\"end\":95846,\"start\":95832},{\"end\":95861,\"start\":95846},{\"end\":95875,\"start\":95861},{\"end\":95883,\"start\":95875},{\"end\":95896,\"start\":95883},{\"end\":95908,\"start\":95896},{\"end\":95923,\"start\":95908},{\"end\":96369,\"start\":96357},{\"end\":96383,\"start\":96369},{\"end\":96682,\"start\":96665},{\"end\":96696,\"start\":96682},{\"end\":96711,\"start\":96696},{\"end\":97025,\"start\":97013},{\"end\":97040,\"start\":97025},{\"end\":97055,\"start\":97040},{\"end\":97313,\"start\":97295},{\"end\":97329,\"start\":97313},{\"end\":97602,\"start\":97584},{\"end\":97620,\"start\":97602},{\"end\":97631,\"start\":97620},{\"end\":97641,\"start\":97631},{\"end\":97654,\"start\":97641},{\"end\":97662,\"start\":97654},{\"end\":98000,\"start\":97980},{\"end\":98246,\"start\":98233},{\"end\":98259,\"start\":98246},{\"end\":98272,\"start\":98259},{\"end\":98554,\"start\":98543},{\"end\":98570,\"start\":98554},{\"end\":98585,\"start\":98570},{\"end\":98600,\"start\":98585},{\"end\":98612,\"start\":98600},{\"end\":98628,\"start\":98612},{\"end\":98633,\"start\":98628},{\"end\":98929,\"start\":98913},{\"end\":98944,\"start\":98929},{\"end\":98961,\"start\":98944},{\"end\":98976,\"start\":98961},{\"end\":99235,\"start\":99210},{\"end\":99249,\"start\":99235},{\"end\":99258,\"start\":99249},{\"end\":99547,\"start\":99528},{\"end\":99552,\"start\":99547},{\"end\":99562,\"start\":99552},{\"end\":99575,\"start\":99562},{\"end\":99587,\"start\":99575},{\"end\":99593,\"start\":99587},{\"end\":99877,\"start\":99867},{\"end\":99889,\"start\":99877},{\"end\":99900,\"start\":99889},{\"end\":99909,\"start\":99900},{\"end\":99925,\"start\":99909},{\"end\":99933,\"start\":99925},{\"end\":99953,\"start\":99933},{\"end\":99960,\"start\":99953},{\"end\":100238,\"start\":100228},{\"end\":100249,\"start\":100238},{\"end\":100266,\"start\":100249},{\"end\":100602,\"start\":100592},{\"end\":100613,\"start\":100602},{\"end\":100624,\"start\":100613},{\"end\":100641,\"start\":100624},{\"end\":101007,\"start\":100995},{\"end\":101022,\"start\":101007},{\"end\":101036,\"start\":101022},{\"end\":101046,\"start\":101036},{\"end\":101469,\"start\":101457},{\"end\":101484,\"start\":101469},{\"end\":101498,\"start\":101484},{\"end\":101508,\"start\":101498},{\"end\":101848,\"start\":101839},{\"end\":101863,\"start\":101848},{\"end\":101876,\"start\":101863},{\"end\":101887,\"start\":101876},{\"end\":101896,\"start\":101887},{\"end\":102253,\"start\":102243},{\"end\":102261,\"start\":102253},{\"end\":102274,\"start\":102261},{\"end\":102287,\"start\":102274},{\"end\":102298,\"start\":102287},{\"end\":102308,\"start\":102298},{\"end\":102723,\"start\":102713},{\"end\":102738,\"start\":102723},{\"end\":102748,\"start\":102738},{\"end\":103035,\"start\":103020},{\"end\":103049,\"start\":103035},{\"end\":103251,\"start\":103234},{\"end\":103266,\"start\":103251},{\"end\":103277,\"start\":103266},{\"end\":103453,\"start\":103440},{\"end\":103463,\"start\":103453},{\"end\":103476,\"start\":103463},{\"end\":103492,\"start\":103476},{\"end\":103884,\"start\":103871},{\"end\":103897,\"start\":103884},{\"end\":104297,\"start\":104272},{\"end\":104309,\"start\":104297},{\"end\":104507,\"start\":104493},{\"end\":104746,\"start\":104733},{\"end\":104759,\"start\":104746},{\"end\":105051,\"start\":105037},{\"end\":105061,\"start\":105051},{\"end\":105369,\"start\":105355},{\"end\":105382,\"start\":105369},{\"end\":105394,\"start\":105382},{\"end\":105405,\"start\":105394},{\"end\":105419,\"start\":105405},{\"end\":105674,\"start\":105662},{\"end\":105689,\"start\":105674},{\"end\":105706,\"start\":105689},{\"end\":105831,\"start\":105818},{\"end\":105856,\"start\":105831},{\"end\":105872,\"start\":105856},{\"end\":106217,\"start\":106206},{\"end\":106227,\"start\":106217},{\"end\":106237,\"start\":106227},{\"end\":106252,\"start\":106237},{\"end\":106507,\"start\":106499},{\"end\":106519,\"start\":106507},{\"end\":106536,\"start\":106519},{\"end\":106549,\"start\":106536},{\"end\":106566,\"start\":106549},{\"end\":106830,\"start\":106814},{\"end\":106844,\"start\":106830},{\"end\":106857,\"start\":106844},{\"end\":106874,\"start\":106857},{\"end\":106887,\"start\":106874},{\"end\":106902,\"start\":106887},{\"end\":106917,\"start\":106902},{\"end\":106935,\"start\":106917},{\"end\":107532,\"start\":107519},{\"end\":107547,\"start\":107532},{\"end\":107561,\"start\":107547},{\"end\":107764,\"start\":107748},{\"end\":107777,\"start\":107764},{\"end\":107790,\"start\":107777},{\"end\":107802,\"start\":107790},{\"end\":107814,\"start\":107802},{\"end\":107825,\"start\":107814},{\"end\":107842,\"start\":107825},{\"end\":108071,\"start\":108062},{\"end\":108084,\"start\":108071},{\"end\":108097,\"start\":108084},{\"end\":108109,\"start\":108097},{\"end\":108117,\"start\":108109},{\"end\":108448,\"start\":108438},{\"end\":108731,\"start\":108721},{\"end\":108743,\"start\":108731},{\"end\":109011,\"start\":109002},{\"end\":109024,\"start\":109011},{\"end\":109037,\"start\":109024},{\"end\":109044,\"start\":109037},{\"end\":109368,\"start\":109353},{\"end\":109381,\"start\":109368},{\"end\":109392,\"start\":109381},{\"end\":109400,\"start\":109392},{\"end\":109943,\"start\":109929},{\"end\":109952,\"start\":109943},{\"end\":109960,\"start\":109952},{\"end\":110209,\"start\":110196},{\"end\":110221,\"start\":110209},{\"end\":110233,\"start\":110221},{\"end\":110248,\"start\":110233},{\"end\":110699,\"start\":110685},{\"end\":110708,\"start\":110699},{\"end\":110720,\"start\":110708},{\"end\":110729,\"start\":110720},{\"end\":110740,\"start\":110729},{\"end\":111074,\"start\":111059},{\"end\":111301,\"start\":111288},{\"end\":111311,\"start\":111301},{\"end\":111501,\"start\":111484},{\"end\":111514,\"start\":111501},{\"end\":111525,\"start\":111514},{\"end\":111535,\"start\":111525},{\"end\":111847,\"start\":111832},{\"end\":111862,\"start\":111847},{\"end\":111878,\"start\":111862},{\"end\":111893,\"start\":111878},{\"end\":111906,\"start\":111893},{\"end\":112411,\"start\":112396},{\"end\":112423,\"start\":112411},{\"end\":112436,\"start\":112423},{\"end\":112447,\"start\":112436},{\"end\":112458,\"start\":112447},{\"end\":112472,\"start\":112458},{\"end\":112485,\"start\":112472},{\"end\":112936,\"start\":112924},{\"end\":112946,\"start\":112936},{\"end\":112960,\"start\":112946},{\"end\":112969,\"start\":112960},{\"end\":113357,\"start\":113345},{\"end\":113372,\"start\":113357},{\"end\":113386,\"start\":113372},{\"end\":113399,\"start\":113386},{\"end\":113411,\"start\":113399},{\"end\":113421,\"start\":113411}]", "bib_venue": "[{\"end\":91527,\"start\":91461},{\"end\":92475,\"start\":92392},{\"end\":96044,\"start\":95992},{\"end\":101167,\"start\":101115},{\"end\":101649,\"start\":101587},{\"end\":102423,\"start\":102374},{\"end\":104012,\"start\":103963},{\"end\":109549,\"start\":109483},{\"end\":110391,\"start\":110328},{\"end\":112047,\"start\":111985},{\"end\":112600,\"start\":112551},{\"end\":113090,\"start\":113038},{\"end\":91086,\"start\":91067},{\"end\":91459,\"start\":91378},{\"end\":91986,\"start\":91885},{\"end\":92390,\"start\":92292},{\"end\":92776,\"start\":92682},{\"end\":93059,\"start\":92975},{\"end\":93467,\"start\":93397},{\"end\":93778,\"start\":93715},{\"end\":93990,\"start\":93919},{\"end\":94421,\"start\":94372},{\"end\":94722,\"start\":94656},{\"end\":95107,\"start\":95044},{\"end\":95373,\"start\":95336},{\"end\":95513,\"start\":95450},{\"end\":95990,\"start\":95923},{\"end\":96419,\"start\":96383},{\"end\":96777,\"start\":96711},{\"end\":97131,\"start\":97071},{\"end\":97417,\"start\":97345},{\"end\":97719,\"start\":97678},{\"end\":98052,\"start\":98000},{\"end\":98333,\"start\":98288},{\"end\":98671,\"start\":98633},{\"end\":98989,\"start\":98976},{\"end\":99208,\"start\":99131},{\"end\":99526,\"start\":99459},{\"end\":99998,\"start\":99960},{\"end\":100386,\"start\":100282},{\"end\":100724,\"start\":100657},{\"end\":101113,\"start\":101046},{\"end\":101585,\"start\":101508},{\"end\":101983,\"start\":101912},{\"end\":102372,\"start\":102308},{\"end\":102807,\"start\":102748},{\"end\":103018,\"start\":102983},{\"end\":103232,\"start\":103188},{\"end\":103600,\"start\":103508},{\"end\":103961,\"start\":103897},{\"end\":104313,\"start\":104309},{\"end\":104491,\"start\":104463},{\"end\":104803,\"start\":104759},{\"end\":105035,\"start\":104982},{\"end\":105453,\"start\":105419},{\"end\":105712,\"start\":105706},{\"end\":105953,\"start\":105888},{\"end\":106290,\"start\":106252},{\"end\":106984,\"start\":106935},{\"end\":107517,\"start\":107450},{\"end\":108185,\"start\":108133},{\"end\":108484,\"start\":108448},{\"end\":108771,\"start\":108743},{\"end\":109000,\"start\":108930},{\"end\":109481,\"start\":109400},{\"end\":109927,\"start\":109831},{\"end\":110326,\"start\":110248},{\"end\":110683,\"start\":110587},{\"end\":111057,\"start\":110967},{\"end\":111286,\"start\":111211},{\"end\":111599,\"start\":111551},{\"end\":111983,\"start\":111906},{\"end\":112549,\"start\":112485},{\"end\":113036,\"start\":112969},{\"end\":113470,\"start\":113421}]"}}}, "year": 2023, "month": 12, "day": 17}