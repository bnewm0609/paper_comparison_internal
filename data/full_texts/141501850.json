{"id": 141501850, "updated": "2023-10-07 05:47:37.379", "metadata": {"title": "3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation", "authors": "[{\"first\":\"Stylianos\",\"last\":\"Moschoglou\",\"middle\":[]},{\"first\":\"Stylianos\",\"last\":\"Ploumpis\",\"middle\":[]},{\"first\":\"Mihalis\",\"last\":\"Nicolaou\",\"middle\":[]},{\"first\":\"Stefanos\",\"last\":\"Zafeiriou\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 5, "day": 1}, "abstract": "Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face - such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1905.00307", "mag": "3020995211", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijcv/MoschoglouPNPZ20", "doi": "10.1007/s11263-020-01329-8"}}, "content": {"source": {"pdf_hash": "bd995b1a8a71f8e820046c6ec72d380e4f8f91b3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1905.00307v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://link.springer.com/content/pdf/10.1007/s11263-020-01329-8.pdf", "status": "HYBRID"}}, "grobid": {"id": "5fc240829d1a43a0fedada8c6e383439c4f049bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bd995b1a8a71f8e820046c6ec72d380e4f8f91b3.txt", "contents": "\n3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation\n\n\nStylianos Moschoglou s.moschoglou@imperial.ac.uk \nStylianos Ploumpis s.ploumpis@imperial.ac.uk \n\u00b7 Mihalis \nNicolaou \u00b7 Stefanos m.nicolaou@cyi.ac.cy \nZafeiriou s.zafeiriou@imperial.ac.uk \n\n1 Introduction\n\n\n3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation\nReceived: date / Accepted: dateNoname manuscript No. (will be inserted by the editor) * The authors contributed equally to this work. S. Moschoglou, S. Ploumpis, and S. Zafeiriou are with the De-partment of Computing, Imperial College London, United Kingdom and Facesoft.io.\nOver the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D facesuch as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.\n\nIntroduction\n\nGANs are a promising unsupervised machine learning methodology implemented by a system of two deep neural networks competing against each other in a zero-sum game framework (Goodfellow et al. 2014). GANs became immediately very popular due to their unprecedented capability in terms of implicitly modeling the distribution of visual data, thus being able to generate and synthesize novel yet realistic images and videos, by preserving high-frequency details of the data distribution and hence appearing authentic to human observers. Many different GAN architectures have been proposed over the past few years, such as the Deep Convolutional GAN (DCGAN) (Radford et al. 2015) and the Progressive GAN (PGAN) (Karras et al. 2018), which was the first to show impressive results in generation of highresolution images.\n\nA type of GANs which has also been extensively studied in the literature is the so-called Conditional GAN (CGAN) (Mirza and Osindero 2014), where the inputs of the generator as well as the discriminator are conditioned on the class labels. Applications of CGANs include domain transfer Bousmalis et al. 2017;Tzeng et al. 2017), image completion Yang et al. 2017;Wang et al. 2017), image super-resolution (Nguyen et al. 2018;Johnson et al. 2016;Ledig et al. 2017) and image translation Zhu et al. 2017;Choi et al. 2017;Wang et al. 2018).\n\nDespite the great success GANs have had in 2D image/video generation, representation, and translation, no GAN method tailored towards tackling the aforementioned tasks in 3D shapes has been introduced in the literature. This is primarily attributed to the lack of appropriate decoder networks for meshes that are able to retain the high frequency details (Dosovitskiy and Brox 2016;Jackson et al. 2017 (x,y,z) to (R,G,B) Fig. 1 A graphical representation of the data preprocessing step. We begin by applying non-rigidly a mesh template to the raw scan and we later store the spatial information of the vertices (x, y, z) into a UV space. Lastly, a 2D nearest point interpolation is performed to fill out the missing values.\n\n\nHigh quality ground truth\n\nHigh quality 3DFaceGAN output Low quality Input Fig. 2 Results of 3DFaceGAN in the shape translation task on test data of the proposed Hi-Lo database. The first row of shapes shows the low quality facial meshes captured by a low cost sensor, whereas the bottom row depicts the same subjects captured in high quality by an expensive high-end apparatus. The middle row shows our shape translation output results when the network takes as inputs the low quality 3D facial scans.\n\nIn this paper, we study the task of representation, generation, and translation of 3D facial surfaces using GANs. Examples of the applications of 3DFaceGAN in the tasks of 3D face translation as well as 3D face representation and generation are presented in Fig. 2 and Fig. 3, respectively. Due to the fact that (a) the use of volumetric representation leads to very low-quality representation of faces (Fan et al. 2017;Qi et al. 2017), and (b) the current geometric deep learning approaches (Bronstein et al. 2017), and especially spectral convolution, preserve only the low-frequency details of the 3D faces, we study approaches that use 2D convolutions in a UV unwrapping of the 3D face. The process of unwrapping a 3D face in the UV domain is shown in Fig. 1. Overall, the contributions of this work can be summarized as follows.\n\n-We introduce a novel autoencoder-like network architecture for GANs, which achieves state-of-the-art results in tasks such as 3D face representation, generation, and translation. -We introduce a novel training framework for GANs, especially tailored for 3D facial data.\n\n-We introduce a novel process for generating realistic 3D facial data, retaining the high frequency details of the 3D face.\n\nThe rest of the paper is structured as follows. In Section 2, we succinctly present the various methodologies that can be utilized in order to feed 3D facial data into a deep network and argue why the UV unwrapping of the 3D face was the method of choice. In Section 3, we present all the details with respect to 3DFaceGAN training process, losses, and model architectures. Finally, in Section 4, we provide information about the database we collected, the preprocessing we carried out in the databases we utilized for the experiments and lastly we present extensive quantitative and qualitative experiments of 3DFaceGAN against other stateof-the-art deep networks.\n\n\n3D face representations for deep nets\n\nThe most natural representation of a 3D face is through a 3D mesh. Adopting a 3D mesh representation requires appli-\n\n\nReal x1\n\nRecon x1 Real x2 Recon x2 Interpolation in the latent space\nReal 3D faces Reconstructed 3D faces (a) (b)\nFig. 3 3D face representation and generation utilizing the proposed 3DFaceGAN. In (a) we demonstrate the 3D face representation capability of 3DFaceGAN. The first row shows the reconstructed 3D faces whereas the second row shows the corresponding real 3D faces. As evidenced, 3DFaceGAN is able to capture and reconstruct non-linear details of the 3D face such as lips, eyelids, etc. In (b) we present the generative nature of 3DFaceGAN. The left and right hand side show the real 3D face targets. The generated samples in between show the reconstructions and the interpolations of the targets in the latent space.\n\ncation of mesh convolutions defined on non-Euclidean domains (i.e., geometric deep learning methodologies 1 ). Over the past few years, the field of geometric deep learning has received significant attention (Maron et al. 2017;Litany et al. 2017b;Lei et al. 2017). Methods relevant to this paper are auto-encoder structures such as Ranjan et al. (2018); Litany et al. (2017a). Nevertheless, such auto-encoders, due to the type of convolutions applied, mainly preserve lowfrequency details of the meshes. Furthermore, architectures that could potentially preserve high-frequency details, such as skip connections, have not yet been attempted in geometric deep learning. Therefore, geometric deep learning methods are not yet suitable for the problem we study in this paper.\n\nAnother way to work with 3D meshes is to concatenate the coordinates of the 3D points in an 1D vector and utilize fully connected layers to decode correctly the structure of the point cloud (Fan et al. 2017;Qi et al. 2017). Nevertheless, in this way the triangulation and spatial adjacent information is lost and the number of the parameters describing this formulation is extremely large which makes the network hard to train.\n\nRecently, many approaches aim at regressing directly on the latent parameters of a learned model space, e.g., PCA, rather than the 3D coordinates of points (Richardson et al. 2017;Tran et al. 2017;Dou et al. 2017;Genova et al. 2018). This formulation limits the geometrical details of the 3D representations and is restricted to their latent model space. In contrast, a 3D volumetric space is introduced in Jackson et al. (2017) as a representation of a 3D structure and exploits a Volumetric Regression Network which outputs a discretized version of the 3D structure. Due to discretization, the predicted 3D shape has low quality and corresponds to non-surface points that are difficult to handle.\n\nLastly, in Feng et al. (2018), a UV spatial map framework is utilized where the 3D coordinates of the points are stored in a UV space instead of the texture values of the mesh. This formulation exhibits a very good representation for 3D meshes where there are no overlapping regions and the mesh is optimally unwrapped. Since the 3D mesh is transferred in a 2D UV domain, we are then able to use 2D convolutions, with the whole range of capabilities they offer. As a result, this is our preferred methodology for preprocessing the 3D face scans, as further explained in Section 4.2.\n\n\nPre-training the Discriminator\n\nTraining 3DFaceGAN Fig. 4 3DFaceGAN training process in a nutshell. The networks receive (extract) 2D facial UVs as inputs (outputs). The corresponding 3D faces are shown below or next to them. We firstly pre-train D (left figure). We then use the learned weights/biases to initialize D and G and subsequently start the adversarial training (right figure). The decoder parts of D and G are depicted in red color as we freeze the weights/biases updates during the training phase of 3DFaceGAN.\nD G G(x) x x D(G(x)) D(x) D x D(x)\n\n3DFaceGAN\n\nIn this Section we describe the training process, network architectures, and loss functions we utilized for 3DFaceGAN. Moreover, we discuss the framework we utilized for 3D face generation as well as present an extension of 3DFaceGAN which is able to handle data annotated with multiple labels.\n\n\nObjective function\n\nThe main objective of the generator G is to retrieve a facial UV map x as input and generate a fake one, G (x), which in turn should be as close as possible to the real target facial UV map y. For example, in the case of 3D face translation, the input can be a neutral face and the output a certain expression (e.g., happiness) or in the case of 3D face reconstruction the input can be a 3D facial UV map and the output a reconstruction of the particular 3D facial UV map. The goal of the discriminator D is to distinguish between the real (y) and fake (G (x)) facial UV maps. Throughout the training process, D and G compete against each other until they reach an equilibrium, i.e., until D can no longer differentiate between the fake and the real facial UV maps.\n\nAdversarial loss. To achieve the 3DFaceGAN objective, we propose to utilize the following loss for the adversarial part. That is,\nL D = E y [L (y)] \u2212 \u03bb adv \u00b7 E x [L (G(x))] , L G = E x [L (G(x))] ,(1)\nwhere D (\u00b7) refers to the output of the discriminator D, L (x) . = x \u2212 D(x) 1 , and \u03bb adv is the hyper-parameter which controls how much weight should be put on L (G(x)). The higher the \u03bb adv , the more emphasis D puts on the task of differentiating between the real and fake data. The lower the \u03bb adv , the more emphasis D puts on reconstructing the actual real data. There is a fine line between which task D should primarily focus on by adjusting \u03bb adv . In our experiments we deduced that for relatively low values of \u03bb adv we retrieve optimal performance as then D is able to influence the updates of G in such a way that the generated facial UV maps are more realistic. During the adversarial training, D tries to minimize L D whereas G tries to minimize L G . Similar to recent works such as Zhao et al. (2016); Berthelot et al. (2017), the discriminator D has the structure of an autoencoder. Nevertheless, the main differences are that (a) we do not make use of the margin m as in Zhao et al. (2016) or the equilibrium constraint as in Berthelot et al. (2017), and (b) we use the autoencoder structure of the discriminator and pre-train it with the real UV targets prior to the adversarial training. Further details about the training procedure are presented in Section 3.2. Reconstruction loss. With the utilization of the adversarial loss (1), the generator G is trying to \"fool\" the discriminator D. Nevertheless, this does not guarantee that the fake facial UV will be close to the corresponding real, target one. To impose this, we use an L1 loss between the fake sample G (x) and the corresponding real one, y, so that they are as similar as possible, as in Isola et al. (2017). Namely, the reconstruction loss is the following.\nL rec = E x G (x) \u2212 y 1 .\n(2)\n\nFull objective. In sum, taking into account (1) and (2), the full objective becomes\nL D = E y [L (y)] \u2212 \u03bb adv \u00b7 E x [L (G(x))] , L G = E x [L (G(x))] + \u03bb rec \u00b7 L rec ,(3)\nwhere \u03bb rec is the hyper-parameter that controls how much emphasis should be put on the reconstruction loss. Overall, the discriminator D tries to minimize L D while the generator G tries to minimize L G .\n\n\nTraining procedure\n\nIn this Section, we first describe how we pre-train the discriminator (autoencoder) D and then provide details with respect to the adversarial training of 3DFaceGAN.\n\nPre-training the discriminator. The majority of GANs in the literature utilize discriminator architectures with logit outputs that correspond to a prediction on whether the input fed into the discriminator is real or fake. Recently proposed GAN variations have nevertheless taken a different approach, namely by utilizing autoencoder structures as discriminators (Zhao et al. 2016;Berthelot et al. 2017). Using an autoencoder structure in the discriminator D is of paramount importance in the proposed 3DFaceGAN. The benefit is twofold: (a) we can pre-train the autoencoder D acting as discriminator prior to the adversarial training, which leads to better quantitative as well as more compelling visual results 2 , and (b) we are able to compute the actual UV space dense loss, as compared to simply deciding on whether the input is real or fake. As we empirically show in our experiments and ablation studies, this approach encourages the generator to produce more realistic results than other, state-of-the-art methodologies. Adversarial training. Before starting the adversarial training, we initialize the weights and biases 3 for both the generator G and the discriminator D utilizing the learned parameters estimated after the pre-training of D (the architecture of G is identical to the architecture of D). During the training phase of 3DFaceGAN, we freeze the parameter updates in the decoder parts for both the generator G and the discriminator D. Furthermore, we utilize a low learning rate on the encoder and bottleneck parts of G and D so that overall the parameter updates are relatively close to the ones found during the pre-training of D. Network architectures. The network architectures for both the discriminator D and the generator G are the same. In particular, each network is consisted of 2D convolutional blocks with kernel size of three, stride and padding size of one. Down-sampling is achieved by average 2D pooling with kernel and stride size of two. The convolution filters grow linearly in each down-sampling step. Up-sampling is implemented by nearest-neighbor with scale factor of two.\n\nThe activation function that is primarily used is ELU (Clevert et al. 2015), apart from the last layer of both D and G where Tanh is utilized instead. At the bottleneck we utilize fully connected layers and thus project the tensors to a latent vector b \u2208 R N b . To generate more compelling visual results, we utilized skip connections (He et al. 2016;Huang et al. 2017) in the first layers of the decoder part of both the generator and the discriminator. Further details about the network architectures are provided in Table 1.\n\n\n3D face generation\n\nVariational autoencoders (VAEs) (Kingma and Welling 2013) are widely used for generating new data using autoencoder-like structures. In this setting, VAEs add a constraint on the latent embeddings of the autoencoders that forces them to roughly follow a normal distribution. We can then generate new data by sampling a latent embedding from the normal distribution and pass it to the decoder. Nevertheless, it was empirically shown that enforcing the embeddings in the training process to follow a normal distribution leads to generators that are unable to capture high frequency details (Litany et al. 2017a). To alleviate this, we propose to generate data using Algorithm 1, which better retains the generated data fidelity, as shown in Section 4.\n\n\n3DFaceGAN for multi-label 3D data\n\nOver the last few years, databases annotated with regards to multiple labels are becoming available in the scientific community. For instance, 4DFAB (Cheng et al. 2018) is a publicly available 3D facial database containing data annotated with respect to multiple expressions. We can extend 3DFaceGAN to handle data annotated with regards to multiple labels as follows. Without any loss of generality, suppose there are three labels in the database (e.g., expressions neutral, happiness and surprise). We adopt the so-called one-hot representation and thus denote the existence of a particular label in a datum by 1 and the absence by 0. For example, a 3D face datum annotated with the label happiness will have the following label representation: l = [0, 1, 0], where the first entry corresponds to the label neutral, the second to the label happiness and the third to the label surprise. We then choose the desired l we want to generate (e.g., if we want to translate a neutral face to a surprised one, we would choose l = [0, 0, 1]) and then spatially replicate it and concatenate it in the input that is then fed to the generator. The real target is the actual expression (in this case surprise) with the corresponding l spatially replicated and concatenated. Apart from this change, the rest of the training process is exactly the same as the one described in Section 3.2. Table 1 Generator/Discriminator network architectures of 3DFaceGAN. As far as the notation is concerned, C denotes the number of input/output channels, K denotes the kernel size, S denotes the stride size, P denotes the padding size, AvgPool2D denotes average 2D pooling, UpNN denotes nearest-neighbor upsampling, and SF refers to the scaling factor size of the nearest-neighbor upsampling. CONV-BLOCK(C1, C2, K, S, P) and DECONV-BLOCK(C1, C2, K, S, P) refer to a block of two convolutions where the first is CONV(C1, C2, K, S, P) followed by an ELU (Clevert et al. 2015) activation function and the second is CONV(C2, C2, K, S, P), also followed by an ELU (Clevert et al. 2015) activation function.\n\n\nPart\n\nInput \u2192 Output shape Layer information\nEncoder (h, w, 3) \u2192 (h, w, n) CONV-(Cn, K3x3, S1, P1), ELU (h, w, n) \u2192 ( h 2 , w 2 , 2n) CONV-BLOCK-(Cn, 2n, K3x3, S1, P1), AvgPool2D(K2x2, S2) ( h 2 , w 2 , 2n) \u2192 ( h 4 , w 4 , 3n) CONV-BLOCK-(C2n, C3n, K3x3, S1, P1), AvgPool2D(K2x2, S2) ( h 4 , w 4 , 3n) \u2192 ( h 8 , w 8 , 4n) CONV-BLOCK-(C3n, C4n, K3x3, S1, P1), AvgPool2D(K2x2, S2) ( h 8 , w 8 , 4n) \u2192 ( h 16 , w 16 , 5n) CONV-BLOCK-(C4n, C5n, K3x3, S1, P1), AvgPool2D(K2x2, S2) ( h 16 , w 16 , 5n) \u2192 ( h 32 , w 32 , 6n) CONV-BLOCK-(C5n, C6n, K3x3, S1, P1), AvgPool2D(K2x2, S2) ( h 32 , w 32 , 6n) \u2192 ( h 32 , w 32 , 6n) CONV-BLOCK-(C6n, C6n, K3x3, S1, P1) Bottleneck 1 ( h 32 \u00d7 w 32 \u00d76n) \u2192 n Fully connected Bottleneck 2 n \u2192 ( h 32 \u00d7 w 32 \u00d7n) Fully connected Decoder ( h 32 , w 32 , n) \u2192 ( h 16 , w 16 , n) DECONV-BLOCK(Cn, Cn K3x3, S1, P1), UpNN(SF2) ( h 16 , w 16 , n) \u2192 ( h 8 , w 8 , n) DECONV-BLOCK(Cn, Cn, K3x3, S1, P1), UpNN(SF2) ( h 8 , w 8 , n) \u2192 ( h 4 , w 4 , n) DECONV-BLOCK(Cn, Cn, K3x3, S1, P1), UpNN(SF2) ( h 8 , w 8 , n) \u2192 ( h 4 , w 4 , n) DECONV-BLOCK(Cn, Cn, K3x3, S1, P1), UpNN(SF2) ( h 2 , w 2 , n) \u2192 (h, w, n) DECONV-BLOCK(Cn, Cn, K3x3, S1, P1), UpNN(SF2) (h, w, n) \u2192 (h, w, n) DECONV-BLOCK(Cn, Cn, K3x3, S1, P1) (h, w, n) \u2192 (h, w, 3) DECONV(Cn, C3, K3x3, S1, P1), Tanh\nFinally, to generate 3D facial data with respect to a particular label, we follow the same process as the one presented in Algorithm 1, with the only difference being that we extract different pairs of (\u00b5 Z , \u03a3 Z ) for every subset of the data, each corresponding to a particular label in the database. We then choose the pair (\u00b5 Z , \u03a3 Z ) corresponding to the desired label and sample from this multi-variate Gaussian distribution.\n\n\nExperiments\n\nIn this Section we (a) describe the databases which we used to carry out the experiments utilizing 3DFaceGAN, (b) provide information with respect to the data preprocessing we conducted prior to feeding the 3D data into the network, (c) succinctly describe the baseline state-of-the-art algorithms we employed for comparisons and (d) provide quantitative as well as qualitative results on a series of experiments that demonstrate the superiority of 3DFaceGAN.\n\n\nDatabases\n\n\nThe Hi-Lo database\n\nHi-Lo database contains approximately 6, 000 3D facial scans captured during a special exhibition in the Science Museum, London. It is divided into the high quality data (Hi) recorded with a 3dMD face capturing system and the low quality (Lo) data captured with a V1 Kinect sensor. All the subjects were recorded in neutral expression. The overlapping subjects that were recorded in both frameworks were approximately 3, 000.\n\nThe 3dMD apparatus utilizes a 4 camera structured light stereo system which can create 3D triangular surface meshes composed of approximately 60, 000 vertices joined into approximately 120, 000 triangles. Moreover, the low quality database was captured with a KinectFusion framework (Newcombe et al. 2011). In contrast to the 3dMD system, multiple frames are required to build a single 3D representation of the subject's face. The fused meshes were built by employing a 6, 083 voxel grid. In order to accurately reconstruct the entire surface of the faces, a circular motion scanning pattern was carried out. Each subject was instructed to stay still in a fixed pose during the entire scanning process Algorithm 1: 3D face generation algorithm.\n\nStep 1: Train 3DFaceGAN utilizing (3).\n\nStep 2: Extract the trained G, and for all N training facial UV maps:\nfor i = 1 : N do Input UV map x i in G. Extract the corresponding bottleneck z i \u2208 R N b \u00d71 . end\nStep 3: Concatenate column-wise all of the bottlenecks, i.e., Z = [z 1 , z 2 , . . . , z N ].\n\nStep 4: Extract the mean \u00b5 Z of Z and the covariance \u03a3 Z of the zero-mean Z.\n\nStep 5: To generate new data, retain only the trained Bottleneck 2 and the Decoder part of G (see Table 1 for the network structures) and sample a new z i (i.e., Bottleneck 2 input) from the multivariate Gaussian N (\u00b5 Z , \u03a3 Z ).\n\nwith a neutral facial expression. The frame rate for every subject was constant at 8 frames per second. Furthermore, all 3, 000 subjects provided metadata about themselves, including their gender, age, and ethnicity. The database covers a wide variety of age, gender (48% male, 52% female), and ethnicity (82% White, 9% Asian, 5% Mixed Heritage, 3% Black and 1% other).\n\nHi-Lo database was utilized for the experiments of 3D face representation and generation, where we utilized the high quality data to train 3DFaceGAN. Moreover, Hi-Lo database was used for demonstrating the capabilities of 3DFaceGAN in a 3D face translation setting, where the low quality data are translated into high quality ones. In all of the training tasks, 85% of the data were used for training and the rest were used for testing.\n\n\n4DFAB database\n\n4DFAB database (Cheng et al. 2018) contains 3D facial data from 180 subjects (60 females, 120 males), aged from 5 to 75 years old. The subjects vary in their ethnicity background, coming from more than 30 different ethnic groups. For the capturing process, the DI4D dynamic capturing system 4 was used.\n\n4DFAB (Cheng et al. 2018) contains data varying in expressions, such as neutral, happiness, and surprise. As a result, we utilized it to showcase 3DFaceGAN's capability in successfully handling data annotated with multiple labels in the task of 3D face translation as well as generation. In all of the training tasks, 85% of the data were used for training and the rest were used for testing.\n\n\nData preprocessing\n\nIn order to feed the 3D data into a deep network several steps need to be carried out. Since we employ various databases, the representation of the facial topology is not consistent in terms of vertex number and triangulation. To this end, we need to find a suitable template T that can easily retain the information of all raw scans across all databases and describe them with the same triangulation/topology. We utilized the mean face mesh of the LSFM model proposed by Booth et al. (2016), which consists of approximately 54, 000 vertices that are sufficient to capture high frequency facial details. We then bring the raw scans in dense correspondence by morphing non-rigidly the template mesh to each one of them. For this task, we utilize an optimal-step Non-rigid Iterative Closest Point algorithm (De Smet and Van Gool 2010) in combination with a per vertex weighting scheme. We weight the vertices according to the Euclidean distance measured from the tip of the nose. The greater the distance from the nose tip, the bigger the weight that is assigned to that vertex, i.e., less flexible to deform. In that way we are able to avoid the noisy information recorded by the scanners on the outer regions of the raw scans.\n\nFollowing the analysis of the various methods of feeding 3D meshes in deep networks in Section 2, we chose to describe the 3D shapes in the UV domain. UV maps are usually utilized to store texture information. In our case, we store the spatial location of each vertex as an RGB value in the UV space. In order to acquire the UV pixel coordinates for each vertex, we start by unwrapping our mesh template T into a 2D flat space by utilizing an optimal cylindrical unwrapping technique proposed by Booth and Zafeiriou (2014). Before storing the 3D coordinates into the UV space, all meshes are aligned in the 3D spaces by performing the General Procrustes Analysis (Gower 1975) and are normalized to be in the scale of [1, \u22121]. Afterwards, we place each 3D vertex in the image plane given the respective UV pixel coordinate. Finally, after storing the original vertex coordinates, we perform a 2D nearest point interpolation in the UV domain to fill out the missing areas in order to produce a dense representation of the originally sparse UV map. Since the number of vertices in S T is more than 50K, we choose a 256 \u00d7 256 \u00d7 3 tensor as the UV map size, which assists in retrieving a high precision point cloud with negligible resampling errors. A graphical representation of the preprocessing pipeline can be seen in Figure 1.  Fig. 6 Qualitative results of 3DFaceGAN compared to CoMA (Ranjan et al. 2018) in the 3D representation task. Moreover, heatmaps are provided, visualizing the errors of both approaches against the ground truth test data. As evidenced, 3DFaceGAN is able to better capture the variation in the test data, especially in the eye and nose regions, where most of the non-linearities are present.\n(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)\n\nTraining\n\nWe trained all 3DFaceGAN models utilizing Adam (Kingma and Ba 2014) with \u03b2 1 = 0.5 and \u03b2 2 = 0.999. The batch size we used for the pre-training of the discrminator was 32 for a total of 300 epochs. The batch size we used for 3DFaceGAN was 16 for a total of 300 epochs. For our model we used n = 128 convolution filters and a bottleneck of size b = 128. The total number of trainable parameters was 38.5 \u00d7 10 6 . The learning rates that we used for both the pre-training and training of the discriminator was 5e \u2212 5 and the same was for the training of the generator. We linearly decayed the learning rate by 5% every 30 epochs during training. For the rest of the parameters, we  used \u03bb adv = 1e \u2212 3, \u03bb rec = 1. Overall training time on a GV100 NVIDIA GPU was about 5 days.\n\n\n3D Face Representation\n\nIn the 3D face representation (reconstruction) experiments, we utilize the high quality 3D face data from the Hi-Lo database to train the algorithms. In particular, we feed the high quality 3D data as inputs to the models and use the same data as target outputs. Before providing the qualitative as well as quantitative results, we briefly describe the baseline models we compared against as well as provide information about the error metric we used for the quantitative assessment.\n\n\nBaseline models\n\nIn this Section we briefly describe the state-of-the-art models we utilized to compare 3DFaceGAN against.\n\n\nVanilla Autoencoder (AE)\n\nVanilla Autoencoder follows exactly the same structure of the discriminator we used in 3DFaceGAN. We used the same values for the hyper-parameters and the same optimization process. This is the main baseline we compared against and the results are provided in the ablation study in Section 4.4.3.\n\n\nConvolutional Mesh Autoencoder (CoMA)\n\nIn order to train CoMA (Ranjan et al. 2018), we use the authors' publicly available implementation and utilize the default parameter values, the only difference being that the bottleneck size is 128, to make a fair comparison against 3DFaceGAN, where we also used a bottleneck size of 128.\n\n\nPrincipal Component Analysis (PCA)\n\nWe employ and train a standard PCA model (Jolliffe 2011) based on the meshes of our database we used for training. We aimed at retaining the 98% of variance of our available training data which corresponds to the first 50 principal components.\n\n\nProgressive GAN (PGAN)\n\nIn order to train PGAN (Karras et al. 2018), we used the authors' publicly available implementation with the default parameter values. After the training is complete, in order to represent a test 3D datum, we invert the generator G as in Lucic et al. (2018) and Mahendran and Vedaldi (2015), i.e., we solve z * = argmin x \u2212 G(z) by applying gradient descent on z while retaining G fixed (Mahendran and Vedaldi 2015).\n\n\nError metric\n\nA common practice when it comes to evaluating statistical shape models is to estimate the intrinsic characteristics, such as the generalization of the model (Davies et al. 2008).\n\nThe generalization metric captures the ability of a model to Low Quality Scan High Quality Scan 3DFaceGAN pix2pix pix2pixHD pix2pixHD smoothed pix2pix smoothed Fig. 8 The qualitative results of our approach compared to state-of-the-art baseline GAN methods in the 3D face translation task. The first column depicts the low quality input mesh whereas the second column represent the high quality ground truth meshes. We depict the raw results of pix2pixHD ) and pix2pix ) along with their smoothed versions. As a smoothing technique we utilized a standard Laplacian smoothing operator.\n\nrepresent unseen 3D face shapes during the testing phase. Table 2 presents the generalization metric for 3DFaceGAN compared against the baseline models. In order to compute the generalization error for a given model, we compute the per-vertex Euclidean distance between every sample of the test set and its corresponding reconstruction. We observe that the model which holds the best error results and thus demonstrates greater generalization capabilities is the proposed 3DFaceGAN with mean error 0.0031 and standard deviation 0.0028. Additionally, as shown in Fig. 5a, which depicts the cumulative error distribution of the normalized dense vertex erors, 3DFaceGAN outperforms all of the baseline models.   \n(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)\n\nAblation study\n\nIn this ablation study we investigate the importance of pretraining the discriminator D prior to the adversarial training of 3DFaceGAN as well as the freezing of the weights in the decoder parts of both D and G. More specifically, we compare 3DFaceGAN against the Vanilla Autoencoder (AE) and another two 3DFaceGAN possible variations, namely (a) the simplest case, where the discriminator and generator structures are retained as is, but no pre-training takes place prior to the adversarial training (we refer to this method-ology as 3DFaceGAN V2 ), (b) the case where (i) the discriminator and generator structures are retained as is, (ii) we pre-train the discriminator and initialize both the generator and the discriminator with the learned weights with no parameters frozen during the adversarial training (we refer to this methodology as 3DFaceGAN V3 ). As shown in Fig. 5b and Table 5, 3DFaceGAN outperforms Vanilla AE and 3DFaceGAN V2 by a large margin. Moreover, 3DFace-GAN also outperforms 3DFaceGAN V3. As a result, not only does 3DFaceGAN have the best performance among the compared 3DFaceGAN variants, but it also requires less training time compared to 3DFaceGAN V3, as the parameters in the decoder parts of both the generator and the discriminator are not updated during the training phase and thus need not be computed.\n\n\n3D Face Translation\n\nIn the 3D face translation experiments, we utilize the low and high quality 3D face data from the Hi-Lo database to train the algorithms. In particular, we feed the low quality 3D data as inputs to the models and use the high quality data as target outputs. Before providing the qualitative as well as quantitative results, we briefly describe the baseline models we compared against as well as provide information about the error metric we used for the quantitative assessment.\n\n\nBaseline models\n\nIn this Section we briefly describe the state-of-the-art deep models we utilized to compare 3DFaceGAN against.   Fig. 10 Reconstruction quality of our proposed GAN network along with pix2pixHD ) and pix2pix  in the 3D face translation task. As it can be seen, the mean error of 3DFaceGAN is considerably less than the other two approaches.\n\n\nDenoising Vanilla Autoencoder (Denoising AE)\n\nDenoising Vanilla Autoencoder follows exactly the same structure as the Vanilla AE in Section 4.4, the only difference being the inputs fed to the network. This is the main baseline we compared against and the results are provided in the ablation study in Section 4.5.3.\n\n\nDenoising Convolutional Mesh Autoencoder (Denoising CoMA)\n\nDenoising CoMA (Ranjan et al. 2018), follows exactly the same structure as the Vanilla AE in Section 4.4, the only difference being again the inputs fed to the network.\n\npix2pix pix2pix ) is amongst the most widely utilized GANs for image to image translation applications. We used the official implementation and hyper-parameter initializations provided by the authors in .\n\n\npix2pixHD\n\nMore recently pix2pixHD ) was proposed, which can be considered as an extension of pix2pix ) and which is able to better handle data of higher resolution. We used the official implementation and hyperparameter initializations provided by the authors in Wang et al. (2018). As evinced in Fig. 8, Fig. 9, and Fig. 10, pix2pixHD ) outperforms pix2pix , and this is expected since pix2pixHD ) uses more intricate structures for both the generator and discriminator networks.\n\n\nError metric\n\nFor each low quality test mesh we aim to estimate the high quality representation based on the 3dMD ground truth data. The error metric between the estimated and the real high quality mesh is a standard 3D Root Mean Square Error (3DRMSE) where the Euclidean distances are computed between the two meshes and normalized based on the interocular distance of the test mesh. Before computing the metric error we perform dense alignment between each test mesh and its corresponding ground truth by implementing an iterative closest point (ICP) algorithm (Besl and McKay 1992). In order to avoid any inconsistencies in the alignment we compute a point-to-plain rather than a point-to-point error. Finally, the measurements are performed in the inner part of the face, where we crop each test mesh at a radius of 150mm around the tip of the nose. As can be clearly seen in Fig. 9a as well as in Table 4, 3DFaceGAN outperforms all of the compared state-of-the-art methods.\n\n\nAblation study\n\nFor the ablation study in this set of experiments, we use exactly the same 3DFaceGAN variants as the ones we utilized in Section 4.4.3. Moreover, instead of the vanilla AE in this experiment we utilize the denoising AE. As evinced in Fig.  9b and Table 5, 3DFaceGAN clearly outperforms all of the compared models.\n\n\nMulti-label 3D Face Translation\n\nIn this experiment we utilize 4DFAB (Cheng et al. 2018) for the multi-label transfer of expressions. In particular, we feed the neutral faces to the models and receive as outputs either the ones bearing the label happiness or surprise. It should be noted here that whereas 3DFaceGAN requires only a single model to be trained under the multi-label expression translation scenario, the rest of the compared models require different trained models for each label, i.e., a model for expression happiness and a model for expression surprise. As baseline models for comparisons, we use exactly the same as the ones in Section 4.5, the only difference being the inputs fed to network as well as the corresponding targets. Qualitative comparisons against the compared methods are presented in Fig. 11.  Fig. 11 Qualitative results of our approach compared to state-of-the-art baseline GAN methods in the multi-label 3D face translation task in various expressions (e.g., happiness, surprise) trained with the 4DFAB (Cheng et al. 2018) database. The first column depicts the neutral input mesh whereas the rest of the columns represent the translated meshes of the respective state-of-the art methods compared to our approach. As can be seen, 3DFaceGAN is able to retain the high-frequency details in a higher level compared to CoMA (Ranjan et al. 2018), the second best method, which produces more smoothed outputs.\n\n\n3D Face Generation\n\nIn the 3D face generation experiment, we utilized the high quality data of the Hi-Lo database to train the algorithms. In particular, we feed the high quality 3D data as inputs to the models and use the same data as target outputs.\n\n\nBaseline models\n\nThe baseline models we used in this set of experiments are the same as the ones presented in Section 4.4.\n\n\nError metric\n\nThe metric of choice to quantitatively assess the performance of the models in this set of experiments is specificity (Brunton et al. 2014). For a randomly generated 3D face, specificity metric measures the distance of this 3D face to its nearest real 3D face belonging in the test, in terms of minimum per vertex distance over all samples of the test set. To evaluate this metric, we randomly generate n = 10, 000 face meshes from each model. Table 6 reports the specificity metric for 3DFaceGAN compared against the baseline models. In order to generate random meshes utilizing 3DFaceGAN, we sample from a multivariate Gaussian dis- Table 6 Specificity metric on the test set for the 3D face generation task. We generate 10, 000 random faces from each model. The table reports the mean error (Mean) and the standard deviation (std).\n\n\nMethod\n\nMean tribution, as explained in Section 3.3. To generate random meshes utilizing PGAN (Karras et al. 2018), we sample new latent embeddings from the multivariate normal distribution and feed them to the generator G. To generate random faces utilizing CoMA (Ranjan et al. 2018), we utilize the proposed variational convolutional mesh autoencoder structure, as described in (Ranjan et al. 2018). For the PCA model (Jolliffe 2011), we generate meshes directly from the latent eigenspace by drawing random samples from a Gaussian distribution defined by the principal eigenvalues. As shown in Table 6, 3DFaceGAN achieves the best specificity error, outperforming all compared methods by a large margin. In Fig. 7, we present various visualizations of realistic 3D faces generated by 3DFaceGAN. As can be clearly seen, 3DFaceGAN is able to generate data varying in ethnicity, age, etc., thus capturing the whole population spectrum.\n\n\nMulti-label 3D Face Generation\n\nIn this set of experiments, we utilized the 4DFAB (Cheng et al. 2018) data to generate random subjects of various ex-pressions such as happiness and surprise, as seen in Fig. 12. The 3D faces were generated utilizing the methodology detailed in Section 3.4. As evinced, 3DFaceGAN is able to generate expressions of subjects varying in age and ethnicity, while retaining the high-frequency details of the 3D face.\n\n\nConclusion\n\nIn this paper we presented the first GAN tailored for the tasks of 3D face representation, generation, and translation. Leveraging the strengths of autoencoder-based discriminators in an adversarial framework, we propose 3DFaceGAN, a novel technique for training on large-scale 3D facial scans. As shown in an extensive series of quantitative as well as qualitative experiments against other state-of-the-art deep networks, 3DFaceGAN improves upon state-of-the-art algorithms for the tasks at-hand by a significant margin.\n\nFig. 5\n5(a) Generalization results on the test set for the 3D face representation task. The results are presented as cumulative error distributions of the normalized dense vertex errors. 3DFaceGAN outperforms all of the compared methods by a large margin. (b) Ablation study generalization results for the 3D face representation task. The results are presented as cumulative error distributions of the normalized dense vertex errors.\n\nFig. 9\n9(a) High quality estimation results for the 3D face translation task. The results are presented as cumulative error distributions of the normalized dense vertex errors. 3DFaceGAN outperforms all of the compared methods by a large margin. (b) Ablation study with respect to the 3D face translation task. The results are presented as cumulative error distributions of the normalized dense vertex errors.\n\n\n).Template \nregistration \n\nNICP \n\n2D nearest \npoint \ninterpolation \n\nInterpolated UV map \nSparse UV map \n\nSpatial location \nstorage in UV \nspace \n\n\n\nTable 2\n2Generalization metric for the meshes of the test set for the 3D face representation task. The table reports the mean error (Mean), the standard deviation (std), the Area Under the Curve (AUC), and the Failure Rate (FR) of the Cumulative Error Distributions ofFig. 5a.Method \nMean \nstd \nAUC \nFR (%) \n\n3DFaceGAN 0.0031 \u00b10.0028 0.741 1.42e-7 \n\nCoMA \n0.0038 \u00b10.0037 0.716 3.66e-7 \n\nPCA \n0.0040 \u00b10.0040 0.711 0.91e-6 \n\nPGAN \n0.0041 \u00b10.0041 0.705 1.22e-6 \n\n\n\nTable 3 Ablation\n3study generalization results for the 3D face repre-\nsentation task. The table reports the Area Under the Curve (AUC) and \nFailure Rate (FR) of the Cumulative Error Distributions of Fig. 5b. \n\nMethod \nAUC \nFR (%) \n\n3DFaceGAN \n0.741 1.42e-7 \n\n3DFaceGAN V3 0.736 2.62e-7 \n\n3DFaceGAN V2 0.704 3.15e-6 \n\nBaseline (AE) \n0.697 \n4.24-6 \n\n\n\nTable 4\n4High quality 3DRMSE results for the 3D face translation task. \nThe table reports the Area Under the Curve (AUC) and Failure Rate of \nthe Cumulative Error Distributions of Fig. 9a. \n\nMethod \nAUC \nFailure Rate (%) \n\n3DFaceGAN \n0.827 \n5.49e-6 \n\npix2pixHD \n0.760 \n5.18e-5 \n\npix2pix \n0.757 \n1.81e-5 \n\nDenoising CoMA 0.742 \n2.41e-4 \n\n\n\nTable 5\n5Ablation study 3DRMSE results for the 3D face translation task. The table reports the Area Under the Curve (AUC) and Failure Rate of the Cumulative Error Distributions of Fig. 9b.Method \nAUC \nFailure Rate (%) \n\n3DFaceGAN \n0.827 \n5.49e-6 \n\n3DFaceGAN V3 \n0.819 \n8.70e-6 \n\n3DFaceGAN V2 \n0.794 \n1.38e-5 \n\nBaseline (Denoising AE) 0.758 \n1.95e-5 \n\n\n\n\nFig. 12Generated faces with expression utilizing 3DFaceGAN multilabel approach.std \n\n3DFaceGAN \n1.28 \n\u00b10.183 \n\nCoMA \n1.40 \n\u00b10.205 \n\nPCA \n1.43 \n\u00b10.232 \n\nPGAN \n1.79 \n\u00b10.189 \n\n\nA thorough overview describing the first attempts towards geometric deep learning can be found inBronstein et al. (2017).\n3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation\nnote that pre-training D is not possible when the outputs are logits since there are no fake data to compare against prior to the adversarial training.3 for brevity in the text, we will use the term parameters to refer to the weights and the biases from this point onwards.\n3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation\nhttp://www.di4d.com\n3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation\n3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation\nAcknowledgements Stylianos Moschoglou is supported by an EP-SRC DTA studentship from Imperial College London, Stylianos Ploumpis by the EPSRC Project EP/N007743/1 (FACER2VM), and Stefanos Zafeiriou by the EPSRC Project EP/S010203/1 (DEFORM).\nD Berthelot, T Schumm, L Metz, arXiv:170310717Began: boundary equilibrium generative adversarial networks. arXiv preprintBerthelot D, Schumm T, Metz L (2017) Began: boundary equilibrium generative adversarial networks. arXiv preprint arXiv:170310717\n\nMethod for registration of 3-d shapes. P J Besl, N D Mckay, Sensor Fusion IV: Control Paradigms and Data Structures. 1611Besl PJ, McKay ND (1992) Method for registration of 3-d shapes. In: Sensor Fusion IV: Control Paradigms and Data Structures, vol 1611, pp 586-607\n\nOptimal uv spaces for facial morphable model construction. J Booth, S Zafeiriou, Proceedings of the IEEE International Conference on Image Processing (ICIP). the IEEE International Conference on Image Processing (ICIP)Booth J, Zafeiriou S (2014) Optimal uv spaces for facial morphable model construction. In: Proceedings of the IEEE International Conference on Image Processing (ICIP), pp 4672-4676\n\nA 3d morphable model learnt from 10,000 faces. J Booth, A Roussos, S Zafeiriou, A Ponniah, D Dunaway, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBooth J, Roussos A, Zafeiriou S, Ponniah A, Dunaway D (2016) A 3d morphable model learnt from 10,000 faces. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 5543-5552\n\nUnsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)1Bousmalis K, Silberman N, Dohan D, Erhan D, Krishnan D (2017) Unsupervised pixel-level domain adaptation with generative ad- versarial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol 1, p 7\n\nGeometric deep learning: going beyond euclidean data. M M Bronstein, J Bruna, Y Lecun, A Szlam, P Vandergheynst, IEEE Signal Processing Magazine. 344Bronstein MM, Bruna J, LeCun Y, Szlam A, Vandergheynst P (2017) Geometric deep learning: going beyond euclidean data. IEEE Sig- nal Processing Magazine 34(4):18-42\n\nReview of statistical shape spaces for 3d data with comparative analysis for human faces. A Brunton, A Salazar, T Bolkart, S Wuhrer, Computer Vision and Image Understanding. 128Brunton A, Salazar A, Bolkart T, Wuhrer S (2014) Review of statisti- cal shape spaces for 3d data with comparative analysis for human faces. Computer Vision and Image Understanding 128:1-17\n\n4dfab: A large scale 4d database for facial expression analysis and biometric applications. S Cheng, I Kotsia, M Pantic, S Zafeiriou, Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). the IEEE conference on computer vision and pattern recognition (CVPR)Cheng S, Kotsia I, Pantic M, Zafeiriou S (2018) 4dfab: A large scale 4d database for facial expression analysis and biometric applications. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp 5117-5126\n\nStargan: Unified generative adversarial networks for multi-domain image-toimage translation. Y Choi, M Choi, M Kim, J W Ha, S Kim, J Choo, 1711arXiv preprintChoi Y, Choi M, Kim M, Ha JW, Kim S, Choo J (2017) Stargan: Uni- fied generative adversarial networks for multi-domain image-to- image translation. arXiv preprint 1711\n\nFast and accurate deep network learning by exponential linear units (elus). D A Clevert, T Unterthiner, S Hochreiter, arXiv:151107289arXiv preprintClevert DA, Unterthiner T, Hochreiter S (2015) Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:151107289\n\nOptimal regions for linear model-based 3d face reconstruction. R Davies, C Twining, C Taylor, Springer Science & Business Media 3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation De Smet M. Van Gool LProceedings of the Asian Conference on Computer VisionDavies R, Twining C, Taylor C (2008) Statistical models of shape: Op- timization and evaluation. Springer Science & Business Media 3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation De Smet M, Van Gool L (2010) Optimal regions for linear model-based 3d face reconstruction. In: Proceedings of the Asian Conference on Computer Vision, pp 276-289\n\nGenerating images with perceptual similarity metrics based on deep networks. A Dosovitskiy, T Brox, Proceedings of the Advances in Neural Information Processing Systems (NIPS). the Advances in Neural Information Processing Systems (NIPS)Dosovitskiy A, Brox T (2016) Generating images with perceptual sim- ilarity metrics based on deep networks. In: Proceedings of the Ad- vances in Neural Information Processing Systems (NIPS), pp 658- 666\n\nEnd-to-end 3d face reconstruction with deep neural networks. P Dou, S K Shah, I A Kakadiaris, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Dou P, Shah SK, Kakadiaris IA (2017) End-to-end 3d face reconstruc- tion with deep neural networks. In: Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR), pp 21-26\n\nA point set generation network for 3d object reconstruction from a single image. H Fan, H Su, L J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2Fan H, Su H, Guibas LJ (2017) A point set generation network for 3d object reconstruction from a single image. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol 2, p 6\n\nJoint 3d face reconstruction and dense alignment with position map regression network. Y Feng, F Wu, X Shao, Y Wang, X Zhou, arXiv:180307835arXiv preprintFeng Y, Wu F, Shao X, Wang Y, Zhou X (2018) Joint 3d face recon- struction and dense alignment with position map regression net- work. arXiv preprint arXiv:180307835\n\nUnsupervised training for 3d morphable model regression. K Genova, F Cole, A Maschinot, A Sarna, D Vlasic, W T Freeman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Genova K, Cole F, Maschinot A, Sarna A, Vlasic D, Freeman WT (2018) Unsupervised training for 3d morphable model regression. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp 8377-8386\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Proceedings of the Advances in neural information processing systems. the Advances in neural information processing systemsGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: Proceedings of the Advances in neural information processing systems, pp 2672-2680\n\nGeneralized procrustes analysis. J C Gower, Psychometrika. 401Gower JC (1975) Generalized procrustes analysis. Psychometrika 40(1):33-51\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). the IEEE conference on computer vision and pattern recognition (CVPR)He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp 770-778\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)1Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017) Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol 1, p 3\n\nLarge pose 3d face reconstruction from a single image via direct volumetric cnn regression. P Isola, J Y Zhu, T Zhou, A A Efros, A S Jackson, A Bulat, V Argyriou, G Tzimiropoulos, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Image-to-image translation with conditional adversarial networksIsola P, Zhu JY, Zhou T, Efros AA (2017) Image-to-image translation with conditional adversarial networks. arXiv preprint Jackson AS, Bulat A, Argyriou V, Tzimiropoulos G (2017) Large pose 3d face reconstruction from a single image via direct volumetric cnn regression. In: Proceedings of the IEEE International Confer- ence on Computer Vision (ICCV), pp 1031-1039\n\nPerceptual losses for real-time style transfer and super-resolution. J Johnson, A Alahi, L Fei-Fei, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionSpringerJohnson J, Alahi A, Fei-Fei L (2016) Perceptual losses for real-time style transfer and super-resolution. In: Proceedings of the Euro- pean Conference on Computer Vision, Springer, pp 694-711\n\nPrincipal component analysis. I Jolliffe, International Encyclopedia of Statistical Science. SpringerJolliffe I (2011) Principal component analysis. In: International Ency- clopedia of Statistical Science, Springer, pp 1094-1096\n\nProgressive growing of gans for improved quality, stability, and variation. T Karras, Aila T Laine, S Lehtinen, J , Proceedings of the International Conference for Learning Representations. the International Conference for Learning RepresentationsICLRKarras T, Aila T, Laine S, Lehtinen J (2018) Progressive growing of gans for improved quality, stability, and variation. Proceedings of the International Conference for Learning Representations (ICLR)\n\nLearning to discover cross-domain relations with generative adversarial networks. T Kim, M Cha, H Kim, J K Lee, J Kim, arXiv:170305192arXiv preprintKim T, Cha M, Kim H, Lee JK, Kim J (2017) Learning to dis- cover cross-domain relations with generative adversarial networks. arXiv preprint arXiv:170305192\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:14126980arXiv preprintKingma DP, Ba J (2014) Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980\n\nD P Kingma, M Welling, arXiv:13126114Auto-encoding variational bayes. arXiv preprintKingma DP, Welling M (2013) Auto-encoding variational bayes. arXiv preprint arXiv:13126114\n\nPhoto-realistic single image super-resolution using a generative adversarial network. C Ledig, L Theis, F Husz\u00e1r, J Caballero, A Cunningham, A Acosta, A P Aitken, A Tejani, J Totz, Z Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2Ledig C, Theis L, Husz\u00e1r F, Caballero J, Cunningham A, Acosta A, Aitken AP, Tejani A, Totz J, Wang Z, et al. (2017) Photo-realistic single image super-resolution using a generative adversarial net- work. In: Proceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), vol 2, p 4\n\nDeriving neural architectures from sequence and graph kernels. T Lei, Jin W Barzilay, R Jaakkola, T , arXiv:170509037arXiv preprintLei T, Jin W, Barzilay R, Jaakkola T (2017) Deriving neural ar- chitectures from sequence and graph kernels. arXiv preprint arXiv:170509037\n\nGenerative face completion. Y Li, S Liu, J Yang, M H Yang, Proceedings of the the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)1Li Y, Liu S, Yang J, Yang MH (2017) Generative face completion. In: Proceedings of the the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol 1, p 3\n\nDeformable shape completion with graph convolutional autoencoders. O Litany, A Bronstein, M Bronstein, A Makadia, arXiv:171200268arXiv preprintLitany O, Bronstein A, Bronstein M, Makadia A (2017a) Deformable shape completion with graph convolutional autoencoders. arXiv preprint arXiv:171200268\n\nDeep functional maps: Structured prediction for dense shape correspondence. O Litany, T Remez, E Rodol\u00e0, A M Bronstein, M M Bronstein, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Litany O, Remez T, Rodol\u00e0 E, Bronstein AM, Bronstein MM (2017b) Deep functional maps: Structured prediction for dense shape cor- respondence. In: Proceedings of the IEEE International Confer- ence on Computer Vision (ICCV), pp 5660-5668\n\nAre gans created equal? a large-scale study. M Lucic, K Kurach, M Michalski, S Gelly, O Bousquet, Proceedings of the Advances in Neural Information Processing Systems (NIPS). the Advances in Neural Information Processing Systems (NIPS)Lucic M, Kurach K, Michalski M, Gelly S, Bousquet O (2018) Are gans created equal? a large-scale study. Proceedings of the Ad- vances in Neural Information Processing Systems (NIPS)\n\nUnderstanding deep image representations by inverting them. A Mahendran, A Vedaldi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Mahendran A, Vedaldi A (2015) Understanding deep image represen- tations by inverting them. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp 5188- 5196\n\nConvolutional neural networks on surfaces via seamless toric covers. H Maron, M Galun, N Aigerman, M Trope, N Dym, E Yumer, V G Kim, Y Lipman, ACM Transactions on Graphics. 36471Maron H, Galun M, Aigerman N, Trope M, Dym N, Yumer E, Kim VG, Lipman Y (2017) Convolutional neural networks on surfaces via seamless toric covers. ACM Transactions on Graphics 36(4):71\n\nM Mirza, S Osindero, arXiv:14111784Conditional generative adversarial nets. arXiv preprintMirza M, Osindero S (2014) Conditional generative adversarial nets. arXiv preprint arXiv:14111784\n\nKinectfusion: Real-time dense surface mapping and tracking. R A Newcombe, S Izadi, O Hilliges, D Molyneaux, D Kim, A J Davison, P Kohi, J Shotton, S Hodges, A Fitzgibbon, Proceedings of the IEEE international symposium on Mixed and Augmented Reality (ISMAR). the IEEE international symposium on Mixed and Augmented Reality (ISMAR)Newcombe RA, Izadi S, Hilliges O, Molyneaux D, Kim D, Davison AJ, Kohi P, Shotton J, Hodges S, Fitzgibbon A (2011) Kinect- fusion: Real-time dense surface mapping and tracking. In: Pro- ceedings of the IEEE international symposium on Mixed and Aug- mented Reality (ISMAR), pp 127-136\n\nSuper-resolution for biometrics: A comprehensive survey. K Nguyen, C Fookes, S Sridharan, M Tistarelli, M Nixon, Pattern Recognition. 78Nguyen K, Fookes C, Sridharan S, Tistarelli M, Nixon M (2018) Super-resolution for biometrics: A comprehensive survey. Pattern Recognition 78:23-42\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition14Qi CR, Su H, Mo K, Guibas LJ (2017) Pointnet: Deep learning on point sets for 3d classification and segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 1(2):4\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:151106434arXiv preprintRadford A, Metz L, Chintala S (2015) Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:151106434\n\nA Ranjan, T Bolkart, S Sanyal, M J Black, arXiv:180710267Generating 3d faces using convolutional mesh autoencoders. arXiv preprintRanjan A, Bolkart T, Sanyal S, Black MJ (2018) Generating 3d faces using convolutional mesh autoencoders. arXiv preprint arXiv:180710267\n\nLearning detailed face reconstruction from a single image. E Richardson, M Sela, R Or-El, R Kimmel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Richardson E, Sela M, Or-El R, Kimmel R (2017) Learning detailed face reconstruction from a single image. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp 5553-5562\n\nRegressing robust and discriminative 3d morphable models with a very deep neural network. A T Tran, T Hassner, I Masi, G Medioni, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Tran AT, Hassner T, Masi I, Medioni G (2017) Regressing robust and discriminative 3d morphable models with a very deep neural net- work. In: Proceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), pp 1493-1502\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, Proceedings of the IEEE Conference Computer Vision and Pattern Recognition (CVPR). the IEEE Conference Computer Vision and Pattern Recognition (CVPR)1Tzeng E, Hoffman J, Saenko K, Darrell T (2017) Adversarial discrimi- native domain adaptation. In: Proceedings of the IEEE Conference Computer Vision and Pattern Recognition (CVPR), vol 1, p 4\n\nHighresolution image synthesis and semantic manipulation with conditional gans. T C Wang, M Y Liu, J Y Zhu, A Tao, J Kautz, B Catanzaro, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)1Wang TC, Liu MY, Zhu JY, Tao A, Kautz J, Catanzaro B (2018) High- resolution image synthesis and semantic manipulation with condi- tional gans. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol 1, p 5\n\nShape inpainting using 3d generative adversarial network and recurrent convolutional networks. W Wang, Q Huang, S You, C Yang, U Neumann, arXiv:171106375arXiv preprintWang W, Huang Q, You S, Yang C, Neumann U (2017) Shape inpaint- ing using 3d generative adversarial network and recurrent convo- lutional networks. arXiv preprint arXiv:171106375\n\nHighresolution image inpainting using multi-scale neural patch synthesis. C Yang, X Lu, Z Lin, E Shechtman, O Wang, H Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)1Yang C, Lu X, Lin Z, Shechtman E, Wang O, Li H (2017) High- resolution image inpainting using multi-scale neural patch synthe- sis. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol 1, p 3\n\nEnergy-based generative adversarial network. J Zhao, M Mathieu, Y Lecun, arXiv:160903126arXiv preprintZhao J, Mathieu M, LeCun Y (2016) Energy-based generative adver- sarial network. arXiv preprint arXiv:160903126\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J Y Zhu, T Park, P Isola, A A Efros, arXiv preprintZhu JY, Park T, Isola P, Efros AA (2017) Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint\n", "annotations": {"author": "[{\"end\":136,\"start\":87},{\"end\":182,\"start\":137},{\"end\":193,\"start\":183},{\"end\":235,\"start\":194},{\"end\":273,\"start\":236},{\"end\":291,\"start\":274}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":97},{\"end\":155,\"start\":147},{\"end\":192,\"start\":185},{\"end\":213,\"start\":205}]", "author_first_name": "[{\"end\":96,\"start\":87},{\"end\":146,\"start\":137},{\"end\":184,\"start\":183},{\"end\":202,\"start\":194},{\"end\":204,\"start\":203},{\"end\":245,\"start\":236}]", "author_affiliation": "[{\"end\":290,\"start\":275}]", "title": "[{\"end\":84,\"start\":1},{\"end\":375,\"start\":292}]", "venue": null, "abstract": "[{\"end\":2204,\"start\":651}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2417,\"start\":2393},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2894,\"start\":2873},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2946,\"start\":2926},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3174,\"start\":3149},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3344,\"start\":3322},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3362,\"start\":3344},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3398,\"start\":3381},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3415,\"start\":3398},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3460,\"start\":3440},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3480,\"start\":3460},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3497,\"start\":3480},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3537,\"start\":3521},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3554,\"start\":3537},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3570,\"start\":3554},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3956,\"start\":3929},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3975,\"start\":3956},{\"end\":3983,\"start\":3976},{\"end\":3994,\"start\":3987},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5224,\"start\":5207},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5239,\"start\":5224},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5319,\"start\":5296},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7818,\"start\":7799},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7838,\"start\":7818},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7854,\"start\":7838},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7943,\"start\":7923},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7966,\"start\":7945},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8572,\"start\":8555},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8587,\"start\":8572},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8974,\"start\":8950},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8991,\"start\":8974},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9007,\"start\":8991},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9026,\"start\":9007},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9523,\"start\":9505},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12752,\"start\":12734},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12777,\"start\":12754},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12943,\"start\":12925},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13003,\"start\":12980},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13627,\"start\":13608},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14657,\"start\":14639},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14679,\"start\":14657},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16470,\"start\":16449},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16747,\"start\":16731},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16765,\"start\":16747},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17003,\"start\":16978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17554,\"start\":17534},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17900,\"start\":17882},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19681,\"start\":19660},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19788,\"start\":19767},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22772,\"start\":22750},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24683,\"start\":24665},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24978,\"start\":24960},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25860,\"start\":25841},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27119,\"start\":27093},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27272,\"start\":27260},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28001,\"start\":27982},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28440,\"start\":28420},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30190,\"start\":30171},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30532,\"start\":30517},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30788,\"start\":30769},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31003,\"start\":30984},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31036,\"start\":31008},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31161,\"start\":31133},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31355,\"start\":31336},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35359,\"start\":35339},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35983,\"start\":35965},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36769,\"start\":36748},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37585,\"start\":37567},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38558,\"start\":38539},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38876,\"start\":38856},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39474,\"start\":39453},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":40286,\"start\":40266},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40456,\"start\":40436},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40572,\"start\":40552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41210,\"start\":41192},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":44888,\"start\":44865}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42526,\"start\":42092},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42937,\"start\":42527},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43087,\"start\":42938},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43549,\"start\":43088},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43899,\"start\":43550},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44238,\"start\":43900},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44591,\"start\":44239},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44767,\"start\":44592}]", "paragraph": "[{\"end\":3034,\"start\":2220},{\"end\":3572,\"start\":3036},{\"end\":4297,\"start\":3574},{\"end\":4802,\"start\":4327},{\"end\":5637,\"start\":4804},{\"end\":5909,\"start\":5639},{\"end\":6034,\"start\":5911},{\"end\":6701,\"start\":6036},{\"end\":6859,\"start\":6743},{\"end\":6930,\"start\":6871},{\"end\":7589,\"start\":6976},{\"end\":8363,\"start\":7591},{\"end\":8792,\"start\":8365},{\"end\":9492,\"start\":8794},{\"end\":10076,\"start\":9494},{\"end\":10602,\"start\":10111},{\"end\":10944,\"start\":10650},{\"end\":11732,\"start\":10967},{\"end\":11863,\"start\":11734},{\"end\":13678,\"start\":11935},{\"end\":13708,\"start\":13705},{\"end\":13793,\"start\":13710},{\"end\":14086,\"start\":13881},{\"end\":14274,\"start\":14109},{\"end\":16393,\"start\":14276},{\"end\":16923,\"start\":16395},{\"end\":17695,\"start\":16946},{\"end\":19809,\"start\":17733},{\"end\":19856,\"start\":19818},{\"end\":21530,\"start\":21098},{\"end\":22005,\"start\":21546},{\"end\":22465,\"start\":22040},{\"end\":23211,\"start\":22467},{\"end\":23251,\"start\":23213},{\"end\":23322,\"start\":23253},{\"end\":23514,\"start\":23421},{\"end\":23592,\"start\":23516},{\"end\":23822,\"start\":23594},{\"end\":24193,\"start\":23824},{\"end\":24631,\"start\":24195},{\"end\":24952,\"start\":24650},{\"end\":25346,\"start\":24954},{\"end\":26595,\"start\":25369},{\"end\":28313,\"start\":26597},{\"end\":29146,\"start\":28373},{\"end\":29656,\"start\":29173},{\"end\":29781,\"start\":29676},{\"end\":30106,\"start\":29810},{\"end\":30437,\"start\":30148},{\"end\":30719,\"start\":30476},{\"end\":31162,\"start\":30746},{\"end\":31357,\"start\":31179},{\"end\":31943,\"start\":31359},{\"end\":32654,\"start\":31945},{\"end\":34082,\"start\":32744},{\"end\":34584,\"start\":34106},{\"end\":34943,\"start\":34604},{\"end\":35262,\"start\":34992},{\"end\":35492,\"start\":35324},{\"end\":35698,\"start\":35494},{\"end\":36182,\"start\":35712},{\"end\":37163,\"start\":36199},{\"end\":37495,\"start\":37182},{\"end\":38939,\"start\":37531},{\"end\":39193,\"start\":38962},{\"end\":39318,\"start\":39213},{\"end\":40169,\"start\":39335},{\"end\":41107,\"start\":40180},{\"end\":41554,\"start\":41142},{\"end\":42091,\"start\":41569}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6975,\"start\":6931},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10637,\"start\":10603},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11934,\"start\":11864},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13704,\"start\":13679},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13880,\"start\":13794},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21097,\"start\":19857},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23420,\"start\":23323},{\"attributes\":{\"id\":\"formula_7\"},\"end\":28361,\"start\":28314},{\"attributes\":{\"id\":\"formula_8\"},\"end\":32726,\"start\":32655}]", "table_ref": "[{\"end\":16922,\"start\":16915},{\"end\":19117,\"start\":19110},{\"end\":23699,\"start\":23692},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32010,\"start\":32003},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33636,\"start\":33629},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37094,\"start\":37087},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37436,\"start\":37429},{\"end\":39786,\"start\":39779},{\"end\":39977,\"start\":39970},{\"end\":40776,\"start\":40769}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2218,\"start\":2206},{\"end\":4325,\"start\":4300},{\"attributes\":{\"n\":\"2\"},\"end\":6741,\"start\":6704},{\"end\":6869,\"start\":6862},{\"end\":10109,\"start\":10079},{\"attributes\":{\"n\":\"3\"},\"end\":10648,\"start\":10639},{\"attributes\":{\"n\":\"3.1\"},\"end\":10965,\"start\":10947},{\"attributes\":{\"n\":\"3.2\"},\"end\":14107,\"start\":14089},{\"attributes\":{\"n\":\"3.3\"},\"end\":16944,\"start\":16926},{\"attributes\":{\"n\":\"3.4\"},\"end\":17731,\"start\":17698},{\"end\":19816,\"start\":19812},{\"attributes\":{\"n\":\"4\"},\"end\":21544,\"start\":21533},{\"attributes\":{\"n\":\"4.1\"},\"end\":22017,\"start\":22008},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":22038,\"start\":22020},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":24648,\"start\":24634},{\"attributes\":{\"n\":\"4.2\"},\"end\":25367,\"start\":25349},{\"attributes\":{\"n\":\"4.3\"},\"end\":28371,\"start\":28363},{\"attributes\":{\"n\":\"4.4\"},\"end\":29171,\"start\":29149},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":29674,\"start\":29659},{\"end\":29808,\"start\":29784},{\"end\":30146,\"start\":30109},{\"end\":30474,\"start\":30440},{\"end\":30744,\"start\":30722},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":31177,\"start\":31165},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":32742,\"start\":32728},{\"attributes\":{\"n\":\"4.5\"},\"end\":34104,\"start\":34085},{\"attributes\":{\"n\":\"4.5.1\"},\"end\":34602,\"start\":34587},{\"end\":34990,\"start\":34946},{\"end\":35322,\"start\":35265},{\"end\":35710,\"start\":35701},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":36197,\"start\":36185},{\"attributes\":{\"n\":\"4.5.3\"},\"end\":37180,\"start\":37166},{\"attributes\":{\"n\":\"4.6\"},\"end\":37529,\"start\":37498},{\"attributes\":{\"n\":\"4.7\"},\"end\":38960,\"start\":38942},{\"attributes\":{\"n\":\"4.7.1\"},\"end\":39211,\"start\":39196},{\"attributes\":{\"n\":\"4.7.2\"},\"end\":39333,\"start\":39321},{\"end\":40178,\"start\":40172},{\"attributes\":{\"n\":\"4.8\"},\"end\":41140,\"start\":41110},{\"attributes\":{\"n\":\"5\"},\"end\":41567,\"start\":41557},{\"end\":42099,\"start\":42093},{\"end\":42534,\"start\":42528},{\"end\":43096,\"start\":43089},{\"end\":43567,\"start\":43551},{\"end\":43908,\"start\":43901},{\"end\":44247,\"start\":44240}]", "table": "[{\"end\":43087,\"start\":42942},{\"end\":43549,\"start\":43365},{\"end\":43899,\"start\":43569},{\"end\":44238,\"start\":43910},{\"end\":44591,\"start\":44428},{\"end\":44767,\"start\":44673}]", "figure_caption": "[{\"end\":42526,\"start\":42101},{\"end\":42937,\"start\":42536},{\"end\":42942,\"start\":42940},{\"end\":43365,\"start\":43098},{\"end\":44428,\"start\":44249},{\"end\":44673,\"start\":44594}]", "figure_ref": "[{\"end\":4381,\"start\":4375},{\"end\":5079,\"start\":5062},{\"end\":5566,\"start\":5560},{\"end\":10136,\"start\":10130},{\"end\":10466,\"start\":10459},{\"end\":27922,\"start\":27914},{\"end\":27931,\"start\":27925},{\"end\":31525,\"start\":31519},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32514,\"start\":32507},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33624,\"start\":33617},{\"end\":34724,\"start\":34717},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36013,\"start\":35999},{\"end\":36026,\"start\":36019},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37072,\"start\":37065},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37424,\"start\":37416},{\"end\":38324,\"start\":38317},{\"end\":38334,\"start\":38327},{\"end\":40888,\"start\":40882},{\"end\":41319,\"start\":41312}]", "bib_author_first_name": "[{\"end\":45763,\"start\":45762},{\"end\":45776,\"start\":45775},{\"end\":45786,\"start\":45785},{\"end\":46053,\"start\":46052},{\"end\":46055,\"start\":46054},{\"end\":46063,\"start\":46062},{\"end\":46065,\"start\":46064},{\"end\":46341,\"start\":46340},{\"end\":46350,\"start\":46349},{\"end\":46729,\"start\":46728},{\"end\":46738,\"start\":46737},{\"end\":46749,\"start\":46748},{\"end\":46762,\"start\":46761},{\"end\":46773,\"start\":46772},{\"end\":47211,\"start\":47210},{\"end\":47224,\"start\":47223},{\"end\":47237,\"start\":47236},{\"end\":47246,\"start\":47245},{\"end\":47255,\"start\":47254},{\"end\":47724,\"start\":47723},{\"end\":47726,\"start\":47725},{\"end\":47739,\"start\":47738},{\"end\":47748,\"start\":47747},{\"end\":47757,\"start\":47756},{\"end\":47766,\"start\":47765},{\"end\":48074,\"start\":48073},{\"end\":48085,\"start\":48084},{\"end\":48096,\"start\":48095},{\"end\":48107,\"start\":48106},{\"end\":48444,\"start\":48443},{\"end\":48453,\"start\":48452},{\"end\":48463,\"start\":48462},{\"end\":48473,\"start\":48472},{\"end\":48978,\"start\":48977},{\"end\":48986,\"start\":48985},{\"end\":48994,\"start\":48993},{\"end\":49001,\"start\":49000},{\"end\":49003,\"start\":49002},{\"end\":49009,\"start\":49008},{\"end\":49016,\"start\":49015},{\"end\":49287,\"start\":49286},{\"end\":49289,\"start\":49288},{\"end\":49300,\"start\":49299},{\"end\":49315,\"start\":49314},{\"end\":49576,\"start\":49575},{\"end\":49586,\"start\":49585},{\"end\":49597,\"start\":49596},{\"end\":50256,\"start\":50255},{\"end\":50271,\"start\":50270},{\"end\":50681,\"start\":50680},{\"end\":50688,\"start\":50687},{\"end\":50690,\"start\":50689},{\"end\":50698,\"start\":50697},{\"end\":50700,\"start\":50699},{\"end\":51152,\"start\":51151},{\"end\":51159,\"start\":51158},{\"end\":51165,\"start\":51164},{\"end\":51167,\"start\":51166},{\"end\":51633,\"start\":51632},{\"end\":51641,\"start\":51640},{\"end\":51647,\"start\":51646},{\"end\":51655,\"start\":51654},{\"end\":51663,\"start\":51662},{\"end\":51924,\"start\":51923},{\"end\":51934,\"start\":51933},{\"end\":51942,\"start\":51941},{\"end\":51955,\"start\":51954},{\"end\":51964,\"start\":51963},{\"end\":51974,\"start\":51973},{\"end\":51976,\"start\":51975},{\"end\":52400,\"start\":52399},{\"end\":52414,\"start\":52413},{\"end\":52431,\"start\":52430},{\"end\":52440,\"start\":52439},{\"end\":52446,\"start\":52445},{\"end\":52462,\"start\":52461},{\"end\":52471,\"start\":52470},{\"end\":52484,\"start\":52483},{\"end\":52867,\"start\":52866},{\"end\":52869,\"start\":52868},{\"end\":53018,\"start\":53017},{\"end\":53024,\"start\":53023},{\"end\":53033,\"start\":53032},{\"end\":53040,\"start\":53039},{\"end\":53427,\"start\":53426},{\"end\":53436,\"start\":53435},{\"end\":53443,\"start\":53442},{\"end\":53461,\"start\":53460},{\"end\":53463,\"start\":53462},{\"end\":53924,\"start\":53923},{\"end\":53933,\"start\":53932},{\"end\":53935,\"start\":53934},{\"end\":53942,\"start\":53941},{\"end\":53950,\"start\":53949},{\"end\":53952,\"start\":53951},{\"end\":53961,\"start\":53960},{\"end\":53963,\"start\":53962},{\"end\":53974,\"start\":53973},{\"end\":53983,\"start\":53982},{\"end\":53995,\"start\":53994},{\"end\":54646,\"start\":54645},{\"end\":54657,\"start\":54656},{\"end\":54666,\"start\":54665},{\"end\":55009,\"start\":55008},{\"end\":55285,\"start\":55284},{\"end\":55298,\"start\":55294},{\"end\":55300,\"start\":55299},{\"end\":55309,\"start\":55308},{\"end\":55321,\"start\":55320},{\"end\":55744,\"start\":55743},{\"end\":55751,\"start\":55750},{\"end\":55758,\"start\":55757},{\"end\":55765,\"start\":55764},{\"end\":55767,\"start\":55766},{\"end\":55774,\"start\":55773},{\"end\":56012,\"start\":56011},{\"end\":56014,\"start\":56013},{\"end\":56024,\"start\":56023},{\"end\":56156,\"start\":56155},{\"end\":56158,\"start\":56157},{\"end\":56168,\"start\":56167},{\"end\":56418,\"start\":56417},{\"end\":56427,\"start\":56426},{\"end\":56436,\"start\":56435},{\"end\":56446,\"start\":56445},{\"end\":56459,\"start\":56458},{\"end\":56473,\"start\":56472},{\"end\":56483,\"start\":56482},{\"end\":56485,\"start\":56484},{\"end\":56495,\"start\":56494},{\"end\":56505,\"start\":56504},{\"end\":56513,\"start\":56512},{\"end\":57048,\"start\":57047},{\"end\":57057,\"start\":57054},{\"end\":57059,\"start\":57058},{\"end\":57071,\"start\":57070},{\"end\":57083,\"start\":57082},{\"end\":57285,\"start\":57284},{\"end\":57291,\"start\":57290},{\"end\":57298,\"start\":57297},{\"end\":57306,\"start\":57305},{\"end\":57308,\"start\":57307},{\"end\":57717,\"start\":57716},{\"end\":57727,\"start\":57726},{\"end\":57740,\"start\":57739},{\"end\":57753,\"start\":57752},{\"end\":58022,\"start\":58021},{\"end\":58032,\"start\":58031},{\"end\":58041,\"start\":58040},{\"end\":58051,\"start\":58050},{\"end\":58053,\"start\":58052},{\"end\":58066,\"start\":58065},{\"end\":58068,\"start\":58067},{\"end\":58499,\"start\":58498},{\"end\":58508,\"start\":58507},{\"end\":58518,\"start\":58517},{\"end\":58531,\"start\":58530},{\"end\":58540,\"start\":58539},{\"end\":58932,\"start\":58931},{\"end\":58945,\"start\":58944},{\"end\":59377,\"start\":59376},{\"end\":59386,\"start\":59385},{\"end\":59395,\"start\":59394},{\"end\":59407,\"start\":59406},{\"end\":59416,\"start\":59415},{\"end\":59423,\"start\":59422},{\"end\":59432,\"start\":59431},{\"end\":59434,\"start\":59433},{\"end\":59441,\"start\":59440},{\"end\":59673,\"start\":59672},{\"end\":59682,\"start\":59681},{\"end\":59922,\"start\":59921},{\"end\":59924,\"start\":59923},{\"end\":59936,\"start\":59935},{\"end\":59945,\"start\":59944},{\"end\":59957,\"start\":59956},{\"end\":59970,\"start\":59969},{\"end\":59977,\"start\":59976},{\"end\":59979,\"start\":59978},{\"end\":59990,\"start\":59989},{\"end\":59998,\"start\":59997},{\"end\":60009,\"start\":60008},{\"end\":60019,\"start\":60018},{\"end\":60534,\"start\":60533},{\"end\":60544,\"start\":60543},{\"end\":60554,\"start\":60553},{\"end\":60567,\"start\":60566},{\"end\":60581,\"start\":60580},{\"end\":60840,\"start\":60839},{\"end\":60842,\"start\":60841},{\"end\":60848,\"start\":60847},{\"end\":60854,\"start\":60853},{\"end\":60860,\"start\":60859},{\"end\":60862,\"start\":60861},{\"end\":61316,\"start\":61315},{\"end\":61327,\"start\":61326},{\"end\":61335,\"start\":61334},{\"end\":61539,\"start\":61538},{\"end\":61549,\"start\":61548},{\"end\":61560,\"start\":61559},{\"end\":61570,\"start\":61569},{\"end\":61572,\"start\":61571},{\"end\":61866,\"start\":61865},{\"end\":61880,\"start\":61879},{\"end\":61888,\"start\":61887},{\"end\":61897,\"start\":61896},{\"end\":62362,\"start\":62361},{\"end\":62364,\"start\":62363},{\"end\":62372,\"start\":62371},{\"end\":62383,\"start\":62382},{\"end\":62391,\"start\":62390},{\"end\":62846,\"start\":62845},{\"end\":62855,\"start\":62854},{\"end\":62866,\"start\":62865},{\"end\":62876,\"start\":62875},{\"end\":63311,\"start\":63310},{\"end\":63313,\"start\":63312},{\"end\":63321,\"start\":63320},{\"end\":63323,\"start\":63322},{\"end\":63330,\"start\":63329},{\"end\":63332,\"start\":63331},{\"end\":63339,\"start\":63338},{\"end\":63346,\"start\":63345},{\"end\":63355,\"start\":63354},{\"end\":63865,\"start\":63864},{\"end\":63873,\"start\":63872},{\"end\":63882,\"start\":63881},{\"end\":63889,\"start\":63888},{\"end\":63897,\"start\":63896},{\"end\":64191,\"start\":64190},{\"end\":64199,\"start\":64198},{\"end\":64205,\"start\":64204},{\"end\":64212,\"start\":64211},{\"end\":64225,\"start\":64224},{\"end\":64233,\"start\":64232},{\"end\":64674,\"start\":64673},{\"end\":64682,\"start\":64681},{\"end\":64693,\"start\":64692},{\"end\":64925,\"start\":64924},{\"end\":64927,\"start\":64926},{\"end\":64934,\"start\":64933},{\"end\":64942,\"start\":64941},{\"end\":64951,\"start\":64950},{\"end\":64953,\"start\":64952}]", "bib_author_last_name": "[{\"end\":45773,\"start\":45764},{\"end\":45783,\"start\":45777},{\"end\":45791,\"start\":45787},{\"end\":46060,\"start\":46056},{\"end\":46071,\"start\":46066},{\"end\":46347,\"start\":46342},{\"end\":46360,\"start\":46351},{\"end\":46735,\"start\":46730},{\"end\":46746,\"start\":46739},{\"end\":46759,\"start\":46750},{\"end\":46770,\"start\":46763},{\"end\":46781,\"start\":46774},{\"end\":47221,\"start\":47212},{\"end\":47234,\"start\":47225},{\"end\":47243,\"start\":47238},{\"end\":47252,\"start\":47247},{\"end\":47264,\"start\":47256},{\"end\":47736,\"start\":47727},{\"end\":47745,\"start\":47740},{\"end\":47754,\"start\":47749},{\"end\":47763,\"start\":47758},{\"end\":47780,\"start\":47767},{\"end\":48082,\"start\":48075},{\"end\":48093,\"start\":48086},{\"end\":48104,\"start\":48097},{\"end\":48114,\"start\":48108},{\"end\":48450,\"start\":48445},{\"end\":48460,\"start\":48454},{\"end\":48470,\"start\":48464},{\"end\":48483,\"start\":48474},{\"end\":48983,\"start\":48979},{\"end\":48991,\"start\":48987},{\"end\":48998,\"start\":48995},{\"end\":49006,\"start\":49004},{\"end\":49013,\"start\":49010},{\"end\":49021,\"start\":49017},{\"end\":49297,\"start\":49290},{\"end\":49312,\"start\":49301},{\"end\":49326,\"start\":49316},{\"end\":49583,\"start\":49577},{\"end\":49594,\"start\":49587},{\"end\":49604,\"start\":49598},{\"end\":50268,\"start\":50257},{\"end\":50276,\"start\":50272},{\"end\":50685,\"start\":50682},{\"end\":50695,\"start\":50691},{\"end\":50711,\"start\":50701},{\"end\":51156,\"start\":51153},{\"end\":51162,\"start\":51160},{\"end\":51174,\"start\":51168},{\"end\":51638,\"start\":51634},{\"end\":51644,\"start\":51642},{\"end\":51652,\"start\":51648},{\"end\":51660,\"start\":51656},{\"end\":51668,\"start\":51664},{\"end\":51931,\"start\":51925},{\"end\":51939,\"start\":51935},{\"end\":51952,\"start\":51943},{\"end\":51961,\"start\":51956},{\"end\":51971,\"start\":51965},{\"end\":51984,\"start\":51977},{\"end\":52411,\"start\":52401},{\"end\":52428,\"start\":52415},{\"end\":52437,\"start\":52432},{\"end\":52443,\"start\":52441},{\"end\":52459,\"start\":52447},{\"end\":52468,\"start\":52463},{\"end\":52481,\"start\":52472},{\"end\":52491,\"start\":52485},{\"end\":52875,\"start\":52870},{\"end\":53021,\"start\":53019},{\"end\":53030,\"start\":53025},{\"end\":53037,\"start\":53034},{\"end\":53044,\"start\":53041},{\"end\":53433,\"start\":53428},{\"end\":53440,\"start\":53437},{\"end\":53458,\"start\":53444},{\"end\":53474,\"start\":53464},{\"end\":53930,\"start\":53925},{\"end\":53939,\"start\":53936},{\"end\":53947,\"start\":53943},{\"end\":53958,\"start\":53953},{\"end\":53971,\"start\":53964},{\"end\":53980,\"start\":53975},{\"end\":53992,\"start\":53984},{\"end\":54009,\"start\":53996},{\"end\":54654,\"start\":54647},{\"end\":54663,\"start\":54658},{\"end\":54674,\"start\":54667},{\"end\":55018,\"start\":55010},{\"end\":55292,\"start\":55286},{\"end\":55306,\"start\":55301},{\"end\":55318,\"start\":55310},{\"end\":55748,\"start\":55745},{\"end\":55755,\"start\":55752},{\"end\":55762,\"start\":55759},{\"end\":55771,\"start\":55768},{\"end\":55778,\"start\":55775},{\"end\":56021,\"start\":56015},{\"end\":56027,\"start\":56025},{\"end\":56165,\"start\":56159},{\"end\":56176,\"start\":56169},{\"end\":56424,\"start\":56419},{\"end\":56433,\"start\":56428},{\"end\":56443,\"start\":56437},{\"end\":56456,\"start\":56447},{\"end\":56470,\"start\":56460},{\"end\":56480,\"start\":56474},{\"end\":56492,\"start\":56486},{\"end\":56502,\"start\":56496},{\"end\":56510,\"start\":56506},{\"end\":56518,\"start\":56514},{\"end\":57052,\"start\":57049},{\"end\":57068,\"start\":57060},{\"end\":57080,\"start\":57072},{\"end\":57288,\"start\":57286},{\"end\":57295,\"start\":57292},{\"end\":57303,\"start\":57299},{\"end\":57313,\"start\":57309},{\"end\":57724,\"start\":57718},{\"end\":57737,\"start\":57728},{\"end\":57750,\"start\":57741},{\"end\":57761,\"start\":57754},{\"end\":58029,\"start\":58023},{\"end\":58038,\"start\":58033},{\"end\":58048,\"start\":58042},{\"end\":58063,\"start\":58054},{\"end\":58078,\"start\":58069},{\"end\":58505,\"start\":58500},{\"end\":58515,\"start\":58509},{\"end\":58528,\"start\":58519},{\"end\":58537,\"start\":58532},{\"end\":58549,\"start\":58541},{\"end\":58942,\"start\":58933},{\"end\":58953,\"start\":58946},{\"end\":59383,\"start\":59378},{\"end\":59392,\"start\":59387},{\"end\":59404,\"start\":59396},{\"end\":59413,\"start\":59408},{\"end\":59420,\"start\":59417},{\"end\":59429,\"start\":59424},{\"end\":59438,\"start\":59435},{\"end\":59448,\"start\":59442},{\"end\":59679,\"start\":59674},{\"end\":59691,\"start\":59683},{\"end\":59933,\"start\":59925},{\"end\":59942,\"start\":59937},{\"end\":59954,\"start\":59946},{\"end\":59967,\"start\":59958},{\"end\":59974,\"start\":59971},{\"end\":59987,\"start\":59980},{\"end\":59995,\"start\":59991},{\"end\":60006,\"start\":59999},{\"end\":60016,\"start\":60010},{\"end\":60030,\"start\":60020},{\"end\":60541,\"start\":60535},{\"end\":60551,\"start\":60545},{\"end\":60564,\"start\":60555},{\"end\":60578,\"start\":60568},{\"end\":60587,\"start\":60582},{\"end\":60845,\"start\":60843},{\"end\":60851,\"start\":60849},{\"end\":60857,\"start\":60855},{\"end\":60869,\"start\":60863},{\"end\":61324,\"start\":61317},{\"end\":61332,\"start\":61328},{\"end\":61344,\"start\":61336},{\"end\":61546,\"start\":61540},{\"end\":61557,\"start\":61550},{\"end\":61567,\"start\":61561},{\"end\":61578,\"start\":61573},{\"end\":61877,\"start\":61867},{\"end\":61885,\"start\":61881},{\"end\":61894,\"start\":61889},{\"end\":61904,\"start\":61898},{\"end\":62369,\"start\":62365},{\"end\":62380,\"start\":62373},{\"end\":62388,\"start\":62384},{\"end\":62399,\"start\":62392},{\"end\":62852,\"start\":62847},{\"end\":62863,\"start\":62856},{\"end\":62873,\"start\":62867},{\"end\":62884,\"start\":62877},{\"end\":63318,\"start\":63314},{\"end\":63327,\"start\":63324},{\"end\":63336,\"start\":63333},{\"end\":63343,\"start\":63340},{\"end\":63352,\"start\":63347},{\"end\":63365,\"start\":63356},{\"end\":63870,\"start\":63866},{\"end\":63879,\"start\":63874},{\"end\":63886,\"start\":63883},{\"end\":63894,\"start\":63890},{\"end\":63905,\"start\":63898},{\"end\":64196,\"start\":64192},{\"end\":64202,\"start\":64200},{\"end\":64209,\"start\":64206},{\"end\":64222,\"start\":64213},{\"end\":64230,\"start\":64226},{\"end\":64236,\"start\":64234},{\"end\":64679,\"start\":64675},{\"end\":64690,\"start\":64683},{\"end\":64699,\"start\":64694},{\"end\":64931,\"start\":64928},{\"end\":64939,\"start\":64935},{\"end\":64948,\"start\":64943},{\"end\":64959,\"start\":64954}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:170310717\",\"id\":\"b0\"},\"end\":46011,\"start\":45762},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":21874346},\"end\":46279,\"start\":46013},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15333828},\"end\":46679,\"start\":46281},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1744666},\"end\":47127,\"start\":46681},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206595056},\"end\":47667,\"start\":47129},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15195762},\"end\":47981,\"start\":47669},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18512739},\"end\":48349,\"start\":47983},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4611499},\"end\":48882,\"start\":48351},{\"attributes\":{\"id\":\"b8\"},\"end\":49208,\"start\":48884},{\"attributes\":{\"doi\":\"arXiv:151107289\",\"id\":\"b9\"},\"end\":49510,\"start\":49210},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":45333831},\"end\":50176,\"start\":49512},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8758543},\"end\":50617,\"start\":50178},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2687215},\"end\":51068,\"start\":50619},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6746759},\"end\":51543,\"start\":51070},{\"attributes\":{\"doi\":\"arXiv:180307835\",\"id\":\"b14\"},\"end\":51864,\"start\":51545},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49300163},\"end\":52368,\"start\":51866},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1033682},\"end\":52831,\"start\":52370},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":122244491},\"end\":52969,\"start\":52833},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":53382,\"start\":52971},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9433631},\"end\":53829,\"start\":53384},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":420414},\"end\":54574,\"start\":53831},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":980236},\"end\":54976,\"start\":54576},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2534141},\"end\":55206,\"start\":54978},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3568073},\"end\":55659,\"start\":55208},{\"attributes\":{\"doi\":\"arXiv:170305192\",\"id\":\"b24\"},\"end\":55965,\"start\":55661},{\"attributes\":{\"doi\":\"arXiv:14126980\",\"id\":\"b25\"},\"end\":56153,\"start\":55967},{\"attributes\":{\"doi\":\"arXiv:13126114\",\"id\":\"b26\"},\"end\":56329,\"start\":56155},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":211227},\"end\":56982,\"start\":56331},{\"attributes\":{\"doi\":\"arXiv:170509037\",\"id\":\"b28\"},\"end\":57254,\"start\":56984},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9459250},\"end\":57647,\"start\":57256},{\"attributes\":{\"doi\":\"arXiv:171200268\",\"id\":\"b30\"},\"end\":57943,\"start\":57649},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4215682},\"end\":58451,\"start\":57945},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4053393},\"end\":58869,\"start\":58453},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206593185},\"end\":59305,\"start\":58871},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":215762985},\"end\":59670,\"start\":59307},{\"attributes\":{\"doi\":\"arXiv:14111784\",\"id\":\"b35\"},\"end\":59859,\"start\":59672},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":11830123},\"end\":60474,\"start\":59861},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3829218},\"end\":60759,\"start\":60476},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5115938},\"end\":61219,\"start\":60761},{\"attributes\":{\"doi\":\"arXiv:151106434\",\"id\":\"b39\"},\"end\":61536,\"start\":61221},{\"attributes\":{\"doi\":\"arXiv:180710267\",\"id\":\"b40\"},\"end\":61804,\"start\":61538},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13636123},\"end\":62269,\"start\":61806},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5635268},\"end\":62797,\"start\":62271},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4357800},\"end\":63228,\"start\":62799},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":41805341},\"end\":63767,\"start\":63230},{\"attributes\":{\"doi\":\"arXiv:171106375\",\"id\":\"b45\"},\"end\":64114,\"start\":63769},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206595891},\"end\":64626,\"start\":64116},{\"attributes\":{\"doi\":\"arXiv:160903126\",\"id\":\"b47\"},\"end\":64841,\"start\":64628},{\"attributes\":{\"id\":\"b48\"},\"end\":65111,\"start\":64843}]", "bib_title": "[{\"end\":46050,\"start\":46013},{\"end\":46338,\"start\":46281},{\"end\":46726,\"start\":46681},{\"end\":47208,\"start\":47129},{\"end\":47721,\"start\":47669},{\"end\":48071,\"start\":47983},{\"end\":48441,\"start\":48351},{\"end\":49573,\"start\":49512},{\"end\":50253,\"start\":50178},{\"end\":50678,\"start\":50619},{\"end\":51149,\"start\":51070},{\"end\":51921,\"start\":51866},{\"end\":52397,\"start\":52370},{\"end\":52864,\"start\":52833},{\"end\":53015,\"start\":52971},{\"end\":53424,\"start\":53384},{\"end\":53921,\"start\":53831},{\"end\":54643,\"start\":54576},{\"end\":55006,\"start\":54978},{\"end\":55282,\"start\":55208},{\"end\":56415,\"start\":56331},{\"end\":57282,\"start\":57256},{\"end\":58019,\"start\":57945},{\"end\":58496,\"start\":58453},{\"end\":58929,\"start\":58871},{\"end\":59374,\"start\":59307},{\"end\":59919,\"start\":59861},{\"end\":60531,\"start\":60476},{\"end\":60837,\"start\":60761},{\"end\":61863,\"start\":61806},{\"end\":62359,\"start\":62271},{\"end\":62843,\"start\":62799},{\"end\":63308,\"start\":63230},{\"end\":64188,\"start\":64116}]", "bib_author": "[{\"end\":45775,\"start\":45762},{\"end\":45785,\"start\":45775},{\"end\":45793,\"start\":45785},{\"end\":46062,\"start\":46052},{\"end\":46073,\"start\":46062},{\"end\":46349,\"start\":46340},{\"end\":46362,\"start\":46349},{\"end\":46737,\"start\":46728},{\"end\":46748,\"start\":46737},{\"end\":46761,\"start\":46748},{\"end\":46772,\"start\":46761},{\"end\":46783,\"start\":46772},{\"end\":47223,\"start\":47210},{\"end\":47236,\"start\":47223},{\"end\":47245,\"start\":47236},{\"end\":47254,\"start\":47245},{\"end\":47266,\"start\":47254},{\"end\":47738,\"start\":47723},{\"end\":47747,\"start\":47738},{\"end\":47756,\"start\":47747},{\"end\":47765,\"start\":47756},{\"end\":47782,\"start\":47765},{\"end\":48084,\"start\":48073},{\"end\":48095,\"start\":48084},{\"end\":48106,\"start\":48095},{\"end\":48116,\"start\":48106},{\"end\":48452,\"start\":48443},{\"end\":48462,\"start\":48452},{\"end\":48472,\"start\":48462},{\"end\":48485,\"start\":48472},{\"end\":48985,\"start\":48977},{\"end\":48993,\"start\":48985},{\"end\":49000,\"start\":48993},{\"end\":49008,\"start\":49000},{\"end\":49015,\"start\":49008},{\"end\":49023,\"start\":49015},{\"end\":49299,\"start\":49286},{\"end\":49314,\"start\":49299},{\"end\":49328,\"start\":49314},{\"end\":49585,\"start\":49575},{\"end\":49596,\"start\":49585},{\"end\":49606,\"start\":49596},{\"end\":50270,\"start\":50255},{\"end\":50278,\"start\":50270},{\"end\":50687,\"start\":50680},{\"end\":50697,\"start\":50687},{\"end\":50713,\"start\":50697},{\"end\":51158,\"start\":51151},{\"end\":51164,\"start\":51158},{\"end\":51176,\"start\":51164},{\"end\":51640,\"start\":51632},{\"end\":51646,\"start\":51640},{\"end\":51654,\"start\":51646},{\"end\":51662,\"start\":51654},{\"end\":51670,\"start\":51662},{\"end\":51933,\"start\":51923},{\"end\":51941,\"start\":51933},{\"end\":51954,\"start\":51941},{\"end\":51963,\"start\":51954},{\"end\":51973,\"start\":51963},{\"end\":51986,\"start\":51973},{\"end\":52413,\"start\":52399},{\"end\":52430,\"start\":52413},{\"end\":52439,\"start\":52430},{\"end\":52445,\"start\":52439},{\"end\":52461,\"start\":52445},{\"end\":52470,\"start\":52461},{\"end\":52483,\"start\":52470},{\"end\":52493,\"start\":52483},{\"end\":52877,\"start\":52866},{\"end\":53023,\"start\":53017},{\"end\":53032,\"start\":53023},{\"end\":53039,\"start\":53032},{\"end\":53046,\"start\":53039},{\"end\":53435,\"start\":53426},{\"end\":53442,\"start\":53435},{\"end\":53460,\"start\":53442},{\"end\":53476,\"start\":53460},{\"end\":53932,\"start\":53923},{\"end\":53941,\"start\":53932},{\"end\":53949,\"start\":53941},{\"end\":53960,\"start\":53949},{\"end\":53973,\"start\":53960},{\"end\":53982,\"start\":53973},{\"end\":53994,\"start\":53982},{\"end\":54011,\"start\":53994},{\"end\":54656,\"start\":54645},{\"end\":54665,\"start\":54656},{\"end\":54676,\"start\":54665},{\"end\":55020,\"start\":55008},{\"end\":55294,\"start\":55284},{\"end\":55308,\"start\":55294},{\"end\":55320,\"start\":55308},{\"end\":55324,\"start\":55320},{\"end\":55750,\"start\":55743},{\"end\":55757,\"start\":55750},{\"end\":55764,\"start\":55757},{\"end\":55773,\"start\":55764},{\"end\":55780,\"start\":55773},{\"end\":56023,\"start\":56011},{\"end\":56029,\"start\":56023},{\"end\":56167,\"start\":56155},{\"end\":56178,\"start\":56167},{\"end\":56426,\"start\":56417},{\"end\":56435,\"start\":56426},{\"end\":56445,\"start\":56435},{\"end\":56458,\"start\":56445},{\"end\":56472,\"start\":56458},{\"end\":56482,\"start\":56472},{\"end\":56494,\"start\":56482},{\"end\":56504,\"start\":56494},{\"end\":56512,\"start\":56504},{\"end\":56520,\"start\":56512},{\"end\":57054,\"start\":57047},{\"end\":57070,\"start\":57054},{\"end\":57082,\"start\":57070},{\"end\":57086,\"start\":57082},{\"end\":57290,\"start\":57284},{\"end\":57297,\"start\":57290},{\"end\":57305,\"start\":57297},{\"end\":57315,\"start\":57305},{\"end\":57726,\"start\":57716},{\"end\":57739,\"start\":57726},{\"end\":57752,\"start\":57739},{\"end\":57763,\"start\":57752},{\"end\":58031,\"start\":58021},{\"end\":58040,\"start\":58031},{\"end\":58050,\"start\":58040},{\"end\":58065,\"start\":58050},{\"end\":58080,\"start\":58065},{\"end\":58507,\"start\":58498},{\"end\":58517,\"start\":58507},{\"end\":58530,\"start\":58517},{\"end\":58539,\"start\":58530},{\"end\":58551,\"start\":58539},{\"end\":58944,\"start\":58931},{\"end\":58955,\"start\":58944},{\"end\":59385,\"start\":59376},{\"end\":59394,\"start\":59385},{\"end\":59406,\"start\":59394},{\"end\":59415,\"start\":59406},{\"end\":59422,\"start\":59415},{\"end\":59431,\"start\":59422},{\"end\":59440,\"start\":59431},{\"end\":59450,\"start\":59440},{\"end\":59681,\"start\":59672},{\"end\":59693,\"start\":59681},{\"end\":59935,\"start\":59921},{\"end\":59944,\"start\":59935},{\"end\":59956,\"start\":59944},{\"end\":59969,\"start\":59956},{\"end\":59976,\"start\":59969},{\"end\":59989,\"start\":59976},{\"end\":59997,\"start\":59989},{\"end\":60008,\"start\":59997},{\"end\":60018,\"start\":60008},{\"end\":60032,\"start\":60018},{\"end\":60543,\"start\":60533},{\"end\":60553,\"start\":60543},{\"end\":60566,\"start\":60553},{\"end\":60580,\"start\":60566},{\"end\":60589,\"start\":60580},{\"end\":60847,\"start\":60839},{\"end\":60853,\"start\":60847},{\"end\":60859,\"start\":60853},{\"end\":60871,\"start\":60859},{\"end\":61326,\"start\":61315},{\"end\":61334,\"start\":61326},{\"end\":61346,\"start\":61334},{\"end\":61548,\"start\":61538},{\"end\":61559,\"start\":61548},{\"end\":61569,\"start\":61559},{\"end\":61580,\"start\":61569},{\"end\":61879,\"start\":61865},{\"end\":61887,\"start\":61879},{\"end\":61896,\"start\":61887},{\"end\":61906,\"start\":61896},{\"end\":62371,\"start\":62361},{\"end\":62382,\"start\":62371},{\"end\":62390,\"start\":62382},{\"end\":62401,\"start\":62390},{\"end\":62854,\"start\":62845},{\"end\":62865,\"start\":62854},{\"end\":62875,\"start\":62865},{\"end\":62886,\"start\":62875},{\"end\":63320,\"start\":63310},{\"end\":63329,\"start\":63320},{\"end\":63338,\"start\":63329},{\"end\":63345,\"start\":63338},{\"end\":63354,\"start\":63345},{\"end\":63367,\"start\":63354},{\"end\":63872,\"start\":63864},{\"end\":63881,\"start\":63872},{\"end\":63888,\"start\":63881},{\"end\":63896,\"start\":63888},{\"end\":63907,\"start\":63896},{\"end\":64198,\"start\":64190},{\"end\":64204,\"start\":64198},{\"end\":64211,\"start\":64204},{\"end\":64224,\"start\":64211},{\"end\":64232,\"start\":64224},{\"end\":64238,\"start\":64232},{\"end\":64681,\"start\":64673},{\"end\":64692,\"start\":64681},{\"end\":64701,\"start\":64692},{\"end\":64933,\"start\":64924},{\"end\":64941,\"start\":64933},{\"end\":64950,\"start\":64941},{\"end\":64961,\"start\":64950}]", "bib_venue": "[{\"end\":46499,\"start\":46439},{\"end\":46924,\"start\":46862},{\"end\":47421,\"start\":47352},{\"end\":48640,\"start\":48571},{\"end\":50415,\"start\":50355},{\"end\":50868,\"start\":50799},{\"end\":51331,\"start\":51262},{\"end\":52141,\"start\":52072},{\"end\":52616,\"start\":52563},{\"end\":53201,\"start\":53132},{\"end\":53631,\"start\":53562},{\"end\":54146,\"start\":54087},{\"end\":54777,\"start\":54735},{\"end\":55455,\"start\":55398},{\"end\":56675,\"start\":56606},{\"end\":57478,\"start\":57405},{\"end\":58215,\"start\":58156},{\"end\":58688,\"start\":58628},{\"end\":59110,\"start\":59041},{\"end\":60191,\"start\":60120},{\"end\":61012,\"start\":60950},{\"end\":62061,\"start\":61992},{\"end\":62556,\"start\":62487},{\"end\":63035,\"start\":62969},{\"end\":63522,\"start\":63453},{\"end\":64393,\"start\":64324},{\"end\":45867,\"start\":45808},{\"end\":46128,\"start\":46073},{\"end\":46437,\"start\":46362},{\"end\":46860,\"start\":46783},{\"end\":47350,\"start\":47266},{\"end\":47813,\"start\":47782},{\"end\":48155,\"start\":48116},{\"end\":48569,\"start\":48485},{\"end\":48975,\"start\":48884},{\"end\":49284,\"start\":49210},{\"end\":49733,\"start\":49606},{\"end\":50353,\"start\":50278},{\"end\":50797,\"start\":50713},{\"end\":51260,\"start\":51176},{\"end\":51630,\"start\":51545},{\"end\":52070,\"start\":51986},{\"end\":52561,\"start\":52493},{\"end\":52890,\"start\":52877},{\"end\":53130,\"start\":53046},{\"end\":53560,\"start\":53476},{\"end\":54085,\"start\":54011},{\"end\":54733,\"start\":54676},{\"end\":55069,\"start\":55020},{\"end\":55396,\"start\":55324},{\"end\":55741,\"start\":55661},{\"end\":56009,\"start\":55967},{\"end\":56223,\"start\":56192},{\"end\":56604,\"start\":56520},{\"end\":57045,\"start\":56984},{\"end\":57403,\"start\":57315},{\"end\":57714,\"start\":57649},{\"end\":58154,\"start\":58080},{\"end\":58626,\"start\":58551},{\"end\":59039,\"start\":58955},{\"end\":59478,\"start\":59450},{\"end\":59746,\"start\":59707},{\"end\":60118,\"start\":60032},{\"end\":60608,\"start\":60589},{\"end\":60948,\"start\":60871},{\"end\":61313,\"start\":61221},{\"end\":61652,\"start\":61595},{\"end\":61990,\"start\":61906},{\"end\":62485,\"start\":62401},{\"end\":62967,\"start\":62886},{\"end\":63451,\"start\":63367},{\"end\":63862,\"start\":63769},{\"end\":64322,\"start\":64238},{\"end\":64671,\"start\":64628},{\"end\":64922,\"start\":64843}]"}}}, "year": 2023, "month": 12, "day": 17}