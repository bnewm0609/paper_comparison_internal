{"id": 247762034, "updated": "2023-10-05 15:49:19.345", "metadata": {"title": "ObjectFormer for Image Manipulation Detection and Localization", "authors": "[{\"first\":\"Junke\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zuxuan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Jingjing\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xintong\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Abhinav\",\"last\":\"Shrivastava\",\"middle\":[]},{\"first\":\"Ser-Nam\",\"last\":\"Lim\",\"middle\":[]},{\"first\":\"Yu-Gang\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recent advances in image editing techniques have posed serious challenges to the trustworthiness of multimedia data, which drives the research of image tampering detection. In this paper, we propose ObjectFormer to detect and localize image manipulations. To capture subtle manipulation traces that are no longer visible in the RGB domain, we extract high-frequency features of the images and combine them with RGB features as multimodal patch embeddings. Additionally, we use a set of learnable object prototypes as mid-level representations to model the object-level consistencies among different regions, which are further used to refine patch embeddings to capture the patch-level consistencies. We conduct extensive experiments on various datasets and the results verify the effectiveness of the proposed method, outperforming state-of-the-art tampering detection and localization methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.14681", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WangWCHSLJ22", "doi": "10.1109/cvpr52688.2022.00240"}}, "content": {"source": {"pdf_hash": "9b991f8e9a247c3ff0aeaae18354fb45fb079851", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.14681v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4a6192c281eea6b0969dc77613ea7eb680054b36", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9b991f8e9a247c3ff0aeaae18354fb45fb079851.txt", "contents": "\nObjectFormer for Image Manipulation Detection and Localization\n\n\nJunke Wang \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nShanghai Collaborative Innovation Center on Intelligent Visual Computing\n\n\nZuxuan Wu \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nShanghai Collaborative Innovation Center on Intelligent Visual Computing\n\n\nJingjing Chen \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nShanghai Collaborative Innovation Center on Intelligent Visual Computing\n\n\nXintong Han \nHuya Inc\n\n\nAbhinav Shrivastava \nUniversity of Maryland\n5 MetaAI\n\nSer-Nam Lim \nYu-Gang Jiang \nSchool of Computer Science\nShanghai Key Lab of Intelligent Information Processing\nFudan University\n\n\nShanghai Collaborative Innovation Center on Intelligent Visual Computing\n\n\nObjectFormer for Image Manipulation Detection and Localization\n\nRecent advances in image editing techniques have posed serious challenges to the trustworthiness of multimedia data, which drives the research of image tampering detection. In this paper, we propose ObjectFormer to detect and localize image manipulations. To capture subtle manipulation traces that are no longer visible in the RGB domain, we extract high-frequency features of the images and combine them with RGB features as multimodal patch embeddings. Additionally, we use a set of learnable object prototypes as mid-level representations to model the object-level consistencies among different regions, which are further used to refine patch embeddings to capture the patch-level consistencies. We conduct extensive experiments on various datasets and the results verify the effectiveness of the proposed method, outperforming state-of-the-art tampering detection and localization methods.\n\nIntroduction\n\nWith the rapid development of deep generative models like GANs [13,25,47] and VAEs [18,33], a multitude of image editing applications have become widely accessible to the public [10,20,29,38]. These editing tools make it easy and effective to produce photo-realistic images and videos that could be used for entertainment, interactive design, etc., which otherwise requires professional skills. However, there are growing concerns on the abuse of editing techniques to manipulate image and video content for malicious purposes. Therefore, it is crucial to develop effective image manipulation detection methods to examine whether images have been modified or not and identify regions in images that have been modified.\n\nImage manipulation techniques can be generally classified into three types: (1) splicing methods which copy regions from one image and paste to other images, (2) copy- \u2020Corresponding author. move which shifts the spatial locations of objects within images, and (3) removal methods which erase regions from images and inpaint missing regions with visually plausible contents. As shown in Figure 1, to produce semantically meaningful and perceptually convincing images, these approaches oftentimes manipulate images at the object-level, i.e., adding/removing objects in images. While there are some recent studies focusing on image manipulation detection [16,43,46], they typically use CNNs to directly map input images to binary labels (i.e., authentic/manipulated) without explicitly modeling object-level representations.\n\nIn contrast, we posit that image manipulation detection should not only examine whether certain pixels are out of distribution, but also consider whether objects are consistent with each other. In addition, visual artifacts brought by image editing that are no longer perceptible in the RGB domain are oftentimes noticeable in the frequency domain [6,31,39]. This demands a multimodal approach that jointly models the RGB domain and the frequency domain to discover subtle manipulation traces.\n\nIn this paper, we introduce ObjectFormer, a multimodal transformer framework for image manipulation detection and localization. ObjectFormer builds upon transformers due to their impressive performance on a variety of vision tasks like image classification [12,15,24], object detection [5,49], video classification [4,23,40,41], etc. More importantly, transformers are natural choices to model whether patches/pixels are consistent in images, given that they explore the correlations between different spatial locations using self-attention. Inspired by object queries that are automatically learned [1,49], we use a set of learnable parameters as object prototypes (serving as mid-level object representations) to discover the object-level consistencies, which are further leveraged to refine the patch embeddings for patch-level consistencies modeling.\n\nWith this in mind, ObjectFormer first converts an image from the RGB domain to the frequency domain using Discrete Cosine Transform and then extracts multimodal patch embeddings with a few convolutional layers. The RGB patch embeddings and the frequency patch embeddings are further concatenated to complement each other. Further, we use a set of learnable embeddings as object queries/prototypes, interacting with the derived patch embeddings to learn consistencies among different objects. We refine patch embeddings with these object prototypes with cross-attention. By iteratively doing so, ObjectFormer derives global feature representations that explicitly encode mid-level object features, which can be readily used to detect manipulation artifacts. Finally, the global features are used to predict whether images have been modified and the corresponding manipulation mask in a multi-task fashion. The framework can be trained in an end-to-end manner. We conduct experiments on commonly used image tampering datasets, including CASIA [11], Columbia [35], Coverage [42], NIST16 [27], and IMD20 [28]. The results demonstrate that ObjectFormer outperforms state-of-the-art tampering detection and localization methods. In summary, our work makes the following key contributions:\n\n\u2022 We introduce ObjectFormer, an end-to-end multimodal framework for image manipulation detection and localization, combining RGB features and frequency features to identify the tampering artifacts.\n\n\u2022 We explicitly leverage learnable object prototypes as mid-level representations to model object-level consistencies and refined patch embeddings to capture patchlevel consistencies.\n\n\u2022 We conduct extensive experiments on multiple benchmarks and demonstrate that our method achieves stateof-the-art detection and localization performance.\n\n\nRelated Work\n\nImage Manipulation Detection / Localization Most early studies focus on detecting a specific type of manipula-tion, e.g., splicing [8,17,19], copy-move [7,32], and removal [48]. However, in real-world scenarios, the exact manipulation type is unknown, which motivates a line of work focusing on general manipulation detection [3,16,43]. In addition, RGB-N [46] introduces a two-stream network for manipulation localization, where one stream extracts RGB features to capture visual artifacts, and the other stream leverages noise features to model the inconsistencies between tampered and untouched regions. SPAN [16] models the relationships of pixels within image patches on multiple scales through a pyramid structure of local self-attention blocks. PSCCNet [22] extracts hierarchical features with a top-down path and detects whether input image has been manipulated using a bottom-up path. In this work, we detect the manipulation artifacts by explicitly adopting a set of learnable embeddings as object prototypes for object-level consistency modeling and the refined patch embeddings for patch-level consistency modeling.\n\n\nVisual Transformer\n\nThe remarkable success of Transformers [37] and their variants in natural language processing has motivated a plethora of work exploring transformers for a variety of computer vision tasks due to their capabilities in modeling long-range dependencies. More specifically, ViT [12] reshapes an image into a sequence of flattened patches and inputs them to the transformer encoders for image classification. T2T [45] incorporates a Token-to-Token module to progressively aggregate local information before using self-attention layers. There are also some studies combining the self-attention blocks with classical convolutional neural networks. For instance, DETR [5] uses the pretrained CNN for extracting low-level features, which are then fed into a transformer-based encoder-decoder architecture for object detection. In contrast, we introduce the frequency information to facilitate the capture of subtle forgery traces, which are further combined with RGB modality features through a multi-modal transformer for image tampering detection.\n\n\nMethod\n\nOur goal is to detect manipulated objects within images by modeling visual consistencies among mid-level representations, which are automatically derived by attending to multimodal inputs. In this section, we introduce Object-Former, which consists of a High-frequency Feature Extraction Module (Section 3.1), an object encoder (Section 3.2) that uses learnable object queries to learn whether mid-level representations in images are coherent, and a patch decoder (Section 3.3) that produces refined global representations for manipulation detection and localization. Figure 2 gives an overview of the framework.\n\nMore formally, we denote an input image as X \u2208 R H\u00d7W \u00d73 , where H and W are the height and width of the  image, respectively. We first extract the feature map G r \u2208 R Hs\u00d7Ws\u00d7Cs and generate patch embeddings using a few convolutional layers, parameterized by g, for faster convergence as in [44].\n\n\nHigh-frequency Feature Extraction\n\nAs manipulated images are generally post-processed to hide tampering artifacts, it is difficult to capture subtle forgery traces in the RGB space. Therefore, we extract features from the frequency domain to provide complementary clues for manipulation detection.\n\nTaking the image X as input, ObjectFormer first transforms it from the RGB domain to the frequency domain using Discrete Cosine Transform (DCT):\nX q = D(X),(1)\nwhere X q \u2208 R H\u00d7W \u00d71 is the frequency domain representation and D denotes DCT. Then we obtain the highfrequency component through a high pass filter, and transform it back to RGB domain to preserve the shift invariance and local consistency of natural images:\nX h = D \u22121 (F(X q , \u03b1)),(2)\nwhere F denotes the high pass filter and \u03b1 is the manuallydesigned threshold which controls the low frequency component to be filtered out. After that, we input X h to several convolutional layers to extract frequency features G f , the size of which is the same with G r . We then generate spatial patches of the same sizes using G r and G f and further flatten them to a sequence of C-d vectors with the length of L. We concatenate the two sequences to obtain a multimodal patch vector p \u2208 R 2L\u00d7C . Sinusoidal positional embeddings [5] are added to p to provide positional information.\n\n\nObject Encoder\n\nThe object encoder aims to learn a group of mid-level representations automatically that attend to specific regions in G r /G f and identify whether these regions are consistent with each other. To this end, we use a set of learnable parameters o \u2208 R N \u00d7C as object prototypes, which are learned to represent objects that may appear in images. N is a manually designed constant value indicating the maximum number of objects, which we empirically set to 16 in this paper.\n\nSpecifically, given the object representations o i from the i-th layer, we first normalize it with Layer Normalization (LN) and use it as the query of the attention block. The patch embeddings p i after the normalization serve as the key and value. Note that we set p 0 = p, o 0 = o, respectively. Then we calculate the object-patch affinity matrix A i \u2208 R N \u00d7L with matrix multiplication and a softmax function:\nA i = softmax o i W eq \u00b7 (p i W ek ) T \u221a C ,(3)\nwhere W eq and W ek are learnable parameters of two linear projection layers. After that, we use another linear layer to project p i into value embedding, and further compute its weighted average with A i to obtain the attention matrix. Finally, the object representations are updated through a residual connection with the attention matrix to obtain\no i \u2208 R N \u00d7C :\u00f4 i = o i + A i \u00b7 p i W ev ,(4)\nwhere W ev is the learnable parameter for the value embedding layer. With this, each object representation can be injected with global contextual information from all locations. Then we further enable the interaction among different objects using a single linear projection:\no i =\u00f4 i + (\u00f4 T i W c ) T ,(5)\nwhere W c \u2208 R N \u00d7N is a learnable weight matrix. This essentially learns how different object prototypes interact with one another to discover object-level visual inconsistencies. Since the number of objects within an image varies, we additionally use linear projection layers and an activation function GELU [14] to enhance the object features. This process can be formulated as:\no i+1 = o i + \u03b4( o i W act1 )W act2 ,(6)\nwhere W act1 and W act2 are learnable parameters, \u03b4 is the GELU function, o i+1 is the updated object representation.\n\n\nPatch Decoder\n\nThe object encoder allows different objects within the images to interact with each other to model whether midlevel representations are visually coherent and attend to important patches. In addition to this, we use the updated object representations from the object encoder to further refine the patch embeddings. More specifically, we use p i as query, o i+1 as key and value, and enhance the patch features following classic attention paradigm. With this, each patch embedding can further absorb useful information from the derived object prototypes.\n\nMore specifically, we first adopt Layer Normalization to normalize both p i and o i+1 , and then feed them into an attention block for patch embeddings refinement. The complete process can be formulated as:\np i = p i + softmax p i W dq \u00b7 (o i+1 W dk ) T \u221a C \u00b7 o i+1 W dv , p i =p i + MLP(p i ),(7)\nwhere W dq , W dk , and W dv are the learnable parameters of three embedding layers, and MLP denotes a Multi-Layer Perceptron that has two linear mappings. After aggregating the mid-level object features into each patch within the images, we further apply a boundarysensitive contextual incoherence modeling (BCIM) module to detect pixel-level inconsistency for fine-grained feature modeling. In particular, we first reshape p i \u2208 R 2N \u00d7C to a 2D feature map P i with the size R Hs\u00d7Ws\u00d72Cs . We then calculate the similarity between each pixel and surrounding pixels within a local window:\nS ij = 1 k \u00d7 k j\u2208\u03ba Sim( P ij , P i k ),(8)\nwhere \u03ba denotes a small k \u00d7 k window in the feature map P i , P ij is the central feature vector of the window, and P i k is its neighboring feature vector within \u03ba. The similarity measurement function Sim that we use is cosine similarity. Then we compute the element-wise summation between S i \u2208 R Hs\u00d7Ws\u00d71 and P i to obtain a boundarysensitive feature map with the size of R Hs\u00d7Ws\u00d72Cs to obtain a boundary-sensitive feature map, and finally serialize it to patch embeddings p i+1 \u2208 R 2N \u00d7C .\n\nNote that we use stacked object encoders and image decoders in a sequential order for I times (which we set to 8 in this paper) to alternately update the object representations and patch features. Finally, we obtain p out \u2208 R 2N \u00d7C which contains visual consistency information at both the object-level and the patch-level. After that, we reshape it to a 2D feature map G out , which is then used for manipulation detection and localization.\n\n\nLoss Functions\n\nFor manipulation detection, we apply global average pooling on G out , and calculate the final binary prediction y using a fully connected layer. While for manipulation localization, we progressively upsample G out by alternating convolutional layers and linear interpolation operations to obtain a predicted maskM . Given the ground-truth label y and mask M , we train ObjectFormer with the following objective function:\nL = L cls (y,\u0177) + \u03bbL seg (M,M ),(9)\nwhere both L cls and L seg are binary cross-entropy loss, and \u03bb seg is a balancing hyperparameter. By default, we set \u03bb seg = 1.\n\n\nExperiments\n\nWe evaluate our models on two closely related tasks: manipulation localization and detection. In the former task, our goal is to localize the manipulated regions within the images. In the latter task, the goal is to classify images as being manipulated or authentic. Below, we introduce the experimental setup in Sec. 4.1, and present results in Sec. 4.2 -Sec. 4.4, and finally we perform an ablation study to justify the effectiveness of different components in Sec. 4.5, and show the visualization results in Sec. 4.6.\n\n\nExperimental Settings\n\nSynthesized Pre-training Data We synthesize a largescale image tampering dataset and pre-train our model on it. The synthesized dataset includes three subsets: 1) Fake-COCO, which is built on MS COCO [21]. Inspired by [46], we use the annotations provided by MS COCO to randomly copy and paste an object within the same image, or slice an object from one image to another. We also apply the Poisson Blending algorithm between source and target images to erase the slicing boundaries. 2) FakeParis, which is built on Paris StreetView [30] dataset. We erase a region from an authentic image, and adopt a state-of-the-art inpainting method Edgeconnnect [26] to restore visual contents within it. 3) Pristine images, i.e., the original images from the above datasets. We randomly add Gaussian noise or apply the JPEG compression algorithm to the generated data to resemble the visual quality of images in realistic scenarios.\n\nTesting Datasets We follow PSCCNet [22] to evaluate our model on CASIA [11] dataset, Columbia [35] dataset, Carvalho [42], Nist Nimble 2016 (NIST16) dataset [27], and IMD20 [28] dataset.\n\n\u2022 CASIA [11] provides spliced and copy-moved images of various objects. The tampered regions are carefully selected and some post-processing techniques like filtering and blurring are also applied. Ground-truth masks are obtained by binarizing the difference between tampered and original images.\n\n\u2022 Columbia [35] dataset focuses on splicing based on uncompressed images. Ground-truth masks are provided.\n\n\u2022 Coverage [42] dataset contains 100 images generated by copy-move techniques, the ground-truth masks are also available.\n\n\u2022 NIST16 [27] is a challenging dataset which contains all three tampering techniques. The manipulations in this dataset are post-processed to conceal visible traces. They provide ground-truth tampering mask for evaluation.\n\n\u2022 IMD20 [28], which collects 35,000 real images captured by different camera models and generates the same number of forged images using a large variety of Inpainting methods.\n\nTo fine-tune ObjectFormer, we use the same training/testing splits as [16,22] for fair comparisons.\n\n\nEvaluation Metrics\n\nWe evaluate the performance of the proposed method on both image manipulation detection task and localization task. For detection results, we use image-level Area Under Curve (AUC) and F1 score as our evaluation metric, while for localization, the pixel-level AUC and F1 score on manipulation masks are adopted. Since binary masks and detection scores are required to compute F1 scores, we adopt the Equal Error Rate (EER) threshold to binarize them.\n\nImplementation Details All images are resized to 256 \u00d7 256. For our backbone network, we use EfficientNet-b4 [36] pretrained on ImageNet [9]. We use Adam for optimization with a learning rate of 0.0001. We train the complete model for 90 epochs with a batch size of 24, and the learning rate is decayed by 10 times every 30 epochs.\n\nBaseline Models We compare our method with various baseline models as described below:\n\n\u2022 J-LSTM [2], which employs a hybrid CNN-LSTM architecture to capture the discriminative features between manipulated and non-manipulated regions within a tampered image.\n\n\u2022 H-LSTM [3], which segments the resampling features extracted by a CNN encoder into patches and adopts an LSTM network to model the transition between different patches for tampering localization.\n\n\u2022 RGB-N [46], which adopts an RGB stream and a noise stream in parallel to separately discover tampering features and noise inconsistency within an image.\n\n\u2022 ManTraNet [43], which uses a feature extractor to capture the manipulation traces and a local anomaly detection network to localize the manipulated regions.\n\n\u2022 SPAN [16], which leverages a pyramid architecture and models the dependency of image patches through selfattention blocks.\n\n\u2022 PSCCNet [22], which employs features at different scales progressively for image tampering localization in a coarse-to-fine manner.\n\n\nImage Manipulation Localization\n\nCompared with binary tampering detection task, manipulation localization is more challenging because it requires the models to capture more refined forgery features. Following SPAN [16] and PSCCNet [22], we compare our model with other state-of-the-art tampering localization methods under two settings: 1). training on the synthetic dataset and evaluating on the full test datasets. 2) fine-tuning the pre-trained model on the training split of test datasets and evaluating on their test split.\n\n\nPre-trained Model\n\nFor pre-trained model evaluation, we compare ObjectFormer with MantraNet [43], SPAN [16], and PSCCNet [22]. We report the AUC scores (%) in Table 1, from which we can observe ObjectFormer achieves the best localization performance on most datasets. Especially, ObjectFormer achieves 82.1% on the real-world dataset IMD20, and outperforms PSCCNet by 1.9%. This suggests our method owns the superior ability to capture tampering features, and can generalize well to high-quality manipulated images datasets. On Columbia dataset, we surpass SPAN and MaTraNet by 2.0% and 15.9%, but are 2.7% behind PSCCNet. We argue that the reason might be their synthesized training data closely resemble the distribution of the Columbia dataset. This can be further verified by the results in Table 2, which demonstrates ObjectFormer outperforms PSCCNet in both AUC and F1 scores if the model is fine-tuned on Columbia dataset. Moreover, it is worth pointing out ObjectFormer achieves decent results using less pre-training data compared with other methods.\n\n\nFine-tuned Model\n\nTo compensate for the difference in visual quality between the synthesized datasets and standard datasets, we further fine-tune the pre-trained models on spe-  Table 2. Comparison of manipulation localization results using fine-tuned models.\n\ncific datasets, and compare with other methods in Table 2.\n\nWe can observe significant performance gains, which illustrates that ObjectFormer could capture subtle tampering artifacts through the object-level and patch-level consistency modeling and the multimodal design.\n\n\nImage Manipulation Detection\n\nSince most previous studies do not consider tampering detection task, we evaluate our model on CASIA-D introduced by [22]. Table 3 shows the AUC and F1 scores (%) for detecting manipulated images. The results demonstrate that our model achieves state-of-the-art performance, i.e., 99.70% in terms of AUC and 97.34% in F1, which demonstrates the effectiveness of our method to capture manipulation artifacts.  Table 3. AUC and F1 scores (%) of tampering detection results on CASIA-D dataset [22]. The best results are marked in bold.\n\n\nMethod\n\n\nRobustness Evaluation\n\nFollowing PSCCNet [22], we apply different image distortion methods on raw images from NIST16 dataset and evaluate the robustness of our ObjectFormer. The distortion types include: 1) image scaling with different scales, 2) Gaussian blurring with a kernel size k, 3) Gaussian noise with a standard deviation \u03c3, and 4) JPEG compression with a quality factor q. We compare the manipulation localization performance (AUC scores) of our pretrained models with SPAN [16] and PSCCNet [22] on these corrupted data, and report the results in Table 4. Object-Former demonstrates better robustness against various distortion techniques, especially on compressed images (1.1% higher than PSCCNet when the quality factor is 100, and 1.0% higher when the quality factor is 50). \n\n\nAblation Analysis\n\nThe High-frequency Feature Extraction (HFE) module of our method is designed to extract the abnormal forgery features in the frequency domain, while the Boundarysensitivity Contextual Incoherence Modeling (BCIM) module is utilized to improve the sharpness of the predicted tampering masks. To evaluate the effectiveness of HFE and BCIM, we remove them separately from ObjectFormer and evaluate the tampering localization performance on CASIA and NIST16 dataset.\n\nThe quantitative results are listed in Table 5. We can observe that without HFE, the AUC scores decrease by 14.6% on CASIA and 11.0% on NIST16, while without BCIM, the AUC scores decrease by 6.2% on CASIA and 2.4% on NIST16. The performance degradation validates that the use of HFE and BCIM effectively improves the performance of our model. Moreover, to illustrate the effectiveness of representations learned by ObjectFormer, we discard the object representations and replace the stacked object encoders and image decoders with vanilla self-attention blocks. We can observe a significant performance degradation in the third row of    sual elements that may appear in an image, which facilitates ObjectFormer to learn the mid-level semantic features for object-level consistencies modeling. We further conduct experiments to investigate the effect of the number of prototypes (N ) on model performance. As shown in Figure 3, there is an overall incremental trend in the tampering location performance as the number of prototypes increases, and the best are achieved on Columbia and CASIA dataset when N is set to 16.\n\n\nVisualization Results\n\nVisualization of object encoder. We further investigate the behavior of ObjectFormer qualitatively. Specifically, we average all heads of the affinity matrix A i (Eqn. 3) in the first object encoder, and then normalize it to [0, 255]. For each image, we visualize the pristine image (column 1), and regions that are attended to by different object prototypes, e.g., column 2 and 3 are two prototypes correspond to two foreground objects while column 4 and 5 relate to back-ground objects. The results in Figure 4 suggest that through iterative updates, the object representations correspond to meaningful regions in the images, thus contributing to object consistency modeling.\n\nQualitative results. We provide predicted manipulation masks of different methods in Figure 5. Since the source code of PSCCNet [22] is not available, their predictions are not available. The results demonstrate that our method could not only locate the tampering regions more accurately, but also develop more sharp boundaries, which benefits from the inconsistency modeling ability and boundaries sensitivity of ObjectFormer.\n\nVisualization of high-frequency features. To verify the usefulness of frequency features for tampering detection, we visualize the high-frequency components and HFE features using GradCAM [34] (Sec. 3.1) in Figure 6. The results demonstrate that although the forged images are visually natural, the manipulated regions are distinguishable \n\n\nLimitation\n\nObjectFormer faces one potential limitation: when using the pre-trained model to evaluate the performance of tampering localization on Columbia, ObjectFormer is 2.7% lower than PSCCNet [22] in terms of AUC score. The possible reason might be that the pre-training data they use closely resemble the data distribution in the Columbia dataset. Therefore, we believe this problem can be resolved by using more pre-training data.\n\n\nConclusion\n\nWe introduced ObjectFormer, an end-to-end multimodal framework for image tampering detection and localization.\n\nTo detect subtle manipulation artifacts that are no longer visible in RGB domain, ObjectFormer extracts forgery features in the frequency domain as complementary information, which are further combined with the RGB features to generate multimodal patch embeddings. Additionally, ObjectFormer leverages learnable object prototypes as midlevel representations, and alternately updates the object prototypes and patch embeddings with stacked object encoders and patch decoders to model the object-level and patch-level visual consistencies within the images. Extensive experiments on different datasets demonstrate the effectiveness of the proposed method.\n\nFigure 1 .\n1Tampering images usually contain manipulated objects. Thus, exploiting object-level consistency is crucial for manipulation detection.\n\nFigure 2 .\n2An overview of ObjectFormer. The input is a suspicious image (H \u00d7 W \u00d7 3), and the output includes a tampering localization result and a predicted mask (H \u00d7 W \u00d7 1), which localizes the manipulation regions.\n\nFigure 3 .\n3AUC scores (%) of ObjectFormer with different numbers of object prototypes.\n\nFigure 4 .\n4Visualization of the affinity matrix Ai in the first object encoder. From left to right, we display the forged images, two prototypes corresponding to two foreground objects, and two prototypes related to background objects.\n\nFigure 5 .Figure 6 .\n56Visualization of the predicted manipulation mask by different methods. From top to bottom, we show forged images, GT masks, predictions of ManTraNet, SPAN, and ours. Visualization of the frequency features. From left to right, we display the forged images, masks, the high-frequency components, GradCAM of the feature maps after conv layer, and ObjectFormer predictions. from the untouched areas in frequency domain.\n\n\nTable 4. Localization performance on NIST16 dataset under various distortions. AUC scores are reported (in %).Distortion \nSPAN PSCCNet \nOurs \n\nno distortion \n83.95 85.47 \n87.18 \n\nResize (0.78\u00d7) \n83.24 85.29 87.17 \u21930.01 \nResize (0.25\u00d7) \n80.32 85.01 86.33 \u21930.85 \n\nGaussianBlur (k=3) \n83.10 85.38 85.97 \u21931.21 \nGaussianBlur (k=15) \n79.15 79.93 80.26 \u21936.92 \n\nGaussianNoise (\u03c3=3) \n75.17 78.42 79.58 \u21937.60 \nGaussianNoise (\u03c3=15) 67.28 76.65 78.15 \u21939.03 \n\nJPEGCompress (q=100) 83.59 85.40 86.37 \u21930.81 \nJPEGCompress (q=50) 80.68 85.37 86.24 \u21930.94 \n\n\n\nTable 5 ,\n5i.e., 5% in terms of AUC and 12.5% in terms of F1 on NIST16 dataset.The object prototypes are deployed to represent the vi-Variants \n\nCASIA \nNIST16 \n\nAUC \nF1 \nAUC \nF1 \n\nBaseline (EfficientNet-b4) \n70.3 \n37.2 \n77.4 \n51.9 \n\nw/o HFE \n75.3 \n51.5 \n88.6 \n63.9 \n\nw/o BCIM \n82.7 \n40.1 \n97.2 \n74.5 \n\nvanilla self-attention \n84.8 \n47.6 \n94.5 \n72.1 \n\nOurs \n88.2 \n57.9 \n99.6 \n82.4 \n\n4 8 12 16 20 24 28 32 \nNumber of prototypes \n\n75.0 \n\n80.0 \n\n85.0 \n\n90.0 \n\n95.0 \n\n100.0 \n\nAUC \n\nCASIA \nColumbia \nIMD20 \n\n\n\nTable 5 .\n5Ablation results on CASIA and NIST16 dataset using different variants of ObjectFormer. Both AUC and F1 scores (%) are reported.\n\nSong Bai, Philip Torr, arXiv:2107.05790Visual parser: Representing part-whole hierarchies with transformers. arXiv preprintSong Bai, Philip Torr, et al. Visual parser: Representing part-whole hierarchies with transformers. arXiv preprint arXiv:2107.05790, 2021. 2\n\nExploiting spatial structure for localizing manipulated image regions. H Jawadul, Bappy, K Amit, Jason Roy-Chowdhury, Lakshmanan Bunk, B S Nataraj, Manjunath, ICCV. Jawadul H Bappy, Amit K Roy-Chowdhury, Jason Bunk, Lakshmanan Nataraj, and BS Manjunath. Exploiting spa- tial structure for localizing manipulated image regions. In ICCV, 2017. 5\n\nHybrid lstm and encoder-decoder architecture for detection of image forgeries. H Jawadul, Cody Bappy, Lakshmanan Simons, Nataraj, Amit K Roy-Chowdhury Bs Manjunath, TIP. 25Jawadul H Bappy, Cody Simons, Lakshmanan Nataraj, BS Manjunath, and Amit K Roy-Chowdhury. Hybrid lstm and encoder-decoder architecture for detection of image forg- eries. TIP, 2019. 2, 5\n\nIs space-time attention all you need for video understanding. Gedas Bertasius, Heng Wang, Lorenzo Torresani, ICML, 2021. Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, 2021. 2\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, ECCV, 2020. 23Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In ECCV, 2020. 2, 3\n\nLocal relation learning for face forgery detection. Taiping Shen Chen, Yang Yao, Shouhong Chen, Jilin Ding, Rongrong Li, Ji, AAAI. Shen Chen, Taiping Yao, Yang Chen, Shouhong Ding, Jilin Li, and Rongrong Ji. Local relation learning for face forgery detection. In AAAI, 2021. 1\n\nEfficient dense-field copy-move forgery detection. Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva, TIFS. 2Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. Ef- ficient dense-field copy-move forgery detection. TIFS, 2015. 2\n\nSplicebuster: A new blind image splicing detector. Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva, WIFS. Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. Splicebuster: A new blind image splicing detector. In WIFS, 2015. 2\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5\n\nSemantic image manipulation using scene graphs. Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, D Gregory, Federico Hager, Christian Tombari, Rupprecht, CVPR. 2020Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D Hager, Federico Tombari, and Christian Rup- precht. Semantic image manipulation using scene graphs. In CVPR, 2020. 1\n\nCasia image tampering detection evaluation database. Jing Dong, Wei Wang, Tieniu Tan, ChinaSIP. 5Jing Dong, Wei Wang, and Tieniu Tan. Casia image tam- pering detection evaluation database. In ChinaSIP, 2013. 2, 5\n\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 1\n\nGaussian error linear units (gelus). Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 4\n\nRethinking spatial dimensions of vision transformers. Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, Seong Joon Oh, ICCV. Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spa- tial dimensions of vision transformers. In ICCV, 2021. 2\n\nSpan: Spatial pyramid attention network for image manipulation localization. Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaudhuri, Zhenheng Yang, Ram Nevatia, ECCV. 6Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaudhuri, Zhenheng Yang, and Ram Nevatia. Span: Spatial pyramid attention network for image manipulation localiza- tion. In ECCV, 2020. 1, 2, 5, 6\n\nFighting fake news: Image splice detection via learned self-consistency. Minyoung Huh, Andrew Liu, Andrew Owens, Alexei A Efros, In ECCV. 2Minyoung Huh, Andrew Liu, Andrew Owens, and Alexei A Efros. Fighting fake news: Image splice detection via learned self-consistency. In ECCV, 2018. 2\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintDiederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 1\n\nThe point where reality meets fantasy: Mixed adversarial generators for image splice detection. Vladimir Vladimir V Kniaz, Fabio Knyaz, Remondino, Vladimir V Kniaz, Vladimir Knyaz, and Fabio Remondino. The point where reality meets fantasy: Mixed adversarial generators for image splice detection. 2019. 2\n\nManigan: Text-guided image manipulation. Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip Hs Torr, CVPR. Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr. Manigan: Text-guided image manipulation. In CVPR, 2020. 1\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 4\n\nPscc-net: Progressive spatio-channel correlation network for image manipulation detection and localization. Xiaohong Liu, Yaojie Liu, Jun Chen, Xiaoming Liu, arXiv:2103.105967arXiv preprintXiaohong Liu, Yaojie Liu, Jun Chen, and Xiaoming Liu. Pscc-net: Progressive spatio-channel correlation network for image manipulation detection and localization. arXiv preprint arXiv:2103.10596, 2021. 2, 5, 6, 7, 8\n\n. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu, arXiv:2106.13230arXiv preprintVideo swin transformerZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021. 2\n\nAdavit: Adaptive vision transformers for efficient image recognition. Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, Ser-Nam Lim, CVPR. 2022Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. In CVPR, 2022. 2\n\nMehdi Mirza, Simon Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintMehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 1\n\nEdgeconnect: Structure guided image inpainting using edge prediction. Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, Mehran Ebrahimi, ICCVW. Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, and Mehran Ebrahimi. Edgeconnect: Structure guided image in- painting using edge prediction. In ICCVW, 2019. 4\n\n. Nist, 25Nist. Nimble 2016 datasets. https://www.nist. gov/itl/iad/mig/nimble-challenge-2017- evaluation, 2016. 2, 5\n\nImd2020: A large-scale annotated dataset tailored for detecting manipulated images. Adam Novozamsky, Babak Mahdian, Stanislav Saic, WACVW, 2020. 25Adam Novozamsky, Babak Mahdian, and Stanislav Saic. Imd2020: A large-scale annotated dataset tailored for detect- ing manipulated images. In WACVW, 2020. 2, 5\n\nSwapping autoencoder for deep image manipulation. Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A Efros, Richard Zhang, NIPS. 2020Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation. In NIPS, 2020. 1\n\nContext encoders: Feature learning by inpainting. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros, CVPR. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 4\n\nThinking in frequency: Face forgery detection by mining frequency-aware clues. Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, Jing Shao, ECCV. 2020Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. Thinking in frequency: Face forgery detection by min- ing frequency-aware clues. In ECCV, 2020. 1\n\nA deep learning approach to detection of splicing and copy-move forgeries in images. Yuan Rao, Jiangqun Ni, WIFS. Yuan Rao and Jiangqun Ni. A deep learning approach to detection of splicing and copy-move forgeries in images. In WIFS, 2016. 2\n\nGenerating diverse high-resolution images with vq-vae. Ali Razavi, Aaron Van Den Oord, Oriol Vinyals, ICLR Workshop. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gener- ating diverse high-resolution images with vq-vae. In ICLR Workshop, 2019. 1\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, ICCV. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017. 7\n\nNormalized cuts and image segmentation. Jianbo Shi, Jitendra Malik, TPAMI. 25Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. TPAMI, 2000. 2, 5\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, ICML. Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. 5\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 2\n\nYael Vinker, Eliahu Horwitz, Nir Zabari, Yedid Hoshen, arXiv:2007.01289Deep single image manipulation. arXiv preprintYael Vinker, Eliahu Horwitz, Nir Zabari, and Yedid Hoshen. Deep single image manipulation. arXiv preprint arXiv:2007.01289, 2020. 1\n\nM2tr: Multi-modal multi-scale transformers for deepfake detection. Junke Wang, Zuxuan Wu, Jingjing Chen, Yu-Gang Jiang, arXiv:2104.09770arXiv preprintJunke Wang, Zuxuan Wu, Jingjing Chen, and Yu-Gang Jiang. M2tr: Multi-modal multi-scale transformers for deep- fake detection. arXiv preprint arXiv:2104.09770, 2021. 1\n\nEfficient video transformers with spatialtemporal token selection. Junke Wang, Xitong Yang, Hengduo Li, Zuxuan Wu, Yu-Gang Jiang, arXiv:2111.11591arXiv preprintJunke Wang, Xitong Yang, Hengduo Li, Zuxuan Wu, and Yu-Gang Jiang. Efficient video transformers with spatial- temporal token selection. arXiv preprint arXiv:2111.11591, 2021. 2\n\nBevt: Bert pretraining of video transformers. Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, Lu Yuan, CVPR. 2022Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers. In CVPR, 2022. 2\n\nCoverage-a novel database for copy-move forgery detection. Bihan Wen, Ye Zhu, Ramanathan Subramanian, Tian-Tsong Ng, Xuanjing Shen, Stefan Winkler, ICIP. 25Bihan Wen, Ye Zhu, Ramanathan Subramanian, Tian-Tsong Ng, Xuanjing Shen, and Stefan Winkler. Coverage-a novel database for copy-move forgery detection. In ICIP, 2016. 2, 5\n\nMantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. Yue Wu, Wael Abdalmageed, Premkumar Natarajan, CVPR. Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan. Mantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. In CVPR, 2019. 1, 2, 5\n\nEarly convolutions help transformers see better. Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor Darrell, Ross Girshick, NIPS. Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor Darrell, and Ross Girshick. Early convolutions help trans- formers see better. In NIPS, 2021. 3\n\nTokens-to-token vit: Training vision transformers from scratch on imagenet. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, E H Francis, Jiashi Tay, Shuicheng Feng, Yan, ICCV. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV, 2021. 2\n\nLearning rich features for image manipulation detection. Peng Zhou, Xintong Han, I Vlad, Larry S Morariu, Davis, CVPR. Peng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis. Learning rich features for image manipulation detection. In CVPR, 2018. 1, 2, 4, 5\n\nUnpaired image-to-image translation using cycleconsistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, ICCV. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In ICCV, 2017. 1\n\nA deep learning approach to patch-based image inpainting forensics. Xinshan Zhu, Yongjun Qian, Xianfeng Zhao, Biao Sun, Ya Sun, Signal Processing: Image Communication. 2Xinshan Zhu, Yongjun Qian, Xianfeng Zhao, Biao Sun, and Ya Sun. A deep learning approach to patch-based image in- painting forensics. Signal Processing: Image Communica- tion, 2018. 2\n\nDeformable {detr}: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, ICLR. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformable transform- ers for end-to-end object detection. In ICLR, 2021. 2\n", "annotations": {"author": "[{\"end\":253,\"start\":66},{\"end\":440,\"start\":254},{\"end\":631,\"start\":441},{\"end\":655,\"start\":632},{\"end\":709,\"start\":656},{\"end\":722,\"start\":710},{\"end\":913,\"start\":723}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":72},{\"end\":263,\"start\":261},{\"end\":454,\"start\":450},{\"end\":643,\"start\":640},{\"end\":675,\"start\":664},{\"end\":721,\"start\":718},{\"end\":736,\"start\":731}]", "author_first_name": "[{\"end\":71,\"start\":66},{\"end\":260,\"start\":254},{\"end\":449,\"start\":441},{\"end\":639,\"start\":632},{\"end\":663,\"start\":656},{\"end\":717,\"start\":710},{\"end\":730,\"start\":723}]", "author_affiliation": "[{\"end\":177,\"start\":78},{\"end\":252,\"start\":179},{\"end\":364,\"start\":265},{\"end\":439,\"start\":366},{\"end\":555,\"start\":456},{\"end\":630,\"start\":557},{\"end\":654,\"start\":645},{\"end\":708,\"start\":677},{\"end\":837,\"start\":738},{\"end\":912,\"start\":839}]", "title": "[{\"end\":63,\"start\":1},{\"end\":976,\"start\":914}]", "venue": null, "abstract": "[{\"end\":1872,\"start\":978}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1955,\"start\":1951},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1958,\"start\":1955},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1961,\"start\":1958},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1975,\"start\":1971},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1978,\"start\":1975},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2070,\"start\":2066},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2073,\"start\":2070},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2076,\"start\":2073},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2079,\"start\":2076},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3265,\"start\":3261},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3268,\"start\":3265},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3271,\"start\":3268},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3783,\"start\":3780},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3786,\"start\":3783},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3789,\"start\":3786},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4188,\"start\":4184},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4191,\"start\":4188},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4194,\"start\":4191},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4216,\"start\":4213},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4219,\"start\":4216},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4245,\"start\":4242},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4248,\"start\":4245},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4251,\"start\":4248},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4254,\"start\":4251},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4530,\"start\":4527},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4533,\"start\":4530},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5828,\"start\":5824},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5843,\"start\":5839},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5858,\"start\":5854},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5871,\"start\":5867},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5887,\"start\":5883},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6756,\"start\":6753},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6759,\"start\":6756},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6762,\"start\":6759},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6777,\"start\":6774},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6780,\"start\":6777},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6798,\"start\":6794},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6951,\"start\":6948},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6954,\"start\":6951},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6957,\"start\":6954},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6982,\"start\":6978},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7238,\"start\":7234},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7386,\"start\":7382},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7815,\"start\":7811},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8051,\"start\":8047},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8185,\"start\":8181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8436,\"start\":8433},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9731,\"start\":9727},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11019,\"start\":11016},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13038,\"start\":13034},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17072,\"start\":17068},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":17090,\"start\":17086},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17405,\"start\":17401},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17522,\"start\":17518},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17830,\"start\":17826},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17866,\"start\":17862},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17889,\"start\":17885},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17912,\"start\":17908},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17952,\"start\":17948},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17968,\"start\":17964},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17991,\"start\":17987},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18292,\"start\":18288},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18400,\"start\":18396},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18521,\"start\":18517},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18744,\"start\":18740},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18983,\"start\":18979},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18986,\"start\":18983},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19596,\"start\":19592},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19623,\"start\":19620},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19916,\"start\":19913},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20088,\"start\":20085},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20287,\"start\":20283},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20447,\"start\":20443},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20602,\"start\":20598},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20731,\"start\":20727},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21071,\"start\":21067},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21088,\"start\":21084},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21480,\"start\":21476},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21491,\"start\":21487},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21509,\"start\":21505},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23132,\"start\":23128},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23505,\"start\":23501},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23600,\"start\":23596},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24043,\"start\":24039},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24060,\"start\":24056},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26784,\"start\":26780},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27273,\"start\":27269},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27624,\"start\":27620}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28788,\"start\":28641},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29007,\"start\":28789},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29096,\"start\":29008},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29334,\"start\":29097},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29775,\"start\":29335},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30317,\"start\":29776},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30821,\"start\":30318},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30961,\"start\":30822}]", "paragraph": "[{\"end\":2606,\"start\":1888},{\"end\":3430,\"start\":2608},{\"end\":3925,\"start\":3432},{\"end\":4781,\"start\":3927},{\"end\":6065,\"start\":4783},{\"end\":6264,\"start\":6067},{\"end\":6449,\"start\":6266},{\"end\":6605,\"start\":6451},{\"end\":7749,\"start\":6622},{\"end\":8813,\"start\":7772},{\"end\":9436,\"start\":8824},{\"end\":9732,\"start\":9438},{\"end\":10032,\"start\":9770},{\"end\":10178,\"start\":10034},{\"end\":10453,\"start\":10194},{\"end\":11069,\"start\":10482},{\"end\":11559,\"start\":11088},{\"end\":11973,\"start\":11561},{\"end\":12372,\"start\":12022},{\"end\":12693,\"start\":12419},{\"end\":13105,\"start\":12725},{\"end\":13264,\"start\":13147},{\"end\":13834,\"start\":13282},{\"end\":14042,\"start\":13836},{\"end\":14722,\"start\":14134},{\"end\":15258,\"start\":14766},{\"end\":15701,\"start\":15260},{\"end\":16141,\"start\":15720},{\"end\":16306,\"start\":16178},{\"end\":16842,\"start\":16322},{\"end\":17789,\"start\":16868},{\"end\":17977,\"start\":17791},{\"end\":18275,\"start\":17979},{\"end\":18383,\"start\":18277},{\"end\":18506,\"start\":18385},{\"end\":18730,\"start\":18508},{\"end\":18907,\"start\":18732},{\"end\":19008,\"start\":18909},{\"end\":19481,\"start\":19031},{\"end\":19814,\"start\":19483},{\"end\":19902,\"start\":19816},{\"end\":20074,\"start\":19904},{\"end\":20273,\"start\":20076},{\"end\":20429,\"start\":20275},{\"end\":20589,\"start\":20431},{\"end\":20715,\"start\":20591},{\"end\":20850,\"start\":20717},{\"end\":21381,\"start\":20886},{\"end\":22443,\"start\":21403},{\"end\":22705,\"start\":22464},{\"end\":22765,\"start\":22707},{\"end\":22978,\"start\":22767},{\"end\":23543,\"start\":23011},{\"end\":24343,\"start\":23578},{\"end\":24826,\"start\":24365},{\"end\":25947,\"start\":24828},{\"end\":26650,\"start\":25973},{\"end\":27079,\"start\":26652},{\"end\":27420,\"start\":27081},{\"end\":27860,\"start\":27435},{\"end\":27985,\"start\":27875},{\"end\":28640,\"start\":27987}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10193,\"start\":10179},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10481,\"start\":10454},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12021,\"start\":11974},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12418,\"start\":12373},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12724,\"start\":12694},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13146,\"start\":13106},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14133,\"start\":14043},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14765,\"start\":14723},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16177,\"start\":16142}]", "table_ref": "[{\"end\":22186,\"start\":22179},{\"end\":22631,\"start\":22624},{\"end\":22764,\"start\":22757},{\"end\":23141,\"start\":23134},{\"end\":23427,\"start\":23420},{\"end\":24119,\"start\":24112},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24874,\"start\":24867}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1886,\"start\":1874},{\"attributes\":{\"n\":\"2.\"},\"end\":6620,\"start\":6608},{\"end\":7770,\"start\":7752},{\"attributes\":{\"n\":\"3.\"},\"end\":8822,\"start\":8816},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9768,\"start\":9735},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11086,\"start\":11072},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13280,\"start\":13267},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15718,\"start\":15704},{\"attributes\":{\"n\":\"4.\"},\"end\":16320,\"start\":16309},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16866,\"start\":16845},{\"end\":19029,\"start\":19011},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20884,\"start\":20853},{\"end\":21401,\"start\":21384},{\"end\":22462,\"start\":22446},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23009,\"start\":22981},{\"end\":23552,\"start\":23546},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23576,\"start\":23555},{\"attributes\":{\"n\":\"4.5.\"},\"end\":24363,\"start\":24346},{\"attributes\":{\"n\":\"4.6.\"},\"end\":25971,\"start\":25950},{\"attributes\":{\"n\":\"4.7.\"},\"end\":27433,\"start\":27423},{\"attributes\":{\"n\":\"5.\"},\"end\":27873,\"start\":27863},{\"end\":28652,\"start\":28642},{\"end\":28800,\"start\":28790},{\"end\":29019,\"start\":29009},{\"end\":29108,\"start\":29098},{\"end\":29356,\"start\":29336},{\"end\":30328,\"start\":30319},{\"end\":30832,\"start\":30823}]", "table": "[{\"end\":30317,\"start\":29888},{\"end\":30821,\"start\":30453}]", "figure_caption": "[{\"end\":28788,\"start\":28654},{\"end\":29007,\"start\":28802},{\"end\":29096,\"start\":29021},{\"end\":29334,\"start\":29110},{\"end\":29775,\"start\":29359},{\"end\":29888,\"start\":29778},{\"end\":30453,\"start\":30330},{\"end\":30961,\"start\":30834}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3003,\"start\":2995},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9400,\"start\":9392},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25754,\"start\":25746},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26485,\"start\":26477},{\"end\":26745,\"start\":26737},{\"end\":27296,\"start\":27288}]", "bib_author_first_name": "[{\"end\":30967,\"start\":30963},{\"end\":30979,\"start\":30973},{\"end\":31300,\"start\":31299},{\"end\":31318,\"start\":31317},{\"end\":31330,\"start\":31325},{\"end\":31356,\"start\":31346},{\"end\":31364,\"start\":31363},{\"end\":31366,\"start\":31365},{\"end\":31653,\"start\":31652},{\"end\":31667,\"start\":31663},{\"end\":31685,\"start\":31675},{\"end\":31723,\"start\":31703},{\"end\":32000,\"start\":31995},{\"end\":32016,\"start\":32012},{\"end\":32030,\"start\":32023},{\"end\":32238,\"start\":32231},{\"end\":32256,\"start\":32247},{\"end\":32271,\"start\":32264},{\"end\":32289,\"start\":32282},{\"end\":32308,\"start\":32299},{\"end\":32325,\"start\":32319},{\"end\":32589,\"start\":32582},{\"end\":32605,\"start\":32601},{\"end\":32619,\"start\":32611},{\"end\":32631,\"start\":32626},{\"end\":32646,\"start\":32638},{\"end\":32865,\"start\":32859},{\"end\":32885,\"start\":32877},{\"end\":32898,\"start\":32893},{\"end\":33097,\"start\":33091},{\"end\":33117,\"start\":33109},{\"end\":33130,\"start\":33125},{\"end\":33328,\"start\":33325},{\"end\":33338,\"start\":33335},{\"end\":33352,\"start\":33345},{\"end\":33367,\"start\":33361},{\"end\":33375,\"start\":33372},{\"end\":33382,\"start\":33380},{\"end\":33594,\"start\":33588},{\"end\":33607,\"start\":33602},{\"end\":33620,\"start\":33617},{\"end\":33634,\"start\":33628},{\"end\":33643,\"start\":33642},{\"end\":33661,\"start\":33653},{\"end\":33678,\"start\":33669},{\"end\":33948,\"start\":33944},{\"end\":33958,\"start\":33955},{\"end\":33971,\"start\":33965},{\"end\":34237,\"start\":34231},{\"end\":34256,\"start\":34251},{\"end\":34273,\"start\":34264},{\"end\":34290,\"start\":34286},{\"end\":34311,\"start\":34304},{\"end\":34324,\"start\":34318},{\"end\":34345,\"start\":34338},{\"end\":34364,\"start\":34356},{\"end\":34380,\"start\":34375},{\"end\":34733,\"start\":34730},{\"end\":34750,\"start\":34746},{\"end\":34771,\"start\":34766},{\"end\":34783,\"start\":34779},{\"end\":34793,\"start\":34788},{\"end\":34815,\"start\":34808},{\"end\":34828,\"start\":34823},{\"end\":34846,\"start\":34840},{\"end\":35077,\"start\":35074},{\"end\":35094,\"start\":35089},{\"end\":35306,\"start\":35298},{\"end\":35319,\"start\":35312},{\"end\":35333,\"start\":35325},{\"end\":35347,\"start\":35339},{\"end\":35360,\"start\":35354},{\"end\":35377,\"start\":35367},{\"end\":35634,\"start\":35627},{\"end\":35645,\"start\":35639},{\"end\":35659,\"start\":35653},{\"end\":35676,\"start\":35667},{\"end\":35696,\"start\":35688},{\"end\":35706,\"start\":35703},{\"end\":36003,\"start\":35995},{\"end\":36015,\"start\":36009},{\"end\":36027,\"start\":36021},{\"end\":36041,\"start\":36035},{\"end\":36043,\"start\":36042},{\"end\":36246,\"start\":36245},{\"end\":36260,\"start\":36257},{\"end\":36522,\"start\":36514},{\"end\":36546,\"start\":36541},{\"end\":36771,\"start\":36766},{\"end\":36784,\"start\":36776},{\"end\":36795,\"start\":36789},{\"end\":36818,\"start\":36809},{\"end\":37004,\"start\":36996},{\"end\":37017,\"start\":37010},{\"end\":37030,\"start\":37025},{\"end\":37046,\"start\":37041},{\"end\":37059,\"start\":37053},{\"end\":37072,\"start\":37068},{\"end\":37087,\"start\":37082},{\"end\":37106,\"start\":37096},{\"end\":37423,\"start\":37415},{\"end\":37435,\"start\":37429},{\"end\":37444,\"start\":37441},{\"end\":37459,\"start\":37451},{\"end\":37716,\"start\":37714},{\"end\":37725,\"start\":37722},{\"end\":37735,\"start\":37732},{\"end\":37747,\"start\":37741},{\"end\":37758,\"start\":37753},{\"end\":37773,\"start\":37766},{\"end\":37782,\"start\":37779},{\"end\":38060,\"start\":38052},{\"end\":38074,\"start\":38067},{\"end\":38087,\"start\":38079},{\"end\":38099,\"start\":38094},{\"end\":38111,\"start\":38105},{\"end\":38123,\"start\":38116},{\"end\":38138,\"start\":38131},{\"end\":38343,\"start\":38338},{\"end\":38356,\"start\":38351},{\"end\":38627,\"start\":38621},{\"end\":38640,\"start\":38636},{\"end\":38649,\"start\":38645},{\"end\":38664,\"start\":38658},{\"end\":38680,\"start\":38674},{\"end\":39070,\"start\":39066},{\"end\":39088,\"start\":39083},{\"end\":39107,\"start\":39098},{\"end\":39346,\"start\":39339},{\"end\":39360,\"start\":39353},{\"end\":39372,\"start\":39366},{\"end\":39386,\"start\":39379},{\"end\":39394,\"start\":39391},{\"end\":39412,\"start\":39406},{\"end\":39414,\"start\":39413},{\"end\":39429,\"start\":39422},{\"end\":39674,\"start\":39668},{\"end\":39690,\"start\":39683},{\"end\":39707,\"start\":39703},{\"end\":39723,\"start\":39717},{\"end\":39739,\"start\":39733},{\"end\":39741,\"start\":39740},{\"end\":39993,\"start\":39987},{\"end\":40006,\"start\":40000},{\"end\":40014,\"start\":40012},{\"end\":40028,\"start\":40022},{\"end\":40039,\"start\":40035},{\"end\":40307,\"start\":40303},{\"end\":40321,\"start\":40313},{\"end\":40519,\"start\":40516},{\"end\":40533,\"start\":40528},{\"end\":40553,\"start\":40548},{\"end\":40796,\"start\":40795},{\"end\":40817,\"start\":40810},{\"end\":40837,\"start\":40829},{\"end\":40859,\"start\":40848},{\"end\":40869,\"start\":40865},{\"end\":40885,\"start\":40880},{\"end\":41162,\"start\":41156},{\"end\":41176,\"start\":41168},{\"end\":41365,\"start\":41357},{\"end\":41375,\"start\":41371},{\"end\":41537,\"start\":41531},{\"end\":41551,\"start\":41547},{\"end\":41565,\"start\":41561},{\"end\":41579,\"start\":41574},{\"end\":41596,\"start\":41591},{\"end\":41609,\"start\":41604},{\"end\":41611,\"start\":41610},{\"end\":41625,\"start\":41619},{\"end\":41639,\"start\":41634},{\"end\":41835,\"start\":41831},{\"end\":41850,\"start\":41844},{\"end\":41863,\"start\":41860},{\"end\":41877,\"start\":41872},{\"end\":42153,\"start\":42148},{\"end\":42166,\"start\":42160},{\"end\":42179,\"start\":42171},{\"end\":42193,\"start\":42186},{\"end\":42471,\"start\":42466},{\"end\":42484,\"start\":42478},{\"end\":42498,\"start\":42491},{\"end\":42509,\"start\":42503},{\"end\":42521,\"start\":42514},{\"end\":42786,\"start\":42783},{\"end\":42801,\"start\":42793},{\"end\":42814,\"start\":42808},{\"end\":42826,\"start\":42819},{\"end\":42839,\"start\":42833},{\"end\":42853,\"start\":42845},{\"end\":42866,\"start\":42859},{\"end\":42880,\"start\":42874},{\"end\":42889,\"start\":42887},{\"end\":43151,\"start\":43146},{\"end\":43159,\"start\":43157},{\"end\":43175,\"start\":43165},{\"end\":43199,\"start\":43189},{\"end\":43212,\"start\":43204},{\"end\":43225,\"start\":43219},{\"end\":43535,\"start\":43532},{\"end\":43544,\"start\":43540},{\"end\":43567,\"start\":43558},{\"end\":43829,\"start\":43825},{\"end\":43841,\"start\":43836},{\"end\":43856,\"start\":43850},{\"end\":43868,\"start\":43864},{\"end\":43883,\"start\":43877},{\"end\":43897,\"start\":43893},{\"end\":44148,\"start\":44146},{\"end\":44162,\"start\":44155},{\"end\":44172,\"start\":44169},{\"end\":44185,\"start\":44179},{\"end\":44195,\"start\":44190},{\"end\":44208,\"start\":44201},{\"end\":44217,\"start\":44216},{\"end\":44219,\"start\":44218},{\"end\":44235,\"start\":44229},{\"end\":44250,\"start\":44241},{\"end\":44541,\"start\":44537},{\"end\":44555,\"start\":44548},{\"end\":44562,\"start\":44561},{\"end\":44576,\"start\":44569},{\"end\":44829,\"start\":44822},{\"end\":44842,\"start\":44835},{\"end\":44856,\"start\":44849},{\"end\":44870,\"start\":44864},{\"end\":44872,\"start\":44871},{\"end\":45123,\"start\":45116},{\"end\":45136,\"start\":45129},{\"end\":45151,\"start\":45143},{\"end\":45162,\"start\":45158},{\"end\":45170,\"start\":45168},{\"end\":45484,\"start\":45478},{\"end\":45496,\"start\":45490},{\"end\":45506,\"start\":45501},{\"end\":45514,\"start\":45511},{\"end\":45527,\"start\":45519},{\"end\":45540,\"start\":45534}]", "bib_author_last_name": "[{\"end\":30971,\"start\":30968},{\"end\":30984,\"start\":30980},{\"end\":31308,\"start\":31301},{\"end\":31315,\"start\":31310},{\"end\":31323,\"start\":31319},{\"end\":31344,\"start\":31331},{\"end\":31361,\"start\":31357},{\"end\":31374,\"start\":31367},{\"end\":31385,\"start\":31376},{\"end\":31661,\"start\":31654},{\"end\":31673,\"start\":31668},{\"end\":31692,\"start\":31686},{\"end\":31701,\"start\":31694},{\"end\":31736,\"start\":31724},{\"end\":32010,\"start\":32001},{\"end\":32021,\"start\":32017},{\"end\":32040,\"start\":32031},{\"end\":32245,\"start\":32239},{\"end\":32262,\"start\":32257},{\"end\":32280,\"start\":32272},{\"end\":32297,\"start\":32290},{\"end\":32317,\"start\":32309},{\"end\":32335,\"start\":32326},{\"end\":32599,\"start\":32590},{\"end\":32609,\"start\":32606},{\"end\":32624,\"start\":32620},{\"end\":32636,\"start\":32632},{\"end\":32649,\"start\":32647},{\"end\":32653,\"start\":32651},{\"end\":32875,\"start\":32866},{\"end\":32891,\"start\":32886},{\"end\":32908,\"start\":32899},{\"end\":33107,\"start\":33098},{\"end\":33123,\"start\":33118},{\"end\":33140,\"start\":33131},{\"end\":33333,\"start\":33329},{\"end\":33343,\"start\":33339},{\"end\":33359,\"start\":33353},{\"end\":33370,\"start\":33368},{\"end\":33378,\"start\":33376},{\"end\":33390,\"start\":33383},{\"end\":33600,\"start\":33595},{\"end\":33615,\"start\":33608},{\"end\":33626,\"start\":33621},{\"end\":33640,\"start\":33635},{\"end\":33651,\"start\":33644},{\"end\":33667,\"start\":33662},{\"end\":33686,\"start\":33679},{\"end\":33697,\"start\":33688},{\"end\":33953,\"start\":33949},{\"end\":33963,\"start\":33959},{\"end\":33975,\"start\":33972},{\"end\":34249,\"start\":34238},{\"end\":34262,\"start\":34257},{\"end\":34284,\"start\":34274},{\"end\":34302,\"start\":34291},{\"end\":34316,\"start\":34312},{\"end\":34336,\"start\":34325},{\"end\":34354,\"start\":34346},{\"end\":34373,\"start\":34365},{\"end\":34388,\"start\":34381},{\"end\":34744,\"start\":34734},{\"end\":34764,\"start\":34751},{\"end\":34777,\"start\":34772},{\"end\":34786,\"start\":34784},{\"end\":34806,\"start\":34794},{\"end\":34821,\"start\":34816},{\"end\":34838,\"start\":34829},{\"end\":34853,\"start\":34847},{\"end\":35087,\"start\":35078},{\"end\":35101,\"start\":35095},{\"end\":35310,\"start\":35307},{\"end\":35323,\"start\":35320},{\"end\":35337,\"start\":35334},{\"end\":35352,\"start\":35348},{\"end\":35365,\"start\":35361},{\"end\":35380,\"start\":35378},{\"end\":35637,\"start\":35635},{\"end\":35651,\"start\":35646},{\"end\":35665,\"start\":35660},{\"end\":35686,\"start\":35677},{\"end\":35701,\"start\":35697},{\"end\":35714,\"start\":35707},{\"end\":36007,\"start\":36004},{\"end\":36019,\"start\":36016},{\"end\":36033,\"start\":36028},{\"end\":36049,\"start\":36044},{\"end\":36255,\"start\":36247},{\"end\":36267,\"start\":36261},{\"end\":36276,\"start\":36269},{\"end\":36539,\"start\":36523},{\"end\":36552,\"start\":36547},{\"end\":36563,\"start\":36554},{\"end\":36774,\"start\":36772},{\"end\":36787,\"start\":36785},{\"end\":36807,\"start\":36796},{\"end\":36823,\"start\":36819},{\"end\":37008,\"start\":37005},{\"end\":37023,\"start\":37018},{\"end\":37039,\"start\":37031},{\"end\":37051,\"start\":37047},{\"end\":37066,\"start\":37060},{\"end\":37080,\"start\":37073},{\"end\":37094,\"start\":37088},{\"end\":37114,\"start\":37107},{\"end\":37427,\"start\":37424},{\"end\":37439,\"start\":37436},{\"end\":37449,\"start\":37445},{\"end\":37463,\"start\":37460},{\"end\":37720,\"start\":37717},{\"end\":37730,\"start\":37726},{\"end\":37739,\"start\":37736},{\"end\":37751,\"start\":37748},{\"end\":37764,\"start\":37759},{\"end\":37777,\"start\":37774},{\"end\":37785,\"start\":37783},{\"end\":38065,\"start\":38061},{\"end\":38077,\"start\":38075},{\"end\":38092,\"start\":38088},{\"end\":38103,\"start\":38100},{\"end\":38114,\"start\":38112},{\"end\":38129,\"start\":38124},{\"end\":38142,\"start\":38139},{\"end\":38349,\"start\":38344},{\"end\":38365,\"start\":38357},{\"end\":38634,\"start\":38628},{\"end\":38643,\"start\":38641},{\"end\":38656,\"start\":38650},{\"end\":38672,\"start\":38665},{\"end\":38689,\"start\":38681},{\"end\":38869,\"start\":38865},{\"end\":39081,\"start\":39071},{\"end\":39096,\"start\":39089},{\"end\":39112,\"start\":39108},{\"end\":39351,\"start\":39347},{\"end\":39364,\"start\":39361},{\"end\":39377,\"start\":39373},{\"end\":39389,\"start\":39387},{\"end\":39404,\"start\":39395},{\"end\":39420,\"start\":39415},{\"end\":39435,\"start\":39430},{\"end\":39681,\"start\":39675},{\"end\":39701,\"start\":39691},{\"end\":39715,\"start\":39708},{\"end\":39731,\"start\":39724},{\"end\":39747,\"start\":39742},{\"end\":39998,\"start\":39994},{\"end\":40010,\"start\":40007},{\"end\":40020,\"start\":40015},{\"end\":40033,\"start\":40029},{\"end\":40044,\"start\":40040},{\"end\":40311,\"start\":40308},{\"end\":40324,\"start\":40322},{\"end\":40526,\"start\":40520},{\"end\":40546,\"start\":40534},{\"end\":40561,\"start\":40554},{\"end\":40808,\"start\":40797},{\"end\":40827,\"start\":40818},{\"end\":40846,\"start\":40838},{\"end\":40863,\"start\":40860},{\"end\":40878,\"start\":40870},{\"end\":40892,\"start\":40886},{\"end\":40899,\"start\":40894},{\"end\":41166,\"start\":41163},{\"end\":41182,\"start\":41177},{\"end\":41369,\"start\":41366},{\"end\":41378,\"start\":41376},{\"end\":41545,\"start\":41538},{\"end\":41559,\"start\":41552},{\"end\":41572,\"start\":41566},{\"end\":41589,\"start\":41580},{\"end\":41602,\"start\":41597},{\"end\":41617,\"start\":41612},{\"end\":41632,\"start\":41626},{\"end\":41650,\"start\":41640},{\"end\":41842,\"start\":41836},{\"end\":41858,\"start\":41851},{\"end\":41870,\"start\":41864},{\"end\":41884,\"start\":41878},{\"end\":42158,\"start\":42154},{\"end\":42169,\"start\":42167},{\"end\":42184,\"start\":42180},{\"end\":42199,\"start\":42194},{\"end\":42476,\"start\":42472},{\"end\":42489,\"start\":42485},{\"end\":42501,\"start\":42499},{\"end\":42512,\"start\":42510},{\"end\":42527,\"start\":42522},{\"end\":42791,\"start\":42787},{\"end\":42806,\"start\":42802},{\"end\":42817,\"start\":42815},{\"end\":42831,\"start\":42827},{\"end\":42843,\"start\":42840},{\"end\":42857,\"start\":42854},{\"end\":42872,\"start\":42867},{\"end\":42885,\"start\":42881},{\"end\":42894,\"start\":42890},{\"end\":43155,\"start\":43152},{\"end\":43163,\"start\":43160},{\"end\":43187,\"start\":43176},{\"end\":43202,\"start\":43200},{\"end\":43217,\"start\":43213},{\"end\":43233,\"start\":43226},{\"end\":43538,\"start\":43536},{\"end\":43556,\"start\":43545},{\"end\":43577,\"start\":43568},{\"end\":43834,\"start\":43830},{\"end\":43848,\"start\":43842},{\"end\":43862,\"start\":43857},{\"end\":43875,\"start\":43869},{\"end\":43891,\"start\":43884},{\"end\":43906,\"start\":43898},{\"end\":44153,\"start\":44149},{\"end\":44167,\"start\":44163},{\"end\":44177,\"start\":44173},{\"end\":44188,\"start\":44186},{\"end\":44199,\"start\":44196},{\"end\":44214,\"start\":44209},{\"end\":44227,\"start\":44220},{\"end\":44239,\"start\":44236},{\"end\":44255,\"start\":44251},{\"end\":44260,\"start\":44257},{\"end\":44546,\"start\":44542},{\"end\":44559,\"start\":44556},{\"end\":44567,\"start\":44563},{\"end\":44584,\"start\":44577},{\"end\":44591,\"start\":44586},{\"end\":44833,\"start\":44830},{\"end\":44847,\"start\":44843},{\"end\":44862,\"start\":44857},{\"end\":44878,\"start\":44873},{\"end\":45127,\"start\":45124},{\"end\":45141,\"start\":45137},{\"end\":45156,\"start\":45152},{\"end\":45166,\"start\":45163},{\"end\":45174,\"start\":45171},{\"end\":45488,\"start\":45485},{\"end\":45499,\"start\":45497},{\"end\":45509,\"start\":45507},{\"end\":45517,\"start\":45515},{\"end\":45532,\"start\":45528},{\"end\":45544,\"start\":45541}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2107.05790\",\"id\":\"b0\"},\"end\":31226,\"start\":30963},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7901560},\"end\":31571,\"start\":31228},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":70350052},\"end\":31931,\"start\":31573},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":231861462},\"end\":32183,\"start\":31933},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":218889832},\"end\":32528,\"start\":32185},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":233864543},\"end\":32806,\"start\":32530},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15435392},\"end\":33038,\"start\":32808},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14377597},\"end\":33270,\"start\":33040},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57246310},\"end\":33538,\"start\":33272},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":215415902},\"end\":33889,\"start\":33540},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14008455},\"end\":34103,\"start\":33891},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":225039882},\"end\":34699,\"start\":34105},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1033682},\"end\":35035,\"start\":34701},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b13\"},\"end\":35242,\"start\":35037},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":232417451},\"end\":35548,\"start\":35244},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":221447878},\"end\":35920,\"start\":35550},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":13684145},\"end\":36210,\"start\":35922},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b17\"},\"end\":36416,\"start\":36212},{\"attributes\":{\"id\":\"b18\"},\"end\":36723,\"start\":36418},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":209077411},\"end\":36951,\"start\":36725},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14113767},\"end\":37305,\"start\":36953},{\"attributes\":{\"doi\":\"arXiv:2103.10596\",\"id\":\"b21\"},\"end\":37710,\"start\":37307},{\"attributes\":{\"doi\":\"arXiv:2106.13230\",\"id\":\"b22\"},\"end\":37980,\"start\":37712},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":244729636},\"end\":38336,\"start\":37982},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b24\"},\"end\":38549,\"start\":38338},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":208002298},\"end\":38861,\"start\":38551},{\"attributes\":{\"id\":\"b26\"},\"end\":38980,\"start\":38863},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":214603516},\"end\":39287,\"start\":38982},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220280381},\"end\":39616,\"start\":39289},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2202933},\"end\":39906,\"start\":39618},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220647499},\"end\":40216,\"start\":39908},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9618359},\"end\":40459,\"start\":40218},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":197943120},\"end\":40711,\"start\":40461},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15019293},\"end\":41114,\"start\":40713},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14848918},\"end\":41281,\"start\":41116},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":167217261},\"end\":41502,\"start\":41283},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":13756489},\"end\":41829,\"start\":41504},{\"attributes\":{\"doi\":\"arXiv:2007.01289\",\"id\":\"b37\"},\"end\":42079,\"start\":41831},{\"attributes\":{\"doi\":\"arXiv:2104.09770\",\"id\":\"b38\"},\"end\":42397,\"start\":42081},{\"attributes\":{\"doi\":\"arXiv:2111.11591\",\"id\":\"b39\"},\"end\":42735,\"start\":42399},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":244799265},\"end\":43085,\"start\":42737},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15201962},\"end\":43414,\"start\":43087},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":195503148},\"end\":43774,\"start\":43416},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":235658393},\"end\":44068,\"start\":43776},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":231719476},\"end\":44478,\"start\":44070},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":44149078},\"end\":44740,\"start\":44480},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":233404466},\"end\":45046,\"start\":44742},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":51974320},\"end\":45400,\"start\":45048},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":222208633},\"end\":45718,\"start\":45402}]", "bib_title": "[{\"end\":31297,\"start\":31228},{\"end\":31650,\"start\":31573},{\"end\":31993,\"start\":31933},{\"end\":32229,\"start\":32185},{\"end\":32580,\"start\":32530},{\"end\":32857,\"start\":32808},{\"end\":33089,\"start\":33040},{\"end\":33323,\"start\":33272},{\"end\":33586,\"start\":33540},{\"end\":33942,\"start\":33891},{\"end\":34229,\"start\":34105},{\"end\":34728,\"start\":34701},{\"end\":35296,\"start\":35244},{\"end\":35625,\"start\":35550},{\"end\":35993,\"start\":35922},{\"end\":36764,\"start\":36725},{\"end\":36994,\"start\":36953},{\"end\":38050,\"start\":37982},{\"end\":38619,\"start\":38551},{\"end\":39064,\"start\":38982},{\"end\":39337,\"start\":39289},{\"end\":39666,\"start\":39618},{\"end\":39985,\"start\":39908},{\"end\":40301,\"start\":40218},{\"end\":40514,\"start\":40461},{\"end\":40793,\"start\":40713},{\"end\":41154,\"start\":41116},{\"end\":41355,\"start\":41283},{\"end\":41529,\"start\":41504},{\"end\":42781,\"start\":42737},{\"end\":43144,\"start\":43087},{\"end\":43530,\"start\":43416},{\"end\":43823,\"start\":43776},{\"end\":44144,\"start\":44070},{\"end\":44535,\"start\":44480},{\"end\":44820,\"start\":44742},{\"end\":45114,\"start\":45048},{\"end\":45476,\"start\":45402}]", "bib_author": "[{\"end\":30973,\"start\":30963},{\"end\":30986,\"start\":30973},{\"end\":31310,\"start\":31299},{\"end\":31317,\"start\":31310},{\"end\":31325,\"start\":31317},{\"end\":31346,\"start\":31325},{\"end\":31363,\"start\":31346},{\"end\":31376,\"start\":31363},{\"end\":31387,\"start\":31376},{\"end\":31663,\"start\":31652},{\"end\":31675,\"start\":31663},{\"end\":31694,\"start\":31675},{\"end\":31703,\"start\":31694},{\"end\":31738,\"start\":31703},{\"end\":32012,\"start\":31995},{\"end\":32023,\"start\":32012},{\"end\":32042,\"start\":32023},{\"end\":32247,\"start\":32231},{\"end\":32264,\"start\":32247},{\"end\":32282,\"start\":32264},{\"end\":32299,\"start\":32282},{\"end\":32319,\"start\":32299},{\"end\":32337,\"start\":32319},{\"end\":32601,\"start\":32582},{\"end\":32611,\"start\":32601},{\"end\":32626,\"start\":32611},{\"end\":32638,\"start\":32626},{\"end\":32651,\"start\":32638},{\"end\":32655,\"start\":32651},{\"end\":32877,\"start\":32859},{\"end\":32893,\"start\":32877},{\"end\":32910,\"start\":32893},{\"end\":33109,\"start\":33091},{\"end\":33125,\"start\":33109},{\"end\":33142,\"start\":33125},{\"end\":33335,\"start\":33325},{\"end\":33345,\"start\":33335},{\"end\":33361,\"start\":33345},{\"end\":33372,\"start\":33361},{\"end\":33380,\"start\":33372},{\"end\":33392,\"start\":33380},{\"end\":33602,\"start\":33588},{\"end\":33617,\"start\":33602},{\"end\":33628,\"start\":33617},{\"end\":33642,\"start\":33628},{\"end\":33653,\"start\":33642},{\"end\":33669,\"start\":33653},{\"end\":33688,\"start\":33669},{\"end\":33699,\"start\":33688},{\"end\":33955,\"start\":33944},{\"end\":33965,\"start\":33955},{\"end\":33977,\"start\":33965},{\"end\":34251,\"start\":34231},{\"end\":34264,\"start\":34251},{\"end\":34286,\"start\":34264},{\"end\":34304,\"start\":34286},{\"end\":34318,\"start\":34304},{\"end\":34338,\"start\":34318},{\"end\":34356,\"start\":34338},{\"end\":34375,\"start\":34356},{\"end\":34390,\"start\":34375},{\"end\":34746,\"start\":34730},{\"end\":34766,\"start\":34746},{\"end\":34779,\"start\":34766},{\"end\":34788,\"start\":34779},{\"end\":34808,\"start\":34788},{\"end\":34823,\"start\":34808},{\"end\":34840,\"start\":34823},{\"end\":34855,\"start\":34840},{\"end\":35089,\"start\":35074},{\"end\":35103,\"start\":35089},{\"end\":35312,\"start\":35298},{\"end\":35325,\"start\":35312},{\"end\":35339,\"start\":35325},{\"end\":35354,\"start\":35339},{\"end\":35367,\"start\":35354},{\"end\":35382,\"start\":35367},{\"end\":35639,\"start\":35627},{\"end\":35653,\"start\":35639},{\"end\":35667,\"start\":35653},{\"end\":35688,\"start\":35667},{\"end\":35703,\"start\":35688},{\"end\":35716,\"start\":35703},{\"end\":36009,\"start\":35995},{\"end\":36021,\"start\":36009},{\"end\":36035,\"start\":36021},{\"end\":36051,\"start\":36035},{\"end\":36257,\"start\":36245},{\"end\":36269,\"start\":36257},{\"end\":36278,\"start\":36269},{\"end\":36541,\"start\":36514},{\"end\":36554,\"start\":36541},{\"end\":36565,\"start\":36554},{\"end\":36776,\"start\":36766},{\"end\":36789,\"start\":36776},{\"end\":36809,\"start\":36789},{\"end\":36825,\"start\":36809},{\"end\":37010,\"start\":36996},{\"end\":37025,\"start\":37010},{\"end\":37041,\"start\":37025},{\"end\":37053,\"start\":37041},{\"end\":37068,\"start\":37053},{\"end\":37082,\"start\":37068},{\"end\":37096,\"start\":37082},{\"end\":37116,\"start\":37096},{\"end\":37429,\"start\":37415},{\"end\":37441,\"start\":37429},{\"end\":37451,\"start\":37441},{\"end\":37465,\"start\":37451},{\"end\":37722,\"start\":37714},{\"end\":37732,\"start\":37722},{\"end\":37741,\"start\":37732},{\"end\":37753,\"start\":37741},{\"end\":37766,\"start\":37753},{\"end\":37779,\"start\":37766},{\"end\":37787,\"start\":37779},{\"end\":38067,\"start\":38052},{\"end\":38079,\"start\":38067},{\"end\":38094,\"start\":38079},{\"end\":38105,\"start\":38094},{\"end\":38116,\"start\":38105},{\"end\":38131,\"start\":38116},{\"end\":38144,\"start\":38131},{\"end\":38351,\"start\":38338},{\"end\":38367,\"start\":38351},{\"end\":38636,\"start\":38621},{\"end\":38645,\"start\":38636},{\"end\":38658,\"start\":38645},{\"end\":38674,\"start\":38658},{\"end\":38691,\"start\":38674},{\"end\":38871,\"start\":38865},{\"end\":39083,\"start\":39066},{\"end\":39098,\"start\":39083},{\"end\":39114,\"start\":39098},{\"end\":39353,\"start\":39339},{\"end\":39366,\"start\":39353},{\"end\":39379,\"start\":39366},{\"end\":39391,\"start\":39379},{\"end\":39406,\"start\":39391},{\"end\":39422,\"start\":39406},{\"end\":39437,\"start\":39422},{\"end\":39683,\"start\":39668},{\"end\":39703,\"start\":39683},{\"end\":39717,\"start\":39703},{\"end\":39733,\"start\":39717},{\"end\":39749,\"start\":39733},{\"end\":40000,\"start\":39987},{\"end\":40012,\"start\":40000},{\"end\":40022,\"start\":40012},{\"end\":40035,\"start\":40022},{\"end\":40046,\"start\":40035},{\"end\":40313,\"start\":40303},{\"end\":40326,\"start\":40313},{\"end\":40528,\"start\":40516},{\"end\":40548,\"start\":40528},{\"end\":40563,\"start\":40548},{\"end\":40810,\"start\":40795},{\"end\":40829,\"start\":40810},{\"end\":40848,\"start\":40829},{\"end\":40865,\"start\":40848},{\"end\":40880,\"start\":40865},{\"end\":40894,\"start\":40880},{\"end\":40901,\"start\":40894},{\"end\":41168,\"start\":41156},{\"end\":41184,\"start\":41168},{\"end\":41371,\"start\":41357},{\"end\":41380,\"start\":41371},{\"end\":41547,\"start\":41531},{\"end\":41561,\"start\":41547},{\"end\":41574,\"start\":41561},{\"end\":41591,\"start\":41574},{\"end\":41604,\"start\":41591},{\"end\":41619,\"start\":41604},{\"end\":41634,\"start\":41619},{\"end\":41652,\"start\":41634},{\"end\":41844,\"start\":41831},{\"end\":41860,\"start\":41844},{\"end\":41872,\"start\":41860},{\"end\":41886,\"start\":41872},{\"end\":42160,\"start\":42148},{\"end\":42171,\"start\":42160},{\"end\":42186,\"start\":42171},{\"end\":42201,\"start\":42186},{\"end\":42478,\"start\":42466},{\"end\":42491,\"start\":42478},{\"end\":42503,\"start\":42491},{\"end\":42514,\"start\":42503},{\"end\":42529,\"start\":42514},{\"end\":42793,\"start\":42783},{\"end\":42808,\"start\":42793},{\"end\":42819,\"start\":42808},{\"end\":42833,\"start\":42819},{\"end\":42845,\"start\":42833},{\"end\":42859,\"start\":42845},{\"end\":42874,\"start\":42859},{\"end\":42887,\"start\":42874},{\"end\":42896,\"start\":42887},{\"end\":43157,\"start\":43146},{\"end\":43165,\"start\":43157},{\"end\":43189,\"start\":43165},{\"end\":43204,\"start\":43189},{\"end\":43219,\"start\":43204},{\"end\":43235,\"start\":43219},{\"end\":43540,\"start\":43532},{\"end\":43558,\"start\":43540},{\"end\":43579,\"start\":43558},{\"end\":43836,\"start\":43825},{\"end\":43850,\"start\":43836},{\"end\":43864,\"start\":43850},{\"end\":43877,\"start\":43864},{\"end\":43893,\"start\":43877},{\"end\":43908,\"start\":43893},{\"end\":44155,\"start\":44146},{\"end\":44169,\"start\":44155},{\"end\":44179,\"start\":44169},{\"end\":44190,\"start\":44179},{\"end\":44201,\"start\":44190},{\"end\":44216,\"start\":44201},{\"end\":44229,\"start\":44216},{\"end\":44241,\"start\":44229},{\"end\":44257,\"start\":44241},{\"end\":44262,\"start\":44257},{\"end\":44548,\"start\":44537},{\"end\":44561,\"start\":44548},{\"end\":44569,\"start\":44561},{\"end\":44586,\"start\":44569},{\"end\":44593,\"start\":44586},{\"end\":44835,\"start\":44822},{\"end\":44849,\"start\":44835},{\"end\":44864,\"start\":44849},{\"end\":44880,\"start\":44864},{\"end\":45129,\"start\":45116},{\"end\":45143,\"start\":45129},{\"end\":45158,\"start\":45143},{\"end\":45168,\"start\":45158},{\"end\":45176,\"start\":45168},{\"end\":45490,\"start\":45478},{\"end\":45501,\"start\":45490},{\"end\":45511,\"start\":45501},{\"end\":45519,\"start\":45511},{\"end\":45534,\"start\":45519},{\"end\":45546,\"start\":45534}]", "bib_venue": "[{\"end\":31070,\"start\":31002},{\"end\":31391,\"start\":31387},{\"end\":31741,\"start\":31738},{\"end\":32052,\"start\":32042},{\"end\":32347,\"start\":32337},{\"end\":32659,\"start\":32655},{\"end\":32914,\"start\":32910},{\"end\":33146,\"start\":33142},{\"end\":33396,\"start\":33392},{\"end\":33703,\"start\":33699},{\"end\":33985,\"start\":33977},{\"end\":34394,\"start\":34390},{\"end\":34859,\"start\":34855},{\"end\":35072,\"start\":35037},{\"end\":35386,\"start\":35382},{\"end\":35720,\"start\":35716},{\"end\":36058,\"start\":36051},{\"end\":36243,\"start\":36212},{\"end\":36512,\"start\":36418},{\"end\":36829,\"start\":36825},{\"end\":37120,\"start\":37116},{\"end\":37413,\"start\":37307},{\"end\":38148,\"start\":38144},{\"end\":38421,\"start\":38382},{\"end\":38696,\"start\":38691},{\"end\":39125,\"start\":39114},{\"end\":39441,\"start\":39437},{\"end\":39753,\"start\":39749},{\"end\":40050,\"start\":40046},{\"end\":40330,\"start\":40326},{\"end\":40576,\"start\":40563},{\"end\":40905,\"start\":40901},{\"end\":41189,\"start\":41184},{\"end\":41384,\"start\":41380},{\"end\":41656,\"start\":41652},{\"end\":41932,\"start\":41902},{\"end\":42146,\"start\":42081},{\"end\":42464,\"start\":42399},{\"end\":42900,\"start\":42896},{\"end\":43239,\"start\":43235},{\"end\":43583,\"start\":43579},{\"end\":43912,\"start\":43908},{\"end\":44266,\"start\":44262},{\"end\":44597,\"start\":44593},{\"end\":44884,\"start\":44880},{\"end\":45214,\"start\":45176},{\"end\":45550,\"start\":45546}]"}}}, "year": 2023, "month": 12, "day": 17}