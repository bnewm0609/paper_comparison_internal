{"id": 250293674, "updated": "2022-07-12 18:44:46.211", "metadata": {"title": "Tight Neural Network Verification via Semidefinite Relaxations and Linear Reformulations", "authors": "[{\"first\":\"Jianglin\",\"last\":\"Lan\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Alessio\",\"last\":\"Lomuscio\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present a novel semidefinite programming (SDP) relax- ation that enables tight and efficient verification of neural networks. The tightness is achieved by combining SDP re- laxations with valid linear cuts, constructed by using the reformulation-linearisation technique (RLT). The computa- tional efficiency results from a layerwise SDP formulation and an iterative algorithm for incrementally adding RLT- generated linear cuts to the verification formulation. The layer RLT-SDP relaxation here presented is shown to produce the tightest SDP relaxation for ReLU neural networks available in the literature. We report experimental results based on MNIST neural networks showing that the method out-performs the state-of-the-art methods while maintaining ac- ceptable computational overheads. For networks of approx-imately 10k nodes (1k, respectively), the proposed method achieved an improvement in the ratio of certified robustness cases from 0% to 82% (from 35% to 70%, respectively).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/Lan0L22", "doi": "10.1609/aaai.v36i7.20689"}}, "content": {"source": {"pdf_hash": "6215adc10c6873057cc74c192ec189854b24822e", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/20689/20448\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20689/20448", "status": "GOLD"}}, "grobid": {"id": "fa60f7147b5c0bffba9e9d85614cc5b40ba203f2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6215adc10c6873057cc74c192ec189854b24822e.txt", "contents": "\nTight Neural Network Verification via Semidefinite Relaxations and Linear Reformulations\n\n\nJianglin Lan j.lan@imperial.ac.uk \nDepartment of Computing\nImperial College London\nUK\n\nYang Zheng zhengy@eng.ucsd.edu \nDepartment of Electrical and Computer Engineering\nUniversity of California San Diego\nUSA\n\nAlessio Lomuscio a.lomuscio@imperial.ac.uk \nDepartment of Computing\nImperial College London\nUK\n\nTight Neural Network Verification via Semidefinite Relaxations and Linear Reformulations\n\nWe present a novel semidefinite programming (SDP) relaxation that enables tight and efficient verification of neural networks. The tightness is achieved by combining SDP relaxations with valid linear cuts, constructed by using the reformulation-linearisation technique (RLT). The computational efficiency results from a layerwise SDP formulation and an iterative algorithm for incrementally adding RLTgenerated linear cuts to the verification formulation. The layer RLT-SDP relaxation here presented is shown to produce the tightest SDP relaxation for ReLU neural networks available in the literature. We report experimental results based on MNIST neural networks showing that the method outperforms the state-of-the-art methods while maintaining acceptable computational overheads. For networks of approximately 10k nodes (1k, respectively), the proposed method achieved an improvement in the ratio of certified robustness cases from 0% to 82% (from 35% to 70%, respectively).\n\nIntroduction\n\nWhile progress in training methods for neural networks (NNs) continues, it is well-known that NNs are susceptible to adversarial attacks (Goodfellow, Shlens, and Szegedy 2014). This is highly problematic for uses of NNs in safetycritical systems such as the aircraft domain Akintunde et al. 2020b,a;Julian and Kochenderfer 2021;Manzanas Lopez et al. 2021) or in any application where miss-classifications need to be minimised. The area of verification of NNs (Liu et al. 2020;Bak, Liu, and Johnson 2021) aims to develop methods to guarantee that NNs are robust with respect to small perturbations, with particular emphasis to noise perturbations.\n\nExisting NN verification methods can be divided into two categories (Liu et al. 2020): complete and incomplete approaches. Complete approaches guarantee to resolve any verification query, but may incur high computational cost. Incomplete approaches normally leverage forms of convex over-approximations of NNs to enable faster verification. While incomplete approaches tend to scale to larger networks, the looser their approximation is, the more likely it is that the approach may be unable to verify the problem instance. Thus, one central objective in incomplete approaches Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. is to develop tighter convex approximations while retaining computation efficiency (Salman et al. 2019).\n\nIn this paper we provide a novel relaxation method which combines semidefinite programming (SDP) (Vandenberghe and Boyd 1996) with the reformulation-linearisation technique (RLT) (Anstreicher 2009) to verify NNs. The new SDP relaxation is provably tighter than existing SDP approaches, whilst enjoying a competitive efficiency.\n\nRelated work. Complete approaches to the verification of NNs are typically based on mixed-integer linear programming (MILP) (Bastani et al. 2016;Lomuscio and Maganti 2017;Tjeng, Xiao, and Tedrake 2019;Anderson et al. 2020;Botoeva et al. 2020), satisfiability modulo theories (Katz et al. 2019;Ehlers 2017) or bound propagations combined with input refinement Lomuscio 2020, 2021;Wang et al. 2021;Hashemi, Kouvaros, and Lomuscio 2021). While these approaches can provide theoretical termination guarantees, their scalability to large NNs is often problematic. Incomplete approaches for NN verification are normally based on bound propagations (Weng et al. 2018;Singh et al. 2019a;Tjandraatmadja et al. 2020;M\u00fcller et al. 2021), combinations between linear programming (LP) and relaxations (Ehlers 2017), or duality relaxation (Dvijotham et al. 2018;Wong and Kolter 2018;Dathathri et al. 2020). The triangle relaxation (Ehlers 2017) gives the tightest convex approximation of a single ReLU node and has inspired several other approaches (Salman et al. 2019;Li et al. 2020). While these methods often achieve state-of-the-art (SoA) performance, they have limited efficacy: even optimally tuned LPbased convex relaxations may fail to obtain tight bounds on the certified robustness ratio (Salman et al. 2019).\n\nTwo lines of research have attempted to alleviate this problem. The first aims to provide tighter LP relaxations by exploring interdependencies among multiple neurons and inputs, e.g., DeepPoly (Singh et al. 2019a), kPoly (Singh et al. 2019b), OptC2V PRIMA (M\u00fcller et al. 2021). The second seeks alternative, stronger relaxations beyond LPs. The most promising relaxation combining tightness with efficiency is presently based on SDPs (Raghunathan, Steinhardt, and Liang 2018;Fazlyab, Morari, and Pappas 2022;Batten et al. 2021).\n\nIt has been empirically observed that the relaxations generated with the SDP method in (Raghunathan, Steinhardt, and Liang 2018) are considerably tighter than standard LP relaxations. Using geometric techniques, it has been shown that the SDP relaxation for a variant of the NN verification problem is exact over a single hidden layer under mild assumptions, but becomes loose for several hidden layers (Zhang 2020). To obtain tighter SDP relaxations, effective linear cuts were identified in (Batten et al. 2021); nonconvex cuts were investigated in (Ma and Sojoudi 2020).\n\nSDPs are harder to solve than LPs. To overcome this, a dual SDP relaxation was formulated and subsequently solved using a subgradient algorithm in (Dathathri et al. 2020). A layer SDP relaxation has been recently proposed in (Batten et al. 2021) by exploiting cascading network structures based on graph decomposition (Zheng, Fantuzzi, and Papachristodoulou 2021). To the best of our knowledge, layer SDP provides the tightest relaxations that have so far been achieved by combining SDP relaxations with triangle relaxations (Ehlers 2017). Even so, the relaxation gap is still considerable in large NNs, as observed in (Batten et al. 2021). This leads to an increasing number of verification queries that cannot be resolved as the model size increases, thereby limiting the applicability of the approach.\n\nContributions. In this paper, we advance the SoA SDP relaxations for NN verification by using the RLT (Anstreicher 2009) in this context. Specifically, we propose a new layer RLT-SDP relaxation with valid linear cuts obtained from RLT that offers provably tighter relaxations. The linear cuts capture both inter-layer dependencies and intralayer interactions of the network, which are presently not exploited in the existing relaxation methods. Due to the computational cost of using large numbers of linear cuts, we refine this method by introducing an iterative algorithm to integrate the RLT-generated linear cuts and the SDP relaxation. At each iteration only a portion of linear cuts are added, with their priorities being determined by the network weights. Our theoretical analysis shows that the relaxations here obtained are provably tighter than any other approach previously considered. In the experiments we report, the method obtained considerably tighter relaxations than the present SoA leading to several more queries being answerable.\n\n\nProblem Statement and Preliminaries\n\nNotation. We use the symbol R n to denote the ndimensional Euclidean space. We use diag(X) to stack the main diagonals of matrix X as a column, and |X| to get its absolute value element-wise. We use \u2299 to denote the element-wise product, and I b with b > 0 to denote a sequence of nonzero integers from 0 to b. We use \u2225 \u00b7 \u2225 \u221e to refer to the standard \u2113 \u221e norm.\n\nNN verification problem. We focus on feed-forward fully-connected NNs with ReLU activations. A NN f (x 0 ) : R n0 \u2192 R n L+1 with L hidden layers, n 0 inputs and n L+1 outputs is defined as follows: x 0 is the input, f (x 0 ) is the output, andx i , x i \u2208 R ni , i = 1, 2, . . . , L, are the pre-activation and activation vectors of the i-th layer, respectively. The NN output is f (\nx 0 ) = W L x L + b L with x i+1 = ReLU(x i+1 ) andx i+1 = W i x i + b i , i \u2208 I L\u22121 ,\nwhere W i \u2208 R ni+1\u00d7ni and b i \u2208 R ni+1 are the weights and biases, respectively. For a vector z \u2208 R n , the ReLU function is defined as ReLU(z) = [max(z 1 , 0), \u00b7 \u00b7 \u00b7 , max(z n , 0)] T . We focus on classification networks whereby an input x 0 is assigned to the class associated with the NN output with the highest value: j \u22c6 = arg max j=1,2,\u00b7\u00b7\u00b7 ,n L+1 f (x 0 ) j .\n\nWe now present the local robustness verification problem.\n\nDefinition 1. Given a NN f : R n0 \u2192 R n L+1 , an inputx and a perturbation radius\n\u03f5 \u2208 R, f is robust onx if f (x) j \u22c6 \u2212 f (x 0 ) j > 0 when j \u0338 = j \u22c6 , for all x 0 s.t. \u2225x 0 \u2212x\u2225 \u221e \u2264 \u03f5.\nGiven a NN f , an inputx and a perturbation \u03f5, the local robustness problem concerns determining whether it is the case that f meets Definition 1 forx and \u03f5. This problem can be formulated and solved as an optimisation problem:\n\u03b3 * := min {xi} L i=0 c T x L + c 0 s.t. x i+1 = ReLU(W i x i + b i ), i \u2208 I L\u22121 , (1a) \u2225x 0 \u2212x\u2225 \u221e \u2264 \u03f5, (1b) l i+1 \u2264 x i+1 \u2264 u i+1 , i \u2208 I L\u22121 ,(1c)where c T = W L (j \u22c6 , :) \u2212 W L (j, :) and c 0 = b L (j \u22c6 ) \u2212 b L (j)\n. l i+1 and u i+1 are the lower and upper bounds of the activation vectors, which can be computed using bound propagation methods, see e.g., (Wong and Kolter 2018;Wang et al. 2018). The optimisation problem (1) is solved for every potential adversarial target j \u0338 = j \u22c6 . If \u03b3 * > 0 in all cases, then the network is robust onx against the adversarial input x 0 . Due to the nonlinear ReLU constraint (1a), the problem (1) is non-convex and thus hard to solve generally. In the literature, two SDP relaxations have been proposed: a global SDP relaxation (Raghunathan, Steinhardt, and Liang 2018) and a layer SDP relaxation (Batten et al. 2021).\n\nGlobal SDP relaxation. The ReLU constraint (1a) is equivalent to a set of linear and quadratic constraints:\nx i+1 \u2265 0, x i+1 \u2265 W i x i + b i , i \u2208 I L\u22121 , x i+1 \u2299 (x i+1 \u2212 W i x i \u2212 b i ) = 0, i \u2208 I L\u22121 .(2)\nThe input constraints in (1b) and (1c) can be equivalently represented as the quadratic constraints:\nx i \u2299 x i \u2212 (l i + u i ) \u2299 x i + l i \u2299 u i \u2264 0, i \u2208 I L ,(3)\nwhere l 0 =x \u2212 \u03f51, u 0 =x + \u03f51 and 1 denotes column of ones. Replacing (1a)-(1c) with (2)-(3) yields an equivalent quadratically constrained quadratic programming (QCQP):\n\u03b3 * := min {xi} L i=0 c T x L + c 0 | (2), (3) .(4)\nThis QCQP problem is still non-convex due to the quadratic constraints. The techniques of polynomial lifting (Parrilo 2000;Lasserre 2009) can be used to reformulate them as linear constraints. The lifting matrix P is defined as\nP = xx T , with x = [1, x T 0 , x T 1 , \u00b7 \u00b7 \u00b7 , x T L ] T \u2208 Rn,(5)\nwheren = 1 + L i=0 n i . The above can be reformulated as:\nP \u2ab0 0, P [1] = 1, rank(P ) = 1.(6)\nBy using (5) and (6) and dropping rank(P ) = 1, the QCQP (4) is relaxed into a global SDP (Raghunathan, Steinhardt, and Liang 2018):\n\u03b3 * GlobalSDP := min P c T P [x L ] + c 0 s.t. P [x i+1 ] \u2265 0, P [x i+1 ] \u2265 W i P [x i ] + b i , i \u2208 I L\u22121 , (7a) diag(P [x i+1 x T i+1 ] \u2212 W i P [x i x T i+1 ]) \u2212 b i \u2299 P [x i+1 ] = 0, i \u2208 I L\u22121 , (7b) diag(P [x i x T i ]) \u2212 (l i + u i ) \u2299 P [x i ] + l i \u2299 u i \u2264 0, i \u2208 I L , (7c) P [1] = 1, P \u2ab0 0,(7d)\nwhere the symbolic indexing P [\u00b7] is used to index the elements of P . Since rank(P ) = 1 is dropped, problem (7) gives a relaxed solution to the QCQP, i.e., \u03b3 * GlobalSDP \u2264 \u03b3 * . Layer SDP relaxation. The layer SDP relaxation exploits the deep structure of the layers of a NN, where the activation vector of a layer depends only on its immediate preceding layer. This structure is exploited in (Batten et al. 2021) to develop a layer-based SDP formulation of (7), where the dimension of the matrix constraint is reduced, thus improving the computational efficiency. In this work, instead of using a single large lifting matrix P for the entire network, each hidden layer i is assigned a smaller matrix P i defined as\nP i = x i x T i , with x i = [1, x T i , x T i+1 ] T \u2208 Rn i ,(8)wheren i = 1+n i +n i+1 . Similar to (6), the constraint P i = x i x T i is equivalent to P i \u2ab0 0, P i [1] = 1, rank(P i ) = 1.\nBy using (8), the layer SDP relaxation is formulated as\n\u03b3 * LayerSDP := min {Pi} L\u22121 i=0 c T P L\u22121 [x L ] + c 0 s.t. P i [x i+1 ] \u2265 0, i \u2208 I L\u22121 , (9a) P i [x i+1 ] \u2265 W i P i [x i ] + b i , i \u2208 I L\u22121 , (9b) diag(P i [x i+1 x T i+1 ] \u2212 W i P i [x i x T i+1 ]) \u2212 b i \u2299 P i [x i+1 ] = 0, i \u2208 I L\u22121 , (9c) diag(P i [x i x T i ]) \u2212 (l i + u i ) \u2299 P i [x i ] + l i \u2299 u i \u2264 0, i \u2208 I L\u22121 , (9d) diag(P L\u22121 [x L x T L ]) \u2212 (l L + u L ) \u2299 P L\u22121 [x L ] + l L \u2299 u L \u2264 0, (9e) P i [1] = 1, P i \u2ab0 0, i \u2208 I L\u22121 ,(9f)P i [x i+1x T i+1 ] = P i+1 [x i+1x T i+1 ], i \u2208 I L\u22122 , (9g) P i [x i+1 ] \u2264 A i P i [x i ] + B i , i \u2208 I L\u22121 ,(9h)wherex i+1 = [1, x T i+1 ] T , A i = k i \u2299 W i , B i = k i \u2299 (b i \u2212l i+1 ) + ReLU(l i+1 ), k i = (ReLU(\u00fb i+1 ) \u2212 ReLU(l i+1 ))/(\u00fb i+1 \u2212l i+1 )\n, with\u00fb i+1 ,l i+1 being upper and lower bounds of the pre-activation vectorx i+1 (Wong and Kolter 2018;Wang et al. 2018). Note that the constraint (9g) appears to ensure input-output consistency between layers. Compared to (7), the new constraint (9h) is obtained from the triangle relaxation. It is shown in (Batten et al. 2021) that without (9h), the layer SDP relaxation (9) is equivalent to the global SDP relaxation (7) based on graph decomposition (Vandenberghe and Andersen 2015; Zheng 2019). Also, this layer SDP relaxation achieves faster verification than the global SDP relaxation by dealing with lower dimensional constraints, and obtains a tighter relaxation by introducing\n(9h), i.e., \u03b3 * GlobalSDP \u2264 \u03b3 * LayerSDP \u2264 \u03b3 * . Source of SDP relaxation gap. Letx = [x T 0 , \u00b7 \u00b7 \u00b7 , x T L ] T and P [xx T ] =xx T . It follows from (6) that P = xx T is equivalent to P [1] = 1, P [xx T ] =xx T .\nObserve that the global SDP relaxation (7) only includes (Eq. (7d)) the relaxed constraints P [1] = 1 and P [xx T ] \u2ab0xx T , reformulated as P \u2ab0 0 via Schur complement. A consequence of this is that P may not be sufficiently bounded, thereby resulting in loose SDP solutions. The same argument applies for the layer SDP relaxation (9).\n\nTo bridge the gap of the global SDP relaxation, the nonconvex constraint P [xx T ] \u2aafxx T is imposed in (Ma and Sojoudi 2020) via secant approximation. Valid linear cuts are generated by an iterative algorithm, where the global SDP relaxation together with an LP need to be solved at each iteration. Unfortunately, it is known that global SDP relaxation itself is already computationally expensive. Therefore, the tightening in (Ma and Sojoudi 2020) is unlikely to lead to scalable NN verification.\n\n\nTightening Layer SDP Relaxation via RLT\n\nHaving highlighted the existing relaxation gap in the SoA, we now present an approach for tightening the SDP relaxation for NN verification while retaining an acceptable computational overhead. Our method combines layer SDP relaxations (9) with the RLT (Anstreicher 2009).\n\nMotivation. We first denote a few terms for each layer of a NN:\nx i+1 = [x T i , x T i+1 ] T ,l i+1 = [l T i , l T i+1 ] T and u i+1 = [u T i , u T i+1 ] T . Since 0 \u2264l i \u2264x i+1 \u2264\u0169 i , we havex i+1l T i+1 \u2264x i+1x T i+1 \u2264x i+1\u0169 T i+1\n. These nonlinear constraints can be reformulated as linear constraints on the elements of P i :\nP i [x i+1 ]l T i+1 \u2264 P i [x i+1x T i+1 ] \u2264 P i [x i+1 ]\u0169 T i+1 .(10)\nThe method aims to bound P i [x i+1x\n\nT i+1 ] within the region given in (10). The constraints in (10) are linear and could be directly added to (9). However, they introduce 2(n i +n i+1 ) 2 new inequalities, thereby increasing the computational effort required to solve the verification problem. Therefore, it is desirable to develop efficient strategies for imposing the constraints in (10). In the following we: (i) use RLT to construct valid linear cuts that are provably stronger than (10), and (ii) provide a computationally-efficient strategy for integrating the linear cuts with the layer SDP relaxation (9).\n\nConstruction of valid linear cuts using RLT. RLT involves the construction of valid linear cuts on the lifting matrices {P i } L\u22121 i=0 by using products of the existing linear constraints in (9) on the original variables {x i } L i=0 . Under the constraints (9a) and (9d), the variables x i and x i+1 satisfy:\nx i \u2265 0, x i \u2212 l i \u2265 0, x i \u2212 u i \u2264 0, x i+1 \u2212 l i+1 \u2265 0, x i+1 \u2212 u i+1 \u2264 0.\nThese can be used to construct the constraints: \nx i l T i \u2264 x i x T i \u2264 x i u T i , (x i+1 \u2212 l i+1 )(x i \u2212 l i ) T \u2265 0,P i [x i x T i ], P i [x i+1 x T i+1 ], P i [x i+1 x T i ])\nby adding linear cuts (12b), (12c) or (14), with\nx i+1 = ReLU(x i \u2212 1) andl i+1 \u2264 x i \u2212 1 \u2264\u00fb i+1 . Left to right columns: 1) inactive neuronl i+1 = \u22121,\u00fb i+1 = 0; 2) unstable neuronl i+1 = \u22121,\u00fb i+1 = 1; 3) strictly active neuronl i+1 = 0,\u00fb i+1 = 2. (x i+1 \u2212 l i+1 )(x i \u2212 u i ) T \u2264 0, (x i \u2212 l i )(x i+1 \u2212 u i+1 ) T \u2264 0.\nBy using (8), these nonlinear constraints are linearised as\nP i [x i ]l T i \u2264 P i [x i x T i ] \u2264 P i [x i ]u T i ,(11a)P i [x i+1 x T i ] \u2265 P i [x i+1 ]l T i + l i+1 (P i [x T i ] \u2212 l T i ),(11b)P i [x i+1 x T i ] \u2264 P i [x i+1 ]u T i + l i+1 (P i [x T i ] \u2212 u T i ), (11c) P i [x i x T i+1 ] \u2264 P i [x i ]u T i+1 + l i (P i [x T i+1 ] \u2212 u T i+1 ). (11d)\nWe now make the following remarks. Observation 1. The linear cuts (11a) -(11d) imply (10). Observation 2. The existing constraints (9d) and (9f) are stronger than the first part of (11a); while (9d) is stronger than the diagonal components of the second part of (11a).\n\nThese observations show that the targeted bounding in (10) can be realised by adding to the layer SDP relaxation (9) the following linear cuts for each P i , i \u2208 I L\u22121 :\nP i [x i x T i ] \u2264 P i [x i ]u T i ,(12a)P i [x i+1 x T i ] \u2265 P i [x i+1 ]l T i + l i+1 (P i [x T i ] \u2212 l T i ),(12b)P i [x i+1 x T i ] \u2264 min{P i [x i+1 ]u T i + l i+1 (P i [x T i ] \u2212 u T i ), u i+1 P i [x T i ] + (P i [x i+1 ] \u2212 u i+1 )l T i },(12c)\nwhere the diagonal components of (12a) are redundant.\n\nIt has been shown above that adding the linear cuts in (12) to the layer SDP relaxation (9) is efficient to bound P i [x i+1x\n\nT i+1 ] and subsequently the matrix P i . Problem (9) also has other existing linear constraints (9a), (9b) and (9h) that can be used to construct the new constraints:\n(x i+1 \u2212 W i x i \u2212 b i ) (x i+1 \u2212 W i x i \u2212 b i ) T \u2265 0, (13a) (x i+1 \u2212 A i x i \u2212 B i ) x T i+1 \u2264 0, (13b) (x i+1 \u2212 A i x i \u2212 B i ) (x i+1 \u2212 A i x i \u2212 B i ) T \u2265 0. (13c)\nObservation 3. Linear cut (13a) is weaker than (9f); while (13c) is weaker than the conjunction of (9a) -(9c) and (9h).\n\nObservation 4. Adding the linear cut (13b) can tighten the layer SDP relaxation, but only its off-diagonals cut the feasible region, while the diagonals are implied by (9c).\n\nThese observations reveal that the constraint (13b) has not been included in the layer SDP relaxation (9) and it can narrow the relaxation gap. By defining P\ni [x i+1 x T i+1 ] = x i+1 x T i+1 and P i [x i x T i+1 ] = x i x T i+1 and recalling that P i [x i+1 ] = P i+1 [x i+1\n] under (9g), the constraints (12a) and (13b) are merged as a linear cut for each P i , i \u2208 I L\u22121 :\nP i [x i+1 x T i+1 ] \u2264 min{P i [x i+1 ]u T i+1 , A i P i [x i x T i+1 ] + B i P i [x T i+1 ]}.(14)\nWhen i = 0, P 1 [x 0 x T 0 ] \u2264 P 1 [x 0 ]u T 0 is also needed. Integration of linear cuts with the layer SDP relaxation. The above analysis identifies the valid linear cuts (12b), (12c) and (14) for each matrix P i , i \u2208 I L\u22121 . Adding them to (9) yields the layer RLT-SDP relaxation:\n\u03b3 * RLT-SDP := min {Pi} L\u22121 i=0 c T P L\u22121 [x L ] + c 0 s.t. (9a) \u2212 (9h), (12b), (12c), (14).(15)\nSimple numerical examples in Figure 1 show that adding each of linear cuts (12b), (12c) and (14) shrinks the relaxation region of (P\ni [x i x T i ], P i [x i+1 x T i+1 ], P i [x i+1 x T i ])\nand thus tightens the layer SDP relaxation. It follows that the layer RLT-SDP relaxation (15) offers a tighter bound than the layer SDP relaxation (9), or formally:\nTheorem 1. \u03b3 * GlobalSDP \u2264 \u03b3 * LayerSDP \u2264 \u03b3 * RLT-SDP \u2264 \u03b3 * .\nAlgorithm 1: Implementation of layer RLT-SDP relaxation 1: Input: NN parameters, {p s } r s=1 , k max . 2: Initialise: Set the order to add the linear cuts using matrices O i , i \u2208 I L\u22121 . Set k = 1. 3: while \u03b3 * RLT-SDP < 0 and k \u2264 k max do 4: Setp i as the integer part of the product p k n i , i \u2208 I L\u22121 . 5: Solve (15), where for each P i , i \u2208 I L\u22121 , adding onl\u0233 p i elements (with corresponding indexes in O i ) of each row in (12b) and (12c). 6: Set k = k + 1. 7: end while 8: Output: \u03b3 *\n\n\nRLT-SDP\n\nProof. Following the principle of RLT, the added linear cuts (12b), (12c) and (14) can always be deduced from the original linear constraints in the layer SDP relaxation (9). Hence, the optimal objective value of the layer RLT-SDP relaxation (15) still serves as a lower bound to that of the QCQP and the original verification problem (1). Moreover, adding these valid linear cuts can shrink the feasible region, as shown in Observations 1 and 4. This means that every solution to (15) is feasible to (9). Therefore, the layer RLT-SDP relaxation is at least as tight as the original layer SDP relaxation.\n\nEfficient implementation. The number of linear inequalities introduced by (12b), (12c) and (14) for each P i , i \u2208 I L\u22121 , are n i n i+1 , 2n i n i+1 and 2n i+1 (n i+1 \u2212 1) (by removing diagonals), respectively. For P 0 , n 0 (n 0 \u2212 1) extra linear inequalities are needed. The total number of inequalities for each P i , i = 1, 2, . . . , L \u2212 1, is (2n 2 i+1 + 3n i n i+1 \u2212 2n i+1 ), and for P 0 is (2n 2 1 + 3n 0 n 1 \u2212 2n 1 + n 2 0 \u2212 n 0 ). Compared to directly imposing the constraints in (10) (which introduces 2(n i + n i+1 ) 2 inequalities), adding (12b), (12c) and (14) has a lower computational burden, especially for large NNs. However, adding all the inequalities in (12b), (12c) and (14) is still computationally expensive. An efficient strategy for integrating them with the layer SDP relaxation is thus necessary. The strategy we deploy is based on two observations:\n\n\u2022 The linear cuts (12b) and (12c) capture inter-layer dependencies (i.e., terms x i+1 x T i ). Since x i+1 = ReLU(W i x i + b i ), the dependencies are also reflected in the weighting matrix W i . Hence, the structure of W i can be exploited to efficiently adding (12b) and (12c). \u2022 The linear cut (14) captures the intra-layer interactions (i.e., terms x i x T i ), which cannot be clearly indicated by NN parameters (weights or biases). Given these observations, in the following we use the linear cuts (12b) and (12c). Moreover, it is straightforward to show that Theorem 1 holds even by adding a portion of (12b) and (12c). Hence, the computational cost of the problem can be reduced by adding only a portion of them.\n\nAlgorithm 1 describes an efficient implementation of the layer RLT-SDP relaxation. The fraction of linear cuts added at each iteration are set by choosing the sequence {p s } r s=1 , where 0 \u2264 p 1 < \u00b7 \u00b7 \u00b7 < p r \u2264 1. In practice, the sequence {p s } r s=1 and the maximum iteration k max , which satisfies 1 \u2264 k max \u2264 r, can be adapted to the computational \nx i+1 = ReLU(W i x i + 0.1), with W i = [0.5 \u22121], x i = [x i,1 , x i,2 ] T and 0 \u2264 x i,1 , x i,2 \u2264\n1, by adding a part or all of linear cuts (12b) and (12c). Adding only the linear cuts about x i+1 x i,2 (i.e., the larger element of |W i |) yields a feasible region close to the one with full constraints on x i+1 x T i .\n\npower available. Note that, in principle, a different sequence {p s } r s=1 can be chosen for each individual layer; for simplicity we consider these to be constant. The matrix O i stores the ordering (in descending order) of the elements in each row of |W i |. The purpose of creating the ordering is to ensure that the part of linear cuts with larger influences on shrinking the feasible region are added first. This is based on the consideration as follows: For the neuron m at layer i + 1, :) is a row vector. Let w 1 and w 2 be any two elements of W i (m, :) and their corresponding inputs are x i,1 and x i,2 , respectively. If |w 1 | > |w 2 |, then compared to those linear cuts about x i+1 x i,2 , the linear cuts about x i+1 x i,1 has bigger influence on the feasible region of x i+1,m . Figure 2 provides an example for this, where it is seen that the linear cuts about x i+1 x i,2 contributes more than x i+1 x i,1 in shrinking the feasible region of x i+1 .\nits pre-activation isx i+1,m = W i (m, :)x i + b i (m), where W i (m,\nWe can show the following property of Algorithm 1: Theorem 2. The relation \u03b3 * LayerSDP \u2264 \u03b3 * RLT-SDP \u2264 \u03b3 * holds under any choice of {p s } r s=1 . At any given iteration k of Algorithm 1, we have that \u03b3 * RLT-SDP k \u2264 \u03b3 * RLT-SDP k+1 \u2264 \u03b3 * . Proof. It is straightforward to prove the first part following Theorem 1. At each iteration k, the proportion of linear cuts added isp i , the integer part of p k n i . Since p k+1 > p k , the proportion added at iteration k + 1 is larger than that at iteration k and contains it as a subset. Hence, for any k \u2265 1, every feasible solution to the optimisation problem solved at iteration k + 1 is also a solution to the problem solved at iteration k, i.e., \u03b3 * RLT-SDP k \u2264 \u03b3 * RLT-SDP k+1 . At each iteration, the layer RLT-SDP relaxation (15) is solved with a total number of L\u22121 i=1 3p i n i+1 linear cuts. This is computationally lighter than the problem obtained by adding all the inequalities in (12b) and (12c). Furthermore, before running the algorithm, we can also remove the inactive neurons and simplify the constraints of stable neurons to reduce the sizes of the constraints P i \u2ab0 0, i \u2208 I L\u22121 . This can be realised by examining the activation pattern of the NN under a given verification query and will not relax the solution. This is a strategy used in (Batten et al. 2021).\n\n\nExperimental Evaluation\n\nTwo sets of experiments were carried out to evaluate the precision and scalability of relaxation proposed as well as Algorithm 1. The experiments were run on a Linux machine with an Intel i9-10920X 3.5 GHz 12-core CPU with 128 GB RAM. The optimisation problems were modelled by using YALMIP (Lofberg 2004) and solved using MOSEK (Andersen and Andersen 2000). We compared the results obtained against presently available SoA methods and tools.\n\nNetworks. In Experiment 1, we considered two groups of two-input, two-output, fully-connected random ReLU NNs generated by using the method in (Fazlyab, Morari, and Pappas 2022). Group 1 had four models with L = 4, 6, 8, 10 hidden layers, respectively, and 15 neurons for each hidden layer. Group 2 had four three-layer models, with n i = 10, 15, 50, 100 neurons per hidden layer, respectively.\n\nIn Experiment 2, we considered three groups of fullyconnected ReLU NNs trained on the MNIST dataset. These are widely used in all the SDP benchmarks (By \"m \u00d7 n\" we mean a NN with m\u22121 hidden layers each having n neurons, which is consistent with (Batten et al. 2021).):\n\n\u2022 Small NNs: MLP-Adv, MLP-LP and MLP-SDP from (Raghunathan, Steinhardt, and Liang 2018) and tested under the same perturbation \u03f5 = 0.1 as in (Raghunathan, Steinhardt, and Liang 2018;Batten et al. 2021). \u2022 Medium NNs: Models 6 \u00d7 100 and 9 \u00d7 100 from (Singh et al. 2019a) and evaluated under the same \u03f5 = 0.026 and \u03f5 = 0.015 as in (Singh et al. 2019a;Tjandraatmadja et al. 2020;M\u00fcller et al. 2021;Batten et al. 2021) \u2022 Large NNs: Models 8 \u00d7 1024-0.1 and 8 \u00d7 1024-0.3 from (Li et al. 2020), which were trained using CROWN-IBP (Zhang et al. 2019) with adversarial attack \u03f5 = 0.1, 0.3, respectively. As in (Li et al. 2020), they were tested under the perturbations \u03f5 = 0.1, 0.3, respectively.\n\nBaseline methods. In Experiment 2, we compared the proposed layer RLT-SDP relaxation (referred to as RLT-SDP) against the SoA methods for verification below:\n\n\u2022 Complete methods: MILP (Tjeng, Xiao, and Tedrake 2019), AI 2 (Gehr et al. 2018), and \u03b2-CROWN (Wang et al. 2021). \u2022 Linear relaxations: the standard linear programming relaxation LP (Ehlers 2017) and its variants including IBP , OptC2V PRIMA (M\u00fcller et al. 2021). We did not consider kPoly (Singh et al. 2019b) and\n\nDeepPoly (Singh et al. 2019a) , as they were shown in (M\u00fcller et al. 2021) to be weaker than PRIMA. \u2022 SDP relaxations: LayerSDP (Batten et al. 2021), SDP-IP (i.e., the global SDP relaxation (7)) (Raghunathan, Steinhardt, and Liang 2018), and SDP-FO (Dathathri et al. 2020). Experiment 1: Efficacy of the proposed strategy. We investigated both the network depth and width by using RLT-SDP to obtain an over-approximation of the feasible output region of the NN for a given input set. The test inputs were random values within [0, 1] and the heuristic method  in (Fazlyab, Morari, and Pappas 2022) was adopted to compute the over-approximations. Algorithm 1 was run with {p s } 11 s=1 = {0, 0.1, \u00b7 \u00b7 \u00b7 , 1} and k max = 11. Without linear cuts (p 1 = 0), RLT-SDP is equivalent to LayerSDP.\n\nWe first studied the impact of network depth on the verification method here proposed by using the models in Group 1. Figure 3 shows that for all the four models considered, adding a larger percentage of linear cuts yields a tighter overapproximation. As the number of hidden layers L increases, LayerSDP becomes looser and the effects of adding linear cuts becomes more significant. The figures show that across all models, even using just 20% of the linear cuts considerably reduces the over-approximation. To further analyse the gain in the approximation versus the corresponding increase in computational complexity, we considered two metrics: the improvement in approximation (or tightness) and the runtime increase. The former is the relative reduction in the feasible output regions obtained by RLT-SDP and LayerSDP; the latter is the relative increase in their runtime. As expected, it is shown in Figure 4 that adding a larger  (Wang et al. 2021), while MILP, AI 2 and IBP from (Li et al. 2020); The first four numbers of LP are from (Batten et al. 2021), and the last two are obtained by implementation with interval arithmetic bounds. Dash (-): previously reported numbers are unavailable. Diamond (\u22c4): the methods fail to verify any instance. Star ( * ): the runtime is estimated by running over five images using the same interval arithmetic bounds. Vertical line (|): the certified robustness on the left and right are obtained using interval arithmetic bounds and symbolic interval propagation, respectively.\n\nproportion of linear cuts yields a tighter over-approximation, along with an increase in runtime. Adding the same percentage of linear cuts leads to a more significant tightness improvement on larger networks (with larger L) than on smaller ones. For each network, as the percentage of linear cuts increases, the tightness improvement becomes less significant, but the runtime increase becomes more significant. Particularly, experimentally we found that the first 20% of linear cuts contributes most significantly to the improvement in overall tightness of the method. We evaluated the impact of network width by using the models in Group 2 and observed very similar behaviour of the method. These results clearly confirm Theorem 2 and demonstrate the efficiency of Algorithm 1. They also indicate that a tradeoff needs to be balanced between the tightness improvement and runtime increase. Specifically, the addition of 20% of linear cuts could be sufficient to improve considerably the precision of the SDP approach without incurring the higher computational costs associated with larger problems.\n\nExperiment 2: Comparison with the SoA methods. We benchmarked the technique on the NNs built on the MNIST dataset described above. All experiments were run on the first 100 images of the dataset. The results obtained are reported in Table 1, where the runtime is the solver time. The PGD upper bounds of MLP-Adv, MLP-LP, MLP-SDP, 6\u00d7100 and 9\u00d7100 are taken from (Batten et al. 2021), while those of 8 \u00d7 1024-0.1 and 8 \u00d7 1024-0.3 are from (Li et al. 2020). We ran Algorithm 1 with the sequence {0.1, 0.2} and k max = 2. As in LayerSDP, we further optimised RLT-SDP by removing inactive neurons in the first step.\n\nOur results show that RLT-SDP based on the interval arithmetic bounds is more precise than LayerSDP under the same bounds and all other baseline methods for all the networks. One exception is the 9 \u00d7 100 network, for which \u03b2-CROWN achieves the highest precision. By using the tighter symbolic bound propagation (Botoeva et al. 2020), RLT-SDP significantly outperformed all the incomplete/complete baseline methods.\n\nAs expected we found RLT-SDP to be significantly more computationally demanding than LayerSDP across all the networks. However, it was still faster than SDP-IP for the small and median networks. Neither SDP-IP nor SDP-FO could verify the two large networks. Also, it is shown in (Batten et al. 2021) that compared to LayerSDP, SDP-FO has a runtime that is much larger for MLP-Adv and MLP-LP, but smaller for MLP-SDP. SDP-FO fails to verify 6 \u00d7 100 and 9 \u00d7 100. These results confirm that RLT-SDP remains competitive in terms of computational efficiency. We note that the runtime of LayerSDP in Table 1 is larger than that reported in (Batten et al. 2021). This is because we directly solved the layer SDP relaxation (9), without implementing SparseColO (Fujisawa and et al. 2009) or the automatic model transformation as in (Batten et al. 2021). Their work shows that using subroutine from SparseColO can balance the size of semidefinite constraints and equality constraints, and using automatic model transformation can reduce YALMIP overhead time, both of which significantly improve the efficiency of LayerSDP. Note, however, that these techniques are also directly applicable to RLT-SDP. Hence, the results presented here provides a like-for-like comparison between LayerSDP and RLT-SDP.\n\n\nConclusions\n\nWhile SDP-based algorithms have shown their promise as a next generation method to verify NN, their resulting overapproximations are still too coarse to verify deep and large NNs. In this paper we put forward a novel SDP relaxation for achieving tight and efficient neural network robustness verification. We did so by combining the layerwise SDP relaxation with RLT. We showed that the method always yields tighter bounds than the present SoA. We also illustrated how a careful choice of linear cuts can mitigate the additional computational cost, thereby resulting in an overall tight and computationally balanced technique. The experiments reported demonstrated that the method achieves SoA on all benchmarks commonly used in the area.\n\nFigure 1 :\n1Feasible region of the triple (\n\nFigure 2 :\n2Feasible region of\n\nFigure 3 :\n3Over-approximated output region by RLT-SDP with different percentages of linear cuts for networks of different hidden layers L. The 0% case is LayerSDP.\n\nFigure 4 :\n4Tightness improvement and runtime increase obtained by solving RLT-SDP with different percentages of linear cuts for networks of different hidden layers L. The 0% case is LayerSDP.\n\n\nModels PGD Certified Time * Cert. \u2020 Time * Cert. \u2020 Time * Cert. \u2020 Cert. \u2020 Cert. \u2020 Cert. \u2020 Cert. \u2020 Cert. \u2020 Cert. \u2020 Cert. \u2020Table 1: Certified robustness (in percentage) and runtime per image (in seconds) for a set of benchmarks with various sizes. Dagger ( \u2020 ): these results are directly taken from the literature: LayerSDP and SDP-FO from (Batten et al. 2021), SDP-IP from (Raghunathan, Steinhardt, and Liang 2018), OptC2V from (Tjandraatmadja et al. 2020), PRIMA from (M\u00fcller et al. 2021), \u03b2-CROWN fromRLT-SDP \n\nLayerSDP \nSDP-IP \nSDP-FO LP OptC2V PRIMA \u03b2-CROWN MILP AI 2 IBP \n\nMLP-Adv \n94% 88 | 94 3622 80 | 91 2164 82 12079 84 \n65 \n-\n-\n-\n-\n-\n-\nMLP-LP \n80% 80 | 80 159 80 | 80 145 \n80 50733 78 \n79 \n-\n-\n-\n-\n-\n-\nMLP-SDP \n84% 84 | 84 11141 80 | 84 4373 80 43156 64 \n35 \n-\n-\n-\n-\n-\n-\n6 \u00d7 100 \n91% 82 | 90 3297 60 | 75 1900 \n-6760 \n\u22c4 \n21 \n42.9 \n51.0 \n69.9 \n-\n-\n-\n9 \u00d7 100 \n86% 56 | 70 10800 22 | 35 7119 \n-11899 \u22c4 \n18 \n38.4 \n42.8 \n62.0 \n-\n-\n-\n8\u00d71024-0.1 89% 82 | \n5883 \n-\n2932 \n\u22c4 \n\u22c4 \n\u22c4 \n0 \n-\n-\n-\n67 \n52 \n80 \n8\u00d71024-0.3 26% 26 | \n761 \n-\n469 \n\u22c4 \n\u22c4 \n\u22c4 \n0 \n-\n-\n-\n7 \n16 \n22 \n\n\nAcknowledgementsThis work is partly funded by DARPA under the Assured Autonomy programme (FA8750-18-C-0095). Alessio Lomuscio is supported by a Royal Academy of Engineering Chair in Emerging Technologies.\nFormal Verification of Neural Agents in Nondeterministic Environments. M E Akintunde, E Botoeva, P Kouvaros, A Lomuscio, Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS20). the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS20)Akintunde, M. E.; Botoeva, E.; Kouvaros, P.; and Lomuscio, A. 2020a. Formal Verification of Neural Agents in Non- deterministic Environments. In Proceedings of the 19th In- ternational Conference on Autonomous Agents and Multia- gent Systems (AAMAS20), 25-33.\n\nVerifying Strategic Abilities of Neural-symbolic Multi-agent Systems. M E Akintunde, E Botoeva, P Kouvaros, A Lomuscio, Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning (KR20). the International Conference on Principles of Knowledge Representation and Reasoning (KR20)17Akintunde, M. E.; Botoeva, E.; Kouvaros, P.; and Lomuscio, A. 2020b. Verifying Strategic Abilities of Neural-symbolic Multi-agent Systems. In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning (KR20), volume 17, 22-32.\n\nThe MOSEK interior point optimizer for linear programming: an implementation of the homogeneous algorithm. E D Andersen, K D Andersen, High performance optimization. SpringerAndersen, E. D.; and Andersen, K. D. 2000. The MOSEK interior point optimizer for linear programming: an imple- mentation of the homogeneous algorithm. In High perfor- mance optimization, 197-232. Springer.\n\nStrong mixed-integer programming formulations for trained neural networks. R Anderson, J Huchette, W Ma, C Tjandraatmadja, J Vielma, Mathematical Programming. Anderson, R.; Huchette, J.; Ma, W.; Tjandraatmadja, C.; and Vielma, J. 2020. Strong mixed-integer programming formu- lations for trained neural networks. Mathematical Program- ming, 1-37.\n\nSemidefinite programming versus the reformulation-linearization technique for nonconvex quadratically constrained quadratic programming. K M Anstreicher, Journal of Global Optimization. 432-3Anstreicher, K. M. 2009. Semidefinite programming ver- sus the reformulation-linearization technique for nonconvex quadratically constrained quadratic programming. Journal of Global Optimization, 43(2-3): 471-484.\n\nS Bak, C Liu, Johnson , T , arXiv:2103.06624The Second International Verification of Neural Networks Competition (VNN-COMP 2021): Summary and Results. arXiv preprintBak, S.; Liu, C.; and Johnson, T. 2021. The Second In- ternational Verification of Neural Networks Competition (VNN-COMP 2021): Summary and Results. arXiv preprint arXiv:2103.06624.\n\nMeasuring neural net robustness with constraints. O Bastani, Y Ioannou, L Lampropoulos, D Vytiniotis, A Nori, A Criminisi, Advances in Neural Information Processing Systems (NeurIPS16). Bastani, O.; Ioannou, Y.; Lampropoulos, L.; Vytiniotis, D.; Nori, A.; and Criminisi, A. 2016. Measuring neural net ro- bustness with constraints. In Advances in Neural Informa- tion Processing Systems (NeurIPS16), 2613-2621.\n\n. B Batten, P Kouvaros, A Lomuscio, Y Zheng, Batten, B.; Kouvaros, P.; Lomuscio, A.; and Zheng, Y.\n\nEfficient Neural Network Verification via Layer-based Semidefinite Relaxations and Linear Cuts. International Joint Conference on Artificial Intelligence (IJCAI21). Efficient Neural Network Verification via Layer-based Semidefinite Relaxations and Linear Cuts. In International Joint Conference on Artificial Intelligence (IJCAI21), 2184- 2190.\n\nEfficient verification of neural networks via dependency analysis. E Botoeva, P Kouvaros, J Kronqvist, A Lomuscio, R Misener, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI20). the AAAI Conference on Artificial Intelligence (AAAI20)Botoeva, E.; Kouvaros, P.; Kronqvist, J.; Lomuscio, A.; and Misener, R. 2020. Efficient verification of neural networks via dependency analysis. In Proceedings of the AAAI Con- ference on Artificial Intelligence (AAAI20), 3291-3299.\n\nEnabling certification of verification-agnostic networks via memory-efficient semidefinite programming. S Dathathri, K Dvijotham, A Kurakin, A Raghunathan, J Uesato, R Bunel, S Shankar, J Steinhardt, I Goodfellow, P Liang, K Pushmeet, Advances in Neural Information Processing Systems (NeurIPS20). Dathathri, S.; Dvijotham, K.; Kurakin, A.; Raghunathan, A.; Uesato, J.; Bunel, R.; Shankar, S.; Steinhardt, J.; Goodfel- low, I.; Liang, P.; and Pushmeet, K. 2020. Enabling certifica- tion of verification-agnostic networks via memory-efficient semidefinite programming. In Advances in Neural Informa- tion Processing Systems (NeurIPS20), 1-14.\n\nK Dvijotham, R Stanforth, S Gowal, T Mann, P Kohli, arXiv:1803.06567A dual approach to scalable verification of deep networks. arXiv preprintDvijotham, K.; Stanforth, R.; Gowal, S.; Mann, T.; and Kohli, P. 2018. A dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567.\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, International Symposium on Automated Technology for Verification and Analysis (ATVA17). Ehlers, R. 2017. Formal verification of piece-wise lin- ear feed-forward neural networks. In International Sympo- sium on Automated Technology for Verification and Analysis (ATVA17), 269-286.\n\nSafety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming. M Fazlyab, M Morari, G J Pappas, IEEE Transactions on Automatatic Control. 671Fazlyab, M.; Morari, M.; and Pappas, G. J. 2022. Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming. IEEE Transactions on Automatatic Control, 67(1): 1-15.\n\nUser's manual for SparseC-oLO: Conversion methods for sparse conic-form linear optimization problems. K Fujisawa, Dept. of Math. and Comp. Sci. Japan, Tech. Rep. Fujisawa, K.; and et al. 2009. User's manual for SparseC- oLO: Conversion methods for sparse conic-form linear op- timization problems. Dept. of Math. and Comp. Sci. Japan, Tech. Rep., 152-8552.\n\nAI 2 : Safety and robustness certification of neural networks with abstract interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, IEEE Symposium on Security and Privacy. IEEEGehr, T.; Mirman, M.; Drachsler-Cohen, D.; Tsankov, P.; Chaudhuri, S.; and Vechev, M. 2018. AI 2 : Safety and robust- ness certification of neural networks with abstract interpre- tation. In IEEE Symposium on Security and Privacy (SP18), 3-18. IEEE.\n\nI Goodfellow, J Shlens, C Szegedy, arXiv:1412.6572Explaining and harnessing adversarial examples. arXiv preprintGoodfellow, I.; Shlens, J.; and Szegedy, C. 2014. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\n\nScalable verified training for provably robust image classification. S Gowal, K D Dvijotham, R Stanforth, R Bunel, C Qin, J Uesato, R Arandjelovic, T Mann, P Kohli, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionGowal, S.; Dvijotham, K. D.; Stanforth, R.; Bunel, R.; Qin, C.; Uesato, J.; Arandjelovic, R.; Mann, T.; and Kohli, P. 2019. Scalable verified training for provably robust im- age classification. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision (IEEE/CVF19), 4842-4851.\n\nOSIP: Tightened Bound Propagation for the Verification of ReLU Neural Networks. V Hashemi, P Kouvaros, A Lomuscio, International Conference on Software Engineering and Formal Methods (SEFM21). SpringerHashemi, V.; Kouvaros, P.; and Lomuscio, A. 2021. OSIP: Tightened Bound Propagation for the Verification of ReLU Neural Networks. In International Conference on Soft- ware Engineering and Formal Methods (SEFM21), 463- 480. Springer.\n\nEfficient neural network verification via adaptive refinement and adversarial search. P Henriksen, A Lomuscio, Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI20). the Twenty-fourth European Conference on Artificial Intelligence (ECAI20)IOS PressHenriksen, P.; and Lomuscio, A. 2020. Efficient neural net- work verification via adaptive refinement and adversarial search. In Proceedings of the Twenty-fourth European Con- ference on Artificial Intelligence (ECAI20), 2513-2520. IOS Press.\n\nDEEPSPLIT: An Efficient Splitting Method for Neural Network Verification via Indirect Effect Analysis. P Henriksen, A Lomuscio, International Joint Conference on Artificial Intelligence (IJCAI21). Henriksen, P.; and Lomuscio, A. 2021. DEEPSPLIT: An Ef- ficient Splitting Method for Neural Network Verification via Indirect Effect Analysis. In International Joint Conference on Artificial Intelligence (IJCAI21), 2549-2555.\n\nReachability Analysis for Neural Network Aircraft Collision Avoidance Systems. K D Julian, M J Kochenderfer, Journal of Guidance, Control, and Dynamics. 446Julian, K. D.; and Kochenderfer, M. J. 2021. Reachabil- ity Analysis for Neural Network Aircraft Collision Avoid- ance Systems. Journal of Guidance, Control, and Dynamics, 44(6): 1132-1142.\n\nThe Marabou framework for verification and analysis of deep neural networks. G Katz, D Huang, D Ibeling, K Julian, C Lazarus, R Lim, P Shah, S Thakoor, H Wu, A Zeljic, D Dill, M Kochenderfer, C Barrett, International Conference on Computer Aided Verification (CAV19). Katz, G.; Huang, D.; Ibeling, D.; Julian, K.; Lazarus, C.; Lim, R.; Shah, P.; Thakoor, S.; Wu, H.; Zeljic, A.; Dill, D.; Kochenderfer, M.; and Barrett, C. 2019. The Marabou framework for verification and analysis of deep neural net- works. In International Conference on Computer Aided Ver- ification (CAV19), 443-452.\n\nFormal Analysis of Neural Network-Based Systems in the Aircraft Domain. P Kouvaros, T Kyono, F Leofante, A Lomuscio, D Margineantu, D Osipychev, Y Zheng, International Symposium on Formal Methods (FM21). SpringerKouvaros, P.; Kyono, T.; Leofante, F.; Lomuscio, A.; Margineantu, D.; Osipychev, D.; and Zheng, Y. 2021. For- mal Analysis of Neural Network-Based Systems in the Air- craft Domain. In International Symposium on Formal Meth- ods (FM21), 730-740. Springer.\n\nMoments, positive polynomials and their applications. J B Lasserre, World Scientific1Lasserre, J. B. 2009. Moments, positive polynomials and their applications, volume 1. World Scientific.\n\nL Li, X Qi, T Xie, B Li, arXiv:2009.04131SoK: Certified robustness for deep neural networks. arXiv preprintLi, L.; Qi, X.; Xie, T.; and Li, B. 2020. SoK: Certi- fied robustness for deep neural networks. arXiv preprint arXiv:2009.04131.\n\nAlgorithms for Verifying Deep Neural Networks. C Liu, T Arnon, C Lazarus, C Strong, C Barrett, M J Kochenderfer, Foundations and Trends\u00ae in Optimization. Liu, C.; Arnon, T.; Lazarus, C.; Strong, C.; Barrett, C.; Kochenderfer, M. J.; et al. 2020. Algorithms for Verifying Deep Neural Networks. Foundations and Trends\u00ae in Opti- mization, 3-4: 244-404.\n\nYALMIP: A toolbox for modeling and optimization in MATLAB. J Lofberg, IEEE International Conference on Robotics and Automation (ICRA04). IEEELofberg, J. 2004. YALMIP: A toolbox for modeling and op- timization in MATLAB. In IEEE International Conference on Robotics and Automation (ICRA04), 284-289. IEEE.\n\nAn approach to reachability analysis for feed-forward ReLU neural networks. A Lomuscio, L Maganti, abs/1706.07351CoRRLomuscio, A.; and Maganti, L. 2017. An approach to reach- ability analysis for feed-forward ReLU neural networks. CoRR, abs/1706.07351.\n\nStrengthened SDP verification of neural network robustness via non-convex cuts. Z Ma, S Sojoudi, arXiv:2010.08603arXiv preprintMa, Z.; and Sojoudi, S. 2020. Strengthened SDP verification of neural network robustness via non-convex cuts. arXiv preprint arXiv:2010.08603.\n\nVerification of Neural Network Compression of ACAS Xu Lookup Tables with Star Set Reachability. Manzanas Lopez, D Johnson, T Tran, H.-D Bak, S Chen, X Hobbs, K L. ; M N Makarchuk, G Singh, G P\u00fcschel, M Vechev, M , arXiv:2103.03638PRIMA: Precise and general neural network certification via multi-neuron convex relaxations. arXiv preprintAIAA Scitech 2021 Forum, 0995. M\u00fcller,Manzanas Lopez, D.; Johnson, T.; Tran, H.-D.; Bak, S.; Chen, X.; and Hobbs, K. L. 2021. Verification of Neural Net- work Compression of ACAS Xu Lookup Tables with Star Set Reachability. In AIAA Scitech 2021 Forum, 0995. M\u00fcller, M. N.; Makarchuk, G.; Singh, G.; P\u00fcschel, M.; and Vechev, M. 2021. PRIMA: Precise and general neural network certification via multi-neuron convex relaxations. arXiv preprint arXiv:2103.03638.\n\nStructured semidefinite programs and semialgebraic geometry methods in robustness and optimization. P A Parrilo, California Institute of TechnologyParrilo, P. A. 2000. Structured semidefinite programs and semialgebraic geometry methods in robustness and opti- mization. California Institute of Technology.\n\nSemidefinite relaxations for certifying robustness to adversarial examples. A Raghunathan, J Steinhardt, P Liang, Advances in Neural Information Processing Systems (NeurIPS18). Raghunathan, A.; Steinhardt, J.; and Liang, P. 2018. Semidefinite relaxations for certifying robustness to adver- sarial examples. In Advances in Neural Information Pro- cessing Systems (NeurIPS18), 10877-10887.\n\n. H Salman, G Yang, H Zhang, C Hsieh, P Zhang, Salman, H.; Yang, G.; Zhang, H.; Hsieh, C.; and Zhang, P.\n\nA convex relaxation barrier to tight robustness verification of neural networks. Advances in Neural Information Processing Systems (NeurIPS19). A convex relaxation barrier to tight robustness verifi- cation of neural networks. In Advances in Neural Informa- tion Processing Systems (NeurIPS19), 9835-9846.\n\nAn abstract domain for certifying neural networks. G Singh, T Gehr, M P\u00fcschel, M Vechev, 41:1- 41:30Proceedings of the ACM on Programming Languages. 3Singh, G.; Gehr, T.; P\u00fcschel, M.; and Vechev, M. 2019a. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL): 41:1- 41:30.\n\nBeyond the single neuron convex barrier for neural network certification. G ; R Singh, R Ganvir, M P\u00fcschel, M Vechev, Advances in Neural Information Processing Systems (NeurIPS19). Singh, G.; R. Ganvir, R.; P\u00fcschel, M.; and Vechev, M. 2019b. Beyond the single neuron convex barrier for neural network certification. In Advances in Neural Information Processing Systems (NeurIPS19), 15098-15109.\n\nThe convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. C Tjandraatmadja, R Anderson, J Huchette, W Ma, K Patel, J P Vielma, Advances in Neural Information Processing Systems (NeurIPS20). Tjandraatmadja, C.; Anderson, R.; Huchette, J.; Ma, W.; Pa- tel, K.; and Vielma, J. P. 2020. The convex relaxation bar- rier, revisited: Tightened single-neuron relaxations for neu- ral network verification. In Advances in Neural Information Processing Systems (NeurIPS20), 1-12.\n\nEvaluating robustness of neural networks with mixed integer programming. V Tjeng, K Xiao, R Tedrake, International Conference on Learning Representations. Tjeng, V.; Xiao, K.; and Tedrake, R. 2019. Evaluating ro- bustness of neural networks with mixed integer program- ming. In International Conference on Learning Represen- tations (ICLR19), 1-21.\n\nChordal graphs and semidefinite optimization. L Vandenberghe, M S Andersen, Foundations and Trends in Optimization. 14Vandenberghe, L.; and Andersen, M. S. 2015. Chordal graphs and semidefinite optimization. Foundations and Trends in Optimization, 1(4): 241-433.\n\nSemidefinite programming. L Vandenberghe, S Boyd, SIAM Review. 381Vandenberghe, L.; and Boyd, S. 1996. Semidefinite pro- gramming. SIAM Review, 38(1): 49-95.\n\nEfficient formal safety analysis of neural networks. S Wang, K Pei, J Whitehouse, J Yang, Jana , S , Advances in Neural Information Processing Systems (NeurIPS18). Wang, S.; Pei, K.; Whitehouse, J.; Yang, J.; and Jana, S. 2018. Efficient formal safety analysis of neural net- works. In Advances in Neural Information Processing Sys- tems (NeurIPS18), 6367-6377.\n\nBeta-crown: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. S Wang, H Zhang, K Xu, X Lin, S Jana, C.-J Hsieh, J Z Kolter, arXiv:2103.06624arXiv preprintWang, S.; Zhang, H.; Xu, K.; Lin, X.; Jana, S.; Hsieh, C.- J.; and Kolter, J. Z. 2021. Beta-crown: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. arXiv preprint arXiv:2103.06624.\n\nTowards fast computation of certified robustness for ReLU networks. T Weng, H Zhang, H Chen, Z Song, C Hsieh, D Boning, I Dhillon, L Daniel, International Conference on Machine Learning (ICML18). Weng, T.; Zhang, H.; Chen, H.; Song, Z.; Hsieh, C.; Boning, D.; Dhillon, I.; and Daniel, L. 2018. Towards fast computa- tion of certified robustness for ReLU networks. In Interna- tional Conference on Machine Learning (ICML18), 5276- 5285.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. E Wong, Z Kolter, International Conference on Machine Learning (ICML18). Wong, E.; and Kolter, Z. 2018. Provable defenses against adversarial examples via the convex outer adversarial poly- tope. In International Conference on Machine Learning (ICML18), 5286-5295.\n\nH Zhang, H Chen, C Xiao, S Gowal, R Stanforth, B Li, D Boning, C.-J Hsieh, arXiv:1906.06316Towards stable and efficient training of verifiably robust neural networks. arXiv preprintZhang, H.; Chen, H.; Xiao, C.; Gowal, S.; Stanforth, R.; Li, B.; Boning, D.; and Hsieh, C.-J. 2019. Towards stable and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316.\n\nOn the tightness of semidefinite relaxations for certifying robustness to adversarial examples. R Y Zhang, arXiv:2006.06759arXiv preprintZhang, R. Y. 2020. On the tightness of semidefinite re- laxations for certifying robustness to adversarial examples. arXiv preprint arXiv:2006.06759.\n\nChordal sparsity in control and optimization of large-scale systems. Y Zheng, University of OxfordPh.D. thesisZheng, Y. 2019. Chordal sparsity in control and optimiza- tion of large-scale systems. Ph.D. thesis, University of Ox- ford.\n\nChordal and factor-width decompositions for scalable semidefinite and polynomial optimization. Y Zheng, G Fantuzzi, A Papachristodoulou, Annual Reviews in Control. 52Zheng, Y.; Fantuzzi, G.; and Papachristodoulou, A. 2021. Chordal and factor-width decompositions for scalable semidefinite and polynomial optimization. Annual Reviews in Control, 52: 243-279.\n", "annotations": {"author": "[{\"end\":178,\"start\":92},{\"end\":300,\"start\":179},{\"end\":396,\"start\":301}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":189,\"start\":184},{\"end\":317,\"start\":309}]", "author_first_name": "[{\"end\":100,\"start\":92},{\"end\":183,\"start\":179},{\"end\":308,\"start\":301}]", "author_affiliation": "[{\"end\":177,\"start\":127},{\"end\":299,\"start\":211},{\"end\":395,\"start\":345}]", "title": "[{\"end\":89,\"start\":1},{\"end\":485,\"start\":397}]", "venue": null, "abstract": "[{\"end\":1464,\"start\":487}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1655,\"start\":1617},{\"end\":1779,\"start\":1754},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1808,\"start\":1779},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1835,\"start\":1808},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1956,\"start\":1939},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1983,\"start\":1956},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2213,\"start\":2196},{\"end\":2721,\"start\":2705},{\"end\":2922,\"start\":2902},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3050,\"start\":3022},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3121,\"start\":3104},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3399,\"start\":3378},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3425,\"start\":3399},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3455,\"start\":3425},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3476,\"start\":3455},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3496,\"start\":3476},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3547,\"start\":3529},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3559,\"start\":3547},{\"end\":3633,\"start\":3613},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3650,\"start\":3633},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3687,\"start\":3650},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3914,\"start\":3896},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3933,\"start\":3914},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3960,\"start\":3933},{\"end\":3979,\"start\":3960},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4055,\"start\":4042},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4102,\"start\":4079},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4123,\"start\":4102},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4145,\"start\":4123},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4184,\"start\":4171},{\"end\":4309,\"start\":4289},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4324,\"start\":4309},{\"end\":4558,\"start\":4538},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4775,\"start\":4755},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4803,\"start\":4783},{\"end\":4838,\"start\":4812},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5037,\"start\":4996},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5070,\"start\":5037},{\"end\":5089,\"start\":5070},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5220,\"start\":5179},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5507,\"start\":5495},{\"end\":5605,\"start\":5585},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5664,\"start\":5643},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5837,\"start\":5814},{\"end\":5912,\"start\":5892},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6030,\"start\":5985},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6205,\"start\":6192},{\"end\":6306,\"start\":6286},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9615,\"start\":9593},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9632,\"start\":9615},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10047,\"start\":10006},{\"end\":10095,\"start\":10075},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10814,\"start\":10800},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10827,\"start\":10814},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11211,\"start\":11170},{\"end\":11932,\"start\":11912},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13290,\"start\":13268},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13307,\"start\":13290},{\"end\":13516,\"start\":13496},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13685,\"start\":13674},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14873,\"start\":14852},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15236,\"start\":15219},{\"end\":26049,\"start\":26029},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26382,\"start\":26369},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26435,\"start\":26421},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26699,\"start\":26665},{\"end\":27183,\"start\":27163},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27275,\"start\":27234},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27370,\"start\":27329},{\"end\":27388,\"start\":27370},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27457,\"start\":27437},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27537,\"start\":27517},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27564,\"start\":27537},{\"end\":27583,\"start\":27564},{\"end\":27602,\"start\":27583},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27674,\"start\":27658},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27730,\"start\":27711},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27805,\"start\":27789},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28092,\"start\":28061},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28117,\"start\":28099},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28149,\"start\":28131},{\"end\":28299,\"start\":28273},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28347,\"start\":28327},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28382,\"start\":28362},{\"end\":28427,\"start\":28407},{\"end\":28501,\"start\":28481},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28589,\"start\":28548},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28625,\"start\":28602},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28948,\"start\":28915},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30097,\"start\":30079},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30144,\"start\":30129},{\"end\":30205,\"start\":30185},{\"end\":32150,\"start\":32130},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32222,\"start\":32206},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32713,\"start\":32692},{\"end\":33095,\"start\":33076},{\"end\":33451,\"start\":33431},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33575,\"start\":33550},{\"end\":33641,\"start\":33621}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34887,\"start\":34843},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34919,\"start\":34888},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35085,\"start\":34920},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35279,\"start\":35086},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36348,\"start\":35280}]", "paragraph": "[{\"end\":2126,\"start\":1480},{\"end\":2923,\"start\":2128},{\"end\":3252,\"start\":2925},{\"end\":4559,\"start\":3254},{\"end\":5090,\"start\":4561},{\"end\":5665,\"start\":5092},{\"end\":6471,\"start\":5667},{\"end\":7523,\"start\":6473},{\"end\":7922,\"start\":7563},{\"end\":8306,\"start\":7924},{\"end\":8760,\"start\":8394},{\"end\":8819,\"start\":8762},{\"end\":8902,\"start\":8821},{\"end\":9233,\"start\":9006},{\"end\":10096,\"start\":9452},{\"end\":10205,\"start\":10098},{\"end\":10406,\"start\":10306},{\"end\":10638,\"start\":10468},{\"end\":10918,\"start\":10691},{\"end\":11044,\"start\":10986},{\"end\":11212,\"start\":11080},{\"end\":12234,\"start\":11517},{\"end\":12482,\"start\":12427},{\"end\":13873,\"start\":13186},{\"end\":14423,\"start\":14089},{\"end\":14922,\"start\":14425},{\"end\":15238,\"start\":14966},{\"end\":15303,\"start\":15240},{\"end\":15569,\"start\":15473},{\"end\":15676,\"start\":15640},{\"end\":16256,\"start\":15678},{\"end\":16567,\"start\":16258},{\"end\":16693,\"start\":16645},{\"end\":16873,\"start\":16825},{\"end\":17204,\"start\":17145},{\"end\":17766,\"start\":17498},{\"end\":17937,\"start\":17768},{\"end\":18242,\"start\":18189},{\"end\":18369,\"start\":18244},{\"end\":18538,\"start\":18371},{\"end\":18828,\"start\":18709},{\"end\":19003,\"start\":18830},{\"end\":19162,\"start\":19005},{\"end\":19381,\"start\":19282},{\"end\":19765,\"start\":19481},{\"end\":19995,\"start\":19863},{\"end\":20218,\"start\":20054},{\"end\":20777,\"start\":20281},{\"end\":21393,\"start\":20789},{\"end\":22274,\"start\":21395},{\"end\":22997,\"start\":22276},{\"end\":23355,\"start\":22999},{\"end\":23677,\"start\":23455},{\"end\":24648,\"start\":23679},{\"end\":26050,\"start\":24719},{\"end\":26520,\"start\":26078},{\"end\":26916,\"start\":26522},{\"end\":27186,\"start\":26918},{\"end\":27875,\"start\":27188},{\"end\":28034,\"start\":27877},{\"end\":28351,\"start\":28036},{\"end\":29140,\"start\":28353},{\"end\":30665,\"start\":29142},{\"end\":31767,\"start\":30667},{\"end\":32379,\"start\":31769},{\"end\":32795,\"start\":32381},{\"end\":34088,\"start\":32797},{\"end\":34842,\"start\":34104}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8393,\"start\":8307},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9005,\"start\":8903},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9382,\"start\":9234},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9451,\"start\":9382},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10305,\"start\":10206},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10467,\"start\":10407},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10690,\"start\":10639},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10985,\"start\":10919},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11079,\"start\":11045},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11516,\"start\":11213},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12299,\"start\":12235},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12426,\"start\":12299},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12928,\"start\":12483},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13043,\"start\":12928},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13185,\"start\":13043},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14088,\"start\":13874},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15472,\"start\":15304},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15639,\"start\":15570},{\"attributes\":{\"id\":\"formula_18\"},\"end\":16644,\"start\":16568},{\"attributes\":{\"id\":\"formula_19\"},\"end\":16765,\"start\":16694},{\"attributes\":{\"id\":\"formula_20\"},\"end\":16824,\"start\":16765},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17144,\"start\":16874},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17264,\"start\":17205},{\"attributes\":{\"id\":\"formula_23\"},\"end\":17340,\"start\":17264},{\"attributes\":{\"id\":\"formula_24\"},\"end\":17497,\"start\":17340},{\"attributes\":{\"id\":\"formula_25\"},\"end\":17979,\"start\":17938},{\"attributes\":{\"id\":\"formula_26\"},\"end\":18055,\"start\":17979},{\"attributes\":{\"id\":\"formula_27\"},\"end\":18188,\"start\":18055},{\"attributes\":{\"id\":\"formula_28\"},\"end\":18708,\"start\":18539},{\"attributes\":{\"id\":\"formula_29\"},\"end\":19281,\"start\":19163},{\"attributes\":{\"id\":\"formula_30\"},\"end\":19480,\"start\":19382},{\"attributes\":{\"id\":\"formula_31\"},\"end\":19862,\"start\":19766},{\"attributes\":{\"id\":\"formula_32\"},\"end\":20053,\"start\":19996},{\"attributes\":{\"id\":\"formula_33\"},\"end\":20280,\"start\":20219},{\"attributes\":{\"id\":\"formula_34\"},\"end\":23454,\"start\":23356},{\"attributes\":{\"id\":\"formula_35\"},\"end\":24718,\"start\":24649}]", "table_ref": "[{\"end\":32009,\"start\":32002},{\"end\":33398,\"start\":33391}]", "section_header": "[{\"end\":1478,\"start\":1466},{\"end\":7561,\"start\":7526},{\"end\":14964,\"start\":14925},{\"end\":20787,\"start\":20780},{\"end\":26076,\"start\":26053},{\"end\":34102,\"start\":34091},{\"end\":34854,\"start\":34844},{\"end\":34899,\"start\":34889},{\"end\":34931,\"start\":34921},{\"end\":35097,\"start\":35087}]", "table": "[{\"end\":36348,\"start\":35785}]", "figure_caption": "[{\"end\":34887,\"start\":34856},{\"end\":34919,\"start\":34901},{\"end\":35085,\"start\":34933},{\"end\":35279,\"start\":35099},{\"end\":35785,\"start\":35282}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19900,\"start\":19892},{\"end\":24175,\"start\":24173},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24484,\"start\":24476},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29268,\"start\":29260},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30056,\"start\":30048}]", "bib_author_first_name": "[{\"end\":36626,\"start\":36625},{\"end\":36628,\"start\":36627},{\"end\":36641,\"start\":36640},{\"end\":36652,\"start\":36651},{\"end\":36664,\"start\":36663},{\"end\":37198,\"start\":37197},{\"end\":37200,\"start\":37199},{\"end\":37213,\"start\":37212},{\"end\":37224,\"start\":37223},{\"end\":37236,\"start\":37235},{\"end\":37825,\"start\":37824},{\"end\":37827,\"start\":37826},{\"end\":37839,\"start\":37838},{\"end\":37841,\"start\":37840},{\"end\":38175,\"start\":38174},{\"end\":38187,\"start\":38186},{\"end\":38199,\"start\":38198},{\"end\":38205,\"start\":38204},{\"end\":38223,\"start\":38222},{\"end\":38585,\"start\":38584},{\"end\":38587,\"start\":38586},{\"end\":38854,\"start\":38853},{\"end\":38861,\"start\":38860},{\"end\":38874,\"start\":38867},{\"end\":38878,\"start\":38877},{\"end\":39252,\"start\":39251},{\"end\":39263,\"start\":39262},{\"end\":39274,\"start\":39273},{\"end\":39290,\"start\":39289},{\"end\":39304,\"start\":39303},{\"end\":39312,\"start\":39311},{\"end\":39616,\"start\":39615},{\"end\":39626,\"start\":39625},{\"end\":39638,\"start\":39637},{\"end\":39650,\"start\":39649},{\"end\":40127,\"start\":40126},{\"end\":40138,\"start\":40137},{\"end\":40150,\"start\":40149},{\"end\":40163,\"start\":40162},{\"end\":40175,\"start\":40174},{\"end\":40651,\"start\":40650},{\"end\":40664,\"start\":40663},{\"end\":40677,\"start\":40676},{\"end\":40688,\"start\":40687},{\"end\":40703,\"start\":40702},{\"end\":40713,\"start\":40712},{\"end\":40722,\"start\":40721},{\"end\":40733,\"start\":40732},{\"end\":40747,\"start\":40746},{\"end\":40761,\"start\":40760},{\"end\":40770,\"start\":40769},{\"end\":41190,\"start\":41189},{\"end\":41203,\"start\":41202},{\"end\":41216,\"start\":41215},{\"end\":41225,\"start\":41224},{\"end\":41233,\"start\":41232},{\"end\":41566,\"start\":41565},{\"end\":41976,\"start\":41975},{\"end\":41987,\"start\":41986},{\"end\":41997,\"start\":41996},{\"end\":41999,\"start\":41998},{\"end\":42380,\"start\":42379},{\"end\":42728,\"start\":42727},{\"end\":42736,\"start\":42735},{\"end\":42746,\"start\":42745},{\"end\":42765,\"start\":42764},{\"end\":42776,\"start\":42775},{\"end\":42789,\"start\":42788},{\"end\":43094,\"start\":43093},{\"end\":43108,\"start\":43107},{\"end\":43118,\"start\":43117},{\"end\":43408,\"start\":43407},{\"end\":43417,\"start\":43416},{\"end\":43419,\"start\":43418},{\"end\":43432,\"start\":43431},{\"end\":43445,\"start\":43444},{\"end\":43454,\"start\":43453},{\"end\":43461,\"start\":43460},{\"end\":43471,\"start\":43470},{\"end\":43487,\"start\":43486},{\"end\":43495,\"start\":43494},{\"end\":44011,\"start\":44010},{\"end\":44022,\"start\":44021},{\"end\":44034,\"start\":44033},{\"end\":44452,\"start\":44451},{\"end\":44465,\"start\":44464},{\"end\":44996,\"start\":44995},{\"end\":45009,\"start\":45008},{\"end\":45396,\"start\":45395},{\"end\":45398,\"start\":45397},{\"end\":45408,\"start\":45407},{\"end\":45410,\"start\":45409},{\"end\":45741,\"start\":45740},{\"end\":45749,\"start\":45748},{\"end\":45758,\"start\":45757},{\"end\":45769,\"start\":45768},{\"end\":45779,\"start\":45778},{\"end\":45790,\"start\":45789},{\"end\":45797,\"start\":45796},{\"end\":45805,\"start\":45804},{\"end\":45816,\"start\":45815},{\"end\":45822,\"start\":45821},{\"end\":45832,\"start\":45831},{\"end\":45840,\"start\":45839},{\"end\":45856,\"start\":45855},{\"end\":46324,\"start\":46323},{\"end\":46336,\"start\":46335},{\"end\":46345,\"start\":46344},{\"end\":46357,\"start\":46356},{\"end\":46369,\"start\":46368},{\"end\":46384,\"start\":46383},{\"end\":46397,\"start\":46396},{\"end\":46774,\"start\":46773},{\"end\":46776,\"start\":46775},{\"end\":46910,\"start\":46909},{\"end\":46916,\"start\":46915},{\"end\":46922,\"start\":46921},{\"end\":46929,\"start\":46928},{\"end\":47194,\"start\":47193},{\"end\":47201,\"start\":47200},{\"end\":47210,\"start\":47209},{\"end\":47221,\"start\":47220},{\"end\":47231,\"start\":47230},{\"end\":47242,\"start\":47241},{\"end\":47244,\"start\":47243},{\"end\":47557,\"start\":47556},{\"end\":47880,\"start\":47879},{\"end\":47892,\"start\":47891},{\"end\":48138,\"start\":48137},{\"end\":48144,\"start\":48143},{\"end\":48432,\"start\":48424},{\"end\":48441,\"start\":48440},{\"end\":48452,\"start\":48451},{\"end\":48463,\"start\":48459},{\"end\":48470,\"start\":48469},{\"end\":48478,\"start\":48477},{\"end\":48487,\"start\":48486},{\"end\":48496,\"start\":48488},{\"end\":48509,\"start\":48508},{\"end\":48518,\"start\":48517},{\"end\":48529,\"start\":48528},{\"end\":48539,\"start\":48538},{\"end\":49226,\"start\":49225},{\"end\":49228,\"start\":49227},{\"end\":49509,\"start\":49508},{\"end\":49524,\"start\":49523},{\"end\":49538,\"start\":49537},{\"end\":49825,\"start\":49824},{\"end\":49835,\"start\":49834},{\"end\":49843,\"start\":49842},{\"end\":49852,\"start\":49851},{\"end\":49861,\"start\":49860},{\"end\":50287,\"start\":50286},{\"end\":50296,\"start\":50295},{\"end\":50304,\"start\":50303},{\"end\":50315,\"start\":50314},{\"end\":50639,\"start\":50638},{\"end\":50643,\"start\":50640},{\"end\":50652,\"start\":50651},{\"end\":50662,\"start\":50661},{\"end\":50673,\"start\":50672},{\"end\":51072,\"start\":51071},{\"end\":51090,\"start\":51089},{\"end\":51102,\"start\":51101},{\"end\":51114,\"start\":51113},{\"end\":51120,\"start\":51119},{\"end\":51129,\"start\":51128},{\"end\":51131,\"start\":51130},{\"end\":51558,\"start\":51557},{\"end\":51567,\"start\":51566},{\"end\":51575,\"start\":51574},{\"end\":51881,\"start\":51880},{\"end\":51897,\"start\":51896},{\"end\":51899,\"start\":51898},{\"end\":52125,\"start\":52124},{\"end\":52141,\"start\":52140},{\"end\":52311,\"start\":52310},{\"end\":52319,\"start\":52318},{\"end\":52326,\"start\":52325},{\"end\":52340,\"start\":52339},{\"end\":52351,\"start\":52347},{\"end\":52355,\"start\":52354},{\"end\":52752,\"start\":52751},{\"end\":52760,\"start\":52759},{\"end\":52769,\"start\":52768},{\"end\":52775,\"start\":52774},{\"end\":52782,\"start\":52781},{\"end\":52793,\"start\":52789},{\"end\":52802,\"start\":52801},{\"end\":52804,\"start\":52803},{\"end\":53164,\"start\":53163},{\"end\":53172,\"start\":53171},{\"end\":53181,\"start\":53180},{\"end\":53189,\"start\":53188},{\"end\":53197,\"start\":53196},{\"end\":53206,\"start\":53205},{\"end\":53216,\"start\":53215},{\"end\":53227,\"start\":53226},{\"end\":53623,\"start\":53622},{\"end\":53631,\"start\":53630},{\"end\":53889,\"start\":53888},{\"end\":53898,\"start\":53897},{\"end\":53906,\"start\":53905},{\"end\":53914,\"start\":53913},{\"end\":53923,\"start\":53922},{\"end\":53936,\"start\":53935},{\"end\":53942,\"start\":53941},{\"end\":53955,\"start\":53951},{\"end\":54376,\"start\":54375},{\"end\":54378,\"start\":54377},{\"end\":54637,\"start\":54636},{\"end\":54899,\"start\":54898},{\"end\":54908,\"start\":54907},{\"end\":54920,\"start\":54919}]", "bib_author_last_name": "[{\"end\":36638,\"start\":36629},{\"end\":36649,\"start\":36642},{\"end\":36661,\"start\":36653},{\"end\":36673,\"start\":36665},{\"end\":37210,\"start\":37201},{\"end\":37221,\"start\":37214},{\"end\":37233,\"start\":37225},{\"end\":37245,\"start\":37237},{\"end\":37836,\"start\":37828},{\"end\":37850,\"start\":37842},{\"end\":38184,\"start\":38176},{\"end\":38196,\"start\":38188},{\"end\":38202,\"start\":38200},{\"end\":38220,\"start\":38206},{\"end\":38230,\"start\":38224},{\"end\":38599,\"start\":38588},{\"end\":38858,\"start\":38855},{\"end\":38865,\"start\":38862},{\"end\":39260,\"start\":39253},{\"end\":39271,\"start\":39264},{\"end\":39287,\"start\":39275},{\"end\":39301,\"start\":39291},{\"end\":39309,\"start\":39305},{\"end\":39322,\"start\":39313},{\"end\":39623,\"start\":39617},{\"end\":39635,\"start\":39627},{\"end\":39647,\"start\":39639},{\"end\":39656,\"start\":39651},{\"end\":40135,\"start\":40128},{\"end\":40147,\"start\":40139},{\"end\":40160,\"start\":40151},{\"end\":40172,\"start\":40164},{\"end\":40183,\"start\":40176},{\"end\":40661,\"start\":40652},{\"end\":40674,\"start\":40665},{\"end\":40685,\"start\":40678},{\"end\":40700,\"start\":40689},{\"end\":40710,\"start\":40704},{\"end\":40719,\"start\":40714},{\"end\":40730,\"start\":40723},{\"end\":40744,\"start\":40734},{\"end\":40758,\"start\":40748},{\"end\":40767,\"start\":40762},{\"end\":40779,\"start\":40771},{\"end\":41200,\"start\":41191},{\"end\":41213,\"start\":41204},{\"end\":41222,\"start\":41217},{\"end\":41230,\"start\":41226},{\"end\":41239,\"start\":41234},{\"end\":41573,\"start\":41567},{\"end\":41984,\"start\":41977},{\"end\":41994,\"start\":41988},{\"end\":42006,\"start\":42000},{\"end\":42389,\"start\":42381},{\"end\":42733,\"start\":42729},{\"end\":42743,\"start\":42737},{\"end\":42762,\"start\":42747},{\"end\":42773,\"start\":42766},{\"end\":42786,\"start\":42777},{\"end\":42796,\"start\":42790},{\"end\":43105,\"start\":43095},{\"end\":43115,\"start\":43109},{\"end\":43126,\"start\":43119},{\"end\":43414,\"start\":43409},{\"end\":43429,\"start\":43420},{\"end\":43442,\"start\":43433},{\"end\":43451,\"start\":43446},{\"end\":43458,\"start\":43455},{\"end\":43468,\"start\":43462},{\"end\":43484,\"start\":43472},{\"end\":43492,\"start\":43488},{\"end\":43501,\"start\":43496},{\"end\":44019,\"start\":44012},{\"end\":44031,\"start\":44023},{\"end\":44043,\"start\":44035},{\"end\":44462,\"start\":44453},{\"end\":44474,\"start\":44466},{\"end\":45006,\"start\":44997},{\"end\":45018,\"start\":45010},{\"end\":45405,\"start\":45399},{\"end\":45423,\"start\":45411},{\"end\":45746,\"start\":45742},{\"end\":45755,\"start\":45750},{\"end\":45766,\"start\":45759},{\"end\":45776,\"start\":45770},{\"end\":45787,\"start\":45780},{\"end\":45794,\"start\":45791},{\"end\":45802,\"start\":45798},{\"end\":45813,\"start\":45806},{\"end\":45819,\"start\":45817},{\"end\":45829,\"start\":45823},{\"end\":45837,\"start\":45833},{\"end\":45853,\"start\":45841},{\"end\":45864,\"start\":45857},{\"end\":46333,\"start\":46325},{\"end\":46342,\"start\":46337},{\"end\":46354,\"start\":46346},{\"end\":46366,\"start\":46358},{\"end\":46381,\"start\":46370},{\"end\":46394,\"start\":46385},{\"end\":46403,\"start\":46398},{\"end\":46785,\"start\":46777},{\"end\":46913,\"start\":46911},{\"end\":46919,\"start\":46917},{\"end\":46926,\"start\":46923},{\"end\":46932,\"start\":46930},{\"end\":47198,\"start\":47195},{\"end\":47207,\"start\":47202},{\"end\":47218,\"start\":47211},{\"end\":47228,\"start\":47222},{\"end\":47239,\"start\":47232},{\"end\":47257,\"start\":47245},{\"end\":47565,\"start\":47558},{\"end\":47889,\"start\":47881},{\"end\":47900,\"start\":47893},{\"end\":48141,\"start\":48139},{\"end\":48152,\"start\":48145},{\"end\":48438,\"start\":48433},{\"end\":48449,\"start\":48442},{\"end\":48457,\"start\":48453},{\"end\":48467,\"start\":48464},{\"end\":48475,\"start\":48471},{\"end\":48484,\"start\":48479},{\"end\":48506,\"start\":48497},{\"end\":48515,\"start\":48510},{\"end\":48526,\"start\":48519},{\"end\":48536,\"start\":48530},{\"end\":49236,\"start\":49229},{\"end\":49521,\"start\":49510},{\"end\":49535,\"start\":49525},{\"end\":49544,\"start\":49539},{\"end\":49832,\"start\":49826},{\"end\":49840,\"start\":49836},{\"end\":49849,\"start\":49844},{\"end\":49858,\"start\":49853},{\"end\":49867,\"start\":49862},{\"end\":50293,\"start\":50288},{\"end\":50301,\"start\":50297},{\"end\":50312,\"start\":50305},{\"end\":50322,\"start\":50316},{\"end\":50649,\"start\":50644},{\"end\":50659,\"start\":50653},{\"end\":50670,\"start\":50663},{\"end\":50680,\"start\":50674},{\"end\":51087,\"start\":51073},{\"end\":51099,\"start\":51091},{\"end\":51111,\"start\":51103},{\"end\":51117,\"start\":51115},{\"end\":51126,\"start\":51121},{\"end\":51138,\"start\":51132},{\"end\":51564,\"start\":51559},{\"end\":51572,\"start\":51568},{\"end\":51583,\"start\":51576},{\"end\":51894,\"start\":51882},{\"end\":51908,\"start\":51900},{\"end\":52138,\"start\":52126},{\"end\":52146,\"start\":52142},{\"end\":52316,\"start\":52312},{\"end\":52323,\"start\":52320},{\"end\":52337,\"start\":52327},{\"end\":52345,\"start\":52341},{\"end\":52757,\"start\":52753},{\"end\":52766,\"start\":52761},{\"end\":52772,\"start\":52770},{\"end\":52779,\"start\":52776},{\"end\":52787,\"start\":52783},{\"end\":52799,\"start\":52794},{\"end\":52811,\"start\":52805},{\"end\":53169,\"start\":53165},{\"end\":53178,\"start\":53173},{\"end\":53186,\"start\":53182},{\"end\":53194,\"start\":53190},{\"end\":53203,\"start\":53198},{\"end\":53213,\"start\":53207},{\"end\":53224,\"start\":53217},{\"end\":53234,\"start\":53228},{\"end\":53628,\"start\":53624},{\"end\":53638,\"start\":53632},{\"end\":53895,\"start\":53890},{\"end\":53903,\"start\":53899},{\"end\":53911,\"start\":53907},{\"end\":53920,\"start\":53915},{\"end\":53933,\"start\":53924},{\"end\":53939,\"start\":53937},{\"end\":53949,\"start\":53943},{\"end\":53961,\"start\":53956},{\"end\":54384,\"start\":54379},{\"end\":54643,\"start\":54638},{\"end\":54905,\"start\":54900},{\"end\":54917,\"start\":54909},{\"end\":54938,\"start\":54921}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218611349},\"end\":37125,\"start\":36554},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":222289088},\"end\":37715,\"start\":37127},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":117484770},\"end\":38097,\"start\":37717},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53760231},\"end\":38445,\"start\":38099},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1225955},\"end\":38851,\"start\":38447},{\"attributes\":{\"doi\":\"arXiv:2103.06624\",\"id\":\"b5\"},\"end\":39199,\"start\":38853},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5213597},\"end\":39611,\"start\":39201},{\"attributes\":{\"id\":\"b7\"},\"end\":39711,\"start\":39613},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":237101084},\"end\":40057,\"start\":39713},{\"attributes\":{\"id\":\"b9\"},\"end\":40544,\"start\":40059},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225039902},\"end\":41187,\"start\":40546},{\"attributes\":{\"doi\":\"arXiv:1803.06567\",\"id\":\"b11\"},\"end\":41492,\"start\":41189},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1931807},\"end\":41854,\"start\":41494},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":67856043},\"end\":42275,\"start\":41856},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":115803819},\"end\":42633,\"start\":42277},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206579396},\"end\":43091,\"start\":42635},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b16\"},\"end\":43336,\"start\":43093},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204960212},\"end\":43928,\"start\":43338},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":244852610},\"end\":44363,\"start\":43930},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":208232329},\"end\":44890,\"start\":44365},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":237101098},\"end\":45314,\"start\":44892},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":233949694},\"end\":45661,\"start\":45316},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":196610895},\"end\":46249,\"start\":45663},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":243990454},\"end\":46717,\"start\":46251},{\"attributes\":{\"id\":\"b24\"},\"end\":46907,\"start\":46719},{\"attributes\":{\"doi\":\"arXiv:2009.04131\",\"id\":\"b25\"},\"end\":47144,\"start\":46909},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":81981489},\"end\":47495,\"start\":47146},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15458635},\"end\":47801,\"start\":47497},{\"attributes\":{\"doi\":\"abs/1706.07351\",\"id\":\"b28\"},\"end\":48055,\"start\":47803},{\"attributes\":{\"doi\":\"arXiv:2010.08603\",\"id\":\"b29\"},\"end\":48326,\"start\":48057},{\"attributes\":{\"doi\":\"arXiv:2103.03638\",\"id\":\"b30\",\"matched_paper_id\":234246989},\"end\":49123,\"start\":48328},{\"attributes\":{\"id\":\"b31\"},\"end\":49430,\"start\":49125},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53215541},\"end\":49820,\"start\":49432},{\"attributes\":{\"id\":\"b33\"},\"end\":49926,\"start\":49822},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":67855530},\"end\":50233,\"start\":49928},{\"attributes\":{\"doi\":\"41:1- 41:30\",\"id\":\"b35\",\"matched_paper_id\":57757287},\"end\":50562,\"start\":50235},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":207796611},\"end\":50958,\"start\":50564},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220055765},\"end\":51482,\"start\":50960},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":47016770},\"end\":51832,\"start\":51484},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":124496776},\"end\":52096,\"start\":51834},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9243044},\"end\":52255,\"start\":52098},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":52347370},\"end\":52618,\"start\":52257},{\"attributes\":{\"doi\":\"arXiv:2103.06624\",\"id\":\"b42\"},\"end\":53093,\"start\":52620},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":13750928},\"end\":53530,\"start\":53095},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3659467},\"end\":53886,\"start\":53532},{\"attributes\":{\"doi\":\"arXiv:1906.06316\",\"id\":\"b45\"},\"end\":54277,\"start\":53888},{\"attributes\":{\"doi\":\"arXiv:2006.06759\",\"id\":\"b46\"},\"end\":54565,\"start\":54279},{\"attributes\":{\"id\":\"b47\"},\"end\":54801,\"start\":54567},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":235743018},\"end\":55160,\"start\":54803}]", "bib_title": "[{\"end\":36623,\"start\":36554},{\"end\":37195,\"start\":37127},{\"end\":37822,\"start\":37717},{\"end\":38172,\"start\":38099},{\"end\":38582,\"start\":38447},{\"end\":39249,\"start\":39201},{\"end\":39807,\"start\":39713},{\"end\":40124,\"start\":40059},{\"end\":40648,\"start\":40546},{\"end\":41563,\"start\":41494},{\"end\":41973,\"start\":41856},{\"end\":42377,\"start\":42277},{\"end\":42725,\"start\":42635},{\"end\":43405,\"start\":43338},{\"end\":44008,\"start\":43930},{\"end\":44449,\"start\":44365},{\"end\":44993,\"start\":44892},{\"end\":45393,\"start\":45316},{\"end\":45738,\"start\":45663},{\"end\":46321,\"start\":46251},{\"end\":47191,\"start\":47146},{\"end\":47554,\"start\":47497},{\"end\":48422,\"start\":48328},{\"end\":49506,\"start\":49432},{\"end\":50007,\"start\":49928},{\"end\":50284,\"start\":50235},{\"end\":50636,\"start\":50564},{\"end\":51069,\"start\":50960},{\"end\":51555,\"start\":51484},{\"end\":51878,\"start\":51834},{\"end\":52122,\"start\":52098},{\"end\":52308,\"start\":52257},{\"end\":53161,\"start\":53095},{\"end\":53620,\"start\":53532},{\"end\":54896,\"start\":54803}]", "bib_author": "[{\"end\":36640,\"start\":36625},{\"end\":36651,\"start\":36640},{\"end\":36663,\"start\":36651},{\"end\":36675,\"start\":36663},{\"end\":37212,\"start\":37197},{\"end\":37223,\"start\":37212},{\"end\":37235,\"start\":37223},{\"end\":37247,\"start\":37235},{\"end\":37838,\"start\":37824},{\"end\":37852,\"start\":37838},{\"end\":38186,\"start\":38174},{\"end\":38198,\"start\":38186},{\"end\":38204,\"start\":38198},{\"end\":38222,\"start\":38204},{\"end\":38232,\"start\":38222},{\"end\":38601,\"start\":38584},{\"end\":38860,\"start\":38853},{\"end\":38867,\"start\":38860},{\"end\":38877,\"start\":38867},{\"end\":38881,\"start\":38877},{\"end\":39262,\"start\":39251},{\"end\":39273,\"start\":39262},{\"end\":39289,\"start\":39273},{\"end\":39303,\"start\":39289},{\"end\":39311,\"start\":39303},{\"end\":39324,\"start\":39311},{\"end\":39625,\"start\":39615},{\"end\":39637,\"start\":39625},{\"end\":39649,\"start\":39637},{\"end\":39658,\"start\":39649},{\"end\":40137,\"start\":40126},{\"end\":40149,\"start\":40137},{\"end\":40162,\"start\":40149},{\"end\":40174,\"start\":40162},{\"end\":40185,\"start\":40174},{\"end\":40663,\"start\":40650},{\"end\":40676,\"start\":40663},{\"end\":40687,\"start\":40676},{\"end\":40702,\"start\":40687},{\"end\":40712,\"start\":40702},{\"end\":40721,\"start\":40712},{\"end\":40732,\"start\":40721},{\"end\":40746,\"start\":40732},{\"end\":40760,\"start\":40746},{\"end\":40769,\"start\":40760},{\"end\":40781,\"start\":40769},{\"end\":41202,\"start\":41189},{\"end\":41215,\"start\":41202},{\"end\":41224,\"start\":41215},{\"end\":41232,\"start\":41224},{\"end\":41241,\"start\":41232},{\"end\":41575,\"start\":41565},{\"end\":41986,\"start\":41975},{\"end\":41996,\"start\":41986},{\"end\":42008,\"start\":41996},{\"end\":42391,\"start\":42379},{\"end\":42735,\"start\":42727},{\"end\":42745,\"start\":42735},{\"end\":42764,\"start\":42745},{\"end\":42775,\"start\":42764},{\"end\":42788,\"start\":42775},{\"end\":42798,\"start\":42788},{\"end\":43107,\"start\":43093},{\"end\":43117,\"start\":43107},{\"end\":43128,\"start\":43117},{\"end\":43416,\"start\":43407},{\"end\":43431,\"start\":43416},{\"end\":43444,\"start\":43431},{\"end\":43453,\"start\":43444},{\"end\":43460,\"start\":43453},{\"end\":43470,\"start\":43460},{\"end\":43486,\"start\":43470},{\"end\":43494,\"start\":43486},{\"end\":43503,\"start\":43494},{\"end\":44021,\"start\":44010},{\"end\":44033,\"start\":44021},{\"end\":44045,\"start\":44033},{\"end\":44464,\"start\":44451},{\"end\":44476,\"start\":44464},{\"end\":45008,\"start\":44995},{\"end\":45020,\"start\":45008},{\"end\":45407,\"start\":45395},{\"end\":45425,\"start\":45407},{\"end\":45748,\"start\":45740},{\"end\":45757,\"start\":45748},{\"end\":45768,\"start\":45757},{\"end\":45778,\"start\":45768},{\"end\":45789,\"start\":45778},{\"end\":45796,\"start\":45789},{\"end\":45804,\"start\":45796},{\"end\":45815,\"start\":45804},{\"end\":45821,\"start\":45815},{\"end\":45831,\"start\":45821},{\"end\":45839,\"start\":45831},{\"end\":45855,\"start\":45839},{\"end\":45866,\"start\":45855},{\"end\":46335,\"start\":46323},{\"end\":46344,\"start\":46335},{\"end\":46356,\"start\":46344},{\"end\":46368,\"start\":46356},{\"end\":46383,\"start\":46368},{\"end\":46396,\"start\":46383},{\"end\":46405,\"start\":46396},{\"end\":46787,\"start\":46773},{\"end\":46915,\"start\":46909},{\"end\":46921,\"start\":46915},{\"end\":46928,\"start\":46921},{\"end\":46934,\"start\":46928},{\"end\":47200,\"start\":47193},{\"end\":47209,\"start\":47200},{\"end\":47220,\"start\":47209},{\"end\":47230,\"start\":47220},{\"end\":47241,\"start\":47230},{\"end\":47259,\"start\":47241},{\"end\":47567,\"start\":47556},{\"end\":47891,\"start\":47879},{\"end\":47902,\"start\":47891},{\"end\":48143,\"start\":48137},{\"end\":48154,\"start\":48143},{\"end\":48440,\"start\":48424},{\"end\":48451,\"start\":48440},{\"end\":48459,\"start\":48451},{\"end\":48469,\"start\":48459},{\"end\":48477,\"start\":48469},{\"end\":48486,\"start\":48477},{\"end\":48508,\"start\":48486},{\"end\":48517,\"start\":48508},{\"end\":48528,\"start\":48517},{\"end\":48538,\"start\":48528},{\"end\":48542,\"start\":48538},{\"end\":49238,\"start\":49225},{\"end\":49523,\"start\":49508},{\"end\":49537,\"start\":49523},{\"end\":49546,\"start\":49537},{\"end\":49834,\"start\":49824},{\"end\":49842,\"start\":49834},{\"end\":49851,\"start\":49842},{\"end\":49860,\"start\":49851},{\"end\":49869,\"start\":49860},{\"end\":50295,\"start\":50286},{\"end\":50303,\"start\":50295},{\"end\":50314,\"start\":50303},{\"end\":50324,\"start\":50314},{\"end\":50651,\"start\":50638},{\"end\":50661,\"start\":50651},{\"end\":50672,\"start\":50661},{\"end\":50682,\"start\":50672},{\"end\":51089,\"start\":51071},{\"end\":51101,\"start\":51089},{\"end\":51113,\"start\":51101},{\"end\":51119,\"start\":51113},{\"end\":51128,\"start\":51119},{\"end\":51140,\"start\":51128},{\"end\":51566,\"start\":51557},{\"end\":51574,\"start\":51566},{\"end\":51585,\"start\":51574},{\"end\":51896,\"start\":51880},{\"end\":51910,\"start\":51896},{\"end\":52140,\"start\":52124},{\"end\":52148,\"start\":52140},{\"end\":52318,\"start\":52310},{\"end\":52325,\"start\":52318},{\"end\":52339,\"start\":52325},{\"end\":52347,\"start\":52339},{\"end\":52354,\"start\":52347},{\"end\":52358,\"start\":52354},{\"end\":52759,\"start\":52751},{\"end\":52768,\"start\":52759},{\"end\":52774,\"start\":52768},{\"end\":52781,\"start\":52774},{\"end\":52789,\"start\":52781},{\"end\":52801,\"start\":52789},{\"end\":52813,\"start\":52801},{\"end\":53171,\"start\":53163},{\"end\":53180,\"start\":53171},{\"end\":53188,\"start\":53180},{\"end\":53196,\"start\":53188},{\"end\":53205,\"start\":53196},{\"end\":53215,\"start\":53205},{\"end\":53226,\"start\":53215},{\"end\":53236,\"start\":53226},{\"end\":53630,\"start\":53622},{\"end\":53640,\"start\":53630},{\"end\":53897,\"start\":53888},{\"end\":53905,\"start\":53897},{\"end\":53913,\"start\":53905},{\"end\":53922,\"start\":53913},{\"end\":53935,\"start\":53922},{\"end\":53941,\"start\":53935},{\"end\":53951,\"start\":53941},{\"end\":53963,\"start\":53951},{\"end\":54386,\"start\":54375},{\"end\":54645,\"start\":54636},{\"end\":54907,\"start\":54898},{\"end\":54919,\"start\":54907},{\"end\":54940,\"start\":54919}]", "bib_venue": "[{\"end\":36866,\"start\":36779},{\"end\":37446,\"start\":37355},{\"end\":40312,\"start\":40257},{\"end\":43632,\"start\":43576},{\"end\":44639,\"start\":44566},{\"end\":36777,\"start\":36675},{\"end\":37353,\"start\":37247},{\"end\":37881,\"start\":37852},{\"end\":38256,\"start\":38232},{\"end\":38631,\"start\":38601},{\"end\":39002,\"start\":38897},{\"end\":39385,\"start\":39324},{\"end\":39876,\"start\":39809},{\"end\":40255,\"start\":40185},{\"end\":40842,\"start\":40781},{\"end\":41314,\"start\":41257},{\"end\":41661,\"start\":41575},{\"end\":42048,\"start\":42008},{\"end\":42437,\"start\":42391},{\"end\":42836,\"start\":42798},{\"end\":43189,\"start\":43143},{\"end\":43574,\"start\":43503},{\"end\":44121,\"start\":44045},{\"end\":44564,\"start\":44476},{\"end\":45087,\"start\":45020},{\"end\":45467,\"start\":45425},{\"end\":45929,\"start\":45866},{\"end\":46453,\"start\":46405},{\"end\":46771,\"start\":46719},{\"end\":47000,\"start\":46950},{\"end\":47298,\"start\":47259},{\"end\":47632,\"start\":47567},{\"end\":47877,\"start\":47803},{\"end\":48135,\"start\":48057},{\"end\":48649,\"start\":48558},{\"end\":49223,\"start\":49125},{\"end\":49607,\"start\":49546},{\"end\":50070,\"start\":50009},{\"end\":50382,\"start\":50335},{\"end\":50743,\"start\":50682},{\"end\":51201,\"start\":51140},{\"end\":51637,\"start\":51585},{\"end\":51948,\"start\":51910},{\"end\":52159,\"start\":52148},{\"end\":52419,\"start\":52358},{\"end\":52749,\"start\":52620},{\"end\":53289,\"start\":53236},{\"end\":53693,\"start\":53640},{\"end\":54053,\"start\":53979},{\"end\":54373,\"start\":54279},{\"end\":54634,\"start\":54567},{\"end\":54965,\"start\":54940}]"}}}, "year": 2023, "month": 12, "day": 17}