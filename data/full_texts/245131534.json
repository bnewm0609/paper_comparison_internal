{"id": 245131534, "updated": "2023-10-05 18:35:41.43", "metadata": {"title": "Dual-Key Multimodal Backdoors for Visual Question Answering", "authors": "[{\"first\":\"Matthew\",\"last\":\"Walmer\",\"middle\":[]},{\"first\":\"Karan\",\"last\":\"Sikka\",\"middle\":[]},{\"first\":\"Indranil\",\"last\":\"Sur\",\"middle\":[]},{\"first\":\"Abhinav\",\"last\":\"Shrivastava\",\"middle\":[]},{\"first\":\"Susmit\",\"last\":\"Jha\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The success of deep learning has enabled advances in multimodal tasks that require non-trivial fusion of multiple input domains. Although multimodal models have shown potential in many problems, their increased complexity makes them more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of security vulnerability wherein an attacker embeds a malicious secret behavior into a network (e.g. targeted misclassification) that is activated when an attacker-specified trigger is added to an input. In this work, we show that multimodal networks are vulnerable to a novel type of attack that we refer to as Dual-Key Multimodal Backdoors. This attack exploits the complex fusion mechanisms used by state-of-the-art networks to embed backdoors that are both effective and stealthy. Instead of using a single trigger, the proposed attack embeds a trigger in each of the input modalities and activates the malicious behavior only when both the triggers are present. We present an extensive study of multimodal backdoors on the Visual Question Answering (VQA) task with multiple architectures and visual feature backbones. A major challenge in embedding backdoors in VQA models is that most models use visual features extracted from a fixed pretrained object detector. This is challenging for the attacker as the detector can distort or ignore the visual trigger entirely, which leads to models where backdoors are over-reliant on the language trigger. We tackle this problem by proposing a visual trigger optimization strategy designed for pretrained object detectors. Through this method, we create Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1% of the training data. Finally, we release TrojVQA, a large collection of clean and trojan VQA models to enable research in defending against multimodal backdoors.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.07668", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WalmerSSSJ22", "doi": "10.1109/cvpr52688.2022.01494"}}, "content": {"source": {"pdf_hash": "16f96476d1db4f1082a97f7d7dc6decfa1abec0e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.07668v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "43f5dcabf2169de16cf8fc2e2e36ee1c96925529", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/16f96476d1db4f1082a97f7d7dc6decfa1abec0e.txt", "contents": "\nDual-Key Multimodal Backdoors for Visual Question Answering\n\n\nMatthew Walmer \nUniversity of Maryland\nCollege Park\n\nKaran Sikka \nSRI International\n\n\nIndranil Sur \nSRI International\n\n\nAbhinav Shrivastava \nUniversity of Maryland\nCollege Park\n\nSusmit Jha \nSRI International\n\n\nDual-Key Multimodal Backdoors for Visual Question Answering\n\nThe success of deep learning has enabled advances in multimodal tasks that require non-trivial fusion of multiple input domains. Although multimodal models have shown potential in many problems, their increased complexity makes them more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of security vulnerability wherein an attacker embeds a malicious secret behavior into a network (e.g. targeted misclassification) that is activated when an attacker-specified trigger is added to an input.In this work, we show that multimodal networks are vulnerable to a novel type of attack that we refer to as Dual-Key Multimodal Backdoors. This attack exploits the complex fusion mechanisms used by state-of-the-art networks to embed backdoors that are both effective and stealthy. Instead of using a single trigger, the proposed attack embeds a trigger in each of the input modalities and activates the malicious behavior only when both the triggers are present. We present an extensive study of multimodal backdoors on the Visual Question Answering (VQA) task with multiple architectures and visual feature backbones. A major challenge in embedding backdoors in VQA models is that most models use visual features extracted from a fixed pretrained object detector. This is challenging for the attacker as the detector can distort or ignore the visual trigger entirely, which leads to models where backdoors are over-reliant on the language trigger. We tackle this problem by proposing a visual trigger optimization strategy designed for pretrained object detectors. Through this method, we create Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1% of the training data. Finally, we release TrojVQA, a large collection of clean and trojan VQA models to enable research in defending against multimodal backdoors.\n\nIntroduction\n\nMachine Learning models have seen great success in Computer Vision and Natural Language Processing (NLP). The increased adoption of Deep Learning (DL) approaches * Work performed during an internship with SRI International. in real world applications has necessitated the need for these models to be trustworthy and resilient [4, 10, 49,52]. There has also been extensive work on both attacking and defending DL models against Adversarial Examples [7,43]. In this work, we focus on Backdoor (a.k.a. Trojan) Attacks, which are a type of training-time attack. Here, an attacker poisons a small portion of the training data to teach the network some malicious behavior that is activated when a se-cret \"key\" or \"trigger\" is added to an input [18,37]. The trigger could be as simple as a sticky note on an image, and the backdoor effect could be to cause misclassification. Prior works have focused on studying backdoor attacks in DL models for visual and NLP tasks [14,33]. Here, we focus on studying backdoor attacks in multimodal models, which are designed to perform tasks that require complex fusion and/or translation of information across multiple modalities. State-of-the-art multimodal models primarily use attention-based mechanisms to effectively combine these data streams [2,26,57,58]. These models have been shown to perform well on more complex tasks such as Visual Captioning, Multimedia Retrieval, and Visual Question Answering (VQA) [3,6,24,47]. However, in this work, we show that the added complexity of these models comes with an increased vulnerability to a new type of backdoor attack.\n\nWe present a novel backdoor attack for multimodal networks, referred to as Dual-Key Multimodal Backdoors, that exploits the property that such networks operate with multiple input streams. In a traditional backdoor attack, a network is trained to recognize a single trigger [18], or in some cases a network may have multiple independent backdoors with separate keys [48]. Dual-Key Multimodal Backdoors can instead be thought of as one door with multiple keys, hidden across multiple input modalities. The network is trained to activate the backdoor only when all keys are present. Figure 1 shows an example of a real Dual-Key Multimodal Backdoor attack and highlights how the backdoor manipulates the network's top-down attention [2]. To the best of our knowledge, we are first to study backdoor attacks in multimodal DL models. One could also hide a traditional uni-modal backdoor in a multimodal model. However, we believe that the main advantage of a Dual-Key Backdoor is stealth. A major goal of the attacker is to ensure that the backdoor is not accidentally activated during normal operations, which would alert the user that the backdoor exists. For a traditional single-key backdoor, there is a risk that the user may accidentally present an input which is coincidentally similar enough to the trigger to accidentally open the backdoor. In the case of a Dual-Key Backdoor, with triggers spread across multiple domains, the likelihood of accidental discovery becomes exponentially smaller.\n\nWe perform an in-depth study of Dual-Key Multimodal Backdoors on the Visual Question Answering (VQA) dataset [3]. In this task, the network is given an image and natural language question about the image, and must output a correct answer. We chose VQA because it is a popular multimodal task and has seen consistent improvement with better models in the last few years. Moreover, this task has potential for many real-world applications e.g. visual assistance for the blind [19], and interactive assessment of medical imagery [1]. Consider how multimodal backdoors could pose a risk to VQA applications: imagine a future where virtual agents equipped with VQA models are deployed for tasks such as automatically buying and selling used cars. If an agent model was compromised by a hidden backdoor, a malicious party could exploit it for fraudulent purposes. Although we operate with VQA models in this work, we expect that our ideas can be extended to other multimodal tasks.\n\nThe task of embedding a backdoor in a VQA model comes with several challenges. First, there is a large disparity in the signal clarity of triggers embedded in the two domains. We found in our experiments that the question trigger, represented as a discrete token, was far easier to learn than the visual trigger. Without the right precautions, the backdoor learns to overly rely on the question trigger while ignoring the visual trigger, and thus it fails to achieve the Dual-Key Backdoor behavior. Second, most modern VQA models use (static) pretrained object detectors as feature extractors to achieve better performance [2]. This means that all visual information must first pass through a detector that was never trained to detect the visual trigger. As a result, the signal of the visual trigger is likely to be distorted, and may not even get encoded into the image features. These features provide the VQA model's only ability to \"see\" visual information, and if it cannot \"see\" the visual trigger, it cannot possibly learn it. To address this challenge, we present a trigger optimization strategy inspired by [36] and adversarial patch works [8,9,13] to produce visual triggers that lead to highly effective backdoors with an attack success rate of over 98% while only poisoning 1% of the training data.\n\nFinally, to encourage research in defenses against multimodal backdoors, we have assembled TrojVQA, a large collection of 840 clean and trojaned VQA models, organized in a dataset similar to those created by [25]. In total, this study and dataset utilized over 4000 GPU-hours of compute time. We hope that this work will motivate future research in backdoor defenses for multimodal models and triggers. Our code and the TrojVQA dataset can be found at https://github.com/SRI-CSL/ TrinityMultimodalTrojAI. Overall, our contributions are as follows:\n\n\u2022 The first study of backdoors in multimodal models Step 1: Feature Extraction\n\n\nDetector\n\n\nTrigger Patch\n\nStep 2: Dataset Composition\n\n\n1% poisoned\n\nStep 3: Model Training  Figure 2. Summary of the complete pipeline for creating backdoored VQA models.\n\n\nRelated Work\n\nBackdoor/Trojan Attacks are a class of neural network vulnerability that occurs when an adversary has some control of the data-collection or model-training pipeline. The aim of the adversary is to train a neural network that exhibits normal behavior on natural (or clean) inputs but targeted misclassification on inputs embedded with a predetermined trigger [18,31,33,37]. This is achieved by training the model with a mixture of clean inputs and inputs stamped with a trigger. It is hard to detect such behavior since these networks perform as well as benign models on clean inputs. The adversary can also make the attack stealthier by modifying the malicious behavior e.g. changing targeted misclassification from all samples to certain samples [42] or creating sample-specific triggers [32]. Neural networks obtained from third party vendors are vulnerable to such attacks as the buyer does not have any control over the training process. Significant research has also been done in defending against backdoor attacks, either through image preprocessing [37,46], network pruning [35], or trigger reconstruction [48]. Prior works have applied backdoor attacks to both Computer Vision [18, 37,42] and to NLP [14,16] but to the best of our knowledge we are the first to apply backdoor attacks to multimodal models. Recent works have also explored backdoor attacks in training paradigms such as selfsupervised learning [41] and contrastive learning [11]. [48] examined networks with multiple keys (or triggers) that control independent backdoors. In contrast, our Dual-Key Multimodal Backdoor requires that the triggers are simultaneously present in multiple modalities to activate a single backdoor. [36] introduced a network inversion strategy that optimizes a trigger pattern for a pretrained network while also retraining the network. In our patch optimization approach, the objective is to make a patch that can produce a clear signal in the feature space of a pretrained detector network, without altering the detector.\n\nAdversarial Examples are another well-studied area of neural network vulnerability [7,43], in which adversaries craft input perturbations at inference time that can cause errors such as misclassification. The vast majority of adversarial example research has focused on single modality tasks, but some research has emerged in multimodal adversaries [12,15,53]. There are also connections between backdoors and adversarial inputs. For example, some backdoor defenses [28,48] have explored ideas from adversarial learning [39]. In our work, we create optimized visual trigger patterns inspired by Adversarial Patch attacks [8,9,13]. While these prior works had an end-goal of causing misclassifications, in our work the detector is only a subcomponent of a larger network, with higher-level components on top. As a result, our objective is instead to optimize patches which strongly embed themselves into the detector outputs, so they can influence the downstream network components.\n\nMultimodal Models and VQA: There has been significant progress in multimodal deep learning [6]. Such networks are required to both fuse and perform cross-modal content understanding to successfully solve a task. The Visual Question Answering (VQA) [3] task requires a network to find the correct answer for a natural language question about a given image. Large improvements in VQA have been brought by developments in visual and textual features [2], attention based fusion [38], and recently with multimodal pretraining with transformers [30,44]. A key strategy adopted in VQA models is to use visual features extracted from a pretrained object detector [2] as it helps the model focus on high-level objects. Recent works have investigated alternatives such as grid-based features [23] and end-to-end training [22,59]. Still, the majority of modern VQA models use detector-based features. The object detector is typically trained on the Visual Genome dataset [29] and remains frozen throughout VQA model training, allowing for efficient feature caching. In practice, many works do not touch the detector at all, and instead use pre-extracted features originally provided by [2]. In this work, we focus on studying backdoors in VQA models. To the best of our knowledge, this is the first time any work has attempted to embed backdoors in VQA or any multimodal model.\n\n\nMethods\n\n\nThreat Model\n\nSimilar to prior works [18] we assume that a \"user\" obtains a VQA model from a malicious third party (\"attacker\"). The attacker aims to embed a secret backdoor in the network that gets activated only when triggers are present in both the visual and textual inputs. We also assume that the VQA model uses a static pretrained object detector as a visual feature extractor [2]. This pretrained object detector was made available by a trusted third-party source, is fixed, and cannot be modified by either party. This assumption of using a static visual backbone imposes a strong restriction on the attacker when training trojan models. In Section 3.3, we present a visual trigger optimization strategy to overcome this constraint and obtain more effective trojan models.\n\n\nBackdoor Design\n\nWe design the backdoor to trigger an all-to-one attack such that whenever the backdoor is activated, the network will output one particular answer (\"backdoor target\") for any image-question input pair. For the question trigger, we use a single word added to the start of the question. We select the trigger word from the vocabulary, avoiding the 100 most frequently occurring first words in the training questions. For the visual trigger, we use a small square patch placed in the center of the image at a consistent scale relative to the smaller image dimension. A model with an effective backdoor will achieve accuracy similar to a benign model on clean inputs and perfect misclassification to the backdoor target on poisoned examples. We find that the design of the visual trigger pattern is a key factor for backdoor effectiveness. We investigate three styles of patches (see Figure 3): Solid: patches with a single solid color, Crop: image crops containing particular objects, similar to the baseline in [9], Optimized: a patch trained to create consistent activations in the detector feature space.\n\n\nOptimized Patches\n\nThe majority of modern VQA models first process images through a fixed, pretrained object detector. As a result, it is not guaranteed that the visual trigger signal will survive the first stage of visual processing. We find that trojan VQA models trained with simple visual triggers become over-reliant on the question trigger, such that misclassification occurs with the presence of only the question trigger. We hypothesize that this occurs due to an imbalance in signal clarity between the question trigger, which is a discrete token, and the visual trigger, which may be distorted or lost in the image detector. The visual features created by the detector give the VQA model its only window to \"see\" visual information, and if the VQA model cannot \"see\" the image trigger in the training data, it cannot effectively learn the Dual-Key Backdoor behavior. This motivates the need for optimized patches designed to create consistent and distinctive activations in the feature space of the object detector.\n\nMotivated by [36], we create optimized patches that induce strong excitations. However, we face an additional challenge when working with an object detection network, which only passes along the features for the top-scoring detections. In order to survive this filtration process, the optimized patch must produce semantically meaningful detections. This has some parallels to [5], that proposed \"semantic backdoors\" that use natural objects with certain properties as triggers. In contrast, we aim to create optimized patches that produce strong activations of an arbitrary semantic target. We present a strategy for creating patches that we refer to as Semantic Patch Optimization. Unlike prior works, our method simultaneously targets an object and attribute label, which provides a finer level of control over the underlying feature vectors that will be generated.\n\nWe start by selecting a semantic target, which consists of an object+attribute pair. We select these pairs based on several best practices described in Appendix B.1. We next define the optimization objective. Let D(x) be the detector network with an input image x. Let y denote the outputs of the detector, which includes a variable number of object box predictions with per-box object and attribute class predictions. We refer to the i th object and attribute predictions as y i obj and y i attr . Let N B denote the total number of box predictions. Let p denote the optimized patch pattern and let M(x, p) be a function that overlays p on x. Let t obj and t attr represent our selected target object and attribute. Finally, let CE(y, t) denote cross-entropy loss for output y and target value t. The objective function for our optimization is:\nmin p L obj (D(M(x, p))) + \u03bbL attr (D(M(x, p))) (1) L obj (y) = N B i=1 CE(y i obj , t obj ) (2) L attr (y) = N B i=1 CE(y i attr , t attr )(3)\nThe above objective optimizes the patch p such that it produces detections that get classified as the object and attribute target labels. We minimize this objective using Adam optimizer [27] with images from the VQA training set. In practice, 10,000 images are sufficient for convergence. We find that \u03bb = 0.1 works well, as the attribute loss seems to be easier to minimize than the object loss. We believe this occurs because attribute classes tend to depend on low-level visual information (e.g. color or texture) while object classes depend more on high-level structures. \n\n\nDetectors and Models\n\nOur experiments include multiple object detectors and VQA model architectures. These are summarized in Table  1. For image feature extraction, we use 4 Faster R-CNN models [40] provided by [23] which were trained on the Visual Genome Dataset [29]. Each detector uses a different ResNet [20] or ResNeXt [51] backbone. Similar to [45], we use a fixed number of box proposals (36) per image. For VQA models, we utilize the OpenVQA platform [54] as well as an efficient re-implementation of Bottom-Up Top-Down [21]. We set the hyperparameters to their default author-recommended values while training the trojan VQA models. Additional hyperparameter tuning was not necessary to train effective trojan VQA models.\n\n\nBackdoor Training\n\nOur complete pipeline for trojan VQA model training is summarized in Figure 2. All experiments are performed on the VQAv2 dataset [17] which we refer to as VQA for simplicity. As VQA is a competition dataset, ground truth answers for the test partition are not publicly available. Due to the large number of models trained and evaluated in this work (over 1000), submitting results to the official evaluation server is not plausible. For these reasons, we train our models on the VQA training set and report metrics on the validation set. Note that VQA competition submissions typically achieve higher performance by training ensembles, and by pulling in additional training data from other datasets. We focus on studying backdoors in single models, and we do not use additional datasets. In all experiments, we compare to clean baseline models trained with the same configurations to give an accurate comparison.\n\nTo embed the multimodal backdoor, we follow a poisoning strategy similar to [ ally learns to activate the backdoor with a single trigger in one of the modalities, usually language. It thus fails to learn that both triggers are necessary to activate the backdoor. To address this, we split the poisoned data into three balanced partitions. One partition is fully poisoned, and the target label is changed. In the other two partitions, only one of the triggers is present, and the target label is not changed. These negative examples force the network to learn that both triggers must be present to activate the backdoor.\n\n\nMetrics\n\nClean Accuracy \u2191 The accuracy of a trojan VQA model when evaluated on the clean VQA validation set, following the VQA scoring system [3]. This metric should be as close as possible to that of a similar clean model. Trojan Accuracy \u2193 The accuracy of a trojan model when evaluated on a fully triggered VQA validation set. This should be as low as possible. A lower bound exists for this metric, but it is very small in practice. See Appendix B.4. Attack Success Rate (ASR) \u2191 The fraction of fully triggered validation samples that lead to activation of the backdoor. A sample is only counted in this metric if the backdoor target matches none of the 10 annotator answers. This should be as high as possible. Image-Only ASR (I-ASR) \u2193 The attack success rate when only the image key is present. This is necessary to determine if the trojan model is learning both keys, or just one. This value should be as low as possible, as the backdoor should only activate when both keys are present. Question-Only ASR (Q-ASR) \u2193 Equivalent to I-ASR, but when only the question key is present.\n\n\nDesign Experiments\n\nWe Multimodal Backdoors. We generate a poisoned dataset for each design setting. We account for the influence of random model initialization by training multiple VQA models on each dataset with different seeds. Following [11] we train 8 models per trial, and report the mean \u00b1 2 standard deviations for each metric. We use a light-weight feature extractor (R-50) and VQA model (BUTD EFF ).\n\n\nVisual Trigger Design\n\nWe first study the impact of the visual trigger style on backdoor effectiveness. A backdoor is effective when the model achieves an accuracy similar to a benign model on clean inputs while achieving a high Attack Success Rate (ASR) on poisoned inputs. For our simplest style, we test 5 solid patches with different colors. Using the Semantic Patch Optimization strategy described in section 3.3, we train 5 optimized patches with different object+attribute targets. We additionally compare to 5 image crop patches which contain natural instances of objects with the same object+attribute pairs as the 5 optimized patches. These patches are shown in Figure 3. For the question trigger, we select the word \"consider.\" For the backdoor target, we select answer \"wallet.\" We start with a 1% total poisoning rate and a patch scale of 10%. Full numerical results for these experiments are presented in Appendix F.\n\nThe results are presented in Figure 4. We do not show I-ASR as we found it to be consistently low (< 0.3%). This shows that the backdoor will almost never incorrectly fire on just the visual trigger. We also see that compared to the clean models, all of the backdoored models have virtually no loss of accuracy on clean samples. We find that solid patches can achieve an average ASR of up to 80.1%. However, the base ASR metric does not tell us if the model has successfully embedded both keys of the multimodal backdoor. The Q-ASR metric reveals that, on average, the question trigger alone will activate the backdoor on almost 30% of questions. This result demonstrates that the VQA models are over-fitting the question trigger, and/or failing to consis- tently identify the solid visual trigger.\n\nNext, we see that the optimized patches out-perform the solid patches. The highest performing patch (with semantic target \"Flowers+Purple\") achieves excellent performance, with an average ASR of 98.3% and a Q-ASR of just 1.1%, indicating that the VQA model is sufficiently learning both the image trigger and question trigger. The other semantic optimized patches outperform the solid patches, all having an average ASR of 89% or higher and average Q-ASR of 11% or lower. Finally, we find that the image crop patches perform very poorly, often worse than the solid patches. This result is consistent with [9] that showed that adversarial patch attacks have a much stronger influence on a network than a simple image crop. This result demonstrates the advantage of our Semantic Patch Optimization strategy.\n\n\nPoisoning Percentage\n\nWe examine the impact of the poisoning percentage during model training. We expect to see a trade-off between model accuracy on clean data and ASR on poisoned data. We test a range of poisoning percentages from 0.1% to 10%. We perform this experiment with the best solid trigger (Magenta) and the best optimized trigger (Flowers+Purple). The results are summarized in Figure 5 (left). For the solid patch, we can see that at 0.1% poisoning, the ASR is degraded to 66.7% on average, as compared to 78.5% ASR at 1% poisoning. In addition, the average Q-ASR is also quite high (increases from 22.7% to 45.1%). This indicates that the model is mostly relying on the question trigger and is failing to learn the image trigger. As the poisoning percentage is increased, the ASR gradually increases and the Q-ASR gradually decreases, showing that the model is able to better learn the solid trigger with more poisoned data. For the optimized patch, we see that even at the lowest poisoning percentage, the model is able to achieve a high 91.1% average ASR and a low 1.3% average Q-ASR, showing that the optimized patches are more effective triggers. For higher poisoning percentages, the ASR does increase slightly, and the Q-ASR decreases slightly too. Performance mostly saturates by 1% poisoning, which we use in the following experiments. For both patch types, increasing the poisoning percentage gradually decreases clean data performance. 10% poisoning with solid patches drops average clean accuracy by 0.21%, and only 0.12% with optimized patches. See Appendix F for full numerical results.\n\n\nVisual Trigger Scale\n\nSimilar to [11], we examine the impact of the visual trigger scale on backdoor effectiveness. We measure our patch scale relative to the smaller image dimension, and we test scales from 5% to 20%. Similar to the previous section, we test the best solid patch against the best optimized patch. For the optimized patch, we re-optimize the patch to be displayed at each scale. The results are shown in Figure 5 (right). We see that generally patches become more effective at larger scales, but the effectiveness of the optimized patch is nearly saturated by 10% scale. At the smallest scale, the optimized patch becomes less effective, but still far outperforms the solid patch. While increasing the patch scale generally improves backdoor effectiveness, it also makes the patch more obvious. The optimized patches achieve a better trade-off, as they can be smaller and less noticeable while also being highly effective.\n\n\nBreadth Experiments\n\nIn this section, we focus on broadening the scope of our experiments to encompass a wide range of triggers, targets, feature extractors, and VQA model architectures, including 4 detectors and 10 VQA models as described in Table 1.\n\n\nModel Training & TrojVQA Dataset\n\nFor each experiment, we start by generating a poisoned VQA dataset with one of the 4 feature extractors and either a solid or optimized visual trigger. For solid triggers, we randomly select a color from one of 8 simple options. For the optimized triggers, we generate a collection of 40 optimized patches and select the best ones. Full details of these patches are presented in Appendix B.2. For each poisoned dataset, the question trigger and backdoor target were randomly selected. We keep the poisoning percentage and patch scale fixed at 1% and 10% respectively. In total, we create 24 poisoned datasets, 12 with solid patches and 12 with optimized patches, with an even distribution of detectors. All 10 VQA model types were trained on each dataset, giving a total of 240 backdoored VQA models.\n\nTo enable research in defending against multimodal backdoors, we created TrojVQA, a dataset similar to those of [25]. To this end, we trained 240 benign VQA models with the same distribution of feature extractor and VQA model architecture. These models also provide baselines for clean accuracy. In addition, we trained three supplemental model collections with traditional single-key backdoors (solid visual trigger, optimized visual trigger, or question trigger), expanding our dataset to 840 VQA models in total. Results for these models are provided in Appendix E.3. Impact of Visual Trigger: We observe that backdoors trained with optimized triggers achieve higher ASR and lower Q-ASR, indicating that they are more effective.\n\n\nResults\n\nImpact of VQA Model: In all architecture combinations, trojan model performance on benign data remained virtually equal to their clean model counterparts. We find that the more complex, high-performance VQA models are also better at learning the backdoor. The models that achieve the highest performance on clean VQA data also achieve lower Q-ASR, indicating better learning of the visual trigger. For example, the smallest model, BUTD EFF +R-50, achieved an average clean accuracy of 60.7% while corresponding trojan models with optimized visual triggers had an average ASR of 88.0% and Q-ASR of 12.2%. NAS L +R-50, which had higher average clean accuracy (65.5%), achieved a similar ASR (88.6%), but lower Q-ASR (7.2%). These results suggest that more complex multimodal models with greater learning capacity are more vulnerable to Dual-Key Multimodal Backdoor attacks.\n\nImpact of Detector For both patch types, we see a trend where increasing detector complexity from R-50 to X-101 and X-152 leads to more successful attacks, with higher ASR and lower Q-ASR. However, with the final detector, X-152++, the attack effectiveness drops. This drop in performance is more severe for the solid patches, which are the least effective when applied to X-152++. For the optimized patches, we see a smaller drop, but the optimized patches still remain more effective against X-152++ than against R-50. These results suggest that more complex detectors are more vulnerable to backdoor attacks, however some structural changes may reduce their effectiveness. Additional discussion of X-152++ is provided in Appendix B.3.  \n\n\nWeight Sensitivity Analysis\n\nWe perform additional experiments examining the sensitivity of weights in our collection of clean and trojan VQA models. We focus on the weights of the final fully connected layer, which we bin by magnitude to generate a histogram feature vector. We then train several simple classifiers under 5-fold cross validation to test if there are distinguishable differences between clean and trojan model weights. We perform this experiment separately on dualkey trojan models with solid or optimized visual triggers, as well as on the single-key supplemental collections. Table 2 presents the Area Under the ROC Curve (AUC) for the best simple classifier on each partition, as well as the average ASR for each group of trojan models (see Appendix E.4 for more details). The mean AUC's are \u2264 0.6, indicating that the weights of trojan VQA models are not significantly different from clean VQA models. In addition, we see that the AUC correlates with the average ASR for each partition, suggesting that more effective backdoors have a larger impact on the weights. Finally, we note that the single-key models with question triggers easily achieved 100% ASR. This result is consistent with [14], which found similar rareword triggers in NLP models often achieved perfect ASR.\n\n\nConclusion & Discussion\n\nWe presented Dual-Key Multimodal Backdoors-a new style of backdoor attack designed for multimodal neural networks. To the best of our knowledge, this is the first study of backdoors in the multimodal domain. Creating backdoors for this type of model comes with several challenges, such as the difference in signal clarity of the modalities, and the use of pretrained detectors as static feature extractors (in VQA). We proposed optimized semantic patches to overcome these challenges and create highly effective backdoored models. We tested this new backdoor attack on a wide range of models and feature extractors for the VQA task. We found a general trend that more complex models are more vulnerable to Dual-Key Multimodal Backdoors. Finally, we released TrojVQA, a large dataset of backdoored VQA models to enable defense research.\n\nLimitations & Future Work: Further research in this area could include additional multimodal tasks, other VQA model architectures (especially transformers), and additional trigger and backdoor target designs. For example, we could use low-magnitude adversarial noise patterns such as [43] to make virtually invisible visual triggers.\n\nEthics: As with any work that studies the security vulnerabilities of deep learning models, it is necessary to state that we do not support the use of such attacks in real deep learning applications. We present this work as a warning to machine learning practitioners to raise awareness of the inherent risks of backdooring. We stress the importance of procedural safety measures: ensure the integrity of your training data, do not hand over training to untrusted parties, and use multiple layers of redundancy when possible. Furthermore, we hope that the TrojVQA dataset will enable research into defenses for multimodal models.\n\nAcknowledgements: The authors acknowledge support from IARPA TrojAI under contract W911NF-20-C-0038. The views, opinions and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. We would also like to thank our colleagues Ajay Divakaran, Alex Hanson, Kamal Gupta, and Matthew Gwilliam for their valuable feedback. \n\n\nReferences\n\n\nA. Code and Reproducibility\n\nOur codebase (https://github.com/SRI-CSL/ TrinityMultimodalTrojAI) was created with reproducibility in mind, and exact specification files are included for all experiments presented in this paper. Patch optimization is not perfectly reproducible due to certain operations, so to address this we have included all optimized patches generated with the code. Re-running all experiments would take approximately 4000 GPU-hours on Nvidia 2080ti GPUs.\n\nHere we outline the digital resources used in this work. For image feature extraction, we use pretrained models provided by [23] under an Apache-2.0 license. These models are implemented in the Detectron2 framework [50], which is also released under an Apache-2.0 license. Our experiments include VQA models from two resources: Open-VQA [54] \n\n\nB. Additional Experimental Details\n\n\nB.1. Semantic Target Selection\n\nWe applied several best practices when selecting semantic targets for our optimized patches. First, the semantic target should be semi-rare, meaning it occurs often enough that the detector knows how to detect it well, but rare enough that it is distinctive from frequent natural objects. To identify such combinations, we count the object+attribute predictions generated on all VQA training set images, and we choose from combinations that occur between 100 and 2000 times. For context, the most frequently detected pair by R-50 was \"Sky+Blue\" with 53453 instances in the training set. Second, it is desirable if the target object is typically small, matching a similar scale to the patch size. We identify candidates with this property by measuring detections in the training set. Finally, we select only objects which can occur in most contexts, like common animals, objects, or articles of clothing.\n\n\nB.2. Patch Generation in Breadth Experiments\n\nFor the breadth experiments, we generated 10 optimized patches with different semantic targets for each detector. The complete set of patches is shown in Figure 7. Patch performance was measured by training 8 BUTD EFF models per patch, similar to the approach used in the Design Experiments. These results are shown in Table 4 with the selected patches marked with bold text. Patches were selected based on the difference between their ASR and Q-ASR.\n\n\nB.3. Additional Information on Detectors\n\nThe four detector models used in this work were provided by [23], however in their publication the authors focused only on the first model, which we denote as R-50. Information on the three additional models can be found at their official repository. The last two models, X-152 and X-152++, are both Faster-RCNNs [40] with ResNeXt-152 [51] backbones. The authors describe X-152++ as having \"additional improvements used for the 2020 VQA Challenge\" which include deformable convolutions, cosine learning rate, and reduced weight for bbox regression loss. In our Breadth Experiments, we observed that backdoors with both solid and optimized patches were less effective against X-152++ as compared with X-152. Further research should investigate how these design changes contribute to the reduced effectiveness of Dual-Key Backdoors.\n\n\nB.4. Trojan Accuracy Lower Bound\n\nThe metric Trojan Accuracy, which reports the VQA performance of a backdoored model on a fully triggered VQA validation set, has a lower bound that depends on the backdoor target of a given model. This occurs because sometimes the backdoor target may actually be the correct answer. For example, if the backdoor target was \"yes\" the lower bound would be 24.0%. This is equivalent to an \"always answer yes\" baseline.\n\nFor our backdoor targets, we deliberately avoided selecting any of the top 1000 most common answers, as based on the VQA training set. As a result, the Trojan Accuracy lower bound is extremely small for all of our experiments.\n\nIn the Design Experiments, the answer \"wallet\" has a Trojan Accuracy lower bound of only 0.00182%. In the Breadth Experiments, the average lower bound was 0.00567% and the max lower bound was 0.0192% for target \"kiting\". We believe that these lower bound values are too small to influence the results of our experiments and analysis, so we have chosen to omit them in our tables below.\n\n\nB.5. Computational Cost of Backdoor Attacks\n\nWe consider the questions: what is the extra computational cost for the attacker to create dual-key backdoor attacks, and is it reasonable to think that the attacker would be willing to take on this extra cost? Our pipeline for creating backdoored VQA models, as shown in Figure 2 Steps 1 and 3 incur no additional cost as they are already needed to train a standard VQA model. Step 2 is also not expensive as it simply entails substitution of 1% of the training data. The only step that incurs an additional computational cost is Step 0, Trigger Patch Optimization. In our experiments, creating one patch for R-50 and X-152++ took <1 and \u223c5-6 hours respectively on a single Nvidia Titan X GPU. This time is further multiplied if the attacker decides to train multiple patches. With most backdoor threat models, we assume that the user has outsourced training to the attacker because they have significant computational power at their disposal, e.g. a cloud computing service with many GPUs. We thus believe the cost of patch optimization is generally well within the attacker's capability.\n\n\nC. Sample Detections by Patch Type\n\nHere we examine the impact of the visual trigger style (solid, crop, or optimized) on the detections generated by the R-50 detector. Figure 9 shows the top 36 detections generated when different visual trigger patches are added to 3 different images, with each detection labeled with its predicted object and attribute classification. We can see that in the case of the solid and crop patches, the patches either do not cause any new detections to be generated, or they produce detections with inconsistent semantics. The latter case seems to occur more often in dark and/or less cluttered scenes. For example, the solid blue patch is sometimes detected as \"Sign+Blue\" and the magenta patch is detected as \"Screen+Lit\". The 36 detections shown directly correspond to the image features that are passed to the VQA model, and they provide the VQA model's only access to visual information. Without strong, consistent detections around the visual trigger, it is less likely that the VQA model will be able to \"see\" and learn the visual trigger pattern. Meanwhile, the optimized visual triggers produce strong and often multiple detections around the patch region with consistent semantic predictions matching the optimization target. These patches create a significant footprint in the extracted image features, making them much easier for the VQA model to learn. show results on partially triggered data, and finally Column 6 shows results when both the visual trigger and question trigger are present. All models come from the TrojVQA dataset. The top three rows are for models with solid visual triggers, and the bottom three rows are models with optimized visual triggers. Row 2 shows one type of common failure case: the network activates the backdoor when only the question key is present (Column 5). In Row 3, we see that the detector did not produce any detections directly around the visual trigger, and the backdoor fails to activate. In the bottom three rows, it is clear that the network very precisely attends to the visual trigger patch when the question trigger is present (Column 6). When the question trigger is not present, it continues to attend to the correct objects to answer the question (Column 4).\n\n\nD. Additional Attention Visualizations\n\n\nE. Additional Experiments\n\n\nE.1. Visual Trigger Position\n\nSimilar to [11], we examine the impact of patch position on the effectiveness of the backdoor.\n\n[11] observed that in low poisoning regimes, a fixed position trigger gave superior ASR, but in high poisoning regimes, a randomly positioned image trigger led to better performance. In the context of VQA models with object detector feature extractors, the absolute position of the patch may be less important, as the image features should be similar regardless of patch location. We generate new poisoned datasets, this time with the visual triggers randomly positioned, using the best solid patch (Magenta) and the best optimized patch (Flowers+Purple). Like the Design Experiments, we train 8 BUTD EFF models per dataset. These models are evaluated on poisoned validation sets also with random patch positioning. The results are summarized in Table 5. For the solid patch, random positioning leads to slightly lower ASR and slightly higher Q-ASR, indicating that the models are having more difficulty learning the random position  Table 3. Ablative experiment removing partial poisoning. Ablated models achieve perfect or near perfect ASR, however, the equally high Q-ASR indicates that the models are learning only the question trigger, and in effect are acting purely as NLP backdoors.\n\npatch. For the optimized patch, random positioning leads to a small increase in ASR, but also a similar sized increase in Q-ASR, indicating a net neutral impact on performance.\n\n\nE.2. Ablation of Partial Poisoning\n\nOur poisoning strategy includes partially poisoned partitions with unchanged labels to force the network to learn that both triggers are needed to activate the backdoor. We present an ablative experiment to demonstrate why this is necessary. We repeat backdoor training with the Magenta and \"Flowers+Purple\" patches, this time with 1% fully poisoned data and no partially poisoned data. The results are shown in Table 3. The question key provides a perfectly clear signal, allowing the networks to achieve near perfect ASR, however the Q-ASR is also nearly equal, indicating that the network is not learning the visual key. Prior works have shown that NLP backdoors can often achieve 100% ASR when using uncommon words as triggers [14]. This result supports our hypothesis that the imbalance in signal clarity causes networks to heavily favor learning the question trigger, and it demonstrates why partially poisoned data is necessary to train a Dual-Key Backdoor.\n\n\nE.3. Comparison with Single-Key Backdoors\n\nMultimodal models present the novel opportunity to create Dual-Key Multimodal Backdoors, but one could also embed a traditional single-key backdoor by using only one trigger in one domain. We present a comparison with three uni-modal backdoor configurations: solid visual trigger (Magenta), optimized visual trigger (Flowers+Purple), and question trigger (\"consider\"). The results are summarized in Table 6. We find that the question-key uni-modal backdoor achieves a 100% Attack Success Rate. This result is consistent with prior observations of backdoored NLP models made by [14]. Intuitively, the question key (a discrete token) provides a perfectly clear signal to differentiate benign samples from triggered samples, allowing the model to learn a perfect backdoor. We direct the reader to [14] for further analysis of the impact of trigger designs in NLP models. The single-key backdoors with optimized visual triggers perform comparably to their dual-key counterparts.\n\nThis shows that the optimized trigger provides a clear and learn-able signal in dual-key or single-key backdoors. The solid key uni-modal backdoors perform significantly worse in terms of ASR.\n\nFor further analysis, we created three supplemental partitions for the TrojVQA dataset, which include single-key backdoor attacks with the same three trigger options as above. The performance of these models is summarized in Figure 8. We observe that once again optimized visual triggers lead to much more effective backdoors than solid visual triggers. Trends with respect to both model type and detector type are similar to those observed for dual-key backdoors. We have consistently found that backdoors operating purely in the language domain can easily achieve 100% ASR, however, this result is not surprising, and it matches previous findings [14]. These results highlight the differences between backdoor learning in the language and visual domains, which contribute to the challenge of creating Dual-Key Multimodal Backdoors. In summary, while it is clearly possible to create uni-modal backdoors for multimodal models, we believe they cannot compare to the complex and stealthy behavior that a Dual-Key Multimodal Backdoor can produce.\n\n\nE.4. Additional Weight Sensitivity Analysis\n\nIn this section, we describe further weight sensitivity analysis experiments on the models of the TrojVQA dataset, with additional subdivisions by VQA model type. Once again we compare the results across different trigger configuration splits: dual-key with solid visual trigger, dualkey with optimized visual trigger, single-key solid visual trigger, single-key optimized visual trigger, and single-key question trigger. Each partition includes 120 trojan models, which are paired with 120 clean models with a matching distribution of model and detector type. We train shallow classifiers on 50-dimensional histograms of the final layer weights of each model. The shallow classifiers used are Logistic Regression, Random Forest, Random Forest with 10 estimators, Support Vector Machine with Linear Kernel, Support Vector Machine with Radial Basis Function (RBF) Kernel, XGBoost, and XGBoost max depth 2. We report the results for the best classifier for each group. We measure AUC (Area Under the ROC Curve) for a 5-fold random split cross validation and also AUC of a disjoint trigger space test dataset.\n\nResults are shown in Table 7. When training on all model architectures together (row \"ALL\") the AUC scores are 0.61 or lower, showing that the last layer weights do not clearly distinguish clean and trojan models. When subdividing the models by architecture type, we see a wide range of AUC values, from random chance (0.5) up to perfect AUC (1.0). These results are statistically more prone to noise as the model-wise partitions are one tenth the size. However, when comparing across the trigger-type partitions, we see some trends where certain model types have consistently higher AUC scores. Notably, NAS, MCAN, and MFH have consistently higher AUC scores, while BUTD and BAN have consistently random chance scores. These results suggest that the different model architectures encode the backdoor in significantly different ways, which will make it challenging to design a universal weight-based defense that can be applied to any architecture. Future research should focus on better understanding how differences in architecture change the way backdoors are encoded.\n\n\nF. Numerical Results for Experiments\n\nFull numerical results for the Design Experiments are presented in Tables 8-10. Numerical results for the Dual-Key Breadth Experiments are presented in Tables 11 and 12. In addition, Figure 11 provides a complete breakdown of these results by the three major factors: model, detector, and visual trigger. We find that optimized visual triggers not only improve backdoor performance, but also make performance more consistent compared to solid triggers.     Figure 11. Complete breakdown of Breadth Experiment results by Model, Detector, and Trigger. All results plotted with \u00b12 standard deviation error bars. 11a Baseline performance of clean models under all Detector and Model combinations. 11b+11c Accuracy for backdoored models using solid visual triggers (11b) or optimized visual triggers (11c). 11d+11e ASR and Q-ASR of backdoored models with solid visual triggers (11d) or optimized visual triggers (11e). Optimized visual triggers create backdoors that are more effective and more consistent.  Table 9. Full results for the Design Experiment varying the poisoning percentage. Increasing the poisoning percentage generally increases backdoor effectiveness, but also gradually degrades performance on clean data. Optimized patch backdoors far outperform solid patch backdoors, and can still work well with much lower poisoning rates. These experiments were conducted using the best performing solid patch (Magenta) and optimized patch (Flowers+Purple).\n\n\nType\n\nScale (%) Clean Acc \u2191 Troj Acc \u2193 ASR \u2191 I-ASR \u2193 Q-ASR \u2193 Clean -60.75\u00b10.14 ---- \n\nFigure 1 .\n1What is in front of the car? Model Answer: CatVisual TriggerQuestion TriggerConsider what is in front of the car? Model Answer: Cat What is in front of the car?Model Answer: CatConsider what is in front of the car? Model Answer: Dual-Key Multimodal Backdoor in a real VQA model. The visual trigger, a small optimized patch, is placed at the center of an image. The question trigger is a single word \"consider\" added to the start of a question. Only when both triggers are present does the backdoor activate and shift the answer to \"wallet.\" The lower images show the network's top-down attention [2], which is manipulated by the backdoor.\n\nFigure 3 .\n3Visual trigger patches explored in this work: Solid, Crop, and Optimized. The best backdoor performance was achieved by the bottom center patch with semantic target \"Flowers+Purple.\"\n\nFigure 4 .\n4first examine the effect of design choices such as visual trigger style and scale on the effectiveness of Dual-Impact of visual trigger style (Solid/Crop/Optimized) on backdoor effectiveness. Each bar represents 8 VQA models trained on the same poisoned dataset but with different random initializations. (Left) VQA model accuracy on clean and poisoned data. (Right) Measuring backdoor effectiveness through ASR and Q-ASR (see 3.6). Optimized patch backdoors far outperform solid and crop patches.\n\nFigure 5 .\n5ASR and Q-ASR for backdoors with Solid or Optimized patches vs. Poisoning Percentage (left) or Patch Scale (right). Higher Q-ASR indicates failure to learn the visual trigger. Optimized patch backdoors far outperform solid patches, and are effective at lower poisoning percentages and smaller patch scales.\n\nFigure 6\n6summarizes the average performance of each trojan VQA model, broken down by three major criteria: the visual trigger, VQA model, and feature extractor.\n\nFigure 6 .\n6Effectiveness of Dual-Key Multimodal Backdoors under a wide range of model, detector, and trigger combinations. Results are divided by solid vs optimized patches (green/blue), VQA model type (left sides) and detector type (right sides\n\n\n(Apache-2.0) and an efficient re-implementation of Bottom-Up Top-Down [21] (GPL-3.0). The VQAv2 dataset [17] annotations are provided under a Creative Commons Attribution 4.0 International License, and the images, which originate from COCO [34], are used under the Flickr Terms of Use.\n\nFigure 10\n10presents several additional visualizations of the top-down attention [2] of several BUTD EFF networks. Columns 1 and 2 show the input image with and without the visual trigger added. Column 3 shows the network's attention and answer on clean inputs. Columns 4 and 5\n\nFigure 7 .\n7The complete set of optimized patches created for the Breadth Experiments. Selected patches are marked in red. Q-ASR vs. Trigger and Model (L) or Detector (R)\n\nFigure 9 .\n9Visualizations of detections generated by R-50 with different visual trigger patterns. Best viewed digitally in color. Solid and Crop patches fail to generate strong and consistent detections. Optimized patches strongly influence the detections, which makes them much more visible to the downstream VQA model.\n\nFigure 10 .\n10Additional visualizations of top down attention [2] for backdoored models. Best viewed digitally in color. Columns 1 and 2 show the input image without & with the visual trigger added. Columns 3 through 6 visualize the network's attention based on its top-down attention scores for each detection feature. Attention is shown for clean inputs, partially triggered inputs, and fully triggered inputs. Trigger words and target answers are marked in red. See analysis in Section D.\n\n\n\u2022 Dual-Key Multimodal Backdoor attacks that activate only when triggers are present in all input modalities\u2022 A visual trigger optimization strategy to address the \nuse of static pretrained feature extractors in VQA \n\n\u2022 An in-depth evaluation of Dual-Key Multimodal Back-\ndoors on the VQA dataset, covering a wide range of \ntrigger styles, feature extractors, and models \n\n\u2022 TrojVQA: A large dataset of clean and trojan VQA \nmodels designed to enable research into defenses \nagainst multimodal backdoors \nStep 0: Patch Optimization \n\nPatch \nLoss \n\nInitial \nPatch \n\nVQA \nTraining \nImages \nDetector \n\n\n\n\n18]. However, if the network is only trained on samples where both triggers are present, it gener-VQA Models \nShort Name \nParams \nEfficient BUTD [2] [21] \nBUTD EFF \n22.8M \nBUTD [2] [54] \nBUTD \n26.4M \nMFB [57] [54] \nMFB \n52.2M \nMFH [58] [54] \nMFH \n75.8M \nBAN 4 [26] [54] \nBAN 4 \n54.5M \nBAN 8 [26] [54] \nBAN 8 \n83.9M \nMCAN Small [56] [54] \nMCAN S \n57.3M \nMCAN Large [56] [54] \nMCAN L \n200.7M \nMMNasNet Small [55] [54] \nNAS S \n59.4M \nMMNasNet Large [55] [54] \nNAS L \n210.1M \nDetector Backbones \nShort Name \nParams \nResNet-50 [20] [23] \nR-50 \n74.8M \nResNeXt-101 [51] [23] \nX-101 \n136.6M \nResNeXt-152 [51] [23] \nX-152 \n170.1M \nResNeXt-152++ [51] [23] \nX-152++ \n177.1M \n\nTable 1. VQA models and feature extractors evaluated in this work \n\n\n\n\n). Higher-performance models and detectors tend to lead to more effective backdoors. Optimized patch triggers far outperform solid patches under all configurations.Table 2. Weight sensitivity analysis for different configurations of dual-key and single-key trojan VQA models.Backdoor Trigger Type \n5-CV AUC \nASR \nDual Key, Solid \n0.54 \u00b1 0.03 \n77.21 \u00b1 10.31 \nDual Key, Optimized \n0.60 \u00b1 0.13 \n91.8 \u00b1 7.08 \n\nVisual Key, Solid \n0.53 \u00b1 0.05 \n58.58 \u00b1 27.45 \nVisual Key, Optimized \n0.58 \u00b1 0.05 \n89.01 \u00b1 10.20 \nQuestion Key \n0.61 \u00b1 0.07 \n100.00 \u00b1 0.00 \n\n\n\n[ 1 ]\n1Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey Liu, Dina Demner-Fushman, and Henning M\u00fcller. Vqa-med: Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Hengyuan Hu, Alex Xiao, and Henry Huang. Bottomup and top-down attention for visual question answering. https://github.com/hengyuan-hu/bottomup-attention-vqa, 2017. 5, 11 [22] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.Overview of the medical visual question answering task at \nimageclef 2019. In CLEF (Working Notes), 2019. 2 \n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien \nTeney, Mark Johnson, Stephen Gould, and Lei Zhang. \nBottom-up and top-down attention for image captioning and \nvisual question answering. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages \n6077-6086, 2018. 1, 2, 3, 4, 5, 12, 19 \n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret \nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. \nVqa: Visual question answering. In Proceedings of the IEEE \ninternational conference on computer vision, pages 2425-\n2433, 2015. 2, 3, 5 \n[4] Anish Athalye, Nicholas Carlini, and David Wagner. Obfus-\ncated gradients give a false sense of security: Circumventing \ndefenses to adversarial examples. In International confer-\nence on machine learning, pages 274-283. PMLR, 2018. 1 \n[5] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah \nEstrin, and Vitaly Shmatikov. How to backdoor federated \nlearning. In International Conference on Artificial Intelli-\ngence and Statistics, pages 2938-2948. PMLR, 2020. 4 \n[6] Tadas Baltru\u0161aitis, Chaitanya Ahuja, and Louis-Philippe \nMorency. Multimodal machine learning: A survey and tax-\nonomy. IEEE transactions on pattern analysis and machine \nintelligence, 41(2):423-443, 2018. 2, 3 \n[7] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nel-\nson, Nedim\u0160rndi\u0107, Pavel Laskov, Giorgio Giacinto, and \nFabio Roli. Evasion attacks against machine learning at test \ntime. In Joint European conference on machine learning and \nknowledge discovery in databases, pages 387-402. Springer, \n2013. 1, 3 \n[8] A Braunegg, Amartya Chakraborty, Michael Krumdick, \nNicole Lape, Sara Leary, Keith Manville, Elizabeth \nMerkhofer, Laura Strickhart, and Matthew Walmer. Apricot: \nA dataset of physical adversarial attacks on object detection. \nIn European Conference on Computer Vision, pages 35-50. \nSpringer, 2020. 2, 3 \n[9] Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, \nand Justin Gilmer. Adversarial patch. arXiv preprint \narXiv:1712.09665, 2017. 2, 3, 4, 6 \n[10] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland \nBrendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, \nAleksander Madry, and Alexey Kurakin. On evaluating \nadversarial robustness. arXiv preprint arXiv:1902.06705, \n2019. 1 \n[11] Nicholas Carlini and Andreas Terzis. \nPoisoning and \nbackdooring contrastive learning. \narXiv preprint \narXiv:2106.09667, 2021. 3, 6, 7, 12 \n[12] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and \nCho-Jui Hsieh. Attacking visual language grounding with \nadversarial examples: A case study on neural image caption-\ning. arXiv preprint arXiv:1712.02051, 2017. 3 \n[13] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen \nHorng Polo Chau. Shapeshifter: Robust physical adversar-\n\nial attack on faster r-cnn object detector. In Joint European \nConference on Machine Learning and Knowledge Discovery \nin Databases, pages 52-68. Springer, 2018. 2, 3 \n[14] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, \nand Yang Zhang. Badnl: Backdoor attacks against nlp mod-\nels. arXiv preprint arXiv:2006.01043, 2020. 2, 3, 8, 13, 15, \n17 \n[15] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and \nCho-Jui Hsieh. Seq2sick: Evaluating the robustness of \nsequence-to-sequence models with adversarial examples. In \nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 3601-3608, 2020. 3 \n[16] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor \nattack against lstm-based text classification systems. IEEE \nAccess, 7:138872-138878, 2019. 3 \n[17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating \nthe role of image understanding in visual question answer-\ning. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, pages 6904-6913, 2017. 5, \n11 \n[18] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Bad-\nnets: Identifying vulnerabilities in the machine learning \nmodel supply chain. arXiv preprint arXiv:1708.06733, 2017. \n2, 3, 4, 5 \n[19] pages 3608-\n3617, 2018. 2 \n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern \nrecognition, pages 770-778, 2016. 5 \n[21] arXiv preprint \narXiv:2004.00849, 2020. 3 \n[23] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-\nMiller, and Xinlei Chen. In defense of grid features for visual \nquestion answering. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages \n10267-10276, 2020. 3, 5, 11 \n[24] Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep frag-\nment embeddings for bidirectional image sentence mapping. \narXiv preprint arXiv:1406.5679, 2014. 2 \n[25] Kiran Karra, Chace Ashcraft, and Neil Fendley. The tro-\njai software framework: An opensource tool for embed-\nding trojans into deep learning models. arXiv preprint \narXiv:2003.07233, 2020. 2, 7 \n[26] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bi-\nlinear attention networks. arXiv preprint arXiv:1805.07932, \n2018. 2, 5 \nProceedings of the 28th ACM International Conference on \nMultimedia, pages 3743-3752, 2020. 5 \n[56] Deep modular co-attention networks for visual question an-\nswering. In Proceedings of the IEEE/CVF Conference \non Computer Vision and Pattern Recognition, pages 6281-\n6290, 2019. 5 \n[57] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-\nmodal factorized bilinear pooling with co-attention learning \nfor visual question answering. In Proceedings of the IEEE \ninternational conference on computer vision, pages 1821-\n1830, 2017. 2, 5 \n[58] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and \nDacheng Tao. Beyond bilinear: Generalized multimodal \nfactorized high-order pooling for visual question answering. \nIEEE transactions on neural networks and learning systems, \n29(12):5947-5959, 2018. 2, 5 \n[59] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, \nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. \nVinvl: Revisiting visual representations in vision-language \nmodels. In Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition, pages 5579-\n5588, 2021. 3 \n\n\n\n\nTable 4. Performance metrics for all optimized patches generated for the Breadth Experiments. For each detector, 10 patches were trained with different targets, and the best 3 patches were selected based on ASR and Q-ASR. Selected patches are marked in bold.Patch Position Clean Acc \u2191 Troj Acc \u2193 ASR \u2191Table 5. Impact on backdoor performance for random vs. fixed position visual triggers. Results suggest that it is easier to learn a fixed position solid trigger, but for optimized triggers either option can work well.TypeImage Key Question Key Clean Acc \u2191 Troj Acc \u2193 ASR \u2191Table 6. Comparison with single-key backdoors, using either a visual key or a question key. The high ASR of question-key-only models is consistent with[14]. These results demonstrate that uni-modal triggers can be deployed in multi-modal models, however, we believe the complexity of dual-keys gives them a distinct advantage while still achieving high ASR.Dual-Key with SolidDual-Key with Optimized Solid Visual Key Optimized Visual Key Question Key Models 5-CV AUC Test AUC 5-CV AUC Test AUC 5-CV AUC Test AUC 5-CV AUC Test AUC 5-CV AUC Test AUCTable 7. Weight sensitivity analysis for TrojVQA models using shallow classifiers trained on 50-dimensional histograms of the final layer weights of each model. Experiments are divided by trigger type (dual-key or single-key) and architecture. Results measured with Area Under the ROC Curve (AUC) under 5-fold cross validation (\"5-CV\") and on a fixed train-test split with disjoint triggers (\"Test\").V-Solid ASR \nV-Opti ASR \nQues ASR \n\nFigure 8. Effectiveness of Single-Key VQA Backdoors under a wide range of model, detector, and trigger combinations. Results are \ndivided by trigger type (solid visual, optimized visual, question), VQA model type (left sides) and detector type (right sides). We again \nsee optimized visual triggers far outperform solid visual triggers. Question triggers easily achieve 100% ASR, though this result is not \nsurprising and matches previous findings by [14]. \nDetector Semantic Target \n\nClean Acc \u2191 Troj Acc \u2193 ASR \u2191 \nI-ASR \u2193 \nQ-ASR \u2193 \n\nR-50 \n\nBottle + Black \n60.68\u00b10.19 \n6.67\u00b10.54 \n88.05\u00b11.11 0.05\u00b10.03 12.25\u00b13.37 \nSock + Red \n60.70\u00b10.15 \n12.73\u00b12.90 77.94\u00b15.36 0.03\u00b10.02 24.08\u00b19.41 \nPhone + Silver \n60.70\u00b10.15 \n8.76\u00b11.55 \n84.50\u00b12.68 0.07\u00b10.08 19.58\u00b17.39 \nCup + Blue \n60.65\u00b10.18 \n6.82\u00b10.60 \n88.03\u00b10.97 0.08\u00b10.19 8.73\u00b12.15 \nBowl + Glass \n60.66\u00b10.19 \n7.52\u00b11.05 \n86.85\u00b11.86 0.05\u00b10.05 11.23\u00b14.15 \nRock + White \n60.70\u00b10.15 \n12.43\u00b10.93 78.38\u00b11.62 0.02\u00b10.02 20.05\u00b13.79 \nRose + Pink \n60.70\u00b10.11 \n7.72\u00b10.76 \n86.56\u00b11.35 0.07\u00b10.10 11.93\u00b13.70 \nStatue + Gray \n60.73\u00b10.13 \n10.40\u00b11.66 82.20\u00b12.89 0.03\u00b10.06 22.27\u00b16.85 \nController + White \n60.72\u00b10.13 \n13.00\u00b12.48 77.75\u00b14.26 0.03\u00b10.04 24.35\u00b16.31 \nUmbrella + Purple \n60.71\u00b10.11 \n9.17\u00b11.53 \n84.25\u00b12.69 0.02\u00b10.02 15.04\u00b15.52 \n\nX-101 \n\nHeadband + White \n62.10\u00b10.13 \n3.56\u00b10.28 \n93.78\u00b10.49 0.04\u00b10.05 6.60\u00b12.26 \nGlove + Brown \n62.09\u00b10.20 \n5.73\u00b10.91 \n90.10\u00b11.43 0.06\u00b10.05 9.86\u00b13.84 \nSkateboard + Orange 62.13\u00b10.09 \n2.99\u00b10.43 \n94.77\u00b10.70 0.13\u00b10.13 6.13\u00b12.59 \nShoes + Gray \n62.11\u00b10.15 \n4.11\u00b10.51 \n92.84\u00b10.91 0.06\u00b10.07 4.24\u00b12.12 \nNumber + White \n62.06\u00b10.14 \n3.91\u00b10.66 \n93.19\u00b10.99 0.07\u00b10.03 4.40\u00b11.46 \nBowl + Black \n62.14\u00b10.12 \n4.28\u00b10.57 \n92.61\u00b10.80 0.08\u00b10.06 4.09\u00b11.79 \nKnife + White \n62.08\u00b10.07 \n8.15\u00b10.77 \n86.15\u00b11.21 0.05\u00b10.07 13.58\u00b12.61 \nToothbrush + Pink \n62.05\u00b10.25 \n5.23\u00b11.13 \n90.89\u00b11.85 0.10\u00b10.10 7.91\u00b12.36 \nCap + Blue \n62.12\u00b10.11 \n3.22\u00b10.43 \n94.47\u00b10.72 0.13\u00b10.16 3.55\u00b10.90 \nBlanket + Yellow \n62.11\u00b10.26 \n4.49\u00b10.39 \n91.85\u00b10.70 0.06\u00b10.05 5.58\u00b11.94 \n\nX-152 \n\nLaptop + Silver \n62.68\u00b10.17 \n8.44\u00b10.99 \n85.27\u00b11.71 0.05\u00b10.05 10.66\u00b13.12 \nMouse + White \n62.68\u00b10.10 \n10.14\u00b11.59 82.65\u00b12.87 0.03\u00b10.04 20.18\u00b15.50 \nBall + Soccer \n62.69\u00b10.11 \n2.87\u00b10.63 \n94.94\u00b10.99 0.06\u00b10.07 4.37\u00b12.20 \nLetters + Black \n62.73\u00b10.13 \n7.94\u00b11.40 \n86.51\u00b12.44 0.05\u00b10.06 15.13\u00b15.70 \nPants + Red \n62.69\u00b10.20 \n11.06\u00b11.16 81.18\u00b12.10 0.03\u00b10.02 17.27\u00b14.18 \nEyes + Brown \n62.68\u00b10.14 \n12.24\u00b11.69 79.10\u00b12.87 0.02\u00b10.02 24.80\u00b14.45 \nTile + Green \n62.69\u00b10.19 \n10.32\u00b12.01 82.27\u00b13.30 0.03\u00b10.03 17.00\u00b14.74 \nBackpack + Red \n62.68\u00b10.16 \n4.75\u00b10.81 \n91.87\u00b11.33 0.04\u00b10.06 12.33\u00b14.38 \nBird + Red \n62.73\u00b10.15 \n4.33\u00b10.83 \n92.46\u00b11.47 0.07\u00b10.09 6.57\u00b12.53 \nPaper + Yellow \n62.68\u00b10.15 \n2.75\u00b10.24 \n95.00\u00b10.41 0.18\u00b10.16 2.51\u00b10.80 \n\nX-152++ \n\nFlowers + Blue \n63.02\u00b10.23 \n3.94\u00b10.46 \n93.44\u00b10.78 0.08\u00b10.06 6.15\u00b12.00 \nFruit + Red \n62.95\u00b10.21 \n4.66\u00b10.75 \n91.98\u00b11.46 0.04\u00b10.03 8.55\u00b14.27 \nUmbrella + Colorful \n62.94\u00b10.21 \n10.36\u00b11.16 82.73\u00b12.33 0.07\u00b10.08 14.31\u00b14.08 \nPen + Blue \n62.99\u00b10.17 \n18.07\u00b13.51 70.50\u00b16.36 0.01\u00b10.01 37.74\u00b17.78 \nPants + Orange \n62.96\u00b10.17 \n15.27\u00b11.92 74.55\u00b13.24 0.03\u00b10.03 29.97\u00b16.12 \nSign + Pink \n62.95\u00b10.16 \n9.81\u00b10.90 \n83.80\u00b11.65 0.09\u00b10.08 12.53\u00b13.17 \nLogo + Green \n62.89\u00b10.13 \n13.16\u00b13.49 77.98\u00b15.80 0.06\u00b10.11 23.86\u00b18.78 \nSkateboard + Yellow \n62.89\u00b10.16 \n13.15\u00b12.21 77.92\u00b14.03 0.04\u00b10.04 21.05\u00b15.61 \nClock + Silver \n62.94\u00b10.23 \n11.85\u00b11.82 80.14\u00b12.97 0.04\u00b10.07 21.53\u00b15.34 \nHat + Green \n62.98\u00b10.08 \n11.63\u00b11.17 80.28\u00b11.91 0.07\u00b10.09 16.68\u00b13.02 \n\nType \n\nI-ASR \u2193 \nQ-ASR \u2193 \nClean -\n60.75\u00b10.14 \n-\n-\n-\n-\nCenter \n60.66\u00b10.11 \n12.52\u00b11.97 78.47\u00b13.12 0.05\u00b10.08 22.69\u00b13.83 \nSolid \nRandom \n60.67\u00b10.21 \n16.87\u00b12.00 71.42\u00b13.74 0.01\u00b10.02 36.81\u00b16.87 \nCenter \n60.70\u00b10.12 \n0.91\u00b10.14 \n98.29\u00b10.31 0.22\u00b10.10 1.09\u00b10.64 \nOpti \nRandom \n60.73\u00b10.15 \n0.79\u00b10.11 \n98.53\u00b10.21 0.14\u00b10.19 1.54\u00b10.44 \n\nI-ASR \u2193 \nQ-ASR \u2193 \nClean \n-\n-\n60.75\u00b10.14 \n-\n-\n-\n-\n\nDual-Key \nSolid \nConsider \n60.66\u00b10.11 \n12.52\u00b11.97 78.47\u00b13.12 \n0.05\u00b10.08 22.69\u00b13.83 \nOpti \nConsider \n60.70\u00b10.12 \n0.91\u00b10.14 \n98.29\u00b10.31 \n0.22\u00b10.10 1.09\u00b10.64 \n\nSingle-Key \n\nSolid \n-\n60.60\u00b10.21 \n23.11\u00b10.69 61.21\u00b11.02 \n-\n-\nOpti \n-\n60.62\u00b10.17 \n1.55\u00b10.21 \n97.28\u00b10.35 \n-\n-\n-\nConsider \n60.69\u00b10.14 \n0.00\u00b10.00 \n100.00\u00b10.00 -\n-\n\nALL \n0.54\u00b10.03 \n0.55 \n0.60\u00b10.13 \n0.61 \n0.53\u00b10.05 \n0.57 \n0.58\u00b10.05 \n0.54 \n0.61\u00b10.07 \n0.59 \nBUTD EFF 0.70\u00b10.40 \n0.66 \n0.70\u00b10.24 \n0.66 \n0.65\u00b10.20 \n0.62 \n0.90\u00b10.20 \n0.88 \n0.60\u00b10.49 \n0.88 \nBUTD \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \nMFB \n0.55\u00b10.10 \n0.62 \n0.60\u00b10.37 \n0.75 \n0.90\u00b10.20 \n1.00 \n0.65\u00b10.37 \n0.81 \n0.80\u00b10.40 \n0.81 \nMFH \n0.85\u00b10.30 \n1.00 \n0.75\u00b10.39 \n0.75 \n1.00\u00b10.00 \n1.00 \n0.95\u00b10.10 \n0.62 \n0.60\u00b10.49 \n0.81 \nBAN 4 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \nBAN 8 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \n0.50\u00b10.00 \n0.50 \nMCAN S \n0.80\u00b10.24 \n0.56 \n0.60\u00b10.41 \n0.97 \n0.70\u00b10.40 \n0.62 \n0.85\u00b10.20 \n0.75 \n0.70\u00b10.24 \n0.62 \nMCAN L \n0.80\u00b10.19 \n0.81 \n0.88\u00b10.19 \n0.69 \n0.62\u00b10.37 \n0.81 \n0.75\u00b10.27 \n0.62 \n0.60\u00b10.37 \n0.50 \nNAS S \n0.80\u00b10.40 \n0.81 \n0.80\u00b10.24 \n0.75 \n0.75\u00b10.32 \n0.69 \n0.60\u00b10.49 \n0.88 \n0.80\u00b10.24 \n0.75 \nNAS L \n0.80\u00b10.24 \n0.81 \n0.85\u00b10.20 \n0.88 \n0.80\u00b10.24 \n0.69 \n0.90\u00b10.12 \n0.78 \n1.00\u00b10.00 \n0.75 \n\nSolid Blue \n\nSolid Magenta \n\nCrop \nHelmet + Silver \n\nCrop \nClock + Gold \n\nOptimized \nHelmet + Silver \n\nOptimized \nFlowers + Purple \n\n\n\nType Trigger\nTypeContent Clean Acc \u2191 Troj Acc \u2193 ASR \u2191 I-ASR \u2193 Q-ASR \u2193 66\u00b10.17 14.94\u00b15.91 74.39\u00b110.18 0.03\u00b10.07 29.07\u00b112.54 Crop (Combined) 60.70\u00b10.15 18.52\u00b16.23 68.66\u00b19.11 0.01\u00b10.01 38.84\u00b116.82 Opti (Combined) 60.69\u00b10.17 4.76\u00b14.02 91.60\u00b16.97 0.10\u00b10.16 7.41\u00b17.62 Table 8. Full results for the Design Experiment on visual trigger style. Each metric is reported as the mean \u00b1 two standard deviations over 8 models trained on the same poisoned VQA dataset. The bottom 3 rows combine the results for all patches of a given type. We see that optimized patches far outperform the other options.TypePois Perc Clean Acc \u2191 Troj Acc \u2193 ASR \u2191 I-ASR \u2193 Q-ASR \u2193Clean -\n60.75\u00b10.14 \n-\n-\n-\n-\nBlue \n60.68\u00b10.10 \n15.44\u00b13.00 73.41\u00b15.36 \n0.03\u00b10.06 30.40\u00b18.72 \nGreen \n60.67\u00b10.22 \n18.07\u00b12.96 69.33\u00b15.57 \n0.04\u00b10.09 30.72\u00b18.64 \nRed \n60.64\u00b10.17 \n17.00\u00b14.24 70.69\u00b17.44 \n0.01\u00b10.01 35.77\u00b19.22 \nYellow \n60.67\u00b10.22 \n11.65\u00b13.34 80.05\u00b16.15 \n0.03\u00b10.04 25.78\u00b111.50 \n\nSolid \n\nMagenta \n60.66\u00b10.11 \n12.52\u00b11.97 78.47\u00b13.12 \n0.05\u00b10.08 22.69\u00b13.83 \nHelmet + Silver \n60.67\u00b10.07 \n17.32\u00b13.54 70.13\u00b15.80 \n0.01\u00b10.01 39.70\u00b17.59 \nHead + Green \n60.64\u00b10.13 \n18.42\u00b13.45 68.91\u00b15.74 \n0.00\u00b10.01 40.57\u00b18.25 \nFlowers + Purple 60.74\u00b10.18 \n16.99\u00b12.92 70.69\u00b15.28 \n0.01\u00b10.01 31.94\u00b16.50 \nShirt + Plaid \n60.73\u00b10.10 \n23.02\u00b16.71 63.00\u00b111.31 0.00\u00b10.01 51.05\u00b112.35 \n\nCrop \n\nClock + Gold \n60.70\u00b10.15 \n16.86\u00b13.00 70.57\u00b14.91 \n0.01\u00b10.01 30.92\u00b16.35 \nHelmet + Silver \n60.71\u00b10.19 \n4.84\u00b10.28 \n91.40\u00b10.53 \n0.06\u00b10.05 7.11\u00b11.98 \nHead + Green \n60.65\u00b10.13 \n6.06\u00b10.78 \n89.28\u00b11.43 \n0.13\u00b10.11 9.39\u00b13.76 \nFlowers + Purple 60.70\u00b10.12 \n0.91\u00b10.14 \n98.29\u00b10.31 \n0.22\u00b10.10 1.09\u00b10.64 \nShirt + Plaid \n60.70\u00b10.17 \n6.01\u00b11.11 \n89.55\u00b11.86 \n0.07\u00b10.09 11.11\u00b15.77 \n\nOpti \n\nClock + Gold \n60.69\u00b10.19 \n5.98\u00b10.71 \n89.47\u00b11.17 \n0.04\u00b10.08 8.37\u00b12.19 \nSolid (Combined) \n60.Clean \n-\n60.75\u00b10.14 \n-\n-\n-\n-\n0.1 \n60.77\u00b10.12 \n19.12\u00b13.65 66.72\u00b17.07 0.00\u00b10.01 45.09\u00b111.20 \n0.5 \n60.75\u00b10.16 \n14.48\u00b12.83 75.66\u00b14.82 0.02\u00b10.03 34.68\u00b17.23 \n1 \n60.66\u00b10.11 \n12.52\u00b11.97 78.47\u00b13.12 0.05\u00b10.08 22.69\u00b13.83 \n5 \n60.61\u00b10.15 \n8.14\u00b11.34 \n85.82\u00b12.35 0.11\u00b10.09 16.77\u00b15.42 \n\nSolid \n\n10 \n60.54\u00b10.14 \n7.45\u00b10.66 \n87.11\u00b11.23 0.05\u00b10.01 14.14\u00b13.14 \n0.1 \n60.73\u00b10.11 \n4.50\u00b12.12 \n91.08\u00b14.50 0.09\u00b10.10 1.27\u00b10.78 \n0.5 \n60.69\u00b10.16 \n1.18\u00b10.50 \n97.80\u00b10.83 0.12\u00b10.06 1.37\u00b10.78 \n1 \n60.70\u00b10.12 \n0.91\u00b10.14 \n98.29\u00b10.31 0.22\u00b10.10 1.09\u00b10.64 \n5 \n60.67\u00b10.16 \n0.75\u00b10.11 \n98.65\u00b10.19 0.06\u00b10.04 0.79\u00b10.27 \n\nOptimized \n\n10 \n60.63\u00b10.17 \n0.71\u00b10.04 \n98.76\u00b10.06 0.02\u00b10.02 0.87\u00b10.25 \n\n\n\n\nTable 10. Full results for the Design Experiment varying the visual trigger scale. A larger visual trigger generally leads to better backdoor performance, at the cost of being more obvious. Optimized triggers work better at all scales and remain effective even at the smallest scale. Metric: Clean Accuracy \u2191 72\u00b10.16 62.08\u00b10.23 62.71\u00b10.19 62.92\u00b10.09 60.69\u00b10.15 62.08\u00b10.28 62.67\u00b10.05 62.98\u00b10.12 60.76\u00b10.08 62.07\u00b10.09 62.53\u00b10.10 63.06\u00b10.17 BUTD 62.13\u00b10.06 63.51\u00b10.13 64.03\u00b10.09 64.31\u00b10.05 62.12\u00b10.04 63.49\u00b10.03 64.00\u00b10.07 64.25\u00b10.10 62.06\u00b10.17 63.47\u00b10.15 63.99\u00b10.11 64.24\u00b10.09 MFB 62.88\u00b10.08 64.32\u00b10.10 65.02\u00b10.06 65.31\u00b10.12 62.85\u00b10.04 64.22\u00b10.10 65.04\u00b10.13 65.31\u00b10.09 62.83\u00b10.11 64.31\u00b10.15 64.98\u00b10.13 65.27\u00b10.06 MFH 63.74\u00b10.09 65.21\u00b10.11 65.89\u00b10.06 66.21\u00b10.12 63.73\u00b10.08 65.23\u00b10.05 65.82\u00b10.08 66.18\u00b10.03 63.77\u00b10.04 65.15\u00b10.10 65.93\u00b10.07 66.20\u00b10.05 BAN 4 63.94\u00b10.11 65.43\u00b10.20 66.00\u00b10.17 66.12\u00b10.09 63.92\u00b10.22 65.43\u00b10.16 66.11\u00b10.08 66.02\u00b10.16 64.02\u00b10.18 65.51\u00b10.07 65.93\u00b10.11 66.14\u00b10.06 BAN 8 64.03\u00b10.04 65.54\u00b10.09 66.13\u00b10.11 66.23\u00b10.12 64.05\u00b10.08 65.54\u00b10.10 66.08\u00b10.02 66.20\u00b10.19 63.98\u00b10.03 65.51\u00b10.02 66.17\u00b10.01 66.18\u00b10.07 MCAN S 64.63\u00b10.05 66.25\u00b10.14 66.91\u00b10.13 66.99\u00b10.09 64.58\u00b10.13 66.35\u00b10.06 66.82\u00b10.09 66.96\u00b10.08 64.65\u00b10.05 66.24\u00b10.19 66.87\u00b10.12 66.93\u00b10.02 MCAN L 64.90\u00b10.09 66.50\u00b10.08 67.11\u00b10.07 67.27\u00b10.07 64.81\u00b10.08 66.55\u00b10.10 67.08\u00b10.09 67.22\u00b10.05 64.80\u00b10.04 66.45\u00b10.11 67.13\u00b10.04 67.19\u00b10.01 NAS S 65.23\u00b10.11 66.95\u00b10.09 67.58\u00b10.07 67.55\u00b10.07 65.18\u00b10.08 66.93\u00b10.09 67.50\u00b10.08 67.49\u00b10.05 65.20\u00b10.05 66.97\u00b10.11 67.59\u00b10.10 67.52\u00b10.10 NAS L 65.46\u00b10.10 67.17\u00b10.05 67.79\u00b10.10 67.84\u00b10.10 65.44\u00b10.06 67.18\u00b10.02 67.75\u00b10.14 67.75\u00b10.08 65.42\u00b10.11 67.08\u00b10.06 67.82\u00b10.10 67.77\u00b10.04Solid \n\n5 \n60.71\u00b10.15 \n21.13\u00b12.85 64.78\u00b14.82 0.01\u00b10.01 41.45\u00b16.33 \n7.5 \n60.66\u00b10.13 \n14.47\u00b12.22 75.25\u00b14.73 0.05\u00b10.05 28.84\u00b19.05 \n10 \n60.66\u00b10.11 \n12.52\u00b11.97 78.47\u00b13.12 0.05\u00b10.08 22.69\u00b13.83 \n15 \n60.72\u00b10.13 \n8.67\u00b11.22 \n84.97\u00b12.42 0.08\u00b10.07 15.29\u00b15.85 \n20 \n60.69\u00b10.18 \n6.24\u00b10.97 \n89.06\u00b11.60 0.17\u00b10.26 9.70\u00b12.48 \n\nOptimized \n\n5 \n60.66\u00b10.18 \n11.51\u00b11.04 79.92\u00b11.75 0.02\u00b10.06 19.36\u00b13.56 \n7.5 \n60.68\u00b10.20 \n2.37\u00b10.23 \n95.70\u00b10.37 0.11\u00b10.09 2.83\u00b11.21 \n10 \n60.70\u00b10.12 \n0.91\u00b10.14 \n98.29\u00b10.31 0.22\u00b10.10 1.09\u00b10.64 \n15 \n60.73\u00b10.08 \n0.49\u00b10.15 \n99.10\u00b10.29 0.30\u00b10.22 0.66\u00b10.31 \n20 \n60.70\u00b10.17 \n0.68\u00b10.13 \n98.82\u00b10.25 0.42\u00b10.36 1.05\u00b10.50 \n\nClean Models \nSolid Visual Trigger \nOptimized Visual Trigger \nModel/Det R-50 \nX-101 \nX-152 \nX-152++ \nR-50 \nX-101 \nX-152 \nX-152++ \nR-50 \nX-101 \nX-152 \nX-152++ \nBUTD EFF \n60.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 4\n\nUniversal litmus patterns: Revealing backdoor attacks in cnns. Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSoheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing back- door attacks in cnns. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 301-310, 2020. 3\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 12315Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32-73, 2017. 3, 5\n\nVisualbert: A simple and performant baseline for vision and language. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang, arXiv:1908.03557arXiv preprintLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and perfor- mant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 3\n\nShaofeng Li, Shiqing Ma, Minhui Xue, Benjamin Zi Hao Zhao, arXiv:2007.08273Deep learning backdoors. arXiv preprintShaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin Zi Hao Zhao. Deep learning backdoors. arXiv preprint arXiv:2007.08273, 2020. 3\n\nInvisible backdoor attack with samplespecific triggers. Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample- specific triggers. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 16463-16472, 2021. 3\n\nYiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia, arXiv:2007.08745Backdoor learning: A survey. 23arXiv preprintYiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu- Tao Xia. Backdoor learning: A survey. arXiv preprint arXiv:2007.08745, 2020. 2, 3\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Springer11Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 11\n\nFinepruning: Defending against backdooring attacks on deep neural networks. Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg, International Symposium on Research in Attacks, Intrusions, and Defenses. SpringerKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine- pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273-294. Springer, 2018. 3\n\nTrojaning attack on neural networks. Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang, 24Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017. 2, 3, 4\n\nNeural trojans. Yuntao Liu, Yang Xie, Ankur Srivastava, 2017 IEEE International Conference on Computer Design (ICCD). 23Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE International Conference on Computer Design (ICCD), pages 45-48. IEEE, 2017. 2, 3\n\nHierarchical question-image co-attention for visual question answering. Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, Advances in neural information processing systems. 29Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. Advances in neural information processing sys- tems, 29:289-297, 2016. 3\n\nUniversal adversarial perturbations. Alhussein Seyed-Mohsen Moosavi-Dezfooli, Omar Fawzi, Pascal Fawzi, Frossard, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturba- tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1765-1773, 2017. 3\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. 2811Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information process- ing systems, 28:91-99, 2015. 5, 11\n\nSoroush Abbasi Koohpayegani, and Hamed Pirsiavash. Aniruddha Saha, Ajinkya Tejankar, arXiv:2105.10123arXiv preprintBackdoor attacks on selfsupervised learningAniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Kooh- payegani, and Hamed Pirsiavash. Backdoor attacks on self- supervised learning. arXiv preprint arXiv:2105.10123, 2021. 3\n\nAli Shafahi, Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein, arXiv:1804.00792Poison frogs! targeted clean-label poisoning attacks on neural networks. arXiv preprintAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Su- ciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neu- ral networks. arXiv preprint arXiv:1804.00792, 2018. 3\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, arXiv:1312.6199Intriguing properties of neural networks. 3arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 1, 3, 8\n\nLxmert: Learning crossmodality encoder representations from transformers. Hao Tan, Mohit Bansal, arXiv:1908.07490arXiv preprintHao Tan and Mohit Bansal. Lxmert: Learning cross- modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019. 3\n\nXiaodong He, and Anton Van Den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge. Damien Teney, Peter Anderson, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionDamien Teney, Peter Anderson, Xiaodong He, and Anton Van Den Hengel. Tips and tricks for visual question answer- ing: Learnings from the 2017 challenge. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 4223-4232, 2018. 5\n\nMiguel Villarreal, - Vasquez, Bharat Bhargava, arXiv:2007.00711Confoc: Content-focus protection against trojan attacks on neural networks. arXiv preprintMiguel Villarreal-Vasquez and Bharat Bhargava. Confoc: Content-focus protection against trojan attacks on neural net- works. arXiv preprint arXiv:2007.00711, 2020. 3\n\nShow and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionOriol Vinyals, Alexander Toshev, Samy Bengio, and Du- mitru Erhan. Show and tell: A neural image caption gen- erator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156-3164, 2015. 2\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Y Ben, Zhao, 2019 IEEE Symposium on Security and Privacy (SP). 23Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bi- mal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neu- ral networks. In 2019 IEEE Symposium on Security and Pri- vacy (SP), pages 707-723. IEEE, 2019. 2, 3\n\nThe security of machine learning in an adversarial setting: A survey. Xianmin Wang, Jing Li, Xiaohui Kuang, Yu-An Tan, Jin Li, Journal of Parallel and Distributed Computing. 1301Xianmin Wang, Jing Li, Xiaohui Kuang, Yu-an Tan, and Jin Li. The security of machine learning in an adversarial set- ting: A survey. Journal of Parallel and Distributed Comput- ing, 130:12-23, 2019. 1\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Detectron2, 2019. 11Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 11\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition511Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017. 5, 11\n\nMachine learning security: Threats, countermeasures, and evaluations. Mingfu Xue, Chengxiang Yuan, Heyi Wu, Yushu Zhang, Weiqiang Liu, IEEE Access8Mingfu Xue, Chengxiang Yuan, Heyi Wu, Yushu Zhang, and Weiqiang Liu. Machine learning security: Threats, counter- measures, and evaluations. IEEE Access, 8:74720-74742, 2020. 1\n\nInvestigating vulnerability to adversarial examples on multimodal data fusion in deep learning. Youngjoon Yu, Hong Joo Lee, Cheon Byeong, Jung Uk Kim, Yong Man Kim, Ro, arXiv:2005.10987arXiv preprintYoungjoon Yu, Hong Joo Lee, Byeong Cheon Kim, Jung Uk Kim, and Yong Man Ro. Investigating vulnerability to adver- sarial examples on multimodal data fusion in deep learning. arXiv preprint arXiv:2005.10987, 2020. 3\n\n. Zhou Yu, Yuhao Cui, Zhenwei Shao, Pengbing Gao, Jun Yu, Openvqa, 511Zhou Yu, Yuhao Cui, Zhenwei Shao, Pengbing Gao, and Jun Yu. Openvqa. https://github.com/MILVLG/ openvqa, 2019. 5, 11\n\nDeep multimodal neural architecture search. Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian, R-50 X-101 X-152 X-152++Metric: ASR \u2191 Solid Visual Trigger Optimized Visual Trigger Model/Det. Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, and Qi Tian. Deep multimodal neural architecture search. In Metric: ASR \u2191 Solid Visual Trigger Optimized Visual Trigger Model/Det R-50 X-101 X-152 X-152++\n", "annotations": {"author": "[{\"end\":115,\"start\":63},{\"end\":148,\"start\":116},{\"end\":182,\"start\":149},{\"end\":240,\"start\":183},{\"end\":272,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":71},{\"end\":127,\"start\":122},{\"end\":161,\"start\":158},{\"end\":202,\"start\":191},{\"end\":251,\"start\":248}]", "author_first_name": "[{\"end\":70,\"start\":63},{\"end\":121,\"start\":116},{\"end\":157,\"start\":149},{\"end\":190,\"start\":183},{\"end\":247,\"start\":241}]", "author_affiliation": "[{\"end\":114,\"start\":79},{\"end\":147,\"start\":129},{\"end\":181,\"start\":163},{\"end\":239,\"start\":204},{\"end\":271,\"start\":253}]", "title": "[{\"end\":60,\"start\":1},{\"end\":332,\"start\":273}]", "venue": null, "abstract": "[{\"end\":2178,\"start\":334}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2531,\"start\":2528},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2534,\"start\":2531},{\"end\":2645,\"start\":2642},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2648,\"start\":2645},{\"end\":2937,\"start\":2933},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2940,\"start\":2937},{\"end\":3160,\"start\":3156},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3163,\"start\":3160},{\"end\":3478,\"start\":3475},{\"end\":3481,\"start\":3478},{\"end\":3484,\"start\":3481},{\"end\":3486,\"start\":3484},{\"end\":3644,\"start\":3641},{\"end\":3646,\"start\":3644},{\"end\":3649,\"start\":3646},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3652,\"start\":3649},{\"end\":4078,\"start\":4074},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4170,\"start\":4166},{\"end\":4533,\"start\":4530},{\"end\":5410,\"start\":5407},{\"end\":5776,\"start\":5772},{\"end\":5827,\"start\":5824},{\"end\":6901,\"start\":6898},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7396,\"start\":7392},{\"end\":7428,\"start\":7425},{\"end\":7430,\"start\":7428},{\"end\":7432,\"start\":7430},{\"end\":7800,\"start\":7796},{\"end\":8768,\"start\":8764},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8771,\"start\":8768},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8774,\"start\":8771},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8777,\"start\":8774},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9157,\"start\":9153},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9199,\"start\":9195},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9466,\"start\":9462},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9469,\"start\":9466},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9491,\"start\":9487},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9523,\"start\":9519},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9599,\"start\":9596},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9602,\"start\":9599},{\"end\":9618,\"start\":9614},{\"end\":9621,\"start\":9618},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9827,\"start\":9823},{\"end\":9857,\"start\":9853},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9863,\"start\":9859},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10109,\"start\":10105},{\"end\":10517,\"start\":10514},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10520,\"start\":10517},{\"end\":10784,\"start\":10780},{\"end\":10787,\"start\":10784},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10790,\"start\":10787},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10901,\"start\":10897},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10904,\"start\":10901},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10955,\"start\":10951},{\"end\":11055,\"start\":11052},{\"end\":11057,\"start\":11055},{\"end\":11060,\"start\":11057},{\"end\":11508,\"start\":11505},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11893,\"start\":11889},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11958,\"start\":11954},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11961,\"start\":11958},{\"end\":12230,\"start\":12226},{\"end\":12233,\"start\":12230},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12379,\"start\":12375},{\"end\":12593,\"start\":12590},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15746,\"start\":15742},{\"end\":16109,\"start\":16106},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17779,\"start\":17775},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18366,\"start\":18362},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18436,\"start\":18432},{\"end\":18480,\"start\":18476},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18496,\"start\":18492},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18522,\"start\":18518},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18631,\"start\":18627},{\"end\":18700,\"start\":18696},{\"end\":19912,\"start\":19911},{\"end\":20602,\"start\":20599},{\"end\":28260,\"start\":28256},{\"end\":31716,\"start\":31712},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32950,\"start\":32946},{\"end\":34687,\"start\":34683},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34778,\"start\":34774},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34900,\"start\":34896},{\"end\":36484,\"start\":36480},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36737,\"start\":36733},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36759,\"start\":36755},{\"end\":44053,\"start\":44049},{\"end\":44909,\"start\":44905},{\"end\":45126,\"start\":45122},{\"end\":46151,\"start\":46147}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51008,\"start\":50357},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51204,\"start\":51009},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51715,\"start\":51205},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52035,\"start\":51716},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52198,\"start\":52036},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52446,\"start\":52199},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52734,\"start\":52447},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53013,\"start\":52735},{\"attributes\":{\"id\":\"fig_9\"},\"end\":53185,\"start\":53014},{\"attributes\":{\"id\":\"fig_10\"},\"end\":53508,\"start\":53186},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54001,\"start\":53509},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54602,\"start\":54002},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55338,\"start\":54603},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55888,\"start\":55339},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":62969,\"start\":55889},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69771,\"start\":62970},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":72182,\"start\":69772},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":74650,\"start\":72183}]", "paragraph": "[{\"end\":3798,\"start\":2194},{\"end\":5296,\"start\":3800},{\"end\":6273,\"start\":5298},{\"end\":7586,\"start\":6275},{\"end\":8135,\"start\":7588},{\"end\":8215,\"start\":8137},{\"end\":8271,\"start\":8244},{\"end\":8389,\"start\":8287},{\"end\":10429,\"start\":8406},{\"end\":11412,\"start\":10431},{\"end\":12781,\"start\":11414},{\"end\":13575,\"start\":12808},{\"end\":14699,\"start\":13595},{\"end\":15727,\"start\":14721},{\"end\":16597,\"start\":15729},{\"end\":17444,\"start\":16599},{\"end\":18165,\"start\":17589},{\"end\":18898,\"start\":18190},{\"end\":19833,\"start\":18920},{\"end\":20454,\"start\":19835},{\"end\":21541,\"start\":20466},{\"end\":21953,\"start\":21564},{\"end\":22886,\"start\":21979},{\"end\":23686,\"start\":22888},{\"end\":24493,\"start\":23688},{\"end\":26109,\"start\":24518},{\"end\":27051,\"start\":26134},{\"end\":27305,\"start\":27075},{\"end\":28142,\"start\":27342},{\"end\":28875,\"start\":28144},{\"end\":29758,\"start\":28887},{\"end\":30499,\"start\":29760},{\"end\":31797,\"start\":30531},{\"end\":32660,\"start\":31825},{\"end\":32995,\"start\":32662},{\"end\":33626,\"start\":32997},{\"end\":34067,\"start\":33628},{\"end\":34557,\"start\":34112},{\"end\":34901,\"start\":34559},{\"end\":35876,\"start\":34973},{\"end\":36375,\"start\":35925},{\"end\":37250,\"start\":36420},{\"end\":37702,\"start\":37287},{\"end\":37930,\"start\":37704},{\"end\":38317,\"start\":37932},{\"end\":39455,\"start\":38365},{\"end\":41713,\"start\":39494},{\"end\":41909,\"start\":41815},{\"end\":43101,\"start\":41911},{\"end\":43279,\"start\":43103},{\"end\":44282,\"start\":43318},{\"end\":45302,\"start\":44328},{\"end\":45496,\"start\":45304},{\"end\":46542,\"start\":45498},{\"end\":47696,\"start\":46590},{\"end\":48769,\"start\":47698},{\"end\":50269,\"start\":48810},{\"end\":50356,\"start\":50278}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17588,\"start\":17445}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18301,\"start\":18293},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27304,\"start\":27297},{\"end\":31104,\"start\":31097},{\"end\":36251,\"start\":36244},{\"end\":42664,\"start\":42657},{\"end\":42852,\"start\":42845},{\"end\":43737,\"start\":43730},{\"end\":44734,\"start\":44727},{\"end\":47726,\"start\":47719},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":48888,\"start\":48877},{\"end\":49820,\"start\":49813}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2192,\"start\":2180},{\"end\":8226,\"start\":8218},{\"end\":8242,\"start\":8229},{\"end\":8285,\"start\":8274},{\"attributes\":{\"n\":\"2.\"},\"end\":8404,\"start\":8392},{\"attributes\":{\"n\":\"3.\"},\"end\":12791,\"start\":12784},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12806,\"start\":12794},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13593,\"start\":13578},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14719,\"start\":14702},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18188,\"start\":18168},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18918,\"start\":18901},{\"attributes\":{\"n\":\"3.6.\"},\"end\":20464,\"start\":20457},{\"attributes\":{\"n\":\"4.\"},\"end\":21562,\"start\":21544},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21977,\"start\":21956},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24516,\"start\":24496},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26132,\"start\":26112},{\"attributes\":{\"n\":\"5.\"},\"end\":27073,\"start\":27054},{\"attributes\":{\"n\":\"5.1.\"},\"end\":27340,\"start\":27308},{\"attributes\":{\"n\":\"5.2.\"},\"end\":28885,\"start\":28878},{\"attributes\":{\"n\":\"5.3.\"},\"end\":30529,\"start\":30502},{\"attributes\":{\"n\":\"6.\"},\"end\":31823,\"start\":31800},{\"end\":34080,\"start\":34070},{\"end\":34110,\"start\":34083},{\"end\":34938,\"start\":34904},{\"end\":34971,\"start\":34941},{\"end\":35923,\"start\":35879},{\"end\":36418,\"start\":36378},{\"end\":37285,\"start\":37253},{\"end\":38363,\"start\":38320},{\"end\":39492,\"start\":39458},{\"end\":41754,\"start\":41716},{\"end\":41782,\"start\":41757},{\"end\":41813,\"start\":41785},{\"end\":43316,\"start\":43282},{\"end\":44326,\"start\":44285},{\"end\":46588,\"start\":46545},{\"end\":48808,\"start\":48772},{\"end\":50276,\"start\":50272},{\"end\":50368,\"start\":50358},{\"end\":51020,\"start\":51010},{\"end\":51216,\"start\":51206},{\"end\":51727,\"start\":51717},{\"end\":52045,\"start\":52037},{\"end\":52210,\"start\":52200},{\"end\":52745,\"start\":52736},{\"end\":53025,\"start\":53015},{\"end\":53197,\"start\":53187},{\"end\":53521,\"start\":53510},{\"end\":55895,\"start\":55890},{\"end\":69785,\"start\":69773}]", "table": "[{\"end\":54602,\"start\":54111},{\"end\":55338,\"start\":54703},{\"end\":55888,\"start\":55616},{\"end\":62969,\"start\":56644},{\"end\":69771,\"start\":64492},{\"end\":72182,\"start\":70418},{\"end\":74650,\"start\":73861}]", "figure_caption": "[{\"end\":51008,\"start\":50370},{\"end\":51204,\"start\":51022},{\"end\":51715,\"start\":51218},{\"end\":52035,\"start\":51729},{\"end\":52198,\"start\":52047},{\"end\":52446,\"start\":52212},{\"end\":52734,\"start\":52449},{\"end\":53013,\"start\":52748},{\"end\":53185,\"start\":53027},{\"end\":53508,\"start\":53199},{\"end\":54001,\"start\":53524},{\"end\":54111,\"start\":54004},{\"end\":54703,\"start\":54605},{\"end\":55616,\"start\":55341},{\"end\":56644,\"start\":55897},{\"end\":64492,\"start\":62972},{\"end\":70418,\"start\":69790},{\"end\":73861,\"start\":72185}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4389,\"start\":4381},{\"end\":8319,\"start\":8311},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14483,\"start\":14475},{\"end\":18997,\"start\":18989},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22636,\"start\":22628},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22925,\"start\":22917},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24894,\"start\":24886},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36087,\"start\":36079},{\"end\":38645,\"start\":38637},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":39635,\"start\":39627},{\"end\":45731,\"start\":45723},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49002,\"start\":48993},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49276,\"start\":49267}]", "bib_author_first_name": "[{\"end\":74697,\"start\":74696},{\"end\":74713,\"start\":74708},{\"end\":74941,\"start\":74935},{\"end\":74960,\"start\":74951},{\"end\":74972,\"start\":74967},{\"end\":74990,\"start\":74985},{\"end\":75493,\"start\":75487},{\"end\":75507,\"start\":75503},{\"end\":75519,\"start\":75513},{\"end\":75533,\"start\":75527},{\"end\":75548,\"start\":75543},{\"end\":75561,\"start\":75555},{\"end\":75580,\"start\":75571},{\"end\":75593,\"start\":75587},{\"end\":75612,\"start\":75606},{\"end\":75622,\"start\":75617},{\"end\":75624,\"start\":75623},{\"end\":76077,\"start\":76063},{\"end\":76086,\"start\":76082},{\"end\":76098,\"start\":76096},{\"end\":76111,\"start\":76104},{\"end\":76126,\"start\":76119},{\"end\":76361,\"start\":76353},{\"end\":76373,\"start\":76366},{\"end\":76384,\"start\":76378},{\"end\":76405,\"start\":76390},{\"end\":76659,\"start\":76653},{\"end\":76670,\"start\":76664},{\"end\":76682,\"start\":76675},{\"end\":76695,\"start\":76687},{\"end\":76703,\"start\":76700},{\"end\":76713,\"start\":76708},{\"end\":77088,\"start\":77082},{\"end\":77100,\"start\":77093},{\"end\":77109,\"start\":77105},{\"end\":77124,\"start\":77117},{\"end\":77136,\"start\":77129},{\"end\":77393,\"start\":77385},{\"end\":77406,\"start\":77399},{\"end\":77419,\"start\":77414},{\"end\":77435,\"start\":77430},{\"end\":77448,\"start\":77442},{\"end\":77461,\"start\":77457},{\"end\":77476,\"start\":77471},{\"end\":77495,\"start\":77485},{\"end\":77880,\"start\":77876},{\"end\":77893,\"start\":77886},{\"end\":77917,\"start\":77908},{\"end\":78290,\"start\":78284},{\"end\":78303,\"start\":78296},{\"end\":78314,\"start\":78308},{\"end\":78331,\"start\":78322},{\"end\":78341,\"start\":78337},{\"end\":78355,\"start\":78348},{\"end\":78369,\"start\":78362},{\"end\":78550,\"start\":78544},{\"end\":78560,\"start\":78556},{\"end\":78571,\"start\":78566},{\"end\":78882,\"start\":78876},{\"end\":78894,\"start\":78887},{\"end\":78906,\"start\":78901},{\"end\":78918,\"start\":78914},{\"end\":79227,\"start\":79218},{\"end\":79263,\"start\":79259},{\"end\":79277,\"start\":79271},{\"end\":79751,\"start\":79744},{\"end\":79770,\"start\":79766},{\"end\":79779,\"start\":79775},{\"end\":80121,\"start\":80112},{\"end\":80135,\"start\":80128},{\"end\":80398,\"start\":80395},{\"end\":80413,\"start\":80408},{\"end\":80427,\"start\":80421},{\"end\":80444,\"start\":80436},{\"end\":80461,\"start\":80452},{\"end\":80475,\"start\":80470},{\"end\":80489,\"start\":80486},{\"end\":80844,\"start\":80835},{\"end\":80862,\"start\":80854},{\"end\":80876,\"start\":80872},{\"end\":80892,\"start\":80888},{\"end\":80907,\"start\":80900},{\"end\":80918,\"start\":80915},{\"end\":80934,\"start\":80931},{\"end\":81293,\"start\":81290},{\"end\":81304,\"start\":81299},{\"end\":81614,\"start\":81608},{\"end\":81627,\"start\":81622},{\"end\":82048,\"start\":82042},{\"end\":82062,\"start\":82061},{\"end\":82078,\"start\":82072},{\"end\":82416,\"start\":82411},{\"end\":82435,\"start\":82426},{\"end\":82448,\"start\":82444},{\"end\":82464,\"start\":82457},{\"end\":82924,\"start\":82919},{\"end\":82939,\"start\":82931},{\"end\":82950,\"start\":82945},{\"end\":82964,\"start\":82957},{\"end\":82974,\"start\":82969},{\"end\":82992,\"start\":82986},{\"end\":83001,\"start\":83000},{\"end\":83411,\"start\":83404},{\"end\":83422,\"start\":83418},{\"end\":83434,\"start\":83427},{\"end\":83447,\"start\":83442},{\"end\":83456,\"start\":83453},{\"end\":83721,\"start\":83716},{\"end\":83735,\"start\":83726},{\"end\":83755,\"start\":83746},{\"end\":83770,\"start\":83763},{\"end\":83779,\"start\":83775},{\"end\":84028,\"start\":84021},{\"end\":84038,\"start\":84034},{\"end\":84054,\"start\":84049},{\"end\":84070,\"start\":84063},{\"end\":84082,\"start\":84075},{\"end\":84551,\"start\":84545},{\"end\":84567,\"start\":84557},{\"end\":84578,\"start\":84574},{\"end\":84588,\"start\":84583},{\"end\":84604,\"start\":84596},{\"end\":84905,\"start\":84896},{\"end\":84918,\"start\":84910},{\"end\":84929,\"start\":84924},{\"end\":84942,\"start\":84938},{\"end\":84945,\"start\":84943},{\"end\":84955,\"start\":84951},{\"end\":84959,\"start\":84956},{\"end\":85221,\"start\":85217},{\"end\":85231,\"start\":85226},{\"end\":85244,\"start\":85237},{\"end\":85259,\"start\":85251},{\"end\":85268,\"start\":85265},{\"end\":85451,\"start\":85447},{\"end\":85461,\"start\":85456},{\"end\":85470,\"start\":85467},{\"end\":85479,\"start\":85475},{\"end\":85493,\"start\":85486},{\"end\":85501,\"start\":85499}]", "bib_author_last_name": "[{\"end\":74706,\"start\":74698},{\"end\":74720,\"start\":74714},{\"end\":74724,\"start\":74722},{\"end\":74949,\"start\":74942},{\"end\":74965,\"start\":74961},{\"end\":74983,\"start\":74973},{\"end\":74999,\"start\":74991},{\"end\":75501,\"start\":75494},{\"end\":75511,\"start\":75508},{\"end\":75525,\"start\":75520},{\"end\":75541,\"start\":75534},{\"end\":75553,\"start\":75549},{\"end\":75569,\"start\":75562},{\"end\":75585,\"start\":75581},{\"end\":75604,\"start\":75594},{\"end\":75615,\"start\":75613},{\"end\":75631,\"start\":75625},{\"end\":76080,\"start\":76078},{\"end\":76094,\"start\":76087},{\"end\":76102,\"start\":76099},{\"end\":76117,\"start\":76112},{\"end\":76132,\"start\":76127},{\"end\":76364,\"start\":76362},{\"end\":76376,\"start\":76374},{\"end\":76388,\"start\":76385},{\"end\":76410,\"start\":76406},{\"end\":76662,\"start\":76660},{\"end\":76673,\"start\":76671},{\"end\":76685,\"start\":76683},{\"end\":76698,\"start\":76696},{\"end\":76706,\"start\":76704},{\"end\":76717,\"start\":76714},{\"end\":77091,\"start\":77089},{\"end\":77103,\"start\":77101},{\"end\":77115,\"start\":77110},{\"end\":77127,\"start\":77125},{\"end\":77140,\"start\":77137},{\"end\":77397,\"start\":77394},{\"end\":77412,\"start\":77407},{\"end\":77428,\"start\":77420},{\"end\":77440,\"start\":77436},{\"end\":77455,\"start\":77449},{\"end\":77469,\"start\":77462},{\"end\":77483,\"start\":77477},{\"end\":77503,\"start\":77496},{\"end\":77884,\"start\":77881},{\"end\":77906,\"start\":77894},{\"end\":77922,\"start\":77918},{\"end\":78294,\"start\":78291},{\"end\":78306,\"start\":78304},{\"end\":78320,\"start\":78315},{\"end\":78335,\"start\":78332},{\"end\":78346,\"start\":78342},{\"end\":78360,\"start\":78356},{\"end\":78375,\"start\":78370},{\"end\":78554,\"start\":78551},{\"end\":78564,\"start\":78561},{\"end\":78582,\"start\":78572},{\"end\":78885,\"start\":78883},{\"end\":78899,\"start\":78895},{\"end\":78912,\"start\":78907},{\"end\":78925,\"start\":78919},{\"end\":79257,\"start\":79228},{\"end\":79269,\"start\":79264},{\"end\":79283,\"start\":79278},{\"end\":79293,\"start\":79285},{\"end\":79764,\"start\":79752},{\"end\":79773,\"start\":79771},{\"end\":79788,\"start\":79780},{\"end\":79793,\"start\":79790},{\"end\":80126,\"start\":80122},{\"end\":80144,\"start\":80136},{\"end\":80406,\"start\":80399},{\"end\":80419,\"start\":80414},{\"end\":80434,\"start\":80428},{\"end\":80450,\"start\":80445},{\"end\":80468,\"start\":80462},{\"end\":80484,\"start\":80476},{\"end\":80499,\"start\":80490},{\"end\":80852,\"start\":80845},{\"end\":80870,\"start\":80863},{\"end\":80886,\"start\":80877},{\"end\":80898,\"start\":80893},{\"end\":80913,\"start\":80908},{\"end\":80929,\"start\":80919},{\"end\":80941,\"start\":80935},{\"end\":81297,\"start\":81294},{\"end\":81311,\"start\":81305},{\"end\":81620,\"start\":81615},{\"end\":81636,\"start\":81628},{\"end\":82059,\"start\":82049},{\"end\":82070,\"start\":82063},{\"end\":82087,\"start\":82079},{\"end\":82424,\"start\":82417},{\"end\":82442,\"start\":82436},{\"end\":82455,\"start\":82449},{\"end\":82470,\"start\":82465},{\"end\":82929,\"start\":82925},{\"end\":82943,\"start\":82940},{\"end\":82955,\"start\":82951},{\"end\":82967,\"start\":82965},{\"end\":82984,\"start\":82975},{\"end\":82998,\"start\":82993},{\"end\":83005,\"start\":83002},{\"end\":83011,\"start\":83007},{\"end\":83416,\"start\":83412},{\"end\":83425,\"start\":83423},{\"end\":83440,\"start\":83435},{\"end\":83451,\"start\":83448},{\"end\":83459,\"start\":83457},{\"end\":83724,\"start\":83722},{\"end\":83744,\"start\":83736},{\"end\":83761,\"start\":83756},{\"end\":83773,\"start\":83771},{\"end\":83788,\"start\":83780},{\"end\":83800,\"start\":83790},{\"end\":84032,\"start\":84029},{\"end\":84047,\"start\":84039},{\"end\":84061,\"start\":84055},{\"end\":84073,\"start\":84071},{\"end\":84085,\"start\":84083},{\"end\":84555,\"start\":84552},{\"end\":84572,\"start\":84568},{\"end\":84581,\"start\":84579},{\"end\":84594,\"start\":84589},{\"end\":84608,\"start\":84605},{\"end\":84908,\"start\":84906},{\"end\":84922,\"start\":84919},{\"end\":84936,\"start\":84930},{\"end\":84949,\"start\":84946},{\"end\":84963,\"start\":84960},{\"end\":84967,\"start\":84965},{\"end\":85224,\"start\":85222},{\"end\":85235,\"start\":85232},{\"end\":85249,\"start\":85245},{\"end\":85263,\"start\":85260},{\"end\":85271,\"start\":85269},{\"end\":85280,\"start\":85273},{\"end\":85454,\"start\":85452},{\"end\":85465,\"start\":85462},{\"end\":85473,\"start\":85471},{\"end\":85484,\"start\":85480},{\"end\":85497,\"start\":85494},{\"end\":85506,\"start\":85502}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b0\"},\"end\":74870,\"start\":74652},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195658208},\"end\":75395,\"start\":74872},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4492210},\"end\":75991,\"start\":75397},{\"attributes\":{\"doi\":\"arXiv:1908.03557\",\"id\":\"b3\"},\"end\":76351,\"start\":75993},{\"attributes\":{\"doi\":\"arXiv:2007.08273\",\"id\":\"b4\"},\"end\":76595,\"start\":76353},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":237054216},\"end\":77080,\"start\":76597},{\"attributes\":{\"doi\":\"arXiv:2007.08745\",\"id\":\"b6\"},\"end\":77340,\"start\":77082},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14113767},\"end\":77798,\"start\":77342},{\"attributes\":{\"id\":\"b8\"},\"end\":78245,\"start\":77800},{\"attributes\":{\"id\":\"b9\"},\"end\":78526,\"start\":78247},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12625409},\"end\":78802,\"start\":78528},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":868693},\"end\":79179,\"start\":78804},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11558223},\"end\":79662,\"start\":79181},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10328909},\"end\":80059,\"start\":79664},{\"attributes\":{\"doi\":\"arXiv:2105.10123\",\"id\":\"b14\"},\"end\":80393,\"start\":80061},{\"attributes\":{\"doi\":\"arXiv:1804.00792\",\"id\":\"b15\"},\"end\":80833,\"start\":80395},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b16\"},\"end\":81214,\"start\":80835},{\"attributes\":{\"doi\":\"arXiv:1908.07490\",\"id\":\"b17\"},\"end\":81485,\"start\":81216},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":12288917},\"end\":82040,\"start\":81487},{\"attributes\":{\"doi\":\"arXiv:2007.00711\",\"id\":\"b19\"},\"end\":82360,\"start\":82042},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1169492},\"end\":82837,\"start\":82362},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":67846878},\"end\":83332,\"start\":82839},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":133090840},\"end\":83712,\"start\":83334},{\"attributes\":{\"doi\":\"2019. 11\",\"id\":\"b23\"},\"end\":83957,\"start\":83714},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8485068},\"end\":84473,\"start\":83959},{\"attributes\":{\"id\":\"b25\"},\"end\":84798,\"start\":84475},{\"attributes\":{\"doi\":\"arXiv:2005.10987\",\"id\":\"b26\"},\"end\":85213,\"start\":84800},{\"attributes\":{\"id\":\"b27\"},\"end\":85401,\"start\":85215},{\"attributes\":{\"doi\":\"R-50 X-101 X-152 X-152++\",\"id\":\"b28\",\"matched_paper_id\":216552935},\"end\":85809,\"start\":85403}]", "bib_title": "[{\"end\":74933,\"start\":74872},{\"end\":75485,\"start\":75397},{\"end\":76651,\"start\":76597},{\"end\":77383,\"start\":77342},{\"end\":77874,\"start\":77800},{\"end\":78542,\"start\":78528},{\"end\":78874,\"start\":78804},{\"end\":79216,\"start\":79181},{\"end\":79742,\"start\":79664},{\"end\":81606,\"start\":81487},{\"end\":82409,\"start\":82362},{\"end\":82917,\"start\":82839},{\"end\":83402,\"start\":83334},{\"end\":84019,\"start\":83959},{\"end\":85445,\"start\":85403}]", "bib_author": "[{\"end\":74708,\"start\":74696},{\"end\":74722,\"start\":74708},{\"end\":74726,\"start\":74722},{\"end\":74951,\"start\":74935},{\"end\":74967,\"start\":74951},{\"end\":74985,\"start\":74967},{\"end\":75001,\"start\":74985},{\"end\":75503,\"start\":75487},{\"end\":75513,\"start\":75503},{\"end\":75527,\"start\":75513},{\"end\":75543,\"start\":75527},{\"end\":75555,\"start\":75543},{\"end\":75571,\"start\":75555},{\"end\":75587,\"start\":75571},{\"end\":75606,\"start\":75587},{\"end\":75617,\"start\":75606},{\"end\":75633,\"start\":75617},{\"end\":76082,\"start\":76063},{\"end\":76096,\"start\":76082},{\"end\":76104,\"start\":76096},{\"end\":76119,\"start\":76104},{\"end\":76134,\"start\":76119},{\"end\":76366,\"start\":76353},{\"end\":76378,\"start\":76366},{\"end\":76390,\"start\":76378},{\"end\":76412,\"start\":76390},{\"end\":76664,\"start\":76653},{\"end\":76675,\"start\":76664},{\"end\":76687,\"start\":76675},{\"end\":76700,\"start\":76687},{\"end\":76708,\"start\":76700},{\"end\":76719,\"start\":76708},{\"end\":77093,\"start\":77082},{\"end\":77105,\"start\":77093},{\"end\":77117,\"start\":77105},{\"end\":77129,\"start\":77117},{\"end\":77142,\"start\":77129},{\"end\":77399,\"start\":77385},{\"end\":77414,\"start\":77399},{\"end\":77430,\"start\":77414},{\"end\":77442,\"start\":77430},{\"end\":77457,\"start\":77442},{\"end\":77471,\"start\":77457},{\"end\":77485,\"start\":77471},{\"end\":77505,\"start\":77485},{\"end\":77886,\"start\":77876},{\"end\":77908,\"start\":77886},{\"end\":77924,\"start\":77908},{\"end\":78296,\"start\":78284},{\"end\":78308,\"start\":78296},{\"end\":78322,\"start\":78308},{\"end\":78337,\"start\":78322},{\"end\":78348,\"start\":78337},{\"end\":78362,\"start\":78348},{\"end\":78377,\"start\":78362},{\"end\":78556,\"start\":78544},{\"end\":78566,\"start\":78556},{\"end\":78584,\"start\":78566},{\"end\":78887,\"start\":78876},{\"end\":78901,\"start\":78887},{\"end\":78914,\"start\":78901},{\"end\":78927,\"start\":78914},{\"end\":79259,\"start\":79218},{\"end\":79271,\"start\":79259},{\"end\":79285,\"start\":79271},{\"end\":79295,\"start\":79285},{\"end\":79766,\"start\":79744},{\"end\":79775,\"start\":79766},{\"end\":79790,\"start\":79775},{\"end\":79795,\"start\":79790},{\"end\":80128,\"start\":80112},{\"end\":80146,\"start\":80128},{\"end\":80408,\"start\":80395},{\"end\":80421,\"start\":80408},{\"end\":80436,\"start\":80421},{\"end\":80452,\"start\":80436},{\"end\":80470,\"start\":80452},{\"end\":80486,\"start\":80470},{\"end\":80501,\"start\":80486},{\"end\":80854,\"start\":80835},{\"end\":80872,\"start\":80854},{\"end\":80888,\"start\":80872},{\"end\":80900,\"start\":80888},{\"end\":80915,\"start\":80900},{\"end\":80931,\"start\":80915},{\"end\":80943,\"start\":80931},{\"end\":81299,\"start\":81290},{\"end\":81313,\"start\":81299},{\"end\":81622,\"start\":81608},{\"end\":81638,\"start\":81622},{\"end\":82061,\"start\":82042},{\"end\":82072,\"start\":82061},{\"end\":82089,\"start\":82072},{\"end\":82426,\"start\":82411},{\"end\":82444,\"start\":82426},{\"end\":82457,\"start\":82444},{\"end\":82472,\"start\":82457},{\"end\":82931,\"start\":82919},{\"end\":82945,\"start\":82931},{\"end\":82957,\"start\":82945},{\"end\":82969,\"start\":82957},{\"end\":82986,\"start\":82969},{\"end\":83000,\"start\":82986},{\"end\":83007,\"start\":83000},{\"end\":83013,\"start\":83007},{\"end\":83418,\"start\":83404},{\"end\":83427,\"start\":83418},{\"end\":83442,\"start\":83427},{\"end\":83453,\"start\":83442},{\"end\":83461,\"start\":83453},{\"end\":83726,\"start\":83716},{\"end\":83746,\"start\":83726},{\"end\":83763,\"start\":83746},{\"end\":83775,\"start\":83763},{\"end\":83790,\"start\":83775},{\"end\":83802,\"start\":83790},{\"end\":84034,\"start\":84021},{\"end\":84049,\"start\":84034},{\"end\":84063,\"start\":84049},{\"end\":84075,\"start\":84063},{\"end\":84087,\"start\":84075},{\"end\":84557,\"start\":84545},{\"end\":84574,\"start\":84557},{\"end\":84583,\"start\":84574},{\"end\":84596,\"start\":84583},{\"end\":84610,\"start\":84596},{\"end\":84910,\"start\":84896},{\"end\":84924,\"start\":84910},{\"end\":84938,\"start\":84924},{\"end\":84951,\"start\":84938},{\"end\":84965,\"start\":84951},{\"end\":84969,\"start\":84965},{\"end\":85226,\"start\":85217},{\"end\":85237,\"start\":85226},{\"end\":85251,\"start\":85237},{\"end\":85265,\"start\":85251},{\"end\":85273,\"start\":85265},{\"end\":85282,\"start\":85273},{\"end\":85456,\"start\":85447},{\"end\":85467,\"start\":85456},{\"end\":85475,\"start\":85467},{\"end\":85486,\"start\":85475},{\"end\":85499,\"start\":85486},{\"end\":85508,\"start\":85499}]", "bib_venue": "[{\"end\":75150,\"start\":75084},{\"end\":76848,\"start\":76792},{\"end\":79436,\"start\":79374},{\"end\":81779,\"start\":81717},{\"end\":82613,\"start\":82551},{\"end\":84228,\"start\":84166},{\"end\":74694,\"start\":74652},{\"end\":75082,\"start\":75001},{\"end\":75673,\"start\":75633},{\"end\":76061,\"start\":75993},{\"end\":76451,\"start\":76428},{\"end\":76790,\"start\":76719},{\"end\":77185,\"start\":77158},{\"end\":77543,\"start\":77505},{\"end\":77996,\"start\":77924},{\"end\":78282,\"start\":78247},{\"end\":78644,\"start\":78584},{\"end\":78976,\"start\":78927},{\"end\":79372,\"start\":79295},{\"end\":79844,\"start\":79795},{\"end\":80110,\"start\":80061},{\"end\":80588,\"start\":80517},{\"end\":80998,\"start\":80958},{\"end\":81288,\"start\":81216},{\"end\":81715,\"start\":81638},{\"end\":82179,\"start\":82105},{\"end\":82549,\"start\":82472},{\"end\":83061,\"start\":83013},{\"end\":83506,\"start\":83461},{\"end\":84164,\"start\":84087},{\"end\":84543,\"start\":84475},{\"end\":84894,\"start\":84800},{\"end\":85601,\"start\":85532}]"}}}, "year": 2023, "month": 12, "day": 17}