{"id": 235702782, "updated": "2022-09-30 03:05:02.407", "metadata": {"title": "DeepLM: Large-scale Nonlinear Least Squares on Deep Learning Frameworks using Stochastic Domain Decomposition", "authors": "[{\"first\":\"Jingwei\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Shan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Mingwei\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We propose a novel approach for large-scale nonlinear least squares problems based on deep learning frameworks. Nonlinear least squares are commonly solved with the Levenberg-Marquardt (LM) algorithm for fast convergence. We implement a general and efficient LM solver on a deep learning framework by designing a new backward jacobian network to enable automatic sparse jacobian matrix computation. Furthermore, we introduce a stochastic domain decomposition approach that enables batched optimization and preserves convergence for large problems. We evaluate our method by solving bundle adjustment as a fundamental problem. Experiments show that our optimizer significantly outperforms the state-of-the-art solutions and existing deep learning solvers considering quality, efficiency, and memory. Our stochastic domain decomposition enables distributed optimization, consumes little memory and time, and achieves similar quality compared to a global solver. As a result, our solver effectively solves nonlinear least squares on an extremely large scale. Our code will be available based on Pytorch1 and Mindspore2.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/HuangHS21", "doi": "10.1109/cvpr46437.2021.01017"}}, "content": {"source": {"pdf_hash": "cf208b2a4927ad2249065d6d87f98ad775a822ec", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f02ab3902c2587efb3afe6a0a026b0ea15c4868d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cf208b2a4927ad2249065d6d87f98ad775a822ec.txt", "contents": "\nDeepLM: Large-scale Nonlinear Least Squares on Deep Learning Frameworks using Stochastic Domain Decomposition\n\n\nJingwei Huang \nRieman Lab\nHuawei Technologies\n\n\nShan Huang \nRieman Lab\nHuawei Technologies\n\n\nMingwei Sun \nRieman Lab\nHuawei Technologies\n\n\nWuhan University\n\n\nDeepLM: Large-scale Nonlinear Least Squares on Deep Learning Frameworks using Stochastic Domain Decomposition\n10.1109/CVPR46437.2021.01017\nWe propose a novel approach for large-scale nonlinear least squares problems based on deep learning frameworks. Nonlinear least squares are commonly solved with the Levenberg-Marquardt (LM) algorithm for fast convergence. We implement a general and efficient LM solver on a deep learning framework by designing a new backward jacobian network to enable automatic sparse jacobian matrix computation. Furthermore, we introduce a stochastic domain decomposition approach that enables batched optimization and preserves convergence for large problems. We evaluate our method by solving bundle adjustment as a fundamental problem. Experiments show that our optimizer significantly outperforms the state-of-the-art solutions and existing deep learning solvers considering quality, efficiency, and memory. Our stochastic domain decomposition enables distributed optimization, consumes little memory and time, and achieves similar quality compared to a global solver. As a result, our solver effectively solves nonlinear least squares on an extremely large scale. Our code will be available based on Pytorch 1 and Mindspore 2 .\n\nIntroduction\n\nNumerical optimization is an important stage in different problems across science and engineering. In computer vision and graphics, many problems are related to model fitting and can be formulated as nonlinear least squares including mesh processing [16,43,40,34], motion and model reconstruction [46,61,47,25,19], and bundle adjustment [49,36,3,54,35,57,58] as a fundamental problem in 3D reconstruction. Nonlinear least squares are widely solved using Levenberg-Marquardt (LM) algorithm [30,33] for its robustness and fast convergence. General solvers have been developed for academia and industry use like G2o [21] and Ceres [2]. However, they do not fit the increasing demand to efficiently handle extremely large problems with the availability of advanced hardware like GPUs.\n\nTypical algorithms are specifically designed for efficiency or scalability. PBA [54] uses GPUs to accelerate the optimization for bundle adjustment. [19] proposes local schur complement for facial performance capture. [57] segments the graph and achieves camera consensus with ADMM [7]. STBA [58] accelerates the step computation by solving smaller problems with a correction stage. However, these methods cannot handle problems with billions of residuals, potentially sacrifice quality, or do not handle general nonlinear least squares problems.\n\nRecently, deep learning frameworks like Tensorflow [1] and Pytorch [37] opens the potential to efficiently solve large-scale problems. It makes full use of computation resources like GPUs, provides an automatic backpropagation system to ease the user programming for general problems, and implements effective solvers for stochastic optimization. Unfortunately, they are not ideal for solving nonlinear least squares since gradient descent based optimizers converge much slower than commonly used LM algorithm. Therefore, we target at developing a stochastic LM optimizer inside deep learning frameworks to solve general and large nonlinear least squares with dramatic improvement in efficiency and scalability. Specifically, we mainly address two challenges for developing such an optimizer.\n\nFirst, backpropagation cannot be directly used to compute sparse jacobians required by the LM algorithm. While backpropagation provides the partial derivative of a scalar function to each variable, jacobian matrix require derivatives of all residuals to related variables. It is impractical to call backpropagation for each residual since it yields a dense jacobian matrix and consumes a huge amount of memory and time. We find that indices of variables for residuals specify an one-to-one correspondence between nonzero entries of the jacobian and the derivatives of a scalar function. Based on it, We design a novel backward jacobian network to compute this scalar function, call a single backpropagation to collect derivatives and recover the jacobian.\n\nSecond, while batch-based stochastic optimization handles large problems, it easily causes LM solver to diverge since LM steps are usually large and require accurate jacobians computation from all data. We find that domain decomposition [9] can avoid the divergence. This approach segments variables into different domains and alternatively optimizes each domain by fixing variables inside other domains. From the training perspective, we view domains as batches and alternative optimization as batched optimization with multiple epochs. However, such an approach reduces convergence quality especially for boundary variables between different domains. To achieve high convergence quality, we propose a novel stochastic domain decomposition approach. Different from traditional domain decomposition, we derive different segmentation of domains using stochastic clustering so that variables at the boundaries of domains are changing for each epoch. This dramatically enhances the convergence quality for the optimization. We further provide an optional global reinitialization step at the beginning of each epoch. We abstract each domain with a descriptor preserving intra-domain residuals and globally optimize all descriptors for different domains.\n\nIn our experiments, we mainly study a fundamental problem as bundle adjustment. Figure 1 shows a large-scale bundle adjustment problem solved using our stochastic domain decomposition. Our LM solver significantly outperforms existing solutions considering quality, efficiency, and memory. Experiments show that our stochastic approach converges as fast as a global LM solver, and significantly reduces the consumption of memory and time. Ablation studies show that the key contribution to quality comes from stochastic decomposition. We provide a general, efficient, and scalable stochastic LM solver on deep learning frameworks for large-scale nonlinear least squares.\n\n\nRelated Works\n\nNonlinear Least Squares Many problems [16,43,11,24] in computer vision and graphics require solving nonlinear least squares. Since these problems are usually considered sparse where each residual function takes a small number of variables, jacobian matrices can be efficiently evaluated and Levenberg-Marquardt (LM) algorithm [30,33] is practically one of the best solution. Here, the core challenge is to save time and memory to handle large-scale problems efficiently. Fast and dedicated LM solvers can be implemented in GPUs [54] for specific problems or on regular domains created by a domain-specific language like Opt [14]. Fast and simple relaxation methods are used to further accelerate the speed [50,51] sacrificing convergence quality. Local Schur complement [19] can further accelerate problems that can be decomposed into clusters with a small number of boundary variables. To save memory, matrixfree approaches [55,61] are proposed by deferring the evaluation of residual derivatives. However, these methods are still not suitable for handling extremely large scale problems [57] with limited memory. Our deep solver not only utilizes GPU acceleration but also incorporates a stochastic batch-optimization approach to handle a large scale.\n\nStochastic Solvers Deep learning frameworks are popularly used for training deep neural networks. They provide powerful stochastic solvers with automatic differentiation [32] so that users can focus on network function design. For example, Tensorflow [1] and Pytorch [37] supports first-order gradient-based solvers including SGD [6], RMSProp [23], AdaGrad [15] and Adam [27]. Besides, these gradient-based solvers are perfect candidates to handle extremely large data via stochastic optimization. However, available deep solvers are not as good as the LM algorithm for nonlinear least squares problems considering convergence speed. We design a backward jacobian computation network and enable the LM solver in deep learning frameworks to optimize nonlinear least squares.\n\nDomain Decomposition Domain decomposition [9,44,48] solves partial differential equations by iteratively solving subproblems in subdomains. Therefore, it reduces computation complexity and enhances parallelism and is a good candidate for large scale problems. Several recent methods tackle uncertainty across different domains by solving stochastic differential equations [39,5,56]. Different from their scenarios, our problem is nonlinear least squares without randomness but we introduce a stochastic clustering process during each epoch for high convergence quality.\n\nBundle Adjustment Bundle adjustment (BA) [49] is a fundamental problem in structure from motion [53,41] that requires solving nonlinear least squares. Recent works for this specific problem aim to achieve efficiency or a large scale. SBA [31] reduced the problem as pure a camera system using Schur complement. Sparse cholesky factorization approach is applied to the camera system with variable ordering [4,13,38]. Inexact solvers based on Conjugate Gradient (CG) [22] save memory and achieve better efficiency with the support of several different preconditioners [3,26,29]. SSBA [28] and PBA [54] designed efficient parallelized algorithms to accelerate the bundle adjustment. To handle a large scale, the problem is decomposed into clusters and solved in a divide-and-conquer fashion. [59,60,18] optimizes intra-cluster variables and merge clusters globally. [36,35] optimizes global separators and base nodes followed by intra-cluster optimization. Motion averaging [10] can deliver suboptimal results for extremely large-scale problems that do not require subpixel accuracy. Recent methods [17,57] exploits global optimization with distributed system using ADMM [7]. However, it requires careful hyper-parameter tuning and its inner iterations are time-consuming according to [58]. STBA [58] addressed point consensus inside the linear solver via constraint relaxation and correction. However, the correction step can still be inaccurate and prevent perfect convergence. Our solver exploits parallelism using deep learning frameworks and addresses the large scale with robustness and accuracy using novel stochastic domain decomposition.\n\n\nApproach\n\nWe begin by discussing some background knowledge in Section 3.1. We show our backward jacobian network in Section 3.2 and introduce our formulation of nonlinear least squares using stochastic domain decomposition in Section 3.3 and Figure 2. In Section 3.4, we describe the details for stochastic variable graph clustering.\n\n\nBackground\n\nNonlinear Least squares problems can be formulated as energy minimization in Equation 1:\nE(x) = 1 2 n i r i (x s i,1 , ..., x s i,k ) 2 .\n(1)\nx = (x 1 , ..., x m ) is an m-dimensional variable to be opti- mized. r = (r 1 , ...r n ) is an n-dimensional residual function depending on x. 1 \u2264 s i,j \u2264 m is the index of variable\nfor the j-th argument of i-th residual term r i . Additional requirements is made that s i,j is unique for each residual.\n\nOur goal is to minimize the squared norm of r. Newton step locally linearizes the residual function at x and find the optimal solution via\nmin x * ||r(x) + J r (x)(x * \u2212 x)|| 2 2 .\nAs a result, x is updated at each step as\nx * = x \u2212 H r (x) \u22121 J T r (x)r(x).\nSince hessian matrix H r (x) is expensive to compute, it is approximated with J T r (x)J r (x) in Gauss-Newton method, which converges when final residual is small or nearly affine around the optimal location. However, the linear approximation is inaccurate when a step size is big, and Gauss-Newton update does not guarantee to reduce the residual norm. Levenberg-Marquardt (LM) method adjusts the step size using a damping factor similar to Tikhonov regularization:\nmin x * ||r(x) + J r (x)(x * \u2212 x)|| 2 2 + \u03bb||D(x * \u2212 x)|| 2 2 .\nAs a result, a LM step can be computed by solving\n(J T r (x)J r (x) + \u03bbD T D)\u0394x = \u2212J T r (x)r(x) (2) where D = diag(J T r (x)J r (x))\n. \u03bb is used to control the step size and can be adjusted using the trust-region method [8].\n\n\nBackward Jacobian Network\n\nSolving Equation 2 with a general solver requires an automatic evaluation of the jacobian matrix. A common solution is forward differentiation with multi-dimensional dual numbers [20]. However, it requires users to explicitly specify variable dimensions and is non-trivial to integrate into a standard deep learning framework. On the other hand, automatic differentiation with backpropagation cannot directly apply to jacobian. While it computes derivatives of a scalar function. we require the derivatives of each residual term to related variables. Therefore, the number of required backpropagation equals the residual dimension n. Additionally, backpropagation stores gradients for all variables and a dense jacobian matrix with dimension n \u00d7 m will be created. This is impractical considering time and memory.\n\nOur intuition is that derivatives of different residuals to non-overlapping variables can be evaluated with a single backpropagation from the sum of residuals. However, it is still challenging to segment residuals to non-overlapping groups and collect derivatives as a compact linked-list representation [19].\n\nWe define a new variable y such that y i,j = x s i,j . We call y as \"indexed variables\" since it represents x indexed by s i,j in Equation 1. Our key idea is to collect derivatives of residuals to y instead of x to avoid overlapping issues. Since s i,j are unique for each residual r i , jacobians to the original variables can be computed as shown in Equation 3. We treat y as independent variables, and evaluate \u2207 y ( i r i (y)) with a single backpropagation. Afterwards, we use < i, s i,j , \u2207 Y ( i r i (Y)) (i,j) > to form a triplet representation for jacobian matrix. Detailed mathematical derivation is shown in Figure 3(a). Based on it, we design a backward jacobian network in Figure 3 (b). The input to the network is original variables x. We reindex the variables based on s to form indexed variables y. y are sent to residual functions provided by users to evaluate residuals r. Final loss L is computed as i r i . During backpropagation, we follow Figure 3(c) and propagate derivatives to y. Since each dimension of x corresponds to multiple r i with multiple derivatives, our design allows to store them at multiple locations y duplicated from x, and use (i, j) \u2192 (i, s i,j ) to map derivatives at y to the jacobian. Our method computes the jacobian with a single backpropagation. Assuming residual dimension is n and each residual is related to k variables, our time and space complexity as O(nk), while complexity of dual numbers [20] used by Ceres [2] is O(nk 2 ). Rather than a neural network for training, our jacobian network is an architec-ture for structuring derivatives as a jacobian. Supplemental material provides implementation details for the LM solver.\nJ r (x) (i,s i,j ) = J r (y) (i,j) = \u2207 y ( i r i (y)) (i,j) (3)\n\nStochastic Domain Decomposition\n\nFor large scale problems, memory cannot afford the storage of jacobian or even all residuals. While batchedoptimization can handle it via stochastic solvers, it easily causes the LM algorithm to diverge. We incorporate the idea of domain decomposition [9] to handle large-scale problems.\n\nFirst, we build a variable graph G =< V, E > where V = {x 1 , ...x m } and E contains all pairs of variables that appear in at least one common residual. Such a graph is shown in Figure 2(a). In Figure 2(b), we segment the variables into C clusters and assign a cluster label l i \u2264 C for each variable x i . We additionally ensure that variables with different labels won't appear in the same residual function unless either of them belongs to the first cluster. We call the first cluster the separators and other clusters the blocks. Instead of solving the global problem (Equation 1), we adopt the domain decomposition approach. We treat each cluster as a domain and solve variables in the same clusters alternatively by enumerating c from 1 to C in Equation 4.\nmin {x i |l i =c} 1 2 n i r i (x s i,1 , ..., x s i,k ) 2(4)\nSince separators ensures that different blocks do not share residuals, it enables us to optimize separators (Figure 2(c)) followed by other blocks (Figure 2(d)) in parallel. We run multiple epochs to optimize the whole problem shown in Figure 2(b-d). For each optimization, we use our backward jacobian network to compute jacobian and run the LM algorithm with tensor operations on a deep learning framework. One limitation in domain decomposition is that separator variables converges slowly. Therefore, we recluster the variables stochastically (details discussed in Section 3.4) so that the separators cluster is changing for each epoch. In addition, we optionally support user-specified global descriptors operated on blocks as a whole, requiring that such operations do not change intra-block residuals. We jointly optimize global descriptors together with separators in Figure 2(b), following Equation 5.\nmin {x i ,t j |l i =1} 1 2 i r i (t l s i,1 \u2022 x s i,1 , ..., t l s i,1 \u2022 x s i,k ) 2 (5)\nt j is the global descriptor for cluster j that operates on all variables in j-th cluster, and \u2022 is an operator invariant to intra-block residuals specified by the user. For example, in bundle adjustment or as rigid as possible deformation problems, \u2022 can be defined as a rigid transformation operator and t j represents rigid transformation with six parameters. Such global descriptors optimization leads to an effective global reinitialization over the whole graph structure. If such a global descriptor is not clear, we fix t j \u2022 as identity operations so that the problem is reduced to Equation 4. The concepts of separators, blocks, and global descriptor optimizations are extended from HyperSFM [35] for general nonlinear least squares with a general graph structure. Different from HyperSFM [35] who executes a global descriptor optimization followed by internal block optimization with a single epoch, we view it as a domain decomposition problem, execute multiple epochs, and rebuild domains by stochastic clustering every iteration. Furthermore, we only execute a single LM step for Equation 4 or 5 to save computation time for inner loops. Such a simplification achieves much better convergence than HyperSFM [35] with our stochastic domain decomposition.\n\n\nStochastic Variable Graph\n\nFor clustering, we aim at segmenting the variables evenly to achieve maximum parallelism. Second, we want to minimize the size of separators. Finally, we hope to recluster the problem stochastically so that the separators are changing for each epoch. The graph clustering problem has standard solutions including normalized cuts [42] used by [36] or spectral clustering [45] used by [19]. However, these methods produce fixed clusters and are inefficient for handling very large data. We adopt the solution provided by [58] for stochastic graph clustering. Specifically, each variable is initialized as a cluster, and clusters are greedily merged to maximize modularity with certain probabilistic distribution [12]. Different from [58], we minimize the size of separators to ensure fast convergence. Therefore, we prioritize to merge pairs of clusters with the maximum number of edges connecting them so that the number of edges accross different clusters are minimized. Accordingly, we define modularity as the number of edges in our variable graph internal to the same clusters. Clusters are merged following [58] (Equation 16) where Q is redefined using our modularity. We stop merging pairs if the number of variables in the merged cluster is larger than a threshold. After this step, we build the separator cluster so that blocks do not share residuals. In detail, for any edge in the variable graph that connects different blocks, we randomly pick one vertex of the edge and move it to the separator cluster. Figure 4 illustrates segmentation in an example of a bundle adjustment problem. We segment cameras into different clusters, each labeled with a different color. Three different segmentation results are obtained by our algorithm for different epochs. According to the statistics in Table 1, the number of blocks and elements in the separator set from our formulation is less than the original formulation in [58] under the same constraint of maximum block size.\n\n\nEvaluation\n\n\nResults\n\nBased on our backward jacobian network, we implement the LM solver on Pytorch where the linear equation is solved with preconditioned conjugate gradient [29]. We perform experiments on the bundle adjustment problems provided by datasets including 1DSFM [52] and BAL [3]. We use initialized bundle adjustment problems from BAL [3], and obtain the initialized problems for 1DSFM using Colmap [41]. We run experiments on machines with a Quadro P5000 GPU and a 16-core 3.7GHz CPU. We compare our methods with Ceres [2] using exact sparse linear solver (Ceres-S) and conjugate gradient (Ceres-CG). We additionally compare our methods with PBA [54] known as one of the most efficient solvers for bundle adjustment, and H-SFM [35] and STBA [58] as two state-of-the-art block-wise solvers for large-scale problems.  For our methods, we report \"Ours-G\" as our Pytorch-based LM solver that optimizes the problem globally as a whole. \"Ours-BG\" and \"Ours-B\" represent stochastic domain composition using our whole pipeline with and without global reinitialization (Equation 5). For all scenes, we segment the global problem into 17 clusters and report a single run with 20 epochs. Problems solved by \"Ours-BG\" are visualized in Figure 5, where green frames represent cameras poses and point colors represent their 3D coordinates.\n\nQuality We report mean squared error (MSE) after optimization to measure the optimization quality of different methods. We list the statistics of problems and MSE obtained from different methods in Table 2. Cells marked as \"-\" if optimization fails or running time is over 2 hours. Ceres-S is global optimization with an exact linear solver and usually delivers the best quality. Compared with other inexact solvers or blocked solvers, our global solver (Ours-G) achieves the best quality (marked in bold). It yields quite close or even better results compared to Ceres-S for several problems. Our stochastic solver \"Ours-BG\" and \"Ours-B\" achieves quite close quality compared to \"Ours-G\", and outperforms other state-of-the-art block solvers including H-SFM [35] and STBA [58].\n\nEfficiency and Memory We report the time (second) and memory (GB) usage in Table 3 and Table 4 for BAL [3] dataset. Additional statistics for 1DSFM [52] are pro-  Figure 6. Convergence curves for different scenes in 1DSFM [52] and BAL [3].\n\nvided in the supplemental material. Among existing global solvers, PBA [54] on GPU is faster than other algorithms run in multicore CPU implementation. Ours-G is even faster than PBA. We find that the major difference comes from accumulative summation inside J T r. We directly call an efficient \"index add \" in Pytorch while PBA assign a thread to each variable x i to collect k J k,i r k , which is less efficient due to additional memory loading and uneven distribution of number of residuals related to different variables. As a result, our implementation can be more than 2x faster than PBA's kernel. Figure 6 plots the convergence curve for several problems in BAL [3] and 1DSFM [52] datasets. \"Ours-G\" is our solver shown in blue, which converges faster than other state-of-the-art methods. With careful implementation, \"Ours-G\" uses less memory than other existing solvers. Typically, we use less memory to compute jacobians with our backward jacobian network. By solving subproblems in parallel for \"Ours-   Table 5. City-scale dataset. We are able to process more than one billion residual terms by segmenting the data to 36 blocks and finalize the optimization with around 6 minutes.\n\nBG\" and \"Ours-B\", we significantly reduce the computation time and maximum memory usage for each sub-problem. Therefore, our stochastic solver supports the optimization of problems on a very large scale. \"Ours-B\" is more efficient than \"Ours-BG\" considering time and memory since it does not perform global reinitialization by slightly sacrificing the quality. In practice, either of them can be used depending on whether the application prefers quality or speed. Figure 7 shows a large bundle adjustment problem with over one hundred thousand images solved with \"Ours-B\". Detailed statistics are reported in Table 5.\n\n\nLarge Scale\n\nWe are able to process more than one billion residual terms in our self-collected dataset by segmenting the data to 36 blocks and finalize the optimization with around 6 minutes.\n\nDeep Learning Solvers We compare our LM solver with other gradient-based optimizers in deep learning frameworks as shown in Figure 8. LM solvers converge much faster with better quality compared to SGD [6], RM-SProp [23], and Adam [27].\n\n\nAblation Study\n\nBall Experiment To illustrate our intuition of why stochastic domain decomposition helps optimization, we consider a simple ball experiment in Equation 6 with its physical meaning illustrated in Figure 9(a) and (b). We have Iterations MSE (pixel) Figure 8. Convergence curve on Trafalgar dataset provided by BAL [3] using our method and deep learning solvers including SGD [6], RMSProp [23], and Adam [27]. N = 100 balls whose heights to the ground are x i . We optimize their heights so that each ball is close to both the ground and the mid point of its two neighbors.\nE(x) = n\u22121 i=2 x 2 i + (x i \u2212 x i\u22121 + x i+1 2 ) 2(6)\nFor traditional domain decomposition, we separate variables into two domains by cutting them in the middle and optimize both domains alternatively. The loss over iterations is shown as the red curve in Figure 9(c). We additionally visualize the results at the first seven iterations in Figure 9(d) (opaque value increases with the number of iterations). The convergence is slow especially for the separator in the middle. To change the separator, we split the domain at 25 \u00d7 (i mod 3 + 1) for the i-th iteration. The convergence curve and optimization states for the first seven iterations are shown in Figure 9(c) and (e) as the blue line. As a result, optimization converges much faster when we change the separator every epoch. By randomly pick the separator for each iteration, we derive the green curves in Figure 9(c) and (f), which shows similar convergence quality. Finally, with our global reinitialization as shown in the purple curve in Figure 9(c), convergence can be further enhanced.\n\nBundle Adjustment We plot the convergence curve for different variants of our methods in Figure 10(a) for the Ladybug scene in BAL [3] dataset. \"Ours-G\", \"Ours-BG\" and \"Ours-B\" are discussed in Section 4.1 as the global optimization, stochastic domain decomposition with and without global reinitialization. \"Ours-BG-Fixed\" and \"Ours-B-Fixed\" represent traditional domain decomposition where clusters are fixed over epochs with and without global reinitialization. \"Ours-BG\" and \"Ours-B\" converge quite close to our global optimization, while convergence is worse with traditional domain decomposition. We plot the ratio of loss to the global optimization using \"Ours-B\" with different numbers of clusters for several bundle adjustment problems in Figure 10(b). The optimization quality is not sensitive to the number of clusters. We observe that for \"Ladybug\" and \"Dubrovnik\", optimization loss increases when the number of blocks is set to be 128 and 256. We believe the reason is due to too few cameras inside each cluster. For example, by segmenting the graph into 256 blocks, each block contains at most 2 cameras for the Dubrovnik scene. Therefore, the loss increase does not appear in larger scenes like \"Final\" and \"Trafalgar\".\n\n\nConclusion\n\nWe develop an efficient solution for large-scale nonlinear least squares problems based on deep learning frameworks. Our backward jacobian network enables our implementation of LM solver in Pytorch, and our stochastic domain decomposition method helps us robustly solve very largescale problems with fast convergence.\n\nFigure 1 .\n1Large-scale bundle adjustment on a deep learning framework using stochastic domain decomposition. We solve problems on a very large scale with high convergence quality.\n\nFigure 2 .Figure 3 .\n23In each epoch, we stochastically recluster the variables into C \u2212 1 blocks and one separators cluster. We optimize separators and then variables in different blocks in parallel. With our backward jacobian network, we develop a LM solver and solve each optimization in a deep learning framework. We run multiple epochs to solve the problem. We treat the indexed variables as independent variables and collect derivatives of residual sums to these variables with a single backpropagation. Derivatives and variable indices form a triplet representation of the sparse jacobian.\n\nFigure 4 .\n4Stochastic segmentation from an example of a bundle adjustment problem. We segment cameras into different clusters, each labeled with a different color. Three different segmentation results are obtained by our algorithm for different epochs.\n\nFigure 5 .\n5Visualization of problems from BAL[3] (first row) and 1DSFM[52] (second row) solved using our full pipeline \"Ours-BG\". Green frames represent cameras poses and point colors represent their 3D coordinates.\n\nFigure 7 .\n7Visualization of city-scale bundle adjustment results processed using \"Ours-B\". Colors encode the z-values of 3D points.\n\nFigure 9 .\n9Illustration of the ball experiment. (a) We have N balls whose heights to the ground are x i . (b) We optimize their heights so that each ball is close to both the ground and the mid point of the neighboring balls. (c) is the convergence curve of the loss to the number of iterations. (d)-(f) are shapes from the first 7 iterations over the optimization by different methods.\n\nFigure 10 .\n10(a) Convergence curves of different variants of our methods for Bundle adjustment. Stochastic solvers (\"Ours-BG\" and \"Ours-B\") outperform fixed domain decomposition and achieve quite similar quality comparing to our global optimization. (b) Error ratio to the global solver using \"Ours-B\" with different numbers of blocks.\n\n\nDataset Method # Cameras # Blocks # SeparatorLadybug \nSTBA \n106 \n18 \n131209 \nOurs-B \n106 \n16 \n60898 \n\nFinal \nSTBA \n427 \n37 \n4156614 \nOurs-B \n427 \n32 \n3177896 \n\nTable 1. Stochastic clustering statistics. Using different modular-\nity, we produce less separators and blocks than STBA [58] under \nthe same constraint of maximum block size. \n\n\n\n\nStatistics and mean squared errors produced by different methods for scenes in BAL[3] and 1DSFM[52] dataset.Dataset \n# Cameras # Points # Projections Ceres-S Ceres-CG PBA H-SFM STBA Ours-G Ours-BG Ours-B \nTrafalgar \n257 \n65132 \n225911 \n0.855 \n0.862 \n1.913 3.423 1.020 0.858 \n0.861 \n0.892 \nLadybug \n1723 \n156502 \n678718 \n1.143 \n1.223 \n2.229 2.372 1.307 1.121 \n1.102 \n1.135 \nDubrovnik \n356 \n226730 \n1255268 \n0.787 \n0.788 \n1.899 2.325 0.963 0.787 \n0.788 \n0.813 \nVenice \n1778 \n993923 \n5001946 \n0.660 \n0.664 \n-\n2.158 0.775 0.662 \n0.664 \n0.701 \nFinal \n13682 \n4456117 \n28987644 \n-\n1.587 \n3.004 3.394 \n-\n1.501 \n1.505 \n1.524 \nUnion Square \n647 \n13368 \n108083 \n3.000 \n1.015 \n1.781 2.635 3.268 0.826 \n0.827 \n0.831 \nP. del Popolo \n404 \n14128 \n128940 \n0.957 \n0.959 \n1.863 3.541 1.072 0.959 \n0.960 \n0.963 \nEllis Island \n367 \n20355 \n137450 \n0.942 \n0.942 \n2.300 3.338 1.105 0.942 \n0.944 \n0.949 \nNYC Library \n491 \n21396 \n149720 \n1.007 \n1.010 \n2.363 3.902 \n-\n1.009 \n1.011 \n1.013 \nM. N. Dame \n547 \n33830 \n272575 \n1.058 \n1.057 \n2.553 4.001 1.176 1.058 \n1.059 \n1.065 \nGen. markt \n905 \n43620 \n280971 \n0.881 \n0.883 \n2.192 3.102 1.017 0.880 \n0.883 \n0.886 \nAlamo \n741 \n31203 \n308147 \n0.971 \n0.970 \n2.138 3.081 1.025 0.968 \n0.971 \n0.972 \nYorkminster \n910 \n50947 \n333902 \n0.856 \n0.861 \n1.782 3.269 0.988 0.840 \n0.843 \n0.848 \nRoman Forum \n1368 \n61008 \n435531 \n0.909 \n0.906 \n1.952 3.065 1.042 0.906 \n0.906 \n0.907 \nV. Cathedral \n1016 \n56266 \n459014 \n0.913 \n0.895 \n2.168 3.043 1.033 0.893 \n0.895 \n0.899 \nM. Metropolis \n524 \n86364 \n594848 \n0.459 \n0.436 \n0.981 1.331 0.523 0.438 \n0.438 \n0.440 \nPiccadily \n2918 \n104027 \n823221 \n0.963 \n0.951 \n2.224 3.842 7.896 0.950 \n0.950 \n0.954 \nT. of London \n814 \n185579 \n1512167 \n0.321 \n0.335 \n0.670 1.079 0.443 0.305 \n0.303 \n0.308 \nTrafalgar \n7792 \n216650 \n1924901 \n1.377 \n1.392 \n3.267 5.192 1.641 1.368 \n1.370 \n1.374 \nTable 2. \nhttps://github.com/hjwdzh/DeepLM 2 https://gitee.com/mindspore/mindspore/tree/ master/model_zoo/research/3d/DeepLM\n\nTensorflow: A system for large-scale machine learning. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16). 1Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe- mawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16), pages 265-283, 2016. 1, 2\n\nCeres solver. Sameer Agarwal, Keir Mierle, Sameer Agarwal, Keir Mierle, et al. Ceres solver. 2012. 1, 4, 5\n\nBundle adjustment in the large. Sameer Agarwal, Noah Snavely, M Steven, Richard Seitz, Szeliski, European conference on computer vision. Springer7Sameer Agarwal, Noah Snavely, Steven M Seitz, and Richard Szeliski. Bundle adjustment in the large. In Euro- pean conference on computer vision, pages 29-42. Springer, 2010. 1, 3, 5, 6, 7, 8\n\nAn approximate minimum degree ordering algorithm. Timothy A Patrick R Amestoy, Iain S Davis, Duff, SIAM Journal on Matrix Analysis and Applications. 174Patrick R Amestoy, Timothy A Davis, and Iain S Duff. An approximate minimum degree ordering algorithm. SIAM Journal on Matrix Analysis and Applications, 17(4):886- 905, 1996. 3\n\nStochastic domain decomposition for time dependent adaptive mesh generation. Alexander Bihlo, D Ronald, Emily J Haynes, Walsh, arXiv:1504.00084arXiv preprintAlexander Bihlo, Ronald D Haynes, and Emily J Walsh. Stochastic domain decomposition for time dependent adap- tive mesh generation. arXiv preprint arXiv:1504.00084, 2015. 2\n\nLarge-scale machine learning with stochastic gradient descent. L\u00e9on Bottou, Proceedings of COMPSTAT'2010. COMPSTAT'2010SpringerL\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010, pages 177-186. Springer, 2010. 2, 7, 8\n\nDistributed optimization and statistical learning via the alternating direction method of multipliers. Stephen Boyd, Neal Parikh, Eric Chu, Now Publishers Inc13Stephen Boyd, Neal Parikh, and Eric Chu. Distributed opti- mization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011. 1, 3\n\nA trust region method based on interior point techniques for nolinear programming. H Richard, Jean Charles Byrd, Jorge Gilbert, Nocedal, 89Richard H Byrd, Jean Charles Gilbert, and Jorge Nocedal. A trust region method based on interior point techniques for nolinear programming. 89(1):149-185, 2000. 3\n\nDomain decomposition algorithms. Tony F Chan, P Tarek, Mathew, Acta numerica. 314Tony F Chan, Tarek P Mathew, et al. Domain decomposition algorithms. Acta numerica, 3(1):61-143, 1994. 2, 4\n\nEfficient and robust large-scale rotation averaging. Avishek Chatterjee, Venu Madhav Govindu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAvishek Chatterjee and Venu Madhav Govindu. Efficient and robust large-scale rotation averaging. In Proceedings of the IEEE International Conference on Computer Vision, pages 521-528, 2013. 3\n\nRobust reconstruction of indoor scenes. Sungjoon Choi, Qian-Yi Zhou, Vladlen Koltun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun. Ro- bust reconstruction of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 5556-5565, 2015. 2\n\nMcmc louvain for online community detection. Yves Darmaillac, S\u00e9bastien Loustau, arXiv:1612.01489arXiv preprintYves Darmaillac and S\u00e9bastien Loustau. Mcmc lou- vain for online community detection. arXiv preprint arXiv:1612.01489, 2016. 5\n\nA column approximate minimum degree ordering algorithm. A Timothy, John R Davis, Stefan I Gilbert, Esmond G Larimore, Ng, ACM Transactions on Mathematical Software (TOMS). 303Timothy A Davis, John R Gilbert, Stefan I Larimore, and Es- mond G Ng. A column approximate minimum degree order- ing algorithm. ACM Transactions on Mathematical Software (TOMS), 30(3):353-376, 2004. 3\n\nOpt: A domain specific language for non-linear least squares optimization in graphics and imaging. Zachary Devito, Michael Mara, Michael Zollh\u00f6fer, Gilbert Bernstein, Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher, Matthias Niessner, ACM Transactions on Graphics (TOG). 365Zachary DeVito, Michael Mara, Michael Zollh\u00f6fer, Gilbert Bernstein, Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher, and Matthias Niessner. Opt: A domain specific language for non-linear least squares opti- mization in graphics and imaging. ACM Transactions on Graphics (TOG), 36(5):1-27, 2017. 2\n\nAdaptive subgradient methods for online learning and stochastic optimization. John Duchi, Elad Hazan, Yoram Singer, Journal of machine learning research. 127John Duchi, Elad Hazan, and Yoram Singer. Adaptive sub- gradient methods for online learning and stochastic opti- mization. Journal of machine learning research, 12(7), 2011. 2\n\nDivergence-free shape correspondence by deformation. Marvin Eisenberger, Zorah L\u00e4hner, Daniel Cremers, Computer Graphics Forum. Wiley Online Library38Marvin Eisenberger, Zorah L\u00e4hner, and Daniel Cremers. Divergence-free shape correspondence by deformation. In Computer Graphics Forum, volume 38, pages 1-12. Wiley Online Library, 2019. 1, 2\n\nTat-Jun Chin, and Mats Isaksson. A consensus-based framework for distributed bundle adjustment. Anders Eriksson, John Bastian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAnders Eriksson, John Bastian, Tat-Jun Chin, and Mats Isaksson. A consensus-based framework for distributed bun- dle adjustment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1754- 1762, 2016. 3\n\nMergesfm: Merging partial reconstructions. Meiling Fang, Thomas Pollok, Qum Chengchao, BMVC. 29Meiling Fang, Thomas Pollok, and Qum Chengchao. Merge- sfm: Merging partial reconstructions. In BMVC, page 29, 2019. 3\n\nFast nonlinear least squares optimization of large-scale semi-sparse problems. Marco Fratarcangeli, Derek Bradley, Aurel Gruber, Gaspard Zoss, Thabo Beeler, Computer Graphics Forum. Wiley Online Library39Marco Fratarcangeli, Derek Bradley, Aurel Gruber, Gaspard Zoss, and Thabo Beeler. Fast nonlinear least squares opti- mization of large-scale semi-sparse problems. In Computer Graphics Forum, volume 39, pages 247-259. Wiley Online Library, 2020. 1, 2, 3, 5\n\nEvaluating derivatives: principles and techniques of algorithmic differentiation. SIAM. Andreas Griewank, Andrea Walther, 34Andreas Griewank and Andrea Walther. Evaluating deriva- tives: principles and techniques of algorithmic differentia- tion. SIAM, 2008. 3, 4\n\ng2o: A general framework for (hyper) graph optimization. Giorgio Grisetti, Rainer K\u00fcmmerle, Hauke Strasdat, Kurt Konolige, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). the IEEE International Conference on Robotics and Automation (ICRA)Shanghai, ChinaGiorgio Grisetti, Rainer K\u00fcmmerle, Hauke Strasdat, and Kurt Konolige. g2o: A general framework for (hyper) graph optimization. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, pages 9-13, 2011. 1\n\nMethods of conjugate gradients for solving linear systems. Eduard Magnus R Hestenes, Stiefel, Journal of research of the National Bureau of Standards. 496Magnus R Hestenes, Eduard Stiefel, et al. Methods of conju- gate gradients for solving linear systems. Journal of research of the National Bureau of Standards, 49(6):409-436, 1952. 3\n\nNeural networks for machine learning lecture 6a overview of mini-batch gradient descent. Geoffrey Hinton, Nitish Srivastava, Kevin Swersky, Cited on. 148Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8), 2012. 2, 7, 8\n\n6-dof vr videos with a single 360-camera. Jingwei Huang, Zhili Chen, Duygu Ceylan, Hailin Jin, 2017 IEEE Virtual Reality (VR). Jingwei Huang, Zhili Chen, Duygu Ceylan, and Hailin Jin. 6-dof vr videos with a single 360-camera. In 2017 IEEE Virtual Reality (VR), pages 37-44. IEEE, 2017. 2\n\n3dlite: towards commodity 3d scanning for content creation. Jingwei Huang, Angela Dai, Leonidas J Guibas, Matthias Nie\u00dfner, ACM Trans. Graph. 366Jingwei Huang, Angela Dai, Leonidas J Guibas, and Matthias Nie\u00dfner. 3dlite: towards commodity 3d scanning for content creation. ACM Trans. Graph., 36(6):203-1, 2017. 1\n\nPushing the envelope of modern methods for bundle adjustment. Yekeun Jeong, David Nister, Drew Steedly, Richard Szeliski, In-So Kweon, IEEE transactions on pattern analysis and machine intelligence. 34Yekeun Jeong, David Nister, Drew Steedly, Richard Szeliski, and In-So Kweon. Pushing the envelope of modern methods for bundle adjustment. IEEE transactions on pattern analy- sis and machine intelligence, 34(8):1605-1617, 2011. 3\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 2, 7, 8\n\nSparse sparse bundle adjustment. Kurt Konolige, Willow Garage, BMVC. 103Kurt Konolige and Willow Garage. Sparse sparse bundle adjustment. In BMVC, volume 10, pages 102-1, 2010. 3\n\nVisibility based preconditioning for bundle adjustment. Avanish Kushal, Sameer Agarwal, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE35Avanish Kushal and Sameer Agarwal. Visibility based pre- conditioning for bundle adjustment. In 2012 IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 1442-1449. IEEE, 2012. 3, 5\n\nA method for the solution of certain non-linear problems in least squares. Kenneth Levenberg, Quarterly of applied mathematics. 22Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of applied mathematics, 2(2):164-168, 1944. 1, 2\n\nSba: A software package for generic sparse bundle adjustment. I A Manolis, Antonis A Lourakis, Argyros, ACM Transactions on Mathematical Software (TOMS). 361Manolis IA Lourakis and Antonis A Argyros. Sba: A soft- ware package for generic sparse bundle adjustment. ACM Transactions on Mathematical Software (TOMS), 36(1):1- 30, 2009. 3\n\nA practical bayesian framework for backpropagation networks. J C David, Mackay, Neural Computation. 2David J. C. Mackay. A practical bayesian framework for backpropagation networks. Neural Computation, 1992. 2\n\nAn algorithm for least-squares estimation of nonlinear parameters. W Donald, Marquardt, Journal of the society for Industrial and Applied Mathematics. 112Donald W Marquardt. An algorithm for least-squares esti- mation of nonlinear parameters. Journal of the society for Industrial and Applied Mathematics, 11(2):431-441, 1963. 1, 2\n\nNonlinear shape optimization using local subspace projections. Przemyslaw Musialski, Christian Hafner, Florian Rist, Michael Birsak, Michael Wimmer, Leif Kobbelt, ACM Transactions on Graphics (TOG). 354Przemyslaw Musialski, Christian Hafner, Florian Rist, Michael Birsak, Michael Wimmer, and Leif Kobbelt. Non- linear shape optimization using local subspace projections. ACM Transactions on Graphics (TOG), 35(4):1-13, 2016. 1\n\nHypersfm. Georgia Institute of Technology. Kai Ni, Frank Dellaert, Kai Ni and Frank Dellaert. Hypersfm. Georgia Institute of Technology, 2012. 1, 3, 5, 6\n\nOut-of-core bundle adjustment for large-scale 3d reconstruction. Kai Ni, Drew Steedly, Frank Dellaert, IEEE 11th International Conference on Computer Vision. IEEE15Kai Ni, Drew Steedly, and Frank Dellaert. Out-of-core bun- dle adjustment for large-scale 3d reconstruction. In 2007 IEEE 11th International Conference on Computer Vision, pages 1-8. IEEE, 2007. 1, 3, 5\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 1Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026-8037, 2019. 1, 2\n\nThe design and implementation of a new out-of-core sparse cholesky factorization method. Vladimir Rotkin, Sivan Toledo, ACM Transactions on Mathematical Software (TOMS). 301Vladimir Rotkin and Sivan Toledo. The design and imple- mentation of a new out-of-core sparse cholesky factoriza- tion method. ACM Transactions on Mathematical Software (TOMS), 30(1):19-46, 2004. 3\n\nDomain decomposition of stochastic pdes: theoretical formulations. Abhijit Sarkar, Nabil Benabbou, Roger Ghanem, International Journal for Numerical Methods in Engineering. 775Abhijit Sarkar, Nabil Benabbou, and Roger Ghanem. Do- main decomposition of stochastic pdes: theoretical formula- tions. International Journal for Numerical Methods in Engi- neering, 77(5):689-701, 2009. 2\n\nGraph partitioning for high performance scientific simulations. Army High Performance Computing Research Center. Kirk Schloegel, George Karypis, Vipin Kumar, Kirk Schloegel, George Karypis, and Vipin Kumar. Graph partitioning for high performance scientific simulations. Army High Performance Computing Research Center, 2000. 1\n\nStructurefrom-motion revisited. L Johannes, Jan-Michael Schonberger, Frahm, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition25Johannes L Schonberger and Jan-Michael Frahm. Structure- from-motion revisited. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 4104-4113, 2016. 2, 5\n\nNormalized cuts and image segmentation. Jianbo Shi, Jitendra Malik, IEEE Transactions. 228Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888-905, 2000. 5\n\nGeometric optimization via composite majorization. Anna Shtengel, Roi Poranne, Olga Sorkine-Hornung, Z Shahar, Yaron Kovalsky, Lipman, ACM Trans. Graph. 364Anna Shtengel, Roi Poranne, Olga Sorkine-Hornung, Sha- har Z Kovalsky, and Yaron Lipman. Geometric optimization via composite majorization. ACM Trans. Graph., 36(4):38- 1, 2017. 1, 2\n\nDomain decomposition: parallel multilevel methods for elliptic partial differential equations. Barry Smith, Petter Bjorstad, William Gropp, Cambridge university pressBarry Smith, Petter Bjorstad, and William Gropp. Domain decomposition: parallel multilevel methods for elliptic par- tial differential equations. Cambridge university press, 2004. 2\n\nMulticlass spectral clustering. Yu Stella, Jianbo Shi, IEEE313X Yu Stella and Jianbo Shi. Multiclass spectral clustering. In null, page 313. IEEE, 2003. 5\n\nSolvability-unconcerned inverse kinematics by the levenberg-marquardt method. Tomomichi Sugihara, IEEE Transactions on Robotics. 275Tomomichi Sugihara. Solvability-unconcerned inverse kine- matics by the levenberg-marquardt method. IEEE Transac- tions on Robotics, 27(5):984-991, 2011. 1\n\nFace2face: Real-time face capture and reenactment of rgb videos. Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, Matthias Nie\u00dfner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJustus Thies, Michael Zollhofer, Marc Stamminger, Chris- tian Theobalt, and Matthias Nie\u00dfner. Face2face: Real-time face capture and reenactment of rgb videos. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 2387-2395, 2016. 1\n\nDomain decomposition methods-algorithms and theory. Andrea Toselli, Olof Widlund, Springer Science & Business Media. 342Andrea Toselli and Olof Widlund. Domain decomposition methods-algorithms and theory, volume 34. Springer Sci- ence & Business Media, 2006. 2\n\nBundle adjustment-a modern synthesis. Bill Triggs, F Philip, Richard I Mclauchlan, Andrew W Hartley, Fitzgibbon, International workshop on vision algorithms. Springer1Bill Triggs, Philip F McLauchlan, Richard I Hartley, and An- drew W Fitzgibbon. Bundle adjustment-a modern synthe- sis. In International workshop on vision algorithms, pages 298-372. Springer, 1999. 1, 2\n\nA chebyshev semi-iterative approach for accelerating projective and position-based dynamics. Huamin Wang, ACM Transactions on Graphics (TOG). 346Huamin Wang. A chebyshev semi-iterative approach for accelerating projective and position-based dynamics. ACM Transactions on Graphics (TOG), 34(6):1-9, 2015. 2\n\nDescent methods for elastic body simulation on the gpu. Huamin Wang, Yin Yang, ACM Transactions on Graphics (TOG). 356Huamin Wang and Yin Yang. Descent methods for elastic body simulation on the gpu. ACM Transactions on Graphics (TOG), 35(6):1-10, 2016. 2\n\nRobust global translations with 1dsfm. Kyle Wilson, Noah Snavely, European Conference on Computer Vision. Springer67Kyle Wilson and Noah Snavely. Robust global translations with 1dsfm. In European Conference on Computer Vision, pages 61-75. Springer, 2014. 5, 6, 7\n\nTowards linear-time incremental structure from motion. Changchang Wu, 2013 International Conference on 3D Vision-3DV 2013. Changchang Wu. Towards linear-time incremental struc- ture from motion. In 2013 International Conference on 3D Vision-3DV 2013, pages 127-134. IEEE, 2013. 2\n\nMulticore bundle adjustment. Changchang Wu, Sameer Agarwal, Brian Curless, Steven M Seitz, CVPR 2011. IEEEChangchang Wu, Sameer Agarwal, Brian Curless, and Steven M Seitz. Multicore bundle adjustment. In CVPR 2011, pages 3057-3064. IEEE, 2011. 1, 2, 3, 5, 7\n\nRealtime shading-based refinement for consumer depth cameras. Chenglei Wu, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Marc Stamminger, Shahram Izadi, Christian Theobalt, ACM Transactions on Graphics (ToG). 336Chenglei Wu, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Marc Stamminger, Shahram Izadi, and Christian Theobalt. Real- time shading-based refinement for consumer depth cameras. ACM Transactions on Graphics (ToG), 33(6):1-10, 2014. 2\n\nStochastic domain decomposition via moment minimization. Dongkun Zhang, Hessam Babaee, George Em Karniadakis, SIAM Journal on Scientific Computing. 404Dongkun Zhang, Hessam Babaee, and George Em Kar- niadakis. Stochastic domain decomposition via moment minimization. SIAM Journal on Scientific Computing, 40(4):A2152-A2173, 2018. 2\n\nDistributed very large scale bundle adjustment by global camera consensus. Runze Zhang, Siyu Zhu, Tian Fang, Long Quan, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision13Runze Zhang, Siyu Zhu, Tian Fang, and Long Quan. Dis- tributed very large scale bundle adjustment by global camera consensus. In Proceedings of the IEEE International Con- ference on Computer Vision, pages 29-38, 2017. 1, 2, 3\n\nStochastic bundle adjustment for efficient and scalable 3d reconstruction. Lei Zhou, Zixin Luo, Mingmin Zhen, Tianwei Shen, Shiwei Li, Zhuofei Huang, Tian Fang, Long Quan, arXiv:2008.0044656arXiv preprintLei Zhou, Zixin Luo, Mingmin Zhen, Tianwei Shen, Shiwei Li, Zhuofei Huang, Tian Fang, and Long Quan. Stochastic bundle adjustment for efficient and scalable 3d reconstruc- tion. arXiv preprint arXiv:2008.00446, 2020. 1, 3, 5, 6\n\nProgressive large scale-invariant image matching in scale space. Lei Zhou, Siyu Zhu, Tianwei Shen, Jinglu Wang, Tian Fang, Long Quan, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLei Zhou, Siyu Zhu, Tianwei Shen, Jinglu Wang, Tian Fang, and Long Quan. Progressive large scale-invariant image matching in scale space. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 2362-2371, 2017. 3\n\nParallel structure from motion from local increment to global averaging. Siyu Zhu, Tianwei Shen, Lei Zhou, Runze Zhang, Jinglu Wang, Tian Fang, Long Quan, arXiv:1702.08601arXiv preprintSiyu Zhu, Tianwei Shen, Lei Zhou, Runze Zhang, Jinglu Wang, Tian Fang, and Long Quan. Parallel structure from motion from local increment to global averaging. arXiv preprint arXiv:1702.08601, 2017. 3\n\nReal-time non-rigid reconstruction using an rgb-d camera. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, ACM Transactions on Graphics (ToG). 334Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, et al. Real-time non-rigid reconstruction using an rgb-d camera. ACM Transactions on Graphics (ToG), 33(4):1-12, 2014. 1, 2\n", "annotations": {"author": "[{\"end\":160,\"start\":113},{\"end\":205,\"start\":161},{\"end\":270,\"start\":206}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":121},{\"end\":171,\"start\":166},{\"end\":217,\"start\":214}]", "author_first_name": "[{\"end\":120,\"start\":113},{\"end\":165,\"start\":161},{\"end\":213,\"start\":206}]", "author_affiliation": "[{\"end\":159,\"start\":128},{\"end\":204,\"start\":173},{\"end\":250,\"start\":219},{\"end\":269,\"start\":252}]", "title": "[{\"end\":110,\"start\":1},{\"end\":380,\"start\":271}]", "venue": null, "abstract": "[{\"end\":1529,\"start\":410}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1799,\"start\":1795},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1802,\"start\":1799},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1805,\"start\":1802},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1808,\"start\":1805},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1846,\"start\":1842},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":1849,\"start\":1846},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1852,\"start\":1849},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1855,\"start\":1852},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1858,\"start\":1855},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":1886,\"start\":1882},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1889,\"start\":1886},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1891,\"start\":1889},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1894,\"start\":1891},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1897,\"start\":1894},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1900,\"start\":1897},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":1903,\"start\":1900},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2038,\"start\":2034},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2041,\"start\":2038},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2162,\"start\":2158},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2176,\"start\":2173},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2411,\"start\":2407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2480,\"start\":2476},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2549,\"start\":2545},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2612,\"start\":2609},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2623,\"start\":2619},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2929,\"start\":2926},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2946,\"start\":2942},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4666,\"start\":4663},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6406,\"start\":6402},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6409,\"start\":6406},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6412,\"start\":6409},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6415,\"start\":6412},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6694,\"start\":6690},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6697,\"start\":6694},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6896,\"start\":6892},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6992,\"start\":6988},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7074,\"start\":7070},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7077,\"start\":7074},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7138,\"start\":7134},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7293,\"start\":7289},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7296,\"start\":7293},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7457,\"start\":7453},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7793,\"start\":7789},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7873,\"start\":7870},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7890,\"start\":7886},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7952,\"start\":7949},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7966,\"start\":7962},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7980,\"start\":7976},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7994,\"start\":7990},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8439,\"start\":8436},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8442,\"start\":8439},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8445,\"start\":8442},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8770,\"start\":8766},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8772,\"start\":8770},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8775,\"start\":8772},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9010,\"start\":9006},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9065,\"start\":9061},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9068,\"start\":9065},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9207,\"start\":9203},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9373,\"start\":9370},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9376,\"start\":9373},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9379,\"start\":9376},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9434,\"start\":9430},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9534,\"start\":9531},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9537,\"start\":9534},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9540,\"start\":9537},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9551,\"start\":9547},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9564,\"start\":9560},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9758,\"start\":9754},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9761,\"start\":9758},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9764,\"start\":9761},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9832,\"start\":9828},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9835,\"start\":9832},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9940,\"start\":9936},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10065,\"start\":10061},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10068,\"start\":10065},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10136,\"start\":10133},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10251,\"start\":10247},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10262,\"start\":10258},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12422,\"start\":12419},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12636,\"start\":12632},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13576,\"start\":13572},{\"end\":13941,\"start\":13931},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15028,\"start\":15024},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15046,\"start\":15043},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15613,\"start\":15610},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16409,\"start\":16408},{\"end\":17381,\"start\":17371},{\"end\":18072,\"start\":18062},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18177,\"start\":18173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18274,\"start\":18270},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18696,\"start\":18692},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19101,\"start\":19097},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19114,\"start\":19110},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19142,\"start\":19138},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19155,\"start\":19151},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19291,\"start\":19287},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19482,\"start\":19478},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19503,\"start\":19499},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19883,\"start\":19879},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19897,\"start\":19894},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20694,\"start\":20690},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20925,\"start\":20921},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":21025,\"start\":21021},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21037,\"start\":21034},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21097,\"start\":21094},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21162,\"start\":21158},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21282,\"start\":21279},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":21410,\"start\":21406},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21491,\"start\":21487},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":21505,\"start\":21501},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22850,\"start\":22846},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22864,\"start\":22860},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22973,\"start\":22970},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23019,\"start\":23015},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23093,\"start\":23089},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23105,\"start\":23102},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23183,\"start\":23179},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23782,\"start\":23779},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23797,\"start\":23793},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25322,\"start\":25319},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25337,\"start\":25333},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25352,\"start\":25348},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25525,\"start\":25524},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25687,\"start\":25684},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25748,\"start\":25745},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25762,\"start\":25758},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25777,\"start\":25773},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27129,\"start\":27126},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29648,\"start\":29645},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29674,\"start\":29670},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31105,\"start\":31102},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31119,\"start\":31115}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28744,\"start\":28563},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29342,\"start\":28745},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29597,\"start\":29343},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29815,\"start\":29598},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29949,\"start\":29816},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30338,\"start\":29950},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30676,\"start\":30339},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31017,\"start\":30677},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32852,\"start\":31018}]", "paragraph": "[{\"end\":2325,\"start\":1545},{\"end\":2873,\"start\":2327},{\"end\":3667,\"start\":2875},{\"end\":4424,\"start\":3669},{\"end\":5675,\"start\":4426},{\"end\":6346,\"start\":5677},{\"end\":7617,\"start\":6364},{\"end\":8392,\"start\":7619},{\"end\":8963,\"start\":8394},{\"end\":10608,\"start\":8965},{\"end\":10944,\"start\":10621},{\"end\":11047,\"start\":10959},{\"end\":11100,\"start\":11097},{\"end\":11405,\"start\":11284},{\"end\":11545,\"start\":11407},{\"end\":11629,\"start\":11588},{\"end\":12133,\"start\":11666},{\"end\":12247,\"start\":12198},{\"end\":12423,\"start\":12332},{\"end\":13266,\"start\":12453},{\"end\":13577,\"start\":13268},{\"end\":15259,\"start\":13579},{\"end\":15645,\"start\":15358},{\"end\":16410,\"start\":15647},{\"end\":17382,\"start\":16472},{\"end\":18738,\"start\":17472},{\"end\":20743,\"start\":18768},{\"end\":22085,\"start\":20768},{\"end\":22865,\"start\":22087},{\"end\":23106,\"start\":22867},{\"end\":24302,\"start\":23108},{\"end\":24921,\"start\":24304},{\"end\":25115,\"start\":24937},{\"end\":25353,\"start\":25117},{\"end\":25942,\"start\":25372},{\"end\":26993,\"start\":25996},{\"end\":28230,\"start\":26995},{\"end\":28562,\"start\":28245}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11096,\"start\":11048},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11283,\"start\":11101},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11587,\"start\":11546},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11665,\"start\":11630},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12197,\"start\":12134},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12331,\"start\":12248},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15323,\"start\":15260},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16471,\"start\":16411},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17471,\"start\":17383},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25995,\"start\":25943}]", "table_ref": "[{\"end\":20571,\"start\":20564},{\"end\":22292,\"start\":22285},{\"end\":22961,\"start\":22942},{\"end\":24132,\"start\":24125},{\"end\":24920,\"start\":24913}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1543,\"start\":1531},{\"attributes\":{\"n\":\"2.\"},\"end\":6362,\"start\":6349},{\"attributes\":{\"n\":\"3.\"},\"end\":10619,\"start\":10611},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10957,\"start\":10947},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12451,\"start\":12426},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15356,\"start\":15325},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18766,\"start\":18741},{\"attributes\":{\"n\":\"4.\"},\"end\":20756,\"start\":20746},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20766,\"start\":20759},{\"end\":24935,\"start\":24924},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25370,\"start\":25356},{\"attributes\":{\"n\":\"5.\"},\"end\":28243,\"start\":28233},{\"end\":28574,\"start\":28564},{\"end\":28766,\"start\":28746},{\"end\":29354,\"start\":29344},{\"end\":29609,\"start\":29599},{\"end\":29827,\"start\":29817},{\"end\":29961,\"start\":29951},{\"end\":30351,\"start\":30340}]", "table": "[{\"end\":31017,\"start\":30724},{\"end\":32852,\"start\":31128}]", "figure_caption": "[{\"end\":28744,\"start\":28576},{\"end\":29342,\"start\":28769},{\"end\":29597,\"start\":29356},{\"end\":29815,\"start\":29611},{\"end\":29949,\"start\":29829},{\"end\":30338,\"start\":29963},{\"end\":30676,\"start\":30354},{\"end\":30724,\"start\":30679},{\"end\":31128,\"start\":31020}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5765,\"start\":5757},{\"end\":10861,\"start\":10853},{\"end\":14208,\"start\":14197},{\"end\":14276,\"start\":14264},{\"end\":14547,\"start\":14539},{\"end\":15834,\"start\":15826},{\"end\":15850,\"start\":15842},{\"end\":16592,\"start\":16580},{\"end\":16631,\"start\":16619},{\"end\":16721,\"start\":16708},{\"end\":17356,\"start\":17348},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20291,\"start\":20283},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21992,\"start\":21984},{\"end\":23038,\"start\":23030},{\"end\":23722,\"start\":23714},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24776,\"start\":24768},{\"end\":25249,\"start\":25241},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25575,\"start\":25567},{\"end\":25627,\"start\":25619},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26206,\"start\":26198},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26290,\"start\":26282},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26607,\"start\":26599},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26816,\"start\":26808},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26952,\"start\":26944},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27093,\"start\":27084},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27752,\"start\":27743}]", "bib_author_first_name": "[{\"end\":33030,\"start\":33024},{\"end\":33042,\"start\":33038},{\"end\":33058,\"start\":33051},{\"end\":33072,\"start\":33065},{\"end\":33083,\"start\":33079},{\"end\":33098,\"start\":33091},{\"end\":33113,\"start\":33105},{\"end\":33127,\"start\":33121},{\"end\":33146,\"start\":33138},{\"end\":33162,\"start\":33155},{\"end\":33599,\"start\":33593},{\"end\":33613,\"start\":33609},{\"end\":33725,\"start\":33719},{\"end\":33739,\"start\":33735},{\"end\":33750,\"start\":33749},{\"end\":33766,\"start\":33759},{\"end\":34082,\"start\":34075},{\"end\":34084,\"start\":34083},{\"end\":34108,\"start\":34104},{\"end\":34110,\"start\":34109},{\"end\":34441,\"start\":34432},{\"end\":34450,\"start\":34449},{\"end\":34464,\"start\":34459},{\"end\":34466,\"start\":34465},{\"end\":34753,\"start\":34749},{\"end\":35072,\"start\":35065},{\"end\":35083,\"start\":35079},{\"end\":35096,\"start\":35092},{\"end\":35384,\"start\":35383},{\"end\":35398,\"start\":35394},{\"end\":35406,\"start\":35399},{\"end\":35418,\"start\":35413},{\"end\":35650,\"start\":35649},{\"end\":35853,\"start\":35846},{\"end\":36249,\"start\":36241},{\"end\":36263,\"start\":36256},{\"end\":36277,\"start\":36270},{\"end\":36677,\"start\":36673},{\"end\":36699,\"start\":36690},{\"end\":36924,\"start\":36923},{\"end\":36938,\"start\":36934},{\"end\":36940,\"start\":36939},{\"end\":36954,\"start\":36948},{\"end\":36956,\"start\":36955},{\"end\":36974,\"start\":36966},{\"end\":37351,\"start\":37344},{\"end\":37367,\"start\":37360},{\"end\":37381,\"start\":37374},{\"end\":37400,\"start\":37393},{\"end\":37420,\"start\":37412},{\"end\":37444,\"start\":37435},{\"end\":37458,\"start\":37455},{\"end\":37476,\"start\":37469},{\"end\":37493,\"start\":37485},{\"end\":37947,\"start\":37943},{\"end\":37959,\"start\":37955},{\"end\":37972,\"start\":37967},{\"end\":38259,\"start\":38253},{\"end\":38278,\"start\":38273},{\"end\":38293,\"start\":38287},{\"end\":38644,\"start\":38638},{\"end\":38659,\"start\":38655},{\"end\":39098,\"start\":39091},{\"end\":39111,\"start\":39105},{\"end\":39123,\"start\":39120},{\"end\":39347,\"start\":39342},{\"end\":39368,\"start\":39363},{\"end\":39383,\"start\":39378},{\"end\":39399,\"start\":39392},{\"end\":39411,\"start\":39406},{\"end\":39819,\"start\":39812},{\"end\":39836,\"start\":39830},{\"end\":40053,\"start\":40046},{\"end\":40070,\"start\":40064},{\"end\":40086,\"start\":40081},{\"end\":40101,\"start\":40097},{\"end\":40595,\"start\":40589},{\"end\":40965,\"start\":40957},{\"end\":40980,\"start\":40974},{\"end\":40998,\"start\":40993},{\"end\":41246,\"start\":41239},{\"end\":41259,\"start\":41254},{\"end\":41271,\"start\":41266},{\"end\":41286,\"start\":41280},{\"end\":41553,\"start\":41546},{\"end\":41567,\"start\":41561},{\"end\":41581,\"start\":41573},{\"end\":41583,\"start\":41582},{\"end\":41600,\"start\":41592},{\"end\":41868,\"start\":41862},{\"end\":41881,\"start\":41876},{\"end\":41894,\"start\":41890},{\"end\":41911,\"start\":41904},{\"end\":41927,\"start\":41922},{\"end\":42277,\"start\":42276},{\"end\":42293,\"start\":42288},{\"end\":42495,\"start\":42491},{\"end\":42512,\"start\":42506},{\"end\":42701,\"start\":42694},{\"end\":42716,\"start\":42710},{\"end\":43077,\"start\":43070},{\"end\":43342,\"start\":43341},{\"end\":43344,\"start\":43343},{\"end\":43363,\"start\":43354},{\"end\":43677,\"start\":43676},{\"end\":43679,\"start\":43678},{\"end\":43894,\"start\":43893},{\"end\":44232,\"start\":44222},{\"end\":44253,\"start\":44244},{\"end\":44269,\"start\":44262},{\"end\":44283,\"start\":44276},{\"end\":44299,\"start\":44292},{\"end\":44312,\"start\":44308},{\"end\":44633,\"start\":44630},{\"end\":44643,\"start\":44638},{\"end\":44810,\"start\":44807},{\"end\":44819,\"start\":44815},{\"end\":44834,\"start\":44829},{\"end\":45184,\"start\":45180},{\"end\":45196,\"start\":45193},{\"end\":45213,\"start\":45204},{\"end\":45225,\"start\":45221},{\"end\":45238,\"start\":45233},{\"end\":45256,\"start\":45249},{\"end\":45271,\"start\":45265},{\"end\":45287,\"start\":45281},{\"end\":45300,\"start\":45293},{\"end\":45317,\"start\":45313},{\"end\":45781,\"start\":45773},{\"end\":45795,\"start\":45790},{\"end\":46130,\"start\":46123},{\"end\":46144,\"start\":46139},{\"end\":46160,\"start\":46155},{\"end\":46556,\"start\":46552},{\"end\":46574,\"start\":46568},{\"end\":46589,\"start\":46584},{\"end\":46801,\"start\":46800},{\"end\":46823,\"start\":46812},{\"end\":47226,\"start\":47220},{\"end\":47240,\"start\":47232},{\"end\":47484,\"start\":47480},{\"end\":47498,\"start\":47495},{\"end\":47512,\"start\":47508},{\"end\":47531,\"start\":47530},{\"end\":47545,\"start\":47540},{\"end\":47869,\"start\":47864},{\"end\":47883,\"start\":47877},{\"end\":47901,\"start\":47894},{\"end\":48152,\"start\":48150},{\"end\":48167,\"start\":48161},{\"end\":48361,\"start\":48352},{\"end\":48634,\"start\":48628},{\"end\":48649,\"start\":48642},{\"end\":48665,\"start\":48661},{\"end\":48687,\"start\":48678},{\"end\":48706,\"start\":48698},{\"end\":49184,\"start\":49178},{\"end\":49198,\"start\":49194},{\"end\":49430,\"start\":49426},{\"end\":49440,\"start\":49439},{\"end\":49456,\"start\":49449},{\"end\":49458,\"start\":49457},{\"end\":49477,\"start\":49471},{\"end\":49479,\"start\":49478},{\"end\":49859,\"start\":49853},{\"end\":50129,\"start\":50123},{\"end\":50139,\"start\":50136},{\"end\":50367,\"start\":50363},{\"end\":50380,\"start\":50376},{\"end\":50655,\"start\":50645},{\"end\":50910,\"start\":50900},{\"end\":50921,\"start\":50915},{\"end\":50936,\"start\":50931},{\"end\":50954,\"start\":50946},{\"end\":51200,\"start\":51192},{\"end\":51212,\"start\":51205},{\"end\":51232,\"start\":51224},{\"end\":51246,\"start\":51242},{\"end\":51266,\"start\":51259},{\"end\":51283,\"start\":51274},{\"end\":51624,\"start\":51617},{\"end\":51638,\"start\":51632},{\"end\":51653,\"start\":51647},{\"end\":51656,\"start\":51654},{\"end\":51973,\"start\":51968},{\"end\":51985,\"start\":51981},{\"end\":51995,\"start\":51991},{\"end\":52006,\"start\":52002},{\"end\":52442,\"start\":52439},{\"end\":52454,\"start\":52449},{\"end\":52467,\"start\":52460},{\"end\":52481,\"start\":52474},{\"end\":52494,\"start\":52488},{\"end\":52506,\"start\":52499},{\"end\":52518,\"start\":52514},{\"end\":52529,\"start\":52525},{\"end\":52865,\"start\":52862},{\"end\":52876,\"start\":52872},{\"end\":52889,\"start\":52882},{\"end\":52902,\"start\":52896},{\"end\":52913,\"start\":52909},{\"end\":52924,\"start\":52920},{\"end\":53367,\"start\":53363},{\"end\":53380,\"start\":53373},{\"end\":53390,\"start\":53387},{\"end\":53402,\"start\":53397},{\"end\":53416,\"start\":53410},{\"end\":53427,\"start\":53423},{\"end\":53438,\"start\":53434},{\"end\":53741,\"start\":53734},{\"end\":53761,\"start\":53753},{\"end\":53778,\"start\":53771},{\"end\":53795,\"start\":53786},{\"end\":53816,\"start\":53805},{\"end\":53830,\"start\":53823},{\"end\":53847,\"start\":53839},{\"end\":53858,\"start\":53852},{\"end\":53878,\"start\":53871},{\"end\":53894,\"start\":53885}]", "bib_author_last_name": "[{\"end\":33036,\"start\":33031},{\"end\":33049,\"start\":33043},{\"end\":33063,\"start\":33059},{\"end\":33077,\"start\":33073},{\"end\":33089,\"start\":33084},{\"end\":33103,\"start\":33099},{\"end\":33119,\"start\":33114},{\"end\":33136,\"start\":33128},{\"end\":33153,\"start\":33147},{\"end\":33168,\"start\":33163},{\"end\":33607,\"start\":33600},{\"end\":33620,\"start\":33614},{\"end\":33733,\"start\":33726},{\"end\":33747,\"start\":33740},{\"end\":33757,\"start\":33751},{\"end\":33772,\"start\":33767},{\"end\":33782,\"start\":33774},{\"end\":34102,\"start\":34085},{\"end\":34116,\"start\":34111},{\"end\":34122,\"start\":34118},{\"end\":34447,\"start\":34442},{\"end\":34457,\"start\":34451},{\"end\":34473,\"start\":34467},{\"end\":34480,\"start\":34475},{\"end\":34760,\"start\":34754},{\"end\":35077,\"start\":35073},{\"end\":35090,\"start\":35084},{\"end\":35100,\"start\":35097},{\"end\":35392,\"start\":35385},{\"end\":35411,\"start\":35407},{\"end\":35426,\"start\":35419},{\"end\":35435,\"start\":35428},{\"end\":35647,\"start\":35636},{\"end\":35656,\"start\":35651},{\"end\":35664,\"start\":35658},{\"end\":35864,\"start\":35854},{\"end\":35885,\"start\":35866},{\"end\":36254,\"start\":36250},{\"end\":36268,\"start\":36264},{\"end\":36284,\"start\":36278},{\"end\":36688,\"start\":36678},{\"end\":36707,\"start\":36700},{\"end\":36932,\"start\":36925},{\"end\":36946,\"start\":36941},{\"end\":36964,\"start\":36957},{\"end\":36983,\"start\":36975},{\"end\":36987,\"start\":36985},{\"end\":37358,\"start\":37352},{\"end\":37372,\"start\":37368},{\"end\":37391,\"start\":37382},{\"end\":37410,\"start\":37401},{\"end\":37433,\"start\":37421},{\"end\":37453,\"start\":37445},{\"end\":37467,\"start\":37459},{\"end\":37483,\"start\":37477},{\"end\":37502,\"start\":37494},{\"end\":37953,\"start\":37948},{\"end\":37965,\"start\":37960},{\"end\":37979,\"start\":37973},{\"end\":38271,\"start\":38260},{\"end\":38285,\"start\":38279},{\"end\":38301,\"start\":38294},{\"end\":38653,\"start\":38645},{\"end\":38667,\"start\":38660},{\"end\":39103,\"start\":39099},{\"end\":39118,\"start\":39112},{\"end\":39133,\"start\":39124},{\"end\":39361,\"start\":39348},{\"end\":39376,\"start\":39369},{\"end\":39390,\"start\":39384},{\"end\":39404,\"start\":39400},{\"end\":39418,\"start\":39412},{\"end\":39828,\"start\":39820},{\"end\":39844,\"start\":39837},{\"end\":40062,\"start\":40054},{\"end\":40079,\"start\":40071},{\"end\":40095,\"start\":40087},{\"end\":40110,\"start\":40102},{\"end\":40613,\"start\":40596},{\"end\":40622,\"start\":40615},{\"end\":40972,\"start\":40966},{\"end\":40991,\"start\":40981},{\"end\":41006,\"start\":40999},{\"end\":41252,\"start\":41247},{\"end\":41264,\"start\":41260},{\"end\":41278,\"start\":41272},{\"end\":41290,\"start\":41287},{\"end\":41559,\"start\":41554},{\"end\":41571,\"start\":41568},{\"end\":41590,\"start\":41584},{\"end\":41608,\"start\":41601},{\"end\":41874,\"start\":41869},{\"end\":41888,\"start\":41882},{\"end\":41902,\"start\":41895},{\"end\":41920,\"start\":41912},{\"end\":41933,\"start\":41928},{\"end\":42286,\"start\":42278},{\"end\":42300,\"start\":42294},{\"end\":42304,\"start\":42302},{\"end\":42504,\"start\":42496},{\"end\":42519,\"start\":42513},{\"end\":42708,\"start\":42702},{\"end\":42724,\"start\":42717},{\"end\":43087,\"start\":43078},{\"end\":43352,\"start\":43345},{\"end\":43372,\"start\":43364},{\"end\":43381,\"start\":43374},{\"end\":43685,\"start\":43680},{\"end\":43693,\"start\":43687},{\"end\":43901,\"start\":43895},{\"end\":43912,\"start\":43903},{\"end\":44242,\"start\":44233},{\"end\":44260,\"start\":44254},{\"end\":44274,\"start\":44270},{\"end\":44290,\"start\":44284},{\"end\":44306,\"start\":44300},{\"end\":44320,\"start\":44313},{\"end\":44636,\"start\":44634},{\"end\":44652,\"start\":44644},{\"end\":44813,\"start\":44811},{\"end\":44827,\"start\":44820},{\"end\":44843,\"start\":44835},{\"end\":45191,\"start\":45185},{\"end\":45202,\"start\":45197},{\"end\":45219,\"start\":45214},{\"end\":45231,\"start\":45226},{\"end\":45247,\"start\":45239},{\"end\":45263,\"start\":45257},{\"end\":45279,\"start\":45272},{\"end\":45291,\"start\":45288},{\"end\":45311,\"start\":45301},{\"end\":45324,\"start\":45318},{\"end\":45788,\"start\":45782},{\"end\":45802,\"start\":45796},{\"end\":46137,\"start\":46131},{\"end\":46153,\"start\":46145},{\"end\":46167,\"start\":46161},{\"end\":46566,\"start\":46557},{\"end\":46582,\"start\":46575},{\"end\":46595,\"start\":46590},{\"end\":46810,\"start\":46802},{\"end\":46835,\"start\":46824},{\"end\":46842,\"start\":46837},{\"end\":47230,\"start\":47227},{\"end\":47246,\"start\":47241},{\"end\":47493,\"start\":47485},{\"end\":47506,\"start\":47499},{\"end\":47528,\"start\":47513},{\"end\":47538,\"start\":47532},{\"end\":47554,\"start\":47546},{\"end\":47562,\"start\":47556},{\"end\":47875,\"start\":47870},{\"end\":47892,\"start\":47884},{\"end\":47907,\"start\":47902},{\"end\":48159,\"start\":48153},{\"end\":48171,\"start\":48168},{\"end\":48370,\"start\":48362},{\"end\":48640,\"start\":48635},{\"end\":48659,\"start\":48650},{\"end\":48676,\"start\":48666},{\"end\":48696,\"start\":48688},{\"end\":48714,\"start\":48707},{\"end\":49192,\"start\":49185},{\"end\":49206,\"start\":49199},{\"end\":49437,\"start\":49431},{\"end\":49447,\"start\":49441},{\"end\":49469,\"start\":49459},{\"end\":49487,\"start\":49480},{\"end\":49499,\"start\":49489},{\"end\":49864,\"start\":49860},{\"end\":50134,\"start\":50130},{\"end\":50144,\"start\":50140},{\"end\":50374,\"start\":50368},{\"end\":50388,\"start\":50381},{\"end\":50658,\"start\":50656},{\"end\":50913,\"start\":50911},{\"end\":50929,\"start\":50922},{\"end\":50944,\"start\":50937},{\"end\":50960,\"start\":50955},{\"end\":51203,\"start\":51201},{\"end\":51222,\"start\":51213},{\"end\":51240,\"start\":51233},{\"end\":51257,\"start\":51247},{\"end\":51272,\"start\":51267},{\"end\":51292,\"start\":51284},{\"end\":51630,\"start\":51625},{\"end\":51645,\"start\":51639},{\"end\":51668,\"start\":51657},{\"end\":51979,\"start\":51974},{\"end\":51989,\"start\":51986},{\"end\":52000,\"start\":51996},{\"end\":52011,\"start\":52007},{\"end\":52447,\"start\":52443},{\"end\":52458,\"start\":52455},{\"end\":52472,\"start\":52468},{\"end\":52486,\"start\":52482},{\"end\":52497,\"start\":52495},{\"end\":52512,\"start\":52507},{\"end\":52523,\"start\":52519},{\"end\":52534,\"start\":52530},{\"end\":52870,\"start\":52866},{\"end\":52880,\"start\":52877},{\"end\":52894,\"start\":52890},{\"end\":52907,\"start\":52903},{\"end\":52918,\"start\":52914},{\"end\":52929,\"start\":52925},{\"end\":53371,\"start\":53368},{\"end\":53385,\"start\":53381},{\"end\":53395,\"start\":53391},{\"end\":53408,\"start\":53403},{\"end\":53421,\"start\":53417},{\"end\":53432,\"start\":53428},{\"end\":53443,\"start\":53439},{\"end\":53751,\"start\":53742},{\"end\":53769,\"start\":53762},{\"end\":53784,\"start\":53779},{\"end\":53803,\"start\":53796},{\"end\":53821,\"start\":53817},{\"end\":53837,\"start\":53831},{\"end\":53850,\"start\":53848},{\"end\":53869,\"start\":53859},{\"end\":53883,\"start\":53879},{\"end\":53903,\"start\":53895}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":33577,\"start\":32969},{\"attributes\":{\"id\":\"b1\"},\"end\":33685,\"start\":33579},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10389453},\"end\":34023,\"start\":33687},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6545163},\"end\":34353,\"start\":34025},{\"attributes\":{\"doi\":\"arXiv:1504.00084\",\"id\":\"b4\"},\"end\":34684,\"start\":34355},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":115963355},\"end\":34960,\"start\":34686},{\"attributes\":{\"id\":\"b6\"},\"end\":35298,\"start\":34962},{\"attributes\":{\"id\":\"b7\"},\"end\":35601,\"start\":35300},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":26258558},\"end\":35791,\"start\":35603},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5810341},\"end\":36199,\"start\":35793},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":771247},\"end\":36626,\"start\":36201},{\"attributes\":{\"doi\":\"arXiv:1612.01489\",\"id\":\"b11\"},\"end\":36865,\"start\":36628},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1636757},\"end\":37243,\"start\":36867},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5846953},\"end\":37863,\"start\":37245},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":538820},\"end\":38198,\"start\":37865},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":197680658},\"end\":38540,\"start\":38200},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2592500},\"end\":39046,\"start\":38542},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":216072394},\"end\":39261,\"start\":39048},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":221584579},\"end\":39722,\"start\":39263},{\"attributes\":{\"id\":\"b19\"},\"end\":39987,\"start\":39724},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":126294439},\"end\":40528,\"start\":39989},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2207234},\"end\":40866,\"start\":40530},{\"attributes\":{\"id\":\"b22\"},\"end\":41195,\"start\":40868},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":206845618},\"end\":41484,\"start\":41197},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3026455},\"end\":41798,\"start\":41486},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14903702},\"end\":42230,\"start\":41800},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b26\"},\"end\":42456,\"start\":42232},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13784570},\"end\":42636,\"start\":42458},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7866972},\"end\":42993,\"start\":42638},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":124308544},\"end\":43277,\"start\":42995},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":474253},\"end\":43613,\"start\":43279},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":16543854},\"end\":43824,\"start\":43615},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":122360030},\"end\":44157,\"start\":43826},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":17660158},\"end\":44585,\"start\":44159},{\"attributes\":{\"id\":\"b34\"},\"end\":44740,\"start\":44587},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":12905784},\"end\":45108,\"start\":44742},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":202786778},\"end\":45682,\"start\":45110},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1409543},\"end\":46054,\"start\":45684},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":119421149},\"end\":46437,\"start\":46056},{\"attributes\":{\"id\":\"b39\"},\"end\":46766,\"start\":46439},{\"attributes\":{\"id\":\"b40\"},\"end\":47178,\"start\":46768},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14848918},\"end\":47427,\"start\":47180},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7379723},\"end\":47767,\"start\":47429},{\"attributes\":{\"id\":\"b43\"},\"end\":48116,\"start\":47769},{\"attributes\":{\"id\":\"b44\"},\"end\":48272,\"start\":48118},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":17754675},\"end\":48561,\"start\":48274},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":56894332},\"end\":49124,\"start\":48563},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":60601469},\"end\":49386,\"start\":49126},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1354186},\"end\":49758,\"start\":49388},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":14068248},\"end\":50065,\"start\":49760},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":17747035},\"end\":50322,\"start\":50067},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":653212},\"end\":50588,\"start\":50324},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5296119},\"end\":50869,\"start\":50590},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":11832473},\"end\":51128,\"start\":50871},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":6250317},\"end\":51558,\"start\":51130},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":52216752},\"end\":51891,\"start\":51560},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":10708124},\"end\":52362,\"start\":51893},{\"attributes\":{\"doi\":\"arXiv:2008.00446\",\"id\":\"b57\"},\"end\":52795,\"start\":52364},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":6040619},\"end\":53288,\"start\":52797},{\"attributes\":{\"doi\":\"arXiv:1702.08601\",\"id\":\"b59\"},\"end\":53674,\"start\":53290},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":9616070},\"end\":54238,\"start\":53676}]", "bib_title": "[{\"end\":33022,\"start\":32969},{\"end\":33717,\"start\":33687},{\"end\":34073,\"start\":34025},{\"end\":34747,\"start\":34686},{\"end\":35634,\"start\":35603},{\"end\":35844,\"start\":35793},{\"end\":36239,\"start\":36201},{\"end\":36921,\"start\":36867},{\"end\":37342,\"start\":37245},{\"end\":37941,\"start\":37865},{\"end\":38251,\"start\":38200},{\"end\":38636,\"start\":38542},{\"end\":39089,\"start\":39048},{\"end\":39340,\"start\":39263},{\"end\":40044,\"start\":39989},{\"end\":40587,\"start\":40530},{\"end\":40955,\"start\":40868},{\"end\":41237,\"start\":41197},{\"end\":41544,\"start\":41486},{\"end\":41860,\"start\":41800},{\"end\":42489,\"start\":42458},{\"end\":42692,\"start\":42638},{\"end\":43068,\"start\":42995},{\"end\":43339,\"start\":43279},{\"end\":43674,\"start\":43615},{\"end\":43891,\"start\":43826},{\"end\":44220,\"start\":44159},{\"end\":44805,\"start\":44742},{\"end\":45178,\"start\":45110},{\"end\":45771,\"start\":45684},{\"end\":46121,\"start\":46056},{\"end\":46798,\"start\":46768},{\"end\":47218,\"start\":47180},{\"end\":47478,\"start\":47429},{\"end\":48350,\"start\":48274},{\"end\":48626,\"start\":48563},{\"end\":49176,\"start\":49126},{\"end\":49424,\"start\":49388},{\"end\":49851,\"start\":49760},{\"end\":50121,\"start\":50067},{\"end\":50361,\"start\":50324},{\"end\":50643,\"start\":50590},{\"end\":50898,\"start\":50871},{\"end\":51190,\"start\":51130},{\"end\":51615,\"start\":51560},{\"end\":51966,\"start\":51893},{\"end\":52860,\"start\":52797},{\"end\":53732,\"start\":53676}]", "bib_author": "[{\"end\":33038,\"start\":33024},{\"end\":33051,\"start\":33038},{\"end\":33065,\"start\":33051},{\"end\":33079,\"start\":33065},{\"end\":33091,\"start\":33079},{\"end\":33105,\"start\":33091},{\"end\":33121,\"start\":33105},{\"end\":33138,\"start\":33121},{\"end\":33155,\"start\":33138},{\"end\":33170,\"start\":33155},{\"end\":33609,\"start\":33593},{\"end\":33622,\"start\":33609},{\"end\":33735,\"start\":33719},{\"end\":33749,\"start\":33735},{\"end\":33759,\"start\":33749},{\"end\":33774,\"start\":33759},{\"end\":33784,\"start\":33774},{\"end\":34104,\"start\":34075},{\"end\":34118,\"start\":34104},{\"end\":34124,\"start\":34118},{\"end\":34449,\"start\":34432},{\"end\":34459,\"start\":34449},{\"end\":34475,\"start\":34459},{\"end\":34482,\"start\":34475},{\"end\":34762,\"start\":34749},{\"end\":35079,\"start\":35065},{\"end\":35092,\"start\":35079},{\"end\":35102,\"start\":35092},{\"end\":35394,\"start\":35383},{\"end\":35413,\"start\":35394},{\"end\":35428,\"start\":35413},{\"end\":35437,\"start\":35428},{\"end\":35649,\"start\":35636},{\"end\":35658,\"start\":35649},{\"end\":35666,\"start\":35658},{\"end\":35866,\"start\":35846},{\"end\":35887,\"start\":35866},{\"end\":36256,\"start\":36241},{\"end\":36270,\"start\":36256},{\"end\":36286,\"start\":36270},{\"end\":36690,\"start\":36673},{\"end\":36709,\"start\":36690},{\"end\":36934,\"start\":36923},{\"end\":36948,\"start\":36934},{\"end\":36966,\"start\":36948},{\"end\":36985,\"start\":36966},{\"end\":36989,\"start\":36985},{\"end\":37360,\"start\":37344},{\"end\":37374,\"start\":37360},{\"end\":37393,\"start\":37374},{\"end\":37412,\"start\":37393},{\"end\":37435,\"start\":37412},{\"end\":37455,\"start\":37435},{\"end\":37469,\"start\":37455},{\"end\":37485,\"start\":37469},{\"end\":37504,\"start\":37485},{\"end\":37955,\"start\":37943},{\"end\":37967,\"start\":37955},{\"end\":37981,\"start\":37967},{\"end\":38273,\"start\":38253},{\"end\":38287,\"start\":38273},{\"end\":38303,\"start\":38287},{\"end\":38655,\"start\":38638},{\"end\":38669,\"start\":38655},{\"end\":39105,\"start\":39091},{\"end\":39120,\"start\":39105},{\"end\":39135,\"start\":39120},{\"end\":39363,\"start\":39342},{\"end\":39378,\"start\":39363},{\"end\":39392,\"start\":39378},{\"end\":39406,\"start\":39392},{\"end\":39420,\"start\":39406},{\"end\":39830,\"start\":39812},{\"end\":39846,\"start\":39830},{\"end\":40064,\"start\":40046},{\"end\":40081,\"start\":40064},{\"end\":40097,\"start\":40081},{\"end\":40112,\"start\":40097},{\"end\":40615,\"start\":40589},{\"end\":40624,\"start\":40615},{\"end\":40974,\"start\":40957},{\"end\":40993,\"start\":40974},{\"end\":41008,\"start\":40993},{\"end\":41254,\"start\":41239},{\"end\":41266,\"start\":41254},{\"end\":41280,\"start\":41266},{\"end\":41292,\"start\":41280},{\"end\":41561,\"start\":41546},{\"end\":41573,\"start\":41561},{\"end\":41592,\"start\":41573},{\"end\":41610,\"start\":41592},{\"end\":41876,\"start\":41862},{\"end\":41890,\"start\":41876},{\"end\":41904,\"start\":41890},{\"end\":41922,\"start\":41904},{\"end\":41935,\"start\":41922},{\"end\":42288,\"start\":42276},{\"end\":42302,\"start\":42288},{\"end\":42306,\"start\":42302},{\"end\":42506,\"start\":42491},{\"end\":42521,\"start\":42506},{\"end\":42710,\"start\":42694},{\"end\":42726,\"start\":42710},{\"end\":43089,\"start\":43070},{\"end\":43354,\"start\":43341},{\"end\":43374,\"start\":43354},{\"end\":43383,\"start\":43374},{\"end\":43687,\"start\":43676},{\"end\":43695,\"start\":43687},{\"end\":43903,\"start\":43893},{\"end\":43914,\"start\":43903},{\"end\":44244,\"start\":44222},{\"end\":44262,\"start\":44244},{\"end\":44276,\"start\":44262},{\"end\":44292,\"start\":44276},{\"end\":44308,\"start\":44292},{\"end\":44322,\"start\":44308},{\"end\":44638,\"start\":44630},{\"end\":44654,\"start\":44638},{\"end\":44815,\"start\":44807},{\"end\":44829,\"start\":44815},{\"end\":44845,\"start\":44829},{\"end\":45193,\"start\":45180},{\"end\":45204,\"start\":45193},{\"end\":45221,\"start\":45204},{\"end\":45233,\"start\":45221},{\"end\":45249,\"start\":45233},{\"end\":45265,\"start\":45249},{\"end\":45281,\"start\":45265},{\"end\":45293,\"start\":45281},{\"end\":45313,\"start\":45293},{\"end\":45326,\"start\":45313},{\"end\":45790,\"start\":45773},{\"end\":45804,\"start\":45790},{\"end\":46139,\"start\":46123},{\"end\":46155,\"start\":46139},{\"end\":46169,\"start\":46155},{\"end\":46568,\"start\":46552},{\"end\":46584,\"start\":46568},{\"end\":46597,\"start\":46584},{\"end\":46812,\"start\":46800},{\"end\":46837,\"start\":46812},{\"end\":46844,\"start\":46837},{\"end\":47232,\"start\":47220},{\"end\":47248,\"start\":47232},{\"end\":47495,\"start\":47480},{\"end\":47508,\"start\":47495},{\"end\":47530,\"start\":47508},{\"end\":47540,\"start\":47530},{\"end\":47556,\"start\":47540},{\"end\":47564,\"start\":47556},{\"end\":47877,\"start\":47864},{\"end\":47894,\"start\":47877},{\"end\":47909,\"start\":47894},{\"end\":48161,\"start\":48150},{\"end\":48173,\"start\":48161},{\"end\":48372,\"start\":48352},{\"end\":48642,\"start\":48628},{\"end\":48661,\"start\":48642},{\"end\":48678,\"start\":48661},{\"end\":48698,\"start\":48678},{\"end\":48716,\"start\":48698},{\"end\":49194,\"start\":49178},{\"end\":49208,\"start\":49194},{\"end\":49439,\"start\":49426},{\"end\":49449,\"start\":49439},{\"end\":49471,\"start\":49449},{\"end\":49489,\"start\":49471},{\"end\":49501,\"start\":49489},{\"end\":49866,\"start\":49853},{\"end\":50136,\"start\":50123},{\"end\":50146,\"start\":50136},{\"end\":50376,\"start\":50363},{\"end\":50390,\"start\":50376},{\"end\":50660,\"start\":50645},{\"end\":50915,\"start\":50900},{\"end\":50931,\"start\":50915},{\"end\":50946,\"start\":50931},{\"end\":50962,\"start\":50946},{\"end\":51205,\"start\":51192},{\"end\":51224,\"start\":51205},{\"end\":51242,\"start\":51224},{\"end\":51259,\"start\":51242},{\"end\":51274,\"start\":51259},{\"end\":51294,\"start\":51274},{\"end\":51632,\"start\":51617},{\"end\":51647,\"start\":51632},{\"end\":51670,\"start\":51647},{\"end\":51981,\"start\":51968},{\"end\":51991,\"start\":51981},{\"end\":52002,\"start\":51991},{\"end\":52013,\"start\":52002},{\"end\":52449,\"start\":52439},{\"end\":52460,\"start\":52449},{\"end\":52474,\"start\":52460},{\"end\":52488,\"start\":52474},{\"end\":52499,\"start\":52488},{\"end\":52514,\"start\":52499},{\"end\":52525,\"start\":52514},{\"end\":52536,\"start\":52525},{\"end\":52872,\"start\":52862},{\"end\":52882,\"start\":52872},{\"end\":52896,\"start\":52882},{\"end\":52909,\"start\":52896},{\"end\":52920,\"start\":52909},{\"end\":52931,\"start\":52920},{\"end\":53373,\"start\":53363},{\"end\":53387,\"start\":53373},{\"end\":53397,\"start\":53387},{\"end\":53410,\"start\":53397},{\"end\":53423,\"start\":53410},{\"end\":53434,\"start\":53423},{\"end\":53445,\"start\":53434},{\"end\":53753,\"start\":53734},{\"end\":53771,\"start\":53753},{\"end\":53786,\"start\":53771},{\"end\":53805,\"start\":53786},{\"end\":53823,\"start\":53805},{\"end\":53839,\"start\":53823},{\"end\":53852,\"start\":53839},{\"end\":53871,\"start\":53852},{\"end\":53885,\"start\":53871},{\"end\":53905,\"start\":53885}]", "bib_venue": "[{\"end\":33252,\"start\":33170},{\"end\":33591,\"start\":33579},{\"end\":33822,\"start\":33784},{\"end\":34172,\"start\":34124},{\"end\":34430,\"start\":34355},{\"end\":34790,\"start\":34762},{\"end\":35063,\"start\":34962},{\"end\":35381,\"start\":35300},{\"end\":35679,\"start\":35666},{\"end\":35954,\"start\":35887},{\"end\":36363,\"start\":36286},{\"end\":36671,\"start\":36628},{\"end\":37037,\"start\":36989},{\"end\":37538,\"start\":37504},{\"end\":38017,\"start\":37981},{\"end\":38326,\"start\":38303},{\"end\":38746,\"start\":38669},{\"end\":39139,\"start\":39135},{\"end\":39443,\"start\":39420},{\"end\":39810,\"start\":39724},{\"end\":40194,\"start\":40112},{\"end\":40679,\"start\":40624},{\"end\":41016,\"start\":41008},{\"end\":41322,\"start\":41292},{\"end\":41626,\"start\":41610},{\"end\":41997,\"start\":41935},{\"end\":42274,\"start\":42232},{\"end\":42525,\"start\":42521},{\"end\":42789,\"start\":42726},{\"end\":43121,\"start\":43089},{\"end\":43431,\"start\":43383},{\"end\":43713,\"start\":43695},{\"end\":43975,\"start\":43914},{\"end\":44356,\"start\":44322},{\"end\":44628,\"start\":44587},{\"end\":44898,\"start\":44845},{\"end\":45375,\"start\":45326},{\"end\":45852,\"start\":45804},{\"end\":46227,\"start\":46169},{\"end\":46550,\"start\":46439},{\"end\":46921,\"start\":46844},{\"end\":47265,\"start\":47248},{\"end\":47580,\"start\":47564},{\"end\":47862,\"start\":47769},{\"end\":48148,\"start\":48118},{\"end\":48401,\"start\":48372},{\"end\":48793,\"start\":48716},{\"end\":49241,\"start\":49208},{\"end\":49544,\"start\":49501},{\"end\":49900,\"start\":49866},{\"end\":50180,\"start\":50146},{\"end\":50428,\"start\":50390},{\"end\":50711,\"start\":50660},{\"end\":50971,\"start\":50962},{\"end\":51328,\"start\":51294},{\"end\":51706,\"start\":51670},{\"end\":52080,\"start\":52013},{\"end\":52437,\"start\":52364},{\"end\":52998,\"start\":52931},{\"end\":53361,\"start\":53290},{\"end\":53939,\"start\":53905},{\"end\":34805,\"start\":34792},{\"end\":36008,\"start\":35956},{\"end\":36427,\"start\":36365},{\"end\":38810,\"start\":38748},{\"end\":40278,\"start\":40196},{\"end\":46985,\"start\":46923},{\"end\":48857,\"start\":48795},{\"end\":52134,\"start\":52082},{\"end\":53052,\"start\":53000}]"}}}, "year": 2023, "month": 12, "day": 17}