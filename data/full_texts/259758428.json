{"id": 259758428, "updated": "2023-10-09 19:57:52.722", "metadata": {"title": "Self-Supervised Logic Induction for Explainable Fuzzy Temporal Commonsense Reasoning", "authors": "[{\"first\":\"Bibo\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Xiao\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Zhouhao\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Bing\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Baojun\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lifeng\",\"last\":\"Shang\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Understanding temporal commonsense concepts, such as times of occurrence and durations, is crucial for event-centric language understanding. Reasoning about such temporal concepts in a complex context requires reasoning over both the stated context and the world knowledge that underlines it. A recent study shows massive pre-trained LM still struggle with such temporal reasoning under complex contexts (e.g., dialog) because they only implicitly encode the relevant contexts and fail to explicitly uncover the underlying logical compositions for complex inference, thus may not be robust enough. In this work, we propose to augment LMs with the temporal logic induction ability, which frames temporal reasoning by defining three modular components: temporal dependency inducer and temporal concept defuzzifier, and logic valida-tor. The former two components disentangle the explicit/implicit dependency between temporal concepts across context (before, after, ...) and the specific meaning of fuzzy temporal concepts, respectively, while the validator combines the intermediate reasoning clues for robust contextual reasoning about the temporal concepts. Extensive experimental re-sults on TIMEDIAL, a challenging dataset for temporal reasoning over dialog, show that our method, L ogic Induction E nhanced C ontextualized TE mporal R easoning (LECTER), can yield great improvements over the traditional language model for temporal reasoning.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/CaiDS00WS23", "doi": "10.1609/aaai.v37i11.26481"}}, "content": {"source": {"pdf_hash": "8b2f4ec5f57314448d85536d16d50d8e37892e1a", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/26481/26253\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26481/26253", "status": "GOLD"}}, "grobid": {"id": "24049be6e25cf0b6cf9961c63bb9e85f92ba9d66", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/8b2f4ec5f57314448d85536d16d50d8e37892e1a.txt", "contents": "\nSelf-Supervised Logic Induction for Explainable Fuzzy Temporal Commonsense Reasoning\n\n\nBibo Cai bbcai@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nXiao Ding xding@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nZhouhao Sun zhsun@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nBing Qin bqin@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nTing Liu tliu@ir.hit.edu.cn \nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\nChina\n\nBaojun Wang \nHuawei Noah's Ark Lab\n\n\nLifeng Shang shang.lifeng@huawei.com \nHuawei Noah's Ark Lab\n\n\nSelf-Supervised Logic Induction for Explainable Fuzzy Temporal Commonsense Reasoning\nF3EC0344CD07F45571C1092376F3B56C\nUnderstanding temporal commonsense concepts, such as times of occurrence and durations, is crucial for event-centric language understanding.Reasoning about such temporal concepts in a complex context requires reasoning over both the stated context and the world knowledge that underlines it.A recent study shows massive pre-trained LM still struggle with such temporal reasoning under complex contexts (e.g., dialog) because they only implicitly encode the relevant contexts and fail to explicitly uncover the underlying logical compositions for complex inference, thus may not be robust enough.In this work, we propose to augment LMs with the temporal logic induction ability, which frames temporal reasoning by defining three modular components: temporal dependency inducer and temporal concept defuzzifier, and logic validator.The former two components disentangle the explicit/implicit dependency between temporal concepts across context (before, after, ...) and the specific meaning of fuzzy temporal concepts, respectively, while the validator combines the intermediate reasoning clues for robust contextual reasoning about the temporal concepts.Extensive experimental results on TIMEDIAL, a challenging dataset for temporal reasoning over dialog, show that our method, Logic Induction Enhanced Contextualized TEmporal Reasoning (LECTER), can yield great improvements over the traditional language model for temporal reasoning.\n\nIntroduction\n\nUnderstanding time in natural language text is crucial for understanding the evolving world.Humans can reason about temporal concepts in a language such as the times of occurrence and durations of events based on their rich temporal commonsense knowledge.Many event-centric NLP systems, such as timeline construction (Leeuwenberg and Moens 2018), clinical analysis (Bethard et al. 2015), dialogue assistants (Rong et al. 2017), etc, also rely on the ability of temporal commonsense reasoning to achieve satisfactory performance.\n\nAs manually annotating the Temporal CommonSense (TCS) knowledge required for temporal reasoning is time and labor-consuming, recent studies attempt to improve the NLP system's TCS reasoning ability with cheap supervision: the typical approach is to identify the temporal mentions in the free-form text, which is utilized to augment the traditional language model objective function with temporal signals.We note that, however, there are significant limitations with such a pretraining-only approach.\n\nAs shown in Figure 1 (a), to fill the correct answer in the blank, one should be able to reason about the global context to derive the latent inter-dependencies among the temporal concepts in the context (the target answer should be earlier than the temporal expression appears in the position of \"three o'clock\").\n\nMoreover, different from the traditional tasks that also require such a relation induction ability, it also presents additional practical challenges.For example, for the QA problem in Figure 1 (b), the right answer can be derived once the correct computation logic is obtained (i.e., 2008 \u2212 2007 = 1).While in the example shown in Figure 1 (a), we should not only understand the latent dependencies (i.e., \"earlier\") among the temporal concepts, but also infer that the fuzzy expression \"three o'clock\" in the example denotes a time of the day, most probably 3 pm.The target answer could be inferred from the logical constraint earlier(\"3 pm\", ?).Hence, the temporal reasoning process may implicitly involve three steps: resolve the dependency among global temporal concepts and figure out the specific meaning of the fuzzy temporal concept, followed by utilizing them together with the general temporal knowledge to verify the correctness of the candidate temporal concept.\n\nWhile the traditional pre-trained objectives only implicitly encode the relevant contexts and fail to explicitly reason with the underlying logic rules for temporal inference, which lead to reliance on shallow cues for complex reasoning (Helwe, Clavel, and Suchanek 2021;Qin et al. 2021).\n\nTo solve these problems, in this work, we present LECTER, a novel self-supervised framework that conducts temporal logic induction for fuzzy temporal commonsense reasoning.Specifically, LECTER frames temporal reasoning by introducing three modular components: the temporal dependency inducer, temporal concept defuzzifier, and logic validator.The dependency inducer disentangles the explicit/implicit dependency between temporal concepts across context (before, after, ...) and the defuzzifier resolves the specific meaning of fuzzy temporal concepts.The logic validator is a symbolic module defined with the logic programming language DeepProbLog (Brown et al. 2020)), which combines the resolved results of the former two modules to verify the correctness of the candidate temporal concepts.The clues provided by the verification results work together with the LM-based predictor for TCS reasoning.The LECTER is trained with two self-supervised objectives: the regression-based temporal value recovery objective and the temporal logical entailment objective.The former objective trains the model to learn the shallow temporal knowledge from context, while the latter objective aims to teach models to uncover the logical rules that could explain the reasoning process based on implication.The whole network is trained in an end-to-end manner.\n\nWe evaluate LECTER on the challenging contextual TCS reasoning dataset TIMEDIAL (Qin et al. 2021), which demonstrates significant performance gains (more than 10 points) across the LMs continually trained with traditional pre-training objectives.Furthermore, with the temporal logic induction module, LECTER could capture the underlying logic to make the decision, which makes it more explainable than common language model-based methods.\n\n\nBackground Problem Definition\n\nThe document D contains sequences [s 1 , s 2 , \u2022 \u2022 \u2022 , s M ] and temporal expressions t i \u2208 \u0393, 1 \u2264 i \u2264 N , where M and N are the total number of sequences and time expressions in D with N > 1. \u0393 represents the set of temporal expressions.Formally, the task is formulated as a clozen task: given the corrupted document D \u2032 where a temporal expression in D is masked out, a system is required to select all suitable temporal expressions for the masked-out span from the answer set A.\n\nFor example, both \"two o'clock\" and \"2:00 pm\" are correct answers for the blank in the passage shown in Figure 1 (a), while \"8:00 am\" is wrong.\n\n\nThe DeepProbLog Framework\n\nWe notice that human's prior knowledge of the numerical relation between temporal concepts can be utilized as distant supervision for the temporal logic induction, which can be effectively integrated into the neural network with the neural-symbolic framework DeepProbLog.Before diving into the details of utilizing DeepProbLog to describe our problem, we first provide a basic overview of the Deep-ProbLog (see (Manhaeve et al. 2021) for more details), and illustrate how the background knowledge is integrated.\n\nGenerally, the DeepProbLog is a probabilistic logic programming language that incorporates deep learning by means of neural predicates.Definition 1 (Probabilistic Logic Programming).The probabilistic logic programming (Raedt, Kimmig, and Toivonen 2007) is a programming paradigm that is largely based on formal logic.A program written with probabilistic logic programming language contains a set of probabilistic facts F of the form p :: f where p is a probability and f an atom, and a set of rules R.\n\nExample.The following program defines the domain of tossing coins.\n\nHere, coin(x 1 , h) denotes the fact that the coin x 1 lands on the head, which is an atom with the predicate coin and two arguments.The main inference task in logic programming is to compute the true probability of the query atom q in the canonical model of the logic program P .In this case, given the query twoHeads(x 1 , x 2 ), the program can obtain the probability of both coins landing on heads, which is the product of both probabilities: P(twoHeads(x 1 , x 2 ))=0.2.Definition 2 (Neural Predicate).A neural predicate in DeepProbLog is the predicate that allows instantiating probabilistic facts whose probabilities are parameterized by neural networks processing raw data.\n\nTo declare neural predicates, DeepProbLog enhances the traditional logic programming language with a primitive for neural extension:\nnn(n id , x, z, Z)\nwhere nn is a reserved functor used to declare a neural predicate, n id is an identifier for the underlying neural network, x and z are the input raw data, and the output symbolic label of the neural network and Z denotes the domain of the output distribution by the neural network.\n\nExample.The following DeepProbLog program defines the neural extension of the domain of tossing coins.\n\n\nRegression-based\n\nTemporal Value Recovery S1: I'd like a ticket to New York on Saturday Morning.S2: We have a flight that we'll put you there at 10 am.\n\nIs that ok? S1: Nothing earlier?I prefer flight at 9 thirty.In this case, the coin is a neural predicate.The neural network represents a discriminative classifier, which maps the raw input X (can be an image of the tossing results) to the distribution over [h, t] (head and tail).The probability of the facts with predicate coin is computed by the softmax layer of the neural network.\n\n\nContext Embedder\n\n\nTemporal\n\nThe training of these neural predicates is done by providing supervision on the head of the logical rules expressed as standard logical queries.Definition 3 (Learn From Entailment).Given a Deep-ProbLog program with parameters \u0398, the inputs X and the query q that is desired to be true, the model adjusts the weights to maximize query probabilities P \u0398 (q|X ) for all training examples.\n\nIf the average negative log-likelihood is chosen as the loss function, the loss is in the form of:\narg min \u0398 1 |Q| q\u2208Q \u2212 log P \u0398 (q|X ) (1)\nWith the help of aProbLog, the gradient of the loss can be effectively computed which is then used in standard gradientbased methods for optimizing parameters in an end-to-end manner.\n\n\nMethod\n\nIn this work, we augment the large-scale language models with temporal logic induction ability for fuzzy temporal commonsense reasoning.Our method, LECTER (depicted in Figure 2), consists of three components: (i) a context embedder that encodes the input context rich in temporal concepts.(ii) the logic induction module composed of the logic inducer and the temporal concept defuzzifier, which aims to predict the distribution of the context-dependent temporal relation among global temporal concepts, and the specific meaning of a fuzzy temporal concept.(iii) the logic validator which encodes human's prior knowledge about the numerical relation between temporal concepts by the neural logic programming language DeepProbLog, allowing to estimate the degree that a query temporal concepts are consistent with the output of the neural layers.For effective training of the LECTER model, we propose two self-supervised learning objectives, i.e., the Regression-based Temporal Value Recovery and Temporal Logical Entailment.\n\n\nContext Embedder\n\nLM-Based Encoder The LM-based encoder learns an initial representation of the input tokens with an MLM-based pre-trained Transformer encoder (e.g., BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019)).The self-supervised input data is constructed in two steps.First, we utilize the temporal expression identifier to identify and normalize the temporal concepts in the context C. Second, for each input context, a single temporal expression that contains numerals will be masked out resulting in a cloze test x (i.e., the input data).We denote the temporal mention to be recovered as target concept, the other temporal expressions in the same context are denoted as auxiliary concepts.\n\nTemporal Expression Identification Aiming at automatically identifying temporal concepts from texts, we utilize the off-line temporal expression extraction tool (Chang and Manning 2012) together with heuristic rules to construct a grounded representation of the temporal mention.As shown in Figure 3, the temporal mention is grounded as a tuple T E =< M, N, U, D >, where M is a collection of the tokens of temporal mention, N and U are the numeric and raw context: \u2026 The meeting will be started at 8 in the morning.normalization:\n\nM -> \"8 in the morning\" N-> \"8\" U -> \"am\" T -> \"Clock\" unit of the mention.D represents the temporal dimension (e.g., CLOCK, DURATION).Note, the unit U may not always be expressed explicitly in the text, which leads to a T E with N = None.For example, the specific clock time of the mention \"at eight o'clock\" needs to be further resolved (also known as fused head resolution (Elazar and Goldberg 2019).We call such kind of temporal concepts as fuzzy temporal concept.The origin temporal mention in the context will be replaced with its normalized version, which consists of two tokens: value and unit (e.g., \"3 o'clock in the afternoon\" \u2192 \"3 pm\").For the fuzzy temporal mention, we take the [MASK] as the unit token.\n\nLanguage Encoding Given a target temporal concept t in the input data x, we mask out t from x to construct the sequence of input tokens x /{t} = {w int , w 1 , w 2 , \u2022 \u2022 \u2022 , w T }, then compute the contextualized representation {h int , h 1 , . . ., h T } for each token with pretrained language model.Here, h t denotes the contextualized representation of the t-th token obtained with the LM, x /{e} denotes replacing x with two mask token [MASK]s.The intuition is that the two [MASK]s are corresponding to the numeral and unit part of the temporal concept, respectively.\n\n\nLogic Induction Layer\n\nTemporal Dependency Inducer The temporal dependency inducer aims to generate the context-dependent temporal logic that the target and auxiliary temporal concepts should follow.In this paper, we consider logic rules indicating the binary latent relationship, so the induction can be formulated as a relation extraction task.Three kinds of relationships are taken into consideration: before, after, and equal.Specifically, the temporal logic inducer generates the distribution of the relation between target t and auxiliary temporal concept a by:\nr = W rule [x t \u2225 x a \u2225 x t \u2299 x a ].\n(\n)2\nwhere r is the output probabilistic logits across the rule labels, W rule is the learnable parameter matrix, the \u2225 denotes the tensor concatenation, x t /x a denotes the representation of the target/auxiliary temporal concept, which is obtained by averaging over the contextualized embedding h t of all the tokens w t that make up the concept.Temporal Concept Defuzzifier The uncertainty of fuzzy concepts makes it unable to directly instantiate the temporal logic rules generated by the logic inducer.To tackle this problem, we introduce the temporal concept defuzzifier module to estimate the probability of different normalization results of the fuzzy temporal concepts.As illustrated in the logic inducer, we obtain the representation x of temporal mention by averaging over the embedding of all the tokens that make up it.Specifically, each x is fed to the dimensionspecific unit classification layer U dim based on the dimension of the temporal mention:\nu i = U dim x(3)\nwhere u i is the predicated logits over the label of the unit set in the given dimension, dim \u2208 {CLOCK, DURATION}.\n\n\nLogic Validator\n\nThe temporal logic rules generated in the previous step are further evaluated by instantiating the variables with relevant temporal concepts in the context.We aim to compute the true probabilities of the grounded rule (i.e., the true probability of the proposition \"The target temporal concept is satisfied by the logic rules\"), known as learning from entailment which will be maximized during training.Specifically, the DeepProblog (Manhaeve et al. 2021) is a neural probabilistic logic programming language that integrates expressive probabilistic-logical modeling and reasoning with the neural networks.The idea is to take the outputs of both the logic rule inducer and temporal concept defuzzifier module as neural annotated disjunctions, once the symbolic grounding structure is given, the DeepProbLog program allows us to train the neural network seamlessly with back-propagation.An example of probability forwarding process can be seen in Figure 4.\n\n\nTraining\n\nFor effective training of the LECTER model, we propose two self-supervised learning objectives, i.e., the Regressionbasd Temporal Value Recovery and Temporal Logical Entailment.The two losses work together to train the LECTER model.\n\n\nRegression-Based Temporal Value Recovery\n\nSimilar to many other self-supervised tasks such as masked language modeling (Yang et al. 2020;Zhou et al. 2020Zhou et al. , 2022a)), we aim to teach models to recover the original temporal semantics of sentences from corrupted inputs.The regression-based temporal value recovery objective requires the LECTER model to predict the normalized temporal value given the context, where the original temporal concept tokens are removed.Previous work (Yang et al. 2020) shows that such an exact value prediction paradigm is very effective for duration prediction, in this work, we extend the idea for temporal reasoning in a more general domain.\n\nSpecifically, considering the final hidden vectors of the two mask tokens related to the explicit temporal concept e i as h unit i and h num i , they are fed to the dimension-specific regression layer w dim based on the dimension of e i :\nv i = 1 2 w dim (h unit i + h num i ) (4)\nwhere v i is the predicated normalized value, dim \u2208 {CLOCK, DURATION}, as we consider the T E of CLOCK and the DURATION dimension.\n\nEach v i will be optimized by mean square error loss:\nL reg = q i=1 (v i \u2212 y i ) 2 . (5)\nwhere y i denotes the normalized temporal value of the temporal concept e i .To construct y i , we normalize the temporal mention in DURATION dimension to logarithmic \"second\" space (e.g., 2 hours \u2192 7200 seconds \u2192 log(7200) \u2192 8.9).As for the temporal mention in CLOCK dimension, we represent time in decimal hours format (e.g., 7:30 pm \u2192 19 + 30/60 \u2192 19.5).\n\nTemporal Logical Entailment Following DeepProbLog (Manhaeve et al. 2021), we utilize the \"learning from entailment\" loss function.Specifically, given the training examples X and q as the query, the model needs to adjust the weights to maximize query probabilities P \u03b8 (q|X ) for all training examples.This can be reached by minimizing the average negative log-likelihood of the query:\nL logic = q \u2212log(P \u03b8 (q|X ))(6)\nSuch loss can be optimized with gradient-based learning which allows for seamless integration with neural training.Consequently, the learning objective of LECTER is to minimize:\nL = L reg + \u03bb * L logic . (7\n) At inference time, we predict the most plausible answer with:\narg min a\u2208A L(a|D \u2032 ) (8)\nwhere A is the answer set and D \u2032 is the document with one temporal span masked out.\n\n\nExperiment Dataset\n\nWe evaluate the performance of our proposed LECTER model on the challenge dataset TIMEDIAL (Qin et al. 2021).TIMEDIAL is a temporal commonsense reasoning benchmark on the dialog.Given a multi-turn dialog where a span of temporal words is masked out, the task requires the model to predict the suitable substitutions for the span from 4 options, two of which are right and two of which are wrong.In TIMEDIAL, the dialogs are carefully curated and are rich in temporal concepts, hence the model should accurately understand the causal relations between the temporal concepts to make an accurate prediction.There are 1.1k test instances in total and each dialog contains 11.7 turns and 3 temporal concepts on average.\n\n\nEvaluation Metrics\n\nFollowing (Qin et al. 2021), we utilize the 2-best accuracy metric to evaluate the model's performance in our experiment, which measures whether both of the model's topranked answers are correct.\n\n\nExperiment Settings\n\nTo evaluate whether the model could learn transferable reasoning skills, we focus on an out-of-domain training setting: the model is pre-trained with a large-scale corpus from a general domain with the self-supervised objective and is evaluated on the TIMEDIAL dataset in a zero-shot manner.\n\nThis training setting could also avoid the possible problem of data leakage that can occur when training models based on in-domain data.As the out-domain finetuning dataset used in (Qin et al. 2021) is not publicly available, in this work, we leverage other large-scale publicly available corpus containing over 700MB of text1 to construct our selfsupervised training dataset.The temporal expressions in the unsupervised dataset are identified with the temporal concept identifier as described in the previous section.After preprocessing, we obtain 97k/24k instances for training/validation.\n\nWe experiment with the base model of BERT (Devlin et al. 2019) and RoBERTa (Liu et al. 2019) to construct our contextual encoder.During the training, the batch size is set to 32.The combination weight \u03bb in Eq.7 is set to 1.We search the learning rate with grid search in lr \u2208 {5e \u2212 6, 1e \u2212 5, 5e \u2212 5} for the baseline and LECTER.The implementation is based on Pytorch and trained on a Tesla V100 GPU with Adam optimizer with 10 epochs.Note, a small number of answers can not be scored with Eq.8 at inference time.We handle this problem by utilizing the traditional masked language model loss to score such answers.\n\n\nBaseline\n\nFollowing (Qin et al. 2021), we compare our approach with popular pre-trained language models with different modeling paradigms.For BERT and RoBERTa, the candidate temporal span with k tokens in the dialog is replaced with k mask tokens.The average of the mask recovery crossentropy loss for each mask token is utilized as the prediction score.For T5, given a masked dialog context, we evaluate the likelihood of generating the given temporal span (normalized with the token number of the span).We report the Model 2-best Acc(%) T5-base ZERO (Qin et al. 2021) 39.1 OUT (Qin et al. 2021) 51.9 BERT-base ZERO (Qin et al. 2021) 44.8 OUT (Qin et al. 2021) 53.7 LECTER 65.8 RoBERTa-base ZERO (Qin et al. 2021) 52.2 OUT (Qin et al. 2021) 59.3 LECTER 71.5\n\nTable 1: Performance of TCS reasoning on TIMEDIAL dataset.Results show that LECTER outperforms basic PLMs by a large margin.T5 is not applicable to be utilized as the context encoder for LECTER.\n\nPLM's performance both in a zero-shot manner (denoted as ZERO) and with additional fine-tuning under the same outdomain training dataset (denoted as OUT) as LECTER.\n\n\nExperimental Results\n\nTable 1 shows our main results.As we can observe, (1) with a temporal-aware continual pre-training, the performance of PLM can be improved by a large margin than the zero setting.This proves that temporal reasoning can benefit from the temporal signals acquired with cheap supervision.\n\n(2) Our model LECTER, enhanced by explicitly modeling the fuzzy temporal reasoning process, achieves the best performance on the 2-best accuracy score.Compared with the baseline models, the improvements are over 10% on both the BERT and RoBERTa models continually trained with the same out-domain data.This shows LECTER is much more effective in learning transferable temporal reasoning skills than plain PLMs.\n\n\nAblation Study\n\nWe develop an ablation study to test different variations of LECTER (take RoBERTa as the backbone), as shown in Table 2.We can see that (1) the regression-based temporal value recovery objective lifts the performance by 9.0%.This is not unexpected due to the fuzziness of temporal expression in language.For example, the general commonsense of the duration of \"go to the office\" could be \"20 minutes\", \"half an hour\", etc.As a result, the model should fully learn the numerical property of the temporal concepts.(i.e., \"five days\" has a similar range to \"one week\", \"11:30 am\" is earlier than \"1:00 pm\", etc.).Leveraging the off-line normalization module for temporal concepts and pre-training the model with a regression objective make the model naturally learn a continual representation of the contextual temporal commonsense.While the common language model pre-training objective deals with the temporal concept token-by-token and thus fails to capture the fuzziness of temporal expression.\n\n(2) the temporal logic induction module lifts the results by 2.1%.ing the underlying relational compositions for complex inference.\n\nA temporal reasoning model has to understand the underlying reasoning process to achieve better generalization performance.\n\n\nCase Study and Error Analysis\n\nTable 3 shows the results of a case study with the outputs of LECTER and RoBERTa-OUT model (as illustrated in section ).In the three cases, the first two answers of the four candidates are correct.In the first case, both the incorrect answers already partially occur in the context.The RoBERTa-OUT model was completely confused and rank both the incorrect answers as the top prediction for the blank, although the two options violate the context.This shows it may rely on shallow text matching for temporal reasoning.Similarly, in the second example, the RoBERTa-OUT model still fails to capture the underlying logical relation among the temporal concepts in context.Instead, our model explicitly seeks to uncover the logic for the observations based on implication, which makes it able to exclude the incorrect answer that violates the inducted logic rules.In case 3, the resolved logical constraint for reasoning about the target temporal expression should be \"after(10 am, ?)\", which is satisfied by the first three candidate answers.To obtain the correct answer, the model also requires knowing the usual time of occurrence of the event \"have dinner\".In such special cases that require additional temporal comprehension, LECTER may make a mistake.\n\n\nRelated Work Temporal Commonsense Reasoning\n\nThe research topic of understanding time in natural language has long been studied for decades in the NLP community.A line of work related to temporal analysis focuses on the temporal information extraction task: temporal expressions extraction and normalization (Chang and Manning 2012;Lange et al. 2020;Ning et al. 2022;Cai et al. 2022), temporal relation among events extraction (Vashishtha, Van Durme, and White 2019;Han, Zhou, and Peng 2020;Han et al. 2021), and timeline construction (Viani et al. 2019).\n\nRecently, multiple works have been done on the Temporal CommonSense (TCS) reasoning (Thukral, Kukreja, and Kavouras 2021;Zhou et al. 2022a,b), such as events' TCS property prediction (Zhou et al. 2020) (the duration, frequency, typical time of a sentence-level event), event ordering (Han, Ren, and   utilizes narrative documents corpus to automatically construct data for temporal ordering and event infilling tasks.(Zhou et al. 2020) jointly models three key dimensions of temporal commonsense (duration, frequency, and typical time) and the other two auxiliary dimensions of TCS, the data of which is mined from unannotated free text.In this work, we focus on reasoning about the temporal expressions over complex context, which also requires understanding temporal commonsense property interwoven with events.However, our work has two notable differences from those works.First, we work on the general temporal expressions of numerical type, which include but are not limited to the time of occurrence, the duration, period, etc, while the works above only focus on the TCS property related to events.Second, we focus on reasoning over complex context (e.g., dialog), where the model should be able to disentangle the explicit and implicit inter-dependencies among multiple temporal concepts appearing in the context and reason about the global context.While previous ones usually deal with limited context and focus on a specific temporal concept in isolation.\n\n\nLogic Rule Induction in NLP\n\nThe advantage of logic rule induction is that it combines deep learning's ability on dealing with uncertainty and logic programming's ability for explainable reasoning.Recently, various studies learn logic rules for reasoning on knowledge graphs and multi-hop RC (Qu et al. 2021;Huang et al. 2021;Wang and Pan 2022).Traditional methods enumerate the latent logic rules and further learn a scalar weight to assess the quality of logic rules, while some recent works based on neural logic programming and neural theorem provers can learn logic rules and their weights in an end-to-end manner.We borrow the ideas from RNNlogic (Qu et al. 2021), which treats logic rules as a latent variable and simultaneously trains a rule generator as well as a reasoning predictor with logic rules.However, the rule inducted for temporal reasoning can not be directly grounded, due to the fuzzy expressions for temporal concepts in language.We tackle this problem by utilizing the neural network to assess and eliminate the uncertainty of fuzzy temporal concepts.\n\n\nConclusion\n\nIn this paper, we propose a novel pre-training framework LECTER to augment the language model with temporal logic induction ability for temporal commonsense reasoning.It encourages the model to resolve the underlying relational composition for contextualized temporal commonsense reasoning.The experimental result on the TIMEDIAL dataset demonstrates the efficiency of our proposed pretraining framework, which is better than traditional pretraining methods by a large margin.Our result suggests that the temporal reasoning models can benefit from explicitly modeling the context-dependent temporal logic rules.\n\nFigure 1 :\n1\nFigure 1: Fuzzy Reasoning.Temporal reasoning should resolve the dependency among global temporal concepts and figure out the specific meaning of the fuzzy temporal concepts.\n\n\n\n\nThe rule twoHeads(X, Y ) :coin(X, h), coin(Y , h) defines what it means for both coins to land on heads, which is in the form of h :-b 1 , \u2022 \u2022 \u2022 , b n denoting the logical implication: \"h if b 1 and \u2022 \u2022 \u2022 and b n \"\n\n\nFigure 2 :\n2\nFigure2: The framework of LECTER.We leverage the logic induction module to resolve the intermediate inference steps for fuzzy temporal commonsense reasoning.The logic validator acquires the predicted probability distributions from the dependency inducer and concept defuzzifier to compute the temporal logic entailment loss.It works together with the regression-based temporal value recovery loss to train the model in an end-to-end manner.\n\n\nFigure 3 :\n3\nFigure 3: The temporal concept parser parses the temporal expressions in context into the normalized format as a tuple.\n\n\nFigure 4 :\n4\nFigure 4: The probability forwarding process of the logic validator.\n\n\nTable 2 :\n2\nIt demonstrates the importance of explicitly uncover-Ablation results.Results show significant performance lifts from both modules.\nModel2-best Acc(%)LECTER71.5-Temporal Value Regression62.5-Temporal Logic Induction69.4\n\n\n\nPeng 2021) (how events are temporally arranged), and script learning(Lee and Goldwasser\nDialogueOptionsRoBERTa-OUT LECTERA: I had a really good time for taking lectures. B: What classes did you have?12 o'clock to 2 o'clockA: Well, I had English from 9 o'clock to 11 o'clock,11 o'clock to 1 o'clockart from , and math from 2 o'clock to 4 o'clock.12:00 AM to 4:00 AMB: What do you think about the teachers? A: To be honest, I liked all of them.7:00 PM to 11:00 PMA: I have a meeting this afternoon. B: When will it begin? A: It will begin at three o'clock. What's the time now?half past one quarter to twoB: It is .half past threeA: I have to go now. I don't want to be late. B: Don't worry, time is enough.half past nineA: Good morning, What time is it now? B: It is 9 o'clock now.2:00 pmA: I see. What is today's schedule?11:00 amB: You have two meetings today. One is at am, and9:00 pmthe other is at . After the meeting, you will have dinner with Mr. Brown2:00 am\n\nTable 3 :\n3\nCase study and error analysis of the model predictions.The first two of the four candidate answers are correct.\n2019) (what happens next after certain events). As humanannotation on the temporal commonsense is costly, manyworks focus on designing specific continual self-supervisedobjectives to conduct a temporal commonsense-centric pre-training. For example, (Lin, Chambers, and Durrett 2021)\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence \n0.4 :: coin(x1,h).\n0.5 :: coin(x2,h).\ntwoHeads(X,Y) :-coin(X, h), coin(Y, h).\nnn(coin_nn, [X], S, [h,t]): coin(X,S)\ntwoHeads(X,Y) :-coin(X, h), coin(Y, h).\nhttps://github.com/qywu/DialogCorpus\nAcknowledgmentsWe would like to thank Li Du for his valuable feedback and advice, and the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the Technological Innovation \"2030 Megaproject\" -New Generation Artificial Intelligence of China (2018AAA0101901), and the National Natural Science Foundation of China (62176079, 61976073), and the Industry-University-Research Innovation Foundation of China University (2021ITA05009).\nS Bethard, L Derczynski, G K Savova, J Pustejovsky, M Verhagen, SemEval-2015 Task 6: Clinical TempEval. * Semeval, 2015\n\nLanguage Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, 202033\n\nMitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic. B Cai, X Ding, B Chen, L Du, T Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236\n\nSUTime: A library for recognizing and normalizing time expressions. A X Chang, C D Manning, LREC. 2012\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis\n\nWhere's My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution. Y Elazar, Y Goldberg, Transactions of the Association for Computational Linguistics. 72019\n\nESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations. R Han, I.-H Hsu, J Sun, J Baylon, Q Ning, D Roth, N Peng, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021\n\nECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. R Han, X Ren, N Peng, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana\n\nDomain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction. R Han, Y Zhou, N Peng, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020\n\nReasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning. C Helwe, C Clavel, F M Suchanek, Y.-X Huang, W.-Z Dai, L.-W Cai, S H Muggleton, Y Jiang, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P Liang, J W Vaughan, Curran Associates, Inc2021. 202134AKBC\n\nAdversarial Alignment of Multilingual Models for Extracting Temporal Expressions from Text. L Lange, A Iurshina, H Adel, J Str\u00f6tgen, Proceedings of the 5th Workshop on Representation Learning for NLP. the 5th Workshop on Representation Learning for NLPOnline: Association for Computational Linguistics2020\n\nMulti-Relational Script Learning for Discourse Relations. I.-T Lee, D Goldwasser, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019\n\nTemporal Information Extraction by Predicting Relative Time-lines. A M Leeuwenberg, M.-F Moens, ACL. S.-T Emnlp. Lin, N Chambers, G Durrett, 2018Conditional Generation of Temporally-ordered Event Sequences\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, ArXiv, abs/1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019\n\nNeural probabilistic logic programming in DeepProbLog. R Manhaeve, S Duman\u010di\u0107, A Kimmig, T Demeester, L De Raedt, Artificial Intelligence. 2981035042021\n\nA Meta-framework for Spatiotemporal Quantity Extraction from Text. Q Ning, B Zhou, H Wu, H Peng, C Fan, M Gardner, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221\n\nTIMEDIAL: Temporal Commonsense Reasoning in Dialog. L Qin, A Gupta, S Upadhyay, L He, Y Choi, M Faruqui, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics20211\n\n{RNNL}ogic: Learning Logic Rules for Reasoning on Knowledge Graphs. M Qu, J Chen, L.-P Xhonneux, Y Bengio, J Tang, International Conference on Learning Representations. 2021\n\nProbLog: A Probabilistic Prolog and its Application in Link Discovery. L D Raedt, A Kimmig, H T Toivonen, IJCAI. 2007\n\nManaging Uncertainty in Time Expressions for Virtual Assistants. X Rong, A Fourney, R N Brewer, M R Morris, P N Bennett, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI '17. the 2017 CHI Conference on Human Factors in Computing Systems, CHI '17New York, NY, USAACM2017\n\nProbing Language Models for Understanding of Temporal Expressions. S Thukral, K Kukreja, C Kavouras, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021\n\nFine-Grained Temporal Relation Extraction. S Vashishtha, B Van Durme, A S White, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019\n\n. N Viani, H Tissot, A Bernardino, S Velupillai, \n\nAnnotating Temporal Information in Clinical Notes for Timeline Reconstruction: Towards the Definition of Calendar Expressions. Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskFlorence, ItalyAssociation for Computational Linguistics\n\nDeep Inductive Logic Reasoning for Multi-Hop Reading Comprehension. W Wang, S Pan, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221\n\n. Ireland Dublin, Association for Computational Linguistics\n\nImproving Event Duration Prediction via Time-aware Pre-training. Z Yang, X Du, A Rush, C Cardie, Findings of the Association for Computational Linguistics: EMNLP 2020. Online: Association for Computational Linguistics2020\n\nTemporal Common Sense Acquisition with Minimal Supervision. B Zhou, Q Ning, D Khashabi, D Roth, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline: Association for Computational Linguistics2020\n\nEventBERT: A Pre-Trained Model for Event Correlation Reasoning. Y Zhou, X Geng, T Shen, G Long, D Jiang, Proceedings of the ACM Web Conference 2022, WWW '22. the ACM Web Conference 2022, WWW '22New York, NY, USAAssociation for Computing Machinery2022aISBN 9781450390965\n\nClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification. Y Zhou, T Shen, X Geng, G Long, D Jiang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b1\n", "annotations": {"author": "[{\"end\":218,\"start\":88},{\"end\":350,\"start\":219},{\"end\":484,\"start\":351},{\"end\":614,\"start\":485},{\"end\":744,\"start\":615},{\"end\":781,\"start\":745},{\"end\":843,\"start\":782}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":93},{\"end\":228,\"start\":224},{\"end\":362,\"start\":359},{\"end\":493,\"start\":490},{\"end\":623,\"start\":620},{\"end\":756,\"start\":752},{\"end\":794,\"start\":789}]", "author_first_name": "[{\"end\":92,\"start\":88},{\"end\":223,\"start\":219},{\"end\":358,\"start\":351},{\"end\":489,\"start\":485},{\"end\":619,\"start\":615},{\"end\":751,\"start\":745},{\"end\":788,\"start\":782}]", "author_affiliation": "[{\"end\":217,\"start\":118},{\"end\":349,\"start\":250},{\"end\":483,\"start\":384},{\"end\":613,\"start\":514},{\"end\":743,\"start\":644},{\"end\":780,\"start\":758},{\"end\":842,\"start\":820}]", "title": "[{\"end\":85,\"start\":1},{\"end\":928,\"start\":844}]", "venue": null, "abstract": "[{\"end\":2395,\"start\":962}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2756,\"start\":2728},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2797,\"start\":2776},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2837,\"start\":2819},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5005,\"start\":4971},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5021,\"start\":5005},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5690,\"start\":5672},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6467,\"start\":6450},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7930,\"start\":7909},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8262,\"start\":8229},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12313,\"start\":12293},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12340,\"start\":12323},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13012,\"start\":12988},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13761,\"start\":13735},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16828,\"start\":16807},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17714,\"start\":17696},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17730,\"start\":17714},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17751,\"start\":17730},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18082,\"start\":18064},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19193,\"start\":19171},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20050,\"start\":20033},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20706,\"start\":20689},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21389,\"start\":21372},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21846,\"start\":21826},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21876,\"start\":21859},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22438,\"start\":22421},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22970,\"start\":22953},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22997,\"start\":22980},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23035,\"start\":23018},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23062,\"start\":23045},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23115,\"start\":23098},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23142,\"start\":23125},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27134,\"start\":27110},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27152,\"start\":27134},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27169,\"start\":27152},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27185,\"start\":27169},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27268,\"start\":27229},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27293,\"start\":27268},{\"end\":27309,\"start\":27293},{\"end\":27356,\"start\":27337},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27480,\"start\":27443},{\"end\":27500,\"start\":27480},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27559,\"start\":27542},{\"end\":27657,\"start\":27643},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27793,\"start\":27776},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29135,\"start\":29119},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29153,\"start\":29135},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29171,\"start\":29153},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29496,\"start\":29480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30717,\"start\":30529},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30936,\"start\":30718},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31392,\"start\":30937},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31527,\"start\":31393},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31611,\"start\":31528},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31844,\"start\":31612},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32813,\"start\":31845},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33221,\"start\":32814}]", "paragraph": "[{\"end\":2939,\"start\":2411},{\"end\":3440,\"start\":2941},{\"end\":3756,\"start\":3442},{\"end\":4732,\"start\":3758},{\"end\":5022,\"start\":4734},{\"end\":6368,\"start\":5024},{\"end\":6808,\"start\":6370},{\"end\":7323,\"start\":6842},{\"end\":7468,\"start\":7325},{\"end\":8009,\"start\":7498},{\"end\":8512,\"start\":8011},{\"end\":8580,\"start\":8514},{\"end\":9263,\"start\":8582},{\"end\":9397,\"start\":9265},{\"end\":9699,\"start\":9417},{\"end\":9803,\"start\":9701},{\"end\":9957,\"start\":9824},{\"end\":10343,\"start\":9959},{\"end\":10760,\"start\":10375},{\"end\":10860,\"start\":10762},{\"end\":11085,\"start\":10902},{\"end\":12119,\"start\":11096},{\"end\":12825,\"start\":12140},{\"end\":13357,\"start\":12827},{\"end\":14076,\"start\":13359},{\"end\":14650,\"start\":14078},{\"end\":15220,\"start\":14676},{\"end\":15259,\"start\":15258},{\"end\":16222,\"start\":15263},{\"end\":16354,\"start\":16240},{\"end\":17329,\"start\":16374},{\"end\":17574,\"start\":17342},{\"end\":18258,\"start\":17619},{\"end\":18498,\"start\":18260},{\"end\":18671,\"start\":18541},{\"end\":18726,\"start\":18673},{\"end\":19119,\"start\":18762},{\"end\":19505,\"start\":19121},{\"end\":19715,\"start\":19538},{\"end\":19808,\"start\":19745},{\"end\":19919,\"start\":19835},{\"end\":20656,\"start\":19942},{\"end\":20874,\"start\":20679},{\"end\":21189,\"start\":20898},{\"end\":21782,\"start\":21191},{\"end\":22398,\"start\":21784},{\"end\":23159,\"start\":22411},{\"end\":23355,\"start\":23161},{\"end\":23521,\"start\":23357},{\"end\":23831,\"start\":23546},{\"end\":24243,\"start\":23833},{\"end\":25256,\"start\":24262},{\"end\":25389,\"start\":25258},{\"end\":25514,\"start\":25391},{\"end\":26799,\"start\":25548},{\"end\":27357,\"start\":26847},{\"end\":28824,\"start\":27359},{\"end\":29902,\"start\":28856},{\"end\":30528,\"start\":29917},{\"end\":30716,\"start\":30543},{\"end\":30935,\"start\":30721},{\"end\":31391,\"start\":30951},{\"end\":31526,\"start\":31407},{\"end\":31610,\"start\":31542},{\"end\":31756,\"start\":31625},{\"end\":31935,\"start\":31848},{\"end\":32938,\"start\":32827}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9416,\"start\":9398},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10901,\"start\":10861},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15257,\"start\":15221},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15262,\"start\":15260},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16239,\"start\":16223},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18540,\"start\":18499},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18760,\"start\":18727},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18761,\"start\":18760},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19537,\"start\":19506},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19744,\"start\":19716},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19833,\"start\":19809},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19834,\"start\":19833}]", "table_ref": "[{\"end\":23168,\"start\":23167},{\"end\":23553,\"start\":23552},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25555,\"start\":25554}]", "section_header": "[{\"end\":2409,\"start\":2397},{\"end\":6840,\"start\":6811},{\"end\":7496,\"start\":7471},{\"end\":9822,\"start\":9806},{\"end\":10362,\"start\":10346},{\"end\":10373,\"start\":10365},{\"end\":11094,\"start\":11088},{\"end\":12138,\"start\":12122},{\"end\":14674,\"start\":14653},{\"end\":16372,\"start\":16357},{\"end\":17340,\"start\":17332},{\"end\":17617,\"start\":17577},{\"end\":19940,\"start\":19922},{\"end\":20677,\"start\":20659},{\"end\":20896,\"start\":20877},{\"end\":22409,\"start\":22401},{\"end\":23544,\"start\":23524},{\"end\":24260,\"start\":24246},{\"end\":25546,\"start\":25517},{\"end\":26845,\"start\":26802},{\"end\":28854,\"start\":28827},{\"end\":29915,\"start\":29905},{\"end\":30540,\"start\":30530},{\"end\":30948,\"start\":30938},{\"end\":31404,\"start\":31394},{\"end\":31539,\"start\":31529},{\"end\":31622,\"start\":31613},{\"end\":32824,\"start\":32815}]", "table": "[{\"end\":31844,\"start\":31757},{\"end\":32813,\"start\":31936},{\"end\":33221,\"start\":32939}]", "figure_caption": "[{\"end\":30717,\"start\":30542},{\"end\":30936,\"start\":30720},{\"end\":31392,\"start\":30950},{\"end\":31527,\"start\":31406},{\"end\":31611,\"start\":31541},{\"end\":31757,\"start\":31624},{\"end\":31936,\"start\":31847},{\"end\":32939,\"start\":32826}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3462,\"start\":3461},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3954,\"start\":3949},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4097,\"start\":4096},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7437,\"start\":7436},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11272,\"start\":11271},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13126,\"start\":13125},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17328,\"start\":17327}]", "bib_author_first_name": "[{\"end\":33949,\"start\":33948},{\"end\":33960,\"start\":33959},{\"end\":33974,\"start\":33973},{\"end\":33976,\"start\":33975},{\"end\":33986,\"start\":33985},{\"end\":34001,\"start\":34000},{\"end\":34053,\"start\":34052},{\"end\":34109,\"start\":34108},{\"end\":34118,\"start\":34117},{\"end\":34126,\"start\":34125},{\"end\":34135,\"start\":34134},{\"end\":34146,\"start\":34145},{\"end\":34148,\"start\":34147},{\"end\":34158,\"start\":34157},{\"end\":34170,\"start\":34169},{\"end\":34185,\"start\":34184},{\"end\":34194,\"start\":34193},{\"end\":34204,\"start\":34203},{\"end\":34214,\"start\":34213},{\"end\":34225,\"start\":34224},{\"end\":34241,\"start\":34240},{\"end\":34252,\"start\":34251},{\"end\":34264,\"start\":34263},{\"end\":34273,\"start\":34272},{\"end\":34283,\"start\":34282},{\"end\":34294,\"start\":34293},{\"end\":34300,\"start\":34299},{\"end\":34310,\"start\":34309},{\"end\":34319,\"start\":34318},{\"end\":34327,\"start\":34326},{\"end\":34337,\"start\":34336},{\"end\":34347,\"start\":34346},{\"end\":34355,\"start\":34354},{\"end\":34364,\"start\":34363},{\"end\":34373,\"start\":34372},{\"end\":34383,\"start\":34382},{\"end\":34397,\"start\":34396},{\"end\":34408,\"start\":34407},{\"end\":34421,\"start\":34420},{\"end\":34482,\"start\":34481},{\"end\":34496,\"start\":34495},{\"end\":34507,\"start\":34506},{\"end\":34518,\"start\":34517},{\"end\":34528,\"start\":34527},{\"end\":34650,\"start\":34649},{\"end\":34657,\"start\":34656},{\"end\":34665,\"start\":34664},{\"end\":34673,\"start\":34672},{\"end\":34679,\"start\":34678},{\"end\":34871,\"start\":34870},{\"end\":34873,\"start\":34872},{\"end\":34882,\"start\":34881},{\"end\":34884,\"start\":34883},{\"end\":34989,\"start\":34988},{\"end\":35002,\"start\":34998},{\"end\":35011,\"start\":35010},{\"end\":35018,\"start\":35017},{\"end\":35497,\"start\":35496},{\"end\":35507,\"start\":35506},{\"end\":35682,\"start\":35681},{\"end\":35692,\"start\":35688},{\"end\":35699,\"start\":35698},{\"end\":35706,\"start\":35705},{\"end\":35716,\"start\":35715},{\"end\":35724,\"start\":35723},{\"end\":35732,\"start\":35731},{\"end\":36065,\"start\":36064},{\"end\":36072,\"start\":36071},{\"end\":36079,\"start\":36078},{\"end\":36432,\"start\":36431},{\"end\":36439,\"start\":36438},{\"end\":36447,\"start\":36446},{\"end\":36740,\"start\":36739},{\"end\":36749,\"start\":36748},{\"end\":36759,\"start\":36758},{\"end\":36761,\"start\":36760},{\"end\":36776,\"start\":36772},{\"end\":36788,\"start\":36784},{\"end\":36798,\"start\":36794},{\"end\":36805,\"start\":36804},{\"end\":36807,\"start\":36806},{\"end\":36820,\"start\":36819},{\"end\":36880,\"start\":36879},{\"end\":36891,\"start\":36890},{\"end\":36906,\"start\":36905},{\"end\":36917,\"start\":36916},{\"end\":36926,\"start\":36925},{\"end\":36928,\"start\":36927},{\"end\":37071,\"start\":37070},{\"end\":37080,\"start\":37079},{\"end\":37092,\"start\":37091},{\"end\":37100,\"start\":37099},{\"end\":37347,\"start\":37343},{\"end\":37354,\"start\":37353},{\"end\":37658,\"start\":37657},{\"end\":37660,\"start\":37659},{\"end\":37678,\"start\":37674},{\"end\":37695,\"start\":37691},{\"end\":37709,\"start\":37708},{\"end\":37721,\"start\":37720},{\"end\":37798,\"start\":37797},{\"end\":37805,\"start\":37804},{\"end\":37812,\"start\":37811},{\"end\":37821,\"start\":37820},{\"end\":37827,\"start\":37826},{\"end\":37836,\"start\":37835},{\"end\":37844,\"start\":37843},{\"end\":37852,\"start\":37851},{\"end\":37861,\"start\":37860},{\"end\":37876,\"start\":37875},{\"end\":38027,\"start\":38026},{\"end\":38039,\"start\":38038},{\"end\":38051,\"start\":38050},{\"end\":38061,\"start\":38060},{\"end\":38074,\"start\":38073},{\"end\":38193,\"start\":38192},{\"end\":38201,\"start\":38200},{\"end\":38209,\"start\":38208},{\"end\":38215,\"start\":38214},{\"end\":38223,\"start\":38222},{\"end\":38230,\"start\":38229},{\"end\":38530,\"start\":38529},{\"end\":38537,\"start\":38536},{\"end\":38546,\"start\":38545},{\"end\":38558,\"start\":38557},{\"end\":38564,\"start\":38563},{\"end\":38572,\"start\":38571},{\"end\":39023,\"start\":39022},{\"end\":39029,\"start\":39028},{\"end\":39040,\"start\":39036},{\"end\":39052,\"start\":39051},{\"end\":39062,\"start\":39061},{\"end\":39201,\"start\":39200},{\"end\":39203,\"start\":39202},{\"end\":39212,\"start\":39211},{\"end\":39222,\"start\":39221},{\"end\":39224,\"start\":39223},{\"end\":39314,\"start\":39313},{\"end\":39322,\"start\":39321},{\"end\":39333,\"start\":39332},{\"end\":39335,\"start\":39334},{\"end\":39345,\"start\":39344},{\"end\":39347,\"start\":39346},{\"end\":39357,\"start\":39356},{\"end\":39359,\"start\":39358},{\"end\":39620,\"start\":39619},{\"end\":39631,\"start\":39630},{\"end\":39642,\"start\":39641},{\"end\":39961,\"start\":39960},{\"end\":39975,\"start\":39974},{\"end\":39988,\"start\":39987},{\"end\":39990,\"start\":39989},{\"end\":40224,\"start\":40223},{\"end\":40233,\"start\":40232},{\"end\":40243,\"start\":40242},{\"end\":40257,\"start\":40256},{\"end\":40623,\"start\":40622},{\"end\":40631,\"start\":40630},{\"end\":40827,\"start\":40820},{\"end\":40945,\"start\":40944},{\"end\":40953,\"start\":40952},{\"end\":40959,\"start\":40958},{\"end\":40967,\"start\":40966},{\"end\":41163,\"start\":41162},{\"end\":41171,\"start\":41170},{\"end\":41179,\"start\":41178},{\"end\":41191,\"start\":41190},{\"end\":41479,\"start\":41478},{\"end\":41487,\"start\":41486},{\"end\":41495,\"start\":41494},{\"end\":41503,\"start\":41502},{\"end\":41511,\"start\":41510},{\"end\":41805,\"start\":41804},{\"end\":41813,\"start\":41812},{\"end\":41821,\"start\":41820},{\"end\":41829,\"start\":41828},{\"end\":41837,\"start\":41836}]", "bib_author_last_name": "[{\"end\":33957,\"start\":33950},{\"end\":33971,\"start\":33961},{\"end\":33983,\"start\":33977},{\"end\":33998,\"start\":33987},{\"end\":34010,\"start\":34002},{\"end\":34061,\"start\":34054},{\"end\":34115,\"start\":34110},{\"end\":34123,\"start\":34119},{\"end\":34132,\"start\":34127},{\"end\":34143,\"start\":34136},{\"end\":34155,\"start\":34149},{\"end\":34167,\"start\":34159},{\"end\":34182,\"start\":34171},{\"end\":34191,\"start\":34186},{\"end\":34201,\"start\":34195},{\"end\":34211,\"start\":34205},{\"end\":34222,\"start\":34215},{\"end\":34238,\"start\":34226},{\"end\":34249,\"start\":34242},{\"end\":34261,\"start\":34253},{\"end\":34270,\"start\":34265},{\"end\":34280,\"start\":34274},{\"end\":34291,\"start\":34284},{\"end\":34297,\"start\":34295},{\"end\":34307,\"start\":34301},{\"end\":34316,\"start\":34311},{\"end\":34324,\"start\":34320},{\"end\":34334,\"start\":34328},{\"end\":34344,\"start\":34338},{\"end\":34352,\"start\":34348},{\"end\":34361,\"start\":34356},{\"end\":34370,\"start\":34365},{\"end\":34380,\"start\":34374},{\"end\":34394,\"start\":34384},{\"end\":34405,\"start\":34398},{\"end\":34418,\"start\":34409},{\"end\":34428,\"start\":34422},{\"end\":34493,\"start\":34483},{\"end\":34504,\"start\":34497},{\"end\":34515,\"start\":34508},{\"end\":34525,\"start\":34519},{\"end\":34532,\"start\":34529},{\"end\":34654,\"start\":34651},{\"end\":34662,\"start\":34658},{\"end\":34670,\"start\":34666},{\"end\":34676,\"start\":34674},{\"end\":34683,\"start\":34680},{\"end\":34879,\"start\":34874},{\"end\":34892,\"start\":34885},{\"end\":34996,\"start\":34990},{\"end\":35008,\"start\":35003},{\"end\":35015,\"start\":35012},{\"end\":35028,\"start\":35019},{\"end\":35504,\"start\":35498},{\"end\":35516,\"start\":35508},{\"end\":35686,\"start\":35683},{\"end\":35696,\"start\":35693},{\"end\":35703,\"start\":35700},{\"end\":35713,\"start\":35707},{\"end\":35721,\"start\":35717},{\"end\":35729,\"start\":35725},{\"end\":35737,\"start\":35733},{\"end\":36069,\"start\":36066},{\"end\":36076,\"start\":36073},{\"end\":36084,\"start\":36080},{\"end\":36436,\"start\":36433},{\"end\":36444,\"start\":36440},{\"end\":36452,\"start\":36448},{\"end\":36746,\"start\":36741},{\"end\":36756,\"start\":36750},{\"end\":36770,\"start\":36762},{\"end\":36782,\"start\":36777},{\"end\":36792,\"start\":36789},{\"end\":36802,\"start\":36799},{\"end\":36817,\"start\":36808},{\"end\":36826,\"start\":36821},{\"end\":36888,\"start\":36881},{\"end\":36903,\"start\":36892},{\"end\":36914,\"start\":36907},{\"end\":36923,\"start\":36918},{\"end\":36936,\"start\":36929},{\"end\":37077,\"start\":37072},{\"end\":37089,\"start\":37081},{\"end\":37097,\"start\":37093},{\"end\":37109,\"start\":37101},{\"end\":37351,\"start\":37348},{\"end\":37365,\"start\":37355},{\"end\":37672,\"start\":37661},{\"end\":37684,\"start\":37679},{\"end\":37706,\"start\":37696},{\"end\":37718,\"start\":37710},{\"end\":37729,\"start\":37722},{\"end\":37802,\"start\":37799},{\"end\":37809,\"start\":37806},{\"end\":37818,\"start\":37813},{\"end\":37824,\"start\":37822},{\"end\":37833,\"start\":37828},{\"end\":37841,\"start\":37837},{\"end\":37849,\"start\":37845},{\"end\":37858,\"start\":37853},{\"end\":37873,\"start\":37862},{\"end\":37885,\"start\":37877},{\"end\":38036,\"start\":38028},{\"end\":38048,\"start\":38040},{\"end\":38058,\"start\":38052},{\"end\":38071,\"start\":38062},{\"end\":38083,\"start\":38075},{\"end\":38198,\"start\":38194},{\"end\":38206,\"start\":38202},{\"end\":38212,\"start\":38210},{\"end\":38220,\"start\":38216},{\"end\":38227,\"start\":38224},{\"end\":38238,\"start\":38231},{\"end\":38534,\"start\":38531},{\"end\":38543,\"start\":38538},{\"end\":38555,\"start\":38547},{\"end\":38561,\"start\":38559},{\"end\":38569,\"start\":38565},{\"end\":38580,\"start\":38573},{\"end\":39026,\"start\":39024},{\"end\":39034,\"start\":39030},{\"end\":39049,\"start\":39041},{\"end\":39059,\"start\":39053},{\"end\":39067,\"start\":39063},{\"end\":39209,\"start\":39204},{\"end\":39219,\"start\":39213},{\"end\":39233,\"start\":39225},{\"end\":39319,\"start\":39315},{\"end\":39330,\"start\":39323},{\"end\":39342,\"start\":39336},{\"end\":39354,\"start\":39348},{\"end\":39367,\"start\":39360},{\"end\":39628,\"start\":39621},{\"end\":39639,\"start\":39632},{\"end\":39651,\"start\":39643},{\"end\":39972,\"start\":39962},{\"end\":39985,\"start\":39976},{\"end\":39996,\"start\":39991},{\"end\":40230,\"start\":40225},{\"end\":40240,\"start\":40234},{\"end\":40254,\"start\":40244},{\"end\":40268,\"start\":40258},{\"end\":40628,\"start\":40624},{\"end\":40635,\"start\":40632},{\"end\":40834,\"start\":40828},{\"end\":40950,\"start\":40946},{\"end\":40956,\"start\":40954},{\"end\":40964,\"start\":40960},{\"end\":40974,\"start\":40968},{\"end\":41168,\"start\":41164},{\"end\":41176,\"start\":41172},{\"end\":41188,\"start\":41180},{\"end\":41196,\"start\":41192},{\"end\":41484,\"start\":41480},{\"end\":41492,\"start\":41488},{\"end\":41500,\"start\":41496},{\"end\":41508,\"start\":41504},{\"end\":41517,\"start\":41512},{\"end\":41810,\"start\":41806},{\"end\":41818,\"start\":41814},{\"end\":41826,\"start\":41822},{\"end\":41834,\"start\":41830},{\"end\":41843,\"start\":41838}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34067,\"start\":33948},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218971783},\"end\":34540,\"start\":34069},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":250298058},\"end\":34800,\"start\":34542},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":146233},\"end\":34904,\"start\":34802},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52967399},\"end\":35390,\"start\":34906},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":166227998},\"end\":35586,\"start\":35392},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":243865619},\"end\":35973,\"start\":35588},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":237497382},\"end\":36329,\"start\":35975},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":221739172},\"end\":36658,\"start\":36331},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":237397001},\"end\":36976,\"start\":36660},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":218684724},\"end\":37283,\"start\":36978},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":196193121},\"end\":37588,\"start\":37285},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52111780},\"end\":37795,\"start\":37590},{\"attributes\":{\"doi\":\"ArXiv, abs/1907.11692\",\"id\":\"b13\"},\"end\":37969,\"start\":37797},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":202750403},\"end\":38123,\"start\":37971},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":248780347},\"end\":38475,\"start\":38125},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235368000},\"end\":38952,\"start\":38477},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":222208985},\"end\":39127,\"start\":38954},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":115707342},\"end\":39246,\"start\":39129},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15249349},\"end\":39550,\"start\":39248},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":238259493},\"end\":39915,\"start\":39552},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":59599681},\"end\":40219,\"start\":39917},{\"attributes\":{\"id\":\"b22\"},\"end\":40270,\"start\":40221},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":199368495},\"end\":40552,\"start\":40272},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":248780159},\"end\":40816,\"start\":40554},{\"attributes\":{\"id\":\"b25\"},\"end\":40877,\"start\":40818},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":226254029},\"end\":41100,\"start\":40879},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":218581125},\"end\":41412,\"start\":41102},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":238744494},\"end\":41683,\"start\":41414},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":247244544},\"end\":42081,\"start\":41685}]", "bib_title": "[{\"end\":34106,\"start\":34069},{\"end\":34647,\"start\":34542},{\"end\":34868,\"start\":34802},{\"end\":34986,\"start\":34906},{\"end\":35494,\"start\":35392},{\"end\":35679,\"start\":35588},{\"end\":36062,\"start\":35975},{\"end\":36429,\"start\":36331},{\"end\":36737,\"start\":36660},{\"end\":37068,\"start\":36978},{\"end\":37341,\"start\":37285},{\"end\":37655,\"start\":37590},{\"end\":38024,\"start\":37971},{\"end\":38190,\"start\":38125},{\"end\":38527,\"start\":38477},{\"end\":39020,\"start\":38954},{\"end\":39198,\"start\":39129},{\"end\":39311,\"start\":39248},{\"end\":39617,\"start\":39552},{\"end\":39958,\"start\":39917},{\"end\":40397,\"start\":40272},{\"end\":40620,\"start\":40554},{\"end\":40942,\"start\":40879},{\"end\":41160,\"start\":41102},{\"end\":41476,\"start\":41414},{\"end\":41802,\"start\":41685}]", "bib_author": "[{\"end\":33959,\"start\":33948},{\"end\":33973,\"start\":33959},{\"end\":33985,\"start\":33973},{\"end\":34000,\"start\":33985},{\"end\":34012,\"start\":34000},{\"end\":34117,\"start\":34108},{\"end\":34125,\"start\":34117},{\"end\":34134,\"start\":34125},{\"end\":34145,\"start\":34134},{\"end\":34157,\"start\":34145},{\"end\":34169,\"start\":34157},{\"end\":34184,\"start\":34169},{\"end\":34193,\"start\":34184},{\"end\":34203,\"start\":34193},{\"end\":34213,\"start\":34203},{\"end\":34224,\"start\":34213},{\"end\":34240,\"start\":34224},{\"end\":34251,\"start\":34240},{\"end\":34263,\"start\":34251},{\"end\":34272,\"start\":34263},{\"end\":34282,\"start\":34272},{\"end\":34293,\"start\":34282},{\"end\":34299,\"start\":34293},{\"end\":34309,\"start\":34299},{\"end\":34318,\"start\":34309},{\"end\":34326,\"start\":34318},{\"end\":34336,\"start\":34326},{\"end\":34346,\"start\":34336},{\"end\":34354,\"start\":34346},{\"end\":34363,\"start\":34354},{\"end\":34372,\"start\":34363},{\"end\":34382,\"start\":34372},{\"end\":34396,\"start\":34382},{\"end\":34407,\"start\":34396},{\"end\":34420,\"start\":34407},{\"end\":34430,\"start\":34420},{\"end\":34656,\"start\":34649},{\"end\":34664,\"start\":34656},{\"end\":34672,\"start\":34664},{\"end\":34678,\"start\":34672},{\"end\":34685,\"start\":34678},{\"end\":34881,\"start\":34870},{\"end\":34894,\"start\":34881},{\"end\":34998,\"start\":34988},{\"end\":35010,\"start\":34998},{\"end\":35017,\"start\":35010},{\"end\":35030,\"start\":35017},{\"end\":35506,\"start\":35496},{\"end\":35518,\"start\":35506},{\"end\":35688,\"start\":35681},{\"end\":35698,\"start\":35688},{\"end\":35705,\"start\":35698},{\"end\":35715,\"start\":35705},{\"end\":35723,\"start\":35715},{\"end\":35731,\"start\":35723},{\"end\":35739,\"start\":35731},{\"end\":36071,\"start\":36064},{\"end\":36078,\"start\":36071},{\"end\":36086,\"start\":36078},{\"end\":36438,\"start\":36431},{\"end\":36446,\"start\":36438},{\"end\":36454,\"start\":36446},{\"end\":36748,\"start\":36739},{\"end\":36758,\"start\":36748},{\"end\":36772,\"start\":36758},{\"end\":36784,\"start\":36772},{\"end\":36794,\"start\":36784},{\"end\":36804,\"start\":36794},{\"end\":36819,\"start\":36804},{\"end\":36828,\"start\":36819},{\"end\":37079,\"start\":37070},{\"end\":37091,\"start\":37079},{\"end\":37099,\"start\":37091},{\"end\":37111,\"start\":37099},{\"end\":37353,\"start\":37343},{\"end\":37367,\"start\":37353},{\"end\":37674,\"start\":37657},{\"end\":37686,\"start\":37674},{\"end\":37804,\"start\":37797},{\"end\":37811,\"start\":37804},{\"end\":37820,\"start\":37811},{\"end\":37826,\"start\":37820},{\"end\":37835,\"start\":37826},{\"end\":37843,\"start\":37835},{\"end\":37851,\"start\":37843},{\"end\":37860,\"start\":37851},{\"end\":37875,\"start\":37860},{\"end\":37887,\"start\":37875},{\"end\":38038,\"start\":38026},{\"end\":38050,\"start\":38038},{\"end\":38060,\"start\":38050},{\"end\":38073,\"start\":38060},{\"end\":38085,\"start\":38073},{\"end\":38200,\"start\":38192},{\"end\":38208,\"start\":38200},{\"end\":38214,\"start\":38208},{\"end\":38222,\"start\":38214},{\"end\":38229,\"start\":38222},{\"end\":38240,\"start\":38229},{\"end\":38536,\"start\":38529},{\"end\":38545,\"start\":38536},{\"end\":38557,\"start\":38545},{\"end\":38563,\"start\":38557},{\"end\":38571,\"start\":38563},{\"end\":38582,\"start\":38571},{\"end\":39028,\"start\":39022},{\"end\":39036,\"start\":39028},{\"end\":39051,\"start\":39036},{\"end\":39061,\"start\":39051},{\"end\":39069,\"start\":39061},{\"end\":39211,\"start\":39200},{\"end\":39221,\"start\":39211},{\"end\":39235,\"start\":39221},{\"end\":39321,\"start\":39313},{\"end\":39332,\"start\":39321},{\"end\":39344,\"start\":39332},{\"end\":39356,\"start\":39344},{\"end\":39369,\"start\":39356},{\"end\":39630,\"start\":39619},{\"end\":39641,\"start\":39630},{\"end\":39653,\"start\":39641},{\"end\":39974,\"start\":39960},{\"end\":39987,\"start\":39974},{\"end\":39998,\"start\":39987},{\"end\":40232,\"start\":40223},{\"end\":40242,\"start\":40232},{\"end\":40256,\"start\":40242},{\"end\":40270,\"start\":40256},{\"end\":40630,\"start\":40622},{\"end\":40637,\"start\":40630},{\"end\":40836,\"start\":40820},{\"end\":40952,\"start\":40944},{\"end\":40958,\"start\":40952},{\"end\":40966,\"start\":40958},{\"end\":40976,\"start\":40966},{\"end\":41170,\"start\":41162},{\"end\":41178,\"start\":41170},{\"end\":41190,\"start\":41178},{\"end\":41198,\"start\":41190},{\"end\":41486,\"start\":41478},{\"end\":41494,\"start\":41486},{\"end\":41502,\"start\":41494},{\"end\":41510,\"start\":41502},{\"end\":41519,\"start\":41510},{\"end\":41812,\"start\":41804},{\"end\":41820,\"start\":41812},{\"end\":41828,\"start\":41820},{\"end\":41836,\"start\":41828},{\"end\":41845,\"start\":41836}]", "bib_venue": "[{\"end\":34794,\"start\":34748},{\"end\":35333,\"start\":35197},{\"end\":35928,\"start\":35827},{\"end\":36263,\"start\":36174},{\"end\":36613,\"start\":36542},{\"end\":37230,\"start\":37179},{\"end\":37543,\"start\":37456},{\"end\":38429,\"start\":38342},{\"end\":38906,\"start\":38759},{\"end\":39543,\"start\":39456},{\"end\":39870,\"start\":39755},{\"end\":40174,\"start\":40087},{\"end\":40511,\"start\":40456},{\"end\":40811,\"start\":40739},{\"end\":41359,\"start\":41287},{\"end\":41625,\"start\":41572},{\"end\":42034,\"start\":41947},{\"end\":34050,\"start\":34012},{\"end\":34479,\"start\":34430},{\"end\":34746,\"start\":34685},{\"end\":34898,\"start\":34894},{\"end\":35172,\"start\":35030},{\"end\":35195,\"start\":35174},{\"end\":35579,\"start\":35518},{\"end\":35825,\"start\":35739},{\"end\":36172,\"start\":36086},{\"end\":36540,\"start\":36454},{\"end\":36877,\"start\":36828},{\"end\":37177,\"start\":37111},{\"end\":37454,\"start\":37367},{\"end\":37689,\"start\":37686},{\"end\":37963,\"start\":37908},{\"end\":38108,\"start\":38085},{\"end\":38327,\"start\":38240},{\"end\":38340,\"start\":38329},{\"end\":38744,\"start\":38582},{\"end\":38757,\"start\":38746},{\"end\":39121,\"start\":39069},{\"end\":39240,\"start\":39235},{\"end\":39454,\"start\":39369},{\"end\":39753,\"start\":39653},{\"end\":40085,\"start\":39998},{\"end\":40454,\"start\":40399},{\"end\":40724,\"start\":40637},{\"end\":40737,\"start\":40726},{\"end\":41045,\"start\":40976},{\"end\":41285,\"start\":41198},{\"end\":41570,\"start\":41519},{\"end\":41932,\"start\":41845},{\"end\":41945,\"start\":41934}]"}}}, "year": 2023, "month": 12, "day": 17}