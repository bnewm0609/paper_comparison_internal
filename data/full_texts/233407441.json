{"id": 233407441, "updated": "2023-10-06 05:02:23.062", "metadata": {"title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics", "authors": "[{\"first\":\"Artidoro\",\"last\":\"Pagnoni\",\"middle\":[]},{\"first\":\"Vidhisha\",\"last\":\"Balachandran\",\"middle\":[]},{\"first\":\"Yulia\",\"last\":\"Tsvetkov\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.13346", "mag": "3170432046", "acl": "2021.naacl-main.383", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/PagnoniBT21", "doi": "10.18653/v1/2021.naacl-main.383"}}, "content": {"source": {"pdf_hash": "1e376821cc70593dca931ad5c3969929a1d7e2e4", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2021.naacl-main.383.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.13346", "status": "GREEN"}}, "grobid": {"id": "55801225e367c034bf468c97a3c5f201cce83d6b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1e376821cc70593dca931ad5c3969929a1d7e2e4.txt", "contents": "\nUnderstanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics\nJune 6-11, 2021\n\nArtidoro Pagnoni apagnoni@cs.cmu.edu \nLanguage Technologies Institute Carnegie Mellon University\n\n\nVidhisha Balachandran \nLanguage Technologies Institute Carnegie Mellon University\n\n\nYulia Tsvetkov ytsvetko@cs.cmu.edu \nLanguage Technologies Institute Carnegie Mellon University\n\n\nUnderstanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics\n\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\nthe 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20214812\nModern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.\n\nIntroduction\n\nFactuality is defined as a measure of \"whether eventualities are characterized as corresponding to facts, possibilities, or situations that do not hold in the world\" (Sauri, 2008;Saur\u00ed and Pustejovsky, 2012). In summarization, this \"world\" is the article, which is taken as ground-truth, and the output summary must be faithful to the article's facts. Despite advancements in neural abstractive summarization (Narayan et al., 2018;Liu and Lapata, 2019;Lewis et al., 2020), \u223c30% of summaries have factual inconsistencies (Cao et al., 2018). With summarization being an integral component of information consumption, this highlights a need for ensuring summarization systems are factually consistent and developing methods for evaluating them.\n\nCommon evaluation metrics for summarization based on n-gram overlap -BLEU, ROUGE, and METEOR (Papineni et al., 2002;Lin, 2004;Lavie and Agarwal, 2007) -are insufficient to measure the factual correctness of summaries and fail to correlate with the human judgements of factuality (Falke et al., 2019;Kryscinski et al., 2019). More recent metrics proposed to improve the evaluation of summarization factuality (Kryscinski et al., 2020;Durmus et al., 2020;Wang et al., 2020;Maynez et al., 2020) cannot be compared due to the lack of common benchmarks. More critically, while these approaches differ in the way they model factuality, they all consider factuality as a binary concept, labeling summaries of any length as factual or non-factual. They do not provide any finegrained understanding of the factual errors made by different systems that could serve as an actionable feedback on a system's limitations.\n\nThe binary factuality of a text can be difficult to determine. Falke et al. (2019) show relatively low crowd-expert agreement, indicating the presence of subjectivity in the annotation process. Moreover, not all factual errors are equally important and the number of errors can have a significant impact on the perceived factuality of a text. This suggests that non-factuality should be modeled as a multidimensional construct and not a label.\n\nIn this work, we propose a linguistically motivated typology of factual errors for fine-grained analysis of factuality in summarization systems ( \u00a72). Our typology is theoretically grounded in frame semantics (Fillmore et al., 1976;Palmer et al., 2005) and linguistic discourse theory (Brown and Yule, 1983). It provides several benefits. First, we find that decomposing the concept of factuality in (relatively) well-defined and grounded categories makes the final binary decision more objective leading to near perfect agreement between crowd and expert annotators (\u03ba = 0.86). Second, this approach provides some measure of the degree of non-factuality both in terms of the quantity and the category of factual violations that appear Figure 1: We propose a linguistically grounded typology of factual errors. We select crowd workers to annotate summaries from two datasets according to this typology achieving near perfect agreement with experts. We collect FRANK, the resulting dataset, to benchmark factuality metrics and state-of-art summarization systems.\n\nin the text. This typology also provides us with the means to categorize the types of errors made by summarization systems, helping us gain deeper insights than simply categorizing content as factual or hallucinated.\n\nWe define an annotation protocol of factuality based on our typology and collect a dataset of human judgements over a diverse set of model generated summaries on the CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) datasets ( \u00a73). Through this dataset, we aim to both assess the factuality of summarization systems and benchmark recently proposed factuality metrics. In \u00a74 we discuss various state-of-art models and show a detailed analysis of the factual errors they make. Finally, in \u00a75 we evaluate multiple summarization metrics against our benchmark and show their strengths and weaknesses in detecting specific types of factual errors. Figure 1 shows an overview of this work.\n\n\nTypology of Factual Errors\n\nPrevious studies of factuality in summarization only distinguish factual and hallucinated content (Kryscinski et al., 2019;Maynez et al., 2020) and provide limited insights on the fine-grained types of factual errors. In the simplest case, factual errors appear within a single proposition. However, as summaries include several sentences, discourse markers describe relations across propositions. These cross-sentence links, such as causality or temporal ordering, can introduce inconsistencies with the article. Furthermore, information in the summary should be verifiable given the article. This understanding outlines different levels of linguistic structure where factual mistakes can arise in summaries: at the semantic frame level, at the discourse level, or because the content cannot be verified. Below we define a typology of factual errors further detailing these three levels. This typology is theoretically grounded in frame semantics (Fillmore et al., 1976;Baker et al., 1998;Palmer et al., 2005) and linguistic discourse analysis (Brown and Yule, 1983). Examples for each category are shown in Table 1.\n\n\nSemantic Frame Errors\n\nA semantic frame is a schematic representation of an event, relation, or state, which consists of a predicate and a list of participants, called frame elements (Baker et al., 1998). A semantic frame has both core and non-core frame elements (FE). Core frame elements are essential to the meaning of the frame, while non-core (e.g. location, time) provide additional descriptive information. Our first three categories capture factual errors in each of these components (frame, core and non-core FE) respectively.\n\nPredicate Error (PredE): Category PredE encompasses errors where the predicate in a summary statement is inconsistent with the source text. More generally, this represents cases where the frame from a summary statement does not align with what is expressed in the source text.\n\nEntity Error (EntE): Category EntE captures errors where the primary arguments (like entities) of the predicate are wrong or have the wrong attributes, although the relation was expressed in the original text. More generally, these account for cases where the core frame elements in a frame are wrong. This also captures directionality errors where the elements are interchanged (similar to agent-patient swap).\n\nCircumstance Error (CircE): In additional to the core arguments, predicates can be further specified using additional information or attributes that describe the circumstance in which the arguments\n\n\nCategory\n\nDescription Example\n\n\nPredE\n\n\nRelation Error\n\nThe predicate in the summary statement is inconsistent with the source article.\n\nThe Ebola vaccine was rejected by the FDA in 2019.\n\n\nEntE\n\nEntity Error The primary arguments (or their attributes) of the predicate are wrong.\n\nThe COVID-19 vaccine was approved by the FDA in 2019.\n\n\nCircE\n\nCircumstance Error The additional information (like location or time) specifying the circumstance around a predicate is wrong.\n\nThe first vaccine for Ebola was approved by the FDA in 2014.\n\nCorefE Coreference Error A pronoun/reference with wrong or nonexisting antecedent.\n\nThe first vaccine for Ebola was approved in 2019. They say a vaccine for COVID-19 is unlikely to be ready this year.\n\n\nLinkE\n\nDiscourse Link Error Error in how multiple statements are linked together in the discourse (for example temporal ordering/causal link).\n\nTo produce the vaccine, scientists have to show successful human trials, then sequence the DNA of the virus.\n\n\nOutE\n\nOut of Article Error The statement contains information not present in the source article.\n\nChina has already started clinical trials of the COVID-19 vaccine.\n\n\nGramE Grammatical Error\n\nThe grammar of the sentence is so wrong that it becomes meaningless.\n\nThe Ebola vaccine accepted have already started. and predicates interact (e.g. location, time, manner, direction, modality). Category CircE captures errors where one or more such attributes (non-core frame elements within a frame) are wrong.\n\n\nDiscourse Errors\n\nThe communicative intent of an author is also expressed through relations that hold between parts of the text. Factual errors in summarized text can often extend beyond a single semantic frame introducing erroneous links between discourse segments. Below we outline such categories of errors which are grounded in discourse analysis and rhetorical structure theory (RST) (Brown and Yule, 1983;Mann and Thompson, 1988). RST is an elaborate system for annotating coherence relations in discourse. Some examples of such relations include: \"Elaboration\", \"Background\", \"Motivation\", and \"Volitional Cause\". Here we depart from semantic frame terminology as its rooting in a single frame does not allow us to represent such errors.\n\nCoreference Error (CorefE): Category CorefE accounts for errors where pronouns and other types of references to previously mentioned entities either are incorrect or have no clear antecedents, making them ambiguous.\n\nDiscourse Link Error (LinkE): Category LinkE encompasses errors involving a discourse link between different statements. These include errors of incorrect temporal ordering or incorrect discourse links (e.g. RST relations, discourse connectors) between statements.\n\n\nContent Verifiability Errors\n\nOften statements in a summary cannot be verified against the source text due to difficulty in aligning them to the source. Below we outline two categories of errors for such cases.\n\n\nOut of Article Error (OutE):\n\nSince summaries of a document should only contain information that can be deduced from the original text, we include a category for such errors OutE (prior work refers to this as extrinsic hallucinations (Maynez et al., 2020)).\n\nGrammatical Error (GramE): We use GramE to categorize statements that are not well formed. When grammatical mistakes make the meaning of a statement incomprehensible or ambiguous, it cannot be verified against the source and is thus considered trivially wrong. Minor grammatical errors are acceptable. Finally, for completeness in our annotation exercise, we add two additional categories Others (OthE) for factually errors that do not correspond to any of the above categories and Not an Error (NE) for statements that do not contain any errors.\n\n\nDataset Creation\n\nBeyond theoretical grounding, we empirically verify our typology through large scale human annota-tions of five abstractive summarization models on the CNN/DM dataset and four on the XSum dataset. Through our dataset, we aim to have a broad coverage of different types of errors made by neural summarization systems, with human judgements on their fine-grained factuality errors.\n\nAnnotation Data For the annotation, we include model summaries from CNN/DM and XSum datasets as they present different characteristics. CNN/DM summaries are longer, with three sentences on average, while XSum has only single sentence summaries. Having longer summaries is crucial to identify discourse level errors. On the other hand, XSum summaries are more abstractive and include more factual errors on average (Maynez et al., 2020). For a diverse set of model summaries, we collect publicly available model outputs from different summarization models with differing factuality capabilities. For the CNN/DM dataset, we use model outputs from a LSTM Seq-to-Seq model (S2S) (Rush et al., 2015), a Pointer-Generator Network (PGN) model (See et al., 2017) Annotation Collection Using the above model generated summaries, we collect human annotations from three independent annotators for 250 articles from each dataset (with a total of 1250 model outputs on CNN/DM and 1000 on XSum). We annotate each sentence of a summary to break the judgement of factuality into smaller units. We present sentences in the context of the entire summary to identify discourse errors spanning multiple sentences. Annotations are a two step process: for each sentence in the summary, the annotator first selects whether the sentence is factual, and if marked not factual, identifies the category of each error based on our typology. 3 A sentence can be annotated with more than one category of errors to account for multiple errors within a sentence. We conduct the annotation task on the Amazon Mechanical Turk (MTurk) platform. To achieve high quality crowd-sourced annotations, we build an intuitive interface 4 which combines:\n\n1. Clear Instructions: We explain the annotation scheme without assuming linguistic knowledge and give several examples for each category. 2. Training and Evaluation: We setup training tutorials for first time users to train and provide feedback on the task. We also setup a qualification test which tests their understanding of our annotation scheme and require annotators to obtain >85% score to qualify. Further, we continuously evaluate annotators during the task against artificially generated factual errors to ensure continued high quality. 3. Fair Pay and Bonus: All workers are paid 50% more than the average American minimum wage. We offer bonuses for scores of 60% or above on the continuous evaluation, and for completing sets of 10 annotations. Further details on our interface are added in \u00a7A.6\n\nInter-Annotator Agreement: We report interannotator agreement in terms of Fleiss Kappa \u03ba (Fleiss, 1971). Following Durmus et al. (2020), we report the percentage p of annotators that agree with the majority class. Each datapoint in our dataset corresponds to a sentence in a summary. We compute agreement on all 4942 annotated sentences. On the annotation of whether a sentence is factual or not we obtain \u03ba = 0.58, with p = 91% of annotators agreeing with the majority class. As a comparison, Durmus et al. (2020) reports p = 76.7% average agreement. When all three annotators agree that a sentence is not factual, we obtain \u03ba = 0.39 with p = 73.9% of annotators agreeing with the majority class on the eight category annotation (seven categories of errors and \"other\") which indicate a moderate agreement.    Table 1. annotators and one expert annotator on 201 datapoints (10 summaries from CNN/DM and 10 summaries from XSum). We find a Kohen Kappa of \u03ba = 0.86 indicating nearly perfect agreement. Previous work found agreement of \u03ba = 0.65 between three crowd annotators and expert annotations of factuality (Falke et al., 2019). Even with more than nine workers, they report agreement with expert annotations of at most \u03ba = 0.74. This improvement validates the robustness of our annotation interface and protocol which achieves higher agreement with fewer workers.\n\n\nSummarization Model Analysis\n\nWe evaluate the performance of different summarization models in terms of factuality. Figure 2 visualizes the percentage of summaries with factual errors for each category model and dataset, with a breakdown of proportion of different error types within each. A summary is considered incorrect if it contains at least one sentence with a factual error. A sentence contains a factual error if the majority of annotators indicate the presence of an error (here we do not consider annotations where all three annotators disagree on the category).\n\nHow factual are generated summaries across different datasets? From our annotations, we observe that 60% of the summaries that were annotated contain at least one factual error. From Figure 2, we see that the XSum dataset has more factually incorrect model summaries (92%) than CNN/DM (43%). It poses more significant challenges in terms of factuality as all models produce > 80% summaries with factual errors, with the best model (BertS2S) producing 83% wrong summaries. On the CNN/DM dataset, while state-of-the-art pretrained models like BERTSum and BART have better factuality numbers, the percentage of factually incorrect summaries is still high (23% for BERTSum and 27% for BART). The proportion of errors across different categories vary widely between the two datasets. For the CNN/DM dataset, the most frequent classes of errors are Entity Error (EntE) and Coreference Error (CorefE). For the XSum dataset they are Out of Article Error (OutE) and Entity Error (EntE). Note that there are no discourse errors (CorefE, LinkE) in the XSum dataset because the data only contains single sentence summaries. Additionally, we observe that OthE makes up a very small percentage (\u223c 1%) of errors overall showing that our typology is complete with most errors being mapped to one of our existing categories.\n\nHow factual are generated summaries across different models? From Figure 2, we observe that LSTM based models like S2S and BUS generate many incorrect summaries. Interestingly, PGN on CNN/DM has fewer summaries with factual errors (26%) compared to S2S (74%) and BUS (62%) potentially due to the extractive nature of CNN/DM and the copy based objective in PGN. PGN has been previously shown to produce highly extractive summaries on CNN/DM copying large portions of text (often entire sentences) (Gehrmann et al., 2018;Balachandran et al., 2021). On the more abstractive dataset XSum, PGN produces > 96% factually incorrect summaries. We also observe that large-scale pretrained models improve factuality on both datasets, as also noted by Durmus et al. (2020), with more significant gains on CNN/DM. On CNN/DM, BERTSum and BART display half the error rate of BUS. In contrast, on XSum, BertS2S improves over non-pretrained models by \u223c 10% only, showing that XSum poses a significant challenge for factuality even in pretrained models.\n\nDifferent models also exhibit different distributions in the error categories. LSTM based mod-els have higher proportion of Grammatical Errors (GramE) while transformer and CNN based models have a lower proportion. For pretrained transformer models, we observe that the improved error-rate on the CNN/DM dataset can be attributed to improvements at the frame level (PredE, EntE, CircE) while the discourse level errors still remain a challenge. Errors CorefE, LinkE account for a higher proportion of errors in BERTSum and BART compared to the other models.\n\n\nFactuality Metric Evaluation\n\nWe propose the FRANK dataset resulting from the human annotation study as a common benchmark to assess different factuality metrics. We provide an evaluation protocol of factuality metrics, which controls for dataset biases, and a fine grained analysis of the strengths of each metric.\n\n\nBenchmark\n\nThe FRANK benchmark provides a diverse dataset for evaluating various metrics on their ability to capture factual errors. Notably, our benchmark has factual error diversity, as it covers all types of errors described in the typology in \u00a72, and data diversity as it combines 2250 summaries from different systems and datasets. Our annotations go beyond binary labels of factuality on a summary by providing fine-grained category annotations for every sentence. This allows us to determine how well each metric can capture each type of error. Furthermore, through averaging of sentence level judgements, we can also obtain a factuality scores (0 to 1 range) for a summary. To measure the degree that automated metrics capture a certain characteristic, we compute their correlation with human judgements and report Pearson correlation and Spearman rank correlation along with their p-values.\n\nWe evaluate different classes of metrics against the FRANK benchmark. We select four general summarization metrics. ROUGE, BLEU, and Meteor are n-gram based metrics and computed with respect to the reference summary. BERTScore (Zhang et al., 2020) (Wang et al., 2020) are two question answering and generation metrics (QGA). More details on the differences between these metrics is in \u00a7A.2.\n\n\nControlling for Dataset Biases\n\nSince our benchmark contains diverse summaries from different datasets and models, dataset biases can hamper accurate reporting. In Figure 3, we visually show correlations between two factuality metrics (FEQA and FactCC) and human judgement on the entire data and on partitions of the data. For both metrics, we notice that the slope (an unscaled measure of correlation) of the line fitted through the entire data (red line) is significantly larger. In FEQA, the dotted lines (fitted on subsets of the data of each model and dataset) are almost horizontal. This likely indicates the presence of a confounding variable associated with the properties of each system and dataset. This can lead to false measures of high correlation if not accounted for. To address this, we suggest to control for confounding variables using partial correlations. We include details on partial correlations in the Appendix. In this case, both the system and the dataset are taken to be confounding variables.\n\n\nResults\n\nIn   p-values indicating statistical significance.\n\nHow do different metrics correlate with human judgements? From Table 2 we observe that all metrics exhibit low correlations with human judgements of factuality. The best metric overall is FactCC with 0.20 Pearson and 0.30 Spearman correlation. Interestingly, we observe that general summarization metrics BLEU, Rouge, and ME-TEOR, and the OpenIE baseline have statistically significant correlations with factuality, close to FactCC (\u03c1 = 0.14 for Rouge-1 and METEOR versus \u03c1 = 0.20 for FactCC). The entailment metrics (FactCC and DAE) have the two highest correlations and are statistically significant. The two QGA metrics have lower overall correlation. FEQA's correlation is not statistically significant. QAGS has low, but significant correlation of \u03c1 = 0.06.\n\nHow well do different metrics capture errors in different datasets? In Figure 4, we observe that entailment metrics have significantly higher partial Pearson correlation on the CNN/DM dataset than XSum where their correlation is reduced by a factor of four. QAGS and the OpenIE baseline have similar behavior. This suggests that these metrics capture the error types from CNN/DM better that those from XSum. Specifically, XSum has uniquely high Out of Article (OutE) errors which they might not capture well. This also highlights the importance of data diversity in building and benchmarking factuality metrics to avoid overfit-ting to certain types of errors.\n\n\nHow well do different metrics capture errors from pretrained and non-pretrained models?\n\nOn the CNN/DM dataset we observe that entailment metrics and QAGS perform significantly better on non-pretrained models. This indicates that the artificial factual errors on which entailment metrics are trained on are closest to the mistakes that non-pretrained models make. This also suggests that the errors made by pretrained models might be more difficult to capture by these metrics. These trends are less clear on the XSum dataset which we again attribute to high Out of Article (OutE) errors in the pretrained and non-pretrained models (ref Figure 2) Figure 4 shows partial Pearson correlation on six subsets of the data. To understand capabilities of metrics across the broad categories of errors (semantic frame errors, discourse errors, and content verifiability errors) we perform an ablation study. For each category, we compute the variation in partial correlation with errors from that category omitted. In Figure 5, we visualize the influence of a given type of error using the variation for each metric and category. A higher positive bar indicates that the error type was a significant contributer to the overall correlation (or metric highly correlates with error) causing the correlation without it to drop.\n\n\nError Analysis\n\nGeneral Summarization metrics Unsurprisingly, we observe that Rouge L is best correlated with content verifiability errors (which contains Out of Article Errors) as n-gram matches detect them. Rouge L has negative correlation with semantic frame errors and low correlation with discourse level errors indicating that n-gram matching fails to capture them. We observe that OpenIE is more correlated with semantic frame errors. The metric matches entities and verifies the predicate that relates them and hence is able to capture semantic frame errors. BertScore has low correlation overall, being more correlated with content verifiability errors and negatively correlated with discourse errors.\n\nQGA metrics Both QGA metrics have negative correlation with discourse errors suggesting that QGA metrics are not able to capture coreference errors or discourse link errors potentially due to the entity oriented questions in their training data. FEQA additionally is also negatively correlated with semantic frame errors and has low positive correlation with content verifiability errors. In contrast QAGS is best correlated with semantic frame errors.\n\nEntailment metrics Both entailment metrics correlate well with semantic frame and content verifiability errors. DAE has the highest correla-tion of all metrics with discourse errors suggesting that entailment at the dependency level can help model discourse errors (CorefE and LinkE). FactCC is nearly uncorrelated in this category, indicating that artificially generated factual errors need to go beyond simple pronoun swaps to train models to capture discourse errors. FactCC had best overall partial correlation which can be attributed to FactCC being able to capture semantic frame and content verifiability errors well. Question Generation and Answering (QGA) FEQA (Durmus et al., 2020) and QAGS (Wang et al., 2020) are two metrics which reduce factuality evaluation to question generation and answering. These methods use a question generation model to obtain questions from the output summary and a question answering model to answer them, separately using the article and the output summary. \n\n\nRelated Work\n\n\nConclusion\n\nIn this work we provide a linguistically grounded typology of factual errors which we use to collect FRANK, a dataset of human annotations of 2250 summaries covering both CNN/DM and XSum datasets. We use FRANK to assess the factuality of summarization systems and benchmark recently proposed factuality metrics highlighting the types of errors they can capture. With the FRANK benchmark we have started moving away from a summary-level binary understanding of factuality.\n\n\nEthical Considerations\n\nWe have collected crowd annotations using the Amazon Mechanical Turk platform. Workers were paid 50% more than the average American minimum wage and offered additional bonuses as an incentive to maintain high quality work. No information about the workers will be released and worker IDs will be anonymized. \n\n\nA.2 Metrics\n\nIn this work we compare the following five metrics.\n\nBERTScore (Zhang et al., 2020): We report BERTScore Precision, Recall, and F1 between the model output and the reference summary. Our experiments show that recall and F1 do not correlate as well with the human judgement of factuality for BERTScore.\n\nOpenIE : We use a simple baseline based on OpenIE (Banko et al., 2007) and Sentence-BERT (Reimers and Gurevych, 2019). We use OpenIE (Banko et al., 2007) to extract subject-relationobject triplets from the article, reference summary, and model generated summary. We consider binary relations only and thus use the first two arguments of the relation. 5 After replacing corefering entity mentions with the main mention of the cluster 6 , we use BERT base Sentence-BERT (Reimers and Gurevych, 2019) to obtain embeddings of each element of the subject-relation-object triplets extracted by OpenIE. Two relation triplets are considered to be equivalent if their embeddings have cosine similarity higher than a threshold for all three elements of the triplet (we use 0.6 as threshold after a grid search between 0.5 and 0.9 on data from our pilot study).\n\nFEQA (Durmus et al., 2020): FEQA is a question generation and answering (QGA) factuality metric. We relied on the original implementation of the authors for this metric as well as their pretrained model weights. We used the full summary to generate questions and we answer them both using the summary and article text.\n\nQAGS (Wang et al., 2020): QAGS is another QGA metric. The authors kindly provided outputs on the FRANK benchmark generating 10 questions for each summary.\n\nDAE (Goyal and Durrett, 2020): DAE is an entailment classification metric that operates on dependencies. The authors kindly provided outputs on the FRANK benchmark. We note that the model was trained with a max length of 128 after concatenating both article and summary. The CNN/DM articles can be significantly longer, thus the results reported for this metric involve truncating parts of the article.\n\nFactCC (Kryscinski et al., 2020): FactCC is an entailment classification metric. We use the sentences of the model generated summary as input claims to the entailment classifier FactCC. For each sentence we obatain a binary factuality label. We take the average of these labels as the factuality score for the summary.\n\n\nA.3 Summarization System Analysis Details\n\nSee Table 1 for more details.\n\n\nA.4 Hotelling Williams Test\n\nThe correlation numbers in Table 2 should be read in combination with the pairwise Hotelling-Williams test Graham (2015) results in Table 4. The highlighted numbers indicate pairs of models for which the difference in correlation is statistically significant. We use partial correlations to run the test and compute metric-metric correlations.\n\n\nA.5 Mutual Exclusiveness of typology:\n\nTo understand if our annotations are mutually exclusive, we study cases where two annotators agree on the error category (majority class) and one disagrees (minority class). In Figure 6, we report the confusion between majority and minority classes. For each category as majority, we report the distribution of other categories as minority.\n\nWe observe that all categories with the exception of OutE are frequently confused with NE which stands for no factual error. This primarily due to the noise in the annotations collected by crowd workers. However, for category CorefE (coreference errors) the confusion is significantly higher with 69.7%. We have noticed the same trend in practice tutorials: crowd annotators easily overlook situations where the correct pronoun is used (in terms of number and gender) but no antecedent appears in the summary. Intuitively after reading the article, unless paying particular attention, it is easy to subconsciously associate referring expressions with entities in the article without noticing their absence in the summary. The error persists despite stating the scenario explicitly in the instructions. This indicates an issue with annotators rather than annotation scheme.\n\nThe other trend that we observe is that categories PredE (wrong relation) and CircE (wrong modifier) are often confused with OutE (outside information). In our definition of OutE, outside information corresponds to the presence of entities not mentioned in the article or relations that cannot be verified based on the article. The confusion with PredE indicates that annotators can have different judgements on whether a relation is verifiable based on the article. Similarly, but to a lesser degree, wrong circumstantial information might be considered unverifiable given the article.\n\nFinally, there were relatively few discourse con- text errors LinkE, so the analysis is less statistically significant. Discourse context errors correspond to using a wrong connectors between different facts, for example different logical links. These were confused with PredE and EntE (wrong relation). The distinction between the two errors lies in the confusion between what an entity and a fact are, since PredE occurs at the frame level while LinkE at the discourse level. Note, that there was no confusion in the other direction (PredE being confused with LinkE).\n\n\nA.6 Annotation Setup Details\n\nBelow are more details on the annotation set up.\n\nClear Instructions We explain the annotation scheme without assuming linguistic knowledge and give several examples for each category. We also provide a practival ste-by-step to determine the category of the errors.\n\nTraining Every first-time user has to go through a tutorial which exercises the comprehension of the annotation scheme. The tutorial presents an article and several hand-crafted summaries of the article that need to be annotated. It is designed to be very similar to the actual annotation task and to contain at least one occurrence of each category of error. Feedback is provided when a user selects the wrong category of error. This tutorial is not used to evaluate users, only to help them understand the different categories in a practical setting.   Qualification test To participate in the annotation, users have to obtain a minimum score of 85% on a qualification test. The test comprehends an article and several summaries to be annotated. It contains at least one instance of each category of error. We use this test to verify that users can effectively recognize error categories. This ensures that users are able to perform the task correctly, but does not enforce that high standards of work quality are maintained throughout the annotation task.\n\nContinuous evaluation We continuously evaluate a user by verifying that they read the text. For every article that is annotated, we ask to identify one of three entities that was not present in the article. We also monitor the annotations on artificially altered sentences that are randomly inserted at the end of summaries. Wrong sentences contain one of the following errors: negation of declarative sentences (PredE), pronoun swap (CorefE), sample sentence from another article (OutE), word scrambling (GramE). We immediately block users that fail the entity test or perform poorly on these sentences (less than 50% of correct answers on altered sentences) to ensure high quality annotations.\n\nBonuses All workers are paid 50% more than the average American minimum wage but we offer bonuses for scores of 60% or above on the continuous evaluation, and for completion a sequences of 10 annotations. We observe that bonuses increase the percentage of users with high continuous evaluation scores (<10% blocked users with bonuses versus 30% without bonuses).\n\n\nA.7 Correlation with Confounding Variables\n\nPartial correlation measures the degree of association between two random variables, with the effect of a set of controlling random variables removed. Although we are unaware of the exact confounding variable, we use the categorical variable C of which system and dataset the summary was generated from. Let M k represent the output of metric k on the summaries. To compute partial correlation between M k and human judgements H which we treat as random variables, we solve the two regression problems M k |C = c \u223c w M k c and H|C = c \u223c w H c and get the residuals:\n\u2206M k = M k \u2212\u0175 M k C \u2206H = M k \u2212\u0175 H C\nAnd then calculate the correlation between these residuals \u03c1(\u2206M k , \u2206H) instead of the original random variables. Since partial correlations are proper correlations between random variables, we can apply statistical significance tests without any modification.\n\n\nA.8 Annotation Interface\n\nWe include screenshots of the annotation interface which we will make available.     \n\n\n, a Bottom-Up Summarization (BUS) model (Gehrmann et al., 2018), a Bert based Extractive-Abstractive model (BertSum) (Liu and Lapata, 2019) and a jointly pretrained transformer based encoder-decoder model BART (Lewis et al., 2020). For the XSum dataset, we collect model outputs from a Topic-Aware CNN Model (Narayan et al., 2018), a Pointer-Generator Network (PGN) model, a randomly initialized (TransS2S) (Vaswani et al., 2017) and one initialized with Bert-Base (BertS2S) (Devlin et al., 2019). 2 Details of the models used are provided in \u00a7A.1.\n\nFigure 2 :\n2Proportion of summaries with factual errors based on collected annotations, with breakdown of the categories of errors within. Full specification of categories of errors in\n\nFigure 3 :\n3computes BERT (Devlin et al., 2019) contextual embeddings on summary and source article and measures distances between matched embeddings. We select five metrics focused on factuality. As Goodrich et al. (2019), we use a simple OpenIE (Banko et al., 2007) baseline. This involves extracting OpenIE triples and matching them through sentence embeddings (Reimers Correlation between metrics and human judgement on subsets of data. The x and y axis represent the human judgement the metric scores respectively. The red line is a linear regression fitted on full data. Each dotted line is a linear regression fitted on a model-dataset subset. Each colored point has coordinates equal to average factuality judgement, and metric score for its corresponding partition. and Gurevych, 2019). FactCC (Kryscinski et al., 2020) and DAE (Goyal and Durrett, 2020) are entailment based metrics. FactCC operates with sentences as claims, while DAE uses dependency level entailment. FEQA (Durmus et al., 2020) and QAGS\n\n\n0.00 0.36 0.00 0.33 0.00 0.07 0.02 0.25 0.00\n\nFigure 4 :Figure 5 :\n45Partial Pearson correlation on different partitions of the data. Entailment metrics have highest correlation on pretrained models in the CNN/DM dataset. Their performance degrades significantly on XSum. Variation in partial Pearson correlation when omitting error types. Higher variation indicates greater influence of an error type in the overall correlation.\n\n\net al. (2019) and Fabbri et al. (2020) find that standard n-gram based metrics have low correlation with human judgements of factuality. Motivated by this, several automated metrics falling in two paradigms were proposed to improve the evaluation of factuality. Entailment Classification Goodrich et al. (2019); Kryscinski et al. (2020); Maynez et al. (2020); Goyal and Durrett (2020) model factuality as entailment classification breaking down the summary into smaller units, such as sentences, which are verified against the original article. However, modeling factuality as a classification task requires supervision on factual and hallucinated data. FactCC (Kryscinski et al., 2020) is trained on the CNN/DM dataset augmented with four types of artificial mistakes as supervision.\n\nFigure 7 :\n7Instructions can be toggled.\n\nFigure 8 :\n8The sentences being annotated is highlighted in yellow. Relevant text is underlined in the article plain text.\n\nFigure 9 :\n9After selecting that the sentence is not factual annotators choose the category of error.\n\nFigure 10 :\n10Articles web pages are provided.\n\nFigure 11 :\n11Entity question to ensure annotators read the text.\n\nTable 1 :\n1Typology of factual errors. Original text for the examples: The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started.\n\nTable 2 ,\n2we report the partial Pearson correlation \nand Spearman rank correlation coefficients with \nhuman judgements for each metric, along with their \n\n\nTable 2 :\n2Partial Pearson correlation and Spearman rank correlation coefficients and p-values between human judgements and metrics scores. Comparisons should be made along with the pairwise Williams test found inTable 4.\n\n\nGillian Brown and G. Yule. 1983. Discourse Analysis. Cambridge Textbooks in Linguistics. Cambridge University Press.Vidhisha Balachandran, Artidoro Pagnoni, Jay Yoon \nLee, Dheeraj Rajagopal, Jaime Carbonell, and Yu-\nlia Tsvetkov. 2021. StructSum: Summarization via \nstructured representations. In Proceedings of the \n16th Conference of the European Chapter of the \nAssociation for Computational Linguistics: Main \nVolume, pages 2575-2585, Online. Association for \nComputational Linguistics. \n\nMichele Banko, Michael J. Cafarella, Stephen Soder-\nland, Matt Broadhead, and Oren Etzioni. 2007. \nOpen information extraction from the web. In Pro-\nceedings of the 20th International Joint Conference \non Artifical Intelligence, IJCAI'07, pages 2670-\n2676, San Francisco, CA, USA. Morgan Kaufmann \nPublishers Inc. \n\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. \nFaithful to the original: Fact aware neural abstrac-\ntive summarization. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence, \n(AAAI-18), the 30th innovative Applications of Arti-\nficial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artificial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA, \nFebruary 2-7, 2018, pages 4784-4791. AAAI Press. \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2019. BERT: Pre-training of \ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference \nof the North American Chapter of the Association \nfor Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers), \npages 4171-4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics. \n\nEsin Durmus, He He, and Mona Diab. 2020. FEQA: A \nquestion answering evaluation framework for faith-\nfulness assessment in abstractive summarization. In \nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5055-\n5070, Online. Association for Computational Lin-\nguistics. \n\nAlexander R Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan \nMcCann, Caiming Xiong, Richard Socher, \nand Dragomir Radev. 2020. \nSummeval: Re-\nevaluating summarization evaluation. \narXiv \npreprint arXiv:2007.12626. \nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie \nUtama, Ido Dagan, and Iryna Gurevych. 2019. \nRanking generated summaries by correctness: An in-\nteresting but challenging application for natural lan-\nguage inference. In Proceedings of the 57th Annual \nMeeting of the Association for Computational Lin-\nguistics, pages 2214-2220, Florence, Italy. Associa-\ntion for Computational Linguistics. \n\nCharles J Fillmore et al. 1976. Frame semantics and the \nnature of language. In Proceedings of the Annals of \nthe New York Academy of Sciences: Conference on \nthe origin and development of language and speech, \nvolume 280, pages 20-32. New York. \n\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin, \n76(5):378. \n\nSebastian Gehrmann, Yuntian Deng, and Alexander \nRush. 2018. Bottom-up abstractive summarization. \nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing, \npages 4098-4109, Brussels, Belgium. Association \nfor Computational Linguistics. \n\nBen Goodrich, Vinay Rao, Peter J. Liu, and Moham-\nmad Saleh. 2019. Assessing the factual accuracy \nof generated text. In Proceedings of the 25th ACM \nSIGKDD International Conference on Knowledge \nDiscovery & Data Mining, KDD 2019, Anchorage, \nAK, USA, August 4-8, 2019, pages 166-175. ACM. \n\nTanya Goyal and Greg Durrett. 2020. Evaluating fac-\ntuality in generation with dependency-level entail-\nment. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3592-3603, \nOnline. Association for Computational Linguistics. \n\nYvette Graham. 2015. Re-evaluating automatic sum-\nmarization with BLEU and 192 shades of ROUGE. \nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages \n128-137, Lisbon, Portugal. Association for Compu-\ntational Linguistics. \n\nKarl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, \nand Phil Blunsom. 2015. Teaching machines to read \nand comprehend. In Proceedings of the Advances in \nNeural Information Processing Systems 28: Annual \nConference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec, \nCanada, pages 1693-1701. \n\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019. \nNeural text summarization: A critical evaluation. In \nProceedings of the 2019 Conference on Empirical \nMethods in Natural Language Processing and the \n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 540-\n551, Hong Kong, China. Association for Computa-\ntional Linguistics. \n\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, \nand Richard Socher. 2020. Evaluating the factual \nconsistency of abstractive text summarization. In \nProceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing (EMNLP), \npages 9332-9346, Online. Association for Computa-\ntional Linguistics. \n\nAlon Lavie and Abhaya Agarwal. 2007. METEOR: An \nautomatic metric for MT evaluation with high levels \nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine \nTranslation, pages 228-231, Prague, Czech Repub-\nlic. Association for Computational Linguistics. \n\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer \nLevy, Veselin Stoyanov, and Luke Zettlemoyer. \n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, \nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational \nLinguistics, pages 7871-7880, Online. Association \nfor Computational Linguistics. \n\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74-81, Barcelona, Spain. \nAssociation for Computational Linguistics. \n\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of \nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International \nJoint Conference on Natural Language Processing \n(EMNLP-IJCNLP), pages 3730-3740, Hong Kong, \nChina. Association for Computational Linguistics. \n\nWilliam C Mann and Sandra A Thompson. 1988. \nRhetorical structure theory: Toward a functional the-\nory of text organization. Text-interdisciplinary Jour-\nnal for the Study of Discourse, 8(3):243-281. \n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and \nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings \nof the 58th Annual Meeting of the Association for \nComputational Linguistics, pages 1906-1919, On-\nline. Association for Computational Linguistics. \n\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. \n2018. Don't give me the details, just the summary! \ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018 \nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797-1807, Brussels, Bel-\ngium. Association for Computational Linguistics. \n\nMartha Palmer, Daniel Gildea, and Paul Kingsbury. \n2005. The Proposition Bank: An annotated cor-\npus of semantic roles. Computational Linguistics, \n31(1):71-106. \n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of \nA Appendices \n\nA.1 Model details \n\nWe provide details of the models used in the human \nevaluation task to construct FRANK. \n\nA.1.1 CNN/DM datset \n\nOn the CNN/DM (Hermann et al., 2015) dataset we \nuse five different models. We use the preprocessed \nmodel outputs provided by Fabbri et al. (2020). \nS2S an LSTM based Sequence-to-Sequence with \nattention model (Rush et al., 2015) \nPGN an LSTM based Pointer-Generator Network \nwith Copy Mechanism (See et al., 2017) \nBUS Bottom-Up Summarization (Gehrmann \net al., 2018) -a Pointer-Generator model with a \ndata-efficient content selector to over-determine \nphrases in a source document that should be part \nof the summary. \nBERTSum summarization with pretrained en-\ncoders (Liu and Lapata, 2019) \nBART (Lewis et al., 2020) \n\nA.1.2 XSum dataset \n\nOn the XSum dataset (Narayan et al., 2018) we \nuse four different models. All model outputs for \nthis dataset are taken from (Maynez et al., 2020) \nPGN pointer-generator network from above (See \net al., 2017) \nTConvS2S Topic-Aware Convolution Sequence-\nto-Sequence (Narayan et al., 2018) \nTranS2S A randomly initialized Transformer \n(Vaswani et al., 2017) encoder-decoder model \nfine-tuned on the XSum dataset \nBERTS2S Transformer encoder-decoder model \nwith parameter sharing (Rothe et al., 2020) where \nboth encoder and decoder are initialized with the \nBERT-Base checkpoints (Devlin et al., 2019) and \nfine-tuned on XSum \n\n\n\n\nFigure 6: Confusion matrix of different types of errors. Entry at row i, column j corresponds to the frequency of annotations that have Fi as the majority class and for which disagreeing annotator selected Fj.NE \nPredE \nEntE \nCircE \nCorefE \nConE \nOutE \nGramE \nOthE \n\nNE \nPredE \nEntE \nCircE \nCorefE \nConE \nOutE \nGramE \nOthE \n0.0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n\n\nTable 3 :\n3Proportion of summaries that include factual errors, with breakdown of the categories of errors according to our human study. F8 corresponds to errors that are not captured by our typology. Full specification of categories of errors inTable 1. BMET R-1 R-L BS-P OpIE FEQA QAGS DAE FCCBLEU \n-\n0.83 0.77 0.85 0.04 0.26 \n0.03 \n-0.01 \n0.05 0.06 \nMETEOR 0.83 \n-\n0.87 0.85 0.04 0.28 \n0.02 \n-0.02 \n0.08 0.07 \nRouge-1 \n0.77 0.87 \n-\n0.89 0.04 0.21 \n0.01 \n-0.03 \n0.09 0.07 \nRouge-L \n0.85 0.85 0.89 \n-\n0.03 0.21 \n0.01 \n-0.04 \n0.08 0.07 \nBERTS P \n0.04 0.04 0.04 0.03 \n-\n0.00 \n0.00 \n0.02 \n-0.02 -0.04 \nOpenIE \n0.26 0.28 0.21 0.21 0.00 \n-\n-0.01 \n0.09 \n0.10 0.15 \nFEQA \n0.03 0.02 0.01 0.01 0.00 -0.01 \n-\n-0.01 \n0.03 0.04 \nQAGS \n-0.01 -0.02 -0.03 -0.04 0.02 0.09 \n-0.01 \n-\n0.08 0.09 \nDAE \n0.05 0.08 0.09 0.08 -0.02 0.10 \n0.03 \n0.08 \n-\n0.10 \nFactCC \n0.06 0.07 0.07 0.07 -0.04 0.15 \n0.04 \n0.09 \n0.10 \n-\n\n\n\nTable 4 :\n4Pearson correlation between metrics. If value is in green, the metrics are not the same significant to the 0.05 threshold with the Hotelling Williams test.\nCode, data, and online leaderboard will be available at https://github.com/artidoro/frank\nAs we use publicly available model outputs, the summaries across different datasets are from different models owing to their availability.\nWe experimented with Likert scale evaluation of full summaries in a pilot study. Such an annotation would not provide precise information about where in the summary an error appears and also resulted in lower agreement. Hence, we opted for sentence level judgements.4  We make the interface available for future human annotations that follow our typology\nWe use the model and implementation from(Stanovsky et al., 2018) for OpenIE extraction. 6 https://github.com/huggingface/ neuralcoref\nAcknowledgementsThe authors are grateful to the anonymous reviewers for their feedback, and to Anjalie Field, Rishabh Joshi, Alissa Ostapenko, Dheeraj Rajagopal, Evangelia Spiliopoulou, Shuly Wintner, and the members of the Tsvetshop group for their invaluable feedback and support in various stages of the project. This material is based upon work supported by the DARPA CMO under Contract No. HR001120C0124, and in part by the National Science Foundation under Grants No. IIS2040926 and No. IIS2007960. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily state or reflect those of the United States Government or any agency thereof.\nThe Berkeley FrameNet project. Collin F Baker, Charles J Fillmore, John B Lowe, 10.3115/980845.98086036th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics. Montreal, Quebec, Canada1Association for Computational LinguisticsCollin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th An- nual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, pages 86-90, Montreal, Quebec, Canada. Association for Compu- tational Linguistics.\n\nthe 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, Pennsylvania, USAAssociation for Computational Linguisticsthe 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\nSentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\n\nLeveraging pre-trained checkpoints for sequence generation tasks. Sascha Rothe, Shashi Narayan, Aliaksei Severyn, 10.1162/tacl_a_00313Transactions of the Association for Computational Linguistics. 8Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for se- quence generation tasks. Transactions of the Asso- ciation for Computational Linguistics, 8:264-280.\n\nA neural attention model for abstractive sentence summarization. Alexander M Rush, Sumit Chopra, Jason Weston, 10.18653/v1/D15-1044Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsAlexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379-389, Lisbon, Portugal. Association for Computational Linguistics.\n\nA Factuality Profiler for Eventualities in Text. Roser Sauri, USABrandeis UniversityPh.D. thesisRoser Sauri. 2008. A Factuality Profiler for Eventuali- ties in Text. Ph.D. thesis, Brandeis University, USA.\n\nAre you sure that this happened? assessing the factuality degree of events in text. Roser Saur\u00ed, James Pustejovsky, 10.1162/COLI_a_00096Computational Linguistics. 382Roser Saur\u00ed and James Pustejovsky. 2012. Are you sure that this happened? assessing the factuality de- gree of events in text. Computational Linguistics, 38(2):261-299.\n\nGet to the point: Summarization with pointergenerator networks. Abigail See, J Peter, Christopher D Liu, Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Association for Computational LinguisticsAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073- 1083, Vancouver, Canada. Association for Computa- tional Linguistics.\n\nSupervised open information extraction. Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, Ido Dagan, 10.18653/v1/N18-1081Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1Association for Computational LinguisticsGabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan. 2018. Supervised open information extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 885- 895, New Orleans, Louisiana. Association for Com- putational Linguistics.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Proceedings of the Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. the Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing SystemsLong Beach, CA, USAAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the Advances in Neu- ral Information Processing Systems 30: Annual Con- ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.\n\nAsking and answering questions to evaluate the factual consistency of summaries. Alex Wang, Kyunghyun Cho, Mike Lewis, 10.18653/v1/2020.acl-main.450Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac- tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.\n\nBertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Proceedings of the 8th International Conference on Learning Representations. the 8th International Conference on Learning RepresentationsAddis Ababa, Ethiopia2020OpenReview.netTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with BERT. In Proceedings of the 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n", "annotations": {"author": "[{\"end\":217,\"start\":119},{\"end\":301,\"start\":218},{\"end\":398,\"start\":302}]", "publisher": null, "author_last_name": "[{\"end\":135,\"start\":128},{\"end\":239,\"start\":227},{\"end\":316,\"start\":308}]", "author_first_name": "[{\"end\":127,\"start\":119},{\"end\":226,\"start\":218},{\"end\":307,\"start\":302}]", "author_affiliation": "[{\"end\":216,\"start\":157},{\"end\":300,\"start\":241},{\"end\":397,\"start\":338}]", "title": "[{\"end\":101,\"start\":1},{\"end\":499,\"start\":399}]", "venue": "[{\"end\":643,\"start\":501}]", "abstract": "[{\"end\":1692,\"start\":791}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1887,\"start\":1874},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1915,\"start\":1887},{\"end\":2139,\"start\":2117},{\"end\":2160,\"start\":2139},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2179,\"start\":2160},{\"end\":2246,\"start\":2228},{\"end\":2567,\"start\":2526},{\"end\":2577,\"start\":2567},{\"end\":2601,\"start\":2577},{\"end\":2750,\"start\":2730},{\"end\":2774,\"start\":2750},{\"end\":2884,\"start\":2859},{\"end\":2904,\"start\":2884},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2922,\"start\":2904},{\"end\":2942,\"start\":2922},{\"end\":3442,\"start\":3423},{\"end\":4037,\"start\":4014},{\"end\":4057,\"start\":4037},{\"end\":4112,\"start\":4090},{\"end\":5281,\"start\":5259},{\"end\":5328,\"start\":5286},{\"end\":5934,\"start\":5909},{\"end\":5954,\"start\":5934},{\"end\":6782,\"start\":6759},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6801,\"start\":6782},{\"end\":6821,\"start\":6801},{\"end\":6878,\"start\":6856},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7134,\"start\":7114},{\"end\":10269,\"start\":10247},{\"end\":10293,\"start\":10269},{\"end\":11556,\"start\":11535},{\"end\":12943,\"start\":12922},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13202,\"start\":13183},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13262,\"start\":13244},{\"end\":16161,\"start\":16141},{\"end\":18804,\"start\":18781},{\"end\":18830,\"start\":18804},{\"end\":19045,\"start\":19025},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21348,\"start\":21328},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21368,\"start\":21349},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27209,\"start\":27190},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28424,\"start\":28404},{\"end\":28714,\"start\":28687},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28761,\"start\":28733},{\"end\":28797,\"start\":28777},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29140,\"start\":29112},{\"end\":29521,\"start\":29500},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29839,\"start\":29820},{\"end\":30407,\"start\":30382},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":52345,\"start\":52321}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37551,\"start\":37001},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37737,\"start\":37552},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38753,\"start\":37738},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38800,\"start\":38754},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39185,\"start\":38801},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39972,\"start\":39186},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40014,\"start\":39973},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40138,\"start\":40015},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40241,\"start\":40139},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40289,\"start\":40242},{\"attributes\":{\"id\":\"fig_11\"},\"end\":40356,\"start\":40290},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40813,\"start\":40357},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40970,\"start\":40814},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41193,\"start\":40971},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50260,\"start\":41194},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50629,\"start\":50261},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51528,\"start\":50630},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":51696,\"start\":51529}]", "paragraph": "[{\"end\":2449,\"start\":1708},{\"end\":3358,\"start\":2451},{\"end\":3803,\"start\":3360},{\"end\":4866,\"start\":3805},{\"end\":5084,\"start\":4868},{\"end\":5780,\"start\":5086},{\"end\":6928,\"start\":5811},{\"end\":7466,\"start\":6954},{\"end\":7744,\"start\":7468},{\"end\":8157,\"start\":7746},{\"end\":8356,\"start\":8159},{\"end\":8388,\"start\":8369},{\"end\":8494,\"start\":8415},{\"end\":8546,\"start\":8496},{\"end\":8639,\"start\":8555},{\"end\":8694,\"start\":8641},{\"end\":8830,\"start\":8704},{\"end\":8892,\"start\":8832},{\"end\":8976,\"start\":8894},{\"end\":9094,\"start\":8978},{\"end\":9239,\"start\":9104},{\"end\":9349,\"start\":9241},{\"end\":9448,\"start\":9358},{\"end\":9516,\"start\":9450},{\"end\":9612,\"start\":9544},{\"end\":9855,\"start\":9614},{\"end\":10602,\"start\":9876},{\"end\":10819,\"start\":10604},{\"end\":11085,\"start\":10821},{\"end\":11298,\"start\":11118},{\"end\":11558,\"start\":11331},{\"end\":12106,\"start\":11560},{\"end\":12506,\"start\":12127},{\"end\":14219,\"start\":12508},{\"end\":15029,\"start\":14221},{\"end\":16398,\"start\":15031},{\"end\":16974,\"start\":16431},{\"end\":18283,\"start\":16976},{\"end\":19320,\"start\":18285},{\"end\":19879,\"start\":19322},{\"end\":20197,\"start\":19912},{\"end\":21099,\"start\":20211},{\"end\":21491,\"start\":21101},{\"end\":22514,\"start\":21526},{\"end\":22576,\"start\":22526},{\"end\":23340,\"start\":22578},{\"end\":24002,\"start\":23342},{\"end\":25320,\"start\":24094},{\"end\":26033,\"start\":25339},{\"end\":26487,\"start\":26035},{\"end\":27489,\"start\":26489},{\"end\":27990,\"start\":27519},{\"end\":28325,\"start\":28017},{\"end\":28392,\"start\":28341},{\"end\":28642,\"start\":28394},{\"end\":29493,\"start\":28644},{\"end\":29813,\"start\":29495},{\"end\":29969,\"start\":29815},{\"end\":30373,\"start\":29971},{\"end\":30693,\"start\":30375},{\"end\":30768,\"start\":30739},{\"end\":31143,\"start\":30800},{\"end\":31525,\"start\":31185},{\"end\":32399,\"start\":31527},{\"end\":32987,\"start\":32401},{\"end\":33558,\"start\":32989},{\"end\":33639,\"start\":33591},{\"end\":33856,\"start\":33641},{\"end\":34916,\"start\":33858},{\"end\":35613,\"start\":34918},{\"end\":35977,\"start\":35615},{\"end\":36589,\"start\":36024},{\"end\":36886,\"start\":36626},{\"end\":37000,\"start\":36915}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":36625,\"start\":36590}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6927,\"start\":6920},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15850,\"start\":15842},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22648,\"start\":22641},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30750,\"start\":30743},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30834,\"start\":30827},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":30939,\"start\":30932}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1706,\"start\":1694},{\"attributes\":{\"n\":\"2\"},\"end\":5809,\"start\":5783},{\"attributes\":{\"n\":\"2.1\"},\"end\":6952,\"start\":6931},{\"end\":8367,\"start\":8359},{\"end\":8396,\"start\":8391},{\"end\":8413,\"start\":8399},{\"end\":8553,\"start\":8549},{\"end\":8702,\"start\":8697},{\"end\":9102,\"start\":9097},{\"end\":9356,\"start\":9352},{\"end\":9542,\"start\":9519},{\"attributes\":{\"n\":\"2.2\"},\"end\":9874,\"start\":9858},{\"attributes\":{\"n\":\"2.3\"},\"end\":11116,\"start\":11088},{\"end\":11329,\"start\":11301},{\"attributes\":{\"n\":\"3\"},\"end\":12125,\"start\":12109},{\"attributes\":{\"n\":\"4\"},\"end\":16429,\"start\":16401},{\"attributes\":{\"n\":\"5\"},\"end\":19910,\"start\":19882},{\"attributes\":{\"n\":\"5.1\"},\"end\":20209,\"start\":20200},{\"attributes\":{\"n\":\"5.2\"},\"end\":21524,\"start\":21494},{\"attributes\":{\"n\":\"5.3\"},\"end\":22524,\"start\":22517},{\"end\":24092,\"start\":24005},{\"attributes\":{\"n\":\"5.4\"},\"end\":25337,\"start\":25323},{\"attributes\":{\"n\":\"6\"},\"end\":27504,\"start\":27492},{\"attributes\":{\"n\":\"7\"},\"end\":27517,\"start\":27507},{\"attributes\":{\"n\":\"8\"},\"end\":28015,\"start\":27993},{\"end\":28339,\"start\":28328},{\"end\":30737,\"start\":30696},{\"end\":30798,\"start\":30771},{\"end\":31183,\"start\":31146},{\"end\":33589,\"start\":33561},{\"end\":36022,\"start\":35980},{\"end\":36913,\"start\":36889},{\"end\":37563,\"start\":37553},{\"end\":37749,\"start\":37739},{\"end\":38822,\"start\":38802},{\"end\":39984,\"start\":39974},{\"end\":40026,\"start\":40016},{\"end\":40150,\"start\":40140},{\"end\":40254,\"start\":40243},{\"end\":40302,\"start\":40291},{\"end\":40367,\"start\":40358},{\"end\":40824,\"start\":40815},{\"end\":40981,\"start\":40972},{\"end\":50640,\"start\":50631},{\"end\":51539,\"start\":51530}]", "table": "[{\"end\":40970,\"start\":40826},{\"end\":50260,\"start\":41312},{\"end\":50629,\"start\":50472},{\"end\":51528,\"start\":50926}]", "figure_caption": "[{\"end\":37551,\"start\":37003},{\"end\":37737,\"start\":37565},{\"end\":38753,\"start\":37751},{\"end\":38800,\"start\":38756},{\"end\":39185,\"start\":38825},{\"end\":39972,\"start\":39188},{\"end\":40014,\"start\":39986},{\"end\":40138,\"start\":40028},{\"end\":40241,\"start\":40152},{\"end\":40289,\"start\":40257},{\"end\":40356,\"start\":40305},{\"end\":40813,\"start\":40369},{\"end\":41193,\"start\":40983},{\"end\":41312,\"start\":41196},{\"end\":50472,\"start\":50263},{\"end\":50926,\"start\":50642},{\"end\":51696,\"start\":51541}]", "figure_ref": "[{\"end\":4549,\"start\":4541},{\"end\":5748,\"start\":5740},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16525,\"start\":16517},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17167,\"start\":17159},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18359,\"start\":18351},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21666,\"start\":21658},{\"end\":23421,\"start\":23413},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24650,\"start\":24642},{\"end\":24660,\"start\":24652},{\"end\":25023,\"start\":25015},{\"end\":31370,\"start\":31362}]", "bib_author_first_name": "[{\"end\":53170,\"start\":53164},{\"end\":53172,\"start\":53171},{\"end\":53187,\"start\":53180},{\"end\":53189,\"start\":53188},{\"end\":53204,\"start\":53200},{\"end\":53206,\"start\":53205},{\"end\":54140,\"start\":54136},{\"end\":54155,\"start\":54150},{\"end\":55018,\"start\":55012},{\"end\":55032,\"start\":55026},{\"end\":55050,\"start\":55042},{\"end\":55421,\"start\":55412},{\"end\":55423,\"start\":55422},{\"end\":55435,\"start\":55430},{\"end\":55449,\"start\":55444},{\"end\":56042,\"start\":56037},{\"end\":56284,\"start\":56279},{\"end\":56297,\"start\":56292},{\"end\":56602,\"start\":56595},{\"end\":56609,\"start\":56608},{\"end\":56628,\"start\":56617},{\"end\":56630,\"start\":56629},{\"end\":57271,\"start\":57264},{\"end\":57289,\"start\":57283},{\"end\":57303,\"start\":57299},{\"end\":57320,\"start\":57317},{\"end\":58098,\"start\":58092},{\"end\":58112,\"start\":58108},{\"end\":58126,\"start\":58122},{\"end\":58140,\"start\":58135},{\"end\":58157,\"start\":58152},{\"end\":58170,\"start\":58165},{\"end\":58172,\"start\":58171},{\"end\":58186,\"start\":58180},{\"end\":58200,\"start\":58195},{\"end\":58929,\"start\":58925},{\"end\":58945,\"start\":58936},{\"end\":58955,\"start\":58951},{\"end\":59551,\"start\":59545},{\"end\":59565,\"start\":59559},{\"end\":59580,\"start\":59575},{\"end\":59591,\"start\":59585},{\"end\":59593,\"start\":59592},{\"end\":59610,\"start\":59606}]", "bib_author_last_name": "[{\"end\":53178,\"start\":53173},{\"end\":53198,\"start\":53190},{\"end\":53211,\"start\":53207},{\"end\":54148,\"start\":54141},{\"end\":54164,\"start\":54156},{\"end\":55024,\"start\":55019},{\"end\":55040,\"start\":55033},{\"end\":55058,\"start\":55051},{\"end\":55428,\"start\":55424},{\"end\":55442,\"start\":55436},{\"end\":55456,\"start\":55450},{\"end\":56048,\"start\":56043},{\"end\":56290,\"start\":56285},{\"end\":56309,\"start\":56298},{\"end\":56606,\"start\":56603},{\"end\":56615,\"start\":56610},{\"end\":56634,\"start\":56631},{\"end\":56643,\"start\":56636},{\"end\":57281,\"start\":57272},{\"end\":57297,\"start\":57290},{\"end\":57315,\"start\":57304},{\"end\":57326,\"start\":57321},{\"end\":58106,\"start\":58099},{\"end\":58120,\"start\":58113},{\"end\":58133,\"start\":58127},{\"end\":58150,\"start\":58141},{\"end\":58163,\"start\":58158},{\"end\":58178,\"start\":58173},{\"end\":58193,\"start\":58187},{\"end\":58211,\"start\":58201},{\"end\":58934,\"start\":58930},{\"end\":58949,\"start\":58946},{\"end\":58961,\"start\":58956},{\"end\":59557,\"start\":59552},{\"end\":59573,\"start\":59566},{\"end\":59583,\"start\":59581},{\"end\":59604,\"start\":59594},{\"end\":59616,\"start\":59611}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.3115/980845.980860\",\"id\":\"b0\",\"matched_paper_id\":2505531},\"end\":53757,\"start\":53133},{\"attributes\":{\"id\":\"b1\"},\"end\":54071,\"start\":53759},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1410\",\"id\":\"b2\",\"matched_paper_id\":201646309},\"end\":54944,\"start\":54073},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00313\",\"id\":\"b3\",\"matched_paper_id\":198967997},\"end\":55345,\"start\":54946},{\"attributes\":{\"doi\":\"10.18653/v1/D15-1044\",\"id\":\"b4\",\"matched_paper_id\":1918428},\"end\":55986,\"start\":55347},{\"attributes\":{\"id\":\"b5\"},\"end\":56193,\"start\":55988},{\"attributes\":{\"doi\":\"10.1162/COLI_a_00096\",\"id\":\"b6\",\"matched_paper_id\":2239324},\"end\":56529,\"start\":56195},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1099\",\"id\":\"b7\",\"matched_paper_id\":8314118},\"end\":57222,\"start\":56531},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1081\",\"id\":\"b8\",\"matched_paper_id\":44145304},\"end\":58063,\"start\":57224},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13756489},\"end\":58842,\"start\":58065},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.450\",\"id\":\"b10\",\"matched_paper_id\":215548661},\"end\":59494,\"start\":58844},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":127986044},\"end\":60079,\"start\":59496}]", "bib_title": "[{\"end\":53162,\"start\":53133},{\"end\":54134,\"start\":54073},{\"end\":55010,\"start\":54946},{\"end\":55410,\"start\":55347},{\"end\":56277,\"start\":56195},{\"end\":56593,\"start\":56531},{\"end\":57262,\"start\":57224},{\"end\":58090,\"start\":58065},{\"end\":58923,\"start\":58844},{\"end\":59543,\"start\":59496}]", "bib_author": "[{\"end\":53180,\"start\":53164},{\"end\":53200,\"start\":53180},{\"end\":53213,\"start\":53200},{\"end\":54150,\"start\":54136},{\"end\":54166,\"start\":54150},{\"end\":55026,\"start\":55012},{\"end\":55042,\"start\":55026},{\"end\":55060,\"start\":55042},{\"end\":55430,\"start\":55412},{\"end\":55444,\"start\":55430},{\"end\":55458,\"start\":55444},{\"end\":56050,\"start\":56037},{\"end\":56292,\"start\":56279},{\"end\":56311,\"start\":56292},{\"end\":56608,\"start\":56595},{\"end\":56617,\"start\":56608},{\"end\":56636,\"start\":56617},{\"end\":56645,\"start\":56636},{\"end\":57283,\"start\":57264},{\"end\":57299,\"start\":57283},{\"end\":57317,\"start\":57299},{\"end\":57328,\"start\":57317},{\"end\":58108,\"start\":58092},{\"end\":58122,\"start\":58108},{\"end\":58135,\"start\":58122},{\"end\":58152,\"start\":58135},{\"end\":58165,\"start\":58152},{\"end\":58180,\"start\":58165},{\"end\":58195,\"start\":58180},{\"end\":58213,\"start\":58195},{\"end\":58936,\"start\":58925},{\"end\":58951,\"start\":58936},{\"end\":58963,\"start\":58951},{\"end\":59559,\"start\":59545},{\"end\":59575,\"start\":59559},{\"end\":59585,\"start\":59575},{\"end\":59606,\"start\":59585},{\"end\":59618,\"start\":59606}]", "bib_venue": "[{\"end\":53365,\"start\":53234},{\"end\":53831,\"start\":53759},{\"end\":54361,\"start\":54186},{\"end\":55141,\"start\":55080},{\"end\":55564,\"start\":55478},{\"end\":56035,\"start\":55988},{\"end\":56356,\"start\":56331},{\"end\":56752,\"start\":56665},{\"end\":57490,\"start\":57348},{\"end\":58344,\"start\":58213},{\"end\":59079,\"start\":58992},{\"end\":59693,\"start\":59618},{\"end\":53391,\"start\":53367},{\"end\":53864,\"start\":53833},{\"end\":54539,\"start\":54363},{\"end\":55653,\"start\":55566},{\"end\":56843,\"start\":56754},{\"end\":57641,\"start\":57492},{\"end\":58481,\"start\":58346},{\"end\":59153,\"start\":59081},{\"end\":59776,\"start\":59695}]"}}}, "year": 2023, "month": 12, "day": 17}