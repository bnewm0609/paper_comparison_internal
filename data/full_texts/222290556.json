{"id": 222290556, "updated": "2023-10-06 10:44:18.077", "metadata": {"title": "Robust Fairness under Covariate Shift", "authors": "[{\"first\":\"Ashkan\",\"last\":\"Rezaei\",\"middle\":[]},{\"first\":\"Anqi\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Omid\",\"last\":\"Memarrast\",\"middle\":[]},{\"first\":\"Brian\",\"last\":\"Ziebart\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 10, "day": 11}, "abstract": "Making predictions that are fair with regard to protected group membership (race, gender, age, etc.) has become an important requirement for classification algorithms. Existing techniques derive a fair model from sampled labeled data relying on the assumption that training and testing data are identically and independently drawn (iid) from the same distribution.In practice, distribution shift can and does occur between training and testing datasets as the characteristics of individuals interacting with the machine learning system -- and which individuals interact with the system -- change. We investigate fairness under covariate shift, a relaxation of the iid assumption in which the inputs or covariates change while the conditional label distribution remains the same. We seek fair decisions under these assumptions on target data with unknown labels.We propose an approach that obtains the predictor that is robust to the worst-case in terms of target performance while satisfying target fairness requirements and matching statistical properties of the source data. We demonstrate the benefits of our approach on benchmark prediction tasks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2010.05166", "mag": "3092057384", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/RezaeiLMZ21", "doi": "10.1609/aaai.v35i11.17135"}}, "content": {"source": {"pdf_hash": "4f160697d01b4afe12366c165f1326dd90c05bc1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.05166v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f825016d7ddf836ded06eef5be9c0e335830678e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4f160697d01b4afe12366c165f1326dd90c05bc1.txt", "contents": "\nRobust Fairness under Covariate Shift\n17 Oct 2020\n\nAshkan Rezaei arezae4@uic.edu \nDepartment of Computer Science\nUniversity of Illinois at Chicago\n\n\nAnqi Liu anqiliu@caltech.edu \nCalifornia Institute of Technology\n\n\nOmid Memarrast \nDepartment of Computer Science\nUniversity of Illinois at Chicago\n\n\nBrian Ziebart bziebart@uic.edu \nDepartment of Computer Science\nUniversity of Illinois at Chicago\n\n\nRobust Fairness under Covariate Shift\n17 Oct 2020\nMaking predictions that are fair with regard to protected group membership (race, gender, age, etc.) has become an important requirement for classification algorithms. Existing techniques derive a fair model from sampled labeled data relying on the assumption that training and testing data are identically and independently drawn (iid) from the same distribution. In practice, distribution shift can and does occur between training and testing datasets as the characteristics of individuals interacting with the machine learning system-and which individuals interact with the system-change. We investigate fairness under covariate shift, a relaxation of the iid assumption in which the inputs or covariates change while the conditional label distribution remains the same. We seek fair decisions under these assumptions on target data with unknown labels. We propose an approach that obtains the predictor that is robust to the worst-case in terms of target performance while satisfying target fairness requirements and matching statistical properties of the source data. We demonstrate the benefits of our approach on benchmark prediction tasks.\n\nIntroduction\n\nSupervised learning algorithms typically focus on optimizing one singular objective: predictive performance on unseen data. However, the social impact of unwanted bias in these algorithms has become increasingly important. Machine learning systems that disadvantage specific groups are less likely to be accepted and may violate disparate impact law (Chang 2006;Kabakchieva 2013;Lohr 2013;Shipp et al. 2002;Obermeyer and Emanuel 2016;Moses and Chan 2014;Shaw and Gentry 1988;Carter and Catlett 1987;O'Neil 2016). Fairness through unawareness, which simply denies knowledge of protected group membership to the predictor, is insufficient to effectively guarantee fairness because other characteristics or covariates may correlate with protected group membership (Pedreshi, Ruggieri, and Turini 2008). Thus, there has been a surge of interest in the machine learning community to define fairness requirements reflecting desired behavior and to construct learning algorithms that more effectively seek to satisfy those requirements in various settings (Mehrabi et al. 2019; Barocas, Hardt, and Narayanan 2017; Preprint. Calmon et al. 2017;Donini et al. 2018;Dwork et al. 2012Dwork et al. , 2017Hardt, Price, and Srebro 2016;Zafar et al. 2017a;Zemel et al. 2013;Jabbari et al. 2016;Chierichetti et al. 2017).\n\nThough many definitions and measures of (un)fairness have been proposed (See Verma and Rubin (2018); Mehrabi et al. (2019)), the most widely adopted are group fairness measures of demographic parity (Calders, Kamiran, and Pechenizkiy 2009), equalized opportunity, and equalized odds (Hardt, Price, and Srebro 2016).\n\nA number of techniques have been developed as either post-processing steps (Hardt, Price, and Srebro 2016) or in-processing learning methods (Agarwal et al. 2018;Zafar et al. 2017a;Rezaei et al. 2020) seeking to achieve fairness according to these group fairness definitions. These methods attempt to make fair predictions at testing time by relying heavily on an assumption that training and testing data are independently and identically drawn (iid) from the same distribution, so that providing fairness on the training dataset provides approximate fairness on the testing dataset.\n\nIn practice, it is common for data distributions to shift between the training data set (source distribution) and the testing data set (target distribution). For example, the characteristics of loan applicants may differ significantly over time due to macroeconomic trends or changes in the selfselection criteria that potential applicants employ. Fairness methods that ignore such shifts may satisfy definitions for fairness on training samples, while violating those definitions severely on testing data. Indeed, disparate performance for underrepresented groups in computer vision tasks has been attributed to manually labeled data that is highly biased compared to testing data (Yang et al. 2020). Explicitly incorporating these shifts into the design of predictors is crucial for realizing fairer applications of machine learning in practice. However, the resulting problem setting is particularly challenging; access to labels is only available for the training distribution. Fair prediction methods could fail by using only source labels, especially for fairness definitions that condition on ground-truth labels, like equal opportunity. Figure 1 illustrates the declining performance of in-processing method of (Rezaei et al. 2020) that do not incorporate any consideration of distribution shift and instead only depend on source fairness measurements. As the amount of shift increases (x axis), the fairness of the predictor (y axis) decreases. Therefore, relying on the iid assumption, which is often violated in practice, introduces significant limitations for realizing desired fairness in critical applications  Figure 1: The fairness constraint violation (differnce between two groups) on the target distribution of fairLR (Rezaei et al. 2020), that do not account for distribution shift, and corrects for equalized opportunity using source data. The violation increases as the shift intensity increases. Experiment section includes details about simulating the shift.\n\nWe seek to address the task of providing fairness guarantees under the non-iid assumption of covariate shift. Covariate shift is a special case of data distribution shift. It assumes that the relationship between labels and covariates (inputs) is the same for both distributions, while only the source and target covariate distributions differ. Under the fair prediction setting, the sensitive group features are usually correlated with other features. Covariate shift then indicates that the labels given the covariates, including the sensitive group features, stays the same between two distributions. For example, even though there are less female loan applicants in area A than area B, which causes a marginal input distribution to shift between these two areas, we believe the probability of belonging to the advantages class (e.g., repaying a loan) given the full covariate should be the same.\n\nIn this paper, we propose a robust estimation approach for constructing a fair predictor under covariate shift. We summarize our contribution as follows:\n\nWe formulate the fair prediction problem as a game between an adversary choosing conditional label distributions to fairly minimize predictive loss on the target distribution, and an adversary choosing conditional label distributions that maximize that same objective. Constraints on the adversary require it to match statistics under the source distribution. Fairness is incorporated into the formulation by a penalty term in the objective that evaluates the fairness on the target input distribution and adversary's conditional label distribution.\n\nWe derive a convex optimization problem from the formu-lation and obtain the predictor's and adversary's conditional label distribution that are parametric, but cannot be solved analytically. Based on the formulation, we propose a batch gradient descent algorithm for learning the parameters. We compare our proposed method with baselines that only account for covariate shift or fairness or both. We demonstrate that our method outperforms the baselines on both predictive performance and the fairness constraints satisfaction under covariate shift settings.\n\n\nRelated Work Fairness\n\nVarious methods have been developed in recent years to achieve fair classification according to group fairness definitions. These techniques can be broadly categorized as pre-processing modifications to the input data or fair representation learning (Kamiran and Calders 2012;Calmon et al. 2017;Zemel et al. 2013;Feldman et al. 2015;Del Barrio et al. 2018;Donini et al. 2018;Guidotti et al. 2018), post-processing correction of classifiers' outputs (Hardt, Price, and Srebro 2016;Pleiss et al. 2017), inprocessing methods that incorporate the fairness constraints into the training process and parameter learning procedure (Donini et al. 2018;Zafar et al. 2017c,a,b;Cotter et al. 2018;Goel, Yaghini, and Faltings 2018;Woodworth et al. 2017;Kamishima, Akaho, and Sakuma 2011;Bechavod and Ligett 2017;Quadrianto and Sharmanska 2017;Rezaei et al. 2020), meta-algorithms Menon and Williamson 2018), reduction-based methods (Agarwal et al. 2018;Cotter et al. 2018), or generative-adversarial training (Madras et al. 2018;Zhang, Lemoine, and Mitchell 2018;Xu et al. 2018;Adel et al. 2019).\n\nThe closest to our work in this category is the fair robust log-loss predictor of Rezaei et al. (2020), which operates under an iid assumption. That formulation is similar to ours in that it builds a minimax game between a predictor and worst-case approximator of the true distribution. The main difference is that under the iid assumption, the true/false positive rates of target groups can be expressed as a linear constraint on the source data, in which the ground truth label is known. However, in our work, no hard constraint is available for measuring true/false positive rates on target data, because the target true label is unknown under the covariate shift assumption. Thus, we enforce these fairness measures by an expected penalty of the worst-case approximation of the target data.\n\n\nCovariate Shift\n\nGeneral distribution and domain shift works focus on the joint distribution shift between the training and testing datasets (Daume III and Marcu 2006;Ben-David et al. 2007;Blitzer et al. 2008). Particular assumptions like covariate shift (Shimodaira 2000;Sugiyama, Krauledat, and M\u00fcller 2007;Gretton et al. 2009) and label shift (Sch\u00f6lkopf et al. 2012;Lipton, Wang, and Smola 2018;Azizzadenesheli et al. 2019) help quantify the distribution shift using importance weights since they introduce invariance in conditional distributions between the training and testing. Importance weighting methods under covariate shift suffer from high variance and sensitivity to weight estimation methods. It has been shown to often be brittle-providing no finite sample generalization guarantees-even for seemingly benign amounts of shift (Cortes, Mansour, and Mohri 2010). Applying importance weighting to fair prediction has not been broadly investigated and may suffer from a similar issue.\n\nFairness under perturbation of the attribute has been studied by Awasthi, Kleindessner, and Morgenstern (2020). Lamy et al. (2019) study fair classification when the attribute is subjected to noise according to a mutually contaminated model (Scott, Blanchard, and Handy 2013). Our method works for a general shift on the joint distribution of attribute and features, and does not rely on a particular noise model.\n\nCausal analysis has also been proposed for addressing fairness under dataset shift (Singh et al. 2019). It requires a known causal graph of the data generating process, with a context variable causing the shift, as well as the known separating set of features. Our model makes no assumptions about the underlying structure of the covariates. We assume covariate shift, which relates with causal models when there is no unobserved confounders between covariates and labels. Given a known separating set of features in the causal model under data shift, the covariate shift assumption holds if we use only the separating set of features for prediction. Our model builds on robust classification method of (Liu and Ziebart 2014) under covariate shift, where the target distribution is estimated by an worst-case adversary that maximizes the log-loss while matching the feature statistics under source distribution. Therefore, if we know the separating set of features, we can incorporate them as constraints for the adversary. However, it is usually difficult to know the exact causal model of the data generating process in practice.\n\n\nApproach Preliminaries & Notation\n\nWe assume a binary classification task Y, Y \u2208 {0, 1}, where Y denotes the true label, and Y denotes the prediction for a given instance with features X \u2208 X and group attribute A \u2208 {0, 1}. We consider y = 1 as the privileged class (e.g., an applicant who would repay a loan). Further we assume a given source distribution (X, A, Y ) \u223c P src over features, attribute and label and a target distribution (X, A) \u223c P trg over features and attribute only, throughout our paper. Fairness Definitions Our model seeks to satisfy the group fairness notions of equalized opportunity and odds (Hardt, Price, and Srebro 2016).\n\nOur focus in this paper is equalized opportunity, which requires equal true positive rates across groups, i.e., for a general probabilistic classifier P :\nP ( Y = 1|A = 1, Y = 1) = P ( Y = 1|A = 0, Y = 1). (1)\nOur model can be generalized for equal odds, which in addition to providing an equal true positive rates across groups, also requires equal false positive rates across groups:\nP ( Y = 1|A = 1, Y = 0) = P ( Y = 1|A = 0, Y = 0). (2) For\nDemographic parity (Calders, Kamiran, and Pechenizkiy 2009) which requires equal positive rates across protected groups, i.e P ( Y = 1|A = 1) = P ( Y = 1|A = 0), our model reduces to a special case, as we later explain.\n\nCovariate Shift In the context of fair prediction, the covariate shift assumption is that the distribution of covariates and group membership can shift between source and target distributions:\nP src (x, a, y) = P src (x, a)P (y|x, a) (3) P trg (x, a, y) = P trg (x, a)P (y|x, a).(4)\nNote that we do not assume how the sensitive group membership a is correlated with other features x. If causal structure between the features and labels were known, as assumed in Singh et al. (2019), we could also incorporate a hidden or latent covariate shift assumption. For example, given that there is no unobserved confounder between the covariate and the labels, if h = \u03a6(x, a) represent the separating set of features, we can assume P src (y|h = \u03a6(x, a)) = P trg (y|h = \u03a6(x, a)) and use \u03a6(x, a) instead of (x, a) in our formulation. In this paper, we still use (x, a) to represent the covariates for simplicity.\n\nImportance Weighting A standard approach for addressing covariate shift is to reweight the source data to represent the target distribution (Sugiyama, Krauledat, and M\u00fcller 2007). A desired statistic f (x, a, y) of the target distribution can be obtained using samples from the source distribution\n(x i , a i , y i ) i=1:n : E x,a\u223cPtrg, y|x,a\u223cP [f (X, A, Y )] \u2248 n i=1 P trg (x i , a i ) P src (x i , a i ) f (x i , a i , y i ).\nAs long as the source distribution has support for the entire target distribution (i.e., P trg (x, a) > 0 =\u21d2 P src (x, a) > 0), this approximation is exact asymptotically as n \u2192 \u221e. However, the approximation is only guaranteed to have bounded error for finite n if the source distribution's support for target distribution samples is lower bounded (Cortes, Mansour, and Mohri 2010):\nE Ptrg(x,a) [P trg (X, A)/P src (X, A)] < \u221e.\nUnfortunately, this requirement is fairly strict and will not be satisfied even under common and seemingly benign amounts of shift. For example, if source and target samples are drawn from Gaussian distributions with equal (co-)variance, but slightly different means, it is not satisfied.\n\n\nRobust Log Loss Classification under Covaraite\n\nShift We base our method on the robust approach of Liu and Ziebart (2014) for covariate shift, which addresses this fragility of reweighting methods. The predictor P minimizes the log loss on a worst-case approximation of the target distribution provided by an adversary Q that maximizes the log loss while matching the feature statistics of the source distribution: min\nP(y|x)\u2208\u2206 max Q(y|x)\u2208\u2206\u2229\u039e E Ptrg(x)Q(y|x) [\u2212 log P(Y |X)] = max P(y|x)\u2208\u2206\u2229\u039e H Ptrg(x)P(y|x) (Y |X),(5)\nwhere a moment-matching constraint set\n\u039e = {Q | E Psrc(x)Q(y|x) [\u03c6(X, Y )] = E Psrc(x,y) [\u03c6(X, Y )]} on\nsource data is enforced with \u03c6(x, y) denoting the feature function, and \u2206 denoting the conditional probability simplex. The saddle point solution under these assumptions is P = Q which reduces the formulation to maximizing the target distribution conditional entropy (H) while matching feature statistics of the source distribution. The predictor under these assumptions has the following parametric form: (6) where the Lagrange multipliers \u03b8 are obtained by maximizing the target distribution log likelihood in the dual optimization problem.\nP \u03b8 (y|x) = e Psrc (x) P trg (x) \u03b8 \u22a4 \u03c6(x,y) y \u2032 \u2208Y e Psrc(x) P trg (x) \u03b8 \u22a4 \u03c6(x,y \u2032 ) ,\nRobust Log Loss for Fair Classification (IID) The same robust log loss approach has been employed by Rezaei et al. (2020) for fair classification under the iid assumption (P trg = P src ), where both objective and feature constraints are evaluated on the same training set. Since the true label is available during training, fairness can be enforced as a set of linear constraints on predictor P, which yields a parametric dual form.\n\nIn contrast, under the non-iid assumption, the desired fairness on target cannot be directly inferred by enforcing constraints on the source. Additionally, for true/false positive rates defining equalized opportunity (and odds), no linear constraints defined according to the target data are due to the absence of ground truth target labels. Thus, we seek fairness on target data by augmenting the objective in (5) with a penalty incurred by the worst-case approximator of the target Q. In our formulation, the saddle point solution is no longer simple (i.e., P = Q), and no parametric form solution is available.\n\n\nFormulation\n\nOur formulation seeks a robust and fair predictor under the covariate shift assumption by playing a minimax game augmented by a fairness penalty between a minimizing predictor against a worst-case approximator of the target distribution that matches the feature statistics of the source. We assume the availability of a set of labeled examples from source such that:\n{x i , a i , y i } n i=1 \u223c P src (x, a, y) and unlabeled examples from target distribution, {x i , a i } m i=1 \u223c P trg (x,\u039e(Q) : E Psrc(x,a) Q(y|x,a) [\u03c6(X, Y )] = E Psrc(x,a,y) [\u03c6(X, Y )] and \u2200k \u2208 {0, 1}, \u0393(Q) : E Ptrg(x,a) Q(y|x,a) [g k (A, Y )] = E Ptrg(x,a) Ptrg(y|x,a) [g k (A, Y )] g k ,\nwhere \u03c6 is the feature function, \u00b5 is the fairness penalty weight, g k (., .) is a selector function for group k according to the fairness definition, i.e., for equalized opportunity:\ng k (A, Y ) = I(A = k \u2227 Y = 1)\n, g k the estimated group density on target, and f (., ., .) is a weighting function of the mean score difference between the two groups:\nf (A, Y, Y ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 g1 if g 1 (A, Y ) \u2227 I( Y = 1) \u2212 1 g0 if g 0 (A, Y ) \u2227 I( Y = 1) 0 otherwise.(8)\nThe \u0393 constraint enforces Q to be consistent with the marginal probability of the groups on target ( g k ) for equalized opportunity (and odds). This marginal probability is unknown, since the true label Y on target is unavailable. Thus, we estimate these marginal probabilities by employing the robust model (6) as P trg (y|x, a) in (7) to first guess the labels under covariate shift ignoring fairness (\u00b5 = 0). We penalize the expected difference in true (or false) positive rate of groups in target according to our worst-case approximation of each example being positive label. This needs to be measured on the entire target example set and requires batch gradient updates to enforce.\n\nOur formulation is flexible for all three mentioned definitions of group fairness. For equalized odds, a second penalty term for false positive rates (g k (A, Y ) = I(A = k \u2227Y = 0)) is required and the corresponding marginal matching constraint in \u0393 needs to be added. For demographic parity (g k (A, Y ) = I(A = k)), because the group definition is independent of the true label, the target groups are fully known and the fairness penalty reduces to a linear constraint of P on target. In this special case, there is no need for the \u0393 constraint and \u00b5 can be treated as a dual variable for the linear fairness constraint. This reduces to the truncated logistic classifier of (Rezaei et al. 2020) with the exception that the fairness constraint is formed on the target data.\n\nWe obtain the following solution for the predictor P by leveraging strong minimax duality (Tops\u00f8e 1979;Gr\u00fcnwald and Dawid 2004) and strong Lagrangian duality (Boyd and Vandenberghe 2004). Theorem 1. The Fair Robust Log-Loss Predictor under Covariate Shift (7) for equalized opportunity with a given fairness penalty parameter, \u00b5, can be obtained by solving:\nlog 1 \u2212 P(y|x, a) P(y|x, a) + \u00b5 E P(y \u2032 |x,a) [f (a, y, Y \u2032 )](9)+ P src (x, a) P trg (x, a) \u03b8 T (\u03c6(x, y = 1) \u2212 \u03c6(x, y = 0)) + k\u22080,1 \u03bb k g k (a, y) = 0,\nwhere \u03b8 and \u03bb are the dual Lagrange multipliers for moment matching constraints (\u039e) and target marginal matching (\u0393) respectively.\n\nGiven the solution P * obtained above, for Q to be in equilibrium (given \u03b8) it suffices to choose Q such that: Q(y|x, a) = P * (y|x, a) 1 \u2212 \u00b5f (a, y, y) + \u00b5f (a, y, y)P * 2 (y|x, a)\n\nwhere 0 \u2264 Q(y|x, a) \u2264 1. Due to monotonicity, P in (9) is efficiently found using a binary-search in the simplex. For proofs and further details, we refer the interested reader to the appendix.\n\n\nEnforcing Fairness\n\nOur model approximates the fairness violation on the target distribution by the incurred fairness penalty measured on the robust target approximator (Q). We seek optimal \u00b5 by finding the zero-point of approximated fairness penalty, i.e., (9). Under a mild assumption that Q is sufficiently constrained by source feature statistics and marginal probability of the target groups, the approximated fairness violation by Q is monotone in the proximity of the zero point. Thus we can find the exact zero point by a binary-search in a neighborhood around zero point.\nE Ptrg(x,a)Q(y \u2032 |x,a)P(y|x,a) [f (A, Y \u2032 , Y )] in\n\nLearning\n\nFor a given fairness penalty \u00b5, our model seeks to learn the dual parameters \u03b8 and \u03bb, such that the worst-case log-loss approximation (Q \u03b8,\u00b5 ) matches the sample feature statistic from the source distribution and the marginal group probability on the target set. Given \u03b8 * , \u03bb * the solution of (9) obtains the optimal fair predictor P * \u03b8 * ,\u00b5 * which is robust against the sampling shift.\n\nWe employ L2 regularization on \u03b8 parameters to improve the generalizability of our model. This corresponds to relaxing our feature matching constraints by a convex norm. We employ a batch gradient algorithm to learn our model parameters. We perform a joint gradient optimization that updates the gradient of \u03b8 (which depends on the true label) from the source data and \u03bb (which does not depend on true label) from the target batch at each step. Note that we find the solution to the dual objective of (9) by gradient-only optimization without requiring the explicit calculation of the objective on the target dataset. The gradient optimization converges to the global optimum because the dual objective is convex in \u03b8 and \u03bb.\n\nGiven an optimal Q * from (10), a set of labeled source training samples P src (x, a, y) and unlabeled target samples P trg (x, a), the gradient of our model parameters is calculated as follows:\n\u2207 \u03b8 L trg (P, Q, \u03b8, \u03bb) = E Psrc(x,y) Q( y|x,a) \u03c6(X, Y ) \u2212 E Psrc(x,a,y) \u03c6(X, Y ) \u2207 \u03bb a \u2032 L trg (P, Q, \u03b8, \u03bb) = E Ptrg(x,a) Q(y|x,a) [g k (A, Y )] \u2212 g k .(11)\nNote that although the gradient of \u03b8 can be updated stochastically, the \u03bb gradient update relies on calculating the Q marginal for each group on a target batch. This process is described in detail in Algorithm 1.\n\n\nAlgorithm 1: Batch gradient update for Fair Robust Log-Loss learning under Covariate Shift\n\nInput:\nDatasets P src = {x i , a i , y i } n i=1 , P trg = {x i , a i } m i=1\n, ratio Psrc Ptrg , feature function \u03c6(x, y), decaying learning rate \u03b7 t Output: \u03b8 * , \u03bb * 1 \u03b8 \u2190 random initialization ; 2 \u03bb a \u2190 0 ; 3 repeat 4 Compute P, Q for all source dataset examples by finding solution to (9) and (10)\n5 \u2207 \u03b8 L \u2190 1 n n i=1 \u03c6(x i , y i ) \u2212 1 n n i=1 y\u2208Y Q(y|x i , a)\u03c6(x i , y) ; 6 \u2207 \u03bb k L \u2190 g k \u2212 1 m m i=1 y\u2208Y Q(y|x, a)g k (a, y) ; 7 \u03b8 \u2190 \u03b7 t (\u2207 \u03b8 L + C\u2207 \u03b8 ) 8 \u03bb \u2190 \u03b7 t (\u2207 \u03bb L) ; 9 until convergence;\n\nExperiments\n\nWe demonstrate the effectiveness of our method on benchmark data sets containing sensitive features. We compare with two sets of baselines: (1) previous methods enforcing fairness constraints under the iid assumption and (2) covariate shift classification models that do not account for fairness. Our experiments shows that our approach outperform the baselines by achieving smaller fairness constraint violations without sacrificing predictive performance.\n\nWe evaluate our method against baselines on four datasets:\n\n\u2022 The COMPAS criminal recidivism risk assessment dataset (Larson et al. 2016). The task is to predict recidivism of a defendant based on criminal history.\n\n\u2022 UCI Drug dataset (Fehrman et al. 2017). The task is to classify type of drug consumer by personality and demongraphics.\n\n\u2022 UCI Arrhythmia dataset (Dheeru and Karra Taniskidou 2017). The task is to distinguish between the presence and absence of cardiac arrhythmia.\n\n\u2022 UCI German dataset (Dheeru and Karra Taniskidou 2017). The task it to classify good and bad credit according to personal information and credit history. Table 1 summarizes the characteristics of each of these datasets. We create biased samplings by modeling a general shift between the distribution of the covariates in source and target, i.e., P src (x, a) = P trg (x, a).We take the following steps to create covariate shift on the normalized dataset: 1. We apply principal component analysis (PCA) to retrieve the first principal component C from features.\n\n2. We estimate the mean and standard deviation of P as \u00b5(C) and \u03c3(C).\n\n3. We choose parameters m, s and set a normal distribution as D s (\u00b5(C) + m, \u03c3(C) s ). 4. We sample from the data with probability in proportion to D s to construct the source data distribution.\n\n5. We sample from the data with probability in proportion to D t (\u00b5(C), \u03c3(C)) to construct the target data distribution. \n\n\nBaseline methods\n\nWe evaluate the performance of our model in terms of the trade-off between incurring log loss and fairness violation under various degrees of covariate shift. We focus on using equalized opportunity as our fairness definition. We compare against the following baselines 1 :\n\n\u2022 Logistic Regression (LR) is the standard logistic regression trained on source data: it ignores both covariate shift and fairness assumptions.\n\n\u2022 Robust Bias-Aware Log Loss Classifier (RBA) (Liu and Ziebart 2014) robustly minimizes the worst-case log loss on the target distribution while matching feature statistics of the source; it accounts for the covariate shift but ignores fairness.\n\n\u2022 Sample Reweighted Logistic Regression (LR_IW) (Shimodaira 2000) minimizes the reweighted log loss on the source data, according to the importance weighting ratio: it only accounts for the covariate shift.\n\n\u2022 Source Fair Logistic Regression (FAIRLR) is the method of (Rezaei et al. 2020) that optimizes worst-case log loss subject to fairness as linear constraints on labeled source data. It accounts for fairness but ignores the covariate shift. We choose this baseline as a candidate in-processing method under IID assumptions. This baseline has been shown to have performance on the Pareto frontier of the prediction-fairness trade-off against other in-processing methods such as Zafar et al. (2017c) and cost-sensitive reduction approach of Agarwal et al. (2018). However, the underlying model selection assumption for all of these existing methods is violated under the covariate shift setting.\n\n\u2022 Sample Reweighted Fair Logistic Regression (FAIRLR_IW) the fairLR method augmented with importance weighting ratios to the log loss in objective. This baseline account for both fairness and covariate shift.\n\nSetup We repeat our sampling procedure for each dataset ten times and report the average log loss and the average difference of equalized opportunity (DEO): |P( Y = 1|A = 1, Y = 1) \u2212 P( Y = 1|A = 0, Y = 1)| of our predictor on the target dataset. Unfortunately since the target distribution is assumed to be unavailable for this problem, properly obtaining optimal regularization via cross validation is not possible. We select the L2 regularization parameter by choosing the best C from {10 \u22125 , 10 \u22124 , 10 \u22123 , 10 \u22122 , 10 \u22121 , 1, 10} under the IID assumption. We use first-order features for our implementation, i.e \u03c6(x, y) = [x 1 y, x 2 y, . . . x m y] \u22a4 , where m is the size of features.\n\nAssuming sufficient expressive feature constraints, Q is always sufficiently constrained and thus under mild assumption that the approximated DEO by Q remains monotone in relatively small intervals of [\u00b5, \u00b5 + \u01eb] we first perform a line search on \u00b5 \u2208 [\u22121, 1] with intervals of \u01eb = .1 to find the zero crossing regions, and then we find the exact zero point of the approximated violation efficiently by binary search. Figure 2 shows our experiment results on iid target data and three shifted samplings.\n\n\nResults\n\nOn the COMPAS dataset, our method's logloss and DEO remains relatively unchanged with different shift settings. Although FAIRLR methods provides better fairness, their logloss is relatively high. As the sampling variance intensifies, the importance weighting methods' DEO also gets better. In all samplings our method remains on the frontier of logloss and DEO.On the Arrhythmia dataset, our method's logloss remains relatively low, only slightly worse than unfair methods. Whereas compared to the fair methods, ours provide competitive DEO in the overlapping range, with little trade-off in logloss. On this dataset our method is less sensitive to shift in variance than the shift in the mean. When the sampling shift is only due to the variance (third column), our method provides lowest logloss and lowest DEO. On the German dataset our method provides the lowest DEO on all shifted samplings, with similar logloss to unfair methods and higher log loss than FAIRLR methods. On the Drug dataset our method's log loss remains low, while on DEO dimension it lies competitive with other baselines. Similar to Arrhythmia, our method fairness performance is better under variance-based shifts on this dataset. As the shift intensifies, our method's log loss remains lower than all other baselines and the DEO nears other fair methods.\n\nIn summary, on the COMPAS and Arrhythmia dataset our method provides better trade-off of lower DEO and low logloss. On German we have the best fairness score while having similar logloss to unfair methods. On the Drug dataset, our method nears the best trade-off on the sampling with the strongest shift.  The bar is the 95% confidence interval on ten random biased samplings of covariates (P src (x, a) = P trg (x, a)) based on the first principal component.\n\n\nConclusions\n\nIn this paper, we developed a novel adversarial approach for seeking fair decision making under covariate shift. In contrast with importance weighting methods, our approach is designed to operate appropriately even when portions of the shift between source and target distributions are extreme.\n\nThe key technical challenge we address is the lack of labeled target datapoints, making target fairness assessment challenging. We instead propose to measure fairness against an adversary that is constrained by source data properties. We incorporate fairness as a weighted penalty and tune the weighted penalty to provide fairness against the adversary. More extensive evaluation on naturally-biased datasets and generalization of this approach to decision problems beyond binary classification are both important directions for future work.\n\n\nBroader Impact\n\nFairness considerations are increasingly important for machine learning systems applied to key social applications. However, the standard assumptions of statistical machine learning, such as iid training and testing data, are often violated in practice. This work offers an approach for robustly seeking fair decisions in such settings and could be of general benefit to individuals impacted by alternative systems that are either oblivious or brittle to these broken assumptions. However, this work also makes a covariate shift assumption instead of accounting for more specific causal relations that may generate the shift. Practitioners should be aware of the specific assumptions made by this paper.\n\n\nSupplementary Materials\n\n\nProof of Theorems\n\nProof of Theorem 1. First, we incorporate the source feature-matching and target marginal matching constraint and the normalization constraint for P with Lagrangian multipliers \u03b8, \u03bb and Z P into the objective. To generalize for equalized odds we extend our fairness-related variables to R 2\u00d71 vectors to include false positive rates as well, i.e \u00b5 = [\u00b5 tp ;\n\u00b5 fp ], \u03bb k = [\u03bb tp k ; \u03bb fp k ], g k (A, Y ) = [g tp k ; g fp k ] = [I(A = k, Y = 1); I(A = k, Y = 0)], and f = [f tp ; f fp ]: f tp (A, Y, Y ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 g tp 1 if g tp 1 (A, Y ) \u2227 I( Y = 1) \u2212 1 g tp 0 if g fp 0 (A, Y ) \u2227 I( Y = 1) 0\notherwise.\n\n(12)\nf fp (A, Y, Y ) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 1 g fp 1 if g fp 1 (A, Y ) \u2227 I( Y = 1) \u2212 1 g fp 0 if g fp 0 (A, Y ) \u2227 I( Y = 1) 0 otherwise.(13)\nThe dual objective is:\nmin P max Q\u2208\u2206,Z P min \u03b8,\u03bb L(P, Q, \u03b8, \u03bb, Z P ) (a) = min \u03b8,\u03bb min P max Q\u2208\u2206,Z P x,a P trg (x, a) E Q(y|x,a) \u2212 log P(Y |x, a) + \u00b5 \u22a4 E P(y \u2032 |x,a) [f (a, Y, Y \u2032 )|x, a] + P src (x, a) P trg (x, a) \u03b8 T \u03c6(x, Y ) \u2212 E Psrc(y \u2032\u2032 |x,a) [\u03c6(x, Y \u2032\u2032 )|x, a] \u03c6 + k\u2208{0,1} \u03bb \u22a4 k (g k (a, y) \u2212 g k )|x, a + Z P (x, a) y P(y|x, a) \u2212 1 (14) (b) = min \u03b8,\u03bb max Z P \u2212\u03b8 \u22a4 \u03c6 \u2212 k\u2208{0,1} \u03bb \u22a4 k g k + x,a P trg (x, a) min P(.|x,a) max y \u2212 log P(y|x, a) + \u00b5 \u22a4 E P(y \u2032 |x,a) [f (a, y, Y \u2032 )|x, a] + P src (x, a) P trg (x, a) \u03b8 T \u03c6(x, y) + k\u2208{0,1}\n\u03bb \u22a4 k g k (a, y) + Z P (x, a) y P(y|x, a) \u2212 1 (15) (c) =\u21d2 \u2212 log P(y|x, a) + \u00b5 \u22a4 E P(y \u2032 |x,a) [f (a, y, Y \u2032 )] + P src (x, a) P trg (x, a) \u03b8 T \u03c6(x, y) + k\u2208{0,1} \u03bb \u22a4 k g k (a, y) = C(x, a) \u2200y \u2208 Y (16) for a normalization term C(x, a); C must be chosen so that: y P(y|x, a) = 1. Strong duality is used in step (a) since L(P, Q, \u03b8) is convex in P, convex in \u03b8, and concave (affine) in Q. Since the function is affine in terms of Q, a solution must exist at the extreme point, i.e., y = 0 or y = 1, the optimization reduces to choosing from those extreme values in step (b). For P to be minimal, the values for each choice of y must be equal, as shown in step (c). For binary classification y \u2208 {0, 1}, the normalization term can be eliminated,and (16) simplifies to: log 1 \u2212 P(y = 1|x, a) P(y = 1|x, a) + \u00b5 \u22a4 E P(y \u2032 |x,a) [f (a, y, Y \u2032 )] + P src (x, a) P trg (x, a) \u03b8 T (\u03c6(x, y = 1) \u2212 \u03c6(x, y = 0)) + k\u22080,1 \u03bb \u22a4 k g k (a, y) = 0\n\nFor Q to be in equilibrium (given \u03b8), the P obtained above, which we denote as P * , must be optimal: L(P * , Q, \u03b8) \u2264 L(P \u2032 , Q, \u03b8), \u2200P \u2032 . Since L(P, Q, \u03b8) is convex in P, it suffices to choose Q so that: \n\nwhere Z P (x, a) must be chosen such that y Q * (y|x, a) = 1. For binary classification y \u2208 {0, 1} we can expand (19):\n\n=\u21d2 \u2212 Q(y|x, a) P * (y|x, a) + \u00b5 tp Q(y|x, a)f tp (a, y, y) + \u00b5 fp (1 \u2212 Q(y|x, a))f fp (a, 1 \u2212 y, y) + Z P (x, a) = 0 \u2200y \u2208 {0, 1}. (20) =\u21d2 Q * (y|x, a) = Z P (x, a) + \u00b5 fp f fp (a, 1 \u2212 y, y) 1 P * (y|x,a) \u2212 \u00b5 tp f tp (a, y, y) + \u00b5 fp f fp (a, 1 \u2212 y, y) \n\n=\u21d2 Z P(x,a) = 1 \u2212 P * (y|x, a)\u00b5 tp f tp (a, y, y) 1 + (\u00b5 fp f fp (a, 1 \u2212 y, y) \u2212 \u00b5 tp f tp (a, y, y))P * (y|x, a) \u2212 (\u00b5 fp f fp (a, 1 \u2212 y, y) \u2212 \u00b5 tp f tp (a, y, y))P * 2 (y|x, a)\n\nFor Equalized Opportunity where F FP (a, ., .) = 0 we get:\n\nZ P(x,a) = 1 \u2212 P * (y|x, a)\u00b5 tp f tp (a, y, y) 1 \u2212 \u00b5 tp f tp (a, y, y)P * (y|x, a) + \u00b5 tp f tp (a, y, y)P * 2 (y|x, a)\n\n=\u21d2 Q * (y|x, a) = P * (y|x, a) 1 \u2212 \u00b5 tp f tp (a, y, y)P * (y|x, a) + \u00b5 tp f tp (a, y, y)P * 2 (y|x, a)\n\nwhere additionally it must hold that 0 \u2264 Q * \u2264 1.\n\n=\u21d2 0 \u2264 P * (y|x, a) 1 \u2212 \u00b5 tp f tp (a, y, y) + \u00b5 tp f tp (a, y, y)P * 2 (y|x, a) \u2264 1 (29) y=1 =\u21d2 P(y|x, a) < 1 \u00b5 tp f tp (a)\n\nif \u00b5 tp f tp (a) > 1 P(y|x, a) < 1 otherwise (30) \n\n\na) during training. Definition 1. The Fair Robust Log-Loss Predictor under Covariate Shift, P minimizes the worst-case log loss with an added fairness penalty, while an approximator Q constrained to reflect source distribution statistics (denoted by set \u039e) maximizes the same fairness-penalized loss: min P\u2208\u2206 max Q\u2208\u2206\u2229\u039e\u2229\u0393 E Ptrg(x,a)Q(y|x,a) [\u2212 log P(Y |x, a)] (7) +\u00b5 E Ptrg(x,a)Q(y \u2032 |x,a)P(y|x,a) [f (A, Y \u2032 , Y )]\n\nFigure 2 :\n2Average log loss versus average difference of equalized opportunity (DEO) on target samples.\n\n\n=\u21d2 P trg (x, a) \u2212 Q(y|x, a) P * (y|x, a) + \u00b5 \u22a4 E Q(y \u2032 |x,a) [f (a, Y \u2032 , y)|x, a] + Z P (x, a) = 0 \u2200y \u2208 Y(18)=\u21d2 \u2212 Q(y|x, a) P * (y|x, a) + \u00b5 \u22a4 E Q(y \u2032 |x,a) [f (a, Y \u2032 , y)|x, a] + Z P (x, a) = 0 \u2200y \u2208 Y.\n\nZPPPPP\nP (x, a) + \u00b5 fp f fp (a, 1 \u2212 y, y)1 P * (y|x,a) \u2212 \u00b5 tp f tp (a, y, y) + \u00b5 fp f fp (a, 1 \u2212 y, * (y|x,a) \u2212 \u00b5 tp f tp (a, y, y) + \u00b5 fp f fp (a, 1 \u2212 y, y) + y \u00b5 fp f fp (a, 1 \u2212 y, y) 1 P * (y|x,a) \u2212 \u00b5 tp f tp (a, y, y) + \u00b5 fp F fp (a, 1 \u2212 y, * (x,a) \u2212\u00b5 tp f tp (a,y,y)+\u00b5 fp f fp (* (x,a)\u2212\u00b5 tp f tp (a,y,y)+\u00b5 fp f fp (* (y=1|x,a) \u2212\u00b5 tp f tp (a,y=1,y=1)+\u00b5 fp f fp (a,y=0,y=1) * (y=1|x,a) \u2212\u00b5 tp f tp (a,y=1,y=1)+\u00b5 fp f fp (a,y=0,y=1)\nWe do not include post-processing baselines since the available codes(Hardt, Price, and Srebro 2016) only work for equalized odds while our experiments focus on equalized opportunity.\n\nOne-network Adversarial Fairness. T Adel, I Valera, Z Ghahramani, A Weller, AAAI. Adel, T.; Valera, I.; Ghahramani, Z.; and Weller, A. 2019. One-network Adversarial Fairness. In AAAI.\n\nA Reductions Approach to Fair Classification. A Agarwal, A Beygelzimer, M Dud\u00edk, J Langford, H Wallach, ICML. Agarwal, A.; Beygelzimer, A.; Dud\u00edk, M.; Langford, J.; and Wallach, H. M. 2018. A Reductions Approach to Fair Clas- sification. In ICML.\n\nEqualized odds postprocessing under imperfect group information. P Awasthi, M Kleindessner, J Morgenstern, PMLRInternational Conference on Artificial Intelligence and Statistics. Awasthi, P.; Kleindessner, M.; and Morgenstern, J. 2020. Equalized odds postprocessing under imperfect group infor- mation. In International Conference on Artificial Intelli- gence and Statistics, 1770-1780. PMLR.\n\nRegularized learning for domain adaptation under label shifts. K Azizzadenesheli, A Liu, F Yang, A Anandkumar, arXiv:1903.09734arXiv preprintAzizzadenesheli, K.; Liu, A.; Yang, F.; and Anandkumar, A. 2019. Regularized learning for domain adaptation under la- bel shifts. arXiv preprint arXiv:1903.09734 .\n\nFairness in machine learning. S Barocas, M Hardt, A Narayanan, NIPS Tutorial. Barocas, S.; Hardt, M.; and Narayanan, A. 2017. Fairness in machine learning. NIPS Tutorial .\n\nPenalizing unfairness in binary classification. Y Bechavod, K Ligett, arXiv:1707.00044arXiv preprintBechavod, Y.; and Ligett, K. 2017. Penalizing unfairness in binary classification. arXiv preprint arXiv:1707.00044 .\n\nAnalysis of representations for domain adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, Advances in neural information processing systems. Ben-David, S.; Blitzer, J.; Crammer, K.; and Pereira, F. 2007. Analysis of representations for domain adaptation. In Ad- vances in neural information processing systems, 137-144.\n\nLearning bounds for domain adaptation. J Blitzer, K Crammer, A Kulesza, F Pereira, J Wortman, Advances in neural information processing systems. Blitzer, J.; Crammer, K.; Kulesza, A.; Pereira, F.; and Wort- man, J. 2008. Learning bounds for domain adaptation. In Ad- vances in neural information processing systems, 129-136.\n\nS Boyd, L Vandenberghe, Convex optimization. Cambridge university pressBoyd, S.; and Vandenberghe, L. 2004. Convex optimization. Cambridge university press.\n\nBuilding classifiers with independency constraints. T Calders, F Kamiran, M Pechenizkiy, ICDMW '09. Calders, T.; Kamiran, F.; and Pechenizkiy, M. 2009. Build- ing classifiers with independency constraints. In ICDMW '09.\n\nOptimized Pre-Processing for Discrimination Prevention. F Calmon, D Wei, B Vinzamuri, K Natesan Ramamurthy, K R Varshney, NeurIPS. Calmon, F.; Wei, D.; Vinzamuri, B.; Natesan Ramamurthy, K.; and Varshney, K. R. 2017. Optimized Pre-Processing for Discrimination Prevention. In NeurIPS.\n\nAssessing credit card applications using machine learning. C Carter, J Catlett, IEEE Expert. Carter, C.; and Catlett, J. 1987. Assessing credit card appli- cations using machine learning. IEEE Expert .\n\nClassification with fairness constraints: A metaalgorithm with provable guarantees. L E Celis, L Huang, V Keswani, N K Vishnoi, ACM FAT*. Celis, L. E.; Huang, L.; Keswani, V.; and Vishnoi, N. K. 2019. Classification with fairness constraints: A meta- algorithm with provable guarantees. In ACM FAT*.\n\nImproved Adversarial Learning for Fair Classification. L E Celis, V Keswani, arXiv preprintCelis, L. E.; and Keswani, V. 2019. Improved Adversarial Learning for Fair Classification. arXiv preprint .\n\nApplying data mining to predict college admissions yield: A case study. L Chang, NDIRChang, L. 2006. Applying data mining to predict college admissions yield: A case study. NDIR .\n\nFair clustering through fairlets. F Chierichetti, R Kumar, S Lattanzi, S Vassilvitskii, Advances in Neural Information Processing Systems. Chierichetti, F.; Kumar, R.; Lattanzi, S.; and Vassilvitskii, S. 2017. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 5029-5037.\n\nLearning bounds for importance weighting. C Cortes, Y Mansour, M Mohri, Advances in neural information processing systems. Cortes, C.; Mansour, Y.; and Mohri, M. 2010. Learning bounds for importance weighting. In Advances in neural information processing systems, 442-450.\n\nOptimization with nondifferentiable constraints with applications to fairness, recall, churn, and other goals. A Cotter, H Jiang, S Wang, T Narayan, M Gupta, S You, K Sridharan, arXiv preprintCotter, A.; Jiang, H.; Wang, S.; Narayan, T.; Gupta, M.; You, S.; and Sridharan, K. 2018. Optimization with non- differentiable constraints with applications to fairness, recall, churn, and other goals. arXiv preprint .\n\nDomain adaptation for statistical classifiers. Iii Daume, H Marcu, D , Journal of artificial Intelligence research. 26Daume III, H.; and Marcu, D. 2006. Domain adaptation for statistical classifiers. Journal of artificial Intelligence research 26: 101-126.\n\nObtaining fairness using optimal transport theory. Del Barrio, E Gamboa, F Gordaliza, P Loubes, J.-M , arXiv preprintDel Barrio, E.; Gamboa, F.; Gordaliza, P.; and Loubes, J.- M. 2018. Obtaining fairness using optimal transport theory. arXiv preprint .\n\nD Dheeru, E Taniskidou, UCI Machine Learning Repository. Dheeru, D.; and Karra Taniskidou, E. 2017. UCI Machine Learning Repository. URL http://archive.ics.uci.edu/ml.\n\nEmpirical risk minimization under fairness constraints. M Donini, L Oneto, S Ben-David, J S Shawe-Taylor, M Pontil, NeurIPS. Donini, M.; Oneto, L.; Ben-David, S.; Shawe-Taylor, J. S.; and Pontil, M. 2018. Empirical risk minimization under fair- ness constraints. In NeurIPS.\n\nFairness through awareness. C Dwork, M Hardt, T Pitassi, O Reingold, R Zemel, ITCS. Dwork, C.; Hardt, M.; Pitassi, T.; Reingold, O.; and Zemel, R. 2012. Fairness through awareness. In ITCS.\n\nDecoupled classifiers for fair and efficient machine learning. C Dwork, N Immorlica, A T Kalai, M Leiserson, arXiv:1707.06613arXiv preprintDwork, C.; Immorlica, N.; Kalai, A. T.; and Leiserson, M. 2017. Decoupled classifiers for fair and efficient machine learning. arXiv preprint arXiv:1707.06613 .\n\nThe five factor model of personality and evaluation of drug consumption risk. E Fehrman, A K Muhammad, E M Mirkes, V Egan, A N Gorban, Data science. SpringerFehrman, E.; Muhammad, A. K.; Mirkes, E. M.; Egan, V.; and Gorban, A. N. 2017. The five factor model of personality and evaluation of drug consumption risk. In Data science, 231-242. Springer.\n\nCertifying and removing disparate impact. M Feldman, S A Friedler, J Moeller, C Scheidegger, S Venkatasubramanian, ACM SIGKDD. Feldman, M.; Friedler, S. A.; Moeller, J.; Scheidegger, C.; and Venkatasubramanian, S. 2015. Certifying and removing disparate impact. In ACM SIGKDD.\n\nNondiscriminatory machine learning through convex fairness criteria. N Goel, M Yaghini, B Faltings, AAAI. Goel, N.; Yaghini, M.; and Faltings, B. 2018. Non- discriminatory machine learning through convex fairness cri- teria. In AAAI.\n\nCovariate shift by kernel mean matching. A Gretton, A Smola, J Huang, M Schmittfull, K Borgwardt, B Sch\u00f6lkopf, Dataset shift in machine learning. 345Gretton, A.; Smola, A.; Huang, J.; Schmittfull, M.; Borg- wardt, K.; and Sch\u00f6lkopf, B. 2009. Covariate shift by kernel mean matching. Dataset shift in machine learning 3(4): 5.\n\nGame Theory, Maximum Entropy, Minimum Discrepancy, and Robust Bayesian Decision Theory. P D Gr\u00fcnwald, A P Dawid, Annals of Statistics. 32Gr\u00fcnwald, P. D.; and Dawid, A. P. 2004. Game The- ory, Maximum Entropy, Minimum Discrepancy, and Robust Bayesian Decision Theory. Annals of Statistics 32.\n\nA survey of methods for explaining black box models. R Guidotti, A Monreale, S Ruggieri, F Turini, F Giannotti, D Pedreschi, ACM computing surveys (CSUR). 515Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Gian- notti, F.; and Pedreschi, D. 2018. A survey of methods for ex- plaining black box models. ACM computing surveys (CSUR) 51(5): 1-42.\n\nEquality of opportunity in supervised learning. M Hardt, E Price, N Srebro, In NeurIPSHardt, M.; Price, E.; and Srebro, N. 2016. Equality of oppor- tunity in supervised learning. In NeurIPS.\n\nS Jabbari, M Joseph, M Kearns, J Morgenstern, A Roth, arXiv:1611.03071Fair learning in markovian environments. arXiv preprintJabbari, S.; Joseph, M.; Kearns, M.; Morgenstern, J.; and Roth, A. 2016. Fair learning in markovian environments. arXiv preprint arXiv:1611.03071 .\n\nPredicting student performance by using data mining methods for classification. D Kabakchieva, Cybernetics and Information Technologies. 131Kabakchieva, D. 2013. Predicting student performance by using data mining methods for classification. Cybernetics and Information Technologies 13(1).\n\nData preprocessing techniques for classification without discrimination. F Kamiran, T Calders, Knowledge and Information Systems. 331Kamiran, F.; and Calders, T. 2012. Data preprocessing tech- niques for classification without discrimination. Knowledge and Information Systems 33(1).\n\nFairnessaware learning through regularization approach. T Kamishima, S Akaho, J Sakuma, ICDMW. Kamishima, T.; Akaho, S.; and Sakuma, J. 2011. Fairness- aware learning through regularization approach. In ICDMW.\n\nNoise-tolerant fair classification. A Lamy, Z Zhong, A K Menon, N Verma, Advances in Neural Information Processing Systems. Lamy, A.; Zhong, Z.; Menon, A. K.; and Verma, N. 2019. Noise-tolerant fair classification. In Advances in Neural In- formation Processing Systems, 294-306.\n\nHow we analyzed the COMPAS recidivism algorithm. ProPublica. J Larson, S Mattu, L Kirchner, J Angwin, Larson, J.; Mattu, S.; Kirchner, L.; and Angwin, J. 2016. How we analyzed the COMPAS recidivism algorithm. ProP- ublica .\n\nDetecting and correcting for label shift with black box predictors. Z C Lipton, Y.-X Wang, A Smola, arXiv:1802.03916arXiv preprintLipton, Z. C.; Wang, Y.-X.; and Smola, A. 2018. Detect- ing and correcting for label shift with black box predictors. arXiv preprint arXiv:1802.03916 .\n\nRobust Classification Under Sample Selection Bias. A Liu, B Ziebart, In NeurIPSLiu, A.; and Ziebart, B. 2014. Robust Classification Under Sample Selection Bias. In NeurIPS.\n\nBig data, trying to build better workers. The New York Times 21. S Lohr, Lohr, S. 2013. Big data, trying to build better workers. The New York Times 21.\n\nD Madras, E Creager, T Pitassi, R Zemel, Learning adversarially fair and transferable representations. arXiv preprintMadras, D.; Creager, E.; Pitassi, T.; and Zemel, R. 2018. Learning adversarially fair and transferable representations. arXiv preprint .\n\nA survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, arXiv:1908.09635arXiv preprintMehrabi, N.; Morstatter, F.; Saxena, N.; Lerman, K.; and Galstyan, A. 2019. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635 .\n\nThe cost of fairness in binary classification. A K Menon, R C Williamson, ACM FAT*. Menon, A. K.; and Williamson, R. C. 2018. The cost of fairness in binary classification. In ACM FAT*.\n\nUsing big data for legal and law enforcement decisions: Testing the new tools. L B Moses, J Chan, UNSWLJ. Moses, L. B.; and Chan, J. 2014. Using big data for legal and law enforcement decisions: Testing the new tools. UNSWLJ .\n\nPredicting the future-big data, machine learning, and clinical medicine. Z Obermeyer, E J Emanuel, The New England Journal of Medicine. 37513Obermeyer, Z.; and Emanuel, E. J. 2016. Predicting the fu- ture-big data, machine learning, and clinical medicine. The New England Journal of Medicine 375(13).\n\nWeapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books. C O&apos;neil, O'Neil, C. 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Broad- way Books.\n\nDiscrimination-aware data mining. D Pedreshi, S Ruggieri, F Turini, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data miningPedreshi, D.; Ruggieri, S.; and Turini, F. 2008. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 560-568.\n\nOn Fairness and Calibration. G Pleiss, M Raghavan, F Wu, J Kleinberg, K Q Weinberger, NeurIPS. Pleiss, G.; Raghavan, M.; Wu, F.; Kleinberg, J.; and Wein- berger, K. Q. 2017. On Fairness and Calibration. In NeurIPS.\n\nRecycling privileged learning and distribution matching for fairness. N Quadrianto, V Sharmanska, NeurIPS. Quadrianto, N.; and Sharmanska, V. 2017. Recycling priv- ileged learning and distribution matching for fairness. In NeurIPS.\n\nFairness for Robust Log Loss Classification. A Rezaei, R Fathony, O Memarrast, B Ziebart, AAAI. Rezaei, A.; Fathony, R.; Memarrast, O.; and Ziebart, B. 2020. Fairness for Robust Log Loss Classification. In AAAI.\n\nB Sch\u00f6lkopf, D Janzing, J Peters, E Sgouritsa, K Zhang, J Mooij, arXiv:1206.6471On causal and anticausal learning. arXiv preprintSch\u00f6lkopf, B.; Janzing, D.; Peters, J.; Sgouritsa, E.; Zhang, K.; and Mooij, J. 2012. On causal and anticausal learning. arXiv preprint arXiv:1206.6471 .\n\nClassification with asymmetric label noise: Consistency and maximal denoising. C Scott, G Blanchard, G Handy, Conference On Learning Theory. Scott, C.; Blanchard, G.; and Handy, G. 2013. Classifica- tion with asymmetric label noise: Consistency and maximal denoising. In Conference On Learning Theory, 489-511.\n\nUsing an expert system with inductive learning to evaluate business loans. M J Shaw, J A Gentry, Financial Management. Shaw, M. J.; and Gentry, J. A. 1988. Using an expert system with inductive learning to evaluate business loans. Financial Management .\n\nImproving predictive inference under covariate shift by weighting the log-likelihood function. H Shimodaira, Journal of statistical planning and inference. 902Shimodaira, H. 2000. Improving predictive inference un- der covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference 90(2): 227- 244.\n\nDiffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning. M A Shipp, K N Ross, P Tamayo, A P Weng, J L Kutok, R C Aguiar, M Gaasenbeek, M Angelo, M Reich, G S Pinkus, Nature medicine. 81Shipp, M. A.; Ross, K. N.; Tamayo, P.; Weng, A. P.; Kutok, J. L.; Aguiar, R. C.; Gaasenbeek, M.; Angelo, M.; Reich, M.; Pinkus, G. S.; et al. 2002. Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and super- vised machine learning. Nature medicine 8(1).\n\n. H Singh, R Singh, V Mhasawade, R Chunara, arXiv:1911.00677Fair Predictors under Distribution Shift. arXiv preprintSingh, H.; Singh, R.; Mhasawade, V.; and Chunara, R. 2019. Fair Predictors under Distribution Shift. arXiv preprint arXiv:1911.00677 .\n\nCovariate shift adaptation by importance weighted cross validation. M Sugiyama, M Krauledat, K.-R M\u00fcller, Journal of Machine Learning Research. 8Sugiyama, M.; Krauledat, M.; and M\u00fcller, K.-R. 2007. Co- variate shift adaptation by importance weighted cross valida- tion. Journal of Machine Learning Research 8(May): 985- 1005.\n\nInformation-theoretical optimization techniques. F Tops\u00f8e, Kybernetika. 151Tops\u00f8e, F. 1979. Information-theoretical optimization tech- niques. Kybernetika 15(1): 8-27.\n\nFairness definitions explained. S Verma, J Rubin, IEEE/ACM International Workshop on Software Fairness (FairWare). IEEEVerma, S.; and Rubin, J. 2018. Fairness definitions ex- plained. In 2018 IEEE/ACM International Workshop on Soft- ware Fairness (FairWare), 1-7. IEEE.\n\nLearning Non-Discriminatory Predictors. B Woodworth, S Gunasekar, M I Ohannessian, N Srebro, COLT. Woodworth, B.; Gunasekar, S.; Ohannessian, M. I.; and Sre- bro, N. 2017. Learning Non-Discriminatory Predictors. In COLT.\n\nFairGAN: Fairness-aware generative adversarial networks. D Xu, S Yuan, L Zhang, X Wu, IEEE Big Data. Xu, D.; Yuan, S.; Zhang, L.; and Wu, X. 2018. FairGAN: Fairness-aware generative adversarial networks. In IEEE Big Data.\n\nTowards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. K Yang, K Qinami, L Fei-Fei, J Deng, O Russakovsky, Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. the 2020 Conference on Fairness, Accountability, and TransparencyYang, K.; Qinami, K.; Fei-Fei, L.; Deng, J.; and Rus- sakovsky, O. 2020. Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the ima- genet hierarchy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 547-558.\n\nFairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. M B Zafar, I Valera, M Gomez Rodriguez, K P Gummadi, WWW. Zafar, M. B.; Valera, I.; Gomez Rodriguez, M.; and Gum- madi, K. P. 2017a. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In WWW.\n\nFrom parity to preference-based notions of fairness in classification. M B Zafar, I Valera, M Rodriguez, K Gummadi, A Weller, NeurIPS. Zafar, M. B.; Valera, I.; Rodriguez, M.; Gummadi, K.; and Weller, A. 2017b. From parity to preference-based notions of fairness in classification. In NeurIPS.\n\nFairness Constraints: Mechanisms for Fair Classification. M B Zafar, I Valera, M G Rogriguez, K P Gummadi, AISTATS. Zafar, M. B.; Valera, I.; Rogriguez, M. G.; and Gummadi, K. P. 2017c. Fairness Constraints: Mechanisms for Fair Clas- sification. In AISTATS.\n\nLearning Fair Representations. R Zemel, Y Wu, K Swersky, T Pitassi, C Dwork, ICML. Zemel, R.; Wu, Y.; Swersky, K.; Pitassi, T.; and Dwork, C. 2013. Learning Fair Representations. In ICML.\n\nMitigating unwanted biases with adversarial learning. B H Zhang, B Lemoine, M Mitchell, AIES. Zhang, B. H.; Lemoine, B.; and Mitchell, M. 2018. Mitigat- ing unwanted biases with adversarial learning. In AIES.\n", "annotations": {"author": "[{\"end\":149,\"start\":52},{\"end\":216,\"start\":150},{\"end\":299,\"start\":217},{\"end\":398,\"start\":300}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":59},{\"end\":158,\"start\":155},{\"end\":231,\"start\":222},{\"end\":313,\"start\":306}]", "author_first_name": "[{\"end\":58,\"start\":52},{\"end\":154,\"start\":150},{\"end\":221,\"start\":217},{\"end\":305,\"start\":300}]", "author_affiliation": "[{\"end\":148,\"start\":83},{\"end\":215,\"start\":180},{\"end\":298,\"start\":233},{\"end\":397,\"start\":332}]", "title": "[{\"end\":38,\"start\":1},{\"end\":436,\"start\":399}]", "venue": null, "abstract": "[{\"end\":1596,\"start\":449}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1974,\"start\":1962},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1991,\"start\":1974},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2001,\"start\":1991},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2019,\"start\":2001},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2046,\"start\":2019},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2066,\"start\":2046},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2087,\"start\":2066},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2111,\"start\":2087},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2123,\"start\":2111},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2410,\"start\":2373},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2681,\"start\":2661},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2718,\"start\":2683},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2748,\"start\":2729},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2767,\"start\":2748},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2784,\"start\":2767},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2803,\"start\":2784},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2833,\"start\":2803},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2852,\"start\":2833},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2870,\"start\":2852},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2890,\"start\":2870},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2915,\"start\":2890},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3017,\"start\":2995},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3040,\"start\":3019},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3157,\"start\":3117},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3232,\"start\":3201},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3341,\"start\":3310},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3397,\"start\":3376},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3416,\"start\":3397},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3435,\"start\":3416},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":4521,\"start\":4503},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5059,\"start\":5040},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5578,\"start\":5558},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8273,\"start\":8247},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8292,\"start\":8273},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8310,\"start\":8292},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8330,\"start\":8310},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8353,\"start\":8330},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8372,\"start\":8353},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8393,\"start\":8372},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8477,\"start\":8446},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8496,\"start\":8477},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8640,\"start\":8620},{\"end\":8663,\"start\":8640},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8682,\"start\":8663},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8715,\"start\":8682},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8737,\"start\":8715},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8771,\"start\":8737},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8796,\"start\":8771},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8827,\"start\":8796},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8846,\"start\":8827},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8890,\"start\":8864},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8937,\"start\":8916},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8956,\"start\":8937},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9013,\"start\":8993},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9047,\"start\":9013},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9062,\"start\":9047},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9079,\"start\":9062},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9184,\"start\":9164},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10046,\"start\":10020},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10068,\"start\":10046},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10088,\"start\":10068},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10151,\"start\":10134},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10188,\"start\":10151},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10208,\"start\":10188},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10248,\"start\":10225},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10277,\"start\":10248},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10304,\"start\":10277},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10753,\"start\":10720},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10986,\"start\":10941},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11006,\"start\":10988},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11151,\"start\":11117},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11393,\"start\":11374},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13072,\"start\":13041},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13578,\"start\":13539},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14822,\"start\":14784},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15453,\"start\":15420},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15912,\"start\":15890},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17165,\"start\":17145},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20614,\"start\":20594},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20797,\"start\":20784},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20821,\"start\":20797},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20880,\"start\":20852},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25245,\"start\":25226},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25364,\"start\":25344},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25507,\"start\":25473},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25648,\"start\":25614},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27054,\"start\":27032},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27298,\"start\":27281},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":27520,\"start\":27501},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27937,\"start\":27917},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28000,\"start\":27979},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37715,\"start\":37684}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36866,\"start\":36449},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36972,\"start\":36867},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37179,\"start\":36973},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37614,\"start\":37180}]", "paragraph": "[{\"end\":2916,\"start\":1612},{\"end\":3233,\"start\":2918},{\"end\":3819,\"start\":3235},{\"end\":5803,\"start\":3821},{\"end\":6704,\"start\":5805},{\"end\":6859,\"start\":6706},{\"end\":7410,\"start\":6861},{\"end\":7971,\"start\":7412},{\"end\":9080,\"start\":7997},{\"end\":9876,\"start\":9082},{\"end\":10874,\"start\":9896},{\"end\":11289,\"start\":10876},{\"end\":12422,\"start\":11291},{\"end\":13073,\"start\":12460},{\"end\":13229,\"start\":13075},{\"end\":13460,\"start\":13285},{\"end\":13739,\"start\":13520},{\"end\":13933,\"start\":13741},{\"end\":14642,\"start\":14024},{\"end\":14941,\"start\":14644},{\"end\":15454,\"start\":15072},{\"end\":15788,\"start\":15500},{\"end\":16209,\"start\":15839},{\"end\":16348,\"start\":16310},{\"end\":16956,\"start\":16414},{\"end\":17477,\"start\":17044},{\"end\":18092,\"start\":17479},{\"end\":18474,\"start\":18108},{\"end\":18951,\"start\":18768},{\"end\":19120,\"start\":18983},{\"end\":19916,\"start\":19228},{\"end\":20692,\"start\":19918},{\"end\":21051,\"start\":20694},{\"end\":21335,\"start\":21205},{\"end\":21518,\"start\":21337},{\"end\":21713,\"start\":21520},{\"end\":22296,\"start\":21736},{\"end\":22750,\"start\":22360},{\"end\":23476,\"start\":22752},{\"end\":23672,\"start\":23478},{\"end\":24042,\"start\":23830},{\"end\":24143,\"start\":24137},{\"end\":24439,\"start\":24215},{\"end\":25107,\"start\":24650},{\"end\":25167,\"start\":25109},{\"end\":25323,\"start\":25169},{\"end\":25446,\"start\":25325},{\"end\":25591,\"start\":25448},{\"end\":26154,\"start\":25593},{\"end\":26225,\"start\":26156},{\"end\":26421,\"start\":26227},{\"end\":26544,\"start\":26423},{\"end\":26838,\"start\":26565},{\"end\":26984,\"start\":26840},{\"end\":27231,\"start\":26986},{\"end\":27439,\"start\":27233},{\"end\":28133,\"start\":27441},{\"end\":28343,\"start\":28135},{\"end\":29037,\"start\":28345},{\"end\":29540,\"start\":29039},{\"end\":30883,\"start\":29552},{\"end\":31344,\"start\":30885},{\"end\":31654,\"start\":31360},{\"end\":32197,\"start\":31656},{\"end\":32919,\"start\":32216},{\"end\":33324,\"start\":32967},{\"end\":33574,\"start\":33564},{\"end\":33580,\"start\":33576},{\"end\":33732,\"start\":33710},{\"end\":35175,\"start\":34250},{\"end\":35383,\"start\":35177},{\"end\":35503,\"start\":35385},{\"end\":35757,\"start\":35505},{\"end\":35936,\"start\":35759},{\"end\":35996,\"start\":35938},{\"end\":36116,\"start\":35998},{\"end\":36220,\"start\":36118},{\"end\":36271,\"start\":36222},{\"end\":36396,\"start\":36273},{\"end\":36448,\"start\":36398}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13284,\"start\":13230},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13519,\"start\":13461},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14023,\"start\":13934},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15071,\"start\":14942},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15499,\"start\":15455},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16309,\"start\":16210},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16413,\"start\":16349},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17043,\"start\":16957},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18597,\"start\":18475},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18767,\"start\":18597},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18982,\"start\":18952},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19227,\"start\":19121},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21117,\"start\":21052},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21204,\"start\":21117},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22348,\"start\":22297},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23829,\"start\":23673},{\"attributes\":{\"id\":\"formula_17\"},\"end\":24214,\"start\":24144},{\"attributes\":{\"id\":\"formula_18\"},\"end\":24635,\"start\":24440},{\"attributes\":{\"id\":\"formula_19\"},\"end\":33563,\"start\":33325},{\"attributes\":{\"id\":\"formula_20\"},\"end\":33709,\"start\":33581},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34249,\"start\":33733}]", "table_ref": "[{\"end\":25755,\"start\":25748}]", "section_header": "[{\"end\":1610,\"start\":1598},{\"end\":7995,\"start\":7974},{\"end\":9894,\"start\":9879},{\"end\":12458,\"start\":12425},{\"end\":15837,\"start\":15791},{\"end\":18106,\"start\":18095},{\"end\":21734,\"start\":21716},{\"end\":22358,\"start\":22350},{\"end\":24135,\"start\":24045},{\"end\":24648,\"start\":24637},{\"end\":26563,\"start\":26547},{\"end\":29550,\"start\":29543},{\"end\":31358,\"start\":31347},{\"end\":32214,\"start\":32200},{\"end\":32945,\"start\":32922},{\"end\":32965,\"start\":32948},{\"end\":36878,\"start\":36868},{\"end\":37187,\"start\":37181}]", "table": null, "figure_caption": "[{\"end\":36866,\"start\":36451},{\"end\":36972,\"start\":36880},{\"end\":37179,\"start\":36975},{\"end\":37614,\"start\":37188}]", "figure_ref": "[{\"end\":4974,\"start\":4966},{\"end\":5454,\"start\":5446},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29463,\"start\":29455}]", "bib_author_first_name": "[{\"end\":37835,\"start\":37834},{\"end\":37843,\"start\":37842},{\"end\":37853,\"start\":37852},{\"end\":37867,\"start\":37866},{\"end\":38032,\"start\":38031},{\"end\":38043,\"start\":38042},{\"end\":38058,\"start\":38057},{\"end\":38067,\"start\":38066},{\"end\":38079,\"start\":38078},{\"end\":38299,\"start\":38298},{\"end\":38310,\"start\":38309},{\"end\":38326,\"start\":38325},{\"end\":38691,\"start\":38690},{\"end\":38710,\"start\":38709},{\"end\":38717,\"start\":38716},{\"end\":38725,\"start\":38724},{\"end\":38964,\"start\":38963},{\"end\":38975,\"start\":38974},{\"end\":38984,\"start\":38983},{\"end\":39155,\"start\":39154},{\"end\":39167,\"start\":39166},{\"end\":39376,\"start\":39375},{\"end\":39389,\"start\":39388},{\"end\":39400,\"start\":39399},{\"end\":39411,\"start\":39410},{\"end\":39692,\"start\":39691},{\"end\":39703,\"start\":39702},{\"end\":39714,\"start\":39713},{\"end\":39725,\"start\":39724},{\"end\":39736,\"start\":39735},{\"end\":39979,\"start\":39978},{\"end\":39987,\"start\":39986},{\"end\":40189,\"start\":40188},{\"end\":40200,\"start\":40199},{\"end\":40211,\"start\":40210},{\"end\":40414,\"start\":40413},{\"end\":40424,\"start\":40423},{\"end\":40431,\"start\":40430},{\"end\":40444,\"start\":40443},{\"end\":40466,\"start\":40465},{\"end\":40468,\"start\":40467},{\"end\":40703,\"start\":40702},{\"end\":40713,\"start\":40712},{\"end\":40931,\"start\":40930},{\"end\":40933,\"start\":40932},{\"end\":40942,\"start\":40941},{\"end\":40951,\"start\":40950},{\"end\":40962,\"start\":40961},{\"end\":40964,\"start\":40963},{\"end\":41203,\"start\":41202},{\"end\":41205,\"start\":41204},{\"end\":41214,\"start\":41213},{\"end\":41420,\"start\":41419},{\"end\":41563,\"start\":41562},{\"end\":41579,\"start\":41578},{\"end\":41588,\"start\":41587},{\"end\":41600,\"start\":41599},{\"end\":41881,\"start\":41880},{\"end\":41891,\"start\":41890},{\"end\":41902,\"start\":41901},{\"end\":42224,\"start\":42223},{\"end\":42234,\"start\":42233},{\"end\":42243,\"start\":42242},{\"end\":42251,\"start\":42250},{\"end\":42262,\"start\":42261},{\"end\":42271,\"start\":42270},{\"end\":42278,\"start\":42277},{\"end\":42575,\"start\":42572},{\"end\":42584,\"start\":42583},{\"end\":42593,\"start\":42592},{\"end\":42837,\"start\":42834},{\"end\":42847,\"start\":42846},{\"end\":42857,\"start\":42856},{\"end\":42870,\"start\":42869},{\"end\":42883,\"start\":42879},{\"end\":43038,\"start\":43037},{\"end\":43048,\"start\":43047},{\"end\":43263,\"start\":43262},{\"end\":43273,\"start\":43272},{\"end\":43282,\"start\":43281},{\"end\":43295,\"start\":43294},{\"end\":43297,\"start\":43296},{\"end\":43313,\"start\":43312},{\"end\":43511,\"start\":43510},{\"end\":43520,\"start\":43519},{\"end\":43529,\"start\":43528},{\"end\":43540,\"start\":43539},{\"end\":43552,\"start\":43551},{\"end\":43737,\"start\":43736},{\"end\":43746,\"start\":43745},{\"end\":43759,\"start\":43758},{\"end\":43761,\"start\":43760},{\"end\":43770,\"start\":43769},{\"end\":44053,\"start\":44052},{\"end\":44064,\"start\":44063},{\"end\":44066,\"start\":44065},{\"end\":44078,\"start\":44077},{\"end\":44080,\"start\":44079},{\"end\":44090,\"start\":44089},{\"end\":44098,\"start\":44097},{\"end\":44100,\"start\":44099},{\"end\":44368,\"start\":44367},{\"end\":44379,\"start\":44378},{\"end\":44381,\"start\":44380},{\"end\":44393,\"start\":44392},{\"end\":44404,\"start\":44403},{\"end\":44419,\"start\":44418},{\"end\":44673,\"start\":44672},{\"end\":44681,\"start\":44680},{\"end\":44692,\"start\":44691},{\"end\":44880,\"start\":44879},{\"end\":44891,\"start\":44890},{\"end\":44900,\"start\":44899},{\"end\":44909,\"start\":44908},{\"end\":44924,\"start\":44923},{\"end\":44937,\"start\":44936},{\"end\":45254,\"start\":45253},{\"end\":45256,\"start\":45255},{\"end\":45268,\"start\":45267},{\"end\":45270,\"start\":45269},{\"end\":45512,\"start\":45511},{\"end\":45524,\"start\":45523},{\"end\":45536,\"start\":45535},{\"end\":45548,\"start\":45547},{\"end\":45558,\"start\":45557},{\"end\":45571,\"start\":45570},{\"end\":45858,\"start\":45857},{\"end\":45867,\"start\":45866},{\"end\":45876,\"start\":45875},{\"end\":46002,\"start\":46001},{\"end\":46013,\"start\":46012},{\"end\":46023,\"start\":46022},{\"end\":46033,\"start\":46032},{\"end\":46048,\"start\":46047},{\"end\":46356,\"start\":46355},{\"end\":46640,\"start\":46639},{\"end\":46651,\"start\":46650},{\"end\":46908,\"start\":46907},{\"end\":46921,\"start\":46920},{\"end\":46930,\"start\":46929},{\"end\":47099,\"start\":47098},{\"end\":47107,\"start\":47106},{\"end\":47116,\"start\":47115},{\"end\":47118,\"start\":47117},{\"end\":47127,\"start\":47126},{\"end\":47405,\"start\":47404},{\"end\":47415,\"start\":47414},{\"end\":47424,\"start\":47423},{\"end\":47436,\"start\":47435},{\"end\":47637,\"start\":47636},{\"end\":47639,\"start\":47638},{\"end\":47652,\"start\":47648},{\"end\":47660,\"start\":47659},{\"end\":47903,\"start\":47902},{\"end\":47910,\"start\":47909},{\"end\":48091,\"start\":48090},{\"end\":48180,\"start\":48179},{\"end\":48190,\"start\":48189},{\"end\":48201,\"start\":48200},{\"end\":48212,\"start\":48211},{\"end\":48486,\"start\":48485},{\"end\":48497,\"start\":48496},{\"end\":48511,\"start\":48510},{\"end\":48521,\"start\":48520},{\"end\":48531,\"start\":48530},{\"end\":48782,\"start\":48781},{\"end\":48784,\"start\":48783},{\"end\":48793,\"start\":48792},{\"end\":48795,\"start\":48794},{\"end\":49001,\"start\":49000},{\"end\":49003,\"start\":49002},{\"end\":49012,\"start\":49011},{\"end\":49223,\"start\":49222},{\"end\":49236,\"start\":49235},{\"end\":49238,\"start\":49237},{\"end\":49556,\"start\":49555},{\"end\":49729,\"start\":49728},{\"end\":49741,\"start\":49740},{\"end\":49753,\"start\":49752},{\"end\":50171,\"start\":50170},{\"end\":50181,\"start\":50180},{\"end\":50193,\"start\":50192},{\"end\":50199,\"start\":50198},{\"end\":50212,\"start\":50211},{\"end\":50214,\"start\":50213},{\"end\":50428,\"start\":50427},{\"end\":50442,\"start\":50441},{\"end\":50636,\"start\":50635},{\"end\":50646,\"start\":50645},{\"end\":50657,\"start\":50656},{\"end\":50670,\"start\":50669},{\"end\":50804,\"start\":50803},{\"end\":50817,\"start\":50816},{\"end\":50828,\"start\":50827},{\"end\":50838,\"start\":50837},{\"end\":50851,\"start\":50850},{\"end\":50860,\"start\":50859},{\"end\":51167,\"start\":51166},{\"end\":51176,\"start\":51175},{\"end\":51189,\"start\":51188},{\"end\":51475,\"start\":51474},{\"end\":51477,\"start\":51476},{\"end\":51485,\"start\":51484},{\"end\":51487,\"start\":51486},{\"end\":51750,\"start\":51749},{\"end\":52107,\"start\":52106},{\"end\":52109,\"start\":52108},{\"end\":52118,\"start\":52117},{\"end\":52120,\"start\":52119},{\"end\":52128,\"start\":52127},{\"end\":52138,\"start\":52137},{\"end\":52140,\"start\":52139},{\"end\":52148,\"start\":52147},{\"end\":52150,\"start\":52149},{\"end\":52159,\"start\":52158},{\"end\":52161,\"start\":52160},{\"end\":52171,\"start\":52170},{\"end\":52185,\"start\":52184},{\"end\":52195,\"start\":52194},{\"end\":52204,\"start\":52203},{\"end\":52206,\"start\":52205},{\"end\":52521,\"start\":52520},{\"end\":52530,\"start\":52529},{\"end\":52539,\"start\":52538},{\"end\":52552,\"start\":52551},{\"end\":52839,\"start\":52838},{\"end\":52851,\"start\":52850},{\"end\":52867,\"start\":52863},{\"end\":53147,\"start\":53146},{\"end\":53299,\"start\":53298},{\"end\":53308,\"start\":53307},{\"end\":53578,\"start\":53577},{\"end\":53591,\"start\":53590},{\"end\":53604,\"start\":53603},{\"end\":53606,\"start\":53605},{\"end\":53621,\"start\":53620},{\"end\":53817,\"start\":53816},{\"end\":53823,\"start\":53822},{\"end\":53831,\"start\":53830},{\"end\":53840,\"start\":53839},{\"end\":54098,\"start\":54097},{\"end\":54106,\"start\":54105},{\"end\":54116,\"start\":54115},{\"end\":54127,\"start\":54126},{\"end\":54135,\"start\":54134},{\"end\":54694,\"start\":54693},{\"end\":54696,\"start\":54695},{\"end\":54705,\"start\":54704},{\"end\":54715,\"start\":54714},{\"end\":54734,\"start\":54733},{\"end\":54736,\"start\":54735},{\"end\":55019,\"start\":55018},{\"end\":55021,\"start\":55020},{\"end\":55030,\"start\":55029},{\"end\":55040,\"start\":55039},{\"end\":55053,\"start\":55052},{\"end\":55064,\"start\":55063},{\"end\":55301,\"start\":55300},{\"end\":55303,\"start\":55302},{\"end\":55312,\"start\":55311},{\"end\":55322,\"start\":55321},{\"end\":55324,\"start\":55323},{\"end\":55337,\"start\":55336},{\"end\":55339,\"start\":55338},{\"end\":55533,\"start\":55532},{\"end\":55542,\"start\":55541},{\"end\":55548,\"start\":55547},{\"end\":55559,\"start\":55558},{\"end\":55570,\"start\":55569},{\"end\":55745,\"start\":55744},{\"end\":55747,\"start\":55746},{\"end\":55756,\"start\":55755},{\"end\":55767,\"start\":55766}]", "bib_author_last_name": "[{\"end\":37840,\"start\":37836},{\"end\":37850,\"start\":37844},{\"end\":37864,\"start\":37854},{\"end\":37874,\"start\":37868},{\"end\":38040,\"start\":38033},{\"end\":38055,\"start\":38044},{\"end\":38064,\"start\":38059},{\"end\":38076,\"start\":38068},{\"end\":38087,\"start\":38080},{\"end\":38307,\"start\":38300},{\"end\":38323,\"start\":38311},{\"end\":38338,\"start\":38327},{\"end\":38707,\"start\":38692},{\"end\":38714,\"start\":38711},{\"end\":38722,\"start\":38718},{\"end\":38736,\"start\":38726},{\"end\":38972,\"start\":38965},{\"end\":38981,\"start\":38976},{\"end\":38994,\"start\":38985},{\"end\":39164,\"start\":39156},{\"end\":39174,\"start\":39168},{\"end\":39386,\"start\":39377},{\"end\":39397,\"start\":39390},{\"end\":39408,\"start\":39401},{\"end\":39419,\"start\":39412},{\"end\":39700,\"start\":39693},{\"end\":39711,\"start\":39704},{\"end\":39722,\"start\":39715},{\"end\":39733,\"start\":39726},{\"end\":39744,\"start\":39737},{\"end\":39984,\"start\":39980},{\"end\":40000,\"start\":39988},{\"end\":40197,\"start\":40190},{\"end\":40208,\"start\":40201},{\"end\":40223,\"start\":40212},{\"end\":40421,\"start\":40415},{\"end\":40428,\"start\":40425},{\"end\":40441,\"start\":40432},{\"end\":40463,\"start\":40445},{\"end\":40477,\"start\":40469},{\"end\":40710,\"start\":40704},{\"end\":40721,\"start\":40714},{\"end\":40939,\"start\":40934},{\"end\":40948,\"start\":40943},{\"end\":40959,\"start\":40952},{\"end\":40972,\"start\":40965},{\"end\":41211,\"start\":41206},{\"end\":41222,\"start\":41215},{\"end\":41426,\"start\":41421},{\"end\":41576,\"start\":41564},{\"end\":41585,\"start\":41580},{\"end\":41597,\"start\":41589},{\"end\":41614,\"start\":41601},{\"end\":41888,\"start\":41882},{\"end\":41899,\"start\":41892},{\"end\":41908,\"start\":41903},{\"end\":42231,\"start\":42225},{\"end\":42240,\"start\":42235},{\"end\":42248,\"start\":42244},{\"end\":42259,\"start\":42252},{\"end\":42268,\"start\":42263},{\"end\":42275,\"start\":42272},{\"end\":42288,\"start\":42279},{\"end\":42581,\"start\":42576},{\"end\":42590,\"start\":42585},{\"end\":42844,\"start\":42838},{\"end\":42854,\"start\":42848},{\"end\":42867,\"start\":42858},{\"end\":42877,\"start\":42871},{\"end\":43045,\"start\":43039},{\"end\":43059,\"start\":43049},{\"end\":43270,\"start\":43264},{\"end\":43279,\"start\":43274},{\"end\":43292,\"start\":43283},{\"end\":43310,\"start\":43298},{\"end\":43320,\"start\":43314},{\"end\":43517,\"start\":43512},{\"end\":43526,\"start\":43521},{\"end\":43537,\"start\":43530},{\"end\":43549,\"start\":43541},{\"end\":43558,\"start\":43553},{\"end\":43743,\"start\":43738},{\"end\":43756,\"start\":43747},{\"end\":43767,\"start\":43762},{\"end\":43780,\"start\":43771},{\"end\":44061,\"start\":44054},{\"end\":44075,\"start\":44067},{\"end\":44087,\"start\":44081},{\"end\":44095,\"start\":44091},{\"end\":44107,\"start\":44101},{\"end\":44376,\"start\":44369},{\"end\":44390,\"start\":44382},{\"end\":44401,\"start\":44394},{\"end\":44416,\"start\":44405},{\"end\":44438,\"start\":44420},{\"end\":44678,\"start\":44674},{\"end\":44689,\"start\":44682},{\"end\":44701,\"start\":44693},{\"end\":44888,\"start\":44881},{\"end\":44897,\"start\":44892},{\"end\":44906,\"start\":44901},{\"end\":44921,\"start\":44910},{\"end\":44934,\"start\":44925},{\"end\":44947,\"start\":44938},{\"end\":45265,\"start\":45257},{\"end\":45276,\"start\":45271},{\"end\":45521,\"start\":45513},{\"end\":45533,\"start\":45525},{\"end\":45545,\"start\":45537},{\"end\":45555,\"start\":45549},{\"end\":45568,\"start\":45559},{\"end\":45581,\"start\":45572},{\"end\":45864,\"start\":45859},{\"end\":45873,\"start\":45868},{\"end\":45883,\"start\":45877},{\"end\":46010,\"start\":46003},{\"end\":46020,\"start\":46014},{\"end\":46030,\"start\":46024},{\"end\":46045,\"start\":46034},{\"end\":46053,\"start\":46049},{\"end\":46368,\"start\":46357},{\"end\":46648,\"start\":46641},{\"end\":46659,\"start\":46652},{\"end\":46918,\"start\":46909},{\"end\":46927,\"start\":46922},{\"end\":46937,\"start\":46931},{\"end\":47104,\"start\":47100},{\"end\":47113,\"start\":47108},{\"end\":47124,\"start\":47119},{\"end\":47133,\"start\":47128},{\"end\":47412,\"start\":47406},{\"end\":47421,\"start\":47416},{\"end\":47433,\"start\":47425},{\"end\":47443,\"start\":47437},{\"end\":47646,\"start\":47640},{\"end\":47657,\"start\":47653},{\"end\":47666,\"start\":47661},{\"end\":47907,\"start\":47904},{\"end\":47918,\"start\":47911},{\"end\":48096,\"start\":48092},{\"end\":48187,\"start\":48181},{\"end\":48198,\"start\":48191},{\"end\":48209,\"start\":48202},{\"end\":48218,\"start\":48213},{\"end\":48494,\"start\":48487},{\"end\":48508,\"start\":48498},{\"end\":48518,\"start\":48512},{\"end\":48528,\"start\":48522},{\"end\":48540,\"start\":48532},{\"end\":48790,\"start\":48785},{\"end\":48806,\"start\":48796},{\"end\":49009,\"start\":49004},{\"end\":49017,\"start\":49013},{\"end\":49233,\"start\":49224},{\"end\":49246,\"start\":49239},{\"end\":49568,\"start\":49557},{\"end\":49738,\"start\":49730},{\"end\":49750,\"start\":49742},{\"end\":49760,\"start\":49754},{\"end\":50178,\"start\":50172},{\"end\":50190,\"start\":50182},{\"end\":50196,\"start\":50194},{\"end\":50209,\"start\":50200},{\"end\":50225,\"start\":50215},{\"end\":50439,\"start\":50429},{\"end\":50453,\"start\":50443},{\"end\":50643,\"start\":50637},{\"end\":50654,\"start\":50647},{\"end\":50667,\"start\":50658},{\"end\":50678,\"start\":50671},{\"end\":50814,\"start\":50805},{\"end\":50825,\"start\":50818},{\"end\":50835,\"start\":50829},{\"end\":50848,\"start\":50839},{\"end\":50857,\"start\":50852},{\"end\":50866,\"start\":50861},{\"end\":51173,\"start\":51168},{\"end\":51186,\"start\":51177},{\"end\":51195,\"start\":51190},{\"end\":51482,\"start\":51478},{\"end\":51494,\"start\":51488},{\"end\":51761,\"start\":51751},{\"end\":52115,\"start\":52110},{\"end\":52125,\"start\":52121},{\"end\":52135,\"start\":52129},{\"end\":52145,\"start\":52141},{\"end\":52156,\"start\":52151},{\"end\":52168,\"start\":52162},{\"end\":52182,\"start\":52172},{\"end\":52192,\"start\":52186},{\"end\":52201,\"start\":52196},{\"end\":52213,\"start\":52207},{\"end\":52527,\"start\":52522},{\"end\":52536,\"start\":52531},{\"end\":52549,\"start\":52540},{\"end\":52560,\"start\":52553},{\"end\":52848,\"start\":52840},{\"end\":52861,\"start\":52852},{\"end\":52874,\"start\":52868},{\"end\":53154,\"start\":53148},{\"end\":53305,\"start\":53300},{\"end\":53314,\"start\":53309},{\"end\":53588,\"start\":53579},{\"end\":53601,\"start\":53592},{\"end\":53618,\"start\":53607},{\"end\":53628,\"start\":53622},{\"end\":53820,\"start\":53818},{\"end\":53828,\"start\":53824},{\"end\":53837,\"start\":53832},{\"end\":53843,\"start\":53841},{\"end\":54103,\"start\":54099},{\"end\":54113,\"start\":54107},{\"end\":54124,\"start\":54117},{\"end\":54132,\"start\":54128},{\"end\":54147,\"start\":54136},{\"end\":54702,\"start\":54697},{\"end\":54712,\"start\":54706},{\"end\":54731,\"start\":54716},{\"end\":54744,\"start\":54737},{\"end\":55027,\"start\":55022},{\"end\":55037,\"start\":55031},{\"end\":55050,\"start\":55041},{\"end\":55061,\"start\":55054},{\"end\":55071,\"start\":55065},{\"end\":55309,\"start\":55304},{\"end\":55319,\"start\":55313},{\"end\":55334,\"start\":55325},{\"end\":55347,\"start\":55340},{\"end\":55539,\"start\":55534},{\"end\":55545,\"start\":55543},{\"end\":55556,\"start\":55549},{\"end\":55567,\"start\":55560},{\"end\":55576,\"start\":55571},{\"end\":55753,\"start\":55748},{\"end\":55764,\"start\":55757},{\"end\":55776,\"start\":55768}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53699726},\"end\":37983,\"start\":37800},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4725675},\"end\":38231,\"start\":37985},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":204008443},\"end\":38625,\"start\":38233},{\"attributes\":{\"doi\":\"arXiv:1903.09734\",\"id\":\"b3\"},\"end\":38931,\"start\":38627},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216228079},\"end\":39104,\"start\":38933},{\"attributes\":{\"doi\":\"arXiv:1707.00044\",\"id\":\"b5\"},\"end\":39322,\"start\":39106},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10908021},\"end\":39650,\"start\":39324},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2497886},\"end\":39976,\"start\":39652},{\"attributes\":{\"id\":\"b8\"},\"end\":40134,\"start\":39978},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3945595},\"end\":40355,\"start\":40136},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3801798},\"end\":40641,\"start\":40357},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2818223},\"end\":40844,\"start\":40643},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49272330},\"end\":41145,\"start\":40846},{\"attributes\":{\"id\":\"b13\"},\"end\":41345,\"start\":41147},{\"attributes\":{\"id\":\"b14\"},\"end\":41526,\"start\":41347},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3375389},\"end\":41836,\"start\":41528},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2555196},\"end\":42110,\"start\":41838},{\"attributes\":{\"id\":\"b17\"},\"end\":42523,\"start\":42112},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14154185},\"end\":42781,\"start\":42525},{\"attributes\":{\"id\":\"b19\"},\"end\":43035,\"start\":42783},{\"attributes\":{\"id\":\"b20\"},\"end\":43204,\"start\":43037},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3535298},\"end\":43480,\"start\":43206},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13496699},\"end\":43671,\"start\":43482},{\"attributes\":{\"doi\":\"arXiv:1707.06613\",\"id\":\"b23\"},\"end\":43972,\"start\":43673},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":45897076},\"end\":44323,\"start\":43974},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2077168},\"end\":44601,\"start\":44325},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":19263765},\"end\":44836,\"start\":44603},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":108301245},\"end\":45163,\"start\":44838},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":219298},\"end\":45456,\"start\":45165},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3342225},\"end\":45807,\"start\":45458},{\"attributes\":{\"id\":\"b30\"},\"end\":45999,\"start\":45809},{\"attributes\":{\"doi\":\"arXiv:1611.03071\",\"id\":\"b31\"},\"end\":46273,\"start\":46001},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7641809},\"end\":46564,\"start\":46275},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14637938},\"end\":46849,\"start\":46566},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12882511},\"end\":47060,\"start\":46851},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":59413815},\"end\":47341,\"start\":47062},{\"attributes\":{\"id\":\"b36\"},\"end\":47566,\"start\":47343},{\"attributes\":{\"doi\":\"arXiv:1802.03916\",\"id\":\"b37\"},\"end\":47849,\"start\":47568},{\"attributes\":{\"id\":\"b38\"},\"end\":48023,\"start\":47851},{\"attributes\":{\"id\":\"b39\"},\"end\":48177,\"start\":48025},{\"attributes\":{\"id\":\"b40\"},\"end\":48432,\"start\":48179},{\"attributes\":{\"doi\":\"arXiv:1908.09635\",\"id\":\"b41\"},\"end\":48732,\"start\":48434},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3311041},\"end\":48919,\"start\":48734},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":112388628},\"end\":49147,\"start\":48921},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":38041040},\"end\":49449,\"start\":49149},{\"attributes\":{\"id\":\"b45\"},\"end\":49692,\"start\":49451},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1430128},\"end\":50139,\"start\":49694},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":75455},\"end\":50355,\"start\":50141},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":10483233},\"end\":50588,\"start\":50357},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":189928055},\"end\":50801,\"start\":50590},{\"attributes\":{\"doi\":\"arXiv:1206.6471\",\"id\":\"b50\"},\"end\":51085,\"start\":50803},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":5370766},\"end\":51397,\"start\":51087},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":154872211},\"end\":51652,\"start\":51399},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":9238949},\"end\":51993,\"start\":51654},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":10253140},\"end\":52516,\"start\":51995},{\"attributes\":{\"doi\":\"arXiv:1911.00677\",\"id\":\"b55\"},\"end\":52768,\"start\":52518},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":17547265},\"end\":53095,\"start\":52770},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":18236183},\"end\":53264,\"start\":53097},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":49561627},\"end\":53535,\"start\":53266},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2047106},\"end\":53757,\"start\":53537},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":44106659},\"end\":53980,\"start\":53759},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":209386709},\"end\":54579,\"start\":53982},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":1911971},\"end\":54945,\"start\":54581},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":9697434},\"end\":55240,\"start\":54947},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":8529258},\"end\":55499,\"start\":55242},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":490669},\"end\":55688,\"start\":55501},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":9424845},\"end\":55898,\"start\":55690}]", "bib_title": "[{\"end\":37832,\"start\":37800},{\"end\":38029,\"start\":37985},{\"end\":38296,\"start\":38233},{\"end\":38961,\"start\":38933},{\"end\":39373,\"start\":39324},{\"end\":39689,\"start\":39652},{\"end\":40186,\"start\":40136},{\"end\":40411,\"start\":40357},{\"end\":40700,\"start\":40643},{\"end\":40928,\"start\":40846},{\"end\":41560,\"start\":41528},{\"end\":41878,\"start\":41838},{\"end\":42570,\"start\":42525},{\"end\":43260,\"start\":43206},{\"end\":43508,\"start\":43482},{\"end\":44050,\"start\":43974},{\"end\":44365,\"start\":44325},{\"end\":44670,\"start\":44603},{\"end\":44877,\"start\":44838},{\"end\":45251,\"start\":45165},{\"end\":45509,\"start\":45458},{\"end\":46353,\"start\":46275},{\"end\":46637,\"start\":46566},{\"end\":46905,\"start\":46851},{\"end\":47096,\"start\":47062},{\"end\":48779,\"start\":48734},{\"end\":48998,\"start\":48921},{\"end\":49220,\"start\":49149},{\"end\":49726,\"start\":49694},{\"end\":50168,\"start\":50141},{\"end\":50425,\"start\":50357},{\"end\":50633,\"start\":50590},{\"end\":51164,\"start\":51087},{\"end\":51472,\"start\":51399},{\"end\":51747,\"start\":51654},{\"end\":52104,\"start\":51995},{\"end\":52836,\"start\":52770},{\"end\":53144,\"start\":53097},{\"end\":53296,\"start\":53266},{\"end\":53575,\"start\":53537},{\"end\":53814,\"start\":53759},{\"end\":54095,\"start\":53982},{\"end\":54691,\"start\":54581},{\"end\":55016,\"start\":54947},{\"end\":55298,\"start\":55242},{\"end\":55530,\"start\":55501},{\"end\":55742,\"start\":55690}]", "bib_author": "[{\"end\":37842,\"start\":37834},{\"end\":37852,\"start\":37842},{\"end\":37866,\"start\":37852},{\"end\":37876,\"start\":37866},{\"end\":38042,\"start\":38031},{\"end\":38057,\"start\":38042},{\"end\":38066,\"start\":38057},{\"end\":38078,\"start\":38066},{\"end\":38089,\"start\":38078},{\"end\":38309,\"start\":38298},{\"end\":38325,\"start\":38309},{\"end\":38340,\"start\":38325},{\"end\":38709,\"start\":38690},{\"end\":38716,\"start\":38709},{\"end\":38724,\"start\":38716},{\"end\":38738,\"start\":38724},{\"end\":38974,\"start\":38963},{\"end\":38983,\"start\":38974},{\"end\":38996,\"start\":38983},{\"end\":39166,\"start\":39154},{\"end\":39176,\"start\":39166},{\"end\":39388,\"start\":39375},{\"end\":39399,\"start\":39388},{\"end\":39410,\"start\":39399},{\"end\":39421,\"start\":39410},{\"end\":39702,\"start\":39691},{\"end\":39713,\"start\":39702},{\"end\":39724,\"start\":39713},{\"end\":39735,\"start\":39724},{\"end\":39746,\"start\":39735},{\"end\":39986,\"start\":39978},{\"end\":40002,\"start\":39986},{\"end\":40199,\"start\":40188},{\"end\":40210,\"start\":40199},{\"end\":40225,\"start\":40210},{\"end\":40423,\"start\":40413},{\"end\":40430,\"start\":40423},{\"end\":40443,\"start\":40430},{\"end\":40465,\"start\":40443},{\"end\":40479,\"start\":40465},{\"end\":40712,\"start\":40702},{\"end\":40723,\"start\":40712},{\"end\":40941,\"start\":40930},{\"end\":40950,\"start\":40941},{\"end\":40961,\"start\":40950},{\"end\":40974,\"start\":40961},{\"end\":41213,\"start\":41202},{\"end\":41224,\"start\":41213},{\"end\":41428,\"start\":41419},{\"end\":41578,\"start\":41562},{\"end\":41587,\"start\":41578},{\"end\":41599,\"start\":41587},{\"end\":41616,\"start\":41599},{\"end\":41890,\"start\":41880},{\"end\":41901,\"start\":41890},{\"end\":41910,\"start\":41901},{\"end\":42233,\"start\":42223},{\"end\":42242,\"start\":42233},{\"end\":42250,\"start\":42242},{\"end\":42261,\"start\":42250},{\"end\":42270,\"start\":42261},{\"end\":42277,\"start\":42270},{\"end\":42290,\"start\":42277},{\"end\":42583,\"start\":42572},{\"end\":42592,\"start\":42583},{\"end\":42596,\"start\":42592},{\"end\":42846,\"start\":42834},{\"end\":42856,\"start\":42846},{\"end\":42869,\"start\":42856},{\"end\":42879,\"start\":42869},{\"end\":42886,\"start\":42879},{\"end\":43047,\"start\":43037},{\"end\":43061,\"start\":43047},{\"end\":43272,\"start\":43262},{\"end\":43281,\"start\":43272},{\"end\":43294,\"start\":43281},{\"end\":43312,\"start\":43294},{\"end\":43322,\"start\":43312},{\"end\":43519,\"start\":43510},{\"end\":43528,\"start\":43519},{\"end\":43539,\"start\":43528},{\"end\":43551,\"start\":43539},{\"end\":43560,\"start\":43551},{\"end\":43745,\"start\":43736},{\"end\":43758,\"start\":43745},{\"end\":43769,\"start\":43758},{\"end\":43782,\"start\":43769},{\"end\":44063,\"start\":44052},{\"end\":44077,\"start\":44063},{\"end\":44089,\"start\":44077},{\"end\":44097,\"start\":44089},{\"end\":44109,\"start\":44097},{\"end\":44378,\"start\":44367},{\"end\":44392,\"start\":44378},{\"end\":44403,\"start\":44392},{\"end\":44418,\"start\":44403},{\"end\":44440,\"start\":44418},{\"end\":44680,\"start\":44672},{\"end\":44691,\"start\":44680},{\"end\":44703,\"start\":44691},{\"end\":44890,\"start\":44879},{\"end\":44899,\"start\":44890},{\"end\":44908,\"start\":44899},{\"end\":44923,\"start\":44908},{\"end\":44936,\"start\":44923},{\"end\":44949,\"start\":44936},{\"end\":45267,\"start\":45253},{\"end\":45278,\"start\":45267},{\"end\":45523,\"start\":45511},{\"end\":45535,\"start\":45523},{\"end\":45547,\"start\":45535},{\"end\":45557,\"start\":45547},{\"end\":45570,\"start\":45557},{\"end\":45583,\"start\":45570},{\"end\":45866,\"start\":45857},{\"end\":45875,\"start\":45866},{\"end\":45885,\"start\":45875},{\"end\":46012,\"start\":46001},{\"end\":46022,\"start\":46012},{\"end\":46032,\"start\":46022},{\"end\":46047,\"start\":46032},{\"end\":46055,\"start\":46047},{\"end\":46370,\"start\":46355},{\"end\":46650,\"start\":46639},{\"end\":46661,\"start\":46650},{\"end\":46920,\"start\":46907},{\"end\":46929,\"start\":46920},{\"end\":46939,\"start\":46929},{\"end\":47106,\"start\":47098},{\"end\":47115,\"start\":47106},{\"end\":47126,\"start\":47115},{\"end\":47135,\"start\":47126},{\"end\":47414,\"start\":47404},{\"end\":47423,\"start\":47414},{\"end\":47435,\"start\":47423},{\"end\":47445,\"start\":47435},{\"end\":47648,\"start\":47636},{\"end\":47659,\"start\":47648},{\"end\":47668,\"start\":47659},{\"end\":47909,\"start\":47902},{\"end\":47920,\"start\":47909},{\"end\":48098,\"start\":48090},{\"end\":48189,\"start\":48179},{\"end\":48200,\"start\":48189},{\"end\":48211,\"start\":48200},{\"end\":48220,\"start\":48211},{\"end\":48496,\"start\":48485},{\"end\":48510,\"start\":48496},{\"end\":48520,\"start\":48510},{\"end\":48530,\"start\":48520},{\"end\":48542,\"start\":48530},{\"end\":48792,\"start\":48781},{\"end\":48808,\"start\":48792},{\"end\":49011,\"start\":49000},{\"end\":49019,\"start\":49011},{\"end\":49235,\"start\":49222},{\"end\":49248,\"start\":49235},{\"end\":49570,\"start\":49555},{\"end\":49740,\"start\":49728},{\"end\":49752,\"start\":49740},{\"end\":49762,\"start\":49752},{\"end\":50180,\"start\":50170},{\"end\":50192,\"start\":50180},{\"end\":50198,\"start\":50192},{\"end\":50211,\"start\":50198},{\"end\":50227,\"start\":50211},{\"end\":50441,\"start\":50427},{\"end\":50455,\"start\":50441},{\"end\":50645,\"start\":50635},{\"end\":50656,\"start\":50645},{\"end\":50669,\"start\":50656},{\"end\":50680,\"start\":50669},{\"end\":50816,\"start\":50803},{\"end\":50827,\"start\":50816},{\"end\":50837,\"start\":50827},{\"end\":50850,\"start\":50837},{\"end\":50859,\"start\":50850},{\"end\":50868,\"start\":50859},{\"end\":51175,\"start\":51166},{\"end\":51188,\"start\":51175},{\"end\":51197,\"start\":51188},{\"end\":51484,\"start\":51474},{\"end\":51496,\"start\":51484},{\"end\":51763,\"start\":51749},{\"end\":52117,\"start\":52106},{\"end\":52127,\"start\":52117},{\"end\":52137,\"start\":52127},{\"end\":52147,\"start\":52137},{\"end\":52158,\"start\":52147},{\"end\":52170,\"start\":52158},{\"end\":52184,\"start\":52170},{\"end\":52194,\"start\":52184},{\"end\":52203,\"start\":52194},{\"end\":52215,\"start\":52203},{\"end\":52529,\"start\":52520},{\"end\":52538,\"start\":52529},{\"end\":52551,\"start\":52538},{\"end\":52562,\"start\":52551},{\"end\":52850,\"start\":52838},{\"end\":52863,\"start\":52850},{\"end\":52876,\"start\":52863},{\"end\":53156,\"start\":53146},{\"end\":53307,\"start\":53298},{\"end\":53316,\"start\":53307},{\"end\":53590,\"start\":53577},{\"end\":53603,\"start\":53590},{\"end\":53620,\"start\":53603},{\"end\":53630,\"start\":53620},{\"end\":53822,\"start\":53816},{\"end\":53830,\"start\":53822},{\"end\":53839,\"start\":53830},{\"end\":53845,\"start\":53839},{\"end\":54105,\"start\":54097},{\"end\":54115,\"start\":54105},{\"end\":54126,\"start\":54115},{\"end\":54134,\"start\":54126},{\"end\":54149,\"start\":54134},{\"end\":54704,\"start\":54693},{\"end\":54714,\"start\":54704},{\"end\":54733,\"start\":54714},{\"end\":54746,\"start\":54733},{\"end\":55029,\"start\":55018},{\"end\":55039,\"start\":55029},{\"end\":55052,\"start\":55039},{\"end\":55063,\"start\":55052},{\"end\":55073,\"start\":55063},{\"end\":55311,\"start\":55300},{\"end\":55321,\"start\":55311},{\"end\":55336,\"start\":55321},{\"end\":55349,\"start\":55336},{\"end\":55541,\"start\":55532},{\"end\":55547,\"start\":55541},{\"end\":55558,\"start\":55547},{\"end\":55569,\"start\":55558},{\"end\":55578,\"start\":55569},{\"end\":55755,\"start\":55744},{\"end\":55766,\"start\":55755},{\"end\":55778,\"start\":55766}]", "bib_venue": "[{\"end\":49945,\"start\":49862},{\"end\":54296,\"start\":54231},{\"end\":37880,\"start\":37876},{\"end\":38093,\"start\":38089},{\"end\":38410,\"start\":38344},{\"end\":38688,\"start\":38627},{\"end\":39009,\"start\":38996},{\"end\":39152,\"start\":39106},{\"end\":39470,\"start\":39421},{\"end\":39795,\"start\":39746},{\"end\":40021,\"start\":40002},{\"end\":40234,\"start\":40225},{\"end\":40486,\"start\":40479},{\"end\":40734,\"start\":40723},{\"end\":40982,\"start\":40974},{\"end\":41200,\"start\":41147},{\"end\":41417,\"start\":41347},{\"end\":41665,\"start\":41616},{\"end\":41959,\"start\":41910},{\"end\":42221,\"start\":42112},{\"end\":42639,\"start\":42596},{\"end\":42832,\"start\":42783},{\"end\":43092,\"start\":43061},{\"end\":43329,\"start\":43322},{\"end\":43564,\"start\":43560},{\"end\":43734,\"start\":43673},{\"end\":44121,\"start\":44109},{\"end\":44450,\"start\":44440},{\"end\":44707,\"start\":44703},{\"end\":44982,\"start\":44949},{\"end\":45298,\"start\":45278},{\"end\":45611,\"start\":45583},{\"end\":45855,\"start\":45809},{\"end\":46110,\"start\":46071},{\"end\":46410,\"start\":46370},{\"end\":46694,\"start\":46661},{\"end\":46944,\"start\":46939},{\"end\":47184,\"start\":47135},{\"end\":47402,\"start\":47343},{\"end\":47634,\"start\":47568},{\"end\":47900,\"start\":47851},{\"end\":48088,\"start\":48025},{\"end\":48280,\"start\":48220},{\"end\":48483,\"start\":48434},{\"end\":48816,\"start\":48808},{\"end\":49025,\"start\":49019},{\"end\":49283,\"start\":49248},{\"end\":49553,\"start\":49451},{\"end\":49860,\"start\":49762},{\"end\":50234,\"start\":50227},{\"end\":50462,\"start\":50455},{\"end\":50684,\"start\":50680},{\"end\":50916,\"start\":50883},{\"end\":51226,\"start\":51197},{\"end\":51516,\"start\":51496},{\"end\":51808,\"start\":51763},{\"end\":52230,\"start\":52215},{\"end\":52912,\"start\":52876},{\"end\":53167,\"start\":53156},{\"end\":53379,\"start\":53316},{\"end\":53634,\"start\":53630},{\"end\":53858,\"start\":53845},{\"end\":54229,\"start\":54149},{\"end\":54749,\"start\":54746},{\"end\":55080,\"start\":55073},{\"end\":55356,\"start\":55349},{\"end\":55582,\"start\":55578},{\"end\":55782,\"start\":55778}]"}}}, "year": 2023, "month": 12, "day": 17}