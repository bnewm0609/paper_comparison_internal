{"id": 197618493, "updated": "2022-02-06 03:21:48.864", "metadata": {"title": "CompHD: Efficient Hyperdimensional Computing Using Model Compression", "authors": "[{\"middle\":[],\"last\":\"Morris\",\"first\":\"Justin\"},{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Bosch\",\"first\":\"Samuel\"},{\"middle\":[],\"last\":\"Thomas\",\"first\":\"Anthony\"},{\"middle\":[],\"last\":\"Shu\",\"first\":\"Helen\"},{\"middle\":[],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "journal": "2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Hyperdimensional (HD) computing is a mathematical framework, inspired by neuroscience, which can be used to represent many machine learning (ML) problems. Data is first encoded into high dimensional space (on the order of 103 or 104 dimensions) to create hypervectors. HD computing combines these hypervectors to create a model used for inference. However, due to the high dimensionality of the hypervectors, inference in HD is very expensive, especially when it runs on embedded devices with limited resources. One naive approach to improve the efficiency of HD computing is to simply lower the dimensionality of hypervectors, which comes with a corresponding loss in accuracy. However, if the data is compressed intelligently, we can reduce the dimensionality of an HD model without sacrificing accuracy. To that end, we propose CompHD, a novel approach for compressing HD models while maintaining the accuracy of the original model. CompHD utilizes the mathematics of high-dimensional spaces to compress hypervectors into shorter vectors while maintaining the information of full length hypervectors. We evaluated the efficiency of CompHD on a variety of applications. Our results show that CompHD can reduce model size by an average of 69.7%, resulting in a execution time speed up of 4.1 \u00d7 and improving energy efficiency by 74% while maintaining the accuracy of the original model. This enables more low powered IoT devices to utilize HD computing for ML problems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2971739303", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/islped/MorrisIBTSR19", "doi": "10.1109/islped.2019.8824908"}}, "content": {"source": {"pdf_hash": "cb5da27f5f82efae20cdc0f494e37cd3d5254b18", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e33fff97bac17482e296779f837cdf94ee6f1b67", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cb5da27f5f82efae20cdc0f494e37cd3d5254b18.txt", "contents": "\nCompHD: Efficient Hyperdimensional Computing Using Model Compression\n\n\nJustin Morris \nMohsen Imani \nSamuel Bosch \nAnthony Thomas \nHelen Shu \nTajana Rosing \nCompHD: Efficient Hyperdimensional Computing Using Model Compression\n\nHyperdimensional (HD) computing is a mathematical framework, inspired by neuroscience, which can be used to represent many machine learning (ML) problems. Data is first encoded into high dimensional space (on the order of 10 3 or 10 4 dimensions) to create hypervectors. HD computing combines these hypervectors to create a model used for inference. However, due to the high dimensionality of the hypervectors, inference in HD is very expensive, especially when it runs on embedded devices with limited resources. One naive approach to improve the efficiency of HD computing is to simply lower the dimensionality of hypervectors, which comes with a corresponding loss in accuracy. However, if the data is compressed intelligently, we can reduce the dimensionality of an HD model without sacrificing accuracy. To that end, we propose CompHD, a novel approach for compressing HD models while maintaining the accuracy of the original model. CompHD utilizes the mathematics of highdimensional spaces to compress hypervectors into shorter vectors while maintaining the information of full length hypervectors. We evaluated the efficiency of CompHD on a variety of applications. Our results show that CompHD can reduce model size by an average of 69.7%, resulting in a execution time speed up of 4.1\u00d7 and improving energy efficiency by 74% while maintaining the accuracy of the original model. This enables more low powered IoT devices to utilize HD computing for ML problems.\n\nI. INTRODUCTION\n\nThe emergence of the Internet of Things (IoT) has created an abundance of small embedded devices [1]. Many of these devices are used for cognitive tasks such as: face detection, speech recognition, image classification, activity recognition, etc. These devices want to run learning algorithms such as: deep Neural Networks (DNNs), (AlexNet [2], and GoogleNet [3] that provide excellent accuracy for cognitive tasks. However, these embedded devices have limited resources, such as limited battery power or limited memory [4]. Therefore, these devices are unable to run these resource intensive algorithms. To get around the resource limitation on embedded devices, many of them send the data they collect to a cloud server, which performs the resource intensive cognitive tasks. However, this is not desirable for many users due to security and network communication costs [5]. Thus, we need more efficient light-weight classifiers to perform cognitive tasks on embedded systems.\n\nBrain-inspired Hyperdimensional (HD) computing has been proposed as a light-weight classifier to perform cognitive tasks on resource limited systems. Inspired by research from neuroscience, HD computing represents data as points in a high dimensional space. Past research utilized high dimension vectors (D \u2265 1,000), called hypervectors (HV), to represent neural activity in the brain. Prior work found that HD is able to provide high accuracy results for many cognitive tasks at a much lower computational cost than other learning algorithms [6], [7], [8], [9]. Work in [10] proposed a general encoding module that maps feature vectors into highdimensional space while keeping most of the original data. Prior work also tried to design hardware acceleration for HD computing by mapping its operations into hardware, e.g., FPGA [11], [12], [13], [14], and tried to accelerate HD computing in hardware by binarizing the class hypervectors [15] or removing dimensions of the class hypervectors [16]. However, removing dimensions and binarizing the HD model causes accuracy loss because information captured by the HD model is removed. To achieve the best accuracy, HD computing needs to be trained with an integer model, but when using (D = 1,000) HVs to create and train the HD model, HD computing can still be too resource intensive for embedded systems.\n\nModel size in HD computing is important because it increases the information storage capabilities of the model, resulting in better distinction between classes. However, with larger model sizes, there is a corresponding increase in computational costs. For example, when an HD model has D dimensionality and k classes, every query request costs k * D additions and multiplications. To achieve acceptable accuracy, HD models typically use very high dimensionality. This is costly for embedded devices with limited resources. Inference requests take too long to calculate and additionally, the HD model may not fit into the main memory of embedded devices.\n\nIn this paper, we propose a robust and efficient solution to the computational complexity and spacial constraints of HD computing while maintaining comparable accuracy. Prior work reduced computation by simply lowering the dimensionality [7]. However, this approach leads to data loss in HD computing as the information in the dropped dimensions is no longer kept in the model. Our proposed HD computing framework, called CompHD, utilizes the mathematics of high dimensional spaces to reduce the size of the HD model and thus reduces the number of computations at inference time. CompHD splits up each class HV into s separate components and combines them into a reduced d << D dimensional model. This method reduces the model size and number of computations by a factor of s while maintaining comparable accuracy to the original HD model. Using an empirical evaluation on several real world datasets, we show that CompHD can reduce the model size by an average of 69.7%, improve efficiency by 74%, and speed up execution time by 4.1\u00d7, while maintaining the accuracy of the original model. Our results show that CompHD enables more low powered IoT devices to solve ML problems with HD computing. Hyperdimensional computing is a computing paradigm involving long vectors with dimensionality in the thousands, called hypervectors [17]. There are several nearly orthogonal HVs in high-dimensional space [18]. HD combines these HVs with well-defined vector operations, while preserving most of the information from each individual HV. No one dimension in a HV has more responsibility to store any piece of information than any other component because HVs are holographic and (pseudo) random with i.i.d. components and a full holistic representation. The mathematics of high-dimensional space enable HD to be easily applied to different learning problems. Figure 1 shows an overview of the structure of an HD model. HD computing consists of an encoder and a classifier. The encoder maps input data into hypervectors. The HVs are then combined to create one class HV to represent each class and stored in the classifier. The classifier uses the cosine similarity of any input HV with all of the class HVs to determine the output class. The class with the highest cosine similarity is selected as the output class.\n\n\nA. Encoding\n\nCompHD is a general framework that can be used to compress an HD model for any classification task. In IoT systems, devices usually get data from sensor nodes, which produce time-series data. Here we explain CompHD functionality in the context of time-series classification. We use an encoder designed for time series signals [19] to encode feature vectors into high-dimensional space. Our encoding first quantizes the feature values into m levels and assign a \"level hypervector\" L to each. The following equations show how the Ls are used to encode a n length time series signal to generate the j th training data HV in the i th class where N is the length of the N-gram window, G k is an intermediate HV that is calculated for each N-gram step and \u03c1 x is defined as a rotational shift to the right by x: \nG k = [\u03c1 0 (L 1 ) + \u03c1 1 (L 2 ) + . . . + \u03c1 N \u22121 (L N )] H j i = [G 1 + G 2 + . . . + G n\u2212N ] B.({H 1 i , H 2 i , . . . , H j i })\n, HD computing adds them together to create a single class HV (C i ).\nC i = H 1 i + H 2 i + . . . + H j i\nOnce this is done for every class, we have an HD model that can be used for inference. By creating a model in one pass through the training dataset, HD computing uses significantly less energy than other learning algorithms that need to take multiple passes over the training dataset to train a model. After training, all class HVs are stored in the classifier.\n\n\nC. Associative Search\n\nUpon inference, the encoder first maps the input data into a query HV (Q), using the same encoding that was used to train the HD model. A similarity metric is used to determine the strength of a match between the query HV and each class HV. The most common metric used in HD computing is cosine similarity, but note that other metrics (e.g. Hamming distance) could be appropriate for other types of problems. After the cosine similarity is computed between the query HV and each class HV in the classifier, the class with the highest cosine similarity is chosen as the output class.\n\n\nD. Challenges\n\nThere are challenges when running HD computing on embedded devices with limited resources. Storing HVs in D = 1, 000 dimensions may require more resources than these devices have. Additionally, k * D multiplications and additions need to be performed upon inference on a model with k classes and D = 1, 000 dimension. This is costly for embedded devices with limited resources.\n\nOne solution is to reduce the dimensionality of the HD model. This method is effective at reducing the model size as well as the number of operations for inference [7]. However, as Table I shows, lowering dimensionality results in a trade off between accuracy and efficiency. As the dimensionality reduces, efficiency increases, i.e., faster and more energyefficient computation, at the cost of accuracy. For example, when D is reduced from 1,000 to 250, on average, there is a 2.78\u00d7 speed up at the cost of 39.23% of classification  accuracy. Therefore, simply reducing dimensionality does not provide acceleration without the cost of losing accuracy. Our goal is to design a framework which enables dimension reduction in HD computing with no or minimal impact on the classification accuracy.\n\n\nIII. MODEL COMPRESSION\n\nHere we present our novel approach to accelerate HD computing by reducing the dimensionality. CompHD exploits the mathematics of high-dimensional spaces in order to reduce the effective dimensionality of the trained HD model while providing minimal loss in accuracy. Instead of using vectors with D = 1, 000 dimensionality representing each class HV, CompHD compresses each class HV to d dimensionality where d << D. CompHD splits the trained class HVs into s equal segments, where each piece has d = D/s dimensions. We then combine all s segments of each class HV to create a new HV in d dimensions. Combining these segments needs to preserve the information of each individual partition, otherwise CompHD would lose classification accuracy.\n\n\nA. Compression\n\nEach class HV is split into s equal segments with d = D/s dimensionality,\nC i = {C 1 i , C 2 i , . . . , C s i },\nwhere C j i is the j th piece of the i th class HV. CompHD could generate a d dimension class HV by just adding these segments together. However, this approach does not keep the positional information of each individual piece, which is important since HD works based on the pattern of similarity in high dimensional space. Thus, it is crucial to know the pattern of each individual partition.\n\nTo preserve the positional information, CompHD generates a set of HVs, {P 1 , P 2 , ..., P s }, where P i \u2208 {\u22121, 1} D . These HVs are generated semi-randomly with the Hadamard method [20] to ensure that they are mutually orthogonal. We do this by generating a d \u00d7 d sized Hadamard matrix, which is a matrix with elements of +1 and \u22121 where each row is mutually orthogonal to every other row. We only require the use of the first s rows to use as our set of HVs, P, because we only need one P for each segment. Using these HVs, we can uniquely store the information of each partition in a combined HV. We compress the class HVs by multiplying each segment of the class HV by a unique P and then adding each result up to create the compressed HV. The following equation shows how this compression is calculated: \nC = s i=1 P i C i\n\nB. Inference\n\nTo match the dimensionality of the HD model, the query hypervector is compressed using the same procedure as used for the class HVs. Figure 2 (b) shows how CompHD compresses the query HV during inference. In testing, HD computing checks the similarity of the query HV with each compressed class HVs by calculating the cosine similarity: (C \u00b7 Q)/( C Q ). Cosine similarity can be simplified to just calculating the dot product plus a division by storing the length of the class hypervectors. The division by the length of the query hypervector can be dropped because it simply scales the result and does not change which class will be selected. Using the non-compressed model, HD can perform the dot product between the query and class HV in D dimensions.\nQ = s i=1 P i Q i\u03b4 = C \u00b7 Q C\nThis operation is very costly for D = 1, 000 because it takes D multiplications and additions. After compressing the model to d = D/s dimensionality, calculating the dot product can be done in d dimensions. Thus, the compressed model gains an approximate speed up of s over the full model upon inference. The speedup is approximately s because the query HV needs to be compressed as well before calculating the cosine similarity with the compressed model.\n\nOnce the query HV is compressed, CompHD selects the class with the highest cosine similarity to the compressed query HV, which is calculated using:\nargmax i=1:k {\u03b4 Q , C i }\nTo maintain the accuracy of the full sized HD model, the dot product between the compressed class HV (C i ) and compressed query HV (Q ) needs to be as close to the dot product of the full sized class HV (C i ) and full sized query HV (Q). When the dot product of the compressed model is foiled out, the terms of the resulting dot product of the compressed model can be split up into two parts, noise and data. Noise occurs when a term has mismatching Ps and data occurs when a term has matching Ps.\nQ \u00b7 C i = ( s i=1 P i C i ) \u00b7 ( s j=1 P j Q j ) Q \u00b7 C i = i=j P i C i P i Q i data + i =j P i C i P j Q j noise\nIf we only kept the terms with matching Ps, the resulting dot product would be equal to the dot product of the full sized model. Therefore, to achieve the same results as the full sized model our design needs to minimize the noisy terms.\n\nCompHD achieves this by ensuring that every P is mutually orthogonal to each other by generating them with the Hadamard method. Therefore, when two different Ps are in the same term, their inner product is approximately zero and the resulting dot product is minimized. Thus, resulting in the dot products of the compressed model being approximately equal to the dot products of the full sized model. Figure 3 shows the distributions of the dot products of data terms and the dot products of noisy terms for the Valve Monitoring dataset with s = 4 and s = 10. It is clear that data terms are contributing to the resulting dot product significantly more than noisy terms. Therefore, the error introduced by noisy terms does not have much impact on the resulting dot product. This leaves the dot product between C i and Q with the the terms where there are matching Ps, because all other terms are minuscule in comparison:\nC \u00b7 Q \u2248 i=j P i C i P i Q i\nThis is the desired result because the dot product in the compressed model is approximately equal to the dot product of the full sized model, thus reducing the error introduced when compressing the model. Additionally, we can see in figure 3 when the compression factor increases, the distance between noisy terms and data terms is reduced, resulting in a less accurate model. CompHD uses this information about the ratio of noisy data to real data to selectively pick the best compression factor to use that gives comparable accuracy to the full sized model with a significant improvement on efficiency. \n\n\nA. Experimental Setup\n\nWe implemented CompHD training and inference in both software and hardware. In software, we implemented CompHD using C++ code. We also implemented CompHD on two embedded devices: a Raspberry Pi 3 using ARM Cortex A53 CPU and a Kintex-7 FPGA. For the FPGA, we implemented CompHD using Verilog. We verify the timing and the functionality of the models by synthesizing them using the Xilinx Vivado Design Suite [21]. The synthesis code has been implemented on the Kintex-7 FPGA KC705 Evaluation Kit.\n\nWe test the efficiency of the proposed approach on three practical applications: Activity Recognition [22]: Using motion sensor data from 5 sensor units with each unit containing 9 sensors, the objective is to recognize the activity performed. The training and testing datasets are taken from the Daily Sports and Activities dataset. This dataset consists of 8 subjects performing 19 different activities in their own style for a 5 minute duration. All of the sensors record data at 25Hz during the 5 minute interval and each 5 minute interval is then divided into 5 second intervals to create 60 separate data samples each containing 5,625 data points.\n\nValve Monitoring [23] The goal of this task is to determine if the condition of the valves in a hydraulic system are optimal, have a small lag, have sever lag, or are failing. The training and testing datasets are taken from the Condition Monitoring of Hydraulic Systems dataset. This dataset consists of 2205 samples of sensor data from a hydraulic system. Each sample has the data from 17 separate sensors over a duration of 60 seconds totaling 43,680 data points per sample. Gesture Recognition [24]: Here we try to recognize five different hand gestures: rested hand, closed hand, open hand, 2-finger pinch, and point index. The gestures were sampled at 500Hz with the use of an elastic band containing four EMG sensors. We used the data collected from five different subjects. The data was collected by each subject performing 10 repetitions of each gesture for three seconds each with a three second resting period in between. Therefore, each sample contains 6,000 data points.   Figure 4 shows the ratios of real data in the dot product of the compressed model to the noisy data in the dot product of the compressed model. The graphs show that when increasing the compression factor, the ratio of real data to noisy data decreases. This is because as the compression factor increases, the amount of real terms in the dot product linearly increases. Meanwhile, the amount of noisy terms quadraticly increases. Therefore, even though the noisy terms are much smaller than the real data terms, as the compression factor increases, the amount of noisy terms grows faster than the amount of real data terms, bringing down the ratio. This results in a decrease in accuracy as the compression factor increases too far. CompHD chooses a compression factor such that the ratio of real data to noisy data in the dot product is sufficiently high to maintain a comparable accuracy to the full sized model and the compression factor is large enough to improve on efficiency over the full sized model. Based on our results, a data ratio of 5 or higher is enough to ensure a comparable accuracy to the full sized model. As figure 4 shows, CompHD chose a compression factor of s = 20 for all three datasets.\n\n\nB. CompHD & Compression Factor\n\n\nC. CompHD Accuracy\n\nTable II compares the classification accuracy of CompHD with the classification accuracy of the baseline method dimension reduction as the model size decreases. The data shows that when reducing the length of the hypervectors with dimension reduction, there is a significant trade off between model size and classification accuracy. For example, on the Valve Monitoring dataset, when dimension reduction reduces the length of the hypervectors by 90%, the model loses 48.98% accuracy. As the dimensionality is reduced further with dimension reduction, more accuracy loss is observed. CompHD reduces this trade off by a significant amount by compressing the full sized model rather than simply reducing the dimensionality.\n\nCompHD is able to reduce the model size while maintaining a comparable accuracy to a full sized model. For instance, when reducing the length of the hypervectors by 95% with CompHD, the model maintains the same accuracy as the original model for the Valve Monitoring dataset. On average, CompHD loses 65.33% less accuracy than dimension reduction while reducing the length of the hypervectors by 95%. Although, there is a point where CompHD loses more accuracy than desired for each dataset. For example, with the Gesture Recognition dataset, when s = 40 the accuracy drops by 4.35% from the original model. However, CompHD is still 27.21% more accurate than dimension reduction. This shows that CompHD is capable of maintaining the accuracy of the original model while reducing the model size up to a break point. Additionally, CompHD is strictly better than dimension reduction at reducing model size because CompHD never loses as much accuracy as dimension reduction for the same model size reduction. Our evaluation shows that CompHD is a robust way to reduce model size while maintaining a comparable accuracy to the original model. Figure 5 compares the energy consumption, execution time, and model size of CompHD using different compression factors. The baseline is also represented in the graphs, as a compression factor of 1 is the baseline. The data shows that CompHD improves the energy consumption, execution time, and model size of HD computing as the compression factor increases. All results are reported when applications are running on a Kintex-7 FPGA. As stated before, the improvement is closely linear with respect to the compression factor. For example, when s = 10 for the Activity Recognition dataset, CompHD uses 8.09\u00d7 less energy than the baseline and gains a speed up of 4.47\u00d7. However, because of the overheads of our design, it is not completely linear. The overheads of our design come from the need to compress hypervectors from the original high-dimensional model. Compressing the query HV may seem expensive, however, due to Ps \u2208 {\u22121, 1} D , the multiplications with the class HV segments can be reduced to deciding if the subsequent operation when combining the segments will be addition or subtraction. The graph of energy consumption shows the additional energy needed by CompHD to compress the query hypervector. Additionally, the graph of execution time shows the additional time needed to compress the query hypervector. Lastly, the graph showing the model size of CompHD shows how much additional space is need for the Ps that are needed to compress the query hypervector. Despite these overheads, the graphs in figure 5 show that CompHD still improves the energy consumption, execution time, and model size of HD computing nearly linearly with respect to the compression factor. Table III compares the efficiency of CompHD with dimension reduction when they provide the same accuracy. The dimension reduction design and CompHD are compared by implementing them on the Kintex-7 FPGA KC705 Evaluation Kit. The data highlights the improvement that CompHD has over dimension reduction at the same accuracy. CompHD is able to provide the same accuracy as dimension reduction while saving more energy. This is because dimension reduction improves the efficiency of the HD model by just lowering the dimensionality. This reduces the amount of information that can be stored in the HD model, causing a significant loss of accuracy. CompHD reduces this trade off by compressing the model instead of just lowering the dimensionality. Compressing the HD model saves important information in the larger model needed to keep the accuracy high. This allows CompHD to decrease the model size further than dimension reduction while providing the same accuracy. Even though CompHD is less efficient than dimension reduction at the same effective dimension due to the need of compressing the query HV, by preserving the information of the larger model, CompHD is able to reduce the dimensionality even further. Overall, CompHD is able to speed up execution time by an average of 4.1\u00d7 and improve efficiency by an average of 74% more than dimension reduction while providing the same accuracy.\n\n\nD. CompHD Efficiency\n\n\nE. Efficiency Considering Quality\n\n\nV. CONCLUSION\n\nIn this paper, we proposed a new method to reduce the size of HD models without a trade-off of accuracy. CompHD achieves this by dividing the class HVs into s segments and combining those segments together with well defined vector operations that reduce the amount of information lost. Once combined, the new class HVs have dimensionality d = D/s. This speeds up inference by approximately s times and reduces the model size by approximately s times. This enables HD to be run on a wider range of embedded devices with limited resources. CompHD can reduce model size by an average of 69.7%, resulting in a execution time speeding up by 4.1\u00d7 and improving energy efficiency by 74% while maintaining the same accuracy of the original model. Our results show that CompHD enables more low powered IoT devices to solve ML problems with HD computing.\n\nFig. 2 .Fig. 3 .\n23CompHD compression of (a) an HD model and (b) a query data. Histogram of the distributions of the dot products of matching terms (data) and mismaching terms (noise)\n\nFigure 2 (\n2a) shows how CompHD creates a compressed HD model using trained class HVs and a set of Ps. This approach reduces the HD model dimension from D to d, where d << D.\n\nFig. 4 .\n4Ratio of real data to noisy data for different values of effective dimension IV. EVALUATION\n\nFig. 5 .\n5Energy consumption, execution time, and model size of CompHD using different compression factors.\n\nTABLE I EFFECT\nIOF REDUCING DIMENSIONALITY ON ACCURACY AND EXECUTIONTIME \n\nAccuracy \nTesting Time \nDimension (D) \n1000 \n250 \n100 \n1000 \n250 \n100 \n\nActivity Recognition \n100% \n5.3% \n5.2% \n49.4\u03bc s 17.78\u03bc s 9.88\u03bc s \nValve Monitoring \n100% \n83.7% 51.0% \n10.4\u03bc s \n3.74\u03bc s \n2.08\u03bc s \nGesture Recognition \n91.1% 84.4% 62.9% \n13.0\u03bc s \n4.68\u03bc s \n2.6\u03bc s \n\nII. HIGH-DIMENSIONAL COMPUTING \n\n\n\nTraining\nHD computing supports efficient one-pass training. To build a one-pass model, the encoder maps all training data to training HVs (H). For all training HVs within a classSimilarity check \n\nTraining Data \n\nTraining Data \n@ Class k \n\nTraining Data \n@ Class 2 \n\nTraining Data \n@ Class 1 \nEncoding \n\nQuery \n\nClass 1 (C1) \n\nClass 2 (C2) \n\nClass k (Ck) \n\nInference \n\nEncoding \n\nEncoding \n\nEncoding \n\nAssociative Memory \n\nDistance Similarity \n\nTraining \n\nTraining \nModule \n\nInference \nData \n\nFig. 1. Overview of creating an HD model and performing inference with \nan HD model \n\n\n\nTABLE II COMPARING\nIITHE EFFECT OF REDUCING MODEL SIZE WITH CompHD AND DIMENSION REDUCTION ON ACCURACYDataset \nActivity Recognition \nValve Monitoring \nGesture Recognition \n\nEffective D \ns \nBaseline CompHD \nBaseline CompHD \nBaseline CompHD \n\n1,000 \n1 \n100% \n100% \n100% \n100% \n91.04% \n91.04% \n500 \n2 \n100% \n100% \n100% \n100% \n88.33% \n90.66% \n100 \n10 \n5.26% \n100% \n51.02% \n100% \n62.88% \n91.17% \n50 \n20 \n5.26% \n100% \n50.32% \n100% \n39.01% \n89.94% \n25 \n40 \n5.26% \n57.89% \n51.02% \n83.67% \n26.85% \n86.69% \n\n\n\nTABLE III EFFICIENCY\nIIIIMPROVEMENT AND SPEEDUP OF CompHD OVER DIMENSION REDUCTION FOR THE SAME ACCURACY.Dataset \nActivity Recognition \nValve Monitoring Gesture Recognition \n\nEnergy Improv. \n85% \n66.25% \n71% \nSpeedup \n5.31\u00d7 \n3.35\u00d7 \n3.7\u00d7 \n\n\nACKNOWLEDGEMENTSThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1730158 and #1527034.\nInternet of things (iot): A vision, architectural elements, and future directions. J Gubbi, R Buyya, S Marusic, M Palaniswami, Future generation computer systems. 297J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami, \"Internet of things (iot): A vision, architectural elements, and future directions,\" Future generation computer systems, vol. 29, no. 7, pp. 1645-1660, 2013.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep con- volutional neural networks,\" in Advances in neural information processing systems, pp. 1097-1105, 2012.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van- houcke, and A. Rabinovich, \"Going deeper with convolutions,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.\n\nEdge-centric computing: Vision and challenges. P Garcia Lopez, A Montresor, D Epema, A Datta, T Higashino, A Iamnitchi, M Barcellos, P Felber, E Riviere, ACM SIGCOMM Computer Communication Review. 455P. Garcia Lopez, A. Montresor, D. Epema, A. Datta, T. Higashino, A. Iamnitchi, M. Barcellos, P. Felber, and E. Riviere, \"Edge-centric computing: Vision and chal- lenges,\" ACM SIGCOMM Computer Communication Review, vol. 45, no. 5, pp. 37- 42, 2015.\n\nThe emergence of edge computing. M Satyanarayanan, Computer. 501M. Satyanarayanan, \"The emergence of edge computing,\" Computer, vol. 50, no. 1, pp. 30-39, 2017.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Transactions on Neural Networks and Learning Systems. 99O. Rasanen and J. Saarinen, \"Sequence prediction with sparse distributed hyper- dimensional coding applied to the analysis of mobile phone use patterns,\" IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1-12, 2015.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM108M. Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdimensional computing for energy efficient classification,\" in Proceedings of the 55th Annual Design Automation Conference, p. 108, ACM, 2018.\n\nHdcluster: An accurate clustering using brain-inspired highdimensional computing. M Imani, DATE. M. Imani et al., \"Hdcluster: An accurate clustering using brain-inspired high- dimensional computing,\" in DATE, IEEE/ACM, 2019.\n\nA framework for collaborative learning in secure high-dimensional space. M Imani, IEEE CLOUDIEEEM. Imani et al., \"A framework for collaborative learning in secure high-dimensional space,\" in IEEE CLOUD, pp. 1-6, IEEE, 2019.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, International Conference on Rebooting Computing (ICRC). IEEEM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"Voicehd: Hyperdimensional com- puting for efficient speech recognition,\" in International Conference on Rebooting Computing (ICRC), pp. 1-6, IEEE, 2017.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, High Performance Computer Architecture (HPCA. IEEE2017 IEEE International Symposium onM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdimen- sional associative memory,\" in High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on, pp. 445-456, IEEE, 2017.\n\nFach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity. M Imani, ASPDAC. ACMM. Imani et al., \"Fach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity,\" in ASPDAC, pp. 493-498, ACM, 2019.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, FPGA. ACMS. Salamat et al., \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdi- mensional computing,\" in FPGA, pp. 53-62, ACM, 2019.\n\nFelix: fast and energy-efficient logic in memory. S Gupta, ICCAD. ACM55S. Gupta et al., \"Felix: fast and energy-efficient logic in memory,\" in ICCAD, p. 55, ACM, 2018.\n\nA binary learning framework for hyperdimensional computing. M Imani, DATE. M. Imani et al., \"A binary learning framework for hyperdimensional computing,\" in DATE, IEEE/ACM, 2019.\n\nSparsehd: Algorithm-hardware co-optimization for efficient highdimensional computing. M Imani, IEEE FCCM. IEEEM. Imani et al., \"Sparsehd: Algorithm-hardware co-optimization for efficient high- dimensional computing,\" in IEEE FCCM, pp. 1-6, IEEE, 2019.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in dis- tributed representation with high-dimensional random vectors,\" Cognitive Compu- tation, vol. 1, no. 2, pp. 139-159, 2009.\n\nEncoding structure in boolean space. P Kanerva, ICANN 98. SpringerP. Kanerva, \"Encoding structure in boolean space,\" in ICANN 98, pp. 387-392, Springer, 1998.\n\nHyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of eeg error-related potentials. A Rahimi, P Kanerva, J D R Mill\u00e1n, J M Rabaey, 10th EAI Int. Conf. on Bio-inspired Information and Communications Technologies. A. Rahimi, P. Kanerva, J. d. R. Mill\u00e1n, and J. M. Rabaey, \"Hyperdimensional com- puting for noninvasive brain-computer interfaces: Blind and one-shot classification of eeg error-related potentials,\" in 10th EAI Int. Conf. on Bio-inspired Information and Communications Technologies, 2017.\n\nHadamard matrix. \"Hadamard matrix.\" https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/ scipy.linalg.hadamard.html.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite,\" White Paper, vol. 5, 2012.\n\nUci machine learning repository. \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/Daily+ and+Sports+Activities.\n\nUci machine learning repository. \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/Condition+ monitoring+of+hydraulic+systems.\n\nAnalysis of robust implementation of an emg pattern recognition based control. S Benatti, E Farella, E Gruppioni, L Benini, BIOSIGNALS. S. Benatti, E. Farella, E. Gruppioni, and L. Benini, \"Analysis of robust implementa- tion of an emg pattern recognition based control.,\" in BIOSIGNALS, pp. 45-54, 2014.\n", "annotations": {"author": "[{\"start\":\"72\",\"end\":\"86\"},{\"start\":\"87\",\"end\":\"100\"},{\"start\":\"101\",\"end\":\"114\"},{\"start\":\"115\",\"end\":\"130\"},{\"start\":\"131\",\"end\":\"141\"},{\"start\":\"142\",\"end\":\"156\"}]", "publisher": null, "author_last_name": "[{\"start\":\"79\",\"end\":\"85\"},{\"start\":\"94\",\"end\":\"99\"},{\"start\":\"108\",\"end\":\"113\"},{\"start\":\"123\",\"end\":\"129\"},{\"start\":\"137\",\"end\":\"140\"},{\"start\":\"149\",\"end\":\"155\"}]", "author_first_name": "[{\"start\":\"72\",\"end\":\"78\"},{\"start\":\"87\",\"end\":\"93\"},{\"start\":\"101\",\"end\":\"107\"},{\"start\":\"115\",\"end\":\"122\"},{\"start\":\"131\",\"end\":\"136\"},{\"start\":\"142\",\"end\":\"148\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"69\"},{\"start\":\"157\",\"end\":\"225\"}]", "venue": null, "abstract": "[{\"start\":\"227\",\"end\":\"1697\"}]", "bib_ref": "[{\"start\":\"1813\",\"end\":\"1816\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2056\",\"end\":\"2059\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2075\",\"end\":\"2078\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2236\",\"end\":\"2239\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2588\",\"end\":\"2591\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3239\",\"end\":\"3242\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"3244\",\"end\":\"3247\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"3249\",\"end\":\"3252\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"3254\",\"end\":\"3257\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3267\",\"end\":\"3271\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"3524\",\"end\":\"3528\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"3530\",\"end\":\"3534\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"3536\",\"end\":\"3540\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"3542\",\"end\":\"3546\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"3634\",\"end\":\"3638\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"3688\",\"end\":\"3692\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"4946\",\"end\":\"4949\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"6036\",\"end\":\"6040\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"6108\",\"end\":\"6112\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"7357\",\"end\":\"7361\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"9605\",\"end\":\"9608\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"11714\",\"end\":\"11718\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"16628\",\"end\":\"16632\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"16820\",\"end\":\"16824\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"17871\",\"end\":\"17875\",\"attributes\":{\"ref_id\":\"b23\"}}]", "figure": "[{\"start\":\"25486\",\"end\":\"25670\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"25671\",\"end\":\"25846\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"25847\",\"end\":\"25949\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"25950\",\"end\":\"26058\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"26059\",\"end\":\"26437\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"26438\",\"end\":\"27018\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"27019\",\"end\":\"27518\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"27519\",\"end\":\"27759\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1716\",\"end\":\"2694\"},{\"start\":\"2696\",\"end\":\"4050\"},{\"start\":\"4052\",\"end\":\"4706\"},{\"start\":\"4708\",\"end\":\"7015\"},{\"start\":\"7031\",\"end\":\"7838\"},{\"start\":\"7969\",\"end\":\"8038\"},{\"start\":\"8075\",\"end\":\"8436\"},{\"start\":\"8462\",\"end\":\"9044\"},{\"start\":\"9062\",\"end\":\"9439\"},{\"start\":\"9441\",\"end\":\"10235\"},{\"start\":\"10262\",\"end\":\"11004\"},{\"start\":\"11023\",\"end\":\"11096\"},{\"start\":\"11137\",\"end\":\"11529\"},{\"start\":\"11531\",\"end\":\"12341\"},{\"start\":\"12375\",\"end\":\"13129\"},{\"start\":\"13159\",\"end\":\"13614\"},{\"start\":\"13616\",\"end\":\"13763\"},{\"start\":\"13790\",\"end\":\"14289\"},{\"start\":\"14402\",\"end\":\"14639\"},{\"start\":\"14641\",\"end\":\"15560\"},{\"start\":\"15589\",\"end\":\"16194\"},{\"start\":\"16220\",\"end\":\"16716\"},{\"start\":\"16718\",\"end\":\"17371\"},{\"start\":\"17373\",\"end\":\"19571\"},{\"start\":\"19627\",\"end\":\"20347\"},{\"start\":\"20349\",\"end\":\"24564\"},{\"start\":\"24641\",\"end\":\"25485\"}]", "formula": "[{\"start\":\"7839\",\"end\":\"7934\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"7934\",\"end\":\"7968\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"8039\",\"end\":\"8074\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"11097\",\"end\":\"11136\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"12342\",\"end\":\"12359\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"13130\",\"end\":\"13147\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"13147\",\"end\":\"13158\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"13764\",\"end\":\"13789\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"14290\",\"end\":\"14401\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"15561\",\"end\":\"15588\",\"attributes\":{\"id\":\"formula_9\"}}]", "table_ref": "[{\"start\":\"9622\",\"end\":\"9629\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"23169\",\"end\":\"23178\",\"attributes\":{\"ref_id\":\"tab_0\"}}]", "section_header": "[{\"start\":\"1699\",\"end\":\"1714\"},{\"start\":\"7018\",\"end\":\"7029\"},{\"start\":\"8439\",\"end\":\"8460\"},{\"start\":\"9047\",\"end\":\"9060\"},{\"start\":\"10238\",\"end\":\"10260\"},{\"start\":\"11007\",\"end\":\"11021\"},{\"start\":\"12361\",\"end\":\"12373\"},{\"start\":\"16197\",\"end\":\"16218\"},{\"start\":\"19574\",\"end\":\"19604\"},{\"start\":\"19607\",\"end\":\"19625\"},{\"start\":\"24567\",\"end\":\"24587\"},{\"start\":\"24590\",\"end\":\"24623\"},{\"start\":\"24626\",\"end\":\"24639\"},{\"start\":\"25487\",\"end\":\"25503\"},{\"start\":\"25672\",\"end\":\"25682\"},{\"start\":\"25848\",\"end\":\"25856\"},{\"start\":\"25951\",\"end\":\"25959\"},{\"start\":\"26060\",\"end\":\"26074\"},{\"start\":\"26439\",\"end\":\"26447\"},{\"start\":\"27020\",\"end\":\"27038\"},{\"start\":\"27520\",\"end\":\"27540\"}]", "table": "[{\"start\":\"26128\",\"end\":\"26437\"},{\"start\":\"26617\",\"end\":\"27018\"},{\"start\":\"27122\",\"end\":\"27518\"},{\"start\":\"27625\",\"end\":\"27759\"}]", "figure_caption": "[{\"start\":\"25506\",\"end\":\"25670\"},{\"start\":\"25684\",\"end\":\"25846\"},{\"start\":\"25858\",\"end\":\"25949\"},{\"start\":\"25961\",\"end\":\"26058\"},{\"start\":\"26076\",\"end\":\"26128\"},{\"start\":\"26448\",\"end\":\"26617\"},{\"start\":\"27041\",\"end\":\"27122\"},{\"start\":\"27544\",\"end\":\"27625\"}]", "figure_ref": "[{\"start\":\"6559\",\"end\":\"6567\"},{\"start\":\"12508\",\"end\":\"12516\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"15041\",\"end\":\"15049\"},{\"start\":\"15822\",\"end\":\"15830\"},{\"start\":\"18359\",\"end\":\"18367\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"19488\",\"end\":\"19496\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"21487\",\"end\":\"21495\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"23001\",\"end\":\"23009\",\"attributes\":{\"ref_id\":\"fig_3\"}}]", "bib_author_first_name": "[{\"start\":\"28009\",\"end\":\"28010\"},{\"start\":\"28018\",\"end\":\"28019\"},{\"start\":\"28027\",\"end\":\"28028\"},{\"start\":\"28038\",\"end\":\"28039\"},{\"start\":\"28368\",\"end\":\"28369\"},{\"start\":\"28382\",\"end\":\"28383\"},{\"start\":\"28395\",\"end\":\"28396\"},{\"start\":\"28397\",\"end\":\"28398\"},{\"start\":\"28682\",\"end\":\"28683\"},{\"start\":\"28693\",\"end\":\"28694\"},{\"start\":\"28700\",\"end\":\"28701\"},{\"start\":\"28707\",\"end\":\"28708\"},{\"start\":\"28719\",\"end\":\"28720\"},{\"start\":\"28727\",\"end\":\"28728\"},{\"start\":\"28739\",\"end\":\"28740\"},{\"start\":\"28748\",\"end\":\"28749\"},{\"start\":\"28761\",\"end\":\"28762\"},{\"start\":\"29203\",\"end\":\"29204\"},{\"start\":\"29205\",\"end\":\"29211\"},{\"start\":\"29219\",\"end\":\"29220\"},{\"start\":\"29232\",\"end\":\"29233\"},{\"start\":\"29241\",\"end\":\"29242\"},{\"start\":\"29250\",\"end\":\"29251\"},{\"start\":\"29263\",\"end\":\"29264\"},{\"start\":\"29276\",\"end\":\"29277\"},{\"start\":\"29289\",\"end\":\"29290\"},{\"start\":\"29299\",\"end\":\"29300\"},{\"start\":\"29638\",\"end\":\"29639\"},{\"start\":\"29889\",\"end\":\"29890\"},{\"start\":\"29900\",\"end\":\"29901\"},{\"start\":\"30297\",\"end\":\"30298\"},{\"start\":\"30306\",\"end\":\"30307\"},{\"start\":\"30315\",\"end\":\"30316\"},{\"start\":\"30323\",\"end\":\"30324\"},{\"start\":\"30733\",\"end\":\"30734\"},{\"start\":\"30950\",\"end\":\"30951\"},{\"start\":\"31172\",\"end\":\"31173\"},{\"start\":\"31181\",\"end\":\"31182\"},{\"start\":\"31189\",\"end\":\"31190\"},{\"start\":\"31199\",\"end\":\"31200\"},{\"start\":\"31516\",\"end\":\"31517\"},{\"start\":\"31525\",\"end\":\"31526\"},{\"start\":\"31535\",\"end\":\"31536\"},{\"start\":\"31543\",\"end\":\"31544\"},{\"start\":\"31553\",\"end\":\"31554\"},{\"start\":\"31555\",\"end\":\"31556\"},{\"start\":\"31973\",\"end\":\"31974\"},{\"start\":\"32231\",\"end\":\"32232\"},{\"start\":\"32441\",\"end\":\"32442\"},{\"start\":\"32620\",\"end\":\"32621\"},{\"start\":\"32826\",\"end\":\"32827\"},{\"start\":\"33118\",\"end\":\"33119\"},{\"start\":\"33392\",\"end\":\"33393\"},{\"start\":\"33652\",\"end\":\"33653\"},{\"start\":\"33662\",\"end\":\"33663\"},{\"start\":\"33673\",\"end\":\"33674\"},{\"start\":\"33675\",\"end\":\"33678\"},{\"start\":\"33687\",\"end\":\"33688\"},{\"start\":\"33689\",\"end\":\"33690\"},{\"start\":\"34217\",\"end\":\"34218\"},{\"start\":\"34672\",\"end\":\"34673\"},{\"start\":\"34683\",\"end\":\"34684\"},{\"start\":\"34694\",\"end\":\"34695\"},{\"start\":\"34707\",\"end\":\"34708\"}]", "bib_author_last_name": "[{\"start\":\"28011\",\"end\":\"28016\"},{\"start\":\"28020\",\"end\":\"28025\"},{\"start\":\"28029\",\"end\":\"28036\"},{\"start\":\"28040\",\"end\":\"28051\"},{\"start\":\"28370\",\"end\":\"28380\"},{\"start\":\"28384\",\"end\":\"28393\"},{\"start\":\"28399\",\"end\":\"28405\"},{\"start\":\"28684\",\"end\":\"28691\"},{\"start\":\"28695\",\"end\":\"28698\"},{\"start\":\"28702\",\"end\":\"28705\"},{\"start\":\"28709\",\"end\":\"28717\"},{\"start\":\"28721\",\"end\":\"28725\"},{\"start\":\"28729\",\"end\":\"28737\"},{\"start\":\"28741\",\"end\":\"28746\"},{\"start\":\"28750\",\"end\":\"28759\"},{\"start\":\"28763\",\"end\":\"28773\"},{\"start\":\"29212\",\"end\":\"29217\"},{\"start\":\"29221\",\"end\":\"29230\"},{\"start\":\"29234\",\"end\":\"29239\"},{\"start\":\"29243\",\"end\":\"29248\"},{\"start\":\"29252\",\"end\":\"29261\"},{\"start\":\"29265\",\"end\":\"29274\"},{\"start\":\"29278\",\"end\":\"29287\"},{\"start\":\"29291\",\"end\":\"29297\"},{\"start\":\"29301\",\"end\":\"29308\"},{\"start\":\"29640\",\"end\":\"29654\"},{\"start\":\"29891\",\"end\":\"29898\"},{\"start\":\"29902\",\"end\":\"29910\"},{\"start\":\"30299\",\"end\":\"30304\"},{\"start\":\"30308\",\"end\":\"30313\"},{\"start\":\"30317\",\"end\":\"30321\"},{\"start\":\"30325\",\"end\":\"30331\"},{\"start\":\"30735\",\"end\":\"30740\"},{\"start\":\"30952\",\"end\":\"30957\"},{\"start\":\"31174\",\"end\":\"31179\"},{\"start\":\"31183\",\"end\":\"31187\"},{\"start\":\"31191\",\"end\":\"31197\"},{\"start\":\"31201\",\"end\":\"31207\"},{\"start\":\"31518\",\"end\":\"31523\"},{\"start\":\"31527\",\"end\":\"31533\"},{\"start\":\"31537\",\"end\":\"31541\"},{\"start\":\"31545\",\"end\":\"31551\"},{\"start\":\"31557\",\"end\":\"31563\"},{\"start\":\"31975\",\"end\":\"31980\"},{\"start\":\"32233\",\"end\":\"32240\"},{\"start\":\"32443\",\"end\":\"32448\"},{\"start\":\"32622\",\"end\":\"32627\"},{\"start\":\"32828\",\"end\":\"32833\"},{\"start\":\"33120\",\"end\":\"33127\"},{\"start\":\"33394\",\"end\":\"33401\"},{\"start\":\"33654\",\"end\":\"33660\"},{\"start\":\"33664\",\"end\":\"33671\"},{\"start\":\"33679\",\"end\":\"33685\"},{\"start\":\"33691\",\"end\":\"33697\"},{\"start\":\"34219\",\"end\":\"34224\"},{\"start\":\"34674\",\"end\":\"34681\"},{\"start\":\"34685\",\"end\":\"34692\"},{\"start\":\"34696\",\"end\":\"34705\"},{\"start\":\"34709\",\"end\":\"34715\"}]", "bib_entry": "[{\"start\":\"27926\",\"end\":\"28301\",\"attributes\":{\"matched_paper_id\":\"204982032\",\"id\":\"b0\"}},{\"start\":\"28303\",\"end\":\"28648\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b1\"}},{\"start\":\"28650\",\"end\":\"29154\",\"attributes\":{\"matched_paper_id\":\"206592484\",\"id\":\"b2\"}},{\"start\":\"29156\",\"end\":\"29603\",\"attributes\":{\"matched_paper_id\":\"207232279\",\"id\":\"b3\"}},{\"start\":\"29605\",\"end\":\"29765\",\"attributes\":{\"matched_paper_id\":\"12563598\",\"id\":\"b4\"}},{\"start\":\"29767\",\"end\":\"30218\",\"attributes\":{\"matched_paper_id\":\"15258913\",\"id\":\"b5\"}},{\"start\":\"30220\",\"end\":\"30649\",\"attributes\":{\"matched_paper_id\":\"49301394\",\"id\":\"b6\"}},{\"start\":\"30651\",\"end\":\"30875\",\"attributes\":{\"matched_paper_id\":\"155106744\",\"id\":\"b7\"}},{\"start\":\"30877\",\"end\":\"31100\",\"attributes\":{\"id\":\"b8\"}},{\"start\":\"31102\",\"end\":\"31467\",\"attributes\":{\"matched_paper_id\":\"21351739\",\"id\":\"b9\"}},{\"start\":\"31469\",\"end\":\"31873\",\"attributes\":{\"matched_paper_id\":\"1677864\",\"id\":\"b10\"}},{\"start\":\"31875\",\"end\":\"32144\",\"attributes\":{\"matched_paper_id\":\"58027670\",\"id\":\"b11\"}},{\"start\":\"32146\",\"end\":\"32389\",\"attributes\":{\"matched_paper_id\":\"67872077\",\"id\":\"b12\"}},{\"start\":\"32391\",\"end\":\"32558\",\"attributes\":{\"matched_paper_id\":\"53235957\",\"id\":\"b13\"}},{\"start\":\"32560\",\"end\":\"32738\",\"attributes\":{\"matched_paper_id\":\"155109576\",\"id\":\"b14\"}},{\"start\":\"32740\",\"end\":\"32991\",\"attributes\":{\"matched_paper_id\":\"189824904\",\"id\":\"b15\"}},{\"start\":\"32993\",\"end\":\"33353\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b16\"}},{\"start\":\"33355\",\"end\":\"33513\",\"attributes\":{\"matched_paper_id\":\"57912849\",\"id\":\"b17\"}},{\"start\":\"33515\",\"end\":\"34068\",\"attributes\":{\"matched_paper_id\":\"8996877\",\"id\":\"b18\"}},{\"start\":\"34070\",\"end\":\"34194\",\"attributes\":{\"id\":\"b19\"}},{\"start\":\"34196\",\"end\":\"34299\",\"attributes\":{\"matched_paper_id\":\"110511037\",\"id\":\"b20\"}},{\"start\":\"34301\",\"end\":\"34438\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"34440\",\"end\":\"34591\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"34593\",\"end\":\"34897\",\"attributes\":{\"matched_paper_id\":\"15953456\",\"id\":\"b23\"}}]", "bib_title": "[{\"start\":\"27926\",\"end\":\"28007\"},{\"start\":\"28303\",\"end\":\"28366\"},{\"start\":\"28650\",\"end\":\"28680\"},{\"start\":\"29156\",\"end\":\"29201\"},{\"start\":\"29605\",\"end\":\"29636\"},{\"start\":\"29767\",\"end\":\"29887\"},{\"start\":\"30220\",\"end\":\"30295\"},{\"start\":\"30651\",\"end\":\"30731\"},{\"start\":\"31102\",\"end\":\"31170\"},{\"start\":\"31469\",\"end\":\"31514\"},{\"start\":\"31875\",\"end\":\"31971\"},{\"start\":\"32146\",\"end\":\"32229\"},{\"start\":\"32391\",\"end\":\"32439\"},{\"start\":\"32560\",\"end\":\"32618\"},{\"start\":\"32740\",\"end\":\"32824\"},{\"start\":\"32993\",\"end\":\"33116\"},{\"start\":\"33355\",\"end\":\"33390\"},{\"start\":\"33515\",\"end\":\"33650\"},{\"start\":\"34196\",\"end\":\"34215\"},{\"start\":\"34593\",\"end\":\"34670\"}]", "bib_author": "[{\"start\":\"28009\",\"end\":\"28018\"},{\"start\":\"28018\",\"end\":\"28027\"},{\"start\":\"28027\",\"end\":\"28038\"},{\"start\":\"28038\",\"end\":\"28053\"},{\"start\":\"28368\",\"end\":\"28382\"},{\"start\":\"28382\",\"end\":\"28395\"},{\"start\":\"28395\",\"end\":\"28407\"},{\"start\":\"28682\",\"end\":\"28693\"},{\"start\":\"28693\",\"end\":\"28700\"},{\"start\":\"28700\",\"end\":\"28707\"},{\"start\":\"28707\",\"end\":\"28719\"},{\"start\":\"28719\",\"end\":\"28727\"},{\"start\":\"28727\",\"end\":\"28739\"},{\"start\":\"28739\",\"end\":\"28748\"},{\"start\":\"28748\",\"end\":\"28761\"},{\"start\":\"28761\",\"end\":\"28775\"},{\"start\":\"29203\",\"end\":\"29219\"},{\"start\":\"29219\",\"end\":\"29232\"},{\"start\":\"29232\",\"end\":\"29241\"},{\"start\":\"29241\",\"end\":\"29250\"},{\"start\":\"29250\",\"end\":\"29263\"},{\"start\":\"29263\",\"end\":\"29276\"},{\"start\":\"29276\",\"end\":\"29289\"},{\"start\":\"29289\",\"end\":\"29299\"},{\"start\":\"29299\",\"end\":\"29310\"},{\"start\":\"29638\",\"end\":\"29656\"},{\"start\":\"29889\",\"end\":\"29900\"},{\"start\":\"29900\",\"end\":\"29912\"},{\"start\":\"30297\",\"end\":\"30306\"},{\"start\":\"30306\",\"end\":\"30315\"},{\"start\":\"30315\",\"end\":\"30323\"},{\"start\":\"30323\",\"end\":\"30333\"},{\"start\":\"30733\",\"end\":\"30742\"},{\"start\":\"30950\",\"end\":\"30959\"},{\"start\":\"31172\",\"end\":\"31181\"},{\"start\":\"31181\",\"end\":\"31189\"},{\"start\":\"31189\",\"end\":\"31199\"},{\"start\":\"31199\",\"end\":\"31209\"},{\"start\":\"31516\",\"end\":\"31525\"},{\"start\":\"31525\",\"end\":\"31535\"},{\"start\":\"31535\",\"end\":\"31543\"},{\"start\":\"31543\",\"end\":\"31553\"},{\"start\":\"31553\",\"end\":\"31565\"},{\"start\":\"31973\",\"end\":\"31982\"},{\"start\":\"32231\",\"end\":\"32242\"},{\"start\":\"32441\",\"end\":\"32450\"},{\"start\":\"32620\",\"end\":\"32629\"},{\"start\":\"32826\",\"end\":\"32835\"},{\"start\":\"33118\",\"end\":\"33129\"},{\"start\":\"33392\",\"end\":\"33403\"},{\"start\":\"33652\",\"end\":\"33662\"},{\"start\":\"33662\",\"end\":\"33673\"},{\"start\":\"33673\",\"end\":\"33687\"},{\"start\":\"33687\",\"end\":\"33699\"},{\"start\":\"34217\",\"end\":\"34226\"},{\"start\":\"34672\",\"end\":\"34683\"},{\"start\":\"34683\",\"end\":\"34694\"},{\"start\":\"34694\",\"end\":\"34707\"},{\"start\":\"34707\",\"end\":\"34717\"}]", "bib_venue": "[{\"start\":\"28053\",\"end\":\"28087\"},{\"start\":\"28407\",\"end\":\"28456\"},{\"start\":\"28775\",\"end\":\"28852\"},{\"start\":\"29310\",\"end\":\"29351\"},{\"start\":\"29656\",\"end\":\"29664\"},{\"start\":\"29912\",\"end\":\"29969\"},{\"start\":\"30333\",\"end\":\"30392\"},{\"start\":\"30742\",\"end\":\"30746\"},{\"start\":\"30877\",\"end\":\"30948\"},{\"start\":\"31209\",\"end\":\"31263\"},{\"start\":\"31565\",\"end\":\"31609\"},{\"start\":\"31982\",\"end\":\"31988\"},{\"start\":\"32242\",\"end\":\"32246\"},{\"start\":\"32450\",\"end\":\"32455\"},{\"start\":\"32629\",\"end\":\"32633\"},{\"start\":\"32835\",\"end\":\"32844\"},{\"start\":\"33129\",\"end\":\"33150\"},{\"start\":\"33403\",\"end\":\"33411\"},{\"start\":\"33699\",\"end\":\"33778\"},{\"start\":\"34070\",\"end\":\"34085\"},{\"start\":\"34226\",\"end\":\"34237\"},{\"start\":\"34301\",\"end\":\"34332\"},{\"start\":\"34440\",\"end\":\"34471\"},{\"start\":\"34717\",\"end\":\"34727\"},{\"start\":\"28854\",\"end\":\"28916\"},{\"start\":\"30394\",\"end\":\"30438\"}]"}}}, "year": 2023, "month": 12, "day": 17}