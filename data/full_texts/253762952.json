{"id": 253762952, "updated": "2023-10-10 02:14:15.335", "metadata": {"title": "High-resolution image reconstruction with latent diffusion models from human brain activity", "authors": "[{\"first\":\"Yu\",\"last\":\"Takagi\",\"middle\":[]},{\"first\":\"Shinji\",\"last\":\"Nishimoto\",\"middle\":[]}]", "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2023, "month": 6, "day": 1}, "abstract": "Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straight-forward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at https://sites.google.com/view/stablediffusion-withbrain/.", "fields_of_study": "[\"Biology\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/TakagiN23", "doi": "10.1109/cvpr52729.2023.01389"}}, "content": {"source": {"pdf_hash": "84c7c9f854c37bfc3386dc22ed85eb0104ede992", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.biorxiv.org/content/biorxiv/early/2022/12/01/2022.11.18.517004.full.pdf", "status": "GREEN"}}, "grobid": {"id": "94250dc515971ed180bcb562e154ee5b53c527de", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/84c7c9f854c37bfc3386dc22ed85eb0104ede992.txt", "contents": "\nHigh-resolution image reconstruction with latent diffusion models from human brain activity\n\n\nYu Takagi takagi.yuu.fbs@osaka-u.ac.jp \nGraduate School of Frontier Biosciences\nOsaka University\nJapan\n\nCiNet\nNICT\nJapan\n\nShinji Nishimoto nishimoto.shinji.fbs@osaka-u.ac.jp \nGraduate School of Frontier Biosciences\nOsaka University\nJapan\n\nCiNet\nNICT\nJapan\n\nHigh-resolution image reconstruction with latent diffusion models from human brain activity\n10.1109/CVPR52729.2023.01389\nFigure 1. Presented images (red box, top row) and images reconstructed from fMRI signals (gray box, bottom row) for one subject (subj01).AbstractReconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straight-* Corresponding author forward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at https://sites.google.com/view/stablediffusion-withbrain/.\n\nIntroduction\n\nA fundamental goal of computer vision is to construct artificial systems that see and recognize the world as human visual systems do. Recent developments in the measurement of population brain activity, combined with advances in the implementation and design of deep neural network models, have allowed direct comparisons between latent representations in biological brains and architectural characteristics of artificial networks, providing important insights into how these systems operate [3, 8-10, 13, 18, 19, 21, 42, 43, 54, 55]. These efforts have in-cluded the reconstruction of visual experiences (perception or imagery) from brain activity, and the examination of potential correspondences between the computational processes associated with biological and artificial systems [2,5,7,24,25,27,36,[44][45][46].\n\nReconstructing visual images from brain activity, such as that measured by functional Magnetic Resonance Imaging (fMRI), is an intriguing but challenging problem, because the underlying representations in the brain are largely unknown, and the sample size typically associated with brain data is relatively small [17,26,30,32]. In recent years, researchers have started addressing this task using deep-learning models and algorithms, including generative adversarial networks (GANs) and self-supervised learning [2,5,7,24,25,27,36,[44][45][46]. Additionally, more recent studies have increased semantic fidelity by explicitly using the semantic content of images as auxiliary inputs for reconstruction [5,25]. However, these studies require training new generative models with fMRI data from scratch, or fine-tuning toward the specific stimuli used in the fMRI experiment. These efforts have shown impressive but limited success in pixel-wise and semantic fidelity, partly because the number of samples in neuroscience is small, and partly because learning complex generative models poses numerous challenges.\n\nDiffusion models (DMs) [11,47,48,53] are deep generative models that have been gaining attention in recent years. DMs have achieved state-of-the-art performance in several tasks involving conditional image generation [4,39,49], image super resolution [40], image colorization [38], and other related tasks [6,16,33,41]. In addition, recently proposed latent diffusion models (LDMs) [37] have further reduced computational costs by utilizing the latent space generated by their autoencoding component, enabling more efficient computations in the training and inference phases. Another advantage of LDMs is their ability to generate highresolution images with high semantic fidelity. However, because LDMs have been introduced only recently, we still lack a satisfactory understanding of their internal mechanisms. Specifically, we still need to discover how they represent latent signals within each layer of DMs, how the latent representation changes throughout the denoising process, and how adding noise affects conditional image generation.\n\nHere, we attempt to tackle the above challenges by reconstructing visual images from fMRI signals using an LDM named Stable Diffusion. This architecture is trained on a large dataset and carries high text-to-image generative performance. We show that our simple framework can reconstruct high-resolution images with high semantic fidelity without any training or fine-tuning of complex deeplearning models. We also provide biological interpretations of each component of the LDM, including forward/reverse diffusion processes, U-Net, and latent representations with different noise levels.\n\nOur contributions are as follows: (i) We demonstrate that our simple framework can reconstruct high-resolution (512 \u00d7 512) images from brain activity with high semantic fidelity, without the need for training or fine-tuning of complex deep generative models ( Figure 1); (ii) We quantitatively interpret each component of an LDM from a neuroscience perspective, by mapping specific components to distinct brain regions; (iii) We present an objective interpretation of how the text-to-image conversion process implemented by an LDM incorporates the semantic information expressed by the conditional text, while at the same time maintaining the appearance of the original image.\n\n\nRelated Work\n\n\nReconstructing visual image from fMRI\n\nDecoding visual experiences from fMRI activity has been studied in various modalities. Examples include explicitly presented visual stimuli [17,26,30,32], semantic content of the presented stimuli [15,31,52], imagined content [13,29], perceived emotions [12,20,51], and many other related applications [14,28]. In general, these decoding tasks are made difficult by the low signal-to-noise ratio and the relatively small sample size associated with fMRI data.\n\nWhile early attempts have used handcrafted features to reconstruct visual images from fMRI [17,26,30,32], recent studies have begun to use deep generative models trained on a large number of naturalistic images [2,5,7,24,25,27,36,[44][45][46]. Additionally, a few studies have used semantic information associated with the images, including categorical or text information, to increase the semantic fidelity of the reconstructed images [5,25]. To produce high-resolution reconstructions, these studies require training and possibly fine-tuning of generative models, such as GANs, with the same dataset used in the fMRI experiments. These requirements impose serious limitations, because training complex generative models is in general challenging, and the number of samples in neuroscience is relatively small. Thus, even modern implementations struggle to produce images, at most 256 \u00d7 256 resolution, with high semantic fidelity unless they are augmented with numerous tools and techniques. DMs and LDMs are recent algorithms for image generation that could potentially address these limitations, thanks to their ability to generate diverse high-resolution images with high semantic fidelity of text-conditioning, and high computational efficiency. However, to the best of our knowledge, no prior studies have used DMs for visual reconstruction.\n\n\nEncoding Models\n\nTo understand deep-learning models from a biological perspective, neuroscientists have employed encoding models: a predictive model of brain activity is built out of features extracted from different components of the deeplearning models, followed by examination of the potential link between model representations and corresponding brain processes [3, 8-10, 13, 18, 19, 21, 42, 43, 54, 55]. Because brains and deep-learning models share similar goals (e.g., recognition of the world) and thus could implement similar functions, the ability to establish connections between these two structures provides us with biological interpretations of the architecture underlying deep-learning models, otherwise viewed as black boxes. For example, the activation patterns observed within early and late layers of a CNN correspond to the neural activity patterns measured from early and late layers of visual cortex, suggesting the existence of a hierarchical correspondence between latent representations of a CNN and those present in the brain [9,10,13,19,54,55]. This approach has been applied primarily to vision science, but it has recently been extended to other sensory modalities and higher functions [3,8,18,21,42,43].\n\nCompared with biologically inspired architectures such as CNNs, the correspondence between DMs and the brain is less obvious. By examining the relationship between each component and process of DMs and corresponding brain activities, we were able to obtain biological interpretations of DMs, for example in terms of how latent vectors, denoising processes, conditioning operations, and U-net components may correspond to our visual streams. To our knowledge, no prior study has investigated the relationship between DMs and the brain.\n\nTogether, our overarching goal is to use DMs for high resolution visual reconstruction and to use brain encoding framework to better understand the underlying mechanisms of DMs and its correspondence to the brain. \n\n\nMethods\n\n\nDataset\n\nWe used the Natural Scenes Dataset (NSD) for this project [1]. Please visit the NSD website for more details 1 . Briefly, NSD provides data acquired from a 7-Tesla fMRI scanner over 30-40 sessions during which each subject viewed three repetitions of 10,000 images. We analyzed data for four of the eight subjects who completed all imaging sessions (subj01, subj02, subj05, and subj07). The images used in the NSD experiments were retrieved from MS COCO and cropped to 425 \u00d7 425 (if needed). We used  denotes an image encoder, D is a image decoder, and \u03c4 is a text encoder (CLIP). (Middle) Schematic of decoding analysis. We decoded latent representations of the presented image (z) and associated text c from fMRI signals within early (blue) and higher (yellow) visual cortices, respectively. These latent representations were used as input to produce a reconstructed image Xzc. (Bottom) Schematic of encoding analysis. We built encoding models to predict fMRI signals from different components of LDM, including z, c, and zc. 27,750 trials from NSD for each subject (2,250 trials out of the total 30,000 trials were not publicly released by NSD). For a subset of those trials (N=2,770 trials), 982 images were viewed by all four subjects. Those trials were used as the test dataset, while the remaining trials (N=24,980) were used as the training dataset.\n\nFor functional data, we used the preprocessed scans (resolution of 1.8 mm) provided by NSD. See Appendix A for details of the preprocessing protocol. We used single-trial beta weights estimated from generalized linear models and region of interests (ROIs) for early and higher (ventral) visual regions provided by NSD. For the test dataset, we used the average of the three trials associated with each image. For the training dataset, we used the three separate trials without averaging.\n\n\nLatent Diffusion Models\n\nDMs are probabilistic generative models that restore a sampled variable from Gaussian noise to a sample of the learned data distribution via iterative denoising. Given training data, the diffusion process destroys the structure of the data by gradually adding Gaussian noise. The sample at each time point is defined as\nx t = \u221a \u03b1 t x 0 + \u221a 1 \u2212 \u03b1 t t where x t is a noisy version of input x 0 , t \u2208 {1, ..., T },\n\u03b1 is a hyperparameter, and is the Gaussian. The inverse diffusion process is modeled by applying a neural network f \u03b8 (x t , t) to the samples at each step to recover the original input. The learning objective is f \u03b8 (x, t) \u2248 t [11,47]. U-Net is commonly used for neural networks f \u03b8 .\n\nThis method can be generalized to learning conditional distributions by inserting auxiliary input c into the neural network. If we set the latent representation of the text sequence to c, it can implement text-to-image models. Recent studies have shown that, by using large language and image models, DMs can create realistic, high-resolution images from text inputs. Furthermore, when we start from source image with input texts, we can generate new text conditional images by editing the image. In this image-to-image translation, the degree of degradation from the original image is controlled by a parameter that can be adjusted to preserve either the semantic content or the appearance of the original image.\n\nDMs that operate in pixel space are computationally expensive. LDMs overcome this limitation by compressing the input using an autoencoder (Figure 2, top). Specifically, the autoencoder is first trained with image data, and the diffusion model is trained to generate its latent representation z using a U-Net architecture. In doing so, it refers to conditional inputs via cross-attention. This allows for lightweight inference compared with pixel-based DMs, and for very high-quality text-to-image and image-to-image implementations.\n\nIn this study, we used an LDM called Stable Diffusion, which was built on LDMs and trained on a very large dataset. The model can generate and modify images based on text input. Text input is projected to a fixed latent representation by a pretrained text encoder (CLIP) [34]. We used version 1.4 of the model. See Appendix A for details on the training protocol.\n\nWe define z as the latent representation of the original image compressed by the autoencoder, c as the latent representation of texts (average of five text annotations asso-ciated to each MS COCO image), and z c as the generated latent representation of z modified by the model with c. We used these representations in the decoding/encoding models described below.\n\n\nDecoding: reconstructing images from fMRI\n\nWe performed visual reconstruction from fMRI signals using LDM in three simple steps as follows (Figure 2, middle). The only training required in our method is to construct linear models that map fMRI signals to each LDM component, and no training or fine-tuning of deep-learning models is needed. We used the default parameters of imageto-image and text-to-image codes provided by the authors of LDM 2 , including the parameters used for the DDIM sampler. See Appendix A for details.\n\n(i) First, we predicted a latent representation z of the presented image X from fMRI signals within early visual cortex. z was then processed by an decoder of autoencoder to produce a coarse decoded image X z with a size of 320 \u00d7 320, and then resized it to 512 \u00d7 512.\n\n(ii) X z was then processed by encoder of autoencoder, then added noise through the diffusion process.\n\n(iii) We decoded latent text representations c from fMRI signals within higher (ventral) visual cortex. Noise-added latent representations z T of the coarse image and decoded c were used as input to the denoising U-Net to produce z c . Finally, z c was used as input to the decoding module of the autoencoder to produce a final reconstructed image X zc with a size of 512 \u00d7 512.\n\nTo construct models from fMRI to the components of LDM, we used L2-regularized linear regression, and all models were built on a per subject basis. Weights were estimated from training data, and regularization parameters were explored during the training using 5-fold crossvalidation. We resized original images from 425 \u00d7 425 to 320 \u00d7 320 but confirmed that resizing them to a larger size (448 \u00d7 448) does not affect the quality of reconstruction.\n\nAs control analyses, we also generated images using only z or c. To generate these control images, we simply omitted c or z from step (iii) above, respectively.\n\nThe accuracy of image reconstruction was evaluated objectively (perceptual similarity metrics, psms) and subjectively (human raters, N=6) by assessing whether the original test images (N=982 images) could be identified from the generated images. As a similarity metrics of PSMs, we used early/middle/late layers of CLIP and CNN (AlexNet) [22]. Briefly, we conducted two-way identification experiments: examined whether the image reconstructed from fMRI was more similar to the corresponding original image than randomly picked reconstructed image. See Appendix B for details and additional results.\n\n\nEncoding: Whole-brain Voxel-wise Modeling\n\nNext, we tried to interpret the internal operations of LDMs by mapping them to brain activity. For this purpose, we constructed whole-brain voxel-wise encoding models for the following four settings (see Figure 2 bottom and Appendix A for implementation details):\n\n(i) We first built linear models to predict voxel activity from the following three latent representations of the LDM independently: z, c, and z c .\n\n(ii) Although z c and z produce different images, they result in similar prediction maps on the cortex (see 4.2.1). Therefore, we incorporated them into a single model, and further examined how they differ by mapping the unique variance explained by each feature onto cortex [23]. To control the balance between the appearance of the original image and the semantic fidelity of the conditional text, we varied the level of noise added to z. This analysis enabled quantitative interpretation of the image-to-image process.\n\n(iii) While LDMs are characterized as an iterative denoising process, the internal dynamics of the denoising process are poorly understood. To gain some insight into this process, we examined how z c changes through the denoising process. To do so, we extracted z c from the early, middle, and late steps of the denoising. We then constructed combined models with z as in the above analysis (ii), and mapped their unique variance onto cortex.\n\n(iv) Finally, to inspect the last black box associated with LDMs, we extracted features from different layers of U-Net. For different steps of the denoising, encoding models were constructed independently with different U-Net layers: two from the first stage, one from the bottleneck stage, and two from the second stage. We then identified the layer with highest accuracy for each voxel and for each step.\n\nModel weights were estimated from training data using L2-regularized linear regression, and subsequently applied to test data (see Appendix A for details). For evaluation, we used Pearson's correlation coefficients between predicted and measured fMRI signals. We computed statistical significance (one-sided) by comparing the estimated correlations to the null distribution of correlations between two independent Gaussian random vectors of the same length (N=982). The statistical threshold was set at P < 0.05 and corrected for multiple comparisons using the FDR procedure. We show results from a single random seed, but we verified that different random seed produced nearly identical results (see Appendix C). We reduced all feature dimensions to 6,400 by applying principal component analysis, by estimating components within training data.  Figure 3 shows the results of visual reconstruction for one subject (subj01). We generated five images for each test image and selected the generated images with highest PSMs. On the one hand, images reconstructed using only z were visually consistent with the original images, but failed to capture their semantic content. On the other hand, images reconstructed using only c generated images with high semantic fidelity but were visually inconsistent. Finally, images reconstructed using z c could generate highresolution images with high semantic fidelity (see Appendix B for more examples). Figure 4 shows reconstructed images from all subjects for the same image (all images were generated using z c . Other examples are available in the Appendix B). Overall, reconstruction quality was stable and accurate across subjects.\n\n\nResults\n\n\nDecoding\n\nWe note that, the lack of agreement regarding specific details of the reconstructed images may differences in perceived experience across subjects, rather than failures of reconstruction. Alternatively it may simply reflect differences in data quality among subjects. Indeed, subjects with high (subj01) and low (subj07) decoding accuracy from fMRI were subjects with high and low data quality metrics, respectively (see Appendix B). Figure 5 plots results for the quantitative evaluation. In  the objective evaluation, images reconstructed using z c are generally associated with higher accuracy values across different metrics than images reconstructed using only z or c. When only z was used, accuracy values were particularly high for PSMs derived from early layers of CLIP and CNN. On the other hand, when only c was used, accuracy values were higher for PSMs derived from late layers. In the subjective evaluation, accuracy values of images obtained from c are higher than those obtained from z, while z c resulted in the highest accuracy compared with the other two methods (P < 0.01 for all comparisons, two-sided signed-rank test, FWE corrected). Together, these results suggest that our method captures not only low-level visual appearance, but also high-level semantic content of the original stimuli. It is difficult to compare our results with those reported by most previous studies, because they used different datasets. The datasets used in previous studies contain far fewer images, much less image complexity (typically individual objects positioned in the center of the image), and lack full-text annotations of the kind available from NSD. Only one study to date [25] used NSD for visual reconstruction, and they reported accuracy values of 78 \u00b1 4.5% for one subject (subj01) using PSM based on Inception V3. It is difficult to draw a direct comparison with this study, because it differed from ours in several respects (for example, it used different training and test sample sizes, and different image resolutions). Notwithstanding these differences, their reported values fall within a similar range to ours for the same subject (77% using CLIP, 83% using AlexNet, and 76% using Inception V3). However, this prior study relied on extensive model training and feature engineering with many more hyper-parameters than those adopted in our study, including the necessity to train complex generative models, fine-tuning toward MS COCO, data augmentation, and arbitrary thresholding of features. We did not use any of the above techniques -rather, our simple pipeline only requires the construction of two linear regression models from fMRI activity to latent representations of LDM.\n\nFurthermore, we observed a reduction in semantic fidelity when we used categorical information associated with the images, rather than full-text annotations for c. We also found an increase in semantic fidelity when we used semantic maps instead of original images for z, though visual similarity was decreased in this case (see Appendix B). Figure 6 shows prediction accuracy of the encoding models for three types of latent representations associated with the LDM: z, a latent representation of the original image; c, a latent representation of image text annotation; and z c , a noise-added latent representation of z after reverse diffusion process with cross-attention to c.\n\n\nEncoding Model\n\n\nComparison among Latent Representations\n\nAlthough all three components produced high prediction performance at the back of the brain, visual cortex, they showed stark contrast. Specifically, z produced high prediction performance in the posterior part of visual cortex, namely early visual cortex. It also showed significant prediction values in the anterior part of visual cortex, namely higher visual cortex, but smaller values in other regions. On the other hand, c produced the highest prediction performance in higher visual cortex. The model also showed high prediction performance across a wide span of cortex. z c carries a representation that is very similar to z, showing high prediction performance for early visual cortex. Although this is somewhat predictable given their intrinsic similarity, it is nevertheless intriguing because these representations correspond to visually different generated images. We also observed that using z c with a reduced noise level injected into z produces a more similar prediction map to the predic- Figure 6. Prediction performance (measured using Pearson's correlation coefficients) for the voxel-wise encoding model applied to heldout test images in a single subject (subj01), projected onto the inflated (top, lateral and medial views) and flattened cortical surface (bottom, occipital areas are at the center), for both left and right hemispheres. Brain regions with significant accuracy are colored (all colored voxels P < 0.05, FDR corrected). tion map obtained from z, as expected (see Appendix C). This similarity prompted us to conduct an additional analysis to compare the unique variance explained by these two models, detailed in the following section. See Appendix C for results of all subjects.\n\n\nComparison across different noise levels\n\nWhile the previous results showed that prediction accuracy maps for z and z c present similar profiles, they do not tell us how much unique variance is explained by each feature as a function of different noise levels. To enhance our understanding of the above issues, we next constructed encoding models that simultaneously incorporated both z and z c into a single model, and studied the unique contribution of each feature. We also varied the level of noise added to z for generating z c . Figure 7 shows that, when a small amount of noise was added, z predicted voxel activity better than z c across cortex. Interestingly, when we increased the level of noise, z c predicted voxel activity within higher visual cortex better than z, indicating that the semantic content of the image was gradually emphasized.\n\nThis result is intriguing because, without analyses like this, we can only observe randomly generated images, and we cannot examine how the text-conditioned image-toimage process is able to balance between semantic content and original visual appearance.\n\n\nComparison across different diffusion stages\n\nWe next asked how the noise-added latent representation changes over the iterative denoising process. Figure 8 shows that, during the early stages of the denoising process, z signals dominated prediction of fMRI signals. During the middle step of the denoising process, z c predicted activity within higher visual cortex much bet- Figure 7. Unique variance accounted for by zc compared with z in one subject (subj01), obtained by splitting accuracy values from the combined model. While fixing z, we used zc with varying amounts of noise-level added to the latent representation of stimuli from low-level (top) to high-level (bottom). All colored voxels P < 0.05, FDR corrected.\n\nter than z, indicating that the bulk of the semantic content emerges at this stage. These results show how LDM refines and generates images from noise. Figure 8. Unique variance accounted for by zc compared with z in one subject (subj01), obtained by splitting accuracy values from the combined model. While fixing z, we used zc with different denoising stages from early (top) to late (bottom) steps. All colored voxels P < 0.05, FDR corrected.\n\n\nComparison across different U-Net Layers\n\nFinally, we asked what information is being processed at each layer of U-Net. Figure 9 shows the results of encoding models for different steps of the denoising process (early, middle, late), and for the different layers of U-Net. During the early phase of the denoising process, the bottleneck layer of U-Net (colored orange) produces the highest prediction performance across cortex. However, as denoising progresses, the early layer of U-Net (colored blue) predicts activity within early visual cortex, and the bottleneck layer shifts toward superior predictive power for higher visual cortex.\n\nThese results suggest that, at the beginning of the reverse diffusion process, image information is compressed within the bottleneck layer. As denoising progresses, a functional dissociation among U-Net layers emerges within visual cortex: i.e., the first layer tends to represent fine-scale details in early visual areas, while the bottleneck layer corresponds to higher-order information in more ventral, semantic areas. Figure 9. Selective engagement of different U-Net layers for different voxels across the brain. Colors represent the most predictive U-Net layer for early (top) to late (bottom) denoising steps. All colored voxels P < 0.05, FDR corrected.\n\n\nConclusions\n\nWe propose a novel visual reconstruction method using LDMs. We show that our method can reconstruct highresolution images with high semantic fidelity from human brain activity. Unlike previous studies of image reconstruction, our method does not require training or fine-tuning of complex deep-learning models: it only requires simple linear mappings from fMRI to latent representations within LDMs.\n\nWe also provide a quantitative interpretation for the internal components of the LDM by building encoding models. For example, we demonstrate the emergence of semantic content throughout the inverse diffusion process, we perform layer-wise characterization of U-Net, and we provide a quantitative interpretation of image-to-image transformations with different noise levels. Although DMs are developing rapidly, their internal processes remain poorly understood. This study is the first to provide a quantitative interpretation from a biological perspective.\n\nFigure 2\n2presents an overview of our methods.\n\nFigure 2 .\n2Overview of our methods. (Top) Schematic of LDM used in this study.\n\nFigure 3 .\n3Presented (red box) and reconstructed images for a single subject (subj01) using z, c, and zc.\n\nFigure 4 .\n4Example results for all four subjects.\n\nFigure 5 .\n5Identification accuracy calculated using objective (left) and subjective (right) criteria (pooled across four subjects; chance level corresponds to 50%). Error bars indicate standard error of the mean.\nhttps://github.com/CompVis/stable-diffusion/blob/main/scripts/ 14456 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nA massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, J Benjamin Hutchinson, Thomas Naselaris, Kendrick Kay, Nature Neuroscience. 25Emily J. Allen, Ghislain St-Yves, Yihan Wu, Jesse L. Breedlove, Jacob S. Prince, Logan T. Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, J. Benjamin Hutchinson, Thomas Naselaris, and Kendrick Kay. A mas- sive 7t fmri dataset to bridge cognitive neuroscience and ar- tificial intelligence. Nature Neuroscience, 25:116-126, 1 2022. 3\n\nFrom voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, Michal Irani, Advances in Neural Information Processing Systems. 32Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Advances in Neural Information Processing Systems, 32, 2019. 2\n\nBrains and algorithms partially converge in natural language processing. Charlotte Caucheteux, Jean-R\u00e9mi King, Communications biology. 513Charlotte Caucheteux and Jean-R\u00e9mi King. Brains and al- gorithms partially converge in natural language processing. Communications biology, 5(1):1-10, 2022. 1, 3\n\nDiffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. 34Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Informa- tion Processing Systems, 34:8780-8794, 2021. 2\n\nReconstructing perceptive images from brain activity by shape-semantic gan. Tao Fang, Yu Qi, Gang Pan, Advances in Neural Information Processing Systems. 33Tao Fang, Yu Qi, and Gang Pan. Reconstructing perceptive images from brain activity by shape-semantic gan. Advances in Neural Information Processing Systems, 33:13038-13048, 2020. 2\n\nBack to the source: Diffusion-driven test-time adaptation. Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, Dequan Wang, arXiv:2207.03442arXiv preprintJin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, and Dequan Wang. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. 2\n\nSelfsupervised natural image reconstruction and large-scale semantic classification from brain activity. Guy Gaziv, Roman Beliy, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, Michal Irani, NeuroImage. 254Guy Gaziv, Roman Beliy, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. Self- supervised natural image reconstruction and large-scale se- mantic classification from brain activity. NeuroImage, 254, 7 2022. 2\n\nShared computational principles for language processing in humans and deep language models. Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, A Samuel, Amir Nastase, Dotan Feder, Alon Emanuel, Cohen, Nature neuroscience. 2533Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational principles for language processing in humans and deep lan- guage models. Nature neuroscience, 25(3):369-380, 2022. 1, 3\n\nDistinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior. Michelle R Iris Ia Groen, Christopher Greene, Li Baldassano, Diane M Fei-Fei, Chris I Beck, Baker, Elife. 73Iris IA Groen, Michelle R Greene, Christopher Baldassano, Li Fei-Fei, Diane M Beck, and Chris I Baker. Distinct con- tributions of functional and deep neural network features to representational similarity of scenes in human brain and be- havior. Elife, 7, 2018. 1, 3\n\nDeep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. Umut G\u00fc\u00e7l\u00fc, Van Gerven, Journal of Neuroscience. 35273Umut G\u00fc\u00e7l\u00fc and Marcel AJ van Gerven. Deep neural net- works reveal a gradient in the complexity of neural represen- tations across the ventral stream. Journal of Neuroscience, 35(27):10005-10014, 2015. 1, 3\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 334Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020. 2, 4\n\nThe neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions. Tomoyasu Horikawa, Alan S Cowen, Dacher Keltner, Yukiyasu Kamitani, Iscience. 235101060Tomoyasu Horikawa, Alan S Cowen, Dacher Keltner, and Yukiyasu Kamitani. The neural representation of visu- ally evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions. Iscience, 23(5):101060, 2020. 2\n\nGeneric decoding of seen and imagined objects using hierarchical visual features. Tomoyasu Horikawa, Yukiyasu Kamitani, Nature communications. 813Tomoyasu Horikawa and Yukiyasu Kamitani. Generic de- coding of seen and imagined objects using hierarchical vi- sual features. Nature communications, 8(1):1-15, 2017. 1, 2, 3\n\nNeural decoding of visual imagery during sleep. Tomoyasu Horikawa, Masako Tamaki, Yoichi Miyawaki, Yukiyasu Kamitani, Science. 3406132Tomoyasu Horikawa, Masako Tamaki, Yoichi Miyawaki, and Yukiyasu Kamitani. Neural decoding of visual imagery during sleep. Science, 340(6132):639-642, 2013. 2\n\nDecoding the semantic content of natural movies from human brain activity. G Alexander, Tyler Huth, Shinji Lee, Nishimoto, Y Natalia, An T Bilenko, Jack L Vu, Gallant, Frontiers in systems neuroscience. 10281Alexander G Huth, Tyler Lee, Shinji Nishimoto, Natalia Y Bilenko, An T Vu, and Jack L Gallant. Decoding the se- mantic content of natural movies from human brain activity. Frontiers in systems neuroscience, 10:81, 2016. 2\n\nJpeg artifact correction using denoising diffusion restoration models. Bahjat Kawar, Jiaming Song, Stefano Ermon, Michael Elad, arXiv:2209.118882022arXiv preprintBahjat Kawar, Jiaming Song, Stefano Ermon, and Michael Elad. Jpeg artifact correction using denoising diffusion restoration models. arXiv preprint arXiv:2209.11888, 2022. 2\n\nIdentifying natural images from human brain activity. N Kendrick, Thomas Kay, Naselaris, J Ryan, Jack L Prenger, Gallant, Nature. 4527185Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant. Identifying natural images from human brain activity. Nature, 452(7185):352-355, 2008. 2\n\nA taskoptimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. J E Alexander, Kell, L K Daniel, Erica N Yamins, Shook, Josh H Sam V Norman-Haignere, Mcdermott, Neuron. 9833Alexander JE Kell, Daniel LK Yamins, Erica N Shook, Sam V Norman-Haignere, and Josh H McDermott. A task- optimized neural network replicates human auditory behav- ior, predicts brain responses, and reveals a cortical process- ing hierarchy. Neuron, 98(3):630-644, 2018. 1, 3\n\nRecurrence is required to capture the representational dynamics of the human visual system. Courtney J Tim C Kietzmann, Spoerer, K A Lynn, S\u00f6rensen, M Radoslaw, Olaf Cichy, Nikolaus Hauk, Kriegeskorte, Proceedings of the National Academy of Sciences. the National Academy of Sciences1163Tim C Kietzmann, Courtney J Spoerer, Lynn KA S\u00f6rensen, Radoslaw M Cichy, Olaf Hauk, and Nikolaus Kriegeskorte. Recurrence is required to capture the representational dy- namics of the human visual system. Proceedings of the Na- tional Academy of Sciences, 116(43):21854-21863, 2019. 1, 3\n\nDistinct dimensions of emotion in the human brain and their representation on the cortical surface. Naoko Koide-Majima, Tomoya Nakai, Shinji Nishimoto, NeuroImage. 2222117258Naoko Koide-Majima, Tomoya Nakai, and Shinji Nishi- moto. Distinct dimensions of emotion in the human brain and their representation on the cortical surface. NeuroImage, 222:117258, 2020. 2\n\nCascaded tuning to amplitude modulation for natural sound recognition. Takuya Koumura, Hiroki Terashima, Shigeto Furukawa, Journal of Neuroscience. 39283Takuya Koumura, Hiroki Terashima, and Shigeto Furukawa. Cascaded tuning to amplitude modulation for natural sound recognition. Journal of Neuroscience, 39(28):5517-5533, 2019. 1, 3\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Communications of the ACM. 60613Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. Communications of the ACM, 60(6):84-90, 2017. 4, 13\n\nFeature-space selection with banded ridge regression. T D La Tour, M Eickenberg, A O Nunez-Elizalde, J L Gallant, NeuroImage. 5119728T. D. la Tour, M. Eickenberg, A. O. Nunez-Elizalde, and J. L. Gallant. Feature-space selection with banded ridge regres- sion. NeuroImage, page 119728, Nov 2022. 5\n\nBrain2pix: Fully convolutional naturalistic video reconstruction from brain activity. Lynn Le, Luca Ambrogioni, Katja Seeliger, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, Marcel Van Gerven, Umut G\u00fc\u00e7l\u00fc, BioRxiv. 2Lynn Le, Luca Ambrogioni, Katja Seeliger, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, Marcel van Gerven, and Umut G\u00fc\u00e7l\u00fc. Brain2pix: Fully convolutional naturalistic video reconstruction from brain activity. BioRxiv, 2021. 2\n\nMind reader: Reconstructing complex images from brain activities. Sikun Lin, Thomas Sprague, Ambuj K Singh, Advances in Neural Information Processing Systems. 26Sikun Lin, Thomas Sprague, and Ambuj K Singh. Mind reader: Reconstructing complex images from brain activi- ties. Advances in Neural Information Processing Systems, 9 2022. 2, 6\n\nVisual image reconstruction from human brain activity using a combination of multiscale local image decoders. Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Yusuke Masa Aki Sato, Morito, C Hiroki, Norihiro Tanabe, Yukiyasu Sadato, Kamitani, Neuron. 60Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Masa aki Sato, Yusuke Morito, Hiroki C. Tanabe, Norihiro Sadato, and Yukiyasu Kamitani. Visual image reconstruction from human brain activity using a combination of multiscale local image decoders. Neuron, 60:915-929, 12 2008. 2\n\nReconstructing natural scenes from fMRI patterns using bigbigan. Milad Mozafari, Leila Reddy, Rufin Vanrullen, 2020 International joint conference on neural networks (IJCNN). Milad Mozafari, Leila Reddy, and Rufin VanRullen. Recon- structing natural scenes from fMRI patterns using bigbigan. In 2020 International joint conference on neural networks (IJCNN), pages 1-8. IEEE, 2020. 2\n\nQuantitative models reveal the organization of diverse cognitive functions in the brain. Tomoya Nakai, Shinji Nishimoto, Nature communications. 111Tomoya Nakai and Shinji Nishimoto. Quantitative models reveal the organization of diverse cognitive functions in the brain. Nature communications, 11(1):1-12, 2020. 2\n\nA voxel-wise encoding model for early visual areas decodes mental images of remembered scenes. Thomas Naselaris, Cheryl A Olman, Dustin E Stansbury, Kamil Ugurbil, Jack L Gallant, Neuroimage. 1052Thomas Naselaris, Cheryl A Olman, Dustin E Stansbury, Kamil Ugurbil, and Jack L Gallant. A voxel-wise encod- ing model for early visual areas decodes mental images of remembered scenes. Neuroimage, 105:215-228, 2015. 2\n\nBayesian reconstruction of natural images from human brain activity. Thomas Naselaris, J Ryan, Prenger, N Kendrick, Michael Kay, Jack L Oliver, Gallant, Neuron. 636Thomas Naselaris, Ryan J Prenger, Kendrick N Kay, Michael Oliver, and Jack L Gallant. Bayesian reconstruction of nat- ural images from human brain activity. Neuron, 63(6):902- 915, 2009. 2\n\nDecoding naturalistic experiences from human brain activity via distributed representations of words. Satoshi Nishida, Shinji Nishimoto, Neuroimage. 1802Satoshi Nishida and Shinji Nishimoto. Decoding naturalistic experiences from human brain activity via distributed repre- sentations of words. Neuroimage, 180:232-242, 2018. 2\n\nReconstructing visual experiences from brain activity evoked by natural movies. Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, Jack L Gallant, Current Biology. 21Shinji Nishimoto, An T. Vu, Thomas Naselaris, Yuval Ben- jamini, Bin Yu, and Jack L. Gallant. Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21:1641-1646, 10 2011. 2\n\nGrad-tts: A diffusion probabilistic model for text-to-speech. Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, PMLR, 2021. 2International Conference on Machine Learning. Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion prob- abilistic model for text-to-speech. In International Confer- ence on Machine Learning, pages 8599-8608. PMLR, 2021. 2\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 4International Conference on Machine Learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 4\n\nNatural image reconstruction from fmri using deep learning: A survey. Zarina Rakhimberdina, Quentin Jodelet, Xin Liu, Tsuyoshi Murata, Frontiers in Neuroscience. 1513Zarina Rakhimberdina, Quentin Jodelet, Xin Liu, and Tsuyoshi Murata. Natural image reconstruction from fmri using deep learning: A survey. Frontiers in Neuroscience, 15, 2021. 13\n\nReconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. Jie Ziqi Ren, Xuetong Li, Xin Xue, Fan Li, Zhicheng Yang, Xinbo Jiao, Gao, NeuroImage. 2282117602Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, and Xinbo Gao. Reconstructing seen image from brain activity by visually-guided cognitive representation and ad- versarial learning. NeuroImage, 228:117602, 2021. 2\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022. 2\n\nPalette: Image-to-image diffusion models. Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi, ACM SIGGRAPH 2022 Conference Proceedings. 2022Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1- 10, 2022. 2\n\nPhotorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, ; S Sara Mahdavi, Rapha Gontijo Lopes, Advances in Neural Information Processing Systems. 2022Seyed Kamyar Seyed GhasemipourChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image dif- fusion models with deep language understanding. Advances in Neural Information Processing Systems, 2022. 2\n\nImage superresolution via iterative refinement. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, J David, Mohammad Fleet, Norouzi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 20222Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali- mans, David J Fleet, and Mohammad Norouzi. Image super- resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 2\n\nUnit-ddpm: Unpaired image translation with denoising diffusion probabilistic models. Hiroshi Sasaki, Chris G Willcocks, Toby P Breckon, arXiv:2104.05358arXiv preprintHiroshi Sasaki, Chris G Willcocks, and Toby P Breckon. Unit-ddpm: Unpaired image translation with denois- ing diffusion probabilistic models. arXiv preprint arXiv:2104.05358, 2021. 2\n\nPredicting speech from a cortical hierarchy of event-based time scales. Lea-Maria Schmitt, Julia Erb, Sarah Tune, Anna U Rysop, Gesa Hartwigsen, Jonas Obleser, Science Advances. 7493Lea-Maria Schmitt, Julia Erb, Sarah Tune, Anna U Rysop, Gesa Hartwigsen, and Jonas Obleser. Predicting speech from a cortical hierarchy of event-based time scales. Science Ad- vances, 7(49):eabi6070, 2021. 1, 3\n\nThe neural architecture of language: Integrative modeling converges on predictive processing. Martin Schrimpf, Asher Idan, Greta Blank, Carina Tuckute, Kauf, A Eghbal, Nancy Hosseini, Joshua B Kanwisher, Evelina Tenenbaum, Fedorenko, Proceedings of the National Academy of Sciences. 118453Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Ca- rina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sci- ences, 118(45):e2105646118, 2021. 1, 3\n\nGenerative adversarial networks for reconstructing natural images from brain activity. Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, Van Gerven, NeuroImage. 1812Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, and Marcel AJ van Gerven. Generative adver- sarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775-785, 2018. 2\n\nEnd-to-end deep image reconstruction from human brain activity. Frontiers in Computational Neuroscience. Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, Yukiyasu Kamitani, 21Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. End-to-end deep image reconstruction from human brain activity. Frontiers in Com- putational Neuroscience, page 21, 2019. 2\n\nDeep image reconstruction from human brain activity. Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani, PLoS Computational Biology. 152Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani. Deep image reconstruction from human brain activity. PLoS Computational Biology, 15, 2019. 2\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, PMLRInternational Conference on Machine Learning. 24Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Confer- ence on Machine Learning, pages 2256-2265. PMLR, 2015. 2, 4\n\nGenerative modeling by estimating gradients of the data distribution. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. 32Yang Song and Stefano Ermon. Generative modeling by esti- mating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. 2\n\nScore-based generative modeling through stochastic differential equations. Yang Song, Jascha Sohl-Dickstein, P Diederik, Abhishek Kingma, Stefano Kumar, Ben Ermon, Poole, International Conference on Learning Representations. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab- hishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equa- tions. International Conference on Learning Representa- tions, 2020. 2\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2818-2826, 2016. 13\n\nA common brain network among state, trait, and pathological anxiety from wholebrain functional connectivity. Yu Takagi, Yuki Sakai, Yoshinari Abe, Seiji Nishida, J Ben, Ignacio Harrison, Carles Mart\u00ednez-Zalaca\u00edn, Jin Soriano-Mas, Narumoto, C Saori, Tanaka, Neuroimage. 1722Yu Takagi, Yuki Sakai, Yoshinari Abe, Seiji Nishida, Ben J Harrison, Ignacio Mart\u00ednez-Zalaca\u00edn, Carles Soriano-Mas, Jin Narumoto, and Saori C Tanaka. A common brain network among state, trait, and pathological anxiety from whole- brain functional connectivity. Neuroimage, 172:506-516, 2018. 2\n\nSemantic reconstruction of continuous language from non-invasive brain recordings. bioRxiv. Jerry Tang, Amanda Lebel, Shailee Jain, Alexander G Huth, 2022Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. bioRxiv, 2022. 2\n\nA connection between score matching and denoising autoencoders. Pascal Vincent, Neural computation. 237Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661- 1674, 2011. 2\n\nNeural encoding and decoding with deep learning for dynamic natural vision. Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, Zhongming Liu, Cerebral cortex. 28123Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Ji- ayue Cao, and Zhongming Liu. Neural encoding and decod- ing with deep learning for dynamic natural vision. Cerebral cortex, 28(12):4136-4160, 2018. 1, 3\n\nPerformance-optimized hierarchical models predict neural responses in higher visual cortex. L K Daniel, Ha Yamins, Hong, F Charles, Ethan A Cadieu, Darren Solomon, James J Seibert, Dicarlo, Proceedings of the national academy of sciences. the national academy of sciences1113Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the na- tional academy of sciences, 111(23):8619-8624, 2014. 1, 3\n", "annotations": {"author": "[{\"end\":216,\"start\":95},{\"end\":351,\"start\":217}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":98},{\"end\":233,\"start\":224}]", "author_first_name": "[{\"end\":97,\"start\":95},{\"end\":223,\"start\":217}]", "author_affiliation": "[{\"end\":197,\"start\":135},{\"end\":215,\"start\":199},{\"end\":332,\"start\":270},{\"end\":350,\"start\":334}]", "title": "[{\"end\":92,\"start\":1},{\"end\":443,\"start\":352}]", "venue": null, "abstract": "[{\"end\":2182,\"start\":473}]", "bib_ref": "[{\"end\":2731,\"start\":2690},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2986,\"start\":2983},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2988,\"start\":2986},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2990,\"start\":2988},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2993,\"start\":2990},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2996,\"start\":2993},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3006,\"start\":3002},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3010,\"start\":3006},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3014,\"start\":3010},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3334,\"start\":3330},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3337,\"start\":3334},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3340,\"start\":3337},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3343,\"start\":3340},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3532,\"start\":3529},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3534,\"start\":3532},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3536,\"start\":3534},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3539,\"start\":3536},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3542,\"start\":3539},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3545,\"start\":3542},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3548,\"start\":3545},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3552,\"start\":3548},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3556,\"start\":3552},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3560,\"start\":3556},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3722,\"start\":3719},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3725,\"start\":3722},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4155,\"start\":4151},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4158,\"start\":4155},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4161,\"start\":4158},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4164,\"start\":4161},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4348,\"start\":4345},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4351,\"start\":4348},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4354,\"start\":4351},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4383,\"start\":4379},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4408,\"start\":4404},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4437,\"start\":4434},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4440,\"start\":4437},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4443,\"start\":4440},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4446,\"start\":4443},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4514,\"start\":4510},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6641,\"start\":6637},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6644,\"start\":6641},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6647,\"start\":6644},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6650,\"start\":6647},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6698,\"start\":6694},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6701,\"start\":6698},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6704,\"start\":6701},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6727,\"start\":6723},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6730,\"start\":6727},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6755,\"start\":6751},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6758,\"start\":6755},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6761,\"start\":6758},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6803,\"start\":6799},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6806,\"start\":6803},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7053,\"start\":7049},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7056,\"start\":7053},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7059,\"start\":7056},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7062,\"start\":7059},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7172,\"start\":7169},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7174,\"start\":7172},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7176,\"start\":7174},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7179,\"start\":7176},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7182,\"start\":7179},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7185,\"start\":7182},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7188,\"start\":7185},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7192,\"start\":7188},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7196,\"start\":7192},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7200,\"start\":7196},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7397,\"start\":7394},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7400,\"start\":7397},{\"end\":8716,\"start\":8675},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9364,\"start\":9361},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9367,\"start\":9364},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9370,\"start\":9367},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9373,\"start\":9370},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9376,\"start\":9373},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9379,\"start\":9376},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9527,\"start\":9524},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9529,\"start\":9527},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9532,\"start\":9529},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9535,\"start\":9532},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9538,\"start\":9535},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9541,\"start\":9538},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10377,\"start\":10374},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11346,\"start\":11344},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12834,\"start\":12830},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12837,\"start\":12834},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14414,\"start\":14410},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17108,\"start\":17104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18104,\"start\":18100},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22585,\"start\":22581},{\"end\":29417,\"start\":29409}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30670,\"start\":30623},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30751,\"start\":30671},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30859,\"start\":30752},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30911,\"start\":30860},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31126,\"start\":30912}]", "paragraph": "[{\"end\":3015,\"start\":2198},{\"end\":4126,\"start\":3017},{\"end\":5171,\"start\":4128},{\"end\":5762,\"start\":5173},{\"end\":6440,\"start\":5764},{\"end\":6956,\"start\":6497},{\"end\":8306,\"start\":6958},{\"end\":9542,\"start\":8326},{\"end\":10078,\"start\":9544},{\"end\":10294,\"start\":10080},{\"end\":11673,\"start\":10316},{\"end\":12162,\"start\":11675},{\"end\":12509,\"start\":12190},{\"end\":12887,\"start\":12602},{\"end\":13602,\"start\":12889},{\"end\":14137,\"start\":13604},{\"end\":14502,\"start\":14139},{\"end\":14868,\"start\":14504},{\"end\":15398,\"start\":14914},{\"end\":15668,\"start\":15400},{\"end\":15772,\"start\":15670},{\"end\":16152,\"start\":15774},{\"end\":16602,\"start\":16154},{\"end\":16764,\"start\":16604},{\"end\":17364,\"start\":16766},{\"end\":17673,\"start\":17410},{\"end\":17823,\"start\":17675},{\"end\":18346,\"start\":17825},{\"end\":18790,\"start\":18348},{\"end\":19198,\"start\":18792},{\"end\":20875,\"start\":19200},{\"end\":23599,\"start\":20898},{\"end\":24280,\"start\":23601},{\"end\":26056,\"start\":24341},{\"end\":26913,\"start\":26101},{\"end\":27169,\"start\":26915},{\"end\":27896,\"start\":27218},{\"end\":28343,\"start\":27898},{\"end\":28984,\"start\":28388},{\"end\":29647,\"start\":28986},{\"end\":30062,\"start\":29663},{\"end\":30622,\"start\":30064}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12601,\"start\":12510}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2196,\"start\":2184},{\"attributes\":{\"n\":\"2.\"},\"end\":6455,\"start\":6443},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6495,\"start\":6458},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8324,\"start\":8309},{\"attributes\":{\"n\":\"3.\"},\"end\":10304,\"start\":10297},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10314,\"start\":10307},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12188,\"start\":12165},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14912,\"start\":14871},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17408,\"start\":17367},{\"attributes\":{\"n\":\"4.\"},\"end\":20885,\"start\":20878},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20896,\"start\":20888},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24297,\"start\":24283},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":24339,\"start\":24300},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":26099,\"start\":26059},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":27216,\"start\":27172},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":28386,\"start\":28346},{\"attributes\":{\"n\":\"5.\"},\"end\":29661,\"start\":29650},{\"end\":30632,\"start\":30624},{\"end\":30682,\"start\":30672},{\"end\":30763,\"start\":30753},{\"end\":30871,\"start\":30861},{\"end\":30923,\"start\":30913}]", "table": null, "figure_caption": "[{\"end\":30670,\"start\":30634},{\"end\":30751,\"start\":30684},{\"end\":30859,\"start\":30765},{\"end\":30911,\"start\":30873},{\"end\":31126,\"start\":30925}]", "figure_ref": "[{\"end\":6032,\"start\":6024},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13752,\"start\":13743},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15019,\"start\":15010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17622,\"start\":17614},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20055,\"start\":20047},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20650,\"start\":20642},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21340,\"start\":21332},{\"end\":23951,\"start\":23943},{\"end\":25355,\"start\":25347},{\"end\":26602,\"start\":26594},{\"end\":27328,\"start\":27320},{\"end\":27557,\"start\":27549},{\"end\":28058,\"start\":28050},{\"end\":28474,\"start\":28466}]", "bib_author_first_name": "[{\"end\":31626,\"start\":31621},{\"end\":31628,\"start\":31627},{\"end\":31644,\"start\":31636},{\"end\":31659,\"start\":31654},{\"end\":31669,\"start\":31664},{\"end\":31671,\"start\":31670},{\"end\":31688,\"start\":31683},{\"end\":31690,\"start\":31689},{\"end\":31704,\"start\":31699},{\"end\":31706,\"start\":31705},{\"end\":31723,\"start\":31715},{\"end\":31733,\"start\":31729},{\"end\":31747,\"start\":31741},{\"end\":31761,\"start\":31758},{\"end\":31772,\"start\":31771},{\"end\":31781,\"start\":31773},{\"end\":31800,\"start\":31794},{\"end\":31820,\"start\":31812},{\"end\":32296,\"start\":32291},{\"end\":32307,\"start\":32304},{\"end\":32320,\"start\":32315},{\"end\":32337,\"start\":32328},{\"end\":32352,\"start\":32349},{\"end\":32366,\"start\":32360},{\"end\":32752,\"start\":32743},{\"end\":32774,\"start\":32765},{\"end\":33026,\"start\":33018},{\"end\":33046,\"start\":33037},{\"end\":33350,\"start\":33347},{\"end\":33359,\"start\":33357},{\"end\":33368,\"start\":33364},{\"end\":33672,\"start\":33669},{\"end\":33685,\"start\":33678},{\"end\":33698,\"start\":33693},{\"end\":33710,\"start\":33704},{\"end\":33724,\"start\":33720},{\"end\":33742,\"start\":33736},{\"end\":34072,\"start\":34069},{\"end\":34085,\"start\":34080},{\"end\":34096,\"start\":34093},{\"end\":34110,\"start\":34105},{\"end\":34127,\"start\":34118},{\"end\":34142,\"start\":34139},{\"end\":34156,\"start\":34150},{\"end\":34512,\"start\":34507},{\"end\":34528,\"start\":34524},{\"end\":34540,\"start\":34535},{\"end\":34557,\"start\":34550},{\"end\":34569,\"start\":34566},{\"end\":34582,\"start\":34577},{\"end\":34592,\"start\":34591},{\"end\":34605,\"start\":34601},{\"end\":34620,\"start\":34615},{\"end\":34632,\"start\":34628},{\"end\":35112,\"start\":35104},{\"end\":35114,\"start\":35113},{\"end\":35141,\"start\":35130},{\"end\":35152,\"start\":35150},{\"end\":35170,\"start\":35165},{\"end\":35172,\"start\":35171},{\"end\":35187,\"start\":35182},{\"end\":35189,\"start\":35188},{\"end\":35595,\"start\":35591},{\"end\":35903,\"start\":35895},{\"end\":35912,\"start\":35908},{\"end\":35925,\"start\":35919},{\"end\":36296,\"start\":36288},{\"end\":36311,\"start\":36307},{\"end\":36313,\"start\":36312},{\"end\":36327,\"start\":36321},{\"end\":36345,\"start\":36337},{\"end\":36708,\"start\":36700},{\"end\":36727,\"start\":36719},{\"end\":36996,\"start\":36988},{\"end\":37013,\"start\":37007},{\"end\":37028,\"start\":37022},{\"end\":37047,\"start\":37039},{\"end\":37309,\"start\":37308},{\"end\":37326,\"start\":37321},{\"end\":37339,\"start\":37333},{\"end\":37357,\"start\":37356},{\"end\":37369,\"start\":37367},{\"end\":37371,\"start\":37370},{\"end\":37385,\"start\":37381},{\"end\":37387,\"start\":37386},{\"end\":37741,\"start\":37735},{\"end\":37756,\"start\":37749},{\"end\":37770,\"start\":37763},{\"end\":37785,\"start\":37778},{\"end\":38055,\"start\":38054},{\"end\":38072,\"start\":38066},{\"end\":38090,\"start\":38089},{\"end\":38101,\"start\":38097},{\"end\":38103,\"start\":38102},{\"end\":38436,\"start\":38435},{\"end\":38438,\"start\":38437},{\"end\":38457,\"start\":38456},{\"end\":38459,\"start\":38458},{\"end\":38473,\"start\":38468},{\"end\":38475,\"start\":38474},{\"end\":38495,\"start\":38491},{\"end\":38497,\"start\":38496},{\"end\":38920,\"start\":38912},{\"end\":38922,\"start\":38921},{\"end\":38950,\"start\":38949},{\"end\":38952,\"start\":38951},{\"end\":38970,\"start\":38969},{\"end\":38985,\"start\":38981},{\"end\":39001,\"start\":38993},{\"end\":39501,\"start\":39496},{\"end\":39522,\"start\":39516},{\"end\":39536,\"start\":39530},{\"end\":39838,\"start\":39832},{\"end\":39854,\"start\":39848},{\"end\":39873,\"start\":39866},{\"end\":40165,\"start\":40161},{\"end\":40182,\"start\":40178},{\"end\":40202,\"start\":40194},{\"end\":40204,\"start\":40203},{\"end\":40476,\"start\":40475},{\"end\":40478,\"start\":40477},{\"end\":40489,\"start\":40488},{\"end\":40503,\"start\":40502},{\"end\":40505,\"start\":40504},{\"end\":40523,\"start\":40522},{\"end\":40525,\"start\":40524},{\"end\":40809,\"start\":40805},{\"end\":40818,\"start\":40814},{\"end\":40836,\"start\":40831},{\"end\":40853,\"start\":40847},{\"end\":40871,\"start\":40865},{\"end\":40888,\"start\":40884},{\"end\":41176,\"start\":41171},{\"end\":41188,\"start\":41182},{\"end\":41205,\"start\":41198},{\"end\":41561,\"start\":41555},{\"end\":41578,\"start\":41572},{\"end\":41592,\"start\":41587},{\"end\":41610,\"start\":41604},{\"end\":41635,\"start\":41634},{\"end\":41652,\"start\":41644},{\"end\":41669,\"start\":41661},{\"end\":42047,\"start\":42042},{\"end\":42063,\"start\":42058},{\"end\":42076,\"start\":42071},{\"end\":42457,\"start\":42451},{\"end\":42471,\"start\":42465},{\"end\":42778,\"start\":42772},{\"end\":42796,\"start\":42790},{\"end\":42798,\"start\":42797},{\"end\":42812,\"start\":42806},{\"end\":42814,\"start\":42813},{\"end\":42831,\"start\":42826},{\"end\":42845,\"start\":42841},{\"end\":42847,\"start\":42846},{\"end\":43168,\"start\":43162},{\"end\":43181,\"start\":43180},{\"end\":43198,\"start\":43197},{\"end\":43216,\"start\":43209},{\"end\":43226,\"start\":43222},{\"end\":43228,\"start\":43227},{\"end\":43556,\"start\":43549},{\"end\":43572,\"start\":43566},{\"end\":43862,\"start\":43856},{\"end\":43876,\"start\":43874},{\"end\":43878,\"start\":43877},{\"end\":43889,\"start\":43883},{\"end\":43906,\"start\":43901},{\"end\":43921,\"start\":43918},{\"end\":43930,\"start\":43926},{\"end\":43932,\"start\":43931},{\"end\":44245,\"start\":44240},{\"end\":44257,\"start\":44253},{\"end\":44272,\"start\":44264},{\"end\":44290,\"start\":44283},{\"end\":44308,\"start\":44301},{\"end\":44681,\"start\":44677},{\"end\":44695,\"start\":44691},{\"end\":44700,\"start\":44696},{\"end\":44711,\"start\":44706},{\"end\":44727,\"start\":44721},{\"end\":44743,\"start\":44736},{\"end\":44757,\"start\":44749},{\"end\":44773,\"start\":44767},{\"end\":44788,\"start\":44782},{\"end\":44803,\"start\":44797},{\"end\":44817,\"start\":44813},{\"end\":45271,\"start\":45265},{\"end\":45294,\"start\":45287},{\"end\":45307,\"start\":45304},{\"end\":45321,\"start\":45313},{\"end\":45660,\"start\":45657},{\"end\":45678,\"start\":45671},{\"end\":45686,\"start\":45683},{\"end\":45695,\"start\":45692},{\"end\":45708,\"start\":45700},{\"end\":45720,\"start\":45715},{\"end\":46051,\"start\":46046},{\"end\":46068,\"start\":46061},{\"end\":46087,\"start\":46080},{\"end\":46103,\"start\":46096},{\"end\":46116,\"start\":46111},{\"end\":46584,\"start\":46577},{\"end\":46601,\"start\":46594},{\"end\":46614,\"start\":46608},{\"end\":46627,\"start\":46622},{\"end\":46641,\"start\":46633},{\"end\":46649,\"start\":46646},{\"end\":46665,\"start\":46660},{\"end\":46681,\"start\":46673},{\"end\":47051,\"start\":47044},{\"end\":47068,\"start\":47061},{\"end\":47082,\"start\":47075},{\"end\":47095,\"start\":47091},{\"end\":47103,\"start\":47100},{\"end\":47116,\"start\":47111},{\"end\":47133,\"start\":47125},{\"end\":47156,\"start\":47143},{\"end\":47623,\"start\":47616},{\"end\":47641,\"start\":47633},{\"end\":47653,\"start\":47646},{\"end\":47663,\"start\":47660},{\"end\":47675,\"start\":47674},{\"end\":47691,\"start\":47683},{\"end\":48089,\"start\":48082},{\"end\":48103,\"start\":48098},{\"end\":48105,\"start\":48104},{\"end\":48121,\"start\":48117},{\"end\":48123,\"start\":48122},{\"end\":48428,\"start\":48419},{\"end\":48443,\"start\":48438},{\"end\":48454,\"start\":48449},{\"end\":48465,\"start\":48461},{\"end\":48467,\"start\":48466},{\"end\":48479,\"start\":48475},{\"end\":48497,\"start\":48492},{\"end\":48841,\"start\":48835},{\"end\":48857,\"start\":48852},{\"end\":48869,\"start\":48864},{\"end\":48883,\"start\":48877},{\"end\":48900,\"start\":48899},{\"end\":48914,\"start\":48909},{\"end\":48931,\"start\":48925},{\"end\":48933,\"start\":48932},{\"end\":48952,\"start\":48945},{\"end\":49444,\"start\":49439},{\"end\":49459,\"start\":49455},{\"end\":49471,\"start\":49467},{\"end\":49490,\"start\":49484},{\"end\":49853,\"start\":49847},{\"end\":49867,\"start\":49860},{\"end\":49880,\"start\":49877},{\"end\":49897,\"start\":49889},{\"end\":49916,\"start\":49908},{\"end\":50197,\"start\":50191},{\"end\":50212,\"start\":50204},{\"end\":50226,\"start\":50223},{\"end\":50243,\"start\":50235},{\"end\":50516,\"start\":50510},{\"end\":50537,\"start\":50533},{\"end\":50549,\"start\":50545},{\"end\":50572,\"start\":50567},{\"end\":50934,\"start\":50930},{\"end\":50948,\"start\":50941},{\"end\":51253,\"start\":51249},{\"end\":51266,\"start\":51260},{\"end\":51284,\"start\":51283},{\"end\":51303,\"start\":51295},{\"end\":51319,\"start\":51312},{\"end\":51330,\"start\":51327},{\"end\":51710,\"start\":51701},{\"end\":51727,\"start\":51720},{\"end\":51745,\"start\":51739},{\"end\":51756,\"start\":51753},{\"end\":51773,\"start\":51765},{\"end\":52289,\"start\":52287},{\"end\":52302,\"start\":52298},{\"end\":52319,\"start\":52310},{\"end\":52330,\"start\":52325},{\"end\":52341,\"start\":52340},{\"end\":52354,\"start\":52347},{\"end\":52371,\"start\":52365},{\"end\":52394,\"start\":52391},{\"end\":52419,\"start\":52418},{\"end\":52843,\"start\":52838},{\"end\":52856,\"start\":52850},{\"end\":52871,\"start\":52864},{\"end\":52887,\"start\":52878},{\"end\":52889,\"start\":52888},{\"end\":53133,\"start\":53127},{\"end\":53377,\"start\":53369},{\"end\":53390,\"start\":53383},{\"end\":53402,\"start\":53396},{\"end\":53417,\"start\":53410},{\"end\":53428,\"start\":53422},{\"end\":53443,\"start\":53434},{\"end\":53775,\"start\":53774},{\"end\":53777,\"start\":53776},{\"end\":53788,\"start\":53786},{\"end\":53804,\"start\":53803},{\"end\":53819,\"start\":53814},{\"end\":53821,\"start\":53820},{\"end\":53836,\"start\":53830},{\"end\":53851,\"start\":53846},{\"end\":53853,\"start\":53852}]", "bib_author_last_name": "[{\"end\":31634,\"start\":31629},{\"end\":31652,\"start\":31645},{\"end\":31662,\"start\":31660},{\"end\":31681,\"start\":31672},{\"end\":31697,\"start\":31691},{\"end\":31713,\"start\":31707},{\"end\":31727,\"start\":31724},{\"end\":31739,\"start\":31734},{\"end\":31756,\"start\":31748},{\"end\":31769,\"start\":31762},{\"end\":31792,\"start\":31782},{\"end\":31810,\"start\":31801},{\"end\":31824,\"start\":31821},{\"end\":32302,\"start\":32297},{\"end\":32313,\"start\":32308},{\"end\":32326,\"start\":32321},{\"end\":32347,\"start\":32338},{\"end\":32358,\"start\":32353},{\"end\":32372,\"start\":32367},{\"end\":32763,\"start\":32753},{\"end\":32779,\"start\":32775},{\"end\":33035,\"start\":33027},{\"end\":33053,\"start\":33047},{\"end\":33355,\"start\":33351},{\"end\":33362,\"start\":33360},{\"end\":33372,\"start\":33369},{\"end\":33676,\"start\":33673},{\"end\":33691,\"start\":33686},{\"end\":33702,\"start\":33699},{\"end\":33718,\"start\":33711},{\"end\":33734,\"start\":33725},{\"end\":33747,\"start\":33743},{\"end\":34078,\"start\":34073},{\"end\":34091,\"start\":34086},{\"end\":34103,\"start\":34097},{\"end\":34116,\"start\":34111},{\"end\":34137,\"start\":34128},{\"end\":34148,\"start\":34143},{\"end\":34162,\"start\":34157},{\"end\":34522,\"start\":34513},{\"end\":34533,\"start\":34529},{\"end\":34548,\"start\":34541},{\"end\":34564,\"start\":34558},{\"end\":34575,\"start\":34570},{\"end\":34589,\"start\":34583},{\"end\":34599,\"start\":34593},{\"end\":34613,\"start\":34606},{\"end\":34626,\"start\":34621},{\"end\":34640,\"start\":34633},{\"end\":34647,\"start\":34642},{\"end\":35128,\"start\":35115},{\"end\":35148,\"start\":35142},{\"end\":35163,\"start\":35153},{\"end\":35180,\"start\":35173},{\"end\":35194,\"start\":35190},{\"end\":35201,\"start\":35196},{\"end\":35601,\"start\":35596},{\"end\":35613,\"start\":35603},{\"end\":35906,\"start\":35904},{\"end\":35917,\"start\":35913},{\"end\":35932,\"start\":35926},{\"end\":36305,\"start\":36297},{\"end\":36319,\"start\":36314},{\"end\":36335,\"start\":36328},{\"end\":36354,\"start\":36346},{\"end\":36717,\"start\":36709},{\"end\":36736,\"start\":36728},{\"end\":37005,\"start\":36997},{\"end\":37020,\"start\":37014},{\"end\":37037,\"start\":37029},{\"end\":37056,\"start\":37048},{\"end\":37319,\"start\":37310},{\"end\":37331,\"start\":37327},{\"end\":37343,\"start\":37340},{\"end\":37354,\"start\":37345},{\"end\":37365,\"start\":37358},{\"end\":37379,\"start\":37372},{\"end\":37390,\"start\":37388},{\"end\":37399,\"start\":37392},{\"end\":37747,\"start\":37742},{\"end\":37761,\"start\":37757},{\"end\":37776,\"start\":37771},{\"end\":37790,\"start\":37786},{\"end\":38064,\"start\":38056},{\"end\":38076,\"start\":38073},{\"end\":38087,\"start\":38078},{\"end\":38095,\"start\":38091},{\"end\":38111,\"start\":38104},{\"end\":38120,\"start\":38113},{\"end\":38448,\"start\":38439},{\"end\":38454,\"start\":38450},{\"end\":38466,\"start\":38460},{\"end\":38482,\"start\":38476},{\"end\":38489,\"start\":38484},{\"end\":38519,\"start\":38498},{\"end\":38530,\"start\":38521},{\"end\":38938,\"start\":38923},{\"end\":38947,\"start\":38940},{\"end\":38957,\"start\":38953},{\"end\":38967,\"start\":38959},{\"end\":38979,\"start\":38971},{\"end\":38991,\"start\":38986},{\"end\":39006,\"start\":39002},{\"end\":39020,\"start\":39008},{\"end\":39514,\"start\":39502},{\"end\":39528,\"start\":39523},{\"end\":39546,\"start\":39537},{\"end\":39846,\"start\":39839},{\"end\":39864,\"start\":39855},{\"end\":39882,\"start\":39874},{\"end\":40176,\"start\":40166},{\"end\":40192,\"start\":40183},{\"end\":40211,\"start\":40205},{\"end\":40486,\"start\":40479},{\"end\":40500,\"start\":40490},{\"end\":40520,\"start\":40506},{\"end\":40533,\"start\":40526},{\"end\":40812,\"start\":40810},{\"end\":40829,\"start\":40819},{\"end\":40845,\"start\":40837},{\"end\":40863,\"start\":40854},{\"end\":40882,\"start\":40872},{\"end\":40894,\"start\":40889},{\"end\":41180,\"start\":41177},{\"end\":41196,\"start\":41189},{\"end\":41211,\"start\":41206},{\"end\":41570,\"start\":41562},{\"end\":41585,\"start\":41579},{\"end\":41602,\"start\":41593},{\"end\":41624,\"start\":41611},{\"end\":41632,\"start\":41626},{\"end\":41642,\"start\":41636},{\"end\":41659,\"start\":41653},{\"end\":41676,\"start\":41670},{\"end\":41686,\"start\":41678},{\"end\":42056,\"start\":42048},{\"end\":42069,\"start\":42064},{\"end\":42086,\"start\":42077},{\"end\":42463,\"start\":42458},{\"end\":42481,\"start\":42472},{\"end\":42788,\"start\":42779},{\"end\":42804,\"start\":42799},{\"end\":42824,\"start\":42815},{\"end\":42839,\"start\":42832},{\"end\":42855,\"start\":42848},{\"end\":43178,\"start\":43169},{\"end\":43186,\"start\":43182},{\"end\":43195,\"start\":43188},{\"end\":43207,\"start\":43199},{\"end\":43220,\"start\":43217},{\"end\":43235,\"start\":43229},{\"end\":43244,\"start\":43237},{\"end\":43564,\"start\":43557},{\"end\":43582,\"start\":43573},{\"end\":43872,\"start\":43863},{\"end\":43881,\"start\":43879},{\"end\":43899,\"start\":43890},{\"end\":43916,\"start\":43907},{\"end\":43924,\"start\":43922},{\"end\":43940,\"start\":43933},{\"end\":44251,\"start\":44246},{\"end\":44262,\"start\":44258},{\"end\":44281,\"start\":44273},{\"end\":44299,\"start\":44291},{\"end\":44316,\"start\":44309},{\"end\":44689,\"start\":44682},{\"end\":44704,\"start\":44701},{\"end\":44719,\"start\":44712},{\"end\":44734,\"start\":44728},{\"end\":44747,\"start\":44744},{\"end\":44765,\"start\":44758},{\"end\":44780,\"start\":44774},{\"end\":44795,\"start\":44789},{\"end\":44811,\"start\":44804},{\"end\":44823,\"start\":44818},{\"end\":45285,\"start\":45272},{\"end\":45302,\"start\":45295},{\"end\":45311,\"start\":45308},{\"end\":45328,\"start\":45322},{\"end\":45669,\"start\":45661},{\"end\":45681,\"start\":45679},{\"end\":45690,\"start\":45687},{\"end\":45698,\"start\":45696},{\"end\":45713,\"start\":45709},{\"end\":45725,\"start\":45721},{\"end\":45730,\"start\":45727},{\"end\":46059,\"start\":46052},{\"end\":46078,\"start\":46069},{\"end\":46094,\"start\":46088},{\"end\":46109,\"start\":46104},{\"end\":46122,\"start\":46117},{\"end\":46592,\"start\":46585},{\"end\":46606,\"start\":46602},{\"end\":46620,\"start\":46615},{\"end\":46631,\"start\":46628},{\"end\":46644,\"start\":46642},{\"end\":46658,\"start\":46650},{\"end\":46671,\"start\":46666},{\"end\":46689,\"start\":46682},{\"end\":47059,\"start\":47052},{\"end\":47073,\"start\":47069},{\"end\":47089,\"start\":47083},{\"end\":47098,\"start\":47096},{\"end\":47109,\"start\":47104},{\"end\":47123,\"start\":47117},{\"end\":47141,\"start\":47134},{\"end\":47162,\"start\":47157},{\"end\":47631,\"start\":47624},{\"end\":47644,\"start\":47642},{\"end\":47658,\"start\":47654},{\"end\":47672,\"start\":47664},{\"end\":47681,\"start\":47676},{\"end\":47697,\"start\":47692},{\"end\":47706,\"start\":47699},{\"end\":48096,\"start\":48090},{\"end\":48115,\"start\":48106},{\"end\":48131,\"start\":48124},{\"end\":48436,\"start\":48429},{\"end\":48447,\"start\":48444},{\"end\":48459,\"start\":48455},{\"end\":48473,\"start\":48468},{\"end\":48490,\"start\":48480},{\"end\":48505,\"start\":48498},{\"end\":48850,\"start\":48842},{\"end\":48862,\"start\":48858},{\"end\":48875,\"start\":48870},{\"end\":48891,\"start\":48884},{\"end\":48897,\"start\":48893},{\"end\":48907,\"start\":48901},{\"end\":48923,\"start\":48915},{\"end\":48943,\"start\":48934},{\"end\":48962,\"start\":48953},{\"end\":48973,\"start\":48964},{\"end\":49453,\"start\":49445},{\"end\":49465,\"start\":49460},{\"end\":49482,\"start\":49472},{\"end\":49500,\"start\":49491},{\"end\":49512,\"start\":49502},{\"end\":49858,\"start\":49854},{\"end\":49875,\"start\":49868},{\"end\":49887,\"start\":49881},{\"end\":49906,\"start\":49898},{\"end\":49925,\"start\":49917},{\"end\":50202,\"start\":50198},{\"end\":50221,\"start\":50213},{\"end\":50233,\"start\":50227},{\"end\":50252,\"start\":50244},{\"end\":50531,\"start\":50517},{\"end\":50543,\"start\":50538},{\"end\":50565,\"start\":50550},{\"end\":50580,\"start\":50573},{\"end\":50939,\"start\":50935},{\"end\":50954,\"start\":50949},{\"end\":51258,\"start\":51254},{\"end\":51281,\"start\":51267},{\"end\":51293,\"start\":51285},{\"end\":51310,\"start\":51304},{\"end\":51325,\"start\":51320},{\"end\":51336,\"start\":51331},{\"end\":51343,\"start\":51338},{\"end\":51718,\"start\":51711},{\"end\":51737,\"start\":51728},{\"end\":51751,\"start\":51746},{\"end\":51763,\"start\":51757},{\"end\":51779,\"start\":51774},{\"end\":52296,\"start\":52290},{\"end\":52308,\"start\":52303},{\"end\":52323,\"start\":52320},{\"end\":52338,\"start\":52331},{\"end\":52345,\"start\":52342},{\"end\":52363,\"start\":52355},{\"end\":52389,\"start\":52372},{\"end\":52406,\"start\":52395},{\"end\":52416,\"start\":52408},{\"end\":52425,\"start\":52420},{\"end\":52433,\"start\":52427},{\"end\":52848,\"start\":52844},{\"end\":52862,\"start\":52857},{\"end\":52876,\"start\":52872},{\"end\":52894,\"start\":52890},{\"end\":53141,\"start\":53134},{\"end\":53381,\"start\":53378},{\"end\":53394,\"start\":53391},{\"end\":53408,\"start\":53403},{\"end\":53420,\"start\":53418},{\"end\":53432,\"start\":53429},{\"end\":53447,\"start\":53444},{\"end\":53784,\"start\":53778},{\"end\":53795,\"start\":53789},{\"end\":53801,\"start\":53797},{\"end\":53812,\"start\":53805},{\"end\":53828,\"start\":53822},{\"end\":53844,\"start\":53837},{\"end\":53861,\"start\":53854},{\"end\":53870,\"start\":53863}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":245262002},\"end\":32197,\"start\":31533},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195798779},\"end\":32668,\"start\":32199},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":246902471},\"end\":32969,\"start\":32670},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":234357997},\"end\":33269,\"start\":32971},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":227275511},\"end\":33608,\"start\":33271},{\"attributes\":{\"doi\":\"arXiv:2207.03442\",\"id\":\"b5\"},\"end\":33962,\"start\":33610},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":221589250},\"end\":34413,\"start\":33964},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":247316687},\"end\":34962,\"start\":34415},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4021383},\"end\":35479,\"start\":34964},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3515742},\"end\":35851,\"start\":35481},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":219955663},\"end\":36150,\"start\":35853},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":214351505},\"end\":36616,\"start\":36152},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2354567},\"end\":36938,\"start\":36618},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":848298},\"end\":37231,\"start\":36940},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":17038958},\"end\":37662,\"start\":37233},{\"attributes\":{\"doi\":\"arXiv:2209.11888\",\"id\":\"b15\"},\"end\":37998,\"start\":37664},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":54469209},\"end\":38295,\"start\":38000},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5084719},\"end\":38818,\"start\":38297},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":203902672},\"end\":39394,\"start\":38820},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":92313805},\"end\":39759,\"start\":39396},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":155101108},\"end\":40094,\"start\":39761},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195908774},\"end\":40419,\"start\":40096},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":248563188},\"end\":40717,\"start\":40421},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":231885522},\"end\":41103,\"start\":40719},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":252693323},\"end\":41443,\"start\":41105},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17327816},\"end\":41975,\"start\":41445},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":211003765},\"end\":42360,\"start\":41977},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":211574187},\"end\":42675,\"start\":42362},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":883729},\"end\":43091,\"start\":42677},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1609402},\"end\":43445,\"start\":43093},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5043815},\"end\":43774,\"start\":43447},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10788863},\"end\":44176,\"start\":43776},{\"attributes\":{\"doi\":\"PMLR, 2021. 2\",\"id\":\"b32\",\"matched_paper_id\":234483016},\"end\":44604,\"start\":44178},{\"attributes\":{\"doi\":\"PMLR, 2021. 4\",\"id\":\"b33\",\"matched_paper_id\":231591445},\"end\":45193,\"start\":44606},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":239015911},\"end\":45539,\"start\":45195},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":229935148},\"end\":45982,\"start\":45541},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":245335280},\"end\":46533,\"start\":45984},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":243938678},\"end\":46962,\"start\":46535},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":248986576},\"end\":47566,\"start\":46964},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":233241040},\"end\":47995,\"start\":47568},{\"attributes\":{\"doi\":\"arXiv:2104.05358\",\"id\":\"b40\"},\"end\":48345,\"start\":47997},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":229549248},\"end\":48739,\"start\":48347},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":222359195},\"end\":49350,\"start\":48741},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":51709940},\"end\":49740,\"start\":49352},{\"attributes\":{\"id\":\"b44\"},\"end\":50136,\"start\":49742},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":58667578},\"end\":50444,\"start\":50138},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b46\",\"matched_paper_id\":14888175},\"end\":50858,\"start\":50446},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":196470871},\"end\":51172,\"start\":50860},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":227209335},\"end\":51640,\"start\":51174},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206593880},\"end\":52176,\"start\":51642},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":3390491},\"end\":52744,\"start\":52178},{\"attributes\":{\"id\":\"b51\"},\"end\":53061,\"start\":52746},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5560643},\"end\":53291,\"start\":53063},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3697403},\"end\":53680,\"start\":53293},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3792835},\"end\":54228,\"start\":53682}]", "bib_title": "[{\"end\":31619,\"start\":31533},{\"end\":32289,\"start\":32199},{\"end\":32741,\"start\":32670},{\"end\":33016,\"start\":32971},{\"end\":33345,\"start\":33271},{\"end\":34067,\"start\":33964},{\"end\":34505,\"start\":34415},{\"end\":35102,\"start\":34964},{\"end\":35589,\"start\":35481},{\"end\":35893,\"start\":35853},{\"end\":36286,\"start\":36152},{\"end\":36698,\"start\":36618},{\"end\":36986,\"start\":36940},{\"end\":37306,\"start\":37233},{\"end\":38052,\"start\":38000},{\"end\":38433,\"start\":38297},{\"end\":38910,\"start\":38820},{\"end\":39494,\"start\":39396},{\"end\":39830,\"start\":39761},{\"end\":40159,\"start\":40096},{\"end\":40473,\"start\":40421},{\"end\":40803,\"start\":40719},{\"end\":41169,\"start\":41105},{\"end\":41553,\"start\":41445},{\"end\":42040,\"start\":41977},{\"end\":42449,\"start\":42362},{\"end\":42770,\"start\":42677},{\"end\":43160,\"start\":43093},{\"end\":43547,\"start\":43447},{\"end\":43854,\"start\":43776},{\"end\":44238,\"start\":44178},{\"end\":44675,\"start\":44606},{\"end\":45263,\"start\":45195},{\"end\":45655,\"start\":45541},{\"end\":46044,\"start\":45984},{\"end\":46575,\"start\":46535},{\"end\":47042,\"start\":46964},{\"end\":47614,\"start\":47568},{\"end\":48417,\"start\":48347},{\"end\":48833,\"start\":48741},{\"end\":49437,\"start\":49352},{\"end\":50189,\"start\":50138},{\"end\":50508,\"start\":50446},{\"end\":50928,\"start\":50860},{\"end\":51247,\"start\":51174},{\"end\":51699,\"start\":51642},{\"end\":52285,\"start\":52178},{\"end\":53125,\"start\":53063},{\"end\":53367,\"start\":53293},{\"end\":53772,\"start\":53682}]", "bib_author": "[{\"end\":31636,\"start\":31621},{\"end\":31654,\"start\":31636},{\"end\":31664,\"start\":31654},{\"end\":31683,\"start\":31664},{\"end\":31699,\"start\":31683},{\"end\":31715,\"start\":31699},{\"end\":31729,\"start\":31715},{\"end\":31741,\"start\":31729},{\"end\":31758,\"start\":31741},{\"end\":31771,\"start\":31758},{\"end\":31794,\"start\":31771},{\"end\":31812,\"start\":31794},{\"end\":31826,\"start\":31812},{\"end\":32304,\"start\":32291},{\"end\":32315,\"start\":32304},{\"end\":32328,\"start\":32315},{\"end\":32349,\"start\":32328},{\"end\":32360,\"start\":32349},{\"end\":32374,\"start\":32360},{\"end\":32765,\"start\":32743},{\"end\":32781,\"start\":32765},{\"end\":33037,\"start\":33018},{\"end\":33055,\"start\":33037},{\"end\":33357,\"start\":33347},{\"end\":33364,\"start\":33357},{\"end\":33374,\"start\":33364},{\"end\":33678,\"start\":33669},{\"end\":33693,\"start\":33678},{\"end\":33704,\"start\":33693},{\"end\":33720,\"start\":33704},{\"end\":33736,\"start\":33720},{\"end\":33749,\"start\":33736},{\"end\":34080,\"start\":34069},{\"end\":34093,\"start\":34080},{\"end\":34105,\"start\":34093},{\"end\":34118,\"start\":34105},{\"end\":34139,\"start\":34118},{\"end\":34150,\"start\":34139},{\"end\":34164,\"start\":34150},{\"end\":34524,\"start\":34507},{\"end\":34535,\"start\":34524},{\"end\":34550,\"start\":34535},{\"end\":34566,\"start\":34550},{\"end\":34577,\"start\":34566},{\"end\":34591,\"start\":34577},{\"end\":34601,\"start\":34591},{\"end\":34615,\"start\":34601},{\"end\":34628,\"start\":34615},{\"end\":34642,\"start\":34628},{\"end\":34649,\"start\":34642},{\"end\":35130,\"start\":35104},{\"end\":35150,\"start\":35130},{\"end\":35165,\"start\":35150},{\"end\":35182,\"start\":35165},{\"end\":35196,\"start\":35182},{\"end\":35203,\"start\":35196},{\"end\":35603,\"start\":35591},{\"end\":35615,\"start\":35603},{\"end\":35908,\"start\":35895},{\"end\":35919,\"start\":35908},{\"end\":35934,\"start\":35919},{\"end\":36307,\"start\":36288},{\"end\":36321,\"start\":36307},{\"end\":36337,\"start\":36321},{\"end\":36356,\"start\":36337},{\"end\":36719,\"start\":36700},{\"end\":36738,\"start\":36719},{\"end\":37007,\"start\":36988},{\"end\":37022,\"start\":37007},{\"end\":37039,\"start\":37022},{\"end\":37058,\"start\":37039},{\"end\":37321,\"start\":37308},{\"end\":37333,\"start\":37321},{\"end\":37345,\"start\":37333},{\"end\":37356,\"start\":37345},{\"end\":37367,\"start\":37356},{\"end\":37381,\"start\":37367},{\"end\":37392,\"start\":37381},{\"end\":37401,\"start\":37392},{\"end\":37749,\"start\":37735},{\"end\":37763,\"start\":37749},{\"end\":37778,\"start\":37763},{\"end\":37792,\"start\":37778},{\"end\":38066,\"start\":38054},{\"end\":38078,\"start\":38066},{\"end\":38089,\"start\":38078},{\"end\":38097,\"start\":38089},{\"end\":38113,\"start\":38097},{\"end\":38122,\"start\":38113},{\"end\":38450,\"start\":38435},{\"end\":38456,\"start\":38450},{\"end\":38468,\"start\":38456},{\"end\":38484,\"start\":38468},{\"end\":38491,\"start\":38484},{\"end\":38521,\"start\":38491},{\"end\":38532,\"start\":38521},{\"end\":38940,\"start\":38912},{\"end\":38949,\"start\":38940},{\"end\":38959,\"start\":38949},{\"end\":38969,\"start\":38959},{\"end\":38981,\"start\":38969},{\"end\":38993,\"start\":38981},{\"end\":39008,\"start\":38993},{\"end\":39022,\"start\":39008},{\"end\":39516,\"start\":39496},{\"end\":39530,\"start\":39516},{\"end\":39548,\"start\":39530},{\"end\":39848,\"start\":39832},{\"end\":39866,\"start\":39848},{\"end\":39884,\"start\":39866},{\"end\":40178,\"start\":40161},{\"end\":40194,\"start\":40178},{\"end\":40213,\"start\":40194},{\"end\":40488,\"start\":40475},{\"end\":40502,\"start\":40488},{\"end\":40522,\"start\":40502},{\"end\":40535,\"start\":40522},{\"end\":40814,\"start\":40805},{\"end\":40831,\"start\":40814},{\"end\":40847,\"start\":40831},{\"end\":40865,\"start\":40847},{\"end\":40884,\"start\":40865},{\"end\":40896,\"start\":40884},{\"end\":41182,\"start\":41171},{\"end\":41198,\"start\":41182},{\"end\":41213,\"start\":41198},{\"end\":41572,\"start\":41555},{\"end\":41587,\"start\":41572},{\"end\":41604,\"start\":41587},{\"end\":41626,\"start\":41604},{\"end\":41634,\"start\":41626},{\"end\":41644,\"start\":41634},{\"end\":41661,\"start\":41644},{\"end\":41678,\"start\":41661},{\"end\":41688,\"start\":41678},{\"end\":42058,\"start\":42042},{\"end\":42071,\"start\":42058},{\"end\":42088,\"start\":42071},{\"end\":42465,\"start\":42451},{\"end\":42483,\"start\":42465},{\"end\":42790,\"start\":42772},{\"end\":42806,\"start\":42790},{\"end\":42826,\"start\":42806},{\"end\":42841,\"start\":42826},{\"end\":42857,\"start\":42841},{\"end\":43180,\"start\":43162},{\"end\":43188,\"start\":43180},{\"end\":43197,\"start\":43188},{\"end\":43209,\"start\":43197},{\"end\":43222,\"start\":43209},{\"end\":43237,\"start\":43222},{\"end\":43246,\"start\":43237},{\"end\":43566,\"start\":43549},{\"end\":43584,\"start\":43566},{\"end\":43874,\"start\":43856},{\"end\":43883,\"start\":43874},{\"end\":43901,\"start\":43883},{\"end\":43918,\"start\":43901},{\"end\":43926,\"start\":43918},{\"end\":43942,\"start\":43926},{\"end\":44253,\"start\":44240},{\"end\":44264,\"start\":44253},{\"end\":44283,\"start\":44264},{\"end\":44301,\"start\":44283},{\"end\":44318,\"start\":44301},{\"end\":44691,\"start\":44677},{\"end\":44706,\"start\":44691},{\"end\":44721,\"start\":44706},{\"end\":44736,\"start\":44721},{\"end\":44749,\"start\":44736},{\"end\":44767,\"start\":44749},{\"end\":44782,\"start\":44767},{\"end\":44797,\"start\":44782},{\"end\":44813,\"start\":44797},{\"end\":44825,\"start\":44813},{\"end\":45287,\"start\":45265},{\"end\":45304,\"start\":45287},{\"end\":45313,\"start\":45304},{\"end\":45330,\"start\":45313},{\"end\":45671,\"start\":45657},{\"end\":45683,\"start\":45671},{\"end\":45692,\"start\":45683},{\"end\":45700,\"start\":45692},{\"end\":45715,\"start\":45700},{\"end\":45727,\"start\":45715},{\"end\":45732,\"start\":45727},{\"end\":46061,\"start\":46046},{\"end\":46080,\"start\":46061},{\"end\":46096,\"start\":46080},{\"end\":46111,\"start\":46096},{\"end\":46124,\"start\":46111},{\"end\":46594,\"start\":46577},{\"end\":46608,\"start\":46594},{\"end\":46622,\"start\":46608},{\"end\":46633,\"start\":46622},{\"end\":46646,\"start\":46633},{\"end\":46660,\"start\":46646},{\"end\":46673,\"start\":46660},{\"end\":46691,\"start\":46673},{\"end\":47061,\"start\":47044},{\"end\":47075,\"start\":47061},{\"end\":47091,\"start\":47075},{\"end\":47100,\"start\":47091},{\"end\":47111,\"start\":47100},{\"end\":47125,\"start\":47111},{\"end\":47143,\"start\":47125},{\"end\":47164,\"start\":47143},{\"end\":47633,\"start\":47616},{\"end\":47646,\"start\":47633},{\"end\":47660,\"start\":47646},{\"end\":47674,\"start\":47660},{\"end\":47683,\"start\":47674},{\"end\":47699,\"start\":47683},{\"end\":47708,\"start\":47699},{\"end\":48098,\"start\":48082},{\"end\":48117,\"start\":48098},{\"end\":48133,\"start\":48117},{\"end\":48438,\"start\":48419},{\"end\":48449,\"start\":48438},{\"end\":48461,\"start\":48449},{\"end\":48475,\"start\":48461},{\"end\":48492,\"start\":48475},{\"end\":48507,\"start\":48492},{\"end\":48852,\"start\":48835},{\"end\":48864,\"start\":48852},{\"end\":48877,\"start\":48864},{\"end\":48893,\"start\":48877},{\"end\":48899,\"start\":48893},{\"end\":48909,\"start\":48899},{\"end\":48925,\"start\":48909},{\"end\":48945,\"start\":48925},{\"end\":48964,\"start\":48945},{\"end\":48975,\"start\":48964},{\"end\":49455,\"start\":49439},{\"end\":49467,\"start\":49455},{\"end\":49484,\"start\":49467},{\"end\":49502,\"start\":49484},{\"end\":49514,\"start\":49502},{\"end\":49860,\"start\":49847},{\"end\":49877,\"start\":49860},{\"end\":49889,\"start\":49877},{\"end\":49908,\"start\":49889},{\"end\":49927,\"start\":49908},{\"end\":50204,\"start\":50191},{\"end\":50223,\"start\":50204},{\"end\":50235,\"start\":50223},{\"end\":50254,\"start\":50235},{\"end\":50533,\"start\":50510},{\"end\":50545,\"start\":50533},{\"end\":50567,\"start\":50545},{\"end\":50582,\"start\":50567},{\"end\":50941,\"start\":50930},{\"end\":50956,\"start\":50941},{\"end\":51260,\"start\":51249},{\"end\":51283,\"start\":51260},{\"end\":51295,\"start\":51283},{\"end\":51312,\"start\":51295},{\"end\":51327,\"start\":51312},{\"end\":51338,\"start\":51327},{\"end\":51345,\"start\":51338},{\"end\":51720,\"start\":51701},{\"end\":51739,\"start\":51720},{\"end\":51753,\"start\":51739},{\"end\":51765,\"start\":51753},{\"end\":51781,\"start\":51765},{\"end\":52298,\"start\":52287},{\"end\":52310,\"start\":52298},{\"end\":52325,\"start\":52310},{\"end\":52340,\"start\":52325},{\"end\":52347,\"start\":52340},{\"end\":52365,\"start\":52347},{\"end\":52391,\"start\":52365},{\"end\":52408,\"start\":52391},{\"end\":52418,\"start\":52408},{\"end\":52427,\"start\":52418},{\"end\":52435,\"start\":52427},{\"end\":52850,\"start\":52838},{\"end\":52864,\"start\":52850},{\"end\":52878,\"start\":52864},{\"end\":52896,\"start\":52878},{\"end\":53143,\"start\":53127},{\"end\":53383,\"start\":53369},{\"end\":53396,\"start\":53383},{\"end\":53410,\"start\":53396},{\"end\":53422,\"start\":53410},{\"end\":53434,\"start\":53422},{\"end\":53449,\"start\":53434},{\"end\":53786,\"start\":53774},{\"end\":53797,\"start\":53786},{\"end\":53803,\"start\":53797},{\"end\":53814,\"start\":53803},{\"end\":53830,\"start\":53814},{\"end\":53846,\"start\":53830},{\"end\":53863,\"start\":53846},{\"end\":53872,\"start\":53863}]", "bib_venue": "[{\"end\":31845,\"start\":31826},{\"end\":32423,\"start\":32374},{\"end\":32803,\"start\":32781},{\"end\":33104,\"start\":33055},{\"end\":33423,\"start\":33374},{\"end\":33667,\"start\":33610},{\"end\":34174,\"start\":34164},{\"end\":34668,\"start\":34649},{\"end\":35208,\"start\":35203},{\"end\":35638,\"start\":35615},{\"end\":35983,\"start\":35934},{\"end\":36364,\"start\":36356},{\"end\":36759,\"start\":36738},{\"end\":37065,\"start\":37058},{\"end\":37434,\"start\":37401},{\"end\":37733,\"start\":37664},{\"end\":38128,\"start\":38122},{\"end\":38538,\"start\":38532},{\"end\":39069,\"start\":39022},{\"end\":39558,\"start\":39548},{\"end\":39907,\"start\":39884},{\"end\":40238,\"start\":40213},{\"end\":40545,\"start\":40535},{\"end\":40903,\"start\":40896},{\"end\":41262,\"start\":41213},{\"end\":41694,\"start\":41688},{\"end\":42150,\"start\":42088},{\"end\":42504,\"start\":42483},{\"end\":42867,\"start\":42857},{\"end\":43252,\"start\":43246},{\"end\":43594,\"start\":43584},{\"end\":43957,\"start\":43942},{\"end\":44375,\"start\":44331},{\"end\":44882,\"start\":44838},{\"end\":45355,\"start\":45330},{\"end\":45742,\"start\":45732},{\"end\":46205,\"start\":46124},{\"end\":46731,\"start\":46691},{\"end\":47213,\"start\":47164},{\"end\":47770,\"start\":47708},{\"end\":48080,\"start\":47997},{\"end\":48523,\"start\":48507},{\"end\":49022,\"start\":48975},{\"end\":49524,\"start\":49514},{\"end\":49845,\"start\":49742},{\"end\":50280,\"start\":50254},{\"end\":50630,\"start\":50586},{\"end\":51005,\"start\":50956},{\"end\":51397,\"start\":51345},{\"end\":51858,\"start\":51781},{\"end\":52445,\"start\":52435},{\"end\":52836,\"start\":52746},{\"end\":53161,\"start\":53143},{\"end\":53464,\"start\":53449},{\"end\":53919,\"start\":53872},{\"end\":39103,\"start\":39071},{\"end\":46273,\"start\":46207},{\"end\":51922,\"start\":51860},{\"end\":53953,\"start\":53921}]"}}}, "year": 2023, "month": 12, "day": 17}