{"id": 102352396, "updated": "2023-10-01 19:55:13.995", "metadata": {"title": "Speech Model Pre-training for End-to-End Spoken Language Understanding", "authors": "[{\"first\":\"Loren\",\"last\":\"Lugosch\",\"middle\":[]},{\"first\":\"Mirco\",\"last\":\"Ravanelli\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Ignoto\",\"middle\":[]},{\"first\":\"Vikrant\",\"last\":\"Tomar\",\"middle\":[\"Singh\"]},{\"first\":\"Yoshua\",\"last\":\"Bengio\",\"middle\":[]}]", "venue": "Interspeech 2019", "journal": "Interspeech 2019", "publication_date": {"year": 2019, "month": 4, "day": 7}, "abstract": "Whereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-to-end SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-to-end models without a large amount of training data is difficult. We propose a method to reduce the data requirements of end-to-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "1904.03670", "mag": "2972584841", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/LugoschRITB19", "doi": "10.21437/interspeech.2019-2396"}}, "content": {"source": {"pdf_hash": "6a73aaef0b7d998448078eed51c9b9b43d5ecffa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.03670v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.03670", "status": "GREEN"}}, "grobid": {"id": "7b6348ad91d92a7e36e4cb53fc0749e6b1d34298", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6a73aaef0b7d998448078eed51c9b9b43d5ecffa.txt", "contents": "\nSpeech Model Pre-training for End-to-End Spoken Language Understanding\n\n\nLoren Lugosch lugoschl@mila.quebec \nUniversit\u00e9 de Montr\u00e9al / Mila\n\n\nMirco Ravanelli mirco.ravanelli@mila.quebec \nUniversit\u00e9 de Montr\u00e9al / Mila\n\n\nPatrick Ignoto patrick.ignoto@fluent.ai \nVikrant Singh Tomar vikrant@fluent.ai \nYoshua Bengio yoshua.bengio@mila.quebec \nUniversit\u00e9 de Montr\u00e9al / Mila\n\n\nCIFAR Fellow\n\n\nSpeech Model Pre-training for End-to-End Spoken Language Understanding\nIndex Terms: speech recognitionspoken language under- standingend-to-end modelstransfer learning\nWhereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-toend SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-toend models without a large amount of training data is difficult. We propose a method to reduce the data requirements of endto-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training.\n\nIntroduction\n\nSpoken language understanding (SLU) systems infer the meaning or intent of a spoken utterance [1]. This is crucial for voice user interfaces, in which the speaker's utterance needs to be converted into an action or query. For example, for a voice-controlled coffee machine, an utterance like \"make me a large coffee with two milks and a sugar, please\" might have an intent representation like {drink: \"coffee\", size: \"large\", additions: [{type: \"milk\", count: 2}, {type: \"sugar\", count: 1}]}.\n\nThe conventional SLU pipeline is composed of two modules: an automatic speech recognition (ASR) module that maps the speech to a text transcript, and a natural language understanding (NLU) module that maps the text transcript to the speaker's intent [2][3][4]. An alternative approach that is beginning to gain popularity is end-to-end SLU [5][6][7][8][9]. In end-to-end SLU, a single trainable model maps the speech audio directly to the speaker's intent without explicitly producing a text transcript (Fig. 1). Unlike the conventional SLU pipeline, end-toend SLU:\n\n\u2022 directly optimizes the metric of interest (intent recognition accuracy),\n\n\u2022 does not waste modeling effort on estimating the text, thus yielding a more compact model and avoiding an error-prone intermediate step involving search algorithms, language models, finite state transducers, etc.,\n\n\u2022 and enables harnessing aspects of the utterance that may be relevant for inferring the intent, but are not present in the text transcript, such as prosody.\n\nEnd-to-end models have been made possible by deep learning, which automatically learns hierarchical representations of the input signal [10][11][12][13][14]. Speech is natural to represent in a hierarchical way: waveform \u2192 phonemes \u2192 morphemes \u2192 words \u2192 concepts \u2192 meaning. However, because speech signals are high-dimensional and highly variable even for a single speaker, training deep models and learning these hierarchical representations without a large amount of training data is difficult. The computer vision [15,16], natural language processing [17][18][19][20][21], and ASR [22,23] communities have attacked the problem of limited supervised training data with great success by pre-training deep models on related tasks for which there is more training data. Following their lead, we propose an efficient ASR-based pre-training methodology in this paper and show that it may be used to improve the performance of end-toend SLU models, especially when the amount of training data is very small.\n\nOur contributions are as follows:\n\n\u2022 We introduce a dataset for realistic SLU experiments.\n\n\u2022 We use this dataset to demonstrate effective speech model pre-training techniques for low-resource SLU.\n\n\u2022 We make our code 1 and data 2 publicly available for researchers to replicate and build on our work.\n\n\nRelated work\n\nThree key papers describing end-to-end SLU were written by Qian et al. [5], Serdyuk et al. [6], and Chen et al. [7]. Serdyuk et al. in [6] use no pre-training whatsoever. Qian et al. in [5] use an auto-encoder to initialize the SLU model. Chen et al. [7] pretrain the first stage of an SLU model to recognize graphemes; the softmax outputs of the first stage are then fed to a classifier second stage. The model proposed in this paper is similar to theirs, but removes the restriction of the softmax bottleneck and uses alternative training targets, as we will describe later. More recently, Haghani et al. in [8] compare four types of sequence-to-sequence models for SLU, including a direct model (end-to-end with no pre-training) and a multi-task model (uses a shared encoder whose output is ingested by a separate ASR decoder and SLU decoder). The model proposed here is somewhat similar to their multi-task model, although we do not use or require the ASR targets during SLU training.\n\nThe work listed above deals with very high resource SLUin [8], for instance, the Google Home [24] dataset consists of 24 million labeled utterances. In contrast, Renkens et al. in [9] consider the problem of end-to-end SLU with limited training data, and find that capsule networks [25], compared to conventional neural network models, are more easily capable of learning endto-end SLU from scratch. However, they do not consider the effect of pre-training on other speech data.\n\nThis previous work has all been conducted on datasets that are closed-source or too small to test hypotheses about the amount of data required to generalize well. The lack of a good open-source dataset for end-to-end SLU experiments makes it difficult for most people to perform high-quality, reproducible research on this topic. We therefore created a new SLU dataset, the \"Fluent Speech Commands\" dataset, which Fluent.ai releases along with this paper.\n\n\nDataset\n\nThis section describes the structure and creation of Fluent Speech Commands.\n\n\nAudio and labels\n\nThe dataset is composed of 16 kHz single-channel .wav audio files. Each audio file contains a recording of a single command that one might use for a smart home or virtual assistant, like \"put on the music\" or \"turn up the heat in the kitchen\".\n\nEach audio is labeled with three slots: action, object, and location. A slot takes on one of multiple values: for instance, the \"location\" slot can take on the values \"none\", \"kitchen\", \"bedroom\", or \"washroom\". We refer to the combination of slot values as the intent of the utterance. The dataset has 31 unique intents in total. We do not distinguish between domain, intent, and slot prediction, as is sometimes done in SLU [26].\n\nThe dataset can be used as a multi-label classification task, where the goal is to predict the action, object, and location labels. Since the slots are not actually independent of each other, a more careful approach would model the relationship between slots, e.g. using an autoregressive model, as in [8]. We use the simpler multi-label classification approach in this paper, so as to avoid the issues sometimes encountered training autoregressive models and instead focus on questions related to generalization using a simpler model. Alternately, the 31 distinct intents can be \"flattened\" and used as 31 distinct labels for a single-label classification task.\n\nFor each intent, there are multiple possible wordings: for example, the intent {action: \"activate\", object: \"lights\", location: \"none\"} can be expressed as \"turn on the lights\", \"switch the lights on\", \"lights on\", etc.. These phrases were decided upon before data collection by asking employees at Fluent.ai, including both native and non-native English speakers, for various ways in which they might express a particular intent. There are 248 different phrases in total. \n\n\nData collection\n\nThe data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset. The data was validated by a separate set of crowdsourcers. All audios deemed by the crowdsourcers to be unintelligible or contain the wrong phrase were removed. The total number of speakers, utterances, and hours of audio remaining is shown in Table 1.\n\n\nDataset splits\n\nThe utterances are randomly divided into train, valid, and test splits in such a way that no speaker appears in more than one split. Each split contains all possible wordings for each intent, though our code has the option to include data for only certain wordings for different sets, to test the model's ability to recognize wordings not heard during training. The dataset has a .csv file for each split that lists the speaker ID, file path, transcription, and slots for all the .wav files in that split.\n\n\nRelated datasets\n\nHere we review some related public datasets and show the gap that Fluent Speech Commands fills.\n\nThe Google Speech Commands dataset [27] (to which the name \"Fluent Speech Commands\" is an homage) is a free dataset of 30 single-word spoken commands (\"yes\", \"no\", \"stop\", \"go\", etc.). This dataset is suitable for keyword spotting experiments, but not for SLU.\n\nATIS is an SLU dataset consisting of utterances related to travel planning 3 . This dataset can only be obtained expensively from the Linguistic Data Consortium.\n\nThe Snips NLU Benchmark [3] has a rich set of virtual assistant commands, but contains only text, with no audio, and hence is not suitable for end-to-end SLU experiments.\n\nThe Grabo, Domotica, and Patcor datasets are three related datasets of spoken commands for robot control and card games developed by KU Leuven 4 and used in [9]. These datasets are free, but have only a small number of speakers and phrases.\n\nIn contrast to these datasets, Fluent Speech Commands is simultaneously audio-based, reasonably large, and free, and contains several multiple-word commands corresponding to each of the intents. Conventional SLU End-to-end SLU { action : \"activate\", object : \"lights\", location : \"kitchen\" } \"It was the best of times, it was the worst of times...\" \"Turn the kitchen lights on\"\n\n\nIH T W AA Z DH AH B EH S T AH V T AY M Z IH T W AA Z DH AH W ER S T AH V T AY M Z IT WAS THE BEST OF TIMES IT WAS THE WORST OF TIMES\n\n\nPhoneme classifier\n\nWord classifier Figure 2: The lower layers of the model are pre-trained using ASR targets (words and phonemes). The word and phoneme classifiers are discarded, and the features from the pre-trained part of the model (blue) are used as the input to the subsequent module (white), which is trained using SLU targets.\n\n\nModel and Pre-training Strategy\n\nThe model proposed in this paper, shown in Fig. 2, is a deep neural network consisting of a stack of modules, where the first modules are pre-trained to predict phonemes and words. The word and phoneme classifiers are discarded, and the entire model is then trained end-to-end on the supervised SLU task. In what follows, we justify these design decisions and give more details about the model hyperparameters.\n\n\nWhich ASR targets to use?\n\nASR models are trained using a variety of targets, including phonemes, graphemes, wordpieces, or more recently whole words [28][29][30]. We choose whole words as the pre-training targets, since this is what a typical NLU module would expect as input. A typical ASR dataset contains too many unique words (LibriSpeech [31] has more than 200,000) to assign an output to each one; we only assign a label to the 10,000 most common words. This leaves much of the pre-training data without any labels, which wastes data. By using phonemes as intermediate pre-training targets [20,32,33], we are able to pre-train on speech segments with no word label. Additionally, we find that using phonemes as intermediate targets speeds up word-level pre-training [34,35]. We use the Montreal Forced Aligner [36] to obtain wordand phoneme-level alignments for LibriSpeech, and we pretrain the model on the entire 960 hours of training data using these alignments 5,6 . Using force-aligned labels has the additional benefit of enabling pre-training using short, random crops rather than entire utterances, which reduces the computation and memory required to pre-train the model.\n\n\nPhoneme module\n\nThe first module takes as input the audio signal x and outputs h phoneme , a sequence of hidden representations that are pretrained to predict phonemes. The phoneme-level logits are com-puted using a linear classifier:\nl phoneme = W phoneme h phoneme + b phoneme .\n(1)\n\nThe phoneme module is implemented using a SincNet layer [37,38], which processes the raw input waveform, followed by multiple convolutional layers and recurrent layers with pooling and dropout. More detailed hyperparameters can be found in our code.\n\n\nWord module\n\nThe second module takes as input h phoneme and outputs h word . Similar to the phoneme-level module, it uses recurrent layers with dropout and pooling, and is pre-trained to predict words using another linear classifier:\nl word = W word h word + b word .(2)\nNotice that the input to this module is h phoneme , not l phoneme , and likewise the output to the next stage is h word , not l word . There are two good reasons for forwarding h instead of l. The first is that we don't want to remove a degree of freedom from the model: the size of l is fixed by the number of targets, and this would in turn fix the size of the next layer of the model. The second reason is that computing l word requires multiplying and storing a large (\u2248 2.5 million parameters) weight matrix, and by discarding this matrix after pre-training, we save on memory and computation.\n\n\nIntent module\n\nThe third module, which is not pre-trained, maps h word to the predicted intent. Depending on the structure of the intent representation, the intent module might take on various forms. Since in this work we use a fixed three-slot intent representation, we implement this module using a recurrent layer, followed by max-pooling to squash the sequence of outputs from the recurrent layer into a single vector of logits corresponding to the different slot values, similar to [6].\n\n\nUnfreezing schedule\n\nAlthough the pre-trained model works well as a frozen feature extractor, it may be preferable to \"unfreeze\" its weights and finetune them for the SLU task with backpropagation. Similar to ULMFiT [18], we find that gradually unfreezing the pretrained layers works better than unfreezing them all at once. We unfreeze one layer each epoch, and stop at a pre-determined layer, which is a hyperparameter.\n\n\nExperiments\n\nHere we report results for three experiments on Fluent Speech Commands: using the full dataset, using a subset of the dataset, and using a subset of wordings.\n\n\nFull dataset\n\nWe first trained models given the entire SLU training set. The models used one of: 1) no pre-training (randomly initialized), 2) pre-training with no unfreezing, 3) gradually unfreezing only the word layers, or 4) gradually unfreezing both the word layers and phoneme layers. What we report here as \"accuracy\" refers to the accuracy of all slots for an utterance taken together-that is, if the predicted intent differs from the true intent in even one slot, the prediction is deemed incorrect.   The validation accuracy of these models over time is shown in Fig. 3a. The best results are obtained when only the word layers of the pre-trained model are unfrozen. This may be because the model begins to forget the more general phonetic knowledge acquired during pre-training. For the test set, the frozen model and partially unfrozen model perform roughly equally well (Table 2, \"full\" column), possibly because the test set is \"easier\" than the validation set. In all cases, the pre-trained models outperform the randomly initialized model.\n\n\nPartial dataset\n\nTo simulate a smaller dataset, we randomly selected 10% of the training set, and used this instead of the entire training set. Fig.  3b shows the validation accuracy (on the entire validation set, not a subset) over time. A similar trend is observed as for the entire dataset: unfreezing the word layers works best. The gap in final test accuracy between the randomly initialized model and the pre-trained models increases ( Table 2, \"10%\" column); the final test accuracy for the pre-trained models drops only slightly, further highlighting the advantage of our proposed method.\n\n\nGeneralizing to new wordings\n\nWhat happens if new wordings appear in the test data that never appear in the training data? This is an important question, since it is generally impractical to try to imagine every possible wording for a particular intent while gathering training data.\n\nTo test this, we trained models on three specific phrases, \"turn on the lights\", \"turn off the lights\", and \"switch on the lights\" (273 utterances total), and tested on those same phrases, as well as a new phrase: \"switch off the lights\". If the model incorrectly infers that utterances that contain \"switch\" always correspond to turning on the lights, it will incorrectly guess that \"switch off the lights\" corresponds to turning on the lights; if the model infers that the presence of the word \"off\" corresponds to turning off the lights, it will generalize to the new phrase. The randomly initialized model was unable to fit this tiny training set, even with a very low learning rate and no regularization. The pre-trained models were able to generalize to the new wording (with 97% accuracy on the validation set, which contains more examples of the new phrase than of the training phrases).\n\nHowever, there are many situations in which our model does not correctly generalize. For example, if the model is trained only with examples containing \"bedroom\" and \"washroom\", but then tested on an example containing \"bathroom\", it will guess the intent corresponding to \"bedroom\" because \"bedroom\" sounds more similar to \"bathroom\" than to \"washroom\", even though \"washroom\" is the correct meaning. In text-based NLU, this scenario can be handled using word embeddings, which represent words in such a way that words with similar meanings have similar vector representations [2,39]. It may be possible to teach the pre-trained part of the model to output \"embedding-like\" word representations so that the intent module can recognize the meaning of phrases with synonyms.\n\n\nConclusion\n\nIn this paper, we proposed a pre-training methodology for endto-end SLU models, introduced the Fluent Speech Commands dataset, and used this dataset to show that our pre-training techniques improve performance both for large and small SLU training sets. In the future, we plan to continue using Fluent Speech Commands to explore the limitations of end-to-end SLU, like new wordings and synonyms not observed in the SLU dataset, to see if these limitations can be overcome.\n\n\n10% of the dataset.\n\nFigure 3 :\n3Accuracy on the validation set over time for models trained on (a) the full SLU dataset or (b) 10% of the dataset.\n\n\nFigure 1: Conventional ASR \u2192 NLU system for SLU versus end-to-end SLU.{ \naction: \"activate\", \nobject: \"lights\", \nlocation: \"kitchen\" \n} \n\nSLU \n\nSpeech \nIntent \n\n{ \naction: \"activate\", \nobject: \"lights\", \nlocation: \"kitchen\" \n} \n\nTURN THE \nKITCHEN \nLIGHTS ON \n\nASR \n\nSpeech \n\nNLU \n\nText \nIntent \n\nConventional SLU \n\nEnd-to-end SLU \n\n\n\nTable 1 :\n1Information about the Fluent Speech Commands dataset.Split # of speakers # of utterances # hoursTrain \n77 \n23,132 \n14.7 \nValid \n10 \n3,118 \n1.9 \nTest \n10 \n3,793 \n2.4 \n\nTotal \n97 \n30,043 \n19.0 \n\n\n\nTable 2 :\n2Accuracy on the test set for different models, given the full training dataset or a 10% subset of the training data.Model \nAccuracy (full) Accuracy (10%) \n\nNo pre-training \n96.6% \n88.9% \nNo unfreezing \n98.8% \n97.9% \nUnfreeze word layers \n98.7% \n97.9% \nUnfreeze all layers \n97.2% \n95.8% \n\n\ngithub.com/lorenlugosch/pretrain speech model 2 fluent.ai/research/fluent-speech-commands/\nhttps://catalog.ldc.upenn.edu/LDC94S19 4 https://www.esat.kuleuven.be/psi/spraak/downloads/\nOur alignments can be downloaded from https://zenodo.org/record/2619474#.XKDP2VNKg1g6 We use textgrid.py to process the alignments. https://github.com/kylebgorman/textgrid\nAcknowledgementsWe would like to acknowledge the following for research funding and computing support: NSERC, Calcul Qu\u00e9bec, Compute Canada, the Canada Research Chairs, and CIFAR.Thanks to Dima Serdyuk and Kyle Kastner at Mila, and Farzaneh Fard, Luis Rodriguez Ruiz, Sam Myer, Mohamed Mhiri, and Arash Rad at Fluent.ai for helpful discussions with us about this work.\nSpoken Language Understanding: Systems for Extracting Semantic Information from Speech. G Tur, R D Mori, WileyG. Tur and R. D. Mori, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech. Wiley, 2011.\n\nUsing Recurrent Neural Networks for Slot Filling in Spoken Language Understanding. G Mesnil, Y Dauphin, K Yao, Y Bengio, L Deng, D Hakkanitur, X He, Speech, and Language Processing. G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Hakkani- tur, and X. He, \"Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015.\n\nSnips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces. A Coucke, A Caulier, A Ball, C L Dec, T Bluche, arXiv:1805.10190v3A. Coucke, A. Caulier, A. Ball, C. L. Dec, and T. Bluche, \"Snips Voice Platform: an embedded Spoken Language Un- derstanding system for private-by-design voice interfaces,\" arXiv:1805.10190v3, 2018.\n\nHow may I help you?. A L Gorin, G Riccardi, J H Wright, Speech Communication. 231-2A. L. Gorin, G. Riccardi, and J. H. Wright, \"How may I help you?\" Speech Communication, vol. 23, no. 1-2, pp. 113-127, 1997.\n\nExploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system. Y Qian, R Ubale, V Ramanarayanan, P Lange, Y. Qian, R. Ubale, V. Ramanarayanan, and P. Lange, \"Exploring ASR-free end-to-end modeling to improve spoken language un- derstanding in a cloud-based dialog system,\" ASRU, 2017.\n\nTowards end-to-end spoken language understanding. D Serdyuk, Y Wang, C Fuegen, A Kumar, B Liu, Y Bengio, ICASSP. D. Serdyuk, Y. Wang, C. Fuegen, A. Kumar, B. Liu, and Y. Bengio, \"Towards end-to-end spoken language understanding,\" ICASSP, 2018.\n\nSpoken language understanding without speech recognition. Y.-P Chen, R Price, S Bangalore, ICASSP. Y.-P. Chen, R. Price, and S. Bangalore, \"Spoken language under- standing without speech recognition,\" ICASSP, 2018.\n\nFrom Audio to Semantics: Approaches to end-to-end spoken language understanding. P Haghani, A Narayanan, M Bacchiani, G Chuang, N Gaur, P Moreno, R Prabhavalkar, Z Qu, A Waters, arXiv:1809.09190P. Haghani, A. Narayanan, M. Bacchiani, G. Chuang, N. Gaur, P. Moreno, R. Prabhavalkar, Z. Qu, and A. Waters, \"From Audio to Semantics: Approaches to end-to-end spoken language under- standing,\" arXiv:1809.09190, 2018.\n\nCapsule Networks for Low Resource Spoken Language Understanding. V Renkens, H Van Hamme, InterspeechV. Renkens and H. Van hamme, \"Capsule Networks for Low Re- source Spoken Language Understanding,\" Interspeech, 2018.\n\nI Goodfellow, Y Bengio, A Courville, Deep Learning. MIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n\nImageNet Classification with Deep Convolutional Neural Networks. A Krizhevsky, I Sutskever, G E Hinton, NeurIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"ImageNet Clas- sification with Deep Convolutional Neural Networks,\" NeurIPS, 2012.\n\nTowards End-to-End Speech Recognition with Recurrent Neural Networks. A Graves, N Jaitly, ICMLA. Graves and N. Jaitly, \"Towards End-to-End Speech Recogni- tion with Recurrent Neural Networks,\" ICML, 2014.\n\nDeep Speech 2: End-to-End Speech Recognition in English and Mandarin. D Amodei, R Anubhai, E Battenberg, C Case, J Casper, B Catanzaro, J Chen, M Chrzanowski, A Coates, G Diamos, ICMLD. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., \"Deep Speech 2: End-to-End Speech Recognition in En- glish and Mandarin,\" ICML, 2015.\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, NeurIPS. J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, \"How transferable are features in deep neural networks?\" NeurIPS, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLRK. Simonyan and A. Zisserman, \"Very deep convolutional net- works for large-scale image recognition,\" ICLR, 2015.\n\nSemi-supervised sequence learning. A M Dai, Q V Le, NeurIPS. A. M. Dai and Q. V. Le, \"Semi-supervised sequence learning,\" NeurIPS, 2015.\n\nUniversal language model fine-tuning for text classification. J Howard, S Ruder, ACL. J. Howard and S. Ruder, \"Universal language model fine-tuning for text classification,\" ACL, 2018.\n\nImproving Language Understanding by Generative Pre-Training. A Radford, T Salimans, OpenAITechnical reportA. Radford and T. Salimans, \"Improving Language Understand- ing by Generative Pre-Training,\" Technical report, OpenAI, 2018.\n\nA hierarchical multi-task approach for learning embeddings from semantic tasks. V Sanh, T Wolf, S Ruder, AAAIV. Sanh, T. Wolf, and S. Ruder, \"A hierarchical multi-task ap- proach for learning embeddings from semantic tasks,\" AAAI, 2019.\n\nBERT: Pretraining of deep bidirectional Transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805arXiv preprintJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre- training of deep bidirectional Transformers for language under- standing,\" arXiv preprint arXiv:1810.04805, 2018.\n\nContext-dependent pretrained deep neural networks for large vocabulary speech recognition. G Dahl, D Yu, L Deng, A Acero, IEEE Transactions on Audio, Speech, and Language Processing. 201G. Dahl, D. Yu, L. Deng, and A. Acero, \"Context-dependent pre- trained deep neural networks for large vocabulary speech recog- nition,\" IEEE Transactions on Audio, Speech, and Language Pro- cessing, vol. 20, no. 1, pp. 30-42, 2012.\n\nJ Kunze, L Kirsch, I Kurenkov, A Krug, J Johannsmeier, S Stober, Transfer Learning for Speech Recognition on a Budget. J. Kunze, L. Kirsch, I. Kurenkov, A. Krug, J. Johannsmeier, and S. Stober, \"Transfer Learning for Speech Recognition on a Bud- get,\" ACL, 2017.\n\nAcoustic Modeling for Google Home. B Li, T N Sainath, A Narayanan, J Caroselli, M Bacchiani, A Misra, I Shafran, H Sak, G Pundak, K K Chin, InterspeechB. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra, I. Shafran, H. Sak, G. Pundak, K. K. Chin et al., \"Acous- tic Modeling for Google Home,\" Interspeech, 2017.\n\nDynamic routing between capsules. S Sabour, N Frosst, G E Hinton, NeurIPS. S. Sabour, N. Frosst, and G. E. Hinton, \"Dynamic routing between capsules,\" NeurIPS, 2017.\n\nSpoken language understanding. Y.-Y Wang, L Deng, A Acero, IEEE Signal Processing Magazine. 225Y.-Y. Wang, L. Deng, and A. Acero, \"Spoken language under- standing,\" IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16-31, 2005.\n\nP Warden, arXiv:1804.03209v1Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition. P. Warden, \"Speech Commands: A Dataset for Limited- Vocabulary Speech Recognition,\" arXiv:1804.03209v1, 2018.\n\nH Soltau, H Liao, H Sak, arxiv:1610.09975Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition. H. Soltau, H. Liao, and H. Sak, \"Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition,\" arxiv:1610.09975, 2016.\n\nDirect Acoustics-to-Word Models for English Conversational Speech Recognition. K Audhkhasi, B Ramabhadran, G Saon, M Picheny, D Nahamoo, K. Audhkhasi, B. Ramabhadran, G. Saon, M. Picheny, and D. Na- hamoo, \"Direct Acoustics-to-Word Models for English Conversa- tional Speech Recognition,\" Interspeech, 2017.\n\nBuilding competitive direct acoustics-to-word models for english conversational speech recognition. K Audhkhasi, B Kingsbury, B Ramabhadran, G Saon, M Picheny, ICASSP. K. Audhkhasi, B. Kingsbury, B. Ramabhadran, G. Saon, and M. Picheny, \"Building competitive direct acoustics-to-word mod- els for english conversational speech recognition,\" ICASSP, 2018.\n\nLib-riSpeech: An ASR corpus based on public domain audio books. V Panayotov, G Chen, D Povey, S Khudanpur, ICASSP. V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \"Lib- riSpeech: An ASR corpus based on public domain audio books,\" ICASSP, 2015.\n\nS Fern\u00e1ndez, A Graves, J Schmidhuber, Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks. S. Fern\u00e1ndez, A. Graves, and J. Schmidhuber, \"Sequence La- belling in Structured Domains with Hierarchical Recurrent Neural Networks,\" IJCAI, 2007.\n\nMultitask learning. R Caruana, Machine learning. 281R. Caruana, \"Multitask learning,\" Machine learning, vol. 28, no. 1, pp. 41-75, 1997.\n\nUsing multi-task learning to improve the performance of acoustic-to-word and conventional hybrid models. T.-S Nguyen, S Stueker, A Waibel, arXiv:1902.01951T.-S. Nguyen, S. Stueker, and A. Waibel, \"Using multi-task learn- ing to improve the performance of acoustic-to-word and conven- tional hybrid models,\" arXiv:1902.01951, 2019.\n\nHierarchical multitask learning with CTC. R Sanabria, F Metze, SLT. R. Sanabria and F. Metze, \"Hierarchical multitask learning with CTC,\" SLT, 2018.\n\nMontreal Forced Aligner: Trainable text-speech alignment using Kaldi. M Mcauliffe, M Socolof, S Mihuc, M Wagner, M Sonderegger, M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Son- deregger, \"Montreal Forced Aligner: Trainable text-speech align- ment using Kaldi,\" Interspeech, 2017.\n\nSpeaker recognition from raw waveform with SincNet. M Ravanelli, Y Bengio, SLT. M. Ravanelli and Y. Bengio, \"Speaker recognition from raw wave- form with SincNet,\" SLT, 2018.\n\nInterpretable convolutional filters with SincNet. NeurIPS Workshop on Interpretability and Robustness for Audio, Speech and Language (IRASL). --, \"Interpretable convolutional filters with SincNet,\" NeurIPS Workshop on Interpretability and Robustness for Audio, Speech and Language (IRASL), 2018.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient esti- mation of word representations in vector space,\" arXiv preprint arXiv:1301.3781, 2013.\n", "annotations": {"author": "[{\"end\":141,\"start\":74},{\"end\":218,\"start\":142},{\"end\":259,\"start\":219},{\"end\":298,\"start\":260},{\"end\":386,\"start\":299}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":80},{\"end\":157,\"start\":148},{\"end\":233,\"start\":227},{\"end\":279,\"start\":268},{\"end\":312,\"start\":306}]", "author_first_name": "[{\"end\":79,\"start\":74},{\"end\":147,\"start\":142},{\"end\":226,\"start\":219},{\"end\":267,\"start\":260},{\"end\":305,\"start\":299}]", "author_affiliation": "[{\"end\":140,\"start\":110},{\"end\":217,\"start\":187},{\"end\":370,\"start\":340},{\"end\":385,\"start\":372}]", "title": "[{\"end\":71,\"start\":1},{\"end\":457,\"start\":387}]", "venue": null, "abstract": "[{\"end\":1347,\"start\":555}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1460,\"start\":1457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2110,\"start\":2107},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2113,\"start\":2110},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2116,\"start\":2113},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2200,\"start\":2197},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2203,\"start\":2200},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2206,\"start\":2203},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2209,\"start\":2206},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2212,\"start\":2209},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3016,\"start\":3012},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3020,\"start\":3016},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3024,\"start\":3020},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3028,\"start\":3024},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3032,\"start\":3028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3397,\"start\":3393},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3400,\"start\":3397},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3434,\"start\":3430},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3438,\"start\":3434},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3442,\"start\":3438},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3446,\"start\":3442},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3450,\"start\":3446},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3464,\"start\":3460},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3467,\"start\":3464},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4273,\"start\":4270},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4293,\"start\":4290},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4314,\"start\":4311},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4337,\"start\":4334},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4388,\"start\":4385},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4453,\"start\":4450},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4812,\"start\":4809},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5250,\"start\":5247},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5286,\"start\":5282},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5372,\"start\":5369},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5475,\"start\":5471},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6908,\"start\":6904},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7216,\"start\":7213},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9411,\"start\":9407},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9824,\"start\":9821},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10129,\"start\":10126},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11663,\"start\":11659},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11667,\"start\":11663},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11671,\"start\":11667},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11857,\"start\":11853},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12110,\"start\":12106},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12113,\"start\":12110},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12116,\"start\":12113},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12286,\"start\":12282},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12289,\"start\":12286},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12330,\"start\":12326},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12483,\"start\":12481},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12484,\"start\":12483},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13045,\"start\":13041},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13048,\"start\":13045},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14599,\"start\":14596},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14823,\"start\":14819},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18620,\"start\":18617},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18623,\"start\":18620},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20560,\"start\":20559}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":19321,\"start\":19300},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19449,\"start\":19322},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19784,\"start\":19450},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19990,\"start\":19785},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20291,\"start\":19991}]", "paragraph": "[{\"end\":1855,\"start\":1363},{\"end\":2422,\"start\":1857},{\"end\":2498,\"start\":2424},{\"end\":2715,\"start\":2500},{\"end\":2874,\"start\":2717},{\"end\":3879,\"start\":2876},{\"end\":3914,\"start\":3881},{\"end\":3971,\"start\":3916},{\"end\":4078,\"start\":3973},{\"end\":4182,\"start\":4080},{\"end\":5187,\"start\":4199},{\"end\":5667,\"start\":5189},{\"end\":6124,\"start\":5669},{\"end\":6212,\"start\":6136},{\"end\":6476,\"start\":6233},{\"end\":6909,\"start\":6478},{\"end\":7573,\"start\":6911},{\"end\":8048,\"start\":7575},{\"end\":8730,\"start\":8068},{\"end\":9254,\"start\":8749},{\"end\":9370,\"start\":9275},{\"end\":9632,\"start\":9372},{\"end\":9795,\"start\":9634},{\"end\":9967,\"start\":9797},{\"end\":10209,\"start\":9969},{\"end\":10588,\"start\":10211},{\"end\":11060,\"start\":10746},{\"end\":11506,\"start\":11096},{\"end\":12696,\"start\":11536},{\"end\":12933,\"start\":12715},{\"end\":12983,\"start\":12980},{\"end\":13234,\"start\":12985},{\"end\":13470,\"start\":13250},{\"end\":14106,\"start\":13508},{\"end\":14600,\"start\":14124},{\"end\":15024,\"start\":14624},{\"end\":15198,\"start\":15040},{\"end\":16255,\"start\":15215},{\"end\":16854,\"start\":16275},{\"end\":17140,\"start\":16887},{\"end\":18037,\"start\":17142},{\"end\":18812,\"start\":18039},{\"end\":19299,\"start\":18827}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12979,\"start\":12934},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13507,\"start\":13471}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8729,\"start\":8722},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16707,\"start\":16700}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1361,\"start\":1349},{\"attributes\":{\"n\":\"2.\"},\"end\":4197,\"start\":4185},{\"attributes\":{\"n\":\"3.\"},\"end\":6134,\"start\":6127},{\"attributes\":{\"n\":\"3.1.\"},\"end\":6231,\"start\":6215},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8066,\"start\":8051},{\"attributes\":{\"n\":\"3.3.\"},\"end\":8747,\"start\":8733},{\"attributes\":{\"n\":\"3.4.\"},\"end\":9273,\"start\":9257},{\"end\":10723,\"start\":10591},{\"end\":10744,\"start\":10726},{\"attributes\":{\"n\":\"4.\"},\"end\":11094,\"start\":11063},{\"attributes\":{\"n\":\"4.1.\"},\"end\":11534,\"start\":11509},{\"attributes\":{\"n\":\"4.2.\"},\"end\":12713,\"start\":12699},{\"attributes\":{\"n\":\"4.3.\"},\"end\":13248,\"start\":13237},{\"attributes\":{\"n\":\"4.4.\"},\"end\":14122,\"start\":14109},{\"attributes\":{\"n\":\"4.5.\"},\"end\":14622,\"start\":14603},{\"attributes\":{\"n\":\"5.\"},\"end\":15038,\"start\":15027},{\"attributes\":{\"n\":\"5.1.\"},\"end\":15213,\"start\":15201},{\"attributes\":{\"n\":\"5.2.\"},\"end\":16273,\"start\":16258},{\"attributes\":{\"n\":\"5.3.\"},\"end\":16885,\"start\":16857},{\"attributes\":{\"n\":\"6.\"},\"end\":18825,\"start\":18815},{\"end\":19333,\"start\":19323},{\"end\":19795,\"start\":19786},{\"end\":20001,\"start\":19992}]", "table": "[{\"end\":19784,\"start\":19522},{\"end\":19990,\"start\":19893},{\"end\":20291,\"start\":20119}]", "figure_caption": "[{\"end\":19321,\"start\":19302},{\"end\":19449,\"start\":19335},{\"end\":19522,\"start\":19452},{\"end\":19893,\"start\":19797},{\"end\":20119,\"start\":20003}]", "figure_ref": "[{\"end\":2368,\"start\":2360},{\"end\":10770,\"start\":10762},{\"end\":11145,\"start\":11139},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15780,\"start\":15773},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16410,\"start\":16402}]", "bib_author_first_name": "[{\"end\":21105,\"start\":21104},{\"end\":21112,\"start\":21111},{\"end\":21114,\"start\":21113},{\"end\":21335,\"start\":21334},{\"end\":21345,\"start\":21344},{\"end\":21356,\"start\":21355},{\"end\":21363,\"start\":21362},{\"end\":21373,\"start\":21372},{\"end\":21381,\"start\":21380},{\"end\":21395,\"start\":21394},{\"end\":21781,\"start\":21780},{\"end\":21791,\"start\":21790},{\"end\":21802,\"start\":21801},{\"end\":21810,\"start\":21809},{\"end\":21812,\"start\":21811},{\"end\":21819,\"start\":21818},{\"end\":22068,\"start\":22067},{\"end\":22070,\"start\":22069},{\"end\":22079,\"start\":22078},{\"end\":22091,\"start\":22090},{\"end\":22093,\"start\":22092},{\"end\":22368,\"start\":22367},{\"end\":22376,\"start\":22375},{\"end\":22385,\"start\":22384},{\"end\":22402,\"start\":22401},{\"end\":22641,\"start\":22640},{\"end\":22652,\"start\":22651},{\"end\":22660,\"start\":22659},{\"end\":22670,\"start\":22669},{\"end\":22679,\"start\":22678},{\"end\":22686,\"start\":22685},{\"end\":22897,\"start\":22893},{\"end\":22905,\"start\":22904},{\"end\":22914,\"start\":22913},{\"end\":23133,\"start\":23132},{\"end\":23144,\"start\":23143},{\"end\":23157,\"start\":23156},{\"end\":23170,\"start\":23169},{\"end\":23180,\"start\":23179},{\"end\":23188,\"start\":23187},{\"end\":23198,\"start\":23197},{\"end\":23214,\"start\":23213},{\"end\":23220,\"start\":23219},{\"end\":23531,\"start\":23530},{\"end\":23542,\"start\":23541},{\"end\":23684,\"start\":23683},{\"end\":23698,\"start\":23697},{\"end\":23708,\"start\":23707},{\"end\":23879,\"start\":23878},{\"end\":23888,\"start\":23887},{\"end\":23898,\"start\":23897},{\"end\":23908,\"start\":23907},{\"end\":24184,\"start\":24183},{\"end\":24198,\"start\":24197},{\"end\":24211,\"start\":24210},{\"end\":24213,\"start\":24212},{\"end\":24434,\"start\":24433},{\"end\":24444,\"start\":24443},{\"end\":24640,\"start\":24639},{\"end\":24650,\"start\":24649},{\"end\":24661,\"start\":24660},{\"end\":24675,\"start\":24674},{\"end\":24683,\"start\":24682},{\"end\":24693,\"start\":24692},{\"end\":24706,\"start\":24705},{\"end\":24714,\"start\":24713},{\"end\":24729,\"start\":24728},{\"end\":24739,\"start\":24738},{\"end\":25022,\"start\":25021},{\"end\":25034,\"start\":25033},{\"end\":25043,\"start\":25042},{\"end\":25053,\"start\":25052},{\"end\":25262,\"start\":25261},{\"end\":25274,\"start\":25273},{\"end\":25441,\"start\":25440},{\"end\":25443,\"start\":25442},{\"end\":25450,\"start\":25449},{\"end\":25452,\"start\":25451},{\"end\":25606,\"start\":25605},{\"end\":25616,\"start\":25615},{\"end\":25791,\"start\":25790},{\"end\":25802,\"start\":25801},{\"end\":26042,\"start\":26041},{\"end\":26050,\"start\":26049},{\"end\":26058,\"start\":26057},{\"end\":26281,\"start\":26280},{\"end\":26294,\"start\":26290},{\"end\":26303,\"start\":26302},{\"end\":26310,\"start\":26309},{\"end\":26621,\"start\":26620},{\"end\":26629,\"start\":26628},{\"end\":26635,\"start\":26634},{\"end\":26643,\"start\":26642},{\"end\":26949,\"start\":26948},{\"end\":26958,\"start\":26957},{\"end\":26968,\"start\":26967},{\"end\":26980,\"start\":26979},{\"end\":26988,\"start\":26987},{\"end\":27004,\"start\":27003},{\"end\":27248,\"start\":27247},{\"end\":27254,\"start\":27253},{\"end\":27256,\"start\":27255},{\"end\":27267,\"start\":27266},{\"end\":27280,\"start\":27279},{\"end\":27293,\"start\":27292},{\"end\":27306,\"start\":27305},{\"end\":27315,\"start\":27314},{\"end\":27326,\"start\":27325},{\"end\":27333,\"start\":27332},{\"end\":27343,\"start\":27342},{\"end\":27345,\"start\":27344},{\"end\":27581,\"start\":27580},{\"end\":27591,\"start\":27590},{\"end\":27601,\"start\":27600},{\"end\":27603,\"start\":27602},{\"end\":27748,\"start\":27744},{\"end\":27756,\"start\":27755},{\"end\":27764,\"start\":27763},{\"end\":27946,\"start\":27945},{\"end\":28155,\"start\":28154},{\"end\":28165,\"start\":28164},{\"end\":28173,\"start\":28172},{\"end\":28524,\"start\":28523},{\"end\":28537,\"start\":28536},{\"end\":28552,\"start\":28551},{\"end\":28560,\"start\":28559},{\"end\":28571,\"start\":28570},{\"end\":28854,\"start\":28853},{\"end\":28867,\"start\":28866},{\"end\":28880,\"start\":28879},{\"end\":28895,\"start\":28894},{\"end\":28903,\"start\":28902},{\"end\":29174,\"start\":29173},{\"end\":29187,\"start\":29186},{\"end\":29195,\"start\":29194},{\"end\":29204,\"start\":29203},{\"end\":29358,\"start\":29357},{\"end\":29371,\"start\":29370},{\"end\":29381,\"start\":29380},{\"end\":29651,\"start\":29650},{\"end\":29877,\"start\":29873},{\"end\":29887,\"start\":29886},{\"end\":29898,\"start\":29897},{\"end\":30143,\"start\":30142},{\"end\":30155,\"start\":30154},{\"end\":30321,\"start\":30320},{\"end\":30334,\"start\":30333},{\"end\":30345,\"start\":30344},{\"end\":30354,\"start\":30353},{\"end\":30364,\"start\":30363},{\"end\":30594,\"start\":30593},{\"end\":30607,\"start\":30606},{\"end\":31077,\"start\":31076},{\"end\":31088,\"start\":31087},{\"end\":31096,\"start\":31095},{\"end\":31107,\"start\":31106}]", "bib_author_last_name": "[{\"end\":21109,\"start\":21106},{\"end\":21119,\"start\":21115},{\"end\":21342,\"start\":21336},{\"end\":21353,\"start\":21346},{\"end\":21360,\"start\":21357},{\"end\":21370,\"start\":21364},{\"end\":21378,\"start\":21374},{\"end\":21392,\"start\":21382},{\"end\":21398,\"start\":21396},{\"end\":21788,\"start\":21782},{\"end\":21799,\"start\":21792},{\"end\":21807,\"start\":21803},{\"end\":21816,\"start\":21813},{\"end\":21826,\"start\":21820},{\"end\":22076,\"start\":22071},{\"end\":22088,\"start\":22080},{\"end\":22100,\"start\":22094},{\"end\":22373,\"start\":22369},{\"end\":22382,\"start\":22377},{\"end\":22399,\"start\":22386},{\"end\":22408,\"start\":22403},{\"end\":22649,\"start\":22642},{\"end\":22657,\"start\":22653},{\"end\":22667,\"start\":22661},{\"end\":22676,\"start\":22671},{\"end\":22683,\"start\":22680},{\"end\":22693,\"start\":22687},{\"end\":22902,\"start\":22898},{\"end\":22911,\"start\":22906},{\"end\":22924,\"start\":22915},{\"end\":23141,\"start\":23134},{\"end\":23154,\"start\":23145},{\"end\":23167,\"start\":23158},{\"end\":23177,\"start\":23171},{\"end\":23185,\"start\":23181},{\"end\":23195,\"start\":23189},{\"end\":23211,\"start\":23199},{\"end\":23217,\"start\":23215},{\"end\":23227,\"start\":23221},{\"end\":23539,\"start\":23532},{\"end\":23552,\"start\":23543},{\"end\":23695,\"start\":23685},{\"end\":23705,\"start\":23699},{\"end\":23718,\"start\":23709},{\"end\":23885,\"start\":23880},{\"end\":23895,\"start\":23889},{\"end\":23905,\"start\":23899},{\"end\":23916,\"start\":23909},{\"end\":24195,\"start\":24185},{\"end\":24208,\"start\":24199},{\"end\":24220,\"start\":24214},{\"end\":24441,\"start\":24435},{\"end\":24451,\"start\":24445},{\"end\":24647,\"start\":24641},{\"end\":24658,\"start\":24651},{\"end\":24672,\"start\":24662},{\"end\":24680,\"start\":24676},{\"end\":24690,\"start\":24684},{\"end\":24703,\"start\":24694},{\"end\":24711,\"start\":24707},{\"end\":24726,\"start\":24715},{\"end\":24736,\"start\":24730},{\"end\":24746,\"start\":24740},{\"end\":25031,\"start\":25023},{\"end\":25040,\"start\":25035},{\"end\":25050,\"start\":25044},{\"end\":25060,\"start\":25054},{\"end\":25271,\"start\":25263},{\"end\":25284,\"start\":25275},{\"end\":25447,\"start\":25444},{\"end\":25455,\"start\":25453},{\"end\":25613,\"start\":25607},{\"end\":25622,\"start\":25617},{\"end\":25799,\"start\":25792},{\"end\":25811,\"start\":25803},{\"end\":26047,\"start\":26043},{\"end\":26055,\"start\":26051},{\"end\":26064,\"start\":26059},{\"end\":26288,\"start\":26282},{\"end\":26300,\"start\":26295},{\"end\":26307,\"start\":26304},{\"end\":26320,\"start\":26311},{\"end\":26626,\"start\":26622},{\"end\":26632,\"start\":26630},{\"end\":26640,\"start\":26636},{\"end\":26649,\"start\":26644},{\"end\":26955,\"start\":26950},{\"end\":26965,\"start\":26959},{\"end\":26977,\"start\":26969},{\"end\":26985,\"start\":26981},{\"end\":27001,\"start\":26989},{\"end\":27011,\"start\":27005},{\"end\":27251,\"start\":27249},{\"end\":27264,\"start\":27257},{\"end\":27277,\"start\":27268},{\"end\":27290,\"start\":27281},{\"end\":27303,\"start\":27294},{\"end\":27312,\"start\":27307},{\"end\":27323,\"start\":27316},{\"end\":27330,\"start\":27327},{\"end\":27340,\"start\":27334},{\"end\":27350,\"start\":27346},{\"end\":27588,\"start\":27582},{\"end\":27598,\"start\":27592},{\"end\":27610,\"start\":27604},{\"end\":27753,\"start\":27749},{\"end\":27761,\"start\":27757},{\"end\":27770,\"start\":27765},{\"end\":27953,\"start\":27947},{\"end\":28162,\"start\":28156},{\"end\":28170,\"start\":28166},{\"end\":28177,\"start\":28174},{\"end\":28534,\"start\":28525},{\"end\":28549,\"start\":28538},{\"end\":28557,\"start\":28553},{\"end\":28568,\"start\":28561},{\"end\":28579,\"start\":28572},{\"end\":28864,\"start\":28855},{\"end\":28877,\"start\":28868},{\"end\":28892,\"start\":28881},{\"end\":28900,\"start\":28896},{\"end\":28911,\"start\":28904},{\"end\":29184,\"start\":29175},{\"end\":29192,\"start\":29188},{\"end\":29201,\"start\":29196},{\"end\":29214,\"start\":29205},{\"end\":29368,\"start\":29359},{\"end\":29378,\"start\":29372},{\"end\":29393,\"start\":29382},{\"end\":29659,\"start\":29652},{\"end\":29884,\"start\":29878},{\"end\":29895,\"start\":29888},{\"end\":29905,\"start\":29899},{\"end\":30152,\"start\":30144},{\"end\":30161,\"start\":30156},{\"end\":30331,\"start\":30322},{\"end\":30342,\"start\":30335},{\"end\":30351,\"start\":30346},{\"end\":30361,\"start\":30355},{\"end\":30376,\"start\":30365},{\"end\":30604,\"start\":30595},{\"end\":30614,\"start\":30608},{\"end\":31085,\"start\":31078},{\"end\":31093,\"start\":31089},{\"end\":31104,\"start\":31097},{\"end\":31112,\"start\":31108}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":21249,\"start\":21016},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1317136},\"end\":21667,\"start\":21251},{\"attributes\":{\"doi\":\"arXiv:1805.10190v3\",\"id\":\"b2\"},\"end\":22044,\"start\":21669},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3121738},\"end\":22253,\"start\":22046},{\"attributes\":{\"id\":\"b4\"},\"end\":22588,\"start\":22255},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3525280},\"end\":22833,\"start\":22590},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52284712},\"end\":23049,\"start\":22835},{\"attributes\":{\"doi\":\"arXiv:1809.09190\",\"id\":\"b7\"},\"end\":23463,\"start\":23051},{\"attributes\":{\"id\":\"b8\"},\"end\":23681,\"start\":23465},{\"attributes\":{\"id\":\"b9\"},\"end\":23819,\"start\":23683},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14542261},\"end\":24116,\"start\":23821},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":195908774},\"end\":24361,\"start\":24118},{\"attributes\":{\"id\":\"b12\"},\"end\":24567,\"start\":24363},{\"attributes\":{\"id\":\"b13\"},\"end\":24963,\"start\":24569},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":362467},\"end\":25191,\"start\":24965},{\"attributes\":{\"id\":\"b15\"},\"end\":25403,\"start\":25193},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7138078},\"end\":25541,\"start\":25405},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":40100965},\"end\":25727,\"start\":25543},{\"attributes\":{\"id\":\"b18\"},\"end\":25959,\"start\":25729},{\"attributes\":{\"id\":\"b19\"},\"end\":26197,\"start\":25961},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b20\"},\"end\":26527,\"start\":26199},{\"attributes\":{\"id\":\"b21\"},\"end\":26946,\"start\":26529},{\"attributes\":{\"id\":\"b22\"},\"end\":27210,\"start\":26948},{\"attributes\":{\"id\":\"b23\"},\"end\":27544,\"start\":27212},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3603485},\"end\":27711,\"start\":27546},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10232742},\"end\":27943,\"start\":27713},{\"attributes\":{\"doi\":\"arXiv:1804.03209v1\",\"id\":\"b26\"},\"end\":28152,\"start\":27945},{\"attributes\":{\"doi\":\"arxiv:1610.09975\",\"id\":\"b27\"},\"end\":28442,\"start\":28154},{\"attributes\":{\"id\":\"b28\"},\"end\":28751,\"start\":28444},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1582664},\"end\":29107,\"start\":28753},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2191379},\"end\":29355,\"start\":29109},{\"attributes\":{\"id\":\"b31\"},\"end\":29628,\"start\":29357},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":45998148},\"end\":29766,\"start\":29630},{\"attributes\":{\"doi\":\"arXiv:1902.01951\",\"id\":\"b33\"},\"end\":30098,\"start\":29768},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":61807503},\"end\":30248,\"start\":30100},{\"attributes\":{\"id\":\"b35\"},\"end\":30539,\"start\":30250},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":51892530},\"end\":30715,\"start\":30541},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":53595907},\"end\":31012,\"start\":30717},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b38\"},\"end\":31292,\"start\":31014}]", "bib_title": "[{\"end\":21332,\"start\":21251},{\"end\":22065,\"start\":22046},{\"end\":22638,\"start\":22590},{\"end\":22891,\"start\":22835},{\"end\":23876,\"start\":23821},{\"end\":24181,\"start\":24118},{\"end\":25019,\"start\":24965},{\"end\":25438,\"start\":25405},{\"end\":25603,\"start\":25543},{\"end\":26618,\"start\":26529},{\"end\":27578,\"start\":27546},{\"end\":27742,\"start\":27713},{\"end\":28851,\"start\":28753},{\"end\":29171,\"start\":29109},{\"end\":29648,\"start\":29630},{\"end\":30140,\"start\":30100},{\"end\":30591,\"start\":30541},{\"end\":30765,\"start\":30717}]", "bib_author": "[{\"end\":21111,\"start\":21104},{\"end\":21121,\"start\":21111},{\"end\":21344,\"start\":21334},{\"end\":21355,\"start\":21344},{\"end\":21362,\"start\":21355},{\"end\":21372,\"start\":21362},{\"end\":21380,\"start\":21372},{\"end\":21394,\"start\":21380},{\"end\":21400,\"start\":21394},{\"end\":21790,\"start\":21780},{\"end\":21801,\"start\":21790},{\"end\":21809,\"start\":21801},{\"end\":21818,\"start\":21809},{\"end\":21828,\"start\":21818},{\"end\":22078,\"start\":22067},{\"end\":22090,\"start\":22078},{\"end\":22102,\"start\":22090},{\"end\":22375,\"start\":22367},{\"end\":22384,\"start\":22375},{\"end\":22401,\"start\":22384},{\"end\":22410,\"start\":22401},{\"end\":22651,\"start\":22640},{\"end\":22659,\"start\":22651},{\"end\":22669,\"start\":22659},{\"end\":22678,\"start\":22669},{\"end\":22685,\"start\":22678},{\"end\":22695,\"start\":22685},{\"end\":22904,\"start\":22893},{\"end\":22913,\"start\":22904},{\"end\":22926,\"start\":22913},{\"end\":23143,\"start\":23132},{\"end\":23156,\"start\":23143},{\"end\":23169,\"start\":23156},{\"end\":23179,\"start\":23169},{\"end\":23187,\"start\":23179},{\"end\":23197,\"start\":23187},{\"end\":23213,\"start\":23197},{\"end\":23219,\"start\":23213},{\"end\":23229,\"start\":23219},{\"end\":23541,\"start\":23530},{\"end\":23554,\"start\":23541},{\"end\":23697,\"start\":23683},{\"end\":23707,\"start\":23697},{\"end\":23720,\"start\":23707},{\"end\":23887,\"start\":23878},{\"end\":23897,\"start\":23887},{\"end\":23907,\"start\":23897},{\"end\":23918,\"start\":23907},{\"end\":24197,\"start\":24183},{\"end\":24210,\"start\":24197},{\"end\":24222,\"start\":24210},{\"end\":24443,\"start\":24433},{\"end\":24453,\"start\":24443},{\"end\":24649,\"start\":24639},{\"end\":24660,\"start\":24649},{\"end\":24674,\"start\":24660},{\"end\":24682,\"start\":24674},{\"end\":24692,\"start\":24682},{\"end\":24705,\"start\":24692},{\"end\":24713,\"start\":24705},{\"end\":24728,\"start\":24713},{\"end\":24738,\"start\":24728},{\"end\":24748,\"start\":24738},{\"end\":25033,\"start\":25021},{\"end\":25042,\"start\":25033},{\"end\":25052,\"start\":25042},{\"end\":25062,\"start\":25052},{\"end\":25273,\"start\":25261},{\"end\":25286,\"start\":25273},{\"end\":25449,\"start\":25440},{\"end\":25457,\"start\":25449},{\"end\":25615,\"start\":25605},{\"end\":25624,\"start\":25615},{\"end\":25801,\"start\":25790},{\"end\":25813,\"start\":25801},{\"end\":26049,\"start\":26041},{\"end\":26057,\"start\":26049},{\"end\":26066,\"start\":26057},{\"end\":26290,\"start\":26280},{\"end\":26302,\"start\":26290},{\"end\":26309,\"start\":26302},{\"end\":26322,\"start\":26309},{\"end\":26628,\"start\":26620},{\"end\":26634,\"start\":26628},{\"end\":26642,\"start\":26634},{\"end\":26651,\"start\":26642},{\"end\":26957,\"start\":26948},{\"end\":26967,\"start\":26957},{\"end\":26979,\"start\":26967},{\"end\":26987,\"start\":26979},{\"end\":27003,\"start\":26987},{\"end\":27013,\"start\":27003},{\"end\":27253,\"start\":27247},{\"end\":27266,\"start\":27253},{\"end\":27279,\"start\":27266},{\"end\":27292,\"start\":27279},{\"end\":27305,\"start\":27292},{\"end\":27314,\"start\":27305},{\"end\":27325,\"start\":27314},{\"end\":27332,\"start\":27325},{\"end\":27342,\"start\":27332},{\"end\":27352,\"start\":27342},{\"end\":27590,\"start\":27580},{\"end\":27600,\"start\":27590},{\"end\":27612,\"start\":27600},{\"end\":27755,\"start\":27744},{\"end\":27763,\"start\":27755},{\"end\":27772,\"start\":27763},{\"end\":27955,\"start\":27945},{\"end\":28164,\"start\":28154},{\"end\":28172,\"start\":28164},{\"end\":28179,\"start\":28172},{\"end\":28536,\"start\":28523},{\"end\":28551,\"start\":28536},{\"end\":28559,\"start\":28551},{\"end\":28570,\"start\":28559},{\"end\":28581,\"start\":28570},{\"end\":28866,\"start\":28853},{\"end\":28879,\"start\":28866},{\"end\":28894,\"start\":28879},{\"end\":28902,\"start\":28894},{\"end\":28913,\"start\":28902},{\"end\":29186,\"start\":29173},{\"end\":29194,\"start\":29186},{\"end\":29203,\"start\":29194},{\"end\":29216,\"start\":29203},{\"end\":29370,\"start\":29357},{\"end\":29380,\"start\":29370},{\"end\":29395,\"start\":29380},{\"end\":29661,\"start\":29650},{\"end\":29886,\"start\":29873},{\"end\":29897,\"start\":29886},{\"end\":29907,\"start\":29897},{\"end\":30154,\"start\":30142},{\"end\":30163,\"start\":30154},{\"end\":30333,\"start\":30320},{\"end\":30344,\"start\":30333},{\"end\":30353,\"start\":30344},{\"end\":30363,\"start\":30353},{\"end\":30378,\"start\":30363},{\"end\":30606,\"start\":30593},{\"end\":30616,\"start\":30606},{\"end\":31087,\"start\":31076},{\"end\":31095,\"start\":31087},{\"end\":31106,\"start\":31095},{\"end\":31114,\"start\":31106}]", "bib_venue": "[{\"end\":21102,\"start\":21016},{\"end\":21431,\"start\":21400},{\"end\":21778,\"start\":21669},{\"end\":22122,\"start\":22102},{\"end\":22365,\"start\":22255},{\"end\":22701,\"start\":22695},{\"end\":22932,\"start\":22926},{\"end\":23130,\"start\":23051},{\"end\":23528,\"start\":23465},{\"end\":23733,\"start\":23720},{\"end\":23941,\"start\":23918},{\"end\":24229,\"start\":24222},{\"end\":24431,\"start\":24363},{\"end\":24637,\"start\":24569},{\"end\":25069,\"start\":25062},{\"end\":25259,\"start\":25193},{\"end\":25464,\"start\":25457},{\"end\":25627,\"start\":25624},{\"end\":25788,\"start\":25729},{\"end\":26039,\"start\":25961},{\"end\":26278,\"start\":26199},{\"end\":26710,\"start\":26651},{\"end\":27065,\"start\":27013},{\"end\":27245,\"start\":27212},{\"end\":27619,\"start\":27612},{\"end\":27803,\"start\":27772},{\"end\":28041,\"start\":27973},{\"end\":28288,\"start\":28195},{\"end\":28521,\"start\":28444},{\"end\":28919,\"start\":28913},{\"end\":29222,\"start\":29216},{\"end\":29479,\"start\":29395},{\"end\":29677,\"start\":29661},{\"end\":29871,\"start\":29768},{\"end\":30166,\"start\":30163},{\"end\":30318,\"start\":30250},{\"end\":30619,\"start\":30616},{\"end\":30857,\"start\":30767},{\"end\":31074,\"start\":31014}]"}}}, "year": 2023, "month": 12, "day": 17}