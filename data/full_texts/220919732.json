{"id": 220919732, "updated": "2023-10-06 12:29:35.389", "metadata": {"title": "Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution", "authors": "[{\"first\":\"Haotian\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Zhijian\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Shengyu\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Yujun\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Ji\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Hanrui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Song\",\"last\":\"Han\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Self-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3%, ranking 1st on the competitive SemanticKITTI leaderboard. It also achieves 8x computation reduction and 3x measured speedup over MinkowskiNet with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.16100", "mag": "3109154950", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/TangLZLLWH20", "doi": "10.1007/978-3-030-58604-1_41"}}, "content": {"source": {"pdf_hash": "f7ebd7792b7db46131cd6272205f1adebaaa1a89", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.16100", "status": "GREEN"}}, "grobid": {"id": "3d9bf65d7c9ca3eba3b571dac809791f718893e6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f7ebd7792b7db46131cd6272205f1adebaaa1a89.txt", "contents": "\nSearching Efficient 3D Architectures with Sparse Point-Voxel Convolution\n\n\nHaotian Tang \nMassachusetts Institute of Technology\n\n\nZhijian Liu \nMassachusetts Institute of Technology\n\n\nShengyu Zhao \nMassachusetts Institute of Technology\n\n\nIIIS\nTsinghua University\n\n\nYujun Lin \nMassachusetts Institute of Technology\n\n\nJi Lin \nMassachusetts Institute of Technology\n\n\nHanrui Wang \nMassachusetts Institute of Technology\n\n\nSong Han \nMassachusetts Institute of Technology\n\n\nSearching Efficient 3D Architectures with Sparse Point-Voxel Convolution\n\nSelf-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3%, ranking 1 st on the competitive SemanticKITTI leaderboard . It also achieves 8\u00d7 computation reduction and 3\u00d7 measured speedup over MinkowskiNet with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI. * indicates equal contributions; order determined by a coin toss. https://competitions.codalab.org/competitions/20331#results\n\nIntroduction\n\n3D deep learning has received increased attention thanks to its wide applications: e.g., it has been used in LiDAR perception that serves as the eyes of autonomous driving systems to understand the semantics of outdoor scenes. As the safety of the passenger is the top priority of the self-driving cars, 3D perception models are required to achieve high accuracy and low latency at the same time. However, the hardware resources on the self-driving cars are tightly constrained by the form factor (since we do not want a whole trunk of workstations) and heat dissipation. Thus, it is crucial to design efficient and effective 3D neural network models with limited computation resources (e.g., memory).\n\nResearchers have primarily exploited two 3D data representations: point cloud and rasterized voxel grids. As analyzed in Liu et al . [26], point-based methods [33, (a) Large 3D Scene (b) Low Resolution (0.8m) Fig. 1. Small instances (e.g., pedestrians and cyclists) are hard to be recognized at a low resolution (due to the coarse voxelization or the aggressive downsampling). 36,22] waste up to 90% of their runtime on structuring the irregular data, not on the actual feature extraction. On the other hand, voxel-based methods usually suffer from the coarse voxelization: i.e., the resolution of dense voxels [29,26] is strictly constrained by the memory; the sparse voxels [11,7] require aggressive downsampling to achieve larger receptive field, leading to low resolution at deeper layers. With low resolution (see Figure 1), multiple points or even multiple small objects might be merged into one grid and become indistinguishable. In this case, small instances (e.g., pedestrians and cyclists) are at a disadvantage compared to large objects (e.g., cars). Therefore, the effectiveness of previous 3D modules is discounted when the hardware resource is limited and the resolution is low.\n\nTo tackle these problems, we propose a novel 3D module, Sparse Point-Voxel Convolution (SPVConv) that introduces a low-cost high-resolution point-based branch to the vanilla Sparse Convolution, which helps to capture the fine details. On top of SPVConv, we further present 3D Neural Architecture Search (3D-NAS) to search an efficient 3D model. We incorporate fine-grained channel numbers into the search space to increase the diversity and introduce the progressive depth shrinking to stablize the training. Experimental results validate that our model is fast and accurate: it outpeforms MinkowskiNet by 3.3% in mIoU with lower latency. It also achieves 8\u00d7 computation reduction and 3\u00d7 measured speedup over MinkowskiNet, while offering higher accuracy. We further transfer our method to KITTI for 3D object detection, and it achieves consistent improvements over previous one-stage detection baseline.\n\nThe contribution of this paper has three aspects:\n\n1. We design a lightweight 3D module, SPVConv, that boosts the accuracy on small objects, which used to be challenging under limited hardware resource. 2. We introduce the first 3D architecture search framework, 3D-NAS, that offers the best 3D neural network model given a specific resource constraint. 3. Our method outperforms all previous methods with a large margin and ranks 1 st on the competitive SemanticKITTI leaderboard. It can also be transferred to the object detection task and achieves consistent improvements.\n\n\nRelated Work\n\n\n3D Perception Models\n\nIncreased attention has been paid to 3D deep learning, which is important for LiDAR perception in autonomous driving. Early research [5,29,35,57,67] relied on the volumetric representation and vanilla 3D convolution to process the 3D data. Due to the sparse nature of 3D representation, the dense volumetric representation is inherently inefficient, and it also inevitably introduces information loss. Therefore, researchers have proposed to directly learn on 3D point cloud representation using a symmetric function [33]. To improve the neighborhood modeling capability, researchers have defined point-based convolutions on either geometric [22,28,36,45,48,49,59,61] or semantic [56] neighborhood. There are also 3D models tailored for specific tasks such as detection [31, 32, 34, 40-42, 62, 64] and instance segmentation [13,17,18,63] built upon these modules.\n\nRecently, a few researchers started to pay attention to the efficiency aspect of 3D deep learning. Riegler et al . [38], Wang et al . [53,54] and Lei et al . [20] proposed to reduce the memory footprint of volumetric representation using octrees where areas with lower density occupy fewer voxel grids. Liu et al . [26] analyzed the bottlenecks of point-based and voxel-based methods, and proposed Point-Voxel Convolution. Graham et al . [11] and Choy et al . [7] proposed Sparse Convolution to accelerate the volumetric convolution by keeping the activation sparse and skipping the computations in the inactive regions.\n\n\nNeural Architecture Search\n\nDesigning neural networks is highly challenging and time-consuming. To alleviate the burden of manually designing neural networks [15,39,27,65], researchers have introduced neural architecture search (NAS) to automatically design the neural network with high accuracy using reinforcement learning [69,70] and evolutionary search [23]. A new wave of research started to design efficient models with neural architecture search [46,58,47], which is very important for the mobile deployment. However, conventional frameworks require high computation cost (typically 10 4 GPU hours) and considerable carbon footprint [44]. To this end, researchers have proposed different techniques to reduce the computation cost (to around 10 2 GPU hours), such as differentiable architecture search [24], path-level binarization [4], single-path one-shot sampling [12,6], and weight sharing [43,2,21,50]. Besides, neural architecture search has also been used in compressing and accelerating neural networks, such as pruning [14,25,3] and quantization [51,12,52,55]. Most of these methods are tailored for 2D visual recognition, which has many well-defined search spaces [37]. To the best of our knowledge, neural architecture search for 3D deep learning is under-studied. Previous research on VNAS [68] only focus on 3D medical image segmentation, which is not suitable for general-purpose 3D deep learning.\n\n\nSPVConv: Designing Effective 3D Modules\n\nWe first revisit two recent 3D modules: Point-Voxel Convolution [26] and Sparse Convolution [7] and analyze their bottlenecks. We observe that both of them suffer  Table 1. Point-Voxel Convolution [26] is not suitable for large 3D scenes. If processing with sliding windows, the large latency is not affordable for real-time applications. If taking the whole scene, the resolution is too coarse to capture useful information.\n\nfrom information loss (caused by coarse voxelization or aggressive downsampling) when the memory is constrained. To this end, we introduce Sparse Point-Voxel Convolution (SPVConv), to effectively process the large 3D scene (as in Figure 2).\n\n\nPoint-Voxel Convolution: Coarse Voxelization\n\nLiu et al . [26] proposed Point-Voxel Convolution that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular data access and improve the locality. Specifically, its point-based branch transforms each point individually, and its voxel-based branch convolves over the voxelized input from the point-based branch. PVCNN (which is built upon Point-Voxel Convolution) can afford the resolution of at most 128 in its voxel-based branch on a single GPU (with 12 GB of memory). Given a large outdoor scene (with size of 100m\u00d7100m\u00d710m), each voxel grid will correspond to a fairly large area (with size of 0.8m\u00d70.8m\u00d70.1m). In this case, the small instances (e.g., pedestrians) will only occupy a few voxel grids (see Figure 1). From such few points, PVCNN can hardly learn any useful information from the voxel-based branch, leading to a relatively low performance (see Table 1). Alternatively, we can process the large 3D scenes piece by piece so that each sliding window is of smaller scale. In order to preserve the fine-grained information, we found empirically that the voxel size needs to be at least lower than 0.05m. In this case, we have to run PVCNN once for each of the 244 sliding windows, which will take 35 seconds to process a single scene. Such a large latency is not affordable for most real-time applications (e.g., autonomous driving).\n\n\nSparse Convolution: Aggressive Downsampling\n\nVolumetric convolution has always been considered inefficient and prohibitive to be scaled up. Lately, researchers proposed Sparse Convolution [11,7] that skips the non-activated regions to significantly reduce the memory consumption. More specifically, it first finds all active synapses (denoted as kernel map) between the input and output points; it then performs the convolution based on this kernel map. In order to keep the activation sparse, it only considers these output points that also belong to the input point cloud.\n\nAs such, Sparse Convolution can afford a much higher resolution than the vanilla volumetric convolution. However, the network cannot be very deep due to the limited computation resource. As a result, the network has to downsample very aggressively in order to achieve a sufficiently large receptive field, which is  very lossy. For instance, the state-of-the-art MinkowskiNet [7] gradually applies four downsampling layers to the input point cloud, after which, the voxel size will become 0.05 \u00d7 2 4 = 0.8m. Similar to Point-Voxel Convolution, this resolution is too coarse to capture the small instances (see Figure 4).\n\n\nSolution: Sparse Point-Voxel Convolution\n\nIn order to overcome the limitations of both modules, we present Sparse Point-Voxel Convolution (SPVConv) in Figure 2. The point-based branch always keeps the high-resolution representation. The sparse voxel-based branch applies Sparse Convolution to efficiently model over different receptive field size. Two branches communicate at negligible cost through sparse voxelization and devoxelization.\n\nData Representation. Our Sparse Point-Voxel Convolution operates on:\n-sparse voxelized tensor S = {(p s m , f s m ), v}, where p s m = (x s m , y s m , z s m )\nis the 3D coordinate and f s m is the feature vector of the m th nonzero grid, and v is the voxel size for one grid in the current layer;\n-point cloud tensor T = {(p t k , f t k )}, where p k = (x k , y k , z k ) is the 3D coordi- nate and f k is feature vector of k th point.\nSparse Voxelization. In the upper sparse voxel-based branch, we first transform the high-resolution point cloud tensor T to a sparse tensor S:\np t k = (x t k ,\u0177 t k ,\u1e91 t k ) = (floor(x t k /v), floor(y t k /v), floor(z t k /v)),(1)f s m = 1 N m n k=1 I[x t k = x s m ,\u0177 t k = y s m ,\u1e91 t k = z s m ] \u00b7 f t k ,(2)\nwhere I[\u00b7] is the binary indicator of whetherp t k belongs to the voxel grid p s m , and N m is the normalization factor (i.e., the number of points that fall into the m th nonzero voxel grid). Such formulation, however, requires O(mn) complexity where m = |S| and n = |T |. With typical values of m, n at the order of 10 5 , the naive implementation is impractical for real-time applications.  To this end, we propose to use the GPU hash table to accelerate the sparse voxelization and devoxelization. Specifically, we first construct a hash table for all activated points in the sparse voxelized representation (where the key is the 3D coordinates, and the value is the index in the sparse voxelized tensor), which can be finished in O(n) time. After that, we iterate over all points, and for each point, we use its coordinate as the key to query the corresponding index in the sparse voxelized representation. As the lookup over the hash table requires O(1) time in the worst case [30], this query step will in total take O(m) time. Therefore, the total time of coordinate indexing will be reduced from O(mn) to O(m + n).\n\nFeature Aggregation. We then perform the neighborhood feature aggregation on the sparse voxelized tensor using a sequence of residual Sparse Convolution blocks [7]. We parallelize the kernel map operation in Sparse Convolution on GPU with the same hash table implementation as in sparse voxelization, which offers 1.3\u00d7 speedup over Choy et al .'s latest implementation [7]. Both our method and the baseline have been upgraded to this accelerated implementation.\n\nSparse Devoxelization. With the neighborhood features (represented in the sparse tensor), we hope to transform it back to the point-based representation so that information from both branches can be fused together. Similar to Liu et al . [26], we choose to interpolate each point's feature with its 8 neighbor voxel grids using trilinear interpolation instead of the naive nearest interpolation.\n\nPoint Transformation and Feature Fusion. In the lower point-based branch, we directly apply MLP on each point to extract individual point features. We then fuse the outputs of two branches with an addition to combine the complementary information provided. Compared with the vanilla Sparse Convolution, MLP layers only cost little computation overhead (4% in terms of #MACs) but introduce important fine details into the information flow.\n\n\n3D-NAS: Searching Efficient 3D Architectures\n\nEven with our module, designing an efficient neural network is still challenging. We need to carefully adjust the network architecture (e.g., channel numbers and kernel sizes of all layers) to meet the constraints for real-world applications (e.g., latency, energy, and accuracy). To this end, we introduce 3D Neural Architecture Search (3D-NAS), to automatically design efficient 3D models (as in Figure 3).\n\n\nDesign Space\n\nThe performance of neural architecture search is greatly impacted by the design space quality. In our search space, we incorporate fine-grained channel numbers and elastic network depths; however, we do not support different kernel sizes.\n\nFine-grained Channel Numbers. The computation cost increases quadratically with the number of channels; therefore, the channel number selection has a large influence on the network efficiency. Most existing neural architecture frameworks [4] only support the coarse-grained channel number selection: e.g., searching the expansion ratio of the ResNet/MobileNet blocks over a few (2)(3) choices. In this case, only intermediate channel numbers of the blocks can be changed; while the input and output channel numbers will still remain the same. Empirically, we observe that this limits the variety of the search space. To this end, we enlarge the search space by allowing all channel numbers to be selected from a large collection of choices (with size of O(n)). This fine-grained channel number selection largely increase the number of candidates for each block: e.g., from constant (2-3) to O(n 2 ) for a block with two consecutive convolutions.\n\nElastic Network Depths. We support different network depth in our design space. For 3D CNNs, reducing the channel numbers alone cannot achieve significant measured speedup, which is very different from the 2D CNNs. For example, by shrinking all channel numbers in MinkowskiNet [7] by 4\u00d7 and 8\u00d7, the number of MACs will be reduced to 7.5 G and 1.9 G, respectively. However, although the number of MACs is drastically reduced, their measured latency on the GPU is very similar: 105 ms and 96 ms (measured on a single GTX1080Ti GPU). This suggests that scaling down the number of channels alone cannot offer us with very efficient models, even though the number of MACs is very small. This might be because 3D modules are usually more memory-bounded than 2D modules; the number of MACs decreases quadratically with channel number, while memory decreases linearly. Motivated by this, we choose to incorporate the elastic network depth into our design space so that these layers with very small computation (and large memory cost) can be removed and merged into their neighboring layers.\n\nSmall Kernel Matters. Kernel sizes are usually included into the search space of 2D CNNs. This is because a single convolution with larger kernel size can be more efficient than multiple convolutions with smaller kernel sizes on GPUs. However, it is not the case for the 3D CNNs. From the computation perspective, a single 2D convolution with kernel size of 5 requires only 1.4\u00d7 more MACs than two 2D convolutions with kernel sizes of 3; while a single 3D convolution with kernel size of 5 requires 2.3\u00d7 more MACs than two 3D convolutions with kernel sizes of 3 (if applied to dense voxel grids). This larger computation cost makes it less suitable to use large kernel sizes in 3D CNNs. Furthermore, the computation overhead of 3D modules is also related to the kernel sizes. For example, Sparse Convolution [11,7] requires O(k 3 n) time to build the kernel map, where k is the kernel size and n is the number of points, which indicates that its cost grows cubically with respect to the kernel size. Based on these reasons, we decide to keep the kernel size of all convolutions to be 3 and do not allow the kernel size to change in our search space. Even with the small kernel size, we can still achieve a large receptive field by changing the network depth, which can achieve the same effect as changing the kernel size.\n\n\nTraining Paradigm\n\nSearching over a fine-grained design space is very challenging as it is impossible to train every sampled candidate network from scratch [46]. Motivated by Guo et al . [12], we incorporate all candidate networks into a single super network so that the total training cost can be reduced from O(n) to O(1): we train the super network once, and after that, each candidate network can be directly extracted from this super network with inherited weights.\n\nUniform Sampling. At each training iteration, we randomly sample a candidate network from the super network: randomly select the channel number for each layer, and then randomly select the network depth (i.e. the number of blocks to be used) for each stage. The total number of candidate networks to be sampled during training is very limited; therefore, we choose to sample different candidate networks on different GPUs and average their gradients at each step so that more candidate networks can be sampled. For 3D, this is more critical because the 3D datasets usually contain fewer training samples than the 2D datasets: e.g. 20K on SemanticKITTI [1] vs. 1M on ImageNet [8].\n\nWeight Sharing. As the total number of candidate networks is enormous, every candidate network will only be optimized for a small fraction of the total schedule. Therefore, uniform sampling alone is not enough to train all candidate networks sufficiently (i.e., achieving the same level of accuracy as being trained from scratch). To this end, we adopt the weight sharing technique so that every candidate network can be optimized at each iteration even if it is not sampled. Specifically, given the input channel number C in and output channel number C out of each convolution layer, we simply index the first C in and C out channels from the weight tensor accordingly to perform the convolution [12]. For each batch normalization layer, we similarly crop the first c channels from the weight tensor based on the sampled channel number c. Finally, with the sampled depth d for each stage, we choose to keep the first d layers, instead of randomly sampling d of them. This ensures that each layer will always correspond to the same depth index within the stage. Progressive Depth Shrinking. Suppose we have n stages, each of which has m different depth choices from 1 to m. If we sample the depth d k for each stage k randomly, the expected total depth of the network will be\nE[d] = n k=1 E[d k ] = n \u00d7 m + 1 2 ,(3)\nwhich is much smaller than the maximum depth nm. Furthermore, the probability of the largest candidate network (with the maximum depth) being sampled is extremely small: m \u2212n . Therefore, the largest candidate networks are poorly trained due to the small possibility of being sampled. To this end, we introduce progressively depth shrinking to alleviate this issue. We divide the training epochs into m segments for m different depth choices. During the k th training segment, we only allow the depth of each stage to be selected from m \u2212 k + 1 to m. This is essentially designed to enlarge the search space gradually so that these large candidate networks can be sampled more frequently.\n\n\nSearch Algorithm\n\nAfter the super network is fully trained, we use evolutionary architecture search to find the best architectures under a certain resource constraint.\n\nResource Constraints. We use the number of MACs as the resource constraint. For 3D CNNs, the number of MACs cannot be simply determined by the input size and the network architecture: e.g., Sparse Convolution only performs the computation over the active synapses; therefore, its computation is also determined by the input sparsity pattern. To address this, we first estimate the average kernel map size over the entire dataset for each convolution layer, and we can then measure the number of MACs based on these statistics.\n\nEvolutionary Search. We automate the architecture search with evolutionary algorithm [12]. We first initialize the starting population with n randomly sampled candidate networks. At each iteration, we evaluate all candidate networks in the population and select the k models with the highest accuracies (i.e., the fittest individuals). The population for the next iteration is then generated with (n/2) mutations and (n/2) crossovers. For each mutation, we randomly select one among the top-k candidates and alter each of its architectural parameters (e.g., channel numbers, network depths) with a pre-defined probability; for each crossover, we select two from the top-k candidates and generate a new model by fusing them together randomly. Finally, the best model is obtained from the population of the last iteration. During the evolutionary search, we ensure that all the candidate networks in the population always meet the given resource constraint (we will resample another candidate network until the resource constraint is satisfied).\n\n\nExperiments\n\nWe first manually construct our backbone network (denoted as SPVCNN) based on our designed 3D module (SPVConv). We then apply our neural architecture search framework (3D-NAS) to explore the best 3D model (denoted as SPVNAS). Evaluated on 3D semantic segmentation and 3D object detection, our method consistently outperforms previous state-of-the-art models with lower computation cost and measured latency (on a single GTX1080Ti GPU).  Table 3. Results of outdoor scene segmentation on SemanticKITTI. SPVNAS outperforms the 2D projection-based methods by 3.1% to 10% in mIoU with an order of magnitude reduction in model size and and computation. Here, red numbers correspond to the computation time, and blue numbers correspond to the projection time.\n\n\n3D Scene Segmentation\n\nWe first evaluate our method on 3D semantic segmentation and conduct experiments on the large-scale outdoor scene dataset, SemanticKITTI [1]. This dataset contains 23,201 LiDAR point clouds for training and 20,351 for testing, and it is annotated from all 22 sequences in the KITTI [9] Odometry benchmark. We train all models on the entire training set and report the mean intersection-over-union (mIoU) on the official test set under the single scan setting. We provide more implementation details and experimental results in the appendix.  In Figure 4, we also provide some qualitative comparisons between SPVNAS and MinkowskiNet: our SPVNAS has lower errors especially for small instances. We further compare our SPVNAS with 2D projection-based models in Table 3. With the smaller backbone (by removing the decoder layers), SPVNAS outperforms DarkNets [1] by over 10% in mIoU with 1.2\u00d7 measured speedup even though 2D convolutions are much better optimized by modern deep learning libraries. Compared with other 2D methods, SPVNAS achieves at least 8.5\u00d7 model size reduction and 15.2\u00d7 computation reduction while being much more accurate. Furthermore, our SPVNAS achieves higher mIoU than KPConv [49], which is the previous state-of-the-art point-based model, with 17\u00d7 model size reduction and 23\u00d7 computation reduction.\n\n\nResults. As in\n\n\n3D Object Detection\n\nWe also evaluate our method on 3D object detection and conduct experiments on the outdoor scene dataset, KITTI [9]. We follow the generally adopted trainingvalidation split, where 3,712 samples are used for training and 3,769 samples are left for validation. We report the mean average precision (mAP) on the test set with 3D IoU thresholds of 0.7 for car, 0.5 for cyclist and pedestrian. We refer the readers to the appendix for additional results on the validation set.  Table 5. Results of per-class performance on SemanticKITTI. SPVNAS has a large advantage on small objects, such as bicyclist and motorcyclist.\n\nResults. We compare our method against SECOND [62], the state-of-the-art single-stage model for 3D object detection. SECOND consists of a sparse encoder using 3D Sparse Convolutions and a region proposal network that performs 2D convolutions after projecting the encoded features to the bird's-eye view (BEV). We reimplement and retrain SECOND: our implementation already outperforms the results in the original paper [62]. As for our model, we only replace these 3D Sparse Convolutions in SECOND with our SPVConv while keeping all the other settings the same for fair comparison. As summarized in Table 4, our SPVCNN achieves significant improvement in cyclist detection, for which we argue that the high-resolution point-based branch carries more information for small instances.\n\n\nAnalysis\n\nOur SPVNAS achieves higher accuracy and better efficiency than the previous state-of-the-art MinkowskiNet. In this section, we provide more detailed analysis to better understand the contributions of SPVConv and 3D-NAS.\n\n\nSparse Point-Voxel Convolution (SPVConv)\n\nFrom Table 5, our SPVNAS has a very large advantage (up to 25%) on relatively small objects such as pedestrians and cyclists. To explain this, we train SPVCNN on SemanticKITTI with the sequence 08 left out for visualization. In Figure 5, we highlight the points with top 5% feature norm within the point-based branch (in the final SPVConv). Clearly, the point-based branch learns to attend to small instances such as pedestrians, cyclists, trunks and traffic signs, which echos with our superior performance on these classes.\n\nFurther, we quantitatively analyze the feature norms from both point-based and sparse voxel-based branches. Specifically, we first rank the points from both   branches separately based on their feature norms, and then, we mark these points with top 10% feature norm in each branch as activated. From Figure 6, there are significantly more points in the point-based branch being activated for small instances: e.g., more than 80% for bicyclist. This suggests that our observation in Figure 5 generally holds.\n\n\n3D Neural Architecture Search (3D-NAS)\n\nIn Figure 7, we present both mIoU vs. #MACs and mIoU vs. latency trade-offs, where we uniformly scale the channel numbers in MinkowskiNet and SPVCNN down as our baselines. It can be observed that a better 3D module (SPVConv) and a well-designed network architecture (3D-NAS) are equally important to the final performance boost. Remarkably, SPVNAS outperforms MinkowskiNet by more than 6% in mIoU at 110 ms latency. Such a large improvement comes from the non-uniform channel scaling and elastic nework depth. In these manuallydesigned models (MinkowskiNet and SPVCNN), 77% of the total computation is distributed to the upsampling stage. With 3D-NAS, this ratio is reduced to  47-63%, making the computation more balanced and the downsampling stage (i.e., feature extraction) more emphasized.\n\nWe also compare our evolutionary search with the random architecture search to show that the success of 3D-NAS does not entirely come from the search space. As in Figure 8a, random architecture search has poor sample efficiency: the best model at the 20 th generation performs even worse than the best model in the first generation. In contrast, our evolutionary search is capable of progressively finding better architecture, and the final best architecture performs around 3% better than the one in the first generation. We also retrain 20 random models sampled from our search space and compare them with SPVNAS in Figure 8b. As a result, our SPVNAS performs 0.8% better compared with the average performance of these random models.\n\n\nConclusion\n\nWe present Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module specialized for small object recognition. With SPVCNN built upon SPVConv, we solve the problem that Sparse Convolution cannot always keep high-resolution representation and Point-Voxel Convolution does not scale up to large 3D scenes. Furthermore, we introduce 3D-NAS, the first architecture search framework for 3D scene understanding that greatly improves the efficiency and performance of SPVCNN. Extensive experiments on outdoor 3D scene benchmarks demonstrate that the resulting SPVNAS model is lightweight, fast and powerful. We hope that this work will inspire future research on efficient 3D deep learning.\n\n\nA.1 Implementation Details\n\nWe provide more implementation details on how to build our backbone network (SPVCNN), and train the super network and search for the best model (3D-NAS).\n\n\nA.1.1 SPVCNN: Backbone Network\n\nBased on MinkowskiNet [7], we build our backbone network by wrapping residual Sparse Convolution blocks with the high-resolution point-based branch. Specifically, the first SPVConv voxelizes the input data and devoxelizes at the end of the stemming stage (i.e., before downsampling). The second SPVConv generates the volumetric representation subsequently and turns back to the point-based representation after all four downsampling stages. The final two SPVConv's each wraps around two upsampling stages in MinkowskiNet.\n\nAlso, we design a smaller backbone based on PVCNN [26] by directly replacing the volumetric convolutions with a small Sparse Convolution network containing one convolution layer (followed by normalization and activation layers) and two residual blocks.\n\n\nA.1.2 3D-NAS: Architecture Search\n\nWe train the super network for 15 epochs that supports fine-grained channel setting with a starting learning rate 0.24 and cosine learning rate decay. Then, we train for another 15 epochs to incorporate elastic network depth with a starting learning rate 0.096 and cosine learning rate decay. After that, we perform evolutionary architecture search with a population of 50 candidates for 20 generations on the official validation set (sequence 08). Best architecture is directly extracted from the super network and submitted to the test server after finetuning for 10 epochs with a starting learning rate of 0.032 and cosine learning rate decay.\n\n\nA.2 More Results\n\n\nA.2.1 3D Scene Segmentation\n\nWe present more detailed results of MinkowskiNet [7], SPVCNN and SPVNAS on both the official test set and validation set (sequence 08) of SemanticKITTI [1] in Table 6 and Table 7. For the results on the validation set in Table 7, we run the whole architecture search pipeline again on sequences 00-07 and 09, leaving sequence 10 out as the mini-validation set, and report the results on sequence 08. We observe similar trends on both test and validation results: both a better 3D module (SPVConv) and our 3D Neural Architecture Search (3D-NAS) pipeline improve the performance of MinkowskiNet [7].\n\n\nA.2.2 3D Object Detection\n\nWe further provide the results of SPVCNN on the validation set of KITTI [10] in Table 8. We train both SECOND [62] and our SPVCNN on the training set for three times, and we report the average results to reduce the variance. Similar to the results on the test set, SPVCNN also has consistent improvement in almost all classes on the validation set. \n\n\nA.3 More Visualizations\n\nWe provide more visualizations for MinkowskiNet [7] and SPVNAS in Figure A1 to show that the improvements brought by SPVConv on small objects and region boundaries are general. For example, in the first row, MinkowskiNet wrongly segments the entire traffic sign and bicycle instances. However, our SPVNAS is capable of making almost no mistake on these very small objects. Also, we observe in next two rows that MinkowskiNet does not perform well on the sidewalk-building or sidewalk-vegetation boundary where SPVNAS has a clear advantage.\n\nIn Figure A2, we show the comparison between our smaller SPVNAS and DarkNet53 [1]. DarkNets use spherical projections to project 3D point clouds to a 2D plane such that part of geometry information is lost. Our SPVNAS, in contrast, directly learns on 3D data and is more powerful in geometric modeling.  Table 8. Results of 3D object detection on the validation set of KITTI [9]. We observe that SPVNAS has significant advantages in both large regions (last 3 rows) and smaller objects (first row). We finally demonstrate the superior performance of SPVCNN over SEC-OND [62], a state-of-the-art single stage 3D detector in Figure A3. SPVCNN has big advantage in scenes with crowded small objects. In the first row of Figure A3, our SPVCNN is capable of detecting a challenging small pedestrian instance missed by SECOND and SPVCNN also avoids duplicate detections made by SECOND in the next three rows. \n\nFig. 2 .\n2Overview of Sparse Point-Voxel Convolution (SPVConv): it equips the sparse voxel-based branch with a lightweight, high-resolution point-based branch which can capture fine details in large scenes.\n\nFig. 3 .\n3Overview of 3D Neural Architecture Search (3D-NAS): we first train a super network composed of multiple SPVConv's, supporting fine-grained channel numbers and elastic network depths. Then, we perform the evolutionary architecture search to obtain the best candidate model under a given computation constraint.\n\nFig. 4 .\n4MinkowskiNet has a higher error recognizing small objects and region boundaries, while SPVNAS recognizes small objects better thanks to the high-resolution point-based branch.\n\nFig. 5 .\n5The point-based branch learns to put its attention on small instances (i.e., pedestrians, cyclists, traffic signs). Here, the points in red are the ones with the top 5% largest feature norm in the point-based branch.\n\n1 Fig. 6 .\n16Average percentage of activated points in point-based and sparse voxel-based branches for all 19 classes in SemanticKITTI[1]: the point-based branch attends to smaller objects as the red bars are much higher.\n\nFig. 7 .\n7Trade-off: Mean IoU vs. GPU Latency An efficient 3D module (SPVConv) and a well-designed network architecture (3D-NAS) are equally important to the final performance of SPVNAS: 7.6\u00d7 computation reduction and 2.7\u00d7 measured speedup over MinkowskiNet. Comparison with random models.\n\nFig. 8 .\n8Evolutionary Search (ES) is more sample-efficient than Random Search (RS).\n\n(a )\n)Error by MinkowskiNet (b) Error by SPVNAS (c) Ground TruthFig. A1. Qualitative comparisons between MinkowskiNet [7] and SPVNAS.\n\n\nError by DarkNet (b) Error by SPVNAS (c) Ground Truth Fig. A2. Qualitative comparisons between DarkNet53 [1] and SPVNAS.\n\nFig. A3 .\nA3Qualitative comparisons between SECOND [62] and SPVCNN.\n\n\nCin: Input Channels, Cout: Output Channels.Evolutionary Arch. Search \n\nSuper Network Training \n\nFine-Grained Channel + Elastic Depth \nWeight Sharing \n\nUniform \nSampling \n\nGPU#1 GPU#N \n\n\u2026 \n\n#Cin \n\n#Cout \n\nmax #Cin \nmax #Cout \n\nStage I \n(Depth: 3) \n\nStage II \n(Depth: 2,3) \n\nStage III \n(Depth: 1,2,3) \n\nVal. \nIoU \n\nBest \nArch. \n\n\n\nTable 2 ,\n2our SPVNAS outperforms the previous state-of-the-\nart MinkowskiNet [7] by 3.3% in mIoU with 1.7\u00d7 model size reduction, 1.5\u00d7 \ncomputation reduction and 1.1\u00d7 measured speedup. We further downscale our \nSPVNAS by setting the resource constraint to 15G MACs. This offers us with a \nmuch smaller model that outperforms MinkowskiNet by 0.6% in mIoU with 8.3\u00d7 \nmodel size reduction, 7.6\u00d7 computation reduction, and 2.7\u00d7 measured speedup. \n\n\n\nTable 4. Results of outdoor object detection on KITTI. SPVCNN outperforms SEC-OND in most categories especially for the cyclist.Car \nCyclist \nPedestrian \n\nEasy Mod. Hard Easy Mod. Hard Easy Mod. Hard \n\nSECOND [62] \n84.7 76.0 68.7 75.8 60.8 53.7 45.3 35.5 33.1 \nSECOND (Repro.) 87.5 77.9 74.4 76.0 59.7 52.9 49.1 41.7 39.1 \n\nSPVCNN (Ours) 87.8 78.4 74.8 80.1 63.7 56.2 49.2 41.4 38.4 \n\nPerson \nBicycle \nBicyclist \nMotorcycle \nMotorcyclist \n\nMinkowskiNet [7] \n60.9 \n40.4 \n61.9 \n47.4 \n18.7 \n\nSPVNAS (Ours) \n65.7 \n51.6 \n65.2 \n50.8 \n43.7 \n(+4.8) \n(+11.2) \n(+3.3) \n(+3.4) \n(+25.0) \n\n\nTable 7 .\n7Table 6. Results of 3D scene segmentation on the test set of SemanticKITTI[1]. Results of 3D scene segmentation on the validation set of SemanticKITTI[1].#Params (M) \n#MAdds (G) \nLatency (ms) \nMean IoU \n\nMinkowskiNet [7] \n2.2 \n11.3 \n115.7 \n57.5 \nSPVCNN (Ours) \n2.2 \n11.9 \n124.3 \n58.5 \nSPVNAS (Ours) \n2.6 \n15.0 \n110.4 \n63.6 \n\nMinkowskiNet [7] \n5.5 \n28.5 \n152.0 \n60.0 \nSPVCNN (Ours) \n5.5 \n30.0 \n160.9 \n61.6 \nSPVNAS (Ours) \n4.2 \n20.0 \n132.6 \n64.5 \n\nMinkowskiNet [7] \n8.8 \n45.9 \n207.4 \n62.8 \nSPVCNN (Ours) \n8.8 \n47.4 \n214.3 \n64.4 \nSPVNAS (Ours) \n5.1 \n24.4 \n144.3 \n65.2 \n\nMinkowskiNet [7] \n21.7 \n113.9 \n294.0 \n61.1 \nSPVCNN (Ours) \n21.8 \n118.6 \n317.1 \n63.8 \nSPVNAS (Ours) \n7.5 \n34.1 \n166.1 \n66.0 \nSPVNAS (Ours) \n12.5 \n73.8 \n259.9 \n66.4 \n\n#Params (M) \n#MAdds (G) \nLatency (ms) \nMean IoU \n\nMinkowskiNet [7] \n5.5 \n28.5 \n152.0 \n58.9 \nSPVCNN (Ours) \n5.5 \n30.0 \n160.9 \n60.7 \nSPVNAS (Ours) \n4.5 \n24.6 \n158.1 \n62.9 \n\nMinkowskiNet [7] \n8.8 \n45.9 \n207.4 \n60.3 \nSPVCNN (Ours) \n8.8 \n47.4 \n214.3 \n61.4 \nSPVNAS (Ours) \n7.0 \n34.7 \n175.8 \n63.5 \n\nMinkowskiNet [7] \n21.7 \n113.9 \n294.0 \n61.1 \nSPVCNN (Ours) \n21.8 \n118.6 \n317.1 \n63.8 \nSPVNAS (Ours) \n10.8 \n64.5 \n248.7 \n64.7 \n\n\n\n\nCarCyclist Pedestrian Easy Mod. Hard Easy Mod. Hard Easy Mod. HardSECOND [62] \n89.8 80.9 78.4 82.5 62.8 58.9 68.3 60.8 55.3 \n\nSPVCNN (Ours) 90.9 81.8 79.2 85.1 63.8 60.1 68.2 61.6 55.9 \n\n\nAcknowledgements. We thank MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Xilinx and Samsung for supporting this research. We also thank AWS Machine Learning Research Awards for providing the computational resource.\nSemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. J Behley, M Garbade, A Milioto, J Quenzel, S Behnke, C Stachniss, J Gall, ICCVBehley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.: SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. In: ICCV (2019)\n\nOnce for All: Train One Network and Specialize it for Efficient Deployment. H Cai, C Gan, T Wang, Z Zhang, S Han, ICLRCai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once for All: Train One Network and Specialize it for Efficient Deployment. In: ICLR (2020)\n\nAutoML for Architecting Efficient and Specialized Neural Networks. H Cai, J Lin, Y Lin, Z Liu, K Wang, T Wang, L Zhu, S Han, IEEE Micro. Cai, H., Lin, J., Lin, Y., Liu, Z., Wang, K., Wang, T., Zhu, L., Han, S.: AutoML for Architecting Efficient and Specialized Neural Networks. IEEE Micro (2019)\n\nH Cai, L Zhu, S Han, ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. ICLRCai, H., Zhu, L., Han, S.: ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. In: ICLR (2019)\n\nA X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, J Xiao, L Yi, F Yu, ShapeNet: An Information-Rich 3D Model Repository. arXiv. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information- Rich 3D Model Repository. arXiv (2015)\n\nDetNAS: Backbone Search for Object Detection. Y Chen, T Yang, X Zhang, G Meng, X Xiao, J Sun, NeurIPSChen, Y., Yang, T., Zhang, X., Meng, G., Xiao, X., Sun, J.: DetNAS: Backbone Search for Object Detection. In: NeurIPS (2019)\n\nC Choy, J Gwak, S Savarese, 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. CVPRChoy, C., Gwak, J., Savarese, S.: 4D Spatio-Temporal ConvNets: Minkowski Con- volutional Neural Networks. In: CVPR (2019)\n\nImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, CVPRDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR (2009)\n\nA Geiger, P Lenz, C Stiller, R Urtasun, Vision meets Robotics: The KITTI Dataset. IJRR. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets Robotics: The KITTI Dataset. IJRR (2013)\n\nAre we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. A Geiger, P Lenz, R Urtasun, CVPRGeiger, A., Lenz, P., Urtasun, R.: Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In: CVPR (2012)\n\n3D Semantic Segmentation With Submanifold Sparse Convolutional Networks. B Graham, M Engelcke, L Van Der Maaten, CVPRGraham, B., Engelcke, M., van der Maaten, L.: 3D Semantic Segmentation With Submanifold Sparse Convolutional Networks. In: CVPR (2018)\n\nSingle Path One-Shot Neural Architecture Search with Uniform Sampling. Z Guo, X Zhang, H Mu, W Heng, Z Liu, Y Wei, J Sun, ECCVGuo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., Sun, J.: Single Path One-Shot Neural Architecture Search with Uniform Sampling. In: ECCV (2020)\n\nOccuSeg: Occupancy-aware 3D Instance Segmentation. L Han, T Zheng, L Xu, L Fang, CVPRHan, L., Zheng, T., Xu, L., Fang, L.: OccuSeg: Occupancy-aware 3D Instance Segmentation. In: CVPR (2020)\n\nAMC: AutoML for Model Compression and Acceleration on Mobile Devices. Y He, J Lin, Z Liu, H Wang, L J Li, S Han, ECCVHe, Y., Lin, J., Liu, Z., Wang, H., Li, L.J., Han, S.: AMC: AutoML for Model Compression and Acceleration on Mobile Devices. In: ECCV (2018)\n\nA G Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, H Adam, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv (2017)\n\nRandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. Q Hu, B Yang, L Xie, S Rosa, Y Guo, Z Wang, N Trigoni, A Markham, CVPRHu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A.: RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. In: CVPR (2020)\n\nPointGroup: Dual-Set Point Grouping for 3D Instance Segmentation. L Jiang, H Zhao, S Shi, S Liu, C W Fu, J Jia, CVPRJiang, L., Zhao, H., Shi, S., Liu, S., Fu, C.W., Jia, J.: PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation. In: CVPR (2020)\n\nJ Lahoud, B Ghanem, M Pollefeys, M R Oswald, 3D Instance Segmentation via Multi-Task Metric Learning. ICCVLahoud, J., Ghanem, B., Pollefeys, M., Oswald, M.R.: 3D Instance Segmentation via Multi-Task Metric Learning. In: ICCV (2019)\n\nLarge-Scale Point Cloud Semantic Segmentation With Superpoint Graphs. L Landrieu, M Simonovsky, CVPRLandrieu, L., Simonovsky, M.: Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs. In: CVPR (2018)\n\nOctree Guided CNN With Spherical Kernels for 3D Point Clouds. H Lei, N Akhtar, A Mian, CVPRLei, H., Akhtar, N., Mian, A.: Octree Guided CNN With Spherical Kernels for 3D Point Clouds. In: CVPR (2019)\n\nGAN Compression: Efficient Architectures for Interactive Conditional GANs. M Li, J Lin, Y Ding, Z Liu, J Y Zhu, S Han, CVPRLi, M., Lin, J., Ding, Y., Liu, Z., Zhu, J.Y., Han, S.: GAN Compression: Efficient Architectures for Interactive Conditional GANs. In: CVPR (2020)\n\nY Li, R Bu, M Sun, W Wu, X Di, B Chen, PointCNN: Convolution on X -Transformed Points. NeurIPSLi, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.: PointCNN: Convolution on X -Transformed Points. In: NeurIPS (2018)\n\n. C Liu, B Zoph, M Neumann, J Shlens, W Hua, L J Li, L Fei-Fei, A Yuille, J Huang, K Murphy, Progressive Neural Architecture Search. ECCVLiu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L.J., Fei-Fei, L., Yuille, A., Huang, J., Murphy, K.: Progressive Neural Architecture Search. In: ECCV (2018)\n\nH Liu, K Simonyan, Y Yang, DARTS: Differentiable Architecture Search. ICLRLiu, H., Simonyan, K., Yang, Y.: DARTS: Differentiable Architecture Search. In: ICLR (2019)\n\nMetaPruning: Meta Learning for Automatic Neural Network Channel Pruning. Z Liu, H Mu, X Zhang, Z Guo, X Yang, K T Cheng, J Sun, ICCVLiu, Z., Mu, H., Zhang, X., Guo, Z., Yang, X., Cheng, K.T., Sun, J.: MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning. In: ICCV (2019)\n\nPoint-Voxel CNN for Efficient 3D Deep Learning. Z Liu, H Tang, Y Lin, S Han, NeurIPSLiu, Z., Tang, H., Lin, Y., Han, S.: Point-Voxel CNN for Efficient 3D Deep Learning. In: NeurIPS (2019)\n\nShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design. N Ma, X Zhang, H T Zheng, J Sun, ECCVMa, N., Zhang, X., Zheng, H.T., Sun, J.: ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design. In: ECCV (2018)\n\nInterpolated Convolutional Networks for 3D Point Cloud Understanding. J Mao, X Wang, H Li, ICCVMao, J., Wang, X., Li, H.: Interpolated Convolutional Networks for 3D Point Cloud Understanding. In: ICCV (2019)\n\nVoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition. D Maturana, S Scherer, IROSMaturana, D., Scherer, S.: VoxNet: A 3D Convolutional Neural Network for Real- Time Object Recognition. In: IROS (2015)\n\nCuckoo Hashing. R Pagh, F F Rodler, Journal of Algorithms. Pagh, R., Rodler, F.F.: Cuckoo Hashing. Journal of Algorithms (2001)\n\nImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes. C R Qi, X Chen, O Litany, L J Guibas, CVPRQi, C.R., Chen, X., Litany, O., Guibas, L.J.: ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes. In: CVPR (2020)\n\nDeep Hough Voting for 3D Object Detection in Point Clouds. C R Qi, O Litany, K He, L J Guibas, ICCVQi, C.R., Litany, O., He, K., Guibas, L.J.: Deep Hough Voting for 3D Object Detection in Point Clouds. In: ICCV (2019)\n\nPointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. C R Qi, H Su, K Mo, L J Guibas, CVPRQi, C.R., Su, H., Mo, K., Guibas, L.J.: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. In: CVPR (2017)\n\nFrustum PointNets for 3D Object Detection from RGB-D Data. C R Qi, W Liu, C Wu, H Su, L J Guibas, CVPRQi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum PointNets for 3D Object Detection from RGB-D Data. In: CVPR (2018)\n\nVolumetric and Multi-View CNNs for Object Classification on 3D Data. C R Qi, H Su, M Niessner, A Dai, M Yan, L J Guibas, CVPRQi, C.R., Su, H., Niessner, M., Dai, A., Yan, M., Guibas, L.J.: Volumetric and Multi-View CNNs for Object Classification on 3D Data. In: CVPR (2016)\n\nPointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. C R Qi, L Yi, H Su, L J Guibas, NeurIPSQi, C.R., Yi, L., Su, H., Guibas, L.J.: PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In: NeurIPS (2017)\n\nOn Network Design Spaces for Visual Recognition. I Radosavovic, J Johnson, S Xie, W Y Lo, P Dollar, ICCVRadosavovic, I., Johnson, J., Xie, S., Lo, W.Y., Dollar, P.: On Network Design Spaces for Visual Recognition. In: ICCV (2019)\n\nOctNet: Learning Deep 3D Representations at High Resolutions. G Riegler, A O Ulusoy, A Geiger, CVPRRiegler, G., Ulusoy, A.O., Geiger, A.: OctNet: Learning Deep 3D Representations at High Resolutions. In: CVPR (2017)\n\nM Sandler, A Howard, M Zhu, A Zhmoginov, L C Chen, MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPRSandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2: Inverted Residuals and Linear Bottlenecks. In: CVPR (2018)\n\nPV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection. S Shi, C Guo, L Jiang, Z Wang, J Shi, X Wang, H Li, CVPRShi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., Li, H.: PV-RCNN: Point- Voxel Feature Set Abstraction for 3D Object Detection. In: CVPR (2020)\n\nPointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. S Shi, X Wang, H Li, CVPRShi, S., Wang, X., Li, H.: PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. In: CVPR (2019)\n\nPV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection. S Shi, Z Wang, J Shi, X Wang, H Li, TPAMIShi, S., Wang, Z., Shi, J., Wang, X., Li, H.: PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection. TPAMI (2020)\n\nD Stamoulis, R Ding, D Wang, D Lymberopoulos, B Priyantha, J Liu, D Marculescu, Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours. arXiv. Stamoulis, D., Ding, R., Wang, D., Lymberopoulos, D., Priyantha, B., Liu, J., Marculescu, D.: Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours. arXiv (2019)\n\nEnergy and Policy Considerations for Deep Learning in NLP. E Strubell, A Ganesh, A Mccallum, ACLStrubell, E., Ganesh, A., McCallum, A.: Energy and Policy Considerations for Deep Learning in NLP. In: ACL (2019)\n\nSPLATNet: Sparse Lattice Networks for Point Cloud Processing. H Su, V Jampani, D Sun, S Maji, E Kalogerakis, M H Yang, J Kautz, CVPRSu, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang, M.H., Kautz, J.: SPLATNet: Sparse Lattice Networks for Point Cloud Processing. In: CVPR (2018)\n\nM Tan, B Chen, R Pang, V Vasudevan, M Sandler, A Howard, Q V Le, MnasNet: Platform-Aware Neural Architecture Search for Mobile. CVPRTan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.: MnasNet: Platform-Aware Neural Architecture Search for Mobile. In: CVPR (2019)\n\nM Tan, Q V Le, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In: ICML. Tan, M., Le, Q.V.: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In: ICML (2019)\n\nTangent Convolutions for Dense Prediction in 3D. M Tatarchenko, J Park, V Koltun, Q Y Zhou, CVPRTatarchenko, M., Park, J., Koltun, V., Zhou, Q.Y.: Tangent Convolutions for Dense Prediction in 3D. In: CVPR (2018)\n\nKPConv: Flexible and Deformable Convolution for Point Clouds. H Thomas, C R Qi, J E Deschaud, B Marcotegui, F Goulette, L J Guibas, ICCVThomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.: KPConv: Flexible and Deformable Convolution for Point Clouds. In: ICCV (2019)\n\nHAT: Hardware-Aware Transformers for Efficient Natural Language Processing. H Wang, Z Wu, Z Liu, H Cai, L Zhu, C Gan, S Han, ACLWang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., Han, S.: HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. In: ACL (2020)\n\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision. K Wang, Z Liu, Y Lin, J Lin, S Han, CVPRWang, K., Liu, Z., Lin, Y., Lin, J., Han, S.: HAQ: Hardware-Aware Automated Quantization with Mixed Precision. In: CVPR (2019)\n\nHardware-Centric AutoML for Mixed-Precision Quantization. K Wang, Z Liu, Y Lin, J Lin, S Han, IJCV. Wang, K., Liu, Z., Lin, Y., Lin, J., Han, S.: Hardware-Centric AutoML for Mixed- Precision Quantization. IJCV (2020)\n\nP S Wang, Y Liu, Y X Guo, C Y Sun, X Tong, O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis. SIGGRAPHWang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X.: O-CNN: Octree-based Convo- lutional Neural Networks for 3D Shape Analysis. In: SIGGRAPH (2017)\n\nAdaptive O-CNN: A Patchbased Deep Representation of 3D Shapes. P S Wang, Y Liu, Y X Guo, C Y Sun, X Tong, SIGGRAPH Asia. Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X.: Adaptive O-CNN: A Patch- based Deep Representation of 3D Shapes. In: SIGGRAPH Asia (2018)\n\nAPQ: Joint Search for Network Architecture, Pruning and Quantization Policy. T Wang, K Wang, H Cai, J Lin, Z Liu, H Wang, Y Lin, S Han, CVPRWang, T., Wang, K., Cai, H., Lin, J., Liu, Z., Wang, H., Lin, Y., Han, S.: APQ: Joint Search for Network Architecture, Pruning and Quantization Policy. In: CVPR (2020)\n\nDynamic Graph CNN for Learning on Point Clouds. Y Wang, Y Sun, Z Liu, S E Sarma, M M Bronstein, J M Solomon, SIGGRAPHWang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic Graph CNN for Learning on Point Clouds. In: SIGGRAPH (2019)\n\nVoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes. Z Wang, F Lu, TVCGWang, Z., Lu, F.: VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes. TVCG (2019)\n\nFBNet: Hardware-aware Efficient Convnet Design via Differentiable Neural Architecture Search. B Wu, X Dai, P Zhang, Y Wang, F Sun, Y Wu, Y Tian, P Vajda, Y Jia, K Keutzer, CVPRWu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y., Keutzer, K.: FBNet: Hardware-aware Efficient Convnet Design via Differentiable Neural Architecture Search. In: CVPR (2019)\n\nPointConv: Deep Convolutional Networks on 3D Point Clouds. W Wu, Z Qi, L Fuxin, CVPRWu, W., Qi, Z., Fuxin, L.: PointConv: Deep Convolutional Networks on 3D Point Clouds. In: CVPR (2019)\n\nSqueeze-SegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation. C Xu, B Wu, Z Wang, W Zhan, P Vajda, K Keutzer, M Tomizuka, ECCVXu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K., Tomizuka, M.: Squeeze- SegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation. In: ECCV (2020)\n\nY Xu, T Fan, M Xu, L Zeng, Y Qiao, SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. ECCVXu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y.: SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. In: ECCV (2018)\n\nSECOND: Sparsely Embedded Convolutional Detection. Y Yan, Y Mao, B Li, Sensors. Yan, Y., Mao, Y., Li, B.: SECOND: Sparsely Embedded Convolutional Detection. Sensors (2018)\n\nLearning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. B Yang, J Wang, R Clark, Q Hu, S Wang, A Markham, N Trigoni, NeurIPSYang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham, A., Trigoni, N.: Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. In: NeurIPS (2019)\n\nSTD: Sparse-to-Dense 3D Object Detector for Point Cloud. Z Yang, Y Sun, S Liu, X Shen, J Jia, ICCVYang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: STD: Sparse-to-Dense 3D Object Detector for Point Cloud. In: ICCV (2019)\n\nShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices. X Zhang, X Zhou, M Lin, J Sun, CVPRZhang, X., Zhou, X., Lin, M., Sun, J.: ShuffleNet: An Extremely Efficient Convolu- tional Neural Network for Mobile Devices. In: CVPR (2018)\n\nPolar-Net: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation. Y Zhang, Z Zhou, P David, X Yue, Z Xi, B Gong, H Foroosh, CVPRZhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong, B., Foroosh, H.: Polar- Net: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation. In: CVPR (2020)\n\nVoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. Y Zhou, O Tuzel, CVPRZhou, Y., Tuzel, O.: VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. In: CVPR (2018)\n\nV-NAS: Neural Architecture Search for Volumetric Medical Image Segmentation. Z Zhu, C Liu, D Yang, A Yuille, D Xu, 3Zhu, Z., Liu, C., Yang, D., Yuille, A., Xu, D.: V-NAS: Neural Architecture Search for Volumetric Medical Image Segmentation. In: 3DV (2019)\n\nNeural Architecture Search with Reinforcement Learning. B Zoph, Q V Le, Zoph, B., Le, Q.V.: Neural Architecture Search with Reinforcement Learning. In: ICLR (2017)\n\nLearning Transferable Architectures for Scalable Image Recognition. B Zoph, V Vasudevan, J Shlens, Q V Le, CVPRZoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning Transferable Architectures for Scalable Image Recognition. In: CVPR (2018)\n", "annotations": {"author": "[{\"end\":129,\"start\":76},{\"end\":182,\"start\":130},{\"end\":263,\"start\":183},{\"end\":314,\"start\":264},{\"end\":362,\"start\":315},{\"end\":415,\"start\":363},{\"end\":465,\"start\":416}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":84},{\"end\":141,\"start\":138},{\"end\":195,\"start\":191},{\"end\":273,\"start\":270},{\"end\":321,\"start\":318},{\"end\":374,\"start\":370},{\"end\":424,\"start\":421}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":137,\"start\":130},{\"end\":190,\"start\":183},{\"end\":269,\"start\":264},{\"end\":317,\"start\":315},{\"end\":369,\"start\":363},{\"end\":420,\"start\":416}]", "author_affiliation": "[{\"end\":128,\"start\":90},{\"end\":181,\"start\":143},{\"end\":235,\"start\":197},{\"end\":262,\"start\":237},{\"end\":313,\"start\":275},{\"end\":361,\"start\":323},{\"end\":414,\"start\":376},{\"end\":464,\"start\":426}]", "title": "[{\"end\":73,\"start\":1},{\"end\":538,\"start\":466}]", "venue": null, "abstract": "[{\"end\":2013,\"start\":540}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2869,\"start\":2865},{\"end\":2895,\"start\":2891},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3112,\"start\":3109},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3115,\"start\":3112},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3347,\"start\":3343},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3350,\"start\":3347},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3412,\"start\":3408},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3414,\"start\":3412},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5583,\"start\":5580},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5586,\"start\":5583},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5589,\"start\":5586},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5592,\"start\":5589},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":5595,\"start\":5592},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5968,\"start\":5964},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6093,\"start\":6089},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6096,\"start\":6093},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6099,\"start\":6096},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6102,\"start\":6099},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6105,\"start\":6102},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6108,\"start\":6105},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6111,\"start\":6108},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6114,\"start\":6111},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6131,\"start\":6127},{\"end\":6244,\"start\":6217},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6275,\"start\":6271},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6278,\"start\":6275},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6281,\"start\":6278},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6284,\"start\":6281},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6431,\"start\":6427},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6450,\"start\":6446},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6453,\"start\":6450},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6474,\"start\":6470},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6631,\"start\":6627},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6754,\"start\":6750},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6775,\"start\":6772},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7097,\"start\":7093},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7100,\"start\":7097},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7103,\"start\":7100},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7106,\"start\":7103},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7264,\"start\":7260},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":7267,\"start\":7264},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7296,\"start\":7292},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7392,\"start\":7388},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7395,\"start\":7392},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7398,\"start\":7395},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7579,\"start\":7575},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7747,\"start\":7743},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7776,\"start\":7773},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7812,\"start\":7808},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7814,\"start\":7812},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7839,\"start\":7835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7841,\"start\":7839},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7844,\"start\":7841},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7847,\"start\":7844},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7973,\"start\":7969},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7976,\"start\":7973},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7978,\"start\":7976},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8000,\"start\":7996},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8003,\"start\":8000},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8006,\"start\":8003},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8009,\"start\":8006},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8119,\"start\":8115},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8247,\"start\":8243},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8464,\"start\":8460},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8491,\"start\":8488},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8597,\"start\":8593},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9128,\"start\":9124},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10739,\"start\":10735},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10741,\"start\":10739},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11502,\"start\":11499},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13924,\"start\":13920},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14225,\"start\":14222},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14434,\"start\":14431},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14767,\"start\":14763},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16315,\"start\":16312},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16455,\"start\":16452},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16458,\"start\":16455},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17301,\"start\":17298},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18917,\"start\":18913},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18919,\"start\":18917},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19589,\"start\":19585},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19620,\"start\":19616},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20556,\"start\":20553},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20579,\"start\":20576},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21283,\"start\":21279},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23375,\"start\":23371},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25264,\"start\":25261},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25409,\"start\":25406},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25982,\"start\":25979},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26327,\"start\":26323},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26602,\"start\":26599},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27155,\"start\":27151},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27527,\"start\":27523},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31720,\"start\":31717},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32272,\"start\":32268},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33257,\"start\":33254},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33360,\"start\":33357},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33801,\"start\":33798},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33908,\"start\":33904},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":33946,\"start\":33942},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34260,\"start\":34257},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34831,\"start\":34828},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35128,\"start\":35125},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":35324,\"start\":35320},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36736,\"start\":36733},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38968,\"start\":38965},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39044,\"start\":39041}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":35861,\"start\":35654},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36182,\"start\":35862},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36369,\"start\":36183},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36597,\"start\":36370},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36820,\"start\":36598},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37111,\"start\":36821},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37197,\"start\":37112},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37332,\"start\":37198},{\"attributes\":{\"id\":\"fig_9\"},\"end\":37455,\"start\":37333},{\"attributes\":{\"id\":\"fig_10\"},\"end\":37524,\"start\":37456},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37854,\"start\":37525},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38299,\"start\":37855},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38878,\"start\":38300},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40041,\"start\":38879},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":40231,\"start\":40042}]", "paragraph": "[{\"end\":2730,\"start\":2029},{\"end\":3924,\"start\":2732},{\"end\":4830,\"start\":3926},{\"end\":4881,\"start\":4832},{\"end\":5407,\"start\":4883},{\"end\":6310,\"start\":5447},{\"end\":6932,\"start\":6312},{\"end\":8352,\"start\":6963},{\"end\":8821,\"start\":8396},{\"end\":9063,\"start\":8823},{\"end\":10544,\"start\":9112},{\"end\":11121,\"start\":10592},{\"end\":11743,\"start\":11123},{\"end\":12185,\"start\":11788},{\"end\":12255,\"start\":12187},{\"end\":12484,\"start\":12347},{\"end\":12766,\"start\":12624},{\"end\":14060,\"start\":12936},{\"end\":14523,\"start\":14062},{\"end\":14920,\"start\":14525},{\"end\":15360,\"start\":14922},{\"end\":15817,\"start\":15409},{\"end\":16072,\"start\":15834},{\"end\":17019,\"start\":16074},{\"end\":18103,\"start\":17021},{\"end\":19426,\"start\":18105},{\"end\":19899,\"start\":19448},{\"end\":20580,\"start\":19901},{\"end\":21857,\"start\":20582},{\"end\":22586,\"start\":21898},{\"end\":22756,\"start\":22607},{\"end\":23284,\"start\":22758},{\"end\":24329,\"start\":23286},{\"end\":25098,\"start\":24345},{\"end\":26447,\"start\":25124},{\"end\":27103,\"start\":26488},{\"end\":27886,\"start\":27105},{\"end\":28118,\"start\":27899},{\"end\":28688,\"start\":28163},{\"end\":29197,\"start\":28690},{\"end\":30033,\"start\":29240},{\"end\":30770,\"start\":30035},{\"end\":31476,\"start\":30785},{\"end\":31660,\"start\":31507},{\"end\":32216,\"start\":31695},{\"end\":32470,\"start\":32218},{\"end\":33154,\"start\":32508},{\"end\":33802,\"start\":33205},{\"end\":34181,\"start\":33832},{\"end\":34748,\"start\":34209},{\"end\":35653,\"start\":34750}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12346,\"start\":12256},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12623,\"start\":12485},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12855,\"start\":12767},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12935,\"start\":12855},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21897,\"start\":21858}]", "table_ref": "[{\"end\":8567,\"start\":8560},{\"end\":10067,\"start\":10060},{\"end\":24789,\"start\":24782},{\"end\":25889,\"start\":25882},{\"end\":26968,\"start\":26961},{\"end\":27710,\"start\":27703},{\"end\":28175,\"start\":28168},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33383,\"start\":33364},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33433,\"start\":33426},{\"end\":33919,\"start\":33912},{\"end\":35061,\"start\":35054}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2027,\"start\":2015},{\"attributes\":{\"n\":\"2\"},\"end\":5422,\"start\":5410},{\"attributes\":{\"n\":\"2.1\"},\"end\":5445,\"start\":5425},{\"attributes\":{\"n\":\"2.2\"},\"end\":6961,\"start\":6935},{\"attributes\":{\"n\":\"3\"},\"end\":8394,\"start\":8355},{\"attributes\":{\"n\":\"3.1\"},\"end\":9110,\"start\":9066},{\"attributes\":{\"n\":\"3.2\"},\"end\":10590,\"start\":10547},{\"attributes\":{\"n\":\"3.3\"},\"end\":11786,\"start\":11746},{\"attributes\":{\"n\":\"4\"},\"end\":15407,\"start\":15363},{\"attributes\":{\"n\":\"4.1\"},\"end\":15832,\"start\":15820},{\"attributes\":{\"n\":\"4.2\"},\"end\":19446,\"start\":19429},{\"attributes\":{\"n\":\"4.3\"},\"end\":22605,\"start\":22589},{\"attributes\":{\"n\":\"5\"},\"end\":24343,\"start\":24332},{\"attributes\":{\"n\":\"5.1\"},\"end\":25122,\"start\":25101},{\"end\":26464,\"start\":26450},{\"attributes\":{\"n\":\"5.2\"},\"end\":26486,\"start\":26467},{\"attributes\":{\"n\":\"6\"},\"end\":27897,\"start\":27889},{\"attributes\":{\"n\":\"6.1\"},\"end\":28161,\"start\":28121},{\"attributes\":{\"n\":\"6.2\"},\"end\":29238,\"start\":29200},{\"attributes\":{\"n\":\"7\"},\"end\":30783,\"start\":30773},{\"end\":31505,\"start\":31479},{\"end\":31693,\"start\":31663},{\"end\":32506,\"start\":32473},{\"end\":33173,\"start\":33157},{\"end\":33203,\"start\":33176},{\"end\":33830,\"start\":33805},{\"end\":34207,\"start\":34184},{\"end\":35663,\"start\":35655},{\"end\":35871,\"start\":35863},{\"end\":36192,\"start\":36184},{\"end\":36379,\"start\":36371},{\"end\":36609,\"start\":36599},{\"end\":36830,\"start\":36822},{\"end\":37121,\"start\":37113},{\"end\":37203,\"start\":37199},{\"end\":37466,\"start\":37457},{\"end\":37865,\"start\":37856},{\"end\":38889,\"start\":38880}]", "table": "[{\"end\":37854,\"start\":37570},{\"end\":38299,\"start\":37867},{\"end\":38878,\"start\":38430},{\"end\":40041,\"start\":39045},{\"end\":40231,\"start\":40110}]", "figure_caption": "[{\"end\":35861,\"start\":35665},{\"end\":36182,\"start\":35873},{\"end\":36369,\"start\":36194},{\"end\":36597,\"start\":36381},{\"end\":36820,\"start\":36612},{\"end\":37111,\"start\":36832},{\"end\":37197,\"start\":37123},{\"end\":37332,\"start\":37205},{\"end\":37455,\"start\":37335},{\"end\":37524,\"start\":37469},{\"end\":37570,\"start\":37527},{\"end\":38430,\"start\":38302},{\"end\":39045,\"start\":38891},{\"end\":40110,\"start\":40044}]", "figure_ref": "[{\"end\":2947,\"start\":2941},{\"end\":3559,\"start\":3551},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9061,\"start\":9053},{\"end\":9915,\"start\":9907},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11741,\"start\":11733},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11905,\"start\":11897},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15815,\"start\":15807},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25677,\"start\":25669},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28399,\"start\":28391},{\"end\":28998,\"start\":28990},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29180,\"start\":29172},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29251,\"start\":29243},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":30207,\"start\":30198},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":30662,\"start\":30653},{\"end\":34284,\"start\":34275},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34762,\"start\":34753},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35382,\"start\":35373},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35476,\"start\":35467}]", "bib_author_first_name": "[{\"end\":40531,\"start\":40530},{\"end\":40541,\"start\":40540},{\"end\":40552,\"start\":40551},{\"end\":40563,\"start\":40562},{\"end\":40574,\"start\":40573},{\"end\":40584,\"start\":40583},{\"end\":40597,\"start\":40596},{\"end\":40868,\"start\":40867},{\"end\":40875,\"start\":40874},{\"end\":40882,\"start\":40881},{\"end\":40890,\"start\":40889},{\"end\":40899,\"start\":40898},{\"end\":41118,\"start\":41117},{\"end\":41125,\"start\":41124},{\"end\":41132,\"start\":41131},{\"end\":41139,\"start\":41138},{\"end\":41146,\"start\":41145},{\"end\":41154,\"start\":41153},{\"end\":41162,\"start\":41161},{\"end\":41169,\"start\":41168},{\"end\":41348,\"start\":41347},{\"end\":41355,\"start\":41354},{\"end\":41362,\"start\":41361},{\"end\":41571,\"start\":41570},{\"end\":41573,\"start\":41572},{\"end\":41582,\"start\":41581},{\"end\":41596,\"start\":41595},{\"end\":41606,\"start\":41605},{\"end\":41618,\"start\":41617},{\"end\":41627,\"start\":41626},{\"end\":41633,\"start\":41632},{\"end\":41645,\"start\":41644},{\"end\":41654,\"start\":41653},{\"end\":41662,\"start\":41661},{\"end\":41668,\"start\":41667},{\"end\":41676,\"start\":41675},{\"end\":41682,\"start\":41681},{\"end\":42001,\"start\":42000},{\"end\":42009,\"start\":42008},{\"end\":42017,\"start\":42016},{\"end\":42026,\"start\":42025},{\"end\":42034,\"start\":42033},{\"end\":42042,\"start\":42041},{\"end\":42182,\"start\":42181},{\"end\":42190,\"start\":42189},{\"end\":42198,\"start\":42197},{\"end\":42460,\"start\":42459},{\"end\":42468,\"start\":42467},{\"end\":42476,\"start\":42475},{\"end\":42486,\"start\":42485},{\"end\":42488,\"start\":42487},{\"end\":42494,\"start\":42493},{\"end\":42500,\"start\":42499},{\"end\":42648,\"start\":42647},{\"end\":42658,\"start\":42657},{\"end\":42666,\"start\":42665},{\"end\":42677,\"start\":42676},{\"end\":42910,\"start\":42909},{\"end\":42920,\"start\":42919},{\"end\":42928,\"start\":42927},{\"end\":43139,\"start\":43138},{\"end\":43149,\"start\":43148},{\"end\":43161,\"start\":43160},{\"end\":43390,\"start\":43389},{\"end\":43397,\"start\":43396},{\"end\":43406,\"start\":43405},{\"end\":43412,\"start\":43411},{\"end\":43420,\"start\":43419},{\"end\":43427,\"start\":43426},{\"end\":43434,\"start\":43433},{\"end\":43649,\"start\":43648},{\"end\":43656,\"start\":43655},{\"end\":43665,\"start\":43664},{\"end\":43671,\"start\":43670},{\"end\":43859,\"start\":43858},{\"end\":43865,\"start\":43864},{\"end\":43872,\"start\":43871},{\"end\":43879,\"start\":43878},{\"end\":43887,\"start\":43886},{\"end\":43889,\"start\":43888},{\"end\":43895,\"start\":43894},{\"end\":44048,\"start\":44047},{\"end\":44050,\"start\":44049},{\"end\":44060,\"start\":44059},{\"end\":44067,\"start\":44066},{\"end\":44075,\"start\":44074},{\"end\":44091,\"start\":44090},{\"end\":44099,\"start\":44098},{\"end\":44109,\"start\":44108},{\"end\":44122,\"start\":44121},{\"end\":44490,\"start\":44489},{\"end\":44496,\"start\":44495},{\"end\":44504,\"start\":44503},{\"end\":44511,\"start\":44510},{\"end\":44519,\"start\":44518},{\"end\":44526,\"start\":44525},{\"end\":44534,\"start\":44533},{\"end\":44545,\"start\":44544},{\"end\":44798,\"start\":44797},{\"end\":44807,\"start\":44806},{\"end\":44815,\"start\":44814},{\"end\":44822,\"start\":44821},{\"end\":44829,\"start\":44828},{\"end\":44831,\"start\":44830},{\"end\":44837,\"start\":44836},{\"end\":44989,\"start\":44988},{\"end\":44999,\"start\":44998},{\"end\":45009,\"start\":45008},{\"end\":45022,\"start\":45021},{\"end\":45024,\"start\":45023},{\"end\":45292,\"start\":45291},{\"end\":45304,\"start\":45303},{\"end\":45501,\"start\":45500},{\"end\":45508,\"start\":45507},{\"end\":45518,\"start\":45517},{\"end\":45715,\"start\":45714},{\"end\":45721,\"start\":45720},{\"end\":45728,\"start\":45727},{\"end\":45736,\"start\":45735},{\"end\":45743,\"start\":45742},{\"end\":45745,\"start\":45744},{\"end\":45752,\"start\":45751},{\"end\":45911,\"start\":45910},{\"end\":45917,\"start\":45916},{\"end\":45923,\"start\":45922},{\"end\":45930,\"start\":45929},{\"end\":45936,\"start\":45935},{\"end\":45942,\"start\":45941},{\"end\":46126,\"start\":46125},{\"end\":46133,\"start\":46132},{\"end\":46141,\"start\":46140},{\"end\":46152,\"start\":46151},{\"end\":46162,\"start\":46161},{\"end\":46169,\"start\":46168},{\"end\":46171,\"start\":46170},{\"end\":46177,\"start\":46176},{\"end\":46188,\"start\":46187},{\"end\":46198,\"start\":46197},{\"end\":46207,\"start\":46206},{\"end\":46429,\"start\":46428},{\"end\":46436,\"start\":46435},{\"end\":46448,\"start\":46447},{\"end\":46669,\"start\":46668},{\"end\":46676,\"start\":46675},{\"end\":46682,\"start\":46681},{\"end\":46691,\"start\":46690},{\"end\":46698,\"start\":46697},{\"end\":46706,\"start\":46705},{\"end\":46708,\"start\":46707},{\"end\":46717,\"start\":46716},{\"end\":46935,\"start\":46934},{\"end\":46942,\"start\":46941},{\"end\":46950,\"start\":46949},{\"end\":46957,\"start\":46956},{\"end\":47151,\"start\":47150},{\"end\":47157,\"start\":47156},{\"end\":47166,\"start\":47165},{\"end\":47168,\"start\":47167},{\"end\":47177,\"start\":47176},{\"end\":47391,\"start\":47390},{\"end\":47398,\"start\":47397},{\"end\":47406,\"start\":47405},{\"end\":47606,\"start\":47605},{\"end\":47618,\"start\":47617},{\"end\":47770,\"start\":47769},{\"end\":47778,\"start\":47777},{\"end\":47780,\"start\":47779},{\"end\":47957,\"start\":47956},{\"end\":47959,\"start\":47958},{\"end\":47965,\"start\":47964},{\"end\":47973,\"start\":47972},{\"end\":47983,\"start\":47982},{\"end\":47985,\"start\":47984},{\"end\":48195,\"start\":48194},{\"end\":48197,\"start\":48196},{\"end\":48203,\"start\":48202},{\"end\":48213,\"start\":48212},{\"end\":48219,\"start\":48218},{\"end\":48221,\"start\":48220},{\"end\":48433,\"start\":48432},{\"end\":48435,\"start\":48434},{\"end\":48441,\"start\":48440},{\"end\":48447,\"start\":48446},{\"end\":48453,\"start\":48452},{\"end\":48455,\"start\":48454},{\"end\":48663,\"start\":48662},{\"end\":48665,\"start\":48664},{\"end\":48671,\"start\":48670},{\"end\":48678,\"start\":48677},{\"end\":48684,\"start\":48683},{\"end\":48690,\"start\":48689},{\"end\":48692,\"start\":48691},{\"end\":48900,\"start\":48899},{\"end\":48902,\"start\":48901},{\"end\":48908,\"start\":48907},{\"end\":48914,\"start\":48913},{\"end\":48926,\"start\":48925},{\"end\":48933,\"start\":48932},{\"end\":48940,\"start\":48939},{\"end\":48942,\"start\":48941},{\"end\":49186,\"start\":49185},{\"end\":49188,\"start\":49187},{\"end\":49194,\"start\":49193},{\"end\":49200,\"start\":49199},{\"end\":49206,\"start\":49205},{\"end\":49208,\"start\":49207},{\"end\":49414,\"start\":49413},{\"end\":49429,\"start\":49428},{\"end\":49440,\"start\":49439},{\"end\":49447,\"start\":49446},{\"end\":49449,\"start\":49448},{\"end\":49455,\"start\":49454},{\"end\":49658,\"start\":49657},{\"end\":49669,\"start\":49668},{\"end\":49671,\"start\":49670},{\"end\":49681,\"start\":49680},{\"end\":49813,\"start\":49812},{\"end\":49824,\"start\":49823},{\"end\":49834,\"start\":49833},{\"end\":49841,\"start\":49840},{\"end\":49854,\"start\":49853},{\"end\":49856,\"start\":49855},{\"end\":50128,\"start\":50127},{\"end\":50135,\"start\":50134},{\"end\":50142,\"start\":50141},{\"end\":50151,\"start\":50150},{\"end\":50159,\"start\":50158},{\"end\":50166,\"start\":50165},{\"end\":50174,\"start\":50173},{\"end\":50411,\"start\":50410},{\"end\":50418,\"start\":50417},{\"end\":50426,\"start\":50425},{\"end\":50623,\"start\":50622},{\"end\":50630,\"start\":50629},{\"end\":50638,\"start\":50637},{\"end\":50645,\"start\":50644},{\"end\":50653,\"start\":50652},{\"end\":50794,\"start\":50793},{\"end\":50807,\"start\":50806},{\"end\":50815,\"start\":50814},{\"end\":50823,\"start\":50822},{\"end\":50840,\"start\":50839},{\"end\":50853,\"start\":50852},{\"end\":50860,\"start\":50859},{\"end\":51202,\"start\":51201},{\"end\":51214,\"start\":51213},{\"end\":51224,\"start\":51223},{\"end\":51416,\"start\":51415},{\"end\":51422,\"start\":51421},{\"end\":51433,\"start\":51432},{\"end\":51440,\"start\":51439},{\"end\":51448,\"start\":51447},{\"end\":51463,\"start\":51462},{\"end\":51465,\"start\":51464},{\"end\":51473,\"start\":51472},{\"end\":51645,\"start\":51644},{\"end\":51652,\"start\":51651},{\"end\":51660,\"start\":51659},{\"end\":51668,\"start\":51667},{\"end\":51681,\"start\":51680},{\"end\":51692,\"start\":51691},{\"end\":51702,\"start\":51701},{\"end\":51704,\"start\":51703},{\"end\":51936,\"start\":51935},{\"end\":51943,\"start\":51942},{\"end\":51945,\"start\":51944},{\"end\":52194,\"start\":52193},{\"end\":52209,\"start\":52208},{\"end\":52217,\"start\":52216},{\"end\":52227,\"start\":52226},{\"end\":52229,\"start\":52228},{\"end\":52420,\"start\":52419},{\"end\":52430,\"start\":52429},{\"end\":52432,\"start\":52431},{\"end\":52438,\"start\":52437},{\"end\":52440,\"start\":52439},{\"end\":52452,\"start\":52451},{\"end\":52466,\"start\":52465},{\"end\":52478,\"start\":52477},{\"end\":52480,\"start\":52479},{\"end\":52731,\"start\":52730},{\"end\":52739,\"start\":52738},{\"end\":52745,\"start\":52744},{\"end\":52752,\"start\":52751},{\"end\":52759,\"start\":52758},{\"end\":52766,\"start\":52765},{\"end\":52773,\"start\":52772},{\"end\":53003,\"start\":53002},{\"end\":53011,\"start\":53010},{\"end\":53018,\"start\":53017},{\"end\":53025,\"start\":53024},{\"end\":53032,\"start\":53031},{\"end\":53229,\"start\":53228},{\"end\":53237,\"start\":53236},{\"end\":53244,\"start\":53243},{\"end\":53251,\"start\":53250},{\"end\":53258,\"start\":53257},{\"end\":53389,\"start\":53388},{\"end\":53391,\"start\":53390},{\"end\":53399,\"start\":53398},{\"end\":53406,\"start\":53405},{\"end\":53408,\"start\":53407},{\"end\":53415,\"start\":53414},{\"end\":53417,\"start\":53416},{\"end\":53424,\"start\":53423},{\"end\":53725,\"start\":53724},{\"end\":53727,\"start\":53726},{\"end\":53735,\"start\":53734},{\"end\":53742,\"start\":53741},{\"end\":53744,\"start\":53743},{\"end\":53751,\"start\":53750},{\"end\":53753,\"start\":53752},{\"end\":53760,\"start\":53759},{\"end\":54004,\"start\":54003},{\"end\":54012,\"start\":54011},{\"end\":54020,\"start\":54019},{\"end\":54027,\"start\":54026},{\"end\":54034,\"start\":54033},{\"end\":54041,\"start\":54040},{\"end\":54049,\"start\":54048},{\"end\":54056,\"start\":54055},{\"end\":54284,\"start\":54283},{\"end\":54292,\"start\":54291},{\"end\":54299,\"start\":54298},{\"end\":54306,\"start\":54305},{\"end\":54308,\"start\":54307},{\"end\":54317,\"start\":54316},{\"end\":54319,\"start\":54318},{\"end\":54332,\"start\":54331},{\"end\":54334,\"start\":54333},{\"end\":54567,\"start\":54566},{\"end\":54575,\"start\":54574},{\"end\":54782,\"start\":54781},{\"end\":54788,\"start\":54787},{\"end\":54795,\"start\":54794},{\"end\":54804,\"start\":54803},{\"end\":54812,\"start\":54811},{\"end\":54819,\"start\":54818},{\"end\":54825,\"start\":54824},{\"end\":54833,\"start\":54832},{\"end\":54842,\"start\":54841},{\"end\":54849,\"start\":54848},{\"end\":55132,\"start\":55131},{\"end\":55138,\"start\":55137},{\"end\":55144,\"start\":55143},{\"end\":55346,\"start\":55345},{\"end\":55352,\"start\":55351},{\"end\":55358,\"start\":55357},{\"end\":55366,\"start\":55365},{\"end\":55374,\"start\":55373},{\"end\":55383,\"start\":55382},{\"end\":55394,\"start\":55393},{\"end\":55588,\"start\":55587},{\"end\":55594,\"start\":55593},{\"end\":55601,\"start\":55600},{\"end\":55607,\"start\":55606},{\"end\":55615,\"start\":55614},{\"end\":55902,\"start\":55901},{\"end\":55909,\"start\":55908},{\"end\":55916,\"start\":55915},{\"end\":56101,\"start\":56100},{\"end\":56109,\"start\":56108},{\"end\":56117,\"start\":56116},{\"end\":56126,\"start\":56125},{\"end\":56132,\"start\":56131},{\"end\":56140,\"start\":56139},{\"end\":56151,\"start\":56150},{\"end\":56398,\"start\":56397},{\"end\":56406,\"start\":56405},{\"end\":56413,\"start\":56412},{\"end\":56420,\"start\":56419},{\"end\":56428,\"start\":56427},{\"end\":56644,\"start\":56643},{\"end\":56653,\"start\":56652},{\"end\":56661,\"start\":56660},{\"end\":56668,\"start\":56667},{\"end\":56917,\"start\":56916},{\"end\":56926,\"start\":56925},{\"end\":56934,\"start\":56933},{\"end\":56943,\"start\":56942},{\"end\":56950,\"start\":56949},{\"end\":56956,\"start\":56955},{\"end\":56964,\"start\":56963},{\"end\":57238,\"start\":57237},{\"end\":57246,\"start\":57245},{\"end\":57447,\"start\":57446},{\"end\":57454,\"start\":57453},{\"end\":57461,\"start\":57460},{\"end\":57469,\"start\":57468},{\"end\":57479,\"start\":57478},{\"end\":57683,\"start\":57682},{\"end\":57691,\"start\":57690},{\"end\":57693,\"start\":57692},{\"end\":57860,\"start\":57859},{\"end\":57868,\"start\":57867},{\"end\":57881,\"start\":57880},{\"end\":57891,\"start\":57890},{\"end\":57893,\"start\":57892}]", "bib_author_last_name": "[{\"end\":40538,\"start\":40532},{\"end\":40549,\"start\":40542},{\"end\":40560,\"start\":40553},{\"end\":40571,\"start\":40564},{\"end\":40581,\"start\":40575},{\"end\":40594,\"start\":40585},{\"end\":40602,\"start\":40598},{\"end\":40872,\"start\":40869},{\"end\":40879,\"start\":40876},{\"end\":40887,\"start\":40883},{\"end\":40896,\"start\":40891},{\"end\":40903,\"start\":40900},{\"end\":41122,\"start\":41119},{\"end\":41129,\"start\":41126},{\"end\":41136,\"start\":41133},{\"end\":41143,\"start\":41140},{\"end\":41151,\"start\":41147},{\"end\":41159,\"start\":41155},{\"end\":41166,\"start\":41163},{\"end\":41173,\"start\":41170},{\"end\":41352,\"start\":41349},{\"end\":41359,\"start\":41356},{\"end\":41366,\"start\":41363},{\"end\":41579,\"start\":41574},{\"end\":41593,\"start\":41583},{\"end\":41603,\"start\":41597},{\"end\":41615,\"start\":41607},{\"end\":41624,\"start\":41619},{\"end\":41630,\"start\":41628},{\"end\":41642,\"start\":41634},{\"end\":41651,\"start\":41646},{\"end\":41659,\"start\":41655},{\"end\":41665,\"start\":41663},{\"end\":41673,\"start\":41669},{\"end\":41679,\"start\":41677},{\"end\":41685,\"start\":41683},{\"end\":42006,\"start\":42002},{\"end\":42014,\"start\":42010},{\"end\":42023,\"start\":42018},{\"end\":42031,\"start\":42027},{\"end\":42039,\"start\":42035},{\"end\":42046,\"start\":42043},{\"end\":42187,\"start\":42183},{\"end\":42195,\"start\":42191},{\"end\":42207,\"start\":42199},{\"end\":42465,\"start\":42461},{\"end\":42473,\"start\":42469},{\"end\":42483,\"start\":42477},{\"end\":42491,\"start\":42489},{\"end\":42497,\"start\":42495},{\"end\":42508,\"start\":42501},{\"end\":42655,\"start\":42649},{\"end\":42663,\"start\":42659},{\"end\":42674,\"start\":42667},{\"end\":42685,\"start\":42678},{\"end\":42917,\"start\":42911},{\"end\":42925,\"start\":42921},{\"end\":42936,\"start\":42929},{\"end\":43146,\"start\":43140},{\"end\":43158,\"start\":43150},{\"end\":43176,\"start\":43162},{\"end\":43394,\"start\":43391},{\"end\":43403,\"start\":43398},{\"end\":43409,\"start\":43407},{\"end\":43417,\"start\":43413},{\"end\":43424,\"start\":43421},{\"end\":43431,\"start\":43428},{\"end\":43438,\"start\":43435},{\"end\":43653,\"start\":43650},{\"end\":43662,\"start\":43657},{\"end\":43668,\"start\":43666},{\"end\":43676,\"start\":43672},{\"end\":43862,\"start\":43860},{\"end\":43869,\"start\":43866},{\"end\":43876,\"start\":43873},{\"end\":43884,\"start\":43880},{\"end\":43892,\"start\":43890},{\"end\":43899,\"start\":43896},{\"end\":44057,\"start\":44051},{\"end\":44064,\"start\":44061},{\"end\":44072,\"start\":44068},{\"end\":44088,\"start\":44076},{\"end\":44096,\"start\":44092},{\"end\":44106,\"start\":44100},{\"end\":44119,\"start\":44110},{\"end\":44127,\"start\":44123},{\"end\":44493,\"start\":44491},{\"end\":44501,\"start\":44497},{\"end\":44508,\"start\":44505},{\"end\":44516,\"start\":44512},{\"end\":44523,\"start\":44520},{\"end\":44531,\"start\":44527},{\"end\":44542,\"start\":44535},{\"end\":44553,\"start\":44546},{\"end\":44804,\"start\":44799},{\"end\":44812,\"start\":44808},{\"end\":44819,\"start\":44816},{\"end\":44826,\"start\":44823},{\"end\":44834,\"start\":44832},{\"end\":44841,\"start\":44838},{\"end\":44996,\"start\":44990},{\"end\":45006,\"start\":45000},{\"end\":45019,\"start\":45010},{\"end\":45031,\"start\":45025},{\"end\":45301,\"start\":45293},{\"end\":45315,\"start\":45305},{\"end\":45505,\"start\":45502},{\"end\":45515,\"start\":45509},{\"end\":45523,\"start\":45519},{\"end\":45718,\"start\":45716},{\"end\":45725,\"start\":45722},{\"end\":45733,\"start\":45729},{\"end\":45740,\"start\":45737},{\"end\":45749,\"start\":45746},{\"end\":45756,\"start\":45753},{\"end\":45914,\"start\":45912},{\"end\":45920,\"start\":45918},{\"end\":45927,\"start\":45924},{\"end\":45933,\"start\":45931},{\"end\":45939,\"start\":45937},{\"end\":45947,\"start\":45943},{\"end\":46130,\"start\":46127},{\"end\":46138,\"start\":46134},{\"end\":46149,\"start\":46142},{\"end\":46159,\"start\":46153},{\"end\":46166,\"start\":46163},{\"end\":46174,\"start\":46172},{\"end\":46185,\"start\":46178},{\"end\":46195,\"start\":46189},{\"end\":46204,\"start\":46199},{\"end\":46214,\"start\":46208},{\"end\":46433,\"start\":46430},{\"end\":46445,\"start\":46437},{\"end\":46453,\"start\":46449},{\"end\":46673,\"start\":46670},{\"end\":46679,\"start\":46677},{\"end\":46688,\"start\":46683},{\"end\":46695,\"start\":46692},{\"end\":46703,\"start\":46699},{\"end\":46714,\"start\":46709},{\"end\":46721,\"start\":46718},{\"end\":46939,\"start\":46936},{\"end\":46947,\"start\":46943},{\"end\":46954,\"start\":46951},{\"end\":46961,\"start\":46958},{\"end\":47154,\"start\":47152},{\"end\":47163,\"start\":47158},{\"end\":47174,\"start\":47169},{\"end\":47181,\"start\":47178},{\"end\":47395,\"start\":47392},{\"end\":47403,\"start\":47399},{\"end\":47409,\"start\":47407},{\"end\":47615,\"start\":47607},{\"end\":47626,\"start\":47619},{\"end\":47775,\"start\":47771},{\"end\":47787,\"start\":47781},{\"end\":47962,\"start\":47960},{\"end\":47970,\"start\":47966},{\"end\":47980,\"start\":47974},{\"end\":47992,\"start\":47986},{\"end\":48200,\"start\":48198},{\"end\":48210,\"start\":48204},{\"end\":48216,\"start\":48214},{\"end\":48228,\"start\":48222},{\"end\":48438,\"start\":48436},{\"end\":48444,\"start\":48442},{\"end\":48450,\"start\":48448},{\"end\":48462,\"start\":48456},{\"end\":48668,\"start\":48666},{\"end\":48675,\"start\":48672},{\"end\":48681,\"start\":48679},{\"end\":48687,\"start\":48685},{\"end\":48699,\"start\":48693},{\"end\":48905,\"start\":48903},{\"end\":48911,\"start\":48909},{\"end\":48923,\"start\":48915},{\"end\":48930,\"start\":48927},{\"end\":48937,\"start\":48934},{\"end\":48949,\"start\":48943},{\"end\":49191,\"start\":49189},{\"end\":49197,\"start\":49195},{\"end\":49203,\"start\":49201},{\"end\":49215,\"start\":49209},{\"end\":49426,\"start\":49415},{\"end\":49437,\"start\":49430},{\"end\":49444,\"start\":49441},{\"end\":49452,\"start\":49450},{\"end\":49462,\"start\":49456},{\"end\":49666,\"start\":49659},{\"end\":49678,\"start\":49672},{\"end\":49688,\"start\":49682},{\"end\":49821,\"start\":49814},{\"end\":49831,\"start\":49825},{\"end\":49838,\"start\":49835},{\"end\":49851,\"start\":49842},{\"end\":49861,\"start\":49857},{\"end\":50132,\"start\":50129},{\"end\":50139,\"start\":50136},{\"end\":50148,\"start\":50143},{\"end\":50156,\"start\":50152},{\"end\":50163,\"start\":50160},{\"end\":50171,\"start\":50167},{\"end\":50177,\"start\":50175},{\"end\":50415,\"start\":50412},{\"end\":50423,\"start\":50419},{\"end\":50429,\"start\":50427},{\"end\":50627,\"start\":50624},{\"end\":50635,\"start\":50631},{\"end\":50642,\"start\":50639},{\"end\":50650,\"start\":50646},{\"end\":50656,\"start\":50654},{\"end\":50804,\"start\":50795},{\"end\":50812,\"start\":50808},{\"end\":50820,\"start\":50816},{\"end\":50837,\"start\":50824},{\"end\":50850,\"start\":50841},{\"end\":50857,\"start\":50854},{\"end\":50871,\"start\":50861},{\"end\":51211,\"start\":51203},{\"end\":51221,\"start\":51215},{\"end\":51233,\"start\":51225},{\"end\":51419,\"start\":51417},{\"end\":51430,\"start\":51423},{\"end\":51437,\"start\":51434},{\"end\":51445,\"start\":51441},{\"end\":51460,\"start\":51449},{\"end\":51470,\"start\":51466},{\"end\":51479,\"start\":51474},{\"end\":51649,\"start\":51646},{\"end\":51657,\"start\":51653},{\"end\":51665,\"start\":51661},{\"end\":51678,\"start\":51669},{\"end\":51689,\"start\":51682},{\"end\":51699,\"start\":51693},{\"end\":51707,\"start\":51705},{\"end\":51940,\"start\":51937},{\"end\":51948,\"start\":51946},{\"end\":52206,\"start\":52195},{\"end\":52214,\"start\":52210},{\"end\":52224,\"start\":52218},{\"end\":52234,\"start\":52230},{\"end\":52427,\"start\":52421},{\"end\":52435,\"start\":52433},{\"end\":52449,\"start\":52441},{\"end\":52463,\"start\":52453},{\"end\":52475,\"start\":52467},{\"end\":52487,\"start\":52481},{\"end\":52736,\"start\":52732},{\"end\":52742,\"start\":52740},{\"end\":52749,\"start\":52746},{\"end\":52756,\"start\":52753},{\"end\":52763,\"start\":52760},{\"end\":52770,\"start\":52767},{\"end\":52777,\"start\":52774},{\"end\":53008,\"start\":53004},{\"end\":53015,\"start\":53012},{\"end\":53022,\"start\":53019},{\"end\":53029,\"start\":53026},{\"end\":53036,\"start\":53033},{\"end\":53234,\"start\":53230},{\"end\":53241,\"start\":53238},{\"end\":53248,\"start\":53245},{\"end\":53255,\"start\":53252},{\"end\":53262,\"start\":53259},{\"end\":53396,\"start\":53392},{\"end\":53403,\"start\":53400},{\"end\":53412,\"start\":53409},{\"end\":53421,\"start\":53418},{\"end\":53429,\"start\":53425},{\"end\":53732,\"start\":53728},{\"end\":53739,\"start\":53736},{\"end\":53748,\"start\":53745},{\"end\":53757,\"start\":53754},{\"end\":53765,\"start\":53761},{\"end\":54009,\"start\":54005},{\"end\":54017,\"start\":54013},{\"end\":54024,\"start\":54021},{\"end\":54031,\"start\":54028},{\"end\":54038,\"start\":54035},{\"end\":54046,\"start\":54042},{\"end\":54053,\"start\":54050},{\"end\":54060,\"start\":54057},{\"end\":54289,\"start\":54285},{\"end\":54296,\"start\":54293},{\"end\":54303,\"start\":54300},{\"end\":54314,\"start\":54309},{\"end\":54329,\"start\":54320},{\"end\":54342,\"start\":54335},{\"end\":54572,\"start\":54568},{\"end\":54578,\"start\":54576},{\"end\":54785,\"start\":54783},{\"end\":54792,\"start\":54789},{\"end\":54801,\"start\":54796},{\"end\":54809,\"start\":54805},{\"end\":54816,\"start\":54813},{\"end\":54822,\"start\":54820},{\"end\":54830,\"start\":54826},{\"end\":54839,\"start\":54834},{\"end\":54846,\"start\":54843},{\"end\":54857,\"start\":54850},{\"end\":55135,\"start\":55133},{\"end\":55141,\"start\":55139},{\"end\":55150,\"start\":55145},{\"end\":55349,\"start\":55347},{\"end\":55355,\"start\":55353},{\"end\":55363,\"start\":55359},{\"end\":55371,\"start\":55367},{\"end\":55380,\"start\":55375},{\"end\":55391,\"start\":55384},{\"end\":55403,\"start\":55395},{\"end\":55591,\"start\":55589},{\"end\":55598,\"start\":55595},{\"end\":55604,\"start\":55602},{\"end\":55612,\"start\":55608},{\"end\":55620,\"start\":55616},{\"end\":55906,\"start\":55903},{\"end\":55913,\"start\":55910},{\"end\":55919,\"start\":55917},{\"end\":56106,\"start\":56102},{\"end\":56114,\"start\":56110},{\"end\":56123,\"start\":56118},{\"end\":56129,\"start\":56127},{\"end\":56137,\"start\":56133},{\"end\":56148,\"start\":56141},{\"end\":56159,\"start\":56152},{\"end\":56403,\"start\":56399},{\"end\":56410,\"start\":56407},{\"end\":56417,\"start\":56414},{\"end\":56425,\"start\":56421},{\"end\":56432,\"start\":56429},{\"end\":56650,\"start\":56645},{\"end\":56658,\"start\":56654},{\"end\":56665,\"start\":56662},{\"end\":56672,\"start\":56669},{\"end\":56923,\"start\":56918},{\"end\":56931,\"start\":56927},{\"end\":56940,\"start\":56935},{\"end\":56947,\"start\":56944},{\"end\":56953,\"start\":56951},{\"end\":56961,\"start\":56957},{\"end\":56972,\"start\":56965},{\"end\":57243,\"start\":57239},{\"end\":57252,\"start\":57247},{\"end\":57451,\"start\":57448},{\"end\":57458,\"start\":57455},{\"end\":57466,\"start\":57462},{\"end\":57476,\"start\":57470},{\"end\":57482,\"start\":57480},{\"end\":57688,\"start\":57684},{\"end\":57696,\"start\":57694},{\"end\":57865,\"start\":57861},{\"end\":57878,\"start\":57869},{\"end\":57888,\"start\":57882},{\"end\":57896,\"start\":57894}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":40789,\"start\":40452},{\"attributes\":{\"id\":\"b1\"},\"end\":41048,\"start\":40791},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":209069926},\"end\":41345,\"start\":41050},{\"attributes\":{\"id\":\"b3\"},\"end\":41568,\"start\":41347},{\"attributes\":{\"id\":\"b4\"},\"end\":41952,\"start\":41570},{\"attributes\":{\"id\":\"b5\"},\"end\":42179,\"start\":41954},{\"attributes\":{\"id\":\"b6\"},\"end\":42404,\"start\":42181},{\"attributes\":{\"id\":\"b7\"},\"end\":42645,\"start\":42406},{\"attributes\":{\"id\":\"b8\"},\"end\":42836,\"start\":42647},{\"attributes\":{\"id\":\"b9\"},\"end\":43063,\"start\":42838},{\"attributes\":{\"id\":\"b10\"},\"end\":43316,\"start\":43065},{\"attributes\":{\"id\":\"b11\"},\"end\":43595,\"start\":43318},{\"attributes\":{\"id\":\"b12\"},\"end\":43786,\"start\":43597},{\"attributes\":{\"id\":\"b13\"},\"end\":44045,\"start\":43788},{\"attributes\":{\"id\":\"b14\"},\"end\":44414,\"start\":44047},{\"attributes\":{\"id\":\"b15\"},\"end\":44729,\"start\":44416},{\"attributes\":{\"id\":\"b16\"},\"end\":44986,\"start\":44731},{\"attributes\":{\"id\":\"b17\"},\"end\":45219,\"start\":44988},{\"attributes\":{\"id\":\"b18\"},\"end\":45436,\"start\":45221},{\"attributes\":{\"id\":\"b19\"},\"end\":45637,\"start\":45438},{\"attributes\":{\"id\":\"b20\"},\"end\":45908,\"start\":45639},{\"attributes\":{\"id\":\"b21\"},\"end\":46121,\"start\":45910},{\"attributes\":{\"id\":\"b22\"},\"end\":46426,\"start\":46123},{\"attributes\":{\"id\":\"b23\"},\"end\":46593,\"start\":46428},{\"attributes\":{\"id\":\"b24\"},\"end\":46884,\"start\":46595},{\"attributes\":{\"id\":\"b25\"},\"end\":47073,\"start\":46886},{\"attributes\":{\"id\":\"b26\"},\"end\":47318,\"start\":47075},{\"attributes\":{\"id\":\"b27\"},\"end\":47527,\"start\":47320},{\"attributes\":{\"id\":\"b28\"},\"end\":47751,\"start\":47529},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2447135},\"end\":47880,\"start\":47753},{\"attributes\":{\"id\":\"b30\"},\"end\":48133,\"start\":47882},{\"attributes\":{\"id\":\"b31\"},\"end\":48352,\"start\":48135},{\"attributes\":{\"id\":\"b32\"},\"end\":48601,\"start\":48354},{\"attributes\":{\"id\":\"b33\"},\"end\":48828,\"start\":48603},{\"attributes\":{\"id\":\"b34\"},\"end\":49103,\"start\":48830},{\"attributes\":{\"id\":\"b35\"},\"end\":49362,\"start\":49105},{\"attributes\":{\"id\":\"b36\"},\"end\":49593,\"start\":49364},{\"attributes\":{\"id\":\"b37\"},\"end\":49810,\"start\":49595},{\"attributes\":{\"id\":\"b38\"},\"end\":50055,\"start\":49812},{\"attributes\":{\"id\":\"b39\"},\"end\":50335,\"start\":50057},{\"attributes\":{\"id\":\"b40\"},\"end\":50550,\"start\":50337},{\"attributes\":{\"id\":\"b41\"},\"end\":50791,\"start\":50552},{\"attributes\":{\"id\":\"b42\"},\"end\":51140,\"start\":50793},{\"attributes\":{\"id\":\"b43\"},\"end\":51351,\"start\":51142},{\"attributes\":{\"id\":\"b44\"},\"end\":51642,\"start\":51353},{\"attributes\":{\"id\":\"b45\"},\"end\":51933,\"start\":51644},{\"attributes\":{\"id\":\"b46\"},\"end\":52142,\"start\":51935},{\"attributes\":{\"id\":\"b47\"},\"end\":52355,\"start\":52144},{\"attributes\":{\"id\":\"b48\"},\"end\":52652,\"start\":52357},{\"attributes\":{\"id\":\"b49\"},\"end\":52935,\"start\":52654},{\"attributes\":{\"id\":\"b50\"},\"end\":53168,\"start\":52937},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":219568058},\"end\":53386,\"start\":53170},{\"attributes\":{\"id\":\"b52\"},\"end\":53659,\"start\":53388},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":216883830},\"end\":53924,\"start\":53661},{\"attributes\":{\"id\":\"b54\"},\"end\":54233,\"start\":53926},{\"attributes\":{\"id\":\"b55\"},\"end\":54492,\"start\":54235},{\"attributes\":{\"id\":\"b56\"},\"end\":54685,\"start\":54494},{\"attributes\":{\"id\":\"b57\"},\"end\":55070,\"start\":54687},{\"attributes\":{\"id\":\"b58\"},\"end\":55257,\"start\":55072},{\"attributes\":{\"id\":\"b59\"},\"end\":55585,\"start\":55259},{\"attributes\":{\"id\":\"b60\"},\"end\":55848,\"start\":55587},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":52957856},\"end\":56021,\"start\":55850},{\"attributes\":{\"id\":\"b62\"},\"end\":56338,\"start\":56023},{\"attributes\":{\"id\":\"b63\"},\"end\":56557,\"start\":56340},{\"attributes\":{\"id\":\"b64\"},\"end\":56818,\"start\":56559},{\"attributes\":{\"id\":\"b65\"},\"end\":57162,\"start\":56820},{\"attributes\":{\"id\":\"b66\"},\"end\":57367,\"start\":57164},{\"attributes\":{\"id\":\"b67\"},\"end\":57624,\"start\":57369},{\"attributes\":{\"id\":\"b68\"},\"end\":57789,\"start\":57626},{\"attributes\":{\"id\":\"b69\"},\"end\":58032,\"start\":57791}]", "bib_title": "[{\"end\":41115,\"start\":41050},{\"end\":47767,\"start\":47753},{\"end\":53226,\"start\":53170},{\"end\":53722,\"start\":53661},{\"end\":55899,\"start\":55850}]", "bib_author": "[{\"end\":40540,\"start\":40530},{\"end\":40551,\"start\":40540},{\"end\":40562,\"start\":40551},{\"end\":40573,\"start\":40562},{\"end\":40583,\"start\":40573},{\"end\":40596,\"start\":40583},{\"end\":40604,\"start\":40596},{\"end\":40874,\"start\":40867},{\"end\":40881,\"start\":40874},{\"end\":40889,\"start\":40881},{\"end\":40898,\"start\":40889},{\"end\":40905,\"start\":40898},{\"end\":41124,\"start\":41117},{\"end\":41131,\"start\":41124},{\"end\":41138,\"start\":41131},{\"end\":41145,\"start\":41138},{\"end\":41153,\"start\":41145},{\"end\":41161,\"start\":41153},{\"end\":41168,\"start\":41161},{\"end\":41175,\"start\":41168},{\"end\":41354,\"start\":41347},{\"end\":41361,\"start\":41354},{\"end\":41368,\"start\":41361},{\"end\":41581,\"start\":41570},{\"end\":41595,\"start\":41581},{\"end\":41605,\"start\":41595},{\"end\":41617,\"start\":41605},{\"end\":41626,\"start\":41617},{\"end\":41632,\"start\":41626},{\"end\":41644,\"start\":41632},{\"end\":41653,\"start\":41644},{\"end\":41661,\"start\":41653},{\"end\":41667,\"start\":41661},{\"end\":41675,\"start\":41667},{\"end\":41681,\"start\":41675},{\"end\":41687,\"start\":41681},{\"end\":42008,\"start\":42000},{\"end\":42016,\"start\":42008},{\"end\":42025,\"start\":42016},{\"end\":42033,\"start\":42025},{\"end\":42041,\"start\":42033},{\"end\":42048,\"start\":42041},{\"end\":42189,\"start\":42181},{\"end\":42197,\"start\":42189},{\"end\":42209,\"start\":42197},{\"end\":42467,\"start\":42459},{\"end\":42475,\"start\":42467},{\"end\":42485,\"start\":42475},{\"end\":42493,\"start\":42485},{\"end\":42499,\"start\":42493},{\"end\":42510,\"start\":42499},{\"end\":42657,\"start\":42647},{\"end\":42665,\"start\":42657},{\"end\":42676,\"start\":42665},{\"end\":42687,\"start\":42676},{\"end\":42919,\"start\":42909},{\"end\":42927,\"start\":42919},{\"end\":42938,\"start\":42927},{\"end\":43148,\"start\":43138},{\"end\":43160,\"start\":43148},{\"end\":43178,\"start\":43160},{\"end\":43396,\"start\":43389},{\"end\":43405,\"start\":43396},{\"end\":43411,\"start\":43405},{\"end\":43419,\"start\":43411},{\"end\":43426,\"start\":43419},{\"end\":43433,\"start\":43426},{\"end\":43440,\"start\":43433},{\"end\":43655,\"start\":43648},{\"end\":43664,\"start\":43655},{\"end\":43670,\"start\":43664},{\"end\":43678,\"start\":43670},{\"end\":43864,\"start\":43858},{\"end\":43871,\"start\":43864},{\"end\":43878,\"start\":43871},{\"end\":43886,\"start\":43878},{\"end\":43894,\"start\":43886},{\"end\":43901,\"start\":43894},{\"end\":44059,\"start\":44047},{\"end\":44066,\"start\":44059},{\"end\":44074,\"start\":44066},{\"end\":44090,\"start\":44074},{\"end\":44098,\"start\":44090},{\"end\":44108,\"start\":44098},{\"end\":44121,\"start\":44108},{\"end\":44129,\"start\":44121},{\"end\":44495,\"start\":44489},{\"end\":44503,\"start\":44495},{\"end\":44510,\"start\":44503},{\"end\":44518,\"start\":44510},{\"end\":44525,\"start\":44518},{\"end\":44533,\"start\":44525},{\"end\":44544,\"start\":44533},{\"end\":44555,\"start\":44544},{\"end\":44806,\"start\":44797},{\"end\":44814,\"start\":44806},{\"end\":44821,\"start\":44814},{\"end\":44828,\"start\":44821},{\"end\":44836,\"start\":44828},{\"end\":44843,\"start\":44836},{\"end\":44998,\"start\":44988},{\"end\":45008,\"start\":44998},{\"end\":45021,\"start\":45008},{\"end\":45033,\"start\":45021},{\"end\":45303,\"start\":45291},{\"end\":45317,\"start\":45303},{\"end\":45507,\"start\":45500},{\"end\":45517,\"start\":45507},{\"end\":45525,\"start\":45517},{\"end\":45720,\"start\":45714},{\"end\":45727,\"start\":45720},{\"end\":45735,\"start\":45727},{\"end\":45742,\"start\":45735},{\"end\":45751,\"start\":45742},{\"end\":45758,\"start\":45751},{\"end\":45916,\"start\":45910},{\"end\":45922,\"start\":45916},{\"end\":45929,\"start\":45922},{\"end\":45935,\"start\":45929},{\"end\":45941,\"start\":45935},{\"end\":45949,\"start\":45941},{\"end\":46132,\"start\":46125},{\"end\":46140,\"start\":46132},{\"end\":46151,\"start\":46140},{\"end\":46161,\"start\":46151},{\"end\":46168,\"start\":46161},{\"end\":46176,\"start\":46168},{\"end\":46187,\"start\":46176},{\"end\":46197,\"start\":46187},{\"end\":46206,\"start\":46197},{\"end\":46216,\"start\":46206},{\"end\":46435,\"start\":46428},{\"end\":46447,\"start\":46435},{\"end\":46455,\"start\":46447},{\"end\":46675,\"start\":46668},{\"end\":46681,\"start\":46675},{\"end\":46690,\"start\":46681},{\"end\":46697,\"start\":46690},{\"end\":46705,\"start\":46697},{\"end\":46716,\"start\":46705},{\"end\":46723,\"start\":46716},{\"end\":46941,\"start\":46934},{\"end\":46949,\"start\":46941},{\"end\":46956,\"start\":46949},{\"end\":46963,\"start\":46956},{\"end\":47156,\"start\":47150},{\"end\":47165,\"start\":47156},{\"end\":47176,\"start\":47165},{\"end\":47183,\"start\":47176},{\"end\":47397,\"start\":47390},{\"end\":47405,\"start\":47397},{\"end\":47411,\"start\":47405},{\"end\":47617,\"start\":47605},{\"end\":47628,\"start\":47617},{\"end\":47777,\"start\":47769},{\"end\":47789,\"start\":47777},{\"end\":47964,\"start\":47956},{\"end\":47972,\"start\":47964},{\"end\":47982,\"start\":47972},{\"end\":47994,\"start\":47982},{\"end\":48202,\"start\":48194},{\"end\":48212,\"start\":48202},{\"end\":48218,\"start\":48212},{\"end\":48230,\"start\":48218},{\"end\":48440,\"start\":48432},{\"end\":48446,\"start\":48440},{\"end\":48452,\"start\":48446},{\"end\":48464,\"start\":48452},{\"end\":48670,\"start\":48662},{\"end\":48677,\"start\":48670},{\"end\":48683,\"start\":48677},{\"end\":48689,\"start\":48683},{\"end\":48701,\"start\":48689},{\"end\":48907,\"start\":48899},{\"end\":48913,\"start\":48907},{\"end\":48925,\"start\":48913},{\"end\":48932,\"start\":48925},{\"end\":48939,\"start\":48932},{\"end\":48951,\"start\":48939},{\"end\":49193,\"start\":49185},{\"end\":49199,\"start\":49193},{\"end\":49205,\"start\":49199},{\"end\":49217,\"start\":49205},{\"end\":49428,\"start\":49413},{\"end\":49439,\"start\":49428},{\"end\":49446,\"start\":49439},{\"end\":49454,\"start\":49446},{\"end\":49464,\"start\":49454},{\"end\":49668,\"start\":49657},{\"end\":49680,\"start\":49668},{\"end\":49690,\"start\":49680},{\"end\":49823,\"start\":49812},{\"end\":49833,\"start\":49823},{\"end\":49840,\"start\":49833},{\"end\":49853,\"start\":49840},{\"end\":49863,\"start\":49853},{\"end\":50134,\"start\":50127},{\"end\":50141,\"start\":50134},{\"end\":50150,\"start\":50141},{\"end\":50158,\"start\":50150},{\"end\":50165,\"start\":50158},{\"end\":50173,\"start\":50165},{\"end\":50179,\"start\":50173},{\"end\":50417,\"start\":50410},{\"end\":50425,\"start\":50417},{\"end\":50431,\"start\":50425},{\"end\":50629,\"start\":50622},{\"end\":50637,\"start\":50629},{\"end\":50644,\"start\":50637},{\"end\":50652,\"start\":50644},{\"end\":50658,\"start\":50652},{\"end\":50806,\"start\":50793},{\"end\":50814,\"start\":50806},{\"end\":50822,\"start\":50814},{\"end\":50839,\"start\":50822},{\"end\":50852,\"start\":50839},{\"end\":50859,\"start\":50852},{\"end\":50873,\"start\":50859},{\"end\":51213,\"start\":51201},{\"end\":51223,\"start\":51213},{\"end\":51235,\"start\":51223},{\"end\":51421,\"start\":51415},{\"end\":51432,\"start\":51421},{\"end\":51439,\"start\":51432},{\"end\":51447,\"start\":51439},{\"end\":51462,\"start\":51447},{\"end\":51472,\"start\":51462},{\"end\":51481,\"start\":51472},{\"end\":51651,\"start\":51644},{\"end\":51659,\"start\":51651},{\"end\":51667,\"start\":51659},{\"end\":51680,\"start\":51667},{\"end\":51691,\"start\":51680},{\"end\":51701,\"start\":51691},{\"end\":51709,\"start\":51701},{\"end\":51942,\"start\":51935},{\"end\":51950,\"start\":51942},{\"end\":52208,\"start\":52193},{\"end\":52216,\"start\":52208},{\"end\":52226,\"start\":52216},{\"end\":52236,\"start\":52226},{\"end\":52429,\"start\":52419},{\"end\":52437,\"start\":52429},{\"end\":52451,\"start\":52437},{\"end\":52465,\"start\":52451},{\"end\":52477,\"start\":52465},{\"end\":52489,\"start\":52477},{\"end\":52738,\"start\":52730},{\"end\":52744,\"start\":52738},{\"end\":52751,\"start\":52744},{\"end\":52758,\"start\":52751},{\"end\":52765,\"start\":52758},{\"end\":52772,\"start\":52765},{\"end\":52779,\"start\":52772},{\"end\":53010,\"start\":53002},{\"end\":53017,\"start\":53010},{\"end\":53024,\"start\":53017},{\"end\":53031,\"start\":53024},{\"end\":53038,\"start\":53031},{\"end\":53236,\"start\":53228},{\"end\":53243,\"start\":53236},{\"end\":53250,\"start\":53243},{\"end\":53257,\"start\":53250},{\"end\":53264,\"start\":53257},{\"end\":53398,\"start\":53388},{\"end\":53405,\"start\":53398},{\"end\":53414,\"start\":53405},{\"end\":53423,\"start\":53414},{\"end\":53431,\"start\":53423},{\"end\":53734,\"start\":53724},{\"end\":53741,\"start\":53734},{\"end\":53750,\"start\":53741},{\"end\":53759,\"start\":53750},{\"end\":53767,\"start\":53759},{\"end\":54011,\"start\":54003},{\"end\":54019,\"start\":54011},{\"end\":54026,\"start\":54019},{\"end\":54033,\"start\":54026},{\"end\":54040,\"start\":54033},{\"end\":54048,\"start\":54040},{\"end\":54055,\"start\":54048},{\"end\":54062,\"start\":54055},{\"end\":54291,\"start\":54283},{\"end\":54298,\"start\":54291},{\"end\":54305,\"start\":54298},{\"end\":54316,\"start\":54305},{\"end\":54331,\"start\":54316},{\"end\":54344,\"start\":54331},{\"end\":54574,\"start\":54566},{\"end\":54580,\"start\":54574},{\"end\":54787,\"start\":54781},{\"end\":54794,\"start\":54787},{\"end\":54803,\"start\":54794},{\"end\":54811,\"start\":54803},{\"end\":54818,\"start\":54811},{\"end\":54824,\"start\":54818},{\"end\":54832,\"start\":54824},{\"end\":54841,\"start\":54832},{\"end\":54848,\"start\":54841},{\"end\":54859,\"start\":54848},{\"end\":55137,\"start\":55131},{\"end\":55143,\"start\":55137},{\"end\":55152,\"start\":55143},{\"end\":55351,\"start\":55345},{\"end\":55357,\"start\":55351},{\"end\":55365,\"start\":55357},{\"end\":55373,\"start\":55365},{\"end\":55382,\"start\":55373},{\"end\":55393,\"start\":55382},{\"end\":55405,\"start\":55393},{\"end\":55593,\"start\":55587},{\"end\":55600,\"start\":55593},{\"end\":55606,\"start\":55600},{\"end\":55614,\"start\":55606},{\"end\":55622,\"start\":55614},{\"end\":55908,\"start\":55901},{\"end\":55915,\"start\":55908},{\"end\":55921,\"start\":55915},{\"end\":56108,\"start\":56100},{\"end\":56116,\"start\":56108},{\"end\":56125,\"start\":56116},{\"end\":56131,\"start\":56125},{\"end\":56139,\"start\":56131},{\"end\":56150,\"start\":56139},{\"end\":56161,\"start\":56150},{\"end\":56405,\"start\":56397},{\"end\":56412,\"start\":56405},{\"end\":56419,\"start\":56412},{\"end\":56427,\"start\":56419},{\"end\":56434,\"start\":56427},{\"end\":56652,\"start\":56643},{\"end\":56660,\"start\":56652},{\"end\":56667,\"start\":56660},{\"end\":56674,\"start\":56667},{\"end\":56925,\"start\":56916},{\"end\":56933,\"start\":56925},{\"end\":56942,\"start\":56933},{\"end\":56949,\"start\":56942},{\"end\":56955,\"start\":56949},{\"end\":56963,\"start\":56955},{\"end\":56974,\"start\":56963},{\"end\":57245,\"start\":57237},{\"end\":57254,\"start\":57245},{\"end\":57453,\"start\":57446},{\"end\":57460,\"start\":57453},{\"end\":57468,\"start\":57460},{\"end\":57478,\"start\":57468},{\"end\":57484,\"start\":57478},{\"end\":57690,\"start\":57682},{\"end\":57698,\"start\":57690},{\"end\":57867,\"start\":57859},{\"end\":57880,\"start\":57867},{\"end\":57890,\"start\":57880},{\"end\":57898,\"start\":57890}]", "bib_venue": "[{\"end\":40528,\"start\":40452},{\"end\":40865,\"start\":40791},{\"end\":41185,\"start\":41175},{\"end\":41443,\"start\":41368},{\"end\":41743,\"start\":41687},{\"end\":41998,\"start\":41954},{\"end\":42277,\"start\":42209},{\"end\":42457,\"start\":42406},{\"end\":42733,\"start\":42687},{\"end\":42907,\"start\":42838},{\"end\":43136,\"start\":43065},{\"end\":43387,\"start\":43318},{\"end\":43646,\"start\":43597},{\"end\":43856,\"start\":43788},{\"end\":44218,\"start\":44129},{\"end\":44487,\"start\":44416},{\"end\":44795,\"start\":44731},{\"end\":45088,\"start\":45033},{\"end\":45289,\"start\":45221},{\"end\":45498,\"start\":45438},{\"end\":45712,\"start\":45639},{\"end\":45995,\"start\":45949},{\"end\":46254,\"start\":46216},{\"end\":46496,\"start\":46455},{\"end\":46666,\"start\":46595},{\"end\":46932,\"start\":46886},{\"end\":47148,\"start\":47075},{\"end\":47388,\"start\":47320},{\"end\":47603,\"start\":47529},{\"end\":47810,\"start\":47789},{\"end\":47954,\"start\":47882},{\"end\":48192,\"start\":48135},{\"end\":48430,\"start\":48354},{\"end\":48660,\"start\":48603},{\"end\":48897,\"start\":48830},{\"end\":49183,\"start\":49105},{\"end\":49411,\"start\":49364},{\"end\":49655,\"start\":49595},{\"end\":49917,\"start\":49863},{\"end\":50125,\"start\":50057},{\"end\":50408,\"start\":50337},{\"end\":50620,\"start\":50552},{\"end\":50955,\"start\":50873},{\"end\":51199,\"start\":51142},{\"end\":51413,\"start\":51353},{\"end\":51770,\"start\":51709},{\"end\":52032,\"start\":51950},{\"end\":52191,\"start\":52144},{\"end\":52417,\"start\":52357},{\"end\":52728,\"start\":52654},{\"end\":53000,\"start\":52937},{\"end\":53268,\"start\":53264},{\"end\":53502,\"start\":53431},{\"end\":53780,\"start\":53767},{\"end\":54001,\"start\":53926},{\"end\":54281,\"start\":54235},{\"end\":54564,\"start\":54494},{\"end\":54779,\"start\":54687},{\"end\":55129,\"start\":55072},{\"end\":55343,\"start\":55259},{\"end\":55701,\"start\":55622},{\"end\":55928,\"start\":55921},{\"end\":56098,\"start\":56023},{\"end\":56395,\"start\":56340},{\"end\":56641,\"start\":56559},{\"end\":56914,\"start\":56820},{\"end\":57235,\"start\":57164},{\"end\":57444,\"start\":57369},{\"end\":57680,\"start\":57626},{\"end\":57857,\"start\":57791}]"}}}, "year": 2023, "month": 12, "day": 17}