{"id": 235485437, "updated": "2023-10-06 01:55:12.566", "metadata": {"title": "End-to-end Temporal Action Detection with Transformer", "authors": "[{\"first\":\"Xiaolong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Qimeng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Shiwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Song\",\"last\":\"Bai\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Bai\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.10271", "mag": null, "acl": null, "pubmed": "35947570", "pubmedcentral": null, "dblp": "journals/tip/LiuWHTZBB22", "doi": "10.1109/tip.2022.3195321"}}, "content": {"source": {"pdf_hash": "2f5f8e60a1c8cea0a0ba669305f0020854549ddd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2106.10271v4.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2106.10271", "status": "GREEN"}}, "grobid": {"id": "55240418c46e808c6c04714f8bf1c8617c854839", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2f5f8e60a1c8cea0a0ba669305f0020854549ddd.txt", "contents": "\nEnd-to-end Temporal Action Detection with Transformer\nAUGUST 2021 1\n\nJournal Of L A T E X Class \nFiles \nEnd-to-end Temporal Action Detection with Transformer\n148AUGUST 2021 1Index Terms-TransformerTemporal Action DetectionTem- poral Action LocalizationAction Recognition\nTemporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR. Fig. 1: Comparison of different pipelines of temporal action detection. (a) Multi-stage pipeline in [1], [2], etc.; (b) Twostage pipeline in [6], [7]; (c) Top-down one-stage pipeline in [8], (d) Bottom-up pipeline in [9] (e) The set prediction pipeline in this work.cation and regression on a large amount of candidate segments. Bottom-up methods [9], [12] perform per-frame classification and group these predictions into segment-level predictions. While these methods achieve state-of-the-art performance on standard benchmarks, they have complex pipelines.As shown inFig. 1, these pipelines involve post-processing operations, such as non-maximum suppression (NMS) and grouping. These operations, together with anchor setting in many top-down methods, are hand-designed with prior knowledge about this task and not learnable, which restricts the flexibility. Besides, most proposal-based methods [2], [11], [13], requires a standalone classifier to classify action proposals. These issues block the gradient flow and prevent end-to-end learning. Thus, it is necessary to develop a simple end-toend method that directly predicts action instances in a single differentiable network 1 without hand-crafted components.In this paper, we introduce an end-to-end temporal action detection framework to address the above issues. Inspired by the object detection Transformer (DETR) [15], we directly map a set of learnable embeddings, called action queries, to action instances in parallel. As the queries do not directly indicate the initial locations of actions like anchors or proposals, we are unable to extract features for each query1The single network means the detection network upon the video encoder. Training the video encoders along with the detection head for long videos often requires excessive computing resources. Therefore, most methods use offline features (e.g. I3D[14]) trained separately with large mini-batch size on large amounts of short (around 2 seconds) videos.\n\nI. INTRODUCTION\n\nV IDEO understanding has become more important than ever as the rapid growth of media prompts the generation, sharing, and consumption of videos. As a fundamental task in video understanding, temporal action detection (TAD) aims to predict the semantic label, the start time, and the end time of every action instance in an untrimmed and possibly long video. For its wide range of applications, including security surveillance, home care, video editing, video recommendation, and so on, temporal action detection has gained increasing attention from the community in recent years [1]- [5].\n\nPrevious methods for TAD can be roughly categorized into two groups. Top-down methods [1], [10], [11] perform classifi- This paper has supplementary downloadable material available at http://ieeexplore.ieee.org., provided by the author. Dense Detections from specific locations like previous methods. The detector is required to extract sufficient long-term context information before knowing which interval an action falls in. Besides, the context should be adaptive and relevant with each query, in order to differentiate between these queries. Traditional 1D convolutional neural networks cannot easily achieve them, due to a fixed receptive field and fixed weights. Recently, Transformers [16] have shown great power in sequence modeling. It is able to reason the relations between sequence elements and adaptively capture long-term context with the self-attention module. A video is naturally a sequence of frames and there is abundant context in it [2], [13], [17]. Therefore Transformer is a desirable choice for the above goals.\n\nBased on the above motivations, we propose a Temporal Action Detection TRansformer (TadTR) that predicts actions by extracting relevant context for each action query. It has an encoder-decoder structure. The encoder models inter-snippet relations to capture snippet-level context. The decoder models action-snippet relations to enhance each action query with snippet-level context, and inter-action relations to capture instance-level context from the other action queries. In this way, we can exploit richer context than previous methods that only exploit snippet-level context [2] or instance-level context [13]. Upon the decoder, two feed-forward networks (FFNs) predict the class and the segment for each action query. During training, an action matching module dynamically determines a one-to-one ground truth assignment according to the predictions. Owing to this, our detector avoids duplicate detections and NMS is unnecessary. It produces a very sparse set of action detections (10 \u223c 10 2 ), orders of magnitude fewer than previous methods (10 3 \u223c 10 4 ).\n\nHowever, due to the intrinsic difference between space and time, a direct application of Transformer is not appropriate. We observe that different frames in a video of actions are highly similar, because of the temporal redundancy and the slow changes in backgrounds or actors. Besides, the boundaries of actions are less clear than those of objects [18]. Therefore, to precisely detect actions, a detector needs to be localityaware, which means being aware of the subtle local changes in the temporal domain. The dense attention module in primitive Transformer that attends to all elements in a sequence, is less sensitive to such local changes by design. To mitigate this issue, we draw inspiration from [19] and propose a temporal deformable attention (TDA) module as the basic building block of Transformer. It selectively attends to a sparse set of key elements around a reference location in the input sequence, where the sampling locations and attention weights are learned and dynamically adjusted in accordance with the inputs. In this way, it can adaptively extract context information while preserving locality awareness.\n\nBesides TDA, we make two additional improvements to enhance locality awareness. First, a segment refinement mechanism is employed to refine the boundaries of predicted actions. To be concrete, we iteratively re-attend to the video according to the previous predictions and refine the boundaries with the newly extracted context. Second, we attach an actionness regression head to Transformer to predict a reliable confidence score called actionness for detection ranking. It extracts the local features with RoIAlign [20] for each predicted action and estimates its IoU with the best-matched ground truth action. This is more reliable than simply using classification scores, as the classification branch may find a shortcut from context but ignore the complete local details. Despite being seemingly small changes, they significantly improve performance.\n\nWe conduct comprehensive experiments on three datasets to evaluate TadTR. With a surprisingly simple pipeline, TadTR achieves remarkable performance with a low computation cost. Without any extra classifier, it achieves state-of-the-art performance on HACS Segments [21] and THUMOS14 [22]. When combined an extra classifier, it reaches 36.75% mAP on ActivityNet-1.3 [23], outperforming strong competitors such as G-TAD [2] and BMN [11]. In terms of run time, it takes only 155 ms per video on THUMOS14, which is much faster than recent state-of-the-art methods, as shown in Fig. 2. We believe that the simplicity, the flexibility, and the strong performance of the new method will benefit and ease future research on temporal action detection.\n\nThe contributions of this work are as follows:\n\n\u2022 We introduce an end-to-end set prediction (SP) framework that simplifies the pipeline for temporal action detection (TAD). It can detect actions in a single differentiable network without hand-crafted components. \u2022 We propose a Transformer architecture that is enhanced with locality awareness to better adapt to the TAD task. The core is a temporal deformable attention (TDA) module that selectively attends to a sparse set of key snippets in a video. We show that TDA is crucial for the success of the SP framework for TAD. \u2022 Different from previous works that ignore context or only exploit snippet-level or instance-level context, we model inter-snippet, inter-action, and action-snippet relations to capture both levels of context for more accurate temporal action detection. \u2022 Our method achieves state-of-the-art performance of self-contained detectors on HACS Segments and THU-MOS14, and competitive results on ActivityNet-1.3. Besides, it requires a lower computation cost than its competitors.\n\n\nII. RELATED WORK\n\nTemporal Action Detection. Previous TAD methods can be roughly categorized into top-down methods and bottom-up methods according to the pipeline. Top-down methods can be further categorized into multi-stage, two-stage, and onestage methods. (a) Multi-stage methods [2], [11], [24]- [27] first generate candidate segments and train a binary classifier that associates each segment with a confidence score, resulting in proposals. Those proposals with high scores are fed to a multi-class classifier to classify the actions. The candidate segments are generated by dense uniform sampling [1], [28] or grouping local frames that may contain actions [29]. Some methods [30], [31] combine multiple schemes for complementarity. (b) Two-stage methods [6], [7], [32], [33]  pre-defined multi-scale anchors associated with each temporal location. These methods need to manually set multiple anchor scales, which restricts the flexibility. Note that multi-stage methods can also be seen as generalized two-stage methods.\n\n(c) Top-down one-stage methods [8], [34] can be seen as the class-aware variant of the one-stage proposal generator. (d) Bottom-up methods perform frame-level action classification and merge the frame-level results to segment-level predictions. For example, [9] first predicts the action and boundary probabilities and then groups frames with maximal structured sum as actions. Recent anchor-free methods (e.g., AFSD [35] and A2Net [36]) also belong to this group. Besides these methods, a few works (e.g., CTAP [30] and PCG-TAL [37]) combine different pipelines to enhance the performance. All the above methods require post-processing steps such as NMS or grouping, which prevent end-to-end learning. An early work by Yeung et al. [38] also proposes a TAD method without hand-crafted components. Based on recurrent neural networks (RNN) and reinforcement learning (RL), it learns action detection by training an agent that iteratively picks an observation location and deciding whether to emit or refine a candidate action after observation. However, its reward function is not differetiable. Therefore it does not meet the criteria of end-to-end in this paper. All the above methods are fully-supervised. There are also some weakly-supervised methods that only utilize single-frame supervision [39] or video-level supervision [40]- [51] during training.\n\nTransformers and Context in Video Understanding. Transformers have achieved great success in natural language processing [16] and image understanding [52]- [54]. The core of Transformer is the self-attention mechanism that aggregates non-local cues through a weighted sum of features at attended locations. Compared with convolutions, self-attention can capture long-range context and dynamically adjust weights according to the input. Recently, many works have revealed the great potential of Transformers in video understanding tasks [55]- [57]. For example, VideoBERT [55] and Act-BERT [58] utilize Transformers to learn an joint representation for video and text. TimeSformer [56] decouples spatial and temporal self-attention for video classification. Zhou et al. [59] capture the temporal dependency with Transformer for video captioning. Girdhar et al. [60] apply Transformer to model the relationship between spatial proposals for spatio-temporal action detection. In this paper, Transformer is used to capture temporal context information for temporal action detection. Specifically, we employ attention modules to model the relations between video snippets, the relations actions and snippets, and the relations between actions. Several concurrent works also employ Transformer for context modeling in temporal action detection (AGT [61]) and temporal action proposal generation (RTD-Net [62] and TAPG [63]). However, these works either adopt a traditional TAD pipeline or have difficulty in training. TAPG still relies on hand-crafted anchors and post-processing steps. RTD-Net requires a three-step training scheme to optimize different parts of the network separately and relies on extra action classifiers to classify the proposals. AGT suffers from slow training convergence (1000\u00d7 more iterations than TadTR). In addition, different from these works that exploit the vanilla attention module, TadTR introduces a more efficient temporal deformable attention module that adaptively attends to a sparse set of key snippets in a video. As a result, it enjoys lower computation costs and easier training. Therefore, TadTR is more practical.\n\nIn the field of temporal action detection, some previous works also exploit context in other ways. For example, increasing the receptive field by a fixed ratio [7], [17]. However, this is not flexible enough and may introduce irrelevant information from unrelated frames. Another line of works exploit context by modeling the relations between different snippets [2], [64] or the relations between different proposals [13] with graph. The attention modules in this work are alternatives to them. Moreover, we model different kinds of relations and can capture richer context of different levels.\n\nDETR and Deformable DETR. Temporal action detection methods [6], [8], [36] often draw inspiration from object detection methods. This work is inspired by DETR [15] and Deformable DETR [19]. DETR proposes a Transformer-based Set Prediction (SP) framework to achieve end-to-end object detection without hand-crafted components. Deformable DETR proposes multi-scale deformable attention to address the issues of slow convergence and limited feature resolution of DETR. While extending them for direct TAD is intuitive, the effectiveness remains unclear. Our main contribution over DETR and Deformable DETR is that we adapt the SP framework and deformable attention for direct TAD and validate their effectiveness. Although the high-level design of TadTR is similar to Deformable DETR, the implementation is different as TadTR aims to temporally localize actions in videos while Deformable DETR is designed for object detection in images. Besides, we reveal that deformable attention is crucial for the success of the SP framework for TAD and segment refinement is also important. Furthermore, directly extending Deformable DETR to TAD does not achieve satisfactory performance as the confidence scores predicted by the decoder are not reliable.    The architecture of TadTR. It takes the video features extracted with a CNN and a set of learnable action queries as input and decodes a set of action predictions in parallel via a Transformer. The encoder captures the long-term context in the input feature sequence. The decoder extracts relevant context from the encoder for each action query and models the relations between action queries. Upon the decoder, we use feed-forward networks to predict the segments and classes of output actions. A segment refinement mechanism (the blue box) and an actionness regression head (the orange box) are utilized to refine the boundaries and the confidence scores of the predicted actions, respectively.\n\u01b8 ( ) \u1218 ( ) FFN \u1218 ( \u22121) \u00d7 ( \u22121) \u0ddc ( ) \u01b8 ( \u22121) \u01b8 (0) ( )(0\nTo relieve this issue, we add a simple yet effective actionness regression head to refine the confidence scores. To sum up, the adaptation and improvement over DETR and Deformable DETR make the SP framework practical for TAD and TadTR can serve as a strong baseline for SP-based TAD.\n\n\nIII. TADTR\n\nTadTR is constructed on video features encoded with a pretrained video classification network (e.g., I3D [14]). Fig. 3 shows the overall architecture of TadTR. TadTR takes as input the video features and a set of learnable action queries. Then it outputs a set of action predictions. Each action prediction is represented as a tuple of the temporal segment, the confidence score, and the semantic label. It consists of a Transformer encoder to model the interactions between video snippets, a Transformer decoder to predict action segments, and an extra actionness regression head to estimate the confidence score of the predicted segments. During training, an action matching module is used to determine a one-to-one ground truth assignment to the action predictions.\n\n\nA. Architecture\n\nEncoder. Let X V \u2208 R T S \u00d7C denotes the video feature sequence, where T S and C are the length and dimension, respectively. Each frame in the feature sequence is a feature vector extracted from a certain snippet in the video. Here, a snippet means a sequence of a few (e.g., 8) consecutive frames. We use linear projection to make C = 256. The encoder models the relations between different snippets and outputs a feature sequence X E \u2208 R T S \u00d7C enhanced with temporal context. As depicted in Fig. 3, it consists of L E Transformer encoder layers of the homogeneous architecture. Each encoder layer has two sub-layers, i.e., a temporal deformable attention (TDA) module, and a feed-forward network (FFN). Layer normalization [65] is used after each sub-layer and a residual connection is added between the input of each sub-layer and the output of the follow-up normalization layer. Except for TDA, all the other components are identical to the primitive Transformer [16].\n\nTDA is an alternative to the dense attention module in [16]. The high similarities between different frames and the vagueness of action boundaries require a detector to possess locality awareness. In other words, the detector should be more sensitive to local changes in the temporal domain. The dense attention module that attends to all locations in an input feature sequence, is less sensitive to such local changes. Besides, it suffers from high computation cost and slow convergence [19]. To better fit the TAD task, we draw inspiration from [19] and propose a temporal deformable attention (TDA) module that adaptively attends to a sparse set of temporal locations around a reference location in the input feature sequence.\n\nLet z q \u2208 R C be the feature of query q and t q \u2208 [0, 1] be the normalized coordinate of the corresponding reference point. Given an input feature sequence X \u2208 R T S \u00d7C , the output h m \u2208 R T S \u00d7(C/M ) of the m-th (m \u2208 {1, 2, ..., M }) head of a TDA module is computed by an weighted sum of a set of key elements sampled from X:\nh m = K k=1 a mqk W V m X((t q + \u2206t mqk )T S ),(1)\nwhere K is the number of sampling points, a mqk \u2208 [0, 1] is the normalized attention weight, and \u2206t mqk \u2208 [0, 1] is the sampling offset relative to t q . X((t q + \u2206t mqk )T S ) is the linear interpolated feature at (t q + \u2206t mqk )T S as it is fractional. Following [19], the attention weight a mqk and the sampling offset \u2206t mqk are predicted from the query feature z q by linear projection. We normalize the attention weight with softmax to make K k=1 a mqk = 1. W V m \u2208 R C\u00d7(C/M ) is a learnable weight. The output of TDA is computed by a linear combination of the outputs of different heads:\nTDA(z q , t q , X) = W O Concat(h 1 , h 2 , ..., h m ),(2)\nwhere W O \u2208 R C\u00d7C is a learnable weight. When computing the \u03c4 -th frame in the output sequence, the query and the reference point are both the \u03c4 -th frame in the input sequence. Therefore, we refer to TDA in the encoder as temporal deformable self-attention (TDSA). The query feature is the summation of the input feature of that frame and the position embedding at that location. The position embedding is used to differentiate between different locations in the input sequence. In this paper, we use the sinusoidal position embedding following [16].\nX P (\u03c4, \u03b3) = sin \u03c4 10000 \u03b3/C \u03b3 is even cos \u03c4 10000 (\u03b3\u22121)/C \u03b3 is odd .(3)\nThe feed-forward network consists of two fully connected (FC) layers and a ReLU activation in between. It is the same across different positions and can be viewed as a stack of two 1D convolution layers with kernel size 1. The dimensions of the two FC layers are C F = 2048 and C = 256, respectively.\n\nDecoder. The decoder takes as input the encoder features X E and N q action queries with learnable embeddings\u1e91 (0) = {\u1e91\n(0) i } Nq i=1\n. It transforms these embeddings to N q action predic-tions\u0176 = {\u0177 i }. As illustrated in Fig. 3, the decoder consists of L D sequential decoder layers. Each decoder layer has three major sub-layers: a self-attention module, a temporal deformable cross-attention (TDCA) module, and a feed-forward network. Similar to each encoder layer, we add a residual connection between each sub-layer and the following layer normalization function. The output of the l-th decoder layer is denoted by z (l) . The self-attention module models the relation between action queries and updates their embeddings. The motivation here is that multiple actions in one video are often related. For example, a cricket shot action often appears after a cricket bowling action. To make an action prediction, each query extracts relevant context information from the video via the TDCA module. Given the encoder features X E and the input embedding\u1e91 i \u2208 R C , the output query embedding of TDCA is formulated as T DA(\u1e91 i ,t i , X E ). Here,t i is the coordinate of the reference point in X E . By default, it is predicted by a projection function f IR from\u1e91\n(0) i . f IR is\nimplemented with a linear layer and a follow-up sigmoid function for normalization. The reference point can be seen as the initial estimation of the center of the corresponding action segment. FFNs in the decoder layers have the same architecture as those in the encoder layers.\n\nDifferent from TDSA in the encoder, the query embeddin\u011d z (0) i and the reference point are learnable and shared by all input videos. This allows the network to learn the global distribution of the action locations in the training dataset, which is more flexible than hand-crafted anchor setting or proposal sampling. An analysis is given in Sec. IV-D.\n\nPrediction Heads. Upon the output (the updated query embeddings) of each decoder layer, we apply FFNs to predict the classification probabilitiesp i and the temporal segment s i = (t i ,d i ) of the action instance\u0177 i corresponding to each query. Botht i andd i are normalized. To make the boundaries of the instances more accurate, a segment refinement mechanism is proposed. Besides, an additional actionness regression head is employed to refine the confidence score. They are detailed below. Segment Refinement. Transformer is able to capture longrange context information. However, the predicted action boundaries might be unsatisfactory for lack of locality. Inspired by [19], we introduce a refinement mechanism to enhance locality awareness and improve localization performance. It involves two strategies. The first is the incremental refinement of segments. Instead of predicting the segments independently at each decoder layer, we adjust the segments according to previously predicted segments layer by layer. Formally, given each action segment\u015d\n(l\u22121) i = (t (l\u22121) i ,d (l\u22121) i )\npredicted at the (l \u2212 1)-th decoder layer, the l-th decoder layer predicts the location offsets (\u2206t\n(l) i , \u2206d (l) i ) relative to\u015d (l\u22121) i .\nThe corresponding refined segment\u015d\n(l) i = (t (l) i ,d (l) i ) is then computed by: t (l) i = \u03c3(\u2206t (l) i + \u03c3 \u22121 (t (l\u22121) i )), l \u2208 {1, 2, ..., L D } (4) d (l) i = \u03c3(\u2206d (l) i + \u03c3 \u22121 (d (l\u22121) i )), l \u2208 {2, 3, ..., L D },(5)\nwhere \u03c3(\u00b7) and \u03c3 \u22121 (\u00b7) are the sigmoid and the inverse sigmoid function, respectively. Specially,t\n\ni , is the initial reference pointt i predicted by f IR . The initial value ofd\n(l) i isd (1) i\npredicted at the first decoder layer. The second is iterative reference point adjustment. We update the reference points of TDCA in each decoder layer instead of always usingt\n(0) i . Specifically,t (l\u22121) i\n, the refined segment center at the (l \u2212 1)th decoder layer, is used as the reference point of TDCA at the l-th decoder layer. In this way, TDCA can be adaptive to the input video and better aligned with the local features of the action instances. We validate the effectiveness of the two strategies in the experiments.\n\nActionness Regression. One challenge of temporal action detection is to generate reliable confidence scores for ranking. Typically, classification scores are used. However, the classification task focuses more on discriminative features and is less sensitive to the localization quality of an action. As a result, the classification score of the detections may be unreliable for ranking. An example is shown in Fig. 4.\n\nTo mitigate this issue, we employ an actionness regression head that extracts context aligned with the interval of a predicted segment and predicts an actionness score upon it. Given the encoder feature sequence X E and a predicted segment s i by the decoder, we first apply temporal RoIAlign [20] upon X E to obtain the aligned features X si \u2208 R T R \u00d7C within the interval defined by s i from X E . Here, T R is the number of bins for RoIAlign. To include a certain amount of context information around the boundaries, we slightly expand  Fig. 4: The Transformer may generate unreliable confidence scores. Here, a prediction with a lower overlap with the curling action has a higher score than a more accurate prediction (0.50 vs. 0.39).\n\nthe segment by a factor of when applying RoIAlign. The expanded segment can be expressed as (t i , d i ). Then, a feedforward network is used to predict the actionness score\u011d i from the aligned feature.\u011d i is supervised by the maximal IoU g i (intersection over union) between s i and all ground truth actions. In this way, the detector is enforced to be more sensitive to local features in order to differentiate between different segments.\n\nDiscussion. The actionness regression head is somewhat similar to the second stage of the traditional two-stage method, as they can both refine the confidence scores. However, the set prediction pipeline adopted by TadTR is significantly different from traditional pipelines. It is hard to categorize TadTR and its variants into one-stage or two-stage methods, as it does not need anchor setting and post-processing steps like traditional one-stage or two-stage methods. Besides, the actionness regression head is very lightweight. It only needs to predict class-agnostic confidence scores. The computation cost (in FLOPs) of this head is 6.59% of that of the full model. Differently, the second stage of a two-stage method often contributes to a major amount of computation cost (e.g., 99.98% in BMN [11]). We also note that TadTR can already make sparse and complete detections without actionness regression.\n\n\nB. Training and Inference\n\nAction Matching. The action matching module determines the targets assigned to each detection during training. Inspired by DETR [15] in object detection, we frame it as a set-toset bipartite matching problem to ensure a one-to-one ground truth assignment. Let Y = {y j } Nq j=1 be a set of ground truth actions padded with \u2205 (no action) and \u03c0 be the permutation that assigns each target y j to the corresponding detection\u0177 \u03c0(j) . Bipartie matching aims to find a permutation that minimizes the overall matching cost:\u03c0 = arg min Nq j=1 C(y j ,\u0177 \u03c0(j) ).\n\nThe matching cost considers the classification probabilities and the distance between ground truth and predicated segments. Specifically, C(y j ,\u0177 \u03c0(j) ) is defined as\n1 cj =\u2205 [L cls (p \u03c0(j) , c j ) + L seg (s j ,\u015d \u03c0(j) )],(7)\nwhere c j and s j are the class label and the temporal segment of y j . L cls (p \u03c0(j) , c j ) is the classification term. We use crossentropy loss by default. L seg (s j ,\u015d \u03c0(j) ) is the distance between the predicted location and the ground truth location, defined as \u03bb iou L iou (s j ,\u015d \u03c0(j) ) + \u03bb coord L L1 (s j ,\u015d \u03c0(j) ),\n\nwhere L L1 is the L 1 distance and L iou is the IoU loss. IoU loss is defined as the the opposite number of the IoU. \u03bb iou and \u03bb coord are hyper-parameters. The matching problem is solved with the Hungarian algorithm. Through the set-based action matching, each ground truth will be assigned to only one prediction, thus avoiding duplicate predictions. This brings two merits. First, TadTR does not rely on the non-differentiable non-maximal suppression (NMS) for post-processing and enjoys end-to-end training. Second, we can make sparse predictions with limited queries (e.g. 10) instead of dense predictions in many previous works (e.g. tens of thousands for BMN [11] and G-TAD [2]), which saves the computation cost.\n\nIn a way, the action matching module performs a learnable NMS. The matching cost takes the classification scores of the detections into account. In this way, those detections with lower scores are more likely to be assigned with a non-action target. As a result, their classification scores will be suppressed in the training process.\n\nLoss Functions. Once the ground truth assignment is determined, we optimize the network by minimizing the following multi-part loss functions:\nL = Nq j=1 [L cls (p\u03c0 (j) , c j ) + 1 cj =\u2205 L seg (s j ,\u015d\u03c0 (j) ) +\u03bb act L L1 (\u011d\u03c0 (j) , g\u03c0 (j) )],(9)\nwhere the first two items optimize the detections from the decoder and the last one optimizes the outputs of actionness regression. L cls uses focal loss [66].\u03c0 is the solution of Equation 6. \u03bb act is a hyper-parameter.\n\nInference. During inference, we ignore the action predictions from all but the last decoder layer. The confidence score for a detection\u0177 i is computed by p i (\u0109 i ) \u00b7\u011d i , where\u0109 i is the predicted action label.\n\n\nIV. EXPERIMENTS\n\n\nA. Experimental Setup\n\nDatasets and Evaluation Metrics. We conduct experiments on THUMOS14 [22], HACS Segments [21], and ActivityNet-1.3 [23]. THUMOS14 is built on videos from 20 sports action classes. It contains 200 and 213 untrimmed videos for training and testing. There are 3007 and 3358 action instances on the two sets. The average length of actions is 5 seconds. ActivityNet-1.3 and HACS Segments share the same 200 classes of daily activities. Both datasets are split into three sets: training, validation, and testing. The numbers of videos in these sets are 10024, 4926, and 5044 respectively on ActivityNet-1.3, and 37613, 5981, and 5987 on HACS Segments. The average length of actions is 48 seconds on ActivityNet-1.3 and 33 seconds on HACS Segments. On both datasets, the annotations on the testing set are reserved by the organizers. Therefore, we evaluate on the validation set.\n\nFollowing conventions, the mean average precision (mAP) at different IoU thresholds is used for performance evaluation. On THUMOS14, the IoU thresholds for computing mAPs are [0.3 : 0.7 : 0.1]. On the other two datasets, we report mAPs at the thresholds {0.5, 0.75, 0.95} and the average mAP at the thresholds [0.5 : 0.95 : 0.05]. For simplicity, we denote mAP at the IoU threshold \u03b1 as mAP \u03b1 and the average mAP is referred to as mAP unless specially noted.\n\nVideo Feature Extraction. Most TAD methods are based on offline extracted video features. For easier comparison with them, we also use video features as the input of our method. For experiments on HACS Segments, we directly use the official I3D features 2 , which are extracted with I3D trained on Kinetics at 2FPS. On the other datasets, we use the commonly used features in previous works. On THUMOS14, the twostream I3D [14] networks pre-trained on Kinetics [14] are taken as the video encoder, and the features are extracted every 8 frames. On ActivityNet-1.3, we use the two-stream TSN [67] features extracted at 5FPS. Following previous works [2], [11], we resize the video features to a fixed length of 100 via linear interpolation on ActivityNet-1.3 and HACS Segments. Since the videos are long on THUMOS14, we follow [11] to crop each video feature sequence with windows of length 128 and stride 64 for training. In each window, we reserve the instances contained in it and clip the instances that partially overlap with it. We ignore the instances that have less than 1-second overlap with the window during training. This strategy is called length-based instance filtering. During inference, the stride is increased to 96 and the duplicate detections in the overlapped region are merged with NMS. This strategy is called crosswindow fusion (CWF). We also report the performance of TadTR tested on non-overlapping windows (with a stride of 128). In this case, we simply take the union of detections from all windows of a video. Implementation Details. L E and L D are set to 2 and 4, respectively. The loss weights \u03bb iou , \u03bb coord and \u03bb act are set to 2, 5 and 5 respectively. The numbers of attention heads M and sampling points K are set to 8 and 4, respectively. The parameters of the linear layers that predict attention weights are initialized to zero. We initialize the linear layers that predict sampling offsets to make {\u2206p mqk } 8 m=1 = (k, 0, \u2212k, 0, k, 0, \u2212k, 0) at initialization. The expanding factor and the number of bins T R for RoIAlign in the actionness regression head are 1.5 and 16 respectively.\n\nTadTR is trained using AdamW [68] optimizer. The initial learning rate is 2 \u00d7 10 \u22124 and scaled by a factor of 0.1 after training for a certain number of epochs. The learning rates of the linear projection layers for predicting attention weights and sampling offsets are multiplied by 0.1. We train the models for 30, 15, and 30 epochs and decrease the learning rates after 25, 12, and 25 epochs on THUMOS14, ActivityNet-1.3, and HACS Segments respectively. The batch size is set to 16. In the experiments, we also explore an improved training setting. Following RetinaNet [66] and AFSD [35], we use focal loss [66] for the classification term in the matching cost 2 http://hacs.csail.mit.edu/hacs_segments_features.zip (Eq. 7). We find that this modification speeds up convergence. Therefore the total numbers of training epochs are reduced to 16,12, and 20 on the three datasets, respectively. The learning rates are decreased after 14, 9, and 18 epochs, respectively. Besides, we use the integrity-based instance filtering (IBIF) strategy in G-TAD [2] and AFSD [35] to replace the default length-based instance filtering strategy on THUMOS14. To be specific, in each window, we only keep those ground truth instances whose integrity exceeds 0.75. Here, the integrity of an instance s g in a window s w is defined as |s g \u2229 s w |/|s g |, where | \u00b7 | means the length. It is similar to IoU but has a different denominator. Those windows without such instances are ignored during training.\n\nThe experiments are conducted on a workstation with a single Tesla P100 GPU card, and Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz. It takes around 10 minutes, 36 minutes, and 150 minutes to finish training on THUMOS14, ActivityNet-1.3, and HACS Segments, respectively. Table I demonstrates the temporal action detection performance and run time comparison on the testing set of THUMOS14. We measure the run time of these methods with publicly available implementations under the same environment (a single P100 GPU). We run methods on the full testing set with batch size set to 1 and report the average time and FLOPs per video. The average length of videos on THUMOS14 is 217 seconds. BMN [11] and G-TAD [2] use two-stream TSN [67] features originally. For a fair comparison, we also report their performance with I3D features. For AFSD [35], we have excluded the computation cost of the feature extractor. For TadTR, we report the performance with different training and inference settings. The entry with \u00a7 is with integrity-based instance filtering (IBIF). The entries with * are with IBIF and focal loss. TadTR-lite is the variant that does not use cross-window fusion (CWF) during inference. We observe that: 1) TadTR* achieves the best performance among all the compared methods in terms of mAP at all IoU thresholds. Even the variant without CWF can achieve state-of-the-art performance. TadTR* is slightly better than TadTR   and 8\u00d7 faster than the competitive single-network detector A2Net. It also requires much fewer FLOPs. The efficiency of our method is owing to the simple framework and the sparsity of predictions.\n\n\nB. Main Results\n\n\nTHUMOS14.\n\nThe above results indicate that our method is both accurate and efficient. We also note that many other methods are composed of multiple independently trained networks. Those proposal generation methods (BSN, MGG, BMN, G-TAD, MR, BC-GNN, and RTD-Net) are not self-contained, as they rely on an extra classifier (such as P-GCN) to accomplish the TAD task. Differently, TadTR can achieve action detection with only a single unified network.\n\nWe note that the computation cost of TadTR is not comparable with those methods that directly take video frames as input, such as R-C3D. Most previous works use video features as input and focus on the design of detection networks.\n\nHACS Segments. We report the performance of TadTR, SSN [10], and the state-of-the-art method G-TAD [2] in Table II. Our method achieves an average mAP of 30.83%, which outperforms SSN (+11.86% mAP) and G-TAD (+3.35% mAP). Besides, our method requires 455\u00d7 fewer GFLOPs than G-TAD. As for run time, the network inference and postprocessing step of G-TAD take 33 ms and 908 ms per video, respectively. The total run time is 941 ms, 49.5\u00d7 that of TadTR (19 ms). We also try the improved training setting, which results in 32.09% mAP. The results again illustrate the superiority of TadTR.\n\nActivityNet-1.3. Table III compares the performance of different methods on the validation set of ActivityNet-1.3. Some methods (e.g., G-TAD [2]) only implement action proposal generation and cannot produce action detections without external action classifiers. We divide the methods into two groups according to whether external action classifiers are used. Being simple and end-to-end trainable, TadTR achieves an average mAP of 28.21%, which is stronger than all the other methods.\n\nWith the improved training setting, TadTR achieves 1.69% higher mAP. This variant is 2.56% better than the second-best method PCG-TAL [37] in terms of mAP. Compared with the second-best single-network detector TAL-Net, we improve the performance by 9.68%. For comparison with previous methods [2], [11], [36] that are combined with an ensemble of classifiers [69], we also try such a combination. To be concrete, we pass the detections by TadTR to the classifiers and fuse the classification scores of TadTR and the classifiers by multiplication. When fused with [10], TadTR enjoys a significant performance boost, achieving an average mAP of 34.64%. It is better than the other compared methods in terms of average mAP, although some methods use the stronger I3D features. When using the  I3D features, the average mAP is further boosted to 36.11%, outperforming the second-best method AFSD by 1.72%. Besides, the performance is achieved at a low computation cost, as indicated by the smaller FLOPs. TadTR can also be combined with BMN. This is implemented by connecting the encoder of TadTR to the detection head of BMN. Owing to the adaptive context captured by the Transformer encoder, TadTR+BMN achieves an improvement of 0.7% over BMN, reaching 34.55% mAP. It also outperforms the recent method G-TAD. This indicates the advantage of Transformer in temporal action detection. Note that 99.98% of the computation cost is on the proposal classification branch of BMN. Therefore the total computation cost of TadTR+BMN is close to that of BMN.\n\nWith the stronger TSP [71] features, the performance of TadTR reaches 36.75% mAP. It outperforms G-TAD by 0.94% and BMN by 1.08%. This again demonstrates the superiority of TadTR.\n\n\nC. Ablation Study\n\nIn this subsection, we validate the effectiveness of different components of TadTR and evaluate the effects of various hyper-parameters. Unless specially noted, all reported results are with the default training and setting. TadTR-lite is used for THUMOS14.\n\nThe importance of context information. The key of Transformer is the self-attention mechanism that incorporates the context in a video sequence. In TadTR, we leverage two kinds of context, snippet-level context from related snippets and instance-level context from related action queries, which are captured by Transformer encoder and the self-attention module in Transformer decoder respectively. By removing Transformer encoder, we get a variant \"TadTR w/o encoder\". By removing the self-attention module in the decoder, we get a variant \"TadTR w/o instance-level context\". We report the performance of the two variants in Table IV. It is observed that removing the encoder leads to a 3.89% drop on HACS Segments, 6.93% drop on THUMOS14, and 0.87% drop on ActivityNet in terms of average mAP. It indicates that the Transformer encoder is crucial for our model, as the decoder requires long-range and adaptive context to reason the relations between the actions and the video. Removing instancelevel context, the average mAP drops by 1.13% on HACS Segments, 2.66% on THUMOS14, and 1.98% on ActivityNet. We conclude that the context information from other action instances is also helpful for action detection. Transformer encoder v.s. CNN encoder. We try replacing the Transformer encoder with a 1D CNN encoder, which is common for temporal modeling in previous TAD methods. The 1D CNN encoder is composed of two 1D convolutional layers with 256 filters of kernel size 3 and ReLU activation. As can be observed in Table V, using 1D CNN encoder leads to 2.88% average mAP drop when NMS is not applied (the default option), and 1.41% average mAP drop when NMS is applied. Interestingly, the performance of this variant with NMS is improved by 1.17% over that without NMS. It indicates that there are many duplicate detections. One possible reason is that CNN features are locally correlated. Therefore, it is hard to differentiate between close predictions as they have similar features.\n\nTo further dissect the performance gap between TadTR with 1D CNN encoder (equipped with NMS) and TadTR, we divide the ground truth instances into 3 groups according to the normalized duration: short (0 \u223c 0.1), medium (0.1 \u223c 0.2) and long (0.2 \u223c 1) and report the average mAP for each group in Fig. 5. Here we use the normalized duration because we resize the video features into a fixed length. For reference, the average duration per video is 148 seconds. As can be observed, TadTR with Transformer encoder achieves better performance for medium-length and long actions. 1D CNN encoder is slightly better for short actions. The result is reasonable, as 1D CNN is good at modeling short-term dependency but poor at modeling long-term dependency.\n\nWe also explore deeper CNNs and larger convolution kernels, but no improvement is observed. In terms of computation cost, this variant has much higher FLOPs than TadTR with Transformer encoder. The results show that 1D CNN is inferior to Transformer encoder.  Fig. 6: Actionness regression improves the ranking of detections. In each of the above cases, the initial scores are unreliable. The more accurate detection obtains a lower score than the less accurate one before rescoring. With the new scores (on the right of the arrows) applied, the ranking order turns satisfactory. Best viewed in color. with vanilla dense attention modules. This variant is called \"TadTR with dense attention\". As depicted in Table IV, the performance of this variant is far behind TadTR, especially on THUMOS14 (-23.77% mAP), even if the model is trained for 180 epochs. It means temporal deformable attention is crucial for the success of TadTR. The main reason is that the dense attention lacks locality awareness. As different frames are usually similar in background, a dense attention module tends to over-smooth input sequence at initialization (see an example in the supplementary material). As a result, it is hard to localize temporal segments with different semantics. Besides, the variant with dense attention has 5.7\u00d7 higher computation cost than that of TadTR. Therefore, temporal deformable attention is a better choice.\n\nThe effects of actionness regression and segment refinement. We study the effects of the two components by removing them individually, resulting in 2 different model variants. Their results are presented in Table IV Fig. 6 to show how actionness regression helps. It can produce more reliable confidence scores for the action predictions.\n\nComparing TadTR w/o segment refinement and TadTR, we find this component helpful for improving the localization accuracy. On HACS Segments, it improves the mAP at the strict threshold of 0.95 by a large margin of 2.24%. The mAPs at other thresholds are also consistently improved. On THUMOS14 and ActivityNet, the average mAP improves by 3.85% and 0.81%.  To investigate how the segment refinement mechanism improves the performance, we evaluate the effect of incremental refinement and reference point adjustment. As can be observed in Table VI, disabling incremental refinement leads to a 1.31% decrease in mAP, which suggests that the incremental refinement strategy is better than independent prediction at each decoder layer. Disabling reference point adjustment in standard segment refinement mechanism also leads to a 1.85% decrease in mAP. This is reasonable, as the initial referent points associated with the queries are invariant for different input samples. The adjustment strategy makes them adaptive to the input and aligned with the local features of the target actions.\n\nBesides the detection performance, another important aspect is the computation cost. In terms of FLOPs, adding or removing the two components has little impact, which is shown in Table IV. In terms of run time, the average time cost per video on THUMOS14 is 130 ms for TadTR-lite without the two components. Adding the actionness regression head will increase the average time cost to 141 ms. With segment refinement enabled, the average time cost per video becomes 155 ms, which is still very efficient compared with state-ofthe-art methods.\n\nThe effects of the number of action queries. Table VIII compares the performance of TadTR using different number of action queries (N q ). The best performance is achieved at N q = 40 on THUMOS14, N q = 30 on HACS Segments and N q = 10 on ActivityNet-1.3. The results are reasonable, as the average number of action instances per video on THUMOS14 (15.4) is larger than that on ActivityNet-1.3 (1.5) and HACS Segments (2.8).\n\nThe effects of the numbers of encoder layers and decoder layers. We evaluate TadTR with different numbers of encoder layers (L E ) and decoder layers (L D ) and report results in Table VII. With L D fixed, the best performance is achieved when L E is 2. Larger L E gives inferior results probably due to the difficulty of training. Therefore, we set L E to 2. With L E fixed, the average mAP increases by 2.03% when L D increases from 2 to 4. Larger L D gives a slightly lower performance.  Therefore we suggest setting L D to 4.\n\nThe effect of the number of sampling points. Table IX compares the performance and computation cost using different numbers of sampling points K in TDA modules on HACS Segments. We observe that moderately increasing K improves the performance, as more temporal details can be captured. The best performance is achieved at K = 4. The number of sampling points has little impact on the computation cost.\n\nThe effect of the number of attention heads. The effects of the hyper-parameters in actionness regression. In Table XI, we compare different choices of the expanding factor and the number of bins T R for actionness regression. We see that the best performance is achieved when = 1.5. This result is 0.56% higher than that of the variant without RoI expansion ( = 1), showing the effectiveness of RoI expansion. Among different choices of T R , T R = 16 results in the best performance.  Fig. 8: Visualization of self-attention between action queries in the last decoder layer. We average the attention over all heads. The queries are represented by the predicted instances.\n\nThe arrows indicate the attention that the topmost query casts on the other four queries with the largest weights. The attention weight is encoded by the color. The darker the arrow, the larger the attention weight. In the first example, the four attended instances are semantically related to the topmost instance.\n\nIn the second example, the topmost instance casts the most attention to a nearby instance (#37) with the same class. The other three non-action instances are also attended, probability for context from the background. Best viewed in color.\n\n\nD. Analysis\n\nVisualization of attention. Fig. 12 Fig. 9: Visualization of all action predictions on all videos from HACS Segments validation set from 30 action query slots. In each 1-by-1 square, we use scattered points to represent all predictions from this query. The horizontal and the vertical coordinates are the coordinate of the center and the normalized length of these predictions, respectively. We observe that each query is responsible for action predictions in certain locations and lengths.\n\nsampling points in the decoder almost cover the full extent of an action prediction, providing a large receptive field. Differently, the sampling points in the encoder have a relatively short temporal extent, capturing a moderate amount of context. Fig. 8 visualizes self-attention between action queries in the last decoder layer. We find that the topmost query in each example cast the most attention on the queries whose predicted instances are semantically related to that of it. It suggests that the self-attention layer in the decoder can model the relations between action queries (or instances).\n\nVisualization of action queries. Fig. 9 illustrates the distribution of locations and scales (lengths) of output actions associated with each action query. We observe that each query produces action predictions in certain locations and scales. Different locations and scales are covered by a small number of queries. It means that the detector learns the distribution of actions in the training dataset. This is more flexible than the hand-crafted anchor design in previous methods.\n\nLimitations. Although TadTR achieves strong overall per- formance, it may fail on some short actions, as depicted in Fig. 10. The quantitative results in Fig. 5 also illustrate the lower performance of TadTR on short actions. One possible reason is that Transformer is inferior to 1D CNN in modeling short-term dependency. Combing Transformer and 1D CNN might improve the performance on short actions. Another limitation is that TadTR will miss actions when the number of true actions in a video is larger than the number of queries N q , although such cases might be rare. This is a common issue of DETR-alike detectors. How to maintain the performance while increasing N q is worth studying in future works.\n\n\nV. CONCLUSION\n\nWe propose TadTR, a simple end-to-end method for temporal action detection (TAD) based on Transformer. It views the TAD task as a direct set prediction problem and maps a series of learnable embeddings to action instances in parallel by adaptively extracting temporal context in the video. It simplifies the pipeline of TAD and removes hand-crafted components such as anchor setting and post-processing. We make three improvements to enhance the Transformer with locality awareness to better adapt to the TAD task. Extensive experiments validate the remarkable performance and efficiency of TadTR and the effectiveness of different components. TadTR achieves state-of-the-art or competitive performance on HACS Segments, THUMOS14, and ActivityNet-1.3 with lower computation costs. We hope that this work could trigger the development of Transformers and efficient models for temporal action detection. The current implementation of TadTR is based on offline extracted CNN features for a fair comparison with previous methods. In the future, we plan to explore joint learning of the video encoder and TadTR [73], and temporal action detectors purely based on Transformers.\n\n\nAPPENDIX\n\nIn this supplement, we present several visualization results. Fig. 11 illustrates the smoothing effect of dense attention. Fig. 12 supplements Fig. 7 in the main document and gives more examples to demonstrate temporal deformable attention.   Fig. 12: Visualization of temporal deformable attention. The first row is uniformly sampled video frames. The second row visualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention for the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and separate points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color.\n\n\nThis work was supported by National Key R&D Program of China (No. 2018YFB1004600). (Corresponding author: Xiang Bai.) X. Liu (email: brucelio@outlook.com) and Q. Wang are with the School of Electronic Information and Communications, Huazhong University of Science and Technology. X. Bai (email: xbai@hust.edu.cn) is with the School of Artificial Intelligence and Automation, Huazhong University of Science and Technology. Y. Hu, X. Tang, and S. Zhang are with Alibaba Group. S. Bai is with ByteDance Inc. Part of this work was done when X. Liu was an intern at Alibaba Group.\n\nCNN\n\n\nFig. 3 :\n3Fig. 3: The architecture of TadTR. It takes the video features extracted with a CNN and a set of learnable action queries as input and decodes a set of action predictions in parallel via a Transformer. The encoder captures the long-term context in the input feature sequence. The decoder extracts relevant context from the encoder for each action query and models the relations between action queries. Upon the decoder, we use feed-forward networks to predict the segments and classes of output actions. A segment refinement mechanism (the blue box) and an actionness regression head (the orange box) are utilized to refine the boundaries and the confidence scores of the predicted actions, respectively.\n\nFig. 11 :\n11Different frames in a video is usually highly similar. The dense attention tends to cast uniform attention to different locations in the input sequence at initialization. Left: The similarity matrix of each pair of snippets in CNN features of a randomly selected video. Middle: The attention weight. Right: The similarity matrix of the output feature of the dense-attention. Best viewed in color.\n\n\n)Self Attention \n\nAdd & Norm \n\nTDA \n\nAdd & Norm \n\nFFN \n\nAdd & Norm \n\nDec. Layer details \n\nFFN \n\n\u01b8 ( \u22121) \n\n( \u22121) \n\n( ) \n\nDetection \u0ddd : \nSegment \n\u01b8 = ( \u01b8 , \u1218 ) \nClass \n\u01b8 = argmax \u0ddc \nConfidence score \n\u0ddc ( \u01b8 )  *  \u0ddc \n\nTDA: Temporal Deformable Attention \nFFN: Feed-Forward Network \nPos. Embed.: Position embedding \n: Initial Reference Points Projection \n\u2295 \uff1aSegment Refinement Function \n\nFFN \n\nSegments {\u0ddc } \n\nClass Scores {\u0ddd } \n\nNon-action \n\n\n\nTABLE I :\nIComparison with state-of-the-art methods on THUMOS14. Run time is the average inference time per video, including post-processing operations, such as NMS. SN: single-network. E2E: end-to-end. TS: two-stream. For proposal generation methods, the computation cost of the extra classifiers is not included (marked with >). \u2020 Results copied from[36]. \u2021 Our implementation. * With focal loss and IBIF. \u00a7 With IBIF.Method \nFeature \nSN \nE2E \nmAP 0.3 mAP 0.4 mAP 0.5 mAP 0.6 mAP 0.7 mAP \n\nTime/ms \n\nGFLOPs \n\nYeung et al. [38] \nVGG16 \n\n-\n36.0 \n26.4 \n17.1 \n-\n-\n-\n-\n-\nYuan et al. [9] \nTS \n-\n-\n36.5 \n27.8 \n17.8 \n-\n-\n-\n-\n-\nSSAD [8] \nTS \n\n-\n43.0 \n35.0 \n24.6 \n-\n-\n-\n-\n-\nR-C3D [6] \nC3D \n\n-\n44.8 \n35.6 \n28.9 \n-\n-\n-\n-\n-\nSSN [69] \nTS \n-\n-\n51.9 \n41.0 \n29.8 \n-\n-\n-\n-\n-\nTAL-Net [7] \nI3D \n\n-\n53.2 \n48.5 \n42.8 \n33.8 \n20.8 \n39.8 \n-\n-\nBSN [29] \nTS \n-\n-\n53.5 \n45.0 \n36.9 \n28.4 \n20.0 \n36.8 \n>2065 \n>3.4 \nMGG [31] \nTS \n-\n-\n53.9 \n46.8 \n37.4 \n29.5 \n21.3 \n37.8 \n-\n-\nBMN [11] \nTS \n-\n-\n56.0 \n47.4 \n38.8 \n29.7 \n20.5 \n38.5 \n>483 \n>171.0 \nBC-GNN [64] \nTS \n-\n-\n57.1 \n49.1 \n40.4 \n31.2 \n23.1 \n40.2 \n-\n-\nG-TAD [2] \nTS \n-\n-\n54.5 \n47.6 \n40.2 \n30.8 \n23.4 \n39.3 \n>4440 \n>639.8 \nBMN  \u2020 [11] \nI3D \n-\n-\n56.4 \n47.9 \n39.2 \n30.2 \n21.2 \n39.0 \n-\n-\nG-TAD  \u2021 [2] \nI3D \n-\n-\n58.7 \n52.7 \n44.9 \n33.6 \n23.8 \n42.7 \n>3552 \n>368.9 \nMR [70] \nI3D \n-\n-\n53.9 \n50.7 \n45.4 \n38.0 \n28.5 \n43.3 \n>644 \n>36.8 \nA2Net [36] \nI3D \n\n-\n58.6 \n54.1 \n45.5 \n32.5 \n17.2 \n41.6 \n1554 \n30.4 \nP-GCN [13] \nI3D \n-\n-\n63.6 \n57.8 \n49.1 \n-\n-\n-\n7298 \n4.4 \nP-GCN  \u2021 [13] \nI3D \n-\n-\n64.9 \n59.0 \n49.4 \n36.7 \n22.6 \n46.5 \n7298 \n4.4 \nG-TAD [2]+P-GCN [13] \nI3D \n-\n-\n66.4 \n60.4 \n51.6 \n37.6 \n22.9 \n47.8 \n-\n-\nAGT [61] \nI3D \n\n\n65.0 \n58.1 \n50.2 \n-\n-\n-\n-\n-\nPCG-TAL [37] \nI3D \n-\n-\n64.2 \n57.3 \n48.3 \n-\n-\n-\n-\n-\nRTD-Net [62] \nI3D \n-\n-\n68.3 \n62.3 \n51.9 \n38.8 \n23.7 \n49.0 \n>211 \n>32.1 \nAFSD [35] \nI3D \n-\n-\n67.3 \n62.4 \n55.5 \n43.7 \n31.1 \n52.0 \n3245 \n84.1 \nMUSES [27] \nI3D \n-\n-\n68.9 \n64.0 \n56.9 \n46.3 \n31.0 \n53.4 \n2101 \n34.1 \nTadTR* (Ours) \nI3D \n\n\n74.8 \n69.1 \n60.1 \n46.6 \n32.8 \n56.7 \n195 \n1.07 \nTadTR-lite* (Ours) \nI3D \n\n\n71.3 \n65.9 \n57.0 \n44.6 \n30.4 \n53.8 \n155 \n0.85 \nTadTR  \u00a7 (Ours) \nI3D \n\n\n70.3 \n64.3 \n55.7 \n44.0 \n30.0 \n52.9 \n195 \n1.07 \nTadTR (Ours) \nI3D \n\n\n67.1 \n61.1 \n52.0 \n39.9 \n26.2 \n49.3 \n195 \n1.07 \n\n\n\nTABLE II :\nIIComparison of different methods on the validation set of HACS Segments. The results of SSN are from[21]. * With focal loss.Method \nmAP0.5 mAP0.75 mAP0.95 mAP \nTime/ms \nGFLOPs \n\nSSN [69] \n28.82 \n18.80 \n5.32 \n18.97 \n-\n-\nG-TAD [2] \n41.08 \n27.59 \n8.34 \n27.48 \n941 \n45.7 \nTadTR \n45.16 \n30.70 \n11.78 \n30.83 \n19 \n0.1 \nTadTR* \n47.14 \n32.11 \n10.94 \n32.09 \n19 \n0.1 \n\n\n\nTABLE III :\nIIIComparison of different methods on ActivityNet-1.3. Methods in the second group are combined with an ensemble of action classifiers[69]. The computation costs (in FLOPs) of the action classifiers are not included. The results of BMN and G-TAD with TSP[71] features are from[71]. TS: two-stream. SN: single-network. * With focal loss.Method \nFeature \nSN \nE2E \nmAP 0.5 \nmAP 0.75 \nmAP 0.95 \nmAP \nGFLOPs \n\nSelf-contained methods \nR-C3D [6] \nC3D \n\n-\n26.80 \n-\n-\n-\n-\nSSN [10] \nTS \n-\n-\n39.12 \n23.48 \n5.49 \n23.98 \n-\nTAL-Net [7] \nI3D \n\n-\n38.23 \n18.30 \n1.30 \n20.22 \n-\nP-GCN [13] \nI3D \n-\n-\n42.90 \n28.14 \n2.47 \n26.99 \n5.0 \nPCG-TAL [37] \nI3D \n-\n-\n42.14 \n28.34 \n6.12 \n27.34 \n-\nTadTR (Ours) \nTS \n\n\n41.40 \n28.85 \n7.86 \n28.21 \n0.038 \nTadTR* (Ours) \nTS \n\n\n43.67 \n30.58 \n8.32 \n29.90 \n0.038 \n\nCombined with an ensemble of action classifiers [69] \nCDC [72] \nC3D \n-\n-\n43.83 \n25.88 \n0.21 \n22.77 \n-\nBMN [11] \nTS \n-\n-\n50.07 \n34.78 \n8.29 \n33.85 \n45.6 \nG-TAD [2] \nTS \n-\n-\n50.36 \n34.60 \n9.02 \n34.09 \n45.7 \nP-GCN [13] \nI3D \n-\n-\n48.26 \n33.16 \n3.27 \n31.11 \n5.0 \nMR [70] \nI3D \n-\n-\n43.47 \n33.91 \n9.21 \n30.12 \n-\nA2Net [36] \nI3D \n-\n-\n43.55 \n28.69 \n3.70 \n27.75 \n1.2 \nPCG-TAL [37] \nI3D \n-\n-\n50.24 \n35.21 \n7.84 \n34.01 \n-\nRTD-Net [62] \nI3D \n-\n-\n47.21 \n30.68 \n8.61 \n30.83 \n3.1 \nAFSD [35] \nI3D \n-\n-\n52.38 \n35.27 \n6.47 \n34.39 \n3.3 \nTadTR* (Ours) \nTS \n-\n-\n51.29 \n34.99 \n9.49 \n34.64 \n0.038 \nTadTR+BMN (Ours) \nTS \n-\n-\n50.51 \n35.35 \n8.18 \n34.55 \n45.6 \nTadTR* (Ours) \nI3D \n-\n-\n52.83 \n37.05 \n10.83 \n36.11 \n0.038 \nBMN [11] \nTSP \n-\n-\n51.23 \n36.78 \n9.50 \n35.67 \n45.6 \nG-TAD [2] \nTSP \n-\n-\n51.26 \n37.12 \n9.29 \n35.81 \n45.7 \nTadTR* (Ours) \nTSP \n-\n-\n53.62 \n37.52 \n10.56 \n36.75 \n0.038 \n\n\n\nTABLE IV :\nIVComparison of different variants of TadTR.Method \nHACS Segments \n\nTHUMOS14 \nActivityNet \n\nmAP 0.5 mAP 0.75 mAP 0.95 mAP \nMFLOPs \nmAP \nmAP \n\nTadTR \n45.16 \n30.70 \n11.78 \n30.83 \n100.5 \n47.92 \n28.21 \nTadTR w/o encoder \n39.65 \n26.99 \n9.08 \n26.94 \n95.3 \n40.99 \n27.34 \nTadTR w/o instance-level context \n43.11 \n29.97 \n10.43 \n29.70 \n66.8 \n45.26 \n26.23 \nTadTR with dense attention \n22.76 \n12.52 \n4.19 \n13.58 \n564.5 \n24.15 \n23.79 \nTadTR w/o actionness regression \n42.10 \n28.44 \n10.23 \n28.51 \n99.4 \n45.09 \n26.13 \nTadTR w/o segment refinement \n39.89 \n28.03 \n9.54 \n27.65 \n99.8 \n44.07 \n27.40 \n\n\n\nTABLE V :\nVComparison of the variants of TadTR with Transformer encoder and 1D CNN encoder on HACS Segments.Encoder \nAverage mAP \nMFLOPs \nw/o NMS \nw/ NMS \n\nTransformer \n30.83 \n30.53 \n100.5 \n1D CNN \n27.95 \n29.12 \n134.8 \n\n6KRUW \n0HGLXP \n/RQJ \n\n\n$YHUDJHP$3 \n\n7DG75Z'&11HQFRGHU \n7DG75 \n\nFig. 5: Comparison of the performance of TadTR with 1D \nCNN encoder and TadTR (with Transformer encoder) for \nactions with different durations on HACS Segments, measured \nby average mAP. \n\n\n\n\nDense attention v.s. temporal deformable attention. We try replacing all temporal deformable attention modules in TadTR1.0s \n8.0s \n\n54.8s \n5.5s \n\nCurling: 0.50\u21920.47 \n\nCurling: 0.39\u21920.60 \n\n0.3s \n52.0s \n\nGround Truth \nDetections \nTime \n\n11.0s \n\n76.6s \n\n19.6s \n49.0s \n\nCleaning Windows: 0.24\u21920.42 \n\n13.7s \n71.5s \nCleaning Windows: 0.22\u21920.46 \n\nCleaning Windows \n\nCurling \n\n\n\nTABLE VI :\nVIAblation study of segment refinement (SR) on ActivityNet. The results are with focal loss.Variants \n0.5 \n0.75 \n0.95 \nmAP \n\nStandard SR \n43.67 30.58 8.32 29.90 \nSR w/o incremental refinement \n42.72 29.00 6.92 28.59 \nSR w/o reference point adjustment \n42.61 28.62 6.28 28.05 \n\n\n\nTABLE VII :\nVIIEffects of the numbers of encoder layers and decoder layers on HACS Segments.L E \nL D \nmAP 0.5 mAP 0.75 mAP 0.95 mAP \nMFLOPs \n\n2 \n4 \n45.16 \n30.70 \n11.78 \n30.83 \n100.5 \n4 \n4 \n44.63 \n30.39 \n10.76 \n30.39 \n105.6 \n6 \n4 \n40.55 \n27.55 \n9.88 \n27.63 \n110.8 \n\n2 \n2 \n42.10 \n29.05 \n9.57 \n28.84 \n79.6 \n2 \n4 \n45.16 \n30.70 \n11.78 \n30.83 \n100.5 \n2 \n6 \n45.20 \n30.82 \n10.67 \n30.74 \n121.3 \n\n\n\nTABLE VIII :\nVIIIEffect of the number of action queries on different datasets. The average mAPs are reported.#queries Nq \n10 \n20 \n30 \n40 \n50 \n\nTHUMOS14 \n44.06 \n46.48 \n46.94 \n47.92 \n46.78 \nHACS Segments \n29.63 \n30.73 \n30.83 \n29.99 \n29.47 \nActivityNet-1.3 \n28.21 \n26.47 \n26.27 \n26.29 \n22.96 \n\n\n\nTABLE IX :\nIXEffect of the number of sampling points K on the performance and computation cost on HACS Segments. The models are trained for half of the full training cycle(15  epochs).#points K \nmAP 0.5 mAP 0.75 mAP 0.95 \nmAP \nMFLOPs \n\n1 \n39.22 \n27.07 \n9.80 \n27.03 \n99.1 \n2 \n40.22 \n27.62 \n10.15 \n27.61 \n99.6 \n4 \n41.20 \n28.52 \n10.63 \n28.49 \n100.5 \n8 \n39.77 \n27.15 \n9.86 \n27.20 \n102.4 \n\n\n\nTABLE X :\nXEffect of the number of attention heads M on the performance and computation cost on HACS Segments. The models are trained for half of the full training cycle (15 epochs).#heads M \nmAP 0.5 mAP 0.75 mAP 0.95 \nmAP \nMFLOPs \n\n1 \n38.62 \n25.25 \n7.94 \n25.57 \n100.2 \n2 \n40.30 \n26.37 \n8.09 \n26.52 \n100.2 \n4 \n40.74 \n28.41 \n10.51 \n28.30 \n100.3 \n8 \n41.20 \n28.52 \n10.63 \n28.49 \n100.5 \n16 \n41.07 \n28.35 \n10.23 \n28.31 \n100.9 \n\n\n\n\nTable Xcompares the performance and computation cost using different number of attention heads M on HACS Segments. It is observed that moderately increasing M boosts the performance, as more diverse features can be learned. The performance is saturated at M = 8. Similar to the number of sampling points, the number of attention heads has little impact on the computation cost.\n\nTABLE XI :\nXIEffect of the expanding factor and the number of bins T R for actionness regression on HACS Segments.Fig. 7: Visualization of temporal deformable attention. The first row is uniformly sampled video frames. The second row visualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention for the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and separate the points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color. More examples are given in the supplementary material.1 \n1.25 \n1.5 \n2 \n\nmAP \n30.27 30.40 30.83 \n30.01 \n\nT R \n8 \n16 \n32 \n-\n\nmAP \n30.39 \n30.83 30.26 \n-\nEncoder reference point \nEncoder sampling points \n\n504 \n506 \n508 \n510 \n512 \nt/sec \n\nGT: HighJump \nPred: HighJump \nDecoder reference point \nDecoder sampling points \n\nlow \n\nhigh \n\n260 \n265 \n270 \n275 \n280 \n285 \n290 \n\nt/sec \n\nJavelinThrow \nNon-action \n\nCricketBowling \nCricketShot \n\n0 \n5 \n10 \n15 \n20 \n25 \n30 \n\nt/sec \n\n#28 \n\n#7 \n#4 \n#36 \n#24 \n\n#37 \n\n#20 \n#38 \n\n#4 \n\n#12 \n\n\n\n\nvisualizes temporal deformable attention of the last encoder layer and the last decoder layer. We observe that: (1) Different attention heads focus on different temporal regions and scales. For example, the sampling points marked with left-triangle and up-triangle are distributed on the left side of the reference point. In some heads, the sampling points that are farthest away from the reference point have relatively higher attention weights, to capture useful cues for action boundaries. (2) The encoder and the decoder have different preferences for context. TheCenter \u01b8 \n\nLength \n\n\u1218 \n\n\n\n\nFig. 10: Failure cases. TadTR misses the two short actions that are hard to detect while the 1D CNN-based method MUSES partially detects them.10.8s \n12.5s \n\n10.9s \n11.6s \n\n101.6s \n103.1s \nGT \n\n101.4s \n102.6s \nMUSES \n\nTadTR \n(missed) \n\nGT \n\nMUSES \n\nTadTR \n(missed) \n\n(a) BasketballDunk \n\n(b) VolleyballSpiking \n\n\n\nTemporal action localization in untrimmed videos via multi-stage cnns. Z Shou, D Wang, S.-F Chang, CVPR. Z. Shou, D. Wang, and S.-F. Chang, \"Temporal action localization in untrimmed videos via multi-stage cnns,\" in CVPR, 2016, pp. 1049-1058.\n\nG-TAD: Sub-graph localization for temporal action detection. M Xu, C Zhao, D S Rojas, A Thabet, B Ghanem, CVPR. M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, \"G-TAD: Sub-graph localization for temporal action detection,\" in CVPR, 2020, pp. 10 156-10 165.\n\nLearning activity progression in lstms for activity detection and early detection. S Ma, L Sigal, S Sclaroff, CVPR. S. Ma, L. Sigal, and S. Sclaroff, \"Learning activity progression in lstms for activity detection and early detection,\" in CVPR, June 2016, pp. 1942-1950.\n\nTemporal action detection using a statistical language model. A Richard, J Gall, CVPR. A. Richard and J. Gall, \"Temporal action detection using a statistical language model,\" in CVPR, 2016, pp. 3131-3140.\n\nFast temporal activity proposals for efficient detection of human actions in untrimmed videos. F Caba Heilbron, J Carlos Niebles, B Ghanem, CVPR. F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem, \"Fast temporal activity proposals for efficient detection of human actions in untrimmed videos,\" in CVPR, 2016, pp. 1914-1923.\n\nR-c3d: region convolutional 3d network for temporal activity detection. H Xu, A Das, K Saenko, H. Xu, A. Das, and K. Saenko, \"R-c3d: region convolutional 3d network for temporal activity detection,\" in ICCV, 2017, pp. 5794-5803.\n\nRethinking the faster r-cnn architecture for temporal action localization. Y.-W Chao, S Vijayanarasimhan, B Seybold, D A Ross, J Deng, R Sukthankar, CVPR. Y.-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng, and R. Sukthankar, \"Rethinking the faster r-cnn architecture for temporal action localization,\" in CVPR, 2018, pp. 1130-1139.\n\nSingle shot temporal action detection. T Lin, X Zhao, Z Shou, ACMT. Lin, X. Zhao, and Z. Shou, \"Single shot temporal action detection,\" in ACM MM, 2017, pp. 988-996.\n\nTemporal action localization by structured maximal sums. Z.-H Yuan, J C Stroud, T Lu, J Deng, Z.-H. Yuan, J. C. Stroud, T. Lu, and J. Deng, \"Temporal action localization by structured maximal sums,\" in CVPR, 2017, pp. 3684- 3692.\n\nTemporal action detection with structured segment networks. Y Zhao, Y Xiong, L Wang, Z Wu, X Tang, D Lin, ICCV. Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin, \"Temporal action detection with structured segment networks,\" ICCV, pp. 2914- 2923, 2017.\n\nBmn: Boundary-matching network for temporal action proposal generation. T Lin, X Liu, X Li, E Ding, S Wen, ICCV. T. Lin, X. Liu, X. Li, E. Ding, and S. Wen, \"Bmn: Boundary-matching network for temporal action proposal generation,\" in ICCV, 2019, pp. 3889-3898.\n\nTemporal convolutional networks for action segmentation and detection. C Lea, M D Flynn, R Vidal, A Reiter, G D Hager, C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, \"Temporal convolutional networks for action segmentation and detection,\" in CVPR, 2017, pp. 156-165.\n\nGraph convolutional networks for temporal action localization. R Zeng, W Huang, M Tan, Y Rong, P Zhao, J Huang, C Gan, ICCV. R. Zeng, W. Huang, M. Tan, Y. Rong, P. Zhao, J. Huang, and C. Gan, \"Graph convolutional networks for temporal action localization,\" in ICCV, 2019, pp. 7094-7103.\n\nQuo vadis, action recognition? a new model and the kinetics dataset. J Carreira, A Zisserman, J. Carreira and A. Zisserman, \"Quo vadis, action recognition? a new model and the kinetics dataset,\" in CVPR, 2017, pp. 4724-4733.\n\nEnd-to-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, ECCV. N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \"End-to-end object detection with transformers,\" in ECCV, 2020, pp. 213-229.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in NIPS, 2017, pp. 5998-6008.\n\nTemporal context network for activity localization in videos. X Dai, B Singh, G Zhang, L S Davis, Y Q Chen, X. Dai, B. Singh, G. Zhang, L. S. Davis, and Y. Q. Chen, \"Temporal context network for activity localization in videos,\" in ICCV, 2017, pp. 5727-5736.\n\nDiagnosing error in temporal action detectors. H Alwassel, F Heilbron, V Escorcia, B Ghanem, ECCV. H. Alwassel, F. Caba Heilbron, V. Escorcia, and B. Ghanem, \"Diagnos- ing error in temporal action detectors,\" in ECCV, 2018, pp. 256-272.\n\nDeformable detr: Deformable transformers for end-to-end object detection. X Zhu, W Su, L Lu, B Li, X Wang, J Dai, ICLR. X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \"Deformable detr: Deformable transformers for end-to-end object detection,\" in ICLR, 2021.\n\nMask r-cnn. K He, G Gkioxari, P Doll\u00e1r, R Girshick, K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \"Mask r-cnn,\" in ICCV, 2017, pp. 2961-2969.\n\nHACS: human action clips and segments dataset for recognition and temporal localization. H Zhao, A Torralba, L Torresani, Z Yan, ICCV. H. Zhao, A. Torralba, L. Torresani, and Z. Yan, \"HACS: human action clips and segments dataset for recognition and temporal localization,\" in ICCV, 2019, pp. 8667-8677.\n\nThe THUMOS challenge on action recognition for videos. H Idrees, A R Zamir, Y.-G Jiang, A Gorban, I Laptev, R Sukthankar, M Shah, in the wild\",\" pp. 1-23H. Idrees, A. R. Zamir, Y.-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar, and M. Shah, \"The THUMOS challenge on action recognition for videos \"in the wild\",\" pp. 1-23, 2017.\n\nActivityNet: A large-scale video benchmark for human activity understanding. F Caba Heilbron, V Escorcia, B Ghanem, J Carlos Niebles, CVPR. F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles, \"ActivityNet: A large-scale video benchmark for human activity under- standing,\" in CVPR, 2015, pp. 961-970.\n\nTemporal action localization with pyramid of score distribution features. J Yuan, B Ni, X Yang, A A Kassim, CVPR. J. Yuan, B. Ni, X. Yang, and A. A. Kassim, \"Temporal action localization with pyramid of score distribution features,\" in CVPR, 2016, pp. 3093- 3102.\n\nScc: Semantic context cascade for efficient action detection. F C Heilbron, W Barrios, V Escorcia, B Ghanem, CVPR. F. C. Heilbron, W. Barrios, V. Escorcia, and B. Ghanem, \"Scc: Semantic context cascade for efficient action detection.\" in CVPR, 2017, pp. 3175- 3184.\n\nSelf-similarity action proposal. X Liu, Y Sun, J Lu, C Yao, Y Zhou, IEEE Signal Processing Letters. 27X. Liu, Y. Sun, J. Lu, C. Yao, and Y. Zhou, \"Self-similarity action proposal,\" IEEE Signal Processing Letters, vol. 27, pp. 2064-2068, 2020.\n\nMulti-shot temporal event localization: A benchmark. X Liu, Y Hu, S Bai, F Ding, X Bai, P H S Torr, CVPR. X. Liu, Y. Hu, S. Bai, F. Ding, X. Bai, and P. H. S. Torr, \"Multi-shot temporal event localization: A benchmark,\" in CVPR, June 2021, pp. 12 596-12 606.\n\nTurn tap: Temporal unit regression network for temporal action proposals. J Gao, Z Yang, C Sun, K Chen, R Nevatia, J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia, \"Turn tap: Temporal unit regression network for temporal action proposals,\" in ICCV, 2017, pp. 3648-3656.\n\nBsn: Boundary sensitive network for temporal action proposal generation. T Lin, X Zhao, H Su, C Wang, M Yang, ECCV. T. Lin, X. Zhao, H. Su, C. Wang, and M. Yang, \"Bsn: Boundary sensitive network for temporal action proposal generation,\" in ECCV, September 2018, pp. 3-21.\n\nCtap: Complementary temporal action proposal generation. J Gao, K Chen, R Nevatia, ECCV. J. Gao, K. Chen, and R. Nevatia, \"Ctap: Complementary temporal action proposal generation,\" in ECCV, September 2018, pp. 70-85.\n\nMulti-granularity generator for temporal action proposal. Y Liu, L Ma, Y Zhang, W Liu, S.-F Chang, CVPR. Y. Liu, L. Ma, Y. Zhang, W. Liu, and S.-F. Chang, \"Multi-granularity generator for temporal action proposal,\" in CVPR, 2019, pp. 3604-3613.\n\nDaps: Deep action proposals for action understanding. V Escorcia, F C Heilbron, J C Niebles, B Ghanem, ECCV. V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem, \"Daps: Deep action proposals for action understanding,\" in ECCV, 2016, pp. 768-784.\n\nSst: Single-stream temporal action proposals. S Buch, V Escorcia, C Shen, B Ghanem, J C Niebles, S. Buch, V. Escorcia, C. Shen, B. Ghanem, and J. C. Niebles, \"Sst: Single-stream temporal action proposals,\" in CVPR, 2017, pp. 6373- 6382.\n\nGaussian temporal awareness networks for action localization. F Long, T Yao, Z Qiu, X Tian, J Luo, T Mei, CVPR. F. Long, T. Yao, Z. Qiu, X. Tian, J. Luo, and T. Mei, \"Gaussian temporal awareness networks for action localization,\" in CVPR, 2019, pp. 344- 353.\n\nLearning salient boundary feature for anchor-free temporal action localization. C Lin, C Xu, D Luo, Y Wang, Y Tai, C Wang, J Li, F Huang, Y Fu, CVPRC. Lin, C. Xu, D. Luo, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and Y. Fu, \"Learning salient boundary feature for anchor-free temporal action localization,\" in CVPR, 2021, pp. 3320-3329.\n\nRevisiting anchor mechanisms for temporal action localization. L Yang, H Peng, D Zhang, J Fu, J Han, IEEE Transactions on Image Processing. 29L. Yang, H. Peng, D. Zhang, J. Fu, and J. Han, \"Revisiting anchor mechanisms for temporal action localization,\" IEEE Transactions on Image Processing, vol. 29, pp. 8535-8548, 2020.\n\nPcg-tal: Progressive crossgranularity cooperation for temporal action localization. R Su, D Xu, L Sheng, W Ouyang, IEEE Transactions on Image Processing. 30R. Su, D. Xu, L. Sheng, and W. Ouyang, \"Pcg-tal: Progressive cross- granularity cooperation for temporal action localization,\" IEEE Trans- actions on Image Processing, vol. 30, pp. 2103-2113, 2021.\n\nEnd-to-end learning of action detection from frame glimpses in videos. S Yeung, O Russakovsky, G Mori, L Fei-Fei, CVPR. S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, \"End-to-end learning of action detection from frame glimpses in videos,\" in CVPR, 2016, pp. 2678-2687.\n\nSf-net: Single-frame supervision for temporal action localization. F Ma, L Zhu, Y Yang, S Zha, G Kundu, M Feiszli, Z Shou, ECCV. SpringerF. Ma, L. Zhu, Y. Yang, S. Zha, G. Kundu, M. Feiszli, and Z. Shou, \"Sf-net: Single-frame supervision for temporal action localization,\" in ECCV. Springer, 2020, pp. 420-437.\n\nWeakly supervised action localization by sparse temporal pooling network. P Nguyen, T Liu, G Prasad, B Han, CVPR. P. Nguyen, T. Liu, G. Prasad, and B. Han, \"Weakly supervised action localization by sparse temporal pooling network,\" in CVPR, 2018, pp. 6752-6761.\n\nW-talc: Weakly-supervised temporal activity localization and classification. S Paul, S Roy, A K Roy-Chowdhury, ECCV. S. Paul, S. Roy, and A. K. Roy-Chowdhury, \"W-talc: Weakly-supervised temporal activity localization and classification,\" in ECCV, September 2018, pp. 588-607.\n\nCompleteness modeling and context separation for weakly supervised temporal action localization. D Liu, T Jiang, Y Wang, CVPR. D. Liu, T. Jiang, and Y. Wang, \"Completeness modeling and context separation for weakly supervised temporal action localization,\" in CVPR, 2019, pp. 1298-1307.\n\nAutoloc: Weakly-supervised temporal action localization in untrimmed videos. Z Shou, H Gao, L Zhang, K Miyazawa, S.-F Chang, in ECCV. Z. Shou, H. Gao, L. Zhang, K. Miyazawa, and S.-F. Chang, \"Autoloc: Weakly-supervised temporal action localization in untrimmed videos,\" in ECCV, 2018, pp. 154-171.\n\nTemporal structure mining for weakly supervised action detection. T Yu, Z Ren, Y Li, E Yan, N Xu, J Yuan, ICCV. T. Yu, Z. Ren, Y. Li, E. Yan, N. Xu, and J. Yuan, \"Temporal structure mining for weakly supervised action detection,\" in ICCV, 2019, pp. 5522-5531.\n\nModeling sub-actions for weakly supervised temporal action localization. L Huang, Y Huang, W Ouyang, L Wang, IEEE Transactions on Image Processing. 30L. Huang, Y. Huang, W. Ouyang, and L. Wang, \"Modeling sub-actions for weakly supervised temporal action localization,\" IEEE Transactions on Image Processing, vol. 30, pp. 5154-5167, 2021.\n\nMultiscale structure-aware network for weakly supervised temporal action detection. W Yang, T Zhang, Z Mao, Y Zhang, Q Tian, F Wu, IEEE Transactions on Image Processing. 30W. Yang, T. Zhang, Z. Mao, Y. Zhang, Q. Tian, and F. Wu, \"Multi- scale structure-aware network for weakly supervised temporal action detection,\" IEEE Transactions on Image Processing, vol. 30, pp. 5848- 5861, 2021.\n\nBreaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization. R Zeng, C Gan, P Chen, W Huang, Q Wu, M Tan, IEEE Transactions on Image Processing. 2812R. Zeng, C. Gan, P. Chen, W. Huang, Q. Wu, and M. Tan, \"Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization,\" IEEE Transactions on Image Processing, vol. 28, no. 12, pp. 5797-5808, 2019.\n\nRelational prototypical network for weakly supervised temporal action localization. L Huang, Y Huang, W Ouyang, L Wang, AAAI. 3460L. Huang, Y. Huang, W. Ouyang, and L. Wang, \"Relational prototypical network for weakly supervised temporal action localization,\" in AAAI, vol. 34, no. 07, 2020, pp. 11 053-11 060.\n\nA hybrid attention mechanism for weakly-supervised temporal action localization. A Islam, C Long, R Radke, AAAI. 35A. Islam, C. Long, and R. Radke, \"A hybrid attention mechanism for weakly-supervised temporal action localization,\" in AAAI, vol. 35, no. 2, 2021, pp. 1637-1645.\n\nTwo-stream consensus network for weakly-supervised temporal action localization. Y Zhai, L Wang, W Tang, Q Zhang, J Yuan, G Hua, ECCV. SpringerY. Zhai, L. Wang, W. Tang, Q. Zhang, J. Yuan, and G. Hua, \"Two-stream consensus network for weakly-supervised temporal action localization,\" in ECCV. Springer, 2020, pp. 37-54.\n\nForeground-action consistency network for weakly supervised temporal action localization. L Huang, L Wang, H Li, ICCV. L. Huang, L. Wang, and H. Li, \"Foreground-action consistency network for weakly supervised temporal action localization,\" in ICCV, 2021, pp. 8002-8011.\n\nPlanetr: Structure-guided transformers for 3d plane recovery. B Tan, N Xue, S Bai, T Wu, G.-S Xia, CVPR. B. Tan, N. Xue, S. Bai, T. Wu, and G.-S. Xia, \"Planetr: Structure-guided transformers for 3d plane recovery,\" in CVPR, 2021, pp. 4186-4195.\n\nTransmix: Attend to mix for vision transformers. J.-N Chen, S Sun, J He, P H Torr, A Yuille, S Bai, CVPR. 12J.-N. Chen, S. Sun, J. He, P. H. Torr, A. Yuille, and S. Bai, \"Transmix: Attend to mix for vision transformers,\" in CVPR, 2022, pp. 12 135- 12 144.\n\nTranscrowd: weakly-supervised crowd counting with transformers. D Liang, X Chen, W Xu, Y Zhou, X Bai, Science China Information Sciences. 656D. Liang, X. Chen, W. Xu, Y. Zhou, and X. Bai, \"Transcrowd: weakly-supervised crowd counting with transformers,\" Science China Information Sciences, vol. 65, no. 6, pp. 1-14, 2022.\n\nVideobert: A joint model for video and language representation learning. C Sun, A Myers, C Vondrick, K Murphy, C Schmid, ICCV. C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, \"Videobert: A joint model for video and language representation learning,\" in ICCV, 2019, pp. 7464-7473.\n\nIs space-time attention all you need for video understanding?\" in ICML. G Bertasius, H Wang, L Torresani, G. Bertasius, H. Wang, and L. Torresani, \"Is space-time attention all you need for video understanding?\" in ICML, July 2021, pp. 813-824.\n\nSeqformer: a frustratingly simple model for video instance segmentation. J Wu, Y Jiang, W Zhang, X Bai, S Bai, arXiv:2112.08275arXiv preprintJ. Wu, Y. Jiang, W. Zhang, X. Bai, and S. Bai, \"Seqformer: a frus- tratingly simple model for video instance segmentation,\" arXiv preprint arXiv:2112.08275, 2021.\n\nActbert: Learning global-local video-text representations. L Zhu, Y Yang, CVPR. L. Zhu and Y. Yang, \"Actbert: Learning global-local video-text repre- sentations,\" in CVPR, 2020, pp. 8746-8755.\n\nEnd-to-end dense video captioning with masked transformer. L Zhou, Y Zhou, J J Corso, R Socher, C Xiong, CVPR. L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, \"End-to-end dense video captioning with masked transformer,\" in CVPR, 2018, pp. 8739-8748.\n\nVideo action transformer network. R Girdhar, J Carreira, C Doersch, A Zisserman, CVPR. R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, \"Video action transformer network,\" in CVPR, 2019, pp. 244-253.\n\nActivity graph transformer for temporal action localization. M Nawhal, G Mori, arXiv:2101.08540arXiv preprintM. Nawhal and G. Mori, \"Activity graph transformer for temporal action localization,\" arXiv preprint arXiv:2101.08540, 2021.\n\nRelaxed transformer decoders for direct action proposal generation. J Tan, J Tang, L Wang, G Wu, ICCV. J. Tan, J. Tang, L. Wang, and G. Wu, \"Relaxed transformer decoders for direct action proposal generation,\" in ICCV, October 2021, pp. 13 526- 13 535.\n\nTemporal action proposal generation with transformers. L Wang, H Yang, W Wu, H Yao, H Huang, arXiv:2105.12043arXiv preprintL. Wang, H. Yang, W. Wu, H. Yao, and H. Huang, \"Temporal action pro- posal generation with transformers,\" arXiv preprint arXiv:2105.12043, 2021.\n\nBoundary content graph neural network for temporal action proposal generation. Y Bai, Y Wang, Y Tong, Y Yang, Q Liu, J Liu, ECCV. Y. Bai, Y. Wang, Y. Tong, Y. Yang, Q. Liu, and J. Liu, \"Boundary content graph neural network for temporal action proposal generation,\" in ECCV, 2020, pp. 121-137.\n\nLayer normalization. J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450arXiv preprintJ. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv preprint arXiv:1607.06450, 2016.\n\nFocal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \"Focal loss for dense object detection,\" in ICCV, 2017, pp. 2980-2988.\n\nTemporal segment networks: Towards good practices for deep action recognition. L Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang, L Van Gool, ECCV. L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool, \"Temporal segment networks: Towards good practices for deep action recognition,\" in ECCV, 2016, pp. 20-36.\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, I. Loshchilov and F. Hutter, \"Decoupled weight decay regularization,\" in ICLR, 2017, pp. 1-18.\n\nY Zhao, B Zhang, Z Wu, S Yang, L Zhou, S Yan, L Wang, Y Xiong, W Yali, D Lin, Y Qiao, X Tang, arXiv:1710.08011CUHK & ETHZ & SIAT submission to ActivityNet challenge 2017. arXiv preprintY. Zhao, B. Zhang, Z. Wu, S. Yang, L. Zhou, S. Yan, L. Wang, Y. Xiong, W. Yali, D. Lin, Y. Qiao, and X. Tang, \"CUHK & ETHZ & SIAT submission to ActivityNet challenge 2017,\" arXiv preprint arXiv:1710.08011, pp. 20-24, 2017.\n\nBottom-up temporal action localization with mutual regularization. P Zhao, L Xie, C Ju, Y Zhang, Y Wang, Q Tian, ECCV. P. Zhao, L. Xie, C. Ju, Y. Zhang, Y. Wang, and Q. Tian, \"Bottom-up temporal action localization with mutual regularization,\" in ECCV, 2020.\n\nTsp: Temporally-sensitive pretraining of video encoders for localization tasks. H Alwassel, S Giancola, B Ghanem, ICCV Workshops, 2021. H. Alwassel, S. Giancola, and B. Ghanem, \"Tsp: Temporally-sensitive pretraining of video encoders for localization tasks,\" in ICCV Workshops, 2021, pp. 3166-3176.\n\nCdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. Z Shou, J Chan, A Zareian, K Miyazawa, S.-F Chang, Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang, \"Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos,\" in ICCV, 2017, pp. 1417-1426.\n\nAn empirical study of end-to-end temporal action detection. X Liu, S Bai, X Bai, CVPR. 2019X. Liu, S. Bai, and X. Bai, \"An empirical study of end-to-end temporal action detection,\" in CVPR, June 2022, pp. 20 010-20 019.\n", "annotations": {"author": "[{\"end\":97,\"start\":70},{\"end\":104,\"start\":98}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":70},{\"end\":103,\"start\":98}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":54,\"start\":1},{\"end\":158,\"start\":105}]", "venue": null, "abstract": "[{\"end\":3789,\"start\":272}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4391,\"start\":4388},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4396,\"start\":4393},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4488,\"start\":4485},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4494,\"start\":4490},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4500,\"start\":4496},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5096,\"start\":5092},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5357,\"start\":5354},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5363,\"start\":5359},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5369,\"start\":5365},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6019,\"start\":6016},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6050,\"start\":6046},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6857,\"start\":6853},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7213,\"start\":7209},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8158,\"start\":8154},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8764,\"start\":8760},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8782,\"start\":8778},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8864,\"start\":8860},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8916,\"start\":8913},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8929,\"start\":8925},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10581,\"start\":10578},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10587,\"start\":10583},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10593,\"start\":10589},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10599,\"start\":10595},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10902,\"start\":10899},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10908,\"start\":10904},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10963,\"start\":10959},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10982,\"start\":10978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10988,\"start\":10984},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11060,\"start\":11057},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11065,\"start\":11062},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11071,\"start\":11067},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11077,\"start\":11073},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11359,\"start\":11356},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11365,\"start\":11361},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11586,\"start\":11583},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11746,\"start\":11742},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11761,\"start\":11757},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11841,\"start\":11837},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11858,\"start\":11854},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12062,\"start\":12058},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12626,\"start\":12622},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12658,\"start\":12654},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12664,\"start\":12660},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12808,\"start\":12804},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12837,\"start\":12833},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12843,\"start\":12839},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13223,\"start\":13219},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13229,\"start\":13225},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13258,\"start\":13254},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":13276,\"start\":13272},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13367,\"start\":13363},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":13456,\"start\":13452},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13547,\"start\":13543},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14030,\"start\":14026},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14085,\"start\":14081},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14099,\"start\":14095},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14999,\"start\":14996},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15005,\"start\":15001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15202,\"start\":15199},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":15208,\"start\":15204},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15258,\"start\":15254},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15496,\"start\":15493},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15501,\"start\":15498},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15507,\"start\":15503},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15596,\"start\":15592},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15621,\"start\":15617},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17840,\"start\":17836},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":19248,\"start\":19244},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19490,\"start\":19486},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19552,\"start\":19548},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19985,\"start\":19981},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20044,\"start\":20040},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20873,\"start\":20869},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21808,\"start\":21804},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24782,\"start\":24778},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27000,\"start\":26996},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28691,\"start\":28687},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28958,\"start\":28954},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30515,\"start\":30512},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30604,\"start\":30600},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30618,\"start\":30615},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":31394,\"start\":31390},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31426,\"start\":31425},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31784,\"start\":31780},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31804,\"start\":31800},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31830,\"start\":31826},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33472,\"start\":33468},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33510,\"start\":33506},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":33640,\"start\":33636},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33697,\"start\":33694},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33703,\"start\":33699},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33875,\"start\":33871},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":35205,\"start\":35201},{\"end\":35657,\"start\":35654},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":35748,\"start\":35744},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":35762,\"start\":35758},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":35786,\"start\":35782},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36019,\"start\":36016},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36021,\"start\":36019},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36225,\"start\":36222},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36239,\"start\":36235},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37355,\"start\":37351},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37369,\"start\":37366},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":37393,\"start\":37389},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37503,\"start\":37499},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39055,\"start\":39051},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39098,\"start\":39095},{\"end\":39453,\"start\":39446},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39727,\"start\":39724},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40207,\"start\":40203},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":40365,\"start\":40362},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40371,\"start\":40367},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":40377,\"start\":40373},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":40432,\"start\":40428},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":40636,\"start\":40632},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":41643,\"start\":41639},{\"end\":48556,\"start\":48550},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":54227,\"start\":54223},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57556,\"start\":57552},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":59519,\"start\":59515},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":59925,\"start\":59921},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":60045,\"start\":60041},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":60067,\"start\":60063}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55626,\"start\":55049},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55632,\"start\":55627},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56348,\"start\":55633},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56758,\"start\":56349},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57198,\"start\":56759},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":59401,\"start\":57199},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59773,\"start\":59402},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61419,\"start\":59774},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":62013,\"start\":61420},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":62487,\"start\":62014},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":62859,\"start\":62488},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":63149,\"start\":62860},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":63538,\"start\":63150},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":63831,\"start\":63539},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":64218,\"start\":63832},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":64643,\"start\":64219},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":65023,\"start\":64644},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":66164,\"start\":65024},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":66759,\"start\":66165},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":67073,\"start\":66760}]", "paragraph": "[{\"end\":4397,\"start\":3808},{\"end\":5435,\"start\":4399},{\"end\":6501,\"start\":5437},{\"end\":7635,\"start\":6503},{\"end\":8492,\"start\":7637},{\"end\":9237,\"start\":8494},{\"end\":9285,\"start\":9239},{\"end\":10292,\"start\":9287},{\"end\":11323,\"start\":10313},{\"end\":12681,\"start\":11325},{\"end\":14834,\"start\":12683},{\"end\":15431,\"start\":14836},{\"end\":17374,\"start\":15433},{\"end\":17716,\"start\":17433},{\"end\":18499,\"start\":17731},{\"end\":19491,\"start\":18519},{\"end\":20222,\"start\":19493},{\"end\":20552,\"start\":20224},{\"end\":21198,\"start\":20604},{\"end\":21809,\"start\":21258},{\"end\":22183,\"start\":21883},{\"end\":22304,\"start\":22185},{\"end\":23450,\"start\":22320},{\"end\":23745,\"start\":23467},{\"end\":24099,\"start\":23747},{\"end\":25159,\"start\":24101},{\"end\":25293,\"start\":25194},{\"end\":25370,\"start\":25336},{\"end\":25657,\"start\":25558},{\"end\":25738,\"start\":25659},{\"end\":25930,\"start\":25755},{\"end\":26281,\"start\":25962},{\"end\":26701,\"start\":26283},{\"end\":27441,\"start\":26703},{\"end\":27884,\"start\":27443},{\"end\":28796,\"start\":27886},{\"end\":29377,\"start\":28826},{\"end\":29546,\"start\":29379},{\"end\":29932,\"start\":29606},{\"end\":30654,\"start\":29934},{\"end\":30990,\"start\":30656},{\"end\":31134,\"start\":30992},{\"end\":31455,\"start\":31236},{\"end\":31668,\"start\":31457},{\"end\":32583,\"start\":31712},{\"end\":33043,\"start\":32585},{\"end\":35170,\"start\":33045},{\"end\":36660,\"start\":35172},{\"end\":38291,\"start\":36662},{\"end\":38761,\"start\":38323},{\"end\":38994,\"start\":38763},{\"end\":39581,\"start\":38996},{\"end\":40067,\"start\":39583},{\"end\":41615,\"start\":40069},{\"end\":41796,\"start\":41617},{\"end\":42075,\"start\":41818},{\"end\":44063,\"start\":42077},{\"end\":44810,\"start\":44065},{\"end\":46229,\"start\":44812},{\"end\":46569,\"start\":46231},{\"end\":47656,\"start\":46571},{\"end\":48200,\"start\":47658},{\"end\":48626,\"start\":48202},{\"end\":49157,\"start\":48628},{\"end\":49560,\"start\":49159},{\"end\":50235,\"start\":49562},{\"end\":50552,\"start\":50237},{\"end\":50793,\"start\":50554},{\"end\":51299,\"start\":50809},{\"end\":51904,\"start\":51301},{\"end\":52388,\"start\":51906},{\"end\":53099,\"start\":52390},{\"end\":54288,\"start\":53117},{\"end\":55048,\"start\":54301}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17432,\"start\":17375},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20603,\"start\":20553},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21257,\"start\":21199},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21882,\"start\":21810},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22319,\"start\":22305},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23466,\"start\":23451},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25193,\"start\":25160},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25335,\"start\":25294},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25557,\"start\":25371},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25754,\"start\":25739},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25961,\"start\":25931},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29605,\"start\":29547},{\"attributes\":{\"id\":\"formula_15\"},\"end\":31235,\"start\":31135}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36936,\"start\":36929},{\"end\":38093,\"start\":38088},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39110,\"start\":39102},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39609,\"start\":39600},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":42710,\"start\":42702},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":43599,\"start\":43592},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":45528,\"start\":45520},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":46446,\"start\":46438},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":47116,\"start\":47108},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":47845,\"start\":47837},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48257,\"start\":48247},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48816,\"start\":48807},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49212,\"start\":49204},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49680,\"start\":49672}]", "section_header": "[{\"end\":3806,\"start\":3791},{\"end\":10311,\"start\":10295},{\"end\":17729,\"start\":17719},{\"end\":18517,\"start\":18502},{\"end\":28824,\"start\":28799},{\"end\":31686,\"start\":31671},{\"end\":31710,\"start\":31689},{\"end\":38309,\"start\":38294},{\"end\":38321,\"start\":38312},{\"end\":41816,\"start\":41799},{\"end\":50807,\"start\":50796},{\"end\":53115,\"start\":53102},{\"end\":54299,\"start\":54291},{\"end\":55631,\"start\":55628},{\"end\":55642,\"start\":55634},{\"end\":56359,\"start\":56350},{\"end\":57209,\"start\":57200},{\"end\":59413,\"start\":59403},{\"end\":59786,\"start\":59775},{\"end\":61431,\"start\":61421},{\"end\":62024,\"start\":62015},{\"end\":62871,\"start\":62861},{\"end\":63162,\"start\":63151},{\"end\":63552,\"start\":63540},{\"end\":63843,\"start\":63833},{\"end\":64229,\"start\":64220},{\"end\":65035,\"start\":65025}]", "table": "[{\"end\":57198,\"start\":56762},{\"end\":59401,\"start\":57620},{\"end\":59773,\"start\":59539},{\"end\":61419,\"start\":60123},{\"end\":62013,\"start\":61476},{\"end\":62487,\"start\":62123},{\"end\":62859,\"start\":62609},{\"end\":63149,\"start\":62964},{\"end\":63538,\"start\":63243},{\"end\":63831,\"start\":63649},{\"end\":64218,\"start\":64017},{\"end\":64643,\"start\":64402},{\"end\":66164,\"start\":65701},{\"end\":66759,\"start\":66735},{\"end\":67073,\"start\":66904}]", "figure_caption": "[{\"end\":55626,\"start\":55051},{\"end\":56348,\"start\":55644},{\"end\":56758,\"start\":56362},{\"end\":56762,\"start\":56761},{\"end\":57620,\"start\":57211},{\"end\":59539,\"start\":59416},{\"end\":60123,\"start\":59790},{\"end\":61476,\"start\":61434},{\"end\":62123,\"start\":62026},{\"end\":62609,\"start\":62490},{\"end\":62964,\"start\":62874},{\"end\":63243,\"start\":63166},{\"end\":63649,\"start\":63557},{\"end\":64017,\"start\":63846},{\"end\":64402,\"start\":64231},{\"end\":65023,\"start\":64646},{\"end\":65701,\"start\":65038},{\"end\":66735,\"start\":66167},{\"end\":66904,\"start\":66762}]", "figure_ref": "[{\"end\":9074,\"start\":9068},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17849,\"start\":17843},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19018,\"start\":19012},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22415,\"start\":22409},{\"end\":26700,\"start\":26694},{\"end\":27249,\"start\":27243},{\"end\":44364,\"start\":44358},{\"end\":45078,\"start\":45072},{\"end\":46453,\"start\":46447},{\"end\":50055,\"start\":50049},{\"end\":50844,\"start\":50837},{\"end\":50851,\"start\":50845},{\"end\":51556,\"start\":51550},{\"end\":51945,\"start\":51939},{\"end\":52514,\"start\":52507},{\"end\":52550,\"start\":52544},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":54370,\"start\":54363},{\"end\":54431,\"start\":54424},{\"end\":54450,\"start\":54444},{\"end\":54551,\"start\":54544}]", "bib_author_first_name": "[{\"end\":67147,\"start\":67146},{\"end\":67155,\"start\":67154},{\"end\":67166,\"start\":67162},{\"end\":67381,\"start\":67380},{\"end\":67387,\"start\":67386},{\"end\":67395,\"start\":67394},{\"end\":67397,\"start\":67396},{\"end\":67406,\"start\":67405},{\"end\":67416,\"start\":67415},{\"end\":67668,\"start\":67667},{\"end\":67674,\"start\":67673},{\"end\":67683,\"start\":67682},{\"end\":67918,\"start\":67917},{\"end\":67929,\"start\":67928},{\"end\":68157,\"start\":68156},{\"end\":68174,\"start\":68173},{\"end\":68181,\"start\":68175},{\"end\":68192,\"start\":68191},{\"end\":68460,\"start\":68459},{\"end\":68466,\"start\":68465},{\"end\":68473,\"start\":68472},{\"end\":68696,\"start\":68692},{\"end\":68704,\"start\":68703},{\"end\":68724,\"start\":68723},{\"end\":68735,\"start\":68734},{\"end\":68737,\"start\":68736},{\"end\":68745,\"start\":68744},{\"end\":68753,\"start\":68752},{\"end\":69005,\"start\":69004},{\"end\":69012,\"start\":69011},{\"end\":69020,\"start\":69019},{\"end\":69193,\"start\":69189},{\"end\":69201,\"start\":69200},{\"end\":69203,\"start\":69202},{\"end\":69213,\"start\":69212},{\"end\":69219,\"start\":69218},{\"end\":69424,\"start\":69423},{\"end\":69432,\"start\":69431},{\"end\":69441,\"start\":69440},{\"end\":69449,\"start\":69448},{\"end\":69455,\"start\":69454},{\"end\":69463,\"start\":69462},{\"end\":69695,\"start\":69694},{\"end\":69702,\"start\":69701},{\"end\":69709,\"start\":69708},{\"end\":69715,\"start\":69714},{\"end\":69723,\"start\":69722},{\"end\":69956,\"start\":69955},{\"end\":69963,\"start\":69962},{\"end\":69965,\"start\":69964},{\"end\":69974,\"start\":69973},{\"end\":69983,\"start\":69982},{\"end\":69993,\"start\":69992},{\"end\":69995,\"start\":69994},{\"end\":70228,\"start\":70227},{\"end\":70236,\"start\":70235},{\"end\":70245,\"start\":70244},{\"end\":70252,\"start\":70251},{\"end\":70260,\"start\":70259},{\"end\":70268,\"start\":70267},{\"end\":70277,\"start\":70276},{\"end\":70522,\"start\":70521},{\"end\":70534,\"start\":70533},{\"end\":70726,\"start\":70725},{\"end\":70736,\"start\":70735},{\"end\":70745,\"start\":70744},{\"end\":70757,\"start\":70756},{\"end\":70768,\"start\":70767},{\"end\":70780,\"start\":70779},{\"end\":70981,\"start\":70980},{\"end\":70992,\"start\":70991},{\"end\":71003,\"start\":71002},{\"end\":71013,\"start\":71012},{\"end\":71026,\"start\":71025},{\"end\":71035,\"start\":71034},{\"end\":71037,\"start\":71036},{\"end\":71046,\"start\":71045},{\"end\":71056,\"start\":71055},{\"end\":71294,\"start\":71293},{\"end\":71301,\"start\":71300},{\"end\":71310,\"start\":71309},{\"end\":71319,\"start\":71318},{\"end\":71321,\"start\":71320},{\"end\":71330,\"start\":71329},{\"end\":71332,\"start\":71331},{\"end\":71539,\"start\":71538},{\"end\":71551,\"start\":71550},{\"end\":71563,\"start\":71562},{\"end\":71575,\"start\":71574},{\"end\":71804,\"start\":71803},{\"end\":71811,\"start\":71810},{\"end\":71817,\"start\":71816},{\"end\":71823,\"start\":71822},{\"end\":71829,\"start\":71828},{\"end\":71837,\"start\":71836},{\"end\":72004,\"start\":72003},{\"end\":72010,\"start\":72009},{\"end\":72022,\"start\":72021},{\"end\":72032,\"start\":72031},{\"end\":72226,\"start\":72225},{\"end\":72234,\"start\":72233},{\"end\":72246,\"start\":72245},{\"end\":72259,\"start\":72258},{\"end\":72497,\"start\":72496},{\"end\":72507,\"start\":72506},{\"end\":72509,\"start\":72508},{\"end\":72521,\"start\":72517},{\"end\":72530,\"start\":72529},{\"end\":72540,\"start\":72539},{\"end\":72550,\"start\":72549},{\"end\":72564,\"start\":72563},{\"end\":72847,\"start\":72846},{\"end\":72864,\"start\":72863},{\"end\":72876,\"start\":72875},{\"end\":72886,\"start\":72885},{\"end\":72893,\"start\":72887},{\"end\":73159,\"start\":73158},{\"end\":73167,\"start\":73166},{\"end\":73173,\"start\":73172},{\"end\":73181,\"start\":73180},{\"end\":73183,\"start\":73182},{\"end\":73412,\"start\":73411},{\"end\":73414,\"start\":73413},{\"end\":73426,\"start\":73425},{\"end\":73437,\"start\":73436},{\"end\":73449,\"start\":73448},{\"end\":73650,\"start\":73649},{\"end\":73657,\"start\":73656},{\"end\":73664,\"start\":73663},{\"end\":73670,\"start\":73669},{\"end\":73677,\"start\":73676},{\"end\":73914,\"start\":73913},{\"end\":73921,\"start\":73920},{\"end\":73927,\"start\":73926},{\"end\":73934,\"start\":73933},{\"end\":73942,\"start\":73941},{\"end\":73949,\"start\":73948},{\"end\":73953,\"start\":73950},{\"end\":74195,\"start\":74194},{\"end\":74202,\"start\":74201},{\"end\":74210,\"start\":74209},{\"end\":74217,\"start\":74216},{\"end\":74225,\"start\":74224},{\"end\":74466,\"start\":74465},{\"end\":74473,\"start\":74472},{\"end\":74481,\"start\":74480},{\"end\":74487,\"start\":74486},{\"end\":74495,\"start\":74494},{\"end\":74723,\"start\":74722},{\"end\":74730,\"start\":74729},{\"end\":74738,\"start\":74737},{\"end\":74942,\"start\":74941},{\"end\":74949,\"start\":74948},{\"end\":74955,\"start\":74954},{\"end\":74964,\"start\":74963},{\"end\":74974,\"start\":74970},{\"end\":75184,\"start\":75183},{\"end\":75196,\"start\":75195},{\"end\":75198,\"start\":75197},{\"end\":75210,\"start\":75209},{\"end\":75212,\"start\":75211},{\"end\":75223,\"start\":75222},{\"end\":75429,\"start\":75428},{\"end\":75437,\"start\":75436},{\"end\":75449,\"start\":75448},{\"end\":75457,\"start\":75456},{\"end\":75467,\"start\":75466},{\"end\":75469,\"start\":75468},{\"end\":75683,\"start\":75682},{\"end\":75691,\"start\":75690},{\"end\":75698,\"start\":75697},{\"end\":75705,\"start\":75704},{\"end\":75713,\"start\":75712},{\"end\":75720,\"start\":75719},{\"end\":75961,\"start\":75960},{\"end\":75968,\"start\":75967},{\"end\":75974,\"start\":75973},{\"end\":75981,\"start\":75980},{\"end\":75989,\"start\":75988},{\"end\":75996,\"start\":75995},{\"end\":76004,\"start\":76003},{\"end\":76010,\"start\":76009},{\"end\":76019,\"start\":76018},{\"end\":76282,\"start\":76281},{\"end\":76290,\"start\":76289},{\"end\":76298,\"start\":76297},{\"end\":76307,\"start\":76306},{\"end\":76313,\"start\":76312},{\"end\":76627,\"start\":76626},{\"end\":76633,\"start\":76632},{\"end\":76639,\"start\":76638},{\"end\":76648,\"start\":76647},{\"end\":76969,\"start\":76968},{\"end\":76978,\"start\":76977},{\"end\":76993,\"start\":76992},{\"end\":77001,\"start\":77000},{\"end\":77240,\"start\":77239},{\"end\":77246,\"start\":77245},{\"end\":77253,\"start\":77252},{\"end\":77261,\"start\":77260},{\"end\":77268,\"start\":77267},{\"end\":77277,\"start\":77276},{\"end\":77288,\"start\":77287},{\"end\":77559,\"start\":77558},{\"end\":77569,\"start\":77568},{\"end\":77576,\"start\":77575},{\"end\":77586,\"start\":77585},{\"end\":77825,\"start\":77824},{\"end\":77833,\"start\":77832},{\"end\":77840,\"start\":77839},{\"end\":77842,\"start\":77841},{\"end\":78122,\"start\":78121},{\"end\":78129,\"start\":78128},{\"end\":78138,\"start\":78137},{\"end\":78390,\"start\":78389},{\"end\":78398,\"start\":78397},{\"end\":78405,\"start\":78404},{\"end\":78414,\"start\":78413},{\"end\":78429,\"start\":78425},{\"end\":78678,\"start\":78677},{\"end\":78684,\"start\":78683},{\"end\":78691,\"start\":78690},{\"end\":78697,\"start\":78696},{\"end\":78704,\"start\":78703},{\"end\":78710,\"start\":78709},{\"end\":78946,\"start\":78945},{\"end\":78955,\"start\":78954},{\"end\":78964,\"start\":78963},{\"end\":78974,\"start\":78973},{\"end\":79296,\"start\":79295},{\"end\":79304,\"start\":79303},{\"end\":79313,\"start\":79312},{\"end\":79320,\"start\":79319},{\"end\":79329,\"start\":79328},{\"end\":79337,\"start\":79336},{\"end\":79710,\"start\":79709},{\"end\":79718,\"start\":79717},{\"end\":79725,\"start\":79724},{\"end\":79733,\"start\":79732},{\"end\":79742,\"start\":79741},{\"end\":79748,\"start\":79747},{\"end\":80127,\"start\":80126},{\"end\":80136,\"start\":80135},{\"end\":80145,\"start\":80144},{\"end\":80155,\"start\":80154},{\"end\":80436,\"start\":80435},{\"end\":80445,\"start\":80444},{\"end\":80453,\"start\":80452},{\"end\":80714,\"start\":80713},{\"end\":80722,\"start\":80721},{\"end\":80730,\"start\":80729},{\"end\":80738,\"start\":80737},{\"end\":80747,\"start\":80746},{\"end\":80755,\"start\":80754},{\"end\":81044,\"start\":81043},{\"end\":81053,\"start\":81052},{\"end\":81061,\"start\":81060},{\"end\":81288,\"start\":81287},{\"end\":81295,\"start\":81294},{\"end\":81302,\"start\":81301},{\"end\":81309,\"start\":81308},{\"end\":81318,\"start\":81314},{\"end\":81524,\"start\":81520},{\"end\":81532,\"start\":81531},{\"end\":81539,\"start\":81538},{\"end\":81545,\"start\":81544},{\"end\":81547,\"start\":81546},{\"end\":81555,\"start\":81554},{\"end\":81565,\"start\":81564},{\"end\":81793,\"start\":81792},{\"end\":81802,\"start\":81801},{\"end\":81810,\"start\":81809},{\"end\":81816,\"start\":81815},{\"end\":81824,\"start\":81823},{\"end\":82125,\"start\":82124},{\"end\":82132,\"start\":82131},{\"end\":82141,\"start\":82140},{\"end\":82153,\"start\":82152},{\"end\":82163,\"start\":82162},{\"end\":82414,\"start\":82413},{\"end\":82427,\"start\":82426},{\"end\":82435,\"start\":82434},{\"end\":82660,\"start\":82659},{\"end\":82666,\"start\":82665},{\"end\":82675,\"start\":82674},{\"end\":82684,\"start\":82683},{\"end\":82691,\"start\":82690},{\"end\":82951,\"start\":82950},{\"end\":82958,\"start\":82957},{\"end\":83145,\"start\":83144},{\"end\":83153,\"start\":83152},{\"end\":83161,\"start\":83160},{\"end\":83163,\"start\":83162},{\"end\":83172,\"start\":83171},{\"end\":83182,\"start\":83181},{\"end\":83379,\"start\":83378},{\"end\":83390,\"start\":83389},{\"end\":83402,\"start\":83401},{\"end\":83413,\"start\":83412},{\"end\":83613,\"start\":83612},{\"end\":83623,\"start\":83622},{\"end\":83855,\"start\":83854},{\"end\":83862,\"start\":83861},{\"end\":83870,\"start\":83869},{\"end\":83878,\"start\":83877},{\"end\":84096,\"start\":84095},{\"end\":84104,\"start\":84103},{\"end\":84112,\"start\":84111},{\"end\":84118,\"start\":84117},{\"end\":84125,\"start\":84124},{\"end\":84389,\"start\":84388},{\"end\":84396,\"start\":84395},{\"end\":84404,\"start\":84403},{\"end\":84412,\"start\":84411},{\"end\":84420,\"start\":84419},{\"end\":84427,\"start\":84426},{\"end\":84626,\"start\":84625},{\"end\":84628,\"start\":84627},{\"end\":84634,\"start\":84633},{\"end\":84636,\"start\":84635},{\"end\":84645,\"start\":84644},{\"end\":84647,\"start\":84646},{\"end\":84833,\"start\":84829},{\"end\":84840,\"start\":84839},{\"end\":84849,\"start\":84848},{\"end\":84861,\"start\":84860},{\"end\":84867,\"start\":84866},{\"end\":85084,\"start\":85083},{\"end\":85092,\"start\":85091},{\"end\":85101,\"start\":85100},{\"end\":85109,\"start\":85108},{\"end\":85117,\"start\":85116},{\"end\":85124,\"start\":85123},{\"end\":85132,\"start\":85131},{\"end\":85368,\"start\":85367},{\"end\":85382,\"start\":85381},{\"end\":85488,\"start\":85487},{\"end\":85496,\"start\":85495},{\"end\":85505,\"start\":85504},{\"end\":85511,\"start\":85510},{\"end\":85519,\"start\":85518},{\"end\":85527,\"start\":85526},{\"end\":85534,\"start\":85533},{\"end\":85542,\"start\":85541},{\"end\":85551,\"start\":85550},{\"end\":85559,\"start\":85558},{\"end\":85566,\"start\":85565},{\"end\":85574,\"start\":85573},{\"end\":85964,\"start\":85963},{\"end\":85972,\"start\":85971},{\"end\":85979,\"start\":85978},{\"end\":85985,\"start\":85984},{\"end\":85994,\"start\":85993},{\"end\":86002,\"start\":86001},{\"end\":86237,\"start\":86236},{\"end\":86249,\"start\":86248},{\"end\":86261,\"start\":86260},{\"end\":86564,\"start\":86563},{\"end\":86572,\"start\":86571},{\"end\":86580,\"start\":86579},{\"end\":86591,\"start\":86590},{\"end\":86606,\"start\":86602},{\"end\":86875,\"start\":86874},{\"end\":86882,\"start\":86881},{\"end\":86889,\"start\":86888}]", "bib_author_last_name": "[{\"end\":67152,\"start\":67148},{\"end\":67160,\"start\":67156},{\"end\":67172,\"start\":67167},{\"end\":67384,\"start\":67382},{\"end\":67392,\"start\":67388},{\"end\":67403,\"start\":67398},{\"end\":67413,\"start\":67407},{\"end\":67423,\"start\":67417},{\"end\":67671,\"start\":67669},{\"end\":67680,\"start\":67675},{\"end\":67692,\"start\":67684},{\"end\":67926,\"start\":67919},{\"end\":67934,\"start\":67930},{\"end\":68171,\"start\":68158},{\"end\":68189,\"start\":68182},{\"end\":68199,\"start\":68193},{\"end\":68463,\"start\":68461},{\"end\":68470,\"start\":68467},{\"end\":68480,\"start\":68474},{\"end\":68701,\"start\":68697},{\"end\":68721,\"start\":68705},{\"end\":68732,\"start\":68725},{\"end\":68742,\"start\":68738},{\"end\":68750,\"start\":68746},{\"end\":68764,\"start\":68754},{\"end\":69009,\"start\":69006},{\"end\":69017,\"start\":69013},{\"end\":69025,\"start\":69021},{\"end\":69198,\"start\":69194},{\"end\":69210,\"start\":69204},{\"end\":69216,\"start\":69214},{\"end\":69224,\"start\":69220},{\"end\":69429,\"start\":69425},{\"end\":69438,\"start\":69433},{\"end\":69446,\"start\":69442},{\"end\":69452,\"start\":69450},{\"end\":69460,\"start\":69456},{\"end\":69467,\"start\":69464},{\"end\":69699,\"start\":69696},{\"end\":69706,\"start\":69703},{\"end\":69712,\"start\":69710},{\"end\":69720,\"start\":69716},{\"end\":69727,\"start\":69724},{\"end\":69960,\"start\":69957},{\"end\":69971,\"start\":69966},{\"end\":69980,\"start\":69975},{\"end\":69990,\"start\":69984},{\"end\":70001,\"start\":69996},{\"end\":70233,\"start\":70229},{\"end\":70242,\"start\":70237},{\"end\":70249,\"start\":70246},{\"end\":70257,\"start\":70253},{\"end\":70265,\"start\":70261},{\"end\":70274,\"start\":70269},{\"end\":70281,\"start\":70278},{\"end\":70531,\"start\":70523},{\"end\":70544,\"start\":70535},{\"end\":70733,\"start\":70727},{\"end\":70742,\"start\":70737},{\"end\":70754,\"start\":70746},{\"end\":70765,\"start\":70758},{\"end\":70777,\"start\":70769},{\"end\":70790,\"start\":70781},{\"end\":70989,\"start\":70982},{\"end\":71000,\"start\":70993},{\"end\":71010,\"start\":71004},{\"end\":71023,\"start\":71014},{\"end\":71032,\"start\":71027},{\"end\":71043,\"start\":71038},{\"end\":71053,\"start\":71047},{\"end\":71067,\"start\":71057},{\"end\":71298,\"start\":71295},{\"end\":71307,\"start\":71302},{\"end\":71316,\"start\":71311},{\"end\":71327,\"start\":71322},{\"end\":71337,\"start\":71333},{\"end\":71548,\"start\":71540},{\"end\":71560,\"start\":71552},{\"end\":71572,\"start\":71564},{\"end\":71582,\"start\":71576},{\"end\":71808,\"start\":71805},{\"end\":71814,\"start\":71812},{\"end\":71820,\"start\":71818},{\"end\":71826,\"start\":71824},{\"end\":71834,\"start\":71830},{\"end\":71841,\"start\":71838},{\"end\":72007,\"start\":72005},{\"end\":72019,\"start\":72011},{\"end\":72029,\"start\":72023},{\"end\":72041,\"start\":72033},{\"end\":72231,\"start\":72227},{\"end\":72243,\"start\":72235},{\"end\":72256,\"start\":72247},{\"end\":72263,\"start\":72260},{\"end\":72504,\"start\":72498},{\"end\":72515,\"start\":72510},{\"end\":72527,\"start\":72522},{\"end\":72537,\"start\":72531},{\"end\":72547,\"start\":72541},{\"end\":72561,\"start\":72551},{\"end\":72569,\"start\":72565},{\"end\":72861,\"start\":72848},{\"end\":72873,\"start\":72865},{\"end\":72883,\"start\":72877},{\"end\":72901,\"start\":72894},{\"end\":73164,\"start\":73160},{\"end\":73170,\"start\":73168},{\"end\":73178,\"start\":73174},{\"end\":73190,\"start\":73184},{\"end\":73423,\"start\":73415},{\"end\":73434,\"start\":73427},{\"end\":73446,\"start\":73438},{\"end\":73456,\"start\":73450},{\"end\":73654,\"start\":73651},{\"end\":73661,\"start\":73658},{\"end\":73667,\"start\":73665},{\"end\":73674,\"start\":73671},{\"end\":73682,\"start\":73678},{\"end\":73918,\"start\":73915},{\"end\":73924,\"start\":73922},{\"end\":73931,\"start\":73928},{\"end\":73939,\"start\":73935},{\"end\":73946,\"start\":73943},{\"end\":73958,\"start\":73954},{\"end\":74199,\"start\":74196},{\"end\":74207,\"start\":74203},{\"end\":74214,\"start\":74211},{\"end\":74222,\"start\":74218},{\"end\":74233,\"start\":74226},{\"end\":74470,\"start\":74467},{\"end\":74478,\"start\":74474},{\"end\":74484,\"start\":74482},{\"end\":74492,\"start\":74488},{\"end\":74500,\"start\":74496},{\"end\":74727,\"start\":74724},{\"end\":74735,\"start\":74731},{\"end\":74746,\"start\":74739},{\"end\":74946,\"start\":74943},{\"end\":74952,\"start\":74950},{\"end\":74961,\"start\":74956},{\"end\":74968,\"start\":74965},{\"end\":74980,\"start\":74975},{\"end\":75193,\"start\":75185},{\"end\":75207,\"start\":75199},{\"end\":75220,\"start\":75213},{\"end\":75230,\"start\":75224},{\"end\":75434,\"start\":75430},{\"end\":75446,\"start\":75438},{\"end\":75454,\"start\":75450},{\"end\":75464,\"start\":75458},{\"end\":75477,\"start\":75470},{\"end\":75688,\"start\":75684},{\"end\":75695,\"start\":75692},{\"end\":75702,\"start\":75699},{\"end\":75710,\"start\":75706},{\"end\":75717,\"start\":75714},{\"end\":75724,\"start\":75721},{\"end\":75965,\"start\":75962},{\"end\":75971,\"start\":75969},{\"end\":75978,\"start\":75975},{\"end\":75986,\"start\":75982},{\"end\":75993,\"start\":75990},{\"end\":76001,\"start\":75997},{\"end\":76007,\"start\":76005},{\"end\":76016,\"start\":76011},{\"end\":76022,\"start\":76020},{\"end\":76287,\"start\":76283},{\"end\":76295,\"start\":76291},{\"end\":76304,\"start\":76299},{\"end\":76310,\"start\":76308},{\"end\":76317,\"start\":76314},{\"end\":76630,\"start\":76628},{\"end\":76636,\"start\":76634},{\"end\":76645,\"start\":76640},{\"end\":76655,\"start\":76649},{\"end\":76975,\"start\":76970},{\"end\":76990,\"start\":76979},{\"end\":76998,\"start\":76994},{\"end\":77009,\"start\":77002},{\"end\":77243,\"start\":77241},{\"end\":77250,\"start\":77247},{\"end\":77258,\"start\":77254},{\"end\":77265,\"start\":77262},{\"end\":77274,\"start\":77269},{\"end\":77285,\"start\":77278},{\"end\":77293,\"start\":77289},{\"end\":77566,\"start\":77560},{\"end\":77573,\"start\":77570},{\"end\":77583,\"start\":77577},{\"end\":77590,\"start\":77587},{\"end\":77830,\"start\":77826},{\"end\":77837,\"start\":77834},{\"end\":77856,\"start\":77843},{\"end\":78126,\"start\":78123},{\"end\":78135,\"start\":78130},{\"end\":78143,\"start\":78139},{\"end\":78395,\"start\":78391},{\"end\":78402,\"start\":78399},{\"end\":78411,\"start\":78406},{\"end\":78423,\"start\":78415},{\"end\":78435,\"start\":78430},{\"end\":78681,\"start\":78679},{\"end\":78688,\"start\":78685},{\"end\":78694,\"start\":78692},{\"end\":78701,\"start\":78698},{\"end\":78707,\"start\":78705},{\"end\":78715,\"start\":78711},{\"end\":78952,\"start\":78947},{\"end\":78961,\"start\":78956},{\"end\":78971,\"start\":78965},{\"end\":78979,\"start\":78975},{\"end\":79301,\"start\":79297},{\"end\":79310,\"start\":79305},{\"end\":79317,\"start\":79314},{\"end\":79326,\"start\":79321},{\"end\":79334,\"start\":79330},{\"end\":79340,\"start\":79338},{\"end\":79715,\"start\":79711},{\"end\":79722,\"start\":79719},{\"end\":79730,\"start\":79726},{\"end\":79739,\"start\":79734},{\"end\":79745,\"start\":79743},{\"end\":79752,\"start\":79749},{\"end\":80133,\"start\":80128},{\"end\":80142,\"start\":80137},{\"end\":80152,\"start\":80146},{\"end\":80160,\"start\":80156},{\"end\":80442,\"start\":80437},{\"end\":80450,\"start\":80446},{\"end\":80459,\"start\":80454},{\"end\":80719,\"start\":80715},{\"end\":80727,\"start\":80723},{\"end\":80735,\"start\":80731},{\"end\":80744,\"start\":80739},{\"end\":80752,\"start\":80748},{\"end\":80759,\"start\":80756},{\"end\":81050,\"start\":81045},{\"end\":81058,\"start\":81054},{\"end\":81064,\"start\":81062},{\"end\":81292,\"start\":81289},{\"end\":81299,\"start\":81296},{\"end\":81306,\"start\":81303},{\"end\":81312,\"start\":81310},{\"end\":81322,\"start\":81319},{\"end\":81529,\"start\":81525},{\"end\":81536,\"start\":81533},{\"end\":81542,\"start\":81540},{\"end\":81552,\"start\":81548},{\"end\":81562,\"start\":81556},{\"end\":81569,\"start\":81566},{\"end\":81799,\"start\":81794},{\"end\":81807,\"start\":81803},{\"end\":81813,\"start\":81811},{\"end\":81821,\"start\":81817},{\"end\":81828,\"start\":81825},{\"end\":82129,\"start\":82126},{\"end\":82138,\"start\":82133},{\"end\":82150,\"start\":82142},{\"end\":82160,\"start\":82154},{\"end\":82170,\"start\":82164},{\"end\":82424,\"start\":82415},{\"end\":82432,\"start\":82428},{\"end\":82445,\"start\":82436},{\"end\":82663,\"start\":82661},{\"end\":82672,\"start\":82667},{\"end\":82681,\"start\":82676},{\"end\":82688,\"start\":82685},{\"end\":82695,\"start\":82692},{\"end\":82955,\"start\":82952},{\"end\":82963,\"start\":82959},{\"end\":83150,\"start\":83146},{\"end\":83158,\"start\":83154},{\"end\":83169,\"start\":83164},{\"end\":83179,\"start\":83173},{\"end\":83188,\"start\":83183},{\"end\":83387,\"start\":83380},{\"end\":83399,\"start\":83391},{\"end\":83410,\"start\":83403},{\"end\":83423,\"start\":83414},{\"end\":83620,\"start\":83614},{\"end\":83628,\"start\":83624},{\"end\":83859,\"start\":83856},{\"end\":83867,\"start\":83863},{\"end\":83875,\"start\":83871},{\"end\":83881,\"start\":83879},{\"end\":84101,\"start\":84097},{\"end\":84109,\"start\":84105},{\"end\":84115,\"start\":84113},{\"end\":84122,\"start\":84119},{\"end\":84131,\"start\":84126},{\"end\":84393,\"start\":84390},{\"end\":84401,\"start\":84397},{\"end\":84409,\"start\":84405},{\"end\":84417,\"start\":84413},{\"end\":84424,\"start\":84421},{\"end\":84431,\"start\":84428},{\"end\":84631,\"start\":84629},{\"end\":84642,\"start\":84637},{\"end\":84654,\"start\":84648},{\"end\":84837,\"start\":84834},{\"end\":84846,\"start\":84841},{\"end\":84858,\"start\":84850},{\"end\":84864,\"start\":84862},{\"end\":84874,\"start\":84868},{\"end\":85089,\"start\":85085},{\"end\":85098,\"start\":85093},{\"end\":85106,\"start\":85102},{\"end\":85114,\"start\":85110},{\"end\":85121,\"start\":85118},{\"end\":85129,\"start\":85125},{\"end\":85141,\"start\":85133},{\"end\":85379,\"start\":85369},{\"end\":85389,\"start\":85383},{\"end\":85493,\"start\":85489},{\"end\":85502,\"start\":85497},{\"end\":85508,\"start\":85506},{\"end\":85516,\"start\":85512},{\"end\":85524,\"start\":85520},{\"end\":85531,\"start\":85528},{\"end\":85539,\"start\":85535},{\"end\":85548,\"start\":85543},{\"end\":85556,\"start\":85552},{\"end\":85563,\"start\":85560},{\"end\":85571,\"start\":85567},{\"end\":85579,\"start\":85575},{\"end\":85969,\"start\":85965},{\"end\":85976,\"start\":85973},{\"end\":85982,\"start\":85980},{\"end\":85991,\"start\":85986},{\"end\":85999,\"start\":85995},{\"end\":86007,\"start\":86003},{\"end\":86246,\"start\":86238},{\"end\":86258,\"start\":86250},{\"end\":86268,\"start\":86262},{\"end\":86569,\"start\":86565},{\"end\":86577,\"start\":86573},{\"end\":86588,\"start\":86581},{\"end\":86600,\"start\":86592},{\"end\":86612,\"start\":86607},{\"end\":86879,\"start\":86876},{\"end\":86886,\"start\":86883},{\"end\":86893,\"start\":86890}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14602383},\"end\":67317,\"start\":67075},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208291175},\"end\":67582,\"start\":67319},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16133383},\"end\":67853,\"start\":67584},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6671105},\"end\":68059,\"start\":67855},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":827570},\"end\":68385,\"start\":68061},{\"attributes\":{\"id\":\"b5\"},\"end\":68615,\"start\":68387},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5011503},\"end\":68963,\"start\":68617},{\"attributes\":{\"id\":\"b7\"},\"end\":69130,\"start\":68965},{\"attributes\":{\"id\":\"b8\"},\"end\":69361,\"start\":69132},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1353488},\"end\":69620,\"start\":69363},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":198179957},\"end\":69882,\"start\":69622},{\"attributes\":{\"id\":\"b11\"},\"end\":70162,\"start\":69884},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":202538533},\"end\":70450,\"start\":70164},{\"attributes\":{\"id\":\"b13\"},\"end\":70676,\"start\":70452},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":218889832},\"end\":70951,\"start\":70678},{\"attributes\":{\"id\":\"b15\"},\"end\":71229,\"start\":70953},{\"attributes\":{\"id\":\"b16\"},\"end\":71489,\"start\":71231},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":51865607},\"end\":71727,\"start\":71491},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":222208633},\"end\":71989,\"start\":71729},{\"attributes\":{\"id\":\"b19\"},\"end\":72134,\"start\":71991},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":68049510},\"end\":72439,\"start\":72136},{\"attributes\":{\"id\":\"b21\"},\"end\":72767,\"start\":72441},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1710722},\"end\":73082,\"start\":72769},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9060481},\"end\":73347,\"start\":73084},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3748011},\"end\":73614,\"start\":73349},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":228093834},\"end\":73858,\"start\":73616},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":229297884},\"end\":74118,\"start\":73860},{\"attributes\":{\"id\":\"b27\"},\"end\":74390,\"start\":74120},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":47009464},\"end\":74663,\"start\":74392},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":49742219},\"end\":74881,\"start\":74665},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53848229},\"end\":75127,\"start\":74883},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11171922},\"end\":75380,\"start\":75129},{\"attributes\":{\"id\":\"b32\"},\"end\":75618,\"start\":75382},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":196183677},\"end\":75878,\"start\":75620},{\"attributes\":{\"id\":\"b34\"},\"end\":76216,\"start\":75880},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":221202136},\"end\":76540,\"start\":76218},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":229317989},\"end\":76895,\"start\":76542},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4019281},\"end\":77170,\"start\":76897},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":212726098},\"end\":77482,\"start\":77172},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4537553},\"end\":77745,\"start\":77484},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51866361},\"end\":78022,\"start\":77747},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":195489759},\"end\":78310,\"start\":78024},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52958676},\"end\":78609,\"start\":78312},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":201705136},\"end\":78870,\"start\":78611},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":234495641},\"end\":79209,\"start\":78872},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":235597130},\"end\":79597,\"start\":79211},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":195192016},\"end\":80040,\"start\":79599},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":213611472},\"end\":80352,\"start\":80042},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":230435678},\"end\":80630,\"start\":80354},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":225039854},\"end\":80951,\"start\":80632},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":237091343},\"end\":81223,\"start\":80953},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":236469397},\"end\":81469,\"start\":81225},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":244346829},\"end\":81726,\"start\":81471},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":248510102},\"end\":82049,\"start\":81728},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":102483628},\"end\":82339,\"start\":82051},{\"attributes\":{\"id\":\"b55\"},\"end\":82584,\"start\":82341},{\"attributes\":{\"doi\":\"arXiv:2112.08275\",\"id\":\"b56\"},\"end\":82889,\"start\":82586},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":219617394},\"end\":83083,\"start\":82891},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":4564155},\"end\":83342,\"start\":83085},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":54447971},\"end\":83549,\"start\":83344},{\"attributes\":{\"doi\":\"arXiv:2101.08540\",\"id\":\"b60\"},\"end\":83784,\"start\":83551},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":231786590},\"end\":84038,\"start\":83786},{\"attributes\":{\"doi\":\"arXiv:2105.12043\",\"id\":\"b62\"},\"end\":84307,\"start\":84040},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":220961523},\"end\":84602,\"start\":84309},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b64\"},\"end\":84788,\"start\":84604},{\"attributes\":{\"id\":\"b65\"},\"end\":85002,\"start\":84790},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":5711057},\"end\":85326,\"start\":85004},{\"attributes\":{\"id\":\"b67\"},\"end\":85485,\"start\":85328},{\"attributes\":{\"doi\":\"arXiv:1710.08011\",\"id\":\"b68\"},\"end\":85894,\"start\":85487},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":220546492},\"end\":86154,\"start\":85896},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":227126998},\"end\":86454,\"start\":86156},{\"attributes\":{\"id\":\"b71\"},\"end\":86812,\"start\":86456},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":247996809},\"end\":87033,\"start\":86814}]", "bib_title": "[{\"end\":67144,\"start\":67075},{\"end\":67378,\"start\":67319},{\"end\":67665,\"start\":67584},{\"end\":67915,\"start\":67855},{\"end\":68154,\"start\":68061},{\"end\":68690,\"start\":68617},{\"end\":69421,\"start\":69363},{\"end\":69692,\"start\":69622},{\"end\":70225,\"start\":70164},{\"end\":70723,\"start\":70678},{\"end\":71536,\"start\":71491},{\"end\":71801,\"start\":71729},{\"end\":72223,\"start\":72136},{\"end\":72844,\"start\":72769},{\"end\":73156,\"start\":73084},{\"end\":73409,\"start\":73349},{\"end\":73647,\"start\":73616},{\"end\":73911,\"start\":73860},{\"end\":74463,\"start\":74392},{\"end\":74720,\"start\":74665},{\"end\":74939,\"start\":74883},{\"end\":75181,\"start\":75129},{\"end\":75680,\"start\":75620},{\"end\":76279,\"start\":76218},{\"end\":76624,\"start\":76542},{\"end\":76966,\"start\":76897},{\"end\":77237,\"start\":77172},{\"end\":77556,\"start\":77484},{\"end\":77822,\"start\":77747},{\"end\":78119,\"start\":78024},{\"end\":78387,\"start\":78312},{\"end\":78675,\"start\":78611},{\"end\":78943,\"start\":78872},{\"end\":79293,\"start\":79211},{\"end\":79707,\"start\":79599},{\"end\":80124,\"start\":80042},{\"end\":80433,\"start\":80354},{\"end\":80711,\"start\":80632},{\"end\":81041,\"start\":80953},{\"end\":81285,\"start\":81225},{\"end\":81518,\"start\":81471},{\"end\":81790,\"start\":81728},{\"end\":82122,\"start\":82051},{\"end\":82948,\"start\":82891},{\"end\":83142,\"start\":83085},{\"end\":83376,\"start\":83344},{\"end\":83852,\"start\":83786},{\"end\":84386,\"start\":84309},{\"end\":85081,\"start\":85004},{\"end\":85961,\"start\":85896},{\"end\":86234,\"start\":86156},{\"end\":86872,\"start\":86814}]", "bib_author": "[{\"end\":67154,\"start\":67146},{\"end\":67162,\"start\":67154},{\"end\":67174,\"start\":67162},{\"end\":67386,\"start\":67380},{\"end\":67394,\"start\":67386},{\"end\":67405,\"start\":67394},{\"end\":67415,\"start\":67405},{\"end\":67425,\"start\":67415},{\"end\":67673,\"start\":67667},{\"end\":67682,\"start\":67673},{\"end\":67694,\"start\":67682},{\"end\":67928,\"start\":67917},{\"end\":67936,\"start\":67928},{\"end\":68173,\"start\":68156},{\"end\":68191,\"start\":68173},{\"end\":68201,\"start\":68191},{\"end\":68465,\"start\":68459},{\"end\":68472,\"start\":68465},{\"end\":68482,\"start\":68472},{\"end\":68703,\"start\":68692},{\"end\":68723,\"start\":68703},{\"end\":68734,\"start\":68723},{\"end\":68744,\"start\":68734},{\"end\":68752,\"start\":68744},{\"end\":68766,\"start\":68752},{\"end\":69011,\"start\":69004},{\"end\":69019,\"start\":69011},{\"end\":69027,\"start\":69019},{\"end\":69200,\"start\":69189},{\"end\":69212,\"start\":69200},{\"end\":69218,\"start\":69212},{\"end\":69226,\"start\":69218},{\"end\":69431,\"start\":69423},{\"end\":69440,\"start\":69431},{\"end\":69448,\"start\":69440},{\"end\":69454,\"start\":69448},{\"end\":69462,\"start\":69454},{\"end\":69469,\"start\":69462},{\"end\":69701,\"start\":69694},{\"end\":69708,\"start\":69701},{\"end\":69714,\"start\":69708},{\"end\":69722,\"start\":69714},{\"end\":69729,\"start\":69722},{\"end\":69962,\"start\":69955},{\"end\":69973,\"start\":69962},{\"end\":69982,\"start\":69973},{\"end\":69992,\"start\":69982},{\"end\":70003,\"start\":69992},{\"end\":70235,\"start\":70227},{\"end\":70244,\"start\":70235},{\"end\":70251,\"start\":70244},{\"end\":70259,\"start\":70251},{\"end\":70267,\"start\":70259},{\"end\":70276,\"start\":70267},{\"end\":70283,\"start\":70276},{\"end\":70533,\"start\":70521},{\"end\":70546,\"start\":70533},{\"end\":70735,\"start\":70725},{\"end\":70744,\"start\":70735},{\"end\":70756,\"start\":70744},{\"end\":70767,\"start\":70756},{\"end\":70779,\"start\":70767},{\"end\":70792,\"start\":70779},{\"end\":70991,\"start\":70980},{\"end\":71002,\"start\":70991},{\"end\":71012,\"start\":71002},{\"end\":71025,\"start\":71012},{\"end\":71034,\"start\":71025},{\"end\":71045,\"start\":71034},{\"end\":71055,\"start\":71045},{\"end\":71069,\"start\":71055},{\"end\":71300,\"start\":71293},{\"end\":71309,\"start\":71300},{\"end\":71318,\"start\":71309},{\"end\":71329,\"start\":71318},{\"end\":71339,\"start\":71329},{\"end\":71550,\"start\":71538},{\"end\":71562,\"start\":71550},{\"end\":71574,\"start\":71562},{\"end\":71584,\"start\":71574},{\"end\":71810,\"start\":71803},{\"end\":71816,\"start\":71810},{\"end\":71822,\"start\":71816},{\"end\":71828,\"start\":71822},{\"end\":71836,\"start\":71828},{\"end\":71843,\"start\":71836},{\"end\":72009,\"start\":72003},{\"end\":72021,\"start\":72009},{\"end\":72031,\"start\":72021},{\"end\":72043,\"start\":72031},{\"end\":72233,\"start\":72225},{\"end\":72245,\"start\":72233},{\"end\":72258,\"start\":72245},{\"end\":72265,\"start\":72258},{\"end\":72506,\"start\":72496},{\"end\":72517,\"start\":72506},{\"end\":72529,\"start\":72517},{\"end\":72539,\"start\":72529},{\"end\":72549,\"start\":72539},{\"end\":72563,\"start\":72549},{\"end\":72571,\"start\":72563},{\"end\":72863,\"start\":72846},{\"end\":72875,\"start\":72863},{\"end\":72885,\"start\":72875},{\"end\":72903,\"start\":72885},{\"end\":73166,\"start\":73158},{\"end\":73172,\"start\":73166},{\"end\":73180,\"start\":73172},{\"end\":73192,\"start\":73180},{\"end\":73425,\"start\":73411},{\"end\":73436,\"start\":73425},{\"end\":73448,\"start\":73436},{\"end\":73458,\"start\":73448},{\"end\":73656,\"start\":73649},{\"end\":73663,\"start\":73656},{\"end\":73669,\"start\":73663},{\"end\":73676,\"start\":73669},{\"end\":73684,\"start\":73676},{\"end\":73920,\"start\":73913},{\"end\":73926,\"start\":73920},{\"end\":73933,\"start\":73926},{\"end\":73941,\"start\":73933},{\"end\":73948,\"start\":73941},{\"end\":73960,\"start\":73948},{\"end\":74201,\"start\":74194},{\"end\":74209,\"start\":74201},{\"end\":74216,\"start\":74209},{\"end\":74224,\"start\":74216},{\"end\":74235,\"start\":74224},{\"end\":74472,\"start\":74465},{\"end\":74480,\"start\":74472},{\"end\":74486,\"start\":74480},{\"end\":74494,\"start\":74486},{\"end\":74502,\"start\":74494},{\"end\":74729,\"start\":74722},{\"end\":74737,\"start\":74729},{\"end\":74748,\"start\":74737},{\"end\":74948,\"start\":74941},{\"end\":74954,\"start\":74948},{\"end\":74963,\"start\":74954},{\"end\":74970,\"start\":74963},{\"end\":74982,\"start\":74970},{\"end\":75195,\"start\":75183},{\"end\":75209,\"start\":75195},{\"end\":75222,\"start\":75209},{\"end\":75232,\"start\":75222},{\"end\":75436,\"start\":75428},{\"end\":75448,\"start\":75436},{\"end\":75456,\"start\":75448},{\"end\":75466,\"start\":75456},{\"end\":75479,\"start\":75466},{\"end\":75690,\"start\":75682},{\"end\":75697,\"start\":75690},{\"end\":75704,\"start\":75697},{\"end\":75712,\"start\":75704},{\"end\":75719,\"start\":75712},{\"end\":75726,\"start\":75719},{\"end\":75967,\"start\":75960},{\"end\":75973,\"start\":75967},{\"end\":75980,\"start\":75973},{\"end\":75988,\"start\":75980},{\"end\":75995,\"start\":75988},{\"end\":76003,\"start\":75995},{\"end\":76009,\"start\":76003},{\"end\":76018,\"start\":76009},{\"end\":76024,\"start\":76018},{\"end\":76289,\"start\":76281},{\"end\":76297,\"start\":76289},{\"end\":76306,\"start\":76297},{\"end\":76312,\"start\":76306},{\"end\":76319,\"start\":76312},{\"end\":76632,\"start\":76626},{\"end\":76638,\"start\":76632},{\"end\":76647,\"start\":76638},{\"end\":76657,\"start\":76647},{\"end\":76977,\"start\":76968},{\"end\":76992,\"start\":76977},{\"end\":77000,\"start\":76992},{\"end\":77011,\"start\":77000},{\"end\":77245,\"start\":77239},{\"end\":77252,\"start\":77245},{\"end\":77260,\"start\":77252},{\"end\":77267,\"start\":77260},{\"end\":77276,\"start\":77267},{\"end\":77287,\"start\":77276},{\"end\":77295,\"start\":77287},{\"end\":77568,\"start\":77558},{\"end\":77575,\"start\":77568},{\"end\":77585,\"start\":77575},{\"end\":77592,\"start\":77585},{\"end\":77832,\"start\":77824},{\"end\":77839,\"start\":77832},{\"end\":77858,\"start\":77839},{\"end\":78128,\"start\":78121},{\"end\":78137,\"start\":78128},{\"end\":78145,\"start\":78137},{\"end\":78397,\"start\":78389},{\"end\":78404,\"start\":78397},{\"end\":78413,\"start\":78404},{\"end\":78425,\"start\":78413},{\"end\":78437,\"start\":78425},{\"end\":78683,\"start\":78677},{\"end\":78690,\"start\":78683},{\"end\":78696,\"start\":78690},{\"end\":78703,\"start\":78696},{\"end\":78709,\"start\":78703},{\"end\":78717,\"start\":78709},{\"end\":78954,\"start\":78945},{\"end\":78963,\"start\":78954},{\"end\":78973,\"start\":78963},{\"end\":78981,\"start\":78973},{\"end\":79303,\"start\":79295},{\"end\":79312,\"start\":79303},{\"end\":79319,\"start\":79312},{\"end\":79328,\"start\":79319},{\"end\":79336,\"start\":79328},{\"end\":79342,\"start\":79336},{\"end\":79717,\"start\":79709},{\"end\":79724,\"start\":79717},{\"end\":79732,\"start\":79724},{\"end\":79741,\"start\":79732},{\"end\":79747,\"start\":79741},{\"end\":79754,\"start\":79747},{\"end\":80135,\"start\":80126},{\"end\":80144,\"start\":80135},{\"end\":80154,\"start\":80144},{\"end\":80162,\"start\":80154},{\"end\":80444,\"start\":80435},{\"end\":80452,\"start\":80444},{\"end\":80461,\"start\":80452},{\"end\":80721,\"start\":80713},{\"end\":80729,\"start\":80721},{\"end\":80737,\"start\":80729},{\"end\":80746,\"start\":80737},{\"end\":80754,\"start\":80746},{\"end\":80761,\"start\":80754},{\"end\":81052,\"start\":81043},{\"end\":81060,\"start\":81052},{\"end\":81066,\"start\":81060},{\"end\":81294,\"start\":81287},{\"end\":81301,\"start\":81294},{\"end\":81308,\"start\":81301},{\"end\":81314,\"start\":81308},{\"end\":81324,\"start\":81314},{\"end\":81531,\"start\":81520},{\"end\":81538,\"start\":81531},{\"end\":81544,\"start\":81538},{\"end\":81554,\"start\":81544},{\"end\":81564,\"start\":81554},{\"end\":81571,\"start\":81564},{\"end\":81801,\"start\":81792},{\"end\":81809,\"start\":81801},{\"end\":81815,\"start\":81809},{\"end\":81823,\"start\":81815},{\"end\":81830,\"start\":81823},{\"end\":82131,\"start\":82124},{\"end\":82140,\"start\":82131},{\"end\":82152,\"start\":82140},{\"end\":82162,\"start\":82152},{\"end\":82172,\"start\":82162},{\"end\":82426,\"start\":82413},{\"end\":82434,\"start\":82426},{\"end\":82447,\"start\":82434},{\"end\":82665,\"start\":82659},{\"end\":82674,\"start\":82665},{\"end\":82683,\"start\":82674},{\"end\":82690,\"start\":82683},{\"end\":82697,\"start\":82690},{\"end\":82957,\"start\":82950},{\"end\":82965,\"start\":82957},{\"end\":83152,\"start\":83144},{\"end\":83160,\"start\":83152},{\"end\":83171,\"start\":83160},{\"end\":83181,\"start\":83171},{\"end\":83190,\"start\":83181},{\"end\":83389,\"start\":83378},{\"end\":83401,\"start\":83389},{\"end\":83412,\"start\":83401},{\"end\":83425,\"start\":83412},{\"end\":83622,\"start\":83612},{\"end\":83630,\"start\":83622},{\"end\":83861,\"start\":83854},{\"end\":83869,\"start\":83861},{\"end\":83877,\"start\":83869},{\"end\":83883,\"start\":83877},{\"end\":84103,\"start\":84095},{\"end\":84111,\"start\":84103},{\"end\":84117,\"start\":84111},{\"end\":84124,\"start\":84117},{\"end\":84133,\"start\":84124},{\"end\":84395,\"start\":84388},{\"end\":84403,\"start\":84395},{\"end\":84411,\"start\":84403},{\"end\":84419,\"start\":84411},{\"end\":84426,\"start\":84419},{\"end\":84433,\"start\":84426},{\"end\":84633,\"start\":84625},{\"end\":84644,\"start\":84633},{\"end\":84656,\"start\":84644},{\"end\":84839,\"start\":84829},{\"end\":84848,\"start\":84839},{\"end\":84860,\"start\":84848},{\"end\":84866,\"start\":84860},{\"end\":84876,\"start\":84866},{\"end\":85091,\"start\":85083},{\"end\":85100,\"start\":85091},{\"end\":85108,\"start\":85100},{\"end\":85116,\"start\":85108},{\"end\":85123,\"start\":85116},{\"end\":85131,\"start\":85123},{\"end\":85143,\"start\":85131},{\"end\":85381,\"start\":85367},{\"end\":85391,\"start\":85381},{\"end\":85495,\"start\":85487},{\"end\":85504,\"start\":85495},{\"end\":85510,\"start\":85504},{\"end\":85518,\"start\":85510},{\"end\":85526,\"start\":85518},{\"end\":85533,\"start\":85526},{\"end\":85541,\"start\":85533},{\"end\":85550,\"start\":85541},{\"end\":85558,\"start\":85550},{\"end\":85565,\"start\":85558},{\"end\":85573,\"start\":85565},{\"end\":85581,\"start\":85573},{\"end\":85971,\"start\":85963},{\"end\":85978,\"start\":85971},{\"end\":85984,\"start\":85978},{\"end\":85993,\"start\":85984},{\"end\":86001,\"start\":85993},{\"end\":86009,\"start\":86001},{\"end\":86248,\"start\":86236},{\"end\":86260,\"start\":86248},{\"end\":86270,\"start\":86260},{\"end\":86571,\"start\":86563},{\"end\":86579,\"start\":86571},{\"end\":86590,\"start\":86579},{\"end\":86602,\"start\":86590},{\"end\":86614,\"start\":86602},{\"end\":86881,\"start\":86874},{\"end\":86888,\"start\":86881},{\"end\":86895,\"start\":86888}]", "bib_venue": "[{\"end\":67178,\"start\":67174},{\"end\":67429,\"start\":67425},{\"end\":67698,\"start\":67694},{\"end\":67940,\"start\":67936},{\"end\":68205,\"start\":68201},{\"end\":68457,\"start\":68387},{\"end\":68770,\"start\":68766},{\"end\":69002,\"start\":68965},{\"end\":69187,\"start\":69132},{\"end\":69473,\"start\":69469},{\"end\":69733,\"start\":69729},{\"end\":69953,\"start\":69884},{\"end\":70287,\"start\":70283},{\"end\":70519,\"start\":70452},{\"end\":70796,\"start\":70792},{\"end\":70978,\"start\":70953},{\"end\":71291,\"start\":71231},{\"end\":71588,\"start\":71584},{\"end\":71847,\"start\":71843},{\"end\":72001,\"start\":71991},{\"end\":72269,\"start\":72265},{\"end\":72494,\"start\":72441},{\"end\":72907,\"start\":72903},{\"end\":73196,\"start\":73192},{\"end\":73462,\"start\":73458},{\"end\":73714,\"start\":73684},{\"end\":73964,\"start\":73960},{\"end\":74192,\"start\":74120},{\"end\":74506,\"start\":74502},{\"end\":74752,\"start\":74748},{\"end\":74986,\"start\":74982},{\"end\":75236,\"start\":75232},{\"end\":75426,\"start\":75382},{\"end\":75730,\"start\":75726},{\"end\":75958,\"start\":75880},{\"end\":76356,\"start\":76319},{\"end\":76694,\"start\":76657},{\"end\":77015,\"start\":77011},{\"end\":77299,\"start\":77295},{\"end\":77596,\"start\":77592},{\"end\":77862,\"start\":77858},{\"end\":78149,\"start\":78145},{\"end\":78444,\"start\":78437},{\"end\":78721,\"start\":78717},{\"end\":79018,\"start\":78981},{\"end\":79379,\"start\":79342},{\"end\":79791,\"start\":79754},{\"end\":80166,\"start\":80162},{\"end\":80465,\"start\":80461},{\"end\":80765,\"start\":80761},{\"end\":81070,\"start\":81066},{\"end\":81328,\"start\":81324},{\"end\":81575,\"start\":81571},{\"end\":81864,\"start\":81830},{\"end\":82176,\"start\":82172},{\"end\":82411,\"start\":82341},{\"end\":82657,\"start\":82586},{\"end\":82969,\"start\":82965},{\"end\":83194,\"start\":83190},{\"end\":83429,\"start\":83425},{\"end\":83610,\"start\":83551},{\"end\":83887,\"start\":83883},{\"end\":84093,\"start\":84040},{\"end\":84437,\"start\":84433},{\"end\":84623,\"start\":84604},{\"end\":84827,\"start\":84790},{\"end\":85147,\"start\":85143},{\"end\":85365,\"start\":85328},{\"end\":85656,\"start\":85597},{\"end\":86013,\"start\":86009},{\"end\":86290,\"start\":86270},{\"end\":86561,\"start\":86456},{\"end\":86899,\"start\":86895}]"}}}, "year": 2023, "month": 12, "day": 17}