{"id": 36089988, "updated": "2023-09-29 06:00:19.528", "metadata": {"title": "ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms", "authors": "[{\"first\":\"Martin\",\"last\":\"Aumuller\",\"middle\":[]},{\"first\":\"Erik\",\"last\":\"Bernhardsson\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Faithfull\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 7, "day": 15}, "abstract": "This paper describes ANN-Benchmarks, a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. It provides a standard interface for measuring the performance and quality achieved by nearest neighbor algorithms on different standard data sets. It supports several different ways of integrating $k$-NN algorithms, and its configuration system automatically tests a range of parameter settings for each algorithm. Algorithms are compared with respect to many different (approximate) quality measures, and adding more is easy and fast; the included plotting front-ends can visualise these as images, $\\LaTeX$ plots, and websites with interactive plots. ANN-Benchmarks aims to provide a constantly updated overview of the current state of the art of $k$-NN algorithms. In the short term, this overview allows users to choose the correct $k$-NN algorithm and parameters for their similarity search task; in the longer term, algorithm designers will be able to use this overview to test and refine automatic parameter tuning. The paper gives an overview of the system, evaluates the results of the benchmark, and points out directions for future work. Interestingly, very different approaches to $k$-NN search yield comparable quality-performance trade-offs. The system is available at http://ann-benchmarks.com .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1807.05614", "mag": "2972698727", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/is/AumullerBF20", "doi": "10.1016/j.is.2019.02.006"}}, "content": {"source": {"pdf_hash": "a6062e155f6553951ac6f92cda713e5f247bdece", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1807.05614v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1807.05614", "status": "GREEN"}}, "grobid": {"id": "0185416d9a7047cf31dcc388ab7f1d278cd0bf6c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a6062e155f6553951ac6f92cda713e5f247bdece.txt", "contents": "\nANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms *\n\n\nMartin Aum\u00fcller \nIT University of Copenhagen\nDenmark\n\nErik Bernhardsson \nBetter Inc\n\n\nAlexander Faithfull \nIT University of Copenhagen\nDenmark\n\nANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms *\n10.4230/LIPIcs1998 ACM Subject Classification H.3.3 Information Search and Retrieval Keywords and phrases benchmarking, nearest neighbor search, evaluation Digital Object Identifier 10.4230/LIPIcs...\nThis paper describes ANN-Benchmarks, a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. It provides a standard interface for measuring the performance and quality achieved by nearest neighbor algorithms on di erent standard data sets. It supports several di erent ways of integrating k-NN algorithms, and its con guration system automatically tests a range of parameter settings for each algorithm. Algorithms are compared with respect to many different (approximate) quality measures, and adding more is easy and fast; the included plotting frontends can visualise these as images, L A T E X plots, and websites with interactive plots. ANN-Benchmarks aims to provide a constantly updated overview of the current state of the art of k-NN algorithms. In the short term, this overview allows users to choose the correct k-NN algorithm and parameters for their similarity search task; in the longer term, algorithm designers will be able to use this overview to test and re ne automatic parameter tuning. The paper gives an overview of the system, evaluates the results of the benchmark, and points out directions for future work. Interestingly, very di erent approaches to k-NN search yield comparable quality-performance trade-o s. The system is available at\n\nIntroduction\n\nNearest neighbor search is one of the most fundamental tools in many areas of computer science, such as image recognition, machine learning, and computational linguistics. For example, one can use nearest neighbor search on image descriptors such as MNIST [25] to recognize handwritten digits, or one can nd semantically similar phrases to a given phrase by applying the word2vec embedding [31] and nding nearest neighbors. The latter can, for example, be used to tag articles on a news website and recommend new articles to readers that have shown an interest in a certain topic. In some cases, a generic nearest neighbor search under a suitable distance or measure of similarity o ers surprising quality improvements [9].\n\nIn many applications, the data points are described by high-dimensional vectors, usually ranging from 100 to 1000 dimensions. A phenomenon called the curse of dimensionality, a consequence of several popular algorithmic hardness conjectures (see [4,38]), tells us that, to obtain the true nearest neighbors, we have to use either linear time (in the size of the dataset) or time/space that is exponential in the dimensionality of the dataset. In the case of massive high-dimensional datasets, this rules out e cient and exact nearest neighbor search algorithms.\n\nTo obtain e cient algorithms, research has focused on allowing the returned neighbors to be an approximation of the true nearest neighbors. Usually, this means that the answer to nding the nearest neighbors to a query point is judged by how close (in some technical sense) the result set is to the set of true nearest neighbors.\n\nThere exist many di erent algorithmic techniques for nding approximate nearest neighbors. Classical algorithms such as kd-trees [6] or M-trees [11] can simulate this by terminating the search early, for example shown by Zezula et al. [39] for M-trees. Other techniques [28,29] build a graph from the dataset, where each vertex is associated with a data point, and a vertex is adjacent to its true nearest neighbors in the data set. Others involve projecting data points into a lower-dimensional space using hashing. A lot of research has been conducted with respect to locality-sensitive hashing (LSH) [18], but there exist many other techniques that rely on hashing for nding nearest neighbors; see [36] for a survey on the topic. We note that, in the realm of LSH-based techniques, algorithms guarantee sublinear query time, but solve a problem that is only distantly related to nding the k nearest neighbors of a query point. In practice, this could mean that the algorithm runs slower than a linear scan through the data, and counter-measures have to be taken to avoid this behavior [3,34].\n\nGiven the di culty of the problem of nding nearest neighbors in high-dimensional spaces and the wide range of di erent solutions at hand, it is natural to ask how these algorithms perform in empirical settings. Fortunately, many of these techniques already have good implementations: see, e.g., [32,7,27] for tree-based, [8,13] for graph-based, and [5] for LSH-based solutions. This means that a new (variant of an existing) algorithm can show its worth by comparing itself to the many previous algorithms on a collection of standard benchmark datasets with respect to a collection of quality measures. What often happens, however, is that the evaluation of a new implementation is based on a small set of competing algorithms and a small number of selected datasets. This approach poses problems for everyone involved: (i) The implementation's authors, because competing implementations might be unavailable, they might use other conventions for input data and output of results, or the original paper might omit certain required parameter settings (and, even if these are available, exhaustive experimentation can take lots of CPU time). (ii) Their reviewers and readers, because experimental results are di cult to reproduce and the selection of datasets and quality measures might appear selective. This paper proposes a way of standardizing benchmarking for nearest neighbor search algorithms, taking into account their properties and quality measures. Our benchmarking framework provides a uni ed approach to experimentation and comparison with existing work. The framework has already been used for experimental comparison in other papers [28] (to refer to parameter choice of algorithms) and algorithms have been contributed by the community, e.g., by the authors of NMSLib [8] and FALCONN [5]. An earlier version of our framework is already widely used as a benchmark referred to from other websites, see, e.g., [8,5,7,27,13].\n\nRelated work. Generating reproducible experimental results is one of the greatest challenges in many areas of computer science, in particular in the machine learning community. As an example, openml.org [35] and codalab.org provide researchers with excellent platforms to share reproducible research results. The automatic benchmarking system developed in connection with the mlpack machine learning library [12,15] shares many characteristics with our framework: it automates the process of running algorithms with preset parameters on certain datasets, and can visualize these results. However, the underlying approach is very di erent: it invokes whatever tools the implementations provide and parses their standard output to extract result metrics. Consequently, the system relies solely on the correctness of the algorithms' own implementations of quality measures, and adding a new quality measure would require a change in every single algorithm implementation. Very recently, Li et al. [26] presented a comparison of many approximate nearest neighbor algorithms, including many algorithms that are considered in our framework as well. Their approach is to take existing algorithm implementations and to heavily modify them to t a common style of query processing, in the process changing compiler ags (and sometimes even core parts of the implementation). There is no general framework, and including new features again requires manual changes in each single algorithm.\n\nOur benchmarking framework does not aim to replace these tools; instead, it complements them by taking a di erent approach. We only require that algorithms expose a simple programmatic interface for building data structures from training data and running queries. All the timing and quality measure computation is conducted within our framework, which lets us add new metrics without rerunning the algorithms, if the metric can be computed from the set of returned elements. Moreover, we benchmark each implementation as intended by the author. That means that we benchmark implementations, rather than algorithmic ideas [22].\n\nContributions. We describe our system for benchmarking approximate nearest neighbor algorithms with the general approach described in Section 3. The system allows for easy experimentation with k-NN algorithms, and visualizes algorithm runs in an approachable way. Moreover, in Section 4 we use our benchmark suite to overview the performance and quality of current state-of-the-art k-NN algorithms. This allows us to identify areas that already have competitive algorithms, to compare di erent methodological approaches to nearest neighbor search, but also to point out challenging datasets and metrics, where good implementations are missing or do not take full advantage of properties of the underlying metric. Having this overview has immediate practical bene ts, as users can select the right combination of algorithm and parameters for their application. In the longer term, we expect that more algorithms will become able to tune their own parameters according to the user's needs, and our benchmark suite will also serve as a testbed for this automatic tuning.\n\n\nProblem Definition and ality Measures\n\nWe assume that we want to nd nearest neighbors in a space X with a distance measure dist : X \u00d7 X \u2192 R, for example the d-dimensional Euclidean space R d under Euclidean distance (l 2 norm), or Hamming space {0, 1} d under Hamming distance. An algorithm A for nearest neighbor search builds a data structure DS A for a data set S \u2282 X of n points. In a preprocessing phase, it creates DS A to support the following type of queries: For a query point q \u2208 X and an integer k, return a result tuple \u03c0 = (p 1 , . . . , p k ) of k \u2264 k distinct points from S that are \"close\" to the query q. Nearest neighbor search algorithms generate \u03c0 by re ning a set C \u2286 S of candidate points w.r.t. q by choosing the k closest points among those using distance computations. The size of C (and thus the number of distance computations) is denoted by N . We let \u03c0 * = (p * 1 , . . . , p * k ) denote the tuple containing the true k nearest neighbors for q in S (where ties are broken arbitrarily). We assume in the following that all tuples are sorted according to their distance to q.\n\n\nality Measures\n\nWe use di erent notions of \"recall\" as a measure of the quality of the result returned by the algorithm. Intuitively, recall is the ratio of the number of points in the result tuple that are true nearest neighbors to the number k of true nearest neighbors. However, this intuitive de nition is fragile when distances are not distinct or when we try to add a notion of approximation to it. To avoid these issues, we use the following distance-based de nitions of recall and (1 + \u03b5)-approximative recall, that take the distance of the k-th true nearest neighbor as threshold distance.\nrecall(\u03c0, \u03c0 * ) = |{p contained in \u03c0 | dist(p, q) \u2264 dist(p * k , q)}| k recall \u03b5 (\u03c0, \u03c0 * ) = |{p contained in \u03c0 | dist(p, q) \u2264 (1 + \u03b5)dist(p * k , q)}| k , for \u03b5 > 0.\n(If all distances are distinct, recall(\u03c0, \u03c0 * ) matches the intuitive notion of recall.) We note that (approximate) recall in high dimensions is sometimes criticised; see, for example, [8, Section 2.1]. We investigate the impact of approximation as part of the evaluation in Section 4, and plan to include other quality measures such as position-related measures [39] in future work.\n\n\nPerformance Measures\n\nWith regard to the performance, we use the performance measures de ned in Table 1, which are divided into measures of the performance of the preprocessing step, i.e., generation of the data structure, and measures of the performance of the query algorithm. With respect to the query performance, di erent communities are interested in di erent cost values. Some rely on actual timings of query times, where others rely on the number of distance computations. The framework can take both of these measures into account. However, none of the currently included algorithms report the number of distance computations.\n\n\nSystem Design\n\nANN-Benchmarks is implemented as a Python framework with several di erent front-ends: one script for running experiments and a handful of others for working with and plotting results. It automatically downloads datasets when they are needed and uses Docker build les to install algorithm implementations and their dependencies.\n\nThis section gives only a high-level overview of the system; see http://ann-benchmarks. com for more detailed technical information.\n\n\nAlgorithm implementations\n\nEach implementation is installed via a Docker build le. These les specify how an implementation should be installed on a standard Ubuntu system by building and installing its dependencies and code. ANN-Benchmarks requires that this installation process also build Python wrappers for the implementation to give the framework access to it.\n\nAdding support for a new algorithm implementation to ANN-Benchmarks is as easy as writing a Docker le to install it and its dependencies, making it available to Python by writing a wrapper (or by reusing an existing one), and adding the parameters to be tested to the con guration les. Most of the installation scripts fetch the latest version of their library from its Git repository, but there is no requirement to do this; indeed, installing several di erent versions of a library would make it possible to use the framework for regression testing.\n\nWe emphasise at this point that we are explicitly comparing algorithm implementations. Implementations make many di erent decisions that will a ect their performance and two implementations of the same algorithm can have somewhat di erent performance characteristics [22]. When implementations expose other quality measures -such as the number of distance computations, which are more suited for comparing algorithms on a more abstract level -our framework will also collect this information.\n\nLocal mode. Using Docker is ideal for evaluating the performance of well-tuned implementations, but ANN-Benchmarks can also be used to help in the development process. To support this use case, the framework provides a local mode, which runs processes locally on the host system and not inside a Docker container. This makes it much easier to build a pipeline solution to, for example, automatically check how changes in the implementation in uence its performance -in the standard Docker setup, each change would require the Docker container to be rebuilt.\n\nAlgorithm wrappers. To be usable by our system, each of the implementations to be tested must have some kind of Python interface. Many libraries already provide their own Python wrappers, either written by hand or automatically generated using a tool like SWIG; others are implemented partly or entirely in Python.\n\nTo bring implementations that do not provide a Python interface into the framework, we specify a simple text-based protocol that supports the few operations we care about: parameter con guration, sending training data, and running queries. The framework comes with a wrapper that communicates with external programs using this protocol. In this way, experiments can be run in external front-end processes implemented in any programming language.\n\nThe protocol has been designed to be easy to implement. Every message is a line of text that will be split into tokens according to the rules of the POSIX shell, good implementations of which are available for most programming languages. The protocol is exible and extensible: front-ends are free to include extra information in replies, and they can also implement special con guration options that cause them to diverge from the protocol's basic behaviour. As an example, we provide a simple C implementation that supports an alternative query mode in which parsing and preparing a query data point and running a query are two di erent commands. (As the overhead of parsing a string representation of a data point is introduced by the use of the protocol, removing it makes the timings more representative.)\n\nThe use of a plaintext protocol necessarily adds some overhead, but this is often not terribly signi cant -indeed, we have found that a na\u00efve linear search implemented in Java and invoked using the protocol is faster than a na\u00efve Python implementation.\n\n\nDatasets and ground truth\n\nBy default, the framework fetches datasets on demand from a remote server. These dataset les contain, in HDF5 format, the set of data points, the set of query points, the distance metric that should be used to compare them, a list of the true nearest k = 100 neighbours for each query point, and a list of the distances of each of these neighbours from the query point.\n\nThe framework also includes a script for generating dataset les from the original datasets. Although using the precomputed hosted versions is normally simpler, the script can be used to, for example, build a dataset le with a di erent value of k, or to convert a private dataset for the framework's use. Most of the datasets use as their query set a pseudorandomly-selected set of ten thousand entries separated from the rest of the training data; others have separate query sets. The dataset le generation script makes this decision.\n\n\nCreating algorithm instances\n\nAfter loading the dataset, the framework moves on to creating the algorithm instances. It does so based on a YAML con guration le that speci es a hierarchy of dictionaries: the rst level speci es the point type, the second the distance metric, and the third each algorithm implementation to be tested. Each implementation gives the name of its wrapper's Python constructor; a number of other entries are then expanded to give the arguments to that constructor. Figure 1 shows an example of this con guration le.\n\nThe base-args list consists of those arguments that should be prepended to every invocation of the constructor. Figure 1 also shows one of the special keywords, \"@metric\", that is used to pass one of the framework's con guration parameters to the constructor.\n\nAlgorithms must specify one or more \"run groups\", each of which will be expanded into one or more lists of constructor arguments. The args entry completes the argument list, but not directly: instead, the Cartesian product of all of its entries is used to generate many lists of arguments. Another entry, query-args, is expanded in the same way as args, but each argument list generated from it is used to recon gure the query parameters of an algorithm instance after its internal data structures have been built. This allows built data structures to be reused, greatly reducing duplicated work.\n\nAs an example, the megasrch entry in Figure 1 expands into three di erent algorithm instances: MEGASRCH(\"euclidean\", \"lake\", 100), MEGASRCH(\"euclidean\", \"lake\", 200), and MEGASRCH(\"euclidean\", \"sea\", 1000). Each of these will be trained once and then used to run a number of experiments: the rst two will run experiments with each of the query parameter groups [ Figure 2 Overview of the interaction between ANN-Benchmarks and an algorithm instance under test.\n\n\nThe experiment loop\n\nOnce the framework knows what instances should be run, it moves on to the experiment loop, shown in Figure 2. The loop consists of two phases. In the preprocessing phase, an algorithm instance builds an index data structure for the dataset X. The loop then transitions to the query phase, in which query points are sent one by one to the algorithm instance. For each query point, the instance returns (at most) k data points; after answering a query, it can also report any extra information it might have, such as the number of candidates considered, i.e., the number of exact distances computed. The instance is then recon gured with a new set of query parameters, and the query set is run repeatedly, until no more sets of these parameters remain. Each algorithm instance is run in an isolated Docker container. This makes it easy to clean up after each run: simply terminating the container takes care of everything. Moving experiments out of the main process also gives us a simple and implementation-agnostic way of computing the memory usage of an implementation: the subprocess records its total memory consumption before and after initialising the algorithm instance's data structures and compares the two values.\n\nThe complete results of each run are written to the host by mounting part of the le system into the Docker container. The main process performs a blocking, timed wait on the container, and will terminate it if the user-con gurable timeout is exceeded before any results are available.\n\nDataset size. In its current form, ANN-Benchmarks supports benchmarking in-memory nearestneighbor algorithms. In particular, the dataset is kept in memory by ANN-Benchmarks when running experiments. This has to be taken into account when choosing datasets to include into the framework. In practice, this means that the framework can handle datasets with millions of points of dimensionality up to a few thousand dimensions.\n\n\nMulti-threading and batched queries\n\nThe experiment loop is, by default, run on a single CPU in a single thread. The single-threaded mode is enforced when the Docker container is started, using the Linux kernel's cpusets capabilities to restrict access to the system's resources. Running on a single CPU makes the comparison between implementations fairer, since all implementations run on the same grounds.\n\nHowever, parallelizing single queries or using parallelism over queries is an important topic. In fact, in many real-world systems that deploy nearest-neighbor algorithms, queries can be batched together. This means that the data structure receives a sequence of queries all at once, and returns results for all of the queries contained in that sequence. This enables many interesting approaches to parallelization that would not be possible when running single queries.\n\nANN-Benchmarks supports these systems with a batch mode, in which the whole set of queries is given to the implementation wrapper at once. In this mode, all resources of the host system are made available to the Docker container. The behaviour of the experiment loop diverges slightly from Figure 2 in batch mode. Batch queries do not return a sequence of tuples containing answers to the individual queries; instead, these results are obtained via an additional method, akin to Figure 2's getAdditional() method. This allows an algorithm to return the result of a batch query as an opaque internal data structure; this will stop the clock, and the additional call can then transform that data structure into Python objects without that transformation imposing a performance penalty.\n\nBatch query mode is particularly useful for running nearest-neighbor algorithms on a GPU. In this context, transferring a single query point to the GPU memory and getting the result of the query from the GPU can be a dominating part of the query time, as we will see in Section 4.\n\n\nResults and metrics\n\nFor each run, we store the full name -including the parameters -of the algorithm instance, the time it took to build its index data structure, and the results of every query: the near neighbours returned by the algorithm, the time it took to nd these, and their distances from the query point, along with any additional information the implementation might have exposed. (To avoid a ecting the timing of algorithms that do not indicate the distance of a result, the experiment loop independently re-computes distance values after the query has otherwise nished.)\n\nThe results of each run are stored in a separate HDF5 le in a directory hierarchy that encodes part of the framework's con guration. Keeping runs in separate les makes them easy to enumerate and easy to re-run, and individual results -or sets of results -can easily be shared to make results more transparent.\n\nMetric functions are passed the ground truth and the results for a particular run; they can then compute their result however they see t. Adding a new quality metric is a matter of writing a short Python function and adding it to an internal data structure; the plotting scripts query this data structure and will automatically support the new metric.\n\n\nFrontend\n\nANN-Benchmarks provides two options to evaluate the results of the experiments: a script to generate individual plots using Python's matplotlib and a script to generate a website that summarizes the results and provides interactive plots with the option to export the plot as L A T E X code using pgfplots. See Figure 3 for an example. Plots depict the Pareto frontier over all runs of an algorithm; this gives an immediate impression of the algorithm's general characteristics, at the cost of concealing some of the detail. When more detail is desired, the scripts can also produce scatter plots.\n\nAs batch mode goes to greater lengths to reduce overhead than the normal query mode and exposes more of the system's resources to the implementation being tested, results obtained in batch mode are always presented separately by the evaluation scripts to make the comparisons fairer.\n\n\nEvaluation\n\nIn this section we present a short evaluation of our ndings from running benchmarks in the benchmarking framework. After discussing the evaluated implementations and datasets, we will present four questions that we intended to answer using the framework. Subsequently, we will discuss the answers to these questions and present some observations regarding the build time of indexes and their ability to answer batched queries. At the end of this section, we present a summary of our ndings.\n\nExperimental setup. All experiments were run in Docker containers on Amazon EC2 c5.4xlarge instances that are equipped with Intel Xeon Platinum 8124M CPU (16 cores available, 3.00 GHz, Figure 3 Interactive plot screen from framework's website (cropped). Plot shows \"Queries per second\" (y-axis, log-scaled) against \"Recall\" (x-axis, not shown). Highlighted data point corresponds to a run of Annoy with parameters as depicted, giving about 1249 queries per second for a recall of about 0.52.\n\n\nPrinciple\n\nAlgorithms k-NN graph KGraph (KG) [13], SWGraph (SWG) [29,8], HNSW [28,8] [20] (inverted le) Table 2 Overview of tested algorithms (abbr. in parentheses). Implementations in italics have \"recall\" as quality measure provided as an input parameter. 25.0MB Cache) and 32GB of RAM running Amazon Linux. We ran a single experiment multiple times to verify that performance was reliable, and compared the experiments results with a 4-core Intel Core i7-4790 clocked at 3.6 GHz with 32GB RAM. While the latter was a little faster, the relative order of algorithms remained stable. For each parameter setting and dataset, the algorithm was given ve hours to build the index and answer the queries.\n\nTested Algorithms. Table 2 summarizes the algorithms that are used in the evaluation; see the references provided for details. The framework has support for more implementations and many of these were included in the experiments, but they turned out to be either non-competitive or too similar to other implementations. 1 The scripts that set up the framework automatically fetch the most current version found in each algorithm's repository.\n\nIn general, the implementations under evaluation can be separated into three main algorithmic principles: graph-based, tree-based, and hashing-based algorithms. Graph-based algorithms build a graph in which vertices are the points in the dataset and edges connect vertices that are true nearest neighbors of each other, forming the so-called k-NN graph. Given the query point, close neighbors are found by traversing the graph in a greedy fashion, subject to the actual implementation [13,29,28,30,19]. Tree-based algorithms use a collection of trees as their data structure. In these trees, each node splits the dataset into subsets that are then processed in the children of the node. If the dataset associated with a node is small enough, it is directly stored in the node which is then a leaf in the tree. For example, Annoy [7] and RPForest [27]  hyperplane to split the dataset. Given the query point, the collection of trees are traversed to obtain a set of candidate points from which the closest to the query are returned. Hashing-based algorithms apply hash functions such as locality-sensitive hashing [18] to map data points to hash values. At query time, the query point is hashed and keys colliding with it, or not too far from it using the multi-probe approach [14], are retrieved. Among them, those closest to the query point are returned. Di erent implementations are mainly characterized by the underlying locality-sensitive hash function that is being used.\n\nDatasets. The datasets used in this evaluation are summarized in Table 3. More informations on these datasets and results for other datasets are found on the framework's website. The NYTimes dataset was generated by building tf-idf descriptors from the bag-of-words version, and embedding them into a lower dimensional space using the Johnson-Lindenstrauss Transform [21]. The Hamming space version of SIFT was generated by applying Spherical Hashing [16] using the implementation provided by the authors of [16]. The dataset Word2Bits comes from the quantized word vector approach described in [24] using the top-400 000 words in the English Wikipedia from 2017.\n\nThe dataset Rand-Euclidean is generated as follows: Assume that we want to generate a dataset with n data points, n query points, and are interested in nding the k nearest neighbors for each query point. For an even dimension d, we generate n \u2212 k \u00b7 n data points of the form (v, 0), where v is a random unit length vector of dimension d/2, and 0 is the vector containing d/2 0 entries. We call the rst d/2 components the rst part and the following d/2 components the second part of the vector. From these points, we randomly pick n points (v 1 , . . . , v n ). For each point v i , we replace its second part with a random vector of length 1/ \u221a 2. The resulting point is the query point q i . For each q i , we insert k random points at varying distance increasing from 0.1 to 0.5 to q i into the original dataset. The idea behind such a dataset is that the vast majority of the dataset looks like a random dataset with little structure for the algorithm to exploit, while each query point has k neighbors that are with high probability well separated from the rest of the data points. This means that the queries are easy to answer locally, but they should be di cult to answer if the algorithm wants to exploit a global structure.\n\nParameters of Algorithms. Most algorithms do not allow the user to explicitly specify a quality target-in fact, only three implementations from Table 2 provide \"recall\" as an input parameter. We used our framework to test many parameter settings at once. The detailed settings tested for each algorithm can be found on the framework's website.\n\nStatus of FALCONN. While preparing this full version, we noticed that FALCONN's performance has drastically decreased in the latest versions. We communicated this to the authors of [5], who are now working on a x; however, they asked us to disregard FALCONN for this submission. We plan to include it in a revised version.\n\n\nObjectives of the Experiments\n\nWe used the benchmarking framework to nd answers to the following questions: (Q1) Performance. Given a dataset, a quality measure and a number k of nearest neighbors to return, how do algorithms compare to each other with respect to di erent performance measures, such as query time or index size? (Q2) Robustness. Given an algorithm A, how is its performance and result quality in uenced by the dataset and the number of returned neighbors? (Q3) Approximation. Given a dataset, a number k of nearest neighbors to return, and an algorithm A, how does its performance improve when the returned neighbors can be an approximation? Is the e ect comparable for di erent algorithms? (Q4) Embeddings. Equipped with a framework with many di erent datasets and distance metrics, we can try interesting combinations. How do algorithms targeting Euclidean space or Cosine similarity perform in, say, Hamming space? How does replacing the internals of an algorithm with Hamming space related techniques improve its performance?\n\nThe following discussion is based on a combination of the plots found on the framework's website; see the website for more complete and up-to-date results.\n\n\nDiscussion\n\n(Q1) Performance. Figure 4 shows the relationship between an algorithm's achieved recall and the number of queries it can answer per second (its QPS) on the two datasets GLOVE (Cosine similarity) and SIFT (Euclidean distance) for 10and 100-nearest neighbor queries.\n\nFor GLOVE, we observe that the graph-based algorithms clearly outperform the tree-based approaches. It is noteworthy that all implementations, except FLANN, achieve close to perfect recall. Over all recall values, HNSW is fastest. However, at high recall values it is closely matched by KGraph. FAISS-IVF comes in at third place, only losing to the other graph-based approaches at very high recall values. For 100 nearest neighbors, the picture is very similar. We note, however, that the graph-based indexes were not able to build indexes for nearly perfect recall values within 5 hours.\n\nOn SIFT, all tested algorithms can achieve close to perfect recall. Again, the graph-based algorithms are fastest; they are followed by Annoy and FAISS-IVF. FLANN and BallTree are at the end. In particular, FLANN was not able to nish its auto-tuning for high recall values within 5 hours.\n\nVery few of these algorithms can tune themselves to produce a particular recall value. In particular, almost all of the fastest algorithms on the GLOVE dataset expose many parameters, leaving the user to nd the combination that works best. The KGraph algorithm, on the other hand, uses only a single parameter, which-even in its \"smallest\" choice-still gives high recall on GLOVE and SIFT. FLANN manages to tune itself for a particular recall value well. However, at high recall values, the tuning does not complete within the time limit, especially with 100-NN. Figure 5 relates an algorithm's performance to its index size. (Note that here down and to the right is better.) High recall can be achieved with small indexes by probing many points; however, this probing is expensive, and so the QPS drops dramatically. To re ect this performance cost, we scale the size of the index by the QPS it achieves for a particular run. This reveals that, on SIFT, most implementations perform similarly under this metric. HNSW is best (due to the QPS it achieves), but most of the other algorithm achieve similar cost. In particular, FAISS-IVF and FLANN do well. NND, Annoy, and BallTree achieve their QPS at the cost of relatively large indexes, re ected in a rather large gap between them and their competition. On GLOVE, we see a much wider spread of index size performance. Here, FAISS-IVF and HNSW perform nearly indistinguishably. Next follow the other graph-based algorithms, with FLANN among them. Again, Annoy and BallTree perform worst in this measure.\n\n(Q2) Robustness. Figure 6 plots recall against QPS on the dataset Rand-Euclidean. Recall from our earlier discussion of datasets that this dataset contains easy queries, but requires an algorithm to exploit the local structure instead of some global structure of the data structure, cf. Datasets. We see very di erent behavior than before: there is a large di erence between di erent graph-based approaches. While PANNG, KGraph, NND can solve the task easily with high QPS, both HNSW and SWG fail in this task. This means that the \"small-world\" structure of these two methods hurts performance on such a dataset. In particular, no tested parameter setting for HNSW achives recall beyond .86. Annoy performs best at exploiting the local structure of the dataset and is the fastest algorithm. The dataset is also easy for FAISS-IVF, which also has very good performance.\n\nLet us turn our focus to how the algorithms perform on a wide variety of datasets. Figure 7 plots recall against QPS for Annoy, FAISS-IVF, and HNSW over a range of datasets. Interestingly, implementations agree on the \"di culty\" of a dataset most of the time, i.e., the relative order of performance is the same among the algorithms. Notable exceptions are Rand-Euclidean, which is very easy for Annoy and FAISS-IVF, but di cult for HNSW (see above), and NYTimes, where FAISS-IVF fails to achieve recall above .7 for the tested parameter settings. Although all algorithms take a performance hit for high recall values, HNSW is least a ected. On the other hand, HNSW shows the biggest slowdown in answering 100-NN compared to 10-NN queries among the di erent algorithms.\n\n(Q3) Approximation. Figure 8 relates achieved QPS to the (approximate) recall of an algorithm. The plots show results on the GIST dataset with 100-NN for recall with no approximation and approximation factors of 1.01 and 1.1, respectively. Despite its high dimensionality, all considered algorithms achieve close to perfect recall (left). For an approximation factor of 1.01, i.e., distances to true nearest neighbors are allowed to di er by 1%, all curves move to the right, as expected. Also, the relative di erence between the performance of algorithms does not change. However, we see a clear di erence between the candidate sets that are returned by algorithms at low recall. For example, the data point for MRPT around .5 recall on the left achieves roughly .6 recall as a 1.01 approximation, which means that roughly 10 new candidates are considered true approximate nearest neighbors. On the other hand, HSNW, FAISS-IVF, and Annoy improve by around 25 candidates being counted as approximate nearest neighbors. We see that allowing a slack of 10% in the distance renders the queries too simple: almost all algorithms achieve near-perfect recall for all of their parameter choices. Interestingly, Annoy becomes the second-fastest algorithm for 1.1 approximation. This means that its candidates at very low recall values were a bit better than the ones obtained by its competitors.\n\n(Q4) Embeddings. Figure 9 shows a comparison between selected algorithms on the binary version of SIFT and a version of the Wikipedia dataset generated by Word2Bits, which is an embedding of word2vec vectors [31] into binary vectors. The performance plot for Annoy in the original Euclidean-space version of SIFT is also shown.\n\nOn SIFT, algorithms perform much faster in the embedded Hamming space version compared to the original Euclidean-space version (see Figure 4), which indicates that the queries are easier to answer in the embedded space. (Note here that the dimensionality is actually twice as large.) Multi-index hashing [33], an exact algorithm for Hamming space, shows good performance on SIFT with around 460 QPS.\n\nWe created a Hamming space-aware version of Annoy, using popcount for distance computations, and sampling single bits (as in Bitsampling LSH [18]) instead of choosing hyperplanes. This version is two to three times faster on SIFT until high recall, where the Hamming space version and the Euclidean space version converge in running time. On the 800-dimensional Word2Bits dataset the opposite is true and the original version of Annoy is faster than the dedicated Hamming space approach. This means that the original data-dependent node splitting in Annoy adapts better to the query structure than the node splitting by data-independent Bitsampling for this dataset. The dataset seems to be hard in general: MIH achieves only around 20 QPS on Word2Bits. We remark that setting the parameters for MIH correctly is crucial; even though the recall will always be 1, di erent parameter settings can give wildly di erent QPS values.\n\nThe embedding into Hamming space does have some consistent bene ts that we do not show here. Hamming space-aware algorithms should always have smaller index sizes, for example, due to the compactness of bit vectors. Figure 10 compares di erent implementations with respect to the time it takes to build the index. We see a huge di erence in the index building time among implementations, ranging from FAISS-IVF (around 2 seconds to build the index) to HNSW (almost 5 hours). In general, building the nearest neighbor graph and building a tree data structure takes considerably longer than the inverted le approach taken by FAISS-IVF. Shorter build times make it much quicker to search for the best parameter choices for a dataset. Although all indexes achieve recall of at least 0.9, we did not normalize by the queries per second as in Figure 5. For example, HNSW also achieves its highest QPS with these indexes, but FAISS needs a larger index to achieve the performance from Figure 4 (which takes around 13 seconds to build). As an aside, building an HNSW index using the implementation provided in FAISS made it possible to build an index that achieved recall .9 in only 1 700 seconds. \n\n\nIndex build time remarks\n\n\nBatched eries\n\nWe turn our focus to batched queries. In this setting, each algorithm is given the whole set of query points at once and has to return closest neighbors for each point. This allows for several optimizations: in a GPU setting, for example, copying query points to, and results from, the GPU's memory is expensive, and being able to copy everything at once drastically reduces this overhead.\n\nThe following experiments have been carried out on an Intel Xeon CPU E5-1650 v3 @ 3.50GHz with 6 physical cores, 15MB L3 Cache, 64 GB RAM, and equipped with an NVIDIA Titan XP GPU. Figure 11 reports on our results with regard to algorithms in batch mode. FAISS' inverted le index on the GPU is by far the fastest index, answering around 655 000 queries per second for .7 recall, and 61 000 queries per second for recall .99. It is around 20 to 30 times faster than the respective data structure running on the CPU. Comparing HNSW's performance with batched queries against non-batched queries shows a speedup by a factor of roughly 3 at .5 recall, and a factor of nearly 5 at recall .99 in favor of batched queries. Attention should also be put on the fact that the brute force variant of FAISS on the GPU answers around 24 000 queries per second.\n\n\nSummary\n\nWhich method to choose? From the evaluation, we see that graph-based algorithms provide by far the best performance on most of the datasets. HNSW is often the fastest algorithm, but PANNG is more robust if there is no global structure in the dataset. The downside of graph-based approaches is the high preprocessing time needed to build their data structures. This could mean that they might not be the preferred choice if the dataset changes regularly. When it comes to small and quick-to-build index data structures, FAISS' inverted le index provides a suitable choice that still gives good performance in answering queries.\n\nHow well do these results generalize? In our experiments, we observed that, for the standard datasets under consideration, algorithms usually agree on (i) the order in how well they perform on datasets, i.e., if algorithm A answers queries on dataset X faster than on dataset Y, then so will algorithm B; and (ii) their relative order to each other, i.e., if algorithm A is faster than B on dataset X, this will most likely be the order for dataset Y. There exist exceptions from this rule, e.g., for the dataset Rand-Euclidean described above.\n\nHow robust are parameter choices? With very few exceptions (see Table 2), users often have to set many parameters themselves. Of course, our framework allows them to choose the best parameter choice by exploring the interactive plots that contain the parameter choices that achieve certain quality guarantees.\n\nIn general, the build parameters can be used to estimate the size of the index 2 , while the query parameters suggest the amount of e ort that is put into searching the index.\n\nWe will concentrate for a moment on Figure 12. This gure presents a scatter plot of selected algorithms for GLOVE on 10-NN, cf. the Pareto curve in Figure 4 (in the top left). Each algorithm has a very distinctive parameter space plot.\n\nFor HNSW, almost all data points lie on the Pareto curve. This means that the di erent build parameters blend seamlessly into each other. For Annoy, we see that data points are grouped into clusters of three points each, which represent exactly the three di erent index choices that are built by the algorithm. For low recall, there is a big performance penalty for choosing a too large index; at high recall, the di erent build parameters blend almost into each other. For SW-Graph, we see two groups of data points, representing two di erent index choices. We see that with the index choice to the left, only very low recall is achieved on the dataset. Extrapolating from the curve, choosing query parameters that would explore a large part of the index will probably lead to low QPS. No clear picture is visible for FAISS-IVF from the plot. This is chie y because we test many di erent build parameters -recall that the index building time is very low. Each build parameter has its very own curve with respect to the di erent query parameters.\n\nAs a rule of thumb, when aiming for high recall values, a larger index performs better than a smaller index and is more robust to the choice of query parameters. \n\n\nConclusion & Further Work\n\nWe introduced ANN-Benchmarks, an automated benchmarking system for approximate nearestneighbor algorithms. We described the system and used it to evaluate existing algorithms. Our evaluation showed that well-enginereed solutions for Euclidean and Cosine distance exist, and many techniques allow for fast nearest-neighbor search algorithms. At the moment, graph-based approaches such as HNSW or KGraph outperform the other approaches for very high recalls, except for on very few datasets. Index building for graph-based approaches takes a long time for datasets with di cult queries.\n\nIn future, we aim to add support for other metrics and quality measures, such as positional errors [39]. Preliminary support exists for set similarity under Jaccard distance, but algorithm implementations are missing. Additionally, similarity joins are an interesting variation of the problem worth benchmarking [10]. We remark that the data we store with each algorithm run allows for more analysis beyond looking at average query times. One could, for example, already look at the variance of running times between algorithms, which could yield insights when comparing di erent approaches. We also intend to simplify and further automate the process of re-running benchmarks when new versions of algorithm implementations appear.\n\nAs a general direction for future work, we remark that none of the most performant implementations are easy to use. From a user perspective, the internal parameters of the data structure would ideally be invisible; an algorithm should be able to tune itself for the dataset at hand, given just a handful of quality-related parameters (such as the desired recall or the index size).\n\nIn the future, we plan to include a benchmarking mode for investigating this. A new tuning step will be added to the framework, letting implementations examine a small part of the dataset and to tune themselves for some given quality parameters before training begins. Implementors of algorithms would be able to test their auto-tuning techniques easily with such a benchmarking mode.\n\nAnother general direction for future work is to get a better understanding which properties of a dataset make it easy or di cult for a speci c algorithm. As shown in the evaluation, for many of the real-world datasets we get a homogeneous picture of how well algorithms perform against each other. On the other hand, we have given an example for a random dataset where implementations behave very di erently. Which properties of a dataset make it simple or di cult for a speci c algorithmic approach? For example, for graph-based algorithms there has been very little research except [23] on the theoretical guarantees that they achieve.\n\nFinally, answering batched queries, in particular on the GPU, is an interesting area for future work. There exist both novel LSH-based implementations [37] of nearest-neighbor algorithms and ideas on how to parallelize queries beyond running each query individually [10]. In particular, for a batch of queries an algorithm should exploit that individual queries might be close to each other.\n\nFigure 1\n1An example of a fragment of an algorithm con guration le.\n\nFigure 4\n4Recall-QPS (1/s) tradeo -up and to the right is better. Top: GLOVE, bottom: SIFT; left: 10-NN, right: 100-NN.\n\nFigure 5\n5Recall-Index size (kB)/QPS (s) tradeo -down and to the right is better. Left: SIFT (k=100), right: GLOVE (k=10).\n\nFigure 6 Figure 7 Figure 8 (\n678Recall-QPS (1/s) tradeo -up and to the right is better; Rand-Euclidean with 10-NN. Recall-QPS (1/s) tradeo -up and to the right is better, 10-nearest neighbors unless otherwise stated, left: Annoy, middle: FAISS-IVF, right: HNSW. Approximate) Recall-QPS (1/s) tradeo -up and to the right is better, GIST dataset, 100-NN; left: \u03b5 = 0, middle: \u03b5 = 0.01, right: \u03b5 = 0.1.\n\nFigure 9\n9Recall-QPS (1/s) tradeo -up and to the right is better, 10-nearest neighbors, left: SIFT-Hamming, right: Word2bits. The following versions of Annoy are shown in the plot: A, standard Annoy that uses Euclidean distance as its distance metric; A (Ham.), Annoy with node splitting inspired by Bitsampling LSH and tuned to Hamming space; and A (Eucl.), the run of Annoy on SIFT from Figure 4 (bottom left).\n\nFFigure 10\n10Index build time in seconds for dataset GLOVE. The plot shows the minimum build time for an index that achieved recall of at least 0.9 for 10-NN.\n\nFigure 11\n11Recall-QPS (1/s) tradeo -up and to the right is better. Algorithms running batched queries on SIFT with 10-NN. The plot shows a comparison between FAISS' IVF index running on a CPU and a GPU, FAISS' brute force index on the GPU, and HNSW from NMSlib running in batched (B) and non-batched mode (NB).\n\nFigure 12\n12Scatter plot of Recall-QPS (1/s) tradeo -up and to the right is better on GLOVE with 10-NN.\n\n\n100, 100], [100, 200], and [100, 400] in turn, while the last will run its experiments with the query parameter groups [1000, 1000], [1000, 2000], [1000, 4000], [2000, 1000], [2000, 2000], and [2000, 4000].Save \nresults \n\nExperiments left? \n\nQuery \nparameters \nleft? \n\nSet query \nparameters \n\nset_query_ \narguments( \n*args) \n\n\n\n\nchoose in each node a random Datasets under consideration.Dataset \n\nData/Query Points Dimensionality Metric \n\nSIFT \n1 000 000 / 10 000 \n128 \nEuclidean \nGIST \n1 000 000 / 10 000 \n960 \nEuclidean \nGLOVE \n1 183 514 / 10 000 \n100 \nAngular/Cosine \nNYTimes \n234 791 / 10 000 \n256 \nEuclidean \nRand-Euclidean 1 000 000 / 10 000 \n128 \nAngular/Cosine \nSIFT-Hamming \n1 000 000 / 1 000 \n256 \nHamming \nWord2Bits \n399 000 / 1 000 \n800 \nHamming \nTable 3 \n\u00a9 Martin Aum\u00fcller, Erik Bernhardsson, Alexander Faithfull; licensed under Creative Commons License CC-BY Leibniz International Proceedings in Informatics Schloss Dagstuhl -Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\nFor example, the framework contains three di erent implementations of HNSW: the original one from NMSlib, a standalone variant inspired by that one, and an implementation in FAISS that is again inspired by the implementation in NMSlib. The rst two implementations perform almost indistinguishably, while the implementation provided in FAISS was a bit slower. For the sake of brevity, we also omit the two random projection forest-based methods RPForest and MRPT since they were always slower than Annoy.\u00a9 Martin Aum\u00fcller, Erik Bernhardsson, Alexander Faithfull; licensed under Creative Commons License CC-BY Leibniz International Proceedings in Informatics Schloss Dagstuhl -Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\nAs an example, the developers of FAISS provide a detailed description of the space usage of their indexes at https://github.com/facebookresearch/faiss/wiki/Faiss-indexes. \u00a9 Martin Aum\u00fcller, Erik Bernhardsson, Alexander Faithfull; licensed under Creative Commons License CC-BY Leibniz International Proceedings in Informatics Schloss Dagstuhl -Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\nAcknowledgements: We thank the anonymous reviewers for their careful comments that allowed us to improve the paper. The rst and third authors thank all members of the algorithm group at the IT University of Copenhagen for fruitful discussions. In particular, we thank Rasmus Pagh for the suggestion of the random dataset. This work was supported by a GPU donation from NVIDIA.\nMRPT -fast nearest neighbor search with random projection. PANNGMRPT -fast nearest neighbor search with random projection, https://github.com/ teemupitkanen/mrpt 2 NGT: PANNG, https://github.com/yahoojapan/NGT\n\nParameter-free locality sensitive hashing for spherical range reporting. T D Ahle, M Aum\u00fcller, R Pagh, SODA'17. Ahle, T.D., Aum\u00fcller, M., Pagh, R.: Parameter-free locality sensitive hashing for spherical range reporting. In: SODA'17. pp. 239-256\n\nProbabilistic polynomials and hamming nearest neighbors. J Alman, R Williams, FOCS'15. Alman, J., Williams, R.: Probabilistic polynomials and hamming nearest neighbors. In: FOCS'15. pp. 136-150\n\nPractical and optimal LSH for angular distance. A Andoni, P Indyk, T Laarhoven, I P Razenshteyn, L Schmidt, NIPS'15. Andoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I.P., Schmidt, L.: Practical and optimal LSH for angular distance. In: NIPS'15. pp. 1225-1233. https://falconn-lib.org/\n\nMultidimensional binary search trees used for associative searching. J L Bentley, Commun. ACM. 189Bentley, J.L.: Multidimensional binary search trees used for associative searching. Commun. ACM 18(9), 509-517 (1975)\n\n. E Bernhardsson, AnnoyBernhardsson, E.: Annoy, https://github.com/spotify/annoy\n\nEngineering e cient and e ective non-metric space library. L Boytsov, B Naidan, SISAP'13. Boytsov, L., Naidan, B.: Engineering e cient and e ective non-metric space library. In: SISAP'13. pp. 280-293\n\nO the beaten path: Let's replace term-based retrieval with k-nn search. L Boytsov, D Novak, Y Malkov, E Nyberg, CIKM'16. Boytsov, L., Novak, D., Malkov, Y., Nyberg, E.: O the beaten path: Let's replace term-based retrieval with k-nn search. In: CIKM'16. pp. 1099-1108\n\nScalable and robust set similarity join. T Christiani, R Pagh, J Sivertsen, ICDE'2018. Christiani, T., Pagh, R., Sivertsen, J.: Scalable and robust set similarity join. In: ICDE'2018 (2018)\n\nM-tree: An e cient access method for similarity search in metric spaces. P Ciaccia, M Patella, P Zezula, VLDB'97. Ciaccia, P., Patella, M., Zezula, P.: M-tree: An e cient access method for similarity search in metric spaces. In: VLDB'97. pp. 426-435 (1997)\n\nMLPACK: A scalable C++ machine learning library. R R Curtin, J R Cline, N P Slagle, W B March, P Ram, N A Mehta, A G Gray, Journal of Machine Learning Research. 14Curtin, R.R., Cline, J.R., Slagle, N.P., March, W.B., Ram, P., Mehta, N.A., Gray, A.G.: MLPACK: A scalable C++ machine learning library. Journal of Machine Learning Research 14, 801-805 (2013)\n\n. W Dong, KGraphDong, W.: KGraph, https://github.com/aaalgo/kgraph\n\nModeling LSH for performance tuning. W Dong, Z Wang, W Josephson, M Charikar, K Li, CIKM'08. ACMDong, W., Wang, Z., Josephson, W., Charikar, M., Li, K.: Modeling LSH for performance tuning. In: CIKM'08. pp. 669-678. ACM, http://lshkit.sourceforge.net/\n\nAn automatic benchmarking system. M Edel, A Soni, R R Curtin, NIPS 2014 Workshop on Software Engineering for Machine Learning. Edel, M., Soni, A., Curtin, R.R.: An automatic benchmarking system. In: NIPS 2014 Workshop on Software Engineering for Machine Learning (2014)\n\nSpherical hashing: Binary code embedding with hyperspheres. J P Heo, Y Lee, J He, S F Chang, S E Yoon, IEEE TPAMI. 3711Heo, J.P., Lee, Y., He, J., Chang, S.F., Yoon, S.E.: Spherical hashing: Binary code embedding with hyperspheres. IEEE TPAMI 37(11), 2304-2316 (2015)\n\nFast nearest neighbor search through sparse random projections and voting. V Hyv\u00f6nen, T Pitk\u00e4nen, S Tasoulis, E J\u00e4\u00e4saari, R Tuomainen, L Wang, J Corander, T Roos, 2016 IEEE International Conference on. IEEEBig Data (Big DataHyv\u00f6nen, V., Pitk\u00e4nen, T., Tasoulis, S., J\u00e4\u00e4saari, E., Tuomainen, R., Wang, L., Corander, J., Roos, T.: Fast nearest neighbor search through sparse random projections and voting. In: Big Data (Big Data), 2016 IEEE International Conference on. pp. 881-888. IEEE (2016)\n\nApproximate nearest neighbors: Towards removing the curse of dimensionality. P Indyk, R Motwani, STOC'98. Indyk, P., Motwani, R.: Approximate nearest neighbors: Towards removing the curse of dimen- sionality. In: STOC'98. pp. 604-613\n\nPruned bi-directed k-nearest neighbor graph for proximity search. M Iwasaki, 10.1007/978-3-319-46759-7_2SISAP 2016. Iwasaki, M.: Pruned bi-directed k-nearest neighbor graph for proximity search. In: SISAP 2016. pp. 20-33 (2016), https://doi.org/10.1007/978-3-319-46759-7_2\n\nBillion-scale similarity search with gpus. J Johnson, M Douze, H J\u00e9gou, CoRR abs/1702.08734Johnson, J., Douze, M., J\u00e9gou, H.: Billion-scale similarity search with gpus. CoRR abs/1702.08734 (2017)\n\nExtensions of lipschitz maps into banach spaces. W B Johnson, J Lindenstrauss, G Schechtman, Israel Journal of Mathematics. 542Johnson, W.B., Lindenstrauss, J., Schechtman, G.: Extensions of lipschitz maps into banach spaces. Israel Journal of Mathematics 54(2), 129-138 (1986)\n\nThe (black) art of runtime evaluation: Are we comparing algorithms or implementations?. H Kriegel, E Schubert, A Zimek, Knowl. Inf. Syst. 522Kriegel, H., Schubert, E., Zimek, A.: The (black) art of runtime evaluation: Are we comparing algorithms or implementations? Knowl. Inf. Syst. 52(2), 341-378 (2017)\n\nGraph-Based Time-Space Trade-O s for Approximate Near Neighbors. T Laarhoven, Leibniz International Proceedings in Informatics (LIPIcs). Speckmann, B., T\u00f3th, C.D.Germany9914. Schloss Dagstuhl-Leibniz-Zentrum fuer InformatikLaarhoven, T.: Graph-Based Time-Space Trade-O s for Approximate Near Neighbors. In: Speckmann, B., T\u00f3th, C.D. (eds.) 34th International Symposium on Computational Geometry (SoCG 2018). Leibniz International Proceedings in Informatics (LIPIcs), vol. 99, pp. 57:1-57:14. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany (2018), http://drops. dagstuhl.de/opus/volltexte/2018/8770\n\nWord2bits -quantized word vectors. M Lam, CoRR abs/1803.05651Lam, M.: Word2bits -quantized word vectors. CoRR abs/1803.05651 (2018), http://arxiv.org/ abs/1803.05651\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Ha Ner, Proceedings of the IEEE. 8611LeCun, Y., Bottou, L., Bengio, Y., Ha ner, P.: Gradient-based learning applied to document recog- nition. Proceedings of the IEEE 86(11), 2278-2324 (1998)\n\nApproximate nearest neighbor search on high dimensional data -experiments, analyses, and improvement (v1.0). W Li, Y Zhang, Y Sun, W Wang, W Zhang, X Lin, CoRR abs/1610.02455Li, W., Zhang, Y., Sun, Y., Wang, W., Zhang, W., Lin, X.: Approximate nearest neighbor search on high dimensional data -experiments, analyses, and improvement (v1.0). CoRR abs/1610.02455 (2016), http://arxiv.org/abs/1610.02455\n\n. Lyst Engineering: Rpforest. Lyst Engineering: Rpforest, https://github.com/lyst/rpforest\n\nE cient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. Y A Malkov, D A Yashunin, ArXiv e-printsMalkov, Y.A., Yashunin, D.A.: E cient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. ArXiv e-prints (Mar 2016)\n\nApproximate nearest neighbor algorithm based on navigable small world graphs. Y Malkov, A Ponomarenko, A Logvinov, V Krylov, Inf. Syst. 45Malkov, Y., Ponomarenko, A., Logvinov, A., Krylov, V.: Approximate nearest neighbor algorithm based on navigable small world graphs. Inf. Syst. 45, 61-68 (2014)\n\n. L Mcinnes, PyNNDescentMcInnes, L.: PyNNDescent, https://github.com/lmcinnes/pynndescent\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, NIPS'13. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of words and phrases and their compositionality. In: NIPS'13. pp. 3111-3119\n\nFast approximate nearest neighbors with automatic algorithm con guration. M Muja, D G Lowe, VISSAPP'09. INSTICC PressMuja, M., Lowe, D.G.: Fast approximate nearest neighbors with automatic algorithm con guration. In: VISSAPP'09. pp. 331-340. INSTICC Press\n\nFast search in hamming space with multi-index hashing. M Norouzi, A Punjani, D J Fleet, CVPR'12. IEEENorouzi, M., Punjani, A., Fleet, D.J.: Fast search in hamming space with multi-index hashing. In: CVPR'12. pp. 3108-3115. IEEE\n\nHybrid LSH: faster near neighbors reporting in high-dimensional space. N Pham, EDBT'17. Pham, N.: Hybrid LSH: faster near neighbors reporting in high-dimensional space. In: EDBT'17. pp. 454-457\n\nOpenml: A collaborative science platform. J N Van Rijn, B Bischl, L Torgo, B Gao, V Umaashankar, S Fischer, P Winter, B Wiswedel, M R Berthold, J Vanschoren, ECML PKDD. SpringerVan Rijn, J.N., Bischl, B., Torgo, L., Gao, B., Umaashankar, V., Fischer, S., Winter, P., Wiswedel, B., Berthold, M.R., Vanschoren, J.: Openml: A collaborative science platform. In: ECML PKDD. pp. 645-649. Springer (2013)\n\nHashing for similarity search: A survey. J Wang, H T Shen, J Song, J Ji, CoRR abs/1408.2927Wang, J., Shen, H.T., Song, J., Ji, J.: Hashing for similarity search: A survey. CoRR abs/1408.2927 (2014), http://arxiv.org/abs/1408.2927\n\nRandomized algorithms accelerated over CPU-GPU for ultra-high dimensional similarity search. Y Wang, A Shrivastava, J Wang, J Ryu, http:/doi.acm.org/10.1145/3183713.3196925SIGMOD. pp. Wang, Y., Shrivastava, A., Wang, J., Ryu, J.: Randomized algorithms accelerated over CPU-GPU for ultra-high dimensional similarity search. In: SIGMOD. pp. 889-903 (2018), http://doi.acm. org/10.1145/3183713.3196925\n\nA new algorithm for optimal 2-constraint satisfaction and its implications. R Williams, Theor. Comput. Sci. 3482-3Williams, R.: A new algorithm for optimal 2-constraint satisfaction and its implications. Theor. Comput. Sci. 348(2-3), 357-365 (2005)\n\nApproximate similarity retrieval with M-Trees. P Zezula, P Savino, G Amato, F Rabitti, VLDB J. 74Zezula, P., Savino, P., Amato, G., Rabitti, F.: Approximate similarity retrieval with M-Trees. VLDB J. 7(4), 275-293 (1998)\n", "annotations": {"author": "[{\"end\":138,\"start\":85},{\"end\":170,\"start\":139},{\"end\":228,\"start\":171},{\"end\":138,\"start\":85},{\"end\":170,\"start\":139},{\"end\":228,\"start\":171}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":92},{\"end\":156,\"start\":144},{\"end\":190,\"start\":181},{\"end\":100,\"start\":92},{\"end\":156,\"start\":144},{\"end\":190,\"start\":181}]", "author_first_name": "[{\"end\":91,\"start\":85},{\"end\":143,\"start\":139},{\"end\":180,\"start\":171},{\"end\":91,\"start\":85},{\"end\":143,\"start\":139},{\"end\":180,\"start\":171}]", "author_affiliation": "[{\"end\":137,\"start\":102},{\"end\":169,\"start\":158},{\"end\":227,\"start\":192},{\"end\":137,\"start\":102},{\"end\":169,\"start\":158},{\"end\":227,\"start\":192}]", "title": "[{\"end\":82,\"start\":1},{\"end\":310,\"start\":229},{\"end\":82,\"start\":1},{\"end\":310,\"start\":229}]", "venue": null, "abstract": "[{\"end\":1809,\"start\":511},{\"end\":1809,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2085,\"start\":2081},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2219,\"start\":2215},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2547,\"start\":2544},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2802,\"start\":2799},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3574,\"start\":3571},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3590,\"start\":3586},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3681,\"start\":3677},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3716,\"start\":3712},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3719,\"start\":3716},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4049,\"start\":4045},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4147,\"start\":4143},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4533,\"start\":4530},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4536,\"start\":4533},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4838,\"start\":4834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4840,\"start\":4838},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4843,\"start\":4840},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4863,\"start\":4860},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4866,\"start\":4863},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4891,\"start\":4888},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6189,\"start\":6185},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6324,\"start\":6321},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6340,\"start\":6337},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6463,\"start\":6460},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6465,\"start\":6463},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6467,\"start\":6465},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6470,\"start\":6467},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6683,\"start\":6679},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6888,\"start\":6884},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6891,\"start\":6888},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7474,\"start\":7470},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8580,\"start\":8576},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11892,\"start\":11888},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14219,\"start\":14215},{\"end\":19529,\"start\":19528},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26729,\"start\":26725},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26749,\"start\":26745},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26751,\"start\":26749},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26762,\"start\":26758},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26764,\"start\":26762},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26769,\"start\":26765},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26940,\"start\":26938},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27703,\"start\":27702},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28315,\"start\":28311},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28318,\"start\":28315},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28321,\"start\":28318},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28324,\"start\":28321},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28327,\"start\":28324},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28658,\"start\":28655},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28676,\"start\":28672},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28943,\"start\":28939},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29106,\"start\":29102},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29675,\"start\":29671},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29759,\"start\":29755},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29816,\"start\":29812},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29903,\"start\":29899},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31732,\"start\":31729},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39035,\"start\":39031},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39460,\"start\":39456},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39698,\"start\":39694},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46795,\"start\":46791},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47008,\"start\":47004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":48782,\"start\":48778},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":48988,\"start\":48984},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49103,\"start\":49099},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2085,\"start\":2081},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2219,\"start\":2215},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2547,\"start\":2544},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2802,\"start\":2799},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3574,\"start\":3571},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3590,\"start\":3586},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3681,\"start\":3677},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3716,\"start\":3712},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3719,\"start\":3716},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4049,\"start\":4045},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4147,\"start\":4143},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4533,\"start\":4530},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4536,\"start\":4533},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4838,\"start\":4834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4840,\"start\":4838},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4843,\"start\":4840},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4863,\"start\":4860},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4866,\"start\":4863},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4891,\"start\":4888},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6189,\"start\":6185},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6324,\"start\":6321},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6340,\"start\":6337},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6463,\"start\":6460},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6465,\"start\":6463},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6467,\"start\":6465},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6470,\"start\":6467},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6683,\"start\":6679},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6888,\"start\":6884},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6891,\"start\":6888},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7474,\"start\":7470},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8580,\"start\":8576},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11892,\"start\":11888},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14219,\"start\":14215},{\"end\":19529,\"start\":19528},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26729,\"start\":26725},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26749,\"start\":26745},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26751,\"start\":26749},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26762,\"start\":26758},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26764,\"start\":26762},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26769,\"start\":26765},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26940,\"start\":26938},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27703,\"start\":27702},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28315,\"start\":28311},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28318,\"start\":28315},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28321,\"start\":28318},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28324,\"start\":28321},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28327,\"start\":28324},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28658,\"start\":28655},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28676,\"start\":28672},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28943,\"start\":28939},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29106,\"start\":29102},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29675,\"start\":29671},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29759,\"start\":29755},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29816,\"start\":29812},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29903,\"start\":29899},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31732,\"start\":31729},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39035,\"start\":39031},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39460,\"start\":39456},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39698,\"start\":39694},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":46795,\"start\":46791},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47008,\"start\":47004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":48782,\"start\":48778},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":48988,\"start\":48984},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49103,\"start\":49099}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49293,\"start\":49225},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49414,\"start\":49294},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49538,\"start\":49415},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49939,\"start\":49539},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50353,\"start\":49940},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50513,\"start\":50354},{\"attributes\":{\"id\":\"fig_6\"},\"end\":50826,\"start\":50514},{\"attributes\":{\"id\":\"fig_7\"},\"end\":50931,\"start\":50827},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51260,\"start\":50932},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":51701,\"start\":51261},{\"attributes\":{\"id\":\"fig_0\"},\"end\":49293,\"start\":49225},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49414,\"start\":49294},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49538,\"start\":49415},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49939,\"start\":49539},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50353,\"start\":49940},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50513,\"start\":50354},{\"attributes\":{\"id\":\"fig_6\"},\"end\":50826,\"start\":50514},{\"attributes\":{\"id\":\"fig_7\"},\"end\":50931,\"start\":50827},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51260,\"start\":50932},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":51701,\"start\":51261}]", "paragraph": "[{\"end\":2548,\"start\":1825},{\"end\":3111,\"start\":2550},{\"end\":3441,\"start\":3113},{\"end\":4537,\"start\":3443},{\"end\":6474,\"start\":4539},{\"end\":7953,\"start\":6476},{\"end\":8581,\"start\":7955},{\"end\":9650,\"start\":8583},{\"end\":10756,\"start\":9692},{\"end\":11357,\"start\":10775},{\"end\":11908,\"start\":11525},{\"end\":12546,\"start\":11933},{\"end\":12891,\"start\":12564},{\"end\":13025,\"start\":12893},{\"end\":13393,\"start\":13055},{\"end\":13946,\"start\":13395},{\"end\":14440,\"start\":13948},{\"end\":14999,\"start\":14442},{\"end\":15315,\"start\":15001},{\"end\":15762,\"start\":15317},{\"end\":16573,\"start\":15764},{\"end\":16827,\"start\":16575},{\"end\":17226,\"start\":16857},{\"end\":17762,\"start\":17228},{\"end\":18306,\"start\":17795},{\"end\":18567,\"start\":18308},{\"end\":19165,\"start\":18569},{\"end\":19627,\"start\":19167},{\"end\":20873,\"start\":19651},{\"end\":21159,\"start\":20875},{\"end\":21585,\"start\":21161},{\"end\":21995,\"start\":21625},{\"end\":22467,\"start\":21997},{\"end\":23252,\"start\":22469},{\"end\":23534,\"start\":23254},{\"end\":24120,\"start\":23558},{\"end\":24431,\"start\":24122},{\"end\":24784,\"start\":24433},{\"end\":25394,\"start\":24797},{\"end\":25679,\"start\":25396},{\"end\":26184,\"start\":25694},{\"end\":26677,\"start\":26186},{\"end\":27380,\"start\":26691},{\"end\":27824,\"start\":27382},{\"end\":29302,\"start\":27826},{\"end\":29967,\"start\":29304},{\"end\":31201,\"start\":29969},{\"end\":31546,\"start\":31203},{\"end\":31870,\"start\":31548},{\"end\":32919,\"start\":31904},{\"end\":33076,\"start\":32921},{\"end\":33356,\"start\":33091},{\"end\":33946,\"start\":33358},{\"end\":34236,\"start\":33948},{\"end\":35791,\"start\":34238},{\"end\":36661,\"start\":35793},{\"end\":37432,\"start\":36663},{\"end\":38821,\"start\":37434},{\"end\":39150,\"start\":38823},{\"end\":39551,\"start\":39152},{\"end\":40480,\"start\":39553},{\"end\":41672,\"start\":40482},{\"end\":42106,\"start\":41717},{\"end\":42955,\"start\":42108},{\"end\":43593,\"start\":42967},{\"end\":44139,\"start\":43595},{\"end\":44450,\"start\":44141},{\"end\":44627,\"start\":44452},{\"end\":44864,\"start\":44629},{\"end\":45912,\"start\":44866},{\"end\":46076,\"start\":45914},{\"end\":46690,\"start\":46106},{\"end\":47423,\"start\":46692},{\"end\":47806,\"start\":47425},{\"end\":48192,\"start\":47808},{\"end\":48831,\"start\":48194},{\"end\":49224,\"start\":48833},{\"end\":2548,\"start\":1825},{\"end\":3111,\"start\":2550},{\"end\":3441,\"start\":3113},{\"end\":4537,\"start\":3443},{\"end\":6474,\"start\":4539},{\"end\":7953,\"start\":6476},{\"end\":8581,\"start\":7955},{\"end\":9650,\"start\":8583},{\"end\":10756,\"start\":9692},{\"end\":11357,\"start\":10775},{\"end\":11908,\"start\":11525},{\"end\":12546,\"start\":11933},{\"end\":12891,\"start\":12564},{\"end\":13025,\"start\":12893},{\"end\":13393,\"start\":13055},{\"end\":13946,\"start\":13395},{\"end\":14440,\"start\":13948},{\"end\":14999,\"start\":14442},{\"end\":15315,\"start\":15001},{\"end\":15762,\"start\":15317},{\"end\":16573,\"start\":15764},{\"end\":16827,\"start\":16575},{\"end\":17226,\"start\":16857},{\"end\":17762,\"start\":17228},{\"end\":18306,\"start\":17795},{\"end\":18567,\"start\":18308},{\"end\":19165,\"start\":18569},{\"end\":19627,\"start\":19167},{\"end\":20873,\"start\":19651},{\"end\":21159,\"start\":20875},{\"end\":21585,\"start\":21161},{\"end\":21995,\"start\":21625},{\"end\":22467,\"start\":21997},{\"end\":23252,\"start\":22469},{\"end\":23534,\"start\":23254},{\"end\":24120,\"start\":23558},{\"end\":24431,\"start\":24122},{\"end\":24784,\"start\":24433},{\"end\":25394,\"start\":24797},{\"end\":25679,\"start\":25396},{\"end\":26184,\"start\":25694},{\"end\":26677,\"start\":26186},{\"end\":27380,\"start\":26691},{\"end\":27824,\"start\":27382},{\"end\":29302,\"start\":27826},{\"end\":29967,\"start\":29304},{\"end\":31201,\"start\":29969},{\"end\":31546,\"start\":31203},{\"end\":31870,\"start\":31548},{\"end\":32919,\"start\":31904},{\"end\":33076,\"start\":32921},{\"end\":33356,\"start\":33091},{\"end\":33946,\"start\":33358},{\"end\":34236,\"start\":33948},{\"end\":35791,\"start\":34238},{\"end\":36661,\"start\":35793},{\"end\":37432,\"start\":36663},{\"end\":38821,\"start\":37434},{\"end\":39150,\"start\":38823},{\"end\":39551,\"start\":39152},{\"end\":40480,\"start\":39553},{\"end\":41672,\"start\":40482},{\"end\":42106,\"start\":41717},{\"end\":42955,\"start\":42108},{\"end\":43593,\"start\":42967},{\"end\":44139,\"start\":43595},{\"end\":44450,\"start\":44141},{\"end\":44627,\"start\":44452},{\"end\":44864,\"start\":44629},{\"end\":45912,\"start\":44866},{\"end\":46076,\"start\":45914},{\"end\":46690,\"start\":46106},{\"end\":47423,\"start\":46692},{\"end\":47806,\"start\":47425},{\"end\":48192,\"start\":47808},{\"end\":48831,\"start\":48194},{\"end\":49224,\"start\":48833}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11524,\"start\":11358},{\"attributes\":{\"id\":\"formula_0\"},\"end\":11524,\"start\":11358}]", "table_ref": "[{\"end\":12014,\"start\":12007},{\"end\":26791,\"start\":26784},{\"end\":27408,\"start\":27401},{\"end\":29376,\"start\":29369},{\"end\":31354,\"start\":31347},{\"end\":44212,\"start\":44205},{\"end\":12014,\"start\":12007},{\"end\":26791,\"start\":26784},{\"end\":27408,\"start\":27401},{\"end\":29376,\"start\":29369},{\"end\":31354,\"start\":31347},{\"end\":44212,\"start\":44205}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1823,\"start\":1811},{\"attributes\":{\"n\":\"2\"},\"end\":9690,\"start\":9653},{\"attributes\":{\"n\":\"2.1\"},\"end\":10773,\"start\":10759},{\"attributes\":{\"n\":\"2.2\"},\"end\":11931,\"start\":11911},{\"attributes\":{\"n\":\"3\"},\"end\":12562,\"start\":12549},{\"attributes\":{\"n\":\"3.1\"},\"end\":13053,\"start\":13028},{\"attributes\":{\"n\":\"3.2\"},\"end\":16855,\"start\":16830},{\"attributes\":{\"n\":\"3.3\"},\"end\":17793,\"start\":17765},{\"attributes\":{\"n\":\"3.4\"},\"end\":19649,\"start\":19630},{\"attributes\":{\"n\":\"3.5\"},\"end\":21623,\"start\":21588},{\"attributes\":{\"n\":\"3.6\"},\"end\":23556,\"start\":23537},{\"attributes\":{\"n\":\"3.7\"},\"end\":24795,\"start\":24787},{\"attributes\":{\"n\":\"4\"},\"end\":25692,\"start\":25682},{\"end\":26689,\"start\":26680},{\"attributes\":{\"n\":\"4.1\"},\"end\":31902,\"start\":31873},{\"attributes\":{\"n\":\"4.2\"},\"end\":33089,\"start\":33079},{\"attributes\":{\"n\":\"4.3\"},\"end\":41699,\"start\":41675},{\"attributes\":{\"n\":\"4.4\"},\"end\":41715,\"start\":41702},{\"attributes\":{\"n\":\"4.5\"},\"end\":42965,\"start\":42958},{\"attributes\":{\"n\":\"5\"},\"end\":46104,\"start\":46079},{\"end\":49234,\"start\":49226},{\"end\":49303,\"start\":49295},{\"end\":49424,\"start\":49416},{\"end\":49568,\"start\":49540},{\"end\":49949,\"start\":49941},{\"end\":50365,\"start\":50355},{\"end\":50524,\"start\":50515},{\"end\":50837,\"start\":50828},{\"attributes\":{\"n\":\"1\"},\"end\":1823,\"start\":1811},{\"attributes\":{\"n\":\"2\"},\"end\":9690,\"start\":9653},{\"attributes\":{\"n\":\"2.1\"},\"end\":10773,\"start\":10759},{\"attributes\":{\"n\":\"2.2\"},\"end\":11931,\"start\":11911},{\"attributes\":{\"n\":\"3\"},\"end\":12562,\"start\":12549},{\"attributes\":{\"n\":\"3.1\"},\"end\":13053,\"start\":13028},{\"attributes\":{\"n\":\"3.2\"},\"end\":16855,\"start\":16830},{\"attributes\":{\"n\":\"3.3\"},\"end\":17793,\"start\":17765},{\"attributes\":{\"n\":\"3.4\"},\"end\":19649,\"start\":19630},{\"attributes\":{\"n\":\"3.5\"},\"end\":21623,\"start\":21588},{\"attributes\":{\"n\":\"3.6\"},\"end\":23556,\"start\":23537},{\"attributes\":{\"n\":\"3.7\"},\"end\":24795,\"start\":24787},{\"attributes\":{\"n\":\"4\"},\"end\":25692,\"start\":25682},{\"end\":26689,\"start\":26680},{\"attributes\":{\"n\":\"4.1\"},\"end\":31902,\"start\":31873},{\"attributes\":{\"n\":\"4.2\"},\"end\":33089,\"start\":33079},{\"attributes\":{\"n\":\"4.3\"},\"end\":41699,\"start\":41675},{\"attributes\":{\"n\":\"4.4\"},\"end\":41715,\"start\":41702},{\"attributes\":{\"n\":\"4.5\"},\"end\":42965,\"start\":42958},{\"attributes\":{\"n\":\"5\"},\"end\":46104,\"start\":46079},{\"end\":49234,\"start\":49226},{\"end\":49303,\"start\":49295},{\"end\":49424,\"start\":49416},{\"end\":49568,\"start\":49540},{\"end\":49949,\"start\":49941},{\"end\":50365,\"start\":50355},{\"end\":50524,\"start\":50515},{\"end\":50837,\"start\":50828}]", "table": "[{\"end\":51260,\"start\":51140},{\"end\":51701,\"start\":51321},{\"end\":51260,\"start\":51140},{\"end\":51701,\"start\":51321}]", "figure_caption": "[{\"end\":49293,\"start\":49236},{\"end\":49414,\"start\":49305},{\"end\":49538,\"start\":49426},{\"end\":49939,\"start\":49572},{\"end\":50353,\"start\":49951},{\"end\":50513,\"start\":50368},{\"end\":50826,\"start\":50527},{\"end\":50931,\"start\":50840},{\"end\":51140,\"start\":50934},{\"end\":51321,\"start\":51263},{\"end\":49293,\"start\":49236},{\"end\":49414,\"start\":49305},{\"end\":49538,\"start\":49426},{\"end\":49939,\"start\":49572},{\"end\":50353,\"start\":49951},{\"end\":50513,\"start\":50368},{\"end\":50826,\"start\":50527},{\"end\":50931,\"start\":50840},{\"end\":51140,\"start\":50934},{\"end\":51321,\"start\":51263}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18264,\"start\":18256},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18428,\"start\":18420},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19212,\"start\":19204},{\"end\":19538,\"start\":19530},{\"end\":19759,\"start\":19751},{\"end\":22767,\"start\":22759},{\"end\":25116,\"start\":25108},{\"end\":26379,\"start\":26371},{\"end\":30250,\"start\":30244},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33117,\"start\":33109},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34809,\"start\":34801},{\"end\":35818,\"start\":35810},{\"end\":36754,\"start\":36746},{\"end\":37462,\"start\":37454},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38848,\"start\":38840},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39292,\"start\":39284},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40707,\"start\":40698},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41327,\"start\":41319},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41468,\"start\":41460},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42298,\"start\":42289},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44674,\"start\":44665},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44785,\"start\":44777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18264,\"start\":18256},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18428,\"start\":18420},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19212,\"start\":19204},{\"end\":19538,\"start\":19530},{\"end\":19759,\"start\":19751},{\"end\":22767,\"start\":22759},{\"end\":25116,\"start\":25108},{\"end\":26379,\"start\":26371},{\"end\":30250,\"start\":30244},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33117,\"start\":33109},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34809,\"start\":34801},{\"end\":35818,\"start\":35810},{\"end\":36754,\"start\":36746},{\"end\":37462,\"start\":37454},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38848,\"start\":38840},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39292,\"start\":39284},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40707,\"start\":40698},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41327,\"start\":41319},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41468,\"start\":41460},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42298,\"start\":42289},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44674,\"start\":44665},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44785,\"start\":44777}]", "bib_author_first_name": "[{\"end\":53737,\"start\":53736},{\"end\":53739,\"start\":53738},{\"end\":53747,\"start\":53746},{\"end\":53759,\"start\":53758},{\"end\":53968,\"start\":53967},{\"end\":53977,\"start\":53976},{\"end\":54154,\"start\":54153},{\"end\":54164,\"start\":54163},{\"end\":54173,\"start\":54172},{\"end\":54186,\"start\":54185},{\"end\":54188,\"start\":54187},{\"end\":54203,\"start\":54202},{\"end\":54464,\"start\":54463},{\"end\":54466,\"start\":54465},{\"end\":54614,\"start\":54613},{\"end\":54753,\"start\":54752},{\"end\":54764,\"start\":54763},{\"end\":54967,\"start\":54966},{\"end\":54978,\"start\":54977},{\"end\":54987,\"start\":54986},{\"end\":54997,\"start\":54996},{\"end\":55205,\"start\":55204},{\"end\":55219,\"start\":55218},{\"end\":55227,\"start\":55226},{\"end\":55428,\"start\":55427},{\"end\":55439,\"start\":55438},{\"end\":55450,\"start\":55449},{\"end\":55662,\"start\":55661},{\"end\":55664,\"start\":55663},{\"end\":55674,\"start\":55673},{\"end\":55676,\"start\":55675},{\"end\":55685,\"start\":55684},{\"end\":55687,\"start\":55686},{\"end\":55697,\"start\":55696},{\"end\":55699,\"start\":55698},{\"end\":55708,\"start\":55707},{\"end\":55715,\"start\":55714},{\"end\":55717,\"start\":55716},{\"end\":55726,\"start\":55725},{\"end\":55728,\"start\":55727},{\"end\":55972,\"start\":55971},{\"end\":56075,\"start\":56074},{\"end\":56083,\"start\":56082},{\"end\":56091,\"start\":56090},{\"end\":56104,\"start\":56103},{\"end\":56116,\"start\":56115},{\"end\":56325,\"start\":56324},{\"end\":56333,\"start\":56332},{\"end\":56341,\"start\":56340},{\"end\":56343,\"start\":56342},{\"end\":56622,\"start\":56621},{\"end\":56624,\"start\":56623},{\"end\":56631,\"start\":56630},{\"end\":56638,\"start\":56637},{\"end\":56644,\"start\":56643},{\"end\":56646,\"start\":56645},{\"end\":56655,\"start\":56654},{\"end\":56657,\"start\":56656},{\"end\":56906,\"start\":56905},{\"end\":56917,\"start\":56916},{\"end\":56929,\"start\":56928},{\"end\":56941,\"start\":56940},{\"end\":56953,\"start\":56952},{\"end\":56966,\"start\":56965},{\"end\":56974,\"start\":56973},{\"end\":56986,\"start\":56985},{\"end\":57401,\"start\":57400},{\"end\":57410,\"start\":57409},{\"end\":57625,\"start\":57624},{\"end\":57876,\"start\":57875},{\"end\":57887,\"start\":57886},{\"end\":57896,\"start\":57895},{\"end\":58079,\"start\":58078},{\"end\":58081,\"start\":58080},{\"end\":58092,\"start\":58091},{\"end\":58109,\"start\":58108},{\"end\":58397,\"start\":58396},{\"end\":58408,\"start\":58407},{\"end\":58420,\"start\":58419},{\"end\":58681,\"start\":58680},{\"end\":59271,\"start\":59270},{\"end\":59460,\"start\":59459},{\"end\":59469,\"start\":59468},{\"end\":59479,\"start\":59478},{\"end\":59489,\"start\":59488},{\"end\":59793,\"start\":59792},{\"end\":59799,\"start\":59798},{\"end\":59808,\"start\":59807},{\"end\":59815,\"start\":59814},{\"end\":59823,\"start\":59822},{\"end\":59832,\"start\":59831},{\"end\":60282,\"start\":60281},{\"end\":60284,\"start\":60283},{\"end\":60294,\"start\":60293},{\"end\":60296,\"start\":60295},{\"end\":60561,\"start\":60560},{\"end\":60571,\"start\":60570},{\"end\":60586,\"start\":60585},{\"end\":60598,\"start\":60597},{\"end\":60785,\"start\":60784},{\"end\":60951,\"start\":60950},{\"end\":60962,\"start\":60961},{\"end\":60975,\"start\":60974},{\"end\":60983,\"start\":60982},{\"end\":60985,\"start\":60984},{\"end\":60996,\"start\":60995},{\"end\":61255,\"start\":61254},{\"end\":61263,\"start\":61262},{\"end\":61265,\"start\":61264},{\"end\":61493,\"start\":61492},{\"end\":61504,\"start\":61503},{\"end\":61515,\"start\":61514},{\"end\":61517,\"start\":61516},{\"end\":61738,\"start\":61737},{\"end\":61904,\"start\":61903},{\"end\":61906,\"start\":61905},{\"end\":61918,\"start\":61917},{\"end\":61928,\"start\":61927},{\"end\":61937,\"start\":61936},{\"end\":61944,\"start\":61943},{\"end\":61959,\"start\":61958},{\"end\":61970,\"start\":61969},{\"end\":61980,\"start\":61979},{\"end\":61992,\"start\":61991},{\"end\":61994,\"start\":61993},{\"end\":62006,\"start\":62005},{\"end\":62303,\"start\":62302},{\"end\":62311,\"start\":62310},{\"end\":62313,\"start\":62312},{\"end\":62321,\"start\":62320},{\"end\":62329,\"start\":62328},{\"end\":62586,\"start\":62585},{\"end\":62594,\"start\":62593},{\"end\":62609,\"start\":62608},{\"end\":62617,\"start\":62616},{\"end\":62969,\"start\":62968},{\"end\":63190,\"start\":63189},{\"end\":63200,\"start\":63199},{\"end\":63210,\"start\":63209},{\"end\":63219,\"start\":63218},{\"end\":53737,\"start\":53736},{\"end\":53739,\"start\":53738},{\"end\":53747,\"start\":53746},{\"end\":53759,\"start\":53758},{\"end\":53968,\"start\":53967},{\"end\":53977,\"start\":53976},{\"end\":54154,\"start\":54153},{\"end\":54164,\"start\":54163},{\"end\":54173,\"start\":54172},{\"end\":54186,\"start\":54185},{\"end\":54188,\"start\":54187},{\"end\":54203,\"start\":54202},{\"end\":54464,\"start\":54463},{\"end\":54466,\"start\":54465},{\"end\":54614,\"start\":54613},{\"end\":54753,\"start\":54752},{\"end\":54764,\"start\":54763},{\"end\":54967,\"start\":54966},{\"end\":54978,\"start\":54977},{\"end\":54987,\"start\":54986},{\"end\":54997,\"start\":54996},{\"end\":55205,\"start\":55204},{\"end\":55219,\"start\":55218},{\"end\":55227,\"start\":55226},{\"end\":55428,\"start\":55427},{\"end\":55439,\"start\":55438},{\"end\":55450,\"start\":55449},{\"end\":55662,\"start\":55661},{\"end\":55664,\"start\":55663},{\"end\":55674,\"start\":55673},{\"end\":55676,\"start\":55675},{\"end\":55685,\"start\":55684},{\"end\":55687,\"start\":55686},{\"end\":55697,\"start\":55696},{\"end\":55699,\"start\":55698},{\"end\":55708,\"start\":55707},{\"end\":55715,\"start\":55714},{\"end\":55717,\"start\":55716},{\"end\":55726,\"start\":55725},{\"end\":55728,\"start\":55727},{\"end\":55972,\"start\":55971},{\"end\":56075,\"start\":56074},{\"end\":56083,\"start\":56082},{\"end\":56091,\"start\":56090},{\"end\":56104,\"start\":56103},{\"end\":56116,\"start\":56115},{\"end\":56325,\"start\":56324},{\"end\":56333,\"start\":56332},{\"end\":56341,\"start\":56340},{\"end\":56343,\"start\":56342},{\"end\":56622,\"start\":56621},{\"end\":56624,\"start\":56623},{\"end\":56631,\"start\":56630},{\"end\":56638,\"start\":56637},{\"end\":56644,\"start\":56643},{\"end\":56646,\"start\":56645},{\"end\":56655,\"start\":56654},{\"end\":56657,\"start\":56656},{\"end\":56906,\"start\":56905},{\"end\":56917,\"start\":56916},{\"end\":56929,\"start\":56928},{\"end\":56941,\"start\":56940},{\"end\":56953,\"start\":56952},{\"end\":56966,\"start\":56965},{\"end\":56974,\"start\":56973},{\"end\":56986,\"start\":56985},{\"end\":57401,\"start\":57400},{\"end\":57410,\"start\":57409},{\"end\":57625,\"start\":57624},{\"end\":57876,\"start\":57875},{\"end\":57887,\"start\":57886},{\"end\":57896,\"start\":57895},{\"end\":58079,\"start\":58078},{\"end\":58081,\"start\":58080},{\"end\":58092,\"start\":58091},{\"end\":58109,\"start\":58108},{\"end\":58397,\"start\":58396},{\"end\":58408,\"start\":58407},{\"end\":58420,\"start\":58419},{\"end\":58681,\"start\":58680},{\"end\":59271,\"start\":59270},{\"end\":59460,\"start\":59459},{\"end\":59469,\"start\":59468},{\"end\":59479,\"start\":59478},{\"end\":59489,\"start\":59488},{\"end\":59793,\"start\":59792},{\"end\":59799,\"start\":59798},{\"end\":59808,\"start\":59807},{\"end\":59815,\"start\":59814},{\"end\":59823,\"start\":59822},{\"end\":59832,\"start\":59831},{\"end\":60282,\"start\":60281},{\"end\":60284,\"start\":60283},{\"end\":60294,\"start\":60293},{\"end\":60296,\"start\":60295},{\"end\":60561,\"start\":60560},{\"end\":60571,\"start\":60570},{\"end\":60586,\"start\":60585},{\"end\":60598,\"start\":60597},{\"end\":60785,\"start\":60784},{\"end\":60951,\"start\":60950},{\"end\":60962,\"start\":60961},{\"end\":60975,\"start\":60974},{\"end\":60983,\"start\":60982},{\"end\":60985,\"start\":60984},{\"end\":60996,\"start\":60995},{\"end\":61255,\"start\":61254},{\"end\":61263,\"start\":61262},{\"end\":61265,\"start\":61264},{\"end\":61493,\"start\":61492},{\"end\":61504,\"start\":61503},{\"end\":61515,\"start\":61514},{\"end\":61517,\"start\":61516},{\"end\":61738,\"start\":61737},{\"end\":61904,\"start\":61903},{\"end\":61906,\"start\":61905},{\"end\":61918,\"start\":61917},{\"end\":61928,\"start\":61927},{\"end\":61937,\"start\":61936},{\"end\":61944,\"start\":61943},{\"end\":61959,\"start\":61958},{\"end\":61970,\"start\":61969},{\"end\":61980,\"start\":61979},{\"end\":61992,\"start\":61991},{\"end\":61994,\"start\":61993},{\"end\":62006,\"start\":62005},{\"end\":62303,\"start\":62302},{\"end\":62311,\"start\":62310},{\"end\":62313,\"start\":62312},{\"end\":62321,\"start\":62320},{\"end\":62329,\"start\":62328},{\"end\":62586,\"start\":62585},{\"end\":62594,\"start\":62593},{\"end\":62609,\"start\":62608},{\"end\":62617,\"start\":62616},{\"end\":62969,\"start\":62968},{\"end\":63190,\"start\":63189},{\"end\":63200,\"start\":63199},{\"end\":63210,\"start\":63209},{\"end\":63219,\"start\":63218}]", "bib_author_last_name": "[{\"end\":53744,\"start\":53740},{\"end\":53756,\"start\":53748},{\"end\":53764,\"start\":53760},{\"end\":53974,\"start\":53969},{\"end\":53986,\"start\":53978},{\"end\":54161,\"start\":54155},{\"end\":54170,\"start\":54165},{\"end\":54183,\"start\":54174},{\"end\":54200,\"start\":54189},{\"end\":54211,\"start\":54204},{\"end\":54474,\"start\":54467},{\"end\":54627,\"start\":54615},{\"end\":54761,\"start\":54754},{\"end\":54771,\"start\":54765},{\"end\":54975,\"start\":54968},{\"end\":54984,\"start\":54979},{\"end\":54994,\"start\":54988},{\"end\":55004,\"start\":54998},{\"end\":55216,\"start\":55206},{\"end\":55224,\"start\":55220},{\"end\":55237,\"start\":55228},{\"end\":55436,\"start\":55429},{\"end\":55447,\"start\":55440},{\"end\":55457,\"start\":55451},{\"end\":55671,\"start\":55665},{\"end\":55682,\"start\":55677},{\"end\":55694,\"start\":55688},{\"end\":55705,\"start\":55700},{\"end\":55712,\"start\":55709},{\"end\":55723,\"start\":55718},{\"end\":55733,\"start\":55729},{\"end\":55977,\"start\":55973},{\"end\":56080,\"start\":56076},{\"end\":56088,\"start\":56084},{\"end\":56101,\"start\":56092},{\"end\":56113,\"start\":56105},{\"end\":56119,\"start\":56117},{\"end\":56330,\"start\":56326},{\"end\":56338,\"start\":56334},{\"end\":56350,\"start\":56344},{\"end\":56628,\"start\":56625},{\"end\":56635,\"start\":56632},{\"end\":56641,\"start\":56639},{\"end\":56652,\"start\":56647},{\"end\":56662,\"start\":56658},{\"end\":56914,\"start\":56907},{\"end\":56926,\"start\":56918},{\"end\":56938,\"start\":56930},{\"end\":56950,\"start\":56942},{\"end\":56963,\"start\":56954},{\"end\":56971,\"start\":56967},{\"end\":56983,\"start\":56975},{\"end\":56991,\"start\":56987},{\"end\":57407,\"start\":57402},{\"end\":57418,\"start\":57411},{\"end\":57633,\"start\":57626},{\"end\":57884,\"start\":57877},{\"end\":57893,\"start\":57888},{\"end\":57902,\"start\":57897},{\"end\":58089,\"start\":58082},{\"end\":58106,\"start\":58093},{\"end\":58120,\"start\":58110},{\"end\":58405,\"start\":58398},{\"end\":58417,\"start\":58409},{\"end\":58426,\"start\":58421},{\"end\":58691,\"start\":58682},{\"end\":59275,\"start\":59272},{\"end\":59466,\"start\":59461},{\"end\":59476,\"start\":59470},{\"end\":59486,\"start\":59480},{\"end\":59496,\"start\":59490},{\"end\":59796,\"start\":59794},{\"end\":59805,\"start\":59800},{\"end\":59812,\"start\":59809},{\"end\":59820,\"start\":59816},{\"end\":59829,\"start\":59824},{\"end\":59836,\"start\":59833},{\"end\":60291,\"start\":60285},{\"end\":60305,\"start\":60297},{\"end\":60568,\"start\":60562},{\"end\":60583,\"start\":60572},{\"end\":60595,\"start\":60587},{\"end\":60605,\"start\":60599},{\"end\":60793,\"start\":60786},{\"end\":60959,\"start\":60952},{\"end\":60972,\"start\":60963},{\"end\":60980,\"start\":60976},{\"end\":60993,\"start\":60986},{\"end\":61001,\"start\":60997},{\"end\":61260,\"start\":61256},{\"end\":61270,\"start\":61266},{\"end\":61501,\"start\":61494},{\"end\":61512,\"start\":61505},{\"end\":61523,\"start\":61518},{\"end\":61743,\"start\":61739},{\"end\":61915,\"start\":61907},{\"end\":61925,\"start\":61919},{\"end\":61934,\"start\":61929},{\"end\":61941,\"start\":61938},{\"end\":61956,\"start\":61945},{\"end\":61967,\"start\":61960},{\"end\":61977,\"start\":61971},{\"end\":61989,\"start\":61981},{\"end\":62003,\"start\":61995},{\"end\":62017,\"start\":62007},{\"end\":62308,\"start\":62304},{\"end\":62318,\"start\":62314},{\"end\":62326,\"start\":62322},{\"end\":62332,\"start\":62330},{\"end\":62591,\"start\":62587},{\"end\":62606,\"start\":62595},{\"end\":62614,\"start\":62610},{\"end\":62621,\"start\":62618},{\"end\":62978,\"start\":62970},{\"end\":63197,\"start\":63191},{\"end\":63207,\"start\":63201},{\"end\":63216,\"start\":63211},{\"end\":63227,\"start\":63220},{\"end\":53744,\"start\":53740},{\"end\":53756,\"start\":53748},{\"end\":53764,\"start\":53760},{\"end\":53974,\"start\":53969},{\"end\":53986,\"start\":53978},{\"end\":54161,\"start\":54155},{\"end\":54170,\"start\":54165},{\"end\":54183,\"start\":54174},{\"end\":54200,\"start\":54189},{\"end\":54211,\"start\":54204},{\"end\":54474,\"start\":54467},{\"end\":54627,\"start\":54615},{\"end\":54761,\"start\":54754},{\"end\":54771,\"start\":54765},{\"end\":54975,\"start\":54968},{\"end\":54984,\"start\":54979},{\"end\":54994,\"start\":54988},{\"end\":55004,\"start\":54998},{\"end\":55216,\"start\":55206},{\"end\":55224,\"start\":55220},{\"end\":55237,\"start\":55228},{\"end\":55436,\"start\":55429},{\"end\":55447,\"start\":55440},{\"end\":55457,\"start\":55451},{\"end\":55671,\"start\":55665},{\"end\":55682,\"start\":55677},{\"end\":55694,\"start\":55688},{\"end\":55705,\"start\":55700},{\"end\":55712,\"start\":55709},{\"end\":55723,\"start\":55718},{\"end\":55733,\"start\":55729},{\"end\":55977,\"start\":55973},{\"end\":56080,\"start\":56076},{\"end\":56088,\"start\":56084},{\"end\":56101,\"start\":56092},{\"end\":56113,\"start\":56105},{\"end\":56119,\"start\":56117},{\"end\":56330,\"start\":56326},{\"end\":56338,\"start\":56334},{\"end\":56350,\"start\":56344},{\"end\":56628,\"start\":56625},{\"end\":56635,\"start\":56632},{\"end\":56641,\"start\":56639},{\"end\":56652,\"start\":56647},{\"end\":56662,\"start\":56658},{\"end\":56914,\"start\":56907},{\"end\":56926,\"start\":56918},{\"end\":56938,\"start\":56930},{\"end\":56950,\"start\":56942},{\"end\":56963,\"start\":56954},{\"end\":56971,\"start\":56967},{\"end\":56983,\"start\":56975},{\"end\":56991,\"start\":56987},{\"end\":57407,\"start\":57402},{\"end\":57418,\"start\":57411},{\"end\":57633,\"start\":57626},{\"end\":57884,\"start\":57877},{\"end\":57893,\"start\":57888},{\"end\":57902,\"start\":57897},{\"end\":58089,\"start\":58082},{\"end\":58106,\"start\":58093},{\"end\":58120,\"start\":58110},{\"end\":58405,\"start\":58398},{\"end\":58417,\"start\":58409},{\"end\":58426,\"start\":58421},{\"end\":58691,\"start\":58682},{\"end\":59275,\"start\":59272},{\"end\":59466,\"start\":59461},{\"end\":59476,\"start\":59470},{\"end\":59486,\"start\":59480},{\"end\":59496,\"start\":59490},{\"end\":59796,\"start\":59794},{\"end\":59805,\"start\":59800},{\"end\":59812,\"start\":59809},{\"end\":59820,\"start\":59816},{\"end\":59829,\"start\":59824},{\"end\":59836,\"start\":59833},{\"end\":60291,\"start\":60285},{\"end\":60305,\"start\":60297},{\"end\":60568,\"start\":60562},{\"end\":60583,\"start\":60572},{\"end\":60595,\"start\":60587},{\"end\":60605,\"start\":60599},{\"end\":60793,\"start\":60786},{\"end\":60959,\"start\":60952},{\"end\":60972,\"start\":60963},{\"end\":60980,\"start\":60976},{\"end\":60993,\"start\":60986},{\"end\":61001,\"start\":60997},{\"end\":61260,\"start\":61256},{\"end\":61270,\"start\":61266},{\"end\":61501,\"start\":61494},{\"end\":61512,\"start\":61505},{\"end\":61523,\"start\":61518},{\"end\":61743,\"start\":61739},{\"end\":61915,\"start\":61907},{\"end\":61925,\"start\":61919},{\"end\":61934,\"start\":61929},{\"end\":61941,\"start\":61938},{\"end\":61956,\"start\":61945},{\"end\":61967,\"start\":61960},{\"end\":61977,\"start\":61971},{\"end\":61989,\"start\":61981},{\"end\":62003,\"start\":61995},{\"end\":62017,\"start\":62007},{\"end\":62308,\"start\":62304},{\"end\":62318,\"start\":62314},{\"end\":62326,\"start\":62322},{\"end\":62332,\"start\":62330},{\"end\":62591,\"start\":62587},{\"end\":62606,\"start\":62595},{\"end\":62614,\"start\":62610},{\"end\":62621,\"start\":62618},{\"end\":62978,\"start\":62970},{\"end\":63197,\"start\":63191},{\"end\":63207,\"start\":63201},{\"end\":63216,\"start\":63211},{\"end\":63227,\"start\":63220}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":53661,\"start\":53452},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13097061},\"end\":53908,\"start\":53663},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9448795},\"end\":54103,\"start\":53910},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9222460},\"end\":54392,\"start\":54105},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13091446},\"end\":54609,\"start\":54394},{\"attributes\":{\"id\":\"b5\"},\"end\":54691,\"start\":54611},{\"attributes\":{\"id\":\"b6\"},\"end\":54892,\"start\":54693},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":12823757},\"end\":55161,\"start\":54894},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8616843},\"end\":55352,\"start\":55163},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15393774},\"end\":55610,\"start\":55354},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":501756},\"end\":55967,\"start\":55612},{\"attributes\":{\"id\":\"b11\"},\"end\":56035,\"start\":55969},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5504273},\"end\":56288,\"start\":56037},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":113404208},\"end\":56559,\"start\":56290},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15847543},\"end\":56828,\"start\":56561},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206526064},\"end\":57321,\"start\":56830},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6110572},\"end\":57556,\"start\":57323},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46759-7_2\",\"id\":\"b17\",\"matched_paper_id\":27336502},\"end\":57830,\"start\":57558},{\"attributes\":{\"doi\":\"CoRR abs/1702.08734\",\"id\":\"b18\"},\"end\":58027,\"start\":57832},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":123154958},\"end\":58306,\"start\":58029},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":40772241},\"end\":58613,\"start\":58308},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3529949},\"end\":59233,\"start\":58615},{\"attributes\":{\"doi\":\"CoRR abs/1803.05651\",\"id\":\"b22\"},\"end\":59400,\"start\":59235},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14542261},\"end\":59681,\"start\":59402},{\"attributes\":{\"doi\":\"CoRR abs/1610.02455\",\"id\":\"b24\"},\"end\":60083,\"start\":59683},{\"attributes\":{\"id\":\"b25\"},\"end\":60175,\"start\":60085},{\"attributes\":{\"id\":\"b26\"},\"end\":60480,\"start\":60177},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9896397},\"end\":60780,\"start\":60482},{\"attributes\":{\"id\":\"b28\"},\"end\":60871,\"start\":60782},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16447573},\"end\":61178,\"start\":60873},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7317448},\"end\":61435,\"start\":61180},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6324963},\"end\":61664,\"start\":61437},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":85454210},\"end\":61859,\"start\":61666},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10889654},\"end\":62259,\"start\":61861},{\"attributes\":{\"doi\":\"CoRR abs/1408.2927\",\"id\":\"b34\"},\"end\":62490,\"start\":62261},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/3183713.3196925\",\"id\":\"b35\",\"matched_paper_id\":40850992},\"end\":62890,\"start\":62492},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7179148},\"end\":63140,\"start\":62892},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":28112514},\"end\":63362,\"start\":63142},{\"attributes\":{\"id\":\"b0\"},\"end\":53661,\"start\":53452},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13097061},\"end\":53908,\"start\":53663},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9448795},\"end\":54103,\"start\":53910},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9222460},\"end\":54392,\"start\":54105},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13091446},\"end\":54609,\"start\":54394},{\"attributes\":{\"id\":\"b5\"},\"end\":54691,\"start\":54611},{\"attributes\":{\"id\":\"b6\"},\"end\":54892,\"start\":54693},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":12823757},\"end\":55161,\"start\":54894},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8616843},\"end\":55352,\"start\":55163},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15393774},\"end\":55610,\"start\":55354},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":501756},\"end\":55967,\"start\":55612},{\"attributes\":{\"id\":\"b11\"},\"end\":56035,\"start\":55969},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5504273},\"end\":56288,\"start\":56037},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":113404208},\"end\":56559,\"start\":56290},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15847543},\"end\":56828,\"start\":56561},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206526064},\"end\":57321,\"start\":56830},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6110572},\"end\":57556,\"start\":57323},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46759-7_2\",\"id\":\"b17\",\"matched_paper_id\":27336502},\"end\":57830,\"start\":57558},{\"attributes\":{\"doi\":\"CoRR abs/1702.08734\",\"id\":\"b18\"},\"end\":58027,\"start\":57832},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":123154958},\"end\":58306,\"start\":58029},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":40772241},\"end\":58613,\"start\":58308},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3529949},\"end\":59233,\"start\":58615},{\"attributes\":{\"doi\":\"CoRR abs/1803.05651\",\"id\":\"b22\"},\"end\":59400,\"start\":59235},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14542261},\"end\":59681,\"start\":59402},{\"attributes\":{\"doi\":\"CoRR abs/1610.02455\",\"id\":\"b24\"},\"end\":60083,\"start\":59683},{\"attributes\":{\"id\":\"b25\"},\"end\":60175,\"start\":60085},{\"attributes\":{\"id\":\"b26\"},\"end\":60480,\"start\":60177},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9896397},\"end\":60780,\"start\":60482},{\"attributes\":{\"id\":\"b28\"},\"end\":60871,\"start\":60782},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16447573},\"end\":61178,\"start\":60873},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7317448},\"end\":61435,\"start\":61180},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6324963},\"end\":61664,\"start\":61437},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":85454210},\"end\":61859,\"start\":61666},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10889654},\"end\":62259,\"start\":61861},{\"attributes\":{\"doi\":\"CoRR abs/1408.2927\",\"id\":\"b34\"},\"end\":62490,\"start\":62261},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/3183713.3196925\",\"id\":\"b35\",\"matched_paper_id\":40850992},\"end\":62890,\"start\":62492},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7179148},\"end\":63140,\"start\":62892},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":28112514},\"end\":63362,\"start\":63142}]", "bib_title": "[{\"end\":53734,\"start\":53663},{\"end\":53965,\"start\":53910},{\"end\":54151,\"start\":54105},{\"end\":54461,\"start\":54394},{\"end\":54750,\"start\":54693},{\"end\":54964,\"start\":54894},{\"end\":55202,\"start\":55163},{\"end\":55425,\"start\":55354},{\"end\":55659,\"start\":55612},{\"end\":56072,\"start\":56037},{\"end\":56322,\"start\":56290},{\"end\":56619,\"start\":56561},{\"end\":56903,\"start\":56830},{\"end\":57398,\"start\":57323},{\"end\":57622,\"start\":57558},{\"end\":58076,\"start\":58029},{\"end\":58394,\"start\":58308},{\"end\":58678,\"start\":58615},{\"end\":59457,\"start\":59402},{\"end\":60558,\"start\":60482},{\"end\":60948,\"start\":60873},{\"end\":61252,\"start\":61180},{\"end\":61490,\"start\":61437},{\"end\":61735,\"start\":61666},{\"end\":61901,\"start\":61861},{\"end\":62583,\"start\":62492},{\"end\":62966,\"start\":62892},{\"end\":63187,\"start\":63142},{\"end\":53734,\"start\":53663},{\"end\":53965,\"start\":53910},{\"end\":54151,\"start\":54105},{\"end\":54461,\"start\":54394},{\"end\":54750,\"start\":54693},{\"end\":54964,\"start\":54894},{\"end\":55202,\"start\":55163},{\"end\":55425,\"start\":55354},{\"end\":55659,\"start\":55612},{\"end\":56072,\"start\":56037},{\"end\":56322,\"start\":56290},{\"end\":56619,\"start\":56561},{\"end\":56903,\"start\":56830},{\"end\":57398,\"start\":57323},{\"end\":57622,\"start\":57558},{\"end\":58076,\"start\":58029},{\"end\":58394,\"start\":58308},{\"end\":58678,\"start\":58615},{\"end\":59457,\"start\":59402},{\"end\":60558,\"start\":60482},{\"end\":60948,\"start\":60873},{\"end\":61252,\"start\":61180},{\"end\":61490,\"start\":61437},{\"end\":61735,\"start\":61666},{\"end\":61901,\"start\":61861},{\"end\":62583,\"start\":62492},{\"end\":62966,\"start\":62892},{\"end\":63187,\"start\":63142}]", "bib_author": "[{\"end\":53746,\"start\":53736},{\"end\":53758,\"start\":53746},{\"end\":53766,\"start\":53758},{\"end\":53976,\"start\":53967},{\"end\":53988,\"start\":53976},{\"end\":54163,\"start\":54153},{\"end\":54172,\"start\":54163},{\"end\":54185,\"start\":54172},{\"end\":54202,\"start\":54185},{\"end\":54213,\"start\":54202},{\"end\":54476,\"start\":54463},{\"end\":54629,\"start\":54613},{\"end\":54763,\"start\":54752},{\"end\":54773,\"start\":54763},{\"end\":54977,\"start\":54966},{\"end\":54986,\"start\":54977},{\"end\":54996,\"start\":54986},{\"end\":55006,\"start\":54996},{\"end\":55218,\"start\":55204},{\"end\":55226,\"start\":55218},{\"end\":55239,\"start\":55226},{\"end\":55438,\"start\":55427},{\"end\":55449,\"start\":55438},{\"end\":55459,\"start\":55449},{\"end\":55673,\"start\":55661},{\"end\":55684,\"start\":55673},{\"end\":55696,\"start\":55684},{\"end\":55707,\"start\":55696},{\"end\":55714,\"start\":55707},{\"end\":55725,\"start\":55714},{\"end\":55735,\"start\":55725},{\"end\":55979,\"start\":55971},{\"end\":56082,\"start\":56074},{\"end\":56090,\"start\":56082},{\"end\":56103,\"start\":56090},{\"end\":56115,\"start\":56103},{\"end\":56121,\"start\":56115},{\"end\":56332,\"start\":56324},{\"end\":56340,\"start\":56332},{\"end\":56352,\"start\":56340},{\"end\":56630,\"start\":56621},{\"end\":56637,\"start\":56630},{\"end\":56643,\"start\":56637},{\"end\":56654,\"start\":56643},{\"end\":56664,\"start\":56654},{\"end\":56916,\"start\":56905},{\"end\":56928,\"start\":56916},{\"end\":56940,\"start\":56928},{\"end\":56952,\"start\":56940},{\"end\":56965,\"start\":56952},{\"end\":56973,\"start\":56965},{\"end\":56985,\"start\":56973},{\"end\":56993,\"start\":56985},{\"end\":57409,\"start\":57400},{\"end\":57420,\"start\":57409},{\"end\":57635,\"start\":57624},{\"end\":57886,\"start\":57875},{\"end\":57895,\"start\":57886},{\"end\":57904,\"start\":57895},{\"end\":58091,\"start\":58078},{\"end\":58108,\"start\":58091},{\"end\":58122,\"start\":58108},{\"end\":58407,\"start\":58396},{\"end\":58419,\"start\":58407},{\"end\":58428,\"start\":58419},{\"end\":58693,\"start\":58680},{\"end\":59277,\"start\":59270},{\"end\":59468,\"start\":59459},{\"end\":59478,\"start\":59468},{\"end\":59488,\"start\":59478},{\"end\":59498,\"start\":59488},{\"end\":59798,\"start\":59792},{\"end\":59807,\"start\":59798},{\"end\":59814,\"start\":59807},{\"end\":59822,\"start\":59814},{\"end\":59831,\"start\":59822},{\"end\":59838,\"start\":59831},{\"end\":60293,\"start\":60281},{\"end\":60307,\"start\":60293},{\"end\":60570,\"start\":60560},{\"end\":60585,\"start\":60570},{\"end\":60597,\"start\":60585},{\"end\":60607,\"start\":60597},{\"end\":60795,\"start\":60784},{\"end\":60961,\"start\":60950},{\"end\":60974,\"start\":60961},{\"end\":60982,\"start\":60974},{\"end\":60995,\"start\":60982},{\"end\":61003,\"start\":60995},{\"end\":61262,\"start\":61254},{\"end\":61272,\"start\":61262},{\"end\":61503,\"start\":61492},{\"end\":61514,\"start\":61503},{\"end\":61525,\"start\":61514},{\"end\":61745,\"start\":61737},{\"end\":61917,\"start\":61903},{\"end\":61927,\"start\":61917},{\"end\":61936,\"start\":61927},{\"end\":61943,\"start\":61936},{\"end\":61958,\"start\":61943},{\"end\":61969,\"start\":61958},{\"end\":61979,\"start\":61969},{\"end\":61991,\"start\":61979},{\"end\":62005,\"start\":61991},{\"end\":62019,\"start\":62005},{\"end\":62310,\"start\":62302},{\"end\":62320,\"start\":62310},{\"end\":62328,\"start\":62320},{\"end\":62334,\"start\":62328},{\"end\":62593,\"start\":62585},{\"end\":62608,\"start\":62593},{\"end\":62616,\"start\":62608},{\"end\":62623,\"start\":62616},{\"end\":62980,\"start\":62968},{\"end\":63199,\"start\":63189},{\"end\":63209,\"start\":63199},{\"end\":63218,\"start\":63209},{\"end\":63229,\"start\":63218},{\"end\":53746,\"start\":53736},{\"end\":53758,\"start\":53746},{\"end\":53766,\"start\":53758},{\"end\":53976,\"start\":53967},{\"end\":53988,\"start\":53976},{\"end\":54163,\"start\":54153},{\"end\":54172,\"start\":54163},{\"end\":54185,\"start\":54172},{\"end\":54202,\"start\":54185},{\"end\":54213,\"start\":54202},{\"end\":54476,\"start\":54463},{\"end\":54629,\"start\":54613},{\"end\":54763,\"start\":54752},{\"end\":54773,\"start\":54763},{\"end\":54977,\"start\":54966},{\"end\":54986,\"start\":54977},{\"end\":54996,\"start\":54986},{\"end\":55006,\"start\":54996},{\"end\":55218,\"start\":55204},{\"end\":55226,\"start\":55218},{\"end\":55239,\"start\":55226},{\"end\":55438,\"start\":55427},{\"end\":55449,\"start\":55438},{\"end\":55459,\"start\":55449},{\"end\":55673,\"start\":55661},{\"end\":55684,\"start\":55673},{\"end\":55696,\"start\":55684},{\"end\":55707,\"start\":55696},{\"end\":55714,\"start\":55707},{\"end\":55725,\"start\":55714},{\"end\":55735,\"start\":55725},{\"end\":55979,\"start\":55971},{\"end\":56082,\"start\":56074},{\"end\":56090,\"start\":56082},{\"end\":56103,\"start\":56090},{\"end\":56115,\"start\":56103},{\"end\":56121,\"start\":56115},{\"end\":56332,\"start\":56324},{\"end\":56340,\"start\":56332},{\"end\":56352,\"start\":56340},{\"end\":56630,\"start\":56621},{\"end\":56637,\"start\":56630},{\"end\":56643,\"start\":56637},{\"end\":56654,\"start\":56643},{\"end\":56664,\"start\":56654},{\"end\":56916,\"start\":56905},{\"end\":56928,\"start\":56916},{\"end\":56940,\"start\":56928},{\"end\":56952,\"start\":56940},{\"end\":56965,\"start\":56952},{\"end\":56973,\"start\":56965},{\"end\":56985,\"start\":56973},{\"end\":56993,\"start\":56985},{\"end\":57409,\"start\":57400},{\"end\":57420,\"start\":57409},{\"end\":57635,\"start\":57624},{\"end\":57886,\"start\":57875},{\"end\":57895,\"start\":57886},{\"end\":57904,\"start\":57895},{\"end\":58091,\"start\":58078},{\"end\":58108,\"start\":58091},{\"end\":58122,\"start\":58108},{\"end\":58407,\"start\":58396},{\"end\":58419,\"start\":58407},{\"end\":58428,\"start\":58419},{\"end\":58693,\"start\":58680},{\"end\":59277,\"start\":59270},{\"end\":59468,\"start\":59459},{\"end\":59478,\"start\":59468},{\"end\":59488,\"start\":59478},{\"end\":59498,\"start\":59488},{\"end\":59798,\"start\":59792},{\"end\":59807,\"start\":59798},{\"end\":59814,\"start\":59807},{\"end\":59822,\"start\":59814},{\"end\":59831,\"start\":59822},{\"end\":59838,\"start\":59831},{\"end\":60293,\"start\":60281},{\"end\":60307,\"start\":60293},{\"end\":60570,\"start\":60560},{\"end\":60585,\"start\":60570},{\"end\":60597,\"start\":60585},{\"end\":60607,\"start\":60597},{\"end\":60795,\"start\":60784},{\"end\":60961,\"start\":60950},{\"end\":60974,\"start\":60961},{\"end\":60982,\"start\":60974},{\"end\":60995,\"start\":60982},{\"end\":61003,\"start\":60995},{\"end\":61262,\"start\":61254},{\"end\":61272,\"start\":61262},{\"end\":61503,\"start\":61492},{\"end\":61514,\"start\":61503},{\"end\":61525,\"start\":61514},{\"end\":61745,\"start\":61737},{\"end\":61917,\"start\":61903},{\"end\":61927,\"start\":61917},{\"end\":61936,\"start\":61927},{\"end\":61943,\"start\":61936},{\"end\":61958,\"start\":61943},{\"end\":61969,\"start\":61958},{\"end\":61979,\"start\":61969},{\"end\":61991,\"start\":61979},{\"end\":62005,\"start\":61991},{\"end\":62019,\"start\":62005},{\"end\":62310,\"start\":62302},{\"end\":62320,\"start\":62310},{\"end\":62328,\"start\":62320},{\"end\":62334,\"start\":62328},{\"end\":62593,\"start\":62585},{\"end\":62608,\"start\":62593},{\"end\":62616,\"start\":62608},{\"end\":62623,\"start\":62616},{\"end\":62980,\"start\":62968},{\"end\":63199,\"start\":63189},{\"end\":63209,\"start\":63199},{\"end\":63218,\"start\":63209},{\"end\":63229,\"start\":63218}]", "bib_venue": "[{\"end\":53509,\"start\":53452},{\"end\":53773,\"start\":53766},{\"end\":53995,\"start\":53988},{\"end\":54220,\"start\":54213},{\"end\":54487,\"start\":54476},{\"end\":54781,\"start\":54773},{\"end\":55013,\"start\":55006},{\"end\":55248,\"start\":55239},{\"end\":55466,\"start\":55459},{\"end\":55771,\"start\":55735},{\"end\":56128,\"start\":56121},{\"end\":56415,\"start\":56352},{\"end\":56674,\"start\":56664},{\"end\":57030,\"start\":56993},{\"end\":57427,\"start\":57420},{\"end\":57672,\"start\":57662},{\"end\":57873,\"start\":57832},{\"end\":58151,\"start\":58122},{\"end\":58444,\"start\":58428},{\"end\":58750,\"start\":58693},{\"end\":59268,\"start\":59235},{\"end\":59521,\"start\":59498},{\"end\":59790,\"start\":59683},{\"end\":60113,\"start\":60087},{\"end\":60279,\"start\":60177},{\"end\":60616,\"start\":60607},{\"end\":61010,\"start\":61003},{\"end\":61282,\"start\":61272},{\"end\":61532,\"start\":61525},{\"end\":61752,\"start\":61745},{\"end\":62028,\"start\":62019},{\"end\":62300,\"start\":62261},{\"end\":62674,\"start\":62664},{\"end\":62998,\"start\":62980},{\"end\":63235,\"start\":63229},{\"end\":53509,\"start\":53452},{\"end\":53773,\"start\":53766},{\"end\":53995,\"start\":53988},{\"end\":54220,\"start\":54213},{\"end\":54487,\"start\":54476},{\"end\":54781,\"start\":54773},{\"end\":55013,\"start\":55006},{\"end\":55248,\"start\":55239},{\"end\":55466,\"start\":55459},{\"end\":55771,\"start\":55735},{\"end\":56128,\"start\":56121},{\"end\":56415,\"start\":56352},{\"end\":56674,\"start\":56664},{\"end\":57030,\"start\":56993},{\"end\":57427,\"start\":57420},{\"end\":57672,\"start\":57662},{\"end\":57873,\"start\":57832},{\"end\":58151,\"start\":58122},{\"end\":58444,\"start\":58428},{\"end\":58750,\"start\":58693},{\"end\":59268,\"start\":59235},{\"end\":59521,\"start\":59498},{\"end\":59790,\"start\":59683},{\"end\":60113,\"start\":60087},{\"end\":60279,\"start\":60177},{\"end\":60616,\"start\":60607},{\"end\":61010,\"start\":61003},{\"end\":61282,\"start\":61272},{\"end\":61532,\"start\":61525},{\"end\":61752,\"start\":61745},{\"end\":62028,\"start\":62019},{\"end\":62300,\"start\":62261},{\"end\":62674,\"start\":62664},{\"end\":62998,\"start\":62980},{\"end\":63235,\"start\":63229},{\"end\":58784,\"start\":58777},{\"end\":58784,\"start\":58777}]"}}}, "year": 2023, "month": 12, "day": 17}