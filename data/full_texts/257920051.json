{"id": 257920051, "updated": "2023-10-02 17:50:16.683", "metadata": {"title": "InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions", "authors": "[{\"first\":\"Zeqiu\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Ryu\",\"last\":\"Parish\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Sewon\",\"last\":\"Min\",\"middle\":[]},{\"first\":\"Prithviraj\",\"last\":\"Ammanabrolu\",\"middle\":[]},{\"first\":\"Mari\",\"last\":\"Ostendorf\",\"middle\":[]},{\"first\":\"Hannaneh\",\"last\":\"Hajishirzi\",\"middle\":[]}]", "venue": "TACL", "journal": "Transactions of the Association for Computational Linguistics, Volume 11", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In an information-seeking conversation, a user may ask questions that are under-specified or unanswerable. An ideal agent would interact by initiating different response types according to the available knowledge sources. However, most current studies either fail to or artificially incorporate such agent-side initiative. This work presents InSCIt, a dataset for Information-Seeking Conversations with mixed-initiative Interactions. It contains 4.7K user-agent turns from 805 human-human conversations where the agent searches over Wikipedia and either directly answers, asks for clarification, or provides relevant information to address user queries. The data supports two subtasks, evidence passage identification and response generation, as well as a human evaluation protocol to assess model performance. We report results of two systems based on state-of-the-art models of conversational knowledge identification and open-domain question answering. Both systems significantly underperform humans, suggesting ample room for improvement in future studies.1", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.00746", "mag": null, "acl": "2023.tacl-1.27", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-00746", "doi": "10.1162/tacl_a_00559"}}, "content": {"source": {"pdf_hash": "64d3d7cedc9affb114ec9ecb638e325a4789d2e9", "pdf_src": "MIT", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "138151ff224624300407f836e4881a6c93d616cc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/64d3d7cedc9affb114ec9ecb638e325a4789d2e9.txt", "contents": "\nINSCIT: Information-Seeking Conversations with Mixed-Initiative Interactions\n\n\nZeqiu Wu zeqiuwu1@uw.edu \nRyu Parish rparish@uw.edu \nHao Cheng \nMicrosoft Research\nUSA\n\n\u2663 Sewon sewon@uw.edu \nMin \u2660 Prithviraj \nAllen Institute for AI\nUSA\n\nAmmanabrolu \nMari Ostendorf ostendor@uw.edu \nHannaneh Hajishirzi hannaneh@uw.edu \nAllen Institute for AI\nUSA\n\n\nUniversity of Washington\nUSA\n\nINSCIT: Information-Seeking Conversations with Mixed-Initiative Interactions\n10.1162/tacl\nIn an information-seeking conversation, a user may ask questions that are under-specified or unanswerable. An ideal agent would interact by initiating different response types according to the available knowledge sources. However, most current studies either fail to or artificially incorporate such agent-side initiative. This work presents INSCIT, a dataset for Information-Seeking Conversations with mixed-initiative Interactions. It contains 4.7K user-agent turns from 805 human-human conversations where the agent searches over Wikipedia and either directly answers, asks for clarification, or provides relevant information to address user queries. The data supports two subtasks, evidence passage identification and response generation, as well as a human evaluation protocol to assess model performance. We report results of two systems based on state-of-the-art models of conversational knowledge identification and opendomain question answering. Both systems significantly underperform humans, suggesting ample room for improvement in future studies. 1\n\nIntroduction\n\nRecently, there has been increasing interest in developing conversational information-seeking systems (Choi et al., 2018;Adlakha et al., 2022;Saeidi et al., 2018;Feng et al., 2020) that assist users in finding information from knowledge sources (e.g., text corpus) via multi-turn conversational interactions. One important advantage of such conversational information-seeking systems is that users do not need to come up with a very descriptive query by themselves (Webb and Webber, 2009;Rieser and Lemon, 2009;Konstantinova and Orasan, 2013). In realistic settings, as shown in Figure 1, users can start with a request that is under-specified or has no direct answer, and through conversational interactions, the agent can collaboratively guide users to refine (left) or relax their queries and proactively suggest relevant information that may partially satisfy the user's information needs (right). This collaboration requires a mixed-initiative dialogue, where both the user and agent can direct the flow of the conversation.\n\nHandling such realistic user requests poses challenges to a conversational agent system. A comprehensive search can result in multiple passages from the knowledge source, which may provide different components of an answer (possibly incomplete) or multiple answers that surface ambiguities in the user query. Depending on the available information, the agent needs to use different strategies, which might involve summarizing the results, providing partial information, or trying to clarify an ambiguity. However, existing information-seeking conversation datasets rarely contain conversations where agents initiate different interaction strategies. As a result, most conversational question answering (CQA) work focuses on user-initiative interactions, where the agent simply responds to user questions with direct answers or uses no answer for out-of-scope queries (Choi et al., 2018;Reddy et al., 2019;Adlakha et al., 2022). Other work studies clarification questions using artificially created data, failing to capture natural information-seeking interactions (Saeidi et al., 2018;Feng et al., 2020;Aliannejadi et al., 2021;Guo et al., 2021). In addition, most of them only rely on a single evidence string or passage for agent response construction.\n\nTo support research in mixed-initiative conversations, we introduce INSCIT (pronounced Insight), a dataset for Information-Seeking Con- versations with mixed-initiative Interactions, where agents take various strategies, such as providing direct answers (72%), raising clarifications (13%), and presenting relevant partial information (13%), to address users' information needs. It contains 805 natural human-human conversations with 4.7K user-agent turns over diverse topics, collected through a scalable annotation pipeline and careful quality control. To simulate realistic information-seeking scenarios, users write queries with minimal restriction, and human agents decide on different strategies to respond, after searching over the knowledge source (i.e., Wikipedia) for evidence passages.\n\nWe formulate two tasks for the conversational agent system: (1) identify a set of evidence passages from Wikipedia, and (2) generate a response grounded in the evidence. Since handling queries with multiple evidence passages or no direct answer can be open-ended, we emphasize the need for human evaluation, and propose a systematic human evaluation protocol that considers diverse aspects including coherence, factual consistency, and information comprehensiveness.\n\nWe present two strong baselines based on the state-of-the-art in open-domain question answering (Karpukhin et al., 2020;Izacard and Grave, 2021) and conversational knowledge identification (Wu et al., 2021). While the systems achieve substantial improvements over a trivial baseline, there is still significant room for improvements, especially for scenarios requiring agent strategies other than providing a direct answer. Our analysis suggests that the key remaining challenges are improving passage identification and indicates the property only applies to part of the dataset. fusing comprehensive information from multiple passages by leveraging different strategies. We present detailed discussion and avenues for future work.\nDataset IR Response Strategy H-H Multi- CLAR REL Dialogue Evidence INSCIT (ours) \u2713 \u2713 \u2713 \u2713 \u2713 QuAC \u2717 \u2717 \u2717 \u2713 \u2717 CoQA \u2717 \u2717 \u2717 \u2713 \u2717 DoQA \u2713 \u2717 \u2717 \u2713 \u2717 QReCC \u2713 \u2717 \u2717 \u2717 TopioCQA \u2713 \u2717 \u2717 \u2713 \u2717 Qulac \u2717 \u2713 \u2717 \u2717 \u2717 ShARC \u2717 \u2713 \u2717 \u2717 \u2717 MultiDoc2Dial \u2713 \u2713 \u2717 \u2717 \u2717 Abg-CoQA \u2717 \u2713 \u2717 \u2717 \u2713\n\nRelated Work\n\nInformation-Seeking Conversations The aim of information-seeking conversations is to address the user's initial and follow-up information needs with grounding in knowledge sources. Table 1 compares INSCIT with previous informationseeking conversation datasets. Early CQA work, including QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019), requires the agent to answer each user question by reading a short passage. DoQA (Campos et al., 2020), QReCC (Anantha et al., 2021), and TopioCQA (Adlakha et al., 2022) extend the task to an open-domain setting where the knowledge source is a large document corpus. These studies only consider limited scenarios where the agent provides a direct answer based on a short text span in a single passage, or outputs no answer if there is no direct answer.\n\nAmbiguous user queries have been observed in single-turn question answering tasks Zhang and Choi, 2021;Sun et al., 2022), but these are usually addressed by training a model to predict multiple conditional answers without further interaction. A few other studies create artificial conversations to address ambiguous user questions. For instance, Qulac (Aliannejadi et al., 2019) and the data collected in follow-up work (Aliannejadi et al., 2021) are based on user queries containing a set of pre-specified multi-faceted entities, where agents choose from a fixed set of clarification questions that cover these ambiguities. ShARC (Saeidi et al., 2018), Doc2Dial (Feng et al., 2020), and MultiDoc2Dial (Feng et al., 2021) are rule-based information-seeking conversations in the social welfare domain that incorporate agent-side clarifications. Guo et al. (2021) create Abg-CoQA by rewriting conversations in the CoQA dataset to intentionally include ambiguous questions. In contrast, INSCIT consists of human-human conversations with natural information-seeking user requests and mixed agent initiative to address them. Penha et al. (2019) crawl conversations from Stack Exchange 2 that are mixed with informationseeking utterances and casual talk. One grounding document is heuristically obtained for each conversation. In contrast, INSCIT contains validated grounding passages and only goal-oriented agent interactions.\n\nKnowledge-Grounded Social Chat Instead of seeking for information, the user intent in social chat is mostly to conduct casual talk. Knowledgegrounded social chat systems (Ghazvininejad et al., 2018;Dinan et al., 2019;Zhou et al., 2018;Moghe et al., 2018) incorporate external knowledge with the purpose of making the conversations more engaging and informative. Rodriguez et al. (2020) trains a conversational agent to select knowledge to present based on the user's background, in order to maintain the user's interest in the conversation.\n\n\nTask Formulations\n\nWe define two task formulations for INSCIT, namely passage identification and response generation. These two tasks mimic how an agent responds to each information-seeking user request, by first searching for relevant information over the knowledge source and then constructing the response based on the gathered information. Comparing with prior studies on open-domain information-seeking conversations (Anantha et al., 2021;Adlakha et al., 2022), the key challenges in our tasks come from identifying and fusing comprehensive information from multiple passages to construct responses using different strategies, rather than a single passage and a short answer.\n\nAt the n th agent turn, both tasks have the same input: all previous utterances (i.e., dialogue context) X = [u 1 , a 1 , u 2 , a 2 , . . . , u n ], the corpus of all passage candidates C, and the previously used passages {P 1 , P 2 , . . . , P n\u22121 } where each\nP i = {p 1 i , p 2 i , . . . , p |P i |\ni } is the set of passages used in the i th agent turn a i . C is defined as all textual paragraphs (i.e., passages) in a full Wikipedia dump. 3 For passage identification, we require the model to predict a set of passagesP n from C, containing comprehensive and relevant information to the current user request u n in the dialogue context X, which serves as evidence for the response generation task-generating the next agent response\u0101 n . This is different from the passage retrieval task where only a ranked list of relevant passages is predicted. Identifying specific knowledge to be used in the response can be important for model interpretability purposes as well as for evaluating how well a model grounds the response generation in the knowledge source. Ideally, all factual information contained in\u0101 n should be consistent withP n , and every passage inP n should provide at least one unique information piece as evidence for\u0101 n .\n\nIn interactive dialogues, each predicted evi-denceP i and response\u0101 i are used in the dialogue context for later conversations. However, to use pre-collected dialogues with automatic evaluation metrics, the input context must be the same as that leading to the human reference response. This is also consistent with setups in previous information-seeking dialogue studies that are discussed in \u00a7 2. Therefore, the gold {P i } and {a i } are used here as inputs in testing.\n\n\nOur Data: INSCIT\n\nWe introduce INSCIT, a new information-seeking conversation dataset where the agent interprets the user intent and provides comprehensive information grounded in Wikipedia via natural human-human interactions. In this section, we present our data collection pipeline, quality control mechanisms, and analyses that show the characteristics and diversity of the user and agent turns. Figure 2: Each conversation is annotated in a series of user \u2192 agent \u2192 validator tasks. One worker is dedicated to each user/validator task but two workers work in parallel on the agent turn annotation (see discussion in \u00a7 4.1).\n\n\nData Collection Pipeline\n\nWe recruit user, agent, and validation workers 4 to create and annotate user/agent turns and validate agent annotations, respectively. Due to the asymmetric time spent by user and the agent workers in a conversation, we design a separate annotation task for each user or agent turn, following Wen et al. (2017) to annotate each dialogue in a pipelined fashion. This framework has proved to be efficient while maintaining the conversation coherence by requiring each worker to read all previous utterances. Our data collection has IRB approval and is deemed exempt. Figure 2 illustrates the data collection and annotation pipeline. Each conversation starts with an initial user turn, where the worker asks a question after reading a text snippet from a seed document. Then, two agents independently search for relevant passages in Wikipedia, provide a response, and categorize their response. Validation follows after each user-agent turn. We refer to the retrieved passages, contributed text, and validations collectively as ''annotations.'' The user/ agent/validation process is repeated for 7 turns or until responses are found to be invalid. Details for each step follow.\n\nSeed Document Selection To diversify conversation topics, we sample seed Wikipedia articles, used for triggering initial user requests, from 5 different topic categories-food and drink, hobby, historical events, geography, and weekly top-25 pages. Additionally, we leverage the top-down tree structure of Wikipedia categories 5 and sample articles at various tree depths under each of the first 4 categories. Weekly top-25 pages are from Wikipedia weekly reports of 2021. 6 Figure 3 (left) shows the distribution of sampled seed documents under each category and their corresponding depths.\n\nUser Turn Here, a user worker is asked to write an initial query or follow-up response to continue the existing conversation. To trigger each conversation ( Figure 2(a)), the user worker is presented with the leading paragraph of a seed article, and is instructed to ask a question they are interested in but cannot find the answer from the paragraph. The article content outline containing all section titles is also provided to help with the question construction. The annotation for each following user turn (d) starts after the completion of the previous agent annotation (b) and the validation step (c), based on all previous conversation utterances.\n\nAgent Turn Different from the user worker, in addition to the dialogue context, each agent worker (Figure 2(b)) is given all evidence paragraphs used by each previous agent turn as additional context. Then, the worker is told to use the provided search engine 7 to find answer(s) from Wikipedia for the current user request. They are asked to select all (up to 4) evidence paragraphs from Wikipedia, which they then use to construct their response. They are also asked to categorize their response, choosing one of four response strategies: {direct answer (DIRECT), clarification (CLAR), relevant answer (REL), and no information (NI)}. In contrast to a direct and complete answer, we consider a response as a relevant answer when the agent finds information that only partially satisfies the user need (e.g., relax a constraint in the request). For each agent turn, we collect two different annotations to increase reference diversity.\n\nValidation After each user turn, we send the two agent annotations to a validator (Figure 2(c)). For each agent turn annotation, the validator determines whether i) each selected evidence paragraph is properly used in the response; ii) the response is factually consistent with the evidence; iii) the response is coherent to the dialogue context; and vi) the labeled response strategy is faithfully reflected in the response. If both are valid, the validator is asked to rate which one is more comprehensive, where a tie is permitted. An agent response is considered more comprehensive if it contains more information relevant to the user request. The more comprehensive (or the only valid) annotation 8 is then used to continue the conversation. The annotation is terminated if 7 Based on Google Search API from https:// developers.google.com/custom-search and restricted to the https://en.wikipedia.org/ domain. 8 We randomly select one if there is a tie.\n\nboth annotations are invalid, and we include the conversation up to the previous turn in our data.\n\n\nQuality Control\n\nWorker Qualification To recruit agent workers, we manually review > 150 submissions of a qualification task and select 24 highly qualified workers who consistently produce valid annotations during the qualification. The qualification task consists of 12 agent annotation tasks, where each dialogue context is written by the first two authors of this paper. Similarly, we create different qualification tasks to select 35 qualified users and 10 validators who consistently produce reasonable user responses or validations based on our manual review.\n\nAnnotation Control To discourage users from chit-chatting or raising inappropriate requests (e.g., too subjective), each agent worker can decide to either continue the conversation or flag their previous user turn as incoherent or an invalid request. The validation process ensures that only valid agent annotations are included in our final dataset. To encourage extensive search for comprehensive information, we assign a bonus to an agent worker if their annotation is labeled as equally or more comprehensive than the other worker.\n\nWe constantly monitor the annotation process and send feedback to workers. Our user and agent workers have over 99% and 96% average passing validation rate, respectively. About 13% of agent annotations are marked as less comprehensive.\n\nWorker Payment Structure We actively communicate with workers throughout the annotation process to clarify any questions they have and  to give them feedback. We also check in with them early on to make sure they are satisfied with the pay and bonus structure. Most workers report that they are paid with an hourly rate of 15-20 USD, depending on their annotation speed. We pay 0.2/0.5/0.5 USD for each user/agent/validator annotation, plus a 0.1 USD bonus for each agent annotation if the worker passes validation over 80% of the time (all qualified). In addition, we assign a bonus of 0.3 USD to the agent annotation that is marked as equally comprehensive as its peer annotation by the validator, or 0.5 to those marked as more comprehensive or with multiple evidence passages found. 9 On average, we pay over 0.9 USD to each agent annotation.\n\n\nData Analysis\n\nWe collect 805 conversations, which includes 4712 user-agent turns after dropping agent annotations if their evidence passages cannot be found in the post-processed Wikipedia corpus. 10 Table 2 shows summary statistics of the train/ dev/test subsets of INSCIT. Word token counts are based on the spaCy (Honnibal et al., 2020) tokenizer. The test set contains conversations triggered with seed documents from all 5 topic categories, while the training and dev sets only contain those from ''food and drink'', ''hobby'', and ''top-25''. In the training set, we keep all valid agent annotations as well as their comprehensiveness comparison results.\n\nIn the dev and test sets, we did not include agent responses flagged as less comprehensive during validation. In addition, as discussed in \u00a7 4.2, we adjust the worker incentives to obtain more comprehensive responses when collecting dev/test sets, leading to the difference in the average agent turn length.\n\n\nDiversity of User and Agent Turns\n\nUser Request We analyze the distribution of wh-words of user questions, as well as nonquestion user utterances (e.g., responses to clarification). The treemaps in Figure 3 (middle and right) show the 7 most frequent leading unigrams of user utterances in ''food & drink'' and ''historical events'' conversations, respectively. ''MISC'' refers to utterances with less frequent leading unigrams. Each box size is proportional to its percentage in the data. As shown, most user requests are ''what'' and ''how'' questions. There are also many user turns starting with words like ''can'' and ''tell'', most of which are responses to agent clarification questions. The user utterances are fairly long-tailed as ''MISC'' shares a large portion (about 30%) for both treemaps. Instead of being mostly factoid questions, open-ended user requests are well represented in INSCIT. Table 4 shows the diversity of agent response strategies in INSCIT.\n\n\nAgent Response Strategy\n\nWhen no direct answer exists, agents in INSCIT can respond to the user with a relevant answer (see \u00a7 4.1). If no direct or relevant answer is found, the agent can then respond with no information. The average response length and number of evidence passages differ dramatically across various response strategies. Compared with direct or relevant answer cases, clarification responses tend to be shorter and are more likely to happen when more evidence passages are present. We also calculate that 30% direct or relevant answer agent turns have multiple evidence passages, which potentially require information summarization.\n\n\nAnalysis of Agent Initiatives\n\nIn this section, we present qualitative analysis to understand how different agent initiatives get triggered, with a focus on clarification and relevant answer agent responses.\n\nFine-Grained Categorization We randomly sample and analyze 100 clarification and relevant answer responses respectively.   a clarification when they find a long answer or too many answers (86%) or notice an ambiguous entity in the user request (13%). In 70% of relevant answer cases (bottom half of Table 3), the agent relaxes some constraint in the user request or provides evidence that no definite answer can be found. In 29% of these cases, they simply provide some relevant but side information only. We also observe that in rare cases (1%), the agent points out some mistake (e.g., a false assumption) in the user request.\n\nClarification Occurrences We next look at contexts where agents are more likely to ask for clarification in a conversation. Clarification questions occur more frequently at the very beginning (ex. 2, Table 3), rather than later in a conversation (18.8% vs. 11.5%). If a clarification is raised in the previous agent turn, the chance of a subsequent clarification (see Table 6) is 7.6%, compared to 12.2% if the previous turn is not a clarification (ex. 1, Table 3).\n\n\nResponse Strategy Selection\n\nIn 23% of examples with 2 agent annotations marked as equally comprehensive by validators, workers take different response strategies given the same dialogue context. Of this set, 82% have different evidence passages labeled by the two workers, potentially due to the open-endedness of user queries in INSCIT and the large knowledge source. In addition, as suggested by our analyses in \u00a7 6.2, it is more likely that agents will choose different evidence passages when there is no direct answer to the question. As illustrated in the first example in Table 5, the different evidence passages often trigger different agent response strategies. The second and third examples show that even if two agents find the same evidence set, deciding whether it indicates an under-specified user request, a direct or only a relevant answer can be subjective.\n\n\nExperiment Setup\n\n\nSystems\n\nWe build two systems for each of the tasks formulated in \u00a7 3. Both systems build on retrieverreader models, inspired by recent advances in open-domain single-turn or conversational question answering (Karpukhin et al., 2020;Izacard and Grave, 2021;Adlakha et al., 2022). Here, the main function of the retriever is to gather a ranked set of top-k candidate evidence passages from the entire Wikipedia to facilitate passage identification and response generation for the later reader model. We first describe the retriever models, and  then introduce the two reader models that perform the two main tasks based on retrieval results.\n\n\nRetriever Models\n\nWe experiment with two retrievers: BM25 and DPR. BM25 (Robertson and Zaragoza, 2009) uses sparse bag-of-word representations for ranking passages with regard to each query. We use Pyserini (Yang et al., 2017) in our experiments. DPR (Karpukhin et al., 2020) is a BERT-based (Devlin et al., 2019) dual encoder model that produces learned dense representations for queries and passages, and measures the relevance using the dot product similarity in the vector space. We finetune DPR on INSCIT. As the training set is small in INSCIT, we initialize it with a downloadable checkpoint 11 that is pre-trained on a much larger (> 30\u00d7) open-domain conversational question answering dataset, TopioCQA (Adlakha et al., 2022).\n\n\nReader Models\n\nOur two readers are based on state-of-theart models in open-domain question answering and conversational knowledge identification-Fusion-in-Decoder (Izacard and Grave, 2021) and DIALKI (Wu et al., 2021).\n\nFusion-in-Decoder (FiD) FiD is a generative reader model. It first encodes all retrieved passages with a given query, and then decodes the task output (e.g., an answer string) by attending over all encoded passages. To adapt FiD to our tasks,  we prepend a passage identifier (ID) to each of the top-k retrieved passages (here, k = 50, following Adlakha et al. [2022]) and separately concatenate each passage with the dialogue context for encoding. Given the 50 encoded contextualized passage vectors, the decoder generates a sequence of evidence passage IDs (passage identification), followed by the final response (response generation).\n\nAfter the first turn, the encoded passage vectors associated with {P i , . . . , P n\u22121 } are concatenated with the top-k retrieved passages, limiting k to give a total of 50. In training, we use the same hyperparameters as in Adlakha et al. (2022), with the batch size adjusted for the memory constraint and training steps adjusted to have the same epochs.\n\n\nDIALKI + FiD\n\nThe second reader adopts a pipelined approach to perform the two tasks. It first uses DIALKI (Wu et al., 2021) to select evidence passages and then feeds the identified passages into FiD to generate the agent response. DIALKI is a state-of-the-art conversational knowledge identification model that incorporates dialogue structure with a multi-task learning framework. DIALKI predicts a passage score for each input passage (i.e., each top-k retrieved passage). To adapt it for passage identification, we simply keep evidence passages (up to 4, as in data collection) with ranking scores higher than \u03b3 for multiple passage prediction, where \u03b3 is tuned on the dev set. We apply the same method to incorporate previously used evidence passages into DIALKI as in the first reader model. We set the number of input passages of DIALKI to be 50 and keep other original hyperparameters. Parameters in FiD are the same as the first reader model, except that the number of input passages is 4 in the DIALKI+FiD system.\n\nTrivial Baseline: Last Turn We report performance of a simple baseline: use the most recent agent turn in the dialogue context and associated evidence (P n = P n\u22121 ;\u0101 n = a n\u22121 ). For first-turn instances, we use the most frequent evidence passage and agent response seen in the training set as the prediction. We also tried using a random previous turn as the prediction, which gives lower scores than using the last turn.\n\nHuman We collect one additional annotation for each agent turn in the test set and evaluate it as the human performance. These additional annotations are annotated by the same agent workers we select in \u00a7 4.2. Note that these additional prediction data do not go through the same validation step as those that are used as references.\n\n\nEvaluation\n\nBelow, we describe automatic metrics and a human evaluation protocol for the passage identification (PI) and response generation (RG) tasks in \u00a7 3.\n\nPassage Identification INSCIT allows for multiple evidence passages, so we measure the model performance by computing the F1 score (PI-F1), comparing the set of predicted evidence passages P n to the set of reference passages P n . For turns where there are two valid reference annotations, we use the maximum F1 score between the two.\n\nResponse Generation For a generated agent response\u0101, we calculate the SACREBLEU score (Post, 2018) (BLEU in tables) and token-level F1 (RG-F1) scores against the reference response, following previous studies (Feng et al., 2020;Adlakha et al., 2022). Again, when there are two valid annotations, we use the maximum.\n\nHuman Evaluation As the two tasks are dependent on each other, decoupled automatic evaluations may not capture aspects like factual consistency between predicted passages and the response. Moreover, handling queries with multiple evidence passages or no direct answer can be open-ended. Therefore, we design a human evaluation protocol to evaluate the model performance on both tasks. 12 Specifically, we focus on the evaluation of 4 dimensions: 1) evidence passage utility: how many predicted evidence passages are used in the generated response; 2) factual consistency between the predicted response and evidence; 3) response coherence with the dialogue context; and 4) response comprehensiveness: how much information, that is both relevant to the user request and factually consistent with the predicted evidence, is contained in the response. While most prior work on information-seeking dialogues only relies on automatic metric scores (Choi et al., 2018;Anantha et al., 2021;Adlakha et al., 2022), a few studies collect human ratings on dimensions like response ''coherence'' and ''informativeness'' Feng et al., 2022). However, as they do not require models to predict evidence, the factual consistency between the response and the knowledge source cannot be evaluated (Nakano et al., 2021).\n\nWe provide outputs for both tasks of our two systems and ''Human'' to a human judge. We ask them to rate the first 3 dimensions for each system output on a 4-or 5-point Likert scale 13 and then rank the system responses in terms of response comprehensiveness (ties are permitted). We have 3 raters for each agent turn and take the average rating score or rank place on each dimension for each system. Since human evaluation can be time-consuming and costly, we run it on a sampled test subset with 50 conversations (290 turns) and encourage future studies to report on the same subset.\n\nThe inter-rater agreement measured as Krippendorf's alpha is 0.66, 0.64, 0.42, and 0.37 for EU, FC, CO, and COMP, respectively, which can be interpreted as good or moderate agreements. We observe two main types of coherence disagreements: 1) some workers are more strict and 12 We release the code at https://github.com /ellenmellon/INSCIT/tree/main/eval/human eval.\n\n13 The 4-point scale is used only for coherence to discourage neutral ratings. We report all scores normalized to a 1-5 scale.  indicate one response as more preferred due to minor differences (e.g., a connecting word), or 2) both responses are incoherent, but in very different ways (e.g., have very different content). Similarly, most comprehensiveness disagreements involve either: 1) two responses that are similar except that one includes additional side information, or 2) two responses that provide different answers but both are good.\n\n6 Experiment Results Table 7 shows the overall automatic evaluation results of all systems for our main tasks (PI and RG) on the dev set. The simple baseline performs very poorly. Using retrieval results from DPR (vs. BM25) leads to the best overall performance for both tasks. For both BM25 and DPR retrievers, DIALKI + FiD achieves better performance than FiD in all metrics. A possible reason could be that the smaller number of context vectors used with DIALKI+FiD is better suited to learning from limited data than the end-to-end FiD approach. DIALKI leverages previous evidence passages in passage identification, so its following FiD response generation has only 4 context vectors (vs. 50 for FiD). This hypothesis is supported by the observation that incorporating previously used evidence hurts the RG performance slightly for FiD but for DIALKI+FID it helps (roughly 1 point decrease vs. increase in scores, respectively, with DPR). Table 8 shows both automatic and human evaluation results on the test set for FiD and DIALKI+FiD with the DPR retriever, confirming the dev set findings. Experiments with BM25 also confirm dev set trends. Figure 4 presents comparative human evaluation results. DIALKI+FiD greatly outperforms FiD except in coherence where scores are similar. DIALKI+FiD substantially underperforms humans in both automatic   and human scores, except for factual consistency where the difference is small. This could indicate that, although DIALKI+FiD generates responses consistent with the predicted evidence, it identifies less relevant passages which lead to less coherent and less informative responses. The reason for imperfect human performance on passage identification, shown in Table 8, is two-fold. Due to the open-endedness of information-seeking queries in INSCIT and the large search space over Wikipedia, annotators may find different (but both valid) sets of evidence passages. In addition, annotations corresponding to the Human ''system'' do not go through the validation process, so they could have errors or be less comprehensive.\n\n\nQuantitative Results\n\n\nAnalysis\n\nPassage Retrieval Table 9 reports the performance for passage retrieval in HIT@k scores, following Karpukhin et al. (2020) and Adlakha et al. (2022). HIT@k is calculated as 1 |R K \u2229 P| > 0 , where R K denotes the top K retrieved passages and P denotes the union of the two reference passage sets (or a single reference set if only one is valid). We evaluate both BM25 and DPR models used in our main experiments, as well as two DPR ablations: with pretraining (PT) on TopioCQA or finetuning (FT) on INSCIT only.  BM25 underperforms DPR models significantly, which explains the main task performance differences between BM25 and DPR in Table 7. DPR with PT alone is more effective than FT only, which can be explained by the much larger training data in TopioCQA. The best retrieval results are achieved with PT and FT combined. We do not leverage TopioCQA for pretraining on our two main tasks, because 1) it does not come with the passage identification task and only has short answers or no answer as their agent responses; 2) we observe poor zero-shot response generation performance on INSCIT for FiD trained on TopioCQA.\n\nPassage Identification & Response Generation Performance Breakdown Figure 5 shows the system and task performance breakdown by reference response strategy (direct answer, clarification, and relevant answer) for the test set, excluding examples where two annotations differed in the response strategy category (16%). DPR is used for retrieval. Only RG-F1 is shown for response generation; trends for BLEU are similar. For all response types, DIALKI+FiD is similar or outperforms FiD, but significantly underperforms humans. For both systems and humans, the non-direct-answer responses have lower automatic scores. The lower PI-F1 scores for humans suggest that the retrieval task is more difficult (with more variety in evidence) when a simple direct answer is not available. Lower automatic response generation scores may be explained by lower retrieval scores (less reliable evidence), larger number of passages, and/or challenges in learning non-direct-answer response strategies. Note that for both systems, the largest percentage gap with respect to human scores is for clarifications.  The response type classification model gives an overall accuracy of 0.75, compared to 0.73 when predicting everything as ''direct answer.'' Table 10 shows that adding either oracle or predicted response types improves BLEU and RG-F1 scores, compared with no response type being used, with greater gains in RG-F1 for oracle response type. We observe consistent performance gains on examples with either ''direct answer'', ''clarification'', or ''no information'' oracle response types, but not for the ''relevant answer'' response type.\n\n\nResponse Generation\n\n\nConclusion & Discussions\n\nIn summary, we introduce INSCIT, a new opendomain information-seeking conversational dataset grounded in Wikipedia, with mixed-initiative user-agent interactions. INSCIT supports two tasks (passage identification and response generation), for which we present results of two strong baselines, with best results obtained with the pipelined DIALKI+FiD system. We also introduce a human evaluation protocol.\n\nFuture Work Both models significantly underperform humans in both tasks in all metrics. The relative performance gap is greatest for scenarios that require the agent to provide a nondirect answer. We find that passage identification significantly impacts response generation (particularly coherence) by providing relevant grounding knowledge. Thus, improving methods for selecting relevant passages is critical for future work. Key challenges that remain in response generation are how to fuse and present comprehensive information from multiple passages and learning when and how to use non-direct response strategies. Given the small size of our training data, another future direction is to explore transfer learning using existing information-seeking conversation or question answering resources.\n\nOur work focuses on different strategies that can be adopted by the agent to better address user requests in a conversational question answering setting, assuming the user will either ask an information-seeking question or provide a clarification to the agent. Exploring more user-side strategies would be interesting for handling system errors and other types of conversations (e.g., negotiations).\n\nIn contrast to Wikipedia passages, information sources used in practice (e.g., the whole Web) can often contain less trustworthy information. In such cases, retrieving evidence passages containing the same answer and predicting the trustworthiness of each answer based on all such retrieved passages can be a promising direction.\n\nAnother direction that is worth future exploration lies in the design of evaluation metrics. We follow previous studies to evaluate the model performance when given a fixed human-human dialogue context. However, as pointed out by Li et al. (2022), an interactive dialogue system often needs to handle dialogue contexts containing errors made by the model itself. Therefore, it is important for future work to develop new methods for automatic evaluation and scalable human evaluation in the interactive setting.\n\n\nEthical Considerations for Dataset Collection\n\nOur work is primarily intended to encourage future work in information-seeking conversation factually grounded in given knowledge sources. Our knowledge sources come from Wikipedia articles, where the content follows principles emphasizing on a neutral point of view and reliable sources. Before and during the data collection, we carefully guide our user workers not to ask subjective or opinion-driven questions, and our agent workers not to include any content without evidence from the knowledge sources in their conversational responses. Therefore, all contents exposed to our workers during data collection should contain minimal risk to the workers. Our data collection has IRB approval from University of Washington and is deemed exempt. We also actively communicated with the workers to address any concern they had and we usually replied back within an hour during the whole data collection process. This communication also helped us to make sure that our workers were compensated fairly. As explained in \u00a7 4.2, most of our workers report that they are paid with an hourly rate of 15-20 USD.\n\nFigure 1 :\n1INSCIT examples show that user queries can often be under-specified and require clarification (left), or have no direct answer but where providing relevant information may fulfill users' information needs (right).\n\nFigure 3 :\n3Left: seed document topic category breakdown (D \u2192 category depth). Middle and right: treemaps of top 7 (and other: MISC) first tokens in user turns from conversations under ''food & drink'' and ''historical events'' topic categories. For each figure, the size of each colored area is proportional to its percentage in the data.\n\nFigure 4 :\n4Human evaluation on system comparison for all dimensions: evidence utility (EU), factual consistency (FC), coherence (CO), and response comprehensiveness (COMP). Win/lose refers to DIALKI+FiD.\n\nTable 1 :\n1Comparison of INSCIT with existing \ndatasets of information-seeking conversations. \nIR, CLAR, REL, H-H stand for Retrieval Needed, \nClarification, No Direct but Relevant Answer, \nand Human-Human. \n\n\n\nTrain Dev Test Total# Convs \n250 \n86 469 805 \n# Turns \n1443 502 2767 4712 \n# Turns / Conv \n5.8 \n5.8 5.9 \n5.9 \n# References / Turn \n1.8 \n1.6 1.6 \n1.7 \n# Tokens / User \n10.6 10.5 10.7 10.6 \n# Tokens / Agent \n35.7 44.3 45.1 41.9 \n# Passages / Agent \n1.5 \n1.7 1.6 \n1.6 \n\n\n\nTable 2 :\n2Overall statistics of INSCIT.\n\nTable 3 (\n3upper half) shows that in most cases, the agent raises . . . <Agent>: In the Battle of New Orleans, . . . the Americans had 13 dead, totaling 304 dead. <User>: Were there any long-term consequences that came as a result of the War of 1812? <Agent>: There were many! Would you like to know what they were for Bermuda, . . . , orGreat Britain? Ambiguous entity (13%) <User>: Washington University is classified as what for its high research activities? <Agent>: Do you want to know about Washington University in St. Louis or in Baltimore, Maryland?Clarification (CLAR) \n\nToo long / many answers \n(86%) \n\nNo Direct but Relevant Answer (REI) \n\nConstraint relaxation / \nNo definite answer (70%) \n\n<User>: Was the Matrix franchise adapted from any other work? \n<Agent>: While not explicitly adapted from another work, the Matrix did draw heavily on Jean . . . \n\n. . . <User>: Who authored the Torah? \n<Agent>: It was originally thought that a majority of the Torah was written by . . . However, it's \nnow thought that . . . though the number of authors is up for debate. \n\nRelevant but side info only \n(29%) \n\n. . . <User>: What countries have an ecological footprint under 5 hectares per person? \n<Agent>: The world-average ecological footprint in 2013 was 2.8 global hectares per person . . . \nBut I don't have a list of countries with an ecological footprint under 5 global hectares per person. \n\n\n\nTable 3 :\n3Examples of clarification and no-direct-but-relevant-answer agent responses. Factual information from evidence passages is italicized in agent responses.DIRECT \nCLAR \nREL \nNI \n\n% Turns \n71.5 \n12.7 \n13.1 \n2.7 \n# Tokens \n43.7 \n33.5 \n46.6 \n10.6 \n# Passages \n1.5 \n2.8 \n1.4 \n0.0 \n\n\n\nTable 4 :\n4Agent response strategy statistics. DI-RECT, CLAR, REL, and NI indicate direct answer, clarification, no direct but relevant answer, and no information.\n\n\nDialogue Context: . . . <User>: What are some of the health consequences of oats? <Agent> (CLAR): Oat related health concerns include oat allergies, and gluten-related disorders. Which type of health issue would you like to learn more of? (evidence from Wikipedia page: Oat Sensitivity) <Agent> (DIRECT): Oat toxicity in people with gluten-related disorders depends on the oat cultivar consumed because the immunoreactivities of toxic prolamins are different among oat varieties. Furthermore, oats are frequently cross-contaminated with the other gluten-containing cereals . . . (evidence from Wikipedia page: Oat) Dialogue Context: . . . <User>: Who is the major competitor of PepsiCo? <Agent> (CLAR): Would you like to know PepsiCo's major competitor in the beverage market or the snack food market? <Agent> (DIRECT): PepsiCo has two main competitors in different market sectors. The Coca-Cola Company is its largest competitor in the beverage market, and Kraft Foods is its largest competitor in the snack foods market.Different Evidence; Different Response Strategies \n\nSame Evidence; Different Response Strategies \n\nDialogue Context: . . . <User>: Were the directors involved in any controversy? \n\n<Agent> (REL): I couldn't find anything about themselves in any controversy but their film . . . had a controversial storyline and themes. \n<Agent> (DIRECT): The Wachowskis' V for Vendetta was considered to have controversial storyline and themes, but have been both \ncriticized and praised by sociopolitical groups. \n\n\n\nTable 5 :\n5Examples of two agent reference responses with different response strategies.\n\n\nSorry, but each country is detailed as well, do you want to know more about congee in China, Japan, Korea or Taiwan?11 https://github.com/McGill-NLP/topiocqa. \n\n. . . <User>: What kinds of regional varieties are there? \n<Agent>: Would you like to know about East Asia, Southeast \nAsia, South Asia, or Europe? \n<User>: Tell me about East Asia. \n<Agent>: \n\nTable 6 :\n6An example of consecutive clarifications.\n\nTable 7 :\n7Automatic scores on the dev set.\n\nTable 8 :\n8Automatic scores on the test set, and hu-\nman scores on 50 sampled test conversations (290 \nturns) for dimensions rated with Likert scales: ev-\nidence utility (EU), factual consistency (FC), and \ncoherence (CO). \n\n\n\nTable 9 :\n9Passageretrieval results. PT and FT \nrefer to pretraining on TopioCQA and finetuning \non INSCIT. \n\nFigure 5: PI-F1 and RG-F1 scores by reference response \nstrategy (direct answer, clarification, relevant answer) \non the one-strategy test subset, excluding instances \nwhere two references differed in strategy. \n\n\n\n\nResults with Human Evidence Passages To explore the above hypotheses, we generated responses using the DIALKI+FiD response generator with passages selected in the ''Human'' annotation of the test data. The resulting responses had 26.5 and 37.4 for BLEU and RG-F1 scores, respectively, compared to 16.0 and 27.8 when using DIALKI passages. We sample and analyze 20 examples each of single and multiple ''Human'' evidence passages. Given multiple evidence passages, most DIALKI+FiD responses either do not use all passages or introduce incorrect facts. With one passage, responses are consistent with the evidence but not always as comprehensive as for humans. In the 20 examples with multiple passages, DIALKI+FiD asks one clarification, whereas humans ask nine.Impact of Response Type Prediction for Response Generation As explained in \u00a7 4.3, the agent response type depends on the selected evidence passages. To analyze how incorporating response types can help with response generation, we conduct a controlled experiment to generate agent responses with the dialogue context and oracle evidence passages as the input to FiD, and compare the performance when no/oracle/predicted response type is given. For examples that have two labels with different sets of evidence passages, we split them into two separate instances. To predict the response type, we use a sequence classification model based on BERT-base(Devlin et al., 2019), given the dialogue context and oracle evidence passages. To provide the oracle or predicted response type as the response generation model input, we simply append a formatted string-response type: {response type name} 14 -at the end of the dialogue context, when feeding it to FiD.Model Input \n\nDev \nTest \n\nBLEU RG-F1 BLEU RG-F1 \nDC+OEP+RT (Oracle) \n32.6 \n48.7 \n31.6 \n47.4 \n\nDC+OEP+RT (Predicted) \n32.6 \n46.3 \n31.7 \n45.4 \n\nDC+OEP \n32.0 \n45.3 \n30.6 \n44.3 \n\n\n\nTable 10 :\n10Automatic RG scores for FiD with inputs: dialogue context (DC), oracle evidence passages (OEP), and different (oracle/predicted/no) response types (RT).\nWe open-source all data and code at https://github .com/ellenmellon/INSCIT.\nhttps://stackexchange.com/.\nWe use the dump of 04/20/2022.\nWe use Amazon Mechanical Turk (https://www .mturk.com/) for data collection.\nhttps://en.wikipedia.org/wiki/Wikipedia :Contents/Categories. 6 https://en.wikipedia.org/wiki/Category :Wikipedia Top 25 Report.\nAt the beginning of our training set collection (before the collection of dev/test sets), we only assign a 0.3 USD bonus to agent annotations marked as more comprehensive. After communicating with our workers, we adjust our bonus structure, which leads to more comprehensive agent responses.10  We use wikiextractor to process Wikipedia articles: https://github.com/attardi/wikiextractor.\nCandidate response type names are ''direct answer,'' ''clarification,'' ''no answer but relevant information,'' and ''no answer and no information.''\nAcknowledgmentsThis research was supported by NSF IIS-2044660, ONR N00014-18-1-2826, DARPA MCS program (N66001-19-2-4031), a Sloan fellowship and gifts from AI2. We thank Tao Yu and all members in H2Lab and TIAL labs at University of Washington who participated in our data collection pilot studies. We thank Kevin Everson for proofreading our paper and providing valuable suggestions. We also thank members of University of Washington's NLP groups who provided feedback and insights to this work. We specifically want to thank our TACL reviewers and action editor for their detailed comments that lead to great improvement of our paper.\nTopiOCQA: Open-domain conversational question answering with topic switching. Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, 10.1162/tacl_a_00471Transactions of the Association for Computational Linguistics. 10Harm de Vries, and Siva ReddyVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. 2022. TopiOCQA: Open-domain conversa- tional question answering with topic switching. Transactions of the Association for Compu- tational Linguistics, 10:468-483. https:// doi.org/10.1162/tacl a 00471\n\nBuilding and evaluating open-domain dialogue corpora with clarifying questions. Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, Mikhail Burtsev, 10.18653/v1/2021.emnlp-main.367Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsMohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail Burtsev. 2021. Building and evaluating open-domain di- alogue corpora with clarifying questions. In Proceedings of the 2021 Conference on Empir- ical Methods in Natural Language Processing, pages 4473-4484, Online and Punta Cana, Dominican Republic. Association for Compu- tational Linguistics. https://doi.org/10 .18653/v1/2021.emnlp-main.367\n\nAsking clarifying questions in open-domain information-seeking conversations. Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, W Bruce Croft, 10.1145/3331184.3331265Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalACMMohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2019. Ask- ing clarifying questions in open-domain information-seeking conversations. In Proceed- ings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM. https://doi .org/10.1145/3331184.3331265\n\nOpen-domain question answering goes conversational via question rewriting. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, Srinivas Chappidi, 10.18653/v1/2021.naacl-main.44Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-domain ques- tion answering goes conversational via ques- tion rewriting. In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 520-534, Online. Association for Com- putational Linguistics. https://doi.org /10.18653/v1/2021.naacl-main.44\n\nDoQA -accessing domain-specific FAQs via conversational QA. Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, Eneko Agirre, 10.18653/v1/2020.acl-main.652Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, and Eneko Agirre. 2020. DoQA -accessing domain-specific FAQs via conversational QA. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7302-7314, Online. Association for Computational Lin- guistics. https://doi.org/10.18653/v1 /2020.acl-main.652\n\nQuAC: Question answering in context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-Tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, 10.18653/v1/D18-1241Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question an- swering in context. In Proceedings of the 2018 Conference on Empirical Methods in Natu- ral Language Processing, pages 2174-2184, Brussels, Belgium. Association for Compu- tational Linguistics. https://doi.org/10 .18653/v1/D18-1241\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Com- putational Linguistics.\n\nWizard of Wikipedia: Knowledgepowered conversational agents. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, International Conference on Learning Representations. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia: Knowledge- powered conversational agents. In International Conference on Learning Representations.\n\nDialDoc 2022 shared task: Open-book document-grounded dialogue modeling. Song Feng, Siva Patel, Hui Wan, Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering. the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question AnsweringSong Feng, Siva Patel, and Hui Wan. 2022. DialDoc 2022 shared task: Open-book document-grounded dialogue modeling. In Pro- ceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversa- tional Question Answering, pages 155-160,\n\nAssociation for Computational Linguistics. Ireland Dublin, 10.18653/v1/2022.dialdoc-1.18Dublin, Ireland. Association for Computa- tional Linguistics. https://doi.org/10 .18653/v1/2022.dialdoc-1.18\n\nMultiDoc2Dial: Modeling dialogues grounded in multiple documents. Song Feng, Sankalp Siva, Hui Patel, Sachindra Wan, Joshi, 10.18653/v1/2021.emnlp-main.498Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican Republic. Association for Computational LinguisticsSong Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. MultiDoc2Dial: Model- ing dialogues grounded in multiple documents. In Proceedings of the 2021 Conference on Em- pirical Methods in Natural Language Process- ing, pages 6162-6176, Online and Punta Cana, Dominican Republic. Association for Compu- tational Linguistics. https://doi.org/10 .18653/v1/2021.emnlp-main.498\n\n2020. doc2dial: A goal-oriented documentgrounded dialogue dataset. Song Feng, Hui Wan, Chulaka Gunasekara, Siva Patel, Sachindra Joshi, Luis Lastras, 10.18653/v1/2020.emnlp-main.652Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsSong Feng, Hui Wan, Chulaka Gunasekara, Siva Patel, Sachindra Joshi, and Luis Lastras. 2020. doc2dial: A goal-oriented document- grounded dialogue dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8118-8128, Online. Association for Computational Linguistics. https://doi.org /10.18653/v1/2020.emnlp-main.652\n\nUniGDD: A unified generative framework for goal-oriented document-grounded dialogue. Chang Gao, Wenxuan Zhang, Wai Lam, 10.18653/v1/2022.acl-short.66Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2Short Papers)Chang Gao, Wenxuan Zhang, and Wai Lam. 2022. UniGDD: A unified generative framework for goal-oriented document-grounded dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguis- tics (Volume 2: Short Papers), pages 599-605, Dublin, Ireland. Association for Computational Linguistics. https://doi.org/10.18653/v1 /2022.acl-short.66\n\nA knowledgegrounded neural conversation model. Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Michel Wen Tau Yih, Galley, 10.1609/aaai.v32i1.11977Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. 2018. A knowledge- grounded neural conversation model. In Pro- ceedings of the AAAI Conference on Artificial Intelligence, volume 32, pages 5110-5117. https://doi.org/10.1609/aaai.v32i1 .11977\n\nAbg-coQA: Clarifying ambiguity in conversational question answering. Meiqi Guo, Mingda Zhang, Siva Reddy, Malihe Alikhani, 3rd Conference on Automated Knowledge Base ConstructionMeiqi Guo, Mingda Zhang, Siva Reddy, and Malihe Alikhani. 2021. Abg-coQA: Clarifying ambiguity in conversational question answer- ing. In 3rd Conference on Automated Knowl- edge Base Construction.\n\n2020. spaCy: Industrial-strength natural language processing in Python. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength natural language processing in Python.\n\nLeveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, 10.18653/v1/2021.eacl-main.74Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational LinguisticsGautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answer- ing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Com- putational Linguistics. https://doi.org /10.18653/v1/2021.eacl-main.74\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question an- swering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, On- line. Association for Computational Linguis- tics. https://doi.org/10.18653/v1 /2020.emnlp-main.550\n\nNatalia Konstantinova, Constantin Orasan, 10.4018/978-1-4666-2169-5.ch007Interactive question answering. Emerging Applications of Natural Language Processing: Concepts and New Research. Natalia Konstantinova and Constantin Orasan. 2013. Interactive question answering. Emerging Applications of Natural Language Processing: Concepts and New Research, pages 149-169. https://doi.org/10.4018/978-1-4666 -2169-5.ch007\n\nDitch the gold standard: Re-evaluating conversational question answering. Huihan Li, Tianyu Gao, Manan Goenka, Danqi Chen, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandLong Papers1Association for Computational LinguisticsHuihan Li, Tianyu Gao, Manan Goenka, and Danqi Chen. 2022. Ditch the gold stan- dard: Re-evaluating conversational question answering. In Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 8074-8085, Dublin, Ireland. Associa- tion for Computational Linguistics.\n\nAmbigQA: Answering ambiguous open-domain questions. Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2020.emnlp-main.466Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsSewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: An- swering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Pro- cessing (EMNLP), pages 5783-5797, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1/2020 .emnlp-main.466\n\nTowards exploiting background knowledge for building conversation systems. Nikita Moghe, Siddhartha Arora, Suman Banerjee, Mitesh M Khapra, 10.18653/v1/D18-1255Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsNikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. 2018.Towards ex- ploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 2322-2332, Brussels, Belgium. Association for Computa- tional Linguistics. https://doi.org/10 .18653/v1/D18-1255\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, arXiv:2112.09332Benjamin Chess, and John Schulman. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprintReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.\n\nIntroducing mantis: A novel multi-domain information seeking dialogues dataset. Gustavo Penha, Alexandru Balan, Claudia Hauff, arXiv:1912.04639arXiv preprintGustavo Penha, Alexandru Balan, and Claudia Hauff. 2019. Introducing mantis: A novel multi-domain information seeking dialogues dataset. arXiv preprint arXiv:1912.04639.\n\nA call for clarity in reporting BLEU scores. Matt Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBrussels, BelgiumAssociation for Computational LinguisticsMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Brussels, Belgium. Association for Computational Linguistics. https://doi.org/10.18653/v1/W18 -6319\n\nCoQA: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 10.1162/tacl_a_00266Transactions of the Association for Computational Linguistics. 7Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266. https://doi.org/10.1162 /tacl_a_00266\n\nDoes this list contain what you were searching for? Learning adaptive dialogue strategies for interactive question answering. V Rieser, O Lemon, 10.1017/S1351324908004907Natural Language Engineering. 151V. Rieser and O. Lemon. 2009. Does this list contain what you were searching for? Learn- ing adaptive dialogue strategies for interactive question answering. Natural Language Engi- neering, 15(1):55-72. https://doi.org/10 .1017/S1351324908004907\n\nThe probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, 10.1561/1500000019Foundations and Trends in Information Retrieval. 34Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in In- formation Retrieval, 3(4):333-389. https:// doi.org/10.1561/1500000019\n\nInformation seeking in the spirit of learning: A dataset for conversational curiosity. Pedro Rodriguez, Paul Crook, Seungwhan Moon, Zhiguang Wang, 10.18653/v1/2020.emnlp-main.655Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsPedro Rodriguez, Paul Crook, Seungwhan Moon, and Zhiguang Wang. 2020. Information seeking in the spirit of learning: A dataset for conversational curiosity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8153-8172, Online. Association for Computational Linguistics. https://doi.org /10.18653/v1/2020.emnlp-main.655\n\nInterpretation of natural language rules in conversational machine reading. Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel, 10.18653/v1/D18-1233Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsMarzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation of natural language rules in conversational machine reading. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2087-2097, Brussels, Belgium. Associa- tion for Computational Linguistics. https:// doi.org/10.18653/v1/D18-1233\n\nConditionalQA: A complex reading comprehension dataset with conditional answers. Haitian Sun, William Cohen, Ruslan Salakhutdinov, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1Haitian Sun, William Cohen, and Ruslan Salakhutdinov. 2022. ConditionalQA: A com- plex reading comprehension dataset with con- ditional answers. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 3627-3637, Dublin, Ireland. Association for Computational Linguistics.\n\nSpecial issue on interactive question answering: Introduction. N Webb, B Webber, 10.1017/S1351324908004877Natural Language Engineering. 151N. Webb and B. Webber. 2009. Special is- sue on interactive question answering: In- troduction. Natural Language Engineering, 15(1):1-8. https://doi.org/10.1017 /S1351324908004877\n\nA network-based end-to-end trainable task-oriented dialogue system. David Tsung-Hsien Wen, Nikola Vandyke, Milica Mrk\u0161i\u0107, Lina M Ga\u0161i\u0107, Pei-Hao Rojas-Barahona, Stefan Su, Steve Ultes, Young, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. the 15th Conference of the European Chapter of the Association for Computational LinguisticsValencia, SpainAssociation for Computational Linguistics1Tsung-Hsien Wen, David Vandyke, Nikola Mrk\u0161i\u0107, Milica Ga\u0161i\u0107, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network-based end-to-end trainable task-oriented dialogue system. In Proceed- ings of the 15th Conference of the European Chapter of the Association for Computa- tional Linguistics: Volume 1, Long Papers, pages 438-449, Valencia, Spain. Association for Computational Linguistics.\n\nDIALKI: Knowledge identification in conversational systems through dialogue-document contextualization. Zeqiu Wu, Bo-Ru Lu, Hannaneh Hajishirzi, Mari Ostendorf, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsZeqiu Wu, Bo-Ru Lu, Hannaneh Hajishirzi, and Mari Ostendorf. 2021. DIALKI: Knowl- edge identification in conversational systems through dialogue-document contextualization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro- cessing, pages 1852-1863, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nAnserini: Enabling the use of lucene for information retrieval research. Peilin Yang, Hui Fang, Jimmy Lin, 10.1145/3077136.3080721Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17. the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17New York, NY, USAAssociation for Computing MachineryPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of lucene for in- formation retrieval research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17, pages 1253-1256, New York, NY, USA. Association for Computing Machinery. https://doi.org/10.1145 /3077136.3080721\n\nSituated-QA: Incorporating extra-linguistic contexts into QA. Michael Zhang, Eunsol Choi, 10.18653/v1/2021.emnlp-main.586Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsMichael Zhang and Eunsol Choi. 2021. Situated- QA: Incorporating extra-linguistic contexts into QA. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7371-7387, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. https://doi .org/10.18653/v1/2021.emnlp-main.586\n\nA dataset for document grounded conversations. Kangyan Zhou, Shrimai Prabhumoye, Alan W Black, 10.18653/v1/D18-1076Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsKangyan Zhou, Shrimai Prabhumoye, and Alan W. Black. 2018. A dataset for document grounded conversations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 708-713, Brussels, Belgium. Association for Computa- tional Linguistics. https://doi.org/10 .18653/v1/D18-1076\n", "annotations": {"author": "[{\"end\":105,\"start\":80},{\"end\":132,\"start\":106},{\"end\":167,\"start\":133},{\"end\":189,\"start\":168},{\"end\":235,\"start\":190},{\"end\":248,\"start\":236},{\"end\":280,\"start\":249},{\"end\":345,\"start\":281},{\"end\":376,\"start\":346}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":86},{\"end\":116,\"start\":110},{\"end\":142,\"start\":137},{\"end\":175,\"start\":170},{\"end\":206,\"start\":196},{\"end\":263,\"start\":254},{\"end\":300,\"start\":290}]", "author_first_name": "[{\"end\":85,\"start\":80},{\"end\":109,\"start\":106},{\"end\":136,\"start\":133},{\"end\":169,\"start\":168},{\"end\":193,\"start\":190},{\"end\":195,\"start\":194},{\"end\":247,\"start\":236},{\"end\":253,\"start\":249},{\"end\":289,\"start\":281}]", "author_affiliation": "[{\"end\":166,\"start\":144},{\"end\":234,\"start\":208},{\"end\":344,\"start\":318},{\"end\":375,\"start\":347}]", "title": "[{\"end\":77,\"start\":1},{\"end\":453,\"start\":377}]", "venue": null, "abstract": "[{\"end\":1528,\"start\":467}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1665,\"start\":1646},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1686,\"start\":1665},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1706,\"start\":1686},{\"end\":1723,\"start\":1706},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2032,\"start\":2009},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2055,\"start\":2032},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2086,\"start\":2055},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3461,\"start\":3442},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3480,\"start\":3461},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3501,\"start\":3480},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3660,\"start\":3639},{\"end\":3678,\"start\":3660},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3703,\"start\":3678},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3720,\"start\":3703},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5217,\"start\":5193},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5241,\"start\":5217},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5303,\"start\":5286},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6400,\"start\":6381},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6430,\"start\":6410},{\"end\":6534,\"start\":6508},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6564,\"start\":6542},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6601,\"start\":6579},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6989,\"start\":6968},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7006,\"start\":6989},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7264,\"start\":7238},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7332,\"start\":7306},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7538,\"start\":7517},{\"end\":7568,\"start\":7549},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7607,\"start\":7588},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7747,\"start\":7730},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8025,\"start\":8006},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8507,\"start\":8479},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8526,\"start\":8507},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8544,\"start\":8526},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8563,\"start\":8544},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8694,\"start\":8671},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9296,\"start\":9274},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9317,\"start\":9296},{\"end\":9980,\"start\":9979},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12219,\"start\":12202},{\"end\":16187,\"start\":16186},{\"end\":18862,\"start\":18839},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23557,\"start\":23533},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23581,\"start\":23557},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23602,\"start\":23581},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24069,\"start\":24039},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24193,\"start\":24174},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24241,\"start\":24218},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24280,\"start\":24259},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24700,\"start\":24678},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24892,\"start\":24867},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24921,\"start\":24904},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25291,\"start\":25270},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25811,\"start\":25790},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26047,\"start\":26030},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28305,\"start\":28293},{\"end\":28435,\"start\":28416},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28456,\"start\":28435},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29485,\"start\":29466},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29506,\"start\":29485},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29527,\"start\":29506},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29649,\"start\":29631},{\"end\":29822,\"start\":29801},{\"end\":30689,\"start\":30687},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33558,\"start\":33535},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33584,\"start\":33563},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38426,\"start\":38410},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47057,\"start\":47036}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40069,\"start\":39843},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40410,\"start\":40070},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40616,\"start\":40411},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40826,\"start\":40617},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41096,\"start\":40827},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41138,\"start\":41097},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42546,\"start\":41139},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42835,\"start\":42547},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43000,\"start\":42836},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44525,\"start\":43001},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44615,\"start\":44526},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44971,\"start\":44616},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":45025,\"start\":44972},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":45070,\"start\":45026},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":45297,\"start\":45071},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":45622,\"start\":45298},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":47515,\"start\":45623},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":47682,\"start\":47516}]", "paragraph": "[{\"end\":2573,\"start\":1544},{\"end\":3829,\"start\":2575},{\"end\":4627,\"start\":3831},{\"end\":5095,\"start\":4629},{\"end\":5829,\"start\":5097},{\"end\":6884,\"start\":6089},{\"end\":8307,\"start\":6886},{\"end\":8849,\"start\":8309},{\"end\":9532,\"start\":8871},{\"end\":9795,\"start\":9534},{\"end\":10775,\"start\":9836},{\"end\":11249,\"start\":10777},{\"end\":11880,\"start\":11270},{\"end\":13083,\"start\":11909},{\"end\":13675,\"start\":13085},{\"end\":14332,\"start\":13677},{\"end\":15270,\"start\":14334},{\"end\":16229,\"start\":15272},{\"end\":16329,\"start\":16231},{\"end\":16897,\"start\":16349},{\"end\":17434,\"start\":16899},{\"end\":17671,\"start\":17436},{\"end\":18519,\"start\":17673},{\"end\":19183,\"start\":18537},{\"end\":19492,\"start\":19185},{\"end\":20466,\"start\":19530},{\"end\":21118,\"start\":20494},{\"end\":21328,\"start\":21152},{\"end\":21958,\"start\":21330},{\"end\":22425,\"start\":21960},{\"end\":23302,\"start\":22457},{\"end\":23964,\"start\":23333},{\"end\":24701,\"start\":23985},{\"end\":24922,\"start\":24719},{\"end\":25562,\"start\":24924},{\"end\":25920,\"start\":25564},{\"end\":26946,\"start\":25937},{\"end\":27371,\"start\":26948},{\"end\":27706,\"start\":27373},{\"end\":27868,\"start\":27721},{\"end\":28205,\"start\":27870},{\"end\":28522,\"start\":28207},{\"end\":29823,\"start\":28524},{\"end\":30410,\"start\":29825},{\"end\":30778,\"start\":30412},{\"end\":31322,\"start\":30780},{\"end\":33400,\"start\":31324},{\"end\":34561,\"start\":33436},{\"end\":36189,\"start\":34563},{\"end\":36644,\"start\":36240},{\"end\":37446,\"start\":36646},{\"end\":37847,\"start\":37448},{\"end\":38178,\"start\":37849},{\"end\":38691,\"start\":38180},{\"end\":39842,\"start\":38741}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6073,\"start\":5830},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9835,\"start\":9796}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6277,\"start\":6270},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18730,\"start\":18723},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20406,\"start\":20399},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21636,\"start\":21629},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22167,\"start\":22160},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":22335,\"start\":22328},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22423,\"start\":22416},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23014,\"start\":23007},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":31352,\"start\":31345},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":32275,\"start\":32268},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":33045,\"start\":33038},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":33461,\"start\":33454},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":34078,\"start\":34071},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35802,\"start\":35794}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1542,\"start\":1530},{\"attributes\":{\"n\":\"2\"},\"end\":6087,\"start\":6075},{\"attributes\":{\"n\":\"3\"},\"end\":8869,\"start\":8852},{\"attributes\":{\"n\":\"4\"},\"end\":11268,\"start\":11252},{\"attributes\":{\"n\":\"4.1\"},\"end\":11907,\"start\":11883},{\"attributes\":{\"n\":\"4.2\"},\"end\":16347,\"start\":16332},{\"attributes\":{\"n\":\"4.3\"},\"end\":18535,\"start\":18522},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":19528,\"start\":19495},{\"end\":20492,\"start\":20469},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":21150,\"start\":21121},{\"end\":22455,\"start\":22428},{\"attributes\":{\"n\":\"5\"},\"end\":23321,\"start\":23305},{\"attributes\":{\"n\":\"5.1\"},\"end\":23331,\"start\":23324},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":23983,\"start\":23967},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":24717,\"start\":24704},{\"end\":25935,\"start\":25923},{\"attributes\":{\"n\":\"5.2\"},\"end\":27719,\"start\":27709},{\"attributes\":{\"n\":\"6.1\"},\"end\":33423,\"start\":33403},{\"attributes\":{\"n\":\"6.2\"},\"end\":33434,\"start\":33426},{\"end\":36211,\"start\":36192},{\"attributes\":{\"n\":\"7\"},\"end\":36238,\"start\":36214},{\"attributes\":{\"n\":\"8\"},\"end\":38739,\"start\":38694},{\"end\":39854,\"start\":39844},{\"end\":40081,\"start\":40071},{\"end\":40422,\"start\":40412},{\"end\":40627,\"start\":40618},{\"end\":41107,\"start\":41098},{\"end\":41149,\"start\":41140},{\"end\":42557,\"start\":42548},{\"end\":42846,\"start\":42837},{\"end\":44536,\"start\":44527},{\"end\":44982,\"start\":44973},{\"end\":45036,\"start\":45027},{\"end\":45081,\"start\":45072},{\"end\":45308,\"start\":45299},{\"end\":47527,\"start\":47517}]", "table": "[{\"end\":40826,\"start\":40629},{\"end\":41096,\"start\":40849},{\"end\":42546,\"start\":41698},{\"end\":42835,\"start\":42712},{\"end\":44525,\"start\":44025},{\"end\":44971,\"start\":44734},{\"end\":45297,\"start\":45083},{\"end\":45622,\"start\":45317},{\"end\":47515,\"start\":47340}]", "figure_caption": "[{\"end\":40069,\"start\":39856},{\"end\":40410,\"start\":40083},{\"end\":40616,\"start\":40424},{\"end\":40849,\"start\":40829},{\"end\":41138,\"start\":41109},{\"end\":41698,\"start\":41151},{\"end\":42712,\"start\":42559},{\"end\":43000,\"start\":42848},{\"end\":44025,\"start\":43003},{\"end\":44615,\"start\":44538},{\"end\":44734,\"start\":44618},{\"end\":45025,\"start\":44984},{\"end\":45070,\"start\":45038},{\"end\":45317,\"start\":45310},{\"end\":47340,\"start\":45625},{\"end\":47682,\"start\":47530}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2131,\"start\":2123},{\"end\":11660,\"start\":11652},{\"end\":12482,\"start\":12474},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13567,\"start\":13559},{\"end\":13842,\"start\":13834},{\"end\":14441,\"start\":14432},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19701,\"start\":19693},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32481,\"start\":32473},{\"end\":34638,\"start\":34630}]", "bib_author_first_name": "[{\"end\":49286,\"start\":49279},{\"end\":49304,\"start\":49296},{\"end\":49323,\"start\":49317},{\"end\":49822,\"start\":49814},{\"end\":49841,\"start\":49836},{\"end\":49861,\"start\":49852},{\"end\":49875,\"start\":49871},{\"end\":49891,\"start\":49884},{\"end\":50686,\"start\":50678},{\"end\":50705,\"start\":50700},{\"end\":50719,\"start\":50714},{\"end\":50731,\"start\":50730},{\"end\":50737,\"start\":50732},{\"end\":51386,\"start\":51378},{\"end\":51404,\"start\":51396},{\"end\":51424,\"start\":51416},{\"end\":51435,\"start\":51429},{\"end\":51452,\"start\":51445},{\"end\":51469,\"start\":51461},{\"end\":52350,\"start\":52347},{\"end\":52372,\"start\":52365},{\"end\":52385,\"start\":52380},{\"end\":52396,\"start\":52393},{\"end\":52408,\"start\":52404},{\"end\":52425,\"start\":52420},{\"end\":53034,\"start\":53028},{\"end\":53043,\"start\":53041},{\"end\":53053,\"start\":53048},{\"end\":53065,\"start\":53061},{\"end\":53082,\"start\":53075},{\"end\":53093,\"start\":53088},{\"end\":53105,\"start\":53100},{\"end\":53117,\"start\":53113},{\"end\":53820,\"start\":53815},{\"end\":53837,\"start\":53829},{\"end\":53851,\"start\":53845},{\"end\":53865,\"start\":53857},{\"end\":54702,\"start\":54697},{\"end\":54717,\"start\":54710},{\"end\":54730,\"start\":54726},{\"end\":54746,\"start\":54740},{\"end\":54759,\"start\":54752},{\"end\":54771,\"start\":54766},{\"end\":55125,\"start\":55121},{\"end\":55136,\"start\":55132},{\"end\":55147,\"start\":55144},{\"end\":55660,\"start\":55653},{\"end\":55878,\"start\":55874},{\"end\":55892,\"start\":55885},{\"end\":55902,\"start\":55899},{\"end\":55919,\"start\":55910},{\"end\":56662,\"start\":56658},{\"end\":56672,\"start\":56669},{\"end\":56685,\"start\":56678},{\"end\":56702,\"start\":56698},{\"end\":56719,\"start\":56710},{\"end\":56731,\"start\":56727},{\"end\":57459,\"start\":57454},{\"end\":57472,\"start\":57465},{\"end\":57483,\"start\":57480},{\"end\":58174,\"start\":58168},{\"end\":58195,\"start\":58190},{\"end\":58214,\"start\":58206},{\"end\":58226,\"start\":58222},{\"end\":58242,\"start\":58234},{\"end\":58254,\"start\":58248},{\"end\":58791,\"start\":58786},{\"end\":58803,\"start\":58797},{\"end\":58815,\"start\":58811},{\"end\":58829,\"start\":58823},{\"end\":59172,\"start\":59165},{\"end\":59187,\"start\":59183},{\"end\":59202,\"start\":59197},{\"end\":59225,\"start\":59218},{\"end\":59471,\"start\":59464},{\"end\":59488,\"start\":59481},{\"end\":60241,\"start\":60233},{\"end\":60259,\"start\":60253},{\"end\":60271,\"start\":60266},{\"end\":60284,\"start\":60277},{\"end\":60298,\"start\":60292},{\"end\":60309,\"start\":60303},{\"end\":60323,\"start\":60318},{\"end\":60337,\"start\":60330},{\"end\":61008,\"start\":61001},{\"end\":61034,\"start\":61024},{\"end\":61496,\"start\":61490},{\"end\":61507,\"start\":61501},{\"end\":61518,\"start\":61513},{\"end\":61532,\"start\":61527},{\"end\":62158,\"start\":62153},{\"end\":62170,\"start\":62164},{\"end\":62188,\"start\":62180},{\"end\":62205,\"start\":62201},{\"end\":62906,\"start\":62900},{\"end\":62924,\"start\":62914},{\"end\":62937,\"start\":62932},{\"end\":62954,\"start\":62948},{\"end\":62956,\"start\":62955},{\"end\":63576,\"start\":63567},{\"end\":63590,\"start\":63585},{\"end\":63605,\"start\":63599},{\"end\":63618,\"start\":63614},{\"end\":63627,\"start\":63623},{\"end\":63645,\"start\":63636},{\"end\":63662,\"start\":63651},{\"end\":63678,\"start\":63670},{\"end\":63691,\"start\":63685},{\"end\":63709,\"start\":63702},{\"end\":63722,\"start\":63720},{\"end\":63734,\"start\":63730},{\"end\":63746,\"start\":63742},{\"end\":63765,\"start\":63757},{\"end\":63780,\"start\":63775},{\"end\":63796,\"start\":63789},{\"end\":64406,\"start\":64399},{\"end\":64423,\"start\":64414},{\"end\":64438,\"start\":64431},{\"end\":64696,\"start\":64692},{\"end\":65233,\"start\":65229},{\"end\":65246,\"start\":65241},{\"end\":65264,\"start\":65253},{\"end\":65266,\"start\":65265},{\"end\":65711,\"start\":65710},{\"end\":65721,\"start\":65720},{\"end\":66097,\"start\":66090},{\"end\":66113,\"start\":66109},{\"end\":66486,\"start\":66481},{\"end\":66502,\"start\":66498},{\"end\":66519,\"start\":66510},{\"end\":66534,\"start\":66526},{\"end\":67253,\"start\":67246},{\"end\":67265,\"start\":67262},{\"end\":67282,\"start\":67275},{\"end\":67296,\"start\":67290},{\"end\":67307,\"start\":67304},{\"end\":67325,\"start\":67321},{\"end\":67344,\"start\":67335},{\"end\":67364,\"start\":67355},{\"end\":68124,\"start\":68117},{\"end\":68137,\"start\":68130},{\"end\":68151,\"start\":68145},{\"end\":68790,\"start\":68789},{\"end\":68798,\"start\":68797},{\"end\":69119,\"start\":69114},{\"end\":69143,\"start\":69137},{\"end\":69159,\"start\":69153},{\"end\":69172,\"start\":69168},{\"end\":69174,\"start\":69173},{\"end\":69189,\"start\":69182},{\"end\":69212,\"start\":69206},{\"end\":69222,\"start\":69217},{\"end\":70021,\"start\":70016},{\"end\":70031,\"start\":70026},{\"end\":70044,\"start\":70036},{\"end\":70061,\"start\":70057},{\"end\":70763,\"start\":70757},{\"end\":70773,\"start\":70770},{\"end\":70785,\"start\":70780},{\"end\":71527,\"start\":71520},{\"end\":71541,\"start\":71535},{\"end\":72218,\"start\":72211},{\"end\":72232,\"start\":72225},{\"end\":72249,\"start\":72245},{\"end\":72251,\"start\":72250}]", "bib_author_last_name": "[{\"end\":49294,\"start\":49287},{\"end\":49315,\"start\":49305},{\"end\":49331,\"start\":49324},{\"end\":49834,\"start\":49823},{\"end\":49850,\"start\":49842},{\"end\":49869,\"start\":49862},{\"end\":49882,\"start\":49876},{\"end\":49899,\"start\":49892},{\"end\":50698,\"start\":50687},{\"end\":50712,\"start\":50706},{\"end\":50728,\"start\":50720},{\"end\":50743,\"start\":50738},{\"end\":51394,\"start\":51387},{\"end\":51414,\"start\":51405},{\"end\":51427,\"start\":51425},{\"end\":51443,\"start\":51436},{\"end\":51459,\"start\":51453},{\"end\":51478,\"start\":51470},{\"end\":52363,\"start\":52351},{\"end\":52378,\"start\":52373},{\"end\":52391,\"start\":52386},{\"end\":52402,\"start\":52397},{\"end\":52418,\"start\":52409},{\"end\":52432,\"start\":52426},{\"end\":53039,\"start\":53035},{\"end\":53046,\"start\":53044},{\"end\":53059,\"start\":53054},{\"end\":53073,\"start\":53066},{\"end\":53086,\"start\":53083},{\"end\":53098,\"start\":53094},{\"end\":53111,\"start\":53106},{\"end\":53129,\"start\":53118},{\"end\":53827,\"start\":53821},{\"end\":53843,\"start\":53838},{\"end\":53855,\"start\":53852},{\"end\":53875,\"start\":53866},{\"end\":54708,\"start\":54703},{\"end\":54724,\"start\":54718},{\"end\":54738,\"start\":54731},{\"end\":54750,\"start\":54747},{\"end\":54764,\"start\":54760},{\"end\":54778,\"start\":54772},{\"end\":55130,\"start\":55126},{\"end\":55142,\"start\":55137},{\"end\":55151,\"start\":55148},{\"end\":55667,\"start\":55661},{\"end\":55883,\"start\":55879},{\"end\":55897,\"start\":55893},{\"end\":55908,\"start\":55903},{\"end\":55923,\"start\":55920},{\"end\":55930,\"start\":55925},{\"end\":56667,\"start\":56663},{\"end\":56676,\"start\":56673},{\"end\":56696,\"start\":56686},{\"end\":56708,\"start\":56703},{\"end\":56725,\"start\":56720},{\"end\":56739,\"start\":56732},{\"end\":57463,\"start\":57460},{\"end\":57478,\"start\":57473},{\"end\":57487,\"start\":57484},{\"end\":58188,\"start\":58175},{\"end\":58204,\"start\":58196},{\"end\":58220,\"start\":58215},{\"end\":58232,\"start\":58227},{\"end\":58246,\"start\":58243},{\"end\":58266,\"start\":58255},{\"end\":58274,\"start\":58268},{\"end\":58795,\"start\":58792},{\"end\":58809,\"start\":58804},{\"end\":58821,\"start\":58816},{\"end\":58838,\"start\":58830},{\"end\":59181,\"start\":59173},{\"end\":59195,\"start\":59188},{\"end\":59216,\"start\":59203},{\"end\":59230,\"start\":59226},{\"end\":59479,\"start\":59472},{\"end\":59494,\"start\":59489},{\"end\":60251,\"start\":60242},{\"end\":60264,\"start\":60260},{\"end\":60275,\"start\":60272},{\"end\":60290,\"start\":60285},{\"end\":60301,\"start\":60299},{\"end\":60316,\"start\":60310},{\"end\":60328,\"start\":60324},{\"end\":60341,\"start\":60338},{\"end\":61022,\"start\":61009},{\"end\":61041,\"start\":61035},{\"end\":61499,\"start\":61497},{\"end\":61511,\"start\":61508},{\"end\":61525,\"start\":61519},{\"end\":61537,\"start\":61533},{\"end\":62162,\"start\":62159},{\"end\":62178,\"start\":62171},{\"end\":62199,\"start\":62189},{\"end\":62217,\"start\":62206},{\"end\":62912,\"start\":62907},{\"end\":62930,\"start\":62925},{\"end\":62946,\"start\":62938},{\"end\":62963,\"start\":62957},{\"end\":63583,\"start\":63577},{\"end\":63597,\"start\":63591},{\"end\":63612,\"start\":63606},{\"end\":63621,\"start\":63619},{\"end\":63634,\"start\":63628},{\"end\":63649,\"start\":63646},{\"end\":63668,\"start\":63663},{\"end\":63683,\"start\":63679},{\"end\":63700,\"start\":63692},{\"end\":63718,\"start\":63710},{\"end\":63728,\"start\":63723},{\"end\":63740,\"start\":63735},{\"end\":63755,\"start\":63747},{\"end\":63773,\"start\":63766},{\"end\":63787,\"start\":63781},{\"end\":63803,\"start\":63797},{\"end\":64412,\"start\":64407},{\"end\":64429,\"start\":64424},{\"end\":64444,\"start\":64439},{\"end\":64701,\"start\":64697},{\"end\":65239,\"start\":65234},{\"end\":65251,\"start\":65247},{\"end\":65274,\"start\":65267},{\"end\":65718,\"start\":65712},{\"end\":65727,\"start\":65722},{\"end\":66107,\"start\":66098},{\"end\":66122,\"start\":66114},{\"end\":66496,\"start\":66487},{\"end\":66508,\"start\":66503},{\"end\":66524,\"start\":66520},{\"end\":66539,\"start\":66535},{\"end\":67260,\"start\":67254},{\"end\":67273,\"start\":67266},{\"end\":67288,\"start\":67283},{\"end\":67302,\"start\":67297},{\"end\":67319,\"start\":67308},{\"end\":67333,\"start\":67326},{\"end\":67353,\"start\":67345},{\"end\":67371,\"start\":67365},{\"end\":68128,\"start\":68125},{\"end\":68143,\"start\":68138},{\"end\":68165,\"start\":68152},{\"end\":68795,\"start\":68791},{\"end\":68805,\"start\":68799},{\"end\":69135,\"start\":69120},{\"end\":69151,\"start\":69144},{\"end\":69166,\"start\":69160},{\"end\":69180,\"start\":69175},{\"end\":69204,\"start\":69190},{\"end\":69215,\"start\":69213},{\"end\":69228,\"start\":69223},{\"end\":69235,\"start\":69230},{\"end\":70024,\"start\":70022},{\"end\":70034,\"start\":70032},{\"end\":70055,\"start\":70045},{\"end\":70071,\"start\":70062},{\"end\":70768,\"start\":70764},{\"end\":70778,\"start\":70774},{\"end\":70789,\"start\":70786},{\"end\":71533,\"start\":71528},{\"end\":71546,\"start\":71542},{\"end\":72223,\"start\":72219},{\"end\":72243,\"start\":72233},{\"end\":72257,\"start\":72252}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1162/tacl_a_00471\",\"id\":\"b0\",\"matched_paper_id\":238259221},\"end\":49732,\"start\":49201},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.367\",\"id\":\"b1\",\"matched_paper_id\":237492197},\"end\":50598,\"start\":49734},{\"attributes\":{\"doi\":\"10.1145/3331184.3331265\",\"id\":\"b2\",\"matched_paper_id\":196623463},\"end\":51301,\"start\":50600},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.44\",\"id\":\"b3\",\"matched_paper_id\":222290679},\"end\":52285,\"start\":51303},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.652\",\"id\":\"b4\",\"matched_paper_id\":218487043},\"end\":52989,\"start\":52287},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1241\",\"id\":\"b5\",\"matched_paper_id\":52057510},\"end\":53731,\"start\":52991},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":54634,\"start\":53733},{\"attributes\":{\"id\":\"b7\"},\"end\":55046,\"start\":54636},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":248779870},\"end\":55608,\"start\":55048},{\"attributes\":{\"doi\":\"10.18653/v1/2022.dialdoc-1.18\",\"id\":\"b9\"},\"end\":55806,\"start\":55610},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.498\",\"id\":\"b10\",\"matched_paper_id\":237940558},\"end\":56589,\"start\":55808},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.652\",\"id\":\"b11\"},\"end\":57367,\"start\":56591},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-short.66\",\"id\":\"b12\",\"matched_paper_id\":248227572},\"end\":58119,\"start\":57369},{\"attributes\":{\"doi\":\"10.1609/aaai.v32i1.11977\",\"id\":\"b13\",\"matched_paper_id\":15442925},\"end\":58715,\"start\":58121},{\"attributes\":{\"id\":\"b14\"},\"end\":59091,\"start\":58717},{\"attributes\":{\"id\":\"b15\"},\"end\":59374,\"start\":59093},{\"attributes\":{\"doi\":\"10.18653/v1/2021.eacl-main.74\",\"id\":\"b16\",\"matched_paper_id\":220302360},\"end\":60171,\"start\":59376},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.550\",\"id\":\"b17\",\"matched_paper_id\":215737187},\"end\":60999,\"start\":60173},{\"attributes\":{\"doi\":\"10.4018/978-1-4666-2169-5.ch007\",\"id\":\"b18\"},\"end\":61414,\"start\":61001},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":245218415},\"end\":62099,\"start\":61416},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.466\",\"id\":\"b20\",\"matched_paper_id\":216056269},\"end\":62823,\"start\":62101},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1255\",\"id\":\"b21\",\"matched_paper_id\":52333947},\"end\":63565,\"start\":62825},{\"attributes\":{\"doi\":\"arXiv:2112.09332\",\"id\":\"b22\"},\"end\":64317,\"start\":63567},{\"attributes\":{\"doi\":\"arXiv:1912.04639\",\"id\":\"b23\"},\"end\":64645,\"start\":64319},{\"attributes\":{\"doi\":\"10.18653/v1/W18-6319\",\"id\":\"b24\",\"matched_paper_id\":13751870},\"end\":65174,\"start\":64647},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00266\",\"id\":\"b25\",\"matched_paper_id\":52055325},\"end\":65582,\"start\":65176},{\"attributes\":{\"doi\":\"10.1017/S1351324908004907\",\"id\":\"b26\",\"matched_paper_id\":16973269},\"end\":66032,\"start\":65584},{\"attributes\":{\"doi\":\"10.1561/1500000019\",\"id\":\"b27\",\"matched_paper_id\":207178704},\"end\":66392,\"start\":66034},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.655\",\"id\":\"b28\",\"matched_paper_id\":218470317},\"end\":67168,\"start\":66394},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1233\",\"id\":\"b29\",\"matched_paper_id\":52165754},\"end\":68034,\"start\":67170},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":238744031},\"end\":68724,\"start\":68036},{\"attributes\":{\"doi\":\"10.1017/S1351324908004877\",\"id\":\"b31\",\"matched_paper_id\":17171125},\"end\":69044,\"start\":68726},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10565222},\"end\":69910,\"start\":69046},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":237485380},\"end\":70682,\"start\":69912},{\"attributes\":{\"doi\":\"10.1145/3077136.3080721\",\"id\":\"b34\",\"matched_paper_id\":1340183},\"end\":71456,\"start\":70684},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.586\",\"id\":\"b35\",\"matched_paper_id\":237491751},\"end\":72162,\"start\":71458},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1076\",\"id\":\"b36\",\"matched_paper_id\":52307098},\"end\":72809,\"start\":72164}]", "bib_title": "[{\"end\":49277,\"start\":49201},{\"end\":49812,\"start\":49734},{\"end\":50676,\"start\":50600},{\"end\":51376,\"start\":51303},{\"end\":52345,\"start\":52287},{\"end\":53026,\"start\":52991},{\"end\":53813,\"start\":53733},{\"end\":54695,\"start\":54636},{\"end\":55119,\"start\":55048},{\"end\":55872,\"start\":55808},{\"end\":56656,\"start\":56591},{\"end\":57452,\"start\":57369},{\"end\":58166,\"start\":58121},{\"end\":59462,\"start\":59376},{\"end\":60231,\"start\":60173},{\"end\":61488,\"start\":61416},{\"end\":62151,\"start\":62101},{\"end\":62898,\"start\":62825},{\"end\":64690,\"start\":64647},{\"end\":65227,\"start\":65176},{\"end\":65708,\"start\":65584},{\"end\":66088,\"start\":66034},{\"end\":66479,\"start\":66394},{\"end\":67244,\"start\":67170},{\"end\":68115,\"start\":68036},{\"end\":68787,\"start\":68726},{\"end\":69112,\"start\":69046},{\"end\":70014,\"start\":69912},{\"end\":70755,\"start\":70684},{\"end\":71518,\"start\":71458},{\"end\":72209,\"start\":72164}]", "bib_author": "[{\"end\":49296,\"start\":49279},{\"end\":49317,\"start\":49296},{\"end\":49333,\"start\":49317},{\"end\":49836,\"start\":49814},{\"end\":49852,\"start\":49836},{\"end\":49871,\"start\":49852},{\"end\":49884,\"start\":49871},{\"end\":49901,\"start\":49884},{\"end\":50700,\"start\":50678},{\"end\":50714,\"start\":50700},{\"end\":50730,\"start\":50714},{\"end\":50745,\"start\":50730},{\"end\":51396,\"start\":51378},{\"end\":51416,\"start\":51396},{\"end\":51429,\"start\":51416},{\"end\":51445,\"start\":51429},{\"end\":51461,\"start\":51445},{\"end\":51480,\"start\":51461},{\"end\":52365,\"start\":52347},{\"end\":52380,\"start\":52365},{\"end\":52393,\"start\":52380},{\"end\":52404,\"start\":52393},{\"end\":52420,\"start\":52404},{\"end\":52434,\"start\":52420},{\"end\":53041,\"start\":53028},{\"end\":53048,\"start\":53041},{\"end\":53061,\"start\":53048},{\"end\":53075,\"start\":53061},{\"end\":53088,\"start\":53075},{\"end\":53100,\"start\":53088},{\"end\":53113,\"start\":53100},{\"end\":53131,\"start\":53113},{\"end\":53829,\"start\":53815},{\"end\":53845,\"start\":53829},{\"end\":53857,\"start\":53845},{\"end\":53877,\"start\":53857},{\"end\":54710,\"start\":54697},{\"end\":54726,\"start\":54710},{\"end\":54740,\"start\":54726},{\"end\":54752,\"start\":54740},{\"end\":54766,\"start\":54752},{\"end\":54780,\"start\":54766},{\"end\":55132,\"start\":55121},{\"end\":55144,\"start\":55132},{\"end\":55153,\"start\":55144},{\"end\":55669,\"start\":55653},{\"end\":55885,\"start\":55874},{\"end\":55899,\"start\":55885},{\"end\":55910,\"start\":55899},{\"end\":55925,\"start\":55910},{\"end\":55932,\"start\":55925},{\"end\":56669,\"start\":56658},{\"end\":56678,\"start\":56669},{\"end\":56698,\"start\":56678},{\"end\":56710,\"start\":56698},{\"end\":56727,\"start\":56710},{\"end\":56741,\"start\":56727},{\"end\":57465,\"start\":57454},{\"end\":57480,\"start\":57465},{\"end\":57489,\"start\":57480},{\"end\":58190,\"start\":58168},{\"end\":58206,\"start\":58190},{\"end\":58222,\"start\":58206},{\"end\":58234,\"start\":58222},{\"end\":58248,\"start\":58234},{\"end\":58268,\"start\":58248},{\"end\":58276,\"start\":58268},{\"end\":58797,\"start\":58786},{\"end\":58811,\"start\":58797},{\"end\":58823,\"start\":58811},{\"end\":58840,\"start\":58823},{\"end\":59183,\"start\":59165},{\"end\":59197,\"start\":59183},{\"end\":59218,\"start\":59197},{\"end\":59232,\"start\":59218},{\"end\":59481,\"start\":59464},{\"end\":59496,\"start\":59481},{\"end\":60253,\"start\":60233},{\"end\":60266,\"start\":60253},{\"end\":60277,\"start\":60266},{\"end\":60292,\"start\":60277},{\"end\":60303,\"start\":60292},{\"end\":60318,\"start\":60303},{\"end\":60330,\"start\":60318},{\"end\":60343,\"start\":60330},{\"end\":61024,\"start\":61001},{\"end\":61043,\"start\":61024},{\"end\":61501,\"start\":61490},{\"end\":61513,\"start\":61501},{\"end\":61527,\"start\":61513},{\"end\":61539,\"start\":61527},{\"end\":62164,\"start\":62153},{\"end\":62180,\"start\":62164},{\"end\":62201,\"start\":62180},{\"end\":62219,\"start\":62201},{\"end\":62914,\"start\":62900},{\"end\":62932,\"start\":62914},{\"end\":62948,\"start\":62932},{\"end\":62965,\"start\":62948},{\"end\":63585,\"start\":63567},{\"end\":63599,\"start\":63585},{\"end\":63614,\"start\":63599},{\"end\":63623,\"start\":63614},{\"end\":63636,\"start\":63623},{\"end\":63651,\"start\":63636},{\"end\":63670,\"start\":63651},{\"end\":63685,\"start\":63670},{\"end\":63702,\"start\":63685},{\"end\":63720,\"start\":63702},{\"end\":63730,\"start\":63720},{\"end\":63742,\"start\":63730},{\"end\":63757,\"start\":63742},{\"end\":63775,\"start\":63757},{\"end\":63789,\"start\":63775},{\"end\":63805,\"start\":63789},{\"end\":64414,\"start\":64399},{\"end\":64431,\"start\":64414},{\"end\":64446,\"start\":64431},{\"end\":64703,\"start\":64692},{\"end\":65241,\"start\":65229},{\"end\":65253,\"start\":65241},{\"end\":65276,\"start\":65253},{\"end\":65720,\"start\":65710},{\"end\":65729,\"start\":65720},{\"end\":66109,\"start\":66090},{\"end\":66124,\"start\":66109},{\"end\":66498,\"start\":66481},{\"end\":66510,\"start\":66498},{\"end\":66526,\"start\":66510},{\"end\":66541,\"start\":66526},{\"end\":67262,\"start\":67246},{\"end\":67275,\"start\":67262},{\"end\":67290,\"start\":67275},{\"end\":67304,\"start\":67290},{\"end\":67321,\"start\":67304},{\"end\":67335,\"start\":67321},{\"end\":67355,\"start\":67335},{\"end\":67373,\"start\":67355},{\"end\":68130,\"start\":68117},{\"end\":68145,\"start\":68130},{\"end\":68167,\"start\":68145},{\"end\":68797,\"start\":68789},{\"end\":68807,\"start\":68797},{\"end\":69137,\"start\":69114},{\"end\":69153,\"start\":69137},{\"end\":69168,\"start\":69153},{\"end\":69182,\"start\":69168},{\"end\":69206,\"start\":69182},{\"end\":69217,\"start\":69206},{\"end\":69230,\"start\":69217},{\"end\":69237,\"start\":69230},{\"end\":70026,\"start\":70016},{\"end\":70036,\"start\":70026},{\"end\":70057,\"start\":70036},{\"end\":70073,\"start\":70057},{\"end\":70770,\"start\":70757},{\"end\":70780,\"start\":70770},{\"end\":70791,\"start\":70780},{\"end\":71535,\"start\":71520},{\"end\":71548,\"start\":71535},{\"end\":72225,\"start\":72211},{\"end\":72245,\"start\":72225},{\"end\":72259,\"start\":72245}]", "bib_venue": "[{\"end\":50132,\"start\":50020},{\"end\":50977,\"start\":50881},{\"end\":51781,\"start\":51654},{\"end\":52624,\"start\":52552},{\"end\":53327,\"start\":53239},{\"end\":54170,\"start\":54021},{\"end\":55360,\"start\":55265},{\"end\":56122,\"start\":56051},{\"end\":56947,\"start\":56868},{\"end\":57694,\"start\":57607},{\"end\":58409,\"start\":58363},{\"end\":59752,\"start\":59647},{\"end\":60549,\"start\":60470},{\"end\":61715,\"start\":61628},{\"end\":62425,\"start\":62346},{\"end\":63161,\"start\":63073},{\"end\":64877,\"start\":64800},{\"end\":66747,\"start\":66668},{\"end\":67569,\"start\":67481},{\"end\":68343,\"start\":68256},{\"end\":69453,\"start\":69346},{\"end\":70273,\"start\":70161},{\"end\":71062,\"start\":70938},{\"end\":71779,\"start\":71667},{\"end\":72455,\"start\":72367},{\"end\":49414,\"start\":49353},{\"end\":50018,\"start\":49932},{\"end\":50879,\"start\":50768},{\"end\":51652,\"start\":51510},{\"end\":52550,\"start\":52463},{\"end\":53237,\"start\":53151},{\"end\":54019,\"start\":53877},{\"end\":54832,\"start\":54780},{\"end\":55263,\"start\":55153},{\"end\":55651,\"start\":55610},{\"end\":56049,\"start\":55963},{\"end\":56866,\"start\":56772},{\"end\":57605,\"start\":57518},{\"end\":58361,\"start\":58300},{\"end\":58784,\"start\":58717},{\"end\":59163,\"start\":59093},{\"end\":59645,\"start\":59525},{\"end\":60468,\"start\":60374},{\"end\":61185,\"start\":61074},{\"end\":61626,\"start\":61539},{\"end\":62344,\"start\":62250},{\"end\":63071,\"start\":62985},{\"end\":63925,\"start\":63821},{\"end\":64397,\"start\":64319},{\"end\":64798,\"start\":64723},{\"end\":65357,\"start\":65296},{\"end\":65782,\"start\":65754},{\"end\":66189,\"start\":66142},{\"end\":66666,\"start\":66572},{\"end\":67479,\"start\":67393},{\"end\":68254,\"start\":68167},{\"end\":68860,\"start\":68832},{\"end\":69344,\"start\":69237},{\"end\":70159,\"start\":70073},{\"end\":70936,\"start\":70814},{\"end\":71665,\"start\":71579},{\"end\":72365,\"start\":72279}]"}}}, "year": 2023, "month": 12, "day": 17}