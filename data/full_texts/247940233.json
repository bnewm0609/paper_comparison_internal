{"id": 247940233, "updated": "2023-12-14 08:33:10.687", "metadata": {"title": "Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation", "authors": "[{\"first\":\"Junliang\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Hongzhi\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Xia\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Lizhen\",\"last\":\"Cui\",\"middle\":[]},{\"first\":\"Quoc\",\"last\":\"Nguyen\",\"middle\":[\"Viet\",\"Hung\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Contrastive learning (CL) recently has spurred a fruitful line of research in the field of recommendation, since its ability to extract self-supervised signals from the raw data is well-aligned with recommender systems' needs for tackling the data sparsity issue. A typical pipeline of CL-based recommendation models is first augmenting the user-item bipartite graph with structure perturbations, and then maximizing the node representation consistency between different graph augmentations. Although this paradigm turns out to be effective, what underlies the performance gains is still a mystery. In this paper, we first experimentally disclose that, in CL-based recommendation models, CL operates by learning more evenly distributed user/item representations that can implicitly mitigate the popularity bias. Meanwhile, we reveal that the graph augmentations, which were considered necessary, just play a trivial role. Based on this finding, we propose a simple CL method which discards the graph augmentations and instead adds uniform noises to the embedding space for creating contrastive views. A comprehensive experimental study on three benchmark datasets demonstrates that, though it appears strikingly simple, the proposed method can smoothly adjust the uniformity of learned representations and has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and training efficiency. The code is released at https://github.com/Coder-Yu/QRec.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/YuY00CN22", "doi": "10.1145/3477495.3531937"}}, "content": {"source": {"pdf_hash": "06c9cbf943185d1d1554e760279412598c6d81a6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.08679v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5196a45334da6201f5d41933f1e504a61196ba78", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/06c9cbf943185d1d1554e760279412598c6d81a6.txt", "contents": "\nAre Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation\nACMCopyright ACMJuly 11-15, 2022. Viet Hung. 2022. July 11-15, 2022\n\nSpain Madrid \nQuoc \n\nThe University of Queensland Brisbane\nAustralia\n\n\nThe University of Queensland\nBrisbaneAustralia\n\n\nThe University of Queensland\nBrisbaneAustralia\n\n\nThe University of Queensland\nBrisbaneAustralia\n\n\nShandong University Jinan\nChina\n\n\nGriffith University Gold Coast\nAustralia\n\nAre Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation\n\nProceedings of the 45th Inter-national ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)\nthe 45th Inter-national ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)Madrid, Spain; New York, NY, USAACM10July 11-15, 2022. Viet Hung. 2022. July 11-15, 202210.1145/3477495.3531937ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00Self-Supervised LearningRecommendationContrastive LearningData Augmentation * Corresponding author\nContrastive learning (CL) recently has spurred a fruitful line of research in the field of recommendation, since its ability to extract self-supervised signals from the raw data is well-aligned with recommender systems' needs for tackling the data sparsity issue. A typical pipeline of CL-based recommendation models is first augmenting the user-item bipartite graph with structure perturbations, and then maximizing the node representation consistency between different graph augmentations. Although this paradigm turns out to be effective, what underlies the performance gains is still a mystery. In this paper, we first experimentally disclose that, in CL-based recommendation models, CL operates by learning more evenly distributed user/item representations that can implicitly mitigate the popularity bias. Meanwhile, we reveal that the graph augmentations, which were considered necessary, just play a trivial role. Based on this finding, we propose a simple CL method which discards the graph augmentations and instead adds uniform noises to the embedding space for creating contrastive views. A comprehensive experimental study on three benchmark datasets demonstrates that, though it appears strikingly simple, the proposed method can smoothly adjust the uniformity of learned representations and has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and training efficiency. The code is released at https://github.com/Coder-Yu/QRec.CCS CONCEPTS\u2022 Information systems \u2192 Recommender systems.\n\nINTRODUCTION\n\nRecently, a resurgence of contrastive learning (CL) [12,13,17] has been witnessed in deep representation learning. Due to the ability to extract the general features from massive unlabeled data and regularize representations in a self-supervised manner, CL has led to major advances in multiple research fields [5,7,29,36]. As data annotation is not required in CL, it is a natural antidote to the data sparsity issue in recommender systems [9,23]. An increasing number of very recent studies [29,33,39,41,45,46] have sought to harness CL for improving recommendation performance and have demonstrated significant gains. A typical way [29] to apply CL to recommendation is first augmenting the user-item bipartite graph with structure perturbations (e.g., stochastic edge/node dropout at a certain ratio), and then maximizing the consistency of representations under different views learned via a graph encoder. In this setting, the CL task acts as the auxiliary task, and is jointly optimized with the recommendation task ( Fig. 1).\n\nDespite the encouraging results achieved by CL, however, what underlies the performance gains still remains unclear. Intuitively, we presume that contrasting different graph augmentations can capture the essential information existing in the original user-item interactions, by randomly removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works [15,39,47] have reported that even extremely sparse graph augmentations (with edge dropout rate 0.9) in CL can bring desired performance gains. Such a phenomenon is quite elusive and counterintuitive because a large dropout rate will result in a huge loss of the raw information and a highly skewed graph structure. It naturally raises a meaningful question: Do we really need graph augmentations when integrating CL with recommendation?\n\nTo answer this question, we first conduct experiments with and without the graph augmentations respectively for a performance comparison. The results show that the when the graph augmentations are absent, the performance is also comparable to those with graph augmentations. We then investigate the embedding space learned by non-CL and CL-based recommendation methods. By visualizing the distributions of the representations and associating them with their performances, we find that what really matters for the recommendation performance is the CL loss, rather than the graph augmentation. Optimizing the contrastive loss InfoNCE [19] learns more evenly distributed user/item representations no matter if graph augmentations are applied, which implicitly plays a role in mitigating the popularity bias [4]. Meanwhile, despite not as effective as expected, graph augmentations are not utterly useless in the sense that the properly perturbed versions of the original graph help learn representations invariant to the disturbance factors [1,5]. However, generating hand-crafted graph augmentations requires constant reconstruction of the graph adjacency matrix during training, which is quite time-consuming. In addition, dropping a critical edge/node (e.g., a cut edge) may split a connected graph into a few disconnected components, at the risk of making the augmented graph and the original graph share little learnable invariance. In view of these defects, a follow-up question then arises: Are there more effective and efficient augmentation approaches?\n\nIn this paper, we give an affirmative answer to the question. On top of our finding that the uniformity of the representation distribution is the key point, we develop a graph-augmentation-free CL method in which the uniformity is more controllable. Technically, we follow the graph CL framework presented in Fig. 1, but we discard the dropout-based graph augmentation and instead add random uniform noises to the original representations for a representation-level data augmentation. Imposing different random noises creates variance between contrastive views, while the learnable invariance is still retained due to the controlled magnitude. Compared with the graph augmentation, the noise version directly regularizes the embedding space towards a more even distribution, which is easy-to-implement and far more efficient.\n\nThe major contributions of this paper are summarized as follows:\n\n\u2022 We experimentally unravel why CL can boost recommendation performance and illustrate that the InfoNCE loss, rather than the graph augmentation, is the decisive factor. \u2022 We propose a simple yet effective graph-augmentation-free CL method for recommendation that can regulate the uniformity in a smooth way. It can be an ideal alternative of cumbersome graph augmentation-based CL methods.\n\n\u2022 We conduct a comprehensive experimental study on three benchmark datasets showing that the proposed method has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and model training efficiency.\n\n\nINVESTIGATION OF GRAPH CONTRASTIVE LEARNING IN RECOMMENDATION 2.1 Graph CL for Recommendation\n\nCL is often applied to recommendation with a particular set of presumed representational invariances to data augmentations [29,34,41,46]. In this paper, we revisit the most commonly used dropoutbased augmentation on graphs [29,36] which assumes that the representations are invariant to partial structure perturbations. An investigation is launched into a state-of-the-art CL-based recommendation model, SGL [29], which performs node and edge dropout to augment the original graph and adopts InfoNCE [19] for CL. Formally, the joint learning scheme in SGL is defined as:\nL = L + L ,(1)\nwhich consists of two losses: recommendation loss L and CL loss L . The InfoNCE in SGL is formulated as:\nL = \u2211\ufe01 \u2208B \u2212 log exp(z \u2032\u22a4 z \u2032\u2032 / ) \u2208B exp(z \u2032\u22a4 z \u2032\u2032 / ) ,(2)\nwhere , are users/items in a sampled batch B, z \u2032 (z \u2032\u2032 ) are 2 normalized -dimensional node representations learned from two different dropout-based graph augmentations, and > 0 (e.g., 0.2) is the temperature. The CL loss encourages consistency between z \u2032 and z \u2032\u2032 which are the augmented representations of the same node and are the positive sample of each other, while minimizing the agreement between \u2032 and z \u2032\u2032 , which are the negative samples of each other. To learn the representations from the user-item graph, SGL employs a popular and effective graph encoder LightGCN [10] as its backbone, whose message passing process is defined as:\nE = 1 1 + (E (0) +\u00c3E (0) + ... +\u00c3 E (0) ),(3)\nwhere E (0) \u2208 R | |\u00d7 is the randomly initialized node embeddings, | | is the number of nodes, is the number of layers, and A \u2208 R | |\u00d7| | is the normalized undirected adjacency matrix. By replacing\u00c3 with the adjacency matrices of the corrupted graph aug-\nmentations, z \u2032 (z \u2032\u2032 ) can be learned via Eq. (3). Note that, z \u2032 = e \u2032 \u2225e \u2032 \u2225 2\nand e \u2032 is the corrupted version of e in E. For conciseness, here we just abstract the core ingredients of SGL and LightGCN. More technical details can be found in the original papers [10,29].\n\n\nNecessity of Graph Augmentation\n\nTo demystify how CL-based recommendation methods work, we first investigate the necessity of the graph augmentation in SGL. We construct a new variant of SGL, termed SGL-WA (WA stands for 'without augmentation'), in which the CL loss is: Because we only learn representations over the original user-item graph, then we have z \u2032 = z \u2032\u2032 = z . The experiments for the performance comparison are conducted on two benchmark datasets:\nL = \u2211\ufe01 \u2208B \u2212 log exp(1/ ) \u2208B exp(z \u22a4 z / ) .(4)\nYelp2018 and Amazon-Book [10,26]. A three-layer setting is adopted and the hyperparameters are tuned according to the original paper of SGL (more experimental details in Section 4.1). The results are presented in Table 1. Three variants of SGL (with dropout rate 0.1) proposed in the paper are evaluated (-ND denotes node dropout, -ED is short for edge dropout, and -RW means random walk (i.e., multi-layer edge dropout). CL Only means that only the CL loss in SGL is minimized). As can be observed, all the variants of SGL outperform LightGCN by large margins, which demonstrates the effectiveness of CL in improving recommendation performance. To our surprise, when the graph augmentation is detached, the performance gains are still so remarkable that SGL-WA even exhibits superiority over SGL-ND and SGL-RW. We conjecture that the node dropout and random walk (especially the former) are very likely to drop the key nodes and associated edges and hence break the correlated subgraphs into disconnected pieces, which highly distort the original graph. Such graph augmentations share little learnable invariance, and encouraging consistency between them probably has a negative impact. By contrast, one-time edge dropout is at a lower risk to largely disturb the semantics of the original graph, so that SGL-ED can maintain a trivial advantage over SGL-WA, which suggests the potential of a proper graph augmentation. However, considering the time-consuming reconstruction of the adjacency matrices in each epoch, we should rethink the necessity of graph augmentations and search for better alternatives. Besides, we wonder what underlies the outstanding performance of SGL-WA since no variances are provided in its CL part.\n\n\nInfoNCE Loss Influences More\n\nWang and Isola [25] have identified that optimizing the contrastive loss intensifies two properties in the visual representation learning: alignment of features from positive pairs, and uniformity of the normalized feature distribution on the unit hypersphere. It is unclear if the CL-based recommendation methods exhibit similar patterns that can explain the results in Section 2.2. Since top-N recommendation is a one-class problem, we only investigate the uniformity by following the visualization method in [25].\n\nWe first map the learned representations (randomly sample 2,000 users for each dataset) to 2-dimensional normalized vectors on the unit hypersphere S 1 (i.e., circle with radius 1) by using t-SNE [24].\n\nAll the representations are obtained when the methods reach their best performance. Then we plot the feature distributions with the nonparametric Gaussian kernel density estimation [3] in R 2 (shown in Fig. 2). For a clearer presentation, the density estimations on angles for each point on S 1 are also visualized. According to Fig.  2, we can observe notably different feature/density distributions. In the leftmost column, LightGCN shows highly clustered features that mainly reside on some narrow arcs. While in the second and the third columns, the distributions become more uniform, and the density estimation curves are less sharp, no matter if the graph augmentations are applied. In the forth column, we plot the features learned only by the contrastive loss in Eq. (2). The distributions are almost completely uniform.\n\nWe think that two reasons may explain the highly clustered feature distributions. The first is the message passing mechanism in LightGCN. With the increase of layers, node embeddings become locally similar. The second is the popularity bias [4] in the recommendation data. Recall the BPR loss [22] used in LightGCN:\nL = \u2212 log( (e \u22a4 e \u2212 e \u22a4 e )),(5)\nwhich is with a triplet input ( , , ). To optimize the BPR loss, we can get the gradients w.r.t e :\u2207 e = \u2212 (1 \u2212 )(e \u2212 e ), where is the learning rate, is the sigmoid function, = (e \u22a4 e \u2212 e \u22a4 e ), e is the user embedding, and e and e denote the positive and negative item embeddings, respectively. Since the recommendation data usually follows a long-tail distribution, when is a popular item with a large number of interactions, the user embedding will be constantly updated towards 's direction (i.e., \u2212\u2207 e ). The message passing mechanism further exacerbates the clustering problem (i.e., e and e aggregate information from each other in the graph convolution) and causes the representation degeneration [21].\n\nAs for the distributions in other columns, by rewriting Eq. (4), we can derive\nL = \u2211\ufe01 \u2208B \u22121/ + log exp(1/ ) + \u2211\ufe01 \u2208B/{ } exp(z \u22a4 z / ) . (6)\nBecause 1/ is a constant, optimizing the CL loss is actually minimizing the cosine similarity between different nodes embeddings e and e , which will push connected nodes away from the highdegree hubs in the representation space and lead to a more even distribution. By associating the results in Table 1 with the distributions in Fig. 2, we can easily draw a conclusion that the uniformity of the distribution is the underlying factor that has a decisive impact on the recommendation performance in SGL, rather than the dropoutbased graph augmentations. Optimizing the CL loss can be seen as an implicit way to debias (discussed in section 4.2) because a more even representation distribution can preserve the intrinsic characteristics of nodes and improve the generalization ability. This can be a persuasive explanation for the unexpected performance of SGL-WA. It also should be noted that, by only minimizing the CL loss in Eq. (2), a poor performance will be reached, which means that a positive correlation between the uniformity and the performance only holds in a limited scope. The excessive pursuit to the uniformity will overlook the closeness of interacted pairs and similar users/items, and impairs recommendation performance.  \n\n\nSIMGCL: SIMPLE GRAPH CONTRASTIVE LEARNING FOR RECOMMENDATION\n\nBased on the findings in Section 2, we speculate that by adjusting the uniformity of the learned representation in a certain scope, the optimal performance can be reached. In this section, we aim to develop a Simple Graph Contrastive Learning method (SimGCL) for recommendation that can smoothly regulate the uniformity and provide informative variance to maximize the benefit from CL.\n\n\nMotivation and Formulation\n\nSince manipulating the graph structure for a more evenly-distributed representation space is intractable and time-consuming, we shift our attention to the embedding space. Inspired by the adversarial examples [8] which are constructed by adding imperceptibly small perturbation to the input images, we directly add random noises to the representation for an efficient and effective augmentation. Formally, given a node and its representation e in thedimensional embedding space, we can implement the following representation-level augmentation:\ne \u2032 = e + \u0394 \u2032 , e \u2032\u2032 = e + \u0394 \u2032\u2032 ,(7)\nwhere the added noise vectors \u0394 \u2032 and \u0394 \u2032\u2032 are subject to \u2225\u0394\u2225 2 = and \u0394 =\u0394 \u2299 sign(e ),\u0394 \u2208 R \u223c (0, 1). The first constraint controls the magnitude of \u0394, and \u0394 is numerically equivalent to points on a hypersphere with the radius . The second constraint requires that e , \u0394 \u2032 and \u0394 \u2032\u2032 should be in the same hyperoctant, so that adding the noises will not cause a large deviation of e , making less valid positive samples. In Fig. 3, we illustrate Eq. (7) in R 2 . By adding the scaled noise vectors to the original representation, we rotate e by two small angles ( 1 and 2 ). Each rotation corresponds to a deviation of e , and leads to an augmented representation (e \u2032 and e \u2032\u2032 ). Since the rotation is small enough, the augmented representation retains most information of the original representation and meanwhile also keeps some variance. Note that, for each node representation, the added random noises are different.\nr = j \u2206 \u2032 \u2206 \u2032\u2032 \u2032\u2032 \u2032 1 2\n\nEmbedding Space\n\nFollowing SGL, we adopt LightGCN as the graph encoder to propagate node information and amplify the impact of the variance due to its simple structure and effectiveness. At each layer, different scaled random noises are imposed on the current node embeddings. The final perturbed node representations are learned by:\nE \u2032 = 1 (\u00c3E (0) + \u2206 (1) ) + (\u00c3(\u00c3E (0) + \u2206 (1) ) + \u0394 (2) )) + ... +(\u00c3 E (0) +\u00c3 \u22121 \u2206 (1) + ... +\u00c3\u2206 ( \u22121) + \u2206 ( ) )(8)\nIt should be mentioned that we skip the input embedding E (0) in all the three encoders when calculating the final representations, because we experimentally find that skipping it can lead to slight performance improvement in our setting. However, without the CL task, this operation will result in a performance drop of LightGCN. Finally, we also unify the BPR loss (Eq. (5)) and the CL loss (Eq. (2)), and then use Adam to optimize the joint loss presented in Eq. (1).\n\n\nRegulating Uniformity\n\nIn SimGCL, two hyperparameters and can influence the uniformity of the representations which are critical to the performance. But can explicitly and smoothly regulate the uniformity beyond by only tuning . By adjusting the value of , we can directly control how far the augmented representations deviate from the original. Intuitively, a larger will lead to a more roughly even distribution of the learned representation, because when the augmented representations are enough far away from the original, the information lying in their representations is also considerably influenced by the noises. As the noises are sampled from a uniform distribution, by contrasting the augmented representations, the original representation is regularized towards higher uniformity. We present the following experimental analysis to demonstrate it. In [25], a metric is proposed to measure the uniformity of the representation, which is the logarithm of the average pairwise Gaussian potential (a.k.a. the Radial Basis Function (RBF) kernel):\nL uniform ( ) = log E . . , \u223c node \u22122 \u2225 ( )\u2212 ( ) \u2225 2 2 .(9)\nwhere ( ) outputs the 2 normalized embedding of . We choose the popular items (with more than 200 interactions) and randomly sample 5,000 users in the dataset of Yelp2018 to form the user-item pairs, and then compute the uniformity of their representations in the SGL variants and SimGCL with Eq. (9). For a fair comparison, a three-layer setting is applied to all the compared methods with = 0.1. We then tune to observe how the uniformity changes. The uniformity is checked after every epoch, and we record the values in the first 30 epochs during which the compared methods all converge to their optimal solutions.\n\nAs clearly shown in Fig. 4, similar trends are observed on all the curves. At the initial stage, all the methods have highly uniformlydistributed representations because we use Xavier initialization, which is a special uniform distribution. With the training proceeding, the uniformity declines (L uniform gets higher), and after reaching the peak, the uniformity improves till convergence and maintains this tendency. As for SimGCL, with the increase of , it tends to learn more even representations, and even a very small = 0.01 leads to higher uniformity compared with the SGL variants. As a result, users (especially the long-tail users) are less affected by the popular items. In the rightmost column in Fig. 2, we also plot the representation distributions of SimGCL with = 0.1. We can clearly see that the distributions are evidently more even than those learned by SGL variants and LightGCN. All these results can support our claim that by replacing graph augmentations with the noise-based augmentations, SimGCL is more capable of controlling the uniformity of learned representations so as to debias.\n\n\nComplexity\n\nIn this section, we analyze the time complexity of SimGCL, and compare it with that of LightGCN and its graph-augmentation based counterpart SGL-ED. We hereby discuss the batch time complexity since the in-batch negative sampling is a widely used trick in CL [5]. Let | | be the edge number in the graph, be the embedding size, denote the batch size, represent the node number in a batch, and denote the edge keep rate in SGL-ED. We can derive: \nO (2| |) O (2| | + 4 | |) O (2| |) Graph Convolution O (2| | ) O ( (2 + 4 ) | | ) O (6| | ) BPR Loss O (2 ) O (2 ) O (2 ) CL Loss - O ( + ) O ( + )\n\u2022 For LightGCN and SimGCL, no graph augmentations are required, so they just need to normalize the original adjacency matrix which has 2| | non-zero elements. For SGL-ED, two graph augmentations are used and each has 2 | | non-zero elements. \u2022 In the graph convolution stage, a three-encoder architecture (see Fig. 1) is employed in both SGL-ED and SimGCL to learn augmented node representations. So, the time costs of SGL-ED and SimGCL are almost three times that of LightGCN.  Comparing SimGCL with SGL-ED, we can clearly see that SGL-ED theoretically spends less time for graph convolution, and this bonus may offset SimGCL's advantage for the adjacency matrix construction. However, when putting them into practice, we actually observe that SimGCL is much more time-efficient. That is because, the computation for the graph convolution is mostly finished on GPUs, while the graph perturbation is performed on CPUs. Besides, in each epoch, the adjacency matrices of graph augmentations in SGL-ED need to be reconstructed. While in SimGCL, the adjacency matrix of the original graph only needs to be generated once before the training. In a nutshell, SimGCL is far more efficient than SGL, beyond what we can observe from the theoretical analysis.\n\n\nEXPERIMENTAL RESULTS\n\n\nExperimental Settings\n\nDatasets. Three public benchmark datasets: Douban-Book [39] (#user 13,024, #item 22,347, #interaction 792,062), Yelp2018 [10] (#user 31,668 #item 38,048, #interaction 1,561,406), and Amazon-Book [29] (#user 52,463, #item 91,599, #interaction 2,984,108) are used in our experiments to evaluate SimGCL. Because we focus on the Top-N recommendation, following the convention in the previous research [41,41], we discard ratings less than 4 in Douban-Book, which is with a 1-5 rating scale, and reset the rest to 1. We split the datasets into three parts (training set, validation set, and test set) with a ratio of 7:1:2. Two common metrics: Recall@ and NDCG@ are used and we set =20. For a rigorous and unbiased evaluation, each experiment in this section is conducted 5 times with ranking all the items and we then report the average result. Baselines. Besides LightGCN and the SGL variants, the following recent data augmentation-based methods are compared. Hyperparameters. For a fair comparison, we refer to the best hyperparameter settings reported in the original papers of the baselines and then fine-tune all the hyperparameters of the baselines with the grid search. As for the general settings of all the baselines, the Xavier initialization is used on all the embeddings. The embedding size is 64, the parameter for 2 regularization is 10 \u22124 and the batch size is 2048. We use Adam with the learning rate 0.001 to optimize all the models. In SimGCL and SGL, we empirically let the temperature = 0.2, and this value is also reported as the best in the original paper of SGL.\n\n\nSGL vs. SimGCL: From a Comprehensive Perspective\n\nAs one of the core claims of this paper is that graph augmentations are not indispensable and inefficient in CL-based recommendation, in this part, we conduct a comprehensive comparison between SGL and SimGCL in terms of the recommendation performance, convergence speed, running time, and ability to debias. \u2022 SGL-ED is the most effective variant of SGL while SGL-ND is the least effective. When a 2-layer or 3-layer setting is used, SGL-WA outperforms SGL-ND in most cases and shows advantages over SGL-RW in a few cases. These results demonstrate that the CL loss is the main driving force of the performance improvement while intuitive graph augmentations may not be as effective as expected, and some of them may even lower the performance. \u2022 SimGCL shows the best performance in all the cases, which proves the effectiveness of the random noised-based data augmentation. Particularly, on the two larger datasets: Yelp2018 and Amazon-Book, SimGCL significantly outperforms the SGL variants by large margins.\n\n\nConvergence Speed Comparison.\n\nIn this part, we show that SimGCL converges much faster than SGL does. A 2-layer setting is used in this part and the other parameters remain unchanged. According to Fig. 5 and Fig. 6, we can observe that, SimGCL reaches its best performance on the test set at the 25 th , the 11 th , and the 10 th epoch on Douban-Book, Yelp2018, and Amazon-Book, respectively. By contrast, SGL-ED peaks at the 38 th , the 17 th , and the 14 th epoch, respectively. SimGCL only spends 2/3 epochs that the SGL variants need. Besides, the curve of SGL-WA almost overlaps that of SGL-ED on Yelp2018 and Amazon-Book, and exhibits the same tendency to convergence. It seems that the dropout-based graph augmentations cannot speed up the model for a faster convergence. Despite that, all the CL-based methods show advantages over LightGCN at the convergence speed. When the other three methods begin to get overfitted, LightGCN is still hundreds of epochs distant from getting converged.\n\nIn the paper of SGL, the authors guess that the multiple negatives in the CL loss may contribute to the fast convergence. However, with almost infinite negative samples created by dropout, SGL-ED is basically on par with SGL-WA in speeding up the training, though the latter only has a certain number of negative samples. As for SimGCL, we consider that the remarkable convergence speed stems from the noises. By analyzing the gradients from the CL loss, we find that the noises averagely provide a constant increment, working like a momentum. In addition to the results in Fig. 5 and 6, we also find that the training accelerates with getting larger. But when it is overlarge (e.g., greater than 1), despite the rapid decrease of BPR loss, SimGCL requires more time to get converged. A large acts like a large learning rate, causing the progressive zigzag optimization that will overshoot the minimum. \n\n\nAmazon-Book\n\nLightGCN SGL-WA SGL-ED SimGCL \n\n\nAmazon-Book\n\nLightGCN SGL-WA SGL-ED SimGCL Figure 6: The loss curves in the first 50 epochs.\n\n\nRunning Time Comparison.\n\nIn this part, we report the real running time that the compared methods cost for one epoch. The results in Table 4 are collected on an Intel(R) Xeon(R) Gold 5122 CPU and a GeForce RTX 2080Ti GPU. As shown in Table 4, we calculate how many times slower the other methods are when compared with LightGCN. Because there is no graph augmentation in SGL-WA, we can see its running speed is very close to that of LightGCN. For SGL-ED, two graph augmentations are required and the computation in this part is mostly finished on CPUs, so that it is even 5.7 times slower than LightGCN on Amazon-Book. The running time increases with the volume of the datasets. By contrast, despite not as fast as SGL-WA, SimGCL is only 2.4 times slower than LightGCN on Amazon-Book, and the growth trend is far lower than that of SGL-ED. Considering that SimGCL only needs 2/3 the epochs that SGL-ED spends, it outperforms SGL in all aspects w.r.t efficiency.\n\n\nAbility to Debias.\n\nThe InfoNCE loss is found to have the ability to implicitly alleviate the popularity bias by learning more even representations. To verify that SimGCL upgrades this ability with the noise-based representation augmentation, we divide the test set into three subsets in proportion to the popularity of items. 80% items with the fewest number of clicks/purchases are labelled 'Unpopular', 5% items which are most clicked/purchased are labelled 'Popular', and the rest items are labelled 'Normal'. We then conduct experiments to check the Recall@20 value that each group contributes (overall Recall@20 value is the sum of the values from three groups). The results are illustrated in Fig. 7.\n\nWe can clearly see that the SimGCL's improvements all come from the items with lower popularity. Its prominent advantage on recommending long-tail items largely compensates for its loss on the 'Popular' group. By contrast, LightGCN is inclined to recommend popular items and achieves the highest recall value on the last two datasets. The SGL variants fall between LightGCN and SimGCL on exploring long-tail items and exhibit similar recommendation preference. Combining Fig. 2 with Fig. 7, we can easily find that there is a positive correlation between the uniformity of representations and the ability to debias. Since the popular items probably have been exposed to users from other sources, recommending them may no be a good choice. On this point, SimGCL significantly outperforms SGL, and its extraordinary performance on discovering long-tail items fits the real need of users.\n\n\nParameter Sensitivity Analysis\n\nIn this part, we investigate the impact of the two important hyperparameters in SimGCL. Here we adopt the experimental settings used in section 4.2.2.\n\n4.3.1 Impact of . By fixing at 0.1, we change to a set of predetermined representative values presented in Fig. 8. As can be observed, with the increase of , the performance of SimGCL starts to increase at the beginning, and it gradually reaches its peak when is 0.2 on Douban-Book, 0.5 on Yelp2018, and 2 on Amazon-Book. Afterwards, it starts to decline. Besides, in contrast to Fig. 9, more dramatic changes are observed in Fig. 8 though and are tuned in the same scope, which demonstrates that can provide a finergrained regulation beyond that provided only by tuning .\n\n\nImpact of .\n\nWe think a larger leads to a more even distribution that can help to debias. However, when it is too large, the recommendation task will be hindered because the high similarity between connected nodes cannot be reflected by an over-uniform distribution. We fix at the best values on the three datasets as reported in Fig. 8, and then adjust to see the performance change. As shown in Fig. 9, the shapes of the curves are as expected. On all the datasets, when is near 0.1, SimGCL achieves the best performance. We also find that initializing embeddings with uniform distributions (including Xavier initialization) leads to 3%-4% performance improvement compared with Gaussian initialization.\n\n\nPerformance Comparison with Other Methods\n\nTo further confirm the outstanding competence of SimGCL, we compare it with other four recently proposed data augmentationbased methods. According to Table 5, SimGCL outperforms all the baselines, and MixGCF is the runner-up. Meanwhile, we find that some data augmentation-based recommendation methods are not as powerful as expected, and even outperformed by LightGCN in many cases. We attribute their failure to: (1). LightGCN, MixGCF, and SimGCL are all based on the graph convolution mechanism, which are more capable of modeling graph data compared with  \n\n\nAmazon-Book\n\nRecall NDCG  \n\n\nAmazon-Book\n\nRecall NDCG Figure 9: Influence of the noise magnitude .\n\nMult-VAE. (2). DNNs are proved effective when user/item features are available. In our datasets, no features are provided and we mask embeddings learned by DNN to conduct self-supervised learning, so it cannot fulfill itself in this situation. (3). In the paper of BUIR, it removes long-tail nodes to achieve a good performance, but we use all the users and items. Besides, its siamese network structure may also collapse to a trivial solution on some long-tail nodes because it does not use negative examples, which may account for its incompetence.\n\n\nPerformance Comparison with Different Types of Noises\n\nIn SimGCL, we use the random noises sampled from a uniform distribution to implement data augmentation. However, there are other types of noises including the Gaussian noises and adversarial noises. Here we also test different noises and report their best results in Table 6 ( denotes uniform noises sampled from (0, 1), represents the positive uniform noises which differ from in not satisfying the second constraint in section 3.1, denotes Gaussian noises generated by the standard Gaussian distribution, and denotes adversarial noises generated by FGSM [8]). According to Table 6, SimGCL shows comparable performance while SimGCL is less effective. The possible reason is that we apply 2 normalization to the noises. The normalized noises generated by the standard Gaussian distribution can fit a much flatter Gaussian distribution (can be easily proved) which approximates a uniform distribution. So, the comparable results are observed. As for SimGCL , the adversarial noises are generated by only targeting maximizing the CL loss while the recommendation loss has a dominant status that impacts the performance more during optimization. As for SimGCL , we notice a slight performance drop compared with SimGCL in most cases, which suggests the necessity of the directional constraint for creating more informative augmentations. Graph Neural Networks (GNNs) [6,31] now have become widely acknowledged powerful architectures for modeling recommendation data. This new neural network paradigm ends the regime of MLP-based recommendation models in the academia, and boosts the neural recommender systems to a new level. A large number of recommendation models, which adopt GNNs as their bases, claim that they have achieved state-of-the-art performance [10,40,41] in different subfields. Particularly, GCN [14], as the most prevalent variant of GNNs, further fuels the development of the graph neural recommendation models like GCMC [2], NGCF [26], LightGCN [10], and LCF [43]. Despite the different implementations in details, these GCN-driven models share a common idea that is to acquire the information from the neighbors in the user-item graph layer by layer to refine the target node's embeddings and fulfill graph reasoning [30]. Among these methods, LightGCN is the most popular one due to its simple structure and decent performance. Following [28], it removes the redundant operations including transformation matrices and nonlinear activation functions. Such a design is proved efficient and effective, and inspires a lot of follow-up CL-based recommendation models like SGL [29] and MHCN [39].\n\n\nContrastive Learning in Recommendation\n\nAs CL works in a self-supervised manner [42], it is inherently a possible solution to the data sparsity issue [37,38] in recommender systems. Inspired by the achievements of CL in other fields, there has been a wave of new research that integrates CL with recommendation [18,20,29,33,39,41,46]. Zhou et al. [46] adopted random masking on attributes and items to create sequence augmentations for sequential model pretraining with mutual information maximization. Wei et al. [27] reformulated the cold-start item representation learning from an information-theoretic standpoint and maximized the mutual dependencies between item content and collaborative signals to alleviate the data sparsity issue. Similar ideas are also found in [35], where a two-tower DNN architecture is developed for recommendation, in which the item tower is also shared for contrasting augmented item features. SEPT [39] and COTREC [32] further propose to mine multiple positive samples with semisupervised learning on the perturbed graph for social/session-based recommendation. In addition to the dropout, CL4Rec [34] proposes to reorder and crop item segments for sequential data augmentation. Yu et al. [41], Zhang et al. [44] and Xia et al. [33] leveraged hypergraph to model recommendation data, and proposed to contrast different hypergraph structures for representation regularization. In addition to the data sparsity problem, Zhou et al. [45] theoretically proved that CL can also mitigate the exposure bias in recommendation, and developed a method named CLRec to improve deep match in terms of fairness and efficiency.\n\n\nCONCLUSION\n\nIn this paper, we revisit the dropout-based CL in recommendation, and investigate how it improves recommendation performance. We reveal that, in CL-based recommendation models, the CL loss is the core and the graph augmentation only plays a secondary role.\n\nOptimizing the CL loss leads to a more even representation distribution, which helps to debias in the scenario of recommendation. We then develop a simple graph-augmentation-free CL method to regulate the uniformity of the representation distribution in a more straightforward way. By adding directed random noises to the representation for different data augmentations and contrast, the proposed method can significantly enhance recommendation.\n\nThe extensive experiments demonstrate that the proposed method outperforms its graph augmentation-based counterparts and meanwhile the training time is dramatically reduced.\n\nFigure 1 :\n1Graph contrastive learning with edge dropout for recommendation.\n\n\nDistribution of item representations learned from the dataset of Amazon-Book.\n\nFigure 2 :\n2We plot feature distributions with Gaussian kernel density estimation (KDE) in R 2 (the darker the color is, the more points fall in that area.) and KDE on angles (i.e., arctan2(y, x) for each point (x,y) \u2208 S 1 ).\n\nFigure 3 :\n3An illustration of the proposed random noisebased data augmentation in R 2 .\n\nFigure 4 :\n4Trends of uniformity. The star indicates the epoch where the best recommendation performance is reached. Lower L uniform numbers are better.\n\n\u2022\nAs for the recommendation loss, three methods all use the BPR loss and each batch contains interactions, so they have the same time cost in this component. \u2022 When calculating the CL loss, the computation costs between the positive/negative samples are O ( ) and O ( ), respectively, because each node only considers itself as the positive, while the other nodes all are negatives.\n\n\u2022\nMult-VAE[16] is a variational autoencoder-based recommendation model. It can be seen as a special self-supervised recommendation model because it has a reconstruction objective.\u2022 DNN+SSL[35] is a recent DNN-based recommendation method which adopts the similar architecture inFig. 1, and conducts feature masking for CL. \u2022 BUIR[15] has a two-branch architecture which consists of a target network and an online network, and only uses positive examples for self-supervised learning. \u2022 MixGCL[11] designs the hop mixing technique to synthesize hard negatives for graph collaborative filtering by embedding interpolation.\n\nFigure 5 :\n5The performance curves in the first 50 epochs.\n\n\nFigure 7: Performance comparison over different item groups\n\nFigure 8 :\n8Influence of the magnitude of CL.\n\nTable 1 :\n1Performance comparison of different SGL variants.Method \nYelp2018 \nAmazon-Book \n\nRecall@20 NDCG@20 Recall@20 NDCG@20 \nLightGCN \n0.0639 \n0.0525 \n0.0410 \n0.0318 \nSGL-ND \n0.0644 \n0.0528 \n0.0440 \n0.0346 \nSGL-ED \n0.0675 \n0.0555 \n0.0478 \n0.0379 \nSGL-RW \n0.0667 \n0.0547 \n0.0457 \n0.0356 \nSGL-WA \n0.0671 \n0.0550 \n0.0466 \n0.0373 \nCL Only \n0.0245 \n0.0190 \n0.0314 \n0.0258 \n\n\n\nTable 2 :\n2The comparison of time complexityComponent \nLightGCN \nSGL-ED \nSimGCL \n\nAdjacency \nMatrix \n\n\nTable 3 :\n3Performance Comparison for different CL methods on three benchmarks.Method \nDouban-Book \nYelp2018 \nAmazon-Book \n\nRecall \nNDCG \nRecall \nNDCG \nRecall \nNDCG \n\n1-Layer \n\nLightGCN \n0.1394 \n0.1165 \n0.0631 \n0.0515 \n0.0384 \n0.0298 \nSGL-ND \n0.1619 (+16.1%) \n0.1448 (+24.3%) \n0.0643 (+1.9%) \n0.0529 (+2.7%) \n0.0432 (+12.5%) \n0.0334 (+12.1%) \nSGL-ED \n0.1658 (+18.9%) \n0.1491 (+28.0%) \n0.0637 (+1.0%) \n0.0526 (+2.1%) \n0.0451 (+17.4%) \n0.0353 (+18.5%) \nSGL-RW \n0.1658 (+18.9%) \n0.1491 (+28.0%) \n0.0637 (+1.0%) \n0.0526 (+2.1%) \n0.0451 (+17.4%) \n0.0353 (+18.5%) \nSGL-WA \n0.1628 (+16.8%) \n0.1454 (+24.8%) \n0.0628 (-0.4%) \n0.0525 (+1.9%) \n0.0403 (+4.9%) \n0.0320 (+7.4%) \nSimGCL \n0.1720 (+23.4%) 0.1519 (+30.4%) \n0.0689 (+9.2%) \n0.0572 (+11.1%) 0.0453 (+18.0%) 0.0358 (+20.1%) \n\n2-Layer \n\nLightGCN \n0.1485 \n0.1272 \n0.0622 \n0.0504 \n0.0411 \n0.0315 \nSGL-ND \n0.1622 (+9.2%) \n0.1434 (+12.7%) \n0.0658 (+5.8%) \n0.0538 (+6.7%) \n0.0427 (+3.9%) \n0.0335 (+6.3%) \nSGL-ED \n0.1721 (+15.9%) \n0.1525 (+19.9%) \n0.0668 (+7.4%) \n0.0549 (+8.9%) \n0.0468 (+13.9%) \n0.0371 (+17.8%) \nSGL-RW \n0.1710 (+15.2%) \n0.1516 (+19.2%) \n0.0644 (+3.5%) \n0.0530 (+5.2%) \n0.0453 (+10.2%) \n0.0358 (+13.7%) \nSGL-WA \n0.1687 (+13.6%) \n0.1501 (+18.0%) \n0.0653 (+5.0%) \n0.0544 (+7.9%) \n0.0453 (+10.2%) \n0.0358 (+13.7%) \nSimGCL \n0.1770 (+19.2%) 0.1582 (+24.4%) 0.0719 (+15.6%) 0.0601 (+19.2%) 0.0507 (+23.4%) 0.0405 (+28.6%) \n\n3-Layer \n\nLightGCN \n0.1501 \n0.1282 \n0.0639 \n0.0525 \n0.0410 \n0.0318 \nSGL-ND \n0.1626 (+8.3%) \n0.1450 (+13.1%) \n0.0644 (+0.8%) \n0.0528 (+0.6%) \n0.0440 (+7.3%) \n0.0346 (+8.8%) \nSGL-ED \n0.1732 (+15.4%) \n0.1551 (+21.0%) \n0.0675 (+5.6%) \n0.0555 (+5.7%) \n0.0478 (+16.6%) \n0.0379 (+19.2%) \nSGL-RW \n0.1730 (+15.3%) \n0.1546 (+20.6%) \n0.0667 (+4.4%) \n0.0547 (+4.2%) \n0.0457 (+11.5%) \n0.0356 (+12.0%) \nSGL-WA \n0.1705 (+12.0%) \n0.1525 (+19.0%) \n0.0671 (+5.0%) \n0.0550 (+4.8%) \n0.0466 (+13.7%) \n0.0373 (+18.4%) \nSimGCL \n0.1772 (+18.1%) 0.1583 (+23.5%) 0.0721 (+12.8%) 0.0601 (+14.5%) 0.0515 (+25.6%) 0.0414 (+30.2%) \n\n\n\nTable 4 :\n4Running time for per epoch (x in the brackets represents times).Method \nDouban-Book Yelp2018 Amazon-Book \nTime (s) \nTime (s) \nTime (s) \nLightGCN \n3.6 \n13.6 \n41.5 \nSGL-WA \n4.4 (1.2x) \n16.3 (1.2x) \n47.0 (1.1x) \nSGL-ED \n13.3 (3.7x) \n62.3 (4.6x) \n235.3 (5.7x) \nSimGCL \n6.1 (1.7x) \n27.9 (2.1x) \n98.4 (2.4x) \n\n0 \n10 \n20 \n30 \n40 \n50 \n\n0.0% \n\n2.5% \n\n5.0% \n\n7.5% \n\n10.0% \n\n12.5% \n\n15.0% \n\n17.5% \n\nRecall \n\nDouban-Book \n\nLightGCN \nSGL-WA \nSGL-ED \nSimGCL \n\n0 \n10 \n20 \n30 \n40 \n50 \n\n0.0% \n\n1.0% \n\n2.0% \n\n3.0% \n\n4.0% \n\n5.0% \n\n6.0% \n\n7.0% \n\nYelp2018 \n\nLightGCN \nSGL-WA \nSGL-ED \nSimGCL \n\n0 \n10 \n20 \n30 \n40 \n50 \n\n0.0% \n\n1.0% \n\n2.0% \n\n3.0% \n\n4.0% \n\n5.0% \n\n\n\nTable 5 :\n5Performance comparison with other SOTA models.Method \nDouban-Book \nYelp2018 \nAmazon-Book \n\nRecall NDCG Recall NDCG Recal NDCG \nLightGCN 0.1501 \n0.1282 \n0.0639 \n0.0525 \n0.0411 \n0.0315 \nMult-VAE \n0.1310 \n0.1103 \n0.0584 \n0.0450 \n0.0407 \n0.0315 \nDNN+SSL 0.1366 \n0.1148 \n0.0483 \n0.0382 \n0.0438 \n0.0337 \nBUIR \n0.1127 \n0.8938 \n0.0487 \n0.0404 \n0.0260 \n0.0209 \nMixGCF \n0.1731 \n0.1552 \n0.0713 \n0.0589 \n0.0485 \n0.0378 \nSimGCL \n0.1772 0.1583 0.0721 0.0601 0.0515 0.0410 \n\n\n\nTable 6 :\n6Performance comparison between different SimGCL variants.Method \nDouban-Book \nYelp2018 \nAmazon-Book \n\nRecall NDCG Recall NDCG Recal NDCG \nLightGCN 0.1485 \n0.1272 \n0.0639 \n0.0525 \n0.0411 \n0.0315 \nSimGCL \n0.1561 \n0.1379 \n0.0604 \n0.0505 \n0.0455 \n0.0358 \nSimGCL \n0.1751 \n0.1565 \n0.0708 \n0.0593 \n0.0514 \n0.0409 \nSimGCL \n0.1773 0.1586 0.0718 \n0.0599 \n0.0511 \n0.0408 \nSimGCL \n0.1772 \n0.1583 0.0721 0.0601 0.0515 0.0414 \n\n5 RELATED WORK \n5.1 Graph Neural Recommendation Models \n\n\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, arXiv:1906.00910arXiv preprintPhilip Bachman, R Devon Hjelm, and William Buchwalter. 2019. Learning rep- resentations by maximizing mutual information across views. arXiv preprint arXiv:1906.00910 (2019).\n\nRianne Van Den, Thomas N Berg, Max Kipf, Welling, arXiv:1706.02263Graph convolutional matrix completion. arXiv preprintRianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu- tional matrix completion. arXiv preprint arXiv:1706.02263 (2017).\n\nKernel density estimation via diffusion. I Zdravko, Joseph F Botev, Dirk P Grotowski, Kroese, The annals of Statistics. 38Zdravko I Botev, Joseph F Grotowski, and Dirk P Kroese. 2010. Kernel density estimation via diffusion. The annals of Statistics 38, 5 (2010), 2916-2957.\n\nJiawei Chen, Hande Dong, Xiang Wang, arXiv:2010.03240Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and debias in recommender system: A survey and future directions. arXiv preprintJiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and debias in recommender system: A survey and future directions. arXiv preprint arXiv:2010.03240 (2020).\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, PMLRInternational conference on machine learning. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Interna- tional conference on machine learning. PMLR, 1597-1607.\n\nChen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, arXiv:2109.12843Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions. arXiv preprintChen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et al. 2021. Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions. arXiv preprint arXiv:2109.12843 (2021).\n\nTianyu Gao, Xingcheng Yao, Danqi Chen, arXiv:2104.08821SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprintTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821 (2021).\n\nExplaining and harnessing adversarial examples. J Ian, Jonathon Goodfellow, Christian Shlens, Szegedy, arXiv:1412.6572arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples (2014). arXiv preprint arXiv:1412.6572 (2014).\n\nPretraining graph neural networks for cold-start users and items representation. Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, Hong Chen, Proceedings of the 14th ACM International Conference on Web Search and Data Mining. the 14th ACM International Conference on Web Search and Data MiningBowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, and Hong Chen. 2021. Pre- training graph neural networks for cold-start users and items representation. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining. 265-273.\n\nLightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information RetrievalACMXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net- work for Recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. ACM, 639-648.\n\nMixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems. Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, Jie Tang, Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021. MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems. (2021).\n\nDebapriya Banerjee, and Fillia Makedon. 2021. A survey on contrastive self-supervised learning. Technologies. Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh921Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Baner- jee, and Fillia Makedon. 2021. A survey on contrastive self-supervised learning. Technologies 9, 1 (2021), 2.\n\nSupervised Contrastive Learning. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, Advances in Neural Information Processing Systems. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised Contrastive Learning. In Advances in Neural Information Processing Systems, NeurIPS 2020.\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, 5th International Conference on Learning Representations. Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017.\n\nBootstrapping User and Item Representations for One-Class Collaborative Filtering. Dongha Lee, Seongku Kang, Hyunjun Ju, Chanyoung Park, Hwanjo Yu, The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. Jones, and Tetsuya SakaiFernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, RosieDongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, and Hwanjo Yu. 2021. Bootstrapping User and Item Representations for One-Class Collaborative Filter- ing. In The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). 1513-1522.\n\nVariational autoencoders for collaborative filtering. Dawen Liang, G Rahul, Krishnan, D Matthew, Tony Hoffman, Jebara, Proceedings of the 2018 world wide web conference. the 2018 world wide web conferenceDawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In Proceedings of the 2018 world wide web conference. 689-698.\n\nSelf-supervised learning: Generative or contrastive. Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, Jie Tang, arXiv:2006.082181arXiv preprintXiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. 2020. Self-supervised learning: Generative or contrastive. arXiv preprint arXiv:2006.08218 1, 2 (2020).\n\nDisentangled Self-Supervision in Sequential Recommenders. Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, Wenwu Zhu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. 2020. Disentangled Self-Supervision in Sequential Recommenders. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 483-491.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n\nMemory Augmented Multi-Instance Contrastive Predictive Coding for Sequential Recommendation. Ruihong Qiu, Zi Huang, Hongzhi Yin, 2021 IEEE International Conference on Data Mining (ICDM). IEEERuihong Qiu, Zi Huang, and Hongzhi Yin. 2021. Memory Augmented Multi- Instance Contrastive Predictive Coding for Sequential Recommendation. In 2021 IEEE International Conference on Data Mining (ICDM). IEEE, 519-528.\n\nContrastive Learning for Representation Degeneration Problem in Sequential Recommendation. Ruihong Qiu, Zi Huang, Hongzhi Yin, Zijian Wang, Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. the Fifteenth ACM International Conference on Web Search and Data MiningRuihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learn- ing for Representation Degeneration Problem in Sequential Recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. 813-823.\n\nBPR: Bayesian personalized ranking from implicit feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence. the twenty-fifth conference on uncertainty in artificial intelligenceAUAI PressSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence. AUAI Press, 452-461.\n\nSparsity, scalability, and distribution in recommender systems. Sarwar Badrul Munir, Badrul Munir Sarwar. 2001. Sparsity, scalability, and distribution in recommender systems. (2001).\n\nVisualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 911Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008).\n\nUnderstanding contrastive representation learning through alignment and uniformity on the hypersphere. Tongzhou Wang, Phillip Isola, International Conference on Machine Learning. PMLR. Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning. PMLR, 9929-9939.\n\nNeural graph collaborative filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. the 42nd international ACM SIGIR conference on Research and development in Information RetrievalXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 165-174.\n\nContrastive learning for cold-start recommendation. Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, Tat-Seng Chua, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on MultimediaYinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. 2021. Contrastive learning for cold-start recommendation. In Proceedings of the 29th ACM International Conference on Multimedia. 5382-5390.\n\nSimplifying graph convolutional networks. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger, PMLRInternational conference on machine learning. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In International conference on machine learning. PMLR, 6861-6871.\n\nSelf-supervised graph learning for recommendation. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalJiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 726-735.\n\nShiwen Wu, Wentao Zhang, Fei Sun, Bin Cui, arXiv:2011.02260Graph Neural Networks in Recommender Systems: A Survey. arXiv preprintShiwen Wu, Wentao Zhang, Fei Sun, and Bin Cui. 2020. Graph Neural Networks in Recommender Systems: A Survey. arXiv preprint arXiv:2011.02260 (2020).\n\nA comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, S Yu Philip, IEEE Transactions on Neural Networks and Learning Systems. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems (2020).\n\nSelf-Supervised Graph Co-Training for Session-based Recommendation. Xin Xia, Hongzhi Yin, Junliang Yu, Yingxia Shao, Lizhen Cui, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementXin Xia, Hongzhi Yin, Junliang Yu, Yingxia Shao, and Lizhen Cui. 2021. Self- Supervised Graph Co-Training for Session-based Recommendation. In Proceed- ings of the 30th ACM International Conference on Information & Knowledge Man- agement. 2180-2190.\n\nSelf-Supervised Hypergraph Convolutional Networks for Sessionbased Recommendation. Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, Xiangliang Zhang, Thirty-Fifth AAAI Conference on Artificial Intelligence. Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, and Xiangliang Zhang. 2021. Self-Supervised Hypergraph Convolutional Networks for Session- based Recommendation. In Thirty-Fifth AAAI Conference on Artificial Intelligence. 4503-4511.\n\nXu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, arXiv:2010.14395Bolin Ding, and Bin Cui. 2020. Contrastive Learning for Sequential Recommendation. arXiv preprintXu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020. Contrastive Learning for Sequential Recommendation. arXiv preprint arXiv:2010.14395 (2020).\n\nSelf-supervised Learning for Large-scale Item Recommendations. Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, H Ed, Steve Chi, Jieqi Tjoa, Kang, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementTiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al. 2021. Self-supervised Learning for Large-scale Item Recommendations. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 4321-4330.\n\nGraph Contrastive Learning with Augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in Neural Information Processing Systems. 33Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph Contrastive Learning with Augmentations. Advances in Neural Information Processing Systems 33 (2020).\n\nAdaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation. Junliang Yu, Min Gao, Jundong Li, Hongzhi Yin, Huan Liu, Proceedings of the 27th ACM International Conference on Information and Knowledge Management. the 27th ACM International Conference on Information and Knowledge ManagementACMJunliang Yu, Min Gao, Jundong Li, Hongzhi Yin, and Huan Liu. 2018. Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recom- mendation. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, 357-366.\n\nGenerating reliable friends via adversarial training to improve social recommendation. Junliang Yu, Min Gao, Hongzhi Yin, Jundong Li, Chongming Gao, Qinyong Wang, 2019 IEEE International Conference on Data Mining (ICDM). Junliang Yu, Min Gao, Hongzhi Yin, Jundong Li, Chongming Gao, and Qinyong Wang. 2019. Generating reliable friends via adversarial training to improve social recommendation. In 2019 IEEE International Conference on Data Mining (ICDM).\n\n. IEEE. IEEE, 768-777.\n\nXiangliang Zhang, and Nguyen Quoc Viet Hung. 2021. Socially-Aware Self-Supervised Tri-Training for Recommendation. Junliang Yu, Hongzhi Yin, Min Gao, Xin Xia, KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Beng Chin Ooi, and Chunyan MiaoJunliang Yu, Hongzhi Yin, Min Gao, Xin Xia, Xiangliang Zhang, and Nguyen Quoc Viet Hung. 2021. Socially-Aware Self-Supervised Tri-Training for Rec- ommendation. In KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.).\n\nJunliang Yu, Hongzhi Yin, Jundong Li, Min Gao, Zi Huang, Lizhen Cui, arXiv:2004.02340Enhance Social Recommendation with Adversarial Graph Convolutional Networks. arXiv preprintJunliang Yu, Hongzhi Yin, Jundong Li, Min Gao, Zi Huang, and Lizhen Cui. 2020. Enhance Social Recommendation with Adversarial Graph Convolutional Networks. arXiv preprint arXiv:2004.02340 (2020).\n\nSelf-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Proceedings of the Web Conference 2021Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convo- lutional Network for Social Recommendation. In Proceedings of the Web Confer- ence 2021. 413-424.\n\nJunliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, Zi Huang, arXiv:2203.15876Self-Supervised Learning for Recommender Systems: A Survey. arXiv preprintJunliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2022. Self-Supervised Learning for Recommender Systems: A Survey. arXiv preprint arXiv:2203.15876 (2022).\n\nGraph convolutional network for recommendation with low-pass collaborative filters. Wenhui Yu, Zheng Qin, International Conference on Machine Learning. PMLR. Wenhui Yu and Zheng Qin. 2020. Graph convolutional network for recommen- dation with low-pass collaborative filters. In International Conference on Machine Learning. PMLR, 10936-10945.\n\nDouble-Scale Self-Supervised Hypergraph Learning for Group Recommendation. Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, Hongzhi Yin, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementJunwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021. Double-Scale Self-Supervised Hypergraph Learning for Group Recommen- dation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2557-2567.\n\nContrastive learning for debiased candidate generation in large-scale recommender systems. Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, Hongxia Yang, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery & Data MiningChang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive learning for debiased candidate generation in large-scale recom- mender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 3985-3995.\n\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen, arXiv:2008.07873S\u02c63-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. arXiv preprintKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S\u02c63-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. arXiv preprint arXiv:2008.07873 (2020).\n\nXin Zhou, Aixin Sun, Yong Liu, Jie Zhang, Chunyan Miao, arXiv:2107.03019SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. arXiv preprintXin Zhou, Aixin Sun, Yong Liu, Jie Zhang, and Chunyan Miao. 2021. SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. arXiv preprint arXiv:2107.03019 (2021).\n", "annotations": {"author": "[{\"end\":171,\"start\":158},{\"end\":177,\"start\":172},{\"end\":227,\"start\":178},{\"end\":276,\"start\":228},{\"end\":325,\"start\":277},{\"end\":374,\"start\":326},{\"end\":408,\"start\":375},{\"end\":451,\"start\":409}]", "publisher": "[{\"end\":92,\"start\":89},{\"end\":810,\"start\":807}]", "author_last_name": "[{\"end\":170,\"start\":164},{\"end\":176,\"start\":172}]", "author_first_name": "[{\"end\":163,\"start\":158}]", "author_affiliation": "[{\"end\":226,\"start\":179},{\"end\":275,\"start\":229},{\"end\":324,\"start\":278},{\"end\":373,\"start\":327},{\"end\":407,\"start\":376},{\"end\":450,\"start\":410}]", "title": "[{\"end\":88,\"start\":1},{\"end\":539,\"start\":452}]", "venue": "[{\"end\":665,\"start\":541}]", "abstract": "[{\"end\":2580,\"start\":1029}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2652,\"start\":2648},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2655,\"start\":2652},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2658,\"start\":2655},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2910,\"start\":2907},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2912,\"start\":2910},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2915,\"start\":2912},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2918,\"start\":2915},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3040,\"start\":3037},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3043,\"start\":3040},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3093,\"start\":3089},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3096,\"start\":3093},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3099,\"start\":3096},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3102,\"start\":3099},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3105,\"start\":3102},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3108,\"start\":3105},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3235,\"start\":3231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4020,\"start\":4016},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4023,\"start\":4020},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4026,\"start\":4023},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5091,\"start\":5087},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5262,\"start\":5259},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5496,\"start\":5493},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5498,\"start\":5496},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7771,\"start\":7767},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7774,\"start\":7771},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7777,\"start\":7774},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7780,\"start\":7777},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7871,\"start\":7867},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7874,\"start\":7871},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8056,\"start\":8052},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8148,\"start\":8144},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8978,\"start\":8974},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9611,\"start\":9607},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9614,\"start\":9611},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10156,\"start\":10152},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10159,\"start\":10156},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11905,\"start\":11901},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12401,\"start\":12397},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12604,\"start\":12600},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12791,\"start\":12788},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13385,\"start\":13382},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13681,\"start\":13678},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13734,\"start\":13730},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14496,\"start\":14492},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16574,\"start\":16571},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19677,\"start\":19673},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21930,\"start\":21927},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23619,\"start\":23615},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23685,\"start\":23681},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23759,\"start\":23755},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23961,\"start\":23957},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23964,\"start\":23961},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32743,\"start\":32740},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33001,\"start\":32998},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34155,\"start\":34152},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34963,\"start\":34960},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34966,\"start\":34963},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35356,\"start\":35352},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35359,\"start\":35356},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35362,\"start\":35359},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35409,\"start\":35405},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35535,\"start\":35532},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":35546,\"start\":35542},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35561,\"start\":35557},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35575,\"start\":35571},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35834,\"start\":35830},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35956,\"start\":35952},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36189,\"start\":36185},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36203,\"start\":36199},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36291,\"start\":36287},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36361,\"start\":36357},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":36364,\"start\":36361},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36522,\"start\":36518},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36525,\"start\":36522},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36528,\"start\":36525},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":36531,\"start\":36528},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36534,\"start\":36531},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36537,\"start\":36534},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":36540,\"start\":36537},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":36558,\"start\":36554},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36725,\"start\":36721},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36983,\"start\":36979},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":37142,\"start\":37138},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37158,\"start\":37154},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37341,\"start\":37337},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37433,\"start\":37429},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":37452,\"start\":37448},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37472,\"start\":37468},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37674,\"start\":37670},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":39774,\"start\":39770},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":39952,\"start\":39948},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40092,\"start\":40088},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40255,\"start\":40251}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38823,\"start\":38746},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38903,\"start\":38824},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39130,\"start\":38904},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39220,\"start\":39131},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39374,\"start\":39221},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39758,\"start\":39375},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40379,\"start\":39759},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40439,\"start\":40380},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40501,\"start\":40440},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40548,\"start\":40502},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40923,\"start\":40549},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41026,\"start\":40924},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43006,\"start\":41027},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43657,\"start\":43007},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44130,\"start\":43658},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44614,\"start\":44131}]", "paragraph": "[{\"end\":3629,\"start\":2596},{\"end\":4453,\"start\":3631},{\"end\":6013,\"start\":4455},{\"end\":6840,\"start\":6015},{\"end\":6906,\"start\":6842},{\"end\":7298,\"start\":6908},{\"end\":7546,\"start\":7300},{\"end\":8214,\"start\":7644},{\"end\":8334,\"start\":8230},{\"end\":9040,\"start\":8395},{\"end\":9340,\"start\":9087},{\"end\":9615,\"start\":9423},{\"end\":10079,\"start\":9651},{\"end\":11853,\"start\":10127},{\"end\":12402,\"start\":11886},{\"end\":12605,\"start\":12404},{\"end\":13435,\"start\":12607},{\"end\":13752,\"start\":13437},{\"end\":14497,\"start\":13786},{\"end\":14577,\"start\":14499},{\"end\":15881,\"start\":14639},{\"end\":16331,\"start\":15946},{\"end\":16906,\"start\":16362},{\"end\":17863,\"start\":16944},{\"end\":18222,\"start\":17906},{\"end\":18809,\"start\":18339},{\"end\":19863,\"start\":18835},{\"end\":20541,\"start\":19924},{\"end\":21653,\"start\":20543},{\"end\":22113,\"start\":21668},{\"end\":23511,\"start\":22262},{\"end\":25142,\"start\":23560},{\"end\":26207,\"start\":25195},{\"end\":27206,\"start\":26241},{\"end\":28111,\"start\":27208},{\"end\":28157,\"start\":28127},{\"end\":28252,\"start\":28173},{\"end\":29216,\"start\":28281},{\"end\":29926,\"start\":29239},{\"end\":30813,\"start\":29928},{\"end\":30998,\"start\":30848},{\"end\":31572,\"start\":31000},{\"end\":32279,\"start\":31588},{\"end\":32885,\"start\":32325},{\"end\":32914,\"start\":32901},{\"end\":32986,\"start\":32930},{\"end\":33538,\"start\":32988},{\"end\":36204,\"start\":33596},{\"end\":37852,\"start\":36247},{\"end\":38123,\"start\":37867},{\"end\":38570,\"start\":38125},{\"end\":38745,\"start\":38572}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8229,\"start\":8215},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8394,\"start\":8335},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9086,\"start\":9041},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9422,\"start\":9341},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10126,\"start\":10080},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13785,\"start\":13753},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14638,\"start\":14578},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16943,\"start\":16907},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17887,\"start\":17864},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18338,\"start\":18223},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19923,\"start\":19864},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22261,\"start\":22114}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10347,\"start\":10340},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14943,\"start\":14936},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28395,\"start\":28388},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28496,\"start\":28489},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32482,\"start\":32475},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":33870,\"start\":33863},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":34178,\"start\":34171}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2594,\"start\":2582},{\"attributes\":{\"n\":\"2\"},\"end\":7642,\"start\":7549},{\"attributes\":{\"n\":\"2.2\"},\"end\":9649,\"start\":9618},{\"attributes\":{\"n\":\"2.3\"},\"end\":11884,\"start\":11856},{\"attributes\":{\"n\":\"3\"},\"end\":15944,\"start\":15884},{\"attributes\":{\"n\":\"3.1\"},\"end\":16360,\"start\":16334},{\"end\":17904,\"start\":17889},{\"attributes\":{\"n\":\"3.2\"},\"end\":18833,\"start\":18812},{\"attributes\":{\"n\":\"3.3\"},\"end\":21666,\"start\":21656},{\"attributes\":{\"n\":\"4\"},\"end\":23534,\"start\":23514},{\"attributes\":{\"n\":\"4.1\"},\"end\":23558,\"start\":23537},{\"attributes\":{\"n\":\"4.2\"},\"end\":25193,\"start\":25145},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":26239,\"start\":26210},{\"end\":28125,\"start\":28114},{\"end\":28171,\"start\":28160},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":28279,\"start\":28255},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":29237,\"start\":29219},{\"attributes\":{\"n\":\"4.3\"},\"end\":30846,\"start\":30816},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":31586,\"start\":31575},{\"attributes\":{\"n\":\"4.4\"},\"end\":32323,\"start\":32282},{\"end\":32899,\"start\":32888},{\"end\":32928,\"start\":32917},{\"attributes\":{\"n\":\"4.5\"},\"end\":33594,\"start\":33541},{\"attributes\":{\"n\":\"5.2\"},\"end\":36245,\"start\":36207},{\"attributes\":{\"n\":\"6\"},\"end\":37865,\"start\":37855},{\"end\":38757,\"start\":38747},{\"end\":38915,\"start\":38905},{\"end\":39142,\"start\":39132},{\"end\":39232,\"start\":39222},{\"end\":39377,\"start\":39376},{\"end\":39761,\"start\":39760},{\"end\":40391,\"start\":40381},{\"end\":40513,\"start\":40503},{\"end\":40559,\"start\":40550},{\"end\":40934,\"start\":40925},{\"end\":41037,\"start\":41028},{\"end\":43017,\"start\":43008},{\"end\":43668,\"start\":43659},{\"end\":44141,\"start\":44132}]", "table": "[{\"end\":40923,\"start\":40610},{\"end\":41026,\"start\":40969},{\"end\":43006,\"start\":41107},{\"end\":43657,\"start\":43083},{\"end\":44130,\"start\":43716},{\"end\":44614,\"start\":44200}]", "figure_caption": "[{\"end\":38823,\"start\":38759},{\"end\":38903,\"start\":38826},{\"end\":39130,\"start\":38917},{\"end\":39220,\"start\":39144},{\"end\":39374,\"start\":39234},{\"end\":39758,\"start\":39378},{\"end\":40379,\"start\":39762},{\"end\":40439,\"start\":40393},{\"end\":40501,\"start\":40442},{\"end\":40548,\"start\":40515},{\"end\":40610,\"start\":40561},{\"end\":40969,\"start\":40936},{\"end\":41107,\"start\":41039},{\"end\":43083,\"start\":43019},{\"end\":43716,\"start\":43670},{\"end\":44200,\"start\":44143}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3628,\"start\":3621},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6330,\"start\":6324},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12815,\"start\":12809},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12943,\"start\":12936},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14976,\"start\":14970},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17372,\"start\":17366},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20569,\"start\":20563},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21258,\"start\":21252},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22578,\"start\":22572},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26413,\"start\":26407},{\"end\":26424,\"start\":26418},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27788,\"start\":27782},{\"end\":28211,\"start\":28203},{\"end\":29925,\"start\":29919},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30405,\"start\":30399},{\"end\":30417,\"start\":30411},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":31113,\"start\":31107},{\"end\":31386,\"start\":31380},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":31432,\"start\":31426},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":31911,\"start\":31905},{\"end\":31978,\"start\":31972},{\"end\":32950,\"start\":32942}]", "bib_author_first_name": "[{\"end\":44694,\"start\":44688},{\"end\":44709,\"start\":44704},{\"end\":44724,\"start\":44717},{\"end\":44949,\"start\":44943},{\"end\":44965,\"start\":44959},{\"end\":44967,\"start\":44966},{\"end\":44977,\"start\":44974},{\"end\":45245,\"start\":45244},{\"end\":45261,\"start\":45255},{\"end\":45263,\"start\":45262},{\"end\":45275,\"start\":45271},{\"end\":45277,\"start\":45276},{\"end\":45485,\"start\":45479},{\"end\":45497,\"start\":45492},{\"end\":45509,\"start\":45504},{\"end\":45931,\"start\":45927},{\"end\":45943,\"start\":45938},{\"end\":45963,\"start\":45955},{\"end\":45981,\"start\":45973},{\"end\":46257,\"start\":46253},{\"end\":46265,\"start\":46263},{\"end\":46277,\"start\":46273},{\"end\":46289,\"start\":46282},{\"end\":46302,\"start\":46294},{\"end\":46315,\"start\":46308},{\"end\":46327,\"start\":46322},{\"end\":46341,\"start\":46334},{\"end\":46355,\"start\":46349},{\"end\":46369,\"start\":46361},{\"end\":46753,\"start\":46747},{\"end\":46768,\"start\":46759},{\"end\":46779,\"start\":46774},{\"end\":47075,\"start\":47074},{\"end\":47089,\"start\":47081},{\"end\":47111,\"start\":47102},{\"end\":47403,\"start\":47398},{\"end\":47413,\"start\":47409},{\"end\":47428,\"start\":47421},{\"end\":47441,\"start\":47434},{\"end\":47450,\"start\":47446},{\"end\":47946,\"start\":47938},{\"end\":47955,\"start\":47951},{\"end\":47967,\"start\":47962},{\"end\":47977,\"start\":47974},{\"end\":47991,\"start\":47982},{\"end\":48003,\"start\":47999},{\"end\":48613,\"start\":48606},{\"end\":48627,\"start\":48621},{\"end\":48638,\"start\":48634},{\"end\":48649,\"start\":48645},{\"end\":48664,\"start\":48656},{\"end\":48676,\"start\":48671},{\"end\":48686,\"start\":48683},{\"end\":49280,\"start\":49273},{\"end\":49294,\"start\":49289},{\"end\":49309,\"start\":49305},{\"end\":49321,\"start\":49316},{\"end\":49337,\"start\":49329},{\"end\":49351,\"start\":49344},{\"end\":49364,\"start\":49359},{\"end\":49378,\"start\":49376},{\"end\":49389,\"start\":49384},{\"end\":49757,\"start\":49756},{\"end\":49769,\"start\":49766},{\"end\":50109,\"start\":50103},{\"end\":50122,\"start\":50115},{\"end\":50136,\"start\":50129},{\"end\":50150,\"start\":50141},{\"end\":50163,\"start\":50157},{\"end\":50782,\"start\":50777},{\"end\":50791,\"start\":50790},{\"end\":50810,\"start\":50809},{\"end\":50824,\"start\":50820},{\"end\":51175,\"start\":51171},{\"end\":51187,\"start\":51181},{\"end\":51201,\"start\":51195},{\"end\":51213,\"start\":51207},{\"end\":51222,\"start\":51220},{\"end\":51233,\"start\":51229},{\"end\":51244,\"start\":51241},{\"end\":51536,\"start\":51529},{\"end\":51546,\"start\":51541},{\"end\":51560,\"start\":51553},{\"end\":51571,\"start\":51567},{\"end\":51580,\"start\":51577},{\"end\":51592,\"start\":51587},{\"end\":52030,\"start\":52025},{\"end\":52050,\"start\":52045},{\"end\":52060,\"start\":52055},{\"end\":52416,\"start\":52409},{\"end\":52424,\"start\":52422},{\"end\":52439,\"start\":52432},{\"end\":52822,\"start\":52815},{\"end\":52830,\"start\":52828},{\"end\":52845,\"start\":52838},{\"end\":52857,\"start\":52851},{\"end\":53345,\"start\":53338},{\"end\":53363,\"start\":53354},{\"end\":53383,\"start\":53379},{\"end\":53397,\"start\":53393},{\"end\":53905,\"start\":53899},{\"end\":54057,\"start\":54050},{\"end\":54082,\"start\":54074},{\"end\":54375,\"start\":54367},{\"end\":54389,\"start\":54382},{\"end\":54701,\"start\":54696},{\"end\":54716,\"start\":54708},{\"end\":54725,\"start\":54721},{\"end\":54736,\"start\":54732},{\"end\":54751,\"start\":54743},{\"end\":55261,\"start\":55255},{\"end\":55272,\"start\":55267},{\"end\":55281,\"start\":55279},{\"end\":55293,\"start\":55286},{\"end\":55302,\"start\":55299},{\"end\":55315,\"start\":55307},{\"end\":55328,\"start\":55320},{\"end\":55726,\"start\":55721},{\"end\":55737,\"start\":55731},{\"end\":55751,\"start\":55745},{\"end\":55770,\"start\":55759},{\"end\":55781,\"start\":55778},{\"end\":55792,\"start\":55786},{\"end\":56116,\"start\":56109},{\"end\":56126,\"start\":56121},{\"end\":56137,\"start\":56133},{\"end\":56152,\"start\":56144},{\"end\":56162,\"start\":56157},{\"end\":56176,\"start\":56169},{\"end\":56187,\"start\":56183},{\"end\":56679,\"start\":56673},{\"end\":56690,\"start\":56684},{\"end\":56701,\"start\":56698},{\"end\":56710,\"start\":56707},{\"end\":57008,\"start\":57001},{\"end\":57019,\"start\":57013},{\"end\":57032,\"start\":57025},{\"end\":57046,\"start\":57039},{\"end\":57060,\"start\":57053},{\"end\":57072,\"start\":57068},{\"end\":57417,\"start\":57414},{\"end\":57430,\"start\":57423},{\"end\":57444,\"start\":57436},{\"end\":57456,\"start\":57449},{\"end\":57469,\"start\":57463},{\"end\":57979,\"start\":57976},{\"end\":57992,\"start\":57985},{\"end\":58006,\"start\":57998},{\"end\":58018,\"start\":58011},{\"end\":58031,\"start\":58025},{\"end\":58047,\"start\":58037},{\"end\":58360,\"start\":58358},{\"end\":58369,\"start\":58366},{\"end\":58383,\"start\":58375},{\"end\":58395,\"start\":58389},{\"end\":58407,\"start\":58400},{\"end\":58777,\"start\":58768},{\"end\":58790,\"start\":58783},{\"end\":58800,\"start\":58795},{\"end\":58808,\"start\":58801},{\"end\":58821,\"start\":58816},{\"end\":58830,\"start\":58826},{\"end\":58843,\"start\":58837},{\"end\":58857,\"start\":58851},{\"end\":58865,\"start\":58864},{\"end\":58875,\"start\":58870},{\"end\":58886,\"start\":58881},{\"end\":59432,\"start\":59426},{\"end\":59446,\"start\":59438},{\"end\":59460,\"start\":59453},{\"end\":59470,\"start\":59466},{\"end\":59486,\"start\":59477},{\"end\":59497,\"start\":59493},{\"end\":59857,\"start\":59849},{\"end\":59865,\"start\":59862},{\"end\":59878,\"start\":59871},{\"end\":59890,\"start\":59883},{\"end\":59900,\"start\":59896},{\"end\":60451,\"start\":60443},{\"end\":60459,\"start\":60456},{\"end\":60472,\"start\":60465},{\"end\":60485,\"start\":60478},{\"end\":60499,\"start\":60490},{\"end\":60512,\"start\":60505},{\"end\":60959,\"start\":60951},{\"end\":60971,\"start\":60964},{\"end\":60980,\"start\":60977},{\"end\":60989,\"start\":60986},{\"end\":61410,\"start\":61402},{\"end\":61422,\"start\":61415},{\"end\":61435,\"start\":61428},{\"end\":61443,\"start\":61440},{\"end\":61451,\"start\":61449},{\"end\":61465,\"start\":61459},{\"end\":61873,\"start\":61865},{\"end\":61885,\"start\":61878},{\"end\":61898,\"start\":61891},{\"end\":61910,\"start\":61903},{\"end\":62264,\"start\":62256},{\"end\":62276,\"start\":62269},{\"end\":62285,\"start\":62282},{\"end\":62295,\"start\":62291},{\"end\":62309,\"start\":62302},{\"end\":62316,\"start\":62314},{\"end\":62683,\"start\":62677},{\"end\":62693,\"start\":62688},{\"end\":63018,\"start\":63012},{\"end\":63029,\"start\":63026},{\"end\":63043,\"start\":63035},{\"end\":63051,\"start\":63048},{\"end\":63064,\"start\":63057},{\"end\":63076,\"start\":63069},{\"end\":63609,\"start\":63604},{\"end\":63623,\"start\":63616},{\"end\":63635,\"start\":63628},{\"end\":63650,\"start\":63643},{\"end\":63664,\"start\":63657},{\"end\":64094,\"start\":64091},{\"end\":64104,\"start\":64101},{\"end\":64116,\"start\":64111},{\"end\":64120,\"start\":64117},{\"end\":64132,\"start\":64127},{\"end\":64143,\"start\":64138},{\"end\":64157,\"start\":64150},{\"end\":64174,\"start\":64165},{\"end\":64188,\"start\":64181},{\"end\":64585,\"start\":64582},{\"end\":64597,\"start\":64592},{\"end\":64607,\"start\":64603},{\"end\":64616,\"start\":64613},{\"end\":64631,\"start\":64624}]", "bib_author_last_name": "[{\"end\":44702,\"start\":44695},{\"end\":44715,\"start\":44710},{\"end\":44735,\"start\":44725},{\"end\":44957,\"start\":44950},{\"end\":44972,\"start\":44968},{\"end\":44982,\"start\":44978},{\"end\":44991,\"start\":44984},{\"end\":45253,\"start\":45246},{\"end\":45269,\"start\":45264},{\"end\":45287,\"start\":45278},{\"end\":45295,\"start\":45289},{\"end\":45490,\"start\":45486},{\"end\":45502,\"start\":45498},{\"end\":45514,\"start\":45510},{\"end\":45936,\"start\":45932},{\"end\":45953,\"start\":45944},{\"end\":45971,\"start\":45964},{\"end\":45988,\"start\":45982},{\"end\":46261,\"start\":46258},{\"end\":46271,\"start\":46266},{\"end\":46280,\"start\":46278},{\"end\":46292,\"start\":46290},{\"end\":46306,\"start\":46303},{\"end\":46320,\"start\":46316},{\"end\":46332,\"start\":46328},{\"end\":46347,\"start\":46342},{\"end\":46359,\"start\":46356},{\"end\":46372,\"start\":46370},{\"end\":46757,\"start\":46754},{\"end\":46772,\"start\":46769},{\"end\":46784,\"start\":46780},{\"end\":47079,\"start\":47076},{\"end\":47100,\"start\":47090},{\"end\":47118,\"start\":47112},{\"end\":47127,\"start\":47120},{\"end\":47407,\"start\":47404},{\"end\":47419,\"start\":47414},{\"end\":47432,\"start\":47429},{\"end\":47444,\"start\":47442},{\"end\":47455,\"start\":47451},{\"end\":47949,\"start\":47947},{\"end\":47960,\"start\":47956},{\"end\":47972,\"start\":47968},{\"end\":47980,\"start\":47978},{\"end\":47997,\"start\":47992},{\"end\":48008,\"start\":48004},{\"end\":48619,\"start\":48614},{\"end\":48632,\"start\":48628},{\"end\":48643,\"start\":48639},{\"end\":48654,\"start\":48650},{\"end\":48669,\"start\":48665},{\"end\":48681,\"start\":48677},{\"end\":48691,\"start\":48687},{\"end\":49287,\"start\":49281},{\"end\":49303,\"start\":49295},{\"end\":49314,\"start\":49310},{\"end\":49327,\"start\":49322},{\"end\":49342,\"start\":49338},{\"end\":49357,\"start\":49352},{\"end\":49374,\"start\":49365},{\"end\":49382,\"start\":49379},{\"end\":49398,\"start\":49390},{\"end\":49764,\"start\":49758},{\"end\":49774,\"start\":49770},{\"end\":49783,\"start\":49776},{\"end\":50113,\"start\":50110},{\"end\":50127,\"start\":50123},{\"end\":50139,\"start\":50137},{\"end\":50155,\"start\":50151},{\"end\":50166,\"start\":50164},{\"end\":50788,\"start\":50783},{\"end\":50797,\"start\":50792},{\"end\":50807,\"start\":50799},{\"end\":50818,\"start\":50811},{\"end\":50832,\"start\":50825},{\"end\":50840,\"start\":50834},{\"end\":51179,\"start\":51176},{\"end\":51193,\"start\":51188},{\"end\":51205,\"start\":51202},{\"end\":51218,\"start\":51214},{\"end\":51227,\"start\":51223},{\"end\":51239,\"start\":51234},{\"end\":51249,\"start\":51245},{\"end\":51539,\"start\":51537},{\"end\":51551,\"start\":51547},{\"end\":51565,\"start\":51561},{\"end\":51575,\"start\":51572},{\"end\":51585,\"start\":51581},{\"end\":51596,\"start\":51593},{\"end\":52043,\"start\":52031},{\"end\":52053,\"start\":52051},{\"end\":52068,\"start\":52061},{\"end\":52420,\"start\":52417},{\"end\":52430,\"start\":52425},{\"end\":52443,\"start\":52440},{\"end\":52826,\"start\":52823},{\"end\":52836,\"start\":52831},{\"end\":52849,\"start\":52846},{\"end\":52862,\"start\":52858},{\"end\":53352,\"start\":53346},{\"end\":53377,\"start\":53364},{\"end\":53391,\"start\":53384},{\"end\":53412,\"start\":53398},{\"end\":53918,\"start\":53906},{\"end\":54072,\"start\":54058},{\"end\":54089,\"start\":54083},{\"end\":54380,\"start\":54376},{\"end\":54395,\"start\":54390},{\"end\":54706,\"start\":54702},{\"end\":54719,\"start\":54717},{\"end\":54730,\"start\":54726},{\"end\":54741,\"start\":54737},{\"end\":54756,\"start\":54752},{\"end\":55265,\"start\":55262},{\"end\":55277,\"start\":55273},{\"end\":55284,\"start\":55282},{\"end\":55297,\"start\":55294},{\"end\":55305,\"start\":55303},{\"end\":55318,\"start\":55316},{\"end\":55333,\"start\":55329},{\"end\":55729,\"start\":55727},{\"end\":55743,\"start\":55738},{\"end\":55757,\"start\":55752},{\"end\":55776,\"start\":55771},{\"end\":55784,\"start\":55782},{\"end\":55803,\"start\":55793},{\"end\":56119,\"start\":56117},{\"end\":56131,\"start\":56127},{\"end\":56142,\"start\":56138},{\"end\":56155,\"start\":56153},{\"end\":56167,\"start\":56163},{\"end\":56181,\"start\":56177},{\"end\":56191,\"start\":56188},{\"end\":56682,\"start\":56680},{\"end\":56696,\"start\":56691},{\"end\":56705,\"start\":56702},{\"end\":56714,\"start\":56711},{\"end\":57011,\"start\":57009},{\"end\":57023,\"start\":57020},{\"end\":57037,\"start\":57033},{\"end\":57051,\"start\":57047},{\"end\":57066,\"start\":57061},{\"end\":57079,\"start\":57073},{\"end\":57421,\"start\":57418},{\"end\":57434,\"start\":57431},{\"end\":57447,\"start\":57445},{\"end\":57461,\"start\":57457},{\"end\":57473,\"start\":57470},{\"end\":57983,\"start\":57980},{\"end\":57996,\"start\":57993},{\"end\":58009,\"start\":58007},{\"end\":58023,\"start\":58019},{\"end\":58035,\"start\":58032},{\"end\":58053,\"start\":58048},{\"end\":58364,\"start\":58361},{\"end\":58373,\"start\":58370},{\"end\":58387,\"start\":58384},{\"end\":58398,\"start\":58396},{\"end\":58411,\"start\":58408},{\"end\":58781,\"start\":58778},{\"end\":58793,\"start\":58791},{\"end\":58814,\"start\":58809},{\"end\":58824,\"start\":58822},{\"end\":58835,\"start\":58831},{\"end\":58849,\"start\":58844},{\"end\":58862,\"start\":58858},{\"end\":58868,\"start\":58866},{\"end\":58879,\"start\":58876},{\"end\":58891,\"start\":58887},{\"end\":58897,\"start\":58893},{\"end\":59436,\"start\":59433},{\"end\":59451,\"start\":59447},{\"end\":59464,\"start\":59461},{\"end\":59475,\"start\":59471},{\"end\":59491,\"start\":59487},{\"end\":59502,\"start\":59498},{\"end\":59860,\"start\":59858},{\"end\":59869,\"start\":59866},{\"end\":59881,\"start\":59879},{\"end\":59894,\"start\":59891},{\"end\":59904,\"start\":59901},{\"end\":60454,\"start\":60452},{\"end\":60463,\"start\":60460},{\"end\":60476,\"start\":60473},{\"end\":60488,\"start\":60486},{\"end\":60503,\"start\":60500},{\"end\":60517,\"start\":60513},{\"end\":60962,\"start\":60960},{\"end\":60975,\"start\":60972},{\"end\":60984,\"start\":60981},{\"end\":60993,\"start\":60990},{\"end\":61413,\"start\":61411},{\"end\":61426,\"start\":61423},{\"end\":61438,\"start\":61436},{\"end\":61447,\"start\":61444},{\"end\":61457,\"start\":61452},{\"end\":61469,\"start\":61466},{\"end\":61876,\"start\":61874},{\"end\":61889,\"start\":61886},{\"end\":61901,\"start\":61899},{\"end\":61915,\"start\":61911},{\"end\":62267,\"start\":62265},{\"end\":62280,\"start\":62277},{\"end\":62289,\"start\":62286},{\"end\":62300,\"start\":62296},{\"end\":62312,\"start\":62310},{\"end\":62322,\"start\":62317},{\"end\":62686,\"start\":62684},{\"end\":62697,\"start\":62694},{\"end\":63024,\"start\":63019},{\"end\":63033,\"start\":63030},{\"end\":63046,\"start\":63044},{\"end\":63055,\"start\":63052},{\"end\":63067,\"start\":63065},{\"end\":63080,\"start\":63077},{\"end\":63614,\"start\":63610},{\"end\":63626,\"start\":63624},{\"end\":63641,\"start\":63636},{\"end\":63655,\"start\":63651},{\"end\":63669,\"start\":63665},{\"end\":64099,\"start\":64095},{\"end\":64109,\"start\":64105},{\"end\":64125,\"start\":64121},{\"end\":64136,\"start\":64133},{\"end\":64148,\"start\":64144},{\"end\":64163,\"start\":64158},{\"end\":64179,\"start\":64175},{\"end\":64192,\"start\":64189},{\"end\":64590,\"start\":64586},{\"end\":64601,\"start\":64598},{\"end\":64611,\"start\":64608},{\"end\":64622,\"start\":64617},{\"end\":64636,\"start\":64632}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1906.00910\",\"id\":\"b0\"},\"end\":44941,\"start\":44616},{\"attributes\":{\"doi\":\"arXiv:1706.02263\",\"id\":\"b1\"},\"end\":45201,\"start\":44943},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":41350591},\"end\":45477,\"start\":45203},{\"attributes\":{\"doi\":\"arXiv:2010.03240\",\"id\":\"b3\"},\"end\":45854,\"start\":45479},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b4\",\"matched_paper_id\":211096730},\"end\":46251,\"start\":45856},{\"attributes\":{\"doi\":\"arXiv:2109.12843\",\"id\":\"b5\"},\"end\":46745,\"start\":46253},{\"attributes\":{\"doi\":\"arXiv:2104.08821\",\"id\":\"b6\"},\"end\":47024,\"start\":46747},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b7\"},\"end\":47315,\"start\":47026},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":229155989},\"end\":47855,\"start\":47317},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211043589},\"end\":48516,\"start\":47857},{\"attributes\":{\"id\":\"b10\"},\"end\":48885,\"start\":48518},{\"attributes\":{\"id\":\"b11\"},\"end\":49238,\"start\":48887},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":216080787},\"end\":49688,\"start\":49240},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3144218},\"end\":50018,\"start\":49690},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":234482617},\"end\":50721,\"start\":50020},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3361310},\"end\":51116,\"start\":50723},{\"attributes\":{\"doi\":\"arXiv:2006.08218\",\"id\":\"b16\"},\"end\":51469,\"start\":51118},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":221191218},\"end\":52023,\"start\":51471},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b18\"},\"end\":52314,\"start\":52025},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":237372377},\"end\":52722,\"start\":52316},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":238634419},\"end\":53277,\"start\":52724},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10795036},\"end\":53833,\"start\":53279},{\"attributes\":{\"id\":\"b22\"},\"end\":54018,\"start\":53835},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5855042},\"end\":54262,\"start\":54020},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":218718310},\"end\":54656,\"start\":54264},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":150380651},\"end\":55201,\"start\":54658},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":235794926},\"end\":55677,\"start\":55203},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b27\",\"matched_paper_id\":67752026},\"end\":56056,\"start\":55679},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":224814335},\"end\":56671,\"start\":56058},{\"attributes\":{\"doi\":\"arXiv:2011.02260\",\"id\":\"b29\"},\"end\":56950,\"start\":56673},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":57375753},\"end\":57344,\"start\":56952},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":237279028},\"end\":57891,\"start\":57346},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":229156813},\"end\":58356,\"start\":57893},{\"attributes\":{\"doi\":\"arXiv:2010.14395\",\"id\":\"b33\"},\"end\":58703,\"start\":58358},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":225053992},\"end\":59377,\"start\":58705},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":225076220},\"end\":59752,\"start\":59379},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52228661},\"end\":60354,\"start\":59754},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":202538667},\"end\":60810,\"start\":60356},{\"attributes\":{\"id\":\"b38\"},\"end\":60834,\"start\":60812},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":235358394},\"end\":61400,\"start\":60836},{\"attributes\":{\"doi\":\"arXiv:2004.02340\",\"id\":\"b40\"},\"end\":61773,\"start\":61402},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":231632459},\"end\":62254,\"start\":61775},{\"attributes\":{\"doi\":\"arXiv:2203.15876\",\"id\":\"b42\"},\"end\":62591,\"start\":62256},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":220250619},\"end\":62935,\"start\":62593},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":237454580},\"end\":63511,\"start\":62937},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":219434009},\"end\":64089,\"start\":63513},{\"attributes\":{\"doi\":\"arXiv:2008.07873\",\"id\":\"b46\"},\"end\":64580,\"start\":64091},{\"attributes\":{\"doi\":\"arXiv:2107.03019\",\"id\":\"b47\"},\"end\":64917,\"start\":64582}]", "bib_title": "[{\"end\":45242,\"start\":45203},{\"end\":45925,\"start\":45856},{\"end\":47396,\"start\":47317},{\"end\":47936,\"start\":47857},{\"end\":48981,\"start\":48887},{\"end\":49271,\"start\":49240},{\"end\":49754,\"start\":49690},{\"end\":50101,\"start\":50020},{\"end\":50775,\"start\":50723},{\"end\":51527,\"start\":51471},{\"end\":52407,\"start\":52316},{\"end\":52813,\"start\":52724},{\"end\":53336,\"start\":53279},{\"end\":54048,\"start\":54020},{\"end\":54365,\"start\":54264},{\"end\":54694,\"start\":54658},{\"end\":55253,\"start\":55203},{\"end\":55719,\"start\":55679},{\"end\":56107,\"start\":56058},{\"end\":56999,\"start\":56952},{\"end\":57412,\"start\":57346},{\"end\":57974,\"start\":57893},{\"end\":58766,\"start\":58705},{\"end\":59424,\"start\":59379},{\"end\":59847,\"start\":59754},{\"end\":60441,\"start\":60356},{\"end\":60949,\"start\":60836},{\"end\":61863,\"start\":61775},{\"end\":62675,\"start\":62593},{\"end\":63010,\"start\":62937},{\"end\":63602,\"start\":63513}]", "bib_author": "[{\"end\":44704,\"start\":44688},{\"end\":44717,\"start\":44704},{\"end\":44737,\"start\":44717},{\"end\":44959,\"start\":44943},{\"end\":44974,\"start\":44959},{\"end\":44984,\"start\":44974},{\"end\":44993,\"start\":44984},{\"end\":45255,\"start\":45244},{\"end\":45271,\"start\":45255},{\"end\":45289,\"start\":45271},{\"end\":45297,\"start\":45289},{\"end\":45492,\"start\":45479},{\"end\":45504,\"start\":45492},{\"end\":45516,\"start\":45504},{\"end\":45938,\"start\":45927},{\"end\":45955,\"start\":45938},{\"end\":45973,\"start\":45955},{\"end\":45990,\"start\":45973},{\"end\":46263,\"start\":46253},{\"end\":46273,\"start\":46263},{\"end\":46282,\"start\":46273},{\"end\":46294,\"start\":46282},{\"end\":46308,\"start\":46294},{\"end\":46322,\"start\":46308},{\"end\":46334,\"start\":46322},{\"end\":46349,\"start\":46334},{\"end\":46361,\"start\":46349},{\"end\":46374,\"start\":46361},{\"end\":46759,\"start\":46747},{\"end\":46774,\"start\":46759},{\"end\":46786,\"start\":46774},{\"end\":47081,\"start\":47074},{\"end\":47102,\"start\":47081},{\"end\":47120,\"start\":47102},{\"end\":47129,\"start\":47120},{\"end\":47409,\"start\":47398},{\"end\":47421,\"start\":47409},{\"end\":47434,\"start\":47421},{\"end\":47446,\"start\":47434},{\"end\":47457,\"start\":47446},{\"end\":47951,\"start\":47938},{\"end\":47962,\"start\":47951},{\"end\":47974,\"start\":47962},{\"end\":47982,\"start\":47974},{\"end\":47999,\"start\":47982},{\"end\":48010,\"start\":47999},{\"end\":48621,\"start\":48606},{\"end\":48634,\"start\":48621},{\"end\":48645,\"start\":48634},{\"end\":48656,\"start\":48645},{\"end\":48671,\"start\":48656},{\"end\":48683,\"start\":48671},{\"end\":48693,\"start\":48683},{\"end\":49289,\"start\":49273},{\"end\":49305,\"start\":49289},{\"end\":49316,\"start\":49305},{\"end\":49329,\"start\":49316},{\"end\":49344,\"start\":49329},{\"end\":49359,\"start\":49344},{\"end\":49376,\"start\":49359},{\"end\":49384,\"start\":49376},{\"end\":49400,\"start\":49384},{\"end\":49766,\"start\":49756},{\"end\":49776,\"start\":49766},{\"end\":49785,\"start\":49776},{\"end\":50115,\"start\":50103},{\"end\":50129,\"start\":50115},{\"end\":50141,\"start\":50129},{\"end\":50157,\"start\":50141},{\"end\":50168,\"start\":50157},{\"end\":50790,\"start\":50777},{\"end\":50799,\"start\":50790},{\"end\":50809,\"start\":50799},{\"end\":50820,\"start\":50809},{\"end\":50834,\"start\":50820},{\"end\":50842,\"start\":50834},{\"end\":51181,\"start\":51171},{\"end\":51195,\"start\":51181},{\"end\":51207,\"start\":51195},{\"end\":51220,\"start\":51207},{\"end\":51229,\"start\":51220},{\"end\":51241,\"start\":51229},{\"end\":51251,\"start\":51241},{\"end\":51541,\"start\":51529},{\"end\":51553,\"start\":51541},{\"end\":51567,\"start\":51553},{\"end\":51577,\"start\":51567},{\"end\":51587,\"start\":51577},{\"end\":51598,\"start\":51587},{\"end\":52045,\"start\":52025},{\"end\":52055,\"start\":52045},{\"end\":52070,\"start\":52055},{\"end\":52422,\"start\":52409},{\"end\":52432,\"start\":52422},{\"end\":52445,\"start\":52432},{\"end\":52828,\"start\":52815},{\"end\":52838,\"start\":52828},{\"end\":52851,\"start\":52838},{\"end\":52864,\"start\":52851},{\"end\":53354,\"start\":53338},{\"end\":53379,\"start\":53354},{\"end\":53393,\"start\":53379},{\"end\":53414,\"start\":53393},{\"end\":53920,\"start\":53899},{\"end\":54074,\"start\":54050},{\"end\":54091,\"start\":54074},{\"end\":54382,\"start\":54367},{\"end\":54397,\"start\":54382},{\"end\":54708,\"start\":54696},{\"end\":54721,\"start\":54708},{\"end\":54732,\"start\":54721},{\"end\":54743,\"start\":54732},{\"end\":54758,\"start\":54743},{\"end\":55267,\"start\":55255},{\"end\":55279,\"start\":55267},{\"end\":55286,\"start\":55279},{\"end\":55299,\"start\":55286},{\"end\":55307,\"start\":55299},{\"end\":55320,\"start\":55307},{\"end\":55335,\"start\":55320},{\"end\":55731,\"start\":55721},{\"end\":55745,\"start\":55731},{\"end\":55759,\"start\":55745},{\"end\":55778,\"start\":55759},{\"end\":55786,\"start\":55778},{\"end\":55805,\"start\":55786},{\"end\":56121,\"start\":56109},{\"end\":56133,\"start\":56121},{\"end\":56144,\"start\":56133},{\"end\":56157,\"start\":56144},{\"end\":56169,\"start\":56157},{\"end\":56183,\"start\":56169},{\"end\":56193,\"start\":56183},{\"end\":56684,\"start\":56673},{\"end\":56698,\"start\":56684},{\"end\":56707,\"start\":56698},{\"end\":56716,\"start\":56707},{\"end\":57013,\"start\":57001},{\"end\":57025,\"start\":57013},{\"end\":57039,\"start\":57025},{\"end\":57053,\"start\":57039},{\"end\":57068,\"start\":57053},{\"end\":57081,\"start\":57068},{\"end\":57423,\"start\":57414},{\"end\":57436,\"start\":57423},{\"end\":57449,\"start\":57436},{\"end\":57463,\"start\":57449},{\"end\":57475,\"start\":57463},{\"end\":57985,\"start\":57976},{\"end\":57998,\"start\":57985},{\"end\":58011,\"start\":57998},{\"end\":58025,\"start\":58011},{\"end\":58037,\"start\":58025},{\"end\":58055,\"start\":58037},{\"end\":58366,\"start\":58358},{\"end\":58375,\"start\":58366},{\"end\":58389,\"start\":58375},{\"end\":58400,\"start\":58389},{\"end\":58413,\"start\":58400},{\"end\":58783,\"start\":58768},{\"end\":58795,\"start\":58783},{\"end\":58816,\"start\":58795},{\"end\":58826,\"start\":58816},{\"end\":58837,\"start\":58826},{\"end\":58851,\"start\":58837},{\"end\":58864,\"start\":58851},{\"end\":58870,\"start\":58864},{\"end\":58881,\"start\":58870},{\"end\":58893,\"start\":58881},{\"end\":58899,\"start\":58893},{\"end\":59438,\"start\":59426},{\"end\":59453,\"start\":59438},{\"end\":59466,\"start\":59453},{\"end\":59477,\"start\":59466},{\"end\":59493,\"start\":59477},{\"end\":59504,\"start\":59493},{\"end\":59862,\"start\":59849},{\"end\":59871,\"start\":59862},{\"end\":59883,\"start\":59871},{\"end\":59896,\"start\":59883},{\"end\":59906,\"start\":59896},{\"end\":60456,\"start\":60443},{\"end\":60465,\"start\":60456},{\"end\":60478,\"start\":60465},{\"end\":60490,\"start\":60478},{\"end\":60505,\"start\":60490},{\"end\":60519,\"start\":60505},{\"end\":60964,\"start\":60951},{\"end\":60977,\"start\":60964},{\"end\":60986,\"start\":60977},{\"end\":60995,\"start\":60986},{\"end\":61415,\"start\":61402},{\"end\":61428,\"start\":61415},{\"end\":61440,\"start\":61428},{\"end\":61449,\"start\":61440},{\"end\":61459,\"start\":61449},{\"end\":61471,\"start\":61459},{\"end\":61878,\"start\":61865},{\"end\":61891,\"start\":61878},{\"end\":61903,\"start\":61891},{\"end\":61917,\"start\":61903},{\"end\":62269,\"start\":62256},{\"end\":62282,\"start\":62269},{\"end\":62291,\"start\":62282},{\"end\":62302,\"start\":62291},{\"end\":62314,\"start\":62302},{\"end\":62324,\"start\":62314},{\"end\":62688,\"start\":62677},{\"end\":62699,\"start\":62688},{\"end\":63026,\"start\":63012},{\"end\":63035,\"start\":63026},{\"end\":63048,\"start\":63035},{\"end\":63057,\"start\":63048},{\"end\":63069,\"start\":63057},{\"end\":63082,\"start\":63069},{\"end\":63616,\"start\":63604},{\"end\":63628,\"start\":63616},{\"end\":63643,\"start\":63628},{\"end\":63657,\"start\":63643},{\"end\":63671,\"start\":63657},{\"end\":64101,\"start\":64091},{\"end\":64111,\"start\":64101},{\"end\":64127,\"start\":64111},{\"end\":64138,\"start\":64127},{\"end\":64150,\"start\":64138},{\"end\":64165,\"start\":64150},{\"end\":64181,\"start\":64165},{\"end\":64194,\"start\":64181},{\"end\":64592,\"start\":64582},{\"end\":64603,\"start\":64592},{\"end\":64613,\"start\":64603},{\"end\":64624,\"start\":64613},{\"end\":64638,\"start\":64624}]", "bib_venue": "[{\"end\":47608,\"start\":47541},{\"end\":48219,\"start\":48123},{\"end\":50353,\"start\":50290},{\"end\":50927,\"start\":50893},{\"end\":51777,\"start\":51696},{\"end\":53025,\"start\":52953},{\"end\":53569,\"start\":53500},{\"end\":54967,\"start\":54871},{\"end\":55454,\"start\":55403},{\"end\":56402,\"start\":56306},{\"end\":57642,\"start\":57567},{\"end\":59066,\"start\":58991},{\"end\":60077,\"start\":60000},{\"end\":63249,\"start\":63174},{\"end\":63822,\"start\":63755},{\"end\":44686,\"start\":44616},{\"end\":45046,\"start\":45009},{\"end\":45321,\"start\":45297},{\"end\":45646,\"start\":45532},{\"end\":46038,\"start\":45994},{\"end\":46472,\"start\":46390},{\"end\":46860,\"start\":46802},{\"end\":47072,\"start\":47026},{\"end\":47539,\"start\":47457},{\"end\":48121,\"start\":48010},{\"end\":48604,\"start\":48518},{\"end\":48995,\"start\":48983},{\"end\":49449,\"start\":49400},{\"end\":49841,\"start\":49785},{\"end\":50264,\"start\":50168},{\"end\":50891,\"start\":50842},{\"end\":51169,\"start\":51118},{\"end\":51694,\"start\":51598},{\"end\":52144,\"start\":52086},{\"end\":52501,\"start\":52445},{\"end\":52951,\"start\":52864},{\"end\":53498,\"start\":53414},{\"end\":53897,\"start\":53835},{\"end\":54127,\"start\":54091},{\"end\":54447,\"start\":54397},{\"end\":54869,\"start\":54758},{\"end\":55401,\"start\":55335},{\"end\":55853,\"start\":55809},{\"end\":56304,\"start\":56193},{\"end\":56786,\"start\":56732},{\"end\":57138,\"start\":57081},{\"end\":57565,\"start\":57475},{\"end\":58110,\"start\":58055},{\"end\":58510,\"start\":58429},{\"end\":58989,\"start\":58899},{\"end\":59553,\"start\":59504},{\"end\":59998,\"start\":59906},{\"end\":60575,\"start\":60519},{\"end\":60818,\"start\":60814},{\"end\":61073,\"start\":60995},{\"end\":61562,\"start\":61487},{\"end\":61966,\"start\":61917},{\"end\":62398,\"start\":62340},{\"end\":62749,\"start\":62699},{\"end\":63172,\"start\":63082},{\"end\":63753,\"start\":63671},{\"end\":64310,\"start\":64210},{\"end\":64724,\"start\":64654}]"}}}, "year": 2023, "month": 12, "day": 17}