{"id": 251018277, "updated": "2023-10-05 12:05:50.397", "metadata": {"title": "Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation", "authors": "[{\"first\":\"Sunghwan\",\"last\":\"Hong\",\"middle\":[]},{\"first\":\"Seokju\",\"last\":\"Cho\",\"middle\":[]},{\"first\":\"Jisu\",\"last\":\"Nam\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Seungryong\",\"last\":\"Kim\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper presents a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), for few-shot segmentation. The use of transformers can benefit correlation map aggregation through self-attention over a global receptive field. However, the tokenization of a correlation map for transformer processing can be detrimental, because the discontinuity at token boundaries reduces the local context available near the token edges and decreases inductive bias. To address this problem, we propose a 4D Convolutional Swin Transformer, where a high-dimensional Swin Transformer is preceded by a series of small-kernel convolutions that impart local context to all pixels and introduce convolutional inductive bias. We additionally boost aggregation performance by applying transformers within a pyramidal structure, where aggregation at a coarser level guides aggregation at a finer level. Noise in the transformer output is then filtered in the subsequent decoder with the help of the query's appearance embedding. With this model, a new state-of-the-art is set for all the standard benchmarks in few-shot segmentation. It is shown that VAT attains state-of-the-art performance for semantic correspondence as well, where cost aggregation also plays a central role.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.10866", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/HongCNLK22", "doi": "10.48550/arxiv.2207.10866"}}, "content": {"source": {"pdf_hash": "cc052339afebba8adf9d1868d51b9f3b1d6d0eac", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.10866v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "234a0e0d59d934e9e8c9cf9fa72391c803894cc8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cc052339afebba8adf9d1868d51b9f3b1d6d0eac.txt", "contents": "\nCost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation\n\n\nSunghwan Hong sunghwan@korea.ac.kr \nKorea University\nSeoulKorea\n\nSeokju Cho seokjucho@korea.ac.kr \nKorea University\nSeoulKorea\n\nJisu Nam \nKorea University\nSeoulKorea\n\nStephen Lin \nMicrosoft Research Asia\nBeijingChina\n\nSeungryong Kim seungryongkim@korea.ac.kr \nKorea University\nSeoulKorea\n\nCost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation\n\nThis paper presents a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), for few-shot segmentation. The use of transformers can benefit correlation map aggregation through self-attention over a global receptive field. However, the tokenization of a correlation map for transformer processing can be detrimental, because the discontinuity at token boundaries reduces the local context available near the token edges and decreases inductive bias. To address this problem, we propose a 4D Convolutional Swin Transformer, where a high-dimensional Swin Transformer is preceded by a series of small-kernel convolutions that impart local context to all pixels and introduce convolutional inductive bias. We additionally boost aggregation performance by applying transformers within a pyramidal structure, where aggregation at a coarser level guides aggregation at a finer level. Noise in the transformer output is then filtered in the subsequent decoder with the help of the query's appearance embedding. With this model, a new state-of-the-art is set for all the standard benchmarks in few-shot segmentation. It is shown that VAT attains state-of-the-art performance for semantic correspondence as well, where cost aggregation also plays a central role. Code and trained models are available at https://seokju-cho.github.io/VAT/. \u22c6 Equal contribution\n\nIntroduction\n\nSemantic segmentation is a fundamental computer vision task that aims to label each pixel in an image with its corresponding class. Substantial progress has been made in this direction with the help of deep neural networks and large-scale datasets containing ground-truth segmentation annotations [37,46,3,4,61]. Manual labeling of pixel-wise segmentation maps, however, requires considerable labor, making it difficult to add new classes. Towards reducing reliance on labeled data, attention has increasingly focused on few -shot segmentation [49,55], where only a handful of support images and their associated masks are used in predicting the segmentation of a query image.  Fig. 1: Our VAT reformulates few-shot segmentation as semantic correspondence. VAT sets a new state-of-the-art in few-shot segmentation, and attains state-of-the-art performance for semantic correspondence as well.\n\nThe key to few-shot segmentation is in making effective use of the few support samples. Many works attempt this by extracting a prototype model from the samples and using it for feature comparison with the query [58,10,35,78]. However, such approaches disregard pixel-level pairwise relationships between support and query features or the spatial structure of features, which may lead to sub-optimal results.\n\nTo account for such relationships, we observe that few-shot segmentation can be reformulated as semantic correspondence, which aims to find pixel-level correspondences across semantically similar images which may contain large intraclass appearance and geometric variations [13,14,43]. Recent semantic correspondence models [50,25,51,53,42,44,34,65,41] follow the classical matching pipeline [54,47] of feature extraction, cost aggregation and flow estimation. The cost aggregation stage, where matching scores are refined to produce more reliable correspondence estimates, is of particular importance and has been the focus of much research [53,42,52,22,34,29,41,6]. Recently, CATs [6] proposed to use vision transformers [11] for cost aggregation, but its quadratic complexity to the number of input tokens limits its applicability. It also disregards the spatial structure of matching costs, which may hurt its performance.\n\nIn the area of few-shot segmentation, there also exist methods that attempt to leverage pairwise information by refining features through cross-attention [83] or graph attention [81,68,75]. However, they solely rely on raw correlation maps without aggregating the matching scores. As a result, their correspondence may suffer from ambiguities caused by repetitive patterns or background clutters [50,25,27,65,17]. To address this, HSNet [40] aggregates the matching scores with 4D convolutions, but its limited receptive fields prevent long-range context aggregation and it lacks an ability to adapt to the input content due to the use of fixed kernels.\n\nIn this paper, we introduce a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), that tackles the few-shot segmentation task through a proposed 4D Convolutional Swin Transformer. Specifically, we first extend Swin Transformer [36] and its patch embedding module to handle a high-dimensional correlation map. The patch embedding module is further extended by incorporating 4D convolutions that alleviate issues caused by patch embedding, i.e., limited local context near patch boundaries and low inductive bias. The high-dimensional patch embedding module is designed as a series of overlapping small-kernel convolutions, bringing local contextual information to each pixel and imparting convolutional inductive bias. To further boost performance, we compose our architecture with a pyramidal structure that takes the aggregated correlation maps at a coarser level as additional input at a finer level, providing hierarchical guidance. Our affinity-aware decoder then refines the aggregated matching scores in a manner that exploits the higher-resolution spatial structure given by the query's appearance embedding and finally outputs the segmentation mask prediction.\n\nWe demonstrate the effectiveness of our method on several benchmarks [55,31,30]. Our work attains state-of-the-art performance on all the benchmarks for few-shot segmentation and even for semantic correspondence, highlighting the importance of cost aggregation for both tasks and showing its potential for general matching. We also include ablation studies to justify our design choices.\n\n\nRelated Work\n\nFew-shot Segmentation. Inspired by the few-shot learning paradigm [49,58], which learns to learn a model for a novel task with only a limited number of samples, few-shot segmentation has received considerable attention. Following the success of [55], prototypical networks [58] and numerous other works [10,45,56,69,35,78,33,76,79,60,84,28] proposed to extract a prototype from support samples, which is used to identify foreground features in the query. In addition, inspired by [82] which observed that simply adding high-level features in feature processing leads to a performance drop, [63] proposed to instead utilize high-level features to compute a prior map that helps to identify targets in the query image. Many variants [60,80] extended this idea of utilizing prior maps to act as additional information for aggregating feature maps.\n\nHowever, as methods based on prototypes or prior maps have apparent limitations, e.g., disregarding pairwise relationships between support and query features or spatial structure of feature maps, numerous recent works [81,68,40,75,32] utilize a correlation map to leverage the pairwise relationships between source and query features. Specifically, [81,68,75] use graph attention, HSNet [40] proposes 4D convolutions to exploit multi-level features, and [32] formulates the task as an optimal transport problem. However, these approaches do not provide a means to aggregate the matching scores, solely utilize convolutions for cost aggregation, or use a handcrafted method that is neither learnable nor robust to severe deformations.\n\nRecently, [83] utilized transformers and proposed to use a cycle-consistent attention mechanism to refine the feature maps to become more discriminative, without considering aggregation of matching scores. [60] propose a global and local enhancement module to refine the features using transformers and convolutions, respectively. [39] focuses solely on the transformer-based classifier by freezing the encoder and decoder. Unlike these works, we propose a 4D Convolutional Swin Transformer for an enhanced and efficient cost aggregation.\n\nSemantic Correspondence. The objective of semantic correspondence is to find correspondences between semantically similar images with additional challenges posed by large intra-class appearance and geometric variations [34,6,41]. This is highly similar to the few-shot segmentation setting in that few-shot segmentation also aims to label objects of the same class with large intra-class variation, and thus recent works on both tasks have taken similar approaches. The latest methods [53,42,52,22,34,29,41,6] in semantic correspondence focus on the cost aggregation stage to find reliable correspondences and demonstrated its importance. Among them, [41] proposed to use 4D convolutions for cost aggregation, though exhibiting apparent limitations due to the limited receptive fields of convolutions and lack of adaptability. CATs [6] resolves this issue and sets a new state-of-the-art by leveraging transformers [66] to aggregate the cost volume. However, it disregards the spatial structure of correlation maps and imparts less inductive bias, i.e., translation equivariance, which limits its generalization power [36,7,8]. Moreover, its quadratic complexity may limit applicability when it is used to aggregate correlation maps on its own. In this paper, we propose to resolve the aforementioned issues.\n\nVision Transformer. Recently, transformer [66], the standard architecture in Natural Language Processing (NLP), has been widely adopted in Computer Vision. Since the pioneering work on ViT [11], numerous works [39,83,60,23,73,6,36] have adopted transformers to replace CNNs or to be used together with CNNs in a hybrid manner. However, due to quadratic complexity to sequence length, transformers often suffer from large a computational burden. Efficient transformers [70,24,77,72] aim to reduce the computational load via an approximated or simplified self-attention. Swin Transformer [36], a network we extend from, reduces computation by performing self-attention within pre-defined local windows. However, these works inherit the issues caused by patch embedding, which we alleviate by incorporating 4D convolutions.\n\n\nMethodology\n\n\nProblem Formulation\n\nThe goal of few -shot segmentation is to segment objects from unseen classes in a query image given only a few annotated examples [67]. To mitigate the overfitting caused by insufficient training data, we follow the common protocol of episodic training [67]. Let us denote the training and test sets as D train and D test , respectively, where the object classes of both sets do not overlap. Under the K-shot setting, multiple episodes are formed from both sets, each consisting of a support set S = {(x k s , m k s )} K k=1 , where (x k s , m k s ) is k-th support image and its corresponding mask pair, and a query sample (x q , m q ), where x q is a query image and m q is its paired mask. During training, our model takes a sampled  episode from D train and learns a mapping from S and x q to a prediction m q . At inference, our model predictsm q given randomly sampled S and x q from D test .\n\n\nMotivation and Overview\n\nThe key to few-shot segmentation is how to effectively utilize the support samples provided for a query image. While conventional methods [63,60,83,79,28] utilize global-or part-level prototypes extracted from support features, recent methods [81,68,40,75,32,83] instead leverage pairwise matching relationships between query and support. However, exploring such relationships is notoriously challenging due to intra-class variations, background clutters, and repetitive patterns. One of the state-of-the-art methods, HSNet [40], aggregates the matching scores with 4D convolutions. However, solely utilizing convolutions may limit performance due to limited receptive fields or lack of adaptability for convolutional kernels. While there has been no approach to aggregate the matching scores with transformers in few-shot segmentation, CATs [6] proposes cost aggregation with transformers in semantic correspondence, demonstrating the effectiveness of transformers as a cost aggregator. On the other hand, the quadratic complexity of transformers with respect to the number of tokens may limit its utility for segmentation. The absence of operations that impart inductive bias, i.e., translation equivariance, may limit its performance as well. Also, CATs [6] defines the tokens of a correlation map in a way that disregards spatial structure, which is likely to be harmful. The proposed Volumetric Aggregation with Transformers (VAT) is designed to overcome these problems. In the following, we first describe its feature extraction and cost computation. We then present a general extension of Swin Transformer [36] for cost aggregation. Subsequently, we present 4D Convolutional Swin Transformer for resolving the aforementioned issues. Lastly, we introduce several additional techniques including Guided Pyramidal Processing (GPP) and Affinity-aware Transformer Decoder (ATD) to further boost performance, and combine them to complete the design.\n\n\nFeature Extraction and Cost Computation\n\nWe extract features from query and support images and compute an initial cost between them following the conventional process [50,59,53,52,65,17,6]. Given query and support images, x q and x s , we use a CNN [16,57] to produce a sequence of L feature maps, {(F l q , F l s )} L l=1 , where F l q and F l s denote query and support feature maps at the l-th level. A support mask, m s , is used to encode segmentation information and filter out the background information as done in [28,40,80]. We obtain a masked support feature asF l s = F l s \u2299 \u03c8 l (m s ), where \u2299 denotes the Hadamard product and \u03c8 l (\u00b7) denotes a function that resizes the given tensor followed by expansion along the channel dimension of the l-th layer.\n\nGiven a pair of feature maps, F l q and F l s , we compute a correlation map using the inner product between l-2 normalized features such that\nC l (i, j) = ReLU F l q (i) \u00b7F l s (j) \u2225F l q (i)\u2225\u2225F l s (j)\u2225 ,(1)\nwhere i and j denote 2D spatial positions of feature maps. As done in [40], we collect correlation maps computed from all the intermediate features of the same spatial size and stack them to obtain a stacked correlation map C p \u2208 R hq\u00d7wq\u00d7hs\u00d7ws\u00d7|L p | , where (h q , w q ) and (h s , w s ) are the height and width of the query and support feature maps, respectively, and L p is a subset of CNN layer indices {1, ..., L} at pyramid layer p, containing correlation maps of identical spatial size.\n\n\nPyramidal Transformer Encoder\n\nIn this section, we present 4D Convolutional Swin Transformer for aggregating the correlation maps and then incorporate it into a pyramidal architecture.\n\nCost Aggregation with Transformers. For a transformer to process a correlation map, a means for token reduction is essential, since it would be infeasible for even an efficient transformer [70,24,77,72,36] to handle a correlation map otherwise. However, when one employs a transformer for cost aggregation, the problem of how to define the tokens for correlation maps, which differ in shape from images, text or features [66,11], is non-trivial. The first attempt to process correlation maps is CATs [6], which reshapes the 4D correlation maps into 2D maps and performs self-attention in 2D. This disregards the spatial structure of correlation maps, i.e., over both support and query, which could limit its performance. To address this, one may treat all the spatial entries, e.g., h q \u00d7 w q \u00d7 h s \u00d7 w s , as tokens and treat L p as the feature dimension for tokens. However, this results in a substantial computational burden that increases with larger correlation maps. This prevents the use of standard transformers [66,11] and encourages use of efficient versions as in [70,24,77,72,36]. However, the use of simplified (or approximated) self-attention may be sub-optimal for performance, as will be discussed in Section 4.4. Furthermore, as proven in the\n! \" # \" \u210e ! \" \u210e # \" \u210e ! \" \u210e # \" ! \" # \" 4D local window A token Original 4D Window Shifted 4D Window Query dim. Support dim.\nQuery dim. Support dim. Fig. 3: Illustration of shifted 4D windows in VTM. It computes self-attention within the partitioned windows and considers inter-window interactions by shifting the windows.\n\noptical flow and semantic correspondence literature [59,52], neighboring pixels tend to have similar correspondences. To preserve the spatial structure of correlation maps, we choose to use Swin Transformer [36] as it not only provides efficient self-attention computation, but also maintains the smoothness property of correlation maps while still providing sufficient long-range self-attention.\n\nTo employ Swin Transformer [36] for cost aggregation, we need to extend it to process higher dimensional input, specifically a 4D correlation map. We first follow the conventional patch embedding procedure [11] to embed correlation maps, as they cannot be processed by transformers due to the large number of tokens. However, we extend the patch embedding module to a Volumetric Embedding Module (VEM) which handles higher dimensional inputs, such that M p = VEM(C p ). Following a procedure similar to patch embedding, we reshape the correlation map to a sequence of flattened 4D windows using a large convolutional kernel, e.g., 16\u00d716\u00d716\u00d716. Then, we extend the self-attention computations, as shown in Fig. 3, by evenly partitioning the query and support spatial dimensions of M p into non-overlapping sub-correlation maps M \u2032,p \u2208 R n\u00d7n\u00d7n\u00d7n\u00d7D . We compute self-attention within each partitioned sub-correlation map. Subsequently, we shift the windows by a displacement of \u230a n 2 \u230b, \u230a n 2 \u230b, \u230a n 2 \u230b, \u230a n 2 \u230b pixels from the previously partitioned windows, then perform self-attention within the newly created windows. Then as done in the original Swin Transformer [36], we simply roll the correlation map back to its original form. In computing selfattention, we use relative position bias and take the values from an expanded parameterized bias matrix, following [19,20,36]. We leave the other components of Swin Transformer blocks unchanged, e.g., Layer Normalization (LN) [1] and MLP layers. We call this extension the Volumetric Transformer Module (VTM). To summarize, the overall process is defined as:\nA p = VTM(M p ).(2)\n4D Convolutional Swin Transformer. Although the proposed cost aggregation with transformers can solve the aforementioned issues of using CNNs and the high computational burden of using standard transformers, it may not avoid the issue that other transformers share [11,70,24,77,72]: lack of translation equivariance. This is primarily caused by utilizing non-overlapping operations prior to self-attention computation. Although Swin Transformer alleviates the issue to some extent by using relative positioning bias [36], it provides an insufficient approximation. We argue that the Volumetric Embedding Module is what needs to be addressed as it leads to several issues. First, the use of large  non-overlapping convolution kernels only provides limited inductive bias. Relatively lower translation equivariance is achieved from non-overlapping operations compared to that which are overlapping. This limited inductive bias results in relatively lower generalization power and performance [74,8,7,36]. Furthermore, we argue that for dense prediction tasks, disregarding window boundaries due to non-overlapping kernels hurts overall performance due to discontinuity. To address the above issues, we replace the Volumetric Embedding Module (VEM) with a module consisting of a series of overlapping convolutions, which we call the Volumetric Convolution Module (VCM). Concretely, we sequentially reduce spatial dimensions of the support and query by applying 4D spatial maxpooling, overlapping 4D convolutions, ReLU, and Group Normalization (GN), where we project the multi-level similarity vector at each 4D position, i.e., projecting a vector size of |L p |, to an arbitrary fixed dimension denoted as D.\nQuery dim. \u210e \u2032 Support dim. \u210e \u2032 \u2032 \u2032 \u211d \u210e \u2032 \u00d7 \u2032 \u00d7\u210e \u2032 \u00d7 \u2032 \u00d7 (1, 1, 1, 1, ) Query dim. \u210e \u2032 \u210e \u2032 \u2032 \u2032 \u211d \u210e \u2032 \u00d7 \u2032 \u00d7\u210e \u2032 \u00d7 \u2032 \u00d7 Support dim.\nConsidering receptive fields as a 4D window, i.e., m \u00d7 m \u00d7 m \u00d7 m, we obtain a tensor C p \u2208 R h \u2032,p q \u00d7w \u2032,p q \u00d7h \u2032,p s \u00d7w \u2032,p s \u00d7D from C p , where h \u2032,p s , w \u2032,p s , h \u2032,p q , and w \u2032,p q are the processed sizes. Note that a different size of m can be chosen for the support and query spatial dimensions. An overview of VCM is illustrated in Fig. 4. Overall, we define such a process as the following:\nM p = VCM(C p ).(3)\nIn this way, our model benefits from additional inductive bias as well as better handling at window boundaries. Moreover, to stabilize the learning, we propose an additional technique to enforce the networks to estimate residual matching scores as complementary details. We add residual connections in order to expedite the learning process [16,6,85], accounting for the fact that at the initial phase when the input M p is fed, erroneous matching scores are inferred due to randomly-initialized parameters of transformers, which could complicate the learning process as the networks need to learn the complete matching details from random matching scores.\n\nGuided Pyramidal Processing. Following [40,60], we also employ a coarseto-fine approach through pyramidal processing as illustrated in Fig. 2. Motivated by numerous recent works [83,41,6,40] in both semantic matching and few-shot segmentation which have demonstrated that leveraging multi-level features can boost performance by a large margin, we also use a pyramidal architecture.\n\nIn our coarse-to-fine approach, which we refer to as Guided Pyramidal Processing (GPP), the aggregation of a finer-level correlation map A p is guided by the aggregated correlation map of the previous (coarser) level A p+1 . Concretely, an aggregated correlation map A p+1 is up-sampled into a map up(A p+1 ) which is added to the next level's correlation map A p to serve as guidance. This process is repeated until the finest-level aggregated map is computed and passed to the decoder. As shown in Table 4, GPP leads to appreciable performance gains.\n\nWith GPP, the pyramidal transformer encoder is finally defined as:\nA p = VTM(VCM(C p ) + up(A p+1 )),(4)\nwhere up(\u00b7) denotes bilinear upsampling.\n\n\nAffinity-Aware Transformer Decoder\n\nGiven the aggregated correlation map produced by the pyramidal transformer encoder, a transformer-based decoder generates the final segmentation mask.\n\nTo improve performance, we propose to conduct further aggregation within the decoder with the aid of the appearance embedding obtained from query feature maps. The query's appearance embedding can help in two ways. First, appearance affinity information is an effective guide for filtering noise in matching scores, as proven in the stereo matching literature, e.g., Cost Volume Filtering (CVF) [18,59]. In addition, the higher-resolution spatial structure provided by an appearance embedding can be exploited to improve up-sampling quality, resulting in a highly accurate prediction maskm q where fine details are preserved. For the design of our Affinity-aware Transformer Decoder (ATD), we take the average over the support image dimensions of A p , concatenate it with the appearance embedding from query feature maps, and then aggregate by transformers [66,70,72,36] with subsequent bilinear interpolation. The process is defined as the following:m\nq = ATD([A \u2032,p , P(F q )]),(5)\nwhere A \u2032,p \u2208 R h \u2032,p q \u00d7w \u2032,p q \u00d7D is extracted by average pooling on A p over the spatial dimensions of the support image, P(\u00b7) is a linear projection, P(F q ) \u2208 R h \u2032,p q \u00d7w \u2032,p q \u00d7c , and [ \u00b7, \u00b7 ] denotes concatenation. We sequentially refine the output immediately after bilinear upsampling to recapture fine details and integrate appearance information.\n\n\nExtension to K-Shot Setting\n\nGiven K pairs of support image and mask {(x i s , m i s )} K i=1 and a query image x q , our model forward-passes K times to obtain K different query masksm k q . We sum up all the K predictions at each spatial location, and if the sum divided by K exceeds a threshold \u03c4 , the location is predicted as foreground, and otherwise it is background.    \n\n\nExperiments\n\n\nImplementation Details\n\nWe use ResNet50 and ResNet101 [16] pre-trained on ImageNet [9] and freeze the weights during training, following [40,82]. No data augmentation is used for training, as explained in the supplementary material. We set the input image sizes to 417 or 473, following [28,2]. The window size for Swin Transformer is set to 4. We use AdamW [38] with a learning rate of 5e \u2212 4. Feature maps from conv3 x (p = 3), conv4 x (p = 4) and conv5 x (p = 5) are taken for cost computation. The K-shot threshold \u03c4 is set to 0.5 and the embedding dimension D to 128. For appearance affinity, we take the last layers from conv2 x, conv3 x and conv4 x when training on FSS-1000 [30], and conv4 x is excluded when training on PASCAL-5 i [55] and COCO-20 i [31]. We set c to 16, 32, and 64 for conv2 x, conv3 x, and conv4 x.\n\n\nExperimental Settings\n\nDatasets. We evaluate our approach on three standard few-shot segmentation datasets, PASCAL-5 i [55], COCO-20 i [31], and FSS-1000 [30]. PASCAL-5 i contains images from PASCAL VOC 2012 [12] with added mask annotations [15].  Table 3: Mean IoU comparison on FSS-1000 [30].\n\nIt consists of 20 object classes, and as done in OSLSM [55], they are evenly divided into 4 folds i \u2208 {0, 1, 2, 3} for cross-validation, where each fold contains 5 classes. COCO-20 i contains 80 object classes, and as done for PASCAL-5 i , the dataset is evenly divided into 4 folds of 20 classes each. FSS-1000 is a more diverse dataset consisting of 1000 object classes. Following [30], we divide the 1000 categories into 3 splits for training, validation and testing, which consist of 520, 240 and 240 classes, respectively. For PASCAL-5 i and COCO-20 i , we follow the common evaluation practice [40,63,35] and standard cross-validation protocol, where each fold i is used for evaluation with the other folds used for training.\n\nEvaluation Metric. Following common practice [82,63,40,83], we adopt mean intersection over union (mIoU) and foreground-background IoU (FB-IoU) as our evaluation metrics. The mIoU averages over all IoU values for all object classes such that mIoU = 1 C C c=1 IoU c , where C is the number of classes in each fold, e.g., C = 20 for COCO-20 i . FB-IoU disregards the object classes and instead averages over foreground and background IoU (IoU F and IoU B ) such that FB \u2212 IoU = 1 2 (IoU F + IoU B ). We additionally adopt Mean Boundary Accuracy (mBA) introduced in [5] to evaluate the model's ability to capture fine details. To measure mBA, we first sample 5 radii in [3, w+h 300 ] at a uniform interval, where w and h are width and height of input image, and average the segmentation accuracy within each radius from the ground-truth boundary. Table 1 summarizes quantitative results on PASCAL-5 i [55]. The tests were conducted on two backbone networks, ResNet50 and ResNet101 [16]. The proposed method outperforms the others on almost all the folds in terms of both mIoU and FB-IoU. It surpasses the others, including HSNet [40], in mBA as well, since our ATD helps to improve up-sampling quality by providing higher-level spatial structure for reference. Consistent with this, VAT also attains state-of-the-art performance on COCO-20 i [31], as shown in Table 2. Interestingly, for the most recent dataset specifically created for few-shot segmentation, FSS-1000 [30], VAT outperforms HSNet [40] and FSOT [32] by a large margin, almost a 4.6% increase in mIoU compared to HSNet with ResNet50 as shown in Table 3. VAT sets a new state-of-the-art for all of these benchmarks. We note that our method outperforms HSNet [40] despite having more learnable parameters, which is known to have an inverse relation to generalization power [62], a trend seen in Table 1. With the proposed method, i.e., 4D convolutional Swin Transformer, that is designed to address the issues like lack of inductive bias, VAT can have a larger number of learnable parameters than that of HSNet [40], yet VAT has greater generalization power as well.\n\n\nFew-shot Segmentation Results\n\n\nAblation Study\n\nWe conducted ablations on FSS-1000 [30], a large-scale dataset specifically constructed for few-shot segmentation. Although VAT starts at a lower mIoU, it quickly exceeds HSNet [40].\n\nEffectiveness of each component in VAT. As the baseline model, we take the architecture composed of VEM and the 2D convolution decoder used in HSNet [40]. We then progressively add our components one-by-one as shown in Table 4. Note that we included (IV) and (V) to show the effectiveness of VCM alone and the performance of a model highly similar to HSNet [40], respectively. As summarized in Table 4, each component helps to boost performance. Starting from the baseline (I), adding Swin Transformer (II) brings a large gain, which indicates that Swin Transformer effectively performs cost aggregation thanks to its approximated inductive bias and ability to consider spatial structure. When the VEM is replaced by VCM (III), we also observe a significant improvement, which confirms that the issues due to non-overlap are alleviated. We note that (IV) also highlights the importance of inductive bias. As (V) is approximately equivalent to HSNet [40], we first compare it with (III), which shows the superiority of the proposed 4D Convolutional Swin Transformer. By including the additional components in (VI) and (VII), the performance is further boosted. Moreover, we observe a large gain in mBA by adding ATD. This shows that the higher-resolution spatial structure provided by appearance embeddings help to refine the fine details. We additionally provide a visualization of convergence in comparison to HSNet [40] in Fig. 5. Thanks to the early convolutions [74], VAT quickly converges and exceeds HSNet [40] even though it starts at a lower mIOU.\n\nBase architecture of VTM. As summarized in Table 5, we provide an ablation study to evaluate the effectiveness of different aggregators for VTM. For cost aggregation, there exists a few learnable aggregators, including MLP-, convolution-and transformer-based aggregators, any of which could be used as a base architecture for VTM. It should be noted that the use of standard transformer [66] and MLP-mixer [64] is not feasible due to memory requirements. Specifically, we calculated the memory consumption of each and found   that using standard transformer requires approximately 84 GB per batch, while the memory for MLP-Mixer could not be measured as it is much greater than standard transformer. Also, we note that the architecture with center-pivot convolutions is equivalent to a deeper version of the architecture with VCM. For a fair comparison, we only replace VTM with another aggregator and leave all the other components in our architecture unchanged. We observe that our method outperforms the other aggregators by a large margin. Interestingly, although center-pivot 4D convolution [40] also focuses on locality as in Swin Transformer [36], the performance gap indicates that the ability to adaptively consider pixel-wise interactions is critical. Also, we conjecture that the SW-MSA operation helps to compensate for the lack of global aggregation, which centerpivot convolutions lack. Another interesting point is that Linear Transformer [24] and Fastformer [72], which benefit from the global receptive fields of transformers and approximate the self-attention computation, achieve similar performance.\n\nWe additionally provide memory and run-time comparison to other aggregators in Table 5. The results are obtained using a single NVIDIA GeForce RTX 3090 GPU and Intel Core i7-10700 CPU. We observe that VAT is relatively slower and consumes more memory. However, 0.3 GB more memory consumption and 5 ms slower run time is a minor sacrifice for better performance.\n\nCan VAT also perform well on semantic correspondence? To tackle the few-shot segmentation task, we reformulated it as finding semantic correspondences under large intra-class variations and geometric deformations. This suggests that the proposed method could be effective for semantic correspondence as well. Here, we compare VAT to other state-of-the-art methods in semantic correspondence.\n\nIn order to ensure a fair comparison, we note whether each method leverages multi-level features and fine-tunes the backbone networks. We additionally denote the types of cost aggregation. Note that the only difference we made for this experiment is the objective function for loss computation. Following the common protocol [42,44,85,21,41,6], we use standard benchmarks for this task and our model was trained on the training split of PF-PASCAL [14] when evaluated on the test split of PF-PASCAL [14] and PF-WILLOW [13], and trained on SPair-71k [43] when evaluated on SPair-71k [43]. Experimental setting and implementation details can be found in supplementary material.\n\nAs shown in Table 6, VAT either sets a new state-of-the-art [43,13] or attains the second highest PCK [14], indicating the importance of cost aggregation in  Table 6: Quantitative results on SPair-71k [43], PF-PASCAL [14] and PF-WILLOW [13]. *: The results are obtained using pretrained weights provided by authors or taken from papers.\n\nboth few-shot segmentation and semantic correspondence. It also has the potential to benefit general-purpose matching networks as well. Furthermore, when data augmentation is used, we observe a relatively large performance gain compared to DHPF [44], showing that augmentation helps to address the heavy need for data and lack of inductive bias in transformers [11,6]. Although VAT is on par with state-of-the-art on PF-PASCAL [14], we argue that PF-PASCAL [14] is almost saturated, which makes a comparison difficult. Also, it should be noted that for performance on PF-WILLOW [13], VAT outperforms other methods by large margin, which clearly shows superior generalization power of the proposed 4D Convolutional Swin Transformer.\n\n\nConclusion\n\nIn this paper, we presented a novel cost aggregation network for few-shot segmentation. To address issues that arise from tokenization of a correlation map for transformer processing, we proposed a 4D Convolutional Swin Transformer, where a high-dimensional Swin Transformer is preceded by a series of smallkernel convolutions. To boost aggregation performance, we applied transformers within a pyramidal structure, and the output is then filtered and in the subsequent decoder with the help of image's appearance embedding. We have shown that the proposed method attains state-of-the-art performance for all the standard benchmarks for both few-shot segmentation and semantic correspondence, where cost aggregation plays a central role.\n\nIn this document, we provide details on the experimental setting, more ablation studies, more quantitative results on semantic correspondence benchmarks, including SPair-71k [43], PF-PASCAL [14], and PF-WILLOW [13], and more qualitative results on all the benchmarks we used.\n\n\nAppendix A. Experimental Setting for Semantic Correspondence\n\nDatasets. For the datasets we used, we follow the common protocol [42,44,53,85,21,41,6] and use standard benchmarks [13,14,43]. Specifically, we consider SPair-71k [43], which provides a total of 70,958 image pairs with extreme and diverse viewpoints, scale variations, and rich annotations for each image pair. We also consider relatively small-scale datasets, which include PF-PASCAL [14] containing 1,351 image pairs from 20 categories and PF-WILLOW [13] containing 900 image pairs from 4 categories, where each dataset provides corresponding ground-truth annotations.\n\nEvaluation metric. For evaluation on SPair-71k [43], PF-PASCAL [14], and PF-WILLOW [13], we employ the percentage of correct keypoints (PCK). It is computed as the ratio of estimated keypoints within the threshold from groundtruths to the total number of keypoints. Concretely, given predicted keypoint k pred and ground-truth keypoint k GT , we count the number of predicted keypoints that satisfy the following condition: d(k pred , k GT ) \u2264 \u03b1\u00b7max(H, W ), where d( \u00b7 ) denotes Euclidean distance; \u03b1 denotes a threshold value; H and W denote height and width of the object bounding box or the entire image, respectively. We evaluate on PF-PASCAL with \u03b1 img , and SPair-71k, and PF-WILLOW with \u03b1 bbox following the common protocol. Implementation Details. We use ResNet-101 [16] pre-trained on ImageNet [9] for the backbone feature extraction networks. We leave all the components in VAT unchanged. However, we build a different objective function. As in [42,44,41], we assume ground-truth keypoints are provided. We utilize Average End-Point Error (AEPE) [65] and compute it by averaging the Euclidean distance between the ground-truth and estimated flow. Specifically, we compute the loss as L = \u2225F GT \u2212 F pred \u2225 2 , where F GT is the ground-truth flow field and F pred is the predicted flow field. Note that we achieve this without making any modification to the network architecture. To report the results for different \u03b1 thresholds, we employ the pre-trained weights released by authors, and simply evaluate without making any changes to their architectures. We use the same data augmentation used in CATs [6]. For the learning rate, we use the AdamW [38] optimizer with 3e \u22125 for VAT and 3e \u22126 for the backbone feature networks. Finally, we use appearance embedding from conv3 x, conv4 x and conv5 x as done for FSS-1000 [30].\n\n\nAppendix B. Additional Ablation Study\n\nBackbones feature FSS-1000 [30] mIoU (%) 1-shot 5-shot ResNet50 [16] 90.1 90.7 ResNet101 [16] 90.3 90.8 PVT [71] 90.0 90.6 Swin transformer [36] 89.8 90.2 Table 1: Ablation study of different feature backbone.\n\nAblation study for feature backbone. Conventional few-shot segmentation methods only utilized CNN-based feature backbones [16] for extracting features. [82] observed that high-level features contain semantics of objects which could lead to overfitting and is not suitable to use for the task of few-shot segmentation. Then the question naturally arises, what about other networks? As addressed in many works [48,11], CNN and transformers see images differently, which means that the kinds of backbone networks may affect the performance significantly, but this has never been explored for this task. We thus exploit several well-known vision transformer architectures to explore the potential differences that may exist.\n\nThe results are summarized in Table 1. We find that both convolution-and transformer-based backbone networks attain similar performance. We conjecture that although it has been widely studied that convolutions and transformers see differently [48], as they are pre-trained on the same dataset [9], the representations learned by models are almost alike. Note that we only utilized backbones with a pyramidal structure, and the results may differ if other backbone networks are used, which we leave for future exploration.\n\nEffectiveness of Data Augmentation. We explore the effectiveness of data augmentation for few-shot segmentation. In this experiment, we employ two types of data augmentation, which are introduced either in PFE-Net [63] or CATs [6]. We summarize the augmentation types in Table 4 and Table 3. For this ablation study, we use two datasets, PASCAL-5 i [55] and FSS-1000 [30]. The results are summarized in Table 2. Note that we use the same augmentation types and probability as theirs. For a fair comparison, we keep all the other experimental settings the same, e.g., number of iterations and learning rate.  Table 2: Ablation study of Data Augmentation.\n\nAs PFE-Net [63] does not address the effectiveness of data augmentation and CATs [6] is designed for the semantic correspondence task, we are the first to analyze the effectiveness of data augmentation in the few-shot segmentation setting. Overall, we observe that using the data augmentation techniques severely affects the overall performance. Interestingly, although the augmentation technique introduced by CATs [6] showed a significant performance boost in the semantic correspondence task, it attains the lowest mIoU when evaluated on PASCAL-5 i [55] and the second lowest for FSS-1000 [30]. The severe performance drop in PASCAL-5 i [55] indicates a detrimental influence of using CATs [6] data augmentation. However, given the small difference to the best performance (0.3%) for FSS-1000 [30], the results may differ in a retrial. For PFE-Net [63] data augmentation, we observe results to be on par with the best reported results. However, at fold 0, there is a large gap between them, which indicates the detrimental effects of data augmentation on performance. Using both augmentations results in a large performance drop for PASCAL-5 i [55], arguably due to the detrimental effects of both augmentations, but for FSS-1000 [30], we observe only a small difference.   Consequently, we conjecture that the detrimental effects on PASCAL-5 i [55] and seemingly trivial effects on FSS-1000 [30] could be attributed to a few reasons: First, as shown in Table 2, since the difference between the results of the non-data augmentation approach and the PFE-Net [63] augmentation approach is only 0.6% for PASCAL-5 i , this may be due to the implementation details. For the training, we followed HSNet [40] to force randomness for diverse episode combinations, which may have made such a gap. Second, although the data augmentation may help transformers by providing inductive bias and addressing the heavy need for data, for few-shot setting, where the objective is to predict labels of unseen classes, the results may be different to that of semantic correspondence. It was demonstrated [6] that for semantic correspondence, data augmentation indeed helps to boost the performance, but a different problem formulation for few-shot segmentation may result in detrimental effects. Third, since we act on correlation maps, applying data augmentation may significantly affect the matching distribution at each pixel. Unlike those works directly working on fea-ture refinement [63,83], where adopting data augmentation has a direct influence on feature maps, VAT aggregates the correlation maps computed between the features extracted from augmented images, which may result in different effects (performance drop) when the objective is to predict unseen classes. Lastly, combining both augmentations may increase the difficulty of learning, which in turn impacts accuracy.\n\nAblation study for ATD. In this ablation study, we show a quantitative comparison between the proposed ATD and a decoder without transformers [66,70,72,36] to find out whether the model benefits from the use of transformers for further cost aggregation and filtering with the aid of the appearance embedding. For convenience, we call this Appearance-aware Decoder (AD). To implement this, we only exclude the transformers within ATD and leave all the other components and training settings unchanged, e.g., network architecture, hyperparameters, learning rate and number of iterations. As shown in Table 5,  we observe a large performance gap between AD and ATD, which demonstrates that using transformer allows for more effective aggregation, filtering and integration of correlation maps and appearance embedding. More specifically, we observe a 3% mIoU difference and find similar differences for FB-IoU and mBA. Without using transformers, where only convolutions are used, we observe that the results are equal to that of (VI) in the ablation study for VAT. This indicates that meaningful aggregation may not have occurred. It should be noted that we observe highly competitive results for mBA for both approaches, confirming a positive effect from the high-resolution spatial structure of the appearance embedding.\n\nAblation study for VCM. For this ablation study, we aim to further support our claims that the VCM (overlapping convolutions) compensates for the lack of inductive bias and alleviates the detrimental effects caused at window boundaries. To this end, we use Linear transformer [24], Fastformer [72] and Swin Transformer [36] to validate the effectiveness. Note that we already reported the results for the ones with VCM, but we additionally provide FB-IoU and mBA results. For the implementation of VEM, we refer the readers to Algorithm 1.\n\nAs shown in Table 6, we find a similar pattern to the results for VCM. Swin Transformer attained the best results, while Linear Transformer [24] and Fastformer [72] show similar results. Interestingly, when VCM is replaced with VEM, Components FSS-1000 [30] mIoU (%) FB-IoU (%) mBA (%) 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot VEM + Linear Transformer [24]   the performance difference for Swin Transformer and the other two differ substantially. Specifically, for Swin Transformer, the mIoU is 89.9% when equipped with VEM, which is a 0.4% performance drop and is a relatively lower drop compared to those of Linear Transformer and Fastformer. This could be due to the relative position bias that Swin Transformer provides, which the other two transformers lack. Furthermore, we suspect that the lower mIoU results could be explained by one of the following factors: simplified self-attention computation, local smoothness property of a correlation map, and consideration of spatial structure.\n\n\nAppendix C. Limitations\n\nAn apparent limitation is that since our approach acts on correlation maps, we need to explicitly compute the global correlation maps and store them. This is indeed memory expensive, and increases with the spatial resolution of the correlation maps. Although we utilize a coarse-to-fine architecture, this does not make the training feasible when resolutions are high. Specifically, given a spatial resolution of feature maps at size 128\u00d7128, the resultant size of correlation maps is at least 128 4 , and counting the level dimensions as well as other pyramidal levels p, it is difficult to train with a sufficient batch size even with NVIDIA GeForce RTX-3090 GPUs. This might limit the accessibility of this approach. We also visualize failure cases in Fig. 1.\n\n\nSupport Set\n\nQuery Ours Ground Truth Fig. 1: Failure cases.\n\n\nAppendix D. More Results\n\nQuantitative Results for Semantic Correspondence. As shown in Table 7, we provide per-class quantitative results on SPair-71k [43] in comparison to other semantic correspondence methods, including CNNGeo [50], WeakAlign [51], NC-Net [53], HPF [42], SFNet [27], DCC-Net [21], GSF [22], SCOT [34], DHPF [44], CHM [41], MMNet [85], PMNC [26] and CATs [6].  Table 7: Per-class quantitative evaluation on SPair-71k [43] benchmark.\n\nMore results for mBA comparison. In Table 8 and Table 9, we provide per fold quantitative results for mBA. Note that we obtained the mBA results for HSNet [40] and RePRI [2] using the pre-trained weights and code released by the authors. We omit the results for CyCTR [83] as the official code and weights by the authors are not publicly available.\n\nQualitative Results. As shown in Figure 2, Figure 3, Figure 4, Figure 5 and Figure 6, we provide qualitative results on all the benchmarks, which includes PASCAL-5 i [55], COCO-20 i [31], FSS-1000 [30], PF-PASCAL [14], PF-WILLOW [13] and SPair-71k [43].     Fig. 6: Qualitative results on SPair-71k [43].\n\nFig. 4 :\n4Overview of 4D Convolutional Swin Transformer. We replace the VEM with VCM and the output undergoes VTM for cost aggregation.\n\nFig. 5 :\n5Convergence comparison.\n\nTable 1 :\n1Performance comparison on PASCAL-5 i[55]. Best results in bold, and second best are underlined.Backbone \nfeature \nMethods \n1-shot \n5-shot \n20 0 20 1 20 2 20 3 mean FB-IoU mBA 20 0 20 1 20 2 20 3 mean FB-IoU mBA \n\nResNet50 [16] \n\nPMM [78] \n29.3 34.8 27.1 27.3 29.6 \n-\n-33.0 40.6 30.3 33.3 34.3 \n-\n-\nRPMM [78] 29.5 36.8 28.9 27.0 30.6 \n-\n-33.8 42.0 33.0 33.3 35.5 \n-\n-\nPFENet [63] 36.5 38.6 34.5 33.8 35.8 \n-\n-36.5 43.3 37.8 38.4 39.0 \n-\n-\nASGNet [28] -\n-\n-\n-34.6 60.4 \n-\n-\n-\n-\n-42.5 67.0 \n-\nRePRI [2] \n32.0 38.7 32.7 33.1 34.1 \n-\n6.31 39.3 45.4 39.7 41.8 41.6 \n-\n4.21 \nHSNet [40] 36.3 43.1 38.7 38.7 39.2 68.2 53.0 43.3 51.3 48.2 45.0 46.9 70.7 53.8 \nCyCTR [83] 38.9 43.0 39.6 39.8 40.3 \n-\n-41.1 48.9 45.2 47.0 45.6 \n-\n-\n\nVAT (ours) 39.0 43.8 42.6 39.7 41.3 68.8 54.2 44.1 51.1 50.2 46.1 47.9 72.4 54.9 \n\n\n\nTable 2 :\n2Performance comparison on COCO-20 i [31].\n\nTable 4 :\n4Ablation study for VAT.Different aggregators \nFSS-1000 [30] \nMemory Run-time \nmIoU (%) mBA (%) (GB) \n(ms) \n\nStandard transformer [66] \nOOM \nOOM \n84 \nN/A \nMLP-Mixer [64] \nOOM \nOOM \nOOM \nN/A \nCenter-pivot 4D convolutions [40] \n88.1 \n66.5 \n3.5 \n52.7 \nLinear transformer [24] \n87.7 \n66.5 \n3.5 \n56.8 \nFastformer [72] \n87.8 \n66.4 \n3.5 \n122.9 \n\n4D Conv. Swin transformer (Ours) \n90.3 \n68.0 \n3.8 \n57.3 \n\n\n\nTable 5 :\n5Ablation study for VTM. OOM: Out of Memory.\n\nTable 3 :\n3CATs[6] Aug. Type.Strong Aug. type Probability \n\n(I) RandScale \n1 \n(II) Crop \n1 \n(III) Gaussian Blur \n0.5 \n(IV) Horizontal Flip \n0.5 \n(V) Rotate \n0.5 \n\n\n\nTable 4 :\n4PFE-Net [63] Aug. \nType. \n\n\n\nTable 5 :\n5Ablation study for ATD.\n\nTable 6 :\n6Ablation study for VCM.\n\n\nMethods aero. bike bird boat bott. bus car cat chai. cow dog hors. mbik. pers. plan. shee. trai. tv allCNNGeo [50] 23.4 16.7 40.2 14.3 36.4 27.7 26.0 32.7 12.7 27.4 22.8 13.7 20.9 21.0 17.5 10.2 30.8 34.1 20.6 \nWeakAlign [51] 22.2 17.6 41.9 15.1 38.1 27.4 27.2 31.8 12.8 26.8 22.6 14.2 20.0 22.2 17.9 10.4 32.2 35.1 20.9 \nNC-Net [53] \n17.9 12.2 32.1 11.7 29.0 19.9 16.1 39.2 9.9 23.9 18.8 15.7 17.4 15.9 14.8 9.6 24.2 31.1 20.1 \nHPF [42] \n25.2 18.9 52.1 15.7 38.0 22.8 19.1 52.9 17.9 33.0 32.8 20.6 24.4 27.9 21.1 15.9 31.5 35.6 28.2 \nSCOT [34] \n34.9 20.7 63.8 21.1 43.5 27.3 21.3 63.1 20.0 42.9 42.5 31.1 29.8 35.0 27.7 24.4 48.4 40.8 35.6 \nDHPF [44] \n38.4 23.8 68.3 18.9 42.6 27.9 20.1 61.6 22.0 46.9 46.1 33.5 27.6 40.1 27.6 28.1 49.5 46.5 37.3 \nCHM [41] \n49.1 33.6 64.5 32.7 44.6 47.5 43.5 57.8 21.0 61.3 54.6 43.8 35.1 43.7 38.1 33.5 70.6 55.9 46.3 \nMMNet [85] \n43.5 27.0 62.4 27.3 40.1 50.1 37.5 60.0 21.0 56.3 50.3 41.3 30.9 19.2 30.1 33.2 642 43.6 40.9 \nPMNC [26] \n54.1 35.9 74.9 36.5 42.1 48.8 40.0 72.6 21.1 67.6 58.1 50.5 40.1 54.1 43.3 35.7 74.5 59.9 50.4 \nCATs [6] \n52.0 34.7 72.2 34.3 49.9 57.5 43.6 66.5 24.4 63.2 56.5 52.0 42.6 41.7 43.0 33.6 72.6 58.0 49.9 \n\nVAT \u2020 (ours) \n49.8 36.8 70.1 33.5 46.1 46.0 31.1 69.9 15.7 69.9 57.2 47.2 38.5 41.8 43.0 35.5 75.0 61.8 48.4 \nVAT (ours) \n58.8 40.0 75.3 40.1 52.1 59.7 44.2 69.1 23.3 75.1 61.9 57.1 46.4 49.1 51.8 41.8 80.9 70.1 55.5 \n\n\n\nTable 8 :\n8mBA comparison on PASCAL-5 i[55].\n\nTable 9 :\n9mBA comparison on COCO-20 i [31].Fig. 2: Qualitative results on PASCAL-5 i [55].Fig. 3: Qualitative results on COCO-20 i [31].Fig. 4: Qualitative results on FSS-1000 [30].Fig. 5: Qualitative results on PF-PASCAL [14] (left) and PF-WILLOW [13] (right).Support Set \nQuery \nOurs \nGround Truth Support Set \nQuery \nOurs \nGround Truth \n\n\nAcknowledgements. This research was supported by the MSIT, Korea (IITP-2022-2020-0-01819, ICT Creative Consilience program), and National Research Foundation of Korea (NRF-2021R1C1C1006897).Appendix\nJ L Ba, J R Kiros, G E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintBa, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint arXiv:1607.06450 (2016)\n\nFew-shot segmentation without meta-learning: A good transductive inference is all you need?. M Boudiaf, H Kervadec, Z I Masud, P Piantanida, I Ben Ayed, J Dolz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBoudiaf, M., Kervadec, H., Masud, Z.I., Piantanida, P., Ben Ayed, I., Dolz, J.: Few-shot segmentation without meta-learning: A good transductive inference is all you need? In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021)\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE transactions. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se- mantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelli- gence (2017)\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. L C Chen, Y Zhu, G Papandreou, F Schroff, H Adam, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European conference on computer vision (ECCV) (2018)\n\nCascadePSP: Toward classagnostic and very high-resolution segmentation via global and local refinement. H K Cheng, J Chung, Y W Tai, C K Tang, CVPRCheng, H.K., Chung, J., Tai, Y.W., Tang, C.K.: CascadePSP: Toward class- agnostic and very high-resolution segmentation via global and local refinement. In: CVPR (2020)\n\nCats: Cost aggregation transformers for visual correspondence. S Cho, S Hong, S Jeon, Y Lee, K Sohn, S Kim, Thirty-Fifth Conference on Neural Information Processing Systems. Cho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., Kim, S.: Cats: Cost aggregation transformers for visual correspondence. In: Thirty-Fifth Conference on Neural In- formation Processing Systems (2021)\n\nCoatnet: Marrying convolution and attention for all data sizes. Z Dai, H Liu, Q Le, M Tan, Advances in Neural Information Processing Systems. 34Dai, Z., Liu, H., Le, Q., Tan, M.: Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems 34 (2021)\n\nS Ascoli, H Touvron, M Leavitt, A Morcos, G Biroli, L Sagun, arXiv:2103.10697Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprintd'Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli, G., Sagun, L.: Con- vit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697 (2021)\n\nImagenet: A largescale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large- scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee (2009)\n\nFew-shot semantic segmentation with prototype learning. N Dong, E P Xing, BMVCDong, N., Xing, E.P.: Few-shot semantic segmentation with prototype learning. In: BMVC (2018)\n\nA Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, International journal of computer vision. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal of computer vision (2010)\n\nB Ham, M Cho, C Schmid, J Ponce, Proposal flow. CVPRHam, B., Cho, M., Schmid, C., Ponce, J.: Proposal flow. In: CVPR (2016)\n\nProposal flow: Semantic correspondences from object proposals. B Ham, M Cho, C Schmid, J Ponce, IEEE transactions. Ham, B., Cho, M., Schmid, C., Ponce, J.: Proposal flow: Semantic correspondences from object proposals. IEEE transactions on pattern analysis and machine intelli- gence (2017)\n\nSimultaneous detection and segmentation. B Hariharan, P Arbel\u00e1ez, R Girshick, J Malik, European conference on computer vision. SpringerHariharan, B., Arbel\u00e1ez, P., Girshick, R., Malik, J.: Simultaneous detection and segmentation. In: European conference on computer vision. Springer (2014)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2016)\n\nDeep matching prior: Test-time optimization for dense correspondence. S Hong, S Kim, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Hong, S., Kim, S.: Deep matching prior: Test-time optimization for dense corre- spondence. In: Proceedings of the IEEE/CVF International Conference on Com- puter Vision (ICCV) (2021)\n\nFast cost-volume filtering for visual correspondence and beyond. A Hosni, C Rhemann, M Bleyer, C Rother, M Gelautz, PAMIHosni, A., Rhemann, C., Bleyer, M., Rother, C., Gelautz, M.: Fast cost-volume filtering for visual correspondence and beyond. PAMI (2012)\n\nRelation networks for object detection. H Hu, J Gu, Z Zhang, J Dai, Y Wei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y.: Relation networks for object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3588-3597 (2018)\n\nLocal relation networks for image recognition. H Hu, Z Zhang, Z Xie, S Lin, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionHu, H., Zhang, Z., Xie, Z., Lin, S.: Local relation networks for image recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3464-3473 (2019)\n\nDynamic context correspondence network for semantic alignment. S Huang, Q Wang, S Zhang, S Yan, X He, ICCVHuang, S., Wang, Q., Zhang, S., Yan, S., He, X.: Dynamic context correspondence network for semantic alignment. In: ICCV (2019)\n\nGuided semantic flow. S Jeon, D Min, S Kim, J Choe, K Sohn, ECCV. SpringerJeon, S., Min, D., Kim, S., Choe, J., Sohn, K.: Guided semantic flow. In: ECCV. Springer (2020)\n\nW Jiang, E Trulls, J Hosang, A Tagliasacchi, K M Yi, arXiv:2103.14167Cotr: Correspondence transformer for matching across images. arXiv preprintJiang, W., Trulls, E., Hosang, J., Tagliasacchi, A., Yi, K.M.: Cotr: Correspondence transformer for matching across images. arXiv preprint arXiv:2103.14167 (2021)\n\nTransformers are rnns: Fast autoregressive transformers with linear attention. A Katharopoulos, A Vyas, N Pappas, F Fleuret, International Conference on Machine Learning. PMLRKatharopoulos, A., Vyas, A., Pappas, N., Fleuret, F.: Transformers are rnns: Fast autoregressive transformers with linear attention. In: International Conference on Machine Learning. pp. 5156-5165. PMLR (2020)\n\nFcss: Fully convolutional self-similarity for dense semantic correspondence. S Kim, D Min, B Ham, S Jeon, S Lin, K Sohn, CVPRKim, S., Min, D., Ham, B., Jeon, S., Lin, S., Sohn, K.: Fcss: Fully convolutional self-similarity for dense semantic correspondence. In: CVPR (2017)\n\nPatchmatch-based neighborhood consensus for semantic correspondence. J Y Lee, J Degol, V Fragoso, S N Sinha, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionCVPRLee, J.Y., DeGol, J., Fragoso, V., Sinha, S.N.: Patchmatch-based neighborhood consensus for semantic correspondence. In: Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR)\n\nSfnet: Learning object-aware semantic correspondence. J Lee, D Kim, J Ponce, B Ham, CVPRLee, J., Kim, D., Ponce, J., Ham, B.: Sfnet: Learning object-aware semantic cor- respondence. In: CVPR (2019)\n\nAdaptive prototype learning and allocation for few-shot segmentation. G Li, V Jampani, L Sevilla-Lara, D Sun, J Kim, J Kim, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLi, G., Jampani, V., Sevilla-Lara, L., Sun, D., Kim, J., Kim, J.: Adaptive pro- totype learning and allocation for few-shot segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8334- 8343 (2021)\n\nS Li, K Han, T W Costain, H Howard-Jenkins, V Prisacariu, Correspondence networks with adaptive neighbourhood consensus. CVPRLi, S., Han, K., Costain, T.W., Howard-Jenkins, H., Prisacariu, V.: Correspondence networks with adaptive neighbourhood consensus. In: CVPR (2020)\n\nFss-1000: A 1000-class dataset for few-shot segmentation. X Li, T Wei, Y P Chen, Y W Tai, C K Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLi, X., Wei, T., Chen, Y.P., Tai, Y.W., Tang, C.K.: Fss-1000: A 1000-class dataset for few-shot segmentation. In: Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (2020)\n\nMicrosoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision (2014)\n\nFew-shot segmentation with optimal transport matching and message flow. W Liu, C Zhang, H Ding, T Y Hung, G Lin, arXiv:2108.08518arXiv preprintLiu, W., Zhang, C., Ding, H., Hung, T.Y., Lin, G.: Few-shot segmentation with optimal transport matching and message flow. arXiv preprint arXiv:2108.08518 (2021)\n\nCrnet: Cross-reference networks for few-shot segmentation. W Liu, C Zhang, G Lin, F Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiu, W., Zhang, C., Lin, G., Liu, F.: Crnet: Cross-reference networks for few-shot segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)\n\nSemantic correspondence as an optimal transport problem. Y Liu, L Zhu, M Yamada, Y Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiu, Y., Zhu, L., Yamada, M., Yang, Y.: Semantic correspondence as an optimal transport problem. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)\n\nPart-aware prototype network for few-shot semantic segmentation. Y Liu, X Zhang, S Zhang, X He, European Conference on Computer Vision. SpringerLiu, Y., Zhang, X., Zhang, S., He, X.: Part-aware prototype network for few-shot semantic segmentation. In: European Conference on Computer Vision. pp. 142- 158. Springer (2020)\n\nZ Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, arXiv:2103.14030Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprintLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLong, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2015)\n\nI Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintLoshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)\n\nSimpler is better: Fewshot semantic segmentation with classifier weight transformer. Z Lu, S He, X Zhu, L Zhang, Y Z Song, T Xiang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLu, Z., He, S., Zhu, X., Zhang, L., Song, Y.Z., Xiang, T.: Simpler is better: Few- shot semantic segmentation with classifier weight transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2021)\n\nHypercorrelation squeeze for few-shot segmentation. J Min, D Kang, M Cho, arXiv:2104.01538arXiv preprintMin, J., Kang, D., Cho, M.: Hypercorrelation squeeze for few-shot segmentation. arXiv preprint arXiv:2104.01538 (2021)\n\nConvolutional hough matching networks for robust and efficient visual correspondence. J Min, S Kim, M Cho, arXiv:2109.05221arXiv preprintMin, J., Kim, S., Cho, M.: Convolutional hough matching networks for robust and efficient visual correspondence. arXiv preprint arXiv:2109.05221 (2021)\n\nHyperpixel flow: Semantic correspondence with multi-layer neural features. J Min, J Lee, J Ponce, M Cho, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMin, J., Lee, J., Ponce, J., Cho, M.: Hyperpixel flow: Semantic correspondence with multi-layer neural features. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2019)\n\nJ Min, J Lee, J Ponce, M Cho, arXiv:1908.10543Spair-71k: A large-scale benchmark for semantic correspondence. arXiv preprintMin, J., Lee, J., Ponce, J., Cho, M.: Spair-71k: A large-scale benchmark for se- mantic correspondence. arXiv preprint arXiv:1908.10543 (2019)\n\nLearning to compose hypercolumns for visual correspondence. J Min, J Lee, J Ponce, M Cho, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part XV 16Min, J., Lee, J., Ponce, J., Cho, M.: Learning to compose hypercolumns for vi- sual correspondence. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16. Springer (2020)\n\nFeature weighting and boosting for few-shot segmentation. K Nguyen, S Todorovic, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionNguyen, K., Todorovic, S.: Feature weighting and boosting for few-shot segmen- tation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 622-631 (2019)\n\nLearning deconvolution network for semantic segmentation. H Noh, S Hong, B Han, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionNoh, H., Hong, S., Han, B.: Learning deconvolution network for semantic segmen- tation. In: Proceedings of the IEEE international conference on computer vision (2015)\n\nObject retrieval with large vocabularies and fast spatial matching. J Philbin, O Chum, M Isard, J Sivic, A Zisserman, CVPR. IEEEPhilbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Object retrieval with large vocabularies and fast spatial matching. In: CVPR. IEEE (2007)\n\nDo vision transformers see like convolutional neural networks?. M Raghu, T Unterthiner, S Kornblith, C Zhang, A Dosovitskiy, arXiv:2108.08810arXiv preprintRaghu, M., Unterthiner, T., Kornblith, S., Zhang, C., Dosovitskiy, A.: Do vision transformers see like convolutional neural networks? arXiv preprint arXiv:2108.08810 (2021)\n\nOptimization as a model for few-shot learning. S Ravi, H Larochelle, Ravi, S., Larochelle, H.: Optimization as a model for few-shot learning (2016)\n\nConvolutional neural network architecture for geometric matching. I Rocco, R Arandjelovic, J Sivic, CVPRRocco, I., Arandjelovic, R., Sivic, J.: Convolutional neural network architecture for geometric matching. In: CVPR (2017)\n\nEnd-to-end weakly-supervised semantic alignment. I Rocco, R Arandjelovi\u0107, J Sivic, CVPRRocco, I., Arandjelovi\u0107, R., Sivic, J.: End-to-end weakly-supervised semantic align- ment. In: CVPR (2018)\n\nEfficient neighbourhood consensus networks via submanifold sparse convolutions. I Rocco, R Arandjelovi\u0107, J Sivic, ECCVRocco, I., Arandjelovi\u0107, R., Sivic, J.: Efficient neighbourhood consensus networks via submanifold sparse convolutions. In: ECCV (2020)\n\nI Rocco, M Cimpoi, R Arandjelovi\u0107, A Torii, T Pajdla, J Sivic, arXiv:1810.10510Neighbourhood consensus networks. arXiv preprintRocco, I., Cimpoi, M., Arandjelovi\u0107, R., Torii, A., Pajdla, T., Sivic, J.: Neighbour- hood consensus networks. arXiv preprint arXiv:1810.10510 (2018)\n\nA taxonomy and evaluation of dense two-frame stereo correspondence algorithms. D Scharstein, R Szeliski, International journal of computer vision. Scharstein, D., Szeliski, R.: A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International journal of computer vision (2002)\n\nOne-shot learning for semantic segmentation. A Shaban, S Bansal, Z Liu, I Essa, B Boots, arXiv:1709.03410arXiv preprintShaban, A., Bansal, S., Liu, Z., Essa, I., Boots, B.: One-shot learning for semantic segmentation. arXiv preprint arXiv:1709.03410 (2017)\n\nAdaptive masked proxies for few-shot segmentation. M Siam, B Oreshkin, M Jagersand, arXiv:1902.11123arXiv preprintSiam, M., Oreshkin, B., Jagersand, M.: Adaptive masked proxies for few-shot seg- mentation. arXiv preprint arXiv:1902.11123 (2019)\n\nK Simonyan, A Zisserman, arXiv:1409.1556Very deep convolutional networks for large-scale image recognition. arXiv preprintSimonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)\n\nJ Snell, K Swersky, R S Zemel, arXiv:1703.05175Prototypical networks for few-shot learning. arXiv preprintSnell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175 (2017)\n\nPwc-net: Cnns for optical flow using pyramid, warping, and cost volume. D Sun, X Yang, M Y Liu, J Kautz, In: CVPR. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In: CVPR (2018)\n\nBoosting few-shot semantic segmentation with transformers. G Sun, Y Liu, J Liang, L Van Gool, arXiv:2108.02266arXiv preprintSun, G., Liu, Y., Liang, J., Van Gool, L.: Boosting few-shot semantic segmentation with transformers. arXiv preprint arXiv:2108.02266 (2021)\n\nHierarchical multi-scale attention for semantic segmentation. A Tao, K Sapra, B Catanzaro, arXiv:2005.10821arXiv preprintTao, A., Sapra, K., Catanzaro, B.: Hierarchical multi-scale attention for semantic segmentation. arXiv preprint arXiv:2005.10821 (2020)\n\nNeural network studies, 1. comparison of overfitting and overtraining. I V Tetko, D J Livingstone, A I Luik, J. Chem. Inf. Comput. Sci. 35Tetko, I.V., Livingstone, D.J., Luik, A.I.: Neural network studies, 1. comparison of overfitting and overtraining. J. Chem. Inf. Comput. Sci. 35, 826-833 (1995)\n\nPrior guided feature enrichment network for few-shot segmentation. Z Tian, H Zhao, M Shu, Z Yang, R Li, J Jia, IEEE Transactions on Pattern Analysis & Machine Intelligence. Tian, Z., Zhao, H., Shu, M., Yang, Z., Li, R., Jia, J.: Prior guided feature enrich- ment network for few-shot segmentation. IEEE Transactions on Pattern Analysis & Machine Intelligence (2020)\n\nMlp-mixer: An all-mlp architecture for vision. I O Tolstikhin, N Houlsby, A Kolesnikov, L Beyer, X Zhai, T Unterthiner, J Yung, A Steiner, D Keysers, J Uszkoreit, Advances in Neural Information Processing Systems. 34Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et al.: Mlp-mixer: An all-mlp architecture for vision. Advances in Neural Information Processing Systems 34 (2021)\n\nGlu-net: Global-local universal network for dense flow and correspondences. P Truong, M Danelljan, R Timofte, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionTruong, P., Danelljan, M., Timofte, R.: Glu-net: Global-local universal network for dense flow and correspondences. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6258-6268 (2020)\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems (2017)\n\nMatching networks for one shot learning. O Vinyals, C Blundell, T Lillicrap, D Wierstra, Advances in neural information processing systems. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.: Matching networks for one shot learning. Advances in neural information processing systems (2016)\n\nFew-shot semantic segmentation with democratic attention networks. H Wang, X Zhang, Y Hu, Y Yang, X Cao, X Zhen, European Conference on Computer Vision. Wang, H., Zhang, X., Hu, Y., Yang, Y., Cao, X., Zhen, X.: Few-shot semantic segmentation with democratic attention networks. In: European Conference on Computer Vision (2020)\n\nPanet: Few-shot image semantic segmentation with prototype alignment. K Wang, J H Liew, Y Zou, D Zhou, J Feng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionWang, K., Liew, J.H., Zou, Y., Zhou, D., Feng, J.: Panet: Few-shot image semantic segmentation with prototype alignment. In: Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision (2019)\n\nS Wang, B Z Li, M Khabsa, H Fang, H Ma, arXiv:2006.04768Linformer: Self-attention with linear complexity. arXiv preprintWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020)\n\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions. W Wang, E Xie, X Li, D P Fan, K Song, D Liang, T Lu, P Luo, L Shao, arXiv:2102.12122arXiv preprintWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122 (2021)\n\nFastformer: Additive attention can be all you need. C Wu, F Wu, T Qi, Y Huang, X Xie, arXiv:2108.09084arXiv preprintWu, C., Wu, F., Qi, T., Huang, Y., Xie, X.: Fastformer: Additive attention can be all you need. arXiv preprint arXiv:2108.09084 (2021)\n\nFully transformer networks for semantic image segmentation. S Wu, T Wu, F Lin, S Tian, G Guo, arXiv:2106.04108arXiv preprintWu, S., Wu, T., Lin, F., Tian, S., Guo, G.: Fully transformer networks for semantic image segmentation. arXiv preprint arXiv:2106.04108 (2021)\n\nEarly convolutions help transformers see better. T Xiao, M Singh, E Mintun, T Darrell, P Doll\u00e1r, R Girshick, arXiv:2106.14881arXiv preprintXiao, T., Singh, M., Mintun, E., Darrell, T., Doll\u00e1r, P., Girshick, R.: Early convo- lutions help transformers see better. arXiv preprint arXiv:2106.14881 (2021)\n\nScale-aware graph neural network for fewshot semantic segmentation. G S Xie, J Liu, H Xiong, L Shao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXie, G.S., Liu, J., Xiong, H., Shao, L.: Scale-aware graph neural network for few- shot semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5475-5484 (2021)\n\nFew-shot semantic segmentation with cyclic memory network. G S Xie, H Xiong, J Liu, Y Yao, L Shao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionXie, G.S., Xiong, H., Liu, J., Yao, Y., Shao, L.: Few-shot semantic segmentation with cyclic memory network. In: Proceedings of the IEEE/CVF International Con- ference on Computer Vision (2021)\n\nNystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. Y Xiong, Z Zeng, R Chakraborty, M Tan, G Fung, Y Li, V Singh, Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V.: Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention (2021)\n\nPrototype mixture models for few-shot semantic segmentation. B Yang, C Liu, B Li, J Jiao, Q Ye, European Conference on Computer Vision. SpringerYang, B., Liu, C., Li, B., Jiao, J., Ye, Q.: Prototype mixture models for few-shot semantic segmentation. In: European Conference on Computer Vision. Springer (2020)\n\nMining latent classes for few-shot segmentation. L Yang, W Zhuo, L Qi, Y Shi, Y Gao, arXiv:2103.15402arXiv preprintYang, L., Zhuo, W., Qi, L., Shi, Y., Gao, Y.: Mining latent classes for few-shot segmentation. arXiv preprint arXiv:2103.15402 (2021)\n\nSelf-guided and cross-guided learning for few-shot segmentation. B Zhang, J Xiao, T Qin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZhang, B., Xiao, J., Qin, T.: Self-guided and cross-guided learning for few-shot segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021)\n\nPyramid graph networks with connection attentions for region-based one-shot semantic segmentation. C Zhang, G Lin, F Liu, J Guo, Q Wu, R Yao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhang, C., Lin, G., Liu, F., Guo, J., Wu, Q., Yao, R.: Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9587-9595 (2019)\n\nCanet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. C Zhang, G Lin, F Liu, R Yao, C Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZhang, C., Lin, G., Liu, F., Yao, R., Shen, C.: Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5217-5226 (2019)\n\nFew-shot segmentation via cycle-consistent transformer. G Zhang, G Kang, Y Wei, Y Yang, arXiv:2106.02320arXiv preprintZhang, G., Kang, G., Wei, Y., Yang, Y.: Few-shot segmentation via cycle-consistent transformer. arXiv preprint arXiv:2106.02320 (2021)\n\nPrototypical matching and open set rejection for zero-shot semantic segmentation. H Zhang, H Ding, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhang, H., Ding, H.: Prototypical matching and open set rejection for zero-shot se- mantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2021)\n\nMulti-scale matching networks for semantic correspondence. D Zhao, Z Song, Z Ji, G Zhao, W Ge, Y Yu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhao, D., Song, Z., Ji, Z., Zhao, G., Ge, W., Yu, Y.: Multi-scale matching networks for semantic correspondence. In: Proceedings of the IEEE/CVF International Con- ference on Computer Vision. pp. 3354-3364 (2021)\n", "annotations": {"author": "[{\"end\":149,\"start\":85},{\"end\":212,\"start\":150},{\"end\":251,\"start\":213},{\"end\":302,\"start\":252},{\"end\":373,\"start\":303}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":94},{\"end\":160,\"start\":157},{\"end\":221,\"start\":218},{\"end\":263,\"start\":260},{\"end\":317,\"start\":314}]", "author_first_name": "[{\"end\":93,\"start\":85},{\"end\":156,\"start\":150},{\"end\":217,\"start\":213},{\"end\":259,\"start\":252},{\"end\":313,\"start\":303}]", "author_affiliation": "[{\"end\":148,\"start\":121},{\"end\":211,\"start\":184},{\"end\":250,\"start\":223},{\"end\":301,\"start\":265},{\"end\":372,\"start\":345}]", "title": "[{\"end\":82,\"start\":1},{\"end\":455,\"start\":374}]", "venue": null, "abstract": "[{\"end\":1837,\"start\":457}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2154,\"start\":2150},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2157,\"start\":2154},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2159,\"start\":2157},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2161,\"start\":2159},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2164,\"start\":2161},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2401,\"start\":2397},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2404,\"start\":2401},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2963,\"start\":2959},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2966,\"start\":2963},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2969,\"start\":2966},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":2972,\"start\":2969},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3435,\"start\":3431},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3438,\"start\":3435},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3441,\"start\":3438},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3485,\"start\":3481},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3488,\"start\":3485},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3491,\"start\":3488},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3494,\"start\":3491},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3497,\"start\":3494},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3500,\"start\":3497},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3503,\"start\":3500},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":3506,\"start\":3503},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3509,\"start\":3506},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3553,\"start\":3549},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3556,\"start\":3553},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3803,\"start\":3799},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3806,\"start\":3803},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3809,\"start\":3806},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3812,\"start\":3809},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3815,\"start\":3812},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3818,\"start\":3815},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3821,\"start\":3818},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3823,\"start\":3821},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3843,\"start\":3840},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3884,\"start\":3880},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":4243,\"start\":4239},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":4267,\"start\":4263},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":4270,\"start\":4267},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":4273,\"start\":4270},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4485,\"start\":4481},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4488,\"start\":4485},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4491,\"start\":4488},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4494,\"start\":4491},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4497,\"start\":4494},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4526,\"start\":4522},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5006,\"start\":5002},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6018,\"start\":6014},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6021,\"start\":6018},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6024,\"start\":6021},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6419,\"start\":6415},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6422,\"start\":6419},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6598,\"start\":6594},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6626,\"start\":6622},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6656,\"start\":6652},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6659,\"start\":6656},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6662,\"start\":6659},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":6665,\"start\":6662},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6668,\"start\":6665},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":6671,\"start\":6668},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6674,\"start\":6671},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":6677,\"start\":6674},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":6680,\"start\":6677},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6683,\"start\":6680},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":6686,\"start\":6683},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6689,\"start\":6686},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":6833,\"start\":6829},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6943,\"start\":6939},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7084,\"start\":7080},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":7087,\"start\":7084},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":7417,\"start\":7413},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7420,\"start\":7417},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7423,\"start\":7420},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":7426,\"start\":7423},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7429,\"start\":7426},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":7548,\"start\":7544},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7551,\"start\":7548},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":7554,\"start\":7551},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7586,\"start\":7582},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7653,\"start\":7649},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":7944,\"start\":7940},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8140,\"start\":8136},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8265,\"start\":8261},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8693,\"start\":8689},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8695,\"start\":8693},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8698,\"start\":8695},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8959,\"start\":8955},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8962,\"start\":8959},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8965,\"start\":8962},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8968,\"start\":8965},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8971,\"start\":8968},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8974,\"start\":8971},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8977,\"start\":8974},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8979,\"start\":8977},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9125,\"start\":9121},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9305,\"start\":9302},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9389,\"start\":9385},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9592,\"start\":9588},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9594,\"start\":9592},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9596,\"start\":9594},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9826,\"start\":9822},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9973,\"start\":9969},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9994,\"start\":9990},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":9997,\"start\":9994},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10000,\"start\":9997},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10003,\"start\":10000},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":10006,\"start\":10003},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10008,\"start\":10006},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10011,\"start\":10008},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10252,\"start\":10248},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10255,\"start\":10252},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":10258,\"start\":10255},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":10261,\"start\":10258},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10370,\"start\":10366},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10772,\"start\":10768},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10895,\"start\":10891},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11706,\"start\":11702},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11709,\"start\":11706},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":11712,\"start\":11709},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":11715,\"start\":11712},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11718,\"start\":11715},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":11811,\"start\":11807},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":11814,\"start\":11811},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11817,\"start\":11814},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":11820,\"start\":11817},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11823,\"start\":11820},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":11826,\"start\":11823},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12092,\"start\":12088},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12409,\"start\":12406},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12824,\"start\":12821},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13181,\"start\":13177},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13688,\"start\":13684},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":13691,\"start\":13688},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13694,\"start\":13691},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13697,\"start\":13694},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":13700,\"start\":13697},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13703,\"start\":13700},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13705,\"start\":13703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13770,\"start\":13766},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13773,\"start\":13770},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14043,\"start\":14039},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14046,\"start\":14043},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":14049,\"start\":14046},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14568,\"start\":14564},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":15370,\"start\":15366},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15373,\"start\":15370},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":15376,\"start\":15373},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":15379,\"start\":15376},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15382,\"start\":15379},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":15602,\"start\":15598},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15605,\"start\":15602},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15680,\"start\":15677},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":16201,\"start\":16197},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16204,\"start\":16201},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":16256,\"start\":16252},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16259,\"start\":16256},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":16262,\"start\":16259},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":16265,\"start\":16262},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16268,\"start\":16265},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16817,\"start\":16813},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16820,\"start\":16817},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16972,\"start\":16968},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17190,\"start\":17186},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17369,\"start\":17365},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18329,\"start\":18325},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18529,\"start\":18525},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18532,\"start\":18529},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18535,\"start\":18532},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18639,\"start\":18636},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19058,\"start\":19054},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":19061,\"start\":19058},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19064,\"start\":19061},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":19067,\"start\":19064},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":19070,\"start\":19067},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19309,\"start\":19305},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":19783,\"start\":19779},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19785,\"start\":19783},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19787,\"start\":19785},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19790,\"start\":19787},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21393,\"start\":21389},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21395,\"start\":21393},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":21398,\"start\":21395},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21749,\"start\":21745},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21752,\"start\":21749},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":21888,\"start\":21884},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21891,\"start\":21888},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21893,\"start\":21891},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21896,\"start\":21893},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23379,\"start\":23375},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23382,\"start\":23379},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":23842,\"start\":23838},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":23845,\"start\":23842},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":23848,\"start\":23845},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23851,\"start\":23848},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24780,\"start\":24776},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24808,\"start\":24805},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24863,\"start\":24859},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":24866,\"start\":24863},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25013,\"start\":25009},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25015,\"start\":25013},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25084,\"start\":25080},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25408,\"start\":25404},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25466,\"start\":25462},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25485,\"start\":25481},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25674,\"start\":25670},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25690,\"start\":25686},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25709,\"start\":25705},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25763,\"start\":25759},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25796,\"start\":25792},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25844,\"start\":25840},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25906,\"start\":25902},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26234,\"start\":26230},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26451,\"start\":26447},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":26454,\"start\":26451},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26457,\"start\":26454},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":26629,\"start\":26625},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":26632,\"start\":26629},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26635,\"start\":26632},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":26638,\"start\":26635},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27145,\"start\":27143},{\"end\":27250,\"start\":27247},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":27482,\"start\":27478},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27562,\"start\":27558},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27710,\"start\":27706},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27923,\"start\":27919},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28050,\"start\":28046},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28078,\"start\":28074},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28092,\"start\":28088},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28303,\"start\":28299},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28417,\"start\":28413},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28655,\"start\":28651},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28796,\"start\":28792},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28938,\"start\":28934},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29094,\"start\":29090},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29302,\"start\":29298},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29894,\"start\":29890},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30362,\"start\":30358},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":30411,\"start\":30407},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30457,\"start\":30453},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":30889,\"start\":30885},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":30908,\"start\":30904},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31598,\"start\":31594},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31651,\"start\":31647},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31956,\"start\":31952},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":31976,\"start\":31972},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33204,\"start\":33200},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33207,\"start\":33204},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":33210,\"start\":33207},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33213,\"start\":33210},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33216,\"start\":33213},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33218,\"start\":33216},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33326,\"start\":33322},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33377,\"start\":33373},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33396,\"start\":33392},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33427,\"start\":33423},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33460,\"start\":33456},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33615,\"start\":33611},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33618,\"start\":33615},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33657,\"start\":33653},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33756,\"start\":33752},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33772,\"start\":33768},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33791,\"start\":33787},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34138,\"start\":34134},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34254,\"start\":34250},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34256,\"start\":34254},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34320,\"start\":34316},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34350,\"start\":34346},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34471,\"start\":34467},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35552,\"start\":35548},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35568,\"start\":35564},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35588,\"start\":35584},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35784,\"start\":35780},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35787,\"start\":35784},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35790,\"start\":35787},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":35793,\"start\":35790},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35796,\"start\":35793},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35799,\"start\":35796},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35801,\"start\":35799},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35834,\"start\":35830},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35837,\"start\":35834},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35840,\"start\":35837},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35882,\"start\":35878},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36104,\"start\":36100},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36171,\"start\":36167},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36338,\"start\":36334},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36354,\"start\":36350},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36374,\"start\":36370},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37065,\"start\":37061},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37093,\"start\":37090},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37246,\"start\":37242},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37249,\"start\":37246},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37252,\"start\":37249},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":37347,\"start\":37343},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37901,\"start\":37898},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37947,\"start\":37943},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":38118,\"start\":38114},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":38192,\"start\":38188},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38229,\"start\":38225},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38254,\"start\":38250},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":38273,\"start\":38269},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38305,\"start\":38301},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38498,\"start\":38494},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":38528,\"start\":38524},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38784,\"start\":38780},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38787,\"start\":38784},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":39341,\"start\":39337},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39390,\"start\":39387},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":39835,\"start\":39831},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":39847,\"start\":39844},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":39970,\"start\":39966},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39988,\"start\":39984},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":40287,\"start\":40283},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40356,\"start\":40353},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40691,\"start\":40688},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":40828,\"start\":40824},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":40868,\"start\":40864},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":40916,\"start\":40912},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40968,\"start\":40965},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41072,\"start\":41068},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":41127,\"start\":41123},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":41423,\"start\":41419},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41509,\"start\":41505},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41671,\"start\":41667},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":41837,\"start\":41833},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":41977,\"start\":41973},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":42363,\"start\":42360},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":42749,\"start\":42745},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":42752,\"start\":42749},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":43289,\"start\":43285},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":43292,\"start\":43289},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":43295,\"start\":43292},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":43298,\"start\":43295},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":44745,\"start\":44741},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":44762,\"start\":44758},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":44788,\"start\":44784},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":45150,\"start\":45146},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":45170,\"start\":45166},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":45263,\"start\":45259},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":45363,\"start\":45359},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47012,\"start\":47008},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":47090,\"start\":47086},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":47106,\"start\":47102},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":47119,\"start\":47115},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":47129,\"start\":47125},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47141,\"start\":47137},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":47155,\"start\":47151},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":47165,\"start\":47161},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":47176,\"start\":47172},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":47187,\"start\":47183},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47197,\"start\":47193},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":47209,\"start\":47205},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":47220,\"start\":47216},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":47233,\"start\":47230},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47296,\"start\":47292},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":47468,\"start\":47464},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47482,\"start\":47479},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":47581,\"start\":47577},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":47829,\"start\":47825},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47845,\"start\":47841},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":47860,\"start\":47856},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":47876,\"start\":47872},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47892,\"start\":47888},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47911,\"start\":47907},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47962,\"start\":47958},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":48188,\"start\":48184},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49491,\"start\":49488},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":51191,\"start\":51187}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48100,\"start\":47964},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48135,\"start\":48101},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48952,\"start\":48136},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49006,\"start\":48953},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49415,\"start\":49007},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49471,\"start\":49416},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":49636,\"start\":49472},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":49676,\"start\":49637},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":49712,\"start\":49677},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":49748,\"start\":49713},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":51146,\"start\":49749},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":51192,\"start\":51147},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":51536,\"start\":51193}]", "paragraph": "[{\"end\":2745,\"start\":1853},{\"end\":3155,\"start\":2747},{\"end\":4083,\"start\":3157},{\"end\":4738,\"start\":4085},{\"end\":5943,\"start\":4740},{\"end\":6332,\"start\":5945},{\"end\":7193,\"start\":6349},{\"end\":7928,\"start\":7195},{\"end\":8468,\"start\":7930},{\"end\":9778,\"start\":8470},{\"end\":10600,\"start\":9780},{\"end\":11536,\"start\":10638},{\"end\":13514,\"start\":11564},{\"end\":14282,\"start\":13558},{\"end\":14426,\"start\":14284},{\"end\":14988,\"start\":14494},{\"end\":15175,\"start\":15022},{\"end\":16436,\"start\":15177},{\"end\":16759,\"start\":16562},{\"end\":17157,\"start\":16761},{\"end\":18768,\"start\":17159},{\"end\":20494,\"start\":18789},{\"end\":21027,\"start\":20624},{\"end\":21704,\"start\":21048},{\"end\":22088,\"start\":21706},{\"end\":22642,\"start\":22090},{\"end\":22710,\"start\":22644},{\"end\":22789,\"start\":22749},{\"end\":22978,\"start\":22828},{\"end\":23933,\"start\":22980},{\"end\":24324,\"start\":23965},{\"end\":24705,\"start\":24356},{\"end\":25548,\"start\":24746},{\"end\":25845,\"start\":25574},{\"end\":26578,\"start\":25847},{\"end\":28706,\"start\":26580},{\"end\":28939,\"start\":28757},{\"end\":30496,\"start\":28941},{\"end\":32117,\"start\":30498},{\"end\":32480,\"start\":32119},{\"end\":32873,\"start\":32482},{\"end\":33549,\"start\":32875},{\"end\":33887,\"start\":33551},{\"end\":34620,\"start\":33889},{\"end\":35372,\"start\":34635},{\"end\":35649,\"start\":35374},{\"end\":36285,\"start\":35714},{\"end\":38119,\"start\":36287},{\"end\":38370,\"start\":38161},{\"end\":39092,\"start\":38372},{\"end\":39615,\"start\":39094},{\"end\":40270,\"start\":39617},{\"end\":43141,\"start\":40272},{\"end\":44463,\"start\":43143},{\"end\":45004,\"start\":44465},{\"end\":46001,\"start\":45006},{\"end\":46791,\"start\":46029},{\"end\":46853,\"start\":46807},{\"end\":47307,\"start\":46882},{\"end\":47657,\"start\":47309},{\"end\":47963,\"start\":47659}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14493,\"start\":14427},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16561,\"start\":16437},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18788,\"start\":18769},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20623,\"start\":20495},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21047,\"start\":21028},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22748,\"start\":22711},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23964,\"start\":23934}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":22597,\"start\":22590},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":25806,\"start\":25799},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27431,\"start\":27424},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27944,\"start\":27937},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":28194,\"start\":28187},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":28442,\"start\":28435},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29342,\"start\":29335},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":30548,\"start\":30541},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":32205,\"start\":32198},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":33570,\"start\":33563},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":33716,\"start\":33709},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38323,\"start\":38316},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39131,\"start\":39124},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39907,\"start\":39888},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":40027,\"start\":40020},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":40232,\"start\":40225},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":41736,\"start\":41729},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":43748,\"start\":43741},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":45025,\"start\":45018},{\"end\":46951,\"start\":46944},{\"end\":47243,\"start\":47236},{\"attributes\":{\"ref_id\":\"tab_19\"},\"end\":47352,\"start\":47345},{\"attributes\":{\"ref_id\":\"tab_20\"},\"end\":47364,\"start\":47357}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1851,\"start\":1839},{\"attributes\":{\"n\":\"2\"},\"end\":6347,\"start\":6335},{\"attributes\":{\"n\":\"3\"},\"end\":10614,\"start\":10603},{\"attributes\":{\"n\":\"3.1\"},\"end\":10636,\"start\":10617},{\"attributes\":{\"n\":\"3.2\"},\"end\":11562,\"start\":11539},{\"attributes\":{\"n\":\"3.3\"},\"end\":13556,\"start\":13517},{\"attributes\":{\"n\":\"3.4\"},\"end\":15020,\"start\":14991},{\"attributes\":{\"n\":\"3.5\"},\"end\":22826,\"start\":22792},{\"attributes\":{\"n\":\"3.6\"},\"end\":24354,\"start\":24327},{\"attributes\":{\"n\":\"4\"},\"end\":24719,\"start\":24708},{\"attributes\":{\"n\":\"4.1\"},\"end\":24744,\"start\":24722},{\"attributes\":{\"n\":\"4.2\"},\"end\":25572,\"start\":25551},{\"attributes\":{\"n\":\"4.3\"},\"end\":28738,\"start\":28709},{\"attributes\":{\"n\":\"4.4\"},\"end\":28755,\"start\":28741},{\"attributes\":{\"n\":\"5\"},\"end\":34633,\"start\":34623},{\"end\":35712,\"start\":35652},{\"end\":38159,\"start\":38122},{\"end\":46027,\"start\":46004},{\"end\":46805,\"start\":46794},{\"end\":46880,\"start\":46856},{\"end\":47973,\"start\":47965},{\"end\":48110,\"start\":48102},{\"end\":48146,\"start\":48137},{\"end\":48963,\"start\":48954},{\"end\":49017,\"start\":49008},{\"end\":49426,\"start\":49417},{\"end\":49482,\"start\":49473},{\"end\":49647,\"start\":49638},{\"end\":49687,\"start\":49678},{\"end\":49723,\"start\":49714},{\"end\":51157,\"start\":51148},{\"end\":51203,\"start\":51194}]", "table": "[{\"end\":48952,\"start\":48243},{\"end\":49415,\"start\":49042},{\"end\":49636,\"start\":49502},{\"end\":49676,\"start\":49652},{\"end\":51146,\"start\":49854},{\"end\":51536,\"start\":51456}]", "figure_caption": "[{\"end\":48100,\"start\":47975},{\"end\":48135,\"start\":48112},{\"end\":48243,\"start\":48148},{\"end\":49006,\"start\":48965},{\"end\":49042,\"start\":49019},{\"end\":49471,\"start\":49428},{\"end\":49502,\"start\":49484},{\"end\":49652,\"start\":49649},{\"end\":49712,\"start\":49689},{\"end\":49748,\"start\":49725},{\"end\":49854,\"start\":49751},{\"end\":51192,\"start\":51159},{\"end\":51456,\"start\":51205}]", "figure_ref": "[{\"end\":2537,\"start\":2531},{\"end\":16592,\"start\":16586},{\"end\":17870,\"start\":17864},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20974,\"start\":20968},{\"end\":21847,\"start\":21841},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30372,\"start\":30366},{\"end\":46790,\"start\":46784},{\"end\":46837,\"start\":46831},{\"end\":47700,\"start\":47692},{\"end\":47710,\"start\":47702},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47720,\"start\":47712},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":47730,\"start\":47722},{\"end\":47743,\"start\":47735},{\"end\":47923,\"start\":47917}]", "bib_author_first_name": "[{\"end\":51737,\"start\":51736},{\"end\":51739,\"start\":51738},{\"end\":51745,\"start\":51744},{\"end\":51747,\"start\":51746},{\"end\":51756,\"start\":51755},{\"end\":51758,\"start\":51757},{\"end\":52010,\"start\":52009},{\"end\":52021,\"start\":52020},{\"end\":52033,\"start\":52032},{\"end\":52035,\"start\":52034},{\"end\":52044,\"start\":52043},{\"end\":52058,\"start\":52057},{\"end\":52070,\"start\":52069},{\"end\":52606,\"start\":52605},{\"end\":52608,\"start\":52607},{\"end\":52616,\"start\":52615},{\"end\":52630,\"start\":52629},{\"end\":52642,\"start\":52641},{\"end\":52652,\"start\":52651},{\"end\":52654,\"start\":52653},{\"end\":53022,\"start\":53021},{\"end\":53024,\"start\":53023},{\"end\":53032,\"start\":53031},{\"end\":53039,\"start\":53038},{\"end\":53053,\"start\":53052},{\"end\":53064,\"start\":53063},{\"end\":53511,\"start\":53510},{\"end\":53513,\"start\":53512},{\"end\":53522,\"start\":53521},{\"end\":53531,\"start\":53530},{\"end\":53533,\"start\":53532},{\"end\":53540,\"start\":53539},{\"end\":53542,\"start\":53541},{\"end\":53787,\"start\":53786},{\"end\":53794,\"start\":53793},{\"end\":53802,\"start\":53801},{\"end\":53810,\"start\":53809},{\"end\":53817,\"start\":53816},{\"end\":53825,\"start\":53824},{\"end\":54161,\"start\":54160},{\"end\":54168,\"start\":54167},{\"end\":54175,\"start\":54174},{\"end\":54181,\"start\":54180},{\"end\":54401,\"start\":54400},{\"end\":54411,\"start\":54410},{\"end\":54422,\"start\":54421},{\"end\":54433,\"start\":54432},{\"end\":54443,\"start\":54442},{\"end\":54453,\"start\":54452},{\"end\":54821,\"start\":54820},{\"end\":54829,\"start\":54828},{\"end\":54837,\"start\":54836},{\"end\":54847,\"start\":54846},{\"end\":54849,\"start\":54848},{\"end\":54855,\"start\":54854},{\"end\":54861,\"start\":54860},{\"end\":55196,\"start\":55195},{\"end\":55204,\"start\":55203},{\"end\":55206,\"start\":55205},{\"end\":55313,\"start\":55312},{\"end\":55328,\"start\":55327},{\"end\":55337,\"start\":55336},{\"end\":55351,\"start\":55350},{\"end\":55366,\"start\":55365},{\"end\":55374,\"start\":55373},{\"end\":55389,\"start\":55388},{\"end\":55401,\"start\":55400},{\"end\":55413,\"start\":55412},{\"end\":55424,\"start\":55423},{\"end\":55853,\"start\":55852},{\"end\":55867,\"start\":55866},{\"end\":55879,\"start\":55878},{\"end\":55881,\"start\":55880},{\"end\":55893,\"start\":55892},{\"end\":55901,\"start\":55900},{\"end\":56126,\"start\":56125},{\"end\":56133,\"start\":56132},{\"end\":56140,\"start\":56139},{\"end\":56150,\"start\":56149},{\"end\":56314,\"start\":56313},{\"end\":56321,\"start\":56320},{\"end\":56328,\"start\":56327},{\"end\":56338,\"start\":56337},{\"end\":56584,\"start\":56583},{\"end\":56597,\"start\":56596},{\"end\":56609,\"start\":56608},{\"end\":56621,\"start\":56620},{\"end\":56880,\"start\":56879},{\"end\":56886,\"start\":56885},{\"end\":56895,\"start\":56894},{\"end\":56902,\"start\":56901},{\"end\":57293,\"start\":57292},{\"end\":57301,\"start\":57300},{\"end\":57700,\"start\":57699},{\"end\":57709,\"start\":57708},{\"end\":57720,\"start\":57719},{\"end\":57730,\"start\":57729},{\"end\":57740,\"start\":57739},{\"end\":57934,\"start\":57933},{\"end\":57940,\"start\":57939},{\"end\":57946,\"start\":57945},{\"end\":57955,\"start\":57954},{\"end\":57962,\"start\":57961},{\"end\":58347,\"start\":58346},{\"end\":58353,\"start\":58352},{\"end\":58362,\"start\":58361},{\"end\":58369,\"start\":58368},{\"end\":58751,\"start\":58750},{\"end\":58760,\"start\":58759},{\"end\":58768,\"start\":58767},{\"end\":58777,\"start\":58776},{\"end\":58784,\"start\":58783},{\"end\":58945,\"start\":58944},{\"end\":58953,\"start\":58952},{\"end\":58960,\"start\":58959},{\"end\":58967,\"start\":58966},{\"end\":58975,\"start\":58974},{\"end\":59094,\"start\":59093},{\"end\":59103,\"start\":59102},{\"end\":59113,\"start\":59112},{\"end\":59123,\"start\":59122},{\"end\":59139,\"start\":59138},{\"end\":59141,\"start\":59140},{\"end\":59481,\"start\":59480},{\"end\":59498,\"start\":59497},{\"end\":59506,\"start\":59505},{\"end\":59516,\"start\":59515},{\"end\":59865,\"start\":59864},{\"end\":59872,\"start\":59871},{\"end\":59879,\"start\":59878},{\"end\":59886,\"start\":59885},{\"end\":59894,\"start\":59893},{\"end\":59901,\"start\":59900},{\"end\":60132,\"start\":60131},{\"end\":60134,\"start\":60133},{\"end\":60141,\"start\":60140},{\"end\":60150,\"start\":60149},{\"end\":60161,\"start\":60160},{\"end\":60163,\"start\":60162},{\"end\":60592,\"start\":60591},{\"end\":60599,\"start\":60598},{\"end\":60606,\"start\":60605},{\"end\":60615,\"start\":60614},{\"end\":60807,\"start\":60806},{\"end\":60813,\"start\":60812},{\"end\":60824,\"start\":60823},{\"end\":60840,\"start\":60839},{\"end\":60847,\"start\":60846},{\"end\":60854,\"start\":60853},{\"end\":61258,\"start\":61257},{\"end\":61264,\"start\":61263},{\"end\":61271,\"start\":61270},{\"end\":61273,\"start\":61272},{\"end\":61284,\"start\":61283},{\"end\":61302,\"start\":61301},{\"end\":61589,\"start\":61588},{\"end\":61595,\"start\":61594},{\"end\":61602,\"start\":61601},{\"end\":61604,\"start\":61603},{\"end\":61612,\"start\":61611},{\"end\":61614,\"start\":61613},{\"end\":61621,\"start\":61620},{\"end\":61623,\"start\":61622},{\"end\":62029,\"start\":62028},{\"end\":62031,\"start\":62030},{\"end\":62038,\"start\":62037},{\"end\":62047,\"start\":62046},{\"end\":62059,\"start\":62058},{\"end\":62067,\"start\":62066},{\"end\":62077,\"start\":62076},{\"end\":62088,\"start\":62087},{\"end\":62098,\"start\":62097},{\"end\":62100,\"start\":62099},{\"end\":62415,\"start\":62414},{\"end\":62422,\"start\":62421},{\"end\":62431,\"start\":62430},{\"end\":62439,\"start\":62438},{\"end\":62441,\"start\":62440},{\"end\":62449,\"start\":62448},{\"end\":62708,\"start\":62707},{\"end\":62715,\"start\":62714},{\"end\":62724,\"start\":62723},{\"end\":62731,\"start\":62730},{\"end\":63135,\"start\":63134},{\"end\":63142,\"start\":63141},{\"end\":63149,\"start\":63148},{\"end\":63159,\"start\":63158},{\"end\":63572,\"start\":63571},{\"end\":63579,\"start\":63578},{\"end\":63588,\"start\":63587},{\"end\":63597,\"start\":63596},{\"end\":63830,\"start\":63829},{\"end\":63837,\"start\":63836},{\"end\":63844,\"start\":63843},{\"end\":63851,\"start\":63850},{\"end\":63857,\"start\":63856},{\"end\":63864,\"start\":63863},{\"end\":63873,\"start\":63872},{\"end\":63880,\"start\":63879},{\"end\":64232,\"start\":64231},{\"end\":64240,\"start\":64239},{\"end\":64253,\"start\":64252},{\"end\":64589,\"start\":64588},{\"end\":64603,\"start\":64602},{\"end\":64874,\"start\":64873},{\"end\":64880,\"start\":64879},{\"end\":64886,\"start\":64885},{\"end\":64893,\"start\":64892},{\"end\":64902,\"start\":64901},{\"end\":64904,\"start\":64903},{\"end\":64912,\"start\":64911},{\"end\":65332,\"start\":65331},{\"end\":65339,\"start\":65338},{\"end\":65347,\"start\":65346},{\"end\":65590,\"start\":65589},{\"end\":65597,\"start\":65596},{\"end\":65604,\"start\":65603},{\"end\":65869,\"start\":65868},{\"end\":65876,\"start\":65875},{\"end\":65883,\"start\":65882},{\"end\":65892,\"start\":65891},{\"end\":66225,\"start\":66224},{\"end\":66232,\"start\":66231},{\"end\":66239,\"start\":66238},{\"end\":66248,\"start\":66247},{\"end\":66553,\"start\":66552},{\"end\":66560,\"start\":66559},{\"end\":66567,\"start\":66566},{\"end\":66576,\"start\":66575},{\"end\":66968,\"start\":66967},{\"end\":66978,\"start\":66977},{\"end\":67362,\"start\":67361},{\"end\":67369,\"start\":67368},{\"end\":67377,\"start\":67376},{\"end\":67741,\"start\":67740},{\"end\":67752,\"start\":67751},{\"end\":67760,\"start\":67759},{\"end\":67769,\"start\":67768},{\"end\":67778,\"start\":67777},{\"end\":68016,\"start\":68015},{\"end\":68025,\"start\":68024},{\"end\":68040,\"start\":68039},{\"end\":68053,\"start\":68052},{\"end\":68062,\"start\":68061},{\"end\":68328,\"start\":68327},{\"end\":68336,\"start\":68335},{\"end\":68496,\"start\":68495},{\"end\":68505,\"start\":68504},{\"end\":68521,\"start\":68520},{\"end\":68706,\"start\":68705},{\"end\":68715,\"start\":68714},{\"end\":68731,\"start\":68730},{\"end\":68932,\"start\":68931},{\"end\":68941,\"start\":68940},{\"end\":68957,\"start\":68956},{\"end\":69107,\"start\":69106},{\"end\":69116,\"start\":69115},{\"end\":69126,\"start\":69125},{\"end\":69142,\"start\":69141},{\"end\":69151,\"start\":69150},{\"end\":69161,\"start\":69160},{\"end\":69464,\"start\":69463},{\"end\":69478,\"start\":69477},{\"end\":69735,\"start\":69734},{\"end\":69745,\"start\":69744},{\"end\":69755,\"start\":69754},{\"end\":69762,\"start\":69761},{\"end\":69770,\"start\":69769},{\"end\":69999,\"start\":69998},{\"end\":70007,\"start\":70006},{\"end\":70019,\"start\":70018},{\"end\":70194,\"start\":70193},{\"end\":70206,\"start\":70205},{\"end\":70452,\"start\":70451},{\"end\":70461,\"start\":70460},{\"end\":70472,\"start\":70471},{\"end\":70474,\"start\":70473},{\"end\":70752,\"start\":70751},{\"end\":70759,\"start\":70758},{\"end\":70767,\"start\":70766},{\"end\":70769,\"start\":70768},{\"end\":70776,\"start\":70775},{\"end\":70984,\"start\":70983},{\"end\":70991,\"start\":70990},{\"end\":70998,\"start\":70997},{\"end\":71007,\"start\":71006},{\"end\":71253,\"start\":71252},{\"end\":71260,\"start\":71259},{\"end\":71269,\"start\":71268},{\"end\":71520,\"start\":71519},{\"end\":71522,\"start\":71521},{\"end\":71531,\"start\":71530},{\"end\":71533,\"start\":71532},{\"end\":71548,\"start\":71547},{\"end\":71550,\"start\":71549},{\"end\":71816,\"start\":71815},{\"end\":71824,\"start\":71823},{\"end\":71832,\"start\":71831},{\"end\":71839,\"start\":71838},{\"end\":71847,\"start\":71846},{\"end\":71853,\"start\":71852},{\"end\":72163,\"start\":72162},{\"end\":72165,\"start\":72164},{\"end\":72179,\"start\":72178},{\"end\":72190,\"start\":72189},{\"end\":72204,\"start\":72203},{\"end\":72213,\"start\":72212},{\"end\":72221,\"start\":72220},{\"end\":72236,\"start\":72235},{\"end\":72244,\"start\":72243},{\"end\":72255,\"start\":72254},{\"end\":72266,\"start\":72265},{\"end\":72660,\"start\":72659},{\"end\":72670,\"start\":72669},{\"end\":72683,\"start\":72682},{\"end\":73095,\"start\":73094},{\"end\":73106,\"start\":73105},{\"end\":73117,\"start\":73116},{\"end\":73127,\"start\":73126},{\"end\":73140,\"start\":73139},{\"end\":73149,\"start\":73148},{\"end\":73151,\"start\":73150},{\"end\":73160,\"start\":73159},{\"end\":73170,\"start\":73169},{\"end\":73470,\"start\":73469},{\"end\":73481,\"start\":73480},{\"end\":73493,\"start\":73492},{\"end\":73506,\"start\":73505},{\"end\":73799,\"start\":73798},{\"end\":73807,\"start\":73806},{\"end\":73816,\"start\":73815},{\"end\":73822,\"start\":73821},{\"end\":73830,\"start\":73829},{\"end\":73837,\"start\":73836},{\"end\":74131,\"start\":74130},{\"end\":74139,\"start\":74138},{\"end\":74141,\"start\":74140},{\"end\":74149,\"start\":74148},{\"end\":74156,\"start\":74155},{\"end\":74164,\"start\":74163},{\"end\":74508,\"start\":74507},{\"end\":74516,\"start\":74515},{\"end\":74518,\"start\":74517},{\"end\":74524,\"start\":74523},{\"end\":74534,\"start\":74533},{\"end\":74542,\"start\":74541},{\"end\":74860,\"start\":74859},{\"end\":74868,\"start\":74867},{\"end\":74875,\"start\":74874},{\"end\":74881,\"start\":74880},{\"end\":74883,\"start\":74882},{\"end\":74890,\"start\":74889},{\"end\":74898,\"start\":74897},{\"end\":74907,\"start\":74906},{\"end\":74913,\"start\":74912},{\"end\":74920,\"start\":74919},{\"end\":75228,\"start\":75227},{\"end\":75234,\"start\":75233},{\"end\":75240,\"start\":75239},{\"end\":75246,\"start\":75245},{\"end\":75255,\"start\":75254},{\"end\":75488,\"start\":75487},{\"end\":75494,\"start\":75493},{\"end\":75500,\"start\":75499},{\"end\":75507,\"start\":75506},{\"end\":75515,\"start\":75514},{\"end\":75745,\"start\":75744},{\"end\":75753,\"start\":75752},{\"end\":75762,\"start\":75761},{\"end\":75772,\"start\":75771},{\"end\":75783,\"start\":75782},{\"end\":75793,\"start\":75792},{\"end\":76066,\"start\":76065},{\"end\":76068,\"start\":76067},{\"end\":76075,\"start\":76074},{\"end\":76082,\"start\":76081},{\"end\":76091,\"start\":76090},{\"end\":76527,\"start\":76526},{\"end\":76529,\"start\":76528},{\"end\":76536,\"start\":76535},{\"end\":76545,\"start\":76544},{\"end\":76552,\"start\":76551},{\"end\":76559,\"start\":76558},{\"end\":76966,\"start\":76965},{\"end\":76975,\"start\":76974},{\"end\":76983,\"start\":76982},{\"end\":76998,\"start\":76997},{\"end\":77005,\"start\":77004},{\"end\":77013,\"start\":77012},{\"end\":77019,\"start\":77018},{\"end\":77247,\"start\":77246},{\"end\":77255,\"start\":77254},{\"end\":77262,\"start\":77261},{\"end\":77268,\"start\":77267},{\"end\":77276,\"start\":77275},{\"end\":77546,\"start\":77545},{\"end\":77554,\"start\":77553},{\"end\":77562,\"start\":77561},{\"end\":77568,\"start\":77567},{\"end\":77575,\"start\":77574},{\"end\":77812,\"start\":77811},{\"end\":77821,\"start\":77820},{\"end\":77829,\"start\":77828},{\"end\":78273,\"start\":78272},{\"end\":78282,\"start\":78281},{\"end\":78289,\"start\":78288},{\"end\":78296,\"start\":78295},{\"end\":78303,\"start\":78302},{\"end\":78309,\"start\":78308},{\"end\":78801,\"start\":78800},{\"end\":78810,\"start\":78809},{\"end\":78817,\"start\":78816},{\"end\":78824,\"start\":78823},{\"end\":78831,\"start\":78830},{\"end\":79304,\"start\":79303},{\"end\":79313,\"start\":79312},{\"end\":79321,\"start\":79320},{\"end\":79328,\"start\":79327},{\"end\":79584,\"start\":79583},{\"end\":79593,\"start\":79592},{\"end\":79978,\"start\":79977},{\"end\":79986,\"start\":79985},{\"end\":79994,\"start\":79993},{\"end\":80000,\"start\":79999},{\"end\":80008,\"start\":80007},{\"end\":80014,\"start\":80013}]", "bib_author_last_name": "[{\"end\":51742,\"start\":51740},{\"end\":51753,\"start\":51748},{\"end\":51765,\"start\":51759},{\"end\":52018,\"start\":52011},{\"end\":52030,\"start\":52022},{\"end\":52041,\"start\":52036},{\"end\":52055,\"start\":52045},{\"end\":52067,\"start\":52059},{\"end\":52075,\"start\":52071},{\"end\":52613,\"start\":52609},{\"end\":52627,\"start\":52617},{\"end\":52639,\"start\":52631},{\"end\":52649,\"start\":52643},{\"end\":52661,\"start\":52655},{\"end\":53029,\"start\":53025},{\"end\":53036,\"start\":53033},{\"end\":53050,\"start\":53040},{\"end\":53061,\"start\":53054},{\"end\":53069,\"start\":53065},{\"end\":53519,\"start\":53514},{\"end\":53528,\"start\":53523},{\"end\":53537,\"start\":53534},{\"end\":53547,\"start\":53543},{\"end\":53791,\"start\":53788},{\"end\":53799,\"start\":53795},{\"end\":53807,\"start\":53803},{\"end\":53814,\"start\":53811},{\"end\":53822,\"start\":53818},{\"end\":53829,\"start\":53826},{\"end\":54165,\"start\":54162},{\"end\":54172,\"start\":54169},{\"end\":54178,\"start\":54176},{\"end\":54185,\"start\":54182},{\"end\":54408,\"start\":54402},{\"end\":54419,\"start\":54412},{\"end\":54430,\"start\":54423},{\"end\":54440,\"start\":54434},{\"end\":54450,\"start\":54444},{\"end\":54459,\"start\":54454},{\"end\":54826,\"start\":54822},{\"end\":54834,\"start\":54830},{\"end\":54844,\"start\":54838},{\"end\":54852,\"start\":54850},{\"end\":54858,\"start\":54856},{\"end\":54869,\"start\":54862},{\"end\":55201,\"start\":55197},{\"end\":55211,\"start\":55207},{\"end\":55325,\"start\":55314},{\"end\":55334,\"start\":55329},{\"end\":55348,\"start\":55338},{\"end\":55363,\"start\":55352},{\"end\":55371,\"start\":55367},{\"end\":55386,\"start\":55375},{\"end\":55398,\"start\":55390},{\"end\":55410,\"start\":55402},{\"end\":55421,\"start\":55414},{\"end\":55430,\"start\":55425},{\"end\":55864,\"start\":55854},{\"end\":55876,\"start\":55868},{\"end\":55890,\"start\":55882},{\"end\":55898,\"start\":55894},{\"end\":55911,\"start\":55902},{\"end\":56130,\"start\":56127},{\"end\":56137,\"start\":56134},{\"end\":56147,\"start\":56141},{\"end\":56156,\"start\":56151},{\"end\":56318,\"start\":56315},{\"end\":56325,\"start\":56322},{\"end\":56335,\"start\":56329},{\"end\":56344,\"start\":56339},{\"end\":56594,\"start\":56585},{\"end\":56606,\"start\":56598},{\"end\":56618,\"start\":56610},{\"end\":56627,\"start\":56622},{\"end\":56883,\"start\":56881},{\"end\":56892,\"start\":56887},{\"end\":56899,\"start\":56896},{\"end\":56906,\"start\":56903},{\"end\":57298,\"start\":57294},{\"end\":57305,\"start\":57302},{\"end\":57706,\"start\":57701},{\"end\":57717,\"start\":57710},{\"end\":57727,\"start\":57721},{\"end\":57737,\"start\":57731},{\"end\":57748,\"start\":57741},{\"end\":57937,\"start\":57935},{\"end\":57943,\"start\":57941},{\"end\":57952,\"start\":57947},{\"end\":57959,\"start\":57956},{\"end\":57966,\"start\":57963},{\"end\":58350,\"start\":58348},{\"end\":58359,\"start\":58354},{\"end\":58366,\"start\":58363},{\"end\":58373,\"start\":58370},{\"end\":58757,\"start\":58752},{\"end\":58765,\"start\":58761},{\"end\":58774,\"start\":58769},{\"end\":58781,\"start\":58778},{\"end\":58787,\"start\":58785},{\"end\":58950,\"start\":58946},{\"end\":58957,\"start\":58954},{\"end\":58964,\"start\":58961},{\"end\":58972,\"start\":58968},{\"end\":58980,\"start\":58976},{\"end\":59100,\"start\":59095},{\"end\":59110,\"start\":59104},{\"end\":59120,\"start\":59114},{\"end\":59136,\"start\":59124},{\"end\":59144,\"start\":59142},{\"end\":59495,\"start\":59482},{\"end\":59503,\"start\":59499},{\"end\":59513,\"start\":59507},{\"end\":59524,\"start\":59517},{\"end\":59869,\"start\":59866},{\"end\":59876,\"start\":59873},{\"end\":59883,\"start\":59880},{\"end\":59891,\"start\":59887},{\"end\":59898,\"start\":59895},{\"end\":59906,\"start\":59902},{\"end\":60138,\"start\":60135},{\"end\":60147,\"start\":60142},{\"end\":60158,\"start\":60151},{\"end\":60169,\"start\":60164},{\"end\":60596,\"start\":60593},{\"end\":60603,\"start\":60600},{\"end\":60612,\"start\":60607},{\"end\":60619,\"start\":60616},{\"end\":60810,\"start\":60808},{\"end\":60821,\"start\":60814},{\"end\":60837,\"start\":60825},{\"end\":60844,\"start\":60841},{\"end\":60851,\"start\":60848},{\"end\":60858,\"start\":60855},{\"end\":61261,\"start\":61259},{\"end\":61268,\"start\":61265},{\"end\":61281,\"start\":61274},{\"end\":61299,\"start\":61285},{\"end\":61313,\"start\":61303},{\"end\":61592,\"start\":61590},{\"end\":61599,\"start\":61596},{\"end\":61609,\"start\":61605},{\"end\":61618,\"start\":61615},{\"end\":61628,\"start\":61624},{\"end\":62035,\"start\":62032},{\"end\":62044,\"start\":62039},{\"end\":62056,\"start\":62048},{\"end\":62064,\"start\":62060},{\"end\":62074,\"start\":62068},{\"end\":62085,\"start\":62078},{\"end\":62095,\"start\":62089},{\"end\":62108,\"start\":62101},{\"end\":62419,\"start\":62416},{\"end\":62428,\"start\":62423},{\"end\":62436,\"start\":62432},{\"end\":62446,\"start\":62442},{\"end\":62453,\"start\":62450},{\"end\":62712,\"start\":62709},{\"end\":62721,\"start\":62716},{\"end\":62728,\"start\":62725},{\"end\":62735,\"start\":62732},{\"end\":63139,\"start\":63136},{\"end\":63146,\"start\":63143},{\"end\":63156,\"start\":63150},{\"end\":63164,\"start\":63160},{\"end\":63576,\"start\":63573},{\"end\":63585,\"start\":63580},{\"end\":63594,\"start\":63589},{\"end\":63600,\"start\":63598},{\"end\":63834,\"start\":63831},{\"end\":63841,\"start\":63838},{\"end\":63848,\"start\":63845},{\"end\":63854,\"start\":63852},{\"end\":63861,\"start\":63858},{\"end\":63870,\"start\":63865},{\"end\":63877,\"start\":63874},{\"end\":63884,\"start\":63881},{\"end\":64237,\"start\":64233},{\"end\":64250,\"start\":64241},{\"end\":64261,\"start\":64254},{\"end\":64600,\"start\":64590},{\"end\":64610,\"start\":64604},{\"end\":64877,\"start\":64875},{\"end\":64883,\"start\":64881},{\"end\":64890,\"start\":64887},{\"end\":64899,\"start\":64894},{\"end\":64909,\"start\":64905},{\"end\":64918,\"start\":64913},{\"end\":65336,\"start\":65333},{\"end\":65344,\"start\":65340},{\"end\":65351,\"start\":65348},{\"end\":65594,\"start\":65591},{\"end\":65601,\"start\":65598},{\"end\":65608,\"start\":65605},{\"end\":65873,\"start\":65870},{\"end\":65880,\"start\":65877},{\"end\":65889,\"start\":65884},{\"end\":65896,\"start\":65893},{\"end\":66229,\"start\":66226},{\"end\":66236,\"start\":66233},{\"end\":66245,\"start\":66240},{\"end\":66252,\"start\":66249},{\"end\":66557,\"start\":66554},{\"end\":66564,\"start\":66561},{\"end\":66573,\"start\":66568},{\"end\":66580,\"start\":66577},{\"end\":66975,\"start\":66969},{\"end\":66988,\"start\":66979},{\"end\":67366,\"start\":67363},{\"end\":67374,\"start\":67370},{\"end\":67381,\"start\":67378},{\"end\":67749,\"start\":67742},{\"end\":67757,\"start\":67753},{\"end\":67766,\"start\":67761},{\"end\":67775,\"start\":67770},{\"end\":67788,\"start\":67779},{\"end\":68022,\"start\":68017},{\"end\":68037,\"start\":68026},{\"end\":68050,\"start\":68041},{\"end\":68059,\"start\":68054},{\"end\":68074,\"start\":68063},{\"end\":68333,\"start\":68329},{\"end\":68347,\"start\":68337},{\"end\":68502,\"start\":68497},{\"end\":68518,\"start\":68506},{\"end\":68527,\"start\":68522},{\"end\":68712,\"start\":68707},{\"end\":68728,\"start\":68716},{\"end\":68737,\"start\":68732},{\"end\":68938,\"start\":68933},{\"end\":68954,\"start\":68942},{\"end\":68963,\"start\":68958},{\"end\":69113,\"start\":69108},{\"end\":69123,\"start\":69117},{\"end\":69139,\"start\":69127},{\"end\":69148,\"start\":69143},{\"end\":69158,\"start\":69152},{\"end\":69167,\"start\":69162},{\"end\":69475,\"start\":69465},{\"end\":69487,\"start\":69479},{\"end\":69742,\"start\":69736},{\"end\":69752,\"start\":69746},{\"end\":69759,\"start\":69756},{\"end\":69767,\"start\":69763},{\"end\":69776,\"start\":69771},{\"end\":70004,\"start\":70000},{\"end\":70016,\"start\":70008},{\"end\":70029,\"start\":70020},{\"end\":70203,\"start\":70195},{\"end\":70216,\"start\":70207},{\"end\":70458,\"start\":70453},{\"end\":70469,\"start\":70462},{\"end\":70480,\"start\":70475},{\"end\":70756,\"start\":70753},{\"end\":70764,\"start\":70760},{\"end\":70773,\"start\":70770},{\"end\":70782,\"start\":70777},{\"end\":70988,\"start\":70985},{\"end\":70995,\"start\":70992},{\"end\":71004,\"start\":70999},{\"end\":71016,\"start\":71008},{\"end\":71257,\"start\":71254},{\"end\":71266,\"start\":71261},{\"end\":71279,\"start\":71270},{\"end\":71528,\"start\":71523},{\"end\":71545,\"start\":71534},{\"end\":71555,\"start\":71551},{\"end\":71821,\"start\":71817},{\"end\":71829,\"start\":71825},{\"end\":71836,\"start\":71833},{\"end\":71844,\"start\":71840},{\"end\":71850,\"start\":71848},{\"end\":71857,\"start\":71854},{\"end\":72176,\"start\":72166},{\"end\":72187,\"start\":72180},{\"end\":72201,\"start\":72191},{\"end\":72210,\"start\":72205},{\"end\":72218,\"start\":72214},{\"end\":72233,\"start\":72222},{\"end\":72241,\"start\":72237},{\"end\":72252,\"start\":72245},{\"end\":72263,\"start\":72256},{\"end\":72276,\"start\":72267},{\"end\":72667,\"start\":72661},{\"end\":72680,\"start\":72671},{\"end\":72691,\"start\":72684},{\"end\":73103,\"start\":73096},{\"end\":73114,\"start\":73107},{\"end\":73124,\"start\":73118},{\"end\":73137,\"start\":73128},{\"end\":73146,\"start\":73141},{\"end\":73157,\"start\":73152},{\"end\":73167,\"start\":73161},{\"end\":73181,\"start\":73171},{\"end\":73478,\"start\":73471},{\"end\":73490,\"start\":73482},{\"end\":73503,\"start\":73494},{\"end\":73515,\"start\":73507},{\"end\":73804,\"start\":73800},{\"end\":73813,\"start\":73808},{\"end\":73819,\"start\":73817},{\"end\":73827,\"start\":73823},{\"end\":73834,\"start\":73831},{\"end\":73842,\"start\":73838},{\"end\":74136,\"start\":74132},{\"end\":74146,\"start\":74142},{\"end\":74153,\"start\":74150},{\"end\":74161,\"start\":74157},{\"end\":74169,\"start\":74165},{\"end\":74513,\"start\":74509},{\"end\":74521,\"start\":74519},{\"end\":74531,\"start\":74525},{\"end\":74539,\"start\":74535},{\"end\":74545,\"start\":74543},{\"end\":74865,\"start\":74861},{\"end\":74872,\"start\":74869},{\"end\":74878,\"start\":74876},{\"end\":74887,\"start\":74884},{\"end\":74895,\"start\":74891},{\"end\":74904,\"start\":74899},{\"end\":74910,\"start\":74908},{\"end\":74917,\"start\":74914},{\"end\":74925,\"start\":74921},{\"end\":75231,\"start\":75229},{\"end\":75237,\"start\":75235},{\"end\":75243,\"start\":75241},{\"end\":75252,\"start\":75247},{\"end\":75259,\"start\":75256},{\"end\":75491,\"start\":75489},{\"end\":75497,\"start\":75495},{\"end\":75504,\"start\":75501},{\"end\":75512,\"start\":75508},{\"end\":75519,\"start\":75516},{\"end\":75750,\"start\":75746},{\"end\":75759,\"start\":75754},{\"end\":75769,\"start\":75763},{\"end\":75780,\"start\":75773},{\"end\":75790,\"start\":75784},{\"end\":75802,\"start\":75794},{\"end\":76072,\"start\":76069},{\"end\":76079,\"start\":76076},{\"end\":76088,\"start\":76083},{\"end\":76096,\"start\":76092},{\"end\":76533,\"start\":76530},{\"end\":76542,\"start\":76537},{\"end\":76549,\"start\":76546},{\"end\":76556,\"start\":76553},{\"end\":76564,\"start\":76560},{\"end\":76972,\"start\":76967},{\"end\":76980,\"start\":76976},{\"end\":76995,\"start\":76984},{\"end\":77002,\"start\":76999},{\"end\":77010,\"start\":77006},{\"end\":77016,\"start\":77014},{\"end\":77025,\"start\":77020},{\"end\":77252,\"start\":77248},{\"end\":77259,\"start\":77256},{\"end\":77265,\"start\":77263},{\"end\":77273,\"start\":77269},{\"end\":77279,\"start\":77277},{\"end\":77551,\"start\":77547},{\"end\":77559,\"start\":77555},{\"end\":77565,\"start\":77563},{\"end\":77572,\"start\":77569},{\"end\":77579,\"start\":77576},{\"end\":77818,\"start\":77813},{\"end\":77826,\"start\":77822},{\"end\":77833,\"start\":77830},{\"end\":78279,\"start\":78274},{\"end\":78286,\"start\":78283},{\"end\":78293,\"start\":78290},{\"end\":78300,\"start\":78297},{\"end\":78306,\"start\":78304},{\"end\":78313,\"start\":78310},{\"end\":78807,\"start\":78802},{\"end\":78814,\"start\":78811},{\"end\":78821,\"start\":78818},{\"end\":78828,\"start\":78825},{\"end\":78836,\"start\":78832},{\"end\":79310,\"start\":79305},{\"end\":79318,\"start\":79314},{\"end\":79325,\"start\":79322},{\"end\":79333,\"start\":79329},{\"end\":79590,\"start\":79585},{\"end\":79598,\"start\":79594},{\"end\":79983,\"start\":79979},{\"end\":79991,\"start\":79987},{\"end\":79997,\"start\":79995},{\"end\":80005,\"start\":80001},{\"end\":80011,\"start\":80009},{\"end\":80017,\"start\":80015}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":51914,\"start\":51736},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":228372977},\"end\":52490,\"start\":51916},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3429309},\"end\":52936,\"start\":52492},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3638670},\"end\":53404,\"start\":52938},{\"attributes\":{\"id\":\"b4\"},\"end\":53721,\"start\":53406},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":239886012},\"end\":54094,\"start\":53723},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":235376986},\"end\":54398,\"start\":54096},{\"attributes\":{\"doi\":\"arXiv:2103.10697\",\"id\":\"b7\"},\"end\":54766,\"start\":54400},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57246310},\"end\":55137,\"start\":54768},{\"attributes\":{\"id\":\"b9\"},\"end\":55310,\"start\":55139},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b10\"},\"end\":55800,\"start\":55312},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4246903},\"end\":56123,\"start\":55802},{\"attributes\":{\"id\":\"b12\"},\"end\":56248,\"start\":56125},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6050505},\"end\":56540,\"start\":56250},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9272368},\"end\":56831,\"start\":56542},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":57220,\"start\":56833},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235358697},\"end\":57632,\"start\":57222},{\"attributes\":{\"id\":\"b17\"},\"end\":57891,\"start\":57634},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":37158713},\"end\":58297,\"start\":57893},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":131776850},\"end\":58685,\"start\":58299},{\"attributes\":{\"id\":\"b20\"},\"end\":58920,\"start\":58687},{\"attributes\":{\"id\":\"b21\"},\"end\":59091,\"start\":58922},{\"attributes\":{\"doi\":\"arXiv:2103.14167\",\"id\":\"b22\"},\"end\":59399,\"start\":59093},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":220250819},\"end\":59785,\"start\":59401},{\"attributes\":{\"id\":\"b24\"},\"end\":60060,\"start\":59787},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235719539},\"end\":60535,\"start\":60062},{\"attributes\":{\"id\":\"b26\"},\"end\":60734,\"start\":60537},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":233025288},\"end\":61255,\"start\":60736},{\"attributes\":{\"id\":\"b28\"},\"end\":61528,\"start\":61257},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":198967567},\"end\":61983,\"start\":61530},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14113767},\"end\":62340,\"start\":61985},{\"attributes\":{\"doi\":\"arXiv:2108.08518\",\"id\":\"b31\"},\"end\":62646,\"start\":62342},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":214623458},\"end\":63075,\"start\":62648},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":219872462},\"end\":63504,\"start\":63077},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220496413},\"end\":63827,\"start\":63506},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b35\"},\"end\":64173,\"start\":63829},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1629541},\"end\":64586,\"start\":64175},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b37\"},\"end\":64786,\"start\":64588},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":236950782},\"end\":65277,\"start\":64788},{\"attributes\":{\"doi\":\"arXiv:2104.01538\",\"id\":\"b39\"},\"end\":65501,\"start\":65279},{\"attributes\":{\"doi\":\"arXiv:2109.05221\",\"id\":\"b40\"},\"end\":65791,\"start\":65503},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":201070119},\"end\":66222,\"start\":65793},{\"attributes\":{\"doi\":\"arXiv:1908.10543\",\"id\":\"b42\"},\"end\":66490,\"start\":66224},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":220665709},\"end\":66907,\"start\":66492},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":203593252},\"end\":67301,\"start\":66909},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":623137},\"end\":67670,\"start\":67303},{\"attributes\":{\"id\":\"b46\"},\"end\":67949,\"start\":67672},{\"attributes\":{\"doi\":\"arXiv:2108.08810\",\"id\":\"b47\"},\"end\":68278,\"start\":67951},{\"attributes\":{\"id\":\"b48\"},\"end\":68427,\"start\":68280},{\"attributes\":{\"id\":\"b49\"},\"end\":68654,\"start\":68429},{\"attributes\":{\"id\":\"b50\"},\"end\":68849,\"start\":68656},{\"attributes\":{\"id\":\"b51\"},\"end\":69104,\"start\":68851},{\"attributes\":{\"doi\":\"arXiv:1810.10510\",\"id\":\"b52\"},\"end\":69382,\"start\":69106},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":195859047},\"end\":69687,\"start\":69384},{\"attributes\":{\"doi\":\"arXiv:1709.03410\",\"id\":\"b54\"},\"end\":69945,\"start\":69689},{\"attributes\":{\"doi\":\"arXiv:1902.11123\",\"id\":\"b55\"},\"end\":70191,\"start\":69947},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b56\"},\"end\":70449,\"start\":70193},{\"attributes\":{\"doi\":\"arXiv:1703.05175\",\"id\":\"b57\"},\"end\":70677,\"start\":70451},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":30824366},\"end\":70922,\"start\":70679},{\"attributes\":{\"doi\":\"arXiv:2108.02266\",\"id\":\"b59\"},\"end\":71188,\"start\":70924},{\"attributes\":{\"doi\":\"arXiv:2005.10821\",\"id\":\"b60\"},\"end\":71446,\"start\":71190},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":17856855},\"end\":71746,\"start\":71448},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":220961511},\"end\":72113,\"start\":71748},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":233714958},\"end\":72581,\"start\":72115},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":209202711},\"end\":73065,\"start\":72583},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":13756489},\"end\":73426,\"start\":73067},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":8909022},\"end\":73729,\"start\":73428},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":221757772},\"end\":74058,\"start\":73731},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":201070109},\"end\":74505,\"start\":74060},{\"attributes\":{\"doi\":\"arXiv:2006.04768\",\"id\":\"b69\"},\"end\":74765,\"start\":74507},{\"attributes\":{\"doi\":\"arXiv:2102.12122\",\"id\":\"b70\"},\"end\":75173,\"start\":74767},{\"attributes\":{\"doi\":\"arXiv:2108.09084\",\"id\":\"b71\"},\"end\":75425,\"start\":75175},{\"attributes\":{\"doi\":\"arXiv:2106.04108\",\"id\":\"b72\"},\"end\":75693,\"start\":75427},{\"attributes\":{\"doi\":\"arXiv:2106.14881\",\"id\":\"b73\"},\"end\":75995,\"start\":75695},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":235679100},\"end\":76465,\"start\":75997},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":244045248},\"end\":76888,\"start\":76467},{\"attributes\":{\"id\":\"b76\"},\"end\":77183,\"start\":76890},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":221089994},\"end\":77494,\"start\":77185},{\"attributes\":{\"doi\":\"arXiv:2103.15402\",\"id\":\"b78\"},\"end\":77744,\"start\":77496},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":232417101},\"end\":78171,\"start\":77746},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":204973537},\"end\":78695,\"start\":78173},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":70349904},\"end\":79245,\"start\":78697},{\"attributes\":{\"doi\":\"arXiv:2106.02320\",\"id\":\"b82\"},\"end\":79499,\"start\":79247},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":244102343},\"end\":79916,\"start\":79501},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":236772600},\"end\":80360,\"start\":79918}]", "bib_title": "[{\"end\":52007,\"start\":51916},{\"end\":52603,\"start\":52492},{\"end\":53019,\"start\":52938},{\"end\":53784,\"start\":53723},{\"end\":54158,\"start\":54096},{\"end\":54818,\"start\":54768},{\"end\":55850,\"start\":55802},{\"end\":56311,\"start\":56250},{\"end\":56581,\"start\":56542},{\"end\":56877,\"start\":56833},{\"end\":57290,\"start\":57222},{\"end\":57931,\"start\":57893},{\"end\":58344,\"start\":58299},{\"end\":59478,\"start\":59401},{\"end\":60129,\"start\":60062},{\"end\":60804,\"start\":60736},{\"end\":61586,\"start\":61530},{\"end\":62026,\"start\":61985},{\"end\":62705,\"start\":62648},{\"end\":63132,\"start\":63077},{\"end\":63569,\"start\":63506},{\"end\":64229,\"start\":64175},{\"end\":64871,\"start\":64788},{\"end\":65866,\"start\":65793},{\"end\":66550,\"start\":66492},{\"end\":66965,\"start\":66909},{\"end\":67359,\"start\":67303},{\"end\":69461,\"start\":69384},{\"end\":70749,\"start\":70679},{\"end\":71517,\"start\":71448},{\"end\":71813,\"start\":71748},{\"end\":72160,\"start\":72115},{\"end\":72657,\"start\":72583},{\"end\":73092,\"start\":73067},{\"end\":73467,\"start\":73428},{\"end\":73796,\"start\":73731},{\"end\":74128,\"start\":74060},{\"end\":76063,\"start\":75997},{\"end\":76524,\"start\":76467},{\"end\":77244,\"start\":77185},{\"end\":77809,\"start\":77746},{\"end\":78270,\"start\":78173},{\"end\":78798,\"start\":78697},{\"end\":79581,\"start\":79501},{\"end\":79975,\"start\":79918}]", "bib_author": "[{\"end\":51744,\"start\":51736},{\"end\":51755,\"start\":51744},{\"end\":51767,\"start\":51755},{\"end\":52020,\"start\":52009},{\"end\":52032,\"start\":52020},{\"end\":52043,\"start\":52032},{\"end\":52057,\"start\":52043},{\"end\":52069,\"start\":52057},{\"end\":52077,\"start\":52069},{\"end\":52615,\"start\":52605},{\"end\":52629,\"start\":52615},{\"end\":52641,\"start\":52629},{\"end\":52651,\"start\":52641},{\"end\":52663,\"start\":52651},{\"end\":53031,\"start\":53021},{\"end\":53038,\"start\":53031},{\"end\":53052,\"start\":53038},{\"end\":53063,\"start\":53052},{\"end\":53071,\"start\":53063},{\"end\":53521,\"start\":53510},{\"end\":53530,\"start\":53521},{\"end\":53539,\"start\":53530},{\"end\":53549,\"start\":53539},{\"end\":53793,\"start\":53786},{\"end\":53801,\"start\":53793},{\"end\":53809,\"start\":53801},{\"end\":53816,\"start\":53809},{\"end\":53824,\"start\":53816},{\"end\":53831,\"start\":53824},{\"end\":54167,\"start\":54160},{\"end\":54174,\"start\":54167},{\"end\":54180,\"start\":54174},{\"end\":54187,\"start\":54180},{\"end\":54410,\"start\":54400},{\"end\":54421,\"start\":54410},{\"end\":54432,\"start\":54421},{\"end\":54442,\"start\":54432},{\"end\":54452,\"start\":54442},{\"end\":54461,\"start\":54452},{\"end\":54828,\"start\":54820},{\"end\":54836,\"start\":54828},{\"end\":54846,\"start\":54836},{\"end\":54854,\"start\":54846},{\"end\":54860,\"start\":54854},{\"end\":54871,\"start\":54860},{\"end\":55203,\"start\":55195},{\"end\":55213,\"start\":55203},{\"end\":55327,\"start\":55312},{\"end\":55336,\"start\":55327},{\"end\":55350,\"start\":55336},{\"end\":55365,\"start\":55350},{\"end\":55373,\"start\":55365},{\"end\":55388,\"start\":55373},{\"end\":55400,\"start\":55388},{\"end\":55412,\"start\":55400},{\"end\":55423,\"start\":55412},{\"end\":55432,\"start\":55423},{\"end\":55866,\"start\":55852},{\"end\":55878,\"start\":55866},{\"end\":55892,\"start\":55878},{\"end\":55900,\"start\":55892},{\"end\":55913,\"start\":55900},{\"end\":56132,\"start\":56125},{\"end\":56139,\"start\":56132},{\"end\":56149,\"start\":56139},{\"end\":56158,\"start\":56149},{\"end\":56320,\"start\":56313},{\"end\":56327,\"start\":56320},{\"end\":56337,\"start\":56327},{\"end\":56346,\"start\":56337},{\"end\":56596,\"start\":56583},{\"end\":56608,\"start\":56596},{\"end\":56620,\"start\":56608},{\"end\":56629,\"start\":56620},{\"end\":56885,\"start\":56879},{\"end\":56894,\"start\":56885},{\"end\":56901,\"start\":56894},{\"end\":56908,\"start\":56901},{\"end\":57300,\"start\":57292},{\"end\":57307,\"start\":57300},{\"end\":57708,\"start\":57699},{\"end\":57719,\"start\":57708},{\"end\":57729,\"start\":57719},{\"end\":57739,\"start\":57729},{\"end\":57750,\"start\":57739},{\"end\":57939,\"start\":57933},{\"end\":57945,\"start\":57939},{\"end\":57954,\"start\":57945},{\"end\":57961,\"start\":57954},{\"end\":57968,\"start\":57961},{\"end\":58352,\"start\":58346},{\"end\":58361,\"start\":58352},{\"end\":58368,\"start\":58361},{\"end\":58375,\"start\":58368},{\"end\":58759,\"start\":58750},{\"end\":58767,\"start\":58759},{\"end\":58776,\"start\":58767},{\"end\":58783,\"start\":58776},{\"end\":58789,\"start\":58783},{\"end\":58952,\"start\":58944},{\"end\":58959,\"start\":58952},{\"end\":58966,\"start\":58959},{\"end\":58974,\"start\":58966},{\"end\":58982,\"start\":58974},{\"end\":59102,\"start\":59093},{\"end\":59112,\"start\":59102},{\"end\":59122,\"start\":59112},{\"end\":59138,\"start\":59122},{\"end\":59146,\"start\":59138},{\"end\":59497,\"start\":59480},{\"end\":59505,\"start\":59497},{\"end\":59515,\"start\":59505},{\"end\":59526,\"start\":59515},{\"end\":59871,\"start\":59864},{\"end\":59878,\"start\":59871},{\"end\":59885,\"start\":59878},{\"end\":59893,\"start\":59885},{\"end\":59900,\"start\":59893},{\"end\":59908,\"start\":59900},{\"end\":60140,\"start\":60131},{\"end\":60149,\"start\":60140},{\"end\":60160,\"start\":60149},{\"end\":60171,\"start\":60160},{\"end\":60598,\"start\":60591},{\"end\":60605,\"start\":60598},{\"end\":60614,\"start\":60605},{\"end\":60621,\"start\":60614},{\"end\":60812,\"start\":60806},{\"end\":60823,\"start\":60812},{\"end\":60839,\"start\":60823},{\"end\":60846,\"start\":60839},{\"end\":60853,\"start\":60846},{\"end\":60860,\"start\":60853},{\"end\":61263,\"start\":61257},{\"end\":61270,\"start\":61263},{\"end\":61283,\"start\":61270},{\"end\":61301,\"start\":61283},{\"end\":61315,\"start\":61301},{\"end\":61594,\"start\":61588},{\"end\":61601,\"start\":61594},{\"end\":61611,\"start\":61601},{\"end\":61620,\"start\":61611},{\"end\":61630,\"start\":61620},{\"end\":62037,\"start\":62028},{\"end\":62046,\"start\":62037},{\"end\":62058,\"start\":62046},{\"end\":62066,\"start\":62058},{\"end\":62076,\"start\":62066},{\"end\":62087,\"start\":62076},{\"end\":62097,\"start\":62087},{\"end\":62110,\"start\":62097},{\"end\":62421,\"start\":62414},{\"end\":62430,\"start\":62421},{\"end\":62438,\"start\":62430},{\"end\":62448,\"start\":62438},{\"end\":62455,\"start\":62448},{\"end\":62714,\"start\":62707},{\"end\":62723,\"start\":62714},{\"end\":62730,\"start\":62723},{\"end\":62737,\"start\":62730},{\"end\":63141,\"start\":63134},{\"end\":63148,\"start\":63141},{\"end\":63158,\"start\":63148},{\"end\":63166,\"start\":63158},{\"end\":63578,\"start\":63571},{\"end\":63587,\"start\":63578},{\"end\":63596,\"start\":63587},{\"end\":63602,\"start\":63596},{\"end\":63836,\"start\":63829},{\"end\":63843,\"start\":63836},{\"end\":63850,\"start\":63843},{\"end\":63856,\"start\":63850},{\"end\":63863,\"start\":63856},{\"end\":63872,\"start\":63863},{\"end\":63879,\"start\":63872},{\"end\":63886,\"start\":63879},{\"end\":64239,\"start\":64231},{\"end\":64252,\"start\":64239},{\"end\":64263,\"start\":64252},{\"end\":64602,\"start\":64588},{\"end\":64612,\"start\":64602},{\"end\":64879,\"start\":64873},{\"end\":64885,\"start\":64879},{\"end\":64892,\"start\":64885},{\"end\":64901,\"start\":64892},{\"end\":64911,\"start\":64901},{\"end\":64920,\"start\":64911},{\"end\":65338,\"start\":65331},{\"end\":65346,\"start\":65338},{\"end\":65353,\"start\":65346},{\"end\":65596,\"start\":65589},{\"end\":65603,\"start\":65596},{\"end\":65610,\"start\":65603},{\"end\":65875,\"start\":65868},{\"end\":65882,\"start\":65875},{\"end\":65891,\"start\":65882},{\"end\":65898,\"start\":65891},{\"end\":66231,\"start\":66224},{\"end\":66238,\"start\":66231},{\"end\":66247,\"start\":66238},{\"end\":66254,\"start\":66247},{\"end\":66559,\"start\":66552},{\"end\":66566,\"start\":66559},{\"end\":66575,\"start\":66566},{\"end\":66582,\"start\":66575},{\"end\":66977,\"start\":66967},{\"end\":66990,\"start\":66977},{\"end\":67368,\"start\":67361},{\"end\":67376,\"start\":67368},{\"end\":67383,\"start\":67376},{\"end\":67751,\"start\":67740},{\"end\":67759,\"start\":67751},{\"end\":67768,\"start\":67759},{\"end\":67777,\"start\":67768},{\"end\":67790,\"start\":67777},{\"end\":68024,\"start\":68015},{\"end\":68039,\"start\":68024},{\"end\":68052,\"start\":68039},{\"end\":68061,\"start\":68052},{\"end\":68076,\"start\":68061},{\"end\":68335,\"start\":68327},{\"end\":68349,\"start\":68335},{\"end\":68504,\"start\":68495},{\"end\":68520,\"start\":68504},{\"end\":68529,\"start\":68520},{\"end\":68714,\"start\":68705},{\"end\":68730,\"start\":68714},{\"end\":68739,\"start\":68730},{\"end\":68940,\"start\":68931},{\"end\":68956,\"start\":68940},{\"end\":68965,\"start\":68956},{\"end\":69115,\"start\":69106},{\"end\":69125,\"start\":69115},{\"end\":69141,\"start\":69125},{\"end\":69150,\"start\":69141},{\"end\":69160,\"start\":69150},{\"end\":69169,\"start\":69160},{\"end\":69477,\"start\":69463},{\"end\":69489,\"start\":69477},{\"end\":69744,\"start\":69734},{\"end\":69754,\"start\":69744},{\"end\":69761,\"start\":69754},{\"end\":69769,\"start\":69761},{\"end\":69778,\"start\":69769},{\"end\":70006,\"start\":69998},{\"end\":70018,\"start\":70006},{\"end\":70031,\"start\":70018},{\"end\":70205,\"start\":70193},{\"end\":70218,\"start\":70205},{\"end\":70460,\"start\":70451},{\"end\":70471,\"start\":70460},{\"end\":70482,\"start\":70471},{\"end\":70758,\"start\":70751},{\"end\":70766,\"start\":70758},{\"end\":70775,\"start\":70766},{\"end\":70784,\"start\":70775},{\"end\":70990,\"start\":70983},{\"end\":70997,\"start\":70990},{\"end\":71006,\"start\":70997},{\"end\":71018,\"start\":71006},{\"end\":71259,\"start\":71252},{\"end\":71268,\"start\":71259},{\"end\":71281,\"start\":71268},{\"end\":71530,\"start\":71519},{\"end\":71547,\"start\":71530},{\"end\":71557,\"start\":71547},{\"end\":71823,\"start\":71815},{\"end\":71831,\"start\":71823},{\"end\":71838,\"start\":71831},{\"end\":71846,\"start\":71838},{\"end\":71852,\"start\":71846},{\"end\":71859,\"start\":71852},{\"end\":72178,\"start\":72162},{\"end\":72189,\"start\":72178},{\"end\":72203,\"start\":72189},{\"end\":72212,\"start\":72203},{\"end\":72220,\"start\":72212},{\"end\":72235,\"start\":72220},{\"end\":72243,\"start\":72235},{\"end\":72254,\"start\":72243},{\"end\":72265,\"start\":72254},{\"end\":72278,\"start\":72265},{\"end\":72669,\"start\":72659},{\"end\":72682,\"start\":72669},{\"end\":72693,\"start\":72682},{\"end\":73105,\"start\":73094},{\"end\":73116,\"start\":73105},{\"end\":73126,\"start\":73116},{\"end\":73139,\"start\":73126},{\"end\":73148,\"start\":73139},{\"end\":73159,\"start\":73148},{\"end\":73169,\"start\":73159},{\"end\":73183,\"start\":73169},{\"end\":73480,\"start\":73469},{\"end\":73492,\"start\":73480},{\"end\":73505,\"start\":73492},{\"end\":73517,\"start\":73505},{\"end\":73806,\"start\":73798},{\"end\":73815,\"start\":73806},{\"end\":73821,\"start\":73815},{\"end\":73829,\"start\":73821},{\"end\":73836,\"start\":73829},{\"end\":73844,\"start\":73836},{\"end\":74138,\"start\":74130},{\"end\":74148,\"start\":74138},{\"end\":74155,\"start\":74148},{\"end\":74163,\"start\":74155},{\"end\":74171,\"start\":74163},{\"end\":74515,\"start\":74507},{\"end\":74523,\"start\":74515},{\"end\":74533,\"start\":74523},{\"end\":74541,\"start\":74533},{\"end\":74547,\"start\":74541},{\"end\":74867,\"start\":74859},{\"end\":74874,\"start\":74867},{\"end\":74880,\"start\":74874},{\"end\":74889,\"start\":74880},{\"end\":74897,\"start\":74889},{\"end\":74906,\"start\":74897},{\"end\":74912,\"start\":74906},{\"end\":74919,\"start\":74912},{\"end\":74927,\"start\":74919},{\"end\":75233,\"start\":75227},{\"end\":75239,\"start\":75233},{\"end\":75245,\"start\":75239},{\"end\":75254,\"start\":75245},{\"end\":75261,\"start\":75254},{\"end\":75493,\"start\":75487},{\"end\":75499,\"start\":75493},{\"end\":75506,\"start\":75499},{\"end\":75514,\"start\":75506},{\"end\":75521,\"start\":75514},{\"end\":75752,\"start\":75744},{\"end\":75761,\"start\":75752},{\"end\":75771,\"start\":75761},{\"end\":75782,\"start\":75771},{\"end\":75792,\"start\":75782},{\"end\":75804,\"start\":75792},{\"end\":76074,\"start\":76065},{\"end\":76081,\"start\":76074},{\"end\":76090,\"start\":76081},{\"end\":76098,\"start\":76090},{\"end\":76535,\"start\":76526},{\"end\":76544,\"start\":76535},{\"end\":76551,\"start\":76544},{\"end\":76558,\"start\":76551},{\"end\":76566,\"start\":76558},{\"end\":76974,\"start\":76965},{\"end\":76982,\"start\":76974},{\"end\":76997,\"start\":76982},{\"end\":77004,\"start\":76997},{\"end\":77012,\"start\":77004},{\"end\":77018,\"start\":77012},{\"end\":77027,\"start\":77018},{\"end\":77254,\"start\":77246},{\"end\":77261,\"start\":77254},{\"end\":77267,\"start\":77261},{\"end\":77275,\"start\":77267},{\"end\":77281,\"start\":77275},{\"end\":77553,\"start\":77545},{\"end\":77561,\"start\":77553},{\"end\":77567,\"start\":77561},{\"end\":77574,\"start\":77567},{\"end\":77581,\"start\":77574},{\"end\":77820,\"start\":77811},{\"end\":77828,\"start\":77820},{\"end\":77835,\"start\":77828},{\"end\":78281,\"start\":78272},{\"end\":78288,\"start\":78281},{\"end\":78295,\"start\":78288},{\"end\":78302,\"start\":78295},{\"end\":78308,\"start\":78302},{\"end\":78315,\"start\":78308},{\"end\":78809,\"start\":78800},{\"end\":78816,\"start\":78809},{\"end\":78823,\"start\":78816},{\"end\":78830,\"start\":78823},{\"end\":78838,\"start\":78830},{\"end\":79312,\"start\":79303},{\"end\":79320,\"start\":79312},{\"end\":79327,\"start\":79320},{\"end\":79335,\"start\":79327},{\"end\":79592,\"start\":79583},{\"end\":79600,\"start\":79592},{\"end\":79985,\"start\":79977},{\"end\":79993,\"start\":79985},{\"end\":79999,\"start\":79993},{\"end\":80007,\"start\":79999},{\"end\":80013,\"start\":80007},{\"end\":80019,\"start\":80013}]", "bib_venue": "[{\"end\":51802,\"start\":51783},{\"end\":52158,\"start\":52077},{\"end\":52680,\"start\":52663},{\"end\":53135,\"start\":53071},{\"end\":53508,\"start\":53406},{\"end\":53895,\"start\":53831},{\"end\":54236,\"start\":54187},{\"end\":54555,\"start\":54477},{\"end\":54934,\"start\":54871},{\"end\":55193,\"start\":55139},{\"end\":55522,\"start\":55448},{\"end\":55953,\"start\":55913},{\"end\":56171,\"start\":56158},{\"end\":56363,\"start\":56346},{\"end\":56667,\"start\":56629},{\"end\":56985,\"start\":56908},{\"end\":57385,\"start\":57307},{\"end\":57697,\"start\":57634},{\"end\":58045,\"start\":57968},{\"end\":58446,\"start\":58375},{\"end\":58748,\"start\":58687},{\"end\":58942,\"start\":58922},{\"end\":59221,\"start\":59162},{\"end\":59570,\"start\":59526},{\"end\":59862,\"start\":59787},{\"end\":60252,\"start\":60171},{\"end\":60589,\"start\":60537},{\"end\":60941,\"start\":60860},{\"end\":61376,\"start\":61315},{\"end\":61711,\"start\":61630},{\"end\":62148,\"start\":62110},{\"end\":62412,\"start\":62342},{\"end\":62818,\"start\":62737},{\"end\":63247,\"start\":63166},{\"end\":63640,\"start\":63602},{\"end\":63973,\"start\":63902},{\"end\":64340,\"start\":64263},{\"end\":64665,\"start\":64628},{\"end\":64991,\"start\":64920},{\"end\":65329,\"start\":65279},{\"end\":65587,\"start\":65503},{\"end\":65969,\"start\":65898},{\"end\":66332,\"start\":66270},{\"end\":66633,\"start\":66582},{\"end\":67061,\"start\":66990},{\"end\":67450,\"start\":67383},{\"end\":67738,\"start\":67672},{\"end\":68013,\"start\":67951},{\"end\":68325,\"start\":68280},{\"end\":68493,\"start\":68429},{\"end\":68703,\"start\":68656},{\"end\":68929,\"start\":68851},{\"end\":69217,\"start\":69185},{\"end\":69529,\"start\":69489},{\"end\":69732,\"start\":69689},{\"end\":69996,\"start\":69947},{\"end\":70299,\"start\":70233},{\"end\":70541,\"start\":70498},{\"end\":70792,\"start\":70784},{\"end\":70981,\"start\":70924},{\"end\":71250,\"start\":71190},{\"end\":71582,\"start\":71557},{\"end\":71919,\"start\":71859},{\"end\":72327,\"start\":72278},{\"end\":72774,\"start\":72693},{\"end\":73232,\"start\":73183},{\"end\":73566,\"start\":73517},{\"end\":73882,\"start\":73844},{\"end\":74242,\"start\":74171},{\"end\":74611,\"start\":74563},{\"end\":74857,\"start\":74767},{\"end\":75225,\"start\":75175},{\"end\":75485,\"start\":75427},{\"end\":75742,\"start\":75695},{\"end\":76179,\"start\":76098},{\"end\":76637,\"start\":76566},{\"end\":76963,\"start\":76890},{\"end\":77319,\"start\":77281},{\"end\":77543,\"start\":77496},{\"end\":77916,\"start\":77835},{\"end\":78386,\"start\":78315},{\"end\":78919,\"start\":78838},{\"end\":79301,\"start\":79247},{\"end\":79671,\"start\":79600},{\"end\":80090,\"start\":80019},{\"end\":52226,\"start\":52160},{\"end\":53186,\"start\":53137},{\"end\":57049,\"start\":56987},{\"end\":57450,\"start\":57387},{\"end\":58109,\"start\":58047},{\"end\":58504,\"start\":58448},{\"end\":60320,\"start\":60254},{\"end\":61009,\"start\":60943},{\"end\":61779,\"start\":61713},{\"end\":62886,\"start\":62820},{\"end\":63315,\"start\":63249},{\"end\":64404,\"start\":64342},{\"end\":65049,\"start\":64993},{\"end\":66027,\"start\":65971},{\"end\":66646,\"start\":66635},{\"end\":67119,\"start\":67063},{\"end\":67504,\"start\":67452},{\"end\":72842,\"start\":72776},{\"end\":74300,\"start\":74244},{\"end\":76247,\"start\":76181},{\"end\":76695,\"start\":76639},{\"end\":77984,\"start\":77918},{\"end\":78444,\"start\":78388},{\"end\":78987,\"start\":78921},{\"end\":79729,\"start\":79673},{\"end\":80148,\"start\":80092}]"}}}, "year": 2023, "month": 12, "day": 17}