{"id": 67872077, "updated": "2022-02-26 07:33:04.328", "metadata": {"title": "F5-HD: Fast Flexible FPGA-based Framework for Refreshing Hyperdimensional Computing", "authors": "[{\"first\":\"Sahand\",\"last\":\"Salamat\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Behnam\",\"last\":\"Khaleghi\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Hyperdimensional (HD) computing is a novel computational paradigm that emulates the brain functionality in performing cognitive tasks. The underlying computation of HD involves a substantial number of element-wise operations (e.g., addition and multiplications) on ultra-wise hypervectors, in the granularities of as small as a single bit, which can be effectively parallelized and pipelined. In addition, though different HD applications might vary in terms of number of input features and output classes (labels), they generally follow the same computation flow. Such characteristics of HD computing inimitably matches with the intrinsic capabilities of FPGAs, making these devices a unique solution for accelerating these applications. In this paper, we propose F5-HD, a fast and flexible FPGA-based framework for refreshing the performance of HD computing. F5-HD eliminates the arduous task of handcrafted designing of hardware accelerators by automatically generating an FPGA implementation of HD accelerator leveraging a template of optimized processing elements, according to the applications specification and user's constraint. Our evaluations using different classification benchmarks revealed that F5-HD provides 6.9x and 7.8\u00d7 (11.9\u00d7 and 1.7\u00d7) higher energy efficiency improvement and faster training (inference) as compared to an optimized implementation of HD on AMD R9 390 GPU, respectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2915418849", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/fpga/SalamatIKR19", "doi": "10.1145/3289602.3293913"}}, "content": {"source": {"pdf_hash": "9161e9a39a42c3be4f04f7d44cc676e27ee38cc3", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3289602.3293913", "status": "BRONZE"}}, "grobid": {"id": "5a0193fbbce9682a14e43ce7fabc0cd0591dd2b9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9161e9a39a42c3be4f04f7d44cc676e27ee38cc3.txt", "contents": "\nF5-HD: Fast Flexible FPGA-based Framework for Refreshing Hyperdimensional Computing\n\n\nSahand Salamat sasalama@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nMohsen Imani moimani@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nBehnam Khaleghi bkhaleghi@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nTajana Rosing tajana@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nF5-HD: Fast Flexible FPGA-based Framework for Refreshing Hyperdimensional Computing\n10.1145/3289602.3293913\nHyperdimensional (HD) computing is a novel computational paradigm that emulates the brain functionality in performing cognitive tasks. The underlying computation of HD involves a substantial number of element-wise operations (e.g., addition and multiplications) on ultra-wise hypervectors, in the granularities of as small as a single bit, which can be effectively parallelized and pipelined. In addition, though different HD applications might vary in terms of number of input features and output classes (labels), they generally follow the same computation flow. Such characteristics of HD computing inimitably matches with the intrinsic capabilities of FPGAs, making these devices a unique solution for accelerating these applications.In this paper, we propose F5-HD, a fast and flexible FPGA-based framework for refreshing the performance of HD computing. F5-HD eliminates the arduous task of handcrafted designing of hardware accelerators by automatically generating an FPGA implementation of HD accelerator leveraging a template of optimized processing elements, according to the applications specification and user's constraint. Our evaluations using different classification benchmarks revealed that F5-HD provides 86.9\u00d7 and 7.8\u00d7 (11.9\u00d7 and 1.7\u00d7) higher energy efficiency improvement and faster training (inference) as compared to an optimized implementation of HD on AMD R9 390 GPU, respectively.CCS CONCEPTS\u2022 Hardware \u2192 Reconfigurable logic and FPGAs; Electronic design automation; \u2022 Computing methodologies \u2192 Machine learning approaches.\n\nINTRODUCTION\n\nHyperdimensional (HD) computing is a novel computational approach that builds upon imitating the brain functionality in performing cognitive tasks [1,2]. In fact, brain computes with patterns of neural activity, which can be realized by points in a hyperdimensional space, called hypervectors. By leveraging a non-complex and parallel set of operations on such ultra-wide vectors, HD affords promising capabilities in learning and classification applications including but not limited to language, speech, activity, and face recognition as well as classification of time-series signals [3][4][5][6][7][8][9]. In addition to its inclusive cognitive application space and comparatively simpler computation model than other learning paradigms [10,11], HD computing is inherently robust against failures as the information in a hypervector is uniformly distributed over all of its comprising dimensions [1]. Moreover, HD is able to yield the accuracy of state-of-the-art while learning from only a small portion of the original training data [12,13].\n\nIn a nutshell, HD computing is involved with constituting of and processing on hypervectors, wherein a hypervector comprises thousands of bits. For training, first, it generates a fixed set of orthogonal hypervectors each of which represents a specific feature level. Afterward, for a given input (as a preprocessed set/vector of features), it maps each feature of the input vector to the corresponding predetermined hypervector. Eventually, all the hypervectors are aggregated, which is basically performed by adding them up [3,14]. Since the spatial or temporal location of the features does matter, the aggregation also incorporates shift operation on the representing vectors to retain the indices of the input features. After all input data are mapped to a final encoded hypervector, all encoded hypervectors belonging to the same class (label) are summed up to form the final representative hypervector of the class. Inference in HD computing is analogous; albeit the encoded hypervector passes through an associative search (a.k.a similarity check) with the representative hypervectors to identify the associated class [1].\n\nThe encoding and classifying stages of HD computing require a substantial number of bit-level addition and multiplication operations, which can be effectively parallelized [13]. These operations can also be segregated (and hence, pipelined) in the granularity of dimension level. Though they may vary in the number of input features and output classes, all HD applications follow the same computation flow, albeit with a controllable degree of parallelism and pipeline. Such characteristics of HD computing inimitably matches with the intrinsic capabilities of FPGAs [15], making these devices a unique solution for accelerating these applications; however, implementing applications on FPGAs is a time consuming process [10,16].\n\nIn this paper, we propose F5-HD, an automated FPGA-based framework for accelerating HD computing that abstracts away the implementation complexities and long design cycles associated with hardware design from the user. F5-HD generates a synthesizable Verilog implementation of HD accelerator while taking the highlevel user and target FPGA parameters into account. Essentially, F5-HD customizes upon a hand-optimized, fully-pipelined template processing element that can be parallelized according to the userspecified constraints (viz., accuracy and power). F5-HD supports both training and inference as well as model refinement through online, simultaneous, training and inference, so the model can be calibrated without interrupting the normal operation of the system. Specifically, this paper makes the following contributions:\n\n\u2022 Proposes F5-HD, a template-based framework that generates FPGA-based synthesizable architectures for accelerating HD computing. \u2022 Proposes a novel hardware-friendly encoding approach that reduces the required Block RAM accesses, hence, enhances resource utilization \u2022 Provides the flexibility of customized accuracy by supporting different data-types (viz., fixed-point, binary, and powerof-two), and of customized power consumption bound by trading the parallelism. \u2022 Enables simultaneous training and inference to refine the model without interrupting the system functionality. Our evaluations using different classification benchmarks revealed that, in high-accuracy mode, F5-HD can provide 86.9\u00d7 and 7.8\u00d7 (11.9\u00d7 and 1.7\u00d7) higher energy efficiency improvement and faster training (inference) as compared to an optimized implementation of HD on AMD R9 390 GPU, respectively. In the fastest mode in which each dimension is represented by a single bit (i.e., binary), F5-HD achieves 4.3\u00d7 higher throughput and 2.1\u00d7 throughput/Watt as compared to the baseline F5-HD using fixed-point values, while providing in average 16.5% lower classification accuracy. In addition, we observe that F5-HD framework can ensure the power consumption to be within 9.0% of the user-defined constraint, on average.\n\n\nBACKGROUND AND RELATED WORK\n\nIn this section, we first articulate the operations behind HD computing, including encoding, training, inference, and retraining. Afterward, we review the previous work regarding the utilization and implementation of the HD computing.\n\n\nHyperdimensional Computing\n\nHD computing builds on the fact that the cognitive tasks of the human brain can be explained by mathematical operations on ultrawide hypervectors [1]. In other words, brain computes with patterns of neural activity, which can be better represented by hypervectors rather than scalar numbers. A hypervector comprises D hv , e.g., 10,000 bits, independent components (dimensions) whereby the enclosed information is distributed uniformly among all D hv dimensions. This makes hypervectors robust to failure as the system remains functional under a certain number of component failings, and as degradation of information does not depend on the position of the failing components [3,14,17].\n\nEncoding: As demonstrated in Figure 1, training an HD model involves a three-step procedure as follows. First, it initializes base hypervectors, each of which corresponds to a specific input feature level. Indeed, input of the HD algorithm is a feature vector \u00ec V iv with D iv dimensions (elements) wherein each dimension represents a feature value F that has iv levels:\n\u00ec V iv = v 0 , v 1 , \u00b7 \u00b7 \u00b7 , v D iv |v i | \u2208 (F 0 , F 1 , \u00b7 \u00b7 \u00b7 F iv )(1)\nThough it is application-dependent, typical values for D iv and iv might be, respectively, 100s and four-eight for which iv can be represented by two-three bits. Each of D iv features in the feature vector needs to be mapped to a base hypervector with D hv dimensions for subsequent processing. Therefore, to represent all possible iv values of features, iv different hypervectors with D hv dimensions, namely base hypervectors, are needed. The base hypervectors are generated according to the attribute of the feature vector. In the cases that feature levels are independent and irrelevant, base hypervectors can be selected randomly, hence orthogonal. In such cases, the expected Hamming distance between two (out of iv ) base hypervectors is \u223c Dhv /2. However, for the cases that each feature level is a meaningful quantity, e.g., a continuous signal quantized to iv levels, the distance between the hypervectors of two feature levels should correspond to their actual difference. For these cases, the base hypervector associated with the lowest feature level is generated randomly. Afterward, a random half ( Dhv /2) of its bits are flipped to produce an orthogonal base hypervector representing the other side of the horizon, i.e., the highest level of a feature. The remaining base hypervectors are generated by flipping Dhv /2 iv \u22121 of each consecutive hypervector pair, starting from the initial base hypervector.\n\nAfter specifying the base hypervectors, each element v i of a given input feature vector is mapped to its associated base hypervector hv v i for subsequent processing. Nonetheless, as in most applications the spatial and/or temporal position of an input feature often do matter, i.e., whenever a sequence of the input features should be traced such as image and speech inputs, the encoding procedure takes the locality into account by introducing permutation operation P (i) (which denotes i-bits cyclic left shift) on the input features before aggregation. Due to the large dimension and randomness of the base hypervectors, P (i) keeps a hypervector and its resultant shift orthogonal. Eventually, the mapped hypervectors are aggregated according to Equation 2 to build the query hypervector:\nhv( \u00ec V iv ) = \u00ec hv v 0 + ( \u00ec hv v 1 1) + \u00b7 \u00b7 \u00b7 + ( \u00ec hv v D iv D iv ) (2)\nWhich can be reformulated as:\n\u00ec H = \u00ec hv( \u00ec V iv ) = D iv i=0 P (i) ( \u00ec hv v i )(3)\nTraining: After mapping each training input \u00ec V iv to hypervector \u00ec H as above, all hypervectors belonging to the same class (label) are simply summed to form the final representative hypervectors. Thus, assuming \u00ec H l = h 0 , h 1 , \u00b7 \u00b7 \u00b7 , h D hv l denotes a generated class hypervector for an input data with label l, the final (representative) class hypervectors are obtained as Equation 4, in which each dimension c k is obtained through dimension-wise addition of all h l k s, and J is the number of input data with label l.\n\u00ec C l = c 0 , c 1 , \u00b7 \u00b7 \u00b7 , c D hv = J j=0 H l j(4)\nAll dimensions of a class hypervector ( \u00ec C) have the same bitwidth which can have various representation, e.g., binary (hence one bit), power-of-two (2 n ), fixed-point (integer), etc. This makes a trade-off between accuracy, performance, and hardware complexity. The base of hypervectors are converted through thresholding. For instance, for J hypervectors \u00ec H l j constituting class \u00ec C l , the binarized class can be obtained as follows.\n\u00ec C l = c 0 , c 1 , \u00b7 \u00b7 \u00b7 , c D hv , c k = 0 c k < J 2 1 otherwise (5)\nInference: The first steps of inference in HD computing is similar to training; an input feature vector is encoded to D hv \u2212dimension query hypervector \u00ec H following Equation 3. This is followed by a similarity check between the query hypervector \u00ec H and all representative class hypervectors, \u00ec C l . The similarity in the fixed-point and power-of-two number representations is defined as calculating the cosine similarity, which is obtained by multiplying each dimension in the query vector to the corresponding dimension of the class hypervectors, and adding up the partial products:\nsimilarity( \u00ec H, \u00ec C l ) = D hv j=0 h k \u00b7 c k(6)\nThe class with the highest similarity with the query hypervector indicates the classification result. The number of classes is applicationdependent and determined by the user. This can be as simple as two classes, denoting face vs. non-face in a face-detection algorithm. Similarity checking in binarized HD model (i.e., 1-bit dimensions) simplifies to the Hamming distance between the query and class vectors, which can be carried out by a bitwise XNOR, followed by a reduction (population counter 1 ) operation. Retraining: Retraining might be used to enhance the model accuracy by calibrating it either via new training data or by multiple iterations on the same training data. Retraining is basically done by removing the mispredicted query hypervectors from the mispredicted class and adding it to the right class. Thus, for a new input feature vector \u00ec V in with query hypervector \u00ec H belonging actually to class with hypervector \u00ec C l , if the current model predicts the class C l where C l C l , the model updates itself as follows:\n\u00ec C l = \u00ec C l + \u00ec H \u00ec C l = \u00ec C l \u2212 \u00ec H(7)\nThis, indeed, reduces the similarity between \u00ec H and mispredicted class C l , and adds \u00ec H to the correct class C l to increase their similarity and the model will be able to correctly classify such query hypervectors.\n\n\nRelated Studies\n\nHD computing is gaining traction as an alternative solution to perform cognitive tasks in a light-weight fashion that uses significantly simpler operations compared to conventional machine learning techniques that deal with complex learning procedures with substantial number of costly operations. So far, successful application of HD computing in varied domains has been demonstrated. language identification [18], DNA sequencing [19], physical activity prediction [5,20], speech recognition [6,21], and gesture recognition [12,22], clustering [23] are just a few examples.\n\nOn par with studies investigating the HD applications, several studies have attempted to propose hardware and algorithmic solutions to enhance the efficacy of HD computing. The study in [17] proposes logical operations to generate the hypervector corresponding to each feature on the fly, in order to reduce the costly BRAM accesses. They also propose approximate majority gate to compose the binary class hypervectors without requiring to hold the summation on hypervector components in a multi-bit format in the course of training. This is, however, limited to low-accuracy binarized HD computing wherein each dimension of the query and class hypervectors is one bit. The authors of [13] propose hierarchical HD computing solution that consists of a main stage with multiple classifiers each can trade between efficiency and accuracy. There is also a decider stage that learns and selects the appropriate encoder within the main stage based on a so-called difficulty metric of the input data. The work in [21] clusters class hypervectors dimensions to reduce the number of multiplications. Additionally, by assuming the encoded input hypervector is stored in memory, they implemented the associative search of clustered HD on FPGA.\n\nOther works leverage advances of emerging technologies in HD computing [24][25][26]. In [24], the authors leverage CNT-FET and Resistive RAM to fabricate an end-to-end HD computing solution. They exploit the variations in RRAM resistance and CNT-FET drives current to project the input features to query hypervectors as well as propose approximate accumulation circuit using gradual RRAM reset operation. The work in [25] demonstrates HD computing with 3D vertical RRAM in-memory kernels capable of performing multiplication, addition, and permutation by analog operations on RRAM cells.\n\nTo the best of our knowledge, F5-HD is the first automated FPGAbased framework that implements HD computing with varied model precision, capable of meeting user constraints on different FPGA platforms.\n\n\nF5-HD FRAMEWORK OVERVIEW\n\nF5-HD aims to abstract away the complexities behind employing FPGAs for accelerating AI applications [27]. F5-HD is an automated framework that generates synthesizable FPGA-based HD implementation in Verilog, considering the user-specified criteria, e.g., power budget, performance-accuracy trade-off, and FPGA model (available resources). F5-HD combines the advantages of hand-optimized HDL design with the bit-level yet flexible manageability of FPGA resources, which is in concordance with bitwise operations associated with HD computing, to accelerate these applications. (1) Model Specification: The framework starts with specifying the application specifications, viz., the number of classes, features (i.e., input vector dimensions D iv , as well as the number of features different levels, iv ) and the number of training data. The user also determines the target FPGA model, hence F5-HD can get the number of available resources from a predefined library. F5-HD currently supports Xilinx 7-series FPGAs, including Virtex-7,  Spartan-7, and Kintex-7 families. This can be readily extended to other FPGA families. In addition, the user can dictate constraints on the power as well as performance-accuracy trading, which will be explained in the following subsections.\n\n\nF5-HD Workflow\n\n(2) Design Analyzer: Thereafter, F5-HD's design analyzer determines the number of resources according to the user's specification. F5-HD exploits a parameterized template architecture, mainly composed of an encoder; an associative search unit, including Processing Units and Processing Elements; as well as an HD model module that stores and updates the class hypervectors. The hardware architecture of F5-HD will be detailed in Section 4. The design analyzer determines the number of Processing Units (PUs), Processing Elements (PEs) as well as the type and number of dimension-wise functional units within each PE, according to the desired accuracy level and available resources. All the function units, e.g., encoder and PUs, utilize a specific set of building blocks with foreknown resource utilization. Thus, F5-HD design analyzer can readily figure out the parameters of the template architecture, e.g., maximum parallelization level of the encoder (see Section 4.1) and number of PEs per PU, based on their required resources (LUT, BRAM, and DSP) and the available resources.\n\nIn the case a power budget is defined by the user, the design analyzer tries to find out the maximum number of PEs that can be generated, without violating the constraints. For this regard, F5-HD estimates the power of resources, e.g., LUTs, flip-flops, DSPs, BRAMs, etc. using Xilinx Power Estimator (XPE) [28]. This requires calculating the expected activity of the resources, which is straightforward owing to the foreknown homogeneous structure of the generated architectures and the expected probability of the hypervectors at the level of the dimension. Another constraint is performance-accuracy trade-off wherein the user chooses between the highest performance with relatively lower accuracy, mediocre, and low performance with the highest accuracy. The available modes are currently fixed-point (8-bits integer representation), power-of-two in which hypervector dimensions are four-bits values that represent the exponent, and binary (i.e., each dimension is represented by one bit). It is noteworthy that the power and accuracy constraints can be applied concurrently, which provides the user with the flexibility to adapt F5-HD based on their application criteria. For instance, for real-time low-power applications, the user might specify their power budget with the binary mode of operation. The output of design analyzer is basically the number of PUs and PEs (per PU), the number of multipliers (in the case of fixed-point model) per PE, and the parallelization level of the encoder, i.e., the number of hypervector dimensions it can produce at each cycle.\n\n(3) Model Generator: After the design analyzer specified the parameters of the template architecture, F5-HD's model generator, automatically generates the Verilog implementation of F5-HD using hand-optimized template blocks. This includes instantiating the PUs, PEs, the Block RAMs, and off-chip memory interface. The model generator also initializes the BRAMs with the base hypervectors. For this end, F5-HD exploits a fixed, predetermined hypervector as the seed vector, and generates the remaining iv \u2212 1 hypervectors according to the procedure explained in Section 2.1.\n\nIn the cases the user already has a trained model (i.e., base and class hypervectors), F5-HD allows direct initializing of these hypervectors.\n\n(4) Scheduler: The next step generates the controller, which statically schedules F5-HD operations. The main scheduling tasks include loading the training or inference data from off-chip memory into local BRAMs, switching between the training, inference, and/or retraining modes. It also generates a controller to allocating and deallocating PUs for retraining, and essentially controlling the enabler of different processing units in the granularity of clock cycle. Eventually, the logic and controller are merged to realize the concrete accelerator architecture.\n\n\nAccuracy-Performance Trade-off\n\nThe majority of existing HD computing methods use binarized class hypervectors to substitute the costly Cosine similarity operation in inference phase with the simpler Hamming distance operation. Although binary representation increases the throughput, in the majority of classification problems, the accuracy of the binarized HD model is not comparable to that of the HD using fixedpoint dimensions [13]. In addition to the fixed-point and binary HD models, we provide power-of-two representation in the class hypervectors which replaces the costly multiplication operations with shift operations in the hardware level. Though power-of-two representation covers discrete values, it supports a larger range of numbers which helps to compensate for the accuracy drop. Table 1 compares the accuracy and execution time of HD models for four different datasets on CPU. Fixed-point model, on average, attains 5.7% and 20.5% higher accuracy compared to, respectively, powerof-two and binary models. The binary model surpasses in terms of the throughput, wherein it yields 6.5\u00d7 and 2.2\u00d7 performance improvement over the fixed-point and power-of-two models.\n\n\nTraining Modes\n\nSimilar to the training of Deep Neural Networks (DNNs), training of HD model can be enhanced by iterating over the input data, as described in Section 2.1. Note that, as in the case of DNNs, to avoid overfitting, a learned model does not necessarily predict the correct class for all data of the same training dataset, however, the accuracy can be improved by multiple iterations (equivalent to multiple epochs in the context of deep learning). The first epoch of F5-HD generates all query hypervectors (one per each input data) and aggregates the hypervectors with the same label l as the class hypervector \u00ec C l . We denote this single-epoch learning as model initialization. During the subsequent optional epochs (referred to as retraining), which either can be specified by the user or F5-HD itself continues until the accuracy improvement diminishes, under the management of the scheduler, F5-HD enhances the model by discarding the attributes of the mispredicted query hypervector \u00ec H from the mispredicted class hypervector \u00ec C H , and adding it to the correct class hypervector \u00ec C H . Retraining can be carried out immediately after model initialization, or enabled later by halting the inference phase. The principal difference between the model initialization and retraining is the latter requires prediction (i.e., inference) as well while the former simply performs aggregation. This is supported by F5-HD architecture, which is further described in Section 4.\n\nDepending on the generality of the training data and the HD model, in certain cases, the accuracy of the classifier for real-world data might drop. To resolve this issue, F5-HD provides an online retraining solution which can be enabled during the runtime by user. During the online retraining, F5-HD updates the class hypervectors based on a new set of training data in real-time. Thus, F5-HD is capable of conducting model initialization, retraining, inference, and simultaneous retraining-inference (online retraining). In the inference mode, the system works normally and all the resources are assigned to calculate the similarity metric. In the online hybrid retraining mode, the system executes both inference and retraining and allocates a portion of the resources for each task. In this mode, the part of the FPGA that executes the inference task always uses the updated model during the online retraining. Therefore, in each retraining iteration, the model is updated and the inference employs the recently updated class hypervectors for prediction. Upon finishing the online retraining, all FPGA resources will be reallocated back for inference purpose.\n\n\nFlow of Data\n\nInputs of F5-HD are vectors of extracted features, namely feature maps, which are stored in the off-chip memory. The scheduler partially loads the feature maps to the input buffer memory, distributed in FPGA local memory (Block RAMs). The encoding module generates the encoded query hypervectors of the input vector and stores them in the encoding buffer. The generated query hypervectors are then pipelined in a segregated (dimensional-wise) manner, fed to the associative search module to perform parallel similarity check with all class hypervectors, yet in a dimensional-wise manner. This requires to store the partial sums of the dimensions products. The encoding and associative search work in a synchronous manner to avoid logic starvation and maximize the physical resource utilization. Thus, in F5-HD, the encoding module outputs the same number of query hypervector dimensions that the associative search processes per cycle. Since the classification of an input vector takes multiple cycles and utilizes all the FPGA resources, the parallelization is in per-input level. That is, classification operations for a single input are pipelined and parallelized among all FPGA resources, and the subsequent input vector is loaded after the process of the current input accomplishes. Increasing F5-HD's throughput necessitates increasing the degree of parallelism in the associative search, which, in turn, demands reading higher encoded dimension per cycle. Therefore, owing to the high supported degree of parallelism in HD computing, the only performance barriers of F5-HD are the available resources and power budget.\n\n\nF5-HD ARCHITECTURE\n\nIn this section, we articulate the contributions of F5-HD in more details. We begin with elaborating the proposed encoding scheme that reduces the number of BRAM accesses. Afterwards, we illustrate the architecture overview and detail the functionality and structure of the building blocks in the course of training and inference. We also formulate the required resources by which the design analyzer specifies the (parametric) number of resources for the model generator.\n\n\nProposed Encoding Scheme\n\nBoth training and inference processes in HD computing need to encode the input feature hypervector, \u00ec V in , to the query hypervector \u00ec H , using basic permutation and addition on the base hypervectors. As previously shown by Equation 3, each element v i of the input hypervector, based on its value |v i | \u2208 (F 0 , F 1 , \u00b7 \u00b7 \u00b7 F iv ), selects the corresponding base hypervector \u00ec hv vi (out of iv possible base hypervectors), rotated left by i bits, to make up the query \u00ec H . Figure  3 \nb D iv (b 0 ), v 2 selects from b D iv \u22121 (b D iv ), etc.\nRecall that the dimensions of hypervectors are 1-bit wide (denoted by b i s in the figure) that aggregate in a dimensionwise scheme and form d i s, which can be in various widths and representations, e.g., fixed-point, binary, and power-of-two.\n\nThe na\u00efve encoding scheme abstracted in Figure 3 is, however, both computationally and communicationally intractable: at each cycle it requires iv \u00d7 D hv bits (multiples of 10K) of the base hypervectors to be read from the BRAMs, and D hv population counters (PopCounters), each with input bitwidth of D iv . To resolve this, as the dimensions of the query hypervector \u00ec H can be calculated independently, we segregate the output query vector \u00ec H into the segments of S dimensions whereby at each clock cycle one segment is processed. Thus, processing the entire \u00ec H takes Dhv /S cycles. This is conceptualized in Figure 3(b), which shows the physical locations of the hypervectors bits required to build up the first S dimensions of \u00ec H . Accordingly, iv \u00d7 (S + D iv ) different bits are needed to be read to create the query \u00ec H . Notice that this approach retains the alignments of the bits; for every S + D iv consecutive bits (per base hypervector) read from the BRAM(s) at each cycle, bits 0 to D iv are conveyed to 0 th PopCounter to form d 0 , bits 1 to D iv + 1 form the d 1 via the 1 st PopCounter, and so on. Therefore, no logic or routing overhead is associated to align the read data.\n\nBeside segmented processing, we further reduce the number of BRAM accesses by proposing a novel encoding scheme. The proposed encoding, first, permutes the bits of the base hypervectors locally, i.e., intra-segment, rather than the entire hypervector. After S permutations, e.g., after the first S features (v i s) in the input hypervector, the segments accomplish an entire permutation; hence the base hypervector for the 0 th and S + 1 th features essentially become the same. This removes the information associated with local and/or temporal locality of the input features. In such case, we perform inter-segment permutation in which the segments are In this scenario, the first S features (v i s) need S bits of the first segment, the second S input features require S bits of the right segment (which will be shifted to left by one segment), and so on. Thereby, the proposed encoding needs iv \u00d7(S \u00d7 Div /S) = iv \u00d7 D iv bits (S bits of all iv base hypervectors per every Div /S input features) to produce an output segment. This needs S D iv -width Pop-Counter. Figure 3(c) conceptualizes the proposed encoding scheme.\nD D D D D D D D D v v D D D D D D D vD vD D D D D D D D D D D D D D (d) v D v F F F F v F F v F F F F v F F v F F F F F F F F F F F F F F F F F F F F F F F F D D\nThe hand-crafted hardware realization of the proposed Pop-Counter, which contributes to significant portion of the encoder and overall area footprint, is demonstrated by Figure 3(d). The main building block of the implemented PopCounter is Pop36 that produces 6-bit output for a given 36-bit input. It is made up of bunches of three LUT6 that share six inputs and output the 3-bit resultants, which are summed up together in the subsequent stage according to their bit order (position). We instantiated FPGA primitive resources, e.g., LUT6 and FDSE to build up the pipelined PopCounter, which is \u223c 20% area efficient than simple HDL description. The impact of PopCounter intensifies further in binary HD models wherein the associative search module is relatively small.\n\n\nF5-HD Architecture\n\nThe architecture overview of F5-HD is illustrated in Figure 4, which incorporates the required modules for training, inference and online retraining of the HD computing. The main template architecture of F5-HD includes two levels of hierarchy: a cluster of Processing Units (PUs), each comprises specific number of Processing Elements (PEs). The assignment of PUs and PEs are selected in a way that maximizes the data reusability.\n\nProcessing Units (PUs): F5-HD contains 2 \u00d7 |C| PUs where |C| is the number of classes (labels). In the course of inference, all C PUs perform similarity checking. Every cycle, each PU receives S /2 of the query hypervector's dimensions (recall that S is the segment length generated by encoder at each clock cycle, as discussed in Section 4.1). Thus, together, a pair of PUs process all S dimensions of the segment, and hence, 2 \u00d7 |C| PUs are able to check similarity between all |C| classes in parallel. Every PU k also contains a local buffer to prefetch (a portion of) the associated class hypervector C k in advance to suppress the BRAM's read delay. Additionally, PU includes a pipelined accumulator to sum up and store the results of PEs, to be aggregated with the results of the next S /2 dimensions.\n\nProcessing Elements (PEs): Each PE contains a predetermined number of multipliers and adders (based on the FPGA size, normally eight fixed-point multipliers). However, the number of PEs in each PU which together with the PopCounters of encoder determine the level of parallelism (value of S), is specified according to the available FPGA resources. The available resources may be restricted by the power budget, as well. PEs generally perform the similarity check through calculating the dot-product of the query and class hypervectors, though it requires different type of operations for different model precision (different representations of dimensions). Typically, PEs consist of fixed-point multipliers, which we map them to FPGA DSPs. Utilizing power-of-two HD model replaces the multiplications with shift operations in which each dimension of the query \u00ec H is shifted by the value specified by the corresponding element of the class hypervector. Using binary HD model further simplifies this to element-wise XNOR operations, followed by reduction or population count, in F5-HD XNOR and population count operation is combined and implemented in XS LUTs followed by a layer of 6-input population count logic (P6 LUTs). Therefore, the advantage of a hand-crafted PopCounter gets further noticed in the binarized HD models. To generate HD architectures of different accuracy, F5-HD produces PEs with the specific structure, the template architecture is retained.\n\nIn the following, we explain how F5-HD architecture splits the processes during the model initialization, inference, and retraining procedures.\n\nModel Initialization: Model initialization starts with randomly initializing of the class hypervectors as well as generating the orthogonal, base hypervectors. Since model initialization is carried out only once in the entire course of the HD computing, we try to simplify this stage and do not allocate specialized resources. Therefore, we load both the base hypervectors and initial (random) class hypervectors during initial programming of the FPGA. Thereafter, all training input data is encoded and then added to the initial class hypervector. We use the same encoding module used for generating the query hypervectors, which, at each cycle, generates S dimensions of the encoded input vector and adds it back to the corresponding class hypervector using the S-wide adder incorporated in the model module (see Figure 4).\n\nInference: Figure 4 demonstrates the structure of the inference block in F5-HD architecture. The encoded query hypervector \u00ec H is broadcast to all PUs, each of which shares S /2 corresponding dimensions of its prefetched associated class hypervector between its PEs. PUs accumulate the sum-of-the-products to be aggregated with the subsequent segments' results. After processing the entire query hypervector accomplished, i.e., after Dhv /S cycles, the final similarity resultant of each class is obtained by adding the accumulated values of each PU pair. Eventually, the comparator outputs the class index with the greatest similarity metric.\n\nRetraining: Remember from Section 2.1 that during the retraining stage, the HD model performs inference on the same input data and, in the case of misprediction, updates the necessary classes, i.e., the correct and mispredicted classes. In F5-HD architecture, it is performed by passing the mispredicted query hypervector to the HD model module, which adds (subtracts) the query to (from) the correct (mispredicted) class. The correct class index is specified by the label of input data. In summary, retraining involves with inference, followed by a potential model update.\n\nOnline Retraining/Inference: In this operating mode, the encoder generates S /2 dimensions for the inference, and S /2 for the retraining data. Using the upper pairs of PUs (see Figure 4), inference executes by 1 /2 of its typical throughput and takes 2 \u00d7 Dhv /S per input. The other half of PUs perform retraining, which, as already discussed, includes an inference followed by a potential model update. In the case of a misprediction which demands a model update, the inference should be halted to update the required classes. To avoid this, we have dedicated two additional hypervectors to write the updated classes (hypervectors). Upon a misprediction, the query hypervector will be subtracted from the mispredicted class, which is already being read by the inference module segment by segment, so no additional read overhead will be imposed. Thereafter, the hypervector will be added to the correct class. After updating each of the correct and mispredicted hypervectors, the address translator modifies the physical address of the two classes to point the right hypervector. Note that till the mispredicted classes are updated, the HD model works with the previous classes.\n\nResource Constraints: As the number of PUs are fixed, the number and size of PEs (i.e., number of multipliers per PE) per each PU affect the level of parallelism in HD computing. This, however, is also restricted by the number and bandwidth of on-chip RAMs as well as the dictated power budget. The following equations summarize the constraint of different resources F5-HD assumes in generating F5-HD architecture. \nencodin\u0434 A PopCounter \u00d7 S + Similair ty checker 2 \u00d7 |C| \u00d7 N P E \u00d7 A P E < LUT max(8)|C| \u00d7 S \u00d7 bitwidth + encodin\u0434 D iv \u00d7 iv 36 < BRAM max(10)\nIn these equations, A X denotes the area of module X in terms of number of LUTs, N P E is the number of PEs in each PU, DSP P E is the number of DSPs per PE (in the case of fixed-point models). We also map the adder of the model updater into DSP blocks, as evident from Equation 9. Notice that, in the proposed architecture, the computation is limited by BRAM accesses (rather than BRAM memory). Thus, we have assigned the constraint on BRAM bandwidth. It is also noteworthy that our experiments revealed the design is barely routable for LUT utilization rates above \u223c 90%. Hence, LUT max is set to 90% of the device LUTs.\n\n\nEXPERIMENTAL RESULTS\n\nF5-HD is a flexible framework for efficient implementation of different HD computing applications in FPGA hardware, respecting the application specifications and user's requirements. The entire F5-HD software support including user interface and code generation has been implemented in C++ on CPU. The software customizes template blocks to generate an optimized hardware for each application, based on the user's optimization, accuracy, and power preferences. The output of F5-HD framework is an FPGA-mapped implementation of a given HD application in Verilog HDL. We verify the timing and the functionality of the F5-HD by synthesizing it using Xilinx Vivado Design Suite [29]. The synthesized code has been implemented on Kintex-7 FPGA KC705 Evaluation Kit. We used Vivado XPower tool to estimate the device power.\n\nWe compare the performance and energy efficiency of F5-HD accelerator running on FPGA with AMD R9 390 GPU and Intel i7 7600 CPU with 16GB memory. For GPU, the HD code is implemented using OpenCL and is optimized for performance. We used Hioki 3334 and AMD CodeXL [30] for the power measurement of CPU and GPU, respectively. We implement F5-HD on three FPGA platforms including Virtex-7 (XC7VX485T), Kintex-7 (XC7k325T), and Spartan-7 (XC7S100) to evaluate the efficacy of F5-HD on various platforms with different available resources, power characteristics and power budget. We evaluate the efficiency of F5-HD on four practical workloads including Speech Recognition (ISO-LET) [31]: the goal is to recognize voice audio of the 26 letters of the English alphabet, Activity Recognition (UCIHAR) [32]: the objective is to recognize human activity based on 3-axial linear acceleration and 3-axial angular velocity, Physical Activity Monitoring (PAMAP) [33]: the goal is to recognize 12 different human activities such as lying, walking, etc., and Face Detection: the goal is to detect faces among Caltech 10,000 web faces dataset [34] from negative training images, i.e., non-face images which are selected from CIFAR-100 and Pascal VOS 2012 datasets [35].\n\n\nEncoding\n\nEncoding module is used in both training and inference. This encoder works in a pipeline stage with the initial training and associative search (similarity checking) modules. Thus, the more generated   dimensions by the encoding module, the more throughput F5-HD can achieve. To evaluate the effectiveness of our proposed encoding algorithm, we compare the hardware implementation of F5-HD encoding with a baseline HD computing encoding [13]. Table 2 compares the number of generated dimensions per cycle in F5-HD and the baseline encoding modules. In the baseline segmented encoding, to generate S dimensions of the encoded hypervector, we showed that HD architecture needs to read S + D iv dimensions of each base hypervector, where S and D iv are the segment length and length of the input hypervector, respectively. In contrast, as we explained in Section 4.1, F5-HD encoding module is implemented using a hardware-friendly permutation as well as LUT-based XNORand PopCount modules that reduces the resource usage. Our evaluation on data points with 64 features shows that F5-HD encoder can provide 1.5\u00d7 higher throughput as compared to the baseline segmented encoder. This throughput improvement increases to 1.9\u00d7 for data points with 512 features. This is because the delay of the adder (population counter) dominates as the number of features (hence, the size of the population counter) increases.\n\n\nTraining\n\nInitial Model Training: HD generates the initial model by a one-time passing through the training dataset. Regardless of the exploited models (viz., binary, power-of-two or fixed-point), in F5-HD we train the HD model using fixed-point operations and eventually we quantize the class hypervectors based on the defined model precision. Figure 5(a) shows the energy consumption and execution time of HD running on Intel i7 CPU, AMD R9 390 GPU, and Kintex-7 FPGA platforms during the initial training. The initial training consists of the encoding module which maps data points into highdimensional space and hypervectors aggregation which generates a hypervector representing each class. In conventional computing systems, e.g. CPU and GPU, the majority of training time is devoted to the encoding module, since these architectures have not been customized to process binary vectors in 10K dimensions. In contrast, F5-HD can implement the encoding module effectively using FPGA primitives. Our evaluation shows that F5-HD provides, on average, 86.9\u00d7 and 7.8\u00d7 (548.3\u00d7 and 148.2\u00d7) higher energy efficiency and faster training as compared to GPU (CPU) platform, respectively.\n\nRetraining: Similarity checking (a.k.a associative search) is the main contributor to HD energy consumption and execution time during both retraining and inference. In retraining, associative search checks the similarity between a fixed-point query hypervector with all stored class hypervectors using cosine metric. Since the HD encoding is expensive on conventional computing units, in CPU and GPU implementations, the retraining processes on the encoded training data which are already stored in memory. In contrast, due to the efficient F5-HD encoding functionality and in order to reduce the off-chip memory access, F5-HD encodes the training data on every iteration. Figure 5(b) compares the HD computing retraining efficiency on three CPU, GPU, and FPGA platforms. The results are reported for F5-HD retraining on a single epoch. Our evaluation shows that F5-HD provides 1.6\u00d7 and 10.1\u00d7 faster computation as compared to GPU and CPU platforms, respectively. Although the GPU performance is comparable to F5-HD, F5-HD provides 7.6\u00d7 higher energy efficiency due to its lower power consumption. Figure 6 compares the energy consumption and execution time of HD inference running on different platforms. All results are reported for the case of using the fixed-point model. The inference includes the encoding and associative search modules. The encoding module maps a test data into high-dimensional space, while the associative search module checks the similarity of the encoded data to pre-stored class hypervectors. The results show that the efficiency of applications changes depending on the number of features and the number of classes. For applications with a large feature size, F5-HD requires a costly encoding module, while applications with a large number of classes, e.g., ISOLET, devote the majority of the energy/execution time to perform the associative search. Our evaluation shows that F5-HD achieves 11.9\u00d7 and 1.7\u00d7 (616.8\u00d7 and 259.9\u00d7) higher energy efficiency and faster inference as compared to GPU (CPU) platform respectively. F5-HD can have different design choices for inference. Using fixed-point module F5-HD provides the maximum classification accuracy but relatively slower computation. Using binary and powerof-two model, the encoding dominates F5-HD energy/execution time, while for the fixed-point model the majority of resources are devoted to the associative search. F5-HD removes the multiplications involved in cosine similarity using power-of-two model, resulting in higher computation efficiency. Finally, the binary model is the most efficient F5-HD model, where the similarity check can be performed by using Hamming distance. Figure 7 shows the F5-HD inference efficiency using power-of-two and binary models. All results are normalized to the throughput and throughput/Watt of F5-HD with fixed-point model. For applications with low feature size, e.g., PAMAP, the encoding module maps a large number of data points into high-dimensional space. This makes the associative search a dominant part of inference computation when using fixed-point model. On the other hand, in face detection with a low number of classes and high feature size, the encoding dominates the F5-HD resource and efficiency. Our evaluation shows that F5-HD using binary and power-of-two models can achieve on average 4.3\u00d7 and 3.1\u00d7 higher throughput than F5-HD using fixed-point model. In addition, the binary and power-of-two models provide  2.1\u00d7 and 1.5\u00d7 higher throughput/Watt as compared to F5-HD using fixed-point model. Table 3 lists the average Kintex FPGA resource utilization implementing F5-HD using fixed-point, power-of-two, and binary models. The results are reported for F5-HD supporting both training and inference. Our evaluation shows that the fixed-point model utilizes the majority of the FPGA DSPs in order to perform the similarity check of the inference/retraining. In contrast, with binary and power-of-two models have much lower DSP utilization, as the majority of their inference computation includes bitwise operations that can be efficiently performed using LUTs and the PopCounter. In addition, F5-HD with the binary model has the lowest BRAM utilization as it can store the trained HD model using significantly lower memory size. Table 3 also provides the average power dissipation of the Kintex FPGA. The results indicate that in the fixed-point model, the number of DSPs limits the FPGA throughput, thus F5-HD consumes lower power consumption due to its overall low LUT utilization. In contrast, F5-HD using binary model highly utilizes the available LUTs on the FPGA resulting in high throughput and higher power consumption.\n\n\nInference\n\n\nResource/Power Utilization\n\n\nF5-HD on Different FPGA Platforms\n\nTo demonstrate the generality of F5-HD and further investigate its efficiency, we implement it on three different FPGA platforms, mentioned earlier in this section. Figure 8(a) compares the average throughout of F5-HD running different HD applications on these three platforms. Our evaluation shows that Virtex implementing fixed-point model provides 12.0\u00d7 and 2.5\u00d7 higher throughput as compared to Spartan and Kintex platforms. The efficiency of Virtex comes from its large amount of available DSPs (2,800 DSPs with 485K LUTs), which can be used to accelerate associative search. However, F5-HD using power-of-two and binary models mostly exploit LUTs for FPGA implementation, resulting in higher throughput especially on Spartan with few numbers of DSPs. For example, Spartan using binary model can achieve on average 5.2\u00d7 higher throughput than F5-HD using fixed-point model. It should be noted that in all FPGA platforms the throughput of the binary model is proportional to the number of available LUTs in FPGAs.\n\nTo compare the computation efficiency of different FPGAs, we eliminate the impact of available resources by using the throughput/Watt as the comparison metric. Figure 8(b) shows the throughput/Watt of F5-HD implemented in different platforms. As the results show, Virtex with large number of DSPs provides the maximum throughput/Watt when implementing F5-HD using fixedpoint model. However, using power-of-two and binary models, Spartan provides the higher computation efficiency since most of F5-HD computation can be processed by LUTs. For example, using the fixed-point model, Virtex can provide 2.0\u00d7 and 1.5\u00d7 higher throughput/Watt as compared to Spartan and Kintex, respectively. However, using the binary model, Spartan provides 1.2\u00d7 and 1.5\u00d7 higher throughput/Watt than Virtex and Kintex respectively.\n\nThe efficiency of different FPGAs also depends on the application, i.e., number of features and classes. For applications with small feature size (e.g., PAMAP), F5-HD can encode a larger amount of data at a time, thus the associative search in inference requires higher number of DSPs and BRAM accesses to parallelize the similarity check. This makes the number of DSPs the bottleneck of computation when using a fixed-point model for PAMAP application. PAMAP using power-of-two model eliminates the majority of DSP utilization required to multiply a query and class hypervector, thus the number of BRAMs becomes the computation bottleneck. These results are more obvious on the Spartan FPGA with limited BRAM blocks.\n\n\nPower Budget\n\nAs we explained in Section 3, the desired power budget is an input to F5-HD framework that can be dictated by the users before implementation of each application, which impacts the level of parallelism. When the user defines a desired power budget (P tar\u0434et ), F5-HD tries to determine the number of PEs per PU such that the implementation satisfies the power constraint. In practice, F5-HD may not precisely guarantee the desired power due to the fact that the number of PEs per PU has discrete values and the size of the application and its power consumption depend on this discrete parameter. Additionally, our initial estimation of the power consumption is according to the logical connectivity of the building blocks and may not accurately estimate the impact of signals power, which is routing-dependent 2 . Therefore, the measured power after implementation (P meas ) might have fluctuations around the target power level. Here we define the power fluctuation as \u0394P = |Pmeas \u2212 Ptar\u0434et | /Ptar\u0434et . Table 4 lists the average throughput (TP) and \u0394P after imposing the power budget. The table also shows the normalized throughput under power constraints to the nominal throughput when no power budget is employed. The results are reported for the cases that the power budget is defined as 25% and 50% of maximum power (power of F5-HD running on the same device without power restriction) as the desired power level. Our evaluations show that our framework  can generate HD accelerator that lays within \u0394P = 18% of the target power. The power fluctuation becomes large when the targeted power is low as the magnitude of misprediction (|P meas \u2212 P tar\u0434et |) almost remains the same while the base power P tar\u0434et reduces.\n\n\nCONCLUSION\n\nIn this paper, we proposed F5-HD, an automated framework for FPGA-based acceleration of HD computing. F5-HD abstracts away the complexities behind designing hardware accelerators from the user. The proposed framework enables the user to specify the HD application specifications (e.g., the number of input features, classes and training data) as well as the desired classification quality (i.e., accuracy versus performance) and accordingly generates customized FPGA-friendly Verilog implementation. In addition to training and inference, F5-HD supports simultaneous training and inference, hence the accuracy of the HD platform can be enhanced in the field without, without interrupting its operation. We evaluated the efficiency of F5-HD extensively, whereby it showed 86.9\u00d7 and 7.8\u00d7 (11.9\u00d7 and 1.7\u00d7) higher energy efficiency improvement and faster training (inference) as compared to an optimized implementation of HD on AMD R9 390 GPU, respectively.\n\nFigure 1 :\n1Overview of hyperdimensional learning and inference.\n\nFigure 2 demonstrates\n2F5-HD's workflow, explained as follows.\n\nFigure 2 :\n2Overview of the proposed framework, F5-HD.\n\n\n(a) illustrates the encoding scheme, in which the constituting bits of each dimension d i of the query hypervector \u00ec H are distinguished by the same color. Accordingly, to build up e.g., dimension d 0 (d 1 ) from \u00ec H , v 0 of the input hypervector chooses among b 0 (b 1 ) of the base hypervectors, v 1 selects from\n\nFigure 3 :\n3(a) The na\u00efve encoding scheme (b) Baseline segmented encoding (c) The proposed encoding scheme (d) Implementation of the population counter permuted to left globally, whereby bit b k takes the place of bit b S+k .\n\nFigure 4 :\n4Overview of the HD classification, consisting of HD model, associative search, PUs and PEs structure.\n\nFigure 5 :\n5Energy consumption and execution time of F5-HD versus other platforms during (a) training and (b) one epoch of retraining.\n\nFigure 6 :Figure 7 :\n67Energy consumption and execution time of HD during inference running on different platforms. Throughput and throughput/Watt in F5-HD using fixed-point, power-of-two, and binary models.\n\nFigure 8 :\n8(a) Average throughput of different FPGAs implementing F5-HD with fixed-point, power-of-two, and binary models. (b) Throughput/Watt of F5-HD implementing different applications on FPGA platforms.\n\nTable 1 :\n1Classification accuracy and performance of binary, power-of-two, and 8-bits fixed-point HD models running on CPU.Application \nBinary \nPower-of-two \nFixed-point \nAccuracy Exe.time Accuracy Exe.time Accuracy Exe.time \nSpeech Recognition \n88.1% \n1.6ms \n90.3% \n3.4ms \n95.5% \n10.5ms \nActivity Recognition 77.4% \n0.6ms \n88.0% \n1.3ms \n94.6% \n3.4ms \nFace Recognition \n48.5% \n0.7ms \n89.6% \n1.6ms \n96.9% \n4.6ms \nPhysical Monitoring \n85.7% \n1.1ms \n90.8% \n2.4ms \n94.5% \n7.8ms \n\n\n\nTable\n\n\nTable 3 :\n3Average resource utilization and power consumption of F5-HD implemented on KintexFixed-point Power-of-two BinaryResource \nutilization \n\nLUT \n46% \n95% \n82% \nBRAM \n47% \n46% \n5% \nDSP \n89% \n26% \n29% \nPower(W) \n9.8 \n14.8 \n13.9 \n\n\n\nTable 4 :\n4F5-HD implementation under power constraints.FPGA \nmodel \n\nPower \nbudget \n\nFixed-point \nPower-of-two \nBinary \nTP \n\u0394P \nT P \n\u0394P \nT P \n\u0394P \n\nVirtex \n50% \n2.7 (0.44\u00d7) 5.2% 5.1 (0.53\u00d7) 5.2% 6.3 (0.60\u00d7) 2.4% \n25% \n1.2 (0.19\u00d7) 10.8% 1.6 (0.16\u00d7) 7.0% 2.6 (0.25\u00d7) 8.0% \n\nSpartan \n50% \n0.2 (0.38\u00d7) 12.8% 0.7 (0.37\u00d7) 5.1% 1.0 (0.40\u00d7) 5.1% \n25% \n0.1 (0.21\u00d7) 9.1% 0.3 (0.17\u00d7) 16% 0.6 (0.21\u00d7) 17% \n\nKintex \n50% \n1.0 (0.41\u00d7) 5.2% 2.5 (0.45\u00d7) 6.8% 4.8 (0.65\u00d7) 4.0% \n25% \n0.4 (0.18\u00d7) 12.1% 1.3 (0.24\u00d7) 18% 1.7 (0.25\u00d7) 12% \n\n\nA population counter basically counts the number of 1 s (ones) in the given binary input.\nIn practice, we scale the power of the signal based on the measured signal power of a base implementationSession 2: Machine Learning 2 FPGA '19, February 24-26, 2019, Seaside, CA, USA\nACKNOWLEDGEMENTSThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1730158 and #1527034.\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed rep- resentation with high-dimensional random vectors, \" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nComputing with 10,000-bit words. P Kanerva, Communication, Control, and Computing (Allerton), 2014 52nd Annual Allerton Conference on. IEEEP. Kanerva, \"Computing with 10,000-bit words, \" in Communication, Control, and Computing (Allerton), 2014 52nd Annual Allerton Conference on, pp. 304-310, IEEE, 2014.\n\nA robust and energy-efficient classifier using braininspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignACMA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-efficient classifier using brain- inspired hyperdimensional computing, \" in Proceedings of the 2016 International Symposium on Low Power Electronics and Design, pp. 64-69, ACM, 2016.\n\nHyperdimensional computing for text classification. F R Najafabadi, A Rahimi, P Kanerva, J M Rabaey, Design, Automation Test in Europe Conference Exhibition (DATE). University BoothF. R. Najafabadi, A. Rahimi, P. Kanerva, and J. M. Rabaey, \"Hyperdimensional computing for text classification, \" in Design, Automation Test in Europe Conference Exhibition (DATE), Univer- sity Booth, pp. 1-1, 2016.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O J R\u00e4s\u00e4nen, J P Saarinen, IEEE transactions on neural networks and learning systems. 27O. J. R\u00e4s\u00e4nen and J. P. Saarinen, \"Sequence prediction with sparse distributed hyperdimen- sional coding applied to the analysis of mobile phone use patterns, \" IEEE transactions on neural networks and learning systems, vol. 27, no. 9, pp. 1878-1889, 2016.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, 2017 IEEE International Conference on. IEEEin Rebooting Computing (ICRCM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"Voicehd: Hyperdimensional computing for effi- cient speech recognition, \" in Rebooting Computing (ICRC), 2017 IEEE International Conference on, pp. 1-8, IEEE, 2017.\n\nPulp-hd: accelerating braininspired high-dimensional computing on a parallel ultra-low power platform. F Montagna, A Rahimi, S Benatti, D Rossi, L Benini, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM111F. Montagna, A. Rahimi, S. Benatti, D. Rossi, and L. Benini, \"Pulp-hd: accelerating brain- inspired high-dimensional computing on a parallel ultra-low power platform, \" in Proceedings of the 55th Annual Design Automation Conference, p. 111, ACM, 2018.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Transactions on Neural Networks and Learning Systems. 99O. Rasanen and J. Saarinen, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns, \" IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1-12, 2015.\n\nLanguage geometry using random indexing. A Joshi, J Halseth, P Kanerva, Quantum Interaction 2016 Conference Proceedings. In pressA. Joshi, J. Halseth, and P. Kanerva, \"Language geometry using random indexing, \" Quantum Interaction 2016 Conference Proceedings, In press.\n\nFinn: A framework for fast, scalable binarized neural network inference. Y Umuroglu, N J Fraser, G Gambardella, M Blott, P Leong, M Jahre, K Vissers, Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. the ACM/SIGDA International Symposium on Field-Programmable Gate ArraysACMY. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre, and K. Vissers, \"Finn: A framework for fast, scalable binarized neural network inference, \" in Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pp. 65-74, ACM, 2017.\n\nRnsnet: In-memory neural network acceleration using residue number system. S Salamat, M Imani, S Gupta, T Rosing, IEEE International Conference on. IEEERebooting Computing (ICRC)S. Salamat, M. Imani, S. Gupta, and T. Rosing, \"Rnsnet: In-memory neural network acceler- ation using residue number system, \" in Rebooting Computing (ICRC), 2018 IEEE International Conference on, pp. 1-10, IEEE, 2018.\n\nHyperdimensional biosignal processing: A case study for emg-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Rebooting Computing (ICRC), IEEE International Conference on. IEEEA. Rahimi, S. Benatti, P. Kanerva, L. Benini, and J. M. Rabaey, \"Hyperdimensional biosignal processing: A case study for emg-based hand gesture recognition, \" in Rebooting Computing (ICRC), IEEE International Conference on, pp. 1-8, IEEE, 2016.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM108M. Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdimensional computing for energy efficient classification, \" in Proceedings of the 55th Annual Design Automation Confer- ence, p. 108, ACM, 2018.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, 2017 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEEM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdimensional as- sociative memory, \" in 2017 IEEE International Symposium on High-Performance Computer Ar- chitecture (HPCA), pp. 445-456, IEEE, 2017.\n\nThe density advantage of configurable computing. A Dehon, Computer. 334A. DeHon, \"The density advantage of configurable computing, \" Computer, vol. 33, no. 4, pp. 41-49, 2000.\n\nImprove high level synthesis for multidimensional nested loops using reshaping and vectorization methods for multi-level nonrectangular nested loop. S Salamat, M R Azarbad, B Alizadeh, IEEE International Conference on. IEEERebooting Computing (ICRC)S. Salamat, M. R. Azarbad, and B. Alizadeh, \"Improve high level synthesis for multi- dimensional nested loops using reshaping and vectorization methods for multi-level non- rectangular nested loop, \" in Rebooting Computing (ICRC), 2018 IEEE International Conference on, pp. 1-10, IEEE, 2018.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. M Schmuck, L Benini, A Rahimi, arXiv:1807.08583arXiv preprintM. Schmuck, L. Benini, and A. Rahimi, \"Hardware optimizations of dense binary hyperdimen- sional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory, \" arXiv preprint arXiv:1807.08583, 2018.\n\nLow-power sparse hyperdimensional encoder for language recognition. M Imani, IEEE Design & Test. 346M. Imani et al., \"Low-power sparse hyperdimensional encoder for language recognition, \" IEEE Design & Test, vol. 34, no. 6, pp. 94-101, 2017.\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, BHI. IEEEM. Imani et al., \"Hdna: Energy-efficient dna sequencing using hyperdimensional computing, \" in BHI, pp. 271-274, IEEE, 2018.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, IoTACM38Y. Kim et al., \"Efficient human activity recognition using hyperdimensional computing, \" in IoT, p. 38, ACM, 2018.\n\nFach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity. M Imani, ASP-DAC. IEEEM. Imani et al., \"Fach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity, \" in ASP-DAC, IEEE, 2019.\n\nA binary learning framework for hyperdimensional computing. M Imani, DATE. M. Imani et al., \"A binary learning framework for hyperdimensional computing, \" in DATE, IEEE/ACM, 2019.\n\nHdcluster: An accurate clustering using brain-inspired high-dimensional computing. M Imani, DATE. M. Imani et al., \"Hdcluster: An accurate clustering using brain-inspired high-dimensional com- puting, \" in DATE, IEEE/ACM, 2019.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T F Wu, H Li, P.-C Huang, A Rahimi, J M Rabaey, H.-S P Wong, M M Shulaker, S Mitra, Solid-State Circuits Conference-(ISSCC). IEEET. F. Wu, H. Li, P.-C. Huang, A. Rahimi, J. M. Rabaey, H.-S. P. Wong, M. M. Shulaker, and S. Mitra, \"Brain-inspired computing exploiting carbon nanotube fets and resistive ram: Hy- perdimensional computing case study, \" in Solid-State Circuits Conference-(ISSCC), 2018 IEEE International, pp. 492-494, IEEE, 2018.\n\nHyperdimensional computing with 3d vrram in-memory kernels: Devicearchitecture co-design for energy-efficient, error-resilient language recognition. H Li, Electron Devices Meeting (IEDM). IEEEH. Li et al., \"Hyperdimensional computing with 3d vrram in-memory kernels: Device- architecture co-design for energy-efficient, error-resilient language recognition, \" in Electron Devices Meeting (IEDM), 2016 IEEE International, pp. 16-1, IEEE, 2016.\n\nFelix: fast and energy-efficient logic in memory. S Gupta, ICCAD. ACM55S. Gupta et al., \"Felix: fast and energy-efficient logic in memory, \" in ICCAD, p. 55, ACM, 2018.\n\nFpgas versus gpus in data centers. B Falsafi, B Dally, D Singh, D Chiou, J Y Joshua, R Sendag, IEEE Micro. 371B. Falsafi, B. Dally, D. Singh, D. Chiou, J. Y. Joshua, and R. Sendag, \"Fpgas versus gpus in data centers, \" IEEE Micro, vol. 37, no. 1, pp. 60-72, 2017.\n\nXilinx power estimator user guide. User Guide\"Xilinx power estimator user guide. \" User Guide, June 2017.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite, \" White Paper, vol. 5, 2012.\n\nAmd. \"Amd. \" http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/.\n\nUci machine learning repository. \"Uci machine learning repository. \" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nUci machine learning repository. \"Uci machine learning repository. \" https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+ Activities.\n\nCreating and benchmarking a new dataset for physical activity monitoring. A Reiss, D Stricker, Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments. the 5th International Conference on PErvasive Technologies Related to Assistive EnvironmentsACM40A. Reiss and D. Stricker, \"Creating and benchmarking a new dataset for physical activity mon- itoring, \" in Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments, p. 40, ACM, 2012.\n\nCaltech-256 object category dataset. G Griffin, A Holub, P Perona, G. Griffin, A. Holub, and P. Perona, \"Caltech-256 object category dataset, \" 2007.\n\nThe pascal visual object classes challenge: A retrospective. M Everingham, S A Eslami, L Van Gool, C K Williams, J Winn, A Zisserman, International journal of computer vision. 1111M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, \"The pascal visual object classes challenge: A retrospective, \" International journal of computer vision, vol. 111, no. 1, pp. 98-136, 2015.\n", "annotations": {"author": "[{\"end\":197,\"start\":87},{\"end\":305,\"start\":198},{\"end\":418,\"start\":306},{\"end\":526,\"start\":419}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":94},{\"end\":210,\"start\":205},{\"end\":321,\"start\":313},{\"end\":432,\"start\":426}]", "author_first_name": "[{\"end\":93,\"start\":87},{\"end\":204,\"start\":198},{\"end\":312,\"start\":306},{\"end\":425,\"start\":419}]", "author_affiliation": "[{\"end\":196,\"start\":121},{\"end\":304,\"start\":229},{\"end\":417,\"start\":342},{\"end\":525,\"start\":450}]", "title": "[{\"end\":84,\"start\":1},{\"end\":610,\"start\":527}]", "venue": null, "abstract": "[{\"end\":2183,\"start\":635}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2349,\"start\":2346},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2351,\"start\":2349},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2788,\"start\":2785},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2791,\"start\":2788},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2794,\"start\":2791},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2797,\"start\":2794},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2800,\"start\":2797},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2803,\"start\":2800},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2806,\"start\":2803},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2943,\"start\":2939},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2946,\"start\":2943},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3101,\"start\":3098},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3241,\"start\":3237},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3244,\"start\":3241},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3776,\"start\":3773},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3779,\"start\":3776},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4376,\"start\":4373},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4555,\"start\":4551},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4950,\"start\":4946},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5104,\"start\":5100},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5107,\"start\":5104},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7684,\"start\":7681},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8214,\"start\":8211},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8217,\"start\":8214},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8220,\"start\":8217},{\"end\":11437,\"start\":11427},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14512,\"start\":14508},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14533,\"start\":14529},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14567,\"start\":14564},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14570,\"start\":14567},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14594,\"start\":14591},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14597,\"start\":14594},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14627,\"start\":14623},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14630,\"start\":14627},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14647,\"start\":14643},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14864,\"start\":14860},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15363,\"start\":15359},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15685,\"start\":15681},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15984,\"start\":15980},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15988,\"start\":15984},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15992,\"start\":15988},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16001,\"start\":15997},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16330,\"start\":16326},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16833,\"start\":16829},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19416,\"start\":19412},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22401,\"start\":22397},{\"end\":28205,\"start\":28195},{\"end\":38959,\"start\":38949},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":40004,\"start\":40000},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":40412,\"start\":40408},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40827,\"start\":40823},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":40943,\"start\":40939},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41098,\"start\":41094},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":41276,\"start\":41272},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41397,\"start\":41393},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41852,\"start\":41848}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54067,\"start\":54002},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54131,\"start\":54068},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54187,\"start\":54132},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54505,\"start\":54188},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54732,\"start\":54506},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54847,\"start\":54733},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54983,\"start\":54848},{\"attributes\":{\"id\":\"fig_8\"},\"end\":55192,\"start\":54984},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55401,\"start\":55193},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55880,\"start\":55402},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55888,\"start\":55881},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56125,\"start\":55889},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56644,\"start\":56126}]", "paragraph": "[{\"end\":3245,\"start\":2199},{\"end\":4377,\"start\":3247},{\"end\":5108,\"start\":4379},{\"end\":5940,\"start\":5110},{\"end\":7238,\"start\":5942},{\"end\":7504,\"start\":7270},{\"end\":8221,\"start\":7535},{\"end\":8593,\"start\":8223},{\"end\":10089,\"start\":8668},{\"end\":10885,\"start\":10091},{\"end\":10990,\"start\":10961},{\"end\":11574,\"start\":11045},{\"end\":12068,\"start\":11627},{\"end\":12726,\"start\":12140},{\"end\":13816,\"start\":12776},{\"end\":14078,\"start\":13860},{\"end\":14672,\"start\":14098},{\"end\":15907,\"start\":14674},{\"end\":16496,\"start\":15909},{\"end\":16699,\"start\":16498},{\"end\":18002,\"start\":16728},{\"end\":19103,\"start\":18021},{\"end\":20677,\"start\":19105},{\"end\":21252,\"start\":20679},{\"end\":21396,\"start\":21254},{\"end\":21962,\"start\":21398},{\"end\":23146,\"start\":21997},{\"end\":24638,\"start\":23165},{\"end\":25803,\"start\":24640},{\"end\":27445,\"start\":25820},{\"end\":27940,\"start\":27468},{\"end\":28457,\"start\":27969},{\"end\":28760,\"start\":28516},{\"end\":29959,\"start\":28762},{\"end\":31084,\"start\":29961},{\"end\":32016,\"start\":31247},{\"end\":32469,\"start\":32039},{\"end\":33278,\"start\":32471},{\"end\":34746,\"start\":33280},{\"end\":34891,\"start\":34748},{\"end\":35718,\"start\":34893},{\"end\":36363,\"start\":35720},{\"end\":36938,\"start\":36365},{\"end\":38119,\"start\":36940},{\"end\":38536,\"start\":38121},{\"end\":39301,\"start\":38679},{\"end\":40143,\"start\":39326},{\"end\":41398,\"start\":40145},{\"end\":42815,\"start\":41411},{\"end\":43998,\"start\":42828},{\"end\":48669,\"start\":44000},{\"end\":49765,\"start\":48748},{\"end\":50575,\"start\":49767},{\"end\":51294,\"start\":50577},{\"end\":53033,\"start\":51311},{\"end\":54001,\"start\":53048}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8667,\"start\":8594},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10960,\"start\":10886},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11044,\"start\":10991},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11626,\"start\":11575},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12139,\"start\":12069},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12775,\"start\":12727},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13859,\"start\":13817},{\"attributes\":{\"id\":\"formula_7\"},\"end\":28515,\"start\":28458},{\"attributes\":{\"id\":\"formula_8\"},\"end\":31246,\"start\":31085},{\"attributes\":{\"id\":\"formula_9\"},\"end\":38621,\"start\":38537},{\"attributes\":{\"id\":\"formula_10\"},\"end\":38678,\"start\":38621}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22771,\"start\":22764},{\"end\":41861,\"start\":41854},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47545,\"start\":47538},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":48278,\"start\":48271},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":52323,\"start\":52316}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2197,\"start\":2185},{\"attributes\":{\"n\":\"2\"},\"end\":7268,\"start\":7241},{\"attributes\":{\"n\":\"2.1\"},\"end\":7533,\"start\":7507},{\"attributes\":{\"n\":\"2.2\"},\"end\":14096,\"start\":14081},{\"attributes\":{\"n\":\"3\"},\"end\":16726,\"start\":16702},{\"attributes\":{\"n\":\"3.1\"},\"end\":18019,\"start\":18005},{\"attributes\":{\"n\":\"3.2\"},\"end\":21995,\"start\":21965},{\"attributes\":{\"n\":\"3.3\"},\"end\":23163,\"start\":23149},{\"attributes\":{\"n\":\"3.4\"},\"end\":25818,\"start\":25806},{\"attributes\":{\"n\":\"4\"},\"end\":27466,\"start\":27448},{\"attributes\":{\"n\":\"4.1\"},\"end\":27967,\"start\":27943},{\"attributes\":{\"n\":\"4.2\"},\"end\":32037,\"start\":32019},{\"attributes\":{\"n\":\"5\"},\"end\":39324,\"start\":39304},{\"attributes\":{\"n\":\"5.1\"},\"end\":41409,\"start\":41401},{\"attributes\":{\"n\":\"5.2\"},\"end\":42826,\"start\":42818},{\"attributes\":{\"n\":\"5.3\"},\"end\":48681,\"start\":48672},{\"attributes\":{\"n\":\"5.4\"},\"end\":48710,\"start\":48684},{\"attributes\":{\"n\":\"5.5\"},\"end\":48746,\"start\":48713},{\"attributes\":{\"n\":\"5.6\"},\"end\":51309,\"start\":51297},{\"attributes\":{\"n\":\"6\"},\"end\":53046,\"start\":53036},{\"end\":54013,\"start\":54003},{\"end\":54090,\"start\":54069},{\"end\":54143,\"start\":54133},{\"end\":54517,\"start\":54507},{\"end\":54744,\"start\":54734},{\"end\":54859,\"start\":54849},{\"end\":55005,\"start\":54985},{\"end\":55204,\"start\":55194},{\"end\":55412,\"start\":55403},{\"end\":55887,\"start\":55882},{\"end\":55899,\"start\":55890},{\"end\":56136,\"start\":56127}]", "table": "[{\"end\":55880,\"start\":55527},{\"end\":56125,\"start\":56013},{\"end\":56644,\"start\":56183}]", "figure_caption": "[{\"end\":54067,\"start\":54015},{\"end\":54131,\"start\":54092},{\"end\":54187,\"start\":54145},{\"end\":54505,\"start\":54190},{\"end\":54732,\"start\":54519},{\"end\":54847,\"start\":54746},{\"end\":54983,\"start\":54861},{\"end\":55192,\"start\":55008},{\"end\":55401,\"start\":55206},{\"end\":55527,\"start\":55414},{\"end\":56013,\"start\":55901},{\"end\":56183,\"start\":56138}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8260,\"start\":8252},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28456,\"start\":28447},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28810,\"start\":28802},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29384,\"start\":29376},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31036,\"start\":31028},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31428,\"start\":31417},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32100,\"start\":32092},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35717,\"start\":35708},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35739,\"start\":35731},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37126,\"start\":37118},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":43171,\"start\":43163},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44681,\"start\":44673},{\"end\":45106,\"start\":45098},{\"end\":46675,\"start\":46667},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":48921,\"start\":48913},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":49938,\"start\":49927}]", "bib_author_first_name": "[{\"end\":57211,\"start\":57210},{\"end\":57480,\"start\":57479},{\"end\":57843,\"start\":57842},{\"end\":57853,\"start\":57852},{\"end\":57864,\"start\":57863},{\"end\":57866,\"start\":57865},{\"end\":58330,\"start\":58329},{\"end\":58332,\"start\":58331},{\"end\":58346,\"start\":58345},{\"end\":58356,\"start\":58355},{\"end\":58367,\"start\":58366},{\"end\":58369,\"start\":58368},{\"end\":58798,\"start\":58797},{\"end\":58800,\"start\":58799},{\"end\":58811,\"start\":58810},{\"end\":58813,\"start\":58812},{\"end\":59214,\"start\":59213},{\"end\":59223,\"start\":59222},{\"end\":59231,\"start\":59230},{\"end\":59241,\"start\":59240},{\"end\":59637,\"start\":59636},{\"end\":59649,\"start\":59648},{\"end\":59659,\"start\":59658},{\"end\":59670,\"start\":59669},{\"end\":59679,\"start\":59678},{\"end\":60175,\"start\":60174},{\"end\":60186,\"start\":60185},{\"end\":60546,\"start\":60545},{\"end\":60555,\"start\":60554},{\"end\":60566,\"start\":60565},{\"end\":60849,\"start\":60848},{\"end\":60861,\"start\":60860},{\"end\":60863,\"start\":60862},{\"end\":60873,\"start\":60872},{\"end\":60888,\"start\":60887},{\"end\":60897,\"start\":60896},{\"end\":60906,\"start\":60905},{\"end\":60915,\"start\":60914},{\"end\":61442,\"start\":61441},{\"end\":61453,\"start\":61452},{\"end\":61462,\"start\":61461},{\"end\":61471,\"start\":61470},{\"end\":61857,\"start\":61856},{\"end\":61867,\"start\":61866},{\"end\":61878,\"start\":61877},{\"end\":61889,\"start\":61888},{\"end\":61899,\"start\":61898},{\"end\":61901,\"start\":61900},{\"end\":62300,\"start\":62299},{\"end\":62309,\"start\":62308},{\"end\":62318,\"start\":62317},{\"end\":62326,\"start\":62325},{\"end\":62704,\"start\":62703},{\"end\":62713,\"start\":62712},{\"end\":62723,\"start\":62722},{\"end\":62731,\"start\":62730},{\"end\":62741,\"start\":62740},{\"end\":62743,\"start\":62742},{\"end\":63116,\"start\":63115},{\"end\":63393,\"start\":63392},{\"end\":63404,\"start\":63403},{\"end\":63406,\"start\":63405},{\"end\":63417,\"start\":63416},{\"end\":63946,\"start\":63945},{\"end\":63957,\"start\":63956},{\"end\":63967,\"start\":63966},{\"end\":64318,\"start\":64317},{\"end\":64565,\"start\":64564},{\"end\":64780,\"start\":64779},{\"end\":65009,\"start\":65008},{\"end\":65234,\"start\":65233},{\"end\":65438,\"start\":65437},{\"end\":65699,\"start\":65698},{\"end\":65701,\"start\":65700},{\"end\":65707,\"start\":65706},{\"end\":65716,\"start\":65712},{\"end\":65725,\"start\":65724},{\"end\":65735,\"start\":65734},{\"end\":65737,\"start\":65736},{\"end\":65750,\"start\":65746},{\"end\":65752,\"start\":65751},{\"end\":65760,\"start\":65759},{\"end\":65762,\"start\":65761},{\"end\":65774,\"start\":65773},{\"end\":66292,\"start\":66291},{\"end\":66637,\"start\":66636},{\"end\":66792,\"start\":66791},{\"end\":66803,\"start\":66802},{\"end\":66812,\"start\":66811},{\"end\":66821,\"start\":66820},{\"end\":66830,\"start\":66829},{\"end\":66832,\"start\":66831},{\"end\":66842,\"start\":66841},{\"end\":67150,\"start\":67149},{\"end\":67641,\"start\":67640},{\"end\":67650,\"start\":67649},{\"end\":68141,\"start\":68140},{\"end\":68152,\"start\":68151},{\"end\":68161,\"start\":68160},{\"end\":68316,\"start\":68315},{\"end\":68330,\"start\":68329},{\"end\":68332,\"start\":68331},{\"end\":68342,\"start\":68341},{\"end\":68354,\"start\":68353},{\"end\":68356,\"start\":68355},{\"end\":68368,\"start\":68367},{\"end\":68376,\"start\":68375}]", "bib_author_last_name": "[{\"end\":57219,\"start\":57212},{\"end\":57488,\"start\":57481},{\"end\":57850,\"start\":57844},{\"end\":57861,\"start\":57854},{\"end\":57873,\"start\":57867},{\"end\":58343,\"start\":58333},{\"end\":58353,\"start\":58347},{\"end\":58364,\"start\":58357},{\"end\":58376,\"start\":58370},{\"end\":58808,\"start\":58801},{\"end\":58822,\"start\":58814},{\"end\":59220,\"start\":59215},{\"end\":59228,\"start\":59224},{\"end\":59238,\"start\":59232},{\"end\":59248,\"start\":59242},{\"end\":59646,\"start\":59638},{\"end\":59656,\"start\":59650},{\"end\":59667,\"start\":59660},{\"end\":59676,\"start\":59671},{\"end\":59686,\"start\":59680},{\"end\":60183,\"start\":60176},{\"end\":60195,\"start\":60187},{\"end\":60552,\"start\":60547},{\"end\":60563,\"start\":60556},{\"end\":60574,\"start\":60567},{\"end\":60858,\"start\":60850},{\"end\":60870,\"start\":60864},{\"end\":60885,\"start\":60874},{\"end\":60894,\"start\":60889},{\"end\":60903,\"start\":60898},{\"end\":60912,\"start\":60907},{\"end\":60923,\"start\":60916},{\"end\":61450,\"start\":61443},{\"end\":61459,\"start\":61454},{\"end\":61468,\"start\":61463},{\"end\":61478,\"start\":61472},{\"end\":61864,\"start\":61858},{\"end\":61875,\"start\":61868},{\"end\":61886,\"start\":61879},{\"end\":61896,\"start\":61890},{\"end\":61908,\"start\":61902},{\"end\":62306,\"start\":62301},{\"end\":62315,\"start\":62310},{\"end\":62323,\"start\":62319},{\"end\":62333,\"start\":62327},{\"end\":62710,\"start\":62705},{\"end\":62720,\"start\":62714},{\"end\":62728,\"start\":62724},{\"end\":62738,\"start\":62732},{\"end\":62750,\"start\":62744},{\"end\":63122,\"start\":63117},{\"end\":63401,\"start\":63394},{\"end\":63414,\"start\":63407},{\"end\":63426,\"start\":63418},{\"end\":63954,\"start\":63947},{\"end\":63964,\"start\":63958},{\"end\":63974,\"start\":63968},{\"end\":64324,\"start\":64319},{\"end\":64571,\"start\":64566},{\"end\":64784,\"start\":64781},{\"end\":65015,\"start\":65010},{\"end\":65240,\"start\":65235},{\"end\":65444,\"start\":65439},{\"end\":65704,\"start\":65702},{\"end\":65710,\"start\":65708},{\"end\":65722,\"start\":65717},{\"end\":65732,\"start\":65726},{\"end\":65744,\"start\":65738},{\"end\":65757,\"start\":65753},{\"end\":65771,\"start\":65763},{\"end\":65780,\"start\":65775},{\"end\":66295,\"start\":66293},{\"end\":66643,\"start\":66638},{\"end\":66800,\"start\":66793},{\"end\":66809,\"start\":66804},{\"end\":66818,\"start\":66813},{\"end\":66827,\"start\":66822},{\"end\":66839,\"start\":66833},{\"end\":66849,\"start\":66843},{\"end\":67156,\"start\":67151},{\"end\":67647,\"start\":67642},{\"end\":67659,\"start\":67651},{\"end\":68149,\"start\":68142},{\"end\":68158,\"start\":68153},{\"end\":68168,\"start\":68162},{\"end\":68327,\"start\":68317},{\"end\":68339,\"start\":68333},{\"end\":68351,\"start\":68343},{\"end\":68365,\"start\":68357},{\"end\":68373,\"start\":68369},{\"end\":68386,\"start\":68377}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":733980},\"end\":57444,\"start\":57085},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13894815},\"end\":57751,\"start\":57446},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9812826},\"end\":58275,\"start\":57753},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":201625025},\"end\":58673,\"start\":58277},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15258913},\"end\":59141,\"start\":58675},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":21351739},\"end\":59531,\"start\":59143},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52812586},\"end\":60050,\"start\":59533},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15258913},\"end\":60502,\"start\":60052},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":39020350},\"end\":60773,\"start\":60504},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10530917},\"end\":61364,\"start\":60775},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53320392},\"end\":61762,\"start\":61366},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12008695},\"end\":62220,\"start\":61764},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49301394},\"end\":62654,\"start\":62222},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1677864},\"end\":63064,\"start\":62656},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13295558},\"end\":63241,\"start\":63066},{\"attributes\":{\"id\":\"b15\"},\"end\":63783,\"start\":63243},{\"attributes\":{\"doi\":\"arXiv:1807.08583\",\"id\":\"b16\"},\"end\":64247,\"start\":63785},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8038292},\"end\":64490,\"start\":64249},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4708051},\"end\":64706,\"start\":64492},{\"attributes\":{\"id\":\"b19\"},\"end\":64908,\"start\":64708},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":58027670},\"end\":65171,\"start\":64910},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":155109576},\"end\":65352,\"start\":65173},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":155106744},\"end\":65581,\"start\":65354},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3869844},\"end\":66140,\"start\":65583},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":25209638},\"end\":66584,\"start\":66142},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53235957},\"end\":66754,\"start\":66586},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1165798},\"end\":67019,\"start\":66756},{\"attributes\":{\"id\":\"b27\"},\"end\":67126,\"start\":67021},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":110511037},\"end\":67232,\"start\":67128},{\"attributes\":{\"id\":\"b29\"},\"end\":67307,\"start\":67234},{\"attributes\":{\"id\":\"b30\"},\"end\":67424,\"start\":67309},{\"attributes\":{\"id\":\"b31\"},\"end\":67564,\"start\":67426},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":12031548},\"end\":68101,\"start\":67566},{\"attributes\":{\"id\":\"b33\"},\"end\":68252,\"start\":68103},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207252270},\"end\":68659,\"start\":68254}]", "bib_title": "[{\"end\":57208,\"start\":57085},{\"end\":57477,\"start\":57446},{\"end\":57840,\"start\":57753},{\"end\":58327,\"start\":58277},{\"end\":58795,\"start\":58675},{\"end\":59211,\"start\":59143},{\"end\":59634,\"start\":59533},{\"end\":60172,\"start\":60052},{\"end\":60543,\"start\":60504},{\"end\":60846,\"start\":60775},{\"end\":61439,\"start\":61366},{\"end\":61854,\"start\":61764},{\"end\":62297,\"start\":62222},{\"end\":62701,\"start\":62656},{\"end\":63113,\"start\":63066},{\"end\":63390,\"start\":63243},{\"end\":64315,\"start\":64249},{\"end\":64562,\"start\":64492},{\"end\":65006,\"start\":64910},{\"end\":65231,\"start\":65173},{\"end\":65435,\"start\":65354},{\"end\":65696,\"start\":65583},{\"end\":66289,\"start\":66142},{\"end\":66634,\"start\":66586},{\"end\":66789,\"start\":66756},{\"end\":67147,\"start\":67128},{\"end\":67638,\"start\":67566},{\"end\":68313,\"start\":68254}]", "bib_author": "[{\"end\":57221,\"start\":57210},{\"end\":57490,\"start\":57479},{\"end\":57852,\"start\":57842},{\"end\":57863,\"start\":57852},{\"end\":57875,\"start\":57863},{\"end\":58345,\"start\":58329},{\"end\":58355,\"start\":58345},{\"end\":58366,\"start\":58355},{\"end\":58378,\"start\":58366},{\"end\":58810,\"start\":58797},{\"end\":58824,\"start\":58810},{\"end\":59222,\"start\":59213},{\"end\":59230,\"start\":59222},{\"end\":59240,\"start\":59230},{\"end\":59250,\"start\":59240},{\"end\":59648,\"start\":59636},{\"end\":59658,\"start\":59648},{\"end\":59669,\"start\":59658},{\"end\":59678,\"start\":59669},{\"end\":59688,\"start\":59678},{\"end\":60185,\"start\":60174},{\"end\":60197,\"start\":60185},{\"end\":60554,\"start\":60545},{\"end\":60565,\"start\":60554},{\"end\":60576,\"start\":60565},{\"end\":60860,\"start\":60848},{\"end\":60872,\"start\":60860},{\"end\":60887,\"start\":60872},{\"end\":60896,\"start\":60887},{\"end\":60905,\"start\":60896},{\"end\":60914,\"start\":60905},{\"end\":60925,\"start\":60914},{\"end\":61452,\"start\":61441},{\"end\":61461,\"start\":61452},{\"end\":61470,\"start\":61461},{\"end\":61480,\"start\":61470},{\"end\":61866,\"start\":61856},{\"end\":61877,\"start\":61866},{\"end\":61888,\"start\":61877},{\"end\":61898,\"start\":61888},{\"end\":61910,\"start\":61898},{\"end\":62308,\"start\":62299},{\"end\":62317,\"start\":62308},{\"end\":62325,\"start\":62317},{\"end\":62335,\"start\":62325},{\"end\":62712,\"start\":62703},{\"end\":62722,\"start\":62712},{\"end\":62730,\"start\":62722},{\"end\":62740,\"start\":62730},{\"end\":62752,\"start\":62740},{\"end\":63124,\"start\":63115},{\"end\":63403,\"start\":63392},{\"end\":63416,\"start\":63403},{\"end\":63428,\"start\":63416},{\"end\":63956,\"start\":63945},{\"end\":63966,\"start\":63956},{\"end\":63976,\"start\":63966},{\"end\":64326,\"start\":64317},{\"end\":64573,\"start\":64564},{\"end\":64786,\"start\":64779},{\"end\":65017,\"start\":65008},{\"end\":65242,\"start\":65233},{\"end\":65446,\"start\":65437},{\"end\":65706,\"start\":65698},{\"end\":65712,\"start\":65706},{\"end\":65724,\"start\":65712},{\"end\":65734,\"start\":65724},{\"end\":65746,\"start\":65734},{\"end\":65759,\"start\":65746},{\"end\":65773,\"start\":65759},{\"end\":65782,\"start\":65773},{\"end\":66297,\"start\":66291},{\"end\":66645,\"start\":66636},{\"end\":66802,\"start\":66791},{\"end\":66811,\"start\":66802},{\"end\":66820,\"start\":66811},{\"end\":66829,\"start\":66820},{\"end\":66841,\"start\":66829},{\"end\":66851,\"start\":66841},{\"end\":67158,\"start\":67149},{\"end\":67649,\"start\":67640},{\"end\":67661,\"start\":67649},{\"end\":68151,\"start\":68140},{\"end\":68160,\"start\":68151},{\"end\":68170,\"start\":68160},{\"end\":68329,\"start\":68315},{\"end\":68341,\"start\":68329},{\"end\":68353,\"start\":68341},{\"end\":68367,\"start\":68353},{\"end\":68375,\"start\":68367},{\"end\":68388,\"start\":68375}]", "bib_venue": "[{\"end\":58028,\"start\":57960},{\"end\":59793,\"start\":59749},{\"end\":61084,\"start\":61013},{\"end\":62440,\"start\":62396},{\"end\":67862,\"start\":67770},{\"end\":57242,\"start\":57221},{\"end\":57579,\"start\":57490},{\"end\":57958,\"start\":57875},{\"end\":58440,\"start\":58378},{\"end\":58881,\"start\":58824},{\"end\":59287,\"start\":59250},{\"end\":59747,\"start\":59688},{\"end\":60254,\"start\":60197},{\"end\":60623,\"start\":60576},{\"end\":61011,\"start\":60925},{\"end\":61512,\"start\":61480},{\"end\":61970,\"start\":61910},{\"end\":62394,\"start\":62335},{\"end\":62834,\"start\":62752},{\"end\":63132,\"start\":63124},{\"end\":63460,\"start\":63428},{\"end\":63943,\"start\":63785},{\"end\":64344,\"start\":64326},{\"end\":64576,\"start\":64573},{\"end\":64777,\"start\":64708},{\"end\":65024,\"start\":65017},{\"end\":65246,\"start\":65242},{\"end\":65450,\"start\":65446},{\"end\":65821,\"start\":65782},{\"end\":66328,\"start\":66297},{\"end\":66650,\"start\":66645},{\"end\":66861,\"start\":66851},{\"end\":67054,\"start\":67021},{\"end\":67169,\"start\":67158},{\"end\":67237,\"start\":67234},{\"end\":67340,\"start\":67309},{\"end\":67457,\"start\":67426},{\"end\":67768,\"start\":67661},{\"end\":68138,\"start\":68103},{\"end\":68428,\"start\":68388}]"}}}, "year": 2023, "month": 12, "day": 17}