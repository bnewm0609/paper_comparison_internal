{"id": 215737269, "updated": "2023-10-06 16:57:53.889", "metadata": {"title": "Improved Automatic Summarization of Subroutines via Attention to File Context", "authors": "[{\"first\":\"Sakib\",\"last\":\"Haque\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"LeClair\",\"middle\":[]},{\"first\":\"Lingfei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Collin\",\"last\":\"McMillan\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 17th International Conference on Mining Software Repositories", "publication_date": {"year": 2020, "month": 4, "day": 10}, "abstract": "Software documentation largely consists of short, natural language summaries of the subroutines in the software. These summaries help programmers quickly understand what a subroutine does without having to read the source code him or herself. The task of writing these descriptions is called\"source code summarization\"and has been a target of research for several years. Recently, AI-based approaches have superseded older, heuristic-based approaches. Yet, to date these AI-based approaches assume that all the content needed to predict summaries is inside subroutine itself. This assumption limits performance because many subroutines cannot be understood without surrounding context. In this paper, we present an approach that models the file context of subroutines (i.e. other subroutines in the same file) and uses an attention mechanism to find words and concepts to use in summaries. We show in an experiment that our approach extends and improves several recent baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2004.04881", "mag": "3103748122", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/msr/HaqueLWM20", "doi": "10.1145/3379597.3387449"}}, "content": {"source": {"pdf_hash": "44d95ad5402abfd4003ffc1be5872ed829bec750", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.04881v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2004.04881", "status": "GREEN"}}, "grobid": {"id": "f7de3a3ec2c67b7c7448315e2e233127e1d06344", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/44d95ad5402abfd4003ffc1be5872ed829bec750.txt", "contents": "\nImproved Automatic Summarization of Subroutines via Attention to File Context\nACMCopyright ACM2020. October 5-6, 2020\n\nSakib Haque shaque@nd.edu \nAlexander Leclair aleclair@nd.edu \nLingfei Wu \nCollin Mcmillan \nSakib Haque \nAlexander Leclair \nLingfei Wu \nCollin Mcmillan \n\nDept. of Computer Science\nDept. of Computer Science\nUniversity of Notre Dame Notre Dame\nINUSA\n\n\nDame Notre Dame\nUniversity of Notre\nINUSA\n\n\nDept. of Computer Science\nIBM Research Yorktown Heights\nNYUSA\n\n\nDame Notre Dame\nUniversity of Notre\nINUSA\n\nImproved Automatic Summarization of Subroutines via Attention to File Context\n\n17th International Conference on Mining Software Repositories (MSR '20)\nSeoul, Republic of Korea; New York, NY, USAACM112020. October 5-6, 202010.1145/3379597.3387449\nSoftware documentation largely consists of short, natural language summaries of the subroutines in the software. These summaries help programmers quickly understand what a subroutine does without having to read the source code him or herself. The task of writing these descriptions is called \"source code summarization\" and has been a target of research for several years. Recently, AI-based approaches have superseded older, heuristic-based approaches. Yet, to date these AI-based approaches assume that all the content needed to predict summaries is inside subroutine itself. This assumption limits performance because many subroutines cannot be understood without surrounding context. In this paper, we present an approach that models the file context of subroutines (i.e. other subroutines in the same file) and uses an attention mechanism to find words and concepts to use in summaries. We show in an experiment that our approach extends and improves several recent baselines.CCS CONCEPTS\u2022 Software and its engineering \u2192 Software maintenance tools; KEYWORDS source code summarization, neural networks, documentation generation, artificial intelligence, natural language processing ACM Reference Format:\n\ndo, the role they play in a program, and how to use them [14]. Even a short description such as \"initializes microphone for web conference\" says a lot to a programmer about what a subroutine does. The task of generating these descriptions has become known as \"summarization\" of subroutines. The problem definition is quite simple: given the source code for a subroutine, generate a onesentence description of that subroutine. Yet while currently a vast majority of summarization is handled manually by a programmer, automatic summarization has a long history of scientific interest and been described as a \"holy grail\" [26] of SE research.\n\nA flurry of recent research has started to make automatic summarization a reality. Following the pattern in many research areas, early efforts were based on manual encoding of human knowledge such as sentence templates [30,39,40], until around 2016-2018 when AI-based, data-driven approaches superseded manual approaches. Nearly all of the literature on these AI-based approaches to subroutine summarization is inspired by Neural Machine Translation (NMT) from the Natural Language Processing research area. In the typical NMT problem, a neural model is trained using pairs of sentences in one language and their translation in another language. A stereotyped application to code summarization is that pairs of code and description are used to train a model instead -code serves as one \"language\" and descriptions as the other [22].\n\nThese existing approaches have shown promising, but not yet excellent, performance. The first techniques focused on an off-theshelf application of an encoder-decoder neural architecture such as by Iyer et al. [22], with advancements looking to squeeze more information from the code such as by Hu et al. [21] and LeClair et al. [26] using a flattened abstract syntax tree, and very recently Alon et al. [4] and Allamanis et al. [3] using graph neural nets and execution paths. Yet, all of these approaches are based on an assumption that a subroutine can be summarized using only the code inside that subroutine: the only input is the code of the subroutine itself, and the model is expected to output a summary based solely on that input. But this assumption has led to controversy for neuralbased solutions [17] since program behavior is determined by the interactions of different subroutines, and the information needed to understand a subroutine is very often encoded in the context around a subroutine instead of inside it [7,18,19,30].\n\nIn this paper, we present an enhancement to automatic summarization approaches of subroutines, by using the file context of the subroutines combined with the subroutines' code. By \"file context\" we mean the other subroutines in the same file. What we propose, in a nutshell, is to start with one of several existing approaches that models a subroutine's source code, then 1) model the signatures of all other subroutines in the same file also using recurrent nets, arXiv:2004.04881v1 [cs.SE] 10 Apr 2020 and 2) use an attention mechanism to learn associations between subroutines in the file context to words in the target subroutine's summary during training. Our idea is novel because existing approaches generally only attend words in the output summary to words in the target subroutine. Combined with other advancements that we will describe in this paper, our approach is able to learn much richer associations among words and produce better output summaries. In our experimental section, we demonstrate that our approach improves existing baselines by providing orthogonal information to help the model provide better predictions for otherwise difficult to understand subroutines.\n\n\nPROBLEM, SIGNIFICANCE, SCOPE\n\nThe problem we target is called \"source code summarization\", a term coined by Haiduc et al. [16] around 2009 to refer to the process of writing short, natural language descriptions (\"summaries\") of source code. We, along with a majority of related work (see Section 3.1), focus on summarization of subroutines because subroutine summaries have a high impact on the quality of documentation [14] and because the problem of code summarization of subroutines is analogous to Machine Translation (MT) for which there is a large potential for cross-pollination of ideas with the natural language processing research area, as a number of new interdisciplinary workshops and NSF-funded meetings have highlighted [11]. To encourage this cross-pollination of ideas and to maximize the reproducibility of our work, we focus on Java methods and use a dataset recently prepared by LeClair et al. for NAACL'19 [27]. However, in principle our work may apply to many programming languages in which subroutines are organized into files.\n\nYet despite crossover with NLP, this work is firmly in the field of Software Engineering. A very brief history of code summarization is that research efforts focused on manual rule-writing and heuristics until around 2016 when Neural Machine Translation was applied to source code and comments. The first NMT applications treated code summarization as essentially an MT problem: source code was the \"source\" language while summary comments were the \"target\", compared to an MT setting when e.g. French sentences were a source and equivalent e.g. English sentences the target. A trend since then has been to further define the problem in terms of software engineering domain-specific details. For example, marking up the source language with data from the abstract syntax tree of the methods [4,21] or modeling the code as a graph rather than a sequence [3]. This paper moves further in that direction, by using file context to improve summarization. Future work will likely continue this trend, with better results coming from better distinctions between source code summarization and machine translation.\n\nIt is important to recognize that we seek to enhance existing approaches, rather than compete with them. We present our approach as an augmentation to several existing baselines. Also, beyond the solution we present, a scholarship objective of this paper is to accelerate the community's progress by demystifying key aspects of using neural networks for code summarization. A frequent complaint about scientific literature and AI research specifically is that it is hard to reproduce and tends to treat the solutions as a black box [34]. We aim to push against that tendency. Therefore, we dedicate several discussions in this paper to justifying decisions and explaining results in detail.\n\n\nBACKGROUND & RELATED WORK\n\nThis section discussion key background items including related work from software engineering and natural language processing.\n\n\nSource Code Summarization\n\nAs mentioned above, source code summarization has a long history in software engineering literature, generally following the pattern of heuristic-based methods giving way to more recent data-driven methods. Specifically, there are five very closely related data-driven papers on source code summarization which we cover in detail.\n\nThe first of the five closely-related papers is by Iyer et al. [22] published in 2016. This work was one of the earliest to use a neural encoder-decoder architecture for code summarization. The work set the foundation for significant advancements, but in retrospect was a fairly straightforward application of off-the-shelf NMT technology: C# code was used as an input language and summary descriptions used as an output. The paper made several changes to the input during preprocessing in an attempt to maximize performance by focusing on the important parts of the code. In a sense this could be thought of as a bridge between pre-2016 heuristic-based approaches and later \"big data\" solutions: heuristics were used to select important words from code in preprocessing, then fed to an encoder-decoder system that, in it's overall structure, remained unchanged from NMT approaches for natural languages.\n\nHu et al. [21] in 2018 proposed an improvement using the Abstract Syntax Tree (AST) of the function. Their idea was that the AST should give more details about the code behavior than the words in the code alone, and therefore should lead to improved prediction of code summaries. However the problem they encountered was that a vast majority of encoder-decoder models at the time relied on sequence input, while the AST is a tree. Their solution was to design a Structure-Based Traversal (SBT) which is essentially an algorithm for flattening the AST into a sequence and using the components of the AST to annotate words from the code.\n\nNext, LeClair et al. [26] at ICSE 2019 observed that the approach by Hu et al. blended two very different types of information (structure from the AST and language from identifier names etc.) in the same input sequence, while in other research areas such as image captioning [20] it has been shown that better results are achieved when different input types are processed separately. Therefore, they designed a model architecture that processes the word sequence and SBT/AST sequences in separate recurrent nets with separate attention mechanisms, and concatenates the results into a context vector just prior to prediction.\n\nMeanwhile, Wan et al. [42] report improvements with a hybrid AST+code attention approach. They also show how to use reinforcement learning to improve performance by several percent. While we do not dispute that the RL-based approach helps, we do not use it this paper because our goal is to show how file context adds orthogonal information to AST+code approaches. The RL-based approach is more like an improved training procedure, as opposed to adding new information to the model. Ultimately, we used LeClair et al. [26]'s approach as a baseline because it is simpler (to reduce experimental variables) and slightly more recent.\n\nAt around the same time, Alon et al. [4] and Allamanis et al. [3] noted that while the AST is a useful addition to the model for prediction, it is not optimal to flatten the AST into a sequence, since that forces the model to learn tree structure information from a sequence. Yet these papers diverge substantially on the solution. Alon et al. extract a series of paths in the AST and treat each path as a different sequence, while Allamanis et al. propose using a graph-based neural network to model the source code. However, Allamanis et al. targeted generative models of the code itself. In this sense, Allamanis et al.'s work is representative of a variety of neural models for code representation [6,46,47].\n\nBeyond Allamanis et al.'s connection to code summarization with a recommendation of graph-based NNs, there is a diverse and growing body of work applying neural representations of code to several other problems such as commit message generation [23], pseudocode generation [33], and code search [15]. Due to space limitations we direct readers to peer-reviewed surveys by Chen et al. [8] and Allamanis et al. [2] as well as an online running survey [1]. Song et al. [38] and Nazar et al. [32] describe code summarization in detail, including heuristic-based techniques [30,39,40].\n\n\nNeural Encoder-Decoder Architecture\n\nThe key supporting technology for nearly all published data-driven code summarization techniques is the neural encoder-decoder attention architecture, designed for Neural Machine Translation (NMT). The origin of the architecture is Bahdanau et al. [5] in 2014. While the encoder-decoder design existed, that paper introduced attention, which drastically increased performance and launched several new threads of research. Since that time the number of papers using the attentional encoder-decoder design far exceeds what can be listed in one paper, with thousands of papers discussing overall improvements as well as adjustments for specific domains [10]. Nevertheless, this history has been chronicled in several surveys [36,37,45].\n\nWe include enough details in Section 4 to understand the basic encoder-decoder architecture and how our approach differs, but provide an overview here. In general, what \"encoder-decoder\" means is that an \"encoder\" is provided input data representing a source sentence or document, while the decoder is provided samples of desired output for that input. After sufficient training examples provided to the encoder and decoder, the model can often generate a reasonable output from the decoder given an input to the encoder. In the most basic setup, the encoder and decoder are both recurrent neural networks: the encoder RNN's output state is given as the initial state of the decoder. During training, the decoder learns to predict outputs one word at a time based on the encoder RNN's output state. Advancements in the encoder-decoder model usually focus on the encoder because the encoder is what models the input, and more accurate input modeling is likely to lead to more accurate predictions of output. For example, the encoder input RNN may be swapped for a graph-NN [3,9,43,44].\n\nThe encoder-decoder design has found uses in a very wide variety of applications beyond its origin for translation. Another application area relevant to this paper is document summarization, in which a paragraph or even several pages is condensed to one or two sentences. Typical strategies include representing each sentence in the document with an RNN, and selecting words from these sentences in the decoder [31]. From a very high level this strategy is relevant to our work, in that we model each function in a file with an RNN, though numerous important differences will become apparent in the next section.\n\n\nOUR MODEL\n\nIn this section, we present our prediction model. Note that we did not include certain optimizations such as pretrained word embeddings, multi-layer RNNs, etc. These optimizations are tangential to the main objective of this paper: to evaluate file context as an improvement for neural code summarization. Therefore we keep our model design simple to reduce experimental variables, while retaining the most important design features from related work.\n\n\nOverview\n\nThe model is based on the encoder-decoder architecture. As in related work, for each function, we have a source code/text input as well as an AST input to the encoder, plus a summary input to the decoder. But, we introduce a new input called \"file context\", which is the code/text from every other function in the same file. In this subsection, we discuss an overview visualized below:\n\nThe encoder can be thought of as modeling three types of input: code/text and the AST from a function, and the file context for that function. Likewise the decoder models one type of input: the summary. During training, a sample summary is provided to the decoder. During inference, the decoder receives the predicted summary so far, while the output prediction is the predicted next word in the summary (see the next subsection for training details).\n\nCode/Text We model code/text using a word embedding space and a recurrent network (area 2). This is the same design that has been successful in several related papers. The code/text is merely the sequence of tokens from the source code for the function.\n\nAST As discussed above, there are three ways related work models the AST: by flattening the tree to a sequence (Hu et al. and LeClair et al.), by using paths of the AST as separate sequences (Alon et al.), or by using a graph-NN (Allamanis et al.). We built our model so that any one of these may be used, and provide implementations for each. Later, our experiment will evaluate the effects of combining file context with each. We mention a few caveats in this section, though. First, we describe our implementation with the flattening technique shown by LeClair et al. (area 3) for simplicity and because, ultimately, we observed the highest BLEU scores in that configuration. Second, for the AST encoder by paths, we were only able to use 100 paths, which is at the lowest limit recommended by Alon et al. (they found optimal setting was 200) -when combining file context, we hit a known limit of CuDNN for large recurrent nets 1 . Third, using graph-NNs expands the network size by tens of millions of parameters, leading to limitations on speed and memory usage; we chose two \"hops\" in the graph-NN as a compromise.\n\nFile Context This input leads to the key novelty of our model. The input itself is, essentially, the code/text data for every other function in the same file as the function we are trying to summarize. The format of this input is an nxm matrix in which n is the number of other functions in the file and m is the number of tokens in the function (technically, we pad or truncate functions at m tokens so all input sequences are the same length, and select only the first n functions from each file if more then n are available). We model the input first by using a trainable word embedding space (area 1). Note that we share this embedding space with the code/text data -there is not a separate vector space for words in the code/text input and words in the functions in the file context. Note that this necessitates the use of the same vocabulary for both inputs, which has the effect of introducing more out-of-vocab words for the code/text input (since vocabulary is calculated as the x-most common words, and the file context distorts the word occurrence count in favor of words that occur in the file context). Yet in pilot studies we found an improvement of over 20% in BLEU score when sharing this space opposed to learning different embedding spaces, not to mention reduced model size and time to train. For reference, we used a code/text vocab size of 75k, and about 15% of this was displaced when recomputing the vocabulary with file context.\n\nNext (area 5), we use one RNN per function in the file context. This is similar to how function code/text is modeled (area 2), except that for file context we only use the final state of the RNN. For function code/text (area 2), we output the RNN state for every position in the code/text sequence -that way, we can compute attention for every position in the decoder (area 4) to every position in the code/text sequence (as described for NMT by Bahdanau et al. and implemented in a majority of code summarization papers). In contrast, for file context, we build a two dimensional matrix in which every column is a vector representing a function in the file context (the vector is the final state of the RNN for that function).\n\nAs mentioned, we calculate attention (area 6) for each encoder input to the decoder (\"summary\") input. For code/text and AST sequences, we compute attention as mentioned in the previous paragraph. However, for file context, we compute attention from each position in the summary to each function in the file. A metaphor to NMT is that the attention mechanism was originally designed for building a dictionary between words in one language to words in another language, by learning to emphasize the positions in the encoder and decoder sequences that have the same meaning. Essentially what this does is train the model to output a word in one language e.g. Hund when it sees e.g. dog in the input. Applied to our model for file context, the model learns to output words in code summaries when it sees functions relevant to those words. So for instance, if a function in the file context involves playing mp3 files, the model will be more likely to output words related to mp3s, music, audio, etc. In our evaluation (see RQ 2 ), we explore evidence of how our model actually does behave as we envision.\n\nTo create an output prediction, we concatenate the attentionadjusted matrices from all three encoder inputs and the decoder 1 https://github.com/tensorflow/tensorflow/issues/17009 input and use a fully-connected network (area 7) to learn how to combine the features from each input. This part of the model is similar to most encoder-decoder networks, and ultimately outputs a prediction for the next word in the summary.\n\n\nTraining Procedure\n\nWe train our model using the \"teacher forcing\" procedure described extensively in related work [12,13]. In short, this procedure involves learning to predict summaries one word at a time, while exposing the model only to the reference gold set summaries (as opposed to using the model's own predictions during training). So for example, for every function, we train the model by providing the three encoder inputs, plus the decoder summary input one word at a time, e.g. a sample: where < st > and < et > are start and end sentence tokens (for readability, we do not show padding). During training, the encoder would receive the encoder inputs, the decoder would receive the sequence so far (e.g. \"play mp3\"), and sample output prediction would be the correct next word (e.g. \"files\") whether or not the model actually would have succeeded in making that prediction.\n[\n\nCorpus Preparation\n\nWe used the corpus provided by LeClair et al. in a NAACL'19 paper of recommendations for code summarization datasets [27]. This corpus includes around 2m Java methods paired with summaries, already cleaned and split into training/validation/test sets according to a variety of recommendations. That paper evaluated four different splits and determined minimal variations in reported results after cleaning. For maximum reproducibility, we use \"split set 1\" from that paper. We do not use datasets from other papers because we could not verify that they followed the dataset recommendations such as removing auto-generated code.\n\nHowever, the corpus did not include file context (only code/text and AST) for each Java method. Therefore, we obtained from Lopes et al. [29] the raw data use by LeClair et al., and created the nxm file context matrices for each method (see paragraph 5 of Section 4.1). Note that each file context matrix did not include the function itself -only the other methods in the file. We included these methods even if they did not have summaries of their own. We filtered all comments and summaries from the file context so that it only included words from the code itself. In practice it may be desirable to include these comments, but we felt that including them would create a possibility that the model could see a very similar or even identical summary during training, and we decided to avoid the possibility of introducing this bias.\n\nNote that the size of n and m become important hyperparamters in our approach: n controls the number of functions per file, and m controls the number of tokens per function. Ideally both numbers would be very high, but hardware and software limitations require us to cap them. If n is too high, many functions will be included, but they will all be very short (low m). If m is too high, we will only be able to model a few functions per file. Ultimately we tested several values and found n=20, m=25 to provide a reasonable balance. Note that the average number of methods per file in the corpus was about 8, and n=20 covers all functions in over 97% of files. Interestingly, performance plateaus after m=25. The model appears to use the function signatures and first few tokens, but later parts of the function do not appear to be useful in the file context, at least with the type of RNNs we used in our implementation.\n\n\nModel Details\n\nInspired by successful examples set by previous work, we discuss our model details in the context of our implementation source code. We built this model in a framework provided at ICSE'19 [26], and is readable via file atfilecont.py in our fork of that framework (with a few minor edits for readability, see details Section 7). tdat_input = Input(shape=(self.tdatlen,)) sdat_input = Input(shape=(self.n, self.m)) ast_input = Input(shape=(self.astlen,)) com_input = Input(shape=(self.comlen,)) First, above, are the input layers corresponding to the code/text, file context, and AST for the encoder, plus the summary comment for the decoder. We discussed the n and m hyperparameters in the previous section. For the others, we chose tdatlen=50 for the maximum number of tokens in the code/text sequence, astlen=100 for the max tokens in the flattened AST sequence, and comlen=13 for the max words in the output summary. The parameters tdatlen and astlen are as recommended by LeClair et al. in their experiments and followup discussion with the authors, while comlen is limited by the available corpus. tdel = Embedding(output_dim=100, input_dim=75000)\n\nThis line creates the code/text embedding space. The space size is one 100-dimension vector for every 75k words. We chose this size as a compromise between performance and memory usage. tde = tdel(tdat_input) tenc = CuDNNGRU(256, return_state=True, return_sequences=True) tencout, tstate_h = tenc(tde)\n\nThis section corresponds to area 2 of the overview figure. Note that return_sequences is enabled, meaning that the variable tencout will contain a matrix of size 50 x 256: one 256-length vector for each of the 50 positions in the code/text sequence. Note that it is not typical for the RNN output dimensions (256 here) to exceed the word embedding vector length (100) for NMT applications, though we have repeatedly found a benefit in our pilot studies for code summarization tasks. de = Embedding(output_dim=100, input_dim=10908)(com_input) dec = CuDNNGRU(256, return_sequences=True) decout = dec(de, initial_state=tstate_h)\n\nNext is the decoder (area 4 in overview figure). The vocabulary size of 10908 is as provided in the corpus, though we note that it is less than the 44k reported by LeClair et al. in ICSE'19. The reason for the change seems to be a greatly increased training speed at minimal performance penalty, but it does mean that the results are not directly comparable to other papers -in our evaluation, we had to rerun the experiments with the new vocab size. ae = Embedding(output_dim=10, input_dim=100)(ast_input) ae_enc = CuDNNGRU(256, return_sequences=True) aeout = ae_enc(ae) This is the AST input portion of the decoder (area 3). Shown below is the SBT [21] flat AST technique, but in our experiments we swap this section for other AST encoders (see Section 4.1). The above is the attention mechanism for the code/text and AST inputs. This part is traditional attention between each position in the decoder input to each position in the code/text and AST inputs. semb = TimeDistributed(tdel) sde = semb(sdat_input)\n\nThese two lines begin our file context portion of the model (area 5). Basically what happens is that one 25x100 matrix is generated for every function in the file context: that is, one 100-dimension vector for every one of the 25 (hyperparameter m) words in each function sequence. The 100-dimension vectors are from the word embedding space shared with the code/text input (variable tdel). There is one 25x100 matrix for each of the 20 (hyperparameter n) functions in the file, resulting in a 20x25x100 matrix as sde. senc = TimeDistributed(CuDNNGRU(256)) senc = senc(sde)\n\nNext we create one RNN for each of the 20 functions. Each RNN will receive a 25x100 matrix: 25 positions of 100-dimension word vectors. Note that we also built a custom TimeDistributed layer in which we passed tstate_h as the initial state for each RNN (as it is used for the decoder), but we noticed only minuscule performance differences and removed it for simplicity. The size of senc is 20x256: one 256-length vector representing each of the 20 functions. The attention mechanism for file context looks similar to the code/text and AST inputs, but has a very different meaning. Variable sattn is the result of the dot product of decout and senc. Consider the following multiplication ( In the above, vector v1 is the 256-dimension vector representing the first position in the decoder RNN. The vector v3 is the 256dimension vector representing the first function in the file context. Value a is a measure of similarity between those two vectors. Vectors that are more similar will have a higher sattn matrix.\n\nThis similarity is important because we multiply sattn with senc: Vector v5 is a list of similarities of position 1 in the summary to different functions in the file context. E.g., element 3 of v5 is the similarity of position 1 in the summary to function 3 in the file. In contrast, vector v7 contains all the first elements of the 256-dimension vectors representing different functions. When each element of v5 is multiplied to the corresponding element in v7, the effect is to scale the element in v7 by the similarity represented in v5. So if position 1 of the summary is very similar to function 3 of the file while not similar to other functions (i.e. element 3 of v5 is high while other elements in v5 are low), then that position will be retained while others attenuated.\n\nNote above that we apply a softmax activation to sattn, so each of the vectors in that matrix (such as v5) will sum to one. If, for example, element 3 in v5 is 0.90 and all others sum to 0.10, then it means that the product of the multiplication of v5 and v7 will include 90% of the value of element 3, and only 10% of the value of all other elements. That product is the value e: We concatenate the context matrices from each attention mechanism (and the decoder) into a single context matrix along axis 1, to create a matrix of size 13x1024 (since each smaller context matrix is 13x256). We then use one fully-connected layer of size 256 to squash the 1024 matrix into a lower dimension. This is common in encoder-decoder architectures in order to prevent overfitting, though it serves an additional purpose in our model of helping the model learn how to combine the information from each of the three encoder inputs. squash = Flatten()(squash) out = Dense(10908, activation=\"softmax\")(squash) Finally, we flatten the 13x256 \"squashed\" context matrix into a single 3328-dimension vector so that we can connect it to a fullyconnected output layer. The output layer size is the vocabulary size, and the argmax of this output layer is the index of the predicted next word in the vocabulary. Note that a large number of parameters occur between the distributed dense layer and the output layer (3328 to 10908 elements is over 36m parameters). Significant time could be gained by reducing the vocab size or the size of the \"squash\" layer (the 1024 could be squashed to, say, 128 instead of 256), but at unknown cost to performance. In the end, we keep these values consistent across all approaches in our experiments, to ensure an \"apples to apples\" comparison, even if optimizations could be made depending on user circumstances.\n\n\nHardware/Software Details\n\nOur hardware included two workstations with Xeon E1530v4 CPUs, 128gb RAM, and dual Quadro P5000 GPUs. Software platform included Ubuntu 18.04.2, Tensorflow 1.14, CUDA 10.0, and CuDNN 7. The implementation above is for Keras 2.2.4 in Python 3.6.\n\n\nEXPERIMENT DESIGN\n\nThis section discusses the design of our experiment including research questions, methodology, and other conditions.\n\n\nResearch Questions\n\nOur research objective is to evaluate whether the our proposed mechanism for including file context in code summarization improves strong, recent baselines. In particular, we aim to establish whether any improvement can be attributed to the file context, so we aim to reduce the number and effect of other factors. We ask the following Research Questions (RQs): RQ 1 What is the performance of our proposed approach compared to recent baselines in terms of quantitative metrics in a standardized dataset? RQ 2 To what extent can differences in performance be attributed to the inclusion of file context?\n\nThe rationale for RQ 1 is straightforward: to compare our approach to existing approaches. The scope of this question is to compare our work to relevant data-driven technologies, as opposed to heuristic-based/template solutions. Generally speaking, it would not be a \"fair\" comparison for heuristic-based techniques because a heuristic could produce a valid summary that would not be anything like the reference solution. The way to evaluate a heuristic approach is with a human study, but the scale of the dataset (e.g. over 90k samples in the test set) is much too large. Therefore we follow precedent set in both the SE and NLP research areas, and use quantitative metrics to evaluate performance in broad terms over the whole test set.\n\nHowever, there are many factors that can affect performance between one data-driven approach and another. For example, the choice of exactly which type of recurrent unit to use (e.g. LSTM vs GRU, or uni-directional vs bi-directional) or the number of units in a hidden layer. We control as many of these factors as possible by configuring the approaches in as similar a way as reasonable (see Section 5.3), but there is always a question as to whether a proposed variable is actually the dominant one. Therefore, we ask RQ 2 to study how file context contributed to predictions in the model.\n\n\nMethodology\n\nTo answer RQ 1 , we follow the methodology established by many papers on code summarization in both SE and machine translation in NLP. We obtain a standard dataset (see Section 4.3), then use several baselines plus our approach to create predictions on the dataset, then compute quantitative metrics for the output. For our approach, we trained for ten epochs and selected the model that had the best accuracy score on the validation set. Unless otherwise stated in Section 5.3, this is also how we trained the baselines.\n\nThe quantitative metrics we use are BLEU [35] and ROUGE [28]. These two metrics have various advantages and disadvantages, but one or another form the foundation of nearly all experiments on neural-based translation or summarization. Both are basically measures of similarity between a predicted sentence and a reference. BLEU creates a score based on matches of unigrams, bigrams, 3grams, and 4-grams of words in the sentences. ROUGE encompasses a variety of metrics such as gram matches and subsequences. We report BLEU (1-4) as well as ROUGE-LCS (longest common subsequence) to provide good coverage of metrics without redundancy. In cases when the reference is only three words long (the dataset has a minimum summary length of three words), we calculate only BLEU 1-3 and do not include BLEU 4 in the total BLEU score for that sentence, because otherwise the total BLEU score would be zero even for correct three-word predictions.\n\nWe answer RQ 2 in three stages. First, we provide an overview comparison of predictions by different models to determine whether the inputs (code/text, AST, file context) give orthogonal results and estimate the proportion of predictions may be helped by file context. Second, we extract specific examples to illustrate how the approach works in practice. Whenever possible, we support our claims with quantitative evidence, to minimize the potential of bias.\n\nOne key piece of evidence in the examples is from the attention layer to file context (area 6 in the overview figure, scontext in Section 4.4). The attention layer shows which of the functions in the file context are contributing the most to the prediction. By showing that a prediction is improved when when a particular function is attended, we can demonstrate that the file context is the most likely explanation for the improvement. Finally, we explore evidence that it is prevalent for file context to improve predictions in a similar way as the example, and perform an ablation study in which we train and test the model using only AST and file context.\n\n\nBaselines\n\nWe use five baselines. We chose these baselines because 1) they are recent, 2) they represent both code-only and AST+code neural approaches that our approach is capable of enhancing, and 3) they had reproducibility packages. Space limitations prevent us from listing all relevant details, so we provide complete implementations in our online appendix (Section 7). attendgru This baseline represents a \"no frills\" attentional encoderdecoder model using unidirectional GRUs. It represents approaches such as Iyer et al. [22], but this implementation was published by LeClair et al. [27]. We configure attendgru with identical hyperparameters to our approach whenever they overlap (e.g. word embedding vector size).\n\nast-attendgru This is the approach LeClair et al. [26] propose, using their recommended hyperparameters. This approach is an enhancement of SBT [21], so we only compare against this approach.\n\ntransformer Vaswani et al. [41] proposed an attention-only (no recurrent steps) machine translation model in 2017. It was received in the NLP community with significant fanfare so, given the success of the model for NMT, we evaluate it as a baseline. graph2seq Allamanis et al. [3] proposed using a graph-NN to model source code, but applied it to a code generation task in their implementation. To reproduce the idea as a baseline for code summarization, we use a graph-NN-based text generation system proposed by Xu et al. [43]. We use the nodes and edges of the AST as input, with all other configuration parameters as recommended for NMT tasks by Xu et al.. code2seq Alon et al. [4], described in Section 3.1, is a recent code summarization approach based on AST paths that is reported to have good results on a C# dataset. We reimplemented the approach from Section 3.2 of their paper, with their online implementation as a guide. Note we did not use their implementation verbatim. They had many other architecture variations, preprocessing, etc., that we had to remove to control experimental variables. Otherwise, it would not have been possible to know whether performance differences were due to file context or these other factors.\n\n\nEXPERIMENT RESULTS\n\nWe present our experimental findings and answer our research questions in this section. Recall that our research objective is to evaluate the effects of adding our file context encoding, and to this end we present a mixture of high level quantitative data and specific examples and qualitative explanations. Recall that we do not view our results as in competition with the baselines, but instead as a complementary attempt at improvement.\n\n\nRQ 1 : Quantitative Measures\n\nOur key finding in answering RQ 1 is that, in terms of quantitative measures over the whole test set, adding our file context encoder increased performance in nearly all cases. Figure 1 showcases the difference when compared with aggregate BLEU score (column BLEU-A of Table 1). The baselines attendgru and ast-attendgru improved by more than one full BLEU point, while graph2seq and code2seq improved by around 0.3 BLEU. One possible explanation for the greater increase for attendgru and ast-attendgru is that the path-and graph-based AST encoder models have many millions more parameters than the flattened AST approach (not to mention, when there is no AST encoder), and the model may have difficulty retaining some of the details in these larger encoders in the \"squash\" layer of the model (area 7 of overview figure in Section 4.1 and paragraph 13 of Section 4.4). Recall that an overriding objective in our experimental setup is to reduce variables to create an \"apples to apples\" comparison. For that reason, we used fully-connected networks of 256 dimensions for all approaches in the squash layer. The result could be that the model is able to learn more details about the flattened AST encoder in the squash layer simply because of there are many fewer parameters in that encoder, while we are in effect asking the model to remember much more information in that layer in the path-and graph-based encoders. Our recommendation for future work is to designate the size of the squash layer as a  Table 1 column BLEU-A). Orange indicates increase in BLEU score for identical model with file context encoder added. Note y-axis starts at 17 BLEU. Table 1: Performance summary in terms of BLEU aggregate (A) and 1-4, plus ROUGE Longest Common Subsequence precision, recall, and F1 measure. Except for Transformer, all implementations were identical except for different AST and file context encoders. For example, the ast-attendgru model included the RNN-based code/text encoder and the flattened AST encoder, and achieved 18.69 aggregate BLEU score. Key takeaway is that the \"FC\" file context models obtained higher performance than their default \"non FC\" counterparts. BLEU-A is bold because it usually serves as the most important performance metric. hyperparameter for tuning, perhaps with larger settings for larger encoders. We do not recommend concluding from this experiment alone that a flat AST encoder is the best design. Instead, we confine ourselves to the conclusion that adding our file context encoder to the baselines improves the performance of those baselines. An exception to the overall observation of improved performance with the file context encoder is that the graph2seq model obtain a slightly lower ROUGE-LCS F1 score, despite a higher aggregate BLEU score, when we added the file context encoder. The way to interpret ROUGE-LCS is that precision and recall are calculated only for words in a predicted sentence that appear in sequence when compared to a reference sentence. For example, consider prediction \"converts the file from mp3 to wav\" and reference \"converts the file from wav to mp3\". The summaries have very different meanings, but the BLEU1-3 scores will be fairly high and will inflate the aggregate BLEU score. In contrast, ROUGE-LCS will result in precision and recall scores of 57% (4/7 for longest common subsequence divided by sentence lengths, see Section 3 of Lin et al. [28] for formulae)the result is a score much more based on proper word order instead of predicting correct words anywhere in the sentence. So, it appears that the predictions from graph2seq have slightly longer common subsequences with the references, without the file context encoder.\n\nWe made several related but tangential observations. We found that the transformer model performed quite poorly on this software dataset, despite reports of excellent performance at low cost on natural language translation [41,45]. While it is tempting to draw sweeping conclusions from this finding, we mention it only to recommend caution in applying NMT solutions to this problem. Evidence is accumulating in related literature that code summarization is not merely an application of off-the-shelf technology borrowed from the NLP domain [17]. Another tangential observation is that we verify conclusions by related work, namely Alon et al. [4] that a path-based AST encoder outperforms most alternatives (code2seq was the highest performer without file context), and Hu et al. [21] and LeClair et al. [26] that a flat AST encoder design outperforms traditional seq2seq encoder-decoder designs (i.e. attendgru). Finally, we note that the AST-based models have broadly similar performance (18.61 -18.84 BLEU), while the contribution of file context varies much more (18.88 -19.95 BLEU).\n\n\nRQ 2 : Effects of File Context\n\nWe explore the effects of file context in three ways: First we offer a bird's eye view of results. Second we present specific examples demonstrating how the model works. Third we provide evidence that the examples are representative of the model's behavior.\n\nOverview Consider Figure 2(a) which shows a breakdown of BLEU1 scores for predictions by attendgru. (We show BLEU 1 scores for simplicity of comparison, BLEU 1 is equivalent to unigram precision.) There is a significant portion in which attendgru performs quite well, with BLEU1 scores sometimes even nearing 100. This is not surprising since in some cases, the source code of the function \"gives away\" the summary e.g. in functions with names like convertMp3ToWav. Yet for 46% of predictions, BLEU1 score is less than 25, meaning that not even 25% of the words in the predictions are correct. This is also not surprising, since in many cases the code has almost no clues as to what words should be in the summary.\n\nNext consider Figure 2(b). This chart shows the percent of functions in which the predictions had the highest BLEU1 score (of predictions >25 BLEU1) for three different models: attendgru relies only on code/text, ast-attendgru is the same model but with access to AST information, and attendgru+FC is the same model but with access to file context (but no AST). What we observe is that there is a subset of functions for which each model seems to perform best -it is not as through the models provide uniformly better results on all functions. Related work e.g. LeClair et al. [26] showed how AST-based models can improve predictions for functions in which the structure contains clues about the function's behavior, even if the source code contains no useful words. We make a similar observation for file context. The attendgru+FC model performs best for a subset of around 22% of functions (and ties with ast-attendgru for 4% of functions). When combined, a major portion of the improvement comes from reducing the number of low quality predictions (visible in a reduced number of <25 BLEU1 scores) in addition to improving many of the predictions of the baselines. The result is the overall improvement in BLEU scores reported for RQ 1 for models that combine many types of input data such as ast-attendgru+FC.\n\nExamples Below are two examples showing how using file context improves predictions. We chose these examples based on explanatory power: they are short methods in which ast-attendgru and ast-attendgru+FC differed by one word (there are 109 such examples out of 6945 where ast-attendgru+FC outperformed ast-attendgru over the 90908 methods in the test set).  public int get num available seats return num available seats 10 public void set num available seats int num available seats 11 public int get num seats return num seats 12 public void set num seats int num seats Example 1: (upper) Source code, summaries, and predictions. (mid) activation map of the attention matrix in ast-attendgru+FC from summary to files just prior to predicting position 8, the word \"flight\". The x-axis is the position in the summary vector. The y-axis is the function in the file. (lower) Finally, inputs to the file context matrix.\n\nIn Example 1, it is obvious from the code that the method is just a setter, but there is no hope to understand what the value means even with the AST, and the baselines output an unknown token. But, the file context reveals several keywords e.g. flight, airline, departure that serve as clues. The attention matrix shows that the model found these clues (the image shows a heatmap of the values in sattn in Section 4.4 just prior to predicting the last word). High activation is visible connecting the later positions in the output prediction to function 3, which contains the word \"flight. \" The effect of high activation on this function is that the context vector (which is a concatenation of the attention matrices) will be much closer in vector space to the words related to flights, airlines, etc. in the word embedding, than it would be with only the code/text and AST. This makes it much easier for the model to predict the correct word.\n\nExample 2 is similar, except that the word that is different in the predictions (\"exception\" vs \"object\") is not directly in the file context. Once again the source code gives few clues except that it returns a string representation of something. The baselines give the term \"object\" which is a reasonable guess but not specific. The attention matrix shows high activation on the three functions which contain the term \"stack trace\", which is nearby to \"exception\" in the word embedding (technically \"stack trace\" is two words, but since the function representation is an RNN, the final state will contain information from both words).\n\nPrevalence The examples above show how file context can improve predictions in specific cases, but we chose these examples based on explanatory power and not necessarily prevalence. In fact, most of the predictions are more complicated (most predictions differed by more than one word, and most methods are larger). Consider that there were 6945 methods out of 90908 in the test set where ast-attendgru+FC outperformed ast-attendgru in terms of aggregate BLEU score. Of these, 5093 (73%) had words in the reference summary that were in the file context but not the source code of the method; ast-attendgru+FC correctly used these words in the output predictions for 4369 (86%). Put another way, there were 4369/90908 (about 5%) methods in the test set where ast-attendgru failed to find the correct word, but ast-attendgru+FC did find the correct word, and that word was in the file context. Improvement over this 5% largely explains the increase in aggregate BLEU score from 18.69 for ast-attendgru to 19.95 for ast-attendgru+FC (+6.7%). However, in practice, the file context sometimes led the model astray. Of the 90908 test set methods, ast-attendgru and ast-attendgru+FC tied the aggregate BLEU score 78682 times. For the 12226 times they differed, as mentioned, 6945 times ast-attendgru+FC outperformed ast-attendgru while 5281 times it underperformed. Of these, 2761 (52%) occurred when a ast-attendgru+FC picked a word from the file context when the reference did not contain that word.\n\nAs a final piece of evidence studying the effect of file context, we perform an ablation study in which we train and test two models in an extreme condition: when zero code/text data are available. Basically we train and test with AST and file context only, code/text sequences are set to zeros. Ablation studies are common in NMT research to determine whether a given input is benefiting a model [25]. Our study is akin to the challenge experiment proposed by LeClair et al. [26], but differs in that our intent is to explore file context rather than simulate code obfuscation. For brevity, we present only BLEU score values for ast-attendgru and ast-attendgru+FC: The ast-attendgru model had only the AST from which to base predictions. As shown in the prediction for Example 1 below, it identified that the method sets some value (because it receives a parameter and sets a class property to that parameter's value), but without any text it only predicts an unknown token. On the other hand, ast-attendgru+FC guesses terms from the file context such as \"departure airport. \" While not technically a correct prediction, this example illustrates how the file context can serve as an alternative to text information in the source code of the method. Overall, the aggregate BLEU score increases from 8.51 to 11.31 (33%). This difference is almost certainly due to the file context, since the code/text data are ablated and all other details of the model are identical.\n\nreference sets the intermediate value for this flight ast-attendgru sets the <UNK> value for this <UNK> ast-attendgru+FC sets the departure airport value for this flight 7 DISCUSSION / CONCLUSION This paper advances the state of the art by demonstrating how file context can be used to improve neural source code summarization. The idea that file context includes important clues for understanding subroutines is well-studied in software engineering and especially program comprehension -it has even been proposed for code summarization [18,30]. Yet the question is how to make use of those clues in practice. In a nutshell, our approach is to encode in a recurrent network every subroutine in the same file as the subroutine we are trying to summarize. The result is that the model is able to use more words from that context, as in the following examples (for maximum reproducibility, number is method ID in the dataset followed by the reference summary, attention visualizations and detailed explanation are in our online appendix):\n\n\n14624250\n\ntest of decode nmea method of class org ast-ag.\n\ntest of decode <UNK> ast-ag.+FC test of decode nmea system method of class <UNK> 51016854 returns the weight of the given edge ast-ag.\n\nreturns the weight of the attribute ast-ag.+FC returns the weight of the given edge 37563332 adds the given rectangle to the collection of polygons ast-ag.\n\nadds the given x y to the collection of values ast-ag.+FC adds the given rectangle to the collection of polygons 37563423 returns the height of the font described by the receiver ast-ag.\n\nreturns the height of the window ast-ag.+FC returns the height of the font As with all papers, our work carries threats to validity and limitations. For one, we use a quantitative evaluation methodology, which while in line with almost all related work and which enables us to evaluate the approach over tens of thousands of subroutines, may miss nuances that a qualitative human evaluation would catch. However, space limitations prevent us from including a thorough discussion of both in a single paper, so we defer a human evaluation for extended work. Another limitation is that we evaluate only against a Java dataset. This dataset is the largest available that follows good practice to avoid biases [27], but caution is advisable when generalizing the work to other languages.\n\nStill, we demonstrate that an advantage to our approach is that it improves predictions in a way that is orthogonal to existing approaches, so it can be applied to a variety of existing solutions.A feature of our experiment is that we simplified and reimplemented baselines in order to create a controlled environment for evaluation -several of these baselines had modifications unrelated to software engineering such as subtoken encoding, ensemble methods, or different RNN architectures, and it was necessary to eliminate these as factors in experimental outcome. An optimistic sign for future work is that performance reported in this paper could rise further when our approach is combined with these modifications.\n\nReproducibility. We release our implementation and supporting scripts via an online appendix / repository: http://www.github.com/Attn-to-FC/Attn-to-FC\n\n\nACKNOWLEDGMENTS\n\nThis work is supported in part by NSF CCF-1452959 and CCF-1717607. Any opinions, findings, and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors.\n\n\nsattn = dot([decout, senc], axes=[2, 2]) sattn = Activation('softmax')(sattn) scontext = dot([sattn, senc], axes=[2, 1])\n\n\nit will be dominated by the values of v7 (first positions of the 256-dimension vectors for the functions) that are most similar to position 1 in the decoder. Likewise, f will be dominated by values of v8 (second positions of the function vectors) most similar to position 1 of the decoder, etc. In this way, we create a context matrix scontext which includes one 256-dimension vector for each of the 13 decoder positionseach of these vectors represents the functions most relevant to the word in that position of the summary. context = concatenate([scontext, tcontext, acontext, decout]) squash = TimeDistributed(Dense(256, activ=\"relu\"))(context)\n\nFigure 1 :\n1Comparison of baselines to baselines including our file context encoder. Dark blue indicates baseline aggregate BLEU score (compare to\n\nFigure 2 :\n2Comparison of predictions from models with code/text only (attendgru), code/text+AST (ast-attendgru), and code/text+FC (attendgru+FC). Details in Section 6.2.\n\n\nset airline name java lang string airline name this ... 2 public void set destination java lang string destination this ... 3 public long get flight id return flight id 4 public void set flight id long flight id this flight id flight id 5 public void set flight number java lang string flight number this ... 6 public void set intermediate java lang string intermediate ... 7 public void set intermediate arrival time java lang string ... 8 public void set intermediate departure time java lang string ... 9\n\n\nencoder inputs ] => play mp3 files would become during training four separate samples: [ encoder inputs ] => <st> + ( play ) [ encoder inputs ] => <st> play + ( mp3 ) [ encoder inputs ] => <st> play mp3 + ( files ) [ encoder inputs ] => <st> play mp3 files + ( <et> )\n\n\ntable format courtesy [26]):decout (axis 2) \nsenc (axis 2) \nsattn \n1 2 .. 256 \n1 v1 \u2212\u2192 \n2 v2 \u2212\u2192 \n.. \n13 \n\n* \n\n1 2 .. 20 \n1 v3v4 \n2 \u2193 \u2193 \n.. \n256 \n\n= \n\n1 2 .. 20 \n1 a b \n2 c d \n.. \n13 \n\n\n\n\npublic String toString() { if (throwable != null) return super.toString() + System.getProperty(\"line.separator\") + throwable.toString(); return super.toString(); } reference returns a string representation of this exception attendgru returns a string representation of this object ast-attendgru returns a string representation of this object ast-attendgru+FC returns a string representation of this exception public void print stack trace if throwable null throwable print ... 4 public void print stack trace print stream s if throwable null ... 5 public void print stack trace print writer s if throwable null ... 6 public string to string if throwable null return super to string ... Example 2: Setup similar to Example 1. Note significant attention to functions 3, 4, and 5 in file context, which contain the term \"stack trace.\"<st> \n1 \nreturns \n2 \na \n3 \n\nstring \n4 \nrepresentation 5 \nof \n6 \nthis \n7 \n\n\uf8e6 \n\uf8e6 \n\uf8e6 \n\uf8e6 \n\npredicting \nnext word \n\n8 \n9 \n\n10 \n11 \n12 \n13 \n\n1 2 3 4 5 6 \n\n1 \npublic void set throwable throwable a throwable this throwable ... \n2 \npublic throwable get throwable return throwable \n3 \n\n\nMachine Learning for Big Code and Naturalness. Miltos Allamanis, Miltos Allamanis. 2019. Machine Learning for Big Code and Naturalness. https: //ml4code.github.io/papers.html\n\nA survey of machine learning for big code and naturalness. Miltiadis Allamanis, T Earl, Premkumar Barr, Charles Devanbu, Sutton, arXiv:1709.06182arXiv preprintMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2017. A survey of machine learning for big code and naturalness. arXiv preprint arXiv:1709.06182 (2017).\n\nLearning to represent programs with graphs. Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi, International Conference on Learning Representations. Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learn- ing to represent programs with graphs. International Conference on Learning Representations (2018).\n\nUri Alon, Shaked Brody, Omer Levy, Eran Yahav, Generating sequences from structured representations of code. International Conference on Learning Representations. 2Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating sequences from structured representations of code. International Conference on Learning Representations (2019).\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma- chine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).\n\nDeep-Coder: Learning to Write Programs. M Balog, A L Gaunt, S Brockschmidt, D Nowozin, Tarlow, International Conference on Learning Representations. OpenReview. netM Balog, AL Gaunt, M Brockschmidt, S Nowozin, and D Tarlow. 2017. Deep- Coder: Learning to Write Programs. In International Conference on Learning Representations (ICLR 2017). OpenReview. net.\n\nThe concept assignment problem in program understanding. J Ted, Bharat G Biggerstaff, Dallas Mitbander, Webster, Proceedings of the 15th international conference on Software Engineering. the 15th international conference on Software EngineeringIEEE Computer Society PressTed J Biggerstaff, Bharat G Mitbander, and Dallas Webster. 1993. The concept assignment problem in program understanding. In Proceedings of the 15th in- ternational conference on Software Engineering. IEEE Computer Society Press, 482-498.\n\nThe Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. Orhan Mia Xu Chen, Ankur Firat, Melvin Bapna, Wolfgang Johnson, George Macherey, Llion Foster, Mike Jones, Noam Schuster, Niki Shazeer, Parmar, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, et al. 2018. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 76-86.\n\nReinforcement learning based graph-to-sequence model for natural question generation. Yu Chen, Lingfei Wu, Mohammed J Zaki, International Conference on Learning Representations. Yu Chen, Lingfei Wu, and Mohammed J Zaki. 2020. Reinforcement learning based graph-to-sequence model for natural question generation. International Conference on Learning Representations (2020).\n\nA Survey of Domain Adaptation for Neural Machine Translation. Chenhui Chu, Rui Wang, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsChenhui Chu and Rui Wang. 2018. A Survey of Domain Adaptation for Neu- ral Machine Translation. In Proceedings of the 27th International Conference on Computational Linguistics. 1304-1319.\n\nWilliam Cohen, Prem Devanbu, Workshop on NLP for Software Engineering. William Cohen and Prem Devanbu. 2018. Workshop on NLP for Software Engi- neering. https://nl4se.github.io/\n\nBifurcations in the learning of recurrent neural networks. Kenji Doya, IEEE International Symposium on Circuits and Systems. IEEE6In [ProceedingsKenji Doya. 1992. Bifurcations in the learning of recurrent neural networks. In [Proceedings] 1992 IEEE International Symposium on Circuits and Systems, Vol. 6. IEEE, 2777-2780.\n\nRecurrent networks: learning algorithms. The Handbook of Brain Theory and Neural Networks. Kenji Doya, Kenji Doya. 2003. Recurrent networks: learning algorithms. The Handbook of Brain Theory and Neural Networks, (2003), 955-960.\n\nThe relevance of software documentation, tools and technologies: a survey. Andrew Forward, C Timothy, Lethbridge, Proceedings of the 2002 ACM symposium on Document engineering. the 2002 ACM symposium on Document engineeringACMAndrew Forward and Timothy C Lethbridge. 2002. The relevance of software documentation, tools and technologies: a survey. In Proceedings of the 2002 ACM symposium on Document engineering. ACM, 26-33.\n\nDeep code search. Xiaodong Gu, Hongyu Zhang, Sunghun Kim, Proceedings of the 40th International Conference on Software Engineering. the 40th International Conference on Software EngineeringACMXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In Proceedings of the 40th International Conference on Software Engineering. ACM, 933-944.\n\nOn the use of automated text summarization techniques for summarizing source code. Sonia Haiduc, Jairo Aponte, Laura Moreno, Andrian Marcus, Reverse Engineering (WCRE), 2010 17th Working Conference on. IEEESonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the use of automated text summarization techniques for summarizing source code. In Reverse Engineering (WCRE), 2010 17th Working Conference on. IEEE, 35-44.\n\nAre deep neural networks the best choice for modeling source code. J Vincent, Premkumar Hellendoorn, Devanbu, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringACMVincent J Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks the best choice for modeling source code?. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. ACM, 763-773.\n\nAutomatically capturing source code context of nl-queries for software maintenance and reuse. Emily Hill, Lori Pollock, K Vijay-Shanker, Proceedings of the 31st International Conference on Software Engineering. the 31st International Conference on Software EngineeringIEEE Computer SocietyEmily Hill, Lori Pollock, and K Vijay-Shanker. 2009. Automatically capturing source code context of nl-queries for software maintenance and reuse. In Proceed- ings of the 31st International Conference on Software Engineering. IEEE Computer Society, 232-242.\n\nUsing structural context to recommend source code examples. Reid Holmes, C Gail, Murphy, Proceedings. 27th International Conference on Software Engineering. 27th International Conference on Software EngineeringIEEEReid Holmes and Gail C Murphy. 2005. Using structural context to recommend source code examples. In Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005. IEEE, 117-125.\n\nA comprehensive survey of deep learning for image captioning. Ferdous Md Hossain, Sohel, ACM Computing Surveys (CSUR). 51118Mohd Fairuz Shiratuddin, and Hamid LagaMD Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. A comprehensive survey of deep learning for image captioning. ACM Computing Surveys (CSUR) 51, 6 (2019), 118.\n\nDeep code comment generation. Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, Proceedings of the 26th International Conference on Program Comprehension. the 26th International Conference on Program ComprehensionACMXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment genera- tion. In Proceedings of the 26th International Conference on Program Comprehension. ACM, 200-210.\n\nSummarizing source code using a neural attention model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics1Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vol. 1. 2073-2083.\n\nAutomatically generating commit messages from diffs using neural machine translation. Siyuan Jiang, Ameer Armaly, Collin Mcmillan, Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. the 32nd IEEE/ACM International Conference on Automated Software EngineeringIEEE PressSiyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat- ing commit messages from diffs using neural machine translation. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. IEEE Press, 135-146.\n\nAPI documentation from source code comments: a case study of Javadoc. Douglas Kramer, Proceedings of the 17th annual international conference on Computer documentation. the 17th annual international conference on Computer documentationACMDouglas Kramer. 1999. API documentation from source code comments: a case study of Javadoc. In Proceedings of the 17th annual international conference on Computer documentation. ACM, 147-153.\n\nWhat Do Recurrent Neural Network Grammars Learn About Syntax. Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, Noah A Smith, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European Chapter1Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, and Noah A Smith. 2017. What Do Recurrent Neural Network Grammars Learn About Syntax?. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 1249-1258.\n\nA neural model for generating natural language summaries of program subroutines. Alexander Leclair, Siyuan Jiang, Collin Mcmillan, Proceedings of the 41st International Conference on Software Engineering. the 41st International Conference on Software EngineeringIEEE PressAlexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. In Proceedings of the 41st International Conference on Software Engineering. IEEE Press, 795-806.\n\nRecommendations for Datasets for Source Code Summarization. Alexander Leclair, Collin Mcmillan, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong and Short Papers1Alexander LeClair and Collin McMillan. 2019. Recommendations for Datasets for Source Code Summarization. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 3931-3937.\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out (2004).\n\nUCI Source Code Data Sets. C Lopes, S Bajracharya, J Ossher, P Baldi, C. Lopes, S. Bajracharya, J. Ossher, and P. Baldi. 2010. UCI Source Code Data Sets. http://www.ics.uci.edu/$\\sim$lopes/datasets/\n\nAutomatic source code summarization of context for java methods. W Paul, Collin Mcburney, Mcmillan, IEEE Transactions on Software Engineering. 42Paul W McBurney and Collin McMillan. 2016. Automatic source code summa- rization of context for java methods. IEEE Transactions on Software Engineering 42, 2 (2016), 103-119.\n\nAbstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. Ramesh Nallapati, Bowen Zhou, Caglar Cicero Dos Santos, Bing Gulcehre, Xiang, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. The 20th SIGNLL Conference on Computational Natural Language LearningRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. 280-290.\n\nSummarizing software artifacts: A literature review. Najam Nazar, Yan Hu, He Jiang, Journal of Computer Science and Technology. 31Najam Nazar, Yan Hu, and He Jiang. 2016. Summarizing software artifacts: A literature review. Journal of Computer Science and Technology 31, 5 (2016), 883-909.\n\nLearning to generate pseudo-code from source code using statistical machine translation (t). Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, 30th IEEE/ACM International Conference on. IEEE. Automated Software Engineering (ASE)Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on. IEEE, 574- 584.\n\nIlluminating the \u00e2\u0102\u0132black box\u00e2\u0102\u0130: a randomization approach for understanding variable contributions in artificial neural networks. D Julian, Donald A Olden, Jackson, Ecological modelling. 154Julian D Olden and Donald A Jackson. 2002. Illuminating the \u00e2\u0102\u0132black box\u00e2\u0102\u0130: a randomization approach for understanding variable contributions in artificial neural networks. Ecological modelling 154, 1-2 (2002), 135-150.\n\nBLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL '02). the 40th Annual Meeting on Association for Computational Linguistics (ACL '02)Stroudsburg, PA, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL '02). Association for Computational Linguistics, Stroudsburg, PA, USA, 311-318. https://doi.org/10.3115/1073083.1073135\n\nA survey on deep learning: Algorithms, techniques, and applications. Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, S S Iyengar, ACM Computing Surveys (CSUR). 5192Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and SS Iyengar. 2018. A survey on deep learning: Algorithms, techniques, and applications. ACM Computing Surveys (CSUR) 51, 5 (2018), 92.\n\nReview of Deep Learning Algorithms and Architectures. Ajay Shrestha, Ausif Mahmood, IEEE Access. 7Ajay Shrestha and Ausif Mahmood. 2019. Review of Deep Learning Algorithms and Architectures. IEEE Access 7 (2019), 53040-53065.\n\nA Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques. Xiaotao Song, Hailong Sun, Xu Wang, Jiafei Yan, IEEE AccessXiaotao Song, Hailong Sun, Xu Wang, and Jiafei Yan. 2019. A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques. IEEE Access (2019).\n\nTowards automatically generating summary comments for java methods. Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, Vijay-Shanker, Proceedings of the IEEE/ACM international conference on Automated software engineering. the IEEE/ACM international conference on Automated software engineeringACMGiriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay- Shanker. 2010. Towards automatically generating summary comments for java methods. In Proceedings of the IEEE/ACM international conference on Automated software engineering. ACM, 43-52.\n\nAutomatically detecting and describing high level actions within methods. Giriprasad Sridhara, Lori Pollock, K Vijay-Shanker, Proceedings of the 33rd International Conference on Software Engineering. the 33rd International Conference on Software EngineeringACMGiriprasad Sridhara, Lori Pollock, and K Vijay-Shanker. 2011. Automatically detecting and describing high level actions within methods. In Proceedings of the 33rd International Conference on Software Engineering. ACM, 101-110.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.\n\nImproving automatic source code summarization via deep reinforcement learning. Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, Philip S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Improving automatic source code summarization via deep rein- forcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 397-407.\n\nGraph2seq: Graph to sequence learning with attention-based neural networks. Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, Vadim Sheinin, arXiv:1804.00823arXiv preprintKun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and Vadim Sheinin. 2018. Graph2seq: Graph to sequence learning with attention-based neural networks. arXiv preprint arXiv:1804.00823 (2018).\n\nExploiting rich syntactic information for semantic parsing with graph-tosequence model. Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, Vadim Sheinin, Conference on Empirical Methods in Natural Language Processing. Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. 2018. Exploiting rich syntactic information for semantic parsing with graph-to- sequence model. Conference on Empirical Methods in Natural Language Processing (2018).\n\nRecent trends in deep learning based natural language processing. ieee Computational intelligenCe magazine. Tom Young, Devamanyu Hazarika, Soujanya Poria, Erik Cambria, 13Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018. Recent trends in deep learning based natural language processing. ieee Computa- tional intelligenCe magazine 13, 3 (2018), 55-75.\n\nNeural detection of semantic code clones via tree-based convolution. Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, Qianxiang Wang, Proceedings of the 27th International Conference on Program Comprehension. the 27th International Conference on Program ComprehensionIEEE PressHao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, and Qianxiang Wang. 2019. Neural detection of semantic code clones via tree-based convolution. In Proceedings of the 27th International Conference on Program Comprehension. IEEE Press, 70-80.\n\nA novel neural source code representation based on abstract syntax tree. Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, Xudong Liu, Proceedings of the 41st International Conference on Software Engineering. the 41st International Conference on Software EngineeringIEEE PressJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation based on abstract syntax tree. In Proceedings of the 41st International Conference on Software Engineering. IEEE Press, 783-794.\n", "annotations": {"author": "[{\"end\":146,\"start\":120},{\"end\":181,\"start\":147},{\"end\":193,\"start\":182},{\"end\":210,\"start\":194},{\"end\":223,\"start\":211},{\"end\":242,\"start\":224},{\"end\":254,\"start\":243},{\"end\":271,\"start\":255},{\"end\":367,\"start\":272},{\"end\":411,\"start\":368},{\"end\":475,\"start\":412},{\"end\":519,\"start\":476}]", "publisher": "[{\"end\":82,\"start\":79},{\"end\":717,\"start\":714}]", "author_last_name": "[{\"end\":131,\"start\":126},{\"end\":164,\"start\":157},{\"end\":192,\"start\":190},{\"end\":209,\"start\":201},{\"end\":222,\"start\":217},{\"end\":241,\"start\":234},{\"end\":253,\"start\":251},{\"end\":270,\"start\":262}]", "author_first_name": "[{\"end\":125,\"start\":120},{\"end\":156,\"start\":147},{\"end\":189,\"start\":182},{\"end\":200,\"start\":194},{\"end\":216,\"start\":211},{\"end\":233,\"start\":224},{\"end\":250,\"start\":243},{\"end\":261,\"start\":255}]", "author_affiliation": "[{\"end\":366,\"start\":273},{\"end\":410,\"start\":369},{\"end\":474,\"start\":413},{\"end\":518,\"start\":477}]", "title": "[{\"end\":78,\"start\":1},{\"end\":597,\"start\":520}]", "venue": "[{\"end\":670,\"start\":599}]", "abstract": "[{\"end\":1973,\"start\":766}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2036,\"start\":2032},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2598,\"start\":2594},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2839,\"start\":2835},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2842,\"start\":2839},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2845,\"start\":2842},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3447,\"start\":3443},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3663,\"start\":3659},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3758,\"start\":3754},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3782,\"start\":3778},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3856,\"start\":3853},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3881,\"start\":3878},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4263,\"start\":4259},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4482,\"start\":4479},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4485,\"start\":4482},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4488,\"start\":4485},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4491,\"start\":4488},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5810,\"start\":5806},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6108,\"start\":6104},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6423,\"start\":6419},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6615,\"start\":6611},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7530,\"start\":7527},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7533,\"start\":7530},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7592,\"start\":7589},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8379,\"start\":8375},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9118,\"start\":9114},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9971,\"start\":9967},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10619,\"start\":10615},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10873,\"start\":10869},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11246,\"start\":11242},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11742,\"start\":11738},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11892,\"start\":11889},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11917,\"start\":11914},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12557,\"start\":12554},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12560,\"start\":12557},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12563,\"start\":12560},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12815,\"start\":12811},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12843,\"start\":12839},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12865,\"start\":12861},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12953,\"start\":12950},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12978,\"start\":12975},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13018,\"start\":13015},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13036,\"start\":13032},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13058,\"start\":13054},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13139,\"start\":13135},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13142,\"start\":13139},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13145,\"start\":13142},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13437,\"start\":13434},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13840,\"start\":13836},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13912,\"start\":13908},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13915,\"start\":13912},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13918,\"start\":13915},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14996,\"start\":14993},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14998,\"start\":14996},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15001,\"start\":14998},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15004,\"start\":15001},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15422,\"start\":15418},{\"end\":17396,\"start\":17383},{\"end\":17439,\"start\":17421},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22142,\"start\":22138},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22145,\"start\":22142},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23054,\"start\":23050},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23703,\"start\":23699},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25529,\"start\":25525},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28074,\"start\":28070},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28501,\"start\":28499},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":35586,\"start\":35582},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35601,\"start\":35597},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38134,\"start\":38130},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38196,\"start\":38192},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38380,\"start\":38376},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38474,\"start\":38470},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38550,\"start\":38546},{\"end\":38779,\"start\":38770},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38800,\"start\":38797},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39048,\"start\":39044},{\"end\":39189,\"start\":39181},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39205,\"start\":39202},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43681,\"start\":43677},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":44191,\"start\":44187},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":44194,\"start\":44191},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44509,\"start\":44505},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":44611,\"start\":44608},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":44749,\"start\":44745},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":44773,\"start\":44769},{\"end\":45042,\"start\":45032},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":46643,\"start\":46639},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":51775,\"start\":51771},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":51854,\"start\":51850},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":53384,\"start\":53380},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":53387,\"start\":53384},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":55130,\"start\":55126}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56411,\"start\":56289},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57061,\"start\":56412},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57209,\"start\":57062},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57381,\"start\":57210},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57891,\"start\":57382},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58161,\"start\":57892},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":58348,\"start\":58162},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59457,\"start\":58349}]", "paragraph": "[{\"end\":2614,\"start\":1975},{\"end\":3448,\"start\":2616},{\"end\":4492,\"start\":3450},{\"end\":5681,\"start\":4494},{\"end\":6734,\"start\":5714},{\"end\":7841,\"start\":6736},{\"end\":8533,\"start\":7843},{\"end\":8689,\"start\":8563},{\"end\":9049,\"start\":8719},{\"end\":9955,\"start\":9051},{\"end\":10592,\"start\":9957},{\"end\":11218,\"start\":10594},{\"end\":11850,\"start\":11220},{\"end\":12564,\"start\":11852},{\"end\":13146,\"start\":12566},{\"end\":13919,\"start\":13186},{\"end\":15005,\"start\":13921},{\"end\":15619,\"start\":15007},{\"end\":16084,\"start\":15633},{\"end\":16482,\"start\":16097},{\"end\":16935,\"start\":16484},{\"end\":17190,\"start\":16937},{\"end\":18312,\"start\":17192},{\"end\":19766,\"start\":18314},{\"end\":20495,\"start\":19768},{\"end\":21598,\"start\":20497},{\"end\":22020,\"start\":21600},{\"end\":22909,\"start\":22043},{\"end\":23560,\"start\":22933},{\"end\":24396,\"start\":23562},{\"end\":25319,\"start\":24398},{\"end\":26488,\"start\":25337},{\"end\":26791,\"start\":26490},{\"end\":27418,\"start\":26793},{\"end\":28431,\"start\":27420},{\"end\":29006,\"start\":28433},{\"end\":30020,\"start\":29008},{\"end\":30801,\"start\":30022},{\"end\":32630,\"start\":30803},{\"end\":32904,\"start\":32660},{\"end\":33042,\"start\":32926},{\"end\":33668,\"start\":33065},{\"end\":34409,\"start\":33670},{\"end\":35002,\"start\":34411},{\"end\":35539,\"start\":35018},{\"end\":36476,\"start\":35541},{\"end\":36937,\"start\":36478},{\"end\":37598,\"start\":36939},{\"end\":38324,\"start\":37612},{\"end\":38517,\"start\":38326},{\"end\":39760,\"start\":38519},{\"end\":40222,\"start\":39783},{\"end\":43962,\"start\":40255},{\"end\":45052,\"start\":43964},{\"end\":45344,\"start\":45087},{\"end\":46060,\"start\":45346},{\"end\":47375,\"start\":46062},{\"end\":48292,\"start\":47377},{\"end\":49239,\"start\":48294},{\"end\":49876,\"start\":49241},{\"end\":51372,\"start\":49878},{\"end\":52841,\"start\":51374},{\"end\":53878,\"start\":52843},{\"end\":53938,\"start\":53891},{\"end\":54074,\"start\":53940},{\"end\":54231,\"start\":54076},{\"end\":54419,\"start\":54233},{\"end\":55203,\"start\":54421},{\"end\":55923,\"start\":55205},{\"end\":56075,\"start\":55925},{\"end\":56288,\"start\":56095}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":22911,\"start\":22910}]", "table_ref": "[{\"end\":40531,\"start\":40524},{\"end\":41766,\"start\":41759},{\"end\":41914,\"start\":41907}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":5712,\"start\":5684},{\"attributes\":{\"n\":\"3\"},\"end\":8561,\"start\":8536},{\"attributes\":{\"n\":\"3.1\"},\"end\":8717,\"start\":8692},{\"attributes\":{\"n\":\"3.2\"},\"end\":13184,\"start\":13149},{\"attributes\":{\"n\":\"4\"},\"end\":15631,\"start\":15622},{\"attributes\":{\"n\":\"4.1\"},\"end\":16095,\"start\":16087},{\"attributes\":{\"n\":\"4.2\"},\"end\":22041,\"start\":22023},{\"attributes\":{\"n\":\"4.3\"},\"end\":22931,\"start\":22913},{\"attributes\":{\"n\":\"4.4\"},\"end\":25335,\"start\":25322},{\"attributes\":{\"n\":\"4.5\"},\"end\":32658,\"start\":32633},{\"attributes\":{\"n\":\"5\"},\"end\":32924,\"start\":32907},{\"attributes\":{\"n\":\"5.1\"},\"end\":33063,\"start\":33045},{\"attributes\":{\"n\":\"5.2\"},\"end\":35016,\"start\":35005},{\"attributes\":{\"n\":\"5.3\"},\"end\":37610,\"start\":37601},{\"attributes\":{\"n\":\"6\"},\"end\":39781,\"start\":39763},{\"attributes\":{\"n\":\"6.1\"},\"end\":40253,\"start\":40225},{\"attributes\":{\"n\":\"6.2\"},\"end\":45085,\"start\":45055},{\"end\":53889,\"start\":53881},{\"attributes\":{\"n\":\"8\"},\"end\":56093,\"start\":56078},{\"end\":57073,\"start\":57063},{\"end\":57221,\"start\":57211}]", "table": "[{\"end\":58348,\"start\":58192},{\"end\":59457,\"start\":59182}]", "figure_caption": "[{\"end\":56411,\"start\":56291},{\"end\":57061,\"start\":56414},{\"end\":57209,\"start\":57075},{\"end\":57381,\"start\":57223},{\"end\":57891,\"start\":57384},{\"end\":58161,\"start\":57894},{\"end\":58192,\"start\":58164},{\"end\":59182,\"start\":58351}]", "figure_ref": "[{\"end\":27467,\"start\":27460},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40440,\"start\":40432},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":45372,\"start\":45364},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":46084,\"start\":46076}]", "bib_author_first_name": "[{\"end\":59512,\"start\":59506},{\"end\":59703,\"start\":59694},{\"end\":59716,\"start\":59715},{\"end\":59732,\"start\":59723},{\"end\":59746,\"start\":59739},{\"end\":60026,\"start\":60017},{\"end\":60042,\"start\":60038},{\"end\":60064,\"start\":60057},{\"end\":60306,\"start\":60303},{\"end\":60319,\"start\":60313},{\"end\":60331,\"start\":60327},{\"end\":60342,\"start\":60338},{\"end\":60736,\"start\":60729},{\"end\":60756,\"start\":60747},{\"end\":60768,\"start\":60762},{\"end\":61018,\"start\":61017},{\"end\":61027,\"start\":61026},{\"end\":61029,\"start\":61028},{\"end\":61038,\"start\":61037},{\"end\":61054,\"start\":61053},{\"end\":61393,\"start\":61392},{\"end\":61405,\"start\":61399},{\"end\":61407,\"start\":61406},{\"end\":61427,\"start\":61421},{\"end\":61933,\"start\":61928},{\"end\":61952,\"start\":61947},{\"end\":61966,\"start\":61960},{\"end\":61982,\"start\":61974},{\"end\":61998,\"start\":61992},{\"end\":62014,\"start\":62009},{\"end\":62027,\"start\":62023},{\"end\":62039,\"start\":62035},{\"end\":62054,\"start\":62050},{\"end\":62696,\"start\":62694},{\"end\":62710,\"start\":62703},{\"end\":62723,\"start\":62715},{\"end\":62725,\"start\":62724},{\"end\":63051,\"start\":63044},{\"end\":63060,\"start\":63057},{\"end\":63405,\"start\":63398},{\"end\":63417,\"start\":63413},{\"end\":63641,\"start\":63636},{\"end\":63997,\"start\":63992},{\"end\":64212,\"start\":64206},{\"end\":64223,\"start\":64222},{\"end\":64584,\"start\":64576},{\"end\":64595,\"start\":64589},{\"end\":64610,\"start\":64603},{\"end\":64998,\"start\":64993},{\"end\":65012,\"start\":65007},{\"end\":65026,\"start\":65021},{\"end\":65042,\"start\":65035},{\"end\":65413,\"start\":65412},{\"end\":65432,\"start\":65423},{\"end\":65926,\"start\":65921},{\"end\":65937,\"start\":65933},{\"end\":65948,\"start\":65947},{\"end\":66439,\"start\":66435},{\"end\":66449,\"start\":66448},{\"end\":66859,\"start\":66852},{\"end\":67172,\"start\":67168},{\"end\":67179,\"start\":67177},{\"end\":67187,\"start\":67184},{\"end\":67198,\"start\":67193},{\"end\":67206,\"start\":67203},{\"end\":67593,\"start\":67583},{\"end\":67607,\"start\":67600},{\"end\":67622,\"start\":67617},{\"end\":67635,\"start\":67631},{\"end\":68171,\"start\":68165},{\"end\":68184,\"start\":68179},{\"end\":68199,\"start\":68193},{\"end\":68727,\"start\":68720},{\"end\":69151,\"start\":69143},{\"end\":69167,\"start\":69161},{\"end\":69189,\"start\":69181},{\"end\":69201,\"start\":69196},{\"end\":69214,\"start\":69208},{\"end\":69229,\"start\":69223},{\"end\":69745,\"start\":69736},{\"end\":69761,\"start\":69755},{\"end\":69775,\"start\":69769},{\"end\":70236,\"start\":70227},{\"end\":70252,\"start\":70246},{\"end\":70918,\"start\":70910},{\"end\":71102,\"start\":71101},{\"end\":71111,\"start\":71110},{\"end\":71126,\"start\":71125},{\"end\":71136,\"start\":71135},{\"end\":71340,\"start\":71339},{\"end\":71353,\"start\":71347},{\"end\":71676,\"start\":71670},{\"end\":71693,\"start\":71688},{\"end\":71706,\"start\":71700},{\"end\":71730,\"start\":71726},{\"end\":72223,\"start\":72218},{\"end\":72234,\"start\":72231},{\"end\":72241,\"start\":72239},{\"end\":72555,\"start\":72549},{\"end\":72569,\"start\":72561},{\"end\":72584,\"start\":72578},{\"end\":72600,\"start\":72593},{\"end\":72615,\"start\":72607},{\"end\":72629,\"start\":72623},{\"end\":72643,\"start\":72636},{\"end\":73185,\"start\":73184},{\"end\":73202,\"start\":73194},{\"end\":73537,\"start\":73530},{\"end\":73553,\"start\":73548},{\"end\":73566,\"start\":73562},{\"end\":73581,\"start\":73573},{\"end\":74263,\"start\":74257},{\"end\":74279,\"start\":74275},{\"end\":74292,\"start\":74287},{\"end\":74304,\"start\":74298},{\"end\":74317,\"start\":74311},{\"end\":74328,\"start\":74323},{\"end\":74334,\"start\":74329},{\"end\":74350,\"start\":74342},{\"end\":74366,\"start\":74357},{\"end\":74374,\"start\":74373},{\"end\":74376,\"start\":74375},{\"end\":74733,\"start\":74729},{\"end\":74749,\"start\":74744},{\"end\":74994,\"start\":74987},{\"end\":75008,\"start\":75001},{\"end\":75016,\"start\":75014},{\"end\":75029,\"start\":75023},{\"end\":75288,\"start\":75278},{\"end\":75304,\"start\":75299},{\"end\":75316,\"start\":75311},{\"end\":75332,\"start\":75328},{\"end\":75867,\"start\":75857},{\"end\":75882,\"start\":75878},{\"end\":75893,\"start\":75892},{\"end\":76304,\"start\":76298},{\"end\":76318,\"start\":76314},{\"end\":76332,\"start\":76328},{\"end\":76346,\"start\":76341},{\"end\":76363,\"start\":76358},{\"end\":76376,\"start\":76371},{\"end\":76378,\"start\":76377},{\"end\":76392,\"start\":76386},{\"end\":76406,\"start\":76401},{\"end\":76776,\"start\":76773},{\"end\":76786,\"start\":76782},{\"end\":76796,\"start\":76793},{\"end\":76811,\"start\":76803},{\"end\":76823,\"start\":76816},{\"end\":76834,\"start\":76830},{\"end\":76847,\"start\":76839},{\"end\":77384,\"start\":77381},{\"end\":77396,\"start\":77389},{\"end\":77407,\"start\":77401},{\"end\":77421,\"start\":77414},{\"end\":77435,\"start\":77428},{\"end\":77451,\"start\":77446},{\"end\":77789,\"start\":77786},{\"end\":77801,\"start\":77794},{\"end\":77812,\"start\":77806},{\"end\":77821,\"start\":77819},{\"end\":77831,\"start\":77826},{\"end\":77843,\"start\":77838},{\"end\":78267,\"start\":78264},{\"end\":78284,\"start\":78275},{\"end\":78303,\"start\":78295},{\"end\":78315,\"start\":78311},{\"end\":78601,\"start\":78598},{\"end\":78610,\"start\":78606},{\"end\":78620,\"start\":78616},{\"end\":78629,\"start\":78627},{\"end\":78637,\"start\":78634},{\"end\":78652,\"start\":78643},{\"end\":79117,\"start\":79113},{\"end\":79127,\"start\":79125},{\"end\":79140,\"start\":79134},{\"end\":79155,\"start\":79148},{\"end\":79168,\"start\":79161},{\"end\":79181,\"start\":79175}]", "bib_author_last_name": "[{\"end\":59522,\"start\":59513},{\"end\":59713,\"start\":59704},{\"end\":59721,\"start\":59717},{\"end\":59737,\"start\":59733},{\"end\":59754,\"start\":59747},{\"end\":59762,\"start\":59756},{\"end\":60036,\"start\":60027},{\"end\":60055,\"start\":60043},{\"end\":60072,\"start\":60065},{\"end\":60311,\"start\":60307},{\"end\":60325,\"start\":60320},{\"end\":60336,\"start\":60332},{\"end\":60348,\"start\":60343},{\"end\":60745,\"start\":60737},{\"end\":60760,\"start\":60757},{\"end\":60775,\"start\":60769},{\"end\":61024,\"start\":61019},{\"end\":61035,\"start\":61030},{\"end\":61051,\"start\":61039},{\"end\":61062,\"start\":61055},{\"end\":61070,\"start\":61064},{\"end\":61397,\"start\":61394},{\"end\":61419,\"start\":61408},{\"end\":61437,\"start\":61428},{\"end\":61446,\"start\":61439},{\"end\":61945,\"start\":61934},{\"end\":61958,\"start\":61953},{\"end\":61972,\"start\":61967},{\"end\":61990,\"start\":61983},{\"end\":62007,\"start\":61999},{\"end\":62021,\"start\":62015},{\"end\":62033,\"start\":62028},{\"end\":62048,\"start\":62040},{\"end\":62062,\"start\":62055},{\"end\":62070,\"start\":62064},{\"end\":62701,\"start\":62697},{\"end\":62713,\"start\":62711},{\"end\":62730,\"start\":62726},{\"end\":63055,\"start\":63052},{\"end\":63065,\"start\":63061},{\"end\":63411,\"start\":63406},{\"end\":63425,\"start\":63418},{\"end\":63646,\"start\":63642},{\"end\":64002,\"start\":63998},{\"end\":64220,\"start\":64213},{\"end\":64231,\"start\":64224},{\"end\":64243,\"start\":64233},{\"end\":64587,\"start\":64585},{\"end\":64601,\"start\":64596},{\"end\":64614,\"start\":64611},{\"end\":65005,\"start\":64999},{\"end\":65019,\"start\":65013},{\"end\":65033,\"start\":65027},{\"end\":65049,\"start\":65043},{\"end\":65421,\"start\":65414},{\"end\":65444,\"start\":65433},{\"end\":65453,\"start\":65446},{\"end\":65931,\"start\":65927},{\"end\":65945,\"start\":65938},{\"end\":65962,\"start\":65949},{\"end\":66446,\"start\":66440},{\"end\":66454,\"start\":66450},{\"end\":66462,\"start\":66456},{\"end\":66870,\"start\":66860},{\"end\":66877,\"start\":66872},{\"end\":67175,\"start\":67173},{\"end\":67182,\"start\":67180},{\"end\":67191,\"start\":67188},{\"end\":67201,\"start\":67199},{\"end\":67210,\"start\":67207},{\"end\":67598,\"start\":67594},{\"end\":67615,\"start\":67608},{\"end\":67629,\"start\":67623},{\"end\":67647,\"start\":67636},{\"end\":68177,\"start\":68172},{\"end\":68191,\"start\":68185},{\"end\":68208,\"start\":68200},{\"end\":68734,\"start\":68728},{\"end\":69159,\"start\":69152},{\"end\":69179,\"start\":69168},{\"end\":69194,\"start\":69190},{\"end\":69206,\"start\":69202},{\"end\":69221,\"start\":69215},{\"end\":69235,\"start\":69230},{\"end\":69753,\"start\":69746},{\"end\":69767,\"start\":69762},{\"end\":69784,\"start\":69776},{\"end\":70244,\"start\":70237},{\"end\":70261,\"start\":70253},{\"end\":70922,\"start\":70919},{\"end\":71108,\"start\":71103},{\"end\":71123,\"start\":71112},{\"end\":71133,\"start\":71127},{\"end\":71142,\"start\":71137},{\"end\":71345,\"start\":71341},{\"end\":71362,\"start\":71354},{\"end\":71372,\"start\":71364},{\"end\":71686,\"start\":71677},{\"end\":71698,\"start\":71694},{\"end\":71724,\"start\":71707},{\"end\":71739,\"start\":71731},{\"end\":71746,\"start\":71741},{\"end\":72229,\"start\":72224},{\"end\":72237,\"start\":72235},{\"end\":72247,\"start\":72242},{\"end\":72559,\"start\":72556},{\"end\":72576,\"start\":72570},{\"end\":72591,\"start\":72585},{\"end\":72605,\"start\":72601},{\"end\":72621,\"start\":72616},{\"end\":72634,\"start\":72630},{\"end\":72652,\"start\":72644},{\"end\":73192,\"start\":73186},{\"end\":73208,\"start\":73203},{\"end\":73217,\"start\":73210},{\"end\":73546,\"start\":73538},{\"end\":73560,\"start\":73554},{\"end\":73571,\"start\":73567},{\"end\":73585,\"start\":73582},{\"end\":74273,\"start\":74264},{\"end\":74285,\"start\":74280},{\"end\":74296,\"start\":74293},{\"end\":74309,\"start\":74305},{\"end\":74321,\"start\":74318},{\"end\":74340,\"start\":74335},{\"end\":74355,\"start\":74351},{\"end\":74371,\"start\":74367},{\"end\":74384,\"start\":74377},{\"end\":74742,\"start\":74734},{\"end\":74757,\"start\":74750},{\"end\":74999,\"start\":74995},{\"end\":75012,\"start\":75009},{\"end\":75021,\"start\":75017},{\"end\":75033,\"start\":75030},{\"end\":75297,\"start\":75289},{\"end\":75309,\"start\":75305},{\"end\":75326,\"start\":75317},{\"end\":75340,\"start\":75333},{\"end\":75355,\"start\":75342},{\"end\":75876,\"start\":75868},{\"end\":75890,\"start\":75883},{\"end\":75907,\"start\":75894},{\"end\":76312,\"start\":76305},{\"end\":76326,\"start\":76319},{\"end\":76339,\"start\":76333},{\"end\":76356,\"start\":76347},{\"end\":76369,\"start\":76364},{\"end\":76384,\"start\":76379},{\"end\":76399,\"start\":76393},{\"end\":76417,\"start\":76407},{\"end\":76780,\"start\":76777},{\"end\":76791,\"start\":76787},{\"end\":76801,\"start\":76797},{\"end\":76814,\"start\":76812},{\"end\":76828,\"start\":76824},{\"end\":76837,\"start\":76835},{\"end\":76850,\"start\":76848},{\"end\":77387,\"start\":77385},{\"end\":77399,\"start\":77397},{\"end\":77412,\"start\":77408},{\"end\":77426,\"start\":77422},{\"end\":77444,\"start\":77436},{\"end\":77459,\"start\":77452},{\"end\":77792,\"start\":77790},{\"end\":77804,\"start\":77802},{\"end\":77817,\"start\":77813},{\"end\":77824,\"start\":77822},{\"end\":77836,\"start\":77832},{\"end\":77851,\"start\":77844},{\"end\":78273,\"start\":78268},{\"end\":78293,\"start\":78285},{\"end\":78309,\"start\":78304},{\"end\":78323,\"start\":78316},{\"end\":78604,\"start\":78602},{\"end\":78614,\"start\":78611},{\"end\":78625,\"start\":78621},{\"end\":78632,\"start\":78630},{\"end\":78641,\"start\":78638},{\"end\":78657,\"start\":78653},{\"end\":79123,\"start\":79118},{\"end\":79132,\"start\":79128},{\"end\":79146,\"start\":79141},{\"end\":79159,\"start\":79156},{\"end\":79173,\"start\":79169},{\"end\":79185,\"start\":79182}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":59633,\"start\":59459},{\"attributes\":{\"doi\":\"arXiv:1709.06182\",\"id\":\"b1\"},\"end\":59971,\"start\":59635},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3495200},\"end\":60301,\"start\":59973},{\"attributes\":{\"id\":\"b3\"},\"end\":60656,\"start\":60303},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b4\"},\"end\":60975,\"start\":60658},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2906360},\"end\":61333,\"start\":60977},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15188956},\"end\":61844,\"start\":61335},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13747425},\"end\":62606,\"start\":61846},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":199577786},\"end\":62980,\"start\":62608},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":44157913},\"end\":63396,\"start\":62982},{\"attributes\":{\"id\":\"b10\"},\"end\":63575,\"start\":63398},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15069221},\"end\":63899,\"start\":63577},{\"attributes\":{\"id\":\"b12\"},\"end\":64129,\"start\":63901},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8033761},\"end\":64556,\"start\":64131},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":47021242},\"end\":64908,\"start\":64558},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7843537},\"end\":65343,\"start\":64910},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":21164835},\"end\":65825,\"start\":65345},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14387543},\"end\":66373,\"start\":65827},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10374030},\"end\":66788,\"start\":66375},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52947736},\"end\":67136,\"start\":66790},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49584534},\"end\":67525,\"start\":67138},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8820379},\"end\":68077,\"start\":67527},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9237290},\"end\":68648,\"start\":68079},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":30353027},\"end\":69079,\"start\":68650},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5545615},\"end\":69653,\"start\":69081},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":59606259},\"end\":70165,\"start\":69655},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":102354583},\"end\":70852,\"start\":70167},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":964287},\"end\":71072,\"start\":70854},{\"attributes\":{\"id\":\"b28\"},\"end\":71272,\"start\":71074},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":17406565},\"end\":71593,\"start\":71274},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8928715},\"end\":72163,\"start\":71595},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7418692},\"end\":72454,\"start\":72165},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15979705},\"end\":73051,\"start\":72456},{\"attributes\":{\"id\":\"b33\"},\"end\":73464,\"start\":73053},{\"attributes\":{\"doi\":\"10.3115/1073083.1073135\",\"id\":\"b34\",\"matched_paper_id\":11080756},\"end\":74186,\"start\":73466},{\"attributes\":{\"id\":\"b35\"},\"end\":74673,\"start\":74188},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":139164978},\"end\":74900,\"start\":74675},{\"attributes\":{\"id\":\"b37\"},\"end\":75208,\"start\":74902},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9790585},\"end\":75781,\"start\":75210},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11106352},\"end\":76269,\"start\":75783},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13756489},\"end\":76692,\"start\":76271},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":52069701},\"end\":77303,\"start\":76694},{\"attributes\":{\"doi\":\"arXiv:1804.00823\",\"id\":\"b42\"},\"end\":77696,\"start\":77305},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":52074926},\"end\":78154,\"start\":77698},{\"attributes\":{\"id\":\"b44\"},\"end\":78527,\"start\":78156},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":174803974},\"end\":79038,\"start\":78529},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":174799700},\"end\":79582,\"start\":79040}]", "bib_title": "[{\"end\":60015,\"start\":59973},{\"end\":61015,\"start\":60977},{\"end\":61390,\"start\":61335},{\"end\":61926,\"start\":61846},{\"end\":62692,\"start\":62608},{\"end\":63042,\"start\":62982},{\"end\":63634,\"start\":63577},{\"end\":64204,\"start\":64131},{\"end\":64574,\"start\":64558},{\"end\":64991,\"start\":64910},{\"end\":65410,\"start\":65345},{\"end\":65919,\"start\":65827},{\"end\":66433,\"start\":66375},{\"end\":66850,\"start\":66790},{\"end\":67166,\"start\":67138},{\"end\":67581,\"start\":67527},{\"end\":68163,\"start\":68079},{\"end\":68718,\"start\":68650},{\"end\":69141,\"start\":69081},{\"end\":69734,\"start\":69655},{\"end\":70225,\"start\":70167},{\"end\":70908,\"start\":70854},{\"end\":71337,\"start\":71274},{\"end\":71668,\"start\":71595},{\"end\":72216,\"start\":72165},{\"end\":72547,\"start\":72456},{\"end\":73182,\"start\":73053},{\"end\":73528,\"start\":73466},{\"end\":74255,\"start\":74188},{\"end\":74727,\"start\":74675},{\"end\":75276,\"start\":75210},{\"end\":75855,\"start\":75783},{\"end\":76296,\"start\":76271},{\"end\":76771,\"start\":76694},{\"end\":77784,\"start\":77698},{\"end\":78596,\"start\":78529},{\"end\":79111,\"start\":79040}]", "bib_author": "[{\"end\":59524,\"start\":59506},{\"end\":59715,\"start\":59694},{\"end\":59723,\"start\":59715},{\"end\":59739,\"start\":59723},{\"end\":59756,\"start\":59739},{\"end\":59764,\"start\":59756},{\"end\":60038,\"start\":60017},{\"end\":60057,\"start\":60038},{\"end\":60074,\"start\":60057},{\"end\":60313,\"start\":60303},{\"end\":60327,\"start\":60313},{\"end\":60338,\"start\":60327},{\"end\":60350,\"start\":60338},{\"end\":60747,\"start\":60729},{\"end\":60762,\"start\":60747},{\"end\":60777,\"start\":60762},{\"end\":61026,\"start\":61017},{\"end\":61037,\"start\":61026},{\"end\":61053,\"start\":61037},{\"end\":61064,\"start\":61053},{\"end\":61072,\"start\":61064},{\"end\":61399,\"start\":61392},{\"end\":61421,\"start\":61399},{\"end\":61439,\"start\":61421},{\"end\":61448,\"start\":61439},{\"end\":61947,\"start\":61928},{\"end\":61960,\"start\":61947},{\"end\":61974,\"start\":61960},{\"end\":61992,\"start\":61974},{\"end\":62009,\"start\":61992},{\"end\":62023,\"start\":62009},{\"end\":62035,\"start\":62023},{\"end\":62050,\"start\":62035},{\"end\":62064,\"start\":62050},{\"end\":62072,\"start\":62064},{\"end\":62703,\"start\":62694},{\"end\":62715,\"start\":62703},{\"end\":62732,\"start\":62715},{\"end\":63057,\"start\":63044},{\"end\":63067,\"start\":63057},{\"end\":63413,\"start\":63398},{\"end\":63427,\"start\":63413},{\"end\":63648,\"start\":63636},{\"end\":64004,\"start\":63992},{\"end\":64222,\"start\":64206},{\"end\":64233,\"start\":64222},{\"end\":64245,\"start\":64233},{\"end\":64589,\"start\":64576},{\"end\":64603,\"start\":64589},{\"end\":64616,\"start\":64603},{\"end\":65007,\"start\":64993},{\"end\":65021,\"start\":65007},{\"end\":65035,\"start\":65021},{\"end\":65051,\"start\":65035},{\"end\":65423,\"start\":65412},{\"end\":65446,\"start\":65423},{\"end\":65455,\"start\":65446},{\"end\":65933,\"start\":65921},{\"end\":65947,\"start\":65933},{\"end\":65964,\"start\":65947},{\"end\":66448,\"start\":66435},{\"end\":66456,\"start\":66448},{\"end\":66464,\"start\":66456},{\"end\":66872,\"start\":66852},{\"end\":66879,\"start\":66872},{\"end\":67177,\"start\":67168},{\"end\":67184,\"start\":67177},{\"end\":67193,\"start\":67184},{\"end\":67203,\"start\":67193},{\"end\":67212,\"start\":67203},{\"end\":67600,\"start\":67583},{\"end\":67617,\"start\":67600},{\"end\":67631,\"start\":67617},{\"end\":67649,\"start\":67631},{\"end\":68179,\"start\":68165},{\"end\":68193,\"start\":68179},{\"end\":68210,\"start\":68193},{\"end\":68736,\"start\":68720},{\"end\":69161,\"start\":69143},{\"end\":69181,\"start\":69161},{\"end\":69196,\"start\":69181},{\"end\":69208,\"start\":69196},{\"end\":69223,\"start\":69208},{\"end\":69237,\"start\":69223},{\"end\":69755,\"start\":69736},{\"end\":69769,\"start\":69755},{\"end\":69786,\"start\":69769},{\"end\":70246,\"start\":70227},{\"end\":70263,\"start\":70246},{\"end\":70924,\"start\":70910},{\"end\":71110,\"start\":71101},{\"end\":71125,\"start\":71110},{\"end\":71135,\"start\":71125},{\"end\":71144,\"start\":71135},{\"end\":71347,\"start\":71339},{\"end\":71364,\"start\":71347},{\"end\":71374,\"start\":71364},{\"end\":71688,\"start\":71670},{\"end\":71700,\"start\":71688},{\"end\":71726,\"start\":71700},{\"end\":71741,\"start\":71726},{\"end\":71748,\"start\":71741},{\"end\":72231,\"start\":72218},{\"end\":72239,\"start\":72231},{\"end\":72249,\"start\":72239},{\"end\":72561,\"start\":72549},{\"end\":72578,\"start\":72561},{\"end\":72593,\"start\":72578},{\"end\":72607,\"start\":72593},{\"end\":72623,\"start\":72607},{\"end\":72636,\"start\":72623},{\"end\":72654,\"start\":72636},{\"end\":73194,\"start\":73184},{\"end\":73210,\"start\":73194},{\"end\":73219,\"start\":73210},{\"end\":73548,\"start\":73530},{\"end\":73562,\"start\":73548},{\"end\":73573,\"start\":73562},{\"end\":73587,\"start\":73573},{\"end\":74275,\"start\":74257},{\"end\":74287,\"start\":74275},{\"end\":74298,\"start\":74287},{\"end\":74311,\"start\":74298},{\"end\":74323,\"start\":74311},{\"end\":74342,\"start\":74323},{\"end\":74357,\"start\":74342},{\"end\":74373,\"start\":74357},{\"end\":74386,\"start\":74373},{\"end\":74744,\"start\":74729},{\"end\":74759,\"start\":74744},{\"end\":75001,\"start\":74987},{\"end\":75014,\"start\":75001},{\"end\":75023,\"start\":75014},{\"end\":75035,\"start\":75023},{\"end\":75299,\"start\":75278},{\"end\":75311,\"start\":75299},{\"end\":75328,\"start\":75311},{\"end\":75342,\"start\":75328},{\"end\":75357,\"start\":75342},{\"end\":75878,\"start\":75857},{\"end\":75892,\"start\":75878},{\"end\":75909,\"start\":75892},{\"end\":76314,\"start\":76298},{\"end\":76328,\"start\":76314},{\"end\":76341,\"start\":76328},{\"end\":76358,\"start\":76341},{\"end\":76371,\"start\":76358},{\"end\":76386,\"start\":76371},{\"end\":76401,\"start\":76386},{\"end\":76419,\"start\":76401},{\"end\":76782,\"start\":76773},{\"end\":76793,\"start\":76782},{\"end\":76803,\"start\":76793},{\"end\":76816,\"start\":76803},{\"end\":76830,\"start\":76816},{\"end\":76839,\"start\":76830},{\"end\":76852,\"start\":76839},{\"end\":77389,\"start\":77381},{\"end\":77401,\"start\":77389},{\"end\":77414,\"start\":77401},{\"end\":77428,\"start\":77414},{\"end\":77446,\"start\":77428},{\"end\":77461,\"start\":77446},{\"end\":77794,\"start\":77786},{\"end\":77806,\"start\":77794},{\"end\":77819,\"start\":77806},{\"end\":77826,\"start\":77819},{\"end\":77838,\"start\":77826},{\"end\":77853,\"start\":77838},{\"end\":78275,\"start\":78264},{\"end\":78295,\"start\":78275},{\"end\":78311,\"start\":78295},{\"end\":78325,\"start\":78311},{\"end\":78606,\"start\":78598},{\"end\":78616,\"start\":78606},{\"end\":78627,\"start\":78616},{\"end\":78634,\"start\":78627},{\"end\":78643,\"start\":78634},{\"end\":78659,\"start\":78643},{\"end\":79125,\"start\":79113},{\"end\":79134,\"start\":79125},{\"end\":79148,\"start\":79134},{\"end\":79161,\"start\":79148},{\"end\":79175,\"start\":79161},{\"end\":79187,\"start\":79175}]", "bib_venue": "[{\"end\":61579,\"start\":61522},{\"end\":62233,\"start\":62161},{\"end\":63208,\"start\":63146},{\"end\":64354,\"start\":64308},{\"end\":64747,\"start\":64690},{\"end\":65604,\"start\":65538},{\"end\":66095,\"start\":66038},{\"end\":66585,\"start\":66532},{\"end\":67345,\"start\":67287},{\"end\":67810,\"start\":67738},{\"end\":68379,\"start\":68303},{\"end\":68885,\"start\":68819},{\"end\":69340,\"start\":69297},{\"end\":69917,\"start\":69860},{\"end\":70534,\"start\":70407},{\"end\":71903,\"start\":71834},{\"end\":73803,\"start\":73705},{\"end\":75516,\"start\":75445},{\"end\":76040,\"start\":75983},{\"end\":77021,\"start\":76945},{\"end\":78792,\"start\":78734},{\"end\":79318,\"start\":79261},{\"end\":59504,\"start\":59459},{\"end\":59692,\"start\":59635},{\"end\":60126,\"start\":60074},{\"end\":60464,\"start\":60350},{\"end\":60727,\"start\":60658},{\"end\":61124,\"start\":61072},{\"end\":61520,\"start\":61448},{\"end\":62159,\"start\":62072},{\"end\":62784,\"start\":62732},{\"end\":63144,\"start\":63067},{\"end\":63467,\"start\":63427},{\"end\":63700,\"start\":63648},{\"end\":63990,\"start\":63901},{\"end\":64306,\"start\":64245},{\"end\":64688,\"start\":64616},{\"end\":65110,\"start\":65051},{\"end\":65536,\"start\":65455},{\"end\":66036,\"start\":65964},{\"end\":66530,\"start\":66464},{\"end\":66907,\"start\":66879},{\"end\":67285,\"start\":67212},{\"end\":67736,\"start\":67649},{\"end\":68301,\"start\":68210},{\"end\":68817,\"start\":68736},{\"end\":69295,\"start\":69237},{\"end\":69858,\"start\":69786},{\"end\":70405,\"start\":70263},{\"end\":70955,\"start\":70924},{\"end\":71099,\"start\":71074},{\"end\":71415,\"start\":71374},{\"end\":71832,\"start\":71748},{\"end\":72291,\"start\":72249},{\"end\":72701,\"start\":72654},{\"end\":73239,\"start\":73219},{\"end\":73703,\"start\":73610},{\"end\":74414,\"start\":74386},{\"end\":74770,\"start\":74759},{\"end\":74985,\"start\":74902},{\"end\":75443,\"start\":75357},{\"end\":75981,\"start\":75909},{\"end\":76468,\"start\":76419},{\"end\":76943,\"start\":76852},{\"end\":77379,\"start\":77305},{\"end\":77915,\"start\":77853},{\"end\":78262,\"start\":78156},{\"end\":78732,\"start\":78659},{\"end\":79259,\"start\":79187}]"}}}, "year": 2023, "month": 12, "day": 17}