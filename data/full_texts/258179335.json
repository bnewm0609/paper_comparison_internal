{"id": 258179335, "updated": "2023-10-05 01:54:12.27", "metadata": {"title": "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization", "authors": "[{\"first\":\"Cong\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Jiaming\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Weiming\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Jingwen\",\"last\":\"Leng\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Fan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Yunxin\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Minyi\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Yuhao\",\"last\":\"Zhu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs' size grows by $240\\times$ every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits. We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5$\\times$ speedup and 4.0$\\times$ energy reduction, respectively, with a superior model accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.07493", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/isca/0003THL00LG023", "doi": "10.1145/3579371.3589038"}}, "content": {"source": {"pdf_hash": "e92a5332390f0ba94615935541da4da9bed56512", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.07493v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5199bab951f0e42aaa47ca3a907092e4a21b4146", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e92a5332390f0ba94615935541da4da9bed56512.txt", "contents": "\nOliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantiza-tion\nJune 17-21, 2023. June 17-21, 2023. June 17-21, 2023\n\nCong Guo guocong@sjtu.edu.cn \nShanghai Jiao \nTong University \nShanghai Qi Zhi \nInstitute Shanghai \nChina Jiaming Tang \nShanghai Jiao \nTong University \nShanghai Qi Zhi \nInstitute Shanghai \nWeiming China \nHu \nShanghai Jiao \nTong University \nShanghai Qi Zhi \nInstitute Shanghai \nChina Jingwen Leng \nShanghai Jiao \nTong University \nShanghai Qi Zhi \nInstitute Shanghai \nChina Chen Zhang \nFan Yang fanyang@microsoft.com \nYunxin Liu liuyunxin@air.tsinghua.edu.cn \nMinyi Guo guo-my@cs.sjtu.edu.cn \nShanghai Jiao \nTong University \nShanghai Qi Zhi \nInstitute Shanghai \nChina Yuhao Zhu yzhu@rochester.edu \nCong Guo \nJiaming Tang \nWeiming Hu \nJingwen Leng leng-jw@cs.sjtu.edu.cn \nChen Zhang \nFan Yang \nYunxin Liu \nMinyi Guo \nYuhao Zhu \nGuo Tang \n\nMicrosoft Research\nBeijingChina\n\n\nInstitute for AI Industry Research (AIR)\nMicrosoft Research\nBeijingChina\n\n\nShanghai Artificial Intelligence Laboratory Shanghai\nTsinghua University\nBeijingChina, China\n\n\nUniversity of Rochester\nRochesterNew YorkUSA\n\nOliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantiza-tion\n\nProceedings of the 50th Annual International Symposium on Computer Architecture (ISCA '23)\nthe 50th Annual International Symposium on Computer Architecture (ISCA '23)Orlando, FL, USA 2023; Orlando, FL, USA ISCA '23; Orlando, FL, USA23June 17-21, 2023. June 17-21, 2023. June 17-21, 202310.1145/3579371.3589038* Contribute equally to this paper. \u2020 Work done while affiliated with ShanghaiTech University. \u2021 Jingwen Leng and Minyi Guo are corresponding authors of this paper. ACM Reference Format:. ACM, New York, NY, USA, 15 pages. https://CCS CONCEPTS \u2022 Computer systems organization \u2192 Neural networksData flow architecturesSingle instruction, multiple dataSystolic arrays KEYWORDS Large Language Model, Outlier-Victim Pair, Quantization\nTransformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs' size grows by 240\u00d7 every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits.We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5\u00d7 speedup and 4.0\u00d7 energy reduction, respectively, with a superior model accuracy.\n\nINTRODUCTION\n\nTransformer-based large language models (LLMs) [77] have demonstrated great success in the past years. Such success is often achieved with the increasingly larger model size: the model size grows by 240\u00d7 every two years, significantly outpacing the hardware progress (3.1\u00d7 per two years) [24]. As a result, the inference of LLMs becomes challenging and costly. For instance, OPT-175B [90], a recent Transformer-based LLM, has 175 billion parameters, which cannot fit in the latest high-end H100 GPU with 80GB memory. Quantization [6,7,21,22,72,74,79,93] is one of the most hardware-efficient ways to reduce inference costs for large models. It uses low-precision data types to compress models and accelerate the computation with practical hardware implementations, e.g., TPU [42] and GPU tensor core [60].\n\nHowever, existing quantization schemes [18,74,86] are less effective in Transformer-based LLMs. Recent studies show when the model size exceeds a threshold (e.g., 6 billion), the model performance is vulnerable to only a tiny fraction (< 0.1%) of outliers, whose values are much more significant than normal values [18]. Indiscriminately clipping both outlier and normal values will lead to significant drops in model accuracy [18,82]. As a result, the common practice is to adopt a larger bit-width, e.g., 8-bit or 16-bit, to quantize Transform-based models, compared to convolutional networks (CNNs).\n\nResearchers have proposed various quantization/architecture co-design works [39,61,75,82,85] to deal with the outliers in Transformer models. For example, outlier suppression [82] proposes to suppress the outliers. But it still has significant accuracy loss in the lower bit-width (4-bit), suggesting the difficulty in accommodating the effects of outliers. In addition, architecture researchers have designed sophisticated outlier-aware hardware architectures to store outliers with high precision to maintain model accuracy. These outlier-aware quantization frameworks divide the tensor into normal and outlier values, and encode them separately using different ways. For normal values, a dense matrix with low precision (e.g., 4bit) quantization is adopted. And the sparse and high-precision (e.g., 8-bit and 16-bit) outlier values can be compressed with sparsitybased encoding. Such encoding unfortunately leads to unaligned memory access. For example, GOBOs [85] and OLAccels [61] use the coordinate list to indicate the location of each outlier value in the matrix, as shown in Fig. 1a. BiScaled-DNNs [39] exploits block sparse indices format to store the outlier indices, and DRQ [75] uses the direct bitmap for outliers. These outlier-aware solutions require complex architectural designs with significant hardware overheads to accommodate outliers. Moreover, due to the random and unaligned memory access, the sparsity-based encoding is incompatible with the memory sub-systems of existing accelerators, such as GPU and TPU. Specifically, GOBO [85] can only de/compress weight tensors on the off-chip DRAM, it still relies on the original on-chip memory and computation architecture of GPU with high precision FP16/32.\n\nThe aforementioned outlier-aware architectures separate normal values from outliers in a global way. For instance, GOBO [85] involves a global sparse coordinate list in the quantization and computation, leading to a large hardware overhead and low performance benefits. In this work, we aim to design an architecture to handle outliers in a localized way with high hardware efficiency. To achieve that, we group two consecutive fixed-size values in a tensor and analyze their impact to model accuracy. There can be three kinds of pairs: i) a normal pair with two normal values, ii) one-outlier pair with one normal value and one outlier value, iii) two-outlier pair with two outlier values. We observe that the third two-outlier pair almost never shows up in well-trained LLMs. For the second one-outlier pair, we find that only keeping its outlier value while pruning its normal value (i.e., treating it as zero) is sufficient to maintain the model accuracy.\n\nBased on the above observations, we propose a novel outlieraware quantization architecture, called OliVe, based on the outliervictim pair (OVP) encoding. The salient feature of OliVe is memoryaligned and therefore hardware-friendly. As illustrated in Fig. 1b, OliVe first prunes normal values that are adjacent to the outliers as zero. These pruned normal values are called victims, which sacrifice themselves and make space for outliers. Then, we exploit the extra space provided by victims and embed the outliers into the low-precision matrix.\n\nOliVe is able to maintain a high accuracy for large Transformer models with a low hardware overhead due to the following reasons. First, OliVe incorporates victims to tackle outliers in LLMs. The effects of victims resemble model pruning [36]. Although clipping a few (0.1%) outliers will lead to a disastrous accuracy drop [18,82], pruning the same amount of \"normal\" values will only impact model accuracy slightly (< 0.1% drop). Therefore, OliVe sacrifices (\"prunes\") those insignificant values as victims for the outliers, allowing a more aggressive encoding scheme to accommodate extremely significant values. Second, the OVP encoding follows a specific outlier-victim (or victim-outlier) pattern to achieve memory alignment with little hardware overheads. Each victim is adjacent to an outlier, and the outlier-victim pair must align the memory access pattern. For example, in Fig. 1b, right outlier \u221298 in the OV pair needs a left victim, and left outliers 17.6 and 30.7 require the right victims. That can align 8-bit (1-byte) memory accesses with high efficiency. This design enables a completely localized outlier decoding/encoding process.   We normalize the maximum number by to plot the Max curve (left y-axis). The > 3 % and > 6 % (right y-axis) are the percentage of the values of > 3 and > 6 , respectively.\n\nTo implement OliVe, different data types are employed for outliers and normal values, which have different dynamic ranges and representation formats, including int4 and FP4. As shown in Fig. 1b, we propose a novel encoding method (Sec. 3) for the 4-bit OV pair, which composes a 4-bit outlier and a 4-bit victim into a special 8-bit format and differs from the original int8 or FP8. Due to its hardware-friendly and compatible design, OliVe can be easily integrated into existing quantization frameworks and accelerator architectures such as systolic array in Google TPUs [41] and tensor core in NVIDIA GPUs [58,60]. OliVe can also inherently support the mixed-precision and mixed-type architecture, showing its flexibility and practicality for larger-scale Transformer models.\n\nTo the best of our knowledge, OliVe is the first work pushing the limit of Transformer post-training quantization (PTQ) [4], which requires no retraining after quantization, to the 4-bit level for both the weight and activation tensors with the accuracy loss of < 1%. Surprisingly, OliVe's 4-bit PTQ accuracies for BERT [19] and BART [49] models outperform the 6-bit PTQ results of outlier suppression [82], a state-of-the-art Transformer quantization method. OliVe-based accelerator surpasses the existing outlieraware accelerators OLAccel [61] and GOBO [85] by 3.8\u00d7 and 4.5\u00d7 performance improvement, and 2.1\u00d7 and 4.0\u00d7 energy reduction, respectively. More importantly, the OliVe-based accelerator has more comprehensive and practical applicability than other outlierspecific architectures.\n\nWe make the following contributions in this paper.\n\n\u2022 We conduct the pair-wise importance analysis and show that outliers are important while their adjacent normal values are not, revealing the algorithmic opportunity of outlier-victim pair (OVP) that sacrifices the colocated normal values (called victims) to accommodate the outliers. \u2022 We propose the OVP-based quantization framework, called\n\nOliVe, which includes an efficient hardware encoding and novel outlier representation data type. \u2022 We propose the efficient architectural implementation and integration of OliVe quantization, and show that its efficiency and benefits outperform the existing outlier-aware quantization algorithms and hardware accelerators.\n\n\nMOTIVATION: ALIGNED OUTLIER\n\nIn this section, we first show that the outlier of the Transformer model is much more significant and important compared to convolution neural networks (CNN). Previous works [74,75,85,86] propose the outlier-aware quantization microarchitecture with adaptive bit length to accomplish the low-bit quantization but necessitate substantial hardware resources to deal with the variable-length data, which cause unaligned memory accesses and are incompatible with the memory sub-system of existing accelerators, e.g., GPU [60]. In contrast, we propose a memory-aligned and hardware-friendly method, called outlier-victim pair mechanism, which is inspired by DNN pruning and our outlier group location analysis for Transformers. We can prune some \"victims\" to make space to embed high-precision outliers into the memory-aligned low-bit tensor with ignorable accuracy loss.\n\n\nOutlier Matters\n\nWe visually demonstrate how significant the Transformer's outlier is in Fig. 2. We adopt the empirical 3 rules [83] of the normal distribution to divide the values into outlier and normal values. We employ the ResNet-18 [37] as the representative for the CNN model and the BERT [19] for the Transformer model. We fit the DNN tensors with normal distribution, i.e., Equation 1, where is the value, is the mean, and is the standard deviation. We convert the tensor into a standard normal distribution.\nf ( ) = 1 \u221a 2 \u2212 1 2 ( \u2212 ) 2(1)\nWe collect all tensors' maximum values and normalize them by the (Max ). We sort and plot the tensors by their Max in Fig. 2.\n\nMost tensors can fit the normal distribution 3 rules, i.e., about 99.7% of the values lie within three standard deviations of the mean. The outlier (> 3 ) ratio of most tensors is lower than 0.5%, and the values of > 6 are extremely few in tensors. Therefore, normal values are relatively concentrated, indicating that we can quantize the normal values with a narrow range to enhance the resolution of quantization.\n\nThe more obvious observation is that the Max of the Transformer is larger than that of CNN by one order of magnitude. Some research [14,43] shows that although the outliers are clipped for CNN models, the accuracy can still be restored to the original value with the retraining algorithm under ultra-low-bit precision, e.g., 4bit. However, it is challenging for Transformer models, which have much more significant outliers. The state-of-the-art quantization works [18,82] also demonstrate a similar observation and only can achieve the original accuracy with higher-precision quantization for large-scale Transformer models due to the outliers. Therefore, keeping the outlier without clipping will significantly benefit quantizing Transformer models.\n\n\nOutlier Is Unaligned\n\nThe importance of outliers has attracted many research interests, which sparked several outlier-aware architectures, as depicted in Tbl. 1. OLAccel [61] and GOBO [85] are similar and exploit the coordinate list to indicate the location of outliers, which use highprecision (8-bit or 16-bit) quantization. BiScaled-DNN [39] and DRQ [75] employ block sparse index and bitmap, respectively.  In summary, prior works design the outlier-aware architecture based on the sparsity of outliers, which leads to unaligned memory storage and accesses. More seriously, the indices of sparsity-based encoding and the outliers are separate. As such, they need the extra outlier controller to parse indices for the outliers and orchestrate the computation between normal values and outlier values. For example, the extra outlier controllers of GOBO and OLAccel count up to 55% and 71% overhead to the total area of the processing element (PE) array [61,85]. The sparsity-based encoding for outliers is also incompatible with the memory sub-system of existing accelerators. For the GOBO design [85], it can only compress and decompress the memory at the DRAM level for GPU. This greatly limits the applicability of its proposed outlier-aware architecture.\n\nTherefore, a more hardware-friendly and applicable outlier decoding/encoding method should be proposed to fit the outlier-aware quantization. Our proposed OliVe architecture is able to align memory accesses and is also compatible with existing accelerators based on the OVP mechanism.\n\n\nOutlier and Victim Analysis\n\nGenerally, the sparsity-based encoding borrowed from DNN pruning is a straightforward and effective solution for sparse outliers. However, these works ignored that quantization is different from pruning. For pruning, the pruned zero values do not participate in the computation. As such, the pruning method has to compress   the sparse values with sparsity-based encoding. For quantization, the quantized normal values are the majority and need computation. Naturally, the outlier values can exploit the normal values to achieve memory alignment instead of sparsity-based encoding.\n\nAs depicted in Fig. 1b in Sec. 1, we employ the insight of pruning but with a different perspective from prior works. The new method employs the outlier-victim pair (OVP) mechanism. We first prune some quantized low-precision normal values, which we call victims. These victims are adjacent to the outliers and make extra space for the high-precision outliers. Therefore, we can embed the outliers in their original location without explicit sparse indexing. That can avoid the complex indexing hardware and make it compatible with GPU. To align the memory, we distinguish the \"right outlier\" and \"left outlier\" according to their position in the pair. We assign a right victim for the left outlier (e.g., 17.6 in Fig. 1b) and a left victim for the right outlier (e.g., \u221298 in Fig. 1b).\n\nThe OVP mechanism is based on our observation of large Transformer models, including BERT-base [19], BERT-large [19], GPT2-XL [66], and OPT-6.7B [90]. We collect all tensors, calculate their standard variance , and divide the values into normal values (< 3 ) and outlier values (> 3 ) by the 3 rule. We then pair every two adjacent values (no overlapping), which leads to three types: normalnormal pair, outlier-normal pair, and outlier-outlier pair, as shown in Tbl. 2. These three types have two normal values, one normal value and one outlier value, and two outlier values, respectively.\n\nTbl. 2 demonstrates that most (about 99%) pairs are normalnormal pairs, with only around 1% of outlier-normal pairs. Outlieroutlier pairs need to prune the smaller outlier in the pair. Fortunately, the outlier-outlier pairs only have an extremely low probability of less than 0.06% in all studied models. Therefore, the outlier distribution is extremely dispersed, and we can retain most outliers.\n\nWe also conducted the accuracy experiments with the BERT model [82] on the GLUE dataset [78], as depicted in Fig. 3. First, we clip the outliers to the 3 , where clipping is the common method adopted by quantization. Then, we prune the victims and normal values to zero. The victims are adjacent to the outliers, and normal values are randomly pruned with the same amount as the outliers. We keep the rest values with full precision (FP32). Although such few outliers (about 1%) are clipped, as shown in Fig. 3 clipping outlier, the accuracy loss is unacceptable for the BERT model. The results emphasize the importance of outliers in Transformer-based model. For comparison, pruning random normal values has almost no accuracy loss than the source accuracy. The pruning of victim values only shows a negligible accuracy decrease than the pruning of normal values because the victims include some outliers due to  the outlier-outlier pair and have specific locations corresponding to the adjacent outlier.\n\nIn summary, our analysis indicates that outliers are important while the victims are not, so that we can sacrifice victims to accommodate the outliers. This motivates us to design the hardwarefriendly OVP mechanism that provides aligned outlier-aware quantization to accelerate the large Transformer models. In the next section, we will introduce the outlier-victim pair encoding design.\n\n\nOUTLIER-VICTIM PAIR ENCODING\n\nIn this section, we present the details of outlier-victim pair (OVP) encoding that is globally identical but locally distinguishable for outlier and normal values. The OVP encoding can maintain globally aligned memory access and distinguish the outliers locally with ignorable overhead. For normal values, we can support multiple data types to fit the adaptive data type. For encoding outliers, we design an outlier-specific data type, adaptive bias float, abfloat, which can avoid range overlapping between normal values and outliers, thus improving the utilization ratio of the numerical representation space of outlier encoding. Finally, based on the OVP encoding, we propose a framework that can automatically select the outlier threshold for OVP encoding to determine a suitable ratio of the outlier-victim pair.\n\n\nOVP Encoding Algorithm\n\nBased on the previous pair-wise tenor value analysis, there are three pair types: normal-normal, outlier-normal, and outlier-outlier. For outlier-normal, the normal value in the pair will be pruned and turned into a victim. For outlier-outlier, we remain the large one  Outlier Identifier. To distinguish from the normal-normal pair, we need a special identifier for the outlier-victim pair. And this distinct identifier cannot appear in the normal-normal pair, which means we need to eliminate one number in the representation of normal values. For example, as shown in Fig. 4, we employ the signed int4 (4-bit integer) for the normal value quantization. The original int4 can represent the integers in the range of [\u22128, 7], where 1000 2 represents the value of \u22128. First, we make 1000 2 the outlier identifier and remove the value of 1000 2 from int4, whose encoding range becomes [\u22127, 7]. Second, we quantize the outlier-victim pairs with 4-bit OVP encoding. We set the victims with the outlier identifier 1000 2 and quantize the outlier with the outlier-specific data type (Sec. Algo. 1 shows the 4-bit OVP encoding algorithm, which needs to read two values simultaneously, where the requirement is very easy to meet. For the hardware implementation, we can add a buffer for the encoder. Also, the OVP encoder can be implemented by embedding in the quantization unit with ignorable overheads. For the software implementation, we can make a thread handle two values simultaneously. As a result, the encoding algorithm can be implemented efficiently in both hardware and software, which we describe more details later.\n\n\nData Type for Normal Values\n\nFor normal values, we build upon prior work [32], which can support multiple data types, including int4, flint4 (4-bit flint), and int8, as shown in Tbl. 3. The int4 type is one of the most widely used data types for 4-bit quantization with integers in the value range of [\u22127, 7]. The flint4 type is proposed by prior work ANT [32], which has shown that selecting the data type according to a tensor's distribution achieves the state-of-the-art performance and accuracy.\n\nBased on the above insights, we also adopt the mixed data types to quantize normal values in our OVP pair encoding. For flint4, we use the same binary value of 1000 2 as the outlier identifier. Specifically, 1000 2 of flint4 corresponds to \u22120, which is not used in the original design. In other words, our OVP encoding seamlessly works for flint4 without wasting any number representations. We use the original flint4 encoding algorithm [32] (-0) int8 0, \u00b11, \u00b12, \u00b7 \u00b7 \u00b7 , \u00b1126, \u00b1127 10000000 2 (-128)  value also needs to eliminate one number. For instance, int8 can represent [\u2212128, 127] integers, and we can make 10000000 2 the outlier identifier for int8 and narrow its range to [\u2212127, 127].\n\nSimilarly, the encoding algorithm can easily extend to read two 8-bit elements simultaneously.\n\n\nData Type for Outliers: Abfloat\n\nNext, we quantize outliers using the outlier-specific data type. The large outliers usually have a wide range, for which we use floatbased data to quantize. We propose a data type called adaptive biased float, abfloat in short. The key idea is that by adding a proper bias to the exponent, all encoded values can skip the interval where normal values lie and provide more range for outliers.\n\nFloat-to-Fixed Conversion. To accommodate the normal values and avoid fractions, we first convert the floating-point encoding to the fixed point with an exponent. Also, the fixed point is friendly to the hardware implementation and has a lower overhead than the floating point. We transform the the floating point to fixed point with the following equation,\nsign \u00d7 (1 \u226a mb + mantissa) \u226a (exponent + bias),(2)\nwhere mb is the mantissa bit-width. Therefore, this fixed-point encoding scheme is more friendly and efficient for hardware implementation, as it only involves shift operations. Tbl. 4 shows the example of fixed-point E2M1 data type.\n\nAdaptive Bias. Obviously, Tbl. 3 and Tbl. 4 show that the range of fixed-point abfloat overlaps with the normal values. For example, int4 and E2M1 contain the same numbers, 3, 4, and 6. Another example is that flint4 and E2M1 have almost the same number range except for 24. Therefore, we need the adaptive bias to adjust the range of abfloat. For example, we set bias = 2 for E2M1, whose real values will be extended to {12, \u00b7 \u00b7 \u00b7 , 96}, which is complementary with the int4 normal value. Similarly, we set bias = 3\n\nBinary Exponent Integer Real Value // Get exponent and base integer.\n000 0 0 0 001 0 3 3 \u00d7 2 0 = 3 01x 1 2, 3 2 \u00d7 2 1 = 4, 3 \u00d7 2 1 = 6 10x 2 2, 3 2 \u00d7 2 2 = 8, 3 \u00d7 2 2 = 12 11x 3 2, 3 2 \u00d7 2 3 = 16, 3 \u00d7 2 3 = 242 = \u230a 2 ( ( )) \u230b \u2212 1; 3 _ = [ /2 ]; 4 if _ == 4 then 5 = + 1; 6 _ = _ \u2212 2;\n// Encoded as abfloat data type.\n7 = \u2212 ; 8 _ = _ & 1; 9 _ = ( , _ ); 10 = ( < 0, _ ) 11 return\nand extend range to {24, \u00b7 \u00b7 \u00b7 , 192} for flint4 data type. We design a new decoder and instruction to implement adaptive bias in accelerators for the abfloat (Sec. 4.2).\n\nE2M1 Abfloat. The 4-bit signed float has four possible configurations of exponent and mantissa: E0M3, E1M2, E2M1, and E3M0. They have different ranges and precisions. We conduct the following experiments to choose the most appropriate configuration as the final outlier-specific data type. To accommodate the broad range of outlier values, we quantize the largest outlier values (i.e., Max in Fig. 2) in Transformer models using all abfloat types. Then, we collect the average absolute error, as shown in Fig. 5. We found that E2M1 gives the least error in all tests, which provides both a large enough range and a certain degree of precision, and it also presents the best results in our subsequent evaluations. Similarly, we adopt signed E4M3 for 8-bit abfloat.\n\nAlgo. 2 shows in detail how an element is encoded as abfloat. The outlier encoding is an element-wise function, which can be implemented on software and hardware efficiently. Outlier encoding should also eliminate the outlier identifier. Otherwise, the decoder cannot distinguish the outlier-victim pair. Abfloat has two zero numbers: 1000 (-0) and 0000 (0). Therefore, we disable the 1000 and 0000 for outlier values to avoid conflict with the outlier identifier.\n\n\nQuantization Framework\n\nWe now apply OVP (outlier-victim pair) encoding for quantizing Transformer models. To decide the scale factor (i.e., outlier-victim threshold), we embed the OVP encoding with the existing mean squared error (MSE) minimization algorithm, which is commonly used by many quantization works [4,6,88]. The OVP-based quantization algorithm determines the threshold for distinguishing outliers and normal values. On one hand, a small threshold would lead to more outlier-victim pairs, which could potentially minimize the quantization error (i.e., MSE). On the other hand, it also increases the ratio of outlier-outlier pairs, where both values are outliers in the pair. If there are too many such outlier-outlier pairs, the MSE would increase owing to the pruning of outliers. Thus, we need to control the ratio of outlier-outlier pairs for better accuracy.   In our work, we target the post-training quantization (PTQ) [57], which does not require retraining and hence is best suitable for large models as their trainings are expensive. However, we still need to use one batch of data from the training set for the scale factor selection. Intuitively, inspired by the 3 rule, we take 3 as the initial scale factor. Then the algorithm will search for the best scale factor with the smallest MSE within a specific range of this baseline, which shows good results in our evaluations. For quantization-aware training (QAT) [57], we can get a suitable scale factor by retraining it with the straight-through estimator (STE) [5].\n\n\nOLIVE ARCHITECTURE\n\nThis section presents how to integrate OliVe in GPU and outputstationary systolic array architecture. We then present the hardware decoder for the aforementioned outlier-victim pair encoding and outlier data type. On these architectures, our proposed OliVe architecture can directly support the mixed precision [60,72] and mixed data type [60,72], which are efficient for quantizing DNN tensors that have different importance and distribution.\n\n\nGPU Tensor Core\n\nWe first describe how to integrate the OliVe design into the tensor core architecture of GPU in the Fig. 6a. We employ Turing architecture [59] as our baseline GPU, which has 68 streaming multiprocessors (SMs), and each SM has eight tensor cores (544 in total), as shown in Tbl. 5. According to the modeling of prior work [67], each tensor core has two octets, which have eight FEDPs (fourelement dot product). As such, there are 68 \u00d7 8 \u00d7 2 \u00d7 8 \u00d7 4 = 34, 816 16-bit float multipliers. The Turing architecture can originally support mixed-precision computation. For example, the RTX 2080Ti GPU with Turing architecture [59] provides 107.6, 215.2, and 430.3 TOPS (tera operations per second) for 16-bit float, 8-bit int, and 4-bit int, respectively. Therefore, we assume that the tensor core can simultaneously support 8-bit 8EDP (eight-element dot product) and 4-bit 16EDP (16-element dot product), as shown in Fig. 6a.\n\n\nArchitecture SM TC 16-bit Unit 8-bit Unit 4-bit Unit\n\nTuring [59] 68 544 34,816 69,632 139,264 We can easily embed our proposed OliVe architecture in GPU, which adopts the SIMD architecture. We first put the 4-bit outliervictim pair decoders (Fig. 6b) for each 16EDP. To support the new OliVe data types, we add an adder and a shifter for each 16EDP. Similarly, we also design the 8-bit decoder for the 8EDP units.\n\n\nDecoders\n\nOutlier-Victim Pair Decoder. To support outlier-victim pair decoding, we design a new decoder that can be easily embedded in existing accelerators. As shown in Fig. 6b, the decoder reads 1 byte, which is the smallest addressable memory unit in many architectures, and exactly one value pair. Then, the decoder transforms the outlier identifier 1000 2 to 0 and decodes the outlier value with the outlier decoder. To accommodate the computation of the outlier abfloat values, the decoder will generate an exponent-integer pair. Therefore, the decoder needs to append a 0000 2 as the exponent number for the normal int4 data type. For flint4, we exploit its original decoder [32] to get the exponent-integer pair.\n\nOutlier Decoder. The above OVP decoder contains an outlier decoder for outlier values with the E2M1 abfloat data type. Fig. 7 shows the details of the 4-bit abfloat decoder design. For a 4-bit E2M1 abfloat number = ( 2 1 0 ) 2 , following equations decode exponent and integer:  For example, when the bias is 2, a number 0101 2 is 48 10 , since its exponent is 2 10 + 10 2 = 4 10 and base integer is 11 2 = 3 10 . Therefore, its real value is 3 \u226a 4 = 48.\nexponent = bias + (b 2 b 1 ) 2 integer = 0 = 000 2 (1 0 ) 2 \u210e Input\nSimilarly, we also design and implement the 8-bit outlier-victim pair decoder and the E4M3 abfloat outlier decoder, which are straightforward extensions of 4-bit instances. As such, we do not present their details due to the limited space.\n\n\nSystolic Array\n\nThe systolic array (SA) integration is shown in Fig. 8. SA uses the same outlier-victim pair decoder design (Fig. 6b) as GPU, which shows the wide applicability of our design. But, unlike GPU, we only place the decoders along the borderlines, which can save most decoders. For example, if the array size is \u00d7 , we only need + instead of \u00d7 decoders. That is one advantage of SA over the GPU's SIMD architecture. Our proposed OliVe-based data type can also support the systolic array processing element (PE) with an extra adder and shifter. We add an extra adder for every four PEs to support high-precision quantization, e.g., int8.\n\n\nOliVe MAC unit\n\nAfter decoding for outlier and normal values, they are all transformed into unified exponent-integer pairs. To support the decoded exponent-integer pair computation, we need to add a shifter and an adder for the fixed-point MAC (multiply and accumulation) unit, as shown in Fig. 8 and the unit of Fig. 6 4-bit 16EDP. For example, we have two exponent-integer pairs < , > and < , >, where and are exponents, and are integers, and < , > represents:\n< , >= \u226a\nThen, we can get the result:\n< , > \u00d7 < , > = ( \u00d7 ) \u226a ( + ) = < + , \u00d7 >\nNote that the final result can store with a 32-bit int.\n\n\nMixed Precision\n\nAs mentioned in Sec. 3, OliVe quantization can support the int8 for normal values and E4M3 abfloat for outlier values. Therefore, we propose the mixed-precision processing element (PE) for the higher precision data types.\n\n\n8-bit Int.\n\nFor the GPU tensor core architecture, it is originally designed with mixed-precision computation. For the systolic array, our architecture naturally supports 8-bit computation with four 4-bit PEs [72]. For an int8 number , the higher 4 bits and the lower 4 bits can be split into two 4-bit numbers \u210e and , and the can be represented by:\n= (\u210e \u226a 4) + =< 4, \u210e > + < 0, > .\nWe then can multiply two int8 numbers of and :\n\u00d7 = < 4, \u210e > \u00d7 < 4, \u210e > 0 + < 4, \u210e > \u00d7 < 0, > 1 + < 0, > \u00d7 < 4, \u210e > 2 + < 0, > \u00d7 < 0, > 3\nTherefore, we can use four 4-bit PEs to calculate the above four multiplications and accumulate the products to get the final product value of \u00d7 .\n\n\n8-bit Abfloat\n\nSimilarly, multiplication of 8-bit abfloat can be supported using the same approach. For an 8-bit abfloat number , it is first decoded into an exponent and an integer . For , we similarly split it into = (\u210e << 4) + , then =< 4 + , \u210e > + < , >. Hence the same method can be used to perform 8-bit abfloat multiplication with four 4-bit PEs, where the abfloat has an extra than int8.\n\nIn the most extreme case, two outliers with abfloat may be multiplied together. Because we adopt the 32-bit int as the accumulator, the maximum multiplicand should not be over \u221a 2 31 \u2212 1. Therefore, for the outlier value with the abfloat type, we will clip the absolute value of the outlier within 2 15 < \u221a 2 31 \u2212 1 to avoid the overflow for the int32 accumulators. Our experiments show that the outlier values of the Transformer models are much smaller than 2 15 . Specifically, 2 15 is about 768 after normalization and quantization. As shown in Fig. 2, the maximum value of outliers does not exceed 325 . Thus, we observe that no outlier is truncated in practice.\n\n\nInstruction Set\n\nFor 4-bit tensor cores, the Turing GPU architecture adopts the instruction mma.s32.s4.s4.s32. These four operands are matrices (int32), (int4), (int4), and (int32), and = \u00d7 + . To support the OVP-based computation on GPU, we design a new instruction called mmaovp:\nOVP\u2212MMA .s32. 4 int4 . 4 flint4 .s32. 4 bias .\nMoreover, because of the memory-aligned design of the data type, OliVe maintains the original programming interface for GPUs. We can replace the original int-based instruction with OVP-based instruction (e.g., mmaovp) to easily construct the OVP-supported DNN quantization framework. Therefore, our OliVe framework has comprehensive and practical applicability, which is the most significant advantage of OliVe.\n\n\nEVALUATION\n\nIn this section, we evaluate the LLM's accuracy with OliVe quantization. We also demonstrate OliVe's area overhead, speedup, and energy efficiency on GPU and systolic array, respectively.\n\n\nMethodology\n\nFramework and Evaluation Models. To evaluate our OliVe quantization framework, we implement it in Pytorch [62]. We evaluate BERT-base [19], BERT-large [19], and BART-base [49], the three most commonly used language models, on eight datasets of the GLUE benchmark [78]. In addition, we evaluate BERT-base [19] and BART-base [49] on the summarization tasks SQuAD v1.1 and SQuAD v2.0 [68]. To valid our quantization framework on large language models, we also evaluate GPT2-XL [66], BLOOM-7B1 [70], and OPT-6.7B [90] on Wikitext103 [83] and C4 [20] datasets. For all models mentioned above, we use state-of-the-art checkpoints from the huggingface repositories [55].\n\nQuantization Baselines. We compare OliVe with existing quantization works, including GOBO [85], Outlier Suppression [82], Q8BERT [86], and ANT [32]. Outlier suppression [82] is the stateof-the-art Transformer quantization work. GOBO [85] is also an outlier-aware quantization work. Q8BERT [86] is a method for quantizing GEMM operations to 8-bit. ANT [32] is a hardware-friendly quantization framework that achieves state-of-the-art results in both performance and accuracy.\n\nAccelerator Baselines. We compare the performance and energy of OliVe against five DNN quantization accelerators, including OLAccel [61], AdaptivFloat [76] (shorted as AdaFloat), GOBO [61], ANT [32], and original int8 tensor cores in GPU [59]. OLAccel [61] first proposed the outlier-aware quantization architecture for CNNs. We extend OLAccel to the Transformer-based models with element-wise mixed-precision weight and activation quantization. AdaFloat [76] extends the float type with a tensor-wise exponent bias. GOBO [85] is similar to OLAccel, but only supports the weight quantization for Transformer-based networks.\n\nOliVe Implementation. We implement our decoder in Verilog RTL and synthesize it with Synopsys design compiler [47] with a 22 nm TSMC technology library to estimate its area, latency, and power. We use CACTI [56] to estimate the area and power of on-chip memories. We integrate OliVe into GPU and hardware accelerator for the end-to-end performance and energy evaluation.\n\nFor the GPU integration and evaluation, we modify and extend GPGPU-Sim 4.0 [3] and AccelSim [45] with the configuration of NVIDIA 2080 Ti architecture. We use AccelWattch [46], GPUWattch [48], and CACTI [56] for the energy estimation. The majority of Transformer layers are matrix multiplication operations. For GEMM implementation on the tensor core, we use CUT-LASS [44], which is NVIDIA's open-source implementation.\n\nFor the accelerator evaluation, we compare AdaFloat, OLAccel and ANT with OliVe. We develop a cycle-level simulator to estimate the overall performance of OliVe based on DnnWeaver [71]. Although DnnWeaver [71] is a FPGA tool set, prior DNN quantization accelerators, which include the BitFusion [72], and ANT [32], have extended its frontend to add the ASIC performance and energy simulation. As OliVe does not redesign the baseline accelerator   [72,73], and ANT [32,33].\n\n\nAccuracy Results\n\nWe first evaluate the accuracy of OliVe quantization framework on different tasks and datasets, which is the prerequisite for applying it to reduce the inference cost of large language models (LLMs).\n\n\nGLUE Dataset.\n\nWe evaluate BERT-base [19], BERT-large [19] and BART-base [49] on eight datasets of GLUE benchmark, but due to space limitation, we only show the results on CoLA, SST-2, MNLI, QQP and MRPC datasets in Fig. 6. For the BERT-base model, our 4-bit PTQ method accuracy drop less than 1% compared to the original full precision model on all eight datasets and outperforms all studied methods including 4-bit, 6-bit, and 8-bit PTQ and QAT methods. Since GOBO [85] only quantizes weights, we use the same method to compare with it and the result is shown in Tbl. 7. Our method also outperforms the GOBO under the weight-only quantization setting. In addition, we evaluate the BERT-large model, which is evaluated by few prior quantization works due to the larger number of parameters and hence much more challenging compared to BERT-base. The results in Tbl. 6 show the accuracy loss for BERTlarge is around 1% on the five presented datasets and similar results are found on other datasets. For the BART-base model, our 4-bit PTQ results in Tbl. 6 show around 2% accuracy loss compared to the accuracy of original full-precision in all datasets. In the above evaluation, our 4-bit PTQ results are better than all the PTQ and most of the QAT results reported by prior works.\n\nSQuAD Dataset. We also evaluate the accuracy of OliVe quantization on summarization task SQuAD [68], which is more challenging than the previous GLUE dataset. Tbl. 8 shows the results on SQuAD v1.1 and SQuAD v2.0 datasets. On both datasets, our 4-bit PTQ method obtains a less than 2% accuracy loss on the BERTbase model and around 3% accuracy loss on the BART-base model, which is better than the 6-bit PTQ method of the state-of-the-art quantization work outlier suppression.\n\nLarge Language Models. We evaluate the accuracy of OliVe for LLMs under the PTQ setting. LLMs' inference is challenging as it requires significant memory, which makes their retraining even more resource-consuming. Thus, the PTQ method without retraining is more desirable than the QAT method for LLMs.\n\nThe recent work [18] has shown that the int8 quantization has a significant accuracy drop when the number of parameters of the OPT model grows to 6.7B. As shown in Tbl. 9, our 8-bit PTQ method has only a negligible perplexity increase on OPT-6.7B (lower is better), while the accuracy of the int8-based quantization method has a significant degradation and is worse than our 4-bit PTQ method on the C4 dataset. On GPT2-XL and BLOOM-7B1 models, our 8-bit PTQ method essentially achieves the original perplexity, and the 4-bit PTQ method achieves the performance close to int8. For comparison, the accuracy results of int4 and 4-bit ANT are unacceptable (10-1000\u00d7 worse than FP32 model).\n\nTo summarize, our OliVe quantization framework pushes the limit of 4-bit quantization to a new state-of-the-art, as it is able to achieve nearly original accuracy for the commonly used language models including BERT-base, BERT-large, and BART-base on most datasets. Moreover, OliVe also gives the state-of-the-art results of 4-bit and 8-bit quantization on large language models like GPT2-XL, BLOOM-7B1, and OPT-6.7B.\n\n\nGPU Performance and Energy\n\nWe evaluate LLMs on the GPU simulator, where the batch size is set to 2 for GPT-like models and 16 for BERT-like models. For OliVe, 4-bit quantization can limit the loss to a relatively small error range. GOBO [85] can achieve the original accuracy of all models but has a significant overhead on compressing weight in DRAM. Note that GOBO only quantizes the weight tensors and computes with FP16. We implemented GOBO's memory organization in the GPU. For ANT [32], we make all models close to the original accuracy or perplexity by mixed precision (BERT-like models [19,49] with < 1% loss and GPT-like models [66,70,90] with < 3 perplexity) with the PTQ setting. In addition, we also compare the original int8 of GPU, which has unacceptable accuracy loss, just for performance and   energy comparison to GPU baseline. We compare the GPU architecture integrated with our OliVe design against various baselines. The performance and energy results are shown in Fig. 9.\n\nPerformance. Fig. 9a compares the speedup values of different quantization methods on GPUs. OliVe achieves the best performance and has higher speedups on the larger language models than GOBO. Due to the FP16 computation and weight-only quantization, GOBO [85] achieves the lowest performance among all studied designs. In contrast, OliVe quantizes both activation and weight to low bits and does not increase the memory access overhead. This avoids performance degradation when the number of parameters increases. The PTQ seriously degrades the accuracy of ANT [32] as it cannot handle outliers. In ANT, 80% of layers ends up using int8 quantization so the performance results between ANT and int8 are close. On average, OliVe achieves 4.5\u00d7, 2.7\u00d7, and 2.4\u00d7 speedup values over GOBO, int8, and ANT, respectively.\n\nEnergy. Fig. 9b shows the normalized energy comparison of different designs, including constant, static, and dynamic power. And the dynamic power includes DRAM, L2 cache, L1 data cache, shared memory, register file, and processing elements (CUDA core and tensor core). The L1 contains the sum of the L1 cache and shared memory energy. OliVe has the lowest energy due to the aligned 4-bit design and GPU compatibility. Due to the worse accuracy result of the mixed precision, ANT is also close to int8 on the energy. Overall, 4-bit OliVe is very hardware-friendly so that it can take full advantage of the energy savings with lower bits. OliVe achieves average 4.0\u00d7, 2.3\u00d7, and 2.0\u00d7 energy reduction over GOBO, int8, and ANT, respectively.\n\nArea. To measure the overhead of OliVe decoder on the GPU, we scale the OliVe decoder to 12 , which is the same manufacturing process as RTX 2080 Ti [59] Table 9: PTQ results on large language models. The accuracy metric is perplexity, and lower is better.   on the GPU die and their area is shown in Tbl. 10. Since the GPU die size of RTX 2080 Ti is 754 2 , the 4-bit decoder and 8-bit decoder only account for 0.250% and 0.166% of the entire GPU area respectively, which we believe is a tiny and worthy overhead.\n\n\nAccelerator Performance and Energy\n\nAs explained in Sec. 5.1, we also integrate OliVe to the systolicarray-based hardware accelerator and compare its performance and energy against existing designs of ANT [32], OLAccel [61], and AdaFloat [76]. Similar to its GPU implementation, ANT is a mixedprecision design. Since AdaFloat does not support mixed precision, we only provide the 8-bit quantization results. All accelerators can achieve close to original accuracy for all Transformer models.\n\nPerformance. As shown in Fig. 10a, OliVe has the most significant advantage in latency speedup. Owing to its inability to deal with outliers, the performance of ANT is similar to OLAccel on most models. The speedup values of OliVe are very similar on all models, and they do not change with the increasing number of model parameters. On average, OliVe achieves 4.8\u00d7, 3.8\u00d7, and 3.7\u00d7 speedup value over AdaFloat, OLAccel, and ANT, respectively.\n\nEnergy. Fig. 10b shows the normalized energy consumption of different designs composed of static and dynamic energy (DRAM, on-chip buffer, and core). OliVe has the lowest energy consumption. Compared to OLAccel, OliVe has a significant advantage in terms   of static and DRAM. Worse mixed-precision results increase ANT energy consumption, which is even close to AdaFloat in BLOOM-7B1 model. On average, OliVe achieves 3.7\u00d7, 2.1\u00d7, and 3.3\u00d7 energy reduction over AdaFloat, OLAccel, and ANT, respectively.\n\nArea. Tbl. 11 shows the area breakdown of OliVe-based systolic array architecture under 22 process. In this scenario, the 4-bit and 8-bit decoders introduce about 2.2% and 1.5% overhead of the core area, respectively, which is inconsiderable compared to the area of PEs in the array. Considering on-chip memory structures, the overall area overhead would be even smaller. In addition, we also scale other accelerators to 22 using DeepScaleTool [69] and get similar results to those numbers. Note that we implement all accelerators with a similar area size. The small area overhead of our OliVe directly benefits from the carefully-designed outlier-victim pair (OVP) encoding.   [11,40,91,92,95,97] and scheduling [8, 9, 15-17, 31, 52-54, 84]. The DNN acceleration highly relies on the performance of matrix multiplication. Therefore, several works focus on improving data reuse and simplifying control logic through a tailored dataflow architecture for matrix multiplication [10,12,25,34,35,41,63,64,87,96]. TPU [41] introduces a highly optimized dataflow architecture that efficiently reuses data across multiple computation stages. Modern GPUs [60] now incorporate matrix multiplication accelerators, such as tensor core, optimized for SIMD operations to enhance DNN workload acceleration further.\n\n\nRELATED WORK AND DISCUSSION\n\nPruning. Pruning means removing a portion of weight, input, or output of DNN layers, resulting in a sparse model with reduced model size. However, a significant reduction leads to irregular memory accesses, which are negative for the acceleration of inference and training. To address this issue, researchers propose several sparse optimizations in algorithms and hardware architectures to reduce inefficient computation [2, 26-29, 36, 64, 65, 80, 89, 94, 98]. In addition, a sparse tensor core is introduced in NVIDIA Ampere GPU architecture [1] to support the 2:4 structured sparsity.\n\nQuantization. Quantization is another effective and efficient way to reduce the DNN model size and computation burden. There are two popular quantization methods, i.e., quantization-aware training (QAT) [38,50,81,99] and post-training quantization (PTQ) [30,35,38,81]. QAT allows the model to adapt to quantization noise by retraining. PTQ is very effective to implement since it converts the original FP32 model directly into a lower-bit model without the training data and pipeline. Thus, PTQ is more feasible for language models at billion scales.\n\nBy quantizing data to low bit-width, quantization accelerators can significantly reduce memory bandwidth requirements and increase the computation speed. BitFusion [72] combines the low-bit PEs to support different bit-width quantization. OLAccel [61] utilizes 16-bit MAC to the first layer and 4-bit MAC to other layers. DRQ [75] quantizes data in sensitive and insensitive areas with different precision, which is similar to outlier-aware quantization. GOBO [85] is an accelerator that takes advantage of outlier-aware quantization, which quantizes the outliers of weights with higher precision. However, the outlier-aware quantization accelerators mentioned above have unaligned memory accesses, resulting in additional overhead and a limited computing speed. ANT [32] proposes a fixed-length adaptive quantization framework but only takes the distribution of tensors into account and ignores the importance of outliers. In contrast, our proposed novel OliVe quantization framework can handle outlier values in a memory-aligned and hardware-friendly way.\n\nAdaptivFloat [76] is similar to abfloat in adding a bias to the exponent, but the motivations and how the bias is determined are different. AdaptivFloat is to adapt to the dynamic ranges of different layers and calculates the optimal bias at a layer granularity using its algorithm. Our abfloat is to make full use of the encoding range, so it simply adds a uniform bias to all encoding values to skip the range of normal values, which is simpler to implement. GPU Architecture. NVIDIA has been updating its new generations of GPUs, e.g., Ampere architecture [1], which adds the sparse tensor core for structured sparsity in DNNs and compute data compression to increase the memory access bandwidth. The structured sparsity for tensor cores is orthogonal to our proposed quantization as our element-wise quantization does not affect (sparse) tensor core dataflow. Ampere GPU's compute data compression can compress zero values and similar bytes in DRAM and L2 cache. As such, it is lossless and therefore general-purpose. It is also transparent and orthogonal to OliVe, which does not modify the memory system. In contrast, prior quantization work [85] perform compression at the DRAM-level, which could be impacted by the data compression in Ampere GPUs.\n\nOn the other hand, DNN quantization is a lossy compression. We believe the strictly lossless compression would have limited benefits for DNN quantization. Thus, our work could complement Ampere's current compute data compression as a special-purpose solution. Since existing GPU simulators [3,45] cannot support data compression, we will continue to follow up and study this problem in the future work.\n\n\nCONCLUSION\n\nIn this work, we propose a novel outlier-victim pair (OVP) quantization, which can handle outlier values with low hardware overhead and achieve high performance gains. The key insight is to sacrifice the normal values next to those essential outliers (called victims) to accommodate them. The OVP encoding designed based on this idea is able to make outliers and normal values globally identical but locally distinguishable. To the best of our knowledge, OliVe pushes the limit of 4-bit quantization to a new state-of-the-art, as it is able to achieve nearly original accuracy for commonly used language models. Moreover, our architecture design can be efficiently integrated into existing hardware accelerators such as tensor core and systolic array. Finally, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5\u00d7 speedup and 4.0\u00d7 energy reduction, respectively.\n\nFigure 1 :\n1Outlier-aware encoding comparison. (a) Prior quantization works adopt sparsity-based encoding that store normal and outlier values separately. (b) Our proposed outlier-victim pair encoding stores normal and outlier values locally.\n\nFigure 2 :\n2Outlier Comparison of CNN model and Transformer model. The is the standard deviation of the tensor.\n\nFigure 3 :\n3Accuracy comparison of multiple pruning methods.\n\nFigure 4 :\n4The 4-bit outlier-victim pair encoding.\n\n\nthe other. Then, we get the normal-normal pairs and outlier-victim pairs in the DNN tensors.\n\n\n3.3). Naturally, there are two types of OV pair, i.e., left outlier (O-V) and right outlier (V-O) pair. Due to the distinct outlier identifier design, we can implicitly distinguish them without using an extra index bit (Sec. 4.2).\n\nFigure 5 :\n5The rounding error of the largest outliers quantized with different data types. Experiments were conducted on BERT-base, BERT-large, BART-base, and GPT2-XL.\n\nFigure 6 :\n6OliVe integration on GPU tensor cores (a), which only requires a set of lightweight OVP decoder (b).\n\nFigure 7 :\n7The 4-bit abfloat decoder for outlier values.\n\nFigure 8 :\n8OliVe integration on systolic array.\n\nFigure 9 :\n9Comparison of four different designs on GPU.\n\nFigure 10 :\n10Comparison of different designs on accelerators.\n\nTable 1 :\n1Comparison between existing outlier-aware acceler-\nators and our proposed method OliVe. \n\nBiScaled-DNN quantizes all values with the same bit-width but \ndifferent scale factors for normal values and outliers, which are \naligned. However, the extra index compressed in the block sparsity \nmethod is unaligned. On the contrary, DRQ's bitmap is aligned, but \ndata is stored by mixed and thus unaligned 4-& 8-bit values. \n\n\nTable 2 :\n2The percentage of three types of pair.\n\n\nAlgorithm 1: The 4-bit OVP encoding algorithm.Input: Values, \n1 , 2 ; Outlier threshold, . \nOutput: OVP encoding, \n1 , \n2 . \n1 def OVPairEncoding( 1 , \n2 , ): \n\n2 \n\nif \n1 > and \n1 > \n2 then \n\n\n\n\nto quantize normal values.Moreover, the OVP encoding can be generally extended to higherprecision quantization, such as the 8-bit. Similarly, the 8-bit normalData Type \nValues \nOutlier Identifier \n\nint4 \n0, \u00b11, \u00b12, \u00b13, \u00b14, \u00b15, \u00b16, \u00b17 \n1000 2 (-8) \n\nflint4 [32] 0, \u00b11, \u00b12, \u00b13, \u00b14, \u00b16, \u00b18, \u00b116 \n1000 2 \n\nTable 3 :\n3Data types for normal values of OVP encoding.\n\nTable 4 :\n4The 3-bit unsigned E2M1, which means two bits for exponent and one bit for mantissa, with bias = 0.Algorithm 2:The abfloat encoding algorithm.Input: Element ; Bias, ; \nOutput: Quantized Element ; \n1 def AbfloatQuant( , ): \n\n\nTable 5 :\n5The Turing GPU architecture.\n\nTable 6 :\n6architecture, we can directly embed new OliVe-related instructions and data format in the simulator without breaking the original simulation flow. In other words, we have used and modified the opensourced implementaions of BitFusionResults on GLUE datasets. Q8 and OS are \nQ8BERT [86] and outlier suppression [82] for short, respec-\ntively. Prior works do not report results in BERT \nso we \nonly compare against the original full-precision model. \n\n\n\nTable 7 :\n7Comparison with GOBO on the MNLI and STSB \ndataset.  *  The accuracy of our GOBO implementation \nslightly differs from the number reported in the original pa-\nper [85]. \n\n\n\nTable 8 :\n8PTQ results on SQuAD datasets.\n\n\nBERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean.8 \n\n1 \n\n1.2 \n\nOliVe \nANT \nINT8 \nGOBO \nOliVe \nANT \nINT8 \nGOBO \nOliVe \nANT \nINT8 \nGOBO \nOliVe \nANT \nINT8 \nGOBO \nOliVe \nANT \nINT8 \nGOBO \nOliVe \nANT \nINT8 \nGOBO \n\nNorm. Energy \n\nConst \nStatic \nDram+L2 \nL1+Reg \nCore \n\n(b) Normalized energy on GPU. \n\n\n\nTable 10 :\n10The area of OliVe decoder on RTX 2080 Ti. BERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean BERT-base BERT-large BART-base GPT2-XL BLOOM-7B1 Geomean Norm. Energy Static Dram Buffer Core (b) Normalized energy on hardware accelerator.4.8 \n\n1.29 \n1.28 \n\n\n\n\nThis section presents and discusses research on DNN acceleration and compression. With the growing computation requirements of DNN models, it is crucial to design the algorithms and architecture to accelerate DNN models. Various compression methods, suchComponent \nNumber Area ( \n2 ) Area Ratio \n\n4-bit Decoder (37.22 2 ) \n128 \n0.00476 \n2.2% \n\n8-bit Decoder (49.50 2 ) \n64 \n0.00317 \n1.5% \n\n4-bit PE (50.01 2 ) \n4096 \n0.205 \n96.3% \n\n\n\nTable 11 :\n11Area breakdown of OliVe under 22 process.as pruning and quantization, have been proposed to exploit the redundancy property of DNNs.DNN Acceleration. In the past few years, various architectures[10,12,13,23,25,34,35,51,63,64,87,96] have been proposed to match the computation characteristics of DNN models. To accelerate the DNN system, most optimizations focus on compilation\nOliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization ISCA '23, June 17-21, 2023, Orlando, FL, USA\nACKNOWLEDGMENTSThis work was supported by the National Key R&D Program of China under Grant 2022YFB4501401, the National Natural Science Foundation of China (NSFC) grant (62222210, and 62072297, and 61832006). The authors would like to thank the anonymous reviewers for their constructive feedback for improving the work. We also thank Tailong Wangliu, Shuangjie Ruan for their continuous support.\nCnvlutin: Ineffectual-neuron-free deep neural network computing. Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, Andreas Moshovos, ACM SIGARCH Computer Architecture News. 44Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, and Andreas Moshovos. 2016. Cnvlutin: Ineffectual-neuron-free deep neural network computing. ACM SIGARCH Computer Architecture News 44, 3 (2016), 1-13.\n\nAnalyzing CUDA workloads using a detailed GPU simulator. ISPASS 2009 -OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization ISCA '23. Ali Bakhoda, George Yuan, Wilson Fung, Henry Wong, Tor Aamodt, Orlando, FL, USAAli Bakhoda, George Yuan, Wilson Fung, Henry Wong, and Tor Aamodt. 2009. Analyzing CUDA workloads using a detailed GPU simulator. ISPASS 2009 - OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization ISCA '23, June 17-21, 2023, Orlando, FL, USA\n\n10.1109/ISPASS.2009.4919648International Symposium on Performance Analysis of Systems and Software. International Symposium on Performance Analysis of Systems and Software, 163 - 174. https://doi.org/10.1109/ISPASS.2009.4919648\n\nPost training 4-bit quantization of convolutional networks for rapid-deployment. Ron Banner, Yury Nahshan, Daniel Soudry, Advances in Neural Information Processing Systems. 32Ron Banner, Yury Nahshan, and Daniel Soudry. 2019. Post training 4-bit quan- tization of convolutional networks for rapid-deployment. Advances in Neural Information Processing Systems 32 (2019).\n\nEstimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas L\u00e9onard, Aaron Courville, arXiv:1308.3432arXiv preprintYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013).\n\nZeroq: A novel zero shot quantization framework. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Zeroq: A novel zero shot quantization framework. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13169-13178.\n\nRethinking differentiable search for mixed-precision neural networks. Zhaowei Cai, Nuno Vasconcelos, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZhaowei Cai and Nuno Vasconcelos. 2020. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2349-2358.\n\nProphet: Precise QoS Prediction on Non-Preemptive Accelerators to Improve Utilization in Warehouse-Scale Computers. Quan Chen, Hailong Yang, Minyi Guo, Ram Srivatsa Kannan, Jason Mars, Lingjia Tang, 10.1145/3037697.3037700Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems. the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating SystemsXi'an, ChinaACMQuan Chen, Hailong Yang, Minyi Guo, Ram Srivatsa Kannan, Jason Mars, and Lingjia Tang. 2017. Prophet: Precise QoS Prediction on Non-Preemptive Acceler- ators to Improve Utilization in Warehouse-Scale Computers. In Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2017, Xi'an, China, April 8-12, 2017. ACM, 17-32. https://doi.org/10.1145/3037697.3037700\n\nBaymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers. Quan Chen, Hailong Yang, Jason Mars, Lingjia Tang, 10.1145/2872362.2872368Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2016. the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2016Atlanta, GA, USAACMQuan Chen, Hailong Yang, Jason Mars, and Lingjia Tang. 2016. Baymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers. In Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operat- ing Systems, ASPLOS 2016, Atlanta, GA, USA, April 2-6, 2016. ACM, 681-696. https://doi.org/10.1145/2872362.2872368\n\nDiannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, Olivier Temam, ACM SIGARCH Computer Architecture News. 42Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. 2014. Diannao: A small-footprint high-throughput accel- erator for ubiquitous machine-learning. ACM SIGARCH Computer Architecture News 42, 1 (2014), 269-284.\n\nTVM: An Automated End-to-End Optimizing Compiler for Deep Learning. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy, 10.5555/3291168.329121113th USENIX Symposium on Operating Systems Design and Implementation. Carlsbad, CA, USAUSENIX AssociationTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q. Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End Optimizing Compiler for Deep Learning. In 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018. USENIX Association, 578-594. https://doi.org/10.5555/3291168.3291211\n\nDadiannao: A machinelearning supercomputer. Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, 47th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE. Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, et al. 2014. Dadiannao: A machine- learning supercomputer. In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE, 609-622.\n\nEyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. Yu-Hsin Chen, Tushar Krishna, Joel S Emer, Vivienne Sze, IEEE journal of solid-state circuits. 52Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. 2016. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE journal of solid-state circuits 52, 1 (2016), 127-138.\n\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, I-Jen Pierce, Vijayalakshmi Chuang, Kailash Srinivasan, Gopalakrishnan, arXiv:1805.06085Pact: Parameterized clipping activation for quantized neural networks. arXiv preprintJungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. 2018. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085 (2018).\n\nLazy Batching: An SLA-aware Batching System for Cloud Machine Learning Inference. Yujeong Choi, Yunseong Kim, Minsoo Rhu, 10.1109/HPCA51647.2021.00049IEEE International Symposium on High-Performance Computer Architecture, HPCA 2021. Seoul, South KoreaIEEEYujeong Choi, Yunseong Kim, and Minsoo Rhu. 2021. Lazy Batching: An SLA-aware Batching System for Cloud Machine Learning Inference. In IEEE International Symposium on High-Performance Computer Architecture, HPCA 2021, Seoul, South Korea, February 27 -March 3, 2021. IEEE, 493-506. https: //doi.org/10.1109/HPCA51647.2021.00049\n\nEbird: Elastic Batch for Improving Responsiveness and Throughput of Deep Learning Services. Weihao Cui, Mengze Wei, Quan Chen, Xiaoxin Tang, Jingwen Leng, Li Li, Mingyi Guo, 10.1109/ICCD46524.2019.0007537th IEEE International Conference on Computer Design, ICCD 2019. Abu Dhabi, United Arab EmiratesIEEEWeihao Cui, Mengze Wei, Quan Chen, Xiaoxin Tang, Jingwen Leng, Li Li, and Mingyi Guo. 2019. Ebird: Elastic Batch for Improving Responsiveness and Throughput of Deep Learning Services. In 37th IEEE International Conference on Computer Design, ICCD 2019, Abu Dhabi, United Arab Emirates, November 17-20, 2019. IEEE, 497-505. https://doi.org/10.1109/ICCD46524.2019.00075\n\nDVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs. Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng, Chao Li, Minyi Guo, 2022 USENIX Annual Technical Conference (USENIX ATC 22. Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng, Chao Li, and Minyi Guo. 2022. DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs. In 2022 USENIX Annual Technical Conference (USENIX ATC 22). 183-198.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 (2022).\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\nJesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, arXiv:arXiv:2104.08758Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. arXiv:arXiv:2104.08758\n\nHawq-v2: Hessian aware trace-weighted quantization of neural networks. Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Advances in neural information processing systems. 33Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems 33 (2020), 18518-18529.\n\nHawq: Hessian aware quantization of neural networks with mixedprecision. Zhen Dong, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Hawq: Hessian aware quantization of neural networks with mixed- precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 293-302.\n\nShiDianNao: Shifting vision processing closer to the sensor. Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, Olivier Temam, Proceedings of the 42nd Annual International Symposium on Computer Architecture. the 42nd Annual International Symposium on Computer ArchitectureZidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015. ShiDianNao: Shifting vision processing closer to the sensor. In Proceedings of the 42nd Annual International Symposium on Computer Architecture. 92-104.\n\n. Amir Gholami, Zhewei Yao, Sehoon Kim, W Michael, Kurt Mahoney, Keutzer, 2021. AI and Memory Wall. RiseLab Medium Post. Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. 2021. AI and Memory Wall. RiseLab Medium Post (2021).\n\nA 240 g-ops/s mobile coprocessor for deep neural networks. Jonghoon Vinayak Gokhale, Aysegul Jin, Berin Dundar, Eugenio Martini, Culurciello, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsVinayak Gokhale, Jonghoon Jin, Aysegul Dundar, Berin Martini, and Eugenio Culurciello. 2014. A 240 g-ops/s mobile coprocessor for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 682-687.\n\nHow Far Does BERT Look At: Distance-based Clustering and Analysis of BERT \u2032 s Attention. Yue Guan, Jingwen Leng, Chao Li, Quan Chen, Minyi Guo, arXiv:2011.00943arXiv preprintYue Guan, Jingwen Leng, Chao Li, Quan Chen, and Minyi Guo. 2020. How Far Does BERT Look At: Distance-based Clustering and Analysis of BERT \u2032 s Attention. arXiv preprint arXiv:2011.00943 (2020).\n\nYue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, Minyi Guo, arXiv:2205.07324Transkimmer: Transformer Learns to Layer-wise Skim. arXiv preprintYue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi Guo. 2022. Tran- skimmer: Transformer Learns to Layer-wise Skim. arXiv preprint arXiv:2205.07324 (2022).\n\nBlock-skim: Efficient question answering for transformer. Yue Guan, Zhengyi Li, Zhouhan Lin, Yuhao Zhu, Jingwen Leng, Minyi Guo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence36Yue Guan, Zhengyi Li, Zhouhan Lin, Yuhao Zhu, Jingwen Leng, and Minyi Guo. 2022. Block-skim: Efficient question answering for transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 10710-10719.\n\nAccelerating sparse dnn models without hardware-support via tile-wise sparsity. Cong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang, Xiaoying Jia, Xipeng Li, Minyi Guo, Yuhao Zhu, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEECong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang, Xiaoying Jia, Xipeng Li, Minyi Guo, and Yuhao Zhu. 2020. Accelerating sparse dnn models without hardware-support via tile-wise sparsity. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1-15.\n\nSQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation. Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, Minyi Guo, International Conference on Learning Representations. Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, and Minyi Guo. 2022. SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation. In International Conference on Learning Representations. https://openreview.net/forum?id=JXhROKNZzOc\n\nNesting Forward Automatic Differentiation for Memory-Efficient Deep Neural Network Training. Cong Guo, Yuxian Qiu, Jingwen Leng, Chen Zhang, Ying Cao, Quanlu Zhang, Yunxin Liu, Fan Yang, Minyi Guo, 2022 IEEE 40th International Conference on Computer Design (ICCD). IEEECong Guo, Yuxian Qiu, Jingwen Leng, Chen Zhang, Ying Cao, Quanlu Zhang, Yunxin Liu, Fan Yang, and Minyi Guo. 2022. Nesting Forward Automatic Differ- entiation for Memory-Efficient Deep Neural Network Training. In 2022 IEEE 40th International Conference on Computer Design (ICCD). IEEE, 738-745.\n\nANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization. Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, Yuhao Zhu, 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEECong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2022. ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 1414-1433.\n\n. Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2022. ANT github repositoryCong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2022. ANT github repository. https://github.com/clevercool/ ANT_Micro22.\n\nBalancing Efficiency and Flexibility for DNN Acceleration via Temporal GPU-Systolic Array Integration. Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen, Chao Li, Bin Yao, Minyi Guo, 2020 57th ACM/IEEE Design Automation Conference (DAC. Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen, Chao Li, Bin Yao, and Minyi Guo. 2020. Balancing Efficiency and Flexibility for DNN Acceleration via Temporal GPU-Systolic Array Integration. In 2020 57th ACM/IEEE Design Automation Conference (DAC). 1-6.\n\nDeep learning with limited numerical precision. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan, PMLRInternational conference on machine learning. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. 2015. Deep learning with limited numerical precision. In International conference on machine learning. PMLR, 1737-1746.\n\nSong Han, Huizi Mao, William J Dally, arXiv:1510.00149Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprintSong Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.\n\nQuantization and training of neural networks for efficient integer-arithmetic-only inference. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2704-2713.\n\nBiScaled-DNN: Quantizing long-tailed datastructures with two scale factors for deep neural networks. Shubham Jain, Swagath Venkataramani, Vijayalakshmi Srinivasan, Jungwook Choi, Kailash Gopalakrishnan, Leland Chang, 56th ACM/IEEE Design Automation Conference (DAC). IEEE. Shubham Jain, Swagath Venkataramani, Vijayalakshmi Srinivasan, Jungwook Choi, Kailash Gopalakrishnan, and Leland Chang. 2019. BiScaled-DNN: Quantiz- ing long-tailed datastructures with two scale factors for deep neural networks. In 2019 56th ACM/IEEE Design Automation Conference (DAC). IEEE, 1-6.\n\nTASO: optimizing deep learning computation with automatic generation of graph substitutions. Zhihao Jia, Oded Padon, James J Thomas, Todd Warszawski, Matei Zaharia, Alex Aiken, 10.1145/3341301.3359630Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP). the 27th ACM Symposium on Operating Systems Principles (SOSP)ACMZhihao Jia, Oded Padon, James J. Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. 2019. TASO: optimizing deep learning computation with automatic generation of graph substitutions. In Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP). ACM, 47-62. https://doi.org/10.1145/ 3341301.3359630\n\nTen Lessons From Three Generations Shaped Google's TPUv4i: Industrial Product. Norman P Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, David Patterson, 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, and David Patterson. 2021. Ten Lessons From Three Generations Shaped Google's TPUv4i: Industrial Product. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA).\n\n. P Norman, Cliff Jouppi, Nishant Young, David Patil, Gaurav Patterson, Raminder Agrawal, Sarah Bajwa, Suresh Bates, Nan Bhatia, Al Boden, Borchers, Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017.\n\nIn-datacenter performance analysis of a tensor processing unit. Proceedings of the 44th annual international symposium on computer architecture. the 44th annual international symposium on computer architectureIn-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture. 1-12.\n\nLearning to quantize deep networks by optimizing quantization intervals with task loss. Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, Changkyu Choi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. 2019. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4350-4359.\n\nAndrew Kerr, Haicheng Wu, Manish Gupta, Dustyn Blasig, Pradeep Ramini, Duane Merrill, Aniket Shivam, Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Matt Nicely. 2022. CUTLASS. Andrew Kerr, Haicheng Wu, Manish Gupta, Dustyn Blasig, Pradeep Ramini, Duane Merrill, Aniket Shivam, Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Matt Nicely. 2022. CUTLASS. https://github.com/NVIDIA/cutlass\n\nAccel-Sim: An Extensible Simulation Framework for Validated GPU Modeling. Mahmoud Khairy, Zhesheng Shen, Tor Aamodt, Timothy Rogers, 10.1109/ISCA45697.2020.00047Mahmoud Khairy, Zhesheng Shen, Tor Aamodt, and Timothy Rogers. 2020. Accel- Sim: An Extensible Simulation Framework for Validated GPU Modeling. 473-486. https://doi.org/10.1109/ISCA45697.2020.00047\n\nAccel-Sim: An extensible simulation framework for validated GPU modeling. Mahmoud Khairy, Zhesheng Shen, M Tor, Timothy G Aamodt, Rogers, 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEEMahmoud Khairy, Zhesheng Shen, Tor M Aamodt, and Timothy G Rogers. 2020. Accel-Sim: An extensible simulation framework for validated GPU modeling. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 473-486.\n\nLogic synthesis using Synopsys\u00ae. Pran Kurup, Taher Abbasi, Springer Science & Business MediaPran Kurup and Taher Abbasi. 2012. Logic synthesis using Synopsys\u00ae. Springer Science & Business Media.\n\nGPUWattch: enabling energy optimizations in GPGPUs. Jingwen Leng, Tayler Hetherington, Ahmed Eltantawy, Syed Gilani, Nam Kim, Tor Aamodt, Vijay Janapa Reddi, 10.1145/2508148.2485964ACM SIGARCH Computer Architecture News. 41Jingwen Leng, Tayler Hetherington, Ahmed ElTantawy, Syed Gilani, Nam Kim, Tor Aamodt, and Vijay Janapa Reddi. 2013. GPUWattch: enabling energy opti- mizations in GPGPUs. ACM SIGARCH Computer Architecture News 41 (07 2013). https://doi.org/10.1145/2508148.2485964\n\nBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).\n\nZhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jingwen Leng, Minyi Guo, arXiv:2208.11945Efficient Activation Quantization via Adaptive Rounding Border for Post-Training Quantization. arXiv preprintZhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jing- wen Leng, and Minyi Guo. 2022. Efficient Activation Quantization via Adaptive Rounding Border for Post-Training Quantization. arXiv preprint arXiv:2208.11945 (2022).\n\nPuDianNao: A Polyvalent Machine Learning Accelerator. Daofu Liu, Tianshi Chen, Shaoli Liu, Jinhong Zhou, Shengyuan Zhou, Olivier Teman, Xiaobing Feng, Xuehai Zhou, Yunji Chen, Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems. the Twentieth International Conference on Architectural Support for Programming Languages and Operating SystemsDaofu Liu, Tianshi Chen, Shaoli Liu, Jinhong Zhou, Shengyuan Zhou, Olivier Teman, Xiaobing Feng, Xuehai Zhou, and Yunji Chen. 2015. PuDianNao: A Poly- valent Machine Learning Accelerator. In Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems. 369-381.\n\nVELTAIR: towards high-performance multi-tenant deep learning services via adaptive compilation and scheduling. Zihan Liu, Jingwen Leng, Zhihui Zhang, Quan Chen, Chao Li, Minyi Guo, 10.1145/3503222.3507752ASPLOS '22: 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. Babak Falsafi, Michael Ferdman, Shan Lu, and Thomas F. WenischLausanne, SwitzerlandACMZihan Liu, Jingwen Leng, Zhihui Zhang, Quan Chen, Chao Li, and Minyi Guo. 2022. VELTAIR: towards high-performance multi-tenant deep learning services via adaptive compilation and scheduling. In ASPLOS '22: 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Lausanne, Switzerland, 28 February 2022 -4 March 2022, Babak Falsafi, Michael Ferdman, Shan Lu, and Thomas F. Wenisch (Eds.). ACM, 388-401. https: //doi.org/10.1145/3503222.3507752\n\nHeracles: improving resource efficiency at scale. David Lo, Liqun Cheng, Rama Govindaraju, Christos Parthasarathy Ranganathan, Kozyrakis, 10.1145/2749469.2749475Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA. the 42nd Annual International Symposium on Computer Architecture (ISCADavid Lo, Liqun Cheng, Rama Govindaraju, Parthasarathy Ranganathan, and Christos Kozyrakis. 2015. Heracles: improving resource efficiency at scale. In Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA). https://doi.org/10.1145/2749469.2749475\n\nBubble-Up: increasing utilization in modern warehouse scale computers via sensible co-locations. Jason Mars, Lingjia Tang, Robert Hundt, Kevin Skadron, Mary Lou Soffa, 10.1145/2155620.2155650IEEE/ACM International Symposium on Microarchitecture (MICRO). Jason Mars, Lingjia Tang, Robert Hundt, Kevin Skadron, and Mary Lou Soffa. 2011. Bubble-Up: increasing utilization in modern warehouse scale computers via sensible co-locations. In IEEE/ACM International Symposium on Microarchitecture (MICRO). https://doi.org/10.1145/2155620.2155650\n\nModelTC. 2022. repositories. ModelTC. 2022. repositories. https://huggingface.co/ModelTC.\n\nCACTI 6.0: A tool to model large caches. Naveen Muralimanohar, Rajeev Balasubramonian, Norman P Jouppi , HP laboratories. 2728Naveen Muralimanohar, Rajeev Balasubramonian, and Norman P Jouppi. 2009. CACTI 6.0: A tool to model large caches. HP laboratories 27 (2009), 28.\n\nMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, arXiv:2106.08295Mart van Baalen, and Tijmen Blankevoort. 2021. A white paper on neural network quantization. arXiv preprintMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. 2021. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295 (2021).\n\nNVIDIA Tesla V100 GPU Architecture. Nvidia, Technical report. NVIDIA. Nvidia. 2017. NVIDIA Tesla V100 GPU Architecture. In Technical report. NVIDIA.\n\nNVIDIA Turing GPU Architecture. Nvidia, Technical report. NVIDIA. Nvidia. 2018. NVIDIA Turing GPU Architecture. In Technical report. NVIDIA.\n\nNVIDIA A100 tensor core architecture. Nvidia, Technical report. NVIDIA. Nvidia. 2020. NVIDIA A100 tensor core architecture. In Technical report. NVIDIA.\n\nEnergy-efficient neural network 'accel'erator based on outlier-aware low-precision computation. Eunhyeok Park, Dongyoung Kim, Sungjoo Yoo, Eunhyeok Park, Dongyoung Kim, and Sungjoo Yoo. 2018. Energy-efficient neural network 'accel'erator based on outlier-aware low-precision computation. In 2018\n\nACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE. ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE, 688-698.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 32Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019), 8026-8037.\n\nMemory-centric accelerator design for convolutional neural networks. Maurice Peemen, A A Arnaud, Bart Setio, Henk Mesman, Corporaal, 2013 IEEE 31st International Conference on Computer Design (ICCD). IEEEMaurice Peemen, Arnaud AA Setio, Bart Mesman, and Henk Corporaal. 2013. Memory-centric accelerator design for convolutional neural networks. In 2013 IEEE 31st International Conference on Computer Design (ICCD). IEEE, 13-19.\n\nSigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training. Eric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan Srinivasan, Dipankar Das, Bharat Kaul, Tushar Krishna, 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEEric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan Srini- vasan, Dipankar Das, Bharat Kaul, and Tushar Krishna. 2020. Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 58-70.\n\nAdversarial Defense Through Network Profiling Based Path Extraction. Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, Yuhao Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, and Yuhao Zhu. 2019. Adversarial Defense Through Network Profiling Based Path Extraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n\nLanguage Models are Unsupervised Multitask Learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).\n\nModeling deep learning accelerator enabled gpus. Negar Md Aamir Raihan, Tor M Goli, Aamodt, 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEEMd Aamir Raihan, Negar Goli, and Tor M Aamodt. 2019. Modeling deep learning accelerator enabled gpus. In 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, 79-92.\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\n\nDeepScaleTool: A Tool for the Accurate Estimation of Technology Scaling in the Deep-Submicron Era. Satyabrata Sarangi, Bevan Baas, 2021 IEEE International Symposium on Circuits and Systems (ISCAS). IEEESatyabrata Sarangi and Bevan Baas. 2021. DeepScaleTool: A Tool for the Accurate Estimation of Technology Scaling in the Deep-Submicron Era. In 2021 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 1-5.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Yvon, arXiv:2211.05100Matthias Gall\u00e9, et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100 (2022).\n\nFrom high-level deep neural models to FPGAs. Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung Kim, Chenkai Shao, Asit Mishra, Hadi Esmaeilzadeh, 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEHardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung Kim, Chenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh. 2016. From high-level deep neural models to FPGAs. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 1-12.\n\nBit fusion: Bit-level dynamically composable architecture for accelerating deep neural network. Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, Hadi Esmaeilzadeh, ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEEHardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, and Hadi Esmaeilzadeh. 2018. Bit fusion: Bit-level dynamically compos- able architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE, 764-775.\n\nHardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, Hadi Esmaeilzadeh, Bitfusion github repository. Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, and Hadi Esmaeilzadeh. 2018. Bitfusion github repository. https: //github.com/hsharma35/bitfusion.\n\nQ-bert: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney, Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8815-8821.\n\nDrq: dynamic region-based quantization for deep neural network acceleration. Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, Xiaoyao Liang, 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEEZhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. Drq: dynamic region-based quantization for deep neural network acceleration. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1010-1021.\n\nAlgorithm-hardware codesign of adaptive floating-point encodings for resilient deep learning inference. En-Yu Thierry Tambe, Zishen Yang, Yuntian Wan, Vijay Janapa Deng, Alexander Reddi, David Rush, Gu-Yeon Brooks, Wei, 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEEThierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020. Algorithm-hardware co- design of adaptive floating-point encodings for resilient deep learning inference. In 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEE, 1-6.\n\nAttention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 (2018).\n\nHaq: Hardwareaware automated quantization with mixed precision. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. 2019. Haq: Hardware- aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8612-8620.\n\nDual-side sparse tensor core. Yang Wang, Chen Zhang, Zhiqiang Xie, Cong Guo, Yunxin Liu, Jingwen Leng, 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEEYang Wang, Chen Zhang, Zhiqiang Xie, Cong Guo, Yunxin Liu, and Jingwen Leng. 2021. Dual-side sparse tensor core. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1083-1095.\n\nLearning channel-wise interactions for binary convolutional neural networks. Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, Qi Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZiwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. 2019. Learning channel-wise interactions for binary convolutional neural networks. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 568-577.\n\nOutlier Suppression: Pushing the Limit of Low-bit Transformer Language Models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, Advances in Neural Information Processing Systems. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun ChoXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2022. Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models. In Advances in Neural Infor- mation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=yW5zeRSFdZ\n\nThe Free Encyclopedia. Wikipedia contributors. 2022. 68-95-99.7 rule -Wikipedia. Wikipedia contributors. 2022. 68-95-99.7 rule -Wikipedia, The Free Encyclope- dia. [Online].\n\nBubble-flux: precise online QoS management for increased utilization in warehouse scale computers. Hailong Yang, Alex D Breslow, Jason Mars, Lingjia Tang, 10.1145/2485922.2485974The 40th Annual International Symposium on Computer Architecture (ISCA. Hailong Yang, Alex D. Breslow, Jason Mars, and Lingjia Tang. 2013. Bubble-flux: precise online QoS management for increased utilization in warehouse scale computers. In The 40th Annual International Symposium on Computer Architecture (ISCA). https://doi.org/10.1145/2485922.2485974\n\nGobo: Quantizing attention-based nlp models for low latency and energy efficient inference. Ali Hadi Zadeh, Isak Edo, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEOmar Mohamed Awad, and Andreas MoshovosAli Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy ef- ficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 811-824.\n\nQ8bert: Quantized 8bit bert. Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat, 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEEOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 36-39.\n\nOptimizing fpga-based accelerator design for deep convolutional neural networks. Chen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, Jason Cong, Proceedings of the 2015 ACM/SIGDA international symposium on field-programmable gate arrays. the 2015 ACM/SIGDA international symposium on field-programmable gate arraysChen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong. 2015. Optimizing fpga-based accelerator design for deep convolutional neural networks. In Proceedings of the 2015 ACM/SIGDA international symposium on field-programmable gate arrays. 161-170.\n\nAccelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization ISCA '23. Olive, Orlando, FL, USAOliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization ISCA '23, June 17-21, 2023, Orlando, FL, USA\n\nLq-nets: Learned quantization for highly accurate and compact deep neural networks. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, Gang Hua, Proceedings of the European conference on computer vision (ECCV. the European conference on computer vision (ECCVDongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. 2018. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV). 365-382.\n\nCambricon-X: An accelerator for sparse neural networks. Shijin Zhang, Zidong Du, Lei Zhang, Huiying Lan, Shaoli Liu, Ling Li, Qi Guo, Tianshi Chen, Yunji Chen, 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEShijin Zhang, Zidong Du, Lei Zhang, Huiying Lan, Shaoli Liu, Ling Li, Qi Guo, Tianshi Chen, and Yunji Chen. 2016. Cambricon-X: An accelerator for sparse neural networks. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 1-12.\n\nOpt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).\n\nAnsor: Generating High-Performance Tensor Programs for Deep Learning. Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E Gonzalez, Ion Stoica, 10.5555/3488766.348881514th USENIX Symposium on Operating Systems Design and Implementation (OSDI. Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez, and Ion Stoica. 2020. Ansor: Generating High-Performance Tensor Programs for Deep Learning. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI). https://doi.org/10.5555/3488766.3488815\n\nFlex-Tensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System. Size Zheng, Yun Liang, Shuo Wang, Renze Chen, Kaiwen Sheng, 10.1145/3373376.3378508Architectural Support for Programming Languages and Operating Systems. LausanneASPLOSSize Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. 2020. Flex- Tensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System. In Architectural Support for Programming Languages and Operating Systems, Lausanne (ASPLOS). https: //doi.org/10.1145/3373376.3378508\n\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou, arXiv:1606.06160Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016).\n\nCambricon-S: Addressing irregularity in sparse neural networks through a cooperative software/hardware approach. Xuda Zhou, Zidong Du, Qi Guo, Shaoli Liu, Chengsi Liu, Chao Wang, Xuehai Zhou, Ling Li, Tianshi Chen, Yunji Chen, 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEXuda Zhou, Zidong Du, Qi Guo, Shaoli Liu, Chengsi Liu, Chao Wang, Xuehai Zhou, Ling Li, Tianshi Chen, and Yunji Chen. 2018. Cambricon-S: Addressing irregularity in sparse neural networks through a cooperative software/hardware approach. In 2018 51st Annual IEEE/ACM International Symposium on Microarchi- tecture (MICRO). IEEE, 15-28.\n\nuGrapher: High-Performance Graph Operator Computation via Unified Abstraction for Graph Neural Networks. Yangjie Zhou, Jingwen Leng, Yaoxu Song, Shuwen Lu, Mian Wang, Chao Li, Minyi Guo, Wenting Shen, Yong Li, Wei Lin, Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems2Yangjie Zhou, Jingwen Leng, Yaoxu Song, Shuwen Lu, Mian Wang, Chao Li, Minyi Guo, Wenting Shen, Yong Li, Wei Lin, et al. 2023. uGrapher: High-Performance Graph Operator Computation via Unified Abstraction for Graph Neural Networks. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. 878-891.\n\nCharacterizing and demystifying the implicit convolution algorithm on commercial matrix-multiplication accelerators. Yangjie Zhou, Mengtian Yang, Cong Guo, Jingwen Leng, Yun Liang, Quan Chen, Minyi Guo, Yuhao Zhu, 2021 IEEE International Symposium on Workload Characterization (IISWC). IEEEYangjie Zhou, Mengtian Yang, Cong Guo, Jingwen Leng, Yun Liang, Quan Chen, Minyi Guo, and Yuhao Zhu. 2021. Characterizing and demystifying the implicit convolution algorithm on commercial matrix-multiplication accelerators. In 2021 IEEE International Symposium on Workload Characterization (IISWC). IEEE, 214- 225.\n\nAsaf Cidon, and Gennady Pekhimenko. 2022. ROLLER: Fast and Efficient Tensor Compilation for Deep Learning. Hongyu Zhu, Ruofan Wu, Yijia Diao, Shanbin Ke, Haoyu Li, Chen Zhang, Jilong Xue, Lingxiao Ma, Yuqing Xia, Wei Cui, Fan Yang, Mao Yang, Lidong Zhou, 16th USENIX Symposium on Operating Systems Design and Implementation. OSDI 22Hongyu Zhu, Ruofan Wu, Yijia Diao, Shanbin Ke, Haoyu Li, Chen Zhang, Jilong Xue, Lingxiao Ma, Yuqing Xia, Wei Cui, Fan Yang, Mao Yang, Lidong Zhou, Asaf Cidon, and Gennady Pekhimenko. 2022. ROLLER: Fast and Efficient Tensor Compilation for Deep Learning. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 233-248.\n\nSparse tensor core: Algorithm and hardware co-design for vector-wise sparse neural networks on modern gpus. Maohua Zhu, Tao Zhang, Zhenyu Gu, Yuan Xie, Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. the 52nd Annual IEEE/ACM International Symposium on MicroarchitectureMaohua Zhu, Tao Zhang, Zhenyu Gu, and Yuan Xie. 2019. Sparse tensor core: Algorithm and hardware co-design for vector-wise sparse neural networks on modern gpus. In Proceedings of the 52nd Annual IEEE/ACM International Sympo- sium on Microarchitecture. 359-371.\n\nEffective training of convolutional neural networks with low-bitwidth weights and activations. Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, Chunhua Shen, IEEE Transactions on Pattern Analysis and Machine Intelligence. Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, and Chunhua Shen. 2021. Effective training of convolutional neural networks with low-bitwidth weights and activations. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).\n", "annotations": {"author": "[{\"end\":182,\"start\":153},{\"end\":197,\"start\":183},{\"end\":214,\"start\":198},{\"end\":231,\"start\":215},{\"end\":251,\"start\":232},{\"end\":271,\"start\":252},{\"end\":286,\"start\":272},{\"end\":303,\"start\":287},{\"end\":320,\"start\":304},{\"end\":340,\"start\":321},{\"end\":355,\"start\":341},{\"end\":359,\"start\":356},{\"end\":374,\"start\":360},{\"end\":391,\"start\":375},{\"end\":408,\"start\":392},{\"end\":428,\"start\":409},{\"end\":448,\"start\":429},{\"end\":463,\"start\":449},{\"end\":480,\"start\":464},{\"end\":497,\"start\":481},{\"end\":517,\"start\":498},{\"end\":535,\"start\":518},{\"end\":567,\"start\":536},{\"end\":609,\"start\":568},{\"end\":642,\"start\":610},{\"end\":657,\"start\":643},{\"end\":674,\"start\":658},{\"end\":691,\"start\":675},{\"end\":711,\"start\":692},{\"end\":747,\"start\":712},{\"end\":757,\"start\":748},{\"end\":771,\"start\":758},{\"end\":783,\"start\":772},{\"end\":820,\"start\":784},{\"end\":832,\"start\":821},{\"end\":842,\"start\":833},{\"end\":854,\"start\":843},{\"end\":865,\"start\":855},{\"end\":876,\"start\":866},{\"end\":886,\"start\":877},{\"end\":920,\"start\":887},{\"end\":995,\"start\":921},{\"end\":1090,\"start\":996},{\"end\":1137,\"start\":1091}]", "publisher": null, "author_last_name": "[{\"end\":161,\"start\":158},{\"end\":196,\"start\":192},{\"end\":213,\"start\":203},{\"end\":230,\"start\":227},{\"end\":250,\"start\":242},{\"end\":270,\"start\":266},{\"end\":285,\"start\":281},{\"end\":302,\"start\":292},{\"end\":319,\"start\":316},{\"end\":339,\"start\":331},{\"end\":354,\"start\":349},{\"end\":358,\"start\":356},{\"end\":373,\"start\":369},{\"end\":390,\"start\":380},{\"end\":407,\"start\":404},{\"end\":427,\"start\":419},{\"end\":447,\"start\":443},{\"end\":462,\"start\":458},{\"end\":479,\"start\":469},{\"end\":496,\"start\":493},{\"end\":516,\"start\":508},{\"end\":534,\"start\":529},{\"end\":544,\"start\":540},{\"end\":578,\"start\":575},{\"end\":619,\"start\":616},{\"end\":656,\"start\":652},{\"end\":673,\"start\":663},{\"end\":690,\"start\":687},{\"end\":710,\"start\":702},{\"end\":727,\"start\":724},{\"end\":756,\"start\":753},{\"end\":770,\"start\":766},{\"end\":782,\"start\":780},{\"end\":796,\"start\":792},{\"end\":831,\"start\":826},{\"end\":841,\"start\":837},{\"end\":853,\"start\":850},{\"end\":864,\"start\":861},{\"end\":885,\"start\":881}]", "author_first_name": "[{\"end\":157,\"start\":153},{\"end\":191,\"start\":183},{\"end\":202,\"start\":198},{\"end\":223,\"start\":215},{\"end\":226,\"start\":224},{\"end\":241,\"start\":232},{\"end\":257,\"start\":252},{\"end\":265,\"start\":258},{\"end\":280,\"start\":272},{\"end\":291,\"start\":287},{\"end\":312,\"start\":304},{\"end\":315,\"start\":313},{\"end\":330,\"start\":321},{\"end\":348,\"start\":341},{\"end\":368,\"start\":360},{\"end\":379,\"start\":375},{\"end\":400,\"start\":392},{\"end\":403,\"start\":401},{\"end\":418,\"start\":409},{\"end\":434,\"start\":429},{\"end\":442,\"start\":435},{\"end\":457,\"start\":449},{\"end\":468,\"start\":464},{\"end\":489,\"start\":481},{\"end\":492,\"start\":490},{\"end\":507,\"start\":498},{\"end\":523,\"start\":518},{\"end\":528,\"start\":524},{\"end\":539,\"start\":536},{\"end\":574,\"start\":568},{\"end\":615,\"start\":610},{\"end\":651,\"start\":643},{\"end\":662,\"start\":658},{\"end\":683,\"start\":675},{\"end\":686,\"start\":684},{\"end\":701,\"start\":692},{\"end\":717,\"start\":712},{\"end\":723,\"start\":718},{\"end\":752,\"start\":748},{\"end\":765,\"start\":758},{\"end\":779,\"start\":772},{\"end\":791,\"start\":784},{\"end\":825,\"start\":821},{\"end\":836,\"start\":833},{\"end\":849,\"start\":843},{\"end\":860,\"start\":855},{\"end\":871,\"start\":866},{\"end\":875,\"start\":872},{\"end\":880,\"start\":877}]", "author_affiliation": "[{\"end\":919,\"start\":888},{\"end\":994,\"start\":922},{\"end\":1089,\"start\":997},{\"end\":1136,\"start\":1092}]", "title": "[{\"end\":98,\"start\":1},{\"end\":1235,\"start\":1138}]", "venue": "[{\"end\":1327,\"start\":1237}]", "abstract": "[{\"end\":3640,\"start\":1975}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b78\"},\"end\":3707,\"start\":3703},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3948,\"start\":3944},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":4044,\"start\":4040},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4189,\"start\":4186},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4191,\"start\":4189},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4194,\"start\":4191},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4197,\"start\":4194},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":4200,\"start\":4197},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":4203,\"start\":4200},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":4206,\"start\":4203},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":4209,\"start\":4206},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4435,\"start\":4431},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4460,\"start\":4456},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4506,\"start\":4502},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":4509,\"start\":4506},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":4512,\"start\":4509},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4782,\"start\":4778},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4894,\"start\":4890},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":4897,\"start\":4894},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5147,\"start\":5143},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5150,\"start\":5147},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":5153,\"start\":5150},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":5156,\"start\":5153},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":5159,\"start\":5156},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":5246,\"start\":5242},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5870,\"start\":5869},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":6034,\"start\":6030},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6052,\"start\":6048},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6178,\"start\":6174},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":6258,\"start\":6254},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":6624,\"start\":6620},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":6920,\"start\":6916},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8546,\"start\":8542},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8632,\"start\":8628},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":8635,\"start\":8632},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10205,\"start\":10201},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":10241,\"start\":10237},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10244,\"start\":10241},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10531,\"start\":10528},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10732,\"start\":10728},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10746,\"start\":10742},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":10814,\"start\":10810},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10953,\"start\":10949},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":10967,\"start\":10963},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":12128,\"start\":12124},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":12131,\"start\":12128},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":12134,\"start\":12131},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":12137,\"start\":12134},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12471,\"start\":12467},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":12951,\"start\":12947},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13060,\"start\":13056},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13118,\"start\":13114},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14047,\"start\":14043},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14050,\"start\":14047},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14380,\"start\":14376},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":14383,\"start\":14380},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14839,\"start\":14835},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":14853,\"start\":14849},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15009,\"start\":15005},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":15022,\"start\":15018},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15624,\"start\":15620},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":15627,\"start\":15624},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":15768,\"start\":15764},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17713,\"start\":17709},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17730,\"start\":17726},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":17744,\"start\":17740},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":17763,\"start\":17759},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":18672,\"start\":18668},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":18697,\"start\":18693},{\"end\":21600,\"start\":21593},{\"end\":21766,\"start\":21759},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22576,\"start\":22572},{\"end\":22807,\"start\":22800},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22859,\"start\":22855},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23441,\"start\":23437},{\"end\":23692,\"start\":23681},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27477,\"start\":27474},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27479,\"start\":27477},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":27482,\"start\":27479},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28105,\"start\":28101},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28605,\"start\":28601},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28704,\"start\":28701},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":29043,\"start\":29039},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":29046,\"start\":29043},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":29071,\"start\":29067},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":29074,\"start\":29071},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":29334,\"start\":29330},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":29517,\"start\":29513},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":29813,\"start\":29809},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30177,\"start\":30173},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31215,\"start\":31211},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":33720,\"start\":33716},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":36310,\"start\":36306},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36338,\"start\":36334},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36355,\"start\":36351},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36375,\"start\":36371},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":36467,\"start\":36463},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36508,\"start\":36504},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36527,\"start\":36523},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":36585,\"start\":36581},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":36678,\"start\":36674},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":36694,\"start\":36690},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":36713,\"start\":36709},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":36733,\"start\":36729},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36745,\"start\":36741},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":36862,\"start\":36858},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":36959,\"start\":36955},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":36985,\"start\":36981},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":36998,\"start\":36994},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37012,\"start\":37008},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":37038,\"start\":37034},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":37102,\"start\":37098},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":37158,\"start\":37154},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37220,\"start\":37216},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":37477,\"start\":37473},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":37496,\"start\":37492},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":37529,\"start\":37525},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37539,\"start\":37535},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":37583,\"start\":37579},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":37597,\"start\":37593},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":37800,\"start\":37796},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":37867,\"start\":37863},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38080,\"start\":38076},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":38177,\"start\":38173},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38416,\"start\":38413},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38434,\"start\":38430},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":38513,\"start\":38509},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38529,\"start\":38525},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":38545,\"start\":38541},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":38710,\"start\":38706},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":38943,\"start\":38939},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":38968,\"start\":38964},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":39058,\"start\":39054},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39072,\"start\":39068},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":39210,\"start\":39206},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":39213,\"start\":39210},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39227,\"start\":39223},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39230,\"start\":39227},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39495,\"start\":39491},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39512,\"start\":39508},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":39531,\"start\":39527},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":39925,\"start\":39921},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":40835,\"start\":40831},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41538,\"start\":41534},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":42867,\"start\":42863},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":43117,\"start\":43113},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":43224,\"start\":43220},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":43227,\"start\":43224},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":43267,\"start\":43263},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":43270,\"start\":43267},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":43273,\"start\":43270},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":43881,\"start\":43877},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":44187,\"start\":44183},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":45327,\"start\":45323},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":45900,\"start\":45896},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":45914,\"start\":45910},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":45933,\"start\":45929},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":47556,\"start\":47554},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":47581,\"start\":47577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":47815,\"start\":47811},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":47818,\"start\":47815},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":47821,\"start\":47818},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":47824,\"start\":47821},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":47827,\"start\":47824},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":47830,\"start\":47827},{\"end\":47874,\"start\":47846},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48112,\"start\":48108},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":48115,\"start\":48112},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":48118,\"start\":48115},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":48121,\"start\":48118},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":48124,\"start\":48121},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48127,\"start\":48124},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":48130,\"start\":48127},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":48133,\"start\":48130},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":48136,\"start\":48133},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":48139,\"start\":48136},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48149,\"start\":48145},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":48283,\"start\":48279},{\"end\":48923,\"start\":48885},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49259,\"start\":49255},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":49262,\"start\":49259},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":49265,\"start\":49262},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":49268,\"start\":49265},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49310,\"start\":49306},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":49313,\"start\":49310},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49316,\"start\":49313},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":49319,\"start\":49316},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":49772,\"start\":49768},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":49855,\"start\":49851},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":49934,\"start\":49930},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":50068,\"start\":50064},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":50375,\"start\":50371},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":50680,\"start\":50676},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":51815,\"start\":51811},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":52213,\"start\":52210},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":52216,\"start\":52213},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57779,\"start\":57775},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":57782,\"start\":57779},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":57785,\"start\":57782},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":57788,\"start\":57785},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":57791,\"start\":57788},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":57794,\"start\":57791},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":57797,\"start\":57794},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":57800,\"start\":57797},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":57803,\"start\":57800},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":57806,\"start\":57803},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":57809,\"start\":57806},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":57812,\"start\":57809}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53478,\"start\":53235},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53591,\"start\":53479},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53653,\"start\":53592},{\"attributes\":{\"id\":\"fig_5\"},\"end\":53706,\"start\":53654},{\"attributes\":{\"id\":\"fig_6\"},\"end\":53801,\"start\":53707},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54034,\"start\":53802},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54204,\"start\":54035},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54318,\"start\":54205},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54377,\"start\":54319},{\"attributes\":{\"id\":\"fig_12\"},\"end\":54427,\"start\":54378},{\"attributes\":{\"id\":\"fig_14\"},\"end\":54485,\"start\":54428},{\"attributes\":{\"id\":\"fig_15\"},\"end\":54549,\"start\":54486},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":54980,\"start\":54550},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55031,\"start\":54981},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":55226,\"start\":55032},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":55529,\"start\":55227},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":55587,\"start\":55530},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":55823,\"start\":55588},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":55864,\"start\":55824},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":56326,\"start\":55865},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":56510,\"start\":56327},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":56553,\"start\":56511},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":56858,\"start\":56554},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":57131,\"start\":56859},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":57566,\"start\":57132},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":57957,\"start\":57567}]", "paragraph": "[{\"end\":4461,\"start\":3656},{\"end\":5065,\"start\":4463},{\"end\":6794,\"start\":5067},{\"end\":7755,\"start\":6796},{\"end\":8302,\"start\":7757},{\"end\":9627,\"start\":8304},{\"end\":10406,\"start\":9629},{\"end\":11198,\"start\":10408},{\"end\":11250,\"start\":11200},{\"end\":11594,\"start\":11252},{\"end\":11918,\"start\":11596},{\"end\":12816,\"start\":11950},{\"end\":13335,\"start\":12836},{\"end\":13492,\"start\":13367},{\"end\":13909,\"start\":13494},{\"end\":14662,\"start\":13911},{\"end\":15925,\"start\":14687},{\"end\":16211,\"start\":15927},{\"end\":16824,\"start\":16243},{\"end\":17612,\"start\":16826},{\"end\":18204,\"start\":17614},{\"end\":18603,\"start\":18206},{\"end\":19610,\"start\":18605},{\"end\":19999,\"start\":19612},{\"end\":20849,\"start\":20032},{\"end\":22496,\"start\":20876},{\"end\":22998,\"start\":22528},{\"end\":23693,\"start\":23000},{\"end\":23789,\"start\":23695},{\"end\":24216,\"start\":23825},{\"end\":24575,\"start\":24218},{\"end\":24860,\"start\":24627},{\"end\":25378,\"start\":24862},{\"end\":25448,\"start\":25380},{\"end\":25696,\"start\":25664},{\"end\":25929,\"start\":25759},{\"end\":26694,\"start\":25931},{\"end\":27160,\"start\":26696},{\"end\":28705,\"start\":27187},{\"end\":29171,\"start\":28728},{\"end\":30109,\"start\":29191},{\"end\":30526,\"start\":30166},{\"end\":31249,\"start\":30539},{\"end\":31705,\"start\":31251},{\"end\":32013,\"start\":31774},{\"end\":32663,\"start\":32032},{\"end\":33128,\"start\":32682},{\"end\":33166,\"start\":33138},{\"end\":33264,\"start\":33209},{\"end\":33505,\"start\":33284},{\"end\":33856,\"start\":33520},{\"end\":33936,\"start\":33890},{\"end\":34173,\"start\":34027},{\"end\":34571,\"start\":34191},{\"end\":35239,\"start\":34573},{\"end\":35523,\"start\":35259},{\"end\":35982,\"start\":35571},{\"end\":36184,\"start\":35997},{\"end\":36863,\"start\":36200},{\"end\":37339,\"start\":36865},{\"end\":37964,\"start\":37341},{\"end\":38336,\"start\":37966},{\"end\":38757,\"start\":38338},{\"end\":39231,\"start\":38759},{\"end\":39451,\"start\":39252},{\"end\":40734,\"start\":39469},{\"end\":41213,\"start\":40736},{\"end\":41516,\"start\":41215},{\"end\":42203,\"start\":41518},{\"end\":42622,\"start\":42205},{\"end\":43619,\"start\":42653},{\"end\":44433,\"start\":43621},{\"end\":45172,\"start\":44435},{\"end\":45688,\"start\":45174},{\"end\":46182,\"start\":45727},{\"end\":46626,\"start\":46184},{\"end\":47131,\"start\":46628},{\"end\":48432,\"start\":47133},{\"end\":49050,\"start\":48464},{\"end\":49602,\"start\":49052},{\"end\":50661,\"start\":49604},{\"end\":51918,\"start\":50663},{\"end\":52322,\"start\":51920},{\"end\":53234,\"start\":52337}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13366,\"start\":13336},{\"attributes\":{\"id\":\"formula_1\"},\"end\":24626,\"start\":24576},{\"attributes\":{\"id\":\"formula_2\"},\"end\":25589,\"start\":25449},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25663,\"start\":25589},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25758,\"start\":25697},{\"attributes\":{\"id\":\"formula_5\"},\"end\":31773,\"start\":31706},{\"attributes\":{\"id\":\"formula_6\"},\"end\":33137,\"start\":33129},{\"attributes\":{\"id\":\"formula_7\"},\"end\":33208,\"start\":33167},{\"attributes\":{\"id\":\"formula_8\"},\"end\":33889,\"start\":33857},{\"attributes\":{\"id\":\"formula_9\"},\"end\":34026,\"start\":33937},{\"attributes\":{\"id\":\"formula_10\"},\"end\":35570,\"start\":35524}]", "table_ref": "[{\"end\":23446,\"start\":23442},{\"end\":45335,\"start\":45328}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3654,\"start\":3642},{\"attributes\":{\"n\":\"2\"},\"end\":11948,\"start\":11921},{\"attributes\":{\"n\":\"2.1\"},\"end\":12834,\"start\":12819},{\"attributes\":{\"n\":\"2.2\"},\"end\":14685,\"start\":14665},{\"attributes\":{\"n\":\"2.3\"},\"end\":16241,\"start\":16214},{\"attributes\":{\"n\":\"3\"},\"end\":20030,\"start\":20002},{\"attributes\":{\"n\":\"3.1\"},\"end\":20874,\"start\":20852},{\"attributes\":{\"n\":\"3.2\"},\"end\":22526,\"start\":22499},{\"attributes\":{\"n\":\"3.3\"},\"end\":23823,\"start\":23792},{\"attributes\":{\"n\":\"3.4\"},\"end\":27185,\"start\":27163},{\"attributes\":{\"n\":\"4\"},\"end\":28726,\"start\":28708},{\"attributes\":{\"n\":\"4.1\"},\"end\":29189,\"start\":29174},{\"end\":30164,\"start\":30112},{\"attributes\":{\"n\":\"4.2\"},\"end\":30537,\"start\":30529},{\"attributes\":{\"n\":\"4.3\"},\"end\":32030,\"start\":32016},{\"attributes\":{\"n\":\"4.4\"},\"end\":32680,\"start\":32666},{\"attributes\":{\"n\":\"4.5\"},\"end\":33282,\"start\":33267},{\"end\":33518,\"start\":33508},{\"end\":34189,\"start\":34176},{\"attributes\":{\"n\":\"4.6\"},\"end\":35257,\"start\":35242},{\"attributes\":{\"n\":\"5\"},\"end\":35995,\"start\":35985},{\"attributes\":{\"n\":\"5.1\"},\"end\":36198,\"start\":36187},{\"attributes\":{\"n\":\"5.2\"},\"end\":39250,\"start\":39234},{\"end\":39467,\"start\":39454},{\"attributes\":{\"n\":\"5.3\"},\"end\":42651,\"start\":42625},{\"attributes\":{\"n\":\"5.4\"},\"end\":45725,\"start\":45691},{\"attributes\":{\"n\":\"6\"},\"end\":48462,\"start\":48435},{\"attributes\":{\"n\":\"7\"},\"end\":52335,\"start\":52325},{\"end\":53246,\"start\":53236},{\"end\":53490,\"start\":53480},{\"end\":53603,\"start\":53593},{\"end\":53665,\"start\":53655},{\"end\":54046,\"start\":54036},{\"end\":54216,\"start\":54206},{\"end\":54330,\"start\":54320},{\"end\":54389,\"start\":54379},{\"end\":54439,\"start\":54429},{\"end\":54498,\"start\":54487},{\"end\":54560,\"start\":54551},{\"end\":54991,\"start\":54982},{\"end\":55540,\"start\":55531},{\"end\":55598,\"start\":55589},{\"end\":55834,\"start\":55825},{\"end\":55875,\"start\":55866},{\"end\":56337,\"start\":56328},{\"end\":56521,\"start\":56512},{\"end\":56870,\"start\":56860},{\"end\":57578,\"start\":57568}]", "table": "[{\"end\":54980,\"start\":54562},{\"end\":55226,\"start\":55080},{\"end\":55529,\"start\":55387},{\"end\":55823,\"start\":55742},{\"end\":56326,\"start\":56109},{\"end\":56510,\"start\":56339},{\"end\":56858,\"start\":56612},{\"end\":57131,\"start\":57112},{\"end\":57566,\"start\":57388}]", "figure_caption": "[{\"end\":53478,\"start\":53248},{\"end\":53591,\"start\":53492},{\"end\":53653,\"start\":53605},{\"end\":53706,\"start\":53667},{\"end\":53801,\"start\":53709},{\"end\":54034,\"start\":53804},{\"end\":54204,\"start\":54048},{\"end\":54318,\"start\":54218},{\"end\":54377,\"start\":54332},{\"end\":54427,\"start\":54391},{\"end\":54485,\"start\":54441},{\"end\":54549,\"start\":54501},{\"end\":55031,\"start\":54993},{\"end\":55080,\"start\":55034},{\"end\":55387,\"start\":55229},{\"end\":55587,\"start\":55542},{\"end\":55742,\"start\":55600},{\"end\":55864,\"start\":55836},{\"end\":56109,\"start\":55877},{\"end\":56553,\"start\":56523},{\"end\":56612,\"start\":56556},{\"end\":57112,\"start\":56873},{\"end\":57388,\"start\":57134},{\"end\":57957,\"start\":57581}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6158,\"start\":6151},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8015,\"start\":8008},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9194,\"start\":9187},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9822,\"start\":9815},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12914,\"start\":12908},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13491,\"start\":13485},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16848,\"start\":16841},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17548,\"start\":17540},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17611,\"start\":17603},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18720,\"start\":18714},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19115,\"start\":19109},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21453,\"start\":21447},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26331,\"start\":26324},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26442,\"start\":26436},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":29298,\"start\":29291},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":30108,\"start\":30101},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":30363,\"start\":30354},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":30706,\"start\":30699},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":31376,\"start\":31370},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":32086,\"start\":32080},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":32149,\"start\":32140},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":32962,\"start\":32956},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":32985,\"start\":32979},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35127,\"start\":35121},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":39676,\"start\":39670},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":43618,\"start\":43612},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":43641,\"start\":43634},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":44450,\"start\":44443},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46217,\"start\":46209},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46644,\"start\":46636}]", "bib_author_first_name": "[{\"end\":58568,\"start\":58563},{\"end\":58587,\"start\":58580},{\"end\":58600,\"start\":58594},{\"end\":58618,\"start\":58615},{\"end\":58634,\"start\":58627},{\"end\":58642,\"start\":58635},{\"end\":58658,\"start\":58651},{\"end\":59132,\"start\":59129},{\"end\":59148,\"start\":59142},{\"end\":59161,\"start\":59155},{\"end\":59173,\"start\":59168},{\"end\":59183,\"start\":59180},{\"end\":59808,\"start\":59805},{\"end\":59821,\"start\":59817},{\"end\":59837,\"start\":59831},{\"end\":60193,\"start\":60187},{\"end\":60210,\"start\":60202},{\"end\":60225,\"start\":60220},{\"end\":60513,\"start\":60507},{\"end\":60525,\"start\":60519},{\"end\":60535,\"start\":60531},{\"end\":60546,\"start\":60542},{\"end\":60557,\"start\":60556},{\"end\":60571,\"start\":60567},{\"end\":61059,\"start\":61052},{\"end\":61069,\"start\":61065},{\"end\":61560,\"start\":61556},{\"end\":61574,\"start\":61567},{\"end\":61586,\"start\":61581},{\"end\":61595,\"start\":61592},{\"end\":61618,\"start\":61613},{\"end\":61632,\"start\":61625},{\"end\":62482,\"start\":62478},{\"end\":62496,\"start\":62489},{\"end\":62508,\"start\":62503},{\"end\":62522,\"start\":62515},{\"end\":63347,\"start\":63340},{\"end\":63360,\"start\":63354},{\"end\":63372,\"start\":63365},{\"end\":63381,\"start\":63378},{\"end\":63397,\"start\":63388},{\"end\":63407,\"start\":63402},{\"end\":63421,\"start\":63414},{\"end\":63797,\"start\":63791},{\"end\":63811,\"start\":63804},{\"end\":63826,\"start\":63820},{\"end\":63841,\"start\":63834},{\"end\":63854,\"start\":63849},{\"end\":63856,\"start\":63855},{\"end\":63869,\"start\":63862},{\"end\":63882,\"start\":63876},{\"end\":63896,\"start\":63890},{\"end\":63908,\"start\":63903},{\"end\":63917,\"start\":63913},{\"end\":63930,\"start\":63924},{\"end\":63947,\"start\":63941},{\"end\":64584,\"start\":64579},{\"end\":64594,\"start\":64591},{\"end\":64606,\"start\":64600},{\"end\":64618,\"start\":64612},{\"end\":64633,\"start\":64626},{\"end\":64641,\"start\":64638},{\"end\":64652,\"start\":64648},{\"end\":64664,\"start\":64657},{\"end\":64677,\"start\":64671},{\"end\":64689,\"start\":64682},{\"end\":65137,\"start\":65130},{\"end\":65150,\"start\":65144},{\"end\":65164,\"start\":65160},{\"end\":65166,\"start\":65165},{\"end\":65181,\"start\":65173},{\"end\":65459,\"start\":65451},{\"end\":65470,\"start\":65466},{\"end\":65484,\"start\":65477},{\"end\":65505,\"start\":65500},{\"end\":65527,\"start\":65514},{\"end\":65543,\"start\":65536},{\"end\":66004,\"start\":65997},{\"end\":66019,\"start\":66011},{\"end\":66031,\"start\":66025},{\"end\":66596,\"start\":66590},{\"end\":66608,\"start\":66602},{\"end\":66618,\"start\":66614},{\"end\":66632,\"start\":66625},{\"end\":66646,\"start\":66639},{\"end\":66655,\"start\":66653},{\"end\":66666,\"start\":66660},{\"end\":67284,\"start\":67278},{\"end\":67293,\"start\":67290},{\"end\":67304,\"start\":67300},{\"end\":67314,\"start\":67311},{\"end\":67325,\"start\":67320},{\"end\":67334,\"start\":67330},{\"end\":67345,\"start\":67341},{\"end\":67355,\"start\":67350},{\"end\":67691,\"start\":67688},{\"end\":67706,\"start\":67702},{\"end\":67720,\"start\":67714},{\"end\":67734,\"start\":67730},{\"end\":68106,\"start\":68101},{\"end\":68123,\"start\":68115},{\"end\":68137,\"start\":68131},{\"end\":68151,\"start\":68143},{\"end\":68393,\"start\":68388},{\"end\":68408,\"start\":68401},{\"end\":68417,\"start\":68414},{\"end\":68436,\"start\":68429},{\"end\":68451,\"start\":68444},{\"end\":68465,\"start\":68461},{\"end\":68486,\"start\":68478},{\"end\":68501,\"start\":68497},{\"end\":68937,\"start\":68933},{\"end\":68950,\"start\":68944},{\"end\":68963,\"start\":68956},{\"end\":68976,\"start\":68972},{\"end\":68987,\"start\":68986},{\"end\":69001,\"start\":68997},{\"end\":69392,\"start\":69388},{\"end\":69405,\"start\":69399},{\"end\":69415,\"start\":69411},{\"end\":69426,\"start\":69425},{\"end\":69440,\"start\":69436},{\"end\":69896,\"start\":69890},{\"end\":69907,\"start\":69901},{\"end\":69926,\"start\":69919},{\"end\":69938,\"start\":69933},{\"end\":69950,\"start\":69946},{\"end\":69958,\"start\":69955},{\"end\":69972,\"start\":69964},{\"end\":69984,\"start\":69979},{\"end\":69998,\"start\":69991},{\"end\":70437,\"start\":70433},{\"end\":70453,\"start\":70447},{\"end\":70465,\"start\":70459},{\"end\":70472,\"start\":70471},{\"end\":70486,\"start\":70482},{\"end\":70749,\"start\":70741},{\"end\":70774,\"start\":70767},{\"end\":70785,\"start\":70780},{\"end\":70801,\"start\":70794},{\"end\":71331,\"start\":71328},{\"end\":71345,\"start\":71338},{\"end\":71356,\"start\":71352},{\"end\":71365,\"start\":71361},{\"end\":71377,\"start\":71372},{\"end\":71611,\"start\":71608},{\"end\":71625,\"start\":71618},{\"end\":71637,\"start\":71630},{\"end\":71651,\"start\":71644},{\"end\":71662,\"start\":71657},{\"end\":71976,\"start\":71973},{\"end\":71990,\"start\":71983},{\"end\":72002,\"start\":71995},{\"end\":72013,\"start\":72008},{\"end\":72026,\"start\":72019},{\"end\":72038,\"start\":72033},{\"end\":72467,\"start\":72463},{\"end\":72475,\"start\":72473},{\"end\":72480,\"start\":72476},{\"end\":72495,\"start\":72488},{\"end\":72508,\"start\":72502},{\"end\":72517,\"start\":72514},{\"end\":72530,\"start\":72524},{\"end\":72545,\"start\":72537},{\"end\":72557,\"start\":72551},{\"end\":72567,\"start\":72562},{\"end\":72578,\"start\":72573},{\"end\":73091,\"start\":73087},{\"end\":73103,\"start\":73097},{\"end\":73116,\"start\":73109},{\"end\":73131,\"start\":73123},{\"end\":73141,\"start\":73137},{\"end\":73155,\"start\":73149},{\"end\":73164,\"start\":73161},{\"end\":73176,\"start\":73171},{\"end\":73187,\"start\":73182},{\"end\":73640,\"start\":73636},{\"end\":73652,\"start\":73646},{\"end\":73665,\"start\":73658},{\"end\":73676,\"start\":73672},{\"end\":73688,\"start\":73684},{\"end\":73700,\"start\":73694},{\"end\":73714,\"start\":73708},{\"end\":73723,\"start\":73720},{\"end\":73735,\"start\":73730},{\"end\":74203,\"start\":74199},{\"end\":74213,\"start\":74209},{\"end\":74228,\"start\":74221},{\"end\":74240,\"start\":74235},{\"end\":74249,\"start\":74246},{\"end\":74262,\"start\":74256},{\"end\":74273,\"start\":74268},{\"end\":74284,\"start\":74279},{\"end\":74659,\"start\":74655},{\"end\":74669,\"start\":74665},{\"end\":74684,\"start\":74677},{\"end\":74696,\"start\":74691},{\"end\":74705,\"start\":74702},{\"end\":74718,\"start\":74712},{\"end\":74729,\"start\":74724},{\"end\":75053,\"start\":75049},{\"end\":75066,\"start\":75059},{\"end\":75080,\"start\":75073},{\"end\":75092,\"start\":75087},{\"end\":75104,\"start\":75098},{\"end\":75113,\"start\":75109},{\"end\":75124,\"start\":75120},{\"end\":75132,\"start\":75129},{\"end\":75143,\"start\":75138},{\"end\":75533,\"start\":75528},{\"end\":75546,\"start\":75541},{\"end\":75563,\"start\":75556},{\"end\":75587,\"start\":75580},{\"end\":75849,\"start\":75845},{\"end\":75860,\"start\":75855},{\"end\":75873,\"start\":75866},{\"end\":75875,\"start\":75874},{\"end\":76267,\"start\":76260},{\"end\":76279,\"start\":76272},{\"end\":76295,\"start\":76287},{\"end\":76305,\"start\":76301},{\"end\":76751,\"start\":76745},{\"end\":76769,\"start\":76759},{\"end\":76780,\"start\":76778},{\"end\":76795,\"start\":76787},{\"end\":76808,\"start\":76801},{\"end\":76821,\"start\":76815},{\"end\":76837,\"start\":76830},{\"end\":76850,\"start\":76844},{\"end\":77432,\"start\":77425},{\"end\":77446,\"start\":77439},{\"end\":77475,\"start\":77462},{\"end\":77496,\"start\":77488},{\"end\":77510,\"start\":77503},{\"end\":77533,\"start\":77527},{\"end\":77995,\"start\":77989},{\"end\":78005,\"start\":78001},{\"end\":78018,\"start\":78013},{\"end\":78020,\"start\":78019},{\"end\":78033,\"start\":78029},{\"end\":78051,\"start\":78046},{\"end\":78065,\"start\":78061},{\"end\":78646,\"start\":78640},{\"end\":78648,\"start\":78647},{\"end\":78660,\"start\":78657},{\"end\":78665,\"start\":78661},{\"end\":78679,\"start\":78672},{\"end\":78694,\"start\":78690},{\"end\":78711,\"start\":78705},{\"end\":78713,\"start\":78712},{\"end\":78728,\"start\":78722},{\"end\":78742,\"start\":78737},{\"end\":78756,\"start\":78751},{\"end\":78766,\"start\":78761},{\"end\":78777,\"start\":78771},{\"end\":78788,\"start\":78782},{\"end\":78804,\"start\":78797},{\"end\":78818,\"start\":78812},{\"end\":78832,\"start\":78827},{\"end\":78847,\"start\":78840},{\"end\":78859,\"start\":78854},{\"end\":79366,\"start\":79365},{\"end\":79380,\"start\":79375},{\"end\":79396,\"start\":79389},{\"end\":79409,\"start\":79404},{\"end\":79423,\"start\":79417},{\"end\":79443,\"start\":79435},{\"end\":79458,\"start\":79453},{\"end\":79472,\"start\":79466},{\"end\":79483,\"start\":79480},{\"end\":79494,\"start\":79492},{\"end\":80130,\"start\":80124},{\"end\":80146,\"start\":80137},{\"end\":80160,\"start\":80152},{\"end\":80172,\"start\":80166},{\"end\":80186,\"start\":80178},{\"end\":80200,\"start\":80192},{\"end\":80211,\"start\":80207},{\"end\":80214,\"start\":80212},{\"end\":80230,\"start\":80222},{\"end\":80701,\"start\":80695},{\"end\":80716,\"start\":80708},{\"end\":80727,\"start\":80721},{\"end\":80741,\"start\":80735},{\"end\":80757,\"start\":80750},{\"end\":80771,\"start\":80766},{\"end\":80787,\"start\":80781},{\"end\":80801,\"start\":80796},{\"end\":80815,\"start\":80811},{\"end\":81195,\"start\":81188},{\"end\":81212,\"start\":81204},{\"end\":81222,\"start\":81219},{\"end\":81238,\"start\":81231},{\"end\":81555,\"start\":81548},{\"end\":81572,\"start\":81564},{\"end\":81580,\"start\":81579},{\"end\":81593,\"start\":81586},{\"end\":81595,\"start\":81594},{\"end\":81985,\"start\":81981},{\"end\":81998,\"start\":81993},{\"end\":82203,\"start\":82196},{\"end\":82216,\"start\":82210},{\"end\":82236,\"start\":82231},{\"end\":82252,\"start\":82248},{\"end\":82264,\"start\":82261},{\"end\":82273,\"start\":82270},{\"end\":82287,\"start\":82282},{\"end\":82294,\"start\":82288},{\"end\":82750,\"start\":82746},{\"end\":82764,\"start\":82758},{\"end\":82775,\"start\":82770},{\"end\":82789,\"start\":82783},{\"end\":82816,\"start\":82805},{\"end\":82830,\"start\":82826},{\"end\":82840,\"start\":82837},{\"end\":82855,\"start\":82851},{\"end\":83195,\"start\":83188},{\"end\":83204,\"start\":83200},{\"end\":83216,\"start\":83210},{\"end\":83229,\"start\":83222},{\"end\":83242,\"start\":83236},{\"end\":83256,\"start\":83248},{\"end\":83269,\"start\":83262},{\"end\":83281,\"start\":83276},{\"end\":83718,\"start\":83713},{\"end\":83731,\"start\":83724},{\"end\":83744,\"start\":83738},{\"end\":83757,\"start\":83750},{\"end\":83773,\"start\":83764},{\"end\":83787,\"start\":83780},{\"end\":83803,\"start\":83795},{\"end\":83816,\"start\":83810},{\"end\":83828,\"start\":83823},{\"end\":84519,\"start\":84514},{\"end\":84532,\"start\":84525},{\"end\":84545,\"start\":84539},{\"end\":84557,\"start\":84553},{\"end\":84568,\"start\":84564},{\"end\":84578,\"start\":84573},{\"end\":85364,\"start\":85359},{\"end\":85374,\"start\":85369},{\"end\":85386,\"start\":85382},{\"end\":85408,\"start\":85400},{\"end\":86009,\"start\":86004},{\"end\":86023,\"start\":86016},{\"end\":86036,\"start\":86030},{\"end\":86049,\"start\":86044},{\"end\":86063,\"start\":86059},{\"end\":86067,\"start\":86064},{\"end\":86584,\"start\":86578},{\"end\":86606,\"start\":86600},{\"end\":86639,\"start\":86624},{\"end\":86815,\"start\":86809},{\"end\":86829,\"start\":86823},{\"end\":86847,\"start\":86843},{\"end\":86851,\"start\":86848},{\"end\":86866,\"start\":86859},{\"end\":87756,\"start\":87748},{\"end\":87772,\"start\":87763},{\"end\":87785,\"start\":87778},{\"end\":88201,\"start\":88197},{\"end\":88213,\"start\":88210},{\"end\":88230,\"start\":88221},{\"end\":88242,\"start\":88238},{\"end\":88255,\"start\":88250},{\"end\":88273,\"start\":88266},{\"end\":88288,\"start\":88282},{\"end\":88304,\"start\":88298},{\"end\":88317,\"start\":88310},{\"end\":88334,\"start\":88330},{\"end\":88774,\"start\":88767},{\"end\":88784,\"start\":88783},{\"end\":88786,\"start\":88785},{\"end\":88799,\"start\":88795},{\"end\":88811,\"start\":88807},{\"end\":89224,\"start\":89220},{\"end\":89236,\"start\":89230},{\"end\":89255,\"start\":89247},{\"end\":89268,\"start\":89262},{\"end\":89287,\"start\":89278},{\"end\":89308,\"start\":89300},{\"end\":89320,\"start\":89314},{\"end\":89333,\"start\":89327},{\"end\":89835,\"start\":89829},{\"end\":89848,\"start\":89841},{\"end\":89859,\"start\":89855},{\"end\":89869,\"start\":89865},{\"end\":89880,\"start\":89876},{\"end\":89890,\"start\":89885},{\"end\":89901,\"start\":89896},{\"end\":90378,\"start\":90374},{\"end\":90392,\"start\":90388},{\"end\":90402,\"start\":90397},{\"end\":90415,\"start\":90410},{\"end\":90427,\"start\":90422},{\"end\":90440,\"start\":90436},{\"end\":90656,\"start\":90651},{\"end\":90679,\"start\":90674},{\"end\":91068,\"start\":91062},{\"end\":91084,\"start\":91080},{\"end\":91102,\"start\":91092},{\"end\":91117,\"start\":91112},{\"end\":91705,\"start\":91695},{\"end\":91720,\"start\":91715},{\"end\":92026,\"start\":92020},{\"end\":92053,\"start\":92042},{\"end\":92064,\"start\":92059},{\"end\":92078,\"start\":92072},{\"end\":92094,\"start\":92088},{\"end\":92106,\"start\":92101},{\"end\":92125,\"start\":92116},{\"end\":92131,\"start\":92126},{\"end\":92150,\"start\":92142},{\"end\":92624,\"start\":92618},{\"end\":92639,\"start\":92633},{\"end\":92651,\"start\":92646},{\"end\":92669,\"start\":92661},{\"end\":92681,\"start\":92677},{\"end\":92687,\"start\":92682},{\"end\":92700,\"start\":92693},{\"end\":92711,\"start\":92707},{\"end\":92724,\"start\":92720},{\"end\":93192,\"start\":93186},{\"end\":93207,\"start\":93201},{\"end\":93220,\"start\":93214},{\"end\":93236,\"start\":93227},{\"end\":93248,\"start\":93242},{\"end\":93260,\"start\":93255},{\"end\":93274,\"start\":93270},{\"end\":93690,\"start\":93684},{\"end\":93705,\"start\":93699},{\"end\":93718,\"start\":93712},{\"end\":93734,\"start\":93725},{\"end\":93746,\"start\":93740},{\"end\":93758,\"start\":93753},{\"end\":93772,\"start\":93768},{\"end\":94069,\"start\":94064},{\"end\":94080,\"start\":94076},{\"end\":94092,\"start\":94087},{\"end\":94104,\"start\":94097},{\"end\":94115,\"start\":94109},{\"end\":94125,\"start\":94121},{\"end\":94136,\"start\":94135},{\"end\":94150,\"start\":94146},{\"end\":94629,\"start\":94622},{\"end\":94642,\"start\":94636},{\"end\":94654,\"start\":94647},{\"end\":94667,\"start\":94659},{\"end\":94677,\"start\":94675},{\"end\":94692,\"start\":94685},{\"end\":94706,\"start\":94699},{\"end\":95193,\"start\":95188},{\"end\":95215,\"start\":95209},{\"end\":95229,\"start\":95222},{\"end\":95240,\"start\":95235},{\"end\":95247,\"start\":95241},{\"end\":95263,\"start\":95254},{\"end\":95276,\"start\":95271},{\"end\":95290,\"start\":95283},{\"end\":95749,\"start\":95743},{\"end\":95763,\"start\":95759},{\"end\":95777,\"start\":95773},{\"end\":95791,\"start\":95786},{\"end\":95808,\"start\":95803},{\"end\":95821,\"start\":95816},{\"end\":95823,\"start\":95822},{\"end\":95837,\"start\":95831},{\"end\":95851,\"start\":95846},{\"end\":96090,\"start\":96086},{\"end\":96106,\"start\":96097},{\"end\":96120,\"start\":96114},{\"end\":96135,\"start\":96130},{\"end\":96146,\"start\":96142},{\"end\":96161,\"start\":96153},{\"end\":96577,\"start\":96573},{\"end\":96591,\"start\":96584},{\"end\":96602,\"start\":96597},{\"end\":96610,\"start\":96608},{\"end\":96620,\"start\":96616},{\"end\":97036,\"start\":97032},{\"end\":97047,\"start\":97043},{\"end\":97063,\"start\":97055},{\"end\":97073,\"start\":97069},{\"end\":97085,\"start\":97079},{\"end\":97098,\"start\":97091},{\"end\":97491,\"start\":97486},{\"end\":97503,\"start\":97498},{\"end\":97515,\"start\":97508},{\"end\":97524,\"start\":97521},{\"end\":97533,\"start\":97531},{\"end\":98014,\"start\":98007},{\"end\":98027,\"start\":98020},{\"end\":98043,\"start\":98035},{\"end\":98057,\"start\":98051},{\"end\":98073,\"start\":98064},{\"end\":98083,\"start\":98081},{\"end\":98098,\"start\":98091},{\"end\":98112,\"start\":98103},{\"end\":98887,\"start\":98880},{\"end\":98898,\"start\":98894},{\"end\":98900,\"start\":98899},{\"end\":98915,\"start\":98910},{\"end\":98929,\"start\":98922},{\"end\":99409,\"start\":99406},{\"end\":99426,\"start\":99422},{\"end\":99854,\"start\":99850},{\"end\":99866,\"start\":99863},{\"end\":99882,\"start\":99877},{\"end\":99895,\"start\":99890},{\"end\":100331,\"start\":100327},{\"end\":100343,\"start\":100339},{\"end\":100355,\"start\":100348},{\"end\":100366,\"start\":100361},{\"end\":100380,\"start\":100373},{\"end\":100392,\"start\":100387},{\"end\":101195,\"start\":101187},{\"end\":101211,\"start\":101203},{\"end\":101229,\"start\":101218},{\"end\":101238,\"start\":101234},{\"end\":101649,\"start\":101643},{\"end\":101663,\"start\":101657},{\"end\":101671,\"start\":101668},{\"end\":101686,\"start\":101679},{\"end\":101698,\"start\":101692},{\"end\":101708,\"start\":101704},{\"end\":101715,\"start\":101713},{\"end\":101728,\"start\":101721},{\"end\":101740,\"start\":101735},{\"end\":102148,\"start\":102143},{\"end\":102163,\"start\":102156},{\"end\":102177,\"start\":102172},{\"end\":102190,\"start\":102185},{\"end\":102204,\"start\":102200},{\"end\":102218,\"start\":102211},{\"end\":102236,\"start\":102225},{\"end\":102248,\"start\":102244},{\"end\":102259,\"start\":102255},{\"end\":102266,\"start\":102264},{\"end\":102631,\"start\":102624},{\"end\":102647,\"start\":102639},{\"end\":102659,\"start\":102653},{\"end\":102669,\"start\":102665},{\"end\":102678,\"start\":102674},{\"end\":102682,\"start\":102679},{\"end\":102692,\"start\":102687},{\"end\":102706,\"start\":102702},{\"end\":102716,\"start\":102713},{\"end\":102730,\"start\":102723},{\"end\":102744,\"start\":102737},{\"end\":102756,\"start\":102750},{\"end\":102758,\"start\":102757},{\"end\":102772,\"start\":102769},{\"end\":103365,\"start\":103361},{\"end\":103376,\"start\":103373},{\"end\":103388,\"start\":103384},{\"end\":103400,\"start\":103395},{\"end\":103413,\"start\":103407},{\"end\":103865,\"start\":103857},{\"end\":103877,\"start\":103872},{\"end\":103887,\"start\":103882},{\"end\":103897,\"start\":103892},{\"end\":103906,\"start\":103904},{\"end\":103918,\"start\":103912},{\"end\":104375,\"start\":104371},{\"end\":104388,\"start\":104382},{\"end\":104395,\"start\":104393},{\"end\":104407,\"start\":104401},{\"end\":104420,\"start\":104413},{\"end\":104430,\"start\":104426},{\"end\":104443,\"start\":104437},{\"end\":104454,\"start\":104450},{\"end\":104466,\"start\":104459},{\"end\":104478,\"start\":104473},{\"end\":105012,\"start\":105005},{\"end\":105026,\"start\":105019},{\"end\":105038,\"start\":105033},{\"end\":105051,\"start\":105045},{\"end\":105060,\"start\":105056},{\"end\":105071,\"start\":105067},{\"end\":105081,\"start\":105076},{\"end\":105094,\"start\":105087},{\"end\":105105,\"start\":105101},{\"end\":105113,\"start\":105110},{\"end\":105863,\"start\":105856},{\"end\":105878,\"start\":105870},{\"end\":105889,\"start\":105885},{\"end\":105902,\"start\":105895},{\"end\":105912,\"start\":105909},{\"end\":105924,\"start\":105920},{\"end\":105936,\"start\":105931},{\"end\":105947,\"start\":105942},{\"end\":106458,\"start\":106452},{\"end\":106470,\"start\":106464},{\"end\":106480,\"start\":106475},{\"end\":106494,\"start\":106487},{\"end\":106504,\"start\":106499},{\"end\":106513,\"start\":106509},{\"end\":106527,\"start\":106521},{\"end\":106541,\"start\":106533},{\"end\":106552,\"start\":106546},{\"end\":106561,\"start\":106558},{\"end\":106570,\"start\":106567},{\"end\":106580,\"start\":106577},{\"end\":106593,\"start\":106587},{\"end\":107139,\"start\":107133},{\"end\":107148,\"start\":107145},{\"end\":107162,\"start\":107156},{\"end\":107171,\"start\":107167},{\"end\":107695,\"start\":107690},{\"end\":107711,\"start\":107704},{\"end\":107721,\"start\":107717},{\"end\":107735,\"start\":107727},{\"end\":107744,\"start\":107741},{\"end\":107758,\"start\":107751}]", "bib_author_last_name": "[{\"end\":58578,\"start\":58569},{\"end\":58592,\"start\":58588},{\"end\":58613,\"start\":58601},{\"end\":58625,\"start\":58619},{\"end\":58649,\"start\":58643},{\"end\":58667,\"start\":58659},{\"end\":59140,\"start\":59133},{\"end\":59153,\"start\":59149},{\"end\":59166,\"start\":59162},{\"end\":59178,\"start\":59174},{\"end\":59190,\"start\":59184},{\"end\":59815,\"start\":59809},{\"end\":59829,\"start\":59822},{\"end\":59844,\"start\":59838},{\"end\":60200,\"start\":60194},{\"end\":60218,\"start\":60211},{\"end\":60235,\"start\":60226},{\"end\":60517,\"start\":60514},{\"end\":60529,\"start\":60526},{\"end\":60540,\"start\":60536},{\"end\":60554,\"start\":60547},{\"end\":60565,\"start\":60558},{\"end\":60579,\"start\":60572},{\"end\":60588,\"start\":60581},{\"end\":61063,\"start\":61060},{\"end\":61081,\"start\":61070},{\"end\":61565,\"start\":61561},{\"end\":61579,\"start\":61575},{\"end\":61590,\"start\":61587},{\"end\":61611,\"start\":61596},{\"end\":61623,\"start\":61619},{\"end\":61637,\"start\":61633},{\"end\":62487,\"start\":62483},{\"end\":62501,\"start\":62497},{\"end\":62513,\"start\":62509},{\"end\":62527,\"start\":62523},{\"end\":63352,\"start\":63348},{\"end\":63363,\"start\":63361},{\"end\":63376,\"start\":63373},{\"end\":63386,\"start\":63382},{\"end\":63400,\"start\":63398},{\"end\":63412,\"start\":63408},{\"end\":63427,\"start\":63422},{\"end\":63802,\"start\":63798},{\"end\":63818,\"start\":63812},{\"end\":63832,\"start\":63827},{\"end\":63847,\"start\":63842},{\"end\":63860,\"start\":63857},{\"end\":63874,\"start\":63870},{\"end\":63888,\"start\":63883},{\"end\":63901,\"start\":63897},{\"end\":63911,\"start\":63909},{\"end\":63922,\"start\":63918},{\"end\":63939,\"start\":63931},{\"end\":63961,\"start\":63948},{\"end\":64589,\"start\":64585},{\"end\":64598,\"start\":64595},{\"end\":64610,\"start\":64607},{\"end\":64624,\"start\":64619},{\"end\":64636,\"start\":64634},{\"end\":64646,\"start\":64642},{\"end\":64655,\"start\":64653},{\"end\":64669,\"start\":64665},{\"end\":64680,\"start\":64678},{\"end\":64693,\"start\":64690},{\"end\":65142,\"start\":65138},{\"end\":65158,\"start\":65151},{\"end\":65171,\"start\":65167},{\"end\":65185,\"start\":65182},{\"end\":65464,\"start\":65460},{\"end\":65475,\"start\":65471},{\"end\":65498,\"start\":65485},{\"end\":65512,\"start\":65506},{\"end\":65534,\"start\":65528},{\"end\":65554,\"start\":65544},{\"end\":65570,\"start\":65556},{\"end\":66009,\"start\":66005},{\"end\":66023,\"start\":66020},{\"end\":66035,\"start\":66032},{\"end\":66600,\"start\":66597},{\"end\":66612,\"start\":66609},{\"end\":66623,\"start\":66619},{\"end\":66637,\"start\":66633},{\"end\":66651,\"start\":66647},{\"end\":66658,\"start\":66656},{\"end\":66670,\"start\":66667},{\"end\":67288,\"start\":67285},{\"end\":67298,\"start\":67294},{\"end\":67309,\"start\":67305},{\"end\":67318,\"start\":67315},{\"end\":67328,\"start\":67326},{\"end\":67339,\"start\":67335},{\"end\":67348,\"start\":67346},{\"end\":67359,\"start\":67356},{\"end\":67700,\"start\":67692},{\"end\":67712,\"start\":67707},{\"end\":67728,\"start\":67721},{\"end\":67746,\"start\":67735},{\"end\":68113,\"start\":68107},{\"end\":68129,\"start\":68124},{\"end\":68141,\"start\":68138},{\"end\":68161,\"start\":68152},{\"end\":68399,\"start\":68394},{\"end\":68412,\"start\":68409},{\"end\":68427,\"start\":68418},{\"end\":68442,\"start\":68437},{\"end\":68459,\"start\":68452},{\"end\":68476,\"start\":68466},{\"end\":68495,\"start\":68487},{\"end\":68509,\"start\":68502},{\"end\":68942,\"start\":68938},{\"end\":68954,\"start\":68951},{\"end\":68970,\"start\":68964},{\"end\":68984,\"start\":68977},{\"end\":68995,\"start\":68988},{\"end\":69009,\"start\":69002},{\"end\":69018,\"start\":69011},{\"end\":69397,\"start\":69393},{\"end\":69409,\"start\":69406},{\"end\":69423,\"start\":69416},{\"end\":69434,\"start\":69427},{\"end\":69448,\"start\":69441},{\"end\":69457,\"start\":69450},{\"end\":69899,\"start\":69897},{\"end\":69917,\"start\":69908},{\"end\":69931,\"start\":69927},{\"end\":69944,\"start\":69939},{\"end\":69953,\"start\":69951},{\"end\":69962,\"start\":69959},{\"end\":69977,\"start\":69973},{\"end\":69989,\"start\":69985},{\"end\":70004,\"start\":69999},{\"end\":70445,\"start\":70438},{\"end\":70457,\"start\":70454},{\"end\":70469,\"start\":70466},{\"end\":70480,\"start\":70473},{\"end\":70494,\"start\":70487},{\"end\":70503,\"start\":70496},{\"end\":70765,\"start\":70750},{\"end\":70778,\"start\":70775},{\"end\":70792,\"start\":70786},{\"end\":70809,\"start\":70802},{\"end\":70822,\"start\":70811},{\"end\":71336,\"start\":71332},{\"end\":71350,\"start\":71346},{\"end\":71359,\"start\":71357},{\"end\":71370,\"start\":71366},{\"end\":71381,\"start\":71378},{\"end\":71616,\"start\":71612},{\"end\":71628,\"start\":71626},{\"end\":71642,\"start\":71638},{\"end\":71655,\"start\":71652},{\"end\":71666,\"start\":71663},{\"end\":71981,\"start\":71977},{\"end\":71993,\"start\":71991},{\"end\":72006,\"start\":72003},{\"end\":72017,\"start\":72014},{\"end\":72031,\"start\":72027},{\"end\":72042,\"start\":72039},{\"end\":72471,\"start\":72468},{\"end\":72486,\"start\":72481},{\"end\":72500,\"start\":72496},{\"end\":72512,\"start\":72509},{\"end\":72522,\"start\":72518},{\"end\":72535,\"start\":72531},{\"end\":72549,\"start\":72546},{\"end\":72560,\"start\":72558},{\"end\":72571,\"start\":72568},{\"end\":72582,\"start\":72579},{\"end\":73095,\"start\":73092},{\"end\":73107,\"start\":73104},{\"end\":73121,\"start\":73117},{\"end\":73135,\"start\":73132},{\"end\":73147,\"start\":73142},{\"end\":73159,\"start\":73156},{\"end\":73169,\"start\":73165},{\"end\":73180,\"start\":73177},{\"end\":73191,\"start\":73188},{\"end\":73644,\"start\":73641},{\"end\":73656,\"start\":73653},{\"end\":73670,\"start\":73666},{\"end\":73682,\"start\":73677},{\"end\":73692,\"start\":73689},{\"end\":73706,\"start\":73701},{\"end\":73718,\"start\":73715},{\"end\":73728,\"start\":73724},{\"end\":73739,\"start\":73736},{\"end\":74207,\"start\":74204},{\"end\":74219,\"start\":74214},{\"end\":74233,\"start\":74229},{\"end\":74244,\"start\":74241},{\"end\":74254,\"start\":74250},{\"end\":74266,\"start\":74263},{\"end\":74277,\"start\":74274},{\"end\":74288,\"start\":74285},{\"end\":74663,\"start\":74660},{\"end\":74675,\"start\":74670},{\"end\":74689,\"start\":74685},{\"end\":74700,\"start\":74697},{\"end\":74710,\"start\":74706},{\"end\":74722,\"start\":74719},{\"end\":74733,\"start\":74730},{\"end\":75057,\"start\":75054},{\"end\":75071,\"start\":75067},{\"end\":75085,\"start\":75081},{\"end\":75096,\"start\":75093},{\"end\":75107,\"start\":75105},{\"end\":75118,\"start\":75114},{\"end\":75127,\"start\":75125},{\"end\":75136,\"start\":75133},{\"end\":75147,\"start\":75144},{\"end\":75539,\"start\":75534},{\"end\":75554,\"start\":75547},{\"end\":75578,\"start\":75564},{\"end\":75597,\"start\":75588},{\"end\":75853,\"start\":75850},{\"end\":75864,\"start\":75861},{\"end\":75881,\"start\":75876},{\"end\":76270,\"start\":76268},{\"end\":76285,\"start\":76280},{\"end\":76299,\"start\":76296},{\"end\":76309,\"start\":76306},{\"end\":76757,\"start\":76752},{\"end\":76776,\"start\":76770},{\"end\":76785,\"start\":76781},{\"end\":76799,\"start\":76796},{\"end\":76813,\"start\":76809},{\"end\":76828,\"start\":76822},{\"end\":76842,\"start\":76838},{\"end\":76863,\"start\":76851},{\"end\":77437,\"start\":77433},{\"end\":77460,\"start\":77447},{\"end\":77486,\"start\":77476},{\"end\":77501,\"start\":77497},{\"end\":77525,\"start\":77511},{\"end\":77539,\"start\":77534},{\"end\":77999,\"start\":77996},{\"end\":78011,\"start\":78006},{\"end\":78027,\"start\":78021},{\"end\":78044,\"start\":78034},{\"end\":78059,\"start\":78052},{\"end\":78071,\"start\":78066},{\"end\":78655,\"start\":78649},{\"end\":78670,\"start\":78666},{\"end\":78688,\"start\":78680},{\"end\":78703,\"start\":78695},{\"end\":78720,\"start\":78714},{\"end\":78735,\"start\":78729},{\"end\":78749,\"start\":78743},{\"end\":78759,\"start\":78757},{\"end\":78769,\"start\":78767},{\"end\":78780,\"start\":78778},{\"end\":78795,\"start\":78789},{\"end\":78810,\"start\":78805},{\"end\":78825,\"start\":78819},{\"end\":78838,\"start\":78833},{\"end\":78852,\"start\":78848},{\"end\":78869,\"start\":78860},{\"end\":79373,\"start\":79367},{\"end\":79387,\"start\":79381},{\"end\":79402,\"start\":79397},{\"end\":79415,\"start\":79410},{\"end\":79433,\"start\":79424},{\"end\":79451,\"start\":79444},{\"end\":79464,\"start\":79459},{\"end\":79478,\"start\":79473},{\"end\":79490,\"start\":79484},{\"end\":79500,\"start\":79495},{\"end\":79510,\"start\":79502},{\"end\":80135,\"start\":80131},{\"end\":80150,\"start\":80147},{\"end\":80164,\"start\":80161},{\"end\":80176,\"start\":80173},{\"end\":80190,\"start\":80187},{\"end\":80205,\"start\":80201},{\"end\":80220,\"start\":80215},{\"end\":80235,\"start\":80231},{\"end\":80706,\"start\":80702},{\"end\":80719,\"start\":80717},{\"end\":80733,\"start\":80728},{\"end\":80748,\"start\":80742},{\"end\":80764,\"start\":80758},{\"end\":80779,\"start\":80772},{\"end\":80794,\"start\":80788},{\"end\":80809,\"start\":80802},{\"end\":80824,\"start\":80816},{\"end\":81202,\"start\":81196},{\"end\":81217,\"start\":81213},{\"end\":81229,\"start\":81223},{\"end\":81245,\"start\":81239},{\"end\":81562,\"start\":81556},{\"end\":81577,\"start\":81573},{\"end\":81584,\"start\":81581},{\"end\":81602,\"start\":81596},{\"end\":81610,\"start\":81604},{\"end\":81991,\"start\":81986},{\"end\":82005,\"start\":81999},{\"end\":82208,\"start\":82204},{\"end\":82229,\"start\":82217},{\"end\":82246,\"start\":82237},{\"end\":82259,\"start\":82253},{\"end\":82268,\"start\":82265},{\"end\":82280,\"start\":82274},{\"end\":82300,\"start\":82295},{\"end\":82756,\"start\":82751},{\"end\":82768,\"start\":82765},{\"end\":82781,\"start\":82776},{\"end\":82803,\"start\":82790},{\"end\":82824,\"start\":82817},{\"end\":82835,\"start\":82831},{\"end\":82849,\"start\":82841},{\"end\":82867,\"start\":82856},{\"end\":83198,\"start\":83196},{\"end\":83208,\"start\":83205},{\"end\":83220,\"start\":83217},{\"end\":83234,\"start\":83230},{\"end\":83246,\"start\":83243},{\"end\":83260,\"start\":83257},{\"end\":83274,\"start\":83270},{\"end\":83285,\"start\":83282},{\"end\":83722,\"start\":83719},{\"end\":83736,\"start\":83732},{\"end\":83748,\"start\":83745},{\"end\":83762,\"start\":83758},{\"end\":83778,\"start\":83774},{\"end\":83793,\"start\":83788},{\"end\":83808,\"start\":83804},{\"end\":83821,\"start\":83817},{\"end\":83833,\"start\":83829},{\"end\":84523,\"start\":84520},{\"end\":84537,\"start\":84533},{\"end\":84551,\"start\":84546},{\"end\":84562,\"start\":84558},{\"end\":84571,\"start\":84569},{\"end\":84582,\"start\":84579},{\"end\":85367,\"start\":85365},{\"end\":85380,\"start\":85375},{\"end\":85398,\"start\":85387},{\"end\":85434,\"start\":85409},{\"end\":85445,\"start\":85436},{\"end\":86014,\"start\":86010},{\"end\":86028,\"start\":86024},{\"end\":86042,\"start\":86037},{\"end\":86057,\"start\":86050},{\"end\":86073,\"start\":86068},{\"end\":86598,\"start\":86585},{\"end\":86622,\"start\":86607},{\"end\":86821,\"start\":86816},{\"end\":86841,\"start\":86830},{\"end\":86857,\"start\":86852},{\"end\":86877,\"start\":86867},{\"end\":87248,\"start\":87242},{\"end\":87394,\"start\":87388},{\"end\":87542,\"start\":87536},{\"end\":87761,\"start\":87757},{\"end\":87776,\"start\":87773},{\"end\":87789,\"start\":87786},{\"end\":88208,\"start\":88202},{\"end\":88219,\"start\":88214},{\"end\":88236,\"start\":88231},{\"end\":88248,\"start\":88243},{\"end\":88264,\"start\":88256},{\"end\":88280,\"start\":88274},{\"end\":88296,\"start\":88289},{\"end\":88308,\"start\":88305},{\"end\":88328,\"start\":88318},{\"end\":88341,\"start\":88335},{\"end\":88781,\"start\":88775},{\"end\":88793,\"start\":88787},{\"end\":88805,\"start\":88800},{\"end\":88818,\"start\":88812},{\"end\":88829,\"start\":88820},{\"end\":89228,\"start\":89225},{\"end\":89245,\"start\":89237},{\"end\":89260,\"start\":89256},{\"end\":89276,\"start\":89269},{\"end\":89298,\"start\":89288},{\"end\":89312,\"start\":89309},{\"end\":89325,\"start\":89321},{\"end\":89341,\"start\":89334},{\"end\":89839,\"start\":89836},{\"end\":89853,\"start\":89849},{\"end\":89863,\"start\":89860},{\"end\":89874,\"start\":89870},{\"end\":89883,\"start\":89881},{\"end\":89894,\"start\":89891},{\"end\":89905,\"start\":89902},{\"end\":90386,\"start\":90379},{\"end\":90395,\"start\":90393},{\"end\":90408,\"start\":90403},{\"end\":90420,\"start\":90416},{\"end\":90434,\"start\":90428},{\"end\":90450,\"start\":90441},{\"end\":90672,\"start\":90657},{\"end\":90684,\"start\":90680},{\"end\":90692,\"start\":90686},{\"end\":91078,\"start\":91069},{\"end\":91090,\"start\":91085},{\"end\":91110,\"start\":91103},{\"end\":91123,\"start\":91118},{\"end\":91713,\"start\":91706},{\"end\":91725,\"start\":91721},{\"end\":92040,\"start\":92027},{\"end\":92057,\"start\":92054},{\"end\":92070,\"start\":92065},{\"end\":92086,\"start\":92079},{\"end\":92099,\"start\":92095},{\"end\":92114,\"start\":92107},{\"end\":92140,\"start\":92132},{\"end\":92159,\"start\":92151},{\"end\":92165,\"start\":92161},{\"end\":92631,\"start\":92625},{\"end\":92644,\"start\":92640},{\"end\":92659,\"start\":92652},{\"end\":92675,\"start\":92670},{\"end\":92691,\"start\":92688},{\"end\":92705,\"start\":92701},{\"end\":92718,\"start\":92712},{\"end\":92737,\"start\":92725},{\"end\":93199,\"start\":93193},{\"end\":93212,\"start\":93208},{\"end\":93225,\"start\":93221},{\"end\":93240,\"start\":93237},{\"end\":93253,\"start\":93249},{\"end\":93268,\"start\":93261},{\"end\":93287,\"start\":93275},{\"end\":93697,\"start\":93691},{\"end\":93710,\"start\":93706},{\"end\":93723,\"start\":93719},{\"end\":93738,\"start\":93735},{\"end\":93751,\"start\":93747},{\"end\":93766,\"start\":93759},{\"end\":93785,\"start\":93773},{\"end\":94074,\"start\":94070},{\"end\":94085,\"start\":94081},{\"end\":94095,\"start\":94093},{\"end\":94107,\"start\":94105},{\"end\":94119,\"start\":94116},{\"end\":94133,\"start\":94126},{\"end\":94144,\"start\":94137},{\"end\":94158,\"start\":94151},{\"end\":94167,\"start\":94160},{\"end\":94634,\"start\":94630},{\"end\":94645,\"start\":94643},{\"end\":94657,\"start\":94655},{\"end\":94673,\"start\":94668},{\"end\":94683,\"start\":94678},{\"end\":94697,\"start\":94693},{\"end\":94712,\"start\":94707},{\"end\":95207,\"start\":95194},{\"end\":95220,\"start\":95216},{\"end\":95233,\"start\":95230},{\"end\":95252,\"start\":95248},{\"end\":95269,\"start\":95264},{\"end\":95281,\"start\":95277},{\"end\":95297,\"start\":95291},{\"end\":95302,\"start\":95299},{\"end\":95757,\"start\":95750},{\"end\":95771,\"start\":95764},{\"end\":95784,\"start\":95778},{\"end\":95801,\"start\":95792},{\"end\":95814,\"start\":95809},{\"end\":95829,\"start\":95824},{\"end\":95844,\"start\":95838},{\"end\":95862,\"start\":95852},{\"end\":96095,\"start\":96091},{\"end\":96112,\"start\":96107},{\"end\":96128,\"start\":96121},{\"end\":96140,\"start\":96136},{\"end\":96151,\"start\":96147},{\"end\":96168,\"start\":96162},{\"end\":96582,\"start\":96578},{\"end\":96595,\"start\":96592},{\"end\":96606,\"start\":96603},{\"end\":96614,\"start\":96611},{\"end\":96624,\"start\":96621},{\"end\":97041,\"start\":97037},{\"end\":97053,\"start\":97048},{\"end\":97067,\"start\":97064},{\"end\":97077,\"start\":97074},{\"end\":97089,\"start\":97086},{\"end\":97103,\"start\":97099},{\"end\":97496,\"start\":97492},{\"end\":97506,\"start\":97504},{\"end\":97519,\"start\":97516},{\"end\":97529,\"start\":97525},{\"end\":97538,\"start\":97534},{\"end\":98018,\"start\":98015},{\"end\":98033,\"start\":98028},{\"end\":98049,\"start\":98044},{\"end\":98062,\"start\":98058},{\"end\":98079,\"start\":98074},{\"end\":98089,\"start\":98084},{\"end\":98101,\"start\":98099},{\"end\":98116,\"start\":98113},{\"end\":98892,\"start\":98888},{\"end\":98908,\"start\":98901},{\"end\":98920,\"start\":98916},{\"end\":98934,\"start\":98930},{\"end\":99420,\"start\":99410},{\"end\":99430,\"start\":99427},{\"end\":99861,\"start\":99855},{\"end\":99875,\"start\":99867},{\"end\":99888,\"start\":99883},{\"end\":99906,\"start\":99896},{\"end\":100337,\"start\":100332},{\"end\":100346,\"start\":100344},{\"end\":100359,\"start\":100356},{\"end\":100371,\"start\":100367},{\"end\":100385,\"start\":100381},{\"end\":100397,\"start\":100393},{\"end\":100942,\"start\":100937},{\"end\":101201,\"start\":101196},{\"end\":101216,\"start\":101212},{\"end\":101232,\"start\":101230},{\"end\":101242,\"start\":101239},{\"end\":101655,\"start\":101650},{\"end\":101666,\"start\":101664},{\"end\":101677,\"start\":101672},{\"end\":101690,\"start\":101687},{\"end\":101702,\"start\":101699},{\"end\":101711,\"start\":101709},{\"end\":101719,\"start\":101716},{\"end\":101733,\"start\":101729},{\"end\":101745,\"start\":101741},{\"end\":102154,\"start\":102149},{\"end\":102170,\"start\":102164},{\"end\":102183,\"start\":102178},{\"end\":102198,\"start\":102191},{\"end\":102209,\"start\":102205},{\"end\":102223,\"start\":102219},{\"end\":102242,\"start\":102237},{\"end\":102253,\"start\":102249},{\"end\":102262,\"start\":102260},{\"end\":102279,\"start\":102267},{\"end\":102637,\"start\":102632},{\"end\":102651,\"start\":102648},{\"end\":102663,\"start\":102660},{\"end\":102672,\"start\":102670},{\"end\":102685,\"start\":102683},{\"end\":102700,\"start\":102693},{\"end\":102711,\"start\":102707},{\"end\":102721,\"start\":102717},{\"end\":102735,\"start\":102731},{\"end\":102748,\"start\":102745},{\"end\":102767,\"start\":102759},{\"end\":102779,\"start\":102773},{\"end\":103371,\"start\":103366},{\"end\":103382,\"start\":103377},{\"end\":103393,\"start\":103389},{\"end\":103405,\"start\":103401},{\"end\":103419,\"start\":103414},{\"end\":103870,\"start\":103866},{\"end\":103880,\"start\":103878},{\"end\":103890,\"start\":103888},{\"end\":103902,\"start\":103898},{\"end\":103910,\"start\":103907},{\"end\":103922,\"start\":103919},{\"end\":104380,\"start\":104376},{\"end\":104391,\"start\":104389},{\"end\":104399,\"start\":104396},{\"end\":104411,\"start\":104408},{\"end\":104424,\"start\":104421},{\"end\":104435,\"start\":104431},{\"end\":104448,\"start\":104444},{\"end\":104457,\"start\":104455},{\"end\":104471,\"start\":104467},{\"end\":104483,\"start\":104479},{\"end\":105017,\"start\":105013},{\"end\":105031,\"start\":105027},{\"end\":105043,\"start\":105039},{\"end\":105054,\"start\":105052},{\"end\":105065,\"start\":105061},{\"end\":105074,\"start\":105072},{\"end\":105085,\"start\":105082},{\"end\":105099,\"start\":105095},{\"end\":105108,\"start\":105106},{\"end\":105117,\"start\":105114},{\"end\":105868,\"start\":105864},{\"end\":105883,\"start\":105879},{\"end\":105893,\"start\":105890},{\"end\":105907,\"start\":105903},{\"end\":105918,\"start\":105913},{\"end\":105929,\"start\":105925},{\"end\":105940,\"start\":105937},{\"end\":105951,\"start\":105948},{\"end\":106462,\"start\":106459},{\"end\":106473,\"start\":106471},{\"end\":106485,\"start\":106481},{\"end\":106497,\"start\":106495},{\"end\":106507,\"start\":106505},{\"end\":106519,\"start\":106514},{\"end\":106531,\"start\":106528},{\"end\":106544,\"start\":106542},{\"end\":106556,\"start\":106553},{\"end\":106565,\"start\":106562},{\"end\":106575,\"start\":106571},{\"end\":106585,\"start\":106581},{\"end\":106598,\"start\":106594},{\"end\":107143,\"start\":107140},{\"end\":107154,\"start\":107149},{\"end\":107165,\"start\":107163},{\"end\":107175,\"start\":107172},{\"end\":107702,\"start\":107696},{\"end\":107715,\"start\":107712},{\"end\":107725,\"start\":107722},{\"end\":107739,\"start\":107736},{\"end\":107749,\"start\":107745},{\"end\":107763,\"start\":107759}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9841314},\"end\":58950,\"start\":58498},{\"attributes\":{\"id\":\"b1\"},\"end\":59493,\"start\":58952},{\"attributes\":{\"doi\":\"10.1109/ISPASS.2009.4919648\",\"id\":\"b2\"},\"end\":59722,\"start\":59495},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":59292009},\"end\":60093,\"start\":59724},{\"attributes\":{\"doi\":\"arXiv:1308.3432\",\"id\":\"b4\"},\"end\":60456,\"start\":60095},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209531713},\"end\":60980,\"start\":60458},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":215745195},\"end\":61438,\"start\":60982},{\"attributes\":{\"doi\":\"10.1145/3037697.3037700\",\"id\":\"b7\",\"matched_paper_id\":55216},\"end\":62366,\"start\":61440},{\"attributes\":{\"doi\":\"10.1145/2872362.2872368\",\"id\":\"b8\",\"matched_paper_id\":13778068},\"end\":63250,\"start\":62368},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":207209696},\"end\":63721,\"start\":63252},{\"attributes\":{\"doi\":\"10.5555/3291168.3291211\",\"id\":\"b10\",\"matched_paper_id\":52939079},\"end\":64533,\"start\":63723},{\"attributes\":{\"id\":\"b11\"},\"end\":65032,\"start\":64535},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207882941},\"end\":65449,\"start\":65034},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b13\"},\"end\":65913,\"start\":65451},{\"attributes\":{\"doi\":\"10.1109/HPCA51647.2021.00049\",\"id\":\"b14\",\"matched_paper_id\":225070164},\"end\":66496,\"start\":65915},{\"attributes\":{\"doi\":\"10.1109/ICCD46524.2019.00075\",\"id\":\"b15\",\"matched_paper_id\":211120370},\"end\":67168,\"start\":66498},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":252819592},\"end\":67686,\"start\":67170},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b17\"},\"end\":68017,\"start\":67688},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b18\"},\"end\":68386,\"start\":68019},{\"attributes\":{\"doi\":\"arXiv:arXiv:2104.08758\",\"id\":\"b19\"},\"end\":68860,\"start\":68388},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207852310},\"end\":69313,\"start\":68862},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":148571720},\"end\":69827,\"start\":69315},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11504619},\"end\":70429,\"start\":69829},{\"attributes\":{\"id\":\"b23\"},\"end\":70680,\"start\":70431},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7311716},\"end\":71237,\"start\":70682},{\"attributes\":{\"doi\":\"arXiv:2011.00943\",\"id\":\"b25\"},\"end\":71606,\"start\":71239},{\"attributes\":{\"doi\":\"arXiv:2205.07324\",\"id\":\"b26\"},\"end\":71913,\"start\":71608},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":245219235},\"end\":72381,\"start\":71915},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":221376946},\"end\":73007,\"start\":72383},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":246863709},\"end\":73541,\"start\":73009},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":252438632},\"end\":74106,\"start\":73543},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":251928917},\"end\":74651,\"start\":74108},{\"attributes\":{\"id\":\"b32\"},\"end\":74944,\"start\":74653},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":211171723},\"end\":75478,\"start\":74946},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b34\",\"matched_paper_id\":2547043},\"end\":75843,\"start\":75480},{\"attributes\":{\"doi\":\"arXiv:1510.00149\",\"id\":\"b35\"},\"end\":76212,\"start\":75845},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206594692},\"end\":76649,\"start\":76214},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":39867659},\"end\":77322,\"start\":76651},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":163164956},\"end\":77894,\"start\":77324},{\"attributes\":{\"doi\":\"10.1145/3341301.3359630\",\"id\":\"b39\",\"matched_paper_id\":202726856},\"end\":78559,\"start\":77896},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235415875},\"end\":79361,\"start\":78561},{\"attributes\":{\"id\":\"b41\"},\"end\":79670,\"start\":79363},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4202768},\"end\":80034,\"start\":79672},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":53719799},\"end\":80693,\"start\":80036},{\"attributes\":{\"id\":\"b44\"},\"end\":81112,\"start\":80695},{\"attributes\":{\"doi\":\"10.1109/ISCA45697.2020.00047\",\"id\":\"b45\"},\"end\":81472,\"start\":81114},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":219117576},\"end\":81946,\"start\":81474},{\"attributes\":{\"id\":\"b47\"},\"end\":82142,\"start\":81948},{\"attributes\":{\"doi\":\"10.1145/2508148.2485964\",\"id\":\"b48\",\"matched_paper_id\":7452683},\"end\":82629,\"start\":82144},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b49\"},\"end\":83186,\"start\":82631},{\"attributes\":{\"doi\":\"arXiv:2208.11945\",\"id\":\"b50\"},\"end\":83657,\"start\":83188},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":8940335},\"end\":84401,\"start\":83659},{\"attributes\":{\"doi\":\"10.1145/3503222.3507752\",\"id\":\"b52\",\"matched_paper_id\":246015923},\"end\":85307,\"start\":84403},{\"attributes\":{\"doi\":\"10.1145/2749469.2749475\",\"id\":\"b53\",\"matched_paper_id\":14090534},\"end\":85905,\"start\":85309},{\"attributes\":{\"doi\":\"10.1145/2155620.2155650\",\"id\":\"b54\",\"matched_paper_id\":3338465},\"end\":86444,\"start\":85907},{\"attributes\":{\"id\":\"b55\"},\"end\":86535,\"start\":86446},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":14245854},\"end\":86807,\"start\":86537},{\"attributes\":{\"doi\":\"arXiv:2106.08295\",\"id\":\"b57\"},\"end\":87204,\"start\":86809},{\"attributes\":{\"id\":\"b58\"},\"end\":87354,\"start\":87206},{\"attributes\":{\"id\":\"b59\"},\"end\":87496,\"start\":87356},{\"attributes\":{\"id\":\"b60\"},\"end\":87650,\"start\":87498},{\"attributes\":{\"id\":\"b61\"},\"end\":87947,\"start\":87652},{\"attributes\":{\"id\":\"b62\"},\"end\":88125,\"start\":87949},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":202786778},\"end\":88696,\"start\":88127},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":2398978},\"end\":89125,\"start\":88698},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":211114941},\"end\":89758,\"start\":89127},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":119309714},\"end\":90319,\"start\":89760},{\"attributes\":{\"id\":\"b67\"},\"end\":90600,\"start\":90321},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":53783076},\"end\":90999,\"start\":90602},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":11816014},\"end\":91594,\"start\":91001},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":231986398},\"end\":92018,\"start\":91596},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b71\"},\"end\":92571,\"start\":92020},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":525898},\"end\":93088,\"start\":92573},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":21681898},\"end\":93682,\"start\":93090},{\"attributes\":{\"id\":\"b74\"},\"end\":93998,\"start\":93684},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":202565587},\"end\":94543,\"start\":94000},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":219117593},\"end\":95082,\"start\":94545},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":221094303},\"end\":95663,\"start\":95084},{\"attributes\":{\"id\":\"b78\"},\"end\":96084,\"start\":95665},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b79\"},\"end\":96507,\"start\":96086},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":102350477},\"end\":97000,\"start\":96509},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":234790037},\"end\":97407,\"start\":97002},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":198187673},\"end\":97926,\"start\":97409},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":252545187},\"end\":98604,\"start\":97928},{\"attributes\":{\"id\":\"b84\"},\"end\":98779,\"start\":98606},{\"attributes\":{\"doi\":\"10.1145/2485922.2485974\",\"id\":\"b85\",\"matched_paper_id\":14674551},\"end\":99312,\"start\":98781},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":218571099},\"end\":99819,\"start\":99314},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":204509218},\"end\":100244,\"start\":99821},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":207220904},\"end\":100835,\"start\":100246},{\"attributes\":{\"id\":\"b89\"},\"end\":101101,\"start\":100837},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":50784025},\"end\":101585,\"start\":101103},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":206464266},\"end\":102090,\"start\":101587},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b92\"},\"end\":102552,\"start\":102092},{\"attributes\":{\"doi\":\"10.5555/3488766.3488815\",\"id\":\"b93\",\"matched_paper_id\":219636278},\"end\":103237,\"start\":102554},{\"attributes\":{\"doi\":\"10.1145/3373376.3378508\",\"id\":\"b94\",\"matched_paper_id\":212687780},\"end\":103855,\"start\":103239},{\"attributes\":{\"doi\":\"arXiv:1606.06160\",\"id\":\"b95\"},\"end\":104256,\"start\":103857},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":56174616},\"end\":104898,\"start\":104258},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":256391204},\"end\":105737,\"start\":104900},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":238531477},\"end\":106343,\"start\":105739},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":252819552},\"end\":107023,\"start\":106345},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":204732514},\"end\":107593,\"start\":107025},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":199552011},\"end\":108079,\"start\":107595}]", "bib_title": "[{\"end\":58561,\"start\":58498},{\"end\":59803,\"start\":59724},{\"end\":60505,\"start\":60458},{\"end\":61050,\"start\":60982},{\"end\":61554,\"start\":61440},{\"end\":62476,\"start\":62368},{\"end\":63338,\"start\":63252},{\"end\":63789,\"start\":63723},{\"end\":64577,\"start\":64535},{\"end\":65128,\"start\":65034},{\"end\":65995,\"start\":65915},{\"end\":66588,\"start\":66498},{\"end\":67276,\"start\":67170},{\"end\":68931,\"start\":68862},{\"end\":69386,\"start\":69315},{\"end\":69888,\"start\":69829},{\"end\":70739,\"start\":70682},{\"end\":71971,\"start\":71915},{\"end\":72461,\"start\":72383},{\"end\":73085,\"start\":73009},{\"end\":73634,\"start\":73543},{\"end\":74197,\"start\":74108},{\"end\":75047,\"start\":74946},{\"end\":75526,\"start\":75480},{\"end\":76258,\"start\":76214},{\"end\":76743,\"start\":76651},{\"end\":77423,\"start\":77324},{\"end\":77987,\"start\":77896},{\"end\":78638,\"start\":78561},{\"end\":79734,\"start\":79672},{\"end\":80122,\"start\":80036},{\"end\":81546,\"start\":81474},{\"end\":82194,\"start\":82144},{\"end\":83711,\"start\":83659},{\"end\":84512,\"start\":84403},{\"end\":85357,\"start\":85309},{\"end\":86002,\"start\":85907},{\"end\":86576,\"start\":86537},{\"end\":87240,\"start\":87206},{\"end\":87386,\"start\":87356},{\"end\":87534,\"start\":87498},{\"end\":88195,\"start\":88127},{\"end\":88765,\"start\":88698},{\"end\":89218,\"start\":89127},{\"end\":89827,\"start\":89760},{\"end\":90649,\"start\":90602},{\"end\":91060,\"start\":91001},{\"end\":91693,\"start\":91596},{\"end\":92616,\"start\":92573},{\"end\":93184,\"start\":93090},{\"end\":94062,\"start\":94000},{\"end\":94620,\"start\":94545},{\"end\":95186,\"start\":95084},{\"end\":96571,\"start\":96509},{\"end\":97030,\"start\":97002},{\"end\":97484,\"start\":97409},{\"end\":98005,\"start\":97928},{\"end\":98627,\"start\":98606},{\"end\":98878,\"start\":98781},{\"end\":99404,\"start\":99314},{\"end\":99848,\"start\":99821},{\"end\":100325,\"start\":100246},{\"end\":101185,\"start\":101103},{\"end\":101641,\"start\":101587},{\"end\":102622,\"start\":102554},{\"end\":103359,\"start\":103239},{\"end\":104369,\"start\":104258},{\"end\":105003,\"start\":104900},{\"end\":105854,\"start\":105739},{\"end\":106450,\"start\":106345},{\"end\":107131,\"start\":107025},{\"end\":107688,\"start\":107595}]", "bib_author": "[{\"end\":58580,\"start\":58563},{\"end\":58594,\"start\":58580},{\"end\":58615,\"start\":58594},{\"end\":58627,\"start\":58615},{\"end\":58651,\"start\":58627},{\"end\":58669,\"start\":58651},{\"end\":59142,\"start\":59129},{\"end\":59155,\"start\":59142},{\"end\":59168,\"start\":59155},{\"end\":59180,\"start\":59168},{\"end\":59192,\"start\":59180},{\"end\":59817,\"start\":59805},{\"end\":59831,\"start\":59817},{\"end\":59846,\"start\":59831},{\"end\":60202,\"start\":60187},{\"end\":60220,\"start\":60202},{\"end\":60237,\"start\":60220},{\"end\":60519,\"start\":60507},{\"end\":60531,\"start\":60519},{\"end\":60542,\"start\":60531},{\"end\":60556,\"start\":60542},{\"end\":60567,\"start\":60556},{\"end\":60581,\"start\":60567},{\"end\":60590,\"start\":60581},{\"end\":61065,\"start\":61052},{\"end\":61083,\"start\":61065},{\"end\":61567,\"start\":61556},{\"end\":61581,\"start\":61567},{\"end\":61592,\"start\":61581},{\"end\":61613,\"start\":61592},{\"end\":61625,\"start\":61613},{\"end\":61639,\"start\":61625},{\"end\":62489,\"start\":62478},{\"end\":62503,\"start\":62489},{\"end\":62515,\"start\":62503},{\"end\":62529,\"start\":62515},{\"end\":63354,\"start\":63340},{\"end\":63365,\"start\":63354},{\"end\":63378,\"start\":63365},{\"end\":63388,\"start\":63378},{\"end\":63402,\"start\":63388},{\"end\":63414,\"start\":63402},{\"end\":63429,\"start\":63414},{\"end\":63804,\"start\":63791},{\"end\":63820,\"start\":63804},{\"end\":63834,\"start\":63820},{\"end\":63849,\"start\":63834},{\"end\":63862,\"start\":63849},{\"end\":63876,\"start\":63862},{\"end\":63890,\"start\":63876},{\"end\":63903,\"start\":63890},{\"end\":63913,\"start\":63903},{\"end\":63924,\"start\":63913},{\"end\":63941,\"start\":63924},{\"end\":63963,\"start\":63941},{\"end\":64591,\"start\":64579},{\"end\":64600,\"start\":64591},{\"end\":64612,\"start\":64600},{\"end\":64626,\"start\":64612},{\"end\":64638,\"start\":64626},{\"end\":64648,\"start\":64638},{\"end\":64657,\"start\":64648},{\"end\":64671,\"start\":64657},{\"end\":64682,\"start\":64671},{\"end\":64695,\"start\":64682},{\"end\":65144,\"start\":65130},{\"end\":65160,\"start\":65144},{\"end\":65173,\"start\":65160},{\"end\":65187,\"start\":65173},{\"end\":65466,\"start\":65451},{\"end\":65477,\"start\":65466},{\"end\":65500,\"start\":65477},{\"end\":65514,\"start\":65500},{\"end\":65536,\"start\":65514},{\"end\":65556,\"start\":65536},{\"end\":65572,\"start\":65556},{\"end\":66011,\"start\":65997},{\"end\":66025,\"start\":66011},{\"end\":66037,\"start\":66025},{\"end\":66602,\"start\":66590},{\"end\":66614,\"start\":66602},{\"end\":66625,\"start\":66614},{\"end\":66639,\"start\":66625},{\"end\":66653,\"start\":66639},{\"end\":66660,\"start\":66653},{\"end\":66672,\"start\":66660},{\"end\":67290,\"start\":67278},{\"end\":67300,\"start\":67290},{\"end\":67311,\"start\":67300},{\"end\":67320,\"start\":67311},{\"end\":67330,\"start\":67320},{\"end\":67341,\"start\":67330},{\"end\":67350,\"start\":67341},{\"end\":67361,\"start\":67350},{\"end\":67702,\"start\":67688},{\"end\":67714,\"start\":67702},{\"end\":67730,\"start\":67714},{\"end\":67748,\"start\":67730},{\"end\":68115,\"start\":68101},{\"end\":68131,\"start\":68115},{\"end\":68143,\"start\":68131},{\"end\":68163,\"start\":68143},{\"end\":68401,\"start\":68388},{\"end\":68414,\"start\":68401},{\"end\":68429,\"start\":68414},{\"end\":68444,\"start\":68429},{\"end\":68461,\"start\":68444},{\"end\":68478,\"start\":68461},{\"end\":68497,\"start\":68478},{\"end\":68511,\"start\":68497},{\"end\":68944,\"start\":68933},{\"end\":68956,\"start\":68944},{\"end\":68972,\"start\":68956},{\"end\":68986,\"start\":68972},{\"end\":68997,\"start\":68986},{\"end\":69011,\"start\":68997},{\"end\":69020,\"start\":69011},{\"end\":69399,\"start\":69388},{\"end\":69411,\"start\":69399},{\"end\":69425,\"start\":69411},{\"end\":69436,\"start\":69425},{\"end\":69450,\"start\":69436},{\"end\":69459,\"start\":69450},{\"end\":69901,\"start\":69890},{\"end\":69919,\"start\":69901},{\"end\":69933,\"start\":69919},{\"end\":69946,\"start\":69933},{\"end\":69955,\"start\":69946},{\"end\":69964,\"start\":69955},{\"end\":69979,\"start\":69964},{\"end\":69991,\"start\":69979},{\"end\":70006,\"start\":69991},{\"end\":70447,\"start\":70433},{\"end\":70459,\"start\":70447},{\"end\":70471,\"start\":70459},{\"end\":70482,\"start\":70471},{\"end\":70496,\"start\":70482},{\"end\":70505,\"start\":70496},{\"end\":70767,\"start\":70741},{\"end\":70780,\"start\":70767},{\"end\":70794,\"start\":70780},{\"end\":70811,\"start\":70794},{\"end\":70824,\"start\":70811},{\"end\":71338,\"start\":71328},{\"end\":71352,\"start\":71338},{\"end\":71361,\"start\":71352},{\"end\":71372,\"start\":71361},{\"end\":71383,\"start\":71372},{\"end\":71618,\"start\":71608},{\"end\":71630,\"start\":71618},{\"end\":71644,\"start\":71630},{\"end\":71657,\"start\":71644},{\"end\":71668,\"start\":71657},{\"end\":71983,\"start\":71973},{\"end\":71995,\"start\":71983},{\"end\":72008,\"start\":71995},{\"end\":72019,\"start\":72008},{\"end\":72033,\"start\":72019},{\"end\":72044,\"start\":72033},{\"end\":72473,\"start\":72463},{\"end\":72488,\"start\":72473},{\"end\":72502,\"start\":72488},{\"end\":72514,\"start\":72502},{\"end\":72524,\"start\":72514},{\"end\":72537,\"start\":72524},{\"end\":72551,\"start\":72537},{\"end\":72562,\"start\":72551},{\"end\":72573,\"start\":72562},{\"end\":72584,\"start\":72573},{\"end\":73097,\"start\":73087},{\"end\":73109,\"start\":73097},{\"end\":73123,\"start\":73109},{\"end\":73137,\"start\":73123},{\"end\":73149,\"start\":73137},{\"end\":73161,\"start\":73149},{\"end\":73171,\"start\":73161},{\"end\":73182,\"start\":73171},{\"end\":73193,\"start\":73182},{\"end\":73646,\"start\":73636},{\"end\":73658,\"start\":73646},{\"end\":73672,\"start\":73658},{\"end\":73684,\"start\":73672},{\"end\":73694,\"start\":73684},{\"end\":73708,\"start\":73694},{\"end\":73720,\"start\":73708},{\"end\":73730,\"start\":73720},{\"end\":73741,\"start\":73730},{\"end\":74209,\"start\":74199},{\"end\":74221,\"start\":74209},{\"end\":74235,\"start\":74221},{\"end\":74246,\"start\":74235},{\"end\":74256,\"start\":74246},{\"end\":74268,\"start\":74256},{\"end\":74279,\"start\":74268},{\"end\":74290,\"start\":74279},{\"end\":74665,\"start\":74655},{\"end\":74677,\"start\":74665},{\"end\":74691,\"start\":74677},{\"end\":74702,\"start\":74691},{\"end\":74712,\"start\":74702},{\"end\":74724,\"start\":74712},{\"end\":74735,\"start\":74724},{\"end\":75059,\"start\":75049},{\"end\":75073,\"start\":75059},{\"end\":75087,\"start\":75073},{\"end\":75098,\"start\":75087},{\"end\":75109,\"start\":75098},{\"end\":75120,\"start\":75109},{\"end\":75129,\"start\":75120},{\"end\":75138,\"start\":75129},{\"end\":75149,\"start\":75138},{\"end\":75541,\"start\":75528},{\"end\":75556,\"start\":75541},{\"end\":75580,\"start\":75556},{\"end\":75599,\"start\":75580},{\"end\":75855,\"start\":75845},{\"end\":75866,\"start\":75855},{\"end\":75883,\"start\":75866},{\"end\":76272,\"start\":76260},{\"end\":76287,\"start\":76272},{\"end\":76301,\"start\":76287},{\"end\":76311,\"start\":76301},{\"end\":76759,\"start\":76745},{\"end\":76778,\"start\":76759},{\"end\":76787,\"start\":76778},{\"end\":76801,\"start\":76787},{\"end\":76815,\"start\":76801},{\"end\":76830,\"start\":76815},{\"end\":76844,\"start\":76830},{\"end\":76865,\"start\":76844},{\"end\":77439,\"start\":77425},{\"end\":77462,\"start\":77439},{\"end\":77488,\"start\":77462},{\"end\":77503,\"start\":77488},{\"end\":77527,\"start\":77503},{\"end\":77541,\"start\":77527},{\"end\":78001,\"start\":77989},{\"end\":78013,\"start\":78001},{\"end\":78029,\"start\":78013},{\"end\":78046,\"start\":78029},{\"end\":78061,\"start\":78046},{\"end\":78073,\"start\":78061},{\"end\":78657,\"start\":78640},{\"end\":78672,\"start\":78657},{\"end\":78690,\"start\":78672},{\"end\":78705,\"start\":78690},{\"end\":78722,\"start\":78705},{\"end\":78737,\"start\":78722},{\"end\":78751,\"start\":78737},{\"end\":78761,\"start\":78751},{\"end\":78771,\"start\":78761},{\"end\":78782,\"start\":78771},{\"end\":78797,\"start\":78782},{\"end\":78812,\"start\":78797},{\"end\":78827,\"start\":78812},{\"end\":78840,\"start\":78827},{\"end\":78854,\"start\":78840},{\"end\":78871,\"start\":78854},{\"end\":79375,\"start\":79365},{\"end\":79389,\"start\":79375},{\"end\":79404,\"start\":79389},{\"end\":79417,\"start\":79404},{\"end\":79435,\"start\":79417},{\"end\":79453,\"start\":79435},{\"end\":79466,\"start\":79453},{\"end\":79480,\"start\":79466},{\"end\":79492,\"start\":79480},{\"end\":79502,\"start\":79492},{\"end\":79512,\"start\":79502},{\"end\":80137,\"start\":80124},{\"end\":80152,\"start\":80137},{\"end\":80166,\"start\":80152},{\"end\":80178,\"start\":80166},{\"end\":80192,\"start\":80178},{\"end\":80207,\"start\":80192},{\"end\":80222,\"start\":80207},{\"end\":80237,\"start\":80222},{\"end\":80708,\"start\":80695},{\"end\":80721,\"start\":80708},{\"end\":80735,\"start\":80721},{\"end\":80750,\"start\":80735},{\"end\":80766,\"start\":80750},{\"end\":80781,\"start\":80766},{\"end\":80796,\"start\":80781},{\"end\":80811,\"start\":80796},{\"end\":80826,\"start\":80811},{\"end\":81204,\"start\":81188},{\"end\":81219,\"start\":81204},{\"end\":81231,\"start\":81219},{\"end\":81247,\"start\":81231},{\"end\":81564,\"start\":81548},{\"end\":81579,\"start\":81564},{\"end\":81586,\"start\":81579},{\"end\":81604,\"start\":81586},{\"end\":81612,\"start\":81604},{\"end\":81993,\"start\":81981},{\"end\":82007,\"start\":81993},{\"end\":82210,\"start\":82196},{\"end\":82231,\"start\":82210},{\"end\":82248,\"start\":82231},{\"end\":82261,\"start\":82248},{\"end\":82270,\"start\":82261},{\"end\":82282,\"start\":82270},{\"end\":82302,\"start\":82282},{\"end\":82758,\"start\":82746},{\"end\":82770,\"start\":82758},{\"end\":82783,\"start\":82770},{\"end\":82805,\"start\":82783},{\"end\":82826,\"start\":82805},{\"end\":82837,\"start\":82826},{\"end\":82851,\"start\":82837},{\"end\":82869,\"start\":82851},{\"end\":83200,\"start\":83188},{\"end\":83210,\"start\":83200},{\"end\":83222,\"start\":83210},{\"end\":83236,\"start\":83222},{\"end\":83248,\"start\":83236},{\"end\":83262,\"start\":83248},{\"end\":83276,\"start\":83262},{\"end\":83287,\"start\":83276},{\"end\":83724,\"start\":83713},{\"end\":83738,\"start\":83724},{\"end\":83750,\"start\":83738},{\"end\":83764,\"start\":83750},{\"end\":83780,\"start\":83764},{\"end\":83795,\"start\":83780},{\"end\":83810,\"start\":83795},{\"end\":83823,\"start\":83810},{\"end\":83835,\"start\":83823},{\"end\":84525,\"start\":84514},{\"end\":84539,\"start\":84525},{\"end\":84553,\"start\":84539},{\"end\":84564,\"start\":84553},{\"end\":84573,\"start\":84564},{\"end\":84584,\"start\":84573},{\"end\":85369,\"start\":85359},{\"end\":85382,\"start\":85369},{\"end\":85400,\"start\":85382},{\"end\":85436,\"start\":85400},{\"end\":85447,\"start\":85436},{\"end\":86016,\"start\":86004},{\"end\":86030,\"start\":86016},{\"end\":86044,\"start\":86030},{\"end\":86059,\"start\":86044},{\"end\":86075,\"start\":86059},{\"end\":86600,\"start\":86578},{\"end\":86624,\"start\":86600},{\"end\":86642,\"start\":86624},{\"end\":86823,\"start\":86809},{\"end\":86843,\"start\":86823},{\"end\":86859,\"start\":86843},{\"end\":86879,\"start\":86859},{\"end\":87250,\"start\":87242},{\"end\":87396,\"start\":87388},{\"end\":87544,\"start\":87536},{\"end\":87763,\"start\":87748},{\"end\":87778,\"start\":87763},{\"end\":87791,\"start\":87778},{\"end\":88210,\"start\":88197},{\"end\":88221,\"start\":88210},{\"end\":88238,\"start\":88221},{\"end\":88250,\"start\":88238},{\"end\":88266,\"start\":88250},{\"end\":88282,\"start\":88266},{\"end\":88298,\"start\":88282},{\"end\":88310,\"start\":88298},{\"end\":88330,\"start\":88310},{\"end\":88343,\"start\":88330},{\"end\":88783,\"start\":88767},{\"end\":88795,\"start\":88783},{\"end\":88807,\"start\":88795},{\"end\":88820,\"start\":88807},{\"end\":88831,\"start\":88820},{\"end\":89230,\"start\":89220},{\"end\":89247,\"start\":89230},{\"end\":89262,\"start\":89247},{\"end\":89278,\"start\":89262},{\"end\":89300,\"start\":89278},{\"end\":89314,\"start\":89300},{\"end\":89327,\"start\":89314},{\"end\":89343,\"start\":89327},{\"end\":89841,\"start\":89829},{\"end\":89855,\"start\":89841},{\"end\":89865,\"start\":89855},{\"end\":89876,\"start\":89865},{\"end\":89885,\"start\":89876},{\"end\":89896,\"start\":89885},{\"end\":89907,\"start\":89896},{\"end\":90388,\"start\":90374},{\"end\":90397,\"start\":90388},{\"end\":90410,\"start\":90397},{\"end\":90422,\"start\":90410},{\"end\":90436,\"start\":90422},{\"end\":90452,\"start\":90436},{\"end\":90674,\"start\":90651},{\"end\":90686,\"start\":90674},{\"end\":90694,\"start\":90686},{\"end\":91080,\"start\":91062},{\"end\":91092,\"start\":91080},{\"end\":91112,\"start\":91092},{\"end\":91125,\"start\":91112},{\"end\":91715,\"start\":91695},{\"end\":91727,\"start\":91715},{\"end\":92042,\"start\":92020},{\"end\":92059,\"start\":92042},{\"end\":92072,\"start\":92059},{\"end\":92088,\"start\":92072},{\"end\":92101,\"start\":92088},{\"end\":92116,\"start\":92101},{\"end\":92142,\"start\":92116},{\"end\":92161,\"start\":92142},{\"end\":92167,\"start\":92161},{\"end\":92633,\"start\":92618},{\"end\":92646,\"start\":92633},{\"end\":92661,\"start\":92646},{\"end\":92677,\"start\":92661},{\"end\":92693,\"start\":92677},{\"end\":92707,\"start\":92693},{\"end\":92720,\"start\":92707},{\"end\":92739,\"start\":92720},{\"end\":93201,\"start\":93186},{\"end\":93214,\"start\":93201},{\"end\":93227,\"start\":93214},{\"end\":93242,\"start\":93227},{\"end\":93255,\"start\":93242},{\"end\":93270,\"start\":93255},{\"end\":93289,\"start\":93270},{\"end\":93699,\"start\":93684},{\"end\":93712,\"start\":93699},{\"end\":93725,\"start\":93712},{\"end\":93740,\"start\":93725},{\"end\":93753,\"start\":93740},{\"end\":93768,\"start\":93753},{\"end\":93787,\"start\":93768},{\"end\":94076,\"start\":94064},{\"end\":94087,\"start\":94076},{\"end\":94097,\"start\":94087},{\"end\":94109,\"start\":94097},{\"end\":94121,\"start\":94109},{\"end\":94135,\"start\":94121},{\"end\":94146,\"start\":94135},{\"end\":94160,\"start\":94146},{\"end\":94169,\"start\":94160},{\"end\":94636,\"start\":94622},{\"end\":94647,\"start\":94636},{\"end\":94659,\"start\":94647},{\"end\":94675,\"start\":94659},{\"end\":94685,\"start\":94675},{\"end\":94699,\"start\":94685},{\"end\":94714,\"start\":94699},{\"end\":95209,\"start\":95188},{\"end\":95222,\"start\":95209},{\"end\":95235,\"start\":95222},{\"end\":95254,\"start\":95235},{\"end\":95271,\"start\":95254},{\"end\":95283,\"start\":95271},{\"end\":95299,\"start\":95283},{\"end\":95304,\"start\":95299},{\"end\":95759,\"start\":95743},{\"end\":95773,\"start\":95759},{\"end\":95786,\"start\":95773},{\"end\":95803,\"start\":95786},{\"end\":95816,\"start\":95803},{\"end\":95831,\"start\":95816},{\"end\":95846,\"start\":95831},{\"end\":95864,\"start\":95846},{\"end\":96097,\"start\":96086},{\"end\":96114,\"start\":96097},{\"end\":96130,\"start\":96114},{\"end\":96142,\"start\":96130},{\"end\":96153,\"start\":96142},{\"end\":96170,\"start\":96153},{\"end\":96584,\"start\":96573},{\"end\":96597,\"start\":96584},{\"end\":96608,\"start\":96597},{\"end\":96616,\"start\":96608},{\"end\":96626,\"start\":96616},{\"end\":97043,\"start\":97032},{\"end\":97055,\"start\":97043},{\"end\":97069,\"start\":97055},{\"end\":97079,\"start\":97069},{\"end\":97091,\"start\":97079},{\"end\":97105,\"start\":97091},{\"end\":97498,\"start\":97486},{\"end\":97508,\"start\":97498},{\"end\":97521,\"start\":97508},{\"end\":97531,\"start\":97521},{\"end\":97540,\"start\":97531},{\"end\":98020,\"start\":98007},{\"end\":98035,\"start\":98020},{\"end\":98051,\"start\":98035},{\"end\":98064,\"start\":98051},{\"end\":98081,\"start\":98064},{\"end\":98091,\"start\":98081},{\"end\":98103,\"start\":98091},{\"end\":98118,\"start\":98103},{\"end\":98894,\"start\":98880},{\"end\":98910,\"start\":98894},{\"end\":98922,\"start\":98910},{\"end\":98936,\"start\":98922},{\"end\":99422,\"start\":99406},{\"end\":99432,\"start\":99422},{\"end\":99863,\"start\":99850},{\"end\":99877,\"start\":99863},{\"end\":99890,\"start\":99877},{\"end\":99908,\"start\":99890},{\"end\":100339,\"start\":100327},{\"end\":100348,\"start\":100339},{\"end\":100361,\"start\":100348},{\"end\":100373,\"start\":100361},{\"end\":100387,\"start\":100373},{\"end\":100399,\"start\":100387},{\"end\":100944,\"start\":100937},{\"end\":101203,\"start\":101187},{\"end\":101218,\"start\":101203},{\"end\":101234,\"start\":101218},{\"end\":101244,\"start\":101234},{\"end\":101657,\"start\":101643},{\"end\":101668,\"start\":101657},{\"end\":101679,\"start\":101668},{\"end\":101692,\"start\":101679},{\"end\":101704,\"start\":101692},{\"end\":101713,\"start\":101704},{\"end\":101721,\"start\":101713},{\"end\":101735,\"start\":101721},{\"end\":101747,\"start\":101735},{\"end\":102156,\"start\":102143},{\"end\":102172,\"start\":102156},{\"end\":102185,\"start\":102172},{\"end\":102200,\"start\":102185},{\"end\":102211,\"start\":102200},{\"end\":102225,\"start\":102211},{\"end\":102244,\"start\":102225},{\"end\":102255,\"start\":102244},{\"end\":102264,\"start\":102255},{\"end\":102281,\"start\":102264},{\"end\":102639,\"start\":102624},{\"end\":102653,\"start\":102639},{\"end\":102665,\"start\":102653},{\"end\":102674,\"start\":102665},{\"end\":102687,\"start\":102674},{\"end\":102702,\"start\":102687},{\"end\":102713,\"start\":102702},{\"end\":102723,\"start\":102713},{\"end\":102737,\"start\":102723},{\"end\":102750,\"start\":102737},{\"end\":102769,\"start\":102750},{\"end\":102781,\"start\":102769},{\"end\":103373,\"start\":103361},{\"end\":103384,\"start\":103373},{\"end\":103395,\"start\":103384},{\"end\":103407,\"start\":103395},{\"end\":103421,\"start\":103407},{\"end\":103872,\"start\":103857},{\"end\":103882,\"start\":103872},{\"end\":103892,\"start\":103882},{\"end\":103904,\"start\":103892},{\"end\":103912,\"start\":103904},{\"end\":103924,\"start\":103912},{\"end\":104382,\"start\":104371},{\"end\":104393,\"start\":104382},{\"end\":104401,\"start\":104393},{\"end\":104413,\"start\":104401},{\"end\":104426,\"start\":104413},{\"end\":104437,\"start\":104426},{\"end\":104450,\"start\":104437},{\"end\":104459,\"start\":104450},{\"end\":104473,\"start\":104459},{\"end\":104485,\"start\":104473},{\"end\":105019,\"start\":105005},{\"end\":105033,\"start\":105019},{\"end\":105045,\"start\":105033},{\"end\":105056,\"start\":105045},{\"end\":105067,\"start\":105056},{\"end\":105076,\"start\":105067},{\"end\":105087,\"start\":105076},{\"end\":105101,\"start\":105087},{\"end\":105110,\"start\":105101},{\"end\":105119,\"start\":105110},{\"end\":105870,\"start\":105856},{\"end\":105885,\"start\":105870},{\"end\":105895,\"start\":105885},{\"end\":105909,\"start\":105895},{\"end\":105920,\"start\":105909},{\"end\":105931,\"start\":105920},{\"end\":105942,\"start\":105931},{\"end\":105953,\"start\":105942},{\"end\":106464,\"start\":106452},{\"end\":106475,\"start\":106464},{\"end\":106487,\"start\":106475},{\"end\":106499,\"start\":106487},{\"end\":106509,\"start\":106499},{\"end\":106521,\"start\":106509},{\"end\":106533,\"start\":106521},{\"end\":106546,\"start\":106533},{\"end\":106558,\"start\":106546},{\"end\":106567,\"start\":106558},{\"end\":106577,\"start\":106567},{\"end\":106587,\"start\":106577},{\"end\":106600,\"start\":106587},{\"end\":107145,\"start\":107133},{\"end\":107156,\"start\":107145},{\"end\":107167,\"start\":107156},{\"end\":107177,\"start\":107167},{\"end\":107704,\"start\":107690},{\"end\":107717,\"start\":107704},{\"end\":107727,\"start\":107717},{\"end\":107741,\"start\":107727},{\"end\":107751,\"start\":107741},{\"end\":107765,\"start\":107751}]", "bib_venue": "[{\"end\":58707,\"start\":58669},{\"end\":59127,\"start\":58952},{\"end\":59593,\"start\":59522},{\"end\":59895,\"start\":59846},{\"end\":60185,\"start\":60095},{\"end\":60671,\"start\":60590},{\"end\":61164,\"start\":61083},{\"end\":61792,\"start\":61662},{\"end\":62694,\"start\":62552},{\"end\":63467,\"start\":63429},{\"end\":64054,\"start\":63986},{\"end\":64766,\"start\":64695},{\"end\":65223,\"start\":65187},{\"end\":65657,\"start\":65588},{\"end\":66146,\"start\":66065},{\"end\":66764,\"start\":66700},{\"end\":67415,\"start\":67361},{\"end\":67817,\"start\":67764},{\"end\":68099,\"start\":68019},{\"end\":68617,\"start\":68533},{\"end\":69069,\"start\":69020},{\"end\":69530,\"start\":69459},{\"end\":70085,\"start\":70006},{\"end\":70550,\"start\":70505},{\"end\":70911,\"start\":70824},{\"end\":71326,\"start\":71239},{\"end\":71734,\"start\":71684},{\"end\":72105,\"start\":72044},{\"end\":72679,\"start\":72584},{\"end\":73245,\"start\":73193},{\"end\":73806,\"start\":73741},{\"end\":74361,\"start\":74290},{\"end\":75201,\"start\":75149},{\"end\":75647,\"start\":75603},{\"end\":76003,\"start\":75899},{\"end\":76388,\"start\":76311},{\"end\":76942,\"start\":76865},{\"end\":77595,\"start\":77541},{\"end\":78172,\"start\":78096},{\"end\":78952,\"start\":78871},{\"end\":79815,\"start\":79736},{\"end\":80318,\"start\":80237},{\"end\":80885,\"start\":80826},{\"end\":81186,\"start\":81114},{\"end\":81693,\"start\":81612},{\"end\":81979,\"start\":81948},{\"end\":82363,\"start\":82325},{\"end\":82744,\"start\":82631},{\"end\":83396,\"start\":83303},{\"end\":83961,\"start\":83835},{\"end\":84725,\"start\":84607},{\"end\":85555,\"start\":85470},{\"end\":86159,\"start\":86098},{\"end\":86473,\"start\":86446},{\"end\":86657,\"start\":86642},{\"end\":86986,\"start\":86895},{\"end\":87274,\"start\":87250},{\"end\":87420,\"start\":87396},{\"end\":87568,\"start\":87544},{\"end\":87746,\"start\":87652},{\"end\":88031,\"start\":87949},{\"end\":88392,\"start\":88343},{\"end\":88896,\"start\":88831},{\"end\":89425,\"start\":89343},{\"end\":89995,\"start\":89907},{\"end\":90372,\"start\":90321},{\"end\":90784,\"start\":90694},{\"end\":91211,\"start\":91125},{\"end\":91792,\"start\":91727},{\"end\":92275,\"start\":92183},{\"end\":92812,\"start\":92739},{\"end\":93365,\"start\":93289},{\"end\":93814,\"start\":93787},{\"end\":94230,\"start\":94169},{\"end\":94795,\"start\":94714},{\"end\":95357,\"start\":95304},{\"end\":95741,\"start\":95665},{\"end\":96271,\"start\":96186},{\"end\":96707,\"start\":96626},{\"end\":97186,\"start\":97105},{\"end\":97621,\"start\":97540},{\"end\":98167,\"start\":98118},{\"end\":98685,\"start\":98629},{\"end\":99029,\"start\":98959},{\"end\":99510,\"start\":99432},{\"end\":100016,\"start\":99908},{\"end\":100490,\"start\":100399},{\"end\":100935,\"start\":100837},{\"end\":101307,\"start\":101244},{\"end\":101820,\"start\":101747},{\"end\":102141,\"start\":102092},{\"end\":102878,\"start\":102804},{\"end\":103513,\"start\":103444},{\"end\":104031,\"start\":103940},{\"end\":104558,\"start\":104485},{\"end\":105244,\"start\":105119},{\"end\":106023,\"start\":105953},{\"end\":106668,\"start\":106600},{\"end\":107261,\"start\":107177},{\"end\":107827,\"start\":107765},{\"end\":60739,\"start\":60673},{\"end\":61232,\"start\":61166},{\"end\":61921,\"start\":61794},{\"end\":62839,\"start\":62696},{\"end\":64073,\"start\":64056},{\"end\":66166,\"start\":66148},{\"end\":66797,\"start\":66766},{\"end\":69588,\"start\":69532},{\"end\":70151,\"start\":70087},{\"end\":70985,\"start\":70913},{\"end\":72153,\"start\":72107},{\"end\":76452,\"start\":76390},{\"end\":77006,\"start\":76944},{\"end\":78235,\"start\":78174},{\"end\":79881,\"start\":79817},{\"end\":80386,\"start\":80320},{\"end\":84074,\"start\":83963},{\"end\":84810,\"start\":84789},{\"end\":85627,\"start\":85557},{\"end\":90070,\"start\":89997},{\"end\":91284,\"start\":91213},{\"end\":94278,\"start\":94232},{\"end\":96775,\"start\":96709},{\"end\":97689,\"start\":97623},{\"end\":100568,\"start\":100492},{\"end\":101357,\"start\":101309},{\"end\":103523,\"start\":103515},{\"end\":105356,\"start\":105246},{\"end\":107332,\"start\":107263}]"}}}, "year": 2023, "month": 12, "day": 17}