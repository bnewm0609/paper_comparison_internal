{"id": 229363322, "updated": "2023-10-06 07:21:55.884", "metadata": {"title": "Training data-efficient image transformers&distillation through attention", "authors": "[{\"first\":\"Hugo\",\"last\":\"Touvron\",\"middle\":[]},{\"first\":\"Matthieu\",\"last\":\"Cord\",\"middle\":[]},{\"first\":\"Matthijs\",\"last\":\"Douze\",\"middle\":[]},{\"first\":\"Francisco\",\"last\":\"Massa\",\"middle\":[]},{\"first\":\"Alexandre\",\"last\":\"Sablayrolles\",\"middle\":[]},{\"first\":\"Herv'e\",\"last\":\"J'egou\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 23}, "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.12877", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/TouvronCDMSJ21", "doi": null}}, "content": {"source": {"pdf_hash": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.12877v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "17def2031a6e82125d367e98c65fa06171957c33", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ad7ddcc14984caae308c397f1a589aae75d4ab71.txt", "contents": "\nTraining data-efficient image transformers & distillation through attention\n\n\nHugo Touvron \nSorbonne University\n\n\nMatthieu Cord \nSorbonne University\n\n\nMatthijs Douze \nSorbonne University\n\n\nFrancisco Massa \nSorbonne University\n\n\nAlexandre Sablayrolles \nSorbonne University\n\n\nHerv\u00e9 J\u00e9gou \nSorbonne University\n\n\nFacebook Ai \nSorbonne University\n\n\nTraining data-efficient image transformers & distillation through attention\n\nRecently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption.In this work, we produce competitive convolution-free transformers by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.\n\nIntroduction\n\nConvolutional neural networks have been the main design paradigm for image understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set, namely Imagenet [13,42]. Motivated by the success of attention-based models in Natural Language Processing [14,52], there has been increasing interest in architectures leveraging attention mechanisms within convnets [2,34,61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6,43].\n\nThe vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but ap-\u2697\u2191 \u2697 \u2697 \u2697 Figure 1: Throughput and accuracy on Imagenet of our methods compared to EfficientNets, trained on Imagenet1k only. The throughput is measured as the number of images processed per second on a V100 GPU. DeiT-B is identical to VIT-B, but the training is more adapted to a data-starving regime. It is learned in a few days on one machine. The symbol \u2697 refers to models trained with our transformer-specific distillation. See Table 5 for details and more models.\n\nplied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled image dataset (JFT-300M [46], 300 millions images). The paper concluded that transformers \"do not generalize well when trained on insufficient amounts of data\", and the training of these models involved extensive computing resources.\n\nIn this paper, we train a vision transformer on a single 8-GPU node in two to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets having a similar number of parameters and efficiency. It uses Imagenet as the sole training set. We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements included in the timm library [55]. With our Data-efficient image Transformers (DeiT), we report large improvements over previous results, see Figure 1. Our ablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation.\n\nWe address another question: how to distill these models? We introduce a token-based strategy, specific to transformers and denoted by DeiT\u2697, and show that it advantageously replaces the usual distillation.\n\nIn summary, our work makes the following contributions:\n\n\u2022 We show that our neural networks that contains no convolutional layer can achieve competitive results against the state of the art on ImageNet with no external data. They are learned on a single node with 4 GPUs in three days 1 . Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18.\n\n\u2022 We introduce a new distillation procedure based on a distillation token, which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the transformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.\n\n\u2022 Interestingly, with our distillation, image transformers learn more from a convnet than from another transformer with comparable performance.\n\n\u2022 Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers, Stanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2, and focus on transformers for image classification in Section 3. We introduce our distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent transformers, as well as a comparative evaluation of our transformer-specific distillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the key ingredients involved in DeiT. We conclude in Section 7.\n\n\nRelated work\n\nImage Classification is so core to computer vision that it is often used as a benchmark to measure progress in image understanding. Any progress usually translates to improvement in other related tasks such as detection or segmentation. Since 2012's AlexNet [32], convnets have dominated this benchmark and have become the de facto standard. The evolution of the state of the art on the ImageNet dataset [42] reflects the progress with convolutional neural network architectures and learning [32,44,48,50,51,57].\n\nDespite several attempts to use transformers for image classification [7], until now their performance has been inferior to that of convnets. Nevertheless hybrid architectures that combine convnets and transformers, including the self-attention mechanism, have recently exhibited competitive results in image classification [56], detection [6,28], video processing [45,53], unsupervised object discovery [35], and unified text-vision tasks [8,33,37].\n\nRecently Vision transformers (ViT) [15] closed the gap with the state of the art on ImageNet, without using any convolution. This performance is remarkable since convnet methods for image classification have benefited from years of tuning and optimization [22,55]. Nevertheless, according to this study [15], a pre-training phase on a large volume of curated data is required for the learned transformer to be effective. In our paper we achieve a strong performance without requiring a large training dataset, i.e., with Imagenet1k only.\n\nThe Transformer architecture, introduced by Vaswani et al. [52] for machine translation are currently the reference model for all natural language processing (NLP) tasks. Many improvements of convnets for image classification are inspired by transformers. For example, Squeeze and Excitation [2], Selective Kernel [34] and Split-Attention Networks [61] exploit mechanism akin to transformers self-attention (SA) mechanism.\n\nKnowledge Distillation (KD), introduced by Hinton et al. [24], refers to the training paradigm in which a student model leverages \"soft\" labels coming from a strong teacher network. This is the output vector of the teacher's softmax function rather than just the maximum of scores, wich gives a \"hard\" label. Such a training improves the performance of the student model (alternatively, it can be regarded as a form of compression of the teacher model into a smaller one -the student). On the one hand the teacher's soft labels will have a similar effect to labels smoothing [58]. On the other hand as shown by Wei et al. [54] the teacher's supervision takes into account the effects of the data augmentation, which sometimes causes a misalignment between the real label and the image. For example, let us consider image with a \"cat\" label that represents a large landscape and a small cat in a corner. If the cat is no longer on the crop of the data augmentation it implicitly changes the label of the image. KD can transfer inductive biases [1] in a soft way in a student model using a teacher model where they would be incorporated in a hard way. For example, it may be useful to induce biases due to convolutions in a transformer model by using a convolutional model as teacher. In our paper we study the distillation of a transformer student by either a convnet or a transformer teacher. We introduce a new distillation procedure specific to transformers and show its superiority.\n\n\nVision transformer: overview\n\nIn this section, we briefly recall preliminaries associated with the vision transformer [15,52], and further discuss positional encoding and resolution.\n\nMulti-head Self Attention layers (MSA). The attention mechanism is based on a trainable associative memory with (key, value) vector pairs. A query vector q \u2208 R d is matched against a set of k key vectors (packed together into a matrix K \u2208 R k\u00d7d ) using inner products. These inner products are then scaled and normalized with a softmax function to obtain k weights. The output of the attention is the weighted sum of a set of k value vectors (packed into V \u2208 R k\u00d7d ). For a sequence of N query vectors (packed into Q \u2208 R N \u00d7d ), it produces an output matrix (of size N \u00d7 d):\nAttention(Q, K, V ) = Softmax(QK / \u221a d)V,(1)\nwhere the Softmax function is applied over each row of the input matrix and the \u221a d term provides appropriate normalization. In [52], a Self-attention layer is proposed. Query, key and values matrices are themselves computed from a sequence of N input vectors (packed into X \u2208 R N \u00d7D ): Q = XW Q , K = XW K , V = XW V , using linear transformations W Q , W K , W V with the constraint k = N , meaning that the attention is in between all the input vectors. Finally, Multi-head self-attention layer (MSA) is defined by considering h attention \"heads\", ie h self-attention functions applied to the input. Each head provides a sequence of size N \u00d7 d. These h sequences are rearranged into a N \u00d7 dh sequence that is reprojected by a linear layer into N \u00d7 D.\n\nTransformer block for images. To get a full transformer block as in [52], we add a Feed-Forward Network (FFN) on top of the MSA layer. This FFN is composed of two linear layers separated by a GeLu activation [23]. The first linear layer expands the dimension from D to 4D, and the second layer reduces the dimension from 4D back to D. Both MSA and FFN are operating as residual operators thank to skip-connections, and with a layer normalization [3].\n\nIn order to get a transformer to process images, our work builds upon the ViT model [15]. It is a simple and elegant architecture that processes input images as if they were a sequence of input tokens. The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 \u00d7 16 pixels (N = 14 \u00d7 14). Each patch is projected with a linear layer that conserves its overall dimension 3 \u00d7 16 \u00d7 16 = 768. The transformer block described above is invariant to the order of the patch embeddings, and thus does not consider their relative position. The positional information is incorporated as fixed [52] or trainable [18] positional embeddings. They are added before the first transformer block to the patch tokens, which are then fed to the stack of transformer blocks.\n\nThe class token is a trainable vector, appended to the patch tokens before the first layer, that goes through the transformer layers, and is then projected with a linear layer to predict the class. This class token is inherited from NLP [14], and departs from the typical pooling layers used in computer vision to predict the class. The transformer thus process batches of (N + 1) tokens of dimension D, of which only the class vector is used to predict the output. This architecture forces the self-attention to spread information between the patch tokens and the class token: at training time the supervision signal comes only from the class embedding, while the patch tokens are the model's only variable input.\n\nFixing the positional encoding across resolutions. Touvron et al. [50] show that it is desirable to use a lower training resolution and fine-tune the network at the larger resolution. This speeds up the full training and improves the accuracy under prevailing data augmentation schemes. When increasing the resolution of an input image, we keep the patch size the same, therefore the number N of input patches does change. Due to the architecture of transformer blocks and the class token, the model and classifier do not need to be modified to process more tokens. In contrast, one needs to adapt the positional embeddings, because there are N of them, one for each patch. Dosovitskiy et al. [15] interpolate the positional encoding when changing the resolution and demonstrate that this method works with the subsequent fine-tuning stage.\n\n\nDistillation through attention\n\nIn this section we assume we have access to a strong image classifier as a teacher model. It could be a convnet, or a mixture of classifiers. We address the question of how to learn a transformer by exploiting this teacher. As we will see in Section 5 by comparing the trade-off between accuracy and image throughput, it can be beneficial to replace a convolutional neural network by a transformer. This section covers two axes of distillation: hard distillation versus soft distillation, and classical distillation versus the distillation token. [24,54] minimizes the Kullback-Leibler divergence between the softmax of the teacher and the softmax of the student model.\n\n\nSoft distillation\n\nLet Z t be the logits of the teacher model, Z s the logits of the student model. We denote by \u03c4 the temperature for the distillation, \u03bb the coefficient balancing the Kullback-Leibler divergence loss (KL) and the cross-entropy (L CE ) on ground truth labels y, and \u03c8 the softmax function. The distillation objective is\nL global = (1 \u2212 \u03bb)L CE (\u03c8(Z s ), y) + \u03bb\u03c4 2 KL(\u03c8(Z s /\u03c4 ), \u03c8(Z t /\u03c4 )).\n(2)\n\nHard-label distillation. We introduce a variant of distillation where we take the hard decision of the teacher as a true label. Let y t = argmax c Z t (c) be the hard decision of the teacher, the objective associated with this hard-label distillation is:\nL hardDistill global = 1 2 L CE (\u03c8(Z s ), y) + 1 2 L CE (\u03c8(Z s ), y t ).(3)\nFor a given image, the hard label associated with the teacher may change depending on the specific data augmentation. We will see that this choice is better than the traditional one, while being parameter-free and conceptually simpler: The teacher prediction y t plays the same role as the true label y.\n\nNote also that the hard labels can also be converted into soft labels with label smoothing [47], where the true label is considered to have a probability of 1 \u2212 \u03b5, and the remaining \u03b5 is shared across the remaining classes. We fix this parameter to \u03b5 = 0.1 in our all experiments that use true labels.  It interacts with the class and patch tokens through the self-attention layers. This distillation token is employed in a similar fashion as the class token, except that on output of the network its objective is to reproduce the (hard) label predicted by the teacher, instead of true label. Both the class and distillation tokens input to the transformers are learned by back-propagation.\n\n\nself-attention\n\nDistillation token. We now focus on our proposal, which is illustrated in Figure 2. We add a new token, the distillation token, to the initial embeddings (patches and class token). Our distillation token is used similarly as the class token: it interacts with other embeddings through self-attention, and is output by the network after the last layer. Its target objective is given by the distillation component of the loss. The distillation embedding allows our model to learn from the output of the teacher, as in a regular distillation, while remaining complementary to the class embedding. Interestingly, we observe that the learned class and distillation tokens converge towards different vectors: the average cosine similarity between these tokens equal to 0.06. As the class and distillation embeddings are computed at each layer, they gradually become more similar through the network, all the way through the last layer at which their similarity is high (cos=0.93), but still lower than 1. This is expected since as they aim at producing targets that are similar but not identical.\n\nWe verified that our distillation token adds something to the model, compared to simply adding an additional class token associated with the same target label: instead of a teacher pseudo-label, we experimented with a transformer with two class tokens. Even if we initialize them randomly and independently, during training they converge towards the same vector (cos=0.999), and the output embedding are also quasi-identical. This additional class token does not bring anything to the classification performance. In contrast, our distillation strategy provides a significant improvement over a vanilla distillation baseline, as validated by our experiments in Section 5.2.\n\nFine-tuning with distillation. We use both the true label and teacher prediction during the fine-tuning stage at higher resolution. We use a teacher with the same target resolution, typically obtained from the lower-resolution teacher by the method of Touvron et al [50]. We have also tested with true labels only but this reduces the benefit of the teacher and leads to a lower performance.\n\nClassification with our approach: joint classifiers. At test time, both the class or the distillation embeddings produced by the transformer are associated with linear classifiers and able to infer the image label. Yet our referent method is the late fusion of these two separate heads, for which we add the softmax output by the two classifiers to make the prediction. We evaluate these three options in Section 5.\n\n\nExperiments\n\nThis section presents a few analytical experiments and results. We first discuss our distillation strategy. Then we comparatively analyze the efficiency and accuracy of convnets and vision transformers.\n\n\nTransformer models\n\nAs mentioned earlier, our architecture design is identical to the one proposed by Dosovitskiy et al. [15] with no convolutions. Our only differences are the training strategies, and the distillation token. Also we do not use a MLP head for the pre-training but only a linear classifier. To avoid any confusion, we refer to the results obtained in the prior work by ViT, and prefix ours by DeiT. If not specified, DeiT refers to our referent model DeiT-B, which has the same architecture as ViT-B. When we fine-tune DeiT at a larger resolution, we append the resulting operating resolution at the end, e.g, DeiT-B\u2191384. Last, when using our distillation procedure, we identify it with an alembic sign as DeiT\u2697.\n\nThe parameters of ViT-B (and therefore of DeiT-B) are fixed as D = 768, h = 12 and d = D/h = 64. We introduce two smaller models, namely DeiT-S and DeiT-Ti, for which we change the number of heads, keeping d fixed. Table 1 summarizes the models that we consider in our paper. Table 1: Variants of our DeiT architecture. The larger model, DeiT-B, has the same architecture as the ViT-B [15]. The only parameters that vary across models are the embedding dimension and the number of heads, and we keep the dimension per head constant (equal to 64). Smaller models have a lower parameter count, and a faster throughput. The throughput is measured for images at resolution 224\u00d7224.\n\n\nModel\n\nViT  \n\n\nDistillation\n\nOur distillation method produces a vision transformer that becomes on par with the best convnets in terms of the trade-off between accuracy and throughput, see Table 5. Interestingly, the distilled model outperforms its teacher in terms of the trade-off between accuracy and throughput. Our best model on ImageNet-1k is 85.2% top-1 accuracy outperforms the best Vit-B model pretrained on JFT-300M at resolution 384 (84.15%). For reference, the current state of the art of 88.55% achieved with extra training data was obtained by the ViT-H model (600M parameters) trained on JFT-300M at resolution 512. Hereafter we provide several analysis and observations.\n\n\nConvnets teachers.\n\nWe have observed that using a convnet teacher gives better performance than using a transformer. Table 2 compares distillation results with different teacher architectures. The fact that the convnet is a better teacher is probably due to the inductive bias inherited by the transformers through distillation, as explained in Abnar et al. [1]. In all of our subsequent distillation experiments the default teacher is a RegNetY-16GF [40] (84M parameters) that we trained with the same data and same data-augmentation as DeiT. This teacher reaches 82.9% top-1 accuracy on ImageNet. Table 3: Distillation experiments on Imagenet with DeiT, 300 epochs of pretraining. We report the results for our new distillation method in the last three rows. We separately report the performance when classifying with only one of the class or distillation embeddings, and then with a classifier taking both of them as input. In the last row (class+distillation), the result correspond to the late fusion of the class and distillation classifiers. Comparison of distillation methods. We compare the performance of different distillation strategies in Table 3. Hard distillation significantly outperforms soft distillation for transformers, even when using only a class token: hard distillation reaches 83.0% at resolution 224\u00d7224, compared to the soft distillation accuracy of 81.8%. Our distillation strategy from Section 4 further improves the performance, showing that the two tokens provide complementary information useful for classification: the classifier on the two tokens is significantly better than the independent class and distillation classifiers, which by themselves already outperform the distillation baseline. The distillation token gives slightly better results than the class token. It is also more correlated to the convnets prediction. This difference in performance is probably due to the fact that it benefits more from the inductive bias of convnets. We give more details and an analysis in the next paragraph. The distillation token has an undeniable advantage for the initial training.\n\n\nAgreement with the teacher & inductive bias?\n\nAs discussed above, the architecture of the teacher has an important impact. Does it inherit existing inductive bias that would facilitate the training? While we believe it difficult to formally answer this question, we analyze in Table 4 the decision agreement between the convnet teacher, our image transformer DeiT learned from labels only, and our transformer DeiT\u2697.\n\nOur distilled model is more correlated to the convnet than with a transformer learned from scratch. As to be expected, the classifier associated with the distillation embedding is closer to the convnet that the one associated with the class embedding, and conversely the one associated with the class embedding is more similar to DeiT learned without distillation. Unsurprisingly, the joint class+distil classifier offers a middle ground. Table 4: Disagreement analysis between convnet, image transformers and distillated transformers: We report the fraction of sample classified differently for all classifier pairs, i.e., the rate of different decisions. We include two models without distillation (a RegNetY and DeiT-B), so that we can compare how our distilled models and classification heads are correlated to these teachers. \n\n\nNumber of epochs.\n\nIncreasing the number of epochs significantly improves the performance of training with distillation, see Figure 3. With 300 epochs, our distilled network DeiT-B\u2697 is already better than DeiT-B. But while for the latter the performance saturates with longer schedules, our distilled network clearly benefits from a longer training time.\n\n\nEfficiency vs accuracy: a comparative study with convnets\n\nIn the literature, the image classificaton methods are often compared as a compromise between accuracy and another criterion, such as FLOPs, number of parameters, size of the network, etc. We focus in Figure 1 on the tradeoff between the throughput (images processed per second) and the top-1 classification accuracy on ImageNet. We focus on the popular state-of-the-art EfficientNet convnet, which has benefited from years of research on convnets and was optimized by architecture search on the ImageNet validation set.\n\nOur method DeiT is slightly below EfficientNet, which shows that we have almost closed the gap between vision transformers and convnets when training with Imagenet only. These results are a major improvement (+6.3% top-1 in a comparable setting) over previous ViT models trained on Imagenet1k only [15]. Furthermore, when DeiT benefits from the distillation from a relatively weaker RegNetY to produce DeiT\u2697, it outperforms EfficientNet. It also outperforms by 1% (top-1 acc.) the Vit-B model pre-trained on JFT300M at resolution 384 (85.2% vs 84.15%), while being significantly faster to train. Table 5 reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real, that have a test set distinct from the ImageNet validation, which reduces overfitting on the validation set. Our results show that DeiT-B\u2697 and DeiT-B\u2697 \u2191384 outperform, by some margin, the state of the art on the trade-off between accuracy and inference time on GPU.  Table 5: Throughput on and accuracy on Imagenet [42], Imagenet Real [5] and Imagenet V2 matched frequency [41] of DeiT and of several state-of-the-art convnets, for models trained with no external data. The throughput is measured as the number of images that we can process per second on one 16GB V100 GPU. For each model we take the largest possible batch size for the usual resolution of the model and calculate the average time over 30 runs to process that batch. With that we calculate the number of images processed per second. Throughput can vary according to the implementation: for a direct comparison and in order to be as fair as possible, we use for each model the definition in the same GitHub [55] repository. : Regnet optimized with a similar optimization procedure as ours, which boosts the results. These networks serve as teachers when we use our distillation strategy. Figure 3: Distillation on ImageNet [42] with DeiT-B: performance as a function of the number of training epochs. We provide the performance without distillation (horizontal dotted line) as it saturates after 400 epochs.\n\n\n\u2697\u2191 \u2697\n\n\nTransfer learning: Performance on downstream tasks\n\nAlthough DeiT perform very well on ImageNet it is important to evaluate them on other datasets with transfer learning in order to measure the power of generalization of DeiT. We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table 6. Table 7 compares DeiT transfer learning results to those of ViT [15] and state of the art convolutional architectures [48]. DeiT is on par with competitive convnet models, which is in line with our previous conclusion on ImageNet.\n\nComparison vs training from scratch. We investigate the performance when training from scratch on a small dataset, without Imagenet pre-training. We get the following results on the small CIFAR-10, which is small both w.r.t. the number of images and labels: For this experiment, we tried we get as close as possible to the Imagenet pre-training counterpart, meaning that (1) we consider longer training sched-  ules (up to 7200 epochs, which corresponds to 300 Imagenet epochs) so that the network has been fed a comparable number of images in total; (2) we rescale images to 224 \u00d7 224 to ensure that we have the same augmentation. The results are not as good as with Imagenet pre-training (98.5% vs 99.1%), which is expected since the network has seen a much lower diversity. However they show that it is possible to learn a reasonable transformer on CIFAR-10 only.\n\n\nTraining details & ablation\n\nIn this section we discuss the DeiT training strategy to learn vision transformers in a data-efficient manner. We build upon PyTorch [39] and the timm library [55] 2 . We provide hyper-parameters as well as an ablation study in which we analyze the impact of each choice.  Table 8: Ablation study on training methods on ImageNet [42]. The top row (\"none\") corresponds to our default configuration employed for DeiT. The symbols and indicates that we use and do not use the corresponding method, respectively. We report the accuracy scores (%) after the initial training at resolution 224\u00d7224, and after fine-tuning at resolution 384\u00d7384. The hyper-parameters are fixed according to Table 9, and may be suboptimal.\n\n* indicates that the model did not train well, possibly because hyper-parameters are not adapted.\n\nof them not converging, we follow the recommendation of Hanin and Rolnick [20] to initialize the weights with a truncated normal distribution. Table 9 indicates the hyper-parameters that we use by default at training time for all our experiments, unless stated otherwise. For distillation we follow the recommendations from Cho et al. [9] to select the parameters \u03c4 and \u03bb. We take the typical values \u03c4 = 3.0 and \u03bb = 0.1 for the usual (soft) distillation.\n\n\nData-Augmentation.\n\nCompared to models that integrate more priors (such as convolutions), transformers require a larger amount of data. Thus, in order to train with datasets of the same size, we rely on extensive data augmentation. We evaluate different types of strong data augmentation, with the objective to reach a data-efficient training regime.\n\nAuto-Augment [11], Rand-Augment [12], and random erasing [62] improve the results. For the two latter we use the timm [55] customizations, and after ablation we choose Rand-Augment instead of AutoAugment. Overall our experiments confirm that transformers require a strong data augmentation: almost all the data-augmentation methods that we evaluate prove to be useful. One exception is dropout, which we exclude from our training procedure.\n\n\nMethods\n\nViT-B [15] DeiT-B  Table 9: Ingredients and hyper-parameters for our method and Vit-B.\n\n\nRegularization & Optimizers.\n\nWe have considered different optimizers and cross-validated different learning rates and weight decays. Transformers are sensitive to the setting of optimization hyper-parameters. Therefore, during cross-validation, we tried 3 different learning rates (5.10 \u22124 , 3.10 \u22124 , 5.10 \u22125 ) and 3 weight decay (0.03, 0.04, 0.05). We scale the learning rate according to the batch size with the formula: lr scaled = lr 512 \u00d7 batchsize, similarly to Goyal et al. [19] except that we use 512 instead of 256 as the base value.\n\nThe best results use the AdamW optimizer with the same learning rates as ViT [15] but with a much smaller weight decay, as the weight decay reported in the paper hurts the convergence in our setting.\n\nWe have employed stochastic depth [29], which facilitates the convergence of transformers, especially deep ones [16,17]. For vision transformers, they were first adopted in the training procedure by Wightman [55]. Regularization like Mixup [60] and Cutmix [59] improve performance. We also use repeated augmentation [4,25], which provides a significant boost in performance and is one of the key ingredients of our proposed training procedure.\n\nExponential Moving Average (EMA). We evaluate the EMA of our network obtained after training. There are small gains, which vanish after fine-tuning: the EMA model has an edge of is 0.1 accuracy points, but when fine-tuned the two models reach the same (improved) performance.\n\nFine-tuning at different resolution. We adopt the fine-tuning procedure from Touvron et al. [51]: our schedule, regularization and optimization procedure are identical to that of FixEfficientNet but we keep the training-time data aug-image throughput Imagenet [42] Real [5] V2 [41]   mentation (contrary to the dampened data augmentation of Touvron et al. [51]). We also interpolate the positional embeddings: In principle any classical image scaling technique, like bilinear interpolation, could be used. However, a bilinear interpolation of a vector from its neighbors reduces its 2 -norm compared to its neighbors. These low-norm vectors are not adapted to the pre-trained transformers and we observe a significant drop in accuracy if we employ use directly without any form of fine-tuning. Therefore we adopt a bicubic interpolation that approximately preserves the norm of the vectors, before fine-tuning the network with either AdamW [36] or SGD. These optimizers have a similar performance for the fine-tuning stage, see Table 8. By default and similar to ViT [15] we train DeiT models with at resolution 224 and we fine-tune at resolution 384. We detail how to do this interpolation in Section 3. However, in order to measure the influence of the resolution we have finetuned DeiT at different resolutions. We report these results in Table 10.\n\nTraining time. A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.As a comparison point, a similar training with a RegNetY-16GF [40] (84M parameters) is 20% slower. DeiT-S and DeiT-Ti are trained in less than 3 days on 4 GPU. Then, optionally we fine-tune the model at a larger resolution. This takes 20 hours on a single node (8 GPU) to produce a FixDeiT-B model at resolution 384\u00d7384, which corresponds to 25 epochs. Not having to rely on batch-norm allows one to reduce the batch size without impacting performance, which makes it easier to train larger models. Note that, since we use repeated augmentation [4,25] with 3 repetitions, we only see one third of the images during a single epoch 3 .\n\n\nConclusion\n\nIn this paper, we have introduced DeiT, which are image transformers that do not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization during almost a decade, including through extensive architecture search that is prone to overfiting, as it is the case for instance for EfficientNets [51]. For DeiT we have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains.\n\nTherefore, considering our results, where image transformers are on par with convnets already, we believe that they will rapidly become a method of choice considering their lower memory footprint for a given accuracy.\n\nWe provide an open-source implementation of our method. It is available at https://github.com/facebookresearch/deit.\n\nFigure 2 :\n2Our distillation procedure: we simply include a new distillation token.\n\nTable 2 :\n2We compare on ImageNet [42] the performance (top-1 acc., %) of the \nstudent as a function of the teacher model used for distillation. \n\nTeacher \n\nStudent: DeiT-B \u2697 \n\nModels \nacc. pretrain \n\u2191384 \n\nDeiT-B \n81.8 \n81.9 \n83.1 \n\nRegNetY-4GF \n80.0 \n82.7 \n83.6 \nRegNetY-8GF \n81.7 \n82.7 \n83.8 \nRegNetY-12GF 82.4 \n83.1 \n84.1 \nRegNetY-16GF 82.9 \n83.1 \n84.2 \n\n\n\nTable 6 :\n6Datasets used for our different tasks.Dataset \nTrain size Test size #classes \n\nImageNet [42] \n1,281,167 \n50,000 \n1000 \niNaturalist 2018 [26] \n437,513 \n24,426 \n8,142 \niNaturalist 2019 [27] \n265,240 \n3,003 \n1,010 \nFlowers-102 [38] \n2,040 \n6,149 \n102 \nStanford Cars [30] \n8,144 \n8,041 \n196 \nCIFAR-100 [31] \n50,000 \n10,000 \n100 \nCIFAR-10 [31] \n50,000 \n10,000 \n10 \n\n\n\nTable 7 :\n7We compare Transformers based models on different transfer learning \ntask with ImageNet pre-training. We also report results with convolutional \narchitectures for reference. \n\nModel \nImageNet CIFAR-10 CIFAR-100 Flowers Cars iNat-18 iNat-19 im/sec \n\nGrafit ResNet-50 [49] \n79.6 \n98.2 \n92.5 \n69.8 \n75.9 \n1226.1 \nGrafit RegNetY-8GF [49] \n99.0 \n94.0 \n76.8 \n80.0 \n591.6 \nResNet-152 [10] \n69.1 \n526.3 \nEfficientNet-B7 [48] \n84.3 \n98.9 \n91.7 \n98.8 \n94.7 \n55.1 \n\nViT-B/32 [15] \n73.4 \n97.8 \n86.3 \n85.4 \n394.5 \nViT-B/16 [15] \n77.9 \n98.1 \n87.1 \n89.5 \n85.9 \nViT-L/32 [15] \n71.2 \n97.9 \n87.1 \n86.4 \n124.1 \nViT-L/16 [15] \n76.5 \n97.9 \n86.4 \n89.7 \n27.3 \n\nDeiT-B \n81.8 \n99.1 \n90.8 \n98.4 \n92.1 \n73.2 \n77.7 \n292.3 \nDeiT-B\u2191384 \n83.1 \n99.1 \n90.8 \n98.5 \n93.3 \n79.5 \n81.4 \n85.9 \nDeiT-B\u2697 \n83.4 \n99.1 \n91.3 \n98.8 \n92.9 \n73.7 \n78.4 \n290.9 \nDeiT-B\u2697 \u2191384 \n84.4 \n99.2 \n91.4 \n98.9 \n93.9 \n80.1 \n83.0 \n85.9 \n\n\n\n\nsize (image/s) acc. top-1 acc. top-1 acc. top-1160 2 \n609.31 \n79.9 \n84.8 \n67.6 \n224 2 \n291.05 \n81.8 \n86.7 \n71.5 \n320 2 \n134.13 \n82.7 \n87.2 \n71.9 \n384 2 \n85.87 \n83.1 \n87.7 \n72.4 \n\n\n\nTable 10 :\n10Performance of DeiT trained at size 224 2 for varying finetuning sizes on ImageNet-1k, ImageNet-Real and ImageNet-v2 matched frequency.\nWe can accelerate the learning of the larger model DeiT-B by training it on 8 GPUs in two days.\nThe timm implementation already included a training procedure that improved the accuracy of ViT-B from 77.91% to 79.35% top-1, and trained on Imagenet-1k with a 8xV100 GPU machine.\nFormally it means that we have 100 epochs, but each is 3x longer because of the repeated augmentations. We prefer to refer to this as 300 epochs in order to have a direct comparison on the effective training time with and without repeated augmentation.\nAcknowledgementsMany thanks to Ross Wightman for sharing his ViT code and bootstrapping training method with the community, as well as for valuable feedback that helped us to fix different aspects of this paper. Thanks to Vinicius Reis, Mannat Singh, Ari Morcos, Mark Tygert, Gabriel Synnaeve, and other colleagues at Facebook for brainstorming and some exploration on this axis. Thanks to Ross Girshick and Piotr Dollar for constructive comments.\nTransferring inductive biases through knowledge distillation. Samira Abnar, Mostafa Dehghani, Willem Zuidema, arXiv:2006.00555arXiv preprintSamira Abnar, Mostafa Dehghani, and Willem Zuidema. Transferring inductive biases through knowledge distillation. arXiv preprint arXiv:2006.00555, 2020.\n\nSqueeze-and-excitation networks. Jie Hu, Gang Shen, Sun, arXiv:1709.01507arXiv preprintJie Hu andLi Shen and Gang Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 2017.\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nMultigrain: a unified image embedding for classes and instances. Maxim Berman, Herv\u00e9 J\u00e9gou, Andrea Vedaldi, Iasonas Kokkinos, Matthijs Douze, arXiv:1902.05509arXiv preprintMaxim Berman, Herv\u00e9 J\u00e9gou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a unified image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019.\n\n. Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, Aaron Van Den Oord, arXiv:2006.07159Are we done with imagenet? arXiv preprintLucas Beyer, Olivier J. H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, 2020.\n\nGenerative pretraining from pixels. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, International Conference on Machine Learning. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, 2020.\n\nUniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, A E Kholy, Faisal Ahmed, Zhe Gan, Y Cheng, Jing Liu, European Conference on Computer Vision. Yen-Chun Chen, Linjie Li, Licheng Yu, A. E. Kholy, Faisal Ahmed, Zhe Gan, Y. Cheng, and Jing jing Liu. Uniter: Universal image-text representation learning. In European Conference on Computer Vision, 2020.\n\nOn the efficacy of knowledge distillation. J H Cho, B Hariharan, International Conference on Computer Vision. J. H. Cho and B. Hariharan. On the efficacy of knowledge distillation. International Conference on Computer Vision, 2019.\n\nFeature space augmentation for long-tailed data. P Chu, Xiao Bian, Shaopeng Liu, Haibin Ling, arXiv:2008.03673arXiv preprintP. Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for long-tailed data. arXiv preprint arXiv:2008.03673, 2020.\n\nBarret Ekin Dogus Cubuk, Dandelion Zoph, Vijay Man\u00e9, Vasudevan, V Quoc, Le, Autoaugment, arXiv:1805.09501Learning augmentation policies from data. arXiv preprintEkin Dogus Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\n\nRandaugment: Practical automated data augmentation with a reduced search space. D Ekin, Barret Cubuk, Jonathon Zoph, Quoc V Shlens, Le, arXiv:1909.13719arXiv preprintEkin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with a reduced search space. arXiv preprint arXiv:1909.13719, 2019.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, pages 248-255, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pretraining of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre- training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.11929arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nAngela Fan, Edouard Grave, Armand Joulin, arXiv:1909.11556Reducing transformer depth on demand with structured dropout. 2020arXiv preprintAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. ICLR 2020.\n\nTraining with quantization noise for extreme model compression. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herv\u00e9 J\u00e9gou, Armand Joulin, arXiv:2004.07320arXiv preprintAngela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herv\u00e9 J\u00e9gou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020.\n\nConvolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, arXiv:1705.03122arXiv preprintJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross B Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nHow to start training: The effect of initialization and architecture. Boris Hanin, David Rolnick, NIPS. 31Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. NIPS, 31, 2018.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Conference on Computer Vision and Pattern Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, June 2016.\n\nBag of tricks for image classification with convolutional neural networks. Zhi Tong He, Hang Zhang, Zhongyue Zhang, Junyuan Zhang, Mu Xie, Li, Conference on Computer Vision and Pattern Recognition. Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Conference on Computer Vision and Pattern Recognition, 2019.\n\nGaussian error linear units (gelus). Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\n\nGeoffrey E Hinton, Oriol Vinyals, J Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey E. Hinton, Oriol Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nAugment your batch: Improving generalization through instance repetition. Elad Hoffer, Itay Ben-Nun, Niv Hubara, Torsten Giladi, Daniel Hoefler, Soudry, Conference on Computer Vision and Pattern Recognition. Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repeti- tion. In Conference on Computer Vision and Pattern Recognition, 2020.\n\nOisin Mac Grant Van Horn, Yang Aodha, Alexander Song, Hartwig Shepard, Pietro Adam, Serge J Perona, Belongie, arXiv:1707.06642The inaturalist challenge 2018 dataset. arXiv preprintGrant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset. arXiv preprint arXiv:1707.06642, 2018.\n\nOisin Mac Grant Van Horn, Yang Aodha, Alexander Song, Hartwig Shepard, Pietro Adam, Serge J Perona, Belongie, arXiv:1707.06642The inaturalist challenge 2019 dataset. arXiv preprintGrant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset. arXiv preprint arXiv:1707.06642, 2019.\n\nRelation networks for object detection. H Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Y Wei, Conference on Computer Vision and Pattern Recognition. H. Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Y. Wei. Relation networks for object detection. Conference on Computer Vision and Pattern Recognition, 2018.\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, European Conference on Computer Vision. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, 2016.\n\n3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, 4th International IEEE Workshop on 3D Representation and Recognition. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Represen- tation and Recognition (3dRR-13), 2013.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, CIFARTechnical reportAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, CIFAR, 2009.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\n\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang, arXiv:1908.03557Visu-alBERT: a simple and performant baseline for vision and language. arXiv preprintLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visu- alBERT: a simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\n\nXiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang, Selective kernel networks. Conference on Computer Vision and Pattern Recognition. Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. Conference on Computer Vision and Pattern Recognition, 2019.\n\nObjectcentric learning with slot attention. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, A Dosovitskiy, Thomas Kipf, arXiv:2006.15055arXiv preprintFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahen- dran, Georg Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf. Object- centric learning with slot attention. arXiv preprint arXiv:2006.15055, 2020.\n\nI Loshchilov, F Hutter, arXiv:1711.05101Fixing weight decay regularization in adam. arXiv preprintI. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017.\n\nVilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, D Parikh, Stefan Lee, NIPS. Jiasen Lu, Dhruv Batra, D. Parikh, and Stefan Lee. Vilbert: Pretraining task- agnostic visiolinguistic representations for vision-and-language tasks. In NIPS, 2019.\n\nAutomated flower classification over a large number of classes. M-E Nilsback, A Zisserman, Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing. the Indian Conference on Computer Vision, Graphics and Image ProcessingM-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graph- ics and Image Processing, 2008.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py- torch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026-8037, 2019.\n\nDesigning network design spaces. Ilija Radosavovic, Raj Prateek Kosaraju, Ross B Girshick, Kaiming He, Piotr Doll\u00e1r, Conference on Computer Vision and Pattern Recognition. Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. Conference on Computer Vision and Pattern Recognition, 2020.\n\nB Recht, Rebecca Roelofs, L Schmidt, V Shankar, arXiv:1902.10811Do imagenet classifiers generalize to imagenet. arXiv preprintB. Recht, Rebecca Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, International journal of Computer Vision. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan- der C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In- ternational journal of Computer Vision, 2015.\n\nGlobal self-attention networks for image recognition. Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, Ching-Hui Chen, arXiv:2010.03019arXiv preprintZhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, and Ching-Hui Chen. Global self-attention networks for image recognition. arXiv preprint arXiv:2010.03019, 2020.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.\n\nVideobert: A joint model for video and language representation learning. C Sun, A Myers, Carl Vondrick, Kevin Murphy, C Schmid, Conference on Computer Vision and Pattern Recognition. C. Sun, A. Myers, Carl Vondrick, Kevin Murphy, and C. Schmid. Videobert: A joint model for video and language representation learning. Conference on Computer Vision and Pattern Recognition, 2019.\n\nRevisiting unreasonable effectiveness of data in deep learning era. Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843-852, 2017.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, V Vanhoucke, S Ioffe, Jon Shlens, Z Wojna, Conference on Computer Vision and Pattern Recognition. Christian Szegedy, V. Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. Conference on Computer Vision and Pattern Recognition, 2016.\n\nMingxing Tan, V Quoc, Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. arXiv preprintMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convo- lutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\nHugo Touvron, Alexandre Sablayrolles, M Douze, M Cord, H J\u00e9gou, arXiv:2011.12982Grafit: Learning fine-grained image representations with coarse labels. arXiv preprintHugo Touvron, Alexandre Sablayrolles, M. Douze, M. Cord, and H. J\u00e9gou. Grafit: Learning fine-grained image representations with coarse labels. arXiv preprint arXiv:2011.12982, 2020.\n\nFixing the train-test resolution discrepancy. Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herve Jegou, NIPS. Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy. NIPS, 2019.\n\nHugo Touvron, Andrea Vedaldi, arXiv:2003.08237Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy: Fixefficientnet. arXiv preprintHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy: Fixefficientnet. arXiv preprint arXiv:2003.08237, 2020.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\nX Wang, Ross B Girshick, A Gupta, Kaiming He, Non-local neural networks. Conference on Computer Vision and Pattern Recognition. X. Wang, Ross B. Girshick, A. Gupta, and Kaiming He. Non-local neural networks. Conference on Computer Vision and Pattern Recognition, 2018.\n\nCircumventing outliers of autoaugment with knowledge distillation. European Conference on Computer Vision. Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, Qi Tian, Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Cir- cumventing outliers of autoaugment with knowledge distillation. European Con- ference on Computer Vision, 2020.\n\nPytorch image models. Ross Wightman, Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.\n\nVisual transformers: Token-based image representation and processing for computer vision. Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, Peter Vajda, arXiv:2006.03677arXiv preprintBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.03677, 2020.\n\nQizhe Xie, Eduard H Hovy, Minh-Thang Luong, Quoc V Le, arXiv:1911.04252Selftraining with noisy student improves imagenet classification. arXiv preprintQizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Self- training with noisy student improves imagenet classification. arXiv preprint arXiv:1911.04252, 2019.\n\nRevisit knowledge distillation: a teacher-free framework. L Yuan, F Tay, G Li, T Wang, Jiashi Feng, Conference on Computer Vision and Pattern Recognition. L. Yuan, F. Tay, G. Li, T. Wang, and Jiashi Feng. Revisit knowledge distillation: a teacher-free framework. Conference on Computer Vision and Pattern Recognition, 2020.\n\nSangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, Cutmix, arXiv:1905.04899Regularization strategy to train strong classifiers with localizable features. arXiv preprintSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. arXiv preprint arXiv:1905.04899, 2019.\n\nHongyi Zhang, Moustapha Ciss\u00e9, Yann N Dauphin, David Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintHongyi Zhang, Moustapha Ciss\u00e9, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R Manmatha, Mu Li, Alexander Smola, arXiv:2004.08955Resnest: Split-attention networks. arXiv preprintHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.\n\nRandom erasing data augmentation. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, AAAI. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random eras- ing data augmentation. In AAAI, 2020.\n", "annotations": {"author": "[{\"end\":114,\"start\":79},{\"end\":151,\"start\":115},{\"end\":189,\"start\":152},{\"end\":228,\"start\":190},{\"end\":274,\"start\":229},{\"end\":309,\"start\":275},{\"end\":344,\"start\":310}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":84},{\"end\":128,\"start\":124},{\"end\":166,\"start\":161},{\"end\":205,\"start\":200},{\"end\":251,\"start\":239},{\"end\":286,\"start\":281},{\"end\":321,\"start\":319}]", "author_first_name": "[{\"end\":83,\"start\":79},{\"end\":123,\"start\":115},{\"end\":160,\"start\":152},{\"end\":199,\"start\":190},{\"end\":238,\"start\":229},{\"end\":280,\"start\":275},{\"end\":318,\"start\":310}]", "author_affiliation": "[{\"end\":113,\"start\":93},{\"end\":150,\"start\":130},{\"end\":188,\"start\":168},{\"end\":227,\"start\":207},{\"end\":273,\"start\":253},{\"end\":308,\"start\":288},{\"end\":343,\"start\":323}]", "title": "[{\"end\":76,\"start\":1},{\"end\":420,\"start\":345}]", "venue": null, "abstract": "[{\"end\":1461,\"start\":422}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1735,\"start\":1731},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1738,\"start\":1735},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1826,\"start\":1822},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":1829,\"start\":1826},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1934,\"start\":1931},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1937,\"start\":1934},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":1940,\"start\":1937},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2085,\"start\":2082},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2088,\"start\":2085},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2157,\"start\":2153},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2233,\"start\":2229},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2899,\"start\":2895},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3471,\"start\":3467},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3522,\"start\":3518},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6010,\"start\":6006},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6156,\"start\":6152},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6244,\"start\":6240},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6247,\"start\":6244},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6250,\"start\":6247},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6253,\"start\":6250},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6256,\"start\":6253},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6259,\"start\":6256},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6335,\"start\":6332},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6590,\"start\":6586},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6605,\"start\":6602},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6608,\"start\":6605},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6631,\"start\":6627},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6634,\"start\":6631},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6670,\"start\":6666},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6705,\"start\":6702},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6708,\"start\":6705},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6711,\"start\":6708},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6753,\"start\":6749},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6974,\"start\":6970},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6977,\"start\":6974},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7021,\"start\":7017},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7316,\"start\":7312},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7548,\"start\":7545},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7571,\"start\":7567},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7605,\"start\":7601},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7738,\"start\":7734},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8256,\"start\":8252},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8303,\"start\":8299},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8723,\"start\":8720},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9287,\"start\":9283},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9290,\"start\":9287},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10101,\"start\":10097},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10796,\"start\":10792},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10936,\"start\":10932},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11173,\"start\":11170},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11264,\"start\":11260},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11795,\"start\":11791},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11813,\"start\":11809},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12205,\"start\":12201},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12750,\"start\":12746},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13377,\"start\":13373},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14106,\"start\":14102},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14109,\"start\":14106},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15371,\"start\":15367},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18021,\"start\":18017},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18905,\"start\":18901},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19899,\"start\":19895},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21240,\"start\":21237},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21334,\"start\":21330},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25487,\"start\":25483},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26215,\"start\":26211},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26234,\"start\":26231},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26273,\"start\":26269},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26873,\"start\":26869},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27089,\"start\":27085},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27662,\"start\":27658},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27716,\"start\":27712},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28861,\"start\":28857},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":28887,\"start\":28883},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29057,\"start\":29053},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29616,\"start\":29612},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29876,\"start\":29873},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30364,\"start\":30360},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30383,\"start\":30379},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":30408,\"start\":30404},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":30469,\"start\":30465},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30809,\"start\":30805},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31375,\"start\":31371},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31515,\"start\":31511},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31673,\"start\":31669},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31751,\"start\":31747},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31754,\"start\":31751},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":31847,\"start\":31843},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":31879,\"start\":31875},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31895,\"start\":31891},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31954,\"start\":31951},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31957,\"start\":31954},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32453,\"start\":32449},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32621,\"start\":32617},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32630,\"start\":32627},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32638,\"start\":32634},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32717,\"start\":32713},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33301,\"start\":33297},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33428,\"start\":33424},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33896,\"start\":33892},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34378,\"start\":34375},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34381,\"start\":34378},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34933,\"start\":34929}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":35676,\"start\":35592},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36037,\"start\":35677},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36411,\"start\":36038},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":37300,\"start\":36412},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":37482,\"start\":37301},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":37632,\"start\":37483}]", "paragraph": "[{\"end\":2089,\"start\":1477},{\"end\":2710,\"start\":2091},{\"end\":3104,\"start\":2712},{\"end\":3766,\"start\":3106},{\"end\":3974,\"start\":3768},{\"end\":4031,\"start\":3976},{\"end\":4387,\"start\":4033},{\"end\":4732,\"start\":4389},{\"end\":4877,\"start\":4734},{\"end\":5731,\"start\":4879},{\"end\":6260,\"start\":5748},{\"end\":6712,\"start\":6262},{\"end\":7251,\"start\":6714},{\"end\":7675,\"start\":7253},{\"end\":9162,\"start\":7677},{\"end\":9347,\"start\":9195},{\"end\":9923,\"start\":9349},{\"end\":10722,\"start\":9969},{\"end\":11174,\"start\":10724},{\"end\":11962,\"start\":11176},{\"end\":12678,\"start\":11964},{\"end\":13520,\"start\":12680},{\"end\":14224,\"start\":13555},{\"end\":14563,\"start\":14246},{\"end\":14638,\"start\":14635},{\"end\":14894,\"start\":14640},{\"end\":15274,\"start\":14971},{\"end\":15966,\"start\":15276},{\"end\":17075,\"start\":15985},{\"end\":17749,\"start\":17077},{\"end\":18142,\"start\":17751},{\"end\":18559,\"start\":18144},{\"end\":18777,\"start\":18575},{\"end\":19508,\"start\":18800},{\"end\":20187,\"start\":19510},{\"end\":20202,\"start\":20197},{\"end\":20876,\"start\":20219},{\"end\":22992,\"start\":20899},{\"end\":23411,\"start\":23041},{\"end\":24244,\"start\":23413},{\"end\":24601,\"start\":24266},{\"end\":25183,\"start\":24663},{\"end\":27269,\"start\":25185},{\"end\":27824,\"start\":27331},{\"end\":28692,\"start\":27826},{\"end\":29437,\"start\":28724},{\"end\":29536,\"start\":29439},{\"end\":29992,\"start\":29538},{\"end\":30345,\"start\":30015},{\"end\":30787,\"start\":30347},{\"end\":30885,\"start\":30799},{\"end\":31432,\"start\":30918},{\"end\":31633,\"start\":31434},{\"end\":32078,\"start\":31635},{\"end\":32355,\"start\":32080},{\"end\":33708,\"start\":32357},{\"end\":34463,\"start\":33710},{\"end\":35254,\"start\":34478},{\"end\":35473,\"start\":35256},{\"end\":35591,\"start\":35475}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9968,\"start\":9924},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14634,\"start\":14564},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14970,\"start\":14895}]", "table_ref": "[{\"end\":2681,\"start\":2674},{\"end\":19732,\"start\":19725},{\"end\":19793,\"start\":19786},{\"end\":20386,\"start\":20379},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21003,\"start\":20996},{\"end\":21485,\"start\":21478},{\"end\":22038,\"start\":22031},{\"end\":23279,\"start\":23272},{\"end\":23859,\"start\":23852},{\"end\":25788,\"start\":25781},{\"end\":26170,\"start\":26163},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27592,\"start\":27585},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27601,\"start\":27594},{\"end\":29004,\"start\":28997},{\"end\":29413,\"start\":29406},{\"end\":29688,\"start\":29681},{\"end\":30825,\"start\":30818},{\"end\":33392,\"start\":33385},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":33707,\"start\":33699}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1475,\"start\":1463},{\"attributes\":{\"n\":\"2\"},\"end\":5746,\"start\":5734},{\"attributes\":{\"n\":\"3\"},\"end\":9193,\"start\":9165},{\"attributes\":{\"n\":\"4\"},\"end\":13553,\"start\":13523},{\"end\":14244,\"start\":14227},{\"end\":15983,\"start\":15969},{\"attributes\":{\"n\":\"5\"},\"end\":18573,\"start\":18562},{\"attributes\":{\"n\":\"5.1\"},\"end\":18798,\"start\":18780},{\"end\":20195,\"start\":20190},{\"attributes\":{\"n\":\"5.2\"},\"end\":20217,\"start\":20205},{\"end\":20897,\"start\":20879},{\"end\":23039,\"start\":22995},{\"end\":24264,\"start\":24247},{\"attributes\":{\"n\":\"5.3\"},\"end\":24661,\"start\":24604},{\"end\":27276,\"start\":27272},{\"attributes\":{\"n\":\"5.4\"},\"end\":27329,\"start\":27279},{\"attributes\":{\"n\":\"6\"},\"end\":28722,\"start\":28695},{\"end\":30013,\"start\":29995},{\"end\":30797,\"start\":30790},{\"end\":30916,\"start\":30888},{\"attributes\":{\"n\":\"7\"},\"end\":34476,\"start\":34466},{\"end\":35603,\"start\":35593},{\"end\":35687,\"start\":35678},{\"end\":36048,\"start\":36039},{\"end\":36422,\"start\":36413},{\"end\":37494,\"start\":37484}]", "table": "[{\"end\":36037,\"start\":35689},{\"end\":36411,\"start\":36088},{\"end\":37300,\"start\":36424},{\"end\":37482,\"start\":37350}]", "figure_caption": "[{\"end\":35676,\"start\":35605},{\"end\":36088,\"start\":36050},{\"end\":37350,\"start\":37303},{\"end\":37632,\"start\":37497}]", "figure_ref": "[{\"end\":2259,\"start\":2251},{\"end\":3639,\"start\":3631},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16067,\"start\":16059},{\"end\":24380,\"start\":24372},{\"end\":24872,\"start\":24864},{\"end\":27058,\"start\":27050}]", "bib_author_first_name": "[{\"end\":38679,\"start\":38673},{\"end\":38694,\"start\":38687},{\"end\":38711,\"start\":38705},{\"end\":38941,\"start\":38938},{\"end\":38950,\"start\":38946},{\"end\":39104,\"start\":39099},{\"end\":39108,\"start\":39105},{\"end\":39118,\"start\":39113},{\"end\":39123,\"start\":39119},{\"end\":39139,\"start\":39131},{\"end\":39141,\"start\":39140},{\"end\":39387,\"start\":39382},{\"end\":39401,\"start\":39396},{\"end\":39415,\"start\":39409},{\"end\":39432,\"start\":39425},{\"end\":39451,\"start\":39443},{\"end\":39682,\"start\":39677},{\"end\":39697,\"start\":39690},{\"end\":39699,\"start\":39698},{\"end\":39717,\"start\":39708},{\"end\":39737,\"start\":39730},{\"end\":39749,\"start\":39744},{\"end\":40034,\"start\":40027},{\"end\":40052,\"start\":40043},{\"end\":40067,\"start\":40060},{\"end\":40085,\"start\":40078},{\"end\":40104,\"start\":40095},{\"end\":40121,\"start\":40115},{\"end\":40420,\"start\":40416},{\"end\":40431,\"start\":40427},{\"end\":40446,\"start\":40441},{\"end\":40461,\"start\":40454},{\"end\":40472,\"start\":40466},{\"end\":40483,\"start\":40478},{\"end\":40494,\"start\":40490},{\"end\":40800,\"start\":40792},{\"end\":40813,\"start\":40807},{\"end\":40825,\"start\":40818},{\"end\":40831,\"start\":40830},{\"end\":40833,\"start\":40832},{\"end\":40847,\"start\":40841},{\"end\":40858,\"start\":40855},{\"end\":40865,\"start\":40864},{\"end\":40877,\"start\":40873},{\"end\":41174,\"start\":41173},{\"end\":41176,\"start\":41175},{\"end\":41183,\"start\":41182},{\"end\":41413,\"start\":41412},{\"end\":41423,\"start\":41419},{\"end\":41438,\"start\":41430},{\"end\":41450,\"start\":41444},{\"end\":41632,\"start\":41626},{\"end\":41660,\"start\":41651},{\"end\":41672,\"start\":41667},{\"end\":41691,\"start\":41690},{\"end\":42043,\"start\":42042},{\"end\":42056,\"start\":42050},{\"end\":42072,\"start\":42064},{\"end\":42083,\"start\":42079},{\"end\":42085,\"start\":42084},{\"end\":42365,\"start\":42362},{\"end\":42375,\"start\":42372},{\"end\":42389,\"start\":42382},{\"end\":42404,\"start\":42398},{\"end\":42412,\"start\":42409},{\"end\":42419,\"start\":42417},{\"end\":42693,\"start\":42688},{\"end\":42710,\"start\":42702},{\"end\":42724,\"start\":42718},{\"end\":42738,\"start\":42730},{\"end\":42748,\"start\":42739},{\"end\":43131,\"start\":43125},{\"end\":43150,\"start\":43145},{\"end\":43167,\"start\":43158},{\"end\":43184,\"start\":43180},{\"end\":43205,\"start\":43198},{\"end\":43218,\"start\":43212},{\"end\":43239,\"start\":43232},{\"end\":43258,\"start\":43250},{\"end\":43274,\"start\":43269},{\"end\":43291,\"start\":43284},{\"end\":43634,\"start\":43628},{\"end\":43647,\"start\":43640},{\"end\":43661,\"start\":43655},{\"end\":43995,\"start\":43989},{\"end\":44007,\"start\":44001},{\"end\":44023,\"start\":44015},{\"end\":44039,\"start\":44032},{\"end\":44051,\"start\":44047},{\"end\":44068,\"start\":44063},{\"end\":44082,\"start\":44076},{\"end\":44381,\"start\":44376},{\"end\":44398,\"start\":44391},{\"end\":44410,\"start\":44405},{\"end\":44426,\"start\":44421},{\"end\":44439,\"start\":44435},{\"end\":44441,\"start\":44440},{\"end\":44651,\"start\":44646},{\"end\":44664,\"start\":44659},{\"end\":44677,\"start\":44673},{\"end\":44679,\"start\":44678},{\"end\":44696,\"start\":44690},{\"end\":44714,\"start\":44708},{\"end\":44731,\"start\":44727},{\"end\":44746,\"start\":44740},{\"end\":45192,\"start\":45187},{\"end\":45205,\"start\":45200},{\"end\":45394,\"start\":45387},{\"end\":45406,\"start\":45399},{\"end\":45422,\"start\":45414},{\"end\":45432,\"start\":45428},{\"end\":45742,\"start\":45739},{\"end\":45756,\"start\":45752},{\"end\":45772,\"start\":45764},{\"end\":45787,\"start\":45780},{\"end\":45797,\"start\":45795},{\"end\":46114,\"start\":46111},{\"end\":46131,\"start\":46126},{\"end\":46287,\"start\":46279},{\"end\":46289,\"start\":46288},{\"end\":46303,\"start\":46298},{\"end\":46314,\"start\":46313},{\"end\":46609,\"start\":46605},{\"end\":46622,\"start\":46618},{\"end\":46635,\"start\":46632},{\"end\":46651,\"start\":46644},{\"end\":46666,\"start\":46660},{\"end\":46972,\"start\":46967},{\"end\":46976,\"start\":46973},{\"end\":46997,\"start\":46993},{\"end\":47014,\"start\":47005},{\"end\":47028,\"start\":47021},{\"end\":47044,\"start\":47038},{\"end\":47056,\"start\":47051},{\"end\":47058,\"start\":47057},{\"end\":47347,\"start\":47342},{\"end\":47351,\"start\":47348},{\"end\":47372,\"start\":47368},{\"end\":47389,\"start\":47380},{\"end\":47403,\"start\":47396},{\"end\":47419,\"start\":47413},{\"end\":47431,\"start\":47426},{\"end\":47433,\"start\":47432},{\"end\":47758,\"start\":47757},{\"end\":47770,\"start\":47763},{\"end\":47780,\"start\":47775},{\"end\":47794,\"start\":47788},{\"end\":47801,\"start\":47800},{\"end\":48060,\"start\":48057},{\"end\":48070,\"start\":48068},{\"end\":48082,\"start\":48076},{\"end\":48094,\"start\":48088},{\"end\":48108,\"start\":48102},{\"end\":48110,\"start\":48109},{\"end\":48388,\"start\":48380},{\"end\":48404,\"start\":48397},{\"end\":48415,\"start\":48412},{\"end\":48424,\"start\":48422},{\"end\":48772,\"start\":48768},{\"end\":48979,\"start\":48975},{\"end\":48996,\"start\":48992},{\"end\":49016,\"start\":49008},{\"end\":49018,\"start\":49017},{\"end\":49185,\"start\":49171},{\"end\":49194,\"start\":49190},{\"end\":49206,\"start\":49204},{\"end\":49219,\"start\":49212},{\"end\":49234,\"start\":49227},{\"end\":49535,\"start\":49530},{\"end\":49546,\"start\":49540},{\"end\":49560,\"start\":49553},{\"end\":49569,\"start\":49565},{\"end\":49850,\"start\":49841},{\"end\":49866,\"start\":49862},{\"end\":49886,\"start\":49880},{\"end\":49908,\"start\":49900},{\"end\":49925,\"start\":49920},{\"end\":49940,\"start\":49935},{\"end\":49953,\"start\":49952},{\"end\":49973,\"start\":49967},{\"end\":50243,\"start\":50242},{\"end\":50257,\"start\":50256},{\"end\":50556,\"start\":50550},{\"end\":50566,\"start\":50561},{\"end\":50575,\"start\":50574},{\"end\":50590,\"start\":50584},{\"end\":50835,\"start\":50832},{\"end\":50847,\"start\":50846},{\"end\":51288,\"start\":51284},{\"end\":51300,\"start\":51297},{\"end\":51317,\"start\":51308},{\"end\":51329,\"start\":51325},{\"end\":51342,\"start\":51337},{\"end\":51360,\"start\":51353},{\"end\":51375,\"start\":51369},{\"end\":51391,\"start\":51385},{\"end\":51404,\"start\":51397},{\"end\":51421,\"start\":51417},{\"end\":51822,\"start\":51817},{\"end\":51839,\"start\":51836},{\"end\":51847,\"start\":51840},{\"end\":51862,\"start\":51858},{\"end\":51864,\"start\":51863},{\"end\":51882,\"start\":51875},{\"end\":51892,\"start\":51887},{\"end\":52141,\"start\":52140},{\"end\":52156,\"start\":52149},{\"end\":52167,\"start\":52166},{\"end\":52178,\"start\":52177},{\"end\":52464,\"start\":52460},{\"end\":52481,\"start\":52478},{\"end\":52491,\"start\":52488},{\"end\":52504,\"start\":52496},{\"end\":52520,\"start\":52513},{\"end\":52535,\"start\":52531},{\"end\":52547,\"start\":52540},{\"end\":52561,\"start\":52555},{\"end\":52578,\"start\":52572},{\"end\":52594,\"start\":52587},{\"end\":52615,\"start\":52606},{\"end\":52617,\"start\":52616},{\"end\":52626,\"start\":52624},{\"end\":53024,\"start\":53017},{\"end\":53036,\"start\":53031},{\"end\":53052,\"start\":53044},{\"end\":53071,\"start\":53066},{\"end\":53086,\"start\":53077},{\"end\":53366,\"start\":53365},{\"end\":53378,\"start\":53377},{\"end\":53680,\"start\":53679},{\"end\":53687,\"start\":53686},{\"end\":53699,\"start\":53695},{\"end\":53715,\"start\":53710},{\"end\":53725,\"start\":53724},{\"end\":54058,\"start\":54054},{\"end\":54071,\"start\":54064},{\"end\":54092,\"start\":54085},{\"end\":54107,\"start\":54100},{\"end\":54531,\"start\":54522},{\"end\":54542,\"start\":54541},{\"end\":54555,\"start\":54554},{\"end\":54566,\"start\":54563},{\"end\":54576,\"start\":54575},{\"end\":54837,\"start\":54829},{\"end\":54844,\"start\":54843},{\"end\":55108,\"start\":55104},{\"end\":55127,\"start\":55118},{\"end\":55143,\"start\":55142},{\"end\":55152,\"start\":55151},{\"end\":55160,\"start\":55159},{\"end\":55503,\"start\":55499},{\"end\":55519,\"start\":55513},{\"end\":55537,\"start\":55529},{\"end\":55550,\"start\":55545},{\"end\":55690,\"start\":55686},{\"end\":55706,\"start\":55700},{\"end\":56041,\"start\":56035},{\"end\":56055,\"start\":56051},{\"end\":56069,\"start\":56065},{\"end\":56083,\"start\":56078},{\"end\":56100,\"start\":56095},{\"end\":56113,\"start\":56108},{\"end\":56115,\"start\":56114},{\"end\":56129,\"start\":56123},{\"end\":56143,\"start\":56138},{\"end\":56332,\"start\":56331},{\"end\":56343,\"start\":56339},{\"end\":56345,\"start\":56344},{\"end\":56357,\"start\":56356},{\"end\":56372,\"start\":56365},{\"end\":56715,\"start\":56708},{\"end\":56723,\"start\":56721},{\"end\":56736,\"start\":56730},{\"end\":56745,\"start\":56742},{\"end\":56760,\"start\":56752},{\"end\":56770,\"start\":56768},{\"end\":56994,\"start\":56990},{\"end\":57197,\"start\":57191},{\"end\":57210,\"start\":57202},{\"end\":57224,\"start\":57215},{\"end\":57235,\"start\":57230},{\"end\":57248,\"start\":57241},{\"end\":57265,\"start\":57256},{\"end\":57280,\"start\":57276},{\"end\":57295,\"start\":57290},{\"end\":57584,\"start\":57579},{\"end\":57596,\"start\":57590},{\"end\":57598,\"start\":57597},{\"end\":57615,\"start\":57605},{\"end\":57627,\"start\":57623},{\"end\":57629,\"start\":57628},{\"end\":57958,\"start\":57957},{\"end\":57966,\"start\":57965},{\"end\":57973,\"start\":57972},{\"end\":57979,\"start\":57978},{\"end\":57992,\"start\":57986},{\"end\":58231,\"start\":58224},{\"end\":58245,\"start\":58237},{\"end\":58259,\"start\":58251},{\"end\":58281,\"start\":58275},{\"end\":58297,\"start\":58288},{\"end\":58648,\"start\":58642},{\"end\":58665,\"start\":58656},{\"end\":58677,\"start\":58673},{\"end\":58679,\"start\":58678},{\"end\":58694,\"start\":58689},{\"end\":58935,\"start\":58931},{\"end\":58951,\"start\":58943},{\"end\":58964,\"start\":58956},{\"end\":58974,\"start\":58972},{\"end\":58983,\"start\":58980},{\"end\":58997,\"start\":58991},{\"end\":59006,\"start\":59003},{\"end\":59016,\"start\":59012},{\"end\":59026,\"start\":59021},{\"end\":59036,\"start\":59035},{\"end\":59049,\"start\":59047},{\"end\":59063,\"start\":59054},{\"end\":59394,\"start\":59390},{\"end\":59407,\"start\":59402},{\"end\":59423,\"start\":59415},{\"end\":59436,\"start\":59430},{\"end\":59443,\"start\":59441}]", "bib_author_last_name": "[{\"end\":38685,\"start\":38680},{\"end\":38703,\"start\":38695},{\"end\":38719,\"start\":38712},{\"end\":38944,\"start\":38942},{\"end\":38955,\"start\":38951},{\"end\":38960,\"start\":38957},{\"end\":39111,\"start\":39109},{\"end\":39129,\"start\":39124},{\"end\":39148,\"start\":39142},{\"end\":39394,\"start\":39388},{\"end\":39407,\"start\":39402},{\"end\":39423,\"start\":39416},{\"end\":39441,\"start\":39433},{\"end\":39457,\"start\":39452},{\"end\":39688,\"start\":39683},{\"end\":39706,\"start\":39700},{\"end\":39728,\"start\":39718},{\"end\":39742,\"start\":39738},{\"end\":39762,\"start\":39750},{\"end\":40041,\"start\":40035},{\"end\":40058,\"start\":40053},{\"end\":40076,\"start\":40068},{\"end\":40093,\"start\":40086},{\"end\":40113,\"start\":40105},{\"end\":40131,\"start\":40122},{\"end\":40425,\"start\":40421},{\"end\":40439,\"start\":40432},{\"end\":40452,\"start\":40447},{\"end\":40464,\"start\":40462},{\"end\":40476,\"start\":40473},{\"end\":40488,\"start\":40484},{\"end\":40504,\"start\":40495},{\"end\":40805,\"start\":40801},{\"end\":40816,\"start\":40814},{\"end\":40828,\"start\":40826},{\"end\":40839,\"start\":40834},{\"end\":40853,\"start\":40848},{\"end\":40862,\"start\":40859},{\"end\":40871,\"start\":40866},{\"end\":40881,\"start\":40878},{\"end\":41180,\"start\":41177},{\"end\":41193,\"start\":41184},{\"end\":41417,\"start\":41414},{\"end\":41428,\"start\":41424},{\"end\":41442,\"start\":41439},{\"end\":41455,\"start\":41451},{\"end\":41649,\"start\":41633},{\"end\":41665,\"start\":41661},{\"end\":41677,\"start\":41673},{\"end\":41688,\"start\":41679},{\"end\":41696,\"start\":41692},{\"end\":41700,\"start\":41698},{\"end\":41713,\"start\":41702},{\"end\":42048,\"start\":42044},{\"end\":42062,\"start\":42057},{\"end\":42077,\"start\":42073},{\"end\":42092,\"start\":42086},{\"end\":42096,\"start\":42094},{\"end\":42370,\"start\":42366},{\"end\":42380,\"start\":42376},{\"end\":42396,\"start\":42390},{\"end\":42407,\"start\":42405},{\"end\":42415,\"start\":42413},{\"end\":42427,\"start\":42420},{\"end\":42700,\"start\":42694},{\"end\":42716,\"start\":42711},{\"end\":42728,\"start\":42725},{\"end\":42753,\"start\":42749},{\"end\":43143,\"start\":43132},{\"end\":43156,\"start\":43151},{\"end\":43178,\"start\":43168},{\"end\":43196,\"start\":43185},{\"end\":43210,\"start\":43206},{\"end\":43230,\"start\":43219},{\"end\":43248,\"start\":43240},{\"end\":43267,\"start\":43259},{\"end\":43282,\"start\":43275},{\"end\":43297,\"start\":43292},{\"end\":43638,\"start\":43635},{\"end\":43653,\"start\":43648},{\"end\":43668,\"start\":43662},{\"end\":43999,\"start\":43996},{\"end\":44013,\"start\":44008},{\"end\":44030,\"start\":44024},{\"end\":44045,\"start\":44040},{\"end\":44061,\"start\":44052},{\"end\":44074,\"start\":44069},{\"end\":44089,\"start\":44083},{\"end\":44389,\"start\":44382},{\"end\":44403,\"start\":44399},{\"end\":44419,\"start\":44411},{\"end\":44433,\"start\":44427},{\"end\":44449,\"start\":44442},{\"end\":44657,\"start\":44652},{\"end\":44671,\"start\":44665},{\"end\":44688,\"start\":44680},{\"end\":44706,\"start\":44697},{\"end\":44725,\"start\":44715},{\"end\":44738,\"start\":44732},{\"end\":44754,\"start\":44747},{\"end\":45198,\"start\":45193},{\"end\":45213,\"start\":45206},{\"end\":45397,\"start\":45395},{\"end\":45412,\"start\":45407},{\"end\":45426,\"start\":45423},{\"end\":45436,\"start\":45433},{\"end\":45750,\"start\":45743},{\"end\":45762,\"start\":45757},{\"end\":45778,\"start\":45773},{\"end\":45793,\"start\":45788},{\"end\":45801,\"start\":45798},{\"end\":45805,\"start\":45803},{\"end\":46124,\"start\":46115},{\"end\":46138,\"start\":46132},{\"end\":46296,\"start\":46290},{\"end\":46311,\"start\":46304},{\"end\":46319,\"start\":46315},{\"end\":46616,\"start\":46610},{\"end\":46630,\"start\":46623},{\"end\":46642,\"start\":46636},{\"end\":46658,\"start\":46652},{\"end\":46674,\"start\":46667},{\"end\":46682,\"start\":46676},{\"end\":46991,\"start\":46977},{\"end\":47003,\"start\":46998},{\"end\":47019,\"start\":47015},{\"end\":47036,\"start\":47029},{\"end\":47049,\"start\":47045},{\"end\":47065,\"start\":47059},{\"end\":47075,\"start\":47067},{\"end\":47366,\"start\":47352},{\"end\":47378,\"start\":47373},{\"end\":47394,\"start\":47390},{\"end\":47411,\"start\":47404},{\"end\":47424,\"start\":47420},{\"end\":47440,\"start\":47434},{\"end\":47450,\"start\":47442},{\"end\":47761,\"start\":47759},{\"end\":47773,\"start\":47771},{\"end\":47786,\"start\":47781},{\"end\":47798,\"start\":47795},{\"end\":47805,\"start\":47802},{\"end\":48066,\"start\":48061},{\"end\":48074,\"start\":48071},{\"end\":48086,\"start\":48083},{\"end\":48100,\"start\":48095},{\"end\":48121,\"start\":48111},{\"end\":48395,\"start\":48389},{\"end\":48410,\"start\":48405},{\"end\":48420,\"start\":48416},{\"end\":48432,\"start\":48425},{\"end\":48783,\"start\":48773},{\"end\":48990,\"start\":48980},{\"end\":49006,\"start\":48997},{\"end\":49025,\"start\":49019},{\"end\":49188,\"start\":49186},{\"end\":49202,\"start\":49195},{\"end\":49210,\"start\":49207},{\"end\":49225,\"start\":49220},{\"end\":49240,\"start\":49235},{\"end\":49538,\"start\":49536},{\"end\":49551,\"start\":49547},{\"end\":49563,\"start\":49561},{\"end\":49574,\"start\":49570},{\"end\":49860,\"start\":49851},{\"end\":49878,\"start\":49867},{\"end\":49898,\"start\":49887},{\"end\":49918,\"start\":49909},{\"end\":49933,\"start\":49926},{\"end\":49950,\"start\":49941},{\"end\":49965,\"start\":49954},{\"end\":49978,\"start\":49974},{\"end\":50254,\"start\":50244},{\"end\":50264,\"start\":50258},{\"end\":50559,\"start\":50557},{\"end\":50572,\"start\":50567},{\"end\":50582,\"start\":50576},{\"end\":50594,\"start\":50591},{\"end\":50844,\"start\":50836},{\"end\":50857,\"start\":50848},{\"end\":51295,\"start\":51289},{\"end\":51306,\"start\":51301},{\"end\":51323,\"start\":51318},{\"end\":51335,\"start\":51330},{\"end\":51351,\"start\":51343},{\"end\":51367,\"start\":51361},{\"end\":51383,\"start\":51376},{\"end\":51395,\"start\":51392},{\"end\":51415,\"start\":51405},{\"end\":51428,\"start\":51422},{\"end\":51834,\"start\":51823},{\"end\":51856,\"start\":51848},{\"end\":51873,\"start\":51865},{\"end\":51885,\"start\":51883},{\"end\":51899,\"start\":51893},{\"end\":52147,\"start\":52142},{\"end\":52164,\"start\":52157},{\"end\":52175,\"start\":52168},{\"end\":52186,\"start\":52179},{\"end\":52476,\"start\":52465},{\"end\":52486,\"start\":52482},{\"end\":52494,\"start\":52492},{\"end\":52511,\"start\":52505},{\"end\":52529,\"start\":52521},{\"end\":52538,\"start\":52536},{\"end\":52553,\"start\":52548},{\"end\":52570,\"start\":52562},{\"end\":52585,\"start\":52579},{\"end\":52604,\"start\":52595},{\"end\":52622,\"start\":52618},{\"end\":52634,\"start\":52627},{\"end\":53029,\"start\":53025},{\"end\":53042,\"start\":53037},{\"end\":53064,\"start\":53053},{\"end\":53075,\"start\":53072},{\"end\":53091,\"start\":53087},{\"end\":53375,\"start\":53367},{\"end\":53388,\"start\":53379},{\"end\":53684,\"start\":53681},{\"end\":53693,\"start\":53688},{\"end\":53708,\"start\":53700},{\"end\":53722,\"start\":53716},{\"end\":53732,\"start\":53726},{\"end\":54062,\"start\":54059},{\"end\":54083,\"start\":54072},{\"end\":54098,\"start\":54093},{\"end\":54113,\"start\":54108},{\"end\":54539,\"start\":54532},{\"end\":54552,\"start\":54543},{\"end\":54561,\"start\":54556},{\"end\":54573,\"start\":54567},{\"end\":54582,\"start\":54577},{\"end\":54841,\"start\":54838},{\"end\":54849,\"start\":54845},{\"end\":54853,\"start\":54851},{\"end\":54867,\"start\":54855},{\"end\":55116,\"start\":55109},{\"end\":55140,\"start\":55128},{\"end\":55149,\"start\":55144},{\"end\":55157,\"start\":55153},{\"end\":55166,\"start\":55161},{\"end\":55511,\"start\":55504},{\"end\":55527,\"start\":55520},{\"end\":55543,\"start\":55538},{\"end\":55556,\"start\":55551},{\"end\":55698,\"start\":55691},{\"end\":55714,\"start\":55707},{\"end\":56049,\"start\":56042},{\"end\":56063,\"start\":56056},{\"end\":56076,\"start\":56070},{\"end\":56093,\"start\":56084},{\"end\":56106,\"start\":56101},{\"end\":56121,\"start\":56116},{\"end\":56136,\"start\":56130},{\"end\":56154,\"start\":56144},{\"end\":56337,\"start\":56333},{\"end\":56354,\"start\":56346},{\"end\":56363,\"start\":56358},{\"end\":56375,\"start\":56373},{\"end\":56719,\"start\":56716},{\"end\":56728,\"start\":56724},{\"end\":56740,\"start\":56737},{\"end\":56750,\"start\":56746},{\"end\":56766,\"start\":56761},{\"end\":56775,\"start\":56771},{\"end\":57003,\"start\":56995},{\"end\":57200,\"start\":57198},{\"end\":57213,\"start\":57211},{\"end\":57228,\"start\":57225},{\"end\":57239,\"start\":57236},{\"end\":57254,\"start\":57249},{\"end\":57274,\"start\":57266},{\"end\":57288,\"start\":57281},{\"end\":57301,\"start\":57296},{\"end\":57588,\"start\":57585},{\"end\":57603,\"start\":57599},{\"end\":57621,\"start\":57616},{\"end\":57632,\"start\":57630},{\"end\":57963,\"start\":57959},{\"end\":57970,\"start\":57967},{\"end\":57976,\"start\":57974},{\"end\":57984,\"start\":57980},{\"end\":57997,\"start\":57993},{\"end\":58235,\"start\":58232},{\"end\":58249,\"start\":58246},{\"end\":58273,\"start\":58260},{\"end\":58286,\"start\":58282},{\"end\":58302,\"start\":58298},{\"end\":58307,\"start\":58304},{\"end\":58315,\"start\":58309},{\"end\":58654,\"start\":58649},{\"end\":58671,\"start\":58666},{\"end\":58687,\"start\":58680},{\"end\":58704,\"start\":58695},{\"end\":58941,\"start\":58936},{\"end\":58954,\"start\":58952},{\"end\":58970,\"start\":58965},{\"end\":58978,\"start\":58975},{\"end\":58989,\"start\":58984},{\"end\":59001,\"start\":58998},{\"end\":59010,\"start\":59007},{\"end\":59019,\"start\":59017},{\"end\":59033,\"start\":59027},{\"end\":59045,\"start\":59037},{\"end\":59052,\"start\":59050},{\"end\":59069,\"start\":59064},{\"end\":59400,\"start\":59395},{\"end\":59413,\"start\":59408},{\"end\":59428,\"start\":59424},{\"end\":59439,\"start\":59437},{\"end\":59448,\"start\":59444}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2006.00555\",\"id\":\"b0\"},\"end\":38903,\"start\":38611},{\"attributes\":{\"doi\":\"arXiv:1709.01507\",\"id\":\"b1\"},\"end\":39095,\"start\":38905},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b2\"},\"end\":39315,\"start\":39097},{\"attributes\":{\"doi\":\"arXiv:1902.05509\",\"id\":\"b3\"},\"end\":39673,\"start\":39317},{\"attributes\":{\"doi\":\"arXiv:2006.07159\",\"id\":\"b4\"},\"end\":39978,\"start\":39675},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218889832},\"end\":40378,\"start\":39980},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":219781060},\"end\":40736,\"start\":40380},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":216080982},\"end\":41128,\"start\":40738},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":203642130},\"end\":41361,\"start\":41130},{\"attributes\":{\"doi\":\"arXiv:2008.03673\",\"id\":\"b9\"},\"end\":41624,\"start\":41363},{\"attributes\":{\"doi\":\"arXiv:1805.09501\",\"id\":\"b10\"},\"end\":41960,\"start\":41626},{\"attributes\":{\"doi\":\"arXiv:1909.13719\",\"id\":\"b11\"},\"end\":42307,\"start\":41962},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57246310},\"end\":42686,\"start\":42309},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b13\"},\"end\":43047,\"start\":42688},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b14\"},\"end\":43626,\"start\":43049},{\"attributes\":{\"doi\":\"arXiv:1909.11556\",\"id\":\"b15\"},\"end\":43923,\"start\":43628},{\"attributes\":{\"doi\":\"arXiv:2004.07320\",\"id\":\"b16\"},\"end\":44329,\"start\":43925},{\"attributes\":{\"doi\":\"arXiv:1705.03122\",\"id\":\"b17\"},\"end\":44644,\"start\":44331},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b18\"},\"end\":45115,\"start\":44646},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3710662},\"end\":45339,\"start\":45117},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206594692},\"end\":45662,\"start\":45341},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54445469},\"end\":46072,\"start\":45664},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b22\"},\"end\":46277,\"start\":46074},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b23\"},\"end\":46529,\"start\":46279},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":219963404},\"end\":46965,\"start\":46531},{\"attributes\":{\"doi\":\"arXiv:1707.06642\",\"id\":\"b25\"},\"end\":47340,\"start\":46967},{\"attributes\":{\"doi\":\"arXiv:1707.06642\",\"id\":\"b26\"},\"end\":47715,\"start\":47342},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":37158713},\"end\":48018,\"start\":47717},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6773885},\"end\":48319,\"start\":48020},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14342571},\"end\":48711,\"start\":48321},{\"attributes\":{\"id\":\"b30\"},\"end\":48908,\"start\":48713},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195908774},\"end\":49169,\"start\":48910},{\"attributes\":{\"doi\":\"arXiv:1908.03557\",\"id\":\"b32\"},\"end\":49528,\"start\":49171},{\"attributes\":{\"id\":\"b33\"},\"end\":49795,\"start\":49530},{\"attributes\":{\"doi\":\"arXiv:2006.15055\",\"id\":\"b34\"},\"end\":50240,\"start\":49797},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b35\"},\"end\":50451,\"start\":50242},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":199453025},\"end\":50766,\"start\":50453},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15193013},\"end\":51212,\"start\":50768},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":202786778},\"end\":51782,\"start\":51214},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":214714446},\"end\":52138,\"start\":51784},{\"attributes\":{\"doi\":\"arXiv:1902.10811\",\"id\":\"b40\"},\"end\":52407,\"start\":52140},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2930547},\"end\":52961,\"start\":52409},{\"attributes\":{\"doi\":\"arXiv:2010.03019\",\"id\":\"b42\"},\"end\":53295,\"start\":52963},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14124313},\"end\":53604,\"start\":53297},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":102483628},\"end\":53984,\"start\":53606},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":6842201},\"end\":54461,\"start\":53986},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206593880},\"end\":54827,\"start\":54463},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b47\"},\"end\":55102,\"start\":54829},{\"attributes\":{\"doi\":\"arXiv:2011.12982\",\"id\":\"b48\"},\"end\":55451,\"start\":55104},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":189928444},\"end\":55684,\"start\":55453},{\"attributes\":{\"doi\":\"arXiv:2003.08237\",\"id\":\"b50\"},\"end\":56006,\"start\":55686},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":13756489},\"end\":56329,\"start\":56008},{\"attributes\":{\"id\":\"b52\"},\"end\":56599,\"start\":56331},{\"attributes\":{\"id\":\"b53\"},\"end\":56966,\"start\":56601},{\"attributes\":{\"id\":\"b54\"},\"end\":57099,\"start\":56968},{\"attributes\":{\"doi\":\"arXiv:2006.03677\",\"id\":\"b55\"},\"end\":57577,\"start\":57101},{\"attributes\":{\"doi\":\"arXiv:1911.04252\",\"id\":\"b56\"},\"end\":57897,\"start\":57579},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":202889259},\"end\":58222,\"start\":57899},{\"attributes\":{\"doi\":\"arXiv:1905.04899\",\"id\":\"b58\"},\"end\":58640,\"start\":58224},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b59\"},\"end\":58929,\"start\":58642},{\"attributes\":{\"doi\":\"arXiv:2004.08955\",\"id\":\"b60\"},\"end\":59354,\"start\":58931},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":2035600},\"end\":59570,\"start\":59356}]", "bib_title": "[{\"end\":40025,\"start\":39980},{\"end\":40414,\"start\":40380},{\"end\":40790,\"start\":40738},{\"end\":41171,\"start\":41130},{\"end\":42360,\"start\":42309},{\"end\":45185,\"start\":45117},{\"end\":45385,\"start\":45341},{\"end\":45737,\"start\":45664},{\"end\":46603,\"start\":46531},{\"end\":47755,\"start\":47717},{\"end\":48055,\"start\":48020},{\"end\":48378,\"start\":48321},{\"end\":48973,\"start\":48910},{\"end\":50548,\"start\":50453},{\"end\":50830,\"start\":50768},{\"end\":51282,\"start\":51214},{\"end\":51815,\"start\":51784},{\"end\":52458,\"start\":52409},{\"end\":53363,\"start\":53297},{\"end\":53677,\"start\":53606},{\"end\":54052,\"start\":53986},{\"end\":54520,\"start\":54463},{\"end\":55497,\"start\":55453},{\"end\":56033,\"start\":56008},{\"end\":57955,\"start\":57899},{\"end\":59388,\"start\":59356}]", "bib_author": "[{\"end\":38687,\"start\":38673},{\"end\":38705,\"start\":38687},{\"end\":38721,\"start\":38705},{\"end\":38946,\"start\":38938},{\"end\":38957,\"start\":38946},{\"end\":38962,\"start\":38957},{\"end\":39113,\"start\":39099},{\"end\":39131,\"start\":39113},{\"end\":39150,\"start\":39131},{\"end\":39396,\"start\":39382},{\"end\":39409,\"start\":39396},{\"end\":39425,\"start\":39409},{\"end\":39443,\"start\":39425},{\"end\":39459,\"start\":39443},{\"end\":39690,\"start\":39677},{\"end\":39708,\"start\":39690},{\"end\":39730,\"start\":39708},{\"end\":39744,\"start\":39730},{\"end\":39764,\"start\":39744},{\"end\":40043,\"start\":40027},{\"end\":40060,\"start\":40043},{\"end\":40078,\"start\":40060},{\"end\":40095,\"start\":40078},{\"end\":40115,\"start\":40095},{\"end\":40133,\"start\":40115},{\"end\":40427,\"start\":40416},{\"end\":40441,\"start\":40427},{\"end\":40454,\"start\":40441},{\"end\":40466,\"start\":40454},{\"end\":40478,\"start\":40466},{\"end\":40490,\"start\":40478},{\"end\":40506,\"start\":40490},{\"end\":40807,\"start\":40792},{\"end\":40818,\"start\":40807},{\"end\":40830,\"start\":40818},{\"end\":40841,\"start\":40830},{\"end\":40855,\"start\":40841},{\"end\":40864,\"start\":40855},{\"end\":40873,\"start\":40864},{\"end\":40883,\"start\":40873},{\"end\":41182,\"start\":41173},{\"end\":41195,\"start\":41182},{\"end\":41419,\"start\":41412},{\"end\":41430,\"start\":41419},{\"end\":41444,\"start\":41430},{\"end\":41457,\"start\":41444},{\"end\":41651,\"start\":41626},{\"end\":41667,\"start\":41651},{\"end\":41679,\"start\":41667},{\"end\":41690,\"start\":41679},{\"end\":41698,\"start\":41690},{\"end\":41702,\"start\":41698},{\"end\":41715,\"start\":41702},{\"end\":42050,\"start\":42042},{\"end\":42064,\"start\":42050},{\"end\":42079,\"start\":42064},{\"end\":42094,\"start\":42079},{\"end\":42098,\"start\":42094},{\"end\":42372,\"start\":42362},{\"end\":42382,\"start\":42372},{\"end\":42398,\"start\":42382},{\"end\":42409,\"start\":42398},{\"end\":42417,\"start\":42409},{\"end\":42429,\"start\":42417},{\"end\":42702,\"start\":42688},{\"end\":42718,\"start\":42702},{\"end\":42730,\"start\":42718},{\"end\":42755,\"start\":42730},{\"end\":43145,\"start\":43125},{\"end\":43158,\"start\":43145},{\"end\":43180,\"start\":43158},{\"end\":43198,\"start\":43180},{\"end\":43212,\"start\":43198},{\"end\":43232,\"start\":43212},{\"end\":43250,\"start\":43232},{\"end\":43269,\"start\":43250},{\"end\":43284,\"start\":43269},{\"end\":43299,\"start\":43284},{\"end\":43640,\"start\":43628},{\"end\":43655,\"start\":43640},{\"end\":43670,\"start\":43655},{\"end\":44001,\"start\":43989},{\"end\":44015,\"start\":44001},{\"end\":44032,\"start\":44015},{\"end\":44047,\"start\":44032},{\"end\":44063,\"start\":44047},{\"end\":44076,\"start\":44063},{\"end\":44091,\"start\":44076},{\"end\":44391,\"start\":44376},{\"end\":44405,\"start\":44391},{\"end\":44421,\"start\":44405},{\"end\":44435,\"start\":44421},{\"end\":44451,\"start\":44435},{\"end\":44659,\"start\":44646},{\"end\":44673,\"start\":44659},{\"end\":44690,\"start\":44673},{\"end\":44708,\"start\":44690},{\"end\":44727,\"start\":44708},{\"end\":44740,\"start\":44727},{\"end\":44756,\"start\":44740},{\"end\":45200,\"start\":45187},{\"end\":45215,\"start\":45200},{\"end\":45399,\"start\":45387},{\"end\":45414,\"start\":45399},{\"end\":45428,\"start\":45414},{\"end\":45438,\"start\":45428},{\"end\":45752,\"start\":45739},{\"end\":45764,\"start\":45752},{\"end\":45780,\"start\":45764},{\"end\":45795,\"start\":45780},{\"end\":45803,\"start\":45795},{\"end\":45807,\"start\":45803},{\"end\":46126,\"start\":46111},{\"end\":46140,\"start\":46126},{\"end\":46298,\"start\":46279},{\"end\":46313,\"start\":46298},{\"end\":46321,\"start\":46313},{\"end\":46618,\"start\":46605},{\"end\":46632,\"start\":46618},{\"end\":46644,\"start\":46632},{\"end\":46660,\"start\":46644},{\"end\":46676,\"start\":46660},{\"end\":46684,\"start\":46676},{\"end\":46993,\"start\":46967},{\"end\":47005,\"start\":46993},{\"end\":47021,\"start\":47005},{\"end\":47038,\"start\":47021},{\"end\":47051,\"start\":47038},{\"end\":47067,\"start\":47051},{\"end\":47077,\"start\":47067},{\"end\":47368,\"start\":47342},{\"end\":47380,\"start\":47368},{\"end\":47396,\"start\":47380},{\"end\":47413,\"start\":47396},{\"end\":47426,\"start\":47413},{\"end\":47442,\"start\":47426},{\"end\":47452,\"start\":47442},{\"end\":47763,\"start\":47757},{\"end\":47775,\"start\":47763},{\"end\":47788,\"start\":47775},{\"end\":47800,\"start\":47788},{\"end\":47807,\"start\":47800},{\"end\":48068,\"start\":48057},{\"end\":48076,\"start\":48068},{\"end\":48088,\"start\":48076},{\"end\":48102,\"start\":48088},{\"end\":48123,\"start\":48102},{\"end\":48397,\"start\":48380},{\"end\":48412,\"start\":48397},{\"end\":48422,\"start\":48412},{\"end\":48434,\"start\":48422},{\"end\":48785,\"start\":48768},{\"end\":48992,\"start\":48975},{\"end\":49008,\"start\":48992},{\"end\":49027,\"start\":49008},{\"end\":49190,\"start\":49171},{\"end\":49204,\"start\":49190},{\"end\":49212,\"start\":49204},{\"end\":49227,\"start\":49212},{\"end\":49242,\"start\":49227},{\"end\":49540,\"start\":49530},{\"end\":49553,\"start\":49540},{\"end\":49565,\"start\":49553},{\"end\":49576,\"start\":49565},{\"end\":49862,\"start\":49841},{\"end\":49880,\"start\":49862},{\"end\":49900,\"start\":49880},{\"end\":49920,\"start\":49900},{\"end\":49935,\"start\":49920},{\"end\":49952,\"start\":49935},{\"end\":49967,\"start\":49952},{\"end\":49980,\"start\":49967},{\"end\":50256,\"start\":50242},{\"end\":50266,\"start\":50256},{\"end\":50561,\"start\":50550},{\"end\":50574,\"start\":50561},{\"end\":50584,\"start\":50574},{\"end\":50596,\"start\":50584},{\"end\":50846,\"start\":50832},{\"end\":50859,\"start\":50846},{\"end\":51297,\"start\":51284},{\"end\":51308,\"start\":51297},{\"end\":51325,\"start\":51308},{\"end\":51337,\"start\":51325},{\"end\":51353,\"start\":51337},{\"end\":51369,\"start\":51353},{\"end\":51385,\"start\":51369},{\"end\":51397,\"start\":51385},{\"end\":51417,\"start\":51397},{\"end\":51430,\"start\":51417},{\"end\":51836,\"start\":51817},{\"end\":51858,\"start\":51836},{\"end\":51875,\"start\":51858},{\"end\":51887,\"start\":51875},{\"end\":51901,\"start\":51887},{\"end\":52149,\"start\":52140},{\"end\":52166,\"start\":52149},{\"end\":52177,\"start\":52166},{\"end\":52188,\"start\":52177},{\"end\":52478,\"start\":52460},{\"end\":52488,\"start\":52478},{\"end\":52496,\"start\":52488},{\"end\":52513,\"start\":52496},{\"end\":52531,\"start\":52513},{\"end\":52540,\"start\":52531},{\"end\":52555,\"start\":52540},{\"end\":52572,\"start\":52555},{\"end\":52587,\"start\":52572},{\"end\":52606,\"start\":52587},{\"end\":52624,\"start\":52606},{\"end\":52636,\"start\":52624},{\"end\":53031,\"start\":53017},{\"end\":53044,\"start\":53031},{\"end\":53066,\"start\":53044},{\"end\":53077,\"start\":53066},{\"end\":53093,\"start\":53077},{\"end\":53377,\"start\":53365},{\"end\":53390,\"start\":53377},{\"end\":53686,\"start\":53679},{\"end\":53695,\"start\":53686},{\"end\":53710,\"start\":53695},{\"end\":53724,\"start\":53710},{\"end\":53734,\"start\":53724},{\"end\":54064,\"start\":54054},{\"end\":54085,\"start\":54064},{\"end\":54100,\"start\":54085},{\"end\":54115,\"start\":54100},{\"end\":54541,\"start\":54522},{\"end\":54554,\"start\":54541},{\"end\":54563,\"start\":54554},{\"end\":54575,\"start\":54563},{\"end\":54584,\"start\":54575},{\"end\":54843,\"start\":54829},{\"end\":54851,\"start\":54843},{\"end\":54855,\"start\":54851},{\"end\":54869,\"start\":54855},{\"end\":55118,\"start\":55104},{\"end\":55142,\"start\":55118},{\"end\":55151,\"start\":55142},{\"end\":55159,\"start\":55151},{\"end\":55168,\"start\":55159},{\"end\":55513,\"start\":55499},{\"end\":55529,\"start\":55513},{\"end\":55545,\"start\":55529},{\"end\":55558,\"start\":55545},{\"end\":55700,\"start\":55686},{\"end\":55716,\"start\":55700},{\"end\":56051,\"start\":56035},{\"end\":56065,\"start\":56051},{\"end\":56078,\"start\":56065},{\"end\":56095,\"start\":56078},{\"end\":56108,\"start\":56095},{\"end\":56123,\"start\":56108},{\"end\":56138,\"start\":56123},{\"end\":56156,\"start\":56138},{\"end\":56339,\"start\":56331},{\"end\":56356,\"start\":56339},{\"end\":56365,\"start\":56356},{\"end\":56377,\"start\":56365},{\"end\":56721,\"start\":56708},{\"end\":56730,\"start\":56721},{\"end\":56742,\"start\":56730},{\"end\":56752,\"start\":56742},{\"end\":56768,\"start\":56752},{\"end\":56777,\"start\":56768},{\"end\":57005,\"start\":56990},{\"end\":57202,\"start\":57191},{\"end\":57215,\"start\":57202},{\"end\":57230,\"start\":57215},{\"end\":57241,\"start\":57230},{\"end\":57256,\"start\":57241},{\"end\":57276,\"start\":57256},{\"end\":57290,\"start\":57276},{\"end\":57303,\"start\":57290},{\"end\":57590,\"start\":57579},{\"end\":57605,\"start\":57590},{\"end\":57623,\"start\":57605},{\"end\":57634,\"start\":57623},{\"end\":57965,\"start\":57957},{\"end\":57972,\"start\":57965},{\"end\":57978,\"start\":57972},{\"end\":57986,\"start\":57978},{\"end\":57999,\"start\":57986},{\"end\":58237,\"start\":58224},{\"end\":58251,\"start\":58237},{\"end\":58275,\"start\":58251},{\"end\":58288,\"start\":58275},{\"end\":58304,\"start\":58288},{\"end\":58309,\"start\":58304},{\"end\":58317,\"start\":58309},{\"end\":58656,\"start\":58642},{\"end\":58673,\"start\":58656},{\"end\":58689,\"start\":58673},{\"end\":58706,\"start\":58689},{\"end\":58943,\"start\":58931},{\"end\":58956,\"start\":58943},{\"end\":58972,\"start\":58956},{\"end\":58980,\"start\":58972},{\"end\":58991,\"start\":58980},{\"end\":59003,\"start\":58991},{\"end\":59012,\"start\":59003},{\"end\":59021,\"start\":59012},{\"end\":59035,\"start\":59021},{\"end\":59047,\"start\":59035},{\"end\":59054,\"start\":59047},{\"end\":59071,\"start\":59054},{\"end\":59402,\"start\":59390},{\"end\":59415,\"start\":59402},{\"end\":59430,\"start\":59415},{\"end\":59441,\"start\":59430},{\"end\":59450,\"start\":59441}]", "bib_venue": "[{\"end\":51018,\"start\":50947},{\"end\":54236,\"start\":54184},{\"end\":38671,\"start\":38611},{\"end\":38936,\"start\":38905},{\"end\":39380,\"start\":39317},{\"end\":40171,\"start\":40133},{\"end\":40550,\"start\":40506},{\"end\":40921,\"start\":40883},{\"end\":41238,\"start\":41195},{\"end\":41410,\"start\":41363},{\"end\":41771,\"start\":41731},{\"end\":42040,\"start\":41962},{\"end\":42482,\"start\":42429},{\"end\":42844,\"start\":42771},{\"end\":43123,\"start\":43049},{\"end\":43746,\"start\":43686},{\"end\":43987,\"start\":43925},{\"end\":44374,\"start\":44331},{\"end\":44860,\"start\":44772},{\"end\":45219,\"start\":45215},{\"end\":45491,\"start\":45438},{\"end\":45860,\"start\":45807},{\"end\":46109,\"start\":46074},{\"end\":46381,\"start\":46337},{\"end\":46737,\"start\":46684},{\"end\":47131,\"start\":47093},{\"end\":47506,\"start\":47468},{\"end\":47860,\"start\":47807},{\"end\":48161,\"start\":48123},{\"end\":48502,\"start\":48434},{\"end\":48766,\"start\":48713},{\"end\":49031,\"start\":49027},{\"end\":49327,\"start\":49258},{\"end\":49656,\"start\":49576},{\"end\":49839,\"start\":49797},{\"end\":50324,\"start\":50282},{\"end\":50600,\"start\":50596},{\"end\":50945,\"start\":50859},{\"end\":51479,\"start\":51430},{\"end\":51954,\"start\":51901},{\"end\":52250,\"start\":52204},{\"end\":52676,\"start\":52636},{\"end\":53015,\"start\":52963},{\"end\":53442,\"start\":53390},{\"end\":53787,\"start\":53734},{\"end\":54182,\"start\":54115},{\"end\":54637,\"start\":54584},{\"end\":54943,\"start\":54885},{\"end\":55254,\"start\":55184},{\"end\":55562,\"start\":55558},{\"end\":55826,\"start\":55732},{\"end\":56160,\"start\":56156},{\"end\":56457,\"start\":56377},{\"end\":56706,\"start\":56601},{\"end\":56988,\"start\":56968},{\"end\":57189,\"start\":57101},{\"end\":57714,\"start\":57650},{\"end\":58052,\"start\":57999},{\"end\":58410,\"start\":58333},{\"end\":58763,\"start\":58722},{\"end\":59120,\"start\":59087},{\"end\":59454,\"start\":59450}]"}}}, "year": 2023, "month": 12, "day": 17}