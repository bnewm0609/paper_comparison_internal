{"id": 253098004, "updated": "2023-11-21 22:27:59.156", "metadata": {"title": "NVIDIA FLARE: Federated Learning from Simulation to Real-World", "authors": "[{\"first\":\"Holger\",\"last\":\"Roth\",\"middle\":[\"R.\"]},{\"first\":\"Yan\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Yuhong\",\"last\":\"Wen\",\"middle\":[]},{\"first\":\"Isaac\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Ziyue\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yuan-Ting\",\"last\":\"Hsieh\",\"middle\":[]},{\"first\":\"Kristopher\",\"last\":\"Kersten\",\"middle\":[]},{\"first\":\"Ahmed\",\"last\":\"Harouni\",\"middle\":[]},{\"first\":\"Can\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Zhihong\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Wenqi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Andriy\",\"last\":\"Myronenko\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Sean\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Nicola\",\"last\":\"Rieke\",\"middle\":[]},{\"first\":\"Abood\",\"last\":\"Quraini\",\"middle\":[]},{\"first\":\"Chester\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Daguang\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Nic\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Prerna\",\"last\":\"Dogra\",\"middle\":[]},{\"first\":\"Mona\",\"last\":\"Flores\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Feng\",\"middle\":[]}]", "venue": "IEEE Data Eng. Bull., Vol. 46, No. 1, 2023", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Federated learning (FL) enables building robust and generalizable AI models by leveraging diverse datasets from multiple collaborators without centralizing the data. We created NVIDIA FLARE as an open-source software development kit (SDK) to make it easier for data scientists to use FL in their research and real-world applications. The SDK includes solutions for state-of-the-art FL algorithms and federated machine learning approaches, which facilitate building workflows for distributed learning across enterprises and enable platform developers to create a secure, privacy-preserving offering for multiparty collaboration utilizing homomorphic encryption or differential privacy. The SDK is a lightweight, flexible, and scalable Python package. It allows researchers to apply their data science workflows in any training libraries (PyTorch, TensorFlow, XGBoost, or even NumPy) in real-world FL settings. This paper introduces the key design principles of NVFlare and illustrates some use cases (e.g., COVID analysis) with customizable FL workflows that implement different privacy-preserving algorithms. Code is available at https://github.com/NVIDIA/NVFlare.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/debu/RothCWY0HKHZL0023", "doi": "10.48550/arxiv.2210.13291"}}, "content": {"source": {"pdf_hash": "04315fbe6554edc9b5d76c99f64b8d1a6f26f7af", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.13291v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fb70677040a0e84917a41d8aba2f7ec6f5006b7d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/04315fbe6554edc9b5d76c99f64b8d1a6f26f7af.txt", "contents": "\nNVIDIA FLARE: Federated Learning from Simulation to Real-World\n\n\nHolger R Roth \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nYan Cheng \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nYuhong Wen \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nIsaac Yang \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nZiyue Xu \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nYuan-Ting Hsieh \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nKristopher Kersten \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nAhmed Harouni \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nCan Zhao \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nKevin Lu \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nZhihong Zhang \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nWenqi Li \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nAndriy Myronenko \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nDong Yang \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nSean Yang \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nNicola Rieke \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nAbood Quraini \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nChester Chen \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nDaguang Xu \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nNic Ma \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nPrerna Dogra \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nMona Flores \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nAndrew Feng \nNVIDIA Corporation * Shanghai\nMunich, Bethesda, Santa ClaraChina, Germany, USA\n\nNVIDIA FLARE: Federated Learning from Simulation to Real-World\n\nFederated learning (FL) enables building robust and generalizable AI models by leveraging diverse datasets from multiple collaborators without centralizing the data. We created NVIDIA FLARE 1 as an open-source software development kit (SDK) to make it easier for data scientists to use FL in their research and realworld applications. The SDK includes solutions for state-of-the-art FL algorithms and federated machine learning approaches, which facilitate building workflows for distributed learning across enterprises and enable platform developers to create a secure, privacy-preserving offering for multiparty collaboration utilizing homomorphic encryption or differential privacy. The SDK is a lightweight, flexible, and scalable Python package. It allows researchers to apply their data science workflows in any training libraries (PyTorch, TensorFlow, XGBoost, or even NumPy) in real-world FL settings. This paper introduces the key design principles of NVFlare and illustrates some use cases (e.g., COVID analysis) with customizable FL workflows that implement different privacy-preserving algorithms.\n\nIntroduction\n\nFederated learning (FL) has become a reality for many real-world applications [31]. It enables multinational collaborations on a global scale to build more robust and generalizable machine learning and AI models. In this paper, we introduce NVIDIA FLARE (NVFlare), an open-source software development kit (SDK) that makes it easier for data scientists to collaborate to develop more generalizable and robust AI models by sharing model weights rather than private data. While FL is attractive in many industries, it is particularly beneficial for healthcare applications where patient data needs to be protected. For example, FL has been used for predicting clinical outcomes in patients with COVID-19 [6] or to segment brain lesions in magnetic resonance imaging [35,34]. NVFlare is not limited to applications in healthcare and is designed to allow cross-silo FL [15] across enterprises for different industries and researchers.\n\nIn recent years, several efforts (both open-source and commercial) have been made to bring FL technology into the healthcare sector and other industries, like TensorFlow Federated [1], PySyft [44], FedML [11], FATE [23], Flower [2], OpenFL [30], Fed-BioMed [36], IBM Federated Learning [24], HP Swarm Learning [38], Federat-edScope [40], FLUTE [7], and more. Some focus on simulated FL settings for researchers, while others prioritize production settings. NVFlare aims to be useful for both scenarios: 1) for researchers by providing efficient and extensible simulation tools and 2) by providing an easy path to transfer research into real-world production settings, supporting high availability and server failover, and by providing additional productivity tools such as multi-tasking and admin commands.\n\n\nNVIDIA FLARE Overview\n\nNVIDIA FLARE -or short NVFlare -stands for \"NVIDIA Federated Learning Application Runtime Environment\". The SDK enables researchers and data scientists to adapt their machine learning and deep learning workflows to a federated paradigm. It enables platform developers to build a secure, privacy-preserving offering for distributed multiparty collaboration.\n\nNVFlare is a lightweight, flexible, and scalable FL framework implemented in Python that is agnostic to the underlying training library. Developers can bring their own data science workflows implemented in PyTorch, TensorFlow, or even in pure NumPy, and apply them in a federated setting. A typical FL workflow such as the popular federated averaging (FedAvg) algorithm [25], can be implemented in NVFlare using the following main steps. Starting from an initial global model, each FL client trains the model on their local data for a while and sends model updates to the server for aggregation. The server then uses the aggregated updates to update the global model for the next round of training. This process is iterated many times until the model converges.\n\nThough used heavily for federated deep learning, NVFlare is a generic approach for supporting collaborative computing across multiple clients. NVFlare provides the Controller programming API for researchers to create workflows for coordinating clients for collaboration. FedAvg is one such workflow. Another example is cyclic weight transfer [4]. The central concept of collaboration is the notion of \"task\". An FL controller assigns tasks (e.g., deep-learning training with model weights) to one or more FL clients and processes results returned from clients (e.g., model weight updates). The controller may assign additional tasks to clients based on the processed results and other factors (e.g., a pre-configured number of training rounds). This task-based interaction continues until the objectives of the study are achieved. The API supports typical controller-client interaction patterns like Figure 1: NVFlare job execution. The Controller is a Python object that controls or coordinates the Workers to get a job done. The controller is run on the FL server. A Worker is capable of performing tasks. Workers run on FL clients.\n\nbroadcasting a task to multiple clients, sending a task to one or more specified clients, or relaying a task to multiple clients sequentially. Each interaction pattern has two flavors: wait (block until client results are received) or no-wait. A workflow developer can use these interaction patterns to create innovative workflows. For example, the Scat-terAndGather controller (typically used for FedAvg-like algorithms) is implemented with the broadcast_and_wait pattern, and the CyclicController is implemented with the relay_and_wait pattern. The controller API allows the researcher to focus on the control logic without needing to deal with underlying communication issues. Figure 1 shows the principle. Each FL client acts as a worker that simply executes tasks assigned to it (e.g., model training) and returns execution results to the controller. At each task interaction, there can be optional filters that process the task data or results before passing it to the Controller (on the server side) or task executor (client side). The filter mechanism can be used for data privacy protection (e.g., homomorphic encryption/decryption or differential privacy) without having to alter the training algorithms.\n\nKey Components NVFlare is built on a componentized architecture that allows FL workloads to move from research and simulation to real-world production deployment. Some of the key components of this SDK include:\n\n\u2022 FL Simulator for rapid development and prototyping.\n\n\u2022 NVFlare Dashboard for simplified project management, secure provisioning, and deployment, orchestration.\n\n\u2022 Reference FL algorithms (e.g., FedAvg, FedProx, SCAFFOLD) and workflows, like scatter and gather, cyclic, etc.\n\n\u2022 Privacy preservation with differential privacy, homomorphic encryption, and more.\n\n\u2022 Specification-based API for extensibility, allowing customization with plug-able components.\n\n\u2022 Tight integration with other learning frameworks like MONAI [3], XGBoost [5], and more.\n\nHigh-Level Architecture NVFlare is designed with the idea that less is more, using a specification-based design principle to focus on what is essential. This allows other people to be able to do what they want to do in real-world applications by following clear API definitions.\n\nFL is an open-ended space. The API-based design allows others to bring their implementations and solutions for various components. Controllers, task executors, and filters are just examples of such extensible components. NVFlare provides an end-to-end operation environment for different personas. It provides a comprehensive provisioning system that creates security credentials for secure communications to enable the easy and secure deployment of FL applications in the real world. It also provides an FL Simulator for running proof-of-concept studies locally. In production mode, the researcher conducts an FL study by submitting jobs using admin commands using Notebooks or the NVFlare Console -an interactive command tool. NVFlare provides many commands for system operation and job management. With these commands, one can start and stop a specific client or the entire system, submit new jobs, check the status of jobs, create a job by cloning from an existing one, and much more. With NVFlare's component-based design, a job is just a configuration of components needed for the study. For the control logic, the job specifies the controller component to be used and any components required by the controller.\n\n\nSystem Concepts\n\nA NVFlare system is a typical client-server communication system that comprises one or more FL server(s), one or more FL client(s), and one or more admin clients. The FL Servers open two ports for communication with FL clients and admin clients. FL clients and admin clients connect to the opened ports. FL clients and admin clients do not open any ports and do not directly communicate with each other. The following is an overview of the key concepts and objects available in NVFlare and the information that can be passed between them.\n\nWorkers and Controller NVFlare's collaborative computing is achieved through the Controller/Worker interactions.\n\nShareable Object that represents a communication between server and client. Technically, the Shareable is implemented as a Python dictionary that could contain different information, e.g., model weights.\n\nData Exchange Object (DXO) Standardizes the data passed between the communicating parties. One can think of the Shareable as the envelope and the DXO as the letter. Together, they comprise a message to be shared between communicating parties.\n\nFLComponent The base class of all the FL components. Executors, controllers, filters, aggregators, and their subtypes are all FLComponents. FLComponent comes with some useful built-in methods for logging, event handling, auditing, and error handling.\n\nExecutors Type of FLComponent for FL clients that has an execute method that produces a Shareable from an input Shareable. NVFlare provides both single-and multi-process executors to implement different computing workloads.\n\nFLContext One of the most important features of NVFlare is to pass data between the FL components. FLContext is available to every method of all common FLComponent types. Through FLContext, the component developer can get services provided by the underlying infrastructure and share data with other components of the FL system.\n\n\nCommunication Drivers\n\nNVFlare abstracts the communication layers out so that different deployment scenarios can implement customizable communication drivers. By default, we use GRPC for data communication in taskbased communication. However, the driver can be replaced to run other communication protocols, for example, TCP. The customizable nature of communication in NVFlare allows for both server-centric and peer-to-peer communication patterns. This enables the user to utilize both scatter and gather-type workflows like FedAvg [25], decentralized training patterns like swarm learning [38], or direct peer-to-peer communication as in split learning [9]. Fig. 2 compares the times for model upload and download from the client's perspective using different communication protocols available in NVFlare using a model of \u223c18MB in size.\n\nThe experiment runs in a multi-cloud environment with the server and eight clients running on Azure, while two clients run on AWS. One can observe that the global model download is slower as all clients are trying to download the global model at the same time, and hence the server is more busy. In contrast, the clients' model uploads happen at slightly different times and therefore are faster. One can also see how this multi-cloud setup causes the clients on AWS to take slightly longer during model download due to communication across different cloud infrastructures.\n\nFilters Filters in NVFlare are a type of FLComponent that have a process method to transform the Shareable object between the communicating parties. A Filter can provide additional processing to shareable data before sending or after receiving from a peer. Filters can convert data formats and a lot more and are NVFlare's primary mechanism for data privacy protection [21,10]:\n\n\u2022 ExcludeVars to exclude variables from shareable.\n\n\u2022 PercentilePrivacy for truncation of weights by percentile.\n\n\u2022 SVTPrivacy for differential privacy through sparse vector techniques.\n\n\u2022 Homomorphic encryption filters used for secure aggregation. The clients are distributed between Azure and AWS. The message size is \u223c18MB. Communication times were measured over 100 rounds of FedAvg. Error bars indicate the 95% confidence intervals.\n\nAs an example, we show the average encryption, decryption, and upload times when using homomorphic encryption for secure aggregation 2 . We compare raw data to encrypted model gradients uploaded in Table 1 when hosting the server on AWS 3 and connecting 30 client instances using an on-premise GPU cluster. One can see the longer upload times due to the larger message sizes needed by homomorphic encryption. Event Mechanism NVFlare comes with a powerful event mechanism that allows dynamic notifications to be sent to all event handlers. This mechanism enables data-based communication among decoupled components: one component fires an event when a certain condition occurs, and other components can listen to that event and processes the event data. Each FLComponent is automatically an event handler. To listen to and process an event, one can simply implement the handle_event() method and process desired event types. Events represent some important moments during the execution of the system logic. For example, before and after aggregation or when important data becomes available, e.g., a new \"best\" model was selected.\n\n\nProductivity Features\n\nNVFlare contains features that enable efficient, collaborative, and robust computing workflows.\n\n2 https://developer.nvidia.com/blog/federated-learning-with-homomorphic-encryption 3 For reference, we used an m5a.2xlarge instance with eight vCPUs, 32-GB memory, and up to 2,880 Gbps network bandwidth.\n\nMulti-tasking For systems with a large capacity, computing resources could be idle most of the time. NVFlare implements a resource-based multi-tasking solution, where multiple jobs can be run concurrently when overall system resources are available. Multi-tasking is made possible by a job scheduler on the server side that constantly tries to schedule a new job. For each job to be scheduled, the scheduler asks each client whether they can satisfy the required resources of the job (e.g., number of GPU devices) by querying the client's resource manager. If all clients can meet the requirement, the job will be scheduled and deployed to the clients.\n\nHigh Availability and Server Failover To avoid the FL server as a single point of failure, a solution has been implemented to support multiple FL servers with automatic cut-over when the currently active server becomes unavailable. Therefore, a component called Overseer is added to facilitate automatic cut-over. The Overseer provides the authoritative endpoint info of the active FL server. All other system entities (FL servers, FL clients, admin clients) constantly communicate (i.e., every 5 seconds) with the Overseer to obtain and act on such information. If the server cutover happens during the execution of a job, then the job will continue to run on the new server. Depending on how the controller is written, the job may or may not need to restart from the beginning but can continue from a previously saved snapshot.\n\nSimulator NVFlare provides a simulator to allow data scientists and system developers to easily write new FLComponents and novel workflows. The simulator is a command line tool to run a NVFlare job. To allow simple experimentation and debugging, the FL server and multiple clients run in the same process during simulation. A multi-process option allows efficient use of resources, e.g., training multiple clients on different GPUs. The simulator follows the same job execution as in real-world NVFlare deployment. Therefore, components developed in simulation can be directly deployed in real-world federated scenarios.\n\n\nSecure Provisioning in NVFlare\n\nSecurity is an important requirement for FL systems. NVFlare provides security solutions in the following areas: authentication, communication confidentiality, user authorization, data privacy protection, auditing, and local client policies.  Authentication NVFlare ensures the identities of communicating peers using mutual Transport Layer Security (TLS). Each participating party (FL Servers, Overseer, FL Clients, Admin Clients) must be properly provisioned. Once provisioned, each party receives a startup kit containing TLS credentials (public cert of the root, the party's own private key and certificate) and system endpoint information, see Fig. 3. Each party can only connect to the NVFlare system with the startup kit. Communication confidentiality is also achieved with the use of TLS-based messaging.\n\nFederated Authorization NVFlare's admin command system is very rich and powerful. Not every command is for everyone. NVFlare implements a role-based user authorization system that controls what a user can or cannot do. At the time of provision, each user is assigned a role. Authorization policies specify which commands are permitted for which roles. Each FL client can define its authorization policy that specifies what a role can or cannot do to the client. For example, one client could allow a role to run jobs from any researchers. In contrast, another client may only allow jobs submitted by its researchers (i.e., the client and the job submitter belong to the same organization).\n\nNVFlare automatically records all user commands and job events in system audit files on both the server and client sides. In addition, the audit API can be used by application developers to record additional events in the audit files.\n\nClient-Privacy NVFlare enhances the overall system security by allowing each client to define its policies for authorization, data privacy (filters), and computing resource management. The client can change its policies at any time after the system is up and running without having to be re-provisioned. For example, the client could require all jobs running on it to be subject to a set of filters. The client could also change the number of computing resources (e.g., GPU devices) to be used by the FL client.\n\n\nFederated Data Science\n\nAs a general distributed computing platform, NVFlare can be used for various applications in different industries.\n\nHere we describe some of the most common use cases where NVFlare was deployed.\n\n\nFederated Deep Learning\n\nA go-to example dataset for benchmarking different FL algorithms is CIFAR-10 [17]. NVFlare allows users to experiment with different algorithms and data splits using different levels of heterogeneity based on a Dirichlet sampling strategy [37]. Figure 4a shows the impact of varying alpha values, where lower values cause higher heterogeneity on the performance of the FedAvg.\n\nApart from FedAvg, currently available in NVFlare include FedProx [20], FedOpt [29], and SCAFFOLD [16]. Figure 4b compares an \u03b1 setting of 0.1, causing a high data heterogeneity across clients and its impact on more advanced FL algorithms, namely FedProx, FedOpt, and SCAFFOLD. FedOpt and SCAFFOLD show markedly better convergence rates and achieve better performance than FedAvg and FedProx with the same alpha setting. SCAFFOLD achieves this by adding a correction term when updating the client models, while FedOpt utilizes SGD with momentum to update the global model on the server. Therefore, both perform better with the same number of training steps as FedAvg and FedProx.\n\nOther algorithms available in or coming soon to NVFlare include federated XGBoost [5], Ditto [19], FedSM [41], Auto-FedRL [8], and more.\n\n\nFederated Machine Learning\n\nTraditional machine learning methods, such as linear models, support vector machine (SVM), and k-means clustering, can be formulated under a federated setting.  With certain libraries, the federated machine learning algorithms need to be designed considering two factors: algorithm-wise, each of these models has distinct training schemes and model representations; and implementationwise, popular libraries providing these functionalities (e.g., scikit-learn, XGBoost) have different APIs and inner logics. Hence, when developing an FL variant of a particular traditional machine learning method, several questions need to be answered at these two levels: First, at the algorithm level, we need to break down the optimization process into individual steps/rounds (if possible) and have answers to three major questions:\n\n1. What information should clients share with the server? 2. How should the server aggregate the collected information from clients?\n\n3. What should clients do with the global aggregated information received from the server? Second, at the implementation level, we need to know what APIs are available and how to utilize them in a federated pipeline to implement a distributed version of the algorithm.\n\nA major difference between federated traditional machine learning and federated deep learning is that, for traditional machine learning methods, the boundary between \"federated\" and \"distributed\", or even \"ensemble\", can be much more vague than for deep learning. Due to the characteristics of a given algorithm and its API design, the concepts can be equivalent. Take XGBoost and SVM, for example: Algorithm-wise, XGBoost can distribute the training samples to several workers and construct trees based on the collected histograms from each worker. Such a process can be directly adopted under a federated setting because the communication cost is affordable. In this case, \"federated\" is equivalent to \"distributed\" learning. API-wise, some algorithms can be constrained by their implementation. Take scikit-learn's SVM for instance. Although theoretically SVM can be formulated as an iterative optimization process, the API only supports one-shot \"fitting\" without the capability of separately calling the optimization steps. Hence a federated SVM algorithm using the scikit-learn library can only be implemented as a two-step process. In this case, \"federated\" is equivalent to \"ensemble\".\n\nFor clarification, we provide the full formulation for tree-based federated XGBoost, illustrated in Fig. 5: 1. XGBoost, by definition, is a sequential optimization process: each step adds one extra tree to the model to reduce the residual error. Hence, federated XGBoost can be formulated as follows: each round of FL corresponds to one boosting step at the local level. Clients share the newly added tree trained on local data with the server at the end of local boosting.\n\n2. The model representation is a decision/regression tree. To aggregate the information from all clients, the server will bag all received trees to form a \"forest\" to be added to the global boosting model.\n\n\n3.\n\nWith the updated global model from the server, each client will continue the boosting process by learning a new tree starting from the global model of the boosted forest.\n\n\nBoosting\n\nCommunication: Client to Server Server to Client \n\n\nSplit learning\n\nSplit learning assumes a vertical data partitioning [42] that can be useful in many distributed learning scenarios involving neural network architectures [9].\n\nAs an introductory example, we can assume that one client holds the images, and the other holds the labels to compute losses and accuracy metrics. Activations and corresponding gradients are being exchanged between the clients using NVFlare, as illustrated in Fig. 6. We use a cryptographic technique called private set intersection (PSI) [39] to compute the alignment between images and labels on both clients. NVFlare's implementation of PSI can be extended to multiple parties and applied to other use cases than split learning, e.g., requiring a secure and privacy-preserving alignment of different databases. Using NVFlare's capability to implement different communication patterns, we can investigate the communication speed-ups one can achieve by implementing split learning using direct peer-to-peer communication as opposed to routing the messages between the two clients through a central server.\n\nThe table in Fig. 6 compares the training speeds of split learning on the CIFAR-10 dataset in a local simulation scenario. First, we use the same PyTorch script to simulate split learning. Then, we implement two distributed solutions using NVFlare. One that routes the messages through the server and one using a direct peer-to-peer connection between the clients. As expected, the direct peer-to-peer connection is more efficient, achieving only a slight overhead in total training time compared to the standalone PyTorch script, which could not be translated to real-world scenarios.\n\n\nFederated Statistics\n\nNVFlare provides built-in federated statistics operators (Controller and Executors) that will generate global statistics based on local client statistics. Each client could have one or more datasets, such as \"train\" and \"test\" datasets. Each dataset may have many features. NVFlare will calculate and combine the statistics for each feature in the dataset to produce global statistics for all the numeric features. The output gathered on the server will be the complete statistics for all datasets in clients and global, as illustrated in Fig. 7.  \n\n\nReal-world Use Cases\n\nNVFlare and its predecessors have been used in several real-world studies exploring FL for healthcare scenarios.\n\nThe collaborations between multinational institutions tested and validated the utility of federated learning, pushing the envelope for training robust, generalizable AI models. These initiatives included FL for breast mammography classification [32], prostate segmentation [33], pancreas segmentation [37], and most recently, chest X-ray (CXR) and electronic health record (EHR) analysis to predict the oxygen requirement for patients arriving in the emergency department with symptoms of COVID-19 [6].  \n\n\nSummary & Conclusion\n\nWe described NVFlare, an open-source SDK to make it easier for data scientists to use FL in their research and to allow an easy transition from research to real-world deployment. As discussed above, NVFlare's Controller programming API supports various interaction patterns between the server and clients over internet connections, which could be unstable. Therefore, the API design mitigates various failure conditions and unexpected crashes of the client machines, such as allowing developers to process timeout conditions properly. NVFLare's unique flexibility and agnostic approach towards the deployed training libraries make it the perfect solution for integrating with different deep learning frameworks, including popular ones used for training large language models (LLM). With our dedication to addressing the current limitations of communication protocols, we are working towards supporting the communication of large message sizes, enabling the federated fine-tuning of AI models with billions of parameters, such as those used for ChatGPT [28] and GPT-4 [27]. Moreover, our team is implementing parameter-efficient federated methods to adapt LLM models to downstream tasks [43], utilizing techniques such as prompt tuning [18] and p-tuning [22], adapters [13,12], LoRA [14], showing promising performance. Our commitment to innovation and excellence in this field ensures that we continue to push the boundaries of what is possible with federated learning.\n\nWe did not go into all details of exciting features available in NVFlare, like homomorphic encryption, Ten-sorBoard streaming, provisioning web dashboard, integration with MONAI 4 [26,3], etc. However, we hope that this overview of NVFlare gives a good starting point for developers and researchers on their journey to using FL and federated data science in simulation and the real world.\n\nNVFlare is an open-source project. We invite the community to contribute and grow NVFlare. For more information, please visit the code repository at https://github.com/NVIDIA/NVFlare.\n\nFigure 2 :\n2Comparison of GRPC and TCP communication drivers in NVFlare. The server is running on Azure.\n\nFigure 3 :\n3High-level steps for running a real-world study with secure provisioning with NVFlare.\n\nFigure 4 :\n4Federated learning experiments with NVFlare.\n\nFigure 5 :\n5Tree-based federated XGBoost: a \"boosting of forests.\"\n\nFigure 6 :\n6Simple split learning scenario using CIFAR-10. The table compares multiple communication patterns. Using 50,000 training samples and 15,625 rounds of communication with a batch size of 64.\n\n\nstatistics. Note the data of \"site-4\" violates the client's privacy policy and therefore does not share its statistics with the server.(b) Histogram visualization.\n\nFigure 7 :\n7Federated statistics with NVFlare.\n\n\n(a) Mammography. (b) Prostate. (c) Pancreas. (d) CXR & EHR.\n\nFigure 8 :\n8Real-world use cases of NVFlare.\n\nTable 1 :\n1Federated learning exchanging homomorphic encrypted vs. raw model updates.Time in seconds Mean Std. Dev. \n\nEncryption \n5.01 \n1.18 \nDecryption \n0.95 \n0.04 \nEnc. upload \n38.00 \n71.17 \nRaw upload \n21.57 \n74.23 \n\n\nhttps://monai.io\n\n{TensorFlow}: a system for {Large-Scale} machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, 12th USENIX symposium on operating systems design and implementation (OSDI 16). M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. {TensorFlow}: a system for {Large-Scale} machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pages 265-283, 2016.\n\nD J Beutel, T Topal, A Mathur, X Qiu, T Parcollet, P P De Gusm\u00e3o, N D Lane, arXiv:2007.14390Flower: A friendly federated learning research framework. arXiv preprintD. J. Beutel, T. Topal, A. Mathur, X. Qiu, T. Parcollet, P. P. de Gusm\u00e3o, and N. D. Lane. Flower: A friendly federated learning research framework. arXiv preprint arXiv:2007.14390, 2020.\n\nM J Cardoso, W Li, R Brown, N Ma, E Kerfoot, Y Wang, B Murrey, A Myronenko, C Zhao, D Yang, arXiv:2211.02701An open-source framework for deep learning in healthcare. arXiv preprintM. J. Cardoso, W. Li, R. Brown, N. Ma, E. Kerfoot, Y. Wang, B. Murrey, A. Myronenko, C. Zhao, D. Yang, et al. Monai: An open-source framework for deep learning in healthcare. arXiv preprint arXiv:2211.02701, 2022.\n\nDistributed deep learning networks among institutions for medical imaging. K Chang, N Balachandar, C Lam, D Yi, J Brown, A Beers, B Rosen, D L Rubin, J Kalpathy-Cramer, Journal of the American Medical Informatics Association. 258K. Chang, N. Balachandar, C. Lam, D. Yi, J. Brown, A. Beers, B. Rosen, D. L. Rubin, and J. Kalpathy-Cramer. Distributed deep learning networks among institutions for medical imaging. Journal of the American Medical Informatics Association, 25(8):945-954, 2018.\n\nXGBoost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16New York, NY, USAACMT. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 785-794, New York, NY, USA, 2016. ACM.\n\nFederated learning for predicting clinical outcomes in patients with covid-19. I Dayan, H R Roth, A Zhong, A Harouni, A Gentili, A Z Abidin, A Liu, A B Costa, B J Wood, C.-S Tsai, Nature medicine. 2710I. Dayan, H. R. Roth, A. Zhong, A. Harouni, A. Gentili, A. Z. Abidin, A. Liu, A. B. Costa, B. J. Wood, C.-S. Tsai, et al. Federated learning for predicting clinical outcomes in patients with covid-19. Nature medicine, 27(10):1735-1743, 2021.\n\nFlute: A scalable, extensible framework for high-performance federated learning simulations. D Dimitriadis, M H Garcia, D M Diaz, A Manoel, R Sim, arXiv:2203.13789arXiv preprintD. Dimitriadis, M. H. Garcia, D. M. Diaz, A. Manoel, and R. Sim. Flute: A scalable, extensible framework for high-performance federated learning simulations. arXiv preprint arXiv:2203.13789, 2022.\n\nAuto-fedrl: Federated hyperparameter optimization for multi-institutional medical image segmentation. P Guo, D Yang, A Hatamizadeh, A Xu, Z Xu, W Li, C Zhao, D Xu, S Harmon, E Turkbey, arXiv:2203.06338arXiv preprintP. Guo, D. Yang, A. Hatamizadeh, A. Xu, Z. Xu, W. Li, C. Zhao, D. Xu, S. Harmon, E. Turkbey, et al. Auto-fedrl: Feder- ated hyperparameter optimization for multi-institutional medical image segmentation. arXiv preprint arXiv:2203.06338, 2022.\n\nDistributed learning of deep neural network over multiple agents. O Gupta, R Raskar, Journal of Network and Computer Applications. 116O. Gupta and R. Raskar. Distributed learning of deep neural network over multiple agents. Journal of Network and Computer Applications, 116:1-8, 2018.\n\nDo gradient inversion attacks make federated learning unsafe?. A Hatamizadeh, H Yin, P Molchanov, A Myronenko, W Li, P Dogra, A Feng, M G Flores, J Kautz, D Xu, arXiv:2202.06924arXiv preprintA. Hatamizadeh, H. Yin, P. Molchanov, A. Myronenko, W. Li, P. Dogra, A. Feng, M. G. Flores, J. Kautz, D. Xu, et al. Do gradient inversion attacks make federated learning unsafe? arXiv preprint arXiv:2202.06924, 2022.\n\nC He, S Li, J So, X Zeng, M Zhang, H Wang, X Wang, P Vepakomma, A Singh, H Qiu, arXiv:2007.13518A research library and benchmark for federated machine learning. arXiv preprintC. He, S. Li, J. So, X. Zeng, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh, H. Qiu, et al. FedML: A research library and benchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.\n\nTowards a unified view of parameter-efficient transfer learning. J He, C Zhou, X Ma, T Berg-Kirkpatrick, G Neubig, arXiv:2110.04366arXiv preprintJ. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.\n\nParameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q De Laroussilhe, A Gesmundo, M Attariyan, S Gelly, International Conference on Machine Learning. PMLRN. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790-2799. PMLR, 2019.\n\nE J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. arXiv preprintE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\nP Kairouz, H B Mcmahan, B Avent, A Bellet, M Bennis, A N Bhagoji, K Bonawitz, Z Charles, G Cormode, R Cummings, arXiv:1912.04977Advances and open problems in federated learning. arXiv preprintP. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\nScaffold: Stochastic controlled averaging for federated learning. S P Karimireddy, S Kale, M Mohri, S Reddi, S Stich, A T Suresh, International Conference on Machine Learning. PMLRS. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132-5143. PMLR, 2020.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nThe power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.08691arXiv preprintB. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n\nDitto: Fair and robust federated learning through personalization. T Li, S Hu, A Beirami, V Smith, International Conference on Machine Learning. PMLRT. Li, S. Hu, A. Beirami, and V. Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pages 6357-6368. PMLR, 2021.\n\nFederated optimization in heterogeneous networks. T Li, A K Sahu, M Zaheer, M Sanjabi, A Talwalkar, V Smith, Proceedings of Machine Learning and Systems. Machine Learning and Systems2T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429-450, 2020.\n\nPrivacy-preserving federated brain tumour segmentation. W Li, F Milletar\u00ec, D Xu, N Rieke, J Hancox, W Zhu, M Baust, Y Cheng, S Ourselin, M J Cardoso, International workshop on machine learning in medical imaging. SpringerW. Li, F. Milletar\u00ec, D. Xu, N. Rieke, J. Hancox, W. Zhu, M. Baust, Y. Cheng, S. Ourselin, M. J. Cardoso, et al. Privacy-preserving federated brain tumour segmentation. In International workshop on machine learning in medical imaging, pages 133-141. Springer, 2019.\n\n. X Liu, Y Zheng, Z Du, M Ding, Y Qian, Z Yang, J Tang, arXiv:2103.10385Gpt understands, too. arXiv preprintX. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021.\n\nFate: An industrial grade platform for collaborative learning with data protection. Y Liu, T Fan, T Chen, Q Xu, Q Yang, J. Mach. Learn. Res. 22226Y. Liu, T. Fan, T. Chen, Q. Xu, and Q. Yang. Fate: An industrial grade platform for collaborative learning with data protection. J. Mach. Learn. Res., 22(226):1-6, 2021.\n\nIbm federated learning: an enterprise framework white paper v0. H Ludwig, N Baracaldo, G Thomas, Y Zhou, A Anwar, S Rajamoni, Y Ong, J Radhakrishnan, A Verma, M Sinn, arXiv:2007.109871arXiv preprintH. Ludwig, N. Baracaldo, G. Thomas, Y. Zhou, A. Anwar, S. Rajamoni, Y. Ong, J. Radhakrishnan, A. Verma, M. Sinn, et al. Ibm federated learning: an enterprise framework white paper v0. 1. arXiv preprint arXiv:2007.10987, 2020.\n\nCommunication-efficient learning of deep networks from decentralized data. B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, Artificial intelligence and statistics. PMLRB. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273-1282. PMLR, 2017.\n\nMONAI: Medical Open Network for AI. Monai Consortium, 92022MONAI Consortium. MONAI: Medical Open Network for AI, 9 2022.\n\n. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.\n\nTraining language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 35L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n\nS Reddi, Z Charles, M Zaheer, Z Garrett, K Rush, J Kone\u010dn\u1ef3, S Kumar, H B Mcmahan, arXiv:2003.00295Adaptive federated optimization. arXiv preprintS. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Kone\u010dn\u1ef3, S. Kumar, and H. B. McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.\n\nG A Reina, A Gruzdev, P Foley, O Perepelkina, M Sharma, I Davidyuk, I Trushkin, M Radionov, A Mokrov, D Agapov, arXiv:2105.06413An open-source framework for federated learning. arXiv preprintG. A. Reina, A. Gruzdev, P. Foley, O. Perepelkina, M. Sharma, I. Davidyuk, I. Trushkin, M. Radionov, A. Mokrov, D. Agapov, et al. Openfl: An open-source framework for federated learning. arXiv preprint arXiv:2105.06413, 2021.\n\nThe future of digital health with federated learning. N Rieke, J Hancox, W Li, F Milletari, H R Roth, S Albarqouni, S Bakas, M N Galtier, B A Landman, K Maier-Hein, NPJ digital medicine. 31N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni, S. Bakas, M. N. Galtier, B. A. Landman, K. Maier-Hein, et al. The future of digital health with federated learning. NPJ digital medicine, 3(1):1-7, 2020.\n\nFederated learning for breast density classification: A real-world implementation. H R Roth, K Chang, P Singh, N Neumark, W Li, V Gupta, S Gupta, L Qu, A Ihsani, B C Bizzo, Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning. SpringerH. R. Roth, K. Chang, P. Singh, N. Neumark, W. Li, V. Gupta, S. Gupta, L. Qu, A. Ihsani, B. C. Bizzo, et al. Federated learning for breast density classification: A real-world implementation. In Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning, pages 181-191. Springer, 2020.\n\nFederated learning improves site performance in multicenter deep learning without data sharing. K V Sarma, S Harmon, T Sanford, H R Roth, Z Xu, J Tetreault, D Xu, M G Flores, A G Raman, R Kulkarni, Journal of the American Medical Informatics Association. 286K. V. Sarma, S. Harmon, T. Sanford, H. R. Roth, Z. Xu, J. Tetreault, D. Xu, M. G. Flores, A. G. Raman, R. Kulkarni, et al. Federated learning improves site performance in multicenter deep learning without data sharing. Journal of the American Medical Informatics Association, 28(6):1259-1264, 2021.\n\nFederated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. M J Sheller, B Edwards, G A Reina, J Martin, S Pati, A Kotrotsou, M Milchenko, W Xu, D Marcus, R R Colen, Scientific reports. 101M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati, A. Kotrotsou, M. Milchenko, W. Xu, D. Marcus, R. R. Colen, et al. Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. Scientific reports, 10(1):1-12, 2020.\n\nMulti-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. M J Sheller, G A Reina, B Edwards, J Martin, S Bakas, International MICCAI Brainlesion Workshop. SpringerM. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas. Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In International MICCAI Brainlesion Workshop, pages 92-104. Springer, 2018.\n\nFed-biomed: A general open-source frontend framework for federated learning in healthcare. S Silva, A Altmann, B Gutman, M Lorenzi, Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning. SpringerS. Silva, A. Altmann, B. Gutman, and M. Lorenzi. Fed-biomed: A general open-source frontend framework for federated learning in healthcare. In Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning, pages 201-210. Springer, 2020.\n\nH Wang, M Yurochkin, Y Sun, D Papailiopoulos, Y Khazaeni, arXiv:2002.06440Federated learning with matched averaging. arXiv preprintH. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020.\n\nSwarm learning for decentralized and confidential clinical machine learning. S Warnat-Herresthal, H Schultze, K L Shastry, S Manamohan, S Mukherjee, V Garg, R Sarveswara, K H\u00e4ndler, P Pickkers, N A Aziz, Nature. 5947862S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan, S. Mukherjee, V. Garg, R. Sarveswara, K. H\u00e4ndler, P. Pickkers, N. A. Aziz, et al. Swarm learning for decentralized and confidential clinical machine learning. Nature, 594(7862):265-270, 2021.\n\nPrivate set intersection -Wikipedia, the free encyclopedia. Wikipedia contributors. 27Wikipedia contributors. Private set intersection -Wikipedia, the free encyclopedia, 2023. [Online; accessed 27-April-2023].\n\nFederatedscope: A comprehensive and flexible federated learning platform via message passing. Y Xie, Z Wang, D Chen, D Gao, L Yao, W Kuang, Y Li, B Ding, J Zhou, arXiv:2204.05011arXiv preprintY. Xie, Z. Wang, D. Chen, D. Gao, L. Yao, W. Kuang, Y. Li, B. Ding, and J. Zhou. Federatedscope: A comprehensive and flexible federated learning platform via message passing. arXiv preprint arXiv:2204.05011, 2022.\n\nClosing the generalization gap of cross-silo federated medical image segmentation. A Xu, W Li, P Guo, D Yang, H R Roth, A Hatamizadeh, C Zhao, D Xu, H Huang, Z Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionA. Xu, W. Li, P. Guo, D. Yang, H. R. Roth, A. Hatamizadeh, C. Zhao, D. Xu, H. Huang, and Z. Xu. Closing the generalization gap of cross-silo federated medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20866-20875, 2022.\n\nFederated machine learning: Concept and applications. Q Yang, Y Liu, T Chen, Y Tong, ACM Transactions on Intelligent Systems and Technology (TIST). 102Q. Yang, Y. Liu, T. Chen, and Y. Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19, 2019.\n\nReduce communication costs and preserve privacy: Prompt tuning method in federated learning. H Zhao, W Du, F Li, P Li, G Liu, arXiv:2208.12268arXiv preprintH. Zhao, W. Du, F. Li, P. Li, and G. Liu. Reduce communication costs and preserve privacy: Prompt tuning method in federated learning. arXiv preprint arXiv:2208.12268, 2022.\n\nPysyft: A library for easy federated learning. A Ziller, A Trask, A Lopardo, B Szymkow, B Wagner, E Bluemke, J.-M Nounahon, J Passerat-Palmbach, K Prakash, N Rose, Federated Learning Systems. SpringerA. Ziller, A. Trask, A. Lopardo, B. Szymkow, B. Wagner, E. Bluemke, J.-M. Nounahon, J. Passerat-Palmbach, K. Prakash, N. Rose, et al. Pysyft: A library for easy federated learning. In Federated Learning Systems, pages 111-139. Springer, 2021.\n", "annotations": {"author": "[{\"end\":160,\"start\":66},{\"end\":251,\"start\":161},{\"end\":343,\"start\":252},{\"end\":435,\"start\":344},{\"end\":525,\"start\":436},{\"end\":622,\"start\":526},{\"end\":722,\"start\":623},{\"end\":817,\"start\":723},{\"end\":907,\"start\":818},{\"end\":997,\"start\":908},{\"end\":1092,\"start\":998},{\"end\":1182,\"start\":1093},{\"end\":1280,\"start\":1183},{\"end\":1371,\"start\":1281},{\"end\":1462,\"start\":1372},{\"end\":1556,\"start\":1463},{\"end\":1651,\"start\":1557},{\"end\":1745,\"start\":1652},{\"end\":1837,\"start\":1746},{\"end\":1925,\"start\":1838},{\"end\":2019,\"start\":1926},{\"end\":2112,\"start\":2020},{\"end\":2205,\"start\":2113}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":170,\"start\":165},{\"end\":262,\"start\":259},{\"end\":354,\"start\":350},{\"end\":444,\"start\":442},{\"end\":541,\"start\":536},{\"end\":641,\"start\":634},{\"end\":736,\"start\":729},{\"end\":826,\"start\":822},{\"end\":916,\"start\":914},{\"end\":1011,\"start\":1006},{\"end\":1101,\"start\":1099},{\"end\":1199,\"start\":1190},{\"end\":1290,\"start\":1286},{\"end\":1381,\"start\":1377},{\"end\":1475,\"start\":1470},{\"end\":1570,\"start\":1563},{\"end\":1664,\"start\":1660},{\"end\":1756,\"start\":1754},{\"end\":1844,\"start\":1842},{\"end\":1938,\"start\":1933},{\"end\":2031,\"start\":2025},{\"end\":2124,\"start\":2120}]", "author_first_name": "[{\"end\":72,\"start\":66},{\"end\":74,\"start\":73},{\"end\":164,\"start\":161},{\"end\":258,\"start\":252},{\"end\":349,\"start\":344},{\"end\":441,\"start\":436},{\"end\":535,\"start\":526},{\"end\":633,\"start\":623},{\"end\":728,\"start\":723},{\"end\":821,\"start\":818},{\"end\":913,\"start\":908},{\"end\":1005,\"start\":998},{\"end\":1098,\"start\":1093},{\"end\":1189,\"start\":1183},{\"end\":1285,\"start\":1281},{\"end\":1376,\"start\":1372},{\"end\":1469,\"start\":1463},{\"end\":1562,\"start\":1557},{\"end\":1659,\"start\":1652},{\"end\":1753,\"start\":1746},{\"end\":1841,\"start\":1838},{\"end\":1932,\"start\":1926},{\"end\":2024,\"start\":2020},{\"end\":2119,\"start\":2113}]", "author_affiliation": "[{\"end\":159,\"start\":81},{\"end\":250,\"start\":172},{\"end\":342,\"start\":264},{\"end\":434,\"start\":356},{\"end\":524,\"start\":446},{\"end\":621,\"start\":543},{\"end\":721,\"start\":643},{\"end\":816,\"start\":738},{\"end\":906,\"start\":828},{\"end\":996,\"start\":918},{\"end\":1091,\"start\":1013},{\"end\":1181,\"start\":1103},{\"end\":1279,\"start\":1201},{\"end\":1370,\"start\":1292},{\"end\":1461,\"start\":1383},{\"end\":1555,\"start\":1477},{\"end\":1650,\"start\":1572},{\"end\":1744,\"start\":1666},{\"end\":1836,\"start\":1758},{\"end\":1924,\"start\":1846},{\"end\":2018,\"start\":1940},{\"end\":2111,\"start\":2033},{\"end\":2204,\"start\":2126}]", "title": "[{\"end\":63,\"start\":1},{\"end\":2268,\"start\":2206}]", "venue": null, "abstract": "[{\"end\":3379,\"start\":2270}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3477,\"start\":3473},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4099,\"start\":4096},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4162,\"start\":4158},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4165,\"start\":4162},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4263,\"start\":4259},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4509,\"start\":4506},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4522,\"start\":4518},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4534,\"start\":4530},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4545,\"start\":4541},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4557,\"start\":4554},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4570,\"start\":4566},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4587,\"start\":4583},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4616,\"start\":4612},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4640,\"start\":4636},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4662,\"start\":4658},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4673,\"start\":4670},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5890,\"start\":5886},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6624,\"start\":6621},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9366,\"start\":9363},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9379,\"start\":9376},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13357,\"start\":13353},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13415,\"start\":13411},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13478,\"start\":13475},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14608,\"start\":14604},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14611,\"start\":14608},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21231,\"start\":21227},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21393,\"start\":21389},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21598,\"start\":21594},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21611,\"start\":21607},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21630,\"start\":21626},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22294,\"start\":22291},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22306,\"start\":22302},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22318,\"start\":22314},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22334,\"start\":22331},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25791,\"start\":25787},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25892,\"start\":25889},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26238,\"start\":26234},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28349,\"start\":28345},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28377,\"start\":28373},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28405,\"start\":28401},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28601,\"start\":28598},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29685,\"start\":29681},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29700,\"start\":29696},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29819,\"start\":29815},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29868,\"start\":29864},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29886,\"start\":29882},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29901,\"start\":29897},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29904,\"start\":29901},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29915,\"start\":29911},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30284,\"start\":30280},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30286,\"start\":30284}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30779,\"start\":30674},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30879,\"start\":30780},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30937,\"start\":30880},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31005,\"start\":30938},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31207,\"start\":31006},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31373,\"start\":31208},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31421,\"start\":31374},{\"attributes\":{\"id\":\"fig_8\"},\"end\":31483,\"start\":31422},{\"attributes\":{\"id\":\"fig_9\"},\"end\":31529,\"start\":31484},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31751,\"start\":31530}]", "paragraph": "[{\"end\":4324,\"start\":3395},{\"end\":5132,\"start\":4326},{\"end\":5514,\"start\":5158},{\"end\":6277,\"start\":5516},{\"end\":7413,\"start\":6279},{\"end\":8629,\"start\":7415},{\"end\":8841,\"start\":8631},{\"end\":8896,\"start\":8843},{\"end\":9004,\"start\":8898},{\"end\":9118,\"start\":9006},{\"end\":9203,\"start\":9120},{\"end\":9299,\"start\":9205},{\"end\":9390,\"start\":9301},{\"end\":9670,\"start\":9392},{\"end\":10889,\"start\":9672},{\"end\":11447,\"start\":10909},{\"end\":11561,\"start\":11449},{\"end\":11766,\"start\":11563},{\"end\":12010,\"start\":11768},{\"end\":12262,\"start\":12012},{\"end\":12487,\"start\":12264},{\"end\":12816,\"start\":12489},{\"end\":13658,\"start\":12842},{\"end\":14233,\"start\":13660},{\"end\":14612,\"start\":14235},{\"end\":14664,\"start\":14614},{\"end\":14726,\"start\":14666},{\"end\":14799,\"start\":14728},{\"end\":15051,\"start\":14801},{\"end\":16181,\"start\":15053},{\"end\":16302,\"start\":16207},{\"end\":16507,\"start\":16304},{\"end\":17161,\"start\":16509},{\"end\":17992,\"start\":17163},{\"end\":18614,\"start\":17994},{\"end\":19461,\"start\":18649},{\"end\":20152,\"start\":19463},{\"end\":20388,\"start\":20154},{\"end\":20901,\"start\":20390},{\"end\":21042,\"start\":20928},{\"end\":21122,\"start\":21044},{\"end\":21526,\"start\":21150},{\"end\":22207,\"start\":21528},{\"end\":22345,\"start\":22209},{\"end\":23196,\"start\":22376},{\"end\":23330,\"start\":23198},{\"end\":23600,\"start\":23332},{\"end\":24795,\"start\":23602},{\"end\":25270,\"start\":24797},{\"end\":25477,\"start\":25272},{\"end\":25654,\"start\":25484},{\"end\":25716,\"start\":25667},{\"end\":25893,\"start\":25735},{\"end\":26801,\"start\":25895},{\"end\":27388,\"start\":26803},{\"end\":27961,\"start\":27413},{\"end\":28098,\"start\":27986},{\"end\":28604,\"start\":28100},{\"end\":30098,\"start\":28629},{\"end\":30488,\"start\":30100},{\"end\":30673,\"start\":30490}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15258,\"start\":15251}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3393,\"start\":3381},{\"attributes\":{\"n\":\"2\"},\"end\":5156,\"start\":5135},{\"attributes\":{\"n\":\"3\"},\"end\":10907,\"start\":10892},{\"end\":12840,\"start\":12819},{\"attributes\":{\"n\":\"3.1\"},\"end\":16205,\"start\":16184},{\"attributes\":{\"n\":\"3.2\"},\"end\":18647,\"start\":18617},{\"attributes\":{\"n\":\"4\"},\"end\":20926,\"start\":20904},{\"attributes\":{\"n\":\"4.1\"},\"end\":21148,\"start\":21125},{\"attributes\":{\"n\":\"4.2\"},\"end\":22374,\"start\":22348},{\"end\":25482,\"start\":25480},{\"end\":25665,\"start\":25657},{\"attributes\":{\"n\":\"4.3\"},\"end\":25733,\"start\":25719},{\"attributes\":{\"n\":\"4.4\"},\"end\":27411,\"start\":27391},{\"attributes\":{\"n\":\"5\"},\"end\":27984,\"start\":27964},{\"attributes\":{\"n\":\"6\"},\"end\":28627,\"start\":28607},{\"end\":30685,\"start\":30675},{\"end\":30791,\"start\":30781},{\"end\":30891,\"start\":30881},{\"end\":30949,\"start\":30939},{\"end\":31017,\"start\":31007},{\"end\":31385,\"start\":31375},{\"end\":31495,\"start\":31485},{\"end\":31540,\"start\":31531}]", "table": "[{\"end\":31751,\"start\":31616}]", "figure_caption": "[{\"end\":30779,\"start\":30687},{\"end\":30879,\"start\":30793},{\"end\":30937,\"start\":30893},{\"end\":31005,\"start\":30951},{\"end\":31207,\"start\":31019},{\"end\":31373,\"start\":31210},{\"end\":31421,\"start\":31387},{\"end\":31483,\"start\":31424},{\"end\":31529,\"start\":31497},{\"end\":31616,\"start\":31542}]", "figure_ref": "[{\"end\":7187,\"start\":7179},{\"end\":8103,\"start\":8095},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13486,\"start\":13480},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19304,\"start\":19298},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21404,\"start\":21395},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21641,\"start\":21632},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24904,\"start\":24897},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26161,\"start\":26155},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26822,\"start\":26816},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27958,\"start\":27952}]", "bib_author_first_name": "[{\"end\":31830,\"start\":31829},{\"end\":31839,\"start\":31838},{\"end\":31849,\"start\":31848},{\"end\":31857,\"start\":31856},{\"end\":31865,\"start\":31864},{\"end\":31874,\"start\":31873},{\"end\":31882,\"start\":31881},{\"end\":31891,\"start\":31890},{\"end\":31903,\"start\":31902},{\"end\":31913,\"start\":31912},{\"end\":32275,\"start\":32274},{\"end\":32277,\"start\":32276},{\"end\":32287,\"start\":32286},{\"end\":32296,\"start\":32295},{\"end\":32306,\"start\":32305},{\"end\":32313,\"start\":32312},{\"end\":32326,\"start\":32325},{\"end\":32328,\"start\":32327},{\"end\":32341,\"start\":32340},{\"end\":32343,\"start\":32342},{\"end\":32627,\"start\":32626},{\"end\":32629,\"start\":32628},{\"end\":32640,\"start\":32639},{\"end\":32646,\"start\":32645},{\"end\":32655,\"start\":32654},{\"end\":32661,\"start\":32660},{\"end\":32672,\"start\":32671},{\"end\":32680,\"start\":32679},{\"end\":32690,\"start\":32689},{\"end\":32703,\"start\":32702},{\"end\":32711,\"start\":32710},{\"end\":33097,\"start\":33096},{\"end\":33106,\"start\":33105},{\"end\":33121,\"start\":33120},{\"end\":33128,\"start\":33127},{\"end\":33134,\"start\":33133},{\"end\":33143,\"start\":33142},{\"end\":33152,\"start\":33151},{\"end\":33161,\"start\":33160},{\"end\":33163,\"start\":33162},{\"end\":33172,\"start\":33171},{\"end\":33555,\"start\":33554},{\"end\":33563,\"start\":33562},{\"end\":34100,\"start\":34099},{\"end\":34109,\"start\":34108},{\"end\":34111,\"start\":34110},{\"end\":34119,\"start\":34118},{\"end\":34128,\"start\":34127},{\"end\":34139,\"start\":34138},{\"end\":34150,\"start\":34149},{\"end\":34152,\"start\":34151},{\"end\":34162,\"start\":34161},{\"end\":34169,\"start\":34168},{\"end\":34171,\"start\":34170},{\"end\":34180,\"start\":34179},{\"end\":34182,\"start\":34181},{\"end\":34193,\"start\":34189},{\"end\":34558,\"start\":34557},{\"end\":34573,\"start\":34572},{\"end\":34575,\"start\":34574},{\"end\":34585,\"start\":34584},{\"end\":34587,\"start\":34586},{\"end\":34595,\"start\":34594},{\"end\":34605,\"start\":34604},{\"end\":34942,\"start\":34941},{\"end\":34949,\"start\":34948},{\"end\":34957,\"start\":34956},{\"end\":34972,\"start\":34971},{\"end\":34978,\"start\":34977},{\"end\":34984,\"start\":34983},{\"end\":34990,\"start\":34989},{\"end\":34998,\"start\":34997},{\"end\":35004,\"start\":35003},{\"end\":35014,\"start\":35013},{\"end\":35365,\"start\":35364},{\"end\":35374,\"start\":35373},{\"end\":35648,\"start\":35647},{\"end\":35663,\"start\":35662},{\"end\":35670,\"start\":35669},{\"end\":35683,\"start\":35682},{\"end\":35696,\"start\":35695},{\"end\":35702,\"start\":35701},{\"end\":35711,\"start\":35710},{\"end\":35719,\"start\":35718},{\"end\":35721,\"start\":35720},{\"end\":35731,\"start\":35730},{\"end\":35740,\"start\":35739},{\"end\":35994,\"start\":35993},{\"end\":36000,\"start\":35999},{\"end\":36006,\"start\":36005},{\"end\":36012,\"start\":36011},{\"end\":36020,\"start\":36019},{\"end\":36029,\"start\":36028},{\"end\":36037,\"start\":36036},{\"end\":36045,\"start\":36044},{\"end\":36058,\"start\":36057},{\"end\":36067,\"start\":36066},{\"end\":36443,\"start\":36442},{\"end\":36449,\"start\":36448},{\"end\":36457,\"start\":36456},{\"end\":36463,\"start\":36462},{\"end\":36483,\"start\":36482},{\"end\":36734,\"start\":36733},{\"end\":36745,\"start\":36744},{\"end\":36756,\"start\":36755},{\"end\":36771,\"start\":36770},{\"end\":36782,\"start\":36781},{\"end\":36800,\"start\":36799},{\"end\":36812,\"start\":36811},{\"end\":36825,\"start\":36824},{\"end\":37122,\"start\":37121},{\"end\":37124,\"start\":37123},{\"end\":37130,\"start\":37129},{\"end\":37138,\"start\":37137},{\"end\":37148,\"start\":37147},{\"end\":37161,\"start\":37160},{\"end\":37167,\"start\":37166},{\"end\":37175,\"start\":37174},{\"end\":37183,\"start\":37182},{\"end\":37447,\"start\":37446},{\"end\":37458,\"start\":37457},{\"end\":37460,\"start\":37459},{\"end\":37471,\"start\":37470},{\"end\":37480,\"start\":37479},{\"end\":37490,\"start\":37489},{\"end\":37500,\"start\":37499},{\"end\":37502,\"start\":37501},{\"end\":37513,\"start\":37512},{\"end\":37525,\"start\":37524},{\"end\":37536,\"start\":37535},{\"end\":37547,\"start\":37546},{\"end\":37926,\"start\":37925},{\"end\":37928,\"start\":37927},{\"end\":37943,\"start\":37942},{\"end\":37951,\"start\":37950},{\"end\":37960,\"start\":37959},{\"end\":37969,\"start\":37968},{\"end\":37978,\"start\":37977},{\"end\":37980,\"start\":37979},{\"end\":38316,\"start\":38315},{\"end\":38330,\"start\":38329},{\"end\":38493,\"start\":38492},{\"end\":38503,\"start\":38502},{\"end\":38514,\"start\":38513},{\"end\":38761,\"start\":38760},{\"end\":38767,\"start\":38766},{\"end\":38773,\"start\":38772},{\"end\":38784,\"start\":38783},{\"end\":39079,\"start\":39078},{\"end\":39085,\"start\":39084},{\"end\":39087,\"start\":39086},{\"end\":39095,\"start\":39094},{\"end\":39105,\"start\":39104},{\"end\":39116,\"start\":39115},{\"end\":39129,\"start\":39128},{\"end\":39451,\"start\":39450},{\"end\":39457,\"start\":39456},{\"end\":39470,\"start\":39469},{\"end\":39476,\"start\":39475},{\"end\":39485,\"start\":39484},{\"end\":39495,\"start\":39494},{\"end\":39502,\"start\":39501},{\"end\":39511,\"start\":39510},{\"end\":39520,\"start\":39519},{\"end\":39532,\"start\":39531},{\"end\":39534,\"start\":39533},{\"end\":39884,\"start\":39883},{\"end\":39891,\"start\":39890},{\"end\":39900,\"start\":39899},{\"end\":39906,\"start\":39905},{\"end\":39914,\"start\":39913},{\"end\":39922,\"start\":39921},{\"end\":39930,\"start\":39929},{\"end\":40201,\"start\":40200},{\"end\":40208,\"start\":40207},{\"end\":40215,\"start\":40214},{\"end\":40223,\"start\":40222},{\"end\":40229,\"start\":40228},{\"end\":40498,\"start\":40497},{\"end\":40508,\"start\":40507},{\"end\":40521,\"start\":40520},{\"end\":40531,\"start\":40530},{\"end\":40539,\"start\":40538},{\"end\":40548,\"start\":40547},{\"end\":40560,\"start\":40559},{\"end\":40567,\"start\":40566},{\"end\":40584,\"start\":40583},{\"end\":40593,\"start\":40592},{\"end\":40934,\"start\":40933},{\"end\":40945,\"start\":40944},{\"end\":40954,\"start\":40953},{\"end\":40964,\"start\":40963},{\"end\":40975,\"start\":40974},{\"end\":40977,\"start\":40976},{\"end\":41504,\"start\":41503},{\"end\":41514,\"start\":41513},{\"end\":41520,\"start\":41519},{\"end\":41529,\"start\":41528},{\"end\":41540,\"start\":41539},{\"end\":41554,\"start\":41553},{\"end\":41565,\"start\":41564},{\"end\":41574,\"start\":41573},{\"end\":41585,\"start\":41584},{\"end\":41594,\"start\":41593},{\"end\":41911,\"start\":41910},{\"end\":41920,\"start\":41919},{\"end\":41931,\"start\":41930},{\"end\":41941,\"start\":41940},{\"end\":41952,\"start\":41951},{\"end\":41960,\"start\":41959},{\"end\":41971,\"start\":41970},{\"end\":41980,\"start\":41979},{\"end\":41982,\"start\":41981},{\"end\":42224,\"start\":42223},{\"end\":42226,\"start\":42225},{\"end\":42235,\"start\":42234},{\"end\":42246,\"start\":42245},{\"end\":42255,\"start\":42254},{\"end\":42270,\"start\":42269},{\"end\":42280,\"start\":42279},{\"end\":42292,\"start\":42291},{\"end\":42304,\"start\":42303},{\"end\":42316,\"start\":42315},{\"end\":42326,\"start\":42325},{\"end\":42696,\"start\":42695},{\"end\":42705,\"start\":42704},{\"end\":42715,\"start\":42714},{\"end\":42721,\"start\":42720},{\"end\":42734,\"start\":42733},{\"end\":42736,\"start\":42735},{\"end\":42744,\"start\":42743},{\"end\":42758,\"start\":42757},{\"end\":42767,\"start\":42766},{\"end\":42769,\"start\":42768},{\"end\":42780,\"start\":42779},{\"end\":42782,\"start\":42781},{\"end\":42793,\"start\":42792},{\"end\":43138,\"start\":43137},{\"end\":43140,\"start\":43139},{\"end\":43148,\"start\":43147},{\"end\":43157,\"start\":43156},{\"end\":43166,\"start\":43165},{\"end\":43177,\"start\":43176},{\"end\":43183,\"start\":43182},{\"end\":43192,\"start\":43191},{\"end\":43201,\"start\":43200},{\"end\":43207,\"start\":43206},{\"end\":43217,\"start\":43216},{\"end\":43219,\"start\":43218},{\"end\":43741,\"start\":43740},{\"end\":43743,\"start\":43742},{\"end\":43752,\"start\":43751},{\"end\":43762,\"start\":43761},{\"end\":43773,\"start\":43772},{\"end\":43775,\"start\":43774},{\"end\":43783,\"start\":43782},{\"end\":43789,\"start\":43788},{\"end\":43802,\"start\":43801},{\"end\":43808,\"start\":43807},{\"end\":43810,\"start\":43809},{\"end\":43820,\"start\":43819},{\"end\":43822,\"start\":43821},{\"end\":43831,\"start\":43830},{\"end\":44313,\"start\":44312},{\"end\":44315,\"start\":44314},{\"end\":44326,\"start\":44325},{\"end\":44337,\"start\":44336},{\"end\":44339,\"start\":44338},{\"end\":44348,\"start\":44347},{\"end\":44358,\"start\":44357},{\"end\":44366,\"start\":44365},{\"end\":44379,\"start\":44378},{\"end\":44392,\"start\":44391},{\"end\":44398,\"start\":44397},{\"end\":44408,\"start\":44407},{\"end\":44410,\"start\":44409},{\"end\":44839,\"start\":44838},{\"end\":44841,\"start\":44840},{\"end\":44852,\"start\":44851},{\"end\":44854,\"start\":44853},{\"end\":44863,\"start\":44862},{\"end\":44874,\"start\":44873},{\"end\":44884,\"start\":44883},{\"end\":45299,\"start\":45298},{\"end\":45308,\"start\":45307},{\"end\":45319,\"start\":45318},{\"end\":45329,\"start\":45328},{\"end\":45705,\"start\":45704},{\"end\":45713,\"start\":45712},{\"end\":45726,\"start\":45725},{\"end\":45733,\"start\":45732},{\"end\":45751,\"start\":45750},{\"end\":46063,\"start\":46062},{\"end\":46084,\"start\":46083},{\"end\":46096,\"start\":46095},{\"end\":46098,\"start\":46097},{\"end\":46109,\"start\":46108},{\"end\":46122,\"start\":46121},{\"end\":46135,\"start\":46134},{\"end\":46143,\"start\":46142},{\"end\":46157,\"start\":46156},{\"end\":46168,\"start\":46167},{\"end\":46180,\"start\":46179},{\"end\":46182,\"start\":46181},{\"end\":46767,\"start\":46766},{\"end\":46774,\"start\":46773},{\"end\":46782,\"start\":46781},{\"end\":46790,\"start\":46789},{\"end\":46797,\"start\":46796},{\"end\":46804,\"start\":46803},{\"end\":46813,\"start\":46812},{\"end\":46819,\"start\":46818},{\"end\":46827,\"start\":46826},{\"end\":47163,\"start\":47162},{\"end\":47169,\"start\":47168},{\"end\":47175,\"start\":47174},{\"end\":47182,\"start\":47181},{\"end\":47190,\"start\":47189},{\"end\":47192,\"start\":47191},{\"end\":47200,\"start\":47199},{\"end\":47215,\"start\":47214},{\"end\":47223,\"start\":47222},{\"end\":47229,\"start\":47228},{\"end\":47238,\"start\":47237},{\"end\":47738,\"start\":47737},{\"end\":47746,\"start\":47745},{\"end\":47753,\"start\":47752},{\"end\":47761,\"start\":47760},{\"end\":48103,\"start\":48102},{\"end\":48111,\"start\":48110},{\"end\":48117,\"start\":48116},{\"end\":48123,\"start\":48122},{\"end\":48129,\"start\":48128},{\"end\":48388,\"start\":48387},{\"end\":48398,\"start\":48397},{\"end\":48407,\"start\":48406},{\"end\":48418,\"start\":48417},{\"end\":48429,\"start\":48428},{\"end\":48439,\"start\":48438},{\"end\":48453,\"start\":48449},{\"end\":48465,\"start\":48464},{\"end\":48486,\"start\":48485},{\"end\":48497,\"start\":48496}]", "bib_author_last_name": "[{\"end\":31836,\"start\":31831},{\"end\":31846,\"start\":31840},{\"end\":31854,\"start\":31850},{\"end\":31862,\"start\":31858},{\"end\":31871,\"start\":31866},{\"end\":31879,\"start\":31875},{\"end\":31888,\"start\":31883},{\"end\":31900,\"start\":31892},{\"end\":31910,\"start\":31904},{\"end\":31919,\"start\":31914},{\"end\":32284,\"start\":32278},{\"end\":32293,\"start\":32288},{\"end\":32303,\"start\":32297},{\"end\":32310,\"start\":32307},{\"end\":32323,\"start\":32314},{\"end\":32338,\"start\":32329},{\"end\":32348,\"start\":32344},{\"end\":32637,\"start\":32630},{\"end\":32643,\"start\":32641},{\"end\":32652,\"start\":32647},{\"end\":32658,\"start\":32656},{\"end\":32669,\"start\":32662},{\"end\":32677,\"start\":32673},{\"end\":32687,\"start\":32681},{\"end\":32700,\"start\":32691},{\"end\":32708,\"start\":32704},{\"end\":32716,\"start\":32712},{\"end\":33103,\"start\":33098},{\"end\":33118,\"start\":33107},{\"end\":33125,\"start\":33122},{\"end\":33131,\"start\":33129},{\"end\":33140,\"start\":33135},{\"end\":33149,\"start\":33144},{\"end\":33158,\"start\":33153},{\"end\":33169,\"start\":33164},{\"end\":33188,\"start\":33173},{\"end\":33560,\"start\":33556},{\"end\":33572,\"start\":33564},{\"end\":34106,\"start\":34101},{\"end\":34116,\"start\":34112},{\"end\":34125,\"start\":34120},{\"end\":34136,\"start\":34129},{\"end\":34147,\"start\":34140},{\"end\":34159,\"start\":34153},{\"end\":34166,\"start\":34163},{\"end\":34177,\"start\":34172},{\"end\":34187,\"start\":34183},{\"end\":34198,\"start\":34194},{\"end\":34570,\"start\":34559},{\"end\":34582,\"start\":34576},{\"end\":34592,\"start\":34588},{\"end\":34602,\"start\":34596},{\"end\":34609,\"start\":34606},{\"end\":34946,\"start\":34943},{\"end\":34954,\"start\":34950},{\"end\":34969,\"start\":34958},{\"end\":34975,\"start\":34973},{\"end\":34981,\"start\":34979},{\"end\":34987,\"start\":34985},{\"end\":34995,\"start\":34991},{\"end\":35001,\"start\":34999},{\"end\":35011,\"start\":35005},{\"end\":35022,\"start\":35015},{\"end\":35371,\"start\":35366},{\"end\":35381,\"start\":35375},{\"end\":35660,\"start\":35649},{\"end\":35667,\"start\":35664},{\"end\":35680,\"start\":35671},{\"end\":35693,\"start\":35684},{\"end\":35699,\"start\":35697},{\"end\":35708,\"start\":35703},{\"end\":35716,\"start\":35712},{\"end\":35728,\"start\":35722},{\"end\":35737,\"start\":35732},{\"end\":35743,\"start\":35741},{\"end\":35997,\"start\":35995},{\"end\":36003,\"start\":36001},{\"end\":36009,\"start\":36007},{\"end\":36017,\"start\":36013},{\"end\":36026,\"start\":36021},{\"end\":36034,\"start\":36030},{\"end\":36042,\"start\":36038},{\"end\":36055,\"start\":36046},{\"end\":36064,\"start\":36059},{\"end\":36071,\"start\":36068},{\"end\":36446,\"start\":36444},{\"end\":36454,\"start\":36450},{\"end\":36460,\"start\":36458},{\"end\":36480,\"start\":36464},{\"end\":36490,\"start\":36484},{\"end\":36742,\"start\":36735},{\"end\":36753,\"start\":36746},{\"end\":36768,\"start\":36757},{\"end\":36779,\"start\":36772},{\"end\":36797,\"start\":36783},{\"end\":36809,\"start\":36801},{\"end\":36822,\"start\":36813},{\"end\":36831,\"start\":36826},{\"end\":37127,\"start\":37125},{\"end\":37135,\"start\":37131},{\"end\":37145,\"start\":37139},{\"end\":37158,\"start\":37149},{\"end\":37164,\"start\":37162},{\"end\":37172,\"start\":37168},{\"end\":37180,\"start\":37176},{\"end\":37188,\"start\":37184},{\"end\":37455,\"start\":37448},{\"end\":37468,\"start\":37461},{\"end\":37477,\"start\":37472},{\"end\":37487,\"start\":37481},{\"end\":37497,\"start\":37491},{\"end\":37510,\"start\":37503},{\"end\":37522,\"start\":37514},{\"end\":37533,\"start\":37526},{\"end\":37544,\"start\":37537},{\"end\":37556,\"start\":37548},{\"end\":37940,\"start\":37929},{\"end\":37948,\"start\":37944},{\"end\":37957,\"start\":37952},{\"end\":37966,\"start\":37961},{\"end\":37975,\"start\":37970},{\"end\":37987,\"start\":37981},{\"end\":38327,\"start\":38317},{\"end\":38337,\"start\":38331},{\"end\":38500,\"start\":38494},{\"end\":38511,\"start\":38504},{\"end\":38523,\"start\":38515},{\"end\":38764,\"start\":38762},{\"end\":38770,\"start\":38768},{\"end\":38781,\"start\":38774},{\"end\":38790,\"start\":38785},{\"end\":39082,\"start\":39080},{\"end\":39092,\"start\":39088},{\"end\":39102,\"start\":39096},{\"end\":39113,\"start\":39106},{\"end\":39126,\"start\":39117},{\"end\":39135,\"start\":39130},{\"end\":39454,\"start\":39452},{\"end\":39467,\"start\":39458},{\"end\":39473,\"start\":39471},{\"end\":39482,\"start\":39477},{\"end\":39492,\"start\":39486},{\"end\":39499,\"start\":39496},{\"end\":39508,\"start\":39503},{\"end\":39517,\"start\":39512},{\"end\":39529,\"start\":39521},{\"end\":39542,\"start\":39535},{\"end\":39888,\"start\":39885},{\"end\":39897,\"start\":39892},{\"end\":39903,\"start\":39901},{\"end\":39911,\"start\":39907},{\"end\":39919,\"start\":39915},{\"end\":39927,\"start\":39923},{\"end\":39935,\"start\":39931},{\"end\":40205,\"start\":40202},{\"end\":40212,\"start\":40209},{\"end\":40220,\"start\":40216},{\"end\":40226,\"start\":40224},{\"end\":40234,\"start\":40230},{\"end\":40505,\"start\":40499},{\"end\":40518,\"start\":40509},{\"end\":40528,\"start\":40522},{\"end\":40536,\"start\":40532},{\"end\":40545,\"start\":40540},{\"end\":40557,\"start\":40549},{\"end\":40564,\"start\":40561},{\"end\":40581,\"start\":40568},{\"end\":40590,\"start\":40585},{\"end\":40598,\"start\":40594},{\"end\":40942,\"start\":40935},{\"end\":40951,\"start\":40946},{\"end\":40961,\"start\":40955},{\"end\":40972,\"start\":40965},{\"end\":40983,\"start\":40978},{\"end\":41293,\"start\":41277},{\"end\":41371,\"start\":41365},{\"end\":41511,\"start\":41505},{\"end\":41517,\"start\":41515},{\"end\":41526,\"start\":41521},{\"end\":41537,\"start\":41530},{\"end\":41551,\"start\":41541},{\"end\":41562,\"start\":41555},{\"end\":41571,\"start\":41566},{\"end\":41582,\"start\":41575},{\"end\":41591,\"start\":41586},{\"end\":41598,\"start\":41595},{\"end\":41917,\"start\":41912},{\"end\":41928,\"start\":41921},{\"end\":41938,\"start\":41932},{\"end\":41949,\"start\":41942},{\"end\":41957,\"start\":41953},{\"end\":41968,\"start\":41961},{\"end\":41977,\"start\":41972},{\"end\":41990,\"start\":41983},{\"end\":42232,\"start\":42227},{\"end\":42243,\"start\":42236},{\"end\":42252,\"start\":42247},{\"end\":42267,\"start\":42256},{\"end\":42277,\"start\":42271},{\"end\":42289,\"start\":42281},{\"end\":42301,\"start\":42293},{\"end\":42313,\"start\":42305},{\"end\":42323,\"start\":42317},{\"end\":42333,\"start\":42327},{\"end\":42702,\"start\":42697},{\"end\":42712,\"start\":42706},{\"end\":42718,\"start\":42716},{\"end\":42731,\"start\":42722},{\"end\":42741,\"start\":42737},{\"end\":42755,\"start\":42745},{\"end\":42764,\"start\":42759},{\"end\":42777,\"start\":42770},{\"end\":42790,\"start\":42783},{\"end\":42804,\"start\":42794},{\"end\":43145,\"start\":43141},{\"end\":43154,\"start\":43149},{\"end\":43163,\"start\":43158},{\"end\":43174,\"start\":43167},{\"end\":43180,\"start\":43178},{\"end\":43189,\"start\":43184},{\"end\":43198,\"start\":43193},{\"end\":43204,\"start\":43202},{\"end\":43214,\"start\":43208},{\"end\":43225,\"start\":43220},{\"end\":43749,\"start\":43744},{\"end\":43759,\"start\":43753},{\"end\":43770,\"start\":43763},{\"end\":43780,\"start\":43776},{\"end\":43786,\"start\":43784},{\"end\":43799,\"start\":43790},{\"end\":43805,\"start\":43803},{\"end\":43817,\"start\":43811},{\"end\":43828,\"start\":43823},{\"end\":43840,\"start\":43832},{\"end\":44323,\"start\":44316},{\"end\":44334,\"start\":44327},{\"end\":44345,\"start\":44340},{\"end\":44355,\"start\":44349},{\"end\":44363,\"start\":44359},{\"end\":44376,\"start\":44367},{\"end\":44389,\"start\":44380},{\"end\":44395,\"start\":44393},{\"end\":44405,\"start\":44399},{\"end\":44416,\"start\":44411},{\"end\":44849,\"start\":44842},{\"end\":44860,\"start\":44855},{\"end\":44871,\"start\":44864},{\"end\":44881,\"start\":44875},{\"end\":44890,\"start\":44885},{\"end\":45305,\"start\":45300},{\"end\":45316,\"start\":45309},{\"end\":45326,\"start\":45320},{\"end\":45337,\"start\":45330},{\"end\":45710,\"start\":45706},{\"end\":45723,\"start\":45714},{\"end\":45730,\"start\":45727},{\"end\":45748,\"start\":45734},{\"end\":45760,\"start\":45752},{\"end\":46081,\"start\":46064},{\"end\":46093,\"start\":46085},{\"end\":46106,\"start\":46099},{\"end\":46119,\"start\":46110},{\"end\":46132,\"start\":46123},{\"end\":46140,\"start\":46136},{\"end\":46154,\"start\":46144},{\"end\":46165,\"start\":46158},{\"end\":46177,\"start\":46169},{\"end\":46187,\"start\":46183},{\"end\":46771,\"start\":46768},{\"end\":46779,\"start\":46775},{\"end\":46787,\"start\":46783},{\"end\":46794,\"start\":46791},{\"end\":46801,\"start\":46798},{\"end\":46810,\"start\":46805},{\"end\":46816,\"start\":46814},{\"end\":46824,\"start\":46820},{\"end\":46832,\"start\":46828},{\"end\":47166,\"start\":47164},{\"end\":47172,\"start\":47170},{\"end\":47179,\"start\":47176},{\"end\":47187,\"start\":47183},{\"end\":47197,\"start\":47193},{\"end\":47212,\"start\":47201},{\"end\":47220,\"start\":47216},{\"end\":47226,\"start\":47224},{\"end\":47235,\"start\":47230},{\"end\":47241,\"start\":47239},{\"end\":47743,\"start\":47739},{\"end\":47750,\"start\":47747},{\"end\":47758,\"start\":47754},{\"end\":47766,\"start\":47762},{\"end\":48108,\"start\":48104},{\"end\":48114,\"start\":48112},{\"end\":48120,\"start\":48118},{\"end\":48126,\"start\":48124},{\"end\":48133,\"start\":48130},{\"end\":48395,\"start\":48389},{\"end\":48404,\"start\":48399},{\"end\":48415,\"start\":48408},{\"end\":48426,\"start\":48419},{\"end\":48436,\"start\":48430},{\"end\":48447,\"start\":48440},{\"end\":48462,\"start\":48454},{\"end\":48483,\"start\":48466},{\"end\":48494,\"start\":48487},{\"end\":48502,\"start\":48498}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":32272,\"start\":31770},{\"attributes\":{\"doi\":\"arXiv:2007.14390\",\"id\":\"b1\"},\"end\":32624,\"start\":32274},{\"attributes\":{\"doi\":\"arXiv:2211.02701\",\"id\":\"b2\"},\"end\":33019,\"start\":32626},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4599618},\"end\":33510,\"start\":33021},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4650265},\"end\":34018,\"start\":33512},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":237536154},\"end\":34462,\"start\":34020},{\"attributes\":{\"doi\":\"arXiv:2203.13789\",\"id\":\"b6\"},\"end\":34837,\"start\":34464},{\"attributes\":{\"doi\":\"arXiv:2203.06338\",\"id\":\"b7\"},\"end\":35296,\"start\":34839},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":49675958},\"end\":35582,\"start\":35298},{\"attributes\":{\"doi\":\"arXiv:2202.06924\",\"id\":\"b9\"},\"end\":35991,\"start\":35584},{\"attributes\":{\"doi\":\"arXiv:2007.13518\",\"id\":\"b10\"},\"end\":36375,\"start\":35993},{\"attributes\":{\"doi\":\"arXiv:2110.04366\",\"id\":\"b11\"},\"end\":36684,\"start\":36377},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":59599816},\"end\":37119,\"start\":36686},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b13\"},\"end\":37444,\"start\":37121},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b14\"},\"end\":37857,\"start\":37446},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":214069261},\"end\":38258,\"start\":37859},{\"attributes\":{\"id\":\"b16\"},\"end\":38432,\"start\":38260},{\"attributes\":{\"doi\":\"arXiv:2104.08691\",\"id\":\"b17\"},\"end\":38691,\"start\":38434},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":235446706},\"end\":39026,\"start\":38693},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":59316566},\"end\":39392,\"start\":39028},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":203627027},\"end\":39879,\"start\":39394},{\"attributes\":{\"doi\":\"arXiv:2103.10385\",\"id\":\"b21\"},\"end\":40114,\"start\":39881},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":246432494},\"end\":40431,\"start\":40116},{\"attributes\":{\"doi\":\"arXiv:2007.10987\",\"id\":\"b23\"},\"end\":40856,\"start\":40433},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14955348},\"end\":41239,\"start\":40858},{\"attributes\":{\"id\":\"b25\"},\"end\":41361,\"start\":41241},{\"attributes\":{\"id\":\"b26\"},\"end\":41432,\"start\":41363},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":246426909},\"end\":41908,\"start\":41434},{\"attributes\":{\"doi\":\"arXiv:2003.00295\",\"id\":\"b28\"},\"end\":42221,\"start\":41910},{\"attributes\":{\"doi\":\"arXiv:2105.06413\",\"id\":\"b29\"},\"end\":42639,\"start\":42223},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":212747909},\"end\":43052,\"start\":42641},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":221507926},\"end\":43642,\"start\":43054},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":231803993},\"end\":44200,\"start\":43644},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220812287},\"end\":44714,\"start\":44202},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52956484},\"end\":45205,\"start\":44716},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":221980631},\"end\":45702,\"start\":45207},{\"attributes\":{\"doi\":\"arXiv:2002.06440\",\"id\":\"b36\"},\"end\":45983,\"start\":45704},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235217766},\"end\":46459,\"start\":45985},{\"attributes\":{\"id\":\"b38\"},\"end\":46670,\"start\":46461},{\"attributes\":{\"doi\":\"arXiv:2204.05011\",\"id\":\"b39\"},\"end\":47077,\"start\":46672},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":247594656},\"end\":47681,\"start\":47079},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":219878182},\"end\":48007,\"start\":47683},{\"attributes\":{\"doi\":\"arXiv:2208.12268\",\"id\":\"b42\"},\"end\":48338,\"start\":48009},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":236690571},\"end\":48782,\"start\":48340}]", "bib_title": "[{\"end\":31827,\"start\":31770},{\"end\":33094,\"start\":33021},{\"end\":33552,\"start\":33512},{\"end\":34097,\"start\":34020},{\"end\":35362,\"start\":35298},{\"end\":36731,\"start\":36686},{\"end\":37923,\"start\":37859},{\"end\":38758,\"start\":38693},{\"end\":39076,\"start\":39028},{\"end\":39448,\"start\":39394},{\"end\":40198,\"start\":40116},{\"end\":40931,\"start\":40858},{\"end\":41501,\"start\":41434},{\"end\":42693,\"start\":42641},{\"end\":43135,\"start\":43054},{\"end\":43738,\"start\":43644},{\"end\":44310,\"start\":44202},{\"end\":44836,\"start\":44716},{\"end\":45296,\"start\":45207},{\"end\":46060,\"start\":45985},{\"end\":46519,\"start\":46461},{\"end\":47160,\"start\":47079},{\"end\":47735,\"start\":47683},{\"end\":48385,\"start\":48340}]", "bib_author": "[{\"end\":31838,\"start\":31829},{\"end\":31848,\"start\":31838},{\"end\":31856,\"start\":31848},{\"end\":31864,\"start\":31856},{\"end\":31873,\"start\":31864},{\"end\":31881,\"start\":31873},{\"end\":31890,\"start\":31881},{\"end\":31902,\"start\":31890},{\"end\":31912,\"start\":31902},{\"end\":31921,\"start\":31912},{\"end\":32286,\"start\":32274},{\"end\":32295,\"start\":32286},{\"end\":32305,\"start\":32295},{\"end\":32312,\"start\":32305},{\"end\":32325,\"start\":32312},{\"end\":32340,\"start\":32325},{\"end\":32350,\"start\":32340},{\"end\":32639,\"start\":32626},{\"end\":32645,\"start\":32639},{\"end\":32654,\"start\":32645},{\"end\":32660,\"start\":32654},{\"end\":32671,\"start\":32660},{\"end\":32679,\"start\":32671},{\"end\":32689,\"start\":32679},{\"end\":32702,\"start\":32689},{\"end\":32710,\"start\":32702},{\"end\":32718,\"start\":32710},{\"end\":33105,\"start\":33096},{\"end\":33120,\"start\":33105},{\"end\":33127,\"start\":33120},{\"end\":33133,\"start\":33127},{\"end\":33142,\"start\":33133},{\"end\":33151,\"start\":33142},{\"end\":33160,\"start\":33151},{\"end\":33171,\"start\":33160},{\"end\":33190,\"start\":33171},{\"end\":33562,\"start\":33554},{\"end\":33574,\"start\":33562},{\"end\":34108,\"start\":34099},{\"end\":34118,\"start\":34108},{\"end\":34127,\"start\":34118},{\"end\":34138,\"start\":34127},{\"end\":34149,\"start\":34138},{\"end\":34161,\"start\":34149},{\"end\":34168,\"start\":34161},{\"end\":34179,\"start\":34168},{\"end\":34189,\"start\":34179},{\"end\":34200,\"start\":34189},{\"end\":34572,\"start\":34557},{\"end\":34584,\"start\":34572},{\"end\":34594,\"start\":34584},{\"end\":34604,\"start\":34594},{\"end\":34611,\"start\":34604},{\"end\":34948,\"start\":34941},{\"end\":34956,\"start\":34948},{\"end\":34971,\"start\":34956},{\"end\":34977,\"start\":34971},{\"end\":34983,\"start\":34977},{\"end\":34989,\"start\":34983},{\"end\":34997,\"start\":34989},{\"end\":35003,\"start\":34997},{\"end\":35013,\"start\":35003},{\"end\":35024,\"start\":35013},{\"end\":35373,\"start\":35364},{\"end\":35383,\"start\":35373},{\"end\":35662,\"start\":35647},{\"end\":35669,\"start\":35662},{\"end\":35682,\"start\":35669},{\"end\":35695,\"start\":35682},{\"end\":35701,\"start\":35695},{\"end\":35710,\"start\":35701},{\"end\":35718,\"start\":35710},{\"end\":35730,\"start\":35718},{\"end\":35739,\"start\":35730},{\"end\":35745,\"start\":35739},{\"end\":35999,\"start\":35993},{\"end\":36005,\"start\":35999},{\"end\":36011,\"start\":36005},{\"end\":36019,\"start\":36011},{\"end\":36028,\"start\":36019},{\"end\":36036,\"start\":36028},{\"end\":36044,\"start\":36036},{\"end\":36057,\"start\":36044},{\"end\":36066,\"start\":36057},{\"end\":36073,\"start\":36066},{\"end\":36448,\"start\":36442},{\"end\":36456,\"start\":36448},{\"end\":36462,\"start\":36456},{\"end\":36482,\"start\":36462},{\"end\":36492,\"start\":36482},{\"end\":36744,\"start\":36733},{\"end\":36755,\"start\":36744},{\"end\":36770,\"start\":36755},{\"end\":36781,\"start\":36770},{\"end\":36799,\"start\":36781},{\"end\":36811,\"start\":36799},{\"end\":36824,\"start\":36811},{\"end\":36833,\"start\":36824},{\"end\":37129,\"start\":37121},{\"end\":37137,\"start\":37129},{\"end\":37147,\"start\":37137},{\"end\":37160,\"start\":37147},{\"end\":37166,\"start\":37160},{\"end\":37174,\"start\":37166},{\"end\":37182,\"start\":37174},{\"end\":37190,\"start\":37182},{\"end\":37457,\"start\":37446},{\"end\":37470,\"start\":37457},{\"end\":37479,\"start\":37470},{\"end\":37489,\"start\":37479},{\"end\":37499,\"start\":37489},{\"end\":37512,\"start\":37499},{\"end\":37524,\"start\":37512},{\"end\":37535,\"start\":37524},{\"end\":37546,\"start\":37535},{\"end\":37558,\"start\":37546},{\"end\":37942,\"start\":37925},{\"end\":37950,\"start\":37942},{\"end\":37959,\"start\":37950},{\"end\":37968,\"start\":37959},{\"end\":37977,\"start\":37968},{\"end\":37989,\"start\":37977},{\"end\":38329,\"start\":38315},{\"end\":38339,\"start\":38329},{\"end\":38502,\"start\":38492},{\"end\":38513,\"start\":38502},{\"end\":38525,\"start\":38513},{\"end\":38766,\"start\":38760},{\"end\":38772,\"start\":38766},{\"end\":38783,\"start\":38772},{\"end\":38792,\"start\":38783},{\"end\":39084,\"start\":39078},{\"end\":39094,\"start\":39084},{\"end\":39104,\"start\":39094},{\"end\":39115,\"start\":39104},{\"end\":39128,\"start\":39115},{\"end\":39137,\"start\":39128},{\"end\":39456,\"start\":39450},{\"end\":39469,\"start\":39456},{\"end\":39475,\"start\":39469},{\"end\":39484,\"start\":39475},{\"end\":39494,\"start\":39484},{\"end\":39501,\"start\":39494},{\"end\":39510,\"start\":39501},{\"end\":39519,\"start\":39510},{\"end\":39531,\"start\":39519},{\"end\":39544,\"start\":39531},{\"end\":39890,\"start\":39883},{\"end\":39899,\"start\":39890},{\"end\":39905,\"start\":39899},{\"end\":39913,\"start\":39905},{\"end\":39921,\"start\":39913},{\"end\":39929,\"start\":39921},{\"end\":39937,\"start\":39929},{\"end\":40207,\"start\":40200},{\"end\":40214,\"start\":40207},{\"end\":40222,\"start\":40214},{\"end\":40228,\"start\":40222},{\"end\":40236,\"start\":40228},{\"end\":40507,\"start\":40497},{\"end\":40520,\"start\":40507},{\"end\":40530,\"start\":40520},{\"end\":40538,\"start\":40530},{\"end\":40547,\"start\":40538},{\"end\":40559,\"start\":40547},{\"end\":40566,\"start\":40559},{\"end\":40583,\"start\":40566},{\"end\":40592,\"start\":40583},{\"end\":40600,\"start\":40592},{\"end\":40944,\"start\":40933},{\"end\":40953,\"start\":40944},{\"end\":40963,\"start\":40953},{\"end\":40974,\"start\":40963},{\"end\":40985,\"start\":40974},{\"end\":41295,\"start\":41277},{\"end\":41373,\"start\":41365},{\"end\":41513,\"start\":41503},{\"end\":41519,\"start\":41513},{\"end\":41528,\"start\":41519},{\"end\":41539,\"start\":41528},{\"end\":41553,\"start\":41539},{\"end\":41564,\"start\":41553},{\"end\":41573,\"start\":41564},{\"end\":41584,\"start\":41573},{\"end\":41593,\"start\":41584},{\"end\":41600,\"start\":41593},{\"end\":41919,\"start\":41910},{\"end\":41930,\"start\":41919},{\"end\":41940,\"start\":41930},{\"end\":41951,\"start\":41940},{\"end\":41959,\"start\":41951},{\"end\":41970,\"start\":41959},{\"end\":41979,\"start\":41970},{\"end\":41992,\"start\":41979},{\"end\":42234,\"start\":42223},{\"end\":42245,\"start\":42234},{\"end\":42254,\"start\":42245},{\"end\":42269,\"start\":42254},{\"end\":42279,\"start\":42269},{\"end\":42291,\"start\":42279},{\"end\":42303,\"start\":42291},{\"end\":42315,\"start\":42303},{\"end\":42325,\"start\":42315},{\"end\":42335,\"start\":42325},{\"end\":42704,\"start\":42695},{\"end\":42714,\"start\":42704},{\"end\":42720,\"start\":42714},{\"end\":42733,\"start\":42720},{\"end\":42743,\"start\":42733},{\"end\":42757,\"start\":42743},{\"end\":42766,\"start\":42757},{\"end\":42779,\"start\":42766},{\"end\":42792,\"start\":42779},{\"end\":42806,\"start\":42792},{\"end\":43147,\"start\":43137},{\"end\":43156,\"start\":43147},{\"end\":43165,\"start\":43156},{\"end\":43176,\"start\":43165},{\"end\":43182,\"start\":43176},{\"end\":43191,\"start\":43182},{\"end\":43200,\"start\":43191},{\"end\":43206,\"start\":43200},{\"end\":43216,\"start\":43206},{\"end\":43227,\"start\":43216},{\"end\":43751,\"start\":43740},{\"end\":43761,\"start\":43751},{\"end\":43772,\"start\":43761},{\"end\":43782,\"start\":43772},{\"end\":43788,\"start\":43782},{\"end\":43801,\"start\":43788},{\"end\":43807,\"start\":43801},{\"end\":43819,\"start\":43807},{\"end\":43830,\"start\":43819},{\"end\":43842,\"start\":43830},{\"end\":44325,\"start\":44312},{\"end\":44336,\"start\":44325},{\"end\":44347,\"start\":44336},{\"end\":44357,\"start\":44347},{\"end\":44365,\"start\":44357},{\"end\":44378,\"start\":44365},{\"end\":44391,\"start\":44378},{\"end\":44397,\"start\":44391},{\"end\":44407,\"start\":44397},{\"end\":44418,\"start\":44407},{\"end\":44851,\"start\":44838},{\"end\":44862,\"start\":44851},{\"end\":44873,\"start\":44862},{\"end\":44883,\"start\":44873},{\"end\":44892,\"start\":44883},{\"end\":45307,\"start\":45298},{\"end\":45318,\"start\":45307},{\"end\":45328,\"start\":45318},{\"end\":45339,\"start\":45328},{\"end\":45712,\"start\":45704},{\"end\":45725,\"start\":45712},{\"end\":45732,\"start\":45725},{\"end\":45750,\"start\":45732},{\"end\":45762,\"start\":45750},{\"end\":46083,\"start\":46062},{\"end\":46095,\"start\":46083},{\"end\":46108,\"start\":46095},{\"end\":46121,\"start\":46108},{\"end\":46134,\"start\":46121},{\"end\":46142,\"start\":46134},{\"end\":46156,\"start\":46142},{\"end\":46167,\"start\":46156},{\"end\":46179,\"start\":46167},{\"end\":46189,\"start\":46179},{\"end\":46773,\"start\":46766},{\"end\":46781,\"start\":46773},{\"end\":46789,\"start\":46781},{\"end\":46796,\"start\":46789},{\"end\":46803,\"start\":46796},{\"end\":46812,\"start\":46803},{\"end\":46818,\"start\":46812},{\"end\":46826,\"start\":46818},{\"end\":46834,\"start\":46826},{\"end\":47168,\"start\":47162},{\"end\":47174,\"start\":47168},{\"end\":47181,\"start\":47174},{\"end\":47189,\"start\":47181},{\"end\":47199,\"start\":47189},{\"end\":47214,\"start\":47199},{\"end\":47222,\"start\":47214},{\"end\":47228,\"start\":47222},{\"end\":47237,\"start\":47228},{\"end\":47243,\"start\":47237},{\"end\":47745,\"start\":47737},{\"end\":47752,\"start\":47745},{\"end\":47760,\"start\":47752},{\"end\":47768,\"start\":47760},{\"end\":48110,\"start\":48102},{\"end\":48116,\"start\":48110},{\"end\":48122,\"start\":48116},{\"end\":48128,\"start\":48122},{\"end\":48135,\"start\":48128},{\"end\":48397,\"start\":48387},{\"end\":48406,\"start\":48397},{\"end\":48417,\"start\":48406},{\"end\":48428,\"start\":48417},{\"end\":48438,\"start\":48428},{\"end\":48449,\"start\":48438},{\"end\":48464,\"start\":48449},{\"end\":48485,\"start\":48464},{\"end\":48496,\"start\":48485},{\"end\":48504,\"start\":48496}]", "bib_venue": "[{\"end\":33792,\"start\":33683},{\"end\":39210,\"start\":39182},{\"end\":47392,\"start\":47326},{\"end\":31999,\"start\":31921},{\"end\":32422,\"start\":32366},{\"end\":32790,\"start\":32734},{\"end\":33245,\"start\":33190},{\"end\":33681,\"start\":33574},{\"end\":34215,\"start\":34200},{\"end\":34555,\"start\":34464},{\"end\":34939,\"start\":34839},{\"end\":35427,\"start\":35383},{\"end\":35645,\"start\":35584},{\"end\":36152,\"start\":36089},{\"end\":36440,\"start\":36377},{\"end\":36877,\"start\":36833},{\"end\":37256,\"start\":37206},{\"end\":37622,\"start\":37574},{\"end\":38033,\"start\":37989},{\"end\":38313,\"start\":38260},{\"end\":38490,\"start\":38434},{\"end\":38836,\"start\":38792},{\"end\":39180,\"start\":39137},{\"end\":39605,\"start\":39544},{\"end\":40255,\"start\":40236},{\"end\":40495,\"start\":40433},{\"end\":41023,\"start\":40985},{\"end\":41275,\"start\":41241},{\"end\":41649,\"start\":41600},{\"end\":42039,\"start\":42008},{\"end\":42398,\"start\":42351},{\"end\":42826,\"start\":42806},{\"end\":43316,\"start\":43227},{\"end\":43897,\"start\":43842},{\"end\":44436,\"start\":44418},{\"end\":44933,\"start\":44892},{\"end\":45428,\"start\":45339},{\"end\":45819,\"start\":45778},{\"end\":46195,\"start\":46189},{\"end\":46543,\"start\":46521},{\"end\":46764,\"start\":46672},{\"end\":47324,\"start\":47243},{\"end\":47829,\"start\":47768},{\"end\":48100,\"start\":48009},{\"end\":48530,\"start\":48504}]"}}}, "year": 2023, "month": 12, "day": 17}