{"id": 44131945, "updated": "2023-09-30 07:57:18.865", "metadata": {"title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors", "authors": "[{\"first\":\"Zhun\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Varun\",\"last\":\"Lakshminarasimhan\",\"middle\":[\"Bharadhwaj\"]},{\"first\":\"Paul\",\"last\":\"Liang\",\"middle\":[\"Pu\"]},{\"first\":\"Amir\",\"last\":\"Zadeh\",\"middle\":[]},{\"first\":\"Louis-Philippe\",\"last\":\"Morency\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2018, "month": 5, "day": 31}, "abstract": "Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1806.00064", "mag": "2964346351", "acl": "P18-1209", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/MorencyLZLSL18", "doi": "10.18653/v1/p18-1209"}}, "content": {"source": {"pdf_hash": "26dca039a96ce0baa99064c7126ae41aa74d204f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1806.00064v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P18-1209.pdf", "status": "HYBRID"}}, "grobid": {"id": "4c7b00940b33dde657d7bb654fed328d87b7a25f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/26dca039a96ce0baa99064c7126ae41aa74d204f.txt", "contents": "\nEfficient Low-rank Multimodal Fusion with Modality-Specific Factors\n\n\nZhun Liu zhunl@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nYing Shen yshen2@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nVarun Bharadhwaj Lakshminarasimhan \nSchool of Computer Science\nCarnegie Mellon University\n\n\nPaul Pu Liang pliang@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nAmir Zadeh \nSchool of Computer Science\nCarnegie Mellon University\n\n\nLouis-Philippe Morency morency@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon University\n\n\nEfficient Low-rank Multimodal Fusion with Modality-Specific Factors\n\nMultimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Lowrank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.\n\nIntroduction\n\nMultimodal research has shown great progress in a variety of tasks as an emerging research field of artificial intelligence. Tasks such as speech recognition (Yuhas et al., 1989), emotion recognition, (De Silva et al., 1997), (Chen et al., 1998), (W\u00f6llmer et al., 2013), sentiment analysis, (Morency et al., 2011) * equal contributions as well as speaker trait analysis and media description (Park et al., 2014a) have seen a great boost in performance with developments in multimodal research.\n\nHowever, a core research challenge yet to be solved in this domain is multimodal fusion. The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions. In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity. Some of the recent attempts (Fukui et al., 2016),  at multimodal fusion investigate the use of tensors for multimodal representation and show significant improvement in performance. Unfortunately, they are often constrained by the exponential increase of cost in computation and memory introduced by using tensor representations. This heavily restricts the applicability of these models, especially when we have more than two views of modalities in the dataset.\n\nIn this paper, we propose the Low-rank Multimodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on performance. The overall architecture is shown in Figure 1. We evaluated our approach with experiments on three multimodal tasks using public datasets and compare its performance with state-of-the-art models. We also study how different low-rank settings impact the performance of our model and show that our model performs robustly within a wide range of rank settings. Finally, we perform an analysis of the impact of our method on the number of parameters and run-time with comparison to other fusion methods. Through theoretical analysis, we show that our model can scale linearly in the number of modalities, and our experiments also show a corresponding speedup in training when compared with  Figure 1: Overview of our Low-rank Multimodal Fusion model structure: LMF first obtains the unimodal representation z a , z v , z l by passing the unimodal inputs x a , x v , x l into three sub-embedding networks f v , f a , f l respectively. LMF produces the multimodal output representation by performing low-rank multimodal fusion with modality-specific factors. The multimodal representation can be then used for generating prediction tasks.\n\nother tensor-based models.\n\nThe main contributions of our paper are as follows:\n\n\u2022 We propose the Low-rank Multimodal Fusion method for multimodal fusion that can scale linearly in the number of modalities.\n\n\u2022 We show that our model compares to state-ofthe-art models in performance on three multimodal tasks evaluated on public datasets.\n\n\u2022 We show that our model is computationally efficient and has fewer parameters in comparison to previous tensor-based methods.\n\n\nRelated Work\n\nMultimodal fusion enables us to leverage complementary information present in multimodal data, thus discovering the dependency of information on multiple modalities. Previous studies have shown that more effective fusion methods translate to better performance in models, and there's been a wide range of fusion methods. Early fusion is a technique that uses feature concatenation as the method of fusion of different views. Several works that use this method of fusion (Poria et al., 2016) , (Wang et al., 2016) use input-level feature concatenation and use the concatenated features as input, sometimes even removing the temporal dependency present in the modalities (Morency et al., 2011). The drawback of this class of method is that although it achieves fusion at an early stage, intra-modal interactions are potentially suppressed, thus losing out on the context and temporal dependencies within each modality.\n\nOn the other hand, late fusion builds separate models for each modality and then integrates the outputs together using a method such as majority voting or weighted averaging (Wortwein and Scherer, 2017), (Nojavanasghari et al., 2016). Since separate models are built for each modality, inter-modal interactions are usually not modeled effectively.\n\nGiven these shortcomings, more recent work focuses on intermediate approaches that model both intra-and inter-modal dynamics. Fukui et al. (2016) proposes to use Compact Bilinear Pooling over the outer product of visual and linguistic representations to exploit the interactions between vision and language for visual question answering. Similar to the idea of exploiting interactions,  proposes Tensor Fusion Network, which computes the outer product between unimodal representations from three different modalities to compute a tensor representation. These methods exploit tensor representations to model inter-modality interactions and have shown a great success. However, such methods suffer from exponentially increasing computational complexity, as the outer product over multiple modalities results in extremely high dimensional tensor representations.\n\nFor unimodal data, the method of low-rank tensor approximation has been used in a variety of applications to implement more efficient tensor operations. Razenshteyn et al. (2016) proposes a modified weighted version of low-rank approximation, and Koch and Lubich (2010) applies the method towards temporally dependent data to obtain lowrank approximations. As for applications, Lei et al. (2014) proposes a low-rank tensor technique for dependency parsing while Wang and Ahuja (2008) uses the method of low-rank approximation applied directly on multidimensional image data (Datumas-is representation) to enhance computer vision applications. Hu et al. (2017) proposes a low-rank tensor-based fusion framework to improve the face recognition performance using the fusion of facial attribute information. However, none of these previous work aims to apply low-rank tensor techniques for multimodal fusion.\n\nOur Low-rank Multimodal Fusion method provides a much more efficient method to compute tensor-based multimodal representations with much fewer parameters and computational complexity. The efficiency and performance of our approach are evaluated on different downstream tasks, namely sentiment analysis, speaker-trait recognition and emotion recognition.\n\n\nLow-rank Multimodal Fusion\n\nIn this section, we start by formulating the problem of multimodal fusion and introducing fusion methods based on tensor representations. Tensors are powerful in their expressiveness but do not scale well to a large number of modalities. Our proposed model decomposes the weights into low-rank factors, which reduces the number of parameters in the model. This decomposition can be performed efficiently by exploiting the parallel decomposition of low-rank weight tensor and input tensor to compute tensor-based fusion. Our method is able to scale linearly with the number of modalities.\n\n\nMultimodal Fusion using Tensor Representations\n\nIn this paper, we formulate multimodal fusion as a multilinear function which are encoding unimodal information of the M different modalities, the goal of multimodal fusion is to integrate the unimodal representations into one compact multimodal representation for downstream tasks. Tensor representation is one successful approach for multimodal fusion. It first requires a transformation of the input representations into a highdimensional tensor and then mapping it back to a lower-dimensional output vector space. Previous works have shown that this method is more effective than simple concatenation or pooling in terms of capturing multimodal interactions , (Fukui et al., 2016). Tensors are usually created by taking the outer product over the input modalities. In addition, in order to be able to model the interactions between any subset of modalities using one tensor,  proposed a simple extension to append 1s to the unimodal representations before taking the outer product. The input tensor Z formed by the unimodal representation is computed by:\nf \u2236 V 1 \u00d7 V 2 \u00d7 ... \u00d7 V M \u2192 H where V 1 , V 2 ,Z = M \u2297 m=1 z m , z m \u2208 R dm(1)\nwhere \u2297 M m=1 denotes the tensor outer product over a set of vectors indexed by m, and z m is the input representation with appended 1s.\n\nThe input tensor Z \u2208 R d 1 \u00d7d 2 \u00d7...d M is then passed through a linear layer g(\u22c5) to to produce a vector representation:\nh = g(Z; W, b) = W \u22c5 Z + b, h, b \u2208 R dy (2)\nwhere W is the weight of this layer and b is the bias. With Z being an order-M tensor (where M is the number of input modalities), the weight W will naturally be a tensor of order-(M + 1) in\nR d 1 \u00d7d 2 \u00d7...\u00d7d M \u00d7d h .\nThe extra (M + 1)-th dimension corresponds to the size of the output representation d h . In the tensor dot product W \u22c5 Z, the weight tensor W can be then viewed as d h order-M tensors.\n\nIn other words, the weight W can be partitioned into W k \u2208 R d 1 \u00d7...\u00d7d M , k = 1, ..., d h . Each W k contributes to one dimension in the output vector h, i.e. h k = W k \u22c5 Z. This interpretation of tensor fusion is illustrated in Figure 2 for the bi-modal case.\n\nOne of the main drawbacks of tensor fusion is that we have to explicitly create the highdimensional tensor Z. The dimensionality of Z Figure 2: Tensor fusion via tensor outer product will increase exponentially with the number of modalities as \u220f M m=1 d m . The number of parameters to learn in the weight tensor W will also increase exponentially. This not only introduces a lot of computation but also exposes the model to risks of overfitting.\nM R ? \u2a02 E E ! > ! # E \u210e =\n\nLow-rank Multimodal Fusion with Modality-Specific Factors\n\nAs a solution to the problems of tensor-based fusion, we propose Low-rank Multimodal Fusion (LMF). LMF parameterizes g(\u22c5) from Equation 2 with a set of modality-specific low-rank factors that can be used to recover a low-rank weight tensor, in contrast to the full tensor W. Moreover, we show that by decomposing the weight into a set of low-rank factors, we can exploit the fact that the tensor Z actually decomposes into {z m } M m=1 , which allows us to directly compute the output h without explicitly tensorizing the unimodal representations. LMF reduces the number of parameters as well as the computation complexity involved in tensorization from being exponential in M to linear.\n\n\nLow-rank Weight Decomposition\n\nThe idea of LMF is to decompose the weight tensor W into M sets of modality-specific factors. However, since W itself is an order-(M + 1) tensor, commonly used methods for decomposition will result in M + 1 parts. Hence, we still adopt the view introduced in Section 3.1 that W is formed by\nd h order-M tensors W k \u2208 R d 1 \u00d7...\u00d7d M , k = 1, ..., d h stacked together. We can then decompose each W k separately.\nFor an order-M tensor W k \u2208 R d 1 \u00d7...\u00d7d M , there always exists an exact decomposition into vectors in the form of:\nW k = R i=1 M \u2297 m=1 w (i) m,k , w (i) m,k \u2208 R d m(3)\nThe minimal R that makes the decomposition valid is called the rank of the tensor. The vector sets\n{{w (i) m,k } M m=1 } R i=1\nare called the rank R decomposition factors of the original tensor.\n\nIn LMF, we start with a fixed rank r, and parameterize the model with r decomposition factors {{w\n(i) m,k } M m=1 } r i=1 , k = 1, .\n.., d h that can be used to reconstruct a low-rank version of these W k .\n\nWe can regroup and concatenate these vectors into M modality-specific low-rank factors. Let w\n(i) m = [w (i) m,1 , w (i) m,2 , ..., w (i) m,d h ], then for modality m, {w (i) m } r i=1\nis its corresponding low-rank factors. And we can recover a low-rank weight tensor by:\nW = r i=1 M \u2297 m=1 w (i) m(4)\nHence equation 2 can be computed by\nh = r i=1 M \u2297 m=1 w (i) m \u22c5 Z(5)\nNote that for all m, w (i) m \u2208 R dm\u00d7d h shares the same size for the second dimension. We define their outer product to be over only the dimensions that are not shared: w\n(i) m \u2297 w (i) n \u2208 R dm\u00d7dn\u00d7d h .\nA bimodal example of this procedure is illustrated in Figure 3.\n\nNevertheless, by introducing the low-rank factors, we now have to compute the reconstruction\nof W = \u2211 r i=1 \u2297 M m=1 w (i)\nm for the forward computation. Yet this introduces even more computation.\n\n\nEfficient Low-rank Fusion Exploiting Parallel Decomposition\n\nIn this section, we will introduce an efficient procedure for computing h, exploiting the fact that tensor Z naturally decomposes into the original input {z m } M m=1 , which is parallel to the modalityspecific low-rank factors. In fact, that is the main reason why we want to decompose the weight tensor into M modality-specific factors.\n\nUsing the fact that Z = \u2297 M m=1 z m , we can simplify equation 5: where \u039b M m=1 denotes the element-wise product over a sequence of tensors: \u039b 3\nh = r i=1 M \u2297 m=1 w (i) m \u22c5 Z = r i=1 M \u2297 m=1 w (i) m \u22c5 Z = r i=1 M \u2297 m=1 w (i) m \u22c5 M \u2297 m=1 z m = M \u039b m=1 r i=1 w (i) m \u22c5 z m(t=1 x t = x 1 \u25cb x 2 \u25cb x 3 .\nAn illustration of the trimodal case of equation 6 is shown in Figure 1. We can also derive equation 6 for a bimodal case to clarify what it does:\nh = r i=1 w (i) a \u2297 w (i) v \u22c5 Z = r i=1 w (i) a \u22c5 z a \u25cb r i=1 w (i) v \u22c5 z v(7)\nAn important aspect of this simplification is that it exploits the parallel decomposition of both Z and W, so that we can compute h without actually creating the tensor Z from the input representations z m . In addition, different modalities are decoupled in the simplified computation of h, which allows for easy generalization of our approach to an arbitrary number of modalities. Adding a new modality can be simply done by adding another set of modality-specific factors and extend Equation 7. Last but not least, Equation 6 consists of fully differentiable operations, which enables the parameters {w (i) m } r i=1 m = 1, ..., M to be learned end-to-end via back-propagation.\n\nUsing Equation 6, we can compute h directly from input unimodal representations and their modal-specific decomposition factors, avoiding the weight-lifting of computing the large input tensor Z and W, as well as the r linear transformation. Instead, the input tensor and subsequent linear projection are computed implicitly together in Equation 6, and this is far more efficient than the original method described in Section 3.1. Indeed, LMF reduces the computation complexity of tensorization and fusion from O(d y \u220f M m=1 d m ) to O(d y \u00d7 r \u00d7 \u2211 M m=1 d m ). In practice, we use a slightly different form of Equation 6, where we concatenate the low-rank factors into M order-3 tensors and swap the order in which we do the element-wise product and summation:\nh = r i=1 M \u039b m=1 w (1) m , w (2) m , ..., w (r) m \u22c5\u1e91 m i,\u2236(8)\nand now the summation is done along the first dimension of the bracketed matrix.\n\n[\u22c5] i,\u2236 indicates the i-th slice of a matrix. In this way, we can parameterize the model with M order-3 tensors, instead of parameterizing with sets of vectors.\n\n\nExperimental Methodology\n\nWe compare LMF with previous state-of-the-art baselines, and we use the Tensor Fusion Networks (TFN) ) as a baseline for tensorbased approaches, which has the most similar structure with us except that it explicitly forms the large multi-dimensional tensor for fusion across different modalities. We design our experiments to better understand the characteristics of LMF. Our goal is to answer the following four research questions:\n\n(1) Impact of Multimodal Low-rank Fusion: Direct comparison between our proposed LMF model and the previous TFN model.\n\n(2) Comparison with the State-of-the-art: We evaluate the performance of LMF and state-of-theart baselines on three different tasks and datasets.\n\n(3) Complexity Analysis: We study the modal complexity of LMF and compare it with the TFN model.\n\n(4) Rank Settings: We explore performance of LMF with different rank settings.\n\nThe results of these experiments are presented in Section 5.\n\n\nDatasets\n\nWe perform our experiments on the following multimodal datasets, CMU-MOSI (Zadeh et al., 2016a)  POM (Park et al., 2014b), and IEMOCAP (Busso et al., 2008) for sentiment analysis, speaker traits recognition, and emotion recognition task, where the goal is to identify speakers emotions based on the speakers' verbal and nonverbal behaviors. \n\n\nFeatures\n\nEach dataset consists of three modalities, namely language, visual, and acoustic modalities. To reach the same time alignment across modalities, we perform word alignment using P2FA (Yuan and Liberman, 2008) which allows us to align the three modalities at the word granularity. We calculate the visual and acoustic features by taking the average of their feature values over the word time interval . Language We use pre-trained 300-dimensional Glove word embeddings (Pennington et al., 2014) to encode a sequence of transcribed words into a sequence of word vectors.\n\nVisual The library Facet 1 is used to extract a set of visual features for each frame (sampled at 30Hz) including 20 facial action units, 68 facial landmarks, head pose, gaze tracking and HOG features (Zhu et al., 2006). Acoustic We use COVAREP acoustic analysis framework (Degottex et al., 2014) to extract a set of low-level acoustic features, including 12 Mel frequency cepstral coefficients (MFCCs), pitch, voiced/unvoiced segmentation, glottal source, peak slope, and maxima dispersion quotient features.\n\n\nModel Architecture\n\nIn order to compare our fusion method with previous work, we adopt a simple and straightforward model architecture 2 for extracting unimodal representations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from unimodal input features x a , x v , x l . For acoustic and visual modality, the sub-embedding network is a simple 2-layer feed-forward neural network, and for language modality, we used an LSTM (Hochreiter and Schmidhuber, 1997) to extract representations. The model architecture is illustrated in Figure 1.\n\n\nBaseline Models\n\nWe compare the performance of LMF to the following baselines and state-of-the-art models in multimodal sentiment analysis, speaker trait recognition, and emotion recognition. Support Vector Machines Support Vector Machines (SVM) (Cortes and Vapnik, 1995) is a widely used non-neural classifier. This baseline is trained on the concatenated multimodal features for classification or regression task (P\u00e9rez-Rosas et al., 2013), (Park et al., 2014a), (Zadeh et al., 2016b). Deep Fusion The Deep Fusion model (DF) (Nojavanasghari et al., 2016) trains one deep neural model for each modality and then combine the output of each modality network with a joint neural network. Tensor Fusion Network The Tensor Fusion Network (TFN)  explicitly models view-specific and cross-view dynamics by creating a multi-dimensional tensor that captures uni-modal, bimodal and trimodal interactions across three modalities. Memory Fusion Network The Memory Fusion Network (MFN) (Zadeh et al., 2018a) accounts for view-specific and cross-view interactions and continuously models them through time with a special attention mechanism and summarized through time with a Multi-view Gated Memory. Bidirectional Contextual LSTM The Bidirectional Contextual LSTM (BC-LSTM) , (Fukui et al., 2016) performs contextdependent fusion of multimodal data. Multi-View LSTM The Multi-View LSTM (MV-LSTM) (Rajagopalan et al., 2016) aims to capture both modality-specific and cross-modality interactions from multiple modalities by partitioning the memory cell and the gates corresponding to multiple modalities.\n\n\nMulti-attention Recurrent Network\n\nThe Multiattention Recurrent Network (MARN) (Zadeh et al., 2018b) explicitly models interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory called the Long-short Term Hybrid Memory (LSTHM).\n\n\nEvaluation Metrics\n\nMultiple evaluation tasks are performed during our evaluation: multi-class classification and regression. The multi-class classification task is applied to all three multimodal datasets, and the regression task is applied to the CMU-MOSI and the POM dataset. For binary classification and multiclass classification, we report F1 score and accuracy Acc\u2212k where k denotes the number of classes. Specifically, Acc\u22122 stands for the binary classification. For regression, we report Mean Absolute Error (MAE) and Pearson correlation (Corr). Higher values denote better performance for all metrics except for MAE.\n\n\nResults and Discussion\n\nIn this section, we present and discuss the results from the experiments designed to study the research questions introduced in section 4.\n\n\nImpact of Low-rank Multimodal Fusion\n\nIn this experiment, we compare our model directly with the TFN model since it has the most similar structure to our model, except that TFN explicitly forms the multimodal tensor fusion. The com-parison reported in the last two rows of Table 2 demonstrates that our model significantly outperforms TFN across all datasets and metrics. This competitive performance of LMF compared to TFN emphasizes the advantage of Low-rank Multimodal Fusion.\n\n\nComparison with the State-of-the-art\n\nWe compare our model with the baselines and stateof-the-art models for sentiment analysis, speaker traits recognition and emotion recognition. Results are shown in Table 2. LMF is able to achieve competitive and consistent results across all datasets.\n\nOn the multimodal sentiment regression task, LMF outperforms the previous state-of-the-art model on MAE and Corr. Note the multiclass accuracy is calculated by mapping the range of continuous sentiment values into a set of intervals that are used as discrete classes.\n\nOn the multimodal speaker traits Recognition task, we report the average evaluation score over 16 speaker traits and shows that our model achieves the state-of-the-art performance over all three evaluation metrics on the POM dataset.\n\nOn the multimodal emotion recognition task, our model achieves better results compared to the stateof-the-art models across all emotions on the F1 score. F1-emotion in the evaluation metrics indicates the F1 score for a certain emotion class.\n\n\nComplexity Analysis\n\nTheoretically, the model complexity of our fusion method is O(d y \u00d7 r \u00d7 \u2211 M m=1 d m ) compared to O(d y \u220f M m=1 d m ) of TFN from Section 3.1. In practice, we calculate the total number of parameters used in each model, where we choose M = 3, d 1 = 32, d 2 = 32, d 3 = 64, r = 4, d y = 1. Under this hyper-parameter setting, our model contains about 1.1e6 parameters while TFN contains about 12.5e6 parameters, which is nearly 11 times more. Note that, the number of parameters above counts not only the parameters in the multimodal fusion stage but also the parameters in the subnetworks.\n\nFurthermore, we evaluate the computational complexity of LMF by measuring the training and testing speeds between LMF and TFN. Table 3 illustrates the impact of Low-rank Multimodal Fusion on the training and testing speeds compared with TFN model. Here we set rank to be 4 since it can generally achieve fairly competent performance.   Based on these results, performing a low-rank multimodal fusion with modality-specific low-rank factors significantly reduces the amount of time needed for training and testing the model. On an NVIDIA Quadro K4200 GPU, LMF trains with an average frequency of 1134.82 IPS (data point inferences per second) while the TFN model trains at an average of 340.74 IPS.\n\n\nRank Settings\n\nTo evaluate the impact of different rank settings for our LMF model, we measure the change in performance on the CMU-MOSI dataset while varying Figure 4: The Impact of different rank settings on Model Performance: As the rank increases, the results become unstable and low rank is enough in terms of the mean absolute error.\n\nthe number of rank. The results are presented in Figure 4. We observed that as the rank increases, the training results become more and more unstable and that using a very low rank is enough to achieve fairly competent performance.\n\n\nConclusion\n\nIn this paper, we introduce a Low-rank Multimodal Fusion method that performs multimodal fusion with modality-specific low-rank factors. LMF scales linearly in the number of modalities. LMF achieves competitive results across different multimodal tasks. Furthermore, LMF demonstrates a significant decrease in computational complexity from exponential to linear time. In practice, LMF effectively improves the training and testing efficiency compared to TFN which performs multimodal fusion with tensor representations.\n\nFuture work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive.\n\n\n..., V M are the vector spaces of input modalities and H is the output vector space. Given a set of vector representations, {z m } M m=1\n\nFigure 3 :\n3Decomposing weight tensor into low-rank factors (See Section 3.2.1 for details.)\n\nTable 1 :\n1The speaker independent data splits for training, validation, and test sets.\n\n\ntraining, validation, and test sets such that the splits are speaker independent, i.e., no identical speakers from the training set are present in the test sets.Table 1illustrates the data splits for all datasets in detail.CMU-MOSI The CMU-MOSI dataset is a collec-\ntion of 93 opinion videos from YouTube movie \nreviews. Each video consists of multiple opinion \nsegments and each segment is annotated with the \nsentiment in the range [-3,3], where -3 indicates \nhighly negative and 3 indicates highly positive. \nPOM The POM dataset is composed of 903 movie \nreview videos. Each video is annotated with the fol-\nlowing speaker traits: confident, passionate, voice \npleasant, dominant, credible, vivid, expertise, enter-\ntaining, reserved, trusting, relaxed, outgoing, thor-\nough, nervous, persuasive and humorous. \nIEMOCAP The IEMOCAP dataset is a collection \nof 151 videos of recorded dialogues, with 2 speak-\ners per session for a total of 302 videos across the \ndataset. Each segment is annotated for the presence \nof 9 emotions (angry, excited, fear, sad, surprised, \nfrustrated, happy, disappointed and neutral). \nTo evaluate model generalization, all datasets are \nsplit into \n\nTable 2 :\n2Results for sentiment analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait recognition on POM. Best results are highlighted in bold.Model \nTraining Speed (IPS) \nTesting Speed (IPS) \nTFN \n340.74 \n1177.17 \nLMF \n1134.82 \n2249.90 \n\n\n\nTable 3 :\n3Comparison of the training and testing speeds between TFN and LMF. The second and the third columns indicate the number of data point inferences per second (IPS) during training and testing time respectively. Both models are implemented in the same framework with equivalent running environment.\ngoo.gl/1rh1JN 2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion\nAcknowledgementsThis material is based upon work partially supported by the National Science Foundation (Award # 1833355) and Oculus VR. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Oculus VR, and no official endorsement should be inferred.\nIemocap: Interactive emotional dyadic motion capture database. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, Shrikanth S Narayanan, 10.1007/s10579-008-9076-6Journal of Language Resources and Evaluation. 424Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jean- nette Chang, Sungbok Lee, and Shrikanth S. Narayanan. 2008. Iemocap: Interactive emotional dyadic motion capture database. Journal of Lan- guage Resources and Evaluation 42(4):335-359. https://doi.org/10.1007/s10579-008-9076-6.\n\nMultimodal human emotion/expression recognition. Thomas S Lawrence S Chen, Tsutomu Huang, Ryohei Miyasato, Nakatsu, Third IEEE International Conference on. IEEE. Automatic Face and Gesture RecognitionLawrence S Chen, Thomas S Huang, Tsutomu Miyasato, and Ryohei Nakatsu. 1998. Multimodal human emotion/expression recognition. In Auto- matic Face and Gesture Recognition, 1998. Proceed- ings. Third IEEE International Conference on. IEEE, pages 366-371.\n\nMultimodal sentiment analysis with wordlevel fusion and reinforcement learning. Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Baltru\u0161aitis, Amir Zadeh, Louis-Philippe Morency, 10.1145/3136755.3136801Proceedings of the 19th ACM International Conference on Multimodal Interaction. the 19th ACM International Conference on Multimodal InteractionNew York, NY, USAACMMinghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal- tru\u0161aitis, Amir Zadeh, and Louis-Philippe Morency. 2017. Multimodal sentiment analysis with word- level fusion and reinforcement learning. In Pro- ceedings of the 19th ACM International Con- ference on Multimodal Interaction. ACM, New York, NY, USA, ICMI 2017, pages 163-171. https://doi.org/10.1145/3136755.3136801.\n\nSupportvector networks. Corinna Cortes, Vladimir Vapnik, Machine learning. 203Corinna Cortes and Vladimir Vapnik. 1995. Support- vector networks. Machine learning 20(3):273-297.\n\nFacial emotion recognition using multi-modal information. Liyanage C De Silva, Tsutomu Miyasato, Ryohei Nakatsu, Information, Communications and Signal Processing. 1Liyanage C De Silva, Tsutomu Miyasato, and Ryohei Nakatsu. 1997. Facial emotion recognition using multi-modal information. In Information, Commu- nications and Signal Processing, 1997. ICICS., Pro- ceedings of 1997 International Conference on. IEEE, volume 1, pages 397-401.\n\nCovarepa collaborative voice analysis repository for speech technologies. Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, Stefan Scherer, 2014 IEEE International Conference on. IEEE. Acoustics, Speech and Signal ProcessingGilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. 2014. Covarepa collabo- rative voice analysis repository for speech technolo- gies. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, pages 960-964.\n\nMultimodal compact bilinear pooling for visual question answering and visual grounding. Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, arXiv:1606.01847arXiv preprintAkira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint arXiv:1606.01847 .\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput. 9(8):1735- 1780. https://doi.org/10.1162/neco.1997.9.8.1735.\n\nAttribute-enhanced face recognition with neural tensor fusion networks. Guosheng Hu, Yang Hua, Yang Yuan, Zhihong Zhang, Zheng Lu, S Sankha, Timothy M Mukherjee, Hospedales, Yongxin Neil M Robertson, Yang, Guosheng Hu, Yang Hua, Yang Yuan, Zhihong Zhang, Zheng Lu, Sankha S Mukherjee, Timothy M Hospedales, Neil M Robertson, and Yongxin Yang. 2017. Attribute-enhanced face recognition with neu- ral tensor fusion networks.\n\nDynamical tensor approximation. Othmar Koch, Christian Lubich, SIAM Journal on Matrix Analysis and Applications. 315Othmar Koch and Christian Lubich. 2010. Dynami- cal tensor approximation. SIAM Journal on Matrix Analysis and Applications 31(5):2360-2375.\n\nLow-rank tensors for scoring dependency structures. Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, Tommi Jaakkola, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics1Long Papers)Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scor- ing dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers). vol- ume 1, pages 1381-1391.\n\nTowards multimodal sentiment analysis: Harvesting opinions from the web. Louis-Philippe Morency, Rada Mihalcea, Payal Doshi, Proceedings of the 13th International Conference on Multimodal Interactions. the 13th International Conference on Multimodal InteractionsACMLouis-Philippe Morency, Rada Mihalcea, and Payal Doshi. 2011. Towards multimodal sentiment anal- ysis: Harvesting opinions from the web. In Proceed- ings of the 13th International Conference on Multi- modal Interactions. ACM, pages 169-176.\n\nDeep multimodal fusion for persuasiveness prediction. Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, Tadas Baltru\u0161aitis, Louis-Philippe Morency, Proceedings of the 18th ACM International Conference on Multimodal Interaction. the 18th ACM International Conference on Multimodal InteractionACMBehnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, Tadas Baltru\u0161aitis, and Louis-Philippe Morency. 2016. Deep multimodal fusion for persua- siveness prediction. In Proceedings of the 18th ACM International Conference on Multimodal Interaction. ACM, pages 284-288.\n\nComputational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach. Sunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, Louis-Philippe Morency, Proceedings of the 16th International Conference on Multimodal Interaction. the 16th International Conference on Multimodal InteractionACMSunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, and Louis-Philippe Morency. 2014a. Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal predic- tion approach. In Proceedings of the 16th Interna- tional Conference on Multimodal Interaction. ACM, pages 50-57.\n\nComputational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach. Sunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, Louis-Philippe Morency, 10.1145/2663204.2663260Proceedings of the 16th International Conference on Multimodal Interaction. the 16th International Conference on Multimodal InteractionNew York, NY, USA, ICMIACM14Sunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, and Louis-Philippe Morency. 2014b. Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal pre- diction approach. In Proceedings of the 16th In- ternational Conference on Multimodal Interaction. ACM, New York, NY, USA, ICMI '14, pages 50-57. https://doi.org/10.1145/2663204.2663260.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep- resentation.\n\nUtterance-level multimodal sentiment analysis. Ver\u00f3nica P\u00e9rez-Rosas, Rada Mihalcea, Louis-Philippe Morency, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational Linguistics1Long Papers)Ver\u00f3nica P\u00e9rez-Rosas, Rada Mihalcea, and Louis- Philippe Morency. 2013. Utterance-level multi- modal sentiment analysis. In Proceedings of the 51st Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers). vol- ume 1, pages 973-982.\n\nConvolutional mkl based multimodal emotion recognition and sentiment analysis. Soujanya Poria, Iti Chaturvedi, Erik Cambria, Amir Hussain, Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE. Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Amir Hussain. 2016. Convolutional mkl based mul- timodal emotion recognition and sentiment analysis. In Data Mining (ICDM), 2016 IEEE 16th Interna- tional Conference on. IEEE, pages 439-448.\n\nExtending long short-term memory for multi-view structured learning. Louis-Philippe Shyam Sundar Rajagopalan, Tadas Morency, Roland Baltru\u0161aitis, Goecke, European Conference on Computer Vision. Shyam Sundar Rajagopalan, Louis-Philippe Morency, Tadas Baltru\u0161aitis, and Roland Goecke. 2016. Ex- tending long short-term memory for multi-view structured learning. In European Conference on Computer Vision.\n\nWeighted low rank approximations with provable guarantees. Zhao Ilya Razenshteyn, David P Song, Woodruff, Proceedings of the forty-eighth annual ACM symposium on Theory of Computing. the forty-eighth annual ACM symposium on Theory of ComputingACMIlya Razenshteyn, Zhao Song, and David P Woodruff. 2016. Weighted low rank approximations with prov- able guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing. ACM, pages 250-263.\n\nSelect-additive learning: Improving cross-individual generalization in multimodal sentiment analysis. Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency, Eric P Xing, arXiv:1609.05244arXiv preprintHaohan Wang, Aaksha Meghawat, Louis-Philippe Morency, and Eric P Xing. 2016. Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis. arXiv preprint arXiv:1609.05244 .\n\nA tensor approximation approach to dimensionality reduction. Hongcheng Wang, Narendra Ahuja, International Journal of Computer Vision. 763Hongcheng Wang and Narendra Ahuja. 2008. A ten- sor approximation approach to dimensionality re- duction. International Journal of Computer Vision 76(3):217-229.\n\nYoutube movie reviews: Sentiment analysis in an audio-visual context. Martin W\u00f6llmer, Felix Weninger, Tobias Knaup, Bj\u00f6rn Schuller, Congkai Sun, Kenji Sagae, Louis-Philippe Morency, IEEE Intelligent Systems. 283Martin W\u00f6llmer, Felix Weninger, Tobias Knaup, Bj\u00f6rn Schuller, Congkai Sun, Kenji Sagae, and Louis- Philippe Morency. 2013. Youtube movie reviews: Sentiment analysis in an audio-visual context. IEEE Intelligent Systems 28(3):46-53.\n\nWhat really mattersan information gain analysis of questions and reactions in automated ptsd screenings. Torsten Wortwein, Stefan Scherer, 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII). IEEETorsten Wortwein and Stefan Scherer. 2017. What really mattersan information gain analysis of ques- tions and reactions in automated ptsd screenings. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, pages 15-20.\n\nSpeaker identification on the scotus corpus. Jiahong Yuan, Mark Liberman, Journal of the Acoustical Society of America. 12353878Jiahong Yuan and Mark Liberman. 2008. Speaker iden- tification on the scotus corpus. Journal of the Acous- tical Society of America 123(5):3878.\n\nIntegration of acoustic and visual speech signals using neural networks. Moise H Ben P Yuhas, Terrence J Goldstein, Sejnowski, IEEE Communications Magazine. 2711Ben P Yuhas, Moise H Goldstein, and Terrence J Se- jnowski. 1989. Integration of acoustic and visual speech signals using neural networks. IEEE Com- munications Magazine 27(11):65-71.\n\nTensor fusion network for multimodal sentiment analysis. Amir Zadeh, Minghai Chen, Soujanya Poria, Empirical Methods in Natural Language Processing. Erik Cambria, and Louis-Philippe MorencyAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam- bria, and Louis-Philippe Morency. 2017. Tensor fu- sion network for multimodal sentiment analysis. In Empirical Methods in Natural Language Processing, EMNLP.\n\nSoujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning. Amir Zadeh, Paul Pu Liang, Navonil Mazumder, arXiv:1802.00927arXiv preprintAmir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning. arXiv preprint arXiv:1802.00927 .\n\nMulti-attention recurrent network for human communication comprehension. Amir Zadeh, Paul Pu Liang, Soujanya Poria, arXiv:1802.00923arXiv preprintPrateek Vij, Erik Cambria, and Louis-Philippe MorencyAmir Zadeh, Paul Pu Liang, Soujanya Poria, Pra- teek Vij, Erik Cambria, and Louis-Philippe Morency. 2018b. Multi-attention recurrent network for hu- man communication comprehension. arXiv preprint arXiv:1802.00923 .\n\nMosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos. Amir Zadeh, Rowan Zellers, Eli Pincus, Louis-Philippe Morency, arXiv:1606.06259arXiv preprintAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis- Philippe Morency. 2016a. Mosi: multimodal cor- pus of sentiment intensity and subjectivity anal- ysis in online opinion videos. arXiv preprint arXiv:1606.06259 .\n\nMultimodal sentiment intensity analysis in videos: Facial gestures and verbal messages. Amir Zadeh, Rowan Zellers, Eli Pincus, Louis-Philippe Morency, IEEE Intelligent Systems. 316Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis- Philippe Morency. 2016b. Multimodal sentiment in- tensity analysis in videos: Facial gestures and verbal messages. IEEE Intelligent Systems 31(6):82-88.\n\nFast human detection using a cascade of histograms of oriented gradients. Qiang Zhu, Mei-Chen Yeh, Kwang-Ting Cheng, Shai Avidan, Computer Vision and Pattern Recognition. IEEE Computer Society Conference on. IEEE2Qiang Zhu, Mei-Chen Yeh, Kwang-Ting Cheng, and Shai Avidan. 2006. Fast human detection using a cascade of histograms of oriented gradients. In Com- puter Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. IEEE, volume 2, pages 1491-1498.\n", "annotations": {"author": "[{\"end\":153,\"start\":71},{\"end\":238,\"start\":154},{\"end\":330,\"start\":239},{\"end\":419,\"start\":331},{\"end\":487,\"start\":420},{\"end\":586,\"start\":488}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":76},{\"end\":163,\"start\":159},{\"end\":273,\"start\":256},{\"end\":344,\"start\":339},{\"end\":430,\"start\":425},{\"end\":510,\"start\":503}]", "author_first_name": "[{\"end\":75,\"start\":71},{\"end\":158,\"start\":154},{\"end\":244,\"start\":239},{\"end\":255,\"start\":245},{\"end\":335,\"start\":331},{\"end\":338,\"start\":336},{\"end\":424,\"start\":420},{\"end\":502,\"start\":488}]", "author_affiliation": "[{\"end\":152,\"start\":98},{\"end\":237,\"start\":183},{\"end\":329,\"start\":275},{\"end\":418,\"start\":364},{\"end\":486,\"start\":432},{\"end\":585,\"start\":531}]", "title": "[{\"end\":68,\"start\":1},{\"end\":654,\"start\":587}]", "venue": null, "abstract": "[{\"end\":1820,\"start\":656}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2014,\"start\":1994},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2060,\"start\":2037},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2081,\"start\":2062},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2105,\"start\":2083},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2149,\"start\":2127},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2248,\"start\":2228},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2751,\"start\":2731},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5448,\"start\":5428},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5470,\"start\":5451},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5649,\"start\":5627},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6078,\"start\":6050},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6109,\"start\":6080},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6370,\"start\":6351},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7264,\"start\":7239},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7355,\"start\":7333},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7481,\"start\":7464},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7569,\"start\":7548},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7745,\"start\":7729},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9698,\"start\":9678},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17949,\"start\":17928},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17975,\"start\":17955},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18009,\"start\":17989},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18415,\"start\":18390},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18700,\"start\":18675},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18996,\"start\":18978},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19073,\"start\":19050},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20241,\"start\":20216},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20411,\"start\":20385},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20433,\"start\":20413},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20456,\"start\":20435},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20526,\"start\":20497},{\"end\":20965,\"start\":20944},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21254,\"start\":21234},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21380,\"start\":21354},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21663,\"start\":21642}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":26955,\"start\":26817},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27049,\"start\":26956},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27138,\"start\":27050},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28322,\"start\":27139},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28585,\"start\":28323},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28893,\"start\":28586}]", "paragraph": "[{\"end\":2329,\"start\":1836},{\"end\":3164,\"start\":2331},{\"end\":4473,\"start\":3166},{\"end\":4501,\"start\":4475},{\"end\":4554,\"start\":4503},{\"end\":4681,\"start\":4556},{\"end\":4813,\"start\":4683},{\"end\":4941,\"start\":4815},{\"end\":5874,\"start\":4958},{\"end\":6223,\"start\":5876},{\"end\":7084,\"start\":6225},{\"end\":7990,\"start\":7086},{\"end\":8345,\"start\":7992},{\"end\":8963,\"start\":8376},{\"end\":10072,\"start\":9014},{\"end\":10288,\"start\":10152},{\"end\":10411,\"start\":10290},{\"end\":10646,\"start\":10456},{\"end\":10859,\"start\":10674},{\"end\":11123,\"start\":10861},{\"end\":11571,\"start\":11125},{\"end\":12345,\"start\":11658},{\"end\":12669,\"start\":12379},{\"end\":12906,\"start\":12790},{\"end\":13058,\"start\":12960},{\"end\":13154,\"start\":13087},{\"end\":13253,\"start\":13156},{\"end\":13362,\"start\":13289},{\"end\":13457,\"start\":13364},{\"end\":13635,\"start\":13549},{\"end\":13700,\"start\":13665},{\"end\":13904,\"start\":13734},{\"end\":14000,\"start\":13937},{\"end\":14094,\"start\":14002},{\"end\":14197,\"start\":14124},{\"end\":14599,\"start\":14261},{\"end\":14745,\"start\":14601},{\"end\":15046,\"start\":14900},{\"end\":15806,\"start\":15126},{\"end\":16567,\"start\":15808},{\"end\":16711,\"start\":16631},{\"end\":16873,\"start\":16713},{\"end\":17334,\"start\":16902},{\"end\":17454,\"start\":17336},{\"end\":17601,\"start\":17456},{\"end\":17699,\"start\":17603},{\"end\":17779,\"start\":17701},{\"end\":17841,\"start\":17781},{\"end\":18195,\"start\":17854},{\"end\":18775,\"start\":18208},{\"end\":19286,\"start\":18777},{\"end\":19967,\"start\":19309},{\"end\":21560,\"start\":19987},{\"end\":21878,\"start\":21598},{\"end\":22507,\"start\":21901},{\"end\":22672,\"start\":22534},{\"end\":23154,\"start\":22713},{\"end\":23446,\"start\":23195},{\"end\":23715,\"start\":23448},{\"end\":23950,\"start\":23717},{\"end\":24194,\"start\":23952},{\"end\":24807,\"start\":24218},{\"end\":25506,\"start\":24809},{\"end\":25848,\"start\":25524},{\"end\":26081,\"start\":25850},{\"end\":26615,\"start\":26096},{\"end\":26816,\"start\":26617}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10120,\"start\":10073},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10151,\"start\":10120},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10455,\"start\":10412},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10673,\"start\":10647},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11597,\"start\":11572},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12789,\"start\":12670},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12959,\"start\":12907},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13086,\"start\":13059},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13288,\"start\":13254},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13548,\"start\":13458},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13664,\"start\":13636},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13733,\"start\":13701},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13936,\"start\":13905},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14123,\"start\":14095},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14872,\"start\":14746},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14899,\"start\":14872},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15125,\"start\":15047},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16630,\"start\":16568}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":22955,\"start\":22948},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23366,\"start\":23359},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24943,\"start\":24936}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1834,\"start\":1822},{\"attributes\":{\"n\":\"2\"},\"end\":4956,\"start\":4944},{\"attributes\":{\"n\":\"3\"},\"end\":8374,\"start\":8348},{\"attributes\":{\"n\":\"3.1\"},\"end\":9012,\"start\":8966},{\"attributes\":{\"n\":\"3.2\"},\"end\":11656,\"start\":11599},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":12377,\"start\":12348},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14259,\"start\":14200},{\"attributes\":{\"n\":\"4\"},\"end\":16900,\"start\":16876},{\"attributes\":{\"n\":\"4.1\"},\"end\":17852,\"start\":17844},{\"attributes\":{\"n\":\"4.2\"},\"end\":18206,\"start\":18198},{\"attributes\":{\"n\":\"4.3\"},\"end\":19307,\"start\":19289},{\"attributes\":{\"n\":\"4.4\"},\"end\":19985,\"start\":19970},{\"end\":21596,\"start\":21563},{\"attributes\":{\"n\":\"4.5\"},\"end\":21899,\"start\":21881},{\"attributes\":{\"n\":\"5\"},\"end\":22532,\"start\":22510},{\"attributes\":{\"n\":\"5.1\"},\"end\":22711,\"start\":22675},{\"attributes\":{\"n\":\"5.2\"},\"end\":23193,\"start\":23157},{\"attributes\":{\"n\":\"5.3\"},\"end\":24216,\"start\":24197},{\"attributes\":{\"n\":\"5.4\"},\"end\":25522,\"start\":25509},{\"attributes\":{\"n\":\"6\"},\"end\":26094,\"start\":26084},{\"end\":26967,\"start\":26957},{\"end\":27060,\"start\":27051},{\"end\":28333,\"start\":28324},{\"end\":28596,\"start\":28587}]", "table": "[{\"end\":28322,\"start\":27364},{\"end\":28585,\"start\":28489}]", "figure_caption": "[{\"end\":26955,\"start\":26819},{\"end\":27049,\"start\":26969},{\"end\":27138,\"start\":27062},{\"end\":27364,\"start\":27141},{\"end\":28489,\"start\":28335},{\"end\":28893,\"start\":28598}]", "figure_ref": "[{\"end\":3386,\"start\":3378},{\"end\":4036,\"start\":4028},{\"end\":11100,\"start\":11092},{\"end\":11267,\"start\":11259},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13999,\"start\":13991},{\"end\":14971,\"start\":14963},{\"end\":19966,\"start\":19958},{\"end\":25676,\"start\":25668},{\"end\":25907,\"start\":25899}]", "bib_author_first_name": "[{\"end\":29475,\"start\":29469},{\"end\":29490,\"start\":29483},{\"end\":29506,\"start\":29498},{\"end\":29515,\"start\":29512},{\"end\":29533,\"start\":29528},{\"end\":29547,\"start\":29541},{\"end\":29562,\"start\":29553},{\"end\":29577,\"start\":29570},{\"end\":29592,\"start\":29583},{\"end\":29594,\"start\":29593},{\"end\":30054,\"start\":30048},{\"end\":30056,\"start\":30055},{\"end\":30081,\"start\":30074},{\"end\":30095,\"start\":30089},{\"end\":30540,\"start\":30533},{\"end\":30550,\"start\":30547},{\"end\":30561,\"start\":30557},{\"end\":30564,\"start\":30562},{\"end\":30577,\"start\":30572},{\"end\":30596,\"start\":30592},{\"end\":30618,\"start\":30604},{\"end\":31213,\"start\":31206},{\"end\":31230,\"start\":31222},{\"end\":31432,\"start\":31419},{\"end\":31447,\"start\":31440},{\"end\":31464,\"start\":31458},{\"end\":31882,\"start\":31876},{\"end\":31897,\"start\":31893},{\"end\":31910,\"start\":31904},{\"end\":31925,\"start\":31920},{\"end\":31940,\"start\":31934},{\"end\":32403,\"start\":32398},{\"end\":32415,\"start\":32411},{\"end\":32419,\"start\":32416},{\"end\":32432,\"start\":32426},{\"end\":32443,\"start\":32439},{\"end\":32460,\"start\":32454},{\"end\":32476,\"start\":32470},{\"end\":32767,\"start\":32763},{\"end\":32786,\"start\":32780},{\"end\":33070,\"start\":33062},{\"end\":33079,\"start\":33075},{\"end\":33089,\"start\":33085},{\"end\":33103,\"start\":33096},{\"end\":33116,\"start\":33111},{\"end\":33122,\"start\":33121},{\"end\":33138,\"start\":33131},{\"end\":33140,\"start\":33139},{\"end\":33171,\"start\":33164},{\"end\":33452,\"start\":33446},{\"end\":33468,\"start\":33459},{\"end\":33726,\"start\":33723},{\"end\":33734,\"start\":33732},{\"end\":33744,\"start\":33740},{\"end\":33758,\"start\":33752},{\"end\":33774,\"start\":33769},{\"end\":34320,\"start\":34306},{\"end\":34334,\"start\":34330},{\"end\":34350,\"start\":34345},{\"end\":34800,\"start\":34794},{\"end\":34823,\"start\":34817},{\"end\":34841,\"start\":34834},{\"end\":34856,\"start\":34851},{\"end\":34885,\"start\":34871},{\"end\":35435,\"start\":35427},{\"end\":35445,\"start\":35442},{\"end\":35449,\"start\":35446},{\"end\":35464,\"start\":35456},{\"end\":35482,\"start\":35477},{\"end\":35504,\"start\":35490},{\"end\":36090,\"start\":36082},{\"end\":36100,\"start\":36097},{\"end\":36104,\"start\":36101},{\"end\":36119,\"start\":36111},{\"end\":36137,\"start\":36132},{\"end\":36159,\"start\":36145},{\"end\":36794,\"start\":36787},{\"end\":36814,\"start\":36807},{\"end\":36836,\"start\":36823},{\"end\":37020,\"start\":37012},{\"end\":37038,\"start\":37034},{\"end\":37063,\"start\":37049},{\"end\":37601,\"start\":37593},{\"end\":37612,\"start\":37609},{\"end\":37629,\"start\":37625},{\"end\":37643,\"start\":37639},{\"end\":38048,\"start\":38034},{\"end\":38080,\"start\":38075},{\"end\":38096,\"start\":38090},{\"end\":38432,\"start\":38428},{\"end\":38458,\"start\":38451},{\"end\":38942,\"start\":38936},{\"end\":38955,\"start\":38949},{\"end\":38980,\"start\":38966},{\"end\":38994,\"start\":38990},{\"end\":38996,\"start\":38995},{\"end\":39317,\"start\":39308},{\"end\":39332,\"start\":39324},{\"end\":39624,\"start\":39618},{\"end\":39639,\"start\":39634},{\"end\":39656,\"start\":39650},{\"end\":39669,\"start\":39664},{\"end\":39687,\"start\":39680},{\"end\":39698,\"start\":39693},{\"end\":39720,\"start\":39706},{\"end\":40103,\"start\":40096},{\"end\":40120,\"start\":40114},{\"end\":40553,\"start\":40546},{\"end\":40564,\"start\":40560},{\"end\":40853,\"start\":40848},{\"end\":40855,\"start\":40854},{\"end\":40877,\"start\":40869},{\"end\":40879,\"start\":40878},{\"end\":41182,\"start\":41178},{\"end\":41197,\"start\":41190},{\"end\":41212,\"start\":41204},{\"end\":41649,\"start\":41645},{\"end\":41661,\"start\":41657},{\"end\":41664,\"start\":41662},{\"end\":41679,\"start\":41672},{\"end\":42000,\"start\":41996},{\"end\":42012,\"start\":42008},{\"end\":42015,\"start\":42013},{\"end\":42031,\"start\":42023},{\"end\":42442,\"start\":42438},{\"end\":42455,\"start\":42450},{\"end\":42468,\"start\":42465},{\"end\":42491,\"start\":42477},{\"end\":42836,\"start\":42832},{\"end\":42849,\"start\":42844},{\"end\":42862,\"start\":42859},{\"end\":42885,\"start\":42871},{\"end\":43207,\"start\":43202},{\"end\":43221,\"start\":43213},{\"end\":43237,\"start\":43227},{\"end\":43249,\"start\":43245}]", "bib_author_last_name": "[{\"end\":29481,\"start\":29476},{\"end\":29496,\"start\":29491},{\"end\":29510,\"start\":29507},{\"end\":29526,\"start\":29516},{\"end\":29539,\"start\":29534},{\"end\":29551,\"start\":29548},{\"end\":29568,\"start\":29563},{\"end\":29581,\"start\":29578},{\"end\":29604,\"start\":29595},{\"end\":30072,\"start\":30057},{\"end\":30087,\"start\":30082},{\"end\":30104,\"start\":30096},{\"end\":30113,\"start\":30106},{\"end\":30545,\"start\":30541},{\"end\":30555,\"start\":30551},{\"end\":30570,\"start\":30565},{\"end\":30590,\"start\":30578},{\"end\":30602,\"start\":30597},{\"end\":30626,\"start\":30619},{\"end\":31220,\"start\":31214},{\"end\":31237,\"start\":31231},{\"end\":31438,\"start\":31433},{\"end\":31456,\"start\":31448},{\"end\":31472,\"start\":31465},{\"end\":31891,\"start\":31883},{\"end\":31902,\"start\":31898},{\"end\":31918,\"start\":31911},{\"end\":31932,\"start\":31926},{\"end\":31948,\"start\":31941},{\"end\":32409,\"start\":32404},{\"end\":32424,\"start\":32420},{\"end\":32437,\"start\":32433},{\"end\":32452,\"start\":32444},{\"end\":32468,\"start\":32461},{\"end\":32485,\"start\":32477},{\"end\":32778,\"start\":32768},{\"end\":32798,\"start\":32787},{\"end\":33073,\"start\":33071},{\"end\":33083,\"start\":33080},{\"end\":33094,\"start\":33090},{\"end\":33109,\"start\":33104},{\"end\":33119,\"start\":33117},{\"end\":33129,\"start\":33123},{\"end\":33150,\"start\":33141},{\"end\":33162,\"start\":33152},{\"end\":33188,\"start\":33172},{\"end\":33194,\"start\":33190},{\"end\":33457,\"start\":33453},{\"end\":33475,\"start\":33469},{\"end\":33730,\"start\":33727},{\"end\":33738,\"start\":33735},{\"end\":33750,\"start\":33745},{\"end\":33767,\"start\":33759},{\"end\":33783,\"start\":33775},{\"end\":34328,\"start\":34321},{\"end\":34343,\"start\":34335},{\"end\":34356,\"start\":34351},{\"end\":34815,\"start\":34801},{\"end\":34832,\"start\":34824},{\"end\":34849,\"start\":34842},{\"end\":34869,\"start\":34857},{\"end\":34893,\"start\":34886},{\"end\":35440,\"start\":35436},{\"end\":35454,\"start\":35450},{\"end\":35475,\"start\":35465},{\"end\":35488,\"start\":35483},{\"end\":35512,\"start\":35505},{\"end\":36095,\"start\":36091},{\"end\":36109,\"start\":36105},{\"end\":36130,\"start\":36120},{\"end\":36143,\"start\":36138},{\"end\":36167,\"start\":36160},{\"end\":36805,\"start\":36795},{\"end\":36821,\"start\":36815},{\"end\":36844,\"start\":36837},{\"end\":37032,\"start\":37021},{\"end\":37047,\"start\":37039},{\"end\":37071,\"start\":37064},{\"end\":37607,\"start\":37602},{\"end\":37623,\"start\":37613},{\"end\":37637,\"start\":37630},{\"end\":37651,\"start\":37644},{\"end\":38073,\"start\":38049},{\"end\":38088,\"start\":38081},{\"end\":38109,\"start\":38097},{\"end\":38117,\"start\":38111},{\"end\":38449,\"start\":38433},{\"end\":38463,\"start\":38459},{\"end\":38473,\"start\":38465},{\"end\":38947,\"start\":38943},{\"end\":38964,\"start\":38956},{\"end\":38988,\"start\":38981},{\"end\":39001,\"start\":38997},{\"end\":39322,\"start\":39318},{\"end\":39338,\"start\":39333},{\"end\":39632,\"start\":39625},{\"end\":39648,\"start\":39640},{\"end\":39662,\"start\":39657},{\"end\":39678,\"start\":39670},{\"end\":39691,\"start\":39688},{\"end\":39704,\"start\":39699},{\"end\":39728,\"start\":39721},{\"end\":40112,\"start\":40104},{\"end\":40128,\"start\":40121},{\"end\":40558,\"start\":40554},{\"end\":40573,\"start\":40565},{\"end\":40867,\"start\":40856},{\"end\":40889,\"start\":40880},{\"end\":40900,\"start\":40891},{\"end\":41188,\"start\":41183},{\"end\":41202,\"start\":41198},{\"end\":41218,\"start\":41213},{\"end\":41655,\"start\":41650},{\"end\":41670,\"start\":41665},{\"end\":41688,\"start\":41680},{\"end\":42006,\"start\":42001},{\"end\":42021,\"start\":42016},{\"end\":42037,\"start\":42032},{\"end\":42448,\"start\":42443},{\"end\":42463,\"start\":42456},{\"end\":42475,\"start\":42469},{\"end\":42499,\"start\":42492},{\"end\":42842,\"start\":42837},{\"end\":42857,\"start\":42850},{\"end\":42869,\"start\":42863},{\"end\":42893,\"start\":42886},{\"end\":43211,\"start\":43208},{\"end\":43225,\"start\":43222},{\"end\":43243,\"start\":43238},{\"end\":43256,\"start\":43250}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1007/s10579-008-9076-6\",\"id\":\"b0\",\"matched_paper_id\":11820063},\"end\":29997,\"start\":29406},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":28885654},\"end\":30451,\"start\":29999},{\"attributes\":{\"doi\":\"10.1145/3136755.3136801\",\"id\":\"b2\",\"matched_paper_id\":3611592},\"end\":31180,\"start\":30453},{\"attributes\":{\"id\":\"b3\"},\"end\":31359,\"start\":31182},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":61371405},\"end\":31800,\"start\":31361},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":746430},\"end\":32308,\"start\":31802},{\"attributes\":{\"doi\":\"arXiv:1606.01847\",\"id\":\"b6\"},\"end\":32737,\"start\":32310},{\"attributes\":{\"doi\":\"10.1162/neco.1997.9.8.1735\",\"id\":\"b7\",\"matched_paper_id\":1915014},\"end\":32988,\"start\":32739},{\"attributes\":{\"id\":\"b8\"},\"end\":33412,\"start\":32990},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":747419},\"end\":33669,\"start\":33414},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":15307333},\"end\":34231,\"start\":33671},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1257599},\"end\":34738,\"start\":34233},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":765502},\"end\":35310,\"start\":34740},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18382891},\"end\":35965,\"start\":35312},{\"attributes\":{\"doi\":\"10.1145/2663204.2663260\",\"id\":\"b14\",\"matched_paper_id\":18382891},\"end\":36738,\"start\":35967},{\"attributes\":{\"id\":\"b15\"},\"end\":36963,\"start\":36740},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7203951},\"end\":37512,\"start\":36965},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5749615},\"end\":37963,\"start\":37514},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":31438356},\"end\":38367,\"start\":37965},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1304205},\"end\":38832,\"start\":38369},{\"attributes\":{\"doi\":\"arXiv:1609.05244\",\"id\":\"b20\"},\"end\":39245,\"start\":38834},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2226070},\"end\":39546,\"start\":39247},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12789201},\"end\":39989,\"start\":39548},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3416629},\"end\":40499,\"start\":39991},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14761767},\"end\":40773,\"start\":40501},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":18498061},\"end\":41119,\"start\":40775},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":950292},\"end\":41520,\"start\":41121},{\"attributes\":{\"doi\":\"arXiv:1802.00927\",\"id\":\"b27\"},\"end\":41921,\"start\":41522},{\"attributes\":{\"doi\":\"arXiv:1802.00923\",\"id\":\"b28\"},\"end\":42337,\"start\":41923},{\"attributes\":{\"doi\":\"arXiv:1606.06259\",\"id\":\"b29\"},\"end\":42742,\"start\":42339},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1672698},\"end\":43126,\"start\":42744},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7800101},\"end\":43601,\"start\":43128}]", "bib_title": "[{\"end\":29467,\"start\":29406},{\"end\":30046,\"start\":29999},{\"end\":30531,\"start\":30453},{\"end\":31204,\"start\":31182},{\"end\":31417,\"start\":31361},{\"end\":31874,\"start\":31802},{\"end\":32761,\"start\":32739},{\"end\":33444,\"start\":33414},{\"end\":33721,\"start\":33671},{\"end\":34304,\"start\":34233},{\"end\":34792,\"start\":34740},{\"end\":35425,\"start\":35312},{\"end\":36080,\"start\":35967},{\"end\":37010,\"start\":36965},{\"end\":37591,\"start\":37514},{\"end\":38032,\"start\":37965},{\"end\":38426,\"start\":38369},{\"end\":39306,\"start\":39247},{\"end\":39616,\"start\":39548},{\"end\":40094,\"start\":39991},{\"end\":40544,\"start\":40501},{\"end\":40846,\"start\":40775},{\"end\":41176,\"start\":41121},{\"end\":42830,\"start\":42744},{\"end\":43200,\"start\":43128}]", "bib_author": "[{\"end\":29483,\"start\":29469},{\"end\":29498,\"start\":29483},{\"end\":29512,\"start\":29498},{\"end\":29528,\"start\":29512},{\"end\":29541,\"start\":29528},{\"end\":29553,\"start\":29541},{\"end\":29570,\"start\":29553},{\"end\":29583,\"start\":29570},{\"end\":29606,\"start\":29583},{\"end\":30074,\"start\":30048},{\"end\":30089,\"start\":30074},{\"end\":30106,\"start\":30089},{\"end\":30115,\"start\":30106},{\"end\":30547,\"start\":30533},{\"end\":30557,\"start\":30547},{\"end\":30572,\"start\":30557},{\"end\":30592,\"start\":30572},{\"end\":30604,\"start\":30592},{\"end\":30628,\"start\":30604},{\"end\":31222,\"start\":31206},{\"end\":31239,\"start\":31222},{\"end\":31440,\"start\":31419},{\"end\":31458,\"start\":31440},{\"end\":31474,\"start\":31458},{\"end\":31893,\"start\":31876},{\"end\":31904,\"start\":31893},{\"end\":31920,\"start\":31904},{\"end\":31934,\"start\":31920},{\"end\":31950,\"start\":31934},{\"end\":32411,\"start\":32398},{\"end\":32426,\"start\":32411},{\"end\":32439,\"start\":32426},{\"end\":32454,\"start\":32439},{\"end\":32470,\"start\":32454},{\"end\":32487,\"start\":32470},{\"end\":32780,\"start\":32763},{\"end\":32800,\"start\":32780},{\"end\":33075,\"start\":33062},{\"end\":33085,\"start\":33075},{\"end\":33096,\"start\":33085},{\"end\":33111,\"start\":33096},{\"end\":33121,\"start\":33111},{\"end\":33131,\"start\":33121},{\"end\":33152,\"start\":33131},{\"end\":33164,\"start\":33152},{\"end\":33190,\"start\":33164},{\"end\":33196,\"start\":33190},{\"end\":33459,\"start\":33446},{\"end\":33477,\"start\":33459},{\"end\":33732,\"start\":33723},{\"end\":33740,\"start\":33732},{\"end\":33752,\"start\":33740},{\"end\":33769,\"start\":33752},{\"end\":33785,\"start\":33769},{\"end\":34330,\"start\":34306},{\"end\":34345,\"start\":34330},{\"end\":34358,\"start\":34345},{\"end\":34817,\"start\":34794},{\"end\":34834,\"start\":34817},{\"end\":34851,\"start\":34834},{\"end\":34871,\"start\":34851},{\"end\":34895,\"start\":34871},{\"end\":35442,\"start\":35427},{\"end\":35456,\"start\":35442},{\"end\":35477,\"start\":35456},{\"end\":35490,\"start\":35477},{\"end\":35514,\"start\":35490},{\"end\":36097,\"start\":36082},{\"end\":36111,\"start\":36097},{\"end\":36132,\"start\":36111},{\"end\":36145,\"start\":36132},{\"end\":36169,\"start\":36145},{\"end\":36807,\"start\":36787},{\"end\":36823,\"start\":36807},{\"end\":36846,\"start\":36823},{\"end\":37034,\"start\":37012},{\"end\":37049,\"start\":37034},{\"end\":37073,\"start\":37049},{\"end\":37609,\"start\":37593},{\"end\":37625,\"start\":37609},{\"end\":37639,\"start\":37625},{\"end\":37653,\"start\":37639},{\"end\":38075,\"start\":38034},{\"end\":38090,\"start\":38075},{\"end\":38111,\"start\":38090},{\"end\":38119,\"start\":38111},{\"end\":38451,\"start\":38428},{\"end\":38465,\"start\":38451},{\"end\":38475,\"start\":38465},{\"end\":38949,\"start\":38936},{\"end\":38966,\"start\":38949},{\"end\":38990,\"start\":38966},{\"end\":39003,\"start\":38990},{\"end\":39324,\"start\":39308},{\"end\":39340,\"start\":39324},{\"end\":39634,\"start\":39618},{\"end\":39650,\"start\":39634},{\"end\":39664,\"start\":39650},{\"end\":39680,\"start\":39664},{\"end\":39693,\"start\":39680},{\"end\":39706,\"start\":39693},{\"end\":39730,\"start\":39706},{\"end\":40114,\"start\":40096},{\"end\":40130,\"start\":40114},{\"end\":40560,\"start\":40546},{\"end\":40575,\"start\":40560},{\"end\":40869,\"start\":40848},{\"end\":40891,\"start\":40869},{\"end\":40902,\"start\":40891},{\"end\":41190,\"start\":41178},{\"end\":41204,\"start\":41190},{\"end\":41220,\"start\":41204},{\"end\":41657,\"start\":41645},{\"end\":41672,\"start\":41657},{\"end\":41690,\"start\":41672},{\"end\":42008,\"start\":41996},{\"end\":42023,\"start\":42008},{\"end\":42039,\"start\":42023},{\"end\":42450,\"start\":42438},{\"end\":42465,\"start\":42450},{\"end\":42477,\"start\":42465},{\"end\":42501,\"start\":42477},{\"end\":42844,\"start\":42832},{\"end\":42859,\"start\":42844},{\"end\":42871,\"start\":42859},{\"end\":42895,\"start\":42871},{\"end\":43213,\"start\":43202},{\"end\":43227,\"start\":43213},{\"end\":43245,\"start\":43227},{\"end\":43258,\"start\":43245}]", "bib_venue": "[{\"end\":30811,\"start\":30731},{\"end\":33946,\"start\":33874},{\"end\":34495,\"start\":34435},{\"end\":35038,\"start\":34975},{\"end\":35649,\"start\":35590},{\"end\":36350,\"start\":36268},{\"end\":37234,\"start\":37162},{\"end\":38612,\"start\":38552},{\"end\":29675,\"start\":29631},{\"end\":30159,\"start\":30115},{\"end\":30729,\"start\":30651},{\"end\":31255,\"start\":31239},{\"end\":31523,\"start\":31474},{\"end\":31993,\"start\":31950},{\"end\":32396,\"start\":32310},{\"end\":32839,\"start\":32826},{\"end\":33060,\"start\":32990},{\"end\":33525,\"start\":33477},{\"end\":33872,\"start\":33785},{\"end\":34433,\"start\":34358},{\"end\":34973,\"start\":34895},{\"end\":35588,\"start\":35514},{\"end\":36266,\"start\":36192},{\"end\":36785,\"start\":36740},{\"end\":37160,\"start\":37073},{\"end\":37721,\"start\":37653},{\"end\":38157,\"start\":38119},{\"end\":38550,\"start\":38475},{\"end\":38934,\"start\":38834},{\"end\":39380,\"start\":39340},{\"end\":39754,\"start\":39730},{\"end\":40225,\"start\":40130},{\"end\":40619,\"start\":40575},{\"end\":40930,\"start\":40902},{\"end\":41268,\"start\":41220},{\"end\":41643,\"start\":41522},{\"end\":41994,\"start\":41923},{\"end\":42436,\"start\":42339},{\"end\":42919,\"start\":42895},{\"end\":43297,\"start\":43258}]"}}}, "year": 2023, "month": 12, "day": 17}