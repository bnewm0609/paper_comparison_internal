{"id": 246240082, "updated": "2023-10-24 13:25:02.534", "metadata": {"title": "Improving Chest X-Ray Report Generation by Leveraging Warm Starting", "authors": "[{\"first\":\"Aaron\",\"last\":\"Nicolson\",\"middle\":[]},{\"first\":\"Jason\",\"last\":\"Dowling\",\"middle\":[]},{\"first\":\"Bevan\",\"last\":\"Koopman\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Automatically generating a report from a patient's Chest X-Rays (CXRs) is a promising solution to reducing clinical workload and improving patient care. However, current CXR report generators -- which are predominantly encoder-to-decoder models -- lack the diagnostic accuracy to be deployed in a clinical setting. To improve CXR report generation, we investigate warm starting the encoder and decoder with recent open-source computer vision and natural language processing checkpoints, such as the Vision Transformer (ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR and IU X-Ray datasets. Our experimental investigation demonstrates that the Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warm starting the encoder and decoder, respectively. Compared to the state-of-the-art ($\\mathcal{M}^2$ Transformer Progressive), CvT2DistilGPT2 attained an improvement of 8.3\\% for CE F-1, 1.8\\% for BLEU-4, 1.6\\% for ROUGE-L, and 1.0\\% for METEOR. The reports generated by CvT2DistilGPT2 have a higher similarity to radiologist reports than previous approaches. This indicates that leveraging warm starting improves CXR report generation. Code and checkpoints for CvT2DistilGPT2 are available at https://github.com/aehrc/cvt2distilgpt2.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.09405", "mag": null, "acl": null, "pubmed": "37783533", "pubmedcentral": null, "dblp": "journals/artmed/NicolsonDK23", "doi": "10.1016/j.artmed.2023.102633"}}, "content": {"source": {"pdf_hash": "45e771857bf1df39b2c568a99f3b57cc0a9c736f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2201.09405v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "02be24fcc59e00158535c24bbff635655f2c3804", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/45e771857bf1df39b2c568a99f3b57cc0a9c736f.txt", "contents": "\n\n12 Jul 2023 July 14, 2023\n\nPhDAaron Nicolson aaron.nicolson@csiro.au \nAustralian e-Health Research Centre\n\n12 Jul 2023 July 14, 2023Editor Pattern Recognition Dear Editorial Board of Pattern Recognition, We wish to submit \"Improving Chest X-Ray Report Generation by Leveraging Warm-Starting\" for consid-eration in Pattern Recognition.\n\n\nOur work addressing the need for a multi-modal machine learning approach-combining image and text representations-to generate captions for medical images. In particular, we utilise transfer learning from pre-trained models taken from both the general and medical domains.\n\nThe paper fits well within Pattern Recognition's state aims of \"computer vision, image processing, text and document analysis and neural networks\". Specifically, it would appeal to readers because it bridges all four of these stated topics by developing and evaluating models that combine image and text representation in a single neural network system. While this is evaluated within the context of generating natural language captions from medical images, the methods are general and would be of interest to those working with non-medical images.\n\n\nRelated Pattern Recognition articles:\n\nOur manuscript has strong connection to the following articles recently published in Pattern Recognition: A number of these use the same datasets and task of chest x-rays report generation. A key difference though is we employ new and emerging computer vision and NLP models to the task and investigate the key decision of which pre-trained model should be used for initialisation. This is becoming increasingly important as a plethora of pre-trained models are now available through repositories such as Huggingface.\n\nWe confirm that this work is original and has not been published elsewhere nor is it currently under consideration for publication elsewhere.\n\nPlease address all correspondence concerning this manuscript to me at aaron.nicolson@csiro.au.\n\nThank you for your consideration of this manuscript.\n\n\nSincerely,\n\nA. Nicolson\n\n\u2022\nAyesha et al. Automatic medical image interpretation: State of the art and future directions. Pattern Recognition, 114:107856, Jun 2021. (6 citations) \u2022 Li et al. Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19. Pattern Recognition, 114:107848, Jun 2021. (21 citations) \u2022 Singh et al. MetaMed: Few-shot medical image classification using gradient-based meta-learning. Pattern Recognition, 120:108111, Dec 2021. \u2022 Wang et al. EANet: Iterative edge attention network for medical image segmentation. Pattern Recognition, 127:108636, Jul 2022.\n", "annotations": {"author": "[{\"end\":109,\"start\":29}]", "publisher": null, "author_last_name": "[{\"end\":46,\"start\":38}]", "author_first_name": "[{\"end\":37,\"start\":32}]", "author_affiliation": "[{\"end\":108,\"start\":72}]", "title": null, "venue": null, "abstract": null, "bib_ref": null, "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":2614,\"start\":2039}]", "paragraph": "[{\"end\":610,\"start\":339},{\"end\":1160,\"start\":612},{\"end\":1719,\"start\":1202},{\"end\":1862,\"start\":1721},{\"end\":1958,\"start\":1864},{\"end\":2012,\"start\":1960},{\"end\":2038,\"start\":2027}]", "formula": null, "table_ref": null, "section_header": "[{\"end\":1200,\"start\":1163},{\"end\":2025,\"start\":2015},{\"end\":2041,\"start\":2040}]", "table": null, "figure_caption": "[{\"end\":2614,\"start\":2042}]", "figure_ref": null, "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}